
A FIRST COURSE IN RANDOM MATRIX THEORY
The real world is perceived and broken down as data, models and algorithms in the eyes
of physicists and engineers. Data is noisy by nature and classical statistical tools have so
far been successful in dealing with relatively smaller levels of randomness. The recent
emergence of Big Data and the required computing power to analyze them have rendered
classical tools outdated and insufﬁcient. Tools such as random matrix theory and the study
of large sample covariance matrices can efﬁciently process these big datasets and help make
sense of modern, deep learning algorithms. Presenting an introductory calculus course for
random matrices, the book focuses on modern concepts in matrix theory, generalizing
the standard concept of probabilistic independence to non-commuting random variables.
Concretely worked out examples and applications to ﬁnancial engineering and portfolio
construction make this unique book an essential tool for physicists, engineers, data analysts
and economists.
marc potters is Chief Investment Ofﬁcer of CFM, an investment ﬁrm based in Paris.
Marc maintains strong links with academia and, as an expert in random matrix theory, he
has taught at UCLA and Sorbonne University.
jean-philippe bouchaud
is a pioneer in econophysics. His research includes ran-
dom matrix theory, statistics of price formation, stock market ﬂuctuations, and agent-based
models for ﬁnancial markets and macroeconomics. His previous books include Theory of
Financial Risk and Derivative Pricing (Cambridge University Press, 2003) and Trades,
Quotes and Prices (Cambridge University Press, 2018), and he has been the recipient of
several prestigious, international awards.


A FIRST COURSE IN RANDOM
MATRIX THEORY
for Physicists, Engineers and Data Scientists
MARC POTTERS
Capital Fund Management, Paris
JEAN-PHILIPPE BOUCHAUD
Capital Fund Management, Paris

University Printing House, Cambridge CB2 8BS, United Kingdom
One Liberty Plaza, 20th Floor, New York, NY 10006, USA
477 Williamstown Road, Port Melbourne, VIC 3207, Australia
314–321, 3rd Floor, Plot 3, Splendor Forum, Jasola District Centre, New Delhi – 110025, India
79 Anson Road, #06–04/06, Singapore 079906
Cambridge University Press is part of the University of Cambridge.
It furthers the University’s mission by disseminating knowledge in the pursuit of
education, learning, and research at the highest international levels of excellence.
www.cambridge.org
Information on this title: www.cambridge.org/9781108488082
DOI: 10.1017/9781108768900
© Cambridge University Press 2021
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 2021
Printed in the United Kingdom by TJ Books Limited
A catalogue record for this publication is available from the British Library.
Library of Congress Cataloging-in-Publication Data
Names: Potters, Marc, 1969– author. | Bouchaud, Jean-Philippe, 1962– author.
Title: A ﬁrst course in random matrix theory : for physicists, engineers
and data scientists / Marc Potters, Jean-Philippe Bouchaud.
Description: Cambridge ; New York, NY : Cambridge University Press, 2021. |
Includes bibliographical references and index.
Identiﬁers: LCCN 2020022793 (print) | LCCN 2020022794 (ebook) |
ISBN 9781108488082 (hardback) | ISBN 9781108768900 (epub)
Subjects: LCSH: Random matrices.
Classiﬁcation: LCC QA196.5 .P68 2021 (print) | LCC QA196.5 (ebook) |
DDC 512.9/434–dc23
LC record available at https://lccn.loc.gov/2020022793
LC ebook record available at https://lccn.loc.gov/2020022794
ISBN 978-1-108-48808-2 Hardback
Additional resources for this title at www.cambridge.org/potters
Cambridge University Press has no responsibility for the persistence or accuracy
of URLs for external or third-party internet websites referred to in this publication
and does not guarantee that any content on such websites is, or will remain,
accurate or appropriate.

Contents
Preface
page ix
List of Symbols
xiv
Part I
Classical Random Matrix Theory
1
1
Deterministic Matrices
3
1.1
Matrices, Eigenvalues and Singular Values
3
1.2
Some Useful Theorems and Identities
9
2
Wigner Ensemble and Semi-Circle Law
15
2.1
Normalized Trace and Sample Averages
16
2.2
The Wigner Ensemble
17
2.3
Resolvent and Stieltjes Transform
19
3
More on Gaussian Matrices*
30
3.1
Other Gaussian Ensembles
30
3.2
Moments and Non-Crossing Pair Partitions
36
4
Wishart Ensemble and Marˇcenko–Pastur Distribution
43
4.1
Wishart Matrices
43
4.2
Marˇcenko–Pastur Using the Cavity Method
48
5
Joint Distribution of Eigenvalues
58
5.1
From Matrix Elements to Eigenvalues
58
5.2
Coulomb Gas and Maximum Likelihood Conﬁgurations
64
5.3
Applications: Wigner, Wishart and the One-Cut Assumption
69
5.4
Fluctuations Around the Most Likely Conﬁguration
73
5.5
An Eigenvalue Density Saddle Point
78
6
Eigenvalues and Orthogonal Polynomials*
83
6.1
Wigner Matrices and Hermite Polynomials
83
6.2
Laguerre Polynomials
87
6.3
Unitary Ensembles
91
v

vi
Contents
7
The Jacobi Ensemble*
97
7.1
Properties of Jacobi Matrices
97
7.2
Jacobi Matrices and Jacobi Polynomials
102
Part II
Sums and Products of Random Matrices
109
8
Addition of Random Variables and Brownian Motion
111
8.1
Sums of Random Variables
111
8.2
Stochastic Calculus
112
9
Dyson Brownian Motion
121
9.1
Dyson Brownian Motion I: Perturbation Theory
121
9.2
Dyson Brownian Motion II: Itˆo Calculus
124
9.3
The Dyson Brownian Motion for the Resolvent
126
9.4
The Dyson Brownian Motion with a Potential
129
9.5
Non-Intersecting Brownian Motions and the Karlin–McGregor Formula
133
10
Addition of Large Random Matrices
136
10.1
Adding a Large Wigner Matrix to an Arbitrary Matrix
136
10.2
Generalization to Non-Wigner Matrices
140
10.3
The Rank-1 HCIZ Integral
142
10.4
Invertibility of the Stieltjes Transform
145
10.5
The Full-Rank HCIZ Integral
149
11
Free Probabilities
155
11.1
Algebraic Probabilities: Some Deﬁnitions
155
11.2
Addition of Commuting Variables
156
11.3
Non-Commuting Variables
161
11.4
Free Product
170
12
Free Random Matrices
177
12.1
Random Rotations and Freeness
177
12.2
R-Transforms and Resummed Perturbation Theory
181
12.3
The Central Limit Theorem for Matrices
183
12.4
Finite Free Convolutions
186
12.5
Freeness for 2 × 2 Matrices
193
13
The Replica Method*
199
13.1
Stieltjes Transform
200
13.2
Resolvent Matrix
204
13.3
Rank-1 HCIZ and Replicas
209
13.4
Spin-Glasses, Replicas and Low-Rank HCIZ
215

Contents
vii
14
Edge Eigenvalues and Outliers
220
14.1
The Tracy–Widom Regime
221
14.2
Additive Low-Rank Perturbations
223
14.3
Fat Tails
229
14.4
Multiplicative Perturbation
231
14.5
Phase Retrieval and Outliers
234
Part III
Applications
241
15
Addition and Multiplication: Recipes and Examples
243
15.1
Summary
243
15.2
R- and S-Transforms and Moments of Useful Ensembles
245
15.3
Worked-Out Examples: Addition
249
15.4
Worked-Out Examples: Multiplication
252
16
Products of Many Random Matrices
257
16.1
Products of Many Free Matrices
257
16.2
The Free Log-Normal
261
16.3
A Multiplicative Dyson Brownian Motion
262
16.4
The Matrix Kesten Problem
264
17
Sample Covariance Matrices
267
17.1
Spatial Correlations
267
17.2
Temporal Correlations
271
17.3
Time Dependent Variance
276
17.4
Empirical Cross-Covariance Matrices
278
18
Bayesian Estimation
281
18.1
Bayesian Estimation
281
18.2
Estimating a Vector: Ridge and LASSO
288
18.3
Bayesian Estimation of the True Covariance Matrix
295
19
Eigenvector Overlaps and Rotationally Invariant Estimators
297
19.1
Eigenvector Overlaps
297
19.2
Rotationally Invariant Estimators
301
19.3
Properties of the Optimal RIE for Covariance Matrices
309
19.4
Conditional Average in Free Probability
310
19.5
Real Data
311
19.6
Validation and RIE
317
20
Applications to Finance
321
20.1
Portfolio Theory
321
20.2
The High-Dimensional Limit
325
20.3
The Statistics of Price Changes: A Short Overview
330
20.4
Empirical Covariance Matrices
334

viii
Contents
Appendix
Mathematical Tools
339
A.1
Saddle Point Method
339
A.2
Tricomi’s Formula
341
A.3
Toeplitz and Circulant Matrices
343
Index
347

Preface
Physicists have always approached the world through data and models inspired by this
data. They build models from data and confront their models with the data generated by
new experiments or observations. But real data is by nature noisy; until recently, classical
statistical tools have been successful in dealing with this randomness. The recent emergence
of very large datasets, together with the computing power to analyze them, has created a
situation where not only the number of data points is large but also the number of studied
variables. Classical statistical tools are inadequate to tackle this situation, called the large
dimension limit (or the Kolmogorov limit). Random matrix theory, and in particular the
study of large sample covariance matrices, can help make sense of these big datasets, and
is in fact also becoming a useful tool to understand deep learning. Random matrix theory
is also linked to many modern problems in statistical physics such as the spectral theory of
random graphs, interaction matrices of spin-glasses, non-intersecting random walks, many-
body localization, compressed sensing and many more.
This book can be considered as one more book on random matrix theory. But our aim
was to keep it purposely introductory and informal. As an analogy, high school seniors
and college freshmen are typically taught both calculus and analysis. In analysis one learns
how to make rigorous proofs, deﬁne a limit and a derivative. At the same time in calculus
one can learn about computing complicated derivatives, multi-dimensional integrals and
solving differential equations relying only on intuitive deﬁnitions (with precise rules) of
these concepts. This book proposes a “calculus” course for random matrices, based in
particular on the relatively new concept of “freeness”, that generalizes the standard concept
of probabilistic independence to non-commuting random variables.
Rather than make statements about the most general case, concepts are deﬁned with
some strong hypothesis (e.g. Gaussian entries, real symmetric matrices) in order to simplify
the computations and favor understanding. Precise notions of norm, topology, convergence,
exact domain of application are left out, again to favor intuition over rigor. There are many
good, mathematically rigorous books on the subject (see references below) and the hope is
that our book will allow the interested reader to read them guided by his/her newly built
intuition.
ix

x
Preface
Readership
The book was initially conceived as a textbook for a graduate level standard 30 hours course
in random matrix theory, for physicists or applied mathematicians, given by one of us (MP)
during a sabbatical at UCLA in 2017–2018. As the book evolved many new developments,
special topics and applications have been included. Lecturers can then customize their
course offering by complementing the ﬁrst few essential chapters with their own choice
of chapters or sections from the rest of the book.
Another group of potential readers are seasoned researchers analyzing large datasets
who have heard that random matrix theory may help them distinguish signal from noise
in singular value decompositions or eigenvalues of sample covariance matrices. They have
heard of the Marˇcenko–Pastur distribution but do not know how to extend it to more real-
istic settings where they might have non-Gaussian noise, true outliers, temporal (sample)
correlations, etc. They need formulas to compute null hypothesis and so forth. They want
to understand where these formulas come from intuitively without requiring full precise
mathematical proofs.
The reader is assumed to have a background in undergraduate mathematics taught in
science and engineering: linear algebra, complex variables and probability theory. Impor-
tant results from probability theory are recalled in the book (addition of independent vari-
ables, law of large numbers and central limit theorem, etc.) while stochastic calculus and
Bayesian estimation are not assumed to be known. Familiarity with physics approximation
techniques (Taylor expansions, saddle point approximations) is helpful.
How to Read This Book
We have tried to make the book accessible for readers of different levels of expertise. The
bulk of the text is hopefully readable by graduate students, with most calculations laid
out in detail. We provide exercises in most chapters, which should allow the reader to
check that he or she has understood the main concepts. We also tried to illustrate the book
with as many ﬁgures as possible, because we strongly believe (as physicists) that pictures
considerably help forming an intuition about the issue at stake.
More technical issues, directed at experts in rmt or statistical physics, are signaled by
the use of a different, smaller font and an extra margin space. Chapters 3, 6, 7 and 13
are marked with a star, meaning that they are not essential for beginners and they can be
skipped at ﬁrst reading.
At the end of each chapter, we give a non-exhaustive list of references, some general and
others more technical and specialized, which direct the reader to more in-depth information
related to the subject treated in the chapter.
Other Books on Related Subjects
Books for Mathematicians
There are many good recent books on random matrix theory and free probabilities written
by and for mathematicians: Blower [2009], Anderson et al. [2010], Bai and Silverstein

Preface
xi
[2010], Pastur and Scherbina [2010], Tao [2012], Erd˝os and Yau [2017], Mingo and
Speicher [2017]. These books are often too technical for the intended readership of the
present book. We nevertheless rely on these books to extract some relevant material for our
purpose.
Books for Engineers
Communications engineers and now ﬁnancial engineers have become big users of random
matrix theory and there are at least two books speciﬁcally geared towards them. The style
of engineering books is closer to the style of the present book and these books are quite
readable for a physics audience.
There is the short book by Tulino and Verd´u [2004]; it gets straight to the point and gives
many useful formulas from free probabilities to compute the spectrum of random matrices.
The ﬁrst part of this book covers some of the topics covered here, but many other subjects
more related to statistical physics and to ﬁnancial applications are absent.
Part I of Couillet and Debbah [2011] has a greater overlap with the present book. Again
about half the topics covered here are not present in that book (e.g. Dyson Brownian
motion, replica trick, low-rank hciz and the estimation of covariance matrices).
Books for Physicists
Physicists interested in random matrix theory fall into two broad categories:
Mathematical physicists and high-energy physicists use it to study fundamental quan-
tum interactions, from Wigner’s distribution of nuclear energy level spacing to models of
quantum gravity using matrix models of triangulated surfaces.
Statistical physicists encounter random matrices in the interaction matrices of spin-
glasses, in the study of non-intersecting random walks, in the spectral analysis of large
random graphs, in the theory of Anderson localization and many-body localization, and
ﬁnally in the study of sample covariance matrices from large datasets. This book focuses
primarily on statistical physics and data analysis applications.
The classical book by Mehta [2004] is at the crossroads of these different approaches,
whereas Forrester [2010], Br´ezin and Hikami [2016] and Eynard et al. [2006] are examples
of books written by mathematical physicists. Livan et al. [2018] is an introductory book
geared towards statistical physicists. That book is very similar in spirit to ours. The topics
covered do not overlap entirely; for example Livan et al. do not cover the Dyson Brownian
motion, the hciz integral, the problem of eigenvector overlaps, sample covariance matrices
with general true covariance, free multiplication, etc.
We should also mention the handbook by Akemann et al. [2011] and the Les Houches
summer school proceedings [Schehr et al., 2017], in which we co-authored a chapter on
ﬁnancial applications of rmt. That book covers a very wide range of topics. It is a useful
complement to this book but too advanced for most of the intended readers.
Finally, the present book has some overlap with a review article written with Joel Bun
[Bun et al., 2017].

xii
Preface
Acknowledgments
The two of us want to warmly thank the research team at CFM with whom we have had
many illuminating discussions on these topics over the years, and in particular Jean-Yves
Audibert, Florent Benaych-Georges, Raphael B´enichou, R´emy Chicheportiche, Stefano
Ciliberti, Sungmin Hwang, Vipin Kerala Varma, Laurent Laloux, Eric Lebigot, Thomas
Madaule, Iacopo Mastromatteo, Pierre-Alain Reigneron, Adam Rej, Jacopo Rocchi,
Emmanuel S´eri´e, Konstantin Tikhonov, Bence Toth and Dario Vallamaina.
We also want to thank our academic colleagues for numerous, very instructive inter-
actions, collaborations and comments, including Gerard Ben Arous, Michael Benzaquen,
Giulio Biroli, Edouard Br´ezin, Zdzisław Burda, Benoˆıt Collins, David Dean, Bertrand
Eynard, Yan Fyodorov, Thomas Guhr, Alice Guionnet, Antti Knowles, Reimer Kuehn,
Pierre Le Doussal, Fabrizio Lillo, Satya Majumdar, Marc M´ezard, Giorgio Parisi, Sandrine
P´ech´e, Marco Tarzia, Matthieu Wyart, Francesco Zamponi and Tony Zee.
We want to thank some of our students and post-docs for their invaluable contribution
to some of the topics covered in this book, in particular Romain Allez, Joel Bun, Tristan
Gauti´e and Pierre Mergny. We also thank Pierre-Philippe Cr´epin, Th´eo Dessertaine, Tristan
Gauti´e, Armine Karami and Jos´e Moran for carefully reading the manuscript.
Finally, Marc Potters wants to thank Fan Yang, who typed up the original hand-written
notes. He also wants to thank Andrea Bertozzi, Stanley Osher and Terrence Tao, who
welcomed him for a year at UCLA. During that year he had many fruitful discussions with
members and visitors of the UCLA mathematics department and with participants of the
IPAM long program in quantitative linear algebra, including Alice Guionnet, Horng-Tzer
Yau, Jun Yin and more particularly with Nicholas Cook, David Jekel, Dimitri Shlyakhtenko
and Nikhil Srivastava.
Bibliographical Notes
Here is a list of books on random matrix theory that we have found useful.
• Books for mathematicians
– G. Blower. Random Matrices: High Dimensional Phenomena. Cambridge University
Press, Cambridge, 2009,
– G. W. Anderson, A. Guionnet, and O. Zeitouni. An Introduction to Random Matrices.
Cambridge University Press, Cambridge, 2010,
– Z. Bai and J. W. Silverstein. Spectral Analysis of Large Dimensional Random Matri-
ces. Springer-Verlag, New York, 2010,
– L. Pastur and M. Scherbina. Eigenvalue Distribution of Large Random Matrices.
American Mathematical Society, Providence, Rhode Island, 2010,
– T. Tao. Topics in Random Matrix Theory. American Mathematical Society, Provi-
dence, Rhode Island, 2012,
– L. Erd˝os and H.-T. Yau. A Dynamical Approach to Random Matrix Theory. American
Mathematical Society, Providence, Rhode Island, 2017,

Preface
xiii
– J. A. Mingo and R. Speicher. Free Probability and Random Matrices. Springer, New
York, 2017.
• Books for physicists and mathematical physicists
– M. L. Mehta. Random Matrices. Academic Press, San Diego, 3rd edition, 2004,
– B. Eynard, T. Kimura, and S. Ribault. Random matrices. preprint arXiv:1510.04430,
2006,
– P. J. Forrester. Log Gases and Random Matrices. Princeton University Press,
Princeton, NJ, 2010,
– G. Akemann, J. Baik, and P. D. Francesco. The Oxford Handbook of Random Matrix
Theory. Oxford University Press, Oxford, 2011,
– E. Br´ezin and S. Hikami. Random Matrix Theory with an External Source. Springer,
New York, 2016,
– G. Schehr, A. Altland, Y. V. Fyodorov, N. O’Connell, and L. F. Cugliandolo, editors.
Stochastic Processes and Random Matrices, Les Houches Summer School, 2017.
Oxford University Press, Oxford,
– G. Livan, M. Novaes, and P. Vivo. Introduction to Random Matrices: Theory and
Practice. Springer, New York, 2018.
• More “applied” books
– A. M. Tulino and S. Verd´u. Random Matrix Theory and Wireless Communications.
Now publishers, Hanover, Mass., 2004,
– R. Couillet and M. Debbah. Random Matrix Methods for Wireless Communications.
Cambridge University Press, Cambridge, 2011.
• Our own review paper on the subject, with signiﬁcant overlap with this book
– J. Bun, J.-P. Bouchaud, and M. Potters. Cleaning large correlation matrices: Tools
from random matrix theory. Physics Reports, 666:1–109, 2017.

Symbols
Abbreviations
bbp:
Baik, Ben Arous, P´ech´e
cdf:
Cumulative Distribution Function
clt:
Central Limit Theorem
dbm:
Dyson Brownian Motion
ema:
Exponential Moving Average
ﬁd:
Free Identically Distributed
goe:
Gaussian Orthogonal Ensemble
gse:
Gaussian Symplectic Ensemble
gue:
Gaussian Unitary Ensemble
hciz:
Harish-Chandra–Itzykson–Zuber
iid:
Independent Identically Distributed
lln:
Law of Large Numbers
map:
Maximum A Posteriori
mave:
Mean Absolute Value
mmse:
Minimum Mean Square Error
pca:
Principal Component Analysis
pde:
Partial Differential Equation
pdf:
Probability Distribution Function
rie:
Rotationally Invariant Estimator
rmt:
Random Matrix Theory
scm:
Sample Covariance Matrix
sde:
Stochastic Differential Equation
svd:
Singular Value Decomposition
Conventions
0+:
inﬁnitesimal positive quantity
1:
identity matrix
xiv

List of Symbols
xv
∼:
scales as, of the order of, also, for random variables, drawn from
≈:
approximately equal to (mathematically or numerically)
∝:
proportional to
:= :
equal by deﬁnition to
≡:
identically equal to
+ :
free sum
× :
free product
E[.]:
mathematical expectation
V[.]:
mathematical variance
⟨.⟩:
empirical average
[x]:
dimension of x
i:
√
−1
Re:
real part
Im:
imaginary part
−
:
principal value integral
±
⃝√·:
special square root, Eq. (4.56)
AT :
matrix transpose
supp(ρ):
domain where ρ(·) is non-zero
Note: most of the time f (t) means that t is a continuous variable, and ft means that t is
discrete.
Roman Symbols
A:
generic constant, or generic free variable
A:
generic matrix
a:
generic coefﬁcient, as in the gamma distribution, Eq. (4.17), or in the free
log-normal, Eq. (16.15)
B:
generic free variable
B:
generic matrix
b:
generic coefﬁcient, as in the gamma distribution, Eq. (4.17), or in the free
log-normal, Eq. (16.15)
C:
generic coefﬁcient
Ck:
Catalan numbers
C:
often population, or “true” covariance matrix, sometimes C = A + B
C:
total investable capital or cross-covariance matrix
ck:
cumulant of order k
d:
distance between eigenvalues
dB:
Wiener noise
E:
sample, or empirical matrix; matrix corrupted by noise
E:
error
e:
normalized vector of 1’s e = (1,1, . . . ,1)T /
√
N

xvi
List of Symbols
e1:
unit vector in the ﬁrst direction e1 = (1,0, . . . ,0)T
F:
free energy
F(·):
generic function
Fβ(·):
Tracy–Widom distributions
FK(·, · , · ·):
K-point correlation function of eigenvalues
F:
dual of the sample covariance matrix, see Section 4.1.1
F(x):
force ﬁeld
F :
generalized force in the Fokker–Planck equation
Fn:
replica free energy
f (·):
generic function
G(·):
resolvent matrix
G:
return target of a portfolio
gN(·):
normalized trace of the resolvent
g:
vector of expected gains
g(·):
generic Stieltjes transform
gA(·):
Stieltjes transform of the spectrum of A
gX(·):
Stieltjes transform of a Wigner matrix
H:
rectangular N × T matrix with random entries
H(·):
log of the generating function of a random variable, H(k) := log E[eikX]
HX(·):
log of the generating function (rank-1 hciz) of a random matrix
ˆH:
annealed version of HX(t)
Hn:
Hermite polynomial (probabilists’ convention)
H:
Hamiltonian of a spin system
h:
auxiliary function
h(·):
real part of the Stieltjes transform on the real axis (equal to the Hilbert
transform times π)
I:
generic integral
I(A,B):
hciz integral
It(A):
hciz integral when B is of rank-1 with eigenvalue t
I:
interval
i,j,k,ℓ:
generic integer indices
J:
Jacobi matrices, or interaction matrix in spin-glasses
K:
number of terms in a sum or a product, or number of blocks in a cross-
validation experiment
K(·):
kernel
Kη(·):
Cauchy kernel, of width η
K:
Toeplitz matrix
L:
length of an interval containing eigenvalues
L(α)
n :
generalized Laguerre polynomial
LC,β
μ
:
L´evy stable laws
L:
lower triangular matrix

List of Symbols
xvii
L:
log-likelihood
M:
generic matrix
W
p:
inverse-Wishart matrix with coefﬁcient p
mk:
moment of order k
N:
size of the matrix, number of variables
N(μ,σ 2):
Gaussian distribution with mean μ and variance σ 2
N(μ,C):
multivariate Gaussian distribution with mean μ and covariance C
n:
number of eigenvalues in some interval, or number of replicas
O(N):
orthogonal group in N dimensions
O:
generic orthogonal matrix
P(·):
generic probability distribution function deﬁned by its argument: P(x) is
a short-hand for the probability density of variable x
Pγ (·):
gamma distribution
P>(·):
complementary cumulative distribution function
P0(·):
prior distribution in a Bayesian framework
Pi(t):
probability to be in state i at time t
Pn:
Legendre polynomial
P(x|y):
conditional distribution of x knowing y
P (a,b)
n
:
Jacobi polynomial
P:
rank-1 (projector) matrix
P[X]:
probability of event X
p:
variance of the inverse-Wishart distribution, or quantile value
p(x):
generic polynomial of x
p(A):
generic matrix polynomial of A
pN(·):
generic monic orthogonal polynomial
p(y,t|x):
propagator of Brownian motion, with initial position x at t = 0
Q(·):
generic polynomial
QN(·):
(expected) characteristic polynomial
q:
ratio of size of matrix N to size of sample T : q = N/T
q∗:
effective size ratio q∗= N/T ∗
q(A):
generic matrix polynomial of A
qN(·):
normalized characteristic polynomial
R:
circle radius
RA(·):
R-transform of the spectrum of A
R:
portfolio risk, or error
r:
signal-to-noise ratio; also an auxiliary variable in the spin-glass section
ri,t:
return of asset i at time t
SA(·):
S-transform of the spectrum of A
S:
diagonal matrix of singular values
s:
generic singular value
T :
size of the sample

xviii
List of Symbols
T ∗:
effective size of the sample, accounting for correlations
Tn:
Chebyshev polynomials of the ﬁrst kind
T:
matrix T-transform
tA(·):
T-transform of the spectrum of A
U(N):
unitary group in N dimensions
Un:
Chebyshev polynomials of the second kind
U:
generic rotation matrix deﬁning an eigenbasis
u:
eigenvector (often of the population matrix E)
V (·):
generic potential
V:
generic rotation matrix deﬁning an eigenbasis
V:
generalized potential, or variogram
v(x,t):
velocity ﬁeld in Matytsin’s formalism
v:
eigenvector (often of the sample matrix C)
W(·):
Fokker–Planck auxiliary function
Wn(π,σ):
Wiengarten coefﬁcient
W:
white Wishart matrix, sometimes generic multiplicative noise matrix
Wq:
white Wishart matrix of parameter q
w:
generic weight
X:
Wigner matrix, sometimes generic additive noise matrix
x:
generic variable
x:
generic vector
ˆx:
estimator of x
yk:
data points
y:
generic vector
Z:
normalization, or partition function
Zn:
Kesten variable
z:
generic complex variable
z(·):
functional inverse of the Stieltjes function g(·)
Greek Symbols
α:
generic scale factor
β:
effective inverse “temperature” for the Coulomb gas, deﬁning the symmetry
class of the matrix ensemble (β = 1 for orthogonal, β = 2 for unitary,
β = 4 for symplectic)
βi:
exposure of asset i to the common (market) factor
β:
vector of βi’s

:
gamma function

N:
multivariate gamma function
γ :
generic parameter or generic quantity
γc:
inverse correlation time

List of Symbols
xix
:
Jacobian matrix, or discrete Laplacian
(x):
Vandermonde determinant := 
i<j(xj −xi)
δ:
small increment or small distance
δ(·):
Dirac δ-function
ϵ:
generic small quantity ϵ ≪1
ϵi:
deviation of eigenvalue i from its most likely position
ε:
generic iid noise
ζ:
ridge regression parameter
ζ(·):
inverse of the T-function t(·)
η:
small quantity, 1/N ≪η ≪1
θ:
edge exponent
θ,θkℓ:
angle and generalized angles
(·):
Heaviside function, (x < 0) = 0, (x > 0) = 1
κk:
free cumulant of order k
:
diagonal matrix
λ:
eigenvalue, often those of the sample (or “empirical”) matrix
λ1:
largest eigenvalue of a given matrix
λ+:
upper edge of spectrum
λ−:
lower edge of spectrum
μ:
eigenvalue, often those of the population (or “true”) matrix
π:
portfolio weights, or partition
N(·):
auxiliary polynomial
(·):
limit polynomial
(x,t):
pressure ﬁeld in Matytsin’s formalism
:
Rotationally Invariant Estimator (rie)
ξi:
rie eigenvalues
ξ(·):
rie shrinkage function
ρ:
correlation coefﬁcient
ρ(·):
eigenvalue distribution
ρ(x,t):
density ﬁeld in Matytsin’s formalism
ϱ:
square overlap between two vectors
σ:
volatility, or root-mean-square
τ(·):
normalized trace 1/N Tr(·), or free expectation operator
τR(·):
normalized trace, further averaged over rotation matrices appearing inside
the trace operator
τ:
time lag
τc:
correlation time
ϒ:
volume in matrix space
(·):
auxiliary function
(λ,μ):
scaled squared overlap between the eigenvectors of the sample (λ) and
population (μ) matrices

xx
List of Symbols
φ:
angle
φ(·):
effective two-body potential in Matytsin’s formalism
ϕ(·):
generating function, or Fourier transform of a probability distribution
(λ,λ′):
scaled squared overlap between the eigenvectors of two sample matrices
(·),ψ(·):
auxiliary functions
ψ:
auxiliary integration vector
 :
generic rotation matrix
ω(·):
generic eigenvalue ﬁeld in a large deviation formalism (cf. Section 5.5)

Part I
Classical Random Matrix Theory


1
Deterministic Matrices
Matrices appear in all corners of science, from mathematics to physics, computer science,
biology, economics and quantitative ﬁnance. In fact, before Schrodinger’s equation, quan-
tum mechanics was formulated by Heisenberg in terms of what he called “Matrix Mechan-
ics”. In many cases, the matrices that appear are deterministic, and their properties are
encapsulated in their eigenvalues and eigenvectors. This ﬁrst chapter gives several elemen-
tary results in linear algebra, in particular concerning eigenvalues. These results will be
extremely useful in the rest of the book where we will deal with random matrices, and in
particular the statistical properties of their eigenvalues and eigenvectors.
1.1 Matrices, Eigenvalues and Singular Values
1.1.1 Some Problems Where Matrices Appear
Let us give three examples motivating the study of matrices, and the different forms that
those can take.
Dynamical System
Consider a generic dynamical system describing the time evolution of a certain
N-dimensional vector x(t), for example the three-dimensional position of a point in
space. Let us write the equation of motion as
dx
dt = F(x),
(1.1)
where F(x) is an arbitrary vector ﬁeld. Equilibrium points x∗are such that F(x∗) = 0.
Consider now small deviations from equilibrium, i.e. x = x∗+ ϵy where ϵ ≪1. To ﬁrst
order in ϵ, the dynamics becomes linear, and given by
dy
dt = Ay,
(1.2)
where A is a matrix whose elements are given by Aij = ∂jFi(x∗), where i,j are indices
that run from 1 to N. When F can itself be written as the gradient of some potential V , i.e.
Fi = −∂iV (x), the matrix A becomes symmetric, i.e. Aij = Aji = −∂ijV . But this is not
3

4
Deterministic Matrices
always the case; in general the linearized dynamics is described by a matrix A without any
particular property – except that it is a square N × N array of real numbers.
Master Equation
Another standard setting is the so-called Master equation for the evolution of probabilities.
Call i = 1, . . . ,N the different possible states of a system and Pi(t) the probability to
ﬁnd the system in state i at time t. When memory effects can be neglected, the dynamics
is called Markovian and the evolution of Pi(t) is described by the following discrete time
equation:
Pi(t + 1) =
N

j=1
AijPj(t),
(1.3)
meaning that the system has a probability Aij to jump from state j to state i between t and
t +1. Note that all elements of A are positive; furthermore, since all jump possibilities must
be exhausted, one must have, for each j, 
i Aij = 1. This ensures that 
i Pi(t) = 1 at
all times, since
N

i=1
Pi(t + 1) =
N

i=1
N

j=1
AijPj(t) =
N

j=1
N

i=1
AijPj(t) =
N

j=1
Pj(t) = 1.
(1.4)
Matrices such that all elements are positive and such that the sum over all rows is equal
to unity for each column are called stochastic matrices. In matrix form, Eq. (1.3) reads
P(t + 1) = AP(t), leading to P(t) = AtP(0), i.e. A raised to the t-th power applied to the
initial distribution.
Covariance Matrices
As a third important example, let us consider random, N-dimensional real vectors X, with
some given multivariate distribution P(X). The covariance matrix C of the X’s is deﬁned as
Cij = E[XiXj] −E[Xi]E[Xj],
(1.5)
where E means that we are averaging over the distribution P(X). Clearly, the matrix C is
real and symmetric. It is also positive semi-deﬁnite, in the sense that for any vector x,
xT Cx ≥0.
(1.6)
If it were not the case, it would be possible to ﬁnd a linear combination of the vectors X
with a negative variance, which is obviously impossible.
The three examples above are all such that the corresponding matrices are N ×N square
matrices. Examples where matrices are rectangular also abound. For example, one could
consider two sets of random real vectors: X of dimension N1 and Y of dimension N2. The
cross-covariance matrix deﬁned as

1.1 Matrices, Eigenvalues and Singular Values
5
Cia = E[XiYa] −E[Xi]E[Ya];
i = 1, . . . ,N1;
a = 1, . . . ,N2,
(1.7)
is an N1 × N2 matrix that describes the correlations between the two sets of vectors.
1.1.2 Eigenvalues and Eigenvectors
One learns a great deal about matrices by studying their eigenvalues and eigenvectors. For
a square matrix A a pair of scalar and non-zero vector (λ,v) satisfying
Av = λv
(1.8)
is called an eigenvalue–eigenvector pair.
Trivially if v is an eigenvector αv is also an eigenvector when α is a non-zero real
number. Sometimes multiple non-collinear eigenvectors share the same eigenvalue; we say
that this eigenvalue is degenerate and has multiplicity equal to the dimension of the vector
space spanned by its eigenvectors.
If Eq. (1.8) is true, it implies that the equation (A −λ1)v = 0 has non-trivial solutions,
which requires that det(λ1 −A) = 0. The eigenvalues λ are thus the roots of the so-called
characteristic polynomial of the matrix A, obtained by expanding det(λ1−A). Clearly, this
polynomial1 is of order N and therefore has at most N different roots, which correspond
to the (possibly complex) eigenvalues of A. Note that the characteristic polynomial of AT
coincides with the characteristic polynomial of A, so the eigenvalues of A and AT are
identical.
Now, let λ1,λ2, . . . ,λN be the N eigenvalues of A with v1,v2, . . . ,vN the corresponding
eigenvectors. We deﬁne  as the N × N diagonal matrix with λi on the diagonal, and V
as the N × N matrix whose jth column is vj, i.e. Vij = (vj)i is the ith component of vj.
Then, by deﬁnition,
AV = V,
(1.9)
since once expanded, this reads

k
AikVkj = Vijλj,
(1.10)
or Avj = λjvj. If the eigenvectors are linearly independent (which is not true for all
matrices), the matrix inverse V−1 exists and one can therefore write A as
A = VV−1,
(1.11)
which is called the eigenvalue decomposition of the matrix A.
Symmetric matrices (such that A = AT ) have very nice properties regarding their
eigenvalues and eigenvectors.
1 The characteristic polynomial QN (λ) = det(λ1 −A) always has a coefﬁcient 1 in front of its highest power (QN (λ) =
λN + O(λN−1)), such polynomials are called monic.

6
Deterministic Matrices
• They have exactly N eigenvalues when counted with their multiplicity.
• All their eigenvalues and eigenvectors are real.
• Their eigenvectors are orthogonal and can be chosen to be orthonormal (i.e. vT
i vj =
δij). Here we assume that for degenerate eigenvalues we pick an orthogonal set of
corresponding eigenvectors.
If we choose orthonormal eigenvectors, the matrix V has the property VT V = 1 (⇒
VT = V−1). Hence it is an orthogonal matrix V = O and Eq. (1.11) reads
A = OOT,
(1.12)
where  is a diagonal matrix containing the eigenvalues associated with the eigenvectors
in the columns of O. A symmetric matrix can be diagonalized by an orthogonal matrix.
Remark that an N × N orthogonal matrix is fully parameterized by N(N −1)/2 “angles”,
whereas  contains N diagonal elements. So the total number of parameters of the diagonal
decomposition is N(N −1)/2 + N, which is identical, as it should be, to the number of
different elements of a symmetric N × N matrix.
Let us come back to our dynamical system example, Eq. (1.2). One basic question is to
know whether the perturbation y will grow with time, or decay with time. The answer to
this question is readily given by the eigenvalues of A. For simplicity, we assume F to be
a gradient such that A is symmetric. Since the eigenvectors of A are orthonormal, one can
decompose y in term of the v’s as
y(t) =
N

i=1
ci(t)vi.
(1.13)
Taking the dot product of Eq. (1.2) with vi then shows that the dynamics of the coefﬁcients
ci(t) are decoupled and given by
dci
dt = λici,
(1.14)
where λi is the eigenvalue associated with vi. Therefore, any component of the initial
perturbation y(t = 0) that is along an eigenvector with positive eigenvalue will grow expo-
nentially with time, until the linearized approximation leading to Eq. (1.2) breaks down.
Conversely, components along directions with negative eigenvalues decrease exponentially
with time. An equilibrium x∗is called stable provided all eigenvalues are negative, and
marginally stable if some eigenvalues are zero while all others are negative.
The important message carried by the example above is that diagonalizing a matrix
amounts to ﬁnding a way to decouple the different degrees of freedom, and convert a matrix
equation into a set of N scalar equations, as Eqs. (1.14). We will see later that the same
idea holds for covariance matrices as well: their diagonalization allows one to ﬁnd a set of
uncorrelated vectors. This is usually called Principal Component Analysis (pca).

1.1 Matrices, Eigenvalues and Singular Values
7
Exercise 1.1.1
Instability of eigenvalues of non-symmetric matrices
Consider the N × N
square band diagonal matrix M0 deﬁned by
[M0]ij = 2δi,j−1:
M0 =
⎛
⎜⎜⎜⎜⎜⎝
0
2
0
· · ·
0
0
0
2
· · ·
0
0
0
0
...
0
0
0
0
· · ·
2
0
0
0
· · ·
0
⎞
⎟⎟⎟⎟⎟⎠
.
(1.15)
(a)
Show that MN
0
=
0 and so all the eigenvalues of M0 must be zero.
Use a numerical eigenvalue solver for non-symmetric matrices and conﬁrm
numerically that this is the case for N = 100.
(b)
If O is an orthogonal matrix (OOT = 1), OM0OT has the same eigenvalues
as M0. Following Exercise 1.2.4, generate a random orthogonal matrix O.
Numerically ﬁnd the eigenvalues of OM0OT . Do you get the same answer as
in (a)?
(c)
Consider M1 whose elements are all equal to those of M0 except for one
element in the lower left corner [M1]N,1 = (1/2)N−1. Show that MN
1 = 1;
more precisely, show that the characteristic polynomial of M1 is given by
det(M1 −λ1) = λN −1, therefore M1 has N distinct eigenvalues equal to the
N complex roots of unity λk = e2πik/N.
(d)
For N greater than about 60, OM0OT and OM1OT are indistinguishable to
machine precision. Compare numerically the eigenvalues of these two rotated
matrices.
1.1.3 Singular Values
A non-symmetric, square matrix cannot in general be decomposed as A = OOT , where
 is a diagonal matrix and O an orthogonal matrix. One can however ﬁnd a very useful
alternative decomposition as
A = VSUT,
(1.16)
where S is a non-negative diagonal matrix, whose elements are called the singular values
of A, and U,V are two real, orthogonal matrices. Whenever A is symmetric positive semi-
deﬁnite, one has S =  and U = V.
Equation (1.16) also holds for rectangular N×T matrices, where V is N×N orthogonal,
U is T × T orthogonal and S is N × T diagonal as deﬁned below. To construct the
singular value decomposition (svd) of A, we ﬁrst introduce two matrices B and B, deﬁned
as B := AAT and B = AT A. It is plain to see that these matrices are symmetric, since

8
Deterministic Matrices
BT = (AAT )T = AT T AT = B (and similarly for B). They are also positive semi-deﬁnite as
for any vector x we have xT Bx = ||AT x||2 ≥0.
We can show that B and B have the same non-zero eigenvalues. In fact, let λ > 0 be an
eigenvalue of B and v  0 is the corresponding eigenvector. Then we have, by deﬁnition,
AAT v = λv.
(1.17)
Let u = AT v, then we can get from the above equation that
AT AAT v = λAT v ⇒Bu = λu.
(1.18)
Moreover,
∥u∥2 = vT AAT v = vT Bv  0 ⇒u  0.
(1.19)
Hence λ is also an eigenvalue of B. Note that for degenerate eigenvalues λ of B, an
orthogonal set of corresponding eigenvectors {vℓ} gives rise to an orthogonal set {AT vℓ}
of eigenvectors of B. Hence the multiplicity of λ in B is at least that of B. Similarly, we can
show that any non-zero eigenvalue of B is also an eigenvalue of B. This ﬁnishes the proof
of the claim.
Note that B has at most N non-zero eigenvalues and B has at most T non-zero eigen-
values. Thus by the above claim, if T > N, B has at least T −N zero eigenvalues, and if
T < N, B has at least N −T zero eigenvalues. We denote the other min{N,T } eigenvalues
of B and B by {λk}1≤k≤min{N,T }. Then the svd of A is expressed as Eq. (1.16), where V is
the N × N orthogonal matrix consisting of the N normalized eigenvectors of B, U is the
T × T orthogonal matrix consisting of the T normalized eigenvectors of B, and S is an
N ×T rectangular diagonal matrix with Skk = √λk ≥0, 1 ≤k ≤min{N,T } and all other
entries equal to zero.
For instance, if N < T , we have
S =
⎛
⎜⎜⎜⎝
√λ1
0
0
0
· · ·
0
0
√λ2
0
0
· · ·
0
0
0
...
0
· · ·
0
0
0
0
√λN
· · ·
0
⎞
⎟⎟⎟⎠.
(1.20)
Although (non-degenerate) normalized eigenvectors are unique up to a sign, the choice of
the positive sign for the square-root √λk imposes a condition on the combined sign for the
left and right singular vectors vk and uk. In other words, simultaneously changing both vk
and uk to −vk and −uk leaves the matrix A invariant, but for non-zero singular values one
cannot individually change the sign of either vk or uk.
The recipe to ﬁnd the svd, Eq. (1.16), is thus to diagonalize both AAT (to obtain V
and S2) and AT A (to obtain U and again S2). It is insightful to again count the number of
parameters involved in this decomposition. Consider a general N × T matrix with T ≥N
(the case N ≥T follows similarly). The N eigenvectors of AAT are generically unique
up to a sign, while for T −N > 0 the matrix AT A will have a degenerate eigenspace
associated with the eigenvalue 0 of size T −N, hence its eigenvectors are only unique up

1.2 Some Useful Theorems and Identities
9
to an arbitrary rotation in T −N dimension. So generically the svd decomposition amounts
to writing the NT elements of A as
NT ≡1
2N(N −1) + N + 1
2T (T −1) −1
2(T −N)(T −N −1).
(1.21)
The interpretation of Eq. (1.16) for N × N matrices is that one can always ﬁnd an
orthonormal basis of vectors {u} such that the application of a matrix A amounts to a
rotation (or an improper rotation) of {u} into another orthonormal set {v}, followed by a
dilation of each vk by a positive factor √λk.
Normal matrices are such that U = V. In other words, A is normal whenever A com-
mutes with its transpose: AAT = AT A. Symmetric, skew-symmetric and orthogonal matri-
ces are normal, but other cases are possible. For example a 3 × 3 matrix such that each row
and each column has exactly two elements equal to 1 and one element equal to 0 is normal.
1.2 Some Useful Theorems and Identities
In this section, we state without proof very useful theorems on eigenvalues and matrices.
1.2.1 Gershgorin Circle Theorem
Let A be a real matrix, with elements Aij. Deﬁne Ri as Ri = 
ji |Aij|, and Di a disk in
the complex plane centered on Aii and of radius Ri. Then every eigenvalue of A lies within
at least one disk Di. For example, for the matrix
A =
⎛
⎝
1
−0.2
0.2
−0.3
2
−0.2
0
1.1
3
⎞
⎠,
(1.22)
the three circles are located on the real axis at x = 1,2 and 3 with radii 0.4, 0.5 and 1.1
respectively (see Fig. 1.1).
In particular, eigenvalues corresponding to eigenvectors with a maximum amplitude on
i lie within the disk Di.
1.2.2 The Perron–Frobenius Theorem
Let A be a real matrix, with all its elements positive Aij > 0. Then the top eigenvalue λmax
is unique and real (all other eigenvalues have a smaller real part). The corresponding top
eigenvector v∗has all its elements positive:
Av∗= λmaxv∗;
v∗
k > 0, ∀k.
(1.23)
The top eigenvalue satisﬁes the following inequalities:
min
i

j
Aij ≤λmax ≤max
i

j
Aij.
(1.24)

10
Deterministic Matrices
0
1
2
3
4
Re ( )
−1.0
−0.5
0.0
0.5
1.0
Im ( )
Figure 1.1 The three complex eigenvalues of the matrix (1.22) (crosses) and its three Gershgorin
circles. The ﬁrst eigenvalue λ1 ≈0.92 falls in the ﬁrst circle while the other two λ2,3 ≈2.54 ± 0.18i
fall in the third one.
Application: Suppose A is a stochastic matrix, such that all its elements are positive
and satisfy 
i Aij = 1, ∀j. Then clearly the vector ⃗1 is an eigenvector of AT , with
eigenvalue λ = 1. But since the Perron–Frobenius can be applied to AT , the inequalities
(1.24) ensure that λ is the top eigenvalue of AT , and thus also of A. All the elements of the
corresponding eigenvector v∗are positive, and describe the stationary state of the associated
Master equation, i.e.
P ∗
i =

j
AijP ∗
j −→P ∗
i =
v∗
i

k v∗
k
.
(1.25)
Exercise 1.2.1
Gershgorin and Perron–Frobenius
Show that the upper bound in Eq. (1.24) is a simple consequence of the
Gershgorin theorem.
1.2.3 The Eigenvalue Interlacing Theorem
Let A be an N×N symmetric matrix (or more generally Hermitian matrix) with eigenvalues
λ1 ≥λ2 · · · ≥λN. Consider the N −1×N −1 submatrix A\i obtained by removing the ith
row and ith columns of A. Its eigenvalues are μ(i)
1
≥μ(i)
2 · · · ≥μ(i)
N−1. Then the following
interlacing inequalities hold:
λ1 ≥μ(i)
1 ≥λ2 · · · ≥μ(i)
N−1 ≥λN.
(1.26)

1.2 Some Useful Theorems and Identities
11
Very recently, a formula relating eigenvectors to eigenvalues was (re-)discovered. Calling
vi the eigenvector of A associated with λi, one has2
(vi)j
2 =
N−1
k=1 λi −μ(j)
k
N
ℓ=1,ℓi λi −λℓ
.
(1.27)
1.2.4 Sherman–Morrison Formula
The Sherman–Morrison formula gives the inverse of a matrix A perturbed by a rank-1
perturbation:
(A + uvT )−1 = A−1 −A−1uvT A−1
1 + vT A−1u,
(1.28)
valid for any invertible matrix A and vectors u and v such that the denominator does not
vanish. This is a special case of the Woodbury identity, which reads

A + UCVT −1 = A−1 −A−1U

C−1 + VT A−1U
−1
VT A−1,
(1.29)
where U,V are N × K matrices and C is a K × K matrix. Equation (1.28) corresponds to
the case K = 1.
The associated Sherman–Morrison determinant lemma reads
det(A + vuT ) = det A ·

1 + uT A−1v

(1.30)
for invertible A.
Exercise 1.2.2
Sherman–Morrison
Show that Eq. (1.28) is correct by multiplying both sides by (A + uvT ).
1.2.5 Schur Complement Formula
The Schur complement, also called inversion by partitioning, relates the blocks of the
inverse of a matrix to the inverse of blocks of the original matrix. Let M be an invertible
matrix which we divide in four blocks as
M =
 M11
M12
M21
M22

and M−1 = Q =
 Q11
Q12
Q21
Q22

,
(1.31)
where [M11] = n × n, [M12] = n × (N −n), [M21] = (N −n) × n, [M22] = (N −n) ×
(N −n), and M22 is invertible. The integer n can take any values from 1 to N −1.
2 See: P. Denton, S. Parke, T. Tao, X. Zhang, Eigenvalues from Eigenvectors: a survey of a basic identity in linear algebra,
arXiv:1908.03795.

12
Deterministic Matrices
Then the upper left n × n block of Q is given by
Q−1
11 = M11 −M12(M22)−1M21,
(1.32)
where the right hand side is called the Schur complement of the block M22 of the matrix M.
Exercise 1.2.3
Combining Schur and Sherman–Morrison
In the notation of Eq. (1.31) for n = 1 and any N > 1, combine the Schur
complement of the lower right block with the Sherman–Morrison formula to
show that
Q22 = (M22)−1 + (M22)−1M21M12(M22)−1
M11 −M12(M22)−1M21
.
(1.33)
1.2.6 Function of a Matrix and Matrix Derivative
In our study of random matrices, we will need to extend real or complex scalar functions
to take a symmetric matrix M as its argument. The simplest way to extend such a function
is to apply it to each eigenvalue of the matrix M = OOT :
F(M) = OF()OT,
(1.34)
where F() is the diagonal matrix where we have applied the function F to each (diag-
onal) entry of . The function F(M) is now a matrix valued function of a matrix. Scalar
polynomial functions can obviously be extended directly as
F(x) =
K

k=0
akxk ⇒F(M) =
K

k=0
akMk,
(1.35)
but this is equivalent to applying the polynomial to the eigenvalues of M. By extension,
when the Taylor series of the function F(x) converges for every eigenvalue of M the matrix
Taylor series coincides with our deﬁnition.
Taking the trace of F(M) will yield a matrix function that returns a scalar. This con-
struction is rotationally invariant in the following sense:
Tr F(UMUT ) = Tr F(M) for any UUT = 1.
(1.36)
We can take the derivative of a scalar-valued function Tr F(M) with respect to each
element of the matrix M:
d
d[M]ij
Tr(F(M)) = [F ′(M)]ij ⇒
d
dM Tr(F(M)) = F ′(M).
(1.37)
Equation (1.37) is easy to derive when F(x) is a monomial akxk and by linearity for
polynomial or Taylor series F(x).

1.2 Some Useful Theorems and Identities
13
1.2.7 Jacobian of Simple Matrix Transformations
Suppose one transforms an N × N matrix A into another N × N matrix B through some
function of the matrix elements. The Jacobian of the transformation is deﬁned as the
determinant of the partial derivatives:
Gij,kℓ= ∂Bkℓ
∂Aij
.
(1.38)
The simplest case is just multiplication by a scalar: B = αA, leading to Gij,kℓ= αδikδjℓ.
G is therefore the tensor product of α1 with 1, and its determinant is thus equal to αN.
Not much more difﬁcult is the case of an orthogonal transformation B = OAOT , for which
Gij,kℓ= OikOjℓ. G is now the tensor product G = O ⊗O and therefore its determinant is
unity.
Slightly more complicated is the case where B = A−1. Using simple algebra, one readily
obtains, for symmetric matrices,
Gij,kℓ= 1
2[A−1]ik[A−1]jℓ+ 1
2[A−1]iℓ[A−1]jk.
(1.39)
Let us now assume that A has eigenvalues λα and eigenvectors vα. One can easily diago-
nalize Gij,kℓwithin the symmetric sector, since

kℓ
Gij,kℓ

vα,kvβ,ℓ+ vα,ℓvβ,k

=
1
λαλβ

vα,ivβ,j + vα,jvβ,i

.
(1.40)
So the determinant of G is simply 
α,β≥α(λαλβ)−1. Taking the logarithm of this product
helps avoiding counting mistakes, and ﬁnally leads to the result
det G = (det A)−N−1.
(1.41)
Exercise 1.2.4
Random Matrices
We conclude this chapter on deterministic matrices with a numerical exercise
on random matrices. Most of the results of this exercise will be explored
theoretically in the following chapters.
•
Let M be a random real symmetric orthogonal matrix, that is an N ×N matrix
satisfying M = MT = M−1. Show that all the eigenvalues of M are ±1.
•
Let X be a Wigner matrix, i.e. an N×N real symmetric matrix whose diagonal
and upper triangular entries are iid Gaussian random numbers with zero mean
and variance σ 2/N. You can use X = σ(H + HT )/
√
2N where H is a non-
symmetric N × N matrix with iid standard Gaussians.
•
The matrix P+ is deﬁned as P+ = 1
2(M + 1N). Convince yourself that P+ is
the projector onto the eigenspace of M with eigenvalue +1. Explain the effect
of the matrix P+ on eigenvectors of M.

14
Deterministic Matrices
•
An easy way to generate a random matrix M is to generate a Wigner matrix
(independent of X), diagonalize it, replace every eigenvalue by its sign and
reconstruct the matrix. The procedure does not depend on the σ used for the
Wigner.
•
We consider a matrix E of the form E = M+X. To wit, E is a noisy version of
M. The goal of the following is to understand numerically how the matrix E is
corrupted by the Wigner noise. Using the computer language of your choice,
for a large value of N (as large as possible while keeping computing times
below one minute), for three interesting values of σ of your choice, do the
following numerical analysis.
(a)
Plot a histogram of the eigenvalues of E, for a single sample ﬁrst, and then for
many samples (say 100).
(b)
From your numerical analysis, in the large N limit, for what values of σ do
you expect a non-zero density of eigenvalues near zero.
(c)
For every normalized eigenvector vi of E, compute the norm of the vector
P+vi. For a single sample, do a scatter plot of |P+vi|2 vs λi (its eigenvalue).
Turn your scatter plot into an approximate conditional expectation value
(using a histogram) including data from many samples.
(d)
Build an estimator (E) of M using only data from E. We want to minimize
the error E =
1
N ||((E) −M)||2
F where ||A||2
F = TrAAT . Consider ﬁrst
1(E) = E and then 0(E) = 0. What is the error E of these two estimators?
Try to build an ad-hoc estimator (E) that has a lower error E than these two.
(e)
Show numerically that the eigenvalues of E are not iid. For each sample E
rank its eigenvalues λ1 < λ2 < · · · < λN. Consider the eigenvalue spacing
sk = λk −λk−1 for eigenvalues in the bulk (.2N < k < .3N and .7N < k <
.8N). Make a histogram of {sk} including data from 100 samples. Make 100
pseudo-iid samples: mix eigenvalues for 100 different samples and randomly
choose N from the 100N possibilities, do not choose the same eigenvalue
twice for a given pseudo-iid sample. For each pseudo-iid sample, compute
sk in the bulk and make a histogram of the values using data from all 100
pseudo-iid samples. (Bonus) Try to ﬁt an exponential distribution to these two
histograms. The iid case should be well ﬁtted by the exponential but not the
original data (not iid).

2
Wigner Ensemble and Semi-Circle Law
In many circumstances, the matrices that are encountered are large, and with no particular
structure. Physicist Eugene Wigner postulated that one can often replace a large complex
(but deterministic) matrix by a typical element of a certain ensemble of random matrices.
This bold proposal was made in the context of the study of large complex atomic nuclei,
where the “matrix” is the Hamiltonian of the system, which is a Hermitian matrix describ-
ing all the interactions between the neutrons and protons contained in the nucleus. At the
time, these interactions were not well known; but even if they had been, the task of diag-
onalizing the Hamiltonian to ﬁnd the energy levels of the nucleus was so formidable that
Wigner looked for an alternative. He suggested that we should abandon the idea of ﬁnding
precisely all energy levels, but rather rephrase the question as a statistical question: what
is the probability to ﬁnd an energy level within a certain interval, what is the probability
that the distance between two successive levels is equal to a certain value, etc.? The idea
of Wigner was that the answer to these questions could be, to some degree, universal,
i.e. independent of the speciﬁc Hermitian matrix describing the system, provided it was
complex enough. If this is the case, why not replace the Hamiltonian of the system by a
purely random matrix with the correct symmetry properties? In the case of time-reversal
invariant quantum systems, the Hamiltonian is a real symmetric matrix (of inﬁnite size).
In the presence of a magnetic ﬁeld, the Hamiltonian is a complex, Hermitian matrix (see
Section 3.1.1). In the presence of “spin–orbit coupling”, the Hamiltonian is symplectic (see
Section 3.1.2).
This idea has been incredibly fruitful and has led to the development of a subﬁeld
of mathematical physics called “random matrix theory”. In this book we will study the
properties of some ensembles of random matrices. We will mostly focus on symmetric
matrices with real entries as those are the most commonly encountered in data analy-
sis and statistical physics. For example, Wigner’s idea has been transposed to glasses
and spin-glasses, where the interaction between pairs of atoms or pairs of spins is often
replaced by a real symmetric, random matrix (see Section 13.4). In other cases, the ran-
domness stems from noisy observations. For example, when one wants to measure the
covariance matrix of the returns of a large number of assets using a sample of ﬁnite length
(for example the 500 stocks of the S&P500 using 4 years of daily data, i.e. 4 × 250 =
1000 data points per stock), there is inevitably some measurement noise that pollutes the
15

16
Wigner Ensemble and Semi-Circle Law
determination of said covariance matrix. We will be confronted with this precise problem in
Chapters 4 and 17.
In the present chapter and the following one, we will investigate the simplest of all
ensembles of random matrices, which was proposed by Wigner himself in the context
recalled above. These are matrices where all elements are Gaussian random variables, with
the only constraint that the matrix is real symmetric (the Gaussian orthogonal ensemble,
goe), complex Hermitian (the Gaussian unitary ensemble, gue) or symplectic (the Gaus-
sian symplectic ensemble, gse).
2.1 Normalized Trace and Sample Averages
We ﬁrst generalize the notion of expectation value and moments from classical probabilities
to large random matrices. We could simply consider the moments E[Ak] but that object is
very large (N × N dimensional). It is not clear how to interpret it as N →∞. It turns
out that the correct analog of the expectation value is the normalized trace operator τ(.),
deﬁned as
τ(A) := 1
N E[Tr A].
(2.1)
The normalization by 1/N is there to make the normalized trace operator ﬁnite as N →∞.
For example for the identity matrix, τ(1) = 1 independently of the dimension and our
deﬁnition therefore makes sense as N →∞. When using the notation τ(A) we will only
consider the dominant term as N →∞, implicitly taking the large N limit.
For a polynomial function of a matrix F(A) or by extension for a function that can be
written as a power series, the trace of the function can be computed on the eigenvalues:
1
N Tr F(A) = 1
N
N

k=1
F(λk).
(2.2)
In the following, we will denote as ⟨.⟩the average over the eigenvalues of a single matrix
A (sample), i.e.
⟨F(λ)⟩:= 1
N
N

k=1
F(λk).
(2.3)
For large random matrices, many scalar quantities such as τ(F(A)) do not ﬂuctuate from
sample to sample, or more precisely such ﬂuctuations go to zero in the large N limit.
Physicists speak of this phenomenon as self-averaging and mathematicians speak of con-
centration of measure.
τ(F(A)) = 1
N E[Tr F(A)] ≈⟨F(λ)⟩for a single A.
(2.4)
When the eigenvalues of a random matrix A converge to a well-deﬁned density ρ(λ), we
can write

2.2 The Wigner Ensemble
17
τ(F(A)) =

ρ(λ)F(λ)dλ.
(2.5)
Using F(A) = Ak, we can deﬁne the kth moment of a random matrix by mk := τ(Ak).
The ﬁrst moment m1 is simply the normalized trace of A, while m2 = 1/N 
ij A2
ij is
the normalized sum of the squares of all the elements. The square-root of m2 satisﬁes the
axioms of a norm and is called the Frobenius norm of A:
||A||F := √m2.
(2.6)
2.2 The Wigner Ensemble
2.2.1 Moments of Wigner Matrices
We will deﬁne a Wigner matrix X as a symmetric matrix (X = XT ) with Gaussian entries
with zero mean. In a symmetric matrix there are really two types of elements: diagonal and
off-diagonal, which can have different variances. Diagonal elements have variance σ 2
d and
off-diagonal elements have variance σ 2
od. Note that Xij = Xji so they are not independent
variables.
In fact, the elements in a Wigner matrix do not need to be Gaussian or even to be iid,
as there are many weaker (more general) deﬁnitions of the Wigner matrix that yield the
same ﬁnal statistical results in the limit of large matrices N →∞. For the purpose of this
introductory book we will stick to the strong Gaussian hypothesis.
The ﬁrst few moments of our Wigner matrix X are given by
τ(X) = 1
N E[Tr X] = 1
N Tr E[X] = 0,
(2.7)
τ(X2) = 1
N E[Tr XXT ] = 1
N E
⎡
⎣
N

ij=1
X2
ij
⎤
⎦= 1
N [N(N −1)σ 2
od + Nσ 2
d ].
(2.8)
The term containing σ 2
od dominates when the two variances are of the same order of mag-
nitude. So for a Wigner matrix we can pick any variance we want on the diagonal (as long
as it is small with respect to Nσ 2
od). We want to normalize our Wigner matrix so that its
second moment is independent of the size of the matrix (N). Let us pick
σ 2
od = σ 2/N.
(2.9)
For σ 2
d the natural choice seems to be σ 2
d = σ 2/N. However, we will rather choose σ 2
d =
2σ 2/N, which is easy to generate numerically and more importantly respects rotational
invariance for ﬁnite N, as we show in the next subsection. The ensemble described here
(with the choice σ 2
d = 2σ 2
od) is called the Gaussian orthogonal ensemble or goe.1
1 Some authors deﬁne a goe matrix to have σ2 = 1 others as σ2 = N. For us a goe matrix can have any variance and is thus
synonymous with the Gaussian rotationally invariant Wigner matrix.

18
Wigner Ensemble and Semi-Circle Law
To generate a goe matrix numerically, ﬁrst generate a non-symmetric random square
matrix H of size N with iid N(0,σ 2/(2N)) coefﬁcients. Then let the Wigner matrix X
be X = H + HT . The matrix X will then be symmetric with diagonal variance twice the
off-diagonal variance. The reason is that off-diagonal terms are sums of two independent
Gaussian variables, so the variance is doubled. Diagonal elements, on the other hand, are
equal to twice the original variables Hii and so their variance is multiplied by 4.
With any choice of σ 2
d we have
τ(X2) = σ 2 + O(1/N),
(2.10)
and hence we will call the parameter σ 2 the variance of the Wigner matrix.
The third moment τ(X3) = 0 from the fact that the Gaussian distribution is even. Later
we will show that
τ(X4) = 2σ 4.
(2.11)
For standard Gaussian variables E[x4] = 3σ 4, this implies that the eigenvalue density of
a Wigner is not Gaussian. What is this eigenvalue distribution? As we will show many
times over in this book, it is given by the semi-circle law, originally derived by Wigner
himself:
ρ(λ) =
√
4σ 2 −λ2
2πσ 2
for −2σ < λ < 2σ.
(2.12)
2.2.2 Rotational Invariance
We remind the reader that to rotate a vector v, one applies a rotation matrix O: w = Ov
where O is an orthogonal matrix OT = O−1 (i.e. OOT = 1). Note that in general O is not
symmetric. To rotate the basis in which a matrix is written, one writes X = OXOT . The
eigenvalues of X are the same as those of X. The eigenvectors are {Ov} where {v} are the
eigenvectors of X.
A rotationally invariant random matrix ensemble is such that the matrix OXOT is as
probable as the matrix X itself, i.e. OXOT in law
= X.
Let us show that the construction X = H + HT with a Gaussian iid matrix H leads to
a rotationally invariant ensemble. First, note an important property of Gaussian variables,
namely that a Gaussian iid vector v (a white multivariate Gaussian vector) is rotationally
invariant. The reason is that w = Ov is again a Gaussian vector (since sums of Gaussians
are still Gaussian), with covariance given by
E[wiwj] =

kℓ
OikOjℓE[vkvℓ] =

kℓ
OikOjℓδkℓ= [OOT ]ij = δij.
(2.13)
Now, write
X = H + HT,
(2.14)

2.3 Resolvent and Stieltjes Transform
19
where H is a square matrix ﬁlled with iid Gaussian random numbers. Each column of H
is rotationally invariant: OH
in law
= H and the matrix OH is row-wise rotationally invariant:
OHOT in law
= OH. So H is rotationally invariant as a matrix. Now
OXOT = O(H + HT )OT in law
= H + HT = X,
(2.15)
which shows that the Wigner ensemble with σ 2
d = 2σ 2
od is rotationally invariant for any
matrix size N. More general deﬁnitions of the Wigner ensemble (including non-Gaussian
ensembles) are only asymptotically rotationally invariant (i.e. when N →∞).
Another way to see the rotational invariance of the Wigner ensemble is to look at the
joint law of matrix elements:
P({Xij}) =

1
2πσ 2
d
N/2 
1
2πσ 2
od
N(N−1)/4
exp
⎧
⎨
⎩−
N

i=1
X2
ii
2σ 2
d
−
N

i<j
X2
ij
2σ 2
od
⎫
⎬
⎭,
(2.16)
where only the diagonal and upper triangular elements are independent variables. With the
choice σ 2
od = σ 2/N and σ 2
d = 2σ 2/N this becomes
P({Xij}) ∝exp
$
−N
4σ 2 Tr X2
%
.
(2.17)
Under the change of variable X →X = OXOT the argument of the exponential is invariant,
because the trace of a matrix is independent of the basis, and because the Jacobian of the
transformation is equal to 1 (see Section 1.2.7), therefore X
in law
= OXOT .
By the same argument any matrix whose joint probability density of its elements can
be written as P({Mij}) ∝exp {−N Tr V (M)}, where V (.) is an arbitrary function, will be
rotationally invariant. We will study such matrix ensembles in Chapter 5.
2.3 Resolvent and Stieltjes Transform
2.3.1 Deﬁnition and Basic Properties
In this section we introduce the Stieltjes transform of a matrix. It will give us information
about all the moments of the random matrix and also about the density of its eigenvalues in
the large N limit. First we need to deﬁne the matrix resolvent.
Given an N × N real symmetric matrix A, its resolvent is given by
GA(z) = (z1 −A)−1,
(2.18)
where z is a complex variable deﬁned away from all the (real) eigenvalues of A and 1
denotes the identity matrix. Then the Stieltjes transform of A is given by2
gA
N(z) = 1
N Tr (GA(z)) = 1
N
N

k=1
1
z −λk
,
(2.19)
2 In mathematical literature, the Stieltjes transform is more commonly deﬁned as sA(z) = −(1/N) Tr GA(z), i.e. with an extra
minus sign. Some authors prefer the name Cauchy transform.

20
Wigner Ensemble and Semi-Circle Law
where λk are the eigenvalues of A. The subscript N indicates that this is the ﬁnite N Stieltjes
transform of a single realization of A. When it is clear from context which matrix we
consider we will drop the superscript A and write gN(z).
Let us see why the Stieltjes transform gives useful information about the density of
eigenvalues of A. For a given random matrix A, we can deﬁne the empirical spectral
distribution (ESD) also called the sample eigenvalue density:
ρN(λ) = 1
N
N

k=1
δ(λ −λk),
(2.20)
where δ(x) is the Dirac delta function. Then the Stieltjes transform can be written as
gN(z) =
 +∞
−∞
ρN(λ)
z −λ dλ.
(2.21)
Note that gN(z) is well deﬁned for any z  {λk : 1 ≤k ≤N}. In particular, it is well
behaved at ∞:
gN(z) =
∞

k=0
1
zk+1
1
N Tr(Ak),
1
N Tr(A0) = 1.
(2.22)
We will consider random matrices A such that, for large N, the normalized traces of powers
of A converge to their expectation values, which are deterministic numbers:
lim
N→∞
1
N Tr(Ak) = τ(Ak).
(2.23)
We then expect that, for large enough z, the function gA(z) converges to a deterministic
limit g(z) deﬁned as g(z) = limN→∞E[gN(z)], whose Taylor series is
g(z) =
∞

k=0
1
zk+1 τ(Ak),
(2.24)
for z away from the real axis.
Thus g(z) is a moment generating function of A. In other words, the knowledge of g(z)
near inﬁnity is equivalent to the knowledge of all the moments of A. To the level of rigor
of this book, the knowledge of all the moments of A is equivalent to the knowledge of the
density of its eigenvalues. For any function F(x) deﬁned over the support of the eigenvalues
[λ−,λ+] of A we can compute its expectation:
τ(F(A)) =
 λ+
λ−
ρ(λ)F(λ)dλ;
ρ(λ) := E[ρA(λ)].
(2.25)
Alternatively we can approximate the function F(x) arbitrarily well by a polynomial
Q(x) = a0 + a1x + · · · + aKxK and ﬁnd
τ(F(A)) ≈τ(Q(A)) =
K

k=0
akτ(Ak).
(2.26)

2.3 Resolvent and Stieltjes Transform
21
To recap, we only need to know g(z) in the neighborhood of |z| →∞to know all the
moments of A and these moments tell us everything about ρ(λ). In computing the Stieltjes
transform in concrete cases, we will often make use of that fact and only estimate it for
very large values of z.
The Stieltjes transform also gives the negative moments when they exist. If the eigen-
values of A satisfy minℓλℓ> c for some constant c > 0, then the inverse moments of A
exist and are given by the expansion of g(z) around z = 0:
g(z) = −
∞

k=0
zkτ(A−k−1).
(2.27)
In particular, we have
g(0) = −τ(A−1).
(2.28)
Exercise 2.3.1
Stieltjes transform for shifted and scaled matrices
Let A be a random matrix drawn from a well-behaved ensemble with Stieltjes
transform g(z). What are the Stieltjes transforms of the random matrices αA and
A + β1 where α and β are non-zero real numbers and 1 the identity matrix?
2.3.2 Stieltjes Transform of the Wigner Ensemble
We are now ready to compute the Stieltjes transform of the Wigner ensemble. The ﬁrst
technique we will use is sometimes called the cavity method or the self-consistent equation.
We will ﬁnd a relation between the Stieltjes transform of a Wigner matrix of size N and
one of size N −1. In the large N limit, the two converge to the same limiting Stieltjes
transform and give us a self-consistent equation that can be solved easily.
We would like to calculate gX
N(z) when X is a Wigner matrix, with Xij ∼N(0,σ 2/N)
and Xii ∼N(0,2σ 2/N). In the large N limit, we expect that gX
N(z) converges towards a
well-deﬁned limit g(z).
We can use the Schur complement formula (1.32) to compute the (1,1) element of the
inverse of M = z1 −X. Then we have
1
(GX)11
= M11 −
N

k,l=2
M1k(M22)−1
kl Ml1,
(2.29)
where the matrix M22 is the (N −1)×(N −1) submatrix of M with the ﬁrst row and column
removed. For large N, we argue that the right hand side is dominated by its expectation
value with small (O(1/
√
N)) ﬂuctuations. We will only compute its expectation value,
but getting a more precise handle on its ﬂuctuations would not be difﬁcult. First, we note

22
Wigner Ensemble and Semi-Circle Law
that E[M11] = z. We then note that the entries of M22 are independent of the ones of
M1i = −X1i. Thus we can ﬁrst take the partial expectation over the {X1i}, and get
E{X1i}
&
M1i(M22)−1
ij M1j
'
= σ 2
N (M22)−1
ii δij
(2.30)
so we have
E{X1i}
⎡
⎣
N

k,l=2
M1k(M22)−1
kl Ml1
⎤
⎦= σ 2
N Tr

(M22)−1
.
(2.31)
Another observation is that 1/(N −1) Tr

(M22)−1
is the Stieltjes transform of a Wigner
matrix of size N −1 and variance σ 2(N −1)/N. In the large N limit, the Stieltjes transform
should be independent of the matrix size and the difference between N and (N −1) is
negligible. So we have
E
( 1
N Tr

(M22)−1)
→g(z).
(2.32)
We therefore have that 1/(GX)11 equals a deterministic number with negligible ﬂuctua-
tions; hence in the large N limit we have
E
(
1
(GX)11
)
=
1
E[(GX)11].
(2.33)
From the rotational invariance of X and therefore of GX, all diagonal entries of GX must
have the same expectation value:
E [(GX)11] = 1
N E[Tr(GX)] = E[gN] →g.
(2.34)
Putting all the pieces together, we ﬁnd that in the large N limit Eq. (2.29) becomes
1
g(z) = z −σ 2g(z).
(2.35)
Solving (2.35) we obtain that
σ 2g2 −zg + 1 = 0 ⇒g = z ±
√
z2 −4σ 2
2σ 2
.
(2.36)
We know that g(z) should be analytic for large complex z but the square-root above can run
into branch cuts. It is convenient to pull out a factor of z and express the square-root as a
function of 1/z which becomes small for large z:
g(z) = z ± z
*
1 −4σ 2/z2
2σ 2
.
(2.37)
We can now choose the correct root: the + sign gives an incorrect g(z) ∼z/σ 2 for large z
while the −sign gives g(z) ∼1/z for any large complex z as expected, so we have:
g(z) = z −z
*
1 −4σ 2/z2
2σ 2
.
(2.38)

2.3 Resolvent and Stieltjes Transform
23
z ∈C
−2σ
2σ
g(z) is analytic
|z| < 2σ
|z| > 2σ
branch point
branch cut
Figure 2.1 The branch cuts of the Wigner Stieltjes transform.
Note, for numerical applications, it is very important to pick the correct branch of the
square-root. The function g(z) is analytic for |z| > 2σ, the branch cuts of the square-root
must therefore be conﬁned to the interval [−2σ,2σ] (Fig. 2.1). We will come back to this
problem of determining the correct branch of Stieltjes transform in Section 4.2.3.
It might seem strange that g(z) given by Eq. (2.38) has no poles but only branch cuts.
For ﬁnite N, the sample Stieltjes transform
gN(z) := 1
N
N

k=1
1
z −λk
(2.39)
has poles at the eigenvalues of X. As N →∞, the poles fuse together and
1
N
N

k=1
δ(x −λk) ∼ρ(x).
(2.40)
The density ρ(x) can have extended support and/or isolated Dirac masses. Then as N →∞,
we have
g(z) =

supp{ρ}
ρ(x)dx
z −x ,
(2.41)
which is the Stieltjes transform of the limiting measure ρ(x).
2.3.3 Convergence of Stieltjes near the Real Axis
It is natural to ask the following questions: how does gN(z) given by Eq. (2.39) converge
to g(z) =
 ρ(x)dx
z−x , and how do we recover ρ(x) from g(z)?

24
Wigner Ensemble and Semi-Circle Law
We have argued before that gN(z) converges to g(z) for very large complex z such
that the Taylor series around inﬁnity is convergent. The function g(z) is not deﬁned on
the real axis for z = x on the support of ρ(x), nevertheless, immediately below (and
above) the real axis the random function gN(z) converges to g(z). (The case where z is
right on the real axis is discussed in Section 2.3.6.) Let us study the random function gN(z)
just below the support of ρ(x).
We let z = x −iη, with x ∈supp{ρ} and η is a small positive number. Then
gN(x −iη) := 1
N
N

k=1
1
x −iη −λk
= 1
N
N

k=1
x −λk + iη
(x −λk)2 + η2 .
(2.42)
We focus on the imaginary part of gN(x −iη) (the real part is discussed in Section 19.5.2).
Note that it is a convolution of the empirical spectral density ρN(λ) and π times the Cauchy
kernel:
πKη(x) =
η
x2 + η2 .
(2.43)
The Cauchy kernel Kη(x) is strongly peaked around zero with a window width of order
η (Fig. 2.2). Since there are N eigenvalues lying inside the interval [λ−,λ+], the typical
eigenvalue spacing is of order (λ+ −λ−)/N = O(N−1).
(1) Suppose η ≪N−1. Then there are typically 0 or 1 eigenvalue within a window of size
η around x. Then Im gN will be affected by the ﬂuctuations of single eigenvalues of X,
and hence it cannot converge to any deterministic function. (see Fig. 2.3).
−4
−2
0
2
4
x
0.0
0.2
0.4
0.6
K (x)
2
Figure 2.2 The Cauchy kernel for η = 0.5. It is strongly peaked around zero with a window width of
order η. When η →0, the Cauchy kernel is a possible representation of Dirac’s δ-function.

2.3 Resolvent and Stieltjes Transform
25
−2
0
2
x
0.0
0.2
0.4
0.6
0.8
1.0
Im (x−i )
= 1/
N
= 1
analytic
−0.1
0.0
0.1
x
0.0
0.5
1.0
1.5
2.0
2.5
Im (x−i )
= 1/ N
= 1/
N
analytic
Figure 2.3 Imaginary part of g(x −iη) for the Wigner ensemble. The analytic result for η →0+
is compared with numerical simulations (N = 400). On the left for η = 1/
√
N and η = 1. Note
that for η = 1 the density is quite deformed. On the right (zoom near x = 0) for η = 1/
√
N and
η = 1/N. Note that for η = 1/N, the density ﬂuctuates wildly as only a small number of (random)
eigenvalues contribute to the Cauchy kernel.
(2) Suppose N−1 ≪η ≪1 (e.g. η = N−1/2). Then on a small scale η ≪x ≪1, the
density ρ is locally constant and there are a great number n of eigenvalues inside:
n ∼Nρ(x)x ≫Nη ≫1.
(2.44)
The law of large numbers allows us to replace the sum with an integral; we obtain that
1
N

k:λk∈[x−x,x+x]
iη
(x −λk)2 + η2 →i
 x+x
x−x
ρ(x)ηdy
(x −y)2 + η2 →iπρ(x),
(2.45)
where the last limit is obtained by writing u = (y −x)/η and noting that as η →0
we have
 ∞
−∞
du
u2 + 1 = π.
(2.46)
Exercise 2.3.2
Finite N approximation and small imaginary part
Im gN(x −iη)/π is a good approximation to ρ(x) for small positive η,
where gN(z) is the sample Stieltjes transform (gN(z) = (1/N) 
k 1/(z −λk)).
Numerically generate a Wigner matrix of size N and σ 2 = 1.
(a)
For three values of η, {1/N,1/
√
N,1}, plot Im gN(x −iη)/π and the
theoretical ρ(x) on the same plot for x between −3 and 3.

26
Wigner Ensemble and Semi-Circle Law
(b)
Compute the error as a function of η where the error is (ρ(x) −Im gN(x −
iη)/π)2 summed for all values of x between −3 and 3 spaced by intervals of
0.01. Plot this error for η between 1/N and 1. You should see that 1/
√
N is
very close to the minimum of this function.
2.3.4 Stieltjes Inversion Formula
From the above discussions, we extract the following important results:
(1) The Stieltjes inversion formula (also called the Sokhotski–Plemelj formula):
lim
η→0+ Im g(x −iη) = πρ(x).
(2.47)
(2) When applied to ﬁnite size Stieltjes transform gN(z), we should take N−1 ≪η ≪1
for gN(x −iη) to converge to g(x −iη) and for (2.47) to hold. Numerically, η = N−1/2
works quite well.
We discuss brieﬂy why η = N−1/2 works best. First, we want η to be as small as
possible such that the local density ρ(x) is not blurred. If η is too large, one introduces
a systematic error of order ρ′(x)η. On the other hand, we want Nη to be as large as
possible such that we include the statistics of a sufﬁcient number of eigenvalues so that
we measure ρ(x) accurately. In fact, the error between gN and g is of order
1
Nη . Thus we
want to minimize the total error E given by
E = ρ′(x)η +
1
Nη,
ρ′(x)η : systematic error,
1
Nη : statistical error.
(2.48)
Then it is easy to see that the total error is minimized when η is of order 1/
*
Nρ′(x).
2.3.5 Density of Eigenvalues of a Wigner Matrix
We go back to study the Stieltjes transform (2.38) of the Wigner matrix. Note that for
z = x −iη with η →0, g(z) can only have an imaginary part if
√
x2 −4σ 2 is imaginary.
Then, using (2.47), we get the Wigner semi-circle law:
ρ(x) = 1
π
lim
η→0+ Im g(x −iη) =
√
4σ 2 −x2
2πσ 2
,
−2σ ≤x ≤2σ.
(2.49)
Note the following features of the semi-circle law (see Fig. 2.4): (1) asymptotically there
is no eigenvalue for x > 2σ and x < −2σ; (2) the eigenvalue density has square-root
singularities near the edges: ρ(x) ∼
√
x + 2σ near the left edge and ρ(x) ∼
√
2σ −x
near the right edge. For ﬁnite N, some eigenvalues are present in a small region of width
N−2/3 around the edges, see Section 14.1.

2.3 Resolvent and Stieltjes Transform
27
−3
−2
−1
0
1
2
3
0.0
0.1
0.2
0.3
Figure 2.4 Density of eigenvalues of a Wigner matrix with σ = 1: the semi-circle law.
Exercise 2.3.3
From the moments to the density
A large random matrix has moments τ(Ak) = 1/k.
(a)
Using Eq. (2.24) write the Taylor series of g(z) around inﬁnity.
(b)
Sum the series to get a simple expression for g(z). Hint: look up the Taylor
series of log(1 + x).
(c)
Where are the singularities of g(z) on the real axis?
(d)
Use Eq. (2.47) to ﬁnd the density of eigenvalues ρ(λ).
(e)
Check your result by recomputing the moments and the Stieltjes transform
from ρ(λ).
(f)
Redo all the above steps for a matrix whose odd moments are zero and even
moments are τ(A2k) = 1. Note that in this case the density ρ(λ) has Dirac
masses.
2.3.6 Stieltjes Transform on the Real Axis
What about computing the Stieltjes transform when z is real and inside the spectrum?
This seems dangerous at ﬁrst sight, since gN(z) diverges when z is equal to one of the
eigenvalues of X. As these eigenvalues become more and more numerous as N goes to
inﬁnity, z will always be very close to a pole of the resolvent gN(z). Interestingly, one can
turn this predicament on its head and actually exploit these divergences. In a hand-waving
manner, the probability that the difference di = |z −λi| between z (now real) and a given
eigenvalue λi is very small, is given by
P[di < ϵ/N] = 2ϵρ(z),
(2.50)

28
Wigner Ensemble and Semi-Circle Law
where ρ(z) is the normalized density of eigenvalues around z. But as ϵ →0, i.e. when
z is extremely close to λi, the resolvent becomes dominated by a unique contribution –
that of the λi term, all other terms (z −λj)−1, j  i, become negligible. In other words,
gN(z) ≈±(Ndi)−1, and therefore
P[|g| > ϵ−1] = P[di < ϵ/N] = 2ϵρ(z).
(2.51)
Hence, gN(z) does not converge for N →∞when z is real, but the tail of its distribution
decays precisely as ρ(z)/g2. Studying this tail thus allows one to extract the eigenvalue
density ρ(z) while working directly on the real axis. Let us show how this works in the
case of Wigner matrices.
The idea is that, for a rotationally invariant problem, the distribution of a randomly
chosen diagonal element of the resolvent (say G11) is the same as the distribution P(g)
of the normalized trace.3 With this assumption, Eq. (2.29) can be interpreted as giving the
evolution of P(g) itself, i.e.
P (N)(g) =
 +∞
−∞
dg′P (N−1)(g′)δ

g −
1
z −σ 2g′

,
(2.52)
where we have used the fact that, for large N, N
k,ℓ=2 M1k(M22)−1
kℓMℓℓ→σ 2g(N−1).
Now, this functional iteration admits the following Cauchy distribution as a ﬁxed point:
P ∞(g) =
ρ(z)
(g −
z
2σ 2 )2 + π2ρ(z).
(2.53)
This simple result, that the resolvent of a Wigner matrix on the real axis is a Cauchy vari-
able, calls for several comments. First, one ﬁnds that P ∞(g) indeed behaves as ρ(z)/g2
for large g, as argued above. Second, it would have been entirely natural to ﬁnd a Cauchy
distribution for g had the eigenvalues been independent. Indeed, since g is then the sum
of N random variables (i.e. the 1/di’s) distributed with an inverse square power, the
generalized clt predicts that the resulting sum is Cauchy distributed. In the present case,
however, the eigenvalues are strongly correlated – see Section 5.1.4. It was recently proven
that the Cauchy distribution is in fact super-universal and holds for a wide class of point
processes on the real axis, in particular for the eigenvalues of random matrices. It is in fact
even true when these eigenvalues are strictly equidistant, with a random global shift.
Bibliographical Notes
• The overall content of this chapter is covered in many books and textbooks, see for
example
– M. L. Mehta. Random Matrices. Academic Press, San Diego, 3rd edition, 2004,
– T. Tao. Topics in Random Matrix Theory. American Mathematical Society, Provi-
dence, Rhode Island, 2012,
– G. W. Anderson, A. Guionnet, and O. Zeitouni. An Introduction to Random Matrices.
Cambridge University Press, Cambridge, 2010,
– G. Blower. Random Matrices: High Dimensional Phenomena. Cambridge University
Press, Cambridge, 2009,
3 This is not a trivial statement but it can be proven along the lines of Aizenman and Warzel [2015].

2.3 Resolvent and Stieltjes Transform
29
– B. Eynard, T. Kimura, and S. Ribault. Random matrices. preprint arXiv:1510.04430,
2006,
– G. Livan, M. Novaes, and P. Vivo. Introduction to Random Matrices: Theory and
Practice. Springer, New York, 2018.
• For some historical papers, see
– E. P. Wigner. On the statistical distribution of the widths and spacings of nuclear res-
onance levels. Mathematical Proceedings of the Cambridge Philosophical Society,
47(4):790–798, 1951,
– F. J. Dyson. The threefold way: algebraic structure of symmetry groups and ensem-
bles in quantum mechanics. Journal of Mathematical Physics, 3(6):1199–1215,
1962.
• About the Stieltjes transform on the real line and the super-universality of the Cauchy
distribution, see
– M. Aizenman and S. Warzel. On the ubiquity of the Cauchy distribution in spectral
problems. Probability Theory and Related Fields, 163:61–87, 2015,
– Y. V. Fyodorov and D. V. Savin. Statistics of impedance, local density of states, and
reﬂection in quantum chaotic systems with absorption. Journal of Experimental and
Theoretical Physics Letters, 80(12):725–729, 2004,
– Y. V. Fyodorov and I. Williams. Replica symmetry breaking condition exposed by
random matrix calculation of landscape complexity. Journal of Statistical Physics,
129(5-6):1081–1116, 2007,
– J.-P. Bouchaud and M. Potters. Two short pieces around the Wigner problem. Journal
of Physics A: Mathematical and Theoretical, 52(2):024001, 2018,
and for a related discussion see also
– M. Griniasty and V. Hakim. Correlations and dynamics in ensembles of maps: Simple
models. Physical Review E, 49(4):2661, 1994.

3
More on Gaussian Matrices*
In the previous chapter, we dealt with the simplest of all Gaussian matrix ensembles, where
entries are real, Gaussian random variables, and global symmetry is imposed. It was pointed
out by Dyson that there exist precisely three division rings that contain the real numbers,
namely, the real themselves, the complex numbers and the quaternions. He showed that this
fact implies that there are only three acceptable ensembles of Gaussian random matrices
with real eigenvalues: goe, gue and gse. Each is associated with a Dyson index called β
(1, 2 and 4, respectively) and except for this difference in β almost all of the results in
this book (and many more) apply to the three ensembles. In particular their moments and
eigenvalue density are the same as N →∞, while correlations and deviations from the
asymptotic formulas follow families of laws with β as a parameter. In this chapter we will
review the other two ensembles (gue and gse),1 and also discuss the general moments of
Gaussian random matrices, for which some interesting mathematical tools are available,
that are useful beyond rmt.
3.1 Other Gaussian Ensembles
3.1.1 Complex Hermitian Matrices
For matrices with complex entries, the analog of a symmetric matrix is a (complex) Her-
mitian matrix. It satisﬁes A† = A where the dagger operator is the combination of matrix
transposition and complex conjugation. There are two important reasons to study com-
plex Hermitian matrices. First they appear in many applications, especially in quantum
mechanics. There, the energy and other observables are mapped into Hermitian operators,
or Hermitian matrices for systems with a ﬁnite number of states. The ﬁrst large N result
of random matrix theory is the Wigner semi-circle law. As recalled in the introduction to
Chapter 2, it was obtained by Wigner as he modeled the energy levels of complex heavy
nuclei as a random Hermitian matrix.
1 More recently, it was shown how ensembles with an arbitrary value of β can be constructed, see Dumitriu and Edelman
[2002], Allez et al. [2012].
30

3.1 Other Gaussian Ensembles
31
The other reason Hermitian matrices are important is mathematical. In the large N limit,
the three ensembles (real, complex and quaternionic (see below)) behave the same way.
But for ﬁnite N, computations and proofs are much simpler in the complex case. The main
reason is that the Vandermonde determinant which we will introduce in Section 5.1.4
is easier to manipulate in the complex case. For this reason, most mathematicians
discuss the complex Hermitian case ﬁrst and treat the real and quaternionic cases as
extensions. In this book we want to stay close to applications in data science and statistical
physics, so we will discuss complex matrices only in the present chapter. In the rest of
the book we will indicate in footnotes how to extend the result to complex Hermitian
matrices.
A complex Hermitian matrix A has real eigenvalues and it can be diagonalized with
a suitable unitary matrix U. A unitary matrix satisﬁes U†U = 1. So A can be written as
A = UU†, with  the diagonal matrix containing its N eigenvalues.
We want to build the complex Wigner matrix: a Hermitian matrix with iid Gaussian
entries. We will choose a construction that has unitary invariance for every N. Let us study
the unitary invariance of complex Gaussian vectors. First we need to deﬁne a complex
Gaussian variable.
We say that the complex variable z is centered Gaussian with variance σ 2 if z = xr +i xi
where xr and xi are centered Gaussian variables of variance σ 2/2. We have
E[|z|2] = E[x2
r ] + E[x2
i ] = σ 2.
(3.1)
A white complex Gaussian vector x is a vector whose components are iid complex centered
Gaussians. Consider y = Ux where U is a unitary matrix. Each of the components is a linear
combination of Gaussian variables so y is Gaussian. It is relatively straightforward to show
that each component has the same variance σ 2 and that there is no covariance between
different components. Hence y is also a white Gaussian vector. The ensemble of a white
complex Gaussian vector is invariant under unitary transformation.
To deﬁne the Hermitian Wigner matrix, we ﬁrst deﬁne a (non-symmetric) square
matrix H whose entries are centered complex Gaussian numbers and let X be the Hermitian
matrix deﬁned by
X = H + H†.
(3.2)
If we repeat the arguments of Section 2.2.2, we can show that the ensemble of X is invariant
under unitary transformation: UXU† in law
= X.
We did not specify the variance of the elements of H. We would like X to be normalized
as τ(X2) = σ 2 +O(1/N). Choosing the variance of the H as E[|Hij|2] = 1/(2N) achieves
precisely that.
The Hermitian matrix X has real diagonal elements with E[X2
ii] = 1/N and off-diagonal
elements that are complex Gaussian with E[|Xij|2] = 1/N. In other words the real and
imaginary parts of the off-diagonal elements of X have variance 1/(2N). We can put

32
More on Gaussian Matrices
all this information together in the joint law of the matrix elements of the Hermitian
matrix H:
P({Xij}) ∝exp
$
−N
2σ 2 Tr X2
%
.
(3.3)
This law is identical to the real symmetric case (Eq. 2.17) up to a factor of 2. We can then
write both the symmetric and the Hermitian case as
P({Xij}) ∝exp
$
−βN
4σ 2 Tr X2
%
,
(3.4)
where β is 1 or 2 respectively.
The complex Hermitian Wigner ensemble is called the Gaussian unitary ensemble
or gue.
The results of the previous chapter apply equally to the real symmetric and the
complex Hermitian case. Both the self-consistent equation for the Stieltjes transform
and the counting of non-crossing pair partitions (see next section) rely on the independence
of the elements of the matrix and on the fact that E[|Xij|2] = 1/N, true in both cases.
We then have that the Stieltjes transform of the two ensembles is the same and they have
exactly the same semi-circle distribution of eigenvalues in the large N limit. The same will
be true for the quaternionic case (β = 4) in the next section, and in fact for all values of β
provided Nβ →∞when N →∞, see Section 5.3.1:
ρβ(λ) =
√
4σ 2 −λ2
2πσ 2
,
−2σ ≤λ ≤2σ.
(3.5)
3.1.2 Quaternionic Hermitian Matrices
We will deﬁne here the quaternionic Hermitian matrices and the gse. There are many
fewer applications of quaternionic matrices than the more common real or complex matri-
ces. We include this discussion here for completeness. In the literature the link between
symplectic matrices and quaternions can be quite obscure for the novice reader. Except for
the existence of an ensemble of matrices with β = 4 we will never refer to quaternionic
matrices after this section, which can safely be skipped.
Quaternions are non-commutative extensions of the real and complex numbers. They
are written as real linear combinations of the real number 1 and three abstract non-
commuting objects (i,j,k) satisfying
i2 = j2 = k2 = ijk = −1
⇒
ij = −ji = k,
jk = −kj = i,
ki = −ik = j. (3.6)
So we can write a quaternion as h = xr+i xi+j xj+k xk. If only xr is non-zero we say that
h is real. We deﬁne the quaternionic conjugation as 1∗= 1,i∗= −i,j∗= −j,k∗= −k
so that the norm |h|2 := hh∗= x2r + x2
i + x2
j + x2
k is always real and non-negative. The
abstract objects i, j and k can be represented as 2 × 2 complex matrices:
1 =

1
0
0
1

,
i =

i
0
0
−i

,
j =

0
1
−1
0

,
k =

0
i
i
0

,
(3.7)
where the i in the matrices is now the usual unit imaginary number.
Quaternions share all the algebraic properties of real and complex numbers except for
commutativity (they form a division ring). Since matrices in general do not commute,
matrices built out of quaternions behave like real or complex matrices.

3.1 Other Gaussian Ensembles
33
A Hermitian quaternionic matrix is a square matrix A whose elements are quaternions
and satisfy A = A†. Here the dagger operator is the combination of matrix transposition
and quaternionic conjugation. They are diagonalizable and their eigenvalues are real.
Matrices that diagonalize Hermitian quaternionic matrices are called symplectic. Written
in terms of quaternions they satisfy SS† = 1.
Given representation of quaternions as 2×2 complex matrices, an N ×N quaternionic
Hermitian matrix A can be written as a 2N × 2N complex matrix Q(A). We choose a
representation where
Z := Q(1j) =

0
1
−1
0

.
(3.8)
For a 2N × 2N complex matrix Q to be the representation of a quaternionic Hermitian
matrix it has to have two properties. First, quaternionic conjugation acts just like Hermitian
conjugation so Q† = Q. Second, it has to be expressible as a real linear combination of
unit quaternions. One can show that such matrices (and only them) satisfy
QR := ZQT Z−1 = Q†,
(3.9)
where QR is called the dual of Q. In other words an N ×N Hermitian quaternionic matrix
corresponds to a 2N × 2N self-dual Hermitian matrix (i.e. Q = Q† = QR). In this
2N × 2N representation symplectic matrices are complex matrices satisfying
SS† = SSR = 1.
(3.10)
To recap, a 2N × 2N Hermitian self-dual matrix Q can be diagonalized by a symplectic
matrix S. Its 2N eigenvalues are real and they occur in pairs as they are the N eigenvalues
of the equivalent Hermitian quaternionic N × N matrix.
We can now deﬁne the third Gaussian matrix ensemble, namely the Gaussian symplec-
tic ensemble (gse) consisting of Hermitian quaternionic matrices whose off-diagonal ele-
ments are quaternions with Gaussian distribution of zero mean and variance E[|Xij|2] =
1/N. This means that each of the four components of each Xij is a Gaussian number of
zero mean and variance 1/(4N). The diagonal elements of X are real Gaussian numbers
with zero mean and variance 1/(2N). As usual Xij = X∗
ji so only the upper (or lower)
triangular elements are independent. The joint law for the elements of a gse matrix with
variance τ(X2) = σ 2 is given by
P({Xij}) ∝exp
$
−N
σ 2 Tr X2
%
,
(3.11)
which we identify with Eq. (3.4) with β = 4. This parameter β = 4 is a fundamental prop-
erty of the symplectic group and will consistently appear in contrast with the orthogonal
and unitary cases, β = 1 and β = 2 (see Section 5.1.4).
The parameter β can be interpreted as the randomness in the norm of the matrix
elements. More precisely, we have
|Xij|2 =
⎧
⎪⎪⎨
⎪⎪⎩
x2r
for real symmetric,
x2r + x2
i
for complex Hermitian,
x2r + x2
i + x2
j + x2
k
for quaternionic Hermitian,
(3.12)
where xr,xi,xj,xk are real Gaussian numbers such that E[|Xij|2] = 1. We see that the
ﬂuctuations of |Xij|2 decrease with β (precisely V[|Xij|2] = 2/β). By the law of large
numbers (lln), in the β →∞limit (if such an ensemble existed) we would have
|Xij|2 = 1 with no ﬂuctuations.

34
More on Gaussian Matrices
Exercise 3.1.1
Quaternionic matrices of size one
The four matrices in Eq. (3.7) can be thought of as the 2 × 2 complex
representations of the four unit quaternions.
(a)
Deﬁne Z := j and compute Z−1.
(b)
Show that for all four matrices Q, we have ZQT Z−1 = Q† where the dagger
here is the usual transpose plus complex conjugation.
(c)
Convince yourself that, by linearity, any Q that is a real linear combination of
the 2 × 2 matrices i, j, k and 1 must satisfy ZQT Z−1 = Q†.
(d)
Give an example of a matrix Q that does not satisfy ZQT Z−1 = Q†.
3.1.3 The Ginibre Ensemble
The Gaussian orthogonal ensemble is such that all matrix elements of X are iid Gaussian,
but with the strong constraint that Xij = Xji, which makes sure that all eigenvalues of
X are real. What happens if we drop this constraint and consider a square matrix H with
independent entries? In this case, one may choose two different routes, depending on the
context.
• One route is simply to allow eigenvalues to be complex numbers. One can then study
the eigenvalue distribution in the complex plane, so the distribution becomes a two-
dimensional density. Some of the tools introduced in the previous chapter, such as the
Sokhotski–Plemelj formula, can be generalized to complex eigenvalues. The ﬁnal result
is called the Girko circular law: the density of eigenvalues is constant within a disk
centered at zero and of radius σ (see Fig. 3.1). In the general case where E[HijHji] =
ρσ 2, the eigenvalues are conﬁned within an ellipse of half-width (1 + ρ)σ along the
real axis and (1 −ρ)σ in the imaginary direction, interpolating between a circle for
ρ = 0 (independent entries) and a line segment on the real axis of length 4σ for ρ =
1 (symmetric matrices).
• The other route is to focus on the singular values of H. One should thus study the real
eigenvalues of HT H when H is a square random matrix made of independent Gaussian
elements. This is precisely the Wishart problem that we will study in Chapter 4, for the
special parameter value q = 1. Calling s the square-root of these real eigenvalues, the
ﬁnal result is a quarter-circle:
ρ(s) =
√
4σ 2 −s2
πσ 2
;
s ∈(0,2σ).
(3.13)

3.1 Other Gaussian Ensembles
35
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
Rel
−1.0
−0.5
0.0
0.5
1.0
Iml
Figure 3.1 Complex eigenvalues of a random N = 1000 matrix taken from the Gaussian Ginibre
ensemble, i.e. a non-symmetric matrix with iid Gaussian elements with variance σ 2 = 1/N.
The dash line corresponds to the circle |λ|2 = 1. As N →∞the density becomes uniform in
the complex unit disk. This distribution is called the circle law or sometimes, more accurately, the
disk law.
Exercise 3.1.2
Three quarter-circle laws
Let H be a (non-symmetric) square matrix of size N whose entries are iid
Gaussian random variable of variance σ 2/N. Then as a simple consequence
of the above discussion the following three sets of numbers are distributed
according to the quarter-circle law (3.13) in the large N limit. Deﬁne
wi = |λi| where {λi} are the eigenvalues of H + HT
√
2
,
ri = 2| Re λi| where {λi} are the eigenvalues of H,
si =
*
λi where {λi} are the eigenvalues of HHT .
(a)
Generate a large matrix H with say N = 1000 and σ 2 = 1 and plot the
histogram of the three above sets.
(b)
Although these three sets of numbers converge to the same distribution there
is no simple relation between them. In particular they are not equal. For a
moderate N (10 or 20) examine the three sets and realize that they are all
different.

36
More on Gaussian Matrices
3.2 Moments and Non-Crossing Pair Partitions
3.2.1 Fourth Moment of a Wigner Matrix
We have stated in Section 2.2.1 that for a Wigner matrix we have τ(X4) = 2σ 4. We will
now compute this fourth moment directly and then develop a technique to compute all other
moments.
We have
τ(X4) = 1
N E[Tr(X4)] = 1
N

i,j,k,l
E[XijXjkXklXli].
(3.14)
Recall that (Xij : 1 ≤i ≤j ≤N) are independent Gaussian random variables of mean
zero. So for the expectations in the above sum to be non-zero, each X entry needs to be
equal to another X entry.2 There are two possibilities. Either all four are equal or they are
equal pairwise. In the following we will not distinguish between diagonal and off-diagonal
terms; as there are many more off-diagonal terms these terms always dominate.
(1) If Xij = Xjk = Xkl = Xli, then
E[XijXjkXklXli] = 3σ 4
N2 ,
(3.15)
and there are N2 of them. Thus the total contribution from these terms is
1
N N2 3σ 4
N2 = 3σ 4
N
→0.
(3.16)
(2) Suppose there are two different pairs. Then there are three possibilities (see Fig. 3.2):
(i) Xij = Xjk, Xkl = Xli, and Xij is different than Xli (i.e. j  l). Then
(3.14) = 1
N

i,jl
E[X2
ijX2
il] = 1
N (N3 −N2)
σ 2
N
2
→σ 4
(3.17)
as N →∞.
Xi1,i2
Xi2,i3
Xi3,i4
Xi4,i3
Figure 3.2 Graphical representation of the three terms contributing to τ(X4). The last one is a
crossing partition and has a zero contribution.
2 When we say that Xij = Xkl, we mean that they are the same random variable; given that X is a symmetric matrix it means
either (i = k and j = l) or (i = l and j = k).

3.2 Moments and Non-Crossing Pair Partitions
37
(ii) Xij = Xli, Xjk = Xkl, and Xij is different than Xjk (i.e. i  k). Then
(3.14) = 1
N

ik,j
E[X2
ijX2
jk] = 1
N (N3 −N2)
σ 2
N
2
→σ 4
(3.18)
as N →∞.
(iii) Xij = Xkl, Xjk = Xli, and Xij is different than Xjk (i.e. i  k). Then we must have
i = l and j = k from Xij = Xkl, and i = j and k = l from Xjk = Xli. This gives a
contradiction: there are no such terms.
In sum, we obtain that
τ(X4) →σ 4 + σ 4 = 2σ 4
(3.19)
as N →∞, where the two terms come from the two non-crossing partitions, see
Figure 3.2.
In the next technical section, we generalize this calculation to arbitrary moments of
X. Odd moments are zero by symmetry. Even moments τ(X2k) can be written as sums
over non-crossing diagrams (non-crossing pair partitions of 2k elements), where each such
diagram contributes σ 2k. So
τ(X2k) = Ckσ 2k,
(3.20)
where Ck are Catalan numbers, the number of such non-crossing diagrams. They satisfy
Ck =
k

j=1
Cj−1Ck−j =
k−1

j=0
CjCk−j−1,
(3.21)
with C0 = C1 = 1, and can be written explicitly as
Ck =
1
k + 1
2k
k

,
(3.22)
see Section 3.2.3.
3.2.2 Catalan Numbers: Counting Non-Crossing Pair Partitions
We would like to calculate all moments of X. As written above, all the odd moments
τ(X2k+1) vanish (since the odd moments of a Gaussian random variable vanish). We only
need to compute the even moments:
τ(X2k) = 1
N E
&
Tr(X2k)
'
= 1
N

i1,...,i2k
E

Xi1i2Xi2i3 . . . Xi2ki1

.
(3.23)
Since we assume that the elements of X are Gaussian, we can expand the above
expectation value using Wick’s theorem using the covariance of the {Xij}’s. The matrix
X is symmetric, so we have to keep track of the fact that Xij is the same variable as Xji.
For this reason, using Wick’s theorem proves quite tedious and we will not follow this
route here.

38
More on Gaussian Matrices
From the Taylor series at inﬁnity of the Stieltjes transform, we expect every even
moment of X to converge to an O(1) number as N →∞. We will therefore drop any
O(1/N) or smaller term as we proceed. In particular the difference of variance between
diagonal and off-diagonal elements of X does not matter to ﬁrst order in 1/N.
In Eq. (3.23), each X entry must be equal to at least one another X entry, otherwise the
expectation is zero. On the other hand, it is easy to show that for the partitions that contain
at least one group with > 2 (actually ≥4) X entries that are equal to each other, their total
contribution will be of order O(1/N) or smaller (e.g. in case (1) of the previous section).
Thus we only need to consider the cases where each X entry is paired to exactly one other
X entry, which we also referred to as a pair partition.
We need to count the number of types of pairings of 2k elements that contribute to
τ(X2k) as N →∞. We associate to each pairing a diagram. For example, for k = 3, we
have 5! ! = 5 · 3 · 1 = 15 possible pairings (see Fig. 3.3).
To compute the contribution of each of these pair partitions, we will compute the
contribution of non-crossing pair partitions and argue that pair partitions with crossings
do not contribute in the large N limit. First we need to deﬁne what is a non-crossing pair
partition of 2k elements. A pair partition can be draw as a diagram where the 2k elements
are points on a line and each point is joined with its pair partner by an arc drawn above that
line. If at least two arcs cross each other the partition is called crossing, and non-crossing
otherwise. In Figure 3.3 the ﬁve partitions on the left are non-crossing while the ten others
are crossing.
In a non-crossing partition of size 2k, there is always at least one pairing between
consecutive points (the smallest arc). If we remove the ﬁrst such pairing we get a non-
crossing pair partition of 2k −2 elements. We can proceed in this way until we get to
a paring of only two elements: the unique (non-crossing) pair partition contributing to
(Fig. 3.4)
τ(X2) = σ 2.
(3.24)
Figure 3.3 Graphical representation of the 15 terms contributing to τ(X6). Only the ﬁve on the left
are non-crossing and have a non-zero contribution as N →∞.
Xij
Xji
Figure 3.4 Graphical representation of the only term contributing to τ(X2). Note that the indices of
two terms are already equal prior to pairing.

3.2 Moments and Non-Crossing Pair Partitions
39
Xiℓiℓ+1
Xiℓ+1iℓ+2
Xiℓ+2iℓ+3
Xiℓ+3iℓ+4
Figure 3.5 Zoom into the smallest arc of a non-crossing partition. The two middle matrices are paired
while the other two could be paired together or to other matrices to the left and right respectively.
After the pairing of Xiℓ+1,iℓ+2 and Xiℓ+2,iℓ+3, we have iℓ+1 = iℓ+3 and the index iℓ+2 is free.
We can use this argument to prove by induction that each non-crossing partition con-
tributes a factor σ 2k. In Figure 3.5, consecutive elements Xiℓ+1,iℓ+2 and Xiℓ+2,iℓ+3 are
paired; we want to evaluate that pair and remove it from the diagram. The variance con-
tributes a factor σ 2/N. We can make two choices for index matching. First consider
iℓ+1 = iℓ+3 and iℓ+2 = iℓ+2. In that case, the index iℓ+2 is free and its summation
contributes a factor of N. The identity iℓ+1 = iℓ+3 means that the previous matrix Xiℓ,iℓ+1
is now linked by matrix multiplication to the following matrix Xiℓ+1,iℓ+4. In other words
we are left with σ 2 times a non-crossing partition of size 2k −2, which contributes σ 2k−2
by our induction hypothesis. The other choice of index matching, iℓ+1 = iℓ+2 = iℓ+3,
can be viewed as ﬁxing a particular value for iℓ+2 and is included in the sum over iℓ+2
in the previous index matching. So by induction we do have that each non-crossing pair
partition contributes σ 2k.
Before we discuss the contribution of crossing pair partitions, let’s analyze in terms
of powers of N the computation we just did for the non-crossing case. The computation
of each term in τ(X2k) involves 2k matrices that have in total 4k indices. The trace and
the matrix multiplication forces 2k equalities among these indices. The normalization of
the trace and the k variance terms gives a factor of σ 2k/Nk+1. To get a result of order
1 we need to be left with k + 1 free indices whose summation gives a factor of Nk+1.
Each k pairing imposes a matching between pairs of indices. For the ﬁrst k −1 choice of
pairing we managed to match one pair of indices that were already equal. At the last step
we matched to pairs of indices that were already equal. Hence in total we added only k +1
equality constraints which left us with k + 1 free indices as needed.
We can now argue that crossing pair partitions do not contribute in the large N limit.
For crossing partition it is not possible to choose a matching at every step that matches
a pair of indices that are already equal. If we use the previous algorithm of removing at
each step the leftmost smallest arc, at some point, the smallest arc will have a crossing
and we will be pairing to matrices that share no indices, adding two equality constraints
at this step. The result will therefore be down by at least a factor of 1/N with respect to
the non-crossing case. This argument is not really a proof but an intuition why this might
be true.3
We can now complete our moments computation. Let
Ck := # of non-crossing pairings of 2k elements.
(3.25)
Since every non-crossing pair partition contributes a factor σ 2k, summing over all non-
crossing pairings we immediately get that
τ(X2k) = Ckσ 2k.
(3.26)
3 A more rigorous proof can be found in e.g. Anderson et al. [2010], Tao [2012] or Mingo and Speicher [2017]. In this last
reference, the authors compute the moments of X exactly for every N (when σ2
d = σ2
od).

40
More on Gaussian Matrices
1
2
2j −1
2j
2j + 1
2k
Figure 3.6 In a non-crossing pairing, the paring of site 1 with site 2j splits the graph into two disjoint
non-crossing parings.
3.2.3 Recursion Relation for Catalan Numbers
In order to compute the Catalan numbers Ck, we will write a recursion relation for
them. Take a non-crossing pairing, site 1 is linked to some even site 2j (it is easy to see
that 1 cannot link to an odd site in order for the partition to be non-crossing). Then the
diagram is split into two smaller non-crossing pairings of sizes 2(j −1) and 2(k −j),
respectively (see Fig. 3.6). Thus we get the inductive relation4
Ck =
k

j=1
Cj−1Ck−j =
k−1

j=0
CjCk−j−1,
(3.27)
where we let C0 = C1 = 1. One can then prove by induction that Ck is given by the
Catalan number:
Ck =
1
k + 1

2k
k

.
(3.28)
Using the Taylor series for the Stieltjes transform (2.22), we can use the Catalan number
recursion relation to ﬁnd an equation for the Stieltjes transform of the Wigner ensemble:
g(z) =
∞

k=0
Ck
z2k+1 σ 2k.
(3.29)
Thus, using (3.27), we obtain that
g(z) −1
z =
∞

k=1
σ 2k
z2k+1
⎛
⎝
k−1

j=0
CjCk−j−1
⎞
⎠
= σ 2
z
∞

j=0
Cj
z2j+1 σ 2j
⎛
⎝
∞

k=j+1
Ck−j−1
z2(k−j−1)+1 σ 2(k−j−1)
⎞
⎠
= σ 2
z
⎛
⎝
∞

j=0
Cj
z2j+1 σ 2j
⎞
⎠
⎛
⎝
∞

ℓ=0
Cℓ
z2ℓ+1 σ 2ℓ
⎞
⎠= σ 2
z g2(z),
(3.30)
which gives the same self-consistent equation for g(z) as in (2.35) and hence the same
solution:
g(z) = z −z
*
1 −4σ 2/z2
2σ 2
.
(3.31)
4 Interestingly, this recursion relation is also found in the problem of RNA folding. For deep connections between the physics of
RNA and rmt, see Orland and Zee [2002].

3.2 Moments and Non-Crossing Pair Partitions
41
The same result could have been derived by substituting the explicit solution for the
Catalan number Eq. (3.28) into (3.29), but this route requires knowledge of the Taylor
series:
√
1 −x = 1 −
∞

k=0
2
k + 1
2k
k
 x
4
k+1
.
(3.32)
Exercise 3.2.1
Non-crossing pair partitions of eight elements
(a)
Draw all the non-crossing pair partitions of eight elements. Hint: use the
recursion expressed in Figure 3.6.
(b)
If X is a unit Wigner matrix, what is τ(X8)?
Bibliographical Notes
• Again, several books cover the content of this chapter, see for example
– M. L. Mehta. Random Matrices. Academic Press, San Diego, 3rd edition, 2004,
– G. Blower. Random Matrices: High Dimensional Phenomena. Cambridge University
Press, Cambridge, 2009,
– B. Eynard, T. Kimura, and S. Ribault. Random matrices. preprint arXiv:1510.04430,
2006,
and in particular for a detailed discussion of the relation between non-crossing and large
matrices, see
– T. Tao. Topics in Random Matrix Theory. American Mathematical Society, Provi-
dence, Rhode Island, 2012,
– G. W. Anderson, A. Guionnet, and O. Zeitouni. An Introduction to Random Matrices.
Cambridge University Press, Cambridge, 2010,
– J. A. Mingo and R. Speicher. Free Probability and Random Matrices. Springer, New
York, 2017.
• Concerning the construction of generalized beta ensembles, see e.g.
– I. Dumitriu and A. Edelman. Matrix models for beta ensembles. Journal of Mathe-
matical Physics, 43(11):5830–5847, 2002,
– R. Allez, J. P. Bouchaud, and A. Guionnet. Invariant beta ensembles and the Gauss-
Wigner crossover. Physical Review Letters, 109(9):094102, 2012.
• On the Ginibre ensemble and the circular law, see
– V. L. Girko. Circular law. Theory of Probability and Its Applications, 29(4):694–706,
1985,
– H. J. Sommers, A. Crisanti, H. Sompolinsky, and Y. Stein. Spectrum of large random
asymmetric matrices. Physical Review Letters, 60(19):1895–1899, 1988,
– C. Bordenave and D. Chafa¨ı. Around the circular law. Probability Surveys, 9, 2012.

42
More on Gaussian Matrices
• On the connection between non-crossing diagrams and RNA folding, see
– P.-G. de Gennes. Statistics of branching and hairpin helices for the dAT copolymer.
Biopolymers, 6(5):715–729, 1968,
– H. Orland and A. Zee. RNA folding and large N matrix theory. Nuclear Physics B,
620(3):456–476, 2002.

4
Wishart Ensemble and Marˇcenko–Pastur Distribution
In this chapter we will study the statistical properties of large sample covariance matrices of
some N-dimensional variables observed T times. More precisely, the empirical set consists
of N × T data {xt
i }1≤i≤N,1≤t≤T , where we have T observations and each observation
contains N variables. Examples abound: we could consider the daily returns of N stocks,
over a certain time period, or the number of spikes ﬁred by N neurons during T consecutive
time intervals of length t, etc. Throughout this book, we will use the notation q for the
ratio N/T . When the number of observations is much larger than the number of variables,
one has q ≪1. If the number of observations is smaller than the number of variables (a
case that can easily happen in practice), then q > 1.
In the case where q →0, one can faithfully reconstruct the “true” (or population)
covariance matrix C of the N variables from empirical data. For q = O(1), on the other
hand, the empirical (or sample) covariance matrix is a strongly distorted version of C, even
in the limit of a large number of observations. This is not surprising since we are trying
to estimate O(N2/2) matrix elements from O(NT ) observations. In this chapter, we will
derive the well-known Marˇcenko–Pastur law for the eigenvalues of the sample covariance
matrix for arbitrary values of q, in the “white” case where the population covariance matrix
C is the identity matrix C = 1.
4.1 Wishart Matrices
4.1.1 Sample Covariance Matrices
We assume that the observed variables xt
i have zero mean. (Otherwise, we need to remove
the sample mean T −1 
t xt
i from xt
i for each i. For simplicity, we will not consider this
case.) Then the sample covariances of the data are given by
Eij = 1
T
T

t=1
xt
i xt
j.
(4.1)
Thus Eij form an N × N matrix E, called the sample covariance matrix (scm), which we
we write in a compact form as
43

44
Wishart Ensemble and Marˇcenko–Pastur Distribution
E = 1
T HHT,
(4.2)
where H is an N × T data matrix with entries Hit = xt
i .
The matrix E is symmetric and positive semi-deﬁnite:
E = ET,
and
vT Ev = (1/T )∥HT v∥2 ≥0,
(4.3)
for any v ∈RN. Thus E is diagonalizable and has all eigenvalues λE
k ≥0.
We can deﬁne another covariance matrix by transposing the data matrix H:
F = 1
N HT H.
(4.4)
The matrix F is a T ×T matrix, it is also symmetric and positive semi-deﬁnite. If the index
i (1 < i < N) labels the variables and the index t (1 < t < T ) the observations, we can
call the matrix F the covariance of the observations (as opposed to E the covariance of the
variables). Fts measures how similar the observations at t are to those at s – in the above
example of neurons, it would measure how similar is the ﬁring pattern at time t and at
time s.
As we saw in Section 1.1.3, the matrices T E and NF have the same non-zero eigenval-
ues. Also the matrix E has at least N −T zero eigenvalues if N > T (and F has at least
T −N zero eigenvalues if T > N).
Assume for a moment that N ≤T (i.e. q ≤1), then we know that F has N (zero or
non-zero) eigenvalues inherited from E and equal to q−1λE
k , and T −N zero eigenvalues.
This allows us to write an exact relation between the Stieltjes transforms of E and F:
gF
T (z) = 1
T
T

k=1
1
z −λF
k
= 1
T
 N

k=1
1
z −q−1λE
k
+ (T −N)
1
z −0

= q2gE
N(qz) + 1 −q
z
.
(4.5)
A similar argument with T < N leads to the same Eq. (4.5) so it is actually valid for any
value of q. The relationship should be true as well in the large N limit:
gF(z) = q2gE(qz) + 1 −q
z
.
(4.6)
4.1.2 First and Second Moments of a Wishart Matrix
We now study the scm E. Assume that the column vectors of H are drawn independently
from a multivariate Gaussian distribution with mean zero and “true” (or “population”)
covariance matrix C, i.e.
E[HitHjs] = Cijδts,
(4.7)

4.1 Wishart Matrices
45
with, again,
E = 1
T HHT .
(4.8)
Sample covariance matrices of this type were ﬁrst studied by the Scottish mathematician
John Wishart (1898–1956) and are now called Wishart matrices.
Recall that if (X1, . . . ,X2n) is a zero-mean multivariate normal random vector, then by
Wick’s theorem,
E[X1X2 · · · X2n] =

pairings
,
pairs
E[XiXj] =

pairings
,
pairs
Cov(Xi,Xj),
(4.9)
where 
pairings

pairs means that we sum over all distinct pairings of {X1, . . . ,X2n} and
each summand is the product of the n pairs.
First taking expectation, we obtain that
E[Eij] = 1
T E
-
T

t=1
HitHjt
.
= 1
T
T

t=1
Cij = Cij.
(4.10)
Thus, we have E[E] = C: as it is well known, the scm is an unbiased estimator of the true
covariance matrix (at least when E[xt
i ] = 0).
For the ﬂuctuations, we need to study the higher order moments of E. The second
moment can be calculated as
τ(E2) :=
1
NT 2 E

Tr(HHT HHT )

=
1
NT 2

i,j,t,s
E

HitHjtHjsHis

.
(4.11)
Then by Wick’s theorem, we have (see Fig. 4.1)
τ(E2) =
1
NT 2

t,s

i,j
C2
ij +
1
NT 2

t,s

i,j
CiiCjjδts +
1
NT 2

t,s

i,j
C2
ijδts
= τ(C2) + N
T τ(C)2 + 1
T τ(C2).
(4.12)
Suppose N,T →∞with some ﬁxed ratio N/T = q for some constant q > 0. The last
term on the right hand side then tends to zero and we get
Hit
Hjt
Hjs
His
Hit
Hjt
Hjs
His
Hit
Hjt
Hjs
His
Figure 4.1 Graphical representation of the three Wick’s contractions corresponding to the three terms
in Eq. (4.12).

46
Wishart Ensemble and Marˇcenko–Pastur Distribution
τ(E2) →τ(C2) + qτ(C)2.
(4.13)
The variance of the scm is greater than that of the true covariance by a term proportional to
q. When q →0 we recover perfect estimation and the two matrices have the same variance.
If C = α1 (a multiple of the identity) then τ(C2) −τ(C)2 = 0 but τ(E2) −τ(E)2 →qα2.
4.1.3 The Law of Wishart Matrices
Next, we give the joint distribution of elements of E. For each ﬁxed column of H, the joint
distribution of the elements is
P

{Hit}N
i=1

=
1
*
(2π)N det C
exp
⎡
⎣−1
2

i,j
Hit(C)−1
ij Hjt
⎤
⎦.
(4.14)
Taking the product over 1 ≤t ≤T (since the columns are independent), we obtain
P (H) =
1
(2π)
NT
2 det CT/2 exp
(
−1
2 Tr

HT C−1H
)
=
1
(2π)
NT
2 det CT/2 exp
(
−T
2 Tr

EC−1)
.
(4.15)
Let us now make a change in variables H →E. As shown in the technical paragraph 4.1.4,
the Jacobian of the transformation is proportional to (det E)
T −N−1
2
. The following exact
expression for the law of the matrix elements was obtained by Wishart:1
P (E) = (T /2)NT/2

N(T /2)
(det E)(T −N−1)/2
(det C)T/2
exp
(
−T
2 Tr

EC−1)
,
(4.16)
where 
N is the multivariate gamma function. Note that the density is restricted to positive
semi-deﬁnite matrices E. The Wishart distribution can be thought of as the matrix general-
ization of the gamma distribution. Indeed for N = 1, P(E) reduces to a such a distribution:
Pγ (x) =
ba

(a)xa−1e−bx,
(4.17)
where b = T /(2C) and a = T /2. Using the identity det E = exp(Tr log E), we can rewrite
the above expression as2
1 Note that the Wishart distribution is often given with the normalization E[E] = T C as opposed to E[E] = C used here.
2 Complex and quaternionic Hermitian white Wishart matrices have a similar law of the elements with a factor of β in the
exponential:
P (W) ∝exp
(
−βN
2
Tr V (W)
)
with V (x) = N −T −1 + 2/β
N
log x + T
N x,
(4.18)
with β equal to 1, 2 or 4 as usual. The large N limit V (x) is the same in all three cases and is given by Eq. (4.21).

4.1 Wishart Matrices
47
P (E) = (T /2)NT/2

N(T /2)
1
(det C)T/2 exp
(
−T
2 Tr

EC−1
+ T −N −1
2
Tr log E
)
.
(4.19)
We will denote by W a scm with C = 1 and call such a matrix a white Wishart matrix.
In this case, as N,T →∞with q := N/T , we get that
P (W) ∝exp
(
−N
2 Tr V (W)
)
,
(4.20)
where
V (W) := (1 −q−1) log W + q−1W.
(4.21)
Note that the above P(W) is rotationally invariant in the white case. In fact, if a vector
v has Gaussian distribution N(0,1N×N), then Ov has the same distribution N(0,1N×N)
for any orthogonal matrix O. Hence OH has the same distribution as H, which shows that
OEOT has the same distribution as E.
4.1.4 Jacobian of the Transformation H →E
The aim here is to compute the volume ϒ(E) corresponding to all H’s such that
E = T −1HHT :
ϒ(E) =

dH δ(E −T −1HHT ).
(4.22)
Note that this volume is the inverse of the Jacobian of the transformation H →E. Next
note that one can choose E to be diagonal, because one can always rotate the integral over
H to an integral over OH, where O is the rotation matrix that makes E diagonal. Now,
introducing the Fourier representation of the δ function for all N(N + 1)/2 independent
components of E, one has
ϒ(E) =

dHdA exp

i Tr(AE −T −1AHHT )

,
(4.23)
where A is the symmetric matrix of the corresponding Fourier variables, to which we add a
small imaginary part proportional to 1 to make all the following integrals well deﬁned. The
Gaussian integral over H can now be performed explicitly for all t = 1, . . . ,T , leading to

dH exp

−iT −1 Tr(AHHT )

∝(det A)−T/2,
(4.24)
leaving us with
ϒ(E) ∝

dA exp (i Tr(AE)) (det A)−T/2.
(4.25)
We can change variables from A to B = E
1
2 AE
1
2 . The Jacobian of this transformation is
,
i
dAii
,
j>i
dAij =
,
i
E−1
ii
,
j>i
(EiiEjj)−1
2
,
i
dBii
,
j>i
dBij
= (det(E))−N+1
2
,
i
dBii
,
j>i
dBij.
(4.26)

48
Wishart Ensemble and Marˇcenko–Pastur Distribution
So ﬁnally,
ϒ(E) ∝
(
dB exp (i Tr(B)) (det B)−T/2
)
(det(E))
T −N−1
2
,
(4.27)
as announced in the main text.
4.2 Marˇcenko–Pastur Using the Cavity Method
4.2.1 Self-Consistent Equation for the Resolvent
We ﬁrst derive the asymptotic distribution of eigenvalues of the Wishart matrix with C = 1,
i.e. the Marˇcenko–Pastur distribution. We will use the same method as in the derivation of
the Wigner semi-circle law in Section 2.3. In the case C = 1, the N × T matrix H is ﬁlled
with iid standard Gaussian random numbers and we have W = (1/T )HHT .
As in Section 2.3, we wish to derive a self-consistent equation satisﬁed by the Stieltjes
transform:
gW(z) = τ (GW(z)),
GW(z) := (z1 −W)−1.
(4.28)
We ﬁx a large N and ﬁrst write an equation for the element 11 of GW(z). We will argue
later that G11(z) converges to g(z) with negligible ﬂuctuations. (We henceforth drop the
subscript W as this entire section deals with the white Wishart case.)
Using again the Schur complement formula (1.32), we have that
1
(G(z))11
= M11 −M12(M22)−1M21,
(4.29)
where M := z1 −W, and the submatrices of size, respectively, [M11] = 1 × 1, [M12] =
1 × (N −1), [M21] = (N −1) × 1, [M22] = (N −1) × (N −1). We can expand the above
expression and write
1
(G(z))11
= z −W11 −1
T 2
T

t,s=1
N

j,k=2
H1tHjt(M22)−1
jk HksH1s.
(4.30)
Note that the three matrices M22, Hjt (j ≥2) and Hks (k ≥2) are independent of the
entries H1t for all t. We can write the last term on the right hand side as
1
T
N

t,s=1
H1t tsH1s
with
 ts := 1
T
N

j,k=2
Hjt(M22)−1
jk Hks.
(4.31)

4.2 Marˇcenko–Pastur Using the Cavity Method
49
Provided γ 2 := T −1 Tr  2 converges to a ﬁnite limit when T →∞,3 one readily shows
that the above sum converges to T −1 Tr  with ﬂuctuations of the order of γ T −1
2 . So we
have, for large T ,
1
(G(z))11
= z −W11 −1
T

2≤j,k≤N

t HktHjt
T
(M22)−1
jk + O

T −1
2

= z −W11 −1
T

2≤j,k≤N
Wkj(M22)−1
jk + O

T −1
2

= z −1 −1
T Tr W2G2(z) + O

T −1
2

,
(4.32)
where in the last step we have used the fact that W11 = 1 + O

T −1
2 
and noted W2 and
G2(z) the scm and resolvent of the N −1 variables excluding (1). We can rewrite the trace
term:
Tr(W2G2(z)) = Tr

W2(z1 −W2)−1
= −Tr 1 + z Tr

(z1 −W2)−1
= −Tr 1 + z Tr G2(z).
(4.33)
In the region where Tr G(z)/N converges for large N to the deterministic g(z), Tr G2(z)/N
should also converge to the same limit as G2(z) is just an (N −1) × (N −1) version of
G(z). So in the region of convergence we have
1
(G(z))11
= z −1 + q −qzg(z) + O

N−1
2

,
(4.34)
where we have introduced q = N/T = O(1), such that N−1
2 and T −1
2 are of the same
order of magnitude. This last equation states that 1/G11(z) has negligible ﬂuctuations and
can safely be replaced by its expectation value, i.e.
1
(G(z))11
= E
(
1
(G(z))11
)
+ O

N−1
2

=
1
E [(G(z))11] + O

N−1
2

.
(4.35)
By rotational invariance of W, we have
E[G(z)11] = 1
N E[Tr(G(z))] →g(z).
(4.36)
3 It can be self-consistently checked from the solution below that limT →∞γ 2 = −qg′
W(z).

50
Wishart Ensemble and Marˇcenko–Pastur Distribution
In the large N limit we obtain the following self-consistent equation for g(z):
1
g(z) = z −1 + q −qzg(z).
(4.37)
4.2.2 Solution and Density of Eigenvalues
Solving (4.37) we obtain
g(z) = z + q −1 ±
*
(z + q −1)2 −4qz
2qz
.
(4.38)
The argument of the square-root is quadratic in z and its roots (the edge of spectrum) are
given by
λ± = (1 ± √q)2.
(4.39)
Finding the correct branch is quite subtle, this will be the subject of Section 4.2.3. We will
see that the form
g(z) = z −(1 −q) −√z −λ+
√z −λ−
2qz
(4.40)
has all the correct analytical properties. Note that for z = x−iη with x  0 and η →0, g(z)
can only have an imaginary part if
*
(x −λ+)(x −λ−) is imaginary. Then using (2.47),
we get the famous Marˇcenko–Pastur distribution for the bulk:
ρ(x) = 1
π
lim
η→0+ Im g(x −iη) =
*
(λ+ −x)(x −λ−)
2πqx
,
λ−< x < λ+.
(4.41)
Moreover, by studying the behavior of Eq. (4.40) near z = 0 one sees that there is a pole
at 0 when q > 1. This gives a delta mass as z →0:
q −1
q
δ(x),
(4.42)
which corresponds to the N−T trivial zero eigenvalues of E in the N > T case. Combining
the above discussions, the full Marˇcenko–Pastur law can be written as
ρMP(x) =
/
(λ+ −x)(x −λ−)

+
2πqx
+ q −1
q
δ(x)(q −1),
(4.43)
where we denote [a]+ := max{a,0} for any a ∈R, and
(q −1) :=
0
0,
if q ≤1,
1
if q > 1.
(4.44)

4.2 Marˇcenko–Pastur Using the Cavity Method
51
0
1
2
3
4
5
6
l
0.0
0.2
0.4
0.6
0.8
1.0
r(l)
q = 1/ 2
q = 2
Figure 4.2 Marˇcenko–Pastur distribution: density of eigenvalues for a Wishart matrix for q = 1/2
and q = 2. Note that for q = 2 there is a Dirac mass at zero ( 1
2δ(λ)). Also note that the two bulk
densities are the same up to a rescaling and normalization ρ1/q(λ) = q2ρq(qλ).
Note that the Stieltjes transforms (Eq. (4.40)) for q and 1/q are related by Eq. (4.5).
As a consequence the bulk densities for q and 1/q are the same when properly rescaled
(see Fig. 4.2):
ρ1/q(λ) = q2ρq(qλ).
(4.45)
Exercise 4.2.1
Properties of the Marˇcenko–Pastur solution
We saw that the Stieltjes transform of a large Wishart matrix (with q = N/T )
should be given by
g(z) = z + q −1 ±
*
(z + q −1)2 −4qz
2qz
,
(4.46)
where the sign of the square-root should be chosen such that g(z) →1/z when
z →±∞.
(a)
Show that the zeros of the argument of the square-root are given by λ± =
(1 ± √q)2.
(b)
The function
g(z) = z + q −1 −
*
(z −λ−)
*
(z −λ+)
2qz
(4.47)
should have the right properties. Show that it behaves as g(z) →1/z when
z →±∞. By expanding in powers of 1/z up to 1/z3 compute the ﬁrst and
second moments of the Wishart distribution.

52
Wishart Ensemble and Marˇcenko–Pastur Distribution
(c)
Show that Eq. (4.47) is regular at z = 0 when q < 1. In that case, compute
the ﬁrst inverse moment of the Wishart matrix τ(E−1). What happens when
q →1? Show that Eq. (4.47) has a pole at z = 0 when q > 1 and compute
the value of this pole.
(d)
The non-zero eigenvalues should be distributed according to the Marˇcenko–
Pastur distribution:
ρq(x) =
*
(x −λ−)(λ+ −x)
2πqx
.
(4.48)
Show that this distribution is correctly normalized when q < 1 but not when
q > 1. Use what you know about the pole at z = 0 in that case to correctly
write down ρq(x) when q > 1.
(e)
In the case q = 1, Eq. (4.48) has an integrable singularity at x = 0. Write
a simpler formula for ρ1(x). Let u be the square of an eigenvalue from a
Wigner matrix of unit variance, i.e. u = y2 where y is distributed according
to the semi-circular law ρ(y) =
*
4 −y2/(2π). Show that u is distributed
according to ρ1(x). This result is a priori not obvious as a Wigner matrix is
symmetric while the square matrix H is generally not; nevertheless, moments
of high-dimensional matrices of the form HHT are the same whether the
matrix H is symmetric or not.
(f)
Generate three matrices E = HHT /T where the matrix H is an N × T
matrix of iid Gaussian numbers of variance 1. Choose a large N and three
values of T such that q = N/T equals {1/2,1,2}. Plot a normalized histogram
of the eigenvalues in the three cases vs the corresponding Marˇcenko–Pastur
distribution; don’t show the peak at zero. In the case q = 2, how many zero
eigenvalues do you expect? How many do you get?
4.2.3 The Correct Root of the Stieltjes Transform
In our study of random matrices we will often encounter limiting Stieltjes transforms
that are determined by quadratic or higher order polynomial equations, and the problem
of choosing the correct solution (or branch) will come up repeatedly.
Let us go back to the unit Wigner matrix case where we found (see Section 2.3.2)
g(z) = z ±
*
z2 −4
2
.
(4.49)
On the one hand we want g(z) that behaves like 1/z as |z| →∞and we want the solution
to be analytical everywhere but on the real axis in [−2,2]. The square-root term must
thus behave as −z for real z when z →±∞. The standard deﬁnition of the square-root
behaves as
*
z2 ∼|z| and cannot be made to have the correct sign on both sides. Another
issue with
*
z2 −4 is that it has a more extended branch cut than allowed. We expect the

4.2 Marˇcenko–Pastur Using the Cavity Method
53
z ∈C
branch point
branch cut
present in
Stieltjes
branch cut
absent in
Stieltjes
−2
2
f (z) →z
f (z) →−z
Figure 4.3 The branch cuts of f (z) =
*
z2 −4. The vertical branch cut (for z pure imaginary)
should not be present in the Stieltjes transform of the Wigner ensemble. We have f (±0+ + ix) ≈
±i
*
x2 + 4; this branch cut can be eliminated by multiplying f (z) by sign(Re z).
function g(z) to be analytic everywhere except for real z ∈[−2,2]. The branch cut of a
standard square-root is a set of points where its argument is real and negative. In the case
of
*
z2 −4, this includes the interval [−2,2] as expected but also the pure imaginary line
z = ix (Fig. 4.3). The ﬁnite N Stieltjes transform is perfectly regular on the imaginary
axis so we expect its large N to be regular there as well.
For the unit Wigner matrix, there are at least three solutions to the branch problem:
g1(z) =
z −z
/
1 −4
z2
2
,
(4.50)
g2(z) = z −
√
z −2
√
z + 2
2
,
(4.51)
g3(z) = z −sign(Re z)
*
z2 −4
2
.
(4.52)
All three deﬁnitions behave as g(z) ∼1/z at inﬁnity. For the second one, we need to deﬁne
the square-root of a negative real number. If we deﬁne it as i √|z|, the two factors of i give
a −1 for real z < −2. The three functions also have the correct branch cuts. For the ﬁrst
one, one can show that the argument of the square-root can be a negative real number only
if z ∈(−2,2), there are no branch cuts elsewhere in the complex plane. For the second
one, there seems to be a branch cut for all real z < 2, but a closer inspection reveals
that around real z < −2 the function has no discontinuity as one goes up and down the
imaginary axis, as the two branch cuts exactly compensate each other. For the third one,
the discontinuous sign function exactly cancels the branch cut on the pure imaginary axis
(Fig. 4.3).

54
Wishart Ensemble and Marˇcenko–Pastur Distribution
For z with a large positive real part the three functions are clearly the same. Since they
are analytic functions everywhere except on the same branch cuts, they are the same and
unique function g(z).
For a Wigner matrix shifted by λ0 and of variance σ 2 we can scale and shift the
eigenvalues, now equal λ± = λ0 ± 2σ and ﬁnd
g1(z) =
z −λ0 −(z −λ0)
1
1 −
4σ 2
(z−λ0)2
2σ
,
(4.53)
g2(z) = z −λ0 −√z −λ+
√z −λ−
2σ
,
(4.54)
g3(z) = z −λ0 −sign(Re z −λ0)
*
(z −λ0)2 −4σ 2
2σ
.
(4.55)
The three deﬁnitions are still equivalent as they are just the result of a shift and a scaling
of the same function.
For more complicated problems, writing explicitly any one of these three prescriptions
can quickly become very cumbersome (except maybe in cases where λ+ + λ−= 0). We
propose here a new notation. When ﬁnding the correct square-root of a second degree
polynomial we will write
±
⃝*
az2 + bz + c : = √a
*
z −λ+
*
z −λ−
= √a(z −λ0)
2
1 −

(z −λ0)2
= sign(Re z −λ0)
*
az2 + bz + c,
(4.56)
for a > 0 and where λ± = λ0 ±
√
 are the roots of az2 + bz + c assumed to be real.
While the notation is deﬁned everywhere in the complex plane, it is easily evaluated for
real arguments:
±
⃝*
ax2 + bx + c =
0
−
*
ax2 + bx + c
for x ≤λ−,
*
ax2 + bx + c
for x ≥λ+.
(4.57)
The value on the branch cut is ill-deﬁned but we have
lim
z→x−i0+
±
⃝*
az2 + bz + c = i
/
|ax2 + bx + c|
for λ−< x < λ+.
(4.58)
With our new notation, we can now safely write for the white Wishart:
g(z) = z + q −1 −
±
⃝*
(z + q −1)2 −4qz
2qz
,
(4.59)
or, more explicitly, using the second prescription:
g(z) = z + q −1 −√z −λ+
√z −λ−
2qz
,
(4.60)
where λ± = (1 ± √q)2.

4.2 Marˇcenko–Pastur Using the Cavity Method
55
Exercise 4.2.2
Finding the correct root
(a)
For the unit Wigner Stieltjes transform show that regardless of choice of sign
in Eq. (4.49) the point z = 2i is located on a branch cut and the function is
discontinuous at that point.
(b)
Compute the value of Eqs. (4.50), (4.51) and (4.52) at z = 2i. Hint: for g2(z)
write −2 + 2i =
√
8e3iπ/4 and similarly for 2 + 2i. The deﬁnition g3(z) is
ambiguous for z = 2i, compute the limiting value on both sides: z = 0+ + 2i
and z = 0−+ 2i.
4.2.4 General (Non-White) Wishart Matrices
Recall our deﬁnition of a Wishart matrix from Section 4.1.2: a Wishart matrix is a matrix
EC deﬁned as
EC = 1
T HCHT
C,
(4.61)
where HC is an N ×T rectangular matrix with independent columns. Each column is a ran-
dom Gaussian vector with covariance matrix C; EC corresponds to the sample (empirical)
covariance matrix of variables characterized by a population (true) covariance matrix C.
To understand the case where the true matrix C is different from the identity we ﬁrst
discuss how to generate a multivariate Gaussian vector with covariance matrix C. We
diagonalize C as
C = OOT,
 =
⎛
⎜⎝
σ 2
1
...
σ 2
N
⎞
⎟⎠.
(4.62)
The square-root of C can be deﬁned as4
C
1
2 = O
1
2 OT,

1
2 =
⎛
⎜⎝
σ1
...
σN
⎞
⎟⎠.
(4.63)
We now generate N iid unit Gaussian random variables xi, 1 ≤i ≤N, which form a
random column vector x with entries xi. Then we can generate the vector y = C
1
2 x. We
claim that y is a multivariate Gaussian vector with covariance matrix C. In fact, y is a linear
combination of multivariate Gaussians, so it must itself be multivariate Gaussian. On the
other hand, we have, using E[xxT ] = 1,
4 This is the canonical deﬁnition of the square-root of a matrix, but this deﬁnition is not unique – see the technical paragraph
below.

56
Wishart Ensemble and Marˇcenko–Pastur Distribution
E(yyT ) = E(C
1
2 xxT C
1
2 ) = C.
(4.64)
By repeating the argument above for every column of HC, t = 1, . . . ,T , we see that this
matrix can be written as HC = C
1
2 H, with H a rectangular matrix with iid unit Gaussian
entries. The matrix EC is then equivalent to
EC = 1
T HCHT
C = 1
T C
1
2 HHT C
1
2 = C
1
2 WqC
1
2,
(4.65)
where Wq = 1
T HHT is a white Wishart matrix with q = N/T .
We will see later that the above combination of matrices is called the free product of C
and W. Free probability will allow us to compute the resolvent and the spectrum in the case
of a general C matrix.
The variables x deﬁned above are called the “whitened” version of y. If a zero mean
random vector y has positive deﬁnite covariance matrix C, we can deﬁne a whitening of
y as a linear combination x = My such that E[xxT ] = 1. One can show that the matrix
M satisﬁes MT M = C−1 and has to be of the form M = OC−1
2 , where O can be
any orthogonal matrix and C
1
2 the symmetric square-root of C deﬁned above. Since O is
arbitrary the procedure is not unique, which leads to three interesting choices for whitened
varaibles:
• Perhaps the most natural one is the symmetric or Mahalanobis whitening where M =
C−1
2 . In addition to being the only whitening scheme with a symmetric matrix M, the
white variables x = C−1
2 y are the closest to y in the following sense: the distance
||x −y||Cα := E Tr

(x −y)T Cα(x −y)

(4.66)
is minimal over all other choices of O for any α. The case α = −1 is called the
Mahalanobis norm.
• Triangular or Gram–Schmidt whitening where the vector x can be constructed using the
Gram–Schmidt orthonormalization procedure. If one starts from the bottom with xN =
yN/ √CNN, then the matrix M is upper triangular. The matrix M can be computed
efﬁciently using the Cholesky decomposition of C−1. The Cholesky decomposition of
a symmetric positive deﬁnite matrix A amounts to ﬁnding a lower triangular matrix L
such that LLT = A. In the present case, A = C−1 and the matrix M we are looking for
is given by
M = LT .
(4.67)
This scheme has the advantage that the whitened variable xk only depends on physical
variables yℓfor ℓ≥k. In ﬁnance, for example, this allows one to construct whitened
returns of a given stock using only the returns of itself and those of (say) more liquid
stocks.
• Eigenvalue (or pca) whitening where O corresponds to the eigenbasis of C, i.e. such
that C = OOT where  is diagonal. M is then computed as M = −1
2 OT . The
whitened variables x are then called the normalized principal components of y.

4.2 Marˇcenko–Pastur Using the Cavity Method
57
Bibliographical Notes
• For a historical perspective on Wishart matrices and the Marˇcenko–Pastur, see
– J. Wishart. The generalised product moment distribution in samples from a normal
multivariate population. Biometrika, 20A(1-2):32–52, 1928,
– V. A. Marchenko and L. A. Pastur. Distribution of eigenvalues for some sets of
random matrices. Matematicheskii Sbornik, 114(4):507–536, 1967.
• For more recent material on the content of this chapter, see e.g.
– L. Pastur and M. Scherbina. Eigenvalue Distribution of Large Random Matrices.
American Mathematical Society, Providence, Rhode Island, 2010,
– Z. Bai and J. W. Silverstein. Spectral Analysis of Large Dimensional Random Matri-
ces. Springer-Verlag, New York, 2010,
– A. M. Tulino and S. Verd´u. Random Matrix Theory and Wireless Communications.
Now publishers, Hanover, Mass., 2004,
– R. Couillet and M. Debbah. Random Matrix Methods for Wireless Communications.
Cambridge University Press, Cambridge, 2011,
– G. Livan, M. Novaes, and P. Vivo. Introduction to Random Matrices: Theory and
Practice. Springer, New York, 2018,
and also
– J. W. Silverstein and S.-I. Choi. Analysis of the limiting spectral distribution of large
dimensional random matrices. Journal of Multivariate Analysis, 54(2):295–309,
1995,
– A. Sengupta and P. P. Mitra. Distributions of singular values for some random matri-
ces. Physical Review E, 60(3):3389, 1999.
• A historical remark on the Cholesky decomposition: Andr´e-Louis Cholesky served in
the French military as an artillery ofﬁcer and was killed in battle a few months before
the end of World War I; his discovery was published posthumously by his fellow ofﬁcer
Commandant Benoˆıt in the Bulletin G´eod´esique (Wikipedia).

5
Joint Distribution of Eigenvalues
In the previous chapters, we have studied the moments, the Stieltjes transform and the
eigenvalue density of two classical ensembles (Wigner and Wishart). These quantities in
fact relate to single eigenvalue properties of these ensembles. By this we mean that the
Stieltjes transform and the eigenvalue density are completely determined by the univariate
law of eigenvalues but they do not tell us anything about the correlations between different
eigenvalues.
In this chapter we will extend these results in two directions. First we will consider a
larger class of rotationally invariant (or orthogonal) ensembles that contains Wigner and
Wishart. Second we will study the joint law of all eigenvalues. In these models, the eigen-
values turn out to be strongly correlated and can be thought of as “particles” interacting
through pairwise repulsion.
5.1 From Matrix Elements to Eigenvalues
5.1.1 Matrix Potential
Consider real symmetric random matrices M whose elements are distributed as the expo-
nential of the trace of a certain matrix function V (M), often called a potential by analogy
with statistical physics:1
P(M) = Z−1
N exp
$
−N
2 Tr V (M)
%
,
(5.1)
where ZN is a normalization constant. These matrix ensembles are called orthogonal
ensembles for they are rotationally invariant, i.e. invariant under orthogonal transforma-
tions.2 For the Wigner ensemble, for example, we have (see Chapter 2)
1 V (M) is a matrix function best deﬁned in the eigenbasis of M through a transformation of all its eigenvalues through a
function of a scalar, V (x), see Section 1.2.6.
2 The results of this chapter extend to Hermitian (β = 2) or quarternion-Hermitian (β = 4) matrices with the simple
introduction of a factor β in the probability distribution:
P (M) ∝exp
$
−βN
2
Tr V (M)
%
,
(5.2)
58

5.1 From Matrix Elements to Eigenvalues
59
0
1
2
3
4
x
−2
0
2
4
6
V(x)
q = 1/ 2
q = 2
Figure 5.1 The Wishart matrix potential (Eq. (5.4)) for q = 1/2 and q = 2. The integration over
positive semi-deﬁnite matrices imposes that the eigenvalues must be greater than or equal to zero.
For q < 1 the potential naturally ensures that the eigenvalues are greater than zero and the constraint
will not be explicitly needed in the computation. For q ≥1, the constraint is needed to obtain a
sensible result.
V (x) = x2
2σ 2,
(5.3)
whereas the Wishart ensemble (at large N) is characterized by (see Chapter 4)
V (x) = x + (q −1) log x
q
(5.4)
(see Fig. 5.1). We can also consider other matrix potentials, e.g.
V (x) = x2
2 + gx4
4 .
(5.5)
Note that Tr V (M) depends only on the eigenvalues of M. We would like thus to write
down the joint distribution of these eigenvalues alone. The key is to ﬁnd the Jacobian of the
change of variables from the entries of M to the eigenvalues {λ1, . . . ,λN}.
5.1.2 Matrix Jacobian
Before computing the Jacobian of the transformation from matrix elements to eigenvalues
and eigenvectors (or orthogonal matrices), let us count the number of variables in both
parameterizations. Suppose M can be diagonalized as
M = OOT .
(5.6)
this factor will match the factor of β from the Vandermonde determinant. These two other ensembles are called unitary
ensembles and symplectic ensembles, respectively. Collectively they are called the beta ensembles.

60
Joint Distribution of Eigenvalues
The symmetric matrix M has N(N + 1)/2 independent variables, and  has N inde-
pendent variables as a diagonal matrix. To ﬁnd the number of independent variables in
O we ﬁrst realize that OOT = 1 is an equation between two symmetric matrices and
thus imposes N(N + 1)/2 constraints out of N2 potential values for the elements of O,
therefore O has N(N −1)/2 independent variables. In total, we thus have N(N + 1)/2 =
N + N(N −1)/2.
The change of variables from M to (,O) will introduce a factor | det()|, where
 := (M) =
(∂M
∂, ∂M
∂O
)
(5.7)
is the Jacobian matrix of dimension N(N + 1)/2 × N(N + 1)/2.
First, let us establish the scaling properties of the Jacobian. We assume that the matrix
elements of M have some dimension [M] = d (say centimeters). Using dimensional
analysis, we thus have
[DM] = dN(N+1)/2,
[D] = dN,
[DO] = d0,
(5.8)
since rotations are dimensionless. Hence we must have
[| det()|] ∼dN(N−1)/2,
(5.9)
which has the dimension of an eigenvalue raised to the power N(N −1)/2, the number of
distinct off-diagonal elements in M.
We now compute this Jacobian exactly. First, notice that the Jacobian relates the volume
“around” (,O) when  and O change by inﬁnitesimal amounts, to the volume “around”
M when its elements change by inﬁnitesimal amounts. We note that volumes are invariant
under rotations, so in order to compute the inﬁnitesimal volume we can choose the rotation
matrix O to be the identity matrix, which amounts to saying that we work in the basis where
M is diagonal. Another way to see this is to note that the orthogonal transformation
M →UT MU;
UT U = 1
(5.10)
has a Jacobian equal to 1, see Section 1.2.7. One can always choose U such that M is
diagonal.
5.1.3 Inﬁnitesimal Rotations
For rotations O near the identity, we set
O = 1 + ϵ δO,
(5.11)
where ϵ is a small number and δO is some matrix. From the identity
1 = OOT = 1 + ϵ(δO + δOT ) + ϵ2δOδOT,
(5.12)

5.1 From Matrix Elements to Eigenvalues
61
we get δO = −δOT by comparing terms of the ﬁrst order in ϵ, i.e. δO is skew-symmetric.3
A convenient basis to write such inﬁnitesimal rotations is
ϵ δO =

1≤k<l≤N
θklA(kl),
(5.13)
where A(kl) are the elementary skew-symmetric matrices such that A(kl) has only two non-
zero elements: [A(kl)]kl = 1 and [A(kl)]lk = −1:
A(kl) =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
0
. . .
. . .
0
...
...
1
...
−1
...
...
...
0
. . .
. . .
0
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
(5.14)
An inﬁnitesimal rotation is therefore fully described by N(N −1)/2 generalized “angles”
θkl.
5.1.4 Vandermonde Determinant
Now, in the neighborhood of (,O = 1), the matrix M + δM can be parameterized as
M + δM ≈
⎛
⎝1 +

k,l
θklA(kl)
⎞
⎠( + δ)
⎛
⎝1 −

k,l
θklA(kl)
⎞
⎠.
(5.15)
So to ﬁrst order in δ and θkl,
δM ≈δ +

k,l
θkl
&
A(kl) −A(kl)'
.
(5.16)
Using this local parameterization, we can compute the Jacobian matrix and ﬁnd its deter-
minant. For the diagonal contribution, we have
∂Mij
∂nn
= δinδjn,
(5.17)
i.e. perturbing a given eigenvalue only changes the corresponding diagonal element with
slope 1.
For the rotation contribution, one has, for k < l and i < j,
3 The reader familiar with the analysis of compact Lie groups will recognize the statement that skew-symmetric matrices form
the Lie algebra of O(N).

62
Joint Distribution of Eigenvalues
∂Mij
∂θkl
=

A(kl) −A(kl)
ij =
0
λl −λk,
if i = k,j = l,
0,
otherwise.
(5.18)
i.e. an inﬁnitesimal rotation in the direction kl modiﬁes only one distinct off-diagonal
element (Mkl ≡Mlk) with slope λl −λk. In particular, if two eigenvalues are the same
(λk = λl) a rotation of the eigenvectors in that subspace has no effect on the matrix M. This
is expected since eigenvectors in a degenerate subspace are only deﬁned up to a rotation
within that subspace.
Finally, the N(N + 1)/2 × N(N + 1)/2 determinant has its ﬁrst N diagonal elements
equal to unity, and the next N(N −1)/2 are equal to all possible pair differences λi −λj.
Hence,
(M) = det
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
1
...
1
λ2 −λ1
λ3 −λ1
...
λN −λN−1
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
=
,
k<ℓ
(λℓ−λk).
(5.19)
The absolute value of  is then given by
|(M)| =
,
k<ℓ
|λℓ−λk|.
(5.20)
We can check that this result has the expected dimension dN(N−1)/2, since the product
contains exactly N(N −1)/2 terms. The determinant (M) is called the Vandermonde
determinant as it is equal to the determinant of the following N × N Vandermonde matrix:
⎛
⎜⎜⎜⎜⎜⎜⎝
1
1
1
. . .
1
λ1
λ2
λ3
. . .
λN
λ2
1
λ2
2
λ2
3
. . .
λ2
N
...
...
...
...
...
λN−1
1
λN−1
2
λN−1
3
. . .
λN−1
N
⎞
⎟⎟⎟⎟⎟⎟⎠
.
(5.21)
Since the above Jacobian has no dependence on the matrix O, we can integrate out the
rotation part of (5.1) to get the joint distribution of eigenvalues:
P({λi}) ∝
,
k<l
|λk −λl| exp
0
−N
2
N

i=1
V (λi)
3
.
(5.22)

5.1 From Matrix Elements to Eigenvalues
63
A key feature of the above probability density is that the eigenvalues are not independent,
since the term 
k<l |λk −λl| indicates that the probability density vanishes when
two eigenvalues tend towards one another. This can be interpreted as some effective
“repulsion” between eigenvalues, as we will expand on now using an analogy with
Coulomb gases.
Exercise 5.1.1
Vandermonde determinant for 2 × 2 matrices
In this exercise we will explicitly compute the Vandermonde determinant for
2 × 2 matrices. We deﬁne O and  as
O =
 cos(θ)
sin(θ)
−sin(θ)
cos(θ)

and  =
λ1
0
0
λ2

.
(5.23)
Then any 2 × 2 symmetric matrix can be written as M = OOT .
(a)
Write explicitly M11, M12 and M22 as a function of λ1, λ2 and θ.
(b)
Compute the 3 × 3 matrix  of partial derivatives of M11, M12 and M22 with
respect to λ1, λ2 and θ.
(c)
In the special cases where θ equals 0, π/4 and π/2 show that | det | =
|λ1 −λ2|. If you have the courage show that | det | = |λ1 −λ2| for all θ.
Exercise 5.1.2
Wigner surmise
Wigner was interested in the distribution of energy level spacings in heavy
nuclei, which he modeled as the eigenvalues of a real symmetric random
matrix (time reversal symmetry imposes that the Hamiltonian be real). Let
x = |λk+1 −λk| for k in the bulk. In principle we can obtain the probability
density of x by using Eq. (5.22) and integrating out all other variables. In practice
it is very difﬁcult to go much beyond N = 2. Since the N = 2 result (properly
normalized) has the correct small x and large x behavior, Wigner surmised that
it must be a good approximation at any N.
(a)
For an N = 2 goe matrix, i.e. V (λ) = λ2/2σ 2, write the unnormalized law
of λ1 and λ2, its two eigenvalues.
(b)
Change variables to λ± = λ2 ± λ1, integrate out λ+ and write the unnormal-
ized law of x = |λ−|.
(c)
Normalize your law and choose σ such that E[x] = 1; you should ﬁnd
P(x) = π
2 x exp
&
−π
4 x2'
.
(5.24)
(d)
Using Eq. (5.26) redo the computation for gue (β = 2). You should ﬁnd
P(x) = 32
π2 x2 exp
(
−4
π x2
)
.
(5.25)

64
Joint Distribution of Eigenvalues
5.2 Coulomb Gas and Maximum Likelihood Conﬁgurations
5.2.1 A Coulomb Gas Analogy
The orthogonal ensemble deﬁned in the previous section can be generalized to complex
or quaternion Hermitian matrices. The corresponding joint distribution of eigenvalues is
simply obtained by adding a factor of β (equal to 1, 2 or 4) to both the potential and the
Vandermonde determinant:
P({λi}) = Z−1
N
-,
k<l
|λk −λl|β
.
exp
0
−β
2
- N

i=1
NV (λi)
.3
= Z−1
N exp
⎧
⎪⎪⎨
⎪⎪⎩
−β
2
⎡
⎢⎢⎣
N

i=1
NV (λi) −
N

i,j=1
ji
log |λi −λj|
⎤
⎥⎥⎦
⎫
⎪⎪⎬
⎪⎪⎭
.
(5.26)
This joint law is exactly the Gibbs–Boltzmann factor (e−E/T ) for a gas of N particles
moving on a one-dimensional line, at temperature T = 2/β, whose potential energy is
given by NV (x) and that interact with each other via a pairwise repulsive force generated
by the potential VR(x,y) = −log(|x −y|). Formally, the repulsive term happens to be the
Coulomb potential in two dimensions for particles that all have the same sign. In a truly
one-dimensional problem, the Coulomb potential would read V1d(x,y) = −|x −y|, but
with a slight abuse of language one speaks about the eigenvalues of a random matrix as a
Coulomb gas (in one dimension).
Even though we are interested in one particular value of β (namely β = 1), we can build
an intuition by considering this system at various temperatures. At very low temperature
(i.e. β →∞), the N particles all want to minimize their potential energy and sit at the
minimum of NV (x), but if they try to do so they will have to pay a high price in interaction
energy as this energy increases as the particles get close to one another. The particles will
have to spread themselves around the minimum of NV (x) to minimize the sum of the
potential and interaction energy and ﬁnd the conﬁguration corresponding to “mechanical
equilibrium”, i.e. such that the total force on each particle is zero. At non-zero temperature
(ﬁnite β) the particles will ﬂuctuate around this equilibrium solution. Since the repulsion
energy diverges as any two eigenvalues get inﬁnitely close, the particles will always avoid
each other. Figure 5.2 shows a typical conﬁguration of particles/eigenvalues for N = 20 at
β = 1 in a quadratic potential (goe matrix).
In the next section, we will study this equilibrium solution, which is exact at low tem-
perature β →∞or when N →∞, and is the maximum likelihood solution at ﬁnite β and
ﬁnite N.
5.2.2 Maximum Likelihood Conﬁguration and Stieltjes Transform
In the previous section, we saw that in the Coulomb gas analogy, β →∞corresponds to
the zero temperature limit, and that in this limit the eigenvalues freeze to the minimum of

5.2 Coulomb Gas and Maximum Likelihood Conﬁgurations
65
−2
−1
0
1
2
x
0.0
0.5
1.0
1.5
2.0
V(x)
V(x)
N = 20
Figure 5.2 Representation of a typical N = 20 goe matrix as a Coulomb gas. The full curve
represents the potential V (x) = x2/2 and the 20 dots, the positions of the eigenvalues of a typical
conﬁguration. In this analogy, the eigenvalues feel a potential NV (x) and a repulsive pairwise
interaction V (x,y) = −log(|x −y|). They ﬂuctuate according to the Boltzmann weight e−βE/2
with β = 1 in the present case.
the energy (potential plus interaction). We will argue that this freezing (or concentration of
the equilibrium measure) also happens when N →∞for ﬁxed β.
Let us study the minimum energy conﬁguration. We can rewrite Eq. (5.26) as
P({λi}) ∝e
1
2 βNL({λi}),
L({λi}) = −
N

i=1
V (λi) + 1
N
N

i,j=1
ji
log |λi −λj|,
(5.27)
where L stands for “log-likelihood”. For ﬁnite N and ﬁnite β, we can still consider the
solution that maximizes L({λi}). This is the maximum likelihood solution, i.e. the conﬁg-
uration of {λi} that has maximum probability. The maximum of L is determined by the
equations
∂L
∂λi
= 0 ⇒V ′(λi) = 2
N
N

j=1
ji
1
λi −λj
.
(5.28)
These are N coupled equations of N variables which can get very tedious to solve even
for moderate values of N. In Exercise 5.2.1 we will ﬁnd the solution for N = 3 in
the Wigner case. The solution of these equations is the set of equilibrium positions of
all the eigenvalues, i.e. the set of eigenvalues that maximizes the joint probability. To
characterize this solution (which will allow us to obtain the density of eigenvalues), we
will compute the Stieltjes transform of the {λi} satisfying Eq. (5.28). The trick is to make
algebraic manipulations to both sides of the equation to make the Stieltjes transform
explicitly appear.

66
Joint Distribution of Eigenvalues
In a ﬁrst step we multiply both sides of Eq. (5.28) by 1/(z −λi) where z is a complex
variable not equal to any eigenvalues, and then we sum over the index i. This gives
1
N
N

i=1
V ′(λi)
z −λi
= 2
N2
N

i,j=1
ji
1

λi −λj

(z −λi)
= 1
N2
⎡
⎢⎢⎣
N

i,j=1
ji
1

λi −λj

(z −λi) +
N

i,j=1
ji
1

λj −λi

(z −λj)
⎤
⎥⎥⎦
= 1
N2
N

i,j=1
ji
1
(z −λi) (z −λj) = g2
N(z) −1
N2
N

i=1
1
(z −λi)2
= g2
N(z) + g′
N(z)
N
,
(5.29)
where gN(z) is the Stieltjes transform at ﬁnite N:
gN(z) := 1
N
N

i=1
1
z −λi
.
(5.30)
We still need to handle the left hand side of the above equation. First we add and subtract
V ′(z) on the numerator, yielding
1
N
N

i=1
V ′(λi)
z −λi
= V ′(z)gN(z) −1
N
N

i=1
V ′(z) −V ′(λi)
z −λi
= V ′(z)gN(z) −N(z), (5.31)
where we have deﬁned a new function N(z) as
N(z) := 1
N
N

i=1
V ′(z) −V ′(λi)
z −λi
.
(5.32)
This does not look very useful as the equation for gN(z) will depend on some unknown
function N(z) that depends on the eigenvalues whose statistics we are trying to determine.
The key realization is that if V ′(z) is a polynomial of degree k then N(z) is also a
polynomial and it has degree k −1. Indeed, for each i in the sum, V ′(z) −V ′(λi) is a
degree k polynomial having z = λi as a zero, so (V ′(z) −V ′(λi))/(z −λi) is a polynomial
of degree k −1. N(z) is the sum of such polynomials so is itself a polynomial of degree
k −1.
In fact, the argument is easy to generalize to the Laurent polynomials, i.e. such that
zkV ′(z) is a polynomial for some k ∈N. For example, in the Wishart case we have a
Laurent polynomial
V ′(z) = 1
q

1 + q −1
z

.
(5.33)

5.2 Coulomb Gas and Maximum Likelihood Conﬁgurations
67
Nevertheless, from now on we make the assumption that V ′(z) is a polynomial. We will
later discuss how to relax this assumption.
Thus we get from Eq. (5.29) that
V ′(z)gN(z) −N(z) = g2
N(z) + g′
N(z)
N
(5.34)
for some polynomial N of degree deg(V ′(z)) −1, which needs to be determined self-
consistently using Eq. (5.32). For a given V ′(z), the coefﬁcients of N are related to the
moments of the {λi}, which themselves can be obtained from expanding gN(z) around
inﬁnity. In some cases, Eq. (5.34) can be solved exactly at ﬁnite N, for example in the case
where V (z) = z2/2, in which case the solution can be expressed in terms of Hermite poly-
nomials – see Chapter 6. In the present chapter we will study this equation in the large N
limit, which will allow us to derive a general formula for the limiting density of eigenvalues.
Note that since Eq. (5.34) does not depend on the value of β, the corresponding eigenvalue
density will also be independent of β.
Exercise 5.2.1
Maximum likelihood for 3 × 3 Wigner matrices
In this exercise we will write explicitly the three eigenvalues of the maximum
likelihood conﬁguration of a 3 × 3 goe matrix. The potential for this ensemble
is V (x) = x2/2.
(a)
Let λ1,λ2,λ3 be the three maximum likelihood eigenvalues of the 3 × 3 goe
ensemble in decreasing order. By symmetry we expect λ3 = −λ1. What do
you expect for λ2?
(b)
Consider Eq. (5.28). Assuming (λ3 = −λ1), check that your guess for λ2 is
indeed a solution. Now write the equation for λ1 and solve it.
(c)
Using your solution and the deﬁnition (5.30), show that the Stieltjes transform
of the maximum likelihood conﬁguration is given by
g3(z) = z2 −1
3
z3 −z .
(5.35)
(d)
In the simple case V (x) = x2/2, the zero-degree polynomial N(z) is just a
constant (independent of N) that can be evaluated from the deﬁnition (5.32).
What is this constant?
(e)
Verify that your g3(z) satisﬁes Eq. (5.34) with N = 3.
5.2.3 The Large N Limit
In the large N limit, gN(z) is self-averaging so computing gN(z) for the most likely con-
ﬁguration is the same as computing the average g(z). As N →∞, Eq. (5.34) becomes
V ′(z)g(z) −(z) = g2(z).
(5.36)

68
Joint Distribution of Eigenvalues
Each value of N gives a different degree-(k −1) polynomial N(z). From the deﬁnition
(5.32), we can show that the coefﬁcients of N(z) are related to the moments of the
maximum likelihood conﬁguration of size N. In the large N limit these moments converge
so the sequence N(z) converges to a well-deﬁned polynomial of degree (k −1) which we
call (z).
Since Eq. (5.36) is quadratic in g(z), its solution is given by
g(z) = 1
2

V ′(z) ±
*
V ′(z)2 −4(z)

,
(5.37)
where we have to choose the branch where g(z) goes to zero for large |z|.
The eigenvalues of M will be located where g(z) has an imaginary part for z very close
to the real axis. The ﬁrst term V ′(z) is a real polynomial and is always real for real z. The
expression V ′(z)2 −4(z) is also a real polynomial so g(z) cannot be complex on the real
axis unless V ′(z)2 −4(z) < 0. In this case
*
V ′(z)2 −4(z) is purely imaginary. We
conclude that, when x is such that ρ(x)  0,
Re(g(x)) := −
 ρ(λ)dλ
x −λ = V ′(x)
2
,
(5.38)
where −
denotes the principal part of the integral. Re(g(x))/π is also called the Hilbert
transform of ρ(λ).4
We have thus shown that the Hilbert transform of the density of eigenvalues is (within its
support) equal to π/2 times the derivative of the potential. We thus realize that the potential
outside the support of the eigenvalue has no effect on the distribution of eigenvalues. This
is natural in the Coulomb gas analogy. At equilibrium, the particles do not feel the potential
away from where they are. One consequence is that we can consider potentials that are not
conﬁning at inﬁnity as long as there is a conﬁnement region and that all eigenvalues are
within that region. For example, we will consider the quartic potential V (x) = x2/2 +
γ x4/4. For small negative γ the region around x = 0 is convex. If all eigenvalues are
found to be contained in that region, we can modify at will the potential away from it so
that V (x) →+∞for |x| →∞and keep Eq. (5.1) normalizable, as a probability density
should be.
Suppose now that we have a potential that is not a polynomial. In a ﬁnite region we can
approximate it arbitrarily well by a polynomial of sufﬁciently high degree. If we choose the
region of approximation such that for every successive approximation all eigenvalues lie in
that region, we can take the limits of these approximations and ﬁnd that Eq. (5.38) holds
even if V ′(x) is not a polynomial.
We can also ask the reverse question. Given a density ρ(x), does there exist a model from
the orthogonal ensemble (or other β-ensemble) that has ρ(λ) as its eigenvalue density? If
the Hilbert transform of ρ(x) is well deﬁned, then the answer is yes and Eq. (5.38) gives the
4 This is a slight abuse of terms, however, that can lead to paradoxes, if one extends Eq. (5.38) to the region where ρ(x) = 0. In
this case, the right hand side of the equation is not equal to V ′(x)/2. See also Appendix A.2.

5.3 Applications: Wigner, Wishart and the One-Cut Assumption
69
corresponding potential. Note that the potential is only deﬁned up to an additive constant
(it can be absorbed in the normalization of Eq. (5.1)) so knowing its derivative is enough
to compute V (x). Note also that we only know the value of V (x) on the support of ρ(x);
outside this support we can arbitrarily choose V (x) provided it is convex and goes to inﬁnity
as |x| →∞.
Exercise 5.2.2
Matrix potential for the uniform density
In Exercise 2.3.3, we saw that the Stieltjes transform for a uniform density of
eigenvalues between 0 and 1 is given by
g(z) = log

z
z −1

.
(5.39)
(a)
By computing Re(g(x)) for x between 0 and 1, ﬁnd V ′(x) using Eq. (5.38).
(b)
Compute the Hilbert transform of the uniform density to recover your answer
in (a).
(c)
From your answer in (a) and (b), show that the matrix potential is given by
V (x) = 2[(1 −x) log(1 −x) + x log(x)] + C
for 0 < x < 1,
(5.40)
where C is an arbitrary constant. Note that for x < 0 and x > 1 the potential
should be completed by a convex function that goes to inﬁnity as |x| →∞.
5.3 Applications: Wigner, Wishart and the One-Cut Assumption
5.3.1 Back to Wigner and Wishart
Now we apply the results of the previous section to the Gaussian orthogonal case where
V (z) = z2/2σ 2. In this simple case, (z) can be computed from its deﬁnition without
knowing the eigenvalues, since
V ′(z) = z
σ 2 ⇒(z) = 1
σ 2 .
(5.41)
Then (5.37) gives
g(z) = z −
±
⃝√
z2 −4σ 2
2σ 2
,
(5.42)
which recovers, independently of the value of β, the Wigner result Eq. (2.38), albeit within
a completely different framework (the notation
±
⃝√· was introduced in Section 4.2.3). In
particular, the cavity method does not assume that the matrix ensemble is rotationally
invariant, as we do here.

70
Joint Distribution of Eigenvalues
In the Wishart case, we only consider the case q < 1, otherwise (q ≥1) the potential is
not conﬁning and we need to impose the positive semi-deﬁniteness of the matrix to avoid
eigenvalues running to minus inﬁnity. We have
V ′(z) = 1
q

1 + q −1
z

.
(5.43)
In this case zV ′(z) is of degree one, so z(z) is a polynomial of degree zero:
(z) = c
z
(5.44)
for some constant c. Thus (5.37) then gives
g(z) = z + q −1 −
±
⃝*
(z + q −1)2 −4cq2z
2qz
.
(5.45)
As z →+∞, this expression becomes
g(z) = cq
z + O(1/z2).
(5.46)
Imposing g(z) ∼z−1 gives c = q−1. After some manipulations, we recover Eq. (4.40).
5.3.2 General Convex Potentials and the One-Cut Assumption
For more general polynomial potentials, ﬁnding an explicit solution for the limiting Stieltjes
transform is a little bit more involved. We recall Eq. (5.37):
g(z) = 1
2

V ′(z) ±
*
V ′(z)2 −4(z)

.
(5.47)
For a particular polynomial V ′(z), (z) is a polynomial that depends on the moments of the
matrix M. The expansion of g(z) near z →∞will give a set of self-consistent equations
for the coefﬁcients of (z).
The problem simpliﬁes greatly if the support of density of eigenvalues is compact, i.e. if
the density ρ(λ) is non-zero for all λ’s between two edges λ−and λ+. We expect this to be
true if the potential V (x) is convex. Indeed, by the Coulomb gas analogy we could place
all eigenvalues near the minimum of V (x) and let them ﬁnd their equilibrium conﬁguration
by repelling each other. For a convex potential it is natural to assume that the equilibrium
conﬁguration would not have any gaps. This assumption is equivalent to assuming that
the limiting Stieltjes transform has a single branch cut (from λ−and λ+), hence the name
one-cut assumption.
So, for a convex polynomial potential V (x), we expect that there exists a well-deﬁned
equilibrium density ρ(λ) that is non-zero if and only if λ−< λ < λ+ and that g(z) satisﬁes
g(z) =
 λ+
λ−
ρ(λ)
z −λdλ.
(5.48)

5.3 Applications: Wigner, Wishart and the One-Cut Assumption
71
From this equation we notice three important properties of g(z):
• The function g(z) is potentially singular at λ−and λ+.
• Near the real axis (Im z = 0+) g(z) has an imaginary part if z ∈(λ−,λ+) and is real
otherwise.
• The function g(z) is analytic everywhere else.
If we go back to Eq. (5.37), we notice that any non-analytic behavior must come from
the square-root. On the real axis, the only way g(z) can have an imaginary part is if D(z) :=
V ′(z)2−4(z) < 0 for some values of z. So D(z) (a polynomial of degree 2k) must change
sign at some values λ−and λ+, hence these must be zeros of the polynomial. On the real
axis, the other possible zeros D(z) can only be of even multiplicity (otherwise D(z) would
change sign). Elsewhere in the complex plane, zeros should also be of even multiplicity,
otherwise √D(z) would be singular at those zeros. In other words D(z) must be of the
form
D(z) = (z −λ−)(z −λ+)Q2(z),
(5.49)
for some polynomial Q(z) of degree k −1 where k is the degree of V ′(z). We can therefore
write g(z) as
g(z) = V ′(z) ± Q(z)
*
(z −λ−)(z −λ+)
2
,
(5.50)
where again Q(z) is a polynomial with real coefﬁcients of degree one less than V ′(z). The
condition that
g(z) →1
z when |z| →∞
(5.51)
is now sufﬁcient to compute Q(z) and also λ± for a given potential V (z). Indeed, expanding
Eq. (5.50) near z →∞, the coefﬁcients of Q(z) and the values λ± must be such as to
cancel the k + 1 polynomial coefﬁcients of V ′(z) and also ensure that the 1/z term has unit
coefﬁcient. This gives k + 2 equations to determine the k coefﬁcients of Q(z) and the two
edges λ±, see next section for an illustration.
Once the polynomial Q(x) is determined, we can read off the eigenvalue density:
ρ(λ) = Q(λ)
*
(λ+ −λ)(λ −λ−)
2π
for
λ−≤λ ≤λ+.
(5.52)
We see that generically the eigenvalue density behaves as ρ(λ± ∓δ) ∝
√
δ near both edges
of the spectrum. If by chance (or by construction) one of the edges is a zero of Q(z), then
the behavior changes to δθ near that edge, with θ = n + 1
2 and n the multiplicity of root of
Q(z). A potential with generic
√
δ behavior at the edge of the density is called non-critical.
Other non-generic cases are called critical. In Section 14.1 we will see how the
√
δ edge
singularity is smoothed over a region of width N−2/3 for ﬁnite N. In the critical case, the
smoothed region is of width N−2/(3+2n).

72
Joint Distribution of Eigenvalues
5.3.3 M2 + M4 Potential
One of the original motivations of Br´ezin, Itzykson, Parisi and Zuber to study the
ensemble deﬁned by Eq. (5.1) was to count the so-called planar diagrams in some ﬁeld
theories. To do so they considered the potential
V (x) = x2
2 + γ x4
4 .
(5.53)
We will not discuss how one can count planar diagrams from such a potential, but use
this example to illustrate the general recipe given in the main text to compute the Stieltjes
transform and the density of eigenvalues. Interestingly, for a certain value of γ , the edge
singularity is δ3/2 instead of
√
δ.
Since the potential is symmetric around zero we expect λ+ = −λ−=: 2a. We
introduce this extra factor of 2, so that if γ = 0, we obtain the semi-circular law with
a = 1. Since V ′(z) = z + γ z3 is a degree three polynomial, we write
Q(z) = a0 + a1z + γ z2,
(5.54)
where the coefﬁcient of z2 was chosen to cancel the γ z3 term at inﬁnity. Expanding
Eq. (5.50) near z →∞and imposing g(z) = 1/z + O(1/z2) we get
a1 = 0,
1 −a0 + 2γ a2 = 0,
(5.55)
2a4γ + 2a2a0 = 2.
Solving for a0, we ﬁnd
g(z) = z + γ z3 −(1 + 2γ a2 + γ z2)
±
⃝*
z2 −4a2
2
,
(5.56)
where a is a solution of
3γ a4 + a2 −1 = 0
⇒
a2 =
√1 + 12γ −1
6γ
.
(5.57)
The density of eigenvalues for the potential (5.53) reads
ρ(λ) = (1 + 2γ a2 + γ λ2)
*
4a2 −λ2
2π
for
γ > −1
12,
(5.58)
with a deﬁned as above. For positive values of γ , the potential is conﬁning (it is convex
and grows faster than a logarithm for z →±∞). In that case the equation for a always has
a solution, so the Stieltjes transform and the density of eigenvalues are well deﬁned; see
Figure 5.3. For small negative values of γ , the problem still makes sense. The potential is
convex near zero and the eigenvalues will stay near zero as long as the repulsion does not
push them too far in the non-convex region.
There is a critical value of γ at γc = −1/12, which corresponds to a =
√
2. At this
critical point, gc(z) and the density are given by
gc(z) = z3
24
⎡
⎣

1 −8
z2
 3
2
−1 + 12
z2
⎤
⎦and ρc(λ) =

8 −λ23/2
24π
.
(5.59)
At this point the density of eigenvalues at the upper edge (λ+ = 2
√
2) behaves as ρ(λ) ∼
(2
√
2 −λ)θ
+ with θ = 3/2 and similarly at the lower edge (λ−= −2
√
2). For values of

5.4 Fluctuations Around the Most Likely Conﬁguration
73
−2
0
2
l
0.0
0.1
0.2
0.3
0.4
r(l)
g = 1
g = 0
g = −1/ 12
−2.5
0.0
2.5
x
0
1
2
3
4
5
V(x)
g = 1
g = 0
g = −1/ 12
Figure 5.3 (left) Density of eigenvalues for the potential V (x) = 1
2x2 + γ
4 x4 for three values of γ .
For γ = 1, even if the minimum of the potential is at λ = 0, the density develops a double hump due
to the repulsion of the eigenvalues. γ = 0 corresponds to the Wigner case (semi-circle law). Finally
γ = −1/12 is the critical value of γ . At this point the density is given by Eq. (5.59). For smaller
values of γ the density does not exist. (right) Shape of the potential for the same three values of γ .
The dots on the bottom curve indicate the edges of the critical spectrum.
γ more negative than γc, there are no real solutions for a and Eq. (5.56) ceases to make
sense. In the Coulomb gas analogy, the eigenvalues push each other up to a critical point
after which they run off to inﬁnity. There is no simple argument that gives the location of
the critical point (except for doing the above computation). It is given by a delicate balance
between the repulsion of the eigenvalues and the conﬁning potential. In particular it is not
given by the point V ′(2a) = 0 as one might naively expect. Note that at the critical point
V ′′(2a) = −1, so we are already outside the convex region.
5.4 Fluctuations Around the Most Likely Conﬁguration
5.4.1 Fluctuations of All Eigenvalues
The most likely positions of the N eigenvalues are determined by the equations (5.28) that
we derived in Section 5.2.2. These equations balance a Coulomb (repulsive) term and a
conﬁning potential. On distances d of order 1/N, the Coulomb force on each “charge” is
of order d−1 ∼N, whereas the conﬁning force V ′ is of order unity. Therefore, the Coulomb
force is dominant at small scales and the most likely positions must be locally equidistant.
This is expected to be true within small enough intervals
I := [λ −L/2N,λ + L/2N],
(5.60)
where L is sufﬁciently small, such that the average density ρ(λ) does not vary too much,
i.e. ρ′(λ)L/N ≪ρ(λ).

74
Joint Distribution of Eigenvalues
Of course, some ﬂuctuations around this crystalline order must be expected, and the aim
of this section is to introduce a simple method to characterize these ﬂuctuations. Before
doing so, let us discuss how the number of eigenvalues n(L) in interval I behaves. For
a perfect crystalline arrangement, there are no ﬂuctuations at all and one has n(L) =
ρ(λ)L + O(1), where the O(1) term comes from “rounding errors” at the edge of the
interval.
For a Poisson process, where points are placed independently at random with some
density Nρ(λ), then it is well known that
n(L) = ¯n + ξ
√
¯n,
with
¯n := ρ(λ)L,
(5.61)
where ξ is a Gaussian random variable N(0,1). For the eigenvalues of a large symmetric
random matrix, on the other hand, the exact result is given by
n(L) = ¯n +
√
ξ,
 := 2
π2

log(¯n) + C

+ O(¯n−1),
(5.62)
where C is a numerical constant.5 This result means that the typical ﬂuctuations of the
number of eigenvalues is of order √log ¯n for large L, much smaller than the Poisson result
√
¯n. In fact, the growth of  with L is so slow that one can think of the arrangement of
eigenvalues as “quasi-crystalline”, even after factoring in ﬂuctuations.
Let us see how this √log ¯n behavior of the ﬂuctuations can be captured by computing
the Hessian matrix H of the log-likelihood L deﬁned in Eq. (5.27). One ﬁnds
Hij := −∂2L
∂λi∂λj
=
⎧
⎨
⎩
V ′′(λi) + 1
N

ki
2
(λi−λk)2
(i = j),
−2
N
1
(λi−λj )2
(i  j).
This Hessian matrix should be evaluated at the maximum of L, i.e. for the most likely
conﬁgurations of the λi. If we call ϵi/N the deviation of λi away from its most likely
value, and assume all ϵ to be small enough, their joint distribution can be approximated
by the following multivariate Gaussian distribution:
P({ϵi}) ∝exp
⎡
⎣−β
4N

i,j
ϵiHijϵj
⎤
⎦,
(5.63)
from which one can obtain the covariance of the deviations as
E[ϵiϵj] = 2N
β [H−1]ij.
(5.64)
Since the most likely positions of the λi are locally equidistant, one can approximate
λi −λj as (i −j)/Nρ in the above expression. This is justiﬁed when the contribution of
far-away eigenvalues to H−1 is negligible in the region of interest, which will indeed be
the case because the off-diagonal elements of H decay fast enough (i.e. as (i −j)−2).
5 For β = 1, one ﬁnds C = log(2π) + γ + 1 + π2
8 , where γ is Euler’s constant, see Mehta [2004].

5.4 Fluctuations Around the Most Likely Conﬁguration
75
For simplicity, let us consider the case where V (x) = x2/2, in which case V ′′(x) = 1.
The matrix H is then a Toeplitz matrix, up to unimportant boundary terms. It can be
diagonalized using plane waves (see Appendix A.3), with eigenvalues given by6
μq = 1 + 4Nρ2
N−1

ℓ=1
1 −cos 2πqℓ
N
ℓ2
,
q = 0,1, . . . ,N −1,
(5.65)
where ρ is the local average eigenvalue density. In the large N limit, the (convergent) sum
can be replaced by an integral:
μq = 1 + 4Nρ2 × 2π|q|
N
 ∞
0
du1 −cos u
u2
= 1 + 4π2ρ2|q|.
(5.66)
The eigenvalues of H−1 are then given by 1/μq and the covariance of the deviations for
i −j = n is obtained from Eq. (5.64) as an inverse Fourier transform. After transforming
q →uN/2π this reads
E[ϵiϵj] = 2N
β
1
N
 π
−π
du
2π
e−iun
N−1 + 2πρ2|u|.
(5.67)
Now, the ﬂuctuating distance dij between eigenvalues i and j = i + n is, by deﬁnition of
the ϵ,
dij =
n
Nρ + ϵi −ϵj
N
.
(5.68)
Its variance is obtained, for N →∞, from
E[(ϵi −ϵj)2] ≈
2
βπ2(ρN)2
 π
0
du1 −cos un
u
≈n≪1
2
βπ2ρ2 log n.
(5.69)
The variables ϵ are thus long-ranged, log-correlated Gaussian variables. Interestingly,
there has been a ﬂurry of activity concerning this problem in recent years (see references
at the end of this chapter).
Finally, the ﬂuctuating local density of eigenvalues can be computed from the number
of eigenvalues within a distance dij, i.e.
1
N
n
dij
≈ρ + ρ2 ϵi −ϵj
n
(n ≫1).
(5.70)
Its variance is thus given by
V[ρ] =
2ρ2
βπ2n2 log n.
(5.71)
Hence, the ﬂuctuation of the number of eigenvalues in a ﬁxed interval of size L/N and
containing on average ¯n = ρL eigenvalues is
V[ρL] = L2V[ρ] =
2
βπ2 log ¯n.
(5.72)
This argument recovers the leading term of the exact result for all values of β (compare
with Eq. (5.62) for β = 1).
6 In fact, the Hessian H can be diagonalized exactly in this case, without any approximations, see Agarwal et al. [2019] and
references therein.

76
Joint Distribution of Eigenvalues
5.4.2 Large Deviations of the Top Eigenvalue
Another interesting question concerns the ﬂuctuations of the top eigenvalue of a matrix
drawn from a β-ensemble. Consider Eq. (5.26) with λmax isolated:
P(λmax;{λi}) = PN−1({λi}) exp
0
−Nβ
2
-
V (λmax) −2
N
N−1

i=1
log(λmax −λi)
.3
, (5.73)
with
PN−1({λi}) := Z−1
N exp
⎧
⎪⎪⎨
⎪⎪⎩
−β
2
⎡
⎢⎢⎣
N−1

i=1
NV (λi) −
N−1

i,j=1
ji
log |λi −λj|
⎤
⎥⎥⎦
⎫
⎪⎪⎬
⎪⎪⎭
.
(5.74)
At large N, this probability is dominated by the most likely conﬁguration, which is deter-
mined by Eq. (5.27) with λmax removed. Clearly, the most likely positions {λ∗
i } of the N −1
other eigenvalues are only changed by an amount O(N−1), but since the log-likelihood is
close to its maximum, the change of L (i.e. the quantity in the exponential) is only of order
N−2 and we will neglect it. Then one has the following large deviation expression:
P(λmax;{λ∗
i })
P(λ+;{λ∗
i }) := exp
(
−Nβ
2 (λmax)
)
,
(5.75)
with
(x) = V (x) −V (λ+) −2
N
N−1

i=1
log(x −λ∗
i ) + 2
N
N−1

i=1
log(λ+ −λ∗
i ),
(5.76)
where λ+ is the top edge of the spectrum. Note that (λ+) = 0 by construction. To deal
with the large N limit of this expression, we take the derivative of (x) with respect to x
to ﬁnd
′(x) = V ′(x) −2
N
N−1

i=1
1
x −λ∗
i
N→∞
→
V ′(x) −2g(x).
(5.77)
Hence,
(x) =
 x
λ+

V ′(s) −2g(s)

ds.
(5.78)
When the potential V ′(x) is a polynomial of degree k ≥1, we can use Eq. (5.37),
yielding
(x) =
 x
λ+
*
V ′(s)2 −4(s) ds,
(5.79)
where λ+ is the largest zero of the polynomial V ′(s)2 −4(s). Since (s) is a polynomial
of order k −1, for large s one always has
V ′(s)2 ≫|(s)|,
(5.80)

5.4 Fluctuations Around the Most Likely Conﬁguration
77
and therefore
(x) ≈V (x).
(5.81)
As expected, the probability to observe a top eigenvalue very far from the bulk is dominated
by the potential term, and the Coulomb interaction no longer plays any role. When x −λ+
is small but still much larger than any inverse power of N, we have
*
V ′(s)2 −4(s) ≈C(s −λ+)θ,
(5.82)
where C is a constant and θ depends on the type of root of V ′(s)2 −4(s) at s = λ+. For
a single root, which is the typical case, θ = 1/2. Performing the integral we get
(λmax) =
C
θ + 1(λmax −λ+)θ+1,
λmax −λ+ ≪1.
(5.83)
Note that the constant C can be determined from the eigenvalue density near (but slightly
below) λ+, to wit, πρ(λ) ≈C(λ+ −λ)θ.
For a generic edge with θ = 1/2, one ﬁnds that (λmax) ∝(λmax −λ+)3/2, and thus
the probability to ﬁnd λmax just above λ+ decays as
P(λmax) ∼exp
(
−2βC
3
u3/2
)
;
u = N2/3(λmax −λ+),
(5.84)
where we have introduced the rescaled variable u in order to show that this probability
decays on scale N−2/3. We will further discuss this result in Section 14.1, where we will
see that the u3/2 behavior coincides with the right tail of the Tracy–Widom distribution.
For a unit Wigner, we have (see Fig. 5.4)
2
3
4
5
6
7
8
x
0
5
10
15
20
25
F(x)
Wigner
Wishart
Figure 5.4 Large deviation function for the top eigenvalue (x) for a unit Wigner and a Wishart
matrix with λ+ = 2 (q = 3 −2
√
2). The Wishart curve was obtained by numerically integrating
Eq. (5.78).

78
Joint Distribution of Eigenvalues
(x) = 1
2x
*
x2 −4 −2 log
 √
x2 −4 + x
2

for
x > 2.
(5.85)
For a Wishart matrix with parameter q = 1,
(x) =
*
(x −4)x + 2 log
x −√(x −4)x −2
2

for
x > 4.
(5.86)
Remember that the Stieltjes transform g(z) and the density of eigenvalues ρ(λ) are only
sensitive to the potential function V (x) for values in the support of ρ (λ−≤x ≤λ+).
The large deviation function (x), on the other hand, depends on the value of the potential
for x > λ+. For Eq. (5.79) to hold, the same potential function must extend analytically
outside the support of ρ. In Section 5.3.3, we considered a non-conﬁning potential (when
γ < 0). We argued that we could compute g(z) and ρ(λ) as if the potential were conﬁning
as long as we could replace the potential outside (λ−,λ+) by a convex function going
to inﬁnity. In that case, the function (x) depends on the choice of regularization of the
potential. Computing Eq. (5.78) with the non-conﬁning potential would give nonsensical
results.
Exercise 5.4.1
Large deviations for Wigner and Wishart
(a)
Show that Eqs. (5.85) and (5.86) are indeed the large deviation function for
the top eigenvalues of a unit Wigner and a Wishart q = 1. To do so, show
that they satisfy Eq. (5.77) and that (λ+) = 0 with λ+ = 2 for Wigner and
λ+ = 4 for Wishart q = 1.
(b)
Find the dominant contribution of both (x) for large x. Compare to their
respective V (x).
(c)
Expand the two expressions for (x) near their respective λ+ and show that
they have the correct leading behavior. What are the exponent θ and constant
C?
5.5 An Eigenvalue Density Saddle Point
As an alternative approach to the most likely conﬁguration of eigenvalues, one often
ﬁnds in the random matrix literature the following density formalism. One ﬁrst introduces
the density of “charges” ω(x) for a given set of eigenvalues λ1,λ2, . . . ,λN (not necessarily
the most likely conﬁguration), as
ω(x) = 1
N
N

i=1
δ(λi −x).
(5.87)
Expressed in terms of this density ﬁeld, the joint distribution of eigenvalues, Eq. (5.26)
can be expressed as

5.5 An Eigenvalue Density Saddle Point
79
P({ω}) = Z−1 exp

−βN2
2
(
dxω(x)V (x) −−

dxdyω(x)ω(y) log |x −y|
)
−N

dxω(x) log ω(x)

,
(5.88)
where the last term is an entropy term, formally corresponding to the change of variables
from the {λi} to ω(x). Since this term is of order N, compared to the two ﬁrst terms that
are of order N2, it is usually neglected.7
One then proceeds by looking for the density ﬁeld that maximizes the term in the
exponential, which is obtained by taking its functional derivative with respect to all ω(x):
δ
δω(x)
(
dyω(y)V (y) −−

dydy′ω(y)ω(y′) log |y −y′| −ζ

dyω(y)
)
ω∗= 0,
(5.89)
where ζ
is a Lagrange multiplier, used to impose the normalization condition

dxω(x) = 1. This leads to
V (x) = 2−

dyω∗(y) log |x −y| + ζ.
(5.90)
We can now take the derivative with respect to x to get
V ′(x) = 2−

dy ω∗(y)
x −y ,
(5.91)
which is nothing but the continuum limit version of Eq. (5.28), and is identical to
Eq. (5.38). Although this whole procedure looks somewhat ad hoc, it can be fully
justiﬁed mathematically. In the mathematical literature, it is known as the large deviation
formalism.
Equation (5.91) is a singular integral equation for ω(x) of the so-called Tricomi type.
Such equations often have explicit solutions, see Appendix A.2. In the case where V (x) =
x2/2, one recovers, as expected, the semi-circle law:
ω∗(x) = 1
2π
*
4 −x2.
(5.92)
One interesting application of the density formalism is to investigate the case of Gaus-
sian orthogonal random matrices conditioned to have all eigenvalues strictly positive.
What is the probability for this to occur spontaneously? In such a case, what is the resulting
distribution of eigenvalues?
The trick is to solve Eq. (5.91) with the constraint that ω(x < 0) = 0. This leads to
x = 2−
 ∞
0
dy ω∗(y)
x −y .
(5.93)
The solution to this truncated problem can also be found analytically using the general
result of Appendix A.2. One ﬁnds
ω∗(x) = 1
4π
1
λ+ −x
x
(λ+ + 2x),
with
λ+ =
4
√
3
.
(5.94)
The resulting density has a square-root divergence close to zero, which is a stigma of all
the negative eigenvalues being pushed to the positive side. The right edge itself is pushed
from 2 to 4/
√
3, see Figure 5.5.
7 Note however that when β = c/N, as considered in Allez et al. [2012], the entropy term must be retained.

80
Joint Distribution of Eigenvalues
0.0
0.5
1.0
1.5
2.0
2.5
l
0.00
0.25
0.50
0.75
1.00
1.25
1.50
r(l)
Conditioned Wigner
Wigner
Figure 5.5 Density of eigenvalues for a Wigner matrix conditioned to be positive semi-deﬁnite. The
positive part of the density of a standard Wigner is shown for comparison.
Injecting this solution back into Eq. (5.88) and comparing with the result corresponding
to the standard semi-circle allows one to compute the probability for such an excep-
tional conﬁguration to occur. After a few manipulations, one ﬁnds that this probability
is given by e−βCN2 with C = log 3/4 ≈0.2746.... The probability that a Gaussian
random matrix has by chance all its eigenvalues positive therefore decreases extremely fast
with N.
Other constraints are possible as well, for example if one chooses to conﬁne all eigen-
values in a certain interval [ℓ−,ℓ+] with ℓ−≥λ−or ℓ+ ≤λ+. (In the cases where
ℓ−< λ−and ℓ+ > λ+ the conﬁnement plays no role.) Let us study this problem in
the case where there is no external potential at all, i.e. when V (x) ≡C, where C is an
arbitrary constant, but only conﬁning walls. In this case, the general solution of Appendix
A.2 immediately leads to
ω∗(x) =
1
π
*
(x −ℓ−)(ℓ+ −x)
,
(5.95)
which has a square-root divergence at both edges. This law is called the arcsine law, which
appears in different contexts, see Sections 7.2 and 15.3.1.
Note that the minimization of the quadratic form −
dxdyω(x)ω(y)G(|x −y|) for a
general power-law interaction G(u) = u−γ , subject to the constraint

dxω(x) = 1, has
been solved in the very different, ﬁnancial engineering context of optimal execution with
quadratic costs. The solution in that case reads
ω∗(x) = A(γ )ℓ−γ
/
(x −ℓ−)γ −1(ℓ+ −x)γ −1,
(5.96)
with ℓ:= ℓ−+ ℓ+ and A(γ ) := γ 
[γ ]/
2[(1 + γ )/2]. The case G(u) = −log(u)
formally corresponds to γ = 0, in which case one recovers Eq. (5.95).

5.5 An Eigenvalue Density Saddle Point
81
Bibliographical Notes
• For a general introduction to the subject, see
– M. L. Mehta. Random Matrices. Academic Press, San Diego, 3rd edition, 2004,
– P. J. Forrester. Log Gases and Random Matrices. Princeton University Press,
Princeton, NJ, 2010,
and for more technical aspects,
– B. Eynard, T. Kimura, and S. Ribault. Random matrices. preprint arXiv:1510.04430,
2006.
• One-cut solution and the M2 + M4 model:
– E. Br´ezin, C. Itzykson, G. Parisi, and J.-B. Zuber. Planar diagrams. Communications
in Mathematical Physics, 59(1):35–51, 1978.
• On the relation between log gases and the Calogero model, see
– S. Agarwal, M. Kulkarni, and A. Dhar. Some connections between the classical
Calogero-Moser model and the log gas. Journal of Statistical Physics, 176(6), 1463–
1479, 2019.
• About spectral rigidity and log-ﬂuctuations of eigenvalues, see
– M. L. Mehta. Random Matrices. Academic Press, San Diego, 3rd edition, 2004,
– M. V. Berry. Semiclassical theory of spectral rigidity. Proceedings of the Royal Soci-
ety of London, Series A, 400(1819):229–251, 1985,
for classical results, and
– Y. V. Fyodorov and J. P. Keating. Freezing transitions and extreme values: Random
matrix theory, and disordered landscapes. Philosophical Transactions of the Royal
Society A: Mathematical, Physical and Engineering Sciences, 372(2007):2012053,
2014,
– Y. V. Fyodorov, B. A. Khoruzhenko, and N. J. Simm. Fractional Brownian motion
with Hurst index H = 0 and the Gaussian Unitary Ensemble. The Annals of Proba-
bility, 44(4):2980–3031, 2016,
– R. Chhaibi and J. Najnudel. On the circle, gmcγ = cβe∞for γ = √2/β, (γ ≤1).
preprint arXiv:1904.00578, 2019,
for the recent spree of activity in the ﬁeld of log-correlated variables.
• On the probability of a very large eigenvalue, see Chapter 14 and
– S. N. Majumdar and M. Vergassola. Large deviations of the maximum eigenvalue for
Wishart and Gaussian random matrices. Physical Review Letters, 102:060601, 2009.
• On the continuous density formalism, see
– D. S. Dean and S. N. Majumdar. Extreme value statistics of eigenvalues of Gaussian
random matrices. Physical Review E, 77:041108, 2008
for a physicists’ introduction, and

82
Joint Distribution of Eigenvalues
– G. Ben Arous and A. Guionnet. Large deviations for Wigner’s law and Voiculescu’s
non-commutative entropy. Probability Theory and Related Fields, 108(4):517–542,
1997,
– G. W. Anderson, A. Guionnet, and O. Zeitouni. An Introduction to Random Matrices.
Cambridge University Press, Cambridge, 2010
for more mathematical discussions in the context of large deviation theory. A case where
the entropy term must be retained is discussed in
– R. Allez, J. P. Bouchaud, and A. Guionnet. Invariant beta ensembles and the Gauss-
Wigner crossover. Physical Review Letters, 109(9):094102, 2012.
• On the relation with optimal execution problems, see
– J. Gatheral, A. Schied, and A. Slynko. Transient linear price impact and Fredholm
integral equations. Mathematical Finance, 22(3):445–474, 2012,
– J.-P. Bouchaud, J. Bonart, J. Donier, and M. Gould. Trades, Quotes and Prices.
Cambridge University Press, Cambridge, 2nd edition, 2018.

6
Eigenvalues and Orthogonal Polynomials*
In this chapter, we investigate yet another route to shed light on the eigenvalue density
of the Wigner and Wishart ensembles. We show (a) that the most probable positions of
the Coulomb gas problem coincide with the zeros of Hermite polynomials in the Wigner
case, and of Laguerre polynomials in the Wishart case; and (b) that the average (over
randomness) of the characteristic polynomials (deﬁned as det(z 1 −XN)) of Wigner or
Wishart random matrices of size N obey simple recursion relations that allow one to express
them as, respectively, Hermite and Laguerre polynomials. The fact that the two methods
lead to the same result (at least for large N) reﬂects the fact that eigenvalues ﬂuctuate
very little around their most probable positions. Finally we show that for unitary ensembles
β = 2, the expected characteristic polynomial is always an orthogonal polynomial with
respect to some weight function related to the matrix potential.
6.1 Wigner Matrices and Hermite Polynomials
6.1.1 Most Likely Eigenvalues and Zeros of Hermite Polynomials
In the previous chapter, we established a general equation for the Stieltjes transform of
the most likely positions of the eigenvalues of random matrices belonging to a general
orthogonal ensemble, see Eq. (5.34). In the special case of a quadratic potential V (x) =
x2/2, this equation reads
zgN(z) −1 = g2
N(z) + g′
N(z)
N
.
(6.1)
This ordinary differential equation is of the Ricatti type,1 and can be solved by setting
gN(z) := ψ′(z)/Nψ(z). This yields, upon substitution,
ψ′′(z) −Nzψ′(z) + N2ψ(z) = 0,
(6.2)
or, with ψ(z) = (x =
√
Nz),
′′(x) −x′(x) + N(x) = 0.
(6.3)
1 A Ricatti equation is a ﬁrst order differential equation that is quadratic in the unknown function, in our case gN (z).
83

84
Eigenvalues and Orthogonal Polynomials
The solution of this last equation with the correct behavior for large x is the Hermite
polynomial of order N. General Hermite polynomials Hn(x) are deﬁned as the nth order
polynomial that starts as xn and is orthogonal to all previous ones under the unit Gaussian
measure:2

dx
√
2π
Hn(x)Hm(x)e−x2
2 = 0 when n  m.
(6.4)
The ﬁrst few are given by
H0(x) = 1,
H1(x) = x,
H2(x) = x2 −1,
(6.5)
H3(x) = x3 −3x,
H4(x) = x4 −6x2 + 3.
In addition to the above ODE (6.3), they satisfy
d
dx Hn(x) = nHn−1(x),
(6.6)
and the recursion
Hn(x) = xHn−1(x) −(n −1)Hn−2(x),
(6.7)
which combined together recovers Eq. (6.3). Hermite polynomials can be written explic-
itly as
Hn(x) = exp
-
−1
2
 d
dx
2.
xn =
⌊n/2⌋

m=0
(−1)m
2m
n!
m! (n −2m)!xn−2m,
(6.8)
where ⌊n/2⌋is the integer part of n/2.
Coming back to Eq. (6.1), we thus conclude that the exact solution for the Stieltjes
transform gN(z) at ﬁnite N is
gN(z) =
H ′
N(
√
Nz)
√
NHN(
√
Nz)
(6.9)
or, writing HN(x) = N
i=1(x−
√
Nh(N)
i
), where
√
Nh(N)
i
are the N (real) zeros of HN(x),
gN(z) = 1
N
N

i=1
1
z −h(N)
i
.
(6.10)
2 Hermite polynomials can be deﬁned using two different conventions for the unit Gaussian measure. We use here the
“probabilists’ Hermite polynomials”, while the “physicists’ convention” uses a Gaussian weight proportional to e−x2.

6.1 Wigner Matrices and Hermite Polynomials
85
−10
0
10
l
0.00
0.01
0.02
0.03
0.04
r(l)
Figure 6.1 Histogram of the 64 zeros of H64(x). The full line is the asymptotic prediction from the
semi-circle law.
Comparing with the deﬁnition of gN(z), Eq. (5.30), one concludes that the most likely
positions of the Coulomb particles are exactly given by the zeros of Hermite polynomi-
als (scaled by
√
N). This is a rather remarkable result, which holds for more general
conﬁning potentials, to which are associated different kinds of orthogonal polynomials.
Explicit examples will be given later in this section for the Wishart ensemble, where we
will encounter Laguerre polynomials (see also Chapter 7 for Jacobi polynomials).
Since we know that gN(z) converges, for large N, towards the Stieltjes transform of
the semi-circle law, we can conclude that the rescaled zeros of Hermite polynomials are
themselves distributed, for large N, according to the same semi-circle law. This classical
property of Hermite polynomials is illustrated in Figure 6.1.
6.1.2 Expected Characteristic Polynomial of Wigner Matrices
In this section, we will show that the expected characteristic polynomial of a Wigner matrix
XN, deﬁned as QN(z) := E[det(z 1 −XN)], is given by the same Hermite polynomial as
above. The idea is to write a recursion relation for QN(z) by expanding the determinant in
minors. Since we will be comparing Wigner matrices of different size, it is more convenient
to work with unscaled matrices YN =
√
NXN, i.e. symmetric matrices of size N with
elements of zero mean and variance 1 (it will turn out that the variance of diagonal elements
is actually irrelevant, and so can also be chosen to be 1). We deﬁne
qN(z) := E[det(z 1 −YN)].
(6.11)
Using det(αA) = αN det(A), we then have
QN(z) = N−N/2qN(
√
Nz).
(6.12)

86
Eigenvalues and Orthogonal Polynomials
We can compute the ﬁrst two qN(z) by hand:
q1(z) = E[z −Y11] = z;
q2(z) = E
&
(z −Y11)(z −Y22) −Y2
12
'
= z2 −1. (6.13)
To compute the polynomials for N ≥3 we ﬁrst expand the determinant in minors from the
ﬁrst line. We call Mi,j the ij-minor, i.e. the determinant of the submatrix of z 1 −YN with
the line i and column j removed:
det(z 1 −YN) =
N

i=1
(−1)i+1(zδi1 −Y1i)M1,i
= zM1,1 −Y11M1,1 +
N

i=2
(−1)iY1iM1,i.
(6.14)
We would like to take the expectation value of this last expression. The ﬁrst two terms
are easy: the minor M1,1 is the same determinant with a Wigner matrix of size N −1, so
E[M1,1] ≡qN−1(z); the diagonal element Y11 is independent from the rest of the matrix
and its expectation is zero.
For the other terms in the sum, the minor Mi,1 is not independent of Yi1. Indeed, because
XN is symmetric, the corresponding submatrix contains another copy of Y1i. Let us then
expand M1,i itself on the ith row, to make the other term Yi1 appear explicitly. For i  1,
we have
Mi,1 =
N

j=1,ji
(−1)i−jYijM1i,ij
= (−1)i−1Yi1M1i,i1 +
N

j=2,ji
(−1)i−jYijM1i,ij,
(6.15)
where Mij,kl is the “sub-minor”, with rows i,j and columns k,l removed.
We can now take the expectation value of Eq. (6.14) by noting that Y1i is independent
of all the terms in Eq. (6.15) except the ﬁrst one. We also realize that M1i,i1 is the same
determinant with a Wigner matrix of size N −2 that is now independent of Y1i, so we have
E[M1i,i1] = qN−2(z). Putting everything together we get
qN(z) := E[det(z 1 −YN)] = zqN−1(z) −(N −1)qN−2(z).
(6.16)
We recognize here precisely the recursion relation (6.7) that deﬁnes Hermite polynomials.
How should this result be interpreted? Suppose for one moment that the positions of
the eigenvalues λi of YN were not ﬂuctuating from sample to sample, and ﬁxed to their
most likely values λ∗
i . In this case, the expectation operator would not be needed and one
would have
gN(z) = d
dz log QN(z) =
H ′
N(
√
Nz)
√
NHN(
√
Nz)
,
(6.17)

6.2 Laguerre Polynomials
87
recovering the result of the previous section. What is somewhat surprising is that
E
- N
,
i=1
(z −λi)
.
≡
n
,
i=1
(z −λ∗
i )
(6.18)
even when ﬂuctuations are accounted for. In particular, in the limit N →∞, the average
Stieltjes transform should be computed from the average of the logarithm of the character-
istic polynomial:
g(z) = lim
N→∞
1
N E
( d
dz log det(z 1 −XN)
)
;
(6.19)
but the above calculation shows that one can compute the logarithm of the average charac-
teristic polynomial instead. The deep underlying mechanism is the eigenvalue spectrum of
random matrices is rigid – ﬂuctuations around most probable positions are small.
Exercise 6.1.1
Hermite polynomials and moments of the Wigner
Show that (for n ≥4)
QN(x) = xn −n −1
2
xn−2 + (n −1)(n −2)(n −3)
8n
xn−4 + O

xn−6
,
(6.20)
therefore
gN(z) = 1
z −n −1
N
1
z3 −(n −1)(2n −3)
N
1
z5 + O
 1
z7

,
(6.21)
so in the large N limit we recover the ﬁrst few terms of the Wigner Stieltjes
transform
g(z) = 1
z −1
z3 −2
z5 + O
 1
z7

.
(6.22)
6.2 Laguerre Polynomials
6.2.1 Most Likely Characteristic Polynomial of Wishart Matrices
Similarly to the case of Wigner matrices, the Stieltjes transform of the most likely positions
of the Coulomb charges in the Wishart ensemble can be written as gN(z) := ψ′(z)/Nψ(z),
where ψ(z) is a monic polynomial of degree N satisfying
ψ′′(x) −NV ′(x)ψ′(x) + N2N(x)ψ(x) = 0,
(6.23)
with
NV ′(x) = N −T −1 + 2β−1
x
+ T,
(6.24)

88
Eigenvalues and Orthogonal Polynomials
and, using Eq. (5.32),
N(x) = 1
N
N

k=1
V ′(x) −V ′(λ∗
k)
x −λ∗
k
= cN
x ,
(6.25)
where
cN = −N −T −1 + 2β−1
N2
N

k=1
1
λ∗
k
.
(6.26)
Writing now ψ(x) = (T x) and u = T x, Eq. (6.23) becomes
u′′(u) −(N −T −1 + 2β−1 + u)′(u) + N2
T cN(u) = 0.
(6.27)
This is the differential equation for the so-called associated Laguerre polynomials L(α) with
α = T −N −2β−1. It has polynomial solutions of degree N if and only if the coefﬁcient
of the (u) term is an integer equal to N (i.e. if cN = T /N). The solution is then given by
(u) ∝L(T −N−2β−1)
N
(u),
(6.28)
where
L(α)
n (x) = x−α ( d
dx −1)n
n!
xα+n.
(6.29)
Note that associated Laguerre polynomials are orthogonal with respect to the measure
xαe−x, i.e.
 ∞
0
dxxαe−xL(α)
n (x)L(α)
m (x) = δnm
(n + α)!
n!
.
(6.30)
Given that the standard associated Laguerre polynomials have as a leading term
(−1)N/N! zN and that ψ(x) is monic, we ﬁnally ﬁnd
ψ(x) =
0
(−1)NN! T −NL(T −N−2)(T x)
real symmetric
(β = 1),
(−1)NN! T −NL(T −N−1)(T x)
complex Hermitian
(β = 2).
(6.31)
Hence, the most likely positions of the Coulomb–Wishart charges are given by the zeros
of associated Laguerre polynomials, exactly as the most likely positions of the Coulomb–
Wigner charges are given by the zeros of Hermite polynomials.
We should nevertheless check that cN = T /N is compatible with Eq. (6.26), i.e. that the
following equality holds:
T
N = α + 1
N2
N

k=1
1
λ∗
k
,
(6.32)

6.2 Laguerre Polynomials
89
0
200
400
600
l
0.000
0.001
0.002
0.003
0.004
r(l)
Figure 6.2 Histogram of the 100 zeros of L(100)
100 (x). The full line is the Marˇcenko–Pastur distribution
(4.43) with q = 1
2 scaled by a factor T = 200.
where the λ∗
k are the zeros of T −1L(T x), i.e. λ∗
k = ℓ(α)
k /T , where ℓ(α)
k
are the zeros of the
associate Laguerre polynomials L(α)
N , which indeed obey the following relation:3
1
N
N

k=0
1
ℓ(α)
k
=
1
α + 1.
(6.33)
From the results of Section 5.3.1, we thus conclude that the zeros of the Laguerre polyno-
mials L(T −N−2)(T x) converge to a Wishart distribution with q = N/T . Figure 6.2 shows
the histogram of zeros of L(100)
100 (x) with the asymptotic prediction for large N and T . Note
that α ≈N(q−1 −1) in that limit.
6.2.2 Average Characteristic Polynomial
As in the Wigner case we would like to get a recursion relation for Qq,N(z) := E[det(z 1−
W(N)
q
)], where W(N)
q
is a white Wishart matrix of size N and parameter q = T /N. This
time the recursion will be over T at N ﬁxed. So we keep N ﬁxed (we will drop the (N)
index to keep the notation light) and consider an unnormalized white Wishart matrix:
YT =
T

t=1
vtvT
t ,
(6.34)
where vt are T N-dimensional independent random vectors uniformly distributed on the
sphere. We want to compute
3 See e.g. Alıcı and Taeli [2015], where other inverse moments of the ℓ(α)
k
are also derived.

90
Eigenvalues and Orthogonal Polynomials
qT,N(z) = E[det(z 1 −YT )].
(6.35)
The properly normalized expected characteristic polynomial is then given by Qq,N(z) =
T −nqT,N(T z). To construct our recursion relation, we will make use of the Shermann–
Morrison formula (Eq. (1.30)), which states that for an invertible matrix A and vectors u
and v,
det(A + uvT ) = (1 + vT A−1u) det A.
(6.36)
Applying this formula with A = z 1 −YT −1, we get
det(z 1 −YT ) = (1 −vT
T (z 1 −YT −1)−1vT ) det(z 1 −YT −1).
(6.37)
The vector vT is independent of the rest of YT −1, so taking the expectation value with
respect to this last vector we get
EvT
&
vT
T (z 1 −YT −1)−1vT
'
= Tr[(z 1 −YT −1)−1].
(6.38)
Now, using once again the general relation (easily derived in the basis where A is diagonal),
Tr[(z 1 −A)−1] det(z 1 −A) = d
dz det(z 1 −A),
(6.39)
with A = YT −1, we can take the expectation value of Eq. (6.37). We obtain
qT,N(z) =

1 −d
dz

qT −1,N(z).
(6.40)
To start the recursion relation, we note that Y0 is the N-dimensional zero matrix for which
q0(z) = zN. Hence,4
qT,N(z) =

1 −d
dz
T
zN.
(6.41)
If we apply an extra

1 −d
dz

to Eq. (6.41), we get the following recursion relation:
qT +1,N(z) = qT,N(z) −NqT,N−1(z),
(6.42)
which is similar to the classic “three-point rule” for Laguerre polynomials:
L(α+1)
N
(x) = L(α)
N (x) + L(α+1)
N−1 (x).
(6.43)
This allows us to make the identiﬁcation
qT,N(z) = (−1)NN! L(T −N)
N
(z).
(6.44)
The correctly normalized average characteristic polynomial ﬁnally reads:
QT,N(z) = (−1)NT −NN! L(T −N)
N
(T z).
(6.45)
4 This relation will be further discussed in the context of ﬁnite free convolutions, see Chapter 12.

6.3 Unitary Ensembles
91
Hence, the average characteristic polynomial of real Wishart matrices is a Laguerre polyno-
mial, albeit with a slightly different value of α compared to the one obtained in Eq. (6.31)
above (α = T −N instead of α = T −N −2). The difference however becomes small
when N,T →∞.
6.3 Unitary Ensembles
In this section we will discuss the average characteristic polynomial for unitary ensem-
bles, ensembles of complex Hermitian matrices that are invariant under unitary transfor-
mations. Although this book mainly deals with real symmetric matrices, more is known
about complex Hermitian matrices, so we want to give a few general results about these
matrices that do not have a known equivalent in the real case. The main reason unitary
ensembles are easier to deal with than orthogonal ones has to do with the Vandermonde
determinant which is needed to change variables from matrix elements to eigenvalues.
Recall that
| det((M))| = | det V|β with V =
⎛
⎜⎜⎜⎜⎜⎜⎝
1
1
1
. . .
1
λ1
λ2
λ3
. . .
λN
λ2
1
λ2
2
λ2
3
. . .
λ2
N
...
...
...
...
...
λN−1
1
λN−1
2
λN−1
3
. . .
λN−1
N
⎞
⎟⎟⎟⎟⎟⎟⎠
(6.46)
in the orthogonal case β = 1, the absolute value sign is needed to get the correct result.
In the case β = 2, (det V)2 is automatically positive and no absolute value is needed.
The absolute value for β = 1 is very hard to deal with analytically, while for β = 2 the
Vandermonde determinant is a polynomial in the eigenvalues.
6.3.1 Complex Wigner
In Section 6.1.2 we have shown that the expected characteristic polynomial of a unit vari-
ance real Wigner matrix is given by the Nth Hermite polynomial properly rescaled. The
argument relied on two facts: (i) the expectation value of any element of a Wigner matrix
is zero and (ii) all matrix elements are independent, save for
E[WijWji] = 1/N for i  j.
(6.47)
These two properties are shared by complex Wigner matrices. Therefore, the expected
characteristic polynomial of a complex Wigner matrix is the same as for a real Wigner
of the same size. We have shown that for a real or complex Wigner of size N,
QN(z) := E[det(z 1 −W)] = N−N/2HN(
√
Nz),
(6.48)

92
Eigenvalues and Orthogonal Polynomials
where HN(x) is the Nth Hermite polynomial, i.e. the Nth monic polynomial orthogonal in
the following sense:
 ∞
−∞
Hi(x)Hj(x)e−x2/2 = 0 when i  j.
(6.49)
We can actually absorb the factors of N in QN(z) in the measure exp(−x2/2) and realize
that the polynomial QN(z) is the Nth monic polynomial orthogonal with respect to the
measure
wN(x) = exp(−Nx2/2).
(6.50)
There are two important remarks to be made about the orthogonality of QN(x) with respect
to the measure wN(x). First, QN(x) is the Nth in a series of orthogonal polynomials with
respect to an N-dependent measure. In particular QM(x) for M  N is an orthogonal
polynomial coming from a different measure. Second, the measure wN(x) is exactly the
weight coming from the matrix potential exp(−βNV (λ)/2) for β = 2 and V (x) = x2/2.
In Section 6.3.3, we will see that these two statements are true for a general potential V (x)
when β = 2.
6.3.2 Complex Wishart
A complex white Wishart matrix can be written as a normalized sum of rank-1 complex
Hermitian projectors:
W = 1
T
T

t=1
vtv†
t ,
(6.51)
where the vectors vt are vectors of iid complex Gaussian numbers with zero mean and
normalized as
E[vtv†
t ] = 1.
(6.52)
The derivation of the average characteristic polynomial in the Wishart case in Section 6.2.2
only used the independence of the vectors vt and the expectation value E[vtvT
t ] = 1. So,
by replacing the matrix transposition by the Hermitian conjugation in the derivation we
can show that the expected characteristic polynomial of a complex white Wishart of size
N is also given by a Laguerre polynomial, as in Eq. (6.45). The Laguerre polynomials
L(T −N)
k
(x) are orthogonal in the sense of Eq. (6.30), with α = T −N. As in the Wigner
case, we can include the extra factor of T in the orthogonality weight and realize that the
expected characteristic polynomial of a real or complex Wishart matrix is the Nth monic
polynomial orthogonal with respect to the weight:
wN(x) = xT −Ne−T x for 0 ≤x < ∞.
(6.53)
This weight function is precisely the single eigenvalue weight, without the Vandermonde
term, of a complex Hermitian white Wishart of size N (see the footnote on page 46).

6.3 Unitary Ensembles
93
Note that the normalization of the weight wN(x) is irrelevant: the condition that the poly-
nomial is monic uniquely determines its normalization. Note as well that the real case is
given by the same polynomials, i.e. polynomials that are orthogonal with respect to the
complex weight, which is different from the real weight.
6.3.3 General Potential V (x)
The average characteristic polynomial for a matrix of size N in a unitary ensemble with
potential V (M) is given by
QN(x) := E[det(z 1 −M)],
(6.54)
which we can express via the joint law of the eigenvalues of M:
QN(z) ∝

dNx
N
,
k=1
(z −xk) 2(x)e−N N
k=1 V (xk),
(6.55)
where (x) is the Vandermonde determinant:
(x) =
,
k<ℓ
(xℓ−xk).
(6.56)
We do not need to worry about the normalization of the above expectation value as we know
that QN(z) is a monic polynomial of degree N. In other words, the condition QN(z) =
zN + O(zN−1) is sufﬁcient to properly normalize QN(z). The ﬁrst step of the computation
is to combine one of the two Vandermonde determinants with the product of (z −xk):
2(x)
N
,
k=1
(z −xk) = (x)
,
k<ℓ
(xℓ−xk)
N
,
k=1
(z −xk) ≡(x)(x;z),
(6.57)
where (x;z) is a Vandermonde determinant of N + 1 variables, namely the N variables
xk and the extra variable z.
The second step is to write the determinants in the Vandermonde form:
(x) = det
⎛
⎜⎜⎜⎜⎜⎜⎝
1
1
1
. . .
1
x1
x2
x3
. . .
xN
x2
1
x2
2
x2
3
. . .
x2
N
...
...
...
...
...
xN−1
1
xN−1
2
xN−1
3
. . .
xN−1
N
⎞
⎟⎟⎟⎟⎟⎟⎠
.
(6.58)
We can add or subtract to any line a multiple of any other line and not change the above
determinant. By doing so we can transform all monomials xk
ℓinto a monic polynomial of
degree k of our choice, so we have

94
Eigenvalues and Orthogonal Polynomials
(x) = det
⎛
⎜⎜⎜⎜⎜⎝
1
1
1
. . .
1
p1(x1)
p1(x2)
p1(x3)
. . .
p1(xN)
p2(x1)
p2(x2)
p2(x3)
. . .
p2(xN)
...
...
...
...
...
pN−1(x1)
pN−1(x2)
pN−1(x3)
. . .
pN−1(xN)
⎞
⎟⎟⎟⎟⎟⎠
.
(6.59)
We will choose the polynomials pn(x) to be the monic polynomials orthogonal with respect
to the measure w(x) = e−NV (x), this will turn out to be extremely useful. We can now
perform the integral of the vector x in the following expression:
QN(z) ∝

dNx (x)(x;z)e−N N
k=1 V (xk).
(6.60)
If we expand the two determinants (x) and (x;z) as signed sums over all permutations
and take their product, we realize that in each term each variable xk will appear exactly
twice in two polynomials, say pn(xk) and pm(xk), but by orthogonality we have

dxkpn(xk)pm(xk)e−NV (xk) = Znδmn,
(6.61)
where Zn is a normalization constant that will not matter in the end. The only terms that will
survive are those for which every xk appears in the same polynomial in both determinants.
For this to happen, the variable z must appear as pN(z), the only polynomial not in the ﬁrst
determinant. So this trick allows us to conclude with very little effort that
QN(z) ∝pN(z).
(6.62)
But since both QN(z) and pN(z) are monic, they must be equal. We have just shown that
for a Hermitian matrix M of size N drawn from a unitary ensemble with potential V (x),
E[det(z 1 −M)] = pN(z),
(6.63)
where pN(x) is the Nth monic orthogonal polynomial with respect to the measure e−NV (x).
It is possible to generalize this result to expectation of products of characteristic poly-
nomials evaluated at K different points zk, which allows one to study the joint distribution
of K eigenvalues. We give here the result without proof.5 We ﬁrst deﬁne the expectation
value of a product of K characteristic polynomials:
FK(z1,z2, . . . ,zK) := E[det(z11 −M) det(z21 −M). . . det(zK1 −M)].
(6.64)
The multivariate function FK can be expressed as a determinant of orthogonal
polynomials:
FK(z1,z2, . . . ,zK) = 1
 det
⎛
⎜⎜⎝
pN(z1)
pN(z2)
. . .
pN(zK)
pN+1(z1)
pN+1(z2)
. . .
pN+1(zK)
...
...
...
...
pN+K−1(z1)
pN+K−1(z2)
. . .
pN+K−1(zK)
⎞
⎟⎟⎠,
(6.65)
5 See Br´ezin and Hikami [2011] for a derivation. Note that their formula equivalent to Eq. (6.67) is missing the K-dependent
constant factor.

6.3 Unitary Ensembles
95
where  := (z1,z2, . . . ,zK) is the usual Vandermonde determinant and the pℓ(x) are
the monic orthogonal polynomials orthogonal with respect to e−NV (x). When K = 1,
 = 1 by deﬁnition and we recover our previous result F1(z) = pN(z). When the
arguments of FK are not all different, Eq. (6.65) gives an undetermined result (0/0) but
the limit is well deﬁned. A useful case is when all arguments are equal:
FK(z) := FK(z,z, . . . ,z) = E[det(z 1 −M)K].
(6.66)
Taking the limit of Eq. (6.65) we ﬁnd the rather simple result
FK(z) =
1
K−1
ℓ=0 ℓ!
det
⎛
⎜⎜⎜⎜⎜⎝
pN(z)
p′
N(z)
. . .
p(K−1)
N
(z)
pN+1(z)
p′
N+1(z)
. . .
p(K−1)
N+1 (z)
...
...
...
...
pN+K−1(z)
p′
N+K−1(z)
. . .
p(K−1)
N+K−1(z)
⎞
⎟⎟⎟⎟⎟⎠
,
(6.67)
where p(k)
ℓ(x) is the kth derivative of the ℓth polynomial. In particular the average-square
characteristic polynomial is given by
F2(z) = pN(z)p′
N+1(z) −p′
N(z)pN+1(z).
(6.68)
Exercise 6.3.1
Variance of the Characteristic Polynomial of a 2 × 2 Hermitian
Wigner Matrix
(a)
Show that the characteristic polynomial of a 2 × 2 Hermitian Wigner matrix
is given by
QW
2 (z) = (z −w11)(z −w22) −(wR
12)2 −(wI
12)2,
(6.69)
where w11,w22,wR
12 and wI
12 are four real independent Gaussian random
numbers with variance 1 for the ﬁrst two and 1/2 for the other two.
(b)
Compute directly the mean and the variance of QW
2 (z).
(c)
Use Eqs. (6.63) and (6.68) and the ﬁrst few Hermite polynomials given in
Section 6.1.1 to obtain the same result, namely V[QW
2 (z)] = 2z2 + 2.
Bibliographical Notes
• On orthogonal polynomials, deﬁnitions and recursion relations:
– G. Szeg˝o. Orthogonal Polynomials. AMS Colloquium Publications, volume 23.
American Mathematical Society, 1975,
– D. Zwillinger, V. Moll, I. Gradshteyn, and I. Ryzhik, editors. Table of Integrals,
Series, and Products (Eighth Edition). Academic Press, New York, 2014,
– R. Beals and R. Wong. Special Functions and Orthogonal Polynomials. Cambridge
Studies in Advanced Mathematics. Cambridge University Press, Cambridge, 2016.

96
Eigenvalues and Orthogonal Polynomials
• On the relation between Hermite polynomials, log gases and the Calogero model:
– S. Agarwal, M. Kulkarni, and A. Dhar. Some connections between the classical
Calogero-Moser model and the log gas. Journal of Statistical Physics, 176(6), 1463–
1479, 2019.
• Orthogonal polynomials and random matrix theory:
– M. L. Mehta. Random Matrices. Academic Press, San Diego, 3rd edition, 2004,
– E. Br´ezin and S. Hikami. Characteristic polynomials. In The Oxford Handbook of
Random Matrix Theory. Oxford University Press, Oxford, 2011,
– P. Deift. Orthogonal Polynomials and Random Matrices: A Riemann-Hilbert Approach.
Courant Institute, New York, 1999.
• For identities obeyed by the zeros of Laguerre polynomials:
– H. Alıcı and H. Taeli. Uniﬁcation of Stieltjes-Calogero type relations for the zeros
of classical orthogonal polynomials. Mathematical Methods in the Applied Sciences,
38(14):3118–3129, 2015.

7
The Jacobi Ensemble*
So far we have encountered two classical random matrix ensembles, namely Wigner and
Wishart. They are, respectively, the matrix equivalents of the Gaussian and the gamma
distribution. For example a 1 × 1 Wigner matrix is a single Gaussian random number and
a 1 × 1 Wishart is a gamma distributed number (see Eq. (4.16) with N = 1). We also
saw in Chapter 6 that these ensembles are intimately related to classical orthogonal poly-
nomials, respectively Hermite and Laguerre. The Gaussian distribution and its associated
Hermite polynomials appear very naturally in contexts where the underlying variable is
unbounded above and below. Gamma distributions and Laguerre polynomials appear in
problems where the variable is bounded from below (e.g. positive variables). Variables
that are bounded both from above and from below have their own natural distribution
and associated classical orthogonal polynomials, namely the beta distribution and Jacobi
polynomials.
In this chapter, we introduce a third classical random matrix ensemble: the Jacobi
ensemble. It is the random matrix equivalent of the beta distribution (and hence often
called matrix variate beta distribution). It will turn out to be strongly linked to Jacobi
orthogonal polynomials.
Jacobi matrices appear in multivariate analysis of variance and hence the Jacobi ensem-
ble is sometimes called the MANOVA ensemble. An important special case of the Jacobi
ensemble is the arcsine law which we already encountered in Section 5.5, and will again
encounter in Section 15.3.1. It is the law governing Coulomb repelling eigenvalues with
no external forces save for two hard walls. It also shows up in simple problems of matrix
addition and multiplications for matrices with only two eigenvalues.
7.1 Properties of Jacobi Matrices
7.1.1 Construction of a Jacobi Matrix
A beta-distributed random variable x ∈(0,1) has the following law:
Pc1,c2(x) = 
(c1)
(c2)

(c1 + c2) xc1−1(1 −x)c2−1,
(7.1)
where c1 > 0 and c2 > 0 are two parameters characterizing the law.
97

98
The Jacobi Ensemble
To generalize (7.1) to matrices, we could deﬁne J as a matrix generated from a beta
ensemble with a matrix potential that tends to V (x) = −log(Pc1,c2(x)) in the large N
limit. Although this is indeed the result we will get in the end, we would rather use a more
constructive approach that will give us a sensible deﬁnition of the matrix J at ﬁnite N and
for the three standard values of β.
A beta(c1,c2) random number can alternatively be generated from two gamma-
distributed variables:
x =
w1
w1 + w2
,
w1,2 ∼Gamma(c1,2,1).
(7.2)
The same relation can be rewritten as
x =
1
1 + w−1
1 w2
.
(7.3)
This is the formula we need for our matrix generalization. An unnormalized white Wishart
with T = cN is the matrix generalization of a Gamma(c,1) random variable. Combining
two such matrices as above will give us our Jacobi random matrix J. One last point before
we proceed, we need to symmetrize the combination w−1
1 w2 to yield a symmetric matrix.
We choose √w2w−1
1
√w2 which makes sense as Wishart matrices (like gamma-distributed
numbers) are positive deﬁnite.
We can now deﬁne the Jacobi matrix. Let E be the symmetrized product of a white
Wishart and the inverse of another independent white Wishart, both without the usual 1/T
normalization:
E = 
W1/2
2

W−1
1 
W1/2
2 ,
where

W1,2 := H1,2HT
1,2.
(7.4)
The two matrices H1,2 are rectangular matrices of standard Gaussian random numbers with
aspect ratio c1 = T1/N and c2 = T2/N (note that the usual aspect ratio is q = c−1). The
standard Jacobi matrix is deﬁned as
J = (1 + E)−1.
(7.5)
A Jacobi matrix has all its eigenvalues between 0 and 1. For the matrices 
W1,2 to make
sense we need c1,2 > 0. In addition, to ensure that 
W1 is invertible we need to impose
c1 > 1. It turns out that we can relax that assumption later and the ensemble still makes
sense for any c1,2 > 0.
7.1.2 Joint Law of the Elements
The joint law of the elements of a Jacobi matrix for β = 1,2 or 4 is given by
Pβ (J) = cβ,T1,T2
J
[det(J)]β(T1−N+1)/2−1[det(1 −J)]β(T2−N+1)/2−1,
(7.6)
cβ,T1,T2
J
=
N
,
j=1

(1 + β/2)
(β(T1 + T2 −N + j)/2)

(1 + βj/2)
(β(T1 −N + j)/2)
(β(T2 −N + j)/2),
(7.7)

7.1 Properties of Jacobi Matrices
99
over the space of matrices of the proper symmetry such that both J and 1 −J are positive
deﬁnite.
To obtain this result, one needs to know the law of Wishart matrices (Chapter 4) and the
law of a matrix given the law of its inverse (Chapter 1).
Here is the derivation in the real symmetric case. We ﬁrst write the law of the matrix E
by realizing that for a ﬁxed matrix 
W1, the matrix E/T2 is a Wishart matrix with T = T2
and true covariance 
W−1
1 . From Eq. (4.16), we thus have
P (E|W1) = (det E)(T2−N−1)/2(det 
W1)T2/2
2NT2/2
N(T2/2)
exp
(
−1
2 Tr

E
W1
)
.
(7.8)
The matrix 
W1/T1 is itself a Wishart with probability
P

W1

= (det 
W1)(T1−N−1)/2
2NT1/2
N(T1/2)
exp
(
−1
2 Tr

W1
)
.
(7.9)
Averaging Eq. (7.8) with respect to 
W1 we ﬁnd
P (E) =
(det E)(T2−N−1)/2
2N(T1+T2)/2
N(T1/2)
N(T2/2)
×

dW(det W)(T1+T2−N−1)/2 exp
(
−1
2 Tr ((1 + E)W)
)
.
(7.10)
We can perform the integral over W by realizing that W/T is a Wishart matrix with
T = T1 + T2 and true covariance C = (1 + E)−1, see Eq. (4.16). We just need to
introduce the correct power of det C and numerical factors to make the integral equal to 1.
Thus,
P (E) =

N((T1 + T2)/2)

N(T1/2)
N(T2/2)(det E)(T2−N−1)/2(det(1 + E))−(T1+T2)/2.
(7.11)
The miracle that for any N one can integrate exactly the product of a Wishart matrix and
the inverse of another Wishart matrix will appear again in the Bayesian theory of scm (see
Section 18.3).
Writing E+ = E + 1 we ﬁnd
P (E+) =

N((T1 + T2)/2)

N(T1/2)
N(T2/2)(det(E+ −1))(T2−N−1)/2(det E+)−(T1+T2)/2. (7.12)
Finally we want J := E−1
+ . The law of the inverse of a symmetric matrix A = M−1 of
size N is given by (see Section 1.2.7)
PA(A) = PM(A−1) det(A)−N−1.
(7.13)
Hence,
P (J) =

N((T1 + T2)/2)

N(T1/2)
N(T2/2)

det(J−1 −1)
(T2−N−1)/2
(det J)(T1+T2)/2−N−1 .
(7.14)
Using det(J−1 −1) det(J) = det(1 −J) we can reorganize the powers of the determinants
and get
P (J) =

N((T1 + T2)/2)

N(T1/2)
N(T2/2)(det(1 −J))(T2−N−1)/2(det J)(T1−N−1)/2,
(7.15)
which is equivalent to Eq. (7.6) for β = 1.

100
The Jacobi Ensemble
7.1.3 Potential and Stieltjes Transform
The Jacobi ensemble is a beta ensemble satisfying Eq. (5.2) with matrix potential
V (x) = −T1 −N + 1 −2/β
N
log(x) −T2 −N + 1 −2/β
N
log(1 −x).
(7.16)
The derivative of the potential, in the large N limit, is given by
V ′(x) = c1 −1 −(c1 + c2 −2)x
x(x −1)
.
(7.17)
The function V ′(x) is not a polynomial or a Laurent polynomial but the function
x(x −1)V ′(x) is a degree one polynomial. With a slight modiﬁcation of the argument of
Section 5.2.2, we can show that x(x −1)(x) = r + sx is a degree one polynomial. In the
large N limit we then have
g(z) =
c1 −1 −(c+ −2)z +
±
⃝/
c2
+z2 −2(c1c+ + c−)z + (c1 −1)2
2z(z −1)
,
(7.18)
where we have used the fact that we need s = 0 and r = 1 + c1 + c2 to get a 1/z behavior
at inﬁnity; we used the shorthand c± = c2 ± c1.
From the large z limit of (7.18) one can read off the normalized trace (or the average
eigenvalue):
τ(J) =
c1
c1 + c2
,
(7.19)
which is equal to one-half when c1 = c2. For c1 > 1 and c2 > 1, there are no poles, and
eigenvalues exist only when the argument of the square-root is negative. The density of
eigenvalues is therefore given by (see Fig. 7.1)
0.0
0.2
0.4
0.6
0.8
1.0
l
0.0
0.5
1.0
1.5
2.0
2.5
r(l)
Figure 7.1 Density of eigenvalues for the Jacobi ensemble with c1 = 5 and c2 = 2. The histogram
is a simulation of a single N = 1000 matrix with the same parameters.

7.1 Properties of Jacobi Matrices
101
ρ(λ) = c+
*
(λ+ −λ)(λ −λ−)
2πλ(1 −λ)
,
(7.20)
where the edges of the spectrum are given by
λ± = c1c+ + c−± 2
*
c1c2(c+ −1)
c2
+
.
(7.21)
For 0 < c1 < 1 or 0 < c2 < 1, Eq. (7.18) will have Dirac deltas at z = 0 or z = 1,
depending on cases (see Exercise 7.1.1).
In the symmetric case c1 = c2 = c, we have explicitly
g(z) = (c −1)(1 −2z) +
±
⃝*
c2(2z −1)2 −c(c −2)
2z(z −1)
.
(7.22)
The density for c ≥1 has no Dirac mass and is given by
ρ(λ) = c
*
(λ+ −λ)(λ −λ−)
πλ(1 −λ)
,
(7.23)
with the edges given by
λ± = 1
2 ±
√
2c −1
2c
.
(7.24)
Note that the distribution is symmetric around λ = 1/2 (see Fig. 7.2).
As c →1, the edges tend to 0 and 1 and we recover the arcsine law:
ρ(λ) =
1
π √λ(1 −λ).
(7.25)
0.0
0.2
0.4
0.6
0.8
1.0
l
0
1
2
3
r(l)
c = 10
c = 2
arcsine law
Figure 7.2 Density of eigenvalues for a Jacobi matrix in the symmetric case (c1 = c2 = c) for
c = 20,2 and 1. The case c = 1 is the arcsine law.

102
The Jacobi Ensemble
Exercise 7.1.1
Dirac masses in the Jacobi density
(a)
Assuming that c1 > 1 and c2 > 1 show that there are no poles in Eq. (7.18) at
z = 0 or z = 1 by showing that the numerator vanishes for these two values
of z.
(b)
The parameters c1,2 can be smaller than 1 (as long as they are positive). Show
that in that case g(z) can have poles at z = 0 and/or z = 1 and ﬁnd the residue
at these poles.
7.2 Jacobi Matrices and Jacobi Polynomials
7.2.1 Centered-Range Jacobi Ensemble
The standard Jacobi matrix deﬁned above has all its eigenvalues between 0 and 1. We would
like to use another deﬁnition of the Jacobi matrix with eigenvalues between −1 and 1. This
will make easier the link with orthogonal polynomials. We deﬁne the centered-range Jacobi
matrix1
Jc = 2J −1.
(7.26)
This deﬁnition is equivalent to
Jc = 
W−1/2
+
(
W−)
W−1/2
+
, where 
W± = H1HT
1 ± H2HT
2,
(7.27)
with H1,2 as above.
The matrix Jc is still a member of a beta ensemble satisfying Eq. (5.2) with a slightly
modiﬁed matrix potential:
NV (x) = −(T1 −N + 1 −2/β) log(1 + x) −(T2 −N + 1 −2/β) log(1 −x). (7.28)
In the large N limit, the density of eigenvalues can easily be obtained from (7.20):
ρ(λ) = c+
*
(λ+ −λ)(λ −λ−)
2π(1 −λ2)
,
(7.29)
where the edges of the spectrum are given by
λ± = c−(2 −c+) ± 4
*
c1c2(c+ −1)
c2
+
.
(7.30)
The special case c1 = c2 = 1 is the centered arcsine law:
ρ(λ) =
1
π
*
(1 −λ2)
.
(7.31)
1 Note that the matrix Jc is not necessarily centered in the sense τ(Jc) = 0 but the potential range of its eigenvalues is [−1,1]
centered around zero.

7.2 Jacobi Matrices and Jacobi Polynomials
103
7.2.2 Average Expected Characteristic Polynomial
In Section 6.3.3, we saw that the average characteristic polynomial when β = 2 is the Nth
monic polynomial orthogonal to the weight w(x) ∝exp(−NV (x)). For the centered-range
Jacobi matrix we have
w(x) = (1 + x)T1−N(1 −x)T2−N.
(7.32)
The Jacobi polynomials P (a,b)
n
(x) are precisely orthogonal to such weight functions with
a = T2 −N and b = T1 −N. (Note that unfortunately the standard order of the param-
eters is inverted with respect to Jacobi matrices.) Jacobi polynomials satisfy the following
differential equation:
(1 −x2)y′′ + (b −a −(a + b + 2)x)y′ + n(n + a + b + 1)y = 0.
(7.33)
This equation has polynomial solutions if and only if n is an integer. The solution is then
y ∝P (a,b)
n
(x).
The ﬁrst three Jacobi polynomials are
P (a,b)
0
(x) = 1,
P (a,b)
1
(x) = (a + b + 2)
2
x + a −b
2
,
(7.34)
P (a,b)
2
(x) = (a + b + 3)(a + b + 4)
8
(x −1)2 + (a + 2)(a + b + 3)
2
(x −1)
+ (a + 1)(a + 2)
2
.
The normalization of Jacobi polynomials is arbitrary but in the standard normalization
they are not monic. The coefﬁcient of xn in P (a,b)
n
(x) is
an =

[a + b + 2n + 1]
2nn! 
[a + b + n −1].
(7.35)
In summary we have (for β = 2)
E [det[z1 −Jc]] = 2NN! 
[T1 + T2 + 1 −N]

[T1 + T2 + 1]
P (T2−N,T1−N)
N
(z).
(7.36)
Note that we must have T1 ≥N and T2 ≥N.
When T1 = T2 = N (i.e. c1 = c2 = 1, corresponding to the arcsine law), the
polynomials P (0,0)
N
(z) are called Legendre polynomials PN(z):2
E [det[z1 −Yc]] = 2N(N! )2
(2N)! PN(z).
(7.38)
2 Legendre polynomials are deﬁned as the polynomial solution of
d
dx
(
(1 −x2) dPn(x)
dx
)
+ n(n + 1)Pn(x) = 0,
Pn(1) = 1.
(7.37)

104
The Jacobi Ensemble
7.2.3 Maximum Likelihood Conﬁguration at Finite N
In Chapter 6, we studied the most likely conﬁguration of eigenvalues for the beta ensemble
at ﬁnite N. We saw that the ﬁnite-N Stieltjes transform of this solution gN(z) is related to
a monic polynomial ψ(x) via
gN(z) = 1
N
N

i=1
1
z −λi
= ψ′(z)
Nψ(z),
where
ψ(x) =
N
,
i=1
(x −λi).
(7.39)
The polynomial ψ(x) satisﬁes Eq. (6.23) which we recall here:
ψ′′(x) −NV ′(x)ψ′(x) + N2N(x)ψ(x) = 0,
(7.40)
where the function N(x) is deﬁned by Eq. (5.32). For the case of the centered-range
Jacobi ensemble we have
NV ′(x) = a −b + (a + b + 2)x
1 −x2
,
(7.41)
where we have anticipated the result by introducing the notation a = T2 −N −2/β and
b = T1 −N −2/β. The function N(x) is given by
N(x) = rN + sNx
1 −x2 .
(7.42)
We will see below that the coefﬁcient sN is zero because of the symmetry {c1,c2,λk} →
{c2,c1, −λk} of the most likely solution.
The equation for ψ(x) becomes
(1 −x2)ψ′′(x) + (b −a −(a + b + 2)x)ψ′(x) + rNN2ψ(x) = 0.
(7.43)
We recognize the differential equation satisﬁed by the Jacobi polynomials (7.33). Its solu-
tions are polynomials only if rNN = N + a + b + 1, which implies that rN = c1 + c2 −
1 + (1 −4β)/N. This is consistent with the large N limit r = c1 + c2 −1 in Section 7.1.3.
The solutions are given by
ψ(x) ∝P (a,b)
N
(x),
(7.44)
with the proportionality constant chosen such that ψ(x) is monic.
In the special case T1 = T2 = N + 2/β, i.e. T = N + 2 for real symmetric matrices
and T = N + 1 for complex Hermitian matrices, we have a = b = 0 and the polynomials
reduce to Legendre polynomials.
Another special case corresponds to Chebyshev polynomials:
Tn(x) = P

−1
2,−1
2

n
(x)
and
Un(x) = P

1
2, 1
2

n
(x),
(7.45)
where Tn(x) and Un(x) are the Chebyshev polynomials of ﬁrst and second kind respec-
tively. Since a = T2 −N −2/β and b = T1 −N −2/β, they appear as solutions for
T1 = T2 = N + 2/β −1/2 (ﬁrst kind) or T1 = T2 = N + 2/β + 1/2 (second kind).
These values of T1 = T2 are not integers but we can still consider the matrix potential

7.2 Jacobi Matrices and Jacobi Polynomials
105
−1.0
−0.5
0.0
0.5
1.0
l
0.00
0.25
0.50
0.75
1.00
1.25
r(l)
Figure 7.3 Histogram of the 200 zeros of P (200,600)
200
(x). The full line is the Jacobi eigenvalue density,
Eq. (7.29), with c1 = 4 and c2 = 2.
given by Eq. (7.28) without the explicit Wishart matrix construction. In the large N limit,
the density of the zeros of the Jacobi polynomials P (T2−N,T1−N)
N
(x) is given by Eq. (7.29)
with c1,2 = T1,2/N. Figure 7.3 shows a histogram of the zeros of P (200,600)
200
(x).
When c1 →1 and c2 →1, the density becomes the centered arcsine law. As a conse-
quence, we have shown that the zeros of Chebyshev (both kinds) and Legendre polynomials
(for which T1 = T2 = N + O(1)) are distributed according to the centered arcsine law in
the large N limit.
We have seen that in order for Eq. (7.40) to have polynomial solutions we must have
N(1 −x2)N(x) = N + a + b + 1,
(7.46)
where the function N(x) is deﬁned from the most likely conﬁguration or, equivalently,
the roots of the Jacobi polynomial P (a,b)
N
(z) by
N(x) = 1
N
N

k=0
V ′(x) −V ′(λk)
x −λk
.
(7.47)
From these expressions we can ﬁnd a relationship that roots of Jacobi polynomials must
satisfy. Indeed, injecting Eq. (7.41), we ﬁnd
N(1 −x2)N(x) =
N

k=1
(a −b)(x2 −λ2
k) + (a + b + 2)(x −λk)(1 + xλk)
(1 −λ2
k)(x −λk)
.
(7.48)
For each k the numerator is a second degree polynomial in x that is zero at x = λk,
canceling the x −λk factor in the denominator, so the whole expression is a ﬁrst degree
polynomial. Equating this expression to Eq. (7.47), we ﬁnd that the term linear in x of

106
The Jacobi Ensemble
this polynomial must be zero and the constant term equal to N + a + b + 1, yielding two
equations:
1
N
N

k=1
(a + b + 2)λk + (a −b)
(1 −λ2
k)
= 0,
(7.49)
1
N
N

k=1
(a + b + 2) + (a −b)λk
(1 −λ2
k)
= N + a + b + 1.
(7.50)
These equations give us non-trival relations satisﬁed by the roots of Jacobi polynomials
P (a,b)
N
(x). If we sum or subtract the two equations above, we ﬁnally obtain3
1
N
N

k=1
1
(1 −λk) = a + b + N + 1
2(a + 1)
,
(7.51)
1
N
N

k=1
1
(1 + λk) = a + b + N + 1
2(b + 1)
.
(7.52)
7.2.4 Discrete Laplacian in One Dimension and Chebyshev Polynomials
Chebyshev polynomials and the arcsine law are also related via a simple deterministic
matrix: the discrete Laplacian in one dimension, deﬁned as
 = 1
2
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
2
−1
0
· · ·
0
0
−1
2
−1
· · ·
0
0
0
−1
2
· · ·
0
0
0
0
−1
· · ·
0
0
0
0
0
...
0
−1
0
0
0
· · ·
−1
2
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
(7.53)
We will see that the spectrum of −1 at large N is again given by the arcsine law. One way
to obtain this result is to modify the top-right and bottom-left corner elements by adding
−1. The modiﬁed matrix is then a circulant matrix which can be diagonalized exactly (see
Exercise 7.2.1 and Appendix A.3).
We will use a different route which will also uncover the link to Chebyshev polynomials.
We will compute the characteristic polynomial QN(z) of  −1 for all values of N by
induction. The ﬁrst two are
Q1(z) = z,
(7.54)
Q2(z) = z2 −1
4.
(7.55)
3 These relations were recently obtained in Alıcı and Taeli [2015].

7.2 Jacobi Matrices and Jacobi Polynomials
107
For N ≥3 we can write a recursion relation by expanding in minors the ﬁrst line of
the determinant of z1 − + 1. The (11)-minor is just QN−1(z). The ﬁrst column of the
(12)-minor only has one element equal to 1/2; if we expand this column its only minor is
QN−2(z). We ﬁnd
QN(z) = zQN−1(z) −1
4QN−2(z).
(7.56)
This simple recursion relation is similar to that of Chebyshev polynomials UN(x):
UN(x) = 2xUN−1 −UN−2(x).
(7.57)
The standard Chebyshev polynomials are not monic but have leading term UN(x) ∼2NxN.
Monic Chebyshev (
UN(x) = 2−NUN(x)) in fact precisely satisfy Eq. (7.56). Given our
ﬁrst two polynomials are the monic Chebyshev of the second kind, we conclude that
QN(z) = 2−NUN(z) for all N. The eigenvalues of  −1 at size N are therefore given
by the zeros of the Nth Chebyshev polynomial of the second kind. In the large N limit
those are distributed according to the centered arcsine law. QED.
We will see in Section 15.3.1 that the sum of two random symmetric orthogonal matrices
also has eigenvalues distributed according to the arcsine law.
Exercise 7.2.1
Diagonalizing the Discrete Laplacian
Consider M =  −1 with −1/2 added to the top-right and bottom-left
corners.
(a)
Show that the vectors [vk]j = ei2πkj are eigenvectors of M with eigenvalues
λk = cos(2πk).
(b)
Show that the eigenvalue density of M in the large N limit is given by the
centered arcsine law (7.31).
Bibliographical Notes
• For general references on orthogonal polynomials, see
– R. Beals and R. Wong. Special Functions and Orthogonal Polynomials. Cambridge
Studies in Advanced Mathematics. Cambridge University Press, Cambridge, 2016,
– D. Zwillinger, V. Moll, I. Gradshteyn, and I. Ryzhik, editors. Table of Integrals,
Series, and Products (Eighth Edition). Academic Press, New York, 2014.
• On relations obeyed by zeros of Jacobi polynomials, see
– H. Alıcı and H. Taeli. Uniﬁcation of Stieltjes-Calogero type relations for the zeros
of classical orthogonal polynomials. Mathematical Methods in the Applied Sciences,
38(14):3118–3129, 2015.
• On the MANOVA method, see e.g.

108
The Jacobi Ensemble
– R. T. Warne. A primer on multivariate analysis of variance (MANOVA) for behav-
ioral scientists. Practical Assessment, Research & Evaluation, 19(17):1–10, 2014.
• On the spectrum of Jacobi matrices using Coulomb gas methods, see
– H. M. Ramli, E. Katzav, and I. P. Castillo. Spectral properties of the Jacobi ensembles
via the Coulomb gas approach. Journal of Physics A: Mathematical and Theoretical,
45(46):465005, 2012.
and references therein.

Part II
Sums and Products of Random Matrices


8
Addition of Random Variables and Brownian Motion
In the following chapters we will be interested in the properties of sums (and products)
of random matrices. Before embarking on this relatively new ﬁeld, the present chapter
will quickly review some classical results concerning sums of random scalars, and the
corresponding continuous time limit that leads to the Brownian motion and stochastic
calculus.
8.1 Sums of Random Variables
Let us thus consider X = X1+X2 where X1 and X2 are two random variables, independent,
and distributed according to, respectively, P1(x1) and P2(x2). The probability that X is
equal to x (to within dx) is given by the sum over all combinations of x1 and x2 such
that x1 + x2 = x, weighted by their respective probabilities. The variables X1 and X2
being independent, the joint probability that X1 = x1 and X2 = x −x1 is equal to
P1(x1)P2(x −x1), from which one obtains
P (2)(x) =

P1(x′)P2(x −x′) dx′.
(8.1)
This equation deﬁnes the convolution between P1 and P2, which we will write P (2) =
P1 ⋆P2. The generalization to the sum of N independent random variables is immediate. If
X = X1 + X2 + · · · + XN with Xi distributed according to Pi(xi), the distribution of X is
obtained as
P (N)(x) =

N
,
i=1
dxiP1(x1)P2(x2). . . PN(xN)δ

x −
N

i=1
xi

,
(8.2)
where δ(.) is the Dirac delta function. The analytical or numerical manipulations of Eqs.
(8.1) and (8.2) are much eased by the use of Fourier transforms, for which convolutions
become simple products. The equation P (2)(x) = [P1 ⋆P2](x), reads, in Fourier space,
ϕ(2)(k) =

eik(x−x′+x′)

P1(x′)P2(x −x′) dx′ dx ≡ϕ1(k)ϕ2(k),
(8.3)
111

112
Addition of Random Variables and Brownian Motion
where ϕ(k) denotes the Fourier transform of the corresponding probability density P(x). It
is often called its characteristic (or generating) function. Since the characteristic functions
multiply, their logarithms add, i.e. the function H(k) deﬁned below is additive:
H(k) := log ϕ(k) = log E[eikX].
(8.4)
It allows one to recovers its so called cumulants cn (provided they are ﬁnite) through
cn := (−i)n dn
dzn H(k)

k=0
.
(8.5)
The cumulants cn are polynomial combinations of the moments mp with p ≤n. For
example c1 = m1 is the mean of the distribution and c2 = m2 −m2
1 = σ 2 its variance. It
is clear that the mean of the sum of two random variables (independent or not) is equal to
the sum of the individual means. The mean is thus additive under convolution. The same is
true for the variance, but only for independent variables.
More generally, from the additive property of H(k) all the cumulants of two independent
distributions simply add. The additivity of cumulants is a consequence of the linearity
of derivation. The cumulants of a given law convoluted N times with itself thus follow
the simple rule cn,N = Ncn,1, where the {cn,1} are the cumulants of the elementary
distribution P1.
An important case is when P1 is a Gaussian distribution,
P1(x) =
1
√
2πσ 2 e−(x−m)2
2σ2 ,
(8.6)
such that log ϕ1(k) = imk −σ 2k2/2. The Gaussian distribution is such that all cumulants
of order ≥3 are zero. This property is clearly preserved under convolution: the sum of
Gaussian random variables remains Gaussian. Conversely, one can always write a Gaussian
variable as a sum of an arbitrary number of Gaussian variables: Gaussian variables are
inﬁnitely divisible.
In the following, we will consider inﬁnitesimal Gaussian variables, noted dB, such that
E[dB] = 0 and E[dB2] = dt, where dt →0 is an inﬁnitesimal quantity, which we will
interpret as an inﬁnitesimal time increment. In other words, dB is a mean zero Gaussian
random variable which has ﬂuctuations of order
√
dt.
8.2 Stochastic Calculus
8.2.1 Brownian Motion
The starting point of stochastic calculus is the Brownian motion (also called Wiener pro-
cess) Xt, which is a Gaussian random variable of mean μt and variance σ 2t. From the
inﬁnite divisibility property of Gaussian variables, one can always write
Xtk =
k−1

ℓ=0
μδt +
k−1

ℓ=0
σδBℓ,
(8.7)

8.2 Stochastic Calculus
113
0.0
0.2
0.4
0.6
0.8
1.0
t
−0.75
−0.50
−0.25
0.00
0.25
Bt
Figure 8.1 An example of Brownian motion.
where tk = kt/N, 0 ≤k ≤N, δt = T /N and δBℓ∼N(0,δt) for each ℓ. By construction
X(tN) = Xt. In the limit N →∞, we have δt →dt, δBk →dB and Xtk becomes a
continuous time process with
dXt = μdt + σdBt,
X0 = 0,
(8.8)
where dBt are independent, inﬁnitesimal Gaussian variables as deﬁned above (see Fig. 8.1).
The process Xt is continuous but nowhere differentiable. Note that Xt and Xt′ are not
independent but their increments are, i.e. Xt′ and Xt −Xt′ are independent whenever t′ < t.
Note the convention that Xtk is built from past increments δBℓfor ℓ< k but does not
include δBk. This convention is called the Itˆo prescription.1 Its main advantage is that Xt
is independent of the equal-time dBt, but this comes at a price: the usual chain rule for
differentiation has to be corrected by the so-called Itˆo term, which we now discuss.
8.2.2 Itˆo’s Lemma
We now study the behavior of functions F(Xt) of a Wiener process Xt. Because dB2 is of
order dt, and not dt2, one has to be careful when evaluating derivatives of functions of Xt.
Given a twice differentiable function F(.), we consider the process F(Xt). Reverting
for a moment to a discretized version of the process, one has
F(X(t + δt)) = F(Xt) + δX F ′(Xt) + (δX)2
2
F ′′(Xt) + o(δt),
(8.9)
1 In the Stratonovich prescription, half of δBk contributes to Xtk . In this prescription, the Itˆo lemma is not needed, i.e. the chain
rule applies without any correction term, but the price to pay is a correlation between Xt and dB(t). We will not use the
Stratonovich prescription in this book.

114
Addition of Random Variables and Brownian Motion
where
δX = μδt + σδB,
(8.10)
and
(δX)2 = μ2(δt)2 + σ 2δt + σ 2 &
(δB)2 −δt
'
+ 2μσδtδB.
(8.11)
The random variable (δB)2 has mean σ 2δt so the ﬁrst and last terms are clearly o(δt) when
δt →0. The third term has standard deviation given by
√
2σ 2δt; so the third term is also
of order δt but of zero mean. It is thus a random term much like δB, but much smaller since
δB is of order
√
δt ≫δt. The Itˆo lemma is a precise mathematical statement that justiﬁes
why this term can be neglected to ﬁrst order in δt. Hence, letting δt →dt, we get
dFt = ∂F
∂XdXt + σ 2
2
∂2F
∂2Xdt.
(8.12)
When compared to ordinary calculus, there is a correction term – the Itˆo term – that depends
on the second order derivative of F.
More generally, we can consider a general Itˆo process where μ and σ themselves depend
on Xt and t, i.e.
dXt = μ(Xt,t)dt + σ(Xt,t)dBt.
(8.13)
Then, for functions F(X,t) that may have an explicit time dependence, one has
dFt = ∂F
∂XdXt +
(∂F
∂t + σ 2(Xt,t)
2
∂2F
∂2X
)
dt.
(8.14)
Itˆo’s lemma can be extended to functions of several stochastic variables. Consider a
collection of N independent stochastic variables {Xi,t} (written vectorially as Xt) and
such that
dXi,t = μi(Xt,t)dt + dWi,t,
(8.15)
where dWi,t are Wiener noises such that
E

dWi,tdWj,t

:= Cij(Xt,t)dt.
(8.16)
The vectorial form of Itˆo’s lemma states that the time evolution of a function F(Xt,t) is
given by the sum of three contributions:
dFt =
N

i=1
∂F
∂Xi
dXi,t +
⎡
⎣∂F
∂t +
N

i,j=1
Cij(Xt,t)
2
∂2F
∂Xi∂Xj
⎤
⎦dt.
(8.17)
The formula simpliﬁes when all the Wiener noises are independent, in which case the Itˆo
term only contains the second derivatives ∂2F/∂2Xi.

8.2 Stochastic Calculus
115
8.2.3 Variance as a Function of Time
As an illustration of how to use Itˆo’s formula let us recompute the time dependent variance
of Xt. Assume μ = 0 and choose F(X) = X2. Applying Eq. (8.12), we get that
dFt = 2XtdXt + σ 2dt ⇒F(Xt) = 2
 t
0
σXsdBs + σ 2t.
(8.18)
In order to take the expectation value of this equation, we scrutinize the term E[XsdBs]. As
alluded to above, the random inﬁnitesimal element dBs does not contribute to Xs, which
only depends on dBs′<s. Therefore E[XsdBs] = 0, and, as expected,
E[X2
t ] = E[F(Xt)] = σ 2t.
(8.19)
The Brownian motion has a variance from the origin that grows linearly with time. The
same result can of course be derived directly from the integrated form Xt = σBt, where Bt
is a Gaussian random number of variance equal to t.
8.2.4 Gaussian Addition
Itˆo’s lemma can be used to compute a special case of the law of addition of independent
random variables, namely when one of the variables is Gaussian. Consider the random
variable Z = Y + X, where Y is some random variable, and X is an independent Gaussian
(X ∼N(μ,σ 2)). The law of Z is uniquely determined by its characteristic function:
ϕ(k) := E[eikZ].
(8.20)
We now let Z →Zt be a Brownian motion with Z0 = Y:
dZt = μdt + σdBt,
Z0 = Y.
(8.21)
Note that Zt=1 has the same law as Z. The idea is now to study the function F(Zt) := eikZt
using Itˆo’s lemma, Eq. (8.14). Hence,
dFt = ikeikZt dZt −k2σ 2
2
eikZt dt =

ikμF −k2σ 2
2
F

dt + ikFdBt.
(8.22)
Taking the expectation value, writing ϕt(k) = E[F(t)], and noting that the differential d is
a linear operator and therefore commutes with the expectation value, we obtain
dϕt(k) =

ikμ −k2σ 2
2

ϕt(k)dt,
(8.23)
or
1
ϕt(k)
d
dt ϕt(k) = d
dt log (ϕt(k)) =

ikμ −k2σ 2
2

.
(8.24)

116
Addition of Random Variables and Brownian Motion
From its solution at t = 1, we get
log (ϕ1(k)) = log (ϕ0(k)) + ikμ −k2σ 2
2
.
(8.25)
Recognizing the last two terms in the right hand side as the characteristic function of a
Gaussian variable, we recover the fact that the log-characteristic function is additive under
the addition of independent random variables. Although the result is true in general, the
calculation above using stochastic calculus is only valid if one of the random variable is
Gaussian.
8.2.5 The Langevin Equation
We would like to construct a stochastic process for a variable Xt such that in the steady-
state regime the values of Xt are drawn from a given probability distribution P(x). To build
our stochastic process, let us ﬁrst consider the simple Brownian motion with unit variance
per unit time:
dXt = dBt.
(8.26)
As revealed by Eq. (8.19) the variance of Xt grows linearly with time and the process
never reaches a stationary state. To make it stationary we need a mechanism to limit the
variance of Xt. We cannot ‘subtract’ variance but we can reduce Xt by scaling. If at every
inﬁnitesimal time step we replace Xt+dt by Xt+dt/
√
1 + dt, the variance of Xt will remain
equal to unity. We also know that the distribution of Xt is Gaussian (if the initial condition is
Gaussian or constant). With this extra rescaling, Xt is still Gaussian at every step, so clearly
this will describe the stationary state of our rescaled process. As a stochastic differential
equation, we have, neglecting terms of order (dt)3/2
dXt = dBt +
Xt
√
1 + dt
−Xt = dBt −1
2Xtdt.
(8.27)
This stationary version of the random walk is the Ornstein–Uhlenbeck process (see
Fig. 8.2). A physical interpretation of this equation is that of a particle located at Xt
moving in a viscous medium subjected to random forces dBt/dt and a deterministic
harmonic force (“spring”) −Xt/2. The viscous medium is such that velocity (and not
acceleration) is proportional to force.
We would like to generalize the above formalism to generate any distribution P(x) for
the distribution of X in the stationary state. One way to do so is to change the linear force
−Xt/2 to a general non-linear force F(Xt) := −V ′(Xt)/2, where we have written the
force as the derivative of a potential V and introduced a factor of 2 which will prove to be
convenient. If the potential is convex, the force will drive the particle towards the minimum
of the potential while the noise dBt will drive the particle away. We expect that this system
will reach a steady state. Our stochastic equation is now
dXt = dBt + F(Xt)dt.
(8.28)

8.2 Stochastic Calculus
117
0
20
40
t
−3
−2
−1
0
1
2
Xt
−4
−2
0
2
4
X
0.0
0.1
0.2
0.3
0.4
P(X)
Figure 8.2 (left) A simulation of the Langevin equation for the Ornstein–Uhlenbeck process (8.27)
with 50 steps per unit time. Note that the correlation time is τc = 2 here, so excursions away from
zero typically take 2 time units to mean-revert back to zero. Farther excursions take longer to come
back. (right) Histogram of the values of Xt for the same process simulated up to t = 2000 and
comparison with the normal distribution. The agreement varies from sample to sample as a rare far
excursion can affect the sample distribution even for t = 2000.
What is the distribution of Xt in the steady state? To ﬁnd out, let us consider an arbi-
trary test function f (Xt) and see how it behaves in the steady state. Using Itˆo’s lemma,
Eq. (8.12), we have
df (Xt) = f ′(Xt)
(
dBt −1
2V ′(Xt)dt
)
+ 1
2f ′′(Xt)dt.
(8.29)
Taking the expectation value on both sides and demanding that dE[ft]/dt = 0 in the steady
state we ﬁnd
E

f ′(Xt)V ′(Xt)

= E

f ′′(Xt)

.
(8.30)
This must be true for any function f . In order to infer the corresponding stationary distri-
bution P(x), let us write h(x) = f ′(x) and write these expectation values as

h(x)V ′(x)P(x)dx =

h′(x)P(x)dx.
(8.31)
Since we want to relate an integral of h to one of h′ we should use integration by parts:

h′(x)P(x)dx = −

h(x)P ′(x)dx = −

h(x)P ′(x)
P(x) P(x)dx.
(8.32)
Since Eq. (8.31) is true for any function h(x) we must have

118
Addition of Random Variables and Brownian Motion
V ′(x) = −P ′(x)
P(x)
⇒
P(x) = Z−1 exp[−V (x)],
(8.33)
where Z is an integration constant that ﬁxes the normalization of the law P(x).
To recapitulate, given a probability density P(x), we can deﬁne a potential V (x) =
−log P(x) (up to an irrelevant additive constant) and consider the stochastic differential
equation
dXt = dBt −1
2V ′(Xt)dt.
(8.34)
The stochastic variable Xt will eventually reach a steady state. In that steady state the law of
Xt will be given by P(x). Equation (8.34) is called the Langevin equation. The strength of
the Langevin equation is that it allows one to replace the average over the probability P(x)
by a sample average over time of a stochastic process.2 Any rescaling of time t →σ 2t
would yield a Langevin equation with the same stationary state:
dXt = σdBt −σ 2
2 V ′(Xt)dt.
(8.35)
We have learned another useful fact from Eq. (8.30): the random variable V ′(X) acts
as a derivative with respect to X under the expectation value. In that sense V ′(X) can be
considered the conjugate variable to X.
It is very straightforward to generalize our one-dimensional Langevin equation to a set
of N variables {Xi} that are drawn from the joint law P(x) = Z−1 exp[−V (x)]. We get
dXi = σdBi + σ 2
2
∂
∂Xi
log P(x) dt,
(8.36)
where we have dropped the subscript t for clarity.
Exercise 8.2.1
Langevin equation for Student’s t-distributions
The family of Student’s t-distributions, parameterized by the tail exponent μ,
is given by the probability density
Pμ(x) = Z−1
μ

1 + x2
μ
−μ+1
2
with Z−1
μ
=


μ+1
2

√μπ
 μ
2
.
(8.37)
(a)
What is the potential V (x) and its derivative V ′(x) for these laws?
(b)
Using Eq. (8.30), show that for a t-distributed variable x we have
E
(
x2
x2 + μ
)
=
1
1 + μ.
(8.38)
2 A process for which the time evolution samples the entire set of possible values according to the stationary probability is
called ergodic. A discussion of the condition for ergodicity is beyond the scope of this book.

8.2 Stochastic Calculus
119
(c)
Write the Langevin equation for a Student’s t-distribution. What is the
μ →∞limit of this equation?
(d)
Simulate your Langevin equation for μ = 3, 20 time steps per unit time and
run the simulation for 20 000 units of time. Make a normalized histogram of
the sampled values of Xt and compare with the law for μ = 3 given above.
(e)
Compared to the Gaussian process (Ornstein–Uhlenbeck), the Student
t-process has many more short excursions but the long excursions are much
longer than the Gaussian ones. Explain this behavior by comparing the
function V ′(x) in the two cases. Describe their relative small |x| and large
|x| behavior.
8.2.6 The Fokker–Planck Equation
It is interesting to derive, from the Langevin equation Eq. (8.28), the so-called Fokker–
Planck equation that describes the dynamical evolution of the time dependent probability
density P(x,t) of the random variable Xt. The trick is to use Eq. (8.29) again, with f (x)
an arbitrary test function. Taking expectations, one ﬁnds
dE[f (Xt)] = E

f ′(Xt)F(Xt)dt

+ 1
2E[f ′′(Xt)]dt,
(8.39)
where we have used the fact that, in the Itˆo convention, E[f ′(Xt)dBt] = 0. But by deﬁni-
tion of P(x,t), one also has
E[f (Xt)] :=

f (x)P(x,t)dx.
(8.40)
Hence,

f (x)∂P(x,t)
∂t
dx =

f ′(x)F(x)P(x,t)dx + 1
2

f ′′(x)P(x,t)dx.
(8.41)
Integrating by parts the right-hand side leads to

f (x)∂P(x,t)
∂t
dx = −

f (x)∂F(x)P(x,t)
∂x
dx + 1
2

f (x)∂2P(x,t)
∂x2
dx.
(8.42)
Since this equation holds for an arbitrary test function f (x), it must be that
∂P(x,t)
∂t
= −∂F(x)P(x,t)
∂x
+ σ 2
2
∂2P(x,t)
∂x2
,
(8.43)
which is called the Fokker–Planck equation. We have reintroduced an arbitrary value
of σ here, to make the equation more general. One can easily check that when F(x) =
−V ′(x)/2, the stationary state of this equation, such that the left hand side is zero, is
P(x) = Z−1 exp[−V (x)/σ 2],
(8.44)
as expected from the previous section.

120
Addition of Random Variables and Brownian Motion
Bibliographical Notes
• A general introduction to probability theory:
– W. Feller. An Introduction to Probability Theory and Its Applications. Wiley, 1968.
• For introductory textbooks on the Langevin and Fokker–Planck equation, see
– C. W. Gardiner. Handbook of Stochastic Methods for Physics, Chemistry and the
Natural Sciences, volume 13 of Springer Series in Synergetics. Springer-Verlag,
Berlin, 2004,
– N. V. Kampen. Stochastic Processes in Physics and Chemistry. North Holland, Ams-
terdam, 2007.
• The central limit theorem:
– P. L´evy. Th´eorie de l’addition des variables al´eatoires. Gauthier Villars, Paris,
1937–1954,
– B. V. Gnedenko and A. N. Kolmogorov. Limit Distributions for Sums of Independent
Random Variables. Addison-Wesley Publishing Co., Reading, Mass., London, Don
Mills., Ont., 1968.
• For a physicist discussion of the central limit theorem, see
– J.-P. Bouchaud and M. Potters. Theory of Financial Risk and Derivative Pric-
ing: From Statistical Physics to Risk Management. Cambridge University Press,
Cambridge, 2nd edition, 2003.

9
Dyson Brownian Motion
In this chapter we would like to start our investigation of the addition of random matrices,
which will lead to the theory of so-called “free matrices”. This topic has attracted substan-
tial interest in recent years and will be covered in the next chapters.
We will start by studying how a ﬁxed large matrix (random or not) is modiﬁed when
one adds a Wigner matrix. The elements of a Wigner matrix are Gaussian random numbers
and, as we saw in the previous chapter, each of them can be written as a sum of Gaussian
numbers with smaller variance. By pushing this reasoning to the limit we can write the
addition of a Wigner matrix as a continuous process of addition of inﬁnitesimal Wigner
matrices. Such a matrix Brownian motion process, viewed through the lens of eigenvalues
and eigenvectors, is called a Dyson Brownian motion (dbm) after the physicist Freeman
Dyson who ﬁrst introduced it in 1962.
9.1 Dyson Brownian Motion I: Perturbation Theory
9.1.1 Perturbation Theory: A Short Primer
We begin by recalling how perturbation theory works for eigenvalues and eigenvectors.
This is a standard topic in elementary quantum mechanics, but is not necessarily well
known in other circles. Let us consider a matrix H = H0 + ϵH1, where H0 is a real
symmetric matrix whose eigenvalues and eigenvectors are assumed to be known, and H1 is
a real symmetric matrix that gives the perturbation, with ϵ a small parameter. (In quantum
mechanics, H0 and H1 are complex Hermitian, but the ﬁnal equations below are the same
in both cases.)
Suppose λi,0, 1 ≤i ≤N, are the eigenvalues of H0 and vi,0, 1 ≤i ≤N, are the
corresponding eigenvectors. We assume that the perturbed eigenvalues and eigenvectors
are given by the series expansion (in ϵ):
λi = λi,0 +
∞

k=1
ϵkλi,k,
vi = vi,0 +
∞

k=1
ϵkvi,k,
(9.1)
with the constraint that
∥vi∥= ∥vi,0∥= 1,
1 ≤i ≤N.
(9.2)
121

122
Dyson Brownian Motion
Since the quantity ∥vi∥is constant, its ﬁrst order variation with respect to ϵ must be zero.
This constraint gives that vi,1 ⊥vi,0.
We assume that the λi,0 are all different, i.e. we consider non-degenerate perturbation
theory. Then, plugging (9.1) into
Hvi = λivi
(9.3)
and matching the left and right hand side term by term in powers of ϵ, one obtains
λi = λi,0 + ϵ(H1)ii + ϵ2
N

j=1
ji
|(H1)ij|2
λi,0 −λj,0
+ O(ϵ3),
(9.4)
where (H1)ij := vT
j,0H1vi,0, and
vi = vi,0 + ϵ
N

j=1
ji
(H1)ij
λi,0 −λj,0
vj,0 + O(ϵ2).
(9.5)
Notice that the ﬁrst order correction to vi is indeed perpendicular to vi,0 as it does not have
a component in that direction.
9.1.2 A Stochastic Differential Equation for Eigenvalues
Next we use the above formulas to derive the so-called Dyson Brownian motion (dbm),
which gives the evolution of eigenvalues of a random matrix plus a Wigner ensemble whose
variance grows linearly with time. Let M0 be the initial matrix (random or not), and X1 be a
unit Wigner matrix that is independent of M0. Then we study, using (9.4), the eigenvalues of
M = M0 +
√
dt X1,
(9.6)
where dt is a small quantity which will be interpreted as a differential time step.
The derivation of the sde for eigenvalues is much simpler if we use the rotational
invariance of the Wigner ensemble. The matrix X1 has the same law in any basis, we
therefore choose to express it in the diagonal basis of M0. In order to do so we must work
with the exact rotationally invariant Wigner ensemble where the diagonal variance is twice
the off-diagonal variance.
First, for the ﬁrst order term (in terms of ϵ), we have
(X1)ii := vT
i,0X1vi,0 ∼N

0, 2
N

.
(9.7)
Note that the (X1)ii are independent for different i’s.
Then we study the second order term. We have
(X1)ji := vT
j,0X1vi,0 ∼N

0, 1
N

.
(9.8)

9.1 Dyson Brownian Motion I: Perturbation Theory
123
0.0
0.2
0.4
0.6
0.8
1.0
t
−2
−1
0
1
2
lt
Figure 9.1 A simulation of dbm for an N = 25 matrix starting for a Wigner matrix with σ 2 = 1/4
and evolving for one unit of time.
So |(X1)ji|2 is a random variable with mean 1/N and with ﬂuctuations around the mean
also of order 1/N. As in Section 8.2.2, one can argue that these ﬂuctuations are negligible
when integrated over time in the limit dt →0. In other words, |(X1)ji|2 can be treated as
deterministic.
Now using (9.4), we get that
dλi =
2
2
βN dBi + 1
N
N

j=1
ji
dt
λi −λj
,
(9.9)
where dBi denotes a Brownian increment that comes from the (X1)ii term, and we have
added a factor β for completeness (equal to 1 for real symmetric matrices). This is the
fundamental evolution equation for eigenvalues in a ﬁctitious time that describes how much
of a Wigner matrix one progressively adds to the “initial” matrix M0 (see Fig. 9.1). The
astute reader will probably have recognized in the second term a Coulomb force deriving
from the logarithmic Coulomb potential log |λi −λj| encountered in Chapter 5.
One can also derive a similar process for the eigenvectors that we give here for β = 1:
dvi =
1
√
N
N

j=1
ji
dBij
λi −λj
vj −1
2N
N

j=1
ji
dt
(λi −λj)2 vi,
(9.10)
where dBij = dBji (i  j) is a symmetric collection of Brownian motions, independent of
each other and of the {dBi} above.
The formulas (9.9) and (9.10) give the Dyson Brownian motion for the stochastic evo-
lution of the eigenvalues and eigenvectors of matrices of the form

124
Dyson Brownian Motion
M = M0 + Xt,
(9.11)
where M0 is some initial matrix and Xt is an independent Wigner ensemble with parameter
σ 2 = t. We will call the above matrix process a matrix Dyson Brownian motion.
In our study of large random matrices, we will be interested in the dbm when N is large,
but actually the dbm is well deﬁned for any N. As for the Itˆo lemma, we used the fact
that the Gaussian process can be divided into inﬁnitesimal increments and that perturbation
theory becomes exact at that scale. We made no assumption about the size of N. We did
need a rotationally invariant Gaussian process so the diagonal variance must be twice the
off-diagonal one. In the most extreme example of N = 1, the eigenvalue of a 1 × 1 matrix
is just the value of its only element. Under dbm it simply undergoes a standard Brownian
motion with a variance of 2 per unit time.
Exercise 9.1.1
Variance as a function of time under dbm
Consider the Dyson Brownian motion for a ﬁnite N matrix:
dλi =
1
2
N dBi + 1
N
N

j=1
ji
dt
λi −λj
(9.12)
and the function F({λi}) that computes the second moment:
F({λi}) = 1
N
N

i=1
λ2
i .
(9.13)
(a)
Write down the stochastic process for F({λi}) using the Itˆo vectorial formula
(8.17). In the case at hand F does not depend explicitly on time and σ 2
i =
2/N. You will need to use the following identity:
2
N

i,j=1
ji
λi
λi −λj
=
N

i,j=1
ji
λi −λj
λi −λj
= N(N −1).
(9.14)
(b)
Take the expectation value of your equation and show that F(t)
:=
E[F({λi(t)})] follows
F(t) = F(0) + N + 1
N
t.
(9.15)
Do not assume that N is large.
9.2 Dyson Brownian Motion II: Itˆo Calculus
Another way to derive the Dyson Brownian motion for the eigenvalues is to consider the
matrix Brownian motion (9.11) as a Brownian motion of all the elements of the matrix X.
We have to treat the diagonal and off-diagonal elements separately because we want to use

9.2 Dyson Brownian Motion II: Itˆo Calculus
125
the rotationally invariant Wigner matrix (goe) with diagonal variance equal to twice the
off-diagonal variance. Also, only half of the off-diagonal elements are independent (the
matrix X is symmetric). We have
dXkk =
1
2
N dBkk
and
dXkℓ=
1
1
N dBkℓ
for
k < ℓ,
(9.16)
where dBkk and dBkℓare N and N(N −1)/2 independent unit Brownian motions.
Each eigenvalue λi is a (complicated) function of all the matrix elements of X. We can
use the vectorial form of Itˆo’s lemma (8.17) to write a stochastic differential equation (SDE)
for λi(X):
dλi =
N

k=1
∂λi
∂Xkk
1
2
N dBkk +
N

k=1
ℓ=k+1
∂λi
∂Xkℓ
1
1
N dBkℓ+
N

k=1
∂2λi
∂X2
kk
dt
N +
N

k=1
ℓ=k+1
∂2λi
∂X2
kℓ
dt
2N .
(9.17)
The key is to be able to compute the following partial derivatives:
∂λi
∂Xkk
;
∂λi
∂Xkℓ
;
∂2λi
∂X2
kk
;
∂2λi
∂X2
kℓ
,
(9.18)
where k < ℓ.
Since Xt is rotational invariant, we can rotate the basis such that X0 is diagonal:
X0 = diag(λ1(0), . . . ,λN(0)).
(9.19)
In order to compute the partial derivatives above, we can consider adding one small element
to the matrix X0, and compute the corresponding change in eigenvalues. We ﬁrst perturb
diagonal elements and later we will deal with off-diagonal elements.
A shift of the kth diagonal entry of X0 by δXkk affects λi with i = k in a linear fashion
but leaves all other eigenvalues unaffected:
λi →λi + δXkkδki.
(9.20)
Thus we have
∂λi
∂Xkk
= δik;
∂2λi
∂X2
kk
= 0.
(9.21)
Next we discuss how a perturbation in an off-diagonal entry of X0 can affect the eigen-
values. A perturbation of the (kℓ) entry by δXkℓ= δXℓk entry leads to the following
matrix:
X0 + δX =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎜⎝
λ1
...
λk
δXkℓ
...
δXkℓ
λℓ
...
λN
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
(9.22)

126
Dyson Brownian Motion
Since this matrix is block diagonal (after a simple permutation), the eigenvalues in the N−2
other 1 × 1 blocks are not affected by the perturbation, so trivially
∂λi
∂Xkℓ
= 0;
∂2λi
∂X2
kℓ
= 0,
∀i  k,ℓ.
(9.23)
On the other hand, the eigenvalues of the block
 λk
δXkℓ
δXkℓ
λℓ

(9.24)
are modiﬁed and and exact diagonalization gives
λ± = λk + λℓ
2
± λk −λℓ
2
2
1 + 4(δXkℓ)2
(λk −λℓ)2 .
(9.25)
We can expand this result to second order in δXkℓto ﬁnd
λk →λk + (δXkℓ)2
λk −λℓ
and
λℓ→λℓ+ (δXkℓ)2
λℓ−λk
.
(9.26)
We thus readily see that the ﬁrst partial derivative of λi with respect to any off-diagonal
element is zero:
∂λi
∂Xkℓ
= 0 for k < ℓ.
(9.27)
For the second derivative, on the other hand, we obtain
∂2λi
∂X2
kℓ
=
2δik
λi −λℓ
+
2δiℓ
λi −λk
for k < ℓ.
(9.28)
Of the two terms on the right hand side, the ﬁrst term exists only if ℓ> i while the second
term is only present when k < i. So, for a given i, only N −1 terms of the form 2/(λi −λj)
are present (note that the problematic term i = j is absent). Putting everything back into
Eq. (9.17), we ﬁnd, with β = 1 here,
dλi =
2
2
βN dBi + 1
N
N

j=1
ji
dt
λi −λj
,
(9.29)
where dBi are independent Brownian motions (the old dBkk for k = i). We have thus
precisely recovered Equation (9.9) using Itˆo’s calculus.
9.3 The Dyson Brownian Motion for the Resolvent
9.3.1 A Burgers’ Equation for the Stieltjes Transform
Consider a matrix Mt that undergoes a dbm starting from a matrix M0. At each time t, Mt
can be viewed as the sum of the matrix M0 and a Wigner matrix Xt of variance t:
Mt = M0 + Xt.
(9.30)

9.3 The Dyson Brownian Motion for the Resolvent
127
In order to characterize the spectrum of the matrix Mt, one should compute its Stieltjes
transform g(z), which is the expectation value of the large N limit of function gN(z),
deﬁned by
gN (z,{λi}) := 1
N
N

i=1
1
z −λi
.
(9.31)
gN is thus a function of all eigenvalues {λi} that undergo dbm while z is just a constant
parameter. Since the eigenvalues evolve with time gN is really a function of both z and t.
We can use Itˆo’s lemma to write a sde for gN (z,{λi}). The ingredients are, as usual now,
the following partial derivatives:
∂gN
∂λi
= 1
N
1
(z −λi)2 ;
∂2gN
∂λ2
i
= 2
N
1
(z −λi)3 .
(9.32)
We can now apply Itˆo’s lemma (8.17) using the dynamical equation for eigenvalues (9.9)
to ﬁnd
dgN = 1
N
1
2
N
N

i=1
dBi
(z −λi)2 + 1
N2
N

i,j=1
ji
dt
(z −λi)2(λi −λj) + 2
N2
N

i=1
dt
(z −λi)3 .
(9.33)
We now massage the second term to arrive at a symmetric form in i and j. In order to do
so, note that i and j are dummy indices that are summed over, so we can rename i →j
and vice versa and get the same expression. Adding the two versions and dividing by 2, we
get that this term is equal to
1
N2
N

i,j=1
ji
dt
(z −λi)2(λi −λj) =
1
2N2
N

i,j=1
ji
(
dt
(z −λi)2(λi −λj) +
dt
(z −λj)2(λj −λi)
)
= 1
2N2
N

i,j=1
ji
(2z −λi −λj)dt
(z −λi)2(z −λj)2 = 1
N2
N

i,j=1
ji
dt
(z −λi)(z −λj)2
= 1
N2
N

i,j=1
dt
(z −λi)(z −λj)2 −1
N2
N

i=1
dt
(z −λi)3 = −gN
∂gN
∂z dt −1
N2
N

i=1
dt
(z −λi)3 .
(9.34)
Note that very similar manipulations have been used in Section 5.2.2. Thus we have
dgN = 1
N
1
2
N
N

i=1
dBi
(z −λi)2 −gN
∂gN
∂z dt + 1
N2
N

i=1
dt
(z −λi)3
= 1
N
1
2
N
N

i=1
dBi
(z −λi)2 −gN
∂gN
∂z dt + 1
2N
∂2gN
∂z2 dt.
(9.35)

128
Dyson Brownian Motion
Taking now the expectation of this equation (such that the dBi term vanishes), we get
E[dgN(z)] = −E
(
gN(z)∂gN(z)
∂z
)
dt + 1
2N E
(∂2gN(z)
∂z2
)
dt.
(9.36)
This equation is exact for any N. We can now take the N →∞limit, where gN(z) →
gt(z). Using the fact that the Stieltjes transform is self-averaging, we get a pde for the time
dependent Stieltjes transform gt(z):
∂gt(z)
∂t
= −gt(z)∂gt(z)
∂z
.
(9.37)
Equation (9.37) is called the inviscid Burgers’ equation. Such an equation can in fact
develop singularities, so it often needs to be regularized by a viscosity term, which is in
fact present for ﬁnite N: it is precisely the last term of Eq. (9.36). Equation (9.37) can be
exactly solved using the methods of characteristics. This will be the topic of Section 10.1
in the next chapter.
9.3.2 The Evolution of the Resolvent
Let us now consider the full matrix resolvent of Mt, deﬁned as Gt(z) = (z1 −Mt)−1.
Clearly, the quantity gt(z) studied above is simply the trace of Gt(z), but Gt(z) also
contains information on the evolution of eigenvectors. Since each element of Gt depends
on all the elements of Mt, one can again use Itˆo’s calculus to derive an evolution equation
for Gt(z). The calculation is more involved because one needs to carefully keep track of
all indices. In this technical section, we sketch the derivation of Dyson Brownian motion
for the resolvent and brieﬂy discuss the result, which will be used further in Section 10.1.
Since Mt = M0 + Xt, the Itˆo lemma gives
dGij(z) =
N

k,ℓ=1
∂Gij
∂Mkℓ
dXkℓ+ 1
2
N

k,ℓ,m,n=1
∂2Gij
∂Mkℓ∂Mmn
d

XkℓXmn

,
(9.38)
where the last term denotes the covariation of Xkℓand Xmn, and we have considered Mkl
and Mlk to be independent variables following 100% correlated Brownian motions. Next,
we compute the derivatives
∂Gij
∂Mkℓ
= 1
2

GikGjℓ+ GjkGiℓ

,
(9.39)
from which we deduce the second derivatives
∂2Gij
∂Mkℓ∂Mmn
= 1
4

(GimGkn + GimGkn) Gjℓ+ · · ·

,
(9.40)
where we have not written the other GGG products obtained by applying Eq. (9.39) twice.
Now, using the properties of the Brownian noise, the quadratic covariation reads
d

XkℓXmn

= dt
N

δkmδℓn + δknδℓm

,
(9.41)
so that we get from (9.38) and taking into account symmetries:
dGij(z,t) =
N

k,ℓ=1
GikGjℓdXkℓ+ 1
N
N

k,ℓ=1

GikGℓkGℓj + GikGkjGℓℓ

dt.
(9.42)

9.4 The Dyson Brownian Motion with a Potential
129
If we now take the average over the Brownian motions dXkℓ, we ﬁnd the following
evolution for the average resolvent:
∂tE[Gt(z)] = Tr Gt(z) E[G2
t (z)] + 1
N E[G3
t (z)].
(9.43)
Now, one can notice that
G2(z,t) = −∂zG(z,t);
G3(z,t) = 1
2∂2
zzG(z,t),
(9.44)
which hold even before averaging. By sending N →∞, we then obtain the following
matrix pde for the resolvent:
∂tE[Gt(z)] = −gt(z) ∂zE[Gt(z)],
with
E[G0(z)] = GM0(z).
(9.45)
Note that this equation is linear in Gt(z) once the Stieltjes transform gt(z) is known.
Taking the trace of Eq. (9.45) immediately leads back to the Burgers’ equation (9.37) for
gt(z) itself, as expected.
9.4 The Dyson Brownian Motion with a Potential
9.4.1 A Modiﬁed Langevin Equation for Eigenvalues
In this section, we modify Dyson Brownian motion by adding a potential such that the
stationary state of these interacting random walks coincides with the eigenvalue measure
of β-ensembles, namely
P({λi}) = Z−1
N exp
⎧
⎪⎪⎨
⎪⎪⎩
−β
2
⎡
⎢⎢⎣
N

i=1
NV (λi) −
N

i,j=1
ji
log |λi −λj|
⎤
⎥⎥⎦
⎫
⎪⎪⎬
⎪⎪⎭
.
(9.46)
The general vectorial Langevin equation (8.36) leading to such an equilibrium with σ 2 =
2/N immediately gives us the following dbm in a potential V (λ):
dλk =
1
2
N dBk +
⎛
⎜⎝1
N
N

ℓ=1
ℓk
β
λk −λℓ
−β
2 V ′(λk)
⎞
⎟⎠dt,
(9.47)
which recovers Eq. (9.9) in the absence of a potential. See Figure 9.2 for an illustration.
Dyson Brownian motion in a potential has many applications. Numerically it can be used
to generate matrices for an arbitrary potential and an arbitrary value of β, a task not obvious
a priori from the deﬁnition (9.46). Figure 9.2 shows a simulation of the matrix potential
studied in Section 5.3.3. Note that dbm generates the correct density of eigenvalues; it also
generates the proper statistics for the joint distribution of eigenvalues.
It is interesting to see how Burgers’ equation for the Stieltjes transform, Eq. (9.37), is
changed in the case where V (λ) = λ2/2, i.e. in the standard goe case β = 1. Redoing
the steps leading to Eq. (9.36) with the extra V ′ term in the right hand side of Eq. (9.47)
modiﬁes the Burgers’ equation into

130
Dyson Brownian Motion
0
5
10
t
−1
0
1
lt
−1
0
1
l
0.0
0.1
0.2
0.3
0.4
r(l)
Figure 9.2 (left) A simulation of dbm with a potential for an N = 25 matrix starting from a Wigner
matrix with σ 2 = 1/10 and evolving within the potential V (x) = x2
2 + x4
4 for 10 units of time. Note
that the steady state is quickly reached (within one or two units of time). (right) Histogram of the
eigenvalues for the same process for N = 400 and 200 discrete steps per unit time. The histogram
is over all matrices from time 3 to 10 (560 000 points). The agreement with the theoretical density
(Eq. (5.58)) is very good.
∂gt(z)
∂t
= −gt(z)∂gt(z)
∂z
+ 1
2
∂(zgt(z))
∂z
.
(9.48)
The solution to this equation will be discussed in the next chapter.
More theoretically, dbm can be used in proofs of local universality, which is one of the
most important results in random matrix theory. Local universality is the concept that many
properties of the joint law of eigenvalues do not depend on the speciﬁcs of the random
matrix in question, provided one looks at them on a scale ξ comparable to the average
distance between eigenvalues, i.e. on scales N−1 ≲ξ ≪1. Many such properties arise
from the logarithmic eigenvalue repulsion and indeed depend only on the symmetry class
(β) of the model.
Another useful property of dbm is its speed of convergence to the steady state. With time
normalized as in Eq. (9.47), global properties (such as the density of eigenvalues) converge
in a time of order 1, as we discuss in the next subsection. Local properties on the other hand
(e.g. eigenvalue spacing) converge much faster, in a time of order 1/N, i.e. as soon as the
eigenvalues have “collided” a few times with one another.1
1
The time needed for two Brownian motions a distance d apart to meet for the ﬁrst time is of order d2/σ2. In our
case d = 1/N (the typical distance between two eigenvalues) and σ2 = 2/N. The typical collision time is therefore (2N)−1.
Note however that eigenvalues actually never cross under Eq. (9.47), but the corresponding eigenvectors strongly mix when
such quasi-collisions occur.

9.4 The Dyson Brownian Motion with a Potential
131
Exercise 9.4.1
Moments under dbm with a potential
Consider the moments of the eigenvalues as a function of time:
Fk(t) = 1
N
N

i=1
λk
i (t),
(9.49)
for eigenvalues undergoing dbm under a potential V (x), Eq. (9.47), in the
orthogonal case β = 1. In this exercise you will need to show (and to use)
the following identity:
2
N

i,j=1
ji
λk
i
λi −λj
=
N

i,j=1
ji
λk
i −λk
j
λi −λj
=
N

i,j=1
ji
k

ℓ=0
λℓ
i λk−ℓ
j
.
(9.50)
(a)
Using Itˆo calculus, write a SDE for F2(t).
(b)
By taking the expectation value of your equation, show that
d
dt E[F2(t)] = 1 −E
-
1
N
N

i=1
λi(t)V ′(λi(t))
.
+ 1
N .
(9.51)
(c)
In the Wigner case, V ′(x) = x, ﬁnd the steady-state value of E[F2(t)] for any
ﬁnite N.
(d)
For a random matrix X drawn from a generic potential V (x), show that in the
large N limit, we have
τ

V ′(X)X

= 1,
(9.52)
where τ is the expectation value of the normalized trace deﬁned by (2.1).
(e)
Show that this equation is consistent with τ(W) = 1 for a Wishart matrix
whose potential is given by Eq. (5.4).
(f)
In the large N limit, ﬁnd a general expression for τ[V ′(X)Xk] by writing the
steady-state equation for E[Fk+1(t)]; you can neglect the Itˆo term. The ﬁrst
two should be given by
τ[V ′(X)X2] = 2τ[X]
and
τ[V ′(X)X3] = 2τ[X2] + τ[X]2.
(9.53)
(g)
In the unit Wigner case V ′(x) = x, show that your relation in (f) is equivalent
to the Catalan number inductive relation (3.27), with τ(X2m) = Cm and
τ(X2m+1) = 0.
9.4.2 The Fokker–Planck Equation for DBM
From the Langevin equation, Eq. (9.47), one can derive the Fokker–Planck equation
describing the time evolution of the joint distribution of eigenvalues, P({λi},t). It reads

132
Dyson Brownian Motion
∂P
∂t = 1
N
N

i=1
∂
∂λi
( ∂P
∂λi
−FiP
)
,
(9.54)
where we use P as an abbreviation for P({λi},t), and, for a quadratic conﬁning potential
V (λ) = λ2/2, a generalized force given by
Fi := β
N

j=1
ji
1
λi −λj
−Nβλi
2
.
(9.55)
The trick now is to introduce an auxiliary function W({λi},t), deﬁned as
P({λi},t) := exp
⎡
⎢⎢⎢⎣
β
4
N

i,j=1
ji
log |λi −λj| −βN
8
N

i=1
λ2
i
⎤
⎥⎥⎥⎦W({λi},t).
(9.56)
Then after a little work, one ﬁnds the following evolution equation for W:2
∂W
∂t
= 1
N
N

i=1
-
∂2W
∂λ2
i
−ViW
.
,
(9.58)
with
Vi({λj}) := β2N2
16
λ2
i −β(2 −β)
4
N

j=1
ji
1
(λi −λj)2 −Nβ
4

1 + β(N −1)
2

. (9.59)
Looking for W
 such that
∂W
∂t
= −
W
,
(9.60)
one ﬁnds that W
 is the solution of the following eigenvalue problem:
2
N
N

i=1
-
−1
2
∂2W
∂λ2
i
+ 1
2ViW
.
= 
W
.
(9.61)
One notices that Eq. (9.61) is a (real) Schrodinger equation for N interacting “particles” in
a quadratic potential, with an interacting potential that depends on the inverse of the square
distance between particles. This is called the Calogero model, which happens to be exactly
soluble in one dimension, both classically and quantum mechanically. In particular, the
whole spectrum of this Hamiltonian is known, and given by3

(n1,n2, . . . ,nN) = β
2
⎛
⎝
N

i=1
ni −N(N −1)
2
⎞
⎠;
0 ≤n1 < n2 · · · < nN.
(9.62)
2 One has to use, along the line, the following identity:

ijk
1
λi −λj
1
λi −λk
≡0.
(9.57)
3 Because W
({λi}) must vanish as two λ’s coincide, one must choose the so-called fermionic branch of the spectrum.

9.5 Non-Intersecting Brownian Motions and the Karlin–McGregor Formula
133
The smallest eigenvalue corresponds to n1 = 0,n2 = 1, . . . ,nN = N −1 and is such that

 ≡0. This corresponds to the equilibrium state of the Fokker–Planck equation:
W0({λi}) = exp
⎡
⎢⎢⎢⎣
β
4
N

i,j=1
ji
log |λi −λj| −βN
8
N

i=1
λ2
i
⎤
⎥⎥⎥⎦.
(9.63)
All other “excited states” have positive 
’s, corresponding to exponentially decaying
modes (in time) of the Fokker–Planck equation. The smallest, non-zero value of 
 is such
that nN = N, all other values of ni being unchanged. Hence 
1 is equal to β/2.
In conclusion, we have explicitly shown that the equilibration time of the dbm in a
quadratic potential is equal to 2/β. As announced at the end of the previous section, the
density of eigenvalues indeed converges in a time of order unity.
The case β = 2 is particularly simple, since the interaction term in Vi disappears
completely. We will use this result to show that, in the absence of a quadratic potential, the
time dependent joint distribution of eigenvalues, P({λi},t), can be expressed, for β = 2,
as a simple determinant: this is the so-called Karlin–McGregor representation, see next
section.
9.5 Non-Intersecting Brownian Motions and the Karlin–McGregor Formula
Let p(y,t|x) be the probability density that a Brownian motion starting at x at t = 0 is
at y at time t
p(y,t|x) =
1
√
2πt
exp

−(x −y)2
2t

,
(9.64)
where we set σ 2 = 1. Note that p(y,t|x) obeys the diffusion equation
∂p(y,t|x)
∂t
= 1
2
∂2p(y,t|x)
∂y2
.
(9.65)
We now consider N independent Brownian motions starting at x = (x1,x2, . . . xN)
at t = 0, with x1 > x2 > . . . > xN. The Karlin–McGregor formula states that the
probability PKM(y,t|x) that these Brownian motions have reached positions y = (y1 >
y2 > . . . > yN) at time t without ever intersecting between 0 and t is given by the
following determinant:
PKM(y,t|x) =

p(y1,t|x1)
p(y1,t|x2)
. . .
p(y1,t|xN)
p(y2,t|x1)
p(y2,t|x2)
. . .
p(y2,t|xN)
...
...
...
...
p(yN,t|x1)
p(yN,t|x2)
. . .
p(yN,t|xN)

.
(9.66)
One can easily prove this by noting that the determinant involves sums of products of N
terms p(yi,t|xj), each product  involving one and only one yi. Each product  therefore
obeys the multidimensional diffusion equation:
∂
∂t = 1
2
N

i=1
∂2
∂y2
i
.
(9.67)

134
Dyson Brownian Motion
Being the sum of such products, PKM(y,t|x) also obeys the same diffusion equation, as it
should since we consider N independent Brownian motions:
∂PKM(y,t|x)
∂t
= 1
2
N

i=1
∂2PKM(y,t|x)
∂y2
i
,
(9.68)
The determinant structure also ensures that PKM(y,t|x) = 0 as soon as any two y’s are
equal. Finally, because the x’s and the y’s are ordered, PKM(y,t = 0|x) obeys the correct
initial condition.
Note that the total survival probability P(t;x) :=

dyPKM(y,t|x) decreases with time,
since as soon as two Brownian motions meet, the corresponding trajectory is killed. In fact,
one can show that P(t;x) ∼t−N(N−1)/4 at large times.
Now, the probability P(y,t|x) that these N independent Brownian motions end at y at
time t conditional on the fact that the paths never ever intersect, i.e. for any time between
t = 0 and t = ∞, turns out to be given by a very similar formula:
P(y,t|x) = (y)
(x)PKM(y,t|x),
(9.69)
where (x) is the Vandermonde determinant 
i<j (xj −xi) (and similarly for (y)).
What we want to show is that P(y,t|x) is the solution of the Fokker–Planck equation
for the Dyson Brownian motion, Eq. (9.54), with β = 2 and in the absence of any con-
ﬁning potential. Indeed, as shown above, PKM(y,t|x) obeys the diffusion equation for N
independent Brownian motions with the annihilation boundary condition PKM(y,t|x) = 0
when yi = yj for any given pair i,j.
Now compare with the deﬁnition Eq. (9.56) of W for the Dyson Brownian motions
with β = 2 and without any conﬁning potential:
P({λi},t) := exp
⎡
⎢⎢⎢⎣
1
2
N

j=1
ji
log |λi −λj|
⎤
⎥⎥⎥⎦W({λi},t) ≡({λi})W({λi},t).
(9.70)
From Eq. (9.58) we see that in the present case W({λi},t) also obeys the diffusion equation
for N independent Brownian motions. Since P({λi},t) ∼|λi−λj|2 when two eigenvalues
are close, we also see that W({λi},t) vanishes linearly whenever two eigenvalues meet,
and therefore obeys the same boundary conditions as PKM(y,t|x) with yi = λi.
The conclusion is therefore that the Dyson Brownian motion without external forces
is, for β = 2, equivalent to N Brownian motions constrained to never ever intersect.
Bibliographical Notes
• The Dyson Brownian motion was introduced in
– F. J. Dyson. A Brownian-motion model for the eigenvalues of a random matrix.
Journal of Mathematical Physics, 3:1191–1198, 1962.
• It is not often discussed in books on random matrix theory. The subject is treated in
– J. Baik, P. Deift, and T. Suidan. Combinatorics and Random Matrix Theory. Ameri-
can Mathematical Society, Providence, Rhode Island, 2016,
– L. Erd˝os and H.-T. Yau. A Dynamical Approach to Random Matrix Theory. American
Mathematical Society, Providence, Rhode Island, 2017,

9.5 Non-Intersecting Brownian Motions and the Karlin–McGregor Formula
135
the latter discusses dbm with a potential beyond the Ornstein–Uhlenbeck model.
• On the Burgers’ equation for the Stieltjes transform, see
– L. C. G. Rodgers and Z. Shi. Interacting Brownian particles and the Wigner law.
Probability Theory and Related Fields, 95:555–570, 1993.
• On the evolution of the matrix resolvent, see
– R. Allez, J. Bun, and J.-P. Bouchaud. The eigenvectors of Gaussian matrices with an
external source. preprint arXiv:1412.7108, 2014,
– J. Bun, J.-P. Bouchaud, and M. Potters. Cleaning large correlation matrices: Tools
from random matrix theory. Physics Reports, 666:1–109, 2017.
• On the universality of the local properties of eigenvalues, see e.g.
– E. Br´ezin and A. Zee. Universality of the correlations between eigenvalues of large
random matrices. Nuclear Physics B, 402(3):613–627, 1993,
– L. Erd˝os. Universality of Wigner random matrices: A survey of recent results. Rus-
sian Mathematical Surveys, 66(3):507, 2011.
• On the Calogero model, see
– F. Calogero. Solution of the one-dimensional n-body problem with quadratic and/or
inversely quadratic pair potentials. Journal of Mathematical Physics, 12:419–436,
1971,
– P. J. Forrester. Log Gases and Random Matrices. Princeton University Press,
Princeton, NJ, 2010,
– A. P. Polychronakos. The physics and mathematics of Calogero particles. Journal of
Physics A: Mathematical General, 39(41):12793–12845, 2006,
and http://www.scholarpedia.org/article/Calogero-Moser system
• On the Karlin–McGregor formula, see
– S. Karlin and J. McGregor. Coincidence probabilities. Paciﬁc Journal of Mathemat-
ics, 9(4):1141–1164, 1959,
– P.-G. de Gennes. Soluble model for ﬁbrous structures with steric constraints. The
Journal of Chemical Physics, 48(5):2257–2259, 1968,
and for recent applications
– T. Gauti´e, P. Le Doussal, S. N. Majumdar, and G. Schehr. Non-crossing Brownian
paths and Dyson Brownian motion under a moving boundary. Journal of Statistical
Physics, 177(5):752–805, 2019,
and references therein.

10
Addition of Large Random Matrices
In this chapter we seek to understand how the eigenvalue density of the sum of two large
random matrices A and B can be obtained from their individual densities. In the case where,
say, A is a Wigner matrix X, the Dyson Brownian motion formalism of the previous chapter
allows us to swiftly answer that question. We will see that a particular transform of the
density of B, called the R-transform, appears naturally. We then show that the R-transform
appears in the more general context where the eigenbases of A and B are related by a
random rotation matrix O. In this case, one can construct a Fourier transform for matrices,
which allows us to deﬁne the analog of the generating function for random variables. As
in the case of iid random variables, the logarithm of this matrix generating function is
additive when one adds two randomly rotated, large matrices. The derivative of this object
turns out to be the R-transform, leading to the central result of the present chapter (and
of the more abstract theory of free variables, see Chapter 11): the R-transform of the
sum of two randomly rotated, large matrices is equal to the sum of R-transforms of each
individual matrix.
10.1 Adding a Large Wigner Matrix to an Arbitrary Matrix
Let Mt = M0 + Xt be the sum of a large matrix M0 and a large Wigner matrix Xt, such
that the variance of each element grows as t. This deﬁnes a Dyson Brownian motion as
described in the previous chapter, see Eq. (9.11). We have shown in Section 9.3.1 that in
this case the Stieltjes transform gt(z) of Mt satisﬁes the Burgers’ equation:
∂gt(z)
∂t
= −gt(z)∂gt(z)
∂z
,
(10.1)
with initial condition g0(z) := gM0(z). We now proceed to show that the solution of
this Burgers’ equation can be simply expressed using an M0 dependent function: its R-
transform.
Using the so-called method of characteristics, one can show that
gt(z) = g0(z −tgt(z)).
(10.2)
136

10.1 Adding a Large Wigner Matrix to an Arbitrary Matrix
137
If the method of characteristics is unknown to the reader, one can verify that (10.2) indeed
satisﬁes Eq. (10.1) for any function g0(z). Indeed, let us compute ∂tgt(z) and ∂zgt(z) using
Eq. (10.2):
∂tgt(z) = g′
0(z −tgt(z)) [−gt(z) −t∂tgt(z)] ⇒∂tgt(z) = −gt(z)g′
0(z −tgt(z))
1 + tg′
0(z −tgt(z)), (10.3)
and
∂zgt(z) = g′
0(z −tgt(z))

1 −t∂zgt(z)

⇒∂zgt(z) =
g′
0(z −tgt(z))
1 + tg′
0(z −tgt(z)),
(10.4)
such that Eq. (10.1) is indeed satisﬁed.
Example: Suppose M0 = 0. Then we have g0(z) = z−1. Plugging into (10.2), we obtain
that
gt(z) =
1
z −tgt(z),
(10.5)
which is the self-consistent Eq. (2.35) in the Wigner case with σ 2 = t. Indeed, if we start
with the zero matrix, then Mt = Xt is just a Wigner with parameter σ 2 = t.
Back to the general case, we denote as zt(g) the inverse function1 of gt(z). Now ﬁx
g = gt(z) = g0(z −tg) and z = zt(g), we apply the function z0 to g and get
z0(g) = z −tg = zt(g) −tg,
zt(g) = z0(g) + tg.
(10.6)
The inverse of the Stieltjes transform of Mt is given by the inverse of that of M0 plus a
simple shift tg. If we know g0(z) we can compute its inverse z0(g) and thus easily obtain
zt(g), which after inversion hopefully recovers gt(z).
Example: Suppose M0 is a Wigner matrix with variance σ 2. We ﬁrst want to compute
the inverse of g0(z); to do so we use the fact that g0(z) satisﬁes Eq. (2.35), and we get that
z0(g) = σ 2g + 1
g .
(10.7)
Then, by (10.6), we get that
zt(g) = z0(g) + tg =

σ 2 + t

g + 1
g,
(10.8)
which is the inverse Stieltjes transform for Wigner matrices with variance σ 2 + t. In other
words gt(z) satisﬁes the Wigner equation (2.35) with σ 2 replaced by σ 2 + t. This result is
not surprising, each element of the sum of two Wigner matrices is just the sum of Gaussian
random variables. So Mt is itself a Wigner matrix with the sum of the variances as its
variance.
1 We will discuss in Section 10.4 the invertibility of the function g(z).

138
Addition of Large Random Matrices
We can now tackle the more general case when the initial matrix is not necessarily
Wigner. Call B = Mt and A = M0. Then by (10.6), we get
zB(g) = zA(g) + tg = zA(g) + zXt (g) −1
g .
(10.9)
We now deﬁne the R-transform as
R(g) := z(g) −1
g .
(10.10)
Note that the R-transform of a Wigner matrix of variance t is simply given by
RX(g) = tg.
(10.11)
This deﬁnition allows us to rewrite Eq. (10.9) above as a nice additive relation between
R-transforms:
RB(g) = RA(g) + RXt (g).
(10.12)
In the next section we will generalize this law of addition to (large) matrices X that are
not necessarily Wigner. The R-transform will prove to be a very powerful tool to study
large random matrices. Some of its properties are left to be derived in Exercises 10.1.1 and
10.1.2 and will be further discussed in Chapter 15. We ﬁnish this section by computing
the R-transform of a white Wishart matrix. Remember that its Stieltjes transform satisﬁes
Eq. (4.37), i.e.
qzg2 −(z −1 + q)g + 1 = 0,
(10.13)
which can be written in terms of the inverse function z(g):
z(g) =
1
1 −qg + 1
g .
(10.14)
From which we can read off the R-transform:
RW(g) =
1
1 −qg .
(10.15)
Exercise 10.1.1
Taylor series for the R-transform
Let g(z) be the Stieltjes transform of a random matrix M:
g(z) = τ

(z1 −M)−1
=

supp{ρ}
ρ(λ)dλ
z −λ .
(10.16)
We saw that the power series of g(z) around z = ∞is given by the moments of
M (mn := τ(Mn)):
g(z) =
∞

n=0
mn
zn+1 with m0 ≡1.
(10.17)

10.1 Adding a Large Wigner Matrix to an Arbitrary Matrix
139
Call z(g) the functional inverse of g(z) which is well deﬁned in a neighborhood
of g = 0. And deﬁne R(g) as
R(g) = z(g) −1/g.
(10.18)
(a)
By writing the power series of R(g) near zero, show that R(g) is regular at
zero and that R(0) = m1. Therefore the power series of R(g) starts at g0:
R(g) =
∞

n=1
κngn−1.
(10.19)
(b)
Now assume m1 = κ1 = 0 and compute κ2, κ3 and κ4 as a function of m2, m3
and m4 in that case.
Exercise 10.1.2
Scaling of the R-transform
Using your answer from Exercise 2.3.1: If A is a random matrix drawn from
a well-behaved ensemble with Stieltjes transform gA(z) and R-transform RA(g),
what is the R-transform of the random matrices αA and A + b1 where α and b
are non-zero real numbers?
Exercise 10.1.3
Sum of symmetric orthogonal and Wigner matrices
Consider as in Exercise 1.2.4 a random symmetric orthogonal matrix M and a
Wigner matrix X of variance σ 2. We are interested in the spectrum of their sum
E = M + X.
(a)
Given that the eigenvalues of M are ±1 and that in the large N limit each
eigenvalue appears with weight 1
2, write the limiting Stieltjes transform gM(z).
(b)
E can be thought of as undergoing Dyson Brownian motion starting at E(0) =
M and reaching the desired E at t = σ 2. Use Eq. (10.2) to write an equation
for gE(z). This will be a cubic equation in g.
(c)
You can obtain the same equation using the inverse function zM(g) of your
answer in (a). Show that
zM(g) = 1 +
*
1 −4g2
2g
,
(10.20)
where one had to pick the root that makes z(g) ∼1/g near g = 0.
(d)
Using Eq. (10.6), write zE(g) and invert this relation to obtain an equation for
gE(z). You should recover the same equation as in (b).
(e)
Eigenvalues of E will be located where your equation admits non-real
solutions for real z. First look at z = 0; the equation becomes quadratic after
factorizing a trivial root. Find a criterion for σ 2 such that the equation admits
non-real solutions. Compare with your answer in Exercise 1.2.4 (b).
(f)
At σ 2 = 1, the equation is still cubic but is somewhat simpler. A real cubic
equation of the form ax3 + bx2 + cx + d = 0 will have non-real solutions
iff  < 0 where  = 18abcd −4b3d + b2c2 −4ac3 −27a2d2. Using this

140
Addition of Large Random Matrices
criterion show that for σ 2 = 1 the edges of the eigenvalue spectrum are given
by λ = ±3
√
3/2 ≈±2.60.
(g)
Again at σ 2 = 1, the solution near g(0) = 0 can be expanded in fractional
powers of z. Show that we have
g(z) = z1/3 + O(z), which implies ρ(x) =
√
3
2
3*
|x|,
(10.21)
for x near zero.
(h)
For σ 2 = 1/2,1 and 2, solve numerically the cubic equation for gE(z) for
z = x real and plot the density of eigenvalues ρ(x) = | Im(gE(x))|/π for one
of the complex roots if present.
10.2 Generalization to Non-Wigner Matrices
10.2.1 Set-Up
In the previous section, we derived a formula for the Stieltjes transform of the sum of a
Wigner matrix and an arbitrary matrix. We would like to ﬁnd a generalization of this result
to a larger class of matrices.
Take two N × N matrices: A, with eigenvalues {λi}1≤i≤N and eigenvectors {vi}1≤i≤N,
and B, with eigenvalues {μi}1≤i≤N and eigenvectors {ui}1≤i≤N. Then the eigenvalues of
C = B + A will in general depend in a complicated way on the overlaps between the
eigenvectors of B and the eigenvectors of A. In the trivial case where vi = ui for all i, we
have that the eigenvalues of B + A are simply given by νi = λi + μi. However, this is
neither generic nor very interesting.
One important property of Wigner matrices is that their eigenvectors are Haar dis-
tributed, that is, the matrix of eigenvectors is distributed uniformly in the group O(N) and
each eigenvector is uniformly distributed on the unit sphere SN−1. Thus, when N is large,
it is very unlikely that any one of them will have a signiﬁcant overlap with the eigenvectors
of B. This is the property that we want to keep in our generalization. We will study what
happens for general matrices B and A when their eigenvectors are random with respect
to one another. We will deﬁne this relative randomness notion (called “freeness”) more
precisely in the next chapter. Here, to ensure the randomness of the eigenvectors, we will
apply a random rotation to the matrix A and deﬁne the free addition as
C = B + OAOT,
(10.22)
where O is a Haar distributed random orthogonal matrix. Then it is easy to see that
OAOT is rotational invariant since O′O is also Haar distributed for any ﬁxed O′ ∈O(N).

10.2 Generalization to Non-Wigner Matrices
141
10.2.2 Matrix Fourier Transform
We saw in Section 8.1 that the function HX(t) = log E exp(itX) is additive when one adds
independent scalar variables. When X is a matrix, it is plausible that t should also be a
matrix T, but in the end we need to take the exponential of a scalar, so a possible candidate
would be
I(X,T) :=
6
exp
N
2 Tr TOXOT
7
O
.
(10.23)
The notation ⟨·⟩O means that we average over all orthogonal matrices O (with a ﬂat weight)
normalized such that ⟨1⟩O = 1. This deﬁnes the Haar measure on the group of orthogonal
matrices. Equation (10.23) deﬁnes the so-called Harish-Chandra–Itzykson–Zuber (hciz)
integral.2
Note that by deﬁnition, I(O1XOT
1,T) = I(X,O1TOT
1) = I(X,T) for an
arbitrary rotation matrix O1. This means that I(X,T) only depends on the eigenvalues of
X and T.
Now consider C = B+O1AOT
1 with a random O1. For large matrix sizes, the eigenvalue
spectrum of C will turn out not to depend on the speciﬁc choice of O1, provided it is chosen
according to the Haar measure. Therefore, one can average I(C,T) over O1 and obtain
I(C,T) =
6
exp
N
2 Tr TO(B + O1AOT
1)OT
7
O,O1
= I(B,T)I(A,T),
(10.25)
where we have used that OO1 = O′ is a random rotation independent from O itself. Hence
we conclude that log I is additive in this case, as is the logarithm of the characteristic
function in the scalar case.
For a general matrix T, the hciz integral is quite complicated, as will be further dis-
cussed in Section 10.5. Fortunately, for our purpose we can choose the “Fourier” matrix T
to be rank-1 and in this case the integral can be computed. A symmetric rank-1 matrix can
be written as
T = t vvT,
(10.26)
where t is the eigenvalue and v is a unit vector. We will show that the large N behavior of
I(T,B) is given, in this case, by
I(T,B) ≈exp
N
2 HB(t)

,
(10.27)
for some function HB(t) that depends on the particular matrix B.
2 The hciz can be deﬁned with an integral over orthogonal, unitary or symplectic matrices. In the general case it is deﬁned as
Iβ(X,T) :=
6
exp
 Nβ
2
Tr XOTO†
7
O
,
(10.24)
with beta equal to 1, 2 or 4 and O is averaged over the corresponding group. The unitary β = 2 case is the most often studied,
for which some explicit results are available.

142
Addition of Large Random Matrices
More formally we deﬁne
HB(t) = lim
N→∞
2
N log
6
exp
tN
2 Tr vvT OBOT
7
O
.
(10.28)
If C = B + A where A is randomly rotated with respect to B, the precise statement is that
HC(t) = HB(t) + HA(t),
(10.29)
i.e. the function H is additive. We now need to relate this function to the R-transform
encountered in the previous section.
10.3 The Rank-1 HCIZ Integral
To get a useful theory, we need to have a concrete expression for this function HB. Without
loss of generality, we can assume B is diagonal (in fact, we can diagonalize B and absorb
the eigenmatrix into the orthogonal matrix O we integrate over). Moreover, for simplicity
we assume that t > 0. Then OT TO can be regarded as proportional to a random projector:
OT TO = ψψ T,
(10.30)
with ∥ψ∥2 = t and ψ/∥ψ∥uniformly distributed on the unit sphere. Then we make a
change of variable ψ →ψ/
√
N, and calculate
Zt(B) =

dNψ
(2π)N/2 δ

∥ψ∥2 −Nt

exp
1
2ψ T Bψ

,
(10.31)
where we have added a factor of (2π)−N/2 for later convenience. Because Zt(B) is not
properly normalized (i.e. Zt(0)  1), we will need to normalize it to compute I(T,B):
6
exp
N
2 Tr TOBOT
7
O
= Zt(B)
Zt(0) .
(10.32)
10.3.1 A Saddle Point Calculation
We can now express the Dirac delta as an integral over the imaginary axis:
δ(x) =
 ∞
−∞
e−izx
2π dz =
 i∞
−i∞
e−zx/2
4iπ dz.
Now let  be a parameter larger than the maximum eigenvalue of B:  > λmax(B). We
introduce the factor
1 = exp

−

∥ψ∥2 −Nt

2

,
since ∥ψ∥2 = Nt. Then, absorbing  into z, we get that

10.3 The Rank-1 HCIZ Integral
143
Zt(B) =
 +i∞
−i∞
dz
4π

dNψ
(2π)N/2 exp

−1
2ψ T (z −B)ψ + Nzt
2

.
(10.33)
We can now perform the Gaussian integral over the vector ψ:
Zt(B) =
 +i∞
−i∞
dz
4π det (z −B)−1/2 exp
Nzt
2

=
 +i∞
−i∞
dz
4π exp
-
N
2

zt −1
N

k
log(z −λk(B))
.
,
(10.34)
where λk(B), 1 ≤k ≤N, are the eigenvalues of B. Then we denote
Ft(z,B) := zt −1
N

k
log(z −λk(B)).
(10.35)
The integral in (10.34) is oscillatory, and by the stationary phase approximation (see
Appendix A.1), it is dominated by the point where
∂zFt(z,B) = 0 ⇒t −1
N

k
1
z −λk(B) = t −gB
N(z) = 0.
(10.36)
If gB
N(z) can be inverted then we can express z as z(t). For x > λmax, gB
N(x) is mono-
tonically decreasing and thus invertible. So for t < gB
N(λmax), a unique z(t) exists and
z(t) > λmax (see Section 10.4). Since Ft(z,B) is analytic to the right of z = λmax, we
can deform the contour to reach this point (see Fig. 10.1). Using the saddle point formula
(Eq. (A.3)), we have
Re z
Im z
L
(t)
lmax
L+ i
L−i
Figure 10.1 Graphical representation of the integral Eq. (10.34) in the complex plane. The crosses
represent the eigenvalues of B and are singular points of the integrand. The integration is from −i∞
to  + i∞where  > λmax. The saddle point is at z = z(t) > λmax. Since the integrand is analytic
right of λmax, the integration path can be deformed to go through z(t).

144
Addition of Large Random Matrices
Zt(B) ∼
√
4π/(4π)
|N∂2z F(z(t),B)|1/2 exp
-
N
2

z(t)t −1
N

k
log(z(t) −λk(B))
.
∼
1
2
/
Nπ|g′
B(z(t))|
exp
-
N
2

z(t)t −1
N

k
log(z(t) −λk(B))
.
.
(10.37)
For the case B = 0, we have gB(z) = z−1 ⇒z(t) = t−1, so we get
Zt(0) ∼
1
2t
√
Nπ
exp
(N
2 (1 + log t)
)
.
(10.38)
In the large N limit, the prefactor in front of the exponential does not contribute to HB(t)
and we ﬁnally get
lim
N→∞
2
N log
6
exp
N
2 Tr TOBOT
7
O
= z(t)t −1 −log t −1
N

k
log(z(t) −λk(B)).
(10.39)
By the deﬁnition (10.28), we then get that
HB(t) = H(z(t),t),
H(z,t) := zt −1 −log t −1
N

k
log(z −λk(B)).
(10.40)
10.3.2 Recovering R-Transforms
We found an expression for HB(t) but in a form that is not easy to work with. But note
H(z,t) comes from a saddle point approximation and therefore its partial derivative with
respect to z is zero: ∂zH(z(t),t) = 0. This allows us to compute a much simpler expression
for the derivative of HB(t):
dHB(t)
dt
= ∂H
∂z (z(t),t)dz(t)
dt
+ ∂H
∂t (z(t),t) = ∂H
∂t (z(t),t) = z(t) −1
t ≡RB(t), (10.41)
where RB(t) denotes the R-transform deﬁned in (10.10) (we have used the very deﬁnition of
z(t) from the previous section). Moreover, from its deﬁnition, we trivially have HB(0) = 0.
Hence we can write
HB(t) :=
 t
0
RB(x)dx.
(10.42)
We already know that H is additive. Thus its derivative, i.e. the R-transform, is also
additive:
RC(t) = RB(t) + RA(t),
(10.43)
as is the case when A is a Wigner matrix. This property is therefore valid as soon as A is
“free” with respect to B, i.e. when the basis that diagonalizes A is a random rotation of the
basis that diagonalizes B.

10.4 Invertibility of the Stieltjes Transform
145
The discussion leading to Eq. (10.42) can be extended to the hciz integral (Eq. (10.23)),
when the rank of the matrix T is very small compared to N. In this case we get3
I(T,B) ≈exp

N
2
n

i=1
HB(ti)

= exp
N
2 Tr HB(T)

,
(10.45)
where ti are the n non-zero eigenvalues of T and with the same HB(t) as above. When T
has rank-1 we recover that Tr HB(T) = HB(t), where t is the sole non-zero eigenvalue
of T.
The above formalism is based on the assumption that g(z) is invertible, which is gen-
erally only true when t = g(z) is small enough. This corresponds to the case where z is
sufﬁciently large. Recall that the expansion of g(z) at large z has coefﬁcients given by the
moments of the random matrix by (2.22). On the other hand, the expansion of H(t) around
t = 0 will give coefﬁcients called the free cumulants of the random matrix, which are
important objects in the study of free probability, as we will show in the next chapter.
10.4 Invertibility of the Stieltjes Transform
The question of the invertibility of the Stieltjes transform arises often enough that it is
worth spending some time discussing it. In Section 10.1, we used the inverse of the lim-
iting Stieltjes transform g(z) to solve Burgers’ equation, which led to the introduction
of the R-transform R(g) = z(g) −1/g. In Section 10.3.1 we invoked the invertibil-
ity of the discrete Stieltjes transform gN(z) to compute the rank-1 hciz integral.
10.4.1 Discrete Stieltjes Transform
Recall the discrete Stieltjes transform of a matrix A with N eigenvalues {λk}:
gA
N(z) = 1
N
N

k=1
1
z −λk
.
(10.46)
This function is well deﬁned for any z on the real axis except on the ﬁnite set {λk}. For
z > λmax, each of the terms in the sum is positive and monotonically decreasing with z so
gA
N(z) is a positive monotonically decreasing function of z. As z →∞, gA
N(z) →0. By
the same argument, for z < λmin, gA
N(z) is a negative monotonically decreasing function
of z tending to zero as z goes to minus inﬁnity. Actually, the normalization of gA
N(z) is such
that we have
3 The same computation can be done for any value of beta, yielding
Iβ(T,B) ≈exp
⎛
⎝Nβ
2
n

i=1
HB(ti)
⎞
⎠= exp
 Nβ
2
Tr HB(T)

,
(10.44)
where Iβ(T,B) is deﬁned in the footnote on page 141 and T has low rank.

146
Addition of Large Random Matrices
−4
−2
0
2
4
z
−5
0
5
gn(z)
−5
0
5
g
−5
0
5
(g)
(g)
lmax
lmin
Figure 10.2 (left) A particular gA
N(z) for A a Wigner matrix of size N = 5 shown for real values of
z. The gray areas left of λmin and right of λmax show the values of z for which it is invertible. (right)
The inverse function z(g). Note that g(z) behaves as 1/z near zero and tends to λmin and λmax as g
goes to plus or minus inﬁnity respectively.
gA
N(z) = 1
z + O
 1
z2

when
|z| →∞.
(10.47)
For large |z|, gA
N(z) is thus invertible and its inverse behaves as
z(g) = 1
g + regular terms
when
|g| →0.
(10.48)
If we consider values of gA
N(z) for z > λmax, we realize that the function takes all possible
positive values once and only once, from the extremely large (near z = λmax) to almost zero
(when z →∞). Similarly, all possible negative values are attained when z ∈(−∞,λmin)
(see Fig. 10.2 left). We conclude that the inverse function z(g) exists for all non-zero values
of g. The behavior of gA
N(z) near λmin and λmax gives us the asymptotes
lim
g→−∞z(g) = λmin
and
lim
g→∞z(g) = λmax.
(10.49)
10.4.2 Limiting Stieltjes Transform
Let us now discuss the inverse function of the limiting Stieltjes transform g(z). The limiting
Stieltjes transform satisﬁes Eq. (2.41), which we recall here:
g(z) =

supp{ρ}
ρ(x)dx
z −x ,
(10.50)

10.4 Invertibility of the Stieltjes Transform
147
where ρ(λ) is the limiting spectral distribution and may contain Dirac deltas. We denote
λ± the edges of the support of ρ. We have that for z > λ+, g(z) is a positive, monotonically
decreasing function of z. Similarly for z < λ−, g(z) is a negative, monotonically decreasing
function of z. From the normalization of ρ(λ), we again ﬁnd that
g(z) = 1
z + O
 1
z2

when
|z| →∞.
(10.51)
Using the same arguments as for the discrete Stieltjes transform, we have that the inverse
function z(g) exists for small arguments and behaves as
z(g) = 1
g + regular terms
when
|g| →0.
(10.52)
The behavior of g(z) at λ± can be different from that of gN(z) at its extreme eigenval-
ues. The points λ± are singular points of g(z). If the density near λ+ goes to zero as
ρ(λ) ∼(λ+ −λ)θ for some θ > 0 (typically θ = 1/2) then the integral (10.50) converges
at z = λ+ and g+ := g(λ+) is a ﬁnite number. For z < λ+ the function g(z) has a branch
cut and is ill deﬁned for z on the real axis. The point z = λ+ is an essential singularity of
g(z). The function is clearly no longer invertible for z < λ+. Similarly, if ρ(λ) grows as a
positive power near λ−, then g(z) is invertible up to the point g−:= g(λ−).
If the density ρ(λ) does not go to zero at one of its edges (or if it has a Dirac delta), the
function g(z) diverges at that edge. We may still deﬁne g± = limz→λ± g(z) if we allow g±
to be plus or minus inﬁnity.
In all cases, the inverse function z(g) exists in the range g−≤g ≤g+, with the property
z(g±) = λ±.
(10.53)
In the unit Wigner case, we have λ± = ±2 and g± = ±1 and the inverse function z(g)
only exists between −1 and 1 (see Fig. 10.3).
10.4.3 The Inverse Stieltjes Transform for Larger Arguments
In some computations, as in the hciz integral, one needs the value of z(g) beyond g±.
What can we say then? First of all, one should not be fooled by spurious solutions of the
inversion problem. For example in the Wigner case we know that g(z) satisﬁes
g + 1
g −z = 0,
(10.54)
so we would be tempted to write
z(g) = g + 1
g
(10.55)
for all g. But this is wrong as g + 1/g is not the inverse of g(z) for |g| > 1 (Fig. 10.3).
The correct way to extend z(g) beyond g± is to realize that in most computation, we
use g(z) as an approximation for gN(z) for very large N. For z > λ+ the function gN(z)

148
Addition of Large Random Matrices
−5
0
5
z
−1.0
−0.5
0.0
0.5
1.0
(z)
Re (z)
Im (z)
−4
−2
0
2
4
g
−5
0
5
(g)
(g)
l± = ± 2
wrong
Figure 10.3 (left) The limiting function g(z) for a Wigner matrix, a typical density that vanishes at
its edges. The function is plotted against a real argument. In the white part of the graph, the function
is ill deﬁned and it is shown here for a small negative imaginary part of its argument. In the gray
part (z < λ−and z > λ+) the function is well deﬁned, real and monotonic. It is therefore invertible.
(right) The inverse function z(g) only exists for g−≤g ≤g+ and has a 1/g singularity at zero. The
dashed lines show the extension of z(g) to all values of g that are natural when we think of g(z) as
the limit of gN(z) with maximal and minimal eigenvalues λ±. The dotted lines indicate the wrong
branch of the solution of z(g) = g + 1/g.
converges to g(z) and this approximation can be made arbitrarily good for large enough
N. On the other hand we know that on the support of ρ, gN(z) does not converge to g(z).
The former has a series of simple poles at random locations, while the later has typically
a branch cut.
At large but ﬁnite N, there will be a maximum eigenvalue λmax. This eigenvalue is
random but to dominant order in N it converges to λ+, the edge of the spectrum. For z
above but very close to λ+ we should think of gN(z) as
gN(z) ≈g(z) + 1
N
1
z −λmax
≈g(z) + 1
N
1
z −λ+
.
(10.56)
Because 1/N goes to zero, the correction above does not change the limiting value of g(z)
at any ﬁnite distance from λ+. On the other hand, this correction does change the behavior
of the inverse function z(g). We now have
lim
z→λ+
gN(z) →∞
and
z(g) = λ+ for g > g+.
(10.57)
For negative z and negative g, the same argument follows near λ−. We realize that, while
the limiting Stieltjes transform g(z) loses all information about individual eigenvalues,
its inverse function z(g), or really the large N limit of the inverse of the function gN(z),
retains information about the smallest and largest eigenvalues. In Chapter 14 we will study
random matrices where a ﬁnite number of eigenvalues lie outside the support of ρ. In the
large N limit, these eigenvalues do not change the density or g(z) but they do show up in
the inverse function z(g).

10.5 The Full-Rank HCIZ Integral
149
Let us deﬁne zbulk(g), the inverse function of g(z) without considering extreme eigen-
values or outliers. In the presence of outliers we have λmax ≥λ+ and gmax := g(λmax) ≤
g(λ+) and similarly for gmin. With arguments similar to those above we ﬁnd the following
result for the limit of the inverse of gN(z):
z(g) =
⎧
⎪⎨
⎪⎩
λmin
for g ≤gmin,
zbulk(g)
for gmin < g < gmax,
λmax
for g ≥gmax.
(10.58)
In the absence of outliers the result still applies with max and min (extreme eigenvalues)
replaced by + and −(edge of the spectrum), respectively.
10.4.4 Large t Behavior of It
Now that we understand the behavior of z(g) for larger arguments we can go back to
our study of the rank-1 hciz integral. There is indeed an apparent paradox in the result
of our computation of It(B). For a given matrix B there are two immediate bounds to
It(B) = Zt(B)/Zt(0):
exp
Ntλmin
2

≤It(B) ≤exp
Ntλmax
2

,
(10.59)
where λmin and λmax are the smallest and largest eigenvalues of B, respectively. Focusing
on the upper bound, we have
HB(t) ≤tλmax.
(10.60)
On the other hand, the anti-derivative of the R-transform for a unit Wigner matrix reads
RW(t) = t −→HW(t) = t2
2 ,
(10.61)
whereas λmax →λ+ = 2. One might thus think that the quadratic behavior of HW(t)
violates the bound (10.60) for t > 4. We should, however, remember that Eq. (10.42) is in
fact only valid for t < g+, the value at which g(z) ceases to be invertible. In the absence
of outliers, g+ = g(λ+). For a unit Wigner this point is g+ = g(2) = 1; the bound is
not violated. For t > g+, one can still compute HB(t) but the result depends explicitly
on λmax.
Now that we understand the behavior of z(g) for larger arguments, including in the
presence of outliers, we can extend our result for HB(t) for large t’s. We just need to use
Eq. (10.58) into Eq. (10.41):
dHB(t)
dt
=
0
RB(t)
for t ≤gmax := g(λmax),
λmax −1/t
for t > gmax,
(10.62)
where the largest eigenvalue λmax can be either the edge of the spectrum λ+ or a true
outlier. We will show in Section 13.3 how this result can also be derived for Wigner
matrices using the replica method.
10.5 The Full-Rank HCIZ Integral
We have deﬁned in Eq. (10.23) the hciz integral as a generalization of the Fourier transform
for matrices, and have seen how to evaluate this integral in the limit N →∞when one
matrix is of low rank. A generalized hciz integral Iβ(A,B) can be deﬁned as

150
Addition of Large Random Matrices
Iβ(A,B) =

G(N)
dU e
βN
2 Tr AUBU†,
(10.63)
where the integral is over the (ﬂat) Haar measure of the compact group U ∈G(N) =
O(N),U(N) or Sp(N) in N dimensions and A,B are arbitrary N × N symmetric (resp.
Hermitian or symplectic) matrices, with, correspondingly, β = 1,2 or 4. Note that by
construction Iβ(A,B) can only depend on the eigenvalues of A and B, since any change of
basis on B (say) can be reabsorbed in U, over which we integrate. Note also that the Haar
measure is normalized, i.e.

G(N) dU = 1.
Interestingly, it turns out that in the unitary case G(N) = U(N) (β = 2), the hciz
integral can be expressed exactly, for all N, as the ratio of determinants that depend on
A,B and additional N-dependent prefactors. This is the Harish-Chandra–Itzykson–Zuber
celebrated result, which cannot be absent from a book on random matrices:
I2(A,B) =
cN
N(N2−N)/2
det

eNνiλj 
(A)(B) ,
(10.64)
with {νi}, {λi} the eigenvalues of A and B, (A), (B) are the Vandermonde determinants
of A and B, and cN = N−1
ℓ
ℓ!.
Although this result is fully explicit for β = 2, the expression in terms of determinants is
highly non-trivial and quite tricky. For example, the expression becomes degenerate (0/0)
whenever two eigenvalues of A (or B) coincide. Also, as is well known, determinants
contain N! terms of alternating signs, which makes their order of magnitude very hard
to estimate a priori. The aim of this technical section is to discuss how the hciz result can
be obtained using the Karlin–McGregor equation (9.69). We then use the mapping to the
Dyson Brownian motion to derive a large N approximation for the full-rank hciz integral
in the general case.
10.5.1 HCIZ and Karlin–McGregor
In order to understand the origin of Eq. (10.64), the basic idea is to interpret the hciz
integrand in the unitary case, exp[N Tr AUBU†], as a part of the diffusion propagator in
the space of Hermitian matrices, and use the Karlin–McGregor formula.
Indeed, adding to A a sequence of inﬁnitesimal random Gaussian Hermitian matrices
of variance dt/N, the probability to end up with matrix B in a time t = 1 is given by
P(B|A) ∝NN2/2 e−N/2 Tr(B−A)2,
(10.65)
where we drop an overall normalization constant in our attempt to understand the structure
of Eq. (10.64). The corresponding eigenvalues follow a Dyson Brownian motion, namely
dxi =
1
1
N dBi + 1
N
N

j=1
ji
dt
xi −xj
,
(10.66)
with xi(t = 0) = νi and xi(t = 1) = λi. Now, for β = 2 we can use the Karlin–McGregor
equation (9.69) to derive the conditional distribution of the {λi}, given by

10.5 The Full-Rank HCIZ Integral
151
P({λi}|{νi}) = (B)
(A)P(⃗λ,t = 1|⃗ν),
(10.67)
where P(⃗λ,t = 1|⃗ν) is given by a determinant, Eq. (9.66). With the present normalization,
this determinant reads
P(⃗λ,t = 1|⃗ν) =
 N
2π
N/2
e−N
2 (Tr A2+Tr B2) det

eNνiλj

.
(10.68)
Now, the distribution of eigenvalues of B can be computed from Eq. (10.65). First we
make P(B|A) unitary-invariant by integrating over U(N):
P(B|A) →P(B|A) = NN2/2

U(N) dU e−N/2 Tr(UBU†−A)2
 N
= NN2/2e−N
2 (Tr A2+Tr B2) I2(A,B)
 N
,
(10.69)
where  N =

U(N) dU is the “volume” of the unitary group U(N). This new measure,
by construction, only depends on {λi}, the eigenvalues of B. Changing variables from B
to {λi} introduces a Jacobian, which in the unitary case is the square of the Vandermonde
determinant of B, 2(B). We thus ﬁnd a second expression for the distribution of the {λi}:
P({λi}|{νi}) ∝NN2/22(B)e−N
2 (Tr A2+Tr B2)I2(A,B).
(10.70)
Comparing with Eqs. (10.67) and (10.68) we thus ﬁnd
I2(A,B) ∝N(N−N2)/2 det

eNνiλj

(A)(B) ,
(10.71)
which coincides with Eq. (10.64), up to an overall constant cN which can be obtained
by taking the limit A = 1, i.e. when all the eigenvalues of A are equal to 1. The limit is
singular but one can deal with it in a way similar to the one used by Br´ezin and Hikami
to go from Eq. (6.65) to (6.67). In this limit, the right hand side of Eq. (10.71) reads
exp(N Tr B)/cN, while the left hand side is trivially equal to exp(N Tr B). Hence a factor
cN is indeed missing in the right hand side of Eq. (10.71).
Equation (10.64) can also be used to obtain an exact formula for the rank-1 hciz
integral (when β = 2). The trick is to have one of the eigenvalues of νi equal to some
non-zero number t and let the N −1 others go to zero. The limit can again be dealt with
in the same way as Eq. (6.65). One ﬁnally ﬁnds
I2(t,B) = (N −1)!
(Nt)N−1
N

j=1
eNtλj

kj(λj −λk).
(10.72)
The above formula may look singular at t = 0, but we have limt→0 I2(t,B) = 1 as
expected.
10.5.2 HCIZ at Large N: The Euler–Matytsin Equations
We now explain how I2(A,B) can be estimated for large matrix size, using a Dyson
Brownian representation of P(B|A), Eq. (10.66). In terms of these interacting Brownian
motions, the question is how to estimate the probability that the xi(t) start at xi(t = 0) =

152
Addition of Large Random Matrices
νi and end at xi(t = 1) = λi, when their trajectories are determined by Eq. (10.66), which
we rewrite as
dxi =
1
1
N dBi −∂xi V dt,
V ({xi}) := −1
N

i<j
ln |xi −xj|.
(10.73)
The probability of a given trajectory for the N Brownian motions between time t = 0 and
time t = 1 is then given by4
P({xi(t)}) = Z−1 exp −
⎡
⎣N
2
 1
0
dt

i

˙xi + ∂xi V
2
⎤
⎦:= Z−1e−N2S,
(10.74)
where Z is a normalization factor that we will not need explicitly. Expanding the square
as ˙x2
i + 2∂xi V ˙xi + (∂xi V )2, one can decompose S = S1 + S2 into a total derivative term
equal, in the continuum limit, to boundary terms, i.e.
S1 = −1
2
(
dxdyρC(x)ρC(y) ln |x −y|
)C=B
C=A
(10.75)
and
S2 :=
1
2N
 1
0
dt
N

i=1
&
˙x2
i + (∂xi V )2'
.
(10.76)
We now look for the “instanton” trajectory that contributes most to the probability P for
large N, in other words the trajectory that minimizes S2. This extremal trajectory is such
that the functional derivative of S2 with respect to all xi(t) is zero:
−2d2xi
dt2 + 2
N

ℓ=1
∂2
xi,xℓV ∂xℓV = 0,
(10.77)
which leads, after a few algebraic manipulations, to
d2xi
dt2 = −2
N2

ℓi
1
(xi −xℓ)3 .
(10.78)
This can be interpreted as the motion of unit mass particles, accelerated by an attractive
force that derives from an effective two-body potential φ(r) = −(Nr)−2. The hydrody-
namical description of such a ﬂuid, justiﬁed when N →∞, is given by the Euler equations
for the density ﬁeld ρ(x,t) and the velocity ﬁeld v(x,t):
∂tρ(x,t) + ∂x[ρ(x,t)v(x,t)] = 0
(10.79)
and
∂tv(x,t) + v(x,t)∂xv(x,t) = −
1
ρ(x,t)∂x(x,t),
(10.80)
where (x,t) is the pressure ﬁeld, which reads, from the “virial” formula for an interact-
ing ﬂuid at temperature T ,5
4 We neglect here a Jacobian which is not relevant to compute the leading term of I2(A,B) in the large N limit.
5 See e.g. Le Bellac et al. [2004], p. 138.

10.5 The Full-Rank HCIZ Integral
153
 = ρT −1
2ρ

ℓi
|xi −xℓ|φ′(xi −xℓ) ≈−ρ
N2

ℓi
1
(xi −xℓ)2,
(10.81)
because the ﬂuid describing the instanton is at zero temperature, T = 0. Now, writing
xi −xℓ≈(i −ℓ)/(Nρ) and ∞
n=1 n−2 = π2
6 , one ﬁnally ﬁnds
(x,t) = −π2
3 ρ(x,t)3.
(10.82)
Equations (10.79) and (10.80) for ρ and v with  given by (10.82) are called the Euler–
Matytsin equations. They should be solved with the following boundary conditions:
ρ(x,t = 0) = ρA(x);
ρ(x,t) = ρB(x);
(10.83)
the velocity ﬁeld v(x,t = 0) should be chosen such that these boundary conditions are
fulﬁlled.
Expressing S2 in terms of the solution of the Euler–Matytsin equations gives, in the
continuum limit,
S2(A,B) ≈1
2

dxρ(x,t)
-
v2(x,t) + π2
3 ρ2(x,t)
.
.
(10.84)
Hence, the probability P({λi}|{νi}) to observe the set of eigenvalues {λi} of B for a
given set of eigenvalues νi for A is, in the large N limit, proportional to exp[−N2(S1 +
S2)]. Comparing with Eq. (10.70), we get as a ﬁnal expression for F2(A,B)
:=
−limN→∞N−2 ln I2(A,B):
F2(A,B) = 3
4 + S2(A,B) −1
2

dx x2(ρA(x) + ρB(x))
+ 1
2

dxdy [ρA(x)ρA(y) + ρB(x)ρB(y)] ln |x −y|.
(10.85)
This result was ﬁrst derived in Matytsin [1994], and proven rigorously in Guionnet and
Zeitouni [2002]. Note that this expression is symmetric in A,B, as it should be, because
the solution of the Euler–Matytsin equations for the time reversed path from ρB to ρA are
simply obtained from ρ(x,t) →ρ(x,1 −t) and v(x,t) →−v(x,1 −t), which leaves
S2(A,B) unchanged.
The whole calculation above can be repeated for the β = 1 (orthogonal group) or
β = 4 (symplectic group) with the ﬁnal (simple) result Fβ(A,B) = βF2(A,B)/2.
Bibliographical Notes
• The Burgers’ equation in the context of random matrices:
– L. C. G. Rodgers and Z. Shi. Interacting Brownian particles and the Wigner law.
Probability Theory and Related Fields, 95:555–570, 1993,
– J.-P. Blaizot and M. A. Nowak. Universal shocks in random matrix theory. Physical
Review E, 82:051115, 2010,
– G. Menon. Lesser known miracles of Burgers equation. Acta Mathematica Scientia,
32(1):281–94, 2012.

154
Addition of Large Random Matrices
• The hciz integral: Historical papers:
– Harish-Chandra. Differential operators on a semisimple Lie algebra. American Jour-
nal of Mathematics, 79:87–120, 1957,
– C. Itzykson and J.-B. Zuber. The planar approximation. II. Journal of Mathematical
Physics, 21:411–421, 1980,
for a particularly insightful introduction, see T. Tao,
http://terrytao.wordpress.com/2013/02/08/theharish-chandra-itzykson-zuber-integral-
formula/.
• The low-rank hciz integral:
– E. Marinari, G. Parisi, and F. Ritort. Replica ﬁeld theory for deterministic models. II.
A non-random spin glass with glassy behaviour. Journal of Physics A: Mathematical
and General, 27(23):7647, 1994,
– A. Guionnet and M. Ma¨ıda. A Fourier view on the R-transform and related asymp-
totics of spherical integrals. Journal of Functional Analysis, 222(2):435–490, 2005.
• The hciz integral: Large N limit:
– A. Matytsin. On the large-N limit of the Itzykson-Zuber integral. Nuclear Physics B,
411:805–820, 1994,
– A. Guionnet and O. Zeitouni. Large deviations asymptotics for spherical integrals.
Journal of Functional Analysis, 188(2):461–515, 2002,
– B. Collins, A. Guionnet, and E. Maurel-Segala. Asymptotics of unitary and orthogo-
nal matrix integrals. Advances in Mathematics, 222(1):172–215, 2009,
– J. Bun, J. P. Bouchaud, S. N. Majumdar, and M. Potters. Instanton approach to large
N Harish-Chandra-Itzykson-Zuber integrals. Physical Review Letters, 113:070201,
2014,
– G. Menon. The complex Burgers equation, the HCIZ integral and the Calogero-
Moser system, unpublished, 2017, available at: https://www.dam.brown.edu/people/
menon/talks/cmsa.pdf.
• On the classical virial theorem, see
– M. Le Bellac, F. Mortessagne, and G. G. Batrouni. Equilibrium and Non-Equilibrium
Statistical Thermodynamics. Cambridge University Press, Cambridge, 2004.

11
Free Probabilities
In the previous chapter we saw how to compute the spectrum of the sum of two large
random matrices, ﬁrst when one of them is a Wigner and later when one is “rotationally
invariant” with respect to the other. In this chapter, we would like to formalize the notion
of relative rotational invariance, which leads to the abstract concept of freeness.
The idea is as follows. In standard probability theory, one can work abstractly by deﬁn-
ing expectation values (moments) of random variables. The concept of independence is
then equivalent to the factorization of moments (e.g. E[A3B2] = E[A3]E[B2] when A and
B are independent).
However, random matrices do not commute in general and the concept of factorization
of moments is not powerful enough to deal with non-commuting random objects. Follow-
ing von Neumann, Voiculescu extended the concept of independence to non-commuting
objects and called this property freeness. He then showed how to characterize the sum and
the product of free variables. It was later realized that large rotationally invariant matrices
provide an explicit example of (asymptotically) free random variables. In other words, free
probabilities gave us very powerful tools to compute sums and products of large random
matrices. We have already encountered the free addition; the free product will allow us to
study sample covariance matrices in the presence of non-trivial true correlations.
This chapter may seem too dry and abstract for someone looking for applications. Bear
with us, it is in fact not that complicated and we will keep the jargon to a minimum. The
reward will be one of the most powerful and beautiful recent developments in random
matrix theory, which we will expand upon in Chapter 12.
11.1 Algebraic Probabilities: Some Deﬁnitions
The ingredients we will need in this chapter are as follows:1
• A ring R of random variables, which can be non-commutative with respect to the multi-
plication.2
1 In mathematical language, the ﬁrst three items give a *-algebra, while τ gives a tracial state on this algebra.
2 Recall that a ring is a set equipped with two binary operations that generalize the arithmetic operations of addition and
multiplication.
155

156
Free Probabilities
• A ﬁeld of scalars, which is usually taken to be C. The scalars commute with everything.
• An operation ∗, called involution. For instance, ∗denotes the conjugate for com-
plex numbers, the transpose for real matrices, and the conjugate transpose for complex
matrices.
• A positive linear functional τ(.) (R →C) that satisﬁes τ(AB) = τ(BA) for A,B ∈R.
By positive we mean τ(AA∗) is real non-negative. We also require that τ be faithful,
in the sense that τ(AA∗) = 0 implies A = 0. For instance, τ can be the expectation
operator E[.] for standard probability theory, or the normalized trace operator 1
N Tr(.)
for a ring of matrices, or the combined operation 1
N E[Tr(.)].
We will call the elements in R the random variables and denote them by capital letters.
For any A ∈R and k ∈N, we call τ(Ak) the kth moment of A and we assume in what
follows that τ(Ak) is ﬁnite for all k. In particular, we call τ(A) the mean of A and τ(A2) −
τ(A)2 the variance of A. We will say that two elements A and B have the same distribution
if they have the same moments of all orders.3
The ring of variables must have an element called 1 such that A1 = 1A = A for every
A. It satisﬁes τ(1) = 1. We will call 1 and its multiples α1 constants. Adding a constant
simply shifts the mean as
τ(A + α1) = τ(A) + α.
(11.1)
11.2 Addition of Commuting Variables
In this section, we recall some well-known properties of commuting random variables, i.e.
such that
AB = BA,
∀A,B ∈R.
(11.2)
Note that A is not necessarily a real (or complex) number but can be an element of
a more abstract ring. We will say that A and B are independent, if τ(p(A)q(B)) =
τ(p(A))τ(q(B)) for any polynomial p,q. This condition is equivalent to the factorization
of moments.
From a scalar α we can build the constant α1 and write A+α to mean A+α1. Constants
of the ring are independent of all other random variables, so if A and B are independent,
A + α and B are also independent. This setting recovers the classical probability theory of
commutative random variables (with ﬁnite moments to every order).
11.2.1 Moments
Now let us study the moments of the sum of independent random variables A+B. First we
trivially have by linearity
τ(A + B) = τ(A) + τ(B).
(11.3)
3 This is of course not correct for standard commuting random variables: some distributions are not uniquely determined by
their moments.

11.2 Addition of Commuting Variables
157
From now on we will assume τ(A) = τ(B) = 0, i.e. A,B have mean zero, unless stated
otherwise. For a non-zero mean variable ˜A, we write A = ˜A −τ( ˜A) such that τ(A) = 0.
One can recover the formulas for moments and cumulants of ˜A simply by substituting
A →˜A −τ( ˜A) in all formulas written for zero mean A. The procedure is straightforward
but leads to rather cumbersome results.
For the second moment,
τ

(A + B)2
= τ(A2) + τ(B2) + 2τ(AB)
= τ(A2) + τ(B2) + 2τ(A)τ(B) = τ(A2) + τ(B2),
(11.4)
i.e. the variance is also additive. For the third moment, we have
τ

(A + B)3
= τ(A3) + τ(B3) + 3τ(A)τ(B2) + 3τ(B)τ(A2) = τ(A3) + τ(B3),
(11.5)
which is also additive. However, the fourth and higher moments are not additive anymore.
For example we get, expanding (A + B)4,
τ

(A + B)4
= τ(A4) + τ(B4) + 6τ(A2)τ(B2).
(11.6)
11.2.2 Cumulants
For zero mean variables the ﬁrst three moments are additive but not the higher ones.
Nevertheless, certain combinations of higher moments are additive; we call them cumulants
and denote them as κn for the nth cumulant. Note that for a variable with non-zero mean
˜A, the second and third cumulants are the second and third moments of A := ˜A −τ( ˜A):
κ1( ˜A) = τ( ˜A),
κ2( ˜A) = τ(A2) = τ( ˜A2) −τ( ˜A)2,
(11.7)
κ3( ˜A) = τ(A3) = τ( ˜A3) −3τ( ˜A2)τ( ˜A) + 2τ( ˜A)3.
For the fourth cumulant, let us consider for simplicity zero mean variables A and B, and
deﬁne κ4 as
κ4(A) := τ(A4) −3τ(A2)2.
(11.8)
Then we can verify that
κ4 (A + B) = τ

(A + B)4
−3τ

(A + B)22
= τ(A4) + τ(B4) + 6τ(A2)τ(B2) −3

τ(A2) + τ(B2)
2
= τ(A4) −3τ(A2)2 + τ(B4) −3τ(B2)2 = κ4(A) + κ4(B),
(11.9)
which is additive again.
In general, τ((A + B)n) will be of the form τ(An) + τ(Bn) plus some homogeneous
mix of lower order terms. We can then deﬁne the nth cumulant κn iteratively such that

158
Free Probabilities
κn(A + B) = κn(A) + κn(B),
(11.10)
where
κn(A) = τ(An) + lower order terms moments.
(11.11)
In order to have a compact deﬁnition of cumulants, recall that we are looking for quantities
that are additive for independent variables. But we already know that the log-characteristic
function introduced in Eq. (8.4) is additive. In the present context, we deﬁne the character-
istic function as4
ϕA(t) = τ

eitA
,
(11.12)
where the exponential function is formally deﬁned through its power series:
τ(eitA) =
∞

ℓ=0
(it)ℓ
ℓ! τ(Aℓ),
(11.13)
hence the characteristic function is also the moment generating function. Now, from the
formal deﬁnition of the exponential and the factorization of moments one can easily show
that for independent, commuting A,B,
ϕA+B(t) = ϕA(t)ϕB(t).
(11.14)
Here is an algebraic proof. For each k,
τ((A + B)k) =
k

i=0
k
i

τ(Ai)τ(Bk−i),
(11.15)
with which we get
ϕA+B(t) =
∞

k=0

i≤k
(it)k
k!
k
i

τ(Ai)τ(Bk−i) =

i≤k
(it)k−iτ(Bk−i)
(k −i)!
(it)iτ(Ai)
i!
=
⎛
⎝
i
(it)iτ(Ai)
i!
⎞
⎠
⎛
⎝
j
(it)jτ(Bj)
j!
⎞
⎠= ϕA(t)ϕB(t).
(11.16)
We now deﬁne HA(t) := log ϕA(t). Then, for independent, commuting A,B, we have
HA+B(t) = HA(t) + HB(t).
(11.17)
We can expand HA(t) as a power series of t and call the corresponding coefﬁcients the
cumulants, i.e.
HA(t) = log τ

eitA
:=
∞

n=1
κn(A)
n!
(it)n.
(11.18)
4 The factor i in the deﬁnition is not necessary in this setting as the formal power series of the exponential and the logarithm do
not need to converge. We nevertheless include it by analogy with the Fourier transform.

11.2 Addition of Commuting Variables
159
From the additive property of H, the cumulants deﬁned in the above way are automatically
additive. In fact, using the power series for log(1 + x), we have
HA(t) =
∞

n=1
(−1)n−1
n
 ∞

k=1
(it)k
k! τ(Ak)
n
≡
∞

n=1
κn(A)
n!
(it)n.
(11.19)
Matching powers of (it) we obtain an expression for κn for any n. We can work out by hand
the ﬁrst few cumulants. For example, for n = 1, Eq. (11.19) readily yields κ1(A) = τ(A).
We now assume A has mean zero, i.e. τ(A) = 0. Then
τ

eitA
= 1 + (it)2
2 τ(A2) + (it)3
6 τ(A3) + (it)4
24 τ(A4) + · · · ,
(11.20)
whereas the ﬁrst few terms in the expansion of (11.19) are
HA(t) = (it)2
2 τ(A2) + (it)3
6 τ(A3) + (it)4
τ(A4)
24
−τ(A2)2
8

+ · · · ,
(11.21)
from which we recover the ﬁrst four cumulants deﬁned above:
κ1(A) = 0,
κ2(A) = τ(A2),
κ3(A) = τ(A3),
κ4(A) = τ(A4) −3τ(A2)2. (11.22)
The expression for κn soon becomes very cumbersome for larger n. Nevertheless, by
exponentiating Eq. (11.18) and matching with Eq. (11.13), one can extract the following
moment–cumulant relation for commuting variables:
τ(An) =

r1,r2,...,rn≥0
r1+2r2+···+nrn=n
n! κr1
1 κr2
2 · · · κrn
n
(1! )r1(2! )r2 · · · (n! )r2r1! r2! · · · rn!
= κn + products of lower order terms + κn
1 .
(11.23)
In particular, the scaling properties for the moments and cumulants (see (11.28) below)
are consistent due to the relation r1 + 2r2 + · · · + nrn = n.
Exercise 11.2.1
Cumulants of a constant
Show that a constant α1 has κ1 = α and κn = 0 for n ≥2. (Hint: compute
Hα1(k) = log

τ

eikα1
.)
11.2.3 Scaling of Moments and Cumulants
Moments and cumulants obey simple transformation rules under scalar addition and multi-
plication. For example, when adding a constant to a variable, ˜A := A+α, where τ(A) = 0,
we only change the ﬁrst cumulant:
κ1( ˜A) = α
and
κn( ˜A) = κn(A)
for
n ≥2.
(11.24)

160
Free Probabilities
For the case of multiplication by an arbitrary scalar α, by commutativity of scalars and
linearity of τ we have
τ

(αA)k
= αkτ

Ak
.
(11.25)
For the cumulant, we ﬁrst look at the scaling of the log-characteristic function:
HαA(t) = log

τ

eitαA
= HA(αt).
(11.26)
And by (11.18), we have
HαA(t) = HA(αt) =
∞

n=1
αnκn(A)
n!
(it)n.
(11.27)
Thus we have the simple scaling property
κn(αA) = αnκn(A).
(11.28)
11.2.4 Law of Large Numbers and Central Limit Theorem
Continuing our study of algebraic probabilities, we would like to recover two very impor-
tant theorems in probability theory, namely the law of large numbers (lln) and the central
limit theorem (clt). The ﬁrst states that the sample average converges to the mean (a
constant) as the number of observations N →∞, and the second that a large sum of
properly centered and rescaled random variables converge to a Gaussian.
First we need to deﬁne in our context what we mean by a constant and a Gaussian. For
simplicity, we can think of the variables in this section as standard random variables. We
will later introduce non-commutating cumulants. The arguments of this section apply in
the non-commutative case with independence replaced by freeness.
We have deﬁned the constant variable A = α1, which satisﬁes
κ1(A) = α,
κℓ(A) = 0, ∀ℓ> 1.
(11.29)
Then we deﬁne the “Gaussian” random variable as an element A that satisﬁes
κ2(A)  0,
κℓ(A) = 0, ∀ℓ> 2.
(11.30)
Note that this deﬁnition (in the scalar case) is equivalent to the standard Gaussian random
variable with density
Pμ,σ 2(x) =
1
√
2πσ 2 exp

−(x −μ)2
2σ 2

,
(11.31)
with κ1 = μ and κ2 = σ 2.
By extension, we call κ1(A) the mean, and κ2(A) the variance. Now we can give a
simple proof for the lln and clt within our algebraic setting. Let

11.3 Non-Commuting Variables
161
SK := 1
K
K

i=1
Ai,
(11.32)
where Ai are K iid variables.5 Then by (11.28) and the additive property of cumulants, we
get that
κℓ(SK) = K
Kℓκℓ(A)
K→∞
→
0
κ1(A),
if ℓ= 1,
0,
if ℓ> 1.
(11.33)
In other words, SK converges to a constant in the sense of cumulants.
Assume now that κ1(A) = 0 and consider
SK :=
1
√
K
K

i=1
Ai.
(11.34)
Then it is easy to see that
κℓ(SK) =
K
Kℓ/2 κℓ(A) →
⎧
⎪⎪⎨
⎪⎪⎩
0,
if ℓ= 1,
κ2(A),
if ℓ= 2,
0,
if ℓ> 2.
(11.35)
In other words, SK converges to a “Gaussian” random variable with variance κ2(A) =
τ(A2), in the sense of cumulants.
In our algebraic probability setting we have made the assumption that the variables we
consider have ﬁnite moments of all orders. This is a very strong assumption. In particular it
excludes any variable whose probability decays as a power law. If we relaxed this assump-
tion we would ﬁnd that some sums of power-law distributed variables converge not to a
Gaussian but to a L´evy-stable distribution. A similar concept exists in the non-commutative
case, but it is beyond the scope of this book.
11.3 Non-Commuting Variables
We now return to our original goal of developing an extension of standard probabilities for
non-commuting objects. One of the goals is to generalize the law of addition of independent
variables. We consider a variable equal to A+B where A and B are now non-commutative
objects such as large random matrices. If we compute the ﬁrst three moments of A + B,
no particular problems arise thanks to the tracial property of τ, and they behave as in the
commutative case. For example, consider the third moment
5 iid copies are variables Ai that have exactly the same cumulants and are all independent. We did not deﬁne independence for
more than two variables but the factorization of moments can be easily extended to more variables. Note that pairwise
independence is not enough to assure independence as a group. For example if x1, x2 and x3 are iid Gaussians, the Gaussian
variables x1, x2 and y3 = sign(x1x2)|x3| are pairwise independent but not all independent as E[x1x2y3] > 0 whereas
E[x1]E[x2]E[y3] = 0.

162
Free Probabilities
τ

(A + B)3
= τ(A3) + τ(A2B) + τ(ABA) + τ(BA2)
+ τ(AB2) + τ(BAB) + τ(B2A) + τ(B3).
(11.36)
But since τ(A2B) = τ(ABA) = τ(BA2) (and similarly when A appears once and B
twice), the classic independence property
τ(A2B) = τ(A2)τ(B) = 0
(11.37)
appears to be sufﬁcient. Things become more interesting for the fourth moment. Indeed,
τ

(A + B)4
= τ(A4) + τ(A3B) + τ(A2BA) + τ(ABA2) + τ(BA3)
+ τ(A2B2) + τ(ABAB) + τ(BA2B) + τ(AB2A) + τ(BABA)
+ τ(B2A2) + τ(B3A) + τ(B2AB) + τ(BAB2) + τ(AB3) + τ(B4)
= τ(A4) + 4τ(A3B) + 4τ(A2B2) + 2τ(ABAB) + 4τ(AB3) + τ(B4),
(11.38)
where in the second step we again used the tracial property of τ. For commutative random
variables, independence of A,B means τ(A2B2) = τ(A2)τ(B2), and this is enough to
treat all the terms above. In the non-commutative case, we also need to handle the term
τ (ABAB). In general ABAB is not equal to A2B2. “Independence” is therefore not
enough to deal with this term, so we need a new concept. A radical solution would be
to postulate that τ(ABAB) is zero whenever τ(A) = τ(B) = 0. As we compute higher
moments of A + B we will encounter more and more complicated similar mixed moments.
The concept of freeness deals with all such terms at once.
11.3.1 Freeness
Given two random variables A,B, we say they are free if for any polynomials p1, . . . ,pn
and q1, . . . ,qn such that
τ(pk(A)) = 0,
τ(qk(B)) = 0,
∀k,
(11.39)
we have
τ (p1(A)q1(B)p2(A)q2(B) · · · pn(A)qn(B)) = 0.
(11.40)
We will call a polynomial (or a variable) traceless if τ(p(A)) = 0. Note that α1 is free with
respect to any A ∈R because τ(p(α1)) ≡p(α1), from the deﬁnition of 1. Hence,
τ(p(α1)) = 0 ⇔p(α1) = 0.
(11.41)
Moreover, it is easy to see that if A,B are free, then p(A),q(B) are free for any polynomials
p,q. By extension, F(A) and G(B) are also free for any function F and G deﬁned by their
power series.
The freeness is non-trivial only in the non-commutative case. For the commutative case,
it is easy to check that A,B are free if and only if either A or B is a constant. Free random

11.3 Non-Commuting Variables
163
variables are “maximally” non-commuting, in a sense made more precise for the example
of free random matrices in the next chapter. For example, for free and mean zero variables A
and B, we have τ(ABAB) = 0 whereas τ(A2B2) = τ((A2 −τ(A2))B2)+τ(A2)τ(B2) =
τ(A2)τ(B2).
Assuming A,B are free with τ(A) = τ(B) = 0, we can compute the moments of the
free addition A + B. The second moment is easy:
τ

(A + B)2
= τ(A2) + τ(B2) + 2τ(AB) = τ(A2) + τ(B2),
(11.42)
because both τ(A),τ(B) are zero. For the third and higher moments the trick is, as just
above, to add and subtract quantities such that, in each term, at least one object of the form
(C −τ(C)) is present:
τ

(A + B)3
= τ(A3) + τ(B3) + 3τ(A2B) + 3τ(AB2)
= τ(A3) + τ(B3) + 3τ((A2 −τ(A2))B) + 3τ(A2)τ(B)
+ 3τ(A(B2 −τ(B2))) + 3τ(A)τ(B2)
= τ(A3) + τ(B3),
(11.43)
and for the fourth moment:
τ

(A + B)4
= τ(A4) + 4τ(A3B) + 4τ(A2B2) + 2τ(ABAB) + 4τ(AB3) + τ(B4)
= τ(A4) + 4τ((A3 −τ(A3))B) + 4τ(A3)τ(B)
+ 4τ

A2 −τ(A2)

B2 −τ(B2)

+ 4τ(A2)τ(B2)
+ 4τ

A

B3 −τ(B3)

+ 4τ(A)τ(B3) + τ(B4)
= τ(A4) + τ(B4) + 4τ(A2)τ(B2).
(11.44)
In particular, we ﬁnd
τ

(A + B)4
−2τ

(A + B)22 = τ(A4) + τ(B4) −2τ(A2)2 −2τ(B2)2.
(11.45)
11.3.2 Free Cumulants
Let us deﬁne the cumulants of A as
κ1(A) = τ(A),
κ2(A) = τ(A2
0),
κ3(A) = τ(A3
0),
κ4(A) = τ(A4
0) −2τ(A2
0)2,
(11.46)
where A0 is a short-hand for A −τ(A)1. Then these objects are additive for free random
variables. The ﬁrst three are the same as the commutative ones. But for the fourth cumulant,
the coefﬁcient in front of τ(A2
0)2 is now 2 instead of 3. Higher cumulants all differ from
their commutative counterparts.
As in the commutative case, we can deﬁne the kth free cumulant iteratively as
κk(A) = τ(Ak) + homogeneous products of lower order moments,
(11.47)

164
Free Probabilities
such that
κk(A + B) = κk(A) + κk(B),
∀k,
(11.48)
whenever A,B are free.
An important example of non-commutative free random variables is two independent
large random matrices where one of them is rotational invariant – see next chapter.6
11.3.3 Additivity of the R-Transform
In the previous chapter, we saw that the R-transform is additive for large rotationally
invariant matrices. We will show here that we can deﬁne the R-transform in our abstract
algebraic probability setting and that this R-transform is also additive for free variables. In
the next chapter, we will dwell on why large rotationally invariant matrices are free.
First we deﬁne the Stieltjes transform as a moment generating function as in (2.22); we
can deﬁne gA(z) for large z as
gA(z) =
∞

k=0
1
zk+1 τ(Ak).
(11.49)
Then we can also deﬁne the R-transform as before:
RA(g) := zA(g) −1
g
(11.50)
for small enough g. Here the inverse function zA(g) is deﬁned as the formal power series
that satisﬁes gA(zA(g)) = g to all orders.
We now claim that the R-transform is additive for free random variables, i.e.
RA+B(g) = RA(g) + RB(g),
(11.51)
whenever A,B are free.
We let zA(g) be the inverse function of
gA(z) = τ
&
(z −A)−1'
,
(11.52)
whose power series is actually given by (11.49). Consider a ﬁxed scalar g. By construction
τ(g1) = g = gA(zA(g)) = τ
&
(zA(g) −A)−1'
.
(11.53)
The arguments of τ(.) on the left and on the right of the above equation have the same mean
but they are in general different, so let us deﬁne their difference as gXA via
gXA := (zA −A)−1 −g1,
(11.54)
where zA := zA(g). From its very deﬁnition, we have τ(XA) = 0.
6 Freeness is only exact in the large N limit of random matrices.

11.3 Non-Commuting Variables
165
We can invert Eq. (11.54) and ﬁnd
A −zA = −1
g (1 + XA)−1.
(11.55)
Consider another variable B, free from A. For the same ﬁxed g we can ﬁnd the scalar
zB := zB(g) and deﬁne XB with τ(XB) = 0 as for A, to ﬁnd
B −zB = −1
g (1 + XB)−1.
(11.56)
Since XA and XB are functions of A and B, XA and XB are also free. Now,
A + B −zA −zB = −1
g (1 + XA)−1 −1
g (1 + XB)−1
= −1
g (1 + XA)−1(2 + XA + XB)(1 + XB)−1.
(11.57)
Hence, noting that (1 + XA)(1 + XB) + 1 −XAXB = 2 + XA + XB,
A + B −zA −zB + 1
g = −1
g (1 + XA)−1(1 −XAXB)(1 + XB)−1,
(
A + B −

zA + zB −1
g
)−1
= −g(1 + XB)(1 −XAXB)−1(1 + XA).
(11.58)
Using the identity
(1 −XAXB)−1 =
∞

n=0
(XAXB)n,
(11.59)
we can expand the expression
τ
&
(1 + XB)(1 −XAXB)−1(1 + XA)
'
,
(11.60)
which will contain 1 plus terms of the form τ(XAXBXAXB . . . XB) where the initial and
ﬁnal factor might be either XA or XB but the important point is that XA and XB always
alternate. By the freeness and zero mean of XA and XB, all these terms are thus zero. Hence
we get
τ
0(
A + B −

zA + zB −1
g
)−13
= −g ⇒gA+B(zA + zB −g−1) = g,
(11.61)
ﬁnally leading to the announced result:7
zA+B = zA + zB −g−1 ⇒RA+B = RA + RB.
(11.62)
7 The above compact proof is taken from Tao [2012].

166
Free Probabilities
11.3.4 R-Transform and Cumulants
The R-transform is deﬁned as a power series in g. We claim that the coefﬁcients of this
power series are in fact exactly the non-commutative cumulants deﬁned earlier. In other
words, RA(g) is the cumulant generating function:
RA(g) :=
∞

k=1
κk(A)gk−1.
(11.63)
To show that these coefﬁcients are indeed the cumulants we ﬁrst realize that the general
equality R(g) = z(g) −1/g is equivalent to
zgA(z) −1 = gA(z)RA(gA(z)).
(11.64)
We can compute the power series of the two sides of this equality:
zgA(z) −1 =
∞

k=1
mk
zk ,
(11.65)
where mk := τ(Ak) denotes the kth moment, and
gA(z)RA(gA(z)) =
∞

k=1
κk

1
z +
∞

ℓ=1
ml
zℓ+1
k
.
(11.66)
Equating the right hand sides of Eqs. (11.65) and (11.66) and matching powers of 1/z we
get recursive relations between moments (mk) and cumulants (κk):
m1 = κ1
⇒
m1 = κ1,
m2 = κ2 + κ1m1
⇒
m2 = κ2 + κ2
1,
m3 = κ3 + 2κ2m1 + κ1m2
⇒
m3 = κ3 + 3κ2κ1 + κ3
1,
m4 = κ4 + 4κ3m1 + κ2[2m2 + m2
1] + κ1m3
⇒
m4 = κ4 + 6κ2κ2
1 + 2κ2
2 + 4κ3κ1 + κ4
1.
(11.67)
By looking at the z−k term coming from the [1/z + · · · ]k term in Eq. (11.66) we realize
that mk = κk + · · · where “· · · ” are homogeneous combinations of lower order κk and mk.
Since the coefﬁcients of the power series Eq. (11.63) are additive under addition of free
variables and obey the property
κk(A) = τ(Ak) + homogeneous products of lower order moments,
(11.68)
they are therefore the cumulants deﬁned in Section 11.3.2.
11.3.5 Cumulants and Non-Crossing Partitions
We saw that Eq. (11.64) can be used to compute cumulants iteratively. Actually that
equation can be translated into a systematic relation between moments and cumulants:
mn =

π∈NC(n)
κπ1 · · · κπℓπ ,
(11.69)

11.3 Non-Commuting Variables
167
m4 =
+
+
+
+
+
+
+
+
+
+
+
+
+
Figure 11.1 List of all non-crossing partitions of four elements. In Eq. (11.69) for m4, the ﬁrst
partition contributes κ1κ1κ1κ1 = κ4
1. The next six all contribute κ2κ2
1 and so forth. We read
m4 = κ4
1 + 6κ2κ2
1 + 2κ2
2 + 4κ3κ1 + κ4.
mℓ1
mℓ2
mℓ3
mℓ4
mℓ5
κ5
Figure 11.2 A typical non-crossing diagram contributing to a large moment mn. In this example
the ﬁrst element is connected to four others (giving a factor of κ5) which breaks the diagram into
ﬁve disjoint non-crossing diagrams contributing a factor mℓ1mℓ2mℓ3mℓ4mℓ5. Note that we must
have ℓ1 + ℓ2 + ℓ3 + ℓ4 + ℓ5 = n.
where π ∈NC(n) indicates that the sum is over all possible non-crossing partitions of n
elements. For any such partition π the integers {π1,π2, . . . ,πℓπ } (1 ≤ℓπ ≤n) equal the
number elements in each group (see Fig. 11.3). They satisfy
n =
ℓπ

k=1
πk.
(11.70)
We will show that, provided we deﬁne cumulants by Eq. (11.69), we recover
Eq. (11.64). But before we do so, let us ﬁrst show this relation on a simple example.
Figure 11.1 shows the computation of the fourth moment in terms of the cumulants.
The argument is very similar to the recursion relation obtained for Catalan numbers
where we considered non-crossing pair partitions (see Section 3.2.3). Here the argument
is slightly more complicated as we have partitions of all possible sizes. We consider the
moment mn for n ≥1. We break down the sum over all non-crossing partitions of n
elements by looking at ℓ, the size of the set containing the ﬁrst element (for example in
Fig. 11.3, the ﬁrst element belongs to a set of size ℓ= 5). The size of this ﬁrst set can
be 1 ≤ℓ≤n. This initial set breaks the partition into ℓ(possibly empty) disjoint smaller
partitions. They must be disjoint, otherwise there would be a crossing. In Figure 11.2 we
show how an initial 5-set breaks the full partition into 5 blocks. In each of these blocks,
every non-crossing partition is possible, the only constraint is that the total size of the
partition must be n. The sum over all possible non-crossing partitions of size k of the
relevant κ’s is the moment mk. Note that the empty partition contributes a multiplicative
factor 1, so we deﬁne m0 ≡1. Putting everything together we obtain the following
recursion relation for mn:
mn =
n

ℓ=1
κℓ
,
k1,k2,...,kℓ≥0
k1+k2+···+kℓ=n−ℓ
mk1mk2 . . . mkℓ.
(11.71)
Let us multiply both sides of this equation by z−n and sum over n from 1 to ∞. The left
hand side gives zg(z) −1, by deﬁnition of g(z). The right hand side reads

168
Free Probabilities
Figure 11.3 Generic non-crossing partition of 23 elements with two singletons, ﬁve doublets, two
triplets, and one quintent, such that 23 = 5+2·3+5·2+2·1. In Eq. (11.69), this particular partition
appears for m23 and contributes κ5κ2
3κ5
2κ2
1.
τ(A1A2A3) =
+
+
+
+
Figure 11.4 List of all non-crossing partitions of three elements. From this we get Eq. (11.74) for
three elements: τ(A1A2A3) = κ1(A1)κ1(A2)κ1(A3) + κ2(A1,A2)κ1(A3) + κ2(A1,A3)κ1(A2) +
κ2(A2,A3)κ1(A3) + κ3(A1,A2,A3).
∞

n=1
n

ℓ=1
κℓ
,
k1,k2,...,kℓ≥0
δk1+k2+···+kℓ=n−ℓ
mk1mk2 . . . mkℓ
z1+k1z1+k2 . . . z1+kℓ,
(11.72)
which can be transformed into
∞

ℓ=1
κℓ
⎡
⎣
∞

k1=0
mk1
z1+k1
⎤
⎦
ℓ
= g(z)R(g(z)),
(11.73)
where we have used Eq. (11.63). We thus recover exactly Eq. (11.64), showing that the
relation (11.69) is equivalent to our previous deﬁnition of the free cumulants.
It is interesting to contrast the moment–cumulant relation in the standard (commuta-
tive) case (Eq. (11.23)) and the free (non-commutative) case (Eq. (11.69)). Both can be
written as a sum over all partitions on n elements; in the standard case all partitions are
allowed, while in the free case the sum is only over non-crossing partitions.
11.3.6 Freeness as the Vanishing of Mixed Cumulants
We have deﬁned freeness in Section 11.3.1 as the property of two variables A and
B such that the trace of any mixed combination of traceless polynomials in A and in B
vanishes. There exists another equivalent deﬁnition of freeness, namely that every mixed
cumulant of A and B vanish. To make sense of this deﬁnition we ﬁrst need to introduce
cumulants of several variables. They are deﬁned recursively by
τ(A1A2 . . . An) =

π∈NC(n)
κπ(A1A2 . . . An),
(11.74)
where the Ai’s are not necessarily distinct and NC(n) is the set of all non-crossing parti-
tions of n elements. Here
κπ(A1A2 . . . An) = κπ1(. . .). . . κπℓ(. . .)
(11.75)
are the products of cumulants of variables belonging to the same group of the correspond-
ing partition – see Figure 11.4 for an illustration. We also call these generalized cumulants
the free cumulants.
When all the variables in Eq. (11.74) are the same (Ai = A) we recover the previous
deﬁnition of cumulants with a slightly different notation (e.g. κ3(A,A,A) ≡κ3(A)).

11.3 Non-Commuting Variables
169
Cumulants with more than one variable are called mixed cumulants (e.g. κ4(A,A,B,A)).
By applying Eq. (11.74) we ﬁnd for the low generalized cumulants of two variables
m1(A) = κ1(A),
m2(A,B) = κ1(B)κ1(B) + κ2(A,B),
m3(A,A,B) = κ1(A)2κ1(B) + κ2(A,A)κ1(B) + 2κ2(A,B)κ1(A) + κ3(A,A,B).
(11.76)
We can now state more precisely the alternative deﬁnition of freeness: a set of variables
is free if and only if all their mixed cumulants vanish. For example, in the low cumulants
listed above, freeness of A and B implies that κ2(A,B) = κ3(A,A,B) = 0.
This deﬁnition of freeness is easy to generalize to a collection of variables, i.e. a
collection of variables is free if all its mixed cumulants are zero. As noted at the end
of Section 11.3.7 below, pairwise freeness is not enough to ensure that a collection is free.
We remark that vanishing of mixed cumulants implies that free cumulants are additive.
In Speicher’s notation, κk(A,B,C, . . .) is a multi-linear function in each of its arguments,
where k gives the number of variables. Thus we have
κk(A + B,A + B, . . .) = κk(A,A, . . .) + κk(B,B, . . .) + mixed cumulants
= κk(A,A, . . .) + κk(B,B, . . .),
(11.77)
i.e. κk is additive.
We will give a concrete application of the formalism of free mixed cumulants in
Section 12.2.
11.3.7 The Central Limit Theorem for Free Variables
We can now go back and re-read Section 11.2.4. We can replace every occurrence of the
word independent with free, and cumulant with free cumulant. The lln now states that the
sum of K free identically distributed (ﬁd) variables normalized by K−1 converges to a
constant (also called a scalar) with the same mean.
Let us deﬁne a free Wigner variable as a variable with second free cumulant κ2 > 0 and
all other free cumulants equal to zero. In other words, a free Wigner variable is such that
RW(x) = κ2x. The clt then states that the sum of K zero-mean free identical variables
normalized by K−1/2 converges to a free Wigner variable with the same second cumulant.
In the case where our free random variables are large symmetric random matrices,
the Wigner deﬁned here by its cumulant coincides with the Wigner matrices deﬁned in
Chapter 2. We indeed saw that the R-transform of a Wigner matrix is given by R(x) = σ 2x,
i.e. the cumulant generating function has a single term corresponding to κ2 = σ 2.
Alternatively, we note that the moments of a Wigner are given by the sum over non-
crossing pair partitions (Eq. (3.26)). Comparing with Eq. (11.69), we realize that partitions
containing anything other than pairs must contribute zero, hence only the second cumulant
of the Wigner is non-zero.
The lln and the clt require variables to be collectively free, in the sense that all mixed
cumulants are zero. As is the case with independence, pairwise freeness is not enough to
ensure freeness as a collection (see footnote on page 161). Indeed, in Section 12.5 we will
encounter variables that are pairwise free but not free as a collection. One can have A
and B mutually free and both free with respect to C but A + B is not free with respect

170
Free Probabilities
to C. This does not happen for rotationally invariant large matrices but can arise in other
constructions. The deﬁnition of a free collection is just an extension of deﬁnition (11.40)
including traceless polynomials in all variables in the collection. With this deﬁnition, sums
of variables in the collection are free from those not included in the sum (e.g. A + B is
free from C).
11.3.8 Subordination Relation for Addition of Free Variables
We now introduce the subordination relation for free addition, which is just a rewriting of
the addition of R-transforms. For free A and B, we have
RA(g) + RB(g) = RA+B(g) ⇒zA(g) + RB(g) = zA+B(g),
(11.78)
where
gA+B(zA+B) = g = gA(zA) = gA (zA+B −RB(g)) .
(11.79)
We call z := zA+B(g), then the above relations give
gA+B(z) = gA (z −RB(gA+B(z))),
(11.80)
which is called a subordination relation (compare with Eq. (10.2)).
11.4 Free Product
In the previous section, we have studied the property of the sum of free random variables. In
the case of commuting variables, the question of studying the product of (positive) random
variables is trivial, since taking the logarithm of this product we are back to the problem of
sums again. In the case of non-commuting variables, things are more interesting. We will
see below that one needs to introduce the so-called S-transform, which is the counterpart
of the R-transform for products of free variables.
We start by noticing that the free product of traceless variables is trivial. If A,B are free
and τ(A) = τ(B) = 0, we have
τ((AB)k) = τ(ABAB . . . AB) = 0.
(11.81)
11.4.1 Low Moments of Free Products
We will now compute the ﬁrst few moments of the free products of two variables with a
non-zero trace: C := AB where A,B are free and τ(A)  0, τ(B)  0. Without loss of
generality, we can assume that τ(A) = τ(B) = 1 by rescaling A and B. Then
τ(C) = τ ((A −τ(A))(B −τ(B))) + τ(A)τ(B) = τ(A)τ(B) = 1.
(11.82)
We can also use (11.74) to get
τ(C) = κ2(A,B) + κ1(A)κ1(B) = κ1(A)κ1(B) = 1,
(11.83)

11.4 Free Product
171
m4 =
+
+
+
+
+
+
+
+
+
+
+
Figure 11.5 List of all non-crossing partitions of six elements contributing to τ(ABABAB)
excluding mixed AB terms. Terms involving A are in thick gray and B in black. Equation (11.86)
can be read off from these diagrams. Note that κ1(A) = κ2(B) = 1.
since mixed cumulants are zero for mutually free variables. Similarly, using Eq. (11.74) we
can get that (see Fig. 11.1 for the non-crossing partitions of four elements)
τ(C2) = τ(ABAB) = κ1(A)2κ1(B)2 + κ2(A)κ1(B)2 + κ1(A)2κ2(B)
= 1 + κ2(A) + κ2(B),
(11.84)
which gives
κ2(C) := τ(C2) −τ(C)2 = κ2(A) + κ2(B).
(11.85)
For the third moment of C = AB, we can follow Figure 11.5 and get
τ(C3) = τ(ABABAB)
= 1 + 3κ2(A) + 3κ2(B) + 3κ2(A)κ2(B) + κ3(A) + κ3(B),
(11.86)
leading to
κ3(C) : = τ(C3) −3τ(C2)τ(C) + 2τ(C)3
= κ3(A) + κ3(B) + 3κ2(A)κ2(B).
(11.87)
Under free products of unit-trace variables, the mean remains equal to one and the
variance is additive. The third cumulant is not additive; it is strictly greater than the sum of
the third cumulants unless one of the two variables is the identity (unit scalar).
11.4.2 Deﬁnition of the S-Transform
We will now show that the above relations can be encoded into the S-transform S(t) which
is multiplicative for products of free variables:
SAB(t) = SA(t)SB(t)
(11.88)
for A and B free. To deﬁne the S-transform, we ﬁrst introduce the T-transform as
tA(ζ) = τ
&
(1 −ζ −1A)−1'
−1
= ζgA(ζ) −1
=
∞

k=1
mk
ζ k .
(11.89)

172
Free Probabilities
The behavior at inﬁnity of the T-transform depends explicitly on m1, the ﬁrst moment of A
(t(ζ) ∼m1/ζ), unlike the Stieltjes transform which always behaves as 1/z.
The T-transform has the same singularities as the Stieltjes transform except maybe at
zero. When A is a matrix, one can recover the continuous part of its eigenvalue density
ρ(x) using the following T-version of the Sokhotski–Plemelj formula:
lim
η→0+ Im t(x −iη) = πxρ(x).
(11.90)
Poles in the T-transform indicate Dirac masses. If t(ζ) ∼A/(ζ −λ0) near λ0 then the
density is a Dirac mass of amplitude A/λ0 at λ0. The behavior at zero of the T-transform is
a bit different from that of the Stieltjes transform. A regular density at zero gives a regular
Stieltjes and hence t(0) = −1. Deviations from this value indicate a Dirac mass at zero,
hence when t(0)  −1, the density has a Dirac at zero of amplitude t(0) + 1.
The T-transform can also be written as
tA(ζ) = τ
&
A (ζ −A)−1'
.
(11.91)
We deﬁne ζA(t) to be the inverse function of tA(ζ). When m1  0, tA is invertible for large
ζ, and hence ζA exists for small enough t. We then deﬁne the S-transform as8
SA(t) := t + 1
tζA(t),
(11.92)
for variables A such that τ(A)  0.
Let us compute the S-transform of the identity S1(t):
t1(ζ) =
1
ζ −1 ⇒ζ1(t) = t + 1
t
⇒S1(t) = 1,
(11.93)
as expected as the identity is free with respect to any variable. The S-transform scales in a
simple way with the variable A. To ﬁnd its scaling we ﬁrst note that
tαA(ζ) = τ
&
(1 −(α−1ζ)−1A)−1'
−1 = tA(ζ/α),
(11.94)
which gives
ζαA(t) = αζA(t).
(11.95)
Then, using (11.92), we get that
SαA(t) = α−1SA(t).
(11.96)
The above scaling is slightly counterintuitive but it is consistent with the fact that
SA(0) = 1/τ(A). We will be focusing on unit trace objects such that S(0) = 1.
8 Most authors prefer to deﬁne the S-transform in terms of the moment generating function ψ(z) := t(1/z). The deﬁnition
S(t) = ψ−1(t)(t + 1)/t is equivalent to ours (ψ−1(t) is the functional inverse of ψ(z)). We prefer to work with the
T-transform as the function t(ζ) has an analytic structure very similar to that of g(z). The function ψ(z) is analytic near zero
and singular for large values of z corresponding to the reciprocal of the eigenvalues.

11.4 Free Product
173
The construction of the S-transform relies on the properties of mixed moments of free
variables. In that respect it is closely related to the R-transform. Using the relation tA(ζ) =
ζgA(ζ) −1, one can get the following relationships between RA and SA:
SA(t) =
1
RA(tSA(t)),
RA(g) =
1
SA(gRA(g)).
(11.97)
11.4.3 Multiplicativity of the S-Transform
We can now show the multiplicative property (11.88). The proof is similar to the one
given for the additive case and is adapted from it.
We ﬁx t and let ζA and ζB be the inverse T-transforms of tA and tB. Then we deﬁne
EA through
1 + t + EA = (1 −A/ζA)−1,
(11.98)
and similarly for EB. We have τ(EA) = 0, τ(EB) = 0, and, since A and B are free,
EA,EB are also free. Then we have
A
ζA
= 1 −(1 + t + EA)−1,
(11.99)
which gives
AB
ζAζB
=
&
1 −(1 + t + EA)−1' &
1 −(1 + t + EB)−1'
= (1 + t + EA)−1 [(t + EA)(t + EB)] (1 + t + EB)−1.
(11.100)
Using the identity
t(EA + EB) =
t
1 + t
&
(1 + t + EA)(1 + t + EB) −(1 + t)2 −EAEB
'
,
(11.101)
we can rewrite the above expression as
AB
ζAζB
=
t
1 + t + (1 + t + EA)−1
(
−t + EAEB
1 + t
)
(1 + t + EB)−1
⇒1 −1 + t
t
AB
ζAζB
= (1 + t)(1 + t + EA)−1
(
1 −EAEB
t(1 + t)
)
(1 + t + EB)−1
⇒
(
1 −1 + t
t
AB
ζAζB
)−1
=
1
1 + t (1 + t + EB)
(
1 −EAEB
t(1 + t)
)−1
(1 + t + EA).
(11.102)
Using the expansion
(
1 −EAEB
t(1 + t)
)−1
=

n=0
 EAEB
t(1 + t)
n
,
(11.103)
one can check that
τ
-
(1 + t + EB)
(
1 −EAEB
t(1 + t)
)−1
(1 + t + EA)
.
= (1 + t)2,
(11.104)

174
Free Probabilities
where we used the freeness condition for EA and EB. Thus we get that
τ
0(
1 −1 + t
t
AB
ζAζB
)−13
= 1 + t
⇒
tAB
tζAζB
1 + t

= t,
(11.105)
which gives that
SAB(t) = SA(t)SB(t)
(11.106)
thanks to the deﬁnition (11.92).
11.4.4 Subordination Relation for the Free Product
We next derive a subordination relation for the free product using (11.88) and (11.92):
SAB(t) = SA(t)SB(t)
⇒
ζAB(t) = ζA(t)
SB(t),
(11.107)
where
tAB(ζAB(t)) = t = tA(ζA(t)) = tA(ζAB(t)SB(t)).
(11.108)
We call ζ := ζAB(t), then the above relations give
tAB(ζ) = tA (ζSB(tAB(ζ))),
(11.109)
which is the subordination relation for the free product. In fact, the above is true even when
SA does not exist, e.g. when τ(A) = 0.
When applied to free random matrices, the form AB is not very useful since it is not
necessarily symmetric even if A and B are. But if A ⪰0 (i.e. A is positive semi-deﬁnite
symmetric) and B is symmetric, then A
1
2 BA
1
2 has the same moments as AB and is also
symmetric. In our applications below we will always encounter the case A ⪰0 and call
A
1
2 BA
1
2 the free product of A and B.
Exercise 11.4.1
Properties of the S-transform
(a)
Using Eq. (11.92), show that
R(x) =
1
S(xR(x)).
(11.110)
Hint: deﬁne t = xR(x) = zg −1 and identify x as g.
(b)
For a variable such that τ(M) = κ1 = 1, write S(t) as a power series in t,
compute the ﬁrst few terms of the powers series, up to (and including) the t2
term, using Eq. (11.110) and Eq. (11.63). You should ﬁnd
S(t) = 1 −κ2t + (2κ2
2 −κ3)t2 + O(t3).
(11.111)

11.4 Free Product
175
(c)
We have shown that, when A and B are mutually free with unit trace,
τ(AB) = 1,
(11.112)
τ(ABAB) −1 = κ2(A) + κ2(B),
(11.113)
τ(ABABAB) = κ3(A) + κ3(B) + 3κ2(A)κ2(B) + 3(κ2(A) + κ2(B)) + 1.
(11.114)
Show that these relations are compatible with SAB(t) = SA(t)SB(t) and the
ﬁrst few terms of your power series in (b).
(d)
Consider M1 = 1 + σ1W1 and M2 = 1 + σ2W2 where W1 and W2 are
two different (free) unit Wigner matrices and both σ’s are less than 1/2. M1
and M2 have κ3 = 0 and are positive deﬁnite in the large N limit. What is
κ3(M1M2)?
Exercise 11.4.2
S-transform of the matrix inverse
(a)
Consider M an invertible symmetric random matrix and M−1 its inverse.
Using Eq. (11.89), show that
tM(ζ) + tM−1
1
ζ

+ 1 = 0.
(11.115)
(b)
Using Eq. (11.115), show that
SM−1(x) =
1
SM(−x −1).
(11.116)
Hint: write u(x) = 1/ζ(t) where u(x) is such that x = tM−1(u(x)). Equation
(11.115) is then equivalent to x = −1 −t.
Bibliographical Notes
• The concept of freeness was introduced in
– D. Voiculescu. Symmetries of some reduced free product C*-algebras. In H. Araki
et al. (eds.), Operator Algebras and Their Connections with Topology and Ergodic
Theory. Lecture Notes in Mathematics, volume 1132. Springer, Berlin, Heidelberg,
1985,
– D. Voiculescu. Limit laws for random matrices and free products. Inventiones math-
ematicae, 104(1):201–220, 1991,
with the second reference making the link to rmt.
• For general introductions to freeness, see
– J. A. Mingo and R. Speicher. Free Probability and Random Matrices. Springer, New
York, 2017,

176
Free Probabilities
– J. Novak. Three lectures on free probability. In Random Matrix Theory, Interacting
Particle Systems, and Integrable Systems. Cambridge University Press, Cambridge,
2014.
• For a more operational approach to freeness, see
– A. M. Tulino and S. Verd´u. Random Matrix Theory and Wireless Communications.
Now publishers, Hanover, Mass., 2004,.
• The proof of the additivity of the R-transform presented here is from
– T. Tao. Topics in Random Matrix Theory. American Mathematical Society, Provi-
dence, Rhode Island, 2012,
see also
– P. Zinn-Justin. Adding and multiplying random matrices: A generalization of
Voiculescu’s formulas. Physical Review E, 59:4884–4888, 1999,
for an alternative point of view.
• Central limit theorems and L´evy-stable free variables:
– H. Bercovici, V. Pata, and P. Biane. Stable laws and domains of attraction in free
probability theory. Annals of Mathematics, 149(3):1023–1060, 1999,
– Z. Burda, J. Jurkiewicz, M. A. Nowak, G. Papp, and I. Zahed. Free random L´evy and
Wigner-L´evy matrices. Physical Review E, 75:051126, 2007.

12
Free Random Matrices
In the last chapter, we introduced the concept of freeness rather abstractly, as the proper
non-commutative generalization of independence for usual random variables. In the present
chapter, we explain why large, randomly rotated matrices behave as free random variables.
This justiﬁes the use of R-transforms and S-transforms to deal with the spectrum of sums
and products of large random matrices. We also revisit the abstract central limit theorem
of the previous chapter (Section 11.2.4) in the more concrete case of sums of randomly
rotated matrices.
12.1 Random Rotations and Freeness
12.1.1 Statement of the Main Result
Recall the deﬁnition of freeness. A and B are free if for any set of traceless polynomials
p1, . . . ,pn and q1, . . . ,qn the following equality holds:
τ (p1(A)q1(B)p2(A)q2(B). . . pn(A)qn(B)) = 0.
(12.1)
In order to make the link with large matrices we will consider A and B to be large symmetric
matrices and τ(M) := 1/N Tr(M). The matrix A can be diagonalized as UUT and B as
V′VT . A traceless polynomial pi(A) can be diagonalized as UiUT , where U is the same
orthogonal matrix as for A itself and i = pi() is some traceless diagonal matrix, and
similarly for qi(B). Equation (12.1) then becomes
τ

1O′
1OT 2O′
2OT . . . nO′
nOT 
= 0,
(12.2)
where we have introduced O = UT V as the orthogonal matrix of basis change rotating the
eigenvectors of A into those of B.
As we argue below, in the large N limit Eq. (12.2) always holds true when averaged over
the orthogonal matrix O and whenever matrices i and ′
i are traceless. We also expect
that in the large N limit Eq. (12.2) becomes self-averaging, so a single matrix O behaves as
the average over all such matrices. Hence, two large symmetric matrices whose eigenbases
are randomly rotated with respect to one another are essentially free. For example, Wigner
matrices X and white Wishart matrices W are rotationally invariant, meaning that the
177

178
Free Random Matrices
matrices of their eigenvectors are random orthogonal matrices. We conclude that for N
large, both X and W are free with respect to any matrix independent from them, in particular
they are free from any deterministic matrix.
12.1.2 Integration over the Orthogonal Group
We now come back to the central statement that in the large N limit the average over O
of Eq. (12.2) is zero for traceless matrices i and ′
i. In order to compute quantities like
8
τ

1O′
1OT 2O′
2OT . . . nO′
nOT 9
O ,
(12.3)
one needs to understand how to compute the following moments of rotation matrices,
averaged over the Haar (ﬂat) measure over the orthogonal group O(N):
I(i,j,n) :=
8
Oi1j1Oi2j2 . . . Oi2nj2n
9
O .
(12.4)
The general formula has been worked out quite recently and involves the Weingarten
functions. A full discussion of these functions is beyond the scope of this book, but
we want to give here a brief account of the structure of the result. When N →∞, the
leading term is quite simple: one recovers the Wick’s contraction rules, as if Oi1j1 were
independent random Gaussian variables with variance 1/N. Namely,
I(i,j,n) = N−n

pairings π
δiπ(1)iπ(2)δjπ(1)jπ(2) . . . δiπ(2n−1)iπ(2n)δjπ(2n−1)jπ(2n) + O(N−n−1).
(12.5)
Note that all pairings of the i-indices are the same as those of the j-indices. For example,
for n = 1 and n = 2 one has explicitly, for N →∞,
N
8
Oi1j1Oi2j2
9
O = δi1i2δj1j2
(12.6)
and
N2 8
Oi1j1Oi2j2Oi3j3Oi4j4
9
O = δi1i2δj1j2δi3i4δj3j4
+ δi1i3δj1j3δi2i4δj2j4
+ δi1i4δj1j4δi2i3δj2j3.
(12.7)
The case n = 1 is exact and has no subleading corrections in N, so we can use it to
compute
8
τ(1O′
1OT )
9
O = N−1
N

i,j=1
(1)i
:
Oij(′
1)jOT
ji
;
O
= N−2
N

i,j=1
(1)i(′
1)j
= τ(1)τ(′
1).
(12.8)
(Recall that τ(A) is equal to N−1 Tr A.) Clearly the result is zero when τ(1) =
τ(′
1) = 0, as required by the freeness condition.

12.1 Random Rotations and Freeness
179
Figure 12.1 Number of loops ℓ(π,π) for π = (1,2)(3,6)(4,5). The bottom part of the diagram (thick
gray) corresponds to the ﬁrst partition π and the top part (black) to the second partition, here also
equal to π. In this example there are three loops each of size 2.
Now, using only the leading Wick terms for n = 2 and after some index contractions
and manipulations, one would obtain
lim
N→∞
8
τ(1O′
1OT 2O′
2OT )
9
O = τ(12)τ(′
1)τ(′
2) + τ(1)τ(2)τ(′
1′
2).
(12.9)
However, this cannot be correct. Take for example 1 = 2 = 1, for which τ(1O′
1OT
2O′
2OT )
=
τ(′
1′
2) exactly, whereas the formula above adds an extra term
τ(′
1)τ(′
2). The correct formula actually reads
lim
N→∞
8
τ(1O′
1OT 2O′
2OT )
9
O = τ(12)τ(′
1)τ(′
2) + τ(1)τ(2)τ(′
1′
2)
−τ(1)τ(2)τ(′
1)τ(′
2),
(12.10)
which again is zero whenever all individual traces are zero (i.e. the freeness condition).
12.1.3 Beyond Wick Contractions: Weingarten Functions
Where does the last term in Eq. (12.10) come from? The solution to this puzzle lies in
the fact that some subleading corrections to Eq. (12.7) also contribute to the trace we are
computing: summing over indices from 1 to N can prop up some subdominant terms and
make them contribute to the ﬁnal result. Hence we need to know a little more about the
Weingarten functions. This will allow us to conclude that the freeness condition holds for
arbitrary n. The general Weingarten formula reads
I(i,j,n) =

pairings π,σ
Wn(π,σ)δiπ(1)iπ(2)δjσ(1)jσ(2) . . . δiπ(2n−1)iπ(2n)δjσ(2n−1)jσ(2n),
(12.11)
where now the pairings π of i’s and σ of j’s do not need to coincide. The Weingarten
functions Wn(π,σ) can be thought of as matrices with pairings as indices. They are given
by the pseudo-inverse1 of the matrices Mn(π,σ) := Nℓ(π,σ), where ℓ(π,σ) is the number
of loops obtained when superposing π and σ. For example, when π = σ one ﬁnds n
loops, each of length 2, see Figure 12.1. While when π  σ the number of loops is always
less than n (ℓ(π,σ) < n), see Figure 12.2. At large N, the diagonal of the matrix Mn
dominates and the matrix is always invertible. By expanding in powers of 1/N, we see
that its inverse Wn, whose elements are the Weingarten functions, has an N−n behavior
1 The pseudo-inverse of M is such that WMW = W and MWM = M. When M is invertible, W = M−1. If M is
diagonalizable, the eigenvalues of W are the reciprocal of those of M with the rule 1/0 →0.

180
Free Random Matrices
Figure 12.2 Number of loops ℓ(π,σ) for π = (1,4)(2,3) and σ = (1,2)(3,4). In this example there
is only one loop.
on the diagonal and off-diagonal terms are at most N−n−1. More generally, one has the
following beautiful expansion:
Wn(π,σ) = Nℓ(π,σ)−2n
∞

g=0
 g(π,σ)N−g,
(12.12)
where the coefﬁcients  g(π,σ) depend on properties of certain “geodesic paths” in the
space of partitions. The bottom line of this general expansion formula is that non-Wick
contractions are of higher order in N−1.
As an illustration, let us come back to the missing term in Eq. (12.9) when one
restricts to Wick contractions, for which the number of loops ℓ(π,π) = 2. The next term
corresponds to ℓ(π,σ) = 1 for which W2(π,σ) ∼−N−3 (see Exercise 12.1.1). Consider
the pairings i1 = i4, i2 = i3 and j1 = j2, j3 = j4, for which ℓ(π,σ) = 1 (see Fig. 12.2).
Such pairings do not add any constraint on the 2n = 4 free indices that appear in
Eq. (12.3); summing over them thus yields τ(1)τ(2)τ(′
1)τ(′
2), with a −1 coming
from the Weingarten function W2(π,σ). Hence we recover the last term of Eq. (12.10).
Exercise 12.1.1
Exact Weingarten functions at n = 2
There are three possible pair partitions of four elements. They are shown
on Figure 3.2. If we number these π1,π2 and π3, M2 is a 3 × 3 matrix whose
elements are equal to Nℓ(πi,πj ).
(a)
By trying a few combinations of the three pairings, convince yourself that
for n = 2, ℓ(πi,πj) = 2 if i = j and 1 otherwise.
(b)
Build the matrix M2. For N > 1 it is invertible, ﬁnd its inverse W2. Hint:
use an ansatz for W2 with a variable a on the diagonal and b off-diagonal.
For N = 1 ﬁnd the pseudo-inverse of M2.
(c)
Finally show that (when N > 1) of the nine pairs of pairings, the three
Wick contractions (diagonal elements) have
WWick
2
=
N + 1
N3 + N2 −2N
N→∞
→
N−2,
(12.13)
and the six non-Wick parings (off-diagonal) have
Wnon-Wick
2
= −
1
N3 + N2 −2N
N→∞
→
−N−3.
(12.14)
For N = 1, all nine Weingarten functions are equal to 1/9.
(d)
The expression ⟨τ(OOT OOT )⟩is always equal to 1. Write it as a sum over
four indices and expand the expectation value over orthogonal matrices as
nine terms each containing two Dirac deltas multiplied by a Weingarten
function. Each sum of delta terms gives a power of N; ﬁnd these for all
nine terms and using your result from (c), show that indeed the Weingarten
functions give ⟨τ(OOT OOT )⟩= 1 for all N.

12.2 R-Transforms and Resummed Perturbation Theory
181
12.1.4 Freeness of Large Matrices
We are now ready to show that all expectations of the form (12.3) are zero. Let us look
at them more closely:
8
τ

1O′
1OT 2O′
2OT . . . nO′
nOT 9
O
= 1
N

ij
I(i,j,n)[1]i2ni1[2]i2i3 . . . [n]i2n−2i2n−1[′
1]j1j2[′
2]j3j4 . . . [′
n]j2n−1j2n.
(12.15)
The object I(i,j,n) contains all possible pairings of the i indices and all those of the j
indices with a Weingarten function as its prefactor. The i and j indices never mix. We
concentrate on i pairings. Each pairing will give rise to a product of normalized traces of
i matrices. For example, the term
τ(5)τ(412)τ(36)
(12.16)
would appear for n = 6. Each normalized trace introduces a factor of N when going from
Tr(.) to τ(.). Since by hypothesis τ(i) = 0, the maximum number of non-zero traces is
⌊n/2⌋, e.g.
τ(13)τ(56)τ(24).
(12.17)
The maximum factor of N that can be generated is therefore N⌊n/2⌋. Applying the same
reasoning to the j pairing, and using the fact that the Weingarten function is at most
O(N−n), we ﬁnd
8
τ

1O′
1OT 2O′
2OT . . . nO′
nOT 9
O
 ≤O

N−1+2⌊n/2⌋−n N→∞
→
0.
(12.18)
Using the same arguments one can shown that large unitary invariant complex Her-
mitian random matrices are free. In this case one should consider an integral of uni-
tary matrices in Eq. (12.4). The result is also given by Eq. (12.11) where the functions
Wn(π,σ) are now unitary Weingarten functions. They are different than the orthogonal
Weingarten functions presented above but they share an important property in the large N
limit, namely
Wn(π,σ) =
0
O(N−n) if π = σ,
O(N−n−k) if π  σ for some k ≥1,
(12.19)
which was the property needed in our proof of freeness of large rotationally invariant
symmetric matrices.
12.2 R-Transforms and Resummed Perturbation Theory
In this section, we want to explore yet another route to obtain the additivity of R-
transforms, which makes use of perturbation theory and of the mixed cumulant calculus
introduced in the last chapter, Section 11.3.6, exploited in a concrete case.
We want to study the average Stieltjes transform of A + BR where BR is a randomly
rotated matrix B: BR := OBOT . We thus write
g(z) :=
:
τ

(z1 −A −BR)−1;
O := τR

(z1 −A −BR)−1
,
(12.20)

182
Free Random Matrices
where τR is meant for both the normalized trace τ and the average over the Haar measure
of the rotation group. We now formally expand the resolvent in powers of BR. Introducing
GA = (z1 −A)−1, one has
g(z) = τR(GA) + τR(GABRGA) + τR(GABRGABRGA) + · · · .
(12.21)
Now, since in the large N limit GA and BR are free, we can use the general tracial formula,
Eq. (11.74), noting that all mixed cumulants (containing both GA and BR) are identically
zero.
In order to proceed, one needs to introduce three types of mixed moments where BR
appears exactly n times:
m(1)
n
:= τR(GABRGA . . . GABRGA),
m(2)
n
:= τR(BRGA . . . GABR)
(12.22)
and
m(3)
n
:= τR(BRGA . . . BRGA) = τR(GABR . . . GABR).
(12.23)
Note that m(1)
0
≡gA(z) and m(2)
0
= m(3)
0
= 0. We also introduce, for full generality, the
corresponding generating functions:
˜M(a)(u) =
∞

n=0
m(a)
n un,
a = 1,2,3.
(12.24)
Note however that we will only be interested here in g(z) :=
˜M(1)(u = 1) (cf. Eq.
(12.21)).
Let us compute m(1)
n
using the same method as in Section 11.3.5, i.e. expanding in the
size ℓof the group to which the ﬁrst GA belongs (see Eq. (11.71)):
m(1)
n
=
n+1

ℓ=1
κGA,ℓ
,
k1,k2,...,kℓ≥0
k1+k2+···+kℓ=n−ℓ
m(2)
k1 m(2)
k2 . . . m(2)
kℓ−1m(3)
kℓ,
(12.25)
where n ≥1 and κGA,ℓare the free cumulants of GA. Similarly,
m(2)
n
=
n

ℓ=1
κB,ℓ
,
k1,k2,...,kℓ≥0
k1+k2+···+kℓ=n−ℓ
m(1)
k1 m(1)
k2 . . . m(1)
kℓ−1m(3)
kℓ
(12.26)
and
m(3)
n
=
n+1

ℓ=1
κB,ℓ
,
k1,k2,...,kℓ≥0
k1+k2+···+kℓ=n−ℓ
m(1)
k1 m(1)
k2 . . . m(1)
kℓ,
(12.27)
where κB,ℓare the free cumulants of B. Multiplying both sides of these equations by un
and summing over n leads to, respectively,
˜M(1)(u) = gA(z) +
∞

ℓ=1
κGA,ℓuℓ[ ˜M(2)(u)]ℓ−1 ˜M(3)(u),
(12.28)

12.3 The Central Limit Theorem for Matrices
183
and
˜M(2)(u) =
∞

ℓ=1
κB,ℓuℓ[ ˜M(1)(u)]ℓ−1 ˜M(3)(u),
˜M(3)(u) =
∞

ℓ=1
κB,ℓuℓ[ ˜M(1)(u)]ℓ.
(12.29)
Recalling the deﬁnition of R-transforms as a power series of cumulants, we thus get
˜M(1)(u) = gA(z) + u ˜M(3)(u)RGA

u ˜M(2)(u)

(12.30)
and
˜M(2)(u) = u ˜M(3)(u)RB

u ˜M(1)(u)

,
˜M(3)(u) = u ˜M(1)(u)RB

u ˜M(1)(u)

.
(12.31)
Eliminating ˜M(2)(u) and ˜M(3)(u) and setting u = 1 then yields the following relation:
g(z) = gA(z) + g(z) RB(g(z)) RGA

g(z)R2
B(g(z))

.
(12.32)
In order to rewrite this result in more familiar terms, let us consider the case where B = b1,
in which case RB(z) ≡b and, since A+B = A+b1, g(z) ≡gA(z−b). Hence, for arbitrary
b, RGA must obey the relation
RGA(b2g(z)) = g(z) −g(z + b)
bg(z)
.
(12.33)
Now, if for a ﬁxed z we set b = RB(g(z)), we ﬁnd that Eq. (12.32) is obeyed provided
g(z) = gA(z −RB(g(z))), i.e. precisely the subordination relation Eq. (11.80).
12.3 The Central Limit Theorem for Matrices
In the last chapter, we brieﬂy discussed the extension of the clt for non-commuting vari-
ables. We now restate the result in the context of random matrices, with a special focus on
the preasymptotic (cumulant) corrections to the Wigner distribution.
Let us consider the following sum of K large, randomly rotated matrices, all assumed to
be traceless:
MK :=
1
√
K
K

i=1
OiAiOT
i ,
Tr Ai = 0,
(12.34)
where Oi are independent, random rotation matrices, chosen with a ﬂat measure over
the orthogonal group O(N). We also assume, for simplicity, that all Ai have the same
(arbitrary) moments:
τ(Aℓ
i ) ≡mℓ,
∀i.
(12.35)
This means in particular that all Ai’s share the same R-transform:
RAi(z) ≡
∞

ℓ=2
κℓzℓ−1.
(12.36)

184
Free Random Matrices
Using the fact that R-transforms of randomly rotated matrices simply add, together with
RαM(z) = RM(αz), one ﬁnds
RMK(z) =
∞

ℓ=2
K1−ℓ/2κℓzℓ−1.
(12.37)
We thus see that, as K becomes large, all free cumulants except the second one tend to
zero, which implies that the limit of MK when K goes to inﬁnity is a Wigner matrix, with
a semi-circle eigenvalue spectrum.
It is interesting to study the ﬁnite K corrections to this result. First, assume that the
spectrum of Ai is not symmetric around zero, such that the skewness m3 = κ3  0. For
large K, the R-transform of MK can be approximated as
RMK(z) ≈σ 2z +
κ3
√
K
z2 + · · · .
(12.38)
In order to derive the corrections to the semi-circle induced by skewness, we posit that the
Stieltjes transform can be expanded around the Wigner result gX(z) as
gMK(z) = gX(z) +
κ3
√
K
g3(z) + · · ·
(12.39)
and assume the second term to be very small. The R-transform, RMK(z), can be similarly
expanded, yielding
RX

gX(z) +
κ3
√
K
g3(z)

+
κ3
√
K
R3(gX(z)) = z ⇒R3(gX(z)) = −g3(z)
g′
X(z)
(12.40)
or, equivalently,
g3(z) = −g′
X(z)R3(gX(z)) = −g′
X(z)g2
X(z).
(12.41)
For simplicity, we normalize the Wigner semi-circle such that σ 2 = 1, and hence
gX(z) = 1
2

z −
±
⃝√


;
 := z2 −4.
(12.42)
One then ﬁnds
g3(z) = −1
4

1 −z
±
⃝√


 
z2 −2 −z
±
⃝√


.
(12.43)
The imaginary part of this expression when z →λ + i0 gives the correction to the semi-
circle eigenvalue spectrum, and reads, for λ ∈[−2,2],
δρ(λ) =
κ3
2π
√
K
λ(λ2 −3)
√
4 −λ2 .
(12.44)
This correction is plotted in Figure 12.3. Note that it is odd in λ, as expected, and diverges
near the edges of the spectrum, around which the above perturbation approach breaks down

12.3 The Central Limit Theorem for Matrices
185
−2
−1
0
1
2
l
−0.4
−0.2
0.0
0.2
0.4
dr (l)
dr3(l)
dr4(l)
Figure 12.3 First order correction to the Wigner density of eigenvalues for non-zero skewness
(δρ3(λ)) and kurtosis (δρ4(λ)), Eqs. (12.44) and (12.47), respectively.
(because the density of eigenvalues of the Wigner matrix goes to zero). Note that the excess
skewness is computed to be
 2
−2
λ3δρ(λ)dλ =
κ3
√
K
,
(12.45)
as expected.
One can obtain the corresponding correction when Ai is symmetric around zero, in
which case the ﬁrst correction term comes from the kurtosis. For large K, the R-transform
of MK can now be approximated as
RMK(z) ≈σ 2z + κ4
K z3 + · · · .
(12.46)
Following the same path as above, one ﬁnally derives the correction to the eigenvalue
spectrum, which in this case reads, for λ ∈[−2,2],
δρ(λ) =
κ4
2πK
λ4 −4λ2 + 2
√
4 −λ2
(12.47)
(see Fig. 12.3). The correction is now even in λ and one can check that
 2
−2
δρ(λ)dλ = 0,
(12.48)
as it should be, since all the mass is carried by the semi-circle. Another check that this
result is correct is to compute the excess free kurtosis, given by
 2
−2
(λ4 −2λ2)δρ(λ)dλ = κ4
K ,
(12.49)
again as expected.

186
Free Random Matrices
These corrections to the free clt are the analog of the Edgeworth series for the classical
clt. In full generality, the contribution of the nth cumulant to δρ(λ) reads
δρn(λ) =
κn
πKn/2−1
Tn
 λ
2

√
4 −λ2,
(12.50)
where Tn(x) are the Chebyshev polynomials of the ﬁrst kind.
12.4 Finite Free Convolutions
We saw that rotationally invariant random matrices become asymptotically free as their
size goes to inﬁnity. At ﬁnite N freeness is not exact, except in very special cases (see
Section 12.5). In this section we will discuss operations on polynomials that are analogous
to the free addition and multiplication but unfortunately lack the full set of properties
of freeness as deﬁned in Chapter 11. When the polynomials are thought of as expected
characteristic polynomials of large matrices, ﬁnite free addition and multiplication do
indeed converge to the free addition and multiplication when the size of the matrix (degree
of the polynomial) goes to inﬁnity.
12.4.1 Notations: Roots and Coefﬁcients
Let p(z) be a polynomial of degree N. By the fundamental theorem of algebra, this
polynomial will have exactly N roots (when counted with their multiplicity) and can be
written as
p(z) = a0
N
,
i=1
(z −λi).
(12.51)
If a0 = 1 we say that p(z) is monic and if all the λi’s are real the polynomial is called
real-rooted. In this section we will only consider real-rooted monic polynomials. Such a
polynomial can always be viewed as the characteristic polynomial of the diagonal matrix
 containing its roots, i.e.
p(z) = det (z1 −) .
(12.52)
We can expand the product (12.51) as
p(z) =
N

k=0
(−1)kakzN−k.
(12.53)
Note that we have deﬁned the coefﬁcient ak as the coefﬁcient of zN−k and not that of zk
and that we have included alternating signs (−1)k in its deﬁnition. The reason is that we
want a simple link between the coefﬁcients ak and the roots λi. We have
ak =
N

ordered
k-tuples i
λi1λi2 . . . λik,
(12.54)
a0 = 1,
a1 =
N

i=1
λi,
a2 =
N

i=1
j=i+1
λiλj,
. . . ,
aN =
N
,
i=1
λi.
(12.55)

12.4 Finite Free Convolutions
187
Note that the coefﬁcient ak is homogeneous to λk. From the coefﬁcients ak we can
compute the sample moments2 of the set of λi’s. In particular we have
μ({λi}) = a1
N
and
σ 2({λi}) = a2
1
N −
2a2
N −1.
(12.56)
The polynomial p(z) will often be the expected characteristic polynomial of some
random matrix M of size N with joint distribution of eigenvalues P({μi}). The coef-
ﬁcients ai are then multi-linear moments of this joint distribution. If the random
eigenvalues μi do not ﬂuctuate, we have λi = μi but in general we should think
of the λi’s as deterministic numbers ﬁxed by the random ensemble considered. The
case of independent eigenvalues gives a trivial expected characteristic polynomial,
i.e. p(z) = (z −E(μ))N or else λi = E(μ) ∀i.
Shifts and scaling of the matrix M can be mapped onto operations on the polynomial
pM(z). For a shift, we have
pM+α1(z) = pM(z −α).
(12.57)
Multiplication by a scalar gives
pαM(z) = αNpM(α−1z) ⇐⇒ak = αkak.
(12.58)
Finally there is a formula for matrix inversion which is only valid when the eigenvalues
are deterministic (e.g. M = OOT with ﬁxed ):
pM−1(z) = zN
aN
pM(1/z) ⇐⇒ak = aN−k
aN
.
(12.59)
A degree N polynomial can always be written as a degree N polynomial of the deriva-
tive operator acting on the monomial zN. We introduce the notation ˆp as
p(z) =: ˆp (dz) zN
the coefﬁcients of ˆp are
ˆak = (−1)N k!
N!aN−k,
(12.60)
where dz is a shorthand notation for d/dz. It will prove useful to compute ﬁnite free
convolutions.
12.4.2 Finite Free Addition
The equivalent of the free addition for two monic polynomials p1(x) and p2(x) of the
same degree N is the ﬁnite free additive convolution deﬁned as
p1 + p2(z) =
8
det[z1 −1 −O2OT ]
9
O ,
(12.61)
where the diagonal matrices 1,2 contain the roots of p1,2(z) all assumed to be real. The
averaging over the orthogonal matrix O is as usual done over the ﬂat (Haar) measure. We
could have chosen to integrate O over unitary matrices or even permutation matrices; the
ﬁnal result would be the same.
As we will show in Section 12.4.5, the additive convolution can be expressed in a very
concise form using the polynomials ˆp1,2(x):
p1 + p2(z) = ˆp1 (dz) p2(z) = ˆp2 (dz) p1(z) = ˆp1 (dz) ˆp2 (dz) zN.
(12.62)
2 Here we have chosen to normalize the sample variance with a factor (N −1)−1. It may seem odd to use the formula suited to
when the mean is unknown for computing the variance of deterministic numbers, but this deﬁnition will later match that of the
ﬁnite free cumulant and give the Hermite polynomial the variance of the corresponding Wigner matrix.

188
Free Random Matrices
It is easy to see that when ps(z) = p1 + p2(z), ps(z) is again a monic polynomial of
degree N. What is less obvious, but true, is that ps(x) is also real-rooted. A proof of this
is beyond the scope of this book. The additive convolution is bilinear in the coefﬁcients of
p1(z) and p2(z), which means that the operation commutes with the expectation value. If
p1,2(z) are independent random polynomials (for example, characteristic polynomials of
independent random matrices) we have a relation for their expected value:
E[p1 + p2(z)] = E[p1(z)] + E[p2(z)].
(12.63)
The ﬁnite free addition can also be written as a relation between the coefﬁcients a(s)
k
of ps(z) and those of p1,2(z):
a(s)
k
=

i+j=k
(N −i)! (N −j)!
N! (N −k)!
a(1)
i
a(2)
j .
(12.64)
More explicitly, the ﬁrst three coefﬁcients are given by
a(s)
0
= 1,
a(s)
1
= a(1)
1
+ a(2)
1 ,
(12.65)
a(s)
2
= a(1)
2
+ a(2)
2
+ N −1
N
a(2)
1 a(2)
1 .
From which we can verify that both the sample mean and the variance (Eq. (12.56)) are
additive under the ﬁnite free addition.
If we call pμ(z) = (z −μ)N the polynomial with a single root μ so p0(z) = zN is the
trivial monic polynomial, we have that, under additive convolution with any p(z), p0(z)
acts as a null element and pμ(z) acts as a shift:
p + p0(z) = p(z)
and
p + pμ(z) = p(z −μ).
(12.66)
Hermite polynomials are stable under this addition:
HN + HN(z) = 2N/2HN(2−N/2z),
(12.67)
where the factors 2N/2 compensate the doubling of the sample variance.
The average characteristic polynomials of Wishart matrices can easily be understood
in terms the ﬁnite free sum. Consider an N-dimensional rank-1 matrix M = xxT where x
is a vector of iid unit variance numbers. It has one eigenvalue equal to N (on average) and
all others are zero, so its average characteristic polynomial is
p(z) = zN−1(z −N) = (1 −dz) zN,
(12.68)
from which we read that ˆp(dz) = (1 −dz). But since an unnormalized Wishart matrix of
parameter T is just the free sum of T such projectors, Eq. (12.62) immediately leads to
pT (z) = (1 −dz)T zN, which coincides with Eq. (6.41).
12.4.3 A Finite R-Transform
If we look back at Eq. (12.62), we notice that the polynomial ˆp(dz) behaves like the
Fourier transform under free addition. Its logarithm is therefore additive. We need to be
a bit careful of what we mean by the logarithm of a polynomial. The function ˆp(dz) has
two important properties: ﬁrst as a power series in dz it always starts 1 + O(dz); second it
is deﬁned by its action on Nth order polynomials in z, so only its N + 1 ﬁrst terms in its
Taylor series matter. We will say that the polynomial is deﬁned modulo dN+1
z
, meaning

12.4 Finite Free Convolutions
189
that higher order terms are set to zero. When we take the logarithm of ˆp(u) we thus mean:
apply the Taylor series of the logarithm around 1 and expand up to the power uN. The
resulting function
L(u) := −

log ˆp(u)

mod uN+1
(12.69)
is then additive. For average characteristic polynomials of random matrices, it should be
related to the R-transform in the large N limit.
Let us examine more closely L(u) in three simple cases: identity, Wigner and Wishart.
• For the identity matrix of size N we have
p1(z) = (z −1)N =
N

k=0
N
k

(−1)kzN−k =
N

k=0
(−dz)k
k!
zN = exp(−dz)zN.
(12.70)
So we ﬁnd
ˆp1(u) =

exp(−u)

mod uN+1
⇒
L1(u) = u.
(12.71)
• For Wigner matrices, we saw in Chapter 6 that the expected characteristic polynomial
of a unit Wigner matrix is given by a Hermite polynomial normalized as
pX(z) = N−N/2HN(
√
Nz) = exp
-
−1
2N
 d
dz
2.
zN,
(12.72)
where the right hand side comes from Eq. (6.8). We then have
ˆpX(u) =
-
exp

−u2
2N
.
mod uN+1
⇒
LX(u) = u2
2N .
(12.73)
• For Wishart matrices, Eq. (6.41) expresses the expected characteristic polynomial as
a derivative operator acting on the monomial zN. For a correctly normalized Wishart
matrix, we then ﬁnd the monic Laguerre polynomial:
pW(z) =

1 −1
T
d
dz
T
zN,
(12.74)
from which we can immediately read off the polynomial ˆp(z):
ˆpW(u) =
(
1 −qu
N
N/q)
mod uN+1
⇒LW(u) = −N
q
&
log

1 −qu
N
'
mod uN+1.
(12.75)
We notice that in these three cases, the L function is related to the corresponding limiting
R-transform of the inﬁnite size matrices as
L′(u) =

R(u/N)

mod uN+1.
(12.76)
The equality at ﬁnite N holds for these simple cases but not in the general case. But the
equality holds in general in the limiting case:
lim
N→∞L′(Nx) = R(x).
(12.77)

190
Free Random Matrices
12.4.4 Finite Free Product
The free product can also be generalized to an operation on a real-rooted monic poly-
nomial of the same degree. We deﬁne
p1 × p2(z) =
8
det[z1 −1O2OT ]
9
O ,
(12.78)
where as usual the diagonal matrices 1,2 contain the roots of p1,2(z). As in the additive
case, averaging the matrix O over the permutation, orthogonal or unitary group gives the
same result.
We will show in Section 12.4.5 that the result of the ﬁnite free product has a simple
expression in terms of the coefﬁcient ak deﬁned by (12.53). When pm(z) = p1 × p2(z)
we have
a(m)
k
=
(N
k
)−1
a(1)
k a(2)
k .
(12.79)
Note that if 1 = α1 is a multiple of the identity with α  0 we have
pα1 = (z −α)N
⇒
a(α1)
k
=
N
k

αk.
(12.80)
Plugging this into Eq. (12.79), we see that the free product with a multiple of the identity
multiplies each ak by αk which is equivalent to multiplying each root by α. In particular
the identity matrix (α = 1) is the neutral element for that convolution.
When pm(z) = p1 × p2(z), the sample mean of the roots of pm(z) (Eq. (12.56))
behaves as
μ(m) = μ(1)μ(2).
When both means are unity, we have for the sample variance
σ 2
(m) = σ 2
(1) + σ 2
(2) −
σ 2
(1)σ 2
(2)
N
.
(12.81)
At large N the last term becomes negligible and we recover the additivity of the variance
for the product of unit-trace free variables (see Eq. (11.85)).
Exercise 12.4.1
Free product of polynomials with roots 0 and 1
Consider an even degree N = 2M polynomial with roots 0 and 1 both with
multiplicity M: p(z) = zM(z −1)M. We will study the ﬁnite free product of this
polynomial with itself pm(z) = p × p(z). We will study the large N limit of this
problem in Section 15.4.2.
(a)
Expand the polynomial and write the coefﬁcients ak in terms of binomial
coefﬁcients.
(b)
Using Eq. (12.79) show that the coefﬁcients of pm(z) are given by
a(m)
k
=
⎧
⎨
⎩
M
k
2 &N
k
'−1
when k ≤M,
0
otherwise.
(12.82)
(c)
The polynomial pm(z) always has zero as a root. What is its multiplicity?

12.4 Finite Free Convolutions
191
(d)
The degree M polynomial q(z) = z−Mpm(z) only has non-zero roots. What
is its average root?
(e)
For M = 2, show that the two roots of q(z) are 1/2 ± 1/
√
12.
(f)
The case M = 4 can be solved by noticing that q(z) is a symmetric function
of (z −1/2). Show that the four roots are given by
λ±± = 1
2 ± 1
2
2
15 ± 2
√
30
35
.
(12.83)
12.4.5 Finite Free Convolutions: Derivation of the Results
We will ﬁrst study the deﬁnition (12.61) when the matrix O is averaged over the
permutation group SN. In this case we can write
ps(z) := p1 + p2(z) = 1
N!

permutations
σ
N
,
i=1

z −λ(1)
i
−λ(2)
σ(i)

.
(12.84)
The coefﬁcients a(s)
k
are given by the average over the permutation group of the coefﬁ-
cients of the polynomial with roots {λ(1)
i
+ λ(2)
σ(i)}. For example for a(s)
1
we have
a(s)
1
= 1
N!

permutations
σ
N

i

λ(1)
i
+ λ(2)
σ(i)

=
N

i

λ(1)
i
+ λ(2)
i

= a(1)
1
+ a(2)
1 .
(12.85)
For the other coefﬁcients ak, the combinatorics is a bit more involved. Let us ﬁrst ﬁgure
out the structure of the result. For each permutation we can expand the product

z −λ(1)
1
−λ(2)
σ(1)
 
z −λ(1)
2
−λ(2)
σ(2)

· · ·

z −λ(1)
N −λ(2)
σ(N)

.
(12.86)
To get a contribution to ak we need to choose the variable z (N −k) times, we can then
choose a λ(1) i times and a λ(2) (k −i) times. For a given k and for each choice of i, once
averaged over all permutations, the product of the λ(1) and that of the λ(2) must both be
completely symmetric and therefore proportional to a(1)
i
a(2)
k−i. We thus have
a(s)
k
=
k

i=0
C(i,k,N)a(1)
i
a(2)
k−i,
(12.87)
where C(i,k,N) are combinatorial coefﬁcients that we still need to determine. There is
an easy way to get these coefﬁcients: we can use a case that we can compute directly and
match the coefﬁcients. If 2 = 1, the identity matrix, we have
p1(z) = (z −1)N
⇒
a(1)
k
=
N
k

.
(12.88)

192
Free Random Matrices
For a generic polynomial p(z), the free sum with p1(z) is given by a simple shift in the
argument z by 1:
ps(z) = p(z −1) =
N

k=0
(−1)ka(1)
k (z −1)N−k
⇒
a(s)
k
=
k

i=0
N −i
N −k

a(1)
i
.
(12.89)
Combining Eqs. (12.87) and (12.88) and matching the coefﬁcient to (12.89), we arrive at
a(s)
k
=
k

i=0
(N −i)! (N −k + i)!
N! (N −k)!
a(1)
i
a(2)
k−i,
(12.90)
which is equivalent to Eq. (12.64). For the equivalence with Eq. (12.62) see
Exercise 12.4.2.
Now suppose we want to average Eq. (12.61) with respect to the orthogonal or unitary
group (O(N) or U(N)). For a given rotation matrix O, we can expand the determinant,
keeping track of powers of z and of the various λ(1) that appear in products containing at
most one of each λ(1)
i
. After averaging over the group, the combinations of λ(1) must be
permutation invariant, i.e. proportional to a(1)
i
; we then get for the coefﬁcient a(s)
k ,
a(s)
k
=
k

i=0
C

i,k,N,
<
λ(2)
j
=
a(1)
i
,
(12.91)
where the coefﬁcients C

i,k,N,
<
λ(2)
j
=
depend on the roots λ(2)
j . By dimensional anal-
ysis, they must be homogeneous to (λ(2))k−i. Since the expression must be symmetrical
in (1) ↔(2), it must be of the form (12.87). And since the free addition with the
unit matrix is the same for all three groups, Eq. (12.64) must be true in all three cases
(SN, O(N) and U(N)).
We now turn to the proof of Eq. (12.79) for the ﬁnite free product. Consider Eq. (12.78),
where the matrix O is averaged over the permutation group SN. For pm(z) = p1 × p2(z)
we have
pm(z) = 1
N!

permutations
σ
N
,
i=1

z −λ(1)
i
λ(2)
σ(i)

:= 1
N!

permutations
σ
pσ (z).
(12.92)
For a given permutation σ, the coefﬁcients aσ
k are given by
aσ
k =
N

ordered
k-tuples i
λ(1)
i1 λ(1)
i2 · · · λ(1)
ik λ(2)
σ(i1)λ(2)
σ(i2) · · · λ(2)
σ(ik).
(12.93)
After averaging over the permutations σ, we must have that a(s)
k
∝a(1)
k a(2)
k . By counting
the number of terms in the sum deﬁning each ak, we realize that the proportionality
constant must be one over this number. We then have
a(m)
k
=
(N
k
)−1
a(1)
k a(2)
k .
(12.94)
We could have derived the proportionality constant by requiring that the polynomial
p1(z) = (z −1)N is the neutral element of this convolution.

12.5 Freeness for 2 × 2 Matrices
193
Exercise 12.4.2
Equivalence of ﬁnite free addition formulas
For a real-rooted monic polynomial p1(z) of degree N, we deﬁne the
polynomial ˆp1(u) as
ˆp1(u) =
N

k=0
(N −k)!
N!
a(1)
k (−1)kuk,
(12.95)
where the coefﬁcients a(1)
k
are given by Eq. (12.53).
(a)
Show that
p1(z) = ˆp1
 d
dz

zN.
(12.96)
(b)
For another polynomial p2(z), show that
ˆp1
 d
dz

p2(z) =
N

k=0
(−1)k
k

i=0
(N −i)! (N −k + i)!
N! (N −k)!
a(1)
i
a(2)
k−izN−k,
(12.97)
which shows the equivalence of Eqs. (12.62) and (12.64).
12.5 Freeness for 2 × 2 Matrices
Certain low-dimensional random matrices can be free. For N = 1 (1 × 1 matrices) all
matrices commute and thus behave as classical random numbers. As mentioned in Chapter
11, freeness is trivial for commuting variables as only constants (deterministic variables)
can be free with respect to non-constant random variables.
For N = 2, there exist non-trivial matrices that can be mutually free. To be more precise,
we consider the space of 2 × 2 symmetric random matrices and deﬁne the operator3
τ(A) = 1
2E[Tr A].
(12.98)
We now consider matrices that have deterministic eigenvalues but random, rotationally
invariant eigenvectors. We will see that any two such matrices are free. Since 2×2 matrices
only have two eigenvalues we can write these matrices as
A = a1 + σ O
1
0
0
−1

OT,
(12.99)
3 More formally, we need to consider 2 × 2 symmetric random matrices with ﬁnite moments to all orders. This space is closed
under addition and multiplication and forms a ring satisfying all the axioms described in Section 11.1.

194
Free Random Matrices
where a is the mean of the two eigenvalues and σ their half-difference. The matrix O is a
random rotation matrix which for N = 2 only has one degree of freedom and can always
be written as
O =
 cos θ
sin θ
−sin θ
cos θ

,
(12.100)
where the angle θ is uniformly distributed in [0,2π]. Note again that we are considering
matrices for which a and σ are non-random. If we perform the matrix multiplications and
use some standard trigonometric identities we ﬁnd
A = a1 + σ
cos 2θ
sin 2θ
sin 2θ
−cos 2θ

.
(12.101)
12.5.1 Freeness of Matrices with Deterministic Eigenvalues
We can now show that any two such matrices A and B are free. If we put ourselves in
the basis where A is diagonal, we see that traceless polynomials pk(A) and qk(B) are
necessarily of the form
pk(A) = ak
1
0
0
−1

and
qk(B) = bk
cos 2θ
sin 2θ
sin 2θ
−cos 2θ

,
(12.102)
for some deterministic numbers ak and bk and where θ is the random angle between the
eigenvectors of A and B. We can now compute the expectation value of the trace of products
of such polynomials:
τ
- n
,
k=1
pk(A)qk(B)
.
= 1
2
 n
,
k=1
akbk

E Tr
 cos 2θ
sin 2θ
−sin 2θ
cos 2θ
n
.
(12.103)
We notice that the matrix on the right hand side is a rotation matrix (of angle 2θ) raised
to the power n and therefore itself a rotation matrix (of angle 2nθ). The average of such a
matrix is zero, thus ﬁnishing our proof that two rotationally invariant 2 × 2 matrices with
deterministic eigenvalues are free.
As a consequence, we can use the R-transform to compute the average eigenvalue
spectrum of A + OBOT when A and B have deterministic eigenvalues and O is a
random rotation matrix. In particular if A and B have the same variance (σ) then this
spectrum is given by the arcsine law that we encountered in Section 7.1.3. For positive
deﬁnite A we can also compute the spectrum of
√
AOBOT √
A using the S-transform (see
Exercises 12.5.1 and 12.5.2).
Exercise 12.5.1
Sum of two free 2 × 2 matrices
(a)
Consider A1 a traceless rotationally invariant 2 × 2 matrix with deterministic
eigenvalues λ± = ±σ. Show that

12.5 Freeness for 2 × 2 Matrices
195
gA(z) =
z
z2 −σ 2
and
R(g) =
*
1 + 4σ 2g2 −1
2g
.
(12.104)
(b)
Two such matrices A1 and A2 (with eigenvalues ±σ1 and ±σ2 respectively)
are free, so we can sum their R-transforms to ﬁnd the spectrum of their sum.
Show that gA1+A2(z) is given by one of the roots of
gA1+A2(z) =
±z
/
(σ 4
1 + σ 4
2 ) −2(σ 2
1 + σ 2
2 )z2 + z4
.
(12.105)
(c)
In the basis where A1 is diagonal A1 + A2 has the form
A =
σ1 + σ2 cos 2θ
σ2 sin 2θ
σ2 sin 2θ
−σ1 −σ2 cos 2θ

,
(12.106)
for a random angle θ uniformly distributed between [0,2π]. Show that the
eigenvalues of A1 + A2 are given by
λ± = ±
/
σ 2
1 + σ 2
2 + 2σ1σ2 cos 2θ.
(12.107)
(d)
Show that the densities implied by (b) and (c) are the same. For σ1 = σ2, it is
called the arcsine law (see Section 7.1.3).
Exercise 12.5.2
Product of two free 2 × 2 matrices
A rotationally invariant 2 × 2 matrix with deterministic eigenvalues 0 and
a1 ≥0 has the form
A1 = O
0
0
0
a1

OT .
(12.108)
Two such matrices are free so we can use the S-transform to compute the
eigenvalue distribution of their product.
(a)
Show that the T-transform and the S-transform of A1 are given by
t(ζ) =
a1
2(ζ −a1)
and
S(t) = 2
a1
t + 1
2t + 1.
(12.109)
(b)
Consider another such matrix A2 with independent eigenvectors and non-zero
eigenvalue a2. Using the multiplicativity of the S-transform, show that the
T-transform and the density of eigenvalues of √A1A2
√A1 are given by
tA1A2 = 1
2 −1
2
2
ζ
ζ −a1a2
(12.110)
and
ρA1A2(λ) = 1
2δ(λ) +
1
2π √λ(a1a2 −λ) for 0 ≤λ ≤a1a2,
(12.111)

196
Free Random Matrices
where the delta function in the density indicates the fact that one eigenvalue
is always zero.
(c)
By directly computing the matrix product in the basis where A1 is diagonal,
show that, in that basis,
*
A1A2
*
A1 = a1a2
0
0
0
1 −cos 2θ

,
(12.112)
where θ is a random angle uniformly distributed between 0 and 2π.
(d)
Show that the distribution of the non-zero eigenvalue implied by (b) and (c) is
the same. It is the shifted arcsine law.
12.5.2 Pairwise Freeness and Free Collections
For the 2 × 2 matrices A and B to be free, they need to have deterministic eigenvalues.
The proof above does not work if the eigenvalues of one of these matrices are random. As
an illustration, consider three matrices A, B and C as above (2 × 2 symmetric rotationally
invariant matrices with deterministic eigenvalues). Each pair of these matrices is free. On
the other hand, the free sum A + B has random eigenvalues and it is not necessarily free
from C. Actually we can show that it is not free with respect to C.
First we show that A, B and C do not form a free collection. For simplicity, we consider
them traceless with σA = σB = σC = 1. Then one can show that
τ (ABCABC) = τ((ABC)2) = 1,
(12.113)
violating the freeness condition for three variables. Indeed, let us compute explicitly ABC
in the basis where A is diagonal. We ﬁnd
ABC =
cos 2θ cos 2φ + sin 2θ sin 2φ
cos 2θ sin 2φ −sin 2θ cos 2φ
cos 2θ sin 2φ −sin 2θ cos 2φ
−cos 2θ cos 2φ −sin 2θ sin 2φ

,
(12.114)
where φ is the angle between the eigenvectors of A and those of C. The matrix ABC is a
non-zero symmetric matrix, so the trace of its square must be non-zero. Actually one ﬁnds
(ABC)2 = 1.
Another way to see that A, B and C are not free as a group is to compute the mixed
cumulant κ6 (A,B,C,A,B,C), given that odd cumulants such as κ1(A) and κ3(A,B,C)
are zero and that mixed cumulants involving two matrices are zero (they are pairwise
free). The only non-zero term in the moment–cumulant relation for τ (ABCABC) (see
Eq. (11.74)) is
τ (ABCABC) = κ6 (A,B,C,A,B,C) = 1.
(12.115)

12.5 Freeness for 2 × 2 Matrices
197
The matrices A, B and C have therefore at least one non-zero cross-cumulant and cannot
be free as a collection.
Now, to show that A+B is not free from C, we realize that if we expand the sixth cross-
cumulant κ6 (A + B,A + B,C,A + B,A + B,C), we will encounter the above non-zero
cross-cumulant of A, B and C. Indeed, a cross-cumulant is linear in each of its arguments.
Since all other terms in this expansion are zero, we ﬁnd
κ6(A + B,A + B,C,A + B,A + B,C)
= κ6 (A,B,C,A,B,C) + κ6 (B,A,C,A,B,C)
+ κ6 (A,B,C,B,A,C) + κ6 (B,A,C,B,A,C) = 4  0.
(12.116)
As a consequence, even though 2 × 2 symmetric rotationally invariant random matrices
with deterministic eigenvalues are pairwise free, they do not satisfy the free clt. If they
did, it would imply that 2 × 2 matrices with a semi-circle spectrum would be stable under
addition, which is not the case. Note that Gaussian 2×2 Wigner matrices (which are stable
under addition) do not have a semi-circle spectrum (see Exercise 12.5.3).
Exercise 12.5.3
Eigenvalue spectrum of 2 × 2 Gaussian matrices
Real symmetric and complex Hermitian Gaussian 2 × 2 matrices are stable
under addition but they are not free. In this exercise we see that their spectrum is
not given by a semi-circle law.
(a)
Use Eq. (5.22) and the Gaussian potential V (x) = x2/2 to write the joint
probability (up to a normalization) of λ1 and λ2, the two eigenvalues of a real
symmetric Gaussian 2 × 2 matrix.
(b)
To ﬁnd the eigenvalue density, we need to compute
ρ(λ1) =
 ∞
−∞
dλ2P(λ1,λ2).
(12.117)
This integral will involve an error function because of the absolute value in
P(λ1,λ2). If you have the courage compute ρ(λ) leaving the normalization
undetermined.
(c)
It is easier to do the complex Hermitian case. Use Eq. (5.26) and the same
potential to adapt your answer in (a) to the β = 2 case.
(d)
The absolute value has now disappeared and the integral in (b) is now much
easier. Perform this integral and ﬁnd the normalization constant. You should
obtain
ρ(λ) = λ2 + 1
2
√π e−λ2.
(12.118)

198
Free Random Matrices
Bibliographical Notes
• On integration over the orthogonal and unitary groups, see
– B. Collins and P. ´Sniady. Integration with respect to the Haar measure on uni-
tary, orthogonal and symplectic group. Communications in Mathematical Physics,
264(3):773–795, 2006,
– B. Collins, A. Guionnet, and E. Maurel-Segala. Asymptotics of unitary and orthogo-
nal matrix integrals. Advances in Mathematics, 222(1):172–215, 2009,
– M. Berg`ere and B. Eynard. Some properties of angular integrals. Journal of Physics
A: Mathematical and Theoretical, 42(26):265201, 2009.
• On the Weingarten coefﬁcients, see
– D. Weingarten. Asymptotic behavior of group integrals in the limit of inﬁnite rank.
Journal of Mathematical Physics, 19(5):999–1001, 1978,
– P. W. Brouwer and C. W. J. Beenakker. Diagrammatic method of integration over
the unitary group, with applications to quantum transport in mesoscopic systems.
Journal of Mathematical Physics, 37(10):4904–4934, 1996,
– B. Collins and S. Matsumoto. On some properties of orthogonal Weingarten func-
tions. Journal of Mathematical Physics, 50(11):113516, 2009,
– T. Banica. The orthogonal Weingarten formula in compact form. Letters in Mathe-
matical Physics, 91(2):105–118, 2010.
• On the Edgeworth series, see e.g. https://en.wikipedia.org/wiki/Edgeworth series and
– J.-P. Bouchaud and M. Potters. Theory of Financial Risk and Derivative Pric-
ing: From Statistical Physics to Risk Management. Cambridge University Press,
Cambridge, 2nd edition, 2003.
• On the Edgeworth series for the free CLT, see
– G. P. Chistyakov and F. G¨otze. Asymptotic expansions in the CLT in free probability.
Probability Theory and Related Fields, 157:107–156, 2011.
• On ﬁnite N free matrices, see
– A. Marcus. Polynomial convolutions and (ﬁnite) free probability, 2018: preprint
available at https://web.math.princeton.edu/∼amarcus/papers/ff main.pdf,
– A. Marcus, D. A. Spielman, and N. Srivastava. Finite free convolutions of polynomi-
als. preprint arXiv:1504.00350, 2015.

13
The Replica Method*
In this chapter we will review another important tool to perform compact computations in
random systems and in particular in random matrix theory, namely the “replica method”.
For example, one can use replicas to understand the R-transform addition rule when one
adds two large, randomly rotated matrices.
Suppose that we want to compute the free energy of a large random system. The free
energy is the logarithm of some partition function Z.1 We expect that the free energy does
not depend on the particular sample so we can average the free energy with respect to
the randomness in the system to get the typical free energy of a given sample. Unfor-
tunately, averaging the logarithm of a partition function is hard. What we can do more
easily is to compute the partition function to some power n and later let n →0 using the
“replica trick”:
log Z = lim
n→0
Zn −1
n
.
(13.1)
The partition function Zn is just the partition function of n non-interacting copies of the
same system Z, these copies are called “replicas”, hence the name of the technique. Averag-
ing the logarithm is then equivalent to averaging Zn and taking the limit n →0 as above.
The averaging procedure will however couple the n copies and the resulting interacting
system is in general hard to solve. In many interesting cases, the partition function can only
be computed as the size of the system (say N) goes to inﬁnity. Naturally one is tempted to
interchange the limits (n →0 and N →∞) but there is no mathematical justiﬁcation (yet)
for doing so. Another problem is that we can hope to compute E[Zn] for all integers n but
is that really sufﬁcient to do a proper n →0 limit?
For all these reasons, replica computations are not considered rigorous. Nevertheless,
they are a precious source of intuition and they allow one to obtain results mathematicians
would call conjectures, but that often turn out to be mathematically exact. Although a lot
of progress has been made to understand why the replica trick works, there is still a halo of
mystery and magic surrounding the method, and a nagging impression that an equivalent
but more transparent formalism awaits revelation.
1 See e.g. Section 13.4 for an explicit example.
199

200
The Replica Method
In the present chapter, we will show how all the results obtained up to now can be
rederived using replicas. We start by showing how the Stieltjes transform can be expressed
using the replica method, and obtain once more the semi-circle law for Gaussian random
matrices. We then discuss R-transforms and S-transforms in the language of replicas.
13.1 Stieltjes Transform
As we have shown in Chapter 2, the density of eigenvalues ρ(λ) of a random matrix is
encoded in the trace of the resolvent of that matrix, which deﬁnes the Stieltjes transform of
ρ(λ). Here we show how this quantity can be computed using the replica formalism.
13.1.1 General Set-Up
To use the replica trick in random matrix theory, we ﬁrst need to express the Stieltjes
transform as the average logarithm of a (possibly random) determinant. In the large N
limit and for z sufﬁciently far from the real eigenvalues, the discrete Stieltjes transform
gN(z) converges to a deterministic function g(z). The replica trick will allow us to compute
E[gN(z)], which also converges to g(z).
Using the deﬁnition Eq. (2.19) and dropping the N subscript, we have
E[gA(z)] = 1
N E
- N

k=1
1
z −λk
.
,
(13.2)
whereas the determinant of z1 −A is given by
det(z1 −A) =
N
,
k=1
(z −λk).
(13.3)
We can turn the product in the determinant into a sum by taking the logarithm and obtain
(z −λk)−1 from log(z −λk) by taking a derivative with respect to z. We then get
E[gA(z)] = 1
N E
( d
dz log det(z1 −A)
)
.
(13.4)
To compute the determinant we may use the multivariate Gaussian identity

dNψ
(2π)N/2 exp

−ψ T Mψ
2

=
1
√
det M
,
(13.5)
which is exact for any N as long as the matrix M is positive deﬁnite. For z larger than the
top eigenvalue of A, (z1 −A) will be positive deﬁnite. The Gaussian formula allows us to
compute the inverse of the square-root of the determinant, but we can neutralize the power
−1/2 by introducing an extra factor −2 in front of the logarithm. Applying the replica trick
(13.1) we thus get

13.1 Stieltjes Transform
201
E[gA(z)] = −2E
( d
dz lim
n→0
Zn −1
Nn
)
,
(13.6)
with
Zn :=

n
,
α=1
dNψα
(2π)N/2 exp

−
n

α=1
ψT
α(z1 −A)ψα
2

,
(13.7)
where we have written Zn as the product of n copies of the same Gaussian integral. This
is all ﬁne, except our Zn is only deﬁned for integer n and we need to take n →0. The
limiting Stieltjes transform is deﬁned as gA(z) = limN→∞E[gA(z)], but in practice the
replica trick will allow us to compute
gA(z) = −2 d
dz lim
n→0 lim
N→∞
E[Zn] −1
Nn
,
(13.8)
and hope that the two limits n →0 and N →∞commute, such thatgA(z) = gA(z). There
is, however, no guarantee that these limits do commute.
13.1.2 The Wigner Case
As a detailed example of replica trick calculation, we now give all the steps necessary to
compute the Stieltjes transform for the Wigner ensemble. We want to take the expectation
value of Eq. (13.7) in the case where A = X, a symmetric Gaussian rotational invariant
matrix:
E[Zn] =

n
,
α=1
dNψα
(2π)N/2 E
⎡
⎣exp
⎛
⎝−z
2
n

α=1
N

i=1
ψ2
αi −
N

i<j
Xijψαiψαj −1
2
N

i
Xiiψαiψαi
⎞
⎠
⎤
⎦,
=

n
,
α=1
dNψα
(2π)N/2 exp

−z
2
n

α=1
N

i=1
ψ2
αi
 N
,
i<j
E
-
exp

−Xij
n

α=1
ψαiψαj
.
×
N
,
i
E
-
exp

−1
2Xii
n

α=1
ψαiψαi
.
,
(13.9)
where we have isolated the products of expectation of independent terms and separated the
diagonal and off-diagonal terms. We can evaluate the expectation values using the following
identity: for a centered Gaussian variable x of variance σ 2, we have
E[eax] = eσ 2a2/2.
(13.10)
Using the fact that the diagonal and off-diagonal elements have a variance equal to, respec-
tively, 2σ 2/N and σ 2/N (see Section 2.2.2), we get

202
The Replica Method
E[Zn] =

n
,
α=1
dNψα
(2π)N/2 exp

−z
2
n

α=1
N

i=1
ψ2
αi
 N
,
i<j
exp
⎛
⎝σ 2
2N
 n

α=1
ψαiψαj
2⎞
⎠
×
N
,
i
exp
⎛
⎝σ 2
4N
 n

α=1
ψαiψαi
2⎞
⎠.
(13.11)
We can now combine the last two sums in the exponential into a single sum over {ij}, which
we can further transform into
σ 2
4N
N

i,j=1
 n

α=1
ψαiψαj
2
= σ 2
4N
n

α,β=1
 N

i=1
ψαiψβi
2
.
(13.12)
We would like to integrate over the variables ψαi but the argument of the exponential
contains fourth order terms in the ψ’s. To tame this term, one uses the Hubbard–
Stratonovich identity:
exp
ax2
2

=

dq
√
2πa
exp

−q2
2a + xq

.
(13.13)
Before we use Hubbard–Stratonovich, we need to regroup diagonal and off-diagonal terms
in αβ:
σ 2
4N
n

α,β=1
 N

i=1
ψαiψβi
2
= σ 2
2N
n

α<β
 N

i=1
ψαiψβi
2
+ σ 2
N
n

α=1
 N

i=1
ψαiψαi
2
2
,
(13.14)
where in the diagonal terms we have pushed the factor 1/4 in the squared quantity for later
convenience. We can now use Eq. (13.13), introducing diagonal qαα and upper triangular
qαβ to linearize the squared quantities. Writing the q’s as a symmetric matrix q we have
E[Zn] ∝

dq

n
,
α=1
dNψα
(2π)N/2 exp −
⎛
⎝N Tr q2
4σ 2
+
N

i=1
n

α,β=1
(zδαβ −qαβ)ψαiψβi
2
⎞
⎠,
(13.15)
where dq is the integration over the independent component of the n × n symmetric
matrix q; note that we have dropped z-independent constant factors. The integral of ψαi
is now a multivariate Gaussian integral, actually N copies of the very same n-dimensional
Gaussian integral:

n
,
α=1
dψα
√
2π
exp −
⎛
⎝
n

α,β=1
(zδαβ −qαβ)ψαψβ
2
⎞
⎠= (det(z1 −q))−1/2.
(13.16)

13.1 Stieltjes Transform
203
Raising this integral to the Nth power and using det M = exp Tr log M we ﬁnd
E[Zn] ∝

dq exp −
(
N Tr
 q2
4σ 2 + 1
2 log(z1 −q)
)
:=

dq exp

−N
2 F(q)

.
(13.17)
We now ﬁx n and evaluate E[Zn] for very large N, leaving the limit n →0 for later. In
the large N limit, the integral over the matrix q can be done by the saddle point method.
More precisely, we should ﬁnd an extremum of F(q) in the n(n + 1)/2 elements of q.
Alternatively we can diagonalize q, introducing the log of a Vandermonde determinant in
the exponential (see Section 5.1.4).2 In terms of the eigenvalues qα of q, one has
F({qα}) =
n

α=1
q2
α
2σ 2 + log(z −qα) −1
N

αβ
log |qα −qβ|.
(13.18)
To ﬁnd the saddle point, we take the partial derivatives of F{qα} with respect to the {qα}
and equate them to zero:
qα
σ 2 −
1
z −qα
−1
N

αβ
2
qα −qβ
= 0.
(13.19)
The effect of the last term is to push the eigenvalues qα away from one another, such that
in equilibrium their relative distance is of order 1/N and the last term is of the same order
as the ﬁrst two. Since there are only n such eigenvalues, the total spread (from the largest
to smallest) is of order n/N, which we will neglect when N →∞. Hence we can assume
that all eigenvalues are identical and equal to q∗(z), where q∗(z) satisﬁes
z −q∗= σ 2
q∗.
(13.20)
We recognize the self-consistent equation for the Stieltjes transform of the Wigner (Eq.
(2.35)) if we make the identiﬁcation q∗(z) = σ 2gX(z). For N large and n small we indeed
have
E[Zn] = exp

−Nn
2 F1(z,q∗(z))

with F1(z,q) = q2
2σ 2 + log(z −q),
(13.21)
so
lim
n→0 lim
N→∞
E[Zn] −1
Nn
= −F1(z,q∗(z))
2
.
(13.22)
Finally, from Eq. (13.8), we should have
gX(z) = d
dzF1(z,q∗(z)).
(13.23)
2 A third method is used in spin-glass problems where the integrand has permutation symmetry but not necessarily rotational
symmetry, see Section 13.4.

204
The Replica Method
To ﬁnish the computation we need to take the derivative of F1(z,q∗(z)) with respect to z,
but since q∗(z) is an extremum of F1, the partial derivative of F1(z,q) with respect to q is
zero at q = q∗(z). Hence
gX(z) = ∂
∂zF1(z,q)

q=q∗(z)
=
1
(z −q∗) = q∗(z)
σ 2 .
(13.24)
We thus recover the usual solution of the self-consistent Wigner equation.
13.2 Resolvent Matrix
13.2.1 General Case
We saw that the replica trick can be used to compute the average Stieltjes transform of
a random matrix. The Stieltjes transform is the normalized trace of the resolvent matrix
GA(z) = (z1 −A)−1. In Chapter 19 we will need to know the average of the elements
of the resolvent matrix for free addition and multiplication. These averages can also be
computed using the replica trick. An element of an inverse matrix can indeed be written as
a multivariate Gaussian integral:
&
M−1'
ij ≡1
Z

dNψ
(2π)N/2 ψiψj exp

−ψT Mψ
2

,
Z :=

dNψ
(2π)N/2 exp

−ψ T Mψ
2

,
(13.25)
which we can rewrite as
&
M−1'
ij =
lim
m→−1 Zm

dNψ ψiψj exp

−ψT Mψ
2

.
(13.26)
If we express Zm for m ∈N+ as m Gaussian integrals and combine them with the integral
with the ψiψj term (which we label number 1) we get, with n = m + 1:
&
M−1'
ij = lim
n→0

n
,
α=1
dNψα
(2π)nN/2 ψ1iψ1j exp

−
n

α=1
ψT
αMψα
2

:=
8
ψ1iψ1j
9
n=0 .
(13.27)
This equation can then be used to compute averages of elements of the resolvent matrix
by using M = z1 −A for the relevant random matrix A. For example, in the case of
Gaussian Wigner matrices, the correlation
8
ψ1iψ1j
9
n=0 can be computed using the saddle
point conﬁguration of the ψ’s. Since different i all decouple and play the same role, it is
clear that
E[GX(z)]ij =
8
ψ1iψ1j
9
n=0 = δijgX(z),
(13.28)
i.e. the average resolvent of a Gaussian matrix is simply the average Stieltjes transform
times the identity matrix.

13.2 Resolvent Matrix
205
13.2.2 Free Addition
In this section we will show how to use Eq. (13.27) to compute the average of the full
resolvent for the sum of two randomly rotated matrices. Since we know these matrices are
free in the large N limit, we expect to recover the additivity of R-transforms, but we will in
fact obtain a slightly richer result. Also, the replica method is very convenient to manipulate
and resum the perturbation theory alluded to in Section 12.2.
Consider two symmetric matrices A and B and the new matrix C = A+OBOT where O
is a random orthogonal matrix. We want to compute
E[GC(z)] = E
&
(z1 −A −OBOT )−1'
,
(13.29)
where the expectation value is over the orthogonal matrix O. We can always choose B to
be diagonal. If B is not diagonal to start with, we just absorb the orthogonal matrix that
diagonalizes B in the matrix O. Expressing GC(z) in the eigenbasis of A is equivalent
to choosing A to be diagonal. In that basis, the off-diagonal elements of E[GC(z)] must
be zero by the following argument: since both A and B are diagonal, for every matrix O
that contributes to an off-diagonal element of E[GC(z)] there exists an equally probable
matrix O′ with the same contribution but opposite sign, hence the average must be zero.
Note that while the average matrix E[GC(z)] commutes with A, a particular realization of
the random matrix GC(z) (corresponding to a speciﬁc choice for O) will not in general
commute with A.
Now, let us use the replica formalism to compute E[GC(z)], i.e. start with
GC(z)ij = lim
n→0

n
,
α=1
dNψα
(2π)N/2 ψ1iψ1j exp

−
n

α=1
ψ T
α(z1 −A)ψα
2

× E
-
exp
 n

α=1
ψ T
αOBOT ψα
2
.
,
(13.30)
where now we skip the i,j indices on ψα, treated as vectors.
The last term with the expectation value can be rewritten as
I = E
-
exp

N
2
n

α=1
Tr Qα,αOBOT
.
,
(13.31)
where Qα,β = ψαψ T
β/N is an n×n symmetric matrix. We recognize the Harish-Chandra–
Itzykson–Zuber integral discussed in Chapter 10, with one matrix (n
α=1 Qα,α) being at
most of rank n ≪N, so we can use the low-rank formula Eq. (10.45). Our expectation
value thus becomes
I = exp
N
2 Trn HB(Q)

,
(13.32)
where Trn denotes the trace of an n × n matrix and HB is the anti-derivative of the
R-transform of B.

206
The Replica Method
We now need to perform the integral of ψα in Eq. (13.30). But in order to do so we must
deal with Tr HB(Q), which is a non-linear function of the ψα. The trick is to make the
matrix Q an integration variable that we ﬁx to its deﬁnition using a delta function. The
delta function is itself represented as an integral over another (n × n) symmetric matrix Y
along the imaginary axis. In other words, we introduce the following representation of the
delta function in Eq. (13.30):
 i∞
−i∞
Nn(n+1)/2dY
23n/2πn/2
exp
⎛
⎝−N
2 Trn QY + 1
2
n

α,β=1
Yα,βψαψT
β
⎞
⎠,
(13.33)
where the integrals over dQ and dY are over symmetric matrices. We have absorbed a factor
of N in Y and a factor of 2 on its diagonal, hence the extra factors of 2 and N in front of
dY. We can now perform the following Gaussian integral over ψα:
Jij =

N
,
k=1
n
,
α=1
dψαk
√
2π
ψ1iψ1j exp
⎛
⎝−1
2
N

k=1
n

α,β=1
ψαk(zδα,β −akδα,β −Yαβ)ψβk
⎞
⎠,
(13.34)
where we have written the vectors ψα in terms of their components ψαk, and where ak are
the eigenvalues of A. We notice that the Gaussian integral is diagonal in the index k, so we
have N −1 n-dimensional Gaussian integrands differing only by their value of ak, and a
last integral for k = i = j, where the term ψ2
1i is in front of the Gaussian integrand (the
integral is zero if i  j, meaning that GC(z) is diagonal, as expected). The result is then
Jij = δij
&
((z −ai)1n −Y)−1'
11
N
,
k=1
(det((z −ak)1n −Y))−1/2,
(13.35)
where the ﬁrst term is the 11 element of an n × n matrix, coming from the term ψ2
1i.
Returning to our main expression Eq. (13.30) and dropping constants that are 1 as
n →0,
E[GC(z)]ij = lim
n→0
 i∞
−i∞
dQ

dYδij
&
((z −ai)1n −Y)−1'
11
× exp
-
N
2

−Trn QY + Trn HB(Q) −1
N
N

k=1
Trn log((z −ak)1n −Y)
.
.
(13.36)
For large N the integral over Y and Q is dominated by the saddle point, i.e. the extremum
of the argument of the exponential. The inverse-matrix term in front of the exponential does
not contain a power of N so it does not contribute to the determination of the saddle point.
The extremum is over a function of two n × n symmetric matrices. Taking derivatives with
respect to Qαβ and equating it to zero gives
Yαβ = [RB(Q)]αβ,
(13.37)

13.2 Resolvent Matrix
207
and similarly when taking derivatives with respect to Yαβ:
Qαβ = 1
N
N

k=1
&
((z −ak)1n −Y)−1'
αβ .
(13.38)
Let us look at these equations in a basis where Q is diagonal. The ﬁrst equation shows that
Y is also diagonal, so that the second equation reads
Qαα = 1
N
N

k=1
1
z −ak −Yαα
≡gA(z −Yαα).
(13.39)
Hence all n diagonal elements Qαα and Yαα, α = 1, . . . ,n satisfy the same pair of equa-
tions. For large z, there is a unique solution to these equations, hence Q and Y must
be multiples of the identity Q = q∗1n and Y = y∗1n, as expected from the rotational
symmetry of the argument of the exponential that we are maximizing. The quantities q∗
and y∗are the unique solutions of
y = RB(q)
and
q = gA(z −y).
(13.40)
The saddle point for Y is real while our integral representation Eq. (13.33) was over purely
imaginary matrices; but for large values of z, the solutions of Eqs. (13.40) give small values
for q∗and y∗, and for such small values the integral contour can be deformed without
encountering any singularities. We also justify the use of RB(q∗) = H ′
B(q∗) as q∗can be
made arbitrarily small by choosing a large enough z.
The expectation of the resolvent is thus given, for large enough z and N, by
E[GC(z)ij]
≈lim
n→0
δij
(z −ai −y∗) exp
-
nN
2

−q∗y∗+ HB(q∗) −1
N
N

k=1
log(z −ak −y∗)
.
.
(13.41)
As n →0 the exponential drops out and we obtain, in matrix form,
E[GC(z)] = GA(z −RB(q∗))
with
q∗= gA(z −RB(q∗)).
(13.42)
13.2.3 Resolvent Subordination for Addition and Multiplication
Equation (13.42) relates the average resolvent of C to that of A. By taking the normalized
trace on both sides we ﬁnd
q∗= gC(z) = gA(z −RB(q∗)),
(13.43)
which is precisely the subordination relation for the Stieltjes transform of a free sum that
we found in Section 11.3.8. We just have rederived this result once again with replicas.

208
The Replica Method
But what is more interesting is that we have found a relationship for the average of a matrix
element of the full resolvent matrix, namely
E[GC(z)] = GA (z −RB(gC(z))) .
(13.44)
This relation will give precious information on the overlap between the eigenvalues of A
and B and those of C. Note that, by symmetry, one also has
E[GC(z)] = GB (z −RA(gC(z))) .
(13.45)
In the free product case, namely C = A
1
2 BA
1
2 where A and B are large positive deﬁnite
matrices whose eigenvectors are mutually random, a very similar replica computation gives
a subordination relation for the average T matrix:
E[TC(ζ)] = TA[SB(tC(ζ))ζ],
(13.46)
with SB(t) the S-transform of the matrix B. If we take the normalized trace on both sides,
we recover the subordination relation Eq. (11.109). Equation (13.46) can then be turned
into a subordination relation for the full resolvent:
E[GC(z)] = S∗GA(zS∗)
with
S∗:= SB(zgC(z) −1).
(13.47)
13.2.4 “Quenched” vs “Annealed”
The replica trick is quite burdensome as one has to keep track of n copies of an integration
vector ψn and these vectors interact through the averaging process. At large N one typically
has to do a saddle point over one or several n × n matrices (e.g. Q and Y in the free
addition computation of the previous section), and at the end take the n →0 limit. But in
all computations of Stieltjes transforms so far, taking n = 1 instead of n →0 gives the
correct saddle point and the correct ﬁnal result. In other words, assuming that
E[log Z] ≈log E[Z]
(13.48)
leads to the correct result. For historical reasons coming from physics, E[log Z] is called a
quenched average whereas log E[Z] is called an annealed average.
For example, if we go back to Eq. (13.21) we see that taking the logarithm of the n = 1
result gives the same result as the correct n →0 limit. The same is true for the Wishart
case. For the free addition and multiplication one can also compute the Stieltjes transform
using n = 1. This is a general result for bulk properties of random matrices. Most natural
ensembles of random symmetric matrices (such as those from Chapter 5 and those arising
from free addition and multiplication) feature a strong repulsion of eigenvalues. Because
of this repulsion, eigenvalues do not ﬂuctuate much around their classical positions – see
the detailed discussion in Section 5.4.1. It is the absence of eigenvalue ﬂuctuations on the
global scale that makes the n = 1 and n →0 saddle points equivalent.
For the rank-1 hciz integral, on the other hand, things are more subtle. As we show
in the next section, the annealed average n = 1 gives the right answer in some interval

13.3 Rank-1 HCIZ and Replicas
209
of parameters, when the integral is dominated by the bulk properties of eigenvalues. Out-
side this regime, ﬂuctuations of the largest eigenvalue matter and the n = 1 result is no
longer correct.
13.3 Rank-1 HCIZ and Replicas
In Chapter 10 we studied the rank-1 hciz integral and deﬁned the function HB(t) as
HB(t) := lim
N→∞
2
N log
6
exp
N
2 Tr TOBOT
7
O
,
(13.49)
where the averaging is done over the orthogonal group for O, T is a rank-1 matrix with
eigenvalue t and B a ﬁxed matrix. If B is a member of a random ensemble, such as the
Wigner ensemble, the averaging over O should be done for a ﬁxed B and only later the
function HB(t) can be averaged, if needed, over the randomness of B (quenched average).
One could also do an annealed average over B, deﬁning another function ˆH(t) as
ˆH(t) := lim
N→∞
2
N log
6
exp
N
2 Tr TOBOT
7
O,B
.
(13.50)
It turns out that for small enough values of t, the two quantities are equal, i.e. ˆH(t) =
HB(t). For larger values of t, however, these two quantities differ. The aim of this section is
to compute explicitly these quantities in the Wigner case using the replica trick, and show
that there is a phase transition for a well-deﬁned value t = tc beyond which quenched and
annealed averages do not coincide.
13.3.1 Annealed Average
Let us compute directly the “annealed” average when B = X is a Wigner matrix and T =
t e1eT
1, where t is the only non-zero eigenvalue of T and e1 is the unit vector (1,0, . . . ,0)T .
Then
6
exp
N
2 Tr TX
7
X
=
6
exp
Nt
2 eT
1Xe1
7
X
=

dX11
*
4πσ 2/N
exp
NtX11
2
−N
4σ 2 X2
11

= exp
N
2
t2σ 2
2

,
(13.51)
so the annealed ˆHwig(t) is given by
ˆHwig(t) = σ 2
2 t2,
(13.52)
which, at least superﬁcially, coincides with the integral of the R-transform of a Wigner
matrix, Eq. (10.61).

210
The Replica Method
13.3.2 Quenched Average
The annealed average corresponds, in the replica language, to n = 1. Let us now turn to
arbitrary (integer) n. To keep notation light we will set σ 2 = 1. As in Eq. (10.31) we deﬁne
the partition function
Zt(X) =

dNψ
(2π)N/2 δ

∥ψ∥2 −Nt

exp
1
2ψ T Xψ

,
(13.53)
seeking to compute, at the end of the calculation,
E[HX(t)] = lim
N→∞
2
N lim
n→0
Zn
t (X) −1
n

−1 −log t,
(13.54)
where 1+log t is the large N limit of 2/N log Zt(X = 0), with Zt(0) given by Eq. (10.38).
If we write Zn
t (X) as multiple copies of the same integral and express the Dirac deltas as
Fourier integrals over zα, we get
Zn
t (X) =
 i∞
−i∞
n
,
α=1
dzα

n
,
α=1
dNψα
(2π)N/2 exp

1
2
n

α=1

Nzαt −zαψ T
αψα

+ 1
2
n

α=1
ψ T
αXψα

.
(13.55)
In order to take the expectation value over the Gaussian random matrix X, we need as
always to separate the diagonal and off-diagonal elements of X. The steps are the same as
those we took in Section 13.1.2:
E
-
exp

1
2
n

α=1
ψ T
αXψα
.
= exp
⎛
⎝
N

i,j=1
1
4N
 n

α=1
ψαiψαj
2⎞
⎠
= exp
⎛
⎝1
4N
n

α,β=1
 N

i=1
ψαiψβi
2⎞
⎠,
(13.56)
which can be rewritten as a Hubbard–Stratonovich integral over an n × n matrix q:
E[. . .] = C(n)

dq exp
⎛
⎝−N Tr q2
4
+
N

i=1
n

α,β=1
qαβψαiψβi
2
⎞
⎠,
(13.57)
where C(n) is a numerical coefﬁcient. After Gaussian integration, one thus ﬁnds
E[Zn
t ] =
 i∞
−i∞
n
,
α=1
dzα

dqC(n) exp
(N
2

t Tr z −Tr q2
2
−Tr log(z −q)
)
, (13.58)
which makes sense provided that the real part of z is larger than all the eigenvalues of q.
We now deﬁne
Fn(q,z;t) = t Tr z −Tr q2
2
−Tr log(z −q),
(13.59)

13.3 Rank-1 HCIZ and Replicas
211
where z is the vector of zα treated as a diagonal matrix. For n ≥1 we need to ﬁnd a
saddle point in the space of n × n matrices z and q, i.e. a point in that space where the ﬁrst
derivatives of Ft(q,z) are zero with a negative Hessian.
As a check, for n = 1 we have at the saddle point
q∗=
1
z∗−q∗
and
t =
1
z∗−q∗
⇒
z∗= t + 1
t
and
q∗= t.
(13.60)
Hence
F1(q∗,z∗;t) = t2 + 1 −t2
2 + log t,
(13.61)
or
ˆHwig(t) = t2
2 ,
(13.62)
as it should be.
We now go back to the general n case. Using Eq. (1.37) we can take a matrix derivative
of Eq. (13.59) with respect to q and z:
q = (z −q)−1
and
&
(z −q)−1'
αα = t.
(13.63)
In the following technical part, we solve these equations for integer n ≥1. We discuss the
ﬁnal result at the end of this subsection.
The second equation in (13.63) comes from the derivative with respect to z; remember
z is only a diagonal matrix, the derivative with respect to z tells us only about the diagonal
elements.
From this we can argue that z must be a multiple of the identity and q of the form3
q =
⎛
⎜⎜⎝
t
b
. . .
b
b
t
. . .
b
...
...
...
b
b
b
. . .
t
⎞
⎟⎟⎠,
(13.64)
for some b to be determined. To ﬁnd an equation for b and z we need to express Eqs.
(13.63) in terms of those quantities. To do so we ﬁrst write the matrix q as a rank-1
perturbation of a multiple of the identity matrix:
q = (t −b)1 + nbP1,
(13.65)
where P1 = eeT is the projector onto the normalized vector of all 1:
e =
1
√n
⎛
⎜⎜⎝
1
1
...
1
⎞
⎟⎟⎠.
(13.66)
3 More complicated, block diagonal structures for q sometimes need to be considered in the limit n →0. This is called “replica
symmetry breaking”, a phenomenon that occurs in many “complex” optimization problems, such as spin-glasses – see Section
13.4. Fortunately, in the present case, these complications are not present.

212
The Replica Method
Note that the eigenvalues of the matrix q are (t −b) + nb (with multiplicity 1) and (t −b)
(with multiplicity (n −1)). Since z is a multiple of the identity, the matrix z −q is a
rank-1 perturbation of a multiple of the identity and it can be inverted using the Sherman–
Morrison formula, Eq. (1.28). The ﬁrst of Eqs. (13.63) becomes
(t −b)1 + nbP1 =
1
z −t + b +
nbP1
(z −t + b)2(1 −nb(z −t + b)−1).
(13.67)
We can now equate the prefactors in front of the identity matrix 1 and of the projector P1
separately, to get two equations for our two unknowns (z and b). For the identity matrix
we get
(t −b) =
1
z −t + b
⇒
z = (t −b) +
1
t −b .
(13.68)
For the second equation, we ﬁrst replace (z −t + b)−1 by t −b and get
nb =
(t −b)2nb
1 −nb(t −b).
(13.69)
We immediately ﬁnd one solution: b = 0. For this solution both q = q01 and z = z01
are multiples of the identity and we have q0 = t and z0 = t + t−1. This coincides with
the (unique) solution we found in the annealed (n = 1) case. For general n, there are
potentially other solutions. Simplifying off nb, we ﬁnd a quadratic equation for b:
1 −nb(t −b) = (t −b)2,
(13.70)
whose solutions we write as
b± = (n −2)t ±
*
(n2t2 −4(n −1))
2(n −1)
.
(13.71)
From the two solutions for b we can compute the corresponding values of z using Eq.
(13.68). We get a term with a square-root on the denominator that we simplify using
(c ±
√
d)−1 ≡(c ∓
√
d)/(c2 −d). After further simpliﬁcation we ﬁnd
z± = n2t ± (n −2)
*
(n2t2 −4(n −1))
2(n −1)
.
(13.72)
We now need to choose one of the three solutions z0, z+ and z−. First, we consider
integer n ≥1 where the replica method is perfectly legitimate. We will later deal with the
n →0 limit.
For n = 1, z−is ill deﬁned while z+ becomes identical to z0. The only solution for all
t is therefore z0 and we recover the annealed result discussed in the previous subsection.
For n ≥2, we ﬁrst notice that the solutions z± do not exist for t < ts := 2
√
n −1/n;
they yield a complex result when the result must be real. So for t < ts, z0 is the solution.
For larger values of t we should compare the values of Fn(q∗,z∗;t) and choose the
maximum one. For t > ts, the z+ solution always dominates z−, so we only consider
z+ and z0 henceforth.
For n = 2, the analysis is easy, ts = 1 and tc = 1: at t = 1, z+ = z0 and for t > 1
the z+ solution dominates. For n > 2, the situation is a bit more subtle. The z+ solution
appears at ts < 1 but at that point z0 still dominates. At t = 1, z+ dominates. At some
t = tc, with ts < tc < 1, we must have Fn(q0,z0;tc) = Fn(q+,z+;tc). This point could
in principle be shown analytically but it is easier numerically. In particular we do not have
an analytical expression for tc(n) except the above bound tc(n > 2) < 1 and the value
tc(2) = 1 (see Fig. 13.1).

13.3 Rank-1 HCIZ and Replicas
213
2
4
6
8
10
n
0.6
0.7
0.8
0.9
1.0
tc
tc
ts
Figure 13.1 The point tc(n) where the Fn(q0,z0;tc) = Fn(q+,z+;tc) and beyond which the z+
solution dominates. Also shown is the point ts(n) = 2
√
n −1/n where the z+ solution starts to
exist. Note that ts ≤tc ≤1 for all n ≥2. Hence the transition appears below t = 1. Note also that
tc →0 as n →∞.
We can now put everything together but there is a trick to save computation effort.
Given that our solutions cancel the partial derivatives of Ft(q,z;t) with respect to q and z,
we can easily compute its derivative with respect to t:
d
dt Fn(q∗,z∗;t) = ∂
∂t Fn(q∗,z∗;t) = Tr z∗(t) = nz∗(t).
(13.73)
Note that we can follow the value of Fn(q∗,z∗;t) through the critical point tc because
Fn(q∗,z∗;t) is continuous at that point (even if its derivative is not).
The above analysis therefore allows us to ﬁnd, for n ≥1,
log E[Zn
t ] ∼Nn
2 Fn(t),
(13.74)
where
d
dt Fn(t) =
⎧
⎨
⎩
t + 1
t
for t ≤tc(n),
n2t+(n−2)√
(n2t2−4(n−1))
2(n−1)
for t > tc(n),
(13.75)
with the boundary condition Fn(0) = 0.
We can now analytically continue this solution down to n →0. The ﬁrst regime, for
small t, is easy as it does not depend on n. In the large t regime, the extrapolation of z+(t)
to n →0 gives the very simple result (see Eq. (13.72)): z+ = 2 for all t. The most tricky
part is to ﬁnd the critical point where one goes from the z0 = t +1/t solution to the z+ = 2
solution. We cannot analytically continue tc(n) to n →0, as we have no explicit formula
for it. On the other hand, we can directly ﬁnd the point tc(n = 0) at which the two solutions

214
The Replica Method
0
1
2
3
4
5
t
0.0
2.5
5.0
7.5
10.0
12.5
HX(t)
quenched
annealed
upper bound
Figure 13.2 The function HX(t) for a unit Wigner computed with a “quenched” average (hciz
integral) and an “annealed” average. We also show the upper bound given by Eq. (10.59). The
annealed and quenched averages are identical up to t = tc = 1 and differ for larger t. The annealed
average violates the bound, which is expected as in this case λmax ﬂuctuates and exceptionally large
values of λmax dominate the average exponential hciz integral.
lead to the same F0. It is relatively straightforward to show that this point is tc = 1.4
Correspondingly,
d
dt F0(t) =
$ t + 1
t
for t ≤tc(0) = 1,
2
for t > tc(0) = 1.
(13.76)
We can now go back to the deﬁnition of the function E[HX(t)] (Eq. (13.54)). After
taking the n →0 and N →∞limits we ﬁnd (see Fig. 13.2)
d
dt E[HX(t)] =
$ t
for t ≤1,
2 −1
t
for t > 1,
(13.77)
with the condition E[HX(t = 0)] = 0.
The upshot of this (rather complex) calculation is that, as announced in the introduction,
for t ≤tc = 1 the quenched and annealed results coincide, i.e. ˆHwig(t) = E[HX(t)].
For t > tc, on the other hand, the two quantities are different. The reason is that for
sufﬁciently large values of t, the average hciz integral becomes dominated by very rare
Wigner matrices that happen to have a largest eigenvalue signiﬁcantly larger than the
Wigner semi-circle edge λ = 2. This allows ˆHwig(t) to continue its quadratic progression,
while E[HX(t)] is dominated by the edge of the average spectrum and its growth with t
is contained (see Fig. 13.2). When one computes higher and higher moments of the hciz
4 Something peculiar happens as n →0, namely the minimum solution becomes the maximum one and vice versa. In other
words for small t, where we know that z0 is the right solution, we have F0(z0;t) < F0(z+;t) and the opposite at large t. This
paradox is always present within the replica method when n →0.

13.4 Spin-Glasses, Replicas and Low-Rank HCIZ
215
integral (i.e. as the number of replicas n increases), the dominance of extreme eigenvalues
becomes more and more acute, leading to a smaller and smaller transition point tc(n).5
13.4 Spin-Glasses, Replicas and Low-Rank HCIZ
“Spin-glasses” are disordered magnetic materials exhibiting a freezing transition as the
temperature is reduced. Typical examples are silver–manganese (or copper–manganese)
alloys, where the manganese atoms carry a magnetic spin and are randomly dispersed
in a non-magnetic matrix. Contrary to a ferromagnet (i.e. usual magnets like permalloy),
where all microscopic spins agree to point more or less in the same direction when the
temperature is below a certain transition temperature (872 K for permalloy), spins in spin-
glasses freeze, but the conﬁguration they adopt is disordered, “amorphous”, with zero net
magnetization.
A simple model to explain the phenomenon is the following. The energy of N spins
Si = ±1 is given by
H({S}) = 1
2
N

i,j=1
JijSiSj,
(13.78)
where J is a random matrix, which we take to be drawn from a rotational invariant
ensemble, i.e. J = OOT where O is chosen according to the (ﬂat) Haar measure over
O(N) and  is a certain ﬁxed diagonal matrix with τ() = 0, such that any pair of spins is
as likely to want to point in the same direction or in opposite directions. The simplest case
corresponds to J = X, a Wigner matrix, in which case the spectrum of  is the Wigner
semi-circle. This case corresponds to the celebrated Sherrington–Kirkpatrick (SK) model,
but other cases have been considered in the literature as well.
The physical properties of the system are encoded in the average free energy F,
deﬁned as
F := −T EJ

log Z

;
Z :=

{S}
exp
H({S})
T

,
(13.79)
where the partition function Z is obtained as the sum over all 2N conﬁgurations of the N
spins, and T is the temperature. One of the difﬁculties of the theory of spin-glasses is to
perform the average over the interaction matrix J of the logarithm of Z. Once again, one
can try to use the replica trick to perform this average, to wit,
EJ

log Z

=
∂
∂nEJ

Znn=0
.
(13.80)
One then computes the right hand side for integer n and hopes that the analytic continua-
tion to n →0 makes sense. Introducing n replicas of the system, one has
EJ

Zn
= EJ
⎡
⎣

{S1,S2,...,Sn}
exp
⎛
⎝
n
α=1
N
i,j=1 JijSα
i Sα
j
2T
⎞
⎠
⎤
⎦.
(13.81)
Now, for a ﬁxed conﬁguration of all nN spins {S1,S2, . . . ,Sn}, the N ×N matrix K(n)
ij
:=
n
α=1 Sα
i Sα
j /N is at most of rank n, which is small compared to N which we will
5 A very similar mechanism is at play in Derrida’s random energy model, see Derrida [1981].

216
The Replica Method
take to inﬁnity. So we have to compute a low-rank hciz integral, which is given by
Eq. (10.45):
EJ
(
exp
 N
2T Tr OOT K(n)
)
≈exp
N
2 Tr HJ(K(n)/T )

,
(13.82)
where HJ is the anti-derivative of the R-transform of J (or ). Now the non-zero eigen-
values of the N × N matrix K(n) are the same as those of the n × n matrix Qαβ =
N
i=1 Sα
i Sβ
i /N, called the overlap matrix, because its elements measure the similarity of
the conﬁgurations {Sa} and {Sb}. Hence, we have to compute
EJ

Zn
=

{S1,S2,...,Sn}
exp
N
2 Trn HJ(Q/T )

.
(13.83)
It should be clear to the reader that all the steps above are very close to the ones followed in
Section 13.2.2. We continue in the same vein, introducing a new n×n matrix for imposing
the constraint Qαβ = N
i=1 Sα
i Sβ
i /N:
1 =
 i∞
−i∞
Nn(n+1)/2dY
23n/2πn/2

dQ exp
⎛
⎝−N Trn QY +
n

α,β=1
Yα,β
N

i=1
Sα
i Sβ
i
⎞
⎠. (13.84)
The nice thing about this representation is that sums over i become totally decoupled. So
we get
EJ

Zn
= C
 i∞
−i∞
dY

dQ exp
N
2 Trn HJ(Q/T ) −N Trn QY + NS(Y)

,
(13.85)
where C is an irrelevant constant and
S(Y) := log Z
with
Z :=
⎡
⎣
S
e
n
α,β=1 Yα,βSαSβ
⎤
⎦.
(13.86)
In the large N limit, Eq. (13.85) can be estimated using a saddle point method over Y and
Q. As in Section 13.2.2 the ﬁrst equation reads
Yαβ = 1
2T [RJ(Q/T )]αβ,
(13.87)
and taking derivatives with respect to Yαβ, we ﬁnd
Qαβ = 1
Z

S
SαSβe
n
α′,β′=1 Yα′,β′Sα′Sβ′
,
(13.88)
which leads to the following self-consistent equation for Q:
Qαβ =

S SαSβe
1
2T
n
α′,β′=1[RJ(Q/T )]α′β′Sα′Sβ′

S e
1
2T
n
α′,β′=1[RJ(Q/T )]α′β′Sα′Sβ′
:= ⟨SαSβ⟩T .
(13.89)
At sufﬁciently high temperatures, one can expect the solution of these equations to be
“replica symmetric”, i.e.
Qαβ = δαβ(1 −q) + q.
(13.90)

13.4 Spin-Glasses, Replicas and Low-Rank HCIZ
217
This matrix has two eigenvalues, one non-degenerate equal to 1 + (n −1)q and another
(n −1)-fold degenerate equal to 1 −q. Correspondingly, RJ(Q) has eigenvalues
RJ((1 + (n −1)q)/T ) and RJ((1 −q)/T ), from which we reconstruct the diagonal
and off-diagonal elements of RJ(Q):
r : = [RJ(Q)]αβ = 1
n

RJ((1 + (n −1)q)/T ) −RJ((1 −q)/T )

,
rd : = [RJ(Q)]αα = RJ((1 −q)/T ) + r.
(13.91)
Injecting in the deﬁnition of Z, we ﬁnd
Z =

S
exp
⎛
⎝1
2T
⎡
⎣nrd + r
n

αβ=1
SαSβ
⎤
⎦
⎞
⎠,
(13.92)
where we have used S2α ≡1. Writing n
αβ=1 SαSβ =
n
α Sα2 −n and using a
Hubbard–Stratonovich transformation, one gets
Z =

S
en rd −r
2T
 +∞
−∞
dx exp

−x2
2 + x
1 r
T
n

α
Sα

.
(13.93)
The sums of different Sα now again decouple, leading to
Z = en rd −r
2T +n log 2
 +∞
−∞
dx e−x2
2
√
2π
coshn
(
x
1 r
T
)
.
(13.94)
Now, one can notice that
n

αβ=1
⟨SαSβ⟩T = n(n −1)q = 2T ∂log Z
∂r
,
(13.95)
to get, in the limit n →0 and a few manipulations (including an integration by parts), an
equation involving only q:
q =
 +∞
−∞
dx
√
2π
e−x2
2 tanh2
(
x
1 r
T
)
,
(13.96)
where, in the limit n →0,
r = q
T R′
J
1 −q
T

.
(13.97)
Clearly, q = 0 is always a solution of this equation. The physical interpretation of q is
the following: choose randomly two microscopic conﬁgurations of spins {Sα
i } and {Sβ
i },
each with a weight given by exp
 H({S})
T

/Z. Then, the average overlap between these
conﬁgurations, N
i=1 Sα
i Sβ
i /N, is equal to q. When q = 0, these two conﬁgurations are
thus uncorrelated. One expects this to be the case at high enough temperature, where the
system explores randomly all conﬁgurations.
When the spins start freezing, on the other hand, one expects the system to strongly
favor some (amorphous) conﬁgurations over others. Hence, one expects that q > 0

218
The Replica Method
in the spin-glass phase. Expanding the right hand side of Eq. (13.96) for small q
gives
 +∞
−∞
dx
√
2π
e−x2
2 tanh2
(
x
1 r
T
)
= q
R′
J(1/T )
T 2
−q2
⎛
⎝R′′
J(1/T )
T 3
+ 2

R′
J(1/T )
T 2
2⎞
⎠+ O(q3).
(13.98)
Assuming that the coefﬁcient in front of the q2 term is negative, we see that a non-zero q
solution appears continuously below a critical temperature Tc given by
T 2
c = R′
J
 1
Tc

.
(13.99)
When J is a Wigner matrix, the spin-glass model is the one studied originally by Sherring-
ton and Kirkpatrick in 1975. In this case, RJ(x) = x and therefore Tc = 1. There are cases,
however, where the transition is discontinuous, i.e. where the overlap q jumps from zero
for T > Tc to a non-zero value at Tc. In these cases, the small q expansion is unwarranted
and another method must be used to ﬁnd the critical temperature. One example is the
“random orthogonal model”, where the coupling matrix J has N/2 eigenvalues equal to
+1 and N/2 eigenvalues equal to −1.
The spin-glass phase T < Tc is much more complicated to analyze, because the replica
symmetric ansatz Qαβ = δαβ(1 −q) + q is no longer valid. One speaks about “replica
symmetry breaking”, which encodes an exquisitely subtle, hierarchical organization of the
phase space of these models. This is the physical content of the celebrated Parisi solution
of the SK model, but is much beyond the scope of the present book, and we encourage the
curious reader to learn more from the references given below.
Bibliographical Notes
• The replica trick was introduced by
– R. Brout. Statistical mechanical theory of a random ferromagnetic system. Physical
Review, 115:824–835, 1959,
and popularized in
– S. F. Edwards and P. W. Anderson. Theory of spin glasses. Journal of Physics F:
Metal Physics, 5(5):965, 1975,
for an authoritative book on the subject, see
– M. M´ezard, G. Parisi, and M. A. Virasoro. Spin Glass Theory and Beyond. World
Scientiﬁc, Singapore, 1987,
see also
– M. M´ezard and G. Parisi. Replica ﬁeld theory for random manifolds. Journal de
Physique I, France, 1(6):809–836, 1991.
• The replica method was ﬁrst applied to random matrices by
– S. F. Edwards and R. C. Jones. The eigenvalue spectrum of a large symmetric random
matrix. Journal of Physics A: Mathematical and General, 9(10):1595, 1976,
it is rarely discussed in random matrix theory books with the exception of

13.4 Spin-Glasses, Replicas and Low-Rank HCIZ
219
– G. Livan, M. Novaes, and P. Vivo. Introduction to Random Matrices: Theory and
Practice. Springer, New York, 2018.
• For a replica computation for the average resolvent for free addition and free product,
see
– J. Bun, R. Allez, J.-P. Bouchaud, and M. Potters. Rotational invariant estimator for
general noisy matrices. IEEE Transactions on Information Theory, 62:7475–7490,
2016,
– J. Bun, J.-P. Bouchaud, and M. Potters. Cleaning large correlation matrices: Tools
from random matrix theory. Physics Reports, 666:1–109, 2017.
• For Derrida’s random energy model where different moments of Z have different transi-
tion temperatures, see
– B. Derrida. Random-energy model: An exactly solvable model of disordered systems.
Physical Review B, 24:2613–2626, 1981.
• For an introduction to experimental spin-glasses, see
– K. H. Fischer and J. A. Hertz. Spin Glasses. Cambridge University Press, Cambridge,
1991,
– A. P. Young. Spin Glasses and Random Fields. World Scientiﬁc, Singapore, 1997,
see also M. M´ezard, G. Parisi, M. Virasoro, op. cit.
• For general orthogonal invariant spin-glasses, see
– R. Cherrier, D. S. Dean, and A. Lef`evre. Role of the interaction matrix in mean-ﬁeld
spin glass models. Physical Review E, 67:046112, 2003.

14
Edge Eigenvalues and Outliers
In many instances, the eigenvalue spectrum of large random matrices is conﬁned to a
single interval of ﬁnite size. This is of course the case for Wigner matrices, where the
correctly normalized eigenvalues fall between λ−= −2 and λ+ = +2, with a semi-
circular distribution between the two edges, and, correspondingly, a square-root singularity
of the density of eigenvalues close to the edges. This is also the case for Wishart matrices,
for which again the density of eigenvalues has square-root singularities close to both edges.
As discussed in Section 5.3.2, this is a generic property, with a few notable exceptions. One
example is provided by Wishart matrices with parameter q = 1, for which the eigenvalue
spectrum extends down to λ = 0 with an inverse square-root singularity there. Another
case is that of Wigner matrices constrained to have all eigenvalues positive: the spectrum
also has an inverse square-root singularity – see Eq. (5.94). One speaks of a “hard edge” in
that case, because the minimum eigenvalue is imposed by a strict constraint. The Wigner
semi-circle edge at λ+ = 2, on the other hand, is “soft” and appears naturally as a result
of the minimization of the energy of a collection of interacting Coulomb charges in an
external potential.1
Consider for example Wigner matrices of size N. The existence of sharp edges delimit-
ing a region where one expects to see a non-zero density of eigenvalues from a region where
there should be none is only true in the asymptotically large size limit N →∞. For large
but ﬁnite N, on the other hand, one expects that the probability to ﬁnd an eigenvalue beyond
the Wigner sea is very small but non-zero. The width of the transition region, and the tail
of the density of states was investigated a while ago, culminating in the beautiful results by
Tracy and Widom on the distribution of the largest eigenvalue of a random matrix, which
we will describe in the next section. The most important result is that the width of the region
around λ+ within which one expects to observe the largest eigevalue of a Wigner matrix
goes down as N−2/3.
Hence the largest eigenvalue λmax does not ﬂuctuate very far away from the classical
edge λ+. Take for example N = 1000; λmax is within 1000−2/3 = 0.01 away from λ+ = 2.
In real applications the largest eigenvalue can deviate quite substantially from the classical
1 Note that there are also cases where the soft edge has a different singularity, see Section 5.3.3, and cases where the eigenvalue
spectrum extends up to inﬁnity, for example “L´evy matrices” with iid elements of inﬁnite variance.
220

14.1 The Tracy–Widom Regime
221
edge. The origin of such a large eigenvalue is usually not an improbably large Tracy–
Widom ﬂuctuation but rather a true outlier that should be modeled separately. This is the
goal of the present chapter. We will see in particular that perturbing a Wigner (or Wishart)
matrix with a deterministic, low-rank matrix of sufﬁcient amplitude a > ac generates
“true” outliers, which remain at a distance O(a) from the upper edge. For a < ac on the
other hand, the largest eigenvalue remains at distance N−2/3 from λ+.
14.1 The Tracy–Widom Regime
The Tracy–Widom result characterizes precisely the distance between the largest eigen-
value λmax of Gaussian Wigner or Wishart matrices and the upper edge of the spectrum
which we denoted by λ+. This result can be (formally) stated as follows: the rescaled
distribution of λmax −λ+ converges, for N →∞, towards the Tracy–Widom distribution,
usually noted F1:
P

λmax ≤λ+ + γ N−2/3u

= F1(u),
(14.1)
where γ is a constant that depends on the problem and F1(u) is the β = 1 Tracy–Widom
distribution. For the Wigner problem, λ+ = 2 and γ = 1, whereas for Wishart matrices,
λ+ = (1 + √q)2 and γ = √qλ2/3
+ . In fact, Eq. (14.1) holds for a much wider large class
of N × N random matrices, for example symmetric random matrices with arbitrary iid
elements with a ﬁnite fourth moment. The Tracy–Widom distribution for all three values
of β is plotted in Figure 14.1. (The case where the fourth moment is inﬁnite is discussed in
Section 14.3 below.)
−4
−2
0
2
u
0.0
0.1
0.2
0.3
0.4
0.5
fb(u)
f4(u)
f2(u)
f1(u)
Figure 14.1 Rescaled and shifted probability density of the largest eigenvalue for a large class
of random matrices such as Wigner and Wishart: the Tracy–Widom distribution. The distribution
depends on the Dyson index (β) and is shown here for β = 1,2 and 4.

222
Edge Eigenvalues and Outliers
Everything is known about the Tracy–Widom density f1(u) = F ′
1(u), in particular its
left and right far tails:
ln f1(u) ∝−u3/2,
(u →+∞);
ln f1(u) ∝−|u|3,
(u →−∞).
(14.2)
One notices that the left tail is much thinner than the right tail: pushing the largest eigen-
value inside the Wigner sea implies compressing the whole Coulomb gas of repulsive
charges, which is more difﬁcult than pulling one eigenvalue away from λ+. Using this
analogy and the formalism of Section 5.4.2, the large deviation regime of the Tracy–Widom
problem (i.e. for λmax −λ+ = O(1)) can be obtained. Note that the result is exponentially
small in N as the u3/2 behavior for u →∞combines with N2/3 to give a linear in N
dependence.
The distribution of the smallest eigenvalue λmin around the lower edge λ−is also Tracy–
Widom, except in the particular case of Wishart matrices with q = 1. In this case λ−= 0,
which is a “hard” edge since all eigenvalues of the empirical matrix must be non-negative.2
The behavior of the width of the transition region can be understood using a simple
heuristic argument. Suppose that the N = ∞density goes to zero near the upper edge λ+
as (λ+ −λ)θ (generically, θ = 1/2 as is the case for the Wigner and the Marˇcenko–Pastur
distributions). For ﬁnite N, one expects not to be able to say whether the density is zero or
non-zero when the probability to observe an eigenvalue is of order 1/N, i.e. when the O(1)
eigenvalue is within the “blurred” region. This leads to a blurred region of width
|λ∗−λ+|θ+1 ∝1
N →λ∗∼N−
1
1+θ ,
(14.3)
which goes to zero as N−2/3 in the generic square-root case θ = 1/2. More precisely, for
Gaussian ensembles, the average density of states at a distance ∼N−2/3 from the edge
behaves as
ρN(λ ≈λ+) = N−1/31
&
N2/3(λ −λ+)
'
,
(14.4)
with 1(u →−∞) ≈√−u/π so as to recover the asymptotic square-root singularity,
since the N dependence disappears in that limit. Far from the edge, ln 1(u →+∞) ∝
−u3/2, showing that the probability to ﬁnd an eigenvalue outside of the allowed band
decays exponentially with N and super-exponentially with the distance to the edge. The
function 1(u) is not known analytically for real Wigner matrices (β = 1) but an explicit
expression is available for complex Hermitian Wigner matrices, and reads (Fig. 14.2)
2(u) = Ai′2(u) −u Ai2(u),
(14.5)
with the same asymptotic behaviors as 1(u). (Ai(u) is the standard Airy function.)
2 This special case is treated in P´ech´e [2003].

14.2 Additive Low-Rank Perturbations
223
−7.5
−5.0
−2.5
0.0
u
0.0
0.2
0.4
0.6
0.8
1.0
F2(u)
f2(u)
−u/ p
−2.5
0.0
2.5
5.0
u
10−14
10−11
10−8
10−5
10−2
F2(u)
f2(u)
exp(−4u3/ 2/ 3)/ (8pu)
Figure 14.2 Behavior of the density near the edge λ+ at the scale N−2/3 for complex Hermitian
Wigner matrices given by Eq. (14.5). For comparison the probability of the largest eigenvalue
f2(u) is also shown. For positive values, the two functions are almost identical and behave as
exp(−4u3/2/3)/(8πu) for large u (right). For negative arguments the functions are completely
different: 2(u) behaves as √−u/π for large negative u while f2(u) →0 as the largest eigenvalue
cannot be in the bulk (left).
14.2 Additive Low-Rank Perturbations
14.2.1 Eigenvalues
We will now study the outliers for an additive perturbation to a large random matrix. Take a
large symmetric random matrix M (e.g. Wigner or Wishart) with a well-behaved asymptotic
spectrum that has a deterministic right edge λ+. We would like to know what happens when
one adds to M a low-rank (deterministic) perturbation. For simplicity, we only consider the
rank-1 perturbation auuT with ∥u∥= 1 and a of order 1, but the results below easily
generalize to the case of a rank-n perturbation with n ≪N.
We want to know whether there will be an isolated eigenvalue of M + auuT outside the
spectrum of M (i.e. an “outlier”) or not. To answer this question, we calculate the matrix
resolvent
Ga(z) =

z −M −auuT −1 .
(14.6)
The matrix Ga(z) has a pole at every eigenvalue of M + auuT . An alternative approach
would have been to study the zeros of the function det

z −M −auuT 
, but the full resol-
vent Ga(z) also gives us information about the eigenvectors.
Now we apply the Sherman–Morrison formula (1.28); taking A = z −M, we get
Ga(z) = G(z) + a G(z)uuT G(z)
1 −auT G(z)u,
(14.7)

224
Edge Eigenvalues and Outliers
where G(z) is the resolvent of the original matrix M. We are looking for a real eigenvalue
such that λ1 > λ+. Let us take z = λ1 ∈R outside the spectrum of M, so G(λ) is real and
regular. To have an outlier at λ1, we need a pole of Ga at λ1, i.e. the following equation
needs to be satisﬁed:
1 −auT G(λ1)u = 0.
(14.8)
Assume now that M is drawn from a rotationally invariant ensemble or, equivalently, that
the vector u is an independent random vector uniformly distributed on the unit sphere. In
the language of Chapter 11, we say that the perturbation auuT is free from the matrix M.
We then have, in the eigenbasis of G,
uT G(z)u =

i
u2
i Gii(z) ≈1
N Tr G(z) = gN(z)
N→∞
→
g(z).
(14.9)
Thus we have a pole when
ag(λ1) = 1 ⇒g(λ1) = 1/a.
(14.10)
If z(g), the inverse function of g(z), exists, we arrive at
λ1 = z
1
a

.
(14.11)
The condition for the invertibility of g(z) happens to be precisely the same as the condition
to have an outlier, i.e. λ1 > λ+ – see Section 10.4. We have established there that λ1 =
z(1/a) is monotonically increasing in a, and λ1 = λ+ when a = a∗= 1/g(λ+), which
is the critical value of a for which an outlier ﬁrst appears. Generically, g+ = g(λ+) is a
minimum of z(g):
dz(g)
dg

g+
= 0
when
z(g+) = λ+.
(14.12)
For instance, for Wigner matrices, we have z(g) = σ 2g + g−1, for which
σ 2 −g−2
+ = 0 ⇒g+ = σ −1,
(14.13)
and λ+ = z(σ −1) = 2σ, which is indeed the right edge of the semi-circle law.
In sum, for a > a∗= 1/g+, there exists a unique outlier eigenvalue that is increasing
with a. The smallest value for which we can have an outlier is a∗= 1/g+, corresponding
to λ1 = λ+. For a < a∗there is no outlier to the right of λ+.3
Using the relation between the inverse function z(g) and the R-transform (10.10), we
can express the position of the outlier as
λ1 = R
1
a

+ a
for
a > a∗= 1
g+
.
(14.14)
3 Outliers such that λ < λ−behave similarly, we just need to consider the matrix −M −auuT and follow the same logic.

14.2 Additive Low-Rank Perturbations
225
0
1
2
3
4
a
2.0
2.5
3.0
3.5
4.0
l1
simulation
l1 = ˜a+ 1/ ˜a; ˜a = max(a,1)
Figure 14.3 Largest eigenvalue of a Gaussian Wigner matrix with σ 2 = 1 with a rank-1 perturbation
of magnitude a. Each dot is the largest eigenvalue of a single random matrix with N = 200. Equation
(14.16) is plotted as the solid curve. For a < 1, the ﬂuctuations follow a Tracy–Widom law with
N−2/3 scaling, while for a > 1 the ﬂuctuations are Gaussian with N−1/2 scaling. From the graph,
we see ﬂuctuations that are indeed smaller when a < 1. They also have a negative mean and positive
skewness, in agreement with the Tracy–Widom distribution.
Using the cumulant expansion of the R-transform (11.63), we then get a general expression
for large a:
λ1 = a + τ(M) + κ2(M)
a
+ O(a−2).
(14.15)
For Wigner matrices, we actually have for all a (see Fig. 14.3)
λ1 = a + σ 2
a
for
a > a∗= σ.
(14.16)
When a →a∗, on the other hand, one has
dλ1(a)
da

a∗= dz(g)
dg

g+=1/a∗= 0.
(14.17)
Hence, one has, for a →a∗and for generic square-root singularities,
λ1 = λ+ + C(a −a∗)2 + O

(a −a∗)3
,
(14.18)
where C is some problem dependent coefﬁcient.
By studying the ﬂuctuations of uG(λ)uT around g(λ), one can show that the ﬂuctuations
of the outlier around λ1 = R(a−1) + a are Gaussian and of order N−1/2. This is to
be contrasted with the ﬂuctuations of the largest eigenvalue when there are no outliers
(a < g+), which are Tracy–Widom and of order N−2/3. The transition between the two
regimes is called the Baik–Ben Arous–P´ech´e (bbp) transition.

226
Edge Eigenvalues and Outliers
0.5
1.0
1.5
2.0
2.5
3.0
g
2.0
2.5
3.0
3.5
4.0
(g)
g+ 1/ g
g+
Figure 14.4 Plot of the inverse function z(g) = g + 1/g for the unit Wigner function g(z). The gray
dot indicates the point (g+,λ+). The line to the left of this point is the true inverse of g(z): z(g) is
deﬁned on [0,g+) and is monotonously decreasing in g. The line to the right is a spurious solution
introduced by the R-transform. Note that the point g = g+ is a minimum of z(g) = g + 1/g.
We ﬁnish this section with two remarks.
• One concerns the solutions to Eq. (14.14), and the way to ﬁnd the value a∗beyond
which an outlier appears. The point is that, while the function R(g = 1/a) is well
deﬁned for g ∈[0,g+), it also often makes sense even beyond g+ (see the discussion in
Section 10.4). In that case, one will ﬁnd spurious solutions: Figure 14.4 shows a plot of
z(g) = R(g) + 1/g in the unit Wigner case, which is still well deﬁned for g > g+ = 1
even if this function is no longer the inverse of g(z) (Section 10.4). There are two
solutions to z(g) = λ1, one such that g < g+ and the other such that g > g+. As
noted above, the point g+ is a minimum of z(g), beyond which the relation between λ1
and a is monotonically increasing because g(z) is monotonically decreasing for z > λ+.
• The second concerns the case of a free rank-n perturbation, when n ≪N. In this case
one cannot use the Sherman–Morrison formula but one can compute the R-transform
of the perturbed matrix, and infer the 1/N correction to the Stieltjes transform. The
poles of this correction term give the possible outliers. To each eigenvalue ak (k =
1, . . . ,n) of the perturbation, one can associate a candidate outlier λk given by
λk = R
 1
ak

+ ak
when
ak > 1
g+
.
(14.19)
14.2.2 Outlier Eigenvectors
The matrix resolvent in Eq. (14.7) can also tell us about the eigenvectors of the perturbed
matrix. We expect that, for a very strong rank-1 perturbation auuT , the eigenvector v1

14.2 Additive Low-Rank Perturbations
227
associated with the outlier λ1 will be very close to the perturbation vector u. On the other
hand, for λ1 ≈λ+, the vector u will strongly mix with bulk eigenvectors of M so the
eigenvector v1 will not contain much information about u.
To understand this phenomenon quantitatively, we will study the squared overlap |vT
1u|2.
With the spectral decomposition of M + auuT , we can write
Ga(z) =
N

i=1
vivT
i
z −λi
,
(14.20)
where λ1 denotes the outlier and v1 its eigenvector, and λi,vi, i > 1 all other eigenval-
ues/eigenvectors. Thus we have
lim
z→λ1
uT Ga(z)u · (z −λ1) = |vT
1u|2.
(14.21)
Hence, by (14.7) and (14.9), we get
|vT
1u|2 = lim
z→λ1

g(z) + a
g(z)2
1 −ag(z)

(z −λ1)
= lim
z→λ1
g(z) z −λ1
1 −ag(z).
(14.22)
We cannot simply evaluate the fraction above at z = λ1, for at that point g(λ1) = a−1 and
we would get 0/0. We can however use l’Hospital’s rule4 and ﬁnd
|vT
1u|2 = −g(λ1)2
g′(λ1) ,
(14.24)
where we have used a−1 = g(λ1). The right hand side is always positive since g is a
decreasing function for λ > λ+.
We can rewrite Eq. (14.24) in terms of the R-transform and get a more useful formula.
To compute g′(z), we take the derivative with respect to z of the implicit equation z =
R(g(z)) + g−1(z) and get
1 = R′(g(z))g′(z) −g′(z)
g2
⇒g′(z) =
1
R′(g(z)) −g−2(z).
(14.25)
Hence we have
|vT
1u|2 = 1 −g(λ1)2R′(g(λ1))
= 1 −a−2R′(a−1).
(14.26)
We can now check our intuition about the overlap for large and small perturbations. For a
large perturbation a →∞, Eq. (14.26) gives
4 L’Hospital’s rule states that
lim
x→x0
f (x)
g(x) = f ′(x0)
g′(x0) when f (x0) = g(x0) = 0.
(14.23)

228
Edge Eigenvalues and Outliers
0
1
2
3
4
a
0.0
0.2
0.4
0.6
0.8
1.0
|vT
1u|2
simulation
|vT
1u|2 = max(1−a−2,0)
Figure 14.5 Overlap between the largest eigenvector and the perturbation vector of a Gaussian
Wigner matrix with σ 2 = 1 with a rank-1 perturbation of magnitude a. Each dot is the overlap
for a single random matrix with N = 200. Equation (14.31) is plotted as the solid curve.
|vT
1u|2 = 1 −κ2(M)
a2
+ O(a−3)
when
a →∞.
(14.27)
As expected, v1 →u when a →∞: the angle between the two vectors decreases as 1/a.
The overlap near the transition λ1 →λ+ can be analyzed as follows. The derivative of
g(z) can be written as
g′(z) = −
 λ+
λ−
ρ(x)
(z −x)2 dx.
(14.28)
For a density that vanishes at the edge as ρ(λ) ∼(λ+ −λ)θ with exponent θ between 0 and
1, we have that g(z) is ﬁnite at z = λ+ but g′(z) diverges at the same point, as |z −λ+|θ−1.
From Eq. (14.24), we have in that case5
|vT
1u|2 ∝(λ1 −λ+)1−θ
when
λ1 →λ+.
(14.29)
In the generic case, one has θ = 1/2 and, from Eq. (14.18), λ1−λ+ ∝(a−a∗)2, leading to
|vT
1u|2 ∝a −a∗
when
a →a∗.
(14.30)
These general results are nicely illustrated by Wigner matrices, for which R(x) = σ 2x.
The overlap is explicitly given by (see Fig. 14.5)
|vT
1u|2 = 1 −
a∗
a
2
when
a > a∗= σ.
(14.31)
5 In Chapter 5, we encountered a critical density where ρ(λ) behaves as (λ+ −λ)θ with an exponent θ = 3
2 > 1. In this case
g′(z) does not diverge as z →λ+ and the squared overlap at the edge of the bbp transition does not go to zero (ﬁrst order
transition). For example for the density given by Eq. (5.59) we ﬁnd |vT
1 u|2 = 4
9 at the edge λ1 = 2
√
2.

14.3 Fat Tails
229
As a →a∗, λ1 →2σ and |vT
1u|2 = 2(a −a∗)/a∗→0: the eigenvector becomes
delocalized as the eigenvalue merges with the bulk. For a < a∗, one can rigorously show
that there is no information left in the eigenvalues of the perturbed matrix that would allow
us to reconstruct u.
Note that for λ1 > λ+, |vT
1u|2 is of order unity. In Chapter 19 we will see that this is
not the case for the overlaps between perturbed and unperturbed eigenvectors in the bulk,
which have typical sizes of order N−1.
Exercise 14.2.1
Additive perturbation of a Wishart matrix
Deﬁne a modiﬁed Wishart matrix W1 such that every element (W1)ij =
Wij + a/N, where W is a standard Wishart matrix and a is a constant of order
1. W1 is a standard Wishart matrix plus a rank-1 perturbation W1 = W + auuT .
(a)
What is the normalized vector u in this case?
(b)
Using Eqs. (14.14) and (10.15) ﬁnd the value of the outlier and the minimal a
in the Wishart case.
(c)
The square-overlap between the vector u and the new eigenvector v1 is given
by Eq. (14.26). Give an explicit expression in the Wishart case.
(d)
Generate a large modiﬁed Wishart (q = 1/4, N = 1000) for a few a in the
range [1,5]. Compute the largest eigenvalue λ1 and associated eigenvector v1.
Plot λ1 and |vT
1u|2 as a function of a and compare with the predictions of (b)
and (c).
14.3 Fat Tails
The previous section allows us to discuss the very interesting situation of real symmetric
random matrices X with iid elements Xij that have a fat-tailed distribution, but with a ﬁnite
variance (the case of inﬁnite variance will be alluded to at the end of the section). In order
to have eigenvalues of order unity, the random elements must be of typical size N−1/2, so
we write
Xij = xij
√
N
,
(14.32)
with xij distributed according to some density P(x) of mean zero and variance unity, but
that decays as μ|x|−1−μ for large x. This means that most elements Xij are small, of
order N−1/2, with some exceptional elements that are of order unity. The probability that
|Xij| > 1 is actually given by
P(|Xij| > 1) ≈2
 ∞
√
N
dx
μ
x1+μ =
2
Nμ/2 .
(14.33)
Since there are in total N(N −1)/2 ≈N2/2 such random variables, the total number of
such variables that exceed unity is given by N2−μ/2. Hence, for μ > 4, this number tends

230
Edge Eigenvalues and Outliers
2
3
4
5
6
lmax
10−4
10−3
10−2
10−1
100
101
P(lmax)
Tracy-Widom
N = 1000
Figure 14.6 Distribution of the largest eigenvalue of N = 1000 Wigner matrices with elements
drawn from a distribution with μ = 5 compared with the prediction from the Tracy–Widom
distribution. Even though as N →∞this distribution converges to Tracy–Widom, at N = 1000
there is no agreement between the laws as the power law tail P(λmax > x) ∼x−5/
√
N still
dominates.
to zero with N: there are typically no such large elements in the considered random matrix.
Since each pair of large entries Xij = Xji can be considered as a rank-2 perturbation of a
matrix with all elements of order N−1/2, one concludes that for μ > 4 there are no outliers,
and the statistics of the largest eigenvalue is given by the Tracy–Widom distribution around
λ+ = 2. This hand-waving argument can actually be made rigorous: in the large N limit,
the ﬁniteness of the fourth moment of the distribution of matrix elements is sufﬁcient to
ensure that the largest eigenvalue is given by the Tracy–Widom distribution. However, one
should be careful in interpreting this result, because although very large elements appear
with vanishing probability, they still dominate the tail of the Tracy–Widom distribution for
ﬁnite N. The reason is that, whereas the former decreases as N2−μ/2, the latter decreases
much faster, as exp(−N(λmax −λ+)3/2) (see Fig. 14.6).
Now consider the case 2 < μ < 4. Since μ > 2, the variance of Xij is ﬁnite and
one knows that the asymptotic distribution of eigenvalues of X is given by the Wigner
semi-circle, with λ+ = 2. But now the number of large entries in the matrix X grows
with N as N2−μ/2, which is nevertheless still much smaller than N. Each large pair of
entries Xij = Xji = a larger than unity (in absolute value) contributes to two outliers,
given by λ = ±(a + 1/a). So there are in total O(N2−μ/2) outliers, the density of which
is given by
ρout(λ > 2) = N1−μ/2
 ∞
1
dx
μ
x1+μ δ

λ −x −1
x

.
(14.34)

14.4 Multiplicative Perturbation
231
This is a rather strange situation where the density of outliers goes to zero as N →∞as
soon as μ > 2, but at the same time the largest eigenvalue in this problem goes to inﬁnity as
λmax ∼N
2
μ −1
2,
2 < μ < 4.
(14.35)
Finally, let us brieﬂy comment on the case μ < 2, for which the variance of the entries
of X diverges (a case called “L´evy matrices” in the literature). For eigenvalues to remain
of order unity, one needs to scale the matrix elements differently, as
Xij = xij
N
1
μ
,
(14.36)
with
P(x)
∼
|x|→∞

(1 + μ) sin( πμ
2 )
|x|1+μ
,
(14.37)
where the funny factor involving the gamma function is introduced for convenience only.
The eigenvalue distribution is no longer given by a semi-circle. In fact, the support of the
distribution is in this case unbounded. For completeness, we give here the exact expression
of the distribution in terms of L´evy stable laws LC,β
μ
(u), where β is called the asymmetry
parameter and C the scale parameter.6 For a given value of λ, one should ﬁrst solve the
following two self-consistent equations for C and β:
C =
 +∞
−∞
dg|g|μ/2−2LC,β
μ/2(λ −1/g),
Cβ =
 +∞
−∞
dg sign(g)|g|μ/2−2LC,β
μ/2(λ −1/g).
(14.38)
Finally, the distribution of eigenvalues ρL(λ) of L´evy matrices is obtained as
ρL(λ) = LC(λ),β(λ)
μ/2
(λ).
(14.39)
One can check in particular that this distribution decays for large λ exactly as P(x)
itself. In other words, the tail of the eigenvalue distribution is the same as the tail of the
independent entries of the L´evy matrix.
14.4 Multiplicative Perturbation
In data-analysis applications, we often need to understand the largest eigenvalue of a sample
covariance matrix. A true covariance with a few isolated eigenvalues can be treated as a
matrix C0 with no isolated eigenvalue plus a low-rank perturbation. The passage from the
true covariance to the sample covariance is equivalent to the free product of the true covari-
ance with a white Wishart matrix with appropriate aspect ratio q = N/T . To understand
such matrices, we will now study outliers for a multiplicative process.
Consider the free product of a certain covariance matrix C0 with a rank-1 perturbation
and another matrix B:
E = B
1
2 C
1
2
0

1 + auuT 
C
1
2
0 B
1
2,
(14.40)
6 More precisely, LC,β
μ
(x) is the Fourier transform of exp(−C|k|μ(1 + iβsign(k) tan(πμ/2))) for μ  1, and of
exp(−C|k|(1 + i(2β/π)sign(k) log |k|)) for μ = 1.

232
Edge Eigenvalues and Outliers
where u is a normalized eigenvector of C0 with eigenvalue λ0, and B is positive semi-
deﬁnite, free from C0, and with τ(B) = 1. In the special case where B is a white Wishart,
our problem corresponds to a noisy observation of a perturbed covariance matrix, where
one of the modes (the one corresponding to u) has a variance boosted by a factor (1 + a).
The matrix E0 := B
1
2 C0B
1
2 has an unperturbed spectrum with a lower edge λ−and an
upper edge λ+. We want to establish, as in the additive case, the condition for the existence
of an outlier λ1 > λ+ or λ1 < λ−, and the exact position of the outlier when it exists.
The eigenvalues of E are the zeros of its characteristic polynomial, in particular for the
largest eigenvalue λ1 we have
det(λ11 −E0 −aB
1
2 C0uuT B
1
2 ) = 0.
(14.41)
We are looking for an eigenvalue outside the spectrum of E0, i.e. λ1 > λ+ or λ1 < λ−. For
such a λ1, the matrix λ11 −E0 is invertible and we can use the matrix determinant lemma
Eq. (1.30):
det(A + uvT ) = det A ·

1 + vT A−1u

,
(14.42)
with A = λ11 −E0 and u = −v = √aB
1
2 C
1
2
0 u. Equation (14.41) becomes
det(λ11 −E0) ·

1 −auT C
1
2
0 B
1
2 G0(λ1)B
1
2 C
1
2
0 u

= 0,
(14.43)
where we have introduced the matrix resolvent G0(λ1) := (λ11 −E0)−1. As we said, the
matrix λ11−E0 is invertible so its determinant is non-zero. Thus any outlier needs to solve
aλ0uT B
1
2 G0(λ1)B
1
2 u = 1.
(14.44)
Again we assume that B is a rotationally invariant matrix with respect to C0. Then we
know that in the large N limit G0(z) is diagonal in the basis of B and reads (see, mutatis
mutandis, Eq. (13.47))
G0(z) ≈S∗(z)GB(zS∗(z)),
S∗(z) := SC0(zg0(z) −1).
(14.45)
Furthermore, since u and B are also free,
uT B
1
2 G0(λ1)B
1
2 u ≈N−1 Tr

B
1
2 G0(λ1)B
1
2

≡S∗(λ1)tB(λ1S∗(λ1)),
(14.46)
where we have recognized the T-transform of the matrix B:
tB(z) := τ

B(z −B)−1
.
(14.47)
Thus, the position of the outlier λ1 is given by the solution of
aλ0S∗(λ1)tB(λ1S∗(λ1)) = 1.
(14.48)
In order to keep the calculation simple, we now assume that C0 = 1. In this case, S∗= 1
and λ0 = 1, so the equation simpliﬁes to

14.4 Multiplicative Perturbation
233
atB(λ1) = 1.
(14.49)
To know whether this equation has a solution we need to know if tB(ζ) is invertible. The
argument is very similar to the one for g(z) in the additive case. In the large N limit, tB(ζ)
converges to
tB(ζ) =
 λ+
λ−
ρB(x)x
ζ −x dx.
(14.50)
So tB(ζ) is monotonically decreasing for ζ > λ+ and is therefore invertible. We then have
λ1 = ζ(a−1)
when
λ1 > λ+,
(14.51)
where we use the notation ζ(t) for the inverse of the T-transform of B, in the region where
it is invertible.
The inverse function ζ(t) can be expressed in terms of the S-transform via Eq. (11.92).
We get
λ1 = ζ(a−1) =
a + 1
SB(a−1)
when
a >
1
tB(λ+).
(14.52)
Applying the theory to a Wishart matrix B = W with
SW(x) =
1
1 + qx,
λ± = (1 ± √q)2,
(14.53)
one ﬁnds that an outlier appears to the right of λ+ for a > √q, with
λ1 = (a + 1)

1 + q
a

.
(14.54)
For large a, we have λ1 ≈a + 1 + q, i.e. a large eigenvalue a + 1 in the covariance matrix
C will appear shifted by q in the eigenvalues of the sample covariance matrix.
Nothing prevents us from considering negative values of a, such that a > −1 to preserve
the positive deﬁnite nature of C. In this case, an outlier appears to the left of λ−when
a < −√q. Its position is given by the very same equation (14.54) as above.
Exercise 14.4.1
Transpose version of multiplicative perturbation
Consider a positive deﬁnite rotationally invariant random matrix B and a
normalized vector u. In this exercise, we will show that the matrix F deﬁned by
F = (1 + cuuT )B(1 + cuuT ),
(14.55)
with c > 0 sufﬁciently large, has an outlier λ1 given by Eq. (14.52) with b +1 =
(c + 1)2.
(a)
Show that for two positive deﬁnite matrices A and B, B
1
2 AB
1
2 has the same
eigenvalues as A
1
2 BA
1
2 .

234
Edge Eigenvalues and Outliers
(b)
Show that for a normalized vector u

1 + (a −1)uuT  1
2 = 1 + ( √a −1)uuT .
(14.56)
(c)
Finish the proof of the above statement.
Exercise 14.4.2
Multiplicative perturbation of an inverse-Wishart matrix
We will see in Section 15.2.3 that the inverse-Wishart matrix
W
p is deﬁned as
W
p = (1 −q)W−1
q ,
(14.57)
where Wq is a Wishart matrix with parameter q and p the variance of the inverse-
Wishart is given by p =
q
1−q . The S-transform of
W
p is given by
S
W
p(t) = 1 −pt.
(14.58)
Consider the diagonal matrix D with D11 = d and all other diagonal entries
equal to 1.
(a)
D can be written as 1 + cuuT . What is the normalized vector u and the
constant c?
(b)
Using the result from Exercise 14.4.1, ﬁnd the value of the largest eigenvalue
of the matrix D
W
pD as a function of d. Note that your expression will only
be valid for sufﬁciently large d.
(c)
Numerically generate matrices
W
p with N = 1000 and p = 1/2 (q = 1/3).
Find the largest eigenvalue of D
W
pD for various values of d and make a plot
of λ1 vs d. Superimpose your analytical result.
(d)
(Harder) Find analytically the minimum value of d to have an outlier λ1.
14.5 Phase Retrieval and Outliers
Optical detection devices like CCD cameras or photosensitive ﬁlms measure the photon
ﬂux but are blind to the phase of the incoming light. More generally, it is often the case
that one can only measure the power spectral density of a signal, which is the magnitude
of its Fourier transform. Can one recover the full signal based on this partial information?
This problem is called phase retrieval and can be framed mathematically as follows. Let an
unknown vector x ∈RN be “probed” with T vectors ak, in the sense that the measurement
apparatus gives us yk = |aT
k x|2 with k = 1, . . . ,T .7 Vectors x and ak are taken to be real
but they can easily be made complex.
The phase retrieval problem is
ˆx = argmin
x
⎛
⎝
k

aT
k x
2 −yk

2
⎞
⎠.
(14.59)
7 We could consider that there is some additional noise in the measurement of yk but for simplicity we keep here with the
noiseless version.

14.5 Phase Retrieval and Outliers
235
It is a difﬁcult non-convex optimization problem with many local minima. To efﬁciently
ﬁnd an acceptable solution, we need a starting point x0 that somehow points in the direc-
tion of the true solution x. The problem is that in large dimensions the probability that a
random vector x0 has an overlap |xT x0| > ε is exponentially small in N as soon as ε > 0.
We will explore here a technique that allows one to ﬁnd a vector x0 with non-vanishing
overlap with the true x.
The idea is to build some sort of weighted Wishart matrix such that this matrix will have
an outlier with non-zero overlap with the unknown true vector x. Consider the following
matrix:
M = 1
T
T

k=1
f (yk) akaT
k,
(14.60)
where the T vectors ak are of size N and f (y) is a function that we will choose later.
The function f (y) should be bounded above, otherwise we might have outliers domi-
nated by a few large values of f (yk). One such function that we will study is the sim-
ple threshold f (y) := (y −1). In large dimensions the results should not depend
on the precise statistics of the vectors ak provided they are sufﬁciently random. Here
we will assume that all their components are standard iid Gaussian. This assumption
makes the problem invariant by rotation. Without loss of generality, we can assume the
true vector x is in the canonical direction e1. The weights f (yk) are therefore assumed
to be correlated to |[ak]1|2 = |aT
k e1|2 and independent of all other components of the
vectors ak.
Given that the ﬁrst row and column of M contain the element [ak]1, we write the matrix
in block form as in Section 1.2.5:
M =

M11
M12
M21
M22

,
(14.61)
with the (11) block of size 1 × 1 and the (22) block of size (N −1) × (N −1). To ﬁnd a
potential outlier, we look for the zeros of the Stieltjes transform gM(z) = τ((z1−M)−1).
Combining Eqs. (1.32) and (1.33),
NgM(z) = Tr G22(z) + 1 + Tr

G22(z)M21M12G22(z)

z −M11 −M12G22(z)M21
,
(14.62)
where G22(z) is the matrix resolvent of the rotationally invariant matrix M22 (i.e. the
matrix M without its ﬁrst row and column). In the large N limit we expect M22 to
have a continuous spectrum with an edge λ+. When the condition for the denominator
to vanish, i.e.
λ1 −M11 −M12G22(λ1)M21 = 0,
(14.63)
has a solution for λ1 > λ+ we can say that the matrix M has an outlier. The overlap
between the corresponding eigenvector v1 and x is given by the residue
ϱ :=
vT
1x
2
|x|2
=
vT
1e1
2 = lim
z→λ1
z −λ1
z −M11 −M12G22(z)M21
.
(14.64)
In the large N limit the scalar equation (14.63) becomes self-averaging. We have
M11 = 1
T
T

k=1
f (yk)([ak]1)2 T →∞
→
E
&
f (y)([a]1)2'
.
(14.65)

236
Edge Eigenvalues and Outliers
For the second term we have
M12G22(z)M21 =
T

k,ℓ=1
1
T 2 f (yk)f (yℓ)[ak]1[aℓ]1
N

i,j>1
[ak]i[G22(z)]ij[aℓ]j
T →∞
→
qE
&
f 2(y)([a]1)2'
h(z),
(14.66)
where q = N/T and
h(z) = τ
HHT
T
G22(z)

,
[H]ik = [ak]i
i > 1.
(14.67)
We can now put everything together and use l’Hospital’s rule to compute the residue. For
convenience we deﬁne the constants cn := E
&
f n(y)([a]1)2'
. There will be an outlier
with overlap
ϱ =
1
1 −qc2h′(λ1)
(14.68)
when there is a solution λ1 > λ+ of
λ1 = c1 + qc2h(λ1).
(14.69)
We will come back later to the computation of h(z). In the q →0 limit the matrix M
becomes proportional to the identity M = E[f (y)]1 := m11, so gM(z) = 1/(z−m1) and
h(z) = 1/(z −m1). For q = 0 we have a solution λ1 = c1 which satisﬁes c1 ≥m1. In
this limit the overlap tends to one. The linear correction in q is easily obtained as we only
need h(z) to order zero. We obtain
λ1 = c1 + q
c2
c1 −m1
+ O(q2),
ϱ = 1 −q
c2
(c1 −m1)2 + O(q2).
(14.70)
Note that c2/(c1 −m1)2 is always positive so the overlap decreases with q starting from
ϱ = 1 at q = 0. For the unit thresholding function f (y) = (y −1) we have m1 =
erfc(1/
√
2) ≈0.317 and c1 = c2 = m1 + √2/(eπ) ≈0.801 (see Fig. 14.7).
Since we have the freedom to choose any bounded function f (y) we should choose
the one that gives the largest overlap for the value of q given by our dataset. We will do an
easier computation, namely minimize the slope of the linear approximation in q. We want
fopt(y) = argmin
f (y)
c2
(c1 −m1)2 = argmin
f (y)
Ea
&
f 2(a2)a2'
Ea

f (a2)(a2 −1)
2,
(14.71)
where the law of a is N(0,1). A variational minimization gives
fopt(y) = 1 −1
y .
(14.72)
The optimal function is not bounded below and therefore the distribution of eigenvalues is
singular with c2 →∞and m1 →−∞. One should think of this function as the limit of a
series of functions such that c2/(c1 −m1)2 →0. In Figure 14.7 we see that numerically
this function has indeed an overlap as a function of q with zero slope at the origin. As a
consequence it has non-zero overlap for much greater values of q (fewer data T ) than the
simple thresholding function. It turns out that our small q optimum f (y) = 1 −1/y is
actually the optimal function for all values of q.

14.5 Phase Retrieval and Outliers
237
0.0
0.5
1.0
1.5
2.0
q
0.0
0.2
0.4
0.6
0.8
1.0
f(y) = 1−1/ y
f(y) = Q(y−1)
linear approx.
Figure 14.7 Overlap ϱ :=
vT
1x
2 /|x|2 between the largest eigenvector and the true signal as a
function of q := N/T for two functions: the simple f (y) = (y −1) and the optimal f (y) =
1 −1/y. Each dot corresponds to a single matrix of aspect ratio q and NT = 106. The solid line
corresponds to the linear q approximation Eq. (14.70) in the thresholding case. For the optimal case,
the slope at the origin is zero.
For completeness we show here how to compute the function h(z). We have the fol-
lowing subordination relation (for the (22) block of the relevant matrices):
tM(ζ) = tWq

Sf (qtM(ζ))ζ

,
(14.73)
where Sf (t) is the S-transform of a diagonal matrix with entries f (yk). We then have
h(z) = τ(WqG22(z)) = Sτ
&
(Sz1 −M)−1 (M −Sz + Sz)
'
= S(zgM(z) −1),
(14.74)
with S := Sf (q(zgM(z) −1)). Since
SM(t) = Sf (qt)
1 + qt = t + 1
tζ ,
(14.75)
we have
h(z) = gM(z)[1 + q(zgM(z) −1)],
(14.76)
where the function gM(z) can be obtained by inverting the relation
z(g) =
 ∞
−∞
dx
√
2π
f (x2)e−x2/2
1 −qgf (x2) + 1
g .
(14.77)
Bibliographical Notes
• For a recent review on the subject of free low-rank additive or multiplicative perturba-
tions, the interested reader should consult

238
Edge Eigenvalues and Outliers
– M. Capitaine and C. Donati-Martin. Spectrum of deformed random matrices and free
probability. preprint arXiv:1607.05560, 2016.
• Equation (14.16) was ﬁrst published by both physicists and mathematicians, respectively,
in
– S. F. Edwards and R. C. Jones. The eigenvalue spectrum of a large symmetric random
matrix. Journal of Physics A: Mathematical and General, 9(10):1595, 1976,
– Z. F¨uredi and J. Koml´os. The eigenvalues of random symmetric matrices. Combina-
torica, 1(3):233–241, 1981.
• In 2005, Baik, Ben Arous and P´ech´e studied the transition in the statistics of the largest
eigenvector in the presence of a low-rank perturbation. The transition from no outlier to
one outlier is now called the bbp (Baik–Ben Arous–P´ech´e) transition; see
– J. Baik, G. Ben Arous, and S. P´ech´e. Phase transition of the largest eigenvalue for
nonnull complex sample covariance matrices. Annals of Probability, 33(5):1643–
1697, 2005,
– D. F´eral and S. P´ech´e. The largest eigenvalue of rank one deformation of large
Wigner matrices. Communications in Mathematical Physics, 272(1):185–228, 2007.
• The case of outliers with a hard edge is treated in
– S. P´ech´e. Universality of local eigenvalue statistics for random sample covariance
matrices. PhD thesis, EPFL, 2003.
• The generalization to spiked tensors has recently been studied in
– G. Ben Arous, S. Mei, A. Montanari, and M. Nica. The landscape of the spiked ten-
sor model. Communications on Pure and Applied Mathematics, 72(11):2282–2330,
2019.
• The case of additive or multiplicative perturbation to a general matrix (beyond Wigner
and Wishart) was worked out by
– F. Benaych-Georges and R. R. Nadakuditi. The eigenvalues and eigenvectors of
ﬁnite, low rank perturbations of large random matrices. Advances in Mathematics,
227(1):494–521, 2011.
• For studies about the edge properties of Wigner matrices, see e.g.
– M. J. Bowick and E. Brzin. Universal scaling of the tail of the density of eigenvalues
in random matrix models. Physics Letters B, 268(1):21–28, 1991,
and for the speciﬁc problem of the largest eigenvalue, the seminal paper of
– C. A. Tracy and H. Widom. Level-spacing distributions and the Airy kernel. Com-
munications in Mathematical Physics, 159(1):151–174, 1994,
which has led to a ﬂurry of activity, see e.g.
– S. Majumdar. Random matrices, the Ulam problem, directed polymers and growth
models, and sequence matching. In Les Houches Lecture Notes “Complex Systems”,
volume 85. Elsevier Science, 2007.
• When the elements of the random matrix are fat tailed, the Tracy–Widom law is modi-
ﬁed, see
– G. Biroli, J.-P. Bouchaud, and M. Potters. On the top eigenvalue of heavy-tailed
random matrices. Europhysics Letters (EPL), 78(1):10001, 2007,

14.5 Phase Retrieval and Outliers
239
– A. Aufﬁnger, G. Ben Arous, and S. P´ech´e. Poisson convergence for the largest eigen-
values of heavy tailed random matrices. Annales de l’I.H.P. Probabilit´es et statis-
tiques, 45(3):589–610, 2009.
• For studies of inﬁnite variance L´evy matrices, see
– P. Cizeau and J. P. Bouchaud. Theory of L´evy matrices. Physical Review E,
50:1810–1822, 1994,
– Z. Burda, J. Jurkiewicz, M. A. Nowak, G. Papp, and I. Zahed. Free random L´evy and
Wigner-L´evy matrices. Physical Review E, 75:051126, 2007,
– G. Ben Arous and A. Guionnet. The spectrum of heavy tailed random matrices.
Communications in Mathematical Physics, 278(3):715–751, 2008,
– S. Belinschi, A. Dembo, and A. Guionnet. Spectral measure of heavy tailed
band and covariance random matrices. Communications in Mathematical Physics,
289(3):1023–1055, 2009.
• On the outlier method for the phase recovery problem, the reader is referred to
– Y. M. Lu and G. Li. Phase transitions of spectral initialization for high-dimensional
nonconvex estimation. preprint arXiv:1702.06435, 2017,
– W. Luo, W. Alghamdi, and Y. M. Lu. Optimal spectral initialization for signal recov-
ery with applications to phase retrieval. IEEE Transactions on Signal Processing,
67(9):2347–2356, 2019,
and for a review, see
– A. Stern, editor. Optical Compressive Imaging. CRC Press, Boca Raton, Fla., 2017.


Part III
Applications


15
Addition and Multiplication: Recipes and Examples
In the second part of this book, we have built the necessary tools to compute the spectrum
of sums and products of free random matrices. In this chapter we will review the results
previously obtained and show how they work on concrete, simple examples. More sophisti-
cated examples, and some applications to real world data, will be developed in subsequent
chapters.
15.1 Summary
We introduced the concept of freeness which can be summarized by the following intuitive
statement: two large matrices are free if their eigenbases are related by a random rotation.
In particular a large matrix drawn from a rotationally invariant ensemble is free with respect
to any matrix independent of it, for example a deterministic matrix.1 For example A and
OBOT are free when O is a random rotation matrix (in the large dimension limit). When A
and B are free, their R- and S-transforms are, respectively, additive and multiplicative:
RA+B(x) = RA(x) + RB(x),
SAB(t) = SA(t)SB(t).
(15.1)
The free product needs some clariﬁcation as AB is in general not a symmetric matrix,
the S-transform SAB(t) in fact relates to the eigenvalues of the matrix
√
AB
√
A, which are
the same as those of
√
BA
√
B when both A and B are positive semi-deﬁnite (otherwise the
square-root is ill deﬁned).
15.1.1 R- and S-Transforms
The R- and S-transforms are deﬁned by the following relations:
gA(z) = τ
&
(z −A)−1'
,
(15.2)
tA(ζ) = τ
&
(1 −ζ −1A)−1'
−1 = ζgA(ζ) −1;
(15.3)
RA(x) = zA(x) −1
x,
SA(t) = t + 1
tζA(t) if τ(A)  0,
(15.4)
1 By large, we mean that all normalized moments computed using freeness are correct up to corrections that are O(1/N).
243

244
Addition and Multiplication: Recipes and Examples
where zA(x) and ζA(t) are the inverse functions of gA(z) and tA(ζ), respectively.
Under multiplication by a scalar they behave as
RαA(x) = αRA(αx),
SαA(t) = α−1SA(t).
(15.5)
While the R-transform behaves simply under a shift by a scalar,
RA+α1(x) = α + RA(x),
(15.6)
there is no simple formula for computing the S-transform of a shifted matrix. On the other
hand, the S-transform is simple under matrix inversion:
SA−1(x) =
1
SA(−x −1).
(15.7)
The two transforms are related by the following equivalent identities:
SA(t) =
1
RA(tSA(t)),
RA(x) =
1
SA(xRA(x)).
(15.8)
The identity matrix has particularly simple transforms:
g1(z) =
1
z −1
t1(ζ) =
1
ζ −1;
(15.9)
R1(x) = 1,
S1(t) = 1.
(15.10)
The R- and S-transforms have the following Taylor expansion for small arguments:
RA(x) = κ1 + κ2x + κ3x2 + · · · ,
SA(x) = 1
κ1
−κ2
κ3
1
x + 2κ2
2 −κ1κ3
κ5
1
x2 + · · · ,
(15.11)
where κn are the free cumulants of A:
κ1 = τ(A),
κ2 = τ(A2) −τ 2(A),
κ3 = τ(A3) −3τ(A)τ(A2) + 2τ 3(A).
(15.12)
Combining Eqs. (15.7) and (15.11), we can obtain the inverse moments of A from its
S-transform. In particular,
τ(A−1) = SA(−1),
τ(A−2) = SA(−1)

SA(−1) −S′
A(−1)

.
(15.13)
15.1.2 Computing the Eigenvalue Density
The R-transform provides a systematic way to obtain the spectrum of the sum C of two
independent matrices A and B, where at least one of them is rotationally invariant. Here is
a simple recipe to compute the eigenvalue density of a free sum of matrices:
1 Find gB(z) and gA(z).
2 Invert gB(z) and gA(z) to get zB(g) and zA(g), and hence RB(x) and RA(x).

15.2 R- and S-Transforms and Moments of Useful Ensembles
245
3 RC(x) = RB(x) + RA(x), which gives zC(g) = RC(g) −g−1.
4 Solve zC(g) = z for gC(z).
5 Use Eq. (2.47) to ﬁnd the density:
ρC(λ) = limη→0+ Im gC(λ −iη)
π
.
(15.14)
In the multiplicative case (C = A
1
2 BA
1
2 ), the recipe is similar:
1 Find tB(ζ) and tA(ζ).
2 Invert tB(ζ) and tA(ζ) to get ζB(t) and ζA(t), and hence SB(t) and SA(t).
3 SC(t) = SB(t)SA(t), which gives ζC(t)SC(t)t = t + 1.
4 Solve ζC(t) = ζ for tC(ζ).
5 Equation (2.47) for gC(z) = (tC(z) + 1)/z is equivalent to
ρC(λ) = limη→0+ Im tC(λ −iη)
πλ
.
(15.15)
In some cases, the equation in step 4 is exactly solvable. But it is usually a high order
polynomial equation, or worse, a transcendental equation. In these cases numerical solution
is still possible. There always exists at least one solution that satisﬁes
g(z) = z−1 + O(z−2)
(15.16)
for z →∞. Since the eigenvalues of B and A are real, their R- and S-transforms are real
for real arguments. Hence the equation in step 4 is an equation with real coefﬁcients. In
order to ﬁnd a non-zero eigenvalue density we need to ﬁnd solutions with a strictly positive
imaginary part when the parameter η goes to zero. When the equation is quadratic or cubic,
complex solutions come in complex conjugated pairs: therefore, at most one solution will
have a strictly positive imaginary part. As a numerical trick, πρ(λ) can be equated with the
maximum of the imaginary part of all two or three solutions (the density will be zero when
all solutions are real). For higher order polynomial and transcendental equations, we have
to be more careful as there can be spurious complex solutions with positive imaginary part.
Exercise 15.2.1 shows how to do these computations in concrete cases.
15.2 R- and S-Transforms and Moments of Useful Ensembles
15.2.1 Wigner Ensemble
The Wigner ensemble is rotationally invariant, therefore a Wigner matrix is free from any
matrix from which it is independent. For a Wigner matrix X of variance σ 2, the R-transform
reads
RX(x) = σ 2x.
(15.17)
The Wigner matrix is stable under free addition, i.e. the free sum of two Wigner matrices
of variance σ 2
1 and σ 2
2 is a Wigner with variance σ 2
1 + σ 2
2 .

246
Addition and Multiplication: Recipes and Examples
The Wigner matrix is traceless (τ(X) = 0), so its S-transform is ill-deﬁned. However, we
can shift the mean of the entries of X by a certain parameter m. We then have RX+m(x) =
m + σ 2x. We can use Eq. (11.97) and compute the S-transform:
SX+m(t) =
√
m2 + 4σ 2t −m
2σ 2t
=
m
2σ 2t
⎛
⎝
2
1 + 4σ 2t
m2 −1
⎞
⎠.
(15.18)
It is regular at t = 0 whenever m > 0 and tends to (σ √t)−1 when m →0.
Finally, let us recall the formula for the positive moments of Wigner matrices:
τ(X2k) =
(2k)!
(k + 1)k!2 σ 2k;
τ(X2k+1) = 0.
(15.19)
The negative moments of X are all inﬁnite, because the density of zero eigenvalues is
positive.
15.2.2 Wishart Ensemble
For a white Wishart matrix Wq with parameter q = N/T , one has (see Section 10.1)
RWq(x) =
1
1 −qx .
(15.20)
To compute its S-transform we ﬁrst remember that its Stieltjes transform g(z) satisﬁes Eq.
(4.37), which can be written as an equation for t(ζ) or its inverse ζ(t):
ζt −(1 + qt)(t + 1) = 0
⇒
SWq(t) =
1
1 + qt .
(15.21)
The ﬁrst few moments of a white Wishart matrix are given by
τ(Wq) = 1,
τ(W2
q) = 1 + q;
τ(W−1
q ) =
1
1 −q,
τ(W−2
q ) =
1
(1 −q)3 . (15.22)
15.2.3 Inverse-Wishart Ensemble
We take the opportunity of this summary of R- and S-transforms to introduce a very
useful ensemble of matrices, namely the inverse-Wishart ensemble. We will call an inverse-
Wishart matrix2 the inverse of a white Wishart matrix, which, we recall, has unit normalized
trace.
For a Wishart matrix to be invertible we need to have q < 1. Let Wq be such a matrix.
Using Eq. (11.116) we can show that
SW−1
q (t) = 1 −q −qt.
(15.23)
2 More generally the inverse of a Wishart matrix with any covariance C can be called an inverse-Wishart but we will only
consider white inverse-Wishart matrices.

15.2 R- and S-Transforms and Moments of Useful Ensembles
247
Since τ(W−1
q ) = 1/(1 −q), we deﬁne the (normalized) inverse-Wishart as
W
p =
(1 −q)W−1
q
and call p := q/(1 −q). Rescaling and changing variable we obtain
S
W
p(t) = 1 −pt.
(15.24)
By construction the inverse-Wishart has mean 1 and variance p. Using Eq. (15.11), we ﬁnd
that it has κ3(
W
p) = 2p2, which is higher than the skewness of a white Wishart matrix
with the same variance (κ3(Wq) = q2).
From the S-transform we can ﬁnd the R-transform using Eq. (15.8):
R
W
p(x) = 1 −√1 −4px
2px
.
(15.25)
To ﬁnd the Stieltjes transform and the density, it is easier to compute the T-transform from
Eq. (11.63) and convert the result into a Stieltjes transform:
g
W
p(z) = (1 + 2p)z −1 −
±
⃝*
(z −1)2 −4pz
2pz2
.
(15.26)
We can use Eq. (2.47) to ﬁnd the density of eigenvalues or do the following change of
variable in the white Wishart density (Eq. (4.43)):
x = 1 −q
λ
and
p =
q
1 −q .
(15.27)
Both methods give (see Fig. 15.1)
ρ
W
p(x) =
*
(x+ −x)(x −x−)
2πpx2
,
x−< x < x+,
(15.28)
with the edges of the spectrum given by
x± = 2p + 1 ± 2
*
2(p + 1).
(15.29)
From the Stieltjes transform we can obtain
τ(
W
−1
p ) = −lim
z→0 g
W
p(z) = 1 + p.
(15.30)
Other low moments of the inverse-Wishart matrix read
τ(
W
p) = 1,
τ(
W
2
p) = 1 + p;
τ(
W
−1
p ) = 1 + p,
τ(
W
−2
p ) = (1 + p)(1 + 2p).
(15.31)
Finally, the large N inverse-Wishart matrix potential can be obtained from the real part
of the Stieltjes transform using (5.38)
V
W
p(x) = 1
px + 1 + 2p
p
log x.
(15.32)

248
Addition and Multiplication: Recipes and Examples
0
1
2
3
4
l
0.00
0.25
0.50
0.75
1.00
1.25
1.50
r(l)
inverse-Wishart p = 1/ 2
white Wishart q = 1/ 2
Figure 15.1 Density of eigenvalues for an inverse-Wishart distribution with p = 1/2. The white
Wishart distribution (q = 1/2) is shown for comparison. Both laws are normalized, have unit mean
and variance 1/2.
For completeness we give here the law of the elements of a general (not necessarily
white) inverse-Wishart matrix at ﬁnite N. We recall the law of a general Wishart matrix
Eq. (4.16):
P (E) = (T/2)NT/2

N(T/2)
(det E)(T −N−1)/2
(det C)T/2
exp
(
−T
2 Tr

EC−1)
,
(15.33)
where E is an N × N Wishart matrix measured over T time steps with true correlations
C and normalized such that E[E] = C. We deﬁne the inverse-Wishart as
W
= E−1. Note
that a ﬁnite N Wishart matrix has
E
&
E−1'
=
T
T −N −1C−1 =: %,
(15.34)
where we have deﬁned the matrix % such that E[
W
] = %. To do the change of variable
E →
W
in the joint probability density, we need to multiply by the Jacobian (det
W
)−N−1
(see Eq. (1.41)). Putting everything together we obtain
P(
W
) = (T −N −1)NT/2
2NT/2
N(T/2)
(det %)T/2
(det
W
)(T +N+1)/2 exp
(
−T −N −1
2
Tr

W
−1%
)
.
(15.35)
In the scalar case N = 1, the inverse-Wishart distribution reduces to an inverse-gamma
distribution:
P(m) =
ba

(a)m−a−1e−b/m,
(15.36)
with b = (T −2)%/2 and a = T/2.

15.3 Worked-Out Examples: Addition
249
Exercise 15.2.1
Free product of two Wishart matrices
In this exercise, we will compute the eigenvalue distribution of a matrix E =
(Wq0)
1
2 Wq(Wq0)
1
2 ; as we will see in Section 17.1, this matrix would be the
sample covariance matrix of data with true covariance given by a Wishart with
q0 observed over T samples such that q = N/T .
(a)
Using Eq. (15.21) and the multiplicativity of the S-transform, write the S-
transform of E.
(b)
Using the deﬁnition of the S-transform write an equation for tE(z). It is a
cubic equation in t. If either q0 or q goes to zero, it reduces to the standard
Marˇcenko–Pastur quadratic equation.
(c)
Use Eq. (15.15) and a numerical root ﬁnder to plot the eigenvalue density of
E for q0 = 1/2 and q ∈{1/4,1/2,3/4}. In practice you can work with η = 0;
of the three roots of your cubic equation, at most one will have a positive
imaginary part. When all three solutions are real ρE(λ) = 0.
(d)
Generate numerically two independent Wishart matrices with q = 1/2
(N = 1000 and T = 2000) and compute E = (Wq0)
1
2 Wq(Wq0)
1
2 . Note that
the square-root of a matrix is obtained by applying the square-roots to
its eigenvalues. Diagonalize your E and compare its density with your
result in (c).
15.3 Worked-Out Examples: Addition
15.3.1 The Arcsine Law
Consider the free sum of two symmetric orthogonal matrices, i.e. matrices with eigen-
values ±1 with equal weights. Let M1 and M2 be two such matrices, their Stieltjes and
R-transforms are given by
g(z) =
z
z2 −1
and
R(g) =
*
1 + 4g2 −1
2g
,
(15.37)
from which we can deduce that M = 1
2(M1 + M2) has an R-transform given by
RM(g) =
*
1 + g2 −1
g
,
(15.38)
where we have used the scaling RαA(x) = αRA(αx) with α = 1/2.
The corresponding Stieltjes transform reads
gM(z) =
1
z
*
1 −1/z2 .
(15.39)

250
Addition and Multiplication: Recipes and Examples
From this expression we deduce that the density of eigenvalues is given by the centered
arcsine law:
ρM(λ) = 1
π
1
√
1 −λ2,
λ ∈(−1,1),
(15.40)
and zero elsewhere. This corresponds to a special case of the Jacobi ensemble that we have
encountered in Section 7.1.3.
15.3.2 Sum of Uniform Densities
Suppose now we want to compute the eigenvalue distribution of a matrix M = U+OUOT ,
where U is a diagonal matrix with entries uniformly distributed between −1 and 1 (e.g.
[U]kk = 1 + (1 −2k)/N) and O a random orthogonal matrix. This is the free sum of two
matrices with uniform eigenvalue density.
First we need to compute the Stieltjes transform of U. We have
ρU(λ) = 1
2,
λ ∈(−1,1).
(15.41)
The corresponding Stieltjes transform is3
gU(z) = 1
2
 1
−1
dλ
z −λ = 1
2 log
z + 1
z −1

.
(15.42)
Note that when −1 < λ < 1 the argument of the log in gU(z) is negative so Im g(λ −
iη) = iπ/2, consistent with a uniform distribution of eigenvalues. We then compute the
R-transform by ﬁnding the inverse of gU(z):
z(g) = e2g + 1
e2g −1 = coth(g).
(15.43)
And so the R-transform of U is given by
RU(g) = coth(g) −1
g .
(15.44)
The R-transform of M is twice that of U. To ﬁnd the Stieltjes transform of U we thus need
to solve
z = RM(g) + 1
g = 2 coth(g) −1
g,
(15.45)
for g(z). This is a transcendental equation and we need to solve it for complex z near the
real axis. Before attempting to do this, it is useful to plot z(g) (Fig. 15.2). The region where
z = z(g) does not have real solutions is where the eigenvalues are. This region is between
a local maximum and a local minimum of z(g). We should look for complex solutions
of Eq. (15.45) near the real axis for Re(z) between −1.54 and 1.54. We can then put this
equation into a complex non-linear solver. The density will be given by Im g(z)/π for Im(z)
3 A more general uniform density between [m −a,m + a] has mean m, variance a2/3 and
gU(z) = log((z −m + a)/(z −m −a))/(2a).

15.3 Worked-Out Examples: Addition
251
−3
−2
−1
0
1
2
3
g
−10
−5
0
5
10
= R(g)+ 1/ g
1.25 1.50 1.75
1.55
1.56
Figure 15.2 The function z(g) = RM(g) + 1/g for the free sum of two ﬂat distributions. Note that
there is a region of z near [−1.5,1.5] when z = z(g) does not have real solutions. This is where the
eigenvalues lie. The inset shows a zoom of the region near z = 1.5, indicating more clearly that z(g)
has a minimum at g+ ≈1.49, so λ+ = z(g+). The exact edges of the spectrum are λ± ≈±1.5429.
−2
−1
0
1
2
l
0.0
0.1
0.2
0.3
r(l)
Figure 15.3 Density of eigenvalues for the free sum of two uniform distributions. Continuous curve
was computed using a numerical solution of Eq. (15.45). The histogram is a numerical simulation
with N = 5000.
very small and Re(z) in the desired range. Note that complex solutions come in conjugated
pairs, and it is hard to force the solver to ﬁnd the correct one. This is not a problem; since
their imaginary parts have the same absolute value, we can just use
ρ(λ) = | Im g(λ −iη)|
π
for some small η.
(15.46)
We have plotted the resulting density in Figure 15.3.

252
Addition and Multiplication: Recipes and Examples
15.4 Worked-Out Examples: Multiplication
15.4.1 Free Product of a Wishart and an Inverse-Wishart
Consider the free product of a Wishart Wq of parameter q and an independent inverse-
Wishart
W
p of parameter p, i.e. E = *
W
pWq
*
W
p. We already have the building
blocks:
S
W
p(t) = 1 −pt;
SWq(t) =
1
1 + qt
⇒
SE(t) = 1 −pt
1 + qt ,
(15.47)
leading to
ζE(t) = (t + 1)(1 + qt)
t(1 −pt)
.
(15.48)
Inverting this relation to obtain tE(ζ) leads to a quadratic equation for t:
(t + 1)(1 + qt) = ζt(1 −pt),
(15.49)
which can be explicitly solved as
tE(z) = z −q −1 −
±
⃝*
(q + 1 −z)2 −4(q + zp)
2(q + zp)
.
(15.50)
Using Eq. (15.15) ﬁnally yields
ρE(λ) =
*
4(pλ + q) −(1 −q −λ)2
2πλ(pλ + q)
.
(15.51)
The edges of the support are given by
λ± =
&
1 + q + 2p ± 2
*
(1 + p)(q + p)
'
.
(15.52)
One can check that the limit p →0 recovers the trivial case
W
0 = 1 for which the
Marˇcenko–Pastur edges indeed read
λ± = (1 + q) ± 2 √q.
(15.53)
Exercise 15.4.1
Free product of Wishart and inverse-Wishart
(a)
Generate numerically a normalized inverse-Wishart
W
p for p = 1/4 and
N = 1000. Check that τ(
W
p) = 1 and τ(
W
2
p) = 1.25. Plot a normalized
histogram of the eigenvalues of
W
p and compare with Eq. (15.28).
(b)
Generate an independent Wishart Wq with q = 1/4 and compute E =
*
W
pWq
*
W
p. To compute *
W
p, diagonalize
W
p, take the square-root of
its eigenvalues and reconstruct *
W
q. Check that τ(E) = 1 and τ(E2) = 1.5.
Plot a normalized histogram of the eigenvalues of E and compare with
Eq. (15.51).

15.4 Worked-Out Examples: Multiplication
253
(c)
For every eigenvector vk of E compute ξk := vT
k
W
pvk, and make a scatter
plot of ξk vs λk, the eigenvalue of vk. Your scatter plot should show a noisy
straight line. We will see in Chapter 19, Eq. (19.49), that this is related to the
fact that linear shrinkage is the optimal estimator of the true covariance from
the sample covariance when the true covariance is an inverse-Wishart.
15.4.2 Free Product of Projectors
As our last simple example, consider a space of large dimension N, and a projector P1
on a subspace of dimension N1 ≤N, i.e. a diagonal matrix with N1 diagonal elements
equal to unity, and N −N1 elements equal to zero. We now introduce a second projector
P2 on a subspace of dimension N2 ≤N, and would like to study the eigenvalues of the
free product of these two projectors, P = P1P2. Clearly, all eigenvalues of P must lie in the
interval (0,1).
As now usual, we ﬁrst need to compute the T-transform of P1 and P2. We deﬁne the
ratios qa = Na/N, a = 1,2. Since Pa has Na eigenvalues equal to unity and N −Na
eigenvalues equal to zero, one ﬁnds
gPa(z) = 1
N
( Na
z −1 + N −Na
z
)
= qa −1 + z
z(z −1)
⇒
tPa(ζ) =
qa
ζ −1.
(15.54)
Therefore, the inverse of the T-transforms just read ζPa(t) = 1 + qa/t, and
SPa(t) = t + 1
t + qa
⇒
SP =
(t + 1)2
(t + q1)(t + q2).
(15.55)
Now, going backwards,
ζP(t) = (t + q1)(t + q2)
t(t + 1)
,
(15.56)
again leading to a quadratic equation for tP(ζ):
(ζ −1)t2 + (ζ −q1 −q2)t −q1q2 = 0,
(15.57)
whose solution is
t(ζ) = (q1 + q2 −ζ) +
±
⃝*
ζ 2 −2ζ(q1 + q2 −2q1q2) + (q1 −q2)2
2(ζ −1)
,
(15.58)
where the notation
±
⃝√· deﬁned by Eq. (4.56) ensures that we pick the correct root. Note
that the argument under the square-root has zeros for
λ± = q1 + q2 −2q1q2 ± 2
*
q1q2(1 −q1)(1 −q2).
(15.59)
One can check that λ−≥0, the zero bound being reached for q1 = q2. Note also that
λ+λ−= (q1 −q2)2.

254
Addition and Multiplication: Recipes and Examples
The Stieltjes transform of P can thus be written as
gP(z) = 1
z + (q1 + q2 −z) +
±
⃝*
z2 −2z(q1 + q2 −2q1q2) + (q1 −q2)2
2z(z −1)
.
(15.60)
This quantity has poles at z = 0 and z = 1, and an imaginary part when z ∈(λ+,λ−). The
spectrum of P therefore has a continuous part, given by
ρP(λ) =
*
(λ+ −λ)(λ −λ−)
2πλ(1 −λ)
,
(15.61)
and two delta peaks, A0δ(λ) and A1δ(λ −1). To ﬁnd the amplitude of the potential poles,
we need to compute the numerator of (15.60) at the values z = 0 and z = 1. Remember
that
±
⃝√· equals −√· on the real axis left of the left edge and √· right of the right edge.
The amplitude of the z = 0 pole of gP(z) is
A0 = 1 −(q1 + q2) −
*
(q1 −q2)2
2
= 1 −min(q1,q2),
(15.62)
while the amplitude of the z = 1 pole is
A1 = (q1 + q2 −1) +
*
1 −2(q1 + q2 −2q1q2) + (q1 −q2)2
2
= (q1 + q2 −1) +
*
(q1 + q2 −1)2
2
= max(q1 + q2 −1,0).
(15.63)
This makes a lot of sense geometrically: our product of two projectors can only have a unit
eigenvalue if the sum of the dimensions of space spanned by these two projectors exceeds
the total dimension N, i.e. when N1 + N2 > N. Otherwise, there cannot be (generically)
any eigenvalue beyond λ+.
When q1 + q2 < 1, the density of non-zero eigenvalues (15.61) is the same (up to
a normalization) as the density of eigenvalues of a Jacobi matrix (7.20). If we match the
edges of the spectrum we ﬁnd the identiﬁcation c1 = qmax/qmin and c+ = 1/qmin. The ratio
of normalization 1/c+ = qmin implies that the product of projectors density has a missing
mass of 1−qmin, which is precisely the Dirac mass at zero. The special case q1 = q2 = 1/2
was discussed in the context of 2 × 2 matrices in Exercise 12.5.2. In that case, half of the
eigenvalues are zero and the other half are distributed according to the arcsine law: the
arcsine law is the limit of a Jacobi matrix with c1 →1 and c+ →2.
There is an alternative, geometric interpretation of the above calculation that turns out
to be useful in many different contexts, see Section 17.4 for an extended discussion. The
eigenvectors of projector P1 form a set of N1 orthonormal vectors xα, α = 1, . . . ,N1,
from which an N1 ×N matrix X of components (xα)i can be formed. Similarly, we deﬁne
an N2 × N matrix Y of components (yβ)i, β = 1, . . . ,N2. Now, one can write P as
P = XT XYT Y.
(15.64)

15.4 Worked-Out Examples: Multiplication
255
The non-zero eigenvalues of P are the same as the non-zero eigenvalues of MT M (or those
of MMT ), where M is the N1 × N2 matrix of overlaps:
Mα,β :=
N

i=1
(xα)i(yβ)i.
(15.65)
The eigenvalues of P correspond to the square of the singular values s of M. The geo-
metrical interpretation of these singular values is as follows: the largest singular value
corresponds to the maximum overlap between any normalized linear combination of the
xα on the one hand and of the yβ on the other hand. These two linear combinations deﬁne
two one-dimensional subspaces of the spaces spanned by xα and yβ. Once these optimal
directions are removed, one can again ask the same question for the remaining N1 −1 and
N2 −1 dimensional subspaces, deﬁning the second largest singular value of M, and so on.
15.4.3 The Jacobi Ensemble Revisited
We saw in the previous section that the free product of two random projectors has a
very simple S-transform and that its non-zero eigenvalues are given by those of a Jacobi
matrix. We suspect that the Jacobi ensemble has itself a simple S-transform. Rather than
computing its S-transform from its Stieltjes transform (7.18), let us just use the properties
of the S-transform to compute it directly from the deﬁnition of a Jacobi matrix.
Recall from Chapter 7, an N × N Jacobi matrix J is deﬁned as (1 + E)−1 where the
matrix E is the free product of the inverse of an unnormalized Wishart matrix W1 with
T1 = c1N and another unnormalized Wishart W2 with T2 = c2N.
The two Wishart matrices have S-transforms given by
SW1,2(t) = T1,2
1
1 + c−1
1,2t
= N
1
c1,2 + t .
(15.66)
Using the relation for inverse matrices (15.7), we ﬁnd
SW−1
1 (t) = N−1(c1 −t −1).
(15.67)
The S-transform of E is just the product
SE(t) = SW−1
1 (t)SW2(t) = c1 −t −1
c2 + t
.
(15.68)
The next step is to shift the matrix E by 1. As mentioned earlier, there is no easy rule
to compute the S-transform of a shifted matrix. So this will be the hardest part of the
computation.
The R-transform behaves simply under shift. The trick is to use one of Eqs. (15.8) to
write an equation for RE(x), shift E by 1 and use the other R-S relation to ﬁnd back an
equation for SE+1(t). First we write
(c2 + t)SE = c1 −t −1.
(15.69)
The second of Eqs. (15.8) can be interpreted as the replacements S →1/R and t →xR
and gives
(1 + x −c1 + RE)RE + c2 = 0.
(15.70)
Now RE(x) = RE+1(x) −1, so
(x −c1 + RE+1)(RE+1 −1) + c2 = 0.
(15.71)

256
Addition and Multiplication: Recipes and Examples
Following the ﬁrst of Eqs. (15.8), we make the replacements R →1/S and x →tS and
ﬁnd
1 −c1 + t + (c2 + c1 −t −1)SE+1 = 0
⇒
SE+1(t) =
t + 1 −c1
t + 1 −c2 −c1
.
(15.72)
Finally, using the relation for inverse matrices, Eq. (15.7) gives
SJ(t) = t + c2 + c1
t + c1
.
(15.73)
We can verify that the T-transform of the Jacobi ensemble
tJ(ζ) =
c1 + 1 −c+ζ +
±
⃝/
c2
+ζ 2 −2(c1c+ + c+ −2c1)ζ + (c1 −1)2
2(ζ −1)
(15.74)
is compatible with our previous result on the Stieltjes transform, Eq. (7.18). We can use
the Taylor series of the S-transform (15.11) to ﬁnd the ﬁrst few cumulants:
κ1 =
c1
c1 + c2
,
κ2 =
c1c2
(c1 + c2)3,
κ3 = (c1 −c2)c1c2
(c1 + c2)5
.
(15.75)
From the S-transform we can compute the R-transform using Eq. (15.8):
RJ(x) = x −c1 −c2 −
*
x2 + 2(c1 −c2)x + (c1 + c2)2
2x
.
(15.76)
Finally, we note that the arcsine law is a Jacobi matrix with c1 = c2 = 1 and has the
following transform:
S(t) = t + 2
t + 1,
R(x) = x −2 −
*
x2 + 4
2x
.
(15.77)
For the centered arcsine law we have Rs(t) = 2R(2x) + 1 and we recover Eq. (15.38).

16
Products of Many Random Matrices
In this chapter we consider an issue of importance in many different ﬁelds: that of prod-
ucts of many random matrices. This problem arises, for example, when one considers the
transmission of light in a succession of slabs of different optical indices, or the propagation
of an electron in a disordered wire, or the way displacements propagate in granular media.
It also appears in the context of chaotic systems when one wants to understand how a
small difference in initial conditions “propagates” as the dynamics unfolds. In this context,
one usually linearizes the dynamics in the vicinity of the unperturbed trajectory. If one
takes stroboscopic snapshots of the system, the perturbation is obtained as the product
of matrices (corresponding to the linearized dynamics) applied on the initial perturbation
(see Chapter 1). If the phase space of the system is large enough, and the dynamics chaotic
enough, one may expect that approximating the problem as a product of large, free matrices
should be a good starting point.
16.1 Products of Many Free Matrices
The speciﬁc problem we will study is therefore the following: consider the symmetrized
product of K matrices, deﬁned as
MK = AKAK−1 . . . A2A1AT
1AT
2 . . . AT
K−1AT
K,
(16.1)
where all Ai are identically distributed and mutually free, i.e. randomly rotated with respect
to one another. We know now that in such a case the S-transforms simply multiply. Noting
as Si(z) the S-transform of AiAT
i , and SMK(z) the S-transform of MK, one has
SMK(z) =
K
,
i=1
Si(z) ≡S1(z)K.
(16.2)
Now, it is intuitive that all the eigenvalues of MK will behave for large K as μK, where μ
is itself a random variable which we will characterize below. We take this as an assumption
and indeed show that the distribution of μ’s tends to a well-deﬁned function ρ∞(μ) as
K →∞. Note here a crucial difference with the case of sums of random matrices. If we
assume that the eigenvalues of a sum of K free random matrices behave as K × μ, one can
257

258
Products of Many Random Matrices
easily establish that the distribution of μ’s collapses for large K to δ(μ−τ(A)), with, once
again, τ(.) = Tr(.)/N. For products of random matrices, on the other hand, the distribution
of μ remains non-trivial, as we will ﬁnd below.
Let us compute SMK(z) in the large K limit using our ansatz that the eigenvalues of M
are indeed of the form μK. We ﬁrst compute the function tK(z) equal to
tK(z) :=

μK
z −μK ρ∞(μ)dμ = −

1
1 −zμ−K ρ∞(μ)dμ.
(16.3)
Setting z := uK, we see that for K →∞there is no contribution to this integral from the
region μ < u, whereas the region μ > u simply yields
tK(z) ≈−P>(z1/K);
P>(u) :=
 ∞
u
ρ∞(μ)dμ.
(16.4)
The next step to get the S-transform is to compute the functional inverse of tK(z). Within
the same approximation, this is given by
t−1
K (z) =
&
P (−1)
>
(−z)
'K
,
(16.5)
where P (−1)
>
is the functional inverse of the cumulative distribution function P>. Finally,
by deﬁnition,
SMK(z) :=
1 + z
zt−1
K (z)
= S1(z)K.
(16.6)
Hence one ﬁnds, in the large K limit where ((1 + z)/z)1/K →1,
P (−1)
>
(−z) =
1
S1(z) ⇒P>(μ) = −S(−1)
1
 1
μ

(16.7)
and ﬁnally ρ∞(μ) = −P ′
>(μ). The ﬁnal result is therefore quite simple, and entirely
depends on the S-transform of AiAT
i .
A simple case is when AiAT
i is a large Wishart matrix, with parameter q ≤1. In this case
S1(z) = (1+qz)−1, from which one easily works out that ρ∞(μ) = 1/q for μ ∈(1−q,1)
and zero elsewhere (see Fig. 16.1 for an illustration).
In many cases of interest, the eigenvalue spectrum of AiAT
i has some symmetries,
coming from the underlying physical problem one is interested in. For example, when our
chaotic system is invariant under time reversal (like the dynamics of a Hamiltonian system),
each eigenvalue λ must come with its inverse λ−1. A simple example of a spectrum with
such a symmetry is the free log-normal, further discussed in the next section. It is deﬁned
from its S-transform, given by
S0
LN(z) = e−a(z+ 1
2 ),
(16.8)

16.1 Products of Many Free Matrices
259
0.4
0.6
0.8
1.0
1.2
m = l 1/ K
0.0
0.5
1.0
1.5
2.0
2.5
r(m)
Figure 16.1 Sample density of μ = λ1/K for the free product of K = 40 white Wishart matrices
with q = 1/2 and N = 1000. The dark line corresponds to the asymptotic density (K →∞), which
is constant between 1 −q and 1 and zero elsewhere. The two dashed vertical lines give the exact
positions of the edges of the spectrum (μ−= 0.44 and μ+ = 1.10) for K = 40, as computed in
Exercise 16.1.1.
where the parameter a is related to the trace of the corresponding matrices equal to ea/2.
Multiplying K such matrices together leads to eigenvalues of the form μK, with
P>(μ) = −

S0
LN
(−1)  1
μ

= 1
2 −log μ
a
,
(16.9)
corresponding to
ρ∞(μ) = −P ′
>(μ) = 1
aμ,
μ ∈(e−a/2,ea/2),
(16.10)
and zero elsewhere. One can explicitly check that μ−1 has the same probability distribution
function as μ.
One often describes the eigenvalues of large products of random matrices in terms of
the Lyapunov exponents , deﬁned as the eigenvalues of
 = lim
K→∞
1
K log MK.
(16.11)
Therefore the Lyapunov exponents are simply related to the μ’s above as  ≡log μ. For
the free log-normal example, the distribution of  is found to be uniform between −a/2
and a/2.
Let us end this section with an important remark: we have up to now considered products
of K matrices with a ﬁxed spectrum, independent of K, which leads to a non-universal
distribution of Lyapunov exponents (i.e. a distribution that explicitly depends on the full
function S1(z)). Let us now instead assume that these matrices are of the form

260
Products of Many Random Matrices
AAT =

1 + a
2K

1 +
B
√
K
,
(16.12)
where a is a parameter and B is traceless and characterized by its second cumulant b =
τ(B2). For large K, S1(z) can then be expanded as
S1(z) = 1 −a
2K −b
K z + o(K−1).
(16.13)
Therefore, for large K, the product of such matrices converges to a matrix characterized by
SMK(z) =

1 −a
2K −b
K z
K
→e−a/2−bz,
(16.14)
which can be interpreted as a multiplicative clt for free matrices, since the detailed statis-
tics of B has disappeared. The choice b = a corresponds to the free log-normal with
inversion symmetry S0
LN (see next section).
Exercise 16.1.1
Edges of the spectrum for the free product of many white
Wishart matrices
In this exercise, we will compute the edges of the spectrum of eigenvalues of
a matrix M given by the free product of K large white Wishart matrices with
parameter q.
(a)
The S-transform of M is simply given by the S-transform of a white Wishart
raised to the power K. Using Eq. (11.92), write an equation for the inverse of
the T-transform, ζ(t), of the matrix M. This is a polynomial equation of order
K + 1.
(b)
For odd N, plot ζ(t) for various K and 0 < q < 1 and convince yourself that
there is always a region of ζ where ζ(t) = ζ has no real solution. This region
is between a local maximum and a local minimum of ζ(t). For even N, the
argument is more subtle, but the correct branch exists only between the same
two extrema.
(c)
Differentiate ζ(t) with respect to t to ﬁnd an equation for the extrema of ζ(t).
After simpliﬁcations and discarding the t = −1/q solution, this equation is
quadratic in t∗with two solutions corresponding to the local minimum and
maximum. Find the two solutions t∗
± and plug these back in your equation for
ζ(t) to ﬁnd the edges of the spectrum λ±.
(d)
Use your result for K = 40 and q = 0.5 to verify the edges of the spectrum
given in Figure 16.1.
(e)
Compute the large K limit of t∗
±. You should ﬁnd t∗
−→−1 and t∗
+ →
(q(K −1))−1. Show that at large K we have λ1/K
−
→1 −q and λ1/K
+
→1.

16.2 The Free Log-Normal
261
16.2 The Free Log-Normal
There exists a free version of the log-normal. Its S-transform is given by
SLN(t) = e−a/2−bt.
(16.15)
As a two-parameter family, the free log-normal is stable in the sense that the free product of
two free log-normals with parameters a1,b1 and a2,b2 is a free log-normal with parameters
a = a1 +a2, b = b1 +b2. The ﬁrst three free cumulants can be computed from Eq. (16.15):
SLN(t) = e−a/2
(
1 −bt + 1
2b2t2
)
+ O(t3).
(16.16)
Comparing with Eq. (15.11), this leads to
κ1 = ea/2,
κ2 = bea,
κ3 = 3b2
2 e2a.
(16.17)
In the special case b = a, the free log-normal S0
LN has the additional property that its
matrix inverse has exactly the same law. Indeed, we have shown in Section 11.4.4 that the
following general relation holds:
SM−1(t) =
1
SM(−t −1),
(16.18)
or, in the free log-normal case with a = b,
SM−1(t) = ea/2−b(1+t) = SM(t)
(16.19)
when b = a. This implies that the eigenvalue distribution is invariant under λ →1/λ and
therefore that M has unit determinant. Let us study in more detail the eigenvalue spectrum
for the symmetric case a = b. By looking for the real extrema of
ζ(t) = t + 1
t
ea(t+1/2),
(16.20)
we can ﬁnd the points t± where t(ζ) ceases to be invertible, which in turn give the edges
of the spectrum λ± = ζ(t±):
t± =
±
/
1 + 4
a −1
2
(16.21)
or
λ+ = 1
λ−
=
( 1a
4 +
1
1 + a
4
)2
exp
⎛
⎝
2
a + a2
4
⎞
⎠.
(16.22)
Note that λ+ = λ−= 1 when a = b = 0, corresponding to the identity matrix. The
eigenvalue distribution is symmetric in λ →1/λ so the density ρ(ℓ) of ℓ= log(λ) is even.
Figure 16.2 shows the density of ℓfor a = 100.

262
Products of Many Random Matrices
−50
−25
0
25
50
ℓ
0.000
0.002
0.004
0.006
0.008
0.010
0.012
r(ℓ)
free log-normal
Wigner
Figure 16.2 Probability density of ℓ= log(λ) for a symmetric free log-normal (16.15) with a = b =
100 compared with a Wigner semi-circle with the same endpoints. As expected, the distribution is
even in ℓ. For a ≲1 the density of ℓis indistinguishable to the eye from a Wigner semi-circle (not
shown), whereas for a →∞the distribution of ℓ/a tends to a uniform distribution on [−1/2,1/2].
In the more general case a  b, the whole distribution of ℓ= log(λ) is just shifted by
(a −b)/2, as expected from the scaling property of the S-transform upon multiplication by
a scalar.
16.3 A Multiplicative Dyson Brownian Motion
Let us now consider the problem of multiplying random matrices close to unity from a
slightly different angle. Consider the following iterative construction for N × N matrices:
Mn+1 = M
1
2n
&
(1 + aε
2 )1 + √εBn
'
M
1
2n,
(16.23)
where Bn is a sequence of identical, free, traceless N × N matrices and ε ≪1. Using
second order perturbation theory, one can deduce an iteration formula for the eigenvalues
λi,n of Mn, which reads
λi,n+1 = λi,n

1 + aε
2 + √εvT
i,nBnvi,n

+ ε

ji
λi,nλj,n(vT
i,nBnvj,n)2
λi,n −λj,n
,
(16.24)
where vi,n are the corresponding eigenvectors. Noting that Mn and Bn are mutually free
and that τ(Bn) = 0, one has, in the large N limit (using, for example, Eq. (12.8)),
E[vT
i,nBnvi,n] = 0;
E[(vT
i,nBnvj,n)2] = b
N ,
(16.25)

16.3 A Multiplicative Dyson Brownian Motion
263
where b := τ(B2
n). Choosing ε = dt, an inﬁnitesimal time scale, we end up with a
multiplicative version of the Dyson Brownian motion in ﬁctitious time t:
dλi
dt = a
2λi + b
N

ji
λiλj
λi −λj
+
1
b
N λiξi,
(16.26)
where ξi is a Langevin noise, independent for each λi (compare with Eq. (9.9)).
Now, let us consider the “time” dependent Stieltjes transform, deﬁned as usual as
g(z,t) = 1
N

i
1
z −λi(t).
(16.27)
Its evolution is obtained as
∂g
∂t = 1
N

i
1
(z −λi)2
dλi
dt = −1
N
∂
∂z

i
1
(z −λi)
dλi
dt .
(16.28)
After manipulations very similar to those encountered in Section 9.3.1, and retaining only
leading terms in N, one ﬁnally obtains
∂g
∂t = 1
2
∂
∂z
&
(2b −a)zg −bz2g2'
.
(16.29)
Now, introduce the auxiliary function h(ℓ,t) := eℓg(eℓ,t) + a/2b −1, which obeys
∂h
∂t = −bh∂h
∂ℓ.
(16.30)
This is precisely the Burgers’ equation (9.37), up to a rescaling of time t →bt. Its solution
obeys the following self-consistent equation obtained using the method of characteristics
(see Section 10.1):
h(ℓ,t) = h0(ℓ−bth(ℓ,t));
h0(ℓ) := h(ℓ,0) =
1
1 −e−ℓ+ a
2b −1,
(16.31)
where we have assumed that at time t = 0 the dynamics starts from the identity matrix:
M0 = 1, for which g(z,0) = (z −1)−1. Hence, with z = eℓ,
g(z,t) =
1
z −et(bzg(z,t)+a/2−b) .
(16.32)
Now, let us compare this equation to the one obeyed by the Stieltjes transform of the free
log-normal. Injecting t = zgLN −1 in
z =
t + 1
tSLN(t)
(16.33)
and using Eq. (16.15), one ﬁnds
zgLN −1 = gLNea/2−b+bzgLN →gLN =
1
z −ebzgLN+a/2−b,
(16.34)
which coincides with Eq. (16.32) for t = 1, as it should. For arbitrary times, one ﬁnds that
the density corresponding to the multiplicative Dyson Brownian motion, Eq. (16.26), is the
free log-normal, with parameters ta and tb.

264
Products of Many Random Matrices
16.4 The Matrix Kesten Problem
The Kesten iteration for scalar random variables appears in many different situations.
It is deﬁned by
Zn+1 = zn(1 + Zn),
(16.35)
where zn are iid random variables. In the following, we will assume that
zn = 1 + εm + √εσηn,
(16.36)
where ε ≪1 and ηn are iid random variables, of zero mean and unit variance. Setting
Zn = Un/ε and expanding to ﬁrst order in ε, one obtains
Un+1 = ε(1 + εm + √εσηn)

1 + Un
ε

= Un + εmUn + √εσηnUn + ε
(16.37)
or, in the continuous time limit dt = ε, the following Langevin equation:
dU
dt = 1 + mU + σηU.
(16.38)
The corresponding Fokker–Planck equation reads
∂P (U,t)
∂t
= −∂
∂U [(1 + mU)P] + σ 2
2
∂2
∂U2
&
U2P
'
.
(16.39)
This process has a stationary distribution provided the drift m is negative. We will thus
write m = −ˆm with ˆm > 0. The corresponding stationary distribution Peq(U) obeys
(1 −ˆmU)Peq = σ 2
2
∂
∂U
&
U2P
'
,
(16.40)
which leads to
Peq(U) =
2μ

(μ)σ 2μ
e−
2
σ2U
U1+μ ;
μ := 1 + 2 ˆm/σ 2,
(16.41)
to wit, the distribution of U is an inverse-gamma, with a power-law tail U−1−μ with a
non-universal exponent μ = 1 + 2 ˆm/σ 2.
Now we can generalize the Kesten iteration for symmetric matrices as1
Un+1 = ε
1
1 + Un
ε

(1 + mε)1 + √εσB
 1
1 + Un
ε ,
(16.42)
or
Un+1 −Un = ε (1 + mUn) + σ √ε
*
UnB
*
Un.
(16.43)
Following the same steps as in the previous section, we obtain a differential equation for
the eigenvalues of U (where we neglect the noise when N →∞):
dλi
dt = 1 −ˆmλi + σ 2
N

ji
λiλj
λi −λj
,
(16.44)
where we again assume that m < 0 in order to ﬁnd a stationary state for our process. The
corresponding evolution of the Stieltjes transform reads, for large N,
1 The results of this section have been obtained in collaboration with T. Gauti´e and P. Le Doussal.

16.4 The Matrix Kesten Problem
265
∂g
∂t = ∂
∂z
(
−g + (σ 2 + ˆm)zg −1
2σ 2z2g2
)
.
(16.45)
If an equilibrium density exists, its Stieltjes transform must obey
1
2σ 2z2g2 + (1 −(σ 2 + ˆm)z)g + C = 0,
(16.46)
where C is a constant determined by the fact that zg →1 when z →∞. Hence,
C = 1
2σ 2 + ˆm.
(16.47)
From the second order equation on g one gets
g =
1
σ 2z2
(
((σ 2 + ˆm)z −1) −
±
⃝/
ˆm2z2 −2(σ 2 + ˆm)z + 1
)
.
(16.48)
As usual, the density of eigenvalues is non-zero when the square-root becomes imaginary.
The edges are thus given by the roots of the second degree polynomial inside the square-
root, namely
λ± = σ 2 + ˆm ±
*
σ 2(σ 2 + 2 ˆm)
ˆm2
.
(16.49)
So only when ˆm →0 can the spectrum extend to inﬁnity, with a power-law decay as
λ−3/2. Otherwise, the power law is truncated beyond 2σ 2/ ˆm2. Note that, contrary to the
scalar Kesten case, the exponent of the power law is universal, with μ = 1/2.
In fact, if one stares at Eq. (16.48), one realizes that the stationary Kesten matrix U is an
inverse-Wishart matrix. Indeed, the eigenvalue spectrum given by Eq. (16.48) maps into
the Marˇcenko–Pastur law, Eq. (4.43), provided one makes the following transformation:
λ →x =
2
σ 2 + 2 ˆm
1
λ.
(16.50)
The parameter q of the Marˇcenko–Pastur law is then given by
q =
σ 2
σ 2 + 2 ˆm = 1
μ < 1.
(16.51)
Although not trivial, this result is not so surprising: since Wishart matrices are the matrix
equivalent of the scalar gamma distribution, the matrix equivalent of the Kesten variable
distributed as an inverse-gamma, Eq. (16.41), is an inverse-Wishart.
Bibliographical Notes
• For a general reference on products of random matrices and their applications, see
– A. Crisanti, G. Paladin, and A. Vulpiani. Products of Random Matrices in Statisti-
cal Physics. Series in Solid-State Sciences, Vol. 104. Springer Science & Business
Media, 2012.
• For the speciﬁc case of electrons in disordered media, see
– C. W. J. Beenakker. Random-matrix theory of quantum transport. Reviews of Modern
Physics, 69:731–808, 1997,
and for applications to granular media, see

266
Products of Many Random Matrices
– L. Yan, J.-P. Bouchaud, and M. Wyart. Edge mode ampliﬁcation in disordered elastic
networks. Soft Matter, 13:5795–5801, 2017.
• For the numerical calculation of the spectrum of Lyapunov exponents, see
– G. Benettin, L. Galgani, A. Giorgilli, and J.-M. Strelcyn. Lyapunov characteristic
exponents for smooth dynamical systems and for Hamiltonian systems; a method for
computing all of them. Part 1: Theory. Meccanica, 15(1):9–20, 1980.
• For the study of the limiting distribution of the spectrum of Lyapunov exponents, see the
early work of
– C. M. Newman. The distribution of Lyapunov exponents: Exact results for random
matrices. Communications in Mathematical Physics, 103(1):121–126, 1986,
and, using free random matrix methods,
– G. Tucci. Asymptotic products of independent gaussian random matrices with corre-
lated entries. Electronic Communications in Probability, 16:353–364, 2011.
• About the classical Kesten variable, see
– H. Kesten. Random difference equations and renewal theory for products of random
matrices. Acta Math., 131:207–248, 1973,
– C. de Calan, J. M. Luck, T. M. Nieuwenhuizen, and D. Petritis. On the distribution
of a random variable occurring in 1d disordered systems. Journal of Physics A:
Mathematical and General, 18(3):501–523, 1985,
and for the particular equation (16.38),
– J.-P. Bouchaud, A. Comtet, A. Georges, and P. Le Doussal. Classical diffusion of a
particle in a one-dimensional random force ﬁeld. Annals of Physics, 201(2):285–341,
1990,
– J.-P. Bouchaud and M. M´ezard. Wealth condensation in a simple model of economy.
Physica A: Statistical Mechanics and Its Applications, 282(3):536–545, 2000.

17
Sample Covariance Matrices
In this chapter, we will show how to compute the various transforms (S(t), t(z), g(z)) for
sample covariance matrices (scm) when the data has non-trivial true correlations, i.e. is
characterized by a non-diagonal true underlying covariance matrix C and possibly non-
trivial temporal correlations as well. More precisely, N time series of length T are stored
in a rectangular N × T matrix H. The sample covariance matrix is deﬁned as
E = 1
T HHT .
(17.1)
If the N time series are stationary, we expect that for T ≫N, the scm E converges to the
“true” covariance matrix C. The non-trivial correlations encoded in the off-diagonal ele-
ments of C are what we henceforth call spatial (or cross-sectional) correlations. But the T
samples might also be non-independent and we will also model these temporal correlations.
Of course, the data might have both types of correlations (spatial and temporal).
We will be interested in the eigenvalues {λk} of E and their density ρE(λ), which we
will compute from the knowledge of its Stieltjes transform gE(z) using Eq. (2.47). We can
also compute the singular values {sk} of H; note that these singular values are related to the
eigenvalues of E via sk = √T λk.
17.1 Spatial Correlations
Consider the case where H are multivariate Gaussian observations, drawn from N(0,C).
We saw in Section 4.2.4 that E is then a general Wishart matrix with column covariance C,
and can be written as
E = C
1
2 WqC
1
2 .
(17.2)
We recognize this formula as the free product of the covariance matrix C and a white
Wishart of parameter q, Wq. Note that since the white Wishart is rotationally invariant, it
is free from any matrix C. From the multiplicativity of the S-transform and the form of the
S-transform of the white Wishart (Eq. (15.21)), we have
SE(t) = SC(t)
1 + qt .
(17.3)
267

268
Sample Covariance Matrices
We can also use the subordination relation of the free product (Eq. (11.109)) to write this
relation in terms of T-transforms:
tE(z) = tC (Z(z)),
Z(z) =
z
1 + qtE(z).
(17.4)
This last expression can be written in terms of the more familiar Stieltjes transform using
t(z) = zg(z) −1:
zgE(z) = ZgC(Z),
where
Z =
z
1 −q + qzgE(z).
(17.5)
This is the central equation that allows one to infer the “true” spectral density of C, ρC(λ),
from the empirically observed spectrum of E. Note that this equation can equivalently be
rewritten in terms of the spectral density of C as
gE(z) =

ρC(μ)dμ
z −μ(1 −q + qzgE(z)).
(17.6)
We will see in Chapter 20 some real world applications of this formula. One of the most
important properties of Eq. (17.5) is its universality: it holds (in the large N limit) much
beyond the restricted perimeter of multivariate Gaussian observations H. In fact, as soon
as the observations have a ﬁnite second moment, the relation between the “true” spectral
density ρC and the empirical Stieltjes transform gE(z) is given by Eq. (17.5).
Let us discuss some interesting limiting cases. First, when q →0, i.e. when T ≫N,
one expects that E ≈C. This is indeed what one ﬁnds since in that limit Z = z + O(q);
hence gE(z) = gC(z) and ρE = ρC.
Second, consider the case C = 1, for which gC(Z) = 1/(Z −1). We thus obtain
zgE(z) =
Z
Z −1 =
z
(z −1 + q −qzgE(z)) →
1
gE(z) = z1 + q −qzgE(z),
(17.7)
which coincides with Eq. (4.37). In the next exercise, we consider the case where C is an
inverse-Wishart matrix, in which case some explicit results can be obtained.
We can also infer some properties of the spectrum of E using the moment generating
function. The T-transform of E can be expressed as the following power series for z →∞:
tE(z) −→
z→∞
∞

k=1
τ(Ek)z−k.
(17.8)
We thus deduce that
Z(z) −→
z→∞
z
1 + q ∞
k=1 τ(Ek)z−k .
Therefore we have, for z →∞,
tC(Z(z)) −→
z→∞
∞

k=1
τ(Ck)
zk

1 + q
∞

ℓ=1
τ(Eℓ)z−ℓ
k
.
(17.9)

17.1 Spatial Correlations
269
Hence, one can thus relate the moments of ρE with the moments of ρC by taking z →∞
in Eq. (17.4), namely
∞

k=1
τ(Ek)
zk
=
∞

k=1
τ(Ck)
zk

1 + q
∞

ℓ=1
τ(Eℓ)z−ℓ
k
.
(17.10)
In particular, Eq. (17.10) yields the ﬁrst three moments of ρE:
τ(E) = τ(C),
τ(E2) = τ(C2) + q,
(17.11)
τ(E3) = τ(C3) + 3qτ(C2) + q2.
We thus see that the mean of E is equal to that of C, whereas the variance of E is equal to
that of C plus q. As expected, the spectrum of the sample covariance matrix E is always
wider (for q > 0) than the spectrum of the population covariance matrix C.
Another interesting expansion concerns the case where q < 1, such that E is invertible.
Hence gE(z) for z →0 is analytic and one readily ﬁnds
gE(z) −→
z→0 −
∞

k=1
τ

E−k
zk−1.
(17.12)
This allows us to study the moments of E−1, which turn out to be important quantities for
many applications. Using Eq. (17.5), we can actually relate the moments of the spectrum
E−1 to those of C−1. Indeed, for z →0,
Z(z) =
z
1 −q −q ∞
k=1 τ

E−k
zk .
Hence, we obtain the following expansion:
∞

k=1
τ

E−k
zk =
∞

k=1
τ

C−k 
z
1 −q
k 
1
1 −
q
1−q
∞
ℓ=1 τ

E−ℓ
zℓ
k
.
(17.13)
After a little work, we get
τ

E−1
= τ

C−1
1 −q ,
τ

E−2
= τ

C−2
(1 −q)2 + qτ

C−12
(1 −q)3 .
(17.14)
We will discuss in Section 20.2.1 a direct application of these formulas: τ(E−1) turns out
to be related to the “out-of-sample” risk of an optimized portfolio of ﬁnancial instruments.
Exercise 17.1.1
The exponential moving average sample covariance matrix
(EMA-SCM)
Instead of measuring the sample covariance matrix using a ﬂat average over
a ﬁxed time window T , one can compute the average using an exponential

270
Sample Covariance Matrices
weighted moving average. Let us compute the spectrum of such a matrix in the
null case of iid data. Imagine we have an inﬁnite time series of vectors of size N
{xt} for t from minus inﬁnity to now. We deﬁne the ema-scm (on time scale τc) as
E(t) = γc
t
t′=−∞
(1 −γc)t−t′xt′xT
t′,
(17.15)
where γc := 1/τc. Hence,
E(t) = (1 −γc)E(t −1) + γcxt′xT
t′.
(17.16)
The second term on the right hand side can be thought of as a Wishart matrix
with T = 1 (or q = N). Now, both E(t) and E(t −1) are equal in law so we
write
E
in law
= (1 −γc)E + γcWq=N.
(17.17)
(a)
Given that E and W are free, use the properties of the R-transform to get the
equation
RE(x) = (1 −γc)RE((1 −γc)x) + γc(1 −Nγcx).
(17.18)
(b)
Take the limit N →∞, τc →∞with q := N/τc ﬁxed to get the following
differential equation for RE(x):
RE(x) = −x d
dx RE(x) +
1
1 −qx .
(17.19)
(c)
The deﬁnition of E is properly normalized, τ(E) = 1 [show this using
Eq. (17.17)], so we have the initial condition R(0) = 1. Show that
RE(x) = −log(1 −qx)
qx
(17.20)
solves your equation with the correct initial condition. Compute the variance
κ2(E).
(d)
To compute the spectrum of eigenvalues of E, one needs to solve a complex
transcendental equation. First write z(g), the inverse of g(z). For q = 1/2 plot
z as a function of g (for −4 < g < 2). You will see that there are values of
z that are never attained by z(g), in other words g(z) has no real solutions for
these z. Numerically ﬁnd complex solutions for g(z) in that range. Plot the
density of eigenvalues ρE(λ) given by Eq. (2.47). Plot also the density for a
Wishart with the same mean and variance.
(e)
Construct numerically the matrix E as in Eq. (17.15). Use N = 1000, τc =
2000 and use at least 10 000 values for t′. Plot the eigenvalue distribution of
your numerical E against the distribution found in (d).

17.2 Temporal Correlations
271
17.2 Temporal Correlations
17.2.1 General Case
A common problem in data analysis arises when samples are not independent. Intuitively,
correlated samples are somehow redundant and the sample covariance matrix should
behave as if we had observed not T samples but an effective number T ∗< T . Let us
analyze more precisely the sample covariance matrix in the presence of correlated samples.
We will start with the case when the true spatial correlations are zero, i.e. C = 1. Our data
can then be written in a rectangular N × T matrix H satisfying
E[HitHjs] = δijKts,
(17.21)
where K is the T × T temporal covariance matrix that we assumed to be normalized as
τ(K) = 1. Following the same arguments as in Section 4.2.4, we can write
H = H0K
1
2,
(17.22)
where H0 is a white rectangular matrix. So the sample covariance matrix becomes
E = 1
T HHT = 1
T H0KHT
0.
(17.23)
Now this is not quite the free product of the matrix K and a white Wishart, but if we deﬁne
the (T × T ) matrix F as
F = 1
N HT H = 1
N K
1
2 HT
0H0K
1
2 ≡K
1
2 W1/qK
1
2,
(17.24)
then F is the free product of the matrix K and a white Wishart matrix with parameter 1/q.
Hence,
SF(t) =
SK(t)
1 + t/q .
(17.25)
To ﬁnd the S-transform of E, we go back to Section 4.1.1, where we obtained Eq. (4.5)
relating the Stieltjes transforms of E and F. In terms of the T-transform, the relation is even
simpler:
tF(z) = qtE(qz)
⇒
ζE(t) = qζF(qt),
(17.26)
where the functions ζ(t) are the inverse T-transforms. Using the deﬁnition of the
S-transform (Eq. (11.92)), we ﬁnally get
SE(t) = SK(qt)
1 + qt ,
(17.27)
which can be expressed as a relation between inverse T-transforms:
ζE(t) = q(1 + t)ζK(qt).
(17.28)

272
Sample Covariance Matrices
We can also write a subordination relation between the T-transforms:
qtE(z) = tK

z
q(1 + tE(z))

.
(17.29)
This is a general formula that we specialize to the case of exponential temporal correlations
in the next section. Note that in the limit z →0, the above equation gives access to τ(E−1).
Using
tE(z) =
z→0 −1 −τ(E−1)z + O(z2),
(17.30)
we ﬁnd
τ(E−1) = −
1
qζK(−q).
(17.31)
17.2.2 Exponential Correlations
The most common form of temporal correlation in experimental data is the decaying expo-
nential, corresponding to a matrix Kts in Eq. (17.21) given by
Kts := a|t−s|,
(17.32)
where 1/ log(a) deﬁnes the temporal span of the correlations.
In Appendix A.3 we explicitly compute the S-transform of K. The result reads
SK(t) =
t + 1
*
1 + (b2 −1)t2 + bt
,
(17.33)
where b := (1 + a2)/(1 −a2). From SK one can also obtain ζK and its inverse tK, which
read
ζK(t) =
*
1 + (b2 −1)t2
t
+ b,
tK(ζ) = −
1
*
ζ 2 −2ζb + 1
.
(17.34)
Combining Eq. (17.27) with Eq. (17.33), we get
SE(t) =
1
*
1 + (b2 −1)(qt)2 + bqt
.
(17.35)
From the S-transform, we ﬁnd
ζE(t) = 1 + t
tSE(t) = 1 + t
t
 /
1 + (b2 −1)(qt)2 + bqt

,
(17.36)
which when inverted leads to a fourth order equation for tE(z) that must be solved numeri-
cally, leading to the densities plotted in Fig. 17.1. However, one can obtain some informa-
tion on τ(E−1). From Eqs. (17.31) and (17.34), one obtains
τ(E−1) =
1
*
q2(b2 −1) + 1 −bq
:=
1
1 −q∗,
(17.37)

17.2 Temporal Correlations
273
0.0
0.5
1.0
1.5
2.0
2.5
l
0.0
0.2
0.4
0.6
0.8
1.0
1.2
r(l)
q = 0.01
b = 25
q = 0.17
b = 1.5
q = 0.25
b = 1
Figure 17.1 Density of eigenvalues for a sample covariance matrix with exponential temporal
correlations for three choices of parameters q and b such that qb = 0.25. All three densities are
normalized, have mean 1 and variance σ 2
E = qb = 0.25. The solid light gray one is the Marˇcenko–
Pastur density (q = 0.25), the dotted black one is very close to the limiting density for q →0 with
σ 2 = bq ﬁxed.
0.0
0.2
0.4
0.6
0.8
1.0
a
0.0
0.2
0.4
0.6
0.8
1.0
q*
q =0.5
q =0.1
Figure 17.2 Effective value q∗versus the one-lag autocorrelation coefﬁcient a for a sample
covariance matrix with exponential temporal correlations shown for two values of q. The dashed
lines indicate the approximation (valid at small a) q∗= q(1 + 2a2). The approximation means that,
for 10% autocorrelation, q∗is only 2% greater than q.
where q∗= N/T ∗deﬁnes the effective length of the time series, reduced by temporal
correlations (compare with Eq. (17.14) with C = 1). Figure 17.2 shows q∗as a function
of a. As expected, q∗= q for a = 0 (no temporal correlations), whereas q∗→1 when
a →1, i.e. when τc →∞. In this limit, E becomes singular.

274
Sample Covariance Matrices
Looking at Eq. (17.35), one notices that when b ≫1 (corresponding to a →1,
i.e. slowly decaying correlations), the S-transform depends on b and q only through the
combination qb. One can thus deﬁne a new limiting distribution corresponding to the limit
q →0, b →∞with qb = σ 2 (which turns out to be the variance of the distribution, see
below). The S-transform of this limiting distribution is given by
S(t) =
1
*
1 + (σ 2t)2 + σ 2t
,
(17.38)
while the equation for the T-transform boils down to a cubic equation that reads:
z2t2(z) −2σ 2zt2(z)(1 + t(z)) = (1 + t(z))2.
(17.39)
The corresponding R-transform is
R(z) =
1
√
1 −2σ 2z
= 1 + σ 2z + 3
2σ 4z2 + O(z3).
(17.40)
The last equation gives its ﬁrst three cumulants: its average is equal to one, its variance
is σ 2 as announced above, and its skewness is κ3 = 3
2σ 4. We notice that this skewness is
larger than that of a white Wishart with the same variance (q = σ 2) for which κ3 = σ 4.
The equations for the Stieltjes g(z) and the T-transform are both cubic equations. The
corresponding distribution of eigenvalues is shown in Figure 17.3. Note that, unlike the
Marˇcenko–Pastur, there is always a strictly positive lower edge of the spectrum λ−> 0
and no Dirac at zero even when σ 2 > 1. Unfortunately, the equation giving λ± is a fourth
order equation that does not have a concise solution.
0
1
2
3
4
5
l
0.0
0.5
1.0
1.5
2.0
r(l)
s2 = 0.25
s2 = 0.5
s2 = 1
Figure 17.3 Density of eigenvalues for the limiting distribution of sample covariance matrix with
exponential temporal correlations Wσ 2 for three choices of the parameter σ 2: 0.25, 0.5 and 1.

17.2 Temporal Correlations
275
An intuitive way to understand this particular random matrix ensemble is to consider
N independent Ornstein–Uhlenbeck processes with the same correlation time τc that we
record over a long time T . We sample the data at interval , such that the total number
of observations is T /. We then construct a sample covariance matrix of the N variables
from these observations. If  ≫τc, then each sample can be considered independent
and the sample covariance matrix will be a Marˇcenko–Pastur with q = N/T . But if
we “oversample” at intervals  ≪τc, such that our observations are strongly correlated,
then the resulting sample covariance matrix no longer depends on  but only on τc. The
sample covariance matrix converges in this case to our new random matrix characterized
by Eq. (17.38), with parameter σ 2 = qb = Nτc/T .
17.2.3 Spatial and Temporal Correlations
In the general case where spatial and temporal correlations exist, the sample covariance
matrix can be written as
E = 1
T HHT = 1
T C
1
2 H0KHT
0C
1
2,
(17.41)
using the same notations as above. After similar manipulations, the S-transform of E is
found to be given by
SE(t) = SC(t)SK(qt)
1 + qt
,
(17.42)
which leads to
ζE(t) = qtζC(t)ζK(qt),
(17.43)
or, in terms of T-transforms,
qtE(z) = tK

z
qtE(z)ζC(tE(z))

.
(17.44)
When C = 1, ζC(t) = (1 + t)/t and one recovers Eq. (17.29). Specializing to the case of
exponential correlations in the limit q →0, a →1, qb = σ 2, we obtain the following
equation for the T-transform of the limiting distribution, now for an arbitrary covariance
matrix C:
z2 −2σ 2ztE(z)ζC(tE(z)) = ζ 2
C(tE(z)),
(17.45)
where we used tK(z) = −1/
√
z2 −2zb + 1. When C = 1, one recovers Eq. (17.39).
When C is an inverse-Wishart matrix, ζC(t) = (t + 1)/t(1 −pt), the equation for tE(z) is
of fourth order.
Note ﬁnally that Eq. (17.44), in the limit z →0, yields a simple generalization of
Eq. (17.31) that reads
τ(E−1) = −τ(C−1)
qζK(−q).
(17.46)

276
Sample Covariance Matrices
Comparing with Eq. (17.14) allows us to deﬁne an effective length of the time series which,
interestingly, is independent of C and reads
q∗:= N
T ∗= 1 + qζK(−q).
(17.47)
Exercise 17.2.1
On the futility of oversampling
Consider data consisting of N variables (columns) with true correlation C and
T independent observations (rows). Instead of computing the sample covariance
matrix with these T observations, we repeat each one m times and sum over
mT columns. Obviously the redundant columns should not change the sample
covariance matrix, hence it should have the same spectrum as the one using only
the original T observations.
(a)
The redundancy of columns can be modeled as a temporal correlation with
an mT × mT covariance matrix K that is block diagonal with T blocks of
size K and all the values within one block equal to 1 and zero outside the
blocks. Show that this matrix has T eigenvalues equal to m and (T −1)m zero
eigenvalues.
(b)
Compute tK(z) for this model.
(c)
Show that SK(t) = (1 + t)/(1 + mt).
(d)
If we include the redundant columns we have a value of qm = N/(mT ), but
we need to take temporal correlations into account so SE(t) = SC(t)SK(qmt)/
(1 + qmt). Show that in this case SE(t) = SC(t)/(1 + qt) with q = N/T ,
which is the result without the redundant columns.
17.3 Time Dependent Variance
Another common and important situation is when the N correlated time series are het-
eroskedastic, i.e. have a time dependent variance. More precisely, we consider a model
where
xt
i = σtHit,
(17.48)
where σt is time dependent, and
E[HitHjs] = δtsCij,
(17.49)
i.e. xt
i is the product of a time dependent factor σt and a random variable with a general
correlation structure C but no time correlations. The scm E can be expressed as
E =
T

t=1
Pt,
Pt := 1
T σ 2
t HtHT
t ,
(17.50)

17.3 Time Dependent Variance
277
where each Pt is a rank-1 matrix with a non-zero eigenvalue that converges, when N and
T tend to inﬁnity, to qσ 2
t τ(C) with, as always, q = N/T .
We will ﬁrst consider the case C = 1, i.e. a structureless covariance matrix. In this case,
the vectors xt are rotationally invariant, the matrix E can be viewed as the free sum of a
large number of rank-1 matrices, each with a non-zero eigenvalue equal to qσ 2
t . Hence,
RE(g) =
T

t=1
Rt(g).
(17.51)
To compute the R-transform of the matrix E we need to compute the R-transform of a
rank-1 matrix. Note that since there are T terms in the sum, we will need to know Rt(g)
including correction of order 1/N:
gt(z) = 1
N
N −1
z
+
1
z −qσ 2t

= 1
z + 1
N
qσ 2
t
z(z −qσ 2t ).
(17.52)
Inverting to ﬁrst order in 1/N we ﬁnd
zt(g) = 1
g + 1
N
qσ 2
t
1 −qσ 2t g .
(17.53)
Now, since R(z) = z(g) −1/z, we ﬁnd
RE(g) = 1
T
T

t=1
σ 2
t
1 −qσ 2t g .
(17.54)
The ﬂuctuations of σ 2
t can be stochastic or deterministic. In the large T limit we can encode
them with a probability density P(s) for s = σ 2 and convert the sum into an integral,
leading to1
RE(g) =
 ∞
0
sP(s)
1 −qsg ds.
(17.55)
Note that if the variance is always 1 (i.e. P(s) = δ(s −1)), we recover the R-transform of
a Wishart matrix of parameter q:
Rq(g) =
1
1 −qg .
(17.56)
In the general case, the R-transform of E is simply related to the T-transform of the distri-
bution of s:
RE(g) = ts
 1
qg

.
(17.57)
1 When the distribution of s is bounded, the integral (17.55) always converges for small enough g and the R-transform is well
deﬁned near zero. For unbounded s, the R-transform can be singular at zero indicating that the distribution of eigenvalues
doesn’t have an upper edge.

278
Sample Covariance Matrices
In the more general case where C is not the identity matrix, one can again write the scm as
E = C
1
2 EC
1
2 , where E corresponds to the case C = 1 that we just treated. Hence, using
the fact that C and E are mutually free, the S-transform of E is simply given by
SE(t) = SC(t)SE(t).
(17.58)
Another way to treat the problem is to view the ﬂuctuating variance as a diagonal temporal
covariance matrix with entries drawn from P(s). Following Section 17.2.3, we can write
SE(t) = Ss(qt)
1 + qt ,
SE(t) = SC(t)Ss(qt)
1 + qt
,
(17.59)
with Ss(t) the S-transform associated with ts(ζ).
A particular case of interest for ﬁnancial applications is when P(s) is an inverse-gamma
distribution. When xt is a Gaussian multivariate vector, one obtains for σtxt a Student
multivariate distribution (see bibliographical notes for more on this topic).
17.4 Empirical Cross-Covariance Matrices
Let us now consider two time series xt and yt, each of length T , but of different
dimensions, respectively N1 and N2. The empirical cross-covariance matrix is an N1×N2
rectangular matrix deﬁned as
Exy = 1
T
T

t=1
xt(yt)T .
(17.60)
Let us assume that the “true” cross-covariance matrix E[xyT ] is zero, i.e. that there are
no true cross-correlations between our two sets of variables. What is the singular value
spectrum of Exy in this case?
As with scm that are described by the Marˇcenko–Pastur law when N,T →∞with
a ﬁxed ratio q = N/T , we expect that some non-trivial results will appear in the limit
N1,N2,T →∞with q1 = N1/T and q2 = N2/T ﬁnite. A convenient way to perform
this analysis is to consider the eigenvalues of the N1 × N1 matrix Mxy = ExyETxy, which
are equal to the square of the singular values s of Exy.
The matrix Mxy shares the same non-zero eigenvalues as those of ExEy, where Ex
and Ey are the dual T × T sample covariance matrices:
Ex = xT x,
Ey = yT y.
(17.61)
Hence one can compute the spectral density of Mxy using the free product formalism and
infer the spectrum of the product ExEy. However, the result will depend on the “true”
covariance matrices of x and y, which are usually unknown in practical applications.
A way to obtain a universal result is to consider the sample-normalized principal
components of x and of y, which we call x and y, such that the corresponding dual
covariance matrix Ex has N1 eigenvalues exactly equal to 1 and T −N1 eigenvalues
exactly equal to zero, whereas Ey has N2 eigenvalues exactly equal to 1 and T −N2
eigenvalues exactly equal to zero. This is precisely the problem studied in Section 15.4.2.
The singular value spectrum of Exy is thus given by
ρ(s) = max(q1 + q2 −1,0)δ(s −1) + Re
*
(s2 −γ−)(γ+ −s2)
πs(1 −s2)
,
(17.62)

17.4 Empirical Cross-Covariance Matrices
279
where γ± are given by
γ± = q1 + q2 −2q1q2 ± 2
*
q1q2(1 −q1)(1 −q2),
0 ≤γ± ≤1.
(17.63)
The allowed s’s are all between 0 and 1, as they should be, since these singular values can
be interpreted as correlation coefﬁcients between some linear combination of the x’s and
some other linear combination of the y’s.
In the limit T →∞at ﬁxed N1, N2, all singular values collapse to zero, as they
should since there are no true correlations between x and y. The allowed band in the limit
q1,q2 →0 becomes
s ∈

| √q1 −√q2|, √q1 + √q2

,
showing that for ﬁxed N1,N2, the order of magnitude of allowed singular values decays
as T −1
2 . The above result allows one to devise precise statistical tests to detect “true”
cross-correlations between sets of variables.
Bibliographical Notes
• The subject of this chapter is treated in several books, see e.g.
– Z. Bai and J. W. Silverstein. Spectral Analysis of Large Dimensional Random Matri-
ces. Springer-Verlag, New York, 2010,
– A. M. Tulino and S. Verd´u. Random Matrix Theory and Wireless Communications.
Now publishers, Hanover, Mass., 2004,
– L. Pastur and M. Scherbina. Eigenvalue Distribution of Large Random Matrices.
American Mathematical Society, Providence, Rhode Island, 2010,
– R. Couillet and M. Debbah. Random Matrix Methods for Wireless Communications.
Cambridge University Press, Cambridge, 2011.
• The initial “historical” paper is of course
– V. A. Marchenko and L. A. Pastur. Distribution of eigenvalues for some sets of
random matrices. Matematicheskii Sbornik, 114(4):507–536, 1967,
with many posterior rediscoveries – as for example in2
– A. Crisanti and H. Sompolinsky. Dynamics of spin systems with randomly asym-
metric bonds: Langevin dynamics and a spherical model. Physical Review A,
36:4922–4939, 1987,
– A. Sengupta and P. P. Mitra. Distributions of singular values for some random matri-
ces. Physical Review E, 60(3):3389, 1999.
• The universality of Eq. (17.5) is discussed in
– J. W. Silverstein and Z. Bai. On the empirical distribution of eigenvalues of a
class of large dimensional random matrices. Journal of Multivariate Analysis,
54(2):175–192, 1995.
• The case of power-law distributed variables and deviations from Marˇcenko–Pastur are
discussed in
2 In fact, Crisanti and Sompolinsky [1987] themselves cite an unpublished work of D. Movshovitz and H. Sompolinsky.

280
Sample Covariance Matrices
– S. Belinschi, A. Dembo, and A. Guionnet. Spectral measure of heavy tailed
band and covariance random matrices. Communications in Mathematical Physics,
289(3):1023–1055, 2009,
– G. Biroli, J.-P. Bouchaud, and M. Potters. On the top eigenvalue of heavy-tailed
random matrices. Europhysics Letters (EPL), 78(1):10001, 2007,
– A. Aufﬁnger, G. Ben Arous, and S. P´ech´e. Poisson convergence for the largest eigen-
values of heavy tailed random matrices. Annales de l’I.H.P. Probabilit´es et statis-
tiques, 45(3):589–610, 2009.
• The case with spatial and temporal correlations was studied in
– A. Sengupta and P. P. Mitra. Distributions of singular values for some random matri-
ces. Physical Review E, 60(3):3389, 1999,
– Z. Burda, J. Jurkiewicz, and B. Wacław. Spectral moments of correlated Wishart
matrices. Physical Review E, 71:026111, 2005.
• Multivariate Student (or elliptical) variables and their associated sample covariance
matrices are studied in
– G. Biroli, J.-P. Bouchaud, and M. Potters. The Student ensemble of correlation matri-
ces: eigenvalue spectrum and Kullback-Leibler entropy. preprint arXiv:0710.0802,
2007,
– N. El Karoui et al. Concentration of measure and spectra of random matrices: Appli-
cations to correlation matrices, elliptical distributions and beyond. The Annals of
Applied Probability, 19(6):2362–2405, 2009.
• Large cross-correlation matrices and corresponding null-hypothesis statistical tests are
studied in
– I. M. Johnstone. Multivariate analysis and Jacobi ensembles: Largest eigenvalue, Tra-
cyWidom limits and rates of convergence. The Annals of Statistics, 36(6):2638–2716,
2008,
– J.-P. Bouchaud, L. Laloux, M. A. Miceli, and M. Potters. Large dimension forecast-
ing models and random singular value spectra. The European Physical Journal B,
55(2):201–207, 2007,
– Y. Yang and G. Pan. Independence test for high dimensional data based on regu-
larized canonical correlation coefﬁcients. The Annals of Statistics, 43(2):467–500,
2015.
• For an early derivation of Eq. (17.62) without the use of free probability methods, see
– K. W. Wachter. The limiting empirical measure of multiple discriminant ratios. The
Annals of Statistics, 8(5):937–957, 1980.

18
Bayesian Estimation
In this chapter we will review the subject of Bayesian estimation, with a particular focus on
matrix estimation. The general situation one encounters is one where the observed matrix
is a noisy version of the “true” matrix one wants to estimate. For example, in the case of
additive noise, one observes a matrix E which is the true matrix C plus a random matrix X
that plays the role of noise, to wit,
E = C + X.
(18.1)
In the case of multiplicative noise, the observed matrix E has the form
E = C
1
2 WC
1
2 .
(18.2)
When W is a white Wishart matrix, this is the problem of sample covariance matrix encoun-
tered in Chapter 17.
In general, the true matrix C is unknown to us. We would like to know the probability
of C given that we have observed E, i.e. compute P(C|E). This is the general subject of
Bayesian estimation, which we introduce and discuss in this chapter.
18.1 Bayesian Estimation
Before doing Bayesian theory on random matrices (see Section 18.3), we ﬁrst review
Bayesian estimation and see it at work on simpler examples.
18.1.1 General Framework
Imagine we have an observable variable y that we would like to infer from the observation
of a related variable x. The variables x and y can be scalars, vectors, matrices, higher
dimensional objects . . . We postulate that we know the random process that generates y
given x, i.e. y could be a noisy version of x or more generally y could be drawn from
a known distribution with x as a parameter. The generation process of y is encoded in a
probability distribution P(y|x), which is called the sampling distribution or the likelihood
function.
281

282
Bayesian Estimation
Given our knowledge of P(y|x), we would like to write the inference probability
P(x|y), also called the posterior distribution. To do so, we can use Bayes’ rule:
P(x|y) = P(y|x)P0(x)
P(y)
.
(18.3)
To obtain the desired probability, Bayes’ rule tells us that we need to know the prior
distribution P0(x). In theory P0(x) is the distribution from which x is drawn and it is
in some cases knowable. In many practical applications, however, x is actually not random
but simply unknown and P0(x) encodes our ignorance of x. It should represent our best
(probabilistic) guess of x before we observe the data y. The determination (or arbitrariness)
of the prior P0(x) is considered to be one of the weak points of the Bayesian approach.
Often P0(x) is just taken to be constant, i.e. no prior knowledge at all on x. However, note
that P0(x) = constant is not invariant upon changes of variables, for if x′ = f (x) is a non-
linear transformation of x, then P0(x′) is no longer constant! In Section 18.1.3, we will see
how the arbitrariness in the choice of P0(x) can be used to simplify modeling.
The other distribution appearing in Bayes’ rule P(y) is actually just a normalization
factor. Indeed, y is assumed to be known, therefore P(y) is just a ﬁxed number that can be
computed by normalizing the posterior distribution. One therefore often simpliﬁes Bayes’
rule as
P(x|y) = 1
Z P(y|x)P0(x),
Z :=

dx P(y|x)P0(x),
(18.4)
where P(y|x) represents the measurement (or noise) process and P0(x) the (often arbitrary)
prior distribution.
From the posterior distribution P(x|y) we can build an estimator of x. The optimal esti-
mator depends on the problem at hand, namely, which quantity are we trying to optimize.
The most common Bayesian estimators are
1 mmse: The posterior mean E[x]y. It minimizes a quadratic loss function and is hence
called the Minimum Mean Square Error estimator.
2 mave: The posterior median or Minimum Absolute Value Error estimator.
3 map: The Maximum A Posteriori estimator, deﬁned as ˆx = argmaxx P(x|y).
18.1.2 A Simple Estimation Problem
Consider the simplest one-dimensional estimation problem:
y = x + ε,
(18.5)
where x is some signal to be estimated, ε is an independent noise, and y is the observation.
Then P(y|x) is simply Pε(.) evaluated at y −x:
P(y|x) = Pε(y −x).
(18.6)

18.1 Bayesian Estimation
283
Suppose further that ε is a centered Gaussian noise with variance σ 2
n , where the subscript
n means “noise”. Then we have
P(y|x) =
1
*
2πσ 2n
exp

−(y −x)2
2σ 2n

.
(18.7)
Then we get that
P(x|y) ∝P0(x) exp
2xy −x2
2σ 2n

,
(18.8)
where P0(x) is the prior distribution of x and we have dropped x-independent factors.
Depending on the choice of P0(x) we will get different posterior distributions and hence
different estimators of x.
Gaussian Prior
Suppose ﬁrst P0(x) is a Gaussian with variance σ 2
s (for signal) centered at x0. Then
P(x|y) ∝exp

−(x −x0)2
2σ 2s
+ 2xy −x2
2σ 2n

=
1
√
2πσ 2 exp

−

x −ˆx
2
2σ 2

,
(18.9)
with
ˆx := x0 + r(y −x0) = (1 −r)x0 + ry;
σ 2 := rσ 2
n,
(18.10)
where the signal-to-noise ratio r is r = σ 2
s /(σ 2
s + σ 2
n ). The posterior distribution is thus a
Gaussian centered around ˆx and of variance σ 2.
For a Gaussian distribution the mean, median and maximum probability values are all
equal to ˆx, which is therefore the optimal estimator in all three standard procedures, mmse,
mave and map. This estimator is called the linear shrinkage estimator as it is linear in
the observed variable y. The linear coefﬁcient of y is the signal-to-noise ratio r, a number
smaller than one that shrinks the observed value towards the a priori mean x0.
Note that this estimator can also be obtained in a completely different framework: it
is the afﬁne estimator that minimizes the mean square error. The estimator is afﬁne by
construction and minimization only involves ﬁrst and second moments; it is therefore not
too surprising that we recover Eq. (18.10), see Exercise 18.1.2. As so often in optimization
problems, assuming Gaussian ﬂuctuations is equivalent to imposing an afﬁne solution.
Another important property of the linear shrinkage estimator is that it is rather conserva-
tive: it is biased towards x0. By assumption x ﬂuctuates with variance σ 2
s and y ﬂuctuates
with variance σ 2
s + σ 2
n . This allows us to compute the variance of the estimator ˆx(y) as
V[ˆx(y)] = r2(σ 2
s + σ 2
n ) =
σ 4
s
σ 2s + σ 2n
≤σ 2
s .
(18.11)

284
Bayesian Estimation
So the variance of the estimator1 is not only smaller than that of the observed variable y it
is also smaller than the ﬂuctuations of the true variable x!
Exercise 18.1.1
Optimal afﬁne estimator
Suppose that we observe a variable y that has some non-zero covariance with
an unknown variable x that we would like to estimate. We will show that the
best afﬁne estimator of x is given by the linear shrinkage estimator (18.10). The
variables x and y can be drawn from any distribution with ﬁnite variance. We
write the general afﬁne estimator
ˆx = ay + b,
(18.12)
and choose a and b to minimize the expected mean square error.
(a)
Initially assume that x and y have zero mean – we will relax this assumption
later. Show that
E
&
(x −ˆx)2'
= a2σ 2
y + b2 + σ 2
x −2aσ 2
xy,
(18.13)
where σ 2
x , σ 2
y and σ 2
xy are the variances of x, y and their covariance.
(b)
Show that the optimal estimator has a = σ 2
xy/σ 2
y and b = 0.
(c)
Compute b in the non-zero mean case by considering x −x0 estimated using
y −y0.
(d)
Compute σ 2
y and σ 2
xy when y = x + ε with ε independent of x.
(e)
Show that when E[ε] = 0 we recover Eq. (18.10).
Bernoulli Prior
When P0(x) is non-Gaussian, the obtained estimators are in general non-linear. As a second
example suppose that P0(x) is Bernoulli random variable with P0(x = 1) = P0(x = −1) =
1/2. Then, after a few simple manipulations one obtains
P(x|y) = 1
2

1 + tanh
 y
σ 2n

δx,1 +

1 −tanh
 y
σ 2n

δx,−1

.
(18.14)
The posterior distribution is now a discrete function that takes on only two values, namely
±1. In this case the maximum probability and the median are such that
ˆxmap(y) = sign(y).
(18.15)
1 One should not confuse the variance of the posterior distribution rσ2n with the variance of the estimator rσ2s . The ﬁrst one
measures the remaining uncertainty about x once we have observed y while the second measures the variability of ˆx(y) when
we repeat the experiment multiple times with varying x and noise ε.

18.1 Bayesian Estimation
285
It is also easy to calculate the mmse estimator:
ˆxmmse(y) = E[x]y = tanh
 y
σ 2n

.
(18.16)
It may seem odd that the mmse estimator takes continuous values between −1 and 1 while
we postulated that the true x can only be equal to ±1. Nevertheless, in order to minimize the
variance it is optimal to shoot somewhere in the middle of −1 and 1 as choosing the wrong
sign costs a lot in terms of variance. The estimator ˆx(y) is biased, i.e. E[ˆxmmse|x]  x. It
also has a variance strictly less than 1, whereas the variance of the true x is unity.
Laplace Prior
As a third example, consider a Laplace distribution
P0(x) = b
2e−b|x|
(18.17)
for the prior, with variance 2b−2. In this case the posterior distribution is given by
P(x|y) ∝exp

−b|x| + 2xy −x2
2σ 2n

.
(18.18)
The mmse and mave estimators can be computed but the results are not very enlightening
as they are given by an ugly combination of error functions and even inverse error functions
(for mave). The map estimator is both simpler and more interesting in this case. It is
given by
ˆxmap(y) =
0
0
for |y| < bσ 2
n,
y −bσ 2
n sign(y)
otherwise.
(18.19)
The map estimator is sparse in the sense that in a non-zero fraction of cases it takes the
exact value of zero. Note that the true variable x itself is not sparse: it is almost surely
non-zero. This example is a toy-model for the “lasso” regularization that we will study in
Section 18.2.2.
Non-Gaussian Noise
The noise in Eq. (18.5) can also be non-Gaussian. When the noise has fat tails, one can even
be in the counter-intuitive situation where the estimator is not monotonic in the observed
variable, i.e. the best estimate of x decreases as a function of its noisy version y. For
example, if x is centered unit Gaussian and ε is a centered unit Cauchy noise, we have
P(x|y) ∝
e−x2/2
(y −x)2 + 1.
(18.20)
Whereas the Cauchy noise ε and the observation y do not have a ﬁrst moment, the
posterior distribution of x is regularized by the Gaussian weight and all its moments

286
Bayesian Estimation
−6
−4
−2
0
2
4
6
y
−1.0
−0.5
0.0
0.5
1.0
[x]y
Figure 18.1 A non-monotonic optimal estimator. The mmse estimator of a Gaussian variable
corrupted by Cauchy noise (see Eq. (18.21)). For small absolute observations y, the estimator is
almost linear with slope 2 −√2/eπ/erfc(1/
√
2) ≈0.475 (dashed line).
are ﬁnite. After some tedious calculation we arrive at the conditional mean or mmse
estimator:
E[x]y = y + Im()
Re(),
where
 = eiyerfc
1 + iy
√
2

.
(18.21)
The shape of the estimator as a function of y is not obvious from this expression but it is
plotted numerically in Figure 18.1. The interpretation is the following:
• When we observe a small (order 1) value of y, we can assume that it was generated by a
moderate x with moderate noise, hence we are in the regime of the linear estimator with
a signal-to-noise ratio close to one-half (ˆx ≈0.475y).
• On the other hand, when y is much larger than the standard deviation of x it becomes
clear that y can only be large because the noise takes extreme values. When the noise is
large our knowledge of x decreases, hence the estimator tends to zero as |y| →∞.
18.1.3 Conjugate Priors
The main weakness of Bayesian estimation is the reliance on a prior distribution for the
variable we want to estimate. In many practical applications one does not have a prob-
abilistic or statistical knowledge of P0(x). The variable x is a ﬁxed quantity that we do
not know, so how are we supposed to know about P0(x)? In such cases we are left with
making a reasonable practical guess. Since P0(x) is just a guess, we can at least choose a
functional form for P0(x) that makes computation easy. This is the idea behind “conjugate
priors”.

18.1 Bayesian Estimation
287
0.0
0.5
1.0
1.5
2.0
2.5
3.0
x
0.0
0.5
1.0
1.5
2.0
P0(x)
a = 2,b = 1
a = 2,b = 2
a = 30,b = 30
Figure 18.2 The inverse-gamma distribution Eq. (18.24). Its mean is given by b/(a −1) and it
becomes increasingly peaked around this mean as both a and b become large.
When we studied the one-dimensional estimation of a variable x corrupted by additive
Gaussian noise (Eq. (18.5), with Gaussian ε) we found that choosing a Gaussian prior for
x gave us a Gaussian posterior distribution. In many other cases, we can ﬁnd a family of
prior distributions that will similarly keep the posterior distribution in the same family. This
concept is better explained in an example.
Imagine we are given a series of T numbers {yi} generated independently from a cen-
tered Gaussian distribution of variance c that is unknown to us. We use the variable c rather
than σ 2 to avoid the confusion between the estimation of σ and that of c = σ 2. The joint
probability of the y’s is given by
P(y|c) =
1
(2πc)T/2 exp

−yT y
2c

.
(18.22)
The posterior distribution is thus given by
P(c|y) ∝P0(c)c−T/2 exp

−yT y
2c

.
(18.23)
Now if the prior P0(c) has the form P0(c) ∝c−a−1e−b/c, the posterior will also be of that
form with modiﬁed values for a and b. Such a P0(c) will thus be our conjugate prior. This
law is precisely the inverse-gamma distribution (see Fig. 18.2):
P0(c) =
ba

(a)c−a−1e−b/c
(c ≥0).
(18.24)
It describes a non-negative variable, as a variance should. It is properly normalized when
a > 0 and has mean b/(a −1) whenever a > 1. If we choose such a law as our variance

288
Bayesian Estimation
prior, the posterior distribution after having observed the vector y is also an inverse-gamma
with parameters
ap = a + T
2
and
bp = b + yT y
2 .
(18.25)
The mmse estimator can then just be read off from the mean of an inverse-gamma distribu-
tion:
E[c]y =
bp
ap −1 =
2b + yT y
2(a −1) + T ,
(18.26)
which can be written explicitly in the form of a linear shrinkage estimator:
E[c]y = (1 −r)c0 + r yT y
T
with
r =
T
2(a −1) + T ,
(18.27)
and c0 = b/(a −1) is the mean of the prior. We see that r →1 when T →∞: in this case
the prior guess on c0 disappears and one is left with the naive empirical estimator yT y/T .
Exercise 18.1.2
Conjugate prior for the amplitude of a Laplace distribution
Suppose that we observe T variables yi drawn from a Laplace distribution
(18.17) with unknown amplitude b. We would like to estimate b using the
Bayesian method with conjugate prior.
(a)
Write the joint probability density of elements of the vector y for a given b.
This is the likelihood function P(y|b).
(b)
As a function of b, the likelihood function has the same form as a gamma
distribution (4.17). Using a gamma distribution with parameters a0 and b0
for the prior on b show that the posterior distribution of b is also a gamma
distribution. Find the posterior parameters ap and bp.
(c)
Given that the mean of a gamma distribution is given by a/b, write the mmse
estimator in this case.
(d)
Compute the estimator in the two limiting cases T = 0 and T →∞.
(e)
Write your estimator from (c) as a shrinkage estimator interpolating
between these two limits. Show that the signal-to-noise ratio r is given by
r = T m/(T m + 2b0) where m =  |yi|/T . Note that in this case the
shrinkage estimator is non-linear in the naive estimate ˆb = 1/(2m).
18.2 Estimating a Vector: Ridge and LASSO
A very standard problem for which Bayesian ideas are helpful is linear regression. Assume
we want to estimate the parameters ai of a multi-linear regression, where we assume that
an observable y can be written as

18.2 Estimating a Vector: Ridge and LASSO
289
y =
N

i=1
aixi + ε,
(18.28)
where xi are N observable quantities and ε is noise (not directly observable). We observe
a time series of y of length T that we stack into a vector y, whereas the different xi are
stacked into an N × T data matrix Hit = xt
i , and ε is the corresponding T -dimensional
noise vector. We thus write
y = HT a + ε,
(18.29)
where a is an N-dimensional vector of coefﬁcients we want to estimate. We assume the
following structure for the random variables x and ε:
1
T E[εεT ] = σ 2
n 1;
1
T E[HHT ] = C,
(18.30)
where C can be an arbitrary covariance matrix, but we will assume it to be the identity 1 in
the following, unless otherwise stated.
Classical linear regression would ﬁnd the coefﬁcient vector a that minimizes the error
E = ∥y −HT a∥2 on a given dataset. As is well known, the regression coefﬁcients are
given by
areg =

HHT −1 Hy.
(18.31)
This equation can be derived easily by taking the derivatives of E with respect to all ai and
setting them to zero. Note that when q := N/T < 1, HHT is in general invertible, but
when q ≥1 (i.e. when there is not enough data), Eq. (18.31) is a priori ill deﬁned.
In a Bayesian estimation framework, we want to write the posterior distribution P(a|y)
and build an estimator of a from it. We expect that the Bayesian approach will work better
than linear regression “out of sample”, i.e. on a new independent sample. The reason is
that the linear regression method minimizes an “in-sample” error, and is thus devised to ﬁt
best the details of the observed dataset, with no regard to overﬁtting considerations. These
concepts will be clariﬁed in Section 18.2.3.
Following the approach of Section 18.1, we write the posterior distribution as
P(a|y) ∝P0(a) exp

−1
2σ 2n
>>y −HT a
>>2

,
(18.32)
where σ 2
n is the variance of the noise ε. Now, the art is to choose an adequate prior
distribution P0(a).
18.2.1 Ridge Regression
The likelihood function in Eq. (18.32) is a Gaussian function of a, so choosing a
Gaussian prior for P0(a) will give us a Gaussian posterior. To construct a Gaussian
distribution for P0(a) we need to choose a prior mean a0 and a prior covariance matrix.

290
Bayesian Estimation
Regression coefﬁcients can be positive or negative, so the most natural prior mean is the
zero vector a0 = 0. In the absence of any other information about the direction in which
a may point, we should make a rotationally invariant prior for the covariance matrix.2 The
only rotationally invariant choice is a multiple of the identity σ 2
s 1 for the prior covariance.
Assuming that the coefﬁcients ai are iid gives the same answer. However, we do not have a
good argument to set the scale of the covariance σ 2
s ; we will come back to this point later.
The posterior distribution is then written
P(a|y) ∝exp

−1
2σ 2n

aT

HHT + σ 2
n
σ 2s
1

a −2aT Hy

.
(18.33)
As announced, the posterior is a multivariate Gaussian distribution. The mmse, mave and
map estimator are all equal to the mode of the distribution, given by3
E[a]y =
HHT
T
+ ζ1
−1 Hy
T ,
ζ := σ 2
n
T σ 2s
.
(18.34)
This is called the “ridge” regression estimator, as it amounts to adding weight on the
diagonal of the sample covariance matrix (HHT )/T . This can also be seen as a shrinkage
of the covariance matrix towards the identity, as we will discuss further in Section 18.3
below.
Another way to understand what ridge regression means is to notice that Eq. (18.31)
involves the inverse of the covariance matrix (HHT )/T , which can be unstable in large
dimensions. This instability can lead to very large coefﬁcients in a. One can thus regularize
the regression problem by adding a quadratic (or L2-norm) penalty for a so the vector does
not become too big:
aridge = argmin
a
&>>y −HT a
>>2 + T ζ ∥a∥2'
.
(18.35)
Setting ζ = 0 we recover the standard regression. The solution of the regularized opti-
mization problem yields exactly Eq. (18.34); it is often called the Tikhonov regularization.
Note that the resulting equation for aridge remains well deﬁned even when q ≥1 as long as
ζ > 0.
In both approaches (Bayesian and Tikhonov regularization) the result depends on the
choice of the parameter ζ = σ 2
n /(T σ 2
s ) which is hard to estimate a priori. The modern way
of ﬁxing ζ in practical applications is by using a validation (or cross-validation) method.
The idea is to ﬁnd the value of aridge on part of the data (the “training set”) and measure the
quality of the regression on another, non-overlapping part of the data (the “validation set”).
The value of ζ is then chosen as the one that gives the lowest error on the validation set.
2 This assumption relies on our hypothesis that the covariance matrix C of the x’s is the identity matrix. Otherwise, the
eigenvectors of C could be used to construct non-rotationally invariant priors.
3 We have introduced a factor of 1/T in the deﬁnition of ζ so it parameterizes the shift in the normalized covariance matrix
(HHT )/T . It turns out to be the proper scaling in the large N limit with q = N/T ﬁxed. Note that if the elements of a and H
are of order one, the variance of the elements of HT a is of order N; for the noise to contribute signiﬁcantly in the large N limit
we must have σ2n of order N and hence ζ of order 1.

18.2 Estimating a Vector: Ridge and LASSO
291
In cross-validation, the procedure is repeated with multiple validation sets (always disjoint
from the training set) and the error is then averaged over these sets.
18.2.2 LASSO
Another common estimating method for vectors is the “lasso” method4 which combines a
Laplace prior with the map estimator.
In this method, the prior distribution amounts to assuming that the coefﬁcients of a are
iid Laplace random number with variance 2b−2. The posterior then becomes
P(a|y) ∝exp

−b
N

i=1
|ai| −
1
2σ 2n
>>y −HT a
>>2

.
As in the toy model Eq. (18.18), the mmse and mave estimators look rather ugly, but the map
one is quite simple. It is given by the maximum of the argument of the above exponential:
alasso = argmin
a
-
2bσ 2
n
N

i=1
|ai| +
>>y −HT a
>>2
.
.
(18.36)
This minimization amounts to regularizing the standard regression estimation with an abso-
lute value penalty (also called L1-norm penalty), instead of the quadratic penalty for the
ridge regression. Interestingly, the solution to this minimization problem leads to a sparse
estimator: the absolute value penalty strongly disfavors small values of |ai| and prefers
to set these values to zero. Only sufﬁciently relevant coefﬁcients ai are retained – lasso
automatically selects the salient factors (this is the “so” part in lasso), which is very useful
for interpreting the regression results intuitively.
Note that the true vector a is not sparse, as the probability to ﬁnd a coefﬁcient ai to
be exactly zero is itself zero for the prior Laplace distribution, which does not contain a
singular δ(a) peak. The sparsity of the lasso estimator alasso is controlled by the param-
eter b. When bσ 2
n →0, the penalty disappears and all the coefﬁcients of the vector a are
non-zero (barring exceptional cases). When bσ 2
n →∞, on the other hand, all coefﬁcients
are zero. In fact, the number of non-zero coefﬁcients is a monotonic decreasing function of
bσ 2
n . As for the parameter ζ for the ridge regression, it is hard to come up with a good prior
value for b, which should be estimated again using validation or cross-validation methods
(Figure 18.3). Finally we note that it is sometimes useful to combine the L1 penalty of
lasso with the L2 penalty of ridge, the resulting estimator is called an elastic net.
18.2.3 In-Sample and Out-of-Sample Error
Standard linear regression is built to minimize the sum of the squared-residuals on the
dataset at hand. We call this error the in-sample error. In many cases, we are interested in
4 lasso stands for Least Absolute Shrinkage and Selection Operator.

292
Bayesian Estimation
0.0
0.5
1.0
1.5
2.0
b
500
1000
1500
2000
2
0.0
0.2
0.4
0.6
0.8
1.0
fzero
2
out
2
in
fzero
Figure 18.3 Illustration of the validation method in lasso regularization. We built a linear model
with 500 coefﬁcients drawn from a Laplace distribution with b = 1 and Gaussian noise σ 2n = 1000.
The model is estimated using T = 1000 unit Gaussian data and validated using a different set of
the same size. The error on the training set is minimal with no regularization and gets worse as b
increases (dashed line, left axis). The validation error (full line, left axis) is minimal for b about equal
to 1. The dotted line (right axis) shows the fraction of the coefﬁcient estimated to be exactly zero;
this number grows from zero (no regularization) to almost 1 (strong regularization).
the predictive power of a linear model and the relevant error is the mean square error on a
new independent but statistically equivalent dataset: the out-of-sample error. If the number
of ﬁtted variables (degree of freedom) is small with respect to the number of samples,
the in-sample error is a good estimator of the out-of-sample error and the standard linear
regression is also the optimal linear predictive model. The situation changes radically when
the number of ﬁtted variables becomes comparable to the number of samples, as we discuss
below.
To summarize, linear regression, regularized or not, addresses two types of task:
• In-sample estimator: we observe some H1 and y1, and estimate a.
• Out-of-sample prediction: we observe some other H2, non-overlapping with H1, and
use them to predict y2 with the in-sample estimate of a.
The result of the in-sample estimation is given by Eq. (18.31), which we write as
areg = E−1b;
E := 1
T H1HT
1,
b := 1
T H1y1.
(18.37)
This is the best in-sample estimator. However, this is not necessarily the case for the out-
of-sample prediction.
Note that both the standard regression and the ridge regression estimator (Eq. (18.34))
are of the form ˆa = −1b with  = E and  = E + ζ1, respectively. We will compute

18.2 Estimating a Vector: Ridge and LASSO
293
in the following the in-sample and out-of-sample estimation error for any estimator of
that nature.
Recalling that E[εεT ] = σ 2
n 1 and after some calculations, one ﬁnds that the in-sample
(R2
in) error is given by
R2
in(ˆa) = 1
T
<
σ 2
n
&
T −2 Tr(−1E) + Tr(−1E−1E)
'
+ aT 
E −2E−1E + E−1E−1E

a
=
.
(18.38)
In the special case  = E, we have
R2
in(areg) = σ 2
n
T (T −N) = (1 −q)σ 2
n,
which is smaller than the true error, which is simply equal to σ 2
n . In fact, the error goes
to zero as q →1, i.e. when the number of parameters becomes equal to the number of
observations. This error reduction is called “overﬁtting”, or in-sample bias: if the task is
to ﬁnd the best model that explains past data, one can do better than the true error. Note
that the above result is quite special, in the sense that it actually does not depend on either
E or a.
Next we calculate the expected out-of-sample (R2
out) error. We draw another matrix H2
of size N × T2 and consider another independent noise vector ε2 of variance σ 2
n and size
T2 (where T2 does not need to be equal to T , it can even be equal to 1). We calculate
R2
out(ˆa) = 1
T2
EH2,ε2
&>>HT
2a + ε2 −HT
2 ˆa
>>2'
= 1
T2
EH2,ε2
->>>>HT
2a + ε2 −HT
2−1E1a −HT
2−1 1
T H1ε1
>>>>
2.
,
(18.39)
where we denote E1 := T −1H1HT
1. We now assume that T −1
2
E[H2HT
2] = C with a general
covariance C.
In the standard regression case,  = E and ˆa = areg and we have
R2
out(areg) = 1
T2
EH2,ε2
->>>>ε2 −HT
2 −1 1
T H1ε1
>>>>
2.
= σ 2
n + σ 2
n
T Tr(E−1C).
(18.40)
Now since E is a sample covariance matrix with true covariance C, we have
Tr(E−1C) = Tr

C−1
2 W−1
q C−1
2 C

= Tr(W−1
q ) ≈
N
1 −q,
(18.41)
where Wq denotes a standard Wishart matrix. Thus we ﬁnd
R2
out(areg) = σ 2
n + qσ 2
n
1 −q =
σ 2
n
1 −q = R2
in(areg)
(1 −q)2 .
(18.42)

294
Bayesian Estimation
As an illustration see Figure 18.3 where without regularization (b = 0) we have indeed
R2
out/R2
in ≈(1 −q)−2 = 4. Thus, we see that we can make precise statements about the
following intuitive inequalities:
in-sample error ≤true error ≤out-of-sample error.
(18.43)
Note that the out-of-sample error tends to ∞as N →T .
Now, let us compute the expected out-of-sample error (R2
out) for the ridge predictor
aridge, parameterized by ζ. The result reads
R2
out(aridge) = σ 2
n + σ 2
n
T Tr(C−1E−1) + ζ 2 Tr(C−1aaT −1),
(18.44)
with  = E + ζ1. Expanding to linear order for small ζ then leads to
R2
out(aridge) = R2
out(areg) −2σ 2
n
T
Tr(CE−2)ζ + O(ζ 2)
= R2
out(areg) −
2σ 2
n q
(1 −q)3 τ(C−1)ζ + O(ζ 2),
(18.45)
where τ(.) = Tr(.)/N and we have used the fact that τ(W−2
q ) = (1 −q)−3. The important
point here is that the coefﬁcient in front of ζ is negative, i.e. to ﬁrst order, the ridge estimator
has a lower out-of-sample error than the naive regression estimator:
ridge estimation error < naive estimation error.
(18.46)
However, the ridge estimator introduces a systematic bias since ∥aridge∥2 < ∥areg∥2 when
ζ > 0. This gives the third term in Eq. (18.44), which becomes large for larger ζ. So one
indeed expects that there should exist an optimal value of ζ (which depends on the speciﬁc
problem at hand) which minimizes the out-of-sample error. We now show how this optimal
out-of-sample error can be elegantly computed in the large N limit.
The Large N Limit
In the large N limit we can recover the fact that the ridge estimator of Section 18.2.1
minimizes the out-of-sample risk without the need of a Gaussian prior on a. We will also
ﬁnd an interesting relation between the Wishart Stieltjes transform and the out-of-sample
risk of the ridge estimator.
In the following we will assume that the elements of the out-of-sample data H2 are iid
with unit variance, i.e. that C = 1. Then when  = E1 + ζ1, Eq. (18.44) becomes, in the
large N limit,
R2
out(aridge) = σ 2
n

1 −qgWq (−ζ)

+ ζ

qσ 2
n −ζ|a|2
g′
Wq (−ζ),
(18.47)
where we have used that E1 is a Wishart matrix free from aaT , and that
τ((z1 −E1)−1) = gWq (z);
τ((z1 −E1)−2) = −g′
Wq (z).
(18.48)
For ζ = 0, we have gWq (0) = −1/(1 −q) and we thus recover Eq. (18.42). In the
large N limit, the out-of-sample error for an estimator with  = E1 + ζ1 depends on the

18.3 Bayesian Estimation of the True Covariance Matrix
295
vector a only through its norm |a|2, regardless of the distribution of its components. The
optimal value of ζ must then also only depend on |a|2.
Now, we know that when a is drawn from a Gaussian distribution, the value ζopt =
σ 2n /(T σ 2s ) is optimal. In the large N limit, |a|2 is self-averaging and equal to Nσ 2s . So
ζopt = qσ 2n /|a|2. We can check directly that this value is optimal by computing the
derivative of Eq. (18.47) with respect to ζ evaluated at ζopt. Indeed we have
σ 2
n qg′
Wq (−ζopt) −ζopt|a|2g′
Wq (−ζopt) = 0.
(18.49)
For the optimal value of ζ we also have
R2
out(aridge) = σ 2
n

1 −qgWq (−ζopt)

,
(18.50)
where gWq (z) is given by Eq. (4.40). Since −gWq (−z) is positive and monotonically
decreasing for z > 0, we recover that the optimal ridge out-of-sample error is smaller than
that of the standard regression.
18.3 Bayesian Estimation of the True Covariance Matrix
We now apply the Bayesian estimation method to covariance matrices. From empirical data,
we measure the sample covariance matrix E, and want to infer the most reliable information
about the “true” underlying covariance matrix C. Hence we write Bayes’ equation for
conditional probabilities for matrices:
P(C|E) ∝P(E|C)P0(C).
(18.51)
We now recall Eq. (4.16) established in Chapter 4 for Gaussian observations:
P (E|C) ∝(det C)−T/2 exp
(
−T
2 Tr

C−1E
)
.
(18.52)
As explained in Section 18.1.3, in the absence of any meaningful prior information, it is
interesting to pick a conjugate prior, which here is of the form
P0 (C) ∝(det C)a exp
&
−b Tr

C−1X
'
(18.53)
for some matrix X, which turns out to be proportional to the prior mean of C. Indeed,
this prior is in fact the probability density of the elements of an inverse-Wishart matrix.
Consider an inverse-Wishart matrix C of size N, T ∗degree of freedom and centered at a
(positive deﬁnite) matrix X. If T ∗> N + 1, C has the density (see Eq. (15.35))
P(C) ∝(det C)−(T ∗+N+1)/2 exp
(
−T ∗−N −1
2
Tr

C−1X
)
.
(18.54)
Note that here T ∗> N is some parameter that is unrelated to the length of the time series
T . The chosen normalization is such that E0[C] = X. As T ∗→∞, we have C →X.
With this prior we thus obtain
P(C|E) ∝(det C)−(T +T ∗+N+1)/2 exp
(
−T
2 Tr

C−1E∗)
,
(18.55)

296
Bayesian Estimation
where we deﬁne
E∗:= E + T ∗−N −1
T
X.
(18.56)
We now notice that (18.55) is, by construction, also a probability density for an inverse-
Wishart with ¯T = T + T ∗, with
E[C|E] =
T E∗
T + T ∗−N −1 = rE + (1 −r)X,
(18.57)
with
r =
T
T + T ∗−N −1.
(18.58)
Hence we recover a linear shrinkage, similar to Eq. (18.10) in the case of a scalar variable
with a Gaussian prior. We will recover this shrinkage formula in the context of rotationally
invariant estimators in the next chapter, see Eq. (19.49).
We end with the following remarks:
• The linear shrinkage works even for the ﬁnite N case, i.e. without the large N hypothesis.
• In general, if one has no idea of what X should be, one can use the identity matrix, i.e.
E[C|E] = rE + (1 −r)1.
(18.59)
Another simple choice is a covariance matrix X corresponding to a one-factor model (see
Section 20.4.2):
Xij = σ 2
s

δij + ρ(1 −δij)

,
(18.60)
where ρ is the average pairwise correlation (which can also be learned using validation).
• Note that T ∗(or equivalently r) is generally unknown. It may be inferred from the data
or learned using validation.
• As we will see in Chapter 20, the linear shrinkage works quite well in ﬁnancial applica-
tions, showing that inverse-Wishart is not a bad prior for the true covariance matrix in
that case (see Fig. 15.1).
Bibliographical Notes
• Some general references on Bayesian methods and statistical inference:
– E. T. Jaynes. Probability Theory: The Logic of Science. Cambridge University Press,
Cambridge, 2003,
– G. James, D. Witten, T. Hastie, and R. Tibshirani. An Introduction to Statistical
Learning: with Applications in R. Springer, New York, 2013.

19
Eigenvector Overlaps and Rotationally Invariant
Estimators
19.1 Eigenvector Overlaps
19.1.1 Setting the Stage
We saw in the ﬁrst two parts of this book how tools from rmt allow one to infer many
properties of the eigenvalue distribution, encoded in the trace of the resolvent of the random
matrix under scrutiny. As in the previous chapters, random matrices of particular interest
are of the form
E = C + X,
or
E = C
1
2 WC
1
2,
(19.1)
where X and W represent some “noise”, for example X might be a Wigner matrix in the
additive case and W a white Wishart matrix in the multiplicative case, whereas C is the
“true”, uncorrupted matrix that one would like to measure. One often calls E the sample
matrix and C the population matrix.
In this section we want to discuss the properties of the eigenvectors of E, and in particu-
lar their relation with the eigenvectors of C. There are, at least, two natural questions about
the eigenvectors of the sample matrix E:
1 How similar are sample eigenvectors [vi]i∈(1,N) of E and the true ones [ui]i∈(1,N) of C?
2 What information can we learn by observing two independent realizations – say
E = C
1
2 WC
1
2 and E′ = C
1
2 W′C
1
2 in the multiplicative case – that remain correlated
through C?
A natural quantity to characterize the similarity between two arbitrary vectors – say χ
and ζ – is the scalar product of χ and ζ. More formally, we deﬁne the “overlap” as χ T ζ.
Since the eigenvectors of real symmetric matrices are only deﬁned up to a sign, it is in
fact more natural to consider the squared overlaps (χ T ζ)2. In the ﬁrst problem alluded to
above, we want to understand the relation between the eigenvectors of the sample matrix
[vi]i∈(1,N) and those of the population matrix [ui]i∈(1,N). The matrix of squared overlaps
is deﬁned as (vi
T uj)2, which actually forms a so-called bi-stochastic matrix (positive
elements with the sums over both rows and columns all equal to unity).
297

298
Eigenvector Overlaps and Rotationally Invariant Estimators
In order to study these overlaps, the central tool is again the resolvent matrix (and not
its normalized trace as for the Stieltjes transform), which we recall is deﬁned as
GA(z) = (z1 −A)−1 ,
(19.2)
for any arbitrary symmetric matrix A. Now, if we expand GE over the eigenvectors v of E,
we obtain that
uT GE(z)u =
N

i=1

uT vi
2
z −λi
,
(19.3)
for any u in RN.
We thus see from Eq. (19.3) that each pole of the resolvent deﬁnes a projection onto
the corresponding sample eigenvectors. This suggests that the techniques we need to apply
are very similar to the ones used above to study the density of states. However, one should
immediately stress that contrarily to eigenvalues, each eigenvector vi for any given i contin-
ues to ﬂuctuate when N →∞and never reaches a deterministic limit. As a consequence,
we will need to introduce some averaging procedure to obtain a well-deﬁned result. We
will thus consider the following quantity:
(λi,μj) := NE[(vT
i uj)2],
(19.4)
where the expectation E can be interpreted either as an average over different realizations
of the randomness or, perhaps more meaningfully for applications, as an average for a
ﬁxed sample over small intervals of sample eigenvalues, of width dλ = η. We choose η in
the range 1 ≫η ≫N−1 (say η = N−1/2) such that there are many eigenvalues in the
interval dλ, while keeping dλ sufﬁciently small for the spectral density to be approximately
constant. Interestingly, the two procedures lead to the same result for large matrices, i.e.
the locally smoothed quantity (λ,μ) is “self-averaging”. A way to do this smoothing
automatically is, as we explained in Chapter 2, to choose z = λi −iη in Eq. (19.3),
leading to
Im uT
j GE(λi −iη)uj ≈πρE(λi) × (λi,μj),
(19.5)
provided η is in the range 1 ≫η ≫N−1. Note that we have replaced N

uT
j vi
2 with
(λi,μj), to emphasize the fact that we expect typical square overlaps to be of order 1/N,
such that  is of order unity when N →∞. This assumption will indeed be shown to hold
below. In fact, when uj and vi are completely uncorrelated, one ﬁnds (λi,μj) = 1.
For the second question, the main quantity of interest is, similarly, the (mean squared)
overlap between the eigenvectors of two independent noisy matrices E and E′:
(λi,λ′
j) := NE[(vT
i v′
j)2],
(19.6)
where [λ′
i]i∈(1,N) and [v′
i]i∈(1,N) are the eigenvalues and eigenvectors of E′, i.e. another
sample matrix that is independent from E but with the same underlying population

19.1 Eigenvector Overlaps
299
matrix C. In order to get access to (λi,λ′
j) deﬁned in Eq. (19.5), one should consider
the following quantity:
ψ(z,z′) = 1
N Tr

GE(z)GE′(z′)

.
(19.7)
After simple manipulations one readily obtains a generalized Sokhotski–Plemelj formula,
where η is such that 1 ≫η ≫N−1:
Re

ψ(λi −iη,λ′
i + iη) −ψ(λi −iη,λ′
i −iη)

≈2π2ρE(λi)ρE′(λ′
i)(λi,λ′
j). (19.8)
This representation allows one to obtain interesting results for the overlaps between the
eigenvectors of two independently drawn random matrices, see Eq. (19.14).
19.1.2 Overlaps in the Additive Case
Now, we can use the subordination relation for the resolvent of the sum of two free matrices
established in Chapter 13, Eq. (13.44), which in the present case reads
E[GE(z)] = GC (z −RX(gE(z))) .
(19.9)
Since we choose uj to be an eigenvector of C with eigenvalue μj, one ﬁnds
uT
j GE(λi −iη)uj =
1
λi −iη −RX (gE(λi −iη)) −μj
,
(19.10)
where we have dropped the expectation value as the left hand side is self-averaging when η
is in the correct range. The imaginary part of this quantity, calculated for η →0, gives
access to (λi,μj). The formula simpliﬁes in the common case where the noise matrix X
is a Wigner matrix, such that RX(z) = σ 2z. In this case, one ﬁnally obtains a Lorentzian
shape for the squared overlaps:
(λ,μ) =
σ 2
(μ −λ + σ 2 hE(λ))2 + σ 4π2ρE(λ)2,
(19.11)
where we have decomposed the Stieltjes transform into its real and imaginary parts as
gE(x) = hE(x) + iπρE(x); note that hE(x) is equal to π times the Hilbert transform of
ρE(x).
In Figure 19.1, we illustrate this formula in the case where C is a Wigner matrix with
parameter σ 2 = 1. For a ﬁxed λ, the overlap peaks for
μ = λ −σ 2 hE(λ),
(19.12)
with a width ∼σ 2ρE(λ). When σ →0, i.e. in the absence of noise, one recovers
(λ,μ) →δ(λ −μ),
(19.13)
as expected since in this case the eigenvectors of E are trivially the same as those of C.
Note that apart from the singular case σ = 0, (λ,μ) is found to be of order unity when
N →∞. But because of the factor N in Eq. (19.6), the overlaps between vi and uj are of
order N−1/2 as soon as σ > 0.

300
Eigenvector Overlaps and Rotationally Invariant Estimators
−3
−2
−1
0
1
2
3
l
0
1
2
3
4
5
6
F(l, m)
simulation m = −1.5
simulation m = 0.5
theory m = −1.5
theory m = 0.5
Figure 19.1 Normalized squared-overlap function (λ,μ) for E = X1 + X2, the sum of two unit
Wigner matrices, compared with a numerical simulation for μ = −1.5 and μ = 0.5. The simulations
are for a single sample of size N = 2000, each data point corresponds to N times the square overlap
between an eigenvector of E and one of X1 averaged over eigenvectors with eigenvalues within
distance η = 2/
√
N of μ.
Now suppose that E = C + X and E′ = C + X′, where X and X′ are two independent
Wigner matrices with the same variance σ 2. Using Eq. (19.8), one can compute the
expected overlap between the eigenvectors of E and E′. After a little work, one can estab-
lish the following result for (λ,λ), i.e. the typical overlap around the same eigenvalues
for E and E′:
(λ,λ) =
σ 2
2f2(λ)2
∂λf1(λ)
(∂λf1(λ))2 + (∂λf2(λ))2,
(19.14)
where
f1(λ) = λ −σ 2 hE(λ);
f2(λ) = σ 2πρE(λ);
hE(λ) := Re[gE(λ)].
(19.15)
Note that in the large N limit, gE(z) = gE′(z).
The formula for (λ,λ′) is more cumbersome; for a ﬁxed λ′, one ﬁnds again a humped
shaped function with a maximum at λ′ ≈λ. The most striking aspect of this formula,
however, is that only gE(z) (which is measurable from data) is needed to compute the
expected overlap (λ,λ′); the knowledge of the “true” matrix C is not needed to judge
whether or not the observed overlap between the eigenvectors of E and E′ is compatible
with the hypothesis that such matrices are both noisy versions of the same unknown C.
19.1.3 Overlaps in the Multiplicative Case
We now repeat the same steps in the case where E = C
1
2 WqC
1
2 , where Wq is a Wishart
matrix of parameter q. We know that in this case the matrix subordination formula reads as
(13.47), which can be rewritten as

19.2 Rotationally Invariant Estimators
301
GE(z) = Z(z)
z
GC(Z(z)),
with
Z =
z
1 −q + qzgE(z).
(19.16)
This allows us to compute
uT
j GE(λi −iη)uj = Z(λi −iη)
λi −iη
1
Z(λi −iη) −μj
,
(19.17)
and ﬁnally, taking the imaginary part in the limit η →0+,
(λ,μ) =
qμλ
(μ(1 −q) −λ + qμλ hE(λ))2 + q2μ2λ2π2ρ2
E(λ),
(19.18)
where, again, hE denotes the real part of the Stieltjes transform gE. Note that in the limit
q →0, (λ,μ) becomes more and more peaked around λ ≈μ, with an amplitude
that diverges for q = 0. Indeed, in this limiting case, one should ﬁnd that the sample
eigenvectors vi become equal to the population ones ui. More generally, (λ,μ) for a
ﬁxed μ has a Lorentzian humped shape as a function of λ, which peaks for λ ≈μ.
Now suppose that E = C
1
2 WqC
1
2 and E′ = C
1
2 W′qC
1
2 , where Wq and W′q are two
independent Wishart matrices with the same parameter q. Using Eq. (19.8), one can again
compute the expected overlap between the eigenvectors of E and E′. The ﬁnal formula is
however too cumbersome to be reported here, see Bun et al. [2018]. The formula simpliﬁes
in the limit where C is close to the identity matrix, in the sense that τ(C2) = 1 + ϵ, with
ϵ →0. In this case:
(λ,λ′) = 1 + ϵ [2 hE(λ) −1] 
2 hE(λ′) −1

+ O(ϵ2).
(19.19)
More generally, the squared overlaps only depend on gE(z) (which is measurable from
data). Again, the knowledge of the “true” matrix C is not needed to judge whether or not
the observed overlap between the eigenvectors of E and E′ is compatible with the hypoth-
esis that such matrices are both noisy versions of the same unknown C. This is particularly
important in ﬁnancial applications, where E and E′ may correspond to covariance matrices
measured on two non-overlapping periods. In such a case, the hypothesis that the true C is
indeed the same in the two periods may not be warranted and can be directly tested using
the overlap formula.
19.2 Rotationally Invariant Estimators
19.2.1 Setting the Stage
The results derived above concerning the overlaps between the eigenvectors of sample
E and population (or “true”) C matrices allow one to construct a rotationally invariant
estimator of C knowing E. The idea can be framed within the Bayesian approach of the
previous chapter, when the prior knowledge about C is mute about the possible directions
in which the eigenvectors of C are pointing. More formally, this can be expressed by saying
that the prior distribution P0(C) is rotation invariant, i.e.
P0(C) = P0(OCOT ),
(19.20)

302
Eigenvector Overlaps and Rotationally Invariant Estimators
where O is an arbitray rotation matrix. Examples of rotationally invariant priors are pro-
vided by the orthogonal ensemble introduced in Chapter 5, where P0(C) only depends on
Tr(C).
Now, since the posterior probability of C given E is given by
P(C|E) ∝(det C)−T/2 exp
(
−T
2 Tr

C−1E
)
P0(C),
(19.21)
it is easy to verify that the mmse estimator of C transforms in the same way as E under an
arbitrary rotation O, i.e.
E[C|OEOT ] =

C P(C|OEOT )P0(C)DC
= O
(
C P(C|E)P0(C)DC
)
OT
= OE(C|E)OT,
(19.22)
using the change of variable C = OT CO, and the explicit form of P(C|E) given in Eq.
(19.21).
More generally, if we call (E) an estimator of C given E, this estimator is rotationally
invariant if and only if
(OEOT ) = O(E)OT,
(19.23)
for any orthogonal matrix O. This means in words that, if the scm E is rotated by some O,
then our estimation of C must be rotated in the same fashion. Intuitively, this is because we
have no prior assumption on the eigenvectors of C, so the only special directions in which
C can point are those singled out by E itself. Estimators abiding by Eq. (19.23) are called
rotationally invariant estimators (rie).
An alternative interpretation of Eq. (19.23) is that (E) can be diagonalized in the same
basis as E, up to a ﬁxed rotation matrix  . But consistent with our rotationally invariant
prior on C, there is no natural guess for  , except the identity matrix 1. Hence we conclude
that (E) has the same eigenvectors as those of E, and write
(E) =
N

i=1
ξivivT
i ,
(19.24)
where vi are, as above, the eigenvectors of E, and where ξi is a function of all empirical
eigenvalues [λj]j∈(1,N). We now show how these ξi can be optimally chosen, and opera-
tionally computed from data in the limit N →∞.
19.2.2 The Optimal RIE
Suppose we ask the following question: what is the optimal choice of ξi such that (E),
deﬁned by Eq. (19.24), is as close as possible to the true C? If the eigenvectors of E were

19.2 Rotationally Invariant Estimators
303
equal to those of C, i.e. if vi = ui, ∀i, the solution would trivially be ξi = μi. But in
the case where vi  ui, the solution is a priori non-trivial. So we want to minimize the
following least-square error:
Tr((E) −C)2 =
N

i=1
vT
i ((E) −C)2vi =
N

i=1

ξ2
i −2ξivT
i Cvi + vT
i C2vi

.
(19.25)
Minimizing over ξk and noting that the third term in the equation above is independent of
the ξ’s, it is easy to get the following expression for the optimal ξk:
ξk = vT
kCvk.
(19.26)
This is all very well but seems completely absurd: we assume that we do not know the true
C and want to ﬁnd the best estimator of C knowing E, and we ﬁnd an equation for the ξ
that we cannot compute unless we know C.
Because Eq. (19.26) requires in principle knowledge we do not have, it is often called
the “oracle” estimator. But as we will see in the next section, the large N limit allows one
to actually compute the optimal ξ’s from the data alone, without having to know C.
19.2.3 The Large Dimension Miracle
Let us ﬁrst rewrite Eq. (19.26) in terms of the overlaps introduced in Section 19.1. Expand-
ing over the eigenvectors of C we ﬁnd
ξk =
N

j=1
vT
kujμjuT
j vk =
N

j=1
μj

uT
j vk
2
(19.27)
−→
N→∞

dμ ρC(μ)μ(λk,μ),
(19.28)
where λk is the eigenvalue of the sample matrix E associated with vk. In other words, ξk
is an average over the eigenvalues of C, weighted by the square overlaps (λk,μ). Now,
using Eq. (19.5), we can also write
ξk =
N

j=1
μj

uT
j vk
2
=
1
πρE(λk) lim
η→0+ Im
N

j=1
uT
j μjGE(λk −iη)uj
=
1
πρE(λk) lim
η→0+ Im τ (CGE(λk −iη)) .
(19.29)
We now use the fact that in both the additive and the multiplicative case, the matrices GE
and GC are related by a subordination equation of the type
GE(z) = Y(z)GC(Z(z)),
(19.30)

304
Eigenvector Overlaps and Rotationally Invariant Estimators
with Y(z) = 1 in the additive case and Y(z) = Z(z)/z in the multiplicative case. Hence we
can write the following series of equalities:
τ (CGE(z)) = Yτ (CGC(Z)) = Yτ

(C −Z1 + Z1)(Z1 −C)−1
= YZgC(Z) −Y = Z(z)gE(z) −Y(z).
(19.31)
But since Z(z) only depend on gE(z), we see that the ﬁnal formula for ξk does not explicitly
depend on C anymore and reads
ξk =
1
πρE(λk) lim
η→0+ Z(zk)gE(zk) −Y(zk),
zk := λk −iη.
(19.32)
Since all the quantities on the right hand side can be estimated from the data alone, this
formula will lend itself to real world applications. Let us ﬁrst explore this formula for two
simple cases, for an additive model and for a multiplicative model.
19.2.4 The Additive Case
For a general noise matrix X, one has Z(z) = z −RX(gE(z)), leading to the following
mapping between the empirical eigenvalues λ and the rie eigenvalues ξ:
ξ(λ) = λ −limη→0+ Im RX(gE(z))gE(z)
limη→0+ Im gE(z)
,
z = λ −iη.
(19.33)
If there is no noise, i.e. X = 0 and hence RX = 0, we ﬁnd as expected ξ(λ) = λ. If X is
small, then
RX(x) = ϵx + · · · ,
(19.34)
where we have assumed τ(X) = 0 and ϵ = τ(X2) is small. Hence we ﬁnd
ξ(λ) = λ −2ϵ hE(λ) + · · · .
(19.35)
A natural case to consider is when X is Wigner noise, for which RX(x) = σ 2
n x exactly,
such that the equation above is exact with ϵ = σ 2
n , for arbitrary values of σn. When C
is another Wigner matrix with variance σ 2
s , then E is clearly also a Wigner matrix with
variance σ 2 = σ 2
n + σ 2
s . In this case, when −2σ < λ < 2σ,
hE(λ) =
λ
2σ 2 .
(19.36)
Hence we obtain, from Eq. (2.38),
ξ(λ) = λ −λσ 2
n
σ 2 = rλ,
r := σ 2
s
σ 2,
(19.37)

19.2 Rotationally Invariant Estimators
305
which is the linear shrinkage obtained for Gaussian variables in Chapter 18. In fact, this
shrinkage formula is expected elementwise, since all elements are Gaussian random vari-
ables:1
(E)ij = r Eij,
(19.38)
see Eq. (18.10) with x0 = 0.
Exercise 19.2.1
Additive RIE for the sum of two matrices from the same distri-
bution
In this exercise we will ﬁnd a simple form for a rie estimator when the noise
is drawn from the same distribution as the signal, i.e.
E = C + X,
(19.39)
with X and C mutually free matrices drawn from the same ensemble.
(a)
Write a relationship between RX(g) and RE(g).
(b)
Given that gE(z)RE(gE(z)) = zgE(z) −1, what is gE(z)RX(gE(z))?
(c)
Use Eq. (19.33) and the fact that z is real in the limit η →0+ to show that
ξ(λ) = λ/2.
(d)
Given that  = E[C]E (see Section 19.4), ﬁnd a simple symmetry argument
to show that  = E/2.
(e)
Generate numerically two independent symmetric orthogonal matrices M1
and M2 with N = 1000 (see Exercise 1.2.4). Compute the eigenvalues λk
and eigenvectors vk of the sum of these two matrices.
(f)
Plot the normalized histogram of the λk’s and compare with the arcsine law
between −2 and 2 (ρ(λ) = 1/(π
√
4 −λ2)).
(g)
Make a scatter plot vT
kM1vk vs λk and compare with λ/2.
19.2.5 The Multiplicative Case
We can now tackle the multiplicative case, which includes the important practical problem
of estimating the true covariance matrix given a sample covariance matrix. In the mul-
tiplicative case, it is more elegant to use the subordination relation Eq. (13.46) for the
T-matrix rather than for the resolvent. In the present setting we thus write
TE(z) = TC[z SW(tE(z))].
(19.40)
1 Note however that there is a slight subtlety here: the linear shrinkage equation (19.37) only holds in the absence of outliers, i.e.
empirical eigenvalues that fall outside the interval (−2σ,2σ). For such eigenvalues, shrinkage is non-linear. For a similar
situation in the multiplicative case, see Figure 19.2.

306
Eigenvector Overlaps and Rotationally Invariant Estimators
In terms of T-transforms, Eq. (19.29) reads
ξ(λ) = limη→0+ Im τ(CTC[z SW(tE(z))])
limη→0+ Im tE(z)
;
z = λ −iη.
(19.41)
Since TC(z) = C(z1 −C)−1, we have, with t = tE(z) as a shorthand,
τ [CTC(zSW(t))] = τ
&
C2(zSW(t)1 −C)−1'
= τ
&
C(C −zSW(t)1 + zSW(t)1)(zSW(t)1 −C)−1'
= τ(C) + zSW(t)tC(zSW(t))
= τ(C) + zSW(t)tE(z).
(19.42)
The ﬁrst term τ(C) is real and does not contribute to the imaginary part that we have to
compute, so we obtain
ξ(λ) = λlimη→0+ Im SW(tE(z))tE(z)
limη→0+ Im tE(z)
,
z = λ −iη.
(19.43)
Equation (19.43) is very general. It applies to sample covariance matrices where the noise
matrix W is a white Wishart, but it also applies to more general multiplicative noise
processes.
In the special case of sample covariance matrices E = C
1
2 WqC
1
2 with N/T = q, we
know that SWq(t) = (1 + qt)−1. In the bulk region λ−< λ < λ+, t = tE(z) is complex
with non-zero imaginary part when z = λ −iη. Hence
ξ(λ) = λ
limη→0+ Im
t
1+qt
limη→0+ Im t
=
λ
|1 + qtE(λ −iη)|2

η→0
,
(19.44)
where we have used the fact that
Im
t
1 + qt = Im t(1 + qt⋆)
|1 + qt|2 = Im t + q|t|2
|1 + qt|2 =
1
|1 + qt|2 Im t.
(19.45)
Equation (19.44) can be interpreted as a form of non-linear shrinkage. A way to see this is
to note that below λ−and above λ+ (the edges of the sample spectrum) tE(λ) is real. From
the very deﬁnition,
tE(λ) =
 λ+
λ−
dλ′ρE(λ′)
λ′
λ −λ′ = λgE(λ) −1,
(19.46)
for any λ outside or at the edges of the spectrum. Hence, since λ−≥0 for covariance
matrices, tE(λ−) < 0 and tE(λ+) > 0. Hence, one directly establishes that the support of
the rie (E) is narrower than that of E:
ξ(λ−) ≥λ−;
ξ(λ+) ≤λ+,
(19.47)
where the inequalities are saturated for q = 0, in which case, as expected ξ(λ) = λ,
∀λ ∈(λ−,λ+). A more in-depth discussion of the properties of Eq. (19.44) is given in
Section 19.3.

19.2 Rotationally Invariant Estimators
307
0
1
2
3
4
l
0.0
0.5
1.0
1.5
2.0
2.5
3.0
x(l)
RIE
linear
Figure 19.2 The rie estimator (19.44) for a true covariance matrix given by an inverse-Wishart of
variance p = 0.25 observed using data with aspect ratio q = 0.25. On the support of the sample
density (λ ∈[0.17,3.33]), the rie matches perfectly the linear shrinkage estimator (19.49) (with
r = 1/2), but it is different from it outside of the expected spectrum.
Using Eq. (19.46), the shrinkage equation (19.44) can be rewritten as
ξ(λ) =
λ
|1 −q + qλgE(λ −iη)|2

η→0+ ,
(19.48)
a result ﬁrst derived in Ledoit and P´ech´e [2011].
Equation (19.44) considerably simpliﬁes in the case where the true covariance matrix C
is an inverse-Wishart matrix of parameter p. Injecting the explicit form of tE(z) given by
Eq. (15.50) into Eq. (19.44) leads, after simple manipulations, to
ξ(λ) = q + λp
p + q = rλ + (1 −r);
r :=
p
p + q,
(19.49)
i.e. exactly the linear shrinkage result derived in a Bayesian framework in Section 18.3.
Note that the result Eq. (19.49) only holds between λ−and λ+, given in Eq. (15.52). The
full function ξ(λ) when C is an inverse-Wishart matrix is given in Figure 19.2.
Exercise 19.2.2
RIE when the true covariance matrix is Wishart
Assume that the true covariance matrix C is given by a Wishart matrix with
parameter q0. This case is a tractable model for which the computation can be
done semi-analytically (we will get cubic equations!).
We observe a sample covariance matrix E over T = qN time intervals. E is
the free product of C and another Wishart matrix of parameter q:
E = C
1
2 WC
1
2 .
(19.50)

308
Eigenvector Overlaps and Rotationally Invariant Estimators
(a)
Given that the S-transform of the true covariance is SC(t) = 1/(1 + q0t) and
the S-transform of the Wishart is SW(t) = 1/(1 + qt), use the product of
S-transforms for the free product and Eq. (11.92) to write an equation for
tE(z). It should be a cubic equation in t.
(b)
Using a numerical polynomial solver (e.g. np.roots) solve for tE(z) for z real
between 0 and 4, choose q0 = 1/4 and q = 1/2. Choose the root with positive
imaginary part. Use Eqs. (11.89) and (2.47) to ﬁnd the eigenvalue density and
plot this density. The edge of the spectrum should be (slightly below) 0.05594
and (slightly above) 3.746.
(c)
For λ in the range [0.05594, 3.746] plot the optimal cleaning function (use the
same solution tE(z) as in (b)):
ξ(λ) =
λ
|1 + qt(λ)|2 .
(19.51)
(d)
For N = 1000 numerically generate C (q0 = 1/4), two versions of W1,2
(q = 1/2) and hence two versions of E1,2 := C
1
2 W1,2C
1
2 . E1 will be the
“in-sample” matrix and E2 the “out-of-sample” matrix. Check that τ(C) =
τ(W1,2) = τ(E1,2) = 1 and that τ(C2) = 1.25, τ(W2
1,2) = 1.5 and
τ(E2
1,2) = 1.75.
(e)
Plot the normalized histogram of the eigenvalues of E1, it should match your
plot in (b).
(f)
For every eigenvalue, eigenvector pair (λk,vk) of E1 compute ξval(λk) :=
vT
kE2vk. Plot ξval(λk) vs λk and compare with your answer in (c).
Exercise 19.2.3
Multiplicative RIE when the signal and the noise have the same
distribution
(a)
Adapt the arguments of Exercise 19.2.1 to the multiplicative case with C and
W two free matrices drawn from the same ensemble. Show that in this case
ξ(λ) =
√
λ.
(b)
Redo Exercise 19.2.2 with q = q0 = 1/4, compare your ξ(λ) with
√
λ.
19.2.6 RIE for Outliers
So far, we have focused on “cleaning” the bulk eigenvectors. But it turns out that the
formulas above are also valid for outliers of C that appear as outliers of E. One can show
that, outside the bulk, gE(z) and tE(z) are analytic on the real axis and thus, for small η,
Im gE(λ −iη) = −ηg′
E(λ),
Im tE(λ −iη) = −ηt′
E(λ).
(19.52)
Then Eqs. (19.33) and (19.43) simplify to

19.3 Properties of the Optimal RIE for Covariance Matrices
309
ξ(λ) = λ −d
dg

gRX(g)

,
g = gE(λ),
ξ(λ) = λ d
dt [tSW(t)],
t = tE(λ),
(19.53)
respectively for the additive and multiplicative cases.
19.3 Properties of the Optimal RIE for Covariance Matrices
Even though the optimal non-linear shrinkage function (19.44), (19.48) seems relatively
simple, it is not immediately clear what is the effect induced by the transformation λi →
ξ(λi). In this section, we thus give some quantitative properties of the optimal estimator 
to understand the impact of the optimal non-linear shrinkage function.
First let us consider the moments of the spectrum of . From Eqs. (19.24) and (19.26)
we immediately derive that
Tr  =

j=1
μjuT
j

i=1
vivT
i

uj = Tr C,
(19.54)
meaning that the cleaning operation preserves the trace of the population matrix C, as it
should do. For the second moment, we have
Tr 2 =
N

j,k=1
μjμk
N

i=1
(vT
i uj)2(vT
i uk)2.
Now, if we deﬁne the matrix Ajk as N
i=1(vT
i uj)2(vT
i uk)2 for j,k = 1,N, it is not hard to
see that it is a matrix with non-negative entries and whose rows all sum to unity (remember
that all vi’s are normalized to unity). The matrix A is therefore a (bi-)stochastic matrix
and the Perron–Frobenius theorem tells us that its largest eigenvalue is equal to unity (see
Section 1.2.2). Hence, we deduce the following general inequality:
N

j,k=1
Aj,k μjμk ≤
N

j=1
μ2
j,
which implies that
Tr 2 ≤Tr C2 ≤Tr E2,
(19.55)
where the last inequality comes from Eq. (17.11). In words, this result states that the
spectrum of  is narrower than the spectrum of C, which is itself narrower than the
spectrum of E. The optimal rie therefore tells us that we had better be even more cautious
than simply bringing back the sample eigenvalues to their estimated true locations. This is
because we have only partial information about the true eigenbasis of C. In particular, one
should always shrink downward (resp. upward) the small (resp. top) eigenvalues compared
to their true locations μi for any i ∈(1,N), except for the trivial case C = 1.

310
Eigenvector Overlaps and Rotationally Invariant Estimators
Next, we consider the asymptotic behavior of the optimal non-linear shrinkage function
(19.44), (19.48). Throughout the following, suppose that we have an outlier at the left of
the lower bound of ρE and let us assume q < 1 so that E has no exact zero mode. We know
from Section 19.2.6 that the estimator (19.44) holds for outliers. Moreover, we have that
λgE(λ) = O(λ) for λ →0. This allows us to conclude from Eq. (19.26) that, for outliers
very close to zero,
ξ(λ) =
λ
(1 −q)2 + O(λ2),
(19.56)
which is in agreement with Eq. (19.55): small eigenvalues must be pushed upwards for
q > 0.
The other asymptotic limit λ →∞is also useful since it gives us the behavior of the
non-linear shrinkage function ξ for large outliers. In that case, we know from Eq. (17.8)
that limλ→∞λtE(λ) ∼λ−1τ(E). Therefore, we conclude that
ξ(λ) ≈
λ

1 + qλ−1τ(E) + O(λ−2)
2 ≈λ −2qτ(E) + O(λ−1).
(19.57)
If all variances are normalized to unity such that τ(E) = τ(C) = 1, then we simply obtain
ξ(λ) ≈λ −2q + O(λ−1).
(19.58)
It is interesting to compare this with Eq. (14.54) for large rank-1 perturbations, which gives
λ ≈μ + q for λ →∞. As a result, we deduce from Eq. (19.58) that ξ(λ) ≈μ −q and we
therefore ﬁnd the following ordering relation:
ξ(λ) < μ < λ,
(19.59)
for isolated and large eigenvalues λ and for q > 0. Again, this result is in agree-
ment with Eq. (19.55): large eigenvalues should be reduced downward for any q > 0,
even below the “true” value of the outlier μ. More generally, the non-linear shrink-
age function ξ interpolates smoothly between λ/(1 −q)2 for small λ to λ −2q for
large λ.
19.4 Conditional Average in Free Probability
In this section we give an alternative derivation of the rie formula, Eq. (19.29). This
derivation is more elegant, albeit more abstract. In particular, it does not rely on the
computation of eigenvector overlap, so by itself it misses the important link between the
rie and the computation of overlaps.
In the context of free probability, we work with abstract objects (E, C, etc.) that satisfy
the axioms of Chapter 11. We can think of them as inﬁnite-dimensional matrices. We are
given the matrix E that was obtained by free operations from an unknown matrix C. For
instance it could be given by a combination of free product and free sum.
The matrix E is generated from the matrix C; in this sense, E depends on C. We would
like to ﬁnd the best estimator (in the least-square sense) of C given E. It is given by the
conditional average
 = E[C]E.
(19.60)

19.5 Real Data
311
In this abstract context, the only object we know is E so  must be a function of E. Let us
call this function (E). The fact that  is a function of E only imposes that  commutes
with E, i.e. that  is diagonal in the eigenbasis of E. One way to determine the function
(E) is to compute all possible moments of the form mk = τ[(E)Ek]. They can be
combined in the function
F(z) := τ
&
ξ(E)(z1 −E)−1'
(19.61)
via its Taylor series at z →∞. Using Eq. (19.60), we write
F(z) = τ
&
E[C]|E (z1 −E)−1'
.
(19.62)
But the operator τ[.] contains the expectation value over all variables, both trace and
randomness. So by the law of total expectation, τ(E[.]) = τ(.) and
F(z) = τ
&
C(z1 −E)−1'
.
(19.63)
To recover the function ξ(λ) from F(z) we use a spectral decomposition of E:
F(z) =

ρE(λ) ξ(λ)
z −λdλ,
(19.64)
so
lim
η→0+ Im F(λ −iη) = πρE(λ)ξ(λ),
(19.65)
which is equivalent to
ξ(λ) =
lim
η→0+
Im F(λ −iη)
Im gE(λ −iη),
(19.66)
itself equivalent to Eq. (19.29).
19.5 Real Data
As stated above, the good news about the rie estimator is that it only depends on transforms
of the observable matrix E, such as gE(z) and tE(z) and the R- or S-transform of the noise
process. One may think that real world applications should be relatively straightforward.
However, we need to know the behavior of the limiting transforms on the real axis, precisely
where the discrete N transforms gN(z) and tN(z) fail to converge.
We will discuss here how to compute these transforms using either a parametric ﬁt or
a non-parametric approximation on the sample eigenvalues. In both cases we will tackle
the multiplicative case with a Wishart noise but the discussion can be adapted to cover
the additive case or any other type of noise. In Section 19.6 we will discuss an alternative
approach using two datasets (or disjoint subsets of the original data).
19.5.1 Parametric Approach
Ansatz on ρC or SC
One can postulate a convenient functional form for ρC(λ) and ﬁt the associated parameters
on the data. This allows one to obtain analytical formulas for all the relevant transforms,
from which one can extract the exact behavior on the real axis.

312
Eigenvector Overlaps and Rotationally Invariant Estimators
The simplest (most tractable) choice for ρC(λ) is the inverse-Wishart distribution. In
this case ρE(λ) can be computed exactly (see Eq. (15.51)) and the optimal estimator is
linear within the bulk of the spectrum, cf. Eq. (19.49). When the sample covariance matrix
is normalized such that τ(E) = 1, the inverse-Wishart has a single parameter p that
needs to be estimated from the data. As an estimate, one can use for example the second
moment of E:
τ

E2
= 1 + p + q,
(19.67)
or its ﬁrst inverse moment:
τ

E−1
= 1 + p
1 −q ,
(19.68)
which is obtained using Eq. (15.13) with SE(t) = (1 −pt)/(1 + qt), or simply by
noting that τ(W−1
q
W
−1
p ) = τ(W−1
q )τ(
W
−1
p ) for free matrices, and using the results of
Sections 15.2.2 and 15.2.3.
When the distribution of sample eigenvalues appears to be bounded from above and
below, one can use a more complicated but still relatively tractable ansatz for ρC(λ), by
postulating a simple form for its S-transform. For example using
SC(t) = (1 −p1t)(1 −p2t)
1 + q1t
⇔
SE(t) = (1 −p1t)(1 −p2t)
(1 + qt)(1 + q1t) ,
(19.69)
one ﬁnds that tE(ζ) (and hence ρE(λ)) is the solution of a cubic equation. Higher order
terms in t in the numerator or the denominator will give higher order equations for tE(ζ).
The parameters p1, p2, q1, etc. can be evaluated from the ﬁrst few moments and inverse
moments of E or by ﬁtting the observed density of eigenvalues. However, the particularly
convenient choice Eq. (19.69) does not work when the observed distribution of eigenvalues
does not have enough skewness, as in the example shown in Figure 19.3.
Parametric Fit of ρE
Another approach consists of postulating a form for the density of sample eigenvalues and
ﬁtting its parameters. For example, one can postulate that
ρE(λ) = Z−1 (1 + a1λ + a2λ2)
*
(λ −λ−)(λ+ −λ)
1 + b1λ + b2λ2
,
(19.70)
where λ± are ﬁxed to the smallest/largest observed eigenvalues, and a1, a2, b1 and b2
are ﬁtted on the data by minimizing the square error on the cumulative distribution. The
normalization factor Z can be computed during the ﬁtting procedure. This particular form
ﬁts very well for sample data generated numerically (see Fig. 19.3 left). To ﬁnd the optimal
shrinkage function (19.48), we then reconstruct the complex Stieltjes transform gE(x−i0+)
numerically, by using the ﬁtted ρ(λ) and computing its Hilbert transform. The issue with
such an approach is that even when Eq. (19.70) is a good ﬁt to the sample density of
eigenvalues, it cannot be obtained as the result of the free product of a Wishart and some

19.5 Real Data
313
0
1
2
3
l
0.0
0.2
0.4
0.6
0.8
1.0
r(l)
theoretical density
ﬁtted density
sample N = 1000
0
1
2
3
l
0.0
0.5
1.0
1.5
x(l)
simulation oracle
theoretical shrinkage
shrinkage from ﬁt
Figure 19.3 Parametric ﬁt illustrated on an example where the true covariance has a uniform density
of eigenvalues with mean 1 and variance 0.2 (see Eq. (15.42)). A single sample covariance matrix
with N = 1000 and q = 0.4 was generated, and the ad-hoc distribution (19.70) was ﬁtted to the
eigenvalue cdf. The left-hand ﬁgure shows a histogram of the sample eigenvalues compared with
the theoretical distribution and the ad-hoc ﬁt. The right-hand ﬁgure shows the theoretical optimal
shrinkage and the one obtained from the ﬁt. The agreement is barely satisfactory, in particular the
shrinkage from the ﬁt is non-monotonic. The dots show the oracle estimator ξk = vT
k Cvk computed
within the same simulation.
given density. As a consequence the approximate estimator generated by such an ansatz is
typically non-monotonic, whereas the exact shrinkage function should be.2
The Case of an Unbounded Support
On some real datasets, such as ﬁnancial time series, it is hard to detect a clear boundary
between bulk eigenvalues and the large outliers. In this case one may suspect that the
distribution of eigenvalues of the true covariance matrix C is itself unbounded. In that case,
one may try a parametric ﬁt for which the density of C extends to inﬁnity. For example, if
we suspect that the true distribution has a sharp left edge but a power-law right tail, we may
choose to model ρC(λ) as a shifted half Student’s t-distribution, i.e.
ρC(λ) = (λ −λ−)
2
a √πμ


1+μ
2


 μ
2


1 + (λ −λ−)2
a2μ
−1+μ
2
,
(19.71)
where (λ −λ−) indicates that the density is non-zero only for λ > λ−, chosen
to be the center of the Student’s t-distribution. These densities do not have an upper
edge, instead they fall off as ρ(λ) ∼λ−μ−1 for large λ. For integer values of the tail
2 Although we have not been able to ﬁnd a simple proof of this property, we strongly believe that it holds in full generality.

314
Eigenvector Overlaps and Rotationally Invariant Estimators
exponent μ, the Stieltjes transform gC(z) can be computed analytically. For example for
μ = 3 we ﬁnd
gμ=3(z) =
√
3πu3 + 6au2 + 9a2 √
3πu + 36a3 log

−u/
√
3a2

+ 18a3
√
3π

3a2 + u22
,
(19.72)
where u = z −λ−. Note that this Stieltjes transform has an essential singularity at z = λ−
and a branch cut on the real axis from λ−to +∞indicating that the density has no upper
bound. For μ = 3 both the mean and the variance of the eigenvalue density are ﬁnite. We
thus ﬁx λ−= 1−2
√
3a2/π such that τ(C) = 1 and adjust a to obtain the desired variance
given by τ(C2) −1 = 3a2(1 −(2/π)2).
In cases like this one, where we have an analytic form for gC(z) but no simple formula
for its S-transform, we can numerically solve the subordination relation
tE(ζ) = tC

ζ
1 + qtE(ζ)

,
(19.73)
with tC(ζ) = ζgC(ζ) −1, using an efﬁcient numerical ﬁxed point equation solver. Most of
the time a simple iteration would ﬁnd the ﬁxed point, but for some values of ζ and q it is
sometimes difﬁcult to ﬁnd an initial condition for the iteration to converge so it is better to
use a robust ﬁxed point solver.
Let us end on a technical remark: for unbounded densities, g(z) is not analytic at z = ∞,
which does not conform to some hypotheses made throughout the book. Intuitively, there
is no longer any clear distinction between bulk eigenvalues and outliers. For a ﬁxed value
of N, and for sufﬁciently large λ, the distance between two successive eigenvalues will at
some point become much larger than 1/N. Fortunately, the very same rie formula holds
both for bulk and for outlier eigenvalues, so we can close our eyes and safely apply Eq.
(19.27) for unbounded densities as well.
19.5.2 Kernel Methods
Another approach to compute the Stieltjes and/or the T-transform on the real axis is to
work directly with the discrete eigenvalues λk of E. As stated earlier we cannot simply
evaluate the discrete gN(z) at a point z = λk because gN(z) is inﬁnite precisely at the
points z ∈{λk}; this is the reason why gN(z) does not converge to the limiting gE(z) on the
support of ρ(λ).
The idea here is to generalize the standard kernel method to estimate continuous den-
sities from discrete data. Having observed a set of N eigenvalues [λk]k∈(1,N), a smooth
estimator of the density is constructed as
ρs(x) := 1
N
N

k=1
Kηk(x −λk),
(19.74)

19.5 Real Data
315
where Kη is some adequately chosen kernel of width η (possibly k-dependent), normalized
such that
 +∞
−∞
du Kη(u) = 1,
(19.75)
such that
 +∞
−∞
dx ρs(x) = 1.
(19.76)
A standard choice for K is a Gaussian distribution, but we will discuss more appropriate
choices for the Stieltjes transform below.
Now, let us similarly deﬁne a smoothed Stieltjes transform as
gs(z) := 1
N
N

k=1
gK,ηk(z −λk),
(19.77)
where gK,η is the Stieltjes transform of the kernel Kη, treated as a density:
gK,η(z) :=
 +∞
−∞
du Kη(u)
z −u ;
Im(z)  0.
(19.78)
Note that since Im gK,η(x −i0+) = iπKη(x), one immediately concludes that
Im gs(x −i0+) = iπρs(x)
(19.79)
for any smoothing kernel Kη. Hence gs(z) is the natural generalization of smoothed densi-
ties for Stieltjes transforms. Correspondingly, the real part of the smoothed Stieltjes is the
Hilbert transform (up to a π factor) of the smoothed density, i.e.
hs(x) := Re gs(x −i0+) = −
 ∞
−∞
dλ ρs(λ)
x −λ.
(19.80)
Two choices for the kernel Kη are specially interesting. One is the Cauchy kernel:
KC
η (u) := 1
π
η
u2 + η2,
(19.81)
from which one gets
gKC,η(z) =
1
z ± iη,
± = sign (Im(z)) .
(19.82)
Hence, in this case, we ﬁnd that the smoothed Stieltjes transform we are looking for is
nothing but the discrete Stieltjes transform computed with a k-dependent width ηk:
gC
s (z) := 1
N
N

k=1
1
z −λk −iηk
,
Im(z) < 0,
(19.83)
which we can now safely compute numerically on the real axis, i.e. when z = x −i0+, and
plug in the corresponding formulas for the rie estimator ξ(λ).

316
Eigenvector Overlaps and Rotationally Invariant Estimators
0
1
2
3
l
0.0
0.2
0.4
0.6
0.8
1.0
1.2
r(l)
theoretical density
Wigner kernel
Cauchy kernel
0
1
2
3
l
0.0
0.5
1.0
1.5
x(l)
theoretical shrinkage
Wigner kernel
isotonic Wigner kernel
Figure 19.4 Non-parametric kernel methods applied to the same problem as in Figure 19.3. An
approximation of gE(λ −i0+) is computed with the Cauchy kernel (19.82) and the Wigner kernel
(19.85) both with ηk = η = N−1/2. (left) We compare the two smoothed densities with the
theoretical one. Both are quite good but the Wigner kernel is better where the density changes rapidly.
(right) From the smoothed Stieltjes transforms we compute the shrinkage function for both methods.
Only the result of the Wigner kernel is shown (the Cauchy kernel is comparable albeit slightly
worse). Kernel methods give non-monotonic shrinkage functions which can be easily rectiﬁed using
an isotonic regression (19.86), which improves the agreement with the theoretical curve.
Another interesting choice for numerical applications is the semi-circle “Wigner kernel”,
which has sharp edges. To wit,
KW
η (u) =
*
4η2 −u2
2πη2
for
−2η ≤u ≤2η,
(19.84)
and 0 when |u| > 2η. In this case, we obtain
gW
s (z) := 1
N
N

k=1
z −λk
2η2
k
⎛
⎝1 −
2
1 −
4η2
k
(z −λk)2
⎞
⎠.
(19.85)
Figure 19.4 gives an illustration of the kernel method using both the Cauchy kernel and the
Wigner kernel, with ηk = η = N−1/2.
We end this section with two practical implementation points regarding the kernel
methods.
1 Since the optimal rie estimator ξ(λ) should be monotonic in λ, one should rectify
possibly non-monotonic numerical estimators using an isotonic regression. The isotonic
regression ˆyk of some data yk is given by
ˆyk = argmin
T

k=1

ˆyk −yk
2
with
ˆy1 ≤ˆy2 ≤· · · ≤ˆyT −1 ≤ˆyT .
(19.86)

19.6 Validation and RIE
317
It is the monotonic sequence that is the closest (in the least-square sense) to the original
data.
2 In most situations, we are interested in reconstructing the optimal rie matrix  =
 ξ(λk)vkvT
k and hence we need to evaluate the shrinkage function ξ(λ) precisely at the
sample eigenvalues {λk}. We have found empirically that excluding the point λk itself
from the kernel estimator consistently gives better results than including it. For example,
in the Cauchy case, one should compute
gC
s (λℓ−i0+) ≈
1
N −1
N

k=1
kℓ
1
λℓ−λk −iηk
,
(19.87)
when estimating Eq. (19.48).
19.6 Validation and RIE
The idea of validation to determine the rie is to compute the eigenvectors vi of E the scm
of a training set and compute their unbiased variance of a different dataset: the validation
set. More formally, this is written
ξ×(λi) := vi
T E′vi,
(19.88)
where E′ is the validation scm. The training set is also called the in-sample data and the
validation set the out-of-sample data.
In practical applications, we have typically a single dataset that needs to be split into
a training and a validation set. If we are not too worried about temporal order, any block
of the data can serve as the validation set. In K-fold cross-validation, the data is split into
K blocks, one block is the validation set and the union of the K −1 others serves as the
training set. The procedure is then repeated successively choosing the K possible validation
sets, see Figure 19.5.
In the following, we will assume that the true covariance matrix C is the same on both
datasets so that E = C
1
2 WC
1
2 and E′ = C
1
2 W′C
1
2 , where W′ is independent from W.
Expanding over the eigenvectors v′
j of E′, we get
ξ×(λi) =
N

k=1

vi
T v′
k
2 λ′
k
(19.89)
or, in the large N limit and using the deﬁnition of  given in Eq. (19.6),
ξ×(λ) −→
N→∞

ρE′(λ′)(λ,λ′)λ′ dλ′.
(19.90)
Now, there is an exact relation between  and , which reads
(λ,λ′) = 1
N
N

j=1
(λ,μj)(λ′,μj)
(19.91)

318
Eigenvector Overlaps and Rotationally Invariant Estimators
or, in the continuum limit,
(λ,λ′) =

ρC(μ)(λ,μ)(λ′,μ) dμ.
(19.92)
Intuitively, this relation can be understood as follows: we expect that, from the very
deﬁnition of , the eigenvectors of E and E′ can be written as
vi =
1
√
N
N

j=1
εij
/
(λi,μj) uj;
v′
k =
1
√
N
N

ℓ=1
ε′
kℓ
/
(λ′
k,μℓ) uℓ,
(19.93)
where uj are the eigenvectors of C and εij are independent random variables of mean
zero and variance one, such that
E[εijεkℓ] = δikδjℓ,
E[ε′
ijε′
kℓ] = δikδjℓ,
E[εijε′
kℓ] = 0.
(19.94)
This so-called ergodic assumption can be justiﬁed from considerations about the Dyson
Brownian motion of eigenvectors, see Eq. (9.10), but this goes beyond the scope of this
book. In any case, if we now compute E[(vT
i v′
k)2] using the ergodic assumption and
remembering that uT
j uℓ= δjℓ, we ﬁnd
NE[(vT
i v′
k)2] = 1
N
N

j=1
(λi,μj)(λ′
k,μj),
(19.95)
which is precisely Eq. (19.91).
Injecting Eq. (19.91) into Eq. (19.89), we thus ﬁnd
ξ×(λ) = 1
N2
N

k=1
⎛
⎝
N

j=1
(λ,μj)(λ′
k,μj)
⎞
⎠λ′
k
= 1
N2
N

j=1
(λ,μj)
 N

k=1
(λ′
k,μj)λ′
k

.
(19.96)
The last term in parenthesis can be computed by using the very deﬁnition of E′:
N

k=1
(λ′
k,μj)λ′
k ≡Nuj
T E′uj
= NC
1
2 uT
j W′C
1
2 uj = NμjuT
j W′uj.
(19.97)
Now, the idea is that since W′ is independent of C, averaging over any small interval of μj
will amount to replacing uj
T W′uj by its average over randomly oriented vectors u, which
is equal to unity:
E[uT W′u] = τ(W′uuT ) = τ(W′)τ(uuT ) = 1.
(19.98)
Hence, from Eq. (19.96) we ﬁnally obtain
ξ×(λ) = 1
N
N

j=1
(λ,μj)μj −→
N→∞

ρC(μ) (λ,μ) μ dμ,
(19.99)

19.6 Validation and RIE
319
0.0
0.5
1.0
1.5
2.0
2.5
3.0
l
0.50
0.75
1.00
1.25
1.50
1.75
x(l)
cross-validation
theoretical shrinkage
isotonic cross-validation
Figure 19.5 Shrinkage function ξ(λ) computed for the same problem as in Figure 19.3, now using
cross-validation. The dataset is divided into K = 10 blocks of equal length. For each block, we
compute the N = 1000 eigenvalues λb
i and eigenvectors vb
i of the sample covariance matrix using
the rest of the data (of new length 9T/10), and compute ξ×(λb
i ) := vb
i
T E′vb
i , with E′ the sample
covariance matrix of the considered block. The dots correspond to the 10 × 1000 pairs (λb
i ,ξ×(λb
i )).
The full line is an isotonic regression through the dots. The procedure has a slight bias as we in fact
compute the optimal shrinkage for a value of q equal to q× = 10N/9T , but otherwise the agreement
with the optimal curve is quite good.
which precisely coincides with the deﬁnition of the optimal non-linear shrinkage function
ξ(λ), see Eq. (19.27).
This result is very interesting and indicates that one can approximate ξ(λ) by considering
the quadratic form between the eigenvectors of a given realization of C – say E – and
another realization of C – say E′ – even if the two empirical matrices are characterized by
different values of the ratio N/T . This method is illustrated in Figure 19.5.
Bibliographical Notes
• For a recent review on the subject of cleaning noisy covariance matrices and rie, see
– J. Bun, J.-P. Bouchaud, and M. Potters. Cleaning correlation matrices. Risk magazine,
2016,
– J. Bun, J.-P. Bouchaud, and M. Potters. Cleaning large correlation matrices: Tools
from random matrix theory. Physics Reports, 666:1–109, 2017.
• The original work of Ledoit and P´ech´e and its operational implementation, see
– O. Ledoit and S. P´ech´e. Eigenvectors of some large sample covariance matrix ensem-
bles. Probability Theory and Related Fields, 151(1-2):233–264, 2011,
– O. Ledoit and M. Wolf. Nonlinear shrinkage estimation of large-dimensional covari-
ance matrices. The Annals of Statistics, 40(2):1024–1060, 2012,

320
Eigenvector Overlaps and Rotationally Invariant Estimators
– O. Ledoit and M. Wolf. Nonlinear shrinkage of the covariance matrix for port-
folio selection: Markowitz meets Goldilocks. The Review of Financial Studies,
30(12):4349–4388, 2017.
• Rotationally invariant estimators for general additive and multiplicative models:
– J. Bun, R. Allez, J.-P. Bouchaud, and M. Potters. Rotational invariant estimator for
general noisy matrices. IEEE Transactions on Information Theory, 62:7475–7490,
2016.
• Rotationally invariant estimators for outliers:
– J. Bun and A. Knowles. An optimal rotational invariant estimator for general covari-
ance matrices. Unpublished, 2016. preprint available on researchgate.net,,
see also Bun et al. [2017].
• Overlaps between the eigenvectors of correlated matrices:
– J. Bun, J.-P. Bouchaud, and M. Potters. Overlaps between eigenvectors of correlated
random matrices. Physical Review E, 98:052145, 2018.
• Rotationally invariant estimators for cross-correlation matrices:
– F. Benaych-Georges, J.-P. Bouchaud, and M. Potters. Optimal cleaning for singular
values of cross-covariance matrices. preprint arXiv:1901.05543, 2019.

20
Applications to Finance
20.1 Portfolio Theory
One of the arch-problems in quantitative ﬁnance is portfolio construction. For example, one
may consider an investment universe made of N stocks (or more generally N risky assets)
that one should bundle up in a portfolio to achieve optimal performance, according to some
quality measure that we will discuss below.
20.1.1 Returns and Risk Free Rate
We call pi,t the price of stock i at time t and deﬁne the returns over some elementary time
scale (say one day) as
ri,t := pi,t −pi,t−1
pi,t−1
.
(20.1)
The portfolio weight πi is the dollar amount invested on asset i, which can be positive
(corresponding to buys) or negative (corresponding to short sales). The total capital to be
invested is C. Naively, one should have C = 
i πi. But one can borrow cash, so that

i πi > C and pay the risk free rate r0 on the borrowed amount, or conversely under-
invest in stocks (
i πi < C) and invest the remaining capital at the risk free rate, assumed
to be the same r0.1 Then the total return of the portfolio (in dollar terms) is
Rt =

i
πiri,t + (C −

i
πi)r0,
(20.2)
so that the excess return (over the risk free rate) is
Rt −Cr0 :=

i
πi(ri,t −r0),
(20.3)
where ri,t −r0 is the excess return of asset i. From now on, we will denote ri,t −r0 by ri,t.
We will assume that these excess returns are characterized by some expected gains gi and
a covariance matrix C, with
1 In general, the risk free rate to borrow is different from the one to lend, but we will neglect the difference here.
321

322
Applications to Finance
Cij = Cov[rirj].
(20.4)
The problem, of course, is that both the vector of expected gains g and the covariance
matrix C are unknown to the investor, who must come up with his/her best guess for these
quantities. Forming expectations of future returns is the job of the investor, based on his/her
information, anticipations, and hunch. We will not attempt to model the sophisticated
process at work in the mind of investors, and simply assume that g is known. In the simplest
case, the investor has no preferences and g = g1, corresponding to the same expected return
for all assets. Another possibility is to assume that g is, for all practical purposes, a random
vector in RN.
As far as C is concerned, the most natural choice is to use the sample covariance matrix
E, determined using a series of past returns of length T . However, as we already know
from Chapter 17, the eigenvalues of E can be quite far from those of C when q = N/T
is not very small. On the other hand, T cannot be as large as one could wish, the most
important reason being that the (ﬁnancial) world is non-stationary. For a start, many large
ﬁrms that exist in 2019 did not exist 25 years ago. More generally, it is far from clear that
the parameters of the underlying statistical process (if such a thing exists) can be considered
as constant in time, so mixing different epochs is in general not warranted. On the other
hand, due to experimental constraints, the limitation of data points can be a problem even
in a stationary world.
20.1.2 Portfolio Risk
The risk of a portfolio is traditionally measured as the variance of its returns, namely
R2 := V[R] =

i,j
πiπjCov[rirj] = π T Cπ.
(20.5)
Other measures of risk can however be considered, such as the expected shortfall Sp (or
conditional value at risk), deﬁned as
Sp = −1
p
 Rp
−∞
dR R P(R),
(20.6)
where Rp is the p-quantile, with for example p = 0.01 for the 1% negative tail events:
p =
 Rp
−∞
dR P(R).
(20.7)
If P(R) is Gaussian, then all risk measures are equivalent and subsumed in V[R].
20.1.3 Markowitz Optimal Portfolio Theory
For the reader not familiar with Markowitz’s optimal portfolio theory, we recall in this
section some of the most important results. Suppose that an investor wants to invest in a

20.1 Portfolio Theory
323
portfolio containing N different assets, with optimal weights π to be determined. An intu-
itive strategy is the so-called mean-variance optimization: the investor seeks an allocation
such that the variance of the portfolio is minimized given an expected return target. It is not
hard to see that this mean-variance optimization can be translated into a simple quadratic
optimization program with a linear constraint. Markowitz’s optimal portfolio amounts to
solving the following quadratic optimization problem:
$ minπ∈RN 1
2π T Cπ
subject to π T g ≥G
(20.8)
where G is the desired (or should we say hoped for) gain. Without further constraints – such
as the positivity of all weights necessary if short positions are not allowed – this problem
can be easily solved by introducing a Lagrangian multiplier γ and writing
min
π∈RN
1
2π T Cπ −γ π T g.
(20.9)
Assuming that C is invertible, it is not hard to ﬁnd the optimal solution and the value of γ
such that overall expected return is exactly G. It is given by
πC = G C−1g
gT C−1g,
(20.10)
which, as noted above, requires the knowledge of both C and g, which are a priori
unknown. Note that even if the predictions gi of our investor are completely wrong, it still
makes sense to look for the minimum risk portfolio consistent with his/her expectations.
But we are left with the problem of estimating C, or maybe C−1 before applying
Markowitz’s formula, Eq. (20.10). We will see below why one should actually ﬁnd the
best estimator of C itself before inverting it and determining the weights.
What is the risk associated with this optimal allocation strategy, measured as the variance
of the returns of the portfolio? If one knew the population correlation matrix C, the true
optimal risk associated with πC would be given by
R2
true := π T
CCπC =
G2
gT C−1g.
(20.11)
However, the optimal strategy (20.10) is not attainable in practice as the matrix C is
unknown. What can one do then, and how poorly is the realized risk of the portfolio
estimated?
20.1.4 Predicted and Realized Risk
One obvious – but far too naive – way to use the Markowitz optimal portfolio is to apply
(20.10) using the scm E as is, instead of C. Recalling the results of Chapter 17, it is not
hard to see that this strategy will suffer from strong biases whenever T is not sufﬁciently
large compared to N.

324
Applications to Finance
Notwithstanding, the optimal investment weights using the scm E read
πE = G E−1g
gT E−1g,
(20.12)
and the minimum risk associated with this portfolio is thus given by
R2
in = π T
EE πE =
G2
gT E−1g,
(20.13)
which is known as the “in-sample” risk, or the predicted risk. It is “in-sample” because it
is entirely constructed using the available data. The realized risk in the next period, with
fresh data, is correspondingly called out-of-sample.
Using the convexity with respect to E of gT E−1g, we ﬁnd from the Jensen inequality
that, for ﬁxed predicted gains g,
E[gT E−1g] ≥gT E

E
−1g = gT C−1g,
(20.14)
where the last equality holds because E is an unbiased estimator of C. Hence, we conclude
that the in-sample risk is lower than the “true” risk and therefore the optimal portfolio
πE suffers from an in-sample bias: its predicted risk underestimates the true optimal risk
Rtrue. Intuitively this comes from the fact that πE attempts to exploit all the idiosyncracies
that happened during the in-sample period, and therefore manages to reduce the risk below
the true optimal risk. But the situation is even worse, because the future out-of-sample or
realized risk, turns out to be larger than the true risk. Indeed, let us denote by E′ the scm of
this out-of-sample period; the out-of-sample risk is then naturally deﬁned by
R2
out = π T
EE′ πE = G2gT E−1E′E−1g
(gT E−1g)2
.
(20.15)
For large matrices, we expect the result to be self-averaging and given by its expectation
value (over the measurement noise). But if the measurement noise in the in-sample period
(contained in πE) can be assumed to be independent from that of the out-of-sample
period, then πE and E′ are uncorrelated and we get, for N →∞,
π T
EE′πE = π T
ECπE.
(20.16)
Now, from the optimality of πC, we also know that
π T
CCπC ≤π T
ECπE,
(20.17)
so we readily obtain the following general inequalities:
R2
in ≤R2
true ≤R2
out.
(20.18)
We plot in Figure 20.1 an illustration of these inequalities. One can see how using
πE is clearly overoptimistic and can potentially lead to disastrous results in practice.

20.2 The High-Dimensional Limit
325
0.0
0.5
1.0
1.5
2.0
2
0.00
0.25
0.50
0.75
1.00
1.25
1.50
2
in(E)
2
in( )
2
true
2
out( )
2
out(E)
Figure 20.1 Efﬁcient frontier associated with the mean-variance optimal portfolio (20.10) for g = 1
and C an inverse-Wishart matrix with p = 0.5, for q = 0.5. The black line depicts the expected
gain as a function of the true optimal risk (20.11). The gray lines correspond to the realized (out-of-
sample) risk using either the scm E or its rie version . Both estimates are above the true risk, but
less so for rie. Finally, the dashed lines represent the predicted (in-sample) risk, again using either
the scm E or its rie version . R and G in arbitrary units, such that Rtrue = 1 for G = 1.
This conclusion in fact holds for different risk measures, such as the expected shortfall
measure mentioned in Section 20.1.2.
20.2 The High-Dimensional Limit
20.2.1 In-Sample vs Out-of-Sample Risk: Exact Results
In the limit of large matrices and with some assumptions on the structure g, we can make
the general inequalities Eq. (20.18) more precise using the random matrix theory tools from
the previous chapters. Let us suppose that the vector of predictors g points in a random
direction, in the sense that the covariance matrix C and ggT are mutually free. This is not
necessarily a natural assumption. For example, the simplest “agnostic” prediction g = 1 is
often nearly collinear with the top eigenvector of C (see Section 20.4). So we rather think
here of market neutral, sector neutral predictors that attempt to capture very idiosyncratic
characteristics of ﬁrms.
Now, if M is a positive deﬁnite matrix that is free from ggT , then in the large N limit:
gT Mg
N
= 1
N Tr[ggT M]
=
freeness
g2
N τ(M),
(20.19)

326
Applications to Finance
where we recall that τ is the normalized trace operator. We can always normalize the
prediction vector such that g2/N = 1, so setting M = {E−1, C−1}, we can directly estimate
Eqs. (20.13), (20.11) and (20.15) and ﬁnd
R2
in →
G2
Nτ(E−1),
R2
true →
G2
Nτ(C−1),
R2
out →G2τ(E−1CE−1)
Nτ 2(E−1)
.
(20.20)
Let us focus on the ﬁrst two terms above. For q < 1, we know from Eq. (17.14) that, in
the high-dimensional limit, τ(C−1) = (1 −q)τ(E−1). As a result, we have, for N →∞,
R2
in = (1 −q)R2
true.
(20.21)
Hence, for any q ∈(0,1), we see that the in-sample risk associated with πE always
provides an overoptimistic estimator. Even better, we are able to quantify precisely the
risk underestimation factor thanks to Eq. (20.21).
Next we would like to ﬁnd the same type of relation for the “out-of-sample” risk. In
order to do so, we write E = C
1
2 WqC
1
2 where Wq is a white Wishart matrix of parameter
q, independent from C. Plugging this representation into Eq. (20.15), we ﬁnd that the out-
of-sample risk can be expressed as
R2
out =
G2τ(C−1W−2
q )
Nτ 2(E−1)
when N →∞. Now, since Wq and C are asymptotically free, we also have
τ(C−1W−2
q ) = τ(C−1) τ(W−2
q ).
(20.22)
Hence, using again Eq. (17.14) yields
R2
out = G2(1 −q)2 τ(W−2
q )
Nτ(C−1).
(20.23)
Finally, we know from Eq. (15.22) that τ(W−2
q ) = (1 −q)−3 for q < 1. Hence, we ﬁnally
get
R2
out = R2
true
1 −q .
(20.24)
All in all, we have obtained the following asymptotic relation:
R2
in
1 −q = R2
true = (1 −q)R2
out,
(20.25)
which holds for a completely general C.

20.2 The High-Dimensional Limit
327
Hence, if one invests with the “naive” weights πE, it turns out that the predicted risk Rin
underestimates the realized risk Rout by a factor (1 −q), and in the extreme case N = T
or q = 1, the in-sample risk is equal to zero while the out-of-sample risk diverges (for
N →∞). We thus conclude that, as announced, the use of the scm E for the Markowitz
optimization problem can lead to disastrous results. This suggests that we should use a
more reliable estimator of C in order to control the out-of-sample risk.
20.2.2 Out-of-Sample Risk Minimization
We insisted throughout the last section that the right quantity to control in portfolio man-
agement is the realized, out-of-sample risk. It is also clear from Eq. (20.25) that using the
sample estimate E is a very bad idea, and hence it is natural to wonder which estimator
of C one should use to minimize this out-of-sample risk? The Markowitz formula (20.10)
naively suggests that one should look for a faithful estimator of the so-called precision
matrix C−1. But in fact, since the expected out-of-sample risk involves the matrix C lin-
early, it is that matrix that should be estimated.
Let us show this using another route, in the context of rotationally invariant estimators,
which we considered in Chapter 19. Let us deﬁne our rie as
 =
N

i=1
ξ(λi)vivT
i ,
(20.26)
where we recall that vi are the sample eigenvectors of E and ξ(·) is a function that has to
be determined using some optimality criterion.
Suppose that we construct a Markowitz optimal portfolio π using this rie. Again, we
assume that the vector g is random, and independent from . Consequently, the estimate
(20.19) is still valid, such that the realized risk associated with the portfolio π reads, for
N →∞,
R2
out() = G2 Tr

−1C−1

Tr −1
2
.
(20.27)
Using the decomposition (20.26) of , we can rewrite the numerator as
Tr

−1C−1
=
N

i=1
vi
T Cvi
ξ2(λi) ,
(20.28)
while the denominator of Eq. (20.27) is

Tr −12
=
 N

i=1
1
ξ(λi)
2
.
(20.29)
Regrouping these last two equations allows us to express Eq. (20.27) as

328
Applications to Finance
R2
out() = G2
N

i=1
vi
T Cvi
ξ2(λi)
 N

i=1
1
ξ(λi)
−2
.
(20.30)
Our aim is to ﬁnd the optimal shrinkage function ξ(λj) associated with the sample eigen-
values [λj]N
j=1, such that the out-of-sample risk is minimized. This can be done by solving,
for a given j, the following ﬁrst order condition:
∂R2
out()
∂ξ(λj)
= 0,
∀j = 1, . . . ,N.
(20.31)
By performing the derivative with respect to ξ(λj) in (20.30), one obtains
−2vj
T Cvj
ξ3(λj)
 N

i=1
1
ξ(λi)
−2
+
2
ξ2(λj)
 N

i=1
vi
T Cvi
ξ2(λi)
 N

i=1
1
ξ(λi)
−3
= 0.
(20.32)
The solution to this equation is given by
ξ(λj) = Avj
T Cvj,
(20.33)
where A is an arbitrary constant at this stage. But since the trace of the rie must match that
of C, this constant A must be equal to 1. Hence we recover precisely the oracle estimator
that we have studied in Chapter 19.
As a conclusion, the optimal rie (19.26) actually minimizes the out-of-sample risk
within the class of rotationally invariant estimators. Moreover, the corresponding “optimal”
realized risk is given by
R2
out() =
G2
Tr

()−1,
(20.34)
where we used the notable property that, for any n ∈Z,
Tr[()nC] =
N

i=1
ξ(λi)n Tr[vivT
i C] =
N

i=1
ξ(λi)nvT
i Cvi ≡Tr[()n+1].
(20.35)
20.2.3 The Inverse-Wishart Model: Explicit Results
In this section, we specialize the result (20.34) to the case when C is an inverse-Wishart
matrix with parameter p > 0, corresponding to the simple linear shrinkage optimal
estimator. First, we read from Eq. (15.30) that
τ

C−1
= −gC(0) = 1 + p,
(20.36)
so that we get from Eq. (20.20) that, in the large N limit,
R2
true = G2
N
1
1 + p .
(20.37)
Next, we see from Eq. (20.34) that the optimal out-of-sample risk requires the compu-
tation of τ(()−1). In general, the computation of this quantity is highly non-trivial but

20.2 The High-Dimensional Limit
329
some simpliﬁcations appear when C is an inverse-Wishart matrix. In the large-dimension
limit, the ﬁnal result reads
τ(()−1) = −

1 + q
p

gE

−q
p

= 1 +
p2
p + q + pq ,
(20.38)
and therefore we have from Eq. (20.34)
R2
out() = G2
N
p + q + pq
(p + q)(1 + p),
(20.39)
and so it is clear from Eqs. (20.39) and (20.37) that, for any p > 0,
R2out()
R2true
= 1 + q
pq
(p + q)(1 + p) ≥1,
(20.40)
where the last inequality becomes an equality only when q = 0, as it should.
It is also interesting to evaluate the in-sample risk associated with the optimal rie. It is
deﬁned by
R2
in() = G2 Tr

()−1E()−1
Nτ2(()−1)
,
(20.41)
where the most challenging term is the numerator. Using the fact that the eigenvalues of
 are given by the linear shrinkage formula (19.49), one can once again ﬁnd a closed
formula.2 The ﬁnal result is written
R2
in() = G2
N
p + q
(1 + p)(p + q(p + 1)),
(20.42)
and we therefore deduce with Eq. (20.37) that, for any p > 0,
R2
in()
R2true
= 1 −
pq
p + q(1 + p) ≤1,
(20.43)
where the inequality becomes an equality for q = 0 as above.
Finally, one may easily check from Eqs. (20.25), (20.40) and (20.43) that
R2
in() −R2
in(E) ≥0,
R2
out() −R2
out(E) ≤0,
(20.44)
showing explicitly that we indeed reduce the overﬁtting by using the oracle estimator
instead of the scm in the high-dimensional framework: both the in-sample and out-of-
sample risks computed using  are closer to the true risk than when computed with the raw
empirical matrix E. The results shown in Figure 20.1 correspond to the inverse-Wishart
case with p = q = 1
2.
Exercise 20.2.1
Optimal portfolio when the true covariance matrix is Wishart
In this exercise we continue the analysis of Exercise 19.2.2 assuming that
we measure an scm from data with a true covariance given by a Wishart with
parameter q0.
2 Details of this computation can be found in Bun et al. [2017].

330
Applications to Finance
(a)
The minimum risk portfolio with expected gain G is given by
π = G C−1g
gT C−1g,
(20.45)
where C is the covariance matrix (or an estimator of it) and g is the vector of
expected gains. Compute the matrix  by taking the matrix E1 and replacing
its eigenvalues λk by ξ(λk) and keeping the same eigenvectors. Use the result
of Exercise 19.2.2(c) for ξ(λ), if some λk are below 0.05594 or above 3.746
replace them by 0.05594 and 3.746 respectively. This is so that you do not
have to worry about ﬁnding the correct solution tE(z) for z outside of the
bulk.
(b)
Build the three portfolios πC, πE and π by computing Eq. (20.45) for the
three matrices C, E1 and  using G = 1 and g = e1, the vector with 1 in the
ﬁrst component and 0 everywhere else. These three portfolios correspond to
the true optimal, the naive optimal and the cleaned optimal. The true optimal
is in general unobtainable. For these three portfolios compute the in-sample
risk Rin := π T E1π, the true risk Rtrue := π T Cπ and the out-of-sample risk
Rout := π T E2π.
(c)
Comment on these nine values. For πC and πE you should ﬁnd exact
theoretical values. The out-of-sample risk for π should better than for πE
but worse than for πC.
20.3 The Statistics of Price Changes: A Short Overview
20.3.1 Bachelier’s First Law
The simplest property of ﬁnancial prices, dating back to Bachelier’s thesis, states that typi-
cal price variations grow like the square-root of time. More formally, under the assumption
that price returns have zero mean (which is usually a good approximation on short time
scales), then the price variogram
V(τ) := E[(log pt+τ −log pt)2]
(20.46)
grows linearly with time lag τ, such that V(τ) = σ 2τ.
20.3.2 Signature Plots
Assume now that a price series is described by
log pt = log p0 +
t
t′=1
rt′,
(20.47)
where the return series rt is covariance-stationary with zero mean and covariance
Cov (rt′,rt′′) = σ 2Cr(|t′ −t′′|).
(20.48)

20.3 The Statistics of Price Changes: A Short Overview
331
The case of a random walk with uncorrelated price returns corresponds to Cr(u) = δu,0,
where δu,0 is the Kronecker delta function. A trending random walk has Cr(u) > 0 and a
mean-reverting random walk has Cr(u) < 0. How does this affect Bachelier’s ﬁrst law?
One important implication is that the volatility observed by sampling price series on a
given time scale τ is itself dependent on that time scale. More precisely, the volatility at
scale τ is given by
σ 2(τ) := V(τ)
τ
= σ 2(1)
-
1 + 2
τ

u=1

1 −u
τ

Cr(u)
.
.
(20.49)
A plot of σ(τ) versus τ is called a volatility signature plot. The case of an uncorrelated ran-
dom walk leads to a ﬂat signature plot. Positive correlations (which correspond to trends)
lead to an increase in σ(τ) with increasing τ. Negative correlations (which correspond to
mean reversion) lead to a decrease in σ(τ) with increasing τ.
20.3.3 Volatility Signature Plots for Real Price Series
Quite remarkably, the volatility signature plots of most liquid assets (stocks, futures,
FX, . . . ) are nowadays almost ﬂat for values of τ ranging from a few seconds to a
few months (beyond which it becomes dubious whether the statistical assumption of
stationarity still holds). For example, for the S&P500 E-mini futures contract, which is
one of the most liquid contracts in the world, σ(τ) only decreases by about 20% from
short time scales (seconds) to long time scales (weeks). For single stocks, however,
some interesting deviations from a ﬂat horizontal line can be detected, see Figure 20.2.
The exact form of a volatility signature plot depends on the microstructural details of
the underlying asset, but most liquid contracts in this market have a similar volatility
signature plot.
20.3.4 Heavy Tails
An overwhelming body of empirical evidence from a vast array of ﬁnancial instruments
(including stocks, currencies, interest rates, commodities, and even implied volatility)
shows that unconditional distributions of returns have fat tails, which decay as a power law
for large arguments and are much heavier than the tails of a Gaussian distribution.
On short time scales (between about a minute and a few hours), the empirical density
function of returns r can be ﬁt reasonably well by a Student’s t-distribution, see Figure
20.3. Student’s t-distributions read
P(r) =
1
a √πμ


1+μ
2


 μ
2


1 + r2
a2μ
−1+μ
2
,
(20.50)
where a is a parameter ﬁxing the scale of r. Student’s t is such that P(r) decays for large
r as |r|−1−μ, where μ is the tail exponent. Empirically, the tail parameter μ is consistently
found to be around 3 for a wide variety of different markets (see Fig. 20.3), which suggests

332
Applications to Finance
0
50
100
150
200
t
0.90
0.95
1.00
1.05
s2(t)
constant
US stock data
Figure 20.2 Average signature plot for the normalized returns of US stocks, where the x-axis is
in days. The data consists of the returns of 1725 US companies over the period 2012–2019 (2000
business days), returns are normalized by a one-year exponential estimate of their past volatility. To
a ﬁrst approximation σ 2(τ) is independent of τ. The signature plot allows us to see deviations from
this pure random walk behavior. One can see that stocks tend to mean-revert slightly at short times
(τ < 50 days) and trend at longer times. The effect is stronger on the many low liquidity stocks
included in this dataset.
−15
−10
−5
0
5
10
15
x = r/ s
10−6
10−5
10−4
10−3
10−2
10−1
100
P(x)
Student’s t m = 3
Gaussian
US stock data
Figure 20.3 Empirical distribution of normalized daily stock returns compared with a Gaussian and
a Student’s t-distribution with μ = 3 and the same variance. Same data as in Figure 20.2.
some kind of universality in the mechanism leading to extreme returns. This universality
hints at the fact that fundamental factors are probably unimportant in determining the
amplitude of most large price jumps. Interestingly, many studies indeed suggest that large
price moves are often not associated with an identiﬁable piece of news that would rationally
explain wild valuation swings.

20.3 The Statistics of Price Changes: A Short Overview
333
20.3.5 Volatility Clustering
Although considering the unconditional distribution of returns is informative, it is also
somewhat misleading. Returns are in fact very far from being iid random variables –
although they are indeed nearly uncorrelated, as their (almost) ﬂat signature plots demon-
strate. Therefore, returns are not simply independent random variables drawn from
the Student’s t-distribution. Such an iid model would predict that upon time aggregation the
distribution of returns would quickly converge to a Gaussian distribution on longer time
scales. Empirical data indicates that this is not the case, and that returns remain substantially
non-Gaussian on time scales up to weeks or even months.
The dynamics of ﬁnancial markets is in fact highly intermittent, with periods of intense
activity intertwined with periods of relative calm. In intuitive terms, the volatility of ﬁnan-
cial returns is itself a dynamic variable that changes over time with a broad distribution of
characteristic frequencies, a phenomenon called heteroskedasticity. In more formal terms,
returns can be represented by the product of a time dependent volatility component σt and
an iid directional component εt,
rt := σtεt.
(20.51)
In this representation, εt are iid (but not necessarily Gaussian) random variables of unit
variance and σt are positive random variables with long memory. This is illustrated in
Figure 20.4 where we show the autocorrelation of the squared returns, which gives access
to E[σ 2
t σ 2
t+τ].
It is worth pointing out that volatilities σ and scaled returns ε are in fact not independent
random variables. It is well documented that positive past returns tend to decrease
100
101
102
t
10−2
r(r2
t ,r2
t+ t)
US stock data
r
t−0.5
Figure 20.4 Average autocorrelation function of squared daily returns for the US stock data described
in Figure 20.3. The autocorrelation decays very slowly with the time difference τ. A power law τ−γ
with γ = 0.5 is plotted to guide the eye. Note the three peaks at τ = 65, 130 and 195 business days
correspond to the periodicity of highly volatile earning announcements.

334
Applications to Finance
future volatilities and that negative past returns tend to increase future volatilities (i.e.
E[εtσt+τ] < 0 for τ > 0). This is called the leverage effect. Importantly, however, past
volatilities do not give much information on the sign of future returns (i.e. E[εtσt+τ] ≈0
for τ < 0).
20.4 Empirical Covariance Matrices
20.4.1 Empirical Eigenvalue Spectrum
We are now in position to investigate the empirical covariance matrix E of a collection of
stocks. For deﬁniteness, we choose q = 0.25 by selecting N = 500 stocks observed at the
daily time scale, with time series of length T = 2000 days. The distribution of eigenvalues
of E is shown in Figure 20.5. We observe a rather broad distribution centered around 1, but
with a slowly decaying tail and a top eigenvalue λ1 found to be ∼100 times larger than the
mean. The top eigenvector v corresponds to the dominant risk factor. It is closely aligned
with the uniform mode [e]i = 1/
√
N, i.e. all stocks moving in sync – hence the name
“market mode” to describe the top eigenvector. Numerically, one ﬁnds |vT e| ≈0.95.
20.4.2 A One-Factor Model
The simplest model aimed at describing the co-movement of different stocks is to assume
that the return ri,t of stock i at time t can be decomposed into a common factor ft and iid
idiosyncratic components εi,t, to wit,
0
1
2
3
4
l
0.0
0.5
1.0
1.5
P(l)
3
10
30
100
300
l
10−4
10−3
10−2
P> (l)
Figure 20.5 Eigenvalue distribution of the scm, averaged over for three random sets of 500 US stocks,
each measured on 2000 business days. Returns are normalized as in Figure 20.3, corresponding to
¯λ = 0.97. The inset shows the complementary cumulative distribution for the largest eigenvalues
indicating a power-law behavior for large λ, as P>(λ) ≈λ−4/3. Note the largest eigenvalue λ1 ≈
0.2N, which corresponds to the “market mode”, i.e. the risk factor where all stocks move in the same
direction.

20.4 Empirical Covariance Matrices
335
ri,t = βift + εi,t,
(20.52)
where ft is often thought of as the “market factor”. Assuming further that ft,εi,t are
uncorrelated random variables of mean zero and variance, respectively, σ 2
f and σ 2
ε , the
covariance matrix Cij is simply given by
Cij = βiβjσ 2
f + δijσ 2
ε .
(20.53)
Hence, the matrix C is equal to σ 2
ε 1 plus a rank-1 perturbation σ 2
f ββT . The eigenvalues
are all equal to σ 2
ε , save the largest one, equal to σ 2
ε + σ 2
f |β|2. The corresponding top
eigenvector uf is parallel to β. When the βi’s are not too far from one another, this top
eigenvector is aligned with the uniform vector e, as found empirically.
From the analysis conducted in Chapter 14, we know that the eigenvalues of the empir-
ical matrix corresponding to such a model are composed of a Marˇcenko–Pastur “sea”
between λ−= σ 2
ε (1 −√q)2 and λ+ = σ 2
ε (1 + √q)2, and an outlier located at
λ1 = σ 2
ε (1 + a)(1 + q
a ),
(20.54)
with a = σ 2
f |β|2/σ 2
ε , and provided a > √q (see Section 14.4). Since |β|2 = O(N), the
last condition is easily satisﬁed for large portfolios. When a ≫1, one thus ﬁnds
λ1 ≈σ 2
f |β|2.
(20.55)
Since empirically λ1 ≈0.2N for the correlation matrix for which Tr C = N, we deduce
that for that normalization σ 2
ε ≈0.8. The Marˇcenko–Pastur sea for the value of q =
1/4 used in Figure 20.5 should thus extend between λ−≈0.2 and λ+ ≈1.8. Figure
20.5 however reveals that ∼20 eigenvalues lie beyond λ+, a clear sign that more factors
are needed to describe the co-movement of stocks. This is expected: the industrial sector
(energy, ﬁnancial, technology, etc.) to which a given stock belongs is bound to have some
inﬂuence on its returns as well.
20.4.3 The Rotationally Invariant Estimator for Stocks
We now determine the optimal rie corresponding to the empirical spectrum shown in
Figure 20.5. As explained in Chapter 19, there are two possible ways to do this. One is
to use Eq. (19.44) with an appropriately regularized empirical Stieltjes transform – for
example by adding a small imaginary part to λ equal to N−1/2. The second is to use a cross-
validation method, see Eq. (19.88), which is theoretically equivalent as we have shown in
Section 19.6. The two methods are compared in Figure 20.6, and agree quite well provided
one chooses a slightly higher, effective value q∗, so as to mimic the effect of temporal
correlations and ﬂuctuating variance that lead to an effective reduction of the size of the
sample (see the discussion in Section 17.2.3).
The shape of the non-linear function ξ(λ) is interesting. It is broadly in line with the
inverse-Wishart toy model shown in Figure 19.2: ξ(λ) is concave for small λ, becomes
approximately linear within the Marˇcenko–Pastur region, and becomes convex for larger λ.

336
Applications to Finance
0.0
0.5
1.0
1.5
2.0
2.5
3.0
l
0.0
0.5
1.0
1.5
2.0
2.5
3.0
x(l)
cross-validation
RIE q = 1
4
RIE q* = 1
3
0
5
10
15
20
0
5
10
15
20
Figure 20.6 Non-linear shrinkage function ξ(λ) computed using cross-validation and rie averaged
over three datasets. Each dataset consists of 500 US stocks measured over 2000 business days. Cross-
validation is computed by removing a block of 100 days (20 times) to compute the out-of-sample
variance of each eigenvector (see Eq. (19.88)). rie is computed using the sample Stieltjes transform
evaluated with an imaginary part η = N−1/2. Results are shown for q = N/T = 1/4 and also for
q∗= 1/3, chosen to mimic the effects of temporal correlations and ﬂuctuating variance that lead
to an effective reduction of the size of the sample (cf. Section 17.2.3). All three curves have been
regularized through an isotonic ﬁt, i.e. a ﬁt that respects the monotonicity of the function.
For very large λ, however, ξ(λ) becomes linear again (not shown), in a way compatible
with the general formula Eq. (19.57). The use of rie for portfolio construction in real
applications is discussed in the references below.
Bibliographical Notes
• For a general introduction on Markowitz portfolio construction, see
– E. J. Elton and M. J. Gruber. Modern portfolio theory, 1950 to date. Journal of
Banking & Finance, 21(11):1743–1759, 1997,
– M. Rubinstein. Markowitz’s portfolio selection: A ﬁfty-year retrospective. The Jour-
nal of Finance, 57(3):1041–1045, 2002,
– P. N. Kolm, R. Ttnc, and F. J. Fabozzi. 60 years of portfolio optimization: Prac-
tical challenges and current trends. European Journal of Operational Research,
234(2):356–371, 2014.
• For an application of the rie technique to portfolio construction, see
– J. Bun, J.-P. Bouchaud, and M. Potters. Cleaning correlation matrices. Risk magazine,
2016,
– J. Bun, J.-P. Bouchaud, and M. Potters. Cleaning large correlation matrices: Tools
from random matrix theory. Physics Reports, 666:1–109, 2017,

20.4 Empirical Covariance Matrices
337
– P.-A. Reigneron, V. Nguyen, S. Ciliberti, P. Seager, and J.-P. Bouchaud. Agnostic
allocation portfolios: A sweet spot in the risk-based jungle? The Journal of Portfolio
Management, 46 (4), 22–38, 2020,
and references therein.
• For reviews on the main stylized facts of asset returns, see, e.g.
– R. Cont. Empirical properties of asset returns: Stylized facts and statistical issues.
Quantitative Finance, 1(2):223–236, 2001,
– J.-P. Bouchaud and M. Potters. Theory of Financial Risk and Derivative Pric-
ing: From Statistical Physics to Risk Management. Cambridge University Press,
Cambridge, 2nd edition, 2003,
– A. Chakraborti, I. M. Toke, M. Patriarca, and F. Abergel. Econophysics review: I.
Empirical facts. Quantitative Finance, 11(7):991–1012, 2011,
– J.-P. Bouchaud, J. Bonart, J. Donier, and M. Gould. Trades, Quotes and Prices.
Cambridge University Press, Cambridge, 2nd edition, 2018.
• For early work on the comparison between the Marˇcenko–Pastur spectrum and the eigen-
values of ﬁnancial covariance matrices, see
– L. Laloux, P. Cizeau, J.-P. Bouchaud, and M. Potters. Noise dressing of ﬁnancial
correlation matrices. Physical Review Letters, 83:1467–1470, Aug 1999,
– V. Plerou, P. Gopikrishnan, B. Rosenow, L. A. N. Amaral, T. Guhr, and H. E. Stanley.
Random matrix approach to cross correlations in ﬁnancial data. Physical Review E,
65(6):066126, 2002.
• For a study of the dependence of the instantaneous covariance matrix on some dynamical
indicators, and the use of rmt in this case, see
– P.-A. Reigneron, R. Allez, and J.-P. Bouchaud. Principal regression analysis and
the index leverage effect. Physica A: Statistical Mechanics and its Applications,
390(17):3026–3035, 2011,
– A. Karami, R. Benichou, M. Benzaquen, and J.-P. Bouchaud. Conditional correla-
tions and principal regression analysis for futures. preprint arXiv:1912.12354, 2019.


Appendix
Mathematical Tools
A.1 Saddle Point Method
In this appendix, we brieﬂy review the saddle point method (sometimes also called the
Laplace method, the steepest descent or the stationary phase approximation). Consider the
integral
I =
 +∞
−∞
etF(x)dx.
(A.1)
We want to ﬁnd an approximation for this integral when t →∞. First consider the
case where F(x) is real. The key idea of the Laplace method is that when t is large I
is dominated by the maximum of F(x) plus Gaussian ﬂuctuations around it. Suppose F
reaches its maximum at a unique point x∗, then around x∗we have
F(x) = F(x∗) + F ′′(x∗)
2
(x −x∗)2 + O(|x −x∗|3),
(A.2)
where F ′′(x∗) < 0. Thus for large t, we have, after a Gaussian integral over x −x∗,
I ∼
2
2π
−F ′′(x∗)t etF(x∗),
(A.3)
where the symbol ∼means that the ratio of both sides of the equation tends to 1 as t →∞.
Often we are only interested in
lim
t→∞
1
t log I = F(x∗),
(A.4)
in which case we do not need to compute the prefactor.
Things are more subtle when F(x) is a complex analytic function of z = x + iy. One
could be tempted to think that one can just apply the Laplace method to Re(F(x)). But this
is grossly wrong, because exp(it Im(F(x))) oscillates so fast that the contribution of any
small neighborhood of x∗is killed.
The idea of the steepest descent or the stationary phase approximation relies on the fact
that, for any analytic function, the Cauchy–Riemann condition ensures that
⃗∇Re(F(z)) · ⃗∇Im(F(z)) = 0,
(A.5)
339

340
Mathematical Tools
where the gradient is in the two-dimensional complex plane. This means that the contour
lines of Re(F(z)) are everywhere orthogonal to the contour lines of Im(F(z)), or alterna-
tively that along the lines of steepest ascent (or descent) of Re(F(z)), the imaginary part
Im(F(z)) is constant.
Now, one can always deform the integration contour from the real line in Eq. (A.1)
to any curve in the complex plane that starts at z = −∞+ i0 and ends at z = +∞+ i0.
Since exp(tF(z)) has no poles, this (by Cauchy’s theorem) does not change the value of the
integral. But again because of the Cauchy–Riemann condition, ∇2 Re(F(z)) = 0, which
means that Re(F(z)) has no maximum or minimum in the complex plane, but may have
a saddle point z∗where ⃗∇Re(F(z)) = 0, increasing in one direction and decreasing in
another (see Fig. A.1). Choosing the contour that crosses the saddle point z∗by following a
path that is locally orthogonal to the contour lines of Re(F(z)) allows the phase Im(F(z))
to be stationary, hence avoiding the nulliﬁcation of the integral by rapid oscillations. The
ﬁnal result is then
I ∼
2
2π
−tF ′′(z∗) etF(z∗).
(A.6)
The method is illustrated in Figure A.1 for the example of the Airy function, deﬁned as
Ai(t) = 1
2π
 +∞
−∞
dzetF(z,t),
F(z,t) := i

z + z3
3t

.
(A.7)
For a ﬁxed, large positive t, the points for which F ′(z,t) = 0 are given by z± = ±i √t. The
contour lines of Re(F(z,t)) are plotted in Figure A.1. For Im(z) < 0, Re(F(z,t)) →+∞
−6
−4
−2
0
2
4
6
x
−6
−4
−2
0
2
4
6
y
0i
0i
0i
-12
-12
-9
-9
-6
-6
-4
-4
-3
-3
-2
-2
-1
0
1
2
2
3
3
4
4
6
6
9
9
12
12
Figure A.1 Contour lines of the real part of F(z,t) = i

z + z3/3t

for t = 16 (black lines). The
iso-phase line Im F(z,t) = 0 is also shown (gray lines). The black circle and square are the two
solutions z± of F ′(z,t) = 0. The relevant saddle is z∗= +4i, for which F(z∗,t) = −8/3.

A.2 Tricomi’s Formula
341
when Re(z) →±∞, so one cannot deform the contour in that direction without introducing
enormous uncontrollable contributions. When Im(z) > 0, on the contrary, Re(F(z,t)) →
−∞when Re(z) →±∞, so one can start at z = −∞+ i0, and travel upwards in the
complex plane to the line where Im(F(z,t)) = 0 in a region where Re(F(z,t)) is so
negative that there is no contribution to the integral from this part. Then one stays on the
iso-phase line Im(F(z,t)) = 0 and climbs upwards to the saddle point z∗= z+ = +i √t,
for which F(z∗,t) = −2 √t/3. One then continues on the same iso-phase line downwards,
and closes the contour towards z = +∞+ i0. Hence, we conclude that
lim
t→∞log Ai(t) = tF(z∗,t) = −2
3t3/2.
(A.8)
A more precise expression, including the prefactor in Eq. (A.6), is written
Ai(t) ∼
1
2 √πt1/4 e−2
3 t3/2.
(A.9)
Exercise A.1.1
Saddle point method for the factorial function: Stirling’s approx-
imation
We are going to estimate the factorial function for large arguments using an
integral representation and the saddle point approximation:
n! = 
[n + 1] =
 ∞
0
xne−xdx.
(A.10)
(a)
Write n! in the form Eq. (A.1) for some function F(x).
(b)
Show that x∗= n is the solution to F ′(x) = 0.
(c)
Let I0(n) = nF(x∗(n)) be an approximation of log(n! ). Compare this
approximation to the exact value for n = 10 and 100.
(d)
Include the Gaussian corrections to the saddle point: Let I1(n) = log(I) where
I is given by Eq. (A.3) for your function F(x). Show that
I1(n) = n log(n) −n + 1
2 log(2πn).
(A.11)
(e)
Compare I1(n) and log(n! ) for n = 10 and 100.
A.2 Tricomi’s Formula
Suppose one has the following integral equation to solve for ρ(x):
f (x) = −

dx′ ρ(x′)
x −x′,
(A.12)
where f (x) is an arbitrary function. This problem arises in many contexts, in particular
when one studies the equilibrium density of the eigenvalues of some random matrices as in
Section 5.5, or the equilibrium density of dislocations in solids, see Landau et al. [2012],
Chapter 30, from which the material of the present appendix is derived.

342
Mathematical Tools
We will consider the case where some “hard walls” are present at x = a and x = b,
conﬁning the density to an interval smaller than its “natural” extension in the absence of
these walls. By continuity, we will also get the solution when these walls are exactly located
at these natural boundaries of the spectrum.1
The general solution of Eq. (A.12) is given by Tricomi’s formula which reads
ρ(x) = −
1
π2 √(x −a)(b −x)

−
 b
a
dx′ *
(x′ −a)(b −x′) f (x′)
x −x′ + C

,
(A.13)
where C is a constant to be determined by some conditions that ρ(x) must fulﬁll.
The simplest case corresponds to f (x) = 0, i.e. no conﬁning potential apart from the
walls. The solution then reads
ρ(x) = −
C
π2 √(x −a)(b −x).
(A.14)
The normalization of ρ(x) then yields C = −π and one recovers the arcsine law encoun-
tered in Sections 5.5, 7.2 and 15.3.1:
ρ(x) =
1
π √(x −a)(b −x).
(A.15)
The canonical Wigner case corresponds to f (x) = x/2σ 2. We look for the values of a,b
that are precisely such that the density vanishes at these points, so that the conﬁning walls
no longer have any effect. By symmetry one must have a = −b. One then obtains
ρ(x) = −
1
2π2σ 2 √(x + b)(b −x)

−
 b
−b
dx′ *
(x′ + b)(b −x′)x′ −x + x
x −x′
+ C

.
(A.16)
Using
x −
 b
−b
dx′
√(x′ + b)(b −x′)
x −x′
= πx2
(A.17)
one has
ρ(x) = −
1
2π2σ 2 √(x + b)(b −x)

πx2 + C′
,
(A.18)
where C′ = C −
 b
−b dx′ √(x′ + b)(b −x′) = C −πb2/2.
For ρ(x) to vanish at the edges, we need to choose C′ = −πb2, ﬁnally leading to
ρ(x) =
1
2πσ 2
*
b2 −x2.
(A.19)
Finally, for this distribution to be normalized one must choose b = 2σ, as it should be.
1 Formulas directly adapted to two free boundaries, or one free boundary and one hard wall, can be found in Landau et al.
[2012], Chapter 30.

A.3 Toeplitz and Circulant Matrices
343
The case studied by Dean and Majumdar, i.e. when all eigenvalues of a Wigner matrix
are contrained to be positive (Eq. (5.93)), corresponds to Eq. (A.13) with a = 0, b > 0
and σ = 1 (i.e. f (x) = x/2), determined again such that ρ(b) = 0. The solution is
b∗= 4/
√
3 and
ρ(x) = 1
4π
1
b∗−x
x
(b∗+ 2x).
(A.20)
Note that very generically the density of eigenvalues (or dislocations) diverges as 1/
√
d
near a hard wall, where d is the distance to that wall. When the boundary is free, on the
other hand, the density of eigenvalues (or dislocations) vanishes as
√
d.
A.3 Toeplitz and Circulant Matrices
A Toeplitz matrix is such that its elements Kij only depend on the “distance” between
indices |i −j|. An example of a Toeplitz matrix is provided by the covariance of the
elements of a stationary time series. For instance consider an AR(1) process xt in the steady
state, deﬁned by
xt = axt−1 + εt,
(A.21)
where εt are iid centered random numbers with variance 1/(1 −a2) such that xt has unit
variance in the steady state. Then
Kts = E[xtxs] = a−|t−s| with 0 < a < 1,
(A.22)
describing decaying exponential correlations. The parameter a measures the decay of the
correlation; we can deﬁne a correlation time τc := 1/(1 −a). This time is always ≥1
(equal to 1 when K = 1) and tends to inﬁnity as a →1. More explicitly, K is a T × T
matrix that reads
K =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
1
a
a2
. . .
aT −2
aT −1
a
1
a
. . .
aT −3
aT −2
a2
a
1
. . .
aT −4
aT −3
...
...
...
...
...
...
aT −2
aT −3
aT −4
. . .
1
a
aT −1
aT −2
aT −3
. . .
a
1
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
(A.23)
In an inﬁnite system, it can be diagonalized by plane waves (Fourier transform), since
+∞

s=−∞
Ktse2πixs = e2πixt
+∞

s=−∞
Ktse2πix(s−t) = e2πixt
+∞

ℓ=−∞
a|ℓ|e2πixℓ,
(A.24)
showing that e2πixs are eigenvectors of K as soon as its elements only depend on |s −t|.

344
Mathematical Tools
For ﬁnite T , however, this diagonalization is only approximate as there are “boundary
effects” at the edge of the matrix. If the correlation time τc is not too large (i.e. if the
matrix elements Kts decay sufﬁciently fast with |t −s|) these boundary effects should be
negligible. One way to make this diagonalization exact is to modify the matrix so as to have
the distance |t −s| deﬁned on a circle.2 We deﬁne a new matrix ˜K by
˜Kts = a−min(|t−s|,|t−s+T |,|t−s−T |),
(A.25)
˜K =
⎛
⎜⎜⎜⎜⎜⎜⎜⎜⎝
1
a
a2
. . .
a2
a
a
1
a
. . .
a3
a2
a2
a
1
. . .
a4
a3
...
...
...
...
...
...
a2
a3
a4
. . .
1
a
a
a2
a3
. . .
a
1
⎞
⎟⎟⎟⎟⎟⎟⎟⎟⎠
.
(A.26)
It may seem that we have greatly modiﬁed matrix K as we have changed about half of its
elements, but if τc ≪T most of the elements we have changed were essentially zero and
remain essentially zero. Only a ﬁnite number (≈2τ 2
c ) of elements in the bottom right and
top left corners have really changed. Changing a ﬁnite number of off-diagonal elements in
an asymptotically large matrix should not change its spectrum. The matrix ˜K, which we
will call K again, is called a circulant matrix and can be exactly diagonalized by Fourier
transform for ﬁnite T . More precisely, its eigenvectors are
[vk]ℓ= e2πikℓ/T for 0 ≤k ≤T /2.
(A.27)
Note that to each vk correspond two eigenvectors, namely its real and imaginary parts,
except for v0 and vT/2 which are real and have multiplicity 1. The eigenvalues associated
with k = 0 and k = T /2 are, respectively, the largest (λ+) and smallest (λ−) and are
given by
λ+ = 1 + 2
T/2−1

k=1
ak + aT/2 ≈1 + a
1 −a,
λ−= 1 + 2
T/2−1

k=1
(−a)k + (−a)T/2 ≈1 −a
1 + a = 1
λ+
.
(A.28)
In terms of the correlation time: λ+ = 2τc −1. We label the eigenvalues of K by an index
xk = 2k/T so that 0 ≤xk ≤1. As T →∞, xk becomes a continuous parameter x and the
different multiplicity of the ﬁrst and the last eigenvalues does not matter. The eigenvalues
can be written
λ(x) =
1 −a2
1 + a2 −2a cos(πx) for 0 ≤x ≤1.
(A.29)
2 The Toeplitz matrix K can in fact be diagonalized exactly, see O. Narayan, B. Sriram Shastry, arXiv:2006.15436v2.

A.3 Toeplitz and Circulant Matrices
345
For a more general form Kts = K(|t −s|), the eigenvalues read
λ(x) = 1 + 2
∞

ℓ=1
K(ℓ) cos(πxℓ).
(A.30)
The T-transform of K can then be computed as
tK(z) =
 1
0
1 −a2
z(1 + a2 −2a cos(πx)) −(1 −a2)dx.
(A.31)
Using
 1
0
dx
c −d cos(πx) =
1
√c −d √c + d
,
(A.32)
and after some manipulations we ﬁnd
tK(z) =
1
√z −λ−
√z −λ+
with λ± = 1 ± a
1 ∓a .
(A.33)
We can also deduce the density of eigenvalues (see Fig. A.2):
ρK(λ) =
1
πλ
*
(λ −λ−)(λ+ −λ)
for λ−< λ < λ+.
(A.34)
This density has integrable singularities at λ = λ±. It is normalized and its mean is 1. We
can also invert tK(z) with the equation
t2ζ 2
K −2bt2ζK + t2 −1 = 0, where b = 1 + a2
1 −a2,
(A.35)
0
2
4
6
0
1
2
3
4
5
6
a = 0.25
a = 0.5
a = 0.75
Figure A.2 Density of eigenvalues for the decaying exponential covariance matrix K for three values
of a: 0.25, 0.5 and 0.75.

346
Mathematical Tools
and get
ζK(t) = bt2 +
*
(b2 −1)t4 + t2
t2
,
(A.36)
so the S-transform is given by
SK(t) =
t + 1
*
1 + (b2 −1)t2 + bt
(A.37)
= 1 −(b −1)t + O(t2),
where the last equality tells us that the matrix K has mean 1 and variance σ 2
K = b −1 =
a2/(1 −a2).
Bibliographical Notes
• On the saddle point method, see the particularly clear exposition in
– J. Hinch. Perturbation Methods. Cambridge Texts in Applied Mathematics. Cam-
bridge University Press, Cambridge, 1991.
• About the Tricomi method and the role of boundaries, Chapter 30 of Landau and Lif-
shitz’s Theory of Elasticity is enlightening:
– L. D. Landau, L. P. Pitaevskii, A. M. Kosevich, and E. M. Lifshitz. Theory of Elas-
ticity. Butterworth-Heinemann, Oxford, 3rd edition, 2012.
For its use in the context of random matrix theory, see
– D. S. Dean and S. N. Majumdar. Extreme value statistics of eigenvalues of Gaussian
random matrices. Physical Review E, 77:041108, 2008.

Index
AR(1) process, 343
arcsine law, 80, 101, 106, 195
Bachelier, 330
BBP transition, 225
Bernoulli variable, 284
beta, 30
distribution, 97
ensemble, 59
branch cut, 22, 52, 70
Brownian motion, 112
Burgers’ equation, 128, 136, 263
Catalan numbers, 37
Cauchy
distribution, 28
kernel, 24, 315
transform, see Stieltjes transform
central limit theorem, 160, 169, 183
characteristic
function, 112, 158
polynomial, 5
expected, 85
higher moment, 95
Chebyshev polynomial, 104, 186
Cholesky decomposition, 56
circulant matrix, 343
circular law, 34
concentration of measure, 16
conjugate prior, 286
conjugate variable, 118
constant, 156, 162
convolution, 111
Coulomb potential, 64
cross-covariance, 278
cross-validation, 317
cumulant, 112, 157, 163
mixed, 168, 181
Dirac delta, 20, 142
Dyson
Brownian motion, 122
index, see beta
Edgeworth series, 186
eigenvalue, 5
density, 16
critical, 71, 228
edge, 26, 71
Jacobi, 100
sample, 20
white Wishart, 50
Wigner, 18, 26
interlacing theorem, 10
multiplicity, 5
repulsion, 64
eigenvector, 5
overlap, 227, 297
elastic net, 291
empirical spectral distribution, 20
ergodicity, 118
Euler–Matytsin equation, 151
excess return, 321
expected shortfall, 322
exponential
correlation, 343
moving average, 269
fat tail distribution, 229, 285, 331
ﬁnite free
addition, 187
product, 190
Fokker–Planck equation, 119, 131
free
addition, 140, 163
log-normal, 261
product, 56, 170
347

348
Index
freeness, 162
collection, 169, 196
large matrices, 177, 243
Frobenius norm, 17
gamma distribution, 46, 97
Gaussian distribution, 112
Gaussian ensemble, 16
orthogonal (goe), 17
symplectic (gse), 33
unitary (gue), 32
Gershgorin circle theorem, 9
Ginibre ensemble, 34
Girko’s law, see circular law
Gram–Schmidt procedure, 56
Haar measure, 140, 178
Harish-Chandra integral, see hciz integral
hciz integral
low rank, 209
hciz integral, 141
full rank, 149
low rank, 141
Hermite polynomial, 84, 188
Hermitian matrix, 30
Hessian matrix, 74
heteroskedasticity, 276, 333
Hilbert transform, 68, 299
Hubbard–Stratonovich identity, 202
in-sample, 291, 317, 324
independence, 156
pairwise, 161
instanton, 152
inverse-gamma distribution, 248, 287
inverse-Wishart, 246, 295, 328
involution, 156
isotonic regression, 316
Itˆo
lemma, 114, 124
prescription, 113
Itzykson–Zuber integral, see hciz integral
Jacobi
ensemble, 98, 255
centered-range, 102
polynomial, 103, 104
Jacobian, 13, 47, 59
Jensen inequality, 324
Karlin–McGregor formula, 133, 150
Kesten problem, 264
l’Hospital’s rule, 227
Laguerre polynomial, 88, 189
Langevin equation, 116, 119
Laplace distribution, 285
Laplacian, discrete, 106
large deviation, 76, 79
Laurent polynomial, 66
law of large numbers, 160, 169
Legendre polynomial, 103
level spacing, 63
local universality, 130, 135
Lyapunov exponent, 259
Mahalanobis norm, 56
MANOVA ensemble, 97
map estimator, 282
Marˇcenko–Pastur, see Wishart matrix
market mode, 334
Markowitz optimization, 322
Master equation, 4
matrix
derivative, 12
determinant lemma, 11, 232
function, 12
matrix potential, 58
and Hilbert transform, 68
convex, 70
inverse-Wishart, 247
Jacobi, 100
non-polynomial, 68
white Wishart, 47, 59
Wigner, 19, 58
Matytsin, see Euler–Matytsin equation
mave estimator, 282
maximum likelihood, 64
mmse estimator, 282, 302
moment, 17, 156
addition, 156
generating function, 20, 158
Wigner matrix, 17
moment–cumulant relation
commutative, 159
free, 167
multivariate Gaussian, 44
non-crossing partition, 37, 166
normal matrix, 9
normalized trace, 16
one-cut assumption, 70
one-factor model, 334
oracle, 303
Ornstein–Uhlenbeck process, 116
orthogonal
ensemble, 58
matrix, 6, 18
out-of-sample, 292, 317, 324

Index
349
outlier, 149, 221, 308
overﬁtting, 293
Perron–Frobenius theorem, 9, 309
phase retrieval, 234
planar diagrams, 72
polynomial
monic, 5, 186
real-rooted, 186
portfolio, 321
risk, 322
positive semi-deﬁnite, 4
precision matrix, 327
principal component analysis, 6
principal components, 56
pseudo-inverse, 179
quarter-circle law, 34
quaternion, 32
R-transform, 138, 224
arcsine law, 249
identity matrix, 244
inverse-Wishart, 247
Jacobi ensemble, 256
scaling, 139, 244
symmetric orthogonal matrix, 249
uniform density, 250
white Wishart, 138
Wigner, 138
rank-1 matrix, 141, 223
regression, 289
lasso, 291
ridge, 289
replica trick, 199
resolvent, 19, 126, 204
Ricatti equation, 83
risk free rate, 321
rotationally invariant ensemble, 18
rotationally invariant estimator, 302, 327
S-transform, 172, 233
arcsine law, 256
identity matrix, 172, 244
inverse matrix, 175
inverse-Wishart, 247
Jacobi ensemble, 256
scaling, 172, 244
white Wishart, 246
Wigner, 245, 246
saddle point method, 339
sample covariance matrix, 43, 267
scalar, 156
Schur complement, 11, 21, 48
self-averaging, 16
semi-circle law, 18, 26
Sherman–Morrison formula, 11, 223
shrinkage, 283, 296, 305
signature plot, 331
singular values, 7
Sokhotski–Plemelj formula, 26, 172
spin-glass, 215
±
⃝√· notation, 54
*-algebra, 155
stationary phase approximation, 143, 339
Stieltjes transform
correct branch, 23
discrete, 19
identity matrix, 244
inverse-Wishart, 247
inversion formula, 26
invertibility, 145, 224
Jacobi, 100
limiting, 20
uniform density, 250
white Wishart, 50
Wigner, 22
Stirling’s approximation, 341
stochastic
calculus, 112
matrix, 4
Stratonovich prescription, 113
Student’s t-distribution, 118, 331
subordination relation
addition, 170
product, 174
resolvent, 207
symmetric
matrix, 5
orthogonal matrix, 13, 139, 249
symplectic
ensemble, 59
matrix, 33
T-matrix, 172
T-transform, 171
identity matrix, 244
inverse matrix, 175
temporal correlation, 271, 330
Tikhonov regularization, 290
Toeplitz matrix, 343
traceless, 162
tracial state, 155
training set, 317
Tricomi equation, 79, 341
uniform density, 69, 250
unitary
ensemble, 59, 93
matrix, 31
validation set, 290, 317
Vandermonde determinant, 61, 91
variance, 156

350
Index
ﬂuctuation, 276, 333
Wigner matrix, 18
Wishart matrix, 46
Weingarten function
orthogonal, 178
unitary, 181
whitening, 56
Wick’s theorem, 45, 178
Wiener process, 112
Wigner
ensemble, 17, 30
kernel, 316
surmise, 63
Wishart matrix, 45
complex, 92
non-white, 55
white, 47



OLIVER PRETZEL 
Imperial College, London 
Error-Correcting Codes 
and Finite Fields 
CLARENDON PRESS 
· OXFORD 
1992 

Oxford University Press, Walton Street, Oxford OX2 6DP 
Oxford New York Toronto 
Delhi Bombay Calcutta Madras Karachi 
Petaling Jaya Singapore Hong Kong Tokyo 
Nairobi Dar es Salaam Cape Town 
Melbourne Auckland 
and associated companies in 
Berlin lbadan 
Oxford is a trade mark of Oxford University Press 
Published in the United States 
by Oxford University Press, New York 
©Oliver Pretzel, 1992 
All rights reserved. No part of this publication may be 
reproduced, stored in a retrieval system, or transmitted, in any 
form or by any means, without the prior permission in writing of Oxford 
University Press. Within the UK, exceptions are allowed in respect of any 
fair dealing for the purpose of research or private study, or criticism or 
review, as permitted under the Copyright, Designs and Patents Act, 1988, or 
in the case of reprographic reproduction in accordance with the terms of 
licences issued by the Copyright Licensing Agency. Enquiries concerning 
reproduction outside those terms and in other countries should be sent to 
the Rights Department, Oxford University Press, at the address above. 
This book is sold subject to the condition that it shall not, by way 
of trade or otherwise, be lent, re-sold, hired out, or otherwise circulated 
without the publisher's prior consent in any form of binding or cover 
other than that in which it is published and without a similar condition 
including this condition being imposed on the subsequent purchaser 
A catalogue record for this book is available from the British Library 
Library of Congress Cataloging in Publication Data 
Pretzel, Oliver. 
Error-correcting codes and finite fields/Oliver Pretzel. 
(Oxford applied mathematics and computing science series) 
Includes bibliographical references. 
1. Error-correcting codes (Information theory) 
I. Title. 
II. Series. 
QA268.P74 1992 
003'.54--dc20 
92-4088 
ISBN 0 19 859678 2 
Typeset by Integral Typesetting, Gorleston, Norfolk NR31 6RG 
Printed in Great Britain by 
Bookcraft (Bath) Ltd, 
Midsomer Norton, Avon 

To Christl and Raimund 


Preface 
This book arose out of a series of courses given to students of mathematics 
and electrical engineering at Imperial College. The theory of error-correcting 
block codes combines mathematical elegance and practical utility to an 
unusual degree. Thus, the intention of the courses was twofold. On the one 
hand I wished to introduce the mathematicians to some attractive practical 
problems and to address. these as an essential part of the development of a 
mathematical theory. On the other I hoped to persuade engineers of the 
power and elegance of modern mathematics and to give them confidence in 
using it. 
There are many excellent texts on coding theory, notably that by 
MacWilliams and Sloane, but I found that they were either too advanced 
for my purposes or stopped short of providing all the mathematical tools 
required to implement a coding system (like the excellent introductions by 
Hill or Pless). I therefore wrote my own set of lecture notes, which form the 
basis of Parts 1-3 of the book. These start with a standard elementary 
introduction to coding theory, then develop the theory of finite fields (which 
is an essential tool) and in Part 3, exploit it to construct and decode BCH 
and Reed-Solomon codes. 
I abhor tome-like textbooks that skim over a vast array of topics saying 
only trivialities about all of them. So this book does require its reader to 
think. My experience has been that although electrical engineers go through 
a kind of culture shock as the material on finite fields is presented, they 
emerge confident that they can apply them in the many areas of their 
discipline in which they appear. Similarly mathematicians used to abstract 
generalities and existence theorems find the concerns of coding theory 
unfamiliar but gain a deeper understanding of the mathematical theory by 
seeing it at work. 
The standard courses at Imperial College covered most of the material in 
Parts 1-3. The additional sections (the Extras), were only mentioned, or left 
out of the course entirely. However, in writing the book, I could not resist 
the temptation to add a further part on Gappa codes (both classical and 
geometrical) including the decoder of Skorobogatov and Vladut. This part 
was tried out in a postgraduate course at the University of London. During 
that course it became evident that the major difficulty in presenting geometric 
Gappa codes is to find a simple presentation for the geometry of algebraic 
curves. Chapters 21-23 are my 
attempt to do this. I hope that once a reader 
has worked through the first three parts of the book, these chapters will not 

viii 
Preface 
present excessive difficulties. In treating Goppa's codes, I have tried to exhibit 
them as natural generalizations ofBCH codes, and included proofs that BCH 
codes are a special class of both classical and geometric Goppa codes in the 
exercises. 
All parts contain exercises. These range from routine calculations to 
extensions of the theory. Routine calculations are extremely important for 
the understanding of the subject, and all chapters contain extensive examples 
to guide the reader. He or she should work through these carefully and then 
do the corresponding exercises to gain confidence. The mo{e theoretical 
exercises are to some extent optional. They certainly contain material that 
will deepen the reader's understanding of the codes, but on the other hand, 
they are not essential. How many of these should be attempted depends on 
the reader's purpose in studying the book and on his or her 'mathematical 
maturity'. Attempt at least a few, and if you enjoy them try more. 
The book also contains two short appendices, one on linear algebra and 
the other on polynomials. The first is provided mainly because engineers 
frequently have not seen the rank and nullity theorem explicitly and also 
because linear algebra is usually taught only for real and complex numbers, 
whereas coding theory uses finite fields. The second is a quick refresher on 
the properties of polynomials. 
Each of the four parts of the book is divided into chapters. The chapters 
are split into sections or paragraphs, numbered consecutively. Theorems, 
propositions, and definitions are referred to by their section numbers. If the 
reference is to Section 7 in Chapter 3, it is given as 3.7. Some chapters have 
'extra' sections at the end that can be omitted at first reading. The start of 
these sections is marked by the heading 'Extras'. External references are 
given by authors' names and the date; there are separate lists for textbooks 
and papers in journals. 
My thanks are due to my students and colleagues who attended the 
lectures. I frequently learned as much from them as they did from the courses. 
What qualities the book may have are due to them in no small measure. I 
must particularly thank Benjamin Baumslag who read early drafts in detail 
and made many suggestions for improvement. Without his enthusiasm and 
insistence I would never have completed the book. I would also like to thank 
the copy editor for his careful reading of the text. He introduced many 
improvements. Naturally, its deficiencies are my own. 
Imperial College, London 
March 1992 
O.R.L.P. 

Contents 
PART 1 
BASIC CODING THEORY 
1 
Introduction 
3 
Errors of transmission. Examples from natural language. Channel models. 
The binary symmetric channel. Three simple codes (a parity check code, a 
triple repetition code, and a triple parity check code). 
2 
Block codes, weight, and distance 
1.3 
Block codes. Block length, message block length, and rate. Definition of 
Hamming weight and distance. Minimum distance, error detection, and error 
correction. Block and message success probabilities. Calculation of error 
detection/correction probabilities for the examples of Chapter 1. Discussion 
of Shannon's theorem (without proof). 
' 
3 
Linear codes 
27 
Definition of linear codes and fields. Dimension and rate. The generator 
matrix. Standard form generator matrices and systematic encoding. Message 
and check bits. The check matrix. Uniqueness of standard form generator 
and check matrices. 
4 
Error processing for linear codes 
47 
Decoding by cosets (standard array). Coset leaders and syndromes. Code 
can correct single errors if and only if check matrix has distinct non..:zero 
columns. Conditions for multiple error correction. 
5 
Hamming codes and the binary Golay codes 
63 
Definition of the sequence of binary Hamming codes Ham(k) by their check 
matrices. Success probabilities for Hamming codes. Long Hamming codes 
are very efficient, but poor at correcting errors. Perfect codes. Construction 
of the binary Golay codes by Turyn's method. 
Appendix LA 
Linear algebra 
79 
The laws of arithmetic: rings, domains, and fields. Elementary vector space 
theory. Bases and dimension. Elementary matrix theory. Row operations, 
rank, and nullity. Vandermonde matrices. 

x 
Contents 
PART 2 
FINITE FIELDS 
6 
Introduction and an example 
95 
The need for fields other than Z/2. An attempt to construct a field of order 
16. Z/16 will not do. Polynomial arithmetic. Table of GF(16). 
7 
Euclid's algorithm 
106 
Division with remainder. Euclidean domains with F[x] and Z as examples. 
Euclid's algorithm in tabular form for a Euclidean domain. Finding the 
highest common factor in the form (a, b) = ua + vb. 
Extras. Relations between entries in the table for Euclid's algorithm. 
Continued fractions. Convergents, the entries in the tabular form of Euclid's 
algorithm and convergents to continued fractions. 
8 
Invertible and irreducible elements 
122 
Definition of invertible elements in a Euclidean domain. Definition of 
irreducible elements in a Euclidean domain. The 1-trick and the key property 
of irreducible elements. Discussion of unique factorization. 
Extras. Proof of unique factorization. 
9 
The construction of fields 
136 
Construction of the factor ring (residue class ring) Dja. D/a 
is a field if and 
only if a is irreducible. Using Euclid's algorithm to perform field arithmetic 
in F[x]/.f(x). Examples: GF(16) as GF(2)[x]/(x4 + x3 + 1), Z/787. 
10 
The structure of finite fields 
151 
The prime field and the characteristic. The order of a finite field. The 
Frobenius automorphism x -+ xP. Fermat's little theorem: ifF has order q 
then all its elements are roots of xq- x. Example: GF(16). 
1 1  
Roots of polynomials 
166 
The evaluation map. Its basic properties (i.e. it is a homomorphism). The 
formal derivative. Horner's scheme for evaluating a polynomial. Extension 
of Horner's scheme to evaluate the derivative. Multiple roots. The minimal 
polynomial of IX. Characterization of the minimal polynomial and the set 
(ideal) of polynomials with IX as a root. List of minimal polynomials of 
elements of GF(16). Isomorphism F[IX] ॼ F[x]/mp..(x). Construction of a 
field containing a root of a given polynomial. Existence of finite fields of all 
legal orders. 
Extras. Calculation of the minimum polynomial of P using the Frobenius 
automorphism. 

Contents 
xi 
12 
Primitive elements 
179 
Definition of primitive elements. Primitive elements of GF(16). Logarithms 
for calculating products and quotients in finite fields. Zech logarithms for 
calculating sums. Primitive polynomials. Existence of primitive elements. 
Existence of subfields of all legal orders. Isomorphism of fields of the same 
order. The polynomial xq - x is the product of all irreducible polynomials 
of degree dividing q. 
Extras. The number of irreducible polynomials of a given degree. 
Appendix PF 
Polynomials over a field 
191 
Recapitulation of the basic theory of polynomials over a field. Definition, 
addition, multiplication, degree. F[x] is an integral domain. Division with 
remainder. Polynomials in two indeterminates. 
PART 3 
BCH CODES AND OTHER POLYNOMIAL 
CODES 
13 
BCH codes as subcodes of Hamming codes 
201 
Example: BCH(4, 2) constructed from Ham(4) by extending the check matrix 
H4. Extensions must not be linear (or quadratic). View Hk 
as having entries 
in GF(2k). Criterion for multiple error correction. Vandermonde matrices. 
The full check matrix V4•2 
and the reduced check matrix H4,2 (Jik,, and Hk,t 
in general). Example BCH(4, 3). BCH(k, t) can correct t errors per block. It 
has block length 2k - 1 and dimension  2k - 1 - kt. 
14 
BCH codes as polynomial codes 
216 
Example: BCH(4, 3) used throughout to illustrate the theory. Code words 
as polynomials. Redefine BCH(k, t) in terms of polynomials. The generator 
polynomial of BCH(k, t). Dimension of BCH(k, t). 
Encoding by multiplica­
tion. The check polynomial of BCH(k, t). Use of the check polynomial, to 
verify and decode a code word. Systematic encoding by division with 
remainder. 
Extras. Polynomial codes in general. Cyclic codes in general. Recognition 
of polynomial and cyclic codes. 
15 
BCH error correction: (1) the fundamental equation 
233 
Example BCH(4, 3) continued. The error polynomial and error locations. 
Syndromes; calculation via Horner's scheme. Direct solution of case of two 
errors. The syndrome polynomial. Derivation of the fundamental equation. 
The error locator, error evaluator and error co-evaluator polynomials. 
Uniqueness of these as solutions of the fundamental equation. 

xii 
Contents 
16 
BCH error correction: (2) an algorithm 
249 
Example 
BCH(4, 3) 
continued. 
The 
Sugiyama-Kasahara-Hirasawa­
Namekawa error processor using Euclid's algorithm. Failure modes of the 
algorithm. 
17 
Reed-Solomon codes and burst error correction 
267 
Example RS(4, 3) used throughout. The Reed-Solomon code RS(k, t) corre­
sponding to BCH(k, t). Adaptation of the decoding algorithm to RS(k, t). 
Failure modes. RS(k, t) as a cyclic code over GF(2k). Parameters of RS(k, t) 
over GF(2k) and GF(2). RS(k, t) as a burst error-correcting code. Comparison 
with interleaved BCH(k, t). 
Extras. 
Detailed proofs of the statements concerning error modes. 
18 
Bounds on codes 
287 
Extending, shortening and puncturing a code. The Singleton bound. MDS 
codes. Reed-Solomon codes are MOS. Coding bounds based on sphere 
packing: the Had;ming bound, the Gilbert-Varshamov bound. The asymp­
totic Gilbert-Varshamov bound.· 
Good and bad families of codes. BCH codes are bad in relation to their 
designed distance, altH'ough their parameters for moderate block lengths are 
good. Estimates for the true minimum distance. Discussion of the fact that 
BCH codes are still bad for their true minimum distance. 
Extras. 
Proof of the estimates used in establishing the asymptotic Gilbert­
Varshamov bound. 
PART 4 
CLASSICAL AND GEOMETRIC GO PPA 
CODES 
19 
Classical Goppa codes 
303 
Definition of the Goppa Code GC(P, g) with Goppa polynomial g(x). 
Rational functions over GF(q). Dimension of GC(P, g), special case of binary 
Goppa codes. Minimum distance of the GC(P, g). Goppa codes and codes of 
BCH-type. 
20 
Classical Goppa codes: error processing 
320 
The error locator and error evaluator polynomials, the fundamental equa­
tion. Euclid's algorithm decoding for GC(P, g). 
Extras. 
Classical Goppa codes are bad for their designed distance, but there 
exists a sequence of classical Goppa codes that is good for the true minimum 
distance. 

Contents 
xiii 
21 
Introduction to algebraic curves 
333 
Irreducible polynomials in F[x, y] and plane curves. Points of curves. Points 
over extension fields. Projective transformations and points at infinity (using 
three affine coordinate systems). Calculation of points on example curves 
over finite fields. 
22 
Functions on algebraic curves 
343 
The coordinate ring of a curve and the field of functions of a curve. In variance 
under projective transformation. Rational and non-rational curves. Orders 
of functions at points. Non-singular points. 
23 
A survey of the theory of algebraic curves 
355 
The existence of non-singular points. The degree theorem, divisors, £-space, 
rank, and degree of a divisor. Explicit calculation of bases of £-spaces for 
example divisors. Riemann's theorem and the genus. The Plucker formula 
for the genus. The main theorems are not proved, but consequences from 
them are drawn, and some important special cases are dealt with in detail. 
24 
Geometric Goppa codes 
368 
Definition of geometric Goppa codes and their duals. Classical Goppa <;:odes 
as examples. Check matrices for Goppa codes. Dimension and minimal 
distance. One-point codes. Examples based on the curves of Chapter 21. 
Extras. 
Discussion of the fact that primary and dual Goppa codes form 
the same class. 
25 
An error pro.cessor for geometric Goppa codes 
379 
Conditions under which the error processor will work. Specialization to 
one-point codes. The Skorobogatov-Vliidut error processor. Error locator 
functions in the £-space of the auxiliary divisor. Proof that the error 
processor works. Example calculations for short one-point codes. Compari­
son of a long one-point code with a Reed-Solomon code. 
Extras. 
Discussion of Tsfasman-Vliidut-Zink curves as examples of curves 
with many rational points. Estimates for Goppa codes based on such curves. 
Some sequences of geometric Goppa codes meet the asymptotic Gilbert­
Varshamov bound, even when error correction is limited to the SV -processor. 
Bibliography 
391 
Index 
394 


Part 1 
Basic coding theory 


1 
Introduction 
You wake up one morning and in the half-light you see a figure with a 
strange hat crouching in the corner of the room. After a moment your eyes 
adjust and you realize that it is just your clothes thrown over a chair, and 
some bottles on the dressing table behind. Now yott notice that your loved 
one is gone and find a note on the pillow saying 'I LOVE XOU'. 
Almost certainly this will reassure you, as you will assume that in the dark 
the Y became an X. Of course, that is not 100 per cent certain. It is just 
possible that the X was intended for an L and that you have been abandoned 
for your close friend (or so you thought) Lou. 
This example contains the essence of coding theory. In transmitting or 
storing and reading messages there is always a possibility of error and any 
robust communication system must cope with it with a high degree of 
reliability. Natural systems such as our eyes or the English language achieve 
reliability in two ways that have similar features, but are clearly distinct. The 
first part of the story illustrates the way our eyes work. That seems to be to 
use experience to guess the meaning of what they see. Our brains are very 
good at this, perhaps because they use many independent guessing mechan­
isms and then compare the results. We can be fooled, by optical illusions 
or when we are disoriented, but on the whole our eyes are very reliable. 
The mechanism used by language is slightly different. Consider the pillow 
note in the example. Xou is not a word in the English language. So either 
it represents a name, or you know that an error has occurred. Now, certain 
types of error are far more frequent than others. Roughly, we can say that 
errors will, with very high probability, involve only a few letters. There are 
not many words in the English language that can be transformed into each 
other by changing only a few letters. So it is likely that there are very few 
candidates for the correct version of a misspelled word. English operates a 
similar mechanism at the next level. Most word sequences either do not 
make sense or violate the laws of grammar. Thus there is further checking 
to see if a word is correct. 
The difference between the two systems used by vision and language is 
that the second makes use of built-in restrictions in the language rather than 
experience. These restrictions ensure that most sequences of letters do not 
form words, and most sequences of words do not form sentences. English 
uses the letters of the alphabet inefficiently, but gains in robustness from that 
inefficiency. 
The theory of error-correcting codes deals with the general problem of 

4 
Error-correcting codes and finite fields 
transmitting messages reliably. The words 'transmitting' and 'message' are 
taken in the widest possible sense. The message can be a piece of music, a 
text, a picture, or simply a stream of ls and Os. Transmission includes storage 
to be read later, as well as speech, the telegraph, television, or satellite 
communications. 
Both of the natural error-correction methods illustrated in our story are 
emulated by artificial systems. The first is the model for image enhancement 
techniques, where statistical methods are used to improve received pictures. 
The second is the model for the subject of this book, classical coding theory. 
In the classical theory we make no assumptions about the nature of the 
message that is to be transmitted. We therefore have no statistic on which 
to base a guess of the correct message. Instead we must build redundancy 
into the message at the transmitter, much as written English does, in order 
that the receiver can use that redundancy to correct any errors that may 
have occurred on the way. Of course, this goal cannot be achieved with 
certainty, but high probability is possible. 
A weakness of classical coding is that errors that occurred before the 
transmitter 'encoded' and transmitted the message cannot be detected at all. 
On the other hand, the implementation of a modest correcting system is 
extremely simple and easy to understand, as you will see. 
1.1 The basic problem 
We can describe the situation we wish to model very roughly as follows. 
Information is sent via a channel which is prone to errors. The distorted 
information is processed at the receiving end to restore the original message 
as nearly as possible. 
The channel can take many forms. 
Examples 
• 
Radio communications of all kinds, television and satellite pictures. Here 
the channel is the combination of modulator that translates the informa­
tion into electrical signals, transmitter, receiver and demodulator. This 
book will not discuss modulation. For that you should consult a textbook 
on signal processing. 
• 
Computer file transfers such as the links between cash dispensers and the 
banks' central computers. In this example the main part of the channel is 
likely to be a cable. 
• 
Tape recorders, compact disks, floppy disks, textbooks. Now the channel 
is a storage medium together with the means of storing and retrieving 
information from it. Errors can occur either by a failure of the storage or 
reading device or by deterioration of the medium. 

Introduction 
5 
• 
Talking in a noisy pub. loudspeaker announcements at railway stations. 
Oral language also has built in error-processing capabilities, and our ears 
have a remarkable capability for selective hearing. 
In many of the examples the information is passed through the channel 
in separate lumps like the letters in a book, or the individual dots of a 
television picture. But it is also possible that it is sent in some continuously 
varying form like music on the radio, or speech. We shall discuss only the 
first type of channel which is called a discrete channel. 
We assume the message is composed of symbols or characters from a fixed 
finite set which we shall call the alphabet. In the case of English the alphabet 
contains not only the upper and lower case letters and numerals but also 
all the punctuation marks and the space character. All. in all, that gives an 
alphabet with about 80 symbols. All alphabets will be assumed to have a 
null 
character which will be denoted by 0. The null character in English is 
a space. Of course, an alphabet with only one character would be useless 
(why?). So the simplest alphabet is the binary alphabet consisting of two 
symbols 0 and 1, which we denote by B: 
B = {0, 1}. 
The elements of B are called bits (binary digits). 
In this chapter it will do no harm if you assume that the alphabet 
being used is B, but for later use the definitions will be given for general 
alphabets A. 
1.2 Three simple codes 
We shall now construct three very simple binary codes. These are not really 
of great practical use or sophistication, they just represent the kind of 
construction you might first think of, if you were trying to develop coding 
theory from scratch. 
Examples 
• 
Code A. The (8, 7) parity check code. 
Many computers use a sequence 
of eight bits, a byte, as a unit of information. For instance the ASCII code 
which is in almost universal use for microcomputers represents characters 
like 'a', 'B', and '3' by bytes. A byte can represent any value between 0 
and 255. As we have seen, English only needs about 80 characters. So, 
even allowing for 'control codes' representing internal instructions, seven 
bits ought to be enough. We can therefore use the eighth bit to check that 
the byte is being correctly transferred. We set the eighth bit of each byte 

6 
Error-correcting codes and finite fields 
so that the number of 1s in the byte is even. For example, the ASCII code 
for the digit 1 (seven bits in ascending order) is 
' I '..-. 1000110. 
We encode this as 
10001101. 
On the other hand, the ASCII code for the letter A is 
'A'..-. 1000001 
and we encode this as 
10000010. 
Now if a byte is transferred and one of the bits goes wrong, then the 
number of ls becomes odd. So the receiver can ask for a retransmission. 
There is no way the receiver can tell which bit went wrong, and if two 
bits are incorrect the receiver will let the byte through. Incidentally, in 
practice the order of the bits is reversed, so that the check bit comes first. 
We will discuss the performance of this code (and the other twQ 
examples) more mathematically in Chapter 2. But we can already make 
some observations. 
I. 
The code is very economical (the encoded message is ॰th longer than 
the original). 
2. It cannot correct errors. So it is only suitable where the receiver can 
ask for retransmission (because while errors can be detected, they 
cannot be located). 
3. 
The probability of errors during transmission should be fairly low 
(because the code cannot cope with two errors in a byte). 
• 
Code B. The triple repetition code. 
Now let us go to the other extreme. 
Imagine an ultra-conservative telegraph operator who wants to be quite 
sure that his transmissions get through properly. He decides to repeat 
each bit three times. 
0-+ 000, 
1-+111. 
Suppose the receiver gets a block 1 01 .  He can either say 'something's 
gone wrong,, let's ask Haggerty for a retransmit' or he can guess that it is 
more likely that the 0 is wrong than the two ls and correct to 1 1 1. That 
will be quicker but there is some risk because though it is unlikely, it is 
not impossible that the two ls went wrong. 
As above, we can give a rough assessment of the code's characteristics. 
I. 
The code is very uneconomical (the encoded message is three times 
as long as the original). 

Introduction 
7 
2. 
It can correct single errors in a block of three, or alternatively where 
retransmission is possible, it can be used to detect single or double 
errors in a block of three. 
3. 
For correction the error probability can be moderate, and for 
detection it can be quite high. 
Code C. The triple check code. 
Our last code is a first attempt at a 
practical code. We divide the message into blocks of three, say 'abc', 
where each of a, b, and c is 0 or 1, and add three check bits 'xyz', also 
each 0 or 1. The way we do this is such that three conditions are satisfied: 
1. The number of 1s in abx is even. 
2. The number of 1s in acy is even. 
3. 
The number of 1s in bcz is even. 
So if 
abc = 1 10, 
then 
X = 0, 
y = 1 ,  
and 
z = 1 .  
Thus the code word is 
1 1001 1 . 
Before continuing you should write down all the code words of this code 
(there are 8). 
The triple check code can not only detect but also correct single errors 
in a block of 6 because 
If a is incorrect conditions (1) 
and (2) will fail. 
If b is incorrect conditions (1) and (3) will fail. 
If c is incorrect conditions (2) and (3) will fail. 
If x is incorrect condition (1) alone will fail. 
If y is incorrect condition (2) alone will fail. 
If z is incorrect condition (3) alone will fail. 
So by examining the conditions, the receiver can find a single erroneous 
bit. Note that we must not assume that our check bits will be immune 
from error. 
If we only want to detect the presence of errors then the receiver can 
detect any two errors. For if two bits are in error there is a condition 

8 
Error-correcting codes and finite fields 
involving one but not the other . Hence not all conditions will be satisfied. 
It is, however, possible that a diiTerent single error would produce the 
same symptoms of incorrectness. For instance, if a and x are incorrect, 
then only condition (2) will fail, so if the receiver adopted a correction 
strategy he would make a mistake and 'correct' y. 
This code has the following properties. 
1. The code is moderately uneconomical (encoded message twice as 
long as original). 
2. 
For correction it can deal with one error in a block of six. For 
detection it can deal with two errors in a block of six. 
3. 
Since it deals with one or two errors in a block of six rather than a 
block of three it will not be quite as reliable as triple repetition. 
In fact, if it is used for pure error detection the code can do rather better 
than this discussion suggests (see Section 2.9). 
1.3 Channel models 
We now return to the ideas we are trying to model. The examples above are 
very primitive and we have not considered the way that the channel 
introduces errors. But the more accurately we try to emulate the behaviour 
of the real-life system we are modelling, the more complicated the model 
becomes. To begin with we choose the simplest model, the random error 
channel. That was the model implicitly used in our examples. 
Definition 
A channel is called a random error channel if for each pair of 
distinct symbols a, b of the alphabet there is a fixed probability Pa,b that 
when a is transmitted b is received. 
The main point of this definition is that Pa.h does not depend on anything 
else, such as whether the previous symbol was correctly transmitted or not. 
It is common practice to indicate this by the inelegant adjective 'memory less', 
but in this book the word 'random' alone will be used. 
The random channel may be a poor model. Imagine you are standing on 
the platform of a railway station. The loudspeaker starts 'Here is an 
important announcement . . .', and at that instant an express train comes 
through on the opposite platform, and the rest of the announcement is 
swamped. This channel is not random. The error affects a whole chunk of 
the message, and the message is lost rather than distorted. Here are two 
rather more practical examples. With storage media errors tend to affect 
several symbols at a time. The burst error channel, which will be discussed 
later in the book, is a more appropriate simple model for this type of 

Introduction 
9 
situation. If the previous symbol was in error, that will increase the 
probability of the current symbol being corrupted. Having parts of the 
message swamped by noise is common in radio transmission. This type of 
error is called an erasure. With erasures we know where things have gone 
wrong, but not what the correct symbol was. It is not very difficult to extend 
our theory to account for erasures as well, but for the moment let us stick 
to ordinary errors. 
For simplicity we shall also assume that Pa,b is independent of the symbols 
a and b (providing b ¥=a). 
Definition A random error channel is called symmetric if the probabilities 
Pa,b are the same for all possible choices of pairs a, b with a¥= b. 
For the rest of the book we shall assume that we are dealing with a discrete 
random symmetric channel unless we explicitly state otherwise. For the time 
being we shall restrict ourselves to the binary alphabet. So our initial topic 
is 'coding for the binary symmetric channel'. 
We shall use p for the probability of an error occurring in a single bit. 
We can assume that p < t. 
because if p > t 
the probability that the wrong 
bit is received is (1 - p) < t. 
So just by reversing every received bit we would 
change to a channel with p < t. 
If p = t, 
then the output of the channel is 
independent of the input and we might as well stop transmitting. 
Here is a picture of a channel. 
************** 
"' 
Random error • 
+ 
generator 
• 
Di .. ώ""' 
Ϗ""' .. .I'JJ 
Signal 
Distorted signal 
+-------ॷ----------ॸ-+ॹ--------ॺ----ॻ 
1.4 Encoders, error processors, and decoders 
The next stage of our model is to introduce the idea of an encoder. This 
takes the input signal, which we shall call the message, and modifies it in order 
to make it possible to detect, and perhaps also correct, any errors that the 
channel is likely to induce. At the other end we must have a decoder which 
retrieves the original message, but before we apply the decoder we need an 
prror processor. This attempts to correct or detect errors in the received 
message. According to circumstances it may modify the received message to 

10 
Error-correcting codes and finite fields 
enable the decoder to translate it (error correction), or send an error signal, 
in which case the decoder will ignor࡭ part of the incoming message (error 
detection). Often the decoder and error processor are lumped together and 
the error signal is called a decoding failure. But it is better to keep them 
separate in your mind even though in some implementations it is natural to 
combine them. 
The complete picture of our model now looks like this: 
Message 
wordx 
Received 
word u 
************** 
"' 
Random error 
• 
• 
generator 
• 
n) . .. ,. .. l 
.. ,. ... ) n 
Signal 
Distorted signal 
=>--------?®@--A----B----C· 
'---' Code word u 
Received word v 
r-=---९
-८C७od६eǕǔ- 1 
1 
Message 
Error 
word u' 
L 
Decoder _lf-----w- o-rd 
__ ..:,f',.-+ 
processor 
u + error signal 
1.5 Specific cases 
To make this more concrete I shall describe the encoders, error processors 
and decoders of the three example codes explicitly. We shall assume that the 
receiver adopts a correction strategy where possible. 
• 
Code A 
Encoder: Divide message into blocks of seven. To each block add an 
eighth bit to make the number of 1s even. 
Error processor: Count number of 1 s in received block. Error signal if the 
number is odd. 
Decoder: Strip the eighth bit. 
• 
Code B 
Encoder: Repeat each 
bit three times. 
Error processor: Take the majority vote in each block of three and make 
all three equal to that. 

Introduction 
Decoder: Strip the last two bits. 
• 
Code C 
11 
Encoder: Divide message into blocks of three. To each block of three 
calculate a further three bits satisfying conditions (1), (2) and (3). 
Output the amalgamated block of six. 
Error processor: Check conditions (1), (2) 
and (3). If none fail, word is 
correct. If one or two fail, correct the single bit involved in the failing 
conditions and not in the others. If all three fail, send error signal. 
Decoder: Strip last three bits off each block (unless error signal). 
The examples show that the decoder is often an almost trivial component. 
Example B illustrates that the separation of error processor and decoder 
may be somewhat artificial. 
1.6 Summary 
In this initial chapter the ideas of a communication channel and in 
particular the binary symmetric channel were introduced. Three simple 
example codes for the binary channel were defined and their error-processing 
capabilities were discussed. 
Exercises 1 
1.1 
A spelling checker is a kind of error processor for typewritten English. 
Consider what strategies a spelling checker should adopt. Why is the 
symmetric channel model, used for binary codes, not adequate for a 
spelling checker? 
1.2 Extend the definition of the (8, 7) parity check code to define an. 
(n + 1, n) parity check code, adding a parity check bit to every block 
of n 
message bits. What are the advantages and disadvantages of taking 
n 
large in this definition? 
1.3 Extend the definition of the triple repetition code to define an n-fold 
repetition code. What are the advantages and disadvantages of taking 
n 
large in this definition? 
1.4 There is one pattern of incorrect equations that the triple check code 
does not exploit to correct an error. Try to modify the definition of the 
triple check code to produce a code which adds three check bits to 
every block of four message bits and can still correct any single error. 
1.5 Show that it is not possible to devise a code adding three check bits to 
every block of five message bits in such a way that the code' can correct 
every single error. 

12 
Error-correcting codes and finite fields 
1.6 The standard ASCII code used to represent printable and non-printable 
characters in computers contains 128 7-bit symbols: for instance (in the 
usual descending order) '0' is 0110000, a space ' ' is 0100000, 'A' is 
1000001, and 'a' is 1 100001. Devise a single error-correcting code for 
transmitting ASCII, using as few check bits as possible. Give encoding 
and error-correcting rules. 

2 
Block codes, weight, and distance 
All the examples of Chapter 1 divide the message into blocks before they 
process it. We shall concentrate on such codes, as they form the best vehicle 
for introducing the ideas of coding theory. For simplicity the channel will 
be taken to be the binary symmetric channel. Having found some examples, 
we need a method to assess their performance over a given channel. The key 
concept that forms the basis for the assessment is the Hamming distance, 
which is just the number of places in which two words differ. We shall show 
that the worst case error,processing performance of a code is completely 
determined by the minimum distance between code words. 
Then some elementary probability theory can be used to assess the 
performance of a code. This will be illustrated by calculating the success 
probabilities of our examples in transmitting a message of 10 000 bits over 
a channel with a bit error probability of 0.1 per cent. 
Finally we shall discuss Shannon's theorem, which represents the remark, 
able theoretical optimum for average coding performance. 
2.1 Block codes 
Taking the sample codes of Chapter 1 as our model, we adopt the convention 
that our encoders will divide the message into words or blocks, sequences of 
symbols of a fixed length m, the encoder translates each word into a code 
word of a fixed length n. Such codes are called block codes. 
Definition If A is an alphabet an A-word or A-block oflength n is a sequence 
of n symbols from A. The set of A-words of length n is denoted by An. 
If A has q 
symbols, then there are q 
choices for the symbol in each place 
in an A-word of length n. So the total number of such words is qn. 
Using 
the conventional notation IAI for the number of members of the set A, this 
can be expressed by the suggestive equation IAnl = lAin. 
Having defined blocks we can now define block codes formally. Notice 
how the following definition copies the way our example codes were defined. 
Definition An (n, m)-block code C over the alphabet A of size q 
consists of 
a set of precisely qm code words in An. 
An encoder E for Cis a map from Am to C. It translates any A-word x of 

14 
Error-correcting codes and finite fields 
length m into a code word u = E(x). Every message word must correspond 
to a unique code and every code word must represent a unique message 
word. In technical terms, the encoder must be bijective. 
The corresponding decoder D is the inverse map of E. It takes every code 
word u = E(x) back to x. 
The number n is called the block length of the code. 
We shall call the number m the rank of the code. 
The fraction m/n is called the rate of the code. 
For our sample codes the data are as follows. 
Examples 
Numbers: 
Code 
CodeB 
Code C 
Name 
Parity check 
Triple repetition 
Triple check 
Block length 
8 
3 
6 
Rank 
7 
1 
3 
Rate 
1· 
8• 
l. 
3• 
l 
2• 
The reason we require precisely qm 
code words is to ensure that an encoder 
exists. There are qm 
possible message words and each must correspond to a 
code word. These code words must be distinct. Any further code words are 
not used and may as well be discarded. In particular a binary code of rank 
m must have 2m 
code words. Very often the encoder will preserve the message 
word x as the first part of the code word E(x). For instance, all the encoders 
for the sample codes behave like that. Such an encoder is called standard or 
systematic. In that case the code word is divided into message symbols and 
check symbols and the decoder merely strips the check symbols. Of course 
m ř n, because the code lies in A". So the rate is always y 1. 
With a binary symmetric channel the error-processing capabilities of the 
coding system do not depend on the encoder and decoder, but only on the 
set of code words, because these are all that the channel sees. The choice of 
encoder and decoder is thus only a matter of practical convenience. Most 
of coding theory is concerned with the construction of codes C and efficient 
error processors. 
2.2 Weight and distance 
When errors occur in transmission the receiver reads a word v although the 
transmitter sent a word u. 
Definition If u = (u1, •
•
•
 , un) and v = (v1, •
•
• , vn) are words in A", we shall 
refer to u i as 
the entry of 
u in place j and 
we shall say 
v difers from u 
in place 
i if 11, =I= v ,. The words position and location are synonyms for place. 

Block codes, weight, and distance 
15 
In this context it is usual to call the word v that will be analysed by the 
error processor the received word. If the received word v 
differs from the 
transmitted one in k places we say an error of weight k occurred or more 
loosely that k errors occured. 
Examples 
Suppose u = 
(1, 0, 0, 1, 1, 0) is transmitted and v = (1, 1, 0, 1, 0, 0) 
is received. Then an error of weight 2 has occurred (or in loose parlance two 
errors occurred). 
It is useful to formalize this idea by regarding the number of places in 
which two words differ as a distance between them. 
Definition 
The Hamming distance d(u, v) between two words u and v is the 
number of entries in which they differ. The Hamming weight wt(u) of u is 
the number of non-null entries in u. 
If Q 
is the word (0, . . .  , 0) then wt(u) = d(u, Q). 
The term 'distance' in the name Hamming distance is quite appropriate. 
There are certain formal properties that a distance function must satisfy in 
order that it behaves in the way we expect. These are listed below, and 
followed by the straightforward verification that they are satisfied by the 
Hamming distance. 
Definition Distance axioms. A function f(x, y) on pairs of elements of a 
set S is a distance function if it satisfies the following conditions. 
1. f(x, y) is always a non-negative real number. 
2. f(x, y) = 0 if and only if x = y. 
3. f(x, y) = f(y, x). 
4. 
For any three elements x, y, z of S, 
f(x, z) 
y f(x, y) + f(y, z). 
Condition (4) s called the triangle inequality, because if x, y, and z are 
thought of as the corners of a triangle it states that the length of any side 
of a triangle is at most the sum of the lengths of the other two sides. 
9 
X 
Z 
Proposition The Hamming distance is a distance function. 

16 
Error-correcting codes and finite fields 
Proof By its definition the Hamming distance satisfies (1)-(3). To see that 
(4) holds, let x = (x1, •
•
• , xn), y = (y1, •
•
• , y.) and z 
= (z1, 
•
•
. , z.). 
Then 
d(x, z) is the number of places in which x and z differ. If we denote the set 
of these places by U, then 
d(x, z) 
= lVI = l{ilx; i= z;}l . 
Let S = {ilx; i= z; and X; = yJ and T = {ilx; i= z; and X; i= yJ. Then U is 
the disjoint union of S and T. Hence 
d(x, z) 
= lSI + ITI. 
It is immediate from the definition of d(x, y) that x differs from y in all the 
places in T. Thus 
ITI ř d(x, y) 
On the other hand if i E S, then Y; =X; i= z;. So 
lSI y d(y, z), 
and (4) follows. 
• 
2.3 Error processing 
An error processor P for C could just test each word v it receives to see if 
it is a code word or not and send an error signal for a non-code word, or 
it could attempt to correct certain non-code words. To give a flexible formal 
definition, we assume that when it receives a word v the processor puts out 
a word u and a signal ('good' or' bad'). The signal says whether the processor 
is putting out a code word or not. 
Definition 
An error processor P for C 
is a map that accepts a word v 
of 
length n 
(called the received word) and produces a pair (x, u) where x takes 
on two values ('good' or 'bad') and u is a word of length n. The signal x 
has the value 'good' when u is a code word and 'bad' otherwise. An error 
processor that always leaves the received word unchanged is called an error 
detector and we shall call an error processor that always produces a code 
word perfect. 
Any error processor must start by testing the received word v to see if it 
is a code word or not. If v 
is a code word then the error processor's job is 
over. It has no means of knowing what the message was and so it can do 
no better than accept the word it received as correct. It will transmit it 
unchanged with a 'good' signal. 

Block codes, weight, and distance 
17 
This means that error patterns that distort one code word into another 
code word are undetectable and hence uncorrectable. Suppose now that we 
want to be able to detect all errors of weight at most s. It is clearly necessary 
that any two distinct code words are at Hamming distance s + 1. That makes 
it natural to introduce a word for the smallest possible distance between 
distinct code words. 
Definition 
Let C be an (n, m)-code. The minimum distance d(C) of C is the 
smallest Hamming distance between distinct code words of C. This measure 
is so important that we sometimes call attention to it by describing C as an 
(n,m, d)-code. 
Examples 
• 
The parity check code A has minimum distance 2, because it consists of 
all words of even weight. 
• 
The triple repetition code B has minimum distance 3, because it contains 
only the two words (0, 0, 0) and (1, 1, 1). 
• 
The triple check code C also has minimum distance 3. You can see this 
by writing down all eight code words. 
2.4 Error detection: a necessary and sufficient condition 
The condition for error detection we derived above is not only necessary, it 
is also sufficient. 
v 
Proposition Let C be a code. Then it is possible for an error processor for C 
to detect all errors of weight ::;; s if and only if d( C)  s + 1. 
Proof If two code words u and v are at distance at most s ,  then one 
can be distorted into the other by an error of weight at most s. In that case 
no error processor for C can detect all errors of weight at most s. Conversely 
if any two code words are at distance at least s + 1, then any error of weight 
s will distort a code word into a non-code word. An error detector can check 

1 8  
Error-correcting codes and finite fields 
whether a received word is a code word or not (for instance by looking it 
up in a table). Hence all errors of weight at most s are detectable. 
• 
2.5 Error correction: a necessary and sufficient condition 
There is a similar condition for error correction. Suppose we wish to be able 
to correct all single errors. Then given a received word and the information 
that an error of weight one occurred, there must be only one code word that 
could have been transmitted. In other words no two code words u and v 
may be at distance at most 1 from same word w. Thus (by the triangle 
inequality) the code must have minimum distance at least 3. 
A similar argument works for larger errors. If we are to be able to correct 
all errors of weight up to t, then given a word and the information that an 
error of weight at most t occurred, there must be a unique code word that 
could have been transmitted. So no two code words may be at distance at 
most t from the same word w. Hence the code has minimum distance at 
least 2t + 1. 
Again this condition is also sufficient. 
!I 
v 
Proposition There exists an error processor for the code C that corrects all 
errors of weight up to t if and only if C has minimum distance 2t + 1. 
Proof Suppose the code contains two code words u and v at distance at 
most 2t. Let w be a word that agrees with u at all places that u and v 
agree. 
Further let w agree with u at the first t places where u and v disagree and 
with v 
in the remaining places where u and v disagree (if d(u, v) 
< t take 
w = u). Then d(u, w) ::; t and d(v, w) ::; t. Now suppose w is received together 
with the information that at most t errors occurred. Then either u or v could 
have been transmitted (and possibly even some other code word). There is 
no way that from the given information an error processor can decide with 
certainty which code word was transmitted. So it will fail to correct some 
errors of weight ::; t. 
_.C'.':'t"''४::५f'''' 
r••:nMP th ::1 t  thP: ၞ()rlp h!lၟ minimum distance 2t + 1 and a 

Block codes, . weight, and distance 
19 
word w is received, together with the information that an error of weight at 
most t has occurred. If there were two code words u and v at distance at 
most t from w, then by the triangle inequality d(u, v) :::; 2t, contradicting our 
hypothesis. Hence there is a unique code word u at distance at most t from 
w and we can deduce that u must have been transmitted. 
• 
2.6 Mixed strategies 
We have seen that the minimum distance d(C) completely determines the 
worst-case error-detecting and error-correcting capabilities of a code. Often, 
however, we do not want just to detect all errors of a certain weight, and 
correcting all errors that lie within the theoretical capabilities of the code 
may be too time consuming or too expensive. It is possible to have a mixed 
strategy: we correct errors of weight up to some (usually small) value t and 
still detect errors of weight up to t + s. 
The main theorem of this chapter, which generalizes Propositions 2.4 at?.d 
2.5 gives precise bounds for s and t in terms of d(C). 
Theorem A code C can correct all errors of weight up to t and at the same 
time detect all errors of weight up to s + t if and only if d( C)  2t + s + 1. 
Informally this says that error correction costs about twice as much as 
error detection - for every error bit you attempt to correct you lose two bits 
in the number of errors you can detect. That is because if you are employing 
error correction an error that pushes a code word u close enough to another 
code word v will cause the error processor to choose v rather than content 
itself with the statement that the received word is erroneous. 
Example The theorem completely describes the error-correcting and 
-
detecting capabilities of our sample codes. 

20 
Error-correcting codes and finite fields 
• 
Code A. The parity check code has minimum distance 2. It can detect 
single errors but it cannot correct them (s = 1, t = 0). 
• 
Codes B and C. 
Both the triple check code and the triple repetition code 
have minimum distance 3. They can detect double errors (s = 2, t = 0) or 
correct single errors (s = 0, t = 2.) 
Example 
Suppose we have a code C with block length 64 and minimum 
distance 10. 
Then we have the following possibilities for error processing: 
1. Detect errors of weight up to 9. 
2. Correct errors of weight up to 4. 
The code only needs minimum distance 9 for that. So this scheme is a 
bit wasteful. 
3. 
Correct errors of weight 1, detect errors of weight up to 8. Schemes like 
this are quite common in practice (e.g. in compact disc players), because 
weight 1 error correctors are fast and simple to implement. 
4. 
Correct errors of weight ř 2, detect errors of weight y 7. 
5. 
Correct errors of weight ř 3, detect errors of weight ၜ 6. 
6. 
Correct errors of weight + 4, 
detect errors of weight + 5. 
These last three possibilities use the minimum distance to the full. 
Compare (6) with (2). 
Of course, a coding scheme is not obliged to use the full capability of the 
code. Practical considerations may make it necessary to limit the operation 
of the error processor. Still, it is not worth listing all the schemes that are 
weaker than ( 1 )-( 6). 
Proof Consider an error processor for the given code C that works as 
follows: if v is received and there is a unique code word u with d(u, v) + 
t, 
correct v to that code word. Otherwise send an error signal. 
Assume d(C) > 2t + s. Then by Proposition 2.5 our decoder will correct 
errors of weight y t successfully. Suppose that u is transmitted and w is 
received, where t < d(u, w)  t + s. Then by the triangle inequality, a code 
word v with d(v, w) + 
t would have s y d(u, v) y 2t + s. Hence there is no 
such code word and the error processor will send an error signal. 
Conversely, suppose that d(C) y 2t + s. If d(C) + 
2t we know from 
Proposition 2.5 that C cannot correct all errors of weight + 
t. So we may 
assume that d(C) > 2t. Then by the triangle inequality there is never more 
than one code word at distance ř t from any received word w. Let u and v 
be two code words with 2t < d(u, v) ř 2t + s. Divide the places in which u 
and v 
differ into two sets S and T with S y t and t < T ၝ s + t. Now let w 
be defined so that w agrees with u and v outside S u T, with u in S and with 
w in T. Then t < d(u, w) < t + s and d(v, w) ř t. Thus if u is transmitted and 
v is received an error of weight 1 s + t 
has occurred. Yet an error processor 

Block codes, weight, and distance 
21 
that corrects all errors of weight  t will not send an error signal. Instead 
if will 'correct' w to v, because as we noted above, v is the only code word 
with d(v, w)  t. Thus the code cannot successfully detect all errors of weight 
up to s + t. 
• 
2. 7 Probability of errors 
The minimum distance is a worst-case measure for the performance of a 
code, but it would be nice to know how our codes (and other, better codes) 
could be expected to perform on average. To do this we shall need a little 
probability theory. The discussion will not be needed in the sequel and can 
be omitted at the reader's discretion. By sticking to the random channel 
model we can make the probability theory required quite simple. All that we 
require are some counting arguments. 
Proposition A block code of block length n is used to transmit a word u over 
a binary symmetric channel with error probability p. 
The probability of a particular error of weight k occurring in the received 
word is pk(l - p)n-k. 
The probability of some error of weight k occurring is 
(½)l(l - p)n-k, 
(1) 
where(:) is the binomial coefficient 'n choose k'. 
Proof For a particular error word of weight k to occur we specify k places 
in which the received word contains the wrong symbol. Then the other n - k 
places contain the correct symbol. The probability of a wrong symbol in a 
particular place is p, and the probability of a correct symbol is 1 - p. For 
a random channel all these probabilities are independent. Hence the 
probability that they all occur is their product pk(1 - p)n-k. 
The total number of error words of weight k is given by the possible ways 
of choosing the k places in error. That is precisely (½). The possible errors 
are mutually exclusive. So the probability that one of them occurs is the sum 
of their individual probabilities. Hence formula (1) holds. 
• 
2.8 Probability of correct transmission 
A similar argument applies to correctable and detectable errors. 
Proposition A block code is used to transmit a message over a binary 
symmetric channel. 

22 
Error-correcting codes and finite fields 
(a) The probability that an error processor produces a correct word is the 
sum of the probabilities of the error patterns that the error processor can 
correct. 
(b) If the error patterns that an error processor can correct are independent 
of the transmitted code word, then the probability that a complete message 
is transmitted correctly is the probability that the error processor produces 
a correct word taken to the power I, where I is the number of code words 
required to transmit the message. 
Remarks The simplifying assumption in (b) is close to the truth for 
most practical applications, but is by no means a theoretical necessity 
(see Exercise 2.5). In Exercise 2.3 you will be asked to formulate the 
corresponding result for error detection. 
Proof (a) The error words that the error processor can correct are mutually 
exclusive. Hence the probability that one of them occurs is the sum of 
their individual probabilities. 
(b) The probabilities of correctable errors in each of the code words 
transmitted are independent; hence the probability that all received words 
have correctable errors is the product of the probabilities of correctable 
errors in each word. As these are all identical the result follows. 
• 
2.9 Examples 
We can now apply Propositions 2.7 and 2.8 to our example codes. 
We assume we have to transmit a message of 10 000 bits along a 
channel with error probability p = 1/1000. 
With no coding the probability of successful transmission is 
0.99910 000 ț 0.000 045. 
Now let us see how our codes perform. 
• 
Code A. The (8, 7) 
parity check code. To each message block of 8 bits 
add a parity check so that the number of 1s is even. 
Rate: i. 
This can detect up to one error in each transmitted word of length 1 1. 
It cannot correct any errors. 
Probability no error in word: (0.999)8 
Probability 1 error in word: (0.999)7 x 8/1000 
ܑ0.992028. 
ț0.007944. 
0.999972. 
Probability of correct transmission: (0.992028)1000017 
ț0.00001 1. 
Probability of no undetected error: (0.999972)1000017 
ț 0.961. 
This code gives moderate protection against undetected errors. 

Block codes, weight, and distance 
23 
• 
Code B. The triple repetition code 1 ό 1 1 1, 0 4 000. 
Rate: t. 
1. Error detecting. The code can detect two errors in a block of three. So 
the only undetectable error pattern is 1 1 1  which has probability 10-9• 
Probabilty of correct transmission: (0.999)30000 
ǎ 10- 13• 
Probability of no undetected error: (1 - 10-9)10000 
ǎ0.99999. 
In this mode the code gives excellent protection against undetected 
errors, but the extremely low probability of correct transmission 
indicates that a lot of retransmission will be required and the low rate 
already makes the code wasteful. 
2. Error correcting. One error in a block is a thousand times more likely 
than two. So we use majority logic error processing. 
Probability no error in block: (0.999)3 
Probability 1 error ii block: 
(0.999)2 X 
3/1000 
Hence probability of correct transmission: 
Ȝ0.997003. 
Ȝ0.002994. 
ǎ0.999997. 
(0.999997)10000 
Ȝ0.97. 
This is also the probability of no undetected error, because two 
errors in a block cause incorrect decoding. 
In this mode the code produces a pretty good likelihood of correct 
transmission. However incorrect words will not be picked up and will 
be present in about 3 per cent of such messages transmitted. Again, 
the low rate makes the code quite expensive to use. 
• 
Code C. The triple check code. Divide message into blocks of three 
(a, b, c). Encode as (a, b, c, x, y, z) 
wtih x = a 
+ b, y = a 
+ c, z = b + c. 
Rate: !. 
1. Error detecting. The only undetectable error patterns are those which 
affect precisely 2 or 0 bits in each condition. By trial and error (or by 
using the coset table of Chapter 4) we find that these are: 
(000000) no errors, and 
(101 10), (010101), (00101 1), (1 1 1000), (01 1 1 10), (101 101) and (1 10011). 
The probability (a) of any particular error pattern of weight 3 is 
(0.999)3(0.001)3 
ǎ9.97 x 10- 10• 
The probability (b) of any particular error pattern of weight 4 is 
(0.999)2{0.001)4 
Ȝ 9.98 x w- 13. 
Probability (c) of no undetected error in a word: 1 - 4a - 3b 
ǎ 0.999999996. 
Probability of no undetected error in message: c1000017 ύ 0.999987. 
In this mode the code comes close to the performance of the triple 
repetition code at a considerable saving in expense. 

24 
Error-correcting codes and finite fields 
2. Error correctiny. We can correct one error in a block of six. Over 
and above this we can simultaneously also detect the three error 
patterns (1, 0, 0, 0, 0, 1 ), (0, I ,  0, 0, I ,  0) and (0, 0, I, I, 0, 0) as these 
cause all three conditions to fail. 
Probability no error in block: (0.999)6 
Probability 1 error in block: 6(0.999)5/1000 
Z 0.9940 1 5. 
Z 0.005970. 
Z 0.999985. 
Probability of 3 error patterns above: 3(0.999)4/ 106 
[ 0.000003. 
Probability of correct transmission: (0.999985) 1 0 00013 ϋ 0.95 1 .  
Probability of no undetected error: (0.999987) 1 0 00013 Z 0.957. 
In this mode there will be uncorrected errors in about 5 per 
cent of the messages we transmit. The degradation of performance 
compared with the triple repetition code is more significant here, but 
may still be worth the saving in expense. Even if we add the facility 
to send an error signal if all three conditions fail, that will improve 
performance only very slightly. 
2.10 Shannon's theorem 
The calculations we have just made show that the average performance of 
our codes is not very good. So we are Jed naturally to ask the question: is 
it possible to do significantly better? The answer to this is an emphatic yes. 
It was given in Claude Shannon's channel coding theorem of 1 948, proved 
before any practical error-correcting codes were known. 
Shannon showed that there is a constant called the channel capacity C(p) 
for any discrete symmetric channel, such that there exist block codes of rate 
less than but arbitrarily close to C( p) with probability of correct transmission 
arbitrarily close to 1 . 
The formula for C( p) for a binary channel is 
C( p) = 1 + p log2 p + (1 - p) logz(l - p) . 
If p = 0.5, then C(0.5) = 0. This illustrates the fact that no coding scheme 
works for a channel with error probability 0.5. The channel of our example 
has p = 0.999. Here C(0.999) = 0.9886. So Shannon's theorem says that there 
are codes adding only about 15 check bits per 1 000 message bits that achieve 
arbitrarily high probability of correct transmission for our message. 
Clearly there is a lot of scope for improvement in our codes. Shannon's 
theorem is not proved in this book, but it is compared to other bounds on 
codes in Chapter 1 8. 

Block codes, weight, and distance 
25 
2.1 1  Summary 
The key concept of this chapter has been the Hamming distance. In 
Propositions 2.4 and 2.5 and Theorem 2.6 it was used to describe the 
worst-case error-processing capabilities of a block code. Then Propositions 
2.7 and 2.8 used it to calculate average error-processing performance for a 
code over a binary symmetric channel, assuming that the way the error 
processor treats an error is independent of the transmitted code word. 
2.12 Exercises 
2. 1 
Let C be the binary (n + 1, n)-parity check code, defined by adding a 
single check bit to every block of n message bits. Show that as n 
increases, the rate of the code tends to 1, but for a channel with fixed 
error probability, the probability of correct transmission of a code word 
tends to 0 as n grows large. 
2.2 Let C be the (n, I)-repetition code, defined by repeating each message 
bit n times. Show that as n increases the rate of this code tends to 0, 
but that for a channel with fixed error probability and a message of 
fixed length, the probability of detecting all errors tends to 1. Show that 
the same conclusion holds for correction by majority vote. 
2.3 
Formulate and prove the analogue of Proposition 2.8 for error 
detection. 
2.4 Define a code by adding an overall parity check p to the triple check 
code. So abc is encoded as abcxyzp with 
a + b + x = a + c + y = b + c + z  
= a + b + c + x + y + z + p = 0 .  
What are the parameters of this code, including minimum distance? 
2.5 Construct a binary code C containing (among others) two w-ords u and 
v such that if u is transmitted any error ofweight at most 2 can be 
corrected, but if v is transmitted there is an error of weight 1 
that cannot 
be detected. Show that Proposition 2.8(b) does not apply to this code. 
2.6 A binary code C has block length 15 
and rank 5. 
It is capable of 
correcting three random errors in a block and no more. If we define a 
second code R by taking a message block of five bits abcde to the code 
word abcdeabcdeabcde, then R has the same block length and dimension 
as C. The code R cannot correct all triple errors, but it can correct 
some error patterns of weight 5. Give the block error probabilities for 
the two codes and compare their performance for a message of 5000 
bits on a channel with error probability O.ol. 

26 
Error-correcting codes and finite fields 
2.7 A binary code C of block length 12 has minimum distance equal to 3. 
Show that some code words must have odd weight, and some code 
words have even weight. The code is extended to a code K of block 
length 13 by adding an overall parity check bit to each word; that is, 
a bit is appended to each code word so that the total number of 1s in 
the resulting word is even. What is the minimum distance of K? Show 
that C can correct all single errors in received words, while K can 
correct all single errors and simultaneously detect the presence of all 
double errors in received words. 
2.8 Two channels are available for transmission of a long message. The 
first is a binary channel with error probability p = 0.01; the second is 
a symmetric channel with a alphabet A of size 16 and error probability 
q = 0.04. It is possible to send a binary message along the second 
channel by sending blocks of four bits as a single symbol of the alphabet 
A according to some arbitrary translation scheme. Compare the error 
probabilities of a message of 5000 bits encoded 
using the triple check 
code on both channels. 
2.9 A double error-correcting binary code of block length 20 and rank 12 
is used to transmit a message of 18 00 bits through a binary symmetric 
channel with error probability 1 in 250. On a second similar transmission 
the error probability of the channel doubles. What is the probability that 
the message is received and decoded correctly in each transmission? 
2.10 Let E be a binary code of block length 8. Show that if E can correct 
all single errors, then it has at most 28 code words. 
2.1 1  Show that for the triple check code the undetectable error patterns are 
precisely the non-zero code words. 
2.12 It is possible that in transmission the value of a symbol is completely 
lost, so that the receiver recognizes that an error has occurred, but has 
no information about the transmitted symbol. Such errors, which are 
called erasures, can occur in radio transmission or through faulty 
magnetic media. Show that a code can correct t errors and simul­
taneously u erasures if and only if it has minimum distance > 2t + u. 
Formulate and prove a generalization of Theorem 2.6 dealing with 
simultaneous error correction, detection, and erasure correction. 
2.13 A binary code C is extended by adding a further bit so that all code 
words are even. Give the block length, rank, and minimum distance of 
the extended code. What happens if you extend a code twice? 
2.14 A code C is punctured at place i, by deleting the ith entry of all code 
words. Describe the effect of puncturing a code on its rank and its 
minimum distance. 
2.15 A code C is shprtened at place i, by taking only those code words of 
the punctured code that are obtained from words with a zero in the 
ith place. Show that shortening a code does not reduce its minimum 
distance. Does it always reduce its rank? 

3 
Linear codes 
In this chapter we begin by taking another look at our example codes from 
Chapter 1 in the light of a natural arithmetic on B. That will lead us to 
define a special class of codes called linear codes (some authors call these 
group codes). Linear codes are amenable to the standard techniques of linear 
algebra and that makes it possible to devise efficient implementations for 
them. It is fair to say that although some theoretically good non-linear block 
codes are known, virtually all block codes used in practice are linear. 
3.1 Arithmetic in B 
Let us take another look at two of the examples of Chapter 1. 
• 
Code A, the (8, 7) parity check code, is defined so that its words all have 
even weight. 
• 
Code C, the (6, 3) triple check code, is defined so that certain subsets of 
the entries of a word contain an even number of ls. 
If we introduce the convention that in B 
and 
0 + 
1 = 1 + 
0 = 1 ,  
then the fact that a set ·s of symbols of B contains an even number of 1 s  can 
be expressed by stating that the sum of the elements of S is 0. This addition 
is basic to the binary logic chips from which computers are built and is called 
exclusive or in computer science. For our purposes, though, arithmetic is 
more useful than symbolic logic. 
The definition of multiplication for B is even simpler than that for addition: 
O x 1 = 1 x 0 = 0 x 0 = 0  
1 x 1 = 1 . 
Definition 
The binary field is the set B = {0, 1} endowed with the operations 
of addition and multiplication defined above. 
The word 'field' in this definition has a precise technical meaning, that 
will be explained shortly. For the time being it will be sufficient to say that 

28 
Error-correcting codes and finite fields 
all four arithmetic operations: plus, minus, times and divide are possible and 
that they obey the standard rules of arithmetic like the ones for ordinary 
numbers. The binary field has one particular peculiarity, namely that plus 
and minus are the same. That is because the sum of two natural numbers 
has the same parity as their difference. 
Examples We can use the definition to give more concise descriptions of 
the parity and triple check codes. 
• 
Code A. The (8, 7) parity check code. Encode a block (x1, .
. . , x7) by 
(x1, •
•
.
 , x7, y), where 
y = xl + · · · + x7 . 
• 
Code C. The (6, 3) triple check code. Encode a block (a, b, c) by 
(a, b, c, x, y, z), where 
3.2 Arithmetic in Bn 
x = a + b; 
y = a +  c; 
z = b + c .  
Given binary words of the same length, say (a, b, c) and (a', b', c') it is now 
natural to define their sum as (a + a', b + b', c + c'). 
Examples Given two words (1, 0, 0, 1, I, 0) and (0, 1, 0, 1, 0, 1), their sum is 
calculated by: 
0 0 
0 
+ 
0 1 0 
0 
0 0 
1 .  
We will frequently omit the commas and brackets and write the sum above 
as 1001 10 + 010101 = 1 1001 1. It is important to remember when carrying 
out the addition, that it is binary addition without carry. 
This addition looks exactly like the way real vectors are added in ordinary 
linear algebra. That is no accident. It turns out that the standard theorems 
of linear algebra apply to the set An of words of length n over any alphabet 
A provided that A is a field. The words play the role of vectors and the 
symbols in A play the role of scalars. For general alphabets A, it is also 
necessary to define multiplication of vectors by scalars, but for B that is 

Linear codes 
29 
trivial: O.x = Q 
and l.x = x. So we can say the Bn is a vector space over B. 
Our three example codes all respect the vector space structure in the 
following strong sense: If x and y are message words that are encoded as u 
and v, then the message word x + y is encoded as u + v. 
Examples 
• 
Code A. The parity check code. 
Let x = 0101010 and y = 001 101 1. Then 
x is encoded as u = 01010101 and y is encoded as v = 001 101 10. The sum 
x + y = 01 10001 is encoded as 01 10001 1 = u + v. 
• 
Code B. The triple repetition code. 
Let x = y = 1. Then x and y are 
encoded as u = v = 1 1 1. The sum x + y = 0 is encoded as 000 = u + v. 
• 
Code C. The triple check code. Let x = 100 and y = 010. Then x is 
encoded as u = 1001 10 and y is encoded as v = 010101. The sum 
x + y = 1 10 is encoded as 1 1001 1  = u + v. 
It is not an accident that the codes behave like this. If you define the code 
word u encoding a message word x by linear conditions, then it will always 
happen. The arithmetic on B was defined so that the conditions that 'come 
naturally' are linear. So we can expect codes defined in a 'natural' way to 
have the same additive property as our examples. Such codes are called 
linear. 
Definition A binary code is called linear if the sum of two code words is a 
code word. 
3.3 Fields: a definition 
It is now time to give the proper definition of a field. The definition merely 
describes the natural properties of the arithmetic operations that we wish to 
execute. But it is important that all these properties are true, because 
otherwise it will not be possible to use linear algebra. The way to read these 
axioms is to check that they agree with what one means by the 'usual rules 
of arithmetic'. 
Field axioms 
A. Laws of addition 
Al. (a + b) + c = a +  (b + c). 
[Associative law] 
A2. 
There exists 0, such that for all a, 0 + a = a + 0 = a. 
[Zero] 
A3. a +  b = b + a. 
[Commutative law] 
A4. 
For all a, there exists - a  s.t. a +  (-a) = 0. 
[Negatives] 

30 
Error-correcting codes and finite fields 
B. Mixed laws 
Dl. a(b + c) = ab + ac. 
D2. (a + b)c = ac + be. 
C. Laws of multiplication 
[Distributive Jaws] 
MI. (ab)c = a(bc). 
[Associative law] 
M2. There exists 1 =1= 0, such that for all a, Ia = al = a. 
[Identity] 
M3. ab = ba. 
[Commutative law] 
M4. For all a =1= 0, there exists a- 1 such that aa- 1 = 1. 
[Inverses] 
Axiom A4 ensures that subtraction is possible, and M4 that division by 
non-zero field elements is possible. 
Examples 
1. The real numbers form a field with the usual operations,. 
2. The integers (whole numbers) do not form a field, beca'use M4 is not 
satisfied. The number 2 has r 1 = -!- which is not a whole number. But 
the integers do satisfy all the other axioms and a weaker axiom than 
M4, called the cancellation law: 
MS. If ab = 0, then a = 0 or b = 0. 
3. B is a field. 
4. The ternary field Z/3 with three elements I, 0 and - I  satisfying the 
following addition and multiplication is a field. 
Addition: 
0 + I = I + 0 = I ,  
0 + 0 = 0, 
0 + - 1 = - 1 + 0 = - 1; 
1 + 1 = - 1, 
1 + - 1  = - 1  + 1 = 0, 
- 1 +
- 1 = 1 . 
Multiplication: 0 x I = I x 0 = 0 x 0 = 0 x - I  = - 1  x 0 = 0; 
1 X 1 
= - 1  X - 1  = l, l X - I  = - I  X I = - 1 . 
It is left to the reader to check the axioms. The check is rather tedious, 
and in Chapter 9 a general theorem will be proved from which the fact 
that Z/3 is a field follows directly. 
Definition An arithmetic structure S with operations + and x satisfying 
axioms Al-A4, D 1-D2, and Ml-M4 is called a field. If all axioms except 
M4 are satisfied, then S is called a (commutative) ring. If S is a commutative 
ring that also satisfies the cancellation law M5 it is called an (integral) 
domain. 
Examples 
• 
B, the set of real numbers and the set of complex numbers all form fields. 
Of these only B is finite. 

Linear codes 
31 
• The integers Z and also the set of polynomials with coefficients in a field 
form domains. 
• The set of real 2 x 2 diagonal matrices is a (commutative) ring that is not 
a domain. 
3.4 Vector space operations 
If we require the alphabet A to be a field, then there are two natural 
arithmetic:; operations on the words in An. If u = (u1, • • •  , un) and v = 
(v1, •
.
.
 , vn) 
are words, we define u + v = (u1 + v 1 ,  •
•
•
 , un + vn) and for a E A, 
au = (au1, •
•
•
 , aun). Equipped with these operations An becomes a vector 
space over A. The operations satisfy analogues of the axioms Al-A4, Ml, 
M2, and D. 
As shown in Appendix LA, the main theorems of linear algebra hold for 
vector spaces over any field, and they offer powerful computational and 
theoretical tools that are indispensable for coding theory. It is not possible 
to exploit these tools without knowing them. Ideally, you should be familiar 
with ordinary matrix theory and in particular the rank and nullity theorem. 
If you know real linear algebra, but not linear algebra over general fields, a 
good procedure is to continue to read the main text, taking the fact that the 
methods extend to finite fields on faith, and consulting Appendix LA where 
everything is proved when you are uncertain. If you know matrix algebra 
but do not know any theory you will still be able to work through the 
examples, but you will probably find the proofs difficult to follow. Appendix 
LA covers everything needed, but it is rather terse and you may wish to 
consult one of the textbooks (Cohn, 1982; Noble and Daniel, 1977; Strang, 
1980). 
3.5 Binary linear codes 
The most important property of binary linear codes is expressed in the 
following almost obvious lemma. 
Lemma Let C be a binary code. Then it is linear code if and only if it is a 
subspace of the vector space Bn. 
Proof Suppose C is a linear code. We already know that the sum of two 
code words is a code word. 
It remains to show that a scalar multiple of a code word is a code word. 

32 
Error-correcting codes and finite fields 
As 1 · u = u multiplying by 1 obviously takes code words to code words. The 
other multiple, 0 · u = Q, 
the all-zero word of length n. To show that Q is a 
code word note that Q 
= u + u = u - u (binary addition and subtraction are 
the same). But if u encodes x, then u + u encodes x + x = x - x = QP, 
the 
all-zero word of length m. Thus Q 
is a code word. 
Conversely, if C is a subspace, then for any two code words u and v, u + v 
lies in C, hence C is linear. 
• 
Any subspace of B" has exactly 2m elements, where m is its dimension (see 
Exercise 5). Hence it can be used to encode message blocks of length m. 
Since encoding shorter message blocks just means that some code words are 
never used, we shall assume that the rank of a linear code is the same as its 
dimension, and we shall use the two words interchangably. 
It is now natural to extend the definition of linear codes to other alphabets. 
Definition Let A be an alphabet that is a field. Then a code C of block 
length n over A is a linear code if it is a subspace of A". 
From now on propositions and theorems will be stated and proved for 
general linear codes, but the examples will still be binary codes. This is 
because more general codes are needed for practical multiple error correction 
in the third part, and the proofs for general linear codes are essentially 
identical to the proofs for binary codes. For the reader wishing to become 
more familiar with general linear codes, the exercises will contain a series of 
examples on ternary codes, which have three symbols ( 1 ,  0, - 1 ). 
A useful fact about linear codes is that you can read off the minimum 
distance of the code by check ing the weights of its code words. 
Proposition For a linear code C the minimum distance is equal to the minimum 
weight of a non-zero code word of C. 
Remark To find the minimum weight of a code word you have to check 
at most ICI words. Finding the minimum distance of a non-linear code 
involves checking all pairs of code words, about IC12/2 checks. Often the 
structure of a linear code enables the minimum weight to be found without 
checking all code words. 
Examples 
• 
Code A. The (8, 7)-parity check code. The minimum weight of an even 
weight non-zero word is obviously 2, and that is the minimum distance 
of the (8, 7)-parity check code. 
• 
Code C. The triple check code. Permuting the bits of a message word 
in any way will merely pennute the bits of the corresponding code word. 

Linear codes 
33 
So by checking the weights of 1001 10, 1 1001 1 and 1 1 1000 we see that the 
minimum weight of a non-zero word is 3, the minimum distance of the 
code. 
Proof if u and v are code words then d(u, v) = wt(u - v). But u - v is a 
code word because C is linear. 
• 
3.6 Encoders and linearity 
In Section 3.2 it was noted not only that our example codes are linear but 
that they encode sums of message words to sums of code words. When we 
are dealing with linear codes we shall always assume that the encoder 
respects that linearity. 
Definition An encoder E for a linear binary (n, m)-block code C is a linear 
map from Bm 
to B". 
The central theme of linear algebra is the way linear maps can be 
represented by matrices. We can obviously define matrices over any field 
such as B, 
and matrix multiplication can be defined in the standard way. To 
see this examine the following calculation. 
Example Matrix multiplication over B. 
Now it ought to be possible to accomplish encoding for a linear code by 
matrix multiplication. Let us test our examples. 
Examples 
• 
Code A. The parity check code. Suppose x is a word of length 7 encoded 
into the word u of length n. We are looking for a matrix GA, such that 
uT = GAxT, where the exponent T denotes the transpose that converts 
rows to columns and vice versa. 
The first symbol of the code word encoding the message word x is the 
first symbol of x. So the first row of GA is (1 0 0 0 0 0 0). Similarly 
the second row is (0 1 0 0 0 0) and so on until the seventh row is 
(0 0 0 0 0 0 1 ). 
The check symbol is the sum of all the others so the eighth 

34 
• 
Error-correcting codes and finite fields 
row of G is ( 1 1 1 1 1 1 1). Hence 
0 0 0 0 0 0 
0 
0 0 0 
0 0 
0 0 
0 0 
0 0 
0 0 0 
0 0 0 
GA = 
0 0 0 0 
1 
0 0 
0 0 0 0 0 
0 
0 0 0 0 0 0 
We check this by choosing a message word and calculating the code 
word by matrix multiplication and by the rule given in Chapter 1. Take, 
say, x = 0101 1 00. This encodes as u = 0 1 01 1 00 1 .  We verify that uT = 
GAxT: 
0 0 
0 0 0 0 
0 
0 
0 
1 
0 0 0 0 0 
0 0 
0 0 0 0 
0 
0 
0 0 0 
0 0 
0 
= 
0 0 0 0 
0 0 
0 0 0 0 0 
0 
0 
0 
0 0 0 0 0 0 
0 
0 
1 
Code B. The triple repetition code . 
Now we seek a 3 x 1 matrix GB 
taking 1 to (1, 1, 1)T and 0 to (0, 0, O)T. The obvious matrix is 
G,{] 
• 
Code C. The (6, 3)-triple check code. The message word abc is encoded 
to abcxyz, where x = a + b, y = a + c, and z = b + c. Write the words 
as columns. If this transformation is accomplished by multiplying 
(a, b, c? by Gc, then the first three rows of Gc are (1 0 0), (0 1 0) and 
(0 0 1) because the first three bits of the code word are the same as the 

Linear codes 
35 
message bits. The last three rows calculate the check bits from the 
message bits. Hence 
Gc = 
1 0 0 
0 
0 
0 0 1 
1 
1 0 
1 0 
0 1 
Again let us check this on a message word x, say x = 1 10. By the 
equations this encodes as u = 1 1001 1. Now we verify that uT = GcxT. 
0 0 
0 
0 
0 
0 
0 
.3.7 The generator.matrix 
The matrix used to encode a linear code is very important and so it is given 
a name. 
Definition 
Let C be a linear (n, m)-code with encoder E. Let the n x m­
matrix G be chosen so that E(x) = GxT for any word x of length m. Then G 
is called a generator matrix of the code, 
Recall that an encoder is called standard or systematic if the message word 
forms the first m symbols of the code word. That property can easily be read 
off from the generator matrix. 
Proposition Let C be a linear (n, m)-code with generator matrix G. Then the 
encoder is systematic if and only if the first m rows of G form the m x m-identity 
matrix Im. 
Proof 
The rows of G express the equations defining the symbols of the code 

36 
Error-correcting codes and finite fields 
word u encoding x in terms of the symbols of x. Thus the first m symbols 
of u are the symbols of x if and only if the first m rows of G are ( 1 ,  0, . . .  , 0), 
(0, I ,  0, . . .  , 0), . . . , (0, . . .  , I ). 
• 
3.8 The columns of the generator matrix 
The columns of the generator matrix are also significant. 
Examples 
• 
Code A. 
The matrix has seven columns, all having 1 as their last entry 
and a single other 1 .  Thus the third column is (0, 0, 1, 0, 0, 0, 1 )T. That is, 
the transpose of the code word 001 000 1 that encodes 001 000. 
• 
Code C. 
The matrix has three columns that are transposes of the code 
words 1 001 1 0, 0 1 0 1 01 , 00 1 0 1 1 that encode 1 00, 0 1 0  and 00 1 respectively. 
This pattern holds always, even for non-systematic encoding. 
Definition 
The elements 10 . . .  0, 0 1 0  . . .  0, . . .  , 0 . . .  01 of A" having exactly 
one entry equal to 1 and all others equal to 0 are called the unit words of 
length n and denoted by e1, e2, •
•
• 
, e • .  
Proposition 
Let C be a linear (n, m)-code with generator matrix G .  Then the 
columns G 1 ,  •
•
•
 , Gm of G are the transposes (!f the code words encoding the 
unit words (!( /enyt h 111, e 1, • • • , £'111 • 
Proof All the proposition states is that Gei = Gi. That is true for all 
m x n-matrices G and unit words length m from the definition of matrix 
multiplication. For the dot product of the jth row of G with ei is 
(gil• • . . ' gji• . . .  ' gjm) · (O, . . . ' 1 , • 
•
.
, 0) = gji· 
• 
Example 
To convince yourself of this elementary fact try the following 
matrix multiplication with ordinary numbers: 
3.9 Codes and generator matrices 
It is quite possible for a code to have several distinct generator matrices, but 
not all 11 x n-matrices occur as generator matrices of some code. In order 

Linear codes 
37 
for the matrix to implement an encoder it must take distinct message words 
to distinct code words. From the rank and nullity theorem of linear algebra, 
Theorem LA. I 0, it follows that an m x n-matrix M implements an encoder 
if and only if it has rank m or, equivalently, its columns are linearly 
independent. That is always the case if the first m rows of M form an 
m x m-identity matrix, but it may or may not be true for more general 
matrices. 
Examples 
1. The following matrix is a non-systematic encoding matrix for Code A, 
the (8, 7)-parity check code: 
1 
0 0 0 0 0 0 
0 0 0 0 0 
0 
1 0 0 0 0 
0 0 
0 0 0 
M =  
0 0 0 
0 0 
0 0 0 0 
1 
0 
0 0 0 0 0 
0 0 0 0 0 0 
The proof.that the matrix really does have the claimed property is a 
good exercise in matrix manipulation (see Exercise 3.14). 
· 
2. 
The following matrix is not a generator matrix for any code: 
3. 
Finally, here is a matrix that implements an encoder for Code C, the 
triple check cod.e: 

38 
Error-correcting codes and finite fields 
P =  
0 
0 
0 0 1 
0 
0 0 
:\s the c.ode orJy c-en1.ainၛ eig.h: 
:ode \i;0:d5- :l".:.iॶ .:-.:.:. ·:-re \·.::-=:f:c:.:! d.i:-·e..::ly. 
Again the proof is left as an exercise. 
The following proposition gives simple necessary and sufficient conditions 
for a matrix to be a generator matrix for a given linear binary code. 
Proposition 
Let C be an (n, m)-linear code and let G be an n x m-matrix. 
Then G is a generator matrix for C if and only if it has rank m and its columns 
are code words. 
Proof The conditions are necessary. The rank of G is the dimension of its 
image space. If G is a generator matrix for C then that dimension is m. The 
columns of G encode the unit message words. If G is a generator matrix for 
C, they must be code words. 
The conditions are sufficient. Let the columns of G be G1, •
•
•
 , Gm. 
Multiplication by G takes the message word a1 
•
•
•
 am to the word a1 GT 
+ · 
· 
· + am G३. By hypothesis that is a linear combination of code words. 
So, as C is linear, it is a code word. Thus G takes Am to a subspace of C. 
The dimension of that subspace is the rank of G = m = rank C. A sub­
space of full dimension is the whole space. So G maps Bm onto C. Now 
by the rank and nullity theorem, G has nullity 0. Thus it is one-to-one and 
represents an encoder for C. 
• 
This concludes the section on encoding linear codes and we now turn our 
attention to the more demanding problem of error processing. 
3.10 Code word checking 
The major reason for using linear codes is that the first step of error 
processing, checking whether a received word is a code word or not, is 
particularly simple to achieve. Very often a linear code is defined by requiring 
certain equations hold . Then all that is necessary is to check these equations. 

Linear codes 
39 
Even when the code is not defined in this way, it is easy to construct a 
suitable set of equations to check. 
Examples 
• 
Code A. The (8, 7)-parity check code is defined by a single equation. 
The word abcdefgh is a code word if and only if 
h = a + b + c + d + e +f+ g 
or 
a + b + c + d + e + f + g + h = 0. 
• 
Code C. The (6, 3)-triple check code is defined by three equations. The 
word abcxyz is a code word if 
a + b = x  
a + c = y 
b + c = z  
or 
or 
or 
a +  b + x = 0, 
a +  c + y = 0, 
b + c + z = 0. 
These equations can be expressed in matrix form: 
[011 1 0 1 0 
0 1 0 1 0 0 
a 
y 
z 
Definition 
A check matrix for a linear code C over a field A is a 
k x n-matrix H with the property that for a vector v in An, HvT = Q if and 
only if v E C. 
The number k is arbitrary, but we shall show in Section 3.13 that the 
smallest possible value for k is n - m. The check matrix is said to be in 
standard or systematic form if it has the form (D, J), where J is the 
(n - m) x (n - m) identity matrix. Standard form (wth standard encoding) 
means that the non-message bits (the check bits) are each given as combina­
tions of the message bits. 
Remember that multiplication by H 
does not decode. The check matrix 
is part of an error-processing system, and only tests whether a word belongs 
to the code. 

40 
Error-correcting codes and finite fields 
Examples 
Standard and (where they exist) non-standard generator and 
check matrices for all three of our example codes are shown below, with the 
standard form on the left and the non-standard on the right in each case. 
• 
Code A· . 
The (8, ?)-parity check code 
Generator: 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
Check: 
( 1  
I )  none 
• 
Code B. 
The (3, 1 )-repetition code 
Generator: 
r:J 
none . 
Check: 
[: 
I º] 
[¼ »] 
0 
• 
Code C. 
The ( 
6, 3 )-triple check code 
Generator: 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 

Linear codes 
41 
Check: 
[ 
0 
1 
0 7] [ 
1 
0 
1 
0 8] 
0 
0 
0 
0 
1 
1 
0 0 
0 
1 
1 
0 
3.11 Relation between generator and check matrices 
There is a simple relation between the standard form generator and check 
matrices. The standard form generator consists of an m x m-identity matrix 
I on top of an (n - m) x m-matrix A. The check matrix consists of the 
negative of the same matrix - A  followed by an (n - m) x (n - m)-identity 
matrix J. For completeness I include the proposition below that verifies that 
that is always the case. 
Proposition (a) An (n, m)-linear code has unique generator and check matrices 
in standard form if it has either. 
(b) If these are GT = (/, AT) and H = (B, J), then A = -B. 
Proof If the matrices exist, G is m x n and H is n x (n - m). 
(a) If C has generator and check matrices G and H as above, then HG = Q, 
because the columns of G are code words. Multiplying out we get 
BI + JA = Q. 
But BI = B and JA = A. Hence G 
is determined by H and 
vice versa. Thus they are unique. 
(b) To complete the proof we must show that given G = (/, A) and 
HT = ( - AT, J), then vT = GuT for some u if and only if HvT = 0. As HG = 0 
it is immediate that, if vT = GuT, it follows that HvT = HG't;T = OuT = 0: 
Conversely, let HvT = Q 
and split vT into (uT, wT), where u consists 
of the 
first m symbols of v. Then HvT = 0 <=>  - AuT + JwT = - AuT + wT = 0. So 
wT = AuT. Hence vT = (u, w? = (u-uAT? = (/, A)TuT = GuT. 
-
• 
3.12 Existence of generator and check matrices 
Does every linear code have systematic generator and check matrices? Well 
almost. Every linear code can be modified by permuting the code-word 
symbols into a code with a standard generator matrix. The order of symbols 
in a code is not important (although it had better stay fixed once it has been 
chosen). So we may assume that we always have a code with a standard 
generator matrix. It is possible, however, that other considerations take 
precedence over systematic encoding and decoding. 

42 
Error-correcting codes and finite fields 
Here is the rather technical proof that every linear code is equivalent in 
the above sense to a systematic code. 
Propositio11 
Let C be a linear code, then it is possible to 
permute the code 
word symbols in such a way that C has a standard generator matrix. 
Proof Let G be a generator matrix for C, and let M = GT. It is possible by 
elementary row operations to convert M into a matrix M' in row echelon 
form (Lemma LA. l 3), that is: 
The first non-zero entry of the (i + l)th row occurs later than the first 
non-zero entry of the ith row. 
The first non-zero entry of each row is a 1 .  
All the other entries i n  the column containing that 1 are 0. 
Example 
Here is a matrix with ordinary numbers in row echelon form. 
[  ; :  ] 
Proof (cont.) 
The rows of M '  arc l i near combi nations of the rows of M, 
which arc code words. So the rows of M' are code words. Now we perm ute 
the columns of M '  to prod uce a m atrix M "  su ch that the llrst non-zero entry 
of the ith row is in the ith column. This corresponds to permuting the 
symbols of the code words to produce a code C". Clearly M "  starts with an 
m x m-identity matrix. Furthermore M"T is a generator matrix for C" 
because it satisfies the conditions of Proposition 3.9. 
• 
3.13 Rank of the check matrix 
That concludes the basic facts about generator and check matrices. It remains 
to prove the claim about the size of a check matrix made in Section 3.10. 
In addition it will be shown that extending a check matrix by adding columns 
that are linear combinations of columns that are already there has no effect 
whatsoever. Apart from warning the would-be constructor of check matrices 
what not to do, that fact turns out to be useful in the analysis of BCH codes 
in Part 3. 
Proposition (a) The rank of a check matrix H for an (n, m)-linear code C is 
n - m. 
(b) In particular, H has at least n - m rows. 

. Linear codes 
43 
(c) If H is a check matrix for the code C and K is a matrix obtained from 
H by adding columns that are linear combinations of the columns of H, then 
K is a check matrix for the same code. 
Proof (a) In terms of linear algebra, the statement that H is a check matrix 
for C states that C is the null space of H. That implies that the rank m of 
C (which is its dimension as a vector space) is the nullity of H. Now by the 
rank and nullity theorem (Theorem LA.lO), 
m + r = n, 
where r is the rank of H. Hence r = n - m. 
(b) By Theorem LA. l l, the rank of a matrix H is at most equal to the 
number of rows of H. 
(c) Obviously, KvT = (0, . . .  , 0) implies HvT = (0, . . .  , 0), because the 
entries of HvT are the first entries of KvT. But if HvT = 0, then KvT = 0, 
because all the entries of KvT are linear combinations of those of HvT and 
linear combinations of Os are 0. 
• 
3.14 Summary 
This chapter has laid the foundations of linear coding theory. Almost all the 
results are direct consequences of theorems from linear algebra. 
The key definitions are linear code, generator and check matrix. Given a 
linear code C with a generator matrix G 
and check matrix H, we can encode 
a message word by multiplying by G and check whether a received word is 
a code word by multiplying by H. If the result is Q, 
then it is a code word, 
otherwise it is not. If G 
and H are in standard form decoding can then be 
achieved by stripping the check bits from the code word. 
The major outstanding problem is how to correct errors if they have 
occurred. That is the subject of the next chapter, where a simple and powerful 
general technique is introduced. 
Exercise 3 
3.1 Let R be a ring and define subtraction a - b, so that b + (a - b) = a. 
Show that for any a and b a difference a - b exists and is unique. Which 
of the axioms Al-A4 remain true if addition is replaced by subtraction? 
3.2 Let F be a field and define division a/b by b * (a/b) = a. For which 
pairs a and b does a quotient ajb exist? Show that if the quotient exists 
it is unique. Which of the axioms Ml-M5 
remain true if multiplication 
is replaced by division? 

44 
Error-correcting codes and finite fields 
3.3 Let R be a commutative ring and define division as in a field. For which 
pairs a and b does a quotient ajb exist? Show that if R is the ring of 
2 x 2 diagonal matrices over a field then even when it exists, the 
quotient may not be unique. 
3.4 Show that the cancellation M5 law follows from the existence of 
inverses M4. 
3.5 Show that any subspace of B" has exactly 2m elements, where m ,;:; n is 
the dimension of the subspace. 
3.6 Let F be a finite field with q elements. Show that any subspace of F" 
has exactly qm elements, where m ,;:; n is the dimension of the subspace. 
In the next three exercises the ternary field is used. This has three elements 
0, + 1 and - 1. Multiplication is ordinary multiplication, addition is ordinary 
addition except that 1 + 1 = - 1  and - 1 + - 1  = I .  
3.7 A ternary (8, 7) parity check code is defined by adding a 'trit' (that is 
a ternary digit (0, ± 1) to each message block of length seven, so that 
the sum of the trits becomes 0. Show that this code can detect all single 
errors and some double errors. 
3.8 Construct a ternary code analogous to the triple check code. To each 
block abc of three trits add three check trits xyz such that a 
+ b = x, 
a + c = y, and b + c 
= z. Show that this code can correct single errors 
in a block . Give generator and check matrices for the code. 
3.9 
A binary code C has a standard form check matrix H. The ternary 
code D has the same check matrix. How are the code words of C 
and D related? Show that C and D have the same dimension. Show 
also that if C has minimum distance  3, then D also has minimum 
distance  3. 
3.10 Show that for any linear code the undetectable error patterns are 
precisely the non-zero code words. 
3.1 1 In an application requiring modest error correction the input data 
divide naturally into binary blocks of length 16. How many check bits 
must be added to produce a single error-correcting linear code? It is 
suggested that by abandoning linearity a more efficient single error­
correcting binary block code that divides the message into blocks of 
length 16 could perhaps exist. Is this suggestion correct? 
3.12 Design such a single error-correcting binary linear code of dimen­
sion 16 with the minimal number of check bits, by giving a generator 
matrix. 
3.13 Show that if a single error-correcting binary linear code has a standard 
form generator matrix with all rows of even weight, then the code can 
detect double errors while correcting single errors in a block. Is it 
possible to arrange that the code of Exercise 3.12 achieves this result? 
3.14 Show that the following matrix is a non-systematic encoding matrix for 

Linear codes 
45 
the (8, 7)-parity check code (see Section 3.9): 
1 
0 0 0 0 0 0 
1 
1 0 0 0 0 0 
0 
1 
1 
0 0 0 0 
0 0 
1 
1 
0 0 0 
0 0 0 
1 
1 
0 0 
0 0 0 0 
1 
1 
0 
0 0 0 0 0 
1 
1 
0 0 0 0 0 0 
1 
The following multiplication may suggest the answer: 
1 
0 0 0 0 0 0 
1 
1 
1 
0 0 0 0 0 
1 
0 
0 
1 
0 0 0 0 
0 
1 
0 0 
0 0 0 
0 
0 
= 
0 0 0 
1 
1 0 0 
1 
0 0 0 0 
1 
1 
0 
1 
0 
0 0 0 0 0 
1 
1 
1 
0 
0 0 0 0 0 0 
1 
0 
3.15 Prove that the following matrix is a generator matrix for the triple 
check code (see Section 3.9): 
1 
1 
0 
1 
1 
0 
1 
1 
0 0 
1 
1 
0 
1 
1 
0 0 

46 
Error-correcting codes and finite fields 
3.16 A binary linear code C and a ternary code D both have the check 
matrix H : 
· 
[ 1 
1 
1 
0 
0 
0 
I 
0 
0 
0] 
1 
0 
0 
1 
1 
0 
0 
1 
0 
0 
0 
1 
0 
1 
0 
1 
0 
0 
1 
0 
0 
0 
1 
0 
1 
1 
0 
0 
0 
1 
Construct generator matrices for C and D. Find the block length, 
dimension, and minimum distance of C and D. 
3.17 For the codes of Exercise 3. 16 decide for each of the following words 
whether they are (a) code words, (b) at distance 1 from some code word 
or (c) at distance at least two from all code words. In case (b) give the 
code word for which the condition holds: 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 .  
' 
1 ·  
' 
1 ·  
' 
0. 
3.1 8 How many words of length 10 cannot be converted into code words 
of the code C of Exercise 3. 1 6  by correcting at most one bit? 
3. 1 9  Let C be a linear binary code. Show that either half the code words 
have even weight, or all the code words have even weight. 
3.20 Recall that a binary code is extended by adding a bit so that all code 
words have even weight. How should the extended code be defined for 
linear codes over an arbitrary field? 
3.21 Show that extending, puncturing or shortening a linear code produces 
a linear code. Describe the generator and check matrices of these 
derived codes in terms of the generator and check matrices of the 
original codes. 
3.22 Let C be a linear code with generator matrix G. The code with GT as 
its check matrix is called the dual code c1. of C. Show that if C is an 
(n, m)-code, then C'· is an (n. n 
- m)-code. 
3.23 Tlte (u I u + r) consrrucrion. Let C and 
D be linear codes oi 
block length 
n, with C of rank m and D of rank I. Suppose that C has minimum 
distance d and D has minimum distance 2d. Show that the code X 
consisting of all words of the form (ulu + v) with u E C and v E D  is a 
linear code of block length 2n, rank m + l and minimum distance 2d. 

4 
Error processing for linear codes 
This chapter introduces a simple method of error processing for linear codes 
that always produces a closest code word and so gives maximum likelihood 
decoding. The method uses a rather large table called a standard array or 
coset 
table. The error processor then merely looks up the received word in 
the table and reads off the corresponding code word. 
4.1 Constructing the standard array 
Let C be a linear (n, m)-code over a field A 
with q symbols. We shall construct 
a qm x qn-m array T containing all the words in An. 
In order to describe the 
construction, we label the rows of T from 0 to qn-m - 1 and the columns 
from 0 to qm - 1 .  The word in the ith row and kth column will be called 
ui, k ·  The table will be constructed one row at a time. 
Row 0. The top row of T consists of the code words in any order, except 
that we require the first word to be Q. 
So u0, 0 = Q, 
and u0,k runs through 
all the code words in some order. 
Example The top row of a standard array for the triple check code, Code 
C, could be as follows: 
000000 
100 1 10 010101 00101 1  
1 1 1000 
01 1 1 10 
101 101 
1 1001 1  
Row i (Step 1 ). Supposing now we have constructed the table up the row 
i - 1 . Choose an element of An 
that has not appeared and place it in the 
Oth column so that it becomes ui, o · This word is called the row leader. As 
you will see later it is a good idea to choose the row leader to be of smallest 
possible weight. In that case the row leader is called a coset leader. For the 
time being we do not insist that row leaders must be coset leaders. 
Example 
Triple check code. A good choice for the first entry of the second 
row of the table would be 100000. Supposing that the first three rows had 



50 
Error-correcting codes and finite fields 
Proof By the first evaluation in the proof a horizontal difference is a 
difference between two code words and a vertical difference is the difference 
between two row leaders. As the code is linear the difference between two 
code words is a code word, proving the first statement. If the difference 
u;, o - ui. O between two row leaders (with, say, i > j) 
is a code word u0,k, 
then u;, o = ui. O + u0, k = ui.k· That violates the condition in step 1 for row 
i that the row leader must be chosen from the words that have not yet 
appeared in the table. 
• 
4.3 Occurrence of words in a standard array 
From the facts we have just proved we can devise a test to s·ee if two words 
lie in the same row of a standard array. 
Theorem Let C be a linear (n, m)-code and T a standard array for C. Then 
two entries u and v lie in the same row of T if and only if their diference u - v 
is 
a code word. 
Proof Corollary 4.2 above shows that if u and v. lie in the same row, then 
u - v is a code word. To prove the converse consider u = ui. k and vi. 
I •  where, 
say, i > j. Let 
x be the code word 
u0 . k - u 0 • 1 and y be the non-code 
word u;,o - uj, o· Then u - v = x + y. If u - v were a code word then 
y = (u - v) - x would be the difference between two code words. Since C is 
linear such a difference is a code word. As we know that y is not a code 
word the assumption that u - v is a code word is untenable. 
• 
We can now show that every word occurs exactly once in a standard array 
and so error processing with the array is complete (every received word is 
output as a code word) and unambiguous (the code is completely determined 
by the received word). 
Corollary 
For C and T as in Theorem 4.3, every word of length n occurs 
exactly once in T. 
Proof We first show that all the entries are distinct. If two entries u = u;,k 
and v = ui,l are the same, then u - v = Q is a code word. Hence i = j. Thus 
u - v = u0, k - u0, 1. If k :f. I, that is the difference between two distinct code 
words and hence non-zero. Thus k = I. 
If the symbol field A has q 
elements, then the table contains qmqn-m 
= qn 
distinct entries and they are words of length n. As there are only qn elements 
of An every element must occur. 
• 

Error processing for linear codes 
51 
4.4 Another view of Theorem 4.3 
Another way of stating Theorem 4.3 is to say that for any standard array 
the entries in the same row as u are the values u - v, where v runs through 
the code. This fact is so useful that I have given it a separate number. 
Example The triple check code Take the word u = 10001 1 that lies in the 
second row of the standard array T1. The differences between u and the code 
words are 
u - 000000 = 10001 1; 
u - 1001 10 = 000101; 
it - 010101 = 1 101 10; 
u - 00101 1 = 101000; 
u - 1 1 1000 = 01 101 1; 
u - 01 1 1 10 = 1 1 1 101; 
u - 101 101 = 001110; 
u - 1 1001 1  = 010000. 
As you can see these are just the words in the second row of the table. 
Corollary 
If C is a linear (n, m)-code over A with a standard array T and 
u E A", then the entries in the row of T containing u are just the values u - v 
as v ranges over C. 
· 
Proof The term w is in the same row as u if and only if w - u e C. Putting 
v == (w - u) it follows that w is in the same row as u if and only if w = u + v 
for some code word v. 
4.5 Cosets 
Choose any standard array and gather together the elements in each of its 
rows. These collections are called the cosets of the code. Since the test whether 
two elements lie in the same row of the standard array does not depend on 
the actual array we start with, the cosets will be the same whatever array is 
chosen. 
Example The triple check code The cosets are given by the rows of the 
standard array T1• Supposing we chose to construct the table in a different 

52 
Error-correcting codes and finite fields 
manner we would perhaps end up with this table. 
000000 11001 1 
100110 010101 00101 1 
1 1 1000 01 1 1 10 
101 101 
000101 
1 101 10 10001 1 
010000 001 1 10 
1 1 1 101 01 101 1 
101000 
1 00100 0101 1 1  000010 1 1 0001 
1 01 1 1 1  01 1 1 00 
1 1 1010 001 001 
1 10000 00001 1 
010 1 10 
100101 
1 1 101 1 
001000 101 1 10 01 1 101 
100001 
010010 0001 1 1  1 10100 
101010 01 1001 1 1 1 1 1 1  001 100 
0001 10 1 10101 
100000 01001 1 
001 101 
1 1 1 1 10 01 1000 10101 1 
100010 010001 
000100 
1 101 1 1  101001 01 1010 1 1 1 100 001 1 1 1  
010100 
1001 1 1  1 10010 000001 
01 1 1 1 1  101 1 00 001010 1 1 1001 
Although the rows are in a different order and the entries in each row are 
in a different order, the sets of entries in a row are the same. That is obviously 
true for the first row, but it is also true for the others. Check this by finding 
which row of the original table has the same terms as the third row of this 
one. We shall refer to this table as T5 • 
To make this discussion rigorous, we need a formal definition of a coset. 
Definition Let C be a 
linear (n, m)-code over A and let u E An. The coset of 
C containing u is the set of words v for which v - u E C. We shall use u + C 
to denote the coset of u with respect to C. 
In this definition the coset apparently depends on the choice of u. But, of 
course, different choices of u may lead to the same coset. We have to show 
that either we get identical cosets or completely disjoint ones. This fact is 
implicit in the arguments we have outlined so far, but it is worth giving a 
direct proof. 
· 
Proposition If the cosets of u and v with respect to C have any element in 
common, then they are identical. 
Proof Suppose w - u and w - v both lie in C. We shall show that the cosets 
of u and v are identical. Let x lie in u + C. Then x - u E C. Now 
x - y = (x - u) - (w - u) + (w - v). 
That is a sum of three code words and hence a code word. Thus x lies in 
the coset of v. The argument that any member of the coset of v lies in the 
coset of u is identical. 
• 
To sum up, any standard array for a linear (n, m)-code C contains every 
word of length n exactly once. Each row consists of the words of some coset 
of C. It is possible to take the cosets in any order, provided you leave the 
coset consisting of the code words at the top. The row leader for each row 

Error processing for linear codes 
K3 
may be also chosen freely, but then the order of the elements in the row is 
determined by the order of the code words in the top row. 
4.6 Error words 
We now introduce the idea of an error word for a linear code. This is an 
extension of the definition for binary codes given in Chapter 2. The error 
word is just the difference between a received word and the corresponding 
transmitted code word. Rather than considering an error processor as a 
machine for guessing which code word was sent, it is convenient to imagine 
that it tries to guess which error occurred and then uses that to determine 
the code word. 
Definition Let a linear (n, m)-code C be used to transmit a message. Suppose 
the code word u is transmitted and the word v is received. Then we say that 
the error word or error pattern e = v - u has occurred. 
Remember: received word equals transmitted code word plus error word: 
v = u + e. 
Example 
The triple check code Suppose that the word 1 1001 1 is trans­
mitted and 10001 1 is received, then the error that occurred is 010000. On 
the other hand if 1001 10 is transmitted and 10001 1 is received then the error 
that occurred is 000101 . 
Now if 1001 1 is received and we are using T1, our error processor corrects 
it to 1 1001 1, whereas ࡬f we were using T5 it corrects it to 1001 10. So we could 
say that T1 guesses that the error that occurred was 010000 and T5 guesses 
that it was 000101. Notice that these are the row leaders ofthe row containing 
10001 1 in the two arrays. 
Proposition Let P be an error processor using the standard array T. Suppose 
that a code word w is received, then P will correct w to u = w - e where e is 
the row leader of the row containing w in T. 
Proof The word at the head of the column containing e is Q. 
Thus by 
Proposition 4.2, e 
-Q 
= w - u, where u is the code word at the head of the 
column containing w. Hence u = w - e. 
• 
The proposition states that an error processor using a standard array 
always guesses that the error pattern was a row leader. One application of 
the result is to determine whether it is feasible to attempt to correct a certain 
set of error patterns with a linear (n, m)-code. 

54 
Error-correcting codes and finite fields 
Example A standard array for the triple check code or any binary linear 
(6, 3)-code has 8 rows. As there are 15 possible error patterns of weight 2, 
no standard array can have all of them as row leaders. So such a code will 
not enable us to correct them all. 
4.7 Correction to closest code word 
Shortly we shall take this argument further, but first we show that if we 
choose the row leaders of our standard array to be of minimal weight (that 
is we choose coset leaders), then each received word will be corrected to a 
closest code word. 
Theorem Let C be a linear (n, m)-code over A with a standard array T in 
which the row leaders have smallest possible weight. Let u be a word of length 
n and let v be the code word at the head of its column. Then for any code word 
w the distance d(u, w) is greater than or equal to d(u, v). 
Example 
The triple check code. Again take u = 10001 1. From the differ­
ences between u and the code words calculated above we can calculate the 
distances: 
u - 000000 = 10001 1: distance 3; 
u - 1001 10  = 000101 : distance 2; 
u - 010101 = 1 101 1 0: distance 4; 
u - 00101 1 = 101000: distance 2; 
u - 1 1 1000 = 01 101 1: distance 4; 
u - 01 1 1 10 = 1 1 1 101: distance 5; 
u - 101 101 = 001 1 10: distance 3; 
u - 1 1001 1  = 010000: distance 1 .  
The closest code word is 1 1001 1, which lies at the head of the column 
containing u in the standard array T1, but not in the standard array T5• 
Proof The distance d(u, w) is the weight of the difference u - w. As w runs 
through C, u - w runs through the coset of u, which is the row containing 
u in the table. By construction the row reader x has the smallest possible 
weight and u = x + v or v = u - x. Thus d(u, v) is minimal. 
• 
That concludes the basic facts about decoding with a standard array. 
Theoretically the standard array is an optimal decoding technique. However. 

Error processing for linear codes 
55 
there remain two problems to solve. Firstly how do you design 'good' 
multiple error-correcting codes, and secondly how can you implement a 
decoder that does not require a table of q" entries? Multiple error-correcting 
codes may need to have long block lengths and values of over 100 are not 
uncommon. A table with 2100 entries is not feasible, even on a large 
computer. 
4.8 Information from the standard array 
To attack these problems we now look at the information the standard array 
gives us about the error-processing capabilities of a code. The key to further 
progress is the simple observation of Proposition 4.6 that in standard array 
decoding the row leaders are the error patterns that are corrected. Hence if 
two error patterns lie in ti:ie same coset the code can correct at most one of 
them. 
Theorem Let C be a linear code and let S = {e1, • • •  , e.} be a set of error 
patterns. An error processor can distinguish these error patterns (regardless of 
the received word) if and only if they lie in distinct cosets of C. In that case a 
standard array processor can be constructed that corrects them. 
Example 
The triple check code. The error patterns 000101, 100100, 1 1000, 
100001, 0001 10, 100010, 010100 lie in distinct cosets. The standard array T5 
places them as row leaders. So the error processor based on that array will 
correct these errors. 
On the other hand·000101 and 101000 lie in the same coset. It is impossible 
to construct an error processor that will always correct both of these errors. 
Proof Let e be an error and let f '# e lie in the coset of e. Thus f = u + e 
for some fixed code word u '# 0. If an error processor corrects an error e 
regardless of received word, it must take f to u. If it corrects an error f 
regardless of received word it must take f 
to Q. 
Obviously no error processor 
can do both. 
Conversely suppose that f does not lie m the coset of e. Then we can form 
a standard array in which both e and f are row leaders. For all code words 
u, this processor takes the words u + e and u + f to u. Thus it corrects both 
error patterns regardless of the received word. 
• 
The most valuable set of errors we could wish to correct is the set of all 
errors up to some given weight. The proposition gives us a useful criterion 
to check whether a code is capable of this. 

56 
Error-correcting codes and finite fields 
Corollary The code C can correct all error patterns of weight y k if they all 
occur as coset leaders. That is the case if and only if they lie in distinct cosets. 
4.9 Cosets of a code and its check matrix 
There is a close relationship between the cosets of a linear code and its check 
matrix. This can be exploited to reduce the storage required for a standard 
array of an (n, m)-code by a factor 2m, but more importantly it forms the 
basis of all practical error-processing schemes. 
Proposition 
If a linear code C has check matrix H, then two words u and v 
lie in the same coset of C if and only if HuT = HvT. 
Proof By Definition 4.5, u and v lie in the same coset if and only if v - u E C. 
But by the definition of a check matrix, v - u e C if and only if H(v -u)T 
= 0. 
That is equivalent to the condition HuT = HvT. 
i 
When we use a standard array for decoding, we determine the row 
containing a received word u and then assume that the error word is the 
row leader. Since the value HuT determines the row of u we do not need to 
store all the elements, but only this value and the row leader. Since the value 
'diagnoses' the error it is called the syndrome of u. 
Definition Given a linear (n, m)-code C and a check matrix H, the syndrome 
of a word u of length n is (HuT)T (we transpose so that the syndrome is a 
row vector). 
We can restate the basic property of the check matrix by saying that a 
word is a code word if and only if it has syndrome Q. Now we can replace 
the standard array by a table containing the row leaders and syndromes only. 
Example 
Triple check code We take the standard array T1 and the check 
matrix 

Error processing for linear codes 
Then the standard array reduces to the following table: 
Row leader 
Syndrome 
000000 
000 
100000 
1 10 
010000 
101 
001000 
01 1 
000100 
100 
000010 
010 
000001 
001 
100001 
1 1 1  
57 
To show how this abbreviated table can be used, suppose u = 10001 1 is 
received. The syndrome (HuTl 
is 101. So u is decoded by subtracting 010000, 
giving 1 1001 1. Similarly 001 100 has syndrome 1 1 1, so it is decoded as 101 101. 
4.10 Syndromes of received words and error words 
One point of the previous paragraph is that the syndrome of a received word 
is the same as the syndrome of the corresponding error word. That is 
sufficiently important that we state it as a proposition. 
Proposition Let C be a linear code with check matrix H. Suppose the code 
word u is transmitted and the word v = u + e is received. The syndromes of e 
and v are equal: HeT = HuT. 
• 
Normally the error word has a relatively low weight. We can exploit the 
connection of the standard array with the check matrix to investigate in 
more detail which errors a code can correct. We start by asking what 
conditions on the check matrix ensure that a code can correct all error 
patterns of weight one. 
The (transposed) syndrome of a word of weight one is a constant multiple 
of a single column of the check matrix, as you can easily convince yourself 
by checking the following example. 
Example (In ordinary numbers) 
H{ 
1 
0 
1 
0 
0 
1 
0 
1 
1 
1 
0 
0 

58 
Error-correcting codes and finite fields 
v = (0 0 2 0 0 0), 
(HvT)T = (0 2 2). 
The reason this occurs is that in multiplying each row of H by v, the single 
non-zero entry of v is multiplied by the H entry in the corresponding column. 
4.11 Condition for single error correction 
We can now give a necessary and sufficient condition on the columns of the 
check matrix for the code to be able to correct single errors. 
Theorem A linear code C with check matrix H can correct all single errors 
if and only if the columns of H are non-zero and no column is a multiple of 
any other. In particular, a binary linear code can correct all single errors if 
and only if it has a check matrix with distinct non-zero columns. 
Examples The theorem is particularly easy to use for binary codes. Recall 
the check matrices of our example codes. 
• 
The (8, ?)-parity check code has check matrix 
(1 
1 ). 
That clearly does not have distinct columns. So the code cannot correct 
single errors. 
• 
The (3, 1 )-repetition code has check matrix 
I 
OJ . 
0 
1 
This does have distinct columns and the code can correct single errors. 
• 
Finally, the (6, 3)-triple check code has check matrix 
[ 1 
1 0 1 0 OJ 
1
0
1
0
1
0
. 
0 1 
1 0 0 1 
This also has distinct columns. So again the code can correct single errors. 
Proof By Theorem 4.8 the error words of weight one can all be distin­
guished and corrected if and only if they lie in distinct cosets. They lie in 

Error processing for linear codes 
59 
distinct cosets if and only if they have distinct syndromes. But as we observed 
above, the syndromes of errors of weight one are just the multiples of the 
columns of H. So the condition of the theorem is necessary and sufficient 
for the code to correct all errors of weight one. 
• 
4.12 Check matrix and minimum distance 
Recall that in Proposition 2.5 we proved that a code can correct all errors 
of weight at most t if and only if it has minimum distance 2t + 
1. It turns 
out that by combining this result with linear algebra we can adapt the proof 
of Theorem 4.1 1  to show that the check matrix determines the minimum 
distance of the code precisely. That result will be of central importance in 
Part 3. 
· 
To motivate the extended theorem we first restate Theorem 4.1 1. 
Theorem 4.11 
A linear code with check matrix H has minimum distance at 
least 3 if and only if the columns of its check matrix are non-zero and no column 
is a multiple of any other. 
Using the language of linear algebra, we can rephrase this again. 
Theorem 4.11 
A linear code has minimum distance greater than 2 
if and only 
if no 2 rows of its check matrix are linearly dependent. 
The extension we require is the following. 
Theorem Let C be a linear code with check matrix H. Then C has minimum 
distance > d if and only if no set of d columns of H is linearly dependent. 
We denote the columns of H by H1, Hz, . . .  , Hn. Recall that columns 
HI> Hz, . . .  , Hd form a linearly dependent set if there exist elements 
c1, Cz, .
.
.
 , cd 
E F, not all zero, such that . 
(1) 
Pr.oof The theorem follows from the following three observations. 
1. As C is linear, its minimum distance is the minimum weight of a non-zero 
code word. 
2. 
A word c is a code word if and only if HcT 
= 0. 
3. If c = (c1, •
•
•
 , en) 
the equation HcT = 0 tells us that the set of columns 
{Hi I 
ci =J:. 0} is linearly dependent. 

60 
Error-correcting codes and finite fields 
We now put these together to prove the theorem. Note that for 
c = (c 1, c2, •
•
.
 , 
c.), 
(2) 
Suppose there is a non-zero code word c = (c 1 ,  c2, •
•
.
 , 
c.), of weight  d. 
We must show that H has a linearly dependent set of d columns. As c is a 
code word, HcT = 0 and equation (2) gives 
(3) 
Choose a set of precisely d columns of H that include all those for which 
c; :f; 0; for convenience we renumber to make this set H1, . . . , Hd .. Then since 
c; = 0 for i > d, (3) becomes 
(1) 
Since c :f; 0, at least one of the coefficients c; must be non-zero. Hence the 
columns H1, •
•
.
 , 
Hd form a linearly dependent set. 
Now suppose, conversely, that H has d columns that form a linearly 
dependent set. Again renumbering for convenience, suppose the first d 
columns of H form a linearly dependent set. Then equation (1) holds for 
certain c;, 
not all zero. Defining c; = 0 for i > d we find a non-zero word 
c = (c1, c2, •
•
•
 , 
c.), such that equation (3) holds. But this equation states 
that c is a code word of C. Thus c E C, and has weight at most d. 
• 
Example This criterion becomes progressively harder to use as the mini­
mum distance in question increases. But we can derive the precise minimum 
distances of our example codes. 
• 
The (8, ?)-parity check code 
(1 
1). 
The columns of the check matrix are non-zero, but any two are equal. 
Thus the code has minimum distance 2. 
• 
The (3, I )-repetition code 
[1 
1 OJ. 
1 
0 
1 
The columns of the check matrix are distinct but the sum of the second and 
third column is the first. Thus any two columns are linearly independent but 
all three are not. The code has minimum distance three. 

Error processing for linear codes 
• 
The (6, 3)-triple check code [
0
1
1 
1 
0 
1 
0 OJ 
0 1 
0 
1 
0 . 
I 
1 0 0 
1 
61 
This also has distinct columns and again the sum of the second and third 
column is the first. So again the code has minimum distance 3. 
4.13 Summary 
This chapter contains the basic theory of error processing for linear codes. 
Sections 4.1-4.5 discuss the use of a coset table. It is shown that such an 
array will always contain all possible received words, each occurring exactly 
once. The rows of the table are always cosets of the code. Sections 4.6-4.8 
use the table to establish which error patterns can be decoded. In particular 
choosing the row leaders of the array to be coset leaders gives minimum 
distance and hence maximum likelihood correction. Sections 4.9 and 4.10 
relate the coset table to a check matrix of the code and introduce the idea 
of the syndrome of a received word. Instead of storing all the words, we need 
only store the syndromes. Then for a received word we calculate its syndrome 
by multiplying it by the check matrix. This tells us the coset containing the 
word and we correct it by subtracting the coset leader. Finally the last two 
sections analyse the check matrix directly to give a necessary and sufficient 
criterion for a code to correct single errors (Theorem 4. 1 1) and a precise 
determination of the. minimum distance of the code. (Theorem 4.12.) 
In the next chapter we shall return to binary codes and exploit our results 
to produce a class of optimally efficient single error-correcting codes. 
Exercises 4 
4.1 
Let D be a binary linear (8, 5)-code with the following parity checks 
added to the message block abcde: x = a + b + 
e, y = a + 
b + 
c + 
d, 
and z = b + d + e. Give a table of coset leaders for all the syndromes 
of E and exhibit two error patterns of weight 1 with the same syndrome. 
Construct a binary (9, 5)-code that can correct single errors by adding 
a 
further check bit to the code D. 
4.2 Let C be a linear code, and suppose the coset table (standard array) of 
Section 4.1 is used for error processing. Show that the probability that 
a word is correctly received is independent of the transmitted word. 
4.3 Prove that a binary linear code can correct t errors in a block if and 
only if for all k = 1, .
.
.
 , 2t no sum of k 
rows of its check matrix is 0. 

62 
Error-correcting codes and finite fields 
4.4 The matrix below is the generator matrix for a (7, 3)-binary linear code. 
Write down a table of code words. What is the minimum distance of 
the code? 
Write down the standard form check matrix of the code and construct 
a syndrome/coset leader decoding table for the code. Where there is a 
choice of coset leaders indicate all possible choices. 
[ 1 
0 0 0 
1 
0 
1 
0 
1 
0 
1 
0 0 
1 
1 
1 
0 
4.5 Proposition 2.5 states that a code can correct all errors of weight  t 
if and only if it has minimum distance  2t + 1. Derive a proof of this 
theorem for linear codes from Corollary 4.8. 
4.6 Use the fact that for a linear (n, m)-code the standard array has qn-m 
rows to construct a bound on the largest k for which a linear (n, m)-code 
can correct all errors of weight  k. 
4.7 Exercise 3.16 introduced a binary linear code C and a ternary code D 
both having the check matrix H: 
[: ¶ ¶ · · ¸ 0 
0 ¸ ¸
¹l 
0 
I 
0 
I 
0 
0 0 
0 0 
1 
0 
1 
0 0 0 
Construct coset/syndrome tables for C and D. 
Use the tables to check your answers to Exercise 3.1 7  and 3.1 8. 
4.8 In Exercise 1.4 you are asked to construct an improved version of the 
triple check code, in which three check bits are added to a block of 
four bits and it is still possible to correct all single errors. Construct 
such a code by using Theorem 4.1 1  to produce a check matrix. 
Construct a coset table for this improved code. Show that if_the coset 
leaders are chosen with minimal weight, then they are precisely 0, and 
all words of weight 1. How many words are at distance at least 2 from 
all code words? 
4.9 The coset table provides a method for error processing any linear code. 
Show that a coset table can be designed to match the performance of 
any error processor whose performance is independent of the trans­
mitted word. That is, given such an error processor P, it is possible to 
construct a coset table that will correct all the errors P can correct and 
perhaps more. 

5 
Hamming codes and the binary Golay 
codes 
In the previous chapter we saw that a binary linear code C can correct all 
single errors if and only if its check matrix H has distinct columns. In Chapter 
3 it was shown that the rank of C is bounded by the number of columns of 
H minus the rank of H. If H has a sufficiently large set of columns, then its 
rank is just the length of the columns. So to make the rank of the code 
maximal we must choose as many distinct columns for H as possible. This 
leads us to define a class of single error-correcting binary codes by choosing 
the check matrix to have all possible non-zero columns of length k. These 
are the celebrated Hamming codes, which we shall denote by Ham(k). 
5.1 The binary Hamming codes 
Definition The binary Hamming code Ham(k) has as its check matrix Hk 
the matrix whose columns are all non-zero binary words of length k. 
Example 
We shall order the columns of Hk in a manner that will ensure 
that the matrix is in standard form. 
Here are the check ࡫atrices of Ham(3) and Ham(4). 
0 
0 
1 
1 
0 
0 
0 
1 
1 
0 
0 
1 
0 
1 
1 
0 
1 
0 
0 
0 
0 
0 
1 
1 
1 
1 
0 
0 
0 
1 
1 
1 
0 
0 
0 
1 
0 
0 
1 
1 
0 
0 
0 
1 
0 
0 
0 
1 
1 
0 
1 
0 
1 
1 
1 
1 
0 
0 
0 
1 
Notice that H3 contains all non-zero columns of length 3 and H4 contains 
all non-zero columns of length 4. Since the matrices are in standard form 
we can read off the parameters of Ham(3) and Ham(4). Ham(3) is a 
(7, 4)-code and Ham(4) is a (15, 1 1)-code. Ham(3) uses three check bits to 
encode a message block of length 4, and is thus clearly more efficient than 

64 
Error-correcting codes and finite fields 
our sample triple check code. One aim of this chapter is to show that the 
Hamming codes represent the optimum efficiency attainable for a single 
error-correcting code. 
You may ask whether it is legitimate to choose the order of the columns 
of Hk at our convenience. The answer is yes. The only effect of permuting 
the columns of H is to apply the same permutation to the entries in all the 
code words, and for a random channel that does not affect the properties of 
the code in any way (see Exercise 5.1). 
5.2 Parameters of the Hamming codes 
We have seen that it is easy to read off the block length and rank of the 
Hamming codes from their check matrices, but it is useful to write down 
formulae for these values. 
Proposition 
(a) Ham(k) has block length n = 2k - 1 and rank 
m = 2k - k - I .  
(b) The minimum distance of 
Ham(k) is 3. 
Example 
This tells us that Ham(S) is a (3 1 ,  26)-codc with minimum 
distance 3. 
Proof (a) The number n is the number of non-zero binary words of length 
k, and hence the number of columns in Hk. 
Among the columns of H are the k unit columns 
(1, 0, . . . , 0), (0, 1 , 0, . . .  , 0) , . . .  , (0, . . .  ' 0, 1), 
which form a k x k-identity matrix. Thus Hk has rank k, and the rank and 
nullity theorem (LA.lO) tells us that the rank of Ham(k) is 2k - 1 - k. 
(b) This follows from Theorem 4.12. 
The formula shows that the rate of Hamming codes approaches 1 quite 
fast as k grows large. The codes are extremely efficient. On the other hand, 
correcting a single error in a block loses its usefulness as the block length 
increases. So it is the shorter Hamming codes that are most frequently used. 
On the other hand the long Hamming codes form an excellent basis for 
developing multiple error-correcting codes. 

Hamming codes and the binary Golay codes 
65 
5.3 Comparing Ham(3) and TPC 
Let us build coset-syndrome decoding tables for Ham(3) and our triple 
parity check sample code to compare them in a little more detail. Both codes 
produce syndromes of length 3 so that we can amalgamate the two tables. 
Example Syndrome-decoding tables for the triple parity check code (TPC) 
and Ham(3): 
Syndrome 
TPC error 
Ham(3) error 
0 0 0 
0 0 0 0 0 0 
0 0 0 0 Q 0 0 
1 0 0 
0 0 0 1 0 0 
0 0 0 0 
1 0 0 
0 1 0 
0 0 0 0 1 0 
0 0 0 0 0 
1 0 
0 0 1 
0 0 0 0 0 1 
0 0 0 0 0 0 
1 
1 
1 
0 
1 0 0 0 0 0 
1 0 0 0 0 0 0 
1 
0 1 
0 1 0 0 0 0 
0 0 0 
1 
0 0 0 
0 
I 
1 
0 0 1 0 0 0 
0 
1 
0 0 0 0 0 
1 
1 
1 
1 0 0 0 0 1? 
0 0 
1 0 0 0 0. 
Notice that when we reach the last syndrome the TPC code has run out 
of single errors, but there are several possible errors of weight 2 and our 
choice is arbitrary. On the other hand, Ham(3) precisely uses up all the 
errors of weight I and no more. Codes with this property are rare. They are 
called 1 -perfecl. 
This property of Ham(3) can be stated in a more precise form if we notice 
that the syndrome-decoding table tells us that whatever incorrect word we 
receive we can always correct it to a code word by changing one bit. This 
bit is determined by the syndrome: there is no choice. The proposition below 
restates this formally for all Hamming codes. 
Proposition To every word v e B" with n = 2k - 1, there is a unique word 
u e Ham(k) with d(u, v)  l. 
Proof Let v be a word in B". The syndrome HkvT of v is 0 if and only if 
v e Ham(k). Otherwise the syndrome is a non-zero word of length k. But 
every non-zero word appears as a column of H. Suppose that it is the ith 
column. Let ei be the unit word (0, . . .  , 0, 1, 0, . . .  , 0) with 1 in the ith place. 
Then Hkei is also the ith column of Hk. Thus u = v - ei is a code word of 
Ham(k). The uniqueness of u follows from Proposition 5.2(b). 
• 
The property we have just established for Hamming codes is also enjoyed 
by some multiple error-correcting codes. 

66 
Error-correcting codes and finite fields 
Definition An (n, m)-code C is called r-perfect if to every vector v E A" there 
is a unique code word u with d(u, v)  r. Thus Hamming codes are 1-perfect. 
5.4 Perfect codes 
Perfect codes have maximum rank among the codes that can correct error 
patterns of weight  r (see the proposition below) but they are very rare. 
The only possible parameters n, m, d for binary perfect codes are given in 
the following list: 
1. (2r + 1, 1, 2r + 1). These are the parameters of the r-fold repetition code 
which is r-perfect. 
2. (2k - 1, 2k - k - 1, 3). These are the parameters of the Hamming codes 
which are 1-perfect. 
3. (23, 12, 7). These are the parameters of the binary Golay code G2 3 ,  which 
is 3-perfect. This code has a remarkable geometric structure and will be 
constructed at the end of the chapter. 
· 
In the next proposition we establish the fact that r-perfect codes have 
maximum rank among codes of a given block length and minimum distance 
2r + I. For the proof we need the concept of a ball of radius r around a 
word u. It has the obvious definition. 
Definition The r-ball D,(u) = D with centre u consists of all vectors v E 8" 
with d(u, v)  r. 
Proposition 
If there is an r-perfect (n, m)-code then no (n, m')-code with 
m' > m has minimum distance greater than 2r. 
Proof The statement that C is r-perfect says that the r-balls centred on the 
code words are disjoint and cover 8". 
Now the number ID I of elements of an r-ball D is independent of its centre. 
So 2" = 2miDI. Thus if C' is an (n, m')-code the r-balls centred on the words 
of C' cannot be disjoint. 
• 
Remark We can calculate jDj: 
IDI = (µ) +(9) + .. . +C) 
If we insert this value in the formula 2" = zmiDI it places severe restrictions 
on m, n, and r. Thus the Golay code G23 
could not exist but for the fact that 
l + 23 + 23.22/2 + 23.22.21/6 
= 1 + 23 + 253 + 1 77 1  = 2048 = 21 1. 

Hamming codes and the binary Golay codes 
67 
5.5 Length of Hamming codes 
To illustrate the fact that short Hamming codes are more useful than long 
ones we calculate the performances of these codes on the same message and 
channel that we used in Chapter 1. That is a message of 10 000 bits length 
transmitted via a binary symmetric channel with error probability p = 0.001. 
We start by calculating the generator matrices and encoding rules for these 
codes. The generator matrix can be obtained easily from the check matrix 
by Proposition 3.1 1. 
Examples 
• 
Ham(3): 
Generator matrix: 
1 0 0 
0 
0 1 0 0 
0 
0 
1 
0 
0 
0 
0 1 
1 
0 
1 
1 
0 
0 
This matrix encodes a message word abed as a code word abcdxyz, 
where the check bits x, y, and z are calculated as follows: 
x = a +  c + d, 
y = a +  b + c, 
and 
z = b + c + d. 
For a 10 000-bit message on a channel with error probability 0.001 the 
probability of correct transmission is: 
(0.9997 + 7(0.001)(0.999)6)10 00014 ¹ 0.949. 
This is almost as good as the triple check code. On the other hand the 
rate is२. which is somewhat more efficient. The triple check code requires 
transmission of 20 000 bits while Ham(3) requires 17 500. 

68 
Error-correcting codes and finite fields 
• 
Ham(4): 
Generator matrix: 
0 0 0 0 0 0 0 0 0 0 
0 
0 0 0 0 0 0 0 0 0 
0 0 
0 0 0 0 0 0 0 0 
0 0 0 
0 0 0 0 0 0 0 
0 0 0 0 
0 0 0 0 0 0 
0 0 0 0 0 
0 0 0 0 0 
0 0 0 0 0 0 
0 0 0 0 
0 0 0 0 0 0 0 
0 0 0 
0 0 0 0 0 0 0 0 
0 0 
0 0 0 0 0 0 0 0 0 
0 
0 0 0 0 0 0 0 0 0 0 0 
0 
0 0 
0 
0 
0 
0 
0 
0 
0 0 
0 
0 0 
0 
0 
This matrix encodes a message word abcdefghijk as a code word 
abcdefghijkuxyz where the check bits u, x, y and z are calculated as 
follows: 
u = a + d + e + g + i + j + k, 
x = a + b + d + f + g + h + i, 
y = b + c + e + g + h + i + j, 
z = c + d + f + h + i + j + k. 
You can check directly that each single error causes a different 
combination of these conditions to fail (see Exercise 5.2). 
For a message of 1 0 000 bits on a channel with error probability 0.001 
the probability of correct transmission is 
(0.9991 5  + 15(0.00 1 )(0.999) 14) 10 OOO/l l ¹ 0.910. 
Now reliability has dropped noticeably, but the efficiency has increased 
sharply. This code has rate H and requires only 13 637 bits to transmit 
the message. 

Hamming codes and the binary Golay codes 
69 
E X T R A S  
The remainder of this chapter is devoted to a construction of the remarkable 
Golay code mentioned above. We shall in fact construct two closely related 
codes, the Golay codes G23 and G24. The codes are not used in later chapters 
of the book, and are included because of their great interest and beauty. The 
more practically inclined reader may omit this section entirely. 
The construction given here is due to Turyn, see MacWilliams and Sloane 
(1977, pp. 587-8) and Lint (1982, p. 43). Although it is G23 that is perfect, 
G24 is the more highly symmetrical of the two codes and it is this code which 
is constructed first. The construction gives a tiny inkling of the wonderful 
properties possessed by this code. 
5.6 A closer look at Ham(3) 
We begin by looking at Ham(3) in a little more detail. So we list its code 
words. 
Example The code words of Ham(3): 
0000000 
10001 10 
1 100101 
1 1 10010 
1 1 1 1 1 1 1  
010001 1 
1010001 
1101000 
00101 1 1  0001 1 01 
100101 1 01 10100 0101 1 10 001 1010 
101 1 100 01 1 1001 . 
Proposition 
The code words of Ham(3) apart from 0 = 0000000 and 
l = 1 1 1 1 1 1 1  have weight three or four. Those of weight four are found from 
those of weight three by replacing 0 by 1 and vice versa. 
• 
5.7 Symmetries of the Hamming code 
The Hamming code has two nice symmetries that can be read off from the 
list of code words. The first becomes apparent if we list the code words in 
two columns. We start with 0 and 1 .  Then we list the rest of the words 
-
-
forwards in the first column and from the back in the second. The list looks 
like this. 
Example 
The code words of Ham(3) arranged in complementary pairs: 

70 
Error-correcting codes and finite fields 
0000000 
1 000 1 1 0  
0 1 000 1 1 
00 1 0 1 1 1  
000 1 101 
1 1 00 101 
1 1 1 1 1 1 1  
0 1 1 1 001 
1 0 1 1 100 
1 101000 
1 1 10010 
001 1 010 
1010001 
0 1 0 1 1 10 
100 1 01 1 
0 1 10100. 
The words of the second column are obtained from the ones in the first 
column by interchanging Os and Is. 
Definition A binary word v is called the complement of a word u if v is 
obtained from u by interchanging the symbols 0 and 1 .  
Now we put all the words of weight 3 in the first column and arrange 
them to show the second symmetry. 
Example The code words of Ham(3) arranged to show cyclic symmetry: 
0000000 
1 10 1 000 
0 1 1 01 00 
00 1 1 0 1 0  
000 1 1 0 1  
1 000 1 1 0  
010001 1 
1010001 
1 1 1 1 1 1 1  
001 0 1 1 1  
1 00 1 0 1 1 
1 1 00 1 0 1  
1 1 1 00 1 0  
0 1 1 1 00 1  
101 1 1 00 
0 1 0 1 1 10 
You can see that if we push the bits of a code word to the right, wrapping 
the last bit back to the beginning we get another code word. This operation 
is called a rotation or cyclic shift. 
Definition A rotation or cyclic shift of a word by k places is obtained by 
moving each symbol k places to the right, and wrapping to the start of the 
word when a symbol passes the end. 
Codes for which every cyclic shift of a code word is a code word are very 
important and will be investigated in detail in Part 3 of the book. They are 
called cyclic codes. We note the symmetry properties of Ham(3) in a 
proposition. 
Proposition 
The Hamming code Ham(3) is a cyclic code for which the 
complement of any code word 
is a code word. 
• 

Hamming codes and the binary Golay codes 
71 
5.8 Construction of Golay code G24 (1) 
We are going to construct the Golay codes by fitting together two versions 
of Ham(3). As with a jigsaw puzzle these must be made to mesh nicely. For 
the first copy we take the version we have constructed above. We will denote 
it by H. The second code, which we denote by K, is obtained from H by 
reversing the order of the bits. Of course, internally K has the same structure 
as H, but the important fact is the way they fit together. By just writing 
down the code words of K, the reader will immediately establish the following 
fact, but we give a formal proof for completeness. 
Proposition Let H be the Hamming code Ham(3) defined above and let K 
be the code obtained from ii 
by reversing the order of the bits in each code 
word. Then H and K are both (7, 4) codes of minimum distance 3. Furthermore, 
the only words that are code words of both H and K are Q and !-
Proof It is sufficient to prove that no word of weight 3 lies in both H and 
K, because the other words apart from Q and ! are obtained from these by 
taking complements. We shift the word cyclically so that it starts with two 
adjacent Is. If it lies in H the result is 1 101000 and if it lies in K the result 
is 1 100010. So it cannot lie in both H and K. 
• 
5.9 Construction of Golay code G24 (2) 
The next step in the construction of the Golay codes is to extend both H 
and K by adding a parity check bit to the end of each code word, making 
the number of 1s even. This results in two (8, 4)-codes, that we shall call H' 
and K', with minimum distance 4. They still have only the words Q and ! 
(now oflength 8) in common, but it is no longer true that the words of K' are 
those of H' in reverse order. As in the previous section we first calculate the 
code words and then state the properties of the codes in a proposition. 
Example The code words of H' and K ': 
H 
K 
00000000 1 1 1 1 1 1 1 1  
00000000 1 1 1 1 1 1 1 1  
1 1010001 00101 1 10 
000101 1 1  1 1 101000 
01 101001 100101 10 
00101 101 1 1010010 
001 10101 1 1001010 
0101 1001 101001 10 
0001 101 1 
1 1 100100 
101 10001 01001 1 10 
1001 101 01 1 10010 
01 10001 1 
1001 1 100 
010001 1 1  101 1 1000 
1 1000101 001 1 1010 
1010001 1 
0101 1 100 
1000101 1 
01 1 10100 

72 
Error-correcting codes and finite fields 
Proposition 
H '  and K' are linear codes. The code words of H' have weights 
0, 4, and 8. Thus H' has minimum distance 4. 
The only code words H' and K' have in common are 00000000 and 
1 1 1 1 1 1 1 1. 
Proof The codes are linear, because adding a check bit is a linear process. 
The statement about the weights of code words is evident from the table. 
The minimum distance of a linear code is the minimum weight of a non-zero 
code word. 
If u' is a common code word of H' and K', then stripping its parity check 
bit yields a common code word u of H and K. Thus u = 0000000 or 1 1 1 1 1 1 1 . 
Hence u' = 00000000 or 1 1 1 1 1 1 1 1. 
• 
5.10 Construction of Golay code G24 (3) 
We are now in a position to define the extended Golay code G24. This has 
code words of length 24 which we split into three parts of length 8. 
Definition The extended Golay code G24 consists of all words of length 24 
of the form 
a + X, b + X, a + b + X, 
where a and b are code words of H '  and x is a code word of K '. 
Ex:tzmp/e 
A typical code word of G24 is obtnincd by tak i ng a 
= I I  0 1 000 l ,  
b = 100101 10, and x = 0101 1001. That gives the code word 
10001000 1 1001 1 1 1  0001 1 1 10. 
Proposition 
I n  the representation of the code words of the extended Golay 
code above the code words a, b of H '  and x of 
K '  are uniquely determined. 
Proof Suppose a +  x = c + y, with a, c in H' and x, y in K'. Then 
a +  c = x 
+ y. Hence a +  c is in both H' and K'. So a +  c = Q or a + c = !· 
In the former case a = c and X = y. In the latter case that implies that c is 
the complement of a and y is the complement of x. 
Suppose now that a + x, b + x, a + b + x = c + y, d + y, c + d + y, with 
d also in H'. Then either a = c and x = y, in which case it follows that b = d, 
or c is the complement of a and x is the complement of y. But then d is the 
complement of b and c + d is also the complement of a + b. But because in 
B, 1 + 1 = 0 + 0 and 1 + 0 = 0 + 1, the result of adding the complements 
of a and b is a + b itself and not its complement. Hence this case cannot 
o9u: 
• 

Hamming codes and the binary Golay codes 
73 
Corollary 
The extended Golay code G24 has block length 24 and rank 12. 
Proof The block length is immediate from the construction. 
From the proposition G24 is a linear code with 242424 = 212 code words. 
So it has rank 12. 
• 
We can also check the rank by constructing a basis or a generator matrix. 
We do that in the following example. 
Example A generator matrix for the extended Golay code G24• 
We begin by writing down generator matrices for H' and K'. The generator 
matrix for H' is obtained by adding the appropriate parity check row to the 
generator matrix for H. That gives 
A generator matrix for K' can be obtained by reversing the first seven rows 
of the matrix we have just obtained, giving 
Now we substitute the columns of A for a 
and b and those of B for x in the 
formula a + x, b + x, a + b + x, one at a time, taking exactly one of a, b 

74 
Error-correcting codes and finite fields 
and x to be non zero. That gives the 24 x 12 
generator matrix for G24: 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
() 
() 
() 
{) 
() 
() 
() 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
5.1 1  The weights of code words of Golay code G24 
The last major 
task is to determine the minimum distance of G24• We begin 
by showing that the words of G24 have weight divisible by 4. Then in the 

Hamming codes and the binary Golay codes 
75 
next paragraph we shall show that G24 has minimum distance 8. To do this 
we will need to estimate the weight of a sum of words by a calculation 
involving the original words. For that purpose we introduce a product of u 
and v, u * v. Each entry (u * v); is the product U;V;, and define j(u, v) = 
wt(u * v). It is easy to verify that j(u, v) counts the number of places where 
both u and v have a 1, and so 
wt(u + v) = wt(u) + wt(v) - 2j(u, v) 
(see Exercise 5.13). For example, to calculate the weight of 1 1010001 + 
000101 1 1  we note that j(u, v) = 2; so the weight of the sum is 8 - 4 = 4. 
Indeed the sum is 1 10001 10. 
What happens if we add three words u, v, w? If we estimate the weight of 
the sum by wt(u) + wt(v) + wt(w) - 2j(u, v) - 2j(u, w) -"2j(v, w), we will 
correctly assess the contribution of the 1s that lie in exactly one or two of 
the words, but if there is a place where all three have a 1, then that will make 
a contribution of 3 - 6 = - 3 to the sum instead of + 1. So if we define 
j(u, v, w) to be the number of places where all three words have a 1, our 
correct formula is 
wt(u + v + w) = wt(u) + wt(v) + wt(w) - 2j(u, v) 
- 2j(u, w) - 2j(v, w) + 4j(u, v, w). 
Lcl us Lry Lhis oul on 1 1 0 1 000 1 ,  000 1 0 1 1 1 , 0 1 1 1 00 1 0, j(u, v) = j(u, w) = 
j(v, w) = 2 and j(u, v, w) = 1 .  Thॵ formula gives the weight of the sum as 
1 2 - 1 2  + 4 = 4, and this gives the correct weight of the sum 101 10100. 
Proposition Any code word C = a + x, b + x, a + b + x of G24 has weight 
divisible by 4. 
Proof We write C as A + B + X, where A = a, Q, a, B = Q, b, b and C = 
x, x, x. Let us estimate j(A, B), j(A, C) and j(B, C). We want to show they 
are all even. For the functions involving X that is easy: j(A, X) = 2j(a, x) 
and j(B, X) = 2j(b, x). 
For j(A, B) = j(a, b) a little work is necessary. We know that a +  b is a 
code word of H'. So it has weight divisible by 4. Now considering the formula 
2j(a, b) = wt(a) + wt(b) - wt(a + b), 
we see that all the terms on the right-hand side are divisible by 4. Hence 
j(A, B) = j(a, b) is even. 
We can now use the formula to show that C has weight divisible by 4: 
wt(C) = wt(A) + wt(B) + wt(C) 
-2j(A, B)-2j(A, X)-2j(B, X)+ 4j(A, 
B, X). 

76 
Error-correcting codes and finite fields 
Each of the weights of A, B, X is a multiple of the weight of a, b, x and 
hence a multiple of 4. Each of j(A, B), j(A, X) and j(B, X) is even so the 
terms involving these are also multiples of 4. Finally 4j(A, B, X) is obviously 
a multiple of 4. So the weight of C is a multiple of 4 as required. 
• 
5.12 The minimum distance of G24 
As G24 is a linear code its minimum distance is the minimum weight of its 
code words. A look at the generator matrix shows that G24 has code words 
of weight 8, and we know that all the code wods of G24 have weight divisible 
by 4. So it remains to investigate the possibility that G24 has a code word 
of weight 4. 
Proposition 
The extended Golay code G24 has minimum distance 8. 
Proof Suppose C = a + x, b + x, a + b + x has weight 4. a, b, a + b, and 
x all have even weight and by the formula 
wt(u + v) = wt(u) + wt(v) -2j(u, 
v) 
it follows that a +  x, b + x, 
and a +  b + x have even weight. Thus one of 
them must have weight 0. That implies that x 
lies in H. We remark for later 
use that then x = Q or x = .!. but for the moment we only need the 
consequence that the words a + x, b + x and a +  b + x all lie in H. Every 
non-zero word in H has weight Á 4, so exactly two of the three words must 
be Q. Thus two of a, b and a + b must equal x. Hence the third is Q. Therefore 
C is of the form Q, Q, x (or a permutation of this). But as x 
= Q or x = ! the 
weight of Q, Q, x is 0 or 8, contradicting our hypothesis. 
We have shown that all the non-zero code words of G24 have weight at 
least 8. If a is a code word of H of weight 4 the code word A = a, Q, a of 
G24 has weight 8. Hence the minimum weight and thus the minimum distance 
of the code is exactly 8. 
• 
5.13 The Golay code G23 
All the code words of G24 have even weight. So we can regard the last bit 
of each code word as a parity check bit. We remove this bit to find a new 
code G23 with block length 23. This is the binary Golay code G23• 
Theorem The binary Golay code G23 has rank 1 2  and minimum distance 1. 
It 
is 3-perfect. 

Hamming codes and the binary Golay codes 
77 
Proof If two code words of G24 agree in their first 23 positions, then they 
are the same, because the last bit must be chosen to make their weights even. 
Hence G23 has the same number of distinct code words as G24• Thus it has 
212 code words and hence rank 12. 
The minimum weight of a code word of G23 is 7 or 8. The minimum 
will only be 8 if all code words of weight 8 in G' have 0 as their 
last bit. But 1 1010001000000001 1010001 is a code word of G24, so 
1 1010001000000001 101000 is a code word of G23• Thus the minimum weight 
and hence the minimum distance of G23 is 7. 
Since the minimum distance of G23 is 7 the balls of radius 3 around code 
words are pairwise disjoint. So no word of length 23 is at distance :::; 3 from 
two code words. Each 3-ball D = D3(u) contains exactly 
+ 23 + 23.22/2 + 23.22.21/6 
+ 23 + 253 
+ 
1771 
= 2048 
= 21 1 
words. Thus the 3-balls around code words together contain exactly 
21 1212 
= 223 words. That is the whole of B23• Thus every word is at distance 
3 from a unique code word. 
• 
It was shown by V. Pless (1968), S. L. Snover (1973), and Delsarte and 
Goethals (1975) that the Golay codes G23 and G24 are the unique codes with 
their parameters (for details see MacWilliams and Sloane, 1977, p. 646). 
That book contains a great deal more about the structure of these truly · 
wonderful objects, as does Conway and Sloane (1988). For an elementary 
introduction to the geometric aspects of these codes I refer the reader to 
Thompson (1983). 
5.14 Summary 
In this chapter we have constructed two families of perfect codes, first the 
single error-correcting Hamming codes with k check bits, Ham(k), and then 
the remarkable triple error-correcting Golay code G23• There was also a 
short discussion explaining why perfect codes are rather rare. 
Exercises 5 
5.1 
What is the effect on the code Ham(k) of changing the order of the 
columns of the check matrix Hk? 

78 
Error-correcting codes and finite fields 
5.2 Show that the generator matrix for Ham(4) given in Section 5.5 encodes 
a message word abcdefghijk as a code word abcdefghijkuxyz where the 
check bits u, x, 
y, and z are calculated as follows: 
u = a + d + e + g + i + j + k, 
x = a + b + d + f + g + h + i, 
y = b + c + e + g + h + i + j, 
z = c + d + f + h + i + j + k. 
Check that each single error causes a different combination of these 
conditions to fail. 
5.3 Show that the codes H' and K' of Section 5.9 can correct all single 
errors in a block and simultaneously detect the presence. of double 
errors, but that when they detect a double error they cannot determine 
the transmitted word. 
5.4 Show that a ternary code with check matrix H can correct all single 
errors if and only if no two columns of its check matrix have sum or 
difference 0. 
5.5 Use Exercise 5.4 to construct a ternary Hamming code Ham 3(k), with 
check matrix H3,k having as its columns all non-zero length k ternary 
words with first non-zero entry + 1 . Thus (0, 1, - l )T is a column of 
H3.k, but (0, - 1, l)T 
is not. Calculate the block length and rank of 
the code and show that it is perfect. 
5.6 Generalize Exercise 5.5 to arbitrary finite fields F with IFI = q. 
5.7 Show that there is no ! -perfect binary code of block length 8 (see 
Exercise 2. 1 0). 
5.8 Show that if D is a ball in B" of radius I, then ID I is a power of 2 if 
and only if r is of the form 2" - 1. Explain why this implies that !-perfect 
codes must have the same rank as Hamming codes. 
5.9 Show that if D is a ball in 8" of radius r and n = 2r + 1, then IDI is a 
precise power of 2. How many such balls are required to cover B"? 
What are the r-perfect codes corresponding to these cover,s? 
5.10 The ternary ball of radius r about a word of length n consists of all 
words whose distance from u is at most r. Calculate the volume of this 
ball (that is, the number of words contained in the ball). 
5.1 1  For each n construct a binary linear code of maximal rank with 
minimum distance 2. 
5.12 Try to extend the Hamming check matrix H4 to produce a check matrix 
of a double-error-correcting code. Such an extension is described in 
detail in Chapter 1 3. 
5.1 3 Prove the formula for the weight of the sum of three binary vectors 
given in Section 5.1 1. Generalize it to a formula for the weight of the 
sum of n binary vectors. 

Appendix LA 
Linear algebra 
This appendix gives a sketch of that part of linear algebra that is required 
for coding theory. The appendix is not intended to be a learning text and 
the reader who is completely unfamiliar with the subject should study one 
of the textbooks Birkhoff and MacLane (1977), Cohn (1982), Noble and 
Daniel (1977) or Strang (1980) before tackling coding theory. 
On the other hand, although coding theory requires only a small part of 
linear algebra, it uses that in the context of finite fields rather than the real 
numbers used by the textbooks. So, even for a reader familiar with standard 
linear algebra, it may be worth while to skim through the appendix. The 
treatment foregoes the advantages of the standard 'coordinate-free' presenta­
tion of linear algebra, because in coding theory a change of coordinates can 
completely change the characteristics of a code. Instead, linear algebra is 
presented for vectors considered as n-tuples. This allows a quicker introduc­
tion, but makes some proofs less transparent. 
In keeping with its nature as a revision text, the style of the appendix is 
terser than that of the main body of the book. I assume that most readers 
will be at home with vectors and matrices and so do not dwell on their 
definitions. However, it is unfortunately the case that engineering mathe­
matics courses often skimp the presentation of the central result of linear 
algebra, the rank and nullity theorem, so this is presented in a little more 
detail. The appendix ends with a short discussion of row operations and a 
technical section on Vandermonde matrices which is required in Part 3 of 
the book. 
· 
LA.l Matrices 
Linear algebra underlies the theory of matrices and vectors. A matrix is an 
m x n array of entries a;i which we shall assume are taken from a field F. 
In coding theory this is usually a finite field, but the theory holds for any 
field. The matrix with entries a;i is denoted by A = (aii). A row vector is a 
1 x n matrix and a column vector is an n x 1 matrix. We can tum any 
matrix A = (ai) on its side by defining its transpose AT, which is an n x m 
matrix with AT = (ai;). The transpose of a row vector is a column vector 
and vice versa. We shall just speak of vectors when it is indifferent whether 
they are written as rows or columns. 
The set of vectors of length n is denoted by P, its elements will be written 

80 
Error-correcting codes and finite fields 
as columns in matrix calculations but as rows x = (x 1 , . . .  , x.) when this is 
more convenient. F" comes equipped with standard operations for adding 
vectors and for multiplying them by elements of F (called scalars). Thus 
(xl, . . . , x.) + (yl, . . .  , Y.) = (xl + Y1, . . .  , x. + Y.) 
and 
a(x1, •
•
• , x.) = (ax1, •
•
•
 , ax.). 
There is an obvious zero vector Q, with all its entries 0 and each vector x 
has a negative - x, obtained by multiplying it by - 1. These same operations 
apply to m x n matrices. 
LA.2 Vector spaces 
Coding theory is concerned with choosing subsets of F" that have nice 
properties. Two such properties of general importance are closure under the 
vector addition and multiplication by scalars defined above. 
· 
Definition A non-empty subset S s; F" is called a vector space if it is closed 
unщer vector addition and multiplication by scalars. A vector space U 
contained in a vector space V is called a subspace of V. 
Of COlJfSe, F" is a vector space and so in { Q }, which is the smallest vector 
space. We can consider F" as a subspace of F" + 1 by appending a 0 to all 
its vectors. 
LA.3 Linear dependence 
There is an invariant called the dimension or rank associated with each 
vector space. It measures the degrees of freedom available to the elements 
of that space. Thus for a linear code the rank indicates the length of the 
message word corresponding to each code word. The idea of degrees of 
freedom is too vague to be usable in proofs and is replaced by the formal 
concept of linear independence. 
Definition A finite subset S = {v1, •
•
•
 , vd of a vector space V 
is called 
linearly independent if the only way of writing Q as a sum of multiples of the 
vectors in S is 

Appendix LA Linear algebra 
The way this is used formally is to assume that 
Q = a1 · v1 + a2 • v2 + · 
· 
· + ak · vk. 
81 
and see if this forces a1 = a2 = 
· 
· 
· = ak = 0. Incidentally, a sum of multiples 
a1 · v1 + a2 • v2 + · · · + ak · vk is called a linear combination of v1, . . •  , vk. 
A natural choice for a linearly independent subset of pn is the standard 
basis, which consists of the unit vectors 
el 
= (1, 0, . . . ' 0), e2 = (0, 1, 0, . . .  ' 0), . . . ' en = (0, . . .  ' 0, 1). 
If we add any further vector x = (x1, • • •  , xn) to this set it ceases to be linearly 
independent because 
Q = x1 · e1 +  x2 · e2 + · 
· 
· + xn · en + (- 1) · x. 
This example gives another way of defining linear independence: the set S 
is linearly independent if none of its members can be written as a linear 
combination of the others. Thus each vector in S is 'independent' of the rest. 
LA.4 Dimension or rank 
We can now define the dimension of a vector space. 
Definition 
The dimension or rank dim( V) of a vector space V is the largest 
possible number of elements in a linearly independent subset of V. 
Thus if we say dim( V) = 10, we are stating that there exists a linearly 
independent subset' S of V containing 10 vectors, but there is no subset 
containing 1 1. 
This definition of dimension is not practical because it requires us to 
examine all possible subsets of V before we can be sure what the dimension 
of V is. In fact, finding the dimension is much easier. In Section LA.8 we 
shall prove that one just needs to find a single linearly independent set S 
that cannot be extended. The number of vectors in that set is the dimension 
of V. Thus the dimension of F" is n, because the standard basis is a linearly 
independent set that cannot be extended. 
LA.5 Basis 
Defication A linearly independent subset S of a vector space V is called a 
basis of V 
if for any vector v E V, 
v is a linear combination of members of S. 
It is an easy exercise to show that if S is linearly independent and v is not 

82 
Error-correcting codes and finite fields 
a linear combination of members of S, then S v { v} is linearly independent. 
The proof is included here to show how linear independence is used in 
arguments. 
Proposition Let S be a linearly independent subset of the vector space V and 
let v E V. If v is not a linear combination of elements of S, then S v { v} is 
linearly indendent. 
Proof Suppose, on the contrary that S v { v} is not linearly independent, 
and let S =  {x1, •
.
.
 , x,}. Then there exist scalars a1, .
.
•
 , a" and b, not all 
zero, such that 
Q = a1 
• x1 + a2 
• x2 + · 
· 
· + a, · x, + bv. 
If b = 0, we can omit the last term from the equation and find that S is not· 
linearly independent. Thus the assumption that S is linearly independent 
forces b t= 0. Therefore 
v = - (atfb)x1 + · 
· 
· + -(a,/b)x,. 
That shows that v is a linear combination of elements of S, contradicting 
our second hypothesis. 
• 
LA.6 Matrix multiplication 
In this section we recall the essential facts about matrix multiplication. Recall 
the definition. If A = (a;j) is an m x n matrix and B = (bjk) is an n x s matrix 
the product AB is an m x s matrix C with entries 
n 
C;k = L aiibik· 
j ; 1 
Notice that this sum only makes sense because the rows of A have the same 
length as the columns of B. The entries of AB are found by taking a row of 
A and a column of B, multiplying corresponding entries and adding the 
results. A special case of matrix multiplication occurs when B is a column 
vector. 
The statements of the following lemma can be verified by expanding the 
formulae involved. 
Proposition Let A be an m x n matrix, B and C be n x r matrices and D be 
an r x s matrix, all with entries in afield F, and let a be an element of F. Then 
(a) A(B + C) = AB + AC; 
(b) A(aB) = a(AB); 
(c) A(BD) 
= (AB)D. 

Appendix LA Linear algebra 
83 
Note that B, C and B + C are all three n x r matrices, so that the results 
of the operations on both sides of (a) ·are m x r matrices. The addition in 
part (a) consists of adding corresponding entries of B and C. The multiplica­
tion by o: in (b) consists of multiplying all the entries of the matrix by o:. 
They are the same operations as those defined for vectors. In part (c) the 
left-hand side first calculates an n x s matrix, which is then multiplied on 
the left by an m x n matrix, but the right-hand side first calculates an m x r 
matrix, which is then multiplied on the right by an r x s matrix. In both 
cases the result is an m x s matrix. 
The most important application of matrix multiplication is in representing 
linear equations. A set 
a1 1v1 + a12v2 + · 
· 
· + a1.v. = b1 
a21v1 + a22v2 + · 
· 
· + a2.v. = b2 
of m linear equations in n unknowns can be succinctly written as Av = b, 
where A = (au) is the matrix of coefficients, v is the (column) vector of 
unknowns (of length n), and b is the vector of constants (of length m). 
LA.7 Condition for non-zero solution 
We now proceed to establish the claim made in Section LA.4. First we need 
a technical lemma ori matrix equations; this states that if a system of linear 
equations has fewer equations than unknowns and the constants are all 0, 
then the system has non-zero solutions. 
Lemma Let A = (a;) be an r x (r + 1) matrix with entries in afield F. Then 
the equations Av = Q have a non-zero solution in pr+ 1• 
Proof The proof is by induction on r. 
Induction start. r = 1. In that case A = (a b) and Av = 0 is a single equation 
of the form av1 + bv2 = 0. If a = 0, choose v = (1, 0). If a =P 0, choose 
v = ((- b)/a, 1). In either case Av = 0 and v has at least one non-zero 
coefficient. 
Induction step. Let r > 1, and assume the lemma proved for r - 1. If all the 
entries in the last column of A are 0, then Ae,+ 1 = Q, where e,+1 = 
(0, .
.
.
 , 0, 1). Thus we may assume that A has a non-zero entry in its last 
column. As the order of the equations in Av = Q is immaterial, we can also 

84 
Error-correcting codes and finite fields 
assume that ar,r+ 1 =I= 0. Suppose the system of equations is 
a1 1v1 + a12v2 + ..
. + a1,r+ 1vr+ l  = 0 
Gz 1VI + a22V2 + . . . + a2,r+ !Vr+ 1 = 0 
( ! )  
Without changing the set of solutions we can subtract ak,r+ 1/ar,r+ 1 times 
the last equation from the kth equation for k = 1, . .
. , r - 1 .  That produces 
a new system of equations: 
b1 1v1 + b12v2 + · 
· 
· + Ovr+ l = 0 
b2 1v1 + b22v2 + · 
· 
· + Ov,+ 1 
= 0 
(2) 
Let B be the (r - 1) x r matrix (bii). By induction hypothesis there exists 
w = (w1, •
•
.
 , w,) such that Bw = Q, but w; =I= 0 for some i. Put 
where 
Then v satisfies the equations (2). Hence it also satisfies ( I ). Then Av = Q 
and since v contains w as its initial part, v has a non-zero coefficient. 
• 
LA.8 Basis and dimension 
Theorem Let V be a vector space with a basis B = { v 1, ... , v,}; 
then no subset 
with more than r 
elements is linearly independent. 
Proof Let S = {u1, •
.
.
 , u,, u,+ 1, .
.
.
 } 
be a set with more than r elements. By 
assumption each u; is a linear combination of members of B. Thus we can 
write u; = L aiivi for suitable choices of aii e F (notice the reverse order of 
the indices). Let A be the r x (r + 1) matrix obtained by taking the 
coefficients for u1, •
•
• 
, u, + 1 •  By Lemma LA.7, there exists a vector 
b = (b1, •
•
•
 , b,+ 1) such that Ab = Q, but not all b; = 0. We shall show that 
· 
r+ I 
L b;U; = Q, 
i =  I 

Appendix LA Linear algebra 
85 
thus showing that S is not linearly independent. First note that Ab = 0 
implies that 
r+ l 
L b;aii = 0 
for all j = 1, .. . , r. 
i =  1 
Thus 
establishing the claim. 
• 
Corollary Since, by assumption, B is linearly independent, that establishes 
that V has dimension r. 
· 
LA.9 Linearity of maps defined by matrices 
An n x m matrix A defines a map from Fm to Fn 
by taking a vector v e Fm 
to the vector Av e Fn. The maps defined by matrix multiplication are linear 
in the following sense. 
Definition A map f from Fm 
to P is called linear if for all u, v e Fn, and 
all a, b e  F, f(au + bv) = af(u) + bf(v). 
Associated with an m x n matrix A (or linear map) are two natural 
subspaces, one in Fm and the other in Fn. 
They are defined as follows. 
Definition The kernel of A is the set of vectors u E Fm, 
such that Au = Q. 
The image of A is the set of vectors v e Fn 
such that v = Au for some u e Fm. 
The dimension of the image of A is called the rank of A, and the dimension 
of the kernel of A is called its nullity. 
The nullity measures the degrees of freedom in solving an equation Au = v, 
because Au = Au' is the same as A(u - u') = Q. The rank determines the size 
of the set of vectors v e Fn 
for which the equations Au = v have any solutions 
at all. In coding theory it is natural to regard Fm 
as the message space and 
P as the space of receivable words. If A is used for encoding then the set 
of code words forms the image space. If two message words differ by a word 
in the kernel of A, they will be encoded to the same code word. It is therefore 
desirable that the kernel of A should be the zero space {Q}. 
· 
The central result of linear algebra links the dimensions of these two 
spaces. It states that any increase in the dimension of one of them is precisely 

86 
Error-correcting codes and finite fields 
matched by a decrease in the dimension of the other. Thus the more solutions 
there are to any equation Au = v, the fewer v there will be for which the 
equations have a solution. 
LA.lO Rank and nullity 
Theorem (The rank and nullity theorem) Let A be an n x m matrix over 
a field F with rank r and nullity k, then r + k = m. 
Lemma LA.7 can be interpreted as saying that the kernel of an r x ·r + 1 
matrix is not {Q}, or in other words the matrix has nullity ?: 1. That can be 
deduced from the rank and nullity theorem, because the image of A lies in 
F' and hence has dimension  r. That forces the kernel of A to have 
dimension ?: 1. 
Proof Let {u1, . . .  , ud be a basis of the kernel of A and let {w1, •
.
.
 , w, } 
be a basis of the image of A. By the definition of the image, for each 
i = 1, . . .  , r, there exists at least one v; E Fm, with Av; = w; . Choose one such 
v; for each i. We shall show that B = { 
u 1 ,  •
•
•
 , uk > v 1 ,  •
•
•
 , v,} 
is a basis of Fm. 
First we show that B is linearly independent. Suppose that 
We can multiply all the vectors by A and gel 
Now Au; = Q and Avi = W; . So this gives 
b1w1 + · · · + b,w, = Q. 
But, by assumption {w1, .
.
•
 , w,} is a basis, and hence linearly independent. 
Therefore by the definition of linear independence, b1 = · 
· 
· = b, = 0. We 
substitute this in the original equation and obtain 
Again, { u1, •
•
•
 , ud is a basis and hence linearly independent. Thus we now 
have a1 = · 
· 
· = ak = 0. By showing that all the as and bs are 0 we have 
established that B is linearly independent. 
Now we show that if x E Fm, then x is a linear combination of elements 
of B. This is also pnwed by first applying A 
and then using the information 
that gives us. The vector Ax lies in the image of A 
and by assumption 
{w1, •
•
•
 , w,} is a basis of that image. Hence Ax is a linear combination of 

Appendix LA Linear algebra 
members of { w 1 , .
•
•
 , w,} , say 
Ax = d1w1 + · 
· 
· + d,w,. 
Now define an auxiliary vector y in Fm by 
y = d1v1 + · 
· · + d,v,. 
From its definition 
87 
So Ax = Ay, or A(x - y) = Q. That implies that x - y lies in the kernel of 
A and {u1, .
.
•
 , ud is a basis of that kernel. Hence x - y is a linear 
combination of members of {u1, .
•
.
 , ud, say 
Combining the expressions for x - y and y we get 
x = (x - y) + y = c1u1 + · 
· 
· + ckuk + d1v1 + · 
· 
· + d,v,. 
That shows that x is a linear combination of elements of B and completes 
the proof. 
• 
LA.ll Column rank and row rank 
The definitions of rank and nullity given above are somewhat abstract, since 
they rely on finding the dimensions of certain subspaces. In order to use the 
rank and nullity theo_rem we need a practical way to calculate at least one 
of these two numbers. The standard choice is the rank. In this section we 
shall establish that the rank of an n x m matrix A is just the maximal size 
of a set of linearly independent columns of A. Since all these columns lie in 
P, Which has dimension n, that also proves that the rank of an n X 
m matriX 
A is at most n. 
Definition The column rank of a matrix. A is the maximal size of a linearly 
independent set of columns of A. The row rank of A is the maximal size of 
an independent set of rows. 
Theorem Let A be an n x m matrix with entries in a field F. Then the column 
rank of A is equal to the rank of A. 
Proof Denote the columns of A by At> . . .  , An. First notice that A; = Ae;, 
where e; = (0, . . .  , 0, 1, 0 . . .  0) with the 1 in the ith position. Thus the 
columns of A lie in the image space of A. For convenience rearrange the 

88 
Error-correcting codes and finite fields 
columns so that the first r columns {A 1, . . .  , A,} are linearly independent 
and all the later columns are linear combinations of these first r. We must 
show that then A has rank r. We shall do this by showing that {A1, .
•
.
 , A,} 
forms a basis of the image of A. Since we already know that {A1, •
•
•
 , A,} 
is linearly independent, we need only show that every vector in the image 
of A is a linear combination of members of {A1, •
•
•
 , A,}. 
For i = r + 1, . .. , n ,  let 
r 
Ai = L bijAj. 
j= 1 
Now let v = Au lie in the image of A and let u = (ul> . . .  , u,). Then 
and thus 
n 
u = L uiei. 
i= 1 
n 
n 
Au = L uiAei = L uiAi. 
i= 1 
i= 1 
substituting for the later columns using equation (1), we obtain 
n 
r ( 
11 
) 
v = A u =  L 
ui A i = I ui + I ui hii Ai. 
i = l 
j = l 
i = r + l 
(1) 
Thus, as required, v is a linear combination of members of {A 1, •
•
•
 , A,} . 
• 
LA.12 Equivalence of the two ranks 
There is an apparent asymmetry in Theorem LA.ll. 
Why is the column rank 
chosen and not the row rank? The answer is that both these ranks are in 
fact the same, but there are no simple calculations to show that the rank 
equals the row rank of A (assuming that matrices are written on the left). It 
is surprisingly difficult to establish the equality of the row and column rank 
of a matrix. Here is a proof using the rank and nullity theorem. 
Theorem Let A be an m x n matrix with row rank = k, then the column rank 
of A is also k. 
Proof Denote the rows of A by A 1, ... , Am. Rearrange them if necessary 
so that the first k rows are .linearly independent. Then for rows k + 1 to m 

Appendix LA Linear algebra 
we have equations of the form 
k 
Ak+i = "' b Ai 
ॴ 
ij 
• 
Let B be the (m - k) x m matrix 
[ b1 1 • 
· · blk 
bm-k, l ' ' · bm-k,k 
j= 1 
89 
(1) 
Notice that the set of vectors ܏Bek+i for i = 1, . . .  , m - k, forms the 
standard basis of pm -k. By definition, it follows that B has rank m - k. For 
each row Bi, equation (1) for Ak+i is equivalent to the multiplication 
B;A = Q. Thus the complete set of equations (1) can be rewritten as BA = 0. 
Hence for any column A1 of A, BA1 = 0. In other words, the columns of 
A lie in the null-space of B. Hence 
col. rank A ř nullity B 
and by the Rank and Nullity Theorem 
nullity B = m - rank B = m - (m - k) = k = row rank A .  
We have therefore established that the column rank of A is at most equal 
to its row rank. Applying the same argument to the transpose of A, we get 
the opposite inequality. So the row rank of A is equal to the column rank 
ၚA . 
• 
LA.13 Row operations 
Matrices are usually used to represent systems of equations. There are certain 
natural operations on equations and these are reflected in the definition of 
elementary row operations on matrices. 
Definition Let M be a matrix with entries in a field. The following 
operations on M are called elementary row operations: 
ERl. Permute the order of the rows of M; 
ER2. Multiply a row by a non-zero scalar; 
ER3. Add a multiple of one row to another. 
In Chapter 3 we need a lemma that shows that any matrix can be brought 
into row-echelon form by elementary row operations. 
Definition A matrix M is in row-echelon form if 
1. every non-zero row begins with a 1; 

90 
Error-correcting codes and finite fields 
2. all the other entries in the column of that initial 1 are 0; 
3. the first non-zero entry of row i + 1 
occurs later than that of row i (in 
particular if row i is zero, then so are all later rows). 
Lemma Any matrix M with entries in afield can be brought into row-echelon 
form by elementary row operations. 
Proof Let M be an m x n matrix. For k successively equal to 1 ,  . . .  , m apply 
the following procedure: 
• 
Step 1 .  Permute rows k to m so that the leftmost non-zero entry occurs 
in row k. If rows k to m are all zero, then stop. Otherwise, let the first 
non-zero entry in row k after this step be mkl· Then mu =· 0 if i > k and 
j < l. 
• 
Step 2. Multiply row k by 1/mkl· 
• 
Step 3. Add multiples of row k to all others so that mil = 0 for i i= k. 
It is easy to check that after this procedure has been executed i times rows 
1 to i satisfy the conditions of the definition. 
• 
LA.14 Vandermonde matrices 
That concludes the general theory of linear algebra, as far as it is needed for 
this text, but in Part 3 we shall need some facts about a special class of 
matrices called Vandermonde matrices. We prove these facts here, so that 
they do not interrupt the flow of the coding theory later. 
Definition A system A of n equations in n unknowns x1, x2, .
•
•
 , x., with 
coefficients in a field F, of the following form: 
a1x1 + a2x2 + 
· · · + a.x. = 0 
afx1 + aͳx 2 + · 
· 
· + aƼx. = 0 
a'jx1 + a'ix2 + · · · + 
a:x. = 0, 
where the coefficients a1, •
•
•
 , a. are distinct and non-zero, will be called a 
Vandermonde system of order n. 
Theorem Let A as above be a Vandemonde system of order n. Then the only 
solution of A is 
x 1  = x2 = · · · = x. = 0. 

Appendix LA Linear algebra 
91 
Proof The proof is by induction on the order n of the system. If n = 1 the 
statement reduces to the fact that in a field the equation a1x1 = 0 with a1 =/: 0 
implies x1 = 0. 
Now suppose n > 1 and assume that the system has a non-zero solution 
(xl> . . .  , x,). Rearranging the indices if necessary, we may assume that x, =/: 0. 
Divide all the equations by xn denoting xdxn by Y; to obtain 
a1Y1 + azyz + · 
· 
· + an- 1Yn- 1 = -an 
afy1 + aͳYz + · 
· 
· + a;- 1 Yn- 1 = -a; 
Now multiply each of these equations by an and subtract it from its successor 
(note: we do not use the resulting equation in the next subtraction, but return 
to the present system). This gives: 
(af - a1a,)y1 + · 
· 
· + (a;_ 1  - a,_ 1a,)Yn- t  = 0 
(ai - afa,)Yt + · 
· 
· + (a܎- 1 - a;_ 1 a,)Yn- !  = 0 
Finally introduce new variables z1, •
•
.
 , z, _ 1 , where 
Then this system can be rewritten as 
a1Z1 + · 
· 
· + an- !Zn- 1 = 0 
afz1 + 
· 
· 
· + a;_ 1z,_ 1 = 0 
Now this a Vandermonde system of order n - 1. So by the induction 
hypothesis it follows that z1 = Zz = . . . = Zn- 1  = 0. Hence, as a; =/: an for 
i < n and Xn was assumed to be non-zero, it follows that X; = z;(a; - an)xn = 0 
for i = 1, .
.
.
 , n - 1. But now the first equation of our original system reduces 
to 
This implies that a11 or X11 = 0, contradicting our assumptions. 
• 

92 
Error-correcting codes and finite fields 
LA.l5 Rank of a Vandermonde matrix 
Definition 
A matrix V of the form 
[:; :; 
:;] 
a1 aƽ 
aƾ 
with non-zero at> . . .  , a,, is called a Vandermonde matrix. 
Corollary A Vandermonde matrix V has linearly independent columns. 
Equivalently, the rank of an n x n Vandermonde matrix is n. 
Proof Theorem LA. l4 showed that the nullity of a Vandermonde matrix 
is 0. By the rank and nullity theorem the result follows. 
• 
It is perhaps excessive to quote the rank and nullity theorem in this case. 
The linear independence of the columns of V is just the statement that 
[:;] 
X1 + [:;] 
x2 + · 
· 
· + [:;] 
x, = [8] 
a'j 
a2 
a;; 
0 
has as its only set of solutions x1 = x2 = 
· 
· 
· = x, = 0. But that is precisely 
the same as the statement of Theorem LA. l 4. 

Part 2 
Finite fields 


6 
Introduction and an example 
Before attempting to construct an example of a finite field, we should discuss 
why there is any need for fields other than the binary field B. After all, 
practically all computation is done in binary. So why make life harder by 
working over other fields? There are, however, several good reasons why a 
knowledge of finite fields is indispensable in the study of error-correcting 
codes, and I shall list three of the more obvious ones here. 
6.1 Constructing codes for correcting multiple errors 
In Part 1 it was shown that extending the check matrix of a linear code in 
a linear manner does not change the code. It is obvious that if we wish to 
correct more than one error we must add checks in some way. To add 
non-linear checks in a structured way we need 'good' non-linear functions. 
The binary field B is too small to have any such functions. For instance, 
powers in a finite field are particularly promising. These are the functions 
required by the most frequently used block codes, the Bose-Chaudhury­
Hocquenghem codes (BCH codes) and Reed-Solomon codes (RS codes). In 
the binary field B there are no non-trivial powers: 1 n = 1 and on = 0 for all 
positive n. To use powers we must have a larger field. 
6.2 Correcting error bursts 
In many situations the assumption that errors occur entirely independently 
of each other is a poor model. For instance, when faults occur on storage 
devices, they are likely to affect several neighbouring bits. A better model 
for such devices is to assume that errors occur in 'bursts'. An error burst of 
length l is a sequence of l consecutive unreliable symbols in a transmitted 
word. The length l is chosen as small as possible to cover all the actual 
errors. The model then assumes that errors on the channel take the form bf 
bursts up to some fixed maximal length. 
The most straightforward approach to error bursts is to use interleaving: 
take several code words and transmit all their initial bits first, then their 
second bits and so on. So a sequence of, say, four words a =  (a1, a2, •
•
• ), 
b 
= (bt, b2, ... ), c = (c1, c2, • • •  ), d = (d1, d2, •
•
.
 ) 
would be transmitted as a1, 

96 
Error-correcting codes and finite fields 
b1, c1, d1, a2, b2, c2, d2, .
•
•
•
 That has the effect of separating the bits from 
each code word so that burst will tend to affect bits from distinct code words. 
A more subtle and, as you will see in Part 3, more powerful method of 
designing codes for correcting error bursts is to collect the signal bits together 
into blocks. If we can give these blocks a suitable field structure they can be 
viewed as the letters of the alphabet of the code. A single error-correcting 
code over that field will then correct any burst lying inside a block, and a 
double error-correcting code will correct any burst lying in two adjacent 
blocks, and so on. Applying this idea to BCH codes yields RS codes which 
will be discussed in detail in Part 3. A combination of RS codes and 
interleaving is used for error correction on compact audio discs. 
6.3 Finding new codes 
The structure of the base field controls and limits the possible codes. Allowing 
a larger range of base fields may yield new codes with special properties. 
For instance there is a 2-perfect ternary (1 1 ,  6)-code, similar to the binary 
Golay code. It was also discovered by Golay. This code cannot be 
constructed over B. That is just one example of the fact that the choice of 
alphabet field profoundly affects the available range of codes. Indeed finite 
fields arc the most important discrete st ruct u res, precisely beca use they 
control and limit the possibilities of finite patterns. As block codes are such 
patterns, they are naturally closely bound up with finite fields . 
6.4 Four-bit strings 
To give an idea of the techniques involved in constructing finite fields, we 
shall try to construct a field of 1 6  elements from scratch. Such a field would 
be useful it we wanted to manipulate blocks of four bits as single units to 
cope with burst errors. In practical applications, blocks of four are rather 
too small but, just because the field is small, it is relatively easy to use it for 
hand calculations. So this field will be used in the coding examples later in 
the book. 
Before we start the construction, we introdce a notational device. In 
contrast to computers, human beings find strings of Os and 1s difficult to 
distinguish. So we shall denote each string by the integer it represents in 
binary positional notation. For instance, a string (a, b, c, d) of length four 
will be denoted by 8a + 4b + 2c + d. Thus (1, 1, 0, 1), or 1 101 for short, will 
be denoted by 13 = 1 x 8 + 1 x 4 + 0 x 2 + 1 x 1. With this notation 
strings of four bits are denoted by the numbers 0 to 1 5  inclusive (0000 .... 0, 
0001 <-> 1, .
.
.
 , 1 1 1 0 <-> 14, and 1 1 1 1  <-> 1 5). 
Readers who have used hexadecimal notation will recognize where this 

Introduction and an example 
97 
idea comes from and they can, if they wish, substitute the letters A, B, C, D, 
E, F for 10, 1 1, 12, 13, 14 and 15. That has the advantage of requiring only 
one written symbol for each block of four bits, but the disadvantage of a 
further, possibly unfamiliar, notation. 
You should not take the notation to mean that 1 101 really is the number 
1 3. We are merely using the number as a convenient way of writing strings. 
In particular you must not think that addition and multiplication will be 
the same as for ordinary numbers. It may well not be true that 13 + 1 = 14. 
In fact, for ordinary addition 15 + 1 = 16, which does not represent any 
string of four bits. We shall have to redefine addition and multiplication in 
such a way that the result is always a string of four bits (represented by a 
number from 0 to 1 5). 
6.5 The integers modulo 16 
A first attempt at constructing the hypothetical field could be to make it as 
like ordinary numbers as possible. Let the symbol 13 really represent the 
number 1 3. When we do ordinary arithmetic with the numbers 0, .
. . , 15 the 
only problem is that the result may not be one our restricted set of numbeࡅs 
0, .
. . , 1 5. 
The simplest way of getting round this is by subtracting or adding a 
multiple of 1 6  to every answer to put it into the range we need. That is the 
kind of thing we do in calendar calculations: 1 0  days after January the 26th 
is February the 5th, 1 7  days after a Tuesday is a Friday. We can state the 
operation more mathematically by saying that we replace every integer by 
its (non-negative) remainder after division by 16. For the moment we denote 
the operations of addition or multiplication followed by taking remainders 
after division by 1 6  by $ and ®. The set of numbers 0, . . .  , 15 with this 
addition and multiplication is denoted by Z/16 (pronounced 'Zed mod 16'), 
Z being the standard mathematical symbol for the set of integers and 16 the 
number used to obtain the remainders. Just to see how the arithmetic works 
let us do some sums. 
Example Arithmetic in Z/16. 
3 $ 5  = 8, 
3 @  5 = 1 5, 
9 $ 1 1 = 4, 
9 @  1 1  = 3, 
1 1 $ 1 1  = 6; 
1 1  @ 1 1  = 7. 
It almost appears that we have hit the jackpot at our first attempt, and 
indeed Z/n is a useful construction, and is discussed in some detail in the 
next chapter. But of course, there wouldn't be a whole part of this book 
devoted to finite fields if life were that easy. Unfortunately, although it is 
not difficult to check that our operations satisfy most of the usual laws of 

98 
Error-correct in{} codes and .finite .fields 
arithmetic (to be precise, they make the set {0, . . .  , 1 5} into a commutative 
ring-see Exercise 6.2), they do have some bad properties. For instance you 
cannot multiply 4 to get an odd remainder so you cannot 'divide' 5 by 4. 
Worse still: 
4 ® 5 = 4 = 4 ® 1  
and, worst of all: 
4 ® 4  = 0. 
So Z/16 does not satisfy the cancellation law. Its structure is not even a 
domain, let alone a field (if you are uncertain what a commutative ring or 
a domain is look up the definition in Section 3.3). At this point, it is a good 
idea to stop reading and work through Exercise 6.2 before continuing. 
The failure of this attempt to produce a field is due to the fact that 16 is 
not a prime number. Any factorization of 16 into smaller numbers 16 = ab 
becomes a ®  b = 0 in Z/16. In the next chapter we shall show that if p is a 
prime number, then the construction works and Z/p is a field (see also 
Exercises 6.4-7). One special case is when p = 2. The resulting field Z/2 is 
none other than the binary field B. We could at this point consider limiting 
our constructions to Zjp, p prime. But remember where the choice of 16 
came from. We are trying to construct a field whose elements correspond to 
blocks of 4 bits. Any field whose elements correspond to blocks of k bits 
must have 2k elements. So using Zjp for a prime number p # 2 is of no use 
for our purposes. 
6.6 Polynomials with binary coefficients 
The construction of Z/16 has a further weakness. The addition does not 
correspond to the one we have used for our code words because, for example, 
4 EB 4 = 8, whereas for groups of bits such as (1, l, 0, 1) the natural addition 
would give (1, 1, 0, 1) + (1, 1, 0, 1) = 0. Perhaps we can combine this 'exclu­
sive-or' type of addition with the idea of taking remainders as the starting 
point to find an alternative construction. 
We need a structure that looks a bit like Z but has an addition where 
u + u = 0. Such a structure is the set of polynomials in a 'variable' x with 
coefficients in B. We denote this set by B[x]. For the moment we forget 
about the fact that polynomials are functions and just use the familiar rules 
for adding and multiplying them. 
Polynomial operations 
Let f(x) = anxn + 
· 
· 
· + a1x + a0 and g(x) = bmxm + · 
· 
· + b1x + b0 be 
polynomials in B[x] (that is, the coefficients ai, bi lie in B). We adopt the 

Introduction and an example 
99 
convention that for i >  n and j > m, ai = 0 and bi = 0. We may assume that 
n Á m. Then 
and 
where 
ck = aobk + albk- 1  + . . . + akbo. 
In particular if m = n = 3, then 
fg = a3b3x6 + (a2b3 + a3b2)x5 + (a1b3 + a2b2 + a3b1)x4 
+ (a0b3 + a1bi + a2b1) + a3b0)x3 + (a0b2 + a1b1 + a2b0)x2 
+ (a0b1 + a1b0)x + a0b0• 
The rule is the usual one: multiply every term of f(x) by every term of 
g(x) and gather together terms with the same power of x. The calculations 
are simplified by the fact that the only coefficients are 0 and 1, and 1 + 1 = 0. 
For f(x) = x3 + x2 + 1 and g(x) = x3 + x2 + x we get 
(f + g)(x) = (x3 + x3) + (x2 + x2) + x + 1 = x + 1 ;  
(fg)(x) = x6 + (xs + xs) + (x4 + x4) + (x3 + x3) + xz + x 
= x6 + x2 + x .  
Polynomials do indeed form a commutative ring with this addition and 
multiplication. This is proved in Appendix PF where you will find a formal 
development of the theory. The zero polynomial Q is the one with all its 
coefficients eql to 0 and the constant 1 denoted by 1 
(with all coefficients of 
positive powers of x equal to 0) plays the role of identity element. 
If coefficients are taken in B, then 
(f + f)(x) = (an + an)x" + · 
· 
· + (a1 + a1)X + (ao + ao) 
= Ox" + · 
· 
· + Ox + 0 = Q, 
so we do get 'exclusive-or' addition. 
Polynomials share many of the important properties of the integers, the 
most important of which is the cancellation law. If we multiply two non­
zero polynomials of degrees m and n the result has degree m + n and so it 
cannot be the constant Q. To be a bit more specific, if the highest non-zero 
coefficients of f(x) and g(x) are an and bm (both 1 because that is the 
only non-zero element of B), then f(x) = anx" + · · · + a1x + a0 and g(x) = 
bmxm + · 
· 
· + b1x + b0 and the sum defining the highest coefficient of the 

100 
Error-correcting codes and finite fields 
product reduces to a single term c•+m = a.bm = 1 x 1 = 1. Thus the product 
has at least one non-zero coefficient and hence it is not 0. 
We can also copy division with remainder. This is sometimes called 
'synthetic division'. If we want to divide f(x) by g(x) we first match the 
highest terms by multiplying g(x) by x•-m. Then we subtract the result from 
f(x) and repeat the process until the remainder has degree less than m. The 
idea is most easily understood by an example. 
Example Let us divide f(x) = x6 + x2 + x by g(x) = x4 + x3 + l. 
First 
multiply g(x) by x2 and subtract. The result is x5 + x (over B addition and 
subtraction are the same). Then multiply g(x) by x and subtract. The result 
is x4• Finally subtract 1 x g(x) to get x3 + 1. So the quotient is x2 + x + · 1  
and the remainder is x3 + 1. 
We can write this out like an ordinary long division: 
(x2 + x + 1 
A more concise notation is obtained by leaving out the powers x" and the 
+ signs, but then we must include all the coefficients, not j ust the non-zero 
ones, thus 
x6 + x2 + x + 1 = 1 x x6 + 0 x x5 + 0 x x4 + 0 x x3 
+ 
1 X x2 + 1 X + 0 X 1 
can be written as 
1 
0 0 0 
1 
1 
0. 
The long division is then written in the following form: 
1 0 0 
1)1 0 0 0 
1 
0(1 
1 
1 
0 0 1 
1 
0 0 0 
1 
1 
1 
0 0 
1 
1 
0 0 0 0 
1 
0 0 
1 
0 0 1 .  

Introduction and an example 
101 
6,7 The structure B[x]/f(x) 
The second version of long division suggests another representation for the 
block 1 101. We invert the correspondence we set up above. Then 1 101 
corresponds to the polynomial x3 + x2 + 1. In general, we use a.x" + · · · + 
a1x + 
a0 to represent the word (a., . . .  , a1, a0). The words we are interested 
in are represented by polynomials of degree three or less. Addition is fine as 
it stands, because the sum of two such polynomials will still have degree at 
most 3, but multiplication can increase the degree above the limit 3. So we 
copy the idea of Z/16: divide by a suitable polynomial f(x) and take 
remainders. By analogy, we denote the resulting structure by B[x]/f(x). 
In contrast to Z, there is a choice for the divisor polynomial f(x). If Z/n 
is to have 16 elements, then n must be 16, but any polynomial of degree 4 
has as its set of remainders the set of all polynomials of degree 3 or less. 
Thus B[x]/.f(x) will have as its members the polynomials of degree 3 or less, 
of which there are precisely 16. Addition in B[x]/f(x) will be the same for 
all polynomials f(x) of degree 4, but multiplication depends on the choice 
of f(x), because it involves taking remainders after dividing by f(x). 
Some polynomials turn out to be unsuitable. For the same reason that 
made Z/16 fail to be a field, we cannot take a polynomial of degree 4 that 
can be split into the product of two polynomials of smaller degree. For if 
f(x) =g(x)h(x) and g and h both have degree :::; 3, then in B[x]/f(x) 
multiplying g(x) and h(x) gives the remainder of f(x) on division by f(x). 
That is obviously Q, violating the cancellation law. So to produce a field we 
must look for polynomials which do not split. Such polynomials are called 
irreducible and are the polynomial equivalents of prime numbers. Trial and 
error will tell us that there are three choices (see Exercise 6.8). One of them 
is x4 + x3 + 1. 
· 
6.8 The field of order 16 
Using the polynomial x4 + x3 + 1 we make a second try at constructing a 
field of order 16. We let each binary 4-tuple abed represent the polynomial 
ax3 + 
bx2 + 
ex + d. As noted already, humans do not take to sequences of 
bits very well. So we introduce a second translation by representing abed by 
the ordinary number that has abed as its binary notation. Thus 13 +-+ 1 101 +-+ 
x3 + x2 + 1. 
You can now read Table 6.1. the zigzag line separates the addition part 
from the multiplication part. Addition is represented in the lower half and 
multiplication in the upper half. As u(x) + u(x) = Q +-+ 0, we omit the sums 
on the diagonal and only write the squares of the elements there. The field 
is denoted by GF(16). 

102 
Log 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
Error-correcting codes and finite fields 
Table 6. 1  The field GF(16) based on x4 + x3 + 1 
0 
1 2  
2 
9 
1 3  
7 
3 
4 
1 0  
5 
14 
1 1  
8 
6 
0 
2 
3 
4 
5 
6 
7 
8 
9 
10 
1 1  
12 
13 
14 
15 
4 
5 
4 
7 
0 
6 
7 
4 
5 
7 
6 
5 
4 
8 
9 
1 0  
1 1  
9 
8 
1 1  
10 
10 
1 1  
8 
9 
1 1  
10 
9 
8 
1 2  
1 3  
14 
1 5  
1 3  
1 2  
1 5  
14 
1 4  
15 
12 
1 3  
1 5  
1 4  
1 3  
1 2  
0 
0 
0 
0 
6 
7 
1 2  
1 4  
1 0  
9 
5 
2 
2 
3 
1 2  
1 3  
1 5  
14 
1 5  
1 2  
1 3  
1 5  
1 4  
1 3  
1 2  
8 
9 
1 0  
I I  
9 
8 
I I  
1 0  
1 0  
I I  
8 
9 
1 1  
10 
9 
8 
0 
9 
1 1  
2 
0 
7 
4 
5 
0 
0 
0 
0 
0 
1 1  
12 
1 3  
14 
1 5  
1 5  
1 
3 
5 
7 
4 
1 3  
1 4  
1 1  
8 
7 
2 
6 
1 0  
14 
12 
1 4  
1 1  
4 
1 
8 
3 
5 
1 5  
9 
3 
1 5  
8 
1 
6 
14 
4 
1 2  
1 3  
5 
5 
8 
3 
1 0  
4 
5 
8 
2 
6 
1 3  
1 1  
4 
1 2  
3 
Addition is ordinary polynomial addition over B. The rule for performing 
this addition directly with the numbers in the table may be familiar to you 
from the game of Nim. Mentally you split each number into distinct powers 
of 2, then the sum is the sum of those powers of 2 that occur in just one of 
the two numbers you are addiing. For instance, 
I 3 + 1 4 = (8 + 4 + I )  + (8 + 4 + 2) = 2 + 1 = 3 .  
I t  is also useful to remember that if a + b = c, then a + c = b. 
Multiplication (the upper half of the table) is polynomial multiplication 
over B, followed if necessary by taking the remainder after division by 
x4 + x 3 + 1 . Check this by multiplying 1 3  by 14. 
There is no simple method for doing multiplication in your head, but you 
can use the 'logarithms' at the head of the table. Just as with ordinary 
logarithms you multiply two terms by looking up their logarithms and 
adding them. If the result is bigger than 1 4, subtract 1 5  and then look up 
the number with your answer as its logarithm. That is the product. For 
instance to multiply 1 3  by 1 4. First look up their logarithms ( 1 1 and 7). Add 
them; the result is 19 > 14, so subtract 1 5. That gives 4, the logarithm of 9. 
Thus 13 x 14 = 9. 
Many implementations of finite fields use such ' discrete logarithms' to 
perform multiplication, because it is quicker than polynomial arithmetic and 

Introduction and an example 
103 
requires less storage than the full multiplication table. However, the construc­
tion of the logarithms is somewhat mysterious. It will only be possible to 
explain how they appear after we have developed some understanding of the 
structure of finite fields. The theory 'that produces them will be fully explained 
as part of the discussion of primitive elements in Chapter 12. Initially we shall 
base our theory on the conceptually simpler polynomial definition of 
multiplication. 
You should check that the construction does yield a field by finding the 
inverses of the non-zero elements. How do you do that? For a field .as small 
as this example searching is boring but quite feasible. Can you think of a 
systematic way to find the inverse in a large finite field without using 
logarithms? 
6.9 Historical digression 
It is standard practice to denote a field of order q by GF(q). The initials GF 
stand for Galois field, after the French mathematician Evariste Galois 
who died in a duel in 1832 at the age of 20, having invented the theory of 
finite fields and having made at least two further major contributions to 
mathematics. 
Galois' biography is a cautionary tale for teachers. He was unquestionably 
one of the great mathematical geniuses, but he failed the exams to enter the 
Ecole Polytechnique because he refused to write his answers in the form 
required by the examiners. He did, however, get into the Ecole Preparatoire 
in 1 829. In July 1 830 a revolution broke out against the reactionary regime 
and Galois became an ardent republican. After the suppression of the revolt 
he wrote an article. violently attacking the director of the Ecole Preparatoire 
for which he was expelled. 
He devoted much of his time to republican activities but still continued 
his research. In July 1 831 he was arrested during a demonstration and placed 
in detention for illegally wearing a uniform and carrying weapons. In March 
1 832 he was transferred to a nursing home because of the outbreak of a 
cholera epidemic. Here he had an unhappy love affair. At the end of May 
after the break-up of the affair he was provoked to a duel by an unknown 
adversary, believed by some to have been an agent provocateur. On 29 May, 
believing he would be killed, he wrote desperate letters to his republican 
friends and a summary of his major results, which he asked his friends to 
show to Gauss and Jacobi in the event of his death. Nothing seems to have 
come of this, but the letter was published in the Revue Encyclopedique in 
September 1832, though it aroused little interest. On 30 May 1832 Galois 
was admitted to hospital, mortally wounded. He died there on 31 May. His 
funeral, on 2 Jࡆne, was the occasion for a republican demonstration heralding 
the riots in Paris in the following days. 

1 04 
Error-correcting codes and finite .fields 
Galois had submitted work to learned journals and the academy from 
1 829 onwards. His first published major treatise, ' Sur Ia theorie des nombres ' 
was published in Ferussac's Bulletin des Sciences Mathbnatiques in 1 830. It 
defines the so-called 'Galois imaginaries', which are elements of finite fields, 
and provides the fundamental results on finite fields. A memoir on the 
solution of equations had earlier been sent to the Academy. Cauchy reviewed 
it favourably, but advised Galois to rewrite it in the light of the results of the 
young Danish mathematician Niels Henrik Abel who had just died. The 
revised memoir was lost on the death of Fourier, who had been assigned to 
review it, giving rise to the legend that Cauchy had just put Galois' paper 
in a drawer. In 1 83 1  Galois submitted a new version of his memoir. Cauchy 
had left France in 1 830 and Poisson was assigned to review it. He rejected 
it, saying that some of the results could be found in Abel's work and the rest 
were not fully proved. Galois, embittered by this injustice after his earlier 
misfortune, wrote ' On jugera' (posterity will judge) in the margin of his copy. 
Eventually Liouville became interested in Galois' work. In 1 843, 1 1  years 
after Galois' death, he introduced the results to the Academy of Sciences. 
He announced the publication of the memoir rejected by Poisson for the 
end of that year (it was actually published in 1 846). This memoir can be 
regarded as the foundation of modern algebra and became the basis for 
major research efforts over t he next I 00 years. 
Exercises 6 
6. 1 
Check the following calculations in Z/ 1 6  
3 EB s = 8, 
3 ® s = 1 S, 
9 EB I I  = 4, 
9 ® I I = 3, 
1 1  EB 1 1  = 6; 
1 1 ® 1 1 = 9 . 
6.2 
Verify that Z/ 1 6  satisfies all the axioms for a commutative ring. Check 
that your verification requires no special properties of the number 1 6, 
so that the same argument can be applied to Z/n for any n * 2. What 
happens for n = 0 and n = 1? 
6.3 
Show that any factorization of 1 6  into smaller numbers 1 6  = ab 
becomes a ® b = 0 in Z/1 6. 
6.4 
Show that for 1 < a <  7, ab = ac (mod 7), i.e. ab and ac leave the same 
remainder after division by 7, implies b = c mod 7. Deduce that Z/7 
satisfies the cancellation law MS. 
6.S 
Deduce from Exercise 6.4 that for any 1 < a < 7, there exists b so that 
ab = 1 (mod 7). Thus Z/7 is a field. 
6.6 
Show that for any prime p, Z/p satisfies the cancellation law MS. 
6.7 
Deduce from Exercise 6.6 that for any prime p, Z/p is a field. 
6.8 
Write down all binary polynomials of degree 4 (there are 8). Five of 

Introduction and an example 
105 
these polynomials can be factored into products of smaller degree. Find 
factorizations for these polynomials. Check that the remaining poly­
nomials cannot be factored into products of polynomials of smaller 
degree. 
6.9 Check the following calculations in GF(1 6) using the definition and not 
the table. 
3 EEl 5 = 6, 
3 ® 5 = 15, 
9 EEl 1 1 = 2, 
9 ® 1 1  = 5, 
6.10 Construct a field of order 8. 
1 1  EEl 1 1  = 10; 
11 ® 1 1  = 10. 
6. 1 1  Starting with the ternary field Z/3, construct a field of order 9. 

7 
Euclid's algorithm 
This chapter is devoted to an algorithm that is the central technique of this 
book. It is Euclid's algorithm, which was invented about 2000 years ago to 
find highest common factors of integers without first splitting them into their 
prime factors. It turns out that this algorithm is the key that enables us to 
construct all finite fields and to do arithmetic in them. In addition it provides 
an efficient method for error processing of BCH and RS codes. The existence 
of efficient error processors for these codes is the principal reason for their 
dominance of practical block-code implementations. 
Euclid's algorithm itself is both quick to implement and simple to 
understand. It works equally well for integers and polynomials and indeed 
in any arithmetic structure in which division with remainder can be 
reasonably defined. Our applications will be to polynomials, but to avoid 
unnecessary obstacles to u nderstanding I shall initially use the integers as 
the main example. The tex t will then gi ve the general theory. If you get 
confused calculate an integer example to see what is going on. 
7.1 An example 
Suppose, for some reason, perhaps because we need to express a fraction in 
lowest terms, we wish to calculate the highest common factor (greatest 
common divisor) of 1 2  and 1 04. These numbers are small and it is easy to 
do by prime factorization. 12 = 3 x 4, 1 04 = 1 3  x 8, so the highest common 
factor is 4. That is the method commonly taught at school, but for large 
numbers it is very impractical. Try finding the highest common factor of 
303 582 and 263 739 that way. 
The idea of Euclid's algorithm is to replace the original problem by an 
easier one with the same answer. You then repeat the process until the 
problem is so easy that the answer can be read off. Now, any common factor 
of 303 582 and 263 739 is also a factor of (303 582 - 263 739) = 39 843, and 
any common factor of 263 739 and 39 843 is also a factor of 303 582. So the 
HCF of 303 582 and 263 739 is the same as that of 263 739 and 39 843. We 
can thus replace our original problem by the simpler one: find the HCF of 
263 739 and 39 843. Then we repeat this step, always replacing the larger of 
a pair of numbers by their difference, until the numbers are small. The pairs 
we get are: 

Euclid's algorithm 
107 
303 582 
263 739 
39 843 
263 739 
39 843 
223 896 
39 843 
1 84 053 
39 843 
144 210 
39 843 
104 367 
39 843 
64 524 
39 843 
24 681 
15 162 
24 68 1 
15 162 
9 519 
5 643 
9 519 
5 643 
3 876 
1 767 
3 876 
1 767 
2 109 
1 767 
342 
1 425 
342 
1 083 
342 
741 
342 
399 
342 
57 
342 
57 
285 
57 
228 
57 
171 
57 
1 14 
57 
57 
It is obvious that the highest common factor of 57 and 57 is 57 itself and 
that solves the problem. As the highest common factors (HCFs) of all the 
pairs in the table are the same, the highest common factor of 303 582 and 
263 739 is also 57. 
You will agree that this method is simpler than factorizing 303 582 and 
263 739 into prime factors, but it still involves quite a lot of subtractions. 
The procedure can be improved by using division with remainder to get rid 
of repeated subtractions by the same number. We add an initial column with 
the quotients and replace the two columns of numbers by a single one. Each 
number is divided by its successor, and the remainder is placed on the row 
after the successor. The pairs of numbers with the same highest common 
factor are now consecutive entries in the second column. 

108 
Error-correcting codes and finite 
fields 
Q R 
303 582 
1 
263 739 
6 
39 843 
1 
24 68 1 
1 5  1 62 
9 5 1 9  
5 643 
3 876 
2 
1 767 
5 
342 
6 
57 
0 
Notice that the same numbers appear, but now no number appears twice. 
That shortens the table, but we lose our original test that told us when we 
had found the HCF. The signal that the HCF has been found is that the 
next number becomes 0, because the appearance of 0 implies that the last 
number is a factor of its predecessor. Obviously, the HCF of a number x 
and any multiple xy of x must be x itself. 
7.2 Euclidean domains 
You have now learned the essential part of Euclid's algorithm, but it delivers 
more information than is immediately apparent. A slightly more sophisticated 
version that displays the extra information will apear shortly, but first we 
shall extend the scope of the discussion to cover polynomials at the same 
time as integers. We do that by defining clearly the arithmetical conditions 
under which division with remainder makes sense. You can omit this 
paragraph on first reading and return to it after you have seen how Euclid's 
algorithm works for the examples. If you do that, just replace every reference 
to a ' Euclidean domain ' by the 'integers' or 'polynomials over B' and use 
the familiar methods of long division. 
We start with a set of elements that form a domain D. Remember, that 
means that all the standard laws of addition, multiplication and subtraction 
hold and we also have a cancellation law: if ab = ac and a #  0, then b = c. 
The examples that are important for this book are the integers Z or the set 
F [x] of polynomials with coefficients in a field F (e.g. F = B, or F may be 
the real or complex numbers). 
To make use of division with remainder, we need some way of distinguish­
ing a good remainder from a bad one. For instance, 23 = ( 
- 1 ) x 5 + 28, 

Euclid's algorithm 
109 
but no-one would regard 28 as the remainder of 23 divided by 5. So we 
introduce a 'size' function defined on the non-zero elements we want to use 
for division with remainder (defining the size of 0 is tricky, so we just make 
it an exception). The size function needs to satisfy certain technical conditions 
for the theory to work, and a function satisfying these is called a Euclidean 
valuation. 
Definition Let D be a domain. We shall call a function llxll defined on 
the non-zero elements x of D with values in the non-negative integers 
(0, 1, 2, . . . ) a Euclidean valuation, if 
EVl .  For every a, b -# 0, llabii Á II all and llabll Á llbll; 
EV2. For every a, b -# 0 in D there exist a quotient q and remainder r in 
D, such that a =  qb + r, and llrll < lib II or r = 0. 
Example For Z we can take llxll = lx!. It is obvious that EVl and 
EV2 are satisfied, even if a = 0. However, the quotient q and remainder r 
are not unique; e.g. 25 = 3 x 7 + 4 = 4 x 7 + ( - 3), and both 4 and - 3  
satisfy lrl < 7. I n  general that only causes minor book-keeping difficulties 
(this matter is discussed in greater detail in Chapter 9). For the moment we 
side-step all difficulties by insisting that the remainder should satisfy r Á 0. 
For B[x] we can take ll f(x)ll equal to the degree of f(x), that is 
the index of highest non-zero coefficient (it is conventional to take - oo as 
the degree of the polynomial Q). The properties of the degree are developed 
formally in Appendix PF. From the discussion there or the less formal one 
in Chapter 1 you can see that EVl and EV2 hold, but note that in this case 
the axiom EV1 fails for a =  Q. For example if b = x, then deg(Qx) = deg(Q) = 
- oo < 1 = deg(x). That is why we exclude Q from our considerations. In 
the case of polynomials, the quotient q and remainder r are unique. 
Definition A domain with a specified Euclidean valuation is called a 
Euclidean domain. 
We say b divides a if b = ac for some c, and denote this by bia. 
In that case, of course, the remainder r is 0. 
7.3 Highest common factor 
Before discussing Euclid's algorithm in this more general setting, we 
must define what the highest common factor of two objects is. 
Definition Let a and b be non-zero elements of a domain D. We say 
d is a highest common factor or HCF (US terminology: greatest common 
divisor or GCD) of a and b, in symbols d = (a, b), if 

1 10 
Error˺correcting codes and finite fields 
HCFl. dla and dlb, that is d divides both a and b, and 
HCF2. if cia and clb then cld, that is, any other common factor divides d. 
For polynomials the HCF is determined only up to multiplication by a 
constant (and even for integers, its sign is undetermined). That will be 
discussed in Chapter 8, but for the moment we shall not worry about it and 
continue as though the HCF were absolutely unique. 
Examples 
• 
The HCF of 12 and 104 is 4, because certainly 4 divides both 1 2  and 
104, and if any number divides 12 = 4 x 3 and 104 = 8 x 1 3  it must 
divide 4. 
Two comments are appropriate here. Firstly, the argument above 
assumes uniqueness of prime factorization, which has not been proved. 
An alternative argument based on Euclid's algorithm will be given in the 
next section. 
Second,,you may wonder why the definition replaces the natural idea 
that 4 is the biggest common factor by the condition that every other 
factor divides it. The reason is that if the definition uses bigness it 
requires two concepts, divisibility and size. As it stands it only uses 
divisibility. 
• 
Now consider the two real polynomials f(x) = 2x3 - x2 = (2x - 1)x2 
and g(x) = 4x2 - 4x + I 
= (2x - 1)2• Their HCF is clearly 2x - 1. This 
statement again tacitly assumes unique factorization. Furthermore why 
should we choose 2x - 1? What about x - !  or !x - !? They are 
just as good. That is the non-uniqueness that was mentioned above. It 
does not cause serious problems and will be ignored for the time 
being. 
7.4 HCF in terms of a 
and h 
One further point before we introduce Euclid's algorithm in its full ver;ion. 
When we calculate the HCF of a and b by the method introduced in Section 
7.1, we get a series of remainders and the last non-zero term is (a, b). It is 
often useful to express the HCF in terms of a and b themselves and the 
calculations permit us to do that also. 
Examples 
• 
The calculation for (12, 104) is 

Euclid's algorithm 
1 1 1  
Q 
R 
104 
8 
12 
1 
8 
2 
4 
0 
From this we can read off (12, 104) = 4, but also from the divsions we 
know that 
8 = 104 - 8 X 12, 
4 = 12 -l 
X 8 
. 
Using the equation to substitute for 8 in the second we get 
4 = 9 X 12 - 1 X 104. 
The coefficients 9 and - 1 turn out to be just as useful ,as the highest 
common factor itself. Now we can easily check that 4 Is the highest 
common factor of 12 and 104 without assuming unique prime factoriza­
tion, We have already checked that 4 divides both 12 and 104. To verify 
HCF2 observe that any common factor of 12 and 104 must divide 
9 X 12 - 104 = 4. 
• 
For the two real polynomials f(x) = 2x3 - x2 = (2x - l)x2 and g(x) = 
4x2 - 4x + 1 = (2x -1)2 
the calculation is 
Q 
!x + ! 
8x - 4 
From this we can read off that 
R 
2x3 - x2 
4x2 - 4x + 1 
tx-t 
0 
!x - ! = (2x3 - x2) - (!x + !)(4x2 - 4x + 1). 
That equation tell us that any common factor of f(x) and g(x) divides 
!x -t. 
It is clear that the coefficients needed in the examples can always be 
calculated by substitution as above, but that is more clumsy than necessary, 
and we shall modify our procedure to calculate them at the same time as 
the HCF itself. 

1 12 
Error-correcting codes and finite fields 
7.5 The four-column array for Euclid's algorithm 
For ·the full version of Euclid's algorithm we produce a table with four 
columns headed Q, 
R, U, V. The headings Q 
and R stand for 'quotient' and 
'remainder' and are the headings of the columns of the basic algorithm. The 
additional columns headed U and V will contain the elements u and v such 
that (a, b) = ua + vb. We number the rows of the table starting with - 1. 
Each row is calculated from its two predecessors. 
The first two rows are filled in as follows: 
Row 
- 1  
0 
Q 
R 
a 
b 
u 
I 
0 
v 
0 
1 
From row 1 onwards each new row is produced by first calculating the 
Q- and R-entries as before. For row 1 we do this by dividing a by 
b: a = q 1 b + r1• 
That gives us the first two entries. The U -entry is 1 and the 
V-entry is - q1: 
Notice that r1 = I a  + ( - q 1 )h. 
Now I will tell you how to calculate each new row of the table. Suppose 
you have calculated up to row k and the last two rows are: 
k - 1 
k 
Begin the calculation of row k + I by dividing rk- l  by rk: 'i. - t  = qk+ 1rk + 
rk+ 1• 
That produces the Q- and R-entries, qk+ 1 and rk+ 1 ,  of row k + 1 .  To 
get the U- and V-entries put uk + 1 = uk _ 1 - qk + 1 uk and vk + 1 = vk _ 1 - qk + 1 
vk 
using the value qk+ 1 you have already calculated. 
Stop when rk 
becomes 0. This must happen after a finite number of steps 
because at each step we get \\rkll < \Irk- t il, and the values llrkll cannot decrease 
indefinitely. 
7.6 A worked example 
Here is a worked example using 12 and 104 again. Notice that the Q- and 
R-columns are just the same as before. 

Euclid's algorithm 
1 13 
Example Calculate the HCF of 104 and 12: 
Row 
Q 
R 
u 
v 
- 1 
104 
1 
0 
0 
12 
0 
1 
1 
8 
8 
1 
- 8  
2 
1 
4 
- 1  
9 
3 
2 
0 
3 
- 26 
The calculation of row 2 goes as follows. Divide 12 by 8. That gives the 
quotient q2 = 1 and the remainder r2 = 4. Now calculate u2 
= 0 - q2 x 1 = 
- I  and v2 = I 
- q2( - 8) = 9. 
We know already that 4 is the highest common factor of 1 2  and 104 and 
also that 4 =  - I  x 1 04 + 9 x 1 2. These numbers are the entries in the R-, 
U- and V-columns of row 2, which is the last row with a non-zero R-entry. 
The final row with the zero R-entry does not need to be calculated in full, 
but before we go on, let us note in passing that the cancelled form of N4 is 
k 
That is no accident. As will be shown in Section 7.9, it is true in general 
that when rn = 0, - un/vn is the cancelled form of b/a. 
7.7 Formal definition of Euclid's algorithm 
The full rule is as follows. 
Euclid's algorithm The algorithm is performed on a table with four columns 
headed Q, R, U, and V. The rows are numbered starting at - 1. 
Input. Two non-zero elements a and b of a Euclidean domain D. 
• 
Step I. Initialization. In rows - 1  and 0, leave the Q-column empty. 
The entries in the R-, 
U-, and V-columns are a, 1, 0 in row - 1  and b, 
0, 1 in row 0. Set k = 0. 
• 
Step 2. Calculation of Q-entry. Divide rk- 1 by rk producing the quotient 
q and the remainder r: rk _ 1  = qrk + r. Put qk+ 1 = q. 
• 
Step 3. Calculation of R-, U-, and V- entries. The formulae determining 
rk + 1, uk + 1 and vk + 1 are 

1 14 
Error-correcting codes and finite fields 
It is not necessary to recalculate rk + 1 • It is the remainder r calculated 
in Step 2. 
• Step 4. Iterative test. If rk+ 1 "# 0, then increment k and return to Step 
2. If rk + 1 
= 0, the calculation is finished. 
Output. When the algorithm halts rk+ 1 = 0. 
(a) rk is a highest common factor of a and b and 
(b) rk = uka + vkb. 
As remarked above, the U- and V-entries of the final row with rk+ 1 = 0 
do not need to be calculated. 
7.8 More on Euclid's algorithm 
I hope you are convinced that Euclid's algorithm really works. However, I 
have not given formal proofs of the claims made for it. That is the purpose 
of this section. The proofs just follow the calculations and use induction to 
ensure that everything moves along as it ought to. 
Theorem Let Euclid's algorithm be performed on elements a and b of the 
Euclidean domain D. Denote the columns C!{ the tah/e hy Q, R, U, and V 
respectively, and let the entries in the kt h ro w be k, 'ik · 'i " uk and vk . Then the 
following statements hold. 
(a) 
The alyorithm terminates after a .finite numher of steps. 
(b) The last non-zero element of the R-column is a highest commonfactor of 
a and b. 
(c) For any k, rk = uka + vkb. 
Proof (a) Let \lxll denote the Euclidean valuation of x in D. From 
the calculation of qk+ 1  in Step 2 and rk+ 1 in Step 3 it follows that rk+ 1 
is the remainder of rk_ 1 on division by rk. Hence either rk+ 1 = 0 or 
llrk+ 1 \1 < l!rkll· So if the R-entry does not become 0, its value llrkll drops 
by at least I .  It follows that for rk "# 0, llrkll :::; lib II - k. Thus the algorithm 
terminates after at most !lbll + 1 iterations. 
(b1) Let the last non-zero element be r •. In this part we show that 
r. is a common factor of a and b. We prove this inductively, showing 
that for k = 1, . . . , n + 1, r. divides rn-k· 
For k = 1 we have the equation r._ 1 = qn+ 1r. + 0. So rnlr.- 1. Further­
more, for k = 2, r" _ 2 
= q"r" _ 1 + 
r". Since r. divides both summands on 
the right-hand side it divides rn-z· Suppose we have shown that rn divides 
both rn-k+ 
1 and rn-k· As '•-k- 1 = qn-k+ 1rn-k + rn-k+ 1, it follows that rn 
divides rk- t · That completes the inductive proof. The cases k = n and 

Euclid's algorithm 
1 15 
k = n + 1 state that r. divides r0 = b and r˻ 1 = a. Thus r. is a common factor 
of both a and b. 
We defer the proof that r. is a highest common factor of a and b until 
after part (c). 
(c) This part is proved by a similar induction to part (b1), but this time 
we start at the top of the table. It is clear that a = r _ 1 = 1 x a + 0 x b and 
b = r0 = 0 x a + 1 x b. Hence the statement is true for k = - 1, 0. Suppose 
that the statement is true for rk- l and rk. We shall show that it is true for 
rk+ 1 by some simple algebra: 
· 
= uk_ 1a + vk_ 1b - qk+ 1(uka + vkb) 
= (uk- 1 - qk+ luk)a + (vk- 1 - qk+ lvk)b 
= uk+ la + vk+ lb. 
That proves that rk = uka + vkb for all k. 
(b2) It remains to show that r. satisfies the second condition for a highest 
common factor of a and b. So let cia and cjb. Then 
cj(u.a + v.b) = r 
• . 
Thus r. satisfies both conditions for highest common factor and we have 
shown t ha t  1;, is a h ighest common factor of a and h. 
• 
That concludes the discussion of Euclid's algorithm. The algorithm can 
be used in any Euclidean domain, that is any domain with a size function, 
for which division with .remainder is defined. It is designed to calculate the 
highest common factor d of two elements a and b of such a domain, without 
using any form of prime factorization and it automatically produces 
coefficients u 
and v such that d = ua + vb. 
EXTRAS 
7.9 The cross-product theorem 
In the extras we first establish some slightly less immediate facts about 
Euclid's algorithm that will be needed to establish that the error-processing 
algorithm for BCH codes in Part 3 really works. Then we examine the 
connection between Euclid's algorithm and continued fractions. That is not 
used anywhere in this book, but there is an error processor for BCH codes 
using continued fractions, and the information given here will enable you to 
see that any such error processor must be equivalent to a Euclid's algorithm 
error processor. 

1 16 
Error-correcting codes and finite fields 
The information required for the proof that the error processor works is 
contained in two theorems. The first deals with the 'cross products' of 
elements in adjacent rows of Euclid's algorithm, and the second describes the 
way their norms behave: 
Theorem 
The cross-product theorem. 
Consider the four column version of 
Euclid's algorithm. Let j Á O  and let (ri_ 1, ui_ 1, vi_ 1) and (ri, ui, v) be the 
entries in the R, U, and V columns of rows j - 1 and j. Then 
(a) ri_ 1ui - riui- 1 = ± b; 
(b) ri_ 1 vi - rivi_ 1 = ± a; 
(c) ui_ 1vi - uivi_ 1 = ± 1 ; 
Example 
The entries for j = 2, in the table for Euclid's algorithm starting 
with 104 and 12, are 
8 
1 
- 8 
and 
4 
- 1  9. 
8 . - 1 - 4 .  1 = - 1 2; 8 . 9 - 4 .  - 8 = - 1 08; 1 . 9 - - 1 . - 8 = l . 
Proof The proof is by induction, starting withj = 0. For that row the values 
are a · O - b · 1 = - b; a · 1 - b · O  = a; and 1 · 1 - 0 · 0  = 1 . So the statements 
hold. Suppose now that the statements hold for j and let us calculate the 
values for j + 1, using the fact that 
= riui_ 1 - ri_1ui - riqi+ 1ui + riqi+ 1ui 
= - (ri_ 1 ui - riui_ 1) . 
Hence the value for the ru cross-product in row j + 1 is the negative of 
the value in row j. That establishes formula (a). The other formulae are 
established identically by substituting the pairs (r, v) and (u, v) for (r, u) in 
the above formula. 
Formula (c) has a useful corollary: 
Corollary 
The entries in the u and v columns have highest common factor 1. 
The fraction v/u formed from the entries in the last row of the table (in which 
the r entry is 0) is the cancelled version of - ajb. 
Proof To show that the entries have highest comment factor 1, we show 
that any common factor of ui and vi must divide 1. But that follows from 
the fact that such a factor d must divide ui_ 1  vi - uivi_ 1 = ± 1 .  

Euclid's algorithm 
For the entries in the last row we have 
0 = ua + vb. 
Hence ujv = - a/b. 
7.10 The norms of the entries in the table 
1 17 
By the very construction of the algorithm we know that the Euclidean 
valuations lir1 II must decrease strictly as each is the result .of a division with 
remainder. We shall now establish that for the entries in the U 
and V columns 
the opposite holds. Their values increase. We sha11 show this only for 
polynomials, as a general proof requires more information about Euclidean 
valuations than we have at our disposal. 
Theorem In the four-column version of Euclid's algorithm j<Jr polynomials 
over a field F (a) the degrees of the entries in the R-column decrease strictly 
from row 0 onwards; 
(b) the degrees of the entries in the U- and V-columns 
increase strictly from 
row 1 onwards. 
Proof (a) For j  0, r1+ 1 is the remainder when r1_ 1  is divided by r1. So its 
degree is less than that of r1 by definition. 
(b) The U- and V-entries in rows 0 and 1 are (0, 1) and (1, -q1 
# 0). 
So 
we have deg(u 1)  deg(u0) and deg(v1)  deg(v0). We shal show that 
proved deg(uj)  deg(u1 _1), 
and deg(r) 
< deg(1j_ 1), 
then deg(u1+ 1) 
> deg(uJ 
The ass.umption that deg(r1+ 1) < deg(r) 
implies that deg(q1+ 
1) 
> 0. Hence 
deg( -q1+1u1) 
> deg(ui) Ͼ deg(u1_ 1). 
Then 
deg( - q1 +1u1 + u1_1) = deg( -q1 +1u1) 
> deg(u1) 
as required. The proof for the V-entries is identical. 
7.1 1  Continued fractions 
A continued fraction is an expression of the form 1 
at+----
a2+---1 
a3+--1 ... +-an 
•• 

1 1 8 
Error-correcting codes and finite fields 
Continued fractions are closely related to the entries in the table for 
Euclid's algorithm, as you shall shortly see. First we give a formal definition 
of a continued fraction: 
Definition 
The continued fraction (a1, .
.
•
 , an), where a1, . . .  , an are elements 
of a Euclidean domain, is defined inductively. If n = 1, the value is a 1 ,  
otherwise it is al + 1/(a2, . . .  ' an). 
That is not the whole story. Zero values can cause difficulties which we 
shall ignore. More importantly, most of the theory of continued fractions is 
concerned with infinite continued fractions, but an investigation of these 
would lead us too far afield. The interested reader is referred to the classic 
book by Hardy and Wright (1938) or the beautiful little book by H. 
Davenport (1952). 
· 
It is obvious that the later terms of a continued fraction contribute ever 
smaller amounts to its total value. So we can consider what happens if we 
leave them off. 
Definition 
The continued fraction (a1, . • • , am) with m ::; n is called the mth 
convergent 
of (al> . . .  , an). 
Now we come to the startling relation between Euclid's algorithm and 
continued fractions. 
Theorem Let the four column form of Euclid's algorithm be applied to the 
elements a and b (a, b '/= 0), and let the algorithm terminate with rn 
= 0. Then 
(a) ajb = (q1, .
.
•
 , q.); 
(b) Vm/Um = - (ql• · · · • qm). 
Examples 
Using the table for 104 and 1 2, check that 
104/12 = 26/3 = 8 + 1/{ l  + 1/2). 
Furthermore, 9/1 = 8 + 1/1 . 
Remark 
In number theory it is shown that the convergents to a continued 
fraction are the closest approximations possible when the denominator is 
restricted in size. Thus (rather trivially) 9 is the closest integer (denominator 
1) to 26/3. 
7.12 A lemma on continued fractions 
The proof of this theorem involves an investigation of the algebra of 
continued fractions which is due to the great Swiss mathematician Leonhard 

Euclid's algorithm 
1 19 
Euler. We break the argument into two lemmas. These concern a sequence 
offunctions wn of n variables. For n = 0 we define w0 = 1. For n = 1 we define 
w1(x 1) = x 1 •  In general for n ;:;?;  2, 
Proof This is certainly true for n = 1, and for n = 2 we get 
(a1, a2) = (ata2. + 1)/az. 
which is also correct. The induction step is straightforward: 
(al, . . . ' an) = al + 1/(az, . . .  ' an) 
= al + l:Vn-2(a3, • · · ' an)/Wn - l(a2, • · 
• • an) 
= (a1 Wn - l(a2, . . • ' an) + Wn _ z{a3, . . .  , an))/Wn - l(az, • . . ' an) 
7.13 A second lemma 
• 
The next stage statement is a remarkable formula for wn: We begin by 
defining an admissible product of the terms a1, • • •  , an. This is also done 
recursively. The product of all terms is admissible, and if an admissible 
product contains aiai+ 1 ,  then the product obtained by removing this pair is 
also admissible; if this results in removing all terms, we set the product equal 
to 1. Thus the admissible products of the terms a, b, c, d are abed, cd, ad, ab, 
and 1, while the 'admissible products of a, b, c, d, e are abcde, cde, ade, abe. 
abc, a, b, c, d, and e. 
Lemma Euler's formula. The function wn(a1, . . .  , an) is the sum of all 
admissible products of the terms a1, • • .  , an, each product taken once only. 
Proof This is true for w0 and w1• Suppose it is true for Wn - l  and wn - z· 
Then wn{x 1, • . .  , Xn) = x 1 wn _ 1(x2, . •  
· .
,
 Xn) + Wn _ 2(x3, . . .  , Xn). 
But this is x 1  x the sum of admissible products of x2, •
•
• , xn + the 
admissible products of x3, • • •  , Xn. The first sum consists of the admissible 
products of x 1, .
•
•
 , xn in which x1 has not been removed. The second consists 
of the admissible products of x1, • • .  , xn, in which x1x2 has been removed. 
But together these cover all admissible products. 
Proof Euler's formula is symmetric. 

120 
Error-correcting codes and finite fields 
7.14 Proof of Theorem 7.1 1 
Now we are ready to prove the theorem. 
Lemma Vm = ( - l)mwm(ql, . . .  , qm), 
while 
Um = ( - l)m- lwm- 1 (q2, . . .  , qm). 
Proof This is true for v0 = 1 and v1 = - q1, 
and also true for u1 = 1, and 
u2 = - q2• Now if it is true for m and m - 1, then 
vm + 1 = - qm + 1vm + vm- 1 
= ( - l)m +  1qm + 1 wm(q1, . . .  ' qm) + ( ..:. l)m - 1Wm- 1(q1, . . .  , qm- 1) 
= ( - l)m+ 1 (qm +  1 wm(qm, . . .  , q1) + wm - 1(qm - 1> . . .  ' q1)) 
= ( - l)m +  1Wm+ 1(qm + 1• · · ·
'
 q1) 
= ( - l)m +  1Wm+ l(q1, · · · • qm + !) . 
The proof for um is the same. 
Proof of Theorem 7. 1 1  By Lemma 7. 14 
• 
By Lemma 7. 1 2  this fraction is just the mth convergent to (q1, •
•
• , qn)· 
• 
7.15 Summary 
This chapter was devoted to Euclid's algorithm, the basic technique required 
for Parts 2 and 3 of this book. We showed how the algorithm provides an 
efficient means of calculating the highest common factor of two elements 
without having to find their prime factorizations first. We extended the 
algorithm to a tabular form in which the additional columns give factors u 
and v, such that the calculated highest common factor of a and b has the 
form ua + vb. 
In the 'extra' sections we proved the important cross product theorem 
giving relations between the entries of the table in adjacent rows, and finally 
we discussed the connections between the algorithm and continued fractions. 
7.16 Exercises 
7. 1 
Calculate the highest common factor of 1 00 006 561 and 7 234 5 1 7. 

Euclid's algorithm 
121 
7.2 Calculate the highest common factors of the binary polynomials 
1001001001 and 101010101. 
7.3 Find the inverses of 79 and 90 modulo 787 by using Euclid's algorithm 
to express 1 as 79u + 787v and 90u' + 787v'. 
7.4 Calculate the inverses of 5 = 0101 and 7 = 01 11 using Euclid's algorithm 
in GF(16). 
7.5 Show that if a = cb in a Euclidean domain, then the only possible 
form for division with remainder is q = c, r = 0. 
7.6 Show that for Z, the ring of integers, axiom HCF2 can be replaced by 
HCF2' If cia and clb, then lei Ç ldl. 
and that this gives the same HCF. 
7.7 Show that for any Euclidean domain with valuation llall, axiom HCF2 
can be replaced by 
HCF2' If cia and clb, then llcll Ç lid II. 
7.8 Show that for a Euclidean domain D, the element 1 has the minimal 
value. That is, for any a #- 0, 11 1 11 Ç II a 11. 
7.9 Show that for an element a #- 0 of a Euclidean domain D the equation 
ab = 1 has a solution if and only ifl all = 11 1 11. 

8 
Invertible and irreducible elements 
There are two special classes of elements in a Euclidean domain D that are 
in a sense diametrical opposites. The first class consists of those elements 
that have inverses in D. These elements, called invertible, are too nice for 
division with remainder to be of any use. They also have a way of slipping 
in and out of expressions involving products, because if a is invertible and 
ab = I, then for any elements x and y, xy = (xa)(by). For this reason highest 
common factors and prime factorization are unique only up to multiplication 
by invertible elements. . 
The only invertible elements of Z are 1 and 
- 1 .  So for integers 
multiplication by an invertible element amounts at most to a sign change. 
That is not very noticeable, which explains why the problems caused by 
invertible elements are not discussed in school. For polynomials over a field, 
the effects of invertible elements are more pronounced, because in that case 
the invertible polynomials are just the non-zero constants. So many unique­
ness results for polynomials are 'up to multiplication by a constant'. 
The second, more interesting special class of elements corresponds to prime 
numbers. These are non-invertible elements that have no non-trivial fac­
torizations. In general, such elements are called irreducible. As you could see 
in Chapter 6, irreducible polynomials play a key role in the construction of 
finite fields. Indeed, not only do they appear in the construction, but almost 
any calculation in a finite field will involve such a polynomial explicitly or 
implicitly. So it is important to establish their properties. The most important 
of these is the fact that (in a Euclidean domain) if ࡄn irreducible divides a 
product it divides one of the factors. That 'key property' is proved by an 
elegant technique that I call the ' 1-trick '. 
In the extra sections 8.9 onwards, the key property of irreducibles is used 
to prove that in Euclidean domains all non-invertible elements have unique 
factorizations into irreducibles. That fact can be taken on faith if you wish, 
because it is only needed to verify that certain natural calculations always 
produce the right answers. However, the techniques of the first part 
are important and should be mastered. As usual, the examples will be drawn 
from the integers (and occasionally polynomials), but the proofs will be given 
for Euclidean domains. 

Invertible and irreducible elements 
123 
8.1 Invertible elements 
First we discuss invertible elements. Recall from Chapter 2, that highest 
common factors of a single pair of elements a and b are not quite unique. 
In Z they may differ in sign. Thus the possible highest common factors of 
12 and 104 are 4 and - 4. It is natural to exclude the negative possibility 
and declare 4 to be the only highest common factor. However, in the case 
of polynomials over fields highest common factors are only determined up 
to multiplication by a constant, and the choice of a 'best' highest common 
factor is not so obvious. 
Example In the last chapter we used Euclid's algorithm to calculate the 
highest common factor of 
f(x) = 2x3 - x2 = (2x - 1)x2 
and 
g(x) = 4x2 - 4x + 1 = (2x - 1)2 • 
The algorithm gives tx -i 
as the HCF, but that is not a particularly good 
choice. Perhaps the 'nicest' HCF would be 2x - 1, because that has integer 
coefficients, bat such an HCF with integer coefficients may not always 
exist. A good universal choice is to make the highest coefficient of the HCF 
equal to 1. That gives x -t 
as the 'normalized' HCF of 2x3 - x2 and 
4x2 - 4x + 1. 
Division with remainder is unique for polynomials and so Euclid's 
algorithm cannot simply be modified to produce a normalized HCF. To put 
the HCF into a desired form we may have to multiply the result of the 
algorithm by a suitable constant. In this case (!x -;t) 
x 2 produces x - t. 
As the example shows, an attempt to define highest common factors so 
that they are unique may have the effect that Euclid's algorithm calculates 
the wrong value. Instead, it is better to leave the ambiguity and measure 
how far highest common factors can differ. A moment's thought establishes 
that two highest common factors d and d' of the same elements a and b 
must each divide the other. Now if d = xd' and d' = yd, then xy = 1. 
Elements that multiply to 1 are called inverses, and if x has an inverse y, x 
is called invertible. 
Definition An element x of a Euclidean domain D is called invertible if xj 1, 
or in other words, if there exists y E D such that xy = 1. 
If x is invertible then x divides every element a of D, because from xy = 1 
it follows that a = x(ya). Thus for invertible elements the whole of the theory 
of division with remainder collapses into triviality-all remainders are zero. 

124 
Error-correcting codes and finite fields 
8.2 More on HCFs 
The following proposition gives a formal description of the variability of 
highest common factors. 
Proposition Let a and b be non-zero elements of a domain D. If d = (a, b) is 
a highest common factor of a and b, then the complete set of highest common 
factors of a and b is the set of products dx, where x runs through all invertible 
elements of D. 
Proof Let x be invertible. We want to show that dx satisfies HCFl and 
HCF2 (see Section 7.3). 
HCFl. Let a =  dy and b = dz. Then a =  dxx- 1y and b = dxx- 1
z. Thus dx 
divides both a and b. 
HCF2. Let r be a common factor of a and b. Then, since d satisfies HCF2, 
it follows that r!d. So d = ru, for some u. Hence dx = rux and r 
divides dx. Therefore dx satisfies HCF2. 
Conversely, suppose that d' satisfies both HCF I and HCF2. As d satisfies 
HCFl by assumption, it is a common factor of a and b and so since d' 
satisfies HCF2, it follows that d' = dx for some x. By symmetry, d = d'x' for 
some x'. Hence d' = d'x'x, and as D is a domain it follows that x'x = 1. Thus 
d' = dx, where x is invertible. 
• 
8.3 Invertibility and llx II 
It is useful to relate the property that x is invertible to the Euclidean 
valuation llx 11. The rule is easy to guess by looking at examples. The only 
invertible elements of Z are ± 1 and these are also the only elements with 
!xl = 1. Similarly, the invertible elements of the set of polynomials F [x] over 
the field F are the constants and thse are precisely the elements of degree 0. 
That suggests the following lemma, which will be useful later in the chapter. 
Lemma Let D be a Euclidean domain with valuation !!x ll. Then thefollowing 
statements hold: 
(a) 
llxll ;;:: II 1 11 for all non-zero x E D. 
(b) 
llxll = IIlii 
if and only if x  is invertible. 
Proof Statement (a) is an immediate consequence of axiom EVl: 
llx U  = Il l  x x ll ၙ 11 1 11 .  

Invertible and irreducible elements 
125 
To prove statement (b) suppose that II x II = 11 1 11. Divide 1 by x with 
quotient q and remainder r: 1 = qx + r. By definition llrll < llx ll, or r = 0. 
By part (a), there is no non-zero r E D, for which llrll < llxll. Hence r = 0. 
But then it follows that 1 = qx and so x is invertible. 
• 
8.4 Relative primeness 
Now we turn to the consideration of irreducible elements. It is easiest to 
begin, not with the concept of a prime or irreducible element, but with a 
description of the situation when two elements have no common prime 
factors. This can be easily calculated because it is equivalent to the statement 
that the highest common factor of the two elements in question is 1. That 
can be determined by Euclid's algorithm. Algorithms for testing for prime­
ness are much more subtle, and completely factorizing large integers, let alone 
polynomials, is computationally so difficult that it has become a kind of 
sport played by computer buffs. Occasionally you read about a newly 
discovered large prime or a 'fast' factorization in the press. 
Definition 
Two elements a and b of a Euclidean domain D are said to be 
relatively prime if 1 = (a, b). 
Because of the non-uniqueness of highest common factors you must allow 
multiplication by invertible elements. If 1 is a highest common factor of a 
and b then by the discussion above the full set of possible highest common 
factors of a and b consists of the invertible elements of D. 
Proposition Let a and b be non-zero elements of a Euclidean domain. Then 
each of the following statements implies both the others. 
(i) The elements a and b are relatively prime. 
(ii) There exists a highest common factor d of a and b, such that d is invertible. 
(iii) Every highest common factor d of a and b is invertible. 
Remark 
In particular, two polynomials over a field are relatively prime if 
and only if the highest common factor calculated by Euclid's algorithm is a 
constant. 
Proof (i) implies (ii). By assumption 1 is a highest common factor of a and 
b. Certainly, 1 is invertible. So (ii) holds. 
(ii) implies (iii). By assumption there exists d = (a, b) such that d is 
invertible, say de = 1. Let d' be any highest common factor of a and b. By 
Proposition 1, d' = dx where x is invertible, say xy = 1. Then d'ey = 
dxey 
= 1. Thus d' 
is invertible. 

126 
Error-correcting codes and finite fields 
(iii) implies (i). Let d be any highest common factor of a and b. By 
assumption d has an inverse e. Now e is an invertible element (with inverse 
d). Hence by Proposition 8.2, de = 1 is a highest common factor of a and 
b. Thus (i) holds. 
8.5 The ' 1-trick' 
It is a little ironic that the most important application of Euclid's algorithm 
is the case when the two initial numbers a and b have no common prime 
factors. That is because when two numbers have highest common factor 1 ,  we 
can use the auxiliary columns of Euclid's algorithm to write down an 
equation linking them with 1 .  This is the first stage of an ingenious technique 
I call the ' !-trick'. 
Theorem Representability of 1. Let a and b be non-zero elements of a 
Euclidean domain D; then a and b are relatively prime if and only if there are u 
and v in D such that ua + vb = I .  
Examples Suppose that a and b are numbers with a common prime factor, 
say 2, for example, a = 4, b = 6. Then whatever whole numbers we choose 
for u and v, ua + vb will be even. In general, any common prime factor of 
a and b must always divide all numbers of the form ua + vb. In that case 
ua + vb cannot be ever equal to 1. 
The same argument works for polynomials. If f(x) and g(x) have an 
irreducible factor h(x) in common, then it must divide u(x)f(x) + v(x)g(x), 
for any polynomials u(x) and v(x). So u(x)f(x) + v(x)g(x) can never be 1 .  
What about the converse? That is not so easy to see from prime 
factorization. For instance, is it obvious from prime factorization that there 
are whole numbers such that u x 49 + v x I I  = I (answer u = - 2, v = 9)? 
But of course, this drops out of Euclid's algorithm. That is fortunate, because 
we have not defined prime factors and so the arguments given here have a 
somewhat wobbly base. 
Proof Suppose a and b are relatively prime and suppose d is the HCF of 
a and b calculated by Euclid's algorithm, Then firstly d = u'a + v'b for some 
u' and v' in D. Secondly, d is invertible with, say, inverse, e. Putting u = u'e 
and v = v'e we get 
I = de = u' ea + v' eb = ua + vb. 
Conversely suppose there exist u and v in D such that ua + vb = 1. Then 
let d 
be the highest common factor of a and b. 
Since d 
divides both a and 

Invertible and irreducible elements 
127 
b, it also divides ua + vb = 1. Hence there exists e in D such that 1 = de. 
Thus d is invertible and a and b are relatively prime. 
• 
8.6 Irreducibility 
The equation of Theorem 8.5 almost always occurs when prime numbers or 
irreducible polynomials appear on the scene. So now is the time to define 
irreducibilǓty formally. 
Definition We call x =1: 0 irreducible in D if 
(a) x does not have an inverse in D, and 
(b) whenever x is written as yz, one of y and z has an inverse in D. 
Examples For Z irreducibles are just the ordinary prime numbers 2, 3, 5, 
7, 1 1, . . .  , but the number 1 is not regarded as irreducible, because it fails 
test (a) of the definition. 
For polynomials over a field, the invertible elements are the non-zero 
constants. Other polynomials cannot have inverses because multiplying a 
non-zero polynomial f(x) by a non-constant polynomial g(x) will increase 
the degree of f(x). Thus it will produce a non-constant answer. · 
Every linear polynomial x + a is irreducible, but these irreducibles are not 
particularly useful. 
Polynomials of degree 2 or 3 can only split into products involving at 
least one polynomial of degree 1. Factors of f(x) that have degree 1 
correspond to roots of f(x) (as will be shown in Chapter 1 1). Hence a 
polynomial of degree 2 or 3 is irreducible if it has no roots. Thus, for instance, 
x2 - 2x + 2 is irreducible over the real numbers. For finite fields searching 
for roots is easy (unless the field is very large). 
Unfortunately this simple criterion fails for degree 4 onwards. For instance, 
the real polynomial x4 - 4x3 + 8x2 - 8x + 4 has no real roots, but it is not 
irreducible, because it factors as (x2 - 2x + 2)2• Small irreducibles can be 
found by a systematic search called the Sieve of Eratosthenes (see Exercise 
8.6). Finding large irreducibles is not easy, even in the integers, but there are 
moderately efficient algorithms that can find large prime numbers. For­
tunately, constructing a finite field requires only one irreducible, so a search 
algorithm is usually feasible. 
The following lemma links the two concepts of irreducibility and relative 
primeness. The idea of irreducibility is more important, but relative prime­
ness leads to the 1-trick. 
Lemma Let a be an irreducible element in the Euclidean domain D. Thenfor 

128 
Error-correcting codes and finite fields 
any x E D  exactly one of the two following statements holds. Either 
(a) a divides x exactly, or 
(b) a and x are relatively prime. 
Proof Consider d = (a, x). We have a = de and x = df If d is invertible then 
a and x are relatively prime. On the other hand, if d is not invertible, then 
by the irreducibility of a, e has an inverse. So x = ae - 1f 
Thus aix. 
Hence 
one of (a) or (b) must hold. 
If a l x, then a = (a, x) and by hypothesis a is not invertible. So a and x 
are not relatively prime. Thus the statements cannot both be true. 
• 
8.7 The Key Property of irreducible elements 
We can now establish the basic property of irreducibles using the I -trick. It 
is the well-known but rarely proved fact that if a prime divides a product it 
divides one of the factors. At school that is usually deduced from the fact 
that numbers have unique prime factorizations. However, equally often, and 
often in the same class, uniqueness of prime factorization is deduced from 
the fact that if a prime divides a product it divides one of the factors. Now 
it is certainly true that these two statements do imply each other, but that 
does not constitute a proof of either of them. 
Theorem 
The Key Property of irreducible elements in Euclidean domains. 
Let 
a, b, and 
c be elements of a Euclidean domain and suppose that a is 
irreducible. Then if 
a divides be, 
then a divides b or a divides c. 
Warning 
It is quite common in abstract arguments to assume this is true 
for any element a, but that is incorrect. For instance, 4 divides 12 and 
12 = .2 x 6 but 4 does not divide 2 or 6. 
Proof Suppose that a does not divide b. Then by Lemma 8.6, a and b are 
relatively prime. Hence by Theorem 8.5, there exist u and v in D such that 
1 = ua + vb. 
This is where the 1-trick occurs. Since we have an equation with 1 on the 
left-hand side, we can multiply it by any element we are interested in, and 
then that element will be on the left-hand side. In this case the element we 
choose is c. 
c = uac + vbc. 
Now, a 
obviously divides uac, and it divides vbc, because by hypothesis it 
divides be. Hence a 
divides the sum uac + vbc 
= c. 
• 

Invertible and irreducible elements 
129 
The simple manipulation used in the proof constitutes the 1-trick. It is not 
an accident that the proof implicitly uses Euclid's algorithm, because in more 
general domains the theorem may not be true (see Exercise 8.12). Fortuna­
tely, all the domains that are of interest in this part are Euclidean. 
Example 
Suppose that 11 divides 49x. We can deduce that 1 1  divides x 
following the above proof: 
1 = 9 X 
1 1  - 2 X 
49 
Hence 
X = 9 X 
l lX - 2 X 
49x. 
1 1  divides 1 lx obviously. It divides 49x by hypothesis. Hence 11 divides 
99x - 98x = x. 
It is occasionally useful to apply the 1-trick when we do not know that 
a is prime, but are able to establish that a and b are relatively prime. In that 
case the proof of the theorem goes through without the need to invoke 
Theorem 8.5. 
Corollary Let a, b, and c be elements of a Euclidean domain, such that a and 
b are relatively prime. If a divides be then it divides c. 
• 
Example 6 ano 55 are not prime, but they are relatively prime 
(55 - 9 X 
6 = 1). 
Hence if 6 divides 55x, it follows that 6 divides x. 
8.8 LCM of relatively prime elements 
The next proposition, also proved by the 1-trick, expresses the fact that for 
relatively prime elements the least common multiple is the product. 
Proposition Let a, b and c be elements of a Euclidean domain such that a 
and b are relatively prime. If both a and b divide c, then ab divides c. 
Example If 4 and 9 both divide x, then 4 x 9 = 36 divides x. On the other 
hand, if a and b are not relatively prime, then the conclusion may not hold. 
For example, both 4 and 6 divide 12, but 4 x 6 = 24 does not divide 12. 

130 
Error-correcting codes and finite fields 
Proof By Theorem 8.5, 
1 = ua + vb. 
Hence c = uac + vbc. 
Now, a divides ua and, by hypothesis b divides c. Therefore ab divides 
uac. Similarly, b divides vb and a divides c. So ab divides vbc. Hence ab 
divides both summands on the right-hand side. Thus it divides the left-hand 
ၘ 
. 
. 
The most common application of this Proposition is when a and b are 
distinct irreducibles. 
EXTRAS 
8.9 Irreducibility and norm 
These are the basic facts about irreducibles. The chapter concludes with a 
proof of unique factorization in Euclidean domains. The first stage of the 
proof is to show that every element can be written as a product of 
irreducibles, without worrying about uniqueness. The proof is, in a sense, 
constructive, but the algorithm it leads to is roughly 'look for irreducible 
factors'. Yet no significantly better method of factoring a number or 
polynomial is known. Incidentally, that is the basis for the security of the 
famous RSA public key cryptographic system. However, it is also not known 
whether a better factorization method exists. It is conceivable that there is 
a quick factorizing method that has eluded generations of mathematicians. 
For the proof we shall need some information linking the property of 
irreducibility and the Euclidean valuation which we state in the following 
lemma. 
Lemma Let a be a non-zero, non-invertible element of a Euclidean domain 
D with Euclidean valuation l!x 11. Then a is irreducible if 
and only if 
for any 
factorization a =  be, llall = llbll or llall = llcll. 
The lemma says that if an element is neither irreducible nor invertible, 
then it can be split into a product of terms of strictly smaller size. That will 
allow us to use induction on the size. 
Proof Suppose a is irreducible and a = be. Then one of ba and c is 
invertible. Say b has inverse d. Then da = c. From a = be it follows that 
l!al! ;;:.: l!cl!, 
and from da = c it follows that llall ř llcll. Hence l!all = llcll. 
Conversely, suppose a is not irreducible and say a = be with neither b nor 

Invertible and irreducible elements 
1 3 1  
c invertible. Then neither b nor c is a multiple of a. We shall show that 
llbll < Hall and llcll < llall. For suppose llb ll ;;:;: llall. Then b = qa + r and 
r #- 0, because if a divides b, then c is invertible. Hence llr ll < llall and so 
llrll < ll bll .  On the other hand, r = b - qa = b(i - qc). Hence llrll ;;:;: llbll . 
That contradiction invalidates the assumption that llbll ;;:;: llall. Thus llbll < 
Hall. By the same argument, llcll < llall, establishing the claim. 
• 
A 
consequence of the same lemma gives us the induction start. It states 
that among the non-invertible elements, those of smallest size are irreducible. 
In particular, it proves that 2 is a prime number and also that every linear 
polynomial is irreducible. 
Corollary If a is an element with II 
all minimal subject to II 
all > IIIII, 
then a 
is irreducible. 
Proof We prove that if a is not irreducible then llall is not minimal subject 
to !I all > II 1 11. If a is not irreducible, then either a is invertible and so (by 
Lemma 8.3) Hall = IIIII, 
or a = be with neither b 
nor c invertible. From the 
proof of the lemma it then follows that lib II 
< llall. As b is not invertible it 
follows from Lemma 8.3 that IIIII 
< llb ll < Hall. Hence Hall is not minimal 
subject to llall > 11 1 11. 
• 
8.10 Prime factorization: existence 
The proof of the.existence of prime factorizations (but not their uniqueness) 
is now straightforward. 
Theorem Let a be a non-zero, non-invertible element of a Euclidean domain 
D. Then a can be written as a product of irreducible elements of D. 
Remark It does not make sense to try and factorize 0 or 1 or indeed any 
element that divides 1. That is the reason for the exclusion of invertible 
elements. We must also allow products consisting of a single term. Otherwise 
irreducible elements like the prime 7 in Z do not have factorizations. 
Proof The proof is by induction on the Euclidean valuation II all in D. 
If a is irreducible, then we use the one term product a. By Corollary 8.9 
that implies that the theorem is true for a of minimal size II 
all > IIlii. 
If a is not irreducible, then there exists a factorization a =  be with ll bll, 
llcll < llall. By induction hypothesis, b and c 
can be written as products of 

132 
Error-correcting codes and finite fields 
irreducible elements, say 
b = Pl X,,. X Pm> 
C = ql X,,. X 
qn. 
Then 
a = Pl X. 
0
.
 X Pm X ql X 
• • • X q. 
is a representation of a as a product of irreducibles. 
8.11 Prime factorization: uniqueness 
• 
Now that we have established that in a Euclidean domain every non­
invertible element has a factorization into irreducibles, it remains to show 
that the factorization is unique. It is very easy to believe that it must always 
be the case that if an element has a factorization into irreducibles, the 
factorization must be unique, but that is not so. In the exercises there is an 
example to show that for more general domains it may be possible for 
an element to have two quite distinct factorizations into irreducibles. Such 
an unjustified tacit assumption of unique factorization is a common flaw in 
many incorrect 'proofs' of the famous (unproved) conjecture known as 
Fermat's last theorem . 
Example If you require all factorizations to be absolutely identical, then 
unique factorization never holds, because, for example, 
6 = 2 X 
3 = 3 X 2 = ( - 2) X ( 
- 3) = ( - 3) X ( -2). 
Or, for real polynomials, 
4x2-4x 
+ 1 = (tx-i)(8x-4) 
= (2x - 1)(2x
- 1) = · · · 
It is clear that we must allow rearrangements of the factors and also 
multiplications by invertible elements. Subject to these modifications, fac­
torization in Euclidean domains is indeed unique. 
Theorem 
Let a be a non-zero, non-invertible element of a Euclidean domain 
and let 
a = P 1 X 
• 
, 
• X Pm 
= q 1 X 
.
.
• X 
q. 
be two factorizations of a into irreducible elements. Then m = n and the 
products can be ordered in such a way that 
i = 1, .
.
.
 , n, 
(1) 
with di 
invertible. 

Invertible and irreducible elements 
1 33 
Proof The proof is by induction on m. First notice that a is irreducible if 
and only if m = 1, because if m > 1, then the factorization a =  p1b, where 
b = p2 x 
· 
· 
· x Pm has two non-invertible terms. Of course, the same argu­
ment works for the product of the qs. Hence m = 1 if and only if n = 1 and 
in that case a =  Pt = q t .  
For the induction step, assume that the theorem holds for elements with a 
factorization into m - 1 irreducibles. We first show that the products can 
be arranged so that (1) holds for i = 1. By hypothesis, p1 divides the product 
q1 x 
· 
· 
· x qm . Hence by Theorem 8.7 (the key property of irreducibles), p1 
divides one of the factors q;. Rearrange the product so that the index of that 
factor is 1. Then q1 = p1d1• Since q1 is irreducible one of p1 and d1 must be 
invertible, and as p1 is also irreducible the invertible term must be d1. 
We can write the factorizations as 
Pt X P2 X 
• • · X Pm = Pt X dl X q; X • · • X qn . 
Cancel p1 on both sides of the equation and replace q2 by qʌ = d1q2• Then 
let 
b = P2 X 
· • ·
X
 Pm = qԖ X 
• 
• 
• X q 
• .  
This left-hand product has m - 1 terins. So we can apply the induction 
hypothesis to b. That tells us that m = n and we can rearrange the ps so that 
q; = p;d; for i = 3, .
. . , m, 
and 
Since d1 is invertible we can define 
d2 = d܌d 1 1 
which has inve;se d1d܍- 1• Then p2d2 = p2dʌd 1 1 = qʌd 1 1 = q2• We already 
know that q1 = p1d1• So this proves (1) for all i = 1, . . .  , m. 
• 
8.12 Summary 
With hindsight it is apparent that this chapter has described a theory of 
divisibility of Euclidean domains. We ask how can non-zero elements of a 
Euclidean domain split into products? Irreducible and invertible elements 
cannot be split except in trivial ways. Every other element factorizes into 
irreducibles and the factorization is unique up to order and multiplication 
by invertible elements. The factorization completely determines all the 
possible products the element can split into. 
Because there is no efficient way of computing factorizations, they are of 
little practical use. Instead we use Euclid's algorithm. In this chapter that 
occurs implicitly by means of the 1 -trick and Theorem 8.5 on representing 1 .  

1 34 
Error-correcting codes and finite fields 
In particular, it implies that if an irreducible element of a Euclidean domain 
divides a product, then it divides one of the factors. That is a key result for 
the construction of finite fields. 
Exercises 8 
8.1 
Prove that if a and b are invertible elements of a Euclidean domain, 
then so is ab. 
8.2 Prove that if a is an invertible element of any ring, then the cancellation 
law holds for a: ab = ac implies b = c. 
8.3 Prove the statements made in Section 8.1 about the invertible elements 
of Z and .F [x]. 
The invertible elements of Z are ± 1 .  
The invertible elements of F[x] are the non-zero constants. 
8.4 Let D be a Euclidean domain such that for all non-zero a, b, ll a ll = !l bl/. 
Show that D is a field. 
8.5 
The Sieve of Eratosthenes. To find all the prime numbers up to I 00 
write down all the numbers from I to 100. Cross out I. Then repeat 
the following steps until all numbers are circled or crossed out. Circle 
the smallest number not yet marked (the first time this will be 2). Cross 
out all multiples of the number you have just circled (the first time that 
will be all even numbers from 4 to 1 00). Prove that when you have 
finished the circled numbers are the primes. Extend this method to find 
the prime numbers up to any n. 
8.6 Adapt the Sieve of Eratosthenes to F[x], where F is a finite field. Use 
it to find all irreducible binary polynomials of degree at most 6. What 
problem do you encounter if F is not finite? 
In the remaining questions we consider the set Z[.J - 3], which consists 
of all complex numbers of the form a + b.J - 3 with a, b E Z. 
8.7 Prove that Z[.J - 3] forms an integral domain (the most important 
axioms to check are the closure axioms). 
8.8 For a, b E Z, define 1/a + b.J - 3 11 = a2 + 3b2• Show that for 
X, y E Z [.J - 3] ,  
1/xyll = 1/x!I II YII (Hint: lla + b.J - 3 11 = (a + b.J - 3)(a - b.J - 3)). 
8.9 Show that x E Z [.J - 3] is invertible if and only if llx II = 1. Write down 
all the invertible elements of D. 
8.10 Show that for x E Z [ .J - 3], llx II = 4 implies that x is irreducible (the 
condition is sufficient, but not necessary). 
Deduce that 2, I + .J - 3, 
and 1 - J- 3 are all irreducible. 

Invertible and irreducible elements 
135 
8.1 1  Show that in Z[.J - 3], 4 = 2 x 2 and 4 = (1 
+ .J - 3)(1 - .J - 3) are 
two different prime factorizations of the same element. 
8.12 Show that in Z[.J - 3] it is possible for an irreducible element to divide 
a product without dividing any factor. 
8.13 Show that there exist elements a, b in Z[.J -3], such that for all 
q E Z[.J -3], 
a =  qb + r implies llrll ;?: llbll. 

9 
The construction of fields 
This chapter will show you how to use a Euclidean domain D to construct 
a field. The most important case is when D is the set of polynomials over a 
small field that you already know. The construction is used in algebra to 
analyse the solutions of algebraic equations, but that is not our main purpose. 
For us, the important fact is that if the little field we start with is finite, the 
result will be another larger finite field. Later it will turn out that all possible 
finite fields can be constructed by this method. 
The idea is the one that was used to construct the field G F( 16) in Chapter 6: 
find a suitable element a and use the set D/a of remainders that are left 
when the elements of D are divided by a. From the examples given in 
Chapter 6 you can see that not every element will produce a field, and you 
can guess that the extra condition that it will have to satisfy is irreducibility. 
Applying the construction to the integers Z gives fields of prime order Z/p, 
applying it to polynomials over B gives fields of order 2". 
The chapter splits into two sections. First we describe the general 
construction of D/a. Then it is shown that using irreducible elements for the 
construction produces a field. 
9.1 The factor ring 
We start with a Euclidean domain D. In practice, this may be the integers, 
but it is most often the set F [x] of polynomials over some field F that has 
already been constructed. Choose a non-zero element a in D, and denote by 
D/a the set of remainders after division by a. We shall initially establish that 
for any choice of a, the natural definitions of addition, subtraction, and 
multiplication of remainders that are derived from the operations of D work 
correctly. Division is necessarily different, because it is not defined in D itself, 
and so cannot simply be carried over. 
Construction 
The factor ring Dja. Let D be a Euclidean domain and a E D, 
a #  0. Denote by D/a (D modulo a) the set of remainders of elements of D 
when divided by a. Addition and multiplication in D/a are defined as addition 
and multiplication D followed by dividing the result by a to obtain the 
remainder. 
There are two special cases of this construction. By convention D/0 is 

The construction of fields 
137 
defined to be D itself. At the other extreme, if a has an inverse a - l  = b, then 
x = xba + 0 for all x in D. So there is only one remainder: 0. Then Dja is 
the single element set {0}. We exclude these two 'trivial' cases. 
9.2 The uniqueness assumption 
Assume for the moment that for a fixed divisor a the quotient and remainder 
for each b E D  are uniquely determined. That is the case for polynomials over 
B but not for Z. 
Uniqueness assumption Given a, b E D, a ::/= 0, if r1 and r2 satisfy 
(a) For i = 1, 2 there exists qi 
such that b = qi 
+ri and 
(b) for i = 1, 2 llrdl < !Ia!!. 
then r1 = r2• 
In Sections 9.4-9.6 it will be shown that even if the uniqueness assumption 
does not hold, it is always possible to choose quotients and remainders so 
that the arguments work, but it is easier to see how to do that, when you 
know how the uniqueness assumption enters the arguments. 
Notation We denote by 
x mod a the remainder of x on division by a, and 
x div a the corresponding quotient. 
Thus x = (x div a)a + 
(x mod a). 
When it is necessary to distinguish the addition and multiplication in the 
factor ring Dja from those in D, we shall use the symbols E9 and ® for the 
operations in D/a. Thus 
x EB y = (x + y) mod a, and 
x ® y = (xy)mod a. 
9.3 Dfa is a commutative ring 
It is easy to verify that D/a is a commutative ring. 
Theorem Let D be a Euclidean domain and a non-zero, non-invertible element 
of 
D. Then D/a is a commutative ring. 

138 
Error-correcting codes and finite fields 
The proof of the theorem consists of checking the axioms A1-A4, M 1-M4, 
and D. The hardest axioms to check are the associative laws. In the following 
proposition these and a selection of the other axioms are proved. The rest 
can be checked in the same way and are left to the reader as an exercise. 
Proposition (a) Let D be a Euclidean domain and a e D, a i= 0. Then D/a 
satisfies the associative laws: 
A1: (x $ y) $ z  = x $ (y $ z), and 
M1: (x ® y) ® z = x ® (y ® z), 
(b) The elements 0 and I act as zero and identity elements of Dja. The 
negative of an element x in Dja is obtained by taking the remainder of its 
negative -x in D after division by a. 
The proof of this proposition is easy but abstract, so here is a worked 
example. 
Example Let D = Z and a = 1 6. 
Choose x = 1 1 , y = 10 and z = 7. We check the associative law of 
multiplication modulo 16 (addition is similar but easier). 
1 1  X 
10 = 6 X 
16 + 14, 
SO 
1 1  <8) 10 = 14. 
14 X 7 = 6 X 
16 + 2, 
S O  
(1 1 <8) 10) <8) 7 = 2. 
Doing things in the other order we get: 
10 X 
7 = 4  X 
16 + 6 
and 
1 1  X 6 = 4 X 16 + 2. 
So 
1 1  ® (10 ® 7) = 1 1  ® 6  = 2. 
The answer for both multiplications are the same. To produce a general 
proof, the calculations have to be rewritten. The first calculation can be put 
in the form: 
(1 1 X 
10) X 7 = (6 X 
16 + 14) X 7 = 6 X 7 X 16 + 6 X 16 + 2. 
(1) 
The second calculation can similarly be written as 
1 1  x (10 x 7) = 1 1 x (4 x  1 6 + 6) = 1 1  x 4 x  1 6 + 4 x  1 6 + 2. (2) 
Now the left-hand sides of (1) and (2) are eq ual. It follov.·s that the remainders 
on the right hand sides 
hzn e to be eqUEiJ as well. That is the main step of 
the proof. 

The construction of fields 
Proof (a) Proof of Al. Let 
r1 = x E!3 y = (x + y) mod a 
and 
r2 = (x E!3 y) E!3 z = (r1 + z) mod a. 
Then with q1 = (x + y) div a and q2 = (r1 + z) div a, 
(q1 + q2)a + r2 = q1a + r1 + z 
= (x + y) + z. 
Hence by the uniqueness assumption, r2 = ((x + y) + z) mod a. Thus 
(x E!3 y) E!3 z = ((x + y) + z) mod a. 
Similarly 
x E!3 (y E!3 z) = (x + (y + z)) mod a. 
139 
But (x 
+ y) + z = x + (y + z) 
in D, by the associative law of addition. Hence 
(x E!3 y) E!3 z = x E!3 (y E!3 z) in D/a and the associative law of addition holds 
in D/a. 
Proof of M 1. The argument is similar. Let 
r1 = x ® y = xy mod a 
and 
r2 = (x ® y) ® z = r1z mod a. 
Then with q1 = xy div a and q2 = r1z div a, 
(q1z + q2)a + r2 = (q1a + r1)z 
= (xy)z 
Again, the uniqueness assumption implies r2 = (xy)z mod a and thus 
(x ® y) ® z = (xy)z mod a. 
As before, symmetry gives 
x ® (y ® z) = x(yz) mod a. 
Again, the associative law of multiplication in D gives (xy)z = x(yz). Hence 
the same law holds in D/a: (x ® y) ® z = x ® (y ® z). 
(b) Let x e Dja. Then x = 0 + x = 1 x x. Furthermore, since x is in D/a, 
x mod 
a =  x. Thus 0 E!3 x = 1 ® x = x. That establishes that 0 and 1 are the 
zero and identity of Dja. Now, if - x mod a 
= y, and - x div a = q, then 
x -:- y = ( - q;a -:- 0. Hence x S y = 0. Therefore y is the negative of x in D/a . 
• 

140 
Error-correcting codes and finite fields 
9.4 Remainder functions 
The next three sections consider how remainders modulo a can be made 
unique by using an appropriate selection rule. The problem to define 
operations x mod a and x div a, so that the arguments of Proposition 9.3 
always work. For integers this can be achieved by ensuring that the signs 
are consistent. So, if you wish, you can safely skip to Section 9.9. Remember 
only that in our definition of x mod a for integers, x mod a is always 
non-negative. 
We introduce selection by defining a remainder function. 
Definition Let a :f. 0 be an element of a Euclidean domain D with Euclidean 
valuation llxl!. A function x mod a defined on D is called a remainder function, 
if for all x e D, 
x = qa + (x mod a) for some q ,  
and 
x mod a = 0 
or 
l!x mod all < !!all . 
The value q in the above equation is called the corresponding quotient 
function and denoted by x div a. 
From now on the function x mod a incorporates an element of choice. 
That makes it quite possible that the arguments of Proposition 9.3 go 
wrong. Indeed designers of practical systems often forget to check for 
consistency and that produces strange results. For instance, the function a 
MOD b as implemented in most computer languages is inconsistent. As an 
example, consider the calculation of 3 - 2288 + 2279 mod 7. If you perform 
the arithmetic first and then apply the MOD function you get - 6 mod 7, 
which usually produces -6. On the other hand, many algorithms will apply 
the MOD function after each arithmetical operation to keep the numbers 
small. In that case, the result of the first addition is - 2285 mod 7 = 
- 3; so 
the result of the whole calculation is 2276 mod 7 = 1. This effect leads to 
occasional incorrect results for standard algorithms, such as the calculation 
of the date of Easter. These 'bugs' appear erratically and cause great 
difficulties because they cannot be seen in the source code of programs (for 
a discussion of the calculation of Easter see O'Beirne (1965)). Before using 
the MOD function on a computer, you should check its values for all possible 
combinations of signs of a and b, and you may have to take steps to avoid 
its inconsistencies. 

The construction of fields 
9.5 Class representatives 
141 
The arguments of Proposition 9.3 make use of uniqueness in a particular 
way. They start with a known remainder r of some element x, r = x mod a. 
Then they show that for a different y, there is an equation y = qa + r. From 
that it is de4uced that r = y mod a. To make these arguments work when 
there is a choice, our function must satisfy the following condition. 
Consistency condition A remainder function x mod a in D is said to be 
consistent if it satisfies the following condition for all x and y in D. 
(C) if x mod a =  r and there exists q such that y = qa + r, then 
y mod a =  r. 
The possibility of a consistent set of choices depends on the fact that there 
are only two ways the remainders of distinct elements x and y after division 
by a can be related. Either x and y have completely separate sets of possible 
remainders, in which case the choices for x and y are independent and cannot 
lead to conflict, or x and y have precisely the same set of remainders so that 
we can demand that the choice for x is the same as the choice for y. 
Example With the integers Z as D and 7 as the divisor a, the sets of possible 
remainders of 25, 29 and 32 are as follows. 
25: 4, - 3 
29: 1, - 6  
32: 4, - 3 .  
As you see, 25 and 3 2  have the same set of remainders, so we must make 
sure that we make the same choice in both cases. On the other hand, 29 has 
a completely separate set of remainders so the choice here can be made 
independently. 
Thus remainders on division by a fall into classes so that, any division of 
an element x by a, 
x = aq + r  
which results in one member of a given class can be modified to produce 
x = q'a + r' 
with r' 
any other member of the same class. On the other hand, it cannot 

142 
Error-correcting codes and finite fields 
be modified to produce a remainder of a different class. Thus we make 
division with remainder unique by picking a single representative remainder 
from each class. 
9.6 Interchangeability of remainders 
The following proposition gives a simple formulation that is equivalent to 
the discussion above. It says that two possible remainders are interchange­
able if and only if their difference is an exact multiple of a. 
Proposition Let D be a Euclidean domain wih Euclidean valuation /lx 11. Let 
a =!= 0, r1, and r2 be elements of D. Then the following statements hold: 
(a) If there exists x e D  such that x = q1a + r1 and x = q2a + r2, then there 
exists b e D such that ab = r1 - r2. 
(b) If there exists b e D  such that ab = r1 - r2, then for any y = q3a + r1 
there exists q4 such that y = q4a + r2• 
Example With integers Z as D and 7 as the divisor a the remainders + 4 
and - 3  are equivalent because 7 exactly divides 4 - (- 3) = 7. Any number 
that leaves remainder 4 for a quotient q will leave remainder - 3  for quotient 
q + 1. Thus 25 = 3 X 7 + 4 = 4 X 7 - 3. 
Proof (a) From the assumptions q1a + r1 
= q2a + 
r2• Hence r1 - r2 = 
(q2 - q1)a is an exact multiple of a. 
(b) If r1 - r2 = ab and y =  q3a + r1, then 
So take q4 = q3 + b. 
y = (q3 + b)a - ba + r1 
= (q3 + b)a - (r1 - r2) + r1 
= (q3 + b)a + r2• 
Corollary Divide the elements r, s of D with l!r/1, !lsi! < !Ia /I into classes by 
putting r and s in the same class if and only if a divides r - s. Then the 
proposition assures us that a remainder function will be consistent if and only 
it if chooses exactly one remainder from each class. 
Definition The classes defined in the corollary are called the restricted 
residue classes of D modulo a. The ring D/a is called the residue class ring 
of D modulo a. 

The construction of fields 
143 
The class of 0 always contains only 0 (see Exercise 9.2). We shall also 
assume that the identity 1 is always the selected representative of its class. 
You can now verify that in the proof of Theorem 9.3 the uniqueness 
assumption can be replaced by the assumption that the remainder function 
is consistent (see Exercise 9.3). 
Example Possible consistent rules for the integers are (a) always take 
x mod a non-negative, (b) always take it non-positive, (c) take the remainder 
with the smallest absolute value taking it to be negative if there is still a 
choice. The choice (c) is the one made by 2s complement arithmetic as it is 
implemented for signed integers on most computers. It would be disastrous 
if a computer used an inconsistent rule for its internal integer arithmetic. It 
is curious that while the Qasic arithmetic unit of all computers implements 
a consistent mod function, many high-level computer languages do not. We 
shall always use rule (aǒfor integers x mod a ɴ  0 for all x and a. 
9.7 Condition for a field 
It remains to put all the ingredients toge.ther and determine exactly when 
D/a is a field. It will come as no surprise to you that that is the case precisely 
when a is irreducible. As D/a is always a commutative ring, it is only 
necessary to show that for irreducible a, every non-zero remainder has an 
inverse in Dja. To see why that is so, consider an example. 
Example Let D = Z and a = 787. The number 787 is prime, hence Z/787 
should be a field. How do you find the inverse of a non-zero remainder such 
as x = 53 in Z/787? You must find a number y such that 
xy mod a = 1 
In other words if b = xy div a, you must satisfy the equation 
xy + 
ba = 1 .  
Here x and a are known, and y and b are sought. That is precisely the 
problem that the last two columns of Euclid's algorithm were designed to 
solve. Since a =  787 is prime and 0 < x = 53 < 787, it follows that 1 = (a, x). 
So applying Euclid's algorithm to a and x will produce b in the U 
-column 
and y in the V-column (see Table A, overleaf). Thus 1 = ( -20) x 787 + 
297 x 53. In other words, 53 x 297 mod 787 = 1. Thus 297 is the desired 
inverse. 
Sometimes the algorithm produces a negative answer for y. In that case 
all you need to do is replace y by y mod a. 

144 
Error-correcting codes and finite fields 
Table A 
Row 
Q 
R 
ၗ 
v 
- 1  
787 
1 
0 
0 
53 
0 
1 
1 
14 
45 
l 
- 14 
2 
1 
8 
- 1  
1 5  
3 
5 
5 
6 
- 89 
4 
1 
3 
- 7  
104 
5 
1 
2 
1 3  
- 193 
6 
1 
1 
- 20 
297 
7 
2 
0 
53 
-787 
9.8 Proof of the condition 
The proof that D/a is a field when a is irreducible just follows the steps of 
the calculation above, but it replaces the direct use of Euclid's algorithm by 
Theorem 8.5 that states that if a and x are relatively prime, then there exist 
u and v such that 1 
= ua + vx. 
Theorem 
Let D be a Euclidean domain with Euclidean valuation llx ll. Further, 
let a be a non-zero element of D without an inverse. 
(a) If a is not irreducible, then D/a does not satisfy the Cancellation law, 
x ® y = 0 =f x = 0 or y = 0. 
(b) 
If a is irreducible, then D/a is a field. 
Proof (a) If a is not irreducible we can find x and y in D such that xy = a 
and neither x nor y is invertible. Let x' = x mod a and y' = y mod a. We 
shall show that x' =F 0, y' =F 0 but x' ® y' = 0 in D/a. 
Let q = x div a and q' = y div a. If x' = 0, then x = qa. Thus a = xy = qay. 
As the cancellation law holds in D and a =F 0, it follows that qy = l .  Thus 
y has inverse q. That contradicts our hypothesis that y is not invertible. 
Therefore x' =F 0. Similarly, y' =F 0. 
What is x'y' mod a? 
Well, 
x' = x - qa 
and 
y' = y - q'a .  

so 
The construction of fields 
x'y' = xy - (qy + q'x - qq'a)a = (1 - qy - q'x + qq'a)a. 
145 
Hence x' ® y' = x' y' mod a = 0. Thus x' and y' violate the cancellation law 
in Dja. 
(b) Now assume that a is irreducible. As it has already been established 
that Dja is always a commutative ring (Theorem 9.3), the only law that needs 
to be established is that every non-zero element of Dja has a multiplicative 
inverse. 
Let x :F 0 be an element of Dja. To find an inverse of x, we must find y 
in Dja so that xy mod a = 1. 
Step 1. As x is a remainder we must have llxll < llall .  Therefore by EVl, a 
does not divide x. Hence by Lemma 8.6, (a, x) = 1. 
Step 2. Therefore by Theorem 8.5 there exist u and v such that 1 = ua + 
vx. 
Step 3. Keeping the notation of Step 2, let y = v mod a and q 
= v div a. 
Then y e Dja and 
xy = vx - qax = 1 - ua - qax = - (u + qx)a + 1 .  
Thus x ® y = xy mod a = 1 and we have found our inverse, y. 
• 
9.9 Finding inversų 
The algorithm for finding an inverse modulo a, when a is irreducible, is the 
basis for performing division in finite fields, so here is a further example to 
show how it works for polynomials. 
Example Let us calculate the inverse of 9 = 1001 in GF(16) by the 
method indicated by Theorem 9.8. In Chapter 6 we constructed GF(16) as 
B[x]/.f(x), withf(x) = x4 + x3 + 1 .  The.symbol 9 represents the polynomial 
x3 
+ 1 .  
Just as in the discussion of long division for polynomials in Chapter 6, 
the polynomials in the table will. be represented by binary n-tuples. Thus 
x4 + x3 + 1 will be represented ·.by 1 1001 and x3 + 1 by 1001. Poly­
nomial arithmetic is rather more unwieldy than integer arithmetic, so 
we insert extra rows in the table that incorporate the rows of the long-. 
division calculation. The rows that belong to Euclid's Algorithm itself are 
underlined. 

146 
Error-correcting codes and finite fields 
Row 
Q 
R 
u 
v 
- 1  
1 1001 
00001 
00000 
0 
0100 1  
00000 
00001 
0, 1 :  
1 0  
0101 1 
00000 
00010 
0, 2: 
01 
00010 
00001 
0001 1 
2 
100 
00001 
001 00 
01 101 
3 
10 
00000 
01001 
1 1001 
To show· how the intermediate row before row 1 is calculated consider 
the long-division table for dividing 1 1 001 by 01001 : 
1001)1 1001 (1 1 
1001 
(1) 
101 1 
1001 
(2) 
1 0  
The rows that actually appear in the table are those marked (1) and (2). 
These are obtained by shifting 1001 to match the highest remaining non-zero 
coefficient and subtracting. In the table for Euclid's algorithm, the Q column 
indicates the shift, and the precise rule is as follows: 
Choose q to be a power of x so that q x r0 matches the highest term of r _ 1 in 
degree (for more general fields we take q = axm so that the highest terms 
match). 
Now calculate the next row as though this were the correct q. Giving the 
row the number 0, 1 we get the entries: 
qo, 1 = q, 
Uo. 1 = u- 1 - quo, 
The resulting row still has degree greater than the degree of r0• So repeat the 
process choosing q so that q x r0 matches the highest term of r0 . 1 .  That 
produces r0, 2, which gives a new intermediate row with entries 
q0, 2 = q, 
ro,2 = ro, 1 - qro , 
Uo, 2 = Uo, 1  - quo, 
Vo, 2  = Vo, 1 - quo .  
Continue in the same manner until the degree of the R-entry drops below 
the degree of r0• Re-label the last row Row 1. In this case row 0, 2 becomes 
row 1. Now continue to the next stage of Euclid's algorithm. 
The process has the advantage that long division is carried out inside the 
table. But the Q-entry in the underlined row is not the full quotient q;. To 
find the full quotient you must add all the entries below the last underlined 

The construction of fields 
147 
row. We shall not make any use of the q entries and so it is unnecessary to 
rewrite the underlined row to produce the full q;. 
From the table read off the· inverse of 1001 = 9. The highest common 
factor (1001, 1 1001) is r2 = 1. So the inverse is v2 = 1 101 = 13 which agrees 
with the multiplication table at the end of Chapter 6. 
Note that in both Examples 9.7 and 9.9 the last entries of U 
and V are x 
and a respectively. That is always the case when a is irreducible. The last row 
gives no new information, but it does form a useful calculation check. 
9.10 Available field sizes 
What are the sizes of the finite fields obtainable by this method? 
Example First consider D = Z. The theorem tells us that Z/n is a field if 
and only if n is prime. Z/n has exactly n elements {0, 1, . . .  , n - 1}. So we 
get fields with any prime number of elements. For n = 2, we get B, and for 
n = 3 we get the ternary field. That does not help us find fields with 2n 
elements. 
Now take the case D = B[x], the set of polynomials over B. Here the 
method produces only fields that have i" 
elements, because the set of 
remainders of a polynomial of degree n in B[x] is just the set of polynomials 
of degree Ç n - 1. That set has exactly 2n elements. 
The argument of the examples yields the following proposition. 
Proposition (a) For each prime number p the field Z/p has p elements. (b) If 
there is a field F of q elements and an irreducible polynomial f(x) E F[x] of 
degree n, then the field F[x]/fi:x) has qn elements. 
• 
So, provided there are suitable irreducible polynomials, we can construct 
fields of prime-power orders. In the next chapter it will be shown that all 
finite fields have prime-power order. Finally, the question arises whether 
there are fields of all such orders. It is possible to prove directly that there 
are enough irreducible polynomials to produce fields of every prime power 
order, but the proof does not help in finding irreducible polynomials (see 
Exercise 9.6). In Chapter 11 the existence of fields of every prime power order 
will be proved by a different method. 
9.11 GF(16) again 
It is now possible to verify rigorously that GF(16) as constructed in Chapter 
6 is a field. All we need to do is to check that x4 + x3 + 1 is irreducible. 
That can be done by looking for possible factors of smaller degrees. 

148 
Error-correcting codes and finite fields 
Example 
Verification that G F (16) is a field. 
Let f(x) = x4 + x3 + 1. We shall show that f(x) is irreducible. Iff(x) = 
g(x)h(x) and deg(g(x)) ř deg(h(x)) < deg(f(x)), then deg(g(x))  2. So we 
need only verify that f(x) has no factors of degree 1 or 2. 
Does f(x) have a factor of degree 1? The only such polynomials over B 
are x and x + 1. Clearly x does not divide f(x). And x + 1 divides x4 + 1, 
so if it divided x4 + x3 + 1, it would have to divide x3• 
Could f(x) be the product of two polynomials of degree 2? The poly­
nomials of degree 2 are x2, x2 + 1, x2 + x and x2 + x + 1. The polynomial 
x divides both of the first two of these, but it does not divide x4 + x3 + 1, 
so these two are out. (x + 1)2 = x2 + x + x + 1 = x2 + 1, and x + 1 does 
not divide f(x). So the third polynomial is also out. We are left with the 
possibility of the last, x2 + x + 1. If f(x) = (x2 + x + 1)ޘ(x), the second 
factor h(x) would also have degree 2. Hence h(x) may not be one of x2, 
x2 + 1 or x2 + x. Thus it must also be x2 + x + 1. But 
Since f(x) is not invertible (because it is not a constant), it follows that 
f(x) is irreducible. That proves that B[x]ff(x) is a field. The remainders on 
division by f(x) are the binary polynomials of degree up to 3. There are 
precisely 16 such polynomials. Thus we have proved that B[x]/.f(x) is a field 
of order 16. 
A proof of this type is far more efficient than attempting to verify the field 
axioms directly. 
9.11 Summary 
The topic of this chapter was the construction of the factor rings Dja 
(the 'modulo' construction). We generalized the method used to construct 
Z/16 and GF(16), the field of order 16, in Chapter 6. The construc­
tion can be performed using any non-zero, non-invertible element a of a 
Euclidean domain. It yields the factor ring D/a. The main result of 
the chapter, Theorem 9.8, states that the factor ring D/a is a field pre­
cisely when a is irreducible. That gives a general tool for constructing finite 
fields. The construction contains within it algorithms for performing arith­
metic in the field it constructs. In particular, inverses of elements in a 
constructed field can be calculated from the additional columns of Euclid's 
algorithm. 

The construction of fields 
149 
9.12 Exercises 
9.1 Complete the proof of Theorem 9.3 that under the uniqueness assump­
tion, for any non-zero, non-invertible element a in a Euclidean domain 
D, D/a forms a commutative ring. 
9.2 Show that for any non-zero, non-invertible element a of a Euclidean 
domain D, the restricted residue class of 0 mod a, contains only 0. 
9.3 Verify that in the proof of Theorem 9.3 the uniqueness assumption can 
be replaced by the assumption that the remainder function is consistent. 
9.4 
Show that there are 2n binary polynomials of degree exactly n. 
9.5 Show that all polynomials of degree 1 are irreducible. Let the number 
of irreducible binary polynomials of degree n be I(n). Calculate /{2), 
/{3), /{4). Verify that, for these values of n, 
· 
2n = L iJ(d). 
djn 
(A general proof of this formula is given in Exercise 12.15.You can find 
a different one in MacWilliams and Sloane (1977), Chapter 4). 
9.6 Let p be a prime number and let IP(n) be the number of irreducible 
polynomials of degree n with coefficients in Zjp. Assuming the formula 
pn = L d ·Jp(d), 
d j n 
prove that I(n) =I= 0. 
The next four exercises deal with constructions of fields. 
9.7 Using appropriate irreducible binary polynomials, construct fields of 
orders 4, 8 
and 32, giving their addition and multiplication tables. 
9.8 Using appropriate irreducible ternary polynomials, construct fields of 
order 9 and 27. 
9.9 
Construct a field F of order 16 using the irreducible polynomial 
x4 + x3 + x2 + x + 1. 
9.10 Find a root y of the polynomial x4 + x3 + 1 in the field F of Exercise 
9.9. Define a map cjJ from our 'standard' field GF(16) to F by defining 
cjJ(2k) = yk. Show that cjJ 
is bijective (one-to-one and onto) and that 
c/J(x + y) = cjJ(x) + cjJ(y) and cjJ(xy) = cjJ(x)cjJ(y) for all x, y e GF(16). 
This exercise shows that F and GF(16) are essentially identical. 
The next three exercises show how to define residue class rings for any 
commutative ring. 
9.1 1  Let R be an arbitrary commutative ring, and let a E  R. Define b = c 
(mod c) if b- c = qa for some 
q e R. For each element x of R define 

150 
Error-correcting codes and finite fields 
the class [x] = {bix = b (mod a)}. Show if y E [x] then [y] = [x] and 
if y ¢ 
[x] then [x] n [y] = 0. An element y E [x] is called a repre­
sentative of the class x. 
9.12 Continuing from Exercise 9.1 1, define R/a to be the set {[x] lx E R}. 
Introduce multiplication and addition in R/a by defining [x] + [y] = 
[x + 
y] and [x][y] = [xy]. Show that if x' E [x] and y' E [y] then 
x' + 
y' E [x + y] and x'y' E [xy]. Deduce that the addition and multi­
plication defined in Rja do not depend on the choice of representatives 
x and y for the classes [x] and [y]. 
9.13 Continuing from Exercise 9.12, show that R/a is a commutative ring. 

10 The structure of finite fields 
Now that you have seen a method of constructing finite fields, we turn to 
the task of analysing their structure. In this chapter we establish four 
fundamental results. These results are all related to a prime number, called 
the characteristic of the field, that determines its arithmetical properties. As 
the results are analytical in nature, their proofs are not applied directly, 
like the proofs involving Euclid's algorithm. Rather, the characteristic is 
implicitly used by maf!.y algorithms. You can use the algorithms without 
understanding the theory, but you will not be able to design or modify them. 
For reference and by way of motivation, all four properties are first 
presented with examples. The proofs are given later. 
10.1 The prime field and the characteristic 
The first result is the following: 
Theorem Every finite field F contains a field of the form Z/p, where p is a 
prime. 
For any element a e F and any positive integer n, the element b ofF obtained 
by adding a to itselfn times satisfies b = 0 if and only if the prime p divides n. 
Definition 
The special field Z/p contained in F is called the prime field of 
F; p is called the characteristic of F and denoted by x(F). 
The prime field of F can be obtained in the following way. Start with the 
0 and 1 elements of F and then take all possible products and sums. The 
set of elements you get in this way is the prime field of F. You can also take 
differences and quotients, but that will turn out to be unnecessary. 
Example 
In GF(16). 1 + 1 = 0 and 1 x 1 = 1. So the only elements that 
we can get from 0 and 1 by applying arithmetic are 0 and 1 themselves. Thus 
the prime field of GF(16) is Z/2 = B and the characteristic is 2. That is hardly 
surprising, since we started with B when we constructed GF(16). 
The characteristic determines which family a finite field belongs to, in the 
sense that fields of the same characteristic share many of their propeities. 

1 52 
Error-correcting codes and finite fields 
For example, it follows from the theorem that fields of characteristic 2 are 
precisely those fields for which a + a = 0 holds for any non-zero element a 
(see Exercise 10.1) 
10.2 Sizes of finite fields 
The second result determines the possible sizes of finite fields. 
Theorem If a finite field E contains a finite field F and F has q elements, 
then E has exactly q" elements for some positive integer n. 
Definition 
If a field E contains a field F (such that addition and multiplica­
tion in F are defined by restricting the definition in E to elements of F), then 
F is called a subfield of E and E is called an extension field of F. 
The number of elements of a finite field is called its order. 
Example 
The most familiar pair of fields like this consists of the set of real 
numbers R and the set of complex numbers C. The field R is a subfield of 
C, and C is an extension of R (obtained by adjoining i = )  - 1). 
Theorem 10.1 states that every finite field E is an extension of its prime field 
and the answer to the question, ' How do you construct E from its prime 
field?' can be guessed from Chapters 1 and 4. This special case determines 
the possible sizes of all finite fields and is sufficiently important to be stated 
as a corollary. 
Corollary If a finite field E has characteristic p, then E has order p" for some 
positive integer n. 
Example 
The field GF(16) has characteristic 2 and confirms the corollary 
with its 16 = 24 elements. More interesting is the fact that the elements 
(0, 1, 10, 1 1) form a subfield of GF(16). This field has 4 elements and 4 = 22, 
while 16 = 42• 
The theorem implies that GF(16) has no subfield of order 8. The corollary 
implies that there is no field at all of order 10. 
The theorem and its corollary suggest several questions: 
1. Given a prime power p", is there a field of order p"? 
2. If E is a field of order q", 
does E contain a subfield of order q? 
3. How many distinct fields of a given order can there be? 

The structure of finite fields 
1 53 
These questions will be answered in later chapters. It is not hard to guess 
the answers, but proving that they are correct is another matter. 
10.3 A property of r.(F) 
The next result is very important for practical calculations. It states that in 
taking a sum of elements of a field F to the power p = x(F), you can just 
take each element to that power and then add the powers-if p = x(F), 
taking pth powers commutes with addition. 
Theorem If F has characteristic p, then for a, b E  F, (a +  b)P = aP + bP. 
Example 
According to the Theorem, in GF(16), . (a + b)2 = a2 + b2, for 
example 10 = 1 
P = (8 + 3)2 = 82 + 32 = 15 + 5. 
You can verify the characteristic 2 case directly 
(a + b)2 = (a +  b)(a + b) = a2 + ab + ba + b2 
= a2 + b2 + aH + ab = a2 + b2, 
because in characteristic 2, u + u = 0. The general argument is not much 
harder than that. 
Definition 
For a field F of characteristic p, the map a -+  aP is called the 
Frobenius automorphism of F, after the great German mathematician Gustav 
Frobenius (1864-1917). 
The theorem can be restated in the following form. If ¢:· F -+ F is the 
Frobenius automorphism, then 
¢(a + b) = c/>(a) + c/>(b). 
It is also obvious that 
c/>(ab) = ¢(a)¢(b). 
These two equations are the essential properties of an automorphism of a 
field (see Section 10.8). 
10.4 Fermat's little theorem 
The last of our four basic results about finite fields is equivalent, in the case 
of the field Zjp, to a theorem of the famous French mathematiciޙn Pierre 
de Fermat (1601?-1665). Fermat was a counsel to the parlement 
of Toulouse 

154 
Error-correcting codes and finite fields 
and an amateur mathematician. He was a pioneer of analytical geometry 
(where he anticipated Descartes) and differential calculus (where he anticipated 
Newton). He invented a form of proof by induction which he called 'infinite 
descent' but stated many of his theorems without proof, among them this 
one, which he stated in 1640, and his notorious 'last theorem'. Fermat's last 
theorem remains unproved to this day, so it should better be called Fermat's 
last conjecture (there have been many incorrect 'proofs'-see Section 8.1 1). 
Our theorem which was first proved by Leibniz in about 1680, is called 
Fermat's little theorem to distinguish it from the 'last theorem'. 
Theorem 
Fermat's little theorem. Let F be afield of order q. Then for any 
element a e F, aq = a. 
Example 
For every element a of GF(16), a16 = a. In particular, if a of. 0, 
then a15 = 1. That is the reason that in calculating products using logarithms, 
the answer is taken modulo 15. So you can see that Fermat's little theorem 
constitutes the first step in the construction of a logarithm table for F. 
10.5 Integer mJltiples 
In discussing the Frobenius automorphism and Fermat's little theorem, we 
took integer powers of field elements without further ado. It was, I hope, 
obvious that a2 stands for a x a and a3 stands for a x a x a. In the same 
way, we can take integer products for elements of a field. Initially, we shall 
use 0 when we multiply a field element by an integer to distinguish an 
integer multiple from a product of field elements. Thus 2 0 a = a + a and 
3 0 a = a + a + a. Later we shall drop the special symbol and rely on the 
context to determine whether this multiplication by integers or standard field 
multiplication is meant. 
Example 
In GF(1 6), 2 0 2 = 2 + 2 = 0, while 2 x 2 = 4. Notice that all 
the 2s except the very first stand for the element 00 1 0  of the field. The first 
2 really is the number 2. 
Definition 
Let F be any field, a e F, and Jet n be a positive integer. We 
define n 0 a to be the element of F obtained by adding the element a to 
itself n 
times. The definition is extended to all integers by setting 0 0 a = 0, 
and ( - n) 0 a =  n 0 ( - a). 
10.6 Some arithmetic 
The main reason for introducing this multiplication is that the prime field 
of F turns out to be the set of elements of the form n 
0 1 .  To prove that, 

The structure of finite fields 
155 
we need some of the arithmetical properties of this multiplication. Indeed, 
it satisfies all those laws of arithmetic that make sense. Most of them are not 
directly needed. So they are left as an exercise (see Exercise 10.2). The three 
that we do require are the subject of the next proposition. 
Proposition Let m, n e Z, a e F and let 1 denote the identity element of F. 
Then the following equations hold. 
(m + 
n) 0 1 = m 0 1 + 
n 0 1 ,  
(m x n) 0 1 = (m 0 1) 
x (n 0 1 ) .  
and 
m 0 a = (m 0 1) x a. 
Notice that there is something to prove. The formula (m + n) 0 1 means 
'first add the two integers m and n and then take the appropriate multiple 
of 1 ', whereas m 0 1 + n 
0 1 means 'take appropriate multiples of 1 and 
then add them in F. The most confusing thing about the proof is the notation. 
All it does is to show that the laws of arithmetic allow us to expand 
brackets to get the stated results. For any particular m and n this can easily 
be seen directly. For instance, if m = 2 and .n = 3, then the statements of the 
propositions can be written in the form 
and 
(1 + 
1 + 
1) + (1 + 
1) = 1 + 
1 + 
1 + 
1 + 
1 ,  
(1 + 1 + 1 )(1 + 1 )  = 1 + 1 + 1 + 1 + 1 + 1 ,  
( 1  + 
1 + 
1)a = a + a + 
a. 
These equations are direct consequences of the associative law of addition 
A1 and the distributive laws D1 and D2. 
Proof We shall assume in the proof that m and n are positive. The other 
cases follow by manipula-ting the signs of the terms. 
We write the terms m 0 1 and n 0 1 as 
m 
n 
2: 1 
and 
2: 1 . 
k = l 
k = l 
From the associative law of addition it follows that 
m 
n 
m 
m +n 
m + n  
m 0 1 + 
n 0 1 = 2: 1 + 
2: 1 = 2: 1 + 2: 
1 = 2: 1 = em + 
n) o t .  
k = l 
k = l  
k = l  
k = m + l 
k = l 
From the distributive law we get 
(m 0 1) x (n 
0 1) = Ct1 1) x Ct1 1) = ,t1 Ct1 1) 
= k8l 1 = (mn) 0 1 ,  

156 
and 
Error-correcting codes and finite fields 
(m 0 1) x a =  ( f: 1) x a = f: a = m 0 a. 
k = l 
k= l 
10.7 Constructing the prime field 
• 
We are now ready to construct the prime field of any given finite field F. 
Consider the set of elements a of the form a = n 0 1, where n may be any 
integer. It will turn out that this set is precisely the prime field we are looking 
for, but that must still be established, so we shall call it the prime set of F 
for the time being. 
As F is finite, its prime set must be finite. So m 0 1 = n 0 1 .must hold 
for some pair of integers m =I= n. Suppose m > n. Subtracting n 0 1 from both 
sides we get (m - n) 0 1 = 0, and m - n > 0. So there exist positive integers 
k for which k 0 1 = 0. Let p be the smallest such positive integer. The 
number p will turn out to be the characteristic of the field F. That must also 
be proved, so we shall call p the null characteristic of F. 
Example 
In GF(16) the prime set is the set 
{ . .. ' - 3 0 1, - 2 0 1, - 1 0 1, 0 0 1, 1 0 1, 2 0 1, 3 0 1, . .. } 
= { ... , 1, 0, 1, 0, 1, 0, 1, ... } 
= {0, 1 } . 
The null-characteristic is 2, because 1 0 1 =1= 0 and 2 0 1 = 0. As claimed, 
2 is the characteristic of GF(16), and the prime set is the prime field. 
The following theorem proves the first part of Theorem 1, by establishing 
that the null-characteristic is a prime p, and the prime set is essentially Zjp. 
Theorem Let F be a finite field of null characteristic p. Then the following 
statements hold. 
(a) The prime set ofF has p elements. 
(b) The number p is prime. 
(c) The prime set ofF forms a subfield ofF that is a copy of Z/p 
Proof (a) The existence of the null characteristic p has been established 
already. From the fact that p 0 1 = 0, it follows that n 0 1 = m 0 1 if m 
and n leave the same remainder modulo p. On the other hand, the difference 
between two distinct remainders r > s mod p is r - s < 
p. Hence because p 
was chosen minimal with p 0 1 = 0, (r - s) 0 1 # 0 and so r 0 1 # s 0 1. 

The structure of finite fields 
157 
Thus the prime set of F is in one-to-one correspondence with the remainders 
modulo p, that is, elements of Zjp. Therefore it has precisely p elements. 
(b) If p = ab with 0 < a, b < p, then a 0 1 :F 0 · and b 0 1 :F 0. But 
(a 0 1)(b 0 1) = (ab) 0 1 = p 0 1 = 0. That cannot happen in a field F. 
Thus p must be a prime number. 
(c) It has already been established in (a) that the elements of the prime 
set of F are in one-to-one correspondence with those of Zjp. From 
Proposition 10.6 it also follows that they add and multiply in exactly the 
same way. For m 0 1 + 
n 0 1 = (m + 
n) 0 1 = r 0 1, where r = (m + n) 
mod n. 
Similarly m 0 1 x n 0 1 = mn 0 1 = s 0 1, where s = mn mod n. 
· 
CoroUary The prime set ofF is the prime field ofF, and the null characteristic 
ofF is the characteristic of F. 
We can now drop the use of the words 'prime set' and 'null characteristic'. 
10.8 Isomorphisms and automorphisms 
It is worth noting the way we established that the prime field of F 'was a 
copy of or 'essentially the same as' Z/p. We found a one-to-one correspond­
ence between the elements of the prime field and those of Z/p, such that 
products and sums were preserved. It is convenient to replace the informal 
idea of a copy by a formal definition legitimizing the proof. 
Definition 
Two fields F and F' are called isomorphic (Greek: 'of equal 
shape') if there exists a map 4> from F to F' such that 
(a) 
4> is bijective (that is, to each element P e F' there is exactly one element 
a e F with ¢(a) = p), and 
(b) for any a, b e  F, <f>(ab) = <f>(a)<f>(b), and ¢(a +  b) =  <f>(a) + ¢(b). 
The map 4> is called an isomorphism. If F = F', then 4> is called an 
automorphism of F. 
An isomorphism ¢ can be reversed to give a map ¢ - 1  from F' to 
F, which is called the inverse map. It · is straightforward and left as 
an exercise to verify that ¢ - 1 is also an isomorphism. An example of an 
automorphism is the Frobenius ·automorphism defined in Section 10.3. 
10.9 Completing Theorem 10.1 
Theorem 10.1 has not quite been finished yet. We must still prove the second 
statement. 

1 58 
Error-correcting codes and finite fields 
Proposition 
Let F be finite field of characteristic p, and a # 0 an element of 
F. Then for an integer n, n 0 a = 0 if and only if n is a multiple of p. 
Example 
In GF(16), n 0 a =  a if n is odd, and n 0 a =  0 if n is even. 
Proof It has been shown in Theorem 10.7 that n 0 1 = r 0 1 where 
r 
= n mod p. and that the only integer r with 0 :::; r < p for which r 0 1 = d 
is r = 0. Thus n 0 1 = 0 if and only if n is a multiple of p. The general 
statement follows from the fact proved in Proposition 10.6 that n 0 a =  
(n 0 1) x a. For a #  0, it follows that n 0 a =  0 if and only if n 0 1 = 0 . 
• 
10.10 Proof of Theorem 10.2 
We have now proved all the statements of Theorem 10. 1  and a bit more. In 
doing so we have laid the foundations for the entire structure analysis of 
finite fields. The remaining results require no new definitions. 
The next task is to prove Theorem 1 0.2. The proof will follow from the 
following proposition. 
Proposition Let F be a subfield ofthefinitefield E. Then there exist elements 
IX1, •
•
•
 , 1Xn 
of E such that 
(a) every element P E E  can be written in the form b1 IX1 + 
· 
· 
· + bixn ; 
(b) if bliXl + . . . + bn1Xn 
= C tiXl + . . . + Cn1Xn, with bi, ci E F, then bl = 
Cl, 
• • • ' bn = Cn. 
Example 
If F =  Z/2 and E = GF(16) we can consider the elements of E 
as polynomials of degree at most three and choose IX1 = 1, IX2 = x, IX3 = x2, 
1X4 = x3• In the binary representation this corresponds to the sequence 
1, 2, 4, 8. In either of the representations it is easy to see that (a) and (b) hold. 
The proposition often holds for infinite fields as well. For instance, take 
F as the real numbers R and E as the complex numbers C. Then we can 
choose IX1 = 1 and IX2 = i, and, as we know, every complex number has a 
unique representation in the form a + bi. 
Proof Construct cx 1 , •
.
•
 , 1Xn 
as follows. Start with cx1 = 1. Suppose cx1, •
•
• , cxn 
have been chosen. Then if there is still a p E F that cannot be written in the 
form b1 cx 1 + 
· 
· 
· + bncxn, take cxk + 1 = p. Otherwise stop and put k = n. 
The procedure must end because F is finite. When it ends (a) is 
automatically satisfied and all we must show is that (b) also holds. So assume 
that b1 cx1 
+ 
· 
· 
· + bncxn 
= c 1 cx1 
+ 
· 
· 
· + cncxn 
and there is at least one value j 

The structure of finite fields 
for which b1 -:;. c1. Choose k to be the largest such j. Then 
(b1 - c1)1X1 + .
. . + (bn - cn)1Xn = 0. 
Since b1 = c1 for j > k, we can write 
(b1 - c1)a1 + 
· 
· 
· + (bk - ck)ak = 0. 
Using the fact that d = bk - ck -:;. 0, we can rewrite this as 
IXk = ((b1 - C1)/d)IX1 + 
· 
· 
· + ((bk- 1 - Ck- 1)/d)ak- 1> 
contradicting the choice of ak. 
Theorem 10.2 is now just a matter of counting. 
159 
• 
Corollary 1 (Theorem 1 0.2) If a finite field E contains a finite field F and 
F has q elements, then E has exactly qn elemen{s for some positive integer n. 
Proof Choose elements a1, •
•
•
 , 1Xn as in the proposition. Then we can 
represent every element f3 e E with f3 = b1a1 + 
· 
· 
· + bnan by the n-tuple 
(bl> . . .  , bn). There are exactly qn such n-tuples. 
• 
Corollary 10.2 follows immediately by substituting the prime field fo.r F. 
Corollary 2 If a finite field E has characteristic p, then E has order pn for 
some positive integer n. 
• 
10.11 Use of lineaË; algebra 
From Proposition 10.10 we can deduce rather more than just Theorem 10.2. 
It actually shows that E is a vector space over F, so that we can use the 
results of linear algebra. 
Theorem Let F be a sub field of the finite field E. Then the following statements 
hold. 
(a) The zero elements ofF and E are the same. 
(b) The identity elements ofF and E are the same. 
(c) The set E with field addition and multiplication limited to products ab 
where a e F,forms a vector space over F. 
Examples If E is the set of complex numbers and F is the set of real 
numbers, then the vector space we get in part (c) is the 'complex plane'. It 
has dimension 2 and a natural basis is the set (1, i), giving the real and 
imaginary cordinate axes. 

160 
Error-correcting codes and finite fields 
If E = GF(16) and F = B, then E corresponds to binary 4-tuples, and the 
vector space of part (c) is B4. To reduce GF(16) to a binary vector space we 
just forget how to multiply the 4-tuples and restrict ourselves to addition. 
Warning Theorem 10. 1 1(a) holds in most situations where a zero is defined, 
but Theorem lO.ll(b) is not true for most algebraic systems. For example 
consider the set M of 2 x 2 real matrices. This has the standard identity and 
zero matrices 
l =G ³J 
0 = [´ ´] 
as its identity and zero elements . M contains a copy R of the real numbers 
in the form of matrices of the shape 
The zero element of R is 
which is the same as the zero matrix 0, but the identity element of R is 
which is different from the matrix /. 
Proof (a) Let 0 be the zero of E and 0' be the zero of F. Then using addition 
in F, 
0' + 0' = 0'. 
Using addition in E, 
0 + 0' = 0' = 0' + 0'. 
Subtracting 0' from both sides gives 0 = 0'. 
(b) The proof is almost the same. Let 1 be the identity of E and 1' the 
identify of F. Then using multiplication in F 
1 ' x 1' = 1'. 
Using multiplication i n  E, 
1 X 1' = 1' = 1' X 1'. 
From the field axioms 1' :F 0' = 0. So we can divide both sides by 1' to get 
1 = 1'. 

The structure of finite fields 
161 
(c) We choose elements ct1, •
.
•
 , ct" E E as in Proposition 10.10 and repre­
sent elements of E by n-tuples as we did in the proof of Theorem 10.2. Then 
for fJ and y  corresponding to n-tuples (b1, .
•
• , b") and (cl> . .. 
, c") we have 
P 
= blctl + • 
• 
· + bnctn, 
Y = C1IX1 + · 
· 
· + Cnctn . 
Hence p + y = (bl + cl)ctl + . . .  + (bn + cn)ctn corresponds to (bl + 
Cl, . . . ' 
bn + cn). Similarly for a E F, ap = ab1ct1 + · · · + abnctn corresponding to 
(ab1, •
•
•
 , abn). Thus, with these operations E has the same structure as r . 
• 
10.12 Uniqueness of the prime field 
From Theorem 10.2 and its Corollary it follows that the prime field of F 
and hence x(F) is unique. That can also be deduced from Theorem 10.1, 
but the proof using Theorem 10.2 is almost immediate. 
Corollary Let F be a finite field. The only field of the form Z/q contained in 
F ts-the prime field of F. 
Proof The factor ring Z/q is only a field for q prime. If F has characteristic 
p, then F has order p". Thus p" = qm, and since p and q are primes it follows 
that p = q. 
It remains to show that if G is a subfield ofF of order p, then G is precisely 
the prime field of F. By Theorem 10.1l(b), G contains the identity element 
1 of F. Hence G contains n 0 1, for all integers n. Therefore G 
.contains the 
prime field of F. But G has the same order as that prime field. Hence G is 
the prime field of. F. 
• 
10.13 A result on binomial coefficients 
Before proceeding to the proof of the third fundamental result, we prove a 
lemma on binomial coefficients. 
Lemma Let p be a prime number and O<k<p. Then p divides the binomial 
coefficient 
· 
(p) 
p! 
k = k!(p - k)! . 
Example The binomial coefficients for 7 are 1, 7/1 = 7, 7 x 6/2 = 21, 
7 X 6 X 5/3! = 35, 
7 X 6 X 5 X 4/4! = 35, 
7 X 6 X 5 X 4 X 3/5! = 21, 
7 x 6 x 5 x 4 
x 3 x 2/6! = 7, and 7!/7! = 1. 
The prime 7 divides all but the 
first and last coefficients. 

162 
Error-correcting codes and finite fields 
Proof Denote the binomial coefficient (:) by x, and let its numerator p! 
be n and its denominator k!(p - k)! be d. Then d, x, and n are all integers 
and dx = n. The prime p obviously divides n, therefore by the key property 
of irreducibles, Theorem 8.7, it follows that p divides one of d and x. If 
0 < k < p, then p does not divide any of the factors of d = k!(p - k)!. Again 
by Theorem 8.7, it follows that p does not divide d. Hence p divides x. 
• 
10.14 The Frobenius automorphism 
We can now prove our third fundamental result. 
Theorem Let F be a finite field of characteristic p. Then the map ¢ from F 
to itself defined by ¢(a) = aP is an automorphism. 
Example 
You have already seen an example (and indeed a proof) that 
a2 + b2 = (a +  b)2 in GF(1 6) and the fact that a2b2 = (ab)2 is obvious. 
All that remains is to check that taking squares is bijective. From the table, 
we see that the list of squares of elements of GF(16) is 02 = 0, 12 = 1, 22 = 4, 
32 = 5, 42 = 9, 52 = 8, 62 = 13, 72 = 12, 82 = 15, 92 = 14, 102 = 1 1, 112 = 10, 
122 = 6, 132 = 7, 142 = 2, and 152 = 3. As required, each element of GF(1 6) 
occurs exactly once. That proves that taking squares is bijective. 
Incidentally, one consequence of the theorem is that elements of a finite 
field F of characteristic 2 have unique square roots in F, unlike real numbers 
that have exactly two square roots or none at all. One unfortunate side effect 
of that is that the formula for quadratic equations cannot work, because it 
relies on the existence of two square roots. The problems arising from this 
will be discussed at greater length in Part 3. 
Proof There are three conditions to satisfy: 
1. ¢(a + b) = ¢(a) + ¢(b) (this is Theorem 10.3). 
2. ¢(ab) = ¢(a)¢(b) (this is almost trivial). 
3. The map is bijective. 
1. Observe that the binomial theorem allows us to calculate (a + b)P in 
F. That is because the binomial theorem is proved by multiplying out the 
terms (a + b) using the distributive law, and then counting how many times 
each product aP-kbk occurs. Thus: 
(a+ b)P 
= (²)aP + e)ap-lbp + 
· 
• 
• + c 7 Jabp-l + (;)bP. 

The structure of finite fields 
163 
By Lemma 10.13, all the middle binomial coefficients are multiples of p. 
Hence by Theorem 10.1, these terms are all 0 in F. The first and last binomial 
coefficient are both equal to 1. That concludes the proof of (a). 
2. This is straightforward. The equation aPbP = (ab)P holds because the 
commutative law of multiplication allows us to rearrange products. 
3. First we show that if aP = bP, then a = b. 
From statement 1, aP + 
(b - a)P = bP. Hence if aP = bP, it follows that 
(b - a)P = 0. But a product of elements in a field can only be 0 if one of the 
factors is 0. Hence b - a = 0. Therefore a = b. Thus we have shown that ¢ 
is one-to-one. 
To show that it is also onto we exploit the fact that F is finite. Since ¢ is 
one-to-one, the set of pth powers of elements of F has the same number of 
elements as F itself. That implies that every element of F is a pth power and 
hence that ¢ is onto. 
• 
... . 
10.15 Fermat's little theorem 
The last of our basic facts is the little Fermat theorem. This theorem is 
obviously true for a =  0; Oq = 0 for any q. For non-zero a we can cancel one 
factor a on either side of the equation. Thus we arrive at an equivalent 
statement of the theorem, with a remarkably neat proof. 
Theorem IfF is afield with exactly q = pn elements, then for every non-zero 
element a e F, aq- 1  = 1. 
Proof Assume a ¥= 0. List all the elements of F starting with 0 as follows: 
f1 = 0, f2, •
•
•
 , h-. 
Consider the products a/; for all i = 1, . . .  , q. Then, by the 
cancellation law a/; ¥= a/; for i ¥= j. Using the fact that F is finite, it follows 
that the list af1 = 0, af2, •
•
•
 , a/q contains all the elements of F in some new 
order. 
Taking the products of non-zero elements of both lists we get 
fzf3, · · · ,  /q 
= (afz)(af3) · · ·(af,). 
Now rearranging the right-hand side: 
fz/3 · 
· · fq =· aq- t fz/3 · 
· 
· [q. 
By the cancellation law, it follows that aq- 1 = 1. 
• 
10.16 Summary 
We have established the fundamental structural properties of finite fields. 
Each finite field F has a unique characteristic p = x(F) that is a prime 
number. A finite field F of characeristic p has a unique prime field of the 

164 
Error-correcting codes and finite fields 
form Zjp and F has order q = pn. The map ¢ taking a to aP is an 
automorphism of F, and if this map is repeated n times, taking a to aq the 
result is the identity map of F. 
The major questions left unanswered are whether a finite field of given 
prime-power order exists and how many different fields of a given order 
there can be. These questions will be decided by considering the roots of 
polynomials with coefficients in 
a finite field. 
10.17 Exercises 
10.1 Show that a field has characteristic 2 if and only if it contains a 
non-zero element a for which a + a = 0. 
10.2 (Properties of integer multiplication in fields.) Prove that the multipli­
cation defined in Section 10.2 has the following properties (m, n denote 
integers; x, y denote field elements): 
(mn) 0 x = m 0 (n 0 x); 
m 0 (xy) = (m 0 x) 0 y; 
1 0 x = x; 
0 0 X =  0; 
m 0 0 = 0; 
(m + n) 0 x = (m 0 x) + (n 0 x); 
m 0 (x + y) = (m 0 x) + (m 0 y). 
10.3 Show that if ¢ is an isomorphism from F to G, then ¢ - l is an 
isomorphism from G to F. 
10.4 Show that the map ¢ defined in Exercise 9.10 from GF(16) to the field 
F = B[x]/(x4 + x3 + x2 + x + 1) is an isomorphism. 
10.5 Find all subfields of the fields constructed in Exercises 9.7-9. Verify 
that for each the order of the whole field is a power of the order of 
the subfield. 
10.6 Show that there are four automorphisms of GF(16) taking 2 respectively 
to 2, 4, 9, and 14. Write down tables showing the action of these 
automorphisms on the other elements of GF(16). 
10.7 Show that if ¢ is any automorphism of GF(16) and ¢(2) = p, then 
p4 + p3 + 1 = 0. Deduce that the maps of Exercise 10.6 are the only 
automorphisms of GF(16). 
10.8 Show that the elements of proper subfields of GF(16) are precisely 
those y for which yk = y, for some k = 2i 
with j 
< 4. Show that there 

The structure of finite fields 
1 65 
are elements (J of GF(l6) that are not contained in any proper subfield 
but for which (J" 
= (J for some k < 24• 
10.9 Show that in a field F of characteristic p any element ex has at most 
one pth root (J (that is, an element (J e F with (JP 
= ex). 
Show further 
that if F is finite, then every element has exactly one pth root. 

1 1  
Roots of polynomials 
Until now, polynomials have been treated as formal sums. That is appro­
priate for the construction of finite fields. The idea that polynomials are 
functions only gets in the way and causes confusion. However, one functional 
aspect of polynomials is important for our theory, namely the concept of a 
zero or root of a polynomial. Field extensions were originally introduced in 
order to solve polynomial equations. The most notable example of this is 
the introduction of imaginary and then complex numbers to . solve the 
equation x2 + 1 = 0. That occurred in the sixteenth century and caused 
mathematicians of that time great philosophical problems, as can still be 
seen from the very names 'real numbers' and 'imaginary numbers'. It is 
amusing to note that the introduction of the real numbers caused the ancient 
Greeks equal philosophical problems, and that they would have queried the 
attribute 'real' for most irrational numbers. 
Be that as it may, the idea of a root of a polynomial is far too useful to 
be abandoned. None the less, it is better not to revert to considering 
polynomials as functions, because that would require us to rewrite the theory 
we have developed so far. Instead we shall introduce an 'evaluation operator' 
that allows us to substitute a constant element for the indeterminate x in 
the polynomial f(x). As an example, consider polynomials with real coeffi­
cients. We decide on a value, say 1 + i, and substitute it for x in all 
polynomials. The result for each polynomial f(x) will be a constant, which 
we denote in the usual way by /(1 + i). We even call it the value ofj(x) at 
1 + i. Those polynomials, like say x2 - 2x + 2, for which the value becomes 
0, will be said to have 1 + i as a root. 
One advantage of this approach is that the behaviour as the polynomials 
vary but the x-value stays fixed is perfectly regular: 
(f + g)(l + i) = /(1 + i) + g(1 + i) 
and 
jg(1 + i) =/(1 + i) X g(l + i). 
By contrast, if we choose two constants, 1 + i and say 2, then in general 
f(l + i + 2) =F/( 1  + i) + /(2) 
and 
j((l 
+ i) X 2) 
-:/= j(l 
+ i) 
X j(2). 

Roots of polynomials 
167 
You will have noticed that, although I started with real polynomials, the 
constant I used was complex. It is essential to allow the constant to come 
from an extension field for the idea of a root to be truly useful. Thus the 
imaginary number i is a root of the real polynomial x2 + 1, and that is its 
true raison d'etre. 
1 1.1 More on polynomials 
The applications we require later will be to finite fields, but the theory of 
this chapter applies almost without change to any fields. Throughout the 
chapter we shall consider a fixed pair of fields F and E, 
such that E 
is an 
extension of F. The coefficients of polynomials we consider will belong to 
the field F, called the base field. Constants at which we evaluate the 
polynomials may belong to E. For example F could be R, the reals and E 
could be C, the complex numbers, or F = B and E = GF(l6). We shall be 
interested in the roots in E 
of polynomials with coefficients in F. Polynomials 
will be written as sums 
without explicit summation bounds. The lower bound is 0, the upper oo, but 
it is assumed that only finitely many coefficients are non-zero: The set of 
polynomials is denoted by F[x]. If you are unsure about your knowledge 
of polynomials, skim through Appendix· PF before continuing with this 
chapter. 
Definition Let f = L: a; xi e 
F[x] be a polynomial and PeE. 
We define 
f(p) = L: a;pi e E. 
The element fJ 
is called a root or zero ofj(x) ifj({J) = 0. 
The map from F[x] to E takingf(x) to f(p) is called the evaluation map at 
P 
(note that f varies while P 
stays fixed). 
Example Let F = B and E = GF(16). Let p = 5. Then the evaluation map 
at 5 maps the (infinite) set of polynomials over B into the finite set GF(16). 
Choosing a polynomial at random, x6 + x5 + x3 + x + 1 gets mapped to 
56 + 55 + 53 + 5 + 1 = 5 + 1 + 3 + 5 
+ 1 = 3 .  
We first establish that the evaluation map has the properties claimed for 
it in the introduction. The proof of that is a straightforward (and rather 
boring) application of the formulae for polynomial addition and multiplica­
tion. Such proofs are commonly 'left as an exercise for the reader' because 
the author is disinclined to write them down. However, I rather doubt 
whether they are a useful exercise. 

168 
Error-correcting codes and finite fields 
Proposition Let f(x) and g(x) be polynomials in F[x] and P an element of 
E. Then 
(f 
+ g)(fJ) = f(p) 
+ g(fJ) 
and 
fg({J) = f(p) X g({J). 
Proof Let f(x) and g(x) be L aixi and L bixi respectively. Then 
<f + g)<P> = I <ai + bi>Pi = I  aipi + I  bipi = f(P> + g(fJ). 
Similarly, 
Substituting k = i + j, and gathering terms with equal values k, we get 
"2;.2;, (aib)pi+ i 
= L;: (4- aibk- i) Pk = fg(fJ). 
I 
J 
J 
I 
The evaluation map has some of the properties of an isomorphism but it is 
usually neither one-to-one nor onto. It is worth noting the name for such 
maps. 
Definition A map <jJ from a ring A to a ring B is ailed a homomorphism 
(Greek: 
'similar shape') if, for all a, b E  A, 
¢(a + b) = ¢(a) + ¢(b), 
and 
¢(a x b) = <jJ(a) x </J(b). 
From the definitions it is immediate that the term 'isomorphism' is 
synonymous with 'bijective homomorphism'. 
11.2 Evaluating polynomials 
The next proposition gives the underlying reason why the evaluation map 
has good properties. It is also the basis for the most efficient numerical 
method for evaluating polynomials. 
Proposition Iff(x) is considered as a polynomial in E[x] then 
f(fJ) = f(x) mod (x - fJ). 
In particular f(p) 
= 0 
if and only if (x - fJ) divides f(x) in 
E[x]. 

Roots of polynomials 
169 
Proof Using division with remainder in E[x], we get 
f(x) = (x - /J)q(x) + y, 
where y has degree < 1 or y = 0. In any case, y is a constant in E. By 
Proposition 1 1.1 it follows that 
f(/3) = 0 X q(/3) + 
Y = Y. 
The second statement merely reformulates the case when y = 0. 
• 
A notable sequence of the proposition is the following corollary. 
Corollary A polynomial f(x) 
E F[x] of degree n has at most n roots in E. 
Warning 
This is usually false if E does not satisfy the field axioms. For 
instance, in the set of 2 x 2 real diagonal matrices the polynomial x2 - 1 has 
4 roots: 
[-1 o] [-1 
o] . 
0 1 ' 
0 - 1 
The set of quaternions (which are not discussed further in this book) is 
an example where all the field axioms except the law that ab = ba are 
satisfied. In that set the polynomial x2 + 1 has eight roots. 
Proof We shall show that iff(x) has n roots in E, then deg(f(x)) ;;;: n. Let 
/31, .
•
• , Pn 
be distinct roots of f in E, thenf(x) = (x - /31)g(x) and for i > 1, 
f(/3;) = (/3; - /31)g(f3;) = 0. As /31 - /31 # 0, it follows that g(/3;) = 0. Thus 
g(x) has roots /32, •
•
• , 
/3n in E. By induction, deg(g) ;;;: n - 1. Therefore, 
deg(f) ;;;: n. 
• 
1 1.3 The formal derivative 
Over finite fields, we cannot differentiate functions in the usual way by taking 
limits, but we can define the formal derivative of a polynomial by copying 
the formula for real polynomials. This formal derivative retains some of the 
properties of the original. 
Definition 
For f(x) = 2: a1x; e F[x], the derivative f'(x) is defined as 
L iaix•- t. 

170 
Error-correcting codes and finite fields 
Proposition For f(x), g(x) e F[x] and a, b e  F, 
(a) 
(af + bg)'(x) = af'(x) + bg'(x); 
(b) . 
(fg)'(x) = f(x)g'(x) = f'(x)g(x). 
Proof (a) This is immediate from the formula. 
(b) Let f(x) = L a;xi. Then fg = L a;(xig(x)). Therefore by part (a), it is 
sufficient to prove this for f = xk. Let g(x) = L b;xi. 
(xkg(x))' = L (k + i)b;xi+k- l  
= L ib;xi+k- l + L kb;xi+k- l  
= xkg'(x) + kxk- 1g(x). 
• 
The derivative will be useful in certain computations involved in error­
processing Reed-Solomon codes, but it can also be used to check for multiple 
roots in just the same way as the derivative of a real polynomial. 
Definition 
The multiplicity of the root f3 of the polynomial f(x) is the highest 
power n 
for which (x - /3)" divides f(x). If f3 has multiplicity ;:?!: 2, then f3 is 
called a multiple root of f(x). 
Theorem · The multiple roots of f(x) are precisely those that are also roots 
of f'(x). 
Proof Suppose that f3 is a root of f(x). Then f(x) = (x - f3)g(f3). Hence f3 is 
a multiple root of f(x) if and only if it is a root of g(x ). On the other hand, 
f'(x) = (x - f3)g'(x) + g(x). 
As f3 is a root of (x - f3)g'(x), it follows that f3 is a root of g(x) if and only 
if it is a root of f'(x). 
• 
1 1.4 Horner's scheme 
A convenient method for evaluating a polynomial and its derivative at any 
constant f3 was published by Horner in 1819, and is usually known as 
Horner's scheme (but in the electrical engineering literature it is sometimes 
referred to as Goerzel's algorithm). The method was certainly already known 
to Newton, but there are already enough things named after him. 
The method works over any field, and we shall use it in the appropriate 
finite fields. For the sake of clarity the examples we give here will use 

Roots of polynomials 
171 
We will use as our example f(x) = 2x4 + x3 - x + 1, and take f3 = - 1. 
Start by writing the coefficients of the polynomial (including zeros) in 
descending order in a row. 
Example 
2 
1 
0 
-1 
1 
· To the left of the table write {3. Copy the highest coefficient an as the first 
entry bn of the second row. For the later entries put bk = ak + f3bk+ 1. The 
last entry b0 is the value f(/3). 
a4 
a3 
a2 
a1 
a0 
f3 b4 = a4 
b3 = a3 + f3b4 ·b2 = a2 + {3b3 
b1 = a1 + /3b2 
b0 = a0 + f3b1 
Example 
2 
1 
0 
- 1  
1 
- 1  2 
- 1  
1 
- 2 
3 f( - 1)
= 3. 
To find the first derivative repeat the procedure with row b, but end one 
column earlier. Thus cn = bn, ck = bk + f3ck+ 1 for k = n - 1 down to 1. Then 
c1 = f'(/3). 
a4 
a3 
a2 
at 
ao 
f3 
b4 = a4 
b3 = a3 + f3b4 
b2 = a2 + f3b3 
bl = at + f3b2 
b0 = a0 + f3b1 
c4 = b4 
c3 = b3 + f3c4 
c2 ॳ b2 + f3c3 
c1 = b1 + f3c2 
Example 
2 
1 
0 
- 1  
1 
- 1  2 
- 1  
1 
- 2  
3 
2 
- 3 
4 
- 6  
f'( - 1) = - 6. 
For those who would like a proof, here is a sketch. 
Proposition 
Horner's scheme correctly evaluatesf(x) andf'(x) at the place {3. 
Proof First notice that the first calculation can be rewritten as 
f(x) = anx" + · · · + a0 = (x - f3)(bnx"- 1 + · 
· 
· + b1) + b0• 
So b0 is the value f(/3), and differentiating at x = f3 we get 
f'(x) = bnpn- l + · 
· 
· + bl . 
Hence the same calculation for bn, ... , b1 
will give f'(p). 
• 

172 
Error-correcting codes and finite fields 
1 1.5 The minimal polynomial of P 
If E is finite, the powers of f3 e E cannot all be distinct. So f3 is a root of a 
polynomial of the form xm - x" with m > n. An element of an extension field 
E that is a root of a non-zero polynomial over F is called algebraic over F, 
the other elements are called transcendental. As all the fields we shall be 
using will be finite, all elements will be algebraic, but e.g. e and n are 
transcendental over the rational numbers. 
Definition 
If f3 e E is algebraic, then the monic (i.e. highest coefficient = 1) 
polynomial t(x) in F[x] of lowest degree such that t(/3) = 0 is called the 
minimal polynomial of fl. It will be denoted by mpp(x). 
Example F = B, E 
= GF(16). 
Element(s) 
0 
1 
10, 1 1  
6; 7, 12, 13 
2, 4, 9 ,  1 4  
3, 5, 8, 1 5  
Minimal polynomial 
X 
x + 1 
x2 + x + 1 
x4 + x + 1 
x4 + x3 + 1 
x4 + x3 + x2 + x + I .  
Proposition If t(x) = mpp and f(x) E F[x], then there exists a unique 
polynomial g(x) with deg(g) < deg t and g( /3) = f(/3). 
Remark g is the remainder of f divided by t. This means that the set of 
values of polynomials at x = /3, which we denote by F[/3], is in 1 to 1 
correspondence with the remainders in F [x] on division by t. 
Proof If f(x) = t(x)q(x) + g(x) with deg(g) < deg(t), then 
f(/3) = Oq(/3) + g(/3). 
So there is a polynomial of the required type. If g(/3) = h(/3) and g =1= h and 
both have degree less than t, then g - h has f3 as a root and degree less than 
the degree of t, contradicting the fact that t = mpp. 
• 
Important special case f has f3 as a root if and only if t divides f exactly. 
In the next theorem, this is used to show that the minimal polynomial is 
unique. 

Roots of polynomials 
173 
1 L6 Properties of the minimal polynomial 
Theorem (a) The minimal polynomial of an algebraic element P is irreducible. 
(b) If f is a monic irreducible polynomial in F[x] with·P as a root, then 
f = mpp. 
(c) If t = mpp and a = pq where F has q elements, then t = mp ... 
Proof (a) Let t be the minimal polynomial of /3. If t = fg, then 
0 = t(/3) = f(f3)g(f3). 
So f(/3) = 0 or g(/3) = 0. Say f(/3) = 0, then t divides J, so g has an inverse 
and must be a non-zero constant. 
(b) Let t = mpp(x) and let f(x) be a monic irreducible polynomial such 
that f(/3) = 0. Then by Proportion 1 1.4, t divides f But f is irreducible, so 
f = at, where a is a constant. As both t and f are monic a = 1. 
(c) Let q = pk, where p is the characteristic of E. 
By Theorem 10.14, raising 
elements of E to the pth power is an automorphism. Thus raising them to 
the qth power is also an automorphism. By Fermat's little theorem 10.4, 
aq = a for any element a e F. 
Let t = mpp(x) and say t = 2: aixi. Then 
0 = t(/3) = L aipi. 
Hence 
0 = t(f3)q = (L aij3i)q = L aJ pqi = L aipqi = t(f3q). 
By part (b) it follows that t is the minimal polynomial of f3q. 
• 
In the extra section 1 1.12, we shall show that if we start with p and 
successively take qth powers, f3 = /31, •
.
.
 , f3n, /3;+ 1 = py, until we get Pǟ = p, 
then mpp(x) = n (x - /3;). 
. 
Corollary If P is algebraic over J, 
the set of values of polynomials over F at 
x = p, F[/3], is a field and it is isomorphic to F[x]/mPp· 
Construction 
Recall that by Theorem 9.8 
F[x]/f(x) is a field if f(x) is 
irreducible. Given a polynomial f(x) e F, we can construct a field contain­
ing F containing a root of f(x). Let g be an irreducible factor off Consider 
the field F[x]/g(x). It contains the element x which we rename p to avoid 
confusion. Then g(/3) is the remainder of g(x) when divided by g(x), which 
is clearly 0. 
As g(/3) = 0, it follows that f(/3) = 0. 

1 74 
Error-correcting codes and finite fields 
1 1.7 Fields with pn elements 
We are now going to construct a field with q = pn elements for any prime 
p and any power p. The method is based on the following consequence of 
Fermat's little theorem. 
Theorem Let E be a field of q elements and let F be a sub field of E. Then in 
F[x], xq - x is the product of the distinct minimal polynomials of elements of E. 
Proof By Fermat's little theorem, f3 is a root of xq - x. The number of 
distinct roots of xq - x obtained this way is q. So the roots of xq - x are 
precisely the elements of E, and in particular, xq - x has no multiple roots. 
It follows that the polynomials in F [x] dividing xq - x are just the minimal 
polynomials of these elements. Furthermore each of the minimal polynomials 
divides xq - x just once. Thus up to a constant xq - x is the product of the 
minimal polynomials. As all polynomials involved have highest coefficient 
1, the constant must be 1. 
• 
By cancelling out the factor x we obtain a corollary that will be useful in 
Part 3. 
Corollary I 
Under the hypotheses of Theorem 1 1 .7, xq- l  - 1 is the product 
of the minimal polynomials of the non-zero elements of E in F [x]. 
• 
Later we shall use this result with F = B, but for the moment we need the 
special case when F = E. Then the minimal polynomial of f3 is just x - f3 
and so the theorem takes on a particularly simple form. 
Corollary 2 
Let E be a field of q elements. Then in E[x] 
Xq - X = n (x - {3). 
P e E  
This divides the problem of constructing a field of q elements into two 
parts. First find a field in which xq - x 'splits' into linear factors and then 
show that its roots form the field we are looking for. 
1 1.8 The splitting field 
Theorem Let f E F [x ], then there exists a field E containing F over which 
f splits into linear factors. 

Roots of polynomials 
175 
Proof The proof is by induction on the degree of f That just means we 
add one root after another in a systematic way. Enlarge F by adjoining roots 
of f according to the following procedure. 
Step 1 .  If deg(f) = 1, F = E and there is nothing to do. 
Step 2. If deg(f) > 1 and f(x) = (x - f3)g(x) in F, apply the induction 
hypothesis to g(x) to find E 
in which g splits into linear factors. Then E does 
the job for f as well. 
Step 3. If deg(f) > 1 and f has no linear factors. Let g(x) be an irreducible 
factor of f Apply the construction of Section 1 1.6 to obtain F' = F[x]fg(x) 
containing F and the root f3 = x of g(x). Now f has a linear factor over F' 
so go back and apply Step 2 to F'. 
Continue in this way adding roots of f one by one and reducing the 
degree of the polynomial we have to deal with until we have split it up 
entirely. 
• 
11.9 An existence theorem 
Theorem Let F be a field with p elements and q = pn. Then there exists a 
field E 
containing F with exactly q elements. 
Proof By Theorem 1 1 .8 there exists a field K containing F over which 
xq - x splits into linear factors. Let E 
be the set of roots of xq - x in K. 
First we show that E has the right number of elements. The derivative of 
xq - x is qxq- 1 - 1. Now· q is a power of the characteristic of E, so qxq- 1 = 0. 
Thus xq - x has derivative 1. Thus by Theorem 1 1.3, xq - x has no multiple 
roots. Next we shall show that E is a field. To do this we must show it 
contains 0 and 1, is closed under products, sums, negatives and inverses. The 
other laws follow because they hold for any subset of K. 
Certainly Oq = 0 and lq = 1, so 0 and 1 lie in E. Now let f3 and y lie in 
K, so f3q = f3 and yq = y. Now q is a power of the characteristic of E and 
just as in Theorem 1 1.6, we deduce from Theorem 10.14 that raising elements 
to the qth power is an automorphism of E. Hence 
(f3y)q = f3qyq = f3y. 
(/3 + y)q = f3q + yq = f3 + y 
Furthermore, if f3q = f3, then 1/f3q = 1/fJ. 
Finally, if f3q = /3, then (- f3)q = ( - l)q/3. If q is odd ( - l)q = - 1. Otherwise 
the characteristic of -E is 2 and - 1 = + 1 .  In either case ( -f3)q = -[3. • 

176 
Error-correcting codes and finite fields 
1 1.10 Herstein's alternative 
Remark As I. N. Herstein showed in a pretty little note in Amer. Math. 
Monthly (1987) the derivative is not necessary for the proof that xq - x has 
no multiple roots. The reason I have used the derivative is that it provides 
a useful computational simplification for decoding Reed-Solomon codes. 
However, I cannot resist adding Herstein's replacement here. 
Proposition Let E be a field of characteristic p and let q = pn. Then xq has 
no multiple roots in E. 
Proof Let a be a root of xq - x, in E. We must show that (x - a)2 does not 
divide xq - x. Now notice that aq = a, by assumption. Hence 
But q = pn, and we have seen that in a field of characteristic p, 
(a + b)P = aP + bP. 
Hence also 
Thus 
= (x - a)q - (x - a) 
= (x - a)((x - a)9- 1 - 1). 
Now clearly (x - a) does not divide ((x - a)9- 1 - 1) and so (x - a)2 does 
not divide x9 - x. 
• 
1 1.11 Subfields of all orders 
Theorem 1 1.9 .also allows us to show that a finite field has subfields of all 
legitimate orders. 
Theorem Let F be a field of order q = pn, where p is a prime number. Then 
for each k dividing n, F has a unique subfield of order pk. 
Proof That the values of k given above are the only legitimate ones was 
proved in Theorem 10.10. So suppose that k divides n, and let r = pk 

Roots of polynomials 
177 
and l = njk. Then q - 1 = r1 - 1 = (r - 1)(r'- 1  + · 
· 
· + r + 1). We put s = 
r1- 1 + . . . + r + 1 and write this as q - 1 = (r - 1)s. Now 
Xq- 1 - 1 = (x<r- 1)s - 1) = (xr- 1 - 1)(xs- 1 + . . .  + X +  1). 
Hence 
xq - x = (x' - x)(x•- 1 + · · · + x + 1). 
So the polynomial x' - x divides xq - x. 
By Fermat's little theorem F contains a complete set of roots of xq - x. 
Among these must be a complete set of roots of x' - x. By Theorem 1 1.9, the 
latter set forms a subfield G of F with r elements. Since G exhausts the 
available roots of x' - x there can be no other subfield of order r. 
• 
EXTRAS 
1 1.12 A formula for the minimal polynomial 
In this section we shall prove the formula for the minimal polynomial of f3 
given in Section 1 1.6. 
Theorem Let f3 
be an element of a finite field E containing the field F of order 
q. Starting with /3, 
successively take qth powers to get f3 
= /31, •
.
•
 , /3m 
f3i+ 
1 = /31, 
with /3१ 
= /3, 
then mpp = [l (x -f3J 
Proof Let g(x) be the polynomial given by the formula. We shall show that 
its coefficients lie in F. That follows because 
Thus the coefficients ai of g(x) all satisfy af = a1• By Fermat's little theorem 
10.3, F contains q roots of (xq - x). In a field there can be no further roots, 
hence all the coefficients of g(x) lie in F. 
Since g(x) has f3 as a root, g(x) must be a multiple of mpp(x). But since 
all of the roots of g(x) are distinct and are also roots of mpp(x) (by Theorem 
1 1.6c), mpp(x) must be a multiple of g(x). Hence they are equal. 
• 
1 1.13 Summary 
In this chapter we reintroduced the concept of a root of a polynomial. First 
we found the minimal polynomial having a given element as a root, and 
then, exploiting its properties, we showed that every polynomial has roots 
in suitable extensions of its coefficient field. Finally we used this result to 

1 78 
Error-correcting codes and finite fields 
show that there are finite fields of every prime power order. As we already 
know that no other orders are possible, we have now determined the possible 
orders of finite fields completely. There is however still one obvious question 
left unanswered. Are all fields of a given order essentially the same, or is it 
possible to have two fields of the same order but a different algebraic 
structure. That question will be answered in the next chapter. It describes 
how to construct a tool that enables us to perform quick multiplication in 
finite fields, but also to analyse their algebraic structures more closely. 
1 1.14 Exercises 
1 1 . 1  What happens if Horner's scheme is continued for a fourth row 
d, ending another column earlier? Is the answer f"(x) evaluated 
at {3? What about continuing the array in this way until there 
are no columns left to fill? Test your answer on the real polynomial 
x5 + x4 + x3 + x2 + x + 1 .  
1 1 .2 Use the formula of Theorem 1 1 . 1 2  to verify the table of minimal 
polynomials of elements of GF( 1 6) in Section 1 1 .5. 
1 1.3 Let ¢ be an homomorphism of a ring R onto a ring S. Show that ¢ 
is an isomorphism if and only if the only element a of R for which 
¢(a) = 0 is a = 0. 
1 1 .4 Let ¢ be a homomorphism of the field F onto the field G, show that 
¢ is an isomorphism. 
1 1 .5 Let ¢ be an isomorphsim of the finite field F onto the field G; show 
that F and G have the same prime field, and that ¢ acts as the identity 
on that prime field. 
1 1 .6 Let ¢ be an isomorphism of the finite field F onto the field G and let 
H be their common prime field. Show that for any element f3 of F, 
the minimal polynomial of ¢(/3) over H is the same as that of {3. 
1 1 .7 Find the minimal polynomials of all the elements of the fields 
constructed in Exercises 9.7-9.9. 
1 1 .8 Verify that the minimal polynomials found in Exercise 1 1 .6 are all 
binary irreducible polynomials of degree  5, and all (monic) ternary 
irreducible polynomials of degree  3. 
1 1 .9 Let q = p' and u = p• for a prime p. Show that for any field F of 
characteristic p, xq - x divides x" - x in F [x] ifand only if r divides s. 
Deduce that a field of order u contains a subfield of order q if and 
only if r divides s. 

12 
Primitive elements 
You will recall that in our example of GF(16) we introduced 'logarithms' 
for non-zero field elements that could be used like conventional logarithms 
to convert multiplication into addition. This is certainly of practical signi­
ficance, since addition modulo 1 5  is easily implemented on a chip, while 
polynomial multiplication followed by division with remainder (the method 
used to define multiplication) is both more complicated and slower. On the 
other hand, the existence of such logarithms is closely linked to a remarkable 
property possessed by finite fields that is of great theoretical importance. If 
logarithms exist, then every non-zero element of the field is an integer power 
of the element with (X logarithm 1. In the case of GF(16) every non-zero 
element is a power of the element we have denoted by 2. Indeed, choose an 
element x and let its logarithm be y, say x = 9, y = log(x) = 4. Then 
24 = 2 x 2 x 2 x 2. So 
log(24) = log(2) + log(2) + log(2) + log(2) = 4. 
Hence 9 = 2\ and in general x = (XY. The existence of an element whose 
powers produce all the elements of a finite field is perhaps the most crucial 
property of such fields. It is not shared by infinite fields such as the rational 
numbers Q or the real·numbers R. 
12.1 Primitive elements 
Definition A primitive element of a finite field F is an element (X E F, such 
that for every non-zero element f3 E F, f3 = (Xk for some k. 
Once a primitive element a in a field has been found, we can define k to 
be the logarithm to the base (X of f3 
when f3 
= (X
k
. We can then use a table 
of such logarithms to do multiplication rather than work out the whole 
multiplication table. Primitive elements also have very useful properties for 
codes as we shall see later. 
Example The primitive elements of GF(16) are 2, 4, 6, 7, 9, 12, 13, 14. You 
can check this directly for each in tum. Here is the calculation for 2. 

1 80 
Error-correcting codes and finite 
fields 
2° = 1, 2 1  = 1 X 
2 = 2, 22 = 2 X 2 = 4, 23 = 4 X 2 = 8 ,  
24 
= 8 X 2 = 9 ,  25 = 9 X 
2 = 1 1, 26 
= 1 1  X 2 
= 1 5, 27 = 1 5  X 2 = 7 ,  
2 8  = 7 X 
2 = 1 4, 29 = 1 4  X 2 = 5 ,  210 = 5 X 2 = 1 0, 21 1 
= J0 X 2 = 1 3 ,  
212 = 1 3  X 2 = 3, 213 = 3 X 2 = 6, 214 = 6 X 2 = 12. 
Similarly one can check that 3 is not a pnmitive element. 
3° = 1, 3 1 = 1 X 3 = 2, 32 = 3 X 3 = 5, 33 
= 5 X 
3 = 1 5, 34 = 1 5  X 3 = 8, 
35 = 8 X 3 = 1 ,  
36 = 1 X 3 = 2, 37 = 3 X 3 = 5, 38 
= 5 X 
3 = 1 5, 39 = 1 5  X 
3 = 8, 
3 1 0 = 8 x 3 = 1 , 
31 1 = 1 X 3 = 2, 312 = 3 X 
3 = 5, 3 1 3  = 5 X 3 = 1 5, 3 1 4 = 1 5  X 3 = 8 .  
The repeating pattern of these powers I S  clear and ensures that n o  new 
field elements will appear as powers of 3. You should check one of the 
pnmitive elements and one non-primitive element for yourself. We shall 
always use the element 2 as our primitive element. The next proposition 
sums up the results of these calculations. 
Proposition 
IfF has q elements then a is a primitive element ofF if and only 
if the powers a, a2, a
3
, .
.
.
 , aq- 1 = a0 
= 1 are distinct and produce all non-zero 
elements of F. 
Proof From the little Fermat theorem we know that cxq - 1 = 1 .  Hence for 
k ;?:  q - 1, cxk 
= ak -q+ l. Hence ak 
= o:k mod (q- 1) (here division with remainder 
is 
the ordinary division in the integers). Hence the list 
contains all the elements of F that are powers of ex. 
Now suppose that a is primitive; then the list must contain all the non-zero 
elements of 
F. As the list has q - 1 terms and F has q - 1 non-zero elements, 
the terms in the list must all be distinct. 
If, on the other hand, a is not primitive there must be at least one non-zero 
element of F that does not appear on the list. So the list contains at most 
q - 2 distinct entries. Therefore at least one entry must appear twice. 
• 
Definition 
We define the value k with 0 ::::; k :::; q - 2 and f3 = ak to be the 
(discrete) logarithm of f3 to the base a. 
It follows 
that the logarithm of py 
is the remainder of 
the sums of the 
logarithms of {J 
and y when they are 
divided by q - I .  

Primitive elements 
181 
Example The top row of the table of GF(16) gives the logarithms 
to the 
base 2. 
To multiply 10 by 13 take their logarithms, 1 0  and 1 1. The sum of these 
is 21. Its remainder modulo 1 6  - 1 
= 1 5  is 6. The number with logarithm 6 
is 1 5. Thus 10 x 13 = 1 5. 
12.2 Existence of primitive elements: preliminaries 
We shall now prove that primitive elements always exist. The proof is not 
difficult, but requires a little care and patience. Indeed several textbooks 
contain erroneous versions of it (see Exercise 12.7). To set it up we need a 
definition and two lemmas. 
Definition Let F be a field of order q. If 0 =/; f3 
E F, then the order of f3 is 
the smallest positive power k such that f3k = 1. We denote it by ord(/3). 
A 
primitive element is characterized by the fact that ord(a) = q - 1. 
The definition suggests what is needed to find a primitive element of a 
field F: search for an element of highest possible order in F, and then show 
it is a primitive element. To enable us to do that we must establish 
the basic properties of the order function we have just defined. 
Lemma For any element f3 of a finite field F of order q, 
(a) 
pn = 1 if and only if ord(/3) divides n; 
(b) ord(/3) divideܐ q - 1 ;  
(c) if ord(/3) = m and d is the highest common factor of m and n, then 
ord(/3n) = m/d. In particular, if.n divides m, then ord(f3n) = m/n. 
Proof (a) Certainly if n = s ord(/3), then 
{3" 
= ({3ord(P))s 
= 1 s 
= 1. 
Conversely, suppose pn 
= 1 and let n = s ord(/3) + r, with 0 ॲ r < ord(/3). 
Then 
By definition, ord(/3) 
is the smallest positive power of f3 equal to 1. Hence 
r = 0. 
(b) By Fermat's little theorem, pq-t 
= 1, hence by part (a), ord(/3) divides 
q - 1 . . 
(c) Obviously, ( f3n)mld = (pm)nld = (l)n/d = 1 . We claim that m/d is the 
smallest positive power for which that holds. As the highest common factor 

182 
Error-correcting codes and finite fields 
of m and n is d, we have 
d = urn +  vn 
for some u and v. Now suppose (f3")k = pnk = I .  Then 
k = umk/d + vnkjd. 
Since m = ord(/3) it follows that m divides nk. Hence m/d divides nkfd. So 
m/d divides both terms on the right-hand side of the equation for k. Thus 
m/d divides k, proving our claim. 
• 
12.3 Elements of large order 
In general the order of the product of two elements f3 and y is at most equal 
to the least common multiple of their individual orders. But it can be a lot 
smaller than this upper bound. For instance if y = p- 1, then the orders of 
f3 and y are equal, but however large they are, the order of f3y = 1 is 1. This 
shows that it is not in general true that ord(/3) divides ord(f3y). There is an 
example in the exercises to show that this kind of thing can happen even 
when y is not as obviously related to {J as this. 
In searching for a primitive element we are looking for elements with large 
orders. We would like to increase the order of a starting element by 
multiplying it by others; so this collapse is undesirable. The next lemma gives 
a condition under which we can still increase the order of an element in this 
way. 
Lemma If {3, y E F and ord(p) = m and ord(y) = n and the highest common 
factor ofm and n is I ,  then ord(f3y) = mn. 
Proof Certainly ( f3y)m" = pm•gm• = 1, by Lemma 12.2(a). So we need 
only show that no smaller positive power of f3y is 1. Suppose (f3y)k = 1. 
Then 
Hence m divides kn, say kn = am. Similarly, 
1 = ( {3y )km 
= pkmykm 
= ykm . 
Hence n 
divides km, say km = bn. 
Now m and n have highest common factor I . That allows us to apply the 
!-trick. From Euclid's algorithm it follows that I = urn + vn for some 

Primitive elements 
integers u and v. Hence 
k = k(um + vn)(um + vn) 
Thus mn divides k. 
= u2mkm + 2uvkmn + v2nkn 
= u2mbn + 2uvkmn + v2nam 
= (u2b + 2uvk + v2a)mn. 
12.4 Existence of primitive element: proof 
183 
• 
We are now ready to state and prove the theorem we have been working 
up to. 
Theorem The theorem of the primitive element. Let F be a finite field of 
order q. Then F contains a primitive element a, that is F contains an element 
a such that all the non-zero elements fJ e F can be represented in the form a" 
for suitable powers n. 
Proof The proof we shall give is in three stages. We select an element a in 
F of largest possible order. First we show that for any element fJ in F the 
order of fJ divides the order of a. From that we deduce that the order of a 
must be at least q - 1. Finally we conclude that the order is exactly q - 1. 
Suppose, therefore, that a # 0 has largest possible order in F, and let fJ # 0 
be an element of F. Let ord(a) = n and ord({J) = m. If m does not divide n, 
there must be a prime p for which the highest power x = pk dividing m is 
greater than the highest' power y = p1 dividing n. Let y = aY, which by Lemma 
1 2.2(c) has order nfy not divisible by p, and let o = pmtx, 
which has order x. 
Now x and n/y are relatively prime. So by Lemma 12.3, yo has order nxfy > n, 
contradicting our choice of a. Thus we have established that ord({J) must 
divide ord(cx) = n. 
This implies that for every non-zero element fJ e F, {J" = 1. Thus the 
polynomial x" - 1 has all the non-zero elements of F as its roots. But in a 
field a polynomial of degree n has at most n roots, so q - 1 ʝ n. 
On the other hand by the little Fermat theorem, cxq- l = 1. Therefore, by 
definition n = ord(cx) ʝ q - 1. Hence n = q - 1 as required. 
• 
12.5 Discrete logarithms and addition 
Given a primitive element ex of a finite field F of order q, we can associate 
with each non-zero element fJ of F its discrete logarithm, that is the smallest 

184 
Error-correcting codes and finite fields 
power n such that rx" = fl. A table of these powers enables us to perform 
multiplication by adding powers. 
Example In the table of GF(16) the logarithms at the head of the table are 
the powers of the primitive element 2 such that 2" = fl. To multiply 9 = 24 
by 6 = 2 1 3, add the powers to obtain 21 7 
= 22 = 4. 
A problem with this method of multiplication is the need for table look-up 
to find logarithms and antilogarithms. One can avoid that by storing the 
elements of F in the form of logarithms but then addition is no longer 
straightforward. As an aside, we describe here a method, due to Zech, of 
representing a finite field by using the discrete logarithms to denote non­
zero field elements. Multiplication is then just addition modulo q - 1 .  
Addition is performed by means of an auxiliary table. The element 0 is · 
assigned the special discrete logarithm oo ,  so that the same rules apply to 
it. The method is interesting, but in practice its disadvantages are greater 
than its advantages. 
Definition Zech logarithms. Let F be a finite field of order q with primitive 
element rx. Represent the elements of F by their discrete logarithms, adding 
a special discrete logarithm oo to represent 0. The Zech logarithm Z(n) of 
n E { oo, 0, . . .  , q - 1 }  is the discrete logarithm of 1 + rx", where rx "'  = 0. 
The reason for the name is that Julius Zech (1849) published a table of 
these logarithms (which he called 'addition logarithms') for doing arithmetic 
in Zfp. These were, I think, intended for number-theoretical calculations. 
Example A table of Zech logarithms for GF(1 6) using rx = 2 is 
Element: 
Zech logarithm: 
00 
0 1 2 3 4 5 6 
7 8 9 10 12 13 14 
0 00 12 9 4 3 10 8 13 6 2 5 
1 
7 1 1  
Notice that this table is its own antilogarithm table, because GF(1 6) has 
characteristic 2. 
Proposition Addition using Zech logarithms. If the field elements f3 and 
y have discrete logarithms m and n then f3 + y has discrete logarithm 
n + Z(m - n), where Z(m - n) is the Zech logarithm of m - n. 
Example To add 9 = 24 and 5 = 29 by this method we find that the 
discrete logarithm of the sum is 4 + Z(5) = 14. Thus confirming that 
5 + 9 = 2 1 4  = 1 2. 
Note that if you are using Zech logarithms the only data in the calculation 
would be 4, 9 and Z(5) andthe answer would be 14. We have only translated 
the sum into normal notation to check that it is correct. 

Primitive elements 
185 
Proof Z(m - n) is the logarithm of ( 1  + pjy) 
and thus n + 
Z(m - n) is the 
logarithm of p(l + pjy) 
= p + y. 
• 
You can see that addition requires two conventional additions (modulo 
q - 1) and one table look-up, whereas using discrete logs for multiplication 
with elements in their standard representation requires three table look-ups 
(of which one is in reverse) and one conventional addition (modulo q- 1). 
It does appear that Zech addition is more economical than logarithmic 
multiplication, but as addition is a more basic operation than multiplication 
the gain of efficiency in multiplication will not normally outweigh the loss 
for addition. 
12.6 Primitive polynomials 
It is possible to determine whether an element of F is primitive or not from its 
minimal polynomial. Thus primitive elements of a field fall into classes 
according to their minimal polynomials. Anticipating the proofs of these 
facts we make the following definition. 
Definition Let E be an extension field of F. A polynomial in F[x] is called 
primitive for E if it is the minimal polynomial of a primitive root of E. A 
polynomial in F[x] which is primitive for some extension field E is called 
primitive. 
There is another use of the term primitive polynomial in the mathematical 
literature, so if you are reading a book on general algebra and you come 
across the term, check. whether it has the same meaning as here. If finite fields 
form a major topic of the book, that will probably be the case. 
Note also that the choice of extension field E is important in this definition. 
If a polynomial is the minimal polynomial of a primitive element of E and 
F s:; E c: G, then the element will not be a primitive element of G, and so 
the polynomial will not be primitive for G. There are also irreducible 
polynomials that are never primitive such as the minimal polynomial of 3 
in GF(16). 
Proposition IfiFI = q 
and lEI = qn 
= r + 1, then a monic polynomialf(x) is 
primitive for E if and only if f(x) is irreducible, and f(x) divides x' - 1, but 
does not divide xm - 1 for any m < r. 
Proof Over E the polynomial x' - 1 splits into linear factors. So any 
polynomialf(x) dividing x' - 1 has a root in E. If f(x) is also irreducible, 
then it must be the minimal polynomial of any of its roots. 

186 
Error-correcting codes and finite fields 
Let a havef(x) as its minimal polynomial. Then a" = 1 if and only if a is 
a root of x" - 1, and that holds if and only iff(x) divides x" - 1. Now, a is 
a primitive element of E if and only if a has order r. That is the same as saying 
a is a root of x' - 1 but not of xm -1 
for any m < r. 
The statement follows. • 
Example Primitive polynomials for F = B and E = GF(16) 
X 
X + 1 
x2 + x + 1 
x4 + x + 1 
x4 + x3 + 1 
does not divide x1 5 - 1 
divides x - 1 
divides x3 - 1 
x4 + x3 + x2 + x + 1 divides x5 - 1 
not primitive. 
primitive for B but not for E. 
primitive for GF(4) but not for E. 
primitive for E. 
primitive for E. 
not primitive. 
Corollary lff(x) is a primitive polynomial in F[x] and F has order q, then 
the smallest value r for which f(x) divides x' - 1 has the form r = q" 
- 1. 
If f(x) is primitive for E over F, then all its roots lie in E and they are all 
primitive elements of E. 
• 
Example The fact that 2 is a primitive element of GF(16) now automatically 
gives the primitive elements 4, 9, and 14. The field has two classes of primitive 
roots, namely the ones above and the roots of x4 + x + 1 :  6, 7, 12 and 13. 
1 2.7 Isomorphism of fields of same order 
Our discussion of the theory of finite fields is almost complete. There remains 
only one grand final theorem, namely that there is essentially only one field 
of any legitimate order p". To be more precise we shall show that if two finite 
fields have the same order, then there is an isomorphism from one to the 
other. Thus the two fields are algebraically indistinguishable. 
Theorem Let E and K be two finite fields with lEI = IKI = q = p", where p 
is a prime number. Then E and K are isomorphic, that is, there 
is a one-to-one 
map of E onto K that preserves all the arithmetic operations. 
Proof Let F = Z/p which· is contained in both fields. Let r:t. be a primitive 
element of E with minimal polynomial f(x). Then f(x) divides xq- l  -l, 
which splits into linear factors in K. Thus f(x) has a root f3 in K. By 
Proposition 12.6, f3 is a primitive element of K. Now E = F[a] and 
K = F[/3]. But by Corollary 1 1.6 both these are isomorphic to F[x]/f(x). 
Hence they are isomorphic to each other. 
• 
Such a short proof for such a big theorem! 

Primitive elements 
187 
12.8 Factorization of xq - x 
From Theorem 12.7 we can derive a further result that confirms the 
calculations of Exercises 1 1.7-8. 
Theorem Let F be a finite field of order q and let n = qk. Then in F[x] the 
polynomial x" - x is the product of all (monic) irreducible polynomials of degree 
dividing k. 
Proof Let E be a field of order n containing F (such a field exists by 
Theorem 1 1.9. We know from Corollary 1 1.7.1 that x" - x is the product of 
all minimal polynomials of elements fJ of E. If the polynomial has degree l 
then the field F[[J] has q1 elements and by Theorem 10.10, qk must be a 
power of q1• Thus these polynomials are irreducible polynomials of degree 
dividing k. 
We must show that every such irreducible is a minimal polynomial of an 
element of E. So let f(x) be irreducible of degree l dividing k. Let G be the 
field F[x]/.f(x), which has order q1• By Theorem 1 1.11, E has a subfield H of 
order q1
• Now by Theorem 12.7, G and H are isomorphic. Therefore H 
contains a root of f(x). This root y lies in E and has f(x) as its minimal 
polynomial. Thus f(x) occurs as a factor of x" - x. As this holds for all 
irreducibles of suitable degree, the theorem follows. 
• 
EXTRAS 
12.9 Generators of field extensions 
In discussing the theoretical properties of classical Goppa codes in Chapter 
20 we shall need an estimate of the number of irreducible polynomials of a 
given degree over a finite field. The appropriate place to calculate thͭt 
number is in this chapter. We consider a field F of order q and an extension 
field E of order q". We know that there exist primitive elements of E, but 
now we require a slightly weaker concept. 
Definition An element a E E is called a generator of E over F if E =  F[a]. 
Example For F = B and E = GF(16) the element 2, being primitive, is a 
generator, but notice that 3 is also a generator even though it is not primitive, 
because 2 = 35 
+ 3 
and so 2 e F[3]. 

188 
Error-correcting codes and finite fields 
Proposition If F  £ E are fields of order q and q" respectively, then IX E E is 
a generator if and only if the minimum polynomial of IX over F has degree n. 
Proof Let the minimum polynomial of IX be f(x). If deg(f) = n then F[IX] 
being isomorphic to F[x]/.f(x) has q" elements. Thus F[1X] must be the whole 
of E. Conversely if F[IX] = E then it has q" elements. Thus F[x]/f(x) has q" 
elements and therefore f(x) has degree n. 
• 
12.10 Counting generators 
Now we shall estimate the number of generators of E over F. 
Proposition Let F £ E be fields of order q and q" and let m = n/2 + 1, 
then 
E has at least q" - qm generators. 
Examples If n ::; 2, then this estimate only gives 0, but in those cases one 
can directly count the number of generators (see Exercise 12.9). If F = B 
and E = GF(16), then n = 4 and the estimate gives 16 - 8 = 8 generators, 
in fact GF(16) has 12 generators: the only non-generators being 0, 1, 10 
and 1 1 .  
Proof An element IX is a non-generator if and only. if IX lies in a proper 
subfield of E that contains F. For each divisor d or n, E contains precisely 
one subfield of order qd. Thus the number of elements IX that do not lie in 
any proper subfield containing F is 
n/2 
q•/2 
_ 1 
q" - I qd 
 q" - I q' = q" - q 
ၖ q" - qm + q. 
• 
din 
r= l 
q
- 1 
d<n 
12.11 Counting irreducible polynomials 
By moving from field elements to their minimum polynomials we can use 
Proposition 12.10 to count irreducible polynomials. 
Theorem Let F be a field of order q, then for n > 2 there are at least 
(q" - q.Jq")/n irreducible polynomials of degree n in F[x]. 
Example Taking q = 2 we get the following numbers I of irreducible 
polynomials for small values of n: 
n: 
3 
4 
4 
6 
7 
8 
9 
10 
/: 
0.78 
2 
4.1 
8 
1 5.05 
28 
5 1 .8 
96 

Primitive elements 
189 
Of course since the number of polynomials is an integer, fractions must 
be rounded up. The important point is that this number increases strongly 
with n. 
Proof Let E be a field of order qn containing F. Then E has at least 
q" - q.jq" generators over F. We consider their minimum polynomials. 
These are all irreducible and have degree n. Now each such polynomial has 
at most n roots in E, so at most n generators can have the same minimum 
polynomial. Thus the number of these polynomials must be at least 
(q" - q.jq")/n as claimed. 
• 
12.12 Summary 
This chapter was concerned with primitive elements of fields. We proved 
that every finite field has such elements. Then we discussed how they can be 
used to introduce discrete logarithms, which simplify the implementation of 
field multiplication. The existence of primitive elements has profound 
consequences for the arithmetic of finite fields, one example of which is that 
fields of the same order are necessarily isomorphic. 
12.13 Exercises 
1 2. 1  Verify the statement about the primitive elements of GF(16) in Section 
12.1: show that they are precisely the elements 2, 4, 6, 7, 9, 12, 13, 14. 
12.2 Show that a field is finite if and only if it has a primitive element. 
12.3 Show that a field of order 1024 always contains elemens {J, y, of orders 
33 and 93. For such elements {111 = y31 or {111 = y-31• We assume that 
{111 = y31. Verify that in that case ({Jy)341 = {J11y- 1 ·= 1. But as 
3 does not divide 341, deduce that the order of fly is not a multiple 
of ord({J) or of ord(y). 
12.4 Let m and n be relatively prime integers. Show that the least common 
multiple of m and n is mn. 
12.5 Let m and n have HCF = d. Show that the least common multiple of 
m and n is mn/d. (Hint: Use the 'd-trick' variant of the 1-trick.) 
12.6 Show that if m and n are integers and m divides kn arid n divides km, 
. then k is a common multiple of m and n, and hence a multiple of the 
least common multiple of m and n. 
12.7 What is wrong with the following 'proof' of the theorem of the 
primitive element? 
Let a be an element in F of largest possible order n. Suppose that 
fJ is a non-zero element of F such that ord({J) = m does not divide n. 
Let d = HCF(m, n). Then m/d # 1 and n and m/d are relatively prime. 

190 
Error-correcting codes and finite fields 
Thus a(f3d) has order nm/d > n contradicting the assumption. Hence 
for all elements non-zero {3, ord(/3) divides ord(a). The rest of the proof 
is the same as the one given in the text. All non-zero elements of F 
are roots of x" - 1 .  Thus, if IFI = q, n * q - 1 .  But by Fermat's little 
theorem, ord(a) divides q - 1. Hence n = q - 1. 
• 
12.8 Verify that the rules for multiplication and addition using discrete and 
Zech logarithms given in Section 12.5 produce the correct answers 
when applied to 0 + {3, and 0 · {3. 
12.9 Let F be a field of order q and E an extension field of F of 
order q2• How many elements of E are generators over F? 
12.10 Let a be a primitive element of a finite field F of order q. Show that 
ak is a primitive element of F if and only if HCF(k, q - 1) = 1. 
12. 1 1  Euler's totient function c/J(n) counts the number of m such that 
1 X m < n and HCF(m, n) = 1 .  Show that the number of primitive 
elements of a field of order q is ¢(q - 1). 
12.12 If F is a field of order p\ where p is prime, show that the primitive 
elements of F fall into classes of size k, where two primitive elements 
are in the same class if they have the same minimal polynomial. 
Deduce that k divides ¢(pk - 1). 
12.13 Show that every element a #  0, 1 of the field of order 32 is primitive 
and, hence or otherwise, that the only proper subfield of a field of 
order 32 is the binary field. 
12.14 Find primitive elements of Z/p for p = 5, 7, 1 1, 13, 17, 19, 23. 
12.15 Deduce the formula of Exercise 9.5 from Theorem 1 2.8, that is, if the 
number of irreducible binary polynomials of degree n is I(n), then 

Appendix PF 
Polynomials over a field 
The purpose of this appendix is to give a formal outline of the theory of 
polynomials. It is intended for reference rather than for study. A full 
exposition can be found in Cohn (1982). Throughout the appendix F is a 
fixed field. We begin with polynomials in a single indeterminate x. Initially 
the full set of field axioms is not required, so we shall use an integral 
domain R. 
PF.l Polynomials defined 
Definition The set R[x] of polynomials in the indeterminate x is the set of 
formal sums f(x) = L a1x1 such that 
1. the summation index i ranges from 0 to oo, 
2. the coefficients a1 lie in R, and 
3. there exists an n such that for all i > n, a1 = 0. 
The element a1 is called the coefficient of x1 in f(x). The sum is really finite 
and we can write 
· f(x) = anx!' + · · · + a1x + a0 
if a1 = 0 for i > n. 
Let f(x) = L a1x1 and g(x) = L b1x1• Define 
ck 
= :L aibj. 
where i,j = 0, .
.
.
 , k and i + j = k. Then 
(f + g)(x) = L (a1 + b1)x1 
and 
PF.2 Constants 
Definition A polynomial with all its coefficients a1 = 0 for i > 0 is called a 
constant. Constant polynomials will be identified with their coefficients a0• If 
there is a possibility of confusion, the constant will be underlined when it is 
being considered as a polynomial. 

192 
Error-correcting codes and finite .fields 
Proposition Polynomial addition and multiplication in R[x] restricted to 
constants are the same as ring addition and multiplication in R. 
• 
PF.3 Arithmetic 
We must establish that addition and multiplication of polynomials are 
well-behaved. The proof is a tedious verification of details. 
Proposition Let R be a ring and R[x] the set of polynomials over R. With 
polynomial addition and multiplication R[x] forms a commutative ring. 
Proof It is necessary to verify the following axioms for Section 3.3: A l-A4, 
Ml-M3, and Dl-D2. Let f(x) = L a;xig(x) = L b;xi, and h(x) U L c;xi. 
Axioms AJ-A4 These axioms aU reduce to axioms for ring addition of 
individual coefficients (the zero polynomial is the constant 0). 
As an example 
we prove the associative law Al. 
((f + g) + h)(x) = L ((a; + bJ + cJx; 
= L (a; + (b; + c;))x; 
= U + (y + h))(x). 
Axioms Dl-D2 We prove Dl; 02 is proved analogously. Let 
dk = a0(bk + ck) + 
· 
· 
· + ak(b0 + c0), 
rk = a0bk + · 
· 
· + akb0, 
and 
Then by the distributive law in R dk = rk + sk. Hence 
(f x (g + h))(x) = L d;xi, 
= (fg + fh)(x). 
Axioms M 1-M3 
Axioms M2 (with the constant 1 as identity) and M 3  
are immediate from the formula. It remains to prove M l .  
For all / let 
and 
d1 = a0b1 + 
· 
· 
· + a1b0, 
e1 = b0c1 + 
· 
· 
· + b1c0, 
r1 = a0e1 + 
· 
· 
· + a1d0• 
Then r1 = L a;bick, where i,j, k = 0, . . .  , l. and i + j + k = l. 

Appendix PF Polynomials over a field 
By symmetry it follows that r1 = d1c0 + 
· 
· 
· + d0c1• Therefore 
(f(gh))(x) = L r1x1 = ((fh)h)(x). 
PF.4 Degree 
193 
• 
Definition The degree of a non-zero polynomial f(x) = I a;xi E R[x], 
denoted by deg(f(x)), is the maximum i for which a; i= 0. The degree of the 
zero polynomial is defined to be - oo .  
Proposition Let R be an integral domain, and let f(x) = I a;xi and g(x) = 
I b;xi 
be two polynomials in R[x], then 
(a) deg((f 
+ g)(x))  max{deg(f(x)), deg(g(x))}, and 
(b) deg(fg(x)) 
= deg(f(x)) + deg(g(x)). 
Proof Let n = deg(f(x)) and m = deg(g(x)). 
(a) Suppose that n ;:;. m. Then for k > n, the coefficient ak + bk of xk in 
(f 
+ g)(x) is 0. Hence deg((f + g)(x)) ř n. 
(b) The coefficient of xm+n in (fg)(x) is I a;bi, where i + j = n + m. Now 
if i > n the 
ai = 0, and if i < n then j = n + m - i > m, so that bi = 0. Thus 
the sum reduces to anbm. By hypothesis an i= 0 and bm i= 0. Hence anbm i= 0. 
Thus deg(fg(x)) * m + 
n. 
On the other hand, if k > m + n, then the coefficient of xk in (fg)(x) is 
L 
a1 bi, where i + j = k. Now if i > n the a; = 0, and if i  n thenj = k - i > m, 
so that bi = 0. Thus all the terms of the sum are 0. Hence deg(f g(x))  m + n. 
• 
• 
PF.5 Domain property carries over 
Theorem If 
R is an integral domain, then R[x] is an integral domain. 
Proof The only axiom that requires proof is the cancellation law MS. If 
f(x) 
i= 0 and g(x) i= 0, then deg(f(x))  0 and deg(g(x))  0. Therefore by 
Proposition PF.4, deg(fg(x)) 
* 0. Thus fg(x) i= 0. 
• 
PF.6 Division with remainder 
Theorem Let F be a field and let f(x) and g(x) i= 0 be two polynomials in 
F[x], then there exist unique polynomials q(x) and r(x) such that 
f(x) = q(x)g(x) + r(x), 
and deg(r(x)) < deg(g(x)) . 

194 
Error-correcting codes and finite fields 
Proof Uniqueness. Suppose there are also p(x) and s(x) satisfying the same 
conditions. Then 
0 = f(x) - f(x) = q(x)g(x) + r(x) - p(x)g(x) - s(x). 
Hence 
(r 
-s)(x) 
= (p - q)(x)g(x). 
Therefore 
deg(g(x)) > deg((r - s)(x)) = deg((p - q)(x)) + deg(g(x)). 
Thus deg((p - q(x)) < 0. So p(x) = q(x) and therefore also r(x) = s(x). 
Existence. Consider the set S of all polynomials of the form h(x) = f(x) -
q(x)g(x) where q varies among all polynomials in F[x]. Choose r(x) in this 
set with the smallest possible degree. We must show deg(r(x)) < deg(g(x)). 
Suppose n = deg(r(x)) Á m = deg(g(x)) and let r(x) = I a;x; and g(x) = 
I b;x;. Then s(x) = r(x) - (a./bm)x•-mg(x) is in S. Furthermore deg(s(x))  n, 
because s(x) is the difference of two polynomials of degree n. Finally, the 
coefficient of x" in s(x) is a. - (a./bm)bm = 0. Thus deg(s(x)) < n. That 
contradicts the hypothesis that r(x) had the smallest possible degree. 
Therefore the assumption that deg(r(x)) * deg(y(x)) is untenable and the 
theorem is proved. 
• 
EXTRAS 
PF.7 Polynomials in two indeterminates 
In Part 4 we shall require polynomials in two indeterminates for the 
definition of geometric Goppa codes. Polynomials in two indeterminates 
share most of the properties of polynomials in one indeterminate. They form 
an integral domain containing F and indeed F[x] as well, but it is not always 
possible to divide one polynomial by another leaving a remainder of smaller 
degree. We shall also need the concept of quotient fields of integral domains 
in that part, to enable us to use rational functions. These topics will be 
sketched here. 
Definition The set F[x, y] of polynomials in the indeterminates x and y is 
the set of formal sums f(x) = I aijxiyi, such that 
1. the summation indices i and j range from 0 to oo, 
2. the coefficients a;i lie in F, and 
3. there exists an n such that for all i,j with i + j > n, a;i = 0. 
The element aii 
is called the coefficient ofxiyi in f(x). The sum is really finite. 

Appendix P F Polynomials over a field 
Let f(x, y) = L aiixiyi and g(x, y) = L biixiyi. Define 
Ckz = I  aiib<k-i)(l-iJ• 
where i = 0, . . .  , k and j = 0, . . .  , l. Then 
and 
PF.S Constants 
195 
Definition A polynomial with all its coefficients aii = 0 for i + j > 0 is called 
a constant. Constant polynomials will be identified with their coefficients a00. 
If there is a possibility of confusion, the constant will be underlined when it 
is being considered as a polynbmial. Similarly, a polynomial with aii = 0 for 
all j > 0, can be identified.witha polynomial f(x) = L a;0x; in F[x]. 
Proposition Polynomial addition and multiplication in F[x, y] restricted to 
constants are the same as field addition and multiplication in F. 
Similarly, restricted to polynomials with coefficients aii = 0 for all j > 0, they 
correspond to the operations of F[x]. 
• 
PF.9 Arithmetic 
We must establish that addition and multiplication of polynomials are well 
behaved. The proof is simplified by the fact that we earlier allowed our 
coefficients to lie in an integral domain. 
Proposition Let F be a field and F[x, y] the set of polynomials in two 
indeterminates over F. With polynomial addition and multiplication F[x, y] 
forms an integral domain. 
Proof By sorting terms first by powers of y and then by powers of x, we 
can consider a polynomial in F[x, y] to be a polynomial in R[y ], where 
R = F[x]. Now, R is an integral domain by Propositions PF.3 and PF.5. 
So by using these propositions again it follows that R[y] is an integral 
domain. 
• 
PF.lO Degree 
Definition The degree of a non-zero polynomial f(x, y) = I a;ixiyi E F[x], 
denoted by def(f(x)), is the maximum n for which there exist i,j with 
i + j = n 
and 

1 96 
Error-correcting codes and finite fields 
The degree of the zero polynomial is defined to be 
- oo. We shall also 
make use of the partial degrees 
of f(x, y). The degree in y of f(x, y) degy(f(x, y)) 
is its degree when considered as a polynomial in R[y], with R = F[x]. The 
degree in x is defined similarly. 
The main facts about partial degrees of f(x, y) have already been 
established in Proposition PF.4. The full degree has similar properties. 
Proposition 
Let f(x, y) = I. aiJxiJJ and f(x, 
y) = I.  aiJxi}J be t\\'O polynomials 
in F[x, y]; then 
(a) 
deg((f + g)(x, y)) 
ॱ max{deg(f(x, y)), deg(g(x, y))}, 
(b) deg(f g(x, y)) 
= deg(f(x, y)) 
+ deg(g(x, y)), and 
(c) deg(f(x, y)) 
X deg..(.f(x, y)) + degy(.f(x, y)). 
Proof Let n = deg(f(x)) and m = deg(g(x)). 
(a) Suppose that n ;;:. m. Then for k > n, and i + j = k, the coefficient 
aiJ + bii of xiyi 
in (f + g)(x) is 0. Hence deg((f + g)(x))  n. 
(b) Let i + j ='n and aii # 0, k + I = m and bk1 # 0. Then by the same 
argument as in Proposition PF4(b), the coefficient of xi+kyi+t in (fg)(x) is 
aiibkl· Now if i + j + k + l > m + n, then i + j > n or k + l > m. So aii = 0, 
or bk1 = 0. Thus aiibk1 = 0. Hence deg(fg(x)) = m + n. 
(c) If i > degx{f) or j > degy(f), then aii = 0. 
• 
PF.ll Rational functions 
In Part 4, we shall also need to use rational functions, which are simply 
'fractions of polynomials'. The construction of fractions can be mimicked 
for any integral domain, so we conclude the Extras of this Appendix with a 
description of it. I think the reader will agree that using an abstract integral 
domain D is preferable to doing the calculations for polynomials in two 
indeterminates. 
PF.12 The field of fractions 
Construction The field of fractions of an integral domain D. 
Step 1 
Let S be the set of pairs (a, b) with a, b e D, b # 0. For mnemonic 
convenience we denote the pair by ajb, and call it a fraction. We shall call 
a its numerator and b its denominator. 
The main difficulty in constructing a field of fractions is that different 

Appendix PF Polynomials over afield 
197 
fractions may denote the same value (for example, ̭ = t). It would be nice 
to cancel out common factors, but that would make composite fractions 
invalid. So it is better to allow them, but modify our definition of 'equality '. 
Step 2 Define two fractions ajb and cjd to be equivalent and write ajb = cjd 
if ad = be. 
The penalty for this approach is that from now on we must ensure that 
everything we do remains invariant if we change a fraction to an equivalent 
one. Thus t + ! must give the same answer as t 
+ !. The technical term for 
this is that our operators must be well-defined. 
Step 3 Definition of addition and multiplication of fractions 
ajb + cjd = (ad + bc)jbd 
ajb x cjd = acjbd. 
PF.l3 Arithmetic for fractions 
Proposition Addition and multiplication of fractions are well-defined, and the 
results are valid fractions. 
Proof Suppose ajb = ujv and cfd = xfy; then 
av = bu 
and 
cy = dx. 
Hence acvy = bdux, so acjbd = uxjvy. 
Furthermore, (ad + bc)vy = avdy + bvcy = budy + bvdx = bd(uy + vx). So 
(ad + bc)jbd = (uy + vx)jvy. 
To prove that the results are valid fractions, we must check that their 
denominators are non-zero. But by assumption b i= 0 and d i= 0, and as we 
have assumed that D is an integral domain, it follows that bd i= 0. 
• 
PF.l4 Fractions form a field 
Theorem The set of fractions defined by these rules forms a field. 
Remark 
This field is called the field of fractions or sometimes quotient field 
of the integral domain D. The original example is the field of rational 
numbers Q obtained from the ordinary integers Z. More important in our 
applications will be 
fields of rational functions, obtained from polynomial 
rings. 

198 
Error-correcting codes and finite fields 
Proof We must verify the full set of axioms from Section 3.3: A1-A4, 
M1-M4, and 01-02. 
We use three fractions ajb, cjd, elf without further ado. 
Al To check (ajb + c/d) + ejf = ajb + (c/d + ej f), verify that both sides 
produce (adf + cbf + ebd)/bdf. 
A2 To check ajb + cjd = cjd + ajb, note that ad + be = cd + da, and 
bd = db in D. 
A3 The zero is 0/1. 
A4 The negative 0 is (-a)/b. Because ajb + ( -a)jb = Ojbb, and Ojbb = 0/1 
because 0· 1 = bb ·O. 
Ml To check ((ajb)(c/d))(e/f) = (a/b)((c/d)(e/f)), verify that both sides 
produce acejbdf. 
M2 To check (a/b)(c/d) = (c/d)(a/b), note that ac = ca and bd = db. 
M 3 The identity is 1/1. 
M4 If ajb # 0, then a· 1 # b·O = 0. So a #  0. Hence b/a is a valid fraction. 
Now (ajb)(bja) = abjab. But abjab = 1/l as ab · l  = ab · l. 
Dl 
To check (a/b)(c/d + elf) = (a/b)(c/d) + (a/b)(ejf), verify that both 
sides reduce to (acf + ade)jbdf 
D2 
Same as Dl. 
• 

Part 3 
BCH codes and other polynomial codes 


13 
BCH codes as subcodes of Hamming 
codes 
How do you modify a Hamming code to correct two errors? In other words, 
how can you increase its minimum distance from 3 to 5? You will either 
have to lengthen the code words or eliminate some of them from your code. 
Correcting two errors in a long word may not be much better than correcting 
one error in a short one. So we adopt the second approach. That is, we shall 
try to produce a double error-correcting subcode of the Hamming code by 
removing some code words to make the new code sparser. 
The most natural way of reducing the set of words of a Hamming code 
is by introducing further checks, that is, adding new rows to the Hamming 
check matrix Hk. Of course just adding any old rows will probably not get 
us what we want. Firstly the additional checks may be linear combinations 
of ones we already have. In that case (by Proposition 3.13) the set of code 
words will not be changed. Secondly, although the set of code words may 
be reduced the minimum distance may not be increased, or it may not be 
increased enough. 
If you tried Exercise 5.12, you will have found that inventing useful 
additional checks is not at all easy. That is because you have to find 
non-linear extension rpws for the check matrix Hk. On the other hand, if 
you form the extra rows in an unstructured way, it will be difficult to prove 
that the minimum distance increases, even though that may well be the case. 
In practice, that means that the new rows must be defined in a systematic 
way, most simply by an algebraic formula. 
As B has only two elements, it is not easy to define lion-linear functions 
for binary vectors. We get round this by a trick: we gather bits together in 
groups of k and consider the groups to represent elements of the field GF(2k). 
For this larger field there are plenty of non-linear functions, for instance 
f(x) 
= x3 
or any other non-linear polynomial that is not a square. 
13.1 Example Consider the columns of the Hamming check matrix Hk as 
representing the non-zero elements of GF(2k). For instance, for k = 4 we let 
the columns of H4 represent elements of GF(16). 
We are free to permute the columns of Hk to produce a nice order, as this 
only has the effect of permuting the bits of a code word, For our present 

202 
Error-correcting codes and finite fields 
purposes the best way to arrange the elements is as descending powers of a 
primitive element IX. 
Choosing IX = 2 gives us the check matrix H4 in the following form: 
(12 6 
3 
1 3  
10 5 14 7 
15 1 1  9 
8 4 2 1), 
which in binary is 
[ l 
0 
0 
0 
0 
0 
0 
0] 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 1 
Now add further rows of elements of GF(16) (or in general GF(2k)) to 
extend the matrix as simply as possible. In binary, this limits us to adding 
binary rows in batches of four (or k in general). Thus we may have to add 
a few more rows than would strictly be necessary, but we can define them 
more easily. 
The simplest way of extending the matrix is to make each new entry a 
function of the orginal entry at the head of its column. This gives us a check 
matrix with columns of the form: 
where f. is some function for i = 1, .
.
.
 , r. 
It seems reasonable to expect that with a good choice of f1, .
.
.
 , !,. 
the 
extra checks will enable us to correct an additional error per block. The 
problem is to choose the functions f. so that there is a feasible means of 
calculating the minimum distance of a code from its check matrix H. We 
can then choose the number of necessary rows in such a way as to make it 
easy to prove that the resulting code can correct the desired number of errors. 
13.2 Vandermonde matrices 
In Chapter 4 we showed how the check matrix of a linear code completely 
determines its minimum distance. 
Theorem 4.1 2  Let C be a linear codewidth check matrix H. Then C has 
minimum distance > d if and only if no set of d columns of H is linearly 
dependent. 

BCH codes as subcodes of Hamming codes 
203 
The condition is theoretically very useful, but unless H has some special 
structure it is difficult to verify for large d. For example, checking it for a 
small matrix with, say, 15 columns involves 105 pairs of columns for d =  2, 
1365 quadruples of columns for d = 4, and 5055 sextuples of columns for 
d = 6. We obviously need some structure on H that will allow us to verify 
the condition without having to check all cases. 
There are families of matrices for which the condition can be verified 
without checking all the cases. Perhaps the simplest of these families is the 
class of Vandermonde matrices. 
An n x n Vandermonde matrix V is defined as follows: 
The first row of V can be chosen arbitrarily: 
A1 Az, · 
· 
· ' An 
Then the whole matrix is [ Al 
Az 
At A+ 
V =  V(A l, .
.
.
 , An) =  : 
Aࡩ 
Aࡪ 
Matrices of this type are important because there is a simple formula for 
the determinant of V. They are named in honour of Alexandre Tbeophile 
Vandermonde, who was born in Paris in 1735 and died there in 1796. 
However, in none of his four mathematical papers (all published in 1771-72) 
does Vandermonde ever refer to 'his' matrix. Nevertheless, as one of these 
papers can be regarded as the first full development of the theory of 
determinants, it is fitt.ing that his name should be commemorated by a 
determinant formula (see Exercise 13.9). In later life Vandermonde played a 
prominent role in the French revolution. 
We shall not need the precise formula for the determinant, but only the 
following theorem, which follows from the formula. 
Theorem The Vandermonde theorem Let A1, A2, •
•
•
 , An be distinct non­
zero elements of a field F. Then the columns of V = V(A1, A2, •
•
•
 , An) are 
linearly independent over F. 
Proof See the Linear Algebra appendix of Part 1. 
Example Consider the real matrix M = V(1, 2, 3): 
[5 ± 6]. 
1 9 27 
• 

204 
Error-correcting codes and finite fields 
Its columns are linearly independent if the only solution of the equations 
is x = y = z = 0. We rewrite these equations as 
x + 2y + 
3z = 0 
x + 4y + 9z = 0 
x + 8y + 27z = 0 .  
Subtracting the first equation from the second and third, we obtain 
2y + 7z = 0 
6y + 24z = 0 .  
Now we subtract three times the first new equation from the second and get 
3z = 0. 
So z = 0. Substituting back we get y = 0 and x = 0. 
13.3 Extending a Hamming check matrix 
Now the obvious way to extend a Hamming check matrix to achieve a code 
of minium distance 5 is to arrange for every 4 x 4 matrix to be a 
Vandermonde matrix. 
Definition We define the double error-correcting BCH code BCH(k, 2) to 
have the check matrix 11;, 2 with columns: 
[::] 
o:3• 
IX4i 
Example 
ä. 2 has the following form: 
[ 1: 
6 
3 
1 3  
1 0  
5 
1 4  
7 
1 5  
1 1  
9 
8 
4 
2 
tl 
13 
5 
7 
1 1  
8 
2 
12 
3 
10 
14 
15 
9 
4 
5 
1 5  
8 
1 
3 
5 
1 5  
8 
3 
5 
1 5  
8 
1 3  
7 
8 
12 
10 
15 
4 
6 
5 
1 1  
2 
3 
14 
9 

BCH codes as subcodes of Hamming codes 
205 
or in binary: 
0 0 
0 
0 
0 0 0 
0 
0 
1 
0 
0 0 
0 0 
0 
0 
0 
1 
0 0 0 1 0 
0 0 
1 0 
1 0 1 
0 0 0 1 
0 
0 0 1 
1 0 
0 1 
1 
1 0 0 
1 
0 
0 0 
0 0 
1 0 
0 
0 0 
0 1 0 1 
1 
1 0 0 0 
0 
0 0 0 
0 
0 
0 1 
0 0 
0 
0 0 
0 
0 0 
0 
0 
1 0 0 
0 1 
0 0 
0 1 
0 0 
0 
0 0 
0 1 0 0 
0 1 0 0 
1 
0 
0 
1 
0 
0 
1 0 0 0 1 
0 0 1 
1 0 
0 
0 
0 
0 0 
0 0 
0 
0 0 
0 1 0 1 
1 
1 
1 0 0 
0 0 0 
0 0 
0 
0 1 
13.4 Verification 
To verify that the code does have minimum distance 5 and thus can correct 
two errors we must prove that no four columns of J:-4. 
2 are linearly dependent. 
Proposition No four columns of J:-4.. 2 are linearly dependent. 
Proof Choose, say, columns V;, fj, V,., V,. 
They form a Vandermonde 
matrix: 
[ 
•' 
aJ 
ak 
"'] 
a2i 
a2i 
(X2k 
(X2l 
(X3i (X3j 
(X3k 
(X31 
• 
a4i 
(X4j a4k 
a4l 
Furthermore, rxi, rxi, rxk and rx1 are distinct and non-zero. Hence it follows 
from Theorem 13.3 that its columns V;, Jij, Jlk, Jlj 
are linearly independent. 

206 
Error-correcting codes and finite fields 
13.5 Further extension 
We can easily extend our idea to produce codes of block length 2k - I 
correcting t errors per block, provided t is not too large. 
Definition The t error-correcting BCH code BCH(k, t) over the field of 
order 2k based on the primitive element a, 
has as its check matrix an n x 2t 
matrix Ř.1 where n = 2k - 1. We number the columns ԧ of Ԩ ... from 0 to 
n - 1, counting from the right. Then for i = 0, . . .  , n - 1, ԩ is defined by 
the formula: 
[ail a2i a6ti 
For the rows of Ř.1 to be distinct we must have 2t + 
2\ but we can make 
a slightly sharper estimate for the size of t. The block length of the code is 
the number of columns of its check matrix, in this case n = 2k - 1. For the 
code to have minimum distance greater than 2t we must have 2t < n. Hence 
the definition only makes sense for t < 2k- 1 . 
We denote the code by BCH(k, t). We shall tacitly assume that t < 2k- I  
whenever we mention BCH(k, t). BCH codes were originally discovered by 
Hocquenghem (1959) and independent by Bose and Ray-Chaudhuri (1960). 
As delays in publication can easily cause differences of over a year, it is fair 
to name the codes after all three authors: B(ose,)C(haudhuri,)H(ocquenghem) 
codes. It should be noted that these authors introduced the codes and proved 
that they had minimum distance at least 2t + 1 ,  but they did not construct 
an error processor. 
The original BCH codes were generalized by several authors, notably 
Gorenstein and Zierler (1961), and the name is now usually used to refer to 
the larger class of codes. Our codes are then called binary, narrow sense, 
primitive BCH codes. Once you have understood the theory presented here, 
it is easy to extend it to the more general BCH codes, which are discussed 
briefly in Chapter 19. You can find more detail in Blahut (1983), McEliece 
(1977) or MacWilliams and Sloane (1977). 
The parameters given for BCH codes are not standardized. In particular 
the parameter t is often replaced by the designed distance b of the code. That 
is theoretically preferable as it is possible, over certain fields, to design 
BCH-type codes with even values of b. This does not occur for binary fields, 
so I retain the present (perhaps more suggestive) notation. 

BCH codes as subcodes of Hamming codes 
207 
13.6 Using BCH(4, 3) as an example 
I shall use BCH{4, 3) as an example throughout the chapter and its 
successors, interspersing example calculations with the theory. Each example 
will carry on from its predecessor, so that I do not have to repeat definitions 
again and again. The code BCH( 4, 3) was chosen because calculations will 
be in GF(16) and the code is small enough to list all its code words and 
perform all calculations by hand and check them directly. 
Example BCH(4, 3) as a linear subcode of the Hamming code Ham(4) The 
check matrix of BCH(4,3) is V = Y4. 3. Its columns are 
2i 
22i 
23i 
24i 
25i 
26i 
where i runs from 14 down to 1 .  
The complete matrix is 
12 
6 
3 
13 
10 
5 
14 
7 
15 1 1  
9 
8 
4 
2 1 
6 13 
5 
7 
1 1 
8 
2 
12 
3 
10 14 15 
9 
4 1 
3 
5 
15 
8 
3 
5 
15 
8 
1 
3 
5 15 
8 
1 
13 
7 
8 
12 10 15 
4 
6 
5 1 1  
2 
3 
14 
9 
1 
10 1 1  
10 11  
10 1 1  
1 
10 1 1 
1 
10 
1 1  
5 
8 
3 
15 
5 
8 
3 
15 
5 
8 
3 
15 1 
We can also write V4. 3 in binary form as a 24 x 15 matrix, in which each 
column consists of the binary representations of the elements of GF(16) in 
the matrix above. 

208 
Error-correcting codes and finite fields 
1 0 0 
1 
0 
0 
0 0 0 
0 
0 
0 
0 0 
0 0 
0 1 
1 0 
0 1 
1 
0 0 0 
0 
0 0 1 
1 0 
0 
1 0 0 0 
0 
0 0 1 
0 
0 
0 0 
1 
1 0 
0 0 
0 0 
0 
0 
1 0 0 1 
0 1 0 
1 
0 0 0 
0 
1 
0 0 0 
0 
0 
0 
0 0 
0 
0 0 
0 
0 0 
0 
0 
0 0 
0 
0 0 
0 
0 0 
1 0 1 0 0 
0 '{) 0 
0 
0 0 
1 
0 
0 
0 
0 
0 0 0 
0 0 
0 
0 
0 
0 
0 0 
0 0 
0 
0 0 
0 
0 
0 0 
0 0 0 
0 0 
0 
0 
0 
0 
0 
0 
0 
0 0 0 0 0 
0 0 0 0 0 
0 0 0 0 0 
1 
1 0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 1 0 
0 0 
0 
0 0 
0 
0 0 
0 
0 0 
0 
0 
0 
0 0 
0 
0 
0 
0 
It is easy to see directly that many rows of V are superfluous. 
13.7 List of code words 
Below is a table of the code words of BCH(4, 3). Choose a few words from 
the list and check that they are valid code words. Check also that their sums 

BCH codes as subcodes of Hamming codes 
209 
appear in the list. How can you see directly that this code has minimum 
distance 7? 
As the check matrix V4• 3 is not in standard form, we cannot construct a 
generator matrix G for BCH(4, 3) directly. It is, however, possible to produce 
a generator matrix G for the code by finding a largest possible set of linearly 
independent code words and using them as the columns of G. You may care 
to try and find such a set. In the next chapter we' shall find a description of 
BCH(k, t) that makes it easy to write down a generator matrix. 
List of code words of BCH(5, 3): 
0 0 0 0 0 
0 0 0 0 0 
0 0 0 0 0, 
0 0 0 0 1 
1 
1 0 1 
1 
0 0 1 0 1 ,  
0 0 0 1 0 
0 1 
1 0 1 
0 1 
1 
1 
1 ' 
0 0 0 1 
1 
1 0 1 
1 0 
0 1 0 1 0; 
0 0 1 0 0 
1 
1 0 1 0 
1 
1 
1 
1 0, 
0 0 1 
0 1 
0 0 0 0 1 
1 1 0 1 
1 ,  
0 0 1 
1 0 
1 0 1 1 
1 
1 0 0 0 1 ,  
0 0 
1 
1 
1 
0 1 
1 0 0 
1 
0 1 0 O· 
' 
0 
0 0 0 
0 1 
1 
1 0 
1 
1 0 0 1 ,  
0 
0 0 1 
1 0 1 0 1 
1 1 
1 0 0, 
0 
0 1 0 
0 0 0 1 
1 
1 0 1 
1 0, 
0 
0 1 
1 
1 
1 0 0 0 
1 0 0 1 
1 ·  ' 
0 1 
1 0 0 
1 0 1 0 0 
0 0 1 
1 
1 ,  
0 1 
1 0 1 
0 1 1 1 1 
0 0 0 1 0, 
0 1 
1 
1 0 
1 
1 0 0 1 
0 1 0 0 0, 
0 1 
1 
1 
1 
0 0 0 1 0 
0 1 
1 0 1 ·  
' 
1 0 o · o  Q 1 
1 
1 0 1 
1 0 0 1 0, 
1 0 0 0 1 
0 0 1 
1 0 
1 0 1 
1 
1 ,  
1 0 0 1 0 
1 0 0 0 0 
1 1 
1 0 1 ,  
1 0 0 1 
1 
0 1 0 1 
1 
1 
1 0 0 O· ' 
0 1 0 0 
0 0 1 
1 
1 
0 1 
1 0 0, 
0 1 0 1 
1 
1 
1 0 0 
0 1 0 0 1 ,  
0 1 
1 
0 
0 1 0 1 0 
0 0 0 1 
1 ,  
0 1 
1 
1 
1 0 0 0 1 
0 0 1 
1 O· 
' 
1 
1 
0 0 0 
1 0 0 1 
1 
0 
1 0 1 
1 ,  
1 1 0 0 1 
0 1 0 0 0 
0 1 
1 
1 0, 
1 
1 0 1 0 
1 
1 1 
1 0 
0 0 1 0 0, 
1 
1 0 1 
1 
0 0 1 0 1 
0 0 0 0 
I ·  , 
1 
1 0 0 
0 1 0 0 1 
1 0 1 0 1 ,  
1 
1 0 1 
1 0 0 1 . 0 
1 0 0 0 0, 
1 
1 
1 
0 
0 0 1 0 0 
1 
1 0 1 0, 
1 
1 1 1 
1 1 
1 
1 1 
1 1 
1 
1 
1 .  

210 
Error-correcting codes and finite fields 
13.8 The reduced check matrix 
The matrix f-k.r contains too many redundant rows for it to be a practical 
check. Indeed for fields of characteristic 2 squaring is a linear function and 
we know from Proposition 3 .1 3(c) that any row of a check matrix that is 
a linear function of the other rows is superfluous. This suggests that we 
should introduce a reduced check matrix Hk,r which contains only the 
odd-numbered rows of f-k.r· 
Definition The matrix Hk,r obtained from f'k.r by deleting the even-numbered 
rows will be called the reduced check matrix of BCH(k,t). We shall 
number the rows of Hk,r with the same indices as in f-k.,: their numbers are 
I ,  3, .
.
.
 , 2t - I .  
Example 
BCH(4, 3) The reduced check matrix ofBCH(4, 3) is H =  H4, 3: 
3 
13 10 5 
14 
7 15 1 1  
9 8 
4 : 4]. 
8 
10 1 1  
In binary the matrix is: 
0 0 
1 
0 
0 
0 
0 0 
0 0 
0 
0 
1 
0 
0 
0 
0 0 
0 0 
1 0 1 
0 
0 0 0 0 0 
3 
5 1 5  
8 
3 5 1 5  
1 0  1 1  
1 0  1 1  
1 0  1 1  1 
0 
0 
0 
0 0 
0 
0 
0 
0 
1 
-
0 
0 
0 0 
0 0 
0 
0 
0 0 0 0 0 
0 0 0 
0 0 
0 0 
0 0 0 1 0 
0 0 0 1 
0 0 
0 
0 
0 
0 
0 0 
0 0 
1 
0 
0 
0 0 0 0 0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 

BCH codes as subcodes of Hamming codes 
21 1 
Several rows of the binary version of H4, 3 are still superfluous, but 
removing them is not possible without abandoning the representation over 
GF(16). 
Proposition For any k and t < 2k- l  the matrices Hk,t and Vk,t are check 
matrices for the same code, BCH(k, t). 
You could well ask why we introduced J-'k,, 
at all if the smaller matrix Hk,t 
defines the same code. There are three reasons. Firstly, the Vandermonde 
argument for finding the minimum distance of the code does not work with 
Hk, t · Secondly, the reduction to Hk,t works only for binary BCH codes. For 
ternary codes we could remove every third row and for BCH codes defined 
over a field of order q we could remove every qth row. Thirdly, there is an 
important class of codes of BCH type, the Reed-Solomon codes of Chapter 
5, for which no row of v,;,, 
can be removed. 
Proof Let w = (w14, w13, •
•
•
 , w1, w0) be a code word of the code C 
defined by Hk,t· Thus Hk,twT = 0. Writing the product row by row we obtain 
the equations: 
which hold for all odd k < 2t. 
14 
" W·IXik 
= 0 
L,. 
I 
' 
i=O 
As w; is 0 o r 1 for all i, wf = wi. Hence squaring (k) gives: 
for all k. 
14 
" W·IX2ik 
= 0 
L,. 
I 
' 
i=O 
(k) 
(2k) 
So equation (k) holds for all k = 1, . . .  , 2t. That just states that Jlk,,wT = 0. 
Hence w is a code word of BCH(k, t). As the rows of Hk, 1 are a subset of the 
rows of v,;,, any code word of BCH(k, t) must be a code word of C. Thus 
the two codes are equal. 
• 
13.9 Some questions 
Before we can use BCH codes we must answer three major questions: 
1. What is the dimension of BCH(k, t)? If this is too small then the code 
will be hopelessly inefficient, for example if the dimension is 1 then the code 
has become the 2k - 1 repetition code, and we can get that without all the 
machinery we have introduced. 
2. What is the minimum distance of the code? If the code is to correct t 

212 
Error-correcting codes and finite fields 
errors this must be at least 2t + I ,  and we have good reason to expect this 
to be so, but so far we have only proved it explicitly for t = 2. 
3. How can we correct errors in received words efficiently? Just knowing 
that the code is capable of correcting t errors without a practical correcting 
algorithm is of little use. That is certainly the toughest problem we have to 
solve. A simple and efficient algorithm will be introduced in Chapter 16. 
The following theorem gives initial answers to Questions 1 and 2. 
Theorem (a) BCH(k, t) has block length n = 2k - 1 and rank at least n - kt. 
(b) It has minimum distance at least 2t + I ,  and so can correct all error 
patterns of weight cit most t. 
The estimate for the dimension of BCH(4, 3) is 3. A binary code of 
dimension 3 has exactly 8 code words, whereas BCH(4 , 3) has 32 code words, 
and so dimension 5. In the next chapter we shall improve the estimate. The 
bound given here is only sharp if the rows of the reduced check matrix Hk. t 
are all linearly dependent. For that to be the case t must be small by 
comparison with 2k- 1• For large t we may expect BCH(k, t) to have a higher 
dimension and thus a better rate than the theorem predicts. 
The bound for the minimum distance can also be improved by a computer 
search of actual codes. But the improvement is only moderately useful, as 
there is no efficient general error-correcting algorithm that takes advantage 
of a higher minimum distance. Still, for any error processor, a larger 
minimum distance will improve the error-detection capabilities of the code. 
For more information on the true minimum distance of BCH(k, t) see 
MacWilliams and Sloane (1977), Chapter 9. 
Proof (a) The block length is n = 2k - 1, because that is the number of 
distinct powers of ex. 
The code consists of those words u for which Hk,,uT = 0. In other words, 
the transposes of the code words form the null space of Hk,r· Therefore, the 
dimension of BCH(k, t) is the nullity of Hk.r· 
The rank and nullity theorem of linear algebra (see Appendix LA) states 
that for any matrix A the dimension of the space of solutions of the equation 
Av = 0 is equal to the number of columns of A - the rank of A. So the 
dimension of BCH(k, t) is the number of columns of Hk,t - the rank of Hk,r· 
By construction, Hk,r has n columns (considered as a binary matrix) and its 
rank is at most equal to the number of its rows, which is kt. Thus 
dim BCH(k, t) * n - kt. 
The argument for part (b) closely parallels the one we gave for the 
special case BCH(4, 2) in Section 13.4. We again apply the criterion of the 
Vandermonde theorem to the check matrix V = f-k.,. 

BCH codes as subcodes of Hamming codes 
213 
We must show that no 2t columns of V are linearly dependent. Choose 
2t columns li<o• 
. . . , V;<2r>• and consider the matrix formed by these columns. 
Denoting the power ai(kJ by ak, we can write it as 
ct1 
ctz 
<Xzr 
af 
ai 
ct+1 
azr 
2t 
This matrix is a Vandermonde matrix and a1, a2, •
•
•
 , a21 are distinct 
and non-zero. Hence the columns are linearly independent. Therefore by 
Theorem 4. 12, BCH(k, t) has minimum distance greater than 2t. 
• 
Remark 
For later use we note that this proof also shows that these 2t 
columns are linearly independent not just over B, but also over GF(2k). 
13.10 The check matrix and error patterns 
The most useful consequence of Theorem 1 3.9 for the theory presented in the 
following chapters is the fact that the check matrix "i:.r can distinguish error 
patterns of weight at most t. So we state this as an explicit corollary. 
Proposition Let 
u be a code word of BCH(k, t) and let be obtained from u 
by adding an error pattern e of weight at most t, 
v = u + e. 
Then e is uniquely determined by the syndrome Jli:.rvT. 
Proof If an error pattern f =F e of weight at most t produces a word with 
the same syndrome as v, then l'i:,r(v - f? = 0. So v -fis a code word. But 
the Hamming distance from u to v - f = u + e - f is the weight of e -f 
which is at most 2t. That contradicts Theorem 13.9, which states that the 
minimum distance of the code is greater than 2t. 
So such an error pattern 
f cannot exist. 
13.1 1 Summary 
In this {)hapter we have demonstrated a method of adding additional rows 
to the Hamming check matrix Hk. We regard its columns as elements 
fJ E GF(2k) and successively add further rows for i =  2, 
. . .  , 2t, row i contain­
ing the power pi 
of the elements of GF(2k) in the same order as they appear 

214 
Error-correcting codes and finite fields 
in Hk. We have shown that, provided t < 2k - 1, this yields a check matrix 
for a subcode, BCH(k, t) of Ham(k) that can correct t errors. The codes 
produced by the construction are called BCH codes. We can reduce the check 
matrix for BCH(k, t) without changing the code by omitting the even­
numbered rows. We calculated simple bounds for the parameters of BCH 
codes. 
In the next chapter we shall find a more elegant and powerful description 
of BCH codes that will enable us to calculate the precise dimension of a 
code and find simple and efficient encoding, checking and decoding algorithms. 
It is the basis for the efficient error correcting algorithm that is developed 
in Chapter 16. 
13.12 Exercises 
13.1 Show that every function from a field of order q to itself can be 
represented by a polynomial of degree less than q. This result puts a 
limit on the powers one can usefully apply to extend Hk. 
13.2 Prove directly that any 4 columns of the reduced check matrix 
H 
_ [12 6 
3 13 10 5 
14 
7 15 1 1  9 8 
4 2 11] 
4' 2 
-
3 5 15 
8 
1 3 
5 15 
8 
1 3 
5 
1 5  8 
are linearly independent. (Hint: Use the factorization (a + /3)3 = 
(a + f3)(a2 + af3 + f32) to show that the equations a + f3 = y 
+ o. 
0:3 + f33 = y3 
+ {J3 imply that ct = ')I or ct = o.) 
13.3 What are the true rank and minimum distance of BCH(4, 3)? 
13.4 For which value of t will the construction of Vk.r produce repeated 
rows over GF(2k)? 
The next three questions describe a different method of constructing 
multiple error-correcting codes. 
13.5 Define the rth order binary Reed-Muller code of RM(r, m) of block 
length n = 2m as follows. R(O, m) is the n-fold repetition code. Its 
generator matrix is the n x 1 matrix 
G(O, m) = (1, . . .  , l)T. 
For r > m, G(r, m) = G(m, m). For r  m, RM(r, m) is defined recurs­
ively as the code with generator matrix 
[G(r, m -1) 
0 
J 
G(r, m) = 
, 
G(r, m - 1) G(r - 1, m - 1) 
where the second column occurs only if r * 1 .  Write down generator 
matrices for R(r, m) for all r, m with 0  r  m  4. 

BCH codes as subcodes of Hamming codes 
215 
13.6 Prove that for r  m, the rth order Reed-Muller code RM(r, m) has 
minimum distance 2rn-r. (Hint: Use Exercise 13.5 to split each code 
word x into two halves, x = (u I u + v) where u e RM(r, m - 1) and 
v e RM(r - 1, m - 1)). 
1 3.7 Prove that RM(r, m) has rank 
1 + (7) + . . . + (°). 
13.8 Show that extending BCH(k, 3) by a parity check bit produces a code 
of block length 2k minimum distance at least 8, and rank at least 
2k 
- 3k - 1. Compare the ranks of these extended codes with those 
of RM(k - 3, k) for k = 4, 5, 6, 7. 
1 3.9 Show that the determinant of the Vandermonde matrix of Section 13.2 
is ).1).2 . . .  ).. n (l; - l). 
i>j 
You can do this as follows. Working backwards from the last-but-one 
row subtract ).1 times each row from its successor. This makes the 
first column of the matrix (A.1, 0, . . . , O)T. When you expand by this 
column you get a factor A.1 and you are left with an (n - 1) x (n - 1) 
matrix. Extracting a factor A.; - A.1 from each column of this matrix 
leaves V(l2, •
•
•
 , l.). 

14 
BCH codes as polynomial codes 
Throughout this chapter we shall consider a particular code BCH(k, t) with 
block length n = 2k - 1 and rank m. In the last chapter we showed that 
checking whether a binary word (c1, .
.
•
 , c.) was a code word of BCH(k, t) 
is equivalent to verifying the equations 
n - 1 
" c.clk = 0 
L.., 
I 
' 
i= O  
(k) 
for powers r:i of a primitive element a, k = 1, . .
. , 2t. That makes it natural 
to identify code words c with binary polynomials c(x) of degree less than n. 
Hence the equations can be rewritten 
(k') 
This identification of code words with polynomials turns out to be very 
useful, as it allows us to exploit much of the theory of finite fields and 
polynomials that we developed in Part 2. 
14.1 Code polynomials 
We start with a convention suggested by our initial discussion. 
Convention 
Let V be the vector space B" of binary n-tuples. We write an 
element u E V as (u. _ 1, .
•
•
 , u1, u0) and identify it with the polynomial 
n - 1 
u(x) = L U;Xi. 
i=O  
The polynomial corresponding to a word w will be denoted by w(x), using 
the same letter, indeed we shall eventually identify words and polynomials. 
The set of all binary polynomials of degree less than n will be denoted by 
P. (P for· polynomial, but note that the maximum degree is n - 1). If c is a 
code word of a code C we shall call the corresponding polynomial c(x) a 
code polynomial of c. 
Example BCH(4, 3) The code word 
0 
0 
0 
0 
I 
I 
0 
0 
0 
I 
0 
I 

BCH codes as polynomial codes 
corresponds to the polynomial 
x1o + x9 + xs + x6 + xs + x2 + 1 . 
217 
We can now rephrase the definition of BCH(k, t) and Proposition 13.8 in 
the language of polynomials. 
Proposition 
Definition of BCH(k, t) Let n = 2k - 1 and let the columns of 
the check matrix v;,,t of BCH(k, t) be ordered so that 
v;,, 
t = ( O:i(n-j)). 
If c(x) E Pn, then c(x) is a code polynomial of BCH(k, t, o:) if and only if (j) 
for all powers j B 2t of o:. 
Equation (j) holds for all odd j < 2t if and only if it holds for all j B 2t 
. 
• 
14.2 The generator polynomial 
In developing the theory of finite fields in Part 2 we proved a number of 
theorems about roots of polynomials which turn out to be very useful. We 
considered two finite fields F £ E and an element f3 e E. Recall that the 
minimal polynomial mpp(x) of f3 is the polynomial of least degree with 
coefficients in F and highest coefficient 1 that has f3 as a root. 
Proposition 1 1 .5 (special case) If mpp(x) is the minimal polynomial of f3 
over F, then a polynomial p(x) e F[x] has f3 as a root if and only ifmpp(x) 
divides p(x). 
Theorem 1 1 .6 
(a) The minimal polynomial mpp(x) of f3 is irreducilzle in 
F[x]. 
(b) If a monic irreducible polynomial p(x) E F[x] has f3 as a root, then 
p(x) = mpp(x). 
(c) IfF has order q, and y =  {Jq, then mpy(x) = mpp(x). 
In our case the base field is B, so q = 2. We can combine these results 
with Proposition 14. 1 to show that there is a special code polynomial g(x) 
in BCH(k, t) that has the following property. A binary polynomial u(x) of 
degree less than 2k - 1 is a code polynomial of BCH(k, t) if and only if it is 
a multiple of g(x). The polynomial g(x) is easy to calculate. From its degree 
we can read off the precise rank of BCH(k, t) and we shall also use it to 
construct a generator matrix of BCH(k, t). 

21 8 
Error-correcting codes and finite fields 
Proposition Let g(x) be the product of the distinct minimal polynomials of a, 
a2, •
•
•
 , a2' over B (each polynomial is taken only once,· even if it occurs as 
minimal polynomial several times). Then a polynomial c(x) of degree less 
than n = 2k - 1 is a code polynomial of BCH(k, t, a) if and only if 
g(x) divides 
c(x). 
Proof If g(x) divides c(x), then c(ai) = 0 for j = 1, .
.
.
 , 2t, because g(ai) = 0 
for these values ofj. Hence by Proposition 14. 1 , c(x) is a code polynomial. 
Conversely if c(ai) = 0 for j = 1, .
. . , 2t, then by Proposition 1 1.5, the 
minimal polynomial of ai over B divides c(x) for all such j. Now, g(x) is just 
the product of these polynomials, each taken once only. Since they are 
distinct and irreducible by Theorem 11.6, they are relatively prime. Then it 
follows by Proposition 8.8 (quoted below for polynomials) that g(x) divides 
० 
. 
Proposition 8.8 If a(x) and b(x) are polynomials with highest common factor 
(a(x), b(x)) = 1, and both a(x) and b(x) divide c(x), then their product a(x)b(x) 
divides c(x). 
Remark Since (by Theorem 1 1.6) the minimal polynomial of {32 over B is 
the same as that of {3, we can omit all the even powers of a. This reflects the 
fact that in testing a word c to see if it is in BCH(k, t) we need only check 
the equations (j) of Section 14.1 
for odd j. 
Definition The polynomial g(x) is called the generator polynomial of 
BCH(k, t). 
14.3 Rank and generator polynomial of BCH codes 
Corollary (a) The generator polynomial of BCH(k, t) is the unique non-zero 
polynomial g(x) of lowest degree in BCH(k, t). 
(b) The rank of BCH(k, t) is 2k - deg(g(x))
- 1. 
Example BCH(4, 3 )  The generator polynomial is 
mp2(x)mp8(x)mp1 1(x) = (x4 + x3 + 
1)(x4 + 
x3 + x2 + x + 1)(x2 
+ x + 1) 
= x1o + x9 + xa + x6 + xs + x2 + 1 . 
As we saw above, this corresponds to the code word 
0 0 0 0 1 
1 0 
1 
0 0 
1 
0 
1 .  
A check through the list of code words of BCH(4, 3) shows that this is 
the only non-zero code word that starts with four zeros. 

BCH codes as polynomial codes 
219 
Corollary (b) tells us that the rank of BCH(4, 3) is 5, which is the correct 
value (because BCH(4, 3) has 32 code words). By comparison, recall that 
Theorem 13.9 gives rank m at least equal to 3. The corollary will give a rank 
differing from the estimate of Theorem 13.9 if two distinct odd powers of ex, 
ex; and ai, with 1 X i < j < 2t, have the same minimal polynomial, or if ai 
has a minimal polynomial of degree less than k for some 1 X i X 2t. It is 
possible to work out when this can occur and it turns out that the estimate 
of Theorem 13.9 is accurate for 2t < -/(2k) (see MacWilliams and Sloane 
(1977), Chapter 9). 
Proof (a) A nonԥzero multiple b(x)g(x) of a polynomial g(x) has degree at 
least equal to deg(g(x)) and equality holds only if b(x) is a constant. The 
only non-zero constant in B is 1. Hence all non-zero code polynomials have 
degree at least equal to deg(g(x)) and the only one with degree equal to 
deg(g(x)) is g(x) itself. 
· 
(b) We can obtain all code polynomials by multiplying g(x) by poly­
nomials b(x) of degree less than m = 2k - deg(g(x)) - 1. Furthermore, 
multiplying g(x) by distinct polynomials b(x) and a(x) of degree ȉ m yields 
different code polynomials. So the number of code polynomials is 2m. Thus 
its rank is m. 
14.4 Multiplicative encoding 
The corollaries above give a simple encoding algorithm for BCH(k, t). The 
message space consists of Pm, the set of all polynomials b(x) of degree less 
than m = 2k - deg(g(x)) - 1 .  Encode b(x) by multiplying it by g(x). By 
Proposition 14.2 this is a code word and by Corollary 14.3(b) we can 
obtain all code words in this manner. 
Example 
BCH(4, 3) Suppose we want to encode the message word 
b = 
(1 0 1 1 1). We write this as a polynomial b(x) = x4 + x2 + x1 + 1. To 
find the corresponding code word, multiply this polynomial by the generator 
polynomial 
g(x) = xlo + x9 + xs + x6 + xs + x2 + 1. 
In doing polynomial arithmetic there is no need to write down the powers 
of x provided we write down all the coefficients including the zeros. 
1 1 1 0 1 1 0 0 1 0 1 x10 + x9 + x8 + x6 + x5 + x2 + 1 
x 
1 0 1 1 1 x4 + x2 + x1 + 1 
1 1 1 0 1 1 0 0 1 0 1 
1 1 1 0 1 1 0 0 1 0 1 0 
1 1 1 0 1 1 0 0 1 0 1 0 0 
1 I 0 1 1 0 0 1 0 1 0 0 0 0 
1 0 0 0 1 0 0 I 1 0 1 0 1 1 

220 
Error-correcting codes and finite fields 
Note that addition is just binary addition without carry. This gives the code 
word: 
1 
0 
0 0 
1 
0 0 
1 
0 
1 
0 
1 
1 ,  
which corresponds to the code polynomial: 
x14 + x1 3  + x9 + x6 + xs + x3 + x + 1 .  
14.5 A generator matrix for BCH(k, t) 
We can use the generator polynomial to construct a generator matrix for 
BCH(k, t). The idea is to represent polynomial multiplication by the 
generator polynomial by a suitable matrix. Remember that the columns of 
a generator matrix of a code C are all code words of C. We choose as our 
columns the code words corresponding to xig(x) for i = m - 1, .
. . , 0. 
Example 
BCH(4, 3) A generator matrix G for BCH(4, 3) is 
0 
0 0 0 0 
0 0 0 
0 0 
0 
0 
0 
0 
0 
0 0 
0 
0 0 
0 
0 0 
0 
0 0 
0 
0 
0 0 
0 
0 
0 0 0 
0 
0 0 0 0 
The matrix is obtained by writing down as successive columns the 

BCH codes as polynomial codes 
coefficients of xig(x) for i = 4, . . .  , 0, and 
g(x) = x1o + x9 + xs + x6 + xs + x2 + 1 . 
221 
Again suppose our message contains a block b = (1 0 1 1 1). The cor­
responding code word c is Gbr, which gives 
1 0 0 0 
1 0 0 1 
0 1 0 1 
1 .  
This agrees with the result in the previous example. 
If you follow through the calculations in this example and the last, you 
will see that they are exactly the same. That is the idea behind the proof of 
the next proposition, which states that our method of producing a generator 
matrix always works. However, to make the proof neater, we shall show that 
G satisfies the conditions for a generator matrix that we established in 
Chapter 3. 
Proposition 3.9 Let C be an (n, m)-linear code and let G be an n x m­
matrix. Then G is a generator matrix for C if and only if it has rank m and 
its columns are code words. 
Proposition Let g(x) be the generator polynomial of BCH(k, t) and let G be 
the n x m matrix constructed by taking as its columns the coefficients of xig(x) 
for i =  m - 1, . . . , 0. Then G is a generator matrix for BCH(k, t). 
Proof (a) The columns G; are all code words by their construction. 
(b) Let the corresponding code polynomials be g;(x) = x;g(x) for 
i = m - 1, . . .  , 0. To show that G has rank m, we must show that its columns 
are linearly independent. So suppose that 
then 
If b(x) is defined by 
m-1 
I b;G; = Q, 
i= O 
m - 1 
m - 1  
I b;g;(x) = I b;xig(x) = Q. 
i = O  
i = O  
m - 1  
b(x) = I b;xi, 
i = O  
then it follows directly that b(x)g(x) = Q. Hence b(x) is the zero polynomial 
so all coefficients b; must be 0. Thus the columns G0, •
•
• , Gm_ 1  are linearly 
independent and G has rank m. 
• 

222 
Error-correcting codes and finite fields 
14.6 The check polynomial 
Let q = 2k and let n = q - 1. We know from Theorem 1 1 .7, Corollary 1, that 
x" - 1 is the product of the distinct minimal polynomials of the non-zero 
elements of GF(q). The generator polynomial g(x) of BCH(k,t) is the product 
of a certain subset of these minimal polynomials. Hence x" -1 
is a multiple 
of g(x). It should be emphasized that x" - 1 itself is not a code word of 
BCH(k, t) (why not?). 
Let us write x" - 1 = g(x)h(x). Consider a code polynomial c(x) = 
b(x)g(x) for some polynomial b(x) of degree less than m. If we multiply this 
c(x) by h(x) we find that c(x)h(x) = b(x)g(x)h(x) = b(x)(x" - 1). It is easy to 
recognize multiples of x" - 1 by low-degree polynomials and that gives us 
an easily checkable necessary and sufficient condition for c(x) to be a code 
polynomial. 
Definition The polynomial h(x) is called the check polynomial of BCH(k, t). 
Proposition Let c(x) be a polynomial of degree less than n. Then c(x) is a 
code polynomial of BCH(k, t) if and only if x" - I divides c(x)h(x). 
Denoting the rank of BCH(k, t) by m, the condition holds if and only if, 
for i < m, the coefficient of x•+i in c(x)h(x) is the same as that of xi and the 
coefficients of xm, . . .  , x" - I are all zero. 
Example BCH(4, 3) For BCH(4, 3) the check polynomial h(x) is 
(x4 + x +  1)(x + 1) = x5 + x4 + x2 + 1 .  
To use the check polynomial to check a word c(x), multiply c(x) by h(x) 
and see if the coefficients repeat from x1 5. We shall use this test on the code 
word we constructed in the previous example. 
0 0 0 
0 o ,  
0 
I 0 1 
1 
X 
1 0 1 0 1 
1 1 0 0 0 1 0 0 1 
0 1 0 1 1 
1 
1 0 0 0 1 0 0 1 1 0 
0 1 1 0 0 
1 0 0 0 1 0 0 1 1 0 1 0 
1 0 0 0 0 
1 0 0 0 
1 0 0 
1 
1 
0 
1 
0 
I 
I 0 0 0 0 0 
0 
I 
I 
I 0 0 0 0 0 0 0 0 0 0 
0 1 1 1 .  
This shows the repeat pattern confirming it is a code word. 
Proof We have already seen that if c(x) is a code word then c(x)h(x) is a 
multiple of x" - 1. 

BCH codes as polynomial codes 
223 
Suppose conversely that c(x)h(x) = b(x)(xn - 1) = b(x)g(x)h(x). Cancel­
ling the non-zero polynomial h(x) we get c(x) = b(x)g(x). As c(x) has degree 
less than n, it is a code polynomial. Thus we have established that the 
condition is necessary and sufficient. 
Now for b(x)g(x) to be a code word, b(x) must have degree less than 
n - d = m. Hence c(x)h(x) = b(x)(xn - 1) = xnb(x) - b(x) = xnb(x) + b(x) 
has the form stated. 
• 
14.7 Multiplicative decoding for BCH(k, t) 
We can use the check polynomial to provide a multiplicative decoder for 
our code. It incorporates a check for correctness of the received word, but 
does no error processing. It is based on the simple observation that if b(x) 
has degree less than n the coefficient sequence of 
b(x)(xn - 1) = xnb(x) + b(x) 
consists of two disjoint copies of the sequence for b(x) separated by Os. So 
if we multiply a code polynomial c(x) = b(x)g(x) by h(x), the first m 
coefficients (and also the last m coefficients) will be the coefficients of b(x). 
Example 
BCH(4, 3) Look at Examples 14.4 and 14.6 again. In Example 
14.4 we obtained the code word c 
1
0
0
0
1
0
0
1
1
0
1
0
1
 
by encoding the message word b 
1 0 1 
multiplicatively. In Example 14.6 we checked c by multiplying c(x) by h(x). 
The result was 1 0 
1 
1 0 0 0 0 0 0 0 0 0 0 1 0 1 1 
1 .  
This clearly exhibits two copies of b separated by 10 zeros. 
To sum up, we can use the techniques we have developed so far to encode 
and decode code words of BCH(k, t), though we have not yet got a method 
for error processing. The methods are quite simple: 
• Encoding Multiply the message polynomial b(x) by the generator 
polynomial g(x) to obtain the code polynomial c(x). 
• Decoding Multiply the code word c(x) by the check polynomial h(x). 
Verify that the result has the appropriate repeat pattern and read off b(x ) . 

224 
Error-correcting codes and finite fields 
14.8 Systematic encoding for BCH(k, t) 
The main weakness of this method is that the decoder has about as much 
work to do as the encoder. Since the receiver also has to cope with the task 
of error processing, it may be preferable to use a systematic encoder so that 
the message can be read off from the code polynomial directly and the 
receiver's work load is slightly reduced. There is such a method and the 
encoder involves the same amount of calculation as the one above. 
The systematic encoder for BCH(k, t) also uses the generator polynomial 
g(x) but it replaces multiplication by long division. In binary the only number 
we have to divide by is 1 and dividing by 1 is the same as multiplying by 1. 
So division involves no greater effort than multiplication. The idea is as 
follows. 
Algorithm Systematic encoding for BCH(k, t) Let BCH(k, t) have gener­
ator polynomial g(x). Let the block length and rank of BCH(k, t) be n and 
m respectively. 
Step 1. Given a message word b construct a word w of length n with b as 
its initial segment by appending n - m zeros to b. 
Step 2. Consider w as a polynomial w(x). Divide it by g(x) and find the 
remainder r(x); r(x) has degree less than deg(g(x)). 
Step 3. Put c(x) = w(x) - r(x) = w(x) + r(x). Then c(x) is the code poly­
nomial encoding b. 
You will see from the following example calculation that polynomial 
division over B is easy. 
Example BCH(4, 3) Suppose we wish to encode the message block 
1 0 0 0 
systematically. First we add 10 Os to get 
W = 1 1 0 0 0 
0 0 0 0 0 
0 0 0 0 0. 
Then we divide by g(x): 
1 1 1 0 1 1 0 0 1 0 1 )1 I 0 0 0 0 0 0 0 0 0 0 0 0 0( 
1 1 1 0 1 1 0 0 1 0 1 
0 0 1 ࡇ 1 1 0 0 1 0 1 0 0 
1 1 1 0 1 1 0 0 1 0 1 
0 1 0 1 1 1 1 0 0 0 1 0 
1 1 1 0 1 1 0 0 1 0 1 
0 1 0 1 0 0 0 0 1 1 1 0 
1 1 l 0 1 l 0 0 l 0 l 
0 1 0 0 1 1 0 1 0 1 1 

BCH codes as polynomial codes 
225 
So our code word is: 
1 
1 
0 0 0 
1 
0 0 
1 
1 
0 1 
0 1 
1 .  
This process involved four shifts and subtractions. The multiplicative 
encoder also used four shifts and additions to produce the same code word. 
So the effort involved is identical (in the multiplicative example we did 
the additons all at once, but that involved a more complicated adding 
procedure). 
Proposition (a) The algorithm above produces a code polynomial c(x) for 
every message word b of length m. 
(b) The initial coefficients of c(x) are the entries of b. Hence distinct message 
words produce distinct code words. 
(c) If c(x) = a(x)g(x) and a(x) has k non-zero coefficients, the division in 
Step 2 
involves k shifts and subtractions. 
Proof (a) Since w(x) leaves remainder r(x) on division by g(x), c(x) = 
w(x) - r(x) is divisible by g(x). It has degree less than n. Hence it is a code 
polynomial. 
(b) Since r(x) has degree less than deg(g(x)) = n - m, the highest m 
coefficients of c(x) are the same as those of w(x). By construction these are 
precisely the entries of b. 
. 
(c) Since c(x) and w(x) differ only in the coefficients of xi for i < deg(g(x)), 
the processes of dividing w(x) and c(x) by g(x) involve precisely the same 
shifts and subtractions. Dividing c(x) by g(x) by g(x) is the inverse operation 
of multiplying g(x) by a(x). That involves k shifts and additions. Hence 
dividng c(x) by g(x) involves k shifts and subtractions. 
EXTRAS 
14.9 Polynomial codes 
We conclude the chapter with a discussion of the conditions a general linear 
code C over a field F (not necessarily B) has to satisfy for us to be able to 
use the polynomial techniques that we have developed for BCH(k, t). 
We shall assume that the code C has block length n and rank m. We can 
always consider code words as polynomials of degree less than n, and shall 
continue to call these code polynomials, whether or not the code is a 
polynomial code (as defined below). Thus every code can be considered as 
a set of polynomials, but that does not always benefit us much. It is the 
existence of a generator polynomial that makes the representation useful. 

226 
Error-correcting codes and finite fields 
Definition A code C of block length n is called a polynomial code if there 
exists a polynomial g(x) such that C, considered as a set of code polynomials, 
consists of the multiples of g(x) with degree less than n. The polynomial g(x) 
is called a generator polynomial of C. 
Example Let K be the binary code of block length 15 whose polynomials 
have the following properties. 
1. The coefficient of x8 + ; is the same as the coefficient of x; for i = 0, .
.
.
 , 6. 
2. The coefficient of x 7 is always 0. 
Then K is a polynomial code with g(x) = x8 + 1 as generator because each 
code polynomial c(x) can be split as 
6 
1 4 
6 
c(x) = L c;x; + L c;x; = (x8 + 1) L cx;. 
i = O  
i = B  
i = O  
Thus the code polynomials of K are just the multiples of g(x) of degree less 
than 15. 
Proposition The generator polynomial g(x) of a polynomial code C is a code 
polynomial of C. It is unique up to multiplication by a non-zero constant. 
Proof As g(x) is a multiple of itself it is a code polynomial, unless it has 
degree at last equal to n. In that case C has no code words at all. 
If g(x) is another generator polynomial for C, then g(x) is a multiple of 
g(x), g(x) = a(x)g(x), and vice versa, g(x) is a multiple of g(x), g(x) = 
b(x)g(x). Therefore, a(x)b(x) = 1 and a(x) and b(x) are constants. 
• 
14.10 The nature of polynomial codes 
For a polynomial code C we can use the encoding techniques we introduced 
for BCH(k, t) and calculate the rank of C from the degree of its generator 
polynomial. 
· 
Example (continued) The code K has rank 7. For this code multiplicative 
and systematic encoding are the same. The message word v = (0 1 1 0 0 0 1) 
encodes as 
c = (0 
1 
1 0 0 0 
1 
0 0 
1 
1 
0 0 0 1). 
What is now needed is a method for recognizing a polynomial code. Such 
a method is given below. In order to describe it we need a further definition. 

BCH codes as polynomial codes 
227 
Definition The left shift of a word (a, b, . . . , z) is defined to be the word 
(b, . .
. , z, a). Notice that the entries 'wrap around'. 
Theorem Recognition of polynomial codes 
(a) Any polynomial code C is linear. Furthermore its block length n, rank m, 
and the degree d of a generator polynomial g(x) are related by the equation 
n = m + d. 
(b) A linear code is a polynomial code if and only if for every code word 
beginning with 0 the left shift is also a code word. 
Example (continued) With 
c 
= (0 1 
1 0 0 . 0 1 0 0 1 
1 0 0 0 1) 
as above, the left shift of c is 
w = (1 
1 0 0 0 1 0 0 1 
1 0 0 0 1 0) 
which is the code word of K encoding the message (1 1 0 0 0 1 0). 
On the other hand, the left shift of w is 
(1 0 0 0 1 0 0 1 
1 0 0 0 1 0 1). 
This is not a code word of K because the corresponding polynomial is 
x14 + x1o + x1 + x6 + xz + 1 ,  
which has a non-zero x 7 term. 
Proof (a) The sum of two multiples of g(x) is a multiple of g(x). A constant 
multiple of a multiple of g(x) is a multiple of g(x). Hence the set of multiples 
of g(x) that have degree less than n is a vector space. Thus C is linear. As 
in the proof of Corollary 14.3(b) multiplication by g(x) induces an iso­
morphism of the space Pn-d of polynomials of degree less than n - d onto 
C. Hence = rank(C) = dim(Pn - d) = n - d. 
(b) Suppose C is a polynomial code with generator polynomial g(x). Then 
a code word c beginning with 0 corresponds to a code polynomial c(x) of 
degree less than n - 1. The left shift of c corresponds to xc(x) (because its 
rightmost entry is 0). As c(x) is a multiple of g(x), xc(x) is a multiple of 
g(x) and its degree is less than n. Thus it is a code polynomial. So the left 
shift of c is a code word. 
For the converse, suppose that we are given a linear code C of block length 
n, such that for every code word c with leftmost entry 0, the left shift of c is 
a code word. Consider the set S of non-zero code polynomials of C. Choose 
g(x) in S of smallest possible degree d. Let g be the code word corresponding 
to g(x) and suppose that g starts with n - d - 1 = m - 1 zeros. So g can be 

228 
Error-correcting codes and finite fields 
left-shifted m - 1 times, yielding m words (including g itself). By the 
hypothesis, each of these shifted words is a code word. The kth left shift of 
g has xkg(x) as its code polynomial. Combining this with multiplication by 
constants and additon, we see that for any polynomial b(x) of degree less 
than m, b(x)g(x) is a code polynomial. 
We shall now show that these are the only code polynomials. Let c(x) be 
the code polynomial corresponding to a code word c. Then dividing c(x) by 
g(x) we get 
c(x) = q(x)g(x) + r(x), 
where r(x) = 0, or deg(r(x)) < deg(g(x)) and 
deg(q(x)) = deg(c(x)) - deg(g(x)) < n - d = m. 
Thus q(x)g(x) is a code word. By the linearity of C it follows that r(x) is 
a code word. As g(x) has minimal degree among the non-zero code words 
r(x) = 0. So c(x) = q(x)g(x) is a multiple of g(x). This shows that C is a 
polynomial code with g(x) as generator, and that m is the rank of C. 
• 
14.11 Cyclic codes 
Not all polynomial codes have check polynomials. 
Example Consider the polynomial code K defined above. If we divide 
x1 5 + I by x8 + l we get 
1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 
1 0 0 0 0 0 0 0 1 
1 0 0 0 0 0 0 1 
So x1 5  + 1 = x7(x8 + 1) + x7 + 1. That means there is no polynomial h(x) 
such that g(x)h(x) = x15 + 1. 
It is therefore necessary to distinguish those polynomial codes C that do 
have a check polynomial. They are called cyclic codes. The reason for the 
name 'cyclic' will become apparent when we derive the condition which 
characterizes these codes. 
Definition If the generator polynomial g(x) of a polynomial code of block 
length n divides x• - 1, then the code is called cyclic. In that case the 
polynomial h(x) such that h(x)g(x) = x• - 1 is called the check polynomial. 
Example We define a new binary code L of block length 16 with the same 
generator polynomial g(x) = x8 + 1 as K. This code has rank 8. It consists 

BCH codes as polynomial codes 
229 
of all code words of length 16 whose second half is identical to their first 
half. As x16 + 1 = (x8 + 1)2, this code has check polynomial h(x) = g(x) = 
xs + 1. 
14.12 The nature of cyclic codes 
For a cyclic code we can use all the encoding and decoding methods we 
introduced above for BCH(k, t). In particular, we can test whether a 
polynomial is a code polynomial by multiplying it by h(x) to see whether 
the result is a multiple of xn - 1 .  Note however, that we still have not 
produced an error-processing method. 
Example (continued) 
Check whether 
c = (1 
1 
0 0 
1 
1 
0 
1 
1 0 0 1 
1 
0 1) 
is a code word of L. 
1 0 0 1 1 0 1 1 1 0 0 1 1 0 t 
l 0 0 0 0 0 0 0 1 
1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 
1 0 0 1 1 0 1 1 1 0 0 1 1 0 1 
1 0 0 1 1 {) 1 0 0 0 0 0 0 0 0 1 1 0 0 1 1 0 1. 
This shows the repeat pattern characterizing code words. 
The last theorem of the chapter shows that cyclic codes are precisely those 
linear codes for which you can 'cycle' any code word. 
Theorem 
Recognition of cyclic codes A linear code is cyclic if and only if 
for any code word c the left shift of c is also a code word. 
Example (continued) 
Consider the code word c of L that was found above: 
c = (1 
1 0 0 1 1 0 1 
1 
0 0 
1 
1 0 1). 
The left shift of c is 
(1 0 0 
1 
0 1 
1 
1 0 0 
1 
1 0 
1 
1). 
Its second half is identical to its first half. So it is also a code word of L. 
Proof Suppose the code is cyclic, with generator polynomial g(x). Let c be 
a code word. If c starts with 0, then its left shift is a code word by Theorem 

230 
Error-correcting codes and finite fields 
14.10. So assume c starts with a non-zero symbol a. If c(x) is the polynomial 
corresponding to c, then the polynomial corresponding to its left shift is 
w(x) = xc(x) - ax" + a ,  
where the first term moves c one place to the left, and the two last terms 
produce the 'wrap around'. Clearly g(x) divides xc(x). It also divides 
-ax" + a, because it divides x" - 1. Hence it divides w(x). As deg(w(x)) < n, 
it follows that w(x) is a code polynomial. 
Conversely, suppose every left-shifted code word is a code word. Then the 
code is a polynomial code with generator, say, g(x) with highest coefficient 
a. Multiplying by a- 1 if necessary, we may assume that a = I .  We must 
show g(x) divides x" - I .  Let the degree of g(x) be d. Consider the polynomial 
c(x) = xn-d- 1g(x). 
The polynomial c(x) corresponds to a code word c starting with the symbol 
1. As before the left shift of c corresponds to 
xc(x) - x" + l . 
Now this is, by assumption, a code word of C, and so it is divisible by g(x). 
As c(x) was assumed to be a code word, xc(x) is divisible also by g(x). Hence 
g(x) divides x" - 1. Thus the code is cyclic. 
• 
14.13 Summary 
In this chapter we showed how BCH code words could be interpreted 
as polynomials. We gave a definition of the code based on its generator 
polynomial. The exact rank of the code was determined from its generator 
polynomial. The check polynomial was introduced and used to implement 
a check whether a word is a code word. Both systematic and multiplicative 
encoding and decoding were introduced. 
From the discussion gi ven here it migh t appear that the primitive element 
chosen to construct BCH(k, t) is very important. However, that is a false 
impression. When we have completed the main theory of BCH codes, we 
shall return to this (minor) topic in Section 1 6. 1 1. If you are interested, you 
can read that section now. It requires no further theory. 
In the Extras of this chapter, we discussed the general properties of 
polynomial and cyclic codes. 
14.14 Exercises 
14. 1  For each primitive element o: of GF(16) calculate the generator and 
· check polynomials of BCH(4, 3) based on o:. 

BCH codes as polynomial codes 
231 
14.2 Show that for k ɴ 3, k even, BCH(k, 2) has rank 2k 
- 2k - 1. (Hint: 
Show that 3 ! 2k - 1; hence if rx is a primitive element of GF(2k) then 
rx.3 is a generator of GF(2k), but not a primitive element.) 
14.3 Show that the polynomial 
generates a cyclic binary code of block length 15. What is its check 
polynomial? Use your polynomials to construct the code word in this 
code for the message word 1 1000 (using systematic encoding), and to 
determine whether 1 1001 01000 01 1 10 is a code word. 
14.4 Show·that the polynomial 
generates a cyclic code of block length 1 7. What is the check 
polynomial of the code? What is the rank m of the code? Check 
whether the following words are code words: 
1 
1 0 0 0 
1 
1 
1 0 0 1 
0 1 0 0 1 
1 
1 0 0 0 1 1 1 0 1 
1 
0 0 1 1 0. 
Find a code word beginning with a 1 followed by m - 1 zeros. 
The binary Golay code as a polynomial code. 
In Exercises 14.5-10 we shall show a construction of the binary 
Golay code as a polynomial code. To be more precise we shall 
construct a (23, 12)-binary code, and prove that its minimum weight 
is at least 5. In fact that weight is 7 and the code is the Golay code. 
For a complete proof that this construction does produce a code of 
minimum weight. 7, we refer the reader to McEliece (1977), Chapter 
8.7, and MacWilliams and Sloane (1977), Chapter 16. 
14.5 Show that the field GF(21 1) contains an element f3 of order 23. 
14.6 Using Theorem 1 1 . 12 show that the minimal polynomial of f3 over 
B = GF(2) is 
g(x) = [] (x - pi), 
where i = 2k, k = 1, 2, 3, 4, 6, 8, 9, 12, 13, 16, 18. 
14.7 Similarly show that the minimal polynomial of p-1 
is 
where i = 2\ k = 5, 7, 10, 1 1, 14, 1 5, 17, 19, 20, 21, 22. 
14.8 Deduce that x23 - 1 = (x - 1)g(x)g(x) in B[x]. 
14.9 Let G be the cyclic code of block length 23 generated by g(x). Show 
that G has rank 11. 

232 
14.10 
Error-correcting codes and finite fields 
Let H be the matrix 
[ 
f3 /32 P'] 
/32 p4 /321 
/33 p6 /320 . 
/34 /38 p19 
Show that every word u in G satisfies H · u = 0 and deduce that G has 
minimum weight at least 5. 
Exercises 14.1 1-16 deal with an analogous construction of the ternary 
Golay code. This is an (1 1, 6)-code with minimum weight 5. Again we 
produce only the first step in the proof that the code has then required 
minimum weight and refer the reader to the same two sources as above for 
the rest of the proof. 
14.1 1 Show that the field GF(35) contains an element y of order 1 1 . 
14.12 Show that the minimal polynomial of y 
over T =  GF(3) is 
g(x) = n 
(x -yi), 
where i = 3\ k = 1, 3, 4, 5, 9. 
t 4. t 3 Similarly show that the minimal polynomial of y - 1 is 
g(x) = n 
(x - yi), 
where i = 3\ k = 2, 6, 7, 8, 10. 
14.14 Deduce that x1 1 - 1 = (x - 1)g(x)g(x) in T[x]. 
14.15 Let G be the cyclic code of block length 1 1  generated by g(x). Show 
that G has rank 6. 
14.16 Let H be the matrix 
[7 
y 
y3 
.,4 
I ·/ 
y2 
y6 
.,8 
I -;o '"] 
yB 
.,7 
. 
I .. l 
Show that every word u in G satisfies H · u = 0 and (ignoring the first 
row of H) deduce that G has minimum weight at least 4. 

15 
BCH error correction: (1) the 
fundamental equation 
We now come to the major problem that has to be solved before BCH codes 
can be used in practice, namely that of error processing. In this chapter we 
shall analyse the problem and produce the 'fundamental equation' that has 
to be solved in order to find the error pattern of a received word. In the 
next chapter we shall then show how to solve this equation by an efficient 
method. 
15.1 Determining the error word 
Assume we are using the code BCH(k, t) of block length n 
= 2k - 1, designed 
to correct up to t errors. Suppose a code word c is transmitted and the word 
d 
is received. If d is 
not a code word we can tell this by using the check matrix 
Vrc,,d # 0, 
Let the error word be e = d - c .  Then from Proposition 4.10 we know that 
(1) 
If s ::;; t errors occurred, then by Proposition 13.10 there is exactly one 
possible error word e of weight at most t satisfying (1). How should we go 
about determining it? The error word e pinpoints a set of columns of Vrc.r 
whose sum is Vrc.,e. The simplest approach to determining e is just to search 
for the appropriate set of columns. If t = 1 we have a Hamming code and 
the search is easy. We just check the n 
columns of Vrc.r to find which one 
gives the syndrome Vrc,,d. 
However, for BCH(k, t) with t > 1, we would have to consider 
(;) + (;) + 
.
.
.
 + (;) 
combinations of columns and for k = 8 and t = 3 this number is already 
2 763 775. Clearly, we need to find a more efficient procedure. 

234 
Error-correcting codes and finite fields 
To make our assumptions more specific Jet the words above be as 
follows: 
d = (dn- 1• . . .  ' dl, do), 
e 
= (en-1• 
.
.
.
 ' el, eo), 
where, of course, e; = d; - c;. As in the last chapter, the numbering, from 
the right starting at 0, is chosen for coherence with the polynomial 
representation. 
Definition If the component e; of the error word e is non-zero, e; # 0, 
we say i is an error location of d. We let M denote the set of error 
locations. 
If we are dealing with BCH(k, t), we assume the number of errors s is at 
most t. Thus M has s ::; t elements and s is the weight of e. 
Example BCH(4, 3) This example follows on from the one in the last 
chapter and will be continued throughout this chapter. 
Let the transmitted word be 
and the received word be 
so the error word is 
c = (1 1 0 1 1 0 0 1 0 1 0 0 0 0 1); 
d = (l 1 0 0 0 0 0 1 0 1 0 0 0 0 1); 
e = (0 0 0 1 1 0 0 0 0 0 0 0 0 0 0). 
Thus s = 2 and the error locations are 10 and 1 1  (count from right starting 
at 0). 
15.2 The syndromes of a received word 
If we consider f-'k.1 as a matrix over GF(2k), then the full syndrome f-'k.1d is 
a word of length 2t with entries. 
Example BCH(4, 3) Let d be as above: 

BCH error correction: (1) 
the fundamental equation 
235 
From Chapter 14, V4, 3 is 
I 2  
6 
3 
I 3  
1 0  
5 
14 
7 
1 5  
I I  
9 
8 
4 
2 
6 13 
5 
7 1 1  
8 
2 12 
3 10 14 15 
9 
4 1 
3 
5 15 
8 
3 
5 15 
8 
1 
3 
5 
15 
8 1 
13 
7 
8 12 
10 
15 
4 
6 
5 1 1  
2 
3 14 
9 1 
10 1 1  
10 11 
10 11 
10 11 
1 10 
1 1  
1 
5 
8 
3 15 
5 
8 
3 15 
1 
5 
8 
3 15 1 
Thus 
S1 = 12 + 6 +  7 + 1 1 + 1 =  7, . 
s2 = 6 + 13 + 12 + 10 + 1 = 12, 
s3 = 3 +  5 + 15 + 1 + 1 =  9, 
S4 = 13 + 7 + 6 + 1 1 + 1 =  6, 
S5 = 10 + 11 + 1 1  + 10 + 1 = 1 , 
s6 = 5 + 8 +  3 +  1 + 1 = 15. 
Notice that S2 = Si, 
S4 = S܊ and S6 = S܋. 
Now we represent the code by polynomials as in the last chapter. So with 
the corresponding polynomials are 
c(x) = c._ 1x"- 1 + · · · + c0, 
d(x) = d._ 1x"- 1 + · · · + d0, 
e(x) = d(x) - c(x) = e._ 1x"- 1 + · · · + e0• 
Example BCH(4, 3) With the same words as above the polynomials are 
c(x) = x14 + x13 + x1 1  + x10 + x7 + x5 + 1 , 
d(x) = x14 + 
x13 + x 7 + x5 + 
1 ,  
and 
e(x) = x1 1 + x10 • 

236 
Error-correcting codes and finite fields 
By Definition 14.4, the rows of J.-k.r are of the form 
where q = 2\ a is a primitive element of GF(2k) and j runs from . 1  to 2t. 
Thus the ith entry S; can be written as 
n- 1 
S; = L diaii = d(a;). 
j= 1 
15.3 Syndromes and syndrome vectors 
It is inconvenient to keep having to refer to 'the ith entry of the syndrome'. 
Hence the following definition: 
Definition The values S; = d(a;) for i = 1, .
.
.
 , 2t are called the syndromes 
of d(x). The word J.-k,,d = (S1, S2,' •
.
.
 , S2,) will be referred to as the full 
syndrome or syndrome vector. 
Before recalculating the example we gather together the basic facts about 
the syndromes in a proposition. These facts have all been proved in previous 
chapters and are merely rephrased here in terms of polynomials. 
Proposition (a) If c(x) is a code polynomial of BCH(k, t), d(x) is a polynomial 
of degree at most 2k
- 2, and e(x) = d(x) - c(x), then the syndromes S; of d(x) 
and e(x) are identical for i = 1, .
.
.
 , 2t. 
(b) The syndrome S2i = Sf for all i = 1, . . .  , t. 
(c) d(x) is a code polynomial of BCH(k, t) if and only if the syndromes 
S1, S2, .
•
• , S21 are all 0. 
Proof (a) This is Proposition 4.10 rephrased in terms of polynomials; (c) 
is Proposition 14. 1  of the last chapter. To see (b) Note that for d =  (di) E B", 
d f = di. Hence 
• 
As BCH(k, t) is a subcode of BCH(k, l) for l < t, it follows that the first 
21 
syndromes for BCH(k, t) are just the syndromes for BCH(k, l). In 
particular, S1 is the ordinary Hamming syndrome for the code Ham(k) 
interpreted as an element of GF(2k). 

BCH error correction; (I) the fundamental equation 
237 
Example 
BCH(4, 3) The syndromes of c(x) can be calculated as follows: 
c(x) = x14 + x13 + xu + xlo + x1 + xs + 1 ,  
sl = c(2) 
= 1 2  + 6 + 1 3  + 1 0  + 7 + 1 1  + 1 = 0, 
S3 = c(8) 
= 3 + 5 + 8 + 1 + 15 + 1 + 1 = o, 
S5 = c(1 1) = 10 + 11 + 10 + 11 + 11 + 10 + 1 = 0. 
The other syndromes can be found by squaring and so are all 0. 
Now let us try d(x): 
d(x) = x14 + x13 + x1 + x5 + 1 ,  
S1 = d(2) 
= 12 + 6 + 7 + 1 1  + 1 = 7 ,  
s3 = d(8) 
= 3 + 5 + 15 + 1 + 1 = 9 ;  
S5 = d(l l) = 10 + 1 1  + 1 1  + 10 + 1 = 1 . 
Check that these values are also obtained by calculating d(4), d(9) and d(15). 
Unlike a real-life error processor, we know E(x). So we can also use that: 
e(x) = x1 1 + x10, 
sl = e(2) 
= 13 + 10 = 7' 
s3 
= e(8) 
= 8 + 1 = 9' 
S5 = e(l l) = 10 + 11 = 1 .  
We can also use Horner's scheme (described in Section 1 1.4) to find the 
syndromes of d(x): 
1 
1 0 0 0 0 
0 
1 
0 
1 
0 
0 
0 0 
1 
21 = 2 
3 6 12 
2 
4 
9 1 1 
14 
5 
10 13 3 7 
23 
= 8 
1 
9 7 10 
6 2 
9 
6 
2 
15 
5 
3 
1 9 
25 = 1 1  
10 
1 1  10 
1 1  1 1  10 
0 
0 
0 
0 0 1 
15.4 The case s = 2 
To gain some insight into the problem of decoding multiple errors we 
shall devise an ad hoc method of using the syndromes to correct two errors. 
This method only uses S1, S2 and S3• It is not very difficult, but its analogue 
for three errors is considerably more complicated. We shall use the fact that 
we are dealing with a binary code. This makes some things easier but others 
harder, as you will see. We shall assume that there are precisely two errors. 
So, strictly, this algorithm should be preceded by a test for single errors. 

238 
Error-correcting codes and finite fields 
That is quite easy: test whether the syndromes form a column of the check 
matrix V,.,,. 
If they do, the column gives the error location just as with 
Hamming codes. Now let the set of error locations be M = {i,j}. 
Then the 
equations we have to solve are: 
ri + a/ = S1 , 
rx.2i + rx.2i = Sz, 
rx3i + rx3j = s3. 
Equation (2) is just the square of (1), but it is useful nevertheless. 
First we multiply (2) by rxi and add it to (3). This gives 
(1) 
(2) 
(3) 
Now we substitute (1) to eliminate (rxi + rxi) in the penultimate expression: 
rx2iS1 = rx;S2 + S3 . 
Finally we use (2) again to replace rx2i by rx2; on the left-hand side: 
(S2 + rx2i)S1 = rx;S1. + S3. 
This is a quadratic in rx; and its two solutions are rxi and rxi. Here 
we hit a snag. The usual method of solving quadratics fails for fields of 
characteristic 2, because it relies on completing ax2 + bx + c to a square by 
adding a constant. For a binary field (x + {J)2 = x2 + {J2 and has no linear· 
term. So if b # 0, this method will not work. Indeed an element of a binary 
field has only a single square root, while a general quadratic polynomial has 
two roots. Thus there can be no formula for the roots of a quadatic involving 
only square roots and linear terms. 
However, our field is finite. So we can simply search for the roots of the 
quadratic. 
Example 
BCH( 4, 3) The equations are 
rxi + rxi = 7, 
rx2; + rx2i = 12, 
rx3i + rx3i = 9. 
Multiplying (2) by rx; and adding it to (3) we obtain 
rx2i( rxi + rxi) = 12cxi + 9. 
Substituting (1) we get 
(1) 
(2) 
(3) 

BCH error correction: (I) the fundamental equation 
239 
Using (2) to replace a.2i by a.21 we have 
7(12 + a.21) = 12a.1 
+ 9, 
or 
7a.21 + 12a.1 + 15 + 9 = 0. 
Dividing by 7 we have our final equation: 
a.21 + 7a.1 + 15 = 0. 
We use Horner's scheme to check that it has the expected roots 10 = 210 
and 13 = 
21 1. 
7 
15 
10 
13 
0 
13 
13 
0 
15.5 Summary of the method 
We can summarize the method as follows. We seek two quantities a.1 = a  
and a.1 = b, for which we are given the power sums: 
a + b = S1 , 
az + bz = Sz, 
a3 + b3 = S3, 
and 
a4 + b4 = s4. 
The method we use is t.o find a quadratic that has a and b as its roots. So 
what we are initially trying to calculate is the coefficients of the quadratic 
(y - a)(y - b) = y2 - (a + b)y + ab. 
To extend this scheme to triple errors, we would first have to test for single 
or double errors and then we would have to find the coefficients of the cubic 
(y - a)(y - b)(y - c) 
from the first six power sums of a, b and c. 
In general the coefficients of 
(y - al)(y - az) · · 
· (y - an) =  yn + Ayyn
- t + · · · + An 
are called the elementary symmetric functions of a1o a2, •
•
•
 , an and there are 
equations linking them to the power sums. These equations are called 
Newton's identities. The first error-processing procedure proposed for binary 
BCH codes, due to Peterson (1960), followed this path. We shall discuss 
Peterson's method further in the exercises. In the text we shall develop 

240 
Error-correcting codes and finite fields 
Newton's identities in a polynomial form that leads to a more efficient, but 
more sophisticated, error processor. 
15.6 The syndrome polynomial 
Of course, the syndromes really form one word or vector, and as before we 
can represent this by a polynomial. To avoid confusion with code polynomials 
we use a different indeterminate, z. 
Definition The syndrome polynomial of the word d with syndromes S1, .
.
•
 , S2,. 
is the polynomial 
2r- 1  
s(z) 
= s1 + S2z 
+ S3z2 
+ . . .  + s2,z2' - 1 = L si + l zi . 
i = O  
We have chosen the exponents of z so that our polynomial has smallest 
possible degree. 
Example 
BCH(4, 3) The syndrome polynomial of d(x) above is 
s(z) 
= 14z5 + z4 
+ 6z3 
+ 9z2 
+ 12z + 7 .  
The syndrome polynomial can be written as a double sum because its 
coefficients Si can themselves be represented as sums: 
and 
n- 1  
si = 2: epij, 
j=O 
n- 1  
si = L d/xY. 
j=O 
(1) 
(2) 
Both formulae give the same value. Formula (1) is not practical, but it is 
more useful for the theory because the non-zero terms are directly related 
to the error locations that we wish to determine. 
Using formula (1) we can write s(z) as 
2t- 1 
2r- 1 n - 1 
s(z) = L: si+ 1i = 2: 2: epa+ 1)jzi. 
i=O 
i=O j=O 
Now ei is non-zero only ifj is an error location, that is, j e M. We can insert 
this in the second sum and reverse the order of summation to obtain 
2t - 1  
2t - 1 
s(z) = L: si+ 1zj 
= 2: ej(J.j L: (J.ijzi. 
i=O 
j e M  
i=O 
(3) 

BCH error correction: (1) 
the fundamental equation 
241 
15.7 A geometric progression 
Write out the inner sum in detail: 
2t- 1 
L ctYzi = 1 + cx/z + cx.2iz2 + 
.
.
.
 + cx.<2r- 1liz2r- 1 
i = O  
= 1 + q + q2 + 
.
.
.
 + q2t- 1 ' 
where q = rxz. 
This is just a geometric progression, for which we learn a formula at school: 
Lemma The geometric progression 
1 - q2• 
1 + q + q2 + . . .  + q21- 1  = -- . 
1 - q 
Proof Multiply the left-hand side of the equation by ( 1  - q): 
(1 - q)(l + q + q2 + . . . + q2t- 1) 
= 1 + q + q2 + . . . + q2t- 1 
- q - q2 - . . . - q2t- 1 - q2• 
= 1 - q2'. 
• 
15.8 Formula for syndrome polynomial 
Using the formula for the geometric . progression we can rewrite the 
Syndrome polynomial s(z). 
Proposition 
The syndrome polynomial s(z) can be expressed as 
e .cx.i 
e .cx.<2r+ lliz2' 
s(z) = L _J__ - L J 
• 
jeM 1 - CX.1Z 
jeM 
1 - rx1z 
where M is the set of error locations. 
Example 
BCH(4, 3) Compare our formula for s(z) above: 
s(z) = 14z5 + z4 + 6z3 + 9z2 + 12z + 7 
(1) 

242 
Error-correcting codes and finite fields 
with the one obtained by evaluating 
210 
211 
270z6 
277z6 
---:-:- + 
-
- ---,-;-
1 - 210z 
1 - 21 1z 
1 - 210z 
1 - 21 1z 
= 210(1 
_ z60z6) 
21 1(1 
_ 266z6) 
1 - 210z 
1 - 21 1z 
10(1 - 106z6) 
13(1 - 1 36z6) 
1 - 10z 
1 - 13z 
= (1 + 10z + l lz2 + z3 + 10z4 + l lz5) x 10 
+ (1 + 13z + 7z2 + 8z3 + 12z4 + 10z5) x 13 
= 10 + 1 1z + z 2  + 10z3 + 1 1z4 + z5 · 
+ 13 + 7z + 8z2 + 12z3 + 10z4 + 1 5z5 
= 7 + 12z + 9z2 + 6z3 + z4 + 14z5 = s(z) 
Proof Using Lemma 1 5.7 (with q = Clh) 
to evaluate the inner sum we get 
2t - 1  
21- 1 
s(z) = L si+ tZi 
= L ejCI/ L fl.izi, 
i = O  
jeM 
i = O  
15.9 Introduction to the fundamental equation 
(2) 
• 
We can add the 'fractions' of each sum of equation (8.1) by placing them 
all over a common denominator. In this way we can express s(z) as difference 
of two quotients with the same denominator: 
s(z) = w(z) _ u(z)z2'
. 
l(z) 
l(z) 
(1) 
The polynomial l(z) is the product of all the terms (1 -flh) 
where j runs 
through the error locations j e M: 
l(z) = n (1 -flh). 
(2) 
jeM 
The roots of l(z) are the inverses of the powers Cl.i, j 
e M. So l(z) can be 

BCH error correction: (I) the fundamental equation 
243 
used to determine the error locations. It is called the error locator polynomial 
of d(x). However, finding l(z) itself is still a problem, because the whole 
development of the preceding paragraph assumes that the error polynomial 
e(x) is known, and of course that is just the polynomial the error processor 
needs to determine. 
Example 
BCH(4, 3) The error locations are 10 and 11. Hence the error 
locator has the formula 
l(z) = (I - 210z)(l - 21 1z) = (1 + lOz)(l + 13z) 
= 15z2 + 7z + 1 .  
15.10 The numerators 
The formulae for the polynonials w(z) and u(z) in terms of the error values 
and error locations can easily be determined. 
Proposition 
The polynomials u(z) and w(z) satisfy the following formulae: 
w<z> = I epi n c t - ciz) 
jeM 
ieM 
i#j 
u(z) = I ep<2•+ l)j n Ct -ch). 
jeM 
ieM 
i#j 
(1) 
(2) 
Example 
BCH(4, 3) With error locations 10 and 1 1  as above the error 
evaluator and coevaluator have the formulae 
and 
w(z) = 210(1 + 21 1z) + 21 1(1 + 210z) 
= 10(1 + 13.z) + 13(1 + lOz) = 7. 
u(z) = 270(1 + 21 1z) + 277(1 + 210z) 
(as 21 5 = 1, 275 = l 5 = l and 277 = 22 = 4). Hence 
u(z) = 10(1 + 13z) +4(1 + 10z) = 14 + 12z. 
Proof By definition, 
_
w(_z) 
= I 
ep/ 
. . 
l(z) 
jeM 1 - rx'z 

,vi ulhp•) nigby -.,॰7 Ne OO l<UU 
w(z) = L ep.il(Ǒ) = L epJ fleM (1 -oh) 
jeM 1 -rJ.Jz jeM 
1 - a/z 
Cancelling the denominator gives the formula for w(z). 
The proof for u(z) is completely analogous. 
• 
Once we know l(z) and hence the error locations, w(z) can be used to 
calculate the error values (which for BCH(k, t) must all be 1). It is called the 
error evaluator. 
The polynomial u(z) could equally be used to determine the 
error values. So we shall call it the error co-evaluator. In practice only w(z) 
is ever used. 
You may ask what is the purpose of calculating a number you know to 
be 1 .  Well, it does serve as a check on the computation. Furthermore, in 
Chapter 17 we shall meet an important family of codes, the Reed-Solomon 
codes, which are closely related to the present BCH codes and for which the 
error values need not be 1. However, the most important · reason for 
introducing the error evaluator here is that, as we shall see, we can only find 
w(z) and l(z) together. So we do need both. We shall only need u(z) for the 
theory. We use it to prove that the polynomials we calculate by our algorithm 
are the right ones. 
15.11 The fundamental equation 
We rewrite equation (9.1) to clear the denominators: 
l(z)s(z) + u(z)z2' = w(z) . 
In this form we shall call it the fundamental equation for BCH codes. 
(1) 
It is more common in the literature to express the fundamental equation (1) 
as a congruence omitting explicit mention of u(z): 
l(z)s(z) = w(z) 
The error locator is often denoted by cr(z), the error evaluator by w(z) 
and 
the syndrome polynomial by S(z). So you will most often find it in the form 
cr(z)S(z) = w(z) 
Example 
BCH(4, 3) 
We recall the polymomials calculated above. 
Error locator: 
l(z) = (I - 210z)(l - 21 1z) = (1 + IOz)(l + 1 3z) 
= 15z2 + 7z + 1 .  

ca·iOr e.,.,. .... .,_tor: - -
-
w(z) = 10(1 + 13z) + 13(1 + 10z) = 7. 
Error coevaluator: 
u(z) = 10(1 + 13z) + 4(1 + 10z) = 14 + 12z. 
Now we check the fundamental equation: 
l(z)s(z) = (15z2 + 7z + 1)(14z5 + z4 + 6z3 + 9z2 + 12z + 7) 
= 
14z5 + 
z4 + 6z3 + 9z2 + 12z + 7 
+ 
z6 + 7z5 + l lz4 + 13z3 + 15z2 + 12z 
+ 12z 7 + 15z6 + 9z5 + 10z4 + llz3 + 6z2 
12z 7 + 14z6 + Oz5 + Oz4 + Oz3 + Oz2 + Oz + 7 
l(z)s(z) + u(z)z21 = 12z7 + 14z6 + 7 + 12z7 + 14z6 = w(z). · 
We conclude this section by giving formal definitions of the error locator, 
evaluator and co-evaluator and stating an obvious proposition. 
Definition The code BCH(k, t), defined using the primitive element oc 
of the 
field GF(2k), 
is used to transmit a code polynomial c(x). Suppose the 
polynomial d(x) is received and let M be the set of its error locations. Then 
the error locator polynomial of d(x) is 
l(z) = n (1 -ociz), 
jeM 
the error evaluator polynomial of d(x) is 
· w(z) = L: ep.i n (1 -ociz) 
jeM ieM i,t.j 
and the error co-evaluator polynomial of d(x) is 
u(z) = 'L ep.<2t+ l)j n (t-ociz). 
jeM ieM i#j 
(2) 
(3) 
(4) 
Assuming that M has s elements, we can read off from these formulae the 
degrees of each term. This gives the following proposition. 
Proposition 
If s errors occurred in the received word d(x), then the degrees 

246 
Error-correcting codes and finite fields 
of the error locator, evaluator and co-evaluator satisfy 
Furthermore, 1(0) 
= 1 .  
deg(l(z)) = s, 
deg(u(z)) < s, 
deg(w(z)) < s. 
15.12 Proving the fundamental equation 
• 
The discussion so far is formalized in the following proof of the validity of 
the fundamental equation. The proof avoids the use of fractions, working 
only with polynomials. 
Theorem Suppose that a code word of BCH(k, t) is transmitted and the 
polynomial d(x) is received. If at most t errors occurred in transmission, then 
the error locator polynomial l(z), error evaluator and co-evaluator polynomials 
w(z) and u(z), and the syndrome polynomial s(z) of d(x) are connected by the 
fundamental equation: 
l(z)s(z) + u(z)z21 
= w(z). 
Proof Observe that 
2t 
(1 _ aiz) L aiizi- 1 = ai _ ai<2t+ 1lz2' 
j = 1 
Evaluating l(z)s(z) we now get 
2t 
l(z)s(z) = n (1 - aiz) L S;zi- 1 
jeM 
i =  1 
2t 
= f1 (1 - aiz) l: l: ezalizi- 1 
jeM 
i= 1 !eM 
2t 
= l: f1 o- aiz) l: ezalizi- 1 
leM jeM 
i =  1 
2t 
= L: c1 
- ch) z: ezalizi- 1 n ct - aiz) 
leM 
i =  1 
ie M\l 
= l: ezal n C1 - aiz) - l: eza<2t+ 1)lz2' f1 Cl - aiz) 
leM 
ieM\l 
l e M  
ieM\l 
= w(z) - u(z)z21• 
(1) 
• 

BCH error correction: (1) the fundamental equation 
247 
15.13 Summary 
In this chapter we started the task of finding an error-processing method for 
BCH codes. We introduced the syndromes and developed the theory of the 
fundamental equation relating the syndrome polynomial and three other 
polynomials, the error locator l(z), the error evaluator w(z) and the error 
co-evaluator u(z), 
l(z)s(z) = w(z) - u(z)z2'. 
For double errors I showed a straightforward method that could be used 
to find the error locations directly from the syndromes. The method is a 
special case of the first error-processing algorithm for BCH codes due to 
Peterson. It becomes too slow for large numbers of errors. In the next chapter 
I shall present an efficient algorithm for solving the fundamental equation 
and hence correcting up to t errors in a word. 
15.14 Exercises 
1 5.1 Suppose that BCH(4, 3) is used to transmit a message and that the 
error pattern of a received word is 
1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 
calculate the syndrome, error locator, error evaluator and error 
co-evaluator polynomials and check the validity of the fundamental 
equation . .  
In Exercises 15.2-6 we extend the method of Section 15.4 to produce 
an error processor for BCH(4, 3) when three errors occur. Let the error 
locations be i, j, and k, and put a = ri, b = (/J and c = ak. 
15.2 Let (z - a)(z - b)(z - c) = z3 + l1z2 + l2z + 13
• Write down formulae 
for 11, 12, 
and 13. 
15.3 Show that for the given errors, the syndromes Si satisfy Si = ai + hi +  d. 
15.4 Let A = (aii) be the 3 x 4 matrix with aii = S;+ j- 1• Show that if u is 
the vector (1, 11, 12, 
l3)T, then Au = 0, and that all the solutions of this 
system are constant multiples of u. 
15.5 Show that if less than three errors occurred, then the system Au = 0 
of Exercise 15.4 still has a non-zero solution (u0, •
•
•
 , u3), and that the 
polynomial l(z) = u0z3 + u1z2 + u2z + u4 has rxi among its roots for 
all error locations i. 
15.6 Show that in the case that less than three errors occurred it is possible 
to determine the true error locations as follows. Let the non-zero roots 
of l(z) be rxi. For i = i1, .
.
•
 , i,. Let f be a word with entries jj 
which 

248 
Error-correcting codes and finite fields 
are unknown for j = i1, .
•
.
 , ir but zero for all other j. Solve the 
equations V4. JIT = (S1 , .
.
.
 , S2r?· 
Then .f is the error word. 
1 5.7 Using Exercises 1 5.2-6 correct the following words (using a search to 
find the roots of the equation l(z) = 0: 
1 
0 
1 
0 
0 
0 
0 
1 
0 
0 
0 
0 
1 
1 
1 
0, 
0
0
0
0
1 . 
15.8 Adapt the error processor of Exercises 15.2-6 to BCH(k, 4). This 
error processor is equivalent to the ones developed by Peterson, 
Gorenstein and Zierler. So I shall refer to it as the PGZ error 
processor. 
15.9 Suppose BCH(4, 3) is used to transmit a message and that at most 
three errors occurred on transmission of c. Let the first five bits of a 
received word d form the word m. Using systematic encoding, encode 
m to a code word c'. Show that if wt(c' - d) :::; 3, then c' ·= c. 
1 5.10 A partial error processor for BCH(4, 3) 
Apply the method of Exercise 
1 5.8 to the received word d; if it fails (that is wt(c' - d) > 3), shift d 
cyclically and repeat. Continue in this manner. If you succeed with the 
code word c" after k shifts, then c" is c shifted k times. The method 
fails if you return to d without success. Prove that if the method does 
not fail and at most three errors occurred it correctly identifies the 
transmitted word c. Apply the method to the words of Exercise 1 5.7. 
Which error patterns does this error processor correct? This method 
of error processing for cyclic codes is called error trapping. 

16 
BCH error correction: (2) an 
algorithm 
In this chapter we present an efficient algorithm for solving the fundamental 
equation for BCH codes. It can be used as the basis of a practical error 
processor for BCH codes. The existence of such processors is the principal 
reason for the pre-eminence of the BCH family among block codes in 
practical use. 
The first efficient algorithm for solving the equation was invented 
by Berlekamp (1965). Later Massey (1969) produced a modification of 
Berlekamp's algorithm. It is easier to understand, because it can be 
viewed as a method for synthesizing a minimal feedback shift register to 
produce an output sequence starting with the syndromes of a received 
word. 
I shall present a conceptually simpler algorithm invented by Sugiyama et 
al. (1975). It is based on Euclid's algorithm. There is also a further algorithm 
that uses continued fractions. All these algorithms are based on properties 
of Newton's identities relating various symmetric functions of n 
variables to 
each other and are theoretically equivalent. The reader who studied the 
Extras in Chapter 7.2 will appreciate why this is the case for the Euclidean 
and continued fraction methods. 
In practice, the Berlekamp and Berlekamp-Massey algorithms are a little 
faster than the others. However, recent studies suggest that this may only 
be due to inefficient machine implementations of Euclid's algorithm (see 
Eastman 1990). 
16.1 The fundamental equation again 
Before stating the algorithm let us recall the fundamental equation that we 
wish to solve. We are using BCH(k, t) which has block length n 
= 2k - 1, 
and minimum distance greater than 2t. We represent code words by 
polynomials. 
We assume the set-up of the last chapter: 
A code word c is transmitted, the word d is received and the error word 

250 
Error-correcting codes and finite fields 
e is defined as their difference: 
C = (Cn- 1> . . . ' C1, Co), 
These words are represented by polynomials c(x), d(x) and e(x): 
C(X) = Cn- 1X"- 1 + · 
· 
· + Co, 
d(x) = dn_ 1x"- 1 + · · · + d0 , 
e(x) = d(x) - c(x) = en_ 1x"- 1 + · 
· 
· + e0 •  
As we are now interested in a practical error-processing scheme we 
represent the syndromes by the formula that can be calculated from the 
received word d(x). For i =  1, .. . , 2t, the syndrome S; is given by S; = d(ai), 
where a is the primitive element used to define the code. The syndrome 
polynomial s(z) is 
s(z) = s1 + SzZ + S3z2 + ... 
+ SzrZ21 - 1 •  
We seek the error locator polynomial /(z) and with it will also obtain the 
error evaluator w(z) and the error co-evaluator u(z). These are defined in 
terms of the set M of error locations i for which e; =f. 0 and the primitive 
element used to design the code: 
l(z) = n (1 - aiz), 
jeM 
w(z) = :L ejai n (1 - aiz), 
jeM 
ieM 
i #j 
u(z) = :L eja<Zt+ l)j n (1 - aiz). 
jeM 
ieM 
i #j 
Knowledge of the error locator would enable us to calculate its roots and 
thus to find the error locations and correct the received word. The key to 
the solution of this problem is the fundamental equation (Theorem 15.12): 
l(z)s(z) + u(z)z21 = w(z). 
16.2 The BCH algorithm 
The algorithm we shall use is based on the remarkable fact that all the 
polynomials we are looking for appear in the table produced when Euclid's 

BCH error correction: (2) 
an algorithm 
algorithm is applied to z21 and s(z). It is assumed that s ­ t errors occurred 
in d(x). What happens if that assumption is false is discussed later. 
The BCH Algorithm (with Example BCH(4, 3) 
Step 1 .  Calculate S, 
= d(a.i) for i = 1, 3, . . .  , 2t - 1. 
Calculate S2i 
= SJ for i = 1, 2, . .. 
, t. 
Put s(z) = }}!. 1 S/ 
If s(z) = Q, 
the received word has no errors. STOP. 
Example With 
c = (1 1 0 1 
1 0 0 
0 
0 
and 
d = (1 1 0 0 0 0 0 1 0 1 0 
we have already performed t)1is step: 
0 
0 
s(z) 
= 14z + z + 6z + 9z + 12z + 7 
0 0 1) 
0 0 1) 
Step 2. Apply Euclid's algorithm to a(z) = z21 and b(z) = s(z). Finish at the 
first stage where the remainder ri(z) has degree < t. 
Example 
Q 
R 
(U) 
v 
1 
0 0 
0 0 
0 
0 
(0 0 I) 
0 0 
0 
14 1 
6 9 12 
7 
(0 0 
0) 
0 0 
1 
7 12 
7 
14 7 
10 15 
(0 0 
1) 
0 7 
12 
2 10 
5 
(0 2 10) 
14 5 
4. 
The U 
column is not needed for the error computations, but is included 
to illustrate the theory. 
When calculating Euclid's algorithm for polynomials by hand it is 
convenient to use the intermediate rows described in Section 9.9. Then the 
calculation above appears as follows: 
Row Q 
R 
(U) v 
- 1  
1 
0 
0 
0 
0 
0 
0 
(0 0 
1) 
0 0 
0 
0 
14 
1 
6 
9 12 
7 
(0 0 
0) 
0 0 
1 
7 
0 
7 
1 1  13 15 12 
0 
(0 0 
1) 
0 7 
0 
0 12 
7 14 
7 
10 1 5  
(0 0 
1) 
0 7 12 
2 
0 
4 
8 
4 1 1  
7 
( 
2 
0) 
14 1 
1 
2 
0 10 
5 
(0 2 10) 
14 5 
4. 

252 
Error-correcting codes and finite fields 
The rows of the table proper are underlined. The other rows are 
calculated successively to eliminate coefficients of r; _ 2(z) that have indices 
greater than or equal to the degree of r; _ 1 (z). Thus the first auxiliary row 
merely eliminates the highest coefficient or r; _ 2, in the next calculation this 
row replaces r;_ 2• We continue producing new auxiliary rows, each calcu­
lated to reduce the highest coefficient of the R-entry of its predecessor, which 
it then replaces. When the degree of the auxiliary row becomes less than 
that of r; _ 1 (z) the long division is complete, the row is underlined and 
labelled row i. This form of the table will be used in the examples from now 
on and called the polynomial form of Euclid's algorithm. 
Step 3. If r/z) = 0, there are more than t errors: STOP. Otherwise, 
put l0(z) = vi(z). This differs from l(z) only by a non-zero constant factor. 
Find the roots of lo(z): /31, .
. . , f3s· 
Notation 
For the purposes of the discussion of the theory that follows, we 
denote the entries in the final row of the BCH algorithm by ri(z) = w0(z) 
and uj(z) = U0(Z) and vj(z) = nz). 
We note for future use that W0(z), u0(z) and 1°(z) differ from w(z), 
u(z) and l(z), by the same constant factor. This crucial fact will be 
proved in Section 1 6.5 onwards. 
Obviously, if l0(z) differs from l(z) by a constant factor, then l(z) 
and /0(z) have the same roots. 
Example 
[0(Z) = 14z2 + 5z + 4 = 4(1 5z2 + 7z + 1) = 4/(z); 
U0(z) = 2z + 1 0  
= 4(12z + 14) 
= 4u(z); 
W0(Z) = 5 
= 4.7 
= 4w(z). 
Search for roots of l0(z): 
14 
5 
4 
1 
14 
1 1  
1 5  
2 
14 
0 
4 
3 
1 4  1 4  1 5  
4 
14 
15 
10 
5 
14 
1 
1 
6 
14 
10 
10 
7 
14 
4 
1 
8 
14 
8 
1 1  
9 
14 
6 
0 

BCH error correction: (2) 
an algorithm 
253 
From this row we see that 9 is a root of l0(Z) and further that 
l0(z) = (z - 9)(14z - 6). 
Thus the roots of l0(z) are 9 and 6/14 = 1 1. 
Step 4. If the roots of l0(z) are pi 
= aPCi>, 
then the errors occurred at the 
places 2k - p(i) - 1, i = 1, . . .  , e counting from the right, starting with 0 (or 
at p(i) 
counting from the left starting with 1). 
Example The roots are 9 = 24 and 1 1  = 25. 
Error positions are 15 - 4 = 1 1  and 15 - 5 = 10 
Transmitted word: 1 2 0 1 
1 0 0 1 0 1 0 0 0 0 1. 
16.3 Termination of the algorithm 
For the moment, we defer the proof that the polynomials W0(z), U0(z) and 
l0(Z) produced by the BCH algorithm are indeed just constant multiples of 
w(z), u(z) and l(z) as claimed to the end of the chapter. First we show that 
provided that no more than t errors occurred, the algorithm will terminate 
properly. 
Proposition Assume 1 < s ­ t errors occurred. Then the Step 2 of the 
algorithm will end with a non-zero riz), such that deg(riz)) < t and 
deg(rj- 1 (z)) ɴ t. 
Proof From the fundamental equation, the highest common factor of z2t 
and s(z), (z2t, s(z)) divides w(z). So it satisfies 
deg(z2t, s(z)) ­ deg(w(z)) < s ­ t. 
On the other hand the degree of z2t is 2t > t. Since Euclid's algorithm 
terminates with rnCz) = (z2t, s(z)), there must be a j such that ri_ 1  (z) has 
degree at least t but riz) has degree less than t. 
• 
16.4 Failure modes 
Proposition 16.3 relies on the fact that the fundamental equation holds, 
which is true when at most t errors occurred. If more than t errors occur 
the algorithm will possibly produce an incorrect code word, or it may break 
down. Such a failure signals to the receiver that the assumption that at most 
t errors occurred is false, it is therefore preferable to incorrect decoding, but 

254 
Error-correcting codes and finite fields 
allowances must be made for such failures when you design an error 
processor. 
There are three conceivable failure modes. We illustrate the ones that can 
actually occur with examples for BCH(4, 3). 
It is a good exercise for the 
reader to calculate the examples independently. The modes that cannot occur 
for BCH(k, t) are marked by a star. For proofs that the starred failure modes 
cannot occur with BCH(k, t) see the Extras at the end of Chapter 17. 
Mode A 
The algorithm does not terminate properly. This could happen in 
two ways: 
I .  
All the non-zero terms in the R column have degree at least t. 
2*. s(z) has degree less than t. 
Examples 
• 
Failure mode AI This occurs if and only if z' l s(z) 
0 
1 
0 
1 
0 
0 
0 1' 
s(z) = 1 1z4• 
• 
Failure mode A2 The BCH algorithm terminates so to speak before it 
has started, with a non-zero error evaluator w0(z) = s(z), but l0(z) = 1 ,  
which has no roots. This case never occurs for BCH codes because if 
sj 
=1- 0, then s2j 
= s J =1- 0. 
Mode B The algorithm terminates but produces a faulty error locator 
l(z). 
1. 
0 is a root of l(z); 
2*. Mode Bl has not occurred but l(z) has a multiple root. 
In that case two error locations (which must by definition be distinct) are 
the same; 3. l(z) does not split into linear factors. 
The error locator is constructed to split into linear factors with distinct 
non-zero roots. So any of these indicate that something is wrong. 
Examples 
• 
Failure mode B 1 
d = 1 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
s(z) = 3z5 
+ 8z4 + 10z3 + 3z2 
+ 1 5z + 8, 
[0(Z) = 6z3 + 10z2 , 

BCH error correction: (2) an algorithm 
• 
Failure mode B3 
d = 1 
1 0 0 0 
1 
1 0 0 0 
1 
1 0 0 0, 
s(z) = 14z6 + 8z3, 
l0(Z) = 9z3 + 1 .  
255 
Mode C* The algorithm produces valid error locator, but error evaluator 
produces an error value # 1. 
This type of error would not be detected by the algorithm as it stands, 
but it would b੪ picked up by an extended algorithm that calculated the error 
value from l0(z) and W0(z). As this type of failure does not occur, an error 
value =I= 1 indicates a calculation mistake. 
16.5 The polynomials calculated by the algorithm 
We now come to the proof that the polynomials nz), U0(z) and W0(z) 
obtained by the BCH algorithm really are just constant multiples of the true 
error locator, evaluator and co-evaluator. If you want to take that result on 
trust, calculate some of the examples and skip to the next chapter. 
. 
In order to show that the polynomials l0(z), U0(z) and W0(z) produced by 
the algorithm are as we claim, we must work out what the distinguishing 
properties of l(z), u(z) and w(z) are and then check which of them are satisfied 
by /0(z), U0(z) and W0(Z). We already know most of these properties. In this 
paragraph we will establish the last one we need. In the next section we shall 
show that the properties can be used to distinguish l(z), u(z) and w(z). Then 
finally we shall show that l0(z), U0(z) and W0(Z) have all the properties except 
one (which accounts for the constant factor). 
We assume from now on, that s ­ t errors occurred in the transmission 
of the code word c(x). Of course, the most important property of l(z), u(z) 
and w(z) is that they satisfy the fundamental equation (Theorem 15.12), and 
we already have some information about degrees (Proposition 15.1 1). The 
next proposition gives the final ingredient we need. 
Proposition 
The highest common factor of l(z) and u(z) is 1 .  
(l(z), u(z)) = 1 .  
The same proof can also be used to show that (l(z), w(z)) = 1 ,  but we shall 
not make any use of that fact. 
Proof First we check for factors of the form (1 -aiz). 
If i EM, l(z) 
contains 

256 
Error-correcting codes and finite fields 
a factor (1 -ch). 
So l(rx-;) = 0. The formula for u(rx-;) is 
u(z) = L: .e;a(2t+ l)i n (1 - aia-i). 
ieM 
jeM 
j ;« i  
All the terms in the sum except one are zero. That one is 
e; a2ti n (1 - ai-i). 
jeM\i 
It is a product of non-zero values. Hence u(rx-;) 7'= 0. 
Suppose that l(z) and u(z) have a non-constant common factor v(z). Since 
l(z) splits into linear factors, the same is true of v(z). Thus v(z) has at least 
one root a-;, for some i E M. That implies that u(a-;) = 0, contradicting the 
calculation we made above. Therefore l(z) and u(z) have no non-constant 
common factors and thus their highest common factor is 1. 
• 
16.6 Uniqueness of the error locator and evaluator 
We now investigate how far the fundamental equation (Theorem 15.12) and 
the properties we have established in Propositions 16.5 and 15. 1 1  determine 
the error locator, evaluator and co-evaluator polynomials. 
Theorem Uniqueness of l(z), u(z) and w(z) 
(a) IfZO(z), U0(Z) and W0(z) satisfy the fundamental equation and have degrees 
satisfying degOC(z)) ­ t, deg(u0(z)) < t and deg(w0(z)) < t, then there 
exists a polynomial k(z) such that /0(Z) = k(z)l(z); U0(Z) = k(z)u(z) and 
W0(z) = k(z)w(z). 
(b) If, furthermore, /0(z) and U0(z) have highest common factor 1, then the 
polynomial k(z) is a non-zero constant. 
(c) If l0(z) also satisfies /0(0) = 1, then k(z) = 1. So /0(z) = /(z), u0(z) = u(z) 
and W0(z) = w(z). 
Proof (a) We have 
/(z)s(z) + u(z)z21 = w(z) 
(1) 
and 
(2) 
. Eliminate s(z) by multiplying (1) 
by l0(Z) and (2) by /(z) and subtracting. 
This gives 

BCH error correction: (2) 
an algorithm 
257 
Both terms on the right have degree less than s + t and s ­ t. Hence the 
polynomial on the right has degree less than 2t. On the other hand, the 
polynomial on the left is a multiple of z21• It follows that the only way this 
equation can be satisfied is if 
(3) 
and 
nz)w(z) - l(z)W0(Z) = 0. 
(4) 
Informally, these two equations show that the 'ratios' [O(z)/l(z), U0(z)/u(z) 
and W0(z)/w(z) are all equal. 
We shall show that there is a polynomial k(z) such that 
l0(Z) = k(z)l(z). 
Then we shall deduce from (3) and ( 4) that 
U0(z) = k(z)u(z) 
and 
W0(z) = k(z)w(z). 
We know from Proposition 16.5 that l(z) and u(z) have highest common 
factor 1. So we shall use the 1-trick. From Euclid's algorithm it follows that 
there are polynomials f(z) and g(z) such that 
f(z)l(z) + g(z)u(z) = 1 .  
We multiply this equation by /0(z): 
l0(z) = f(z)l(z)l0(Z) + g(zW(z)u(z). 
Now we use (3) to substitute for l0(z)u(z): 
l0(z) = f(z)l(zW(z) + g(z)l(z)u0(z) 
= (f(zW(z) + g(z)u0(z))l(z). 
So k(z) = f(zW(z) + g(z)u0(z) is the polynomial we require. Substituting 
l0(Z) = k(z)l(z) in (3) and (4) gives 
k(z)l(z)u(z) :::;: l(z)u0(z) 
and 
k(z)l(z)w(z) = l(z)w0(z). 
As /(z) ¥= 0 it follows that u"(z) = k(z)u(z) and W0{z) = k(z)w(z). 
(b) From part (a), k(z) divides both l0(z) and u0(z). If they have highest 

258 
Error-correcting codes and finite fields 
common factor WCz), U0(z)) = 1, then k(z) divides 1. As the only polynomials 
dividing 1 are the non-zero constants, the statement follows. 
(c) From part (b) l(z) and l0(z) are non-zero and differ by a constant 
factor K = k(z). By Propositon 1 5.1 1, /(0) = 1. Thus if zo = 1, then K = 1 and 
hence /0(Z) = l(z), U0(Z) 
= u(z) and W0(Z) 
= w(z). 
• 
16.7 Properties of Euclid's algorithm 
In Theorem 16.8 we shall show that zo(z), u0(z) and W0(z) 
satisfy conditions 
(a) and (b) of Proposition 16.6, and hence differ from the true error locator 
and evaluators by a constant factor. The proof of the theorem relies on some 
technical properties of Euclid's algorithm relating the entries in a pair of 
rows to those in the next pair of rows. Formal proofs of these facts are given 
in Theorems 7.8-7.1 0  in the Extras of Chapter 7. 
Here we shall motivate and illustrate them using Euclid's algorithm for 
integers and the properties of 2 x 2 determinants. Exactly the same facts 
hold when Euclid's algorithm is applied to polynomials. 
Recall the calculation of the highest common factor of 104 and 12 by 
Euclid's. algorithm. 
Row 
Q 
R 
u 
v 
- 1  
1 04 
1 
0 
0 
12 
0 
1 
1 
8 
8 
1 
- 8 
2 
1 
4 
- 1  
10 
3 
2 
0 
3 
- 26. 
The first two rows just contain 104, 1, 0 and 1 2, 0, l .  Thereafter the R, U 
and V entries of each new row are calculated by subtracting the same multiple 
q of their immediate predecessors from the entries two rows above: 
For our present purpose it is unimportant how qi + 1 is formed. Consider 
the 2 x 2 determinants 
IXj-1 Yi - 11 
xi 
Yi 
and 
where x and y are any of r, u or v (but the choice is fixed for both 
determinants). Now 
and 

BCH error correction: (2) an algorithm 
259 
So the second determinant is obtained from the first by switching the rows 
and then subtracting qi+ 1 times the first from the second. Hence the value 
of the second determinant is just - 1  times the first. · By looking at the 
determinants obtained from the first two rows we now derive three facts: 
• 
Fact i. For allj, ri_ 1ui - ui_ 1ri = ± b; 
• 
Fact 2. For allj, ri_ 1vi - vi_ 1ri = ±·a; 
• 
Fact 3. For allj, ui - lvi
- vi_ lui = ± 1 . 
These factors constitute the cross-product theorem 7.9. Recall further that 
our table is constructed so that for all j, 
• 
Fact 4. For all j, uia + vib = ri. 
That is proved in Theorem 7.8. Finally, we need some elementary facts 
about the degrees of the entries in the table when Euclid's algorithm is 
applied to polynomials. These are proved in Theorem 7.10. 
• 
Fact 5. The degrees of the entries in the R column decrease strictly. 
This is the way the algorithm was set up. It implies that the degree of the 
entry in the Q column is always at least 1. 
• 
Fact 6. From row 1 onwards, the degrees of the entries in the U and 
V columns increase strictly. 
That is because the highest term of, say vi+ 1(z) comes from qi+ 1(z)vj(z). 
16.8 Relating l"(z), uo(z) and wo(z) to /(z), u(z) and w(z) 
Theorem 
Denote the polynomials calculated by the BCH algorithm by 
nz) = vj(z), U0(Z) = uj(z) and W0(Z) 
= rj(z) and the true error locator, evalua­
tor polynomials l(z), w(z) and u(z). If s ::;; t errors occurred, then there 
exists a non-zero constant K such that ZO(z) = Kl(z), U0(z) = Ku(z) and 
W0(z) 
= Kw(z). 
Proof We denote the polynomials viz), ui(z) and riz) obtained by the 
algorithm by 1°(Z), U0(Z) and W0(Z) 
as before. 
The statement we wish to prove is the conclusion of Proposition 16.6(b). 
The hypotheses required to apply that proposition are as follows: 
1. /0(Z), U0(Z) 
and W0(Z) 
satisfy the fundamental equation; 
2. their degrees satisfy 
degW(z)) ::;; t, 
deg(u0(z)) < t 
deg(wo(z)) < t. 

260 
Error-correcting codes and finite fields 
3. 
The highest common factor of /0(Z) and uo(z) is 1. 
1. To verify that /0(z), U0(z) and W0(z) satisfy the fundamental equation, 
we use Fact 4, substituting s(z) for b and z21 for a: 
or 
(1) 
2. Certainly deg(w0(z)) < t, because that is the stopping condition for 
the BCH algorithm. 
The calculations for the other two degrees are close parallels. 
From Fact 6 we have 
deg(vi_ 1(z)) < degW(z)), 
and from Fact 5, 
Therefore 
Now, from Fact 2, 
Hence 
degW(z)ri_ 1(z)) = 2t . 
Since we stopped at the first row of Euclid's algorithm for which 
deg(rj(z)) < t, 
it follows that 
Hence 
degW(z)) ® t 
as required. From Fact 6 we have 
deg(uj- l (z)) < deg(u0(z)), 
as above: 

BCH error correction:-(2) an 
-a{gorithm 
26i 
Therefore 
Now, from Fact 1, 
Hence 
deg(u0(z)ri- l (z)) < 2t. 
Again from the stopping criterion of the algorithm, 
Hence 
as required. 
3. Fact 3 states that for all j, 
Substituting zo(z) 
for vi and U0(z) for ui, 
ui- 1 (zW(z) - U0(z)vi- 1 (z) = ± 1 .  
Thus any common factor of zo(z) and U0(z) must . divide both terms on 
the left hand side and hence 1. So the highest common factor of zo(z) 
and U0(z) is 1. 
We have now established the three statements we required and the 
conclusion of the theorem now follows from Proposition 6(a) and (b) . 
• 
EXTRAS 
16.9 Changing the primitive element used to calculate syndromes 
In the Extras of this chapter we consider how the choice of primitive 
element affects the code BCH(k, t). There are two ways in which the 
choice may apparently influence the resulting code. Firstly, it is used to 
calculate the syndromes, and secondly it is used to generate the multiplica­
tion table of GF(2k). We shall show that neither of these affects the code 
materially. Indeed, changing the choice in the first case amounts only to a 
permutation of the bits of the code words, while in the second it has no effect 
whatsoever. 

262 
Error-correcting codes and finite fields 
Suppose we choose a different primitive element f3 to evaluate the 
syndromes. Remember that the first appearance of the primitive element 
a was in the choice of ordering the rows of ¯.r· 
So we must permute 
the rows of ɸ.r 
to correspond to the powers of {3. 
Example If we choose a =  2 and f3 = 6 in GF(16), then the powers of 2 are 
(in descending order) 
12 
6 
3 
1 3  
10 
5 
14 7 
1 5  
1 1  9 
8 4 2 1 ,  
while those of 6 are 
4 9 15 14 10 3 12 2 8 1 1  7 5 13 6 1 .  
Thus to transfer from a to f3 we must rearrange the columns of ॥.r,a 
to fit 
the new powers. For instance the first column of ɸ.r,p 
must correspond to 
4 so it must be the column of ¯.r.a· 
The effect of this is to permute the entries of the code words. To make 
the permutation clear, I shall tabulate a code word with corresponding field 
element above each entry. Thus as a code word for BCH(4, 3, 2) is 
12 6 3 13 10 5 14 7 15 
1 1 0 
1 
1 0 
0 
1 
0 
The permuted word 
4 9 15 14 10 3 12 2 8 
0 
0 
0 
0 
1 
0 
1 
0 
0 
is a code word of BCH(4, 3, 6). 
1 1  9 8 4 2 1 
1
0
0
0
0
1 . 
1 1  7 5 13 6 1 
1 
0 
To check this we should calculate the syndromes of this word by evaluating 
its polynomial at 6, 5 and 1 1. I give the calculation for 6 below. 
0 0 0 0 1 0 
1 0 
0 
1 1 0 
1 
1 1 
6 
0 0 0 0 1 6 12 3 10 15 8 
2 1 3  4 0 
I leave it to you to verify the other syndromes, and to write out ¯.1•6• 
Proposition 
Changing the primitive element with respect to which the syn­
dromes are evaluated has the effect of permuting the entries in the code words 
of BCH(k, t). 
Proof The columns of the check matrix ¯. 
1 are defined independently of the 
primitive element selected. That element only determines the order in which 
they are entered. Thus a different choice produces a different order of the 
same columns. That corresponds to a permutation of the entries of the code 
words. 
• 

16.10 Changing the field representation 
Now consider the effect of changing the representation of the field to that 
based on {3. 
Example The primitive element 6 of GF(16) is a root of the polynomial 
x4 + x + 1. If we use that polynomial instead of x4 + x3 + 1 to construct 
GF(16), then our new primitive element which corresponds to the polynomial 
x will be represented by 2 instead of 6. It is a very good exercise for you to 
construct the table yourself. The whole new table is given below. 
Log 
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
1 1  
1 2  
1 3  
14 
1 5  
0 
4 
2 
8 
5 
1 0  
3 
1 4  
9 
7 
6 
1 3  
1 1  
1 2  
0 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
1 1  
12 
13 
1 4  
1 5  
4 
5 
7 
6 
4 
7 
5 
8 
10 
12 
9 
1 1  
1 3  
1 0  
1 1  
8 
14 
1 1  
10 
9 
1 5  
12 
1 3  
14 
8 
1 3  
1 2  
1 5  
9 
1 4  
1 5  
1 2  
1 0  
1 5  
1 4  
1 3  
1 2  
1 1  
1 0  
1 4  
1 5  
1 2  
1 3  
10 
1 1  
10 
8 
9 
9 
8 
7 
0 
0 
0 
9 
10 
1 1  
7 
5 
14 
10 
1 
4 
7 
6 
1 5  
4 
7 
0 
0 
0 
0 
12 
1 3  
14 
1 5  
1 1  
9 
1 5  
1 3  
7 
2 
5 
9 
9 
6 
4 
1 1  
1 
14 
1 2  
3 
8 
7 
5 
10 
Addition remains the same, because it is just polynomial addition in B[x]. 
What represents our old friend a that used to be denoted by 2? Well, from 
the list of powers of 6 above we see that in our original copy of GF(16), 2 
is 67• Hence in the new table it has logarithm 7, so it is denoted by 1 1. 
Consider the code word 
1 0 1 
1 0 0 1 0 1 0 0 0 0 1 
of BCH(k, t, a). To verify that it is still a code word of BCH(4, 3, a), even 
with the new table we must calculate its syndromes at a =  1 1, a3 = 12 and 
et5 = 6. The evaluation at 11 is given below. You should check that the 

264 
Error-correcting codes and finite fields 
powers of a are correctly calculated, and check the syndromes there also. 
Let us evaluate it at a = 1 1  in the new field representation: 
1 1  
1
0
1
1
0
 0
1
 
10 2 4 
1 1  9 
1 2  12 
0 
1 
0 
0 
0 
0 
1 
1 3  7 4 10 2 
5 
0. 
Proposition 
Changing the primitive element used to generate the field GF(2k) 
without changing the primitive element used to evaluate the syndromes has no 
effect on BCH(k, t). 
Proof By Theorem 12.7, there is only one field GF(2k) for a given k. The 
calculation of the syndromes takes place in this unique field, regardless of 
the different binary representation we give its elements. Thus the fact that a 
calculation produces 0 will not be affected by the representation chosen . 
• 
16.1 1 Choice of primitive element is immaterial 
From this discussion it follows that the minimal polynomial of the primitive 
element is more important than the minimal element itself, because when we 
construct the field multiplication table, we cannot tell which of the roots of 
the generating polynomial we have chosen to be represented by x. The field 
is like a symmetric crystal, and if you choose a different root of the same 
polynomial it is like rotating the crystal so that it looks just the same. If you 
choose a root of a different polynomial it is like viewing the crystal from a 
vertex rather than a face - it is the same crystal, but it looks a bit different. 
In particular, ifthe primitive element chosen to calculate the syndromes 
is changed -to a different root of the same primitive polynomial, then the 
resulting permutation of their bits takes code words to code words of exactly 
the same code. The words may be moved around but the code itself is 
unchanged. 
Example Consider the code word 
0 
0 
0 
0 
0 
1 1 0 
of BCH( 4, 3). 
0 
If we change the base primitive element from 2 to 9, which is a root of 
the same minimal polynomial x4 + x3 + 1, then the powers of 9 in descend­
ing order are 
13 7 
8 12 10 
1 5  4 6 
5 
1 1  2 
3 
14 9 1 ,  
as against the powers of 2 
12 6 3 13 10 5 
1 4  7 
1 5 1 1  9 8 4- 2 
1 .  

BCH error correction: (2) an algorithm 
Thus the word changes to 
0 0 1 '1 0 1 0 1 
1 1 0 0 0 
1 ,  
which, as you can verify, is also a code word of BCH(4, 3). 
265 
Proposition Changing the primitive element with respect to which the syn­
dromes are evaluated to a diferent root of the same minimal polynomial 
permutes the codewords of BCH(k, t), but does not change the code itself. 
Proof Suppose we change our root from 2 to another root of the same 
minimal polynomial. This permutes the entries of the code words. Now we 
also change the basis of the multiplication table of GF(2k) to the new root. 
By Proposition 16.10, that does not change the code any further, but now 
our new primitive element is represented by 2. However, the multiplication 
table is constructed using only the minimal polynomial and not the specific 
root selected. The multiplication table now looks exactly as it did before, 
but the code is again based on 2, so it is the original BCH(k, t). Thus the 
effect of permuting the entries has merely interchanged code words among 
themselves. 
• 
It is unimportant which primitive element is chosen to construct BCH(k, t). 
Of course, the encoder and decoder must use the same element and then 
match encoding and decoding techniques. The error processor will still work 
if it uses a different root of the same minimal polynomial. But the main point 
of this discussion is that no particular care has to be taken in choosing the 
primitive element. Any one will do. 
We sum this up with the following theorem. 
Theorem In designing a BCH code BCH(k, t) the choice of primitive element 
is immaterial. 
• 
16.12 Summary 
In this chapter we produced an algorithm for calculating polynomials l0(z), 
u0(z) and W0(z) (with the right range of degrees) satisfying the fundamental 
equation. It is based on Euclid's algorithm. We discussed the ways this 
algorithm might break down if the received word contained more than the 
designed number of errors of the code. 
We proved that, provided the received word has no more than the designed 
number of errors, l0(Z), U0(Z) and W0(Z) differ from the true polynomials l(z), 
u(z) and w(z) 
only by a non-zero constant factor. It is thus possible to 

zoo 
·९ Error-correcting 
cvuc;:; and-.rann! .fie/a.:;-· -
determine the error locations of a received word from the roots of l0(Z). An 
example of the calculation was given in Section 1 6.2. 
In the Extras, we showed that the primitive element used to construct the 
code and determine the syndromes of received words has no effect on the 
properties of the code. 
16.13 Exercises 
1 6. 1  The code BCH(4, 3) is used to transmit a message. One received 
block is 
0 
0 
0 
0 
0 
0 
0 
0. 
Calculate the syndromes and the error locator polynomial. Given 
that not more than three errors occurred in transmission, find the 
transmitted code word. 
1 6.2 The code BCH(4, 3) is used to transmit a message. Two words are 
received as follows 
00001 
1 001 1 
10010, 
10100 
1 01 1 1  1 1 100. 
Show that at least four errors of transmission occurred in the first 
word and decode the second. 
1 6.3 What happens when you apply the PGZ error processor (see Exercises 
1 5.2-6) to the first word of Exercise 1 6.2? 
1 6.4 
Use the Euclidean error processor for BCH(4, 3) to correct the 
received word 
0 
0 
0 
0. 
16.5 Compare the calculations of Exercise 1 6.4 with those in Exercise 1 5.7. 
16.6 Check the claims made for the words in the examples of Section 1 6.4. 
For each word verify that it has the claimed syndrome polynomial 
and that the error processor fails as described. 
1 6.7 This exercise gives an indication of the background from which the 
Euclidean error processor arose. It deals with Pade approximants to 
a power series. Consider a power series P(x) = a0 + a1x + a2x2 + · · · .  
A Pade approximant of P(x) is a rational function p(x)jq(x) with 
q(x)P(x) = p(x) (mod xm +n + 1) where deg p(x) ® 
m and deg q(x) ® n. 
By applying Euclid's algorithm to P(x) and xm +n + 1 show that Pade 
approximants always exist. 

17 
Reed-Solomon codes and burst error 
correction 
If you look through the previous two chapters you will notice that in dealing 
with BCH codes all calculations take place in GF(l6). We only check at the 
end that the answers lie in GF(2) = B. So why not drop that final condition? 
Then we obtain a linear code with an alphabet of elements of GF(16). This 
code is called a Reed-Solomon code. The arguments of the last two chapters 
can easily be adapted to show that this code also corrects up to t errors in 
a word, but now each error affects a symbol from GF(16). 
The code can be converted into a binary code by regarding the elements 
of GF(l6) as strings of 4 bits. Considered as a binary code, the block length 
of the code is multiplied by 4, but the number of errors it can correct remains 
unchanged. That is not very impressive. However, any set of binary errors 
that only affect a single 'block' of four bits is regarded as a single error and 
corrected in one go. Thus although the code is comparatively weak at 
correcting random errors, it is quite powerful in correcting multiple binary 
errors bunched close together and thus affecting only a few blocks. Such 
errors are called error bursts and are often a better model of the errors that 
occur in storage media than random errors. 
Their excellent burst error-correcting capability, together with the existence 
of good error-processing algorithms, is one reason for the widespread use of 
Reed-Solomon codes in practice. 
17.1 Introducing Reed-Solomon codes 
Definition The Reed-Solomon code RS(k, t) consists of al words w 
of length 
q - 1 with entries in GF(q) (where q = 2k) such that Jik,1wT 
= Q. 
Here Jik,r 
is 
the full check matrix of BCH(k, t). 
Remark We cannot use Hk,r because the argument used in Proposition 13.8 
to prove that Hk,r defines the same binary code as Jik.r 
breaks down. The 
argument relies on the fact that for binary polynomials f(x), f(x2) = f(x)2, 
because all the coefficients of f are 0 or 1. This is no longer true when f(x) 
is allowed to have coefficients in GF(2k) with k > 1. 

268 
Error-correcting codes and finite fields 
Proposition As a code over GF(q), RS(k, t) is a (cyclic) polynomial code with 
generator polynomial g(x) = (x - a)(x - a2) • 
• 
• (x - a21). It has block length 
n = q - 1 and dimension m = n - 2t. Its minimum distance is at exactly 2t + 1. 
Proof With Jik,r arranged as for BCH a word u with entries in 
GF(q) lies 
in RS(k, t) if and only if for the corresponding polynomial u(x), u(a) = u(a2) = 
. .
. 
= u((a21) = 0. That is the case if and only if g(x) I u(x). 
The block length is the number of rows 
of Jik.r 
= q - 1. 
The dimension is obtained by subtracting the degree of g(x) from n. 
The proof that the minimum distance is at least 2t + 1 is precisely the 
same as in Chapter 13. We show that no 2t columns of Jik.r 
are linearly 
dependent. Indeed, any 21 columns of Jik.r 
form a Vandermonde matrix, and 
are thus linearly independent. Note that the proof of this fact is independent 
of the field of scalars being used, and so holds equally for G F(2k) and G F(2). 
On the 
other hand the generator polynomial g(x) is 
a code polynomial of 
degree 2t. That corresponds to 
a code word of weight at most 2t + 1. Thus 
the minimum distance is exactly 2t + 1. 
• 
Note that g(x) is not binary. Although all code words of BCH(k, t) are 
code words of RS(k, t), the RS code has many more code words which do 
not consist of only Os and 1s. The generator polynomial g(x) cannot 
distinguish between these different types of code words, whereas the generator 
polynomial for BCH(k, t) does. 
Example The code RS( 4, 3) As in the previous chapters we shall stick with 
this single code and each example will continue from the previous ones, using 
the same code and the same code words. 
(a) RS(4, 3) as a linear code over GF(16). 
Block length n = 15, dimension m = 9, minimum distance d = 7. 
Check matrix Jik,,: 
12 
6 
3 13 10 
5 14 
7 15 1 1  
9 
8 
4 
2 
6 13 
5 
7 1 1  
8 
2 12 
3 
10 14 15 
9 
4 
3 
5 15 
8 
1 
3 
5 15 
8 
3 
5 15 
8 
13 
7 
8 12 10 15 
4 
6 
5 1 1  
2 
3 14 
9 
10 1 1  
1 
10 1 1  
1 
10 1 1  
10 1 1  
1 
10 1 1  
5 
8 
3 15 
5 
8 
3 15 
5 
8 
3 15 
(b) RS(4, 3) as a cyclic code. 
The general polynomial g(x) is 
(x - 2)(x - 4)(x - 8)(x - 9)(x - 1 1)(x - 15) 
= x6 + 3x5 + x4 + 4x3 + 7x2 + 1 3x + 1 5 .  

Reed-Solomon codes and burst error correction 
269 
Since x15 - 1 = npeGF(16) (x - {3), the check polynomial h(x) is 
(x - 1)(x - 3)(x - 5)(x - 6)(x - 7)(x - 10)(x - 12)(x - 13)(x - 14) 
= x9 + 3x8 + 4x7 + l lx6 + l lx5 + 2x4 + 14x3 + 3x2 + 12s + 5. 
Check that multiplying g(x)h(x) gives x15 - 1. 
17.2 Using R S  codes 
As RS(k, t) is a cyclic code, all the techniques of Chapter 14 can be applied. 
We can use the generator polynomial to construct a generator matrix (this 
is left to the exercises) and also to encode message words either multiplicat­
ively or using the systematic algorithm. The check polynomial can be used 
to test whether a word is a code word in the standard way. We illustrate 
these procedures in the following example. 
Example 
• Standard encoding 
The code has dimension 9 so we choose a message word of length 9: 
14 3 8 14 3 8 14 3 8 
Extend it to a word of length 15 by appending 6 zeros and divide that 
word by the generator polynomial 
1 3 1 4 7 13 15 )14 3 8 14 3 
8 14 3 8 0 
0 
0 
0 
0 0 
14 1 1  14 10 1 9 12 
8 6 4 2 1 2 3 
8 1 
8 1 1  10 12 5 
7 12 9 1 1  14 6 8 
7 9 7 5 12 8 6 
5 14 14 2 14 14 
0 
5 15 5 13 2 1 1  1 
1 1 1  15 12 5 1 
0 
1 3 1 4 7 13 15 
8 14 8 2 12 15 0 
8 1 8 1 1  10 12 5 
15 0 9 6 3 5 0 
15 8 15 14 6 4 3 
8 6 8 5 1 3 
0 
8 1 8 1 1  10 12 5 
7 0 14 1 1  15 5 0 
7 9 7 5 12 8 6 
9 
9 14 
3 13 6 

270 
Error-correcting codes and finite fields 
That gives the code word 
c = 14 3 8 14 3 8 14 3 8 9 9 14 3 13 6. 
This is the code word that we shall use throughout the chapter. 
• Multiplicative encoding 
We choose as our message word: 14 8 7 5 1 8 15 8 7. 
Multiply this word by the generator 
14 8 7 5 1 8 15 8 
7 X 
3 
1 
4 7 13 15 
14 8 7 5 1 8 15 8 7 
1 1  1 9 15 3 1 8 1 9 
14 8 7 5 1 8 15 8 7 
10 1 1  5 13 4 1 1  14 1 1  5 
1 10 12 2 7 10 6 10 12 
9 12 8 1 1  13 12 4 12 8 
12 5 6 
1 15 5 3 5 6 
14 3 8 14 3 8 14 3 8 9 9 14 3 13 6 
This gives the same code word as above. 
Now we check c using h(x). Multiply c(x) by h(x). We expect two 
copies of our second message word separated by six zeros. 
14 
3 
8 14 
3 
8 14 
3 
8 
9 
9 14 
3 13 
6 
X 1 3 4 1 1  1 1  
2 14 
3 12 5 
14 
3 
8 14 
3 
8 14 
3 
8 
9 
9 14 
3 1 3  
6 
1 1  
5 
1 1 1  
5 
1 1 1  
5 
1 
2 
2 1 1  
5 14 10 
10 1 2  11 10 1 2  11 1 0  1 2  11 1 5  1 5  10 1 2  
6 
1 
6 
4 14 
6 
4 14 
6 
4 14 
5 
5 
6 
4 
2 8 
6 
4 14 
6 
4 14 
6 
4 14 
5 
5 
6 
4 2 
8 
5 
6 
9 
5 
6 
9 
5 
6 
9 1 1  1 1  
5 6 
3 12 
2 11 1 3  
2 1 1  1 3  
2 1 1  1 3  
3 
3 2 11 
9 1 5  
1 1  
5 
1 1 1  
5 
1 1 1  
5 
1 
2 2 1 1  
5 1 4  10 
7 1 3  
4 
7 1 3  
4 
7 1 3  
4 8 
8 
7 1 3  1 0  
3 
4 1 5  
3 
4 1 5  
3 
4 1 5  3 
6 
6 
4 1 5  1 1  
14 
8 
7 
5 
1 
8 1 5  
8 
7 
0 
0 
0 
0 
0 
0 14 
8 7 
5 
1 
8 1 5  
8 7 
The expected pattern has duly appeared. 
17.3 Burst errors 
As a binary code RS(k, t) has block length n = k(q - 1) 
(where q = 2k) and 
its dimension is n - 2kt. Its minimum distance is not, however, k(2t + 1), as 
a single bit error in a block representing an element of GF(q) contributes 
the full unit distance over GF(q). Thus, all we can easily say is that its 
minimum distance is at least 2t + 1 .  In fact it turns out to be exactly 2t + 1. 
However, as we have already seen, errors occurring close together will affect 

Reed-Solomon codes and burst error correction 
271 
either the same GF(16) symbol or neighbouring symbols. So for such 'burst 
errors' we do obtain good decoding capabilities. 
Example Our code RS( 4, 3) has the following parameters: 
Block length: 4 x 15 = 60, 
Dimension: 4 x 9 = 28. 
Minimum Distance: 7, we see this by noting that the BCH code word 
(1 
1 0 1 
1 0 0 1 0 1 0 0 0 0 1) 
is also an RS code word if its entries are regarded as elements of GF(16), 
giving us a code word 
0001 0001 0000 0001 0001 0000 0000 0001 
0000 . 0001 0000 0000 0000 0000 0001 
of weight 7 in RS(4, 3). 
Definition A symbol is an element of GF(q). In dealing with RS(k, t) 
considered as a binary code, we shall identify symbols with the sets of bits 
representing them. 
A set of binary errors in a word is called a burst. The length of the burst 
is the number of binary positions between the first and last error (inclusive). 
Note that not every position of a burst need contain an error. In practice, 
it is assumed that many, but not necessarily all, places contain an error. 
Example Our chosen code word is 
c = 14 3 
8 
14 3 
8 
14 3 
8 9 9 14 3 
13 6. 
There are altogether 169· = 68 719 476 736 code words. So you will, I hope, 
excuse me for not writing them all down. 
In binary this code word is 
1 1 10 001 1  1000 1 1 10 001 1  1000 1 1 10 001 1  
1000 1001 
1001 1 1 10 001 1  1 101 01 10 
The error burst e of length 1 1  
0000 0000 0000 0000 0000 0000 101 1 
1000 
1 1 10 0000 0000 0000 
000 0000 000 
produces the word 
d = 14 3 
8 
14 3 
8 5 
1 1  6 9 9 
14 3 
13 6. 
Proposition RS(k, t) can correct a burst of length k(t-1) 
+ 1. 

272 
Error-correcting codes and finite fields 
By the proposition our code can correct all bursts of length 9. But we 
shall see that it can also correct the burst of length 1 1  in the example. That 
is because this particular burst only affects four symbols. 
Proof A burst of length k(t
- 1) + 1 cannot affect more than t symbols, 
because if it affected the last bit c; and the first bit c;+r it !VOuld cover the 
t - 1 symbols in between and have length at least 2 + k(t -1). 
• 
17.4 Errors and syndromes 
As with BCH(k, t), we regard words as polynomials, but now these are 
elements of F[x] with F = GF(2k). We use the same conventions that we 
used for BCH(k, t). 
We assume that a code word c corresponding to the polynomial c(x) is 
transmitted and that the word d corresponding to the polynomial d(x) is 
received. The error word e = d - c corresponds to the error polynomial 
e(x) = d(x) - c(x). 
Definition The set M of error locations contains those exponents i for which 
the coefficient e; of xi in e(x) is non-zero. The values e; are called the 
corresponding error values. 
This definition is identical to that for BCH(k, t), but now the error values 
may well be different from I .  
Example With 
c = 14 3 
8 
14 3 8 
14 
3 
8 9 9 
14 3 
13 6, 
d = 14 3 8 
14 3 8 
5 1 1  6 9 9 14 3 
13 6, 
we have 
e = 0 0 0 0 0 0 1 1  8 14 0 0 0 0 0 0. 
The error locations are 6, 7, 8 (we count from the right starting from 0) 
and the corresponding error values are 14, 8, 1 1. 
We define the syndromes and the syndrome polynomial for RS(k, t) in 
precisely the same way as we did for BCH(k, t). The only difference is that 
now the even syndromes must be calculated directly, and cannot be obtained 
by squaring. 
Definition For i =  1, . .
. , 2t, the syndrome S; of d(x) with respect to RS(k, t) 
is defined as d(ci), 
where IX is the primitive element used to construct the code. 

Reed-Solomon codes and burst error correction 
273 
Example We use Horner's scheme to calculate the example syndromes. 
Syndromes for c = 14 3 8 14 3 8 14 3 8 9 9 14 3 13 · 6, 
14 
3 
8 14 
3 
8 14 
3 
8 
9 
9 14 
3 13 6 
2 
14 
6 
4 
6 15 15 
9 
8 
1 1 1  
6 
2 
7 
3 0 
4 
14 
9 
7 1 1  
4 
1 10 
0 
8 
2 
1 10 
0 13 0 
8 
14 14 
5 13 15 13 
2 10 14 
4 
2 
7 
9 10 0 
9 
14 
0 
8 
9 13 
9 
0 
3 10 
5 15 
4 12 
5 0 
1 1  
14 
5 
4 
9 
6 
0 14 
5 
4 14 15 
3 
7 14 0 
15 
14 15 1 1  
3 1 1  
5 15 
0 
8 12 
2 
9 
9 
7 Q, 
Syndromes for d
= 14 3 8 14 3 8 5 11 6 9 9 14 3 13 6, 
14 
3 
8 14 
3 
8 
5 1 1  6 
9 
9 14 
3 13 6 
2 
14 
6 
4 
6 15 15 
2 15 1 1 1  
6 
2 
7 
3 0 
4 
14 
9 
7 1 1  
4 
1 
1 15 8 
2 
1 10 
0 13 0 
8 
14 14 
5 13 15 13 
9 12 2 
0 
9 
9 
4 
6 
4 
9 
14 
0 
8 
9 13 
9 1 1  14 5 15 
3 12 1 1  
8 1 
1 1  
14 
5 
4 
9 
6 
0 
5 
7 5 
5 
5 
2 12 
4 1 
1 5  
14 15 11 
3 1 1  
5 
4 
5 7 15 10 12 
8 
8 3. 
Note that S4 i= Sԓ. That can happen because we are not dealing with a binary 
polynomial. 
Proposition 
A word c e GF(qt 
where q = 2k and n = q - 1, is a code word 
of RS(k, t) if and only if its syndromes S,, . . . , S2, are all zero. If c is a code 
word and d = c + e, then the syndromes of d are the same as those of e. 
Proof Just as with BCH(k, t), the syndromes are the entries of fk.1cT, ੩nd 
Vk,t is defined to be the check matrix of RS(k, t). The second statement qow 
follows because V,., 1dT = V,.,1cT + V,.,,eT. 
• 
17.5 Defining the error locator and evaluator polynomials 
We copy the definitions of the error locator, error evaluator, error co­
evaluator and syndrome polynomials directly from BCH(k, t). 
Definition Let the code RS(k, t), defined over the field GF(2k) with primitive 
element a, be used to transmit a code polynomial c(x). Suppose the 
polynomial d(x) is received and let M be the set of its error locations. 
Then if d(x) has syndromes S1, • • . , S21 its syndrome polynomial is 
2t- 1  
s(z) = sl 
+ S2z + S3z2 + . . .  + S2tzZt- l  = L S;+ 1zi, 
(1) 
i '= O  

274 
Error-correcting codes and finite fields 
its error locator polynomial is 
l(z) = n (1 - cr/z). 
jeM 
its error evaluator polynomial is 
w(z) = I ep.i n (1 -rh) 
jeM 
ieM 
i#'j 
and its error co-evaluator polynomial is 
u(z) = I ep<2t+ l)j n (1 - ch). 
jeM 
ieM 
i#'j 
Example With d as above the syndrome polynomial is 
3z5 + z4 + z3 + 4z2, 
the error locator is 
(1 - 15z)(1 - 7z)(1 - 14z) = 15z3 + 1 1z2 + 6z + 1 ,  
the error evaluator is 
(2) 
(3) 
(4) 
14.15(1 - 7z)(1 - 14z) + 8.7(1 - 15z)(l - 14z) + 1 1.14(1 - 15z)(1 - 7z) = 4z2, 
and the error co-evaluator is 
14.157(1 - 7z)(1 - 14z) + 8.77(1 - 15z)(1 - 14z) + 1 1.147(1 - 15z)(l - 7z) 
= 8z2 + 1 1z + 14. 
The statements of the following proposition are obvious. 
Proposition Let c be a code word of RS(k, t), and let d = c + e be a received 
word. If s errors occurred in 
d, then the degrees of the error locator, evaluator 
and co-evaluator satisfy 
Furthermore, 1(0) = 1. 
deg{l(z)) = s, 
deg(u(z)) < s, 
deg(w(z)) < s. 
17.6 The fundamental equation 
• 
We can copy the proof of validity of the fundamental equation directly from 
Chapter 15. 

Reed-Solomon codes and burst error correction 
275 
Theorem Suppose that a code word c of RS(k, t) is transmitted and the word 
d is received. If at most t errors occurred in transmission, then the error locator 
polynomial l(z), error evaluator and co-evaluator polynomials w(z) and u(z), 
and the syndrome polynomial s(z) of d(x) are connected by the fundamental 
equation: 
Example 
l(z)s(z) = w(z) + u(z)z21• 
l(z)s(z) = (l 5z3 + l lz2 + 6z + l) x (3z5 + z4 + z3 + 4z2) 
1 5  
1 1  
6 
1 X 3 
1 
1 4 0 0 
8 
4 1 1  
3 0 0 0 0 0 
15 1 1  
6 1 0 0 0 0 
15 1 1  6 1 0 0 0 
14 7 1 4 0 0 
8 1 1  14 
0 0 0 4 0 0 
l(z)s(z) = (8z2 + 1 1z + 14)z6 + 4z2 
= u(z)z6 + w(z). 
Proof Evaluating l(z)s(z) we get 
2t 
l(z)s(z) = n (1 - a/z) L S;zi-t 
jeM 
i= 1 
21 
= n (1 - <J.iz) L L epa.Pizi- 1 
jeM 
i= 1 peM 
2t 
= L n (1 - a.iz) L epa.Pizi- 1  
peM jeM 
i= 1 
21 
= L (1 
- a.Gz) L epa.Pizi- 1  n (1 - a.iz) 
peM 
i= l  
ieM\p 
= L epa.P n (1 - a.iz) - L epa.(2t+ 1)pz2t n (1 - a.iz) 
peM 
ieM\p 
peM 
ieM\p 
= w(z) + u(z)z21• 
17.7 Error processing algorithm 
• 
Any error-processing algorithm for BCH(k,t) can be adapted to RS(k, t). The 
original algorithm finds the error locations. Then the error values can be 
found by using the fundamental equation to determine the error evaluator 
polynomial from the error locator and syndrome polynomials. Here is the 
adaptation of the algorithm based on Euclid's algorithm for BCH(k, t). 

276 
Error-correcting codes and finite fields 
The set-up is the same as for BCH. Note only that all syndromes must be 
calculated: it is no longer true that S2 is Sf. The calculations proceed as for 
BCH but, having located the errors, we must also determine their values. 
The version of the algorithm presented here uses a computationally advan­
tageous formula for e;. It will be verified in Theorem 17.8. 
The 
RS algorithm 
Write the code words and received words as words with entries in GF(q) 
and consider them as polynomials. We assume that no more than t symbol 
errors have occurred. Example calculations for the received word d given 
above are interspersed with the steps of the algorithm. 
Step 1. Calculate S; = d(1Xi) for i = 1, 2, . . .  , 2t. Put 
2r- 1 
s(z) = sl + SzZ + S3z2 + 
. . .  + s2tz2t- 1  = L si+ lzi. 
i=O 
If s(z) = 0, there are no errors: STOP. 
Example This has already been done: 
s(z) = 3z5 + z4 + z3 + 4z2 • 
Step 2. Apply Euclid's algorithm to a(z) = z2' and b(z) = s(z). 
Finish at the first stage where ri(z) has degree < t. 
If ri(z) = Q, 
there are more than t errors: STOP. 
Example 
R 
v 
1 
0 0 
0 
0 0 0 
0 
3 1 
1 
4 0 
0 
0 
8 
0 
8 8 
1 1  
0 
0 
0 
1 5  
7 
4 14 0 0 
0 
0 
0 
0 
8 
8 
1 1  
6 
7 
4 0 
0 
14 13 
15 
9 
8 
0 
0 
14 
8 
1 1  14 0 0 
1 3  
1 5  
2 
7 0 
0 
13 10 
Step 3. Put zo(z) = vj(z). Find the roots of /0(z): fJ1o .
. . , {3 • .  
Example 
/0(z) = 13z3 + 10z + 8z + 1 1  = 10/(z) . 
8 
1 
8 
0 
1 
0 
1 5  
1 
2 
1 5  
1 1  

Reed-Solomon codes and burst error correCtion 
2.7 r 
We check for the roots using Horner's scheme, and use it to calculate the 
derivative at the same time, that will be used in the next step. We omit the 
unsuccessful runs. 
13 10 
8 1 1  
5 
13 
1 13 
0 
13 10 
4 
1°(5) = 0, 
1°'(5) = 4 
7 
13 
2 
6 
0 
13 10 
2 
1°(7) = 0, 
1°'(7) = 2 
14 
13 
3 
3 
0 
13 10 11 
1°(14) = 0, 
1°'(14) = 1 1 .  
Step 4. For each root pi, 
if Pi 
= ctp(il, then the error occurred at the place 
2k - p(i) - 1, i = 1, . . . , e. 
Example The roots of ZO(z) are 5 = 29 = r 6, 7 = 27 ;, 2-8, and 14 = 28 = 
2 - 7• Thus the errors occurred at locations 6, 8 and 7. 
Step 5. Put W0(Z) = rj(z). 
Calculate the error values 
Example 
Error values: 
e6 
= 7.5.5/4 
= 14 = 1 1 10 
e1 = 7.14.14/1 1 = 8 = 1000 
e8 = 7.7.7/2 
= 1 1  = 101 1 .  
Thus the corrected word is 
d = 14 3 8 14 3 8 5 + 1 1  1 1  + 8 6 + 14 9 9 14 3 13 6 
= 14 3 8 14 3 8 14 
3 
8 
9 9 14 3 13 6. 
17.8 Correctness of the algorithm 
Theorem (a) The polynomials l0(Z), W0(z) determined by the algorithm are 
equal to Kl(z) and Kw(z) where K is a non-zero constant. 
(b) The algorithm calculates the error values correctly. 

278 
Errorcorrectmg-codes an7Tjinite ]ielifs 
Proof (a) The polynomials l(z), u(z), and w(z) are defined by the same 
formulae as in Chapter 15, and they satisfy the fundamental equation. Thus 
the proof of Theorem 16.8 can be transferred without any change. That 
proves (a). 
(b) From the formula for w(z), 
w(cx-i) = ei · cx-i n (1 
- IXj- l). 
jeM\i 
From the formula for l(x) /'(cx-i) 
= -IX-i n (1 - cxi-i). 
jeM\i 
(l'(z) is a sum, and again all other terms are 0). Thus 
ei = w(cx-i)/l'(cx-i). 
As l0(Z) and w0(z) differ from l(z) and w(z) only by multiplication by the 
same constant it makes no difference if we use them instead. 
• 
17.9 Failure modes 
The possible failure modes are similar to those for BCH(k, t), but because 
the error values are no longer limited to 1, a non-identity error value no 
longer constitutes a failure. That makes failure mode C obsolete. On the 
other hand failure mode A2 can now occur. 
Mode A The algorithm does not terminate properly. This could happen 
in two ways: 
1. All the non-zero terms in the R column have degrees at least t. 
2. s(z) has degree less than t. 
Example 
• Failure mode Al This occurs if and only if z1 1 s(z) 
d = 1 14 10 15 0 0 0 0 0 0 0 0 0 0 0 
s(z) = z5 + 4z4 + z3 
• Failure mode A2 In this case s(z) = w(z) but l(z) = I, which has no 
roots. 
d = 1 
5 
14 8 8 0 0 0 0 0 0 0 0 0 0, 
s(z) = 13z + 10. 

ReiiaSolomoii codes iin(fb-urst error correction 
279 
Mode B The algorithm terminates but produces a faulty error locator 
l(z). 
1. 0 is a root of l(z); 
2. Mode B1 has not occurred but l(z) has a multiple root. 
In that case two error locations (which must by definition be distinct) 
are the same; 
3. l(z) does not split into linear factors. 
The error locator is constructed to split into linear factors with distinct 
non-zero roots. So any of these indicate that something is wrong. 
Example 
• Failure mode BJ 
d = O  
1 0 0 0 0 0 0 0 0 0 0, 
s(z) = 8z5 + 10z4 + 3z3 + 5z2 + 15z + 8, 
l0(Z) = 6z3 + 10z2. 
• Failure mode B2 
d = O  0 0 0 0 0 0 0 0 1 1  13 15 9 13 3, 
s(z) = z4 + z2 + 1 , 
l0(Z) = z2 + 1 = (z + 1)2• 
• Failure mode B3 
d = 1 1 0 0 0 
s(z) = 14z6 + 9z3, 
l0(Z) = 9z3 + .1 .  
17.10 Burst error performance 
1 0 0 0 1 1 0 0 0, 
One of the reasons for the popularity of RS codes is their burst error 
sensitivity, but the easiest way to construct a code for correcting bursts is 
to 'interleave' the code words of an ordinary code, so that errors that occur 
close together affect different words of the code. Thus compact audio discs 
use a coding scheme that combines RS codes and interleaving. In this section 
we shall briefly investigate burst error correction and compare the gains 
obtained by using RS codes with those obtained by plain interleaving. There 
is no need to modify the error processor for RS codes. Any error processor 
capable of correcting all random errors will also correct all amenable bursts. 
Conversely, however, there are conceptually simpler error processors that 

280 
Error-correcting codes andjinlie fields ... 
can correct all amenable bursts, but not all random errors. One of these is 
described in the exercises. 
Definition Ler C be a code of block length n. We define the r-fold interleaved 
code rC to be of block length rn defined to have as its code .words those 
words consisting of the concatenated columns of any r x n matrix whose 
rows are code words of C. 
Example Let u1 = (u1 1, . . .  , un), . . .  , u, = (u,1, .
.
•
 , u,n) by r codes of C. 
Then the matrix formed by the definition is 
Hence the interleaved code word of rC is 
The effect of using rC is the same as using C, but transmitting the code 
words of C in the following way. Take r code words at a time, transmit their 
first entries in order, then their second entries, and so on until their last 
entries have been transmitted. 
The following proposition is very easy to prove. 
Proposition Let C be a code and let D = rC. Then if C is linear, so is D, and 
if C can correct t random errors, then D can correct any burst of length tr. 
Proof If C is linear, then any linear combination of matrices with code 
words of C as rows will also have code words of C as rows. Consequently 
any linear combination of code words of D will be a code word of D. You 
can also see this directly by looking at the formula for the code words of D. 
Now suppose C can correct t random errors, and Jet c be a code word of 
D written as an r x n matrix. A burst of length tr produces at most t errors 
in each row of the matrix. As C can correct t errors, all these errors can be 
corrected. 
• 
To see whether interleaving produces better or worse results than the 
transition from BCH(k, t) to RS(k, t), we compare the performance of 
RS(4, 3) and RS(4, 4) with codes obtained by interleaving four words of 
BCH(4, 2) and BCH(4, 3), which we denote by 4-BCH(k, t). 
All the codes have block length 60 and their ranks are as follows: 
RS(4, 3): 36; RS(4, 4): 28; 4-BCH(4, 2): 28; 4-BCH(4, 3): 20. 

The maximum length of a burst each can correct is calculated from the 
formulae of the text. 
RS(4, 3): 9; RS(4, 4): 13; 4-BCH(4, 2): 8; 4-BCH(4, 3): 12. 
It follows that the RS(4, 3) corrects bursts of length 1 greater than the 
4-BCH(4, 2) but has much greater rank. A similar statement holds for 
RS(4, 4) and 4-BCH(4, 3). The RS codes significantly outperform the inter­
leaved codes. 
EXTRAS 
17.1 1 Possible syndrome polynomials of RS 
The final part of the chapter is devoted to an analysis of the possible 
syndrome polynomials and polynomials l0(Z) produced by Euclid's algorithm, 
when we drop the condition that the number of errors should be less than 
the designed number t. In this discussion we regard BCH(k, t) as a subcode 
of RS(k, t) consisting of those .words in RS(k, t) that have entries in B. We 
shall prove the claims made in Chapter 16 that certain failure modes cannot 
occur. 
First we establish that there is no limitation (apart from degree) on the 
syndrome polynomials of RS(k, t). 
Proposition Let q = 2k, n = q - 1, and E = GF(q). Then every polynomial 
of degree at most 2t -1 
in E [z] is the syndrome polynomial of some word in En. 
Proof The set of polynomials in E [z] of degree at most 2t - 1 forms a 
vector space of dimension 2t. The mapping taking each word of En to its 
syndrome polynomial is linear and by definition RS(k, t) is its null space. 
Now the rank and nullity theorem tells us that 
dim(RS(k, t)) + dim(syndrome polynomials) = dim En = n. 
The dimension of RS(k, t) is n - 2t (Proposition 17.1). So the dimension of 
the set of syndromes is 2t. As that is the dimension of the set of all 
polynomials of degree 2t - 1, the syndromes must exhaust that space, and 
the claim is proved. 
• 
It follows, as confirmed by the examples, that all failures of Type A can 
occur. 
17.12 Possible syndrome polynomials of BCH 
For BCH(k, t), the situation is more complicated, certainly a syndrome 
polynomial S(z) of a binary word must have S2; = S'f, but the (binary) 

282 
Error-correcting codes and finite fields 
dimension of the set of such polynomials in E[z] is kt, while the dimension 
of BCH(k, t) may well be greater than n - kt. So some 'eligible' syndrome 
polynomials are not syndromes of binary words. However, if we restrict our 
attention to words of weight at most t, then it does follow that if the 
syndrome looks like the syndrome of a binary word, the word was indeed 
binary. 
Proposition Let the notation be as in Proposition 17.1 1. 
(a) If s(z) with coefficients Si of zi- l  in E is the syndrome of a binary word 
v, then for all j, s J = s2j· 
(b) If s(z) is the syndrome of a word u in E" of weight at most t, and for all 
j ȉ t, SJ = S2i, then u lies in B". 
Proof (a) We have already proved this in Proposition 15.3. 
(b) Let u have weight s ȉ t, and considering u as a polynomial let 
M = {i l ui i= 0}. Then 
sj = L Uj rxij 
ieM 
Hence 
and 
SJ = L u[rx2ij. 
ieM 
Now consider the s x s Vandermonde matrix A = (aij) = (a2ii), where i e M  
and j = 1, . . .  , t, and the vectors v = (ui) and w = (u[). The statement that 
S2i = S2 is the same as Av = Aw or A(v - w) = 0. But it was shown in 
Appendix LA of Part 1, that for Vandermonde type matrices the only 
solution of this equation is v - w = Q. Thus u; = u[ for all i in M. Now the 
only roots of x2 - x in any field are 0 and 1. Thus u is a binary vector. • 
Corollary If a binary word d lies within distance t of a word c of RS(k, t), 
then c is binary. Thus error mode C cannot occur for BCH(k, t). 
Proof The error word e satisfies the hypothesis of part (b) of the proposition 
. 
• 
17.13 A final result 
Finally, we shall prove that error mode B2 cannot occur for binary 
BCH codes. This proof is intricate. It depends upon the fact that for 

R.eed.:.so-lomon codes and oiirst error i:orrecuon 
283 
the syndrome polynomial s(z) the derivative s'(z) is congruent to s2(z) 
modulo z21• That is because in characteristic 2 the derivative of an even 
power of z is 0. First we prove a lemma that assumes this fact. 
Lemma Let F be a field of characteristic 2 and let 0 # s(z) e F[z] be a 
polynomial of degree ­ (2t- 1) such that s2(z) = 
s'(z) mod z21 for some t. 
Further let v(z), r(z) e F[z] satisfy 
1. v(z)s(z) = r(z) mod z21• 
2. The highest common factor of v(z) and r(z), (v(z), r(z)) is 1. 
3. deg(v(z)) ® 
t, deg(r(z)) < t. 
Then r(z) = v'(z). 
Proof The first stage of the proof is to divide s(z) by a power of z to make 
sure that it has a non-zero constant term. 
Let z" be the highest power of z dividing s(z). Then n is even, for if it were 
not, then s(z) having a non-zero coefficient of z" would imply that s'(z) had 
a non-zero coefficient of z" - 1. Then s2(z), which is divisible by z2", could not 
be congruent to s'(z). Let u = 2t - n (which is also even) and s(z) = s(z)z". 
From the congruence (1) it follows that r(z) = r(z)z" and v(z)s(z) = 
f(z) mod z". 
Furthermore, since z does not divide s(z), the highest common factor of s(z) 
and z" is 1, (s(z), z") = 1. Also (z")' = 0, so s'(z) = s(z)z" and it follows that 
s2(z)z" = 
s'(z) mod z". 
Now as (s(z), z") = 1, there exist polynomials t(z) and t1(z) such that 
s(z)t(z) + z"tl (z) = 1 .  
In other words 
.S(z)t(z) = 1 mod z". 
In the next part of the proof we several times have to differentiate a product 
f(z)zk with k even. The derivative of zk is 0, and so (using the product rule) 
the derivative of f(z)zk is f'(z)zk. 
In particular this implies that if f(z) = 
g(z)(mod z"), then f'(z) = 
g'(z)(mod z"). Hence 
0 = 1' = s'(z)t(z) + s(z)t'(z) 
= 
s2(z)z"t(z) + s(z)t'(z) 
= s(z)(s(z)t(z)z" + t'(z)) 
= 
s(z)(z" + t'(z)) 
mod z". 
Multiplying the first and last polynomials by t(z) and noting that s(z)t(z) = 
1, 
we see that z" + t'(z) = 
0 mod z". Thus t'(z) = 
z" mod z". 

284 
Error-correcting codes and finite fields 
Now v(z)s(z) = 
r(z) mod z" implies r(z)t(z) = 
v(z) mod z". Hence 
v'(z) = 
r'(z)t(z) + r(z)t'(z) = r'(z)t(z) + r(z)zn mod z". 
Replacing r(z)zn by r(z) and rearranging, we get 
v'(z) + r(z) = 
r'(z)t(z) mod z". 
Hence (v'(z) + r(z))s(z) = 
r'(z) mod z". 
Multiplying by zn 
gives 
(v'(z) + r(z))s(z} = 
r'(z)zn mod z21. 
Denote r'(z)z" by r0(z), v'(z) + r(z) by V0(z) and observe that we now have 
two congruences of the same type 
v(z)s(z) = r(z) 
and 
Multiplying the first congruence by V0(z) and the second by v(z) and 
subtracting we obtain 
V0(z)r(z) - v(z)r0(z) = 
mod z21. 
Observe that deg(r0(z)) < deg(r(z)) < t and deg(vo(z)) < t. Thus the degrees 
of both products are at most 2t - 2. Hence the congruence is an equality. 
Since deg(r0(z)) < deg(r(z)), it follows that deg(v0(z)) < deg(v(z)). Further­
more, v(z) divides V0(z)r(z) and thus, since (v(z), r(z)) = 1, v(z) divides V0(z) 
(we used this implication, which is proved in the corollary to Theorem 7.7, 
in the proof of the uniqueness of the error locator and error evaluator 
Proposition 16.6). 
Now the proof is complete because deg V0(z) < deg v(z). Therefore the only 
way that v(z) can divide V0(z) is if V0(z) = 0. Hence v'(z) = r(z). 
• 
Corollary It is not possible that in attempting to correct the syndrome of a 
binary word Euclid's algorithm produces a locator polynomial that has multiple 
roots, but no zero root. 
Proof If s(z) is the syndrome polynomial of a binary word, then 
2t 
s(z) = L Sizi- 1 
with 
i= 1 
and therefore 
t 
2t 
s'(z) = L S2iz2i-2 
while 
s2(z) = L Sfz2i-2. 
i= 1 
i =  1 
Thus s(z) satisfies the congruence of the lemma. Now suppose that the 

Reed=Solomon codes anilburst ertoY 
correcttoii 
28:J 
decoding algorithm reaches the first stage when deg(r(z)) < t, then 
v(z)s(z) = 
r(z) 
mod z21• 
If r1 (z) and v1 (z) are the previous entries in the R and V columns, then 
r1(z)v(z) + r(z)v1(z) = ±z21, so any common factor of r(z) and v(z) must be 
a power of z. The assumption that v(z) has no zero roots implies that z does 
not divide v(z). Thus (r(z), v(z)) = 1. Furthermore since deg(r1(z)) > deg(r(z)) 
and deg(v1(z)) < deg(v(z)), 
deg r1(z) + deg v(z) = 2t. 
By assumption deg r1(z) Ͼ t; so deg v(z) ­ t. From the lemma it now follows 
that r(z) = v'(z), but then the fact that (r(z), v(z)) = 1 implies that v(z) has no 
multiple roots. 
• 
17.14 Summary 
This chapter covered the basic facts about Reed-Solomon codes. These codes 
use the same syndromes and error processing facilities as BCH codes, but 
have code words taken from F", where F = GF(2k) is the field used to define 
the BCH code. We determined the parameters of Reed-Solomon codes and 
showed how to adapt the BCH error processor to RS codes by adding a 
step to evaluate the error at each location. Reed-Solomon codes are 
particularly useful for burst error correction, as was shown by means of 
examples. In the Extras we gave an analysis of exactly which failure modes 
can actually occur with the Euclidean error processor of Chapter 16. 
17.15 Exercises 
17.1 Encode the message word 9 8 7 6 5 4 3 2 1 using RS(4, 3) and 
systematic encoding. 
17.2 The code RS(4, 3) is used to transmit a message. One received word 
has syndromes S1 = 1, S2 = 1, S3 = 13, S4 = 2, S5 = 1, S6 = 10. Find 
the error pattern assuming that no more than three errors occurred. 
17.3 Verify that 
c =  
11 10001 1100011 10001 110001 11001 1 10001001 1001 11 100011 11010110 
is a code word of RS(4, 3). 
Show that the code can correct every binary burst of length ­ 9, 
but cannot in general correct a burst of length 1 3. In an error burst of 
length 6 some of the underlined digits of c are changed and the other 

286 
Error-correcting codes and finite fields 
digits remain unchanged. What is the error locator polynomial of the 
resulting word? 
17.4 The code RS(4, 3) is used to transmit a message. One received word 
has syndromes: 
S1 
= I I, 
S2 = 1, 
S3 
= 15, S4 = I I, 
S5 = 1 1 , 
S6 = 4. 
What was the binary error pattern? 
17.5 The code RS( 4, 3) is used to transmit a message. One received word is 
2 4 8 5 8 4 2 
8 
14 2 10 12 4. 
Assuming that no more than three errors occurred, what was the 
transmitted word? 
17.6 Construct a generator matrix for RS(4, 3). 
17.7 Verify that the examples given in Section 17.9 satisfy the claims made 
for them. For each word check that it has the claimed syndrome 
polynomial and that the error proces,sor fails as described. 
17.8 Show that the dual code of RS(k, t) (see Exercise 3.22) can be defined 
as the set of values of polynomials of degree at most 2t on the non-zero 
elements of GF(2k), arranged in order of descending powers of the 
primitive element. Find the parameters of the dual code. (In fact the 
dual codes are also Reed-Solomon codes). 
17.9 Construct an error trapping algorithm for RS(k, t) analogous to that 
of Exercise 15.10. Show that this algorithm can correct the same burst 
errors as the full algorithm. Use this method to correct the error burst 
in the received word d of Example I 7.3. Which algorithm is faster? 
17.10 Show that the PGZ error processor of Exercises 1 5.2-6 will also work 
for RS(4, 3). Use it to correct the received word d of Example 17.3. 
Compare this method with the method used in the text. 
17.1 1 Use the fundamental equation to design an error processor for up to 
t erasures in RS(k, t). Use your algorithm to correct 
1 ? 3 ? 5 ? 7 8 9 2 8 2 1 1  0 4. 
17.12 Show that it is not possible for the error processor described in the 
text to produce a valid error locator, but an error value of 0 for one 
of the error locations. In other words, it cannot happen that at the 
end of the calculations vi(z) = K 0 (1 -{Jiz) 
and for some i, say i = 1, 
rj(f31 1) = 0. 

18 Bounds on codes 
In Parts 1 and 3 we have constructed codes that are designed to give a 
certain worst case performance. For such codes Shannon's theorem is not 
an appropriate measure because it concerns the average performance of a 
code. In this chapter we shall prove some simple bounds on the worst-case 
performance of codes and compare our codes with them. 
The ideas of sphere packing used in the discussion of Hamming codes lead 
naturally to such bounds. The Hamming bound which counts the number 
of disjoint balls of a fixed radius that can be placed in A" gives an upper 
bound for the minimum distance of a code in terms of its block length and 
its rate. All codes must lie below this bound, and the closer a code gets to 
it the better it is. This bound is very generous and it is known that there are 
very few codes that actually achieve it. 
If instead we count how many balls are required to cover Ar. we obtain 
the Gilbert-Varshamov Bound, which like Shannon's theorem, promises the 
existence of good codes, but now with respect to worst-case performance. It 
is against this bound that we shall measure our codes. It turns out that for 
short block lengths our codes are good, but as the block length increases 
they fall progressively further and further short of the bound. 
The discussion of the Gilbert-Varshamov bound is the main purpose of 
the chapter, but before we embark on it we introduce a further simple upper 
bound on the worst-case performance of codes, the Singleton bound. The 
Singleton bound is generally tighter than the Hamming bound and it will 
turn out that it is achieved by all Reed-Solomon codes. So in a sense 
Reed-Solomon codes are optimal. Towards the end of the chapter I shall 
explain how this statement squares with the seemingly contradictory one in 
the previous paragraph. 
18.1 The ball of radius r in Bn 
Example 
Suppose you are designing a binary code C of block length 7, 
that is to have minimum distance d = 3. You have just picked the code word 
u = (10001 10). What restrictions does that place on further choices? The 
obvious answer is that you must not choose any word v with d(u, v) < 3 as 
a code word. Before you read on write down all words v with d(u, v) < 3. 
To construct all words at distance less than 3 from a given word u, you 

288 
Error-correcting codes and finite fields 
first have to change one place in u in all possible ways, and then change two 
places in all possible ways. In our example there are 7 places and so there 
are 7 ways of changing one place and 21 ways of changing two places. By 
analogy with geometry we make the following definition. 
Definition In B" the ball Diu, r) with centre u E B" and radius r consists of 
all words v such that d(u, v) Ԧ r. 
Our balls actually have a lot of corners, but they are the nearest we can 
get to a sphere with a discrete distance function like the Hamming distance. 
The calculation you ought to have done just above was to determine all the 
words in the ball of radius 2 around u = (10001 10). There are 29 of 
them-the calculated words plus u itself. From the example it is fairly easy 
to guess the formula for the number of words in a ball. 
Theorem Let D be a ball with centre u E B" and radius r then the formula for 
the number of words in D is 
Definition We will denote this number by V(2, n, r). 
Proof D is the disjoint union of the sets of words v with d(u, v) = k, for 
k = 0, . . . , r. The number of words v at distance k is the number of ways of 
choosing the k places at which v should differ from u. That number is just 
the binomial coefficient G). 
18.2 The 
ball of radius r in general 
Exactly the same argument is possible for an alphabet A with q letters. 
Indeed, the definition of distance is the same as in the binary case so the 
definition of a ball of radius r is unchanged. 
Definition The q-ary ball with Dq(u, r) centre u E A" and radius r consists 
of all words v such that d(u, v) Ç r. 
The formula for the number of words m a ball is a slightly more 
complicated than in the binary case. 

- bounds on codes 
΀ 
--
289 
Theorem Let D be a q-ary ball with centre u E Bn and radius r; then the 
formula for the number of words in D is 
Notation This number will be denoted by V(q, n, r). 
Note that for q = 2 we get the same formula as in Theorem 18.1, because 
then q - 1 = 1. The proof is very similar to the original one. The factor 
(q - l)k comes from the fact that there are now q possible symbols that can 
occur in any chosen place. 
Proof Again D is the disjoint union of the sets of words v with d(u, v) = k, 
for k = 0, .
.
.
 , r. To find a word at distance k we must first choose the k 
places at which v should differ from u. The number of ways of doing this is 
the binomial coefficient (:). Then in each of these places we must choose 
a symbol different from the one u has in that location. There are q - 1 ways 
of doing this. Altogether that gives (q - 1? possibilities once the places have 
been chosen. Thus there are (:)<q - ll words at distance k from u. 
• 
18.3 Code sizes 
Example Let us return to the code you were designing in B7 with minimum 
distance 3. What is the biggest such code you could possibly construct? The 
balls of radius l around distinct code words must be disjoint. For if w is at 
distance l from both u and u', then d(u, u') ­ 2 by the triangle inequality. 
So each code word comes with its exclusive clique of 7 immediate neighbours. 
That tells us that the code can have at most 2 7/8 = 24 = 16 
code words. 
This argument can easily be extended to arbitrary alphabets. 
Theorem The Hamming bound Let C be a q-ary code over the alphabet 
A of block length n and minimum distance d = 2r + 1. Then C has at most 
qn/V(q, n, r) code words. 
Proof The balls of radius r around distinct code words must be disjoint. 
Hence ICI V(q, n, r) ­ IAnl = qn. 
• 
For linear codes we can restate this result in terms of the rate (which, you 
will recall, is the rank divided by the block length). 

290 
Error-correcting codes and finite fields 
Corollary A linear q-ary r-error-correcting code of block length n has rate 
at most 1 - logq V(q, n, r)/n. 
Proof If C has rank m, then ICI = qm. Hence 
m = log9(1CI) ­ n - log9 V(q, n, r). 
So the rate of C is at most (n - Jogq V(q, n, r))/n. 
• 
18.4 Upper bounds 
It is very rare for V(2, n, r) to be a precise power of 2. So the Hamming 
bound is attained by only very few codes. For r = l that happens exactly 
when n + I is a power of 2. That leads to the Hamming codes discussed in 
Chapter 5. It also occurs if n = 2r + 1 (see Exercise 18.1), and there is one 
further case n = 23 and r = 3 corresponding to the Golay code G23. 
It can 
be shown that there are no other possibilities. The parameters of codes 
meeting the Hamming bound for larger q are also known and are even more 
restrictive. 
Because of this we discuss a further simple upper bound which is achieved 
by many codes. 
Theorem The Singleton bound A linear q-ary code of block length n and 
minimum distance d has rank at most n - d + I .  
Proof Delete the first d - 1 symbols in all the code words of the code C. 
As C has minimum distance d, we still have qm distinct words but the block 
length is reduced to qn-d+ l. Hence m ř n - d + 1 .  
• 
Definition A code that meets the Hamming bound is called perfect. A code 
that meets the Singleton bound is called maximum distance separable or MDS 
for short. 
Proposition 17.1 tells us that all Reed-Solomon codes are MDS codes, 
and therefore have optimal parameters for their block-length. 
18.5 The Gilbert-Varshamov bound 
Example Theorems 18.3 and 18.4 tell us about the absolute best that you 
can hope to achieve when you are trying to construct a code of minimum 
distance 3 and block length 7. Now let us ask what the worst that can happen 

Bounds on codes 
291 
is. You are designing a binary code of block length 7 and minimum distance 
3. To do this you just pick code words at distance at least 3 from all the 
code words you already have, and stop when there are none left. You use 
no kind of look-ahead in your choice. So you may well end up with rather 
few code words. What is the smallest number you can possibly end up with? 
Look at the situation when you cannot pick any more code words. Since 
you cannot add a further code word to your code every word in B7 must 
be at distance ;::; 2 from some code word. So the balls of radius 2 around 
the code words must cover the whole of B7. Hence ICI · V(2, n, 2) ;:: 27• 
V(2, n, 2) was calculated in Section 18.2. It is 29. So you will certainly get 
at least L128/29 j = 4 code words. That's a fair bit worse than the maximum 
1 6  we found in-the last paragraph, but since we have assumed no intelligence 
in our search we should not expect a brilliant result. 
Theorem The Gilbert-Varshamov bound There exists a q-ary code C of 
block length n, minimum distance d with IC I  ;:: qn/V(q, n, d - 1). 
Proof Let C be a code in An with minimum distance d. We can assume C 
is maximal in the sense that no word can be added to C without reducing 
the minimum distance. For if C were not maximal we could enlarge it by 
adding a word to it that does not reduce the minimum distance. The 
maximality of C implies that every word in An is at distance ;::; d - 1 from 
a code word. Hence the balls of radius d - 1 around code words cover An. 
Thus 
qn = lAin ;::; ICI V(q, n, d - 1). 
Rewritin੧ this inequality we get 
ICI ;:: qn/V(q, n, d - 1). 
• 
18.6 Achieving the Gilbert-Varshamov bound 
It is worth while to note that the Gilbert-Varshamov bound can always be 
achieved by linear codes. 
Theorem Let C be a linear code of block length n over the field F of order 
q with minimum distance ;:: d. If ICI < qn/V(q, n, d - 1), then there exists a 
linear code C' 2 C in Fn such that the minimum distance of C' is still ;:: d. 
Proof From Theorem 18.5 we can choose a word v such that its distance 
from all words of C is at least d. Construct C' as the set of all sums u - av, 
where u E C and a E F. 
The obviously C' is linear because for a, a',bb' E F 

292 
Error-correcting codes and finite fields 
and u, u' E C 
b(u + av) - b'(u' + a'v) = (bu + b'u') - (ba + b'a')v; 
bu + b'u' E C because C is linea:r and ba + b'a' E F. Thus the sum is in C'. 
C' also contains C (take a = 0). To show that C still has minimum 
distance ̎ d, we must show that every non-zero code word has weight at 
least d. Let w = u - av be a non-zero code word, then for a = 0, w E C, and 
so by assumption it has weight ̎ d. If on the other hand a =I 0, let b = a - 1 •  
Then 
wt(w) = wt(bw) = wt(bu - v) = d(bu, v). 
Since C is linear bu is a code word of C, and v was chosen to have 
distance ̎ d from all code words of C. Thus d(bu, v)  d. This establishes 
that all non-zero code words of C' have weight at least d as required. 
• 
Corollary 
There exists a linear code of hlock-length n and minimum distance 
d over an alphabet of size q with rank at least n - logq( V(q, n, d - 1) and hence 
rate at least 1 - (logq( V(q, n, d - 1))/n. 
Proof Choose a code meeting the Gilbert-Varshamov bound. Then it has 
at least q"/V(q, n, d - 1) code words. Thus its rank m is at least 
logq(q"/ V(q, n, d - 1) = n - logq( V(q, n, d - 1). 
The statement about the rate follows immediately from the definition of 
ॡॢॣ। 
. 
18.7 Short blocks 
For short block lengths many codes surpass the Gilbert-Varshamov bound. 
Examples 
• 
The (8, 7) parity check code has minimum distance d = 2, because it 
consists of all the words of even weight. The Hamming bound does not apply 
to even minimum distances and the Hamming bound for minimum distance 
d = 3 is the same as the Gilbert-Varshamov bound for d = 2. The size of 
code this gives is 28/ID11 = 28/9 ϊ 25. So this code surpasses the Gilbert­
Varshamov bound. 
• 
The (3, 1) repetition code has minimum distance d = 3. The Hamming 
bound says that the size of this code must be most 23 /ID11 = 23/4 = 2. So this 
code is one of the few that meets the Hamming bound. 

Bounds on codes 
293 
The (6, 
3) triple check code also has minimum distance d = 3. The 
Hamming bound gives the maximal possible size for such a code as 
26/ID1l =·26/7 = 9t. The Gilbert-Varshamov bound says that a code exists 
ofsize 26/ID21 
= 26/22 = 2Ԕԕ. The code has 8 code words so it is quite good. 
18.8 Longer blocks 
The Gilbert-Varshamov bound becomes harder to achieve as the block 
length of codes increases. But using a long code meeting the bound for a 
given rate to transmit a message will greatly improve the error performance 
(see Exercise 18.5). So to make the bound into a measure for families of 
codes we need to be able to compare codes of differing block lengths. We 
can replace the rank by the rate, which for an (n, m)-code is m/n, but we 
need a similar length-independent measure to replace the minimum distance. 
Definition For a code C of block length n 
and minimum distance d, the 
relative minimum distance is dfn. 
We shall also need an estimate for the second term of the Gilbert­
Varshamov bound in which n no longer appears explicitly. To this end we 
introduce the q-ary analogue of the 'binary entropy' function that appears 
in Shannon's theorem. 
Definition For 0 :::; b Ç (q - 1)/q we define the q-ary entropy function Hq(b) 
by Hq(O) = 0 and, 
Hq(b) = b logq(q - 1) - b logq(b) - (1 - b) logq{1 - b). 
Proposition For all 0 < b < (q - 1)/q, Hq(b) ;;:: bq/(q - 1) with equality for 
[) = 0, (q - 1)/q. 
Proof For 0 < b < (q - 1)/q, the derivative of Hq(b) is 
logq(q - 1) - loၕ(b/(1 - b)) ;;:: 0. 
Its second derivative is (£5- 1)/(b ln(q)) < 0. 
So the curve y = Hq(x) is concave, constantly turning clockwise. 
For b = (q - 1)/q, Hq(b) = 1. Hence the straight line joining the origin to 
((q-1)/q, 1) 
must stay below the curve everywhere. 
• 

294 
Error-correcting codes and finite fields 
18.9 A lemma 
With the help of this function we can replace the function V(q, n, d - 1) in 
the Gilbert-Varshamov bound. 
Lemma Let 0 :;:; [) :;:; (q - 1)/q, and for any integer n let r = r(n) be the 
greatest integer such that r :;:; lJn. Then 
(a) logq( 
V(q, n, r)) Á nHq(lJ) and 
(b) The limit of n- 1 logq(V(q, n, r)) 
is Hq(lJ). 
The proof of this lemma is given in the. Extras. 
18.10 The asymptotic Gilbert-Varshamov bound 
Theorem The asymptotic Gilbert-Varshamov bound For all [) :;:; (q - 1)/q, 
there exists a sequence en of linear block codes over GF(q) with block length 
of en = n, the relative minimum distance of en tending to lJ, and rate tending 
to 1 - Hq( lJ). 
Proof Let r be the greatest integer satisfying r :;:; lJn. By Theorem 18.6 there 
exists a linear code en over GF(q) of block length n, minimum distance r + 1, 
and rate mjn Á 1 - n- 1 log(V(q, n, r)). By Lemma 18.9 the limit of the rates 
of the codes en is 1 - Hq(C>). To determine the limit of the relative minimum 
distances observe that C>n < r + 1 :;:; lJn + 1. Hence () 
< (r + 1)/n :;:; () 
+ 1/n. 
Hence the limit of (r + 1)/n as n tends to infinity is lJ. 
• · 
We use this theorem to define good and bad classes of codes. 
Definition Let W be a family of codes. We call W bad if, for any infinite 
sequence of codes in W, either the rate tends to 0, or the relative minimum 
distance tends to 0. We call W good if it contains an infinite sequence of 
codes that tends to the Gilbert-Varshamov bound. The value 1 - Hq(lJ) 
is 
called the capacity of the q-ary channel. 
Note that while good and bad are certainly mutually exclusive, 'not bad' 
does not imply 'good'. 
Example Hamming codes. For these the rate is (2k - k - l)/(2k - 1), which 
tends to 1 as the block length increases. On the other hand the minimum 
distance is always 3, so the relative minimum distance is 3/n, which tends to 
0. Thus Hamming codes are a bad family. 

· -Bounds -on codes - · · · ·  
18.1 1 BCH codes 
A similar argument can be applied to the design parameters of BCH codes 
to show that with this measure they are also a bad family. 
Proposition Let Ck = BCH(k, t) be a sequence of BCH codes such that 
the block length n = 2k tends to oo, and the designed relative minimum dista;nce 
is greater than e for some e > 0. Then the designed rate of the codes drops 
below 0. 
Proof The designed minimum distance of Ck is 2t + 1, so the designed 
relative minimum distance is (2t + 1)/n. Thus if this is to remain above e, 
we must have t > en/2 - 1. On the other hand, the designed rank of the code 
is n - kt. As k = log2(n) it follows that the designed rank is at most 
n - (en/2 - 1) logz(n). But when n is sufficiently large log2(n) > 3e, so 
n - (en/2 - 1) logz(n) < n - 3n/2 + log2(n) = log2(n) - n/2 < 0 
In other words, if the designed rate stays above 0, then it is impossible for 
the relative minimum distance to remain above e for large n. 
• 
From this proposition it is clear that the designed values must become 
poor estimates of the true dimension and true minimum distance of BCH 
codes. However, one can find upper estimates for the true values for 
minimum distance and dimension and show that even with these estimates 
the conclusion of the proposition holds (see MacWilliams and Sloane (1977), 
Chapter 9), not only for our BCH codes but for all codes of BCH type 
defined over any finite field F. Such codes are discussed briefly in Chapter 
19 in the context of th੨ even more general class of classical Gappa codes. 
They are treated in some detail in Blahut (1983) and MacWilliams and 
Sloane (1977). 
Reed-Solomon codes belong to the BCH class, and so form a bad family. 
That would appear to contradict the optimality of Reed-Solomon codes, 
which can, after all, be constructed with arbitrarily long block lengths. But 
to increase the block length of Reed-Solomon codes you must also increase 
the size of the alphabet. That changes the meaning of the minimum distance. 
If you follow the burst error correcting route and re-interpret the alphabet 
symbols as blocks of binary bits, then the parameters of the Reed..:.Solomon 
code are no longer optimal, because its block length and rank are multiplied 
by a constant k, but its minimum distance remains constant. So in that 
interpretation they form a bad family. On the other hand, if you retain the 
symbols as your base units, then the block length of the Reed-Solomon code 
cannot increase above the size of the alphabet. 

296 
Error-correcting codes and finite fields 
EXTRAS 
18.12 The 
missing proof 
Proof of Lemma 18.9 Note that 0 ::; 1 - o ::; 1/q. Hence for any k Ԓ 0, 
(jk::; 
(q -1)k/l::; 
(q -1)k(1 
-w. 
Thus for 0 ::; i ::; on, taking k = on - i we get 
o<7n -i) ::; ( q - l)<8n -i)(l - o)<8n - i) 
and hence separating powers and multiplying by (1 - o)n 
Now 
1 = 1 n = (o 
+ (1 - o)Y 
Ȣ ± ()(q - 1i (-0-)j (1 - oyn- i) 
i = O  
I 
q - 1 
Ȣ ± ()(q - 1i(-
(j
-)
7n 
(1 - oyn - 8n) 
i = O  
I 
q - 1 
= V(q, n, r)q -nHq(8l. 
Taking logarithms to base q gives 
0 Á logq( V(q, n, r) - n · Hq(o), 
proving (a). 
For the proof of (b) we need to apply Stirling's formula for In (n!): 
(n + !) 
ln (n) - n + K ::; In (n!) ::; (n +!) In (n) - n + K + 1/(12)n), 
where K is a constant (In (2n)/2). If we convert to logarithms to the base q, 
the constant changes and we get 
(n + !) 
logq(n) - n logq(e) + K' ::; logq(n!) 
::; (n + !) 
logq(n) - n logq(e) + K' + logq(e)/(12n). 
Certainly V(q, n, r) is at least as large as any of the terms in the sum defining 
it. Thus 
V(q, n, r) Ȣ (;)<q- 1)', 

Bounds on codes 
297 
We use Stirling's formula and (omitting subscripts q) get: 
log( V(q, n, r)) Ȣ (n + i) 
log(n) - (r + i) 
log(r) 
- (n - r + i) 
log(n - r) + r log(q - 1) 
- n log( e) + r log( e) + (n - r) log( e) + C 
- log(e)/12r - log(e)/12(n - r). 
When we divide by n and let n tend to oo ,  we can ignore the terms that tend 
to 0 and get, 
lim(n- 1 log(V(q, n, r))) Ȣ lim(log(n) - (r/n) Iog(r) 
- ((n - r)/n) Iog(n - r) + (r/n) log(q - 1)). 
Now, if we choose r as the greatest integer satisfying r ­ t5n, then as n tends 
to infinity t5n/r and (1 - t5)n/(n - r) both tend to 1. Hence 
lim(n- 1 Iog(V(q, n, r))) 
Ȣ lim(log(n) - (j log(Jn) 
- (1 -- J) log((1 - J)n) + t5 ·Iog(q - 1)) 
= lim(log(n) - t5 log(J) - (j Iog(n) 
- (1 - ()) log(1 - ()) - (1 - J) log(n) + (j log(q - 1)) 
= lim( -(j 
log(J) - (1 - J) log(1 -J) 
+ t5 log(q - 1)) 
= Hq(t5). 
As log(V(q, n, r)) ř nHq((;) for all n, the limits must be equal. 
• 
18.13 Summary 
In this chapter we introduced some elementary bounds on block codes. The 
Hamming bound and the Singleton bound give upper limits for the rate of 
any q-ary code with a given block length and minimum distance. These are 
rather primitive bounds and several better bounds are discussed in van Lint 
(1982), McEliece (1977) and MacWilliams and Sloane (1977). The Gilbert­
Varshamov bound guarantees the existence of linear codes with a reasonable 
rate, provided the relative minimum distance is not too large. We used this 
bound in its asymptotic form to define good and bad classes of codes. 
The currently most favoured block codes for implementation, the BCH 
family, form a bad class. What that implies, just as for Hamming codes, is 
that the gains available by increasing the block length are limited. Reed­
Solomon codes are optimal for their alphabet. There are many codes of BCH 

298 
Error-correcting codes and finite fields 
type of moderate block lengths with quite good parameters, but very soon 
the codes fall short of the Gilbert-Varshamov bound and for large block 
lengths they become very poor. 
In Part 4 we shall describe two important good classes of codes, both due 
to the Russian mathematician N. V. Goppa. The classical Goppa codes are 
a generalization of BCH codes to which the standard error processors can 
be applied. Their designed parameters are sometimes better than those of 
corresponding BCH codes, but they remain bad. However, we shall show 
that with respect to the true minimum distance and rank there is a good 
family of classical Goppa codes. This result is of limited use, as the proof 
does not show how to construct the family, and the error-correction 
algorithms can only exploit the designed parameters. 
The more recent geometric Goppa codes have much better parameters, 
but they are harder to construct and cannot use standard BCH error 
processors. The error-processing algorithms developed for them so far are 
not sufficiently powerful or efficient. If better error processors appear, they 
may well become the dominant codes of the future. 
18.14 Exercises 
18.1 Show that V(2, n, 1) is a power of 2 if and only if n + 1 is a power of 
2. Show also that V(2, 2r + 1 ,  r) is a power of 2. Which perfect codes 
correspond to the second of these cases? 
18.2 Show that the bound obtained in Exercise 4.6 is weaker than the 
Singleton bound. 
18.3 Compare the Singleton bound and the bound of Exercise 4.6 with the 
Hamming bound. 
18.4 Prove that a binary code with block length n and minimum distance ࡨ 
2n/3 must be a repetition code. Deduce that, for some channels, it is 
impossible to meet the performance of Shannon's theorem using only 
the error-correction capability given by the minimum distance of a 
code. 
18.5 A message of 10 000 bits is transmitted over a channel with error 
probability 0.004. Four codes of block lengths 10, 20, 40, 100 and rate 
t are available, each meeting the Gilbert-Varshamov bound (exactly). 
Calculate the transmission error probabilities. 
18.6 For all n construct a binary linear code of block length n and 
minimum distance 2 with maximum rank. 
18.7 Let C be a binary linear code. Show that either all code words start 
with a 0, or exactly · half the code words do. Explain why the same 
statement is true for any other fixed position in a word. 
18.8 Deduce from Exercise 18.7 that for a binary linear code C of block 

Bounds on codes 
length n and dimension m, 
L wt(u) ::s;; n ·2m- l .  
ueC 
299 
18.9 The Plotkin bound It follows from Exercise 18.8 that the minimum 
distance of a binary {n, m)-code d satisfies d ::s;; n · 2m- 1/{2m - 1). Prove 
this. 
18.10 Compare the Plotkin bound with the Hamming and Singleton bounds. 


Part 4 
Classical and geometric Goppa codes 


19 
Classical Goppa codes 
It is a sad fact that long BCH codes are bad in the sense that their rate 
and relative minimal distance cannot both be bounded away from 0. As 
block lengths increase the performance of the codes deteriorates instead of 
improving as it should. 
The Russian mathematician N. V. Goppa (1970) invented an extended 
class of codes that contains the BCH codes as a special case. He proved that 
this class of codes contains sequences of codes that approach the Gilbert 
bound as the block length increases, though there is still no explicit 
construction for such a sequence. Goppa showed that for these codes a 
variant of the fundamental equation for BCH codes holds. From this it is 
straightforward to design an error processor modelled on Peterson's BCH 
error processor. The Sugiyama, Kasahara, Hirasawa, and Namekawa error 
processor, derived from Euclid's algorithm, was actually designed for these 
'classical' Goppa codes and thus made their use a practical proposition. This 
class of codes forms the topic of the first two chapters of this part. 
In 1980 Goppa took his ideas further, extending his definition by means 
of algebraic curves over a field. His new 'geometric' class of codes contains 
many explicit codes that exceed the Gilbert bound. That makes them 
potentially very exciting. However, this time Goppa did not find an 
equivalent of the fundamental equation. A weak equivalent was discovered 
(but not published) by Justesen in the second half of the 1980s and it was 
used by Skorobogatov and Vliidut (1988) to design an error processor for 
a subclass of Goppa's geometric codes. The subclass still contains many 
explicit codes exceeding the Gilbert bound. The error processor is a variant 
of the Peterson BCH error processor. So, although it is still inefficient in 
comparison with the more modern BCH error processors, the first step 
towards a practical use of Goppa's geometric codes has been achieved. 
In the last four chapters I shall introduce these powerful and exciting 
geometric codes. The basic facts of algebraic geometry will be presented with 
simple examples, but a full introduction to algebraic geometry is unfortun­
ately beyond the scope of this book. So while the development is sufficient 
to enable you to understand the codes, a few deep theorems will be presented 
only by example. 
For the time being, however, we remain on familiar ground. In this chapter 
we introduce the classical codes and in the next we shall show how the BCH 

304 
Error-correcting codes and finite fields 
error processor can be adapted to them, and then prove that in contrast to 
BCH codes they are a good family. 
19.1 The basic idea 
Consider the fundamental equation for BCH codes: 
s(z) = w(z) _ u(z)z2' . 
l(z) 
l(z) 
Goppa's idea is to replace the polynomial z2' by an arbitrarily chosen 
polynomial g(z), define the code so that the fundamental equation still holds 
and then solve that equation by analogous methods to the ones used for 
BCH(k, t). 
The first step is to rewrite the syndrome polynomial for a BCH code in 
a form that exhibits the 'BCH polynomial' z2' explicitly. We take our cue 
from Proposition 15.8 (except that now we calculate s(z) in terms of the 
received polynomial d(x) = d"_ 1x"- 1 + 
· 
· 
· + d0). 
Proposition The BCH syndrome polynomial s(z)for the polynomial d(x) can 
be expressed as 
n 
d ,al 
n d .a;<2t + l)jz2r 
s(z) = L _J
_
. - L J 
. 
J=O 1 - a1z 
J=O 1 - a1z 
Proof The syndrome is defined by the formula 
2r- l 
n 
2r- l 
s(z) = L si+ 
1zi = L dp.i L aiizi 
i = O  
j = O  
i = O  
(1) 
Using the lemma on summing a geometric progression (with q = alz) to 
evaluate the inner sum we get 
n 
. 1 .- a;2tiz2' 
s(z) = L d1a1 
. 
J=O 
1 - a1z 
n 
d .a;i 
n d .a;<2r + l)jz2t 
= L -1-· - L 
J 
• 
i=O 1 - a1z 
J=O 1 - a1z 
19.2 Goppa polynomials 
• 
We can consider this result to say that modulo z2', the syndrome polynomial 
of d(x) with respect to BCH(k, t) is 
n 
d .a;i 
s(z) = L -1-. 
, 
J=O 1 - a1z 

Classical Goppa codes 
305 
If we replace IX by its inverse a - 1  
= y, and s(z) by -s(z), the equation of 
Section 19.1 can be rewritten in the form 
n 
-d· n d-
-s(z) = L . 
J . 
= L _J __ _ 
j =O yJ(l - y-Jz) 
j= O Z - yJ 
Of course -s(z) is just as good a syndrome as s(z)-indeed, in characteristic 
2 it is the same. So we use this formula as the basis for our definition of 
Goppa codes. 
With this definition of the syndrome it is no longer true that code words 
are characterized by the equation s(z) = 0. They only satisfy the congruence 
s(z) = 0 mod z21, but we shall see that that is no great disadvantage. More 
serious is the problem of interpretation. What is the meaning of the fractions 
1/(z - yi) in the formula? We shall discuss that in the next paragraph. 
It turns out that there is no need to choose the values yi as successive 
powers of some primitive element, so long as they are not roots of the defining 
'Goppa polynomial'. 
·· 
Definition 
Let F £; E be finite fields, let g(z) be a polynomial over E, and 
let P = {PI> ... , Pn} 
be a set of elements of E such that for i = 1, . . .  , n, 
g(p;) '# 0. Then the Goppa code GC(P, g) can be defined as the set of words 
d E pn such that 
n 
d. 
s(z) = L _J_ = 0 
J= l z -PJ 
(modulo g(z)). 
The polynomial g(z) is called the Goppa polynomial of the code. If E = F, 
we speak of a full Goppa code; otherwise of a sub.field Goppa code. 
Reed-Solomon codes are full Goppa codes with Goppa polynomial z2' 
and BCH codes are subfield Goppa codes with the same Goppa polynomial. 
Notice that the Goppa polynomial is defined over the larger field and that 
the subfield code is not obtained by interpreting the symbols of the full code. 
It consists of those words of the ful code with al their entries in the subfield.· 
Definition A general BCH code is a Goppa code with Goppa polynomial 
z2' 
and P consisting of the set of successive powers of an element a of E. 
It is not hard to extend the full theory of BCH codes including their 
generator and check polynomials to these general BCH codes (see the 
Exercises of Chapter 20). 
19.3 Congruences 
How should the congruence defining Goppa codes be interpreted? The 
syndrome is defined as a rational function. To use it in this form put all the 

306 
Error-correcting codes and finite fields 
fraction in the sum defining s(z) over a common denominator. Then 
Li= l di Tii= l (z -/3) 
s(z) = 
i # · 
Tii= l (z -/3) 
Denote the numerator and denominator of this fraction by n(z) and u(z) 
respectively. Now as g({J) # 0, it follows that u(z) and g(z) are relatively 
prime. Thus from Euclid's algorithm it follows that there is a polynomial 
h(z) such that u(z)h(z) = 1 mod g(z). Now if g(z) divides s(z) then it divides 
s(z)u(z) = n(z) and conversely, if g(z) divides n(z) then it divides n(z)h(z) = 
s(z). So if we use the following definition of congruence for rational functions 
our theory will be consistent with the computational polynomial approach. 
Definition Congruences for rational functions Let s(z) be a rational func­
tion, the representation n(z)/u(z) of s(z) is said to be cancelled or in lowest 
terms, if the highest common factor of n(z) and u(z) is 1. If g(z) is a polynomial 
then the congruence 
s(z) = 0 mod g(z) 
means that in the representation of s(z) as n(z)/u(z) in lowest terms, g(z) 
divides n(z). It follows that g(z) and u(z) must be relatively prime (see Exercise 
19.4). 
We shall say that g(z) divides the rational function s(z) if s(z) = 0 (mod g(z)). 
For two rational functions s(z) and t(z), the congruence 
s(z) = t(z) mod g(z) 
means that s(z) - t(z) = 0 mod g(z). 
19.4 Another approach 
Instead of defining congruences for rational functions we could adopt the 
approach that we used in Chapter 15. As all the denominators (z - {3) are 
relatively prime to g(z) we can find polynomials hi(z) such that 
hi(z)(z - {3) = 1 mod g(z). 
Then in the formula for s(z) each term 1/(z - P) can be replaced by h/z), 
and we get a syndrome polynomial which we shall denote by sǐ(z). The 
rational and polynomial forms of the syndrome each have their advantages. 
So we shall establish that they define the same code and use both. 
The polynomials hi(z) that function as inverses of (z - pi) modulo g(z) 
are obviously not unique, but it is useful to make a fixed choice. The 

Classical Goppa codes 
307 
proposition below gives a formula for the lowest degree polynomials that 
can be used. 
· 
Proposition Let g(z) be a polynomial with coefficients in a field F and let fJ 
be an element of F. Then (z - f:J) divides g(z) - g(f:J). Thus there exists a 
polynomial k(z) such that (z - p)k(z) = g(z) - g(f:J). If,furthermore, g(f:J) =1: 0, 
then putting h(z) = -k(z)/g(/3) it follows that h(z)(z - {3) = 1 mod g(z)). The 
polynomial h(z) defined in this way is the unique solution of the congruence 
h(z)(z -f:J) 
= 1 (mod g(z)) with deg(h(z)) < deg(g(z)). 
Example 
Let F = GF(16), g(z) = z3 + z + 1, and f3 = 5. Then g(f:J) = 7 and 
g(z) - g(f3) = z3 + z + 6 = (z - 5)(z2 + 
5z + 9). So h(z) = 14z2 + 4z + 3. 
Proof Let f(z) = g(z) - g(f3). Then f({J) = 
0. Hence (z :- {3) divides f(z), 
and k(z) = f(z)/(z - f:J) is a polynomial. With h(z) defined as -k(z)/g(f:J) it 
follows that 
h(z)(z - p) = -k(z)(z - f3)/g(f3) =-f(z)/g(f3) = 1 -g(z)/g(f3) = 1 
(mod g(z)). 
Clearly, deg(f(z)) = deg(g(z)). Hence deg(h(z)) < deg(g(z)). If there is a 
second solution of the congruence, h0(z) =1: h(z), ,with degW(z)) < deg(g(z)), 
then (h - h0)(z)(z -{3) 
= 0 (mod(g(z)). But deg((h __: h0)(z)) < deg((g(z)). Hence 
(h - h0)(z)(z - {3) = g(z). Therefore g({J) = 0, contrary to our assumption . 
• 
19.5 The syndrome polynomial 
We can use Proposition 19.4 to remove all the fractions in the definition of 
s(z). If 
· 
n 
d­
s(z) = L -1-
i= l Z -{Ji 
we can replace 1/(z - {3i) by hi(z) to get a polynomial 
n 
sp(z) = L dihiz). 
j= 1 
Definition 
The function s(z) is called the rational form syndrome of d(x). 
The polynomial sp(z) is called its syndrome polynomial. 
If the degree of g(z) is t, then the degree of hj(z) is at most t - 1. Thus 
the congruence sp(z) = Q (mod g(z)) reduces to an equation, sp(z) = Q. In the 
next proposition we shall show that the polynomial and the rational function 
both define the same code. 

308 
Error-co"ecting codes and finite fields 
Proposition Let P = {P1, •
•
•
 , P,.} and let g(z) be a polynomial such that 
g(p1) 
# 0 for all j, and deg(g(z)) = t. Further let h1(z), j = 1, . . .  , n, be the 
polynomials defined in Proposition 19.4, such that hj(z)(z - fJ) = 1 (mod g(z)). 
For any word d = (dh . . . , d,.), let s(z) = L d1/(z - P1) and sp(z) = L d1h1(z). 
Then s(z) = sp(z) (mod g(z)) and s(z) = 0 (mod g(z)) if and only if sp(z) = 0. 
Thus the two syndromes define the same code. 
Proof For convenience reorder the indices so that d1, .
.
• , dk are the 
non-zero entries of d. Then the two forms are given by the formulae 
and 
ω 
sp(z) = L d1h/z) 
j= 1 
n(z) 
D= 1  di Tif=Ǐ (z - PJ 
s(z) = - = 
k 
, ,. 
u(z) 
Tii= 1 (z - Pi) 
The rational form is already cancelled. To verify that, observe that each 
factor of u(z) divides all the summands of n(z) except one. So it cannot divide 
the whole sum. 
Now s(z) - sp(z) = n(z) - u(z)sp(z))/u(z), and to show that s(z) and sp(z) 
are congruent, we must show that g(z) divides n(z) - u(z)sp(z), or equivalently 
that the polynomial congruence n(z) = u(z)sp(z) (mod g(z)) holds. But since 
(z - P)hiz) = 1 (mod g(z)), modulo g(z) we have 
k 
k 
k 
k 
u(z)sp(z) = TI <z -/J;) I djhiz) = I dj I <z - fJ;) = n(z). 
i = l 
j= l 
j= l 
i = l 
i,,j 
That establishes the congruence. Finally, deg(sp(z)) < deg(g(z)), and hence 
sp(z) = Q (mod g(z)) 
implies sp(z) = Q. 
• 
19.6 Two ful Goppa codes 
Example 
We choose F = GF(16). We shall construct the codes with Goppa 
polynomials 
g(z) = z3 + z + 1 
and 
g1(z) = z6 +z2 + 1 .  
The polynomial g(z) is irreducible over GF(16) and thus has no roots in 
GF(16). So we can take all elements of GF(16) as the set P for both codes, 
giving us codes of block length 16. Thus our first code GC1 consists of all 
words (d0, •
•
•
 , d15) of length 16 with entries in GF(16) such that 
15 
d 
s(z) = L -1 
• = 0 
(mod x3 + x + 1). 
j= O Z - }  

Classical Goppa codes 
309 
and the second code GC2 consists of the words satisfying a similar congruence 
with respect to g2(z). Thus GC2 is a subcode of GC1• 
We calculate the inverse polynomials hiz): k;{z) of (z - j) with respect to 
g(z) and g2(z) by the formula of Proposition 19.4. They are given in the table 
below. 
j 
g(j) 
h(z) 
g2(j) 
k(z) 
z2 
z 
1 
zs 
z4 
z3 
zl 
z 
1 
0 
1 
1 
0 
1 
1 
1 
0 
0 
0 
1 
0 
1 
1 
1 
1 
0 
1 
1 
1 
1 
1 
0 
0 
2 
1 1  
10 
13 
9 
10 
1 1  
1 5  
7 
14 
14 
5 
3 
13 
9 
2 
15 
7 
14 
1 1  
4 
12 
3 
5 
4 
10 
1 1  
7 
14 
1 1  
10 
3 
12 
2 
2 
8 
5 
7 
14 
4 
3 .  
12 
2 
10 
9 
6 
5 
8 
6 
2 
12 
3 
6 
4 
6 
13 
5 
7 
13 
5 
7 
9 
13 
8 
7 
14 
7 
12 
15 
6 
12 
15 
8 
12 
2 
9 
5 
6 
4 
1 1  
14 
13 
8 
15 
9 
1 1 
10 
12 
2 
10 
1 1  
5 
6 
4 
4 
15 
10 
10 
1 1 
1 
1 
1 1  
10 
1 1  
1 
10 
1 
10 
1 1  
1 1  
10 
1 
1 
10 
1 1 
10 
1 
1 1  
1 
1 1  
12 
14 
7 
15 
12 
2 
12 
6 
3 
13 
6 
3 
13 
4 
6 
5 
13 
9 
13 
7 
8 
12 
7 
8 
14 
10 
1 1  
6 
4 
1 1  
10 
8 
13 
9 
9 
3 
15 
6 
4 
14 
8 
13 
9 
10 
2 
7 
15 
3 
By Proposition 19.5, d = (d0, •
•
•
 , d1 5) is a code word if the sum 
1 5  
15 
L dihj(z) 
= 0 
or 
L dik;{z) 
= 0, 
j= O 
j= O 
as the case may be 
19.7 A check 
matrix 
Polynomial addition and vector addition are the same, so we can construct 
a check matrix from the polynomial form of the syndrome. 
Proposition 
Let C = GC(P, g) with P = {/J1, 
•
•
• , Pn} 
and deg(g(z)) = 
t. 
Further let IJj(z), j = 1, .. . , n, be the polynomials defined in Proposition 19.4 
with hj(z)(z -P) 
= 1 (mod g(z)). Construct a t x n matrix H = (hli), so that 
for i = 0, . . .  , t - 1, hli is 
the coefficient of zl-l in hiz). Then f(Jr any word d 
with syndrome sp(z). 
if Hd 
= (s1, 
. . .  , s,)T, then sp(z) 
= }:  sizt-i. Thus H 
is 
a 
check matrix for C. 

310 
Error-correcting codes and finite fields 
Example The check matrices obtained from the proposition and the table 
in Example 19.6 are as follows. 
• 
Check matrix for GC1: 
[ 1 
1 
10 
9 
1 1  14 12 1 3  2 10 1 1  10 
0 
1 
1 3  
2 
7 
1 
0 
9 
15 14 
• 
Check matrix of GC2: 
4 
3 
3 
6 
8 
9 12 
7 
5 
2 
1 
1 1  14 10 
2 
6 
7 
4 
1 1  10 
1 1  12 13 10 
9 
0 
15 1 1  
3 
10 
1 3  12 1 1 
5 
1 1  10 
6 
7 
8 
10 
0 
7 
4 12 
0 1 
14 12 
2 
9 
5 
15 14 
6 
3 
8 
1 3  
2 
6 
7 
6 
13 
4 10 
1 1  1 3  12 
9 
7 
0 
14 
3 
2 
0 0 
5 
5 
8 
5 
13 12 
8 
4 
8 
5 
15 1 5  15 10 
1 1  
Proof Let d = (d1, •
•
•
 , dn). The polynomial 
n 
n 
t 
sp(z) = L dihj(z) = L L dih;izt-i. 
j= l  
j= l i = l  
6 
7 
9 
1 5  
3 
8 
3 
3 
The coefficient of zt-i 
in this polynomial is 2:J h;1d1 = s; , proving the claim. 
The second statement follows because d is a code word if and only if sp(z) 
= Q . 
• 
19.8 Standard matrix form 
Exa,mple 
Reducing a check matrix with standard row operations does not 
change the code. Thus we can transform the matrices into standard form. 
• 
Standard form check matrix for GC1 : 
[
2
8
1 
8 
5 
8 
5 
3 
10 7 
10 
11 
10 
5 3 9 
15 7 
4 
3 
1 1  2 6 
9 7 1 1  
9 1 1  10 
14 
9 14 : 5 
0 
41] 
2 13 12 1 1  0 0 

Classical Goppa codes 
• 
Standard form check matrix for GC2: 
8 
14 
7 5 
3 1 1  1 1  
4 
9 13 1 0 0 0 0 0 
6 
14 13 1 
9 14 
6 
9 
6 
3 0 1 0 0 0 0 
14 13 13 2 13 
2 
8 
12 
13 
2 4 
6 
1 1  13 
4 
3 14 0 0 
1 0 0 0 
1 
14 
8 0 0 0 
0 0 
3 12 
8 5 
1 1  
8 
3 3 
4 
6 
14 
2 12 14 0 0 0 0 1 0 
6 12 
6 
14 13 14 0 0 0 0 0 1 
311 
Now we can produce standard form generator matrixes for the codes. 
• 
Standard form generator matrix for GC1: 
1 
0 
0 
0 
1 
0 
0 
0 
1 
0 
0 
0 
0 0 0 
0 0 
0 
0 0 0 
0 0 
0 
0 0 0 
0 0 
0 
1 0 0 
0 0 
0 
0 
0 
0 
0 
0 
0 
0 
0 1 0 
0 0 
0 
0 
0 .  0 0 1 
0 0 
0 
0 
0 
0 0 0 
1 0 
0 
0 
0 
0 0 0 
0 1 
0 
0 
0 
0
0
0
 0
0
 
0 
0 
0 
0
0
0
 0
0
 
0 
0 
0 
0
0
0
 0
0
 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0 
1 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
0 
0
0
0
0
0
0
0
0
0 
0 
0 
0 
0
0
0
 0
0
 0 
8 
8 
5 
8 5 3 
10 7 10 
0 
0 
1 
0 
0 
0 
9 
1 1  10 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
1 
5 
1 
1 1  10 
5 3 
9 
15 7 
14 
9 
14 
4 
2 
4 
3 
1 1  2 6 
9 7 
1 1  
2 13 12 1 1  

312 
Error-correcting codes and finite fields 
• 
Standard form generator matrix for GC2: 
0 
0
0
 0 
0 
0 
0 
0 
0 
0 
1 
0 0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 
0 . 0 
0 
0 
0 
0 
0 
0
1
0 
0 
0 
0 
0 
0 
0 
0 
0 0 
0 
0 
0 
0 
0 
0 
0 
0
0
 0 
0 
0 
0 
0 
0 
0 
0 0 
0 
0 
0 
0 
0 
0 
0 
0
0
 0 
0 
0 
0 
0 
0 
0 
0 0 
0 
0 
0 
0 
1 
0 
0 
0 
0
0
 0 
0 
0 
0 
0 
8 
14 
7 5 
3 1 1  1 1  
4 
9 13 
6 
14 13 
9 
14 
6 
9 
6 
3 
14 
1 3  13 2 1 3  
2 
8 
12 
13 
2 4 
6 1 1  13 
4 
3 
14 
14 
8 
3 
12 
11 
8 
8 5 
3 3 
4 
6 
14 
2 12 14 
6 
12 
6 
14 13 14 
To confirm that there are indeed generator matrices for our codes we can 
check directly that their columns are code words using the rational form of 
the syndrome. We shall do this for the last two columns 
0
0
0
0
0
0
0
0
0
0
0
0
1
5
4
1 1 
and 
0 0 0 0 0 0 0 0 0 1 
13 3 
14 8 
14 14. 
For the first word we must check to see if g(z) divides 
1 
5 
4 
1 1  
-- + -- + -- + -- .  
z + 12 
z + 13 
z + 14 
z + 15 
We bring the fractions over a common denominator: 
(z + 12)(z + 13)(z + 14)(z + 15) = z4 + 7z2 + 6z + 5. 

Classical Goppa codes 
313 
The numerator is 
(z3 + 12z2 + z + 10) + 
5(z3 + 13z2 + 6) 
+ 
4(z3 + 
14z2 + 
5z + 2) + 
1 1(z3 + 
15z2 + 
l lz + 
1 1) = 1 1z3 + 
l lz + 
1 1 ,  
which is 1 1g(z). 
For the second word we must check to see if g2(z) divides 
1 
13 
3 
14 
8 
14 
14 
--+ --+--+--+ --+--+--. 
z + 
9 
z + 
10 
z + 
1 1  
z + 12 
z + 
13 
z + 
14 
z + 15 
This time the common denominator is 
(z + 
9)(z + 
10)(z + 
l l)(z + 12)(z + 
13)(z + 
14)(z + 
15) 
= z7 + 
8z6 + 
15z5 + 5z4 + 
13z3 + 12z2 + 
1z + 6, 
and the numerator is 
(z6 + z5 + 6z4 + z3 + 4z2 + 3z + 5) 
+ 
13(z6 + 
2z5 + 2z4 + 8z3 + 
llz2 + 
13z + 8) 
+ 3(z6 + 
3z5 + 
1 1z4 + 
15z3 + 
12z + 
14) 
+ 
14(z6 + 
4z5 + 
13z4 + 
15z3 + 6z2 + 
15z + 
12) 
+ 8(z6 + 5z5 + 4z4 + 3z3 + 3z2 + 2z + 4) 
+ 
14(z6 + 
6z5 + 
5z3 + 9z2 + 
15z + 
1 1) 
+ 
14(z6 + 
7z5 + 9z4 + 
15z3 + 14z2 + 
7) 
= 9z6 + 
9z2 + 
9, 
which is 9g2(z). 
It should be remarked that while the matrices are convenient for calculaƳ 
tion, they obscure the structural relations between code words. For instance, 
it is not immediately apparent from the matrices that GC2 is a subcode of 
GC1 (see Exercises 19.1 and 19.2). 
19.9 Rank and minimum distance 
We can now prove the analogue of Theorem 13.9. We consider first the case 
of a full Goppa code. 
Proposition Let C = GC(P, g) be a full goppa code with IPI = n and deg(G) = t. 

314 
Error-correcting codes and finite fields 
Then GC(P, g) is a linear code with block length n, rank at least n - t and 
minimum distance at least t + 1. 
Proof That C is linear follows directly from the definition. For let u and v 
be two code words with syndromes s(z) and t(z). By definition g(z) divides 
both s(z) and t(z). Now if w = au + bv, then w has syndrome as(z) + bt(z), 
which is also divisible by g(z) (see Exercise 19.4). Thus au + .bv is also a code 
word. 
Using the method of Proposition 19.7 we can construct a t x n check 
matrix H 
for C. Thus the rank of H is at most t, and by the rank and nullity 
theorem, the dimension of C, which is the null space of H, is at least n - t. 
To estimate the minimum distance we use the rational form of the 
syndrome. First recall that since C is linear, its minimum dist'!-nce is the 
minimum weight of a non-zero code word. Let d = (d1, •
.
•
 , dn) .:;6 Q be a 
code word of smallest weight and for convenience assume that its non-zero 
entries are d1, •
•
•
 , dk. Then the syndrome of d is 
n(z) 
u(z) 
D= 1 di fl<= 1 (z -/3;) 
i ,. . 
fl/= 1 (z -/3) 
As d is a code word g(z) must divide n(z). Now 
Thus n(z) .:;6 Q. But the degree of n(z) is at most k - 1. Hence if g(z) divides 
n(z), it follows that k - 1 à t, or k à t + 1. 
• 
Example In our examples GC1 and GC2 we found that the check matrices 
had full rank. Thus these codes have block length 16, and ranks 13 and 
10 respectively. The formula for the minimum distance says that their 
minimum distances are at least 4 and 7. The code words we checked at the 
end of the last example had weights 4 and 7. So the formula for the minimum 
distance is also precise in these cases. 
19.10 Subfield Goppa codes 
It is easy to extend the estimates of Proposition 19.9 to subfield Goppa codes. 
The estimate for the rank changes, as the check matrix has to be remodelled 
into a check matrix with entries in the subfield, but the estimate for minimum 
distance stays the same. 
Example Suppose we wish to consider the binary subcodes of the example 

Classical Goppa codes 
315 
codes in Section 19.4 above. They are defined as the sets of binary words d (or 
which Hd = 
Q, where H is one of the two check matrices: 
• 
Check matrix for GC1: 
[ 
1 
1 
10 
9 
0 
1 
13 
2 
1 
0 
9 
1 5  
11 14 12 13 2 1 0  1 1  10 
7 
6 11 
7 
4 
3 
8 9 12 
1 
1 
15 
5 
6 
14 
3 
6 
7 5 
2 
1 
1 12 13 
4 
• 
Check matrix of GC2: 
1 1  14 10 
12 
6 
7 
4 1 1  10 1 1  12 13 10 
9 
0 
15 1 1  
3 10 
13 12 11 
5 
1 1  10 
6 
7 
8 
10 
o 
1 
4 
12 
9 
·5 
15 14 
6 
1 
1 
· 3 
8 
13 
2 
0 1 
14 
12 
2 
6 
7 
6 13 
4 10 1 1  13 12 
9 
7 
0 14 
3 
2 
5 13 12 
8 
4 
1 
6 
7 
9 15 
0 0 
5 
5 
8 
8 
5 15 15 15 
10 11 
3 
8 
3 
3 
Now consider the first equation given by the first matrix. It is 
1d0 + 
1d1 + 10d2 + 9d3 + l ld4 + 14d5 + 
12d6 + 13d7 + 2d8 + 10d9 
+ l ld10 + 10d1 1  + 7d12 + 6d13 + l ld14 + 4dl s  = 
0. 
Since the entries di are all in B, we can replace the elements of GF(16) by 
their binary column representations. So the equation becomes 
[6l·+m 
d, +m 
d, +[1J 
d, +m 
d. +m 
d, +m 
d, 
+m 
d, + m 
d, + m 
d, + m 
d .. + m 
d . . + m 
d., 
+m 
d., +m d .. +m 
d., f o. 

316 
Error-correcting codes and finite fields 
Using this idea on each row we produce binary check matrices for the binary 
subfield codes. These matrices have four times as many rows as those over 
GF(16). 
Binary check matrix for GC1 I B: 
0 0 1 
1 
1 
1 
1 
1 0 1 
1 
1 0 0 1 0 
0 0 0 0 0 1 
1 
1 0 0 0 0 1 
1 0 1 
0 0 1 0 1 1 0 0 1 1 1 1 1 1 1 0 
1 1 0 1 
1 0 0 1 0 0 1 0 1 0 1 0 
0 0 1 0 0 0 0 1 
1 
1 0 0 1 0 0 1 
0 0 1 0 1 
1 0 0 0 1 0 0 1 
1 
1 
1 
0 0 0 1 
1 0 1 0 0 0 0 0 1 
0 1 
1 
0 1 
1 0 1 0 1 0 1 0 1 
1 
1 
1 0 0 
0 0 1 
1 
1 0 0 0 0 0 0 0 1 
1 0 1 ·  
0 0 0 1 
1 0 1 
1 
1 0 0 0 1 
1 
1 0 
0 0 0 1 
1 
1 
1 
1 0 1 0 0 0 0 0 0 
1 0 1 
1 0 1 0 1 
1 
0 
1 
0 
0 0 -
Binary check matrix of GC2 1 B: 
0 0 1 1 
1 0 0 0 0 1 
1 
1 
1 
1 
1 
1 
0 0 0 1 0 0 1 
1 
1 0 0 0 1 
1 
0 0 
0 0 1 
1 
1 
1 
1 
1 0 1 
1 
1 0 0 1 0 
1 
1 
1 0 0 0 0 1 0 1 0 1 0 1 0 
1 
0 0 1 
1 0 1 1 1 
1 0 1 
1 0 0 1 
1 
0 0 1 0 0 0 1 
1 0 1 0 0 
1 
1 0 0 
0 0 1 
1 
1 
1 0 0 1 0 1 
1 
1 
1 0 1 
0 1 1 
1 
1 0 1 0 1 
1 
1 0 0 1 0 0 
0 0 0 0 1 
1 0 1 
1 0 0 0 0 1 
1 0 
0 0 1 
1 
1 0 1 
1 
1 
1 0 0 0 0 1 0 
0 0 1 0 0 0 0 1 
1 
1 0 0 1 0 0 1 
0 1 
1 0 0 1 
1 
1 0 0 1 
1 
1 0 1 0 
0 0 1 
1 0 0 0 0 1 0 1 
1 
1 
1 
1 0 
0 0 1 
1 0 1 
1 
1 
1 
1 0 0 1 
1 0 1 
0 0 1 0 1 
1 
1 
1 0 0 1 
1 0 0 0 1 
0 1 0 0 0 0 1 0 1 0 0 1 
1 0 1 
1 
0 0 1 0 0 0 1 
1 
1 0 0 0 0 0 
1 
1 
0 0 
1 0 0 
1 
1 
1 
0 
1 0 0 
1 
1 
0 
1 
0 0 1 
1 
1 0 0 0 0 0 0 0 1 
1 0 1 
1 0 0 
1 0 1 
1 0 0 0 1 
1 
0 
1 
1 
1 
0 0 0 0 
1 
1 0 1 
1 
1 
1 
1 0 
1 
0 0 
0 0 1 
1 0 0 1 
1 
1 
1 0 0 0 0 0 0 
0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 
1 
0 0 1 
1 0 0 1 
1 
1 
1 0 1 
1 0 1 
1 

Classical Goppa codes 
317 
Proposition Let C = GC(P, g) IF be a subfield Goppa code with IPI = n and 
g E E[z] of degree t. Further, let the dimension of E, considered as a vector 
space over F be m. Then C is a linear code with block length n, dimension at 
least n - mt, and minimum distance at least t + 1. 
Proof The block length is obviously the same as for the full Goppa code 
defined over E, and the minimum distance cannot be lower than for that 
code. So we need only establish the bound for the dimension. 
By Proposition 19.7 we have a check matrix H 
with t rows, so that C 
consists of the words in d in Fn such that Hd = Q. 
This check matrix has 
entries in E rather than F. As E has dimension m as a vector space over F 
its elements can be represented by column vectors of length m over F. In H 
we can replace each entry hii by its column (hii1, •
•
•
 , 
hiim)T. Now, the fact that 
Hd = Q 
is equivalent to the conditions 
n 
L hijdj = o, 
j= 1 
for i = 1, . . .  , t. But, replacing hii by its coordinate column, these conditions 
are equivalent to 
n L hijkdj = o, 
j= l 
for i = 1, .. . , t and k = 1, . . .  , m. Thus the mt x n matrix H' 
with entries in 
F obtained from H 
replacing the entries hii by their coordinate columns is a 
check matrix for C. Thus the rank of H 
is at most mt and hence the rank 
of C is at least n - mt by the rank and nullity theorem. 
• 
EXTRAS 
19.11 Special properties of binary Goppa codes 
It is by no means obvious that the two binary matrices given in Example 
19.10 define the same code, but that is indeed the case. This special property 
of binary Goppa codes analogous to the fact that the two check matrices 
Hk,r and J-lc,r define the same binary BCH code. It follows that some binary 
Goppa codes have much better parameters than Proposition 19.10 indicates, 
because we can use g(z) to estimate the rank and g2(z) to estimate the 
minimum distance. 
Proposition Let E be a finite field of characteristic 2, and let g(z) be a 
square1ree polynomial in E[z], that is g(z) is not divisible by the square of any 
non-constant polynomial in E[z]. Then for any valid set P of elements of E the 
binary subfield codes GC(P, g) I B and GC(P, g2) I B are identical. 

318 
Error-correcting codes and finite fields 
Remark The full codes are usually different, as shown by our example. It 
is only for the subcodes of words with 0-1 entries that equality is claimed. 
Proof First notice that the condition 
implies 
n 
d. 
s(z) = L -1- = 0 (mod g2(z)) 
j=oz-Pi 
(1) 
n 
d. 
s(z) = L 
-1 - = 0 (mod g(z)) . 
(2) 
j=O Z - Pj 
Hence GC(P, g2) f; GC(P, g). 
That holds over any field; now we shall show that provided di E B for all j, 
condition (2) implies condition (1). For convenience we renumber the ele­
ments of P so that di 
= 1 for j = 1, . . .  , k and di = 0 for j > k. Now we can 
rewrite the left-hand side of (2) as 
k 
1 
n(z) 
s(z) = L 
--=
- .  
i = O  z - Pi 
u(z) 
We multiply out and obtain 
n(z) 
u(z) 
flY= l (z -P) 
By the product rule of differentiation n(z) is just the derivative of u(z). From 
the formula for derivatives and the fact that E has characteristic 2 it follows 
that the coefficients of odd powers of z in n(z) are all 0. So let k' be chosen 
so that k - 2 ] 2k'  k - 1. Then there exists elements ccj such that 
k' 
n(z) = L ccjz2i. 
j=O 
In a finite field of characteristic 2, every element is a square (see Exercise 
10.9). So we can find cci so that IXJ = ccj and 
k' 
( k' )2 
n(z) = L ccJz2i = L ccizi . 
j=O 
j=O 
Thus n(z) is a perfect square, say n(z) = m(z)2• The binary word d we started 
with is a code wora of GC(P, g). Therefore g(z) divides n(z) in E[z]. By 
assumption g(z) is square free, so g is a product of distinct irreducible 
polynomials f(z) in E[x], each dividing g(z) only once. Each f(z) divides 
n(z), so being irreducible, it must divide m(z). Furthermore, all the irreducible 
factors of g(z) are distinct. Thus, by Proposition 8.8, their product g(z) divides 
m(z). That implies that g(z)2 divides n(z), and thus that d is a code word of 
GC(P, g2). 
• 

Classical Goppa codes 
319 
19.12 Summary 
In this chapter we have introduced the classical Goppa codes and established 
their basic properties. We showed that the syndrome can be treated as a 
polynomial or a rational function, using the polynomial form to construct 
check matrices for the codes and the rational form to estimate their 
parameters. In the next chapter we shall show how to adapt the Euclidean 
error processor to these codes, and prove that there are good sequences of 
Goppa codes in the sense of Chapter 18. 
19.13 Exercises 
19.1 Produce check matrices for the example codes GC1 and GC2 of Section 
19.8 that exhibit GC2 as a subcode of GC1• 
19.2 Produce generator matrices for GC1 and GC2 that exhibit GC2 as a 
subcode of GC1• 
19.3 Check that all the columns of the generator matrices for GC1 and 
· GC2 given in Section 19.8, satisfy the defining conditions in either 
form. Prove that these matrices are indeed generator matrices for the 
codes. 
19.4 Show that if s(z) = u(z)/n(z) is a rational function in cancelled form 
and g(z) divides s(z), then g(z) and n(z) are relatively prime. 
Show that if t(z) is another rational form, and g(z) divides both s(z) 
and t(z), then g(z) divides as(z) + bt(z) for any constants a and b. 
19.5 Construct a Goppa code of maximum length over GF(16) with Goppa 
polynomial x4 + x3 + 1. Estimate its rank and minimum distance. 
19.6 Let GC(P, g) be a 
classical Goppa code with P = {/31, •
•
• , Pn} and 
g(z) = L: gF1 of degree t. Define matrices A, B and C as follows: A is 
a t x t matrix with entries a;1 = 0 if i 
< j, and a;1 = gt-i+ 1 if i 
j; 
B 
is a t x n matrix with entries bii = Pt 1; C is a diagonal n x n matrix 
with entries c11 = 1/g(/3) 
(and cu = 0 if i =F j). Show that ABC = -H 
where H is the check matrix for GC(P, g) constructed in Proposition 
19.7. 
19.7 With the notation of Exercise 19.6, let K be the t x n matrix with 
entries kij = Pt 1 /g({Jj). Show that K is also a check matrix for 
GC(P, g). 

20 
Classical Goppa codes: 
error processing 
In this chapter we shall describe how the error-processing algorithm of 
Part 3 can be adapted to classical Ooppa codes. We shall define error locator 
and evaluator polynomials in the same way as we did for BCH codes, though 
the definitions have to be slightly modified to take account of tl;le fact that 
the places corresponding to the entries of a code word need not be 
consecutive powers of a primitive element, and indeed can include the zero 
element. At the end of the chapter we shall prove that classical Goppa codes 
are a theoretically good family. 
20.1 Error locator and evaluator 
Suppose we are given a Goppa code GC(P, g) where P = {P1, •
•
•
 , Pn} 
consists of elements of a field F. From the last chapter we know that the 
minimum distance of GC(P, g) is at least d = 
deg(G) + 1. We shall call d 
the designed distance of GC(P, g) and construct an error processor to correct 
t errors, where t is the largest integer with 2t + l 
 
d. As far as error 
correction is concerned, we may assume that we are dealing with the full 
Goppa code, because if a word from a subfield code is transmitted and 
fewer than t errors occurred, then a full code error processor wiD correctly 
return the subfield code word. 
Dejilition Suppose a code word c = 
(c1, •
•
•
 , c.) of GC(P, g) is transmitted 
and in transmission the error word e = (e1, •
•
.
 , e.) is added to c to produce 
the received word d = 
(d1, .
•
.
 , d,.). Then the error locations are the values 
i 
= 1, . .. 
, n, for which e; # 0. We denote the set of error locations by M. 
The error locator polynomial is 
l(z) = Il 
(z -P), 
jeM 
and the error evaluator polynomial is 
w(z) = L ei Il 
(z -P;). 
jeM 
ieM 
i <F j  

Classical Gappa codes: error processing 
321 
Notice that we have replaced (1 - riz) in the products by (z -pi). 
That 
allows Pi 
= 0, but it changes the formulae slightly. It is also possible to define 
an error co-evaluator (see Exercise 20.2). 
Example 
Consider the code GC2 of Chapter 19, with Goppa polynomial 
z6 + z2 + 1. In Section 19.8 we checked that 
c = 0 0 0 0 0 0 0 0 0 
1 
13 3 
14 8 
14 
14 
is a code word. Suppose that 
d = 1 
2 4 0 0 0 0 0 0 
1 
13 
3 
14 8 
14 
14 
is received, then the error word is 
e = 1 
2 4 0 0 0 0 0 0 0 0 0 0 . 0 · 0 0, 
and the error locations are o; 
1 and 2 (if we take the natural order for the 
values pi). 
Then the error locator and evaluator are given by the formulae 
and 
l(z) = z(z - 1)(z - 2) = z3 + 3z2 + 2z 
w(z) = (z - l)(z - 2) + 2z(z - 2) + 4z(z - 1) 
= 7z2 + 
3z + 2. 
Proposition 
The degrees of the error locator and evaluator If s errors 
occurred in the received word d, then the degrees of the error locator and 
evaluator satisfy: 
deg{l(z)) = s; 
deg(w(z)) < s. 
Furthermore, the highest coefficient of l(z) is 1. 
20.2 The fundamental congruence 
• 
With our new definition of the syndrome the fundamental equation holds 
automatically, albeit as a congruence modulo g(z). 
Theorem The fundamental congruence for Goppa codes 
The syndrome, 
error locator and evaluator polynomials of a word with respect to GC(P, g) 
are related by the congruence 
l(z)s(z) = w(z) (mod g(z)). 

322 
Error-correcting codes and finite fields 
Proof Let the code word c be transmitted and the word for which the 
syndrome s(z) is calculated be d = c + e. Then denote the syndromes of these 
three words by sd(z), s.(z), and s.(z) respectively. We have 
n 
d ·  
n 
C ·  
n 
e .  
siz) = L: -1-
= L: -1-
. 
+ L: -
1- .  
j= l z - f3j 
j= t Z - {3j 
j= t Z - {3j 
The first sum on the right-hand side is congruent to 0 modulo G, by 
definition. Thus 
n 
d . 
n 
e .  
sd(z) = L: -1- = L -1-
mod g(z)). 
j= t Z - {3j 
j= 1 Z - {3j 
The sum on the right can be expanded and the congruence becomes 
which is just 
L J= l ei Il7= t (z -/3) 
sd(z) = 
i "" · 
(mod g(z)), 
Ili= t (z - /3) 
w(z) 
sd(z) = - (mod g(z)). 
/(z) 
Multiplying the congruence by /(z) gives the desired result. 
• 
Example 
In practice the formula is not used to calculate the syndrome. 
Instead we use the check matrix obtained in Proposition 19.7. That produces 
the polynomial syndrome sp(z). 
1 
1 
1 1  
14 
10 
2 
6 
7 
4 1 1  10 
1 1  
12 
13 
10 
9 
0 
1 
1 5  
1 1  
3 
10 
1 3  
1 2  1 1  
5 
1 1  
10 
6 
7 
8 
10 
0 1 
7 
4 
12 
9 
5 
1 5  
14 
6 
3 
8 
13 
2 
0 
14 
12 
2 
6 
7 
6 
13 
4 
10 
1 1  13 
12 
9 
7 
0 14 
3 
2 
5 
13 12 
8 
4 
6 
7 
9 
15 
0 0 
5 
5 
8 
8 
5 15 15 15 10 
1 1 
3 
8 
3 
3 
Multiplying d (or e) by this matrix we obtain the syndrome 
sp(z) 
= 4z5 + 1 2z4 + 7z3 + 8z2 + 1 1z + 1 3 .  
As we know the error locator and evaluator we can check the validity of 

Classical Goppa codes: error processing 
the fundamental congruence: 
l(z)sp(z) = (z3 + 3z2 + 2z)(4z5 + 12z4 + 7z3 + 8z2 + l lz + 13) 
= 4z8 + 2z6 + 4z4 + 1z2 + 3z 
= (4z2 + 2)(z6 + z2 + 1) + 7z2 + 3z + 2 
= (4z2 + 2)g(z) + w(z). 
20.3 Uniqueness of /(z) and w(z) 
323 
From their formulae we can read off the fact that 1(z) and w(z) have highest 
common factor 1 (the proof is identical to Proposition 16.6), and just as in 
Chapter 16 we can use this fact to establish the uniqueness of l(z) and w(z). 
Proposition 
Uniqueness of l(z) and w(z). Suppose that in transmitting a 
code word of GC(P, g) at most t errors occurred, where 2t 
 
deg(g(z)) and let 
l(z) and w(z) be the error locator and evaluator of that word. Then the following 
statements hold. 
· 
(a) The highest common factor of l(z) and w(z) is 1, 
(l(z), w(z)) = 1 .  
(b) If l0(z), W0(z) and U0(z) satisfy 
and 
ZO(z)sp(z) + U0(z)g(z) = w(z) 
degOO(z)) 
 
t 
deg(w0(z)) < t, 
where sp(z) is the polynomial form of the syndrome, then there exists a 
polynomial k(z) such that l0(z) = k(z)l(z) and W0(Z) = k(z)w(z). 
(c) If, furthermore, ZO(z) and U0(z) have highest common factor 1, then the 
polynomial k(z) is a non-zero constant. 
(d) If ZO(z) also has highest coefficient 1, then k(z) = 1. So l0(z) = l(z), 
U0(z) = u(z) and W0(Z) = w(z). 
Proof The argument for (a) is unchanged from that for BCH codes, l(z) 
splits into linear factors and none of these divides w(z). 
(b) For some u(z) we have 
l(z)sp(z) + u(z)g(z) = 'w(z) 
(1) 
and 
(2) 

324 
Error-correcting codes and finite fields 
Eliminate s(z) by multiplying (1) by /0(Z) and (2) by /(z) and subtracting. 
This gives 
W(z)u(z) - l(z)u0(z))g(z) = [0(z)w(z) - l(z)w0(Z). 
By assumption s  t errors occurred and thus each term on the right has 
degree less than st and hence less than 2t. On the other hand, each term on 
the left has a factor g(z) which has degree Á 2t. It follows that the only way 
this equation can be satisfied is if 
nz)w(z) - l(z)w0(Z) = 0. 
(3) 
This implies that 
(4) 
We know from part (a) that l(z) and w(z) have highest common factor 1 .  
By Euclid's algorithm this implies that there are polynomials f(z)" and h(z) 
such that 
f(z)l(z) + h(z)w(z) = 1 .  
We multiply this equation by fC(z): 
zo(z) = f(z)l(zW(z) + h(zW(z)w(z). 
Now we use (3) to substitute for zo(z)w(z): 
lo(z) = f(z)l(iW(z) + h(z)l(z)W0(Z) 
= (f(zW(z) + h(z)w0(z)l(z) . 
So of k(z) = f(zW(z) + g(z)u0(z), then /0(z) = k(z)l(z). Substituting /0(z) = 
k(z)l(z) in (3) and (4) 
k(z)l(z)w(z) = l(z)wo(z). 
As l(z) #= 0 it follows that W0(z) = k(z)w(z) and thus also U0(z) = k(z)u(z). 
(c) From part (b), k(z) divides both nz) and u0(z). If they have highest 
common factor W(z), U0(z)) = 1, then k(z) divides 1. As the only polynomials 
dividing 1 are the non-zero constants, the statement follows. 
(d) From part (c) l(z) and l0(z) are non-zero and differ by a constant 
factor K = k(z). By its definition l(z) has highest coefficient 1. Thus if nz) 
also has highest coefficient i, then K = 1 and hence l0(z) = l(z) and 
W0(Z) = w(z). 
• 
20.4 An error processor for GC(P, g) 
Algorithm We assume that no more than t symbol errors have occurred. 
Example calculations for the received word d as above are interspersed with 
the steps of the algorithm. 

Classical Goppa codes: error processing 
325 
Step 1. 
Using the check matrix derived in Proposition 19.7, calculate the 
syndrome polynomial sp(z). If sp(z) = 0, there are no errors: STOP. 
Example 
This has already been done: 
sp(z) = 4z5 + 12z4 
+ 7z3 
+ 8z2 + l lz + 13. 
Step 2. 
Apply Euclid's algorithm to a(z) = g(z) and b(z) = sp(z). Finish at 
the first stage where r;{z) 
has degree less than t. If r1(z) 
= .Q, 
there are more 
than t errors: STOP. 
Example 
We include the V column for check purposes, but do not show 
the auxiliary rows used in the calculation. 
Q 
6 
10 
5 
3 
5 
3 
Step 3. 
Example 
R 
u 
0 
0 
0 .  1 
0 
1 
1 
4 
12 
7 
8 
1 1  13 
0 
14 
6 
1 5 
4 
14 
1 
12 13 
3 
6 
5 
3 
14 
6 
4 
8 
0 
4 
Put l0(z) = viz). Find the roots of l0(z): }'1, •
•
• , Ys· 
ZO(z) = 2z3 + 6z2 + 4z = 2l(z) = 2z(z - l)(z - 2) 
v 
0 
1 
6 
10 
7 
3 
6 
2 
6 
4 
0 
In practice calculate the roots by any convenient search method such as 
Horner's scheme. 
Step 4. 
For each root }';, i = 1, . . .  , s, if }'; = pk, then an error occurred at 
the place k. 
Example 
In our case }'; = /3;- 1  for i = 1, 2, 3. The errors occurred at 
locations 0, l, 2. 
Step 5. 
Put W0(z) = r1(z). Calculate the error values 
e; = W0(}';)/l0'(y;). 
Example 
l0'(z) = 2 0 4. The values for the three zeros are 
0 
1 
2 
W0: 
4 
12 
2 
1°': 
4 
6 
12 
e: 
1 
2 
4 

326 
Error-correcting codes and finite fields 
Thus 
e = l 2 4 0 0 0 0 0 0 0 0 0 0 0 0 0, 
which enables us to recover c. In practice the derivative can be calculated 
using Horner's scheme, or by any other convenient means. 
20.5 Termination of the algorithm 
Proposition Assume that 0 < s 
 
t errors occurred. Then Step 2 of the 
algorithm will end with a non-zero rJ{z), such that deg(riz)) < t and 
deg(ri_ 1(z)) ८ t. 
Proof From the fundamental equation, the highest common factor of g(z) 
and sp(z), (g(z), sp(z)) divides w(z). So it satisfies 
deg(g(z), sp(z)) 
 
deg(w(z)) < s 
 
t. 
On the other hand the degree of g(z) is :;. 2t > t. Since Euclid's algorithm 
terminates with r.(z) = (g(z), sp(z)), there must be a j such that ri_ 1(z) has 
degree at least t but rJ{z) has degree less than t. 
• 
20.6 Correctnes of the algorithm 
Theorem 
Assume that the weight s of the error word e satisfies 2s 
 
deg(g(z)). 
Then the following statements hold. 
(a) 
The polynomials l0(z), W0(z) determined by the algorithm are multiples 
of the error locator and evaluator l(z) and w(z) by a non-zero constant K. 
(b) 
The algorithm calculates the error values correctly. 
Proof (a) When the algorithm terminates let U0(z) be the entry in the 
U column. Then just as in Theorem 16.8 the properties of Euclid's algorithm 
imply that 
U0(z)g(z) + lo(z)s(z) = W0(Z), 
and that U0(z) and lo(z) 
have highest common factor l. Part (a) now 
follows from Proposition 20.3. 
(b) The formulae for l'(z) and w(z) are both sums in which all terms 
except one contain a factor (z - yJ = (z - pk). Evaluating the remaining 
terms we get 
w<Pk) = ek 0 <Pk -P). 
jeM\k 

Classical Goppa codes: error processing 
327 
and 
l'(Pk) = TI <Pk -P). 
jeM \k 
Thus 
As l0(z) and W0(Z) differ from l(z) and w(z) only by multiplication by the 
same constant it makes no difference if we use them instead. 
• 
The most striking thing about this implementation of Goppa codes is its 
similarity to the implementation of Reed-Solomon codes. The additional 
theoretical ballast only appears in two places. Firstly, a check matrix must 
be calculated and used to find the syndromes and secondly the Goppa 
polynomial must be inserted into Euclid's algorithm in place of z2'. On the 
other hand, the benefit to be obtained is potentially very great, because, as 
we shall show in the concluding part of this chapter, there exist long Goppa 
codes that are good, in the sense that their rates and relative minimum 
distances can simultaneously be bounded away from 0. However, one seldom 
gets something for nothing, and the implementation described in these two 
chapters cannot make full use of the goodness of the codes. 
EXTRAS 
20.7 Goodness of classical Goppa codes 
We shall show that for each b < (q - 1)/q there exists a sequence of classical 
Goppa codes defined over GF(q) such that their block lengths tend to co ,  
their relative minimum distance tends to b, and their rate tends to 1 - Hq(b). 
However, this fact holds only if we use the true minimum distance to evaluate 
b. For the designed minimum distance it is false and you should note that 
our error processor corrects only t errors, where 2t + 1 is the designed 
distance of the code. The only benefit we obtain if the true minimum distance 
is greater than the designed minimum distance is a greater ability to detect 
errors beyond those that are decoded. That is useful, but not as good as the 
capability of correcting these errors also. 
First we shall adapt the proof given in Proposition 18. 1 l, which states that 
(with respect to their design parameters) BCH codes are bad, to Goppa 
codes. 
Proposition 
Let GCn = 
GC(Pn, gn) be a sequence ofGoppa codes over afield 
F of order q, such that the block length n tends to oo, and the designed relative 

328 
Error-correcting codes and finite fields 
minimum distance is greater than e for some e > 0. Then the designed rate of 
the codes tends to 0. 
Proof The designed minimum distance of GC,. is deg(g,.(z)) + 1, so the 
designed relative minimum distance is (deg(g,.(z)) + 1 )/n. Thus, if this is to 
remain above e, we must have deg(g,.) > en - 1. On the other hand, to obtain 
a block length of n we must use a field E of size qm ;;: n. And in this case 
the designed dimension of the code is n - m deg(g,.(z}). As m must be at 
least log9(n) it follows that the designed minimum distance is at most 
n - en loǚ(n). But when n is sufficiently large log9(n) > e, and this expression 
becomes negative. In other words, if the designed rank stays above 0, 
then it is impossible for the designed relative minimum distance to remain 
above e for large n. 
·· 
• 
20.8 Measures 
of goodnes 
Clearly, the designed values must become poor estimates of the true 
dimension and true minimum distance of a Goppa code as the block length 
becomes large. In fact, in contrast to BCH codes, there are so many Goppa 
codes that many must have true minimum distance far above the designed 
value. We shall quantify how big 'far above' is and use that estimate to show 
that Goppa codes form a good family. The reason there are so many Goppa 
codes is that there are very many irreducible polynomials, as we proved in 
the Extras of Chapter 1 2. It will be useful to start by recalling some definitions 
and facts about q-ary balls from Chapter 1 8. 
Definition 18.2 Let F be a field of order q. The q-ary ball Dq(u, r) with 
centre u E F" and radius r, consists of all those words with entries in F" 
at distance 
 
r from u. The number of such words is denoted by V(q, n, r). 
Definition 
18.8 The q-ary entropy function Hq(J) is defined as 
H9(J) 
= J loǛ(q - 1) - J loǚ(J) - (1 -b) 
loǛ(l -b). 
Le111Ul18.9(b) The limit as n tends to oo and rjn tends to b 
of( V(q, n, r))/n 
is Hq(b). 
Here finally is a restatement of the Gilbert-Varshamov bound that is our 
measure of goodness. 
Tleorem 18.10 For all b 
 
(q - 1)/q there exists a sequence C,. of linear 
block codes over GF(q) with block length C,. = n, the relative minimum 
distance of C,. greater than b 
- 1/n and rate tending to 1 -Hq(b). 

Classical Goppa codes: error processing 
329 
We shall not construct our good sequence of Goppa codes explicitly and, 
in particular, not try and construct codes for all block lengths. Instead we 
shall only choose block lengths that are precise powers of q. 
Proposition 
Let F be a field of order q and choose m and d < q"'. Let t be 
chosen so that 
Then there exists a classical Goppa code over F of block length qm, minimum 
distance at least d and rank at least qm - mt. 
Proof Let E be a field of order n = qm containing F. We construct the code 
GC(P, g) as a subfield code using the fields F and E. We take as our set of 
points P the whole of the field E and as our Goppa polynomial g(z) we select 
one of the irreducible polynomials of degree t. From Theorem 12.1 1 we know 
that there are at least (q"" - q.J qm')/t of these. How many of our possible 
choices lead to codes with minimum distance less than d? 
If the subfield code defined over F of GC(P, g) has minimum distance less 
than d, then it contains a code word c #; 0, of weight < d. If c = (c1, •
•
•
 , c.), 
then its syndrome is 
f. Þ = n(z) , 
i= l z - P; 
u(z) 
and c is in the code if and only if g(z) divides n(z). As the weight of c is less 
than d, the degree of n(z) is at most d - 2. Hence it can have at most (d - 2)/t 
distinct irreducible factors of degree t. In all there are V(q, n, d - 1) - 1 
possible words of weight less than d; so the total number of excluded 
irreducible polynomials is certainly less than d ·  V(q, n, d - 1)/t. 
Now by our hypothesis this is less than (qm' - q.Jq"")/t. Thus there must 
be at least one irreducible polynomial of degree t over E that has not been 
excluded. If we take that polynomial, then the code will have true minimum 
distance at least d and by Proposition 19.10 its dimension is at least 
n - mt = qm - mt. 
• 
20.9 Goppa codes as a good famlly 
We now come to the theorem that confirms that Goppa codes form a good 
family. 
Theorem 
Let F be ti field of order q and let b < (q - 1)/q. Then there 
exists a sequence of classical Goppa codes GC(P, g) defined over F with block 

330 
Error-correcting codes and finite fields 
lengths qm, relative minimum distance tending to the limit ¬ and rate tending 
to .tl limit r ɴ 1 - Hq(¬). 
Proof For each m ɴ 1 put n == qm. Choose d minimal so that djn ɴ ¬ and 
t minimal so that 
d ·  V(q, ї. d - 1) < n' - n.Jn'. 
Then by Proposition 20.8, there exists GC(P, g) with IPI = 
n, g(z) irreducible 
of degree t and true minimum distance at least d . .  By the minimality of our 
choices, 
n'- 1  _ n(r + 1 l/2 
n' _ n(r + 3)/2 
d 
< V(q, n, d - 1) <  
· .  
d 
To make our estimates simpler we reduce the left-hand term of this inequality 
(on the safe assumption that for large n, t will be at least 3) and we also 
increase the right-hand term. 
n' - 1  
n' - 1  _ n(r + 1 )/2 
n' _ n(r + 3)/2 
n' 
-
 
< V(q, n, d - 1) < 
· 
< - .  
2d 
d 
d 
d 
Taking logarithms to the base q and, using the fact that n = 
qm, this yields 
m(t - 1) - log(2d) < log(V(q, n, d - 1) < mt - log(d) 
Now we divide by n and take the limit as n tends to oo .  
lim((t - 1)m/n) 
 
Hq(¬') 
 
lim((tm/n)), 
where ¬, is the limit of (d - 1)/n. This differs from the limit ¬ of dfn by the 
limit of I/n which is 0. Thus ¬' = ¬- Also m = 
log(n) and the two outer 
expressions differ by the limit of mjn which is also 0. So these limits are the 
same. Notice that, as t also increases with n, it does not follow that the limit 
of mtfn is 0. Indeed the inequality gives 
lim(mt/n) = Hq(¬). 
Now the rates of our codes are at least (n - tm)/n = 
I - tmfn. So in the limit 
they have rate at least 
I - lim(tm/n) = 
1 - Hq(¬). 
• 
20.10 Summary 
In the first part ofthis chapter we adapted the Reed-Solomon error processor 
of Chapter 17 
to classical Goppa codes. This required very little change. For 

Classical Goppa codes: error processing 
331 
this type of error processing, these codes are no harder to implement than 
Reed-Solomon codes. However, Goppa codes are not, in general, cyclic, and 
so they are not amenable to partial error-processing and error-trapping 
techniques. 
In the second half we showed that in contrast to the BCH and Reed­
Solomon family, Goppa codes contain sequences meeting the asymptotic 
Gilbert-Varshamov bound. It would be pleasant but misleading to end the 
chapter on this high note. Classical Goppa codes do indeed form a good 
family, but this result is not as good in practice as in theory. 
Firstly, the theorem proves only that there must be good classical Goppa 
codes. It does not give any practical means of finding them, or of verifying 
that the code one has found really is a good one. 
Secondly, even if we have one of the good codes, our error corrector cannot 
exploit its true minimum distance. So all we gain from the large true 
minimum distance is that the processor detects many additional errors 
beyond those it corrects. That is useful for high reliability, but it does not 
extend the correction capability. 
20.11 Exercises 
20. 1 
Using the code GC2 correct the following words: 
1 
2 
4 
8 
9 
15 6 
1 
9 
10 
1 
2 
5 
8 
9 
15 6 
1 
8 
1 1  
8 
1 1  1 3  9 
1 5  13 
11 
3 
13 4 
5 
13. 
20.2 An error co-evaluator for classical Goppa codes. Show that sp(z) = 
(w(z) + u(z))/l(z) where 
u(z) = -g(z) I (ejfg(pj)) n (z - P;). 
jeM 
i ;Oj 
20.3 
Prove the equivalent of Proposition 20.3 for the rational form of the 
syndrome. 
20.4 Let C be the Goppa code you constructed in Exercise 19.5. Use 
the error-correction algorithm of this chapter to correct the word 
5 0  . . .  0 1 5. 
20.5 Discuss the failure modes of the error-processing algorithm of Section 
20.4. 
20.6 Show that it is impossible for the classical Goppa code GC(P, g) to 
be cyclic for all choices of the set of values P. 
The following questions deal with general BCH codes. In Exercise 20.7 
we give the original definition, from which we develop their theory in analogy 
to Part 3. In Exercises 20. 1 1  and 20. 12 we establish that these are essentially 
the same as the codes defined in Section 19.2. 

332 
Error-correcting codes and finite fields 
20.7 
Let F be a finite field, E a finite extension field of F, and let IX e E have 
order n. The general BCH code GBCH(n, d, r) of block length n and 
designed minimum distance d is constructed as the polynomial code 
with generator polynomial equal to the least common multiple of the 
minimal polynomials in F[x] of IX', IX' +\ 
•
•
•
 , IXr+d- 1• Show that this 
code is cyclic. Show that if IX has order n = lEI- 1 ( = 'primitive'), and 
r = 1 ( = 'narrow-sense'), then for F = 
GF(2) the code is the standard 
BCH code (as constructed in Part 3) 
and for F = 
E the code is a 
Reed-Solomon code. 
20.8 
Show that the code GBCH(n, d, r) has minimum distance at least d. 
Estimate the rank of the code assuming that the minimal polynomials 
are all distinct and have maximal possible 'degree. 
20.9 
Define the syndromes S" .. . , S,+d- 1 of the word v for GBCH(n, d, r) 
by interpreting the word as a polynomial and evaluating . it at 
a.', . .. 
, a.'+d- 1• Define the syndrome polynomial of u as 
s, + s,+ 1z + . . . + s,+d- 1,- 1 ' 
and its error locator as n (1 - IXiZ), Where the index runs OVer 
the powers of a corresponding to the error positions. Define also 
analogous error evaluator and co-evaluator polynomials. Show that 
these polynomials satisfy the fundamental equation of BCH codes. 
20.10 Adapt the BCH error processor of Part 3 to GBCH(n, d, r). 
20.1 1 Verify that the codes GBCH(n, d, 1) are precisely the subfield Goppa 
code with Goppa polynomial zd and P equal to the set of consecutive 
powers IX of E. 
20.12 Show that GBCH(n, d, 1) can be obtained from GBCH(n, d, r) by 
multiplying the entries of the code words by powers of ex, the 
ith entry being multiplied by IX(r- 1li, 

21 
Introduction to algebraic curves 
The key extension in Goppa's definition of his geometric codes is to replace 
the set of polynomials over a finite field by a more general construction. 
Naturally, this generalization requires some effort. Goppa uses the language 
of algebraic curves to introduce the codes, but it is also possible to construct 
them in a purely algebraic manner. That was my original intention, but the 
algebraic approach turns out to be far more intricate and abstract than the 
geometric one. So I have decided to follow Goppa and give a geometric 
description of the codes. 
To keep the introduction simple, we shall restrict our attention to algebraic 
curves in the plane. Higher dimensional curves can be dealt with in a similar 
manner, but require more of the apparatus of algebraic geometry. In this 
chapter we shall assemble the tools needed to calculate with plane curves. 
The next chapter deals with rational functions on curves, defining their zeros 
and poles. In Chapter 23, I shall describe the general theory, mainly by means 
of examples. Then, in Chapter 24 Goppa's new codes are constructed, their 
parameters are calculated, and finally, in Chapter 25, an error-processing 
algorithm for them is produced. 
21.1 Defining a curve 
The natural way to define a plane algebraic curve is to define it as the set 
of points for which some polynomial f(x, y) in two indeterminates is zero. 
We denote the set of polynomials in two indeterminates by F[x, y] and make 
a tentative definition as follows. 
Letf(x, y) be a polynomial in F [x, y]. The set C of points (1X, p) 
for which 
/(IX, P) = 0 forms a curve. We say C is the curve f(x, y) = 0 and write C: 
f(x, y) = 0. 
This definition needs some polishing. So let us consider some examples. 
Example 
Over the real numbers R, the polynomial x2 + y2 
- 1 cor­
responds to the circle x2 + y2 = 1, y - x2 corresponds to the parabola 
y = x2, and the polynomial y corresponds to the straight line y = 0, which 
is just the x-axis. These are all bona fide curves. 

334 
Error-correcting codes and finite fields 
The product (x2 + y2 - 1)(y - x2) defines a circle and a parabola. What 
about the polynomials x2 + y2 + 1, or x2 + l? They have no real points at 
all. 
21.2 Irreducible polynomials in F[x, y] 
To avoid the case when we get two curves, we must ensure that the defining 
polynomial f(x, y) does not factorize. Polynomials in two indeterminates 
share most of the properties of ordinary polynomials f(x), but there is an 
important exception. With respect to addition and multiplication, polyno­
mials in two indeterminates behave as well as onؽ could expect. They form 
an integral domain. We can also define the degree of a polynomial f(x, y) 
by taking the degree of xmy" to be m + n. As with ordinary polynomials, the 
degree of a product of two polynomials f(x, y) and g(x, y) is the sum of their 
degrees and the degree of their sum is at most equal to the larger of their 
individual degrees: 
deg(f(x, y)g(x, y)) = 
deg(f(x, y)) + deg(g(x, y), 
deg(f(x, y) + g(x, y)) o max{deg(f(x, y)), deg(g(x, y)}. 
That is proved in Appendix PF of Part 2. However, it is no longer possible 
to divide f(x, y) by g(x, y) in such a way that the degree of the remainder 
is always smaller than the dؾgree of the divisor. 
Example 
Consider the polynomials x, y and x - y. If we divide x by x - y 
we get x = l · (x - y) + y, but dividing y by x - y gives y = 
- 1  · (x - y) + x. 
It is not possible to write x as q(x, y)(x - y) + r where the degree ofr is o 0. 
Since we do not have division with remainder, it is not possible to perform 
Euclid's algorithm. Nevertheless, we can still examine the factorization of 
polynomials. To this end we make the usual definition of irreducibility. 
Definition 
A polynomial f(x, y) in F[x, y] is called irreducible if in any 
factorization f(x, y) = 
g(x, y)h(x, y), one of g and h is a constant. 
Example 
The polynomials x, y and x - y are all irreducible. So is any 
irreducible polynomial in a single indeterminate x or y alone. There are many 
more. For instance, the polynomial x3y + y3 + x is irreducible over Z/2. 
It turns out that irreducible polynomials in two indeterminates have just 
the same properties as those in a single indeterminate, but it is much harder 
to prove that, because we cannot use Euclid's algorithm or the 1-trick. While 

Introduction to algebraic curves 
335 
we need to know the properties of irreducibles in two indeterminates, the 
proofs are not useful in applications in the way that the 1-trick is, so I shall 
only sketch the theory. You can find full proofs in Birkhoff and MacLane 
(1977) and most other standard texts on algebra. 
Proposition If f(x, y) is irreducible and f(x, y) divides g(x, y)h(x, y) then 
f(x, y) divides one of g(x, y) and h(x, y). 
The idea of the proof is to introduce the field R of rational functions in x. 
Thenf, g and h can be considered as polynomials in R[y]. Next one shows 
that f is still irreducible in R[y], so that one can apply the 1-trick there. 
That establishes that one of g and h is of the form f · r, where r is a polynomial 
in y with coefficients that are rational functions of x. Finally, one shows that 
all the coefficients of r are polynomials in x, so that r e F[x, y]. This proof 
was invented by Gauss. 
It then follows by the standard arguments that every polynomial in two 
indeterminates has a unique factorization into irreducible&. With the aid of 
unique factorization we can still find highest common factors, but it is no 
longer true that the HCF of f(x, y) and g(x, y) can be written in the form 
uf + vg, and there is no quick way of calculating it. 
Our first refinement of the definition of a curve C: f(x, y) = 0 is to require 
the defining polynomial f(x, y) to be irreducible. That eliminates the 
possibility that C splits into two curves. 
21.3 Increasing the supply of curves 
A plane curve over GF(2) cannot have more than 4 points with coefficients 
in GF(2) itself, because the only available points are (0, 0), (0, 1), (1, 0), (1, 1). 
This means that, as far as the points over GF(2) alone are concerned, there 
are only at most 24 = 16 plane curves. That is rather restrictive. To get 
enough different curves, we must admit points with coordinates (ex, p) in fields 
E containing F. For instance, if we define curves over the real numbers, we 
shall still regard the complex point (i, .J2) as part of the circle x2 + y2 = 1 .  
We do not need to allow all possible fields containing F, and restrict our 
attention to finite extensions. 
Definition 
A field E is called a finite extension of F if E is finite dimensional 
as a vector space over F. 
Example The complex numbers C form a finite extension of the real 
numbers R. If F £;; E are finite fields, then E is a finite extension of F. But 
the real numbers R do not form a finite extension of the rational numbers Q. 

336 
Error-correcting codes and finite fields 
Our second refinement of the definition is to admit points with coordinates 
iq finite extension fields. 
21.4 Absolute irreducibility 
When we extend from F to E it is possible that the polynomial defining C: 
f(x, y) = 0 may factor. 
Example 
Suppose we t3ke F = R and consider the 'curve' C: x2 + 
1 = 0. 
This has no real point!i at all, but over C it splits into two straight lines, 
x 
= i and x 
= - i. 
On the other hand the other example C: x2 + y2 + 1 = 0, which also has 
no real points, becomes a 'circle of radius i' with points such as i(a, p), 
where 
(a, p) 
lies on the circle x2 + y2 - 1 = 
0. 
To avoid the problem with x2 + 1, we now require that the defining 
polynomial of any curve remains irreducible in E [x] for any finite extension. 
Definition A polynomiـl f(x, y) in F[x, y] is called absolutely irreducible 
if, for every finite extension E of F, f(x, y) is irreducible in E[x, y]. 
Example 
The circle C: x2 + y2 - 1 = 0 and parabola C: y2 - x = 0 are 
algebraic curves over R. 
21.5 Projective transf()rmations 
Even this definition of a curve is not quite sufficient. It is necessary to add 
a few more points. You will probؿbly be familiar with the fact that for 
many theorems about functions on complex numbers one must admit an 
additional 'point at infinity'. That point ensures that all rational functions 
including 1/x 
have zeros. For our theory we also need to allow points at 
infinity. We can obtain these by allowing certain non-linear coordinate 
transformations which are called projective because of their relation to 
perspective drawing. We shall need just two such transformations. 
Definition 
The coordinate transformations from the standard (x, y)-system 
to the (u, v)- and (w, z)-systems given by the rules 
u = 1/x, 
v = yjx 

Introduction to algebraic curves 
337 
and 
w = ljy, 
z = xfy 
will be called projective coordinate changes. 
The transformation from the (u, v)-system to the (w, z)-system is of the 
same type (see Exercise 21.2). 
Points can be given in any of the three coordinate systems, most points 
are representable in all three systems, but some are only representable in 
two of them and a very few are only representable in one system. 
Example 
The point (x = 1, y = 1) is the same as the point (u = 1, v = 1), 
the point (x = a, y = b) for a #- 0 is the same as the point (u = a- 1, v = ba- 1). 
The point (x = 0, y = 1) has no equivalent in the (u, v)-system, but it is the 
same as (w = 1, z = 0). The point (u = 0, v = 1) has no equivalent in the 
(x, y)-system, but it is the same as (w = 0, z = 1). The point (x = 0, y = 0) 
has no equivalent in the (u, v)- or the (w, z)-system. 
Convention 
We usually specify the coordinate system we are using by the 
notation P: (x = IX, y = p). The systems are used in the following order of 
priority. If a point can be reprsented in the (x, y)-system, then we use that 
system. Failing that, if the point can be represented in the (u, v)-system 
(necessarily with u = 0) we use that system. The single remaining point 
(w = 0, z = 0) is represented in the (w, z)-system. 
We can consider the points that cannot be represented in the (x, y)-system 
as forming a line at infinity or horizon, which intersects every ordinary line 
in a single point. Thus (u = 0, v = a) is the intersection of the line y = ax 
with the horizon, and (w = 0, z = 0) is the intersection of the y-axis, x = 0 
with the horizon. 
21.6 Final definition of algebraic curve 
To decide whether a point (u = 0, v = p) lies on a curve C: f(x, y) = 0, we 
need to transform the equation itself. That can be done provided f(x, y) -:1- x. 
In the exceptional case the curve is a straight line. That line is, of course, 
the horizon of the (u, v)-system. 
Example 
The straight line y = ax has as its polynomial ax - y. The 
equation can be written as yjx = a. In the (u, v)-system that transforms to 
v = a, confirming that (u = 0, v = a) lies on the line. 
The rectangular hyperbola x2 - y2 = 1 transforms to 1/u2 - v2ju2 = 1. 

338 
Error-correcting codes and finite fields 
For x # 0, 1/u of; 0. So we can multiply this equation by u2 to get the 
͜urve l = v2 + u2• This is the equation of a circle with the line u = 0 as a 
diameter. Thus the hyperbola can be viewed as a circle with the horizon as 
a diameter. 
The parabola 2x = y2 + 1 transforms to 2/u = v2 /u2 + 1. Again we 
multiply by u2 to get 2u = v2 + u2 or (u - 1)2 + v2 = 1 . This is also the 
equation of a circle, but this time u = 0 is a tangent line. The parabola can 
be viewed as a circle with the horizon as a tangent. 
The curve x3 + y3 - l = 0 transforms to 1/u3 + 
v3 ju3 - 1 = 0 or 
1 + 
v3 - u3 = 0 the same way. If, as will be the case in our examples, the 
characteristic of the underlying field is 2, then the formula remains the same 
as the original formula in x and y. 
Notice that in all cases the degree of the equation is not changed by the 
transformation. 
We can now give our final definition of an algebraic curve. 
Definition Let f(x, y) be an absolutely irreducible polynomial in F[x, y] of 
degree d and define 
g(u, v) = udj"(lju, vju); 
h(w, z) = wdj"(zjw, 1/w). 
Then the algebraic curve C defined by f(x, y) is the union of the three curves 
C1:f(x, y) = 0, C2: g(u, v) = 0, and C3: h(w, z) = 0, points being allowed to 
have coordinates in any finite extension E of F. We call these three curves 
the affine components of C, and will still denote the full curve by C:f(x, y) = 0. 
When I wish to restrict attention to a particular component I shall speak of 
the affine curve C: d(x, y) = 0. 
If f(x, y) = x, then g(u, v) = 1, so the (u, v)-affine component is empty. 
That is because f(x, y) is then the horizon of the (u, v) system. For other 
cases we must show that the affine components fit together properly, and 
that g and h satisfy the conditions for an affine curve. 
Proposition Let f(x, y) # ax be an absolutely irreducible polynomial of 
degree d. If g(u, v) = udf(1ju, vju), then g is an absolutely irreducible polynomial 
of degree d. Furthermore, if 
a single point P has coordinates P: (x = IX, y = p) 
and also P: (u = y, v = c5), then f(iX, p) 
= 0 if 
and only if 
g(y, c5) = 0. 
Proof Corresponding to a term bx'y' of f(x, y), g(x, y) has a term 
bud-r-v. As f(x, y) # ax and f(x, y) is absolutely irreducible, x does not 

Introduction to algebraic curves 
339 
divide f(x, y). Hence at least one term of f(x, y) has r = 0. That proves that 
g has degree d. 
Suppose that g is not irreducible over some finite extension field, say 
g(u, v) = h(u, v)k(u, v), with h and k of degrees r and s respectively. Then 
f(x, y) = (x'h(1jx, yjx))(x•k(1jx, yjx)), and the two factors are polynomials of 
degree r and s respectively. Since f is absolutely irreducible, it follows that 
r = 0 or s = 0. But that implies that one of h and k is a constant and hence 
that g is absolutely irreducible. 
By assumption y = 1/rr. and b = {Jjrr. and rr. "# 0. Then g(y, b) = rr.df(rr., {J). 
Thus g(y, b) = 0 if and only if f(rr., {J) = 0. 
• 
21.7 Curves over finite fields 
Up until now our examples have been real curves, to aid the reader's 
geometrical intuition, but for codes we need to use curves over finite fields. 
We conclude the chapter with three such curves. These examples will be 
important in later chapters. For all three curves we calculate all the points 
with coordinates in the fields GF(2), GF(4), GF(8) and GF(16). The fields 
GF(2) = {0, 1 }  and GF(4) = {0, 1, 10, 1 1 }  are subfields of GF(16), but GF(8) 
is not. So we provide a separate table for GF(8) based on the polynomial 
x3 + x2 + 1 
here. In order to distinguish elements of GF(8) from those of 
GF(16), we shall write 2', 3' etc. 
Table of GF(8) based on x3 + x2 + 1 
log 
0 
1 
5 
2 
3 
6 
4 
0 
l 
2 
3 
4 
5 
6 
7 
0 
X 0 
0 
l 
+ 1 
7 
2 
2 
3 
3 
3 
4 
4 
4 
6 
5 
5 
1 
6 
6 
5 
7 
7 
2 
We shall observe the convention that we use the (x, y)-system where 
possible, the (u, v)-system for points of the form (u = 0, v) and the (w, z)­
system only for (w = 0, z = 0). 

340 
Error-correcting codes and finite fields 
<1.8 An example of a cubic curve 
Example The curve x3 + y3 + 1 = 0 
This curve has the equations u3 + v3 + 1 = 0 in the (u, v)-system and the 
equation w3 + z3 + 1 = 0 in the (w, z)-system. Points over GF(2): 
(x = 
0, y = 1); (x = 
1, y = 
0); (u = 
0, v = 1); 
Points over GF(4): 
the points over GF(2) and 
(x = 0, y = 10), (x = 0, y = 1 1  ); (x = 10, y = 0), (x = 1 1, y = 0); 
(u = 0, v = 10), (u = 0, v = 1 1); 
Points over GF(8): 
the points of GF(2) and 
(x = 2', y = 5'), (x = 4', y = 6'), (x = 7', y = 3'); 
(x = 5', y = 2'), (x = 6', y = 4'), (x = 3', 
y = 7'). 
Points over GF(16): 
the points over GF(4) and no further points. 
That is because x3 = 0, 1, 3, 
5, 8, or 15. The only two cubes adding to 1 
are 0 and 1 and the only solutions of x3 = 1 are 1, lO and 1 1. 
21.9 The Klein Quartic 
Example The Klein Quartic has equation x3y + y3 + X =  0. This has 
v3u + u3 + v = 0 and w3z + z3 + w = 0 as its equations in the other co-
ordinate systems. Points over GF(2): 
· 
(x = 0, y = 0); (u = 0, v = 0); (w = 0, z = 0). 
Points over GF(4): 
the points over GF(2) and 
(x = 10, y = 1 1), (x = l l, y  = 10). 
Points over GF(8): 
the points over GF(2) and the following (x, y)-points: 
(1', 3'), 
(1', 5'), (1', 6'); (3', 
1'), (5', l'), (6', l'); 
(2'' 2'), ( 4'' 4'), (7'' 7'); 
(2', 5'), (4', 6'), (7', 3'); 
(2', 7'), (4', 2'), (7', 4'); 
(3', 
4'), (5', 7'), (6', 2'); (3', 
5'), (5', 6'), (6', 3'). 

Introduction to algebraic curves 
Points over GF(16): 
the points over GF(4) and the following (x, y)-points: 
(3, 10), (5, 1 1), (8, 10), (15, 1 1); 
(10, 2), (1 1, 4), (10, 9), (1 1, 14); 
(6, 8), (13, 15), (7, 3), (12, 5); 
341 
Notice that once a single point (ex, fJ) on the curve has been found, the point 
(cx2, {32) also lies on the curve. The classes of points obtained this way are 
separated by semicolons. 
· 
21.10 A quintic 
Example The curve x5 + y5 + 1 = 0. The other equations of the curve are 
u5 + v5 + 1 = 0 and w5 + z5 + 1 = 0. Points over GF(2): 
(x = 0, y = 1); (x = l, y = 0); (u 
= 0, v = 1); 
Points over GF(4): 
the points over GF(2) and 
(x = 10, y = 1 1), (x = 1 1, y = 10). 
Points over GF(8): 
the points over GF(2) and the following (x, y) points: 
(2', 5'), (4', 6'), (7', 3'); (5', 2'), 
(6', 4'), (7', 3'). 
Points over GF(16): 
The points over GF(4) and 60 further points obtained as follows. 
There are five fifth roots of 1 : 1, 3, 5, 8 and 15. Points of the form (0, 1) 
have already been counted, but there are 12 more: 
(x = 0, y = 3, 5, 8, 15); (x = 3, 5, 8, 15, y = 0); (u = 0, v = 3, 5, 8, 1 5); 
There are also five fifth roots of 10: 10, 4, 9, 12, 
14; and five fifth roots 
of 1 1: 1 1, 2, 6, 7, 13. The points these produce can all be expressed 
in the (x, y)-system. They are obtained by taking (x = 10, 4, 9, 12, 14, 
y = 1 1, 2, 6, 7, 13) or (x = 1 1, 2, 6, 7, 13, y = 10, 4, 9, 12, 14). The two points 
(10, 1 1) and (1 1, 10) have already been counted. The 48 remaining points 
fall into classes of 4, obtained by the squaring method as in the previous 
example. 
One purpose of this example is to show that curves can have a very large 
number of points. That is the underlying reason why there are such good 
geometric Goppa codes. 

342 
Error-correcting codes and finite fields 
21.1 1 Summary 
In this chapter we have studied the basic properties of polynomials in two 
indeterminates, and used them to define algebraic plane curves. In order to 
complete the curves we permitted field extensions and projective trans­
formations, giving us three different coordinate systems. We finished by 
calculating all the points of three example curves over the fields of orders 2, 
4, 8, and 16. In the next chapter we shall investigate the properties of 
functions on curves. 
21.12 Exercises 
21.1 
Show that if f(x, y) has degree 1, then F[x, y]jf(x, y) ७ F [x]. 
21.2 Show that the transformation from the (u, v)-system to the (w, z)­
system via the (x, y)-system is given by z = 1/v, w = ujv. 
21.3 Homogeneous coordinates. Replace an (x, y)-point in the plane by the 
point (x, y, 1), a (u, v)-point by (1, v, u), and a (w, z) point by (z, 1, w). 
Now regard two points (a, b, c) and (a', b', c') as equivalent if there 
exists a non-zero constant k with a' = ka, b' = kb and c' = kc. Show 
that (a, b, c) is equivalent to an (x, y)-point if and only if c #- 0. Show 
that the (x, y)-point equivalent to (a, b, c) is unique if it exists. Finally, 
show that the points in the (x, y)- and (u, v)-points equivalent to 
(a, b, c) (where a #- 0 and b #- 0) are linked by the projective trans­
formation of the text. 
21.4 Why did I choose the Klein quartic as my example of a curve of 
degree 4, and not the curve x4 + y4 = 1? 
21.5 Find the points of the curve x 7 + y 7 = 1 in the fields G F(2), G F( 4 ), 
GF(8) and GF(16). 

22 
Functions on algebraic curves 
In the previous chapter we obtained a satisfactory definition of a plane 
algebraic curve. The next step is to investigate the behaviour of rational 
functions at points of the curve. 
22.1 Congruence 
Example 
On the unit circle polynomials that differ by multiples of (x2 + 
y2 - 1) have the same values. So as far as the circle is concerned, they are 
the same. We shall make a definition of congruence modulo f(x, y) that 
reflects this idea for the curve C:f(x, y) = 0. The different possible poly­
nomial functions on the affine curve will then correspond to the congruence 
classes modulo f(x, y). 
Definition 
Let f(x, y) be an irreducible polynomial in F[x, y]. Then we 
define polynomials g(x, y) and h(x, y) to be congruent modulo f(x, y), 
g(x, y) = h(x, y) (mod f(x, y)) 
if 
g(x, y) - h(x, y) = 
q(x, y)f(x, y) 
for some polynomial q(x, y). The set of all polynomials congruent to a given 
polynomial g(x, y) modulo_f(x, y) is called a congruence class modulo f(x, y). 
With polynomials in a single indeterminate, division with remainder 
enables us to pick out a 'best' member of each class, namely the common 
remainder on division by f(x). For polynomials in two indeterminates, such 
a choice is no longer possible. So we shall have to operate with the classes 
themselves. 
22.2 The coordinate ring 
Addition and multiplication of polynomial functions on a curve make sense, 
so we shall introduce addition and multiplication of congruence classes. 
When you read 'add congruence class A to congruence class B' you should 

344 
Error-correcting codes and finite fields 
consider this as shorthand for 'take any polynomials g e A and h e  B and 
add them; consider only the class of the result g + h'. This is a bit like the 
childhood rule 'odd + odd = even', if we take odd to mean the class of odd 
integers and even to mean the class of even integers. 
Definition 
Let f(x, y) be an irreducible polynomial in F[x, y]. We define 
the residue class ring F[x, y]/.f(x, y) to have as its elements the congruence 
classes modulo f(x, y). Addition and multiplication are defined by the 
following rule. 
If A and B are congruence classes modulo J, choose g e A and h e B and 
define A + B and AB to be the classes containing g + h and gh respectively. 
If C is the affine curve C:f(x, y) = 0, then F[x, y]/f(x, y) is called the 
coordinate ring of C and denoed by F [ C]. 
Proposition The definition above makes F[x, y]/f(x, y) into an integral 
domain. 
Proof The main effort of the proof is expended in showing that the 
operations are well defined, which means that the result of an operation is 
independent of the elements that are chosen in each class. The verification 
of the axioms then follows a familiar course. 
Addition and multiplication are indeed well defined. For suppose that g 
and go both lie in A and h and h0 both lie in B. We must show that g + h 
and go + ho lie in the same class, and also that gh and goho lie in the same 
class. By assumption f divides g - go and h - h0• Hence it divides g + h ­
(g0 + h0). So g + h and go + h0 lie in the same class. Also f divides 
(g - go)h + g0(h
- h0) = gh - goho. So gh and g0h0 lie in the same class. 
To establish those axioms that express the identity of two formulae, we 
need only choose representatives of the classes in question and appeal to the 
validity of the formulae in F[x, y]. Thus to prove 
Al. A + (B + C) = (A + B) + C, 
we choose g e A, h e B and k e C. Then g + (h + k) = (g + h) + k, establish­
ing the formula. This argument can be adapted to prove all the commutative, 
associative and distributive laws. 
The 0 and 1 class are the classes containing the 0 and 1 of F[x, y]. Thus 
the 0 class is the set of multiples of f(x, y). 
The negative of the class A is the class containing the negatives of its 
elements (if you prefer: choose g e A and take the class of -g). 
There remains the question of the cancellation law. That follows from 
Proposition 22.1. Suppose that AB is the 0 class. That means that for g e  A 
and h e B, f divides gh. Since f is irreducible, it follows that f divides one 
of g 
or h. Thus A = 0 or B = 0. 
• 

Functions on algebraic curves 
345 
22.3 The function field 
The structure of the residue class ring F[x, y]/f(x, y) is closely related to 
the nature of the affine curve f(x, y) = 0. Indeed it is so closely linked that 
the structure can change under projective transformations. To see that, 
consider the polynomial x. For any affine curve C 1 :f(x, y) = 0, x is a poly­
nomial function, but for another component of the same curve C2: g(u, v) = 0, 
x corresponds to the function 1/u. This function will usually not be equivalent 
to a polynomial. 
To avoid having to check which polynomials remain polynomials under 
projective transformations, we extend our residue class ring to rational 
functions. These are invariant under all the transformations we need and are 
also the functions used to generalize BCH and Goppa codes. 
Definition The function field F( C) of the curve C: f(x, y) = 0 over the field 
F is the field of fractions of F[C] = F[x, y]ff(x, y). 
Recall that the field of fractions of D 
is constructed by taking fractions 
ajb, with a, b E  D 
and b =1- 0. Fractions ajb and cjd are considered equal if 
ad = be, and we use the usual rules of addition and multiplication: 
ajb + cjd = (ad + bc)jbd, 
(a/b)(c/d) = acjbd. 
For details of this construction see Appendix PF on polynomials in Part 2. 
22.4 Equivalence of rational functions 
It is important to realize when two rational functions if>(x, y) and t/J(x, y) 
represent the same element of F(C). To that end we adapt Definition 19.3 
to rational functions in two indeterminates. 
Definition 
Let if>(x, y) be a rational function, the representation n(x, y)/ 
u(x, y) of </J(x, y) is said to be cancelled or in lowest terms, if the highest 
common factor of n(x, y) and u(x, y) is 1. If/(x, y) is a polynomial then the 
congruence 
if>(x, y) = 
0 modf(x, y) 
means that in the representation of if>(x, y) as n(x, y)ju(x, y) in lowest terms, 
f(x, y) divides n(x, y). It follows that f(x, y) and u(x, y) must be relatively 
prime. 
We shall say thatf(x, y) divides the rational function if>(x, y) if if>(x, y) = 
0 

346 
Error-correcting codes and finite fields 
(modf(x, y)). For two rational functions t/J(x, y) and t/J(x, y), the congruence 
t/J(x, y) = t/J(x, y) modf(x, y) 
means that t/J(x, y) - t/J(x, y) = 0 modf(x, y). 
Proposition Let C:f(x, y) = 0 be an algebraic curve defined over the field 
F, and for j = 1, 2, let t/Jj(x, y) = ni(x, y)jui(x, y) be two rational functions in 
cancelled form. Assume thatf(x, y) does not divide ui(x, y), j = 1, 2. Then 
(a) 
tPi represents Q 
in F(C) if 
and only if 
tPi = 0 (mod f); 
(b) t/J1 and t/J2 represent the same class of F(C) if 
and only if 
tPt = tP2 
(mod f); 
(c) if u1 = u2 and n1 = n2 (mod f), then t/J1 = t/J2 (mod f). 
Proof (a) By definition 
if and only if 
if and only if 
if and only if 
tPi = 0 (mod f) 
ni = fqi for some qi E F[x, y] 
ni represents Q 
in F[C] 
nijui represents Q 
in F(C). 
(b) The functions t/J1 and t/J2 represent the same element of F(C) if 
and only if t/J1 - tfJ2 represents Q in F(C). By part (a) that is equivalent 
to the statement that t/J1 - t/J2 
= 0 (mod f). 
(c) t/J1 - t/J2 = (u1n2 - u2n1)/u1u2• By hypothesis, f divides u1 - u2 and 
also n1 - n2• Hence f divides 
As f is irreducible it does not divide n1u2• Thus t/J1 - t/J2 = 0 (mod f) . 
• 
Example 
Consider the cubics x3 = y and x3 + y3 = 1. The first gives 
an example of a rational curve. By replacing every occurrence of y by 
x3, every polynomial in F[x, y]j(x3 - y) can be reduced to a polynomial in 
x alone. Thus F[x, y]j(x3 - y) is isomorphic to F[x]. An algebraic one­
dimensional bug living on this curve could not tell that it was not on a 
straight line. 
That always applies for quadratic curves (see Exercises 22.9 and 22.10), 
but not necessarily for cubics.In particular it does not apply for the second 
cubic (see Exercise 22.1). So by doing algebraic calculations a one-dimen­
sional bug living on the curve x3 + y3 = I could determine that it was not 
on a straight line. 

Functions on algebraic curves 
347 
22.5 Independence of coordinate system 
The function field F(C) of a curve is the same for all three of its affine 
components. It does not depend on the choice of coordinate system. 
Proposition Let f(x, y) # ax be an irreducible polynomial of degree d, and 
let g(u, v) = udf(1ju, vju). Then the map q: h(x, y) --.. h(1ju, vju) induces an 
isomorphism of the function fields of the curves f(x, y) = 0 and g(u, v) = 0. 
Notice that h is not multiplied by a power of u. 
Example 
Consider the curve C: x3 + y3 - 1 = 0. Let fjJ be the function 
x3j(x + y), c/J0 the function y3j(x + y). Then in F(C), 
¢ + c/J0 = (x3 + y3)j(x + y) = 1/(x + y). 
In the (u, v)-system C is defined by 1 + v3 - u3 = 0. 
q(¢) = (1/u3)/(1/u + vju) = 1/u2(1 + v), 
q(¢0) = v3 ju2(1 + v) 
0"(1/(x + y)) = uj(l + v). 
q(¢) + q(¢0) = (1 + v3)ju2(1 + v) = u3ju2(1 + v) 
= uj(l + v) = O"(l/(x + y)). 
Proof For the moment, consider the curves C :f(x, y) = 0, and D: g(u, v) = 0 
to be different. We first take q to define a map from F [x, y] into F(D) and 
show that O"(h) = 0 if and only if h is a multiple of f. Suppose that O"(h) = 0. 
That means that h(1ju, vju) = g(u, v)k(u, v) in F [u, v]. Let deg(f(x, y)) = d 
and deg(k(u, v)) = e. Then 
h(lju, vju)jud+e 
= (g(u, v)jud)(k(u, v)jue). 
Hence 
xd+eh(x, y) = f(x, y)xek(ljx, yjx). 
As xek(1jx, yjx) is a polynomial in x and y, it follows that f(x, y) divides 
і+eh(x, y). Since f is irreducible and f # ax it follows that f(x, y) divides 
h(x, y). 
Thus O"(h) = qW) if and only if h and h0 are congruent modulo f(x, y). 
So q defines a map from F [C] into F(D). It is obvious that q(h + h0) = 
q(h) + s(h0) and O"(hh0) = O"(h)O"(h0). We extend the map to the whole of 

348 
Error-correcting codes and finite fields 
F(C) by defining a(h/h0) = a(h)Ja(h0) when ho =I 0 modulo f(x, y). That 
produces an embedding of F( C) in F(D ). 
The same argument can be applied reversing the roles of f and g, 
producing an inverse embedding of F(D) in F(C). Thus the two fields are 
the same. 
• 
22.6 Evaluating functions at points 
A polynomial f(x, y) e F[C] can be evaluated at all (x, y)-points of C. 
Obviously, if the point P: (x = 0!, y = /3) 
has coefficients in an extension field 
E of F the value of f may be in E. However, a rational function cjJ may not 
be defined at all for certain points, for instance 1/x is not defined at the 
origin. These points are called poles of the rational function. If a point P is 
a pole of a function c/J, then the inverse function, cjJ - l  has value 0 at P, and 
we say P is a zero of c/J- 1. For well-behaved points on a curve we can refine 
this notion by defining an order function v at the point P, such that for 
cjJ e F(C), v(c/J) > 0 if P is a zero of cjJ and v(c/J) < 0 if P is a pole of c/J. This 
mimics the definition of the order of a complex rational function at a point 
in the complex plane. It is possible to go further and copy the theory of 
residues, but we shall not need that. 
Using projective transformations we can evaluate rational functions at 
(u, v)-points and (w, z)-points also. 
Example Let F = R and consider the circle C: x2 + y2 = 1. 
Let cjJ = x(x - 1)2/(y - 1)2• Then for P = (1, 0), v(c/J) = 2, because of the 
factor (x - 1)2 in the numerator. For P = (0, 1), v(c/J) = - I, because we have 
factors x in the numerator and (y - 1)2 in the denominator. 
Let 1/J = x2 + 1 = (x + i)(x - i). Then for P = (i, - i), and P = ( -i, i), 
v(t/J) = I. On the other hand, at these points v(c/J) = 0 and this indicates a 
non-zero value ± i(i - W/(i + 1)2 = ± i(i - 1)4/4. 
It is also possible for a function to have poles or zeros at (u, v)- or 
(w, z)-points. Let x = x. Then for P, u = 0, v = i and P, u = 0, v = -i. In 
the (u, v)-system x becomes lju, so it has order - 1  at P and P. It will turn 
out that all non-constant polynomials have poles at the horizon. 
The order of any real function is always the same at conjugate complex 
points P and P, and if the order is 0, then the values at conjugate points are 
conjugate. 
22.7 Discrete valuations 
The word 'order' is rather over-used in mathematics, so in the formal 
definition we replace it by 'discrete valuation'. 

Functions on algebraic curves 
349 
Definition 
Let F be a field. A discrete valuation v on F is a function from 
F * to Z (the asterisk signifies that v is not defined for 0). It has the following 
properties: 
DVl. v(ab) = v(a) + v(b) 
DV2. v(a + b) ć  min{v(a), v(b)} 
DV3. v(a) = 1 for at least one a. 
It is sometimes convenient to put v(O) = oo, which preserves the axioms 
even when a = 0 or b = 0. 
Example Consider rational functions in a single indeterminate x over the 
real numbers R. We use the language of algebraic curves and regard this as 
the function field R(C), where C: y = 0 is a straight line. 
For the point x = 0, we define the order v(f) of a polynomial f(x) to be 
the power to which x divides f of x, for rational functions fig, v(flg) = 
v(f) - v(g). It is easy to check that this is a discrete valuation. The point 
x = 0 is a zero of multiplicity m > 0 of fig if v(flg) = m. It is a pole of 
multiplicity m if v(flg) = -m. 
Now define p.(f) to be the power to which the irreducible polynomial 
(x2 + 1) divides f(x). Again p.(flg) = p.{f) - p.(g). This is again a discrete 
valuation (DV1 holds because (x2 + 1) is irreducible). (x2 + 1) is the minimal 
polynomial of the complex points x = i and x = -i, and now p.(flg) reflects 
the multiplicities of these points as zeros or poles. Notice that for real 
functions the multiplicities are necessarily equal. 
There is also a point at infinity on the line obtained by the projective 
transformation u = 1jx. A polynomial ax" + · · · +  b with a =f; 0 transforms 
to the rational function (a + · · · + bu")/u". At u = 0 the order of this function 
is -n = - deg(j{x)). This extends to rational functions by defining v(flg) = 
deg(g(x)) - deg(j{x)). You can check directly that this defines a discrete 
valuation. 
The following proposition is used implicitly in many order calculations. 
Its easy proof is left as an exercise to the reader (Exercises 22.4 and 22.5). 
Proposition If v is a discrete valuation, then v(1) = 0, if v(a) < v(b), then 
v(a + b) = v(a). 
• 
22.8 Order functions 
We can now define an order function for a point of a curve. There are two 
candidates, the x-order and the y-order. Usually they are the same, but a 
little care will be needed. 

350 
Error-correcting codes and finite fields 
Definition · Let C:f(x, y) = 0 be a curve and let P: (x = IX, y = p) 
be a point 
of C with IX, p 
e F. Let g(x, y) e F[C], then the largest power n for which 
there exist polynomials go(x) e F[x] and ho(x, y) e F[x, y] with h0(0, 0) ::1- 0 
such that 
is called the x-order of g 
at P and denoted by vp,x(g). 
The x-order vp, x(g/h) 
is defined as vp,,(g) - vp,;c(h). 
The y-order is defined analogously. 
Example Consider the circle x2 + y2 = 1 and the point P: (x = 0, y = 1). 
The x-order of x" is obviously n. If the base field is R then 
y - 1 = -x2j(y + 1) mod (x2 + y2 - 1), 
so the x-order of y - 1 is at least 2. We shall shortly show that it is exactly 
2. Similarly y = ( -x2 + 1)/(y + 1) and y + 1 = ( - x2 + 2)/(y + 1) have 
x-order 0. 
The x-order of y3 + y2 - y - 1 = (y - 1)(y + 1)2 should be 2, and indeed 
y3 + y2 - y - 1 = - x2( -x2 + 2)/(y + 1). 
The y-order is not a discrete valuation, because the y-order of x2 = 
(1 - y)(1 + y) is 1 and the y-order of x cannot be defined. 
Note that if the base field is GF(2), then y + 1 has value 0 at P and so 
these calculations become invalid, but similar calculations can be made in 
that case also. 
22.9 Orders as discrete valuations 
For well-behaved points at least one of the two orders is a discrete valuation, 
and usually they are the same. We shall use formal partial derivatives to 
define what we mean by well-behaved. They are defined by the standard 
formula in the same way as the ordinary derivative. 
Definition 
Let f(x, y) = L aijxj Yi be a polynomial. Then ofjox is defined 
by the formula L i · aiixi - 1 yi. Here the factor i indicates standard multiplica­
tion by an integer. 
Theorem Let C:f(x, y) = 0 be an affine curve, and let P: (x = IX, y = p) 
be 
a point of C. 
(a) If ofjoy(1X, p) 
::1- 0, then the x-order is a discrete valuation. 
(b) If also ofjox(IX, p) 
::1- 0, then the x-order and the y-order are the same. 

Functions on algebraic curves 
351 
Example 
Again taking the circle and the point P: (0, 1), iJfjiJy(O, 1) = 2, so 
the x-order is a discrete valuation. As the x-order of y + 1 at P is 0, it follows 
that the x-order of y - 1 = -x2j(y + 1) is 2 - 0  
= 2. 
The calculation iJfjiJx(O, 1) = 0 confirms that the y-order is not a discrete 
valuation. 
Proof The proof is somewhat lengthy, though the details are not hard. 
We can expandf(x, y) in terms of x - ct and y - {J. That gives a kind of 
'Taylor expansion' of f(x, y) at P, but because f(x, y) is a polynomial, 
differentiation is not required to construct it. Let 0 # b = iJfjiJy(ct, {J) and 
a = iJfjiJx(ct, {J). Then 
f(x, y) = a(x - ct) + b(y - {J) + higher degree terms in (x - ct) and (y - {J). 
(1) 
There is no constant term, because P lies on the curve. We gather terms 
involving (y - {J) and write 
(y - {J)(b + (y - {J)g1(x - ct, y - {J)) = f(x, y) + h1(x - ct}, 
(2) 
where g1(x, y) is a polynomial in two indeterminates and h1(x) is a 
polynomial in a single indeterminate. Consider first the case that h1 (x) = Q. 
Then y - fJ dividesf(x, y) and sincef(x, y) is absolutely irreduciblef(x, y) = 
y - {J. In this case replacing y by fJ produces an isomorphism of F[C] with 
F[x] and the x-order is just the power to which x divides a polynomial. 
That is a discrete valuation as claimed, so we may assume that h1(x) # Q. 
In that case we extract the highest power of x - ct dividing h1 (x - ct) and write 
h1 (x - ct) = (x - ctth(x - ct), 
where x - ct does not divide h(x), and 
(3) 
(b + (y - {J)g1(x - ct, y - {J) = g(x, y), 
(4) 
where g(O, 0) # 0. So 
(y - {J) = (x - ct)"h(x - ct)/g(x - ct, y - {J) (modf(x, y)). 
(6) 
Given any polynomial r(x, y), we can expand it as ro(x - ct, y - {J) and 
use equation (6) to replace all terns (y - {J). That gives an expression 
r(x, y) = (x - ct)kho(x - ct}/g0(X - ct, y - {J) 
with g0(0, 0), h0(0) # 0. 
Now we show that the power k in the formula is unique. For if 
(x - ctth0(X - ct)/g0(X - ct, y - {J) = (x - ct)1h1(x - ct)jg1(x - ct, y - {J) 
(mod f), 
where go(O, 0), g1(0, 0), h0(0), h1(0) # 0, and say k > I, then f divides 
(x - ct)1((x - ct)k-1h0(X - ct) - h1(x - ct}). 

352 
Error-correcting codes and finite fields 
Since iJffiJy(a., fJ) -:1: 0, (x - a.) -:1: f(x, y). Thus (x - a.) and f(x, y) are 
relatively prime. Hence f divides 
(x - a.)"-1h0(X - a.) - h1(x - a.). 
Since f(a., fJ) = 0, it follows that h1(a. - a.) = h1(0) = 0, contradicting our 
assumption. 
It is left as an easy exercise (Exercise 22.6) for the reader to check the 
axioms DV1-DV3. 
(b) To show that the two valuations are the same, note that in equation 
(1) we now have a -:1: 0 as well as b -:1: 0. So equation (6) can be written as 
(y - fJ)(b + (y - fJ)g(x - a., y - fJ)) = f(x, y) - (x - a)(a + h(x - a.)) 
Thus the x-order of y - fJ is 1. Therefore the x-order of g(y - fJ) is the same 
as the y-order of g(y -fJ). 
It follows that the x-order of h(x, y) is the same 
as the y-order of h(x, y) for al polynomials. 
• 
22.10 Properties of points and cones 
Definition 
Let C be the curve f(x, y) = 0, defined over a field F. A point 
(x = a., y = fJ) with coefficients in an extension field E of F field F is called 
non-singular if offox(a., fJ) -:1: 0 or offoy(a., {J) -:1: 0. For points defined in the 
(u, v)- and (w, z)-systems the deؼnition is analogous. A curve C with only 
non-singular points is called smooth. 
The order function Vp at a non-singular point P is the x-order if 
offoy(P) -:1: 0. Otherwise it is the y-order. 
Examples 
1. The curve C: x3 + y3 = 1 .  The curve is smooth for fields of characteris­
tic -:1: 3, because of/ox = 3x2 and offoy = 3y2 and (x = 0, y = 0) is not 
a point of C. The same argument works in the other coordinate systems. 
Consider the points P: (x = 0, y = 1). As of fox = 0 at P, we must use 
the x-order. So the order of x is 1. To calculate the order of y - 1 ,  
rewrite the Taylor expansion off(x, y) = x3 + (y + 1)(1 + y + y2). Thus 
y + 1 = x3/(1 + y + y2) and hence the order of y + l is 3. 
2. Tke Klein quartic x3y + y3 + x = 0. 
For a field of characteristic 2, the 
' 
partial derivative of/oX is x2y + 1. If this is 0, then x3y + y3 + X =  
x + y3 + x = y3, so y = 0, but then x = 0 and x2y + 1 = 1, giving a 
contradiction. A similar argument holds in the other coordinate systems. 
Thus the curve is smooth for fields of characteristic 2. Consider the 
point P: (x = 0, y = 0). As iJffiJy(a., fJ) = 0 at P, we must use the y-order. 
Rewritingf(x, y) = 0 as x(x2y + l) + y3 = 0, we se that x : y3f(x2y + 1). 
So the order of x is 3, while the order of y is 1. 

Functions on algebraic curves 
353 
3. The curve C: x5 + y5 = 1. This curve has ofjox = 5x4 and of/oy = 5y4; 
as (x = 0, y = 0) is not on the curve, it is smooth for fields of 
characteristic * 5. Again, the same argument works in the other 
coordinate systems. 
The order function at P: (x = 0, y = 1) can be obtained in the same 
way as for x3 + y3 = l. We write (y + 1)(y4 + y3 + y2 + y + 1) 
+ 
x5 = 0, to see that v(y + 1) = 5, while v(x) = 1. 
22.11 Summary 
We have now established the major properties of rational functions on 
curves. The most important of these is that for non-singular points there 
exists an order function v defined on rational functions, v(<f>) < 0 if 4> has a 
pole, and v(l/J) > 0 if 4> has a zero. This order function behaves in a manner 
similar to the negative of the degree. It is a discrete valuation. In the next 
chapter you will see that this enables us to define vector spaces using a 
rational curve. It is these spaces that determine the geometric Goppa codes. 
22.12 Exercises 
22.1 
Show that x3 + y3 - 1 is not a rational curve, using the following 
method. 
Suppose we could embed F[x, y]/(x3 + y3 - 1)  into the rational 
functions in t. Then let the image of x be p(t)/r(t) and y be q(t)/r(t), 
where we can assume that the polynomials p(t), q(t), r(t) have no 
common factors. Then in F[t] 
p3(t) + q3(t) - r3(t) = 0. 
Differentiate this equation, cancel the 3 and get 
[ p2(t)] 
[p(t) q(t) r(t) J q2(t) 
= O. 
p'(t) q'(t) r'(t) 
2 
-r (t) 
Deduce first that p2, q2 and -r2 are rational multiples of qr' - q'r, 
rp' - r'p and pq' - q'p and then since p, q and r are relatively prime, 
that p2jqr' - q'r, q2 jrp' - r'p, and -r2 j pq' - p'q. Let, say, p have the 
largest degree among p, q and r, then 2 deg(p) o deg(q) + deg(r) - 1, 
which is a contradiction. 
22.2 Show that if (a, p) is a singular point of the Klein Quartic over some 
field F, then 3P3 + 2oc = 0 and - 2P3 + oc = 0. Show that unless F has 

354 
Error-correcting codes and finite fields 
characteristic 7, the only solution for these equations is ex = p = 0. 
Deduce that for characterisitc # 7, the Klein Quartic is smooth. 
22.3 
For F = GF(7) = Z/7, show that (x = 2, y = 4) is a singular point of 
the Klein Quartic. 
22.4 
Show that for a discrete valuation v, v(l) 
= 0. 
22.5 
Show that for a discrete valuation v, v(a) 
< v(b) 
implies v(a + b) 
= v(a). 
22.6 Complete the proof of Theorem 22.9, by verifying that when 
offoy(ex, P> -1= o. 
then the x-order at P: (ex, p) satisfies axioms DVl-DV3. 
22.7 Show that the curve x7 + y7 = 1 is smooth for fields of character-
istic # 7. 
, 
22.8 
Let C:f(x, y) = 0 be an algebraic curve and let its equation in the 
(u, v)-system be g(u, v) = 0. Let P: (x = ex, y = p) be a point of the 
curves with (u, v)-coordinates (y, b). Show that if P is singular with 
respect to f(x, y) it is also singular with res.pect to g(u,v). 
22.9 Euler's substitution. Every quadratic curve is rational. Let 
C:f(x, y) = 0 
be a smooth curve with f of degree 2, such that the point P: (0, 0) lies 
on C. Show that for any t, the equation f(x, tx) = 0 is a quadratic 
with one root = 0 (except for possibly two values of t). Let u be the 
other root, show that there is a rational function g(z) with at most 
two poles such that u = g(t). Deduce that every (x, y) point of the 
curve C is of the form (g(t), tg(t)) for a choice of t. 
22.10 Euler's substitution (cont.) 
With the assumptions of Exercise 22.9, let 
h(t) = f(g(t), t(g(t)). Show that h is the zero function in t. Now let cp 
be the map from F[x, y] to F(t) taking k(x, y) to k(g(t), tg(t)). Show 
that ifJ induces an isomorphism of F(C) with F(t). This proves that C 
is rational. 

23 
A survey of the theory of algebraic 
curves 
In this chapter we state the main theorems of the theory of algebraic curves 
up to Riemann's theorem. These theorems are important in constructing and 
establishing the properties of geometric Goppa codes, but they do not enter 
directly into the calculations, once the codes have been constructed. As the 
proofs of the principal theorems are deep, I shall content myself with stating 
them. I shall show by means of examples and consequences derived from 
them, how they enable us to analyse the properties of curves. The theory is 
developed fully in several textbooks such as Shafarevitch (1974), Chevalley 
(1951 )  and Fulton (1969). The reader who would like a broader view of 
algebraic geometry related to codes is referred to van Lint and van der Geer 
(1988), though the latter book also omits many of the proofs. 
23.1 Conjugate points 
Notice that in Examples 21 .8-10 the points in the extension fields are 
grouped together by semicolons. The coordinates of points in the same group 
have the same minimal polynomials. Hence, if a rational function defined 
over GF(2) is zero on one point of a group it is zero on all the others as 
well. That means that, as far as GF(2) is concerned, we cannot distinguish 
which of the points in a group we have picked out. That is rather like the 
fact that as far as real functions are concerned we cannot distinguish between 
+ i and - i. 
Definition 
Let (x = oc, y = /3) be a point with coefficients in an extension 
field E of F. Then a point (x = 
oc', y = fJ') is called conjugate to (oc, fJ) if for 
any polynomial g(x, y) in F[x, y], g(oc, fJ) = 0 if and only if g(oc', /3') = 0. 
Example 
Recall Example 21.8, the curve x3 + y3 + 1 over GF(2). Its points 
are 
GF(2): 
(x = 
0, y = 1); (x = 1, y = 0); (u = 0, v = 1); 
GF(4): 
(x = 
0, y = 
10), (x = 0, y = 1 1); (x = 10, y = 0), (x = 1 1, y = 0); 
(u = 0, v = 
10), (u = 0, v = 1 1); 

356 
Error-correcting codes and finite fields 
GF(8}: (x = 2', y = 5'), (x = 4', y = 6'}, (x = 7', y = 3'); 
(x = 5', y = 2'), (x = 6', y = 4'), (x = 3', y = 7'). 
Distinct points over GF(2) are not conjugate. The points over GF(4) group 
into pairs of conjugates. The points of GF(8) group into triples. These 
groupings are indicated by semicolons. 
In Chapter 22 we observed the fact that the order functions of complex 
conjugate points on real polynomials 
were the same. From the real point 
of view they are identical twins. That holds in the more general situation of 
algebraic curves. 
Proposition (a) Let P and P' be conjugate points over a field F, then if one 
lies on a curve C defined over F, then so does the other. If one is non-singular, 
then so is the other. 
(b) Conjugate non-singular points of a curve C define the same order 
function. 
Proof (a) If C is defined by f(x, y) = 0 and P: (x = ex, y = /1) and 
P': (x = ex', y = P') are conjugate, then P lies on C implies f(ex, fJ) = 0 and 
that implies f(ex', P') = 0. Thus P' lies on C also. By the same argument 
iJffiJx(ex, fJ) = 0 if and only if iJfliJx(ex'fJ') = 0, and the same statement holds 
for iJf/iJy. Thus if P is non-singular, then so is P'. 
(b) Let (ex, fJ) and (ex', fJ') be conjugate points of C: f(x, y) = 0, defined 
over F and let E and E' be the fields F[ex, fJ] and F[ex', fJ']. Every element 
of E can be written as g(a, fJ) where g is some polynomial in F[x, y]. 
Furthermore, by definition g(ex, fJ) = 0 if and only if g(ex', fJ') = 0. Hence we 
can define a map a from E to E' by taking g(a, fJ) to g(ex', /1'). This map is an 
isomorphism: it is bijective and preserves addition and multiplication. 
Furthermore, it leaves elements of F unchanged and so the equation 
f(x, y) = 0 remains unchanged. Hence if h(x, y) E E[x, y] has x-order n at 
(ex, fJ), ah(x, y) has x-order n at (ex', fJ'). If h E  F[x, y], then ah(x, y) = h(x, y). 
So, on F(C), (ex, /1) and (r.t.', fJ') define the same order. 
• 
23.2 The degree of a point 
To measure how much we must enlarge F to obtain the coordinates of 
P: (ex, fJ), we introduce the notion of the degree of a point. It emerges that, 
more importantly, the degree counts the number of conjugates of P. 
Definition 
Let F be a field and let P: (x = ex, y = fJ) be a point with 
coefficients in a finite extension field E. Let F[ex, fJ] be the extension field 
generated by the coordinates of P. Then the degree of P, denoted by d(P), 
is the dimension of F[ex, fJ] as a vector space over F. 

A survey of the theory of algebraic curves 
357 
Example 
To obtain the point P: (i, 2) of the real circle x2 + y2 = 3, we must 
enlarge R to C. So the degree of P is 2. It has two conjugates, P itself and 
P: ( - i, 2). 
Proposition (a) Conjugate points have the same degree. 
(b) If 
P(x = a:, y = p) is a point of 
degree n of 
the curve C defined 
over a 
finite field F. Then in any extension field of F[a:, fl], P has exactly n conjugates. 
Example 
For the curve x3 + y3 = 1 defined over GF(2) the points (I, 0), 
(10, 0), and (2', 5') given in Example 21.12 have degrees I, 2 and 3. 
The number of conjugates of each is the same as its degree. 
Remark 
If we drop the restriction that F is finite, then part (b) need no 
longer hold. In that case one can only say that P has at most n conjugates 
over any extension field. It is also possible to give a condition when there is 
an extension field in which P has the full quota of n conjugates. 
Proof (a) Let P': (a:', fl') be a conjugate of P. From Proposition 23. 1 we 
have seen that the map u taking g(a:, fl) to g(a:', fl') defines an isomorphism 
of F[a:, PJ onto F[a:', P'] fixing F. Thus the dimensions of these two fields 
as vector spaces over F must be equal. 
(b) We deal with the case that neither a: nor p lies in F itself. A similar 
but simpler argument proves the case when one of a:, p lines in F. Let y be 
a primitive element of F[a:, p] and, say a: =  yi, p = yi. The minimal poly­
nomial of y has degree n and exactly n roots y = y1, •
•
. , Yn in F[a:, /1]. For 
each Yk• the point (YL y[) is a conjugate of P. So P has at least n conjugates 
in F[a:, fl]. 
Now suppose P': (a:', fl') is conjugate to P in some extension field E of 
F[a:, fl]. The isomorphism u of part (a) maps y onto a primitive element y' 
of F[a:', P'] such that a:' = y'i and P' = y'i. But y' has the same minimal 
polynomial as y and the roots ofthat polynomial in E are precisely y1, •
•
•
 , y • .  
Thus y' = Yk for some k = I, . . .  , n, and P' is one of the conjugates we 
calculated for F[a:, p]. 
• 
23.3 Functions on a curve 
We now turn our attention to the behaviour of functions on a curve. We 
shall state, but not prove, two fundamental theorems, which are generaliza­
tions of important theorems of complex number theory. These state that all 
non-constant functions have zeros and poles, and furthermore they have the 
same number if they are counted correctly. 

358 
Error-correcting codes and finite fields 
Theorem 
The existence theorem Let C: f(x, y) = 0 be an affine curve over 
F and let g(x, y) be an irreducible polynomial in F[x, y] such that g(x, y) is 
not congruent to a constant modulo f(x, y). Then there exists a point 
P: (x = IX, y = p) with IX, p in some extension field of F, such that g(IX, p) = 0. 
The number of such points is finite. 
Furthermore, if C is smooth, and for h(x, y) E F[x, y], vp(h) ;;: vp(g) for all 
points P: (x = IX, y = p) with g(IX, p) = 0, then h(x, y) ix congruent to a multiple 
of g(x, y) modulo f(x, y). 
• 
Corollary Since there are irifinitely many irreducible polynomials in F[x, y], 
there are infinitely many points on any curve. Furthermore, diferent irreducible 
polynomials must be zero on non-conjugate points. So there are infinitely many 
non-conjugate points on a curve. 
• 
It also follows that different absolutely irreducible polynomials define 
different curves. As it can be shown that there are infinitely many absolutely 
irreducible polynomials, there are infinitely many different curves. 
23.4 Counting poles and zeros 
For most purposes conjugate points should not be distinguished. So we 
coagulate conjugate points into 'places'. As we can only define orders for 
non-singular points, we restrict our attention to these. 
Definition 
Let C be a curve and P a non-singular point of C; the set of 
conjugates of P is called a place. We abuse notation and denote the 
place of P by P also. As the degrees and order functions of conjugate points 
are the same, we can speak of the degree of order function of a place, and 
use the notation d(P) and vp(f) equally for places or their member points. 
With this concept we can formulate a grand extension of the theorem that 
the number of roots of a polynomial of degree n, counted with the correct 
multiplicity, is exactly n. For this theorem we assume that the curve we are 
dealing with is smooth, that is: all its points are non-singular. 
Theorem 
The degree theorem Let C:f(x, y) = 0 be a smooth curve defined 
over F, and let fjJ E F(C). Then vp(f/J) # 0 for finitely many places P, and 
summing over all places we have 
L vp(l/J)d(P) = 0. 
• 
Corollary Replacing each place P by its d(P) constituent points we see that 

A survey of the theory of algebraic curves 
if we sum over all non-singular points the sum becomes 
L Vp(<j)) = 0. 
359 
That states that if the poles and zeros of <P are counted with the correct order, 
then f has equally many poles and zeros. 
Example 
It is interesting to confirm what the theorem says for ordinary 
polynomials. Consider the curve y = 0. This straight line has as its function 
field ordinary rational functions in one indeterminate. It has points (a, 0) with 
a in some extension field and there is a single point at infinity (u = 0, v = 0). 
This has degree 1 (because its coordinates lie in the base field) and its order 
function assigns to each polynomial f(x) = /(1/u) the value - deg(f). 
Two finite places (a, 0) and (/3, 0) are conjugate if a and f3 have the same 
minimal polynomial. Thus finite places correspond to irreducible polynomials 
in F[x] and the degree of the place P corresponding to the irreducible p(x) 
is just deg(p). The order of a polynomial f(x) at this place is the power to 
which p(x) occurs in the (unique) factorization of f(x). 
As there are only finitely many irreducible factors of/the number of places 
where f has order > 0 is finite. Furthermore the sum 
L Vp(f)d(P) 
over these places is precisely the degree of f. If we add in the point at 
infinity, then V00{f) = - deg(f) and so the sum becomes 0. 
23.5 Specifying subspaces 
In defining a code based on a curve C we shall need to use linear subspaces 
of the field of functions F(C). The most natural way to specify such a space 
is to require its members to have specified orders at certain places. 
Example 
For the field F(x) the set of rational functions with order * -n 
at oo is the vector space of rational functions of degree o n. This is 
infinite-dimensional. 
If we also require the function fig to have no poles at any finite places, 
that means that no irreducible polynomial divides g. Thus g is a constant 
and we now have the set of polynomials of degree o n. This has dimension 
n + l .  
Suppose we relax the restriction and allow order * - 1 at the point x = 0. 
Now we have the set of functions of the form fix where f has degree o n + l .  
The dimension increases to n + 2. I f  we tighten the condition instead and 
require a zero at x = 0, we get the set of polynomials of the form xf 
with 
f of degree < n. The dimension drops to n. Of course, if we require the order 

360 
Error-correcting codes and finite fields 
to be at least 0 at all finite places and > 0  at oo we are asking for a function 
f/g with deg(g) > deg(f) and no poles. There are no such functions (except 
0,' which we give infinite order at all places) and the dimension is 0. 
On a straight line, this behaviour is very regular, but for general curves 
it is more subtle. To perform the corresponding calculations there we need 
a proposition that follows from the existence theorem. 
Proposition Let C: f(x, y) = 0 be a smooth curve and let g(x, y)jh(x, y) be a 
rational function with vp(g/h)  0 for all (x, y)-points P: (x = ex, y = P) of C 
(in other words all the poles of gjh are on the horizon of the (x, y)-system). 
Then in F( C), g(x, y)jh(x, y) = 
g0(X, y) for some go e F[x, y ]. 
Proof Suppose that h(x, y) is not congruent to a constant modulo f(x, y). 
Then we can extract an irreducible factor h0(X, y) from h, such that h0 is also 
not congruent to a constant modulo f. Say h(x, y) = ho(x, y)h'(x, y). For all 
(x, y)-points P of C, for which vpW) > 0, we have vp(g) = Vp(h) + Vp(g/h)  
vp(h). Thus, by the existence theorem, g(x, y) is congruent to h0(X, y)g'(x, y) 
modulo f(x, y). So we can replace gjh by g'jh' where h' has degree less than 
h. Repeating the process as often as required we can reduce the denominator 
to a constant. 
• 
Corollaries 
(1) A function in F(C) without any zeros on C: f(x, y) is a 
constant. 
(2) A function without any poles on C is also a constant. 
Proof (1) If gjh has no zeros at all, then by the proposition, gjh is congruent 
to a polynomial g0(X, y) modulo f(x, y). By the existence theorem, all the 
irreducible factors of go(x, y) are congruent to constants modulo f(x, y). 
Hence g0(X, y) is congruent to a product of constants modulo f(x, y). 
(2) If gjh has no poles, then h/g has no zeros. So by (1) it is a constant. 
• 
23.6 Applying Proposition 23.5 
Proposition 23.5 is very useful for determining the nature of functions with 
specified poles and zeros. 
Example 
Take C: x3 + y3 = 1 defined over GF(2) and consider the point 
P: (x = 0, y = 1). What is the dimension of the set of rational functions 
fig e F(C) with vp(ffg)  - n and vQ(f/g)  0 everywhere else? 
• 
If n = 0, then the space consists of the constants and the dimension is 1 .  
• 
If n = 1 ,  then the space still consists of constants. 

A survey of the theory of algebraic curves 
361 
You can see this as follows. A function satisfying our condition must be 
of the form fig with f(O, l) =F 0 and g(O, l) = 0. Now since vp(x) = l, and 
vQ(x) * 0 for all points Q: (x = a, y = p), xflg satisfies the conditions of 
Proposition 23.5. Thus xf/g is congruent to a polynomial h(x, y) in F(C) 
and our function takes the form h(x, y)/x. In the (u, v)-system this transforms 
to h(l/u, vfu)u. If h has degree > l, it has a prohibited pole at (u = 0, v = 1). 
So h must have degree .:s; l. Our function must be of the form (ax + by + c)fx. 
In order that this has no pole at (x = 0, y = 10), we must have lOb + c = 0, 
but in order that it has no pole at (x = 0, y = 1 1), we must have l 1b + c = 0. 
Hence b = c = 0 and our function is the constant a = axfx. 
Thus there are no non-constant functions of this type and the dimension 
remains 1. 
• 
If n = 2 we have a function xf(y + 1) = y2 + y + lfx2 which has a pole 
of order 2 at P. The function has zeros at (x = 0, y = 10) and (x = 0, 
y = 1 1  ). At all other points the order of this function is 0. So the 
dimension of our space for n = 2 is at least 2. 
• 
If n = 3 there is a further function 1/(y + 1) = y2 + y + lfx3• This has 
a pole of order 3 at P. It has no further zeros or poles in the (x, y)-system. 
In the (u, v)-system it transforms to v2u + vu2 + u3 which has zeros at 
(u = 0, v = 1 ), (u = 0, v = 1 0), and (u = 0, v = 1 1). The dimension of our 
space for n = 3 is at least 3. 
We shall shortly show that the dimensions of these spaces for n = 2, 
3 are 
exactly 2 and 3. 
23.7 Divisors 
If we want to make conditions at several points, we can assemble them into 
one global definition by introducing the concept of a divisor. We use places 
to ensure that conjugate points are treated equally. 
Definition 
A divisor D of a curve C assigns an integer value D(P) to every 
place of C. We require that D(P) =F 0 for only finitely many places. 
We shall use the notation D = L D(P)P to denote the divisor. Addition 
of divisors is defined term by term, and we say D .:s; E if D(P) .:s; E(P) for 
all P. If only one place P has a possibly non-zero value D(P) = n, we write 
D = nP. 
Let S be a set of places and let D be a divisor. The L-space L(D, S) is the 
set of elements q, E F(C) such that vp(l/J) + D(P) Ⱥ 0 for all P E S. If S is the 
set of all places of C it is omitted. 

362 
Error-correcting codes and finite fields 
Example 
In Example 23.5 we first considered the straight line y = 0. We 
calculated L(nP, S) where P = oo and S = { P}. It is the space of all rational 
functions of degree B n. On the other hand, we found that L(nP) is the space 
of polynomials of degree B n. 
In Example 23.6, we then considered the curve x3 + y3 = I. For P: (0, 1) 
we found that L(OP) = L(P) and that both were just the set of constants. 
We found non-trivial elements of L(2P) and L(3P). 
Theorem The L-space L(D, S) is a vector space over F. If S is finite, 
dim(L(D, S)) = oo, but dim(L(D)) is finite. 
23.8 A special case 
Although we cannot prove the Theorem 23.7 here, we shall prove a special 
case that will suffice for our calculations. 
Proposition Let C be a smooth curve defined over F, and let P be a point of 
C with coefficients in F. Let D be a divisor of the form nP with n ɴ 0. Suppose 
1 = ¢1, ¢2, •
•
•
 , tPk e L(D) with Vp(t/Jj) = - ni. Suppose that 
(a) 0 = n1 < n2 < · · · < nk Ŝ n, 
(b) Ifr/1 e L(D) then vp(rf;) = ndor somej = 1, . . . , k. 
Then ¢1, •
•
•
 , ¢dorms a basis of L(nP). 
Example 
Using the proposition, we can now complete the calculations of 
Example 23.6. Let C: x3 + y3 = 1 over GF(2) and P: (0, 1 ). We have shown 
that there are no functions rj; 
with v p( rj;) 
= - 1  and v Q( rj;) 
ć 0 for Q ::/= P. But 
we have given examples of functions with vp(rf;) B - 2  and vQ(r/1) 
ć 0 else­
where. Indeed, if we take the functions ¢ = xiyij(y + 1 )i+i, then as vp(x) = 1, 
vp(y) = 0, and vp(y + 1) = 3, we have vp(t/J) = -2i - 3j. Choosing appro­
priate non-negative values i and j will give every value strictly less than - 1 .  
We must check that the functions have no further poles. As (0, 1) is the 
only point in the (x, y)-system with y = l, ¢ has no further poles in that 
system. In the (u, v)-system ¢ has the form vij(v + u)i +i. As the curve has 
the (u, v)-equation u3 + v3 = 1, there are no poles in the (u, v)-system. The 
only point of the (w, z)-system we need to consider is (0, 0), but this does 
not lie on C. That proves that the only pole of ¢ is P, and thus 
¢ E L((2i + 3j)P). 
Applying the proposition to functions ¢ with vp(t/J) = 0, 2, . . .  , n, we find 
that L(nP) has dimension n for n Ⱥ I, but L(OP) has dimension 1. 
Proof For convenience we assume that P is the (x, y)-point (x = IX ,  y = p) 
with IX, P e F. By axiom DV2 (see Section 22.7), the order of a linear 

A survey of the theory of algebraic curves 
363 
combination of </J1, • • •  , <Pi at P is at least - ni while <Pi+ 1 has order < -ni. 
So <Pi+ 1 is not a linear combination of </11, .
•
.
 , <Pi· Hence the functions <Pi 
are linearly independent. 
To prove that they form a basis, we show that every function t/J e L(nP) 
is a linear combination of </11, • • .  , <Pk by induction on - vp(t/J). If Vp(t/J) ȹ 0, 
then since vQ(t/1) ȹ 0 everywhere else, t/J is a constant and so t/J = a</J1• 
Suppose the proposition is true for vp(t/J) > -ni and suppose we have 
tjJ E L(D) with vp(t/J) 
= - ni. Then vp(t/J/<P) = 0. So the value of t/1/<Pi at P is 
a non-zero constant a in F . .  Let x = t/1/<Pi - a. Then vp(x) > 0, and so 
vp(X</J) > -ni. But x<Pi = tjJ - a</Ji, and since tjJ and <Pi e  L(nP) it follows 
that Xt/Ji e L(nP). By the induction hypothesis, 
X =  a,</JI + · · · + ai- t<Pi- 1 ·  
So 
t/1 = a1</J1 + · · · + aj- t<Pi- 1  + a<fli· 
• 
23.9 Riemann's theorem and the genus 
Definition 
For a divisor L D(P)P, the dimension of L(D) is called the rank 
of D and denoted by l(D). The value L D(P)d(P), where d(P) is the degree 
of the place P, is called the degree of D and denoted by d(D). 
Calculating the value of /(D) is quite dificult, as you have seen. It does not 
behave as regularly as one might expect. For instance we found that 
l(P) = I = l(OP) for x3 + y3 = 1 and P = (0, 1). In general, behaviour 
becomes regular as the degree d(D) becomes large. Riemann's theorem, which 
is the most important result of the theory of algebraic curves, gives a general 
estimate /(D) in terms of a constant g, called the genus of the curve. Using 
this theorem greatly simplifies the calculations required to determine L(D). 
Theorem 
Riemann's theorem Let C be an algebraic curve over F. 
(a) Let A and B be divisors on C with A o B (that is A(P) o B(P) for all 
points P of C). Then 
/(B) - d(B) .o /(A) - d(A). 
(b) There exists a non-negative number g such that for all divisors D with 
d(D) > 2g - 2. I( D) 
- d(D) = 1 - g. 
• 
Definition 
The number g satisfying Riemann's theorem is called the genus 
of the curve C. 

364 
Error-correcting codes and finite fields 
The following two corollaries are immediate consequences ofthe theorem. 
Corollaries 
(1) The genus of C is unique. 
(2) For all divisors D on C, l(D) - d(D)  1 - g. 
Example 
Example 23.6 corroborates (but does not prove) the fact that the 
straight line y = 0 has genus 0. Any quadratic curve has a function field 
isomorphic to ordinary rational functions. It follows that quadratic curves 
also all have genus 0. 
From the fact that l(P) = 1 = d(P) for the point P: (0, 1) of the curve 
x3 + y3 = 1 over GF(2), it follows that this curve cannot have genus 0. The 
calculations in Example 23.8 strongly suggest that this curve has genus 1, 
and that is indeed the case. 
23.10 The Plucker formula for smooth plane curves 
The examples we have calculated hint at the idea that the genus of a cubic 
curve ought to be 1, and one might guess that may be there is a formula for 
the genus in terms of the degree of the defining equation. That is indeed the 
case for smooth curves. 
Theorem 
The Plucker formula 
Let C: f(x, y) = 0 be a smooth curve of 
degree n. Then the genus of C is (n - 1)(n - 2)/2. 
• 
As it stands, the formula is valid only for smooth plane curves. There are 
generalizations to other cases. As expected, the formula gives the genus of 
curves of degree 1 and 2 as 0, and the genus of a cubic curve as 1. 
In the next section we shall use the theorem to perform calculations for 
the Klein quartic and the curve x5 + y5 = 1 similar to those we performed 
for x3 + y3 
= 1. For completeness we first tabulate the results of Example 
23.8. 
Example 
Let C be the curve x3 + y3 
= 1 and P = (0, 1). The genus of C is 
1 and the values l(nP) are as follows: 
n 
0 
n > O  
l(nP) 
n 

A survey of the theory of algebraic curves 
365 
23.11 The Klein quartic 
Example 
For the Klein quartic, the equations in the three systems are 
x3y + y3 + x = 0, v3u + u3 + v = 0, w3z + z3 + w = 0. By the Plucker formula 
the curve has genus 3. We consider the quartic as a curve over GF(4). 
Choose as our base point P the point (x = 0, y = 0). We have 
ofloy(O, 0) = 0. So we must use the y-order. Thus Vp(y) = 1 and as 
x(x2y + 1) = y3, vp(x) = 3. 
Consider the function 4J = yijxi with 0 :::; 3i :::; 2j. This function has a pole 
of order 3j - i at P. Other (x, y)-points on C have x ::/= 0 and are not poles, 
but we must also consider (u, v)-points with u = 0 and the (w, z) point 
Q: (w = 0, z = 0). In the (u, v)-system yijxi becomes ui-ivi which has no pole 
because j  i. In the (w, z)-system it becomes wi-i;zi. Since vQ(w) 
= 3 and 
vQ(z) = 1, vQ(wi-;jzi) = 2j - 3i  0. So 4J has no further poles. 
The values vp(yijxi) = 3j - i, we can obtain for these functions are 0, 3, 
5, and all numbers > 5. So for n  5 we get n - 2 values. From Riemann's 
theorem it follows that for n  5 the functions l/xi with 0 :::; 3i :::; 2j form a 
basis of L(nP). 
Suppose n < 5. If 1/J has a pole only at P then 1/J certainly also lies in L(5P). 
Thus vp(r/J) = vp(yijxi) for some admissible values i,j. By Proposition 23.8 
it follows that appropriate functions 4J = l!xi still form a basis of L(nP). 
We can now write out a table of values for l(nP). 
n 
l(nP) 
23.12 A quintic 
0 
1 
2 
1 
3 
2 
n  4  
n - 2  
Example The curve x5 + y5 = 1 over GF(16). This curve has genus 6. 
We take as our base point P the point (x = 0, y = 1) As of/ox(O, 1) = 0, 
we use the x-order. Thus vp(x) = 1 and (y + 1)(y4 + y3 + y2 + y + 1) = x5, 
SO Vp(y + 1) = 5. 
The function 4J = xiyij(y + l)i+i has vp(4J) = -(4i + 5j) and no poles at 
other points of C. The values of n representable as 4i + 5j, with i,j  0, are 
0, 4, 5, 8, 9, 10 and all values  12. For n > 10 the number of such values 
is n - 5. 
By Riemann's theorem it follows that appropriate functions 4J = 
xiyi/(y + 1)i+i with vp(4J)  - n form a basis of L(nP) for n > lO. lt follows 
that if 1/1 e F(C) is a function with a pole only at P, then vp(r/J) = 4i + 5j for 

366 
Error-correcting codes and finite fields 
some i and j. Hence by Proposition 23.8, appropriate functions q, = 
xiyij(y + 1)i+ i form a basis of L(nP) for all n ;a: 0. The table below lists /(nP). 
n 
0 
l(nP) 
2 
3 
4 
2 
5 
3 
6 
3 
7 
3 
8 
4 
9 
5 
10 
6 
n >  lO 
n - 5  
Just as we proved the dimension formulas for x3 + y3 = 1 directly, it is 
possible to do so for Examples 23. 1 1  and 23.12 also, but it is much easier 
to apply Riemann's theorem. 
23.13 Summary 
We have now assembled the most important facts about curves. The points 
of a curve group together to form places. Their main properties such as 
degree (which, at least for finite fields counts the number of points in a place) 
and the order function depend only on the place. Even for finite fields a 
curve has infinitely many places and there are infinitely many curves. Every 
rational function on a curve has equally many poles and zeros if they are 
counted with the correct multiplicities. 
To combine conditions on the order of functions at various places on a 
curve we define divisors D and their associated spaces L(D). We explicitly 
calculated the spaces L(nP) for three curves and chosen points. The spaces 
L(D) 
are finite-dimensional. The dimension l(D) 
is estimated by Riemann's 
theorem, which relates it to the degree d(D). The difference /(D) - d(D) is 
always at least 1 - g, where g is the genus of the curve. The difference 
decreases as the divisor increases, and for divisors of degree greater than 
2g - 2, /(D) - d(D) = I - g always holds. Straight lines and quadratic curves 
have genus 0, for smooth curves of degree d the genus is given by the PlUcker 
formula (d - 1)(d - 2)/2. 
In the next chapter we shall use the spaces L(D) to construct Goppa's 
codes. 
23.14 Exercises 
23.1 
Why do the coordinates of conjugate points of an algebraic curve have 
the same minimal polynomials? 
23.2 
Let F be a field and ex, P be elements of a finite extension E. Suppose 
that the degrees of the minimum polynomials of ex and p are m and n 
respectively. Show that the elements cxipi, for 0 Ò i Ò m and 0 Òj Ò n, 
contain a basis of F[cx, p]. 

A survey of the theory of algebraic curves 
367 
23.3 Prove that if P: (ct, p) 
is a point of degree n over a finite field F, with 
ct 
E F, then P has exactly n conjugates in any extension field ofF [p]. 
23.4 Show that the conjugates of the point {ct, p) 
defined over a field of 
characteristic 2 are {ct, p), (ct2, p2), {c.t\ p4), 
•
•
•
• 
23.5 Let C be the x-axis defined by y = 0 over GF(l6). Denote the point 
(x = p, 
y = 0) by (p) 
and the point (u = 0, v = 0) by ( oo) taken over 
all non-zero values i, and let D = a(O) + b( oo ). Show that the functions 
xi with -a y i y b form a basis of L(D). 
23.6 Let C be as in Exercise 23.5, and let g(x) be an irreducible polynomial 
of degree d. Let P be the place of C associated with g (see Example 
23.4). Show that if D = aP + b( oo ), then the functions xigi with i ) 0, 
j ) -a, and dj + i y b, form a basis of L(D). 
23.7 Show that for any curve C and any place P, l(nP) 
B n + 1. 
23.8 What is the genus of the curve x 7 + y 7 = 1 over GF(2)? 
23.9 For the point P: (x = 0, y = 1) of x 7 + y 7 
= 1 calculate the values 
l(nP) for all n. 

24 
Geometric .Goppa codes 
We shall begin by defining dual Goppa codes, and then use these to define 
the Goppa codes themselves. In the process it will become clear that 
geometric Goppa codes are a generalization of classical Goppa codes. 
Together with the definitions we will produce estimates for the rank and 
minimum distance of the codes. As with previous classes we shall call these 
estimates the designed rank and minimum distance of the codes. 
24.1 Dual Goppa codes 
Definition 
Let F be a finite field and let C be a smooth algebraic curve 
defined over F. Let { P1, •
•
.
 , Pn} be a set of points of degree I (that is points 
? of C of the form (eti, {Ji) with eti, pi e F), and let B be the divisor that is 
the sum of these places, L ?· Further let D be divisor such that D(?) = 0 
for all j = I, . . .  , n. The dual Goppa code GD(B, D) is defined as the set of 
vectors (d1, 
•
•
• , dn) such that there exists a rational function cp e L(D) with 
di = ljJ(P). 
We need to choose the points í to be of degree 1, so that the values 
di lie in the field F. Points of degree I form places on their own, so the divisor 
B is correct as it stands. Since B determines the set of points ɥ we shall write 
ɥ e B to indicate that ɥ is one of the selected points. 
Example 
To avoid confusion, we shall use only the field GF(I6) and its 
subfield GF(4) in the examples of this chapter. You can tell if we are working 
over GF(4) by the fact that the values are restricted to 0, 1, 10, and I I. 
Consider the curve C: x3 + y3 = I defined over F = GF(4). In Section 21.8 
we found all the points of C over GF(4). They are the same as the points 
over GF(I6). There are 9 points, which we number as follows: 
0: (x = 0, y = 1 ), 1: (x = 0, y = 10), 2: (x = 0, y = 1 1), 
3: (x = 1, y = 0), 4: (x = 10, y = 0), 5: (x = 1 1, y = 0), 
6: (u = 0, v = 1), 7: (u = 0, v = 10), 8: (u = 0, v = 1 1 ). 
We take as B the sum of the points 1 to 8 and D as a multiple aP0• In 
Example 23.8 we showed that the functions xiyif(y + l)i+i, have poles of 

Geometric Goppa codes 
369 
order -(2i + 3j) at P0 and belong to L(D) if 2i + 3j :o:; a. We also showed 
that choosing one such function of each order gives a basis of L(D ). 
The table below shows the values at 11 for a choice of such functions of 
orders down to - 7. Notice that there is no function of order - 1. 
Function 
Order 
(x, y) points 
(u, v) points 
2 3 
4 
5 
6 
7 
8 
1 
0 
1 
1 
1 
1 
1 
1 
1 
xf(y + 1) 
-2 
0 
0 
1 
10 1 1  
1 
1 1 
10 
yf(y + 1 )  
- 3  
1 1  10 0 
0 
0 
1 
1 
1 
x2j(y + 1)2 
-4 
0 
0 
l 
1 1  10 
10 1 1  
xyf(y + 1)2 
- 5  
0 
0 0 
0 
0 
1 1  10 
x3/(y + W 
-6 
0 
0 
1 
1 
1 
1 
1 
x2yj(y + 1)3 
- 7  
0 
0 0 
0 
0 
10 1 1  
Transposing the entries in the rows up to order - a  will give a generator 
matrix for the code GD(B, aP0). Thus for a =  0 or a =  1 we get just the 
eightfold repetition code. For 1 < a <  8 the code has dimension a as can be 
easily checked. It is also an easy exercise to find in each such code a code 
word of weight 8 - a. 
If you try to perform these calculations, you may wish to begin by 
following the calculation of generator and check matrices for the codes that 
will be presented in Example 24.5. 
We can summarize the parameters of the codes in the following table. 
a 
Rank 
Min. dist. 
1 2 3 4 
5 6 7 
1 2 3 4 
5 6 7 
8 6 5 4 3 2 2 
24.2 Parameters of dual Goppa codes 
Proposition Let C be an algebraic curve over the field F and let G be a 
geometric Goppa code GG(B, D) defined over C with d(D) = a. Then for the 
block length n, rank m and minimum distance d of G, 
(a) n = d(B), 
(b) m = /(D) - l(D - B), 
(c) d  n - a. 

370 
Error-correcting codes and finite fields 
Proof Statement (a) is obvious. Statement (b) is a direct consequence of 
the rank and nullity theorem. The map taking a function ¢ in L(D) to its 
sequence of values on the points of B is linear. The map takes ¢ to the all 
zero sequence if and only if ¢(P) = 0 for all points of P of B, which holds 
if and only if ¢ e L(- B). Since B and D are disjoint ¢ e L(D) and ¢ e L( - B) 
is equivalent to ¢ e L(D - B). 
Statement (c) follows from the degree theorem (Theorem 23.4), which 
implies that a non-zero function has the same number of zeros and poles, 
when they are counted with correct multiplicities. Let ¢ be a non-zero 
function of L(D ) and suppose that ¢ has b zeros among the points of B. 
Then by the degree theorem, 
0 = L 
Vp(cp)d(P) + L va(¢)d(Q) 
PeB 
Q¢B 
But for all points P, vp(¢) + D(P}  0, and for the points P e B, D(P) = 0, 
hence ¢ has no poles at points of B, and the first sum above has value at 
least equal to b. Hence 
0  b + L va(¢}d(Q}  b -L D(Q)d(Q) 
Q¢B 
Q¢B 
As D(P) = 0 for all points of B, 
L 
D(Q}d(Q) = L 
D(P)d(P) + L D(Q)d(Q) = d(D} = a. 
Q¢B 
PeB 
Q¢B 
Hence 0  b - a, or b B a. So ¢ has at most a zeros in B and any non-zero 
code word has weight at least n - a. 
• 
The estimates of the proposition are most useful if d(D) < n. In that case 
d(D - B) < 0, so l(D - B) = 0. We can use Riemann's theorem to obtain a 
lower bound for the rank and obtain the following corollary. 
Corollary If d(D) = a < n = d(B), then GD(B, D) has rank at least a + 1 - g, 
where g is the genus of the underlying curve, and minimum distance at least 
n - a. 
• 
Remark 
When g = 0, the codes meet the Singleton bound (see Theorem 
1 8.4). As we shall see, the (full) classical Goppa codes are geometric Goppa 
codes for curves of genus 0, so this establishes that classical Goppa codes 
(and their subclass Reed-Solomon codes) meet the Singleton bound. 
24.3 Table for the Klein quartic 
Example 
In this example we shall calculate a table for the Klein quartic 
over GF(16) analogous to that of Example 24. 1 .  As shown in Example 21.9, 

Geometric Goppa codes 
this curve has 17 points oer GF(16), which we number as follows: 
0: (0, 0), 1: (u = 0, v = 0), 2: (w = 0, z = 0), 3: (10, 1 1), 4: (1 1, 10), 
5: (3, 10), 6: (5, 1 1), 7: (8, 10), 8: (15, 1 1), 
9: (10, 2), 10: (1 1, 4), 1 1: 10, 9), 12: (1 1, 14), 
1 3: (6, 8), 14: (13,1 5), 15: (7, 3), 16: (12, 5). 
(Points are (x, y)-points except for P1 and P2). 
371 
We take for D a multiple aP0 and for B the 16 other points. In Example 
23.1 1 we found that a basis for L(D) could be found by choosing functions 
ofthe form yijxi with 0 Ò 3i Ò 2j. These functions have order -(3j - i) at P0• 
Function 
Ord 
Values 
m 
d 
I 2 
3 
4 
5 
6 
7 
8 
9 1 0  1 1  12 1 3  14 1 5  1 6  
I 
0 
I I 
I 
I 
I 
I 
I 
I 
1 
I 
I 
I 
I 
I 
1 
I 
I 
1 6  
1/x 
- 3  
0 0 1 1
10 
8 1 5  3 
5 1 1
10 1 1
10 
4 
9 14 
2 
2 
13 
y/x2 
- 5  
0 0 
I 
I 
2 
4 
9 14 1 3  
7 1 2  
6 
7 1 2  
6 1 3 
3 
1 1  
l/x2 
- 6  
0 0 1 0 I I  1 5  
3 
5 
8 10 1 1  1 0  1 1  
9 14 
2 
4 
4 
10 
y2/xl 
- 7  
0 1 10 1 1  1 2  
6 1 3  
7 
4 
9 1 4  
2 
3 
5 
8 1 5  
5 
9 
y/x3 
- 8  
0 0 1 1  1 0  9 14 
2 
4 
2 
4 
9 1 4  
5 
8 1 5  
3 
6 
8 
l/x3 
- 9 
0 0 
1 
1 
5 
8 1 5  3 
I 
1 
I 
1 1 5  
3 
5 
8 
7 
7 
y2fx4 
- 10 
0 0 
I 
I 
4 
9 14 
2 
7 1 2  
6 1 3  1 2  6 1 3  
7 
8 
6 
y/x4 
- I I  
0 0 10 I I  
7 1 2  
6 1 3  1 5  
3 
5 
8 1 3 
7 1 2  
6 
9 
5 
l/x4 
- 1 2 
0 0 1 1
10 
3 
5 
8 1 5  1 1  10 I I  10 14 
2 
4 
9 
1 0  
4 
Y2fx' 
- 1 3 
0 0 I I  10 I I  10 I I  1 0  3 
5 
8 1 5  
2 
4 
9 14 
I I  
3 
y/x5 
- 14 
0 0 
I 
I 10 I I  10 I I  1 3  
7 1 2  
6 
6 1 3  
7 1 2  
1 2 
2 
l/x5 
- 1 5 
0 0 10
1 1 
I 
1 
I 
I 10 1 1  10 1 1  10 I I  10 I I  
1 3  
yz;x• 
- 16 
0 0 10 I I  14 
2 
4 
9 
4 
9 14 
2 
8 1 5  3 
5 
14 
yfx• 
- 1 7 
0 0 I I  1 0  
6 1 3 
7 1 2  
2 
4 
9 1 4  
1 
I 
I 
1 
1 4  
1/x6 
- 18 
0 0 
I 
1 
8 1 5  3 
5 
I 
1 
I 
I 
3 
5 
8 1 5  
1 5  
The transpose of the rows of order up to - a  gives a generator matrix for 
GD(B, aP0). The entries in the m and d columns show the true rank and 
minimum distance of the codes. Notice that as a increases from 16 to 17, 
the rank of the code fails to increase. At that stage /( 17 P0 
- B) = 1. The rank 
values can be obtained by applying standard row operations to the matrix. 
The minimum distance values are found by searching for short code words. 
24.4 Primary Goppa codes 
As we have seen, the field of ordinary rational functions F(x) is the algebraic 
function field corresponding to the irreducible polynomial f(x, y) = y. Geo­
metrically, the curve in question is a straight line. However, for this case the 

372 
Error-correcting codes and finite fields 
definition of a dual Goppa code does not match the definition of the classical 
Goppa code GC(P, g), which we recall here from Section 19.2. 
Definition 
Let g(z) be a polynomial over F, and let P = { p,, . . .  , Pn } be 
a set of elements of F such that for i = l, . .
. , n, g(p;) ¥- 0. Then the 
(classical) Goppa code GC(P, g) can be defined as the set of words d E  P 
such that 
n 
d .  
s(z) = L. _;_ = 0 (modulo g(z)). 
J= l z - pj 
In classical terms, the values di are the residues of s(z) at the places z = Pi· 
By its construction s(z) has degree < 0, so V00(s) ) l. Furthermore, the fact 
that g(z) divides s(z) can be expressed by the fact that for a finite place Q 
which is a zero of g(z), vQ(s)  vQ(g). So if we let B be the divisor L Qi where 
Qi is the place z = pi, and we let D be the divisor with value D(Q) = vQ(g) 
for all finite places and D(ooJ = l. Then s(z) E L(B - D). So we could say that 
GC(P, D) consists of the sequences (d1o .
. . , d") where di is the residue of a 
rational function s E L(B - D). That is the way Goppa himself defined the 
codes, but using residues would require a further chapter of theoretical 
algebraic geometry. So let's see if we can remove the residues from the 
definition. 
With the divisors B and D as above, let ¢(z) E L(D). Then all the poles of 
l/>(z) lie among the zeros of g(z), and indeed ¢(z)s(z) has no poles outside 
the set p,, . . .  , Pn· Now in the classical theory, the sum of all the residues 
of a rational function is 0. The reader familiar with the classical theory of 
residues will realize that we should have D(oo) = - 1  to make the residue 
of ¢(z)s(z)dz at oo equal to 0, see Exercises 24.5-8. Thus we have 
L. djcfJ<Pj) = o. 
These considerations lead us to make the following definition of a 
(primary) geometric Goppa code defined over a finite field. 
Definition 
Let F 
be a finite field and let C be a smooth algebraic curve 
defined over F. 
Let {P1, •
•
•
 , P" } be a set of points of C degree l, and let B 
be the divisor that is the sum of these places, L. ?- Further let D be a divisor 
such that D(ѕ) = 0 for all j = 1, .
. . , n. The (primary) geometric Goppa code 
GG(B, D) is defined as the set of vectors (d1, •
•
•
 , d") such that for all 
¢ E L(D), L dj¢(č) = 0. 
Again B determines the set of points P; and we shall write P; e B to indicate 
that P; is one of the selected points. Also, if D is a non-negative multiple of 
a single rational point we shall call the code a one-point code. 

Geometric Goppa codes 
373 
This definition provides an easy method for producing a check matrix for 
a geometric Goppa code. 
Proposition 
If A is a generator matrix for the dual Goppa code GD(B, D), 
then AT is a check matrix for the geometric Goppa code GG(B, D). 
Proof By the definition of the codes, u is a code word of GG(B, D) if and 
only if v · u = 0 for all code words v of GD(B, D). Since the columns of A 
form a basis of GD(B, D), that will hold if and only if v · u = 0 for all columns 
of A, in other words, if and only if ATu = Q. 
• 
24.5 Generator and check matrices 
Ex(llp/e 
In this example we give generator and check matrices in standard 
form for the codes GD(B, aP0) and GG(B, aP0) derived from the function 
table in Example 24.1. The curve is x3 + y3 = 1, P is the point (x = 0, y = 1), 
and B is the sum of the eight other rational points over GF(4). The matrices 
in the first column are obtained by using row operations on appropriate 
rows of the following matrix extracted from the table: 
0 
0 
0 
10 1 1  
0 
10 
l l  10 0 
0 
0 
0 
0 
1 1  
10 
0 
0 0 
0 
0 
0 
0 
0 
0 0 
0 
0 
1 1  10 
0 
1 1  
1 0  1 1  
1 1  10 
10 1 1  
To obtain the matrices in the second column we use the formula of 
Proposition 3.1 1 . In order to make it easy to check the calculations I have 
not altered the order of the columns in reducing the matrix to standard form. 
This means that some columns are permuted (identically in both matrices). 
For the primary code GG(B, aP0), the matrix in the first column is a check 
matrix and the matrix in the second is a transposed generator. For the dual 
code GD(B, aP0) it is the other way round. The matrix in the first column 
is a transposed generator matrix and the matrix in the second is a check 
matrix. We also list the block length rank and minimum distance of both 
codes (the block length is always 8). 

374 
Error-correcting codes and finite fields 
a = 0, 1. GG: (n = 8, m = 7, d = 2); GO: (n = 8, m = 1, d = 8). 
0 0 0 0 0 0 
0 0 0 0 0 0 
0 0 1 0 0 0 0 
[1 1 1 1 1 1 1 1] 
1 0 0 0 0 0 0 
0 0 0 0 1 0 0 
0 0 0 0 0 0 
0 d 0 0 0 0 
a = 2. GG: (n = 8, m = 6, d = 2); GO: (n = 8, m = 2, d = 6). 
0 0 0 0 0 0 
11 0 10 1 0 0 0 0 
[® 0 11 10 0 10 1 1] 10 0 11 0 0 0 0 
0 10 11 11 10 
0 0 1 0 0 0 0 
10 0 11 0 0 0 1 0 
11 0 10 0 0 0 0 
a = 3. GG: (n = 8, m = 5, d = 3); GO: (n = 8, m = 3, d = 5). 
[3l 00 0 
11 1 10 OJ 
1 1  10 10 
1 1 1 0 0 0 0 0 
0 0 0 
10 11 11 10 
0 10 
0 11 
10 
1 0 0 1 0 0 
0 11 0 0 0 1 
0 
0 11 10 0 0 0 0 
a = 4. GG: (n = 8, m = 4, d = 4); GO: (n = 8, m = 4, d = 4). 
[5 ­ ¬ ¯ 
1¬ 1 1: 5] [ ·3 11 
I 0 0 1 0 
o o o
¯1l 
0 1 0 0 11 10 1 
11 10 
0 0 0 1 
0 1 1 
1 1 0 0 0 
0 0 0 

Geometric Goppa codes 
375 
a = 5. GG: (n = 8, m = 3, d = 5); GO: (n = 8, m = 5, d = 3). 
0
0
0
10
0
 0
1 1 
0 0 
0 
0 
0 
10 10 
0 0 1 1  0 
0 0 0 
1 
0 
0 0 0 0 
0 
1 1  
1 
1 1  10 
[ 1« 1 1  
10 
0 1ª 
0 ©
1] 
1 1  1 1  10 
1 
0 
10 0 
a = 6. GG: (n = 8, m = 2, d = 6); GO: (n = 8, m = 6, d = 2). 
0 0 0 0 0 
10 1 1  
0 0 
0 0 0 
1 1  10 
0 
0 0 0 0 
10 1 1  
0 0 0 
0 0 
0 
0 0 0 0 0 
0 0 0 0 
0 
1 1  10 
0 
[10 10 
1 1  0 
1 
1 1  1 
0
1] 
1 1
1 1
10
1
0
10
0
 
a = 7. GG: (n = 8, m = 1, d = 8); GO: (n = 8, m = 7, d = 2). 
0 0 0 0 0 0 
0 
0 
0 0 0 0 
0 
0 0 0 0 0 
0 0 0 
0 0 0 
0 0 0 0 0 
0 
0 0 0 0 
0 0 
0 0 0 0 0 0 
[I 
1 
I 
I 
I 
1 
I 
I ]  
I t  i s  striking that the parameters of the dual codes are just those of the 
primary codes in reverse order. Indeed (as you are asked to show in Exercise 
24.3), the primary code for a and the dual code for 8 - a are identical. We 
shall return to this topic in the Extras. 
24.6 Parameters of Goppa codes 
Proposition 
Let C be an algel>raic curve of genus g over the field F and let 
G be a geometric Goppa code GG(B, D) defined over C with d(D) = a. 

376 
Error-correcting codes and finite fields 
Assume that 2g - 2 < a. Then for the block length n, dimension m and 
minimum distance d of G, 
(a) 
n = d(B), 
(b) 
m = n - a + g - l + l(D - B). 
(c) 
d * a - (2g - 2). 
Example 
The table below gives the parameters for the primary Goppa 
codes calculated in Example 24.5: 
a 
Rank 
Min. dist. 
l 
2 
3 
4 5 
6 
7 
7 
6 
5 
4 
3 
2 
l 
2 
2 
3 
4 5 
6 
8 
The next table gives the parameters for primary and dual Goppa codes, 
GG(B, aP0) and GD(B, aP0) based on the Klein quartic. 
a 
0 
3 
5 
6 
7 
8 
9 
10 
I I  
1 2  
1 3  
14 
15 
16 
17 
18 
rank G D  
I 
2 
3 
4 
5 
6 
7 
8 
9 
10 
1 1  
1 2  
1 3  
1 4  
14 
15 
Min. dist. GO 
16 
1 3  
1 1 
10 
9 
8 
7 
6 
5 
4 
3 
2 
I 
I 
I 
I 
Rank GG 
1 5  
14 
1 3 
1 2  
I I  
10 9 
8 
7 
6 
5 
4 
3 
2 
2 
1 
Min. dist. GG 
2 
2 
2 
2 
3 
4 
5 
6 
7 
8 
9 
10 
l 1  
l 3  
l 3  1 6  
Proof (a) B = 2: 
lj and d(lj) = 1, by hypothesis, so d(B) = n, and by 
definition the block length of GG(B, D) is the number of places in P. 
(b) Let A be a generator matrix for the dual Goppa code GD(B, D). Then 
by Proposition 24.2, AT has rank l(D) - l(D - B). Thus by the rank and 
nullity theorem, C has dimension n - l(D) - l(D - B). As d(D) > 2g - 2, 
Riemann's theorem tells us that /(D) = a + 1 - g. 
(c) Suppose that d is a non-zero code word of C and arrange the places 
Pi so that di # 0 for j = 1, . . . , k, and di = 0 for j > k. Put Bi = Lf= 1 lj. 
We shall show that the assumption that 1 y k < a - (2g - 2) leads to a 
contradiction. For in that case we have d(D - Bk) > 2g - 2, and so also 
d(D - Bk _ 1) > 2g - 2. 
Hence 
by 
Riemann's 
theorem 
l(D - Bk) = 
r - k + 1 - g, 
and 
l(D - Bk_ 1) = r - k + 2 - g. 
Thus 
there 
exists 
c/J E L(D - Bk -:- 1), cjJ f/ L(D - Bd. That implies that c/J(lj) = 0 for j = 
1, . . . , k - 1, and c/J(Pk) # 0. As (D - Bk _ 1) y D, cjJ E L(D) and 2: dic/J(lj) = 
dkc/J(Pk) # 0, contradicting the assumption that d E C. 
• 
The relation between the conditions on d(D) for primary and dual 
geometric Goppa codes is discussed further in the Extras. For the moment 
we just note that if l(D - B) = d(D - B) + 1 - g, then the rank m of the 
code reduces to 0, so a must be chosen at most equal to n + 2g - 2. In order 

Geometric Goppa codes 
377 
for the estimate of (b) to provide information without the need to calculate 
l(D - B), we must also have a <  n + g - l .  
EXTRAS 
24.7 Primary and dual Goppa codes are the same 
I will now reveal the fact, suggested by Example 24.5, that the distinction 
between dual and primary Goppa codes is spurious. 
Theorem 
There exists a divisor K of degree 2g - 2, such that GG(B, D) = 
GD(B, K + B - D). 
• 
The proof of this theorem is an easy consequence of the Riemann-Roch 
refinement of Riemann's theorem. 
You should note, however, that the transformation does not always 
preserve the property of being a one-point code. So one-point codes may 
not be the same as dual one-point codes. In our examples the one-point 
codes for x3 + y3 = 1 are the same as the dual one-point codes, but for the 
Klein quartic the parameters of the two classes do not agree. 
To show that the statement of the theorem is reasonable, we compare the 
parameters of the two codes. If d(B) = n, and d(D) = r > 2g - 2, then 
d(K + B - D) = 2g - 2 + n - r < n. 
So GD(B, K + B - D) has block length n, 
rank m  2g - 2 + n - r + 1 - g = n - r + g - 1 
and minimum distance 
d  n - 2g + 2 - n + r = r - 2g + 2. 
These are the same as the estimates for the rank and minimum distance of 
GG(B, D). 
24.8 Summary 
In this chapter we have introduced primary and dual geometric Goppa codes, 
defined over an algebraic curve. We calculated their parameters and in the 
extras we discussed the fact that the two classes of codes are in reality 
identical. 

378 
Error-correcting codes and finite fields 
µ.9 Exercises 
24. 1 
Calculate the minimum distance of the dual Goppa codes GD(B, nP) 
based on x3 + y3 = 1 directly and check their ranks. 
24.2 Construct dual Goppa codes GD(B, nP) of block length 25 based on 
x5 + y5 = 1. Calculate their parameters. 
24.3 Prove that for x3 + y3 
= 1, the primary and dual Goppa codes of the 
text are pairwise identical. 
24.4 
Construct generator matrices for the primary Goppa codes cor­
responding to the codes of Exercise 24.2. 
24.5 
Let C be the x-axis defined by y = 0 defined over GF(16). Denote the 
point (x = p, y = 0) by (p) and the point (u = 0, v = 0) by ( oo ). Let 
B be the divisor L (i), where the sum is taken over all non-zero values 
i, and let D = 6(0)
- (oo). Use functions 1/x, ljx2, •
•
•
 , 1/x6 (which are 
a basis of L(D) by Exercise 23.5) to produce a check matrix for 
GG(B, D). Show that if the columns of this matrix are arranged 
correctly, then the matrix is the check matrix Z. 3 of BCH(4, 3). 
24.6 
Prove that using C as in Exercise 24.5, all BCH and Reed-Solomon 
codes can be represented as geometric Goppa codes. 
24.7 
With C as in Exercise 24.5, let B' be the divisor L (i), where the sum 
is over all i. Furthermore, let Q be the place defined by the irreducible 
polynomial g(z) = z3 + z + l. Let D' be the divisor Q - ( oo) and D" 
be the divisor 2Q - ( oo ). Using the functions obtained in Exercise 23.6 
construct check matrices for GG(B', D') and GG(B', D"). Verify that 
these are check matrices of the classical Goppa codes GC(B', g) and 
GC(B', g2) of Section 19.6 (they are almost the same as the ones 
constructed in Section 19. 7). 
24.8 
Prove that using C as in Exercise 24.5, all classical Goppa codes can 
be obtained as geometrical Goppa codes. 

25 
An error processor for geometric 
Goppa Codes 
The theoretical properties of a class of codes only bear fruit if there exists a 
practical error-processing scheme that exploits them. So we shall describe 
a correction algorithm for geometric Goppa codes due to Skorobogatov and 
Vladut (1988) (based on ideas of Justesen). This algorithm requires the 
solution of two large systems of linear equations. It does not correct to the 
full capability of the code, but falls short by an amount equal to the genus 
of the underlying curve. That is a genuine problem, because for curves of 
genus 0, geometric Goppa codes are the same as classical codes. On the 
other hand, as we shall show in the Extras, the existence of good families of 
geometric Goppa codes depends on the existence of curves with large 
numbers of rational points. To find such curves it is necessary to consider 
curves of large genus. Nevertheless, we shall show that the algorithm can be 
used to devise coding schemes that are more powerful than those based on 
Reed-Solomon codes, and sometimes exceed the Gilbert-Varshamov bound. 
25.1 Conditions for the error-processing algorithm 
The definition of geometric Goppa codes provides us with a large collection 
of syndromes that we can use as starting points for error correction. The 
algorithm of Skorobogatov and Vladut makes a clever selection of these. It 
uses a subsidiary divisor F to split the syndrome equations in such a way, 
that one obtains a two-stage solution process. We describe the algorithm in 
precise detail below. We assume that 2g - 2 < d(D) < d(B) + g - I. That 
ensures that our code has the parameters given by Proposition 24.6. 
The subsidiary divisor F we require for the error processor must satisfy 
F(P) = 0 for P E B. The number of errors t that the processor can correct 
depends on the choice of F. To be precise, t must satisfy the following 
inequalities: 
I. d(F) < d(D) - (2g - 2) - t. 
2. 
l(F) > t. 
Recall that the designed distance of the code is 
d(C) = d(D) - (2g - 2). 

380 
Error-correcting codes and finite fields 
So if the second condition were d(F) ) t, the algorithm would correct as 
many errors as possible. Unfortunately, if g ) I, then /(F) y d(F) is possible 
and will certainly hold for d(F) > 2g - 2. In that case t will be less than the 
theoretical optimum. 
For one-point codes with D = aP, the auxiliary divisor can be chosen to 
have the form F = bP. The following easy proposition translates the 
conditions (1) and (2) into conditions on the numbers a and b. 
Proposition Let C = GG(B, D) be a one-point code with D = aP for a 
non-negative integer a with 2g - 2 < a Ò n + g - 1. 
(a) If b satisfies 
t + g Ò b Ò a - 2g - t +  1 .  
then F = bP satisfies the conditions for the algorithm to work. 
(b) Jf2t Ò a - 3g + 1, then there exists a b satisfying the inequality of part 
(a). 
For one-point codes with designed minimum distance d = a - 2g + 2, the 
algorithm corrects t errors for 2t + 1 y d - g. 
Example 
In presenting the decoding algorithm I shall use GG(B, 6P0) based 
on x3 + y3 = I. This curve has genus I, so to correct 2 errors we need 
3 y b y 6 - 2 - 2 + 1 .  Hence we can take F = 3P0• Notice that with the 
code GG(B, 5P0), which has minimum distance 5, we cannot find b as 
required. 
Proof (a) The right-hand inequality is just a direct translation of condition 
(I). For condition (2) we need only verify that l(bP) > t. But by Riemann's 
theorem 
l(bP)  b + 1 - g > t. 
(b) The inequality just states that t + g y a - 2g - t + 1. 
• 
25.2 The Skorobogatov-Vlldut error-processing algorithm 
Let c be a code word and let d = c + e, where e has weight y t. 
Example 
As already stated, we take as our code GG(B, 6P0). The divisor 

An error processor for geometric Goppa codes 
F is 3P0. We take c, d and e as follows: 
Algorithm 
c = O  0 tO 1 1  
10 1 1 1  
e = 1 0 l l  
0 0 
0 0 
0 
d = 1 0 
l l  
10 
l l  
381 
Step 0. 
(This step is performed once only for any given code.) Choose bases 
{rp l , . . .  , rf>u}, {t/11, . . . , tjl,} and {x1, . . .  , xd of L(D), L(F) and L(D - F) 
respectively. 
Note that t/I;Xi e L(D) for all i = 1, .
.
.
 , l, j = 1, .
.
.
 , k. 
Example 
For one-point codes we can always choose the functions t/li and 
X; to be equal to r/J; by arranging the initial basis suitably. In our case the 
functions rpi are naturally chosen to be 
1, xj(y + 1), yj(y + l), x2j(y + l)2, xyj(y + 1)2, x3j(y + 1)3• 
F and D - F are the same and of dimension 3. So we choose the functions 
t/1; and Xi to be the first three of those above: 
1, xj(y + 1), yj(y + 1). 
Step 1 .  Given a received word d define the syndromes of d as the values 
s;i(d) as follows: 
n 
s;j(d) = L: t/I;(P,)xif',.)d,., 
r =  I 
where the summation is over the points in B. If all syndromes are 0, STOP. 
Example 
The values s;id) are given in the table below. The fact that 
t/I;Xi = 1/JiXi and that apart from i =j = 3, the result is one of the functions 
rpi, makes the values easy to compute. 
su = 1 ·d = 1 + 0 + l + l l +  1 + 10 +  1 + 1 1 = 10 
s12 = r/>2 ·d = 0 + 0 + 1 + 1 + 1 1  + 10 + 1 + 10 = 10 
S!3 = rp3 ·d = 1 1 + 0 + 0 +  0 +  0 + 10 +  1 + 1 1 = 1 1 
s22 = r/>4 · d  = 0 + 0 + 1 + 10 + 10 + 10 + 10 + 10 = 1 1  
s23 = r/>5 ·d = 0 + 0 + 0 + 0 + 0 + 10 + l l  + 1 = 0 
s33 = rp{ ·d = 10 + 0 + 0 + 0 + 0 + 10 + l + 1 1  = 10 

382 
Error-correcting codes and finite fields 
Of course, as we know e, these syndromes could equally be calculated 
from e. Try using e to calculate the values and check that they are the same 
as the ones given here. 
Step 2. Find a non-zero solution of the set of k linear equations 
I 
L s;1{d)x; = 0, 
j = 1, .. . , k. 
i =  1 
Example 
The equation system is 
[ 
10 
l l  l l][
x1] [OJ 
1 1  
l l  
0 
x2 
= 
0 
1 1  
0 
10 
x3 
0 
A solution is x1 = 1 ,  x2 = 1, x3 = 10. 
Step 3. For the solution x found in Step, 2, let () = L x;r/1;. Let S s; B be 
the set of P; for which 8(P;) = 0. Assume for convenience that S = {P1, 
•
•
•
 , Pv}· 
Solve the equations 
v 
• 
L lPiP;>zi = L lPi<P;>d;, 
i =  1 
j =  1 
j = 1, . .. 
' u . 
Example 
The function 8 = l·lj)1 
+ l · t/>2 + 1 0 · lj)3 has values 
0
10
0
1 1
10
10
0
1 . 
So S = {(x = 0, y = 10), (x = l, y = 0), (u = 0, v = 10)}. 
The equation system is 
1 
1 
10 
0 
1 
1 1  
[:} 
1 1 
1 1  0 
1 
1 1  
0 
10 
1 1  
0 0 
1 1  
0 
0 
1 1  
The solution is z1 = 1, z2 = 1 1, z3 = 0. 
Step 4. 
Extend the solution of this set of equations by putting zi = 0 for 
j = v + 1, . . .  , n. Then e = (z1, •
•
• , z.). 

An error processor for geometric Gappa codes 
Example 
The algorithm correctly gives 
e = z1 0 z2 0 0 0 z3 0 = 1 0 
l l  0 0 0 0 0. 
25.3 Another code 
383 
Example 
Here is a second example of the algorithm. We use a code 
GG(B, aP0) based on the Klein quartic. The code has block length 16 and 
we shall use it to correct three errors. This will allow us to compare this 
code with RS(4, 3). As the curve has genus 3, the algorithm requires minimum 
distance 10 to correct three errors. From the table in Section 24.7 we see 
that we must use GG(B, 14P0). 
We select the code word c, error word e and received word d as follows: 
c = 9 
9 
15 4 4 10 6 2 
1 1  10 8 13 0 0 
1 
e = O  0 
0 0 0 
0 0 0 0 
0 
0 0 
0 3 2 
d = 9 
9 
15 4 4 10 6 2 
1 1  10 8 13 3 2 0. 
We number the functions of the table in Section 24.3 by their order. Then 
a basis of L(14P0) consists of </>0, </>3, </>5, •
•
•
 , </>14· 
<Po 
</>3 
<Ps 
</>6 
f/>1 
<Ps 
f/Jg 
f/>1o 
f/>1 1 
f/> 12 
</>13 
f/>14 
y 
y2 
y 
1 
y2 
y 
y2 
y 
X 
The auxiliary divisor F = bP0, where 3 + 3 ::; b ::; 14 - 3 - 6 + l .  So 
b = 6. The functions 1/1; = </>; for i = 0, 3, 5, 6. Similarly D-F 
= 8P0, so 
X; = f/J; for i = 0, 3, 5, 6, 7, 8. 
The syndromes are given in the following table. Again, all of them with 
the exception of s57 can be calculated as <Pi · d. 
iV 
0 
3 
5 
6 
0 
0 
5 
12 
1 1  
3 
5 
1 1  
5 
7 
5 
12 
5 
14 
14 
6 
1 1 
7 
14 
7 
7 
9 
14 
l l  
9 
8 
5 
14 
9 
12 

384 
Error-correcting codes and finite fields 
Our first set of equations is 
0 
5 12 
l l  
m+ 
0 
5 1 1  
5 
7 
0 
12 
5 
14 
14 
0 
l l  
7 
14 
7 
0 
9 14 l l  
9 
0 
5 14 
9 
12 
0 
A non-zero solution u = l l, x = 5, y = 6, z = l. 
The function 1 1t/J0 + 5t/J3 + 6t/J5 + t/16 has values 
l 1 
1 1 
1 1 
15 12 10 12 14 2 5 12 13 4 0 0 0. 
So S consists of the last three points of B, 
s 
= {(13, 14), (7, 3)(12, 5)} . 
Therefore the second set of equations is 
1 
0 
9 14 
2 
5 
12 
6 13 
12 
1 4  
2 
4 
l l 
5 
8 15 
[;] 
= 
9 
8 15 
3 
5 
3 
5 
8 
7 
6 13 
7 
14 
7 
12 
6 
14 
2 
4 
9 
7 
4 
9 14 
9 
13 
7 12 
12 
which are of course dependent. The-first three equations are already sufficient 
to determine x = 3, y = 2, z = l. But the rest confirm that this solution is 
valid. 
Thus we obtain e = 0 0 0 0 0 0 0 0 0 0 0 0 0 3 2 1. 
The rank of our code is 4, while the rank of RS(3, 4) is 9. The Goppa code 
is distinctly inferior to the Reed-Solomon code. Even with a full decoder we 
would have to take GG(B, 1 1P) which has rank 7. The reason for this is 
that for ease of calculation we have chosen a curve with very few points. As 
you will see in Example 25.5, the relative merits of the two types of codes 
are reversed when we take curves with larger numbers of rational points. 

An error processor for geometric Goppa codes 
385 
25.4 Why does it work? 
Theorem Assume that F and t satisfy the hypotheses of Proposition 1, then 
the decoding algorithm of Section 2 will correctly identify errors of weight at 
most t. 
Proof The idea behind the algorithm is to find an error locator function 
() in L(F), that is, a non-zero function such that O(Pj) = 0 if ei #- 0. Such a 
function will exist if I = l(F) > t, because then the conditions require us to 
solve t equations 
I 
L xit/Ji(Pj) = 0 
i =  I 
in the I unknowns xi (there is one equation for each j, for which ei #- 0). 
Thus the hypotheses for the correction algorithm allow us to state that 
an error locator exists, but the equations above cannot be used to find it, 
because we do not known the error word e. However, if x E L(D - F), then 
xO E L(D), so 
Hence 
• 
L: xO(P)ci = 0. 
j= I 
n 
n 
n 
n 
L: xO(í)di = L 
xO(í)ei = L 
x(í)O(í)ei = L 
x(í)O = 0. 
j = l 
j= l  
j= l 
j= l 
o 
= J X(í) itt xjt/Jj(P)di =itt (tt x(í)t/li(í)di) xi. 
(1) 
Allowing x to run through a basis of L(D - F) gives the equations solved 
in Step 2. 
We must also ensure that conversely any solution of these equations yields 
an error locator. Observe that equation ( l )  can be interpreted as stating that 
for any solution xѓ> . . . , x1 and 0 = L xit/Ji, the word (O(P1)e1, •
•
•
 , O(P.)e.) 
is a code word ofGG(B, D - F). Its weight cannot be greater than the weight 
of e which is assumed to be at most t. So if we ensure that the minimum 
distance of GG(B, D - F) > t, then the word (O(P1)eє> . . .  , O(P.)e.) must be 
Q. 
Then () will be an error locator. The designed minimum distance of 
GG(B, D - F) is d(D) - d(F) - 2g + 2, and our hypothesis is indeed that 
this is > t. That establishes that the equations of Step 2 have a non-zero 
solution, and that any non-zero solution yields an error locator. 
Any solution z of the equations of Step 3 gives a code word d - z of 

386 
Error-correcting codes and finite fields 
GG(B, D). By assumption z = e is one such solution, but could there be 
others? Two solutions give code words at distance at most v, where (as in the 
algorithm) v is the number of zeros of the error locator among P1, . . •  , P 
• .  
If we ensure that the number of zeros of our error locator is less than the 
minimum distance of GG(B, D), then there cannot be more than one solution. 
But the word (O(P1), • • .  , O(P.)) lies in GD(B, F) which has minimum distance 
) n - d(F). Hence v Ò d(F) < d(D) - 2g + 2, which is the designed distance 
of GG(B, D). That establishes the theorem. 
• 
25.5 Improving performance 
Example 
The example code based on the Klein quartic is clearly inferior 
to the Reed-Solomon code RS(4, 3). But the performance of geometric codes 
improves dramatically when the code is based on a curve with many rational 
points. To give an indication of this, consider the curve x5 + y5 = l over 
GF(l6) which has 65 rational points. Choose one of them, P, 
and consider 
the one-point code with B as the sum of the other 64 points and D = 37 P. 
By the Plucker formula, the curve has genus 6, and so this code has rank 
m ) 32 and minimum distance d ) 27. The SV algorithm can correct 10 
errors. 
For comparison we need a code defined over GF(l6). Consider the code 
RS(4, 4), which has rank m = 7 and minimum distance d =  9. We compare 
the error probabilities for transmission of a code word of the Goppa code 
and four code words of the Reed-Solomon code. This is biased in favour of 
the Reed-Solomon code, because it has a poorer rate and four code words 
of the Reed-Solomon code transmit only 28 message symbols, whereas a 
code word of the Goppa code transmits 32. 
The calculations follow the pattern of those in Chapter 2; we leave the 
details as an exercise for the reader (see Exercise 25.1). 
On a channel of error probability p = 0.001, the probability of an 
uncorrectable error occurring in at least one of four RS code words is 
approximately 2 X lQ-6• 
If the Goppa code is used with a hypothetical full correcting algorithm 
correcting 1 3  errors the probability of an uncorrectable error in a block is 
about 3 X 10- 1 5• 
Even using the SV algorithm, one can still correct lO errors. This gives 
an error probability of 5 x 10- 1 1• 
It is apparent that even with the SV algorithm the performance of the 
Goppa code is a significant improvement over that of the Reed-Solomon 
code. Reed-Solomon codes are in a sense optimal, but their block lengths 
are restricted by the available alphabet. If we repeat words (as here) or use 
a general BCH code to increase the block length the code's parameters 
become poor. By contrast, the advantages ofgeometric Goppa codes appear 

An error processor for geometric Goppa codes 
387 
with only moderately long block lengths, but there they are far superior to 
other known block codes, as we shall demonstrate. 
EXTRAS 
25.6 Geometric Goppa codes and the Gilbert-Varshamov bound 
Tsfasman et al. (1 982) gave an explicit description of a sequence of geometric 
Goppa codes over GF(p2) whose rate and relative minimum distance tend 
to the asymptotic Gilbert-Varshamov bound. Their construction uses the 
so-called Shimura modular curves, and requires deep algebraic geometry. 
There have been several other constructions of good sequences of geometric 
Goppa codes, none of them elementary. The main point in all these 
constructions is to find sequences of curves with large numbers of rational 
points in relation to their genus. That is a difficult and deep problem. So 
the design of highly efficient geometric Goppa codes is not easy. 
The curves of Tsfasman et at. (1982) are examples of the following 
proposition. 
Proposition For a finite field GF(p2) with p prime  7, there exists a family 
of smooth curves such that the number n of rational points on the curves tend 
to infinity but for the genus g, gfn tends to 1/(p - 1). 
• 
25.7 Towards a good family 
In order to exploit these curves to produce a good family of codes we must 
recall the asymptotic Gilbert-Varshamov bound. 
For all (j :::; (q - 1 )/q, there exists a sequence en of linear block codes over 
GF(q) with block length en = n, the relative minimum distance of en 
greater than (j - 1/n, and rate tending to I -Hq(f>). 
The construction exploits the fact that the formulae lead to simple 
expressions for the particular value i5 = (q - l )/(2q - 1). 
Proposition For x = (j = (q - 1)/(2q - I) the tangent to the curve y = Hq(x) 
is y = x + logq(2q - I) - I .  
Proof We shall omit the subscript q from the logarithms in the following 
equations. The value of Hq 
at i5 is given by the formula 
f> 
log(q - I) - b log(i5) - (1 - b) log(l - b). 

388 
Error-correcting codes and finite fields 
Substituting b = (q - l)/(2q - l) we obtain 
· b log(q - 1) - {J log(q - l) + {J log(2q - l) 
- (l - b) + (l - fJ) log(2q - l) = log(2q - l) + b - l .  
The derivative of Hq(x) at b is given by the formula 
log(q - l) - log(fJ) + log(l - b). 
Substituting for {J we obtain 
log(q - l) - log(q - l) + log(2q - l) + l - log(2q - l) =  l . 
• 
25.8 Eliminating logarithms 
The logarithms are inconvenient but can easily be got rid of. 
Proposition For q  49, and b = (q - 1)/(2q - 1), Hq(fJ) > {J + 1/(..jq
- 1). 
For q  361, Hq(b) > {J + 2/(.Jq - 1). 
Proof From Proposition 7, Hq(fJ) = fJ + logq(2q - 1) - 1. So we need only 
show that logq(2q - 1) - l > (..jq - 1). For q = 49 this follows immediately 
from the fact that log49(97) ::: 1.1755 > 7/6. As the right-hand side decreases 
with q and the left increases, the inequality remains in force for larger q. The 
argument for the second statement is entirely analogous. 
• 
25.9 Proof of goodness 
Theorem For q the square of the prime p ػ 7, there exist one-point codes 
over GF(q) with block length n tending to infinity such that for {J = 
(q - 1)/(2q - 1) the relative minimum distance of the codes tends to a limit 
> b and their rate to a limit > l 
- Hq( b). If p  1 9, then the statement holds, 
even if we replace the minimum distance d by d - g, where g is the genus of 
the underlying curve. 
The second statement says that, even with the reduced correction capability 
given by the SV error processor, these codes form a good family. 
Proof Consider the sequence of curves of Tsfasman et al. ( 1982) and choose 
one of their rational points P. 
Let B be the sum of the other rational points 
and let D = aP. Let the genus of the curve be g, then putting d(B) = n, we 
have (g - 1)/n -+ l/(p - 1). Provided that 2g - 2 < a  the rank m and 

An error processor for geometric Goppa codes 
minimum distance d of the code satisfy 
m ć n - a + y - 1 
d ć a - 2g + 2. 
389 
Thus m/n ć 1 - ajn + (g - 1)/n and d/n ६ ajn - 2(g - 1)/n. Choose a so 
that (a - 2(g - 1))/n ... b. Then the limit of d/n is at least {) and the limit of 
mjn is at least 
I - b - lim((y - 1)/n) = I - b - 1/(p - I ) ć  1 - Hq(b) 
by Proposition 25.8. Thus the codes meet the asymptotic Gilbert-Varshamov 
bound for this b. 
It we wish to use the SV error processor, then to have a code that behaves 
as though it had minimum distance d we must make the true minimum 
distance d + g. Thus (d + g)/n converges to b + lim(g/n) = b + 1/(p - 1). If 
we choose a to achieve this limit, then the limit of m/n is bounded below by 
1 - {) - 2/(p - I ). If p ć I 9 this is still at least 1 - Hq( b). 
• 
25.10 Summary 
We have described the Skorobogatov-Vladut error processor, giving examples 
for one-point codes, and showed that it can correct t errors in such codes if 
2t + I ::; d - g, where d is the designed minimum distance, and g is the genus 
of the underlying curve. We also showed that there exist explicit sequences 
of geometric Goppa codes that approach the asymptotic Gilbert-Varshamov 
bound. 
25.1 1 Exercises 
25.1 
Calculate the error probabilities of Example 25.5. 
The numbers involved are very close to 1, so you will need a high 
precision calculator (at least 16 decimal places). If you do not have 
such a calculator, replace the error probability by 0.005. This produces 
values that can be calculated with an 8-digit scientific calculator. 
25.2 
Use the Skorobogatov-Vladut error processor for the code GG(B, 14P0) 
based on the Klein quartic to correct 
9 2 15 4 6 10 6 2 
I 
1 2  10 
8 
13 0 0 I .  
25.3 
Let C be the x-axis defined by y = 0 over G£(1 6). Denote the 
point (x = fJ, 
y = 0) by (fJ) and the point (u = 0, v = 0) by ( oo ). Let B 
be the divisor L (i), where the sum is taken over all non-zero values 
i, let D = 6(0) :- ( oo ), and let F be the divisor 3(0). Show that F satisfies 
the conditions for the SV error processor for GG(B, D) = RS(4, 3). 

390 
Error-correcting codes and finite fields 
Use the functions l . lfx, 1fx2, •
.
•
 , 1fx3 as a basis of L(F), the functions 
1/x, . . .  , 1/x3 as a basis of L(D - F), and the functions 1/x, . . .  , 1fx6 
as a basis of L(D) and the SV error processor to correct the word 
d = 14 3 8 14 3 8 5 1 1  6 9 9 14 3 13 6. 
25.4 Compare the calculations of Exercise 25.3 with those of Exercise 17.10 
(which uses the PGZ error processor on the same word). 
25.5 
Prove that when applied to Reed-Solomon or BCH codes the SV 
error processor is the same as the PGZ error processor. 
25.12 Conclusion 
I hope that the reader who has persevered this far" will have learned enough 
algebra and coding theory to follow the literature. In particular, the theory 
of geometric Goppa codes is growing rapidly, and it has been my intention 
to enable my readers to keep abreast of developments. 
I will be particularly pleased if I have also managed to convey some of 
the power and beauty of the theory of fields and algebraic geometry. 

Bibliography 
Textbooks 
Birkholf, G. and MacLane, S. (1977). A survey of modern algebra (4th edn). 
Macmillan, New York. 
A classic text on algebra. Covers an enormous range including linear algebra and 
polynomials in one or two indeterminates in a clear matter-of-fact style. 
Blahut, R. ( 1983). The theory and practice of error control codes. Addison-Wesley, 
Reading, MA. 
An excellent technical text, giving detailed implementations of many error-process­
sing systems and discussing their advantages and disadvantages. 
Chevalley, C. ( 1 95 1 ). Introduction to the theory of algebraic functions of one variable. 
Math. Surv. VI. American Mathematical Society, Providence, Rl. 
Brilliant, densely written account of algebraic curves from a purely algebraic point 
of view. 
Cohn, P. M. ( 1 982). Algebra, Vol. 1, Wiley, New York. 
Treats linear algebra and polynomials in a single indeterminate in the context of 
a complete algebra course. More 'modern ' than Birkholf and Mac Lane, it does not 
hesitate to introduce and use the power ofabstract concepts. 
Conway, J. H. and Sloane, N. J. A. ( 1 988). Sphere packings, lattices and groups, 
Springer, New York. 
Davenport, H. ( 1952). The higher arithmetic. Hutchinson, London. 
A most elegant little book on number theory (out of print). 
Fulton, W. ( 1 969). Plane algebraic curves. Benjamin, New York. 
A readable geometric introduction with proofs. 
Hardy, G. H. and Wright, E. M. (1938). An Introduction to the theory of numbers. 
Oxford University Press, Oxford. 
Many later editions. A classic treasure house of number theory. 
Hill, R. ( 1986). A first course in coding theory. Oxford University Press, Oxford. 
Lint, J. H. van ( 1982). Introduction to coding theory. Springer, New York. 
Elegant, somewhat terse exposition for mathematicians. 
Lint, J. H. van and Geer, G. van der (1988). Introduction to coding theory and algebraic 
geometry. Birkhiiuser, Basel. 
Brief, concise introduction to general coding theory by van Lint, followed by a 
matching introduction to algebraic geometry by van der Geer. Gives a good overview 
of the theoretical background to geometric Goppa codes. 

392 
Bibliography 
McEliece, R. ( 1 977). The theory of information and coding. Addison-Wesley, Reading, 
MA. 
Two books for the price of one. A beautiful exposition of Shannon's theory 
(requiring some familiarity with probability theory), followed by a pellucid introduc­
tion to coding theory (requiring some knowledge of finite fields). 
MacWilliams, F. J. and Sloane, N. J. A. ( 1 977). Theory of error-correcting codes. 
North-Holland, Amsterdam. 
The bible of Coding Theory. Comprehensive up to its publication date. Clearly and 
comprehensibly written. Few routine exercises, but a bibliography of over 1 500 items. 
Noble, B. and Daniel, J. ( 1977). Applied linear algebra. Prentice-Hall, Englewood 
Cliffs, NJ. 
A comprehensive treatment from an applied point of view. 
O'Beirne, T. H. ( 1965). Puzzles and Paradoxes, Oxford University Press, Oxford. 
A highly entertaining book of articles originally published in the New Scientist. 
Surreptitiously covers much mathematics. Unfortunately out of print. 
Pless, V. ( 1 982). Introduction to the theory of error-correcting codes. Wiley, New York. 
Shafarevich, I. R. ( 1974). Basic algebraic geometry. Springer, New York. 
A readable general introduction. 
Strang, G. (1 980). Linear algebra and its applications. Academic Press, London. 
An excellent elementary introduction. 
Thompson, T. M. ( 1 983). From error-correcting codes through sphere packings to 
simple groups. Mathematical Association of America, Providence R.I. 
Other references 
Berlekamp, E. R. ( 1 965). On decoding binary Bose-Chaudhuri-Hocquenghem codes. 
IEEE Trans. Info. Theory, 11, 577-9. 
Bose, R. C. and Ray-Chaudhuri, D. K. ( 1 960). On a class of error correcting binary 
group codes. Info. and Control, 3, 68-79. 
Delsarte, P. and Goethals, J.-M. ( 1975). Unrestricted codes with the Golay para­
meters are unique. Discrete Math., 12, 21 1 -24. 
Eastman, W. ( 1 990). Inside Euclid's algorithm, coding and design theory, Part 1 ,  I M A 
Vol. Appl. Math., 20, 1 1 3-27. 
Golay, M. J. E. ( 1949). Notes on digital coding. IEEE, 37, 657. 
Goppa, V. D. ( 1970). A new class of linear error-correcting codes, Problems of Info. 
Transmission, 8(3), 207- 12. 
Goppa, V. D. ( 1 98 1 ). Codes on algebraic curves, Dokl. Akad. Nauk SSSR, 259, 
1 289-90 (translated: Soviet Math. Dokl., 24 ( 1 98 1 ), 1 70-2). 
Gorenstein, D. C. and Zierler, N. ( 1 96 1 ). A class of error-correcting codes in pm 
symbols, J. Soc. Indus. Applied Math., 9, 207- 14. 
Hamming, R. W. ( 1950). Error detecting and correcting codes, Bell Syst. Tech. J., 
29, 1 47-60. 
Herstein, I. N. (1987). A remark on finite fields. Amer. Math. Monthly, 94, 290-1. 

Bibliography 
393 
Hocquenghem, A. ( 1959). Codes correcteurs d'erreurs. ChijJres, 2, 147-56. 
Massey, J. L. ( 1 969). Shift register synthesis and BCH decoding. IEEE Trans. Info. 
Theory, 15, 1 22-7. 
Peterson, W. W. ( 1 960). Encoding and error-correction procedures for the Bose­
Chaudhuri codes. IEEE Trans. Info. Theory, 8, 60. 
Pless, V. ( 1 968). On the uniqueness of the Golay codes. J. Comb. Theory, 5, 2 1 5-28. 
Skorobogatov, A. N. and Vhldut, S. G. ( 1988). On the decoding of algebraic 
geometric codes. IEEE Trans. Info. Theory, 36, 105 1 -60. 
Snover, S. L. (1973). The uniqueness of the Nordstrom-Robinson and Golay binary 
codes. Ph.D. Thesis, Department of Mathematics, Michigan State University. 
Sugiyama, Y., Kasahara, M., Hirasawa, S., and Namekawa, T. ( 1 975). A method for 
solving key equation for decoding Goppa codes. Information and Control, 27, 
87-99. 
Tsfasman, M. A., Vliidut, S. G. and Zink, Th. ( 1982). On Goppa codes which are 
better than the Varshamov-Gilbert bound. Math. Nachr. 109, 2 1 -8. 
Zech, J. ( 1 849). Tafeln der Additions- und Subtraktionslogarithmen fur 7 Stellen, 
Weidman, Berlin. 

Index 
Abel, Niels Henrik, 104 
algebraic 
curve, 338 
element, 1 72 
algorithm 
error trapping for BCH(4, 3), 248 
Euclid's, 106 
Euclid's cross product theorem, 1 1 6 
Euclid's four-column version, 1 1 2 
Euclidean error process or for BCH codes, 
250 
Euclidean error processor for classici 
Goppa codes, 324 
Euclidean error processor for Reed-
Solomon codes. 275 
Goerzel, 1 70 
Horner, 1 70 
Peterson error processor for BCH codes, 
239 
Peterson-Gorenstein-Zierler error 
processor, 248 
SkorobogatovMVladut error processor for 
geometric Goppa codes, 380 
alphabet, 5 
binary, 5 
automorphism 
field, 1 57 
Frobenius, 153 
bad (family of codes), 294 
ball (of radius r ), 66 
basis (of vector space), 8 1  
bijective map, 1 57 
bit, 5 
block length, 1 3  
Bose, R .  C., 206 
bound 
Gilbert-Varshamov, 291 
Gilbert-Varshamov, asymptotic, 294 
Hamming, 289 
Plotkin, 299 
Singleton, 290 
Cauchy, A. L., 104 
channel 
binary, 9 
burst error, 8 
capacity, 24, 294 
discrete, 5 
random error, 8 
symmetric, 9 
characteristic of a field, 1 5 1  
check 
matrix, 39 
matrix for BCH code, 206 
matrix for classical Goppa code, 309 
matrix (reduced) of BCH code, 210 
parity 5 
polynomial for BCH code, 222 
code 
(n, m, d)-, 1 7  
(n, m)-, 1 3  
ASCII, 6 
BCH, double error-correcting, 204 
BCH, general, 305, 331 
BCH, t error-correcting, 206 
block, 1 3  
classical Goppa (full and subfield), 305 
cyclic, 70, 228 
dual, 46 
dual (geometric) Goppa, 368 
extended, 26 
extended Golay, 72 
general BCH, 305, 33 1 
Golay, 76 
good and bad families, 294 
Hamming, 63 
interleaved, 280 
linear, 33-4 
maximum distance separable (MDS), 290 
one-point (Goppa), 372 
perfect, 66, 290 
polynomial, 2'26 
primary (geometric) Goppa, 372 
punctured, 26 
Reed-Muller, 214  
Reed-Solomon, 267 
repetition, 6 
shortened, 26 
coefficient of polynomial, 1 9 1  
complement of a binary word, 70 
constant polynomial, 191 
continued fractions, 1 1 8 
convergents of a continued fraction, 1 18 

Index 
395 
coordinate changes, projective, 336 
coset, 51 
table, 48 
curve 
affine, 338 
affine component, 338 
algebraic, 338 
coordinate ring of, 344 
function field of, 345 
genus of, 363 
Klein quartic, 340 
rational algebraic, 346 
Shimura modular, 387 
smooth algebraic, 352 
cyclic 
code, 70, 228 
shift of a word, 70 
decoder, 9 
multiplicative for BCH and cyclic codes, 
233 
degree 
of polynomial in one indeterminate, 193 
of polynomial in two indeterminates, 195 
partial, of polynomial in two indetermines, 
196 
denominator, 196 
derivative (formal) of a polynomial, 169 
dimension, 8 1  
of a code, 32 
discrete 
channel, 5 
valuation, 349 
distance 
axioms, 1 5  
function, 15 
Hamming, 1 5  
minimum, 1 7  
div (operation), 137, 1 40  
division with remainder, 
for polynomials, I 00, 193 
divisor 
of a curve, 361 
degree of, 363 
greatest common divisor = highest 
common factor, 109 
rank of, 363 
domain, 30 
Euclidean, 108 
integral, 30 
element 
generator of field extension, 187 
invertible, 123 
irreducible, 127 
primitive, 179 
encoder, 9 
linear, 33 
multiplicative for BCH and polynomial 
codes, 219 
multiplicative for Reed-Solomon codes, 270 
standard, 14 
systematic, 14 
systematic for BCH and polynomial 
codes, 224 
systematic for Reed-Solomon codes, 269 
entropy function, 293 
equation, fundamental for BCH 
error-processing, 246 
eratosthenes, sieve of, 127, 134 
error 
burst, 271 
burst, correction capability of 
Reed-Solomon code, 271, 279 
burst, length of, 271 
co-evaluator polynomial, BCH, 244 
co-evaluator polynomial, Reed-Solomon, 
274 
detector, 16 
evaluator polynomial, BCH, 244 
evaluator polynomial, classical Goppa 
code, 320 
evaluator polynomial, Reed-Solomon, 274 
locations, BCH, 234 
locations, classical Goppa codes, 320 
locations, Reed-Solomon, 272 
locator polynomial, BCH, 243 
locator polynomial, classical Goppa code, 
320 
locator polynomial, Reed-Solomon, 274 
pattern, 53 
processor, 9 
processor, BCH, 250 
processor, classical Goppa code, 324 
processor, conditions for SV error 
processor, 379 
processor, perfect, 16 
processor, Peterson-Gorenstein-Zierler, 
PGZ, 248 
processor, Reed-Solomon codes, 275 
processor, Skorobogatov-VIadut for 
geometric Goppa code, 380 
trapping, 248 
value, Reed-Solomon code, 272 
weight, 15 
word, 1 3, 53 
Euclidean 
algorithm, 106 
algorithm, cross-product theorem, 1 16 
algorith, four-column version, 1 12 
domain, 108 
valuation, 109 
Euclidean domain, 108 
invertible element, 1 23 
irreducible element, 1 27 

396 
Euclidean domain (cont.) 
relatively prime elements, 125 
··Euler, L., 1 18 
formula for convergents to continued 
fraction, 1 19 
evaluation map, 167 
factor ring, 136 
failure modes 
of BCH error processor, 253 
of Reed-Solomon error processor, 278 
Fermat's theorem, 1 54 
field, 30 
binary, 27 
finite extension, 335 
of fractions, 196-7 
function field of algebraic curve, 345 
GF(8), 339 
GF(16), 102 
prime, 151 
fractions 
continued, 1 1 7  
equivalent, 197 
field of, 196--7 
Frobenius automorphism, 1 53 
function 
distance, 15 
elementary symmetric, 239 
entropy, 293 
rational, 196 
remainder, 140 
Galois, Evariste, 103 
generator 
of field extension, 187 
matrix, 35 
matrix for BCH coe, 220 
matrix for classical Gappa code, 310 
matrix for dual Goppa code, 371 
polynomial, 217 
genus 
of algebraic curve, 363 
Pliicker formula for genus of smooth 
plane curve, 364 
Golay code, 76 
good 
family of codes, 290 
Goppa, N. V., 303 
classical code, 305 
dual (geometric) code, 368 
primary (geometric) code, 372 
Gorenstein, D. Cǆ 206 
Hamming, R. W. 
bound, 289 
code, 63 
Index 
distance, 15 
Herstein, I. N., 1 76 
highest common factor (HCF) = greatest 
common divisor, 109 
Hirasawa, S., 303 
Hocquenghem, A., 206 
homomorphism, 168 
horizon, 337 
Homer's scheme, 1 70 
image of linear map, 85 
independence, linear, 80 
inequality, triangle, 15 
indeterminate, 191 
inverse 
calculation of, modulo a irreducible, 143-6 
invertible element of Euclidean domain, 123 
irreducible element of Euclidean domain, 127 
isomorphic fields, 1 57 
isomorphism of fields, 1 57 
Justesen, J., 379 
Kasahara, M., 303 
kernel of linear map, 85 
law 
associative, 29-30 
cancellation, 30 
commutative, 29-30 
distributive, 30 
line at infinity, 337 
linear 
· 
code, 31-2 
encoder, 33 
independence, 80 
map, 33, 85 
Liouville, J., 104 
locatio.o, 14 
logarithm 
discrete, 178, 180 
Zech, 184 
map 
linear, 85 
matrix, 79 
check, 39 
check, in standard or systematic form, 39 
full check matrix for BCH code, 206 
generator, 3 5 
generator, in standard or systematic form, 
35 
reduced check matrix of BCH code, 210 

Index 
397 
row echelon form, 89 
Vandermonde, 92 
minimum distance 
(designed) of BCH code, 206, 312 
of a general code, 1 7  
relative, o f  a code, 293 
mod (operation), 1 37, 140 
Namekawa, T., 303 
non-singular point, 352 
nullity of linear map, 85 
numerator, 196 
order 
of a field element, 1 8 1  
o f  a finite field, 1 52 
function of a point on an algebraic curve, 
350 
Pade approximant to a power series, 266 
perfect 
code, 66 
error processor, 16 
place 
of an algebraic curve, 358 
degree of, 358 
point 
conjugate points, 355 
degree of, 356 
non-singular, 352 
Poisson, 104 
pole of rational function, 348 
polynomal 
absolutely irreducible in two 
indeterminates, 336 
check for BCH code, 222 
check for cyclic code, 228 
code, 216 
error co-evaluator, 244 
error co-evaluator, Reed-Solomon, 274 
error evaluator, 244 
error evaluator for classical Goppa code, 
320 
error evaluator, Reed-Solomon, 274 
error locator, 243 
error locator, classical Goppa code, 320 
error locator, Reed-Solomon, 274 
generator for BCH code, 217 
generator for polynomial code, 226 
in one indeterminate, 191 
i n  two indeterminates, 194 
irreducible in one indeterminate, 101 
irreducible in two indeterminates, 334 
minimal, 172 
monic, 172 
primitive, 185 
root of, 167 
syndrome, 240 
syndrome, Reed-Solomon, 273 
zero of, 167 
power sum, 239 
prime 
field, 1 5 1  
relatively prime elements, 125 
primitive 
element, 1 79 
polynomial, 1 85 
projective coordinate changes, 336 
rank, 8 1  
o f  a code, 14, 32 
column, of matrix, 87 
of divisor, 363 
of linear map, 85 
row, of matrix, 87 
rate, 14 
rational 
algebraic curve, 346 
congruence modulo a polynomial, 306, 346 
function, 196 
function cancelled form, 306, 346 
functions, field of, 197 
Ray-Chauduri, D. K., 206 
relatively prime elements in Euclidean 
domain, 125 
Riemann's theorem, 363 
ring, 30 
commutative, 30 
coordinate ring of algebraic curve, 344 
factor ring, 136 
residue class ring Dja, 1 36 
residue class ring F[x, y]/f(x, y), 34 
root 
multiple, 1 70 
multiplicity of, 170 
of polynomial, 167 
rotation of a word, 70 
row 
echelon form matrix, 89 
operations, 89 
scalar, 80 
Shannon's theorem, 24 
Singleton bound, 290 
Skorobogatov, A. N., 379 
smooth algebraic curve, 352 
space 
L-space of a divisor, 361 
vector, 3 1, 80 
standard array, 48 
subcode, 201 

398 
Index 
subfield, 1 52 
subspace, 3 1 ,  80 
Sugiyama, Y., 303 
symbol 
check, 14 
message, 14 
syndrome, 56 
in BCH error processing, 234 
polynomial, BCH, 240 
polynomial, classical Goppa code, 307 
polynomial, Reed-Solomon, 273 
rational function, classical Goppa code, 307 
vector for BCH error processor, 236 
theorem 
BCH code is independent of field 
representation, 265 
classical Goppa codes form a good family, 
329 
cross-product theorem for Euclidean 
algorithm, 1 16 
degree theorem for rational functions on 
algebraic curves, 358 
dual and primary Goppa codes are the 
same, 377 
existence of points on algebraic curves, 358 
Fermat's, 1 54 
finite dimension of L-spaces, 362 
fundamental congruence for classical 
Goppa codes, 275 
fundamental equation for BCH codes, 246 
fundamental equation for Reed-Solomon 
codes, 275 
geometric Goppa codes are good, 388 
one-point codes are good w.r.t. their 
designed parameters, 388 
Pliicker formula, 364 
primitive element, 183 
rank and nullity, 86 
recognition of cyclic codes, 229 
recognition of polynomial codes, 227 
representability of I in a Euclidean 
domain, 1 26 
Riemann's, 363 
Shannon's, 24 
Skorobogatov-Vliidut error processor 
works correctly, 385 
unique factorization in Euclidean 
domains, 132 
uniqueness of error locator and evaluator 
for BCH codes, 256 
1-trick, 126, 128 
Tsfasman, M. A., 387 
Turyn construction of Golay codes, 69 
valuation 
discrete, 349 
Euclidean, I 09 
Vandermonde, A. T., 203 
matrix, 92, 203 
system of equations, 90 
vector, 79 
space, 3 1 ,  80 
subspace, 31, 80 
Vliidut, S. G., 379 
weight 
error, 1 5  
Hamming, 1 5  
word, 1 3  
code, 1 3  
entry, 1 4  
error, 53 
received, 15 
Zech, J., 184 
zero 
of polynomial, 167 
of rational runction, 348 
Zierler, N., 206 




 
 
 
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank


Published by
World Scientific Publishing Co. Pte. Ltd.
5 Toh Tuck Link, Singapore 596224
USA office:  27 Warren Street, Suite 401-402, Hackensack, NJ 07601
UK office:  57 Shelton Street, Covent Garden, London WC2H 9HE
Library of Congress Cataloging-in-Publication Data
Names: Ydri, Badis, author.
Title: Computational physics : an introduction to Monte Carlo simulations of matrix field theory / 
	
Badis Ydri (BM Annaba University, Algeria).
Description: Hackensack, NJ : World Scientific, [2016] | Includes bibliographical references and index.
Identifiers: LCCN 2016036155| ISBN 9789813200210 (hardcover ; alk. paper) | 
   ISBN 9813200219 (hardcover ; alk. paper)
Subjects: LCSH: Mathematical physics. | Physics--Data processing. | Monte Carlo method.
Classification: LCC QC20 .Y37 2016 | DDC 530.15/8282--dc23 
LC record available at https://lccn.loc.gov/2016036155
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library.
Copyright © 2017 by World Scientific Publishing Co. Pte. Ltd. 
All rights reserved. This book, or parts thereof, may not be reproduced in any form or by any means, electronic or 
mechanical, including photocopying, recording or any information storage and retrieval system now known or to 
be invented, without written permission from the publisher.
For photocopying of material in this volume, please pay a copying fee through the Copyright Clearance Center, 
Inc., 222 Rosewood Drive, Danvers, MA 01923, USA. In this case permission to photocopy is not required from 
the publisher.
Printed in Singapore

To my father for his continuous support throughout his life...
Saad Ydri
1943–2015
Also to my ...
Nour

 
 
 
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank

Preface
This book is divided into two parts. In the ﬁrst part we give an elementary in-
troduction to computational physics consisting of 21 simulations which originated
from a formal course of lectures and laboratory simulations delivered since 2010 to
physics students at Annaba University. The second part is much more advanced
and deals with the problem of how to set up working Monte Carlo simulations of
matrix ﬁeld theories which involve ﬁnite dimensional matrix regularizations of non-
commutative and fuzzy ﬁeld theories, fuzzy spaces and matrix geometry. The study
of matrix ﬁeld theory in its own right has also become very important to the proper
understanding of all noncommutative, fuzzy and matrix phenomena. The second
part, which consists of 9 simulations, was delivered informally to doctoral students
who are working on various problems in matrix ﬁeld theory. Sample codes as well
as sample key solutions are also provided for convenience and completeness.
vii

 
 
 
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank

Contents
Preface
vii
Introductory Remarks
xv
Introduction to Computational Physics
1
1.
Euler Algorithm
3
1.1
Euler Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
First Example and Sample Code
. . . . . . . . . . . . . . . . . . .
4
1.2.1
Radioactive Decay . . . . . . . . . . . . . . . . . . . . . . .
4
1.2.2
A Sample Fortran Code . . . . . . . . . . . . . . . . . . . .
6
1.3
More Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3.1
Air Resistance . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3.2
Projectile Motion
. . . . . . . . . . . . . . . . . . . . . . .
9
1.4
Periodic Motions and Euler–Cromer and Verlet Algorithms
. . . .
10
1.4.1
Harmonic Oscillator . . . . . . . . . . . . . . . . . . . . . .
10
1.4.2
Euler Algorithm . . . . . . . . . . . . . . . . . . . . . . . .
11
1.4.3
Euler–Cromer Algorithm . . . . . . . . . . . . . . . . . . .
12
1.4.4
Verlet Algorithm . . . . . . . . . . . . . . . . . . . . . . . .
13
1.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.6
Simulation 1: Euler Algorithm — Air Resistance . . . . . . . . . .
14
1.7
Simulation 2: Euler Algorithm — Projectile Motion
. . . . . . . .
15
1.8
Simulation 3: Euler, Euler–Cromer and Verlet Algorithms . . . . .
16
2.
Classical Numerical Integration
17
2.1
Rectangular Approximation . . . . . . . . . . . . . . . . . . . . . .
17
2.2
Trapezoidal Approximation
. . . . . . . . . . . . . . . . . . . . . .
18
2.3
Parabolic Approximation or Simpson’s Rule . . . . . . . . . . . . .
18
2.4
Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.5
Simulation 4: Numerical Integrals . . . . . . . . . . . . . . . . . . .
21
ix

x
An Introduction to Monte Carlo Simulations of Matrix Field Theory
3.
Newton–Raphson Algorithms and Interpolation
23
3.1
Bisection Algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . .
23
3.2
Newton–Raphson Algorithm . . . . . . . . . . . . . . . . . . . . . .
23
3.3
Hybrid Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
3.4
Lagrange Interpolation . . . . . . . . . . . . . . . . . . . . . . . . .
25
3.5
Cubic Spline Interpolation . . . . . . . . . . . . . . . . . . . . . . .
26
3.6
The Method of Least Squares . . . . . . . . . . . . . . . . . . . . .
28
3.7
Simulation 5: Newton–Raphson Algorithm . . . . . . . . . . . . . .
29
4.
The Solar System: The Runge–Kutta Methods
31
4.1
The Solar System . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
4.1.1
Newton’s Second Law . . . . . . . . . . . . . . . . . . . . .
31
4.1.2
Astronomical Units and Initial Conditions
. . . . . . . . .
32
4.1.3
Kepler’s Laws
. . . . . . . . . . . . . . . . . . . . . . . . .
32
4.1.4
The Inverse-Square Law and Stability of Orbits
. . . . . .
34
4.2
Euler–Cromer Algorithm . . . . . . . . . . . . . . . . . . . . . . . .
35
4.3
The Runge–Kutta Algorithm
. . . . . . . . . . . . . . . . . . . . .
36
4.3.1
The Method
. . . . . . . . . . . . . . . . . . . . . . . . . .
36
4.3.2
Example 1: The Harmonic Oscillator
. . . . . . . . . . . .
37
4.3.3
Example 2: The Solar System
. . . . . . . . . . . . . . . .
37
4.4
Precession of the Perihelion of Mercury . . . . . . . . . . . . . . . .
39
4.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
4.6
Simulation 6: Runge–Kutta Algorithm: Solar System . . . . . . . .
40
4.7
Simulation 7: Precession of the perihelion of Mercury . . . . . . . .
41
5.
Chaotic Pendulum
43
5.1
Equation of Motion . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
5.2
Numerical Algorithms
. . . . . . . . . . . . . . . . . . . . . . . . .
45
5.2.1
Euler–Cromer Algorithm . . . . . . . . . . . . . . . . . . .
46
5.2.2
Runge–Kutta Algorithm
. . . . . . . . . . . . . . . . . . .
46
5.3
Elements of Chaos
. . . . . . . . . . . . . . . . . . . . . . . . . . .
47
5.3.1
Butterﬂy Eﬀect: Sensitivity to Initial Conditions . . . . . .
47
5.3.2
Poincar´e Section and Attractors . . . . . . . . . . . . . . .
48
5.3.3
Period-Doubling Bifurcations . . . . . . . . . . . . . . . . .
48
5.3.4
Feigenbaum Ratio . . . . . . . . . . . . . . . . . . . . . . .
49
5.3.5
Spontaneous Symmetry Breaking
. . . . . . . . . . . . . .
49
5.4
Simulation 8: The Butterﬂy Eﬀect
. . . . . . . . . . . . . . . . . .
50
5.5
Simulation 9: Poincar´e Sections . . . . . . . . . . . . . . . . . . . .
50
5.6
Simulation 10: Period Doubling . . . . . . . . . . . . . . . . . . . .
52
5.7
Simulation 11: Bifurcation Diagrams . . . . . . . . . . . . . . . . .
53

Contents
xi
6.
Molecular Dynamics
55
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
6.2
The Lennard-Jones Potential
. . . . . . . . . . . . . . . . . . . . .
56
6.3
Units, Boundary Conditions and Verlet Algorithm
. . . . . . . . .
57
6.4
Some Physical Applications . . . . . . . . . . . . . . . . . . . . . .
59
6.4.1
Dilute Gas and Maxwell Distribution
. . . . . . . . . . . .
59
6.4.2
The Melting Transition . . . . . . . . . . . . . . . . . . . .
60
6.5
Simulation 12: Maxwell Distribution . . . . . . . . . . . . . . . . .
60
6.6
Simulation 13: Melting Transition . . . . . . . . . . . . . . . . . . .
61
7.
Pseudo Random Numbers and Random Walks
63
7.1
Random Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
7.1.1
Linear Congruent or Power Residue Method
. . . . . . . .
63
7.1.2
Statistical Tests of Randomness
. . . . . . . . . . . . . . .
64
7.2
Random Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
7.2.1
Random Walks . . . . . . . . . . . . . . . . . . . . . . . . .
66
7.2.2
Diﬀusion Equation . . . . . . . . . . . . . . . . . . . . . . .
67
7.3
The Random Number Generators RAN 0, 1, 2 . . . . . . . . . . . .
69
7.4
Simulation 14: Random Numbers . . . . . . . . . . . . . . . . . . .
72
7.5
Simulation 15: Random Walks
. . . . . . . . . . . . . . . . . . . .
73
8.
Monte Carlo Integration
75
8.1
Numerical Integration
. . . . . . . . . . . . . . . . . . . . . . . . .
75
8.1.1
Rectangular Approximation Revisited . . . . . . . . . . . .
75
8.1.2
Midpoint Approximation of Multidimensional Integrals . .
76
8.1.3
Spheres and Balls in d Dimensions . . . . . . . . . . . . . .
78
8.2
Monte Carlo Integration: Simple Sampling . . . . . . . . . . . . . .
78
8.2.1
Sampling (Hit or Miss) Method
. . . . . . . . . . . . . . .
79
8.2.2
Sample Mean Method . . . . . . . . . . . . . . . . . . . . .
79
8.2.3
Sample Mean Method in Higher Dimensions
. . . . . . . .
80
8.3
The Central Limit Theorem . . . . . . . . . . . . . . . . . . . . . .
80
8.4
Monte Carlo Errors and Standard Deviation . . . . . . . . . . . . .
82
8.5
Nonuniform Probability Distributions . . . . . . . . . . . . . . . . .
84
8.5.1
The Inverse Transform Method . . . . . . . . . . . . . . . .
84
8.5.2
The Acceptance-Rejection Method . . . . . . . . . . . . . .
86
8.6
Simulation 16: Midpoint and Monte Carlo Approximations
. . . .
86
8.7
Simulation 17: Nonuniform Probability Distributions . . . . . . . .
87
9.
The Metropolis Algorithm and the Ising Model
89
9.1
The Canonical Ensemble . . . . . . . . . . . . . . . . . . . . . . . .
89
9.2
Importance Sampling . . . . . . . . . . . . . . . . . . . . . . . . . .
90
9.3
The Ising Model
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
91

xii
An Introduction to Monte Carlo Simulations of Matrix Field Theory
9.4
The Metropolis Algorithm . . . . . . . . . . . . . . . . . . . . . . .
92
9.5
The Heat-Bath Algorithm . . . . . . . . . . . . . . . . . . . . . . .
94
9.6
The Mean Field Approximation . . . . . . . . . . . . . . . . . . . .
94
9.6.1
Phase Diagram and Critical Temperature . . . . . . . . . .
94
9.6.2
Critical Exponents . . . . . . . . . . . . . . . . . . . . . . .
96
9.7
Simulation of the Ising Model and Numerical Results . . . . . . . .
97
9.7.1
The Fortran Code . . . . . . . . . . . . . . . . . . . . . . .
97
9.7.2
Some Numerical Results
. . . . . . . . . . . . . . . . . . .
99
9.8
Simulation 18: The Metropolis Algorithm and the Ising Model
. . 101
9.9
Simulation 19: The Ferromagnetic Second Order Phase Transition
102
9.10
Simulation 20: The 2-Point Correlator . . . . . . . . . . . . . . . . 103
9.11
Simulation 21: Hysteresis and the First Order Phase Transition . . 104
Monte Carlo Simulations of Matrix Field Theory
105
10. Metropolis Algorithm for Yang–Mills Matrix Models
107
10.1
Dimensional Reduction . . . . . . . . . . . . . . . . . . . . . . . . . 107
10.1.1
Yang–Mills Action . . . . . . . . . . . . . . . . . . . . . . . 107
10.1.2
Chern–Simons Action: Myers Term
. . . . . . . . . . . . . 108
10.2
Metropolis Accept/Reject Step
. . . . . . . . . . . . . . . . . . . . 112
10.3
Statistical Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
10.4
Auto-Correlation Time . . . . . . . . . . . . . . . . . . . . . . . . . 114
10.5
Code and Sample Calculation . . . . . . . . . . . . . . . . . . . . . 115
References
117
11. Hybrid Monte Carlo Algorithm for Yang–Mills Matrix Models
119
11.1
The Yang–Mills Matrix Action
. . . . . . . . . . . . . . . . . . . . 119
11.2
The Leap Frog Algorithm
. . . . . . . . . . . . . . . . . . . . . . . 120
11.3
Metropolis Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 122
11.4
Gaussian Distribution
. . . . . . . . . . . . . . . . . . . . . . . . . 123
11.5
Physical Tests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
11.6
Emergent Geometry: An Exotic Phase Transition . . . . . . . . . . 124
References
129
12. Hybrid Monte Carlo Algorithm for Noncommutative Phi-Four
131
12.1
The Matrix Scalar Action . . . . . . . . . . . . . . . . . . . . . . . 131
12.2
The Leap Frog Algorithm
. . . . . . . . . . . . . . . . . . . . . . . 132
12.3
Hybrid Monte Carlo Algorithm . . . . . . . . . . . . . . . . . . . . 132
12.4
Optimization
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
12.4.1
Partial Optimization . . . . . . . . . . . . . . . . . . . . . . 132

Contents
xiii
12.4.2
Full Optimization . . . . . . . . . . . . . . . . . . . . . . . 134
12.5
The Non-Uniform Order: Another Exotic Phase . . . . . . . . . . . 134
12.5.1
Phase Structure . . . . . . . . . . . . . . . . . . . . . . . . 134
12.5.2
Sample Simulations . . . . . . . . . . . . . . . . . . . . . . 135
References
139
13. Lattice HMC Simulations of Φ4
2: A Lattice Example
141
13.1
Model and Phase Structure
. . . . . . . . . . . . . . . . . . . . . . 141
13.2
The HM Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
13.3
Renormalization and Continuum Limit . . . . . . . . . . . . . . . . 147
13.4
HMC Simulation Calculation of the Critical Line . . . . . . . . . . 149
References
151
14. (Multi-Trace) Quartic Matrix Models
153
14.1
The Pure Real Quartic Matrix Model . . . . . . . . . . . . . . . . . 153
14.2
The Multi-Trace Matrix Model . . . . . . . . . . . . . . . . . . . . 154
14.3
Model and Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 156
14.4
The Disorder-to-Non-Uniform-Order Transition . . . . . . . . . . . 158
14.5
Other Suitable Algorithms . . . . . . . . . . . . . . . . . . . . . . . 160
14.5.1
Over-Relaxation Algorithm . . . . . . . . . . . . . . . . . . 160
14.5.2
Heat-Bath Algorithm . . . . . . . . . . . . . . . . . . . . . 161
References
162
15. The Remez Algorithm and the Conjugate Gradient Method
163
15.1
Minimax Approximations
. . . . . . . . . . . . . . . . . . . . . . . 163
15.1.1
Minimax Polynomial Approximation and Chebyshev
Polynomials
. . . . . . . . . . . . . . . . . . . . . . . . . . 163
15.1.2
Minimax Rational Approximation and Remez Algorithm
. 168
15.1.3
The Code “AlgRemez”
. . . . . . . . . . . . . . . . . . . . 171
15.2
Conjugate Gradient Method . . . . . . . . . . . . . . . . . . . . . . 171
15.2.1
Construction . . . . . . . . . . . . . . . . . . . . . . . . . . 171
15.2.2
The Conjugate Gradient Method as a Krylov Space Solver
175
15.2.3
The Multi-Mass Conjugate Gradient Method . . . . . . . . 177
References
179
16. Monte Carlo Simulation of Fermion Determinants
181
16.1
The Dirac Operator
. . . . . . . . . . . . . . . . . . . . . . . . . . 181
16.2
Pseudo-Fermions and Rational Approximations . . . . . . . . . . . 185
16.3
More on The Conjugate-Gradient . . . . . . . . . . . . . . . . . . . 187
16.3.1
Multiplication by M′ and (M′)+
. . . . . . . . . . . . . . 187

xiv
An Introduction to Monte Carlo Simulations of Matrix Field Theory
16.3.2
The Fermionic Force . . . . . . . . . . . . . . . . . . . . . . 190
16.4
The Rational Hybrid Monte Carlo Algorithm
. . . . . . . . . . . . 192
16.4.1
Statement
. . . . . . . . . . . . . . . . . . . . . . . . . . . 192
16.4.2
Preliminary Tests
. . . . . . . . . . . . . . . . . . . . . . . 193
16.5
Other Related Topics . . . . . . . . . . . . . . . . . . . . . . . . . . 197
References
199
17. U(1) Gauge Theory on the Lattice: Another Lattice Example
201
17.1
Continuum Considerations . . . . . . . . . . . . . . . . . . . . . . . 201
17.2
Lattice Regularization . . . . . . . . . . . . . . . . . . . . . . . . . 203
17.2.1
Lattice Fermions and Gauge Fields
. . . . . . . . . . . . . 203
17.2.2
Quenched Approximation . . . . . . . . . . . . . . . . . . . 205
17.2.3
Wilson Loop, Creutz Ratio and Other Observables . . . . . 206
17.3
Monte Carlo Simulation of Pure U(1) Gauge Theory . . . . . . . . 209
17.3.1
The Metropolis Algorithm
. . . . . . . . . . . . . . . . . . 209
17.3.2
Some Numerical Results
. . . . . . . . . . . . . . . . . . . 212
17.3.3
Coulomb and Conﬁnement Phases . . . . . . . . . . . . . . 215
References
216
18. Codes
217
9.1 metropolis-ym.f . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
9.2 hybrid-ym.f
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
9.3 hybrid-scalar-fuzzy.f . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
9.4 phi-four-on-lattice.f . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
9.5 metropolis-scalar-multitrace.f
. . . . . . . . . . . . . . . . . . . . . . 249
9.6 remez.f . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
9.7 conjugate-gradient.f . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
9.8 hybrid-supersymmetric-ym.f . . . . . . . . . . . . . . . . . . . . . . . 261
9.9 u-one-on-the-lattice.f
. . . . . . . . . . . . . . . . . . . . . . . . . . . 279
Index
291

Introductory Remarks
Introducing Computational Physics
Computational physics is a subﬁeld of computational science and scientiﬁc comput-
ing in which we combine elements from physics (especially theoretical), elements
from mathematics (in particular applied mathematics such as numerical analysis)
and elements from computer science (programming) for the purpose of solving a
physics problem. In physics there are traditionally two approaches which are fol-
lowed: (1) The experimental approach and (2) the theoretical approach. Nowadays,
we may consider “The computational approach” as a third approach in physics. It
can even be argued that the computational approach is independent from the ﬁrst
two approaches and it is not simply a bridge between the two.
The most important use of computers in physics is simulation. Simulations are
suited for nonlinear problems which can not generally solved by analytical methods.
The starting point of a simulation is an idealized model of a physical system of
interest. We want to check whether or not the behaviour of this model is consistent
with observation. We specify an algorithm for the implementation of the model on
a computer. The execution of this implementation is a simulation. Simulations are
therefore virtual experiments. The comparison between computer simulations and
laboratory experiments goes therefore as follows:
Laboratory experiment
Simulation
sample
model
physical apparatus
computer program (the code)
calibration
testing of code
measurement
computation
data analysis
data analysis
A crucial tool in computational physics is programming languages. In simulations
as used by the majority of research physicists codes are written in a high-level
compiled language such as Fortran and C/C++. In such simulations we may also
use calls to routine libraries such as Lapack. The use of mathematical software
xv

xvi
An Introduction to Monte Carlo Simulations of Matrix Field Theory
packages such as Maple, Mathematica and Matlab is only suited for relatively small
calculations.
These packages are interpreted languages and thus the code they
produce run generally far too slowly compared to compiled languages. In this book
we will mainly follow the path of developing and writing all our codes in a high-level
compiled language and not call any libraries. As our programming language we will
use Fortran 77 under the Linux operating system. We adopt exclusively the Ubuntu
distribution of Linux. We will use the Fortran compilers f77 and gfortran. As an
editor we will use mostly Emacs and sometimes Gedit and Nano while for graphics
we will use mostly Gnuplot.
References
The main references which we have followed in developing the ﬁrst part of this book
include the following items:
(1) N. J. Giordano, H. Nakanishi, Computational Physics (2nd edition), Pear-
son/Prentice Hall, (2006).
(2) H. Gould, J. Tobochnick, W. Christian, An Introduction To Computer Simula-
tion Methods: Applications to Physical Systems (3rd Edition), Addison-Wesley
(2006).
(3) R. H. Landau, M. J. Paez, C. C. Bordeianu, Computational Physics: Problem
Solving with Computers (2nd edition), John Wiley and Sons (2007).
(4) R. Fitzpatrick, Introduction to Computational Physics,
http://farside.ph.utexas.edu/teaching/329/329.html.
(5) K. Anagnostopoulos, Computational Physics: A Practical Introduction to Com-
putational Physics and Scientiﬁc Computing, Lulu.com (2014).
(6) J. M. Thijssen, Computational Physics, Cambridge University Press (1999).
(7) M. Hjorth-Jensen, Computational Physics, CreateSpace Publishing (2015).
(8) P. L. DeVries, A First Course in Computational Physics (2nd edition), Jones
and Bartlett Publishers (2010).
Codes and Solutions
The Fortran codes relevant to the problems considered in the ﬁrst part of the book
as well as some key sample solutions can be found at the URL:
http://homepages.dias.ie/ydri/codes_solutions/
Matrix Field Theory
The second part of this book, which is eﬀectively the main part, deals with the
important problem of how to set up working Monte Carlo simulations of matrix
ﬁeld theories in a, hopefully, pedagogical way. The subject of matrix ﬁeld theory
involves non-perturbative matrix regularizations, or simply matrix representations,

Introductory Remarks
xvii
of noncommutative ﬁeld theory and noncommutative geometry, fuzzy physics and
fuzzy spaces, fuzzy ﬁeld theory, matrix geometry and gravity and random matrix
theory. The subject of matrix ﬁeld theory may even include matrix regularizations
of supersymmetry, string theory and M-theory. These matrix regularizations employ
necessarily ﬁnite dimensional matrix algebras so that the problems are amenable
and are accessible to Monte Carlo methods.
The matrix regulator should be contrasted with the, well established, lattice
regulator with advantages and disadvantages which are discussed in their places in
the literature. However, we note that only 5 simulations among the 7 simulations
considered in this part of the book use the matrix regulator whereas the other
2, closely related simulations, use the usual lattice regulator. This part contains
also a special chapter on the Remez and conjugate gradient algorithms which are
required for the simulation of dynamical fermions. The study of matrix ﬁeld theory
in its own right, and not thought of as regulator, has also become very important
to the proper understanding of all noncommutative, fuzzy and matrix phenomena.
Naturally, therefore, the mathematical, physical and numerical aspects, required for
the proper study of matrix ﬁeld theory, which are found in this part of the book
are quite advanced by comparison with what is found in the ﬁrst part of the book.
The set of references for each topic consists mainly of research articles and is
included at the end of each chapter. Sample numerical calculations are also included
as a section or several sections in each chapter. Some of these solutions are quite
detailed whereas others are brief. The relevant Fortran codes for this part of the
book are collected in the last chapter for convenience and completeness.
These
codes are, of course, provided as is and no warranty should be assumed.
Appendices
We attach two appendices at the end of this book relevant to the ﬁrst part of this
book. In the ﬁrst appendix we discuss the ﬂoating point representation of numbers,
machine precision and roundoﬀand systematic errors. In the second appendix we
give an executive summary of the simulations of Part I translated into arabic.
Acknowledgments
Firstly, I would like to thank both the ex-head as well as the current-head of the
physics department, professor M. Benchihab and professor A. Chibani, for their crit-
ical help in formally launching the computational physics course at Badji Mokhtar
Annaba University during the academic year 2009-2010 and thus making the whole
experience possible. This three-semester course, based on the ﬁrst part of this book,
has become since a ﬁxture of the physics curriculum at both the Licence (Bachelor)
and Master levels. Secondly, I should also thank doctor A. Bouchareb and doc-
tor R. Chemam who had helped in a crucial way with the actual teaching of the
course, especially the laboratory simulations, since the beginning. Lastly, I would

xviii
An Introduction to Monte Carlo Simulations of Matrix Field Theory
like to thank my doctoral students and doctor A. Bouchareb for their patience and
contributions during the development of the second part of this book in the weekly
informal meeting we have organized for this purpose.

Chapter 1
Euler Algorithm
1.1
Euler Algorithm
It is a well appreciated fact that ﬁrst order diﬀerential equations are commonplace
in all branches of physics. They appear virtually everywhere and some of the most
fundamental problems of nature obey simple ﬁrst order diﬀerential equations or
second order diﬀerential equations. It is so often possible to recast second order
diﬀerential equations as ﬁrst order diﬀerential equations with a doubled number
of unknown.
From the numerical standpoint the problem of solving ﬁrst order
diﬀerential equations is a conceptually simple one as we will now explain.
We consider the general ﬁrst order ordinary diﬀerential equation
y′ = dy
dx = f(x, y).
(1.1)
We impose the general initial-value boundary condition is
y(x0) = y0.
(1.2)
We solve for the function y = y(x) in the unit x-interval starting from x0. We make
the x-interval discretization
xn = x0 + n∆x , n = 0, 1, ...
(1.3)
The Euler algorithm is one of the oldest known numerical recipe. It consists in
replacing the function y(x) in the interval [xn, xn+1] by the straight line connecting
the points (xn, yn) and (xn+1, yn+1). This comes from the deﬁnition of the derivative
at the point x = xn given by
yn+1 −yn
xn+1 −xn
= f(xn, yn).
(1.4)
This means that we replace the above ﬁrst order diﬀerential equation by the ﬁnite
diﬀerence equation
yn+1 ≃yn + ∆xf(xn, yn).
(1.5)
This is only an approximation. The truncation error is given by the next term in
the Taylor’s expansion of the function y(x) which is given by
yn+1 ≃yn + ∆xf(xn, yn) + 1
2∆x2 df(x, y)
dx

x=xn
+ ....
(1.6)
3
b
y.

4
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The error then reads
1
2(∆x)2 df(x, y)
dx

x=xn.
(1.7)
The error per step is therefore proportional to (∆x)2. In a unit interval we will
perform N = 1/∆x steps. The total systematic error is therefore proportional to
N(∆x)2 = 1
N .
(1.8)
1.2
First Example and Sample Code
1.2.1
Radioactive Decay
It is an experimental fact that radioactive decay obeys a very simple ﬁrst order
diﬀerential equation. In a spontaneous radioactive decay a particle with no external
inﬂuence will decay into other particles. A typical example is the nuclear isotope
uranium 235. The exact moment of decay of any one particle is random. This
means that the number −dN(t) = N(t) −N(t + dt) of nuclei which will decay
during a time interval dt must be proportional to dt and to the number N(t) of
particles present at time t, i.e.
−dN(t) ∝N(t)dt.
(1.9)
In other words the probability of decay per unit time given by (−dN(t)/N(t))/dt
is a constant which we denote 1/τ. The minus sign is due to the fact that dN(t) is
negative since the number of particles decreases with time. We write
dN(t)
dt
= −N(t)
τ
.
(1.10)
The solution of this ﬁrst order diﬀerential equation is given by a simple exponential
function, viz
N(t) = N0 exp(−t/τ).
(1.11)
The number N0 is the number of particles at time t = 0. The time τ is called the
mean lifetime. It is the average time for decay. For the uranium 235 the mean
lifetime is around 109 years.
The goal now is to obtain an approximate numerical solution to the problem
of radioactivity using the Euler algorithm. In this particular case we can compare
to an exact solution given by the exponential decay law (1.11). We start evidently
from the Taylor’s expansion
N(t + ∆t) = N(t) + ∆tdN
dt + 1
2(∆t)2 d2N
dt2 + ...
(1.12)
We get in the limit ∆t approaching 0 the result
dN
dt = Lim

∆t−→0
N(t + ∆t) −N(t)
∆t
.
(1.13)
b
y.

Euler Algorithm
5
We take ∆t small but non zero. In this case we obtain the approximation
dN
dt ≃N(t + ∆t) −N(t)
∆t
.
(1.14)
Equivalently
N(t + ∆t) ≃N(t) + ∆tdN
dt .
(1.15)
By using (1.10) we get
N(t + ∆t) ≃N(t) −∆tN(t)
τ
.
(1.16)
We will start from the number of particles at time t = 0 given by N(0) = N0 which
is known. We substitute t = 0 in (1.16) to obtain N(∆t) = N(1) as a function of
N(0). Next the value N(1) can be used in equation (1.16) to get N(2∆t) = N(2),
etc. We are thus led to the time discretization
t ≡t(i) = i∆t , i = 0, ..., N.
(1.17)
In other words
N(t) = N(i).
(1.18)
The integer N determine the total time interval T = N∆t. The numerical solution
(1.16) can be rewritten as
N(i + 1) = N(i) −∆tN(i)
τ
, i = 0, ..., N.
(1.19)
This is Euler algorithm for radioactive decay. For convenience we shift the integer
i so that the above equation takes the form
N(i) = N(i −1) −∆tN(i −1)
τ
, i = 1, ..., N + 1.
(1.20)
We introduce ˆ
N(i) = N(i −1), i.e. ˆ
N(1) = N(0) = N0. We get
ˆ
N(i + 1) = ˆ
N(i) −∆t
ˆ
N(i)
τ
, i = 1, ..., N + 1.
(1.21)
The corresponding times are
ˆt(i + 1) = i∆t , i = 1, ..., N + 1.
(1.22)
The initial number of particles at time ˆt(1) = 0 is ˆ
N(1) = N0. This approximate
solution should be compared with the exact solution (1.11).
b
y.

6
An Introduction to Monte Carlo Simulations of Matrix Field Theory
1.2.2
A Sample Fortran Code
The goal in this section is to provide a sample Fortran code which implements
the above algorithm (1.21). The reasons behind choosing Fortran were explained
in the introduction. Any Fortran program, like any other programming language,
must start with some program statement and conclude with an end statement. The
program statement allows us to give a name to the program. The end statement
may be preceded by a return statement. This looks like
program radioactivity
c Here is the code
return
end
We have chosen the name “radioactivity” for our program. The “c” in the second
line indicates that the sentence “here is the code” is only a comment and not a part
of the code.
After the program statement come the declaration statements. We state the
variables and their types which are used in the program. In Fortran we have the
integer type for integer variables and the double precision type for real variables. In
the case of (1.21) the variables ˆ
N(i), ˆt(i), τ, ∆t, N0 are real numbers while the
variables i and N are integer numbers.
An array A of dimension K is an ordered list of K variables of a given type
called the elements of the array and denoted A(1), A(2), ..., A(K). In our above
example ˆ
N(i) and ˆt(i) are real arrays of dimension N + 1. We declare that ˆ
N(i)
and ˆt(i) are real for all i = 1, ..., N + 1 by writing ˆ
N(1 : N + 1) and ˆt(1 : N + 1).
Since an array is declared at the beginning of the program it must have a ﬁxed
size. In other words the upper limit must be a constant and not a variable. In
Fortran a constant is declared with a parameter statement. In our above case the
upper limit is N + 1 and hence N must be declared in parameter statement.
In the Fortran code we choose to use the notation A = ˆ
N, A0 = ˆ
N0, time = ˆt,
∆= ∆t and tau = τ. By putting all declarations together we get the following
preliminary lines of code
program radioactivity
integer i,N
parameter (N=100)
doubleprecision
A(1:N+1),A0,time(1:N+1),Delta,tau
c Here is the code
return
end
b
y.

Euler Algorithm
7
The input of the computation in our case are obviously given by the parameters
N0, τ, ∆t and N.
For the radioactivity problem the main part of the code consists of equations
(1.21) and (1.22). We start with the known quantities ˆ
N(1) = N0 at ˆt(1) = 0 and
generate via the successive use of (1.21) and (1.22) ˆ
N(i) and ˆt(i) for all i > 1. This
will be coded using a do loop. It begins with a do statement and ends with an enddo
statement. We may also indicate a step size.
The output of the computation can be saved to a ﬁle using a write statement
inside the do loop. In our case the output is the number of particles ˆ
N(i) and the
time ˆt(i). The write statement reads explicitly
write(10, ∗) ˆt(i), ˆ
N(i).
The data will then be saved to a ﬁle called fort.10.
By including the initialization, the do loop and the write statement we obtain
the complete code
program radioactivity
integer i,N
parameter (N=100)
doubleprecision
A(1:N+1),A0,time(1:N+1),Delta,tau
parameter (A0=1000,Delta=0.01d0,tau=1.0d0)
A(1)=A0
time(1)=0
do i=1,N+1,1
A(i+1)=A(i)-Delta*A(i)/tau
time(i+1)=i*Delta
write(10,*) time(i+1),A(i+1)
enddo
return
end
1.3
More Examples
1.3.1
Air Resistance
We consider an athlete riding a bicycle moving on a ﬂat terrain. The goal is to
determine the velocity. Newton’s second law is given by
mdv
dt = F.
(1.23)
F is the force exerted by the athlete on the bicycle. It is clearly very diﬃcult to
write down a precise expression for F. Formulating the problem in terms of the
power generated by the athlete will avoid the use of an explicit formula for F.
Multiplying the above equation by v we obtain
dE
dt = P.
(1.24)
b
y.

8
An Introduction to Monte Carlo Simulations of Matrix Field Theory
E is the kinetic energy and P is the power, viz
E = 1
2mv2 , P = Fv.
(1.25)
Experimentally we ﬁnd that the output of well trained athletes is around P = 400
watts over periods of 1h. The above equation can also be rewritten as
dv2
dt = 2P
m .
(1.26)
For P constant we get the solution
v2 = 2P
m t + v2
0.
(1.27)
We remark the unphysical eﬀect that v −→∞as t −→∞. This is due to the
absence of the eﬀect of friction and in particular air resistance.
The most important form of friction is air resistance.
The force due to air
resistance (the drag force) is
Fdrag = −B1v −B2v2.
(1.28)
At small velocities the ﬁrst term dominates whereas at large velocities it is the
second term that dominates. For very small velocities the dependence on v given
by Fdrag = −B1v is known as Stokes’ law. For reasonable velocities the drag force
is dominated by the second term, i.e. it is given for most objects by
Fdrag = −B2v2.
(1.29)
The coeﬃcient B2 can be calculated as follows. As the bicycle-rider combination
moves with velocity v it pushes in a time dt a mass of air given by dmair = ρAvdt
where ρ is the air density and A is the frontal cross section. The corresponding
kinetic energy is
dEair = dmairv2/2.
(1.30)
This is equal to the work done by the drag force, i.e.
−Fdragvdt = dEair.
(1.31)
From this we get
B2 = CρA.
(1.32)
The drag coeﬃcient is C = 1
2. The drag force becomes
Fdrag = −CρAv2.
(1.33)
Taking into account the force due to air resistance we ﬁnd that Newton’s law be-
comes
mdv
dt = F + Fdrag.
(1.34)
Equivalently
dv
dt = P
mv −CρAv2
m
.
(1.35)
b
y.

Euler Algorithm
9
It is not obvious that this equation can be solved exactly in any easy way. The
Euler algorithm gives the approximate solution
v(i + 1) = v(i) + ∆tdv
dt (i).
(1.36)
In other words
v(i + 1) = v(i) + ∆t

P
mv(i) −CρAv2(i)
m

, i = 0, ..., N.
(1.37)
This can also be put in the form (with ˆv(i) = v(i −1))
ˆv(i + 1) = ˆv(i) + ∆t

P
mˆv(i) −CρAˆv2(i)
m

, i = 1, ..., N + 1.
(1.38)
The corresponding times are
t ≡ˆt(i + 1) = i∆t , i = 1, ..., N + 1.
(1.39)
The initial velocity ˆv(1) at time t(1) = 0 is known.
1.3.2
Projectile Motion
There are two forces acting on the projectile. The weight force and the drag force.
The drag force is opposite to the velocity. In this case Newton’s law is given by
md⃗v
dt = ⃗F + ⃗Fdrag
= m⃗g −B2v2⃗v
v
= m⃗g −B2v⃗v.
(1.40)
The goal is to determine the position of the projectile and hence one must solve the
two equations
d⃗x
dt = ⃗v.
(1.41)
md⃗v
dt = m⃗g −B2v⃗v.
(1.42)
In components (the horizontal axis is x and the vertical axis is y) we have 4 equations
of motion given by
dx
dt = vx.
(1.43)
mdvx
dt = −B2vvx.
(1.44)
dy
dt = vy.
(1.45)
mdvy
dt = −mg −B2vvy.
(1.46)
b
y.

10
An Introduction to Monte Carlo Simulations of Matrix Field Theory
We recall the constraint
v =
q
v2x + v2y.
(1.47)
The numerical approach we will employ in order to solve the 4 equations of motion
(1.43)–(1.46) together with (1.47) consists in using Euler algorithm. This yields the
approximate solution given by the equations
x(i + 1) = x(i) + ∆tvx(i).
(1.48)
vx(i + 1) = vx(i) −∆tB2v(i)vx(i)
m
.
(1.49)
y(i + 1) = y(i) + ∆tvy(i).
(1.50)
vy(i + 1) = vy(i) −∆tg −∆tB2v(i)vy(i)
m
.
(1.51)
The constraint is
v(i) =
q
vx(i)2 + vy(i)2.
(1.52)
In the above equations the index i is such that i = 0, ..., N. The initial position and
velocity are given, i.e. x(0), y(0), vx(0) and vy(0) are known.
1.4
Periodic Motions and Euler–Cromer and Verlet Algorithms
As discussed above at each iteration using the Euler algorithm there is a systematic
error proportional to 1/N. Obviously this error will accumulate and may become
so large that it will alter the solution drastically at later times. In the particular
case of periodic motions, where the true nature of the motion can only become clear
after few elapsed periods, the large accumulated error can lead to diverging results.
In this section we will discuss simple variants of the Euler algorithm which perform
much better than the plain Euler algorithm for periodic motions.
1.4.1
Harmonic Oscillator
We consider a simple pendulum: a particle of mass m suspended by a massless
string from a rigid support. There are two forces acting on the particle. The weight
and the tension of the string. Newton’s second law reads
md2⃗s
dt = m⃗g + ⃗T.
(1.53)
The parallel (with respect to the string) projection reads
0 = −mg cos θ + T.
(1.54)
The perpendicular projection reads
md2s
dt2 = −mg sin θ.
(1.55)
b
y.

Euler Algorithm
11
The θ is the angle that the string makes with the vertical. Clearly s = lθ. The
force mg sin θ is a restoring force which means that it is always directed toward the
equilibrium position (here θ = 0) opposite to the displacement and hence the minus
sign in the above equation. We get by using s = lθ the equation
d2θ
dt2 = −g
l sin θ.
(1.56)
For small θ we have sin θ ≃θ. We obtain
d2θ
dt2 = −g
l θ.
(1.57)
The solution is a sinusoidal function of time with frequency Ω=
p
g/l. It is given
by
θ(t) = θ0 sin(Ωt + φ).
(1.58)
The constants θ0 and φ depend on the initial displacement and velocity of the
pendulum. The frequency is independent of the mass m and the amplitude of the
motion and depends only on the length l of the string.
1.4.2
Euler Algorithm
The numerical solution is based on Euler algorithm. It is found as follows. First we
replace the equation of motion (1.57) by the following two equations
dθ
dt = ω.
(1.59)
dω
dt = −g
l θ.
(1.60)
We use the deﬁnition of a derivative of a function, viz
df
dt = f(t + ∆t) −f(t)
∆t
, ∆t −→0.
(1.61)
We get for small but non zero ∆t the approximations
θ(t + ∆t) ≃θ(t) + ω(t)∆t
ω(t + ∆t) ≃ω(t) −g
l θ(t)∆t.
(1.62)
We consider the time discretization
t ≡t(i) = i∆t , i = 0, ..., N.
(1.63)
In other words
θ(t) = θ(i) , ω(t) = ω(i).
(1.64)
The integer N determine the total time interval T = N∆t. The above numerical
solution can be rewritten as
ω(i + 1) = ω(i) −g
l θ(i)∆t
θ(i + 1) = θ(i) + ω(i)∆t.
(1.65)
b
y.

12
An Introduction to Monte Carlo Simulations of Matrix Field Theory
We shift the integer i such that it takes values in the range [1, N + 1]. We obtain
ω(i) = ω(i −1) −g
l θ(i −1)∆t
θ(i) = θ(i −1) + ω(i −1)∆t.
(1.66)
We introduce ˆω(i) = ω(i −1) and ˆθ(i) = θ(i −1). We get with i = 1, ..., N + 1 the
equations
ˆω(i + 1) = ˆω(i) −g
l
ˆθ(i)∆t
ˆθ(i + 1) = ˆθ(i) + ˆω(i)∆t.
(1.67)
By using the values of θ and ω at time i we calculate the corresponding values at
time i + 1. The initial angle and angular velocity ˆθ(1) = θ(0) and ˆω(1) = ω(0) are
known. This process will be repeated until the functions θ and ω are determined
for all times.
1.4.3
Euler–Cromer Algorithm
As it turns out the above Euler algorithm does not conserve energy. In fact Euler’s
method is not good for all oscillatory systems. A simple modiﬁcation of Euler’s
algorithm due to Cromer will solve this problem of energy non conservation. This
goes as follows. We use the values of the angle ˆθ(i) and the angular velocity ˆω(i) at
time step i to calculate the angular velocity ˆω(i + 1) at time step i + 1. This step
is the same as before. However we use ˆθ(i) and ˆω(i + 1) (and not ˆω(i)) to calculate
ˆθ(i + 1) at time step i + 1. This procedure as shown by Cromer’s will conserve
energy in oscillatory problems. In other words equations (1.67) become
ˆω(i + 1) = ˆω(i) −g
l
ˆθ(i)∆t
ˆθ(i + 1) = ˆθ(i) + ˆω(i + 1)∆t.
(1.68)
The error can be computed as follows. From these two equations we get
ˆθ(i + 1) = ˆθ(i) + ˆω(i)∆t −g
l
ˆθ(i)∆t2
= ˆθ(i) + ˆω(i)∆t + d2ˆθ
dt

i∆t2.
(1.69)
In other words the error per step is still of the order of ∆t2. However the Euler-
Cromer algorithm does better than Euler algorithm with periodic motion. Indeed
at each step i the energy conservation condition reads
Ei+1 = Ei + g
2l

ω2
i −g
l θ2
i

∆t2.
(1.70)
The energy of the simple pendulum is of course by
Ei = 1
2ω2
i + g
2lθ2
i .
(1.71)
b
y.

Euler Algorithm
13
The error at each step is still proportional to ∆t2 as in the Euler algorithm. However
the coeﬃcient is precisely equal to the diﬀerence between the values of the kinetic
energy and the potential energy at the step i. Thus the accumulated error which
is obtained by summing over all steps vanishes since the average kinetic energy
is equal to the average potential energy. In the Euler algorithm the coeﬃcient is
actually equal to the sum of the kinetic and potential energies and as consequence
no cancellation can occur.
1.4.4
Verlet Algorithm
Another method which is much more accurate and thus very suited to periodic mo-
tions is due to Verlet. Let us consider the forward and backward Taylor expansions
θ(ti + ∆t) = θ(ti) + ∆tdθ
dt

ti + 1
2(∆t)2 d2θ
dt2

ti + 1
6(∆t)3 d3θ
dt3

ti + ...
(1.72)
θ(ti −∆t) = θ(ti) −∆tdθ
dt

ti + 1
2(∆t)2 d2θ
dt2

ti −1
6(∆t)3 d3θ
dt3

ti + ...
(1.73)
Adding these expressions we get
θ(ti + ∆t) = 2θ(ti) −θ(ti −∆t) + (∆t)2 d2θ
dt2

ti + O(∆4).
(1.74)
We write this as
θi+1 = 2θi −θi−1 −g
l (∆t)2θi.
(1.75)
This is the Verlet algorithm for the harmonic oscillator. First we remark that the
error is proportional to ∆t4 which is less than the errors in the Euler, Euler–Cromer
(and even less than the error in the second-order Runge–Kutta) methods so this
method is much more accurate. Secondly in this method we do not need to calculate
the angular velocity ω = dθ/dt. Thirdly this method is not self-starting. In other
words given the initial conditions θ1 and ω1 we need also to know θ2 for the algorithm
to start.
We can for example determine θ2 using the Euler method, viz θ2 =
θ1 + ∆t ω1.
1.5
Exercises
Exercise 1:
We give the diﬀerential equations
dx
dt = v.
(1.76)
dv
dt = a −bv.
(1.77)
• Write down the exact solutions.
• Write down the numerical solutions of these diﬀerential equations using Euler
and Verlet methods and determine the corresponding errors.
b
y.

14
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Exercise 2:
The equation of motion of the solar system in polar coordinates is
d2r
dt2 = l2
r3 −GM
r2 .
(1.78)
Solve this equation using Euler, Euler–Cromer and Verlet methods.
Exercise 3:
The equation of motion of a free falling object is
d2z
dt2 = −g.
(1.79)
• Write down the exact solution.
• Give a solution of this problem in terms of Euler method and determine the
error.
• We choose the initial conditions z = 0, v = 0 at t = 0. Determine the position
and the velocity between t = 0 and t = 1 for N = 4. Compare with the exact
solution and compute the error in each step. Express the result in terms of
l = g∆t2.
• Give a solution of this problem in terms of Euler–Cromer and Verlet methods
and determine the corresponding errors.
Exercise 4:
The equation governing population growth is
dN
dt = aN −bN 2.
(1.80)
The linear term represents the rate of birth while the quadratic term represents
the rate of death. Give a solution of this problem in terms of the Euler and Verlet
methods and determine the corresponding errors.
1.6
Simulation 1: Euler Algorithm — Air Resistance
The equation of motion of a cyclist exerting a force on his bicycle corresponding to
a constant power P and moving against the force of air resistance is given by
dv
dt = P
mv −CρAv2
m
.
The numerical approximation of this ﬁrst order diﬀerential equation which we will
consider in this problem is based on Euler algorithm.
(1) Calculate the speed v as a function of time in the case of zero air resistance and
then in the case of non-vanishing air resistance. What do you observe. We will
take P = 200 and C = 0.5. We also give the values
m = 70kg , A = 0.33m2 , ρ = 1.2kg/m3 , ∆t = 0.1s , T = 200s.
The initial speed is
ˆv(1) = 4m/s , ˆt(1) = 0.
(2) What do you observe if we change the drag coeﬃcient and/or the power. What
do you observe if we decrease the time step.
b
y.

Euler Algorithm
15
1.7
Simulation 2: Euler Algorithm — Projectile Motion
The numerical approximation based on the Euler algorithm of the equations of
motion of a projectile moving under the eﬀect of the forces of gravity and air
resistance is given by the equations
vx(i + 1) = vx(i) −∆tB2v(i)vx(i)
m
.
vy(i + 1) = vy(i) −∆tg −∆tB2v(i)vy(i)
m
.
v(i + 1) =
q
v2x(i + 1) + v2y(i + 1).
x(i + 1) = x(i) + ∆t vx(i).
y(i + 1) = y(i) + ∆t vy(i).
(1) Write a Fortran code which implements the above Euler algorithm.
(2) We take the values
B2
m = 0.00004m−1 , g = 9.8m/s2.
v(1) = 700m/s , θ = 30 degree.
vx(1) = v(1) cos θ , vy(1) = v(1) sin θ.
N = 105 , ∆t = 0.01s.
Calculate the trajectory of the projectile with and without air resistance. What
do you observe.
(3) We can determine numerically the range of the projectile by means of the condi-
tional instruction if. This can be done by adding inside the do loop the following
condition
if (y(i + 1).le.0) exit
Determine the range of the projectile with and without air resistance.
(4) In the case where air resistance is absent we know that the range is maximal
when the initial angle is 45 degrees. Verify this fact numerically by considering
several angles. More precisely add a do loop over the initial angle in order to
be able to study the range as a function of the initial angle.
(5) In the case where air resistance is non zero calculate the angle for which the
range is maximal.
b
y.

16
An Introduction to Monte Carlo Simulations of Matrix Field Theory
1.8
Simulation 3: Euler, Euler–Cromer and Verlet Algorithms
We will consider the numerical solutions of the equation of motion of a simple
harmonic oscillator given by the Euler, Euler–Cromer and Verlet algorithms which
take the form
ωi+1 = ωi −g
l θi ∆t , θi+1 = θi + ωi ∆t , Euler.
ωi+1 = ωi −g
l θi ∆t , θi+1 = θi + ωi+1 ∆t , Euler–Cromer.
θi+1 = 2θi −θi−1 −g
l θi(∆t)2 , Verlet.
(1) Write a Fortran code which implements the Euler, Euler–Cromer and Verlet
algorithms for the harmonic oscillator problem.
(2) Calculate the angle, the angular velocity and the energy of the harmonic os-
cillator as functions of time. The energy of the harmonic oscillator is given
by
E = 1
2ω2 + 1
2
g
l θ2.
We take the values
g = 9.8m/s2 , l = 1m .
We take the number of iterations N and the time step ∆t to be
N = 10000 , ∆t = 0.05s.
The initial angle and the angular velocity are given by
θ1 = 0.1 radian , ω1 = 0.
By using the conditional instruction if we can limit the total time of motion to
be equal to say 5 periods as follows
if (t(i + 1).ge.5 ∗period) exit.
(3) Compare between the value of the energy calculated with the Euler method
and the value of the energy calculated with the Euler–Cromer method. What
do you observe and what do you conclude.
(4) Repeat the computation using the Verlet algorithm. Remark that this method
can not self-start from the initial values θ1 and ω1 only. We must also provide
the angle θ2 which can be calculated using for example Euler, viz
θ2 = θ1 + ω1 ∆t.
We also remark that the Verlet algorithm does not require the calculation of the
angular velocity. However in order to calculate the energy we need to evaluate
the angular velocity which can be obtained from the expression
ωi = θi+1 −θi−1
2∆t
.
b
y.

Chapter 2
Classical Numerical Integration
2.1
Rectangular Approximation
We consider a generic one dimensional integral of the form
F =
Z b
a
f(x)dx.
(2.1)
In general this can not be done analytically. However this integral is straightforward
to do numerically. The starting point is Riemann deﬁnition of the integral F as the
area under the curve of the function f(x) from x = a to x = b. This is obtained as
follows. We discretize the x-interval so that we end up with N equal small intervals
of lenght ∆x, viz
xn = x0 + n∆x , ∆x = b −a
N
(2.2)
Clearly x0 = a and xN = b. Riemann deﬁnition is then given by the following limit
F = lim ∆x−→0 , N−→∞, b−a=ﬁxed


∆x
N−1
X
n=0
f(xn)

.
(2.3)
The ﬁrst approximation which can be made is to drop the limit. We get the so-called
rectangular approximation given by
FN = ∆x
N−1
X
n=0
f(xn).
(2.4)
General integration algorithms approximate the integral F by
FN =
N
X
n=0
f(xn)wn.
(2.5)
In other words we evaluate the function f(x) at N + 1 points in the interval [a, b]
then we sum the values f(xn) with some corresponding weights wn. For example
in the rectangular approximation (2.4) the values f(xn) are summed with equal
weights wn = ∆x, n = 0, N −1 and wN = 0. It is also clear that the estimation FN
of the integral F becomes exact only in the large N limit.
17

18
An Introduction to Monte Carlo Simulations of Matrix Field Theory
2.2
Trapezoidal Approximation
The trapezoid rule states that we can approximate the integral by a sum of trape-
zoids. In the subinterval [xn, xn+1] we replace the function f(x) by a straight line
connecting the two points (xn, f(xn)) and (xn+1, f(xn+1)). The trapezoid has as
vertical sides the two straight lines x = xn and x = xn+1. The base is the inter-
val ∆x = xn+1 −xn. It is not diﬃcult to convince ourselves that the area of this
trapezoid is
(f(xn+1) −f(xn))∆x
2
+ f(xn)∆x = (f(xn+1) + f(xn))∆x
2
.
(2.6)
The integral F computed using the trapezoid approximation is therefore given by
summing the contributions from all the N subinterval, viz
TN =
N−1
X
n=0
(f(xn+1) + f(xn))∆x
2
=
1
2f(x0) +
N−1
X
n=1
f(xn) + 1
2f(xN)

∆x.
(2.7)
We remark that the weights here are given by w0 = ∆x/2, wn = ∆x, n = 1, ..., N −1
and wN = ∆x/2.
2.3
Parabolic Approximation or Simpson’s Rule
In this case we approximate the function in the subinterval [xn, xn+1] by a parabola
given by
f(x) = αx2 + βx + γ.
(2.8)
The area of the corresponding box is thus given by
Z xn+1
xn
dx(αx2 + βx + γ) =
αx3
3
+ βx2
2
+ γx
xn+1
xn
.
(2.9)
Let us go back and consider the integral
Z 1
−1
dx(αx2 + βx + γ) = 2α
3 + 2γ.
(2.10)
We remark that
f(−1) = α −β + γ , f(0) = γ , f(1) = α + β + γ.
(2.11)
Equivalently
α = f(1) + f(−1)
2
−f(0) , β = f(1) −f(−1)
2
, γ = f(0).
(2.12)
Thus
Z 1
−1
dx(αx2 + βx + γ) = f(−1)
3
+ 4f(0)
3
+ f(1)
3
.
(2.13)

Classical Numerical Integration
19
In other words we can express the integral of the function f(x) = αx2 + βx + γ
over the interval [−1, 1] in terms of the values of this function f(x) at x = −1, 0, 1.
Similarly we can express the integral of f(x) over the adjacent subintervals [xn−1, xn]
and [xn, xn+1] in terms of the values of f(x) at x = xn+1, xn, xn−1, viz
Z xn+1
xn−1
dx f(x) =
Z xn+1
xn−1
dx(αx2 + βx + γ)
= ∆x
f(xn−1)
3
+ 4f(xn)
3
+ f(xn+1)
3

.
(2.14)
By adding the contributions from each pair of adjacent subintervals we get the full
integral
SN = ∆x
N−2
2
X
p=0
f(x2p)
3
+ 4f(x2p+1)
3
+ f(x2p+2)
3

.
(2.15)
Clearly we must have N (the number of subintervals) even. We compute
SN = ∆x
3
 f(x0) + 4f(x1) + 2f(x2) + 4f(x3) + 2f(x4) + ...
+ 2f(xN−2) + 4f(xN−1) + f(xN)

.
(2.16)
It is trivial to read from this expression the weights in this approximation.
Let us now recall the trapezoidal approximation given by
TN =

f(x0) + 2
N−1
X
n=1
f(xn) + f(xN)
∆x
2 .
(2.17)
Let us also recall that N∆x = b−a is the length of the total interval which is always
kept ﬁxed. Thus by doubling the number of subintervals we halve the width, viz
4T2N =

2f(ˆx0) + 4
2N−1
X
n=1
f(ˆxn) + 2f(ˆx2N)
∆x
2
=

2f(ˆx0) + 4
N−1
X
n=1
f(ˆx2n) + 4
N−1
X
n=0
f(ˆx2n+1) + 2f(ˆx2N)
∆x
2
=

2f(x0) + 4
N−1
X
n=1
f(xn) + 4
N−1
X
n=0
f(ˆx2n+1) + 2f(xN)
∆x
2 .
(2.18)
In above we have used the identiﬁcation ˆx2n = xn, n = 0, 1, ..., N −1, N. Thus
4T2N −TN =

f(x0) + 2
N−1
X
n=1
f(xn) + 4
N−1
X
n=0
f(ˆx2n+1) + f(xN)

∆ˆx
= 3SN.
(2.19)

20
An Introduction to Monte Carlo Simulations of Matrix Field Theory
2.4
Errors
The error estimates for numerical integration are computed as follows. We start
with the Taylor expansion
f(x) = f(xn) + (x −xn)f (1)(xn) + 1
2!(x −xn)2f (2)(xn) + ...
(2.20)
Thus Z xn+1
xn
dx f(x) = f(xn)∆x + 1
2!f (1)(xn)(∆x)2 + 1
3!f (2)(xn)(∆x)3 + ... (2.21)
The error in the interval [xn, xn+1] in the rectangular approximation is
Z xn+1
xn
dx f(x) −f(xn)∆x = 1
2!f (1)(xn)(∆x)2 + 1
3!f (2)(xn)(∆x)3 + ... (2.22)
This is of order 1/N 2. But we have N subintervals. Thus the total error is of order
1/N.
The error in the interval [xn, xn+1] in the trapezoidal approximation is
Z xn+1
xn
dx f(x) −1
2(f(xn) + f(xn+1))∆x
=
Z xn+1
xn
dx f(x) −1
2

2f(xn) + ∆xf (1)(xn)
+ 1
2!(∆x)2f (2)(xn) + ...

∆x
=
 1
3! −1
2
1
2!

f (2)(xn)(∆x)3 + ...
(2.23)
This is of order 1/N 3 and thus the total error is of order 1/N 2.
In order to compute the error in the interval [xn−1, xn+1] in the parabolic ap-
proximation we compute
Z xn
xn−1
dx f(x) +
Z xn+1
xn
dx f(x) = 2f(xn)∆x + 2
3!(∆x)3f (2)(xn)
+ 2
5!(∆x)5f (4)(xn) + ...
(2.24)
Also we compute
∆x
3 (f(xn+1) + f(xn−1) + 4f(xn)) = 2f(xn)∆x + 2
3!(∆x)3f (2)(xn)
+
2
3.4!(∆x)5f (4)(xn) + ...
(2.25)
Hence the error in the interval [xn−1, xn+1] in the parabolic approximation is
Z xn+1
xn−1
dx f(x) −∆x
3 (f(xn+1) + f(xn−1) + 4f(xn))
=
 2
5! −
2
3.4!

(∆x)5f (4)(xn) + ...
(2.26)
This is of order 1/N 5. The total error is therefore of order 1/N 4.

Classical Numerical Integration
21
2.5
Simulation 4: Numerical Integrals
(1) We take the integral
I =
Z 1
0
f(x)dx ; f(x) = 2x + 3x2 + 4x3.
Calculate the value of this integral using the rectangular approximation. Com-
pare with the exact result.
Hint: You can code the function using either “subroutine” or “function”.
(2) Calculate the numerical error as a function of N. Compare with the theory.
(3) Repeat the computation using the trapezoid method and the Simpson’s rule.
(4) Take now the integrals
I =
Z
π
2
0
cos xdx ,
I =
Z e
1
1
xdx , I =
Z +1
−1
lim
ϵ−→0
 1
π
ϵ
x2 + ϵ2

dx.

Chapter 3
Newton–Raphson Algorithms and
Interpolation
3.1
Bisection Algorithm
Let f be some function. We are interested in the solutions (roots) of the equation
f(x) = 0.
(3.1)
The bisection algorithm works as follows. We start with two values of x say x+ and
x−such that
f(x−) < 0 , f(x+) > 0.
(3.2)
In other words the function changes sign in the interval between x−and x+ and
thus there must exist a root between x−and x+. If the function changes from
positive to negative as we increase x we conclude that x+ ≤x−. We bisect the
interval [x+, x−] at
x = x+ + x−
2
.
(3.3)
If f(x)f(x+) > 0 then x+ will be changed to the point x otherwise x−will be
changed to the point x. We continue this process until the change in x becomes
insigniﬁcant or until the error becomes smaller than some tolerance. The relative
error is deﬁned by
error = x+ −x−
x
.
(3.4)
Clearly the absolute error e = xi −xf is halved at each iteration and thus the rate
of convergence of the bisection rule is linear. This is slow.
3.2
Newton–Raphson Algorithm
We start with a guess x0. The new guess x is written as x0 plus some unknown
correction ∆x, viz
x = x0 + ∆x.
(3.5)
Next we expand the function f(x) around x0, namely
f(x) = f(x0) + ∆x df
dx

x=x0
.
(3.6)
23

24
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The correction ∆x is determined by ﬁnding the intersection point of this linear
approximation of f(x) with the x axis. Thus
f(x0) + ∆x df
dx

x=x0
= 0 =⇒∆x = −
f(x0)
(df/dx)|x=x0
.
(3.7)
The derivative of the function f is required in this calculation.
In complicated
problems it is much simpler to evaluate the derivative numerically than analytically.
In these cases the derivative may be given by the forward-diﬀerence approximation
(with some δx not necessarily equal to ∆x)
df
dx

x=x0 = f(x0 + δx) −f(x0)
δx
.
(3.8)
In summary this method works by drawing the tangent to the function f(x) at the
old guess x0 and then use the intercept with the x axis as the new hopefully better
guess x. The process is repeated until the change in x becomes insigniﬁcant.
Next we compute the rate of convergence of the Newton–Raphson algorithm.
Starting from xi the next guess is xi+1 given by
xi+1 = xi −f(xi)
f ′(x).
(3.9)
The absolute error at step i is ϵi = x −xi while the absolute error at step i + 1 is
ϵi+1 = x −xi+1 where x is the actual root. Then
ϵi+1 = ϵi + f(xi)
f ′(x).
(3.10)
By using Taylor expansion we have
f(x) = 0 = f(xi) + (x −xi)f ′(xi) + (x −xi)2
2!
f ′′(xi) + ...
(3.11)
In other words
f(xi) = −ϵif ′(xi) −ϵ2
i
2! f ′′(xi) + ...
(3.12)
Therefore the error is given by
ϵi+1 = −ϵ2
i
2
f ′′(xi)
f ′(xi) .
(3.13)
This is quadratic convergence. This is faster than the bisection rule.
3.3
Hybrid Method
We can combine the certainty of the bisection rule in ﬁnding a root with the fast
convergence of the Newton–Raphson algorithm into a hybrid algorithm as follows.
First we must know that the root is bounded in some interval [a, c]. We can use for
example a graphical method. Next we start from some initial guess b. We take a
Newton–Raphson step
b′ = b −f(b)
f ′(b).
(3.14)

Newton–Raphson Algorithms and Interpolation
25
We check whether or not this step is bounded in the interval [a, c]. In other words
we must check that
a≤b −f(b)
f ′(b)≤c ⇔(b −c)f ′(b) −f(b)≤0≤(b −a)f ′(b) −f(b).
(3.15)
Therefore if
 (b −c)f ′(b) −f(b)
 (b −a)f ′(b) −f(b)

< 0.
(3.16)
Then the Newton–Raphson step is accepted else we take instead a bisection step.
3.4
Lagrange Interpolation
Let us ﬁrst recall that taylor expansion allows us to approximate a function at a
point x if the function and its derivatives are known in some neighbouring point x0.
The lagrange interpolation tries to approximate a function at a point x if only the
values of the function in several other points are known. Thus this method does
not require the knowledge of the derivatives of the function. We start from taylor
expansion
f(y) = f(x) + (y −x)f ′(x) + 1
2!(y −x)2f ′′(x) + ...
(3.17)
Let us assume that the function is known at three points x1, x2 and x3. In this
case we can approximate the function f(x) by some function p(x) and write
f(y) = p(x) + (y −x)p′(x) + 1
2!(y −x)2p′′(x).
(3.18)
We have
f(x1) = p(x) + (x1 −x)p′(x) + 1
2!(x1 −x)2p′′(x)
f(x2) = p(x) + (x2 −x)p′(x) + 1
2!(x2 −x)2p′′(x)
f(x3) = p(x) + (x3 −x)p′(x) + 1
2!(x3 −x)2p′′(x).
(3.19)
We can immediately ﬁnd
p(x) =
1
1 + a2 + a3
f(x1) +
a2
1 + a2 + a3
f(x2) +
a3
1 + a2 + a3
f(x3).
(3.20)
The coeﬃcients a2 and a3 solve the equations
a2(x2 −x)2 + a3(x3 −x)2 = −(x1 −x)2
a2(x2 −x) + a3(x3 −x) = −(x1 −x).
(3.21)
We ﬁnd
a2 = (x1 −x)(x3 −x1)
(x2 −x)(x2 −x3) , a3 = −(x1 −x)(x2 −x1)
(x3 −x)(x2 −x3).
(3.22)
Thus
1 + a2 + a3 = (x3 −x1)(x2 −x1)
(x2 −x)(x3 −x) .
(3.23)

26
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Therefore we get
p(x) =
(x −x2)(x −x3)
(x1 −x2)(x1 −x3)f(x1) + (x −x1)(x −x3)
(x2 −x1)(x2 −x3)f(x2)
+
(x −x1)(x −x2)
(x3 −x1)(x3 −x2)f(x3).
(3.24)
This is a quadratic polynomial.
Let x be some independent variable with tabulated values xi, i = 1, 2, ..., n. The
dependent variable is a function f(x) with tabulated values fi = f(xi). Let us then
assume that we can approximate f(x) by a polynomial of degree n −1, viz
p(x) = a0 + a1x + a2x2 + ... + an−1xn−1.
(3.25)
A polynomial which goes through the n points (xi, fi = f(xi)) was given by La-
grange. This is given by
p(x) = f1λ1(x) + f2λ2(x) + ... + fnλn(x).
(3.26)
λi(x) =
Yn
j(̸=i)=1
x −xj
xi −xj
.
(3.27)
We remark
λi(xj) = δij.
(3.28)
n
X
i=1
λi(x) = 1.
(3.29)
The Lagrange polynomial can be used to ﬁt the entire table with n equal the number
of points in the table. But it is preferable to use the Lagrange polynomial to to ﬁt
only a small region of the table with a small value of n. In other words use several
polynomials to cover the whole table and the ﬁt considered here is local and not
global.
3.5
Cubic Spline Interpolation
We consider n points (x1, f(x1)),(x2, f(x2)),...,(xn, f(xn)) in the plane. In every
interval xj≤x≤xj+1 we approximate the function f(x) with a cubic polynomial of
the form
p(x) = aj(x −xj)3 + bj(x −xj)2 + cj(x −xj) + dj.
(3.30)
We assume that
pj = p(xj) = f(xj).
(3.31)
In other words the pj for all j = 1, 2, ..., n −1 are known. From the above equation
we conclude that
dj = pj.
(3.32)

Newton–Raphson Algorithms and Interpolation
27
We compute
p′(x) = 3aj(x −xj)2 + 2bj(x −xj) + cj.
(3.33)
p′′(x) = 6aj(x −xj) + 2bj.
(3.34)
Thus we get by substituting x = xj into p′′(x) the result
bj = p′′
j
2 .
(3.35)
By substituting x = xj+1 into p′′(x) we get the result
aj = p′′
j+1 −p′′
j
6hj
.
(3.36)
By substituting x = xj+1 into p(x) we get
pj+1 = ajh3
j + bjh2
j + cjhj + pj.
(3.37)
By using the values of aj and bj we obtain
cj = pj+1 −pj
hj
−hj
6 (p′′
j+1 + 2p′′
j ).
(3.38)
Hence
p(x) = p′′
j+1 −p′′
j
6hj
(x −xj)3 + p′′
j
2 (x −xj)2
+
pj+1 −pj
hj
−hj
6 (p′′
j+1 + 2p′′
j )

(x −xj) + pj.
(3.39)
In other words the polynomials are determined from pj and p′′
j . The pj are known
given by pj = f(xj). It remains to determine p′′
j . We take the derivative of the
above equation
p′(x) = p′′
j+1 −p′′
j
2hj
(x −xj)2 + p′′
j (x −xj)
+
pj+1 −pj
hj
−hj
6 (p′′
j+1 + 2p′′
j )

.
(3.40)
This is the derivative in the interval [xj, xj+1]. We compute
p′(xj) =
pj+1 −pj
hj
−hj
6 (p′′
j+1 + 2p′′
j )

.
(3.41)
The derivative in the interval [xj−1, xj] is
p′(x) = p′′
j −p′′
j−1
2hj−1
(x −xj−1)2 + p′′
j−1(x −xj−1)
+
pj −pj−1
hj−1
−hj−1
6
(p′′
j + 2p′′
j−1)

.
(3.42)

28
An Introduction to Monte Carlo Simulations of Matrix Field Theory
We compute
p′(xj) = p′′
j −p′′
j−1
2
hj−1 + p′′
j−1hj−1 +
pj −pj−1
hj−1
−hj−1
6
(p′′
j + 2p′′
j−1)

.
(3.43)
By matching the two expressions for p′(xj) we get
hj−1p′′
j−1 + 2(hj + hj−1)p′′
j + hjp′′
j+1 = 6
pj+1 −pj
hj
−pj −pj−1
hj−1

.
(3.44)
These are n−2 equations since j = 2, ..., n−1 for n unknown p′′
j . We need two more
equations. These are obtained by computing the ﬁrst derivative p′(x) at x = x1
and x = xn. We obtain the two equations
h1(p′′
2 + 2p′′
1) = 6(p2 −p1)
h1
−6p′
1.
(3.45)
hn−1(p′′
n−1 + 2p′′
n) = −6(pn −pn−1)
hn−1
+ 6p′
n.
(3.46)
The n equations (3.44), (3.45) and (3.46) correspond to a tridiagonal linear system.
In general p′
1 and p′
n are not known. In this case we may use natural spline in which
the second derivative vanishes at the end points and hence
p2 −p1
h1
−p′
1 = pn −pn−1
hn−1
−p′
n = 0.
(3.47)
3.6
The Method of Least Squares
We assume that we have N data points (x(i), y(i)). We want to ﬁt this data to
some curve say a straight line yﬁt = mx + b. To this end we deﬁne the function
∆=
N
X
i=1
(y(i) −yﬁt(i))2 =
N
X
i=1
(y(i) −mx(i) −b)2.
(3.48)
The goal is to minimize this function with respect to b and m. We have
∂∆
∂m = 0 , ∂∆
∂b = 0.
(3.49)
We get the solution
b =
P
i x(i) P
j x(j)y(j) −P
i x(i)2 P
j y(j)
(P
i x(i))2 −N P
i x2
i
.
(3.50)
m =
P
i x(i) P
j y(j) −N P
i x(i)y(i)
(P
i x(i))2 −N P
i x2
i
.
(3.51)

Newton–Raphson Algorithms and Interpolation
29
3.7
Simulation 5: Newton–Raphson Algorithm
A particle of mass m moves inside a potential well of height V and length 2a centered
around 0. We are interested in the states of the system which have energies less
than V , i.e. bound states. The states of the system can be even or odd. The
energies associated with the even wave functions are solutions of the transcendental
equation
α tan αa = β.
α =
r
2mE
ℏ2
, β =
r
2m(V −E)
ℏ2
.
In the case of the inﬁnite potential well we ﬁnd the solutions
En = (n + 1
2)2π2ℏ2
2ma2
, n = 0, 1....
We choose (dropping units)
ℏ= 1 , a = 1 , 2m = 1.
In order to ﬁnd numerically the energies En we will use the Newton–Raphson algo-
rithm which allows us to ﬁnd the roots of the equation f(x) = 0 as follows. From
an initial guess x0, the ﬁrst approximation x1 to the solution is determined from
the intersection of the tangent to the function f(x) at x0 with the x-axis. This is
given by
x1 = x0 −f(x0)
f ′(x0).
Next by using x1 we repeat the same step in order to ﬁnd the second approximation
x2 to the solution. In general the approximation xi+1 to the desired solution in terms
of the approximation xi is given by the equation
xi+1 = xi −f(xi)
f ′(xi).
(1) For V = 10, determine the solutions using the graphical method. Consider the
two functions
f(α) = tan αa , g(α) = β
α =
r
V
α2 −1.
(2) Find using the method of Newton–Raphson the two solutions with a tolerance
equal 10−8. For the ﬁrst solution we take the initial guess α = π/a and for the
second solution we take the initial guess α = 2π/a.
(3) Repeat for V = 20.
(4) Find the 4 solutions for V = 100. Use the graphical method to determine the
initial step each time.
(5) Repeat the above questions using the bisection method.

Chapter 4
The Solar System:
The Runge–Kutta Methods
4.1
The Solar System
4.1.1
Newton’s Second Law
We consider the motion of the Earth around the Sun. Let r be the distance and Ms
and Me be the masses of the Sun and the Earth respectively. We neglect the eﬀect
of the other planets and the motion of the Sun (i.e. we assume that Ms >> Me).
The goal is to calculate the position of the Earth as a function of time. We start
from Newton’s second law of motion
Me
d2⃗r
dt2 = −GMeMs
r3
⃗r
= −GMeMs
r3
(x⃗i + y⃗j).
(4.1)
We get the two equations
d2x
dt2 = −GMs
r3 x.
(4.2)
d2y
dt2 = −GMs
r3 y.
(4.3)
We replace these two second-order diﬀerential equations by the four ﬁrst-order dif-
ferential equations
dx
dt = vx.
(4.4)
dvx
dt = −GMs
r3 x.
(4.5)
dy
dt = vy.
(4.6)
dvy
dt = −GMs
r3 y.
(4.7)
We recall
r =
p
x2 + y2.
(4.8)
31

32
An Introduction to Monte Carlo Simulations of Matrix Field Theory
4.1.2
Astronomical Units and Initial Conditions
The distance will be measured in astronomical units (AU) whereas time will be
measured in years. One astronomical unit of length (1 AU) is equal to the average
distance between the earth and the sun, viz 1 AU = 1.5×1011m. The astronomical
unit of mass can be found as follows. Assuming a circular orbit we have
Mev2
r
= GMsMe
r2
.
(4.9)
Equivalently
GMs = v2r.
(4.10)
The radius is r = 1 AU. The velocity of the earth is v = 2πr/yr = 2π AU/yr.
Hence
GMs = 4π2 AU3/yr2.
(4.11)
For the numerical simulations it is important to determine the correct initial con-
ditions. The orbit of Mercury is known to be an ellipse with eccentricity e = 0.206
and radius (semimajor axis) a = 0.39 AU with the Sun at one of the foci. The
distance between the Sun and the center is ea.
The ﬁrst initial condition is
x0 = r1, y0 = 0 where r1 is the maximum distance from Mercury to the Sun,
i.e. r1 = (1 + e)a = 0.47 AU. The second initial condition is the velocity (0, v1)
which can be computed using conservation of energy and angular momentum. For
example by comparing with the point (0, b) on the orbit where b is the semiminor
axis, i.e. b = a
√
1 −e2 the velocity (v2, 0) there can be obtained in terms of (0, v1)
from conservation of angular momentum as follows
r1v1 = bv2 ⇔v2 = r1v1
b
.
(4.12)
Next conservation of energy yields
1
2Mmv2
1 −GMsMm
r1
= 1
2Mmv2
2 −GMsMm
r2
.
(4.13)
In above r2 =
√
e2a2 + b2 is the distance between the Sun and Mercury when at
the point (0, b). By substituting the value of v2 we get an equation for v1. This is
given by
v1 =
r
GMs
a
1 −e
1 + e = 8.2 AU/yr.
(4.14)
4.1.3
Kepler’s Laws
Kepler’s laws are given by the following three statements:
• The planets move in elliptical orbits around the sun. The sun resides at one
focus.
• The line joining the sun with any planet sweeps out equal areas in equal times.

The Solar System: The Runge–Kutta Methods
33
• Given an orbit with a period T and a semimajor axis a the ratio T 2/a3 is a
constant.
The derivation of these three laws proceeds as follows. We work in polar coordinates.
Newton’s second law reads
Me¨⃗r = −GMsMe
r2
ˆr.
(4.15)
We use ˙ˆr = ˙θˆθ and ˙ˆθ = −˙θˆr to derive ˙⃗r = ˙rˆr + r ˙θˆθ and ¨⃗r = (¨r −r ˙θ2)ˆr + (r¨θ + 2 ˙r ˙θ)ˆθ.
Newton’s second law decomposes into the two equations
r¨θ + 2 ˙r ˙θ = 0.
(4.16)
¨r −r ˙θ2 = −GMs
r2 .
(4.17)
Let us recall that the angular momentum by unit mass is deﬁned by ⃗l = ⃗r × ˙⃗r =
r2 ˙θˆr × ˆθ. Thus l = r2 ˙θ. Equation (4.16) is precisely the requirement that angular
momentum is conserved. Indeed we compute
dl
dt = r(r¨θ + 2 ˙r ˙θ) = 0.
(4.18)
Now we remark that the area swept by the vector ⃗r in a time interval dt is dA =
(r × rdθ)/2 where dθ is the angle traveled by ⃗r during dt. Clearly
dA
dt = 1
2l.
(4.19)
In other words the planet sweeps equal areas in equal times since l is conserved.
This is Kepler’s second law.
The second equation (4.17) becomes now
¨r = l2
r3 −GMs
r2
(4.20)
By multiplying this equation with ˙r we obtain
d
dtE = 0 , E = 1
2 ˙r2 + l2
2r2 −GMs
r
.
(4.21)
This is precisely the statement of conservation of energy. E is the energy per unit
mass. Solving for dt in terms of dr we obtain
dt =
dr
s
2

E −
l2
2r2 + GMs
r

(4.22)
However dt = (r2dθ)/l. Thus
dθ =
ldr
r2
s
2

E −
l2
2r2 + GMs
r

(4.23)

34
An Introduction to Monte Carlo Simulations of Matrix Field Theory
By integrating this equation we obtain (with u = 1/r)
θ =
Z
ldr
r2
s
2

E −
l2
2r2 + GMs
r

= −
Z
du
q
2E
l2 + 2GMs
l2
u −u2
.
(4.24)
This integral can be done explicitly. We get
θ = −arccos
u −C
eC

+ θ′ , e =
s
1 + 2l2E
G2M 2s
, C = GMs
l2
.
(4.25)
By inverting this equation we get an equation of ellipse with eccentricity e since
E < 0, viz
1
r = C(1 + e cos(θ −θ′)).
(4.26)
This is Kepler’s ﬁrst law. The angle at which r is maximum is θ −θ′ = π. This
distance is precisely (1 + e)a where a is the semi-major axis of the ellipse since ea
is the distance between the Sun which is at one of the two foci and the center of
the ellipse. Hence we obtain the relation
(1 −e2)a = 1
C =
l2
GMs
.
(4.27)
From equation (4.19) we can derive Kepler’s third law. By integrating both sides
of the equation over a single period T and then taking the square we get
A2 = 1
4l2T 2.
(4.28)
A is the area of the ellipse, i.e. A = πab where the semi-minor axis b is related the
semi-major axis a by b = a
√
1 −e2. Hence
π2a4(1 −e2) = 1
4l2T 2.
(4.29)
By using equation (4.27) we get the desired formula
T 2
a3 = 4π2
GMs
.
(4.30)
4.1.4
The Inverse-Square Law and Stability of Orbits
Any object with mass generates a gravitational ﬁeld and thus gravitational ﬁeld
lines will emanate from the object and radiate outward to inﬁnity. The number
of ﬁeld lines N is proportional to the mass. The density of ﬁeld lines crossing a
sphere of radius r surrounding this object is given by N/4πr2. This is the origin of
the inverse-square law. Therefore any other object placed in this gravitational ﬁeld
will experience a gravitational force proportional to the number of ﬁeld lines which
intersect it. If the distance between this second object and the source is increased
the force on it will become weaker because the number of ﬁeld lines which intersect
it will decrease as we are further away from the source.

The Solar System: The Runge–Kutta Methods
35
4.2
Euler–Cromer Algorithm
The time discretization is
t ≡t(i) = i∆t , i = 0, ..., N.
(4.31)
The total time interval is T = N∆t. We deﬁne x(t) = x(i), vx(t) = vx(i), y(t) =
y(i), vy(t) = vy(i). Equations (4.4)–(4.8) become (with i = 0, ..., N)
vx(i + 1) = vx(i) −GMs
(r(i))3 x(i)∆t.
(4.32)
x(i + 1) = x(i) + vx(i)∆t.
(4.33)
vy(i + 1) = vy(i) −GMs
(r(i))3 y(i)∆t.
(4.34)
y(i + 1) = y(i) + vy(i)∆t.
(4.35)
r(i) =
p
x(i)2 + y(i)2.
(4.36)
This is Euler algorithm. It can also be rewritten with ˆx(i) = x(i−1), ˆy(i) = y(i−1),
ˆvx(i) = vx(i −1), ˆvy(i) = vy(i −1), ˆr(i) = r(i −1) and i = 1, ..., N + 1 as
ˆvx(i + 1) = ˆvx(i) −GMs
(ˆr(i))3 ˆx(i)∆t.
(4.37)
ˆx(i + 1) = ˆx(i) + ˆvx(i)∆t.
(4.38)
ˆvy(i + 1) = ˆvy(i) −GMs
(ˆr(i))3 ˆy(i)∆t.
(4.39)
ˆy(i + 1) = ˆy(i) + ˆvy(i)∆t.
(4.40)
ˆr(i) =
p
ˆx(i)2 + ˆy(i)2.
(4.41)
In order to maintain energy conservation we employ Euler-Cromer algorithm. We
calculate as in the Euler’s algorithm the velocity at time step i + 1 by using the
position and velocity at time step i. However we compute the position at time step
i + 1 by using the position at time step i and the velocity at time step i + 1, viz
ˆvx(i + 1) = ˆvx(i) −GMs
(ˆr(i))3 ˆx(i)∆t.
(4.42)
ˆx(i + 1) = ˆx(i) + ˆvx(i + 1)∆t.
(4.43)
ˆvy(i + 1) = ˆvy(i) −GMs
(ˆr(i))3 ˆy(i)∆t.
(4.44)
ˆy(i + 1) = ˆy(i) + ˆvy(i + 1)∆t.
(4.45)

36
An Introduction to Monte Carlo Simulations of Matrix Field Theory
4.3
The Runge–Kutta Algorithm
4.3.1
The Method
The problem is still trying to solve the ﬁrst order diﬀerential equation
dy
dx = f(x, y).
(4.46)
In the Euler’s method we approximate the function y = y(x) in each interval
[xn, xn+1] by the straight line
yn+1 = yn + ∆xf(xn, yn).
(4.47)
The slope f(xn, yn) of this line is exactly given by the slope of the function y = y(x)
at the beginning of the interval [xn, xn+1].
Given the value yn at xn we evaluate the value yn+1 at xn+1 using the method
of Runge–Kutta as follows. First the middle of the interval [xn, xn+1] which is at
the value xn + 1
2∆x corresponds to the y-value yn+1 calculated using the Euler’s
method, viz yn+1 = yn + 1
2k1 where
k1 = ∆xf(xn, yn).
(4.48)
Second the slope at this middle point (xn + 1
2∆x, yn + 1
2k1) which is given by
k2
∆x = f

xn + 1
2∆x, yn + 1
2k1

(4.49)
is the value of the slope which will be used to estimate the correct value of yn+1 at
xn+1 using again Euler’s method, namely
yn+1 = yn + k2.
(4.50)
In summary the Runge–Kutta algorithm is given by
k1 = ∆xf(xn, yn)
k2 = ∆xf

xn + 1
2∆x, yn + 1
2k1

yn+1 = yn + k2.
(4.51)
The error in this method is proportional to ∆x3. This can be shown as follows. We
have
y(x + ∆x) = y(x) + ∆xdy
dx + 1
2(∆x)2 d2y
dx2 + ...
= y(x) + ∆xf(x, y) + 1
2(∆x)2 d
dxf(x, y) + ...
= y(x) + ∆x

f(x, y) + 1
2∆x∂f
∂x + 1
2∆xf(x, y)∂f
∂y

+ ...
= y(x) + ∆xf

x + 1
2∆x, y + 1
2∆xf(x, y)

+ O(∆x3)
= y(x) + ∆xf

x + 1
2∆x, y + 1
2k1

+ O(∆x3)
= y(x) + k2 + O(∆x3).
(4.52)

The Solar System: The Runge–Kutta Methods
37
Let us ﬁnally note that the above Runge–Kutta method is strictly speaking the
second-order Runge–Kutta method. The ﬁrst-order Runge–Kutta method is the
Euler algorithm.
The higher-order Runge–Kutta methods will not be discussed
here.
4.3.2
Example 1: The Harmonic Oscillator
Let us apply this method to the problem of the harmonic oscillator. We have the
diﬀerential equations
dθ
dt = ω
dω
dt = −g
l θ.
(4.53)
Euler’s equations read
θn+1 = θn + ∆tωn
ωn+1 = ωn −g
l θn∆t.
(4.54)
First we consider the function θ = θ(t). The middle point is (tn + 1
2∆t, θn + 1
2k1)
where k1 = ∆tωn. For the function ω = ω(t) the middle point is (tn+ 1
2∆t, ωn+ 1
2k3)
where k3 = −g
l ∆tθn. Therefore we have
k1 = ∆tωn
k3 = −g
l ∆tθn.
(4.55)
The slope of the function θ(t) at its middle point is
k2
∆t = ωn + 1
2k3.
(4.56)
The slope of the function ω(t) at its middle point is
k4
∆t = −g
l

θn + 1
2k1

.
(4.57)
The Runge–Kutta solution is then given by
θn+1 = θn + k2
ωn+1 = ωn + k4.
(4.58)
4.3.3
Example 2: The Solar System
Let us consider the equations
dx
dt = vx.
(4.59)
dvx
dt = −GMs
r3 x.
(4.60)

38
An Introduction to Monte Carlo Simulations of Matrix Field Theory
dy
dt = vy.
(4.61)
dvy
dt = −GMs
r3 y.
(4.62)
First we consider the function x = x(t). The middle point is (tn + 1
2∆t, xn + 1
2k1)
where k1 = ∆t vxn. For the function vx = vx(t) the middle point is (tn + 1
2∆t,
vxn + 1
2k3) where k3 = −GMs
rn ∆t xn. Therefore we have
k1 = ∆t vxn
k3 = −GMs
r3n
∆t xn.
(4.63)
The slope of the function x(t) at the middle point is
k2
∆t = vxn + 1
2k3.
(4.64)
The slope of the function vx(t) at the middle point is
k4
∆t = −GMs
R3n

xn + 1
2k1

.
(4.65)
Next we consider the function y = y(t). The middle point is (tn + 1
2∆t, yn + 1
2k′
1)
where k′
1 = ∆t vyn. For the function vy = vy(t) the middle point is (tn + 1
2∆t,
vyn + 1
2k′
3) where k′
3 = −GMs
rn ∆t yn. Therefore we have
k′
1 = ∆t vyn
k′
3 = −GMs
r3n
∆t yn.
(4.66)
The slope of the function y(t) at the middle point is
k′
2
∆t = vyn + 1
2k′
3.
(4.67)
The slope of the function vy(t) at the middle point is
k′
4
∆t = −GMs
R3n

yn + 1
2k′
1

.
(4.68)
In the above equations
Rn =
s
xn + 1
2k1
2
+

yn + 1
2k′
1
2
.
(4.69)
The Runge–Kutta solutions are then given by
xn+1 = xn + k2
vx(n+1) = vxn + k4
yn+1 = yn + k′
2
vy(n+1) = vyn + k′
4.
(4.70)

The Solar System: The Runge–Kutta Methods
39
4.4
Precession of the Perihelion of Mercury
The orbit of Mercury is elliptic. The orientation of the axes of the ellipse rotate
with time. This is the precession of the perihelion (the point of the orbit nearest
to the Sun) of Mercury. Mercury’s perihelion makes one revolution every 23000
years. This is approximately 566 arcseconds per century. The gravitational forces
of the other planets (in particular Jupiter) lead to a precession of 523 arcseconds
per century. The remaining 43 arcseconds per century are accounted for by general
relativity.
For objects too close together (like the Sun and Mercury) the force of gravity
predicted by general relativity deviates from the inverse-square law. This force is
given by
F = GMsMm
r2
 1 + α
r2

, α = 1.1 × 10−8 AU2.
(4.71)
We discuss here some of the numerical results obtained with the Runge–Kutta
method for diﬀerent values of α. We take the time step and the number of iterations
to be N = 20000 and dt = 0.0001. The angle of the line joining the Sun and Mercury
with the horizontal axis when mercury is at the perihelion is found to change linearly
with time. We get the following rates of precession
α = 0.0008 , dθ
dt = 8.414 ± 0.019
α = 0.001 , dθ
dt = 10.585 ± 0.018
α = 0.002 , dθ
dt = 21.658 ± 0.019
α = 0.004 , dθ
dt = 45.369 ± 0.017.
(4.72)
Thus
dθ
dt = aα , α = 11209.2 ± 147.2 degrees/(yr.α).
(4.73)
By extrapolating to the value provided by general relativity, viz α = 1.1 × 10−8 we
get
dθ
dt = 44.4 ± 0.6 arcsec/century.
(4.74)
4.5
Exercises
Exercise 1:
Using the Runge–Kutta method solve the following diﬀerential equa-
tions
d2r
dt2 = l2
r3 −GM
r2 .
(4.75)
d2z
dt2 = −g.
(4.76)
dN
dt = aN −bN 2.
(4.77)

40
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Exercise 2:
The Lorenz model is a chaotic system given by three coupled ﬁrst
order diﬀerential equations
dx
dt = σ(y −x)
dy
dt = −xz + rx −y
dz
dt = xy −bz.
(4.78)
This system is a simpliﬁed version of the system of Navier–Stokes equations of ﬂuid
mechanics which are relevant for the Rayleigh–B´enard problem. Write down the
numerical solution of these equations according to the Runge–Kutta method.
4.6
Simulation 6: Runge–Kutta Algorithm: Solar System
Part I
We consider a solar system consisting of a single planet moving around the
Sun. We suppose that the Sun is very heavy compared to the planet that we can
safely assume that it is not moving at the center of the system. Newton’s second
law gives the following equations of motion
vx = dx
dt , dvx
dt = −GMs
r3 x , vy = dy
dt , dvy
dt = −GMs
r3 y.
We will use here the astronomical units deﬁned by GMs = 4π2 AU3/yr2.
(1) Write a Fortran code in which we implement the Runge–Kutta algorithm for
the problem of solving the equations of motion of the the solar system.
(2) Compute the trajectory, the velocity and the energy as functions of time. What
do you observe for the energy.
(3) According to Kepler’s ﬁrst law the orbit of any planet is an ellipse with the
Sun at one of the two foci. In the following we will only consider planets which
are known to have circular orbits to a great accuracy. These planets are Venus,
Earth, Mars, Jupiter and Saturn. The radii in astronomical units are given by
avenus = 0.72 , aearth = 1 , amars = 1.52 , ajupiter = 5.2 , asaturn = 9.54.
Verify that Kepler’s ﬁrst law indeed holds for these planets.
In order to answer questions 2 and 3 above we take the initial conditions
x(1) = a , y(1) = 0 , vx(1) = 0 , vy(1) = v.
The value chosen for the initial velocity is very important to get a correct
orbit and must be determined for example by assuming that the orbit is indeed
circular and as a consequence the centrifugal force is balanced by the force of
gravitational attraction. We get v =
p
GMs/a.
We take the step and the number of iterations ∆t = 0.01 yr , N = 103 −104.

The Solar System: The Runge–Kutta Methods
41
Part II
(1) According to Kepler’s third law the square of the period of a planet is directly
proportional to the cube of the semi-major axis of its orbit. For circular orbits
the proportionality factor is equal to 1 exactly. Verify this fact for the planets
mentioned above. We can measure the period of a planet by monitoring when
the planet returns to its farthest point from the sun.
(2) By changing the initial velocity appropriately we can obtain an elliptical orbit.
Check this thing.
(3) The fundamental laws governing the motion of the solar system are Newton’s
law of universal attraction and Newton’s second law of motion. Newton’s law
of universal attraction states that the force between the Sun and a planet is
inversely proportional to the square of the distance between them and it is
directed from the planet to the Sun.
We will assume in the following that
this force is inversely proportional to a diﬀerent power of the distance. Modify
the code accordingly and calculate the new orbits for powers between 1 and 3.
What do you observe and what do you conclude.
4.7
Simulation 7: Precession of the perihelion of Mercury
According to Kepler’s ﬁrst law the orbits of all planets are ellipses with the Sun at
one of the two foci. This law can be obtained from applying Newton’s second law to
the system consisting of the Sun and a single planet. The eﬀect of the other planets
on the motion will lead to a change of orientation of the orbital ellipse within the
orbital plane of the planet. Thus the point of closest approach (the perihelion) will
precess, i.e. rotate around the sun. All planets suﬀer from this eﬀect but because
they are all farther from the sun and all have longer periods than Mercury the
amount of precession observed for them is smaller than that of Mercury.
However it was established earlier on that the precession of the perihelion of
Mercury due to Newtonian eﬀects deviates from the observed precession by the
amount 43 arcsecond/century. As it turns out this can only be explained within
general relativity. The large mass of the Sun causes space and time around it to be
curved which is felt the most by Mercury because of its proximity. This spacetime
curvature can be approximated by the force law
F = GMsMm
r2
(1 + α
r2 ) , α = 1.1.10−8 AU 2.
(1) Include the above force in the code. The initial position and velocity of Mercury
are
x0 = (1 + e)a , y0 = 0.
vx0 = 0 , vy0 =
r
GMs
a
1 −e
1 + e.

42
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Thus initially Mercury is at its farthest point from the Sun since a is the semi-
major axis of Mercury (a = 0.39 AU) and e is its eccentricity (e = 0.206) and
hence ea is the distance between the Sun and the center of the ellipse. The semi-
minor axis is deﬁned by b = a
√
1 −e2. The initial velocity was calculated from
applying the principles of conservation of angular momentum and conservation
of energy between the above initial point and the point (0, b).
(2) The amount of precession of the perihelion of Mercury is very small because
α is very small.
In fact it can not be measured directly in any numerical
simulation with a limited amount of time. Therefore we will choose a larger
value of α for example α = 0.0008 AU2. We also work with N = 20000 , dt =
0.0001. Compute the orbit for these values. Compute the angle θ made between
the vector position of Mercury and the horizontal axis as a function of time.
Compute also the distance between Mercury and the sun and its derivative with
respect to time given by
dr
dt = xvx + yvy
r
.
This derivative will vanish each time Mercury reaches its farthest point from the
sun or its closest point from the sun (the perihelion). Plot the angle θp made
between the vector position of Mercury at its farthest point and the horizontal
axis as a function of time. What do you observe. Determine the slope dθp/dt
which is precisely the amount of precession of the perihelion of Mercury for the
above value of α.
(3) Repeat the above question for other values of α say α = 0.001, 0.002, 0.004.
Each time compute dθp/dt. Plot dθp/dt as a function of α. Determine the
slope. Deduce the amount of precession of the perihelion of Mercury for the
value of α = 1.1.10−8 AU2.

Chapter 5
Chaotic Pendulum
5.1
Equation of Motion
We start from a simple pendulum. The equation of motion is given by
mld2θ
dt2 = −mg sin θ.
(5.1)
We consider the eﬀect of air resistance on the motion of the mass m. We will assume
that the force of air resistance is given by Stokes’ law. We get
mld2θ
dt2 = −mg sin θ −mlq dθ
dt .
(5.2)
The air friction will drain all energy from the pendulum. In order to maintain the
motion against the damping eﬀect of air resistance we will add a driving force. We
will choose a periodic force with amplitude mlFD and frequency ωD. This arise for
example if we apply a periodic electric ﬁeld with amplitude ED and frequency ωD
on the mass m which is assumed to have an electric charge q, i.e mlFD = qED.
It can also arise from the periodic oscillations of the pendulum’s pivot point. By
adding the driving force we get then the equation of motion
mld2θ
dt2 = −mg sin θ −mlq dθ
dt + mlFD cos ωDt.
(5.3)
The natural frequency of the oscillations is given by the frequency of the simple
pendulum, viz
ω0 =
rg
l .
(5.4)
We will always take ω0 = 1, i.e. l = g. The equation of motion becomes
d2θ
dt2 + 1
Q
dθ
dt + sin θ = FD cos ωDt.
(5.5)
The coeﬃcient Q = 1/q is known as the quality factor. It measures how many oscil-
lations the pendulum without driving force will make before its energy is drained.
43

44
An Introduction to Monte Carlo Simulations of Matrix Field Theory
We will write the above second order diﬀerential equation as two ﬁrst order diﬀer-
ential equations, namely
dθ
dt = Ω
dΩ
dt = −1
QΩ−sin θ + FD cos ωDt.
(5.6)
This system of diﬀerential equations does not admit a simple analytic solution. The
linear approximation corresponds to small amplitude oscillations, viz
sin θ ≃θ.
(5.7)
The diﬀerential equations become linear given by
dθ
dt = Ω
dΩ
dt = −1
QΩ−θ + FD cos ωDt.
(5.8)
Or equivalently
dθ2
dt2 = −1
Q
dθ
dt −θ + FD cos ωDt.
(5.9)
For FD = 0 the solution is given by
θt0 =

θ(0) cos ω∗t + 1
ω∗

Ω(0) + θ(0)
2Q

sin ω∗t

e−
t
2Q , ω∗=
r
1 −
1
4Q2 .
(5.10)
For FD ̸= 0 a particular solution is given by
θ∞= FD(a cos ωDt + b sin ωDt).
(5.11)
We ﬁnd
a =
1
(1 −ω2
D)2 + ω2
D
Q2
(1 −ω2
D)
b =
1
(1 −ω2
D)2 + ω2
D
Q2
ωD
Q .
(5.12)
For FD ̸= 0 the general solution is given by
θ = θ∞+ θt.
(5.13)
θt =

θ(0) −
FD(1 −ω2
D)
(1 −ω2
D)2 + ω2
D
Q2

cos ω∗t
+ 1
ω∗

Ω(0) + θ(0)
2Q −1
2Q
FD(1 −3ω2
D)
(1 −ω2
D)2 + ω2
D
Q2

sin ω∗t

e−
t
2Q .
(5.14)

Chaotic Pendulum
45
The last two terms depend on the initial conditions and will vanish exponentially at
very large times t −→∞, i.e. they are transients. The asymptotic motion is given
by θ∞. Thus for t −→∞we get
θ = θ∞= FD(a cos ωDt + b sin ωDt).
(5.15)
Also for t −→∞we get
Ω= dθ
dt = FDωD(−a sin ωDt + b cos ωDt).
(5.16)
We compute in the limit of large times t −→∞
θ2 + Ω2
ω2
D
= ˜F 2
D = F 2
D(a2 + b2) =
F 2
D
(1 −ω2
D)2 + ω2
D
Q2
.
(5.17)
In other words the orbit of the system in phase space is an ellipse. The motion is
periodic with period equal to the period of the driving force. This ellipse is also
called a periodic attractor because regardless of the initial conditions the trajectory
of the system will tend at large times to this ellipse.
Let us also remark that the maximum angular displacement is ˜FD. The function
˜FD = ˜FD(ωD) exhibits resonant behavior as the driving frequency approaches the
natural frequency which is equivalent to the limit ωD −→1. In this limit ˜FD =
QFD. The width of the resonant window is proportional to 1/Q so for Q −→∞we
observe that ˜FD −→∞when ωD −→1 while for Q −→0 we observe that ˜FD −→0
when ωD −→1.
In general the time-asymptotic response of any linear system to a periodic drive
is periodic with the same period as the driving force. Furthermore when the driv-
ing frequency approaches one of the natural frequencies the response will exhibits
resonant behavior.
The basic ingredient in deriving the above results is the linearity of the dy-
namical system. As we will see shortly periodic motion is not the only possible
time-asymptotic response of a dynamical system to a periodic driving force.
5.2
Numerical Algorithms
The equations of motion are
dθ
dt = Ω
dΩ
dt = −1
QΩ−sin θ + F(t).
(5.18)
The external force is periodic and it will be given by one of the following expressions
F(t) = FD cos ωDt.
(5.19)
F(t) = FD sin ωDt.
(5.20)

46
An Introduction to Monte Carlo Simulations of Matrix Field Theory
5.2.1
Euler–Cromer Algorithm
Numerically we can employ the Euler–Cromer algorithm in order to solve this sys-
tem of diﬀerential equations. The solution goes as follows. First we choose the
initial conditions. For example
Ω(1) = 0
θ(1) = 0
t(1) = 0.
(5.21)
For i = 1, ..., N + 1 we use
Ω(i + 1) = Ω(i) + ∆t

−1
QΩ(i) −sin θ(i) + F(i)

θ(i + 1) = θ(i) + ∆t Ω(i + 1)
t(i + 1) = ∆t i.
(5.22)
F(i) ≡F(t(i)) = FD cos ωD∆t(i −1).
(5.23)
F(i) ≡F(t(i)) = FD sin ωD∆t(i −1).
(5.24)
5.2.2
Runge–Kutta Algorithm
In order to achieve better precision we employ the Runge–Kutta algorithm. For
i = 1, ..., N + 1 we use
k1 = ∆t Ω(i)
k2 = ∆t

Ω(i) + 1
2k3

, k3 = ∆t

−1
QΩ(i) −sin θ(i) + F(i)

k4 = ∆t

−1
Q

Ω(i) + 1
2k3

−sin

θ(i) + 1
2k1

+ F

i + 1
2

.
(5.25)
θ(i + 1) = θ(i) + k2
Ω(i + 1) = Ω(i) + k4
t(i + 1) = ∆t i.
(5.26)
F(i) ≡F(t(i)) = FD cos ωD∆t(i −1).
(5.27)
F(i) ≡F(t(i)) = FD sin ωD∆t(i −1).
(5.28)
F

i + 1
2

≡F

t(i) + 1
2∆t

= FD cos ωD∆t

i −1
2

.
(5.29)
F

i + 1
2

≡F

t(i) + 1
2∆t

= FD sin ωD∆t

i −1
2

.
(5.30)

Chaotic Pendulum
47
5.3
Elements of Chaos
5.3.1
Butterﬂy Eﬀect: Sensitivity to Initial Conditions
The solution in the linear regime (small amplitude) reads
θ = θ∞+ θt.
(5.31)
The transient is of the form
θt = f(θ(0), Ω(0))e−t/2Q.
(5.32)
This goes to zero at large times t. The time-asymptotic is thus given by
θ∞= FD(a cos ωDt + b sin ωDt).
(5.33)
The motion in the phase space is periodic with period equal to the period of the
driving force. The orbit in phase space is precisely an ellipse of the form
θ2
∞+ Ω2
∞
ω2
D
= F 2
D(a2 + b2).
(5.34)
Let us consider a perturbation of the initial conditions. We can imagine that we
have two pendulums A and B with slightly diﬀerent initial conditions. Then the
diﬀerence between the two trajectories is
δθ = δf(θ(0), Ω(0))e−t/2Q.
(5.35)
This goes to zero at large times. If we plot ln δθ as a function of time we ﬁnd a
straight line with a negative slope. The time-asymptotic motion is not sensitive
to initial conditions. It converges at large times to θ∞no matter what the initial
conditions are. The curve θ∞= θ∞(Ω∞) is called a (periodic) attractor. This is
because any perturbed trajectory will decay exponentially in time to the attractor.
In order to see chaotic behavior we can for example increase Q keeping everything
else ﬁxed. We observe that the slope of the line ln δθ = λt starts to decrease until
at some value of Q it becomes positive. At this value the variation between the
two pendulums increases exponentially with time. This is the chaotic regime. The
value λ = 0 is the value where chaos happens. The coeﬃcient λ is called Lyapunov
exponent.
The chaotic pendulum is a deterministic system (since it obeys ordinary dif-
ferential equations) but it is not predictable in the sense that given two identical
pendulums their motions will diverge from each other in the chaotic regime if there
is the slightest error in determining their initial conditions. This high sensitivity to
initial conditions is known as the butterﬂy eﬀect and could be taken as the deﬁnition
of chaos itself.
However we should stress here that the motion of the chaotic pendulum is not
random. This can be seen by inspecting Poincar´e sections.

48
An Introduction to Monte Carlo Simulations of Matrix Field Theory
5.3.2
Poincar´e Section and Attractors
The periodic motion of the linear system with period equal to the period of the
driving force is called a period-1 motion.
In this motion the trajectory repeats
itself exactly every one single period of the external driving force. This is the only
possible motion in the low amplitude limit.
Generally a period-N motion corresponds to an orbit of the dynamical system
which repeats itself every N periods of the external driving force. These orbits exist
in the non-linear regime of the pendulum.
The Poincar´e section is deﬁned as follows. We plot in the θ–Ωphase space only
one point per period of the external driving force. We plot for example (θ, Ω) for
ωDt = φ + 2πn.
(5.36)
The angle φ is called the Poincar´e phase and n is an integer. For period-1 motion
the Poincar´e section consists of one single point. For period-N motion the Poincar´e
section consists of N points.
Thus in the linear regime if we plot (θ, Ω) for ωDt = 2πn we get a single point
since the motion is periodic with period equal to that of the driving force. The
single point we get as a Poincar´e section is also an attractor since all pendulums
with almost the same initial conditions will converge onto it.
In the chaotic regime the Poincar´e section is an attractor known as strange
attractor.
It is a complicated curve which could have fractal structure and all
pendulums with almost the same initial conditions will converge onto it.
5.3.3
Period-Doubling Bifurcations
In the case of the chaotic pendulum we encounter between the linear regime and
the emergence of chaos the so-called period doubling phenomena.
In the linear
regime the Poincar´e section is a point P which corresponds to a period-1 motion
with period equal TD = 2π/ωD. The θ or Ωcoordinate of this point P will trace
a line as we increase Q while keeping everything ﬁxed. We will eventually reach a
value Q1 of Q where this line bifurcates into two lines. By close inspection we see
that at Q1 the motion becomes period-2 motion, i.e. the period becomes equal to
2TD.
In a motion where the period is TD (below Q1) we get the same value of θ each
time t = mTD and since we are plotting θ each time t = 2nπ/ωD = nTD we will
get a single point in the Poincar´e section. In a motion where the period is 2TD (at
Q2) we get the same value of θ each time t = 2mTD, i.e. the value of θ at times
t = mTD is diﬀerent and hence we get two points in the Poincar´e section.
As we increase Q the motion becomes periodic with period equal 4TD, then
with period equal 8TD and so on. The motion with period 2N TD is called period-N
motion. The corresponding Poincar´e section consists of N distinct points.
The diagram of θ as a function of Q is called a bifurcation diagram. It has a
fractal structure. Let us point out here that normally in ordinary oscillations we get

Chaotic Pendulum
49
harmonics with periods equal to the period of the driving force divided by 2N . In
this case we obtained in some sense subharmonics with periods equal to the period
of the driving force times 2N . This is very characteristic of chaos. In fact chaotic
behavior corresponds to the limit N −→∞. In other words chaos is period-∞
(bounded) motion which could be taken as another deﬁnition of chaos.
5.3.4
Feigenbaum Ratio
Let QN be the critical value of Q above which the Nth bifurcation is triggered. In
other words QN is the value where the transition to period-N motion happens. We
deﬁne the Feigenbaum ratio by
FN = QN−1 −QN−2
QN −QN−1
.
(5.37)
It is shown that FN −→F = 4.669 as N −→∞. This is a universal ratio called
the Feigenbaum ratio and it characterizes many chaotic systems which suﬀer a
transition to chaos via an inﬁnite series of period-doubling bifurcations. The above
equation can be then rewritten as
QN = Q1 + (Q2 −Q1)
N −2
X
j=0
1
F j
(5.38)
Let us deﬁne the accumulation point by Q∞then
Q∞= Q1 + (Q2 −Q1)
F
F −1
(5.39)
This is where chaos occur. In the bifurcation diagram the chaotic region is a solid
black region.
5.3.5
Spontaneous Symmetry Breaking
The bifurcation process is associated with a deep phenomenon known as sponta-
neous symmetry breaking. The ﬁrst period-doubling bifurcation corresponds to the
breaking of the symmetry t −→t + TD. The linear regime respects this symmetry.
However period-2 motion and in general period-N motions with N > 2 do not
respect this symmetry.
There is another kind of spontaneous symmetry breaking which occurs in the
chaotic pendulum and which is associated with a bifurcation diagram. This happens
in the region of period-1 motion and it is the breaking of spatial symmetry or parity
θ −→−θ. Indeed there exists solutions of the equations of motion that are either
left-favoring or right-favoring. In other words the pendulums in such solutions spend
much of its time in the regions to the left of the pendulum’s vertical (θ < 0) or to the
right of the pendulum’s vertical (θ > 0). This breaking of left-right symmetry can
be achieved by a gradual increase of Q. We will then reach either the left-favoring
solution or the right-favoring solution starting from a left-right symmetric solution
depending on the initial conditions. The symmetry θ −→−θ is also spontaneously
broken in period-N motions.

50
An Introduction to Monte Carlo Simulations of Matrix Field Theory
5.4
Simulation 8: The Butterﬂy Eﬀect
We consider a pendulum of a mass m and a length l moving under the inﬂuence of
the force of gravity, the force of air resistance and a driving periodic force. Newton’s
second law of motion reads
d2θ
dt2 = −g
l sin θ −q dθ
dt + FD sin 2πνDt.
We will always take the angular frequency
p
g/l associated with simple oscillations
of the pendulum equal to 1, i.e. l = g. The numerical solution we will consider here
is based on the Euler–Cromer algorithm.
The most important property of a large class of solutions of this diﬀerential
equation is hyper sensitivity to initial conditions known also as the butterﬂy eﬀect
which is the deﬁning characteristic of chaos. For this reason the driven non-linear
pendulum is also known as the chaotic pendulum.
The chaotic pendulum can have two distinct behaviors. In the linear regime
the motion (neglecting the initial transients) is periodic with a period equal to the
period of the external driving force. In the chaotic regime the motion never repeats
and any error even inﬁnitesimal in determining the initial conditions will lead to a
completely diﬀerent orbit in the phase space.
(1) Write a code which implements the Euler–Cromer algorithm for the chaotic
pendulum. The angle θ must always be taken between −π and π which can be
maintained as follows
if(θi.lt. ∓π) θi = θi ± 2π.
(2) We take the values and initial conditions
dt = 0.04s , 2πνD = 2
3s−1 , q = 1
2s−1 , N = 1000 −2000.
θ1 = 0.2 radian , Ω1 = 0 radian/s.
FD = 0 radian/s2 , FD = 0.1 radian/s2 , FD = 1.2 radian/s2.
Plot θ as a function of time. What do you observe for the ﬁrst value of FD.
What is the period of oscillation for small and large times for the second value
of FD. Is the motion periodic for the third value of FD.
5.5
Simulation 9: Poincar´e Sections
In the chaotic regime the motion of the pendulum although deterministic is not
predictable.
This however does not mean that the motion of the pendulum is
random which can clearly be seen from the Poincar´e sections.
A Poincar´e section is a curve in the phase space obtained by plotting one point
of the orbit per period of the external drive. Explicitly we plot points (θ, Ω) which

Chaotic Pendulum
51
corresponds to times t = n/νD where n is an integer.
In the linear regime of
the pendulum the Poincar´e section consists of a single point. Poincar´e section in
the chaotic regime is a curve which does not depend on the initial conditions thus
conﬁrming that the motion is not random and which may have a fractal structure.
As a consequence this curve is called a strange attractor.
(1) We consider two identical chaotic pendulums A and B with slightly diﬀerent
initial conditions. For example we take
θA
1 = 0.2 radian , θB
1 = 0.201 radian.
The diﬀerence between the two motions can be measured by
∆θi = θA
i −θB
i .
Compute ln ∆θ as a function of time for
FD = 0.1 radian/s2 , FD = 1.2 radian/s2.
What do you observe. Is the two motions identical. What happens for large
times. Is the motion of the pendulum predictable. For the second value of FD
use
N = 10000 , dt = 0.01s.
(2) Compute the angular velocity Ωas a function of θ for
FD = 0.5 radian/s2 , FD = 1.2 radian/s2.
What is the orbit in the phase space for small times and what does it represent.
What is the orbit for large times. Compare between the two pendulums A and
B. Does the orbit for large times depend on the initial conditions.
(3) A Poincar´e section is obtained numerically by plotting the points (θ, Ω) of the
orbit at the times at which the function sin πνDt vanishes. These are the times
at which this function changes sign. This is implemented as follows
if(sin πνDti sin πνDti+1.lt.0)then
write(∗, ∗)ti, θi, Ωi.
Verify that Poincar´e section in the linear regime is given by a single point in the
phase space. Take and use FD = 0.5 radian/s2, N = 104 −107, dt = 0.001s.
Verify that Poincar´e section in the chaotic regime is also an attractor. Take
and use FD = 1.2 radian/s2, N = 105, dt = 0.04s. Compare between Poincar´e
sections of the pendulums A and B. What do you observe and what do you
conclude.

52
An Introduction to Monte Carlo Simulations of Matrix Field Theory
5.6
Simulation 10: Period Doubling
Among the most important chaotic properties of the driven non-linear pendulum
is the phenomena of period doubling. The periodic orbit with period equal to the
period of the external driving force are called period-1 motion. There exist however
other periodic orbits with periods equal twice, four times and in general 2N times
the period of the external driving force. The orbit with period equal 2N times the
period of the external driving force is called period-N motion. The period doubling
observed in the driven non-linear pendulum is a new phenomena which belongs to
the world of chaos. In the standard phenomena of mixing the response of a non-
linear system to a single frequency external driving force will contain components
with periods equal to the period of the driving force divided by 2N . In other words
we get “harmonics” as opposed to the “subharmonics” we observe in the chaotic
pendulum.
For period-N motion we expect that there are N diﬀerent values of the angle
θ for every value of FD. The function θ = θ(FD) is called a bifurcation diagram.
Formally the transition to chaos occurs at N −→∞. In other words chaos is deﬁned
as period-inﬁnity motion.
(1) We take the values and initial conditions
l = g , 2πνD = 2
3s−1 , q = 1
2s−1 , N = 3000 −100000 , dt = 0.01s.
θ1 = 0.2 radian , Ω1 = 0 radian/s.
Determine the period of the motion for
FD = 1.35 radian/s2 , FD = 1.44 radian/s2 , FD = 1.465 radian/s2.
What happens to the period when we increase FD. Does the two second values
of FD lie in the linear or chaotic regime of the chaotic pendulum.
(2) Compute the angle θ as a function of FD for the times t which satisfy the
condition 2πνDt = 2nπ. We take FD in the interval
FD = (1.34 + 0.005k) radian/s2 , k = 1, ..., 30.
Determine the interval of the external driving force in which the orbits are
period-1, period-2 and period-4 motions.
In this problem it is very important to remove the initial transients before we start
measuring the bifurcation diagram. This can be done as follows. We calculate the
motion for 2N steps but then only consider the last N steps in the computation of
the Poincar´e section for every value of FD.

Chaotic Pendulum
53
5.7
Simulation 11: Bifurcation Diagrams
Part I
The chaotic pendulum is given by the equation
d2θ
dt2 = −sin θ −1
Q
dθ
dt + FD cos 2πνDt.
In this simulation we take the values FD = 1.5 radian/s2 and 2πνD = 2
3s−1. In
order to achieve a better numerical precision we use the second-order Runge–Kutta
algorithm.
In the linear regime the orbits are periodic with period equal to the period TD
of the external driving force and are symmetric under θ −→−θ. There exists other
solutions which are periodic with period equal TD but are not symmetric under
θ −→−θ. In these solutions the pendulum spends the majority of its time in the
region to the left of its vertical (θ < 0) or in the region to the right of its vertical
(θ > 0).
These symmetry breaking solutions can be described by a bifurcation diagram
Ω= Ω(Q). For every value of the quality factor Q we calculate the Poincar´e section.
We observe that the Poincar´e section will bifurcate at some value Q∗of Q. Below
this value we get one line whereas above this value we get two lines corresponding
to the two symmetry breaking solutions in which the pendulum spends the majority
of its time in the regions (θ > 0) and (θ < 0).
(1) Rewrite the code for the chaotic pendulum using Runge–Kutta algorithm.
(2) We take two diﬀerent sets of initial conditions
θ = 0.0 radian, Ω= 0.0 radian/s.
θ = 0.0 radian, Ω= −3.0 radian/s.
Study the nature of the orbit for the values Q = 0.5s, Q = 1.24s and Q = 1.3s.
What do you observe.
(3) Plot the bifurcation diagram Ω= Ω(Q) for values of Q in the interval [1.2, 1.3].
What is the value Q∗at which the symmetry θ −→−θ is spontaneously broken.
Part II
As we have seen in the previous simulation period doubling can also
be described by a bifurcation diagram. This phenomena is also an example of a
spontaneous symmetry breaking. In this case the symmetry is t −→t+TD. Clearly
only orbits with period TD are symmetric under this transformation.
Let QN be the value of Q at which the Nth bifurcation occurs. In other words
this is the value at which the orbit goes from being a period-(N −1) motion to a
period-N motion. The Feigenbaum ratio is deﬁned by
FN = QN−1 −QN −2
QN −QN−1
.
As we approach the chaotic regime, i.e. as N −→∞the ratio FN converges rapidly
to the constant value F = 4.669. This is a general result which holds for many

54
An Introduction to Monte Carlo Simulations of Matrix Field Theory
chaotic systems. Any dynamical system which can exhibit a transition to chaos via
an inﬁnite series of period-doubling bifurcations is characterized by a Feigenbaum
ratio which approaches 4.669 as N −→∞.
(1) Calculate the orbit and Poincar´e section for Q = 1.36s. What is the period of
the motion. Is the orbit symmetric under t −→t + TD. Is the orbit symmetric
under θ −→−θ.
(2) Plot the bifurcation diagram Ω= Ω(Q) for two diﬀerent sets of initial conditions
for values of Q in the interval [1.3, 1.36]. What is the value Q at which the period
gets doubled. What is the value of Q at which the symmetry t −→t + TD is
spontaneously broken.
(3) In this question we use the initial conditions
θ = 0.0 radian, Ω= 0.0 radian/s.
Calculate the orbit and Poincare section and plot the bifurcation diagram Ω=
Ω(Q) for values of Q in the interval [1.34, 1.38]. Determine from the bifurcation
diagram the values QN for N = 1, 2, 3, 4, 5. Calculate the Feigenbaum ratio.
Calculate the accumulation point Q∞at which the transition to chaos occurs.

Chapter 6
Molecular Dynamics
6.1
Introduction
In the molecular dynamics approach we attempt to understand the behavior of a
classical many-particle system by simulating the trajectory of each particle in the
system. In practice this can be applied to systems containing 109 particles at most.
The molecular dynamics approach is complementary to the more powerful Monte
Carlo method. The Monte Carlo method deals with systems that are in thermal
equilibrium with a heat bath. The molecular dynamics approach on the other hand
is useful in studying how fast in real time a system moves from one microscopic
state to another.
We consider a box containing a collection of atoms or molecules. We will use
Newton’s second law to calculate the positions and velocities of all the molecules
as functions of time.
Some of the questions we can answer with the molecular
dynamics approach are:
• The melting transition.
• The rate of equilibration.
• The rate of diﬀusion.
As state above molecular dynamics allows us to understand classical systems. A
classical treatment can be justiﬁed as follows. We consider the case of liquid argon
as an example. The energy required to excite an argon atom is of the order of 10
eV while the typical kinetic energy of the center of mass of an argon atom is 0.1 eV.
Thus a collision between two argon atoms will not change the electron conﬁguration
of either atoms. Hence for all practical purposes we can ignore the internal structure
of argon atoms. Furthermore the wavelength of an argon atom which is of the order
of 10−7 A is much smaller than the spacing between argon atoms typically of the
order of 1 A which again justiﬁes a classical treatment.
55

56
An Introduction to Monte Carlo Simulations of Matrix Field Theory
6.2
The Lennard-Jones Potential
We consider a box containing N argon atoms. For simplicity we will assume that
our argon atoms move in two dimensions. The equations of motion of the ith atom
which is located at the position (xi, yi) with velocity (vi,x, vi,y) read
dvi,x
dt
= ax,i , dxi
dt = vi,x.
(6.1)
dvi,y
dt
= ay,i , dyi
dt = vi,y.
(6.2)
Each argon atom experience a force from all other argon atoms. In order to calculate
this force we need to determine the interaction potential.
We assume that the
interaction potential between any pair of argon atoms depend only on the distance
between them.
Let rij and u(rij) be the distance and the interaction potential between atoms
i and j. The total potential is then given by
U =
N−1
X
i=1
N
X
j=i+1
u(rij).
(6.3)
The precise form of u can be calculated from ﬁrst principles. In other words, from
quantum mechanics.
However this calculation is very complicated and in most
circumstances a phenomenological form of u will be suﬃcient.
For large separations rij the potential u(rij) must be weakly attractive given
by the Van der Walls force which arises from electrostatic interaction between the
electric dipole moments of the two argon atoms. In other words u(rij) for large rij
is attractive due to the mutual polarization of the two atoms. The Van der Walls
potential can be computed from quantum mechanics where it is shown that it varies
as 1/r6
ij. For small separations rij the potential u(rij) must become strongly repul-
sive due to the overlap of the electron clouds of the two argon atoms. This repulsion
known also as core repulsion is a consequence of Pauli exclusion principle. It is a
common practice to choose the repulsive part of the potential u to be proportional
to 1/r12
ij . The total potential takes the form
u(r) = 4ϵ
σ
r
12
−
σ
r
6
.
(6.4)
This is the Lennard-Jones potential. The parameter σ is of dimension length while ϵ
is of dimension energy. We observe that at r = σ the potential is 0 identically while
for r > 2.5σ the potential approaches zero rapidly. The minimum of the potential
occurs at r = 21/6σ. The depth of the potential at the minimum is ϵ.
The force of atom k on atom i is
⃗fk,i = −⃗∇k,iu(rk,i) = 24ϵ
rki

2
 σ
rki
12
−
 σ
rki
6
ˆrki.
(6.5)

Molecular Dynamics
57
The acceleration of the ith atom is given by
ax,i = 1
m
X
k̸=i
fk,i cos θk,i
= 1
m
X
k̸=i
fk,i
xi −xk
rki
= 24ϵ
m
X
k̸=i
xi −xk
r2
ki

2
 σ
rki
12
−
 σ
rki
6
.
(6.6)
ay,i = 1
m
X
k̸=i
fk,i sin θk,i
= 1
m
X
k̸=i
fk,i
yi −yk
rki
= 24ϵ
m
X
k̸=i
yi −yk
r2
ki

2
 σ
rki
12
−
 σ
rki
6
.
(6.7)
6.3
Units, Boundary Conditions and Verlet Algorithm
Reduced Units
We choose σ and ϵ as the units of distance and energy respec-
tively. We also choose the unit of mass to be the mass m of a single argon atom.
Everything else is measured in terms of σ, ϵ and m. For example velocity is mea-
sured in units of (ϵ/m)1/2 and time in units of σ(ϵ/m)1/2. The reduced units are
given by
σ = ϵ = m = 1.
(6.8)
For argon atoms we have the values
σ = 3.4 × 10−10m, ϵ = 1.65 × 10−21J = 120kBJ, m = 6.69 × 10−26kg.
(6.9)
Thus
σ
rm
ϵ = 2.17 × 10−12s.
(6.10)
Hence a molecular dynamics simulation which runs for 2000 steps with a reduced
time step ∆t = 0.01 corresponds to a total reduced time 2000 × 0.01 = 20 which is
equivalent to a real time 20σ(ϵ/m)1/2 = 4.34 × 10−11s.
Periodic Boundary Conditions
The total number of atoms in a real physical
system is huge of the order of 1023. If the system is placed in a box the fraction of
atoms of the system near the walls of the box is negligible compared to the total
number of atoms. In typical simulations the total number of atoms is only of the
order of 103−105 and in this case the fraction of atoms near the walls is considerable
and their eﬀect can not be neglected.

58
An Introduction to Monte Carlo Simulations of Matrix Field Theory
In order to reduce edge eﬀects we use periodic boundary conditions. In other
words the box is eﬀectively a torus and there are no edges. Let Lx and Ly be the
lengths of the box in the x and y directions respectively. If an atom crosses the
walls of the box in a particular direction we add or subtract the length of the box
in that direction as follows
if (x > Lx) then x = x −Lx
if (x < 0) then x = x + Lx.
(6.11)
if (y > Ly) then y = y −Ly
if (y < 0) then y = y + Ly.
(6.12)
The maximum separation in the x direction between any two particles is only Lx/2
whereas the maximum separation in the y direction between any two particles is
only Ly/2. This can be implemented as follows
if (xij > +Lx/2) then xij = xij −Lx
if (xij < −Lx/2) then xij = xij + Lx.
(6.13)
if (yij > +Ly/2) then yij = yij −Ly
if (yij < −Ly/2) then yij = yij + Ly.
(6.14)
Verlet Algorithm
The numerical algorithm we will use is Verlet algorithm. Let
us consider the forward and backward Taylor expansions of a function f given by
f(tn + ∆t) = f(tn) + ∆tdf
dt

tn + 1
2(∆t)2 d2f
dt2

tn + 1
6(∆t)3 d3f
dt3

tn + ...
(6.15)
f(tn −∆t) = f(tn) −∆tdf
dt

tn + 1
2(∆t)2 d2f
dt2

tn −1
6(∆t)3 d3f
dt3

tn + ...
(6.16)
Adding these expressions we get
f(tn + ∆t) = 2f(tn) −f(tn −∆t) + (∆t)2 d2f
dt2

tn + O(∆t4).
(6.17)
We remark that the error is proportional to ∆t4 which is less than the errors in
the Euler, Euler–Cromer and second-order Runge–Kutta methods so this method
is more accurate. We have therefore for the ith atom
xi,n+1 = 2xi,n −xi,n−1 + (∆t)2ax,i,n.
(6.18)
yi,n+1 = 2yi,n −yi,n−1 + (∆t)2ay,i,n.
(6.19)
The force and the acceleration are given by
fk,i,n = 24ϵ
rki,n

2
 σ
rki,n
12
−
 σ
rki,n
6
.
(6.20)

Molecular Dynamics
59
ax,i,n = 1
m
X
k̸=i
fk,i,n
xi,n −xk,n
rki,n
.
(6.21)
ay,i,n = 1
m
X
k̸=i
fk,i,n
yi,n −yk,n
rki,n
.
(6.22)
The separation rki,n between the two atoms k and i is given by
rki,n =
q
(xi,n −xk,n)2 + (yi,n −yk,n).
(6.23)
In the Verlet method it is not necessary to calculate the components dxi,n/dt and
dyi,n/dt of the velocity. However since the velocity will be needed for other purposes
we will also compute it using the equations
vx,i,n = xi,n+1 −xi,n−1
2∆t
.
(6.24)
vy,i,n = yi,n+1 −yi,n−1
2∆t
.
(6.25)
Let us remark that the Verlet method is not self starting. In other words given the
initial conditions xi,1, yi,1, vx,i,1 and vy,i,1 we need also to know xi,2, yi,2, vx,i,2 and
vy,i,2 for the algorithm to start which can be determined using the Euler method.
6.4
Some Physical Applications
6.4.1
Dilute Gas and Maxwell Distribution
A gas in thermal equilibrium is characterized by a temperature T. Molecular dy-
namics allows us to study how a dilute gas approaches equilibrium. The temper-
ature of the gas can be computed using the molecular dynamics simulations as
follows. According to the equipartition theorem the average thermal energy of each
quadratic degree of freedom in a gas in thermal equilibrium is equal to kBT/2. In
other words
1
2kBT = 1
d
D1
2m⃗v2E
.
(6.26)
The average


can be understood in two diﬀerent but equivalent ways. We can
follow the motion of a single atom and take the time average of its kinetic energy.
The same result can be obtained by taking the average of the kinetic energy over
the diﬀerent atoms. In this latter case we write
1
2kBT =
1
dN
N
X
i=1
1
2m⃗v2
i .
(6.27)
Another way of measuring the temperature T of a dilute gas is through a study
of the distribution of atom velocities. A classical gas in thermal equilibrium obeys

60
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Maxwell distribution. The speed and velocity distributions in two dimensions are
given respectively by
P(v) = C
v
kBT e−mv2
2kBT .
(6.28)
P(vx) = Cx
1
√kBT e−
mv2
x
2kBT , P(vy) = Cy
1
√kBT e−
mv2
y
2kBT .
(6.29)
Recall that the probability per unit v of ﬁnding an atom with speed v is equal
P(v) whereas the probability per unit vx,y of ﬁnding an atom with velocity vx,y is
equal P(vx,y). The constants C and Cx,y are determined from the normalization
conditions. There are peaks in the distributions P(v) and P(vx,y). Clearly the
temperature is related to the location of the peak which occurs in P(v). This is
given by
kBT = mv2
peak.
(6.30)
6.4.2
The Melting Transition
This is a very important subject which we will discuss at great length in the second
lab problem of this chapter.
6.5
Simulation 12: Maxwell Distribution
We consider the motion in two dimensions of N argon atoms in an L × L box. The
interaction potential u between any two atoms in the gas separated by a distance
r is given by the Lennard-Jones potential. The numerical algorithm we will use is
Verlet algorithm.
In this problem we will always take L odd and N a perfect square. The lattice
spacing is deﬁned by
a =
L
√
N
.
Clearly there are N cells of area a × a. We choose L and N such that a > 2σ.
For simplicity we will use reduced units σ = ϵ = m = 1. In order to reduce edge
eﬀects we use periodic boundary conditions. In other words the box is eﬀectively
a torus and there are no edges. Thus the maximum separation in the x direction
between any two particles is only L/2 and similarly the maximum separation in the
y direction between any two particles is only L/2.
The initial positions of the atoms are ﬁxed as follows. The atom k =
√
N(i−1)+j
will be placed at the center of the cell with corners (i, j), (i + 1, j), (i, j + 1) and
(i + 1, j + 1). Next we perturb in a random way these initial positions by adding
random numbers in the interval [−a/4, +a/4] to the x and y coordinates of the
atoms. The initial velocities can be chosen in random directions with a speed equal
v0 for all atoms.

Molecular Dynamics
61
(1) Write a molecular dynamics code along the above lines. Take L = 15, N = 25,
∆t = 0.02, Time = 500 and v0 = 1. As a ﬁrst test verify that the total energy
is conserved. Plot the trajectories of the atoms. What do you observe.
(2) As a second test we propose to measure the temperature by observing how the
gas approaches equilibrium. Use the equipartition theorem
kBT = m
2N
N
X
i=1
(v2
i,x + v2
i,y).
Plot T as a function of time. Take Time = 1000−1500. What is the temperature
of the gas at equilibrium.
(3) Compute the speed distribution of the argon atoms by constructing an appro-
priate histogram as follows. We take the value Time = 2000. We consider the
speeds of all particles at all times. There are Time×N values of the speed in this
sample. Construct the histogram for this sample by (1) ﬁnding the maximum
and minimum, (2) dividing the interval into bins, (3) determining the number
of times a given value of the speed falls in a bin and (4) properly normalizing
the distribution. Compare with the Maxwell distribution
PMaxwell(v) = C v2
kBT e−mv2
2kBT .
Deduce the temperature from the peak of the distribution given by kBT =
mv2
peak. Compare with the value of the temperature obtained from the equipar-
tition theorem. What happens if we increase the initial speed.
6.6
Simulation 13: Melting Transition
We would like to study the melting transition. First we need to establish the correct
conditions for a solid phase. Clearly the temperature must be suﬃciently low and
the density must be suﬃciently high. To make the temperature as low as possible we
will start with all particles at rest. In order to obtain maximum attraction between
atoms we choose a low density of approximately one particle per unit reduced area.
In particular we choose N = 16 and L = 4.
(1) Show that with these conditions you obtain a crystalline solid with a triangular
lattice.
(2) In order to observe melting we must heat up the system. This can be achieved by
increasing the kinetic energy of the atoms by hand. A convenient way of doing
this is to rescale the current and previous positions of the atoms periodically
(say every 1000 steps) as follows
hh = int(n/1000)
if (hh ∗1000.eq.n) then
x(i, n) = x(i, n + 1) −R(x(i, n + 1) −x(i, n))
y(i, n) = y(i, n + 1) −R(y(i, n + 1) −y(i, n))
endif.

62
An Introduction to Monte Carlo Simulations of Matrix Field Theory
This procedure will rescale the velocity by the amount R. We choose R = 1.5.
Verify that we will indeed reach the melting transition by means of this method.
What happens to the energy and the temperature.

Chapter 7
Pseudo Random Numbers
and Random Walks
7.1
Random Numbers
A sequence of numbers r1, r2,... is called random if there are no correlations between
the numbers. The sequence is called uniform if all numbers have an equal probability
to occur. More precisely let the probability that a number ri in the sequence occurs
between r and r + dr be P(r)dr where P(r) is the probability distribution.
A
uniform distribution corresponds P(r) = constant.
Most random number generators on computers generate uniform distributions
between 0 and 1. These are sequences of pseudo random numbers since given ri
and its preceding elements we can compute ri+1. Therefore these sequences are
not really random and correlations among the numbers of the sequence exist. True
random numbers can be found in tables of random numbers determined during say
radioactive decay or other naturally occurring random physical phenomena.
7.1.1
Linear Congruent or Power Residue Method
In this method we generate a set of k random numbers r1, r2, ..., rk in the interval
[0, M −1] as follows. Given a random number ri−1 we generate the next random
number ri by the rule
ri = (ari−1 + c) mod M = remainder
ari−1 + c
M

.
(7.1)
The notation y = z mod M means that we subtract M from z until 0≤y≤M −1.
The ﬁrst random number r1 is supplied by the user and it is called the seed. Also
supplied are the multiplier a, the increment c and the modulus M. The remainder
is a built-in function in most computer languages.
The largest possible integer
number generated by the above rule is M −1. Thus the maximum possible period
is M, i.e. k≤M. In general the period k depends on a, c and M. To get a uniform
sequence in the interval [0, 1] we divide by M −1.
Let us take the following example a = 4,c = 1 and M = 9 with seed r1 = 3. We
get a sequence of length 9 given by
3, 4, 8, 6, 7, 2, 0, 1, 5.
(7.2)
63

64
An Introduction to Monte Carlo Simulations of Matrix Field Theory
After the last number 5 we get 3 and therefore the sequence will repeat. In this
case the period is M = 9.
It is clear that we need to choose the parameters a, c and M and the seed r1
with care so that we get the longest sequence of pseudo random numbers. The
maximum possible period depends on the size of the computer word.
A 32-bit
machine may use M = 231 = 2 × 109. The numbers generated by (7.1) are random
only in the sense that they are evenly distributed over their range. Equation (7.1) is
related to the logistic map which is known to exhibit chaotic behaviour. Although
chaos is deterministic it looks random. In the same way although equation (7.1)
is deterministic the numbers generated by it look random. This is the reason why
they are called pseudo random numbers.
7.1.2
Statistical Tests of Randomness
Period:
The ﬁrst obvious test is to verify that the random number generator has
a suﬃciently long period for a given problem. We can use the random number
generator to plot the position of a random walker. Clearly the plot will repeat itself
when the period is reached.
Uniformity:
The kth moment of the random number distribution is
⟨xk
i ⟩= 1
N
N
X
i=1
xk
i .
(7.3)
Let P(x) be the probability distribution of the random numbers. Then
⟨xk
i ⟩=
Z 1
0
dx xkP(x) + O
 1
√
N

.
(7.4)
For a uniform distribution P(x) = 1 we must have
⟨xk
i ⟩=
1
k + 1 + O
 1
√
N

.
(7.5)
In the words
√
N
 1
N
N
X
i=1
xk
i −
1
k + 1

= O(1).
(7.6)
This is a test of uniformity as well as of randomness. To be more precise if ⟨xk
i ⟩is
equal to 1/(k + 1) then we can infer that the distribution is uniform whereas if the
deviation varies as 1/
√
N then we can infer that the distribution is random.
A direct test of uniformity is to divide the unit interval into K equal subintevals
(bins) and place each random number in one of these bins. For a uniform distribu-
tion we must obtain N/K numbers in each bin where N is the number of generated
random numbers.

Pseudo Random Numbers and Random Walks
65
Chi-Square Statistic:
In the above test there will be statistical ﬂuctuations about
the ideal value N/K for each bin. The question is whether or not these ﬂuctuations
are consistent with the laws of statistics.
The answer is based on the so-called
chi-square statistic deﬁned by
χ2
m =
K
X
i=1
(Ni −nideal)2
nideal
.
(7.7)
In the above deﬁnition Ni is the number of random numbers which fall into bin i
and nideal is the expected number of random numbers in each bin.
The probability of ﬁnding any particular value χ2 which is less than χ2
m is found
to be proportional to the incomplete gamma function γ(ν/2, χ2
m/2) where ν is the
number of degrees of freedom given by ν = K −1. We have
P(χ2 ≤χ2
m) = γ(ν/2, χ2
m/2)
Γ(ν/2)
≡P(ν/2, χ2
m/2).
(7.8)
The most likely value of χ2
m, for some ﬁxed number of degrees of freedom ν, corre-
sponds to the value P(ν/2, χ2
m/2) = 0.5. In other words in half of the measurements
(bin tests), for some ﬁxed number of degrees of freedom ν, the chi-square statistic
predicts that we must ﬁnd a value of χ2
m smaller than the maximum.
Randomness:
Let r1, r2,...,rN be a sequence of random numbers. A very eﬀective
test of randomness is to make a scatterplot of (xi = r2i, yi = r2i+1) for many i.
There must be no regularity in the plot otherwise the sequence is not random.
Short-Term Correlations:
Let us deﬁne the autocorrelation function
C(j) = ⟨xixi+j⟩−⟨xi⟩⟨xi+j⟩
⟨xixi⟩−⟨xi⟩2
= ⟨xixi+j⟩−⟨xi⟩2
⟨xixi⟩−⟨xi⟩2
, j = 1, 2, ...
(7.9)
In the above equation we have used the fact that ⟨xi+j⟩= ⟨xi⟩for a large sample,
i.e. the choice of the origin of the sequence is irrelevant in that case and
⟨xixi+j⟩=
1
N −j
N−j
X
i=1
xixi+j.
(7.10)
Again if xi and xi+j are independent random numbers which are distributed with
the joint probability distribution P(xi, xi+j) then
⟨xixi+j⟩≃
Z 1
0
dx
Z 1
0
dyxyP(x, y).
(7.11)
We have clearly assumed that N is large. For a uniform distribution, viz P(x, y) = 1
we get
⟨xixi+j⟩≃1
4.
(7.12)
For a random distribution the deviation from this result is of order 1/
√
N. Hence
in the case that the random numbers are not correlated we have
C(j) = 0.
(7.13)

66
An Introduction to Monte Carlo Simulations of Matrix Field Theory
7.2
Random Systems
Both quantum and statistical physics deal with systems that are random or
stochastic. These are non deterministic systems as opposed to classical systems.
The dynamics of a deterministic system is given by a unique solution to the equa-
tions of motion which describes the physics of the system at all times.
We take the case of the diﬀusion of ﬂuid molecules. For example the motion of
dust particles in the atmosphere, the motion of perfume molecules in the air or the
motion of milk molecules in a coﬀee. These are all cases of a Brownian motion.
In the case of a drop of milk in a coﬀee the white mass of the drop of milk will
slowly spread until the coﬀee takes on a uniform brown color. At the molecular
level each milk molecule collides with molecules in the coﬀee. Clearly it will change
direction so frequently that its motion will appear random. This trajectory can be
described by a random walk. This is a system in which each milk molecule moves
one step at a time in any direction with equal probability.
The trajectory of a dust, perfume or milk molecule is not really random since
it can in principle be computed by solving Newton’s equations of motion for all
molecules which then allows us to know the evolution of the system in time. Al-
though this is possible in principle it will not be feasible in practice. The random
walk is thus eﬀectively an approximation. However the large number of molecules
and collisions in the system makes the random walk a very good approximation.
7.2.1
Random Walks
Let us consider a one dimensional random walk. It can take steps of length unity
along a line. It begins at s0 = 0 and the ﬁrst step is chosen randomly to be either
to the left or to right with equal probabilities. In other words there is a 50 per cent
chance that the walker moves to the point s1 = +1 and a 50 per cent chance that it
moves to the point s1 = −1. Next the walker will again move either to the right or
to the left from the point s1 to the point s2 with equal probabilities. This process
will be repeated N times and we get the position of the walker xN as a function
of the step number N. In the motion of a molecule in a solution the time between
steps is a constant and hence the step number N is proportional to time. Therefore
xN is the position of the walker as a function of time.
In general a one-dimensional random walker can move to the right with prob-
ability p and to the left with probability q = 1 −p with steps of equal length a.
The direction of each step is independent of the previous one. The displacement or
position of the walker after N steps is
xN =
N
X
i=1
si.
(7.14)
The walker for p = q = 1/2 can be generated by ﬂipping a coin N times. The
position is increased by a for heads and decreased by a for tails.

Pseudo Random Numbers and Random Walks
67
Averaging over many walks each consisting of N steps we get
⟨xN⟩=
N
X
i=1
⟨si⟩= N⟨s⟩.
(7.15)
In above we have used the fact that the average over every step is the same given
by
⟨si⟩= ⟨s⟩= p(a) + q(−a) = (p −q)a.
(7.16)
For p = q = 1/2 we get ⟨xN⟩= 0. A better measure of the walk is given by
x2
N =
 N
X
i=1
si
2
.
(7.17)
The mean square net displacement ∆x2 is deﬁned by
∆x2 = ⟨
 xN −⟨xN⟩
2⟩= ⟨x2
N⟩−⟨xN⟩2.
(7.18)
We compute
∆x2 =
N
X
i=1
N
X
j=1
⟨(si −⟨s⟩)(sj −⟨s⟩)⟩
=
N
X
i̸=j=1
⟨(si −⟨s⟩)(sj −⟨s⟩)⟩+
N
X
i=1
⟨(si −⟨s⟩)2⟩.
(7.19)
In the ﬁrst term since i ̸= j we have ⟨(si −⟨s⟩)(sj −⟨s⟩)⟩= ⟨(si −⟨s⟩)⟩⟨(sj −⟨s⟩)⟩.
But ⟨(si −⟨s⟩)⟩= 0. Thus
∆x2 =
N
X
i=1
⟨(si −⟨s⟩)2⟩
= N(⟨s2
i ⟩−⟨s⟩2)
= N(a2 −(p −q)2a2)
= 4Npqa2.
(7.20)
For p = q = 1/2 and a = 1 we get
⟨x2
N⟩= N.
(7.21)
The main point is that since N is proportional to time we have ⟨x2
N⟩∝t. This is
an example of a diﬀusive behaviour.
7.2.2
Diﬀusion Equation
The random walk is successful in simulating many physical systems because it is
related to the solutions of the diﬀusion equation. To see this we start from the
probability P(i, N) that the random walker is at site si after N steps. This is given
by
P(i, N) = 1
2
 P(i + 1, N −1) + P(i −1, N −1)

.
(7.22)

68
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Let τ be the time between steps and a the lattice spacing. Then t = Nτ and x = ia.
Also we deﬁne P(x, t) = P(i, N)/a. We get
P(x, t) = 1
2
 P(x + a, t −τ) + P(x −a, t −τ)

.
(7.23)
Let us rewrite this equation as
1
τ
 P(x, t) −P(x, t −τ)

= a2
2τ

P(x + a, t −τ) −2P(x, t −τ)
+ P(x −a, t −τ)
 1
a2 .
(7.24)
In the limit a −→0, τ −→0 with the ratio D = a2/2τ kept ﬁxed we obtain the
equation
∂P(x, t)
∂t
= D∂2P(x, t)
∂x2
.
(7.25)
This is the diﬀusion equation. Generalization to 3-dimensions is
∂P(x, y, z, t)
∂t
= D∇2P(x, y, z, t).
(7.26)
A particular solution of (7.25) is given by
P(x, t) = 1
σ e−x2
2σ2 , σ =
√
2Dt.
(7.27)
In other words the spatial distribution of the diﬀusing molecules is always a gaussian
with half-width σ increasing with time as
√
t.
The average of any function f of x is given by
⟨f(x, t)⟩=
Z
f(x)P(x, t)dx.
(7.28)
Let us multiply both sides of (7.25) by f(x) and then integrate over x, viz
Z
f(x)∂P(x, t)
∂t
dx = D
Z
f(x)∂2P(x, t)
∂x2
dx.
(7.29)
Clearly
Z
f(x)∂P(x, t)
∂t
dx =
Z
∂
∂t
 f(x)P(x, t)

dx
= d
dt
Z
f(x)P(x, t)dx
= d
dt⟨f(x)⟩.
(7.30)
Thus
d
dt⟨f(x)⟩= D
Z
f(x)∂2P(x, t)
∂x2
dx
= D

f(x)∂P(x, t)
∂x

|x=+∞
x=−∞−D
Z ∂f(x)
∂x
∂P(x, t)
∂x
dx.
(7.31)

Pseudo Random Numbers and Random Walks
69
We have P(x = ±∞, t) = 0 and also all spatial derivatives are zero at x = ±∞. We
then get
d
dt⟨f(x)⟩= −D
Z ∂f(x)
∂x
∂P(x, t)
∂x
dx.
(7.32)
Let us choose f(x) = x. Then
d
dt⟨x⟩= −D
Z ∂P(x, t)
∂x
dx = 0.
(7.33)
In other words ⟨x⟩= constant and since x = 0 at t = 0 we must have constant = 0.
Thus
⟨x⟩= 0.
(7.34)
Let us next choose f(x) = x2. Then
d
dt⟨x2⟩= −2D
Z
x∂P(x, t)
∂x
dx
= 2D.
(7.35)
Hence
⟨x2⟩= 2Dt.
(7.36)
This is the diﬀusive behaviour we have observed in the random walk problem.
7.3
The Random Number Generators RAN 0, 1, 2
Linear congruential generators are of the form
ri = (ari−1 + c) mod M.
(7.37)
For c > 0 the linear congruential generators are called mixed. They are denoted by
LCG(a, c, M). The random numbers generated with LCG(a, c, M) are in the range
[0, M −1].
For c = 0 the linear congruential generators are called multiplicative. They are
denoted by MLCG(a, M). The random numbers generated with MLCG(a, M) are
in the range [1, M −1].
In the case that a is a primitive root modulo M and M is a prime the period of
the generator is M −1. A number a is a primitive root modulo M means that for
any integer n such that gcd(n, M) = 1 there exists a k such that ak = n mod M.
An example of MLCG is RAN0 due to Park and Miller which is used extensively
on IBM computers. In this case
a = 16807 = 75 , M = 231 −1.
(7.38)
The period of this generator is not very long given by
period = 231 −2 ≃2.15 × 109.
(7.39)

70
An Introduction to Monte Carlo Simulations of Matrix Field Theory
This generator can not be implemented directly in a high level language because of
integer overﬂow. Indeed the product of a and M −1 exceeds the maximum value for
a 32-bit integer. Assemply language implementation using 64-bit product register
is straightforward but not portable.
A better solution is given by Schrage’s algorithm. This algorithm allows the
multiplication of two 32-bit integers without using any intermediate numbers which
are larger than 32 bits. To see how this works explicitly we factor M as
M = aq + r.
(7.40)
r = M mod a , q =
M
r

.
(7.41)
In the above equation [ ] denotes integer part. Remark that
r = M mod a = M −
M
a

a.
(7.42)
Thus by deﬁnition r < a. We will also demand that r < q and hence
r
qa << 1.
(7.43)
We have also
Xi+1 = aXi mod M = aXi −
aXi
M

M
= aXi −
 aXi
aq + r

M.
(7.44)
We compute
aXi
aq + r =
Xi
q + r
a
= Xi
q
1
1 + r
qa
= Xi
q

1 −r
qa

= Xi
q −Xi
aq
r
q .
(7.45)
Clearly
Xi
aq =
Xi
M −r ≃Xi
M < 1.
(7.46)
Hence
aXi
M

=
Xi
q

,
(7.47)
if neglecting ϵ = (rXi)/(aq2) does not aﬀect the integer part of aXi/M and
aXi
M

=
Xi
q

−1,
(7.48)

Pseudo Random Numbers and Random Walks
71
if neglecting ϵ does aﬀect the integer part of aXi/M. Therefore we get
Xi+1 = aXi −
aXi
M

(aq + r)
= a

Xi −
aXi
M

q

−
aXi
M

r
(7.49)
= a

Xi −
Xi
q

q

−
Xi
q

r
(7.50)
= a(Xi mod q) −
Xi
q

r,
(7.51)
if
a(Xi mod q) −
Xi
q

r ≥0.
(7.52)
Also
Xi+1 = aXi −
aXi
M

(aq + r)
= a

Xi −
aXi
M

q

−
aXi
M

r
(7.53)
= a

Xi −
Xi
q

q + q

−
Xi
q

r + r
(7.54)
= a(Xi mod q) −
Xi
q

r + M,
(7.55)
if
a(Xi mod q) −
Xi
q

r < 0.
(7.56)
The generator RAN0 contains serial correlations.
For example D-dimensional vectors (x1, ..., xD), (xD+1, ..., x2D),...
which are
obtained by successive calls of RAN0 will lie on a small number of parallel (D −
1)-dimensional hyperplanes.
Roughly there will be M 1/D such hyperplanes.
In
particular successive points (xi, xi+1) when binned into a 2−dimensional plane for
i = 1, ..., N will result in a distribution which fails the χ2 test for N ≥107 which is
much less than the period M −1.
The RAN1 is devised so that the correlations found in RAN0 is removed using
the Bays-Durham algorithm. The Bays-Durham algorithm shuﬄes the sequence
to remove low-order serial correlations. In other words it changes the order of the
numbers so that the sequence is not dependent on order and a given number is not
correlated with previous numbers. More precisely the jth random number is output
not on the jth call but on a randomized later call which is on average the j + 32th
call on.
The RAN2 is an improvement over RAN1 and RAN0 due to L’Ecuyer. It uses
two sequences with diﬀerent periods so as to obtain a new sequence with a larger

72
An Introduction to Monte Carlo Simulations of Matrix Field Theory
period equal to the least common multiple of the two periods. In this algorithm we
add the two sequences modulo the modulus M of one of them. In order to avoid
overﬂow we subtract rather than add and if the result is negative we add M −1 so
as to wrap around into the interval [0, M −1]. L’Ecuyer uses the two sequences
M1 = 2147483563 , a1 = 40014 , q1 = 53668 , r1 = 12211.
(7.57)
M2 = 2147483399 , a2 = 40692 , q2 = 52774 , r2 = 3791.
(7.58)
The period is 2.3 × 1018.
Let us also point out that RAN2 uses Bays-Durham
algorithm in order to implement an additional shuﬄe.
We conclude this section by discussing another generator based on the linear
congruential method which is the famous random number generator RAND given
by
RAND = LCG(69069, 1, 232).
(7.59)
The period of this generator is 232 and lattice structure is present for higher dimen-
sions D ≥6.
7.4
Simulation 14: Random Numbers
Part I
We consider a linear congruential pseudo-random number generator given
by
ri+1 = remainder
ari + c
M

.
We take the values
a = 899, c = 0, M = 32768, r1 = 12, “good”
a = 57, c = 1, M = 256, r1 = 10, “bad”.
The function “remainder” is implemented in Fortran by
remainder a
b = mod(a, b).
(1) Compute the sequence of the random numbers ri obtained using the above
parameters.
Plot ri as a function of i.
Construct a scatterplot (xi = r2i,
yi = r2i+1).
(2) Compute the average of the random numbers. What do you observe.
(3) Let N be the number of generated random numbers. Compute the correlation
functions deﬁned by
sum1(k) =
1
N −k
N−k
X
i=1
xixi+k.
sum2 = sum1(k) −⟨xi⟩2
sum1(0) −⟨xi⟩2 .
What is the behavior of these functions as a function of k.
(4) Compute the period of the above generators.

Pseudo Random Numbers and Random Walks
73
Part II
We take N random numbers in the interval [0, 1] which we divide into
K bins of length δ = 1/K. Let Ni be the number of random numbers which fall
in the ith bin. For a uniform sequence of random numbers the number of random
numbers in each bin is nideal = N/K.
(1) Verify this result for the generator “rand” found in the standard Fortran library
with seed given by seed = 32768. We take K = 10 and N = 1000. Plot Ni as
a function of the position xi of the ith bin.
(2) The number of degrees of freedom is ν = K −1. The most probable value of
the chi-square statistics χ2 is ν. Verify this result for a total number of bin
tests equal L = 1000 and K = 11. Each time calculate the number of times Li
in the L = 1000 bin tests we get a speciﬁc value of χ2. Plot Li as a function of
χ2. What do you observe.
7.5
Simulation 15: Random Walks
Part I
We consider the motion of a random walker in one dimension. The walker
can move with a step si = a to the right with a probability p or with a step si = −a
to the left with a probability q = 1 −p. After N steps the position of the walker is
xN = P
i si. We take
p = q = 1
2 , a = 1.
In order to simulate the motion of a random walker we need a generator of random
numbers. In this problem we work with the generator “rand” found in the standard
Fortran library. We call this generator as follows
call srand(seed)
rand()
The motion of the random walker is implemented with the code
if (rand() < p) then
xN = xN + a
else
xN = xN −a
endif.
(1) Compute the positions xi of three diﬀerent random walkers as functions of the
step number i. We take i = 1, 100. Plot the three trajectories.
(2) We consider now the motion of K = 500 random walkers. Compute the averages
⟨xN⟩= 1
K
K
X
i=1
x(i)
N , ⟨x2
N⟩= 1
K
K
X
i=1

x(i)
N
2
.
In the above equations x(i)
N is the position of the ith random walker after N
steps. Study the behavior of these averages as a function of N. Compare with
the theoretical predictions.

74
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Part II (optional)
We consider next a random walker in two dimensions on an
inﬁnite lattice of points. From any point (i, j) on the lattice the walker can reach
one of the 4 possible nearest neighbor sites (i+1, j), (i−1, j), (i, j +1) and (i, j −1)
with probabilities px, qx, py and qy respectively such that px + qx + py + qy = 1.
For simplicity we will assume that px = qx = py = qy = 0.25.
(1) Compute the averages ⟨⃗rN⟩and ⟨⃗r2
N⟩as function of the number of steps N for a
collection of L = 500 two dimensional random walkers. We consider the values
N = 10, ..., 1000.

Chapter 8
Monte Carlo Integration
8.1
Numerical Integration
8.1.1
Rectangular Approximation Revisited
As usual let us start with something simple. The approximation of one-dimensional
integrals by means of the rectangular approximation. This is a topic we have already
discussed before.
Let us then begin by recalling how the rectangular approximation of one dimen-
sional integrals works. We consider the integral
F =
Z b
a
f(x)dx.
(8.1)
We discretize the x-interval so that we end up with N equal small intervals of length
∆x, viz
xn = x0 + n∆x , ∆x = b −a
N
(8.2)
Clearly x0 = a and xN = b. Riemann deﬁnition of the integral is given by the
following limit
F = lim ∆x
N−1
X
n=0
f(xn) , ∆x −→0 , N −→∞, b −a = ﬁxed.
(8.3)
The ﬁrst approximation which can be made is to simply drop the limit. We get the
so-called rectangular approximation given by
FN = ∆x
N−1
X
n=0
f(xn).
(8.4)
The error can be computed as follows. We start with the Taylor expansion
f(x) = f(xn) + (x −xn)f (1)(xn) + 1
2!(x −xn)2f (2)(xn) + ...
(8.5)
Thus
Z xn+1
xn
dx f(x) = f(xn)∆x + 1
2!f (1)(xn)(∆x)2
+ 1
3!f (2)(xn)(∆x)3 + ...
(8.6)
75

76
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The error in the interval [xn, xn+1] is
Z xn+1
xn
dx f(x) −f(xn)∆x = 1
2!f (1)(xn)(∆x)2 + 1
3!f (2)(xn)(∆x)3
+ ...
(8.7)
This is of order 1/N 2. But we have N subintervals. Thus the total error is of order
1/N.
8.1.2
Midpoint Approximation of Multidimensional Integrals
Let us start with the two dimensional integral
F =
Z
R
dx dy f(x, y).
(8.8)
R is the domain of integration. In order to give the midpoint approximation of this
integral we imagine a rectangle of sides xb−xa and yb−ya which encloses the region
R and we divide it into squares of length h. The points in the x/y direction are
xi = xa +

i −1
2

h , i = 1, ..., nx.
(8.9)
yi = ya +

i −1
2

h , i = 1, ..., ny.
(8.10)
The number of points in the x/y direction are
nx = xb −xa
h
, ny = yb −ya
h
.
(8.11)
The number of cells is therefore
n = nxny = (xb −xa)(yb −ya)
h2
.
(8.12)
The integral is then approximated by
F = h2
nx
X
i=1
ny
X
j=1
f(xi, yj)H(xi, yj).
(8.13)
The Heaviside function is deﬁned by
H(xi, yj) = 1 if (xi, yj) ∈R otherwise H(xi, yj) = 0.
(8.14)
The generalization to many dimensions is straightforward. We get
F = hd
n1
X
i1=1
...
nd
X
id=1
f(xi1
1 , ..., xid
d )H(xi1
1 , ..., xid
d ).
(8.15)
The meaning of the diﬀerent symbols is obvious.
The midpoint approximation is an improvement over the rectangular approxi-
mation. To see this let us consider a one dimensional integral
F =
Z
R
dx f(x).
(8.16)

Monte Carlo Integration
77
The midpoint approximation reads in this case as follows
F = h
nx
X
i=1
f(xi)H(xi) = h
nx
X
i=1
f(xi).
(8.17)
Let us say that we have nx intervals [xi, xi+1] with x0 = a and xi = xa + (i −0.5)h,
i = 1, ..., nx −1. The term hf(xi+1) is associated with the interval [xi, xi+1]. It is
clear that we can write this approximation as
F = h
nx−1
X
i=0
f
xi + xi+1
2

, xi = xa + ih.
(8.18)
The error in the interval [xi, xi+1] is given by
Z xi+1
xi
f(x) dx −f
xi + xi+1
2

∆x = 1
24f ′′(xi)(∆x)3 + ...
(8.19)
The total error is therefore 1/n2
x as opposed to the 1/nx of the rectangular approx-
imation.
Let us do this in two dimensions. We write the error as
Z xi+1
xi
Z yj+1
yj
f(x, y) dx dy −f
xi + xi+1
2
, yj + yj+1
2

∆x∆y.
(8.20)
As usual we use Taylor series in the form
f(x, y) = f(xi, yj) + f ′
x(xi, yj)(x −xi) + f ′
y(xi, yj)(y −yj)
+ 1
2f ′′
x (xi, yj)(x −xi)2 + 1
2f ′′
y (xi, yj)(y −yj)2
+ f ′′
xy(xi, yj)(x −xi)(y −yj) + ...
(8.21)
We ﬁnd
Z xi+1
xi
Z yj+1
yj
f(x, y) dx dy −f
xi + xi+1
2
, yj + yj+1
2

∆x∆y
= 1
24f ′′
x (xi, yj)(∆x)3∆y + 1
24f ′′
y (xi, yj)∆x(∆y)3 + ...
(8.22)
Since ∆x = ∆y = h. The individual error is proportional to h4. The total error is
nh4 where n = nxny. Since n is proportional to 1/h2, the total error in dimension
two is proportional to h2 or equivalently to 1/n. As we have already seen the same
method led to an error proportional to 1/n2 in dimension one. Thus as we increase
the number of dimensions the error becomes worse. If in one dimension the error
behaves as 1/na then in dimension d it will behave as 1/n
a
d . In other words classical
numerical integration methods become impractical at suﬃciently higher dimensions
(which is the case of quantum mechanics and statistical mechanics).

78
An Introduction to Monte Carlo Simulations of Matrix Field Theory
8.1.3
Spheres and Balls in d Dimensions
The volume of a ball of radius R in d dimensions is given by
Vd =
Z
x2
1+...+x2
d≤R2 dx1...dxd
=
Z
x2
1+...+x2
d≤R2 rd−1 dr dΩd−1
= Rd
d
Z
dΩd−1
= Rd
d
2π
d
2
Γ( d
2).
(8.23)
The surface of a sphere of radius R in d dimensions is similarly given by
Sd−1 =
Z
x2
1+...+x2
d=R2 dx1...dxd
= Rd−1 2π
d
2
Γ( d
2).
(8.24)
Here are some properties of the gamma function
Γ(1) = 1 , Γ
1
2

= √π , Γ(n + 1) = nΓ(n).
(8.25)
In order to compute numerically the volume of the ball in any dimension d we need
a recursion formula which relates the volume of the ball in d dimensions to the
volume of the ball in d −1 dimensions. The derivation goes as follows
Vd =
Z +R
−R
dxd
Z
x2
1+...+x2
d−1≤R2−x2
d
dx1...dxd−1
=
Z +R
−R
dxd
Z √
R2−x2
d
0
rd−2 dr
Z
dΩd−2
= Vd−1
Rd−1
Z +R
−R
dxd (R2 −x2
d)
d−1
2 .
(8.26)
At each dimension d we are thus required to compute only the remaining integral
over xd using, for instance, the midpoint approximation while the volume Vd−1
is determined in the previous recursion step. The starting point of the recursion
process, for example the volume in d = 2, can be determined also using the midpoint
approximation. As we will see in the lab problems this numerical calculation is very
demanding with signiﬁcant errors compared with the Monte Carlo method.
8.2
Monte Carlo Integration: Simple Sampling
Let us start with the one dimensional integral
F =
Z b
a
dx f(x).
(8.27)

Monte Carlo Integration
79
A Monte Carlo method is any procedure which uses (pseudo) random numbers to
compute or estimate the above integral. In the following we will describe two very
simple Monte Carlo methods based on simple sampling which give an approximate
value for this integral. As we progress we will be able to give more sophisticated
Monte Carlo methods. First we start with the sampling (hit or miss) method then
we go on to the sample mean method.
8.2.1
Sampling (Hit or Miss) Method
This method consists of the following three main steps:
• We imagine a rectangle of width b −a and height h such that h is greater than
the maximum value of f(x), i.e. the function is within the boundaries of the
rectangle.
• To estimate the value F of the integral we choose n pairs of uniform random
numbers (xi, yi) where a ≤xi ≤b and 0 ≤yi ≤h.
• Then we evaluate the function f at the points xi. Let nin be the number of
random points (xi, yi) such that yi ≤f(xi). The value F of the integral is given
by
F = Anin
n
, A = h(b −a).
(8.28)
8.2.2
Sample Mean Method
We start from the mean-value theorem of calculus, viz
F =
Z b
a
dx f(x) = (b −a)⟨f⟩.
(8.29)
⟨f⟩is the average value of the function f(x) in the range a ≤x ≤b. The sample
mean method estimates the average ⟨f⟩as follows:
• We choose n random points xi from the interval [a, b] which are distributed
uniformly.
• We compute the values of the function f(x) at these point.
• We take their average. In other words
F = (b −a) 1
n
n
X
i=1
f(xi).
(8.30)
This is formally the same as the rectangular approximation. The only diﬀerence
is that here the points xi are chosen randomly from the interval [a, b] whereas the
points in the rectangular approximation are chosen with equal spacing. For lower
dimensional integrals the rectangular approximation is more accurate whereas for
higher dimensional integrals the sample mean method becomes more accurate.

80
An Introduction to Monte Carlo Simulations of Matrix Field Theory
8.2.3
Sample Mean Method in Higher Dimensions
We start with the two dimensional integral
F =
Z
R
dx dy f(x, y).
(8.31)
Again we consider a rectangle of sides yb −ya and xb −xa which encloses the region
R. The Monte carlo sample mean method yields the approximation
F = A 1
n
n
X
i=1
f(xi, yi)H(xi, yi).
(8.32)
The points xi are random and uniformly distributed in the interval [xa, xb] whereas
the points yi are random and uniformly distributed in the interval [ya, yb]. A is the
areas of the rectangle, i.e. A = (xb −xa)(yb −ya). The Heaviside function is deﬁned
by
H(xi, yi) = 1 if (xi, yi) ∈R otherwise H(xi, yi) = 0.
(8.33)
Generalization to higher dimensions is obvious. For example in three dimensions
we would have
F =
Z
R
dx dy dz f(x, y, z) −→F = V 1
n
n
X
i=1
f(xi, yi, zi)H(xi, yi, zi).
(8.34)
V is the volume of the parallelepiped which encloses the three dimensional region
R.
8.3
The Central Limit Theorem
Let p(x) be a probability distribution function. We generate (or measure) n values
xi of a certain variable x according to the probability distribution function p(x).
The average y1 =< xi > is given by
y1 = ⟨xi⟩= 1
n
n
X
i=1
xip(xi).
(8.35)
We repeat this measurement N times thus obtaining N averages y1, y2, ..., yN. The
mean z of the averages yi is
z = 1
N
N
X
i=1
yi.
(8.36)
The question we want to answer is: what is the probability distribution function
of z.
Clearly the probability of obtaining a particular value z is the product of the
probabilities of obtaining the individual averages yi (which are assumed to be inde-
pendent) with the constraint that the average of yi is z.

Monte Carlo Integration
81
Let ˜p(y) be the probability distribution function of the average y and let P(z)
be the probability distribution of the average z of the averages. We can then write
P(z) as
P(z) =
Z
dy1...
Z
dyN ˜p(y1)...˜p(yN)δ

z −y1 + ... + yN
N

.
(8.37)
The delta function expresses the constraint that z is the average of yi. The delta
function can be written as
δ

z −y1 + ... + yN
N

= 1
2π
Z
dqeiq(z−y1+...+yN
N
).
(8.38)
Let µ be the actual average of yi, i.e.
µ = ⟨yi⟩=
Z
dy˜p(y)y.
(8.39)
We write
P(z) = 1
2π
Z
dqeiq(z−µ)
Z
dy1 ˜p(y1)e
iq
N (µ−y1)...
Z
dyN ˜p(yN)e
iq
N (µ−yN)
= 1
2π
Z
dqeiq(z−µ)
 Z
dy ˜p(y)e
iq
N (µ−y)
N
.
(8.40)
But
Z
dy ˜p(y)e
iq
N (µ−y) =
Z
dy ˜p(y)

1 + iq
N (µ −y) −q2(µ −y)2
2N 2
+ ...

= 1 −q2σ2
2N 2 + ...
(8.41)
We have used
Z
dy ˜p(y)(µ −y)2 = ⟨y2⟩−⟨y⟩2 = σ2.
(8.42)
Hence
P(z) = 1
2π
Z
dqeiq(z−µ)e−q2σ2
2N
= 1
2π e−
N
2σ2 (z−µ)2 Z
dqe−σ2
2N (q−iN
σ (z−µ))2
=
1
√
2π
e
−(z−µ)2
2σ2
N
σN
.
(8.43)
σN =
σ
√
N
.
(8.44)
This is the normal distribution. Clearly the result does not depend on the original
probability distribution functions p(x) and ˜p(y).
The average z of N random numbers yi corresponding to a probability distri-
bution function ˜p(y) is distributed according to the normal probability distribution
function with average equal to the average value of ˜p(y) and variance equal to the
variance of ˜p(y) divided by
√
N.

82
An Introduction to Monte Carlo Simulations of Matrix Field Theory
8.4
Monte Carlo Errors and Standard Deviation
In any Monte Carlo approximation method the error goes as 1/
√
N where N is
the number of samples.
This behaviour is independent of the integrand and is
independent of the number of dimensions. In contrast if the error in a classical
numerical approximation method goes as 1/N a in one dimension (where N is now
the number of intervals) then the error in the same approximation method will go
as 1/N
a
d in d dimensions. Thus as we increase the number of dimensions the error
becomes worse.
In other words classical numerical integration methods become
impractical at suﬃciently higher dimensions. This is the fundamental appeal of
Monte Carlo methods in physics (quantum mechanics and statistical mechanics)
where we usually and so often encounter integrals of inﬁnite dimensionality.
Let us again consider for simplicity the one dimensional integral as an example.
We take
F =
Z b
a
dx f(x).
(8.45)
The Monte Carlo sample mean method gives the approximation
FN = (b −a)⟨f⟩, ⟨f⟩= 1
N
N
X
i=1
f(xi).
(8.46)
The error is by deﬁnition given by
∆= F −FN.
(8.47)
However in general we do not know the exact result F. The best we can do is to
calculate the probability that the approximate result FN is within a certain range
centered around the exact result F.
The starting point is the central limit theorem. This states that the average z
of N random numbers yα corresponding to a probability distribution function ˜p(y)
is distributed according to the normal probability distribution function. Here the
variable y is (we assume for simplicity that b −a = 1)
y = 1
N
N
X
i=1
f(xi).
(8.48)
We make M measurements yα of y. We write
yα = 1
N
N
X
i=1
f(xi,α).
(8.49)
The mean z of the averages is given by
z = 1
M
M
X
α=1
yα.
(8.50)

Monte Carlo Integration
83
According to the central limit theorem the mean z is distributed according to the
normal probability distribution function with average equal to the average value ⟨y⟩
of yα and variance equal to the variance of yα divided by
√
M, viz
s
M
2π˜σ2
M
exp

−M (z −⟨y⟩)2
2˜σ2
M

.
(8.51)
The ˜σM is the standard deviation of the mean given by the square root of the
variance
˜σ2
M =
1
M −1
M
X
α=1
(yα −⟨y⟩)2.
(8.52)
The use of M −1 instead of M is known as Bessel’s correction. The reason for this
correction is the fact that the computation of the mean ⟨y⟩reduces the number of
independent data points yα by one. For very large M we can replace ˜σM with σM
deﬁned by
˜σ2
M ∼σ2
M = 1
M
M
X
α=1
(yα −⟨y⟩)2 = ⟨y2⟩−⟨y⟩2.
(8.53)
The standard deviation of the sample (one single measurement with N data points)
is given by the square root of the variance
˜σ2 =
1
N −1
N
X
i=1
(f(xi) −⟨f⟩)2.
(8.54)
Again since N is large we can replace ˜σ with σ deﬁned by
σ2 = 1
N
N
X
i=1
(f(xi) −⟨f⟩)2 = ⟨f 2⟩−⟨f⟩2.
(8.55)
⟨f⟩= 1
N
N
X
i=1
f(xi) , ⟨f 2⟩= 1
N
N
X
i=1
f(xi)2.
(8.56)
The standard deviation of the mean ˜σM ∼σM is given in terms of the standard
deviation of the sample ˜σ ∼σ by the equation
σM =
σ
√
N
.
(8.57)
The proof goes as follows. We generalize equations (15.80) and (8.56) to the case of
M measurements each with N samples. The total number of samples is MN. We
have
σ2 =
1
NM
M
X
α=1
N
X
i=1
(f(xi,α) −⟨f⟩)2 = ⟨f 2⟩−⟨f⟩2.
(8.58)
⟨f⟩=
1
NM
M
X
α=1
N
X
i=1
f(xi,α) , ⟨f 2⟩=
1
NM
M
X
α=1
N
X
i=1
f(xi,α)2.
(8.59)

84
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The standard deviation of the mean ˜σM ∼σM is given by
σ2
M = 1
M
M
X
α=1
(yα −⟨y⟩)2
= 1
M
M
X
α=1
 1
N
N
X
i=1
f(xi,α) −⟨f⟩
2
=
1
N 2M
M
X
α=1
N
X
i=1
N
X
j=1

f(xi,α) −⟨f⟩

f(xi,α) −⟨f⟩

.
(8.60)
In above we have used the fact that ⟨y⟩= ⟨f⟩. For every set α the sum over i
and j splits into two pieces. The ﬁrst is the sum over the diagonal elements with
i = j and the second is the sum over the oﬀdiagonal elements with i ̸= j. Clearly
f(xi,α)−⟨f⟩and f(xj,α)−⟨f⟩are on the average equally positive and negative and
hence for large numbers M and N the oﬀdiagonal terms will cancel and we end up
with
σ2
M =
1
N 2M
M
X
α=1
N
X
i=1

f(xi,α) −⟨f⟩
2
= σ2
N .
(8.61)
The standard deviation of the mean σM can therefore be interpreted as the probable
error in the original N measurements since if we make M sets of measurements each
with N samples the standard deviation of the mean σM will estimate how much an
average over N measurements will deviate from the exact mean.
This means in particular that the original measurement FN of the integral F
has a 68 per cent chance of being within one standard deviation σM of the true
mean and a 95 per cent chance of being within 2σM and a 99.7 per cent chance of
being within 3σM. In general the proportion of data values within κσM standard
deviations of the true mean is deﬁned by the error function
Z ⟨y⟩+κσM
⟨y⟩−κσM
1
p
2πσ2
M
exp

−(z −⟨y⟩)2
2σ2
M

dz =
2
√π
Z
κ
√
2
0
exp
 −x2
dx
= erf
 κ
√
2

.
(8.62)
8.5
Nonuniform Probability Distributions
8.5.1
The Inverse Transform Method
We consider two discrete events 1 and 2 which occur with probabilities p1 and p2
respectively such that p1 + p2 = 1. The question is how can we choose the two
events with the correct probabilities using only a uniform probability distribution.
The answer is as follows. Let r be a uniform random number between 0 and 1. We
choose the event 1 if r < p1 else we choose the event 2.

Monte Carlo Integration
85
Let us now consider three discrete events 1, 2 and 3 with probabilities p1, p2
and p3 respectively such that p1 + p2 + p3 = 1. Again we choose a random number
r between 0 and 1. If r < p1 then we choose event 1, if p1 < r < p1 + p2 we choose
event 2 else we choose event 3.
We consider now n discrete events with probabilities pi such that Pn
i=1 pi = 1.
Again we choose a random number r between 0 and 1. We choose the event i if the
random number r satisﬁes the inequality
i−1
X
j=1
pj ≤r ≤
i
X
j=1
pj.
(8.63)
In the continuum limit we replace the probability pi with p(x)dx which is the
probability that the event x is found between x and x+dx. The condition Pn
i=1 pi =
1 becomes
Z +∞
−∞
p(x) dx = 1.
(8.64)
The inequality (8.63) becomes the identity
P(x) ≡
Z x
−∞
p(x′) dx′ = r
(8.65)
Thus r is equal to the cumulative probability distribution P(x), i.e. the probability
of choosing a value less than or equal to x.
This equation leads to the inverse
transform method which allows us to generate a nonuniform probability distribution
p(x) from a uniform probability distribution r.
Clearly we must be able to 1)
perform the integral analytically to ﬁnd P(x) then 2) invert the relation P(x) = r
for x.
As a ﬁrst example we consider the Poisson distribution
p(x) = 1
λe−x
λ , 0 ≤x ≤∞.
(8.66)
We ﬁnd
P(x) = 1 −e−x
λ = r.
(8.67)
Hence
x = −λ ln(1 −r).
(8.68)
Thus given the uniform random numbers r we can compute directly using the above
formula the random numbers x which are distributed according to the Poisson
distribution p(x) = 1
λe−x
λ .
The next example is the Gaussian distribution in two dimensions
p(x, y) =
1
2πσ2 e−x2+y2
2σ2 .
(8.69)
We can immediately compute that
1
2πσ2
Z +∞
−∞
dx
Z +∞
−∞
dy e−x2+y2
2σ2
=
Z 1
0
dw
Z 1
0
dv.
(8.70)
x = r cos φ , y = r sin φ.
(8.71)
r2 = −2σ2 ln v , φ = 2πw.
(8.72)

86
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The random numbers v and w are clearly uniformly distributed between 0 and 1.
The random numbers x (or y) are distributed according to the Gaussian distribution
in one dimension. This method is known as the Box-Muller method.
8.5.2
The Acceptance-Rejection Method
This was proposed by Von Neumann. The goal is to generate a sequence of random
numbers distributed according to some normalized probability density y = p(x).
This method consists of the following steps:
• We start by generating a uniform random number rx in the range of interest
xmin ≤rx ≤xmax where [xmin, xmax] is the interval in which y = p(x) does not
vanish.
• We evaluate p(rx).
• Then we generate another uniform random number ry in the range [0, ymax]
where ymax is the maximum value of the distribution y = p(x).
• If ry < p(rx) then we accept the random number rx else we reject it.
• We repeat this process a suﬃcient number of times.
It is not diﬃcult to convince ourselves that the accepted random numbers rx will
be distributed according to y = p(x).
8.6
Simulation 16: Midpoint and Monte Carlo Approximations
Part I
The volume of a ball of radius R in d dimensions is given by
Vd =
Z
x2
1+...+x2
d≤R2 dx1...dxd
= 2
Z
dx1...dxd−1
q
R2 −x2
1 −... −x2
d−1
= Rd
d
2π
d
2
Γ( d
2).
(1) Write a program that computes the three dimensional integral using the mid-
point approximation. We take the stepsize h = 2R/N, the radius R = 1 and
the number of steps in each direction to be N = Nx = Ny = 2p where p = 1, 15.
(2) Show that the error goes as 1/N. Plot the logarithm of the absolute value of
the absolute error versus the logarithm of N.
(3) Try out the two dimensional integral. Work in the positive quadrant and again
take the stepsize h = R/N where R = 1 and N = 2p, p = 1, 15. We know that
generically the theoretical error goes at least as 1/N 2. What do you actually
ﬁnd? Why do you ﬁnd a discrepancy?
Hint: the second derivative of the integrand is singular at x = R which changes
the dependence from 1/N 2 to 1/N 1.5.

Monte Carlo Integration
87
Part II
In order to compute numerically the volume of the ball in any dimension
d we use the recursion formula
Vd = Vd−1
Rd−1
Z +R
−R
dxd (R2 −x2
d)
d−1
2 .
(1) Find the volumes in d = 4, 5, 6, 7, 8, 9, 10, 11 dimensions. Compare with the
exact result given above.
Part III
(1) Use the Monte Carlo sampling (hit or miss) method to ﬁnd the integrals in
d = 2, 3, 4 and d = 10 dimensions. Is the Monte Carlo method easier to apply
than the midpoint approximation?
(2) Use the Monte Carlo sample mean value method to ﬁnd the integrals in d =
2, 3, 4 and d = 10 dimensions. For every d we perform M measurements each
with N samples. We consider M = 1, 10, 100, 150 and N = 2p, p = 10, 19.
Verify that the exact error in this case goes like 1/
√
N.
Hint: Compare the exact error which is known in this case with the standard
deviation of the mean σM and with σ/
√
N where σ is the standard deviation
of the sample, i.e. of a single measurement. These three quantities must be
identical.
Part IV
(1) The value of π can be given by the integral
π =
Z
x2+y2≤R2 dx dy.
Use the Monte Carlo sampling (hit or miss) method to give an approximate
value of π.
(2) The above integral can also be put in the form
π = 2
Z +1
−1
dx
p
1 −x2.
Use the Monte Carlo sample mean value method to give another approximate
value of π.
8.7
Simulation 17: Nonuniform Probability Distributions
Part I
The Gaussian distribution is given by
P(x) =
1
√
2πσ2 exp −(x −µ)2
2σ
.
The parameter µ is the mean and σ is the variance, i.e. the square root of the
standard deviation. We choose µ = 0 and σ = 1.

88
An Introduction to Monte Carlo Simulations of Matrix Field Theory
(1) Write a program that computes a sequence of random numbers x distributed
according to P(x) using the inverse transform method (Box-Muller algorithm)
given by the equations
x = r cos φ.
r2 = −2σ2 ln v , φ = 2πw.
The v and w are uniform random numbers in the interval [0, 1].
(2) Draw a histogram of the random numbers obtained in the previous question.
The steps are as follows:
a- Determine the range of the points x.
b- We divide the interval into u bins. The length of each bin is h = interval/u.
We take for example u = 100.
c- We determine the location of every point x among the bins. We increase
the counter of the corresponding bin by a unit.
d- We plot the fraction of points as a function of x. The fraction of point
is equal to the number of random numbers in a given bin divided by hN
where N is the total number of random numbers. We take N = 10000.
(3) Draw the data on a logarithmic scale, i.e. plot log(fraction) versus x2. Find
the ﬁt and compare with theory.
Part II
(1) Apply the acceptance-rejection method to the above problem.
(2) Apply the Fernandez-Criado algorithm to the above problem. The procedure
is as follows
a- Start with N points xi such that xi = σ.
b- Choose at random a pair (xi, xj) from the sequence and make the following
change
xi −→xi + xj
√
2
xj −→−xi +
√
2xj.
c- Repeat step 2 until we reach equilibrium. For example try it M times
where M = 10, 100, ....

Chapter 9
The Metropolis Algorithm
and the Ising Model
9.1
The Canonical Ensemble
We consider physical systems which are in thermal contact with an environment.
The environment is usually much larger than the physical system of interest and
as a consequence energy exchange between the two of them will not change the
temperature of the environment.
The environment is called heat bath or heat
reservoir. When the system reaches equilibrium with the heat bath its temperature
will be given by the temperature of the heat bath.
A system in equilibrium with a heat bath is described statistically by the canon-
ical ensemble in which the temperature is ﬁxed. In contrast an isolated system is
described statistically by the microcanonical ensemble in which the energy is ﬁxed.
Most systems in nature are not isolated but are in thermal contact with the envi-
ronment. It is a fundamental result of statistical mechanics that the probability of
ﬁnding a system in equilibrium with a heat bath at temperature T in a microstate
s with energy Es is given by the Boltzmann distribution
Ps = 1
Z e−βEs , β =
1
kBT .
(9.1)
The normalization connstant Z is the partition function. It is deﬁned by
Z =
X
s
e−βEs.
(9.2)
The sum is over all the microstates of the system with a ﬁxed N and V .
The
Helmholtz free energy F of a system is given by
F = −kBT ln Z.
(9.3)
In equilibrium the free energy is minimum. All other thermodynamical quantities
can be given by various derivatives of F. For example the internal energy U of the
system which is the expectation value of the energy can be expressed in terms of F
as follows
U = ⟨E⟩=
X
s
EsPs = 1
Z
X
s
Ese−βEs = −∂
∂β ln Z = ∂
∂β (βF).
(9.4)
89

90
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The speciﬁc heat is given by
Cv = ∂
∂T U.
(9.5)
In the deﬁnition of the partition function (9.2) we have implicitly assumed that
we are dealing with a physical system with conﬁgurations (microstates) which have
discrete energies. This is certainly true for many quantum systems. However for
many other systems especially classical ones the energies are not discrete.
For
example the partition function of a gas of N distinguishable classical particles is
given by
Z =
Z
N
Y
i=1
d3pid3qi
h3
e−βH(⃗pi,⃗qi).
(9.6)
For quantum dynamical ﬁeld systems (in Euclidean spacetimes) which are of fun-
damental importance to elementary particles and their interactions the partition
function is given by the so-called path integral which is essentially of the same form
as the previous equation with the replacement of the Hamiltonian H(⃗pi, ⃗qi) by the
action S[Φ] where Φ stands for the ﬁeld variables and the replacement of the mea-
sure QN
i=1(d3pid3qi)/h3 by the relevant (inﬁnite dimensional) measure DΦ on the
space of ﬁeld conﬁgurations. We obtain therefore
Z =
Z
DΦ e−βS[Φ].
(9.7)
Similarly to what happens in statistical mechanics where all observables can be
derived from the partition function the observables of a quantum ﬁeld theory can
all be derived from the path integral. The fundamental problem therefore is how
to calculate the partition function or the path integral for a given physical system.
Normally an analytic solution will be ideal. However ﬁnding such a solution is sel-
dom possible and as a consequence only the numerical approach remains available
to us. The partition function and the path integral are essentially given by multidi-
mensional integrals and thus one should seek numerical approaches to the problem
of integration.
9.2
Importance Sampling
In any Monte Carlo integration the numerical error is proportional to the standard
deviation of the integrand and is inversely proportional to the number of samples.
Thus in order to reduce the error we should either reduce the variance or increase the
number of samples. The ﬁrst option is preferable since it does not require any extra
computer time. Importance sampling allows us to reduce the standard deviation
of the integrand and hence the error by sampling more often the important regions
of the integral where the integrand is largest. Importance sampling uses also in a
crucial way nonuniform probability distributions.

The Metropolis Algorithm and the Ising Model
91
Let us again consider the one dimensional integral
F =
Z b
a
dx f(x).
(9.8)
We introduce the probability distribution p(x) such that
1 =
Z b
a
dx p(x).
(9.9)
We write the integral as
F =
Z b
a
dx p(x) f(x)
p(x) .
(9.10)
We evaluate this integral by sampling according to the probability distribution
p(x). In other words we ﬁnd a set of N random numbers xi which are distributed
according to p(x) and then approximate the integral by the sum
FN = 1
N
N
X
i=1
f(xi)
p(xi) .
(9.11)
The probability distribution p(x) is chosen such that the function f(x)/p(x) is slowly
varying which reduces the corresponding standard deviation.
9.3
The Ising Model
We consider a d-dimensional periodic lattice with n points in every direction so
that there are N = nd points in total in this lattice. In every point (lattice site)
we put a spin variable si (i = 1, ..., N) which can take either the value +1 or −1.
A conﬁguration of this system of N spins is therefore speciﬁed by a set of numbers
{si}. In the Ising model the energy of this system of N spins in the conﬁguration
{si} is given by
EI{si} = −
X
⟨ij⟩
ϵijsisj −H
N
X
i=1
si.
(9.12)
The parameter H is the external magnetic ﬁeld. The symbol ⟨ij⟩stands for nearest
neighbor spins. The sum over ⟨ij⟩extends over γN/2 terms where γ is the number
of nearest neighbors. In 2, 3, 4 dimensions γ = 4, 6, 8. The parameter ϵij is the
interaction energy between the spins i and j. For isotropic interactions ϵij = ϵ. For
ϵ > 0 we obtain ferromagnetism while for ϵ < 0 we obtain antiferromagnetism. We
consider only ϵ > 0. The energy becomes with these simpliﬁcations given by
EI{si} = −ϵ
X
⟨ij⟩
sisj −H
N
X
i=1
si.
(9.13)
The partition function is given by
Z =
X
s1
X
s2
...
X
sN
e−βEI{si}.
(9.14)

92
An Introduction to Monte Carlo Simulations of Matrix Field Theory
There are 2N terms in the sum and β = 1/kBT.
In d = 2 we have N = n2 spins in the square lattice. The conﬁguration {si} can
be viewed as an n × n matrix. We impose periodic boundary condition as follows.
We consider (n + 1) × (n + 1) matrix where the (n + 1)th row is identiﬁed with the
ﬁrst row and the (n + 1)th column is identiﬁed with the ﬁrst column. The square
lattice is therefore a torus.
9.4
The Metropolis Algorithm
The internal energy U = ⟨E⟩can be put into the form
⟨E⟩=
P
s Ese−βEs
P
s e−βEs
.
(9.15)
Generally given any physical quantity A its expectation value ⟨A⟩can be computed
using a similar expression, viz
⟨A⟩=
P
s Ase−βEs
P
s e−βEs
.
(9.16)
The number As is the value of A in the microstate s. In general the number of
microstates N is very large. In any Monte Carlo simulation we can only generate a
very small number n of the total number N of the microstates. In other words ⟨E⟩
and ⟨A⟩will be approximated with
⟨E⟩≃⟨E⟩n =
Pn
s=1 Ese−βEs
Pn
s=1 e−βEs
.
(9.17)
⟨A⟩≃⟨A⟩n =
Pn
s=1 Ase−βEs
Pn
s=1 e−βEs
.
(9.18)
The calculation of ⟨E⟩n and ⟨A⟩n proceeds therefore by 1) choosing at random a
microstate s, 2) computing Es, As and e−βEs then 3) evaluating the contribution of
this microstate to the expectation values ⟨E⟩n and ⟨A⟩n. This general Monte Carlo
procedure is however highly ineﬃcient since the microstate s is very improbable and
therefore its contribution to the expectation values is negligible. We need to use
importance sampling. To this end we introduce a probability distribution ps and
rewrite the expectation value ⟨A⟩as
⟨A⟩=
P
s
As
ps e−βEsps
P
s
1
ps e−βEsps
.
(9.19)
Now we generate the microstates s with probabilities ps and approximate ⟨A⟩with
⟨A⟩n given by
⟨A⟩n =
Pn
s=1
As
ps e−βEs
Pn
s=1
1
ps e−βEs .
(9.20)

The Metropolis Algorithm and the Ising Model
93
This is important sampling. The Metropolis algorithm is importance sampling with
ps given by the Boltzmann distribution, i.e.
ps =
e−βEs
Pn
s=1 e−βEs .
(9.21)
We get then the arithmetic average
⟨A⟩n = 1
n
n
X
s=1
As.
(9.22)
The Metropolis algorithm in the case of spin systems such as the Ising model can
be summarized as follows:
(1) Choose an initial microstate.
(2) Choose a spin at random and ﬂip it.
(3) Compute ∆E = Etrial −Eold. This is the change in the energy of the system
due to the trial ﬂip.
(4) Check if ∆E ≤0. In this case the trial microstate is accepted.
(5) Check if ∆E > 0. In this case compute the ratio of probabilities w = e−β∆E.
(6) Choose a uniform random number r in the interval [0, 1].
(7) Verify if r ≤w. In this case the trial microstate is accepted, otherwise it is
rejected.
(8) Repeat steps 2) through 7) until all spins of the system are tested. This sweep
counts as one unit of Monte Carlo time.
(9) Repeat steps 2) through 8) a suﬃcient number of times until thermalization,
i.e. equilibrium is reached.
(10) Compute the physical quantities of interest in n thermalized microstates. This
can be done periodically in order to reduce correlation between the data points.
(11) Compute averages.
The proof that this algorithm leads indeed to a sequence of states which are dis-
tributed according to the Boltzmann distribution goes as follows.
It is clear that the steps 2) through 7) corresponds to a transition probability
between the microstates {si} and {sj} given by
W(i −→j) = min(1, e−β∆E) , ∆E = Ej −Ei.
(9.23)
Since only the ratio of probabilities w = e−β∆E is needed it is not necessary to
normalize the Boltzmann probability distribution. It is clear that this probability
function satisﬁes the detailed balance condition
W(i −→j) e−βEi = W(j −→i) e−βEj.
(9.24)
Any other probability function W which satisﬁes this condition will generate a
sequence of states which are distributed according to the Boltzmann distribution.
This can be shown by summing over the index j in the above equation and using
P
j W(i −→j) = 1. We get
e−βEi =
X
j
W(j −→i) e−βEj.
(9.25)

94
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The Boltzmann distribution is an eigenvector of W. In other words W leaves the
equilibrium ensemble in equilibrium. As it turns out this equation is also a suﬃcient
condition for any ensemble to approach equilibrium.
9.5
The Heat-Bath Algorithm
The heat-bath algorithm is generally a less eﬃcient algorithm than the Metropolis
algorithm. The acceptance probability is given by
W(i −→j) = min

1,
1
1 + eβ∆E

, ∆E = Ej −Ei.
(9.26)
This acceptance probability satisﬁes also detailed balance for the Boltzmann proba-
bility distribution. In other words the detailed balance condition which is suﬃcient
but not necessary for an ensemble to reach equilibrium does not have a unique
solution.
9.6
The Mean Field Approximation
9.6.1
Phase Diagram and Critical Temperature
We consider N = L2 spins on a square lattice where L is the number of lattice sites
in each direction. Each spin can take only two possible values si = +1 (spin up)
and si = −1 (spin down). Each spin interacts only with its 4 neighbors and also
with a magnetic ﬁeld H. The Ising model in 2 dimensions is given by the energy
E{s} = −J
X
⟨ij⟩
sisj −H
X
i
si.
(9.27)
The system is assumed to be in equilibrium with a heat bath with temperature
T. Thermal equilibrium of the Ising model is described by the canonical ensemble.
The probability of ﬁnding the Ising model in a conﬁguration {s1, ..., s2N } is given
by Boltzmann distribution
P{s} = e−βE{s}
Z
.
(9.28)
The partition function is given by
Z =
X
{s}
e−βE{s} =
X
s1
...
X
s2N
e−βE{s}.
(9.29)
The magnetization M in a conﬁguration {s1, ..., s2N } is the order parameter of the
system. It is deﬁned by
M =
X
i
si.
(9.30)
The average of M is given by
⟨M⟩=
X
i
⟨si⟩= N⟨s⟩.
(9.31)

The Metropolis Algorithm and the Ising Model
95
In above ⟨si⟩= ⟨s⟩since all spins are equivalent. We have
⟨M⟩= 1
β
∂log Z
∂H
= −∂F
∂H .
(9.32)
In order to compute ⟨M⟩we need to compute Z. In this section we use the mean
ﬁeld approximation. First we rewrite the energy E{s} in the form
E{s} =

−J
X
⟨ij⟩
sj

si −H
X
i
si
=
X
i
Hi
eﬀsi −H
X
i
si.
(9.33)
The eﬀective magnetic ﬁeld Hi
eﬀis given by
Hi
eﬀ= −J
X
j(i)
sj(i).
(9.34)
The index j(i) runs over the four nearest neighbors of the spin i. In the mean ﬁeld
approximation we replace the spins sj(i) by their thermal average ⟨s⟩. We obtain
Hi
eﬀ= −Jγ⟨s⟩, γ = 4.
(9.35)
In other words,
E{s} = −(H + Jγ⟨s⟩)
X
i
si = Heﬀ
X
i
si.
(9.36)
The partition function becomes
Z =
 X
s1
e−βHeffsi
N
=
 e−βHeff + eβHeffN
(9.37)
=
 2 cosh βHeﬀ
N.
(9.38)
The free energy and magnetization are then given by
F = −kT ln Z = −kTN ln
 2 cosh βHeﬀ

.
(9.39)
⟨M⟩= N⟨s⟩= N tanh βHeﬀ.
(9.40)
Thus for zero magnetic ﬁeld we get from the second equation the constraint
⟨s⟩= tanh γβJ⟨s⟩.
(9.41)
Clearly ⟨s⟩= 0 is always a solution. This is the high temperature paramagnetic
phase. For small temperature we have also a solution ⟨s⟩̸= 0. This is the ferro-
magnetic phase. There must exist a critical temperature Tc which separates the
two phases. We expect ⟨s⟩to approach ⟨s⟩= 0 as T goes to Tc from below. In
other words near Tc we can treat ⟨s⟩as small and as a consequence we can use the
expansion tanh x = x −1
3x3. We obtain
⟨s⟩= γβJ⟨s⟩−1
3
 γβJ⟨s⟩
3.
(9.42)

96
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Equivalently
⟨s⟩

⟨s⟩2 −3
T
1
(γβJ)3
γJ
kB
−T
 
= 0.
(9.43)
We get the two solutions
⟨s⟩= 0 , paramagnetic phase
⟨s⟩= ±
s
3
T
1
(γβJ)3 (Tc −T)β , ferromagnetic phase.
(9.44)
The critical temperature Tc and the critical exponent β are given by
Tc = γJ
kB
, β = 1
2.
(9.45)
The ferromagnetic solution can only exist for T < Tc.
9.6.2
Critical Exponents
The free energy for zero magnetic ﬁeld is
F = −kTN ln
 2 cosh γβJ⟨s⟩

.
(9.46)
We see that for T < Tc the ferromagnetic solution has a lower free energy than the
paramagnetic solution ⟨s⟩= 0. The phase T < Tc is indeed ferromagnetic. The
transition at T = Tc is second order. The free energy is continuous at T = Tc, i.e.
there is no latent heat while the speciﬁc heat is logarithmically divergent. The mean
ﬁeld theory yields the correct value 0 for the critical exponent α although it does
not reproduce the logarithmic divergence. The susceptibility diverges at T = Tc
with critical exponent γ = 1. These latter statements can be seen as follows.
The speciﬁc heat is given by
Cv = −∂
∂T

kBT 2 ∂
∂T (βF)

= −2kBT ∂
∂T (βF) −kBT 2 ∂2
∂T 2 (βF).
(9.47)
Next we use the expression βF = −N ln(ex + e−x) where x = γβJ⟨s⟩. We ﬁnd
Cv
N = 2kBT tanh x ∂x
∂T + kBT 2 tanh2 x ∂2x
∂T 2
+ kBT 2
1
cosh2 x( ∂x
∂T )2.
(9.48)
We compute
x = ±
s
3kB
γJ (Tc −T)
1
2
∂x
∂T = ∓1
2
s
3kB
γJ (Tc −T)−1
2
∂2x
∂T 2 = ∓1
4
s
3kB
γJ (Tc −T)−3
2 .
(9.49)

The Metropolis Algorithm and the Ising Model
97
It is not diﬃcult to show that the divergent terms cancel and as a consequence
Cv
N ∼(Tc −T)−α , α = 0.
(9.50)
The susceptibility is given by
χ =
∂
∂H ⟨M⟩.
(9.51)
To compute the behavior of χ near T = Tc we consider the equation
⟨s⟩= tanh(γβJ⟨s⟩+ βH).
(9.52)
For small magnetic ﬁeld we can still assume that γβJ⟨s⟩+βH is small near T = Tc
and as a consequence we can expand the above equation as
⟨s⟩= (γβJ⟨s⟩+ βH) −1
3(γβJ⟨s⟩+ βH)3.
(9.53)
Taking the derivative with respect to H of both sides of this equation we obtain
ˆχ = (γβJ ˆχ + β) −(γβJ ˆχ + β)(γβJ⟨s⟩+ βH)2.
(9.54)
ˆχ =
∂
∂H ⟨s⟩.
(9.55)
Setting the magnetic ﬁeld to zero we get
ˆχ = (γβJ ˆχ + β) −(γβJ ˆχ + β)(γβJ⟨s⟩)2.
(9.56)
In other words
 1 −γβJ + γβJ(γβJ⟨s⟩)2
ˆχ = β −β(γβJ⟨s⟩)2.
(9.57)
2Tc −T
T
ˆχ =
1
kBT (1 −(γβJ⟨s⟩)2).
(9.58)
Hence
ˆχ =
1
2kB
(Tc −T)−γ , γ = 1.
(9.59)
9.7
Simulation of the Ising Model and Numerical Results
9.7.1
The Fortran Code
We choose to write our code in Fortran. The reason is simplicity and straightfor-
wardness. A person who is not well versed in programming languages, who has a
strong background in physics and maths, and who wants to get up and running
quickly with the coding so that she starts doing physics (almost) immediately the
choice of Fortran for her is ideal and thus it is only natural. The potential superior
features which may be found in C are peripheral to our purposes here.

98
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The spin found in the intersection point of the ith row and jth column of the
lattice will be represented with the matrix element φ(i, j). The energy will then
read (with N = n2 and n ≡L)
E = −
n
X
i,j=1
J
2 φ(i, j)
 φ(i + 1, j) + φ(i −1, j) + φ(i, j + 1) + φ(i, j −1)

+ Hφ(i, j)

.
(9.60)
We impose periodic boundary condition in order to reduce edge and boundary
eﬀects. This can be done as follows. We consider (n+1)×(n+1) matrix where the
(n + 1)th row is identiﬁed with the ﬁrst row and the (n + 1)th column is identiﬁed
with the ﬁrst column. The square lattice is therefore a torus. The toroidal boundary
condition will read explicitly as follows
φ(0, j) = φ(n, j) , φ(n + 1, j) = φ(1, j)
φ(i, 0) = φ(i, n) , φ(i, n + 1) = φ(i, 1).
(9.61)
The variation of the energy due to the ﬂipping of the spin φ(i, j) is an essential
ingredient in the Metropolis algorithm. This variation is explicitly given by
∆E = 2Jφ(i, j)
 φ(i + 1, j) + φ(i −1, j) + φ(i, j + 1) + φ(i, j −1)

+ 2Hφ(i, j).
(9.62)
The Fortran code contains the following pieces:
• A subroutine which generates pseudo random numbers. We prefer to work
with well established subroutines such as the RAN 2 or the RANLUX.
• A subroutine which implements the Metropolis algorithm for the Ising model.
This main part will read (with some change of notation such as J = exch)
do i=1,L
ip(i)=i+1
im(i)=i-1
enddo
ip(L)=1
im(1)=L
do
i=1,L
do
j=1,L
deltaE=2.0d0*exch*phi(i,j)*(phi(ip(i),j)+phi(im(i),j)+
phi(i,ip(j))+phi(i,im(j)))
deltaE=deltaE + 2.0d0*H*phi(i,j)
if (deltaE.ge.0.0d0)then
probability=dexp(-beta*deltaE)
call ranlux(rvec,len)
r=rvec(1)
if (r.le.probability)then
phi(i,j)=-phi(i,j)

The Metropolis Algorithm and the Ising Model
99
endif
else
phi(i,j)=-phi(i,j)
endif
enddo
enddo
• We compute the energy ⟨E⟩and the magnetization ⟨M⟩of the Ising model in
a separate subroutine.
• We compute the errors using for example the Jackknife method in a separate
subroutine.
• We ﬁx the parameters of the model such as L, J, β = 1/T and H.
• We choose an initial conﬁguration. We consider both cold and hot starts which
are given respectively by
φ(i, j) = +1.
(9.63)
φ(i, j) = random signs.
(9.64)
• We run the Metropolis algorithm for a given thermalization time and study
the history of the energy and the magnetization for diﬀerent values of the
temperature.
• We add a Monte Carlo evolution with a reasonably large number of steps and
compute the averages of E and M.
• We compute the speciﬁc heat and the susceptibility of the system.
9.7.2
Some Numerical Results
Energy:
The energy is continuous through the transition point and as a conse-
quence there is no latent heat. This indicates a second order behavior.
Speciﬁc Heat:
The critical exponent associated with the speciﬁc heat is given
by α = 0. However the speciﬁc heat diverges logarithmically at T = Tc.
This
translates into the fact that the peak grows with n logarithmically, namely
Cv
n2 ∼log n.
(9.65)
Magnetization:
The magnetization near but below the critical temperature in
the two-dimensional Ising model scales as
⟨M⟩
n2
∼(Tc −T)−β , β = 1/8.
(9.66)
Susceptibility:
The susceptibility near the critical temperature in the two-
dimensional Ising model scales as
χ
n2 ∼|T −Tc|−γ , γ = 7/4.
(9.67)

100
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Critical Temperature:
From the behavior of the above observable we can mea-
sure the critical temperature, which marks the point where the second order ferro-
magnetic phase transition occurs, to be given approximately by
kBTc =
2J
ln(
√
2 + 1).
(9.68)
Critical Exponents and 2-Point Correlation Function:
The 2-point corre-
lation function of the two-dimensional Ising model is deﬁned by the expression
f(x) = ⟨s0sx⟩
=
*
1
4n2
X
i,j
φ(i, j)
 φ(i + x, j) + φ(i −x, j) + φ(i, j + x) + φ(i, j −x)

+
.
(9.69)
We can verify numerically the following statements:
• At T = Tc the behaviour of f(x) is given by
f(x) ≃1
xη , η = 1/4.
(9.70)
• At T less than Tc the behavior of f(x) is given by
f(x) = ⟨M⟩2.
(9.71)
• At T larger than Tc the behaviour of f(x) is given by
f(x) ≃a 1
xη e−x
ξ .
(9.72)
• Near Tc the correlation length diverges as
ξ ≃
1
|T −Tc|ν , ν = 1.
(9.73)
Note that near-neighbor lattice sites which are a distance x away in a given
direction from a given index i are given by
do x=1,nn
if (i+x .le. n) then
ipn(i,x)=i+x
else
ipn(i,x)=(i+x)-n
endif
if ((i-x).ge.1)then
imn(i,x)=i-x
else
imn(i,x)=i-x+n
endif
enddo
For simplicity we consider only odd lattices, viz n = 2nn + 1. Clearly because
of the toroidal boundary conditions the possible values of the distance x are
x = 1, 2, ..., nn.

The Metropolis Algorithm and the Ising Model
101
First Order Transition and Hysteresis:
We can also consider the eﬀect of a
magnetic ﬁeld H on the physics of the Ising model. We observe a ﬁrst order phase
transition at H = 0 or H near 0 and a phenomena of hysteresis. We observe the
following:
• For T < Tc we can observe a ﬁrst order phase transition. Indeed we observe a
discontinuity in the energy and the magnetization which happens at a non-zero
value of H due to hysteresis. The jumps in the energy and the magnetization
are typical signal for a ﬁrst order phase transition.
• For T > Tc the magnetization becomes a smooth function of H near H = 0
which means that above Tc there is no distinction between the ferromagnetic
states with M ≥0 and M ≤0.
• We recompute the magnetization as a function of H for a range of H back and
fourth. We observe the following:
– A hysteresis loop.
– The hysteresis window shrinks with increasing temperature or accumu-
lating more Monte Carlo time.
– The hysteresis eﬀect is independent of the size of the lattice.
The phenomena of hysteresis indicates that the behaviour of the system de-
pends on its initial state and history. Equivalently we say that the system is
trapped in a metastable state.
9.8
Simulation 18: The Metropolis Algorithm and the Ising Model
Part I
We consider N = L2 spins on a square lattice where L is the number of
lattice sites in each direction. Each spin can take only two possible values si = +1
(spin up) and si = −1 (spin down). Each spin interacts only with its 4 neighbors
and also with a magnetic ﬁeld H. The Ising model in 2 dimensions is given by the
energy
E = −J
X
⟨ij⟩
sisj −H
X
i
si.
We will impose toroidal boundary condition. The system is assumed to be in equi-
librium with a heat bath with temperature T. Thermal ﬂuctuations of the system
will be simulated using the Metropolis algorithm.
(1) Write a subroutine that computes the energy E and the magnetization M of
the Ising model in a conﬁguration φ. The magnetization is the order parameter
of the system. It is deﬁned by
M =
X
i
si.
(9.74)
(2) Write a subroutine that implements the Metropolis algorithm for this system.
You will need for this the variation of the energy due to ﬂipping the spin φ(i, j).

102
An Introduction to Monte Carlo Simulations of Matrix Field Theory
(3) We choose L = 10, H = 0, J = 1, β = 1/T. We consider both a cold start
and a hot start.
Run the Metropolis algorithm for a thermalization time TTH = 26 and study
the history of the energy and the magnetization for diﬀerent values of the
temperature. The energy and magnetization should approach the values E = 0
and M = 0 when T −→∞and the values E = −2JN and M = +1 when
T −→0.
(4) Add a Monte Carlo evolution with TTM = 210 and compute the averages of
E and M.
(5) Compute the speciﬁc heat and the susceptibility of the system.
These are
deﬁned by
Cv = ∂
∂β ⟨E⟩= β
T (⟨E2⟩−⟨E⟩2) , χ =
∂
∂H ⟨M⟩= β(⟨M 2⟩−⟨M⟩2).
(6) Determine the critical point. Compare with the theoretical exact result
kBTc =
2J
ln(
√
2 + 1).
Part II
Add to the code a separate subroutine which implements the Jackknife
method for any set of data points. Compute the errors in the energy, magnetization,
speciﬁc heat and susceptibility of the Ising model using the Jackknife method.
9.9
Simulation 19: The Ferromagnetic Second Order Phase
Transition
Part I
The critical exponent associated with the speciﬁc heat is given by α = 0,
viz
Cv
L2 ∼(Tc −T)−α , α = 0.
However the speciﬁc heat diverges logarithmically at T = Tc. This translates into
the fact that the peak grows with L logarithmically, namely
Cv
L2 ∼log L.
Verify this behaviour numerically. To this end we take lattices between L = 10−30
with TTH = 210, TMC = 213. The temperature is taken in the range
T = Tc −10−2 × step , step = −50, 50.
Plot the maximum of Cv/L2 versus ln L.

The Metropolis Algorithm and the Ising Model
103
Part II
The magnetization near but below the critical temperature in 2D Ising
model scales as
⟨M⟩
L2
∼(Tc −T)−β , β = 1
8.
We propose to study the magnetization near Tc in order to determine the value of
β numerically. Towards this end we plot |⟨M⟩| versus Tc −T where T is taken in
the the range
T = Tc −10−4 × step , step = 0, 5000.
We take large lattices say L = 30 −50 with TTH = TMC = 210.
Part III
The susceptibility near the critical temperature in 2D Ising model scales
as
χ
L2 ∼|T −Tc|−γ , γ = 7
4.
Determine γ numerically. Use TTH = 210, TMC = 213, L = 50 with the two ranges
T = Tc −5 × 10−4 × step , step = 0, 100.
T = Tc −0.05 −4.5 × 10−3step , step = 0, 100.
9.10
Simulation 20: The 2-Point Correlator
In this exercise we will continue our study of the ferromagnetic second order phase
transition.
In particular we will calculate the 2-point correlator deﬁned by the
expression
f(n) = ⟨s0sn⟩=
*
1
4L2
X
i,j
φ(i, j)
 φ(i + n, j) + φ(i −n, j) + φ(i, j + n)
+ φ(i, j −n)

+
.
(1) Verify that at T = Tc the behaviour of f(n) is given by
f(n) ≃1
nη , η = 1
4.
(2) Verify that at T less than Tc the behaviour of f(n) is given by
f(n) = ⟨M⟩2.
(3) Verify that at T larger than Tc the behaviour of f(n) is given by
f(n) ≃a 1
nη e−n
ξ .
In all the above questions we take odd lattices say L = 2LL + 1 with LL =
20 −50. We also consider the parameters TTH = 210, TTC = 213.
(4) Near Tc the correlation length diverges as
ξ ≃
1
|T −Tc|ν , ν = 1.

104
An Introduction to Monte Carlo Simulations of Matrix Field Theory
In the above question we take LL = 20. We also consider the parameters
TTH = 210, TTC = 215 and the temperatures
T = Tc + 0.1 × step , step = 0, 10.
9.11
Simulation 21: Hysteresis and the First Order Phase
Transition
In this exercise we consider the eﬀect of the magnetic ﬁeld on the physics of the
Ising model. We will observe a ﬁrst order phase transition at H = 0 or H near 0
and a phenomena of hysteresis.
(1) We will compute the magnetization and the energy as functions of H for a
range of temperatures T. The initialization will be done once for all H. The
thermalization will be performed once for the ﬁrst value of the magnetic ﬁeld
H say H = −5. After we compute the magnetization for H = −5, we start
slowly (adiabatically) changing the magnetic ﬁeld with small steps so we do
not loose the thermalization of the Ising system of spins. We try out the range
H = −5, 5 with step equal 0.25.
a- For T < Tc say T = 0.5 and 1.5 determine the ﬁrst order transition
point from the discontinuity in the energy and the magnetization. The
transition should happen at a non-zero value of H due to hysteresis. The
jump in the energy is associated with a non-zero latent heat. The jumps
in the energy and the magnetization are the typical signal for a ﬁrst order
phase transition.
b- For T > Tc say T = 3 and 5 the magnetization becomes a smooth function
of H near H = 0 which means that above Tc there is no distinction
between the ferromagnetic states with M ≥0 and M ≤0.
(2) We recompute the magnetization as a function of H for a range of H from −5
to 5 and back. You should observe a hysteresis loop.
a- Verify that the hysteresis window shrinks with increasing temperature or
accumulating more Monte Carlo time.
b- Verify what happens if we increase the size of the lattice.
The phenomena of hysteresis indicates that the behaviour of the system de-
pends on its initial state and history or equivalently the system is trapped in
metastable states.

Chapter 10
Metropolis Algorithm for Yang–Mills
Matrix Models
10.1
Dimensional Reduction
10.1.1
Yang–Mills Action
In a four dimensional Minkowski spacetime with metric gµν = (+1, −1, −1, −1), the
Yang–Mills action with a topological theta term is given by
S = −1
2g2
Z
d4xTrFµνF µν −
θ
16π2
Z
d4xTrFµν ˜F µν.
(10.1)
We recall the deﬁnitions
Dα = ∂α −i[Aα, ...].
(10.2)
Fµν = ∂µAν −∂νAµ −i[Aµ, Aν].
(10.3)
˜F µν = 1
2ϵµναβFαβ.
(10.4)
The path integral of interest is
Z =
Z
DAµ exp(iS).
(10.5)
This is invariant under the ﬁnite gauge transformations Aµ −→g−1Aµg + ig−1∂µg
with g = eiΛ in some group G (we will consider mostly SU(N)).
We Wick rotate to Euclidean signature as x0 −→x4 = ix0 and as a consequence
d4x −→d4
Ex = id4x, ∂0 −→∂4 = −i∂0 and A0 −→A4 = −iA0. We compute
FµνF µν −→(F 2
µν)E and Fµν ˜F µν −→i(Fµν ˜Fµν)E. We get then
ZE =
Z
DAµ exp(−SE).
(10.6)
SE =
1
2g2
Z
(d4x)ETr(F 2
µν)E +
iθ
16π2
Z
(d4x)ETr(Fµν ˜Fµν)E.
(10.7)
We remark that the theta term is imaginary. In the following we will drop the
subscript E for simplicity. Let us consider ﬁrst the θ = 0 (trivial) sector. The pure
Yang–Mills action is deﬁned by
SYM =
1
2g2
Z
d4xTrF 2
µν.
(10.8)
107

108
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The path integral is of the form
Z
DAµ exp(−1
2g2
Z
d4xTrF 2
µν).
(10.9)
First we ﬁnd the equations of motion. We have
δSYM = 1
g2
Z
d4x TrFµνδFµν
= 2
g2
Z
d4x TrFµνDµδAν
= −2
g2
Z
d4x TrDµFµν.δAν + 2
g2
Z
d4x TrDµ(FµνδAν)
= −2
g2
Z
d4x TrDµFµν.δAν + 2
g2
Z
d4x Tr∂µ(FµνδAν).
(10.10)
The equations of motion for variations of the gauge ﬁeld which vanish at inﬁnity
are therefore given by
DµFµν = 0.
(10.11)
Equivalently
∂µFµν −i[Aµ, Fµν] = 0.
(10.12)
We can reduce to zero dimension by assuming that the conﬁgurations Aa are con-
stant conﬁgurations, i.e. are x-independent. We employ the notation Aa = Xa. We
obtain immediately the action and the equations of motion
SYM = −VR4
2g2 Tr[Xµ, Xν]2.
(10.13)
[Xµ, [Xµ, Xν]] = 0.
(10.14)
10.1.2
Chern–Simons Action: Myers Term
Next we consider the general sector θ ̸= 0. First we show that the second term in
the action SE does not aﬀect the equations of motion. In other words, the theta
term is only a surface term. We deﬁne
Lθ =
1
16π2 TrFµν ˜Fµν.
(10.15)
We compute the variation
δLθ =
1
16π2 ϵµναβTrFµνδFαβ
=
1
8π2 ϵµναβTrFµνDαδAβ.
(10.16)
We use the Jacobi identity
ϵµναβDαFµν = ϵµναβ(∂αFµν −i[Aα, Fµν])
= −ϵµναβ[Aα, [Aµ, Aν]]
= 0.
(10.17)

Metropolis Algorithm for Yang–Mills Matrix Models
109
Thus
δLθ =
1
8π2 ϵµναβTrDα(FµνδAβ)
=
1
8π2 ϵµναβTr
 ∂α(FµνδAβ) −i[Aα, FµνδAβ]

= ∂αδKα.
(10.18)
δKα =
1
8π2 ϵαµνβTrFµνδAβ.
(10.19)
This shows explicitly that the theta term will not contribute to the equations of
motion for variations of the gauge ﬁeld which vanish at inﬁnity.
In order to ﬁnd the current Kα itself we adopt the method of [1]. We consider a
one-parameter family of gauge ﬁelds Aµ(x, τ) = τAµ(x) with 0 ≤τ ≤1. By using
the above result we have immediately
∂
∂τ Kα =
1
8π2 ϵαµνβTrFµν(x, τ) ∂
∂τ Aβ
=
1
8π2 ϵαµνβTr
 τ∂µAν −τ∂νAµ −iτ 2[Aµ, Aν]

Aβ(x).
(10.20)
By integrating both sides with respect to τ between τ = 0 and τ = 1 and setting
Kα(x, 1) = Kα(x) and Kα(x, 0) = 0 we get
Kα =
1
8π2 ϵαµνβTr
1
2∂µAν −1
2∂νAµ −i
3[Aµ, Aν]

Aβ(x).
(10.21)
The theta term is proportional to an integer k (known variously as the Pontryagin
class, the winding number, the instanton number and the topological charge) deﬁned
by
k =
Z
d4xLθ
=
Z
d4x∂αKα.
(10.22)
Now we imagine that the four-dimensional Euclidean spacetime is bounded by a
large three-sphere S3 in the same way that we can imagine that the plane is bounded
by a large S1, viz
∂R4 = S3
∞.
(10.23)
Then
k =
Z
∂R4=S3∞
d3σαKα
=
1
16π2 ϵαµνβ
Z
∂R4=S3
∞
d3σαTr

FµνAβ + i2
3AµAνAβ

.
(10.24)
The Chern–Simons action is deﬁned by
SCS = iθk.
(10.25)

110
An Introduction to Monte Carlo Simulations of Matrix Field Theory
A Yang–Mills instanton is a solution of the equations of motion which has ﬁnite
action. In order to have a ﬁnite action the ﬁeld strength Fµν must approach 0 at
inﬁnity at least as 1/x2, viz1
F I
µν(x) = o(1/x2) , x −→∞.
(10.26)
We can immediately deduce that the gauge ﬁeld must approach a pure gauge at
inﬁnity, viz
AI
µ(x) = ig−1∂µg + o(1/x) , x −→∞.
(10.27)
This can be checked by simple substitution in Fµν = ∂µAν −∂νAµ −i[Aµ, Aν]. Now
a gauge conﬁguration AI
µ(x) at inﬁnity (on the sphere S3
∞) deﬁnes a group element g
which satisﬁes (from the above asymptotic behavior) the equation ∂µg−1 = iAI
µg−1
or equivalently
d
dsg−1(x(s), x0) = idxµ
ds AI
µ(x(s))g−1(x(s), x0).
(10.28)
The solution is given by the path-ordered Wilson line
g−1(x, x0) = P exp

i
Z 1
0
dsdyµ
ds AI
µ(y(s))

.
(10.29)
The path is labeled by the parameter s which runs from s = 0 (y = x0) to s = 1
(y = x) and the path-ordering operator P is deﬁned such that terms with higher
values of s are always put on the left in every order in the Taylor expansion of the
exponential.
In the above formula for g−1 the points x and x0 are both at inﬁnity, i.e. on the
sphere S3
∞. In other words gauge conﬁgurations with ﬁnite action (the instanton
conﬁgurations AI
µ(x)) deﬁne a map from S3
∞into G, viz
g−1 : S3
∞−→G.
(10.30)
These maps are classiﬁed by homotopy theory.
As an example we take the group G = SU(2). The group SU(2) is topologically
a three-sphere since any element g ∈SU(2) can be expanded (in the fundamental
representation) as g = n4+i⃗n⃗τ and as a consequence the unitarity condition g+g = 1
becomes n2
4 + ⃗n2 = 1. In this case we have therefore maps from the three-sphere to
the three-sphere, viz
g−1 : S3
∞−→SU(2) = S3.
(10.31)
These maps are characterized precisely by the integer k introduced above. This
number measures how many times the second S3 (group) is wrapped (covered) by
the ﬁrst sphere S3
∞(space). In fact this is the underlying reason why k must be
quantized. In other words k is an element of the third homotopy group π3(S3), viz2
k ∈π3(SU(2)) = π3(S3) = Z.
(10.32)
1The requirement of ﬁnite action can be neatly satisﬁed if we compactify R4 by adding one point
at ∞to obtain the four-sphere S4.
2In general πn(Sn) = Z. It is obvious that π1(S1) = π2(S2) = Z.

Metropolis Algorithm for Yang–Mills Matrix Models
111
For general SU(N) we consider instanton conﬁgurations obtained by embedding
the SU(2) instanton conﬁgurations into SU(N) matrices as
ASU(N)
µ
=
 
0
0
0 ASU(2)
µ
!
.
(10.33)
We can obviously use any spin j representation of SU(2) provided it ﬁts inside the
N × N matrices of SU(N). The case N = 2j + 1 is equivalent to choosing the
generators of SU(2) in the spin j representation as the ﬁrst 3 generators of SU(N)
and hence ASU(N)a
µ
, a = 1, 2, 3 are given by the SU(2) instanton conﬁgurations
whereas the other components ASU(N)a
µ
, a = 4, ..., N 2 −1 are zero identically. The
explicit constructions of all these instanton solutions will not be given here.
The story of instanton calculus is beautiful but long and complicated and we
can only here refer the reader to the vast literature on the subject. See for example
the pedagogical lectures [2].
We go back to the main issue for us which is the zero dimensional reduction of
the Chern–Simons term. By using the fact that on S3
∞we have Fµν = 0 we can
rewrite (10.24) as
k =
i
24π2 ϵαµνβ
Z
∂R4=S3∞
d3σαTrAµAνAβ.
(10.34)
By using also the fact that Aµ = AI
µ = ig−1∂µg = iXµ on S3
∞we have
k =
1
24π2 ϵαµνβ
Z
∂R4=S3∞
d3σαTrXµXνXβ.
(10.35)
By introducing now a local parametrization ξa = ξa(x) of the G group elements we
can rewrite k as (with Xa = g−1∂ag)
k =
1
24π2 ϵαµνβ
Z
∂R4=S3∞
d3σα
∂ξa
∂xµ
∂ξb
∂xν
∂ξc
∂xβ
TrXaXbXc.
(10.36)
Next we use
d3σα = 1
6ϵαµνβdxµ ∧dxν ∧dxβ.
(10.37)
ϵαµνβϵαµ′ν′β′ = δµ
′ν
′β
′
[µνβ]
= δµ
′
µ (δν
′
ν δβ
′
β −δν
′
β δβ
′
ν ) + δµ
′
ν (δν
′
β δβ
′
µ −δν
′
µ δβ
′
β )
+ δµ
′
β (δν
′
µ δβ
′
ν −δν
′
ν δβ
′
µ ).
(10.38)
We get
k =
1
24π2
1
6δµ
′ν
′β
′
[µνβ]
Z
∂R4=S3∞
dxµ′ ∧dxν′ ∧dxβ′ ∂ξa
∂xµ
∂ξb
∂xν
∂ξc
∂xβ
TrXaXbXc
=
1
24π2
Z
∂R4=S3
∞
dξa ∧dξb ∧dξcTrXaXbXc
=
1
24π2
Z
∂R4=S3∞
d3ξϵabcTrXaXbXc.
(10.39)

112
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The trace Tr is generically (2j + 1)-dimensional, and not N-dimensional, corre-
sponding to the spin j representation of SU(2). The Chern-Simons action becomes
SCS =
iθ
24π2
Z
∂R4=S3∞
d3ξϵabcTrXaXbXc.
(10.40)
As before we can reduce to zero dimension by assuming that the conﬁgurations Xa
are constant. We obtain immediately
SCS = iθVS3
24π2 ϵabcTrXaXbXc.
(10.41)
By putting (10.13) and (10.41) we obtain the matrix action
SE = −VR4
2g2 Tr[Xµ, Xν]2 + iθVS3
24π2 ϵabcTrXaXbXc.
(10.42)
We choose to perform the scaling
Xµ −→
 Ng2
2VR4
1/4
Xµ.
(10.43)
The action becomes
SE = −N
4 Tr[Xµ, Xν]2 + i2Nα
3
ϵabcTrXaXbXc.
(10.44)
The new coupling constant α is given by
α =
1
16π2
θVS3
N
 Ng2
2VR4
3/4
.
(10.45)
10.2
Metropolis Accept/Reject Step
In the remainder we only consider the basic Yang–Mills matrix action to be of
interest. This is given by
SYM[X] = −N
4 Tr[Xµ, Xν]2
= −N
d
X
µ=1
d
X
ν=µ+1
(XµXνXµXν −X2
µX2
ν).
(10.46)
The path integral or partition function of this model is given by
Z =
Z Y
µ
dXµ exp(−SYM).
(10.47)
The meaning of the measure is obvious since Xµ are N × N matrices. The corre-
sponding probability distribution for the matrix conﬁgurations Xµ is given by
P(X) = 1
Z exp(−SYM[X]).
(10.48)
We want to sample this probability distribution in Monte Carlo using the Metropolis
algorithm. Towards this end, we need to compute the variation of the action under
the following arbitrary change
Xλ −→X′
λ = Xλ + ∆Xλ,
(10.49)

Metropolis Algorithm for Yang–Mills Matrix Models
113
where
(∆Xλ)nm = dδniδmj + d∗δnjδmi.
(10.50)
The corresponding variation of the action is
∆SYM = ∆S1 + ∆S2.
(10.51)
The two pieces ∆S1 and ∆S2 are given respectively by
∆S1 = −N
X
σ
Tr[Xσ, [Xλ, Xσ]]∆Xλ
= −Nd
X
σ
[Xσ, [Xλ, Xσ]]ji −Nd∗X
σ
[Xσ, [Xλ, Xσ]]ij.
(10.52)
∆S2 = −N
2
X
σ̸=λ
[∆Xλ, Xσ]2
= −N
2 d
X
σ̸=λ
[Xσ, [∆Xλ, Xσ]]ji −N
2 d∗X
σ̸=λ
[Xσ, [∆Xλ, Xσ]]ij
= −N
X
σ̸=λ

d2(Xσ)ji(Xσ)ji + (d∗)2(Xσ)ij(Xσ)ij + 2dd∗(Xσ)ii(Xσ)jj
−dd∗ (X2
σ)ii + (X2
σ)jj

−1
2(d2 + (d∗)2)
 (X2
σ)ii + (X2
σ)jj

δij

. (10.53)
The Metropolis accept/reject step is based on the probability distribution
P[X] = min(1, exp(−∆SYM).
(10.54)
It is not diﬃcult to show that this probability distribution satisﬁes detailed balance,
and as a consequence, this algorithm is exact, i.e. free from systematic errors.
10.3
Statistical Errors
We use the Jacknife method to estimate statistical errors. Given a set of T = 2P
(with P some integer) data points f(i) we proceed by removing z elements from the
set in such a way that we end up with n = T/z sets (or bins). The minimum number
of data points we can remove is z = 1 and the maximum number is z = T −1. The
average of the elements of the ith bin is
⟨y(j)⟩i =
1
T −z

T
X
j=1
f(j) −
z
X
j=1
f((i −1)z + j)

, i = 1, n.
(10.55)
For a ﬁxed partition given by z the corresponding error is computed as follows
e(z) =
v
u
u
tn −1
n
n
X
i=1
(⟨y(j)⟩i −⟨f⟩)2 , ⟨f⟩= 1
T
T
X
j=1
f(j).
(10.56)
We start with z = 1 and we compute the error e(1) then we go to z = 2 and compute
the error e(2). The true error is the largest value. Then we go to z = 3, compute
e(3), compare it with the previous error and again retain the largest value and so
on until we reach z = T −1.

114
An Introduction to Monte Carlo Simulations of Matrix Field Theory
10.4
Auto-Correlation Time
In any given ergodic process we obtain a sequence (Markov chain) of ﬁeld/matrix
conﬁgurations φ1, φ2,...,φT . We will assume that φi are thermalized conﬁgurations.
Let f some (primary) observable with values fi ≡f(φi) in the conﬁgurations φi
respectively. The average value ⟨f⟩of f and the statistical error δf are given by
the usual formulas
⟨f⟩= 1
T
T
X
i=1
fi.
(10.57)
δf =
σ
√
T
.
(10.58)
The standard deviation (the variance) is given by
σ2 = ⟨f 2⟩−⟨f⟩2.
(10.59)
The above theoretical estimate of the error is valid provided the thermalized conﬁg-
urations φ1, φ2, ..., φT are statistically uncorrelated, i.e. independent. In real sim-
ulations, this is certainly not the case. In general, two consecutive conﬁgurations
will be dependent, and the average number of conﬁgurations which separate two
really uncorrelated conﬁgurations is called the auto-correlation time. The correct
estimation of the error must depend on the auto-correlation time.
We deﬁne the auto-correlation function Γj and the normalized auto-correlation
function ρj for the observable f by
Γj =
1
T −j
T −j
X
i=1
(fi −⟨f⟩)(fi+j −⟨f⟩).
(10.60)
ρj = Γj
Γ0
.
(10.61)
These function vanish if there is no auto-correlation. Obviously Γ0 is the variance
σ2, viz Γ0 = σ2. In the generic case, where the auto-correlation function is not zero,
the statistical error in the average ⟨f⟩will be given by
δf =
σ
√
T
√
2τint.
(10.62)
The so-called integrated auto-correlation time τint is given in terms of the normalized
auto-correlation function ρj by
τint = 1
2 +
∞
X
j=1
ρj.
(10.63)
The auto-correlation function Γj, for large j, can not be precisely determined, and
hence, one must truncate the sum over j in τint at some cut-oﬀM, in order to

Metropolis Algorithm for Yang–Mills Matrix Models
115
not increase the error δτint in τint by simply summing up noise. The integrated
auto-correlation time τint should then be deﬁned by
τint = 1
2 +
M
X
j=1
ρj.
(10.64)
The value M is chosen as the ﬁrst integer between 1 and T such that
M ≥4τint + 1.
(10.65)
The error δτint in τint is given by
δτint =
r
4M + 2
T
τint.
(10.66)
This formalism can be generalized to secondary observables F which are functions
of n primary observables f α, viz F = F(f 1, f 2, ..., f n). See for example [3].
In general two among the three parameters of the molecular dynamics (the time
step dt, the number of iterations n and the time interval T = ndt) should be
optimized in such a way that the acceptance rate is ﬁxed, for example, between
70 and 90 per cent. We ﬁx n and optimize dt along the line discussed in previous
chapters. We make, for every N, a reasonable guess for the value of the number of
iterations n, based on trial and error, and then work with that value throughout.
For example, for N between N = 4 and N = 8, we found the value n = 10, to be
suﬃciently reasonable.
10.5
Code and Sample Calculation
Typically, we run Tther +Tmeas Monte Carlo steps where thermalization is supposed
to occur within the ﬁrst Tther steps, which are then discarded, while measurements
are performed on a sample consisting of the subsequent Tmeas conﬁgurations. We
choose, for N = 4 −8, Tther = 211 and Tmeas = 211. The interval from which we
draw the variations d and d∗is updated after each Metropolis step by requiring that
the acceptance rate is ﬁxed between 25 and 30 per cent. We generate our random
numbers using the algorithm RAN2. We do not discuss auto-correlations while error
bars are estimated using the jackknife method as discussed above. A FORTRAN
code along these lines is included in the last chapter for illustrative purposes. This
seems to go as fast as N 4.
Some thermalized results for N = 8, 10, for dimensions between d = 2 and
d = 10, are shown in ﬁgure 10.1.
The observed linear ﬁt (ﬁgure 10.2) for the average action is in excellent agree-
ment with the exact analytic result, which follows from the invariance of the path
integral under the translations Xµ −→Xµ + ϵXµ, given by
⟨S⟩
N 2 −1 = d
4.
(10.67)

116
An Introduction to Monte Carlo Simulations of Matrix Field Theory
 20
 40
 60
 80
 100
 120
 140
 160
 180
 200
 220
 0
 500
 1000
 1500
 2000
 2500
action
Monte Carlo time
thermalized action for N=8
d=3
d=4
d=10
Figure 10.1: Thermalization of the action in the Yang–Mills matrix model.
 0
 0.5
 1
 1.5
 2
 2.5
 3
 2
 3
 4
 5
 6
 7
 8
 9
 10
<S>/(N2-1)
dimension
thermalized action for N=8,10
N=8
N=10
theory
Figure 10.2: Average value of the action in the Yang–Mills matrix model.

Metropolis Algorithm for Yang–Mills Matrix Models
117
References
[1] A. M. Polyakov, ‘Gauge ﬁelds and strings,’ Contemp. Concepts Phys. 3, 1 (1987).
[2] S. Vandoren and P. van Nieuwenhuizen, ‘Lectures on instantons,’ arXiv:0802.1862
[hep-th].
[3] S. Schaefer, ‘imulations with the Hybrid Monte Carlo Algorithm: implementation and
data analysis’.

Chapter 11
Hybrid Monte Carlo Algorithm for
Yang–Mills Matrix Models
11.1
The Yang–Mills Matrix Action
The hybrid Monte Carlo algorithm is a combination of the molecular dynamics
method and the Metropolis algorithm. In this section we will follow [1,2] and [3–5].
We are still interested in the Euclidean Yang–Mills matrix model
SYM = −Nγ
4
d
X
µ,ν=1
Tr[Xµ, Xν]2 + V (X).
(11.1)
γ is some parameter, and V is some U(N)-invariant potential in the d matrices Xµ.
In this chapter we will take a potential consisting of a harmonic oscillator term and
a Chern–Simons term in the three directions X1, X2 and X3 given by
V = 1
2m2TrX2
µ + 2Niα
3
ϵabcTrXaXbXc.
(11.2)
The path integral we wish to sample in Monte Carlo simulation is
ZYM =
Z
d
Y
µ=1
dXµ exp(−SYM[X]).
(11.3)
Firstly, we will think of the gauge conﬁgurations Xµ as evolving in some ﬁctitious
time-like parameter t, viz
Xµ ≡Xµ(t).
(11.4)
The above path integral is then equivalent to the Hamiltonian dynamical system
ZYM =
Z Y
µ
dPµ
Y
µ
dXµ exp
 
−1
2
d
X
µ=1
TrP 2
µ −SYM[X]
!
.
(11.5)
In other words, we have introduced d Hermitian matrices Pµ which are obviously
N × N, and which are conjugate to Xµ. The Hamiltonian is clearly given by
H = 1
2
d
X
µ=1
TrP 2
µ + SYM[X].
(11.6)
119

120
An Introduction to Monte Carlo Simulations of Matrix Field Theory
In summary, we think of the matrices Xµ as ﬁelds in one dimension with corre-
sponding conjugate momenta Pµ. The Hamiltonian equations of motion read
∂H
∂(Pµ)ij
= ( ˙Xµ)ij ,
∂H
∂(Xµ)ij
= −( ˙Pµ)ij.
(11.7)
We have then the equations of motion
(Pµ)ji = ( ˙Xµ)ij.
(11.8)
∂SYM
∂(Xµ)ij
= −Nγ
d
X
ν=1
[Xν, [Xµ, Xν]]ji +
∂V
∂(Xµ)ij
= −( ˙Pµ)ij.
(11.9)
We will deﬁne
(Vµ)ij(t) =
∂SYM
∂(Xµ)ij(t)
= −Nγ
d
X
ν=1
[Xν, [Xµ, Xν]]ji +
∂V
∂(Xµ)ij
= −Nγ
 2XνXµXν −X2
νXµ −XµX2
ν

ji + m2(Xµ)ji
+ 2iαN[X2, X3]jiδµ1 + 2iαN[X3, X1]jiδµ2 + 2iαN[X1, X2]jiδµ3.
(11.10)
11.2
The Leap Frog Algorithm
The ﬁrst task we must face up with is to solve the above diﬀerential equations.
The numerical solution of these diﬀerential equations is formulated as follows.
We consider Taylor expansions of (Xµ)ij(t + δt) and (Pµ)ij(t + δt) up to order δt2
given by
(Xµ)ij(t + δt) = (Xµ)ij(t) + δt( ˙Xµ)ij(t) + δt2
2 ( ¨Xµ)ij(t) + ....
(11.11)
(Pµ)ij(t + δt) = (Pµ)ij(t) + δt( ˙Pµ)ij(t) + δt2
2 ( ¨Pµ)ij(t) + ....
(11.12)
We calculate that
( ¨Xµ)ij = ( ˙Pµ)ji = −∂SYM
∂(Xµ)ji
= N
d
X
ν=1
[Xν, [Xµ, Xν]]ij −
∂V
∂(Xµ)ji
.
(11.13)
( ¨Pµ)ij = −
X
kl,ν
∂2SYM
∂(Xν)kl∂(Xµ)ij
( ˙Xν)kl
= N
d
X
ν=1
 [P T
ν , [Xµ, Xν]] + [Xν, [P T
µ , Xν]] + [Xν, [Xµ, P T
ν ]]

ji
−
X
kl,ν
∂2V
∂(Xν)kl∂(Xµ)ij
( ˙Xν)kl.
(11.14)

Hybrid Monte Carlo Algorithm for Yang–Mills Matrix Models
121
For generic non-local potentials V the second equation will be approximated by
( ¨Pµ)ij = ( ˙Pµ)ij(t + δt) −( ˙Pµ)ij(t)
δt
= −1
δt

∂SYM
∂(Xµ)ij(t + δt) −
∂SYM
∂(Xµ)ij(t)

.
(11.15)
Taylor expansions of (Xµ)ij(t + δt) and (Pµ)ij(t + δt) become
(Xµ)ij(t + δt) = (Xµ)ij(t) + δt(Pµ)ji(t) −δt2
2
∂SYM
∂(Xµ)ji(t) + ....
(11.16)
(Pµ)ij(t + δt) = (Pµ)ij(t) −δt
2
∂SYM
∂(Xµ)ij(t) −δt
2
∂SYM
∂(Xµ)ij(t + δt) + .... (11.17)
We write these two equations as the three equations
(Pµ)ij

t + δt
2

= (Pµ)ij(t) −δt
2
∂SYM
∂(Xµ)ij(t).
(11.18)
(Xµ)ij(t + δt) = (Xµ)ij(t) + δt(Pµ)ji

t + δt
2

.
(11.19)
(Pµ)ij(t + δt) = (Pµ)ij

t + δt
2

−δt
2
∂SYM
∂(Xµ)ij(t + δt).
(11.20)
By construction (Xµ)ij(t + δt) and (Pµ)ij(t + δt) solve Hamilton equations.
What we have done here is to integrate Hamilton equations of motion according
to the so-called leap-frog algorithm. The main technical point to note is that the
coordinates (Xµ)ij at time t + δt are computed in terms of the coordinates (Xµ)ij
at time t and the conjugate momenta (Pµ)ij not at time t but at time t + δt/2.
The conjugate momenta (Pµ)ij at time t + δt are then computed using the new
coordinates (Xµ)ij at time t+δt and the conjugate momenta (Pµ)ij at time t+δt/2.
The conjugate momenta (Pµ)ij at time t + δt/2 are computed ﬁrst in terms of the
coordinates (Xµ)ij and the conjugate momenta (Pµ)ij at time t.
We consider a lattice of points t = nδt, n = 0, 1, 2, ..., ν −1, ν where (Xµ)ij(t) =
(Xµ)ij(n) and (Pµ)ij(t) = (Pµ)ij(n). The point n = 0 corresponds to the initial
conﬁguration (Xµ)ij(0) = (Xµ)ij whereas n = ν corresponds to the ﬁnal conﬁgu-
ration (Xµ)ij(T) = (Xµ)
′
ij where T = νδt. The momenta (Pµ)ij(t) at the middle
points n + 1/2, n = 0, ..., ν −1 will be denoted by (Pµ)ij(n + 1/2). The above
equations take then the form
(Pµ)ij

n + 1
2

= (Pµ)ij(n) −δt
2 (Vµ)ij(n).
(11.21)
(Xµ)ij(n + 1) = (Xµ)ij(n) + δt(Pµ)ji

n + 1
2

.
(11.22)
(Pµ)ij(n + 1) = (Pµ)ij

n + 1
2

−δt
2 (Vµ)ij(n + 1).
(11.23)
This algorithm applied to the solution of the equations of motion is essentially the
molecular dynamics method.

122
An Introduction to Monte Carlo Simulations of Matrix Field Theory
11.3
Metropolis Algorithm
Along any classical trajectory we know that:
(1) The Hamiltonian is invariant.
(2) The motion is reversible in phase space.
(3) The phase space volume is preserved deﬁned by the condition
∂(X(τ), P(τ))
∂(X(0), P(0)) = 1.
(11.24)
In other words detailed balance holds along a classical trajectory. The leap-frog
method used to solve the above diﬀerential equations maintains only the last two
properties.
The violation of the ﬁrst property introduces systematic errors and
as a consequence detailed balance is violated.
It is a well established fact that
introducing a Metropolis accept/reject step at the end of each classical trajectory
will eliminate the systematic error completely. The algorithm becomes therefore
exact and it is known-together with the initial generation of the P’s according to the
Gaussian distribution-as the hybrid Monte Carlo algorithm. The hybrid algorithm
is the hybrid Monte Carlo algorithm in which the Metropolis accept/reject step is
omitted.
The diﬀerence between the hybrid algorithm and the ordinary molecular dy-
namics algorithm is that in the hybrid algorithm we refresh the momenta (Pµ)ij(t)
at the beginning of each molecular dynamics trajectory in such a way that they are
chosen from a Gaussian ensemble. In this way we avoid the ergodicity problem.
The hybrid Monte Carlo algorithm can be summarized as follows:
(1) Choose an initial conﬁguration Xµ = Xµ(0).
(2) Choose Pµ
= Pµ(0) according to the Gaussian probability distribution
exp(−1
2TrP 2
µ).
(3) Find the conﬁguration (X′
µ, P ′
µ) by solving the above diﬀerential equations of
motion, i.e. (X′
µ, P ′
µ) = (Xµ(T), Pµ(T)).
(4) Accept the conﬁguration (X′
µ, P ′
µ) with a probability min(1, e−∆H[X,P ]) where
∆H is the change in the Hamiltonian.
(5) Go back to step (2) and repeat.
Steps (2)–(4) consists one sweep or one unit of Hybrid Monte Carlo time. The
Metropolis accept/reject step guarantees detailed balance of this algorithm and ab-
sence of systematic errors which are caused by the non-invariance of the Hamiltonian
due to the discretization.

Hybrid Monte Carlo Algorithm for Yang–Mills Matrix Models
123
11.4
Gaussian Distribution
We have
Z
dPµ e−1
2 TrP 2
µ =
Z
d(Pµ)iie−1
2
P
µ
P
i(Pµ)2
ii
Z
d(Pµ)ijd(Pµ)∗
ij
× e−P
µ
P
i
P
j=i+1(Pµ)ij(Pµ)∗
ij.
(11.25)
We are therefore interested in the probability distribution
Z
dx e−1
2 ax2,
(11.26)
where a = 1/2 for diagonal and a = 1 for oﬀ-diagonal. By squaring and including
normalization we have
a
π
Z
dxdy e−1
2 a(x2+y2) =
Z 1
0
dt1
Z 1
0
dt2.
(11.27)
t1 = φ
2π , t2 = e−ar2.
(11.28)
We generate therefore two uniform random numbers t1 and t2 and write down for
diagonal elements (Pµ)ii the following equations
φ = 2πt1
r =
p
−2 ln(1 −t2)
(Pµ)ii = r cos φ.
(11.29)
For oﬀ-diagonal elements Pij we write the following equations
φ = 2πt1
r =
p
−ln(1 −t2)
(Pµ)ij = r cos φ + ir sin φ
(Pµ)ji = (Pµ)∗
ij.
(11.30)
11.5
Physical Tests
The following tests can be conducted to verify the reliability of the written code
based on the above algorithm:
• Test 1: For γ = α = 0 the problem reduces to a harmonic oscillator problem.
Indeed the system in this case is equivalent to N 2d independent harmonic
oscillators with frequency and period given by
ω = m , T = 2π
m .
(11.31)
The Hamiltonian is conserved with error seen to be periodic with period
TH = T
2 = π
m.
(11.32)

124
An Introduction to Monte Carlo Simulations of Matrix Field Theory
• Test 2: In the harmonic oscillator problem we know that the X’s are dis-
tributed according to the Gaussian distribution
Z
dXµ e−m2
2 T rX2
µ.
(11.33)
The Metropolis must generate this distribution.
• Test 3: On general ground we must have
⟨e−∆H⟩= 1
Z
Z
dPdX e−H[X,P ] e−∆H
= 1
Z
Z
dPdX e−H[X′,P ′]
= 1
Z
Z
dP ′dX′ e−H[X′,P ′]
= 1.
(11.34)
• Test 4: On general ground we must also have the Schwinger–Dyson identity
(exact result) given by
4γ⟨YM⟩+ 3α⟨CS⟩+ 2m2⟨HO⟩= d(N 2 −1).
(11.35)
YM = −N
4
d
X
µ,ν=1
Tr[Xµ, Xν]2.
(11.36)
CS = 2Ni
3 ϵabcTrXaXbXc.
(11.37)
HO = 1
2TrX2
µ.
(11.38)
• Test 5: We compute ⟨SYM⟩and Cv = ⟨S2
YM⟩−⟨SYM⟩2 for γ = 1 and m = 0.
There must be an emergent geometry phase transition in α for d = 3 and
d = 4.
• Test 6: We compute the eigenvalues distributions of the X’s in d = 3 and
d = 4 for γ = 1 and α = m = 0.
• Test 7: The Polyakove line is deﬁned by
P(k) = 1
N TreikX1.
(11.39)
We compute ⟨P(k)⟩as a function of k for m = α = 0.
11.6
Emergent Geometry: An Exotic Phase Transition
As a concrete example we consider the Bosonic d = 3 Yang–Mills matrix model
with only a Chern–Simons term, i.e. γ = 1, α ̸= 0 and m = 0. This model depends
on a single (scaled) parameter
˜α = α
√
N.
(11.40)

Hybrid Monte Carlo Algorithm for Yang–Mills Matrix Models
125
The order parameter in this problem is given by the observable radius deﬁned by
radius = TrX2
a.
(11.41)
The radius of the sphere is related to this observable by
r =
˜α2c2
radius , c2 = N 2 −1
4
.
(11.42)
A more powerful set of order parameters is given by the eigenvalues distributions
of the matrices X3, i[X1, X2], and X2
a. Other useful observables are
S3 = YM + CS, YM = −N
4 Tr[Xµ, Xν]2, CS = 2iNα
3
ϵabcTrXaXbXc.
(11.43)
The speciﬁc heat is
Cv = ⟨S2
3⟩−⟨S3⟩2.
(11.44)
An exact Schwinger–Dyson identity is given by
identity = 4⟨YM⟩+ 3⟨CS⟩≡dN 2.
(11.45)
For this so-called ARS model it is important that we remove the trace part of
the matrices Xa after each molecular dynamics step because this mode can never
be thermalized. In other words, we should consider in this case the path integral
(partition function) given by
Z =
Z
dXa exp(−S3)δ(TrXa).
(11.46)
The corresponding hybrid Monte Carlo code is included in the last chapter. We
skip here any further technical details and report only few physical results.
The ARS model is characterized by two phases: the fuzzy sphere phase and the
Yang–Mills phase. Some of the fundamental results are:
(1) The Fuzzy Sphere Phase:
• This appears for large values of ˜α. It corresponds to the class of solutions
of the equations of motion given by
[Xa, Xb] = iαφϵabcXc , φ = 1.
(11.47)
The global minimum is given by the largest irreducible representation of
SU(2) which ﬁts in N × N matrices. This corresponds to the spin l =
(N −1)/2 irreducible representation, viz
Xa = φαLa.
(11.48)
[La, Lb] = iϵabcLc , c2 =
X
a
L2
a = l(l + 1)1N , l = N −1
2
. (11.49)
The values of the various observables in these conﬁgurations are
S3 = φ3˜α4c2(φ
2 −2
3) , YM = φ4˜α4c2
2
CS = −2φ3˜α4c2
3
, radius = φ2˜α2c2.
(11.50)

126
An Introduction to Monte Carlo Simulations of Matrix Field Theory
• The eigenvalues of D3 = X3/α and i[D1, D2] = i[X1, X2]/α2 are given by
λi = −N −1
2
, ..., +N −1
2
.
(11.51)
The spectrum of [D1, D2] is a better measurement of the geometry since
all ﬂuctuations around L3 are more suppressed. Some illustrative data for
˜α = 3 and N = 4 is shown in ﬁgure 11.1.
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
-2.5
-2
-1.5
-1
-0.5
 0
 0.5
 1
 1.5
 2
 2.5
ρ
λ
eigenvalues for the ARS model for α∼  =3, N=4
i[D1,D2]
D3
Figure 11.1: The eigenvalue distribution in the fuzzy sphere phase.
(2) The Yang–Mills (Matrix) Phase:
• This appears for small values of ˜α. It corresponds to the class of solutions
of the equations of motion given by
[Xa, Xb] = 0.
(11.52)
This is the phase of almost commuting matrices. It is characterized by the
eigenvalues distribution
ρ(λ) =
3
4R3 (R2 −λ2).
(11.53)
It is believed that R = 2. We compute
⟨radius⟩= 3⟨TrX2
3⟩
= 3N
Z R
−R
dλρ(λ)λ2
= 3
5R2N.
(11.54)

Hybrid Monte Carlo Algorithm for Yang–Mills Matrix Models
127
• The above eigenvalues distribution can be derived by assuming that the
joint eigenvalues distribution of the the three commuting matrices X1, X2
and X3 is uniform inside a solid ball of radius R. This can be actually
proven by quantizing the system in the Yang–Mills phase around commut-
ing matrices [6].
• The value of the radius R is determined numerically as follows:
– The ﬁrst measurement R1 is obtained by comparing the numerical
result for ⟨radius⟩, for the biggest value of N, with the formula (11.54).
– We use R1 to restrict the range of the eigenvalues of X3.
– We ﬁt the numerical result for the density of eigenvalues of X3, for the
biggest value of N, to the parabola (11.53) in order to get a second
measurement R2.
– We may take the average of R1 and R2.
Example: For α = 0, we ﬁnd the values R1 = 2.34(N = 6), R1 = 2.15(N =
8), R1 = 2.08(N = 10), and R2 = 2.05 ± 0.01(N = 10). Sample data for
˜α = 0 with N = 6, 8 and 10 is shown in ﬁgure 11.2.
• It is found that the eigenvalues distribution, in the Yang-Mills phase, is
independent of ˜α. Sample data for ˜α = 0 −2 and N = 10 is shown in
ﬁgure 11.3.
-0.05
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
-2
-1.5
-1
-0.5
 0
 0.5
 1
 1.5
 2
ρ
λ
eigenvalues of X3 for the ARS model for α∼  =0
N=10
N=8
N=6
fit:prabola law
Figure 11.2:
The eigenvalue distribution in the Yang–Mills matrix phase as a
function of N.

128
An Introduction to Monte Carlo Simulations of Matrix Field Theory
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
-4
-3
-2
-1
 0
 1
 2
 3
ρ
λ
eigenvalues of X3 for the ARS model for N  =10
α∼  =0
α∼  =0.5
α∼  =1
α∼  =1.5
α∼ =2
Figure 11.3:
The eigenvalue distribution in the Yang–Mills matrix phase as a
function of ˜α.
(3) Critical Fluctuations: The transition between the two phases occur at ˜α =
2.1. The speciﬁc heat diverges at this point from the Yang–Mills side while
it remains constant from the fuzzy sphere side. This indicates a second order
behaviour with critical ﬂuctuations only from one side of the transition. The
Yang–Mills and Chern–Simons actions, and as a consequence the total action,
as well as the radii radius and r suﬀer a discontinuity at this point reminiscent
of a ﬁrst order behavior. The diﬀerent phases of the model are characterized
by
fuzzy sphere (˜α > ˜α∗)
matrix phase (˜α << ˜α∗)
r = 1
r = 0
Cv = 1
Cv = 0.75
The Monte Carlo results of [7], derived using the Metropolis algorithm of the
previous chapter and shown in ﬁgure 11.4, should be easily obtainable using the
attached hybrid Monte Carlo code.

Hybrid Monte Carlo Algorithm for Yang–Mills Matrix Models
129
-1.5
-1
-0.5
 0
 0.5
 1
 1.2  1.4  1.6  1.8
 2
 2.2  2.4  2.6  2.8
 3
<S>/N2
α∼
m2= 0  N=16
N=24
N=32
N=48
exact
 0
 1
 2
 3
 4
 5
 6
-2
 0
 2
 4
 6
 8
 10
 12
r
α∼
N=16
N=12
N=10
theory
 0
 5
 10
 15
 20
 25
 30
 35
-0.5
 0
 0.5
 1
 1.5
 2
 2.5
 3
Tr Xa
2/N
α∼
N=16
N=12
N=10
theory
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
 2
 2.2
 0
 1
 2
 3
 4
 5
 6
Cv
α∼
N=16
N=24
N=32
N=48
theory
Figure 11.4: The behavior of various observables across the phase transition between
the fuzzy sphere and the Yang–Mills phase.
References
[1] I. Montvay and G. Munster, ‘Quantum ﬁelds on a lattice,’ Cambridge, UK: Univ. Pr.
1994, 491 p, Cambridge Monographs on Mathematical Physics.
[2] H. J. Rothe, ‘Lattice gauge theories: An Introduction,’ World Sci. Lect. Notes Phys.
74, 1 (2005).
[3] J. Ambjorn, K. N. Anagnostopoulos, W. Bietenholz, T. Hotta and J. Nishimura,
‘Large N dynamics of dimensionally reduced 4D SU(N) super Yang-Mills theory,’
JHEP 0007, 013 (2000) [arXiv:hep-th/0003208].
[4] J. Ambjorn, K. N. Anagnostopoulos, W. Bietenholz, T. Hotta and J. Nishimura,
‘Monte Carlo studies of the IIB matrix model at large N,’ JHEP 0007, 011 (2000)
[arXiv:hep-th/0005147].
[5] K. N. Anagnostopoulos, T. Azuma, K. Nagao and J. Nishimura, ‘Impact of supersym-
metry on the nonperturbative dynamics of fuzzy spheres,’ JHEP 0509, 046 (2005)
[arXiv:hep-th/0506062].
[6] V. G. Filev and D. O’Connor, ‘On the phase structure of commuting matrix models,’
arXiv:1402.2476 [hep-th].
[7] R. Delgadillo-Blando, D. O’Connor and B. Ydri, ‘Geometry in transition: A model of
emergent geometry,’ Phys. Rev. Lett. 100, 201601 (2008) [arXiv:0712.3011 [hep-th]].

Chapter 12
Hybrid Monte Carlo Algorithm for
Noncommutative Phi-Four
12.1
The Matrix Scalar Action
The hybrid Monte Carlo algorithm is a combination of the molecular dynamics
method and the Metropolis algorithm. In this section we will apply this algorithm
to matrix Φ4 on the fuzzy sphere. This problem was studied using other techniques
in [1–4]. We will follow here [5,6].
We are interested in the Euclidean matrix model
S = Tr
 −a[La, Φ]2 + bΦ2 + cΦ4
.
(12.1)
The scaled (collapsed) parameters are given by
˜b =
b
aN
3
2 , ˜c =
c
a2N 2 .
(12.2)
The path integral we wish to sample in Monte Carlo simulation is
Z =
Z
dΦ exp(−S[Φ]).
(12.3)
As before, we will ﬁrst think of the conﬁgurations Φ as evolving in some ﬁctitious
time-like parameter t, viz
Φ ≡Φ(t).
(12.4)
The above path integral is then equivalent to the Hamiltonian dynamical system
Z =
Z
dPdΦ exp

−1
2TrP 2 −S[Φ]

.
(12.5)
In other words, we have introduced a Hermitian N ×N matrix P which is conjugate
to Φ. The Hamiltonian is clearly given by
H = 1
2TrP 2 + S[Φ].
(12.6)
In summary, we think of the matrix Φ as a ﬁeld in one dimension with corresponding
conjugate momentum P. The Hamiltonian equations of motion read
∂H
∂Pij
= ( ˙Φ)ij = Pji ,
∂H
∂Φij
= −( ˙P)ij = ∂S
∂Φij
.
(12.7)
We will deﬁne the scalar force by
Vij(t) =
∂S
∂Φij(t)
= a
 −4LaΦLa + 2L2
aΦ + 2ΦL2
a

ji + 2bΦji + 4c(Φ3)ji.
(12.8)
131

132
An Introduction to Monte Carlo Simulations of Matrix Field Theory
12.2
The Leap Frog Algorithm
The numerical solution of the above diﬀerential equations can be given by the leap
frog equations
(P)ij

t + δt
2

= (P)ij(t) −δt
2 Vij(t).
(12.9)
Φij(t + δt) = Φij(t) + δtPji

t + δt
2

.
(12.10)
Pij(t + δt) = Pij

t + δt
2

−δt
2 Vij(t + δt).
(12.11)
Let us recall that t = nδt, n = 0, 1, 2, ..., ν−1, ν where the point n = 0 corresponds to
the initial conﬁguration Φij(0) whereas n = ν corresponds to the ﬁnal conﬁguration
Φij(T) where T = νδt.
12.3
Hybrid Monte Carlo Algorithm
The hybrid Monte Carlo algorithm can be summarized as follows:
(1) Choose P(0) such that P(0) is distributed according to the Gaussian probability
distribution exp(−1
2TrP 2).
(2) Find the conﬁguration (Φ(T), P(T)) by solving the above diﬀerential equations
of motion.
(3) Accept the conﬁguration (Φ(T), P(T)) with a probability
min(1, e−∆H[Φ,P ]),
(12.12)
where ∆H is the corresponding change in the Hamiltonian when we go from
(Φ(0), P(0)) to (Φ(T), P(T)).
(4) Repeat.
12.4
Optimization
12.4.1
Partial Optimization
We start with some general comment which is not necessarily a part of the optimiza-
tion process. The scalar ﬁeld Φ is a hermitian matrix, i.e. the diagonal elements
are real, while the oﬀdiagonal elements are complex conjugate of each other. We
ﬁnd it crucial that we implement, explicitly in the code, the reality of the diagonal
elements by subtracting from Φii the imaginary part (error) which in each molecular
dynamics iteration is small but can accumulate. The implementation of the other
condition is straightforward.
In actual simulations we can ﬁx ν, for example we take ν = 20, and adjust the
step size δt, in some interval [δtmin, δtmax], in such a way that the acceptance rate pa

Hybrid Monte Carlo Algorithm for Noncommutative Phi-Four
133
is held ﬁxed between some target acceptance rates say palow = 70 and pahigh = 90
per cents. If the acceptance rate becomes larger than the target acceptance rate
pahigh, then we increase the step size δt by a factor inc = 1.2 if the outcome is within
the interval [δtmin, δtmax]. Similarly, if the acceptance rate becomes smaller than
the target acceptance rate palow, we decrease the step size by a factor dec = 0.8 if
the outcome is within the interval [δtmin, δtmax]. The adjusting of δt can be done
at each Monte Carlo step, but it can also be performed only each L simulations.
We take L = 1. A sample pseudo code is attached below. A sample of the results
is shown in ﬁgure 12.1.
pa=(Accept)/(Rejec+Accept)
cou=mod(tmc,L)
if (cou.eq.0)then
if (pa.ge.target_pa_high) then
dtnew=dt*inc
if (dtnew.le.dt_max)then
dt=dtnew
else
dt=dt_max
endif
endif
if (pa.le.target_pa_low) then
dtnew=dt*dec
if (dtnew.ge.dt_min)then
dt=dtnew
else
dt=dt_min
endif
endif
endif
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 4000
 4500
pa
time
a=0,c=1.0,b=-5.3,nu=20,pa=0.7-0.9,dt=10**(-4)-1,N=10 Tth=2**12
L=1,thermalization
measurement
L=10,thermalization
measurement
-60
-55
-50
-45
-40
-35
-30
-25
-20
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 4000
 4500
action
time
a=0,c=1.0,b=-5.3,nu=20,pa=0.7-0.9,dt=10**(-4)-1,N=10 Tth=2**12
L=1,thermalization
L=10,thermalization
Figure 12.1: The acceptance rate and thermalization of the action.

134
An Introduction to Monte Carlo Simulations of Matrix Field Theory
12.4.2
Full Optimization
A more thorough optimization of the algorithm can also be done as follows [1–3].
We take δτ small so that the acceptance rate pa is kept suﬃciently large. Then
we ﬁx ν and look for the value of δ × τ where the speed of motion in the phase
space deﬁned by δτ × pa is maximum. Then we ﬁx δτ at its optimal value and look
for the value of ν where the autocorrelation time Tau is minimum. The number of
iterations ν must also be kept relatively small so that the systematic error (which
is of order ν ×δτ 2 for every hybrid Monte Carlo unit of time) is kept small. Clearly
a small value of ν is better for the eﬃciency of the algorithm.
12.5
The Non-Uniform Order: Another Exotic Phase
12.5.1
Phase Structure
The theory (12.1) is a three-parameter model with the following three known phases:
• The usual 2nd order Ising phase transition between disordered ⟨Φ⟩= 0 and
uniform ordered ⟨Φ⟩∼1 phases. This appears for small values of c. This is the
only transition observed in commutative phi-four.
• A matrix transition between disordered ⟨Φ⟩= 0 and non-uniform ordered ⟨Φ⟩∼
γ phases with γ2 = 1. This transition coincides, for very large values of c, with
the 3rd order transition of the real quartic matrix model, i.e. the model with
a = 0, which occurs at b = −2
√
Nc. See next chapter.
• A transition between uniform ordered ⟨Φ⟩∼1 and non-uniform ordered ⟨Φ⟩∼γ
phases. The non-uniform phase, in which translational/rotational invariance is
spontaneously broken, is absent in the commutative theory. The non-uniform
phase is essentially the stripe phase observed originally on Moyal-Weyl spaces
in [7,8].
The above three phases are already present in the pure potential model V
=
Tr(bΦ2 + cΦ4). The ground state conﬁgurations are given by the matrices
Φ0 = 0.
(12.13)
Φγ =
r
−b
2cUγU + , γ2 = 1N , UU + = U +U = 1N.
(12.14)
We compute V [Φ0] = 0 and V [Φγ] = −b2/4c. The ﬁrst conﬁguration corresponds
to the disordered phase characterized by ⟨Φ⟩= 0. The second solution makes sense
only for b < 0, and it corresponds to the ordered phase characterized by ⟨Φ⟩̸= 0.
As mentioned above, there is a non-perturbative transition between the two phases
which occurs quantum mechanically, not at b = 0, but at b = b∗= −2
√
Nc, which
is known as the one-cut to two-cut transition. The idempotent γ can always be
chosen such that γ = γk = diag(1k, −1N−k). The orbit of γk is the Grassmannian
manifold U(N)/(U(k) × U(N −k)) which is dk-dimensional where dk = 2kN −2k2.

Hybrid Monte Carlo Algorithm for Noncommutative Phi-Four
135
It is not diﬃcult to show that this dimension is maximum at k = N/2, assuming
that N is even, and hence from entropy argument, the most important two-cut
solution is the so-called stripe conﬁguration given by γ = diag(1N/2, −1N/2).
In this real quartic matrix model, we have therefore three possible phases char-
acterized by the following order parameters:
⟨Φ⟩= 0 disordered phase.
(12.15)
⟨Φ⟩= ±
r
−b
2c1N Ising (uniform) phase.
(12.16)
⟨Φ⟩= ±
r
−b
2cγ matrix (nonuniform or stripe) phase.
(12.17)
However, as one can explicitly check by calculating the free energies of the respective
phases, the uniform ordered phase is not stable in the real quartic matrix model
V = Tr(bΦ2 + cΦ4).
The above picture is expected to hold for noncommutative/fuzzy phi-four theory
in any dimension, and the three phases are all stable and are expected to meet at
a triple point. This structure was conﬁrmed in two dimensions by means of Monte
Carlo simulations on the fuzzy sphere in [1,2].
12.5.2
Sample Simulations
We run simulations for every N by running Tth thermalization steps, and then
measuring observables in a sample containing Tmc thermalized conﬁgurations Φ,
where each two successive conﬁgurations are separated by Tco Monte Carlo steps in
order to reduce auto-correlation eﬀects. Most of the detail of the simulations have
already been explained. We only mention again that we estimate error bars using
the jackknife method and use the random number generator RAN2. A sample code
is attached in the last chapter.
We measure the action ⟨S⟩, the speciﬁc heat Cv, the magnetization m and the
associated susceptibility χ, the total power PT , and the power in the zero modes
P0 deﬁned respectively by
Cv = ⟨S2⟩−⟨S⟩2.
(12.18)
m = ⟨|TrΦ|⟩.
(12.19)
χ = ⟨|TrΦ|2⟩−⟨|TrΦ|⟩2.
(12.20)
PT = 1
N TrΦ2.
(12.21)
P0 =
1
N 2 (TrΦ)2.
(12.22)
We will also compute the eigenvalues of the matrix Φ by calling the library LAPACK
and then construct appropriate histograms using known techniques.

136
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Ising:
The Ising transition appears for small values of ˜c and is the easiest one to
observe in Monte Carlo simulations. We choose, for N = 8, the Monte Carlo times
Tth = 211, Tmc = 211 and Tco = 20. In other words, we ignore to take into account
auto-correlations for simplicity. The data for ˜c = 0.1, 0.2 is shown in ﬁgure 12.2.
The transition, marked by the peak of the susceptibility, occurs, for ˜c = 0.1, 0.2,
0.3 and 0.4, at ˜b = −0.5, −0.9, −1.4 and −1.75 respectively. The corresponding
linear ﬁt which goes through the origin is given by
˜c = −0.22˜b∗.
(12.23)
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
PT,P0
bT
N=8
cT=0.1,PT
P0
cT=0.2,PT
P0
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 4
 4.5
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
χ
bT
N=8
cT=0.1
cT=0.2
Figure 12.2: The Ising transition.
Matrix:
The disorder-to-non-uniform phase transition appears for large values
of ˜c and is quite diﬃcult to observe in Monte Carlo simulations due to the fact
that conﬁgurations, which have slightly diﬀerent numbers of pluses and minuses,
strongly competes for ﬁnite N, with the physically relevant stripe conﬁguration
with an equal numbers of pluses and minuses.
In principle then we should run
the simulation until a symmetric eigenvalues distribution is reached which can be
very diﬃcult to achieve in practice. We choose, for N = 8, the Monte Carlo times
Tth = 211, Tmc = 212 and Tco = 24. The data for the speciﬁc heat for ˜c = 1 −4 is
shown in ﬁgure 12.3. We also plot the data for the pure quartic matrix model for
˜c = 1 for comparison.
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
-10
-9
-8
-7
-6
-5
-4
-3
-2
-1
 0
Cv/N2
bT
N=8
a=1.0,cT=1.0
cT=2.0
cT=3.0
cT=4.0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
-4
-3.5
-3
-2.5
-2
-1.5
-1
-0.5
 0
m
bT
N=8
a=1.0,cT=1.0
Figure 12.3: The matrix transition.

Hybrid Monte Carlo Algorithm for Noncommutative Phi-Four
137
The transition for smaller value of ˜c is marked, as before, by the peak in speciﬁc
heat. However, this method becomes unreliable for larger values of ˜c since the peak
disappears. Fortunately, the transition is always marked by the point where the
eigenvalues distribution splits at λ = 0. The corresponding eigenvalues distribu-
tions are shown in ﬁgure 12.4. We include symmetric and slightly non-symmetric
distributions since both were taken into account in the data of the speciﬁc heat. The
non-symmetric distributions cause typically large ﬂuctuations of the magnetization
and peaks in the susceptibility which are very undesirable ﬁnite size eﬀects. But, on
the other hand, as we increase the value of |˜b| we are approaching the non-symmetric
uniform phase and thus the appearance of these non-symmetric distributions is very
natural. This makes the determination of the transition point very hard from the
behavior of these observables.
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
 2
-0.8
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
ρ (λ)
λ
N=8
cT=4.0, bT=-1.5
bT=-5.0
bT=-5.5
bT=-6.0
 0
 0.5
 1
 1.5
 2
 2.5
 3
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
ρ (λ)
λ
N=8
cT=4.0, bT=-7.0
bT=-8.0
bT=-9.0
bT=-10.0
Figure 12.4: The eigenvalue distribution in the matrix phase.
We have determined instead the transition point by simulating, for a given ˜c,
the pure matrix model with a = 0, in which we know that the transition occurs at
˜b∗= −2
√
˜c, and then searching in the full model with a = 1 for the value of ˜b with
an eigenvalues distribution similar to the eigenvalues distribution found for a = 0
and ˜b∗= −2
√
˜c. This exercise is repeated for ˜c = 4, 3,2 and 1 and we found the
transition points given respectively by ˜b∗= −5, −4.5, −4, and −2.75. See graphs
in ﬁgure 12.5. The corresponding linear ﬁt is given by
˜c = −1.3˜b∗−2.77.
(12.24)
Two more observations concerning this transition are in order:
• The eigenvalues distribution for the pure matrix model with a = 0 is such that
it depends only on a single parameter given by g = 4Nc/b2. See the chapter
after the next for more detail.
From the Monte Carlo data (last graph in ﬁgure 12.5) the same statement
seems to hold in the full model with a = 1 along the disorder-to-non-uniform
boundary.
• The disorder-to-non-uniform transition line seems to be better approximated
by a shift of the result ˜b∗= −2
√
˜c by a single unit in the −˜b direction. This is

138
An Introduction to Monte Carlo Simulations of Matrix Field Theory
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
-0.8
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
ρ (λ)
λ
N=8
a=0,cT=4.0, bT=-4.0
theory
a=1,cT=4.0, bT=-4.0
bT=-5.0
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
-0.8
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
ρ (λ)
λ
N=8
a=0,cT=3.0, bT=-3.4
theory
a=1,cT=3.0, bT=-3.4
bT=-4.5
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
-0.8
-0.6
-0.4
-0.2
 0
 0.2
 0.4
 0.6
 0.8
ρ (λ)
λ
N=8
theory: a=0,cT=1.0, bT=-2.0
MC: a=1,cT=1.0, bT=-2.0
theory: a=0,cT=2.0, bT=-2.8
MC: a=1,cT=2.0, bT=-2.8
theory: a=0,cT=3.0, bT=-3.4
MC: a=1,cT=3.0, bT=-3.4
theory: a=0,cT=4.0, bT=-4.0
MC: a=1,cT=4.0, bT=-4.0
Figure 12.5: The eigenvalue distribution across the matrix transition line.
roughly in accord with the analytic result for the critical point found in [9] for
the multitrace approximation (see next chapter) which is given, for a = 1, by
˜b∗= −
√
N
2
−2
√
˜c + N
6
√
˜c
.
(12.25)
Stripe:
The uniform-to-non-uniform phase transition is even more diﬃcult to ob-
serve in Monte Carlo simulations but it is expected, according to [1, 2], to only
be a continuation of the disorder-to-uniform transition line (12.23). The intersec-
tion point between the above two ﬁts equations (12.23) and (12.24) is therefore an
estimation of the triple point. This is given by
(˜c,˜b) = (0.56, −2.57).
(12.26)
However, this is not really what we observe using our code here. The uniform-
to-non-uniform phase transition is only observed for small values of ˜c from the
uniform phase to the non-uniform phase as we increase −˜b. The transition for these
small values of ˜c, such as ˜c = 0.1, 0.2, 0.3, 0.4, corresponds to a second peak in the
susceptibility and the speciﬁc heat. It corresponds to a transition from a one-cut
eigenvalues distribution symmetric around 0 to a one-cut eigenvalues distribution
symmetric around a non-zero value.
The eigenvalues distributions for ˜c = 0.3
are shown in ﬁgure 12.6. In this case we have found it much easier to determine
the transition points from the behavior of the magnetization and the powers. In

Hybrid Monte Carlo Algorithm for Noncommutative Phi-Four
139
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
-1.5
-1
-0.5
 0
 0.5
 1
 1.5
ρ (λ)
λ
N=8
cT=0.3, bT=-0.25
bT=-1.0
bT=-1.25
bT=-1.5
bT=-1.75
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
-1.5
-1
-0.5
 0
 0.5
 1
 1.5
 2
ρ (λ)
λ
N=8
cT=0.3, bT=-2.0
bT=-2.25
bT=-2.5
bT=-3.0
Figure 12.6: The eigenvalue distribution in the non-uniform ordered phase.
particular, we have determined the transition point from the broad maximum of
the magnetization which corresponds to the discontinuity of the power in the zero
modes.
The magnetization and the powers, for ˜c = 0.1, 0.2, 0.3, 0.4, are shown
in ﬁgure 12.7. The transition points were found to be −1.5, −1.7, −2 and −2.1
respectively.
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
 2
-3.5
-3
-2.5
-2
-1.5
-1
-0.5
 0
 0.5
PT,P0
bT
N=8
cT=0.2,PT
P0
cT=0.3,PT
P0
cT=0.4,PT
P0
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9
 10
-3.5
-3
-2.5
-2
-1.5
-1
-0.5
 0
 0.5
m
bT
N=8
cT=0.2
cT=0.3
cT=0.4
Figure 12.7: The magnetization and the powers across the stripe transition line.
The uniform phase becomes narrower as we approach the value ˜c = 0.5. The
speciﬁc heat and the susceptibility have a peak around ˜b = −2.25 which is consistent
with the Ising transition but the powers and the magnetization show the behavior
of the disorder-to-non-uniform-order transition. The eigenvalues distribution is also
consistent with the disorder-to-non-uniform-order transition. The value ˜c = 0.5 is
roughly the location of the triple point.
References
[1] F. Garcia Flores, X. Martin and D. O’Connor, ‘Simulation of a scalar ﬁeld on a fuzzy
sphere,’ Int. J. Mod. Phys. A 24, 3917 (2009) [arXiv:0903.1986 [hep-lat]].
[2] F. Garcia Flores, D. O’Connor and X. Martin, ‘Simulating the scalar ﬁeld on the
fuzzy sphere,’ PoS LAT 2005, 262 (2006) [hep-lat/0601012].

140
An Introduction to Monte Carlo Simulations of Matrix Field Theory
[3] X. Martin, ‘A matrix phase for the phi**4 scalar ﬁeld on the fuzzy sphere,’ JHEP
0404, 077 (2004) [hep-th/0402230].
[4] M. Panero, ‘Numerical simulations of a non-commutative theory: The Scalar model
on the fuzzy sphere,’ JHEP 0705, 082 (2007) [hep-th/0608202].
[5] J. Ambjorn, K. N. Anagnostopoulos, W. Bietenholz, T. Hotta and J. Nishimura,
‘Large N dynamics of dimensionally reduced 4D SU(N) super Yang-Mills theory,’
JHEP 0007, 013 (2000) [arXiv:hep-th/0003208].
[6] J. Ambjorn, K. N. Anagnostopoulos, W. Bietenholz, T. Hotta and J. Nishimura,
‘Monte Carlo studies of the IIB matrix model at large N,’ JHEP 0007, 011 (2000)
[arXiv:hep-th/0005147].
[7] S. S. Gubser and S. L. Sondhi, ‘Phase structure of noncommutative scalar ﬁeld the-
ories,’ Nucl. Phys. B 605, 395 (2001) [hep-th/0006119].
[8] J. Ambjorn and S. Catterall, ‘Stripes from (noncommutative) stars,’ Phys. Lett. B
549, 253 (2002) [hep-lat/0209106].
[9] B. Ydri, ‘A Multitrace Approach to Noncommutative Φ4
2,’ arXiv:1410.4881 [hep-th].

Chapter 13
Lattice HMC Simulations of Φ4
2:
A Lattice Example
References for this chapter include the elegant quantum ﬁeld theory textbook [1]
and the original articles [2–4].
13.1
Model and Phase Structure
The Euclidean φ4 action with O(N) symmetry is given by
S[φ] =
Z
ddx
1
2(∂µφi)2 + 1
2m2φiφi + λ
4 (φiφi)2

.
(13.1)
We will employ lattice regularization in which x = an,
R
ddx = ad P
n, φi(x) = φi
n
and ∂µφi = (φi
n+ˆµ −φi
n)/a. The lattice action reads
S[φ] =
X
n

−2κ
X
µ
Φi
nΦi
n+ˆµ + Φi
nΦi
n + g(Φi
nΦi
n −1)2

.
(13.2)
The mass parameter m2 is replaced by the so-called hopping parameter κ and the
coupling constant λ is replaced by the coupling constant g where
m2a2 = 1 −2g
κ
−2d ,
λ
ad−4 = g
κ2 .
(13.3)
The ﬁelds φi
n and Φi
n are related by
φi
n =
r
2κ
ad−2 Φi
n.
(13.4)
The partition function is given by
Z =
Z Y
n,i
dΦi
n e−S[φ]
=
Z
dµ(Φ) e2κ P
n
P
µ Φi
nΦi
n+ˆ
µ.
(13.5)
The measure dµ(φ) is given by
dµ(Φ) =
Y
n,i
dΦi
n e−P
n
 Φi
nΦi
n+g(Φi
nΦi
n−1)2
=
Y
n
 dN ⃗Φn e−⃗Φ2
n−g(⃗Φ2
n−1)2
≡
Y
n
dµ(Φn).
(13.6)
141

142
An Introduction to Monte Carlo Simulations of Matrix Field Theory
This is a generalized Ising model.
Indeed in the limit g −→∞the dominant
conﬁgurations are such that Φ2
1 + ... + Φ2
N = 1, i.e. points on the sphere SN−1.
Hence
R
dµ(Φn)f(⃗Φn)
R
dµ(Φn)
=
R
dΩN−1f(⃗Φn)
R
dΩN−1
, g −→∞.
(13.7)
For N = 1 we obtain
R
dµ(Φn)f(⃗Φn)
R
dµ(Φn)
= 1
2(f(+1) + f(−1)) , g −→∞.
(13.8)
Thus the limit g −→∞of the O(1) model is precisely the Ising model in d dimen-
sions. The limit g −→∞of the O(3) model corresponds to the Heisenberg model
in d dimensions. The O(N) models on the lattice are thus intimately related to spin
models.
There are two phases in this model. A disordered (paramagnetic) phase char-
acterized by ⟨Φi
n⟩= 0 and an ordered (ferromagnetic) phase characterized by
⟨Φi
n⟩= vi ̸= 0.
This can be seen in various ways.
The easiest way is to look
for the minima of the classical potential
V [φ] = −
Z
ddx
1
2m2φiφi + λ
4 (φiφi)2

.
(13.9)
The equation of motion reads

m2 + λ
2 φjφj

φi = 0.
(13.10)
For m2 > 0 there is a unique solution φi = 0 whereas for m2 < 0 there is a second
solution given by φjφj = −2m2/λ.
A more precise calculation is as follows. Let us compute the expectation value
⟨Φi
n⟩on the lattice which is deﬁned by
⟨Φi
n⟩=
R
dµ(Φ) Φi
ne2κ P
n
P
µ Φi
nΦi
n+ˆ
µ
R
dµ(Φ) e2κ P
n
P
µ ΦinΦi
n+ˆ
µ
=
R
dµ(Φ) Φi
neκ P
n Φi
n
P
µ(Φi
n+ˆ
µ+Φi
n−ˆ
µ)
R
dµ(Φ) eκ P
n Φin
P
n
P
µ(Φi
n+ˆ
µ+Φi
n−ˆ
µ) .
(13.11)
Now we approximate the spins Φi
n at the 2d nearest neighbors of each spin Φi
n by
the average vi = ⟨Φi
n⟩, viz
P
µ(Φi
n+ˆµ + Φi
n−ˆµ)
2d
= vi.
(13.12)
This is a crude form of the mean ﬁeld approximation. Equation (13.11) becomes
vi =
R
dµ(Φ) Φi
ne4κd P
n Φi
nvi
R
dµ(Φ) e4κd P
n Φinvi
=
R
dµ(Φn) Φi
ne4κdΦi
nvi
R
dµ(Φin) e4κdΦinvi
.
(13.13)

Lattice HMC Simulations of Φ4
2: A Lattice Example
143
The extra factor of 2 in the exponents comes from the fact that the coupling between
any two nearest neighbor spins on the lattice occurs twice. We write the above
equation as
vi =
∂
∂Ji ln Z[J]|Ji=4κdvi.
(13.14)
Z[J] =
Z
dµ(Φn) eΦi
nJi
=
Z
dNΦi
n e−Φi
nΦi
n−g(Φi
nΦi
n−1)2+Φi
nJi.
(13.15)
The limit g −→0:
In this case we have
Z[J] =
Z
dNΦi
n e−Φi
nΦi
n+Φi
nJi = Z[0] e
JiJi
4 .
(13.16)
In other words
vi = 2κcdvi ⇒κc = 1
2d.
(13.17)
The limit g −→∞:
In this case we have
Z[J] = N
Z
dNΦi
n δ(Φi
nΦi
n −1) eΦi
nJi
= N
Z
dNΦi
n δ(Φi
nΦi
n −1)

1 + Φi
nJi + 1
2Φi
nΦj
nJiJj + ...

.
(13.18)
By using rotational invariance in N dimensions we obtain
Z
dNΦi
n δ(Φi
nΦi
n −1) Φi
n = 0.
(13.19)
Z
dNΦi
n δ(Φi
nΦi
n −1) Φi
nΦj
n = δij
N
Z
dNΦi
n δ(Φi
nΦi
n −1) Φk
nΦk
n
= δij
N
Z[0]
N .
(13.20)
Hence
Z[J] = Z[0]

1 + JiJi
2N + ...

.
(13.21)
Thus
vi = Ji
N = 4κcdvi
N
⇒κc = N
4d.
(13.22)

144
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The limit of The Ising Model:
In this case we have
N = 1 , g −→∞.
(13.23)
We compute then
Z[J] = N
Z
dΦn δ(Φ2
n −1) eΦnJ
= Z[0] cosh J.
(13.24)
Thus
v = tanh 4κdv.
(13.25)
A graphical sketch of the solutions of this equation will show that for κ < κc there
is only one intersection point at v = 0 whereas for κ > κc there are two intersection
points away from the zero, i.e. v ̸= 0. Clearly for κ near κc the solution v is near 0
and thus we can expand the above equation as
v = 4κdv −1
3(4κd)3v2 + ....
(13.26)
The solution is
1
3(4d)2κ3v2 = κ −κc.
(13.27)
Thus only for κ > κc there is a non-zero solution.
In summary we have the two phases
κ > κc : broken, ordered, ferromagnetic
(13.28)
κ < κc : symmetric, disordered, paramagnetic.
(13.29)
The critical line κc = κc(g) interpolates in the κ −g plane between the two lines
given by
κc = N
4d , g −→∞.
(13.30)
κc = 1
2d , g −→0.
(13.31)
For d = 4 the critical value at g = 0 is κc = 1/8 for all N. This critical value can
be derived in a diﬀerent way as follows. We know that the renormalized mass at
one-loop order in the continuum φ4 with O(N) symmetry is given by the equation
m2
R = m2 + (N + 2)λI(m2, Λ)
= m2 + (N + 2)λ
16π2
Λ2 + (N + 2)λ
16π2
m2 ln m2
Λ2
+ (N + 2)λ
16π2
m2C + ﬁnite terms.
(13.32)

Lattice HMC Simulations of Φ4
2: A Lattice Example
145
This equation reads in terms of dimensionless quantities as follows
a2m2
R = am2 + (N + 2)λ
16π2
+ (N + 2)λ
16π2
a2m2 ln a2m2
+ (N + 2)λ
16π2
a2m2C + a2 × ﬁnite terms.
(13.33)
The lattice space a is formally identiﬁed with the inverse cut oﬀ1/Λ, viz
a = 1
Λ.
(13.34)
Thus we obtain in the continuum limit a −→0 the result
a2m2 −→−(N + 2)λ
16π2
+ (N + 2)λ
16π2
a2m2 ln a2m2
+ (N + 2)λ
16π2
a2m2C + a2 × ﬁnite terms.
(13.35)
In other words (with r0 = (N + 2)/8π2)
a2m2 −→a2m2
c = −r0
2 λ + O(λ2).
(13.36)
This is the critical line for small values of the coupling constant as we will now
show. Expressing this equation in terms of κ and g we obtain
1 −2g
κ
−8 −→−r0
2
g
κ2 + O(λ2).
(13.37)
This can be brought to the form

κ −1
16(1 −2g)
2
−→
1
256

1 + 16r0g −4g

+ O(g2/κ2).
(13.38)
We get the result
κ −→κc = 1
8 + (r0
2 −1
4)g + O(g2).
(13.39)
This result is of fundamental importance. The continuum limit a −→0 corresponds
precisely to the limit in which the mass approaches its critical value. This happens
for every value of the coupling constant and hence the continuum limit a −→0 is
the limit in which we approach the critical line. The continuum limit is therefore a
second order phase transition.
13.2
The HM Algorithm
We start by considering the Hamiltonian
H[φ, P] = 1
2
X
n
P i
nP i
n
+
X
n

−2κ
X
µ
Φi
nΦi
n+ˆµ + Φi
nΦi
n + g(Φi
nΦi
n −1)2

.
(13.40)

146
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The Hamilton equations of motion are
∂H
∂P in
= ˙Φi
n = P i
n
∂H
∂Φin
= −˙P i
n = V i
n.
(13.41)
The force is given by
V i
n = ∂S
∂Φin
= −2κ
X
µ
(Φi
n+ˆµ + Φi
n−ˆµ) + 2Φi
n + 4gΦi
n(Φj
nΦj
n −1).
(13.42)
The leap frog, or Stormer-Verlet, algorithm, which maintains the symmetry un-
der time reversible and the conservation of the phase space volume of the above
Hamilton equations, is then given by the equations
P i
n

t + δt
2

= (P)i
n(t) −δt
2 V i
n(t).
(13.43)
Φi
n(t + δt) = Φi
n(t) + δtP i
n

t + δt
2

.
(13.44)
P i
n(t + δt) = P i
n

t + δt
2

−δt
2 V i
n(t + δt).
(13.45)
We recall that t = nδt, n = 0, 1, 2, ..., ν −1, ν where the point n = 0 corresponds to
the initial conﬁguration Φi
n(0) whereas n = ν corresponds to the ﬁnal conﬁguration
Φi
n(T) where T = νδt. This algorithm does not conserve the Hamiltonian due to
the systematic error associated with the discretization, which goes as O(δt2), but
as can be shown the addition of a Metropolis accept-reject step will nevertheless
lead to an exact algorithm.
The hybrid Monte Carlo algorithm in this case can be summarized as follows:
(1) Choose P(0) such that P(0) is distributed according to the Gaussian probability
distribution exp(−1
2
P
n P i
nP i
n). In particular we choose P i
n such that
P i
n =
p
−2 ln(1 −x1) cos 2π(1 −x2),
(13.46)
where x1 and x2 are two random numbers uniformly distributed in the interval
[0, 1]. This step is crucial if we want to avoid ergodic problems.
(2) Find the conﬁguration (Φ(T), P(T)) by solving the above diﬀerential equations
of motion.
(3) Accept the conﬁguration (Φ(T), P(T)) with a probability
min(1, e−∆H[Φ,P ]),
(13.47)
where ∆H is the corresponding change in the Hamiltonian when we go from
(Φ(0), P(0)) to (Φ(T), P(T)).
(4) Repeat.

Lattice HMC Simulations of Φ4
2: A Lattice Example
147
13.3
Renormalization and Continuum Limit
The continuum and lattice actions for Φ4 theory in two dimensions with N = 1 are
given, with some slight change of notation, by
S[φ] =
Z
d2x
1
2(∂µφ)2 + 1
2µ2
0φ2 + λ
4 φ4

.
(13.48)
S[φ] =
X
n

−2κ
X
µ
ΦnΦn+ˆµ + Φ2
n + g(Φ2
n −1)2

.
(13.49)
µ2
0 = m2.
(13.50)
µ2
0l ≡µ2
0a2 = 1 −2g
κ
−4 , λl ≡λa2 = g
κ2 .
(13.51)
In the simulations we will start by ﬁxing the lattice quartic coupling λl and the
lattice mass parameter µ2
0l which then allows us to ﬁx κ and g as
κ =
p
8λl + (µ2
0l + 4)2 −(µ2
0l + 4)
4λl
.
(13.52)
g = κ2λl.
(13.53)
The phase diagram will be drawn originally in the µ2
0l −λl plane.
This is the
lattice phase diagram. This should be extrapolated to the inﬁnite volume limit
L = Na −→∞.
The Euclidean quantum ﬁeld theory phase diagram should be drawn in terms
of the renormalized parameters and is obtained from the lattice phase diagram by
taking the limit a −→0.
In two dimensions the Φ4 theory requires only mass
renormalization while the quartic coupling constant is ﬁnite. Indeed, the bare mass
µ2
0 diverges logarithmically when we remove the cutoﬀ, i.e. in the limit Λ −→∞
where Λ = 1/a while λ is independent of a. As a consequence, the lattice parameters
will go to zero in the continuum limit a −→0.
We know that mass renormalization is due to the tadpole diagram which is
the only divergent Feynman diagram in the theory and takes the form of a simple
reparametrization given by
µ2
0 = µ2 −δµ2,
(13.54)
where µ2 is the renormalized mass parameter and δµ2 is the counter term which
is ﬁxed via an appropriate renormalization condition. The unltraviolet divergence
ln Λ of µ2
0 is contained in δµ2 while the renormalization condition will split the
ﬁnite part of µ2
0 between µ2 and δµ2. The choice of the renormalization condition
can be quite arbitrary. A convenient choice suitable for Monte Carlo measurements
and which distinguishes between the two phases of the theory is given by the usual
normal ordering prescription [2].

148
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Quantization at one-loop gives explicitly the 2-point function
Γ(2)(p) = p2 + µ2
0 + 3λ
Z
d2k
(2π)2
1
k2 + µ2
0
.
(13.55)
A self-consistent Hartree treatment gives then the result
Γ(2)(p) = p2 + µ2
0 + 3λ
Z
d2k
(2π)2
1
Γ(2)(k)
= p2 + µ2 + 3λ
Z
d2k
(2π)2
1
Γ(2)(k) −δµ2
= p2 + µ2 + 3λ
Z
d2k
(2π)2
1
k2 + µ2
−δµ2 + two −loop.
(13.56)
This should certainly work in the symmetric phase where µ2 > 0. We can also write
this as
Γ(2)(p) = p2 + µ2 + Σ(p) , Σ(p) = 3λAµ2 −δµ2 + two −loop.
(13.57)
Aµ2 is precisely the value of the tadpole diagram given by
Aµ2 =
Z
d2k
(2π)2
1
k2 + µ2 .
(13.58)
The renormalization condition which is equivalent to normal ordering the interaction
in the interaction picture in the symmetric phase is equivalent to the choice
δµ2 = 3λAµ2.
(13.59)
A dimensionless coupling constant can the be deﬁned by
f = λ
µ2 .
(13.60)
The action becomes
S[φ] =
Z
d2x
1
2(∂µφ)2 + 1
2µ2(1 −3fAµ2)φ2 + fµ2
4 φ4

.
(13.61)
For suﬃciently small f the exact eﬀective potential is well approximated by the
classical potential with a single minimum at φcl = 0. For larger f, the coeﬃcient
of the mass term in the above action can become negative and as a consequence
a transition to the broken symmetry phase is possible, although in this regime the
eﬀective potential is no longer well approximated by the classical potential. Indeed,
a transition to the broken symmetry phase was shown to be present in [4], where a
duality between the strong coupling regime of the above action and a weakly coupled
theory normal ordered with respect to the broken phase was explicitly constructed.
The sites on the lattice are located at xµ = nµa where nµ = 0, ..., N −1 with L =
Na. The plane waves on a ﬁnite volume lattice with periodic boundary conditions
are exp(ipx) with pµ = mµ2π/L where mµ = −N/2 + 1, −N/2 + 2, ..., N/2 for N
even. This means that the zero of the x-space is located at the edge of the box while

Lattice HMC Simulations of Φ4
2: A Lattice Example
149
the zero of the p-space is located in the middle of the box. We have therefore the
normalization conditions P
x exp(−i(p −p′)x) = δp,p′ and P
p exp(−i(x −x′)p) =
δx,x′ where, for example, P
p = P
m /L2. In the inﬁnite volume limit deﬁned by L =
Na −→∞with a ﬁxed we have P
p −→
R π/a
−π/a d2p/(2π)2. It is not diﬃcult to show
that on the lattice the propagator 1/(p2+µ2) becomes a2/(4 P
µ sin2 apµ/2+µ2
l ) [1].
Thus on a ﬁnite volume lattice with periodic boundary conditions the Feynman
diagram Aµ2 takes the form
Aµ2 =
X
p1,p2
a2
4 sin2 ap1/2 + 4 sin2 ap2/2 + µ2
l
=
1
N 2
N
X
m1=1
N
X
m2=1
1
4 sin2 πm1/N + 4 sin2 πm2/N + µ2
l
.
(13.62)
In the last line we have shifted the integers m1 and m2 by N/2. Hence on a ﬁnite
volume lattice with periodic boundary conditions equation (13.54), together with
equation (13.59), becomes
F(µ2
l ) = µ2
l −3λlAµ2
l −µ2
0l = 0.
(13.63)
Given the critical value of µ2
0l for every value of λl we need then to determine the
corresponding critical value of µ2
l . This can be done numerically using the Newton-
Raphson algorithm. The continuum limit a −→0 is then given by extrapolating
the results into the origin, i.e. taking λl = a2λ −→0, µ2
l = a2µ2 −→0 in order to
determine the critical value
fc = limλl,µ2
l −→0
λl
µ2
lc
.
(13.64)
13.4
HMC Simulation Calculation of the Critical Line
We measure as observables the average value of the action, the speciﬁc heat, the
magnetization, the susceptibility and the Binder cumulant deﬁned respectively by
⟨S⟩.
(13.65)
Cv = ⟨S2⟩−⟨S⟩2.
(13.66)
M =
1
N 2 ⟨m⟩, m =

X
n
φn
.
(13.67)
χ = ⟨m2⟩−⟨m⟩2.
(13.68)
U = 1 −
⟨m4⟩
3⟨m2⟩2 .
(13.69)
We note the use of the absolute value in the deﬁnition of the magnetization
since the usual deﬁnition M = ⟨P
n φn⟩/N 2 is automatically zero on the lattice

150
An Introduction to Monte Carlo Simulations of Matrix Field Theory
because of the symmetry φ −→−φ. The speciﬁc heat diverges at the critical point
logarithmically as the lattice size is sent to inﬁnity. The susceptibility shows also
a peak at the critical point whereas the Binder cumulant exhibits a ﬁxed point for
all values of N.
We run simulations with Tth + Tmc × Tco steps with Tth = 213 thermalization
steps and Tmc = 214 measurement steps. Every two successive measurements are
separated by Tco = 23 steps to reduce auto-correlations.
We use RAN2 as our
random numbers generator and the Jackknife method to estimate error bars. The
hybrid Monte Carlo code used in these simulations can be found in the last chapter.
We have considered lattices with N = 16, 32 and 49 and values of the quartic
coupling given by λl = 1, 0.7, 0.5, 0.25. Some results are shown in ﬁgure 13.1. The
critical value µ2
0l∗for each value of λl is found from averaging the values at which the
peaks in the speciﬁc heat and the susceptibility occur. The results are shown on the
second column of table 13.1. The ﬁnal step is take the continuum limit a −→0 in
order to ﬁnd the critical value µ2
l∗by solving the renormalization condition (13.63)
using the Newton-Raphson method. This is an iterative method based on a single
iteration given by µ2
l∗= µ2
l∗−F/F
′. The corresponding results are shown on the
third column of table 13.1. The critical line is shown in ﬁgure 13.2 with a linear ﬁt
going through the origin given by
λl = (9.88 ± 0.22)µ2
l∗.
(13.70)
 0.4
 0.45
 0.5
 0.55
 0.6
 0.65
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
-1.6
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.2
Cv/N2
μ0l
2
λl=0.7
N=49
N=32
N=16
 0
 20
 40
 60
 80
 100
 120
 140
-1.6
-1.4
-1.2
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.2
χ  /N2
μ0l
2
λl=0.5
N=49
N=32
N=16
-0.1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
-1
-0.8
-0.6
-0.4
-0.2
 0
 0.2
U
μ0l
2
λl=0.25
N=49
N=32
N=16
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
-2
-1.8
-1.6
-1.4
-1.2
-1
-0.8
-0.6
-0.4
M
μ0l
2
λl=1.0
N=49
N=32
N=16
Figure 13.1: Various observables for φ4
2 on the lattice using HMC.

Lattice HMC Simulations of Φ4
2: A Lattice Example
151
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 1
 0.02
 0.03
 0.04
 0.05
 0.06
 0.07
 0.08
 0.09
 0.1
 0.11
λl
μl
2
data
fit
Figure 13.2: The critical line.
Table 13.1: The critical values.
λl
µ2
0l∗
µ2
l∗
1.0
−1.25 ± 0.05
1.00 × 10−2
0.7
−0.95 ± 0.05
6.89 × 10−2
0.5
−0.7 ± 0.00
5.52 × 10−2
0.25
−0.4 ± 0.00
2.53 × 10−2
This should be compared with the much more precise result λl = 10.8µ2
l∗published
in [3]. The above result is suﬃcient for our purposes here.
References
[1] J. Smit, ‘Introduction to Quantum Fields on a Lattice: A Robust Mate,’ Cambridge
Lect. Notes Phys. 15, 1 (2002).
[2] W. Loinaz and R. S. Willey, ‘Monte Carlo simulation calculation of critical coupling
constant for continuum phi**4 in two-dimensions,’ Phys. Rev. D 58, 076003 (1998)
[hep-lat/9712008].
[3] D. Schaich and W. Loinaz, ‘An Improved lattice measurement of the critical coupling
in phi(2)**4 theory,’ Phys. Rev. D 79, 056008 (2009) [arXiv:0902.0045 [hep-lat]].
[4] S. J. Chang, ‘The existence of a second order phase transition in the two-dimensional
phi**4 ﬁeld theory,’ Phys. Rev. D 13, 2778 (1976) [Phys. Rev. D 16, 1979 (1977)].

Chapter 14
(Multi-Trace) Quartic Matrix Models
14.1
The Pure Real Quartic Matrix Model
This is a very well known, and a very well studied, model which depends on a single
hermitian matrix M. This is given by
V = BTrM 2 + CTrM 4
= N
g

−TrM 2 + 1
4TrM 4

.
(14.1)
The model depends actually on a single coupling g such that
B = −N
g , C = N
4g .
(14.2)
There are two stable phases in this model:
Disordered phase (one-cut) for g ≥gc:
This is characterized by the eigen-
values distribution of the matrix M given by
ρ(λ) =
1
Nπ (2Cλ2 + B + Cδ2)
p
δ2 −λ2
= 1
gπ
1
2λ2 −1 + r2
p
4r2 −λ2.
(14.3)
This is a single cut solution with the cut deﬁned by
−2r ≤λ ≤2r.
(14.4)
r = 1
2δ.
(14.5)
δ2 =
1
3C (−B +
p
B2 + 12NC) = 1
3(1 +
p
1 + 3g).
(14.6)
153

154
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Non-uniform ordered phase (two-cut) for g ≤gc:
This is characterized by
the eigenvalues distribution of the matrix M given by
ρ(λ) = 2C|λ|
Nπ
q
(λ2 −δ2
1)(δ2
2 −λ2)
= |λ|
2gπ
q
(λ2 −r2
−)(r2
+ −λ2).
(14.7)
Here there are two cuts deﬁned by
r−≤|λ| ≤r+.
(14.8)
r−= δ1 , r+ = δ2.
(14.9)
r2
∓=
1
2C (−B ∓2
√
NC)
= 2(1 ∓√g).
(14.10)
A third order transition between the above two phases occurs at the critical point
gc = 1 ↔B2
c = 4NC ↔Bc = −2
√
NC.
(14.11)
There is a third phase in this model: the so-called Ising or uniform ordered phase,
which despite the fact that it is not stable, plays an important role in generalizations
of this model, such as the one discussed in the next section, towards noncommuta-
tive Φ4.
14.2
The Multi-Trace Matrix Model
Our primary interest here is the theory of noncommutative Φ4 on the fuzzy sphere
given by the action
S = 4πR2
N + 1Tr
 1
2R2 Φ∆Φ + 1
2m2Φ2 + λ
4!Φ4

.
(14.12)
The Laplacian is ∆= [La, [La, ...]].
Equivalently with the substitution Φ =
M/
√
2πθ, where M = PN
i,j=1 Mij|i⟩⟨j|, this action reads
S = Tr
 aM∆M + bM2 + cM4
.
(14.13)
The parameters are1
a =
1
2R2 , b = 1
2m2 , c = λ
4!
1
2πθ.
(14.14)
In terms of the matrix M the action reads
S[M] = r2K[M] + Tr

bM 2 + cM 4
.
(14.15)
The kinetic matrix is given by
K[M] = Tr

−Γ+MΓM −
1
N + 1Γ3MΓ3M + EM 2

.
(14.16)
1The noncommutativity parameter on the fuzzy sphere is related to the radius of the sphere by
θ = 2R2/
√
N2 −1.

(Multi-Trace) Quartic Matrix Models
155
The matrices Γ, Γ3 and E are given by
(Γ3)lm = lδlm
(Γ)lm =
r
(m −1)(1 −
m
N + 1)δlm−1
(E)lm = (l −1
2)δlm.
(14.17)
The relationship between the parameters a and r2 is given by
r2 = 2aN
(14.18)
We start from the path integral
Z =
Z
dM e−S[M]
=
Z
dΛ ∆2(Λ) exp

−Tr
 bΛ2 + cΛ4 Z
dU exp

−r2K[UΛU −1]

.
(14.19)
The second line involves the diagonalization of the matrix M (more on this below).
The calculation of the integral over U ∈U(N) is a very long calculation done
in [2,3]. The end result is a multi-trace eﬀective potential given by (assuming the
symmetry M −→−M)
Seﬀ=
X
i
(bλ2
i + cλ4
i ) −1
2
X
i̸=j
ln(λi −λj)2
+
r2
8 v2,1
X
i̸=j
(λi −λj)2 + r4
48v4,1
X
i̸=j
(λi −λj)4
−
r4
24N 2 v2,2
 X
i̸=j
(λi −λj)22 + ...

.
(14.20)
The coeﬃcients v will be given below. If we do not assume the symmetry M −→
−M then obviously there will be extra terms with more interesting consequences
for the phase structure as we will discuss brieﬂy below.
This problem (14.20) is a generalization of the quartic Hermitian matrix poten-
tial model. Indeed, this eﬀective potential corresponds to the matrix model given
by
V =

b + aN 2v2,1
2

TrM 2 +

c + a2N 3v4,1
6

TrM 4 −2ηa2N 2
3

TrM 2
2
.
(14.21)
This can also be solved exactly as shown in [2]. The strength of the multi-trace
term η is given by
η = v2,2 −3
4v4,1.
(14.22)

156
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The coeﬃcients v2,1, v4,1 and v2,2 are given by the following two competing calcu-
lations of [2] and [3] given respectively by
v2,1 = 1 , v4,1 = 0 , v2,2 = 1
8.
(14.23)
v2,1 = −1 , v4,1 = 3
2 , v2,2 = 0.
(14.24)
This discrepancy is discussed in [2].
14.3
Model and Algorithm
We thus start from the potential and the partition function
V = Tr
 BM 2 + CM 4
+ D
 TrM 22.
(14.25)
We may include the odd terms found in [2] without any real extra eﬀort. We will
not do this here for simplicity, but we will include them for completeness in the
attached code. The partition function (path integral) is given by
Z =
Z
dM e−V .
(14.26)
The relationship between the two sets of parameters {a, b, c} and {B, C, D} is given
by
B = b + aN 2v2,1
2
, C = c + a2N 3v4,1
6
, D = −2ηa2N 2
3
.
(14.27)
The collapsed parameters are
˜B = B
N
3
2 = ˜b + ˜av2,1
2
, ˜C = C
N 2 = ˜c + ˜a2v4,1
6
, D = −2η˜a2N
3
.
(14.28)
Only two of these three parameters are independent. For consistency of the large
N limit, we must choose ˜a to be any ﬁxed number. We then choose for simplicity
˜a = 1 or equivalently D = −2ηN/32.
We can now diagonalize the scalar matrix M as
M = UΛU −1.
(14.29)
We compute
δM = U
 δΛ + [U −1δU, Λ]

U −1.
(14.30)
Thus (with U −1δU = iδV being an element of the Lie algebra of SU(N))
Tr(δM)2 = Tr(δΛ)2 + Tr[U −1δU, Λ]2
=
X
i
(δλi)2 +
X
i̸=j
(λi −λj)2δVijδV ∗
ij.
(14.31)
2The authors of [1] chose a = 1 instead.

(Multi-Trace) Quartic Matrix Models
157
We count N 2 real degrees of freedom as there should be. The measure is therefore
given by
dM =
Y
i
dλi
Y
i̸=j
dVijdV ∗
ij
p
det(metric)
=
Y
i
dλi
Y
i̸=j
dVijdV ∗
ij
sY
i̸=j
(λi −λj)2.
(14.32)
We write this as
dM = dΛdU∆2(Λ).
(14.33)
The dU is the usual Haar measure over the group SU(N) which is normalized such
that
R
dU = 1, whereas the Jacobian ∆2(Λ) is precisely the so-called Vandermonde
determinant deﬁned by
∆2(Λ) =
Y
i>j
(λi −λj)2.
(14.34)
The partition function becomes
Z =
Z
dΛ ∆2(Λ) exp
 −Tr
 BΛ2 + CΛ4
−D
 TrΛ22
.
(14.35)
We are therefore dealing with an eﬀective potential given by
Veﬀ= B
X
i=1
λ2
i + C
X
i=1
λ4
i + D
 X
i=1
λ2
i
2
−1
2
X
i̸=j
ln(λi −λj)2.
(14.36)
We will use the Metropolis algorithm to study this model. Under the change λi −→
λi + h of the eigenvalue λi the above eﬀective potential changes as Veﬀ−→Veﬀ+
∆Vi,h where
∆Vi,h = B∆S2 + C∆S4 + D(2S2∆S2 + ∆S2
2) + ∆SVand.
(14.37)
The monomials Sn are deﬁned by Sn = P
i λn
i while the variations ∆Sn and ∆SVand
are given by
∆S2 = h2 + 2hλi.
(14.38)
∆S4 = 6h2λ2
i + 4hλ3
i + 4h3λi + h4.
(14.39)
∆SVand = −2
X
j̸=i
ln
1 +
h
λi −λj
.
(14.40)

158
An Introduction to Monte Carlo Simulations of Matrix Field Theory
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
-6
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
ρ (λ)
λ
g=3,N=10
theory
 0
 0.05
 0.1
 0.15
 0.2
 0.25
 0.3
 0.35
 0.4
-6
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
ρ (λ)
λ
g=2,N=10
theory
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
-6
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
ρ (λ)
λ
g=1,N=10
theory
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
-6
-5
-4
-3
-2
-1
 0
 1
 2
 3
 4
 5
ρ (λ)
λ
g=0.5,N=10
theory
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
-3
-2
-1
 0
 1
 2
 3
ρ (λ)
λ
g=3,N=10
g=2,N=10
g=1,N=10
g=0.5,N=10
Figure 14.1: The eigenvalue distribution across the third-order matrix transition.
14.4
The Disorder-to-Non-Uniform-Order Transition
The pure quartic matrix model (14.1) is characterized by a third-order phase tran-
sition between a disordered phase characterized by ⟨M⟩= 0 and a non-uniform
ordered phase characterized by ⟨M⟩= −Bγ/2C where γ is an N-dimensional idem-
potent, viz γ2 = 1. This transition is also termed one-cut-to-two-cut transition.
Thus the eigenvalues distribution of the scalar ﬁeld M will go from a one-cut solu-
tion centered around 0 in the disordered phase to a two-cut solution with two peaks
symmetric around 0 in the uniform ordered phase. The transition should occur
around g = gc = 1. This transition is critical since the two diﬀerent eigenvalues
distributions in the two phases become identical at the transition point.

(Multi-Trace) Quartic Matrix Models
159
Monte Carlo tests of the above eﬀects, and other physics, can be done using
the code found in the chapter before the last. An illustration with 220 thermalized
conﬁgurations, where each two successive conﬁgurations are separated by 25 Monte
Carlo steps to reduce auto-correlation eﬀects, and with N = 10 and g = 2, 1.5, 1, 0.5,
is shown in ﬁgure 14.1. The pure quartic matrix model is obtained from the multi-
trace matrix model by setting the kinetic parameter ˜a zero. We observe an excellent
agreement with the theoretical predictions (14.3) and (14.7).
The above transition is third-order, as we said, since the ﬁrst derivative of the
speciﬁc heat has a ﬁnite discontinuity at ¯r = B/|Bc| = −1 as is obvious from the
exact analytic result
Cv
N 2 = 1
4 , ¯r < −1.
(14.41)
Cv
N 2 = 1
4 + 2¯r4
27 −¯r
27(2¯r2 −3)
p
¯r2 + 3 , ¯r > −1.
(14.42)
This behavior is also conﬁrmed in Monte Carlo simulation as shown for ˜c = 4 and
N = 8 and N = 10 in ﬁgure 14.2.
The above one-cut-to-two-cut transition persists largely unchanged in the quar-
tic multitrace matrix model (14.21). On the other hand, and similarly to the above
pure quartic matrix model, the Ising phase is not stable in this case and as a conse-
quence the transition between non-uniform order and uniform-order is not observed
in Monte Carlo simulations. The situation is drastically diﬀerent if odd multitrace
terms are included.
 0.16
 0.17
 0.18
 0.19
 0.2
 0.21
 0.22
 0.23
 0.24
 0.25
 0.26
 0.27
-10
-8
-6
-4
-2
 0
Cv/N2
bT
cT=4
N=8
N=10
Figure 14.2: The speciﬁc heat of the pure matrix model.

160
An Introduction to Monte Carlo Simulations of Matrix Field Theory
14.5
Other Suitable Algorithms
14.5.1
Over-Relaxation Algorithm
In the case of scalar Φ4 matrix models two more algorithms are available to us. The
ﬁrst is the over-relaxation algorithm which is very useful in the case of noncommu-
tative Φ4 on the fuzzy sphere given by the action
S = 4πR2
N + 1Tr
 1
2R2 Φ∆Φ + 1
2m2Φ2 + λ
4!Φ4

.
(14.43)
We deﬁne
S2 = 4πR2
N + 1Tr
 1
2R2 Φ∆Φ + 1
2m2Φ2

, S4 = 4πR2
N + 1Tr
 λ
4!Φ4

.
(14.44)
Let Φ0 be some initial conﬁguration obtained at the end of some ergodic procedure
such as the Metropolis algorithm or the hybrid Monte Carlo algorithm. Let Φ∗
be some new completely random conﬁguration and thus completely independent
conﬁguration from Φ0. If S∗= S[Φ∗] < S0 = S[Φ0] then Φ∗will be accepted as the
new conﬁguration. We want to devise an algorithm in which the system is forced to
accept the new conﬁguration Φ∗even if S∗≥S0. This is equivalent to heating up
the system again and then letting it cool down slowly. Towards this end, we scale
the conﬁguration Φ∗as
Φ1 = αΦ∗.
(14.45)
The scale α is chosen such that
S1 = S[Φ1] = S0.
(14.46)
Equivalently
S4∗α4 + S2∗α2 −S0 = 0.
(14.47)
The solution is given by
if S0 > 0 : α2 =
p
S2
2∗+ 4S0S4∗−S2∗
2S4∗
.
(14.48)
if S0 < 0 and {S2∗< −
p
−4S0S4∗< 0} : α2 = ±
p
S2
2∗+ 4S0S4∗−S2∗
2S4∗
.
(14.49)
If the conditions in the above two equations are not met then we should redeﬁne
the matrix Φ∗iteratively as
Φ∗−→Φ∗+ Φ0
2
.
(14.50)
Then repeat. This iterative procedure will obviously create unwanted autocorrela-
tions due to the fact that Φ∗becomes closer in each iteration to Φ0. However, the
process will terminate in a ﬁnite number of steps and the obtained ﬁnal conﬁgu-
ration Φ1 has a greater probability in falling in a diﬀerent orbit than the original
Φ0.
The claim of [5] is that this algorithm solves the ergodic problem observed in
Monte Carlo simulations of noncommutative Φ4 on the fuzzy sphere.

(Multi-Trace) Quartic Matrix Models
161
14.5.2
Heat-Bath Algorithm
The second algorithm is the heat-bath algorithm which works very nicely for the
unbounded Φ4 potential
V = N
g

TrM 2 −1
4TrM 4

.
(14.51)
Remark the minus sign in front of the quartic term. Although this potential is
unbounded from below it has a well deﬁned large N limit due to the metastability
of the origin. The path integral is given by
Z =
Z
dM exp

−N
g TrM 2

exp
 N
4g TrM 4

=
Z
dMdQ exp

−N
g TrM 2 −TrQ2 +
s
N
g TrQM 2

.
(14.52)
The matrices M and Q are fully Gaussian. Let us then consider a Gaussian distri-
bution
r a
π
Z
dx exp(−ax2).
(14.53)
The Gaussian random number x must be chosen, in any Monte Carlo routine, as
R =
r
−1
a ln(1 −r1)
φ = 2πr2
x = R cos φ.
(14.54)
The r1 and r2 are two uniform random numbers between 0 and 1.
The part of the above path integral which depends on Q is Gaussian given by
Z
dQ exp

−Tr

Q −1
2
s
N
g M 2
2
.
(14.55)
The diagonal element Qii comes with a factor a = 1 while the oﬀdiagonal elements
comes with a factor a = 2. Thus we choose
Qii = zii|a=1 + 1
2
s
N
g (M 2)ii , Qij = xij + iyij
√
2

a=1 + 1
2
s
N
g (M 2)ij.
(14.56)
The x, y and z are Gaussian random numbers with a = 1.
The part of the path integral which depends on the diagonal element Mii is
given by
Z Y
i
dMii exp
X
i

−N
g

1 −
r g
N Qii

(Mii)2
+ 1
2
s
N
g
X
j̸=i
(QijMji + QjiMij)Mii

=
Z Y
i
dMii exp
X
i

−li

Mii −hi
2li
2
+ ...

.
(14.57)

162
An Introduction to Monte Carlo Simulations of Matrix Field Theory
li = N
g

1 −
r g
N Qii

, hi = 1
2
s
N
g
X
j̸=i
(QijMji + QjiMij).
(14.58)
Thus the diagonal elements Mii are Gaussian numbers which come with factors
a = li. Thus we choose
Mii = xii
√li

a=1 + hi
2li
.
(14.59)
Finally, the part of the path integral which depends on the oﬀdiagonal element Mij
is given by
Z Y
i̸=j
dMijdM ∗
ij exp
X
i̸=j
 −lijM ∗
ijMij + hijM ∗
ij + h∗
ijMij

=
Z Y
i̸=j
dMijdM ∗
ij exp
X
i̸=j

−lij|Mij −hij
lij
|2 + ...

.
(14.60)
lij = N
g

1 −1
2
r g
N (Qii + Qjj)

hij = 1
4
s
N
g
 X
k̸=i
QikMkj +
X
k̸=j
QkjMik

.
(14.61)
Hence the oﬀdiagonal elements Mij are Gaussian numbers which come with factors
a = lij. Thus we choose
Mij = xij + iyij
p
lij

a=1 + hij
lij
.
(14.62)
This algorithms can also be applied quite eﬀectively to simple Yang–Mills matrix
models as done for example in [6,7].
References
[1] F. Garcia Flores, X. Martin and D. O’Connor, ‘Simulation of a scalar ﬁeld on a fuzzy
sphere,’ Int. J. Mod. Phys. A 24, 3917 (2009) [arXiv:0903.1986 [hep-lat]].
[2] B. Ydri, ‘A multitrace approach to noncommutative Φ4
2,’ arXiv:1410.4881 [hep-th].
[3] D. O’Connor and C. Saemann, ‘Fuzzy scalar ﬁeld theory as a multitrace matrix
model,’ JHEP 0708, 066 (2007) [arXiv:0706.2493 [hep-th]].
[4] N. Kawahara, J. Nishimura and A. Yamaguchi, ‘Monte Carlo approach to nonpertur-
bative strings - demonstration in noncritical string theory,’ JHEP 0706, 076 (2007)
[hep-th/0703209].
[5] M. Panero, ‘Numerical simulations of a non-commutative theory: The scalar model
on the fuzzy sphere,’ JHEP 0705, 082 (2007) [hep-th/0608202].
[6] T. Hotta, J. Nishimura and A. Tsuchiya, ‘Dynamical aspects of large N reduced
models,’ Nucl. Phys. B 545, 543 (1999) [hep-th/9811220].
[7] T. Azuma, S. Bal, K. Nagao and J. Nishimura, ‘Nonperturbative studies of fuzzy
spheres in a matrix model with the Chern-Simons term,’ JHEP 0405, 005 (2004)
[hep-th/0401038].

Chapter 15
The Remez Algorithm and
the Conjugate Gradient Method
15.1
Minimax Approximations
The rational hybrid Monte Carlo algorithm (RHMC) uses in an essential way a
rational approximation to the fermionic determinant. Thus in this section we will
ﬁrst review the issue of rational and polynomial approximations of functions. We
will follow [4,5].
15.1.1
Minimax Polynomial Approximation and Chebyshev
Polynomials
Chebyshev norm:
We start by introducing the Chebyshev norm (also called
uniform, inﬁnity, supremum norm) of a continuous function f over the unit interval
[0, 1] by the relation
||f||∞= limn−→∞||f||n
= limn−→∞
 Z 1
0
dx|f(x)|n
1/n
= maxx|f(x)|.
(15.1)
Minimax approximation:
A minimax polynomial (or rational) approximation
of f is a polynomial (or rational) function p which minimizes the Chebyshev norm
of p −f, viz
||p −f||∞= minpmaxx|p(x) −f(x)|.
(15.2)
Weierstrass theorem:
The fundamental theorem of approximation theorem is
Weierstrass theorem. This can be stated as follows. For every continuous function
f(x) over a closed interval [a, b], and for every speciﬁed tolerance ϵ > 0, there exists
a polynomial pn(x) of some degree n such that for all x ∈[a, b], we have ||f(x) −
pn(x)||∞< ϵ. Thus any continuous function can be arbitrarily well approximated
by a polynomial. This means in particular that the space of polynomials is dense
in the space of continuous functions with respect to the topology induced by the
Chebyshev norm.
163

164
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Chebyshev theorem (minimax polynomial approximation):
We consider
a function f deﬁned on the unit interval. For any given degree n, there exists always
a unique polynomial pn of degree n which minimizes the error function
||e||∞= max0≤x≤1|e(x)| = max0≤x≤1|pn(x) −f(x)|,
(15.3)
iﬀthe error function e(x) takes its maximum absolute value at at least n + 2 points
on the unit interval, which may include the end points, and furthermore the sign of
the error alternate between the successive extrema.
We can go from the function f(x) deﬁned in the interval [−1, +1] to a function
f(y) deﬁned in a generic interval [a, b] by considering the transformation x −→y
given by
x = y −1
2(b + a)
1
2(b −a)
.
(15.4)
A simple proof of this theorem can be found in [4]. This goes as follows:
• Chebyshev’s criterion is necessary: If the error has fewer than n + 2
alternating extrema then the approximation can be improved. Let
p(x) be a polynomial for which the error e(x) = p(x)−f(x) has fewer than n+2
alternating extrema. The next largest extremum of the error, corresponding to
a local extremum, is therefore smaller by some non-zero gap ∆. Between any
two successive alternating extrema the error obviously will pass by zero at some
point zi. If we assume that we have d + 1 alternating extrema, then we will d
zeros zi. We can trivially construct the polynomial
u(x) = A
Y
i
(x −zi).
(15.5)
We choose A such that the sign of u(x) is opposite to the sign of e(x) and its
magnitude ∆′ is less than ∆, viz
u(xi)e(xi) < 0 , ∆′ = max0≤x≤1|u(x)| < ∆.
(15.6)
We consider now the polynomial p′(x) = p(x) + u(x) with corresponding error
function e′(x) = e(x) + u(x). The ﬁrst condition u(xi)e(xi) < 0 yields directly
to the conclusion that the error e′(x) is less than e(x) in the domain of the alter-
nating extrema, whereas it is the condition ∆′ < ∆that yields to the conclusion
that e′(x) is less than e(x) in the domain of the next largest extremum. Thus
e′(x) < e(x) throughout and hence p′(x) is a better polynomial approximation.
• Chebyshev’s criterion is suﬃcient:
If the error is extremal at ex-
actly n + 2 alternating points then the approximation is optimal. Let
us assume that there is another polynomial p′(x) which provides a better ap-
proximation. This means that the uniform norm ||e′||∞= max0≤x≤1|e′(x)| =
max0≤x≤1|p′(x)−f(x)| is less than ||e||∞= max0≤x≤1|e(x)| = max0≤x≤1|p(x)−
f(x)|. Equivalently we must have at the n + 2 extrema of e(xi) the inequalities
|e′(xi)| < |e(xi)|.
(15.7)

The Remez Algorithm and the Conjugate Gradient Method
165
By the requirement of continuity there must therefore exist n + 1 points zi
between the extrema at which we have
e′(zi) = e(zi).
(15.8)
This leads immediately to
p′(zi) = p(zi).
(15.9)
In other words, the polynomial p′(x) −p(x) has n + 1 zeros, but since this
polynomial is of degree n, it must vanish identically. Hence p′(x) = p(x).
Chebyshev polynomials:
The Chebyshev polynomial of degree n is deﬁned by
Tn(cos θ) = cos nθ ↔Tn(x) = cos(n cos−1 x).
(15.10)
We have the explicit expressions
T0 = 1 , T1 = x , T2 = 2x2 −1 , ...
(15.11)
From the results Tn±1 = cos nθ cos θ ∓sin nθ sin θ we deduce the recursion relation
Tn+1 = 2xTn −Tn−1.
(15.12)
These polynomials are orthogonal in the interval [−1, 1] with a weight 1/(1−x2)1/2,
viz
Z +1
−1
dx
√
1 −x2 Ti(x)Tj(x) = π
2 δij.
(15.13)
Z +1
−1
dx
√
1 −x2 T0(x)T0(x) = π.
(15.14)
The zeros of the polynomial Tn(x) are given by
Tn(cos θ) = 0 ⇒cos nθ = 0 ⇒nθ = (2k −1)π
2
⇒x = cos (2k −1)π
2n
, k = 1, 2, ..., n.
(15.15)
Since the angle θ is in the interval between 0 and π. There are therefore n zeros.
The derivative of Tn is given by
d
dxTn = −n d
dx cos−1 x sin(n cos−1 x)
=
n
√
1 −x2 sin(n cos−1 x).
(15.16)
The extrema of the polynomial Tn(x) are given by
d
dxTn = 0 ⇒sin(nθ) = 0 ⇒nθ = kπ ⇒x = cos kπ
n , k = 0, 2, ..., n. (15.17)
There are n + 1 extrema. The maxima satisfy Tn(x) = 1 while the minima satisfy
Tn(x) = −1.

166
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The Chebyshev polynomials satisfy also the following discrete orthogonality re-
lation:
m
X
k=1
Ti(xk)Tj(xk) = m
2 δij.
(15.18)
m
X
k=1
T0(xk)T0(xk) = m.
(15.19)
In the above two equations i, j < m and xk, k = 1, ..., m, are the m zeros of the
Chebyshev polynomial Tm(x).
Since Tn(x) has n + 1 extrema which alternate in value between −1 and +1
for −1 ≤x ≤1, and since the leading coeﬃcient of Tn(x) is 2n−1; the polynomial
pn(x) = xn −21−nTn(x) is the best polynomial approximation of degree n −1
with uniform weight to the function xn over the interval [−1, 1]. This is because
by construction the error en(x) = pn(x) −xn = 21−nTn(x) satisﬁes Chebyshev’s
criterion. The magnitude of the error is just ||en||∞= 21−n = 2e−n ln 2, i.e. the
error decreases exponentially with n.
Chebyshev approximation:
Let f(x) be an arbitrary function in the interval
[−1, +1].
The Chebyshev approximation of this function can be constructed as
follows.
Let N be some large degree and xk, k = 1, ..., N, be the zeros of the
Chebyshev polynomial TN(x).
The function f(x) can be approximated by the
polynomial of order N deﬁned by
fN(x) =
N
X
k=1
ckTk−1(x) −1
2c1.
(15.20)
The coeﬃcients ck are given by
cj = 2
N
N
X
k=1
f(xk)Tj−1(xk).
(15.21)
This approximation is exact for x equal to all of the N zeros of TN(x). Indeed, we
can show
N
X
k=1
Tl−1(xk)fN(xk) =
N
X
k=1
ck
N
X
k=1
Tl−1(xk)Tk−1(xk) −1
2c1
N
X
k=1
Tl−1(xk)
= N
2 cl , l = 1, ..., N.
(15.22)
In other words,
fN(xk) = f(xk).
(15.23)
For very large N, the polynomial fN becomes very close to the function f. The
polynomial fN can be “gracefully”, by using the words of [5], truncated to a lower
degree m << N by considering
fm(x) =
m
X
k=1
ckTk−1(x) −1
2c1.
(15.24)

The Remez Algorithm and the Conjugate Gradient Method
167
The error for rapidly decreasing ck, which is given by the diﬀerence between fN and
fm, is dominated by cm+1Tm which has m + 1 equal extrema distributed smoothly
and uniformly in the interval [−1, +1]. Since the T’s are bounded between −1 and
+1 the total error is the sum of the neglected ck, k = m + 1, ..., N. The Chebyshev
approximation fm(x) is very close to the minimax polynomial which has the small-
est maximum deviation from the function f(x). Although the calculation of the
Chebyshev polynomial fm(x) is very easy, ﬁnding the actual minimax polynomial
is very diﬃcult in practice.
Economization of power series:
This will be explained by means of a speciﬁc
example. We consider the function f(x) = sin x. A quintic polynomial approxima-
tion of this function is given by the Taylor expansion
sin x = x −x3
6 + x5
120.
(15.25)
The domain of deﬁnition of sin x can be taken to be the interval [−π, π]. By making
the replacement x −→x/π we convert the domain of deﬁnition [−π, π] into the
domain [−1, 1], viz
sin x = πx −π3x3
6
+ π5x5
120 .
(15.26)
The error in the above quintic approximation is estimated by the ﬁrst neglected
term evaluated at the end points x = ±1, viz
π7x7
7! |x=π = 0.6.
(15.27)
The error in the 7th degree polynomial approximation can be found in the same
way. We get in this case π9x9/9!|x=π = 0.08.
The monomials xk can be given in terms of Chebyshev polynomials by the
formulas
xk =
1
2k−1

Tk(x) +
k!
1!(k −1)!Tk−2(x) +
k!
2!(k −2)!Tk−4(x) + ...
+
k!
k−1
2 !(k −k−1
2 )!T1(x)

, k odd.
(15.28)
xk =
1
2k−1

Tk(x) +
k!
1!(k −1)!Tk−2(x) +
k!
2!(k −2)!Tk−4(x) + ...
+
k!
k
2!(k −k
2)!T0(x)

, k even.
(15.29)
For example
x = T1(x).
(15.30)
x3 = 1
4[T3(x) + 3T1(x)].
(15.31)

168
An Introduction to Monte Carlo Simulations of Matrix Field Theory
x5 = 1
16[T5(x) + 5T3(x) + 10T1(x)].
(15.32)
By substitution we get the result
sin x = πx −π3x3
6
+ π5x5
120
= π(192 −24π2 + π3)
192
T1 −π3(16 −π2)
384
T3 +
π5
1920T5.
(15.33)
Since |Tn| ≤1, the last term is of the order of 0.16. This is smaller than the error
found in the quintic approximation above. By truncating this term we obtain a
cubic approximation of the sine function given by
sin x = π(192 −24π2 + π3)
192
T1 −π3(16 −π2)
384
T3
(15.34)
By substituting the Chebyshev polynomials by their expressions in terms of the xk,
and then changing back to the interval [−π, +π], we obtain the cubic polynomial
sin x = 383
384x −5x3
32 .
(15.35)
By construction this cubic approximation is better than the above considered quintic
approximation.
15.1.2
Minimax Rational Approximation and Remez Algorithm
Chebyshev theorem revisited:
Chebyshev theorem can be extended to the
case of minimax rational approximation of functions as follows. Again we consider
a function f deﬁned on the unit interval. For any given degree (n, d), there exists
always a unique rational function rn,d of degree (n, d) which minimizes the error
function given by
||e||∞= max0≤x≤1|e(x)| = max0≤x≤1|rn,d(x) −f(x)|,
(15.36)
iﬀthe error function e(x) takes its maximum absolute value at at least n + d + 2
points on the unit interval, which may include the end points, and furthermore the
sign of the error alternate between the successive extrema.
A simple proof of this theorem can be found in [4]. As it can be shown rational
approximations are far more superior to polynomial ones since, for some functions
and some intervals, we can achieve substantially higher accuracy with the same
number of coeﬃcients. However, it should also be appreciated that constructing
the rational approximation is much more diﬃcult than the polynomial one.
We will further explain this very important theorem following the discussion
of [5].
The rational function rn,d is the ratio of two polynomials pn and qd of
degrees n and d respectively, viz
rn,d(x) = pn(x)
qd(x) .
(15.37)

The Remez Algorithm and the Conjugate Gradient Method
169
The polynomials pn and qd can be written as
pn(x) = α0 + α1x + ... + αnxn , qd(x) = 1 + β1x + ... + βdxd.
(15.38)
We will assume that rn,d is non degenerate, i.e.
it has no common polynomial
factors in numerator and denominator. The error function e(x) is the deviation of
rn,d from f(x) with a maximum absolute value e, viz
e(x) = rn,d(x) −f(x) , e = max0≤x≤1|e(x)|.
(15.39)
Equation (15.37) can be rewritten as
α0 + α1x + ... + αnxn = (f(x) + e(x))

1 + β1x + ... + βdxd

.
(15.40)
There are n+d+1 unknowns αi and βi plus one which is the error function e(x). We
can choose the rational approximation rn,x(x) to be exactly equal to the function
f(x) at n + d + 1 points xi in the interval [−1, 1], viz
f(xi) = rn,d(xi) , e(xi) = 0.
(15.41)
As a consequence the n + d + 1 unknowns αi and βi will be given by the n + d + 1
linear equations
α0 + α1xi + ... + αnxn
i = f(xi)
 1 + β1xi + ... + βdxd
i

.
(15.42)
This can be solved any standard method such as LU decomposition.
The points xi which are chosen in the interval [−1, 1] will generically be such
that there exists an extremum of the error function e(x) in each subinterval [xi, xi+1]
plus two more extrema at the endpoints ± −1 for a total of n + d + 1 extrema. In
general, the magnitudes of r(x) at the extrema are not the same.
Alternatively, we can choose the rational approximation rn,x(x), at n + d + 1
points xi, to be equal to f(x) + yi with some ﬁxed values yi of the error function
e(x). Equation (15.42) becomes
α0 + α1xi + ... + αnxn
i = (f(xi) + yi)
 1 + β1xi + ... + βdxd
i

.
(15.43)
If we choose the xi to be the extrema of the error function e(x) then the yi will
be exactly ±e where e is the maximal value of |e(x)|. We get then n + d + 2 (not
n + d + 1) equations for the unknowns αi, βi and e given by
α0 + α1xi + ... + αnxn
i = (f(xi) ± e)
 1 + β1xi + ... + βdxd
i

.
(15.44)
The ± signs are due to the fact that successive extrema are alternating between −e
and +e. Although, this is not exactly a linear system since e enters non-linearly, it
can still be solved using for example methods such as Newton–Raphson.

170
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Remez algorithm:
A practical constructive approach to the minimax rational
approximation of functions is given by Remez (or Remes) algorithm. This is a very
diﬃcult algorithm to get to work completely and properly and some people such as
the authors [5] dislike it.
The Remez algorithm involves two nested iterations; the ﬁrst on e and the second
on the xi’s. Explicitly, it goes through the following steps:
• We choose or guess n + d + 2 initial values of the points xi in the interval [0, 1].
The goal is to make these points converge to the alternating extrema discussed
above.
• The ﬁrst iteration: We keep the xi’s ﬁxed and ﬁnd the best rational approx-
imation which goes through the points (xi, f(xi) + (−1)i∆). Towards this end,
we need to solve the n + d + 2 equations
α0 + α1xi + ... + αnxn
i = (f(xi) + (−1)i∆))
 1 + β1xi + ... + βdxd
i

.
(15.45)
The unknowns are αi, βi and ∆. We write this equation as
Mv = 0.
(15.46)
The (n+d+2)-dimensional vector v is formed from the coeﬃcients αi, i = 0, ..., n
and βj, j = 0, ..., d with β0 = 1. This linear system has a non trivial solution
iﬀdetM = 0.
This condition is a polynomial in ∆.
The real roots of this
polynomial are the allowed values of ∆and each one of them will correspond
to a solution αi and βj. Each solution (αi, βj) corresponds to a certain rational
approximation rn,d(x). We pick the solution which minimizes the error function.
• The second iteration: We keep e or ∆ﬁxed and choose a new set of points
xi’s which is the best alternating set for e(x). This is done as follows. We
choose an arbitrary partition {Ii} of the interval [0, 1] where Ii is such that
xi ∈Ii. Then we choose a new set of points x′
i such that
x′
i ∈Ii , (−1)ie(x′
i) = maxx∈Ii(−1)ie(xi).
(15.47)
Several drawbacks of this algorithm are noted in [4, 5]. Among these, we mention
here the slow rate of convergence and the necessity of multiple precision arithmetic.
Zolotarev’s Theorem:
The case of rational approximations of the sign function,
the square root and the inverse square root are known analytically in the sense that
the coeﬃcients of the optimal and unique Chebyshev rational approximations are
known exactly. This result is due to Zolotarev.
The Numerical Recipes algorithm:
A much simpler but very sloppy approx-
imation, which is claimed in [5] to be “within a fraction of a least signiﬁcant bit
of the minimax one”, and in which we try to bring the error not to zero as in the
minimax case but to ± some consistent value, can be constructed as follows:

The Remez Algorithm and the Conjugate Gradient Method
171
• We start from n + d + 1 values of xi, or even a larger number of xi, which are
spaced approximately like the zeros of a higher order Chebyshev polynomials.
• We solve for αi and βj the linear system:
α0 + α1xi + ... + αnxn
i = f(xi)
 1 + β1xi + ... + βdxd
i

.
(15.48)
In the case that the number of xi’s is larger than n + d + 1 we can use the
singular value decomposition method to solve this system. The solution will
provide our starting rational approximation rn,d(x). Compute e(xi) and e.
• We solve for αi and βj the linear system:
α0 + α1xi + ... + αnxn
i = (f(xi) ± e)
 1 + β1xi + ... + βdxd
i

.
(15.49)
The ± is chosen to be the sign of the observed error function e(xi) at each
point xi.
• We repeat the second step several times.
15.1.3
The Code “AlgRemez”
This code can be found in [6].
15.2
Conjugate Gradient Method
15.2.1
Construction
Our presentation of the conjugate gradient method in this section will follow the
pedagogical note [1]. See also [2,3].
The basic problem:
We consider a symmetric and positive deﬁnite n×n matrix
A and an n-dimensional vector ⃗v. The basic problem here is to solve for the n-
dimensional vector ⃗x which satisﬁes the equation
A⃗x = ⃗v.
(15.50)
We will ﬁnd the solution by means of the conjugate gradient method which is an
iterative algorithm suited for large sparse matrices A.
Principles of the method:
The above problem is equivalent to ﬁnding the min-
imum ⃗x of the function Φ(⃗x) deﬁned by
Φ(⃗x) = 1
2⃗xA⃗x −⃗x⃗v.
(15.51)
The gradient of Φ is given by
⃗∇Φ(⃗x) = A⃗x −⃗v.
(15.52)
This vanishes at the minimum. If not zero, it gives precisely the direction of steepest
ascent of the surface Φ. The residual of the above set of equations is deﬁned by
⃗r = −⃗∇Φ(⃗x) = ⃗v −A⃗x.
(15.53)

172
An Introduction to Monte Carlo Simulations of Matrix Field Theory
We will denote the n linearly independent vectors in the vector space to which ⃗x
belongs by ⃗p(i), i = 1, ..., n. They form a basis in this vector space. The vector ⃗x
can be expanded as
⃗x =
n
X
i=1
si⃗p(i) = P⃗s.
(15.54)
P is the n × n matrix of the linearly independent vectors ⃗p(i), i.e. Pij = p(j)
i , and ⃗s
is the vector of the coeﬃcients si. Typically, we will start from a reference vector
⃗x0. Thus we write
⃗x = ⃗x0 + P⃗s.
(15.55)
The vectors ⃗p(i) are A-conjugate to each other iﬀ
⃗p(i)A⃗p(j) = 0 , i ̸= j.
(15.56)
Thus we can write
P T AP = D.
(15.57)
D is a diagonal matrix with elements given by
di = ⃗p(i)A⃗p(i).
(15.58)
The gradient of Φ takes the form
⃗∇Φ = AP⃗s −⃗r0 , ⃗r0 = ⃗v −A⃗x0.
(15.59)
Next, multiplication with the transpose P T yields
P T ⃗∇Φ = P T AP⃗s −P T⃗r0
= D⃗s −P T⃗r0.
(15.60)
The solution to ⃗∇Φ = 0 is then
D⃗s −P T⃗r0 = 0 ⇒si =
⃗p(i)⃗r0
⃗p(i)A⃗p(i) .
(15.61)
The solution si found by globally minimizing Φ, also locally minimizes Φ along the
direction ⃗p(i). Thus starting from a vector ⃗x0 we obtain the solution
⃗x1 = ⃗x0 + s1⃗p(1) , s1 =
⃗p(1)⃗r0
⃗p(1)A⃗p(1) , ⃗r0 = ⃗v −A⃗x0.
(15.62)
This is the local minimum of Φ along a line from ⃗x0 in the direction ⃗p(1). Indeed,
we can check that
⃗p(1)⃗∇Φ = 0 ⇒s1 =
⃗p(1)⃗r0
⃗p(1)A⃗p(1) .
(15.63)
The vector ⃗r0 is the ﬁrst residual at the point ⃗x0 given by
⃗∇Φ|⃗x0 = −⃗r0.
(15.64)

The Remez Algorithm and the Conjugate Gradient Method
173
Next, starting from the vector ⃗x1 we obtain the solution
⃗x2 = ⃗x1 + s2⃗p(2) , s2 =
⃗p(2)⃗r1
⃗p(2)A⃗p(2) , ⃗r1 = ⃗v −A⃗x1.
(15.65)
This is the local minimum of Φ along a line from ⃗x1 in the direction ⃗p(2). The vector
⃗r1 is the new residual at the point ⃗x1, viz
⃗∇Φ|⃗x1 = −⃗r1.
(15.66)
In general starting from the vector ⃗xi we obtain the solution
⃗xi+1 = ⃗xi + si+1⃗p(i+1) , si+1 =
⃗p(i+1)⃗ri
⃗p(i+1)A⃗p(i+1) , ⃗ri = ⃗v −A⃗xi.
(15.67)
This is the local minimum of Φ along a line from ⃗xi in the direction ⃗p(i+1). The
vector ⃗ri is the residual at the point ⃗xi, viz
⃗∇Φ|⃗xi = −⃗ri.
(15.68)
The residual vectors provide the directions of steepest descent of the function Φ at
each iteration step. Thus if we know the conjugate vectors ⃗p(i) we can compute the
coeﬃcients si and write down the solution ⃗x. Typically, a good approximation of
the true minimum of Φ may be obtained only after a small subset of the conjugate
vectors are visited.
Choosing the conjugate vectors:
The next step is to choose a set of conjugate
vectors. An obvious candidate is the set of eigenvectors of the symmetric matrix
A. However, in practice this choice is made as follows. Given that we have reached
the iteration step i, i.e. we have reached the vector ⃗xi which minimizes Φ in the
direction ⃗p(i), the search direction ⃗p(i+1) will be naturally chosen in the direction of
steepest descent of the function Φ at the point ⃗xi, which since A is positive deﬁnite
is given by the direction of the residual ⃗ri, but conjugate to the previous search
direction ⃗p(i). We start then from the ansatz
⃗p(i+1) = ⃗ri −λ⃗p(i).
(15.69)
This must be A-conjugate to ⃗p(i), viz
⃗p(i)A⃗p(i+1) = 0.
(15.70)
This yields the value
λ = ⃗p(i)A⃗ri
⃗p(i)A⃗p(i) .
(15.71)

174
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The gradient ⃗∇Φ at the point ⃗xi is orthogonal to all previous search directions ⃗p(j),
j < i. Indeed, we compute
⃗p(j)⃗∇Φ|⃗xi = ⃗p(j) A⃗xi −⃗v

= ⃗p(j)

A⃗x0 +
i
X
k=1
skA⃗p(k) −⃗v

= ⃗p(j)

i
X
k=1
skA⃗p(k) −⃗r0

=
i
X
k=1
sk⃗p(j)A⃗p(k) −⃗p(j)⃗r0
= sj⃗p(j)A⃗p(j) −⃗p(j)⃗r0
= 0.
(15.72)
This formula works also for j = i. The gradients ⃗∇Φ|⃗xi is also orthogonal to all
previous gradients ⃗∇Φ|⃗xj, j < i. Indeed, we have
⃗∇Φ|⃗xj ⃗∇Φ|⃗xi = −⃗rj ⃗∇Φ|⃗xi
= −(λ⃗p(j) + ⃗p(j+1))⃗∇Φ|⃗xi
= 0.
(15.73)
The ﬁrst search direction can be chosen arbitrarily. We can for example choose
⃗p(1) = ⃗r0 = −⃗∇Φ|⃗x0. The next search direction ⃗p(2) is by construction A-conjugate
to ⃗p(1). At the third iteration step we obtain ⃗p(3) which is A-conjugate to ⃗p(2).
The remaining question is whether ⃗p(3) is A-conjugate to ⃗p(1) or not. In general
we would like to show that the search direction ⃗p(i) generated at the ith iteration
step, which is A-conjugate to ⃗p(i−1), is also A-conjugate to all previously generated
search directions ⃗p(j), j < i −1. Thus we need to show that
⃗p(j)A⃗p(i) = 0 , j < i −1.
(15.74)
We compute
⃗p(j)A⃗p(i) = ⃗p(j)A(⃗ri−1 −λ⃗p(i−1))
= ⃗p(j)A⃗ri−1 −λ⃗p(j)A⃗p(i−1)
= 1
sj
(⃗xj −⃗xj−1)A⃗ri−1 −λ⃗p(j)A⃗p(i−1)
= 1
sj
(−⃗rj + ⃗rj−1)⃗ri−1 −λ⃗p(j)A⃗p(i−1)
= −λ⃗p(j)A⃗p(i−1)
= 0.
(15.75)

The Remez Algorithm and the Conjugate Gradient Method
175
Summary:
Let us now summarize the main ingredients of the above algorithm.
We have the following steps:
(1) We choose a reference vector ⃗x0. We calculate the initial residual ⃗r0 = ⃗v −A⃗x0.
(2) We choose the ﬁrst search direction as ⃗p(1) = ⃗r0.
(3) The ﬁrst iteration towards the solution is
⃗x1 = ⃗x0 + s1⃗p(1) , s1 =
⃗p(1)⃗r0
⃗p(1)A⃗p(1) .
(15.76)
(4) The above three steps are iterated as follows:
⃗ri = ⃗v −A⃗xi.
(15.77)
⃗p(i+1) = ⃗ri −λ⃗p(i) , λ = ⃗p(i)A⃗ri
⃗p(i)A⃗p(i) .
(15.78)
si+1 =
⃗p(i+1)⃗ri
⃗p(i+1)A⃗p(i+1) .
(15.79)
⃗xi+1 = ⃗xi + si+1⃗p(i+1).
(15.80)
By using equations (15.77) and (15.80) we can show that equation (15.77) can
be replaced by the equation
⃗ri = ⃗ri−1 −siA⃗p(i)
(15.81)
Also we can derive the more eﬃcient formulas
si+1 =
⃗ri⃗ri
⃗p(i+1)A⃗p(i+1) , λ = −
⃗ri⃗ri
⃗ri−1⃗ri−1
.
(15.82)
(5) The above procedure continues as long as |⃗r| ≥ϵ where ϵ is some tolerance,
otherwise stop.
15.2.2
The Conjugate Gradient Method as a Krylov Space Solver
We start this section by introducing some slight change of notation. By making
the replacements ⃗p(i+1) −→⃗pi, si+1 −→−βi, λ −→−αi the conjugate gradient
algorithm will read
⃗xi+1 = ⃗xi −βi⃗pi , βi = −⃗ri⃗ri
⃗piA⃗pi
.
(15.83)
⃗ri+1 = ⃗ri + βiA⃗pi.
(15.84)
⃗pi+1 = ⃗ri+1 + αi+1⃗pi , αi+1 = ⃗ri+1⃗ri+1
⃗ri⃗ri
.
(15.85)
We start iterating from
⃗x0 = 0 , ⃗r0 = ⃗v −A⃗x0 = ⃗v , ⃗p0 = ⃗r0 = ⃗v.
(15.86)

176
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Remark now the following. We have
⃗r0 = ⃗v −A⃗x0 ∈span{⃗r0}.
(15.87)
⃗r1 = ⃗r0 + β0A⃗r0 ∈span{⃗r0, A⃗r0}.
(15.88)
⃗r2 = ⃗r0 + β0A⃗r0 + β1A(⃗r0 + β0A⃗r0) + α1β1A⃗r0 ∈span{⃗r0, A⃗r0, A2⃗r0}.
(15.89)
In general we will have
⃗rn = Pn(A)⃗r0 ∈span{⃗r0, A⃗r0, A2⃗r0, ..., An⃗r0}.
(15.90)
The polynomial Pn(A) is a polynomial of degree n which obviously satisfy
Pn(0) = 1.
It is called the residual polynomial.
On the other hand, the space
span{⃗r0, A⃗r0, ..., An⃗r0} is called a Krylov subspace. Since the residues ⃗rn are or-
thogonal the polynomials Pn(A) are also orthogonal.
Similarly, we observe that
⃗p0 = ⃗r0 ∈span{⃗r0}.
(15.91)
⃗p1 = ⃗r1 + α1⃗r0 ∈span{⃗r0, A⃗r0}.
(15.92)
⃗p2 = ⃗r2 + α2⃗r1 + α1α2⃗r0 ∈span{⃗r0, A⃗r0, A2⃗r0}.
(15.93)
Thus in general
⃗pn ∈span{⃗r0, A⃗r0, A2⃗r0, ..., An⃗r0}.
(15.94)
Also
⃗xn = ⃗x0 −
n−1
X
i=0
βi⃗pi.
(15.95)
Thus
⃗xn −⃗x0 = Qn−1(A)⃗r0 ∈span{⃗r0, A⃗r0, A2⃗r0, ..., An−1⃗r0}.
(15.96)
The Qn−1(A) is a polynomial of exact degree n −1. Hence both the conjugate
gradient directions ⃗pn and the solutions ⃗xn−⃗x0 belong to various Krylov subspaces.
The conjugate gradient method is an example belonging to a large class of Krylov
subspace methods. It is due to Hestenes and Stiefel [8] and it is the method of choice
for solving linear systems that are symmetric positive deﬁnite or Hermitian positive
deﬁnite. We conclude this section by the following two deﬁnitions.
Deﬁnition 1:
Given a non-singular matrix A ∈Cn×n and a non-zero vector
r ∈Cn, the nth Krylov (sub)space Kn(A, r) generated by A from r is
Kn(A, r) = span(r, Ar, ..., An−1r).
(15.97)

The Remez Algorithm and the Conjugate Gradient Method
177
Deﬁnition 2:
A standard Krylov space method for solving a linear system Ax = b
is an iterative method which starts from some initial guess x0 with residual r0 =
b −Ax0 and then generates better approximations xn to the exact solution x∗as
follows
xn −x0 = Qn−1(A)r0 ∈Kn(A, r0) = span{r0, Ar0, A2r0, ..., An−1r0}. (15.98)
The residuals rn of the above so-called Krylov space solver will satisfy
rn = Pn(A)r0 ∈Kn+1(A, r0) = span{r0, Ar0, A2r0, ..., Anr0}.
(15.99)
It is not diﬃcult to show that
Pn(A) = 1 −AQn−1(A).
(15.100)
15.2.3
The Multi-Mass Conjugate Gradient Method
The goal now is to solve a multi-mass linear system of the form
(A + σ)⃗x = ⃗v.
(15.101)
By a direct application of the conjugate gradient method we get the solution
⃗xσ
i+1 = ⃗xσ
i −βσ
i ⃗pσ
i , βσ
i = −
⃗rσ
i ⃗rσ
i
⃗pσ
i (A + σ)⃗pσ
i
.
(15.102)
⃗rσ
i+1 = ⃗rσ
i + βσ
i (A + σ)⃗pσ
i .
(15.103)
⃗pσ
i+1 = ⃗rσ
i+1 + ασ
i+1⃗pσ
i , ασ
i+1 = ⃗rσ
i+1⃗rσ
i+1
⃗rσ
i ⃗rσ
i
.
(15.104)
⃗xσ
0 = 0 , ⃗rσ
0 = ⃗vσ −(A + σ)⃗xσ
0 = ⃗v , ⃗pσ
0 = ⃗rσ
0 = ⃗v.
(15.105)
There is clearly a loop over σ which could be very expensive in practice. Fortunately
we can solve, by following [7], the above multi-mass linear system using only a single
set of vector-matrix operations as follows. First we note that
⃗rσ
i+1 = ⃗rσ
i + βσ
i (A + σ)⃗pσ
i = P σ
i+1(A + σ)⃗rσ
0 ∈Ki+2(A + σ,⃗r0).
(15.106)
As discussed before the polynomials P σ
i+1 are orthogonal in A + σ. This follows
from the fact that ⃗rσ
i+1 ⊥⃗rσ
i and as a consequence
P σ
i+1(A + σ)⃗rσ
0 ⊥Ki+1(A + σ,⃗r0).
(15.107)
However, we have the obvious and fundamental fact that
Ki+1(A + σ,⃗r0) = Ki+1(A,⃗r0).
(15.108)
In other words, the polynomials P σ
i+1 are orthogonal in A as well. We must therefore
have
P σ
i+1(A + σ) = ζσ
i+1Pi+1(A).
(15.109)

178
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The polynomials P σ
i+1 are thus of a shifted structure. By the identity (15.100) it fol-
lows that the polynomials Qσ
i are not of a shifted structure. This single observation
will allow us to reduce the problem to a single set of vector-matrix operations.
By multiplying equation (15.104) by βσ
i+1(A + σ) and using equation (15.103)
we get
βσ
i+1(A + σ)⃗pσ
i+1 = βσ
i+1(A + σ)⃗rσ
i+1 + βσ
i+1ασ
i+1
βσ
i
(⃗rσ
i+1 −⃗rσ
i ).
(15.110)
By substitution in equation (15.103) we get the 3-term recurrence given by
⃗rσ
i+2 =

1 + βσ
i+1ασ
i+1
βσ
i

⃗rσ
i+1 + βσ
i+1(A + σ)⃗rσ
i+1 −βσ
i+1ασ
i+1
βσ
i
⃗rσ
i .
(15.111)
By using (15.109) we obtain
ζσ
i+2⃗ri+2 =

1 + βσ
i+1ασ
i+1
βσ
i

ζσ
i+1⃗ri+1 + βσ
i+1(A + σ)ζσ
i+1⃗ri+1 −βσ
i+1ασ
i+1
βσ
i
ζσ
i ⃗ri.
(15.112)
However, the no-sigma recurrence reads
⃗ri+2 =

1 + βi+1αi+1
βi

⃗ri+1 + βi+1A⃗ri+1 −βi+1αi+1
βi
⃗ri.
(15.113)
By comparing the A⃗ri+1 terms we obtain
βσ
n = βn
ζσ
n+1
ζσn
.
(15.114)
By comparing the ⃗ri terms and also using the above result we obtain
ασ
n = αn
ζσ
nβσ
n−1
ζσ
n−1βn−1
.
(15.115)
By comparing the ⃗ri+1 terms and also using the above two results we ﬁnd after
some calculation
ζσ
n+1 =
ζσ
nζσ
n−1βn−1
αnβn(ζσ
n−1 −ζσn) + ζσ
n−1βn−1(1 −σβn).
(15.116)
Let us conclude by summarizing the main ingredients of this algorithm. These are:
(1) We start from
⃗x = ⃗xσ
0 = 0 , ⃗r0 = ⃗rσ
0 = ⃗v , ⃗p = ⃗pσ
0 = ⃗v.
(15.117)
By setting i = −1 in (15.112) we see that we must also start from
α0 = ασ
0 = 0 , β−1 = βσ
−1 = 1 , ζσ
0 = ζσ
−1 = 1.
(15.118)
(2) We solve the no-sigma problem (we start from n = 0):
βn = −⃗rn⃗rn
⃗pnA⃗pn
⃗xn+1 = ⃗xn −βn⃗pn.
(15.119)
⃗rn+1 = ⃗rn + βnA⃗pn.
(15.120)
αn+1 = ⃗rn+1⃗rn+1
⃗rn⃗rn
⃗pn+1 = ⃗rn+1 + αn+1⃗pn.
(15.121)

The Remez Algorithm and the Conjugate Gradient Method
179
(3) We generate solutions of the sigma problems by the relations (we start from
n = 0):
ζσ
n+1 =
ζσ
nζσ
n−1βn−1
αnβn(ζσ
n−1 −ζσn) + ζσ
n−1βn−1(1 −σβn).
(15.122)
βσ
n = βn
ζσ
n+1
ζσn
.
(15.123)
⃗xσ
n+1 = ⃗xσ
n −βσ
n⃗pσ
n.
(15.124)
⃗rσ
n+1 = ζσ
n+1⃗rn+1.
(15.125)
ασ
n+1 = αn+1
ζσ
n+1βσ
n
ζσnβn
.
(15.126)
⃗pσ
n+1 = ⃗rσ
n+1 + ασ
n+1⃗pσ
n.
(15.127)
Remark how the residues are generated directly from the residues of the no-
sigma problem.
(4) The above procedure continues as long as |⃗r| ≥ϵ where ϵ is some tolerance,
otherwise stop. Thus
|⃗r| ≥ϵ , continue.
(15.128)
We ﬁnally note that in the case of a hermitian matrix, i.e. A+ = A, we must replace
in the above formulas the transpose by hermitian conjugation. For example, we
replace ⃗pT
nA⃗pn by ⃗p+
n A⃗p. The rest remains unchanged.
References
[1] E. Thompson, ‘The Conjugate Gradient Method: A Tutorial Note’.
[2] Martin H. Gutknecht, ‘A Brief Introduction to Krylov Space Methods for Solving
Linear Systems’, Frontiers of Computational Science, pp 53-62, Springer (2007).
[3] L. Chen, ‘Iterative Methods Based on Krylov Space’.
[4] A. D. Kennedy, ‘Approximation theory for matrices,’ Nucl. Phys. Proc. Suppl. 128C,
107 (2004) [hep-lat/0402037].
[5] W. H. Press, S. A. Teukolsky, W. T. Vetterling and B. P. Flannery, ‘Numerical Recipes
in FORTRAN: The Art of Scientiﬁc Computing,’ ISBN-9780521430647.
[6] M. A. Clark and A. D. Kennedy, https://github.com/mikeaclark/AlgRemez, 2005.
[7] B. Jegerlehner, ‘Krylov space solvers for shifted linear systems,’ hep-lat/9612014.
[8] M. R. Hestenes and E. Stiefel, ‘Methods of conjugate gradients for solving linear
systems,’ J. Res. Nat. Bureau Standards, 49:409–435, (1952).

Chapter 16
Monte Carlo Simulation of Fermion
Determinants
As it is well known, simulation of fermion determinants and Pfaﬃans is crucial to
lattice QCD, but as it trurns out, it is also crucial to all supersymmetric matrix
models and quantum mechanical matrix models encountered or needed in matrix
ﬁeld theory, matrix/fuzzy geometry and matrix formulation of noncommutative
geometry, supersymmetry and strings. As done before in this part of the book, the
theoretical background will be kept to a minimum, otherwise we will stray too far
aﬁeld, and we will mostly focus on practical problems. The main reference for this
chapter is [1,2]. See also [3,4]. For some subtle details of the rational hybrid Monte
Carlo algorithm see [5–8].
16.1
The Dirac Operator
The basic problem we want to solve in this section is to simulate the partition
function of N = 1 supersymmetric Yang–Mills matrix model in d = 4 dimensions
given by
ZYM =
Z
4
Y
µ=1
Xµ d¯θdθ exp
 ¯θ
 i[X4, .] + σa[Xa, .] + ξ

θ

exp(−SBYM[X]).
(16.1)
SBYM = −Nγ
4
4
X
µ,ν=1
Tr[Xµ, Xν]2.
(16.2)
The parameter γ will be set to one and we may add to the bosonic Yang–Mills
action a Chern–Simons term and a harmonic oscillator term with parameters α and
m2 respectively. The spinors ¯θ and θ are two independent complex two-component
Weyl spinors. They contain the same number of degrees of Freedom as the four-
component real Majorana spinors in four dimensions.
The scalar curvature or
fermion mass parameter is given by ξ.
The above theory is only supersymmet-
ric for a restricted set of values of the parameters γ, α, m2 and ξ. See [11] and
references therein for a discussion of this matter.
181

182
An Introduction to Monte Carlo Simulations of Matrix Field Theory
We have considered above the Dirac operator given by
D = iX4 −iXR
4 + σaXa −σaXR
a + ξ.
(16.3)
The determinant of this Dirac operator is positive deﬁnite since the eigenvalues
come in complex conjugate pairs [1]. In d = 6 and d = 10 the determinant is,
however, complex valued which presents a serious obstacle to numerical evaluation.
In these three cases, i.e. for d = 4, 6, 10, the supersymmetric path integral is well
behaved. In d = 3 the supersymmetric path integral is ill-deﬁned and only the
bosonic “quenched” approximation makes sense. The source of the divergence lies
in the so-called ﬂat directions, i.e. the set of commuting matrices. See [10] and
references therein.
It is possible to rewrite the Dirac action in the following form (with X34 =
X3 + iX4 and X± = X1 ± iX2)
Tr¯θDθ = Tr
¯θ1(X34 + ξ)θ1 + ¯θ1X−θ2 + ¯θ2X+θ1 + ¯θ2(−X+
34 + ξ)θ2

−Tr

X34¯θ1θ1 + X−¯θ1θ2 + X+¯θ2θ1 −X+
34¯θ2θ2

.
(16.4)
We expand the N × N matrices θ1, θ2 and ¯θ1, ¯θ2 as
θα =
N2
X
A=1
θA
α T A , ¯θα =
N2
X
¯
A=1
¯θA
α T A.
(16.5)
The N × N matrices T A are deﬁned by
(T A)ij = δiiAδjjA , A = N(iA −1) + jA.
(16.6)
Then we ﬁnd that
Tr¯θDθ = ¯χ1M11χ1 + ¯χ1M12χ2 + ¯χ2M21χ2 + ¯χ2M22χ2.
(16.7)
The N 2-dimensional vectors χ1, χ2 and ¯χ1, ¯χ2 are deﬁned by (χα)A = θA
α and
(¯χα)A = ¯θA
α . The matrices MAB
αβ are N 2 × N 2 deﬁned by
(M11)AB = TrT A(X34 + ξ)T B −TrX34T AT B.
(16.8)
(M12)AB = TrT AX−T B −TrX−T AT B.
(16.9)
(M21)AB = TrT AX+T B −TrX+T AT B.
(16.10)
(M22)AB = TrT A(−X+
34 + ξ)T B + TrX+
34T AT B.
(16.11)
We remark that
TrT AXT B −TrXT AT B = XjAiBδiAjB −XjBiAδjAiB.
(16.12)
Tr(T A)+T B = δiAiBδjAjB = δAB, TrT AT B = δjAiBδjBiA = δ ¯
AB. (16.13)
In the above two equations ¯A and B are such that
¯A = N(jA −1) + iA , B = N(iB −1) + jB.
(16.14)

Monte Carlo Simulation of Fermion Determinants
183
In summary, the Dirac operator in terms of the 2N 2-dimensional vectors χ and ¯χ
becomes
Tr¯θDθ = ¯χMχ.
(16.15)
Next, we observe that the trace parts of the matrices Xa drop from the partition
function. Thus the measure should read
R
dXaδ(TrXa) instead of simply
R
dXa.
Similarly, we observe that if we write θ = θ0+η1, then the trace part η will decouple
from the rest since
Tr¯θ
 i[X4, ..] + σa[Xa, ..] + ξ

θ = Tr¯θ0
 i[X4, ..] + σa[Xa, ..] + ξ

θ0 + ξ¯ηη.
(16.16)
Hence, the constant fermion modes ηα can also be integrated out from the partition
function and thus we should consider the measure
R
dθd¯θδ(Trθα)δ(Tr¯θα) instead of
R
dθd¯θ. These facts should be taken into account in the numerical study. We are
thus led to consider the partition function
ZYM =
Z
4
Y
µ=1
dXµ δ(TrXµ) det D e−SBYM[X].
(16.17)
The determinant is given by
detD =
Z
dθd¯θδ(Trθα)δ(Tr¯θα) exp
 Tr¯θDθ

=
Z
dχd¯χδ
 N2
X
A=1
(χα)AδiAjA

δ
 N2
X
A=1
(¯χα)AδiAjA

e¯χMχ
=
Z
dχ′d¯χ′e¯χ′M′χ′.
(16.18)
The vectors χ′
α, ¯χ′
α are (N 2−1)-dimensional. The matrix M′ is 2(N 2−1)×2(N 2−1)
dimensional, and it is given by
M′A′B′
αβ
= MA′B′
αβ
−MN2B′
αβ
δiA′jA′ −MA′N2
αβ
δiB′jB′
+ MN 2N2
αβ
δiA′jA′δiB′jB′.
(16.19)
We remark that
MN2N2
αβ
= ξδαβ.
(16.20)
Thus we must have
det D = det M′.
(16.21)
The partition function thus reads
ZYM =
Z
4
Y
µ=1
dXµ δ(TrXµ) e−SYM[X].
(16.22)
SYM[X] = SBYM[X] + V [X] , V = −ln det M′.
(16.23)

184
An Introduction to Monte Carlo Simulations of Matrix Field Theory
We will need
∂SBYM
∂(Xµ)ij(t) = −Nγ
4
X
ν=1
[Xν, [Xµ, Xν]]ji
= −Nγ
 2XνXµXν −X2
νXµ −XµX2
ν

ji.
(16.24)
The determinant is real positive deﬁnite since the eigenvalues are paired up. Thus,
we can introduce the positive deﬁnite operator ∆by
∆= (M
′)+M
′.
(16.25)
The action V can be rewritten as
V = −1
2 ln det ∆.
(16.26)
The leap-frog algorithm for this problem is given by
(Pµ)ij

n + 1
2

= (Pµ)ij(n) −δt
2
 ∂SBYM
∂(Xµ)ij
(n) + (Vµ)ij(n)

.
(16.27)
(Xµ)ij(n + 1) = (Xµ)ij(n) + δt(Pµ)ji

n + 1
2

.
(16.28)
(Pµ)ij(n + 1) = (Pµ)ij

n + 1
2

−δt
2
 ∂SBYM
∂(Xµ)ij
(n + 1) + (Vµ)ij(n + 1)

.
(16.29)
The eﬀect of the determinant is encoded in the matrix
(Vµ)ij =
∂V
∂(Xµ)ij
= −1
2Trad∆−1
∂∆
∂(Xµ)ij
.
(16.30)
From (16.23) and (16.30) we see that we must compute the inverse and the deter-
minant of the Dirac operator at each hybrid Monte Carlo step. However, the Dirac
operator is an N × N matrix where N = 2N 2 −2. This is proportional to the
number of degrees of freedom. Since the computation of the determinant requires
O(N 3) operations at best, through Gaussian elimination, we see that the computa-
tional eﬀort of the above algorithm will be O(N 6). Recall that the computational
eﬀort of the bosonic theory is O(N 3)1.
1Compare also with ﬁeld theory in which the number of degrees of freedom is proportional to
the volume, the computational eﬀort of the bosonic theory is O(V ) while that of the full theory,
which includes a determinant, is O(V 2).

Monte Carlo Simulation of Fermion Determinants
185
16.2
Pseudo-Fermions and Rational Approximations
We introduce pseudo-fermions in the usual way as follows. The determinant can be
rewritten in the form
det D = det M′ = (det ∆)
1
2
=
Z
dφ+dφ exp(−φ+∆−1/2φ).
(16.31)
Since D, M′ and ∆are N ×N matrices organized as 2×2 matrices, with components
given by ˆ
N × ˆ
N matrices where ˆ
N = N/2, the vectors φ+ and φ can be thought
of as two-component spinors where each component is given by an ˆ
N-dimensional
vector. We will write
φ =
 φ1
φ2

, φ+ =
 φ+
1 φ+
2

.
(16.32)
These are precisely the pseudo-fermions.
They are complex-valued instead of
Grassmann-valued degrees of freedom, and that is why they are pseudo-fermions,
with a positive deﬁnite Laplacian and thus they can be sampled in Monte Carlo
simulations in the usual way.
Furthermore, we will use the so-called rational approximation, which is why the
resulting hybrid Monte Carlo is termed rational, and which allows us to write
(det ∆)
1
2 =
Z
dφ+dφ exp(−φ+r2(∆)φ).
(16.33)
The rational approximation r(x) is given by
x−1/4 ≃r(x) = a0 +
M
X
σ=1
aσ
x + bσ
.
(16.34)
The parameters a0, aσ, bσ and M are real positive numbers which can be optimized
for any strictly positive range such as ϵ ≤x ≤1. This point was discussed at great
length previously.
Thus the pseudo-fermions are given by a heatbath, viz
φ = r−1(∆)ξ,
(16.35)
where ξ is given by the Gaussian noise P(ξ) = exp(−ξ+ξ). We write
φ =

c0 +
M
X
σ=1
cσ
∆+ dσ

ξ.
(16.36)
By using a diﬀerent rational approximation ¯r(x), in order to avoid double inversion
(see below), we rewrite the original path integral in the form
ZYM =
Z
4
Y
µ=1
dXµ
Z
dφ+dφ δ(TrXµ) e−SBYM[X] e−φ+¯r(∆)φ.
(16.37)

186
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The new rational approximation is deﬁned by
x−1/2 ≃¯r(x) = a0 +
M
X
σ=1
aσ
x + bσ
.
(16.38)
The full action becomes
SYM = SBYM[X] + V [X].
(16.39)
The potential is given in this case by
V = φ+¯r(∆)φ
= a0φ+φ +
M
X
σ=1
aσφ+(∆+ bσ)−1φ
= a0φ+φ +
M
X
σ=1
aσφ+Gσ = a0φ+
αφα +
M
X
σ=1
aσφ+
αGσα
= a0φ+φ +
M
X
σ=1
aσG+
σ φ = a0φ+
αφα +
M
X
σ=1
aσG+
σαφα.
(16.40)
This can be rewritten compactly as
V = Wαφα , Wα = a0(φ∗
α)A +
M
X
σ=1
aσ(G∗
σα)A.
(16.41)
The vectors (pseudo-fermions) Gσ are deﬁned by
Gσ = (∆+ bσ)−1φ.
(16.42)
We introduce a ﬁctitious time parameter t and a Hamiltonian H given by
H = 1
2TrP 2
µ + Q+Q + SYM
= 1
2TrP 2
µ + Q+
αQα + SYM.
(16.43)
The equation of motion associated with the matrix φ is given by
−( ˙Qα)A =
∂H
∂(φα)A
=
∂V
∂(φα)A
= a0(φ∗
α)A +
M
X
σ=1
aσ(G∗
σα)A
≡(Wα)A.
(16.44)
( ˙φα)A =
∂H
∂(Qα)A
≡(Q∗
α)A.
(16.45)

Monte Carlo Simulation of Fermion Determinants
187
This last equation is equivalent to
( ˙φ∗
α)A ≡(Qα)A.
(16.46)
The leap-frog algorithm for this part of the problem is given by
(Qα)A

n + 1
2

= (Qα)A(n) −δt
2 (Wα)A(n).
(16.47)
(φα)A(n + 1) = (φα)A(n) + δt(Q∗
α)A

n + 1
2

.
(16.48)
(Qα)A(n + 1) = (Qα)A

n + 1
2

−δt
2 (Wα)A(n + 1).
(16.49)
The ﬁrst set of equations of motion associated with the matrices Xµ are given by
−( ˙Pµ)ij =
∂H
∂(Xµ)ij
= ∂SBYM
∂(Xµ)ij
+
∂V
∂(Xµ)ij
= ∂SBYM
∂(Xµ)ij
−
M
X
σ=1
aσG+
σα
∂∆αβ
∂(Xµ)ij
Gσβ.
(16.50)
The eﬀect of the determinant is now encoded in the matrix (the force)
(Vµ)ij = −
M
X
σ=1
aσG+
σα
∂∆αβ
∂(Xµ)ij
Gσβ.
(16.51)
The second set of equations associated with the matrices Xµ are given by
( ˙Xµ)ij =
∂H
∂(Pµ)ij
= (Pµ)ji.
(16.52)
The leap-frog algorithm for this part of the problem is given by the equations
(16.27), (16.28) and (16.29) with the appropriate re-interpretation of the meaning
of (Vµ)ij.
16.3
More on The Conjugate-Gradient
16.3.1
Multiplication by M′ and (M′)+
Typically we will need to ﬁnd x′, given v, which solves the linear system
(∆+ b)x′ = v.
(16.53)
We will use the conjugate gradient method to do this. The product ∆x′ involves
the products M′x′ and (M′)+y′, viz
y′ = M′x′ ↔(y′
α)A′ = M′A′B′
αβ
(x′
β)B′.
(16.54)
z′ = (M′)+y′ ↔(z′
α)A′ = (M′∗
βα)B′A′(y′
β)B′.
(16.55)

188
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Multiplication by M′:
By using (16.19) we have
(y′
α)A′ = M′A′B′
αβ
(x′
β)B′
= MA′B′
αβ
(x′
β)B′ −MN2B′
αβ
δiA′jA′(x′
β)B′ −MA′N2
αβ
δiB′jB′(x′
β)B′
+ MN2N2
αβ
δiA′jA′δiB′jB′(x′
β)B′.
(16.56)
Recall that the primed indices run from 1 to N 2 −1 while unprimed indices run
from 1 to N 2. We introduce then
(yα)A = MAB
αβ (xβ)B
= MAB′
αβ (xβ)B′ + MAN2
αβ
(xβ)N 2.
(16.57)
We deﬁne
(xβ)B′ = (x′
β)B′ , (xβ)N 2 = −(x′
β)B′δiB′jB′.
(16.58)
Thus
(yα)A = MAB′
αβ (x′
β)B′ −MAN 2
αβ
(x′
β)B′δiB′jB′.
(16.59)
The next deﬁnition is obviously then
(y′
α)A′ = (yα)A′ −(yα)N2δiA′jA′.
(16.60)
This leads immediately to
(y′
α)A′ = MA′B′
αβ
(x′
β)B′ −MA′N2
αβ
(x′
β)B′δiB′jB′ −MN2B′
αβ
(x′
β)B′
+ MN2N2
αβ
(x′
β)B′δiB′jB′.
(16.61)
This is precisely (16.56).
Next we introduce the N × N matrices ˆxα, ˆyα associated with the vectors xα
and yα by the relations
ˆxα =
N2
X
A=1
(xα)AT A , ˆyα =
N2
X
A=1
(yα)AT A.
(16.62)
Thus
(xα) ¯
A = TrˆxαT A = (ˆxα)jAiA , (yα) ¯
A = TrˆyαT A = (ˆyα)jAiA.
(16.63)
And
(xα)A = Trˆxα(T A)+ = (ˆxα)iAjA , (yα)A = Trˆyα(T A)+ = (ˆyα)iAjA. (16.64)
We verify that
MAB
αβ (xβ)B = TrT A(Dˆx)α.
(16.65)
By comparing with
(yα)A = TrT A(ˆy)α,
(16.66)
we get
ˆyT = Dˆx.
(16.67)

Monte Carlo Simulation of Fermion Determinants
189
We recall the Dirac operator
D =
 X34 −XR
34 + ξ
X−−XR
−
X+ −XR
+
−X+
34 + (XR
34)+ + ξ

.
(16.68)
Thus ˆyT = Dˆx is equivalent to
(ˆy1)ij = (D1αˆxα)ji = [X34, ˆx1]ji + [X−, ˆx2]ji + ξ(ˆx1)ji.
(16.69)
(ˆy2)ij = (D2αˆxα)ji = −[X+
34, ˆx2]ji + [X+, ˆx1]ji + ξ(ˆx2)ji.
(16.70)
For completeness we remark
(yα)∗
AMAB
αβ (xβ)B = Trˆy∗
α(Dˆx)α.
(16.71)
Multiplication by (M′)+:
As before the calculation of
(z′
α)A′ = (M′∗
βα)B′A′(y′
β)B′
(16.72)
can be reduced to the calculation of
(zα)A = (M∗
βα)BA(yβ)B,
(16.73)
with the deﬁnitions
(yβ)B′ = (y′
β)B′ , (yβ)N2 = −(y′
β)B′δiB′jB′.
(16.74)
(z′
α)A′ = (zα)A′ −(zα)N2δiA′jA′.
(16.75)
The next step is to note that
M∗BA
βα (yβ)B = TrT A(D+ˆy)α.
(16.76)
The hermitian conjugate of the Dirac operator is deﬁned by the relation
D+ = −
 X∗
34 −(XR
34)∗+ ξ
X∗
+ −(XR
+)∗
X∗
−−(XR
−)∗
−XT
34 + (XR
34)T + ξ

.
(16.77)
Hence
ˆzT = D+ˆy.
(16.78)
Equivalently
(ˆz1)ij = (D+
1αˆyα)ji = −[X∗
34, ˆy1]ji −[X∗
+, ˆy2]ji + ξ(ˆy1)ji.
(16.79)
(ˆz2)ij = (D+
2αˆyα)ji = [XT
34, ˆy2]ji −[X∗
−, ˆy1]ji + ξ(ˆy2)ji.
(16.80)

190
An Introduction to Monte Carlo Simulations of Matrix Field Theory
16.3.2
The Fermionic Force
Also we will need to compute explicitly in the molecular dynamics part the fermionic
force (with (M
′+)αβ = (M
′
βα)+)
(Vµ)ij = −
M
X
σ=1
aσG+
σα
∂∆αβ
∂(Xµ)ij
Gσβ
= −
M
X
σ=1
aσG+
σα
∂(M′
βα)+
∂(Xµ)ij
Fσβ −
M
X
σ=1
aσF +
σβ
∂M′
βα
∂(Xµ)ij
Gσα
= −
M
X
σ=1
aσ

F +
σβ
∂M′
βα
∂(Xµ)∗
ij
Gσα
∗
−
M
X
σ=1
aσF +
σβ
∂M′
βα
∂(Xµ)ij
Gσα.
(16.81)
The vectors Fσα and F +
σα are deﬁned by
Fσα = M′
αβGσβ , F +
σα = G+
σβ(M′
αβ)+.
(16.82)
We can expand the bosonic matrices Xµ similarly to the fermionic matrices as
Xµ =
N 2
X
A=1
XA
µ T A.
(16.83)
Equivalently
(Xµ)iAjA = XA
µ , A = N(iA −1) + jA.
(16.84)
Reality of the bosonic matrices gives
(Xµ)∗
iAjA = X
¯
A
µ = (XA
µ )∗, ¯A = N(jA −1) + iA.
(16.85)
Hence we have
V A
µ ≡(Vµ)iAjA
= −
M
X
σ=1
aσ

F +
σβ
∂M′
βα
∂X ¯
A
µ
Gσα
∗
−
M
X
σ=1
aσF +
σβ
∂M′
βα
∂XA
µ
Gσα
= −
M
X
σ=1
aσ
 T
¯
A
σµ
∗−
M
X
σ=1
aσT A
σµ.
(16.86)
The deﬁnition of T A
σµ is obviously given by
T A
σµ = F +
σβ
∂M′
βα
∂XA
µ
Gσα.
(16.87)
For simplicity we may denote the derivations with respect to XA
µ and X ¯
A
µ by ∂and
¯∂respectively. As before we introduce the vectors in the full Hilbert space:
( ˜Gσα)B′ = (Gσα)B′ , ( ˜Gσα)N2 = −(Gσα)B′δiB′jB′ .
(16.88)
( ˜Fσα)B′ = (Fσα)B′ , ( ˜Fσα)N 2 = −(Fσα)B′δiB′jB′.
(16.89)

Monte Carlo Simulation of Fermion Determinants
191
A straightforward calculation gives
(F ∗
σβ)A′(M′
βα)A′B′(Gσα)Bprime = ( ˜F ∗
σβ)A(Mβα)AB( ˜Gσα)B.
(16.90)
(F ∗
σβ)A′∂(M′
βα)A′B′(Gσα)B′ = ( ˜F ∗
σβ)A∂(Mβα)AB( ˜Gσα)B.
(16.91)
Thus
T A
σµ = ˜F +
σβ
∂Mβα
∂XA
µ
˜Gσα.
(16.92)
Explicitly we have
T A
σµ = ( ˜F ∗
σβ)C
∂MCD
βα
∂XA
µ
( ˜Gσα)D.
(16.93)
We use the result
∂MCD
βα
∂XA
µ
= Tr
∂Mβα
∂XA
µ
[T D, T C],
(16.94)
where
M11 = X34 , M12 = X−, M21 = X+ , M22 = −X+
34.
(16.95)
We also introduce the matrices ˆF and ˆG given by
ˆFα =
N2
X
A=1
( ˜Fα)AT A , ˆGα =
N2
X
A=1
( ˜Gα)AT A.
(16.96)
The reverse of these equations is
( ˜Fα)A = Tr ˆFα(T A)+ , ( ˜Gα)A = Tr ˆGα(T A)+.
(16.97)
We use also the identity
X
A
(T A)ij(T A)+
kl = δilδjk.
(16.98)
A direct calculation yields then the fundamental results
T A
σµ = Tr
∂Mβα
∂XA
µ
[ ˆGσα, ˆF ∗
σβ] , T
¯
A
σµ = Tr
∂Mβα
∂X ¯
A
µ
[ ˆGσα, ˆF ∗
σβ].
(16.99)
Explicitly we have
T A
σ1 = [ ˆGσ1, ˆF ∗
σ2]jAiA + [ ˆGσ2, ˆF ∗
σ1]jAiA
T
¯
A
σ1 = [ ˆGσ1, ˆF ∗
σ2]iAjA + [ ˆGσ2, ˆF ∗
σ1]iAjA.
(16.100)
T A
σ2 = −i[ ˆGσ1, ˆF ∗
σ2]jAiA + i[ ˆGσ2, ˆF ∗
σ1]jAiA
T
¯
A
σ2 = −i[ ˆGσ1, ˆF ∗
σ2]iAjA + i[ ˆGσ2, ˆF ∗
σ1]iAjA.
(16.101)
T A
σ3 = [ ˆGσ1, ˆF ∗
σ1]jAiA −[ ˆGσ2, ˆF ∗
σ2]jAiA
T
¯
A
σ3 = [ ˆGσ1, ˆF ∗
σ1]iAjA −[ ˆGσ2, ˆF ∗
σ2]iAjA.
(16.102)
T A
σ4 = i[ ˆGσ1, ˆF ∗
σ1]jAiA + i[ ˆGσ2, ˆF ∗
σ2]jAiA
T
¯
A
σ4 = i[ ˆGσ1, ˆF ∗
σ1]iAjA + i[ ˆGσ2, ˆF ∗
σ2]iAjA.
(16.103)

192
An Introduction to Monte Carlo Simulations of Matrix Field Theory
16.4
The Rational Hybrid Monte Carlo Algorithm
16.4.1
Statement
In summary the rational hybrid Monte Carlo algorithm in the present setting con-
sists of the following steps:
(1) Initialization of X: Start X (the fundamental ﬁeld in the problem) from a
random conﬁguration.
(2) Initialization of Other Fields:
• Start P (the conjugate ﬁeld to X) from a Gaussian distribution according
to the probability exp(−TrP 2
µ/2). Both Xµ and Pµ are hermitian N × N
matrices.
• Start ξ from a Gaussian distribution according to the probability
exp(−ξ+ξ).
• Calculate φ (the pseudo-fermion) using the formula (16.36). This is done
using the conjugate gradient method (see below). The coeﬃcients c and d
are computed using the Remez algorithm from the rational approximation
of x1/4.
• Start Q (the conjugate ﬁeld to φ) from a Gaussian distribution according
to the probability exp(−Q+Q). The spinors Qα and φα, as well as ξα, are
(N 2 −1)-dimensional complex vectors.
(3) Molecular Dynamics: This consists of two parts:
• Pseudo-Fermion: We evolve the pseudo-fermion φ and its conjugate ﬁeld
Q using the Hamilton equations (16.47), (16.48) and (16.49). This is done
using the conjugate gradient method which, given the input φ, computes
as output the spinors Gσ given by equation (16.42) and the spinor W given
by equation (16.44). On the other hand, in the initialization step above
we call the conjugate gradient method with input ξ to obtain the output
φ = W ∗. Here and below, the coeﬃcients a and b are computed using the
Remez algorithm from the rational approximation of x−1/2.
• Gauge Field: We evolve Xµ and Pµ using the Hamilton equations (16.27),
(16.28) and (16.29). This requires the calculation of the boson contribution
to the force given by equation (16.24) and the fermion contribution given
by equation (16.51). The numerical evaluation of the fermion force is quite
involved and uses the formula (16.86). This requires, among other things,
the calculation of the spinors Gσ and Fσ = M′Gσ using the conjugate
gradient.
(4) Metropolis Step:
This is the central step.
After obtaining the solution
(X(T), P(T), φ(T), Q(T)) of the molecular dynamics evolution starting from the
initial conﬁguration (X(0), P(0), φ(0), Q(0)) we compute the resulting variation
∆H in the Hamiltonian. The new conﬁguration is accepted with probability
probability = min(1, exp(−∆H)).
(16.104)

Monte Carlo Simulation of Fermion Determinants
193
(5) Iteration: Repeat starting from (2).
(6) Other Essential Ingredients: The two other essential ingredients of this
algorithm are:
(a) Conjugate Gradient: This plays a fundamental role in this algorithm. The
multimass Krylov space solver employed here is based on the fundamental
equations (15.117)–(15.128). This allows us to compute the Gσ for all σ
given by equation (16.42) at once. The multiplication by ∆is done in two
steps: ﬁrst we multiply by M′ then we multiply by (M′)+. This is done
explicitly by reducing (16.54) to (16.69)+(16.70) and reducing (16.55) to
(16.79)+(16.80). Here, we obviously need to convert between a given trace-
less vector and its associated matrix and vice versa. The relevant equations
are (16.58), (16.60) and (16.64).
(b) Remez Algorithm: This is discussed at length in the previous chapter.
We only need to re-iterate here that the real coeﬃcients c, d, for the rational
approximation of x1/4, and a and b, for the rational approximation of x−1/2,
as well as the integer M are obtained using the Remez algorithm of [9].
The integer M is supposed to be determined separately for each function by
requiring some level of accuracy whereas the range over which the functions
are approximated by their rational approximations should be determined on
a trial and error basis by inspecting the spectrum of the Dirac operator.
16.4.2
Preliminary Tests
The rational approximations:
The ﬁrst thing we need to do is to ﬁx the param-
eters a, b, c and d of the rational approximations by invoking the Remez algorithm.
For a tolerance equal to 10−4 and over the interval [0.0004, 1] with precision 40 we
have found that the required degrees of the rational approximations, for x−1/2 and
x1/4, are M = 6 and M0 = 5 respectively; M is the minimum value for which the
uniform norm |r −f|∞= max|r −f| is smaller than the chosen tolerance. We can
plot these rational approximations versus the actual functions to see whether or not
these approximations are suﬃciently good over the ﬁxed range.
The conjugate gradient:
The conjugate gradient is a core part in this algo-
rithm and it must be checked thoroughly. A straightforward check is to verify that
(∆+ bσ)Gσ = φ for all values of σ. We must be careful that the matrix-vector mul-
tiplication ∆.Gσ does not vanish. Thus the no-sigma problem should be deﬁned,
not with zero mass bσ = 0, but with the smallest possible value of the mass bσ
which presumably corresponds to the least convergent linear system. In the results
included below we ﬁx the tolerance of the conjugate gradient at 10−5.
The decoupled theory:
This is the theory in which the gauge ﬁeld (Xµ)ij and
the pseudo-fermion ﬁeld φA
α are completely decoupled from each other. This is then
equivalent to the bosonic theory. This is expected to be obtained for suﬃciently

194
An Introduction to Monte Carlo Simulations of Matrix Field Theory
 0
 100
 200
 300
 400
 500
 600
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
SB,KB
molecular dynamics time
ξ =10
SB
KB
 20
 22
 24
 26
 28
 30
 32
 34
 36
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
KF,SF
molecular dynamics time
ξ =10
KF
SF
 0
 100
 200
 300
 400
 500
 600
 700
 800
 900
 1000
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
SB,KB
molecular dynamics time
ξ =1
SB
KB
 20
 30
 40
 50
 60
 70
 80
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
KF,SF
molecular dynamics time
ξ =1
KF
SF
 0
 500
 1000
 1500
 2000
 2500
 3000
 3500
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
SB,KB
molecular dynamics time
ξ =0
SB
KB
 0
 50
 100
 150
 200
 250
 300
 350
 400
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
KF,SF
molecular dynamics time
ξ =0
KF
SF
Figure 16.1: Testing the molecular dynamics part.
large values of the fermion mass ξ. In this theory the fermion ﬁeld behaves exactly
as a harmonic oscillator.
The decoupled theory can also be obtained, both in
the molecular dynamics part and the hybrid Monte Carlo part which includes in
addition the metropolis step, by setting
c0 =
1
√a0
, ai = ci = 0.
(16.105)
In this case the pseudo-fermions decouple from the gauge ﬁelds and behave as
harmonic oscillators with period T = 2π. The corresponding action should then be
periodic with period T = π.
The molecular dynamics:
We can run the molecular dynamics on its own to
verify the prediction of the decoupled theory. In general, it is also useful to monitor

Monte Carlo Simulation of Fermion Determinants
195
 0
 200
 400
 600
 800
 1000
 1200
 1400
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
SB
molecular dynamics time
Bosonic action
ξ =0
ξ =1
ξ =10
 500
 1000
 1500
 2000
 2500
 3000
 3500
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
H
molecular dynamics time
total Hamiltonian
ξ =0
ξ =1
ξ =10
 500
 1000
 1500
 2000
 2500
 3000
 3500
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
KB
molecular dynamics time
Bosonic kinetic term
ξ =0
ξ =1
ξ =10
 50
 100
 150
 200
 250
 300
 350
 400
 450
 500
 550
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
HF
molecular dynamics time
pseudo-fermion Hamiltonian
ξ =0
ξ =1
ξ =10
Figure 16.2: Testing the molecular dynamics part.
the classical dynamics for its own interest and monitor in particular the systematic
error due to the non-conservation of the Hamiltonian.
In the molecular dynamics we need to ﬁx the time step dt and the number of
iterations n. Thus we run the molecular dynamics for a time interval T = ndt.
We choose dt = 10−3 and n = 213 −214. Some results with N = 4 are included
in ﬁgures 16.1 and 16.2. We remark that the drift in the Hamiltonian becomes
pronounced as ξ −→0. This systematic error will be canceled by the Metropolis
step as we have discussed (see below).
We can use the molecular dynamics to obtain an estimation of the range of the
rational approximations needed as follows. Starting from ξ = 0, we increase the
value of ξ until the behavior of the theory becomes that of the decoupled (bosonic)
theory. The value of ξ at which this happens will be taken as an estimation of the
range. In the above example (ﬁgures 16.1 and 16.2) we observe that the pseudo-
fermion sector becomes essentially a harmonic oscillator around the value ξ = 10.
Thus a reasonable range should be taken between 0 and 10.
The metropolis step:
In general two among the three parameters of the molec-
ular dynamics (the time step dt, the number of iterations n and the time interval
T = ndt) should be optimized in such a way that the acceptance rate is ﬁxed, for
example, between 70 and 90 per cent. We ﬁx n and optimize dt along the line
discussed in previous chapters. We make, for every N, a reasonable guess for the

196
An Introduction to Monte Carlo Simulations of Matrix Field Theory
value of the number of iterations n, based on trial and error, and then work with
that value throughout. For example, for N between N = 4 and N = 8, we found
the value n = 10, to be suﬃciently reasonable.
Typically, we run Tther + Tmeas Monte Carlo steps where thermalization is sup-
posed to occur within the ﬁrst Tther steps which are discarded while measurements
are performed on a sample consisting of the subsequent Tmeas conﬁgurations. We
choose, for N = 4 −8, Tther = 211 and Tmeas = 213. We do not discuss in the
following auto-correlation issues while error bars are computed using the jackknife
method. As always, we generate our random numbers using the algorithm RAN2.
Some thermalized results for N = 4, 8 and α = m2 = ξ = 0 are shown in ﬁgure 16.3.
 70
 80
 90
 100
 110
 120
 130
 140
 150
 160
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
H
Monte Carlo time
thermalized observables for α =ξ  =m2=0 and N=4
’fort.9’ u 1:2
-4
-2
 0
 2
 4
 6
 8
 10
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
Δ H
Monte Carlo time
thermalized observables for α =ξ  =m2=0 and N=4
’fort.9’ u 1:8
 0
 5
 10
 15
 20
 25
 30
 35
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
exp(-Δ  H )
Monte Carlo time
thermalized observables for α =ξ  =m2=0 and N=4
’fort.9’ u 1:9
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
actions
Monte Carlo time
thermalized observables for α =ξ  =m2=0 and N=4
S
SB
SF
 65
 70
 75
 80
 85
 90
 95
 100
 105
 110
 115
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
SB
Monte Carlo time
thermalized observables for α =ξ  =m2=0 and N=8
SB
 80
 100
 120
 140
 160
 180
 200
 220
 240
 260
 280
 0
 1000
 2000
 3000
 4000
 5000
 6000
 7000
 8000
 9000
S,SF
Monte Carlo time
thermalized observables for α =ξ  =m2=0 and N=8
S
SF
Figure 16.3: Thermalized conﬁgurations.

Monte Carlo Simulation of Fermion Determinants
197
There are two powerful tests (exact analytic results) which can be used to cali-
brate the simulations. We must have the identities:
• We must have on general grounds the identity:
⟨exp(−∆H)⟩= 1.
(16.106)
• We must also have the Schwinger-Dyson identity:
⟨4γYM⟩+ ⟨3αCS⟩+ ⟨2m2HO⟩+ ⟨ξCOND⟩= (d + 2)(N 2 −1). (16.107)
We have included for completeness the eﬀects of a Chern–Simons term and a
harmonic oscillator term in the bosonic action. This identity is a generalization
of (11.35) where the deﬁnition of the condensation COND can be found in [11].
This identity follows from the invariance of the path integral (16.17) under the
translations Xµ −→Xµ + ϵXµ. For the ﬂat space supersymmetric model for
which ξ = 0 the above Schwinger-Dyson identity reduces to
⟨4γYM⟩+ ⟨3αCS⟩+ ⟨2m2HO⟩= (d + 2)(N 2 −1).
(16.108)
As an illustration some expectation values as functions of α for N = 4 and m2 =
ξ = 0 are shown in ﬁgure 16.4.
Emergent geometry:
We observe from the graph of TrX2
µ that something possi-
bly interesting happens around α ∼1.2. In fact, this is the very dramatic phenom-
ena of emergent geometry which is known to occur in these models when there is a
non-zero mass term (here the Chern–Simons term) included. This can be studied in
great detail using as order parameters the eigenvalues distributions of X4 and Xa. In
the matrix or Yang–Mills phase (small values of α) the matrices Xµ are nearly com-
muting with eigenvalues distributed uniformly inside a solid ball with a parabolic
eigenvalues distributions, or a generalization thereof, whereas in the fuzzy sphere
phase (large values of α) the matrix X4 decouples from Xa and remains distributed
as in the matrix phase, while the matrices Xa will be dominated by ﬂuctuations
around the SU(2) generators in the spin (N −1)/2 irreducible representation.
Code:
The attached code can be used to study the above emergent geometry
eﬀect, and many other issues, in great detail. On an intel dual core E4600 processor
(2.40GHz) running Ubuntu 14.04 LTS this codes goes as N 5.
16.5
Other Related Topics
Many other important topics, requiring techniques similar to the ones discussed in
this chapter, and which have been studied extensively by the Japan group, includes:
(1) IKKT models: The extension of the problem to higher dimensions; for ex-
ample d = 6; but in particular d = 10 which is the famous IKKT model which

198
An Introduction to Monte Carlo Simulations of Matrix Field Theory
 0.7
 0.75
 0.8
 0.85
 0.9
 0.95
 1
 1.05
 1.1
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
<identity>/(6N2-1),<exp(- Δ H)>
α
expectation values for m2=0 and N=4
identity
exp(- Δ H)
 0
 2
 4
 6
 8
 10
 12
 14
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
<YM>/(N2-1)
α
expectation values for m2=0 and N=4
YM
-12
-10
-8
-6
-4
-2
 0
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
<CS>/(N2-1)
α N0.5
expectation values for m2=0 and N=4
CS
 0
 10
 20
 30
 40
 50
 60
 70
 80
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
<HO>/N
α
expectation values for m2=0 and N=4
HO
 1.7
 1.72
 1.74
 1.76
 1.78
 1.8
 1.82
 1.84
 1.86
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
<SF>/(N2-1)
α
expectation values for m2=0 and N=4
SF
Figure 16.4: Various thermalized observables.
provides a non-perturbative deﬁnition of string theory, is the ﬁrst obvious gen-
eralization. However, the determinant in these cases is complex-valued which
makes its numerical evaluation very involved.
(2) Cosmological Yang-Mills matrix models: In recent years a generalization
from Euclidean Yang-Mills matrix models to Minkowski signature was carried
out with dramatic, interesting and novel consequences for cosmological models.
The problem with the complex-valued Pfaﬃans and determinants is completely
resolved in these cases.
(3) Quantum mechanical Yang-Mills matrix models: The extension of Yang-
Mills matrix models to quantum mechanical Yang-Mills matrix models, such as
the BFSS and BMN models which also provide non-perturbative deﬁnitions

Monte Carlo Simulation of Fermion Determinants
199
of string theory and M-theory, involves the introduction of time.
This new
continuous variable requires obviously a lattice regularization. There is so much
physics here relevant to the dynamics of black holes, gauge-gravity duality,
strongly coupled gauge theory and many other fundamental problems.
(4) The noncommutative torus: The noncommutative torus provides another,
seemingly diﬀerent, non-perturbative regularization of noncommutative ﬁeld
theory besides fuzzy spaces.
The phenomena of emergent geometry is also
observed here, as well as the phenomena of stripe phases, and furthermore, we
can add fermions and supersymmetry in an obvious way. The connection to
commutative theory and the commutative limit is more transparent in this case
which is an advantage.
(5) Supersymmetry: A non-perturbative deﬁnition of supersymmetry which al-
lows Monte Carlo treatment is readily available from the above discussed, and
much more, matrix models. These non-lattice simulations seem very promising
to strongly coupled gauge theories.
References
[1] J. Ambjorn, K. N. Anagnostopoulos, W. Bietenholz, T. Hotta and J. Nishimura,
‘Large N dynamics of dimensionally reduced 4-D SU(N) super Yang-Mills theory,’
JHEP 0007, 013 (2000) [hep-th/0003208].
[2] J. Ambjorn, K. N. Anagnostopoulos, W. Bietenholz, T. Hotta and J. Nishimura,
‘Monte Carlo studies of the IIB matrix model at large N,’ JHEP 0007, 011 (2000)
[arXiv:hep-th/0005147].
[3] K. N. Anagnostopoulos, T. Azuma, K. Nagao and J. Nishimura, ‘Impact of supersym-
metry on the nonperturbative dynamics of fuzzy spheres,’ JHEP 0509, 046 (2005)
[hep-th/0506062].
[4] K. N. Anagnostopoulos, T. Azuma and J. Nishimura, ‘Monte Carlo studies of the
spontaneous rotational symmetry breaking in dimensionally reduced super Yang-Mills
models,’ JHEP 1311, 009 (2013) [arXiv:1306.6135 [hep-th]].
[5] A. D. Kennedy, I. Horvath and S. Sint, ‘A New exact method for dynamical fermion
computations with nonlocal actions,’ Nucl. Phys. Proc. Suppl. 73, 834 (1999) [hep-
lat/9809092].
[6] M. A. Clark and A. D. Kennedy, ‘The RHMC algorithm for two ﬂavors of dynamical
staggered fermions,’ Nucl. Phys. Proc. Suppl. 129, 850 (2004) [hep-lat/0309084].
[7] M. A. Clark, P. de Forcrand and A. D. Kennedy, ‘Algorithm shootout: R versus
RHMC,’ PoS LAT 2005, 115 (2006) [hep-lat/0510004].
[8] M. A. Clark, ‘The rational hybrid Monte Carlo algorithm,’ PoS LAT 2006, 004 (2006)
[hep-lat/0610048].
[9] M. A. Clark and A. D. Kennedy, https://github.com/mikeaclark/AlgRemez, 2005.
[10] P. Austing, ‘Yang-Mills matrix theory,’ arXiv:hep-th/0108128.
[11] B. Ydri, ‘Impact of supersymmetry on emergent geometry in Yang-Mills matrix mod-
els II,’ Int. J. Mod. Phys. A 27, 1250088 (2012) [arXiv:1206.6375 [hep-th]].

Chapter 17
U(1) Gauge Theory on the Lattice:
Another Lattice Example
In this chapter we will follow the excellent pedagogical textbook [1] especially on
practical detail regarding the implementation of the Metropolis and other algo-
rithms to lattice gauge theories. The classic textbooks [2–5] were also very useful.
17.1
Continuum Considerations
A ﬁeld theory is a dynamical system with N degrees of freedom where N −→
∞. The classical description is given in terms of the Lagrangian and the action
while the quantum description is given in terms of the Feynman path integral and
the correlation functions.
In a scalar ﬁeld theory the basic ﬁeld has spin j =
0 with respect to Lorentz transformations.
Scalar ﬁeld theories are relevant to
critical phenomena. In gauge theories the basic ﬁelds have spin j = 1 (gauge vector
ﬁelds) and spin j = 1/2 (fermions) and they are relevant to particle physics. The
requirement of renormalizability restricts severely the set of quantum ﬁeld theories
to only few possible models. Quantum electrodynamics or QED is a renormalizable
ﬁeld theory given by the action
SQED =
Z
d4x

−1
4FµνF µν + ¯ψ(iγµ∂µ −M)ψ −e ¯ψγµψAµ

.
(17.1)
The γµ are the famous 4 × 4 Dirac gamma matrices which appear in any the-
ory containing a spin 1/2 ﬁeld.
They satisfy {γµ, γν} = 2ηµν where ηµν =
diag(1, −1, −1, −1). The electromagnetic ﬁeld is given by the U(1) gauge vector
ﬁeld Aµ with ﬁeld strength Fµν = ∂µAν −∂νAµ while the fermion (electron) ﬁeld
is given by the spinor ﬁeld ψ with mass M. The spinor ψ is a 4-component ﬁeld
and ¯ψ = ψ+γ0. The interaction term is proportional to the electric charge e given
by the last term −e ¯ψγµψAµ. The Euler–Lagrange classical equations of motion
derived from the above action are precisely the Maxwell equations ∂µF µν
= jν
with jµ = eψγµψ and the Dirac equation (iγµ∂µ −m −eγµAµ)ψ = 0. The above
theory is also invariant under the following U(1) gauge transformations
Aµ −→Aµ + ∂µΛ ,
ψ −→exp(−ieΛ)ψ ,
¯ψ −→¯ψ exp(ieΛ).
(17.2)
201

202
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The Feynman path integral is
Z =
Z
DAµD ¯ψDψ exp(iSQED).
(17.3)
Before we can study this theory numerically using the Monte Carlo method we need
to:
(1) Rotate to Euclidean signature in order to convert the theory into a statistical
ﬁeld theory.
(2) Regularize the UV behavior of the theory by putting it on a lattice.
As a consequence we obtain an ordinary statistical system accessible to ordinary
sampling techniques such as the Metropolis algorithm.
We start by discussing a little further the above action. The free fermion action
in Minkowski spacetime is given by
SF =
Z
d4x ¯ψ(x)(iγµ∂µ −M)ψ(x).
(17.4)
This action is invariant under the global U(1) transformation ψ(x) −→Gψ(x)
and ¯ψ(x) −→¯ψ(x)G−1 where G = exp(−iΛ). The symmetry U(1) can be made
local (i.e. G becomes a function of x) by replacing the ordinary derivative ∂µ with
the covariant derivative Dµ = ∂µ + ieAµ where the U(1) gauge ﬁeld Aµ is the
electromagnetic 4-vector potential. The action becomes
SF =
Z
d4x ¯ψ(x)(iγµDµ −M)ψ(x).
(17.5)
This action is invariant under
ψ −→G(x)ψ , ¯ψ −→¯ψG−1(x),
(17.6)
provided we also transform the covariant derivative and the gauge ﬁeld as follows
Dµ −→GDµG−1 ⇐⇒Aµ −→G(x)AµG−1(x) −i
eG(x)∂µG−1(x).
(17.7)
Since Aµ and G(x) = exp(−iΛ(x)) commute the transformation law of the gauge
ﬁeld reduces to Aµ −→Aµ + ∂µΛ/e. The dynamics of the gauge ﬁeld Aµ is given
by the Maxwell action
SG = −1
4
Z
d4xFµνF µν , Fµν = ∂µAν −∂νAµ.
(17.8)
This action is also invariant under the local U(1) gauge symmetry Aµ −→Aµ +
∂µΛ/e. The total action is then
SQED = −1
4
Z
d4xFµνF µν +
Z
d4x ¯ψ(x)(iγµDµ −M)ψ(x).
(17.9)
This is precisely (17.1).
The Euclidean action Seucl
F
is obtained by (i) making the replacement x0 −→
−ix4 wherever x0 appears explicitly, (ii) substituting ψE(x) = ψ(⃗x, x4) for ψ(x) =

U(1) Gauge Theory on the Lattice: Another Lattice Example
203
ψ(⃗x, t), (iii) making the replacements A0 −→iA4 and D0 −→iD4 and (iv) multi-
plying the obtained expression by −i. Since in Euclidean space the Lorentz group
is replaced by the 4-dimensional rotation group we introduce new γ-matrices γE
µ as
follows γE
4 = γ0,γE
i = −iγi. They satisfy {γE
µ , γE
ν } = 2δµν. The fermion Euclidean
action is then
SEucl
F
=
Z
d4x ¯ψE(x)(γE
µ Dµ + M)ψE(x).
(17.10)
Similarly the Euclidean action Seucl
G
is obtained by (i) making the replacement
x0 −→−ix4 wherever x0 appears explicitly, (ii) making the replacement A0 −→iA4
and (iii) multiplying the obtained expression by −i. We can check that FµνF µν,
µ, ν = 0, 1, 2, 3 will be replaced with F 2
µν, µ = 1, 2, 3, 4. The gauge Euclidean action
is then
SEucl
G
= 1
4
Z
d4xF 2
µν.
(17.11)
The full Euclidean action is
SEucl
QED = 1
4
Z
d4xF 2
µν +
Z
d4x ¯ψE(x)(γE
µ Dµ + M)ψE(x).
(17.12)
We will drop the labels Eucl in the following.
17.2
Lattice Regularization
17.2.1
Lattice Fermions and Gauge Fields
Free Fermions on the Lattice:
The continuum free fermion action in Euclidean
4d spacetime is
SF =
Z
d4x ¯ψE(x)(γE
µ ∂µ + M)ψE(x).
(17.13)
This has the symmetry ψ−→eiθψ and the symmetry ψ−→eiθγ5ψ when M = 0.
The associated conserved currents are known to be given by Jµ = ¯ψγµψ and J5
µ =
¯ψγµγ5ψ where γ5 = γ1γ2γ3γ4. It is also a known result that in the quantum theory
one can not maintain the conservation of both of these currents simultaneously in
the presence of gauge ﬁelds.
A regularization which maintains exact chiral invariance of the above action
can be achieved by replacing the Euclidean four dimensional spacetime by a four
dimensional hypercubic lattice of N 4 sites. Every point on the lattice is speciﬁed
by 4 integers which we denote collectively by n = (n1, n2, n3, n4) where n4 denotes
Euclidean time. Clearly each component of the 4-vector n is an integer in the range
−N/2≤nµ≤N/2 with N even. The lattice is assumed to be periodic. Thus xµ = anµ
where a is the lattice spacing and L = aN is the linear size of the lattice. Now
to each site x = an we associate a spinor variable ψ(n) = ψ(x) and the derivative
∂µψ(x) is replaced by
∂µψ(x)−→1
a
ˆ∂µψ(n) = 1
2a

ψ(n + ˆµ) −ψ(n −ˆµ)

.
(17.14)

204
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The vector ˆµ is the unit vector in the µ-direction. With this prescription the action
(17.13) becomes (with ˆ
M = aM and ˆψ = a3/2ψ)
SF =
X
n
X
m
X
α
X
β
¯ˆψα(n)Kαβ(n, m) ˆψβ(m)
Kαβ(n, m) = 1
2
X
µ
(γµ)αβ
 δm,n+ˆµ −δm,n−ˆµ

+ ˆ
Mδαβδm,n.
(17.15)
U(1) Lattice Gauge Fields:
The free fermion action on the lattice is therefore
given by
SF = ˆ
M
X
n
X
α
¯ˆψα(n) ˆψα(n)
−1
2
X
n
X
α
X
β
X
µ

(γµ)αβ ¯ˆψα(n + ˆµ) ˆψβ(n) −(γµ)αβ ¯ˆψα(n) ˆψβ(n + ˆµ)

.
(17.16)
This action has the following global U(1) symmetry
ˆψα(n) −→G ˆψα(n) , ¯ˆψα(n) −→¯ˆψα(n)G−1.
(17.17)
The phase G = exp(−iΛ) is an element of U(1). By requiring the theory to be
invariant under local U(1) symmetry, i.e. allowing G to depend on the lattice site
we arrive at a gauge invariant fermion action on the lattice. The problem lies in
how we can make the bilinear fermionic terms (the second and third terms) in the
above action gauge invariant.
We go back to the continuum formulation and see how this problem is solved.
In the continuum the fermionic bilinear ¯ψ(x)ψ(y) transforms under a local U(1)
transformation as follows
¯ψ(x)ψ(y) −→¯ψ(x)G−1(x)G(y)ψ(y).
(17.18)
This bilinear can be made gauge covariant by inserting the Schwinger line integral
U(x, y) = eie
R y
x dzµAµ(z),
(17.19)
which transforms as
U(x, y) −→G(x)U(x, y)G−1(y).
(17.20)
Therefore the fermionic bilinear
¯ψ(x)U(x, y)ψ(y) = ¯ψ(x)eie
R y
x dzµAµ(z)ψ(y)
(17.21)
is U(1) gauge invariant. For y = x + ϵ we have
U(x, x + ϵ) = eieϵµAµ(x).
(17.22)
We conclude that in order to get local U(1) gauge invariance we replace the second
and third bilinear fermionic terms in the above action as follows
¯ˆψ(n)(r −γµ) ˆψ(n + ˆµ) −→¯ˆψ(n)(r −γµ)Un,n+ˆµ ˆψ(n + ˆµ)
¯ˆψ(n + ˆµ)(r −γµ) ˆψ(n) −→¯ˆψ(n + ˆµ)(r −γµ)Un+ˆµ,n ˆψ(n).
(17.23)

U(1) Gauge Theory on the Lattice: Another Lattice Example
205
We obtain then the action
SF = ˆ
M
X
n
X
α
¯ˆψα(n) ˆψα(n)
−1
2
X
n
X
α
X
β
X
µ

(γµ)αβ ¯ˆψα(n + ˆµ)Un+ˆµ,n ˆψβ(n)
−(γµ)αβ ¯ˆψα(n)Un,n+ˆµ ˆψβ(n + ˆµ)

.
(17.24)
The U(1) element Un,n+ˆµ lives on the lattice link connecting the two points n and
n + ˆµ. This link variable is therefore a directed quantity given explicitly by
Un,n+ˆµ = eiφµ(n) ≡Uµ(n) , Un+ˆµ,n = U +
n,n+ˆµ = e−iφµ(n) ≡U +
µ (n). (17.25)
The second equality is much clearer in the continuum formulation but on the lattice
it is needed for the reality of the action. The phase φµ(n) belongs to the compact
interval [0, 2π]. Alternatively we can work with Aµ(n) deﬁned through
φµ(n) = eaAµ(n).
(17.26)
Let us now consider the product of link variables around the smallest possible closed
loop on the lattice, i.e. a plaquette. For a plaquette in the µ −ν plane we have
UP ≡Uµν(n) = Uµ(n)Uν(n + ˆµ)U +
µ (n + ˆν)U +
ν (n).
(17.27)
The links are path-ordered. We can immediately compute
UP ≡Uµν(n) = eiea2Fµν(n)
Fµν = 1
a

Aν(n + ˆµ) −Aν(n) −Aµ(n + ˆν) + Aµ(n)

.
(17.28)
In other words in the continuum limit a −→0 we have
1
e2
X
n
X
µ<ν

1 −1
2
 Uµν(n) + U +
µν(n)

= a4
4
X
n
X
µ,ν
F 2
µν.
(17.29)
The U(1) gauge action on the lattice is therefore
SG = 1
e2
X
P

1 −1
2
 Up + U +
p

.
(17.30)
17.2.2
Quenched Approximation
The QED partition function on a lattice Λ is given by
Z =
Z
DU D ¯ˆψD ˆψ e−SG[U]−SF [U, ¯ˆ
ψ, ˆ
ψ].
(17.31)
The measures are deﬁned by
DU =
Y
n∈Λ
4
Y
µ=1
dUµ(n) , D ¯ˆψ =
Y
n∈Λ
d ¯ˆψ(n) , D ˆψ =
Y
n∈Λ
d ˆψ(n).
(17.32)

206
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The plaquette and the link variable are given by
Uµν(n) = Uµ(n)Uν(n + ˆµ)U +
µ (n + ˆν)U +
ν (n) , Uµ(n) = eiφµ(n).
(17.33)
The action of a U(1) gauge theory on a lattice is given by (with β = 1/e2)
SG[U] = β
X
n∈Λ
X
µ<ν

1 −1
2
 Uµν(n) + U +
µν(n)

= β
X
n∈Λ
X
µ<ν
Re

1 −Uµν(n)

.
(17.34)
The action of fermions coupled to a U(1) gauge ﬁeld on a lattice is given by
SF [U, ¯ˆψ, ˆψ] =
X
α
X
β
X
n
X
m
¯ˆ
ψα(n)Dαβ(U)n,m ˆ
ψβ(m).
(17.35)
Where
Dαβ(U)n,m = ˆ
Mδαβδn,m −1
2(γµ)αβ δn,m+ˆµ Un+ˆµ,n
+ 1
2(γµ)αβ δm,n+ˆµ Un,n+ˆµ.
(17.36)
Using the result
Z
D ¯ˆψD ˆψ e−P
α
P
β
P
n
P
m
¯ˆ
ψα(n)Dαβ(U)n,m ˆ
ψβ(m) = detDαβ(U)n,m.
(17.37)
The partition function becomes
Z =
Z
DU detDαβ(U)n,m e−SG[U].
(17.38)
At this stage we will make the approximation that we can set the determinal equal
1, i.e. the QED partition function will be approximated by
Z =
Z
DU e−SG[U].
(17.39)
This is called the quenched approximation.
17.2.3
Wilson Loop, Creutz Ratio and Other Observables
The ﬁrst observable we would like to measure is the expectation value of the action
which after dropping the constant term is given by
⟨SG[U]⟩= −β
X
n∈Λ
X
µ<ν
⟨Re Uµν(n)⟩.
(17.40)
The speciﬁc heat is the corresponding second moment, viz
Cv = ⟨SG[U]2⟩−⟨SG[U]⟩2.
(17.41)
We will also measure the expectation value of the so-called Wilson loop which has
a length I in one of the spatial direction (say 1) and a width J in the temporal
direction 4. This rectangular loop C is deﬁned by
WC[U] = S(n, n + Iˆ1)T(n + Iˆ1, n + Iˆ1 + Jˆ4)
× S+(n + Jˆ4, n + Iˆ1 + Jˆ4)T +(n, n + Jˆ4).
(17.42)

U(1) Gauge Theory on the Lattice: Another Lattice Example
207
The Wilson lines are
S(n, n + Iˆ1) =
I−1
Y
i=0
U1(n + iˆ1)
S(n + Jˆ4, n + Iˆ1 + Jˆ4) =
I−1
Y
i=0
U1(n + iˆ1 + Jˆ4).
(17.43)
The temporal transporters are
T(n + Iˆ1, n + Iˆ1 + Jˆ4) =
J−1
Y
j=0
U4(n + Iˆ1 + jˆ4)
T(n, n + Jˆ4) =
J−1
Y
j=0
U4(n + jˆ4).
(17.44)
The expectation value of WC[U] will be denoted by
W[I, J] =
R
DU WC[U] e−SG[U]
R
DU e−SG[U]
.
(17.45)
By using the fact that under φµ(n) −→−φµ(n), the partition function is invariant
while the Wilson loop changes its orientation, i.e. WC[U] −→WC[U]+, we obtain
W[I, J] = ⟨Re WC[U]⟩.
(17.46)
It is almost obvious that in the continuum limit
W[I, J] −→W[R, T] =
*
exp(ie
I
C
dxµAµ)
+
.
(17.47)
The loop C is now a rectangular contour with spatial length R = Ia and timelike
length T = Ja. This represents the probability amplitude for the process of creating
an inﬁnitely heavy, i.e.
static, quark-antiquark1 pair at time t = 0 which are
separated by a distance R, then allowing them to evolve in time and then eventually
annihilate after a long time T.
The precise meaning of the expectation value (17.46) is as follows
< O >= 1
L
L
X
i=1

1
N 3NT
X
n
Re WC[Ui]

.
(17.48)
In other words we also take the average over the lattice which is necessary in order
to reduce noise in the measurement of the Creutz ratio (see below).
The above Wilson loop is the order parameter of the pure U(1) gauge theory.
For large time T we expect the behavior
W[R, T −→∞] −→e−V (R)T = e−aV (R)J,
(17.49)
where V (R) is the static quark–antiquark potential. For strong coupling (small β)
we can show that the potential is linear, viz
V (R) = σR.
(17.50)
1For U(1) we should really speak of an electron-positron pair.

208
An Introduction to Monte Carlo Simulations of Matrix Field Theory
The constant σ is called the string tension from the fact that the force between the
quark and the antiquark can be modeled by the force in a string attached to the
quark and antiquark. For a linear potential the Wilson loop follows an area law
W[R, T] = exp(−σA) with A = a2IJ. This behavior is typical in a conﬁning phase
which occurs at high temperature.
For small coupling (large β, low temperature) the lattice U(1) gauge ﬁeld be-
comes weakly coupled and as a consequence we expect the Coulomb potential to
dominate the static quark-antiquark potential, viz
V (R) = Z
R.
(17.51)
Hence for large R the quark and antiquark become eﬀectively free and their energy
is simply the sum of their self-energies.
The Wilson loop in this case follows a
perimeter law W[R, T] = exp(−2ϵT).
In summary for a rectangular R × T Wilson loop with perimeter P = 2(R + T)
and area A = RT we expect the behavior
W[R, T] = e−σA , conﬁnement phase.
(17.52)
W[R, T] = e−ϵP , coulomb phase.
(17.53)
In general the Wilson loop will behave as
W[R, T] = e−B−σA−ϵP .
(17.54)
The perimeter piece actually dominates for any ﬁxed size loop. To measure the
string tension we must therefore eliminate the perimeter behavior which can be
achieved using the so-called Creutz ratio deﬁned by
χ(I, J) = −ln W[I, J]W[I −1, J −1]
W[I, J −1]W[I −1, J].
(17.55)
For large loops clearly
χ(I, J) = a2σ.
(17.56)
This should holds especially in the conﬁnement phase whereas in the Coulomb phase
we should expect χ(I, J) ∼0.
The 1 × 1 Wilson loop W(1, 1) is special since it is related to the average action
per plaquette. We have
W[1, 1] = ⟨Re U1(n)U4(n + ˆ1)U +
4 (n)U +
1 (n + ˆ4)⟩.
(17.57)
Next we compute straightforwardly
−∂ln Z
∂β
=
X
n
X
µ<ν
⟨[1 −Re Uµν(n)]⟩.
(17.58)
Clearly all the planes µν are equivalent and thus we should have
−∂ln Z
∂β
= 6
X
n
⟨[1 −Re U14(n)]⟩
= 6
X
n
⟨[1 −Re U1(n)U4(n + ˆ1)U +
4 (n)U +
1 (n + ˆ4)]⟩.
(17.59)

U(1) Gauge Theory on the Lattice: Another Lattice Example
209
Remark that there are N 3NT lattice sites. Each site corresponds to 4 plaquettes in
every plane µν and thus it corresponds to 4 × 6 plaquettes in all. Each plaquette
in a plane µν corresponds to 4 sites and thus to avoid overcounting we must divide
by 4. In summary we have 4 × 6 × N 3 × NT /4 plaquettes in total. Six is therefore
the ratio of the number of plaquettes to the number of sites.
We have then
−
1
6N 3NT
∂ln Z
∂β
= 1 −
1
N 3NT
X
n
⟨Re U1(n)U4(n + ˆ1)U +
4 (n)U +
1 (n + ˆ4)⟩.
(17.60)
We can now observe that all lattice sites n are the same under the expectation
value, namely
−
1
6N 3NT
∂ln Z
∂β
= 1 −⟨Re U1(n)U4(n + ˆ1)U +
4 (n)U +
1 (n + ˆ4)⟩.
(17.61)
This is the average action per plaquette (the internal energy) denoted by
P = −
1
6N 3NT
∂ln Z
∂β
= 1 −W[1, 1].
(17.62)
17.3
Monte Carlo Simulation of Pure U(1) Gauge Theory
17.3.1
The Metropolis Algorithm
The action of pure U(1) gauge theory, the corresponding partition function and the
measure of interest are given on a lattice Λ respectively by (with β = 1/e2)
SG[U] = β
X
n∈Λ
X
µ<ν
Re

1 −Uµν(n)

.
(17.63)
Z =
Z
DU e−SG[U].
(17.64)
DU =
Y
n∈Λ
4
Y
µ=1
dUµ(n).
(17.65)
The vacuum expectation value of any observable O = O(U) is given by
⟨O⟩= 1
Z
Z
DU O e−SG[U].
(17.66)
For U(1) gauge theory we can write
Uµ(n) = eiφµ(n).
(17.67)
Hence
DU =
Y
n∈Λ
4
Y
µ=1
dφµ(n).
(17.68)

210
An Introduction to Monte Carlo Simulations of Matrix Field Theory
We will use the Metropolis algorithm to solve this problem. This goes as follows.
Starting from a given gauge ﬁeld conﬁguration, we choose a lattice point n and a
direction µ, and change the link variable there, which is Uµ(n), to Uµ(n)′. This link
is shared by 6 plaquettes. The corresponding variation of the action is
∆SG[Uµ(n))] = SG[U ′] −SG[U].
(17.69)
The gauge ﬁeld conﬁgurations U and U ′ diﬀer only by the value of the link variable
Uµ(n). We need to isolate the contribution of Uµ(n) to the action SG. Note the
fact that U +
µν = Uνµ. We write
SG[U] = β
X
n∈Λ
X
µ<ν
1 −β
2
X
n∈Λ
X
µ<ν
 Uµν(n) + U +
µν(n)

.
(17.70)
The second term is
−β
2
X
n∈Λ
X
µ<ν
Uµν(n) = −β
2
X
n∈Λ
X
µ<ν
Uµ(n)Uν(n + ˆµ)U +
µ (n + ˆν)U +
ν (n).
(17.71)
In the µ −ν plane, the link variable Uµ(n) appears twice corresponding to the two
lattice points n and n −ˆν. For every µ there are three relevant planes. The six
relevant terms are therefore given by
−β
2
X
n∈Λ
X
µ<ν
Uµν(n) −→−β
2
X
ν̸=µ
 Uµ(n)Uν(n + ˆµ)U +
µ (n + ˆν)U +
ν (n)
+ U +
µ (n)U +
ν (n −ˆν)Uµ(n −ˆν)Uν(n −ˆν + ˆµ)

+ ...
(17.72)
By adding the complex conjugate terms we obtain
−β
2
X
n∈Λ
X
µ<ν
(Uµν(n) + U +
µν(n)) −→−β
2
 Uµ(n)Aµ(n) + U +
µ (n)A+
µ (n)

+ ...
(17.73)
The Aµ(n) is the sum over the six so-called staples which are the products over the
other three link variables which together with Uµ(n) make up the six plaquettes
which share Uµ(n). Explicitly we have
Aµ(n) =
X
ν̸=µ
 Uν(n + ˆµ)U +
µ (n + ˆν)U +
ν (n)
+ U +
ν (n + ˆµ −ˆν)U +
µ (n −ˆν)Uν(n −ˆν)

.
(17.74)
We have then the result
−β
2
X
n∈Λ
X
µ<ν
(Uµν(n) + U +
µν(n)) −→−βRe(Uµ(n)Aµ(n)) + ...
(17.75)
We compute then
∆SG[Uµ(n))] = SG[U ′] −SG[U]
= −β(Uµ(n)′ −Uµ(n))Aµ(n).
(17.76)

U(1) Gauge Theory on the Lattice: Another Lattice Example
211
Having computed the variation ∆SG[Uµ(n))], next we inspect its sign. If this vari-
ation is negative then the proposed change Uµ(n) −→Uµ(n)′ will be accepted
(classical mechanics). If the variation is positive, we compute the Boltzmann prob-
ability
exp(−∆SG[Uµ(n))]) = exp(β(Uµ(n)′ −Uµ(n))Aµ(n)).
(17.77)
The proposed change Uµ(n) −→Uµ(n)′ will be accepted according to this probabil-
ity (quantum mechanics). In practice we will pick a uniform random number r be-
tween 0 and 1 and compare it with exp(−∆SG[Uµ(n))]). If exp(−∆SG[Uµ(n))]) < r
we accept this change otherwise we reject it.
We go through the above steps for every link in the lattice which constitutes one
Monte Carlo step. Typically equilibration (thermalization) is reached after a large
number of Monte Carlo steps at which point we can start taking measurements
based on the formula (17.66) written as
⟨O⟩= 1
L
L
X
i=1
Oi , Oi = O(Ui).
(17.78)
The L conﬁgurations Ui = {Uµ(n)}i are L thermalized gauge ﬁeld conﬁgurations
distributed according to exp(−SG[U]).
The error bars in the diﬀerent measurements will be estimated using the jack-
knife method. We can also compute auto-correlation time and take it into account
by separating the measured gauge ﬁeld conﬁgurations Ui by at least one unit of
auto-correlation time.
Let us also comment on how we choose the proposed conﬁgurations Uµ(n)′. The
custom is to take Uµ(n)′ = XUµ(n) where X is an element in the gauge group
(which is here U(1)) near the identity. In order to maintain a symmetric selection
probability, X should be drawn randomly from a set of U(1) elements which contains
also X−1. For U(1) gauge group we have X = exp(iφ) where φ ∈[0, 2π]. In principle
the acceptance rate can be maintained around at least 0.5 by tuning appropriately
the angle φ.
Reunitarization of Uµ(n)′ may also be applied to reduce rounding
errors.
The ﬁnal technical remark is with regard to boundary conditions. In order to
reduce edge eﬀects we usually adopt periodic boundary conditions, i.e.
Uµ(N, n2, n3, n4) = Uµ(0, n2, n3, n4)
Uµ(n1, N, n3, n4) = Uµ(n1, 0, n3, n4)
Uµ(n1, n2, N, n4) = Uµ(n1, n2, n, 0, n4)
Uµ(n1, n2, n3, NT ) = Uµ(n1, n2, n3, 0).
(17.79)
This means in particular that the lattice is actually a four dimensional torus. In
the actual code this is implemented by replacing i ± 1 by ip(i) and im(i), ipT(i)
and imT(i) respectively which are deﬁned by

212
An Introduction to Monte Carlo Simulations of Matrix Field Theory
do i=1,N
ip(i)=i+1
im(i)=i-1
enddo
ip(N)=1
im(1)=N
do i=1,NT
ipT(i)=i+1
imT(i)=i-1
enddo
ipT(NT)=1
imT(1)=NT
A code written along the above lines is attached in the last chapter.
17.3.2
Some Numerical Results
(1) We run simulations for N = 3, 4, 8, 10, 12 with the coupling constant in the range
β = 2, ..., 12. We use typically 214 thermalization steps and 214 measurements
steps.
(2) We measure the speciﬁc heat (ﬁgure 17.1). We observe a peak in the speciﬁc
heat at around β = 1. The peak grows with N which signals a critical behavior
typical of 2nd order transition.
 0
 2
 4
 6
 8
 10
 12
 14
 16
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
Cv/N4
beta
N=3
N=4
N=10
N=12
Figure 17.1: The speciﬁc heat.

U(1) Gauge Theory on the Lattice: Another Lattice Example
213
(3) The simplest order parameter is the action per plaquette P, deﬁned in equation
(17.62), which is shown in ﬁgure 17.2. We observe good agreement between
the high-temperature and low-temperature expansions of P from one hand and
the corresponding observed behavior in the strong coupling and weak coupling
regions respectively from the other hand. We note that the high-temperature
and low-temperature expansions of the pure U(1) gauge ﬁeld are given by
P = 1 −β
2 + O(β3) , high T.
(17.80)
P = 1 −1
4β + O(1/β2) , low T.
(17.81)
We do not observe a clear-cut discontinuity in P which is, in any case, consistent
with the conclusion that this phase is second order. We note that for higher
U(N) the transition is ﬁrst order [2].
A related object to P is the total action shown in ﬁgure 17.3.
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
P
beta
N=8
N=10
strong coupling
weak coupling
Figure 17.2: The action per plaquette.
(4) A more powerful order parameters are the Wilson loops which are shown in
ﬁgure 17.4.
We observe that the Wilson loop in the strong coupling region
averages to zero very quickly as we increase the size of the loop. This may be
explained by an area law behavior. In the weak coupling region, the evolution

214
An Introduction to Monte Carlo Simulations of Matrix Field Theory
-10
-9
-8
-7
-6
-5
-4
-3
-2
-1
 0
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
action/N4
beta
N=4
N=12
Figure 17.3: The total action.
-0.1
 0
 0.1
 0.2
 0.3
 0.4
 0.5
 0.6
 0.7
 0.8
 0.9
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
W[I,J]
beta
N=8,1x1 loop
2x2 loop
3x3 loop
N=10,1x1 loop
2x2 loop
3x3 loop
Figure 17.4: The Wilson loop.

U(1) Gauge Theory on the Lattice: Another Lattice Example
215
as a function of the area is much more slower. The demarcation between the
two phases becomes very sharp (possibly a jump) for large loops at β = 1.
(5) Calculating the expectation value of the Wilson loop and then extracting the
string tension is very diﬃcult since the perimeter law is dominant more often.
The Creutz ratios (ﬁgure 17.5) allow us to derive the string tension in a direct
way without measuring the Wilson loop. The string tension is the coeﬃcient
of the linearly rising part of the potential for large (inﬁnite) separations of a
quark-antiquark pair in the absence of pair production processes. In this way,
we hope to measure the physical string tension in a narrow range of the coupling
constant.
We observe that the string tension in the weak coupling regime is eﬀectively
independent of the coupling constant and it is essentially zero. In the strong
coupling regime we reproduce the strong coupling behavior
σ = −ln β
2 .
(17.82)
 0
 0.2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
 2
 0.4
 0.6
 0.8
 1
 1.2
 1.4
 1.6
 1.8
creutz ratio
beta
2X2,N=10
N=12
3X3,N=10
N=12
2X3,N=10
N=10
3X2,N=10
N=12
strong coupling expansion
Figure 17.5: The Creutz ratios.
17.3.3
Coulomb and Conﬁnement Phases
The physics of the compact U(1) theory is clearly diﬀerent in the weak- and strong-
coupling regions. This can be understood from the fact that there is a phase tran-
sition as a function of the bare coupling constant. The compact U(1) theory at

216
An Introduction to Monte Carlo Simulations of Matrix Field Theory
weak coupling is not conﬁning and contains no glueballs but simply the photons
of the free Maxwell theory. One speaks of a Coulomb phase at weak coupling and
a conﬁning phase at strong coupling. In the Coulomb phase photons are massless
and the static potential has the standard Coulomb form
V = −e2
4πr + constant,
(17.83)
whereas in the conﬁnement phase photons become massive and the potential is
linearly conﬁning at large distances
V = σr.
(17.84)
There is a phase transition at a critical coupling β ≈1 at which the string ten-
sion σ(β) vanishes in the Coulomb phase. In the conﬁnement phase topological
conﬁgurations are important such as monopoles and glueballs.
The strong-coupling expansion is an expansion in powers of 1/g2. It has the
advantage over the weak-coupling expansion that it has a non-zero radius of con-
vergence. A lot of eﬀort has been put into using it as a method of computation
similar to the high-temperature or the hopping parameter expansion for scalar ﬁeld
theories. One has to be able to tune on the values of the coupling constant where
the theory exhibits continuum behavior. This turns out to be diﬃcult for gauge
theories. However, a very important aspect of the strong-coupling expansion is that
it gives insight into the qualitative behavior of the theory such as conﬁnement and
the particle spectrum.
The strong-coupling expansion of compact U(1) theory shows explicitly that the
theory is conﬁning, i.e. the potential is linear with a string tension given by (with
a1 = β/2)
σ = −ln a1 −2(d −2)a4
1 + ....
(17.85)
References
[1] C. Gattringer and C. B. Lang, ‘Quantum Chromodynamics on the Lattice,’ Lect.
Notes Phys. 788, Springer (2010).
[2] M. Creutz, ‘Quarks, Gluons And Lattices,’ Cambridge, Uk: Univ. Pr. ( 1983) 169 P.
(Cambridge Monographs On Mathematical Physics).
[3] J. Smit, ‘Introduction to Quantum Fields on a Lattice: A Robust Mate,’ Cambridge
Lect. Notes Phys. 15, 1 (2002).
[4] H. J. Rothe, ‘Lattice Gauge Theories: An Introduction,’ World Sci. Lect. Notes Phys.
74, 1 (2005).
[5] I. Montvay and G. Munster, ‘Quantum Fields on a Lattice,’ Cambridge, UK: Univ.
Pr. (1994) 491 p. (Cambridge monographs on mathematical physics).

Chapter 18
Codes
217

 
 
 
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank
This page intentionally left blank

Codes
219
File: /home/ydri/Desktop/TP_QFT/codes/metropolis-ym.f
Page 1 of 6
      program my_metropolis_ym
      implicit none
      integer dim,dimm,N,ther,mc,Tther,Tmc
      integer lambda,i,j,idum
      parameter (dimm=10,N=8)
      parameter (Tther=2**11,Tmc=2**11)
      double complex X(dimm,N,N)
      double precision xx,y,Accept,Reject,inn,interval,pa 
      double precision act(Tmc),actio,average_act,error_act 
      double precision t_1, t_2
      real x0
      call cpu_time(t_1)
      
      do dim=2,dimm
         if(dim.le.dimm)then
            
c..........initialization of random number generator...........
            
            idum=-148175
            x0=0.0
            idum=idum-2*int(secnds(x0))      
            
c.......inititialization of X................................
            
            inn=1.0d0
            do lambda=1,dimm 
               if (lambda.le.dim)then
                  do i=1,N
                     do j=i,N
                        if (j.ne.i) then
                           xx=interval(idum,inn)
                           y=interval(idum,inn)
                           X(lambda,i,j)=cmplx(xx,y)
                           X(lambda,j,i)=cmplx(xx,-y)
                        else
                           xx=interval(idum,inn)
                           X(lambda,i,j)=xx
                        endif
                     enddo
                  enddo
               else
                  do i=1,N
do j=i,N
                        if (j.ne.i) then
                           xx=0.0d0
                           y=0.0d0
                           X(lambda,i,j)=cmplx(xx,y)
                           X(lambda,j,i)=cmplx(xx,-y)
                        else
                           xx=0.0d0
                           X(lambda,i,j)=xx
                        endif
                     enddo
                  enddo
               endif
            enddo
            
c.... accepts including flips, rejects and the acceptance rate pa...
            
            Reject=0.0d0
            Accept=0.0d0
            pa=0.0d0
            
c.............thermalization.......................................
            
            do ther=1,Tther

220
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/metropolis-ym.f
Page 2 of 6
               call metropolis(dim,dimm,N,X,Reject,Accept,inn,idum)
               call adjust_inn(pa,inn,Reject,Accept)  
               call  action(dim,dimm,N,X,actio)
               write(*,*)ther,actio,pa
               write(10+dim,*)ther,actio,pa
            enddo
            
c............monte carlo evolution.................................
            
            do mc=1,Tmc
               call metropolis(dim,dimm,N,X,Reject,Accept,inn,idum)
               call adjust_inn(pa,inn,Reject,Accept)  
               call  action(dim,dimm,N,X,actio)
               act(mc)=actio
               write(*,*)mc,act(mc),pa
               write(21+dim,*)mc,act(mc),pa
            enddo
            
c.............measurements.........................................
            
            call jackknife_binning(Tmc,act,average_act,error_act)
            write(*,*)dim,average_act,error_act
            write(32,*)dim,average_act,error_act
         endif
      enddo
      
c.........cpu time............................................
      
      call cpu_time(t_2)
      write(*,*)"cpu_time", t_2-t_1
      
      return
      end
c...............action......................................
      
      subroutine action(dim,dimm,N,X,actio)
      implicit none
      integer dim,dimm,N,mu,nu,i,j,k,l
      double complex X(dimm,N,N)
      double precision actio,action0
      
      actio=0.0d0
do mu =1,dimm
         do nu=mu+1,dimm
            action0=0.0d0
            do i=1,N
               do j=1,N
                  do k=1,N
                     do l=1,N
                 action0=action0+X(mu,i,j)*X(nu,j,k)*X(mu,k,l)*X(nu,l,i)
     &                       -X(mu,i,j)*X(mu,j,k)*X(nu,k,l)*X(nu,l,i)
                     enddo
                  enddo
               enddo
            enddo
            action0=-N*action0
            actio=actio+action0
         enddo
      enddo
      
      return
      end
      
c..............metropolis algorithm..........................
      
      subroutine metropolis(dim,dimm,N,X,Reject,Accept,inn,idum)

Codes
221
File: /home/ydri/Desktop/TP_QFT/codes/metropolis-ym.f
Page 3 of 6
      implicit none
      integer dim,dimm,N,i,j,lambda,idum
      double precision Reject,Accept,inn,interval,deltaS,ran2,z1,p1,xx,y
      double complex X(dimm,N,N),dc,dcbar
      
      do lambda=1,dim   
c..............diagonal..........................      
         do i=1,N
            xx=interval(idum,inn)
            y=interval(idum,inn)
            dc=cmplx(xx,0)
            dcbar=cmplx(xx,-0)  
            call variationYM(dim,dimm,N,lambda,i,i,dc,dcbar,X,deltaS)
            if ( deltaS .gt. 0.0d0 ) then
               z1=ran2(idum)
               p1=dexp(-deltaS)
               if ( z1 .lt. p1 ) then
                  X(lambda,i,i)=X(lambda,i,i)+dc+dcbar
                  Accept=Accept+1.0d0
               else
                  Reject=Reject+1.0d0
               endif
            else
               X(lambda,i,i)=X(lambda,i,i)+dc+dcbar
               Accept=Accept+1.0d0
            endif
         enddo
c............off diagonal..........................      
         do i=1,N
            do j=i+1,N
               xx=interval(idum,inn)
               y=interval(idum,inn)
               dc=cmplx(xx,y)
               dcbar=cmplx(xx,-y)  
               call variationYM(dim,dimm,N,lambda,i,j,dc,dcbar,X,deltaS)
               if ( deltaS .gt. 0.0d0 ) then
                  z1=ran2(idum)
                  p1=dexp(-deltaS)
                  if ( z1 .lt. p1 ) then
                     X(lambda,i,j)=X(lambda,i,j)+dc
                     Accept=Accept+1.0d0
                  else
                     Reject=Reject+1.0d0
endif
               else
                  X(lambda,i,j)=X(lambda,i,j)+dc
                  Accept=Accept+1.0d0
               endif
               X(lambda,j,i)=dconjg(X(lambda,i,j))
            enddo
         enddo                
      enddo
      
      return
      end
      
c........variation of the action...........................
      subroutine variationYM(dim,dimm,N,lambda,i,j,dc,dcbar,X,deltaS)
      implicit none
      integer dim,dimm,N,i,j,lambda,sigma,k,l,p,q
      double complex delta0,delta1,del2,del3,delta2
      double precision delta11,delta22,deltaS
      double complex X(dimm,N,N),dc,dcbar
      
      delta0=0.0d0            
      do sigma=1,dim

222
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/metropolis-ym.f
Page 4 of 6
         if (sigma.ne.lambda)then
            do k=1,N
               delta0=delta0-X(sigma,i,k)*X(sigma,k,i)
     &              -X(sigma,j,k)*X(sigma,k,j)
            enddo              
         endif
      enddo
      delta1=0.0d0
      delta1=delta1+dc*dcbar*delta0
      if (i.eq.j) then
         delta1=delta1+0.5d0*(dc*dc+dcbar*dcbar)*delta0
      endif
      do sigma=1,dim
         if (sigma.ne.lambda)then
            delta1=delta1+dc*dc*X(sigma,j,i)*X(sigma,j,i)
     &           +dcbar*dcbar*X(sigma,i,j)*X(sigma,i,j)
     &           +2.0d0*dc*dcbar*X(sigma,i,i)*X(sigma,j,j) 
         endif
      enddo
      delta1=-N*delta1
      delta11=real(delta1)
      del2=0.0d0
      del3=0.0d0
      do sigma=1,dim
         do k=1,N
            do l=1,N
               del2=del2+2.0d0*X(sigma,i,k)*X(lambda,k,l)*X(sigma,l,j)
     &              -1.0d0*X(sigma,i,k)*X(sigma,k,l)*X(lambda,l,j)
     &              -1.0d0*X(lambda,i,k)*X(sigma,k,l)*X(sigma,l,j)
               del3=del3+2.0d0*X(sigma,j,k)*X(lambda,k,l)*X(sigma,l,i)
     &              -1.0d0*X(sigma,j,k)*X(sigma,k,l)*X(lambda,l,i)
     &              -1.0d0*X(lambda,j,k)*X(sigma,k,l)*X(sigma,l,i)
            enddo
         enddo
      enddo
      delta2=0.0d0
      delta2=-N*dcbar*del2-N*dc*del3
      delta22=real(delta2)
      deltaS=delta11+delta22
      
      return       
      end
      
c........the jackknife estimator................................
      
      subroutine jackknife_binning(TMC,f,average,error)
      implicit none
      integer i,j,TMC,zbin,nbin
      double precision xm
      double precision f(1:TMC),sumf,y(1:TMC)
      double precision sig0,sig,error,average
      
c..............TMC is the number of data points. sig0 is the standard deviation. sumf is the sum of all 
the data points f_i whereas xm is the average of f......
      sig0=0.0d0
      sumf=0.0d0
      do i=1,TMC
         sumf=sumf+f(i)
      enddo
      xm=sumf/TMC 
c.... zbin is the number of elements we remove each time from the set of TMC data points. the minimum 
number we can remove is 1 whereas the maximum number we can remove is TMC-1.each time we remove zbin 
elements we end up with  nbin sets (or bins)...........
c     do zbin=1,TMC-1
      zbin=1               
      nbin=int(TMC/zbin)
      sig=0.0d0

Codes
223
File: /home/ydri/Desktop/TP_QFT/codes/metropolis-ym.f
Page 5 of 6
      do i=1,nbin,1
c...  y(i) is the average of the elements in the ith bin.This bin contains TMC-zbin data points after we 
had removed zbin elements. for zbin=1 we have nbin=TMC.In this case there are TMC bins and y_i=sum_{j#i}
x_j/(TMC-1). for zbin=2 we have nbin=TMC/2. In this case there are TMC/2 bins and y_i=  sum_jx_j/(TMC-2)-
x_{2i}/(TMC-2)-x_{2i-1}/(TMC-2)...   
         y(i)=sumf
         do j=1,zbin
            y(i)=y(i)-f((i-1)*zbin+j )
         enddo
         y(i)= y(i)/(TMC-zbin)
c..........the standard deviation computed for the ith bin..............             
         sig=sig+((nbin-1.0d0)/nbin)*(y(i)-xm)*(y(i)-xm)
      enddo
c.... the standard deviation computed for the set of all bins with fixed zbin.....
      sig=sig
c..................the error....................................
      sig=dsqrt(sig)
c.... we compare the result with the error obtained for the previous zbin, if it is larger, then this is 
the new value of the error...
      if (sig0 .lt. sig) sig0=sig
c     enddo
c.... the final value of the error..............................................................          
      error=sig0
      average=xm
      
      return
      end
      
c.............the random number generator ran2..................
      
      function ran2(idum)
      implicit none
      integer idum,IM1,IM2,IMM1,IA1,IA2,IQ1,IQ2,IR1,IR2,NTAB,NDIV
      real AM,EPS,RNMX
      double precision ran2
      parameter (IM1=2147483563,IM2=2147483399,AM=1./IM1,IMM1=IM1-1,
     &     IA1=40014,IA2=40692,IQ1=53668,IQ2=52774,IR1=12211,
     &     IR2=3791,NTAB=32,NDIV=1+IMM1/NTAB,EPS=1.2E-7,RNMX=1.-EPS)
      integer idum2,j,k,iv(NTAB),iy
      SAVE iv,iy,idum2
      DATA idum2/123456789/,iv/NTAB*0/,iy/0/
      
      if (idum.le.0) then
         idum=max(-idum,1)
         idum2=idum
         do j=NTAB+8,1,-1
            k=idum/IQ1
            idum=IA1*(idum-k*IQ1)-k*IR1
            if (idum.lt.0) idum=idum+IM1
            if (j.le.NTAB) iv(j)=idum
         enddo
         iy=iv(1)
      endif
      k=idum/IQ1
      idum=IA1*(idum-k*IQ1)-k*IR1
      if (idum.lt.0) idum=idum+IM1
      k=idum2/IQ2
      idum2=IA2*(idum2-k*IQ2)-k*IR2
      if (idum2.lt.0) idum2=idum2+IM2
      j=1+iy/NDIV
      iy=iv(j)-idum2
      iv(j)=idum
      if (iy.lt.1) iy=iy+IMM1
      ran2=min(AM*iy,RNMX)
      
      return
      end

224
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/metropolis-ym.f
Page 6 of 6
c.............interval..................................
      
      function interval(idum,inn)
      implicit none
      double precision interval,inn,ran2
      integer idum
      
      interval=ran2(idum)
      interval=interval+interval-1.0d0
      interval=interval*inn
      
      return
      end
      
c.........adjusting interval..............................        
      
      subroutine adjust_inn(pa,inn,Reject,Accept)
      implicit none    
      double precision inn,pa,Reject,Accept
      
c.....pa acceptance rate..................................
      pa=(Accept)/(Reject+Accept)
c........fixing the acceptance rate at 30 %..................
      if (pa.ge.0.30) inn=inn*1.20d0
      if (pa.le.0.25) inn=inn*0.80d0
      
      return
      end
      
      
       
   

Codes
225
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-ym.f
Page 1 of 7
      program my_hybrid_ym
      implicit none
      integer d,N,i,j,k,lambda,idum,tt,time,timeT,tther,Tth
      parameter (d=4,N=4)
      parameter (Tth=2**10)
      double precision gamma,mm,alpha,inn,dt,interval
      double complex X(d,N,N),P(d,N,N)
      double precision actio,ham,kin,variationH
      double precision Reject,Accept,pa
      double precision varH(Tth),varH_average,varH_error
      double precision ac(Tth),ac_average,ac_error
      real x0
c..........initialization of random number generator...........
      
      idum=-148175
      x0=0.0
c...  seed should be set to a large odd integer according to the manual. secnds(x) gives number of 
seconds-x elapsed since midnight. the 2*int(secnds(x0)) is always even so seed is always odd....
      idum=idum-2*int(secnds(x0))
      
c...................testing molecular dynamics......................
      
c     call hot(N,d,idum,inn,X,P)   
c      call cold(N,d,X)
c      time=1
c      dt=0.01d0
c      timeT=100
c      do tt=1,timeT
c         call molecular_dynamics(N,d,dt,time,gamma,mm,alpha,X,P)
c         call action(d,N,X,P,alpha,mm,gamma,actio,ham,kin)
c         write(9,*)tt,actio,ham
c         write(*,*)tt,actio,ham
c      enddo
      
c.......parameters of molecular dynamics...........
      
      time=100
      dt=0.01d0
      
c..................parameters..............
      
      mm=0.0d0
      alpha=0.0d0
      do k=0,20     
         gamma=2.1d0-k*0.1d0
         
c............initialization of X and P...............
         
         inn=1.0d0
         call hot(N,d,idum,inn,X,P)  
         call cold(N,d,X)
         
c................accepts including flips, rejects and the acceptance rate pa...............
         
         Reject=0.0d0
         Accept=0.0d0
         pa=0.0d0
         
c..............thermalization................
         
         do tther=1,Tth
            call metropolis(N,d,gamma,mm,alpha,dt,time,X,P,Reject,Accept
     &           ,variationH)
         enddo
         
c..................monte carlo evolution....

226
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-ym.f
Page 2 of 7
         
         do tther=1,Tth
            call metropolis(N,d,gamma,mm,alpha,dt,time,X,P,Reject,Accept
     &           ,variationH)   
            pa=(Accept)/(Reject+Accept)                   
            call action(d,N,X,P,alpha,mm,gamma,actio,ham,kin) 
            ac(tther)=actio
            varH(tther)=dexp(-variationH)                        
            write(10,*)tther,actio,ham,kin,variationH,pa
            write(*,*)tther,actio,ham,kin,variationH,pa
         enddo
         
c..............measurements................
         
         call jackknife_binning(Tth,varH,varH_average,varH_error)
         write(*,*)gamma,alpha,mm,varH_average,varH_error
         write(11,*)gamma,alpha,mm,varH_average,varH_error
         call jackknife_binning(Tth,ac,ac_average,ac_error)
         write(*,*)gamma,alpha,mm,ac_average,ac_error
         write(12,*)gamma,alpha,mm,ac_average,ac_error
      enddo
      
      return
      end
c.................metropolis algorithm................
    
      subroutine metropolis(N,d,gamma,mm,alpha,dt,time,X,P,Reject,Accept
     &     ,variationH)
      implicit none
      integer N,d,i,j,mu,nu,k,l,idum,time
      double precision gamma,mm,alpha,inn,dt,ran2,Reject,Accept
      double complex var(d,N,N),X(d,N,N),X0(d,N,N),P(d,N,N),P0(d,N,N)
      double precision variations,variationH,probabilityS,probabilityH,r
      double precision actio,ham,kin
c........Gaussian initialization.....      
      call gaussian(d,N,P)
      X0=X
      P0=P
      call action(d,N,X,P,alpha,mm,gamma,actio,ham,kin)
      variationS=actio
      variationH=ham
c............molecular dynamics evolution.....
      call molecular_dynamics(N,d,dt,time,gamma,mm,alpha,X,P)
      call action(d,N,X,P,alpha,mm,gamma,actio,ham,kin)
      variationS=actio-variationS
      variationH=ham-variationH
c........metropolis accept-reject step.................
      if(variationH.lt.0.0d0)then
         accept=accept+1.0d0
      else
         probabilityH=dexp(-variationH)
         r=ran2(idum)
         if (r.lt.probabilityH)then
            accept=accept+1.0d0
         else
            X=X0
            P=P0
            Reject=Reject+1.0d0

Codes
227
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-ym.f
Page 3 of 7
         endif
      endif
      
      return
      end
c...........actions and Hamiltonians.........
      
      subroutine action(d,N,X,P,alpha,mm,gamma,actio,ham,kin)
      implicit none
      integer d,N,mu,nu,i,j,k,l
      double complex X(d,N,N),P(d,N,N),ii,CS,action0,ham0,action1,
     &     actio0,action2,ham1
      double precision actio,ham,kin
      double precision mm,gamma,alpha
      
       ii=cmplx(0,1)
       actio0=cmplx(0,0)
       do mu =1,d
          do nu=mu+1,d
             action0=cmplx(0,0)
             do i=1,N
                do j=1,N
                  do k=1,N
                     do l=1,N
                 action0=action0+X(mu,i,j)*X(nu,j,k)*X(mu,k,l)*X(nu,l,i)
     &                       -X(mu,i,j)*X(mu,j,k)*X(nu,k,l)*X(nu,l,i)
                     enddo
                  enddo
               enddo
            enddo
            actio0=actio0+action0
         enddo
      enddo
      actio=real(actio0)
      actio=-N*gamma*actio
      
      ham1=cmplx(0,0)
      action2=cmplx(0,0)
      do mu =1,d
      ham0=cmplx(0,0)
      action1=cmplx(0,0)
      do i=1,N
do j=1,N
            ham0=ham0+P(mu,i,j)*P(mu,j,i)
            action1=action1+X(mu,i,j)*X(mu,j,i)
         enddo
      enddo      
      action2=action2+action1
      ham1=ham1+ham0
      enddo
      ham=0.5d0*real(ham1)
      kin=ham
      actio=actio+0.5d0*mm*real(action2)
      CS=0.0d0
      do i=1,N
         do j=1,N
            do k=1,N
               CS=CS+ii*X(1,i,j)*X(2,j,k)*X(3,k,i)
     &              -ii*X(1,i,j)*X(3,j,k)*X(2,k,i)
            enddo
         enddo
      enddo
      actio=actio+2.0d0*alpha*N*real(CS)
      ham=ham+actio
      

228
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-ym.f
Page 4 of 7
      return
      end
      
c.......the force.............
      
      subroutine variation(N,d,gamma,mm,alpha,X,var)
      implicit none
      integer N,d,i,j,mu,nu,k,l
      double precision gamma,mm,alpha
      double complex var(d,N,N),X(d,N,N),ii
      
      ii=dcmplx(0,1)     
      do mu=1,d  
         do i=1,N
            do j=i,N
             var(mu,i,j)=cmplx(0,0)
               do nu=1,d
                  do k=1,N
                     do l=1,N
             var(mu,i,j)=var(mu,i,j)+2.0d0*X(nu,j,k)*X(mu,k,l)*X(nu,l,i)
     &                       -X(nu,j,k)*X(nu,k,l)*X(mu,l,i)
     &                       -X(mu,j,k)*X(nu,k,l)*X(nu,l,i)
                     enddo
                  enddo
               enddo
               var(mu,i,j)=-N*gamma*var(mu,i,j)+mm*X(mu,j,i)
               if(mu.eq.1)then
                  do k=1,N
             var(mu,i,j)=var(mu,i,j)+2.0d0*ii*alpha*N*X(2,j,k)*X(3,k,i)
     &                    -2.0d0*ii*alpha*N*X(3,j,k)*X(2,k,i)
                 enddo
              endif
              if(mu.eq.2)then
                 do k=1,N
             var(mu,i,j)=var(mu,i,j)+2.0d0*ii*alpha*N*X(3,j,k)*X(1,k,i)
     &                   -2.0d0*ii*alpha*N*X(1,j,k)*X(3,k,i)
                 enddo
              endif
              if(mu.eq.3)then
                 do k=1,N
             var(mu,i,j)=var(mu,i,j)+2.0d0*ii*alpha*N*X(1,j,k)*X(2,k,i)
     &                   -2.0d0*ii*alpha*N*X(2,j,k)*X(1,k,i)
                 enddo
endif
             var(mu,j,i)=conjg(var(mu,i,j))
           enddo
        enddo          
      enddo   
      
      return
      end
      
c.............leap frog..............
      
      subroutine molecular_dynamics(N,d,dt,time,gamma,mm,alpha,X,P)
      implicit none
      integer N,d,i,j,mu,nn,time
      double precision dt,gamma,mm,alpha
      double complex X(d,N,N),P(d,N,N),var(d,N,N)
      
      do nn=1,time         
         call variation(N,d,gamma,mm,alpha,X,var)         
         do mu=1,d                  
            do i=1,N
               do j=i,N
                  P(mu,i,j)=P(mu,i,j)-0.5d0*dt*var(mu,i,j)
                  X(mu,i,j)=X(mu,i,j)+dt*conjg(P(mu,i,j))

Codes
229
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-ym.f
Page 5 of 7
                  X(mu,j,i)=conjg(X(mu,i,j))
               enddo
            enddo
         enddo         
         call variation(N,d,gamma,mm,alpha,X,var)                
         do mu=1,d
            do i=1,N
               do j=i,N
                  P(mu,i,j)=P(mu,i,j)-0.5d0*dt*var(mu,i,j)
                  P(mu,j,i)=conjg(P(mu,i,j))
               enddo
            enddo
         enddo         
      enddo
      
      return
      end
            
c.........generation of Gaussian noise for the field P..................        
      
      subroutine gaussian(d,N,P)
      implicit none
      integer d,N,mu,i,j,idum    
      double precision pi,phi,r,ran2
      double complex ii,P(d,N,N)
      
      pi=dacos(-1.0d0)
      ii=cmplx(0,1)
      do mu=1,d
         do i=1,N
            phi=2.0d0*pi*ran2(idum)
            r=dsqrt(-2.0d0*dlog(1.0d0-ran2(idum))) 
            P(mu,i,i)=r*dcos(phi)
         enddo
         do i=1,N
            do j=i+1,N
               phi=2.0d0*pi*ran2(idum)
               r=dsqrt(-1.0d0*dlog(1.0d0-ran2(idum))) 
               P(mu,i,j)=r*dcos(phi)+ii*r*dsin(phi)
               P(mu,j,i)=conjg(P(mu,i,j))
            enddo
         enddo
      enddo
      return
      end
c........the jackknife estimator..................
      subroutine jackknife_binning(TMC,f,average,error)
      implicit none
      integer i,j,TMC,zbin,nbin
      double precision xm
      double precision f(1:TMC),sumf,y(1:TMC)
      double precision sig0,sig,error,average
      
      sig0=0.0d0
      sumf=0.0d0
      do i=1,TMC
         sumf=sumf+f(i)
      enddo
      xm=sumf/TMC 
c     do zbin=1,TMC-1
      zbin=1               
      nbin=int(TMC/zbin)
      sig=0.0d0
      do i=1,nbin,1

230
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-ym.f
Page 6 of 7
         y(i)=sumf
         do j=1,zbin
            y(i)=y(i)-f((i-1)*zbin+j )
         enddo
         y(i)= y(i)/(TMC-zbin)
         sig=sig+((nbin-1.0d0)/nbin)*(y(i)-xm)*(y(i)-xm)
      enddo
      sig=sig
      sig=dsqrt(sig)
      if (sig0 .lt. sig) sig0=sig
c     enddo         
      error=sig0
      average=xm
      
      return
      end
      
c.............the random number generator ran2.........
      
      function ran2(idum)
      implicit none
      integer idum,IM1,IM2,IMM1,IA1,IA2,IQ1,IQ2,IR1,IR2,NTAB,NDIV
      real AM,EPS,RNMX
      double precision ran2
      parameter (IM1=2147483563,IM2=2147483399,AM=1./IM1,IMM1=IM1-1,
     &     IA1=40014,IA2=40692,IQ1=53668,IQ2=52774,IR1=12211,
     &     IR2=3791,NTAB=32,NDIV=1+IMM1/NTAB,EPS=1.2E-7,RNMX=1.-EPS)
      integer idum2,j,k,iv(NTAB),iy
      SAVE iv,iy,idum2
      DATA idum2/123456789/,iv/NTAB*0/,iy/0/
      
      if (idum.le.0) then
         idum=max(-idum,1)
         idum2=idum
         do j=NTAB+8,1,-1
            k=idum/IQ1
            idum=IA1*(idum-k*IQ1)-k*IR1
            if (idum.lt.0) idum=idum+IM1
            if (j.le.NTAB) iv(j)=idum
         enddo
         iy=iv(1)
      endif
      k=idum/IQ1
      idum=IA1*(idum-k*IQ1)-k*IR1
      if (idum.lt.0) idum=idum+IM1
      k=idum2/IQ2
      idum2=IA2*(idum2-k*IQ2)-k*IR2
      if (idum2.lt.0) idum2=idum2+IM2
      j=1+iy/NDIV
      iy=iv(j)-idum2
      iv(j)=idum
      if (iy.lt.1) iy=iy+IMM1
      ran2=min(AM*iy,RNMX)
      
      return
      end
c........hot start...................
      
      subroutine hot(N,d,idum,inn,X,P) 
      implicit none
      integer lambda,i,j,N,d,idum
      double complex X(d,N,N),P(d,N,N)
      double precision xx,y,inn,interval
      
      do lambda=1,d 
         do i=1,N

Codes
231
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-ym.f
Page 7 of 7
            do j=i,N
               if (j.ne.i) then
                  xx=interval(idum,inn)
                  y=interval(idum,inn)
                  X(lambda,i,j)=cmplx(xx,y)
                  X(lambda,j,i)=cmplx(xx,-y)
                  xx=interval(idum,inn)
                  y=interval(idum,inn)
                  P(lambda,i,j)=cmplx(xx,y)
                  P(lambda,j,i)=cmplx(xx,-y)
               else
                  xx=interval(idum,inn)
                  X(lambda,i,j)=xx
                  xx=interval(idum,inn)
                  P(lambda,i,j)=xx
               endif
            enddo
         enddo
      enddo
      
      return
      end
      
c.............interval..............
      
      function interval(idum,inn)
      implicit none
      double precision interval,inn,ran2
      integer idum
      
      interval=ran2(idum)
      interval=interval+interval-1.0d0
      interval=interval*inn
      
      return
      end
      
c......cold start.....................
      
      subroutine cold(N,d,X) 
      implicit none
      integer lambda,i,j,N,d
      double complex X(d,N,N)
      do lambda=1,d 
         do i=1,N
            do j=1,N
               X(lambda,i,j)=cmplx(0,0)
            enddo
         enddo
      enddo
      
      return
      end   
      
      

232
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-scalar-fuzzy.f
Page 1 of 10
      program my_hybrid_scalar_fuzzy
      implicit none
      integer N,i,j,k,idum,tt,time,tther,Tth,cou,ttco,Tco,Tmc,nn
      parameter (N=6)
      parameter (Tth=2**10,Tmc=2**10,Tco=2**0)
      double precision a,b,c,at,bt,ct
      double complex phi(N,N),P(N,N),phi0(N,N)
      double precision actio,ham,kin,quad,quar,mag,variationH,ev(1:N)
      double precision Reject,Accept,pa,inn,dt,interval,xx,y,t_1,t_2
      double precision varH(Tmc),varH_average,varH_error
      double precision acti(Tmc),acti_average,acti_error
      double precision Cv(Tmc),Cv_average,Cv_error
      double precision ma(Tmc),ma_average,ma_error
      double precision chi(Tmc),chi_average,chi_error
      double precision p0(Tmc),p0_average,p0_error
      double precision pt(Tmc),pt_average,pt_error
      double precision kinet(Tmc),k_average,k_error
      double precision ide_average,ide_error
      double precision qu(Tmc),qu_average,qu_error
      double precision target_pa_high,target_pa_low,dt_max,dt_min,inc
     &     ,dec
      real x0
      
      call cpu_time(t_1)
      
c..........initialization of random number generator...........
      
      idum=-148175
      x0=0.0
      idum=idum-2*int(secnds(x0))
            
c.............parameters..................
      
      at=dsqrt(1.0d0*N)!1.0d0
      a=at/dsqrt(1.0d0*N)
      ct=1.0d0
      c=N*N*ct
      do k=0,0
         bt=-5.0d0+k*0.1d0
         b=N*dsqrt(1.0d0*N)*bt
         
c.............initialization of phi and P.....
         inn=1.0d0
         call hot(N,idum,inn,phi,P)  
         
c.......parameters of molecular dynamics...........
      
         time=10
         dt=0.01d0
c................accepts including flips, rejects and the acceptance rate pa...............
         
         Reject=0.0d0
         Accept=0.0d0
         pa=0.0d0
      
c.....the acceptance rate is fixed in [0.7,0.9] such that dt is in [0.0001,1]....
         
         target_pa_high=0.90d0
         target_pa_low=0.70d0
         dt_max=1.0d0
         dt_min=0.0001d0
         inc=1.2d0
         dec=0.8d0
         nn=1
         

Codes
233
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-scalar-fuzzy.f
Page 2 of 10
c............thermalization................................
         
         do tther=1,Tth
            call metropolis(N,a,b,c,dt,time,phi,P,Reject,Accept
     &           ,variationH,idum)
            call  action(N,phi,P,a,b,c,kin,quad,quar,actio,ham,mag)
            cou=tther
            call adjust_inn(cou,pa,dt,time,Reject,Accept,
     &           nn,target_pa_high,target_pa_low,dt_max,dt_min,inc,dec)
            write(*,*)tther,pa,dt,actio
         enddo
         
c..................monte carlo evolution....................
         
         do tther=1,Tmc
c................removing auto-correlations by separating data points by tco monte carlo steps.....
            do ttco=1,Tco
               call metropolis(N,a,b,c,dt,time,phi,P,Reject,Accept
     &              ,variationH,idum)
            enddo
c...........constructing thermalized obervables as vectors.......
            call  action(N,phi,P,a,b,c,kin,quad,quar,actio,ham,mag)
            acti(tther)=actio
            ma(tther)=mag
            p0(tther)=mag*mag/N**2
            pt(tther)=quad/N
            kinet(tther)=kin  
            qu(tther)=quar
            varH(tther)=dexp(-variationH)
            
c...........adjusting the step dt.................
            cou=tther
            call adjust_inn(cou,pa,dt,time,Reject,Accept,
     &           nn,target_pa_high,target_pa_low,dt_max,dt_min,inc,dec)               
            write(*,*)tther,pa,dt,actio
c.........the eigenvalues of phi...................................................
            phi0=phi
            call eigenvalues(N,phi0,ev)          
            write(62,*)tther,ev
         enddo
         
c..............measurements...................................................
         
c....................energy........................................................               
         call jackknife_binning(Tmc,acti,acti_average,acti_error)
         write(*,*)"action",a,bt,ct,acti_average,acti_error
         write(10,*)a,bt,ct,acti_average,acti_error
c.........specific heat Cv=<(S_i-<S>)^2>............................               
         do tther=1,Tmc
            Cv(tther)=0.0d0
            Cv(tther)=Cv(tther)+acti(tther)
            Cv(tther)=Cv(tther)-acti_average
            Cv(tther)=Cv(tther)*Cv(tther)
         enddo
         call jackknife_binning(Tmc,Cv,Cv_average,Cv_error)
         write(*,*)"specific heat",a,bt,ct,Cv_average,Cv_error
         write(20,*)a,bt,ct,Cv_average,Cv_error
c..............magnetization.................................................
         call jackknife_binning(Tmc,ma,ma_average,ma_error)
         write(*,*)"magnetization",a,bt,ct,ma_average,ma_error

234
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-scalar-fuzzy.f
Page 3 of 10
         write(30,*)a,bt,ct,ma_average,ma_error
c..............susceptibility...........................................................                 
         do tther=1,Tmc
            chi(tther)=0.0d0
            chi(tther)=chi(tther)+ma(tther)
            chi(tther)=chi(tther)-ma_average
            chi(tther)=chi(tther)*chi(tther)
         enddo
         call jackknife_binning(Tmc,chi,chi_average,chi_error)
         write(*,*)"susceptibility", a,bt,ct,chi_average,chi_error
         write(40,*)a,bt,ct,chi_average,chi_error
c.............power in the zero mode.............................................
         call jackknife_binning(Tmc,p0,p0_average,p0_error)
         write(*,*)"zero power", a,bt,ct,p0_average,p0_error
         write(50,*)a,bt,ct,p0_average,p0_error
c.............total power=quadratic term/N.........................................
         call jackknife_binning(Tmc,pt,pt_average,pt_error)
         write(*,*)"total power=quadrtic/N",a,bt,ct,pt_average,pt_error
         write(60,*)a,bt,ct,pt_average,pt_error
c..............kinetic term.........................................................
         call jackknife_binning(Tmc,kinet,k_average,k_error)
         write(*,*)"kinetic",a,bt,ct,k_average,k_error
         write(70,*)a,bt,ct,k_average,k_error
c..............quartic term....
         call jackknife_binning(Tmc,qu,qu_average,qu_error)
         write(*,*)"quartic", a,bt,ct,qu_average,qu_error
         write(80,*)a,bt,ct,qu_average,qu_error
c..............schwinger-dyson identity.....................................
         ide_average=2.0d0*a*k_average+2.0d0*b*N*pt_average
     &        +4.0d0*c*qu_average
         ide_average=ide_average/(N*N)
         ide_error=2.0d0*a*k_error+2.0d0*b*N*pt_error
     &        +4.0d0*c*qu_error
         ide_error=ide_error/(N*N)
         write(*,*)"ide", a,bt,ct,ide_average,ide_error
         write(81,*)a,bt,ct,ide_average,ide_error
c...............variation of hamiltonian.................................
         call jackknife_binning(Tmc,varH,varH_average,varH_error)
         write(*,*)"exp(-\Delta H)",a,bt,ct,varH_average,varH_error
         write(11,*)a,bt,ct,varH_average,varH_error
      enddo
      
c.......................cpu time.............................................
call cpu_time(t_2)
      write(*,*)"cpu_time=", t_2-t_1
      
      return
      end
      
c.....................metropolis algorithm...........................
      
      subroutine metropolis(N,a,b,c,dt,time,phi,P,Reject,Accept
     &     ,variationH,idum)
      implicit none
      integer N,i,j,mu,nu,k,l,idum,time
      double precision a,b,c,inn,dt,ran2,Reject,Accept
      double complex var(N,N),phi(N,N),phi0(N,N),P(N,N),P0(N,N)
      double precision variations,variationH,probabilityS,probabilityH,r
      double precision actio,ham,kin,quad,quar,mag
      
c........Gaussian initialization, molecular dynamics evolution and variation of the Hamiltonian....
      call gaussian(idum,N,P)
      phi0=phi
      P0=P
      call action(N,phi,P,a,b,c,kin,quad,quar,actio,ham,mag)                
      variationS=actio
      variationH=ham              

Codes
235
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-scalar-fuzzy.f
Page 4 of 10
      call molecular_dynamics(N,dt,time,a,b,c,phi,P)
      call action(N,phi,P,a,b,c,kin,quad,quar,actio,ham,mag)                     
      variationS=actio-variationS
      variationH=ham-variationH
c...........metropolis accept-reject step.................                   
      if(variationH.lt.0.0d0)then
         accept=accept+1.0d0
      else
         probabilityH=dexp(-variationH)
         r=ran2(idum)
         if (r.lt.probabilityH)then
            accept=accept+1.0d0
         else
            phi=phi0
            P=P0
            Reject=Reject+1.0d0
         endif
      endif
      
      return
      end
      
c....................eigenvalues............................
      
      subroutine eigenvalues(N,phi0,ev)
      implicit none
      integer N,inf
      double complex cw(1:2*N-1)
      double precision rw(1:3*N-2)
      double complex phi0(1:N,1:N)
      double precision ev(1:N)
      
c.....LAPACK's zheev diagonalizes hermitian matrices...
      call zheev('N','U',N,phi0,N,ev,cw,2*N-1,rw,inf)
      
      return
      end      
      
c................actions and Hamiltonians..................................
      
      subroutine action(N,phi,P,a,b,c,kin,quad,quar,actio,ham,mag)
      implicit none
      integer N,mu,i,j,k,l
double complex phi(N,N),P(N,N)
      double precision a,b,c
      double precision kin,quad,quar,actio,ham,mag
      double complex kine,quadr,quart,ham0
      double complex Lplus(1:N,1:N),Lminus(1:N,1:N),Lz(1:N,1:N)
      double complex X(1:3,1:N,1:N)
      
c..................kinetic term and mass term..................
      call SU2(N,X,Lplus,Lminus)
      kine=cmplx(0,0)
      do i=1,N
         do j=1,N
            do k=1,N
               do l=1,N
                  kine=kine+X(1,i,j)*phi(j,k)*X(1,k,l)*phi(l,i)
     &                 +X(2,i,j)*phi(j,k)*X(2,k,l)*phi(l,i)
     &                 +X(3,i,j)*phi(j,k)*X(3,k,l)*phi(l,i)
               enddo
            enddo
         enddo
      enddo
      kin=-2.0d0*real(kine)
      quadr=cmplx(0,0)
      do i=1,N

236
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-scalar-fuzzy.f
Page 5 of 10
         do j=1,N
            quadr=quadr+phi(i,j)*phi(j,i)
         enddo
      enddo
      kin=kin+0.5d0*(N*N-1.0d0)*real(quadr)
      quad=real(quadr)
c.....................quartic term..........................          
      quart=cmplx(0,0)
      do i=1,N
         do j=1,N
            do k=1,N
               do l=1,N
                  quart=quart+phi(i,j)*phi(j,k)*phi(k,l)*phi(l,i)
               enddo
            enddo
         enddo
      enddo
      quar=real(quart)
c....................action...........................
      actio=a*kin+b*quad+c*quar
c..................Hamiltonian...............................            
      ham0=cmplx(0,0)
      do i=1,N
         do j=1,N
            ham0=ham0+P(i,j)*P(j,i)
         enddo
      enddo            
      ham=0.5d0*real(ham0)
      ham=ham+actio
c.......................magnetization.............................        
      mag=0.0d0
      do i=1,N
         mag=mag+phi(i,i)
      enddo
      mag=dabs(mag)
      
      return
      end
     
c.................the force.............................................  
      
      subroutine variation(N,a,b,c,phi,var)
      implicit none
integer N,i,j,k,l,nu
      doubleprecision a,b,c
      doublecomplex var(N,N),var1(N,N),phi(N,N)
      doublecomplex Lplus(1:N,1:N),Lminus(1:N,1:N),Lz(1:N,1:N)
      doublecomplex X(1:3,1:N,1:N)
      
      call SU2(N,X,Lplus,Lminus)
      do i=1,N
         do j=i,N
            var(i,j)=cmplx(0,0)                 
            do k=1,N
               do l=1,N
                  var(i,j)=var(i,j)+X(1,j,k)*phi(k,l)*X(1,l,i)
     &                 +X(2,j,k)*phi(k,l)*X(2,l,i)
     &                 +X(3,j,k)*phi(k,l)*X(3,l,i)
               enddo
            enddo                                 
            var1(i,j)=cmplx(0,0)
            do k=1,N
               do l=1,N
                  var1(i,j)=var1(i,j)+phi(j,k)*phi(k,l)*phi(l,i)
               enddo
            enddo
            var(i,j)=-4.0d0*a*var(i,j)+(N*N-1.0d0)*a*phi(j,i)

Codes
237
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-scalar-fuzzy.f
Page 6 of 10
     &           +2.0d0*b*phi(j,i)+4.0d0*c*var1(i,j)             
            var(j,i)=conjg(var(i,j))
         enddo
      enddo          
      
      return
      end
      
c..........SU(2) generators....................
      
      subroutine SU2(N,L,Lplus,Lminus)
      implicit none
      integer i,j,N
      double complex Lplus(1:N,1:N),Lminus(1:N,1:N),Lz(1:N,1:N)
      double complex L(1:3,1:N,1:N)
      double complex ii
      
      ii=cmplx(0,1)
      do i=1,N
         do j=1,N
            if( ( i + 1 ) .eq. j )then
               Lplus(i,j)  =dsqrt( ( N - i )*i*1.0d0 )
            else
               Lplus(i,j)=0.0d0
            endif
            if( ( i - 1 ) .eq. j )then
               Lminus(i,j)=dsqrt( ( N - j )*j*1.0d0  )
            else
               Lminus(i,j)=0.0d0
            endif          
            if( i.eq.j)then               
               Lz(i,j) = ( N + 1 - i - i )/2.0d0
            else
               Lz(i,j) = 0.0d0
            endif
            L(1,i,j)=0.50d0*(Lplus(i,j)+Lminus(i,j))
            L(2,i,j)=-0.50d0*ii*(Lplus(i,j)-Lminus(i,j))
            L(3,i,j)=Lz(i,j)
         enddo
      enddo
      
      return
      end
c..............leap frog......................................
      
      subroutine molecular_dynamics(N,dt,time,a,b,c,phi,P)
      implicit none
      integer N,i,j,nn,time
      double precision dt,a,b,c
      double complex phi(N,N),P(N,N),var(N,N),ii
      
      ii=cmplx(0,1)      
      do nn=1,time         
         call variation(N,a,b,c,phi,var)         
         do i=1,N
            do j=i,N
               if (j.ne.i)then
                  P(i,j)=P(i,j)-0.5d0*dt*var(i,j)
                  phi(i,j)=phi(i,j)+dt*conjg(P(i,j))
                  phi(j,i)=conjg(phi(i,j))
               else
                  P(i,i)=P(i,i)-0.5d0*dt*var(i,i)
                  phi(i,i)=phi(i,i)+dt*conjg(P(i,i))
                  phi(i,i)=phi(i,i)-ii*aimag(phi(1,1))
                  endif
               enddo

238
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-scalar-fuzzy.f
Page 7 of 10
            enddo         
c...........last step of leap frog.......................................
          call variation(N,a,b,c,phi,var)
          do i=1,N
             do j=i,N
                if(j.ne.i)then
                   P(i,j)=P(i,j)-0.5d0*dt*var(i,j)
                   P(j,i)=conjg(P(i,j))
                else
                   P(i,i)=P(i,i)-0.5d0*dt*var(i,i)
                   P(i,i)=P(i,i)-ii*aimag(P(i,i))
                endif
             enddo
          enddo         
       enddo
       
       return
       end
      
c.........generation of Gaussian noise for the field P..................        
      
      subroutine gaussian(idum,N,P)
      implicit none
      integer N,mu,i,j,idum    
      double precision pi,phi,r,ran2
      double complex ii,P(N,N)
      
      pi=dacos(-1.0d0)
      ii=cmplx(0,1)
      do i=1,N
         phi=2.0d0*pi*ran2(idum)
         r=dsqrt(-2.0d0*dlog(1.0d0-ran2(idum))) 
         P(i,i)=r*dcos(phi)
      enddo
      do i=1,N
         do j=i+1,N
            phi=2.0d0*pi*ran2(idum)
            r=dsqrt(-1.0d0*dlog(1.0d0-ran2(idum))) 
            P(i,j)=r*dcos(phi)+ii*r*dsin(phi)
            P(j,i)=conjg(P(i,j))
         enddo
      enddo
      
return
      end
c........the jackknife estimator..................
      
      subroutine jackknife_binning(TMC,f,average,error)
      implicit none
      integer i,j,TMC,zbin,nbin
      double precision xm
      double precision f(1:TMC),sumf,y(1:TMC)
      double precision sig0,sig,error,average
      
      sig0=0.0d0
      sumf=0.0d0
      do i=1,TMC
         sumf=sumf+f(i)
      enddo
      xm=sumf/TMC 
c     do zbin=1,TMC-1
      zbin=1               
      nbin=int(TMC/zbin)
      sig=0.0d0
      do i=1,nbin,1
         y(i)=sumf

Codes
239
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-scalar-fuzzy.f
Page 8 of 10
         do j=1,zbin
            y(i)=y(i)-f((i-1)*zbin+j )
         enddo
         y(i)= y(i)/(TMC-zbin)
         sig=sig+((nbin-1.0d0)/nbin)*(y(i)-xm)*(y(i)-xm)
      enddo
      sig=sig
      sig=dsqrt(sig)
      if (sig0 .lt. sig) sig0=sig
c     enddo         
      error=sig0
      average=xm
      
      return
      end
      
c.............the random number generator ran2.........
      
      function ran2(idum)
      implicit none
      integer idum,IM1,IM2,IMM1,IA1,IA2,IQ1,IQ2,IR1,IR2,NTAB,NDIV
      real AM,EPS,RNMX
      double precision ran2
      parameter (IM1=2147483563,IM2=2147483399,AM=1./IM1,IMM1=IM1-1,
     &     IA1=40014,IA2=40692,IQ1=53668,IQ2=52774,IR1=12211,
     &     IR2=3791,NTAB=32,NDIV=1+IMM1/NTAB,EPS=1.2E-7,RNMX=1.-EPS)
      integer idum2,j,k,iv(NTAB),iy
      SAVE iv,iy,idum2
      DATA idum2/123456789/,iv/NTAB*0/,iy/0/
      
      if (idum.le.0) then
         idum=max(-idum,1)
         idum2=idum
         do j=NTAB+8,1,-1
            k=idum/IQ1
            idum=IA1*(idum-k*IQ1)-k*IR1
            if (idum.lt.0) idum=idum+IM1
            if (j.le.NTAB) iv(j)=idum
         enddo
         iy=iv(1)
      endif
      k=idum/IQ1
      idum=IA1*(idum-k*IQ1)-k*IR1
if (idum.lt.0) idum=idum+IM1
      k=idum2/IQ2
      idum2=IA2*(idum2-k*IQ2)-k*IR2
      if (idum2.lt.0) idum2=idum2+IM2
      j=1+iy/NDIV
      iy=iv(j)-idum2
      iv(j)=idum
      if (iy.lt.1) iy=iy+IMM1
      ran2=min(AM*iy,RNMX)
      
      return
      end
c........hot start...................
      
      subroutine hot(N,idum,inn,phi,P) 
      implicit none
      integer lambda,i,j,N,d,idum
      double complex phi(N,N),P(N,N)
      double precision xx,y,inn,interval
            
      do i=1,N
         do j=i,N
            if (j.ne.i) then

240
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-scalar-fuzzy.f
Page 9 of 10
               xx=interval(idum,inn)
               y=interval(idum,inn)
               phi(i,j)=cmplx(xx,y)
               phi(j,i)=cmplx(xx,-y)
               xx=interval(idum,inn)
               y=interval(idum,inn)
               P(i,j)=cmplx(xx,y)
               P(j,i)=cmplx(xx,-y)
            else
               xx=interval(idum,inn)
               phi(i,j)=xx
               xx=interval(idum,inn)
               P(i,j)=xx
            endif
         enddo
      enddo
      
      return
      end
      
c.............interval..............
      
      function interval(idum,inn)
      implicit none
      double precision interval,inn,ran2
      integer idum
      
      interval=ran2(idum)
      interval=interval+interval-1.0d0
      interval=interval*inn
      
      return
      end
      
c......cold start.....................
      
      subroutine cold(N,phi) 
      implicit none
      integer lambda,i,j,N
      double complex phi(N,N)
            
      do i=1,N
         do j=1,N
            phi(i,j)=cmplx(0,0)
         enddo
      enddo
      
      return
      end   
      
c.........adjusting interval..................        
      
      subroutine adjust_inn(cou,pa,dt,time,Rejec,Accept,
     &     nn,target_pa_high,target_pa_low,dt_max,dt_min,inc,dec)  
      implicit none  
      double precision dt,pa,Rejec,Accept
      integer time,cou,cou1
      integer nn
      double precision target_pa_high,target_pa_low,dt_max,dt_min,inc,
     &     dec,rho1,rho2,dtnew
      
c.....pa acceptance rate............
      pa=(Accept)/(Rejec+Accept)        
      cou1=mod(cou,nn)        
      if (cou1.eq.0)then
c........fixing the acceptance rate between 90 % 70 %..................
         if (pa.ge.target_pa_high) then

Codes
241
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-scalar-fuzzy.f
Page 10 of 10
            dtnew=dt*inc
            if (dtnew.le.dt_max)then
               dt=dtnew
            else
               dt=dt_max
            endif
         endif
         if (pa.le.target_pa_low) then
            dtnew=dt*dec
            if (dtnew.ge.dt_min)then
               dt=dtnew
            else
               dt=dt_min
            endif
         endif
      endif
      
      return
      end
      
      
      
      
      
      

242
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/phi-four-on-lattice.f
Page 1 of 7
      program my_phi_four_on_lattice
      implicit none
      integer N,idum,time,cou,nn,kk,ith,imc,ico,Tth,Tmc,Tco
      parameter (N=16)
      parameter (Tth=2**13,Tmc=2**14,Tco=2**3)
      double precision dt,kappa,g,phi(N,N),P(N,N),lambda_l,mu0_sq_l
      double precision mass,linear,kinetic,potential,act,Ham,variationH,
     &     quartic
      double precision target_pa_high,target_pa_low,dt_max,dt_min,inc
     &     ,dec,inn,pa,accept,reject
      real x0
      double precision ac(Tmc),ac_average,ac_error,cv(Tmc),cv_average,
     &  cv_error,lin(Tmc),lin_average,lin_error,susc(Tmc),susc_average,
     &  susc_error,ac2(Tmc),ac2_av,ac2_er,ac4(Tmc),ac4_av,ac4_er,binder,
     &  binder_e
      
c..........initialization of random number generator...........
      
      idum=-148175
      x0=0.0
      idum=idum-2*int(secnds(x0))
      
c.............parameters..................
      
      lambda_l=0.5d0
      do kk=0,15
         mu0_sq_l=-1.5d0+kk*0.1d0
         kappa=dsqrt(8.0d0*lambda_l+(4.0d0+mu0_sq_l)*(4.0d0+mu0_sq_l))
         kappa=kappa/(4.0d0*lambda_l)
         kappa=kappa-(4.0d0+mu0_sq_l)/(4.0d0*lambda_l)
         g=kappa*kappa*lambda_l
         
c.............initialization of phi and P.....
         
         inn=1.0d0
         call hot(N,idum,inn,phi,P)  
         
c.......parameters of molecular dynamics...........
         
         time=10
         dt=0.01d0
         
c................accepts including flips, rejects and the acceptance rate pa...............
         Reject=0.0d0
         Accept=0.0d0
         pa=0.0d0
         
c.....the acceptance rate is fixed in [0.7,0.9] such that dt is in [0.0001,1]....
         
         target_pa_high=0.90d0
         target_pa_low=0.70d0
         dt_max=1.0d0
         dt_min=0.0001d0
         inc=1.2d0
         dec=0.8d0
         nn=1
         
c...............thermalization......
         
         do ith=1,Tth
            call metropolis(time,dt,N,kappa,g,idum,accept,reject,
     &           variationH,P,phi)
            call adjust_inn(cou,pa,dt,time,Reject,Accept,
     &           nn,target_pa_high,target_pa_low,dt_max,dt_min,inc,dec)  
            call action(N,kappa,g,P,phi,mass,linear,kinetic,potential,
     &           act,Ham,quartic)

Codes
243
File: /home/ydri/Desktop/TP_QFT/codes/phi-four-on-lattice.f
Page 2 of 7
            write(9+kk,*) ith,act,Ham,variationH,pa,dt
         enddo
         
c..........Monte Carlo evolution.....
         
         do imc=1,Tmc
            do ico=1,Tco
               call metropolis(time,dt,N,kappa,g,idum,accept,reject,
     &              variationH,P,phi)
               call adjust_inn(cou,pa,dt,time,Reject,Accept,
     &            nn,target_pa_high,target_pa_low,dt_max,dt_min,inc,dec) 
            enddo
            call action(N,kappa,g,P,phi,mass,linear,kinetic,potential,
     &           act,Ham,quartic)
            ac(imc)=act
            lin(imc)=dabs(linear)
            ac2(imc)=linear*linear 
            ac4(imc)=linear*linear*linear*linear 
            write(9+kk,*) imc+Tth,act,Ham,variationH,pa,dt
         enddo
c....................observables........................
         
c.................action..................................
         call jackknife_binning(Tmc,ac,ac_average,ac_error)
         write(50,*)mu0_sq_l,lambda_l,kappa,g,ac_average,ac_error
c.................specific heat..................................
         do imc=1,Tmc
            cv(imc)=ac(imc)-ac_average
            cv(imc)=cv(imc)**(2.0d0)
         enddo
         call jackknife_binning(Tmc,cv,cv_average,cv_error)
         write(60,*)mu0_sq_l,lambda_l,kappa,g,cv_average,cv_error
c...............magnetization....................................
         call jackknife_binning(Tmc,lin,lin_average,lin_error)
         write(70,*)mu0_sq_l,lambda_l,kappa,g,lin_average,lin_error
c...............susceptibility...............................
         do imc=1,Tmc
            susc(imc)=lin(imc)-lin_average
            susc(imc)=susc(imc)**(2.0d0)
         enddo
         call jackknife_binning(Tmc,susc,susc_average,susc_error)
         write(80,*)mu0_sq_l,lambda_l,kappa,g,susc_average,susc_error
c...............Binder cumulant...........................
         call jackknife_binning(Tmc,ac2,ac2_av,ac2_er)
         write(81,*)mu0_sq_l,lambda_l,kappa,g,ac2_av,ac2_er
         call jackknife_binning(Tmc,ac4,ac4_av,ac4_er)
         write(82,*)mu0_sq_l,lambda_l,kappa,g,ac4_av,ac4_er         
         binder=1.0d0-ac4_av/(3.0d0*ac2_av*ac2_av)
         binder_e=-ac4_er/(3.0d0*ac2_av*ac2_av)
     &        +2.0d0*ac4_av*ac2_er/(3.0d0*ac2_av*ac2_av*ac2_av)
         write(90,*)mu0_sq_l,lambda_l,kappa,g,binder,binder_e
      enddo
      
      return
      end
      
      subroutine metropolis(time,dt,N,kappa,g,idum,accept,reject,
     &     variationH,P,phi)
      implicit none
      integer time,N,idum
      double precision dt,kappa,g,accept,reject,P(N,N),phi(N,N),
     &     variationH,P0(N,N),phi0(N,N),r,ran2,probability
      double precision mass,linear,kinetic,potential,act,Ham,quartic
      
      call gaussian(N,idum,P)
      P0=P

244
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/phi-four-on-lattice.f
Page 3 of 7
      phi0=phi
      
      call action(N,kappa,g,P,phi,mass,linear,kinetic,potential,act,Ham,
     &     quartic)
      variationH=-Ham
      call leap_frog(time,dt,N,kappa,g,P,phi)
      call action(N,kappa,g,P,phi,mass,linear,kinetic,potential,act,Ham,
     &     quartic)
      variationH=variationH+Ham
      
      if (variationH.lt.0.0d0)then
         accept=accept+1.0d0
      else
         probability=dexp(-variationH)
         r=ran2(idum)
         if (r.lt.probability)then
            accept=accept+1.0d0
         else
            P=P0
            phi=phi0
            reject=reject+1.0d0
         endif
      endif
      
      return
      end    
      
      subroutine gaussian(N,idum,P)
      implicit none
      integer N,i,j,idum
      double precision P(N,N),ph,r,pi,ran2
      
      pi=dacos(-1.0d0)
      do i=1,N
         do j=1,N
            r=dsqrt(-2.0d0*dlog(1.0d0-ran2(idum)))
            ph=2.0d0*pi*ran2(idum)
            P(i,j)=r*dcos(ph)
         enddo
      enddo
      
      return
      end
      subroutine leap_frog(time,dt,N,kappa,g,P,phi)
      implicit none
      integer time,N,nn,i,j
      double precision kappa,g,phi(N,N),P(N,N),force(N,N),dt
      
      do nn=1,time         
         call scalar_force(N,phi,kappa,g,force)         
         do i=1,N
            do j=1,N
               P(i,j)=P(i,j)-0.5d0*dt*force(i,j)
               phi(i,j)=phi(i,j)+dt*P(i,j)
            enddo
         enddo          
         call scalar_force(N,phi,kappa,g,force)         
         do i=1,N
            do j=1,N
               P(i,j)=P(i,j)-0.5d0*dt*force(i,j)
            enddo
         enddo         
      enddo
      
      return
      end

Codes
245
File: /home/ydri/Desktop/TP_QFT/codes/phi-four-on-lattice.f
Page 4 of 7
            
      subroutine scalar_force(N,phi,kappa,g,force)
      implicit none
      integer N,i,j,ip(N),im(N) 
      double precision phi(N,N),kappa,g,force(N,N)
      double precision force1,force2,force3
      
      call ipp(N,ip)
      call imm(N,im)
      do i=1,N
         do j=1,N
            force1=phi(ip(i),j)+phi(im(i),j)+phi(i,ip(j))+phi(i,im(j))
            force1=-2.0d0*kappa*force1            
            force2=2.0d0*phi(i,j)            
            force3=phi(i,j)*(phi(i,j)*phi(i,j)-1.0d0)
            force3=4.0d0*g*force3            
            force(i,j)=force1+force2+force3
         enddo
      enddo
            
      return
      end
      subroutine action(N,kappa,g,P,phi,mass,linear,kinetic,potential,
     &     act,Ham,quartic)
      implicit none
      integer N,i,j,ip(N) 
      double precision kappa,g
      double precision phi(N,N),P(N,N),act,potential,mass,kinetic,
     &     kineticH,ham,linear,quartic
      
      call ipp(N,ip)
      kinetic=0.0d0      
      mass=0.0d0
      kineticH=0.0d0
      potential=0.0d0
      linear=0.0d0
      quartic=0.0d0
      do i=1,N
         do j=1,N
            linear=linear+phi(i,j)
            quartic=quartic+phi(i,j)*phi(i,j)*phi(i,j)*phi(i,j)
            kinetic=kinetic+phi(i,j)*(phi(ip(i),j)+phi(i,ip(j)))
            mass=mass+phi(i,j)*phi(i,j)
            potential=potential
     &           +(phi(i,j)*phi(i,j)-1.0d0)*(phi(i,j)*phi(i,j)-1.0d0)
            kineticH=kineticH+P(i,j)*P(i,j)
         enddo
      enddo
      kinetic=-2.0d0*kappa*kinetic
      potential=g*potential
      act=kinetic+mass+potential
      kineticH=0.5d0*kineticH
      ham=kineticH+act  
            
      return
      end
      
      subroutine ipp(N,ip)
      implicit none
      integer ip(N),i,N
      
      do i=1,N-1
         ip(i)=i+1
      enddo
      ip(N)=1
      

246
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/phi-four-on-lattice.f
Page 5 of 7
      return
      end
      
      subroutine imm(N,im)
      implicit none
      integer im(N),i,N
      
      do i=2,N
         im(i)=i-1
      enddo
      im(1)=N
      
      return
      end
c........the jackknife estimator..................
      subroutine jackknife_binning(TMC,f,average,error)
      implicit none
      integer i,j,TMC,zbin,nbin
      double precision xm
      double precision f(1:TMC),sumf,y(1:TMC)
      double precision sig0,sig,error,average
      
      sig0=0.0d0
      sumf=0.0d0
      do i=1,TMC
         sumf=sumf+f(i)
      enddo
      xm=sumf/TMC 
c     do zbin=1,TMC-1
      zbin=1               
      nbin=int(TMC/zbin)
      sig=0.0d0
      do i=1,nbin,1
         y(i)=sumf
         do j=1,zbin
            y(i)=y(i)-f((i-1)*zbin+j )
         enddo
         y(i)= y(i)/(TMC-zbin)
         sig=sig+((nbin-1.0d0)/nbin)*(y(i)-xm)*(y(i)-xm)
      enddo
      sig=sig
      sig=dsqrt(sig)
      if (sig0 .lt. sig) sig0=sig
c     enddo         
      error=sig0
      average=xm
      
      return
      end
      
c.............the random number generator ran2.........
      
      function ran2(idum)
      implicit none
      integer idum,IM1,IM2,IMM1,IA1,IA2,IQ1,IQ2,IR1,IR2,NTAB,NDIV
      real AM,EPS,RNMX
      double precision ran2
      parameter (IM1=2147483563,IM2=2147483399,AM=1./IM1,IMM1=IM1-1,
     &     IA1=40014,IA2=40692,IQ1=53668,IQ2=52774,IR1=12211,
     &     IR2=3791,NTAB=32,NDIV=1+IMM1/NTAB,EPS=1.2E-7,RNMX=1.-EPS)
      integer idum2,j,k,iv(NTAB),iy
      SAVE iv,iy,idum2
      DATA idum2/123456789/,iv/NTAB*0/,iy/0/
      
      if (idum.le.0) then

Codes
247
File: /home/ydri/Desktop/TP_QFT/codes/phi-four-on-lattice.f
Page 6 of 7
         idum=max(-idum,1)
         idum2=idum
         do j=NTAB+8,1,-1
            k=idum/IQ1
            idum=IA1*(idum-k*IQ1)-k*IR1
            if (idum.lt.0) idum=idum+IM1
            if (j.le.NTAB) iv(j)=idum
         enddo
         iy=iv(1)
      endif
      k=idum/IQ1
      idum=IA1*(idum-k*IQ1)-k*IR1
      if (idum.lt.0) idum=idum+IM1
      k=idum2/IQ2
      idum2=IA2*(idum2-k*IQ2)-k*IR2
      if (idum2.lt.0) idum2=idum2+IM2
      j=1+iy/NDIV
      iy=iv(j)-idum2
      iv(j)=idum
      if (iy.lt.1) iy=iy+IMM1
      ran2=min(AM*iy,RNMX)
      
      return
      end
c........hot start...................
      
      subroutine hot(N,idum,inn,phi,P) 
      implicit none
      integer lambda,i,j,N,idum
      double precision phi(N,N),P(N,N)
      double precision inn,interval
            
      do i=1,N
         do j=1,N
            phi(i,j)=interval(idum,inn)
            P(i,j)=interval(idum,inn)
         enddo
      enddo
           
      return
      end
c.........adjusting interval..................        
      
      subroutine adjust_inn(cou,pa,dt,time,Reject,Accept,
     &     nn,target_pa_high,target_pa_low,dt_max,dt_min,inc,dec)  
      implicit none  
      double precision dt,pa,Reject,Accept
      integer time,cou,cou1
      integer nn
      double precision target_pa_high,target_pa_low,dt_max,dt_min,inc,
     &     dec,rho1,rho2,dtnew
      
c.....pa acceptance rate............
      pa=(Accept)/(Reject+Accept)        
      cou1=mod(cou,nn)        
      if (cou1.eq.0)then
c........fixing the acceptance rate between 90 % 70 %..................
         if (pa.ge.target_pa_high) then
            dtnew=dt*inc
            if (dtnew.le.dt_max)then
               dt=dtnew
            else
               dt=dt_max
            endif
         endif

248
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/phi-four-on-lattice.f
Page 7 of 7
         if (pa.le.target_pa_low) then
            dtnew=dt*dec
            if (dtnew.ge.dt_min)then
               dt=dtnew
            else
               dt=dt_min
            endif
         endif
      endif
      
      return
      end
      
c.............interval..............
      
      function interval(idum,inn)
      implicit none
      double precision interval,inn,ran2
      integer idum
      
      interval=ran2(idum)
      interval=interval+interval-1.0d0
      interval=interval*inn
      
      return
      end
      

Codes
249
File: /home/ydri/Desktop/TP_QFT/cod…metropolis-scalar-multitrace.f
Page 1 of 7
      program my_metropolis_scalar_multitrace
      implicit none
      integer N,i,k,idum,ither,Tther,imont,ico,tmo,Tmont,Tco,counter,
     &     Pow1,Pow2,Pow3
      parameter (N=10)
      parameter (pow1=20,pow2=20,pow3=5)
      parameter (Tther=2**pow1,Tmont=2**pow2,Tco=2**pow3)
      double precision a,b,c,d,g,at,bt,ct,eta,v22,v41,v21,ap,bp,cp,dp,e,
     &     ep,fp  
      double precision ran2,inn,interval,accept,reject,pa,t_1,t_2 
      double precision lambda(N) 
      double precision actio,actio0,sum1,sum2,sum4,sumv,actio1,actio2,
     &     actio4
      double precision ac(Tmont),ac_average,ac_error      
      double precision id,ide(Tmont),ide_average,ide_error
      double precision cv(Tmont),cv_average,cv_error
      double precision va(Tmont),va_average,va_error
      double precision p0(Tmont),p0_average,p0_error
      double precision pt(Tmont),pt_average,pt_error
      double precision p4(Tmont),p4_average,p4_error
      double precision su(Tmont),su_average,su_error
      double precision sus(Tmont),sus_average,sus_error
      real x0
      
      call cpu_time(t_1)
      
c...........initialization of the random number generator........
      
      idum=-148175
      x0=0.0
      idum=idum-2*int(secnds(x0))
      
c............parameters of the model..................  
    
c............kinetic parameter:the pure quartic matrix model is obtained by setting at=0............
      at=1.0d0
      a=at/dsqrt(1.0d0*N)
c.........Seamann's values..................
      v21=-1.0d0
      v22=0.0d0
      v41=1.5d0
c.........Ydri's proposal....................
c     v21=1.0d0
c     v22=1.0d0/8.0d0
c     v41=0.0d0
c...........principal multitrace coupling........................
      eta=v22-0.75d0*v41
      d=-2.0d0*eta*at*at*N
      d=d/3.0d0
      e=d
c..........further multitrace couplings (odd terms).................
      ap=4.0d0*at*at*v22/3.0d0
      dp=-2.0d0*at*at*v22/3.0d0
      dp=dp/N
      cp=-2.0d0*at*at*N*v41/3.0d0
      bp=-at*dsqrt(1.0d0*N)*v21/2.0d0  
c.......ep and fp are included in c and b respectively....       
      ep=at*at*N*N*v41/6.0d0
      fp=at*N*dsqrt(1.0d0*N)*v21/2.0d0
c............quartic parameter: here c is C=c+ep of note..........................
      ct=1.0d0
      c=N*N*ct
c...........mass parameter: here b is B=b+fp of note...................      
      do k=0,0
         bt=-5.0d0+k*0.1d0
         b=N*dsqrt(1.0d0*N)*bt
c......the parameters b and c in terms of g: the single parameter of the quartic matrix model........

250
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/cod…metropolis-scalar-multitrace.f
Page 2 of 7
c     g=1.0d0
c     b=-N/g
c     c=N      
c     c=c/(4.0d0*g)  
         
c...................initialization of the eigenvalues...   
         
         inn=1.0d0
         do i=1,N
            lambda(i)=interval(idum,inn)           
         enddo
         
c................accepts including flips, rejects and the acceptance rate pa...............
         
         Reject=0.0d0
         Accept=0.0d0
         pa=0.0d0
                       
c.........thermalization.........................................................
         
         do ither=1,Tther
            call standard_metropolis(N,ap,b,bp,c,cp,d,dp,ep,fp,lambda,
     &           accept,reject,idum,inn,pa)
            call action(N,ap,b,bp,c,cp,d,dp,ep,fp,lambda,actio,actio0,
     &           sum1,sum2,sum4,sumv,id,actio1,actio2,actio4)           
            write(*,*)ither,actio0,actio,dabs(sum1),sum2,sum4,id,pa,inn
            write(7,*)ither,actio0,actio,dabs(sum1),sum2,sum4,sumv,id
     &           ,pa,inn   
         enddo
         
c.......monte carlo evolution..................
         counter=0
         do imont=1,Tmont
            
c........removing auto-correlations by separating data points by tco monte carlo setps................
            do ico=1,Tco
               call standard_metropolis(N,ap,b,bp,c,cp,d,dp,ep,fp,lambda
     &              ,accept,reject,idum,inn,pa)
            enddo
c..........construction of thermalized observables......................................
            
call action(N,ap,b,bp,c,cp,d,dp,ep,fp,lambda,actio,actio0,
     &           sum1,sum2,sum4,sumv,id,actio1,actio2,actio4) 
c     if ((id.ge.0.8d0).and.(id.le.1.2d0))then
            counter=counter+1                
            ac(counter)=actio0+actio1
            ide(counter)=id
            va(counter)=sumv
            su(counter)=dabs(sum1)
            p0(counter)=sum1*sum1/(N*N)
            pt(counter)=sum2/N
            p4(counter)=sum4
            write(*,*)imont,counter,sum2,sum4,id
            write(8,*)imont,counter,sum2,sum4,id
c....................eigenvalues........................
            write(150+k,*)counter,lambda
c     endif 
         enddo
         
c...............measurements............
         Tmo=counter
c................action and vandermonde...................       
         call jackknife_binning(Tmo,ac,ac_average,ac_error)

Codes
251
File: /home/ydri/Desktop/TP_QFT/cod…metropolis-scalar-multitrace.f
Page 3 of 7
         write(10,*)bt,ct,d,ac_average,ac_error            
         call jackknife_binning(Tmo,va,va_average,va_error)
         write(11,*)bt,ct,d,va_average,va_error
c..................identity.................
         call jackknife_binning(Tmo,ide,ide_average,ide_error)            
         write(12,*)bt,ct,d,ide_average,ide_error
         write(*,*)bt,ct,d,ide_average,ide_error, "identity"
c............power in zero modes, total power and quartic term.............
         call jackknife_binning(Tmo,p0,p0_average,p0_error)
         write(13,*)bt,ct,d,p0_average,p0_error
         call jackknife_binning(Tmo,pt,pt_average,pt_error)
         write(14,*)bt,ct,d,pt_average,pt_error
         write(*,*)bt,ct,d,pt_average,pt_error, "total power"
         call jackknife_binning(Tmo,p4,p4_average,p4_error)
         write(15,*)bt,ct,d,p4_average,p4_error
c.......magnetization and susceptibility..............            
         call jackknife_binning(Tmo,su,su_average,su_error)
         write(16,*)bt,ct,d,su_average,su_error            
         do i=1,Tmo
            sus(i)= (su(i)-su_average)*(su(i)-su_average)
         enddo
         call jackknife_binning(Tmo,sus,sus_average,sus_error)
         write(17,*)bt,ct,d,sus_average,sus_error                  
c..................specific heat....................
         do i=1,Tmo
            cv(i)=(ac(i)-ac_average)**2
         enddo            
         call jackknife_binning(Tmo,cv,cv_average,cv_error)
         write(20,*)bt,ct,d,cv_average,cv_error      
      enddo
      
c..........cpu time and detail of simulation.......................
      call cpu_time(t_2)
      write(99,*)N,d,bt,ct,tmont,tmo,tco,tther,t_2-t_1
      
      return
      end
      
c.............metropolis algorithm...........................
      
      subroutine standard_metropolis(N,ap,b,bp,c,cp,d,dp,ep,fp,lambda
     &     ,accept,reject,idum,inn,pa)
      implicit none
integer N,i,idum
      double precision lambda(N),var,pro,r,b,c,d,accept,reject,ran2,
     &     h,inn,interval,pa,ap,bp,cp,dp,ep,fp
      
      do i=1,N
c...........variation of the action....................
         h=interval(idum,inn)
         call variation(N,ap,b,bp,c,cp,d,dp,ep,fp,i,h,lambda,Var)
c............metropolis accept-reject step..........................
         if(var.gt.0.0d0)then         
            pro=dexp(-var)
            r=ran2(idum)
            if (r.lt.pro) then    
               lambda(i)=lambda(i)+h
               accept=accept+1.0d0
            else
               reject=reject+1.0d0          
            endif
         else
            lambda(i)=lambda(i)+h
            accept=accept+1.0d0
         endif       
      enddo      
c............adjusting the interval inn................

252
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/cod…metropolis-scalar-multitrace.f
Page 4 of 7
      call adjust_inn(pa,inn,Reject,Accept)
      
      return
      end
c.....................variation of the action............
      
      subroutine variation(N,ap,b,bp,c,cp,d,dp,ep,fp,i,h,lambda,Var)
      implicit none
      integer N,i,k
      double precision lambda(N),var,b,c,d,h,ap,bp,cp,dp,ep,fp
      double precision dsum2,dsum4,sum2,dvand,dd,dvande
      double precision sum1,sum3,var1,var2,var3,var4
      
      dsum2=h*h+2.0d0*h*lambda(i)
      dsum4=6.0d0*h*h*lambda(i)*lambda(i)
     &     +4.0d0*h*lambda(i)*lambda(i)*lambda(i)+4.0d0*h*h*h*lambda(i)
     &     +h*h*h*h
      sum3=0.0d0
      sum2=0.0d0
      sum1=0.0d0
      do k=1,N    
         sum3=sum3+lambda(k)*lambda(k)*lambda(k)  
         sum2=sum2+lambda(k)*lambda(k)
         sum1=sum1+lambda(k)
      enddo       
      dvand=0.0d0
      do k=i+1,N
         dd=1.0d0
         dd=dd+h/(lambda(i)-lambda(k))
         dd=dabs(dd)
         dvand=dvand+dlog(dd)
      enddo        
      dvand=-dvand
      dvande=0.0d0
      do k=1,i-1
         dd=1.0d0
         dd=dd+h/(lambda(i)-lambda(k))
         dd=dabs(dd)
         dvande=dvande+dlog(dd)
      enddo        
      dvande=-dvande
      dvand=dvand+dvande
      dvand=2.0d0*dvand
      var=b*dsum2+c*dsum4+2.0d0*d*dsum2*sum2+d*dsum2*dsum2+dvand         
      var1=h*h+2.0d0*h*sum1
      var4=var1*var1+2.0d0*sum1*sum1*var1
      var1=bp*var1
      var4=dp*var4
      var2=h*sum2+(sum1+h)*dsum2
      var2=ap*var2
      var3=3.0d0*h*lambda(i)*lambda(i)+3.0d0*h*h*lambda(i)+h*h*h
      var3=var3*(sum1+h)
      var3=var3+h*sum3
      var3=cp*var3        
      var=var+var1+var2+var3+var4
      
      return
      end
      
c..............action.......................................
      
      subroutine action(N,ap,b,bp,c,cp,d,dp,ep,fp,lambda,actio,actio0,
     &     sum1,sum2,sum4,sumv,id,actio1,actio2,actio4)
      implicit none
      integer N,i,j
      double precision lambda(N),b,c,d,actio,actio0,sum1,sum2,sum4,sumv,

Codes
253
File: /home/ydri/Desktop/TP_QFT/cod…metropolis-scalar-multitrace.f
Page 5 of 7
     &     id
      double precision sum3,actio1,ap,bp,cp,dp,id1,ep,fp,actio2,actio4
c.............monomial terms............
      sum1=0.0d0
      sum2=0.0d0
      sum3=0.0d0
      sum4=0.0d0
      do i=1,N
         sum1=sum1+lambda(i)
         sum2=sum2+lambda(i)*lambda(i)
         sum3=sum3+lambda(i)*lambda(i)*lambda(i)
         sum4=sum4+lambda(i)*lambda(i)*lambda(i)*lambda(i)
      enddo     
c.......the multitrace model without odd terms..........
      actio0=d*sum2*sum2+b*sum2+c*sum4 
      actio=actio0    
c............odd multitrace terms
      actio1=bp*sum1*sum1+cp*sum1*sum3+dp*sum1*sum1*sum1*sum1
     &     +ap*sum2*sum1*sum1
c...........the multitrace model with odd terms........
      actio=actio+actio1
c........adding the vandrmonde potential..............
      sumv=0.0d0
      do i=1,N         
         do j=1,N
            if (i.ne.j)then
               sumv=sumv+dlog(dabs(lambda(i)-lambda(j)))
            endif
         enddo
      enddo
      sumv=-sumv
      actio=actio+sumv  
c..........the quadratic and quartic corrections explicitly....
      actio2=fp*sum2+bp*sum1*sum1
      actio4=ep*sum4+d*sum2*sum2+cp*sum1*sum3+dp*sum1*sum1*sum1*sum1
     &     +ap*sum2*sum1*sum1     
c...........the schwinger-dyson identity.................
      id=4.0d0*d*sum2*sum2+2.0d0*b*sum2+4.0d0*c*sum4
      id1=2.0d0*bp*sum1*sum1+4.0d0*(cp*sum1*sum3+dp*sum1*sum1*sum1*sum1
     &     +ap*sum2*sum1*sum1)
      id=id+id1
      id=id/(N*N)
      return
      end
      
c........the jackknife estimator..................
      
      subroutine jackknife_binning(TMC,f,average,error)
      implicit none
      integer i,j,TMC,zbin,nbin
      double precision xm
      double precision f(1:TMC),sumf,y(1:TMC)
      double precision sig0,sig,error,average
      
      sig0=0.0d0
      sumf=0.0d0
      do i=1,TMC
         sumf=sumf+f(i)
      enddo
      xm=sumf/TMC 
c     do zbin=1,TMC-1
      zbin=1               
      nbin=int(TMC/zbin)
      sig=0.0d0
      do i=1,nbin,1

254
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/cod…metropolis-scalar-multitrace.f
Page 6 of 7
         y(i)=sumf
         do j=1,zbin
            y(i)=y(i)-f((i-1)*zbin+j )
         enddo
         y(i)= y(i)/(TMC-zbin)
         sig=sig+((nbin-1.0d0)/nbin)*(y(i)-xm)*(y(i)-xm)
      enddo
      sig=sig
      sig=dsqrt(sig)
      if (sig0 .lt. sig) sig0=sig
c     enddo         
      error=sig0
      average=xm
      
      return
      end
      
c.............the random number generator ran2.........
      
      function ran2(idum)
      implicit none
      integer idum,IM1,IM2,IMM1,IA1,IA2,IQ1,IQ2,IR1,IR2,NTAB,NDIV
      real AM,EPS,RNMX
      double precision ran2
      parameter (IM1=2147483563,IM2=2147483399,AM=1./IM1,IMM1=IM1-1,
     &     IA1=40014,IA2=40692,IQ1=53668,IQ2=52774,IR1=12211,
     &     IR2=3791,NTAB=32,NDIV=1+IMM1/NTAB,EPS=1.2E-7,RNMX=1.-EPS)
      integer idum2,j,k,iv(NTAB),iy
      SAVE iv,iy,idum2
      DATA idum2/123456789/,iv/NTAB*0/,iy/0/
      
      if (idum.le.0) then
         idum=max(-idum,1)
         idum2=idum
         do j=NTAB+8,1,-1
            k=idum/IQ1
            idum=IA1*(idum-k*IQ1)-k*IR1
            if (idum.lt.0) idum=idum+IM1
            if (j.le.NTAB) iv(j)=idum
         enddo
         iy=iv(1)
      endif
      k=idum/IQ1
      idum=IA1*(idum-k*IQ1)-k*IR1
      if (idum.lt.0) idum=idum+IM1
      k=idum2/IQ2
      idum2=IA2*(idum2-k*IQ2)-k*IR2
      if (idum2.lt.0) idum2=idum2+IM2
      j=1+iy/NDIV
      iy=iv(j)-idum2
      iv(j)=idum
      if (iy.lt.1) iy=iy+IMM1
      ran2=min(AM*iy,RNMX)
      
      return
      end
c.........adjusting interval inn in such a way that the acceptance rate pa is fixed at 30 per 
cent..................        
      
      subroutine adjust_inn(pa,inn,Reject,Accept)    
      implicit none
      double precision inn,pa,Reject,Accept
      
      pa=(Accept)/(Reject+Accept)
      if (pa.ge.0.30) inn=inn*1.20d0
      if (pa.le.0.25) inn=inn*0.80d0

Codes
255
File: /home/ydri/Desktop/TP_QFT/cod…metropolis-scalar-multitrace.f
Page 7 of 7
      
      return
      end
c.............the interval....................................      
      function interval(idum,inn)
      implicit none
      doubleprecision interval,inn,ran2
      integer idum
      
      interval=ran2(idum)
      interval=interval+interval-1.0d0
      interval=interval*inn
      
      return
      end

256
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/remez.f
Page 1 of 2
      program my_remez
      implicit none
      integer y,z,n,d,precision,i,counter,j,n0
      parameter(n0=100)
      double precision lambda_low, lambda_high,e,tolerance
      double precision a0,a(n0),b(n0),c0,c(n0),dd(n0),coefficient(n0)
      parameter (tolerance=0.0001d0)
      character*100 degree, com
      character*50 h1
      LOGICAL THERE 
c........we choose the function to approximate, the range over which the rational approximation is to be 
calculated, and the precision used.... 
      
      y=1
      z=2
      lambda_low=0.0004d0
      lambda_high=1.0d0
      precision=40
      print*, "Approximating the functions x^{y/z} and x^{-y/z}:"
     &     , "y=",y,"z=",z
      print*, "Approximation bounds:", lambda_low,lambda_high
      print*, "Precision of arithmetic:", precision
      write(*,*)"..................."
c.... we start the iteration on the degree of approximation at n=d=6....
      counter=0
      i=5
14   i=i+1
      counter=counter+1
      print*, "ITERATION:",counter     
      write(degree,'("", I1 )')i
      read(degree,'(i5)')n
      read(degree,'(i5)')d
      write(*,*)"degrees of approximation", n,d
      
c.........we call AlgRemez by the command="./test y z n d lambda_low lambda_high precision".....
      write(com,'(a,i5," ",i5," ",i5," ",i5," ",F10.5," ",F10.5," "  
     &,i5," ",a)') "./test ",y,z,d,n,lambda_low,lambda_high
     &     ,precision,""
      print*, "command:", com        
call system(com)
           
c........we check whether or not the uniform norm is found.......................
      inquire(file='error1.dat', exist=THERE)
11   if ( THERE ) then
         write(*,*) "file exists!"
      else
         go to  11
      end if
      
c......we read the uniform norm and test whether or not it is smaller than some tolerance, if it is not, 
we go back and repeat with  increased degrees of approximation, viz n=n+1 and d=d+1.............
      open(unit=50+i,file='error1.dat',status='old')
      read(50+i,555) e
      write(*,*)"uniform norm", e
      write(*,*)"..................."
555  format(1F20.10)
      close(50+i)
      if (e.gt.tolerance) go to 14
c..............the solution for x^{y/z}..............................................................
     

Codes
257
File: /home/ydri/Desktop/TP_QFT/codes/remez.f
Page 2 of 2
      write(*,*)"rational approximation of x^{y/z}"
      open(unit=60,file='approx.dat',status='old')      
      do j=1,2*n+1
         read(60,*)coefficient(j)
      enddo      
      c0=coefficient(1)
      write(*,*)"c0=",c0
      do i=2,n+1
         c(i-1)=coefficient(i)
         dd(i-1)=coefficient(i+n)
         write(*,*)"i-1=",i-1,"c(i-1)=", c(i-1),"d(i-1)=",dd(i-1)
      enddo
  
c..................the solution for x^{-y/z}.........................................................   
      write(*,*)"rational approximation of x^{-y/z}"
      open(unit=61,file='approx1.dat',status='old')
  
      do j=1,2*n+1
         read(61,*)coefficient(j)
      enddo         
      a0=coefficient(1)
      write(*,*)"a0=",a0
      do i=2,n+1
         a(i-1)=coefficient(i)
         b(i-1)=coefficient(i+n)
         write(*,*)"i-1=",i-1,"a(i-1)=", a(i-1),"b(i-1)=",b(i-1)
      enddo
      
      return
      end

258
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/conjugate-gradient.f
Page 1 of 3
      program my_conjugate_gradient
      implicit none
      integer N,M,i,j,counter,sig
      parameter (N=3,M=2)
      double precision A(N,N),v(N),sigma(M)
      double precision x(N),r(N),p(N),q(N),product,product1,product2,
     &     residue,tolerance
      double precision  alpha,beta,alpha_previous,beta_previous,xii,xii0,
     &     beta_sigma(M),alpha_sigma(M),xi(M),xi_previous(M)
      double precision x_sigma(N,M),p_sigma(N,M),r_sigma(N,M)
      parameter(tolerance=10.0d-100)
      
c............example input...........................
      
      call input(N,M,A,v,sigma)
c..............initialization.................................................................
      
      do i=1,N
         x(i)=0.0d0
         r(i)=v(i)
         do sig=1,M
            x_sigma(i,sig)=0.0d0         
         enddo
      enddo
      
c.............we start with alpha(0)=0, beta(-1)=1, xi^sigma(-1)=xi^sigma(0)=1, alpha^sigma(0)=0 and 
beta^sigma(-1)=1...
      
      alpha=0.0d0
      beta=1.0d0
      do sig=1,M         
         xi_previous(sig)=1.0d0   
         xi(sig)=1.0d0
         alpha_sigma(sig)=0.0d0
         beta_sigma(sig)=1.0d0      
      enddo
      
c.............starting iteration.........
      counter=0
c...............choosing search directions................
13   do i=1,N
         p(i)=r(i)+alpha*p(i)
         do sig=1,M
            p_sigma(i,sig)=xi(sig)*r(i)
     &           +alpha_sigma(sig)*p_sigma(i,sig)
         enddo
      enddo
c.......solving the sigma=0 problem.........
      product=0.0d0
      product1=0.0d0
c.......the only matrix-vector multiplication in the problem..........
      do i=1,N
         q(i)=0.0d0
         do j=1,N
            q(i)=q(i)+A(i,j)*p(j)
         enddo
         product=product+p(i)*q(i)
         product1=product1+r(i)*r(i)
      enddo
      beta_previous=beta
      beta=-product1/product

Codes
259
File: /home/ydri/Desktop/TP_QFT/codes/conjugate-gradient.f
Page 2 of 3
      
      product2=0.0d0
      do i=1,N
         x(i)=x(i)-beta*p(i)
         r(i)=r(i)+beta*q(i)
         product2=product2+r(i)*r(i)
      enddo      
      alpha_previous=alpha
      alpha=product2/product1
c.......solving the sigma problems..............
      do sig=1,M
c......the xi coefficients..........
         xii0=alpha_previous*beta*(xi_previous(sig)-xi(sig))
     &        +xi_previous(sig)*beta_previous*(1.0d0-sigma(sig)*beta)
         xii=xi(sig)*xi_previous(sig)*beta_previous/xii0
         xi_previous(sig)=xi(sig)
         xi(sig)=xii
c........the beta coefficients......
         beta_sigma(sig)=beta*xi(sig)/xi_previous(sig)
c.........the solutions and residues...........
         do i=1,N
            x_sigma(i,sig)=x_sigma(i,sig)-beta_sigma(sig)*p_sigma(i,sig) 
            r_sigma(i,sig)=xi(sig)*r(i)
         enddo
c.......the alpha coefficients.......
         alpha_sigma(sig)=alpha
         alpha_sigma(sig)= alpha_sigma(sig)*xi(sig)*beta_sigma(sig)
         alpha_sigma(sig)=alpha_sigma(sig)/(xi_previous(sig)*beta)
      enddo
         
c......testing whether or not the interation should be continued........
      counter=counter+1
      residue=0.0d0
      do i=1,N
         residue=residue+r(i)*r(i)
      enddo
      residue=dsqrt(residue)
      if(residue.ge.tolerance)  go to 13
c........verification 1: if we set sigma=0 then xi must be equal 1 whereas the other pairs must be 
equal.........
      write(*,*)"verification 1"
      write(*,*)counter,xi(1),xi_previous(1)
      write(*,*)counter,beta,beta_sigma(1)
      write(*,*)counter,alpha,alpha_sigma(1)
c............verification 2.....
      write(*,*)"verification 2"
      do i=1,N
         q(i)=0.0d0
         do j=1,N
            q(i)=q(i)+A(i,j)*x(j)
         enddo
      enddo
      write(*,*)"v",v
      write(*,*)"q",q
        
c............verification 3.....
      write(*,*)"verification 3"
      sig=1
      do i=1,N
         q(i)=sigma(sig)*x_sigma(i,sig)
         do j=1,N
            q(i)=q(i)+A(i,j)*x_sigma(j,sig)

260
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/conjugate-gradient.f
Page 3 of 3
         enddo
      enddo
      write(*,*)"v",v
      write(*,*)"q",q
      return
      end
c................input.........................................
      subroutine input(N,M,A,v,sigma)
      implicit none
      integer N,M
      double precision A(N,N),v(N),sigma(M)
      
      a(1,1)=1.0d0
      a(1,2)=2.0d0
      a(1,3)=0.0d0
      a(2,1)=2.0d0
      a(2,2)=2.0d0
      a(2,3)=0.0d0
      a(3,1)=0.0d0
      a(3,2)=0.0d0
      a(3,3)=3.0d0
      v(1)=1.0d0
      v(2)=0.0d0
      v(3)=10.0d0
      sigma(1)=1.0d0
      sigma(2)=2.0d0
      
      return
      end

Codes
261
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 1 of 18
      program my_hybrid_susy_ym
      implicit none
      integer dim,N,M,M0,i,j,k,sp,A1,idum,time,timeT,tmc0,TMC,TTH,idum0,
     &     cou,nn
      parameter (dim=4,N=8,M0=5,M=6)
      parameter (timeT=2**14,TTH=2**11,TMC=2**13)
      double precision gamma,mass,alpha,zeta,alphat
      double precision a0,a(M),b(M),c0,c(M0),d(M0),coefficient(2*M+1)
     &     ,epsilon     
      double complex X(dim,N,N),P(dim,N,N),phi(2,N*N-1),Q(2,N*N-1),
     &     xx(2,N*N-1)
      double complex G(M,2,N*N-1),W(2,N*N-1),W0(2,N*N-1),xi(2,N*N-1)
      double precision inn,dt,interval, Rejec,Accept,pa
      double precision ham,action,actionB,actionF,kinB,kinF,
     &     variationH,YM,CS,HO,hamB,hamF
      real x0,t_1,t_2
      double complex var(dim,N,N),varF(dim,N,N)
      double precision varH0,varH(TMC),varH_average,varH_error
      double precision h(TMC),h_average,h_error
      double precision ac(TMC),ac_average,ac_error
      double precision ac_B(TMC),acB_average,acB_error
      double precision ac_F(TMC),acF_average,acF_error
      double precision ym0(TMC),ym_average,ym_error
      double precision cs0(TMC),cs_average,cs_error
      double precision ho0(TMC),ho_average,ho_error
      double precision identity_av,identity_er
      double precision target_pa_high,target_pa_low,dt_max,dt_min,inc,
     &     dec
      call cpu_time(t_1)
c............opening output files......................................................
      
      open(10, action='WRITE')
      close(10)
      open(11, action='WRITE')
      close(11)
      open(12, action='WRITE')
      close(12)
      open(13, action='WRITE')
      close(13)
      open(14, action='WRITE')
      close(14)
open(15, action='WRITE')
      close(15)
      open(16, action='WRITE')
      close(16)
      open(17, action='WRITE')
      close(17)
      open(18, action='WRITE')
      close(18)
c........calling output of AlgRemez: M, M_0, c,d,a,b...................................
     
c.........rational approximation of x^{1/4}.................................
      open(unit=60,file='approx_x**+0.25_dat',status='old')      
      do j=1,2*M0+1
         read(60,*)coefficient(j)
      enddo      
      c0=coefficient(1)
c     write(*,*)"c0=",c0
      do i=2,M0+1
         c(i-1)=coefficient(i)
         d(i-1)=coefficient(i+M0)
c     write(*,*)"i-1=",i-1,"c(i-1)=", c(i-1),"d(i-1)=",d(i-1)
      enddo      
c.........rational approximation of x^{-1/2}...................................    

262
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 2 of 18
      open(unit=60,file='approx_x**-0.5_dat',status='old')      
      do j=1,2*M+1
         read(60,*)coefficient(j)
      enddo      
      a0=coefficient(1)
c     write(*,*)"a0=",a0
      do i=2,M+1
         a(i-1)=coefficient(i)
         b(i-1)=coefficient(i+M)
c     write(*,*)"i-1=",i-1,"a(i-1)=", a(i-1),"b(i-1)=",b(i-1)
      enddo
      
c.....shifting the no sigma problem of the conjugate gradient to the smallest mass which is presumably 
the least convergent mass...       
      
      epsilon=b(1)
      if (epsilon.gt.d(1))then
         epsilon=d(1)
      endif
      do i=1,M
         b(i)=b(i)-epsilon
      enddo
      do i=1,M0
         d(i)=d(i)-epsilon
      enddo
      
c...................initialization of random number generator....................
      
      idum=-148175                 
      x0=0
      idum=idum-2*int(secnds(x0))
c.............parameters...............................................................................       
      zeta=0.0d0
      mass=0.0d0
      gamma=1.0d0   
      do k=0,0
         alphat=0.0d0-k*0.25d0
         alpha=alphat/dsqrt(1.0d0*N)
         
c.............initialization of X..............................................................
      
         inn=1.0d0
         call hot(N,dim,idum,inn,X)
c        call cold(N,dim,idum,X) 
      
c.............initialization of the other fields from Gaussian noise...........
      
c     call gaussian(idum,dim,N,P)
c     call gaussian_plus(idum,N,Q)
c     call gaussian_plus(idum,N,xi)      
c...............here we use the coefficients c and d not the coefficients a and b..............
c     call conjugate_gradient(dim,N,M0,zeta,X,c0,c,d,xi,G,phi,W,
c     &     epsilon)
      
c.............molecular dynamics parameters: dt should be optimized in such a way that the acceptance 
rate pa is fixed in [0.7,0.9] and dt is fixed in [0.0001,1]....
         
         time=10
         dt=0.001d0
         Rejec=0.0d0
         Accept=0.0d0
         target_pa_high=0.90d0
         target_pa_low=0.70d0
         dt_max=1.0d0
         dt_min=0.0001d0

Codes
263
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 3 of 18
         inc=1.2d0
         dec=0.8d0
         nn=1
         
c..........testing the molecular dynamics part.................
c     time=1
c     dt=0.001d0         
c     do tmc0=1,timeT
c     call molecular_dynamics(N,dim,M,dt,time,gamma,mass,alpha,
c     &           zeta,a0,a,b,X,P,phi,Q,var,varF,epsilon)
c     call sub_action(dim,N,M,a0,a,b,X,P,phi,Q,alpha,mass,gamma,zeta,
c     &           ham,action,actionB,actionF,kinB,kinF,YM,CS,HO,epsilon)
c     hamB=kinB+actionB
c     hamF=kinF+actionF
c     write(*,*)tmc0,ham,kinB,actionB,hamB,kinF,actionF,hamF
c     write(7,*)tmc0,ham,kinB,actionB,hamB,kinF,actionF,hamF
c     enddo
c.................thermalization..............................
         do tmc0=1,TTH
            call metropolis(N,dim,M,M0,gamma,mass,alpha,zeta,dt,time,X,
     &           P,phi,Q,a0,a,b,c0,c,d,Rejec,Accept,var,varF,variationH,
     &           epsilon,idum)
            cou=tmc0
            call adjust_inn(cou,pa,dt,time,Rejec,Accept,
     &           nn,target_pa_high,target_pa_low,dt_max,dt_min,inc,dec)             
            call sub_action(dim,N,M,a0,a,b,X,P,phi,Q,alpha,mass,gamma,
     &           zeta,ham,action,actionB,actionF,kinB,kinF,YM,CS,HO,
     &           epsilon)    
            varH0=dexp(-variationH)  
            write(*,*)tmc0,ham,action,actionB,kinB,actionF,kinF,
     &           variationH,varH0,pa
            write(8,*)tmc0,ham,action,actionB,kinB,actionF,kinF,
     &           variationH,varH0,pa
         enddo
c....................monte carlo evolution......................
         do tmc0=1,TMC
            call metropolis(N,dim,M,M0,gamma,mass,alpha,zeta,dt,time,X,
     &           P,phi,Q,a0,a,b,c0,c,d,Rejec,Accept,var,varF,variationH,
     &           epsilon,idum)
            cou=tmc0
            call adjust_inn(cou,pa,dt,time,Rejec,Accept,
     &           nn,target_pa_high,target_pa_low,dt_max,dt_min,inc,dec)             
            call sub_action(dim,N,M,a0,a,b,X,P,phi,Q,alpha,mass,gamma,
     &           zeta,ham,action,actionB,actionF,kinB,kinF,YM,CS,HO,
     &            epsilon)    
            ym0(tmc0)=YM
            cs0(tmc0)=CS
            ho0(tmc0)=HO
            ac_B(tmc0)=actionB
            ac_F(tmc0)=actionF
            ac(tmc0)=action
            h(tmc0)=ham
            varH(tmc0)=dexp(-variationH)  
            write(*,*)tmc0,ham,action,actionB,kinB,actionF,kinF,
     &           variationH, varH(tmc0),pa
            write(9,*)tmc0,ham,action,actionB,kinB,actionF,kinF,
     &           variationH,varH(tmc0),pa
         enddo
c.....................measurements......................................
c..................the Hamiltonian........................................

264
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 4 of 18
         call jackknife_binning(TMC,h,h_average,h_error)
c        write(*,*)alpha,gamma,mass,zeta,h_average,h_error
         open(10, status='OLD', action='WRITE', position='APPEND')
         write(10,*)alpha,gamma,mass,zeta,h_average,h_error
         close(10)   
c..................we msut have <e^(-variationH)>=1.................................  
         call jackknife_binning(TMC,varH,varH_average,varH_error)
c        write(*,*)alpha,gamma,mass,zeta,varH_average,varH_error
         open(11, status='OLD', action='WRITE', position='APPEND')
         write(11,*)alpha,gamma,mass,zeta,varH_average,varH_error
         close(11)
c...............the total action..................
         call jackknife_binning(TMC,ac,ac_average,ac_error)
c        write(*,*)alpha,gamma,mass,zeta,ac_average,ac_error
         open(12, status='OLD', action='WRITE', position='APPEND')
         write(12,*)alpha,gamma,mass,zeta,ac_average,ac_error
         close(12)
c..................the bosonic and pseudo-fermion actions and the yang-mills, chern-simons and harmonic 
oscillator terms ....
         call jackknife_binning(TMC,ac_B,acB_average,acB_error)
c        write(*,*)alpha,gamma,mass,zeta,acB_average,acB_error
         open(13, status='OLD', action='WRITE', position='APPEND')
         write(13,*)alpha,gamma,mass,zeta,acB_average,acB_error
         close(13)
         call jackknife_binning(TMC,ym0,ym_average,ym_error)
c        write(*,*)alpha,gamma,mass,zeta,ym_average,ym_error
         open(14, status='OLD', action='WRITE', position='APPEND')
         write(14,*)alpha,gamma,mass,zeta,ym_average,ym_error
         close(14)
         call jackknife_binning(TMC,cs0,cs_average,cs_error)
c        write(*,*)alpha,gamma,mass,zeta,cs_average,cs_error
         open(15, status='OLD', action='WRITE', position='APPEND')
         write(15,*)alpha,gamma,mass,zeta,cs_average,cs_error
         close(15)
         call jackknife_binning(TMC,ho0,ho_average,ho_error)
c        write(*,*)alpha,gamma,mass,zeta,ho_average,ho_error
         open(16, status='OLD', action='WRITE', position='APPEND')
         write(16,*)alpha,gamma,mass,zeta,ho_average,ho_error
         close(16)
         call jackknife_binning(TMC,ac_F,acF_average,acF_error)          
c        write(*,*)alpha,gamma,mass,zeta,acF_average,acF_error
         open(17, status='OLD', action='WRITE', position='APPEND')
         write(17,*)alpha,gamma,mass,zeta,acF_average,acF_error
close(17)
c............for the flat space supersymmetric model for which xi=0 the Schwinger-Dyson identity 
<4*gamma*YM+3*alpha*CS+2*mass*HO>=6(N^2-1) must hold...
         identity_av=4.0d0*gamma*ym_average+3.0d0*alpha*cs_average
     &        +2.0d0*mass*ho_average
         identity_av=identity_av/(6.0d0*(N*N-1.0d0))
         identity_av=identity_av-1.0d0
         identity_er=4.0d0*gamma*ym_error+3.0d0*alpha*cs_error
     &        +2.0d0*mass*ho_error
         identity_er=identity_er/(6.0d0*(N*N-1.0d0))
c        write(*,*)alpha,gamma,mass,zeta,identity_av,identity_er
         open(18, status='OLD', action='WRITE', position='APPEND')
         write(18,*)alpha,gamma,mass,zeta,identity_av,identity_er
         close(18)
      enddo
c...............cpu time........................................................
      call cpu_time(t_2)
      write(*,*)"cpu_time=", t_2-t_1      
      
      return
      end

Codes
265
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 5 of 18
c............the Metropolis algorithm.....................
      subroutine metropolis(N,dim,M,M0,gamma,mass,alpha,zeta,dt,time,X,P
     &     ,phi,Q,a0,a,b,c0,c,d,Rejec,Accept,var,varF,variationH,epsilon
     &     ,idum)
      implicit none
      integer N,dim,M,M0,i,j,mu,nu,k,l,idum,time,A1,sp
      double precision gamma,mass,alpha,zeta
      double precision inn,dt,ran2,Rejec,Accept
      double precision a0,a(M),b(M),c0,c(M0),d(M0),epsilon
      double complex X(dim,N,N),X0(dim,N,N),P(dim,N,N),
     &    P0(dim,N,N),phi(2,N*N-1),phi0(2,N*N-1),Q(2,N*N-1),Q0(2,N*N-1),
     &    xi(2,N*N-1),G(M,2,N*N-1),W(2,N*N-1),W0(2,N*N-1)
      double complex var(dim,N,N),varF(dim,N,N)
      double precision variations,variationH,probabilityS,probabilityH,r
      double precision ham,action,actionB,actionF,kinB,kinF,YM,CS,HO,
     &     hamB
      
c............Gaussian initialization..............................
      
      call gaussian(idum,dim,N,P)
      call gaussian_plus(idum,N,Q)
      call gaussian_plus(idum,N,xi)      
      phi=xi                 
      call conjugate_gradient(dim,N,M,zeta,X,c0,c,d,phi,G,W0,W,
     &     epsilon)
      phi=W0
c............saving the initial configurations................................
      
      X0=X
      P0=P
      phi0=phi
      Q0=Q
c................evaluation of the initial value of hamiltonian and action..............
      
      call sub_action(dim,N,M,a0,a,b,X,P,phi,Q,alpha,mass,gamma,zeta,
     &     ham,action,actionB,actionF,kinB,kinF,YM,CS,HO,epsilon)                    
      hamB=actionB+kinB
      variationS=action
      variationH=ham
      
c..........molecular dynamics evolution.......................................
      call molecular_dynamics(N,dim,M,dt,time,gamma,mass,alpha,zeta
     &     ,a0,a,b,X,P,phi,Q,var,varF,epsilon)                    
      
c...........evaluation of the final value of hamiltonian and action and the differences................
      
      call sub_action(dim,N,M,a0,a,b,X,P,phi,Q,alpha,mass,gamma,zeta,
     &     ham,action,actionB,actionF,kinB,kinF,YM,CS,HO,epsilon)
      hamB=actionB+kinB             
      variationS=action-variationS
      variationH=ham-variationH
      
c............metropolis accept-reject step.......................................................
      
      if(variationH.lt.0.0d0)then
         accept=accept+1.0d0
      else
         probabilityH=dexp(-variationH)
         r=ran2(idum)
         if (r.lt.probabilityH)then
            accept=accept+1.0d0
         else
            X=X0
            P=P0

266
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 6 of 18
            phi=phi0
            Q=Q0
            Rejec=Rejec+1.0d0
         endif
      endif
      
      return
      end
      
c..............the leap frog algorithm.............................
      subroutine molecular_dynamics(N,dim,M,dt,time,gamma,mass,alpha,
     &     zeta,a0,a,b,X,P,phi,Q,var,varF,epsilon)
      implicit none
      integer N,dim,M,i,j,mu,nn,time,A1,A1b,sp
      double precision dt,gamma,mass,alpha,zeta,a0,a(M),b(M),epsilon,
     &     alp
      double complex X(dim,N,N),phi(2,N*N-1),P(dim,N,N),Q(2,N*N-1),
     &     xx(2,N*N-1),var(dim,N,N),varF(dim,N,N),G(M,2,N*N-1),
     &     W(2,N*N-1),W0(2,N*N-1)
      
      alp=1.0d0      
      do nn=1,time                           
         call conjugate_gradient(dim,N,M,zeta,X,a0,a,b,phi,G,W0,W,
     &        epsilon)
         call  boson_force(N,dim,gamma,mass,alpha,X,var)
         call fermion_force(N,dim,M,zeta,a0,a,b,X,G,varF)
         do i=1,N
            do j=i,N
               do mu=1,dim                  
                  P(mu,i,j)=P(mu,i,j)-0.5d0*alp*dt*var(mu,i,j)
     &                 -0.5d0*alp*dt*varF(mu,i,j)
                  X(mu,i,j)=X(mu,i,j)+alp*dt*conjg(P(mu,i,j))
                  X(mu,j,i)=conjg(X(mu,i,j))
               enddo
            enddo
         enddo
         do A1=1,N*N-1
            do sp=1,2
               Q(sp,A1)=Q(sp,A1)-0.5d0*alp*dt*W(sp,A1)
               phi(sp,A1)=phi(sp,A1)+alp*dt*conjg(Q(sp,A1))
            enddo
         enddo         
c....................last step of the leap frog......
         call conjugate_gradient(dim,N,M,zeta,X,a0,a,b,phi,G,W0,W,
     &        epsilon)
         call  boson_force(N,dim,gamma,mass,alpha,X,var)
         call fermion_force(N,dim,M,zeta,a0,a,b,X,G,varF)
        
         do i=1,N
            do j=i,N
               do mu=1,dim                  
                  P(mu,i,j)=P(mu,i,j)-0.5d0*alp*dt*var(mu,i,j)
     &                 -0.5d0*alp*dt*varF(mu,i,j)
                  P(mu,j,i)=conjg(P(mu,i,j))
               enddo
            enddo
         enddo
         do A1=1,N*N-1
            do sp=1,2
               Q(sp,A1)=Q(sp,A1)-0.5d0*alp*dt*W(sp,A1)
            enddo
         enddo         
      enddo
      
      return
      end

Codes
267
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 7 of 18
c.......the conjugate gradient method..............
      
      subroutine conjugate_gradient(dim,N,M,zeta,X,a0,a,b,phi,G,W0,W,
     &     epsilon)
      implicit none
      integer dim,N,M,M0,i,j,counter,A1,sig,sp
      double precision zeta,a0,a(M),b(M),tol,tol0,residue,residue0,
     &     epsilon
      double complex X(dim,N,N)
      double complex xx(2,N*N-1),phi(2,N*N-1),r(2,N*N-1),p(2,N*N-1),
     &     q(2,N*N-1),o(2,N*N-1),xx1(2,N*N-1),q_previous(2,N*N-1)
      double complex  x_traceless_vec(2,N*N-1),y_traceless_vec(2,N*N-1),
     &     z_traceless_vec(2,N*N-1)
      double complex G(M,2,N*N-1),p_sigma(M,2,N*N-1),W(2,N*N-1),
     &     W0(2,N*N-1), G0(M,2,N*N-1)
      double precision rho,rho_previous,rho_sigma(M),beta,beta_previous,
     &     beta_sigma(M),xii0,xii,xi(M),xi_previous(M)      
      double precision product,product1,product2
      parameter(tol=10.0d-5,tol0=10.0d-3)
     
c.........initialization.................
      
      do A1=1,N*N-1
         do sp=1,2
             xx(sp,A1)=cmplx(0,0)
             r(sp,A1)=phi(sp,A1)
             do sig=1,M
                G(sig,sp,A1)=cmplx(0,0)    
             enddo
             q(sp,A1)=cmplx(0,0)
          enddo
       enddo
       
c..............initialization of the coefficients...........       
       
       rho=0.0d0
       beta=1.0d0
       do sig=1,M         
          xi_previous(sig)=1.0d0   
          xi(sig)=1.0d0
          rho_sigma(sig)=0.0d0
          beta_sigma(sig)=1.0d0      
enddo
       
c...........starting the iteration..........................................
       
      counter=0
      
c.........choosing search directions................................
      
13   do A1=1,N*N-1
         do sp=1,2
            p(sp,A1)=r(sp,A1)+rho*p(sp,A1)
            do sig=1,M
               p_sigma(sig,sp,A1)=xi(sig)*r(sp,A1)
     &              +rho_sigma(sig)*p_sigma(sig,sp,A1)
            enddo
         enddo
      enddo
      
c......solving the no-sigma problem.....
      
c........performing the only vector-matrix multiplication in the conjugate gradient method...        
c     q(i)=0.0d0
c     do j=1,2*(N*N-1)
c     q(i)=q(i)+(Delta(i,j)+epsilon*delta(i,j))*p(j)

268
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 8 of 18
c     enddo
      call multiplication(dim,N,M,zeta,X,p,y_traceless_vec)
      o=y_traceless_vec                
c         write(*,*)"o",o
      call multiplication_plus(dim,N,M,zeta,X,o,z_traceless_vec)
      q_previous=q 
      q=z_traceless_vec
      q=q+epsilon*p
c     write(*,*)"q",q
c.................calculating the beta coefficient......
      product=0.0d0
      product1=0.0d0
      do A1=1,N*N-1
         do sp=1,2
            product=product+conjg(p(sp,A1))*q(sp,A1)
            product1=product1+conjg(r(sp,A1))*r(sp,A1)
         enddo
      enddo
      beta_previous=beta
      beta=-product1/product
c...............calculating the solution xx, its residue and the rho coefficient..... 
      product2=0.0d0
      do A1=1,N*N-1
         do sp=1,2
            xx(sp,A1)=xx(sp,A1)-beta*p(sp,A1)
            r(sp,A1)=r(sp,A1)+beta*q(sp,A1)
            product2=product2+conjg(r(sp,A1))*r(sp,A1)
         enddo
      enddo
      rho_previous=rho
      rho=product2/product1
      
c.......solving the sigma problems..............
      
      do sig=1,M
c.........the xi coefficients..................                  
         xii0=rho_previous*beta*(xi_previous(sig)-xi(sig))+
     &        xi_previous(sig)*beta_previous*(1.0d0-b(sig)*beta)
         xii=xi(sig)*xi_previous(sig)*beta_previous/xii0
         xi_previous(sig)=xi(sig)
         xi(sig)=xii     
c.........the beta coefficients............................
         beta_sigma(sig)=beta*xi(sig)/xi_previous(sig)
c.........the solutions......................       
         do A1=1,N*N-1
            do sp=1,2
            G(sig,sp,A1)=G(sig,sp,A1)-beta_sigma(sig)*p_sigma(sig,sp,A1)
            enddo
         enddo
c........the alpha coefficients:alpha=rho..        
       rho_sigma(sig)=rho
       rho_sigma(sig)=rho_sigma(sig)*xi(sig)*beta_sigma(sig)
       rho_sigma(sig)=rho_sigma(sig)/(xi_previous(sig)*beta) 
      enddo
      
c......testing whether or not we continue the iteration................
       
      residue=0.0d0
      do A1=1,N*N-1
         do sp=1,2
            residue=residue+conjg(r(sp,A1))*r(sp,A1)
         enddo
      enddo
      residue=dsqrt(residue)
      counter=counter+1      
      if(residue.ge.tol) go to 13
c     write(*,*)counter,residue

Codes
269
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 9 of 18
c...........computing the pseudo-fermions W and W0......................
      
      do A1=1,N*N-1
         do sp=1,2
            W0(sp,A1)=cmplx(0,0)
            do sig=1,M   
               W0(sp,A1)=W0(sp,A1)+a(sig)*G(sig,sp,A1) 
            enddo
            W0(sp,A1)=W0(sp,A1)+a0*phi(sp,A1)
           W(sp,A1)=conjg(W0(sp,A1))
        enddo
      enddo           
           
c......verification of Delta.xx=phi....................
c     write(*,*)"phi",phi
c     write(*,*)"......................"
c     call multiplication(dim,N,M,zeta,X,xx,y_traceless_vec)
c     o=y_traceless_vec
c     write(*,*)"o",o
c     call multiplication_plus(dim,N,M,zeta,X,o,z_traceless_vec)
c     q=z_traceless_vec
c...............we must have q=phi since Delta.xx=phi....            
c     write(*,*)"q",q
c     write(*,*)"..............................."
            
c......verification of (Delta+b(sigma)).G_sigma=phi....................
c     sig=1
c            call reverse_identification(N,M,sig,G,x_traceless_vec)
c     xx1=x_traceless_vec
c     call multiplication(dim,N,M,zeta,X,xx1,y_traceless_vec)
c     o=y_traceless_vec
c     write(*,*)"o",o
c     call multiplication_plus(dim,N,M,zeta,X,o,z_traceless_vec)
c     q=z_traceless_vec+b(sig)*xx1
c...............we must have q=phi ....
c      write(*,*)"q",q
c     write(*,*)phi(1,1),q(1,1)
c     write(*,*)".........................."
      
      return
      end
      
c.........actions and Hamiltonians.............................      
      
      subroutine sub_action(dim,N,M,a0,a,b,X,P,phi,Q,alpha,mass,gamma,
     &     zeta,ham,action,actionB,actionF,kinB,kinF,YM,CS,HO,epsilon)
      implicit none
      integer dim,N,M,mu,nu,i,j,k,l,A1,sp
      double complex X(dim,N,N),P(dim,N,N),phi(2,N*N-1),Q(2,N*N-1),
     &W(2,N*N-1),W0(2,N*N-1),G(M,2,N*N-1)
      double complex ii,action0,action1,action2,ham0,ym0,cs0,ho0,
     &     kin0,kin1
      double precision action,actionB,actionF,ham,kinB,kinF,YM,CS,HO,
     &a0,a(M),b(M),epsilon
      double precision mass,gamma,alpha,zeta
      
      ii=cmplx(0,1)
      
c................yang-mills action........................
      
      ym0=cmplx(0,0)
      do mu =1,dim
         do nu=mu+1,dim
            action0=cmplx(0,0)
            do i=1,N
               do j=1,N

270
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 10 of 18
                  do k=1,N
                     do l=1,N
                 action0=action0+X(mu,i,j)*X(nu,j,k)*X(mu,k,l)*X(nu,l,i)
     &                       -X(mu,i,j)*X(mu,j,k)*X(nu,k,l)*X(nu,l,i)
                     enddo
                  enddo
               enddo
            enddo
      ym0=ym0+action0
      enddo
      enddo
      action=real(ym0)
      YM=-N*action
      action=-N*gamma*action         
      
c...........the harmonic oscillator and the bosonic kinetic terms..........
      
      kin0=cmplx(0,0)
      ho0=cmplx(0,0)
      do mu =1,dim
         ham0=cmplx(0,0)
         action1=cmplx(0,0)
         do i=1,N
            do j=1,N
               ham0=ham0+P(mu,i,j)*P(mu,j,i)
               action1=action1+X(mu,i,j)*X(mu,j,i)
            enddo
         enddo  
      kin0=kin0+ham0    
      ho0=ho0+action1     
      enddo     
      kinB=0.5d0*real(kin0)
      ham=kinB
      HO=0.5d0*real(ho0)
      action=action+0.5d0*mass*real(ho0)
      
c..........the chern-simons term....................................................
      
      cs0=cmplx(0,0)
      do i=1,N
         do j=1,N
            do k=1,N
               cs0=cs0+ii*X(1,i,j)*X(2,j,k)*X(3,k,i)
     &                -ii*X(1,i,j)*X(3,j,k)*X(2,k,i)
            enddo
         enddo
      enddo
      CS=2.0d0*N*real(cs0)
      action=action+2.0d0*alpha*N*real(cs0)
      ham=ham+action
      actionB=action
c...............fermion contribution.....
      
      call conjugate_gradient(dim,N,M,zeta,X,a0,a,b,phi,G,W0,W,
     &     epsilon)
      action2=cmplx(0,0)
      kin1=cmplx(0,0)
      do A1=1,N*N-1
         do sp=1,2
            action2=action2+W(sp,A1)*phi(sp,A1)
            kin1=kin1+conjg(Q(sp,A1))*Q(sp,A1)
         enddo
      enddo
      actionF=real(action2)
      kinF=real(kin1)
      action=actionB+actionF

Codes
271
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 11 of 18
      ham=ham+kinF+actionF
      
      return
      end
      
c...........the Boson force................
      subroutine boson_force(N,dim,gamma,mass,alpha,X,var)
      implicit none
      integer N,dim,i,j,mu,nu,k,l
      double precision gamma,mass,alpha
      double complex var(dim,N,N),X(dim,N,N),ii
      
      ii=cmplx(0,1)     
      do mu=1,dim  
         do i=1,N
            do j=i,N
               var(mu,i,j)=cmplx(0,0)
               do nu=1,dim
                  do k=1,N
                     do l=1,N
             var(mu,i,j)=var(mu,i,j)+2.0d0*X(nu,j,k)*X(mu,k,l)*X(nu,l,i)
     &                       -X(nu,j,k)*X(nu,k,l)*X(mu,l,i)
     &                       -X(mu,j,k)*X(nu,k,l)*X(nu,l,i)
                     enddo
                  enddo
               enddo
             var(mu,i,j)=-N*gamma*var(mu,i,j)+mass*X(mu,j,i)
               if(mu.eq.1)then
                  do k=1,N
             var(mu,i,j)=var(mu,i,j)+2.0d0*ii*alpha*N*X(2,j,k)*X(3,k,i)
     &                    -2.0d0*ii*alpha*N*X(3,j,k)*X(2,k,i)
                  enddo
               endif
               if(mu.eq.2)then
                  do k=1,N
             var(mu,i,j)=var(mu,i,j)+2.0d0*ii*alpha*N*X(3,j,k)*X(1,k,i)
     &                    -2.0d0*ii*alpha*N*X(1,j,k)*X(3,k,i)
                  enddo
               endif
               if(mu.eq.3)then
                  do k=1,N
             var(mu,i,j)=var(mu,i,j)+2.0d0*ii*alpha*N*X(1,j,k)*X(2,k,i)
     &                    -2.0d0*ii*alpha*N*X(2,j,k)*X(1,k,i)
                  enddo
               endif
              var(mu,j,i)=conjg(var(mu,i,j))
            enddo
         enddo          
      enddo   
      
      return
      end
c............the Fermion force...........................
      
      subroutine fermion_force(N,dim,M,zeta,a0,a,b,X,G,varF)
      implicit none
      integer N,M,dim,sig,i,j,k
      double complex X(dim,N,N),phi(2,N*N-1)
      double precision a0,a(M),b(M),zeta
      double complex T(dim),S(dim),varF(dim,N,N),ii
      double complex G(M,2,N*N-1),G_vec(2,N*N),Gm(2,N,N),F_vec(2,N*N)
     &     ,Fm(2,N,N),W(2,N*N-1),W0(2,N*N-1)
      double complex x_traceless_vec(2,N*N-1),y_traceless_vec(2,N*N-1)
      
      ii=cmplx(0,1)        

272
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 12 of 18
      do i=1,N
         do j=i,N
            varF(1,i,j)=cmplx(0,0)
            varF(2,i,j)=cmplx(0,0)
            varF(3,i,j)=cmplx(0,0)
            varF(4,i,j)=cmplx(0,0)
            do sig=1,M
               call reverse_identification(N,M,sig,G,x_traceless_vec)
               call conversion(N,x_traceless_vec,G_vec,Gm) 
               call multiplication(dim,N,M,zeta,X,x_traceless_vec,
     &              y_traceless_vec)
               call conversion(N,y_traceless_vec,F_vec,Fm)            
               T(1)=cmplx(0,0)
               T(2)=cmplx(0,0)
               T(3)=cmplx(0,0)
               T(4)=cmplx(0,0)
               S(1)=cmplx(0,0)
               S(2)=cmplx(0,0)
               S(3)=cmplx(0,0)
               S(4)=cmplx(0,0)
           do k=1,N
         T(1)=T(1)+Gm(1,j,k)*conjg(Fm(2,k,i))-conjg(Fm(2,j,k))*Gm(1,k,i)
     &            +Gm(2,j,k)*conjg(Fm(1,k,i))-conjg(Fm(1,j,k))*Gm(2,k,i) 
         S(1)=S(1)+Gm(1,i,k)*conjg(Fm(2,k,j))-conjg(Fm(2,i,k))*Gm(1,k,j)
     &            +Gm(2,i,k)*conjg(Fm(1,k,j))-conjg(Fm(1,i,k))*Gm(2,k,j) 
         T(2)=T(2)-Gm(1,j,k)*conjg(Fm(2,k,i))+conjg(Fm(2,j,k))*Gm(1,k,i)
     &            +Gm(2,j,k)*conjg(Fm(1,k,i))-conjg(Fm(1,j,k))*Gm(2,k,i)
         S(2)=S(2)-Gm(1,i,k)*conjg(Fm(2,k,j))+conjg(Fm(2,i,k))*Gm(1,k,j)
     &            +Gm(2,i,k)*conjg(Fm(1,k,j))-conjg(Fm(1,i,k))*Gm(2,k,j)
         T(3)=T(3)+Gm(1,j,k)*conjg(Fm(1,k,i))-conjg(Fm(1,j,k))*Gm(1,k,i)
     &            -Gm(2,j,k)*conjg(Fm(2,k,i))+conjg(Fm(2,j,k))*Gm(2,k,i) 
         S(3)=S(3)+Gm(1,i,k)*conjg(Fm(1,k,j))-conjg(Fm(1,i,k))*Gm(1,k,j)
     &            -Gm(2,i,k)*conjg(Fm(2,k,j))+conjg(Fm(2,i,k))*Gm(2,k,j) 
         T(4)=T(4)+Gm(1,j,k)*conjg(Fm(1,k,i))-conjg(Fm(1,j,k))*Gm(1,k,i)
     &            +Gm(2,j,k)*conjg(Fm(2,k,i))-conjg(Fm(2,j,k))*Gm(2,k,i) 
         S(4)=S(4)+Gm(1,i,k)*conjg(Fm(1,k,j))-conjg(Fm(1,i,k))*Gm(1,k,j)
     &            +Gm(2,i,k)*conjg(Fm(2,k,j))-conjg(Fm(2,i,k))*Gm(2,k,j) 
           enddo
           T(2)=ii*T(2)
           S(2)=ii*S(2)
           T(4)=ii*T(4)
           S(4)=ii*S(4)
           varF(1,i,j)=varF(1,i,j)-a(sig)*(T(1)+conjg(S(1)))
           varF(2,i,j)=varF(2,i,j)-a(sig)*(T(2)+conjg(S(2)))
           varF(3,i,j)=varF(3,i,j)-a(sig)*(T(3)+conjg(S(3)))
           varF(4,i,j)=varF(4,i,j)-a(sig)*(T(4)+conjg(S(4)))                     
        enddo
        varF(1,j,i)=conjg(varF(1,i,j))
        varF(2,j,i)=conjg(varF(2,i,j))
        varF(3,j,i)=conjg(varF(3,i,j))
        varF(4,j,i)=conjg(varF(4,i,j)) 
      enddo
      enddo
      
      return
      end
      
c.............multiplication by M....
      subroutine multiplication(dim,N,M,zeta,X,x_traceless_vec
     &     ,y_traceless_vec)
      implicit none
      integer i,j,k,dim,N,M
      double precision zeta
      double complex y_mat(2,N,N),y_vec(2,N*N),y_traceless_vec(2,N*N-1),
     &     x_mat(2,N,N),x_vec(2,N*N),x_traceless_vec(2,N*N-1)
      double complex ii,X(dim,N,N)

Codes
273
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 13 of 18
      
      ii=cmplx(0,1)    
      call conversion(N,x_traceless_vec,x_vec,x_mat)
      do j=1,N
         do i=1,N
            y_mat(1,j,i)=zeta*x_mat(1,i,j)
            y_mat(2,j,i)=zeta*x_mat(2,i,j)
            do k=1,N
               y_mat(1,j,i)=y_mat(1,j,i)
     &              +X(3,i,k)*x_mat(1,k,j)-x_mat(1,i,k)*X(3,k,j)
     &              +ii*X(4,i,k)*x_mat(1,k,j)-ii*x_mat(1,i,k)*X(4,k,j)
     &              +X(1,i,k)*x_mat(2,k,j)-x_mat(2,i,k)*X(1,k,j)
     &              -ii*X(2,i,k)*x_mat(2,k,j)+ii*x_mat(2,i,k)*X(2,k,j)
               y_mat(2,j,i)=y_mat(2,j,i)
     &              -X(3,i,k)*x_mat(2,k,j)+x_mat(2,i,k)*X(3,k,j)
     &              +ii*X(4,i,k)*x_mat(2,k,j)-ii*x_mat(2,i,k)*X(4,k,j)
     &              +X(1,i,k)*x_mat(1,k,j)-x_mat(1,i,k)*X(1,k,j)
     &              +ii*X(2,i,k)*x_mat(1,k,j)-ii*x_mat(1,i,k)*X(2,k,j)
            enddo
         enddo
      enddo
      call reverse_conversion(N,y_mat,y_vec,y_traceless_vec)
      
      return
      end
      
c.............multiplication by M^+....
      subroutine multiplication_plus(dim,N,M,zeta,X,y_traceless_vec
     &     ,z_traceless_vec)
      implicit none
      integer i,j,k,dim,N,M
      double precision zeta
      double complex z_mat(2,N,N),z_vec(2,N*N),z_traceless_vec(2,N*N-1),
     &     y_mat(2,N,N),y_vec(2,N*N),y_traceless_vec(2,N*N-1)
      double complex ii,X(dim,N,N)
      
      ii=cmplx(0,1)           
      call conversion(N,y_traceless_vec,y_vec,y_mat)
      do j=1,N
         do i=1,N
            z_mat(1,j,i)=zeta*y_mat(1,i,j)
            z_mat(2,j,i)=zeta*y_mat(2,i,j)
do k=1,N
               z_mat(1,j,i)=z_mat(1,j,i)
     &              -X(3,k,i)*y_mat(1,k,j)+y_mat(1,i,k)*X(3,j,k)
     &              +ii*X(4,k,i)*y_mat(1,k,j)-ii*y_mat(1,i,k)*X(4,j,k)
     &              -X(1,k,i)*y_mat(2,k,j)+y_mat(2,i,k)*X(1,j,k)
     &              +ii*X(2,k,i)*y_mat(2,k,j)-ii*y_mat(2,i,k)*X(2,j,k)
               z_mat(2,j,i)=z_mat(2,j,i)
     &              +X(3,k,i)*y_mat(2,k,j)-y_mat(2,i,k)*X(3,j,k)
     &              +ii*X(4,k,i)*y_mat(2,k,j)-ii*y_mat(2,i,k)*X(4,j,k)
     &              -X(1,k,i)*y_mat(1,k,j)+y_mat(1,i,k)*X(1,j,k)
     &              -ii*X(2,k,i)*y_mat(1,k,j)+ii*y_mat(1,i,k)*X(2,j,k)
            enddo
         enddo
      enddo
      call reverse_conversion(N,z_mat,z_vec,z_traceless_vec)
      
      return
      end
      
c.... given x_traceless_vec we construct x_vec and x_mat........
      
      subroutine conversion(N,x_traceless_vec,x_vec,x_mat)
      implicit none
      integer N,i,j,A1,sp

274
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 14 of 18
      double complex x_traceless_vec(2,N*N-1),x_vec(2,N*N),x_mat(2,N,N)
     &     ,xx
      
      do sp=1,2
         xx=0.0d0
         do i=1,N
            do j=1,N
               A1=N*(i-1)+j  
               if (A1.lt.N*N) then
                  x_vec(sp,A1)=x_traceless_vec(sp,A1)
                  if (i.eq.j) then
                     xx=xx-x_traceless_vec(sp,A1)
                  endif
               endif   
               x_mat(sp,i,j)=x_vec(sp,A1)               
            enddo
         enddo
         x_vec(sp,N*N)=xx
         x_mat(sp,N,N)=x_vec(sp,N*N) 
      enddo
      
      return
      end
      
c......given x_mat we construct x_vec and x_traceless_vec...
      
      subroutine reverse_conversion(N,x_mat,x_vec,x_traceless_vec)
      implicit none
      integer N,i,j,A1,sp
      double complex x_mat(2,N,N),x_vec(2,N*N),x_traceless_vec(2,N*N-1)
      
      do sp=1,2
         x_vec(sp,N*N)=x_mat(sp,N,N)
         do i=1,N
            do j=1,N
               A1=N*(i-1)+j
               if (A1.lt.N*N) then
                  x_vec(sp,A1)=x_mat(sp,i,j)
                  if (i.eq.j)then
                     x_traceless_vec(sp,A1)=x_vec(sp,A1)-x_vec(sp,N*N)
                  else
                     x_traceless_vec(sp,A1)=x_vec(sp,A1)
                  endif
endif
            enddo
         enddo
      enddo
      
      return
      end
      
c...............generation of Gaussian noise for the field P............
      
      subroutine gaussian(idum,dim,N,P)
      implicit none
      integer dim,N,mu,i,j,idum    
      double precision pi,phi,r,ran2
      double complex ii,P(dim,N,N)
      
      pi=dacos(-1.0d0)
      ii=cmplx(0,1)
      do mu=1,dim
c.............diagonal.........
         do i=1,N
            phi=2.0d0*pi*ran2(idum)
            r=dsqrt(-2.0d0*dlog(1.0d0-ran2(idum))) 
            P(mu,i,i)=r*dcos(phi)

Codes
275
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 15 of 18
         enddo
c.......off diagonal............
         do i=1,N
            do j=i+1,N
               phi=2.0d0*pi*ran2(idum)
               r=dsqrt(-1.0d0*dlog(1.0d0-ran2(idum))) 
               P(mu,i,j)=r*dcos(phi)+ii*r*dsin(phi)
               P(mu,j,i)=conjg(P(mu,i,j))
            enddo
         enddo
      enddo
      
      return
      end
c...............generation of Gaussian noise for the field Q............
      
      subroutine gaussian_plus(idum,N,Q)
      implicit none
      integer N,A1,sp,idum    
      double precision pi,phi,r,ran2
      double complex Q(2,N*N-1),ii
      
      pi=dacos(-1.0d0)
      ii=cmplx(0,1)
      do A1=1,N*N-1
         do sp=1,2
            phi=2.0d0*pi*ran2(idum)
            r=dsqrt(-1.0d0*dlog(1.0d0-ran2(idum))) 
            Q(sp,A1)=r*dcos(phi)+ii*r*dsin(phi)
         enddo
      enddo
         
      return
      end
c.........hot start.................      
      
      subroutine hot(N,dim,idum,inn,X) 
      integer mu,i,j,N,dim,idum
      double complex X(dim,N,N)
      double precision xx,y,inn,ran2
    
do mu=1,dim
         do i=1,N
            do j=i,N
               if (j.ne.i) then
                  xx=(2.0d0*ran2(idum)-1.0d0)*inn
                  y=(2.0d0*ran2(idum)-1.0d0)*inn
                  X(mu,i,j)=cmplx(xx,y)
                  X(mu,j,i)=cmplx(xx,-y)
               else
                  xx=(2.0d0*ran2(idum)-1.0d0)*inn
                  X(mu,i,j)=xx
               endif
            enddo
         enddo
      enddo      
      return
      end
      
c...........cold start......................
      
      subroutine cold(N,dim,idum,X) 
      integer mu,i,j,N,dim,idum
      double complex X(dim,N,N)

276
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 16 of 18
      
      do mu=1,dim
         do i=1,N
            do j=1,N
               X(mu,i,j)=cmplx(0,0)
            enddo
         enddo
      enddo
      
      return
      end
             
c..........the jackknife estimator...............
      subroutine jackknife_binning(TMC,f,average,error)
      integer i,j,TMC,zbin,nbin
      double precision xm
      double precision f(1:TMC),sumf,y(1:TMC)
      double precision sig0,sig,error,average
      
      sig0=0.0d0
      sumf=0.0d0
      do i=1,TMC
         sumf=sumf+f(i)
      enddo
      xm=sumf/TMC 
      zbin=1               
      nbin=int(TMC/zbin)
      sig=0.0d0
      do i=1,nbin,1
         y(i)=sumf
         do j=1,zbin
            y(i)=y(i)-f((i-1)*zbin+j )
         enddo
         y(i)= y(i)/(TMC-zbin)
         sig=sig+((nbin-1.0d0)/nbin)*(y(i)-xm)*(y(i)-xm)
      enddo
      sig=dsqrt(sig)
      if (sig0 .lt. sig) sig0=sig          
      error=sig0
      average=xm
      
      return
end
        
c.............the random number generator ran2.............
      function ran2(idum)
      implicit none
      integer idum,IM1,IM2,IMM1,IA1,IA2,IQ1,IQ2,IR1,IR2,NTAB,NDIV
      real AM,EPS,RNMX
      double precision ran2
      parameter (IM1=2147483563,IM2=2147483399,AM=1./IM1,IMM1=IM1-1,
     &     IA1=40014,IA2=40692,IQ1=53668,IQ2=52774,IR1=12211,
     &     IR2=3791,NTAB=32,NDIV=1+IMM1/NTAB,EPS=1.2E-7,RNMX=1.-EPS)
      integer idum2,j,k,iv(NTAB),iy
      SAVE iv,iy,idum2
      DATA idum2/123456789/,iv/NTAB*0/,iy/0/
      
      if (idum.le.0) then
         idum=max(-idum,1)
         idum2=idum
         do j=NTAB+8,1,-1
            k=idum/IQ1
            idum=IA1*(idum-k*IQ1)-k*IR1
            if (idum.lt.0) idum=idum+IM1
            if (j.le.NTAB) iv(j)=idum

Codes
277
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 17 of 18
         enddo
         iy=iv(1)
      endif
      k=idum/IQ1
      idum=IA1*(idum-k*IQ1)-k*IR1
      if (idum.lt.0) idum=idum+IM1
      k=idum2/IQ2
      idum2=IA2*(idum2-k*IQ2)-k*IR2
      if (idum2.lt.0) idum2=idum2+IM2
      j=1+iy/NDIV
      iy=iv(j)-idum2
      iv(j)=idum
      if (iy.lt.1) iy=iy+IMM1
      ran2=min(AM*iy,RNMX)
      return
      end
c...........defining an array from a vector....
      subroutine identification(N,M,sig,x_traceless_vec,G)
      implicit none
      integer N,M,sig,sp,A1
      double complex G(M,2,N*N-1),x_traceless_vec(2,N*N-1)
      
      do sp=1,2
         do A1=1,N*N-1
            G(sig,sp,A1)=x_traceless_vec(sp,A1)
         enddo
      enddo
      return
      end
      
c.......defining a vector from an array.......
      subroutine reverse_identification(N,M,sig,G,x_traceless_vec)
      implicit none
      integer N,M,sig,sp,A1
      double complex G(M,2,N*N-1),x_traceless_vec(2,N*N-1)
      do sp=1,2
         do A1=1,N*N-1
            x_traceless_vec(sp,A1)=G(sig,sp,A1)
         enddo
      enddo
      
      return
      end
      
c.........adjusting interval..................        
      subroutine adjust_inn(cou,pa,dt,time,Rejec,Accept,
     &     nn,target_pa_high,target_pa_low,dt_max,dt_min,inc,dec)  
      implicit none  
      double precision dt,pa,Rejec,Accept
      integer time,cou,cou1
      integer nn
      double precision target_pa_high,target_pa_low,dt_max,dt_min,inc,
     &     dec,rho1,rho2,dtnew
         
c.....pa acceptance rate............
      pa=(Accept)/(Rejec+Accept)        
      cou1=mod(cou,nn)        
      if (cou1.eq.0)then
c........fixing the acceptance rate between 90 % 70 %..................
         if (pa.ge.target_pa_high) then

278
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/hybrid-supersymmetric-ym.f
Page 18 of 18
            dtnew=dt*inc
            if (dtnew.le.dt_max)then
               dt=dtnew
            else
               dt=dt_max
            endif
         endif
         if (pa.le.target_pa_low) then
            dtnew=dt*dec
            if (dtnew.ge.dt_min)then
               dt=dtnew
            else
               dt=dt_min
            endif
         endif
      endif
            
      return
      end

Codes
279
File: /home/ydri/Desktop/TP_QFT/codes/u-one-on-the-lattice.f
Page 1 of 11
      program my_u_one_on_the_lattice
      implicit none
      integer dim,N,NT,i,j,k,l,mu,idum,tther,tmont,nther,nmont,counter,T
      integer tcor,ncor,betai,p,q
      double precision accept,reject,flip
      parameter (dim=4,N=4,NT=4,nther=2**(14),nmont=2**(14),ncor=2**4)
      parameter (T=100*(nther+nmont*ncor))
      double precision beta,ran2,variation,epsilon
     &     ,epsilon0,pi,acceptance,avera,erro,tau,deltau
      double complex U(dim,N,N,N,NT),ii,X,XX(0:T)
      double precision W11,W22,W33,W12,W13,W23,W21,W31,W32
      double precision acti(1:nmont),acti_mean,acti_error,
     &     action
      double precision acti_pp(1:nmont),acti_pp_mean,acti_pp_error,
     &     action_pp
      double precision cv(1:nmont),cv_mean,cv_error
      double precision plaq1(1:nmont),plaq1_mean,plaq1_error
      double precision plaq2(1:nmont),plaq2_mean,plaq2_error
      double precision plaq3(1:nmont),plaq3_mean,plaq3_error
      double precision plaq4(1:nmont),plaq4_mean,plaq4_error
      double precision plaq5(1:nmont),plaq5_mean,plaq5_error
      double precision plaq6(1:nmont),plaq6_mean,plaq6_error
      double precision plaq7(1:nmont),plaq7_mean,plaq7_error
      double precision plaq8(1:nmont),plaq8_mean,plaq8_error
      double precision plaq9(1:nmont),plaq9_mean,plaq9_error
      double precision tension1,error_tension1,tension2,error_tension2,
     &     tension3,error_tension3,tension4,error_tension4
      
c.........initialization of the random number generator.....................
      idum=-148175
      call seed(idum)
c..........initialization of other parameters...............................
      counter=0
      accept=0
      reject=0
      flip=0
      ii=cmplx(0,1)
      pi=dacos(-1.0d0)
      epsilon=pi
      
c.................gauge coupling constant..................
      do betai=1,1               
         beta=1.9d0-betai*0.1
         
c.............initialization of the link variables................
         
         do mu=1,dim
            do i=1,N
               do j=1,N
                  do k=1,N
                     do l=1,NT
c.........ordered start for coulomb phase while disordered start for confinment phase..
                        if(beta.ge.1.0d0)then
                           epsilon0=0.0d0
                        else
                           epsilon0=2.0d0*ran2(idum)-1.0d0
                           epsilon0=epsilon*epsilon0
                        endif
                        U(mu,i,j,k,l)=dcos(epsilon0)+ii*dsin(epsilon0)
                     enddo
                  enddo
               enddo
            enddo

280
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/u-one-on-the-lattice.f
Page 2 of 11
         enddo
         
c................thermalization.............          
         
         do tther=1,nther
          call metropolis(U,beta,dim,N,NT,accept,reject,flip,acceptance,
     &           epsilon,counter,XX,T)
         enddo
c..............monte carlo evolution...................................
         
         do tmont=1,nmont
            do tcor=1,ncor
          call metropolis(U,beta,dim,N,NT,accept,reject,flip,acceptance,
     &              epsilon,counter,XX,T)
            enddo           
            call actio(U,dim,N,NT,beta,action,action_pp)
            acti(tmont)=action
            acti_pp(tmont)=action_pp
            plaq1(tmont)=0.0d0
            plaq2(tmont)=0.0d0
            plaq3(tmont)=0.0d0
            plaq4(tmont)=0.0d0
            plaq5(tmont)=0.0d0
            plaq6(tmont)=0.0d0
            plaq7(tmont)=0.0d0
            plaq8(tmont)=0.0d0
            plaq9(tmont)=0.0d0
            do i=1,N
               do j=1,N
                  do k=1,N
                     do l=1,NT
                        p=1
                        q=4
                        call Wilson_Loop(U,dim,N,NT,i,j,k,l,p,q,
     &                       W11,W22,W33,W12,W13,W23,W21,W31,W32)
                        plaq1(tmont)=plaq1(tmont)+W11
                        plaq2(tmont)=plaq2(tmont)+W22
                        plaq3(tmont)=plaq3(tmont)+W33
                        plaq4(tmont)=plaq4(tmont)+W12
                        plaq5(tmont)=plaq5(tmont)+W13
                        plaq6(tmont)=plaq6(tmont)+W23
                        plaq7(tmont)=plaq7(tmont)+W21
                        plaq8(tmont)=plaq8(tmont)+W31
                        plaq9(tmont)=plaq9(tmont)+W32
                     enddo
                  enddo
               enddo
            enddo
            plaq1(tmont)=plaq1(tmont)/(N**3*NT)
            plaq2(tmont)=plaq2(tmont)/(N**3*NT)
            plaq3(tmont)=plaq3(tmont)/(N**3*NT)
            plaq4(tmont)=plaq4(tmont)/(N**3*NT)
            plaq5(tmont)=plaq5(tmont)/(N**3*NT)
            plaq6(tmont)=plaq6(tmont)/(N**3*NT)
            plaq7(tmont)=plaq7(tmont)/(N**3*NT)
            plaq8(tmont)=plaq8(tmont)/(N**3*NT)
            plaq9(tmont)=plaq9(tmont)/(N**3*NT)             
         enddo
c......................measurements........................
c......................action...............
        call jackknife_binning(nmont,acti,acti_mean,acti_error)
        write(11,*)beta,acti_mean,acti_error
c       write(*,*)beta,acti_mean,acti_error

Codes
281
File: /home/ydri/Desktop/TP_QFT/codes/u-one-on-the-lattice.f
Page 3 of 11
c..................action per plaquette..........
        call jackknife_binning(nmont,acti_pp,acti_pp_mean,acti_pp_error)
        write(12,*)beta,acti_pp_mean,acti_pp_error
c       write(*,*)beta,acti_pp_mean,acti_pp_error
c.......................specific heat.............
        do tmont=1,nmont
           cv(tmont)=(acti(tmont)-acti_mean)**2
        enddo
        call jackknife_binning(nmont,cv,cv_mean,cv_error)
        write(13,*)beta,cv_mean,cv_error
c       write(*,*)beta,cv_mean,cv_error      
c................Wilson loops................
        call jackknife_binning(nmont,plaq1,plaq1_mean,plaq1_error)
        write(15,*)beta,plaq1_mean,plaq1_error
c       write(*,*)beta,plaq1_mean,plaq1_error
        call jackknife_binning(nmont,plaq2,plaq2_mean,plaq2_error)
        write(16,*)beta,plaq2_mean,plaq2_error
c       write(*,*)beta,plaq2_mean,plaq2_error
        call jackknife_binning(nmont,plaq3,plaq3_mean,plaq3_error)
        write(17,*)beta,plaq3_mean,plaq3_error
c       write(*,*)beta,plaq3_mean,plaq3_error
        call jackknife_binning(nmont,plaq4,plaq4_mean,plaq4_error)
        write(18,*)beta,plaq4_mean,plaq4_error
c       write(*,*)beta,plaq4_mean,plaq4_error
        call jackknife_binning(nmont,plaq5,plaq5_mean,plaq5_error)
        write(19,*)beta,plaq5_mean,plaq5_error
c       write(*,*)beta,plaq5_mean,plaq5_error
        call jackknife_binning(nmont,plaq6,plaq6_mean,plaq6_error)
        write(20,*)beta,plaq6_mean,plaq6_error
c       write(*,*)beta,plaq6_mean,plaq6_error
        call jackknife_binning(nmont,plaq7,plaq7_mean,plaq7_error)
        write(23,*)beta,plaq7_mean,plaq7_error
c       write(*,*)beta,plaq7_mean,plaq7_error
        call jackknife_binning(nmont,plaq8,plaq8_mean,plaq8_error)
        write(24,*)beta,plaq8_mean,plaq8_error
c       write(*,*)beta,plaq8_mean,plaq8_error
        call jackknife_binning(nmont,plaq9,plaq9_mean,plaq9_error)
        write(25,*)beta,plaq9_mean,plaq9_error
c       write(*,*)beta,plaq9_mean,plaq9_error
        
c..............Creutz ratios:string tension.............
c...........chi22..........
        tension1=(plaq2_mean*plaq1_mean)/(plaq4_mean*plaq7_mean)
c..........chi33.....
        tension2=(plaq3_mean*plaq2_mean)/(plaq6_mean*plaq9_mean)
c..........chi23......
        tension3=(plaq6_mean*plaq4_mean)/(plaq2_mean*plaq5_mean)
c.........chi32..........
        tension4=(plaq9_mean*plaq7_mean)/(plaq2_mean*plaq8_mean)
        
        tension1=dabs(tension1)
        tension2=dabs(tension2)
        tension3=dabs(tension3)
        tension4=dabs(tension4)
        tension1=-dlog(tension1)
        tension2=-dlog(tension2)
        tension3=-dlog(tension3)
        tension4=-dlog(tension4)
        error_tension1=plaq2_error/plaq2_mean+plaq1_error/plaq1_mean
     &       -plaq4_error/plaq4_mean-plaq7_error/plaq7_mean
        error_tension1=dabs(error_tension1)
        error_tension2=plaq3_error/plaq3_mean+plaq2_error/plaq2_mean
     &       -plaq6_error/plaq6_mean -plaq9_error/plaq9_mean
        error_tension2=dabs(error_tension2)
        error_tension3=plaq6_error/plaq6_mean+plaq4_error/plaq4_mean

282
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/u-one-on-the-lattice.f
Page 4 of 11
     &       -plaq2_error/plaq2_mean -plaq5_error/plaq5_mean
        error_tension3=dabs(error_tension3)
        error_tension4=plaq9_error/plaq9_mean+plaq7_error/plaq7_mean
     &       -plaq2_error/plaq2_mean -plaq8_error/plaq8_mean
        error_tension4=dabs(error_tension4)
        write(22,*)beta,tension1,error_tension1,tension2,error_tension2,
     &       tension3,error_tension3,tension4,error_tension4
c       write(*,*)beta,tension1,error_tension1,tension2,error_tension2,
c     &      tension3,error_tension3,tension4,error_tension4
        
      enddo
      
      return
      end
c...............metropolis algorithm.................
           
      subroutine metropolis(U,beta,dim,N,NT,accept,reject,flip,
     &     acceptance,epsilon,counter,XX,T)
      implicit none
      integer dim,N,NT,nu,mu,i,j,k,l,idum,counter,counter0,nn,T
      double precision accept,reject,flip,nn0
      double precision epsilon,epsilon0,beta,variation,proba,r,ran2,pi,
     &     modulus,acceptance
      double complex U(dim,N,N,N,NT),X,ii,XX(0:T)
      
      pi=dacos(-1.0d0)
      ii=cmplx(0,1)
      
      epsilon0=2.0d0*ran2(idum)-1.0d0
      epsilon0=epsilon*epsilon0
      XX(counter)=dcos(epsilon0)+ii*dsin(epsilon0)
      XX(counter+1)=dcos(epsilon0)-ii*dsin(epsilon0)
      counter0=counter+1
      counter=counter+2
      
      do mu=1,dim
         do i=1,N
            do j=1,N
               do k=1,N
                  do l=1,NT 
                     nn0=counter0*ran2(idum)           
                     nn=nint(nn0)
                     X=XX(nn)
                   call variatio(U,X,beta,dim,N,NT,mu,i,j,k,l,variation)           
                     if(variation.gt.0)then
                        proba=dexp(-variation)
                        r=ran2(idum)
                        if(proba.gt.r)then
                           U(mu,i,j,k,l)=X*U(mu,i,j,k,l)
                           accept=accept+1
                        else
                           reject=reject+1
                        endif
                     else
                        U(mu,i,j,k,l)=X*U(mu,i,j,k,l)
                        flip=flip+1
                     endif
                     modulus=U(mu,i,j,k,l)*conjg(U(mu,i,j,k,l))
                     modulus=dsqrt(modulus) 
                     U(mu,i,j,k,l)=U(mu,i,j,k,l)/modulus             
                  enddo
               enddo
            enddo
         enddo
      enddo

Codes
283
File: /home/ydri/Desktop/TP_QFT/codes/u-one-on-the-lattice.f
Page 5 of 11
c.......for the range of N and NT considered the acceptance rate is already sufficiently high so we can 
simply disable the adjust subroutine....we observed that the acceptance rate decreases as we increase N 
and NT......            
      call adjust(epsilon,flip,accept,reject,acceptance)                  
c     write(*,*)flip,accept,reject,acceptance
      return
      end
      
c...........adjusting...........................
      subroutine adjust(epsilon,flip,accept,reject,acceptance)
      implicit none
      double precision epsilon,acceptance
      double precision flip,accept,reject,ran2
      integer idum
      
      acceptance=(flip+accept)/(flip+accept+reject)
      if (acceptance.ge.0.5d0) then        
         epsilon=epsilon*1.2d0
      endif
      if(acceptance.le.0.45d0) then
         epsilon=epsilon*0.8d0
      endif
      
      return
      end  
c........................variation.....................
      
      subroutine variatio(U,X,beta,dim,N,NT,mu,i,j,k,l,variation)
      implicit none
      integer dim,N,NT,nu,mu,i,j,k,l,idum
      double precision epsilon,epsilon0,beta,variation,ran2,pi
      double complex U(dim,N,N,N,NT),staple,ii,X
      
      call  stapl(U,dim,N,NT,mu,i,j,k,l,staple)     
      variation=-0.5d0*beta*((X-1.0d0)*U(mu,i,j,k,l)*staple
     &     + conjg((X-1.0d0)*U(mu,i,j,k,l)*staple))
      
      return
      end      
c.................staple....................................
      
      subroutine stapl(U,dim,N,NT,mu,i,j,k,l,staple)
      implicit none
      integer dim,N,NT,nu,mu,i,j,k,l,i0,ip(N),im(N),ipT(NT),imT(NT),
     &     ipn(1:N,1:N),ipnT(1:N,1:N) 
      double precision beta
      double complex U(dim,N,N,N,NT),staple
      
      call index_array(N,NT,ip,im,ipT,imT,ipn,ipnT)
      
      if(mu.eq.1)then
         staple=U(2,ip(i),j,k,l)*conjg(U(mu,i,ip(j),k,l))*
     &        conjg(U(2,i,j,k,l))
     &        +conjg(U(2,ip(i),im(j),k,l))*conjg(U(mu,i,im(j),k,l))
     &        *U(2,i,im(j),k,l)
     &   +U(3,ip(i),j,k,l)*conjg(U(mu,i,j,ip(k),l))*conjg(U(3,i,j,k,l))
     &        +conjg(U(3,ip(i),j,im(k),l))*conjg(U(mu,i,j,im(k),l))
     &        *U(3,i,j,im(k),l)
     &   +U(4,ip(i),j,k,l)*conjg(U(mu,i,j,k,ipT(l)))*conjg(U(4,i,j,k,l))
     &        +conjg(U(4,ip(i),j,k,imT(l)))*conjg(U(mu,i,j,k,imT(l)))
     &        *U(4,i,j,k,imT(l))        
      endif
      

284
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/u-one-on-the-lattice.f
Page 6 of 11
      if(mu.eq.2)then
         staple=U(1,i,ip(j),k,l)*conjg(U(mu,ip(i),j,k,l))*
     &        conjg(U(1,i,j,k,l))
     &        +conjg(U(1,im(i),ip(j),k,l))*conjg(U(mu,im(i),j,k,l))
     &        *U(1,im(i),j,k,l)
     &   +U(3,i,ip(j),k,l)*conjg(U(mu,i,j,ip(k),l))*conjg(U(3,i,j,k,l))
     &        +conjg(U(3,i,ip(j),im(k),l))*conjg(U(mu,i,j,im(k),l))
     &        *U(3,i,j,im(k),l)
     &   +U(4,i,ip(j),k,l)*conjg(U(mu,i,j,k,ipT(l)))*conjg(U(4,i,j,k,l))
     &        +conjg(U(4,i,ip(j),k,imT(l)))*conjg(U(mu,i,j,k,imT(l)))
     &        *U(4,i,j,k,imT(l))        
      endif
         
      if(mu.eq.3)then
         staple=U(1,i,j,ip(k),l)*conjg(U(mu,ip(i),j,k,l))
     &        *conjg(U(1,i,j,k,l))
     &        +conjg(U(1,im(i),j,ip(k),l))*conjg(U(mu,im(i),j,k,l))
     &        *U(1,im(i),j,k,l)
     &   +U(2,i,j,ip(k),l)*conjg(U(mu,i,ip(j),k,l))*conjg(U(2,i,j,k,l))
     &        +conjg(U(2,i,im(j),ip(k),l))*conjg(U(mu,i,im(j),k,l))
     &        *U(2,i,im(j),k,l)
     &   +U(4,i,j,ip(k),l)*conjg(U(mu,i,j,k,ipT(l)))*conjg(U(4,i,j,k,l))
     &        +conjg(U(4,i,j,ip(k),imT(l)))*conjg(U(mu,i,j,k,imT(l)))
     &        *U(4,i,j,k,imT(l))        
      endif
      
      if(mu.eq.4)then
         staple=U(1,i,j,k,ipT(l))*conjg(U(mu,ip(i),j,k,l))
     &        *conjg(U(1,i,j,k,l))
     &        +conjg(U(1,im(i),j,k,ipT(l)))*conjg(U(mu,im(i),j,k,l))
     &        *U(1,im(i),j,k,l)
     &   +U(2,i,j,k,ipT(l))*conjg(U(mu,i,ip(j),k,l))*conjg(U(2,i,j,k,l))
     &        +conjg(U(2,i,im(j),k,ipT(l)))*conjg(U(mu,i,im(j),k,l))
     &        *U(2,i,im(j),k,l)
     &   +U(3,i,j,k,ipT(l))*conjg(U(mu,i,j,ip(k),l))*conjg(U(3,i,j,k,l))
     &        +conjg(U(3,i,j,im(k),ipT(l)))*conjg(U(mu,i,j,im(k),l))
     &        *U(3,i,j,im(k),l)        
      endif
      
      return
      end
c...............wilson loops...............................
      subroutine Wilson_Loop(U,dim,N,NT,i,j,k,l,p,q,
     &     W11,W22,W33,W12,W13,W23,W21,W31,W32)
      implicit none
      integer dim,N,NT,i,j,k,l,p,q,i0,j0,ipn(1:N,1:N),ipnT(1:N,1:N),
     &     ip(1:N),im(1:N),ipT(1:N),imT(1:N)
      double complex U(dim,N,N,N,NT),W1,W2,W3,W4
      double precision W11,W22,W33,W12,W13,W23,W21,W31,W32
      
      call index_array(N,NT,ip,im,ipT,imT,ipn,ipnT)
      if ((p.eq.1).and.(q.eq.4))then
         
         W1=U(p,i,j,k,l)
         W4=U(q,i,j,k,l)
c        W3=U(q,i+1,j,k,l)
         W3=U(q,ipn(i,1),j,k,l)
c        W2=U(p,i,j,k,l+1)
         W2=U(p,i,j,k,ipnT(l,1))
         W11=0.5d0*(W1*W3*conjg(W2)*conjg(W4)+
     &        conjg(W1)*conjg(W3)*W2*W4)
         
c        W1=U(p,i,j,k,l)*U(p,i+1,j,k,l)
         W1=U(p,i,j,k,l)*U(p,ipn(i,1),j,k,l)
c        W4=U(q,i,j,k,l)*U(q,i,j,k,l+1)

Codes
285
File: /home/ydri/Desktop/TP_QFT/codes/u-one-on-the-lattice.f
Page 7 of 11
         W4=U(q,i,j,k,l)*U(q,i,j,k,ipnT(l,1))
c        W3=U(q,i+2,j,k,l)*U(q,i+2,j,k,l+1)
         W3=U(q,ipn(i,2),j,k,l)*U(q,ipn(i,2),j,k,ipnT(l,1))
c        W2=U(p,i,j,k,l+2)*U(p,i+1,j,k,l+2)
         W2=U(p,i,j,k,ipnT(l,2))*U(p,ipn(i,1),j,k,ipnT(l,2))
         W22=0.5d0*(W1*W3*conjg(W2)*conjg(W4)+
     &        conjg(W1)*conjg(W3)*W2*W4)
             
c        W1=U(p,i,j,k,l)*U(p,i+1,j,k,l)*U(p,i+2,j,k,l)
         W1=U(p,i,j,k,l)*U(p,ipn(i,1),j,k,l)*U(p,ipn(i,2),j,k,l)
c        W4=U(q,i,j,k,l)*U(q,i,j,k,l+1)*U(q,i,j,k,l+2)
         W4=U(q,i,j,k,l)*U(q,i,j,k,ipnT(l,1))*U(q,i,j,k,ipnT(l,2))
c        W3=U(q,i+3,j,k,l)*U(q,i+3,j,k,l+1)*U(q,i+3,j,k,l+2)
         W3=U(q,ipn(i,3),j,k,l)*U(q,ipn(i,3),j,k,ipnT(l,1))*
     &        U(q,ipn(i,3),j,k,ipnT(l,2))
c        W2=U(p,i,j,k,l+3)*U(p,i+1,j,k,l+3)*U(p,i+2,j,k,l+3)
         W2=U(p,i,j,k,ipnT(l,3))*U(p,ipn(i,1),j,k,ipnT(l,3))*
     &        U(p,ipn(i,2),j,k,ipnT(l,3))
         W33=0.5d0*(W1*W3*conjg(W2)*conjg(W4)+
     &        conjg(W1)*conjg(W3)*W2*W4)
         
         W1=U(p,i,j,k,l)
c        W4=U(q,i,j,k,l)*U(q,i,j,k,l+1)
         W4=U(q,i,j,k,l)*U(q,i,j,k,ipnT(l,1))
c        W3=U(q,i+1,j,k,l)*U(q,i+1,j,k,l+1)
         W3=U(q,ipn(i,1),j,k,l)*U(q,ipn(i,1),j,k,ipnT(l,1))
c        W2=U(p,i,j,k,l+2)
         W2=U(p,i,j,k,ipnT(l,2))
         W12=0.5d0*(W1*W3*conjg(W2)*conjg(W4)+
     &        conjg(W1)*conjg(W3)*W2*W4)
c        W1=U(p,i,j,k,l)*U(p,i+1,j,k,l)
         W1=U(p,i,j,k,l)*U(p,ipn(i,1),j,k,l)
         W4=U(q,i,j,k,l)
c        W3=U(q,i+2,j,k,l)
         W3=U(q,ipn(i,2),j,k,l)
c        W2=U(p,i,j,k,l+1)*U(p,i+1,j,k,l+1)
         W2=U(p,i,j,k,ipnT(l,1))*U(p,ipn(i,1),j,k,ipnT(l,1))
         W21=0.5d0*(W1*W3*conjg(W2)*conjg(W4)+
     &        conjg(W1)*conjg(W3)*W2*W4)
         
         W1=U(p,i,j,k,l)
c        W4=U(q,i,j,k,l)*U(q,i,j,k,l+1)*U(q,i,j,k,l+2)
         W4=U(q,i,j,k,l)*U(q,i,j,k,ipnT(l,1))*U(q,i,j,k,ipnT(l,2))
c        W3=U(q,i+1,j,k,l)*U(q,i+1,j,k,l+1)*U(q,i+1,j,k,l+2)
         W3=U(q,ipn(i,1),j,k,l)*U(q,ipn(i,1),j,k,ipnT(l,1))*
     &        U(q,ipn(i,1),j,k,ipnT(l,2))
c        W2=U(p,i,j,k,l+2)
         W2=U(p,i,j,k,ipnT(l,3))
         W13=0.5d0*(W1*W3*conjg(W2)*conjg(W4)+
     &        conjg(W1)*conjg(W3)*W2*W4)
c        W1=U(p,i,j,k,l)*U(p,i+1,j,k,l)*U(p,i+2,j,k,l)
         W1=U(p,i,j,k,l)*U(p,ipn(i,1),j,k,l)*U(p,ipn(i,2),j,k,l)
         W4=U(q,i,j,k,l)
c        W3=U(q,i+3,j,k,l)
         W3=U(q,ipn(i,3),j,k,l)
c        W2=U(p,i,j,k,l+1)*U(p,i+1,j,k,l+1)*U(p,i+2,j,k,l+1)
         W2=U(p,i,j,k,ipnT(l,1))*U(p,ipn(i,1),j,k,ipnT(l,1))*
     &        U(p,ipn(i,2),j,k,ipnT(l,1))
         W31=0.5d0*(W1*W3*conjg(W2)*conjg(W4)+
     &        conjg(W1)*conjg(W3)*W2*W4)
         
c        W1=U(p,i,j,k,l)*U(p,i+1,j,k,l)
         W1=U(p,i,j,k,l)*U(p,ipn(i,1),j,k,l)
c        W4=U(q,i,j,k,l)*U(q,i,j,k,l+1)*U(q,i,j,k,l+2)
         W4=U(q,i,j,k,l)*U(q,i,j,k,ipnT(l,1))*U(q,i,j,k,ipnT(l,2))

286
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/u-one-on-the-lattice.f
Page 8 of 11
c        W3=U(q,i+2,j,k,l)*U(q,i+2,j,k,l+1)*U(q,i+2,j,k,l+2)
         W3=U(q,ipn(i,2),j,k,l)*U(q,ipn(i,2),j,k,ipnT(l,1))*
     &        U(q,ipn(i,2),j,k,ipnT(l,2))
c        W2=U(p,i,j,k,l+3)*U(p,i+1,j,k,l+3)
         W2=U(p,i,j,k,ipnT(l,3))*U(p,ipn(i,1),j,k,ipnT(l,3))
         W23=0.5d0*(W1*W3*conjg(W2)*conjg(W4)+
     &        conjg(W1)*conjg(W3)*W2*W4)
         
c        W1=U(p,i,j,k,l)*U(p,i+1,j,k,l)*U(p,i+2,j,k,l)
         W1=U(p,i,j,k,l)*U(p,ipn(i,1),j,k,l)*U(p,ipn(i,2),j,k,l)
c        W4=U(q,i,j,k,l)*U(q,i,j,k,l+1)
         W4=U(q,i,j,k,l)*U(q,i,j,k,ipnT(l,1))
c        W3=U(q,i+3,j,k,l)*U(q,i+3,j,k,l+1)
         W3=U(q,ipn(i,3),j,k,l)*U(q,ipn(i,3),j,k,ipnT(l,1))
c        W2=U(p,i,j,k,l+2)*U(p,i+1,j,k,l+2)*U(p,i+2,j,k,l+2)
         W2=U(p,i,j,k,ipnT(l,2))*U(p,ipn(i,1),j,k,ipnT(l,2))*
     &        U(p,ipn(i,2),j,k,ipnT(l,2))
         W32=0.5d0*(W1*W3*conjg(W2)*conjg(W4)+
     &        conjg(W1)*conjg(W3)*W2*W4)
      endif
      
      return
      end
c..........................indexing.............................
      subroutine index_array(N,NT,ip,im,ipT,imT,ipn,ipnT)
      implicit none
      integer N,NT,i0,j0,ip(1:N),im(1:N),ipT(1:N),imT(1:N),
     &     ipn(1:N,1:N),ipnT(1:N,1:N)
      
      do i0=1,N
         ip(i0)=i0+1
         im(i0)=i0-1
      enddo
      ip(N)=1
      im(1)=N
      do i0=1,NT
         ipT(i0)=i0+1
         imT(i0)=i0-1
      enddo
      ipT(NT)=1
      imT(1)=NT
do i0=1,N
         do j0=1,N
            if (i0+j0 .le. N) then
               ipn(i0,j0)=i0+j0
            else
               ipn(i0,j0)=(i0+j0)-N
            endif
         enddo
      enddo
      do i0=1,NT
         do j0=1,NT
            if (i0+j0 .le. NT) then
               ipnT(i0,j0)=i0+j0
            else
               ipnT(i0,j0)=(i0+j0)-NT
            endif
         enddo
      enddo
      
      return
      end
c.....................action...............................
      

Codes
287
File: /home/ydri/Desktop/TP_QFT/codes/u-one-on-the-lattice.f
Page 9 of 11
      subroutine actio(U,dim,N,NT,beta,action,action_pp)
      implicit none
      integer dim,N,NT,i,j,k,l,ip(N),im(N),ipT(NT),imT(NT) 
      double precision beta
      double precision action12,action13,action14,action23,action24,
     &     action34,action,action_pp
      double complex U(dim,N,N,N,NT)
      
      do i=1,N
         ip(i)=i+1
         im(i)=i-1
      enddo
      ip(N)=1
      im(1)=N
      do i=1,NT
         ipT(i)=i+1
         imT(i)=i-1
      enddo
      ipT(NT)=1
      imT(1)=NT
      
      i=1
      j=1
      k=1
      l=1
c....................action per plaquette....
      action_pp=U(1,i,j,k,l)*U(2,ip(i),j,k,l)
     &     *conjg(U(1,i,ip(j),k,l))*conjg(U(2,i,j,k,l))
     &     +U(2,i,j,k,l)*U(1,i,ip(j),k,l)
     &     *conjg(U(2,ip(i),j,k,l))*conjg(U(1,i,j,k,l))
      action_pp=0.5d0*action_pp
      action_pp=1.0d0-action_pp
c..................action..........
      action12=0.0d0
      action13=0.0d0
      action14=0.0d0
      action23=0.0d0
      action24=0.0d0
      action34=0.0d0
      do i=1,N
         do j=1,N
            do k=1,N
               do l=1,NT
                  action12=action12+U(1,i,j,k,l)*U(2,ip(i),j,k,l)
     &                 *conjg(U(1,i,ip(j),k,l))*conjg(U(2,i,j,k,l))
     &                 +U(2,i,j,k,l)*U(1,i,ip(j),k,l)
     &                 *conjg(U(2,ip(i),j,k,l))*conjg(U(1,i,j,k,l))
                  action13=action13+U(1,i,j,k,l)*U(3,ip(i),j,k,l)
     &                 *conjg(U(1,i,j,ip(k),l))*conjg(U(3,i,j,k,l))
     &                 +U(3,i,j,k,l)*U(1,i,j,ip(k),l)
     &                 *conjg(U(3,ip(i),j,k,l))*conjg(U(1,i,j,k,l))
                  action14=action14+U(1,i,j,k,l)*U(4,ip(i),j,k,l)
     &                 *conjg(U(1,i,j,k,ipT(l)))*conjg(U(4,i,j,k,l))
     &                 +U(4,i,j,k,l)*U(1,i,j,k,ipT(l))
     &                 *conjg(U(4,ip(i),j,k,l))*conjg(U(1,i,j,k,l))
                  action23=action23+U(2,i,j,k,l)*U(3,i,ip(j),k,l)
     &                 *conjg(U(2,i,j,ip(k),l))*conjg(U(3,i,j,k,l))
     &                 +U(3,i,j,k,l)*U(2,i,j,ip(k),l)
     &                 *conjg(U(3,i,ip(j),k,l))*conjg(U(2,i,j,k,l))
                  action24=action24+U(2,i,j,k,l)*U(4,i,ip(j),k,l)
     &                 *conjg(U(2,i,j,k,ipT(l)))*conjg(U(4,i,j,k,l))
     &                 +U(4,i,j,k,l)*U(2,i,j,k,ipT(l))
     &                 *conjg(U(4,i,ip(j),k,l))*conjg(U(2,i,j,k,l))
                  action34=action34+U(3,i,j,k,l)*U(4,i,j,ip(k),l)
     &                 *conjg(U(3,i,j,k,ipT(l)))*conjg(U(4,i,j,k,l))
     &                 +U(4,i,j,k,l)*U(3,i,j,k,ipT(l))
     &                 *conjg(U(4,i,j,ip(k),l))*conjg(U(3,i,j,k,l))

288
An Introduction to Monte Carlo Simulations of Matrix Field Theory
File: /home/ydri/Desktop/TP_QFT/codes/u-one-on-the-lattice.f
Page 10 of 11
               enddo
            enddo
         enddo
      enddo
      action=action12+action13+action14+action23+action24+action34
      action=-0.5d0*beta*action
      action=action!+6.0d0*beta*N*N*N*NT
      return
      end
c...........................jackknife.........................................
      subroutine jackknife_binning(TMC,f,average,error)
      implicit none
      integer i,j,TMC,zbin,nbin
      doubleprecision xm
      doubleprecision f(1:TMC),sumf,y(1:TMC)
      doubleprecision sig0,sig,error,average
      
      sig0=0.0d0
      sumf=0.0d0
      do i=1,TMC
         sumf=sumf+f(i)
      enddo
      xm=sumf/TMC 
      zbin=1               
      nbin=int(TMC/zbin)
      sig=0.0d0
      do i=1,nbin,1
         y(i)=sumf
         do j=1,zbin
            y(i)=y(i)-f((i-1)*zbin+j )
         enddo
         y(i)= y(i)/(TMC-zbin)
         sig=sig+((nbin-1.0d0)/nbin)*(y(i)-xm)*(y(i)-xm)
      enddo
      sig=dsqrt(sig)
      if (sig0 .lt. sig) sig0=sig
      error=sig0
      average=xm
      
      return
end
c...............seed...................
      
      subroutine seed(idum)
      integer idum1,idum, n
      real x
      x=0.0
      idum=idum-2*int(secnds(x))
      
      return
      end
     
c.........the ran2 generator.................
      
      function ran2(idum)
      implicit none
      integer idum,IM1,IM2,IMM1,IA1,IA2,IQ1,IQ2,IR1,IR2,NTAB,NDIV
      real AM,EPS,RNMX
      doubleprecision ran2
      parameter (IM1=2147483563,IM2=2147483399,AM=1./IM1,IMM1=IM1-1,
     &     IA1=40014,IA2=40692,IQ1=53668,IQ2=52774,IR1=12211,
     &     IR2=3791,NTAB=32,NDIV=1+IMM1/NTAB,EPS=1.2E-7,RNMX=1.-EPS)

Codes
289
File: /home/ydri/Desktop/TP_QFT/codes/u-one-on-the-lattice.f
Page 11 of 11
      integer idum2,j,k,iv(NTAB),iy
      SAVE iv,iy,idum2
      DATA idum2/123456789/,iv/NTAB*0/,iy/0/
      if (idum.le.0) then
         idum=max(-idum,1)
         idum2=idum
         do j=NTAB+8,1,-1
            k=idum/IQ1
            idum=IA1*(idum-k*IQ1)-k*IR1
            if (idum.lt.0) idum=idum+IM1
            if (j.le.NTAB) iv(j)=idum
         enddo
         iy=iv(1)
      endif
      k=idum/IQ1
      idum=IA1*(idum-k*IQ1)-k*IR1
      if (idum.lt.0) idum=idum+IM1
      k=idum2/IQ2
      idum2=IA2*(idum2-k*IQ2)-k*IR2
      if (idum2.lt.0) idum2=idum2+IM2
      j=1+iy/NDIV
      iy=iv(j)-idum2
      iv(j)=idum
      if (iy.lt.1) iy=iy+IMM1
      ran2=min(AM*iy,RNMX)
      
      return
      end
      

Index
Accept/Reject step, 112, 113, 122
Acceptance-Rejection method, 86, 88
Air resistance, 7, 8, 14, 15, 43, 50
ARS matrix model, 125–128
Autocorrelation time, 134
BFSS matrix model, 198
Bisection algorithm, 23
BMN matrix model, 198
Canonical ensemble, 89, 94
Central limit theorem, 80, 82, 83
Chaos, 47–50, 52, 54, 64
Chern–Simons term, 270
Conﬁnement phase, 208, 215, 216
Conjugate gradient method, 163, 171,
175–177, 187, 192, 267
Cosmological Yang-Mills matrix models,
198
Coulomb phase, 208, 216, 279
Creutz ratio, 206–208, 215, 281
Critical exponents, 96, 100
Cubic spline, 26
Diﬀusion equation, 67, 68
Dirac operator, 181–184, 189, 193
Disorder phase, 134, 135, 153, 158
Emergent geometry, 124, 197, 199
Errors, xvii, 13, 14, 20, 58, 78, 82, 99, 102,
113, 122, 211
Euler algorithm, 3–5, 9–15, 35, 37
Euler-Cromer algorithm, 12, 35
Fermion determinant, 181
Ferromagnetic transition, 95, 96, 100–104
Fuzzy sphere, 125, 126, 128, 129, 131, 135,
154, 160, 197
Fuzzy sphere phase, 125, 126
Gauge ﬁelds, 203, 204
Gaussian distribution, 122–124, 161, 192
Harmonic oscillator, 37
Harmonic oscillator term, 119, 181, 197,
264
Heat-Bath algorithm, 94, 161
Histogram, 61, 88
Hit or miss method, 79, 87
Hybrid method, 24
Hybrid Monte Carlo algorithm, 146, 160,
163, 181, 192
Hysteresis, 101, 104
IKKT matrix model, 197
Importance sampling method, 90, 92, 93
Inverse transform method, 84, 85, 88
Ising model, 89, 91, 93, 94, 97–104
Lattice gauge ﬁeld, 204
Lattice regularization, 141, 199, 203
Lattice simulations, 199
Leap frog algorithm, 120, 121, 132, 184,
187, 266
Least square, 28
Lennard-Jones Potential, 56, 60
Linear congruent, 63, 69, 72
Matrix phase, 126–128, 137
Matrix scalar action, 131
291

292
An Introduction to Monte Carlo Simulations of Matrix Field Theory
Maxwell Distribution, 59–61
Mean ﬁeld approximation, 94
Melting Transition, 55, 60–62
Metropolis algorithm, 89, 92–94, 98, 99,
101, 102, 107, 112, 119, 122, 128, 131,
157, 160, 202, 209, 210, 220, 226, 234,
251, 265, 282
Metropolis step, 115, 192, 194, 195
Midpoint approximation, 76–78, 86, 87
Minimax approximation, 163
Molecular dynamics, 55, 57, 59, 61, 115,
119, 121, 122, 125, 131, 132, 190, 192,
194, 195, 225, 226, 228, 232, 234, 235,
237, 242, 262, 263, 265, 266
Monte Carlo error, 82
Monte Carlo integration, 75, 78, 90
Multi-trace matrix models, 153, 154
Myers term, 108
Newton–Raphson algorithm, 23, 29
Non-uniform order phase, 139, 158
Noncommutative torus, 199
Nonuniform probability distribution, 84,
85, 87, 90
Numerical integration, 17, 20, 75, 77, 82
Over-relaxation algorithm, 160
Parabolic approximation, 18, 20
Path integral, 90, 107, 108, 112, 115, 119,
125, 131, 155, 156, 161, 162, 182, 185,
197, 201, 202
Periodic motion, 10, 12, 13, 45, 48
Phase diagram, 94
Phase transition, 124, 129, 134, 136, 138,
145, 158, 215, 216
Precession of the perihelion, 39, 41, 42
Projectile motion, 9, 15
Pseudo-fermions, 185, 186, 194, 269
Quenched approximation, 182, 205, 206
RAN generators, 69
Random number, 60, 63, 64, 69, 71–73,
79, 81, 82, 84–86, 88, 91, 93, 98
Random number generator, 63, 64, 69, 72
Random walk, 63, 64, 66, 67, 69, 73, 74
Rational approximation, 163, 168–171,
185, 186, 192, 193, 195, 256, 257, 261
Real quartic matrix model, 134, 135, 153
Rectangular approximation, 17, 20, 21,
75–77, 79
Remez algorithm, 163, 168, 170, 192, 193
Runge–Kutta method, 36, 40, 46, 53
Sample mean method, 79, 80, 82
Simpson’s Rule, 18, 21
Solar system, 14, 31, 37, 40, 41
Standard deviation, 82–84, 87, 90, 91
Statistical error, 113, 114
Supersymmetry, xvii, 181, 199
Trapezoidal approximation, 18–20
Verlet algorithm, 10, 13, 16, 57, 58, 60
Wilson Loop, 206–208, 213–215, 280, 281,
284
Yang–Mills matrix models, 107, 119, 162,
198
Yang–Mills phase, 127




Signals and Systems
A One Semester Modular Course


Synthesis Lectures on Signal
Processing
Editor
José Moura, Carnegie Mellon University
Synthesis Lectures in Signal Processing publishes 80- to 150-page books on topics of interest to
signal processing engineers and researchers. The Lectures exploit in detail a focused topic. They can
be at different levels of exposition-from a basic introductory tutorial to an advanced
monograph-depending on the subject and the goals of the author. Over time, the Lectures will
provide a comprehensive treatment of signal processing. Because of its format, the Lectures will
also provide current coverage of signal processing, and existing Lectures will be updated by authors
when justified.
Lectures in Signal Processing are open to all relevant areas in signal processing. They will cover
theory and theoretical methods, algorithms, performance analysis, and applications. Some Lectures
will provide a new look at a well established area or problem, while others will venture into a brand
new topic in signal processing. By careful reviewing the manuscripts we will strive for quality both
in the Lectures’ contents and exposition.
Signals and Systems: A One Semester Modular Course
Khalid Sayood
2020
Smartphone-Based Real-Time Digital Signal Processing, Third Edition
Nasser Kehtarnavaz, Abhishek Sehgal, Shane Parris, and Arian Azarang
2020
Anywhere-Anytime Signals and Systems Laboratory: from MATLAB to Smartphones,
Third Edition
Nasser Kehtarnavaz, Fatemeh Saki, Adrian Duran, and Arian Azarang
2020
Reconstructive-Free Compressive Vision for Surveillance Applications
Henry Braun, Pavan Turaga, Andreas Spanias, Sameeksha Katoch, Suren Jayasuriya, and Cihan
Tepedelenlioglu
2019

iv
Smartphone-Based Real-Time Digital Signal Processing, Second Edition
Nasser Kehtarnavaz, Abhishek Sehgal, Shane Parris
2018
Anywhere-Anytime Signals and Systems Laboratory: from MATLAB to Smartphones,
Second Edition
Nasser Kehtarnavaz, Fatemeh Saki, and Adrian Duran
2018
Anywhere-Anytime Signals and Systems Laboratory: from MATLAB to Smartphones
Nasser Kehtarnavaz and Fatemeh Saki
2017
Smartphone-Based Real-Time Digital Signal Processing
Nasser Kehtarnavaz, Shane Parris, and Abhishek Sehgal
2015
An Introduction to Kalman Filtering with MATLAB Examples
Narayan Kovvali, Mahesh Banavar, and Andreas Spanias
2013
Sequential Monte Carlo Methods for Nonlinear Discrete-Time Filtering
Marcelo G.S. Bruno
2013
Processing of Seismic Reflection Data Using MATLAB™
Wail A. Mousa and Abdullatif A. Al-Shuhail
2011
Fixed-Point Signal Processing
Wayne T. Padgett and David V. Anderson
2009
Advanced Radar Detection Schemes Under Mismatched Signal Models
Francesco Bandiera, Danilo Orlando, and Giuseppe Ricci
2009
DSP for MATLAB™and LabVIEW™IV: LMS Adaptive Filtering
Forester W. Isen
2009
DSP for MATLAB™and LabVIEW™III: Digital Filter Design
Forester W. Isen
2008

v
DSP for MATLAB™and LabVIEW™II: Discrete Frequency Transforms
Forester W. Isen
2008
DSP for MATLAB™and LabVIEW™I: Fundamentals of Discrete Signal Processing
Forester W. Isen
2008
The Theory of Linear Prediction
P. P. Vaidyanathan
2007
Nonlinear Source Separation
Luis B. Almeida
2006
Spectral Analysis of Signals: The Missing Data Case
Yanwei Wang, Jian Li, and Petre Stoica
2006

Copyright © 2021 by Morgan & Claypool
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in
any form or by any means—electronic, mechanical, photocopy, recording, or any other except for brief quotations
in printed reviews, without the prior permission of the publisher.
Signals and Systems: A One Semester Modular Course
Khalid Sayood
www.morganclaypool.com
ISBN: 9781636391014
paperback
ISBN: 9781636391021
ebook
ISBN: 9781636391038
hardcover
DOI 10.2200/S01082ED1V01Y202103SPR020
A Publication in the Morgan & Claypool Publishers series
SYNTHESIS LECTURES ON SIGNAL PROCESSING
Lecture #20
Series Editor: José Moura, Carnegie Mellon University
Series ISSN
Print 1932-1236
Electronic 1932-1694

Signals and Systems
A One Semester Modular Course
Khalid Sayood
University of Nebraska–Lincoln
SYNTHESIS LECTURES ON SIGNAL PROCESSING #20
C
M
&
cLaypool
Morgan
publishers
&

ABSTRACT
This book is designed for use as a textbook for a one semester Signals and Systems class. It is
sufficiently user friendly to be used for self study as well. It begins with a gentle introduction
to the idea of abstraction by looking at numbers—the one highly abstract concept we use all
the time. It then introduces some special functions that are useful for analyzing signals and
systems. It then spends some time discussing some of the properties of systems; the goal being
to introduce the idea of a linear time-invariant system which is the focus of the rest of the book.
Fourier series, discrete and continuous time Fourier transforms are introduced as tools for the
analysis of signals. The concepts of sampling and modulation which are very much a part of
everyday life are discussed as applications of the these tools. Laplace transform and Z transform
are then introduced as tools to analyze systems. The notions of stability of systems and feedback
are analyzed using these tools.
The book is divided into thirty bite-sized modules. Each module also links up with a video
lecture through a QR code in each module. The video lectures are approximately thirty minutes
long. There are a set of self study questions at the end of each module along with answers to help
the reader reinforce the concepts in the module.
KEYWORDS
linear time-invariant systems, impulse response, convolution, Fourier series, Fourier
transform, discrete Fourier transform, Laplace transform, Z transform, feedback
systems

ix
To Füsun


xi
Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xix
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxi
0
What is This Course About . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
0.1
What Do We Plan to Cover . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
0.2
Abstractions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1
Complex Numbers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
1.1
Euler’s Formula. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
1.2
Cartesian Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.3
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
1.5
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
2
Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.1
Types of Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
2.1.1
Continuous and Discrete Time Functions . . . . . . . . . . . . . . . . . . . . . . 20
2.1.2
Even and Odd Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.2
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
2.4
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
3
Special Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
3.1
Step Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
3.2
Delta Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
3.2.1
Discrete Time Unit Impulse Function . . . . . . . . . . . . . . . . . . . . . . . . . 30
3.2.2
Continuous Time Delta Function or the Dirac Delta Function . . . . . 33
3.3
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
3.5
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38

xii
4
Classification of Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.1
Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
4.2
Invertibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
4.3
Causality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
4.4
Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
4.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
4.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
4.7
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
5
Linearity and Time Invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
5.1
Linearity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
5.1.1
Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
5.1.2
Additivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
5.2
Time Invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
5.3
Linear Time-Invariant (LTI) Systems. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
5.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
5.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
5.6
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
6
Linearity, Time-Invariance, and the Role of the Impulse Response . . . . . . . . . 63
6.1
The Response to an Impulse for a Discrete-Time System . . . . . . . . . . . . . . . . 63
6.2
The Response to an Impulse for Continuous Time Systems . . . . . . . . . . . . . . 66
6.3
The Convolution Operation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
6.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
6.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
6.6
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
7
Properties of LTI Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
7.1
Memory in LTI Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75
7.2
Invertibility of LTI Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
7.3
Causality of LTI Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78
7.3.1
Stability for LTI Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
7.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
7.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
7.6
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84

xiii
8
Discrete Time Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
8.1
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
8.2
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102
8.3
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
9
Continuous Time Convolution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
9.1
The Impulse Response of a Simple RC Circuit . . . . . . . . . . . . . . . . . . . . . . . . 105
9.2
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118
9.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
9.4
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
10
Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
10.1
Fourier Series Expansion of a Square Wave . . . . . . . . . . . . . . . . . . . . . . . . . . 133
10.2
Time Frequency Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
10.3
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
10.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
10.5
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141
11
Fourier Series – Properties and Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . 143
11.1
Even and Odd Functions and Their Coefficients . . . . . . . . . . . . . . . . . . . . . . 144
11.2
Time Shifts are Phase Shifts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
11.3
The Fourier Coefficients – Meaning and Extraction . . . . . . . . . . . . . . . . . . . . 150
11.4
The Energy in a Signal Does Not Change Based on its Representation . . . . 153
11.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154
11.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
11.7
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
12
The Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
12.1
Extending the Frequency View to Aperiodic Functions . . . . . . . . . . . . . . . . . 159
12.2
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
12.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
12.4
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
13
Properties of the Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
13.1
Convolution Property . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171
13.2
The Differentiation Property and the Fourier Transform of the Unit Step
Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178

xiv
13.3
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
13.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
13.5
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
14
Some More Useful Properties of the Fourier Transform . . . . . . . . . . . . . . . . . . 185
14.1
Integration Property . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
14.2
Time and Frequency Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
14.3
Fourier Transform of Periodic Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
14.4
A Shift in Time is a Phase Change in Frequency . . . . . . . . . . . . . . . . . . . . . . 191
14.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 192
14.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
14.7
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 196
15
Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
15.1
The Multiplication Property of the Fourier Transform . . . . . . . . . . . . . . . . . . 199
15.2
Ideal Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 200
15.3
Nonideal Sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
15.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205
15.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
15.6
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
16
Amplitude Modulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
16.1
AM Reciever . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 215
16.2
AM Stations in the U.S.. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 216
16.3
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217
16.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
16.5
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
17
Discrete Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223
17.1
Discrete Time Fourier Transform (DTFT) . . . . . . . . . . . . . . . . . . . . . . . . . . 223
17.2
Properties of the DTFT. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225
17.2.1 Differentiation in the Frequency Domain . . . . . . . . . . . . . . . . . . . . . 225
17.3
Discrete Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 226
17.4
Properties of the DFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
17.4.1 Linear and Circular Shifts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
17.4.2 Linearity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234

xv
17.4.3 Time Shift . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 234
17.4.4 Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
17.5
The Fast Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 235
17.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
17.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
17.8
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
18
The Laplace Transform – Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
18.1
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
18.2
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
18.3
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
19
Uniqueness and Linearity of the Laplace Transform . . . . . . . . . . . . . . . . . . . . . 249
19.1
The Laplace Transform is Unique – as Long as we Include the ROC in the
Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
19.2
The Laplace Transform is a Linear Transform (but Be Careful with the
Region of Convergence) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
19.3
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
19.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
19.5
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258
20
Laplace Transform – Poles and Zeros . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
20.1
Frequency Response . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
20.2
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
20.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
20.4
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
21
The Inverse Laplace Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
21.1
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
21.2
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279
21.3
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 280
22
Properties of Laplace Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
22.1
Convolution Property . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
22.2
Time Shifting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 283
22.3
Shifting in the S-Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
22.4
Time Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 286

xvi
22.5
Differentiation in the Time Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
22.6
Differentiation in the S-Domain. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
22.7
Integration in the Time Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
22.8
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
22.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
22.10 Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
23
Analysis of Systems with Feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
23.1
Stabilizing a System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
23.2
Making a System More Responsive . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
23.3
Implementing the Inverse of a System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
23.4
Producing a Desired Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 299
23.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
23.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 305
23.7
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
24
Z-Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
24.1
The Z-Transform as a Generalization of the DTFT . . . . . . . . . . . . . . . . . . . 309
24.2
The Z-Transform in the Context of Discrete LTI Systems . . . . . . . . . . . . . . 310
24.3
Exploring the Z-Transform Through Examples . . . . . . . . . . . . . . . . . . . . . . . 311
24.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
24.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
24.6
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
25
Region of Convergence for the Z-Transform . . . . . . . . . . . . . . . . . . . . . . . . . . 321
25.1
Mapping Between s- and z-Planes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323
25.2
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328
25.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329
25.4
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
26
Properties of the Z-Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
26.1
Linearity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
26.2
Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
26.3
Time Shifting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334
26.4
Scaling in the z-Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 335
26.5
Time Reversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 336

xvii
26.6
Differentiation in the z-Domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337
26.7
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 339
26.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
26.9
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
27
The Inverse Z-Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
27.1
Inverting by Dividing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
27.2
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352
27.3
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
27.4
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
28
Filters and Difference Equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
28.1
Frequency Response and Poles and Zeros . . . . . . . . . . . . . . . . . . . . . . . . . . . . 355
28.2
Finding the Input Output Relationship . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 361
28.3
Designing a Simple Discrete Time Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
28.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
28.5
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370
28.6
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
29
Discrete-Time Feedback Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
29.1
Stabilizing a System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 373
29.2
Producing a Desired Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
29.3
Proportional Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 376
29.4
Discrete Time PID Controllers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
29.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
29.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 384
29.7
Answers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 386
Author’s Biography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387


xix
Preface
There are a number of very nice books on Signals and Systems. The book by Oppenheim, Willsky
and Nawab, the one by Lathi and Green, and the one by Mitra, to name just three. So why
another book. The motivation behind this book, as reflected in the title, was to provide a textbook
for a modular one semester class which could also be used for self study. Therefore, the scope of
this book is both more restricted and focused than most Signals and Systems books. For each of
the modules there is also an approximately thirty minute long video available. I recorded those
videos for my Signals and Systems class using Open Broadcaster Software (OBS) which I would
strongly recommend to novices like me for video recording and streaming. I apologize for the
less than professional video editing. The modules in the text are linked to the videos through a
QR code.
The order of topics is a familiar one. We begin with three modules devoted to prelimi-
naries: complex numbers, functions, and special functions. The next six modules are devoted to
understanding properties of systems, in particular linearity and time invariance, which are then
used to develop the convolution relationship for linear time-invariant systems. The following
eight modules are devoted to Fourier analysis and its applications in sampling and modulation.
The sampling application is used to motivate the discrete Fourier transform (DFT). We use the
next six modules to look at the Laplace transform and explore its use on the analysis and design
of feedback systems. The final six modules are focused on the Z transform and the analysis and
design of discrete systems.
In my teaching of this class I go through three modules a week. Our semesters are fifteen
weeks long so this leaves me with five weeks for reviews and tests.
Khalid Sayood
May 2021


xxi
Acknowledgments
There are a number of people who helped me through this process and to whom I am very
grateful. As with all my writing Pat Worster tried to make me follow the rules of the English
language. Mike Hoffman, my long suffering colleague who would really like me to stop writing,
read through several of these modules and provided pungent criticism. Kadri Özçaldıran at
Bogazici University spent many hours pointing out the errors of my ways and discovered some
nasty typos. Jerry Hudgins generously agreed to do a test run of the material for his class—who
could ask for more. My friend John Beck was one of the initial inspirations for this modular form
and has always been very gentle in his critiques. My partner Füsun Sayood who has supported
my many wanderings this time suffered through days of my amateur video productions which
slowly took over the whole house. She was my reality check on various aspects of the video
production.
My thanks to Joel Claypool who has always been very gracious and patient with my various
writing adventures (except one) and who came up with the idea of linking the modules to the
corresponding videos. And of course my thanks to Dr. C.L. Tondo for his able shepherding of
this manuscript through the publication process.
Khalid Sayood
May 2021


1
M O D U L E
0
What is This Course About
A signal is anything that carries information. A smile or a wink is a signal, the flash of lightening
is a signal as is thunder, the song of a bird is a signal, a speech is a signal, as is a song, or a snarl. A
picture is a signal, as is a movie. A strand of DNA is a signal, a protein is a signal, a hormone is
most definitely a signal. Signals surround us so completely that we could if we wanted to classify
the entirety of our world in terms of signals. In these modules we will be more modest and
look only at signals which are one dimensional functions of time. Speech signals fall into this
category, as does music, or temperature variations or the price of a particular stock. In Figure 1
I am saying the word “test.” You can see the plosive t sound in the beginning and end of the
waveform. The sound of the e is the large signal which seems to dominate the plot and the sound
of s is the low amplitude almost noiselike signal before the final plosive t.
Figure 2 contains a signal which shows the concentration of CO2 in the environment over
the last decade or so. This plot is from the NASA website https://climate.nasa.gov/. (This is an
interesting website with all kinds of information about climate change.)
The global change in temperature over the last century and a half is shown in Figure 3.
Figure 4 shows the progression of the Spanish Flu in the United Kingdoms from June
of 1918 to the end of April 1919. You can see the first, second, and third waves in a one year
period.
4000
3500
“guf.out”
3000
2500
2000
1500
3
2
1
0
–1
–2
–3
1000
0
500
Figure 1: “Test.”

2
0. WHAT IS THIS COURSE ABOUT
410
405
400
395
390
385
380
375
2006
2008
2010
2012
Year
CO2 (parts per million)
2014
2016
2018
Figure 2: Concentration of CO2 in the environment. From https://climate.nasa.gov/vital-signs/
carbon-dioxide/.
1.0
0.5
0.0
–0.5
1880
1920
1940
1960
Year
Temperature anomaly (C)
1980
2000
2020
1900
Figure 3: Global change in temperature. From https://climate.nasa.gov/vital-signs/global-
temperature/.
Each of these signals contains information, and information is something that will rule
your professional life. As electrical and computer engineers you will be designing and working
with signals regardless of your particular specialization. You will work with broadcast signals,
power signals, signals from instruments, measurements of control signals, or signals for pro-
cessing in embedded systems. Regardless of the engineering specialty you end up embracing,
signals will be a part of your life. That being the case it makes sense to become proficient in as
many tools as possible which will allow you to understand a signal. In this book we will look at
a number of ways to view signals.

0.1. WHAT DO WE PLAN TO COVER
3
30
25
20
15
10
5
0
6/29
7/27
8/24
9/21
10/19
1918
1919
11/16
12/14
1/11
2/8
3/8
4/5
Density per 1,000 persons
Figure 4: 1918 Spanish Flu progression in the United Kingdom (from 1918 Influenza: the Mother
of All Pandemics, Emerging Infectious Diseases, Jan. 2006).
Once you have a signal you will want to do a number of things with it. You may want to
enhance it, distort it, or extract some particular piece of information from it. Something which
does any of this is a system. As in the case of signals we can look at the world as a collection
of systems. Your body is a system of systems, as is the world. There is the system in the body
which generates speech. There is the system of processes in the world that generate CO2. There
is the system we call the atmosphere which responds to the signals the sun is putting out by
heating up or cooling down. Our economic well being is effected by the stock market which is a
system that processes various signals from political and economic systems. To understand how
all these systems work is a tall order indeed. And we will not come anywhere close to doing so.
In these modules we will limit our attention to a very small class of systems called linear time
invariant (LTI) systems. However, you will find that through creative approaches for partitioning
or approximating systems which do not fit into this small class we will be able to use the methods
of analysis we develop for linear time-invariant systems more widely.
0.1
WHAT DO WE PLAN TO COVER
Our plan in this book is to learn some tools that can be used to analyze both discrete time and
continuous time signals and systems. For analyzing continuous time signals and systems we will
use the Fourier transform and the Laplace transform. For discrete time signals and systems we
will use the discrete Fourier transform and the Z transform. This is most emphatically not a
mathematics class; our goal is to learn some tools that happen to be mathematical tools. As
with any tool practice makes perfect so the more you can practice the more comfortable you will
be with using these tools. As our goal is to study these as tools we will try to include as many

4
0. WHAT IS THIS COURSE ABOUT
applications as we can. This is unfortunately only a single semester course so our opportunities
are somewhat limited but we will try our best.
Having said that, there are some mathematical concepts we will use a lot and which you
should be comfortable with. You should be reasonably comfortable with complex numbers. You
should be able to do simple integrals. These include integrals of xn, eax, and xeax. You should
understand what a function is and finally you should be comfortable with summations, in par-
ticular geometric sums. These are concepts that you should be familiar with but just to be on the
safe side we will go through some of them in the first modules.
0.2
ABSTRACTIONS
A Signals and Systems class is usually considered to be difficult because it deals with abstrac-
tions. However you have been dealing with abstractions all of your life. Consider numbers for
a moment. You deal with numbers daily without really thinking about it. You probably don’t
consciously think of numbers as abstract. Yet to understand how much of an abstraction num-
bers are consider how long it has taken human beings to become comfortable with ideas about
numbers that you would consider obvious.
The first practical use of numbers was to keep count. Tally sticks dating back tens of thou-
sands of years have been discovered. Going from notches to different symbolic representations
for different numbers also occurred many thousands of years ago. So the idea of numbers has
been around for a long time. Yet to get to the present day understanding of numbers took thou-
sands of years. The paths to modern numbers are numerous with one path taken in the Indian
subcontinent and what is now called the Middle East, another path (or several) in China and
yet other paths in Africa and the Americas. Let’s look at the path taken in western Asia and
Europe.
As we said the first use of numbers was probably for counting. We can keep track of things,
like the number of sheep, by making a one-to-one equivalence with pebbles or tokens. But as
the number of sheep grows the number of tokens might become cumbersome. We can then
use different kinds of tokens for ten sheep and for a single sheep which would be a primitive
positional number system. The earliest documented positional number system was that used by
people in Mesopotamia (modern day Iraq and Syria) close to 6000 years ago. They used base 60
numbers—which seems a bit odd—until you remember that we still use the base 60 numbering
system to measure time. The first documented use of the number zero (as opposed to the con-
cept of zero) is in India in the work of the 7th century mathematician Brahmagupta in a work
entitled Brahmasputasiddhanta. In around 825 a mathematician by the name of Muhammad ibn
Musa Al-Khwarizmi who worked at the House of Wisdom in Baghdad wrote a book on Indian
mathematics. Three centuries later his book was translated into Latin by Adelard of Bath finally
introducing the number zero to Europe. This idea of zero being a number was then popularized
by the Italian mathematician Leonardo of Pisa, better known as Fibonacci, in 1202—a centuries
long journey for an abstraction even a child knows today. (Al-Khwarizmi gave several words to

0.2. ABSTRACTIONS
5
the English language. He wrote a book describing a procedural approach to solving equations
called Kitab al-mukhtasar fi hisab al-jabr wal-muqabala—Book on Calculations by Completion
and Balancing. Procedural approaches became known as the method of Al-Khwarizmi which
in time became algoritni which then became algorithm. Kitab al-mukhtasar fi hisab al-jabr wal-
muqabala was translated into Italian by Robert Chester with the title Liber algebrae et almucabala
giving us the word Algebra.)
While zero came to Europe in the 12th century, negative numbers were resisted for an-
other few centuries. (In China on the other hand negative numbers had been accepted since
the second century BC.) The first real treatment of negative numbers in Europe appears in Ars
Magna (The Great Art) in 1545 by Gerolamo Cardano who will show up again in just a bit. At
this point if we have the integers we can also get fractions—ratios of integers. However numbers
which are not the ratio of integers,
p
2 and  to name a couple, were more difficult to accept.
Once you have all of these you can represent numbers as points along a line—the real number
line.
Numbers are abstractions; and as you can see, abstractions are difficult to comprehend—
until they are not. One thing that helps the transition from incomprehensible to obvious is
repeated exposure. The concepts we will study in these modules are nowhere near as abstract as
numbers so the transition to obvious should be relatively swift. You just need to be willing to
expose yourself to these concepts repeatedly.


7
M O D U L E
1
Complex Numbers
I doubt if you spend too much mental energy trying to comprehend a neg-
ative number. But, for the longest time in the history of the world west of
India, a negative number was considered, “absurd,” or“false,” or as late as
the 19th century “imaginary.” Somebody who didn’t like negative numbers
was the Egyptian (or Greek if you are talking to a Greek) mathematician
Hero (or Heron) of Alexandria. Hero was looking for the volume of slices
of a pyramid and ended up with a result of
p
81   144 or
p
 63. Instead of
marching down the road to fame and fortune he let his aversion for nega-
tive numbers take over and rewrote that as
p
63. This aversion to negative
numbers in Europe continued even to the late 17th century. In an 1897 paper W. W. Beman
refers to the book A Treatise of Algebra by John Wallis (Figure 1.1) in which Wallis while pro-
moting the idea of imaginary numbers says
But it is also Impossible that any Quantity (though not a Supposed Square) can be
Negative. Since that it is not possible that any Magnitude can be Less than Nothing
or any Number Fewer than None.
This, by the way, is the same John Wallis who deciphered coded messages from Charles I
which at least tangentially lead to the beheading of Charles I. Mathematics is a bloody field.
Given that you have accepted a negative number since the days when your age was in the
single digits you really shouldn’t get uptight about complex numbers. The concept of complex
numbers first comes up in Gerolamo Cardano’s Ars Magna as the solution of the problem of
finding two numbers whose sum is equal to 10 and whose product is 40 (these would be 5 ˙
p
 15). “As subtle as it is useless” was how Cardano saw the imaginary number. His contempt for
these numbers was shared by Renee Descartes who coined the term “imaginary,” which shows
everyone is capable of saying dumb things. There is nothing imaginary, or more imaginary about
imaginary numbers. They are as imagined as real numbers. The effect of the name imaginary
probably did have some effect in keeping complex numbers somewhat obscure. Gauss in a paper
in 1831 alludes to this:
If C1,  1,
p
 1, had not been called positive, negative, imaginary (or impossible)
unity, but perhaps direct, inverse, lateral unity, such obscurity could hardly have been
suggested.

8
1. COMPLEX NUMBERS
Figure 1.1: The title page of A Treatise of Algebra.
Wallis along with his role in the beheading of Charles I (though to be fair he did ar-
gue against the execution) also came close to coming up with a geometric way of representing
complex numbers called the polar representation. While Wallis came close, the Danish (or Nor-
wegian if you are talking to a Norwegian) cartographer Caspar Wessel in 1797 presented the first
exposition of the Cartesian representation of complex numbers. While he was the first, the rep-
resentation is known as the Argand diagram after the Swiss amateur mathematician Jean-Robert
Argand who independently came up with the same representation. We don’t know much about
Argand except that at the time he came up with this he was working in Paris as a bookkeeper.
He published his work in a pamphlet entitled Essay on the Geometric Interpretation of Imaginary
Quantities. Someone sent a copy of the pamphlet to the French mathematician Adrian–Marie
Legendre who mentioned it in a letter to another mathematician Francois Francais (and if you
didn’t figure out that he was French you need to go get some coffee) without mentioning Ar-

1.1. EULER’S FORMULA
9
Imaginary axis
x
y
r
θ
Real axis
Figure 1.2: A point in the complex plane.
gand’s name. As it happens to all mortals Francois died and his brother Jacques found the letter
among his papers. Jacques submitted this idea of a geometric representation of complex num-
bers for publication, mentioning that he got the idea from Legendre’s letter and requesting the
author of the pamphlet mentioned by Legendre contact him. Argand actually saw the paper,
contacted Jacques, and got credit for his work. Wessel got his credit posthumously. For more
heart stopping stories like this I recommend An Imaginary Tale: The Story of
p
 1.
In this geometric representation of Wessel, Argand, and almost Wallis, complex numbers
are points on the complex plane whose two orthogonal axes are the real number line and the
imaginary number line. A point in two dimensional space can be represented in a number of
ways. We can represent it by its coordinates—the real and imaginary values. Or we can see it
as a vector in two dimensional space with a magnitude and an angle. The first representation is
nowadays called the Cartesian representation while the second is called the polar representation.
The Cartesian representation of the complex number z shown in Figure 1.2 is x C jy where
j D
p
 1. The polar representation of the same number is rej. Converting between polar and
Cartesian coordinates is made very simple because of an identity known as Euler’s formula.
1.1
EULER’S FORMULA
There are some weird and wonderful things in the world. Three of them are e,  and our recent
friend j. We already know about j (or i as people not blessed to be engineers refer to it). The
other two, e and , are irrational numbers—that is they cannot be written as a ratio of two
integers (the irrational comes from ratio not some psychotic tendency of the number), which
show up often when we try to describe relationships in the world. The number  is the value
we get when we divide the circumference (or periphery) of a circle by its diameter (despite the
best efforts of some members of the Indiana legislature). While  has been known in one way or
another since at least the Egyptian and Babylonian civilizations, the number e has a more recent
history and was first discovered by Bernoulli in 1683 when he was trying to solve a compound

10
1. COMPLEX NUMBERS
interest problem. Both constants show up in all kinds of places. One is in an equation that a
fifteen year old Richard Feynman in 1933 called “the most remarkable formula in math.”
ej C 1 D 0
This equation, called Euler’s identity, is a special case of Euler’s formula:
ejx D cos.x/ C j sin.x/
developed by the Swiss mathematician (and everything else) Leonhard Euler who was a student
of Bernoulli and one of the greatest (known) mathematicians of all times. (He loses a few points
for coining the i D
p
 1 notation.) This is a relationship we will use often so let’s take some
time and convince ourselves of its validity. We can easily derive this relationship using a power
series expansion of all terms around x D 0. Recall the power series expansion of f .x/ around
x D 0 is given by
f .x/ D f .0/ C f 0.0/
1Š
x1 C f 00.0/
2Š
x2 C f 000.0/
3Š
x3 C f 000.0/
4Š
x4 C   
where prime (’) corresponds to derivative.
If f .x/ D cos.x/ then
f .0/ D cos.0/ D 1
f 0.x/ D   sin.x/
f 0.0/ D   sin.0/ D 0
f 00.x/ D   cos.x/
f 00.0/ D   cos.0/ D  1
f 000.x/ D sin.x/
f 000.0/ D sin.0/ D 0
f 0000.x/ D cos.x/
f 0000.0/ D cos.0/ D 1
Therefore,
cos.x/
D
1 C 0
1x C  1
2Š x2 C 0
3Šx3 C 1
4Šx4 C   
D
1   x2
2Š C x4
4Š C   
Similarly if f .x/ D sin.x/
f .0/ D sin.0/ D 0
f 0.x/ D cos.x/
f 0.0/ D cos.0/ D 1
f 00.x/ D   sin.x/
f 00.0/ D   sin.0/ D 0
f 000.x/ D   cos.x/
f 000.0/ D   cos.0/ D  1
f 0000.x/ D sin.x/
f 0000.0/ D sin.0/ D 0

1.1. EULER’S FORMULA
11
Therefore,
sin.x/
D
0 C 1
1x C 0
2x2 C  1
3Š x3 C 0
4Šx4 C   
D
x   x3
3Š C x5
5Š C   
Finally, let f .x/ D ejx.
f .0/ D e0 D 1
f 0.x/ D jejx
f 0.0/ D je0 D j
f 00.x/ D j 2ejx
f 00.0/ D j 2e0 D  1
f 000.x/ D j 3ejx
f 000.0/ D j 3e0 D  j
f 0000.x/ D j 4ejx
f 0000.0/ D j 4e0 D 1
f .5/.x/ D j 5ejx
f .5/.0/ D j 5e0 D j
Therefore,
ejx
D
1 C j
1 x C  1
2Š x2 C  j
3Š x3 C 1
4Šx4 C j
5Šx5 C   
D
1   x2
2Š C x4
4Š      C j

x   x3
3Š C x5
5Š     

D
cos.x/ C j sin.x/
This is Euler’s formula! Something we will use often in many different ways. One way we use
this identity is to write cos.x/ and sin.x/ in terms of ejx. In order to see how to do this we begin
with the expression for e jx. We can write this as ej. x/ and use Euler’s formula
ej. x/
D
cos. x/ C j sin. x/
D
cos.x/   j sin.x/
where we have used the fact that cos. x/ D cos.x/ and sin. x/ D   sin.x/; cos.x/ is an even
function and sin.x/ is an odd function. So we have
ejx
D
cos.x/ C j sin.x/
e jx
D
cos.x/   j sin.x/
Adding both equations we get
ejx C e jx D cos.x/ C j sin.x/ C cos.x/   j sin.x/ D 2 cos.x/
from which we get
cos.x/ D ejx C e jx
2

12
1. COMPLEX NUMBERS
If we subtract the second equation from the first
ejx   e jx D cos.x/ C j sin.x/   Œcos.x/   j sin.x/ D 2j sin.x/
which leads to
sin.x/ D ejx   e jx
2j
1.2
CARTESIAN REPRESENTATION
In the cartesian representation the complex number, which is a point in the complex plane, is
written in terms of the two coordinates—the real part and the imaginary part. If the number is
4 C j 5 it’s easy to see which is the real part and which is the imaginary part. But what about
jej3 or
3j
3C4j ? What is the real part and what is the imaginary part of each of these numbers?
To answer that we need two relationships, Euler’s formula (of course) and the concept of the
complex conjugate. The conjugate of a complex number is simply the complex number with j
replaced by  j. So the complex conjugate of 4 C j5 is 4   j5, the complex conjugate of jej3
is  je j3 and the complex conjugate of
3j
3C4j is
 3j
3 4j . Multiplying a number with its complex
conjugate results in a positive real number. If we view the complex number as a vector in the
complex plane, multiplying a number with its complex conjugate gives us the magnitude squared
of that vector. Armed with these facts lets see how to find the real and imaginary parts of jej3
and
3j
3C4j .
jej3 D j.cos.3/ C j sin.3// D j cos.3/ C j 2 sin.3/ D j cos.3/   sin.3/
Re
˚
jej 3	
D   sin.3/
Im
˚
jej 3	
D cos.3/
For
3j
3C4j , the first thing we want to do is remove the complex number from the denom-
inator. We can do that by multiplying the denominator with its (the denominator’s) complex
conjugate. If we just did that, that would change the number. So we multiply both the numer-
ator and the denominator with the complex conjugate of the denominator,
3j
3 C 4j D
3j
3 C 4j  3   4j
3   4j
D
3j.3   4j/
.3 C 4j/.3   4j/
D
9j   12j 2
9   12j C 12j   16j 2
D 9j C 12
9 C 16
D 12
25 C j 9
25

1.3. SUMMARY
13
1.3
SUMMARY
In this module we looked at complex numbers and their representations. Given a complex num-
ber z
•
we can represent it in polar coordinates as
z D rej
•
we can represent it in Cartesian coordinates as
z D x C jy
We also introduced and proved Euler’s excellent formula
ejx D cos.x/ C j sin.x/

14
1. COMPLEX NUMBERS
1.4
EXERCISES
(Answers on the following page)
1. Using Euler’s formula evaluate the following complex numbers:
(a) z D ej=2
(b) z D e j=2
(c) z D ej
(d) z D e j
(e) z D ej2
(f) z D e j2
(g) z D ejn where n is an integer
2. Find the Cartesian representation of the following complex numbers
(a) z D 1=.1 C ejx/ where x is a real number
(b) z D 1=j
3. Write z in rectangular form, x C jy, where x and y are real valued and j D
p
 1
(a) z D jej
(b) z D
1Cj
4Cj3
(c) z D
j
2e3j C1
(d) z D 2ej3C1
j
(e) z D 1Cj
1 j
4. Using Euler’s formula derive the following:
cos2./ D 1
2.1 C cos.2//

1.5. ANSWERS
15
1.5
ANSWERS
1.
(a) j
(b)  j
(c)  1
(d)  1
(e) 1
(f) 1
(g) . 1/n
2.
(a)
1
2   j
sin.x/
2C2 cos.x/
(b)  j
3.
(a)   sin.1/ C j cos.1/
(b)
7
25 C j 1
25
(c)
2 sin.3/
4 cos.3/C5 C j 2 cos.3/C1
4 cos.3/C5
(d) 2 sin.3/   j.2 cos.3/ C 1/
(e) j
4.
cos2./
D
 
ej C e j
2
!2
D
ej 2 C e j 2 C 2
4
D
2
4 C 1
2
 
ej 2 C e j 2
2
!
D
1
2 .1 C cos.2//


17
M O D U L E
2
Functions
While numbers are abstractions you have been familiar with from child-
hood, functions are a different matter altogether. These are abstractions
that you might think you understand (and maybe you do) but there is a
good chance that you are missing some essential aspect of it. Formally, a
function is a mapping from one set, generally called the domain, to another
set, called the range.
Functions will be important to us because we will model, or abstract,
signals as functions. If the signal is an image it will be a function of the
spatial coordinates of the pixels, i.x; y/. If the signal is a video it will be
a function of the two spatial coordinates and time, v.x; y; t/. However, most of the signals we
study in this class will be functions of only one variable—time. Pictorially we could draw it as
shown in Figure 2.1
Or we can think of a function as a machine which given an argument generates a value
(Figure 2.2).
The function operates on its arguments to generate a value. You can specify a function in
a number of ways. You can set up a table which gives a value from the range for each value from
the domain, or you can describe a rule which contains a set of operations to be performed on
the value from the domain to generate a value from the range. An easy way of describing the set
of operations is often a formula. For example, consider the function x defined as
x.t/ D t2 C 2
Pet
Apple
Cat
Fruit
Spinach
Peach
Dog
Lettuce
Vegetable
Figure 2.1: A function.

18
2. FUNCTIONS
x
f ( )
y
Figure 2.2: A function imagined as a machine.
2
x(t)
t
Figure 2.3: The function t2 C 2 imagined as a machine.
if the domain is the set containing the entire real number line, the range will be the set of
non-negative real numbers greater than or equal to 2. The function maps each real number by
following the rule above to generate a positive real number. Or we can think of the function as
a machine, as shown in Figure 2.3, that will take in a number, multiply it with itself and then
add 2 to the result.
Let’s see what happens when we put in different values of t. If we put in 1 in place of t into
this machine we will get 1  1 C 2 or 3. If we put in  2 instead of t we will get  2   2 C 2 or 6.
What if we put in t   1 instead of t? Well, the rule hasn’t changed, only the input. So, let’s follow
the rule which is to multiply the input with itself and then add 2 and we get .t   1/  .t   1/ C 2,
or .t   1/2 C 2.
Often we will be working with functions that are piece-wise continuous. An example of
such a function is
x.t/ D
8
<
:
1
0  t < 1
2
1  t < 2
0
otherwise
graphed in Figure 2.4. A graph expresses the input output relationship of a function so it is
a valid representation of a function. Let’s examine how this function looks as we modify the
argument—change the input. Let’s begin with a simple input. Let’s look at what happens if we
replace t in the argument with t   1. If what is going into the machine is t   1 instead of t then

2. FUNCTIONS
19
–2
–1
1
3
t
x(t)
2
2
1
Figure 2.4: A piecewise continuous function.
–2
–1
1
3
t
x(t – 1)
2
2
1
Figure 2.5: A piecewise continuous function.
we replace t with t   1 in the description of the function.
x.t   1/ D
8
<
:
1
0  t   1 < 1
2
1  t   1 < 2
0
otherwise
Notice that we replaced t with t   1 wherever t appears in the original function description. We
can now simplify the inequalities on the right hand side of the equation.
x.t   1/ D
8
<
:
1
1  t < 2
2
2  t < 3
0
otherwise
If we look at a graph of x.t   1/ we can see that replacing t with t   1 results in a shift of the
function to the right by 1 (Figure 2.5). If you did the same thing replacing t with t C 1 instead
of t   1 you would get a shift of the graph of the function to the left by 1.
What if we change the sign of t. Let’s look at what happens with the simplest of examples
x. t/. Again how we find out the effect of this change is to feed  t instead of t into the machine
which is our function.
x. t/ D
8
<
:
1
0   t < 1
2
1   t < 2
0
otherwise

20
2. FUNCTIONS
–2
–1
1
3
t
x(–t)
2
2
1
Figure 2.6: Flipping the function.
Multiplying through by  1 for the inequalities on the right hand side (and in the process re-
versing the inequalities) we get
x. t/ D
8
<
:
1
0  t >  1
2
 1  t >  2
0
otherwise
or we can write this as
x. t/ D
8
<
:
1
 1 < t  0
2
 2 < t   1
0
otherwise
which we have plotted in Figure 2.6. The effect of replacing t with  t has been to flip the function
around the vertical axis. We can also think of this as mirroring the function around the vertical
axis.
We can do many things to the argument and get many different effects. The important
thing to note is that regardless of what we do to the argument of the function we can figure out
the effect if we treat the argument as the input to a machine whose input/output relationship is
given by the expression defining the function. As long as we do this we will always be able to
figure out what kind of transformation we are working with.
2.1
TYPES OF FUNCTIONS
Functions can be classified in many different ways; the classification used depending on the
application. Let’s look at a couple of ways of classifying functions that will be useful to us.
2.1.1
CONTINUOUS AND DISCRETE TIME FUNCTIONS
Depending on the domain of the function we can define two type of functions, discrete functions,
and continuous functions. Discrete functions are defined over a countable domain. That is, the

2.1. TYPES OF FUNCTIONS
21
domain can at least in theory be listed. Here is a function with a domain consisting of integers.
xŒn D
8
ˆˆˆˆ<
ˆˆˆˆ:
1
n D 0
 1
n D 1
3
n D 2
2
n D 4
0
all other integers
Here, in theory, I could write down all values of the domain for which the function x generates
an answer. Outside of these values the function is not defined. In this example we know the
value of xŒ1 or xŒ10; but there is no value for xŒ1:5. It is not that xŒ1:5 is zero, the expression
xŒ1:5 simply does not make sense in this context.
A continuous function is defined over a continuous connected range of values. For example
x.t/ D
 t
for   1  t  1
0
otherwise
In this case the function is defined for all values of t. The values for which it is defined are
continuous. If the function tells us the value at  1 and at  0:5 it will also tell us the value of the
function at any point between  1 and  0:5.
The reason we are interested in continuous and discrete functions is because the signals
we are interested in will often be one type or another. When I say the word “testing” into a
microphone the output is a continuous function of time. At each instant of time, no matter
how close it is to another instant of time the speech waveform has a value. It is a continuous
function that maps instants of time into voltage values. There are an infinite number of time
instances in each second and the function can (at least in theory) map each of those instances
of time into a value. If we now sample this speech signal at 8000 samples per second—as is in
your phone—we have a discrete function. This function only has values for 8000 points in time
during each second. It is not defined for any other values of time. If we save this sampled speech
we can think of this signal as a function of integer indices independent of time. The first sample
would be xŒ1, the second sample would be xŒ2, etc. We can process this speech sample without
any reference to the “actual” time. Only when we need to replay this signal do we have to be
concerned with the “actual” time again. In this class we will examine and analyze both types of
signals and the systems that operate on them.
2.1.2
EVEN AND ODD FUNCTIONS
Analysis means to break things up into its constituent parts; to decompose a complex problem
into simpler components. In this class we will look at different ways of taking a signal and
breaking it up. Most of these approaches will require breaking things up into an infinite number
of constituents. Before we head off to infinity lets take a look at a very simple decomposition
which will be useful to us: the decomposition of a signal into an even function and an odd
function.

22
2. FUNCTIONS
x(–t)
–t
–t
t
t
x(–t)
x(t)
x(t)
Even function
Odd function
Figure 2.7: An even function and an odd function.
A function x.t/ is said to be even if
x. t/ D x.t/
It is said to be odd if
x. t/ D  x.t/
As we can see from Figure 2.7 an even function is one which the function on both sides of
the vertical axis are mirror images of each other. This should have been evident from the example
we did earlier with x. t/. We can also think of an odd function in terms of symmetry. But this
time the mirror would be placed at a  45ı angle to the vertical axis. Thus, what is positive on
one side of the axis becomes negative on the other side and vice versa.
There are some properties of even and odd functions that at times come in handy. One such
property is that the integral of a continuous odd function over a symmetric interval is zero while
the integral of a continuous (discrete) even function is equal to twice its integral over the positive
or negative half of the integral. This means that if we could decompose a signal into an even and
an odd part its integral over a symmetric interval would be equal to twice the integral of just the
even part over half the interval. Of course in order to do this we would have to decompose our
signal into an even and an odd function. It turns out that we can always decompose any signal
into an even and odd part. Let’s call the even component of x.t/, xe.t/, and the odd part of x.t/,
xo.t/. Then we can obtain xe.t/ and xo.t/ by
xe.t/
D
x.t/ C x. t/
2
xo.t/
D
x.t/   x. t/
2

2.2. SUMMARY
23
We can check that xe.t/ is really even and xo.t/ is really odd by seeing if they satisfy the definition
of an even function and an odd function.
xe. t/
D
x. t/ C x.Ct/
2
D
x.t/ C x. t/
2
D
xe.t/
xo. t/
D
x. t/   x.Ct/
2
D
 x.t/   x. t/
2
D
 xo.t/
If we add the even and odd parts back we should get back the original function.
xe.t/ C xo.t/
D
x.t/ C x. t/
2
C x.t/   x. t/
2
D
x.t/ C x. t/ C x.t/   x. t/
2
D
2x.t/
2
D
x.t/
Let’s find the even and odd parts of
x.t/ D 3t C 2
xe.t/
D
3t C 2   3t C 2
2
D 2
xo.t/
D
3t C 2 C 3t   2
2
D 3t
Clearly adding xe.t/ D 2 and xo.t/ D 3t will give us back x.t/ D 3t C 2.
2.2
SUMMARY
In this module we looked at functions as rules for generating an output given an input. Keeping
this view of functions in mind will help you in what follows.

24
2. FUNCTIONS
2.3
EXERCISES
(Answers on the following page)
1. Given the function x.t/
x.t/ D t2 C 2
(a) What is x.3/‹
(b) What is x. 100/‹
(c) What is x.2t C 1/‹
2. Given the signal
x.t/ D
 t
0  t  1
0
otherwise
Find x.2   t/
3. Given the signal
xŒn D
8
<
:
1
n
n D 1; 2; 3
0
otherwise
Find xŒ2   n
4. Find the even and odd components of x.t/ D ej3t?
5. Which of these are even, which of these are odd, and which of these are neither even nor
odd?
(a) x.t/ D t2
(b) x.t/ D t
(c) x.t/ D cos.t/
(d) x.t/ D sin.t/
(e) x.t/ D ej3t
(f) x.t/ D 3t C 2
(g) xŒn D 4n
(h) xŒn D 2n   2
6. Given
(a) Plot x.t/
(b) Plot x.t   1/

2.3. EXERCISES
25
(c) Plot x.1   t/
7. Find the even and odd parts of the following functions
(a) x.t/ D 1   e 2t
(b) x.t/ D 1   e 2t2
(c) x.t/ D 3t3
(d) x.t/ D ej2t
(e) x.t/ D cos.2t/u.t/
(f) x.t/ D sin.2t/u.t/

26
2. FUNCTIONS
2.4
ANSWERS
1.
(a) 11
(b) 10,002
(c) 4t2 C 4t C 3
2.
x.2   t/ D
 2   t
1  t  2
0
otherwise
3. Given the signal
xŒ2   n D
8
<
:
1
2   n
n D  1; 0; 1
0
otherwise
(If you plot both xŒn and xŒ2   n you might find it instructive.)
4. cos.3t/, j sin.3t/
5.
(a) even
(b) odd
(c) even
(d) odd
(e) neither even nor odd
(f) neither even nor odd
(g) odd
(h) neither even nor odd
6. See Figure 2.8.
7.
(a)
xe.t/
D
1   1
2
 e2t C e 2t
xo.t/
D
1
2
 e2t   e 2t
(b)
xe.t/
D
1   e 2t2
xo.t/
D
0

2.4. ANSWERS
27
x(t)
t
4
2
0
–1
0
1
2
3
x(t–1)
t
4
2
0
–1
0
1
2
3
x(1–t)
t
4
2
0
–1
0
1
2
3
Figure 2.8: Plots of x.t/, x.t   1/, and x.1   t/.
(c)
xe.t/
D
0
xo.t/
D
3t3
(d)
xe.t/
D
cos.2t/
xo.t/
D
j sin.2t/

28
2. FUNCTIONS
(e)
xe.t/
D
1
2 cos.2t/
xo.t/
D
1
2 .cos.2t/u.t/   cos.2t/u. t//
(f)
xe.t/
D
1
2 .sin.2t/u.t/   sin.2t/u. t//
xo.t/
D
1
2 sin.2t/

29
M O D U L E
3
Special Functions
There are some special functions that are going to be particularly useful to
us when we develop tools for the analysis of signals. The two most impor-
tant special functions we will be using are the step function and the delta
function (also known as the impulse function). There is both a discrete and
a continuous version of each.
3.1
STEP FUNCTION
The discrete unit step function has a value of one when its argument is
greater than or equal to zero, and it has a value of zero when its argument
is less than zero.
uŒn D
 1
n  0
0
n < 0
The continuous time unit step function is defined in a similar manner
u.t/ D
 1
t > 0
0
t < 0
The only difference is that we will be agnostic about the value of u.t/ at t D 0 (Figure 3.1).
Remember that these are functions and therefore, they have an input (whatever is in the
parentheses) and an output (the value of the function). So given that
u.t/ D
 1
t > 0
0
t < 0
–3
–2
–1
1
2
3 n
u[n]
u(t)
Discrete step function
Continuous step function
t
Figure 3.1: Discrete and continuous time step functions.

30
3. SPECIAL FUNCTIONS
1
u(t–1)
t
Figure 3.2: A step function shifted to the right by 1.
what is u.t   1/? In order to find this we substitute t   1 wherever we find t in the original
definition.
u.t   1/ D
 1
t   1 > 0
0
t   1 < 0
or
u.t   1/ D
 1
t > 1
0
t < 1
We can plot this as shown in Figure 3.2.
Many of the signals we work with start at a particular time, prior to which they are zero.
The unit step function allows us to write these functions using a shorthand notation. So, instead
of
x.t/ D
 t
t > 0
0
t < 0
we can write
x.t/ D tu.t/
Notice we are multiplying two functions. We do this by multiplying them point by point as
shown in Figure 3.3. So for all values of t less than zero the function t is multiplied by zero and
for values of t greater than 0 we multiply the function t by 1.
3.2
DELTA FUNCTION
The delta function or the impulse function is a function that has a nonzero value at only one
point; the point where the argument of the function is zero.
3.2.1
DISCRETE TIME UNIT IMPULSE FUNCTION
In the discrete case this is a very simple function.
ıŒn D
 1
n D 0
0
n ¤ 0

3.2. DELTA FUNCTION
31
–3
–2
–1
1
2
1
2
1
2
1
2
3
–3
–2
–1
1
2
3
–3
–2
–1
1
2
3
Figure 3.3: Multiplying t with the unit step function u.t/.
For all of its simplicity this is a function which can be used in a lot of interesting ways. We can
obtain the unit step function from the delta function as
uŒn D
n
X
kD 1
ıŒk
Think a bit about why this is true. The process will help you understand both functions and the
ways we will use them.
We can also obtain the delta function from the unit step function by
ıŒn D uŒn   uŒn   1
as shown in Figure 3.4.
We can also use the delta function and its shifted versions to write discrete functions in a
compact way. In the previous module we had an example of a discrete function
xŒn D
8
ˆˆˆˆ<
ˆˆˆˆ:
1
n D 0
 1
n D 1
3
n D 2
2
n D 4
0
all other integers

32
3. SPECIAL FUNCTIONS
–3
–2
–1
1
1
2
3 n
u[n]
–3
–2
–1
1
2
3 n
u[n–1]
–3
–2
–1
1
2
3 n
δ[n]
Figure 3.4: ıŒn D uŒn   uŒn   1.
If we plot this function we see that it consists of a delta function at n D 0,  1 times a delta
function at n D 1; 3 times a delta function at n D 2, and 2 times a delta function at n D 4. We
can write this in a much more compact manner as
xŒn D ıŒn   ıŒn   1 C 3ıŒn   2 C 2ıŒn   4
We could also have written this as
xŒn D xŒ0ıŒn C xŒ1ıŒn   1 C xŒ2ıŒn   2 C xŒ4ıŒn   4
In general rather than write the value of xŒn for each different value of n we can write
xŒn D
1
X
kD 1
xŒkıŒn   k
For this particular function most of the terms in the summation will be zero as xŒn is zero
for all n other than n D 0; 1; 2, and 4.

3.2. DELTA FUNCTION
33
–∆/2
∆/2
Area = 1
1/∆
Figure 3.5: The function ı.t/.
3.2.2
CONTINUOUS TIME DELTA FUNCTION OR THE DIRAC DELTA
FUNCTION
While the discrete time delta function is really simple the continuous time delta function is not.
In fact it is not even clear that it is a function. It is better known by its properties than by a
functional relationship. In fact, we can really only define it as a limit. Let ı be a rectangular
function
ı D
8
<
:
1

 
2 < t < 
2
0
otherwise
We plot this in Figure 3.5. Notice that regardless of the value of  the area under the
curve ı.t/ is always 1. The continuous time delta function, or the impulse function is defined
as the limit as  goes to zero of ı.t/.
ı.t/ D lim
!0 ı.t/
As  becomes smaller and smaller the function ı.t/ will become narrower and narrower. At
the same time as the height of ı.t/ is 1= the function will become taller and taller. In the
limit as  approaches zero the width of this function will go to zero and the height will go to
infinity. This rather strange function is the continuous time delta function.
This function is zero everywhere except at 0. And at zero it has a value approaching infinity.
You can see why we have difficulty talking about the continuous time delta function as a function.
Instead we generally focus on its properties which are, after all, what makes the function useful
for us. First, as we noted before, regardless of how small  becomes the area under the rectangle
and hence the integral of this function is   1
 or 1. Therefore, the integral of the continuous
time delta function is 1. Of course, in order for this to be true, the integral limits have to include
the one point where the delta function is nonzero. We can make use of this property in a number
of different ways. Just as in the case of the discrete delta function we can obtain the unit step

34
3. SPECIAL FUNCTIONS
x(t0)
δ(t – t0)
x(t0)δ(t – t0)
t0
t0
Figure 3.6: Sifting.
function from the delta function by using this property.
u.t/ D
Z t
 1
ı./d
As long as t is less than zero the region of integration does not include the one nonzero point of
the delta function and the integral is zero. When t is greater than zero the region of integration
includes the nonzero point of the function and the integral is one.
We can use the delta function to pick out particular values of other functions. By inte-
grating any function with the delta function shifted to some point t0, we can find, or sift out,
the value of the function at t0 (Figure 3.6).
Z 1
 1
x.t/ı.t   t0/dt D x.t0/
This is called the sifting property of the delta function. To see how this works consider the
product x.t/ı.t   t0/. The value of ı.t   t0/ is zero everywhere except at t D t0 so the product
x.t/ı.t   t0/ is zero everywhere except at t D t0. At t D t0 the value of x.t/ is x.t0) so the
product at t D t0 of x.t/ and ı.t   t0/ is x.t0/ı.t   t0/. Or, x.t/ı.t   t0/ D x.t0/ı.t   t0/ and
Z 1
 1
x.t/ı.t   t0/dt D
Z 1
 1
x.t0/ı.t   t0/dt D x.t0/
Z 1
 1
ı.t   t0/dt D x.t0/
where we have used the fact that
Z 1
 1
ı.t   t0/dt D 1

3.3. SUMMARY
35
If we change the order of t0 and t we would get the same result. This is because the delta
function is zero everywhere except when its argument is zero and the arguments t   t0 and t0   t
are both zero when t D t0. We can use this sifting property in a way that seems redundant but
will be very helpful later on. Just like we used the discrete delta function to provide a compact
representation of xŒn we can use the continuous delta function to provide a representation of
x.t/.
x.t/ D
Z 1
 1
x./ı.t   /d
The only difference between this equation and the previous equation are the variable sym-
bols. Instead of t0 in the previous equation we have used t and the variable of integration has
changed from t to . So just as
Z 1
 1
x.t/ı.t0   t/dt D x.t0/
we have
Z 1
 1
x./ı.t   /d D x.t/
3.3
SUMMARY
In this module we introduced two very special functions, the unit step function, and the delta
function. We also saw how we can use these special functions to obtain compact representations
of many functions. In particular using the delta function we can write a discrete time function
xŒn as
xŒn D
1
X
kD 1
xŒkıŒn   k
and a continuous time function as
x.t/ D
Z 1
 1
x./ı.t   /d

36
3. SPECIAL FUNCTIONS
3.4
EXERCISES
(Answers on the following page)
1. Plot the following:
(a) x.t/ D u.t/   u.t   2/
(b) x.t/ D u.t C 2/   u.t C 1/
(c) xŒn D uŒn   uŒn   2
(d) xŒn D uŒn C 2   uŒn C 1
(e) x.t/ D u.t/ C u.t   1/   2u.t   2/
2. Evaluate the integral
Z 1
 1
e tu.t/dt
3. Evaluate the integral
Z t
 1
e u./d
4. Evaluate the sum
1
X
nD 1
.0:5/nuŒn
5. Evaluate the sum
1
X
nD 1
.0:5/nıŒn   1
6. Evaluate the integral
Z 1
 1
te tı.t   1/dt
7. Evaluate the integral
Z 1
 1
tı.t   3/dt
8. Evaluate the integral
Z 4
0
tı.t   3/dt
9. Evaluate the integral
Z 2
0
tı.t   3/dt

3.4. EXERCISES
37
10. Evaluate the integral
Z 1
 1
Œu.t/   u.t   1/dt

38
3. SPECIAL FUNCTIONS
–2
–1
1
u(t) – u(t–2)
1
2
–2
–1
1
u(t+2) – u(t+1)
1
2
–2
–1
1
u[n] – u[n–2]
1
2
–2
–1
1
u(t) + u(t–1) – 2u(t–2)
1
2
–2
–1
1
u[n+2] – u[n+1]
1
2
Figure 3.7
3.5
ANSWERS
1. Plot Figure 3.7.
2. 1
3. 1   e tu.t/
4. 2.0
5. 0.5
6. 1=e
7. 3
8. 3
9. 0
10. 1

39
M O D U L E
4
Classification of Systems
Shakespeare did not say, all the world’s a system and the signals merely
inputs and outputs but, while it sounds silly the statement is not entirely
bonkers. All around us we see systems, both natural and manufactured.
Closest to us, our body is an incredibly complex system of systems with
the different systems communicating with each other through chemical
and electrical signals. Clearly, putting together the mathematical machin-
ery required to handle all these systems would be close to impossible and
ultimately self defeating. We want tools that are practical, easy to use, and
still give us an insight into systems we are trying to analyze or design. The
tools that would be able to handle the huge diversity of systems all around us would themselves
need to be so complex as to be impractical. In order to develop practical tools for the analysis
of systems what we need to do is to see if we can identify a group of systems which is broad
enough to be useful and for the analysis of which simple tools can be developed. In order to
define this set of systems we need to develop some terminology to describe systems in general.
That is what we will do in this module. Our general framework is that of a system which has
an input—which we will denote by x.t/ or xŒn and an output which we will denote by y.t/ or
yŒn. With this framework let us look at some properties of systems
4.1
MEMORY
We say a system has memory if the output depends not only on the input at the current time
but also on the input that occurred in the past or will occur in the future. Otherwise the system
is said to be memoryless. The current and past times are defined in terms of the index of the
output. So if the output is y.t/ then the input x.t/ is the current input, the input x.t   1/ is the
value of the input one unit of time in the past and x.t C 1/ is the value of the input one unit of
time in the future. Consider the system
y.t/ D 10x.t/
If I wanted to know the value of the output y.t/ at time t D 3, or y.3/ we would only need
the value of the input x.t/ at time t D 3, or x.3/. We wouldn’t need x.3:0001/ or x.2:9999/.
We wouldn’t need to “remember” any value of the input. We would only need the current value
of the input. This lack of the requirement for remembering is why we call a system memoryless.
Other examples of memoryless systems include

40
4. CLASSIFICATION OF SYSTEMS
yŒn D 3xŒn2
y.t/ D cos .x.t//
In each case notice that in order to know the output at a particular time we only need
the input at that particular time. The system does not have to “remember” the past or (and this
sounds a bit weird) the future.
So, what does a system with memory look like. Here is an example. The system with an
input/output relationship
yŒn D xŒn   xŒn   1
is not a memoryless system because it uses the value of the input one time instant in the past.
The output at time n D 3 is given by
yŒ3 D xŒ3   xŒ3   1 D xŒ3   xŒ2
So in order to find the output at time n D 3 we need the value of the input at both time n D 3
and at time n D 2. The system has to remember the value of the input one time instant prior to
the current time instant, therefore, it has to have memory. Another system with memory is the
system with the input/output relationship
y.t/ D 1
2 .x.t C 1/   x.t   1//
Here the system has to remember not only an input from one time unit in the past, it also has
to “remember” the input from one time unit in the future.
Another example is an operation that you use a lot—the derivative operation. The deriva-
tive is not a memoryless operation. Think of the definition of the derivative:
dx
dt D lim
t!0
x.t C t/   x.t/
t
To differentiate a function x.t/ we need the current value of the function and the value
of the function just after the current moment. Therefore, systems that implement a derivative—
think of a differentiator—are not memoryless systems. Neither is another system we use a lot—
an integrator. Integration is not a memoryless operation.
y.t/ D
Z t
 1
x./d
In order to evaluate this integral we not only need the value of x. / at time t, we need its values
at all times prior to t. Therefore, integrators are not memoryless systems.
In terms of simple circuits, purely resistive circuits are memoryless systems. Circuits that
contain capacitors or inductors are systems with memory.

4.2. INVERTIBILITY
41
4.2
INVERTIBILITY
If we can recover the input exactly from the output the system is said to be invertible. The system
described by the input/output relationship
y.t/ D 10x.t/
is invertible as we can obtain the input x.t/ given the output y.t/.
x.t/ D 1
10y.t/
The system described by the input/output relationship
yŒn D x.n/2
is not invertible. If we are told that yŒn D 4 we cannot say if xŒn D 2 or xŒn D  2. Neither is
the system described by the input/output relationship
y.t/ D cos .x.t//
If we know y.t/ we only know x.t/ within 2. For example, if we know that y.t/ D 1=
p
2, x.t/
could be =4 or 2:25, or 4:25, and so on. Therefore, the this system is not invertible without
making some assumptions.
For this case the invertibility or the lack of it was fairly obvious. In other cases it may not
be that easy to see whether a system is invertible. Consider the summer
yŒn D
n
X
kD 1
xŒk
This looks fairly complicated and at first glance you may be tempted to say that this system
is not invertible even though it is. One reason for this confusion might be that in the examples
we have been talking about knowledge of the output seems to be the knowledge of the output
at only the current time. Actually, when we say we have knowledge of the output we mean we
have knowledge of the output for all time. Therefore, in this particular case, we not only know
yŒn we also know yŒn   1, and given these two we can find the value of xŒn. Consider,
yŒn D
n
X
kD 1
xŒk
D
   C xŒ 1 C xŒ0 C    C xŒn   1 C xŒn
yŒn   1 D
n 1
X
kD 1
xŒk
D
   C xŒ 1 C xŒ0 C    C xŒn   1

42
4. CLASSIFICATION OF SYSTEMS
Subtracting the lower sum from the top one we get
yŒn   yŒn   1 D xŒn
In other words, given the output values we can obtain the input values.
The idea of invertibility becomes particularly important when we are trying to understand
the behavior of systems we haven’t designed. For example, we can understand that somebody has
atrial fibrillation by observing the ECG—which in some sense reflects the output of the heart,
but what we may be really interested in is what was causing the atrial fibrillation—in other words
what inputs to the system which is the heart were causing the atrial fibrillation. Similar problems
occur when we are trying to investigate geophysical systems, or even economic systems.
4.3
CAUSALITY
A system is said to be causal if the output is not based on the future values of the input. So,
yŒn D xŒn C yŒn   1
describes a causal system, while
y.t/ D 1
2 .x.t C 1/   x.t   1//
does not.
When we discussed memoryless systems the past and future were interchangeable. A sys-
tem which needed to remember the past to generate an output needed memory as did a system
which used a future value to generate an output. When we speak of causality we differentiate
between past and future values. A system which does not use future values of the input to gener-
ate the current output is a causal system. The two systems described above are simple examples
of causal and non-causal systems.
The term causality is a somewhat loaded term here. In our day to day life we generally
always have the cause of something precede its effect. You hit the snooze button and the alarm
stops. You open your hand and the cup you were holding drops to the floor. You turn the key
in the ignition and the car starts. However, in terms of engineering, causality is not a necessary
requirement for systems. Think of an image processing system in which you smooth out the
image by replacing each pixel by the average of all the neighboring pixels. If we think in terms
of a raster scan the neighboring pixels include pixels that occur prior to the current pixel as well
as the pixel that occurs after the current pixel. So the smoothing system uses information from
the “future” to process the current pixel.
Somewhat more complex are some of the systems you use everyday. When you are using
your cell phone you are using a non-causal system. The cellphone does not transmit your voice
samples. Instead it breaks up the voice samples into frames or sets of samples which are used

4.4. STABILITY
43
to generate a model for the speech. This model, along with some other information is used to
regenerate the speech segment at the receiver. In order to regenerate each sample of a frame the
receiver is using the model which was created using all the samples in the frame—past, current,
and future. The cellphone thus is a non-causal system. When you view digital videos you are
generally using a non-causal system. Generally the video you see consists of three different kinds
of frame, I frames P frames and B frames. The I frames can be thought as simply images. You
could print one out and you would have a picture. A P frame is actually just the difference
between pixel values of a a previous frame and the current frame and a B frame is the difference
in pixel values between the current frame and past and future values. Because you need to use
future values to reconstruct the current frame the system is non-causal.
Note that we are only talking about future values of the input. An input/output relation-
ship given by
y.t/ D .t C 1/x.t/
is a causal system. The t C 1 in the equation has nothing to do with the input. At time t D 2
you can figure out the value of t C 1 without any need for precognition. Remember to keep this
in mind when you are evaluating a system to figure out whether it is causal.
4.4
STABILITY
There are a number of ways one can define stability. The definition we use here is that a system is
stable if for a bounded input the output is always bounded—this is also known as Bounded Input
Bounded Output (BIBO) stability. A more formal way of saying this is if you are guaranteed
that there is a finite constant B such that
jx.t/j < B
for all t
then you can find a finite value C such that
jy.t/j < C
for all t
So which of these is BIBO stable?
y.t/ D 10x.t/
yŒn D exŒn
y.t/ D
1
x.t/
y.t/ D
Z t
 1
x./d
Let’s take each in turn. In the case of the first system
y.t/ D 10x.t/

44
4. CLASSIFICATION OF SYSTEMS
if we are guaranteed that jx.t/j < B for some finite B, we can pick C D 10B and we are guar-
anteed that jy.t/j will be less than C for all t. You tell me that jx.t/j will be less than 15 for all
values of t and I can give you a guarantee that jy.t/j will be less than 150 for all values of t.
For the system in which
yŒn D exŒn
If xŒn is guaranteed to be less than B, we can pick C D eB and guarantee that jyŒnj is
less than C. So this system is also BIBO stable.
This is not the case for the third system defined by
y.t/ D
1
x.t/
No matter what value of C you pick we can find a value of x.t/ small enough (and hence
less than any bound B) such that 1=x.t/ is greater than any given value of C.
Our last system is also not BIBO stable. Let’s say you give me a guarantee that jx.t/j is
less than some number B. Then
jy.t/j
<
Z t
 1
Bd
D
B
Z t
 1
d
!
1
The infinite range of the equation makes it impossible to create any guarantee on the upper
bound of jy.t/j. You can say that this is not reasonable because a practical integrator will never
be integrating from  1. Also infinity is a weird concept, it can lead to strange result. As George
Gamow well knew when he wrote:
There was a young fellow from Trinity,
Who took the square root of infinity.
But the number of digits, Gave him the fidgets;
He dropped Math and took up Divinity.
So let’s modify the input/output relationship to
y.t/ D
Z t
0
x./d
Again, let’s assume there is a number B such that jx.t/j < B for all t then

4.5. SUMMARY
45
jy.t/j
<
Z t
0
Bd
D
B
Z t
0
d
D
tB
At first sight that doesn’t look too bad. To see why this does not rescue this particular
system from instability let’s use some numbers. Let’s say B D 10. Let’s suppose we pick C D
1;000;000. That is we are saying that as long as jx.t/j is less than 10, we guarantee that jy.t/j
will be less than 1;000;000. All we have to do to break this guarantee is to pick t D 100;001! So
even without the lower limit of infinity the integrator is not BIBO stable.
A system not being BIBO stable does not make it unusable. Integrators, after all, are an
integral (no pun intended—well, maybe just a little bit) part of many many systems. But, when
you are designing a large system with many subsystems it is useful to know when you are using
something that can go unstable.
4.5
SUMMARY
In this module we defined four properties of systems:
1. Memory: A system has memory if it does not use values of future and past inputs to
generate the current output.
2. Invertible: A system is invertible if we can recover the input given the output.
3. Causality: A system is causal if it does not use future values of the input to generate the
current output.
4. Stability: A system is BIBO stable if when the input is guaranteed to be bounded by a
finite value we can give a guarantee of a finite bound for the output.

46
4. CLASSIFICATION OF SYSTEMS
4.6
EXERCISES
(Answers on the following page)
1. Are the following systems memoryless?
(a) yŒn D xŒn C yŒn   1
(b) y.t/ D ex.t/
(c) y.t/ D log .x.t//
(d) y.t/ D cos.x.t// C sin.x.t   1//
2. Is the system described by the following input-output relationship invertible
yŒn D xŒn C yŒn   1 yŒn D 0 for n < 0‹
3. Are the systems described by the following input output relationships causal?
(a) yŒn D xŒ n
(b) y.t/ D x.t/ cos.t C 1/
4. Are the following systems BIBO stable?
(a) y.t/ D ex.t/
(b) yŒn D xŒn C 4   xŒn   4
(c) y.t/ D t2x.t/
(d) yŒn D nxŒn
5. Determine whether the following systems are (i) causal, (ii) BIBO stable, (iii) memoryless,
and (iv) invertible. In each case show your reasoning. In each case x.t/ or xŒn is the input
and y.t/ or yŒn is the output.
(a) y.t/ D ax.t/ where a is a constant
(b) y.t/ D x.t/ sin.t/
(c) y.t/ D
1
x.t/
(d) y.t/ D
R t
 1 x./d
(e) yŒn D 2xŒn   xŒn   1
(f) yŒn D Pn
kD 1 xŒk C 1
(g) y.t/ D tx.t   2/
(h) y.t/ D cos.x.t//

4.6. EXERCISES
47
Table 4.1: Causal, BIBO statle, memoryless, and invertible systems
Input/Output Relationship
Causal
BIBO Stable
Memoryless
Invertible
y(t) = 3x(t) + 2
y[n] = nx[n]
y(t) = s−∞
2t x(τ)dτ
y[n] = x[n − 2] − 2x[n − 8]
6. Determine whether the following systems are causal, BIBO stable, memoryless, and in-
vertible. In each case x.t/ or xŒn is the input and y.t/ or yŒn is the output (see Table 4.1).

48
4. CLASSIFICATION OF SYSTEMS
Table 4.2: Causal, BIBO statle, memoryless, and invertible systems
Input/Output Relationship
Causal
BIBO Stable
Memoryless
Invertible
y(t) = 3x(t) + 2
Yes
Yes
Yes
Yes
y[n] = nx[n]
Yes
No
Yes
Yes
y(t) = s−∞
2t x(τ)dτ
No
No
No
Yes
y[n] = x[n − 2] − 2x[n − 8]
Yes
Yes
No
Yes
4.7
ANSWERS
1.
(a) No
(b) Yes
(c) Yes
(d) No
2. Yes
3.
(a) No
(b) Yes
4.
(a) Yes
(b) Yes
(c) No
(d) No
5.
(a) Causal, BIBO Stable, Memoryless, Invertible
(b) Causal, BIBO Stable, Memoryless, Invertible
(c) Causal, Not BIBO Stable, Memoryless, Invertible
(d) Causal, Not BIBO Stable, Not Memoryless, Invertible
(e) Causal, BIBO Stable, Not Memoryless, Invertible
(f) Not Causal, Not BIBO Stable, Not Memoryless, Invertible
(g) Causal, Not BIBO Stable, Not Memoryless, Invertible
(h) Causal, BIBO Stable, Memoryless, Not Invertible (see Table 4.2)

49
M O D U L E
5
Linearity and Time Invariance
We have left the two most important properties of systems—the properties
which we will use to define the set of systems which will be our focus
throughout this course for last. These properties are Linearity and Time
Invariance.
5.1
LINEARITY
The linearity property is one of the most important properties we will dis-
cuss. It is this property that allows you to use superposition when you solve
circuits. It is this property together with time invariance which will allow
us to develop a mathematical characterization of many systems of interest.
And it is very straightforward. The linearity property actually consists of two properties scaling
and additivity.
5.1.1
SCALING
The scaling property (also known as the homogeneity property) states that in systems that have
this property if we scale an input by a certain amount the output gets scaled by the same amount.
Symbolically we could write this property as; if
x.t/ ) y.t/
then
˛x.t/ ) ˛y.t/
So, if we double the input, pick ˛ D 2, the output gets doubled. Most importantly if we
pick ˛ to be zero, that is we zero out the input, the output gets zeroed out as well. So the system
given by the input/output equation
y.t/ D 2x.t/ C 3
does not satisfy the scaling property. In order to see this let’s represent this system as shown in
Figure 5.1.
Whatever we put into the system the system multiplies it by 2 and then adds 3 to the
product. So if x.t/ is the input the output is
y.t/ D 2  Œx.t/ C 3 D 2x.t/ C 3

50
5. LINEARITY AND TIME INVARIANCE
3
2
y(t)
x(t)
Figure 5.1: A system with input/output relationship: y.t/ D 2x.t/ C 3.
2
y[n]
x[n]
x[n–1]
2x[n]
Delay
Figure 5.2: A system with input/output relationship: yŒn D 2xŒn C xŒn   1.
If we now multiply the input with ˛ to get ˛x.t/ as the new input to the system, the system
operates in the same way. The output is given by
2  Œ˛x.t/ C 3 D 2˛x.t/ C 3
Notice that this is not what we would have obtained if we had multiplied y.t/ with ˛. If we did
that we would have obtained
˛y.t/ D ˛ Œ2x.t/ C 3 D 2˛x.t/ C 3˛
Clearly
˛x.t/ 6) ˛y.t/
So lets take a look at a system that does satisfy the scaling property. Let the input output
relationship of a discrete time system be
yŒn D 2xŒn C xŒn   1
shown schematically in Figure 5.2
If xŒn is the input to the system, from the upper arm we will get 2  ŒxŒn and from the
lower arm we will get xŒn delayed by one unit, or xŒn   1. The adder adds these two terms and
we get yŒn as shown above.

5.1. LINEARITY
51
Now let’s replace xŒn with ˛xŒn in the upper arm we will get 2˛xŒn and from the lower
arm we will get ˛xŒn delayed by one unit or ˛xŒn   1. Adding the two terms we get
2˛xŒn C ˛xŒn   1
which is exactly ˛yŒn. So this system satisfies the scaling property.
Let’s look at a couple of more examples. Let’s suppose we have a squarer.
yŒn D .xŒn/2
If we multiplied yŒn by ˛ we would get
˛yŒn D ˛ .xŒn/2
What would happen if we put ˛xŒn into the squarer? We would get
.˛xŒn/2 D ˛2 .xŒn/2
which is not the same as ˛yŒn.
Let’s consider a hard limiter—a useful system in many applications. We can model this
with the input output relationship
y.t/ D 2u.x.t//   1
The output of this system is 1 if x.t/ is positive and  1 if x.t/ is negative. Clearly multiplying
x.t/ with ˛ is not going to change the output from being either C1 or  1. So replacing x.t/
with ˛x.t/ will not change y.t/ to ˛y.t/.
A more familiar system is the diode. Let’s assume the diode is ideal and the input/output
relationship is given by
y.t/ D
(
x.t/
x.t/ > 0
0
x.t/ < 0
At first sight this looks like it satisfies the scaling property but if we take alpha to be negative it
clearly does not.
5.1.2
ADDITIVITY
In a system which satisfies the additivity property, the response to a sum of inputs is the sum of
the responses to the individual inputs. Or symbolically, if
x1.t/ ) y1.t/
and
x2.t/ ) y2.t/

52
5. LINEARITY AND TIME INVARIANCE
then
x.t/ D x1.t/ C x2.t/ ) y.t/ D y1.t/ C y2.t/
What are some of the systems that satisfy additivity. Clearly resistive circuits satisfy addi-
tivity otherwise you would not be able to use superposition. We can easily show that the system
shown in Figure 5.2 also satisfies additivity. Let’s apply the definition to it. If we give the system
x1Œn as the input the output will be
y1Œn D 2x1Œn C x1Œn   1
If the input is x2Œn the output will be
y2Œn D 2x2Œn C x2Œn   1
Now, let’s suppose the input is x1Œn C x2Œn. In the upper branch the system will multiply
whatever the input is by 2, so in this branch we will get 2  Œx1Œn C x2Œn. In the lower branch
we will get the input delayed by one, Œx1Œn   1 C x2Œn   1. Adding the two we get 2x1Œn C
2x2Œn C x1Œn   1 C x2Œn   1 which is exactly y1Œn C y2Œn. Therefore, the system possesses
the additivity property.
Another system that satisfies the additivity property is an integrator.
y.t/ D
Z t
 1
x./d
We can validate this easily by noting that
y1.t/ D
Z t
 1
x1./d
y2.t/ D
Z t
 1
x2./d
And if we give the system the input x1.t/ C x2.t/ the output will be
y.t/
D
Z t
 1
Œx1./ C x2./d
D
Z t
 1
x1./d C
Z t
 1
x2./d
D
y1.t/ C y2.t/
(Note that for all this to be true all these integrals have to exist. In practice this is generally not
a problem.)

5.2. TIME INVARIANCE
53
1
t
2
1
t
2
Figure 5.3: Inputs for which outputs are known.
What are examples of systems that do not have the additivity property. All the examples
described above that didn’t have the scaling property also do not have the additivity property.
Another one is the system with the input output relationship
y.t/ D ex.t/
We can easily see that
y.t/ D ex1.t/Cx2.t/ ¤ ex1.t/ C ex2.t/ D y1.t/ C y2.t/
So if a system possesses the scaling property does it always also satisfy additivity? Not necessarily.
Consider
yŒn D xŒnxŒn   1
xŒn   2
I
xŒn ¤ 0 for all n
This will satisfy the scaling property but not the additivity property.
Together the two properties of scaling and additivity give us the linearity property which
says that if a particular input is the weighted sum of other known inputs, the output will be the
weighted sum of the outputs to those known inputs.
˛x1.t/ C ˇx2.t/ ) ˛y1.t/ C ˇy2.t/
One way in which this property can be very helpful is if we can express our inputs as a
weighted sums of known functions. We could record the outputs of the linear system to the
known functions and then we could find the output of any weighted combinations of these
inputs simply by taking the weighted combinations of the known outputs. Suppose our known
functions are the ones shown in Figure 5.3.
If we have a linear system for which we know the outputs for these inputs. We can find the
outputs of the infinite linear combinations of these inputs. A few of the possible combinations of
the inputs are shown in Figure 5.4. Just knowing the outputs for two inputs, we can immediately
obtain the corresponding outputs for any of these infinite inputs if we can find the weights of
the linear combinations of these two inputs.
5.2
TIME INVARIANCE
Time invariance is easy to define and hard to demonstrate. A system is said to be time invariant
if its behavior does not change with time. If you give it an input today and get a particular output

54
5. LINEARITY AND TIME INVARIANCE
1
t
2
1
t
2
1
t
2
1
t
2
1
t
2
1
t
2
Figure 5.4: A few signals obtained using a linear combination of the signals in the previous
figure.
1
1
x(t)
t
2
Figure 5.5: Triangular function.
then you will also get the same output if you give it the same input tomorrow. Mathematically we
can say that if x.t/ results in an output y.t/ then for a time-invariant system x.t   t0/ results in
the output y.t   t0/—shifting the input simply shifts the output. To see the issues involved when
trying to demonstrate wether a system is time invariant or time varying let’s pick a particular
input x.t/—the triangular function (Figure 5.5)
x.t/ D
8
ˆ<
ˆ:
t
0  t  1
 t C 2
1  t  2
0
otherwise
If we delay this function by t0 we will simply move the entire function to the left by t0. To
see this simply replace t with t   t0 in the equation above (Figure 5.6).
x.t   t0/ D
8
ˆ<
ˆ:
t   t0
0  t   t0  1
 .t   t0/ C 2
1  t   t0  2
0
otherwise

5.2. TIME INVARIANCE
55
t0+1
t0
1
x(t–t0)
t
t0+2
Figure 5.6: Replacing t with t   t0.
from which we get
x.t   t0/ D
8
ˆ<
ˆ:
t   t0
t0  t  t0 C 1
 .t   t0/ C 2
t0 C 1  t  t0 C 2
0
otherwise
Now let’s see what happens to the output given two different systems with the following
input output relationships.
y.t/ D 2x.t/
y.t/ D x
 t
2

Lets name the output y.t/ when the input is x.t/ and let’s name it yd.t/ when the input is
x.t   t0/—the original input delayed by t0. The system is time invariant if when we shift the
output y.t/ by t0 to get y.t   t0/, y.t   t0/ D yd.t/.
In the first system if the input is x.t/, the output will be
y.t/ D 2x.t/ D
8
ˆ<
ˆ:
2t
0  t  1
 t C 2
1  t  2
0
otherwise
If we shift the input by t0 so that the input is x.t   t0/
yd.t/ D 2x.t   t0/ D
8
ˆ<
ˆ:
2.t   t0/
t0  t  t0 C 1
 2.t   t0/ C 4
t0 C 1  t  t0 C 2
0
otherwise
If we plot these two functions we see that yd.t/ is simply y.t/ shifted by t0 (Figure 5.7).
So, clearly the system given by the input-output relationship y.t/ D 2x.t/ is time invari-
ant. Now let’s consider the second system. In this case we replace t with t=2.
y.t/ D x
 t
2

D
8
ˆˆˆ<
ˆˆˆ:
t
2
0  t
2  1
  t
2 C 2
1  t
2  2
0
otherwise

56
5. LINEARITY AND TIME INVARIANCE
t0+1
t0
1
2
yd(t)
t
t0+2
1
1
2
y(t)
t
2
Figure 5.7: yd.t/ is simply y.t/ delayed by t0.
2t0
1
yd(t)
t
2t0+2
2t0+4
1
1
y(t)
t
2
Figure 5.8: The system is time varying.
or
y.t/ D
8
ˆˆˆ<
ˆˆˆ:
t
2
0  t  2
  t
2 C 2
2  t  4
0
otherwise
Now let’s do the same for x.t   t0/, i.e., replace t with t=2 in the expression for x.t   t0)
yt0.t/ D x
 t
2   t0

D
8
ˆˆˆ<
ˆˆˆ:
t
2   t0
t0  t
2  t0 C 1
  t
2   t0

C 2
t0 C 1  t
2  t0 C 2
0
otherwise
or
yt0.t/ D x
 t
2   t0

D
8
ˆˆˆ<
ˆˆˆ:
t
2   t0
2t0  t  2t0 C 2
  t
2   t0

C 2
2t0 C 21  t  2t0 C 4
0
otherwise
Clearly yt0 is y.t/ shifted by 2t0, not t0 (Figure 5.8). In other words y.t   t0/ ¤ yt0.t/ and
the system described by the input output relationship y D x.t=2/ is not time invariant. Another
way of saying the same thing is to say that this system is time varying.
In general you will find that whenever in the input/output relationship the argument of
the input is scaled we will get a system that is not time invariant. Another type of system which is

5.2. TIME INVARIANCE
57
x(t)
y(t–t0)
y(t)
System
Delay
x(t)
yd(t)
xd(t)
x(t–t0)
Delay
System
Figure 5.9: Setup for testing for time-invariance.
always time-varying is one in which the input gets multiplied by a function of time. For example
the system
y.t/ D tx.t/
is a time varying system. As is it’s discrete time counterpart
yŒn D nxŒn
If we replace t or n with functions of t in the continuous time and functions of n in the discrete
time we again get systems that are time varying. So
y.t/ D cos.t/x.t/
is the input output relationship of a time varying system.
We can show that these systems are time varying using a simple approach. In this approach
we will first pass the input x.t/ through the system to get y.t/ and then pass y.t/ through a delay
to get y.t   t0/. In parallel we will pass x.t/ through a delay to get x.t   t0/ which we will call
xd.t/. We will then pass xd.t/ through the system to get yd.t/. If yd.t/ D y.t   t0/ the system
is time invariant. We show the setup in Figure 5.9. Let’s test the system with the input output
relationship
y.t/ D tx.t/
We can picture this system as shown in Figure 5.10. The system takes the input multiplies it
with t and spits it out. In the top configuration
y.t/ D tx.t/
is the system output. After the delay we get
y.t   t0/ D .t   t0/x.t   t0/
In the bottom configuration, after the delay we get
xd.t/ D x.t   t0/

58
5. LINEARITY AND TIME INVARIANCE
t
x(t)
y(t)
Figure 5.10: System with input/output relationship y.t/ D tx.t/.
When we now put xd.t/ into the system, the system multiplies it with t and we get
yd.t/ D txd.t/
Replacing xd.t/ with x.t   t0/ we get
yd.t/ D tx.t   t0/
which is not the same as y.t   t0/. Hence this system is not time-invariant.
Why is this property important? When we design a system we would like it to give us the
same output for the same input regardless of when the input was provided. Whether we turn
on the switch at noon or at midnight we would like the light, or the coffee maker, or the car to
behave in the same way. In practice, the car will behave a bit differently if we start it at noon
during the summer or at midnight during the winter so it is not strictly time invariant. But in
practice what we find is that sometimes “good enough for government work” is good enough.
5.3
LINEAR TIME-INVARIANT (LTI) SYSTEMS
If our system was not simply linear but also time invariant then knowing that the input x.t/
results in the output y.t/, we can find the output for a shifted input x.t   t0/ for any value of t0.
This means that knowing the output for the two inputs of Figure 5.3 we would also know the
output of the system for any linear combination of the shifted inputs.
x.t/ D ˛x1.t   t1/ C ˇx2.t   t2/
In Figure 5.11 we have a few of the infinitely many combinations of the weighted and shifted
signals shown in Figure 5.3. Because of this property, which allows us to obtain the outputs of
an infinite number of possible inputs just by knowing the outputs for a few inputs we can build
some powerful analysis tools for these systems—systems that are both linear and time invariant.
This is why for the rest of the course we will focus almost exclusively on linear time-invariant
(LTI) systems.

5.4. SUMMARY
59
1
t
2
1
t
2
1
t
2
1
t
2
1
t
2
1
t
2
Figure 5.11: A few signals obtained using a linear combination of shifts of the signals in Fig-
ure 5.3.
5.4
SUMMARY
In this module we looked at two of the most important properties (for us) of systems - linearity
and time invariance. If we know the output of a linear time-invariant system for a particular
input we also know the output for any linear combination of time shifted versions of that input.
This property will become enormously useful as we will see in the next module.

60
5. LINEARITY AND TIME INVARIANCE
Table 5.1: Time invariant and linear systems
Input/Output Relationship
Linear
Time-invariant
y(t) = 3x(t) + 2
y[n] = nx[n]
y(t) = s−∞
2t x(τ)dτ
y[n] = x[n − 2] − 2x[n − 8]
y[n] = x[sin[n]]]
y(t) = ex(t)
y(t) =  d—
dt x(t)
5.5
EXERCISES
(Answers on the following page)
1. Determine whether the following systems are linear and/or time invariant. In each case
x.t/ or xŒn is the input and y.t/ or yŒn is the output.
(a) y.t/ D ax.t/ where a is a constant
(b) y.t/ D x.t/ sin.t/
(c) y.t/ D
1
x.t/
(d) y.t/ D
R t
 1 x./d
(e) yŒn D 2xŒn   xŒn   1
(f) yŒn D Pn
kD 1 xŒk C 1
(g) y.t/ D tx.t   2/
(h) y.t/ D cos.x.t//
2. Determine whether the following systems are time invariant, and linear. In each case x.t/
or xŒn is the input and y.t/ or yŒn is the output (see Table 5.1).

5.6. ANSWERS
61
Table 5.2: Time invariant and linear systems
Input/Output Relationship
Linear
Time-invariant
y(t) = 3x(t) + 2
No
Yes
y[n] = nx[n]
Yes
No
y(t) = s−∞
2t x(τ)dτ
Yes
No
y[n] = x[n − 2] − 2x[n − 8]
Yes
Yes
y[n] = x[sin[n]]]
Yes
No
y(t) = ex(t)
No
Yes
y(t) =  d—
dt x(t)
Yes
Yes
5.6
ANSWERS
1.
(a) Linear, time-invariant
(b) Linear, not time-invariant
(c) Not linear, time-invariant
(d) Linear, time invariant
(e) Linear, time-invariant
(f) Linear, time-invariant
(g) Linear, not time-invariant
(h) Not linear, time-invariant (see Table 5.2)


63
M O D U L E
6
Linearity, Time-Invariance,
and the Role of the Impulse
Response
Let’s take a particular signal and see how it plays with LTI systems. That
particular signal is the unit impulse. We will see how the response to the
unit impulse of a linear time-invariant system completely characterizes the
system for both continuous time systems and discrete time systems. We
will begin with discrete systems and then go through the process again
with continuous systems.
6.1
THE RESPONSE TO AN
IMPULSE FOR A DISCRETE-TIME SYSTEM
For shorthand we will adopt the notation
xŒn ) yŒn
to denote a system with input xŒn and output yŒn. Let’s suppose the input in Figure 6.1 is a
discrete impulse function. In other words the input xŒn D ıŒn. Let’s give the output a name.
Given that this output is the response of the system to an impulse we will call it the impulse
response of the system and denote it by hŒn. In other words, for xŒn D ıŒn, yŒn D hŒn. In
terms of our notation
ıŒn ) hŒn
Because this is a time invariant system if we now use a shifted impulse as the input xŒn D
ıŒn   no the output will be a shifted version of the impulse response yŒn D hŒn   no.
ıŒn   no ) hŒn   no
If we also scaled the input, because of linearity the output would be scaled as well. Let’s scale
the input by the value ˛. Then
˛ıŒn   no ) ˛hŒn   no

64
6. LINEARITY, TIME-INVARIANCE, AND THE ROLE OF THE IMPULSE RESPONSE
x[n]
y[n]
LTI
Figure 6.1: A discrete linear time invariant system.
Let’s make the input a little more complicated and add another delta function, so xŒn D ˛ıŒn  n0 C ˇıŒn   n1 then the additivity part of the linearity property kicks in and the output is now
yŒn D ˛hŒn   n0 C ˇhŒn   n1 or
˛ıŒn   n0 C ˇıŒn   n1 ) ˛hŒn   n0 C ˇhŒn   n1
So if I can write an input solely in terms of weighted and shifted delta functions the output can
be written as the sum of weighted and shifted impulse responses. However, we know we can
write any discrete function as the weighted sum of shifted delta functions. If
xŒn D
8
ˆˆˆˆ<
ˆˆˆˆ:
1
n D 0
1:5
n D 1
:5
n D 2
1
n D 3
0
otherwise
we can write this as
xŒn D ıŒn C 1:5ıŒn   1 C :5ıŒn   2 C ıŒn   3
Using the fact that the response to ıŒn is hŒn, using the linearity and time-invariance property
of the system the response to this input would be
yŒn D hŒn C 1:5hŒn   1 C :5hŒn   2 C hŒn   3
or
ıŒn C 1:5ıŒn   1 C :5ıŒn   2 C ıŒn   3 ) hŒn C 1:5hŒn   1 C :5hŒn   2 C hŒn   3
Looking at the expression for xŒn above we can see that the value of xŒn for n D 0 is 1, for
n D 1 is 1.5, for n D 2 is .5 and for n D 3 is 1. In other words xŒ0 D 1; xŒ1 D 1:5; xŒ2 D :5,
and xŒ3 D 1. So we could have written xŒn as
xŒn D xŒ0ıŒn C xŒ1ıŒn   1 C xŒ2ıŒn   2 C xŒ3ıŒn   3
and yŒn as
yŒn D xŒ0hŒn C xŒ1hŒn   1 C xŒ2hŒn   2 C xŒ3hŒn   3

6.1. THE RESPONSE TO AN IMPULSE FOR A DISCRETE-TIME SYSTEM
65
x[3]h[n−3] = h[n−3]
x[0]h[n] = h[n]
x[1]h[n−1] = 1.5h[n−1]
x[2]h[n−2] = 0.5h[n−2]
Figure 6.2: The terms in the sum which result in yŒn.
In fact, in general we can write any xŒn as
xŒn D : : : xŒ 2ıŒn C 2 C xŒ 1ıŒn C 1 C xŒ0ıŒn C xŒ1ıŒn   1 C xŒ2ıŒn   2 C   
So, in general (for a linear time invariant system) we can write the output of the system as
yŒn D    C xŒ 2hŒn C 2 C xŒ 1hŒn C 1 C xŒ0hŒn C xŒ1hŒn   1 C xŒ2hŒn   2 C   
or
yŒn D
1
X
kD 1
xŒkhŒn   k

66
6. LINEARITY, TIME-INVARIANCE, AND THE ROLE OF THE IMPULSE RESPONSE
We can shorten the description above as follows
ıŒn
) hŒn
By definition
ıŒn   k
) hŒn   k
By time-invariance
xŒkıŒn   k
) xŒkhŒn   k
By Linearity
1
X
kD 1
xŒkıŒn   k
) P1
kD 1 xŒkhŒn   k
By Linearity
But the left hand side is simply xŒn. Therefore, the right hand side is yŒn.
We can show through variable substitution that this sum can also be written as
yŒn D
1
X
kD 1
xŒn   khŒk
This sum is called the convolution sum and can be viewed as an operation between xŒn and hŒn.
We denote this operation by ~. So,
yŒn D xŒn ~ hŒn D
1
X
kD 1
xŒkhŒn   k D
1
X
kD 1
xŒn   khŒk
We will have more to say about the convolution operation later in this module. For now let’s
consider what this equation says. What it says is that in order to obtain the output of an LTI
system for a given input all we need is the impulse response of the system. Think how cool that
is. Because knowing the impulse response allows us to compute the output of the LTI system
for every input we can say that if the system is a linear time invariant system it can be completely
characterized by the impulse response. As we will see we can tell whether the system has memory,
whether it’s causal, whether it’s stable; all this by knowing how it responds to an input which
is unity at n D 0 and zero everywhere else. While the theory is lovely the actual computation
can sometimes (OK often) be a pain. Let’s delay the pain for a while and see how we can get a
similar characterization for continuous time systems.
6.2
THE RESPONSE TO AN IMPULSE FOR CONTINUOUS
TIME SYSTEMS
We saw how the impulse response can be used with the convolution sum to give the output of
an LTI discrete time system for any input. We can do the same for continuous time systems. In
this case we use the response of the system to a continuous time delta function and a convolution
integral. Recall the sifting property of the continuous time impulse
Z 1
 1
x.t/ı.t   to/ D x.to/

6.2. THE RESPONSE TO AN IMPULSE FOR CONTINUOUS TIME SYSTEMS
67
x(t)
y(t)
LTI
Figure 6.3: A continuous time linear-time-invariant system.
From which we can derive
x.t/ D
Z 1
 1
x./ı.t   /d
Now, consider the continuous time linear time-invariant system shown in Figure 6.3. Let’s con-
sider the case where the input is the delta function x.t/ D ı.t/. Let’s call the response of the
system to an impulse the impulse response and denote it by h.t/. In other words when the input
x.t/ D ı.t/, the output y.t/ D h.t/. We can denote this symbolically like this:
ı.t/ ) h.t/
Because the system is time invariant if we now shift the input by  so that if the input is ı.t   /
the output will be a shift of the impulse response by the same amount—h.t   /.
ı.t   / ) h.t   /
Now let’s scale the input by x./. Because the system is linear scaling the input by x./ will scale
the output by x./ as well. So, when the input is x./ı.t   / the output will be x./h.t   /.
x./ı.t   / ) x./h.t   /
In the final step we will use the summability part of the linearity property. Recall that for a
linear system the response of a sum of inputs is the sum of the corresponding outputs. Instead
of simply taking a sum we will take the integral of the input. You might remember from calculus
that the integral can be defined as the limit of a sum, so the same rules apply. If we integrate the
input over a certain range the system will respond with the integral of the corresponding output
over the same range. So
Z 1
 1
x./ı.t   /d )
Z 1
 1
x./h.t   /d
But
R 1
 1 x./ı.t   / is simply x.t/, or
x.t/ )
Z 1
 1
x./h.t   /d
Therefore, the output of a linear time invariant system with impulse response h.t/ to an input
x.t/ is given by the convolution integral.
y.t/ D
Z 1
 1
x./h.t   /d

68
6. LINEARITY, TIME-INVARIANCE, AND THE ROLE OF THE IMPULSE RESPONSE
As in the case of the digital systems the convolution operation is denoted by ~.
y.t/ D x.t/ ~ h.t/ D
Z 1
 1
x./h.t   /d
6.3
THE CONVOLUTION OPERATION
Convolution can be thought of as a binary operation like addition multiplication etc. and like
those operations it has some useful properties. One of the most useful is that the convolution
operation is commutative. That is
x.t/ ~ h.t/ D h.t/ ~ x.t/ D
Z 1
 1
x.t   /h./d
We show this using variable substitution. Consider the integral
Z 1
 1
x./h.t   /d
Define
 D t   
which we will use as the variable of integration in the integral. This means that everywhere we
see  we need to replace it with a function of . If we write  in terms of  and t we get
 D t   
and, therefore, taking the differential of both sides we get
d D  d
Notice there is no dt in here. This is because as far as the integral is concerned t is a constant.
Finally, we need to see how this substitution effects the limits of integration. The lower limit in
the original integral was  D  1. If  D  1 then after the variable substitution t    D  1
or, taking t over to the other side and multiplying by  1,  D C1. Similarly where the upper
limit in the original integral was C1, after the variable substitution it becomes  1. Plugging
all this in we get
y.t/ D
Z 1
 1
x./h.t   /d D
Z  1
C1
x.t   /h./. d/
We can swap the limits of integration if we multiply the integral with  1. This  1 will cancel
out the minus in front of d and we get
y.t/ D
Z 1
 1
x.t   /h./d

6.3. THE CONVOLUTION OPERATION
69
x(t)
w(t)
y(t)
h1(t)
h2(t)
Figure 6.4: A series connection of linear-time-invariant systems.
 is simply a dummy variable. We can replace it with any variable we want. We could replace it
with Tom in which case we would get
y.t/ D
Z 1
 1
x.t   Tom/h.Tom/dTom
The Toms of the world may not like this so we replace  with  to get
y.t/ D
Z 1
 1
x.t   /h./d
But this is simply h.t/ ~ x.t/. So the convolution operation is commutative.
The convolution operation, like addition and multiplication, is also associative, that is
.x.t/ ~ h1.t// ~ h2.t/ D x.t/ ~ .h1.t/ ~ h2.t//
This property is especially useful when dealing with systems that are connected in series. Con-
sider the setup in Figure 6.4 consisting of a series connection of two linear time-invariant sys-
tems. The input to the first system with impulse response h1.t/ is x.t/ therefore the output y.t/
is given by
y.t/ D x.t/ ~ h1.t/
For the second system with impulse response h2.t/ the input is y.t/ and the output is w.t/. As
this is a linear time-invariant system
w.t/ D y.t/ ~ h2.t/
Substituting for y.t/ from the previous equation
w.t/ D .x.t/ ~ h1.t// ~ h2.t/
If we use the associative property we can write this as
w.t/ D x.t/ ~ .h1.t/ ~ h2.t//
which means that we could replace the two systems with a single system whose impulse response
would be h1.t/ ~ h2.t/. A more useful application of this property is that we can take a more
complex system with impulse response h1.t/ ~ h2.t/ and replace it with a series connection
of two simpler systems. The question of how you would take the more complicated impulse

70
6. LINEARITY, TIME-INVARIANCE, AND THE ROLE OF THE IMPULSE RESPONSE
response and extract the simpler impulse responses from them we leave aside for the moment.
Turns out it is not as difficult as it seems but we need one more tool before we can tackle that
problem.
Finally, just as multiplication is distributive over addition
˛  .a C b/ D ˛  a C ˛  b
the convolution operation is also distributive over addition
x.t/ ~ .h1.t/ C h2.t// D x.t/ ~ h1.t/ C x.t/ ~ h2.t/
Again we can see how we could take a more complex system with impulse response h1.t/ C h2.t/
and break the impulse response into simpler components.
6.4
SUMMARY
In this module we showed that a linear time-invariant system can be completely characterized
by its response to an impulse. If we denote the response of the system to an impulse - the impulse
response - as hŒn or h.t/ the output of the system for an arbitrary input xŒn or x.t/ is given by
the convolution sum
yŒn D xŒn ~ hŒn D
1
X
kD 1
xŒkhŒn   k D
1
X
kD 1
xŒn   khŒk
or the convolution integral
y.t/ D x.t/ ~ h.t/ D
Z 1
 1
x./h.t   /d D
Z 1
 1
x.t   /h./d

6.5. EXERCISES
71
x(t)
y(t)
h1(t)
h2(t)
Figure 6.5: A parallel connection of linear-time-invariant systems.
6.5
EXERCISES
(Answers on the following page)
1. The input-output relationship of a linear time-invariant system is given by
yŒn D xŒn   xŒn   1
What is the impulse response of this system?
2. The input-output relationship of a linear time-invariant system is given by
yŒn D xŒn C 1   2xŒn C xŒn   1
What is the impulse response of this system?
3. The input-output relationship of a linear time-invariant system is given by
yŒn D xŒn C 0:9yŒn   1
What is the impulse response of this system?
4. For the system shown in Figure 6.5.
Write the output y.t/ in terms of x.t/, h1.t/, and h2.t/.
5. The input-output relationship of a linear time-invariant system is given by
y.t/ D   1
˛
Z t
 1
x./d
What is the impulse response of this system? Can you come up with a circuit that would
have this impulse response?
6. The input-output relationship of a linear time-invariant system is given by
y.t/ D
Z t
 1
e .t /x.   1/d
What is the impulse response of this system?

72
6. LINEARITY, TIME-INVARIANCE, AND THE ROLE OF THE IMPULSE RESPONSE
7. The input-output relationship of a linear time-invariant system is given by
y.t/ D e t
Z t
 1
x./ed
What is the impulse response of this system?

6.6. ANSWERS
73
C
R
x(t)
y(t)
−
+
+
+
Figure 6.6: ˛ D RC.
6.6
ANSWERS
1. hŒn D ıŒn   ıŒn   1
2. hŒn D ıŒn C 1   2ıŒn C ıŒn   1
3. hŒn D .0:9/nuŒn
4. y.t/ D x.t/ ~ h1.t/ C x.t/ ~ h2.t/
5. h.t/ D   1
˛u.t/
(Figure 6.6)
6. h.t/ D e .t 1/u.t   1/
7. h.t/ D e tu.t/


75
M O D U L E
7
Properties of LTI Systems
We have made the claim that the impulse response of a linear time-
invariant system completely determines the behavior the system. If that
is the case we should be able to determine the various system properties
we discussed earlier from the impulse response. Let’s take each in turn and
demonstrate that we can.
7.1
MEMORY IN LTI SYSTEMS
A system is said to be memoryless when the output depends on only the
current value of the input. How is that reflected in the impulse response?
Let’s consider a discrete system first. For a discrete system the input and
output are related by the convolution sum
yŒn D
1
X
kD 1
xŒkhŒn   k
For the system to be memoryless we want only terms containing xŒn on the right hand side. The
only term in this summation that contains xŒn is the term with k D n, or xŒnhŒn   n. Given
that the input can be anything, for only this term to be nonzero we need the only nonzero term
in the impulse response to be hŒn   n or hŒ0. The discrete function that is only nonzero for
n D 0 is ıŒn. So, for a memoryless system the impulse response has to be of the form
hŒn D KıŒn
A system with impulse response
hŒn D uŒn   uŒn   4
is a system with memory. To see this let’s write the input output relationship for this and see if
the output depends on more than the current input. Recall that given the impulse response the
output is given by
yŒn D
1
X
kD 1
xŒkhŒn   k D
1
X
kD 1
xŒn   khŒk

76
7. PROPERTIES OF LTI SYSTEMS
x(t)
w(t)
y(t)
System 1
h1(t)
System 2
h2(t)
Figure 7.1: A series of LTI systems.
If we use the second form of the summation we can expand this summation as
yŒn D hŒ0xŒn C hŒ1xŒn   1 C hŒ2xŒn   2 C hŒ3xŒn   3
D xŒn C xŒn   1 C xŒn   2 C xŒn   3
because hŒ0; hŒ1; hŒ2, and hŒ3 are the only nonzero values of the impulse response hŒn. There-
fore, the output at time n depends not only on the input at time n but also on the input at time
n   1 and n   2 and n   3. Hence the system is not memoryless.
What about continuous systems? Consider the convolution integral.
y.t/ D
Z 1
 1
x./h.t   /d
We want the right hand side to consist of terms containing only x.t/. The only way that can
happen is if
h.t   / D Kı.t   /
Therefore, a system is memoryless if the impulse response is given by
h.t/ D Kı.t/
So, for both discrete and continuous time systems if the impulse response is zero for all
values of the argument except 0 the system is memoryless, otherwise it has memory.
7.2
INVERTIBILITY OF LTI SYSTEMS
Given the series connection of linear time invariant systems in Figure 7.1 we can say that the
system labeled System 1 is invertible if we can find a System 2 such that w.t/ D x.t/. The output
of System 1 is given by
y.t/ D x.t/ ~ h1.t/
and the output of System 2 is given by
w.t/ D y.t/ ~ h2.t/
Substituting for y.t/ in this equation we get
w.t/ D .x.t/ ~ h1.t// ~ h2.t/

7.2. INVERTIBILITY OF LTI SYSTEMS
77
At this point we will use the associative property of convolution. Using this property we can
rewrite the expression for the output as
.x.t/ ~ h1.t// ~ h2.t/ D x.t/ ~ .h1.t/ ~ h2.t//
For an invertible system in the strict sense we want w.t/ D x.t/, which means that
x.t/ ~ .h1.t/ ~ h2.t// D x.t/
We know that
x.t/ ~ ı.t/ D x.t/
So we want
h1.t/ ~ h2.t/ D ı.t/
If we allow some delay in the system so that w.t/ D x.t   to/ then we can relax our requirement
to
h1.t/ ~ h2.t/ D ı.t   t0/
We can go through the same procedure and find that for a system with impulse response h1Œn to
be invertible there has to exist a system with impulse response h2Œn such that h1Œn ~ h2Œn D
ıŒn.
Remember we said that the inverse system to
yŒn D
n
X
kD 1
xŒk
is
yŒn D xŒn   xŒn   1
Let’s figure out the impulse response of each of these systems and show that their convolution
results in a delta function. The easiest way to find the impulse response of a linear time invariant
system is to replace the input with an impulse function. In other words substitute xŒk with ıŒk
in the input output relationship. Therefore
h1Œn D
n
X
kD 1
ıŒk
In this sum there is only one term—ıŒ0—which is nonzero. Therefore, as long as this term is
included in the summation the sum will be one. If this term is not included in the summation
the sum is 0. This term will be included in the summation as long as n  0. Therefore,
h1Œn D
(
1
n  0
0
otherwise

78
7. PROPERTIES OF LTI SYSTEMS
or
h1Œn D uŒn
A somewhat roundabout way of finding the same impulse response is to note that we have
two expressions for the output—the convolution sum and the input output relationship. The two
must by necessity be equal.
yŒn D
1
X
kD 1
xŒkh1Œn   k D
n
X
kD 1
xŒk
This means
h1Œn   k D
(
1
k  n
0
otherwise
Which in turn implies
h1Œ k D
(
1
k  0
0
otherwise
and
h1Œk D
(
1
 k  0
0
otherwise
or
h1Œk D
(
1
k  0
0
otherwise
or h1Œn D uŒn. Clearly the first approach is preferable wherever it is available.
For the second system using the same argument
h2Œn D ıŒn   ıŒn   1
and
h1Œn ~ h2Œn D uŒn ~ .ıŒn   ıŒn   1/ D uŒn   uŒn   1 D ıŒn
Thus, the two systems—the summer and the differencer are inverse of each other.
7.3
CAUSALITY OF LTI SYSTEMS
We want the output at time n not to depend on the future values of the input, namely xŒk for
k > n. In other words
yŒn D
1
X
kD 1
xŒkhŒn   k D
n
X
kD 1
xŒkhŒn   k

7.3. CAUSALITY OF LTI SYSTEMS
79
In order for this to be true
hŒn   k D 0
for k > n
)
hŒn   k D 0
for k   n > 0
)
hŒn   k D 0
for n   k < 0
or
hŒk D 0 for k < 0
So the impulse response of a causal system is zero for all values of its argument less than zero.
For a discrete system this means that the impulse response is zero for n < 0 and for continuous
systems the impulse response is zero for t < 0.
We can also see that this has to be true by considering what an impulse response is. The
impulse response is the response of the system to an input at time t D 0 or n D 0. If the impulse
response is nonzero prior to this time this would mean that the system responded before the
input occurred which would mean that the system was non-causal.
Let’s take a look at a few examples of causal and non-causal systems. Consider the system
with impulse response
h.t/ D e tu.t/
The output from this system is given by
y.t/ D
Z 1
 1
x./h.t   /d D
Z 1
 1
x./e .t /u.t   /d
Let’s look for a moment at the behavior of u.t   /. Remember that
u.t/ D
(
1
t > 0
0
t < 0
Therefore,
u.t   / D
(
1
t    > 0
0
t    < 0
D
(
1
  >  t
0
  <  t
D
(
1
 < t
0
 > t
Because u.t   / is equal to zero for t < , the integrand x./e .t /u.t   / is zero for t < ,
therefore,
y.t/ D
Z t
 1
x./e .t /d
or the only values of the input that effect the output at time t, are the values at t and prior to t.
Hence, as expected this system is causal.
What about the system with an impulse response which is nonzero for t < 0.
h.t/ D e ajtj
a > 0

80
7. PROPERTIES OF LTI SYSTEMS
We can write this impulse response as
h.t/ D
(
e at
t > 0
eat
t < 0
D e atu.t/ C eatu. t/
So we can write the convolution integral as
y.t/
D
Z 1
 1
x./
h
e a.t /u.t   / C ea.t /u. .t   //
i
d
D
Z 1
 1
x./e a.t /u.t   /d C
Z 1
 1
x./ea.t /u.   t/d
D
Z t
 1
x./e a.t /d C
Z 1
t
x./ea.t /d
where, in the second integral, we have used the fact that u.   t/ is zero for  < t. The second
integral also means that we need to use values of the input beyond the time t in order to obtain
the output at time t. Hence the system is noncausal.
Turning to discrete systems, consider the system with impulse response
hŒn D uŒn
which is zero for n < 0. If we examine the output of this system we find that
yŒn D
1
X
kD 1
xŒkhŒn   k D
1
X
kD 1
xŒkuŒn   k
The function uŒn   k is zero for k > n, therefore,
yŒn D
n
X
kD 1
xŒk
and the output at time n depends only on the input at time n and times prior to n. Hence the
system with this impulse response is causal.

7.4. SUMMARY
81
7.3.1
STABILITY FOR LTI SYSTEMS
Suppose you are guaranteed that jx.t/j < B then
jy.t/j
D
ˇˇˇˇ
Z 1
 1
x.t   /h./d
ˇˇˇˇ

Z 1
 1
jx.t   /h./j d
<
Z 1
 1
B jh./j d
D
B
Z 1
 1
jh./j d
So if we want jy.t/j < 1 then we need
Z 1
 1
jh./j d < 1
Another way of stating this is to say that we want h.t/ to be absolutely integrable.
Similarly for the discrete case we want hŒn to be absolutely summable
1
X
kD 1
jhŒkj < 1
Consider for example the integrator with impulse response
h.t/ D u.t/
Clearly
Z 1
 1
h./d D
Z 1
0
d ! 1
Therefore, this system is unstable in the bounded input bounded output sense.
7.4
SUMMARY
In this module we examined our assertion that a linear time-invariant system is completely
characterized by its impulse response. We showed that the impulse response can tell us whether
a system has memory, is stable, or is causal.
•
A memoryless LTI system has an impulse response of the form
h.t/ D Kı.t/
hŒn D KıŒn

82
7. PROPERTIES OF LTI SYSTEMS
•
The impulse response of a causal system is zero for negative values of its argument.
h.t/ D 0 for t < 0
hŒk D 0 for k < 0
•
An LTI system is stable if the impulse response if absolutely integrable or absolutely
summable.
Z 1
 1
jh./j d < 1
1
X
kD 1
jhŒkj < 1
•
An LTI system with impulse response h1.t/ or h1Œn is invertible if we can find another
system with impulse response h2.t/ or h2Œn such that
h1.t/ ~ h2.t/ D ı.t/
h1Œn ~ h2Œn D ıŒn

7.5. EXERCISES
83
7.5
EXERCISES
(Answers on the following page)
Given the following impulse responses determine if the system is memoryless, causal, or
stable.
1. hŒn D uŒn
2. hŒn D ıŒn   ıŒn   1
3. h.t/ D u.t C 1/   u.t   1/
4. h.t/ D u.t C 1/
5. hŒn D ˛nuŒnj˛j < 1
6. h.t/ D e 3tu.t/
7. h.t/ D e 3tu. t/
8. h.t/ D e3tu.t/
9. h.t/ D e3tu. t/
10. h.t/ D e3tu.t C 1/
11. hŒn D ˛nuŒnj˛j > 1
12. hŒn D uŒn   uŒn   1

84
7. PROPERTIES OF LTI SYSTEMS
7.6
ANSWERS
1. hŒn D uŒn; Not memoryless, Causal, Not stable
2. hŒn D ıŒn   ıŒn   1; Not memoryless, Causal, Stable
3. h.t/ D u.t C 1/   u.t   1/; Not memoryless, Not Causal, Stable
4. h.t/ D u.t C 1/; Not memoryless, Not Causal, Not Stable
5. hŒn D ˛nuŒnj˛j < 1; Not memoryless, Causal, Stable
6. h.t/ D e 3tu.t/; Not memoryless, Causal, Stable
7. h.t/ D e 3tu. t/; Not memoryless, Not Causal, Not Stable
8. h.t/ D e3tu.t/; Not memoryless, Causal, Not Stable
9. h.t/ D e3tu. t/; Not memoryless, Not Causal, Stable
10. h.t/ D e3tu.t C 1/; Not memoryless, Not Causal, Not Stable
11. hŒn D ˛nuŒnj˛j > 1; Not memoryless, Causal, Not Stable
12. hŒn D uŒn   uŒn   1; Memoryless, Causal, Stable

85
M O D U L E
8
Discrete Time Convolution
In this module we will look in some detail at discrete time convolution—
mostly through examples. Discrete time convolution is not simply a math-
ematical construct, it is a roadmap for how a discrete system works. This
becomes especially useful when designing or implementing systems in dis-
crete time such as digital filters and others which you may need to imple-
ment in embedded systems.
To reiterate, the input and output of a linear time invariant system
are related through convolution with the impulse response.
yŒn D
1
X
kD 1
xŒkhŒn   k
We saw how this came about mathematically. Let’s see how this equation comes about using a
very simple example. Let’s suppose we have a linear time-invariant system with impulse response
hŒn D 1:5ıŒn C ıŒn   1 C 0:5ıŒn   2 C 0:5ıŒn   3
Suppose we excite it with an input
xŒn D ıŒn C 1:5ıŒn   1 C 1:5ıŒn   2 C ıŒn   3
Let’s work through this one input sample at a time and see how the system responds to
each input value (Figure 8.1). We will then use the additive property of linearity to add the
responses to the individual samples and reconstruct the input and output.
We start out with the sample at n D 0. This being an impulse the output is simply the
impulse response (Figure 8.2).
The next input value is a delta function of weight 1.5. As the input occurs at time n D 1,
because of time invariance the output is the impulse response shifted by one and weighted by
the value of the input (Figure 8.3).
The next input occurs at time n D 2 and is an impulse with a weight of 1.5. The output
then is the impulse response shifted by 2 and weighted by 1.5 (Figure 8.4). Finally, the last input
is an impulse at n D 3 with a weight of 1. The response to this is the impulse response shifted
by 3 (Figure 8.5).
To obtain the response to the complete input xŒn we simply add the response to each
individual sample. In Figure 8.6 we show the input and the corresponding response at time 0,

86
8. DISCRETE TIME CONVOLUTION
n
x[n]
n
h[n]
Figure 8.1: The input xŒn and the impulse response hŒn.
n
x[0]
n
x[0]h[0]
x[0]h[1]
x[0]h[2] x[0]h[3]
Figure 8.2: The response of the system to the input xŒ0.
n
x[1]
n
x[1]h[0]
x[1]h[1]
x[1]h[2] x[1]h[3]
Figure 8.3: The response of the system to the input xŒ1.
n
x[2]
n
x[2]h[0]
x[2]h[1]
x[2]h[2] x[2]h[3]
Figure 8.4: The response of the system to the input xŒ2.
n
x[3]
n
x[3]h[0]
x[3]h[1]
x[3]h[2] x[3]h[3]
Figure 8.5: The input xŒn and the impulse response hŒn.

8. DISCRETE TIME CONVOLUTION
87
n
x[3]
n
x[3]h[0]
x[3]h[1]
x[3]h[2] x[3]h[3]
n
n
n
x[2]
n
x[2]h[0]
x[2]h[1]
x[2]h[2] x[2]h[3]
n
x[1]
n
x[1]h[0]
x[1]h[1]
x[1]h[2] x[1]h[3]
n
x[0]
n
x[0]h[0]
x[0]h[1]
x[0]h[2] x[0]h[3]
Figure 8.6: The input xŒn and the output yŒn.

88
8. DISCRETE TIME CONVOLUTION
1, 2, and 3. In order to get the complete output all we need to do is add these outputs.
At time n D 0 the only non-zero term is the response to the input at time 0. Therefore, the
output is simply xŒ0hŒ0. At time n D 1 there is nonzero output from the input at time 0 and
the input at time 1. The output at time 1 resulting from the input at time 0 is xŒ0hŒ1. Recall
that in this example an impulse at time n D 0 results in an impulse response which continues
until n D 3. So the effect of the input xŒ0 is continuing. But now we also have the system
responding to the input at time n D 1. Because at n D 1 the system has just “seen” the input xŒ1
it responds with the output xŒ1hŒ0. Because of linearity these two responses add up and the
output is xŒ0hŒ1 C xŒ1hŒ0. At time n D 2 the response from the input at time n D 0 and n D 1
are still continuing and we add in the response from the input at time n D 2 to get the output
xŒ0hŒ2 C xŒ1hŒ1 C xŒ2hŒ0. At n D 3 the output is xŒ0hŒ3 C xŒ1hŒ2 C xŒ2hŒ1 C xŒ3hŒ0.
Notice how in each sum we have the index for one factor go up and the index for the other
factor goes down. This is the effect that we see in the terms of the summation xŒkhŒn   k, as k
increases the argument of xŒ  increases and the argument of hŒ  decreases. To really emphasize
this point, let’s rewrite the output at time n D 3 to match the convolution expression
yŒ3 D xŒ0hŒ3   0 C xŒ1hŒ3   1 C xŒ2hŒ3   2 C xŒ3hŒ3   3
At time n D 4 the effect of the input at time n D 0 has died out and there is no new input
so all we have are the responses from the input at times n D 1; 2; and 3 continuing to generate
the output xŒ1hŒ3 C xŒ2hŒ2 C xŒ2hŒ1. At time n D 5 the only responses left are from the
input at time n D 2 and n D 3 which generate the output xŒ2hŒ3 C xŒ3hŒ2. And finally at
time n D 6 the only contribution is from the response to the input at time n D 3 and the output
is xŒ3hŒ3.
Hopefully it is clear why the output for a linear time invariant system is given by the
convolution equation
yŒn D
X
xŒkhŒn   k
Having hopefully convinced ourselves of the physical validity of the convolution sum, let’s
turn our attention to the problem of computing the convolution sum. In the convolution sum
notice that k is the variable in the summation and n is fixed. So, for each n we need to find
the sum P1
kD 1 xŒkhŒn   k. Each time we change n, nothing happens to xŒk, but hŒn   k
changes. So let’s look at the behavior of hŒn   k. We could do this for a generic hŒn but it is
easier to understand the process if we do this for specific impulse responses. Let’s take the easy
way out.
Example 8.1
Let’s suppose
hŒn
D
.0:9/nuŒn
xŒn
D
uŒn

8. DISCRETE TIME CONVOLUTION
89
x[k]
…
k
h[k]
…
k
Figure 8.7: The input xŒk and the impulse response hŒk.
These signals are plotted in Figure 8.7. We will take the transformation of hŒk to hŒn   k
in two steps. First lets look at hŒ k. Recall that replacing k with  k in the argument involves
a mirroring across the vertical axis. We plot hŒk and hŒ k in Figure 8.8.
h[–k]
…
k
h[k]
…
k
Figure 8.8: The impulse response hŒk and it’s mirrored version hŒ k.
In Figure 8.9 we plot hŒn   k. Notice that we have drawn two versions of hŒn   k, one
for n > 0 and one for n < 0. We have also drawn xŒn to provide context. In the case n < 0
wherever hŒn   k ¤ 0 xŒk D 0, and wherever xŒk ¤ 0 hŒn   k D 0, therefore, when we take
the product xŒkhŒn   k, regardless of the value of k the product will be zero. Hence, for n < 0,
yŒn D 0. To see what the output is when n > 0 let’s turn to the convolution sum.
yŒn D
1
X
kD 1
xŒkhŒn   k D
1
X
kD 1
uŒk.0:9/n kuŒn   k
Before we compute this lets plot xŒk and hŒn   k. As we mentioned before while we can plot
xŒk directly as a function of k, the plot of hŒn   k will change depending on the value of n.
Let’s see if we can simplify this. Before we start let’s assume that n  0. We have a pretty
good idea of what happens when n < 0. Let’s split the summation into the three regions.
 1
X
kD 1
uŒk.0:9/n kuŒn   k C
n
X
kD0
uŒk.0:9/n kuŒn   k C
1
X
kDnC1
uŒk.0:9/n kuŒn   k
The first sum is zero because uŒk D 0 for k < 0. The third sum is zero because uŒn   k D 0 for
k > n. Therefore we can rewrite the summation as
yŒn D
n
X
kD0
uŒk.0:9/n kuŒn   k

90
8. DISCRETE TIME CONVOLUTION
h[n–k]
…
k
n
h[n–k]
…
…
k
n
x[k]
k
…
x[k]
k
Figure 8.9: hŒn   k for positive and negative values of n with the input xŒk for context.
In this range (from k D 0 to k D n) both uŒk and uŒn   k are equal to 1 so we can drop them
from the summation altogether and we get
yŒn D
n
X
kD0
.0:9/n k
Or, given that the summation is over k and .0:9/n k D .0:9/n.0:9/ k we can write this as
yŒn D .0:9/n
n
X
kD0
.0:9/ k D .0:9/n
n
X
kD0
.0:9 1/k
This final summation is a form that we will encounter quite often. So, let’s take a brief detour
and explain how to evaluate sums of this form.
Geometric Sum
This kind of sum is called a geometric sum. Let’s define the sum as
Sm;n D
kDn
X
kDm
˛k
We can derive the value of Sm;n pretty easily.
Sm;n
D
˛m C ˛mC1 C ˛mC2 C    C ˛n
˛Sm;n
D
˛mC1 C ˛mC2 C    C ˛n C ˛nC1
Subtracting the bottom equation from the top equation we get
Sm;n   ˛Sm;n D ˛m   ˛nC1
or
.1   ˛/Sm;n D ˛m   ˛nC1

8. DISCRETE TIME CONVOLUTION
91
from which we get
Sm;n D ˛m   ˛nC1
1   ˛
Applying the geometric sum formula we get
yŒn D .0:9/n
.0:9 1/0   .0:9 1/nC1
1   0:9 1

Anything to the power 0 is 1 and 0:9 1 is 1=0:9 so we can simplify this to
yŒn D
.0:9/n

1  1
.0:9/nC1

1   1
0:9
which after simplification gives us
yŒn D 10
 1   .0:9/nC1
But remember this was all for the case where n  0. What happens when n < 0? If n < 0 then
all values of k from  1 to n are negative and in this range uŒk D 0 and, therefore, yŒn D 0.
Therefore, to account for all n the output yŒn is given by
yŒn D 10
 1   .0:9/nC1
uŒn
If you plot this response you can see that the yŒn starts at 1 and reaches an asymptote at 10.
We used
yŒn D
1
X
kD 1
xŒkhŒn   k
to find yŒn. What would have happened if we used
yŒn D
1
X
kD 1
xŒn   khŒk
Let’s find out.
yŒn D
1
X
kD 1
xŒn   khŒk D
1
X
kD 1
uŒn   k.0:9/kuŒk
Using the same arguments as before we note that for n < 0 the sum is over all zero values and
thus is zero. For n > 0 because of uŒk the lower limit becomes zero and because of uŒn   k the
upper limit becomes n and we have
yŒn D
n
X
kD0
.0:9/k

92
8. DISCRETE TIME CONVOLUTION
h[n]
n
–7
–6
–5
–4
–3
–2
–1
1
2
3
4
5
6
7
Figure 8.10: The impulse response hŒn.
h[–k]
k
–7
–6
–5
–4
–3
–2
–1
1
2
3
4
5
6
7
Figure 8.11: The impulse response flipped around the y-axis hŒ k.
Applying the geometric sum formula we get
yŒn D .0:9/0   .0:9/nC1
1   0:9
D 10
 1   .0:9/nC1
which is the same result as we had before. And arguably it is also a much simpler path. When
faced with a convolution problem it is always a good idea to check which configuration will give
you an easier path to the solution.
Example 8.2
Let’s try a different example with the same input but a slightly different impulse
response.
hŒn D 0:9n 2uŒn   2
This is almost the same as the previous impulse response—just with a slight shift. The impulse
response is plotted in Figure 8.10. But remember the variable is k so we should plot hŒk. Of
course hŒk looks exactly the same with the x-axis labeled differently. What about hŒ k? As we
pointed out earlier, it will be hŒk swiveled around the y-axis. What was at 1 will now be at  1
and what was at  1 will be at 1, and so on. We have plotted hŒ k in Figure 8.11.
Notice that we have exaggerated the point marking the value of hŒ0 to indicate that we
flipped the impulse response around that point—this will become important in just a moment.
Let’s now plot hŒn   k for different values of n.
Look at Figure 8.12 where we have drawn hŒn   k for different values of n. Can you see
the pattern? As expected, we can see that as n gets larger we shift the response to the right. But
more specifically we can see that for each value of n we shift the origin of the flipped impulse
response (which we have drawn as a larger black disk) to that value of n. The point around which
you flip either hŒk (to get hŒn   k) or xŒk (to get xŒn   k ) is important.

8. DISCRETE TIME CONVOLUTION
93
h[n–k] (n=3)
k
n–7 n–6 n–5 n–4 n–3 n–2
1
2
3
4
5
6
h[n–k] (n=7)
k
n–7 n–6 n–5 n–4 n–3 n–2
1
2
3
4
5
6
h[n–k] (n=4)
k
n–7 n–6 n–5 n–4 n–3 n–2
1
2
3
4
5
6
h[n–k] (n=9)
k
n–7 n–6 n–5 n–4 n–3 n–2
1
2
3
4
5
6
Figure 8.12: The impulse response flipped around the y-axis and moved by n, hŒn   k.
h[–k]
k
1
2
3
4
5
6
–1
–2
–3
–4
–5
–6
–7
–1
–2
–3
–4
–5
–6
–7
2
1
3
4
5
6
7
7
x[k]
…
k
Figure 8.13: The input and impulse response plotted for n D 0.
If we look at the convolution sum now we can see that we need to then multiply the
corresponding values of xŒk and hŒn   k and take the sum of the products.
yŒn D
1
X
kD 1
xŒkhŒn   k
For n D 0 the plots of hŒn   k and xŒk are shown in Figure 8.13.
If we take the point-by-point product of these two functions the result will be zero; wher-
ever xŒk is nonzero, hŒ k is zero and wherever hŒ k is nonzero, xŒk is zero. If we pull the
flipped impulse response to the right by one by picking n D 1 (the larger black dot will be at
n D 1), nothing about the product will change. The same is true if we pull the flipped impulse
response to the left by picking values of n less than zero. So we can say that for n < 2 for each k,
the product xŒkhŒn   k is zero. As the sum of zeros (even an infinite number of them) is zero,
this means that for n < 2, yŒn D 0.

94
8. DISCRETE TIME CONVOLUTION
h[n–k] (n=3)
k
n–7 n–6 n–5 n–4 n–3 n–2
1
2
3
4
5
6
1
2
3
4
5
6
h[n–k] (n=7)
k
n–7 n–6 n–5 n–4 n–3 n–2
1
2
3
4
5
6
h[n–k] (n=4)
k
h[n–k] (n=3)
k
x[k]
k
n–7 n–6 n–5 n–4 n–3 n–2
1
2
3
4
5
6
1
2
3
4
5
6
7
8
h[n–k] (n=9)
k
n–7 n–6 n–5 n–4 n–3 n–2
1
2
3
4
5
6
Figure 8.14: The input and impulse response plotted for n D 3; 4; 7; 9.
So, what happens for n  2? Lets take a look at Figure 8.14. This is simply Figure 8.12
with xŒk plotted above it. We can see that the values of k for which the product between xŒk
and hŒn   k is nonzero are the values 0  k  n   2. We can therefore change our limits in our
convolution sum to
yŒn D
1
X
kD 1
xŒkhŒn   k D
n 2
X
kD0
xŒkhŒn   k
In this interval (0  k  n   2) xŒk D 1 and hŒn   k D 0:9n k 2, therefore,
yŒn D
n 2
X
kD0
1  0:9n k 2 D
n 2
X
kD0
0:9n k 2 D 0:9n 2
n 2
X
kD0
0:9 k D 0:9n 2
n 2
X
kD0
 1
0:9
k
Plugging the values of the upper and lower limits into the geometric sum formula we
obtain the value of yŒn for n  2.
yŒn D 0:9n 2
 1
0:9
0
  1
0:9
n 2C1
1   1
0:9

8. DISCRETE TIME CONVOLUTION
95
Table 8.1: Output for different values of n
n
y[n]
 2
1
 3
1.9
 4
2.71
A
A
11
6.51
A
A
21
8.78
A
A
31
9.58
or
yŒn D 0:9n 2
1   1
0:9
n 1
1   1
0:9
D
0:9n 2   1
0:9
1   1
0:9
Multiplying top and bottom by 0:9
yŒn D 0:9n 1   1
0:9   1
D 1   0:9n 1
1   0:9
D 10.1   0:9n 1/
We can write the value of yŒn for both ranges as
yŒn D
 0
n < 2
10.1   0:9n 1/
n  2
Or we can make use of the discrete unit step function and write this as
yŒn D 10.1   0:9n 1/uŒn   2
What does this output look like for different values of n? (see Table 8.1).
Does this make sense?
Example 8.3
If it does, let’s do a more difficult example (Figure 8.15). In this case we will keep
the impulse response the same as our first example but change the input from the unit step to a
rectangular pulse, So,
xŒn D uŒn   uŒn   4

96
8. DISCRETE TIME CONVOLUTION
h[n]
n
2
1
3
x[n]
n
Figure 8.15: The input and impulse response.
and the impulse response is
hŒn D 0:9nuŒn
We will use the convolutional sum
yŒn D
1
X
kD 1
xŒkhŒn   k
Again we have to compute the output yŒn for all n. In the previous example we ended up
computing yŒn in two regions, n < 2 and n  2. In this case we will end up finding yŒn for
three separate regions. Take a look at Figure 8.16.
Notice that for n < 0 there is no overlap between the nonzero range for xŒk and hŒn   k.
For n between 0 and 3 the overlap of the nonzero portion of xŒk and hŒn   k is between 0 and
n, and finally when n > 3 the overlap of the nonzero portion is between 0 and 3. Let’s compute
yŒn for each of these intervals.
For n < 0 this is easy. There is no overlap between the nonzero portions of xŒk and hŒn  k so the product xŒkhŒn   k is always zero and hence yŒn D 0.
For 0  n  3 the limits of the convolution sum become 0 and n.
yŒn D
n
X
kD0
xŒkhŒn   k D
n
X
kD0
1  .0:9/n k D 0:9n
n
X
kD0
.0:9/ k
or
yŒn D 0:9n
n
X
kD0
 1
0:9
k

8. DISCRETE TIME CONVOLUTION
97
h[n–k] (n=–1)
h[n–k] (n=5)
x[k]
x[k]
x[k]
1
–1
–2
–3
n–1
n–2
n–3
–4
2
3
4
5
k
n
n–1
n–2
n–3
n–4
n–5
n–6
n
n–1
n–2
n–3
n–4
n–5
n–6
n–7
n–8
n
1
–1
–2
–3
–4
2
3
4
5
k
1
–1
–2
–3
–4
2
3
4
5
k
1
–1
–2
–3
–4
2
3
4
5
k
1
–1
–2
–3 
–4
2
3
4
5
k
1
–1
–2
–3 
–4
2
3
4
5
k
h[n–k] (n=2)
Figure 8.16: The input and impulse response.
Using the geometric sum formula with lower limit 0 and upper limit n we obtain
yŒn D 0:9n
0
BBB@
 1
0:9
0
  1
0:9
nC1
1   1
0:9
1
CCCA
Working out the fiddly bits and multiplying through by 0.9 we get
yŒn D
0:9n   1
0:9
1   1
0:9
D 0:9nC1   1
0:9   1
D 10
 1   0:9nC1
For n > 3 the limits of the convolution sum become 0 and 3.
yŒn D
3
X
kD0
xŒkhŒn   k D
3
X
kD0
1  .0:9/n k D 0:9n
3
X
kD0
 1
0:9
k

98
8. DISCRETE TIME CONVOLUTION
1
2
3
3
2
1
4
5
6
7
8
9
n
y[n]
Figure 8.17: The output.
Applying the geometric sum formula
yŒn D 0:9n
0
BBB@
1   1
0:9
4
1   1
0:9
1
CCCA D 0:9nC1   0:9n 3
0:9   1
D 10
 0:9n 3.1   0:94/

Put all three parts together and you get
yŒn D
8
ˆ<
ˆ:
0
n < 0
10
 1   0:9nC1
0  n  3
10
 0:9n 3.1   0:94/

n > 3
The output is plotted in Figure 8.17.
Example8.4
Let’s look at a final example to explore the effect of the changing regions in which
the product xŒkhŒn   k is non-zero. This time we will take both the input and the impulse
response to be rectangular pulses.
xŒn
D
uŒn   uŒn   4
hŒn
D
uŒn   uŒn   4
The input signal and the impulse response are shown in Figure 8.18.
Note one of the differences between this set of signals and the ones we have used in
previous examples. In the previous examples at least one of the signals was non-zero over a
semi-infinite interval. In this case both signals are non-zero over a finite interval. What this
does is increases the number of boundaries between regions over which the signal is non-zero
and the region over which the signal is zero. The practical effect of this is that the resulting

8. DISCRETE TIME CONVOLUTION
99
2
1
3
x[k]
k
2
1
3
h[k]
h[k]
k
Figure 8.18: The input and impulse response.
k
n
n–1
n–2
n–3
h[n–k]
x[k]
1
2
3
k
k
n
n–1
n–2
n–3
h[n–k]
x[k]
1
2
3
k
k
n
n–1
n–2
n–3
h[n–k]
x[k]
1
2
3
k
k
n
n–1
n–2
n–3
h[n–k]
x[k]
1
2
3
k
Figure 8.19: Four different situations for the input and impulse response; for n < 0, for 0 < n 
3, for 3  n  6, and n > 6.
output has to be computed based on the relative locations of the boundaries. Let’s work through
this example and try to clarify this.
In Figure 8.19 we show the input xŒk and the flipped and shifted impulse response hŒn  k for four different values of n.
Let’s look at the output for each of these conditions. The first condition is easiest to handle.
When n < 0 there is no range in which both signals are nonzero. So in this case yŒn D 0. When
0  n  3 we have the condition in the top right panel of Figure 8.19. In this case the region of
overlap of non-zero values is from 0 to n. Therefore, the output is given by
yŒn D
n
X
kD0
xŒkhŒn   k D
n
X
kD0
1 D n C 1

100
8. DISCRETE TIME CONVOLUTION
1
2
3
4
5
6
7 n
h[k]
y[n]
Figure 8.20: Output fot the case where the input and impulse response are identical rectangular
pulses.
where we have used the fact that in the region 0  k  n for n  3 both xŒk and hŒn   k are
one and therefore the product xŒkhŒn   k will also be one.
The third situation depicted in the bottom left panel of Figure 8.19 is the case where
3  n  6. In this case from the figure we can see that the region of overlap between nonzero
values is between n   3 and 3. Therefore,
yŒn D
3
X
kDn 3
xŒkhŒn   k D
3
X
kDn 3
1 D 3   .n   3/ C 1 D 7   n
The final case is that depicted in the bottom right panel of Figure 8.19. This is the case
where n > 6. In this case again there is no overlap between non-zero regions of xŒk and hŒn   k
and therefore the output is zero. Putting all this together
yŒn D
8
ˆˆ<
ˆˆ:
0
n < 0
n C 1
0  n  3
7   n
3  n  6
0
n > 6
which is plotted in Figure 8.20.
After all of these examples you can see that computing the discrete time convolution can
be tedious but it is really not that difficult. To get reasonably competent at it requires only a
modicum of practice.

8.1. SUMMARY
101
8.1
SUMMARY
In this module we worked through several examples of discrete time convolution. I strongly
recommend that once you have understood the examples and their solutions you close the book
and see if you can solve the examples without looking at the solutions.

102
8. DISCRETE TIME CONVOLUTION
8.2
EXERCISES
(Answers on the following page)
Given the functions aŒn and bŒn, find cŒn, where cŒn D aŒn ~ bŒn where ~ denotes
convolution.
cŒn D aŒn ~ bŒn D
1
X
kD 1
aŒkbŒn   k D
1
X
kD 1
aŒn   kbŒk
1. aŒn D ıŒn   ıŒn   4, bŒn D uŒn
2. aŒn D .0:2/nuŒn, bŒn D uŒn
3. aŒn D .0:2/nuŒn, bŒn D ıŒn   3
4. aŒn D .0:2/nuŒn, bŒn D uŒn   uŒn   5
5. aŒn D uŒn   uŒn   4, bŒn D uŒn   1   uŒn   7
6. aŒn D uŒn   uŒn   5, bŒn D uŒn   uŒn   5
7. aŒn D uŒn   uŒn   5, bŒn D uŒn C 5   uŒn
8. aŒn D uŒn C 3   uŒn   4, bŒn D ıŒn   2
9. aŒn D uŒn   uŒn   5, bŒn D uŒn   1   uŒn   4

8.3. ANSWERS
103
8.3
ANSWERS
1.
cŒn D uŒn   uŒn   4
2.
cŒn D 1:25
 1   .o:2/nC1
3.
cŒn D .0:2/n 3uŒn   3
4.
cŒn D 1:25
 .0:2n 4.1   .0:2/5/

5.
cŒn D
8
ˆˆˆˆ<
ˆˆˆˆ:
0
n < 0
n C 1
0  n  3
4
3  n  6
10   n
6  n  9
0
n > 9
6.
cŒn D
8
ˆˆ<
ˆˆ:
0
n < 0
n C 1
0  n  4
9   n4  n  8
0
n > 8
7.
cŒn D
8
ˆˆ<
ˆˆ:
0
n < 5
n C 6
 5  n   1
4   n
 1  n  3
0
n > 3
8.
cŒn D uŒn C 1   uŒn   6
9.
cŒn D
8
ˆˆ<
ˆˆ:
0
n < 1
n
1  n  3
3
3  n  5
8   n
5  n  7


105
M O D U L E
9
Continuous Time Convolution
We turn our attention now to the response of continuous time linear time
invariant systems. Given the impulse response h.t/ of a continuous time
linear time invariant system (Figure 9.1) we can find the output y.t/ for
any input x.t/ by solving the convolution integral.
y.t/ D
Z 1
 1
x./h.t   /d D
Z 1
 1
x.t   /h.tau/d
As in the case of the discrete time convolution we will try and un-
derstand how to compute this integral through a number of examples. In
order to connect these examples to a system that you are familiar with let’s first derive the im-
pulse response of a simple RC circuit. We can then look at the response of this circuit to inputs
you are familiar with using the convolution integral and see how well the mathematical approach
matches physical reality.
9.1
THE IMPULSE RESPONSE OF A SIMPLE RC CIRCUIT
We will later see an easy way to find the impulse response of this circuit. For now let’s just use
Kirchoff’s laws and differential equations. From the circuit we can see that the input voltage x.t/
is equal to the voltage across the resistor plus the voltage across the capacitor. The voltage across
the capacitor is simply the output y.t/ while the voltage across the resistor is the resistance times
the current through the resistor. The current through the resistor is the same current that flows
through the capacitor and can be obtained as C dy
dt . Putting all this together we get
RCdy
dt C y.t/ D x.t/
or
dy
dt C 1
RCy.t/ D 1
RCx.t/
Let’s now multiply through by the integrating factor et=RC.
dy
dt et=RC C 1
RCy.t/et=RC D 1
RCx.t/et=RC
Using the product rule of differentiation we can write the left hand side of this equation as
d
dt

y.t/et=RC
D 1
RCx.t/et=RC

106
9. CONTINUOUS TIME CONVOLUTION
y(t)
x(t)
C
R
–
+
–
+
Figure 9.1: An example of a continuous time linear-time-invariant system.
To find the impulse response let’s set x.t/ D ı.t/ which means y.t/ D h.t/
d
dt

h.t/et=RC
D 1
RCı.t/et=RC
Let’s take a look at the product at the right hand side. Recall that ı.t/ is zero everywhere except
at t D 0, which means that
1
RCı.t/et=RC D 1
RCı.t/e0 D 1
RCı.t/
Plugging this back into the differential equation we get
d
dt

h.t/et=RC
D 1
RCı.t/
We can replace t with  without changing anything
d
d

h./e=RC
D 1
RCı./
Integrating both sides:
Z t
 1
d
d

h./e=RC
d D
Z t
 1
1
RCı./d
or
h./e=RCjt
 1 D 1
RCu.t/
or
h.t/et=RC   0 D 1
RCu.t/
from which we can get the impulse response as
h.t/ D 1
RCe t=RCu.t/

9.1. THE IMPULSE RESPONSE OF A SIMPLE RC CIRCUIT
107
t = 0
y(t)
y(0–) = 0
1V
C
R
–
+
–
+
Figure 9.2: The RC filter with a unit step input.
For convenience let’s define
˛ D 1
RC
which means
h.t/ D ˛e ˛tu.t/
where ˛ > 0.
Example 9.1
For our first example let’s examine the response of the familiar circuit shown in
Figure 9.2. We have already derived the impulse response of this circuit.
h.t/ D ˛e ˛tu.t/
The closing of the switch at t D 0 can be modeled with unit step input.
x.t/ D u.t/
We know what happens from our previous knowledge; the capacitor will charge up to 1 V with
the charging time depending on the RC time constant. Let’s see if the convolution integral
verifies this result.
y.t/ D
Z 1
 1
x./h.t   /d D
Z 1
 1
u./˛e ˛.t /u.t   /d
Before we try to solve this integral let’s do the same thing with h./ that we did with hŒk to
see what happens when we replace  with t    in the argument. Let’s begin with the functions
themselves in Figure 9.3 where we have picked ˛ D 1.
If we replace  with   in the impulse response we will flip the response around the y
axis. If we now replace it with t    we will drag it back and forth with t being the point around
which we flipped the original function. In Figure 9.4 we show x./ and h.t   / for t < 0.
In Figure 9.5 we show x./ and h.t   / for t > 0.

108
9. CONTINUOUS TIME CONVOLUTION
x(τ)
τ
τ
1.0
0.8
0.6
0.4
0.2
1.0
0.8
0.6
0.4
0.2
–1.0 –0.5
–1.5
0.5 1.0 1.5 2.0 2.5
–1.0 –0.5
–1.5
0.5 1.0 1.5 2.0 2.5
h(τ)
Figure 9.3: Input x.t/ and impulse response h.t/.
x(τ)
h(t–τ), t < 0
τ
τ
1.0
0.8
0.6
0.4
0.2
1.0
0.8
0.6
0.4
0.2
–1
1
2
3
–1
1
2
3
Figure 9.4: x./ and h.t   / for t < 0.
Notice that for t < 0 there is no overlap between the nonzero portions of x./ and h.t  /, therefore, x./h.t   / will be zero for all values of  and, therefore, the integral will also be
zero.

9.1. THE IMPULSE RESPONSE OF A SIMPLE RC CIRCUIT
109
x(τ)
h(t–τ), t > 0
τ
τ
1.0
0.8
0.6
0.4
0.2
1.0
0.8
0.6
0.4
0.2
–1
1
2
3
–1
1
2
3
Figure 9.5: x./ and h.t   / for t > 0.
For t > 0 the overlap between the nonzero portions of x./ and h.t   / is the region
between 0 and t, therefore,
y.t/ D
Z t
0
u./˛e ˛.t /u.t   /d
In this region u./ D 1 as  > 0. Because  < t, t    is greater than zero and u.t   / D 1.
Therefore,
y.t/
D
˛
Z t
0
e ˛.t /d
D
˛e ˛tt
Z t
0
e˛d
D
˛ 1
˛ ete˛jt
0
D
e ˛t 
e˛t   e0
D
e ˛t 
e˛t   1

D

1   e ˛t

110
9. CONTINUOUS TIME CONVOLUTION
y(t)
t
1.0
0.8
0.6
0.4
0.2
–0.5
0.5
1.0
1.5
2.5
2.0
Figure 9.6: Output of the RC filter.
Putting the results for t < 0 and t > 0 together we have
y.t/
D
(
0
t < 0
˛

1   e ˛t
t > 0
D
˛

1   e ˛t
u.t/
(9.1)
Plotting this in Figure 9.6, for ˛ D 1, we see that indeed the voltage across the capacitor
charges up to 1 V and the result of the convolution integral agrees with what we already know
from circuit analysis.
Example 9.2
For our second example let’s look at how the RC circuit responds to an input
pulse with a pulsewidth of two time units. We can represent this pulse as
x.t/ D u.t/   u.t   2/
The impulse response of the RC system is as before. For the figures in this example we have used
˛ D 3.
We will use the same version of the convolution integral
y.t/ D
Z 1
 1
x./h.t   /d
As in the previous case we will flip the impulse response and move it around. In the previous
case we had two situations, t < 0 and t > 0 which resulted in different expressions. In this case
we end up with three situations. The first case is where t < 0. As we can see from Figure 9.7
in this case wherever x./ is nonzero h.t   / is zero and vice versa. Therefore, the product
x./h.t   / is zero for all values of  resulting in y.t/ being zero for t < 0.
Let’s move the impulse response to the right. For 0 < t < 2 the region over which the
product is nonzero is from 0 to t.

9.1. THE IMPULSE RESPONSE OF A SIMPLE RC CIRCUIT
111
x(τ)
h(t–τ), τ < 0
τ
τ
1.0
0.8
0.6
0.4
0.2
1.0
0.8
0.6
0.4
0.2
–1
1
2
3
–1
1
2
3
Figure 9.7: x./ and h.t   / for t < 0.
Therefore we can compute y.t/ as
y.t/
D
Z t
0
x./h.t   /d
D
Z t
0
˛e ˛.t /d
D
˛e ˛t
Z t
0
e˛d
D
˛e ˛t 1
˛ e˛jt
0
D
e ˛t 
e˛t   e0
D
1   e ˛t
If we move the impulse response further to the right so that the leading edge of the flipped
impulse response t is beyond  D 2 we end up in the situation shown in Figure 9.9. In this
configuration the interval over which the product is nonzero is the interval Œ0; 2. The output

112
9. CONTINUOUS TIME CONVOLUTION
x(τ)
h(t–τ), 0 < τ < 2
τ
τ
1.0
0.8
0.6
0.4
0.2
1.0
0.8
0.6
0.4
0.2
–1
1
2
3
–1
1
2
3
Figure 9.8: x./ and h.t   / for 0 < t < 2.
x(τ)
h(t–τ), τ > 2
τ
τ
1.0
0.8
0.6
0.4
0.2
1.0
0.8
0.6
0.4
0.2
–1
1
2
3
–1
1
2
3
Figure 9.9: x./ and h.t   / for t > 2.

9.1. THE IMPULSE RESPONSE OF A SIMPLE RC CIRCUIT
113
y(t)
t
1.0
0.8
0.6
0.4
0.2
1
2
3
4
Figure 9.10: Response of the RC circuit to a pulse input.
then is given by
y.t/
D
Z 2
0
x./h.t   /d
D
Z 2
0
˛e ˛.t /d
D
˛e ˛t
Z 2
0
e˛d
D
˛e ˛t 1
˛ e˛j2
0
D
e ˛t 
e2˛t   e0
D
e ˛.t 2/   e ˛t
Putting all of this together we get
y.t/ D
8
ˆ<
ˆ:
0
t < 0
1   e ˛t
0 < t < 2
1   e ˛.t 2/
t > 2
We plot this in Figure 9.10 for ˛ D 3. As you would expect the response shows the ca-
pacitor charging up during the time the pulse is active and then discharging after the pulse
terminates.
Example 9.3
Finally, let’s look at the convolution of two rectangular pulses of different widths.
Let’s take x.t/ to be a pulse of width one and h.t/ to be a pulse of width two.
x.t/
D
u.t/   u.t   1/
h.t/
D
u.t/   u.t   2/

114
9. CONTINUOUS TIME CONVOLUTION
h(t)
x(t)
t
t
1
2
1
Figure 9.11: The impulse response h.t/ and the input x.t/.
This time we will use the integral
y.t/ D
Z 1
 1
x.t   /h./d
The input and impulse response are plotted in Figure 9.11. Generally when convolving signals
that are both nonzero over a finite interval it is more convenient to flip and move the narrower
signals. However, this might be a matter of preference. You should use whichever formulation
feels more convenient to you. Because of the commutative property of the convolution operation
both will give exactly the same answer.
The signal x.t   / is obtained by flipping x.t/ around the vertical axis and then shifting
it by t. When t is less than zero the impulse response h./ and the flipped and shifted signal
x.t   / are shown in Figure 9.12. As can be seen from the figure there is no region in which
both h./ and x.t   / are nonzero. The impulse response h./ is nonzero for 0 <  < 2 and
x.t   / is zero in this region. Therefore the product x.t   /h./ is zero for all values of  and
the integral, and hence y.t/ is zero for t < 0.
Let’s move x.t   / to the right by letting t be greater than zero while keeping t   1 to be
less than zero. In other words 0 < t < 1. This condition is shown in Figure 9.13. In this situation
the region in which the product x.t   /h./ is nonzero is the interval 0 <  < t. Therefore,
y.t/ D
Z t
0
d D t
where we have used the fact that in the region of integration the product x.t   /h./ is 1.
Continuing to move x.t   / to the right let’s increase t so that t   1 is greater than zero
but keep t to be less than 2. That is 1 < t < 2. This situation is shown in Figure 9.14. We have

9.1. THE IMPULSE RESPONSE OF A SIMPLE RC CIRCUIT
115
h(τ)
x(t–τ)
τ
τ
t
t–1
1
2
Figure 9.12: The impulse response h./ and the input x.t   / for t < 0.
h(τ)
x(t–τ)
τ
τ
t
t–1
1
2
Figure 9.13: The impulse response h./ and the input x.t   / for 0 < t < 1.
shown the region in which both h./ and x.t   / are nonzero with the dashed lines. In this
condition the integral limits will be t   1 and t and
y.t/ D
Z t
t 1
d D t   .t   1/ D 1
where we have again used the fact that the product x.t   /h./ is one in this region. Notice
that this is the region in which the nonzero range of the narrower pulse is completely enclosed
within the range of the wider pulse. As both pulses have constant values the integral under this
condition will remain constant.

116
9. CONTINUOUS TIME CONVOLUTION
h(τ)
x(t–τ)
τ
τ
t
t–1
1
2
Figure 9.14: The impulse response h./ and the input x.t   / for 1 < t < 2.
h(τ)
x(t–τ)
τ
τ
t
t–1
1
2
2
Figure 9.15: The impulse response h./ and the input x.t   / for 2 < t < 3.
If we now increase t to be larger than 2 while keeping t   1 to be less than 2 we will
get the situation shown in Figure 9.15. In this situation The interval over which the product
x.t   /h./ is nonzero is the interval t   1 <  < 2. Therefore, the output is given by
y.t/ D
Z 2
t 1
d D 2   .t   1/ D 3   t
Notice that as t increases in this interval the range of nonzero overlap decreases and hence the
value of y.t/ decreases as well.

9.1. THE IMPULSE RESPONSE OF A SIMPLE RC CIRCUIT
117
h(τ)
x(t–τ)
τ
τ
t
t–1
1
2
2
Figure 9.16: The impulse response h./ and the input x.t   / for t > 3.
y(t)
t
1
1
2
3
Figure 9.17: The result of the convolution of two pulses of unequal width.
And finally we increase t   1, the trailing edge of x.t   / to be greater than 2. This
situation is shown in Figure 9.16. Again there is no overlap between the regions in which h./
is nonzero and the region in which x.t   / is nonzero. Therefore, the product x.t   /h./ is
zero for all values of  and the integral, and hence y.t/, is zero.
We can combine all of these results together to get
y.t/ D
8
ˆˆˆˆˆˆ<
ˆˆˆˆˆˆ:
0
t < 0
t
0 < t < 1
1
1 < t < 2
3   t
2 < t < 3
0
t > 3
The output is plotted in Figure 9.17.

118
9. CONTINUOUS TIME CONVOLUTION
9.2
SUMMARY
In this module we worked through several examples of continuous time convolution. As in
the previous module I strongly recommend that once you have understood the examples and
their solutions you close the book and see if you can solve the examples without looking at the
solutions.

9.3. EXERCISES
119
9.3
EXERCISES
(Answers on the following page)
1. Given the impulse response h.t/ and the input x.t/, in each case find the output y.t/.
(a) x.t/ D u.t   2/;
h.t/ D e ˛tu.t/ where u.t/ is the unit step function
(b) x.t/ D u.t/;
h.t/ D e ˛.t 2/u.t   2/ where u.t/ is the unit step function
(c) x.t/ D u.t/   u.t   1/;
h.t/ D u.t   1/   u.t   2/ where u.t/ is the unit step
function
(d) h.t/ D u.t/   u.t   2/, x.t/ D u.t   1/   u.t   2/, where u.t/ is the unit step func-
tion
(e) h.t/ D e 2tu.t/, x.t/ D u.t   1/   u.t   2/, where u.t/ is the unit step function
(f) h.t/ D e 2tu.t/ and x.t/ D e tu.t/, where u.t/ is the unit step function
(g) h.t/ D e 2tu.t/ and x.t/ D ı.t   2/, where ı.t/ is the impulse function
2. The input-output relationship of a linear time-invariant system is given by
y.t/ D   1
˛
Z t
 1
x./d
What is the impulse response of this system? Can you come up with a circuit that would
have this impulse response?
3. The input-output relationship of a linear time-invariant system is given by
y.t/ D
Z t
 1
e .t /x.   1/d
What is the impulse response of this system?
4. Let
a.t/
D
e 2tu.t/
b.t/
D
u.t/
c.t/
D
ı.t/ C ı.t   1/
Find a.t/ ~ b.t/ ~ c.t/

120
9. CONTINUOUS TIME CONVOLUTION
9.4
ANSWERS
1.
(a)
y.t/ D 1
˛

1   e ˛.t 2/
u.t   2/
(b)
y.t/ D 1
˛

1   e ˛.t 2/
u.t   2/
(c)
y.t/ D
8
ˆˆ<
ˆˆ:
0
t < 1
t   1
1 < t < 2
3   t
2 < t < 3
0
t > 3
(d)
y.t/ D
8
ˆˆˆˆ<
ˆˆˆˆ:
0
t < 1
t   1
1 < t < 2
1
2 < t < 3
4   t
3 < t; 4
0
t > 4
(e)
y.t/ D
8
ˆˆˆˆ<
ˆˆˆˆ:
0
t < 1
1
2

1   e 2.t 1/
1 < t < 2
1
2

e 2.t 2/   e 2.t 1/
t > 2
(f)
y.t/ D
 e t   e 2t
u.t/
(g)
y.t/ D e 2.t 2/u.t   2/
2.
h.t/ D   1
˛ u.t/
3.
h.t/ D e .t 1/u.t   1/

9.4. ANSWERS
121
4.
y.t/ D
8
ˆˆˆˆ<
ˆˆˆˆ:
0
t < 0
1
2
 1   e 2t
0 < t < 1
1
2

2   e 2   e 2.t 1/
t > 1


123
M O D U L E
10
Fourier Series
We have spent most of our time until now on systems. We have defined
their properties and have narrowed our focus to linear time-invariant sys-
tems. We have shown that such systems can be completely characterized
by their impulse response. We will now pivot to focus mostly on the signals
part of the course and develop tools to analyze signals. In order to develop
the tools we will use the properties of linear time-invariant systems.
Let’s begin with a very simple world in which all signals look some-
thing like those shown in Figure 10.1. Suppose I also know that the out-
put of a linear time-invariant system to an input—we will call it a basis
function—b.t/ shown in Figure 10.2 is y.t/.
Comparing b.t/ with the first signal x.t/ in Figure 10.1 we can see that we can write x.t/
in terms of shifted versions of b.t/. We can write x.t/ as
x.t/ D b.t/ C b.t   2/ C b.t   4/
Let’s define
bk.t/ D b.t   k/
Then we can write x.t/ as
x.t/ D 1  b0.t/ C 0  b1.t/ C 1  b2.t/ C 0  b3.t/ C 1  b4.t/
or in a more succinct form as
x.t/ D
4
X
kD0
akbk.t/
where
ak D
(
1
k D 0; 2; 4
0
k D 1; 3
We can use the same approach to express v.t/ in terms of shifted and scaled versions of
b.t/. Looking at the function we can see that we can write v.t/ as
v.t/ D 0:5b.t C 1/   b.t/   0:5b.t   1/ C 0  b.t   2/ C 0:5b.t   3/ C 1:5b.t   4/

124
10. FOURIER SERIES
x(t)
1
2
1
–1
–2
2
3
4
5
w(t)
1
2
1
–1
–2
2
3
4
5
v(t)
1
2
1
–1
–2
2
3
4
5
z(t)
1
2
1
–1
–2
2
3
4
5
Figure 10.1: Signals in a simple world.
b(t)
1
2
1
–1
–2
2
3
4
5
Figure 10.2: The basis function b.t/.
or
v.t/ D
4
X
kD 1
akbk.t/

10. FOURIER SERIES
125
where
ak D
8
ˆˆˆˆˆˆˆˆˆ<
ˆˆˆˆˆˆˆˆˆ:
0:5
k D  1
 1
k D 0
 0:5
k D 1
0
k D 2
0:5
k D 3
1:5
k D 4
What use is this rewriting of x.t/ and v.t/ in terms of shifted and scaled versions of b.t/?
Recall that we said that for a particular linear time invariant system the output of the system for
an input of b.t/ is y.t/. Also recall that a defining property of linear time invariant systems is that
if we know the output for specific inputs we also know the outputs for all linear combinations
of time-shifted versions of those inputs. As the system we are dealing with is a linear time-
invariant system knowing the output for b.t/ means we also know the output of the system if
the input is b.t   k/. The output of the system for this input (because of time-invariance) will be
y.t   k/. And if the input is akb.t   k/, or akbk.t/, the output is aky.t   k/. Finally, if the input
is P akbk.t/. The output will be P aky.t   k/. Knowing the output of a linear time invariant
system for the input b.t/ we can write down the output of the system to any signal from our
simple world. The only thing we need to do is to find the values of ak.
We have been able to find the values of ak by observation. However, in general, we don’t
have to rely on our visual acuity to find the values of ak. We can simply multiply the signal with
bk.t/ and integrate. Let’s explore this with w.t/. Multiplying w.t/ with b0.t/ results in a signal
which is equal to 2 in the interval [0,1] and 0 outside. Integrating this function we get a value of
2 for a0. Similarly multiplying w.t/ with b1.t/ results in a function which has a value of 1.5 for
t between 1 and 2, and 0 otherwise. Integrating this gives us a value of 1.5 for a1. This is shown
in Figure 10.3
We can write this more generally as
ak D
Z 1
 1
w.t/bk.t/dt
We have used limits of  1 to 1 here but we could have used any limits that included bk.t/.
If we know the output of a linear time-invariant system for b.t/, writing signals in terms
of the set fbk.t/g allows us to specify the output of this system for reasonably complex inputs,
albeit from our simple world, as long as we know the output when the input is the very simple
function b.t/. The set fbk.t/g is called a basis set for the signals of our simple world. It would
be nice if we could find a basis set for signals which are not limited to our simple world. As it
happens there are many such basis sets. However, one set of signals is particularly well suited for
linear time-invariant systems. This set is the infinite set consisting of the basis vectors fej!tg.
Why infinite? Well we are talking about a much much more complicated world than our simple

126
10. FOURIER SERIES
w(t)
1
2
1
–1
–2
2
3
4
5
b0(t)
1
2
1
–1
–2
2
3
4
5
w(t)b0(t)
1
2
1
–1
–2
2
2
3
4
5
w(t)
1
2
1
–1
–2
2
3
4
5
b1(t)
1
2
1
–1
–2
2
3
4
5
w(t)b1(t)
1
2
1
–1
–2
2
1.5
3
4
5
Figure 10.3: Expansion of w.t/ in terms of fbk.t/g.
world. So what is the very nice property that differentiates this set from other sets? Consider a
linear time invariant system with impulse response h.t/. We know that the output y.t/ is given
by the convolution integral
y.t/ D
Z 1
 1
x.t   /h./d
If the input x.t/ is given by
x.t/ D ej!t

10. FOURIER SERIES
127
the output of the system is given by
y.t/ D
Z 1
 1
ej!.t /h./d D ej!t
Z 1
 1
h./e j!d
Let’s define
H.!/ D
Z 1
 1
h./e j!d
then regardless of the value of ! the response of the linear time invariant system to an input of
the form ej!t will be given by
y.t/ D ej!tH.!/
But can we write our input in terms of ej!t? Si si puede. We will do this in two steps. First we will
restrict the set of signals we are interested in to periodic signals. Once we are comfortable with
representing periodic signals in terms of our basis set we will extend our approach to (almost)
all signals.
Lets suppose the period of the periodic signal is denoted by T and the fundamental fre-
quency is given by fo D 1=T in Hz, or !o D 2fo D 2=T in radians. Our claim for this pe-
riodic signal is that it can be represented by linear combinations of fejn!otg. In other words we
can write the periodic signal x.t/ as
x.t/ D
1
X
kD 1
akejk!ot
If we could do this then based on our earlier development we can write y.t/ as
y.t/ D
1
X
kD 1
akejk!otH.k!o/
No convolution needed! All we need to do is find H.!/ once and then whenever we need to
find the output for a particular periodic signal all we need to do is find the coefficients fakg. But
how are we to do that?
To answer this let’s look at the integral over one period of the product of ejk!ot and
e jm!ot.
Z
T
ejk!ote jm!otdt

128
10. FOURIER SERIES
We will look at two cases; when k ¤ m and when k D m. For convenience we will integrate
from 0 to T . First the case where k ¤ m.
Z T
0
ejk!ote jm!otdt
D
Z T
0
ej.k m/!otdt
D
1
.k   m/j!o
ej.k m/!otjT
0
D
1
.k   m/j!o

ej.k m/!oT   e0
D
1
.k   m/j!o

ej.k m/ 2
T T   1

D
1
.k   m/j!o

ej.k m/2   1

D
1
.k   m/j!o
.cos ..k   m/2/ C j sin ..k   m/2/   1/
where we have used Euler’s formula to evaluate ej.k m/2. The cosine of any multiple of 2 is
always 1 and the sine of any multiple of 2 is always 0 so this becomes
1
.k   m/j!o
.1 C j0   1/ D 0
Putting all of this together, for k ¤ m
Z
T
ejk!ote jm!otdt D 0
What happens when k D m? If k D m then k   m D 0 and
Z T
0
ejk!ote jm!otdt
D
Z T
0
ej.k m/!otdt
D
Z T
0
e0dt
D
Z T
0
dt
D
T
Combining these we have
Z
T
ejk!ote jm!otdt D
(
0
k ¤ m
T
k D m

10. FOURIER SERIES
129
If we divide this integral by T we get
1
T
Z
T
ejk!ote jm!otdt D
(
0
k ¤ m
1
k D m
Let’s use this relationship to find the coefficients fakg in the expression
x.t/ D
1
X
kD 1
akejk!ot
To find am we will multiply both sides of this equation by 1
T e jm!ot and integrate over a period.
1
T
Z
T
x.t/e jm!otdt D 1
T
Z
T
1
X
kD 1
akejk!ote jm!otdt
Interchanging the order of summation and integration (with genuflection to Fubini)
1
T
Z
T
x.t/e jm!otdt D
1
X
kD 1
1
T
Z
T
ejk!ote jm!otdt
But the integral on the right is zero for all the terms of the summation in which k ¤ m and is
1 when k D m, so the sum reduces to only one term when k D m and that term is 1. So we get
1
T
Z
T
x.t/e jm!otdt D am
An Alternate Representation
(Optional Reading)
Think about a point in two or three dimensional space. We can represent this vector
as a line from the origin to the point with an arrowhead at the point or we can represent
the point with its cartesian coordinates. But what exactly are these coordinates? Let’s take a
simple example of the point .3; 4/ in two dimensional space. We can represent this point as
we did with the cartesian coordinates .3; 4/ or we can represent it as a vector with magnitude
of 5 and an angle of 53:13o or we can represent the vector as
v D 3ux C 4uy
or we can represent the vector as
v D
"
3
4
#
:

130
10. FOURIER SERIES
Consider the representation
v D 3ux C 4uy
What we are saying here is that the vector representing the point is a linear combi-
nation of the basis vectors denoted by ux and uy. The coordinates are simply the weights
of the individual basis vectors. Conceptually this is similar to what we were doing before
with the two basic signals—the rectangle and the triangle—back when we were defining a
linear time-invariant system. Practically though there are several differences. The basis set
fux; uyg allow us to represent any point in two dimensional space. We can go in the other
direction with the same ease; it is easy to obtain the weights on the individual basis vectors
by simply taking the dot product of the vector with the basis vector of interest. So,
v  ux D .3ux C 4uy/  ux D 3ux  ux C 4uy  ux D 3  1 C 4  0 D 3
and
v  uy D .3ux C 4uy/  uy D 3ux  uy C 4uy  uy D 3  0 C 4  1 D 4
What makes life easy for us in this case are two facts
1. The basis vectors are orthogonal to each other and have unit magnitude.
2. We have something called the dot product or the inner product that allows us to both
define what orthogonal means and then gives us the machinery to decompose a vector
in two dimensional space into components in terms of the basis vectors.
Our task is now clear: we need to find a definition of an inner product which will work for
the signals in which we are interested. We need to find a basis set which is orthogonal in
terms of this inner product, in which each element has unit magnitude, and the combina-
tions of which will give us all of the signals in which we are interested. Let’s deal with the
last issue first—what are all the signals of interest to us? Practically speaking these would
be all signals with a finite amount of energy. For an aperiodic signal x.t/ this means that
Z 1
 1
jx.t/j2 < 1
For periodic signals we will limit the integral to be over one period. Are there basis vectors
that can be used to represent all signals with finite energy? Turns out there are quite a few,
and we get to choose which set we want. We will select a set which has the additional very
nice property that given any member of this basis set and the impulse response of a linear
time invariant system we can easily figure out the output.
As we have discussed previously this set is the infinite set consisting of the basis
vectors fej!tg. How can we use these ideas of inner product and basis sets to write our input
in terms of ej!t? To see this let us first restrict the set of signals to periodic signals. Lets

10. FOURIER SERIES
131
suppose the period of the periodic signal is denoted by T and the fundamental frequency
is given by fo D 1=T in Hz, or !o D 2fo D 2=T in radians. Our claim for this periodic
signal is that it can be represented by linear combinations of fejn!otg. In other words we
can write the periodic signal x.t/ as
x.t/ D
1
X
nD 1
anejn!ot
If we could do this then based on our earlier development we can write y.t/ as
y.t/ D
1
X
nD 1
anejn!otH.n!o/
To find the values of an let’s refer back to our simple example of a vector in two dimensional
space. We found the weights of the basis vector by taking the inner product of the vector
with the basis functions. We know what the basis functions are in this case—ejn!ot. All we
need is the inner product. For continuous functions f .t/ and g.t/ we can define an inner
product as
hf .t/; g.t/i D ˛
Z
f .t/g.t/dt
where g.t/ is the complex conjugate of g.t/ and ˛ is a fudge factor we will use to make
sure that our basis vectors have a magnitude of one. For our specific case we will pick ˛ to
be equal to 1=T and we will pick the limits of integration to cover one period. That means
we can integrate from 0 to T or from  T=2 to T=2 or whatever makes the integral more
tractable. So our inner product is
hf .t/; g.t/i D 1
T
Z T
0
f .t/g.t/dt
Let’s use this definition of the inner product to see if the set fejn!otg is orthonormal. First,
lets see if this set is orthogonal. To do this we pick two basis functions ejn!ot and ejm!ot
where m ¤ n and examine their inner product.
hejn!ot; ejm!oti D 1
T
Z T
0
ejn!ote jm!otdt
which we have already shown to be zero if n ¤ m. This means that the elements of the set
fejn!otg are orthogonal. To check orthonormality we need to see if the magnitude of these
elements is one. We can do that by taking the inner product of ejn!ot with itself
hejn!ot; ejn!oti D 1
T
Z T
0
ejn!ote jn!otdt

132
10. FOURIER SERIES
which we have already shown to be 1. So the set fejn!otg is indeed orthonormal. Now if we
want to find the components an all we need to do is to take the inner product of the signal
x.t/ with the unit vector in the nth direction—ejn!ot.
hx.t/; ejn!oti
D
*
1
X
kD 1
akejn!ot; ejn!ot
+
D
1
T
Z T
0
1
X
kD 1
akejk!ote jn!otdt
D
1
X
kD 1
ak
"
1
T
Z T
0
ej.k n/!otdt
#
D
an
where the last equality is a result of the fact that the quantity in brackets is zero for all values
of k except for k D n. So,
an D hx.t/; ejn!oti D 1
T
Z
T
x.t/e jn!otdt
Note that we have replaced the limits of the integral from 0 and T to a more general form.
This is to indicate that integral limits can be anything as long as they cover one whole period.
So instead of taking the integral from 0 to T we could take the integral from  T=2 to T=2
or from  T=4 to 3T=4 etc.
Does this integral always exist? No, it doesn’t but the conditions under which it does not
exist are extremely rare from an engineering point of view. The conditions for the existence of
the integral are called the Dirichlet conditions for convergence. They are:
1. The function x.t/ should be absolutely integrable.
Z
jx.t/j < 1
2. The function has bounded variation which means that in any finite interval it has only a
finite number of zero crossings.
3. It has a finite number of finite discontinuities.
Pretty much every signal we will ever deal with will satisfy these conditions.
We can speak of an as the coefficients of the Fourier series expansion or we can view them
as the components of the signal at different frequencies. In particular a0 is the component of the

10.1. FOURIER SERIES EXPANSION OF A SQUARE WAVE
133
signal at 0 frequency which we refer to as the DC value or the average value of the function. To
check that a0 is indeed the average value of the function we can plug in n D 0 in the equation
for obtaining the Fourier series coefficients and we get
a0 D 1
T
Z
T
x.t/e j.0/!otdt D 1
T
Z
T
x.t/dt
which is indeed the average value of x.t/.
10.1
FOURIER SERIES EXPANSION OF A SQUARE WAVE
Let’s find the Fourier series coefficients an for the square wave shown in Figure 10.4.
an
D
1
T
Z T=2
 T=2
e jn!0tdt
D
1
T
Z T0
 T0
e jn!0tdt
D
1
T
1
 jn!0
 e jn!0T0   ejn!0T0
D
1
jn!0T
 ejn!0T0   e jn!0T0
substituting
!0 D 2
T
an
D
T
jn2T
 ejn!0T0   e jn!0T0
D
1
n
ejn!0T0   e jn!0T0
2j
D
sin.n!0T0/
n
Let’s see what this coefficients look like for different values of the pulse width. First, let’s
pick T0 to be equal to T=4. Plugging in for !o and To in the expression for an we get
an D
sin

n2
T
T
4

n
D sin.n=2/
n
Plugging in for n D 0 in this expression we get
a0 D sin.0/
0
D 0
0

134
10. FOURIER SERIES
–T0
T0
–T
x(t)
T
t
Figure 10.4: A square wave with period T and pulse width 2T0.
Because we have a 0=0 form we can use l’Hopital’s rule to evaluate a0. A quick reminder: the
way we use l’Hopital’s rule is to take the derivative of the numerator and the denominator with
respect to the variable being substituted for and then evaluate the resulting expression. In this
case the variable is n so taking the derivative with respect to n and then evaluating the result at
n D 0 we get
a0 D
d
dn sin.n=2/
d
dnn
jnD0 D =2 cos.n=2/

jnD0 D =2

D 1
2
a1 is much simpler to evaluate
a1 D sin.n=2/
n
jnD1 D sin.=2/

jnD1 D 1

What about a 1?
a 1 D sin.n=2/
n
jnD 1 D sin. =2/
 
jnD1 D  1
  D 1

which is the same as a1. In fact the expression for an is even in terms of n
a n D sin. n=2/
 n
D   sin.n=2/
 n
D sin.n=2/
n
D an
Continuing with our calculation of the values of an
a2
D
sin.n=2/
n
jnD2 D sin./
2
D 0
a3
D
sin.n=2/
n
jnD3 D sin.3=2/
3
D   1
3
a4
D
sin.n=2/
n
jnD4 D sin.2/
4
D 0
a5
D
sin.n=2/
n
jnD5 D 5=2
5
D 1
5
:::

10.2. TIME FREQUENCY DUALITY
135
–5ω0
–4ω0
–3ω0
3ω0
–2ω0
2ω0
4ω0
5ω0
–ω0
ω0
ω
0
Figure 10.5: Fourier series coefficients for a square wave with period T and pulse width T=2.
We don’t need to compute the values of an for negative values of n because the expression for
an is even in terms of n. Plotting these coefficients we obtain Figure 10.5
10.2
TIME FREQUENCY DUALITY
Let’s continue with our previous example of a square wave but his time let’s view it as a train of
pulses and let’s play a bit with the width of the pulse. In the previous example the width of the
pulse was T=2. We will make the pulse narrower by setting To to T=8 so the width of the pulse
is T=4. Plugging in for !o and To in the expression for an we get
an D
sin

n2
T
T
8

n
D sin.n=4/
n
Plugging in n D 0 will again give us a 0=0 form so we again use l’Hopital’s rule to get
a0 D
d
dn sin.n=4/
d
dnn
jnD0 D =4 cos.n=4/

jnD0 D =4

D 1
4

136
10. FOURIER SERIES
–5ω0
–4ω0
–3ω0
3ω0
–2ω0
2ω0
4ω0
5ω0
–ω0
ω0
0
ω
Figure 10.6: Fourier series coefficients for a square wave with period T and pulse width T=4.
Computing an for other values of n we obtain
a1
D
sin.n=4/
n
jnD1 D sin.=4/

D 1=
p
2

D
1
p
2
a2
D
sin.n=4/
n
jnD2 D sin.=2/
2
D 1
2
a3
D
sin.n=4/
n
jnD3 D sin.3=4/
3
D 1=
p
2
3
D
1
3
p
2
a4
D
sin.n=4/
n
jnD4 D sin./
4
D 0
a5
D
sin.n=4/
n
jnD5 D sin.5=4/

D  1=
p
2
5
D  1
5
p
2
:::
Plotting these in Figure 10.6 we see that there is a qualitative difference between the
spread of the Fourier series coefficients. If we think of the energy in the signal as the sum of
the squares of the coefficients, the energy in the coefficients for the narrower pulse is spread
much broader than the energy in the coefficients of the broader pulse. A measure of the spread
of energy in the frequency components is the bandwidth of the signal. Without giving a precise
definition of bandwidth we can see that the bandwidth of the narrower pulse train is larger than
the bandwidth of the broader pulse train. We will find that this is a basic rule. The narrower a
signal is in the time domain the broader will be its spectral footprint corresponding to a broader
bandwidth. When we think about digital communication we can see that bandwidth is very
closely related to rate. The more bits we need to send the narrower the pulses representing the
bits have to be. If we want to send information at 1000 bits per second and we are using one
pulse per bit the width of the pulse has to be less than one millisecond. If we want to up the
rate to a 1 megabits per second the width of the pulse has to be less than .001 milliseconds. The
higher the rate, the narrower the pulse and hence the more bandwidth we need.

10.2. TIME FREQUENCY DUALITY
137
The narrower a signal is in the time domain the wider will be its frequency profile and vice
versa.
For our next example let’s try a really simple periodic function
x.t/ D cos.!ot/
For this particular example we don’t actually need to evaluate an integral. That is because we
already know a way to write the cosine function in terms of complex exponentials
cos.!ot/ D 1
2ej!ot C 1
2e j!ot
So, the Fourier series for x.t/ D cos.!ot/ has only two nonzero coefficients, a1 and a 1 with
a1 D 1
2 D a 1
Let’s look at a the Fourier series expansion, or the frequency representation of a couple
more signals. Let’s begin with
x.t/ D sin.!ot/
We can find the expansion for this in the same way we did for cos.!ot/ by using Euler’s Excellent
Formula.
x.t/ D sin.!ot/ D ej!ot   e j!ot
2j
D   1
2j e j!ot C 1
2j ej!ot
Comparing this with the standard Fourier series representation
x.t/ D
1
X
nD 1
anejn!ot
we can see that the Euler’s formula does indeed give us the signal as a weighted sum of fejn!otg
where
an D
8
ˆˆˆˆ<
ˆˆˆˆ:
  1
2j
n D  1
1
2j
n D 1
0
otherwise
Let’s get a bit more adventurous and find the frequency representation, or the Fourier
series representation of the sawtooth wave shown in Figure 10.7.

138
10. FOURIER SERIES
1
–1
–4
–2
–3
–1
2
x(t)
t
4
1
5
3
Figure 10.7: A sawtooth wave.
Clearly this is a periodic wave. The period is 2, and therefore, !o D . The Fourier series
representation is given by
x.t/ D
1
X
nD 1
anejnt
with the coefficients an given by
an
D
1
2
Z 1
 1
te jntdt
D
1
2

t
 jn  1
.jn/2

e jntj1
 1
D
1
2

1
 jn C
1
.n/2

e jn    1
 jn C
1
.n/2

ejn

D
1
2
  1
jn
 e jn C ejn
C
1
.n/2
 e jn   ejn
D
  1
jn
ejn C e jn
2

 1
.n/2
ejn   e jn
2

The first term in parentheses is cos.n/ while the second term in parentheses is j sin.n/.
cos.n/ D
(
 1
n odd
1
n even
D . 1/n
and sin.n/ is zero for all n. Therefore
an D   1
jn . 1/n D j
n . 1/n

10.3. SUMMARY
139
10.3
SUMMARY
In this module we learned how to represent a periodic function in terms of complex exponentials
as
x.t/ D
1
X
nD 1
anejn!ot
where the coefficients an are given by
an D 1
T
Z
T
x.t/e jn!otdt
We computed the Fourier series for a couple of square waves and a sawtooth wave and looked
at the effect of narrowing the pulse width of the square wave on the Fourier series coefficients.

140
10. FOURIER SERIES
10.4
EXERCISES
(Answers on the following page)
1. Given the signal
x.t/ D 2 cos

4 t

C 3 cos
3
4 t

determine the fundamental frequency !o and the Fourier series coefficients fakg such that
x.t/ D
1
X
kD 1
akejk!ot
2. One period of a periodic signal x.t/ with period T D 2 is given by
x.t/ D
(
1
0 < t < 1
0
1 < t < 2
Find the Fourier series coefficients fakg such that
x.t/ D
1
X
kD 1
akejkt
Can you relate this to what we did in class?
3. One period of a periodic signal x.t/ with period T D 2 is given by
x.t/ D t;
  1 < t < 1
Find the Fourier series coefficients fakg such that
x.t/ D
1
X
kD 1
akejkt
4. One period of a periodic signal x.t/ with period T D 2 is given by
x.t/ D
(
t C 1
 1 < t < 0
1   t
0 < t < 1
Find the Fourier series coefficients fakg such that
x.t/ D
1
X
kD 1
akejkt

10.5. ANSWERS
141
10.5
ANSWERS
1.
!o D 
4
a1 D a 1 D  1I
a3 D a 3 D 3
2
2.
an D e jn 
2
sin
n
2

n
This waveform is a shifted version of the square wave in the example with T0 D 1=2 and
the shift t0 D 1=2.
3.
an D j
n . 1/n
4.
an D
1
.n/2 .1   cos.n//


143
M O D U L E
11
Fourier Series – Properties and
Interpretation
We know how to compute the Fourier series. While this is important the
Fourier series is useful only if we understand how this representation of the
Fourier series relates to the time function from which we calculated it. And
what it can tell us about the function that we could perhaps have not seen
so easily before. Let’s start with the fact that the Fourier series coefficients
are a unique representation of the periodic time function. Given a set of
Fourier series coefficients and the fundamental frequency there is one and
only one periodic function that we can recover from them. And, vice versa.
We could write this as
x.t/ , fang; !o
Another way of thinking about this is that the time function x.t/ and the Fourier coefficients
fang are two ways of looking at the same thing.
One of the obvious things it implies is that the transformation from one representation to
the other is linear; the Fourier coefficients of the sum of two signals are the sum of the Fourier
coefficients of individual signals. In other words, if we have two signals x.t/ and y.t/ which are
both periodic with the same period T and have Fourier series coefficients fang and fbng then the
Fourier series coefficients of any linear combination of x.t/ and y.t/ will be a linear combination
of fang and fbng. Suppose we construct a signal
z.t/ D ˛x.t/ C ˇy.t/
The Fourier series coefficients fcng corresponding to z.t/ are given by
cn D ˛an C ˇbn
This is a very nice property for obvious reasons. There are many times when this property will
make life easy for us and let us work with linear combinations of signals without requiring us to
compute complicated integrals.
We know how to compute the Fourier series coefficients. Remember that the Fourier series
coefficients make up an alternate representation of the time function. Therefore, the properties of
the Fourier series are connected to the properties of the time function. In this module we look

144
11. FOURIER SERIES – PROPERTIES AND INTERPRETATION
at some of those connections. We have computed the Fourier series coefficients for different
square waves and a sawtooth wave. You might have noticed that while all of these functions are
real valued, the Fourier series coefficients for the square waves were real while the Fourier series
coefficients for the sawtooth waveforms were purely imaginary. So what distinguishes the first
two waveforms from the last one? You might have noticed that the two square waves we used
were even functions of time while the sawtooth was an odd functions of time. Let’s connect the
property of evenness and oddness of a time function to the values of the coefficients.
11.1
EVEN AND ODD FUNCTIONS AND THEIR
COEFFICIENTS
The coefficients fang are even functions of n when x.t/ is an even function of t. To show this
let’s begin with the general expression for computing the Fourier series coefficients and then use
the fact that x.t/ D x. t/ to get the following:
an
D
1
T
Z
T
x.t/e jn!0tdt
D
1
T
Z
T
x. t/e jn!0tdt
D
1
T
Z
T
x.t/ejn!0tdt
D
1
T
Z
T
x.t/e j. n/!0tdt
D
a n
(In the third equation above we have used a variable substitution).
The Fourier series coefficients are even functions of n when the periodic function x.t/ is an
even function of t.

11.1. EVEN AND ODD FUNCTIONS AND THEIR COEFFICIENTS
145
What if x.t/ was an odd function? Then following the same logic and using the fact that
for x.t/ odd, x.t/ D  x. t/ we get
an
D
1
T
Z
T
x.t/e jn!0tdt
D
1
T
Z
T
 x. t/e jn!0tdt
D
  1
T
Z
T
x.t/ejn!0tdt
D
1
T
Z
T
x.t/e j. n/!0tdt
D
 a n
The Fourier series coefficients are odd functions of n when the periodic function x.t/ is an
odd function of t.
We can look at this in a slightly different way using Euler’s formula.
an
D
1
T
Z T=2
 T=2
x.t/e jn!0tdt
D
1
T
Z T=2
 T=2
x.t/ .cos.n!0t/   j sin.n!0t// dt
D
1
T
Z T=2
 T=2
x.t/ cos.n!0t/dt   j 1
T
Z T=2
 T=2
x.t/ sin.n!0t/dt
If x.t/ is even then the integrand in the first integral is also even as cos.n!0t/ is even and even
 even D even. The integrand in the second integral is odd as sin.n!0t/ is odd and even  odd
D odd. We are integrating over a symmetric interval and the integral over a symmetric interval
of an odd function is identically zero, therefore, if x.t/ is even the Fourier series coefficients are
given by
an D 1
T
Z T=2
 T=2
x.t/ cos.n!0t/dt
We can see from this that an is an even function of n as
a n D 1
T
Z T=2
 T=2
x.t/ cos. n!0t/dt D 1
T
Z T=2
 T=2
x.t/ cos.n!0t/dt D an
We can also see something else from this equation. If x.t/ besides being even is also real, then
an will be real.

146
11. FOURIER SERIES – PROPERTIES AND INTERPRETATION
The Fourier series coefficients are real even functions of n when the periodic function x.t/
is a real even function of t.
What if x.t/ is odd? In this case the first integrand will be odd and the second integrand
will be even so we will have
an D  j 1
T
Z T=2
 T=2
x.t/ sin.n!0t/dt
If x.t/ is real besides being odd then an will be purely imaginary.
The Fourier series coefficients are purely imaginary, odd functions of n when the periodic
function x.t/ is a real and odd function of t.
So what happens if x.t/ is neither even nor odd? We explore that next in the context of
time shifts.
11.2
TIME SHIFTS ARE PHASE SHIFTS
Let’s begin exploring that by rewriting the Fourier series of the even square wave.
x.t/ D
1
X
nD 1
anejn!ot D
 1
X
nD 1
anejn!ot C a0 C
1
X
nD1
anejn!ot
If we replace n by  n in the first summation we get
 1
X
nD 1
anejn!ot D
 1
X
 nD 1
a nne jn!ot D
1
X
nD1
a ne jn!ot D
1
X
nD1
a ne jn!ot
So now we have
x.t/ D
1
X
nD1
a ne jn!ot C a0 C
1
X
nD1
anejn!ot
The two summations have the same limits so we can put them together
x.t/ D a0 C
1
X
nD1

a ne jn!ot C anejn!ot
Recall that the coefficients we obtained were even functions of n, in other words an D a n.
Therefore, we can replace a n with an in the summation
x.t/ D a0 C
1
X
nD1

ane jn!ot C anejn!ot

11.2. TIME SHIFTS ARE PHASE SHIFTS
147
–0.5T0
1.5T0
–T
x(t)
T
t
Figure 11.1: The square wave with period T and pulse width 2T0 shifted by T0=2.
or
x.t/ D a0 C
1
X
nD1
an

e jn!ot C ejn!ot
but the quantity in brackets is just 2 cos.n!ot/. Therefore,
x.t/ D a0 C
1
X
nD1
2an cos.n!ot/
The square wave in our example is a sum of sinusoids oscillating at frequencies which are integer
multiples of the fundamental frequency of the square wave.
What happens if we take the square wave and shift it in time by To=2 so that it is neither
even nor odd as shown in Figure 11.1. Clearly the coefficient a0 will not change as the area
under the curve over one period remains the same. Let’s take a look at what happens to the
other coefficients.
an
D
Z T=2
 T=2
x.t/e jn!otdt
D
Z  T0=2
 T=2
x.t/e jn!otdt C
Z 3T0=2
 T0=2
x.t/e jn!otdt C
Z T=2
3T0=2
x.t/e jn!otdt
In the interval corresponding to the first and third integrals x.t/ is zero therefore, the integral
is zero. So we are left with
an
D
Z 3T0=2
 T0=2
x.t/e jn!otdt
D
 1
jn!oT
h
e jn!o
3T0
2
  ejn!o
T0
2
i
D
 1
jn2
h
e jn!o
3T0
2
  ejn!o
T0
2
i

148
11. FOURIER SERIES – PROPERTIES AND INTERPRETATION
where we have replaced !o with 2=T in the last equation. We can now take e jn!oT0=2 in
common to get
an
D
 e jn!oT0=2
jn2

e jn!oT0   ejn!oT0
D
e jn!oT0=2
n
ejn!oT0   e jn!oT0
2j

D
e jn!oT0=2 sin.n!oT0/
n
This looks like the expression we had for the even square wave with an additional complex
exponential factor. This complex exponential prevents the coefficients from being even functions
of n. So how does this effect our sinusoidal representation? We could replace this expression for
an in our Fourier series expansion, break up the summation into three intervals—from negative
infinity to  1, 0, and from 1 to infinity as we did previously and see the effect. It is a lot simpler
to just take a look at the effect for one value of n and a numerical value for T0. As before let’s
set T0 D T=4. Then
an D e jn!oT=8 sin.n=2/
n
Let’s plug this in to the Fourier series expression
x.t/
D
 1
X
nD 1
e jn!oT=8 sin.n=2/
n
ejn!ot C a0 C
1
X
nD1
e jn!oT=8 sin.n=2/
n
ejn!ot
D
a0 C
1
X
nD1
ejn!oT=8 sin. n=2/
 n
e jn!ot C
1
X
nD1
e jn!oT=8 sin.n=2/
n
ejn!ot
D
a0 C
1
X
nD1
sin.n=2/
n
h
ejn!oT=8e jn!ot C e jn!oT=8ejn!oti
D
a0 C
1
X
nD1
sin.n=2/
n
h
e jn!o.t T=8/ C ejn!o.t T=8/i
D
a0 C
1
X
nD1
sin.n=2/
n
2 cos.n!o.t   T=8//
We again have a sum of sinusoids. The only difference between the sinusoids here and the sinu-
soids we got before for the even square wave is a phase shift.

11.2. TIME SHIFTS ARE PHASE SHIFTS
149
A delay in a periodic function results in a phase shift in the Fourier representation of that
function.
We will see this effect over and over again in our various representations—a time shift in
the time domain representation will translate to a phase shift in the frequency domain repre-
sentation.
Before we leave this let’s take a look at how the coefficients change for a general periodic
function x.t/ if we shift it in time. Let’s begin with the coefficients for x.t/
an D 1
T
Z
T
x.t/e jn!otdt
Now let’s look at what happens to the coefficients when we shift the periodic function to get
x.t   t0/.
bn D 1
T
Z
T
x.t   t0/e jn!otdt
Let  D t   t0. Then, t D  C t0 and
bn
D
1
T
Z
T
x./e jn!o.Ct0/d
D
1
T
Z
T
x./e jn!oe jn!ot0d
D
e jn!ot0
1
T
Z
T
x./e jn!od
D
e jn!ot0an
(11.1)
So a time shift of t0 results in the Fourier series coefficients being multiplied by e jn!ot0.
For specific values of t0 this quantity can be real or purely imaginary. For example if t0 D T=2.
Then
e jn!ot0 D e jn 2
T
T
2 D e jn D . 1/n

150
11. FOURIER SERIES – PROPERTIES AND INTERPRETATION
11.3
THE FOURIER COEFFICIENTS – MEANING AND
EXTRACTION
Let’s for a moment consider the physical meaning of the Fourier coeffi-
cients. The sinusoidal representation is useful to understand the physical
meaning so let’s begin with that. The general sinusoidal form is
x.t/ D a0 C
1
X
nD1
˛n cos.n!o.t   t0//
where ˛ and t0 in the case of the square wave of the previous example were
2 sin.n=2/
n
and T=8. The coefficient a0 as we have previously said is simply
the DC value of the signal. If we add a positive value to it it will raise the
signal level and if we subtract from it the signal level will be lowered; both without effecting
the shape of the signal. The coefficients ˛n—very closely related to an—are the multipliers of
sinusoids of frequency n!o. These then are the frequency components of a signal. A signal
which has a lot of low frequencies—think of a jackhammer—will have larger magnitudes of an
for smaller values of n. A squeal like you would get from a worn out brake pad will have larger
values for higher values of n and thus the higher frequencies.
If we wanted to isolate these components we could do that by using linear time invariant
systems called filters. A filter that lets through low frequency components while blocking high
frequency components is called a low pass filter. A filter that blocks low frequency components
and lets through high frequency components is called a high pass filter while a filter that blocks
both high and low frequency components letting through only a band of frequency components
is called a bandpass filter.
How these filters work can be easily understood by revisiting our reason for using ej!t
as the basis signal for Fourier analysis. Recall that if the input to a linear time invariant system
with impulse response h.t/ is ej!ot the output is given by
y.t/ D ej!otH.!o/
where
H.!o/ D
Z 1
 1
h.t/e j!otdt
The function H.!/ is called the transfer function of the linear time invariant system. Because
this is a linear time invariant system we can easily find the output of any linear combination of
complex exponentials. So if the input to this system is
x.t/ D ˛0ej!0t C ˛1ej!1t
The output of this system will be
y.t/ D ˛0ej!0tH.!0/ C ˛1ej!1tH.!1/

11.3. THE FOURIER COEFFICIENTS – MEANING AND EXTRACTION
151
Generalizing this if the input is
x.t/ D
1
X
nD 1
anejn!ot
then the output is
y.t/ D
1
X
nD 1
anejn!otH.n!o/
Suppose we wanted to extract the DC and the first frequency component from a periodic
signal. We could do this by using a filter with transfer function
H.!/ D
 1
j!j < 1:5!o
0
otherwise
(The 1.5 in this equation is arbitrary—we could have picked any number greater than one but
less than two). A filter with such a transfer function is physically impossible to build for reasons
we shall see later on (though we can try to approximate it) but it is useful for us right now so we
will ignore its physical impossibility.
If we apply this filter to the input signal we will zero out all components other than for
n D  1; 0; 1. So the output of the filter will be
y.t/ D a 1e j!ot C a0 C a1ej!ot
If we combine the first and third term we will get a raised cosine
y.t/ D a0 C ˛ cos.!o.t   t0//
where ˛ and t0 can be obtained from a1 and a 1. For the even square wave with a period of 0.1
seconds and T0 D :0125 this is plotted in Figure 11.2. If your eyes are good you can extract the
period and frequency of this component from the graph in Figure 11.2.
If we wanted instead to extract the second and third components we could use a filter with
transfer function
H.!/ D
 1
1:5!o < j!j < 3:5!o
0
otherwise
The output of this filter would be the sum of two cosines
y.t/ D ˛1 cos.2!o.t   t0// C ˛2 cos.3!o.t   t0//
For the even square wave with a period of 0.1 seconds and T0 D :0125 this is plotted in Fig-
ure 11.3. You can see that this is not a pure sinusoid but a sum of two sinusoids.

152
11. FOURIER SERIES – PROPERTIES AND INTERPRETATION
0.50
0.45
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
0.00
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Figure 11.2: Output of the lowpass filter which extracts the DC and first component of the
square wave signal.
0.25
0.20
0.15
0.10
0.05
0.00
–0.05
–0.10
–0.15
–0.20
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Figure 11.3: Output of the lowpass filter which extracts the second and third frequency compo-
nent of the square wave signal.

11.4. THE ENERGY IN A SIGNAL DOES NOT CHANGE BASED ON REPRESENTATION
153
11.4
THE ENERGY IN A SIGNAL DOES NOT CHANGE
BASED ON ITS REPRESENTATION
Parseval’s Identity
The last property we will look at is a kind of conservation of energy prop-
erty. Both the time description of the signal x.t/ and the frequency description obtained using
the Fourier series coefficients are the description of the same physical signal. One of the prop-
erties of this signal is the amount of average power in the signal. Because both descriptions are
descriptions of the same signal it makes sense that the signal power computed in either the time
or frequency domain should be identical. The average power for a periodic signal in the time
domain is computed as
P D 1
T
Z
T
jx.t/j2dt
where
jx.t/j2 D x.t/x.t/
In the frequency domain the power is the sum of the magnitude squares of the frequency coef-
ficients
P D
1
X
nD 1
janj2
To show that these two are the same we begin with our usual expression for the Fourier series
expansion.
x.t/ D
1
X
nD 1
anejn!ot
Therefore,
x.t/ D
"
1
X
nD 1
anejn!ot
#
D
1
X
nD 1
a
ne jn!ot

154
11. FOURIER SERIES – PROPERTIES AND INTERPRETATION
Substituting for x.t/ in the integral for the energy we get
1
T
Z
T
jx.t/j2dt
D
1
T
Z
T
x.t/x.t/dt
D
1
T
Z
T
x.t/
1
X
nD 1
a
ne jn!otdt
D
1
X
nD 1
a
n
1
T
Z
T
x.t/e jn!otdt
D
1
X
nD 1
a
nan
D
1
X
nD 1
janj2
11.5
SUMMARY
In this module we examined several useful properties of the Fourier series coefficients.
1. The Fourier series coefficients are even functions of n when the periodic function is an
even function of t.
2. The Fourier series coefficients are odd functions of n when the periodic function is an odd
function of t
3. The Fourier series coefficients are real and even functions of n when the periodic function
is a real and even function of t.
4. The Fourier series coefficients are purely imaginary and odd functions of n when the pe-
riodic function is a real and odd function of t.
5. A delay in the periodic function corresponds to a phase shift in the Fourier series coeffi-
cients.
6. Because both the time function and the Fourier series represent the same physical wave-
form if we calculate physical quantities such as power the result will be the same regardless
of the representation we use.
7. Because the Fourier series coefficients each correspond to a specific frequency we can see
how we can design filters to block, attenuate, or enhance particular frequencies.

11.6. EXERCISES
155
11.6
EXERCISES
(Answers on the following page)
1. Find the Fourier series expansion of
x.t/ D j cos.2t/j
2. You are given a periodic function x.t/ with Fourier series
x.t/ D
1
X
nD 1
anejnt
(a) If y.t/ D x.t   1/ find the coefficients fbng for the Fourier series expansion of y.t/
in terms of fang.
(b) If y.t/ D x.t   1=2/ find the coefficients fbng for the Fourier series expansion of y.t/
in terms of fang.
3. For each of the following determine if x.t/ is real, if x.t/ is even, if x.t/ is odd.
.a/
x.t/ D
1
X
nD0
1
2
n
ejnt
.b/
x.t/ D
1
X
nD 1
1
2
jnj
ejn 
2 t
.c/
x.t/ D
10
X
nD 10
 jn
2

ejnt
4. If
x.t/
D
j sin.!ot/
y.t/
D
j sin.2!ot/
and
z.t/ D x.t/y.t/
(a) Find the Fourier series coefficients for z.t/.
(b) Express z.t/ as a sum of cosines.
5. Given the square wave x.t/ shown in Figure 11.4 which can be represented by the Fourier
series coefficients fang.
Sketch y.t/ if fbng the Fourier series coefficients for y.t/ are given by

156
11. FOURIER SERIES – PROPERTIES AND INTERPRETATION
–8
–7
–6
–5
–4
–3
–2
–1
1
2
3
4
5
6
7
8
x(t)
t
Figure 11.4: Square wave.
(a)
bn D
(
an
n D  1; 0; 1
0
otherwise
(b)
bn D
 an
n D  1; 1
0
otherwise

11.7. ANSWERS
157
11.7
ANSWERS
1.
an D
2 sin
2   4n
4


2   4n
C
2 sin
2 C 4n
4


2 C 4n
2. !o D 
(a)
bn D e jnan
(b)
bn D e jn 
2 an
3.
(a) x.t/ is real, but it is neither even nor odd
(b) x.t/ is real and even
(c) x.t/ is real and odd
4.
(a)
a1 D a 1 D  1
4I
a3 D a 3 D 1
4
(b)
z.t/ D 1
2 cos.3!ot/   1
2 cos.!ot/
5. See Figure 11.5.

158
11. FOURIER SERIES – PROPERTIES AND INTERPRETATION
(a)
1.0
0.5
0.0
–8
–6
–4
–2
0
2
4
6
8
–8
–6
–4
–2
0
2
4
6
8
1.0
0.5
0.0
(b)
t
Figure 11.5: Sketch.

159
M O D U L E
12
The Fourier Transform
We have been looking at the frequency representation of periodic signals.
While periodic signals are important for life, the universe, and everything
we are also interested in the many aperiodic signals that we encounter daily.
For this we need to extend our analysis from periodic to aperiodic signals.
12.1
EXTENDING THE FREQUENCY
VIEW TO APERIODIC FUNCTIONS
So what happens if x.t/ is aperiodic? Let’s assume for the moment that
x.t/ is non-zero over a finite interval Œ0; To. Let’s define a periodic exten-
sion
xp.t/ D
1
X
kD 1
x.t   kT /
where T > To. An example for this function using only three terms is shown in Figure 12.1.
As we increase the number of terms we can see that the final result xp.t/ will be a peri-
odic function. Because xp.t/ is a periodic function with period T we can write a Fourier series
expansion for it with coefficients
ak D 1
T
Z T=2
 T=2
xp.t/e jk!0tdt
Let’s make a few modification to this equation. Let’s move the scale factor to the left and lets
replace !0 with !, so
akT D
Z T=2
 T=2
xp.t/e jk!tdt
When we integrate the right hand side we are left with a function of k!. Let’s make this
explicit by rewriting the left hand side as X.k!/. Now let’s take the limit as T ! 1. When
T ! 1, ! will go to zero, and k! will go to !.
X.!/ D
Z 1
 1
x.t/e j!tdt
where
lim
T !1 xp.t/ D x.t/

160
12. THE FOURIER TRANSFORM
x(t–T )
2T
T
2T
T
2T
T
2T
T
x(t)
x(t–2T )
∑x(t–kT)
t
t
t
t
Figure 12.1: A sum of three shifted versions of a time-limited function.
Now
xp.t/ D
1
X
kD 1
X.k!/
T
ejk!t
but,
T D 2
!
so
xp.t/ D 1
2
1
X
kD 1
X.k!/ejk!t!
Taking the limit as T ! 1 we get
x.t/ D 1
2
Z 1
 1
X.!/ej!td!
These two equations are called the Fourier transform. The equation
X.!/ D
Z 1
 1
x.t/e j!tdt

12.1. EXTENDING THE FREQUENCY VIEW TO APERIODIC FUNCTIONS
161
is sometimes called the forward Fourier transform. A shorthand notation for the forward Fourier
transform is
F Œx.t/ D X.!/ or X.j!/
I prefer using X.!/ so that is what we will use in these modules, but you may find others using
X.ȷ!/ instead. The equation which can be used to “recover” x.t/ from X.!/
x.t/ D 1
2
Z 1
 1
X.!/ej!td!
is often called the inverse Fourier transform. Using the same operator notation as the forward
Fourier transform the inverse Fourier transform is often shown as
F 1 ŒX.!/ D x.t/
Does the Fourier integral always exist? Without proof we will assert that the Fourier
transform of a function x.t/ exists if x.t/ satisfies the Dirichlet conditions—the same conditions
we needed for the Fourier series to exist. This makes sense as the way we obtained the Fourier
transform was by making the aperiodic function x.t/ one period of a periodic function xp.t/.
Remember that these conditions were:
1. The function x.t/ should be absolutely integrable.
Z
jx.t/j < 1
2. The function has bounded variation which means that in any finite interval it has only a
finite number of zero crossings.
3. It has a finite number of finite discontinuities.
Most signals that we will ever deal with will always satisfy these conditions.
There are several properties of the Fourier transform that we can immediately see from
the definition.
The Fourier transform is Linear
The Fourier transform being linear means that if the Fourier
transform of x.t/ is X.!/ and the Fourier transform of y.t/ is Y.!/ then the Fourier transform
of ˛x.t/ C ˇy.t/ is ˛X.!/ C ˇY.!/. The property is easy to prove as the Fourier transform is
an integral transform.
F Œ˛x.t/ C ˇy.t/
D
Z 1
 1
.˛x.t/ C ˇy.t// e j!tdt
D
˛
Z 1
 1
x.t/e j!tdt C ˇ
Z 1
 1
y.t/e j!tdt
D
˛F Œx.t/ C ˇF Œy.t/
D
˛X.!/ C ˇY.!/

162
12. THE FOURIER TRANSFORM
We can show the linearity of the inverse Fourier transform in exactly the same way.
The Fourier Transform is Unique
The Fourier transform X.!/ of the signal x.t/ and the
signal x.t/ form a unique pair. That is if we know x.t/ we exactly know X.!/ and if we know
X.!/ we exactly know x.t/. If we come across a spectral profile Y.!/ which is the same as X.!/
then we know that y.t/, the inverse transform of Y.!/ is equal to x.t/. We can show this using
the linearity property of the Fourier transform. Given two signals x.t/ and y.t/ with Fourier
transforms X.!/ and Y.!/, the Fourier transform of the difference is given by
F Œx.t/   y.t/ D X.!/   Y.!/
If x.t/ equals y.t/ then x.t/   y.t/ is equal to zero. As the Fourier transform of zero is zero,
the left-hand side of this equation is zero, which then implies that
X.!/   Y.!/ D 0
or X.!/ equals Y.!/.
A real signal has a Fourier transform with even magnitude and odd phase
To see this we
begin with a general—not necessarily real—function x.t/ and examine the Fourier transform
of x.t/, the complex conjugate of x.t/. The signal x.t/ can be written as the inverse Fourier
transform of X.!/
x.t/ D 1
2
Z 1
 1
X.!/ej!td!
Taking the complex conjugate of both sides
x.t/
D
 1
2
Z 1
 1
X.!/ej!td!

D
1
2
Z 1
 1
X.!/e j!td!
D
1
2
Z 1
 1
X.!/ej. !/td!
D
F 1X. !/
or
F

x.t/

D X. !/
If x.t/ is real
x.t/ D x.t/
Therefore,
X. !/ D X.!/

12.1. EXTENDING THE FREQUENCY VIEW TO APERIODIC FUNCTIONS
163
Writing X.!/ in polar form
X.!/ D jX.!/j ej.!/
Therefore,
X.!/ D jX.!/j e j.!/
and
X. !/ D jX. !/j e j. !/
For a real x.t/, X. !/ D X.!/ or
jX. !/j e j.!/ D jX.!/j ej.!/
which means that
jX.!/j D jX. !/j
and
.!/ D  . !/
That is, if x.t/ is real the magnitude of the transform X.!/ is real and the phase of X.!/ is odd.
What if x.t/ is real and even. If x.t/ is real
X. !/ D X.!/
and if x.t/ is even
X.!/ D X. !/
Therefore, if x.t/ is real and even
X.!/ D X.!/
and X.!/ is real.
If x.t/ is real but not even we can write it as
x.t/ D Ev.x.t// C Od.x.t//
Also
X.!/ D Re.X.!// C jIm.X.!//
Therefore,
F ŒEv.x.t// D Re.X.!//
and
F ŒOd.x.t// D jIm.X.!//

164
12. THE FOURIER TRANSFORM
Parseval’s relation
x.t/ and X.!/ are two representations of the same signal. Therefore, the
physical properties of the signal, such as the energy of the signal is the same in both domains.
Z 1
 1
jx.t/j2dt
D
Z 1
 1
x.t/x.t/dt
D
Z 1
 1
x.t/
 1
2
Z 1
 1
X.!/ej!td!

dt
D
1
2
Z 1
 1
x.t/
Z 1
 1
X.!/e j!td!dt
D
1
2
Z 1
 1
X.!/
Z 1
 1
x.t/e j!tdtd!
D
1
2
Z 1
 1
X.!/X.!/d!
D
1
2
Z 1
 1
jX.!/j2d!
Example 12.1
Let’s find the Fourier transform of one of our favorite signals the decaying
exponential.
x.t/ D e 2tu.t/
X.!/
D
Z 1
 1
e 2tu.t/e j!tdt
D
Z 1
0
e .2Cj!/tdt
D
 1
2 C j! e .2Cj!/tj1
0
We know what happens when we evaluate e .2Cj!/t at the lower limit of t D 0—we get e0 which
is 1. Let’s take a closer look at what happens at the upper limit. In order to evaluate e .2Cj!/t
at the upper limit of infinty let’s break e .2Cj!/t into its factors e 2t and e j!t. When we
evaluate e 2t at infinity we get zero because this is number smaller than one raised to a power
of infinity. What about e j!t? The quantity e j! is a complex number. What does it mean to
raise a complex number to the power of infinity? Thanks to Euler we know that
e j!t D cos.!t/   j sin.!t/
If we valuate the right hand side at infinity we will get cos.1/   j sin.1/. We don’t know the
value of that expression, however, we do know whatever it is, its finite as both cosine and sine are

12.1. EXTENDING THE FREQUENCY VIEW TO APERIODIC FUNCTIONS
165
bounded by one. So when we evaluate e .2Cj!/t as t ! 1 we get a product of two terms—zero
from e 2t and an unknown but finite value from e j!t. As the product of zero with any finite
value is zero the result is zero. So
X.!/
D
 1
2 C j! e .2Cj!/tj1
0
D
 1
2 C j! Œ0   1
D
1
2 C j!
(12.1)
The function x.t/ was a real function so let’s see if the magnitude and phase of X.!/ follow
the rule we described above. The magnitude of X.!/ is given by
jX.!/j
D
p
X.!/X.!/
D
s
1
2 C j! 
1
2   j!
D
r
1
4 C !2
D
1
p
4 C !2
The phase of X.!/ is given by
X.!/ D tan 1 ImŒX.!
ReŒX.!/
where ImŒX.! is the imaginary part of X.!/ and ReŒX.!/ is the real part of X.!/. We can
write the real and imaginary parts of X.!/ as
X.!/ D
2
4 C !2   j
!
4 C !2
so the phase of X.!/ is
X.!/ D tan 1  !
2


166
12. THE FOURIER TRANSFORM
–10
–5
0
1.0
0.8
0.6
0.4
0.2
0.0
5
10
ω
–10
–5
0
1.5
1.0
0.5
0.0
–0.5
–1.0
–1.5
5
10
ω
|X(ω)|
θ(ω)
Figure 12.2: The magnitude and phase of the Fourier transform of e 2tu.t/.
The magnitude and phase are plotted in Figure 12.2.
Notice that the magnitude is even and the phase is odd.
12.2
SUMMARY
In this module we extended the idea behind the Fourier series to obtain a frequency domain
representation of aperiodic functions. We showed that the Fourier transform of a real function
has a magnitude which is even and a phase which is odd. We also showed that if the time
function is real and even the corresponding Fourier transform is real. As with the Fourier series
coefficients the time function and the Fourier transform represent the same physical signal.
Therefore, if we compute a physical property such as power from either representation we will
get the same result.

12.3. EXERCISES
167
x(t)
t
1
2
1
2
3
Figure 12.3
12.3
EXERCISES
(Answers on the following page)
1. Given
x.t/ D
8
ˆˆ<
ˆˆ:
1
0  t < 1
2
1  t < 2
1
2  t < 3
0
otherwise
shown in Figure 12.3. Without using integration:
(a) Find X.0/
(b) Find
R 1
 1 X.!/d!
2. Find the Fourier transform of
(a)
x.t/ D e 4tu.t/
(b)
x.t/ D e 4jtj
(c)
x.t/ D
 1
jtj < 1
0
otherwise
(d)
x.t/ D ı.t   3/
(e)
x.t/ D
1
X
nD 1
ı.t   nT /
3. Find the inverse Fourier transform of

168
12. THE FOURIER TRANSFORM
(a)
X.!/ D
1
1 C j!
(b)
X.!/ D ı.!   !o/ C ı.! C !o/
(c)
X.!/ D
1
X
nD 1
anı.!   n!o/

12.4. ANSWERS
169
12.4
ANSWERS
1.
(a) X.0/ D 4
(b)
R 1
 1 X.!/d! D 2
2.
(a)
X.!/ D
1
4 C j!
(b)
X.!/ D
8
16 C !2
(c)
X.!/ D 2sin.!/
!
(d)
X.!/ D e j 3!
(e)
X.!/ D
1
X
nD 1
e jn!T
3.
(a)
x.t/ D e tu.t/
(b)
1
 cos.!ot/
(c)
x.t/ D 1
2
1
X
nD 1
anejn!ot


171
M O D U L E
13
Properties of the Fourier
Transform
Let’s acquaint ourselves with this transform and its properties. We will al-
ternate between properties and examples so we can get a physical feel for
this transform. In this module we begin with the property that in some
sense was our motivation for the development of the Fourier transform
and the one property of the Fourier transform we will use over and over
again—to the point we almost forget where it came from—is the convo-
lution property. We will then introduce other properties as we need them.
13.1
CONVOLUTION PROPERTY
The convolution property should maybe more properly be named the con-
volution avoidance property as it is the property of the Fourier transform that allows us to avoid
convolutions when we are relating the input and output of a linear time invariant system.
For a linear time invariant system with impulse response h.t/ and input x.t/, the output
is given by
y.t/ D
Z 1
 1
x./h.t   /d
Taking the Fourier transform of the output y.t/ we get
Y.!/
D
Z 1
 1
y.t/e j!tdt
D
Z 1
 1
Z 1
 1
x./h.t   /de j!tddt
D
Z 1
 1
x./
Z 1
 1
h.t   /de j!tdt

d
set u D t    then t D u C  and
Y.!/ D
Z 1
 1
x./
Z 1
 1
h.u/e j!.uC/du

d
or
Y.!/ D
Z 1
 1
x./e j!
Z 1
 1
h.u/e j!udu

d

172
13. PROPERTIES OF THE FOURIER TRANSFORM
or
Y.!/ D X.!/H.!/
This means convolution in the time domain becomes multiplication in the frequency domain.
This is a very useful property as it gives us a much simpler view of what is going on between
the input and output of a linear time invariant system. If we know the input and have a desired
output, this also allows us to design the linear time invariant system. As, given X.!/ and Y.!/,
H.!/ D Y.!/
X.!/
Filters
The most common linear time invariant system for which we use this relationship is
a filter. A filter is a device or process which allows us to selectively remove or enhance certain
frequency components. In the very first module we used a filter to remove unwanted interference.
Often it is not possible to entirely remove unwanted interference in which case we can use the
filter to attenuate the unwanted parts of the input.
We have been using brick wall filters with transfer function of the form
H.!/ D
(
1
! < W
0
otherwise
in some of our examples with the caveat that these filters are not particularly realistic. Let’s take
a look at the corresponding impulse response to see why this might be the case.
h.t/
D
1
2
Z 1
 1
H.!/ej!td!
D
1
2
Z W
 W
ej!td!
D
1
2
1
jt ej!t ˇˇˇW
 W
D
1
2jt
 ejW t   e jW t
D
sin.W t/
t
We have plotted this impulse response in Figure 13.1.
Notice a couple of things. First note the duality at work here. The Fourier transform of
a pulse in the time domain was a sin.x/=x function in the frequency domain. Here we have
essentially a pulse in the frequency domain and it’s inverse Fourier transform is a sin.x/=x func-
tion. Looking at the impulse response as the response to an impulse notice that it is nonzero for
t < 0. As we know this means that the filter is noncausal. Furthermore, as the impulse response

13.1. CONVOLUTION PROPERTY
173
t
Sin(ωt)/πt
–15
–10
–5
0
5
10
15
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
–0.1
–0.2
Figure 13.1: The impulse response of a brick wall filter.
t
–5
0
5
10
15
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
–0.1
–0.2
Figure 13.2: A causal impulse response.
is nonzero for t !  1, we cannot implement it by simply delaying the response by a little bit.
The best that we could do is to try and approximate this response by delaying it a bit and then
setting the response to zero for t < 0—something like the response shown in Figure 13.2. If we
take the Fourier transform of this we will of course not get our ideal brick wall filter characteristic
back.
So what does the transfer function of a realistic filter look like. If you recall we had found
the impulse response of a simple RC filter to be
h.t/ D 1
RCe t=RCu.t/

174
13. PROPERTIES OF THE FOURIER TRANSFORM
Let’s find the transfer function for this filter. For convenience we replace 1=RC with ˛.
H.!/
D
Z 1
 1
h.t/e j!tdt
D
Z 1
 1
˛e ˛tu.t/e j!tdt
D
Z 1
0
˛e .˛Cj!/tdt
D
 ˛
˛ C j! e .˛Cj!/t j1
0
D
 ˛
˛ C j! Œ0   1
D
˛
˛ C j!
We can write the transfer function in terms of its magnitude and phase
H.!/ D jH.!/j ej.!/
where the magnitude is given by
jH.!/j D
˛
p
˛2 C !2
and the phase is given by
.!/ D   tan 1 !
˛

The magnitude and the phase are plotted in Figure 12.2 for ˛ D 2. Clearly the magnitude is
nothing like what we plotted for the ideal filter. However, remember that this is a very simple
filter.
If we are willing to accept a little more complexity we can come closer to the ideal
characteristics—without ever really achieving it. Some of the known classes of filters include
the Butterworth, Chebyshev, and Elliptic filters. They are outside of the scope of this class but
they are really simple to understand and implement. The magnitude of the transfer function for
a Butterworth filter with the same shape as the filters we have been looking at is given by
jH.!/j D
1
p
1 C !2n
Where n denotes the order of the filter. We have plotted the magnitude of the transfer function
for a third and fifth order Butterworth filter in Figure 13.3.
An implementation of a third order Butterworth filter is shown in Figure 13.4.

13.1. CONVOLUTION PROPERTY
175
|H(ω)|
Third order Butterworth filter
0
2
4
6
8
ω
1.0
0.8
0.6
0.4
0.2
0.0
|H(ω)|
Fifth order Butterworth filter
0
2
4
6
8
ω
1.0
0.8
0.6
0.4
0.2
0.0
Figure 13.3: Magnitude of the transfer function of a third order Butterworth filter and a Fifth
order Butterworth filter.
C
C
C
R
R
R
x(t)
y(t)
R2
R1
+
–
+
–
+
–
Figure 13.4: An implementation of a third order Butterworth filter.
Example 13.1
Let’s take a look at our RC circuit again and examine the response of the circuit
to different inputs. Let’s suppose the impulse response of the circuit is
h.t/ D e tu.t/
and the input is another decaying exponential
x.t/ D e 2tu.t/

176
13. PROPERTIES OF THE FOURIER TRANSFORM
The transforms H.!/ and X.!/, based on our earlier calculations, are given by
H.!/
D
1
1 C j!
X.!/
D
1
2 C j!
We know that
y.t/ D x.t/ ~ h.t/
Therefore, by the convolution property
Y.!/
D
H.!/X.!/
D

1
1 C j!
 
1
2 C j!

D
1
.1 C j!/.2 C j!/
We have Y.!/. How do we find y.t/? There is a reason we have left the denominator in its
factored form. We know that the inverse Fourier transform of expressions of the form 1=.a C
j!/ is e atu.t/. By linearity this means that the inverse Fourier transform of ˛=.a C j!/ is
˛e atu.t/. If we can expand Y.!/ using partial fraction expansion into terms of the form ˛=.a C
j!/ we can use the linearity of the Fourier transform to easily find y.t/. Using partial fraction
expansion we can write Y.!/ as
Y.!/ D
1
.1 C j!/.2 C j!/ D
1
1 C j!  1
2 C j!
and, therefore,
y.t/ D e tu.t/   e 2tu.t/
How does this compare with the result using convolution. The plot of x./ and h.t   /or
t < 0 and t > 0 is shown in Figure 13.5.
We can see from the figure that for t < 0 y.t/ is equal to zero. For t > 0
y.t/
D
Z t
0
e .t /e 2d
D
e t
Z t
0
e d
D
e t 
1   e t
D
e t   e 2t

13.1. CONVOLUTION PROPERTY
177
τ
x(t)° h(t–τ) for t < 0
x(t)° h(t–τ) for t > 0
–2.0
–1.5
–1.0
–0.5
0
0.5
1.0
1.5
2.0
τ
–2.0
–1.5
–1.0
–0.5
0
0.5
1.0
1.5
2.0
1.0
0.8
0.6
0.4
0.2
0.0
–0.2
1.0
0.8
0.6
0.4
0.2
0.0
–0.2
Figure 13.5: A plot of x./ and h.t   /or t < 0 and t > 0.
or
y.t/ D
 e t   e 2t
u.t/
which is the same result as that obtained from using the Fourier transform.
Example 13.2
In our previous module we looked at how the RC circuit responds to a unit
step. Let’s try to solve the same problem using Fourier transforms. The transfer function is still
the same
H.!/ D
1
1 C j!
However, when we try to compute X.!/ we run into a problem. The input x.t/ does not satisfy
the Dirichlet condition
Z 1
 1
u.t/dt ! 1
This means we cannot use the Fourier integral to obtain the Fourier transform of the unit step
function. The unit step is a rather important function. The usefulness of the Fourier transform
would be diminished if we cannot find its transform. We need to reach into our bag of tricks
and pull out something that will allow us to find the Fourier transform of such problematic
functions. The trick we will use is the differentiation property of the Fourier transform.

178
13. PROPERTIES OF THE FOURIER TRANSFORM
13.2
THE DIFFERENTIATION PROPERTY AND THE
FOURIER TRANSFORM OF THE UNIT STEP
FUNCTION
If the Fourier transform of x.t/ is X.!/ the derivative property states that
F
 d
dt x.t/

D j!X.!/
Before we go about deriving this let’s emphasize something. If you see
Bill D 1
2
Z 1
 1
Bob ej!tdt
then regardless of what Bob or Bill may say Bob is the Fourier transform of Bill. Keeping this in
mind the derivative property is easy to derive
x.t/ D 1
2
Z 1
 1
X.!/ej!td!
Taking the derivative of both sides and changing order of integration and differentiation on the
right we get
d
dt x.t/ D
Z 1
 1
j!X.!/ej!td!
Now, remembering Bill and Bob
F
 d
dt x.t/

D j!X.!/
or
F
 d
dt x.t/

D j!F Œx.t/
or
F Œx.t/ D 1
j! F
 d
dt x.t/

(13.1)
So we can find the Fourier transform of a x.t/ by first finding the Fourier transform of the
derivative of x.t/ and dividing that by j!. We have to be a bit careful though. Using the deriva-
tive property to find a Fourier transform can be touchy unless you have a signal whose dc value
is 0 because the derivative will kill the dc value. The unit step definitely has a dc value (of 0.5).
However, the signum function sgn.t/, which is  1 for t < 0 and 1 for t > 0 does not. We can
write the unit step in terms of the signum function as
u.t/ D 1
2sgn.t/ C 1
2

13.2. THE DIFFERENTIATION PROPERTY AND THE FOURIER TRANSFORM
179
One more thing before we proceed. The Fourier transform of a constant (forward or in-
verse) is a delta function. We can see that easily by using the sifting property of the delta function.
So
F Œı.t/ D
Z 1
 1
ı.t/e j!tdt D e0 D 1
and
F 1 Œı.!/ D 1
2
Z 1
 1
ı.!/ej!td! D 1
2 e0 D 1
2
Given that we can write the unit step as a sum of 1
2sgn.t/ and 1
2 we can use the fact that
the Fourier transform is linear to write
F Œu.t/ D F
1
2sgn.t/

C F
1
2

The derivative of the signum function is zero everywhere except at zero where it is 2ı.t/ So we
can find F
 1
2sgn.t/
 using Equation (13.1) as
F
1
2sgn.t/

D
1
j! F
1
2
d
dt sgn.t/

(13.2)
D
1
j! F Œı.t/
(13.3)
D
1
j!
(13.4)
and the Fourier transform of 1
2 is
F
1
2

D 2  1
2  ı.!/ D ı.!/
which gives us the Fourier transform of u.t/.
F Œu.t/ D 1
j! C ı.!/
Example 13.3
Let’s return now to our example where we were investigating the output of
the RC filter when the input is a step function. Using the convolution property of the Fourier
transform we multiply the Fourier transform of the unit step with the transfer function of the
filter to obtain
Y.!/
D
 1
j! C ı.!/


1
1 C j!
D
1
j!.1 C j!/ C ı.!/
1
1 C j!
D
1
j!.1 C j!/ C ı.!/

180
13. PROPERTIES OF THE FOURIER TRANSFORM
where we have used the fact that in the second term ı.!/ is nonzero only when ! D 0. Expand-
ing the first term using partial fraction expansion we get
Y.!/ D 1
j!  1
1 C j! C ı.!/
Using the linearity and uniqueness of the Fourier transform we get
y.t/ D 1
2sgn.t/   e tu.t/ C 1
2 D u.t/   e tu.t/
Or, to put it in a more familiar form
y.t/ D

1   e t
u.t/
One other thing to note is that the differentiation property also clearly shows something
we already know—namely the derivate of a function is proportional to the rate of change of the
function. Let’s take a look at the Fourier transform of the derivative of a function in terms of its
physical meaning.
F
dx.t/
dt

D j!X.!/
The DC or constant value of the signal is X.0/ and as we can see for ! D 0 we get j 0X.0/ D 0.
In other words the derivative of the constant term in x.t/ is zero. For ! small or low frequencies,
or the slow variations in the signal j!X.!/ will be small, and for ! large the high frequency
components of X.!/ will be multiplied with this larger value of !.
13.3
SUMMARY
In this module we learned
1. Convolution in the time domain gets transformed to multiplication in the Fourier domain
F Œx.t/ ~ h.t/ D X.!/H.!/
2. There is a reason we cannot have an ideal brick wall filter.
3. The Fourier transform of the derivative of a function is given by
F
 d
dt x.t/

D j!X.!/
The derivative operation weights low frequency components of x.t/ less and the high
frequency components of x.t/ more.

13.3. SUMMARY
181
4. The Fourier transform of the unit step is given by
F Œu.t/ D 1
j! C ı.!/
We will continue in the next module with properties of the Fourier transform and connect
them to various applications.

182
13. PROPERTIES OF THE FOURIER TRANSFORM
13.4
EXERCISES
(Answers on the following page)
1. The signal y.t/ is the output from a linear time invariant system with impulse response
h.t/ when the input is x.t/. Given that X.!/ the Fourier transform of the input and y.t/
are given by
X.!/
D
1
y.t/
D
e 2tu.t/
find h.t/.
2. Given the input x.t/
x.t/ D e 3tu.t/
to a linear time invariant system with transfer function
H.!/ D
1
4 C j!
Find the output y.t/.
3. Find the Fourier transform of
(a) x.t/ D ı.t/
(b) x.t/ D ı.t   2/
(c) x.t/ D ı.t   2/ C ı.t C 2/
4. Find the inverse Fourier transform of
(a) X.!/ D ı.!/
(b) X.!/ D ı.!   2/
(c) X.!/ D ı.!   2/ C ı.! C 2/
5. The input output relationship of a linear time invariant system is given by
y.t/ D dx.t/
dt
What is the transfer function for this system?
6. A linear time invariant system is described by the differential equation
d 2y.t/
dt2
C 3dy.t/
dt
C 2y.t/ D x.t/
(a) Use the differentiation property of the Fourier transform to find the transfer function
of this system.
(b) Find the impulse response of this system.

13.5. ANSWERS
183
13.5
ANSWERS
1.
h.t/ D e 2tu.t/
2.
y.t/ D e 3tu.t/   e 4tu.t/
3.
(a) X.!/ D 1
(b) X.!/ D e j!
(c) X.!/ D 2 cos.2!/
4.
(a)
x.t/ D 1
2
(b)
x.t/ D 1
2 ej2t
(c)
x.t/ D 1
 cos.2t/
5.
H.!/ D j!
6.
(a)
H.!/ D
1
2 C 3j!   !2
(b)
h.t/ D

e t   e 2t
u.t/


185
M O D U L E
14
Some More Useful Properties
of the Fourier Transform
Let’s look at a few more properties of the Fourier transform which will
come in handy when dealing with applications we are interested in. We
examined the differentiation property in the last module so let’s begin with
the integration property.
14.1
INTEGRATION PROPERTY
If we know that the Fourier transform of x.t/ is X.!/ then the integration
property says that
F
Z t
 1
x./d

D 1
j! X.!/ C X.0/ı.!/
There are a number of different ways of showing the integration property. We will use what
we learned in the previous module. Notice that the limits of integration are  1 to t instead of
from  1 to 1. We can get this form if we convolve x.t/ with u.t/
x.t/ ~ u.t/ D
Z 1
 1
x./u.t   /d D
Z t
 1
x./d
By the convolution property we know that
F Œx.t/ ~ u.t/ D X.!/F Œu.t/
And in the previous module we found that
F Œu.t/ D 1
j! C ı.!/
Therefore,
F Œx.t/ ~ u.t/ D F
Z t
 1
x./d

D X.!/ 
 1
j! C ı.!/

or
F
Z t
 1
x./d

D 1
j! X.!/ C ı.!/X.!/

186
14. SOME MORE USEFUL PROPERTIES OF THE FOURIER TRANSFORM
ı.!/ is nonzero only for ! D 0. Therefore,
F
Z t
 1
x./d

D 1
j! X.!/ C ı.!/X.0/
14.2
TIME AND FREQUENCY SCALING
By time scaling we mean the broadening or narrowing of the function by multiplying the argu-
ment with a scale factor. We can see how this works by considering a pulse in time
x.t/ D
 1
 1  t  1
0
otherwise
Let’s scale the argument by a scale factor ˛ to generate x.˛t/. Examining x.˛t/,
x.˛t/ D
 1
 1  ˛t  1
0
otherwise
For the moment let’s assume that ˛ is positive. Then we get
x.˛t/ D
8
<
:
1
  1
˛  t  1
˛
0
otherwise
The pulse width went from 2 to 2=˛. If ˛ is greater than 1 this will mean a narrowing of the
pulse. If ˛ is less than 1 this will mean a broadening of the pulse. Now let’s see how scaling in
the time domain effects the spectral profile of the signal. Taking the Fourier transform
F Œx.˛t/ D
Z 1
 1
x.˛t/e j!tdt
substitute  D ˛t ) t D =˛. If ˛ is positive the limits stay the same. However if ˛ is negative
the limits flip—when t goes to infinity ˛t goes to negative infinity and when t goes to negative
infinity ˛t goes to positive infinity. Let’s work through both cases. For ˛ > 0 we get
F Œx.˛t/
D
Z 1
 1
x./e j!=˛ 1
˛ d
D
1
˛
Z 1
 1
x./e j. !
˛ /d
D
1
˛ X
!
˛


14.2. TIME AND FREQUENCY SCALING
187
For ˛ < 0 we get
F Œx.˛t/
D
Z  1
1
x./e j!=˛ 1
˛ d
D
  1
˛
Z 1
 1
x./e j. !
˛ /d
D
  1
˛ X
!
˛

Combining both we get
F Œx.˛t/ D 1
j˛jX
!
˛

where we have used the fact that
jxj D
 x
for x > 0
 x
for x < 0
Notice now that instead of getting ˛t as the argument we have !=˛. This means the effect
of the magnitude of ˛ is reversed. When ˛ is greater than one jX.!/j will be broader and when
˛ is less than 1 it will be narrower. This agrees with our earlier discussions. Narrowing of a signal
in the time domain results in a broadening of its spectral profile and vice versa.
When x.t/ is even X.!/ is even
Let’s take a look at the special case where ˛ D  1. Substi-
tuting this value for ˛ we get
F Œx. t/ D
1
j   1jX
 !
 1

D X. !/
If x.t/ is even, x.t/ D x. t/ and given the fact that the Fourier transform of x. t/ is X. !/
and the uniqueness of the Fourier transform, this means that X.!/ D X. !/.
Example 14.1
For our first example let’s pick a square pulse—what we would get if we started
with the square wave and let the period go to infinity.
x.t/ D
 1
jtj < To
0
otherwise

188
14. SOME MORE USEFUL PROPERTIES OF THE FOURIER TRANSFORM
The Fourier transform is given by
X.!/
D
Z 1
 1
x.t/e j!tdt
D
Z To
 To
e j!tdt
D
  1
j! e j!t ˇˇˇTo
 To
D
  1
j!

e j!To   e j!. To/
D
1
j!
 ej!To   e j!To
(14.1)
D
sin.!To/
!=2
This form is very similar to the form of the Fourier series coefficients of the square wave. Recall
that for for a square wave the Fourier series coefficients were given by
an D sin.n!oTo/
n!oT=2
X.!/ is plotted in Figure 14.1 for To D 1.
What happens if we pick To to be equal to a half? Making To smaller narrows our pulse.
In the frequency domain, as seen in Figure 14.2 the effect is to broaden the frequency profile. In
practice this duality has a major impact. If we are using pulses to transmit binary data—positive
pulse for a one and negative pulse for a zero, or a pulse for a one and no pulse for a zero, the
width of the pulse depends on the rate at which we want to send the bits. If we want to send
1000 bits per second the width of the pulse has to be less than a millisecond. If we want to send
10,000 bits per second the width of the pulse has to be less than 0.1 milliseconds. The higher the
rate, the narrower the pulse, and, therefore, based on what we see here the broader will be the
frequency profile. A measure of the width of the frequency profile is referred to as the bandwidth
of the signal. Thus, a higher rate will require a higher bandwidth.
14.3
FOURIER TRANSFORM OF PERIODIC SIGNALS
We know that we can find the frequency components of a periodic signal using the Fourier
series representation. We motivated the Fourier transform by our need to find the frequency
representation of aperiodic signals. It would be nice if we could also include periodic signals
within the Fourier transform framework. That is what we are going to do in this section.

14.3. FOURIER TRANSFORM OF PERIODIC SIGNALS
189
–2
–10
–5
0
5
10
–1
0
1
2
1.2
1.0
0.8
0.6
0.4
0.2
0.0
–0.2
2.0
1.5
1.0
0.5
0.0
–0.5
x(t)
X(ω)
t
ω
Figure 14.1: The Fourier transform of a rectangular pulse with pulse width 2.
–2
–10
–15
–5
0
5
10
15
–1
0
1
2
1.2
1.0
0.8
0.6
0.4
0.2
0.0
–0.2
2.0
1.5
1.0
0.5
0.0
–0.5
x(t)
X(ω)
t
ω
Figure 14.2: The Fourier transform of a rectangular pulse with pulse width 1.
We begin by using the linearity of the Fourier transform. Suppose x.t/ is a periodic signal.
Then we can write it as
x.t/ D
1
X
nD 1
anejn!ot

190
14. SOME MORE USEFUL PROPERTIES OF THE FOURIER TRANSFORM
Using the Linearity of the Fourier transform
F Œx.t/ D
1
X
nD 1
anF

ejn!ot
In order to evaluate this transform we need to find the Fourier transform of ejn!ot. We will
make use of the uniqueness of the Fourier transform to find this.
We begin by evaluating the inverse Fourier transform of ı.!   !o/.
F 1 Œı.!   !0/
D
1
2
Z 1
 1
ı.!   !0/ej!td!
D
1
2 ej!ot
where we have used the sifting property of the delta function. Multiplying both sides by 1=2
we get
F 1 Œ2ı.!   !0/ D ej!ot
Because the Fourier transform is unique, this means that if the inverse transform of 2ı.!   !0/
is ej!ot, then the Fourier transform of ej!ot is 2ı.!   !0/.
Returning to our original quest for the Fourier transform of a periodic function x.t/ we
get
F Œx.t/ D
1
X
nD 1
anF

ejn!ot
D 2
1
X
nD 1
anı.!   n!0/
Example 14.2
Let’s apply this to a particular periodic function—the impulse train.
x.t/ D
1
X
kD 1
ı.t   kT /
The impulse train is clearly periodic and can therefore be written as a Fourier series. To find the
series coefficient take the integral over one period.
an
D
1
T
Z
T
2
  T
2
1
X
nD 1
ı.t   kT /e jk!0tdt
D
1
T
Z
T
2
  T
2
ı.t/e jk!0tdt
D
1
T

14.4. A SHIFT IN TIME IS A PHASE CHANGE IN FREQUENCY
191
Thus,
x.t/ D
1
X
nD 1
1
T ejkn!0t
Therefore,
X.!/ D 2
T
1
X
nD 1
ı.!   k!0/
Therefore, Fourier transform of an impulse train is an impulse train. Notice that the separation
between impulses in the time and frequency domains are inversely related. In the time domain
they are separated by T while in the frequency domain they are separated by !0 D 2=T . There-
fore, if the impulses in the time domain are brought closer the impulses in the frequency domain
will move apart.
14.4
A SHIFT IN TIME IS A PHASE CHANGE IN
FREQUENCY
As in the case of the Fourier series a time shift in the time domain results in a phase shift in the
frequency domain. This is easy to show. Let x.t/ and X.!/ be a Fourier transform pair
F Œx.t/ D X.!/ D
Z 1
 1
x.t/e j!tdt
The Fourier transform of x.t   to/ is
F Œx.t   to/ D
Z 1
 1
x.t   to/e j!tdt
Substituting  D t   to
F Œx.t   to/
D
Z 1
 1
x./e j!.Cto/d
D
e j!to
Z 1
 1
x./e j!d
D
e j!toX.!/
If we write X.!/ in polar form
X.!/ D jX.!/j ej.!/
we can see that
F Œx.t   to/ D jX.!/j ej.!/e j!to D jX.!/j ej ..!/ !to/

192
14. SOME MORE USEFUL PROPERTIES OF THE FOURIER TRANSFORM
Example 14.3
Let’s find the Fourier transform of
x.t/ D cos.2t   2/
First, notice that we can write x.t/ as cos.2.t   1// so we can find the Fourier transform of
cos.2t/ and then apply the shifting property to find the Fourier transform of cos.2t   2/. To
find the Fourier transform of cos.2t/ let’s write the expression using Euler’s formula
cos.2t/ D 1
2ej 2t C 1
2e j 2t
Using the linearity of the Fourier transform we can take the Fourier transform of each individual
term and then combine them together to get the Fourier transform of cos.2t/ .
F
1
2ej2t

D
1
2F

ej 2t
D
1
22ı.!   2/
D
ı.!   2/
F
1
2e j2t

D
1
2F

e j2t
D
1
22ı.! C 2/
D
ı.! C 2/
Therefore,
F Œcos.2t/ D ı.!   2/ C ı.! C 2/
Now using the shifting property
F Œcos.2.t   1// D ı.!   2/e j! C ı.! C 2/e j!
We could go one step further and noting that ı.!   2/ is nonzero only at ! D 2 and ı.! C 2/
is nonzero only at ! D  2,
F Œcos.2.t   1// D ı.!   2/e 2j C ı.! C 2/e2j
14.5
SUMMARY
In this module we looked at the following properties of the Fourier transform:

14.5. SUMMARY
193
1. Integration property: If we integrate a time domain function we tend to smooth out the
high frequency variations. We can see that in the first term below
F
Z t
 1
x./d

D 1
j! X.!/ C X.0/ı.!/
The second term reflects the situation where the time signal has a non-zero average value
or DC bias.
2. Time and Frequency scaling: Narrowing a function in time broadens its frequency profile
while broadening a function in time narrows its frequency profile.
3. Time shift: If we introduce a time shift in a signal this appears as a phase shift in the
frequency domain.
We also extended the Fourier transform to periodic signals.

194
14. SOME MORE USEFUL PROPERTIES OF THE FOURIER TRANSFORM
14.6
EXERCISES
(Answers on the following page)
1. Find the Fourier transform of
(a) x.t/ D cos.3t/.
(b) x.t/ D sin.3t/
(c) x.t/ D cos.3t C 1/
(d) x.t/ D cos.2t C 1/
2. The Fourier transform of
a.t/ D u

t C 1
2

  u

t   1
2

has been shown in the writeup to be
A.!/ D 2 sin.!=2/=!
Without using the Fourier integral find the Fourier transform of the following
(a)
x.t/ D u.t/   u.t   1/
(b)
y.t/ D u.t C 1/   u.t   1/
(c)
z.t/ D u.t/   u.t   2/
3. Find and sketch the Fourier transform of
x.t/ D
1
X
kD 1
p.t   kT/
where
(a)
p.t/ D
 1
jtj < T=4
0
otherwise
(b)
p.t/ D
 1
jtj < T=8
0
otherwise

14.6. EXERCISES
195
4. Find the Fourier transform of
(a)
x.t/ D e 2jtj
(b)
x.t/ D e 2jtj cos.5t/

196
14. SOME MORE USEFUL PROPERTIES OF THE FOURIER TRANSFORM
0
ω0
ω
–ω0
–2ω0
–4ω0
–5ω0
0.4
0.4
–2/3
–2/3
2
2
π
–3ω0
3ω0
2ω0
4ω0
5ω0
Figure 14.3: The Fourier transform of a train of rectangular pulses with pulse width T=4.
14.7
ANSWERS
1.
(a)
F Œcos.3t/ D ı.!   3/ C ı.! C 3/
(b)
F Œsin.3t/ D  jı.!   3/ C jı.! C 3/
(c)
X.!/ D ej ı.!   3/ C e j ı.! C 3/
(d)
X.!/ D ej ı.!   2/ C e j ı.! C 2/
2.
(a)
X.!/ D 2e j!=2 sin.!=2/
!
(b)
Y.!/ D 2sin.!/
!
(c)
Z.!/ D 2e j! sin.!/
!
3.
(a) (Figure 14.3)
X.!/ D
1
X
nD 1
2 sin.n=2/
n
ı.!   n!o/
(b) (Figure 14.4)
X.!/ D
1
X
nD 1
2 sin.n=4/
n
ı.!   n!o/

14.7. ANSWERS
197
0
ω0
–ω0
–2ω0
–4ω0
–5ω0
–0.282
–0.282
0.471
0.471
π/2
1.414
1.414
1.0
1.0
–3ω0
3ω0
2ω0
4ω0
5ω0
ω
Figure 14.4: The Fourier transform of a train of rectangular pulses with pulse width T=8.
4.
(a)
X.!/ D
4
4 C !2
(b)
X.!/ D
4
4 C .!   5/2 C
4
4 C .! C 5/2


199
M O D U L E
15
Sampling
Much of the processing nowadays is conducted in the digital domain.
However, the signals being processed, voice, music, video, etc., are all ana-
log. To go from analog signals to the digital signals we first have to move
from the continuous time domain to the discrete time domain. The oper-
ation which allows us to do this is sampling. Processing the signal in the
digital domain is not the end goal of many applications especially if the
processed signal is for human consumption. We often need to convert the
discrete time signal back into a continuous time signal which can be per-
ceived by humans. In this module we introduce the process of sampling
and the reconstruction of signals from the sampled values. We have almost all the tools we need
to understand this process. The only tool we are missing is the multiplication property of the
Fourier transform which is what we look at next.
15.1
THE MULTIPLICATION PROPERTY OF THE
FOURIER TRANSFORM
Consider for a moment the forward and inverse Fourier transforms
X.!/
D
Z 1
 1
x.t/e j!tdt
x.t/
D
1
2
Z 1
 1
X.!/ej!td!
Notice the similarity of the forward and inverse Fourier transforms. As one would expect this
similarity is reflected in the properties of the Fourier transform. In particular the convolution
property of the Fourier transform is that the Fourier transform of the convolution of signals in
the time domain is the product of the Fourier transform of the signals. You would then expect
that the Fourier transform of the product of signals in the time domain would be the convolution
of the Fourier transform of the signals. You would be right in this expectation. Let’s validate the
expectation Suppose
g.t/ D a.t/b.t/
Then
G.!/ D
Z 1
 1
g.t/ej!tdt D
Z 1
 1
a.t/b.t/ej!tdt

200
15. SAMPLING
Writing b.t/ as the inverse Fourier transform of B.!/
G.!/
D
Z 1
 1
a.t/
 1
2
Z 1
 1
B./ejtd

e j!tdt
D
1
2
Z 1
 1
B./
Z 1
 1
a.t/e j!tejtd!

d
D
1
2
Z 1
 1
B./
Z 1
 1
a.t/e j.! /td!

d
D
1
2
Z 1
 1
B./A.!   /d
or
G.!/ D 1
2 A.!/ ~ B.!/
Let’s now use this property to explore sampling.
15.2
IDEAL SAMPLING
When we sample a signal in time can we recover the original signal exactly from the sampled
signal? At first sight the answers seems to be a clear no. By the process of sampling we have
thrown away all the information about the signal between the samples. However, surprisingly,
the answer to the question is a qualified yes. Even more surprisingly the answer is still yes when
all our intuition yells no. Consider the portion of the sinusoidal signal shown in top panel of
Figure 15.1. In the bottom panel of the figure we show just the samples themselves. It is intu-
itively obvious that given the samples we cannot accurately recover the original signal. After all
there are an infinite number of signals that could have resulted in those samples. Intuition in
this case is completely and utterly wrong.
We owe this humbling of our intuitive self ostensibly to Harry Nyquist and Claude Shan-
non. Nyquist published a version of the result we will now look at in the Bell Systems Technical
Journal in 1928 where it languished until Claude Shannon working in the same lab resurrected
and expanded the result into (almost) the current form in 1949. (Ostensibly because as with
almost all ideas attributed to particular people this idea was also discovered by others—in this
case going back to at least 1897.)
Suppose we have a signal x.t/ with a Fourier transform which is zero for ! greater than
some W . In other words x.t/ is a bandlimited signal. Just as an example we could visualize X.!/
as shown in Figure 15.2. We can model a sampled signal xS.t/ as a product of the original signal
x.t/ with an impulse train.
xS.t/ D x.t/ 
1
X
kD 1
ı.t   kTS/

15.2. IDEAL SAMPLING
201
x(t)
x(t)
0.0
0.5
1.0
1.5
2.0
1.0
0.5
0.0
–0.5
–1.0
1.0
0.5
0.0
–0.5
–1.0
t
0.0
0.5
1.0
Sinusoidal signal
Samples of a sinusoidal signal
1.5
2.0
t
Figure 15.1: Samples of the sinusoidal signal.
W
−W
X(ω)
ω
Figure 15.2: An example signal.
where TS is the sampling interval. Using the multiplication property we can write the Fourier
transform of the sampled signal as the convolution of the original signal and the Fourier trans-
form of the impulse train.
XS.!/ D 1
2 X.!/ ~ F
"
1
X
kD 1
ı.t   kTS/
#

202
15. SAMPLING
ωS
ωS + W
ωS − W
–ωS
–2ωS
2ωS
W
−W
XS(ω)
ω
Figure 15.3: Fourier transform of the sampled version of the signal shown in Figure 15.2.
But as we have previously noted the Fourier transform of an impulse train is an impulse train.
F
"
1
X
kD 1
ı.t   kTS/
#
D 2
TS
1
X
nD 1
ı.!   k!S/
so
XS.!/ D 1
2 X.!/ ~ 2
TS
1
X
nD 1
ı.!   k!S/
We can switch the order of convolution and multiplication to obtain
XS.!/
D
1
TS
1
X
nD 1
X.!/ ~ ı.!   k!S/
D
1
TS
1
X
nD 1
X.!   k!S/
The Fourier transform of the sampled signal is shown in Figure 15.3. It is transform of the
original signal replicated at intervals of length !S.
Clearly we can recover the original signal perfectly from the sampled signal. All we need
to do is use a low pass filter with a bandwidth of W and we have our original signal back. There
is however one little detail we glossed over. Take another look at Figure 15.3. The only reason
we can recover our original signal is that the various replicas (known as images) are distinctly
separate from each other. If the sampling frequency !S had been lower the various replicas
would add to each other and it would be impossible to recover the original signal. The picture
in Figure 15.3 actually tells us exactly what the lower limit for the sampling frequency has to be
in order to be able to recover the original signal. Because the original signal is bandlimited to
W radians/sec, the lower limit of the first replica is at !S   W . In order for the two signals not
to overlap we need
!S   W > W ) !S > 2W
W is the highest frequency in the original signal. So, the requirement for perfect reconstruc-
tion from the samples, at least theoretically, is to sample at a rate which is greater than twice
the highest frequency. This frequency is called the Nyquist frequency after its discoverer Harry

15.2. IDEAL SAMPLING
203
ωS
–ωS
–2ωS
2ωS
W
W
XS(ω)
ω
Figure 15.4: A 440 Hz signal sampled at 1100 samples per second.
ωS
–ωS
–2ωS
2ωS
W
–W
XS(ω)
ω
Figure 15.5: A 440 Hz signal sampled at 660 samples per second.
Nyquist. In practice we would want to sample at a higher rate as the larger the value of !S,
the further apart the replicas will be. The further apart the replicas are the more relaxed the
requirements on the reconstruction filter.
We can see what happens when the sampling frequency is less than the Nyquist frequency
if we sample a pure tone. Let’s sample a sinusoid at a frequency of 440 Hz. If we sample it at 1100
samples per second we will get a sampled signal with a frequency profile which looks something
like that shown in Figure 15.4. In the figure the replicas are shown with dotted lines. We can
use a low pass filter with a 450 Hz cutoff and recover the original tone.
Now let’s take the same signal and sample it at 660 samples per second. Now the spectral
profile of the sampled signal looks like that shown in Figure 15.5. If we tried to use the same low
pass filter to recover the original signal we would instead get two tones back—one at 440 Hz
and one at 220 Hz.
This distortion due to undersampling is called aliasing and in most real world situations
getting rid of it is impossible. There are times when we are stuck with a particular sampling
frequency which is less than twice the highest frequency of the signal being sampled because
of processor or other constraints. This is the situation when sampling voice signals for trans-
mission over telephones. Because of various historical reasons the voice signal transmitted over
cellphones is usually sampled at 8000 samples per second. This would be fine if all frequency
components in the voice signal were at frequencies below 4 kHz. However, the voice contains
components at least up to 8 kHz. Sampling this signal at 8000 samples per second would result
in a huge amount of aliasing. Fortunately for us most of the information in the voice signal is
contained in frequencies below 4 kHz. Therefore, in order to avoid the aliasing distortion we
first filter the voice signal using an antialiasing filter with a cutoff at 3.6 kHz. Note that this
still introduces distortion—the components above 4 kHz have much to do with the audio qual-

204
15. SAMPLING
0
ωS
–ωS
–2ωS
–4ωS
–5ωS
0.059
0.079
0.098
0.112
0.121
0.059
0.079
0.098
0.112
0.121
0.125
–3ωS
3ωS
2ωS
4ωS
5ωS ω
Figure 15.6: The Fourier transform of a pulse train with To D T=16.
ity of the voice signal. However, this distortion is significantly less than the aliasing noise. In
addition it is controlled distortion. Basically the use of the antialiasing filter is there to cut our
losses. Some voice-over-internet protocols (VoIP) uses a higher sampling rate and can therefore
provide a higher quality voice.
Where quality is important—for example for music signals—we use a much higher sam-
pling rate—usually 44,100 samples per second.
Why not just use a higher sampling rate all the time? First, increased sampling rate requires
increased processor power which may not necessarily be available or convenient. Second, each
sample has to be represented by number of bits—usually eight bits per sample for voice and
sixteen bits for other audio signals. Increasing the sampling rate results in increasing the bit rate
which in turn means a higher bandwidth for transmission. In the end it is a matter of tradeoffs.
15.3
NONIDEAL SAMPLING
In the discussion above we modeled the sampling process using an impulse train. An impulse
train is a mathematical abstraction and not particularly realistic. A more realistic model would
use a pulse train in place of the impulse train. Does doing that mess up our development? Not
really. Suppose we used a version of the square wave we found the Fourier transform for in the
previous module. Let’s make the pulse a bit narrower and pick To D T=16. This would give us
values for an as
an D sin.n=8/
n
The Fourier transform of this square wave is shown in Figure 15.6. Convolving this with our
original signal we again get a set of replicas of the original signal. The only difference is that the
replicas each have a different gain. As we are only interested in the replica at the origin this is
not really much of an issue. In fact we could use another shape for the pulse and we would get
pretty much the same result.
Finally, let’s go back to our first example. Why is it that against all intuition we can get
our original signal back despite the fact that there are an infinite number of possible signals that
would generate the samples shown in Figure 15.1. Let’s take a look at the Fourier transform of
the sampled signal. The signal itself is a 1 Hz signal being sampled three times each cycle—or

15.4. SUMMARY
205
–8K
f (Hz)
–7
–6
–5
–4
–3
–2
–1
1
2
3
4
5
6
7
8
9
Figure 15.7: A 1 Hz signal sampled at 3 samples per second.
three samples per second. The Fourier transform of the sampled signal is shown in Figure 15.7
where we have put dashed boxes around the replicas. There are infinitely many sinusoids here
and by picking and choosing we could generate an infinite number of different signals. However,
once we apply a low pass filter to extract only the replica around the origin we get the original
signal back.
15.4
SUMMARY
In this module we looked at what happens to the frequency domain representation of a signal
when we sample it in the time domain as an application of the multiplication property of Fourier
transforms. We introduced the counterintuitive result that as long as we sample a signal at a rate
greater than twice the highest frequency in the signal we can recover the original signal without
distortion. This result forms the basis of much of our digital world today.

206
15. SAMPLING
–8K
f (Hz)
–7K –6K –5K –4K –3K –2K –1K
1K 2K 3K 4K 5K 6K 7K 8K 9K
Figure 15.8: Fourier transform.
15.5
EXERCISES
(Answers on the following page)
1. The signal x.t/ is a 2 KHz tone
x.t/ D cos.4000t/
(a) In theory what is the minimum number of samples per seconds (the Nyquist sampling
rate) we need in order to be able to reconstruct the original signal from its samples.
(b) This 2 KHz signal is undersampled with the resulting Fourier transform shown in
Figure 15.8.
What was the sampling rate used in this case?
2. Suppose we have a signal x.t/ which can be exactly recovered from its samples. If the
sampling rate is 10,000 samples/second (or !S D 2  10;000 radians/sec). What is the
highest nonzero frequency component of this signal?
3. Suppose we generate a new signal
y.t/ D dx.t/
dt
where x.t/ is the signal described in the previous question. What is the highest nonzero
frequency component of this signal?
4. Suppose we have a signal x.t/ with a Nyquist sampling rate of 10,000 samples/second (or
!S D 2  10;000 radians/sec). Suppose we generate a new signal z.t/
z.t/ D x.t   1/
What is the Nyquist sampling rate for z.t/?
5. Suppose we have a signal x.t/ with a Nyquist sampling rate of 10,000 samples/second (or
!S D 2  10;000 radians/sec). We generate a new signal w.t/
w.t/ D x.2t/
What is the Nyquist sampling rate for w.t/?

15.5. EXERCISES
207
6. A signal x.t/ has a Fourier transform which is nonzero for j!j < 25000. We sample this
signal at a rate of 15,000 samples per second. The original signal is recovered from the
sampled signal using an ideal low pass filter with cutoff frequency !c D 2fc. What is the
acceptable range of values for fc.

208
15. SAMPLING
15.6
ANSWERS
1.
(a) 4 kHz
(b) 3 kHz
2. 5000 Hz (or 2  5000 radians/sec)
3. 5000 Hz (or 2  5000 radians/sec)
4. 10,000 Hz (or 2  10;000 radians/sec)
5. 20,000 Hz (or 2  20;000 radians/sec)
6. fc can range from 5–10 kHz

209
M O D U L E
16
Amplitude Modulation
Let’s look at one more application of the multiplication property. This has
to do with broadcasting signals over a shared communication channel. This
shared channel can be the atmosphere as is the case with broadcast ra-
dio and television signals, or it can be a a coaxial or optical cable such as
that used to distribute television programs. Basically, any communication
medium that is being used by a number of users at the same time. If the
signals generated by the users occupy the same frequency band sending
them all over the same channel at the same time will cause them all to be
distorted. We have to separate them somehow. We can do this by separat-
ing them in time, or we could separate them in frequency, or as the Austrian-American actress
Hedy Lamar demonstrated you could do both (remember her the next time you use Bluetooth).
In this module we take a look at one way of moving the signals into separate slots in the fre-
quency domain. At first sight separating similar signals in frequency does not seem like a viable
option because generally the signals we send over a common broadcast medium are similar sig-
nals and therefore have similar spectral profiles. However, remember from our discussion of
sampling that we can move a signal around in the frequency domain by convolving it with a
delta function.
X.!/ ~ ı.!   !o/ D X.!   !o/
If x.t/ is real it has an even magnitude and odd phase. To preserve this symmetry we need to
shift the signal to both ˙!o. So we convolve it with two delta functions
X.!/ ~ .ı.!   !o/ C ı.! C !o// D X.!   !o/ C X.! C !o/
For different signals we can pick different values of !o and as long as these are far enough
apart we can accommodate a host of signals over a common channel. But how do we go about
convolving X.!/ with a delta function? To see this let’s take the inverse Fourier transform of
the shifted signal. In the frequency domain we convolved X.!/ with the pair of delta function.
In the time domain the convolution becomes a multiplication where we use the multiplication
property.
F Œa.t/b.t/ D 1
2 A.!/ ~ B.!/

210
16. AMPLITUDE MODULATION
1.0
0.5
0.0
–0.5
–1.0
1.0
0.5
0.0
–0.5
–1.0
1.0
0.5
0.0
–0.5
–1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Message signal
Carrier signal
Modulated signal
Figure 16.1: An example of a message, carrier, and modulated signal.
We can show that the inverse Fourier transform of .ı.!   !o/ C ı.! C !o// is .1=2/Œej!ot C
e j!ot which is simply .1=/ cos.!ot/. Putting all of this together we get
F 1 ŒX.!   !o/ C X.! C !o/ D 2x.t/ cos.!ot/
This process of multiplying a message bearing signal with a sinusoid to move it around in fre-
quency is called amplitude modulation (or more technically Double Sideband—Suppressed Car-
rier Modulation). The sinusoid is essentially carrying the message signal, therefore, it is called
the carrier signal. The reason for the names becomes clearer if we look at an example. In Fig-
ure 16.1 we show a (sinusoidal) message signal, a carrier signal and the modulated signal. You
can see that the amplitude of the carrier signal is being modulated by the message signal, hence,
amplitude modulation.

16. AMPLITUDE MODULATION
211
So, now that we have moved the signal to a higher frequency how do we recover it at the
receiver? One way is to do exactly what we did to move it up—multiply it again by the carrier
signal cos.!ot/. We can see what happens in the time domain using a trigonometric identity.
x.t/ cos.!ot/  cos.!ot/
D
x.t/ cos2.!ot/
D
x.t/
1
2 C 1
2 cos.2!ot/

D
1
2x.t/ C 1
2x.t/ cos.2!ot/
The second term is the signal moved out to 2!o and can be easily removed by a low pass filter.
This method of recovery or demodulation is called coherent demodulation. In order to use this at
the receiver we need to generate a signal with the exact same frequency and phase as the carrier
signal. What happens if we cannot get the phase right? Suppose at the transmitter we used the
carrier signal cos.!ot/, but at the receiver we only had cos.!ot C / available. If we multiplied
the modulated signal with this phase shifted signal we would get
x.t/ cos.!ot/  cos.!ot C /
D
x.t/
1
2 cos.2!ot C / C 1
2 cos./

D
1
2x.t/ cos./ C 1
2x.t/ cos.2!ot C /
Here we have used the trigonometric identity (which you could prove using Euler’s excellent
formula)
cos.˛/ cos.ˇ/ D 1
2cos.˛ C ˇ/ C 1
2 cos.˛   ˇ/
Once again we can filter out the signal at 2!ot but we are left with not x.t/ but x.t/ cos./. If
 is close to =2 this would mean that the signal would disappear as cos.=2/ D 0. In a sense
a worse situation is when  is not fixed but varies continuously. The received signal then would
fade in and out—not a very desirable situation.
Generating a signal with the exact same frequency is not necessarily a difficult task, how-
ever, the phase requirement can be onerous and expensive.
To overcome this requirement we can do something really simply. In Figure 16.2 we have
plotted the modulated signal with the original signal overlayed and the modulated signal with
the positive envelope emphasized. The positive envelope looks like the message signal when the
message signal is positive and is flipped when the message signal is negative
The positive envelope is something we can get using a simple rectifying filter shown in
Figure 16.3. If we could modify our scheme so that the positive envelope of the modulated
signal is the message signal we would not need to get the frequency or phase of the carrier.
Looking at Figure 16.2 we can see a way to do this. All we need to do is to ensure the original
signal is always positive. We can do that with a DC shift.

212
16. AMPLITUDE MODULATION
1.5
1.0
0.5
0.0
–0.5
–1.0
–1.5
0.0
0.2
0.4
0.6
0.8
1.0
1.5
1.0
0.5
0.0
–0.5
–1.0
–1.5
0.0
0.2
0.4
0.6
0.8
1.0
Modulated signal with the positive envelope emphasized
Modulated signal with message signal overlayed
Figure 16.2: The modulated signal and the message signal.
–
+
Figure 16.3: A simple envelope detector.
We have plotted this situation in Figure 16.4. You can see that the positive envelope of the
modulated signal is the level shifted message signal. To emphasize this we plot the modulated
and message signal overlayed in Figure 16.5. After putting the modulated signal through an
envelope detector we can remove the level shift by using a blocking capacitor.

16. AMPLITUDE MODULATION
213
1
0
–1
2
1
0
–1
–2
3
2
1
0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Message signal
Carrier signal
Modulated signal
Figure 16.4: An example of a level shifted message, carrier, and modulated signal.
This latter method of level shifting the signal and then multiplying (or mixing) with a si-
nusoidal carrier is used in commercial AM systems. Here are some acronyms for those interested
in such things. The method used by the commercial broadcaster where we add a DC value prior
to modulation is called Double Sideband-Large Carrier (DSB-LC). The LC part is because the
addition of the DC value prior to modulation can also be seen as the addition of a carrier term
after modulation.
.x.t/ C A/ cos.!ct/ D x.t/ cos.!ct/ C A cos.!ct/
The amount of additional carrier is measured by a quantity called the modulation index  which
is defined as
 D xm
A

214
16. AMPLITUDE MODULATION
3
2
1
0
–1
–2
–3
0.0
0.2
0.4
0.6
0.8
0.1
0.3
0.5
0.7
0.9
0.1
0.3
0.5
0.7
0.9
1.0
3
2
1
0
–1
–2
–3
0.0
0.2
0.4
0.6
0.8
1.0
Modulated signal with the positive envelope emphasized
Modulated signal with message signal overlayed
Figure 16.5: The modulated signal and the message signal overlayed.
where xm is the maximum value of the absolute value of the message signal jx.t/j. To under-
stand the Double Sideband part consider the Fourier transform of a real signal. We know that
the magnitude of the Fourier transform has even symmetry and the phase has odd symmetry.
This symmetry means that we actually only need to know half of the transform. Either the trans-
form for positive !, or the transform for negative !. We label the transform for positive ! as
the upper sideband and the transform for negative ! as the lower sideband. During the process
of modulation both sidebands get shifted to the carrier frequency and transmitted —hence the
“double sideband” in the name. For the same reason the amplitude modulation scheme without
the level shift is called Double Sideband-Suppressed Carrier (DSB-SC). There are techniques
that transmit only one sideband and are known as (you guessed it) Single Sideband (SSB) sys-
tems. Finally, there are techniques (mostly used for the transmission of TV signals) in which

16.1. AM RECIEVER
215
one sideband plus a part of another sideband is transmitted. This method is called Vesitigial
Sideband (VSB) signaling.
16.1
AM RECIEVER
We now know how to demodulate a DSB-LC signal. We put it through an envelope detector,
remove the DC bias and Bob’s your uncle. But there are multiple AM stations each transmitting
using the same method. In the U.S. the AM band ranges from 530–1700 kHz with any particular
radio station taking up 10 kHz. How can we be sure we will get the station we want? The answer
to that requires a number of steps. Let’s briefly describe them.
It seems the easy way out would be to use a tunable bandpass filter with a bandwidth of
10 kHz tuned to the station we want. Unfortunately, sharp tunable bandpass filters are expensive
to make. Furthermore the signals we want to receive are at high frequencies and the higher the
frequency the more difficult and hence more expensive the amplifier design. Instead the strategy
used is to move the signal of interest to a lower intermediate frequency (IF), which in the U.S.
is 455 kHz. At this intermediate frequency we filter the signal using a sharp (non-tunable)
bandpass filter rejecting all the signals we are not interested in. This filter is referred to as the IF
filter. We still have the problem of moving the signal of interest to the intermediate frequency.
As we have seen moving signals in frequency can be accomplished by multiplying the signal with
a sinusoid. The signal we are moving is a modulated sinusoid so the effect of multiplying this
with another sinusoid is going to be two modulated signals—one at the sum of the frequencies
of the two sinusoids and one at the difference. We want the locally generated sinusoid to be at
a frequency that results in one of these modulated signals to be at the intermediate frequency.
The standard is to use the difference (or hetrodyne) signal be at the intermediate frequency. We
generate a sinusoid using a local oscillator with a frequency !lo which is 455 kHz greater than
the frequency of the signal of interest. Multiplying the signal of interest with a the output of
the local oscillator we get a copy of our signal of interest moved to the intermediate frequency
and another copy to a frequency equal to the sum of the carrier and local oscillator frequency.
.x.t/ C A/ cos.!ct/ cos.!lot/ D1
2.x.t/ C A/ cos..!c   !lo/t/
C 1
2.x.t/ C A/ cos..!c C !lo/t/
There is one slight problem. If there is a signal at a frequency which is the sum of the local
oscillator frequency and 455 kHz, it will also be moved to the intermediate frequency (can you
see why). We remove this image frequency using a tunable bandpass filter called the RF (for radio
frequency) filter. But wasn’t this whole rigmarole to avoid using a tunable bandpass filter? No,
it was to avoid using a sharp tunable bandpass filter. In the case of the RF filter the signal that
has to be knocked out is far away in frequency from the signal of interest. So a poor filter will
suffice.

216
16. AMPLITUDE MODULATION
~
RF ampliﬁer
RF ﬁlter
IF ﬁlter
Mixer
IF ampliﬁer
Demodulator
Audio ampliﬁer
Speaker
Antenna
Local
oscilator
Figure 16.6: A superhetrodyne receiver.
This particular receiver is called a superheterodyne receiver shown in Figure 16.6.
Let’s go through the process using an example. Our example station is KFOR which
broadcasts from Lincoln, Nebraska at a frequency of 1240 kHz. To bring this signal down to
the intermediate frequency of 455 kHz we need the local oscillator signal to be at 1,240 C 455
D 1,695 kHz. This local oscillator frequency will also bring the image frequency of 1695 C 455
D 2,150 kHz down to 455 kHz as the difference between 2,150 kHz and 1,695 kHz is also
455 kHz. It is the job of the RF filter to knock out this image frequency.
16.2
AM STATIONS IN THE U.S.
The AM stations in the U.S. are limited to 530–1700 kHz with a station required to limit its
broadcast to a 10 kHz band. This gives us about 118 slots. However there are more than 4,500
stations currently operating in the U.S. The reason we can do this is because radio waves in
these frequencies have a limited daytime range so the same frequency can be assigned to multi-
ple stations which are geographically separated. For example, there are close to 150 stations that
broadcast at the same frequency as KFOR. During night the amplitude modulated signals can
bounce off the ionosphere vastly extending their range. This also increases the possibility of sta-
tions interfering with each other. To prevent this from happening the Federal Communication
Commission has a number of requirements—one of them being that certain stations either have
to shut down transmission at their assigned frequency at night or transmit at a much reduced
power level. These stations used to be called daytimers.
The FCC classifies AM channels as Clear Channel, Regional, and Local as well as Class
A, B, C, and D. Class A stations are generally clear channel stations and can transmit using
10kW to 50kW transmitters and are allowed to transmit for 24 hours a day. Class B stations
which are generally regional stations may transmit from 250 W to 50 KW and can transmit for
24 hours a day. Class C stations are local stations and are limited to power levels of between
250 W to 1 kW and can transmit at all hours. Class D stations have a wide power range—from
250 W to 50 kW but are restricted in terms of time. Many of the class D stations transmit
only during the day (the daytimers). If these stations transmit during the night they are limited

16.3. SUMMARY
217
to 250 W. There are currently twenty AM stations in Nebraska. Of these KFAB transmitting
at 1110 kHz is the only Clear Channel station transmitting at 50 KW. KCRO transmits at
660 kHz which is the frequency used by a Clear Channel station in New York City. Therefore,
while it transmits at 1 kW during the day it has to reduce it’s night time transmission power
to 54 Watts. There are three AM stations in Lincoln, KFOR, KLIN, and KLMS. KFOR and
KLIN are class C stations and transmit using 1 kW during both day and night. KLMS is the
local ESPN station broadcasting at 1480 kHz. It transmits using a 1 kW during the day and
750 W during the night.
16.3
SUMMARY
In this module we looked at amplitude modulation, another practical application of the mul-
tiplication property of Fourier transforms. We looked at how it is used in AM radio broadcast
and reception. We also looked at some of the features of AM radio broadcast in the US.

218
16. AMPLITUDE MODULATION
1.0
0.5
0.0
–0.5
–1.0
0.0
0.2
0.4
0.1
0.3
0.5
t
x(t)
Figure 16.7
Table 16.1: Match the figures with the signal
Signal
Figure
x(t) cos(ωot)
x(t) + cos(ωot)
x(t) cos2(ωot)
(1 + x(t)) cos(ωot)
16.4
EXERCISES
(Answers on the following page)
1. The signal x.t/ shown in Figure 16.7 combined with a sinusoid in a number of ways to
generate the signals shown in Figure 16.8.
Match the figures with the signal (see Table 16.1).
2. If we wanted to receive KLIN on our AM receiver
(a) What would be the frequency of the local oscillator?
(b) What is the image frequency that the RF filter has to block out?
3. If we wanted to receive KFAB on our AM receiver
(a) What would be the frequency of the local oscillator?
(b) What is the image frequency that the RF filter has to block out?

16.4. EXERCISES
219
1.0
0.5
0.0
–0.5
–1.0
2
1
0
–1
–2
1.0
0.5
0.0
–0.5
–1.0
2
1
0
–1
–2
0.0
0.2
0.4
0.1
0.3
0.5
t
0.0
0.2
0.4
0.1
0.3
0.5
t
(a)
(b)
0.0
0.2
0.4
0.1
0.3
0.5
t
0.0
0.2
0.4
0.1
0.3
0.5
t
(c)
(d)
Figure 16.8
2π
π
ω
X(ω)
–2π
–π
Figure 16.9: Fourier transform of the message signal.
4. Suppose I want to transmit the message signal
x.t/ D cos.!ot/
using amplitude modulation and a modulation index of 0.5. What is the value of the dc
shift A?
5. The Fourier transform of the message signal is shown in Figure 16.9

220
16. AMPLITUDE MODULATION
If we multiply the message signal with a carrier signal at a frequency of 2 Hz sketch the
transform of the modulated signal.

16.5. ANSWERS
221
Table 16.2: Match the figures with the signal
Signal
Figure
x(t) cos(ωot)
D
x(t) + cos(ωot)
A
x(t) cos2(ωot)
B
(1 + x(t)) cos(ωot)
C
–6π
6π
5π
4π
3π
2π
π
ω
X(ω)
–5π
–4π
–3π
–2π
–π
Figure 16.10: Transform of the modulated signal.
16.5
ANSWERS
1. Match the figures with the signal (see Table 16.2).
2.
(a) 1855 kHz
(b) 2310 kHz
3.
(a) 1555 kHz
(b) 2010 kHz
4. A D 2
5. Figure 16.10.


223
M O D U L E
17
Discrete Fourier Transform
Nowadays most of the time when you take the Fourier transform it will be
using a digital processor—either on the computer, on your phone, or some
other device. You are actually evaluating the discrete Fourier transform.
Much of what we have already discussed about the continuous Fourier
transform carries over to the Discrete Fourier transform so this module will
be relatively short. We will begin by developing the discrete time Fourier
transform (DTFT) which maps a discrete sequence from the time do-
main to a continuous transform in the frequency domain. From this we
will develop the discrete Fourier transform (DFT) which takes a discrete
sequence in the time domain to a discrete sequence in the frequency domain.
17.1
DISCRETE TIME FOURIER TRANSFORM (DTFT)
We have actually already encountered the discrete time Fourier transform in a slightly different
guise when we looked at sampling. The sampling process maps a continuous time signal into
a discrete time signal. The Fourier transform of that must then be the discrete time Fourier
transform. Let’s work through the math. Recall that the sampled function can be modeled by
a product of the continuous time function x.t/ and an impulse train. The Fourier transform of
the sampled function is
F
"
x.t/
1
X
nD 1
ı.t   nT /
#
D
Z 1
 1
x.t/
1
X
nD 1
ı.t   nT /e j!tdt
D
1
X
nD 1
Z 1
 1
x.t/ı.t   nT /e j!tdt
D
1
X
nD 1
x.nT /e j!nT
where we have used the sifting property of the delta function. If we set T D 1 we get
1
X
nD 1
x.nT /e j!nT D
1
X
nD 1
xŒne jn!

224
17. DISCRETE FOURIER TRANSFORM
where we have replaced the parentheses with brackets to emphasize the fact that we are now
dealing with a discrete sequence. This is the discrete time Fourier transform of the sequence
fxŒng. The convention is to represent the Fourier transform as X.ej!/. Sticking with the con-
vention we get
X.ej!/ D
1
X
nD 1
xŒne jn!
So what about the inverse Fourier transform? Remember that the Fourier transform of the sam-
pled function was periodic with period 2=T . Given that we set T D 1 the DTFT is periodic
with period 2.
Take another look at the previous equation. On the right hand side you have an expansion
in terms of ejn!. It looks suspiciously like the Fourier expansion of a periodic function so very
likely that the same tricks we used in the case of the Fourier series to come up with an expression
for the Fourier series coefficients an will work to find the expression for xŒn. In order to do this
we need only to notice that
1
2
Z
2
e jn!ejk!d! D
(
1
k D n
0
k ¤ n
xŒn D 1
2
Z
2
X.ej!/ejn!d!
Example 17.1
Find the discrete time Fourier transform of
xŒn D
(
1
n D 0; 1; 2; 3
0
otherwise
X.ej!/
D
1
X
nD 1
xŒne jn!
D
3
X
nD0
e jn!
D
1 C e j! C e j 2! C e j3!
D
e j!=2 
ej!=2 C e j!=2
C e j5!=2 
ej!=2 C e j!=2
D

ej!=2 C e j!=2 
.e j!=2 C e j5!=2
D
2 cos.!=2/

e j!=2 C e j5!=2

17.2. PROPERTIES OF THE DTFT
225
17.2
PROPERTIES OF THE DTFT
Most of the properties of the discrete time Fourier transform are very much like those of the
continuous time Fourier transform. We will simply list them here.
1. Linearity
F Œ˛x1Œn C ˇx2Œn D ˛X1.ej!/ C ˇX2.ej!/
2. Convolution property
F Œx1Œn ~ x2Œn D X1.ej!/  X2.ej!/
3. Time shift property
F ŒxŒn   no D e j!noX.ej!/
4. Multiplication property
F Œx1Œn  x2Œn D X1.ej!/ ~ X2.ej!/
There is one that is a bit different.
17.2.1
DIFFERENTIATION IN THE FREQUENCY DOMAIN
By taking the derivative of the discrete time Fourier transform in terms of ! we can find the
discrete time Fourier transform of nxŒn in terms of the discrete time Fourier transform of xŒn.
This is very easy to show
dX.ej!/
d!
D
d
d!
1
X
nD 1
xŒne jn!
D
1
X
nD 1
d
d! xŒne jn!
D
1
X
nD 1
xŒn d
d! e jn!
D
1
X
nD 1
 jnxŒne jn!
Multiplying both sides by j
j dX.ej!/
d!
D
1
X
nD 1
nxŒne jn!
Remembering Bill and Bob this means that
F ŒnxŒn D j dX.ej!/
d!

226
17. DISCRETE FOURIER TRANSFORM
17.3
DISCRETE FOURIER TRANSFORM
The reason we are interested in this discrete representation is that much of the processing nowa-
days is conducted in the discrete domain. This means that not only is the time domain repre-
sentation of the signal discrete—so are the frequency representations. The discrete time Fourier
transform is a transform for discrete time functions—the representation in the time domain is
discrete. However, the transform itself is a continuous periodic function of !. To obtain a trans-
form which will give us discrete representations in both the time and frequency domain we turn
to the Discrete Fourier Transform (DFT).
We begin with the DTFT and discretize it by sampling the transform X.ej!n/ N times
over a period. Remembering that the period is 2 we get
X.ejk.2=N// D
1
X
nD 1
xŒne jnk.2=N /
Having reduced the number of frequency components from infinity to N can we still recover
xŒn from X.ejk.2=N//? The answer is yes, but only under specific conditions. Let’s first show
the condition under which we can recover xŒn from X.ejk.2=N //. We will then see why this
condition is necessary.
We can recover xŒn from X.ejk.2=N // if xŒn is nonzero only for L consecutive values
of n where L  N. To show this we will use the fact that
N 1
X
nD0
ejk.2=N/n D
(
N
k D 0; ˙N; ˙2N; : : :
0
otherwise
Showing this is simple—we just use the geometric sum formula for k ¤ 0; ˙N; : : :.
N  1
X
nD0
ejk.2=N/n
D
N  1
X
nD0

ejk.2=N /n
D
e0   ejk.2=N /.N 1C1/
1   ejk.2=N /
D
1   ejk2
1   ejk.2=N /
D
0
where we have used the fact that ejk2 D 1.
If xŒn D 0 for n  N we can rewrite the limits of summation in our expression for
X.ejk.2=N// as
X.ejk.2=N// D
N  1
X
nD0
xŒne jnk.2=N /

17.3. DISCRETE FOURIER TRANSFORM
227
Multiply both sides both sides of the equation for X.ejk.2=N // with ejlk.2=N / and sum from
k D 0 to N   1.
N 1
X
kD0
X.ejk.2=N//ejlk.2=N/ D
N 1
X
kD0
N  1
X
nD0
xŒne jk.2=N /nejlk.2=N /
Combining the two exponentials of the right hand side we get
N  1
X
kD0
X.ejk.2=N//ejlk.2=N/
D
N  1
X
kD0
N  1
X
nD0
xŒnej.l n/.2=N /k
D
N  1
X
nD0
xŒn
N 1
X
kD0
ej.l n/.2=N /k
The inner sum on the right hand side is either equal to N or to 0. It is equal to N when n   l D 0
or n D l, and 0 for all other values of n. Therefore,
N  1
X
kD0
X.ejk.2=N//ejlk.2=N / D xŒlN
or
xŒl D 1
N
N 1
X
kD0
X.ejk.2=N //ejlk.2=N /
For convenience let’s define
XŒk D X.ejk.2=N //
Then we get the DFT equations—the forward DFT
XŒk D
N 1
X
nD0
xŒne jnk.2=N /
and the inverse DFT
xŒn D 1
N
N  1
X
kD0
XŒkejnk.2=N /
Therefore, given a discrete time sequence fxŒng which is zero for n < 0 and n  N we
can find the spectral representation at discrete frequency values. And given the discrete spectral
representation fXŒkg we can recover the discrete time sequence fxŒng.

228
17. DISCRETE FOURIER TRANSFORM
What is the effect of sampling the frequency representation in the general case when we
do not have a restriction on the extent of the nonzero values of fxŒng? To answer this we begin
with the sampled DTFT equation
XŒk D
1
X
nD 1
xŒne jnk.2=N /
where we have used XŒk D X.ejk.2=N// and rewrite the infinite sum as a sum of finite sums.
XŒk D    C
 N 1
X
nD 2N
xŒne jnk.2=N/ C
 1
X
nD N
xŒne jnk.2=N / C
N 1
X
nD0
xŒne jnk.2=N /
C
2N 1
X
nDN
xŒne jnk.2=N/ C   
Each of the finite sums are of the form
mN CN  1
X
nDmN
xŒne jnk.2=N /
where m varies from negative infinity to infinity. So we can rewrite the expression for XŒk as
XŒk D
1
X
mD 1
mN CN  1
X
nDmN
xŒne jnk.2=N /
This in turn can be rewritten as
XŒk D
1
X
mD 1
N  1
X
nD0
xŒn C mNe j.nCmN/k.2=N /
Let’s examine the complex exponential in this equation.
e j.nCmN/k.2=N/ D e jnk.2=N /  e jmN k.2=N / D e jnk.2=N /  e jmk2
Noting that
e jmk2 D 1
we have
e j.nCmN/k.2=N / D e jnk.2=N /
Substituting this back into the expression for XŒk
XŒk D
1
X
mD 1
N  1
X
nD0
xŒn C mNe jnk.2=N /

17.3. DISCRETE FOURIER TRANSFORM
229
Switching the order of summation we get
XŒk D
N  1
X
nD0
"
1
X
mD 1
xŒn C mN
#
e jnk.2=N /
Define
xpŒn D
1
X
mD 1
xŒn C mN 
The expression for XŒk now becomes
XŒk D
N  1
X
nD0
xpŒne jnk.2=N /
Comparing this to the earlier boxed expression for XŒk with xpŒn in place of xŒn. From that
earlier work we know that given fXŒkg we can recover xpŒn. Let’s take a look at the relation-
ship between xpŒn and xŒn. The expression for xpŒn should look somewhat familiar. We used
something similar to this when we were developing the Fourier transform of continuous time
signals. We called it the periodic extension of x.t/. We can see that xpŒn is clearly periodic. To
see the relationship between xpŒn and xŒn let’s write out xpŒn for a few values of n.
xpŒ0
D
   C xŒ 2N C xŒ N  C xŒ0 C xŒN  C xŒ2N  C   
xpŒ1
D
   C xŒ1   2N  C xŒ1   N C xŒ1 C xŒ1 C N C xŒ1 C 2N  C   
:::
:::
xpŒN   1
D
   C xŒ N   1 C xŒ 1 C xŒN   1 C xŒ2N   1 C xŒ3N   1 C   
So if xŒn D 0 outside the range 0  n  N   1 then xpŒn D xŒn. If not, xpŒn is an aliased
version of xŒn. Notice the similarity with our study of sampling. Just as sampling in the time
domain resulted in a periodic extension in the frequency domain, sampling in the frequency
domain results in a periodic extension in the time domain. Just as increasing the number of
samples in the time domain spread the replicas in the frequency domain, increasing the number
of samples in the frequency domain spreads out the replicas in the time domain. And just as
an insufficient number of samples (undersampling) in the time domain results in aliasing in the
frequency domain, an insufficient sampling in the frequency domain results in aliasing in the
time domain. The bottom line though is that if we have a time limited sampled signal we can
get a unique spectral profile of it in the discrete domain. This allows us to perform operations in
the spectral domain using digital processors that we otherwise would not be able to do.
In the DFT literature there is an additional notational shortcut that is often used. Define
WN D e j 2
N

230
17. DISCRETE FOURIER TRANSFORM
Table 17.1: Periodic extension of xŒn
x[0]
x[1]
x[2]
x[3]
x[4]
x[5]
x[6]
x[7]
x[8]
x[9]
x[10]
x[11]
1
a
a2
a3
1
a
a2
a3
1
a
a2
a3
then The forward DFT becomes
XŒk D
N 1
X
nD0
xŒnW nk
N
and the inverse DFT becomes
xŒn D 1
N
N 1
X
kD0
XŒkW  nk
N
17.4
PROPERTIES OF THE DFT
Unlike the case of the Fourier transform, the discrete time Fourier transform, and even the
Fourier series, we are dealing with signals that are implicitly periodic in both domains. This
means that while the sequence is only defined for the values of the indices in the interval 0 
n  N   1 the operations we perform on them using the DFT assumes that this is only one
period of a periodic function. Therefore, there is an inherent assumption that the sequence has
nonzero values outside of the range 0  n  N   1. In order to deal with this assumption we
need to take a closer look at operations which need the value of the sequence for indices outside
of this range. Let’s examine shifts in the time domain.
17.4.1
LINEAR AND CIRCULAR SHIFTS
The values of the time sequence xŒn is only available for n D 0; 1; : : : N   1 and the frequency
sequence XŒk for k D 0; 1; : : : ; N   1. If we need to express xŒn for n outside of the range
0; 1; : : : ; N   1 we can do so by evaluating xŒn mod N. Consider a simple example where N D
4. Let’s suppose xŒn D an for n D 0; 1; 2; 3 then we can write the periodic extension of xŒn as
in Table 17.1.
We can see that xŒ4 D xŒ0 D 1 and xŒ5 D xŒ1 D a and so on. In other words xŒn D
xŒn mod N.
So what happens if we shift the signal? Do we have to consider the samples of the periodic
extension of the signal in neighboring periods? Or can we still operate on the values of xŒn in
the interval 0  n  N   1? We will try and answer these questions using a simple example.
Consider the graphical representation of xŒn in Figure 17.1 where we have indicated the range
between 0 and N   1 (where N D 4) with the dashed box.

17.4. PROPERTIES OF THE DFT
231
x[n]
n
Figure 17.1: A discrete function.
x[n–1]
n
x[n–2]
n
x[n–3]
n
Figure 17.2: Shifts of the discrete function.
Shifting the function one time unit at a time we get the sequences shown in Figure 17.2.
As before we have indicated the range of n between 0 and N   1 with a dashed box. Notice that
while the exact values for n D 0; 1; 2; and 3 is different in each shift, all the different values of
xŒn are within the window n D 0; 1; 2; 3.
If we remove all the values outside the range 0  n  N   1 as shown in Figure 17.3 we
can see what each linear shift looks like if we only look at the values in the range 0  n  N   1.
If we only look within the window each time we linearly shift the periodic extension of the
sequence the sequence in the window is circularly shifted. After a shift the last value in the window
prior to the shift becomes the first value in the window after the shift. Therefore, by using circular

232
17. DISCRETE FOURIER TRANSFORM
x[n–1]
n
x[n–2]
n
x[n–3]
n
Figure 17.3: Circular shifts.
shifts when linear shifts are required we can satisfy both the implicit periodicity required by the
development of the DFT and the fact that the sequence is only defined for N values.
Example 17.2
Find the discrete time Fourier transform of
xŒn D
(
1
n D 0; 1; 2; 3
0
otherwise
This is exactly the function for which we found the discrete time Fourier transform. Let’s
pick N D 4 and see what the DFT looks like. Taking N D 4 the forward DFT equation is
XŒk D
N  1
X
nD0
xŒne jnk.2=4/ D
N  1
X
nD0
xŒne jnk.=2/

17.4. PROPERTIES OF THE DFT
233
Plugging in different values of k we get.
XŒ0
D
1 C 1 C 1 C 1
D
4
XŒ1
D
1 C e j 
2 C e jj C e j 3
2
D
1   j   1 C j D 0
XŒ2
D
1 C e j C e j2 C e j3
D
1   1 C 1   1 D 0
XŒ3
D
1 C e j 3
2 C e j 3 C e j 9
2
D
1 C j   1   j D 0
This looks like a very different result than what we obtained using the DTFT! We can check by
sampling the discrete time Fourier transform at multiples of =2. Recall that the DTFT was
X.ej!/ D 2 cos.!=2/

e j!=2 C e j 5!=2
For k D 0 we have
XŒ0 D X.ej0/ D 2 cos.0/
 e j 0 C e j0
D 2.1 C 1/ D 4
For k D 1 we have
XŒ1 D X.ej=2/
D
2 cos.=4/

e j=4 C e j 5=4
D
2
p
2
 1
p
2
  j 1
p
2
  1
p
2
C j 1
p
2

D
0
For k D 2 we have
XŒ2 D X.ej/
D
2 cos.=2/

e j=2 C e j5=2
D
0 . j   j /
(17.1)
D
0
And finally for k D 3 we have
XŒ3 D X.ej3=2/
D
2 cos.3=4/

e j3=4 C e j15=4
D
  2
p
2

  1
p
2
  j 1
p
2
C 1
p
2
C j 1
p
2

D
0

234
17. DISCRETE FOURIER TRANSFORM
We get the same result. So the DFT is a sampling of the DTFT. But why did we get this
particular result? If we take these values of xŒn to be the values of a periodic function with period
4, then the periodic extension of this is a function that is 1 for all values of n! The spectral profile
of a constant function will only have a value for a frequency of zero, which is what XŒ0 is. All
other values will be zero. What can we do with this? Well we could pick N to be larger so that this
effect goes away. If we pick N D 8 we get XŒ0 D 4, XŒ1 D 1   j2:414, XŒ2 D 0, XŒ3 D 1  j 0:414, XŒ4 D 0, XŒ5 D 1 C j0:414, X.6/ D 0, and XŒ7 D 1 C j 2:414—a somewhat more
interesting result. This process of adding zeros is called zero padding and it allows us to sample
the DTFT more finely.
With this under our belt we can begin our look at the properties of the discrete Fourier
transform. All the properties of the continuous time Fourier transform carry over to the discrete
time with appropriate modifications. Let’s begin with the most important and easiest to show.
17.4.2
LINEARITY
The discrete Fourier transform, like it’s continuous time and discrete time counterparts, is
linear—keeping in mind the implied period N
DFT Œ˛xŒn C ˇyŒn D ˛XŒk C ˇY Œk
where XŒk is the discrete Fourier transform of xŒn and Y Œk is the discrete Fourier transform
of yŒn. We can easily show this using the linearity of the summation operation.
17.4.3
TIME SHIFT
If the DFT of the time sequence xŒn is the frequency sequence XŒk let’s find the DFT of the
time shifted sequence xŒn   no.
DFT ŒxŒn   no D
N  1
X
nD0
xŒn   noW nk
multiplying and dividing the right hand side by W  nok.
DFT ŒxŒn   no
D
W nok
N 1
X
nD0
xŒn   noW nkW  nok
D
W nok
N 1
X
nD0
xŒn   noW .n no/k
We would like to only use the values of xŒn in the interval 0 : : : N   1 which we can if we replace
n   no by .n   no/ mod N. To keep the correspondence we also need to do the same to W .n no/k
but can we? Let’s take a look at W k.n mod N /.

17.5. THE FAST FOURIER TRANSFORM
235
If 0  n < N them n mod N D n and
W k.n mod N / D W kn
if n  N then n mod N D n   mN for some integer m and
W k.n mod N/
D
e k.n mod N/j 2
N
D
e k.n mN/ 2
N
D
e kn 2
N ekmN 2
N
D
e kn 2
N ekm.2/
D
e kn 2
N
D
W kn
If n < 0 then n mod N D n C mN for some m and by the same process as above we can
show that W k.n mod N/ D W kn.
Using this we can write
DFT ŒxŒn   no
D
W nok
N  1
X
nD0
xŒ.n   no/ mod N W ..n no/ mod N/k
D
W nokXŒk
where we have used the fact that the order in which we sum elements within a finite sum does
not change the sum.
17.4.4
CONVOLUTION
Just as in the case of the continuous time and discrete time Fourier transforms, the DFT of the
convolution of two sequences is the product of the DFT of the individual sequences. However,
in order to incorporate the implicit periodicity of the sequences we need to map any index that
falls outside of the range between 0 and N   1 back into this range. As shown earlier we can do
this by using the modulo operation. The convolution property thus becomes
DFT
"N  1
X
kD0
xŒkhŒ.n   k/ mod N 
#
D X.k/H.k/
17.5
THE FAST FOURIER TRANSFORM
In practice directly computing the DFT can be very resource intensive with the number of com-
putations growing with N 2. Because of this, a number of fast algorithms have been developed
with the most popular being the Fast Fourier Transform (FFT) algorithm developed by James
W. Cooley and John W. Tukey in 1965. A description of the algorithm is beyond the scope of
this course but if you are interested in using it MATLAB has an fft function you can play with.

236
17. DISCRETE FOURIER TRANSFORM
17.6
SUMMARY
In this module we developed the discrete Fourier transform (DFT) using the discrete time
Fourier transform (DTFT) as a stepping stone. We pointed out some of the possible issues that
may arise when using a DFT but we didn’t really go into much detail. There is much more to
learn about this very useful transform and its implementation. You can get a much more detailed
introduction in digital signal processing courses. These courses are both a great deal of fun and
immensely useful. Please do take one.

17.7. EXERCISES
237
17.7
EXERCISES
(Answers on the following page)
1. Find the discrete time Fourier transform of the sequence
xŒn D
1
2
n
uŒn
2. Find the discrete time Fourier transform of the sequence
xŒn D
1
2
jnj
3. Find the discrete time Fourier transform of
xŒn D
8
<
:
1
2
n
n D 0; 1; 2; 3
0
otherwise
4. Find the discrete Fourier transform of
xŒn D
1
2
n
n D 0; 1; 2; 3
with N D 4. Check your answer by taking samples of the answer to the previous question
every =2.
5. Find the discrete Fourier transform of
xŒn D
1
2
n
n D 0; 1; 2; 3
with N D 8. Check your answer by taking samples of the answer to the question 3 every
=4.

238
17. DISCRETE FOURIER TRANSFORM
17.8
ANSWERS
1.
X.ej!/ D
1
1   0:5e j!
2.
X.ej!/ D
0:75
1   cos.!/ C 0:25
3.
X.ej!/ D

1 C 1
4e j2!
 
1 C 1
2e j!

4. XŒ0 D 1:875, XŒ1 D 0:75   j 0:375, XŒ2 D 0:625, XŒ3 D 0:75 C j0:375
5. XŒ0 D 1:875, XŒ1 D 1:265   j 0:692, XŒ2 D 0:75   j0:375, XŒ3 D 0:735   j 0:192,
XŒ4 D 0:625, XŒ5 D 0:735 C j 0:192, XŒ6 D 0:75 C j0:375, XŒ7 D 1:265 C j0:692

239
M O D U L E
18
The Laplace Transform –
Introduction
The Fourier transform is an immensely useful tool for understanding sig-
nals. However, when it comes to analyzing systems there is a limitation to
the use of the Fourier transform. Recall that the Fourier transform exists
for all signals satisfying the Dirichlet conditions. Namely:
1. The function x.t/ should be absolutely integrable.
Z
jx.t/j < 1
2. The function has bounded variation which means that it has only a
finite number of zero crossings in any finite interval.
3. The function has only a finite number of finite discontinuities.
The last two conditions usually are not a hindrance to analyzing systems as it would be unusual
to have a system that has an impulse response that does not satisfy these conditions. However,
when it comes to analyzing systems, the first condition can sometimes be problematic. A linear
time invariant system is completely described by its impulse response h.t/. Recall that for a
system to be stable we need
Z 1
 1
jh.t/jdt < 1
which is the first Dirichlet condition. This means that a system has to be stable in order for the
Fourier transform of its impulse response—the transfer function—to exist. What if we want
to work with unstable systems (and we do)? The Fourier transform for such a system does not
exist, however, the idea of giving up the convenience of using the transfer function to relate the
input and output is not a pleasant one to contemplate. Luckily we have a workaround which
will allow us to retain the idea of a transfer function, allow us to relate the input and output of
a linear time-invariant system using multiplication, and analyze the stability of systems under
different conditions. To understand this workaround which is at the heart of our view of the
Laplace transform let’s consider a toy example.

240
18. THE LAPLACE TRANSFORM – INTRODUCTION
h(t)
t
0.0
0.2
0.4
0.6
0.8
1.0
10
8
6
4
2
0
Figure 18.1: An example of the impulse response of an unstable system.
h(t)e–σt
t
0.0
0.2
0.4
0.6
0.8
1.0
2.0
1.5
1.0
0.5
Figure 18.2: Weighting the impulse response of an unstable system with a decaying exponential.
Example 18.1
Suppose we have a system with an impulse response shown in Figure 18.1.1
Based on what we can see the first Dirichlet condition is not satisfied and, therefore, we cannot
find the Fourier transform of this impulse response.
So here’s our little dodge. What if we multiply the impulse response with a decaying
exponential e t to get the signal shown in Figure 18.2. This weighted impulse response looks
like it would satisfy the Dirichlet conditions. Given that this weighted impulse response satisfies
the Dirichlet conditions we can find it’s Fourier transform. True, but
1. So what? What is the use of finding the Fourier transform of h.t/e t when we are inter-
ested in the Fourier transform of h.t/.
2. Does the result depend on the value of  used.
The answer to the second question is yes, and we will spend some time looking at that. In
Figure 18.3 we show the effect of different values of  with the impulse response of our toy
example. You can clearly see that certain values of  actually exacerbate the problem while other
1For the curious this is a plot of h.t/ D cos.2t/ C e2t.

18. THE LAPLACE TRANSFORM – INTRODUCTION
241
σ = –4
t
0.0
0.5
1.0
σ = –3
t
0.0
0.5
1.0
σ = –2
t
0.0
0.5
1.0
σ = –1
t
0.0
0.5
1.0
σ = 0
t
0.0
0.5
1.0
σ = 1
t
0.0
0.5
1.0
σ = 2
t
0.0
0.5
1.0
σ = 3
t
0.0
0.5
1.0
σ = 4
t
0.0
0.5
1.0
500
0
50
0
2.0
1.5
1.0
0.0
10
5
0
2
1
0
2
1
0
200
100
0
3
2
1
20
10
0
Figure 18.3: Weighting the impulse response of an unstable system with a decaying exponential
e t for different values of .
values of  result in a product that satisfies the Dirichlet conditions. The range of values for
which the Dirichlet conditions are satisfied is going to be important in what follows.
The answer to the first question is that this method gives us a way of analyzing systems and
understanding the issues related to their stability as well as providing a backdoor to exploring
their spectral properties. And it gives us a transfer function for simply relating the input and
output of a linear time-invariant system. Which is what we will do in this and the following
modules.
Let’s go through this development again, but this time with a general impulse response
h.t/. As with out toy problem we can weight h.t/ with an exponential e t such that for some
values of 
Z 1
 1
jh.t/e tjdt < 1

242
18. THE LAPLACE TRANSFORM – INTRODUCTION
Then as h.t/e t satisfies the Dirichlet condition we can find the Fourier transform of this
weighted function
F

h.t/e t
D
Z 1
 1
Œh.t/e te j!tdt
D
Z 1
 1
h.t/e .Cj!/dt
If we rename  C j! such that s D  C j!, then the transform shown will become a function
of s
H.s/ D
Z 1
 1
h.t/e stdt
H.s/ is called the Laplace transform of h.t/. Unlike the Fourier transform H.!/ which when it
exists exists for all ! the Laplace transform H.s/ may only exist for a restricted set of values of
. To see why this is so consider the exponential functions e 2tu.t/ and e2tu.t/. The first is an
exponentially decreasing function which clearly satisfies the Dirichlet conditions and hence for
which the Fourier transform exists. The second is an exponentially increasing function which
just as clearly violates the first Dirichlet condition and, hence, for which the Fourier transform
does not exist.
Let’s begin with the Laplace transform of x.t/ D e 2tu.t/.
X.s/
D
Z 1
 1
e 2tu.t/e stdt
D
Z 1
0
e 2te stdt
D
Z 1
0
e .sC2/tdt
D
 1
s C 2e .sC2/t j1
0
Things have proceeded pretty smoothly up to this point. But now we have to figure out what
happens when we evaluate e .sC2/t for t ! 1 and t D 0. The latter is not an issue. When we
set t D 0 we get e0 which is 1. The first is a bit more problematic. To see how we resolve this,
let’s write s in terms of its component parts  and !. If we set s D  C j! in the equation above
we get
X.s/
D
 1
s C 2e .Cj!C2/t j1
0
Splitting apart the exponent into the real and imaginary parts we get
X.s/
D
 1
s C 2e .C2/te j!t j1
0

18. THE LAPLACE TRANSFORM – INTRODUCTION
243
Lets see what happens to each of the two factors e .C2/t and e j!t as t ! 1. Let’s start with
e j!t.
Truth be told, it is not at all clear what value e j!t will take as t ! 1. However, thanks
to our old friend Euler we do know that
e j!t D cos.!t/   j sin.!t/
The magnitude of this complex number is given by
ˇˇe j!tˇˇ
D
q
cos2.!t/ C sin2.!t/
D
D
p
1
D
1
So, while we do not know the value of e j!t when t goes to infinity we do know that whatever
it is, its magnitude will be equal to one. Hopefully this much information will suffice.
Let’s now look at the other factor, e .C2/t. First let us look at what the exponent does as
t ! 1.
lim
t!1  . C 2/t D‹
The limit is either  1 or C1 depending on whether  C 2 is positive or negative. For  C 2 >
0, which happens when  >  2, the limit is  1. When  C 2 is negative or  C 2 < 0, which
happens when  <  2, the limit is C1. Therefore,
lim
t!1 e .C2/t D
 0
 >  2
C1
 <  2
We have enough information now to finish evaluating the Laplace transform integral.
X.s/
D
 1
s C 2 .0   1/
 >  2
D
1
s C 2
 >  2
Note that the 1=.s C 2/ is the Laplace transform only for  >  2. The set of values in the s
plane for which  >  2, shown in Figure 18.4, is called the region of convergence (also denoted
by ROC) of the Laplace transform and is an essential part of the Laplace transform.
This bears repeating:
The region of convergence is an integral part of the Laplace transform.
A couple of things to notice before we leave this example. Notice that the function, x.t/ D
e 2tu.t/, satisfies the Dirichlet conditions and, therefore, has a Fourier transform. Looking at

244
18. THE LAPLACE TRANSFORM – INTRODUCTION
jω
σ
σ > –2
Region of convergence
–5
–4
–3
–2
–1
1
2
3
4
5
Figure 18.4: The region of convergence for the Laplace transform of e 2tu.t/.
the integral equation for the Laplace transform
X.s/
D
Z 1
 1
x.t/e stdt
D
Z 1
 1
x.t/e .Cj!/tdt
if we set  D 0 in this equation we get the Fourier transform. But, we can only do this if the
 D 0 line is contained in the region of convergence. Notice in Figure 18.4 the line  D 0 is
contained in the region of convergence. If we set  D 0 in this Laplace transform we get
X.!/ D
1
s C 2jD0 D
1
 C j! C 2jD0 D
1
j! C 2
Computing the Fourier transform of e 2tu.t/ directly we see that this is indeed the case. Finally,
if x.t/ was the impulse response of a linear time invariant system just the fact that the Fourier
transform exists means that the impulse response is absolutely integrable. Which in turn means
that the system is stable in the bounded-input-bounded-output (BIBO) sense. So, for a system
with impulse response h.t/ the following three statements are equivalent.
1. The system is BIBO stable.
2. The Fourier transform of the impulse response exists.
3. The line  D 0 in the s-plane is part of the region of convergence for the Laplace transform
of h.t/.
This also suggests that for an unstable system if we could somehow move the boundary of the
region of convergence so that the  D 0 line is within the region of convergence the unstable
system could be made stable. We will pursue this idea later on when we talk about feedback.

18.1. SUMMARY
245
Let’s go through the same process and find the Laplace transform for x.t/ D e2tu.t/.
Once again we start with the integral definition of the Laplace transform
X.s/ D
Z 1
 1
e2tu.t/e stdt
and proceed as we did for x.t/ D e 2tu.t/.
X.s/
D
Z 1
 1
e2tu.t/e stdt
D
Z 1
0
e2te stdt
D
Z 1
0
e .s 2/tdt
D
 1
s   2e .s 2/t j1
0
D
 1
s   2e .Cj! 2/t j1
0
D
 1
s   2e . 2/te j!t j1
0
D
 1
s   2 .0   1/
 > 2
D
1
s   2
 > 2
As before the region of convergence,  > 2 in this case, consists of those values of  for which
the integral converges, i.e., does not go to infinity. A graphical representation of the region
of convergence is shown in Figure 18.4. In this case the function, x.t/ D e2tu.t/, does not
satisfy the first Dirichlet condition and, therefore, does not have a Fourier transform. Looking
at the region of convergence we can see that the line  D 0 is not included in the region of
convergence so we could not use the trick of setting  D 0 in the Laplace transform to get the
Fourier transform. Also if x.t/ were the impulse response of a linear time invariant system then
we can use the fact that the Fourier transform does not exist to infer that the impulse response
is not absolutely integrable. Which, in turn, means that the system is not BIBO stable.
18.1
SUMMARY
In this module (Figure 18.5)
•
We have introduced the Laplace transform
X.s/ D
Z 1
 1
x.t/e stdt

246
18. THE LAPLACE TRANSFORM – INTRODUCTION
σ
σ > 2
–5
–4
–3
–2
–1
1
2
3
4
5
Region of
convergence
jω
Figure 18.5
•
The transform exists for a range of values of . This range is called the Region of
Convergence (ROC).
•
If the region of convergence includes the j! axis the Fourier transform exists.
•
If the Fourier transform exists and x(t) is an impulse response then the system, for
which x(t) is the impulse response, is stable.

18.2. EXERCISES
247
18.2
EXERCISES
(Answers on the following page)
What is the region of convergence for the Laplace transform of
1.
x.t/ D e 4tu.t/
2.
x.t/ D e4tu.t/
3.
x.t/ D tu.t/
4.
x.t/ D t2
5.
x.t/ D t2u.t/
6.
x.t/ D cos.4t/u.t/
7.
x.t/ D sin.4t/u.t/
8.
x.t/ D e 4tu.t/ C e 2tu.t/
9.
x.t/ D e4tu.t/ C e 2tu.t/

248
18. THE LAPLACE TRANSFORM – INTRODUCTION
18.3
ANSWERS
1. ROC:  >  4
2. ROC:  > 4
3. ROC:  > 0
4. ROC: Does not converge
5. ROC:  > 0
6. ROC:  > 0
7. ROC:  > 0
8. ROC:  >  2
9. ROC:  > 4

249
M O D U L E
19
Uniqueness and Linearity of
the Laplace Transform
In this module we look at two properties of the Laplace transform that
make it very useful—its uniqueness and its linearity. The Laplace trans-
form, like the Fourier transform, affords us an alternative view of a signal.
However, remember that both a function of time and its Laplace transform
are different representations of the same physical process. Therefore, in or-
der to accord with reality the function of time and its Laplace transform
have to form a unique pair—each implying the other. This being the case
it is important that we establish the conditions under which the Laplace
transform is unique—the function of time and its Laplace transform form
a pair.
The linearity property makes the Laplace transform convenient to use. Without this prop-
erty we probably would not have been talking about it. So let’s take a look at these properties.
19.1
THE LAPLACE TRANSFORM IS UNIQUE – AS LONG
AS WE INCLUDE THE ROC IN THE TRANSFORM
A rather long sub-heading, but we really want to emphasize the fact that the region of conver-
gence is an integral part of the Laplace transform. Let’s see why the emphasis needs to be there
with an example. Recall in the last module we obtained the Laplace transform of
x.t/ D e 2tu.t/
as
X.s/ D
1
s C 2
 >  2
To see the reason why we need to include the region of convergence let’s find the Laplace trans-
form of
x.t/ D  e 2tu. t/

250
19. UNIQUENESS AND LINEARITY OF THE LAPLACE TRANSFORM
X.s/
D
 Z 1
 1
e 2tu. t/e stdt
D
 Z 0
 1
e 2te stdt
D
 Z 0
 1
e .sC2/tdt
D
1
s C 2e .sC2/t ˇˇ0
 1
D
1
s C 2e .Cj!C2/t ˇˇ0
 1
D
1
s C 2e .C2/te j!t ˇˇ0
 1
As in the case of the computation of the Laplace transform of e 2tu.t/ we need to consider
what happens at the lower limit of negative infinity. The magnitude of e j!t is always bounded
by one so, as before we need to look at the behavior of the term e .C2/t. This term will either
go to infinity or to zero, in other words diverge or converge, depending on whether the exponent
is positive or negative. The exponent is negative—the integral converges—when  C 2 < 0 or
 <  2. Therefore,
X.s/
D
1
s C 2 .1   0/
 <  2
D
1
s C 2
 <  2
Notice we ended up with same algebraic expression for X.s/, 1=.s C 2/, for both e 2tu.t/
and for  e 2tu. t/ even though as shown in Figure 19.1 the two signals are completely differ-
ent. Therefore, if we simply used the algebraic expression of X.s/ we would be unable to uniquely
describe the particular function of time for which X.s/ is the Laplace transform. We have to in-
clude the region of convergence for X.s/ and x.t/ to be a unique pair. Notice we say region of
convergence. As you can see from Figure 19.2, X.s/ is valid over a region in the s-plane.
Notice the difference in the regions for the two functions. For the signal e 2tu.t/ which
only takes on non-zero values to the right of a point (in this case t D 0)—a right-sided
function—the region of convergence is to the right of the boundary  D  2. For the signal
 e 2tu. t/ which only takes on non-zero values to the left of a point (t D 0)—a left-sided
function—the region of convergence is to the left of the boundary  D  2. Finally, notice that
in one case the region of convergence includes the j! axis while in the other case it does not.
Before we discuss why this is important let us generalize our example.

19.1. THE LAPLACE TRANSFORM IS UNIQUE – AS LONG AS WE INCLUDE THE ROC
251
t
t
e–2tu(t)
–e–2tu(–t)
–0.5
0
0.5
1.0
–0.5
0
0.5
–1.0
1.0
0.5
0.0
–0.5
0
–2
–4
Figure 19.1: The signals e 2tu.t/ and  e 2tu. t/.
jω
σ
σ > –2
Region of
convergence
–5
–4
–3
–2
–1
1
2
3
4
5
jω
σ
σ < –2
Region of
convergence
–5
–4
–3
–2
–1
1
2
3
4
5
Figure 19.2: The region of convergence for the Laplace transform of e 2tu.t/ and  e 2tu. t/.
Let’s consider the signal
x.t/ D e atu.t/
For a > 0 this is a decaying exponential with absolute integral
Z 1
 1
e atu.t/dt
D
Z 1
0
e atdt
D
 1
ae atj1
0

252
19. UNIQUENESS AND LINEARITY OF THE LAPLACE TRANSFORM
When we evaluate e at for t ! 1, because a > 0, limt!1 e at ! 0. Therefore,
F

e atu.t/

D
1
a C j!
However, for a < 0
lim
t!1 e at ! 1
Therefore, the Fourier transform of e atu.t/ exists for a > 0 but not for a < 0. As you might
surmise the Laplace transform of e atu.t/ exists for both all values of a. The Laplace transform
is given by
X.s/
D
Z 1
 1
e atu.t/e stdt
D
Z 1
0
e .sCa/tdt
D
 1
s C ae .sCa/tj1
0
D
1
s C a
for  >  a
We write this as
L

e atu.t/

D
1
s C a
ROC:  >  a
where ROC stands for region of convergence. Notice that the Laplace transform exists regardless
of whether a is positive or negative. However, the sign of a does affect the region of convergence.
Because s D  C j! is a complex number it takes its values over a complex plane. If we look
at the region of convergence in the complex plane it will look like one of the regions shown in
Figure 19.3.
Notice a few things about the ROC. The boundary of the ROC on the left is a vertical
line. The reason for this is that the ROC depends only on the values of . These values are
represented on the horizontal axis. The values of  for which  >  a will be all values in the
semi-infinite plane to the left of the line  D  a. This line is the vertical line that intersects
the  axis at  D  a. We can see from this that the boundaries for the region of convergence
will always be either a vertical line or infinity. Also notice that the ROC in the case where a > 0
includes the j! axis while the ROC for the case a < 0 does not include the j! axis. The j! axis
is the line where  D 0. If we plug  D 0 into the Laplace transform equation we get the Fourier
transform. This means that when the ROC includes the j! axis the equation for the Fourier
transform converges and the Fourier transform exists. If we look back we see this is precisely
true in this case—the Fourier transform exists if a > 0 and does not exist when a < 0. If x.t/
is the impulse response of a linear time invariant system we can draw a further conclusion from
this. Recall that the Fourier transform only exists if the Dirichlet conditions are satisfied. This

19.1. THE LAPLACE TRANSFORM IS UNIQUE – AS LONG AS WE INCLUDE THE ROC
253
σ
σ
a > 0
−a
−a
ROC
ROC
a < 0
jω
jω
Figure 19.3: The region of convergence for the Laplace transform of e atu.t/ for positive and
negative values of a.
means that if the j! axis is included in the region of convergence the Dirichlet conditions are
satisfied. The first Dirichlet condition was that of absolute integrability
Z 1
 1
jx.t/j < 1
But if x.t/ is an impulse response satisfying this condition means that the system is stable.
Therefore, if x.t/ is an impulse response and the region of convergence of its Laplace transform
includes the j! axis, the system for which x.t/ is the impulse response is stable.
Finally, let us look at the limiting condition when a D 0. In this case
x.t/ D e atu.t/ D e0u.t/ D u.t/
and
X.s/ D 1
s
ROC:  > 0
To further understand the importance of stating the region of convergence consider the
function x.t/ D  e atu. t/. For this signal we get the Laplace transform
X.s/ D
1
s C a
 <  a
Notice that the ratio
1
sCa is the same for both e atu.t/ and  eatu. t/. What is different
between the Laplace transforms is the region of convergence. Without the region of convergence
we can’t differentiate between them. Which brings us back to the heading for this section the
Laplace transform is unique—as long as we include the ROC in the transform.

254
19. UNIQUENESS AND LINEARITY OF THE LAPLACE TRANSFORM
19.2
THE LAPLACE TRANSFORM IS A LINEAR
TRANSFORM (BUT BE CAREFUL WITH THE
REGION OF CONVERGENCE)
Just as was the case of the Fourier transform, the Laplace transform is an integral transform and
is, therefore, linear.
L Œ˛x.t/ C ˇy.t/ D ˛L Œx.t/ C ˇL Œy.t/ D X.s/ C Y.s/
ROC: RX \ RY
where RX is the region of convergence corresponding to X.s/ and RY is the region of conver-
gence corresponding to Y.s/. In the equation above we use L Œ to denote the Laplace transform,
˛ and ˇ are constants, and we use lower case letters to represent the time domain functions and
upper case letters to denote their Laplace transforms. Because the Laplace transform of a func-
tion x.t/ can be viewed as the Fourier transform of x.t/e t, many of the properties of the
Fourier transform translate over for the Laplace transform. We will go through and list these
properties later. For now, let’s just go through a few examples.
Example 19.1
Consider the sum of two exponentials
x.t/ D e 2tu.t/ C 2e 3tu.t/
Because of the linearity of the Laplace transform this is simply the sum of the Laplace transforms
of the individual exponentials which we can easily compute as:
L

e 2tu.t/

D
1
s C 2
 >  2
L

e 3tu.t/

D
1
s C 3
 >  3
We can just take the linear combination of these two to find X.s/, however, we have to be a bit
careful with the region of convergence. As you can see the region of convergence for each term
in the sum is different. As the region of convergence for the sum we pick the intersection of the
regions of convergence for each terms as the region of convergence for the sum should be those
values of  for which each term of the sum converges. Thus,
X.s/ D
1
s C 2 C
2
s C 3 D
3s C 7
.s C 2/.s C 3/
 >  2
Example 19.2
Let’s bring in the sinusoids.
x.t/ D e 2t cos.6t/u.t/

19.2. THE LAPLACE TRANSFORM IS A LINEAR TRANSFORM
255
But sinusoids are just exponentials who have met Euler.
x.t/
D
e 2t cos.6t/u.t/
D
e 2t
ej6t C e j 6t
2

u.t/
D
1
2e .2 j6/tu.t/ C 1
2e .2Cj 6/tu.t/
We know that
L

e atu.t/

D
1
s C a
 >  a
We could just replace a with 2   j 6 and 2 C j6, and for the most part we will. The only place
we have to be a bit careful is in the definition of the region of convergence. Recall that  is
the real part of s. So for the region of convergence we only compare  with the real part of the
exponents. The real part of the exponent for both terms is 2, so,
X.s/ D 1
2

1
s C 2   j6 C
1
s C 2 C j 6

 >  2
or
X.s/ D
s C 2
s2 C 4s C 40
 >  2
Example 19.3
Let’s mix it up a bit
x.t/ D 2e 3tu.t/ C e 2t cos.6t/u.t/
Again using the linearity of the Laplace transform
X.s/
D
2
s C 3 C 1
2

1
s C 2   j6 C
1
s C 2 C j 6

ROC: . >  3/ \ . >  2/
D
2
s C 3 C 1
2

2s C 4
s2 C 4s C 40

 >  2
D
2s2 C 8s C 80 C s2 C 5s C 6
.s C 3/.s2 C 4s C 40/
 >  2
D
3s2 C 13s C 86
.s C 3/.s2 C 4s C 40/
 >  2
where we have used the fact that
. >  3/ \ . >  2/ D  >  2

256
19. UNIQUENESS AND LINEARITY OF THE LAPLACE TRANSFORM
to get the region of convergence.
Example 19.4
Let’s do one last signal
x.t/ D e 2jtj
We can write x.t/ as
x.t/ D e 2tu.t/ C e2tu. t/
We know that
L

e 2tu.t/

D
1
s C 2
 >  2
and
L

e2tu. t/

D
1
s   2
 < 2
Combining the two
L
h
e 2jtji
D
1
s C 2 C
1
s   2
  2 <  < 2
or
X.s/ D
2s
.s C 2/.s   2/
  2 <  < 2
19.3
SUMMARY
In this module we have looked at two properties of the Laplace transform.
1. We have seen the importance of the region of convergence in making the Laplace trans-
form unique. If we do not specify the region of convergence the Laplace transform is not
necessarily unique.
2. We have seen the usefulness of the linearity property in the computation of the Laplace
transform.

19.4. EXERCISES
257
19.4
EXERCISES
(Answers on the following page)
Find the Laplace transform including the regions of convergence for the following signals.
1.
x.t/ D e 4t cos.2t/u.t/
2.
x.t/ D e 4t cos.6t/u
3.
x.t/ D e 2t sin.10t/
4.
x.t/ D u.t/
5.
x.t/ D .1 C e t/u.t/
6.
x.t/ D e tu.t/ C e2tu. t/
7.
x.t/ D ı.t/
8.
x.t/ D ı.t   to/
9.
x.t/ D
1
X
nD0
ı.t   nto/
10.
x.t/ D u

t C 1
2

  u

t   1
2


258
19. UNIQUENESS AND LINEARITY OF THE LAPLACE TRANSFORM
19.5
ANSWERS
1.
X.s/ D
s C 4
s2 C 8s C 20
 >  4
2.
X.s/ D
s C 4
s2 C 8s C 52
 >  4
3.
X.s/ D
10
s2 C 4s C 104
4.
X.s/ D 1
s
 > 0
5.
X.s/ D 2s C 1
s2 C s
 > 0
6.
X.s/ D
 3
s2   s   2
  1 <  < 2
7.
X.s/ D 1 8
8.
X.s/ D e sto
8
9.
X.s/ D
1
1   e sto
 > 0
10.
X.s/ D e
s
2   e  s
2
s

259
M O D U L E
20
Laplace Transform – Poles and
Zeros
In this module we introduce the concept of poles and zeros which will
significantly increase the utility of the Laplace transform for us. We will
immediately make use of this concept to show how we can characterize
the transfer function of a system simply by using the location of the poles
and zeros. This will not by any means be the only place we will make use
of poles and zeros. But we are getting ahead of ourselves. Let’s first define
poles and zeros.
Notice that in almost all of the cases we have looked at the Laplace
transform is a ratio of polynomials in s
X.s/ D N.s/
D.s/
Almost the only exception we shall see in our work is the Laplace transform whose region of
convergence was the entire s-plane. This does not mean that all Laplace transforms are ratios
of polynomials. It’s just that most of the Laplace transforms we are interested in are ratios of
polynomials.
We can write the polynomials N.s/ and D.s/ as a product of factors and a gain term in
the following manner
X.s/ D N.s/
D.s/ D A .s   z1/.s   z2/ : : : .s   zm/
.s   p1/.s   p2/ : : : .s   pn/
Assuming that X.s/ is the transfer function of a linear time invariant system the roots of the
numerator polynomial fzig are called the zeros of the system. The name is very appropriate
because whenever s D zi for any i, the transfer function X.s/ goes to zero. Zeros are represented
in the s-plane with circles. The roots of the denominator polynomial fpig are called the poles of
the system and these are the values at which the value of the transfer function goes to infinity.
Poles are represented in the s-plane by crosses.
Let’s look at the pole-zero diagrams of the various examples we have introduced previ-
ously.

260
20. LAPLACE TRANSFORM – POLES AND ZEROS
jω
σ
−3
Region of
convergence
Figure 20.1: Pole zero pattern for e 3tu.t/.
Example 20.1
Let’s begin with the simplest example
x.t/ D e 3tu.t/
The Laplace transform is given by
X.s/ D
1
s C 3
 >  3
There is only one pole and no zeros as can be seen from the pole-zero diagram shown in Fig-
ure 20.1. The pole is at  3.
Notice that the function of time is nonzero to the right of a point—the origin in this
case—and the region of convergence is right-sided—to the right of a boundary. Furthermore
the boundary is defined by the pole.
Example 20.2
If we now consider the left-sided version of this
x.t/ D  e 3tu. t/
The Laplace transform is given by
X.s/ D
1
s C 3
 <  3
We obtain the same pole-zero pattern but the region of convergence is left sided with the bound-
ary being defined by the pole (Figure 20.2).
The correspondence between the “sidedness” of the time function and the region of con-
vergence is something we will see over and over again.
Example 20.3
Consider the case where we had the sum of two exponentials
x.t/ D e 2tu.t/ C 2e 3tu.t/

20. LAPLACE TRANSFORM – POLES AND ZEROS
261
jω
σ
−3
Region of
convergence
Figure 20.2: Pole zero pattern for  e 3tu. t/.
jω
σ
−3
−2
Region of
convergence
Figure 20.3: Pole zero pattern for e 2tu.t/ C 2e 3tu.t/.
The Laplace transform is given by
X.s/ D
1
s C 2 C
2
s C 3 D
3s C 7
.s C 2/.s C 3/
 >  2
There are two poles, at  2 and  3, and a zero at  7=3. We plot the poles and zero as shown in
Figure 20.3.
Notice that as expected the region of convergence is right-sided. Furthermore, the right-
most pole defines the boundary of the region of convergence. This makes sense. If the boundary
was to the left of the rightmost pole the region of convergence would include the pole where we
know that the Laplace transform does not converge. Finally, notice that the poles are both on
the real axis. As the poles match up with the exponents of the two component exponentials and
the exponents are both real it makes sense that the poles would be on the real axis.
Example 20.4
Let’s look at the next example which was a damped sinusoid.
x.t/ D e 2t cos.6t/u.t/

262
20. LAPLACE TRANSFORM – POLES AND ZEROS
jω
σ
−6j
6j
−2
Region of
convergence
Figure 20.4: Pole zero pattern for e 2t cos.6t/u.t/.
The Laplace transform is given by
X.s/ D
s C 2
s2 C 4s C 40
 >  2
which can be written as
X.s/ D
s C 2
.s C 2   j6/.s C 2 C j6/
to make explicit the fact that there is a zero at s D  2 and poles at s D  2   j 6 and s D  2 C
j 6. The pole-zero pattern is shown in Figure 20.4.
Notice again that the region of convergence is right-sided and is bounded by the right-
most poles. Unlike the previous case the poles are complex. We will see that this is always the
case when we have oscillations in the impulse response. The oscillations correspond to sinusoids
which are represented by complex exponentials—hence the complex poles.
Example 20.5
The next example contains a decaying exponential and a damped sinusoid.
x.t/ D 2e 3tu.t/ C e 2t cos.6t/u.t/
The Laplace transform is given by
X.s/ D
3s2 C 13s C 86
.s C 3/.s2 C 4s C 40/
 >  2
The pole-zero pattern is shown in Figure 20.5. As before a right-sided function results in
a right-sided region of convergence, and the region of convergence is bounded on the left by the
rightmost poles.

20. LAPLACE TRANSFORM – POLES AND ZEROS
263
jω
σ
−6j
6j
−2
−3
Region of
convergence
Figure 20.5: Pole zero pattern for 2e 3tu.t/ C e 2t cos.6t/u.t/.
jω
σ
−2
2
Region of
convergence
Figure 20.6: Pole zero pattern for e 2jtj.
Example 20.6
Our final example is the Laplace transform of the signal
x.t/ D e 2jtj
which is
X.s/ D
2s
.s C 2/.s   2/
  2 <  < 2
The pole-zero pattern is shown in Figure 20.6.
In this case unlike the previous cases the time domain signal is two-sided. The region of
convergence becomes the intersection of a left-sided and a right-sided region of convergence
resulting in the bounded region of convergence shown in Figure 20.6.
In all of these examples, the region of convergence is some subset of the s-plane. Are there
functions with regions of convergence that are the entire s-plane? To see the answer recall that

264
20. LAPLACE TRANSFORM – POLES AND ZEROS
the region of convergence consists of those values of  for which the integral of jx.t/e tj is
finite. If x.t/ is a finite valued function which is nonzero only over a finite interval, the integral
of jx.t/e tj will be finite for all finite values of ; and, therefore, the Laplace transform will
have a region of convergence which is the entire s-plane.
Example 20.7
Consider the function
x.t/ D e 2t .u.t/   u.t   1//
The Laplace transform X.s/ is given by
X.s/
D
Z 1
 1
e 2t.u.t/   u.t   1//e stdt
D
Z 1
0
e .sC2/tdt
D
 1
s C 2e .sC2/tj1
0
D
1   e .sC2/
s C 2
There are no infinite limits so we do not need to put restrictions on  which means that the
transform exists over the entire s-plane. In other words the region of convergence is the entire s-
plane. When we look at the Laplace transform though there appears to be a fly in the ointment.
We have said that the region of convergence cannot contain any poles and it looks like there
is a pole at s D  2. But is there really a pole at s D  2? We have been dealing with rational
polynomials in which the roots of the denominator are the places where the function goes to
infinity, and hence the roots of the denominator are the poles. The X.s/ obtained here, however,
is not a rational polynomial, so to check if s D  2 is indeed a pole we need to evaluate X.s/ at
s D  2
X.s/js D  2 D 1   e . 2C2/
 2 C 2
D 0
0
Applying L’Hopitals’s rule we get
e .sC2/
1
jsD 2 D 1
Therefore, s D  2 is not a pole; and the region of convergence is indeed the entire s-plane.
So to recap, here are the rules about the regions of convergence we can deduce from our
examples:

20.1. FREQUENCY RESPONSE
265
1. If x.t/ is right sided, the region of convergence is to the right of a boundary. The boundary
is determined by the right-most pole.
2. If x.t/ is left sided, the region of convergence is to the left of a boundary. The boundary is
determined by the left-most pole.
3. if x.t/ is two sided, the region of convergence is a strip bounded on both sides by bound-
aries determined by poles.
4. The region of convergence cannot contain any poles.
5. If x.t/ is of finite duration and is absolutely integrable then the region of convergence is
the entire plane.
20.1
FREQUENCY RESPONSE
The Laplace transform is a generalization of the Fourier transform. If the region of convergence
of the Laplace transform includes the j! axis we can obtain the Fourier transform by evaluating
the Laplace transform along the j! axis. So if we have a simple linear system with impulse
response h.t/ D e atu.t/. The corresponding Laplace function is
H.s/ D
1
s C aI
 >  a
If a > 0 then the ȷ! axis is included in the region of convergence. So we can obtain the Fourier
transform by simply setting  D 0 in the Laplace transform.
H.s/
D
1
s C aI
 >  a
D
1
 C j! C aI
 >  a
H.!/
D
H.s/ jD0 D
1
j! C a
which, you may recall, is the Fourier transform of e atu.t/. Notice that this only works if  D 0
is in the region of convergence which is true for  a < 0.
The nice thing about the Laplace transform is that simply by looking at the location of
the poles and zeros we can get a fair idea of what the transfer function looks like. Here we will
just look at the magnitude of the transfer function but we can get a fair idea of the phase from
the pole zero locations as well. As our example, let’s use the one pole system above with the pole
at a D  0:5. Given that
jH.s/j D
1
js   0:5j

266
20. LAPLACE TRANSFORM – POLES AND ZEROS
jω
σ
σ = 0
–5
–4
–3
–2
–1
1
3
2
1
–1
–2
–3
2
3
4
5
jω
σ
σ = 0
–5
–4
–3
–2
–1
1
3
2
1
–1
–2
–3
2
3
4
5
jω
σ
σ = 0
–5
–4
–3
–2
–1
1
3
2
1
–1
–2
–3
2
3
4
5
jω
σ
σ = 0
–5
–4
–3
–2
–1
1
3
2
1
–1
–2
–3
2
3
4
5
Figure 20.7: Time response and frequency response of a system with a single pole at s D 0:5.
To evaluate the transfer function we have to stay on the j! axis. As we move up and down the
j! axis the magnitude of the transfer function will be 1 divided by the distance between where
we are on the axis and the pole at s D 0:5. The situation for four conditions ! D  3, ! D  2,
! D  1, and ! D 0 is shown in Figure 20.7.
When ! D  3 the distance to the pole is the length of the line connecting the point
! D  3 to the pole at s D  0:5. We can calculate this using Pythogarus’ theorem to be about
3.04. The reciprocal of that is about 0.33. Therefore, jH. 3/j D 0:33. Similarly we can cal-
culate jH. 2/j D 1=
p
.4 C 0:25/ D 0:48, jH. 1/j D 1=
p
.1 C 0:25/ D 0:89 and jH.0/j D 2. If
we now proceed up the j! axis we will encounter these values in reverse—jH.1/j D 1=
p
.1 C
0:25/ D 0:89, jH.2/j D 1=
p
.4 C 0:25/ D 0:48, and jH.3/j D 0:33. Notice that the magnitude
is even as we would expect for the Fourier transform of a real valued function.
The time response and the magnitude of the transfer function for this system are shown
in Figure 20.8.

20.1. FREQUENCY RESPONSE
267
'Impulse response with a= '0.50
Transfer function with a= 0.50
0
1
2
3
4
5
6
7
8
1.0
0.8
0.6
0.4
0.2
0.0
2.0
1.5
1.0
0.5
0.0
0
–1
–2
–3
–4
1
2
3
4
Figure 20.8: Time response and frequency response of a system with a single pole at s D 0:5.
By moving the pole around we can change the time and frequency response of this sys-
tem. Pick a D 2 that is, move the pole from  0:5 to  2, and we get the responses shown in
Figure 20.9.
The reason we can evaluate the Laplace transform on the j! axis is that in this case the
j! axis is included in the region of convergence. As we move up and down the j! axis we move
closer and farther away from the pole. The closest we get to the pole is at ! D 0. As the pole is
a root of the denominator the closer we approach it the smaller the denominator becomes and
hence the greater the magnitude. We can see in this for both values of the a. If we bring the
pole closer to the j! axis we enhance this behavior and the increase in the amplitude of the
magnitude of the transfer function is greater for a D 0:5 than it is for a D 2.
Consider a different example with the pole-zero pattern shown in Figure 20.10.
There is a zero at 0 and poles at  2 ˙ j12. Let’s move along the j! axis in the positive
direction. At ! D 0 we have a zero, so the transfer function goes to zero at ! D 0. As we move
further in the positive direction we come closer to the pole located at  2 C j12. We would never
actually reach the pole as it is off the j! axis. Instead the transfer function peaks as ! approaches
12 and decays again as we move away from the pole. The magnitude of the transfer function is
plotted in Figure 20.11.
You can see how by placing poles and zeros we can obtain a desired response. Once we
have the desired response we can use the pole-zero patterns to obtain the transfer function as a
function of s. For example, for the case we have been looking at it is easy to see that the transfer

268
20. LAPLACE TRANSFORM – POLES AND ZEROS
'Impulse response with a= '2.00
Transfer function with a= 2.00
0
1
2
3
4
5
6
7
8
1.0
0.8
0.6
0.4
0.2
0.0
0.50
0.45
0.40
0.35
0.30
0.25
0.20
0
–1
–2
–3
–4
1
2
3
4
Figure 20.9: Time response and frequency response of a system with a single pole at s D 2:0.
jω
σ
−12j
12j
−2
Region of
convergence
Figure 20.10: Different example with the pole-zero pattern.

20.2. SUMMARY
269
Frequency (radians)
H( f )
0.30
0.25
0.20
0.15
0.10
0.05
0.00
–0.05
–0.10
–20
–15
–10
–5
0
5
10
15
20
Figure 20.11: The magnitude of the transfer function.
function is
H.s/ D
s
.s C 2 C j12/.s C 2   j12/ D
s
s2 C 4s C 148
From the transfer function we can obtain different hardware realizations using some well known
algorithmic approaches.
20.2
SUMMARY
In this module we learned that:
•
The Laplace transform of most of the functions we are interested in are ratios of poly-
nomials in s.
•
The roots of the numerator polynomial are called the zeros of the system.
•
The roots of the denominator polynomials are called the poles of the system.
•
The region of convergence is bounded by vertical lines through the poles and possibly
infinity.
•
The poles and zeros can tell us a lot about the frequency response of those systems that
have a Fourier transform.
•
We can design simple filters using pole zero placement.

270
20. LAPLACE TRANSFORM – POLES AND ZEROS
20.3
EXERCISES
(Answers on the following page)
1. Given a system with a right-sided impulse response and poles at  2;  1, what is the region
of convergence.
2. Given a system with a right-sided impulse response and poles at  2; 2, what is the region
of convergence.
3. Given a system with a left-sided impulse response and poles at  2;  1, what is the region
of convergence.
4. Given a system with a left-sided impulse response and poles at  2; 2, what is the region
of convergence.
5. Given a system with a two-sided impulse response and poles at  2;  1, what is the region
of convergence.
6. You have a system with a two-sided impulse response and poles at  2;  1. Is this system
stable?
7. You have a system with a two-sided impulse response and poles at  2; 2. Is this system
stable?
8. You have a system with a right-sided impulse response and poles at  2; 2. Is this system
stable?
9. By inspection what are the pole locations of the function x.t/ D e 5tu.t/?
10. By inspection what are the pole locations of the function x.t/ D e5tu.t/?
11. By inspection what are the pole locations of the function x.t/ D e 5tu. t/?
12. By inspection what are the pole locations of the function x.t/ D e5tu. t/?
13. By inspection what are the pole locations of the function x.t/ D e 5t cos.4t/u.t/?

20.4. ANSWERS
271
20.4
ANSWERS
1.  >  1
2.  > 2
3.  <  2
4.  <  2
5.  2 <  <  1
6. No
7. Yes
8. No
9.  5
10. 5
11.  5
12. 5
13.  5 ˙ j 4


273
M O D U L E
21
The Inverse Laplace Transform
Unlike our discussion of the Fourier transform, for the Laplace transform
you might have noticed that we have focused exclusively on the forward
transform and made no mention of the inverse transform. Is that because
the inverse transform is difficult or because it is easy? The answer is Yes.
The integral equation to find the inverse Laplace transform is given by
x.t/ D
1
2j
Z Cj 1
 j 1
X.s/estds
where  is in the region of convergence and s D  C ȷ!. Clearly not a
trivial task for most cases. However, recall that in practice the kinds of Laplace transforms we
are most interested in are rational polynomials.
X.s/ D N.s/
D.s/ D A .s   z1/.s   z2/ : : : .s   zm/
.s   p1/.s   p2/ : : : .s   pn/
We will see that it is really easy to find the inverse Laplace transform of rational polynomials in
s using the uniqueness property of the Laplace transform. So let’s begin.
If for a given X.s/ the degree of the numerator polynomial N.s/ is less than the degree
of the denominator polynomial D.s/ we can always expand X.s/ in terms of the factors of D.s/
using partial fraction expansion. If the degree of the numerator is greater than or equal to the
degree of the denominator then we can simply divide the numerator with the denominator and
we will end up with a polynomial in s and a ratio of polynomials in which the degree of the
denominator is greater than the degree of the numerator. Let’s assume for the moment that the
degree of the denominator is greater than the degree of the numerator. We will deal later with
the case where this is not true. In this module we will deal with the case where the denominator
polynomial D.s/ has distinct roots which may be real or complex. The case where the roots are
repeated is also easy to deal with but we need an additional property of the Laplace transform so
we will hold of on that for the moment. If the denominator polynomial only has distinct roots
pi then we can write X.s/ as
X.s/ D
n
X
iD1
Ai
s   pi

274
21. THE INVERSE LAPLACE TRANSFORM
As in the case of the forward Laplace transform, the inverse Laplace transform is a linear trans-
form so the inverse transform of a sum is the sum of the inverse transforms and
L 1 ŒX.s/ D L 1
" n
X
iD1
Ai
s   pi
#
D
n
X
iD1
AiL 1

1
s   pi

We have previously seen the following Laplace transforms:
L

e atu.t/

D
1
s C a
 >  a
L

 e atu. t/

D
1
s C a
 <  a
So we can take the inverse Laplace transform of the individual terms and then use the
linearity and uniqueness of the Laplace transform (being very careful with the regions of con-
vergence) to add together the individual sequences and determine the overall inverse Laplace
transform.
Let’s take a look at some examples.
Example 21.1
Find the inverse Laplace transform of
X.s/ D
3s C 5
s2 C 3s C 2
 >  1
The first thing we do is to write the denominator polynomial in terms of its factors.
X.s/
D
3s C 5
s2 C 3s C 2
D
3s C 5
.s C 2/.s C 1/
Then we use partial fraction expansion
3s C 5
.s C 2/.s C 1/ D
A1
s C 2 C
A2
s C 1
Let me show you how I do partial fraction expansion. There are a number of techniques
that people use. Whichever one you use is fine.
To find A1 multiply both sides by .s C 2/ and evaluate at s D  2
3s C 5
.s C 2/.s C 1/.s C 2/jsD 2 D A1 C
A2
.s C 1/.s C 2/jsD 2
then
A1 D 3. 2/ C 5
 2 C 1
D 1

21. THE INVERSE LAPLACE TRANSFORM
275
To find A2 multiply both sides by .s C 1/ and evaluate at s D  1
3s C 5
.s C 2/.s C 1/.s C 1/jsD 1 D
A1
s C 2.s C 1/jsD 1 C A2
or
A2 D 3. 1/ C 5
 1 C 2
D 2
So now our problem becomes one of finding the inverse Laplace transform of
1
s C 2 C
2
s C 1
 >  1
Based on our previous experience the inverse Laplace transform of
1
s C 2
 >  1
is e 2tu.t/ and the inverse Laplace transform of
2
s C 1
 >  1
is 2e tu.t/. Therefore,
x.t/ D e 2tu.t/ C 2e tu.t/
Let’s try a different example with the same rational polynomial but a different region of
convergence.
Example 21.2
Find the inverse Laplace transform of
X.s/ D
3s C 5
s2 C 3s C 2
 <  2
We start out in exactly the same way writing the denominator polynomial in terms of its factors
and then expanding using partial fraction expansion. Then we need to find the inverse Laplace
transform of
1
s C 2 C
2
s C 1
 <  2
The inverse Laplace transform of
1
s C 2
 <  2

276
21. THE INVERSE LAPLACE TRANSFORM
is  e 2tu. t/. What about the inverse Laplace transform of
2
s C 1
 <  2
We know what the inverse Laplace transform of 1=.s C 1/ is when the region of convergence is
 <  1, but here the region of convergence is  <  2. If we look at these two regions we can
see that a transform that converges for  <  1 will also converge for  <  2 as  2 is less than
 1. So, in this case, the inverse transform for the region of convergence  <  2 is the same as
the inverse transform for the region of convergence  <  1 which is  2e tu. t/. Therefore,
the inverse Laplace transform if the ROC is  <  2
x.t/ D  e 2tu. t/   2e tu. t/
But what if we have a two sided region of convergence?
Example 21.3
Find the inverse Laplace transform of
X.s/ D
3s C 5
s2 C 3s C 2
  2 <  <  1
Again we start out in exactly the same way writing the denominator polynomial in terms of its
factors and then expanding using partial fraction expansion. Then we need to find the inverse
Laplace transform of
1
s C 2 C
2
s C 1
  2 <  <  1
We again find the inverse transform of each term separately. First lets find the inverse transform
of
1
s C 2
  2 <  <  1
But the only pole we have here is at s D  2. We only have two regions of convergence  >  2
and  <  2. The inverse we get for the region  <  2 would not be appropriate as this transform
would not converge in the region  2 <  <  1. However the inverse we get for the region
 >  2 would converge in the region  2 <  <  1. So, the inverse of this term will be e 2tu.t/.
For the second term
2
s C 1
  2 <  <  1
we use a similar argument. In this case the only pole is at s D  1 and, therefore, the two regions
of convergence are  <  1 and  >  1. Anything that converges for  <  1 will also converge
for  2 <  <  1, so we use the region of convergence  <  1 and the inverse transform for

21. THE INVERSE LAPLACE TRANSFORM
277
this term is the left sided function  2e tu. t/. And the inverse Laplace transform for this case
is
x.t/ D e 2tu.t/   2e tu. t/
If the roots were complex instead of real nothing really changes.
Example 21.4
Find the inverse Laplace transform of
X.s/ D
s C 3
s2 C 6s C 45
 >  3
The first step is to find the roots of the denominator polynomial in order to find the factors. The
roots of the denominator polynomial are
p1;2 D  6 ˙
p
36   180
2
D  6 ˙ j12
2
D  3 ˙ j6
or
X.s/ D
s C 3
.s C 3   j6/.s C 3 C j6/
 >  3
We can expand this using partial fraction expansion to obtain
X.s/ D
1=2
.s C 3   j 6/ C
1=2
.s C 3 C j 6/
 >  3
Again these are familiar forms. The inverse transform of
1=2
.s C 3   j 6/
 >  3
is .1=2/e .3Cj6/tu.t/ and the inverse transform of
1=2
.s C 3 C j6/
 >  3
is .1=2/e .3 j6/tu.t/. Putting them together we get
x.t/
D
1
2e .3Cj6/tu.t/ C 1
2e .3 j 6/tu.t/
D
e 3tu.t/
1
2e j 6t C 1
2ej6t

D
e 3t cos.6t/u.t/
We still haven’t done anything with repeated roots or with exponential terms. Let’s wait
until after we have seen some properties of the Laplace transform for that.

278
21. THE INVERSE LAPLACE TRANSFORM
21.1
SUMMARY
In order to find the inverse Laplace transform of a rational polynomial X.s/ with distinct roots
of the denominator polynomial and a region of convergence R.
1. Find the partial fraction expansion of X.s/
X.s/ D
n
X
iD1
Ai
s   pi
2. For each term determine if the region of convergence R is to the right or left of pi.
(a) If R is to the right of pi then
L 1

Ai
s   pi

D Aiepitu.t/
(b) If R is to the left of pi then
L 1

Ai
s   pi

D  Aiepitu. t/
3. The inverse transform x.t/ is the sum of the inverse transform of each term in the sum-
mation.

21.2. EXERCISES
279
21.2
EXERCISES
(Answers on the following page)
1. Find the inverse Laplace transform of
X.s/ D
2s
s2   1
 > 1
2. Find the inverse Laplace transform of
X.s/ D
2s
s2 C 1
 > 0
3. Find the inverse Laplace transform of
X.s/ D
2s
s2   1
 <  1
4. Find the inverse Laplace transform of
X.s/ D
s C 5
s2 C 10s C 29
 >  5
5. Find the inverse Laplace transform of
X.s/ D
2s C 6
s2 C 6s C 10
 >  3
6. Find the inverse Laplace transform of
X.s/ D
3s C 8
s2 C 5s C 6
 >  2
7. Find the inverse Laplace transform of
X.s/ D
3s C 8
s2 C 5s C 6
  3 <  <  2
8. Find the inverse Laplace transform of
X.s/ D
2s
s2   1
  1 <  < 1
9. Find the inverse Laplace transform of
X.s/ D
3s C 6
s2 C 4s C 13
 >  2
10. Find the inverse Laplace transform of
X.s/ D
 2
s2   1
  1 <  < 1

280
21. THE INVERSE LAPLACE TRANSFORM
21.3
ANSWERS
1.
x.t/ D e tu.t/ C etu.t/
2.
x.t/ D 2 cos.t/u.t/
3.
x.t/ D  e tu. t/   etu. t/
4.
x.t/ D e 5t cos.2t/u.t/
5.
x.t/ D 2e 3t cos.t/u.t/
6.
x.t/ D e 3tu.t/ C 2e 2tu.t/
7.
x.t/ D e 3tu.t/   2e 2tu. t/
8.
x.t/ D e tu.t/   etu. t/
9.
x.t/ D 3e 2t cos.3t/u.t/
10.
x.t/ D e jtj

281
M O D U L E
22
Properties of Laplace
Transforms
Now that we have some feel for the Laplace transform and the importance
of the regions of convergence let us go back as we promised and look at
some of the properties of the Laplace transform. As we had said earlier
these are essentially the same properties as those of the Fourier transform
with the addition of regions of convergence.
22.1
CONVOLUTION PROPERTY
As in the case of the Fourier transform, the Laplace transform of the con-
volution of two signals in the time domain is the product of the Laplace
transform of each signal. For the product to converge, each of the trans-
forms has to converge and, therefore, the region of convergence of the product is a region which
contains the intersection of the region of convergence of the Laplace transform of each signal.
L fx1.t/ ~ x2.t/g D X1.s/X2.s/ ROC W containing R1 \ R2
where R1 is the region of convergence for X1.s/ and R2 is the region of convergence for X2.s/.
The proof of the convolution property for Laplace transform follows the same steps as the
proof of the convolution property for Fourier transforms.
x1.t/ ~ x2.t/
D
Z 1
 1
x1./x2.t   /d
L fx1.t/ ~ x2.t/g
D
Z 1
 1
Z 1
 1
x1./x2.t   /de stdt
substituting u D t    we get
L fx1.t/ ~ x2.t/g
D
Z 1
 1
Z 1
 1
x1./x2.u/e s.Cu/ddu
D
Z 1
 1
Z 1
 1
x1./e sd

x2.u/e sudu
D
X1.s/X2.s/

282
22. PROPERTIES OF LAPLACE TRANSFORMS
Notice that the region of convergence is one containing R1 \ R2. Why “containing?” Con-
sider the following example:
Example 22.1
x1.t/
D
e 2tu.t/   e 3tu.t/
x2.t/
D
3e 5tu.t/   2e 4tu.t/
The Laplace transforms are given by
X1.s/
D
1
s C 2  1
s C 3
ROC W
 >  2
D
1
.s C 2/.s C 3/
ROC W
 >  2
X2.s/
D
3
s C 5  2
s C 4
ROC W
 >  4
D
s C 2
.s C 4/.s C 5/
ROC W
 >  4
where the regions of convergence are determined by the rightmost poles. The intersection of
the two regions would be the region >  2. However, when we multiply X1.s/ and X2.s/, the
.s C 2/ factor cancels out; and the rightmost pole is the pole at s D  3.
X1.s/X2.s/ D
1
.s C 2/.s C 3/ 
s C 2
.s C 4/.s C 5/ D
1
.s C 3/.s C 4/.s C 5/
ROC W  >  3
This pole zero cancellation is what causes the “containing” formulation—a pole that is restricting
the region of convergence in one of the Laplace transforms may get cancelled out by a zero in
the other Laplace transform.
As in the case of Fourier transforms, the convolution property allows us to relate the input
and output of linear time-invariant systems through the transfer function which, in this case,
is the Laplace transform of the impulse response. Given a linear time invariant system with
impulse response h.t/, the output y.t/ is given by the convolution of the input x.t/ with the
impulse response.
y.t/ D x.t/ ~ h.t/
Hence,
Y.s/ D X.s/H.s/
where Y.s/ is the Laplace transform of the output, X.s/ is the Laplace transform of the input
and H.s/ is the Laplace transform of the impulse response.

22.2. TIME SHIFTING
283
22.2
TIME SHIFTING
A shift in the time domain, as in the case of the Fourier transform, results in the Laplace trans-
form of the shifted signal being a product of the Laplace transform of the original signal and a
complex exponential. The difference is that the complex exponential instead of being e j!to is
e sto.
L fx.t   to/g D e stoX.s/ ROC W R
where R is the region of convergence for X.s/. Notice that the region of convergence did not
change. The reason for this is because the multiplication with e sto does not introduce any
new poles or change the location of the poles in the original transform. As poles determine the
boundaries of the region of convergence no change in the pole locations means no change in the
boundaries of the region of convergence.
The proof is quite simple.
L Œx.t   to/ D
Z 1
 1
x.t   to/e stdt
Substitute  D t   to which also means that t D  C to into this equation and we get
L Œx.t   to/
D
Z 1
 1
x./e s.Cto/d
D
Z 1
 1
x./e se stod
D
e sto
Z 1
 1
x./e sd
D
e stoX.s/
In the last module we had looked at how to find the inverse Laplace transform when
X.s/ is a rational polynomial. Using this property we can extend our ability to find the Laplace
transform to situations where we have complex exponents as well.
Example 22.2
Let’s find the inverse Laplace transform of
X.s/ D 2se s C 4e s
s2 C 4s C 40 I
 >  2
We can rewrite this as
X.s/ D X1.s/e sI
 >  2
where
X1.s/ D
2s C 4
s2 C 4s C 40I
 >  2

284
22. PROPERTIES OF LAPLACE TRANSFORMS
We can go through our standard routine to find the inverse Laplace transform of X1.s/ and then
use the time shifting property to find the inverse Laplace transform of X.s/. First we find the
roots of the denominator.
X1.s/ D
2s C 4
.s C 2   j6/.s C 2 C j6/I
 >  2
Then we expand X1.s/ using partial fraction expansion.
X1.s/ D
1
s C 2   j6 C
1
s C 2 C j 6I
 >  2
Taking the inverse transform of each term and then adding them together we get
x1.t/ D 2e 2t cos.6t/u.t/
Because X.s/ D X1.s/e s we can use the time shifting property to find x.t/ as
x.t/ D x1.t   1/ D 2e 2.t 1/ cos.6.t   1//u.t   1/
22.3
SHIFTING IN THE S-DOMAIN
Just as multiplication with a complex exponential in Laplace domain results in a shift in the time
domain, multiplication with a complex exponential in the time domain results in a shift in the
Laplace domain.
L
˚
esotx.t/
	
D X.s   so/ ROC W R C o
where o is the real part of so. To show this let’s evaluate the Laplace transform integral
L
˚
esotx.t/
	
D
Z 1
 1
esotx.t/e stdt
D
Z 1
 1
x.t/e .s so/tdt
We can see that if this integral converges it will result in X.s   so/. Let’s see the conditions
under which it converges. The Laplace transform of x.t/ converges for s 2 R, therefore this
integral will converge for s   so 2 R. We know the region of convergence is going to be one of
four possible regions
All of the s-plane
 > ˛
for some real ˛
 < ˛
for some real ˛
˛ <  < ˇ
for some real ˛ and ˇ

22.3. SHIFTING IN THE S-DOMAIN
285
To find the region of convergence for the Laplace transform of esotx.t/ all we need to do is
replace  with    o. This will change the region of convergence
All of the s-plane
)
All of the s-plane
 > ˛
for some real ˛
)
 > ˛ C o
for some real ˛
 < ˛
for some real ˛
)
 < ˛ C o
for some real ˛
˛ <  < ˇ
for some real ˛ and ˇ
)
˛ C o <  < ˇ C o
for some real ˛ and ˇ
To write it more concisely, the new region of convergence will be R C o.
Multiplication with an exponential causes a shift in the s-domain corresponding to the
real part of the exponent. This results in a movement of the poles and zeros in the s-plane. The
movement can take a stable system and make it unstable or vice versa.
Example 22.3
Consider the time function
x.t/ D e 2tu.t/
with Laplace transform
X.s/ D
1
s C 2
 >  2
If x.t/ is the impulse response of a linear time invariant system you can clearly see that the
system is stable by looking at the time domain response (a decaying exponential) or the s-domain
function (the region of convergence includes the j! axis). If we multiply x.t/ by e3t by this
property the Laplace transform will be
X.s   3/ D
1
s   3 C 2 D
1
s   1
 > 1
The pole shifts from  2 to 1 as does the boundary of the region of convergence. The region of
convergence no longer includes the j! axis so the system is no longer stable. We could also see
this in the time domain
e3tx.t/ D e3te 2tu.t/ D etu.t/
which is a growing exponential.
We can also go the other way. If we have a system which is not stable—say a causal system
whose transfer function contains poles in the right half s-plane, we can use this property to shift
the pole over the j! axis into the left half s-plane and thus make the system stable. We will
come back to this idea of shifting in the s-domain when we discuss feedback systems.

286
22. PROPERTIES OF LAPLACE TRANSFORMS
22.4
TIME SCALING
Scaling in the case of the Laplace transform works exactly the same as scaling in the case of
the Fourier transform with the additional complication of the region of convergence. Let’s start
with the simple case of the Laplace transform of x.˛t/ where ˛ > 0.
L Œx.˛t/ D
Z 1
 1
x.˛t/e stdt
Let’s define  D ˛t. As ˛ > 0 this won’t effect the limits—˛  1 D 1—and d D ˛dt. Sub-
stituting these into the integral we get
L Œx.˛t/
D
Z 1
 1
x./e s 
˛ 1
˛ d
D
1
˛
Z 1
 1
x./e  s
˛ d
If this equation converges the result will be X. s
˛/ where X.s/ is the Laplace transform of x.t/. If
the region of convergence for X.s/ was  > a the region of convergence for X. s
˛/ will be s
˛ > a
or  > ˛a. Similarly if the region of convergence for X.s/ was  < a the region of convergence
for X. s
˛/ will be  < ˛a. Finally, if b <  < a is the region of convergence for X.s/, the region
of convergence for X. s
˛/ is ˛b <  < ˛a. But this was all for ˛ > 0. What happens if ˛ < 0?
First our variable substitution doesn’t go as smoothly as before. When we substitute  D ˛t
this effects the limits. The lower limit for the integral with respect to t was  1. With respect
to , because t is being multiplied by a negative number the lower limit will be C1. Similarly
the upper limit will go from being C1 to  1. We can flip the limits back to the original form
by multiplying the integral with negative one. So the transform will now be
L Œx.˛t/
D
Z  1
1
x./e s 
˛ 1
˛ d
D
  1
˛
Z 1
 1
x./e  s
˛ d
Noting that
1
j˛j D
8
ˆ<
ˆ:
1
˛
for ˛ > 0
  1
˛
for ˛ < 0
We can combine the case for ˛ > 0 and ˛ < 0 into
L Œx.˛t/ D 1
j˛j
Z 1
 1
x./e  s
˛ d
which is equal to
1
j˛jX. s
˛/ when the integral converges.

22.4. TIME SCALING
287
The region of convergence when ˛ < 0 will also change from the case when ˛ > 0. Where
X.s/ converged for  > a, X. s
˛/ will converge for  < ˛a and where X.s/ converged for  < a,
X. s
˛/ will converge for  > ˛a. We put all this together and simply say that
L fx.˛t/g D 1
j˛jX
 s
˛

ROC W ˛R
where R is the region of convergence for X.s/.
Scaling will also effect pole zero locations. If s D a was a pole or zero for X.s/, s D ˛a
will be the pole or zero for X. s
˛/. This effect is particularly drastic when ˛ is negative because it
flips the poles and zeros from the right half plane to the left half plane and vice versa.
Example 22.4
Suppose x.t/ is a function with Laplace transform
X.s/ D
s C 1
.s C 2/.s C 4/I  >  2
Clearly if x.t/ is the impulse response of a system the system is both causal and stable. Let us
take a look at what happens when we scale x.t/ with 1=2 and  1=2.
L fx.t=2/g
D
2X.2s/I  >  1
D
2
2s C 1
.2s C 2/.2s C 4/I  >  1
D
s C 1=2
.s C 1/.s C 2/I  >  1
The system with impulse response x.t=2/ would still be a causal stable system albeit with poles
and zero closer to the j! axis. When we scale by  1=2 the situation changes
L fx. t=2/g
D
2X. 2s/I  < 1
D
2
 2s C 1
. 2s C 2/. 2s C 4/I  < 1
D
 s   1=2
.s   1/.s   2/I  < 1
The system with impulse response x. t=2/ is still stable as the j! axis is still in the region of
convergence but it is no longer causal.

288
22. PROPERTIES OF LAPLACE TRANSFORMS
22.5
DIFFERENTIATION IN THE TIME DOMAIN
To find the Laplace transform of the derivative of a function x.t/ with Laplace transform X.s/
with region of convergence R is relatively simple (assuming all derivatives and integrals involved
exist). We begin with the formal definition of the inverse Laplace transform.
x.t/ D
1
2j
Z Cj 1
 j 1
X.s/estds
Taking the derivative with respect to time of both sides
d
dt x.t/
D
d
dt
"
1
2j
Z Cj 1
 j 1
X.s/estds
#
D
1
2j
Z Cj 1
 j 1
X.s/ d
dt estds
D
1
2j
Z Cj 1
 j 1
sX.s/estds
or in other words,
L
dx.t/
dt

D sX.s/ ROC W containing R
but why “containing” R?
This property is widely used when solving differential equations. Differential equations
have been a common way of representing the dynamics of systems. The reason for that word is
that if X.s/ happened to have a pole at 0 which was a boundary of the region of convergence the
additional factor of s in the numerator might cancel that pole out resulting in a bigger region of
convergence which contains the original region of convergence.
Example 22.5
Consider the continuous-time linear time-invariant system with impulse re-
sponse h.t/ and transfer function H.s/ for which the input x.t/ and the output y.t/ are related
by the differential equation
d 2y.t/
dt2
C 2dy.t/
dt
  8y.t/ D 2dx.t/
dt
C 5x.t/
Let’s assume that the initial conditions are all zero. We can find the transfer function for this
system by taking the Laplace transform of both sides of the differential equation and using the
differentiation property.
s2Y.s/ C 2sY.s/   8Y.s/ D 2sX.s/ C 5X.s/

22.6. DIFFERENTIATION IN THE S-DOMAIN
289
or
Y.s/
 s2 C 2s   8

D X.s/.2s C 5/
Therefore,
H.s/ D Y.s/
X.s/ D
2s C 5
.s   4/.s C 2/
22.6
DIFFERENTIATION IN THE S-DOMAIN
L f tx.t/g D dX.s/
ds
ROC W R
To show this start with the definition of the Laplace transform
X.s/ D
Z 1
 1
x.t/e stdt
Taking the derivative with respect to s on both sides and switching the order of integration and
differentiation we get
d
ds X.s/
D
d
ds
Z 1
 1
x.t/e stdt
D
Z 1
 1
d
ds
 x.t/e st
dt
D
Z 1
 1
. tx.t// e stdt
or
L f tx.t/g D dX.s/
ds
Taking the derivative of a rational polynomial will not change the location of the roots in the
denominator so the region of convergence does not change. This property comes in handy when
we have factors involving tn in x.t/
Example 22.6
Find the Laplace transform of
x.t/ D te 2tu.t/
We know that
L

e 2tu.t/

D
1
s C 2I
 >  2
and
d
ds

1
s C 2

D
 1
.s C 2/2

290
22. PROPERTIES OF LAPLACE TRANSFORMS
Therefore, using the differentiation property
L

te 2tu.t/

D
1
.s C 2/2 I
 >  2
22.7
INTEGRATION IN THE TIME DOMAIN
To find out what happens in the Laplace domain when we integrate in the time domain we take
a slightly circuitous route. What we want to find is
L
Z t
 1
x./d

D‹
Instead what we will do is find the Laplace transform of x.t/ ~ u.t/. We know that the Laplace
transform of a convolution is the product of the individual Laplace transforms.
L Œx.t/ ~ u.t/ D X.s/L Œu.t/
So what is the Laplace transform of u.t/? We know that
L

e atu.t/

D
1
s C aI  >  a
If we pick a to be zero then e at is equal to 1 and we find the Laplace transform of u.t/. That is
L Œu.t/ D 1
s I  > 0
The final piece of the puzzle is the convolution integral
x.t/ ~ u.t/ D
Z 1
 1
x./u.t   /d D
Z t
 1
x./d
where we have used the fact that u.t   / is 1 for  < t and 0 for  > t.
Putting all of this together we get
L
Z t
 1
x./d

D 1
s X.s/ ROC W R \ f > 0g
Notice that in this case we do increase the number of poles. The 1=s term introduces a pole
at s D 0. Hence, the region of convergence shrinks to either the right or left of the line  D 0
depending on whether we have a left sided or right sided function.

22.8. SUMMARY
291
22.8
SUMMARY
In this module we looked at the following properties of the Laplace transform:
1. Convolution property: The Laplace transform of the convolution of two signals is the
product of the Laplace transforms of the individual signals with a region of convergence
which contains the intersection of the regions of convergence of the individual Laplace
transforms.
L fx1.t/ ~ x2.t/g D X1.s/X2.s/ ROC W containing R1 \ R2
2. Time shifting: If we introduce a shift in the time domain representation of a signal this
shows up as a complex exponential factor in the Laplace domain. The region of convergence
is not effected.
L fx.t   to/g D e stoX.s/ ROC W R
3. Frequency shifting: If we multiply the time signal by a complex exponential this results
in a shift in the Laplace domain corresponding to the real part of the exponent. This shift
is also reflected in the poles and zeros and hence in the region of convergence.
L
˚
esotx.t/
	
D X.s   so/ ROC W R C o
4. Time scaling: Scaling a function in the time domain results in a movement in the pole
zero locations and hence the region of convergence.
L fx.˛t/g D 1
j˛jX. s
˛ / ROC W ˛R
5. Time differentiation: Differentiation of a signal in the time domain results in a multipli-
cation of the Laplace transform of the signal with s.
L
dx.t/
dt

D sX.s/ ROC W containing R
6. Frequency differentiation: Multiplication of a signal in time with  t corresponds to the
differentiation of its Laplace transform with respect to s.
L f tx.t/g D dX.s/
ds
ROC W R
7. Integration in time: Integration in the time domain results in the division of the Laplace
transform of the signal by s.
L
Z t
 1
x./d

D 1
s X.s/ ROC W R \ f > 0g

292
22. PROPERTIES OF LAPLACE TRANSFORMS
22.9
EXERCISES
(Answers on the following page)
1. Given the input x.t/ D e 2tu.t/ and a linear time-invariant system with transfer function
H.s/ D
1
s C 1
Find the output y.t/.
2. The input x.t/ and the output y.t/ of a linear time invariant system are given by
x.t/
D
e 2tu.t/
y.t/
D
e 2tu.t/ C e 3tu.t/
Find the transfer function H.s/.
3. Using the differentiation in time property find the Laplace transform of
x.t/ D te 2tu.t/
4. Use the fact that
L

e atu.t/

D
1
s C a
and the time shift property to find the Laplace transform of
x.t/ D u.t/   u.t   4/
Confirm your result by direct evaluation of the Laplace transform.
5. The poles of the Laplace transform of x.t/ are at  2 and  4. Where are the poles of the
Laplace transform of x. t/?
6. If x.t/ is an even function of time what can you say about the poles of x.t/ and x. t/?
7. The input output relationship for a linear time invariant system is given by
d 2y.t/
dt2
C 3dy.t/
dt
C 2y.t/ D dx.t/
dt
C 5x.t/
Find the transfer function for this system.
8. Given a linear time invariant system described by the differential equation
d 2y.t/
dt2
C 3dy.t/
dt
C 2y.t/ D dx.t/
dt
C 5x.t/
and an input e 5tu.t/ what is the output y.t/?

22.9. EXERCISES
293
9. Use the integration property to find the Laplace transform of tu.t/.
10. A linear time invariant system has an impulse response e 2tu.t/. What is the output of
this system if the input is the unit step function u.t/?

294
22. PROPERTIES OF LAPLACE TRANSFORMS
22.10 ANSWERS
1.
y.t/ D e tu.t/   e 2tu.t/
2.
H.s/ D 2s C 5
s C 3
3.
X.s/ D
1
.s C 2/2
4.
X.s/ D 1
s

1   e 4s
5. 2, 4
6. They are the same.
7.
H.s/ D
s C 5
s2 C 3s C 2
8.
y.t/ D e tu.t/   e 2tu.t/
9.
1
s2
 > 0
10.
1
2
 1   e 2t

295
M O D U L E
23
Analysis of Systems with
Feedback
If we examine any complex system we will find that the behavior of the
system depends on the existence of feedback mechanisms. A feedback is
a measure of some or all of the outputs of a system which is then used to
affect the input to the system. This feedback can be positive or negative.
The system can be as simple as the heating system in your house, or more
complicated like the cruise control on your car or much more complicated
like the economy or the political system of a country or probably the most
complicated system we deal with regularly—our own body. We can use
feedback to make an unstable system stable, to reduce the sensitivity of the
system to parameter variations, to make the system more responsive, or to construct the inverse
of a system. We can also make use of feedback to make sure that the system under consideration
is producing a desired output. To study all of this in any detail requires a course of its own but
we can briefly look at some of these uses using some simple examples
23.1
STABILIZING A SYSTEM
Consider a really simple unstable system, a system with impulse response etu.t/. The output of
this system can increase exponentially. We could control this exponential growth by measuring
the output and when the output increased we could reduce the input by a commensurate amount.
A block diagram of this conceptual system is shown in Figure 23.1. The system in this form is
a feedback system with a negative feedback. We have represented the system we are attempting
to stabilize—the system with impulse response etu.t/—with the transfer function where the
transfer function of this system is
H.s/ D
1
s   1I
 > 1
How do we pick what values of K will allow the system to be stable? Here is where we can use
the Laplace domain view of the system. We know the response is right-sided, therefore, the
region of convergence will be to the right of a boundary. To be stable this region of convergence
has to include the j! axis. So we need to pick the gain K in order to make sure that the poles of
the overall system are in the left half plane. In order to do that we have to first find the transfer

296
23. ANALYSIS OF SYSTEMS WITH FEEDBACK
x(t)
y(t)
K
1
—
s – 1
+
−
Figure 23.1: A simple feedback system.
X(s)
E(s)
Y(s)
K
1
—
s – 1
+
−
Figure 23.2: A simple feedback system redrawn.
function of the overall system. Let’s use the labeled version of the system, shown in Figure 23.2,
to compute the transfer function.
In the figure we have replaced the time domain expression of the input and output with
their Laplace transform and we have added another label E.s/ in the figure. This strategy of
adding labels, or defining additional variables, in complicated systems makes these systems much
easier to analyze. At first this seems a bit counterintuitive as we are increasing the number of
variables but as we will see in the end it simplifies the analysis problem considerably.
So let’s find the transfer function of the overall system—the system outlined by the dashed
box. We can see that E.s/ is the difference between the input X.s/ and the feedback. The feed-
back in turn is the gain K times the output Y.s/. Or
E.s/ D X.s/   KY.s/
The output is given by
Y.s/ D E.s/ 
1
s   1
Substituting for E.s/ we get
Y.s/
D
.X.s/   KY.s// 
1
s   1
D
X.s/
s   1   KY.s/
s   1
Collecting all the Y.s/ terms on the lefthand side we get
Y.s/ C KY.s/
s   1 D X.s/
s   1

23.1. STABILIZING A SYSTEM
297
x(t)
y(t)
A(s)
+
−
B(s)
Figure 23.3: A general feedback system.
or
Y.s/

1 C
K
s   1

D X.s/
s   1
Combining the terms on the left hand side
Y.s/  s   1 C K
s   1
D X.s/
s   1
and
H.s/ D Y.s/
X.s/ D
1
s   1 C K
This system has a pole at 1   K. If we now pick K > 1 then that pole will lie in the left half
plane and the system will be stable.
Notice that we haven’t changed the system in any fundamental way apart from making it
stable. We initially had a system with an exponential response. We still have a system with an
exponential response. Except now, if we pick K > 1, the response is a decaying exponential.
Let’s repeat this with more general components. Suppose the transfer function of the
system we want to control is A.s/ and when we feed the output back we not only amplify or
attenuate it we also shape it by passing it through a system with transfer function B.s/. This
more general system is shown in Figure 23.3. Let’s repeat the steps above to calculate the overall
transfer function. As before, it is convenient in calculating the overall transfer function H.s/ to
define a variable E.s/ at the output of the summer.
E.s/ D X.s/   B.s/Y.s/
Then
Y.s/ D E.s/A.s/
Substituting for E.s/
Y.s/
D
.X.s/   B.s/Y.s// A.s/
D
A.s/X.s/   A.s/B.s/Y.s/
and ...
H.s/ D
A.s/
1 C A.s/B.s/

298
23. ANALYSIS OF SYSTEMS WITH FEEDBACK
x(t)
y(t)
A(s)
+
−
K
B(s)
Figure 23.4: A general feedback system.
This is a much more complicated result compared to our initial system. Depending on the poles
and zeros of B.s/ the new system may have a frequency response which is much different from
the original system. We can pick B.s/ such that the overall system response is what we want.
Another commonly used variation of a general system is shown in Figure 23.4. If we go
through our previous analysis to find the overall transfer function we get
H.s/ D
KA.s/
1 C KA.s/B.s/
In our first system increasing the value of the gain K moved the poles further to the right. In this
system depending on the poles and zeros of KA.s/B.s/, increasing the gain can have a number
of different consequences as the poles of the overall system move around. This movement of the
poles is summarized in a diagram called the root locus diagram. We defer further discussion of
this to your control system’s class.
23.2
MAKING A SYSTEM MORE RESPONSIVE
This is almost a cheat as it is simply a variation of the previous idea. Because we can move the
pole around using feedback we can make the system more responsive. Consider a single pole
system with impulse response
a.t/ D e 2tu.t/
and suppose we use a simple gain in the feedback. So in terms of the system of Figure 23.3 we
have
A.s/
D
1
s C 2I  >  2
B.s/
D
K
The overall transfer function for this system would be
H.s/
D
1
s C 2
1 C K
1
s C 2
D
1
s C 2 C K I  >  .2 C K/

23.3. IMPLEMENTING THE INVERSE OF A SYSTEM
299
The response for this system is
h./ D e .KC2/tu.t/
Depending on the value of K this exponential decays faster than the original response.
23.3
IMPLEMENTING THE INVERSE OF A SYSTEM
Suppose we want to cancel out the effect of a system. Finding the inverse system is not always
easy but we can use the properties of feedback to obtain a close approximation to the inverse.
Let’s suppose the system whose inverse we want to obtain has a transfer function G.s/. In the
system of Figure 23.3 set
A.s/
D
K
B.s/
D
G.s/
The transfer function of the overall system then becomes
H.s/ D
K
1 C KG.s/
For K sufficiently large such that KG.s/ >> 1 we can approximate the denominator 1 C KG.s/
by KG.s/ and
H.s/ 
1
G.s/
23.4
PRODUCING A DESIRED OUTPUT
Often what we want from a system is to control a system to generate a prescribed output. There
are many such systems you use in your daily life. Consider your heating system. You set the
temperature you want and your heater or air conditioner cycles on and off to achieve that tem-
perature. Or consider cruise control. Here you set a desired speed. If the measured speed is less
than the desired speed the error signal (the difference between the desired and measured output)
is positive and that might result in an action similar to increasing the pressure on the accelera-
tor. If the measured output is larger than the desired output the error is negative and this might
lead to actions to slow the car down. This is the realm of control systems. To make what we are
trying to do more clear in the control system literature B.s/ is generally placed in-line with A.s/
as shown in Figure 23.5. In the control system literature the system with transfer function A.s/
is often called the plant that needs to be controlled, while the system with transfer function B.s/
is called the controller. Notice that what is driving the controller is the difference between the
desired signal x.t/ and the measured signal y.t/. In the cruise control example the plant is your
car and the output is the measured speed of the car.
Let’s derive the overall transfer function H.s/. For this configuration
E.s/ D X.s/   Y.s/

300
23. ANALYSIS OF SYSTEMS WITH FEEDBACK
x(t)
y(t)
A(s)
+
−
B(s)
Figure 23.5
and
Y.s/ D A.s/B.s/E.s/
Plugging in the expression for E.s/.
Y.s/
D
A.s/B.s/ .X.s/   Y.s//
D
A.s/B.s/X.s/   A.s/B.s/Y.s/
Moving A.s/B.s/Y.s/ over to the left hand side we get
Y.s/ C A.s/B.s/Y.s/ D A.s/B.s/X.s/
or
Y.s/.1 C A.s/B.s// D A.s/B.s/X.s/
and
H.s/ D
A.s/B.s/
1 C A.s/B.s/
If we used our previous examples of A.s/ and B.s/ we get
H.s/ D
K
1
s   1
1 C K
1
s   1
Multiplying through by s   1 we get
H.s/ D
K
s   1 C K
As in the previous configuration the pole is now at 1   K and by setting K > 1 we can make
this system stable.
In this particular case we used the controller B.s/ D K. This meant that the control signal
sent to the plant was proportional to the error signal. This form of a controller is called a pro-
portional controller. When the output deviates from the set-point, a signal proportional to the
amount of deviation controls the plant.

23.4. PRODUCING A DESIRED OUTPUT
301
In some systems the (proportional) amount of error may be too small to effect the plant
and the error can achieve a nonzero steady state value. If you want the error between the desired
signal and the measured signal to go to zero one way you can do that is to add an integrator
to the controller. The integrator will integrate the error signal and even when the error signal
is small over a period of time the cumulative effect will be large. Recall that integration of a
function in the time domain corresponds to multiplication with 1=s in the Laplace domain. So
we set the controller to be
B.s/ D Kp C Ki
1
s
where Kp is the proportional gain and Ki is the gain for the integrator. For obvious reasons this
kind of controller is called a PI controller.
Both the proportional controller and the PI controller react to the error signal. If we want
some level of anticipatory control we can add a derivative term as well. The derivative measures
the slope of the error signal. If the slope is high this would point to an imminent sharp change in
the error signal and the controller can begin to compensate for that sharp change. A derivative
in the time domain corresponds to multiplication in the frequency domain so the controller
becomes
B.s/ D Kp C 1
s Ki C sKd
This kind of controller is called a PID controller.
Example 23.1 Cruise Control
Let’s simulate a simplified version of the cruise control to see
the effect of the different gain parameters. There are a lot of factors that go into modeling a car
including the mass of the car, drag on the car, the rolling friction, lift, etc. We will use a very
simplified model where the force exerted by the engine is used to overcome friction which will
be proportional to the speed of the car and move the car. The force exerted by the engine is the
input f .t/ and the velocity of the car v.t/ is the output. This output is measured and subtracted
from the set point of the cruise control to form the input of the PID controller. The output of
the controller is the force exerted on the car.
The equation relating the force exerted on the car and its speed is
f .t/   cv.t/ D ma.t/
where c is the damping constant. We can rewrite the acceleration as a derivative of the velocity
and rewrite this equation as
mdv
dt C cv.t/ D f .t/
Taking the Laplace transform of both sides we can find the transfer function of the car as
A.s/ D
1
ms C c I  >  c=m

302
23. ANALYSIS OF SYSTEMS WITH FEEDBACK
Time (sec)
Kp = 200, Ki = 0, Kd = 0
Speed
30
25
20
15
10
5
00
10
20
30
40
Time (sec)
Kp = 500, Ki = 10, Kd = 0
Speed
30
25
20
15
10
5
00
10
20
30
40
Time (sec)
Kp = 500, Ki = 0, Kd = 0
Speed
30
25
20
15
10
5
00
10
20
30
40
Time (sec)
Kp = 500, Ki = 100, Kd = 1
Speed
30
25
20
15
10
5
00
10
20
30
40
Figure 23.6: Behavior of a simple cruise control system.
The mass of my 2006 Prius is 1310 kg, and a reasonable value for the damping constant would
be 50 Nsec/m. Assuming we set the cruise control to a little over 40 mph, which translates to
18 m/sec, we simulated this system with different values of Kp, Ki, and Kd. Let’s look at some
of the results shown in Figure 23.6. We begin with setting Ki and Kd set to zero so we only have
a proportional controller. You can see in the top two panels of the figure that as we increase the
value of the proportional gain from 200 to 500 the rise time of the response improves. However,
even with a large value for the proportional constant Kp there is a consistent, or steady state,
error between the speed achieved and the desired speed. The desired speed is plotted as a dotted
line. This is where the integral part of the controller comes in handy. The lower left panel shows
the result when we change the coefficient Ki from 0 to 10. You can see a considerable decrease
in the steady state error. If we increase Ki further we can pretty much eliminate the steady state
error but at the cost of an overshoot.

23.4. PRODUCING A DESIRED OUTPUT
303
The MATLAB program used to generate these result is shown below if you are interested
in playing with this.
mass = 1310;
% mass of the car
fric = 50;
% damping constant
car = tf([1],[mass fric]); % transfer function of the car (plant)
ref = 18; % cruise control set point
figure
Kp = 200; % Proportional gain
Ki = 0; % integral gain
Kd = 0; % derivative gain
control = pid(Kp,Ki,Kd); % Controller
H = feedback(control*car,1); % overall transfer function
t = 0:0.1:40;
[y1] = step(ref*H,t); % step response
Kp = 500; % Proportional gain
Ki = 0; % integral gain
Kd = 0;% derivative gain
control = pid(Kp,Ki,Kd);% Controller
H = feedback(control*car,1); % overall transfer function
t = 0:0.1:40;
[y2] = step(ref*H,t);% step response
Kp = 500;% Proportional gain
Ki = 10;% integral gain
Kd = 0;% derivative gain
control = pid(Kp,Ki,Kd);% Controller
H = feedback(control*car,1); % overall transfer function
t = 0:0.1:40;
[y3] = step(ref*H,t);% step response
Kp = 500;% Proportional gain
Ki = 100;% integral gain
Kd = 0;% derivative gain

304
23. ANALYSIS OF SYSTEMS WITH FEEDBACK
control = pid(Kp,Ki,Kd);% Controller
H = feedback(control*car,1); % overall transfer function
t = 0:0.1:40;
[y4] = step(ref*H,t);% step response
tt = tiledlayout(2,2,’TileSpacing’,’Compact’,’Padding’,’Compact’);
nexttile
plot(t,y1,’-k’,’LineWidth’,2)
axis([0 40 0 30])
title(’Kp=200, Ki=0, Kd=0’)
yline(ref,’:’,’LineWidth’,2)
xlabel(’time (sec)’)
ylabel(’speed’)
nexttile
plot(t,y2,’-k’,’LineWidth’,2)
axis([0 40 0 30])
title(’Kp=500, Ki=0, Kd=0’)
yline(ref,’:’,’LineWidth’,2)
nexttile
plot(t,y3,’-k’,’LineWidth’,2)
axis([0 40 0 30])
title(’Kp=500, Ki=10, Kd=0’)
yline(ref,’:’,’LineWidth’,2)
nexttile
plot(t,y4,’-k’,’LineWidth’,2)
axis([0 40 0 30])
title(’Kp=500, Ki=100, Kd=0’)
yline(ref,’:’,’LineWidth’,2)
23.5
SUMMARY
In this module we introduced the idea of feedback and showed how we can use feedback to
stabilize a system, to make it more responsive, to obtain the inverse of a system, and to control
the output of a system in order to get a desired response.

23.6. EXERCISES
305
x(t)
y(t)
K
1
—
s – 2
+
−
Figure 23.7: A simple feedback system.
x(t)
y(t)
K
1
—
s – 2
+
−
Figure 23.8: Another simple feedback system.
x(t)
y(t)
K
s + 1
—
s – 1
+
−
Figure 23.9: A slightly more complicated system.
23.6
EXERCISES
(Answers on the following page)
1. In the system shown in Figure 23.7 what are the poles of the system if K D 1? Is this
system stable.
2. In the system shown in Figure 23.7 what are the poles of the system if K D 3? Is this
system stable.
3. In the system shown in Figure 23.8 what are the poles of the system if K D 1? Is this
system stable.
4. In the system shown in Figure 23.8 what are the poles of the system if K D 3? Is this
system stable.
5. In the system shown in Figure 23.9 what are the poles and zeros of the system if K D 3?
Is this system stable.
6. In the system shown in Figure 23.9 what are the poles and zeros of the system if K D 5?
Is this system stable.

306
23. ANALYSIS OF SYSTEMS WITH FEEDBACK
7. For the system shown in Figure 23.7 for what values of K is the system stable?
8. For the system shown in Figure 23.8 for what values of K is the system stable?
9. For the system shown in Figure 23.9 for what values of K is the system stable?

23.7. ANSWERS
307
23.7
ANSWERS
1. The pole is at s D 1. The system is not stable.
2. The pole is at s D  1. The system is stable.
3. The pole is at s D 1. The system is not stable.
4. The pole is at s D  1. The system is stable.
5. The pole is at s D  0:5 and the zero is at s D  1. The system is stable.
6. The pole is at s D  2=3 and the zero is at s D  1. The system is stable.
7. For the system to be stable we need K > 2.
8. For the system to be stable we need K > 2.
9. for the system to be stable we need K > 1 or K <  1.


309
M O D U L E
24
Z-Transform
Except for our brief foray into the discrete Fourier transform until now
we have been talking about continuous systems. Now we move from an-
alyzing continuous-time systems to analyzing discrete-time systems. To
analyze continuous-time systems we used the Laplace transform which
we obtained as a generalization of Fourier transform. In the same way, we
can obtain the Z-transform as a generalization of the discrete-time Fourier
transform. We can also develop the Z-transform directly, somewhat akin
to how we got into the idea of using ej!t as a building block for repre-
senting functions of time. Let’s try both—you can pick the approach you
prefer.
24.1
THE Z-TRANSFORM AS A GENERALIZATION OF
THE DTFT
Recall that we introduced the discrete time Fourier transform as a direct consequence of sam-
pling. Given a continuous time signal x.t/ we can model the sampling process as a product of
the continuous time function and an impulse train. If we normalize the sampling interval to 1
we get the discrete time Fourier transform
X.ej!/ D
1
X
nD 1
xŒne jn!
which is periodic with period 2. We can be certain the discrete time Fourier transform exists
if the discrete time sequence fxŒng is absolutely summable, that is
1
X
nD 1
jxŒnj < 1
Note that if fxŒng is the impulse response of a linear time-invariant discrete time system then
this also means that the corresponding system is stable in the bounded input bounded output
(BIBO) sense. What if it isn’t and we still want to analyze the frequency domain behavior of the
system? We can do what we did in the case of the continuous time system. Instead of finding
the discrete time Fourier transform of xŒn we can find the transform of the product of xŒn with

310
24. Z-TRANSFORM
the exponential r n given by
1
X
nD 1
xŒnr ne jn!
This summation will converge if
1
X
nD 1
jxŒnr nj < 1
The values of r for which this summation will converge would be the region of convergence for
this transform. Let’s define a new variable z as
z D rej!
z is simply a complex number with a magnitude of r and angle !. Using this new variable we
can rewrite the transform as
1
X
nD 1
xŒnz n
As we are summing over n we will be left with a function of z—let’s call it X.z/. This is the
Z-transform of the discrete time sequence fxŒng which converges for certain values of r D jzj.
24.2
THE Z-TRANSFORM IN THE CONTEXT OF
DISCRETE LTI SYSTEMS
Recall that for a discrete linear time invariant system with an impulse response hŒn with input
xŒn, the output yŒn is given by the convolution sum
yŒn D
1
X
kD 1
xŒn   khŒk
For the special case where xŒn D zn, where z is a complex number
yŒn D
1
X
kD 1
zn khŒk D
1
X
kD 1
znz khŒk
Pulling zn out of the summation we get
yŒn D zn
1
X
kD 1
hŒkz k
Or, an input of xŒn D zn in a discrete LTI system results in an output of yŒn D H.z/zn where
H.z/ D
1
X
nD 1
hŒnz n

24.3. EXPLORING THE Z-TRANSFORM THROUGH EXAMPLES
311
Again we have arrived at the Z-transform—H.z/ is called the Z-transform of hŒn.
In general given a discrete time function xŒn the Z-transform X.z/ is given by
X.z/ D
1
X
nD 1
xŒnz n
The values of z for which this summation converges defines the region of convergence for the
transform. As we saw earlier the region of convergence is actually defined by the magnitude of
z. As in the case for the Laplace transform the region of convergence is an essential part of
the Z-transform.
24.3
EXPLORING THE Z-TRANSFORM THROUGH
EXAMPLES
As in case of the Laplace transform we will explore the Z-transform through examples. Lets
begin with two prototypic signals which we will use as our basic patterns for obtaining Z-
transforms—one a right sided sequence and the other a left sided sequence. The right sided
sequence is
xŒn D anuŒn
The Z-transform is given by
X.z/
D Z ŒanuŒn
D
1
X
nD 1
anuŒnz n
D
1
X
nD0
anz n
D
1
X
nD0
 az 1n
D
 az 10   az 1!1
1   az 1
The second term in the numerator will go to zero if the magnitude of az 1 is less than 1 and it
will go to infinity if this magnitude of az 1 is greater than 1. Therefore,
X.z/ D
1
1   az 1
jaz 1j < 1
or
X.z/ D
1
1   az 1
jzj > jaj

312
24. Z-TRANSFORM
Im[z]
Re[z]
Region of
convergence
|a|
Figure 24.1: Region of convergence jzj > jaj in the complex z-plane.
What does this region of convergence look like? Recall that z is a complex number. We can
write this in polar form as
z D rej!
Then
jzj D
p
zz D
p
rej!re j! D
p
r2 D r
Therefore, the region jzj > jaj consists of all complex numbers rej! with r > jaj as shown in
Figure 24.1.
Let’s now repeat with a left sided sequence
xŒn D anuŒ n   1
X.z/
D
Z ŒanuŒ n   1
D
1
X
nD 1
anuŒ n   1z n
D
 1
X
nD 1
anz n
D
 1
X
nD 1
 az 1n
D
 az 1! 1   az 10
1   az 1
 az 1! 1 )

1
az 1
!1

24.3. EXPLORING THE Z-TRANSFORM THROUGH EXAMPLES
313
and

1
az 1
!1
! 0 when
ˇˇˇˇ
1
az 1
ˇˇˇˇ < 1
and
ˇˇˇˇ
1
az 1
ˇˇˇˇ < 1 )
ˇˇˇz
a
ˇˇˇ < 1 ) jzj < jaj
Thus,
X.z/ D  1
1   az 1
jzj < jaj
and the region of convergence is the inside of a circle with radius jaj. Again the boundary is
defined by the pole at z D a.
We will use the signals anuŒn and anuŒ n   1 for the Z-transform like we used e atu.t/
and e atu. t/ for the Laplace transform. As in the case of the Laplace transform where most of
the signals we were interested in could be written as some combination of one sided exponentials,
in the discrete case most of the signals we are interested in can be fit into the discrete exponential
form. Let’s see this with a few examples.
Example 24.1
What is the Z-transform of
xŒn D .0:9/nuŒn
This is a pretty straightforward application of the development above.
X.z/
D
1
X
nD 1
.0:9/nuŒnz n
D
1
X
nD0
.0:9/nz n
the lower limit becomes 0 because of uŒn
D
1
X
nD0
.0:9z 1/n
we combine 0:9n and z n
D
.0:9z 1/0   .0:9z 1/!1
1   .0:9z 1/
using the geometric sum formula
D
1
1   0:9z 1
ˇˇ0:9z 1ˇˇ < 1
D
1
1   0:9z 1
j0:9j < jzj
D
z
z   0:9
jzj > j0:9j
The pole-zero diagram and region of convergence are shown in Figure 24.2.

314
24. Z-TRANSFORM
0.9
1.0
Figure 24.2: The zero is at z D 0, the pole is at z D 0:9 and the region of convergence is jzj > 0:9.
The dashed circle is the circle of radius 1.
Let’s try a slightly more complicated looking example.
Example 24.2
Find the Z-transform of
xŒn D sin

4 n

uŒn
This looks a bit messier at first but we have Euler on our side. Using Euler’s identity we
can write the sin function in terms of complex exponentials.
xŒn
D
 1
2j ej 
4 n   1
2j e j 
4 n

uŒn
D
1
2j ej 
4 nuŒn   1
2j e j 
4 nuŒn
Now we will use the linearity of the Z-transform to find the Z-transform of each individual
term and then combine the Z-transforms.
Z
 1
2j ej 
4 nuŒn

D
1
X
nD 1
1
2j ej 
4 nuŒnz n
D
1
2j
1
X
nD0
ej 
4 nz n
D
1
2j
1
X
nD0

ej 
4 z 1n

24.3. EXPLORING THE Z-TRANSFORM THROUGH EXAMPLES
315
D
1
2j
0
B@

ej 
4 z 10
 
ej 
4 z 1!1
1   ej 
4 z 1
1
CA
D
1
2j

1
1   ej 
4 z 1

ˇˇˇej 
4 z 1ˇˇˇ < 1
D
1
2j

z
z   ej 
4

jzj >
ˇˇˇej 
4
ˇˇˇ
What is the magnitude of ej 
4 ? You can probably guess but let’s slog through the derivation.
Using Euler again (this guy Euler is good!).
ej 
4 D cos

4

C j sin

4

and the magnitude of ej 
4 is simply the square root of the sum of the square of the real and
imaginary parts.
ˇˇˇej 
4
ˇˇˇ D
r
cos2

4

C sin2 
4

or
ˇˇˇej 
4
ˇˇˇ D 1
We go through the same process with the second term
1
2j

e j 
4 n
uŒn and we get
Z
 1
2j e j 
4 nuŒn

D 1
2j

z
z   e j 
4

jzj > 1

316
24. Z-TRANSFORM
1.0
Figure 24.3: The zero is at z D 0, the poles are at z D ˙ej 
4 and the region of convergence is
jzj > 1.
Combining the two results (notice we have the same region of convergence for both) we
get
X.z/
D
1
2j

z
z   ej 
4

  1
2j

z
z   e j 
4

jzj > 1
D
1
2j
 
z.z   e j 
4 /   z.z   ej 
4 /
.z   ej 
4 /.z   e j 
4 /
!
jzj > 1
D
1
2j
 
z2   ze j 
4   z2 C zej 
4
z2   zej 
4   ze j 
4 C 1
!
jzj > 1
D
1
2j
 
z.ej 
4   e j 
4 /
z2   z.ej 
4 C e j 
4 / C 1
!
jzj > 1
D
z.ej 
4   e j 
4 /=2j
z2   2z cos

4

C 1
jzj > 1
D
z sin

4

z2   2z cos

4

C 1
jzj > 1
That was a lot of algebraic manipulation but that is all it was. The actual machinery for com-
puting the Z-transform was really not much different from the machinery we used to find the
Z-transform of anuŒn. The pole zero diagram and the region of convergence are shown in Fig-
ure 24.3.
OK let’s take a really simple one now.

24.3. EXPLORING THE Z-TRANSFORM THROUGH EXAMPLES
317
Example 24.3
Find the Z-transform of
xŒn D uŒn
Before we plug this into our Z-transform finding machinery—multiplying by z n and then
summing from  1 to 1—let’s see if we can use what we already know to find the Z-transform
without any of the computation. We already know the Z-transform of anuŒn. The function xŒn
we describe above is simply anuŒn with a D 1. So in order to find X.z/ here all we need to do
is substitute a D 1 into the Z-transform of anuŒn. Therefore
X.z/ D
1
a   z 1 D
z
z   1
jzj > 1
All the examples we have looked at have been of signals which are non-zero over an infinite
interval. What about signals that are non-zero over a finite interval?
Example 24.4
Find the Z-transform of
xŒn D uŒn   uŒn   4
There are only four non-zero terms here so we can simply write them out.
X.z/
D
1
X
nD 1
.uŒn   uŒn   4/z n
D
3
X
nD0
z n
D
1 C z 1 C z 2 C z 3
easy peasy! Notice that the region of convergence is the entire z-plane except for the point z D 0.
OK, but what if the sequence had finite extent but not just non-zero over 3 terms.
Example 24.5
Find the Z-transform of
xn D 0:6n .uŒn   uŒn   100/

318
24. Z-TRANSFORM
Just plug this into the Z-transform equation and turn the crank.
X.z/
D
1
X
nD 1
0:6n .uŒn   uŒn   100/ z n
D
99
X
nD0
0:6nz n
D
99
X
nD0
 0:6z 1n
D
 0:6z 10   0:6z 1100
1   0:6z 1
D
1   0:6z 1100
1   0:6z 1
Notice that it looks like there is a pole at z D 0:6 but if you plug in 0:6 in the expression for
X.z/ you get a 0/0 form. If you apply L’Hopital’s rule you will find that there is no pole at 0.6.
The region of convergence is the entire z-plane except for z D 0.
24.4
SUMMARY
In this module
1. We defined the Z-transform of a discrete time sequence as
X.z/ D
1
X
nD 1
xŒnz n
The region of convergence consists of those values of z for which this summation is finite.
2. We derived the Z-transform of two “template” sequences.
Z ŒanuŒn
D
z
z   aI jzj > jaj
Z ŒanuŒ n   1
D
 z
z   aI jzj < jaj
3. The Z-transform of a sequence that is finite over a finite interval has a region of conver-
gence which is the entire z-plane except possibly for z D 0.

24.5. EXERCISES
319
24.5
EXERCISES
(Answers on the following page)
Find the Z-transforms of the following sequences.
1.
xŒn D 2nuŒn
2.
xŒn D 2 nuŒn
3.
xŒn D 2nuŒ n   1
4.
xŒn D 2 nuŒ n   1
5.
xŒn D 2n .uŒn   uŒn   3/
6.
xŒn D cos

3 n

uŒn
7.
xŒn D an cos

3 n

uŒn
8.
xŒn D ıŒn   1
9.
xŒn D
1
2
n
uŒn   2
10.
xŒn D
1
2
n
uŒ2   n

320
24. Z-TRANSFORM
24.6
ANSWERS
1.
X.z/ D
z
z   2
jzj > 2
2.
X.z/ D
z
z   1=2
jzj > 1
2
3.
X.z/ D
z
z   2
jzj < 2
4.
X.z/ D
2z
1   2z
jzj < 1
2
5.
X.z/ D 1 C 2
z C 4
z2 D z2 C 2z C 4
z2
6.
X.z/ D
z2   0:5
z2   z C 1
jzj > 1
7.
X.z/ D
z2   0:5a
z2   za C a2
jzj > jaj
8.
X.z/ D 1
z
9.
X.z/ D
1
4
z2   1
2z
jzj > 1
2
10.
X.z/ D
1
4z2.1   2z/
jzj < 1
2

321
M O D U L E
25
Region of Convergence for the
Z-Transform
In the previous module we found the Z-transform of a number of se-
quences and in the process also looked at the region of convergence. In
this module we will take a slightly more detailed look at the region of con-
vergence and make an attempt to connect the region of convergence for
the Z-transform of a discrete sequence with the Laplace transform of a
continuous function.
Let’s summarize what we know about the regions of convergence for
the Z-transform. We have seen that a right sided signal
xŒn D anuŒn
has a Z-transform
X.z/ D
1
1   az 1 D
z
z   aI
jzj > a
with a region of convergence which is the outside of a circle of radius jaj. The boundary of the
region of convergence is defined by a pole at z D a.
For a left sided signal
xŒn D  anuŒ n   1
The Z-transform X.z/ is given by
X.z/ D
1
1   az 1 D
z
z   a
jzj < jaj
and the region of convergence is the inside of a circle with radius jaj as we would have predicted
based on the mapping from the Laplace to the Z-domain. Again the boundary is defined by the
pole at z D a.
Let’s take a look at the Z-transform of a two-sided sequence
xŒn D ajnj
We can rewrite xŒn as
xŒn D anuŒn C a nuŒ n   1

322
25. REGION OF CONVERGENCE FOR THE Z-TRANSFORM
Using the linearity of the Z-transform we can find the Z-transform of each term and then
combine them in the intersection of their regions of convergence. For the first term we already
have
Z ŒanuŒn D
1
1   az 1
jzj > jaj
For the second term
Z Œa nuŒ n   1
D
 1
X
nD 1
a nz n
D
 1
X
nD 1
 a 1z 1n
D
 a 1z 1! 1   a 1z 10
1   .az/ 1
For the first term in the numerator
 a 1z 1! 1 D .az/!1
This will go to zero as long as
jazj < 1 ) jzj < 1
jaj
and the Z-transform of the second term becomes
Z Œa nuŒ n   1 D  1
1   .az/ 1
jzj < 1
jaj
Combining the Z-transform of the two terms
Z
h
ajnji
D
1
1   az 1  1
1   .az/ 1
jaj < jzj < 1
jaj
So if jaj < 1 the region of convergence is the annular region between circles of radius jaj and
1=jaj as shown in Figure 25.1
Note that if jaj > 1 then 1=jaj < 1 and jzj cannot simultaneously be greater than jaj and
less than 1=jaj. Therefore, if jaj > 1 the Z-transform of ajnj does not exist.
To summarize what we have seen until now:
1. Each boundary of a region of convergence in the z-plane is a circle centered at the origin.
The radius of the circle is the magnitude of a pole of the Z-transform.
2. The region of convergence does not contain any poles.

25.1. MAPPING BETWEEN s- AND z-PLANES
323
Im[z]
Re[z]
Region
of convergence
|a|
1/|a|
Figure 25.1: An annular region of convergence for a two sided sequence.
3. If xŒn is of finite duration then the region of convergence is the entire plane except perhaps
for z D 0 or z D 1.
4. If xŒn is a right sided sequence the region of convergence is outside of a circular boundary.
5. If xŒn is a left sided sequence the region of convergence is inside a circular boundary.
6. If xŒn is two sided the region of convergence is between two circular boundaries.
Example 25.1
Consider a system with a transfer function with three poles, two at 0:5 ˙ j0:5
and one at  1:5, one zero at z D 0 as shown in Figure 25.2.
The poles define the boundaries of the regions of convergence. Therefore, as shown in
Figure 25.3 we end up with three regions of convergence.
The innermost region of convergence will give rise to a left-sided sequence. As the region
of convergence does not include the unit circle the corresponding system is not guaranteed to be
stable. The outer region of convergence outside of the circle defined by jzj D 1:5 corresponds to a
right sided sequence. This region of convergence also does not include the unit circle, therefore,
the corresponding system is not guaranteed to be stable. Finally, the third region of convergence
is the one between the circle of radius 1.5 and the circle of radius 1=
p
2. This will correspond to
a two sided signal and as the region of convergence includes the unit circle the corresponding
system will be stable.
25.1
MAPPING BETWEEN s- AND z-PLANES
How does the magnitude of a affect the discrete time function? For the moment let’s assume
that a is positive. If the magnitude of a is less than one, then as n increases, the value of jajn will

324
25. REGION OF CONVERGENCE FOR THE Z-TRANSFORM
Im[z]
Re[z]
2
−2
−1
1
Figure 25.2: Poles and zero of a discrete time system.
Im[z]
Re[z]
Unit circle
2
−2
−1
1
Figure 25.3: The regions of convergence for a three pole one zero system.
decrease; and we have an exponentially decreasing function. Furthermore, if xŒn is the impulse
response of a linear time-invariant system, it satisfies the requirement for BIBO stability.
1
X
nD0
jxŒnj D
1
X
nD0
jajn D jaj0   jaj!1
1   jaj
D
1
1   jaj

25.1. MAPPING BETWEEN s- AND z-PLANES
325
However, if the magnitude of a is greater than one then as n increases the value of jajn increases
and
1
X
nD0
jxŒnj D
1
X
nD0
jajn D jaj0   jaj!1
1   jaj
! 1
and the system is not BIBO stable. Looking at Figure 24.1 we can see that in the first case where
jaj < 1 the region of convergence includes the circle with a radius of one—the unit circle, while
in the second case where jaj > 1 the region of convergence does not include the unit circle.
We can understand the importance of the inclusion of the unit circle in the region of
convergence in a couple of different ways. The Z-transform is given by
X.z/ D
1
X
nD 1
xŒnz n
If we write z in polar coordinates z D rej! the Z-transform becomes
1
X
nD 1
xŒnr ne jn!
If we want to evaluate this transform on the unit circle we set r D 1 and this becomes
1
X
nD 1
xŒne jn!
which is simply the discrete time Fourier transform of xŒn. If xŒn is the impulse response of a
linear time-invariant discrete system the existence of the discrete time Fourier transform implies
that the system is stable. Therefore, if we can evaluate the Z-transform of an impulse response on
the unit circle then we can say that the system is stable. But we can only evaluate the Z-transform
on the unit circle if the unit circle is contained in the region of convergence. Therefore, if the
unit circle is contained in the region of convergence we can say that the associated system is
stable.
We can also reach the same conclusion if we look at the transformation between the z-
plane and the s-plane. The transformation between the two planes is given by
z D es D eCj!
The transformation maps points from the s-plane to the z-plane. This means that the points
along a line  D o will be mapped into the points
z D eoCj! D roej!
where
ro D eo

326
25. REGION OF CONVERGENCE FOR THE Z-TRANSFORM
ωS
–ωS
–2ωS
2ωS
W
−W
XS(ω)
ω
Figure 25.4: Spectral profile of a sampled signal.
The equation
z D roej!
is the equation of a circle with radius ro. Thus, vertical lines in the s-plane map into circles in
the z-plane. Therefore, as the boundaries of regions of convergence n the s-plane were vertical
lines, the boundaries of regions of convergence in the z-plane will be circles. Let’s look at the
special case of the j! axis. The j! axis corresponds to the line  D 0. Therefore, in the z-plane it
corresponds to the circle with radius ro D e0 D 1—the unit circle. Just as in the Laplace domain
inclusion of the j! axis in the region of convergence indicated stability of continuous time
systems, inclusion of the unit circle in the z-plane indicates stability of discrete time systems.
In the Laplace domain we were often concerned with whether a region was to the left
or the right of a boundary. How does the notion of being to the left or right of a boundary
translate to the z-domain? Consider two points s1 and s2 in the s-plain with s1 D 1 C j! and
s2 D 2 C j!. If 2 < 1 then the point s2 will be to the left of the line defined by  D 1. If
we look at these points in the z-plane, the points corresponding to the line  D 1 correspond
to a circle with radius e1. A point in the s-plane with  D 2 where 2 < 1 will have a radius
of e2. As 2 < 1, e2 < e1, therefore the point in the z-plane corresponding to s2 will lie
inside the circle of radius e1. Similarly if 2 > 1, s2 will lie to the right of the line defined by
 D 1 in the s-plane and the corresponding point in the z-plane will lie outside of the circle
of radius e1. Thus, points to the left of a line in the s-plane lie inside the corresponding circle
in the z-plane and points to the right of a line in the s-plane lie outside of the corresponding
circle in the z-plane.
Before we leave the topic of mapping between the s and z-planes we need to deal with
one more issue. In the s-domain we obtained the frequency response of a system by traversing
the j! axis and we could do this for distinct values of ! from  1 to 1. The j! axis maps
into the unit circle in the z-domain, but here once ! goes from 0 to 2, or   to  we come
back to the same point and repeat our traversal. We can see why this is so if we recall that a
discrete time signal is generated from a continuous time signal by sampling in the time domain.
And sampling in the time domain corresponds to replication in the frequency domain where
the replicas are separated by the sampling frequency !s (or fs in Hz). The spectral profile of a
sampled signal is shown in Figure 25.4

25.1. MAPPING BETWEEN s- AND z-PLANES
327
ωS
–ωS
–2ωS
2ωS
W
−W
XS(ω)
ω
ωS
–ωS
–2ωS
2ωS
W
−W
XS(ω)
ω
ω = 0
ω = 2π
ω = –π
ω = π
Figure 25.5: Spectral profile of a sampled signal.
Notice that as we traverse the frequency axis we keep encountering replicas of the original
spectrum. When we traverse a circle we encounter the same value every 2 radians, here we
encounter the same value ever !s radians. If we map ! D 0 to re0 D 1, and ! D !s in the s-
plane to ! D 2 in the z-plane we get the same effect. We can think of the mapping in one of
two ways. We can think of the point z D 1 on the unit circle as corresponding to ! D 0 in the
s-plane and a full revolution around the circle brings us to !s, or we can think of the revolution
going from   to  where   corresponds to  !s=2 and  corresponding to !s=2. The two
ways of looking at the frequency response are shown in Figure 25.5. In practice when we are
dealing with real signals the magnitude of the frequency response is even and phase is odd so
knowing the frequency response from 0 to !s=2 is sufficient and we map  to !s=2.
Finally, we can use this transformation to explain why whereas for a continuous function
which took on finite nonzero values over a finite interval the region of convergence for the
Laplace transform was the entire s-plane, for a discrete time sequence which takes on finite
nonzero values over a finite interval the region of convergence for the Z-transform is the entire
z-plane except for z D 0. To see why this is so consider the Laplace transform X.s/ D e s. This
does not go to infinity for any finite value of s. However, if we let  go to negative infinity, X.s/
will also go to infinity. Therefore, we actually do have a pole—it’s just not in the finite s-plane.
However when we go through the transformation
z D es D eCj! D eej!
when  goes to negative infinity z goes to zero, and the pole at infinity in the s-plane gets
mapped into a pole at z D 0 in the z-plane.

328
25. REGION OF CONVERGENCE FOR THE Z-TRANSFORM
Example 25.2
Given a transfer function
H.s/ D
1
s C 1
and a definition of the Laplace domain that includes infinity let’s map the poles and zeros of this
transfer function to the z-plane. There is clearly a pole at s D  1. Using the mapping z D es.
This pole would map to e 1 D 0:368. Note that a pole in the left half plane in the s-domain
maps to a point inside the unit circle in the z-domain.
At first sight it seems like that is it. There are no more poles and zeros. However if we
let s go to infinity H.s/ will go to zero. Therefore, we also have a zero at s D ˙1. The zero at
negative infinity in the s-domain will map to z D 0 in the z-domain.
25.2
SUMMARY
This module was a bit of something old something new. We saw that
1. A left sided discrete time sequence has a region of convergence inside a circular boundary
where the circle is centered at the origin.
2. A right sided sequence has a region of convergence outside a circular boundary where the
circle is centered at the origin.
3. A two sided sequence has a region of convergence between two circular boundaries where
the circles are centered at the origin.
4. The Laplace domain and the z-domain are related through the mapping
z D es
5. If we evaluate the Z-transform around the unit circle we get the frequency profile of the
discrete time sequence. The points 0 to  around the unit circle correspond to the frequen-
cies from 0 Hz to fs=2 Hz where fs is the sampling frequency used to obtain the discrete
time sequence from the continuous time sequence.

25.3. EXERCISES
329
25.3
EXERCISES
(Answers on the following page)
1. Find the z-transform of
xŒn D .0:6/nuŒn C .1:2/nuŒ n
2. Find the z-transform of
xŒn D
1
2
jnj
3. Find the z-transform of
xŒn D
1
2
n
uŒn C 1 C
1
2
 n
uŒ1   n
4. xŒn is the two sided impulse response of a stable system. Which of the following could
be the poles of the system.
(a)  1=4;  1=2
(b)  1=4; 1=2
(c) 1=2;  2
(d)  1=2; 2
5. In the Laplace transform module we constructed a band pass filter with a poles at s D
 2 ˙ j12 and a zero at s D 0. Where would the poles and zeros of the corresponding
discrete time filter be in the z-domain.
6. If you look at the transfer function
H.s/ D
s
s2 C 4s C 148
you can immediately identify a zero at s D 0 and poles at s D  2 ˙ j12. There is another
zero in this system. Can you identify that zero and find the corresponding zero in the
z-plane.
7. A simple low pass filter has a transfer function in the Laplace domain given by
H.s/ D
1
s C 1
Clearly we have a pole at s D  1. Also, notice that if we let  go to infinity the transfer
function will go to zero. Therefore, we also have a zero at infinity.
(a) where would the corresponding pole be in the z-plane?
(b) Where would the corresponding zero be in the z-plane?
(c) What is the transfer function of the discrete time system?

330
25. REGION OF CONVERGENCE FOR THE Z-TRANSFORM
25.4
ANSWERS
1.
X.z/ D z2   2:4z C 0:72
.z   0:6/.z   1:2/
0:6 < jzj < 1:2
2.
X.z/ D
 3
2z

z   1
2

.z   2/
1
2 < jzj < 2
3.
X.z/ D
2z4   4z3   z C 1
2
z

z   1
2

.z   2/
1
2 < jzj < 2
4. (c) and (d)
5. Zero at z D 1 and poles at z D e 2e˙j12.
6. If we let  go to infinity in either the positive or negative direction the transfer function
will go to zero.
7.
(a) The corresponding pole would be at 1=e D 0:368.
(b) In the finite z-plane s !  1 corresponds to z D 0 so the corresponding zero is at
0.
(c)
H.z/ D
z
z   0:368
jzj > 0:368

331
M O D U L E
26
Properties of the Z-Transform
In this module we will look at some of the properties of the Z-transform—
a few of which we have been using already without explicitly identify-
ing them. These properties match those of properties of the Laplace and
Fourier transforms—properties we have already talked about.
26.1
LINEARITY
We begin, as always with the property of linearity. This a property we have
already used to let us take the Z-transform of sums of sequences. The proof
is very simple.
Let
Z ŒxŒn
D
X.z/ ROC W Rx
Z ŒyŒn
D
Y.z/ ROC W Ry
Then
Z Œ˛xŒn C ˇyŒn
D
1
X
nD 1
Œ˛xŒn C ˇyŒn z n
D
˛
1
X
nD 1
xŒnz n C ˇ
1
X
nD 1
yŒnz n
D
˛X.z/ C ˇY.z/ ROC W Rx \ Ry
The one thing we do need to be careful about though is the region of convergence.
Example 26.1
Recall the Z-transform of
xŒn D ajnj
We can write xŒn as the sum of two terms
xŒn D anuŒn C a nuŒ n   1
and then take the Z-transform of each term.
Z ŒanuŒn D
z
z   aI jzj > jaj

332
26. PROPERTIES OF THE Z-TRANSFORM
Z Œa nuŒ n   1 D
az
1   az I jzj < 1
jaj
In order to get the Z-transform of xŒn we can use linearity to add these two terms together.
However, we have to make sure that there is a region in the z-plane in which both terms con-
verge. If jaj < 1 we can find such a region and
X.z/ D
.1   a2/z
.z   a/.1   az/I jaj < jzj < 1
jaj
However, if jaj > 1 there is no intersection between the region outside the circle of radius jaj,
jzj > jaj, and the region inside the circle of radius 1
jaj, jzj <
1
jaj, and despite the linearity property
we cannot add the individual terms to obtain X.z/. In fact, X.z/ does not exist.
26.2
CONVOLUTION
Z ŒxŒn ~ hŒn D X.z/H.z/
ROC W Rx \ Rh
We can prove this exactly the same way as we have done previously for Fourier series,
Fourier transforms, and Laplace transforms.
Z ŒxŒn ~ hŒn
D
Z
"
1
X
kD 1
xŒkhŒn   k
#
D
1
X
nD 1
"
1
X
kD 1
xŒkhŒn   k
#
z n
If we set m D n   k, n D m C k and
Z ŒxŒn ~ hŒn
D
1
X
mD 1
1
X
kD 1
xŒkhŒmz .mCk/
D
1
X
mD 1
"
1
X
kD 1
xŒkz k
#
hŒmz m
D
1
X
mD 1
X.z/hŒmz m
D
X.z/
1
X
mD 1
hŒmz m
D
X.z/H.z/

26.2. CONVOLUTION
333
Clearly if the region of convergence for X.z/ is Rx and the region of convergence for Y.z/ is Ry
then the region of convergence for their product will either be Rx \ Rh or will contain Rx \ Rh.
The reason for the latter would be if a pole of either of the terms is canceled by a zero in the
other.
Example 26.2
The step response of a discrete time linear time invariant system is given by
yŒn D 2uŒn  1
2
n
uŒn
What is the transfer function of this system? What is the impulse response of this system?
We know that
yŒn D xŒn ~ hŒn
By the convolution property
Y.z/ D X.z/H.z/
Noting that the step response is the response of the system to an input which is the step function
uŒn,
X.z/ D Z ŒuŒn D
z
z   1I jzj > 1
If we now find Y.z/ we can obtain H.z/ as
H.z/ D Y.z/
X.z/
So let’s find Y.z/. We know that
Z Œ2uŒn D
2z
z   1I jzj > 1
and
Z
1
2
n
uŒn

D
z
z   1=2I jzj > 1=2
Using the linearity property
Y.z/ D
2z
z   1  z
z   1=2I jzj > 1
or
Y.z/ D
z2
.z   1/.z   1=2/I jzj > 1
Dividing Y.z/ by X.z/ we get
H.z/ D
z
z   1=2I jzj > 1=2

334
26. PROPERTIES OF THE Z-TRANSFORM
where because the pole at z D 1 got cancelled the region of convergence has expanded. Note the
new region of convergence jzj > 1=2 includes the region of convergence jzj > 1.
By inspection we can see that
hŒn D
1
2
n
uŒn
26.3
TIME SHIFTING
Z ŒxŒn   n0 D z n0X.z/
This is going to be a property that we will use often. Proving it is straightforward.
Z ŒxŒn   n0
D
1
X
nD 1
xŒn   n0z n
D
1
X
mD 1
xŒmz .mCn0/
D
z n0
1
X
mD 1
xŒmz m
D
z n0X.z/
The region of convergence for the shifted sequence is the same as the region of convergence for
the original sequence except perhaps for z D 0 as the only additional pole would be at z D 0.
Example 26.3
Let’s see if we can use this property to implement the input output relationship
for a system with a particular impulse response. As an example let’s use
hŒn D anuŒn
We can find the transfer function
H.z/ D
1
1   az 1
Remember that
Y.z/ D X.z/H.z/
we have
Y.z/ D X.z/
1
1   az 1

26.4. SCALING IN THE z-DOMAIN
335
or
Y.z/
 1   az 1
D X.z/
which is
Y.z/   az 1Y.z/ D X.z/
We can now use the linearity property of the Z-transform. The function Y.z/ is the transform of
yŒn and the function X.z/ is the Z-transform of xŒn. According to the time shifting property
z 1Y.z/ is the Z-transform of yŒn   1. Putting all of these together we get the input-output
relationship of this discrete time system.
yŒn   ayŒn   1 D xŒn
or
yŒn D xŒn C ayŒn   1
A couple of things to note in this example which will be useful to us when we try to
implement discrete time systems. The factor z 1 can be implemented simply as a delay. This
means that if we can write our input output relationship in terms of z 1 (and its powers) then
we can implement discrete time systems using just delays and multipliers. The other thing to
note is that this is a system with feedback. The output yŒn is fed back after a delay of one time
interval with a gain of a. The fact that the transfer function has a pole (at a location other than
zero) is also indicative of the fact that the system contains feedback.
26.4
SCALING IN THE z-DOMAIN
If we multiply the time sequence xŒn with powers of a complex number zo, we get a scaling of
the Z-transform.
Z

zn
oxŒn

D
1
X
nD 1
zn
oxŒnz n
D
1
X
nD 1
xŒn
 z
zo
 n
In other words, if X.z/ is the Z-transform of xŒn
Z

zn
oxŒn

D X
 z
zo

;
ROC D jzojR
We can see the reason for the change in the region of convergence if we consider a single pole
in X.z/. Suppose that pole is at po. This means that the denominator of X.z/ has a root at
z D po. Or we can say that the denominator has a factor .z   po/. Now consider X.z=zo/. In

336
26. PROPERTIES OF THE Z-TRANSFORM
this function we replace every z in X.z/ with z=zo. That means the factor .z   po/ becomes
.z=zo   po/.
z
zo
  po D 0 ) z D p0z0
so the pole at po moves to zopo. As in the case of the Laplace domain, scaling in the z-domain
is a way to move poles around. We will see more of this when we look at feedback systems.
Example 26.4
Let’s verify this property by finding the shifted poles and zeros of a system using
this property and then see if we get the same poles and zeros when we substitute z=zo for every
z. Suppose we have a system with a zero at 1 and poles at 2 and 3.
H.z/ D
z   1
.z   2/.z   3/I
jzj > 3
If we multiply the impulse response with .1=4/n then using the property the new zero would be
at 1=4 and the new poles will be at 0.5 and 0:75.
If we replace z with z=.1=4/ or 4z we get
H.4z/
D
4z   1
.4z   2/.4z   3/
D
4.z   1=4/
16.z   2=4/.z   3=4/
D
z   1=4
4.z   0:5/.z   0:75/I jzj > 0:75
where the change in the region of convergence is because the impulse response is still a right
sided sequence and the outermost pole is at 0.75. Notice that this also makes the previously
unstable system stable.
26.5
TIME REVERSAL
Reversal of a time sequence results in an inversion of its poles and zeros and its region of con-
vergence. Let’s first derive the property and then look at the consequences of time reversal.
Z ŒxŒ n
D
1
X
nD 1
xŒ nz n
D
1
X
mD 1
xŒmzm

26.6. DIFFERENTIATION IN THE z-DOMAIN
337
where we have used the variable substitution m D  n. We can rewrite this as
Z ŒxŒ n D
1
X
mD 1
xŒm.z 1/ m
of if X.z/ D Z ŒxŒn with region of convergence R then
Z ŒxŒ n D X.z 1/ D X
1
z

ROC D 1
R
If X.z/ has a pole at po, then the denominator of X.z/ has a root at z D po, and the denominator
of X.z/ has a factor .z   po/. For X.1=z/ this factor becomes 1=z   po, but
1
z   po D 0 ) z D 1
po
Therefore, if X.z/ has a pole at po, X.1=z/ will have a pole at 1=po. If po has a magnitude less
than one then 1=po will have a magnitude greater than one. Thus, all poles of X.z/ that are
inside the unit circle will get mapped to poles outside the unit circle for X.1=z/. And similarly
all poles of X.z/ that lie outside the unit circle will translate to poles inside the unit circle for
X.1=z/.
Despite all this movement of poles a stable system stays a stable system and an unstable
system stays an unstable system under this transformation. To see this remember that by replac-
ing n with  n we are changing a left sided sequence into a right sided sequence and vice versa.
If we have a right sided sequence the region of convergence will be outside of a circle defined
by the outermost pole. If the system is stable this outermost pole, and hence all poles, will be
inside the unit circle. This will make the unit circle be in the region of convergence. When we
replace n with  n we get a left sided sequence with a region of convergence inside a circle with
radius being equal to the innermost pole. As all the poles inside the unit circle have migrated
to outside the unit circle this means that the innermost pole will also be outside the unit circle.
Therefore, the region of convergence will again include the unit circle and the system will be
again stable.
26.6
DIFFERENTIATION IN THE z-DOMAIN
The Z-transform of a sequence xŒn weighted by the time instant n is given by
Z ŒnxŒn D  z dX.z/
dz
ROC D R

338
26. PROPERTIES OF THE Z-TRANSFORM
We can show this easily be setting up the expression for the Z-transform and then taking the
derivative of the Z-transform with respect to Z.
d
dz X.z/
D
d
dz
1
X
nD 1
xŒnz n
D
1
X
nD 1
xŒn d
dz z n
D
1
X
nD 1
xŒn. n/z n 1
D
 z 1
1
X
nD 1
nxŒnz n
D
 z 1Z ŒnxŒn
Multiplying both sides by  z we get
 z dX.z/
dz
D Z ŒnxŒn
Example26.5
This property is very helpful when we need to find the Z-transform of sequences
that include factors of nk. We can show this with a simple example. Lets find the Z-transform
of
xŒn D n2uŒn
We can rewrite this as
xŒn D n ŒnuŒn
We know that
Z ŒuŒn D
z
z   1
So
Z ŒnuŒn
D
 z d
dz
z
z   1
D
z
.z   1/2
and
Z

n2uŒn

D
 z d
dz
z
.z   1/2
D
z.z C 1/
.z   1/3

26.7. SUMMARY
339
26.7
SUMMARY
We have discussed a number of properties of the Z-transform in this module which will be
helpful to us when analyzing discrete time systems.
1. Linearity: The Z-transform of a sum is the sum of the Z-transforms with the region of
convergence being the intersection of the regions of convergence of the individual trans-
forms.
2. Convolution: The Z-transform of the convolution of two discrete time sequences is the
product of the Z-transform of the individual sequences.
3. Time shifting: The Z-transform of a sequence shifted by no time units is the Z-transform
of the original sequence multiplied by z no. An implication of this is that the unit delay
operation in the time domain is represented by z 1 in the z-domain.
4. Scaling in the z-domain: Multiplication with zn
o in the time domain results in scaling by
zo in the z-domain. This results in the movement of poles and zeros.
5. Time reversal: Changing of a left sided sequence to a right sided sequence and vice verso
results in all poles and zeros inside the unit circle being moved to outside the unit circle
and all poles and zeros outside the unit circle being moved to inside the unit circle.
6. Differentiation in the z-domain. Multiplication of xŒn by n in the time domain results in
the a differentiation of X.z/ in the z-domain multiplied by  z.

340
26. PROPERTIES OF THE Z-TRANSFORM
26.8
EXERCISES
(Answers on the following page)
1. The sequence yŒn is the step response of a system with impulse response hŒn given by
hŒn D
1
2
n
uŒn
That is yŒn is the output when the input is the unit step function uŒn. Find Y.z/.
2. The input output relationship of a discrete linear time invariant system is given by
yŒn D xŒn C 0:5yŒn   1
Find the transfer function H.z/.
3. Find the z-transform of
xŒn D n2uŒn
4. X.z/ has a pole at z D 0:5 and a zero at z D 0. Given that
yŒn D
1
2
n
xŒn
where are the pole and zero for Y.z/?
5. What is the z-transform of
xŒn D n
1
2
n
uŒn
6. The impulse response of a discrete linear time invariant system is given by
hŒn D
1
2
n
uŒn
If the input is xŒn D ıŒn find the z-transform of the output.
7. The input output relationship of a discrete linear time invariant system is given by
yŒn D xŒn   3
4yŒn   1   1
8yŒn   2
Find the transfer function H.z/ of this system.
8. The input output relationship of a discrete linear time invariant system is given by
yŒn D xŒn   xŒn   2   0:81yŒn   2
Find the transfer function H.z/ of this system.

26.9. ANSWERS
341
26.9
ANSWERS
1.
Y.z/ D
z2
.z   1/.z   1
2/I
jzj > 1
2.
H.z/ D
z
z   0:5I
jzj > 0:5
3.
X.z/ D z.z C 1/
.z   1/3
4. Pole at z D 0:25, zero at z D 0.
5.
X.z/ D
0:5z
.z   0:5/2
6.
Y.z/ D
z
z   0:5I
jzj > 0:5
7.
H.z/ D
z2
.z C 1
2/.z C 1
4/
8.
H.z/ D
z2   1
z2 C 0:81I
jzj > 0:9


343
M O D U L E
27
The Inverse Z-Transform
The Z-transform provides us with an alternative view of discrete time se-
quences just as the Laplace and Fourier transforms provided us with al-
ternative views of continuous time functions. We have seen that finding
the Z-transform of sequences of interest to us is really quite simple. All
we need is the geometric sum formula and some properties of the Z-
transform. However, for us to be able to use the real power of the Z-
transform we need to know how to move from the z domain to the discrete
time sequence—in other words we need the inverse Z-transform.
The formal definition of the inverse Z-transform which we will de-
note as
Z 1 ŒX.z/ D xŒn
is given by the following contour integral
xŒn D
1
2j
I
X.z/zn 1dz
where the contour lies within the region of convergence of X.z/.
While a bit intimidating at first sight this integral is not that difficult to compute. In order
to solve it we use a method based on the Cauchy residue theorem. Using this theorem we can show
that as long as the contour is a simple curve (no loops) the integral is equal to the sum of the
residues corresponding to the poles inside the contour. A residue Ri corresponding to a simple
pole at pi is given by
Ri D lim
z!pi.z   pi/X.z/zn 1
(looks a bit like partial fraction expansion). For higher order poles the residue Ri for a pole of
order k is given by
Ri D
1
.k   1/Š lim
z!pi
d k 1
dzk 1
h
.z   pi/kX.z/zn 1i
Once we have found the residues the integral is simply the sum of these residues.
1
2j
I
X.z/zn 1dz D
X
i
Ri

344
27. THE INVERSE Z-TRANSFORM
Where the sidedness of the sequence is dictated by the region of convergence.
While the formal approach may not be that difficult we can make life even easier for
ourselves if we use the same shortcut we used for the inverse Laplace transform. And being
lazy (the best engineer is a lazy engineer) that is what we will do. As in the case of the Laplace
transform the shortcut makes use of the fact that most of the Z-transforms we are interested in
are rational polynomials in z. If we have a Z-transform
X.z/ D N.z/
D.z/ D .z   p1/.z   p2/ : : : .z   pn/
.z   z1/.z   z2/ : : : .z   zm/
where the degree of the numerator polynomial is less than the degree of the denominator poly-
nomial then, just as in the case of the Laplace transform, we can use partial fraction expansion
to write this as
X.z/ D
X
Ai
z   zi
We run into a slight problem here. Recall that for the inverse Laplace transform we made use
of the following facts:
1. the Laplace transform and the inverse Laplace transform are linear transforms and
2. we knew that the inverse Laplace transform of 1=.s   zi/ was either a right sided or a left
sided exponential.
Is this also true for the Z-transform? Well the first point certainly holds; the Z-transform and its
inverse are linear transforms. However, the second fact is only partially correct. The Z-transform
of an exponential in the discrete world is 1=.1   ziz 1/ which is equal to z=.z   zi/, not 1=.z  zi/. We can adjust for this by finding the partial fraction expansion of X.z/
z . If
X.z/
z
D
X
Ai
z   zi
then
X.z/ D
X
Ai
z
z   zi
and now we can proceed as we did with the Laplace transform.
Let’s look at some examples of finding the inverse Z-transform using this method.
Example 27.1
Let’s find the inverse transform of
X.z/ D
3z2   5
6z
z2   7
12z C 1
12
jzj > 1
3

27. THE INVERSE Z-TRANSFORM
345
The first thing we do is write the denominator in terms of factors.
X.z/
D
3z2   5
6z
z2   7
12z C 1
12
jzj > 1
3
D
z

3z   5
6


z   1
4
 
z   1
3

and
X.z/
z
D
3z   5
6

z   1
4
 
z   1
3

Now lets find the partial fraction expansion of X.z/=z.
3z   5
6

z   1
4
 
z   1
3
 D
A
z   1
4
C
B
z   1
3
and
A
D
3z   5
6
z   1
3
ˇˇˇzD 1
4
D
3
4   5
6
1
4   1
3
D
9   10
12
3   4
12
D
1

346
27. THE INVERSE Z-TRANSFORM
Similarly
B
D
3z   5
6
z   1
4
ˇˇˇzD 1
3
D
3
3   5
6
1
3   1
4
D
1
6
1
12
D
2
Therefore,
X.z/
z
D
1
z   1
4
C
2
z   1
3
jzj > 1
3
and
X.z/ D
z
z   1
4
C
2z
z   1
3
jzj > 1
3
or dividing top and bottom by z,
X.z/ D
1
1   1
4z 1
C
2
1   1
3z 1
jzj > 1
3
and we can find xŒn, the inverse transform of X.z/ as
xŒn D
1
4
n
uŒn C 2
1
3
n
uŒn
Example 27.2
What is the response of a filter with impulse response
h.n/ D
1
2
n
uŒn
if the input is a unit step?

27. THE INVERSE Z-TRANSFORM
347
We know the Z-transform of
xŒn D anuŒn
to be
X.z/ D
1
1   az 1
jzj > jaj
Therefore, we know that
H.z/ D
1
1   1
2z 1
but what about the unit step uŒn? If we look at the equation for xŒn above we can see that if
we set a D 1, then xŒn D uŒn. Therefore,
Z ŒuŒn D
1
1   z 1
jzj > 1
(As an aside, note that this means that a filter with impulse response uŒn would not be BIBO
stable as the region of convergence for the Z-transform does not include the unit circle). Using
the convolution property we can write
Y Œz D
1
1   1
2z 1 
1
1   z 1
jzj > 1
Multiplying top and bottom by z2 we get
Y Œz D
z2

z   1
2

.z   1/
or
Y.z/
z
D
z

z   1
2

.z   1/
Using partial fraction expansion we can write Y.z/=z as
Y.z/
z
D
A
z   1
2
C
B
z   1
where
A
D
z
z   1
ˇˇˇzD 1
2
D
1
2
1
2   1
D
 1

348
27. THE INVERSE Z-TRANSFORM
and
B
D
z
z   1
2
jzD1
D
1
1   1
2
D
2
Therefore,
Y.z/
D
2z
z   1  z
z   1
2
D
2
1
1   z 1  1
1   1
2z 1
and therefore,
yŒn
D
2uŒn  1
2
n
uŒn
D

2  1
2
n
uŒn
We can confirm this last result using our favorite method—discrete time convolution.
Example 27.3
Using convolution find the output of a filter with impulse response
hŒn D
1
2
n
uŒn
when the input is xŒn D uŒn
yŒn
D
xŒn ~ hŒn
D
1
X
kD 1
xŒn   khŒk
D
1
X
kD 1
uŒn   k
1
2
k
uŒk

27. THE INVERSE Z-TRANSFORM
349
The term uŒk will make the lower limit go to 0 while the term uŒn   k will make the upper
limit n as long as n  0. For n < 0, yŒn D 0. For n  0
yŒn
D
n
X
kD0
1
2
k
D
1
2
0
 1
2
nC1
1   1
2
D
2
 
1  1
2
nC1!
D
2  1
2
n
or
yŒn D

2  1
2
n
uŒn
and we end up with the same result.
Let’s take a look at one more example this time for a two sided sequence.
Example 27.4
Let’s find the inverse transform of
X.z/ D
3z2   5
6z
z2   7
12z C 1
12
1
4 < jzj < 1
3
The rational polynomial we are using here is the same as the one in the first example, but the
region of convergence is different. In the first example we had used a region of convergence
outside of a circle of radius 1/3 resulting in a right sided sequence. In this case we have an
annular region of convergence which will result in a two sided sequence. We already know what
the partial fraction expansion of this function will give us.
X.z/ D
z
z   1
4
C
2z
z   1
3
1
4 < jzj < 1
3

350
27. THE INVERSE Z-TRANSFORM
For the first term the pole is at 1=4 and the region of convergence is outside of the circle with
radius 1=4. Therefore, the inverse Z-transform will be a right sided sequence.
Z 1
2
64
z
z   1
4
I jzj > 1
4
3
75 D
1
4
n
uŒn
The second term has a pole at z D 1=3 and has a region of convergence inside a circle with
radius 1/3. Therefore, the inverse transform will be a left sided sequence.
Z 1
2
64
2z
z   1
3
I jzj < 1
3
3
75 D  2
1
3
n
uŒ n   1
Combining these together we get
Z 1
2
64
3z2   5
6z
z2   7
12z C 1
12
1
4 < jzj < 1
3
3
75 D
1
4
n
uŒn   2
1
3
n
uŒ n   1
27.1
INVERTING BY DIVIDING
There is a third way of finding the inverse Z-transform which makes use of the form of the
Z-transform equation. Remember that
X.z/ D
1
X
nD 1
xŒnz n
For a right sided sequence the summation starts from 0 and we can expand the summation as
X.z/ D
1
X
nD0
xŒnz n D xŒ0 C xŒ1z 1 C xŒ2z 2 C xŒ3z 3 C   
So if could write X.z/ as a polynomial in z 1 we could glean xŒn from there. One way we can
do that is by long division. Let’s see how this work by example.
Example 27.5
Previously we have found that the inverse z-transform of
Y Œz D
z2

z   1
2

.z   1/
jzj > 1

27.1. INVERTING BY DIVIDING
351
is given by
yŒn D

2  1
2
n
uŒn
Or yŒ0 D 2   1 D 1, yŒ1 D 2   1
2 D 1:5, yŒ2 D 2    1
2
2 D 1:75, etc.
Let’s expand Y.z/ using long division.
1
C1:5z 1
C1:75z 2
z2   1:5z C :5p
z2
z2
 1:5z
C:5
1:5z
 0:5
1:5z
 2:25
C:75z 1
1:75
 0:75z 1
1:75
 2:625z 1
C0:875
1:875z 1
 0:875
  
Comparing the results of the division, 1 C 1:5z 1 C 1:75z 2 C    , to yŒ0 C yŒ1z 1 C
yŒ2z 2 C    we can see that yŒ0 D 1, yŒ1 D 1:5, yŒ2 D 1:75, etc.
How would we use long division if the we were dealing with a left sided sequence?
Example 27.6
Suppose we wanted to find the inverse Z-transform of
Y.z/ D
z2

z   1
2

.z   1/
jzj < 1
2
For a left sided sequence we know that the Z-transform will look something like
Y.z/ D
 1
X
nD 1
yŒnz n D yŒ1z C yŒ2z2 C yŒ3z4 C   
Let’s redo the previous division but let’s flip the divisor

352
27. THE INVERSE Z-TRANSFORM
2z2
C6z3
C14z4
0:5   1:5z C z2p
z2
z2
 3z3
C2z4
3z3
 2z4
3z3
 9z4
C6z5
7z4
 6z5
7z4
 21z5
C14z6
15z5
 14z6
  
Comparing the quotient 2z2 C 6z3 C 10z4 C    to yŒ1z C yŒ2z2 C yŒ3z3 C yŒ4z4 C
   we can see that we have
yŒ1
D
0
yŒ2
D
2
yŒ3
D
6
yŒ4
D
14
:::
:::
We can verify that this is indeed the right result by taking the inverse Z-transform using
our favorite method of expanding Y.z/=z using partial fraction expansion
27.2
SUMMARY
We have introduced three different ways of finding the inverse Z-transform.
1. We can evaluate the inverse Z-transform integral using the Cauchy residue theorem.
2. We can expand X.z/=z using partial fraction expansion so that we can write X.z/ as a sum
of terms of the form z=.z   pi/ or z=.z   p1/2. Based on the region of convergence the
corresponding discrete time sequences will be of the form pn
i uŒn, pn
i uŒ n   1,npn
i uŒn,
or npn
i uŒ n   1.
3. We can carry out long division to get a polynomial in terms of z n where the way we
arrange the divisor depends on the region of convergence.
Of these methods we will find that the second one is almost always the most convenient
(your mileage though may differ).

27.3. EXERCISES
353
27.3
EXERCISES
(Answers on the following page)
1. Find the inverse Z-transform of
X.z/ D
z=6
z2   5
6z C 1=6
I
jzj > 1
2
2. Find the inverse Z-transform of
X.z/ D
z
6z2   5z C 1I
jzj > 1
2
3. Find the inverse Z-transform of
X.z/ D
 1:5z
z2   2:5z C 1I
1
2 < jzj < 2
4. Find the inverse Z-transform of
X.z/ D
z=6
z2   5
6z C 1=6
I
jzj < 1
3
5. A causal linear time invariant system is characterized by the following input/output rela-
tionship.
yŒn D 1
6xŒn C 5
6yŒn   1   1
6yŒn   2
What is the impulse response of this system?
6. A causal linear time invariant system is characterized by the following input/output rela-
tionship.
yŒn D xŒn   xŒn   2   0:25yŒn   2
What is the impulse response of this system?
7. If the transfer function for a linear time invariant system is given by
H.z/ D
z2   1
2z

z   1
4
2 I
jzj > 1
2
Find the impulse response hŒn.

354
27. THE INVERSE Z-TRANSFORM
27.4
ANSWERS
1.
xŒn D
1
2
n
uŒn  1
3
n
uŒn
2.
xŒn D
1
2
n
uŒn  1
3
n
uŒn
3.
xŒn D
1
2
jnj
4.
xŒn D  1
2
n
uŒ n   1 C
1
3
n
uŒ n   1
5.
hŒn D
1
2
n
uŒn  1
3
n
uŒn
6.
hŒn D  4ıŒn C 5.0:5/n cos
n
2

uŒn
7.
hŒn D

 1
2
n
uŒn

355
M O D U L E
28
Filters and Difference
Equations
In the s-domain we could get an idea about the frequency domain behavior
of a filter by going up and down the j! axis. Depending on how close
or how far we were from the poles and zeros of the transfer function we
would get either an amplification of the response or an attenuation of the
response. When we map from the Laplace domain to the z-domain the
j! axis maps into the unit circle. When we do the mapping we have to
remember that we are looking at the frequency profile of a sampled signal.
Recall that in order to be able to recover the frequency profile of a signal
we had to keep the frequency profile free from aliasing we had to sample
at more than twice the highest frequency. The other side of this is that given a sampled signal the
only frequencies we can see are those that are less than half the sampling frequency. When we
sampled a signal the frequency profile repeated every fs Hz where fs is the sampling frequency.
So, as we go around the unit circle we are going from a frequency of zero Hz to a frequency of
fs Hz . Keeping this in mind if we want to look at the frequency domain behavior of a discrete
time system we can do that by moving around the unit circle and seeing how close or how far
we are from the poles and zeros of the transfer function.
Using the same logic we can also design systems to have certain frequency characteristics.
We can place zeros close to or on the unit circle at points that correspond to frequencies we wish
to attenuate or eliminate. We can put poles close to points on the unit circle that correspond to
frequencies we wish to enhance. The closer the pole is to the unit circle the higher the level of
amplification.
Let’s see how this work using a simple example.
28.1
FREQUENCY RESPONSE AND POLES AND ZEROS
Consider the system with two complex conjugate poles and a zero at z D 1 shown in Figure 28.1.
In the following Figures 28.2–28.6 we plot the magnitude of the frequency response as
we go around the unit circle. At a frequency of 0, because we have a zero at z D 1 the response
is zero.
As we traverse the unit circle and come close to the pole the magnitude of the frequency
response increases. It peaks at the value of ! for which we are closest to the pole and then de-

356
28. FILTERS AND DIFFERENCE EQUATIONS
Im[z]
Re[z]
ω
Figure 28.1: A two pole one zero system.
Im[z]
Re[z]
ω
Figure 28.2: Beginning the traversal.
Im[z]
Re[z]
ω
Figure 28.3: Approaching the pole.

28.1. FREQUENCY RESPONSE AND POLES AND ZEROS
357
Im[z]
Re[z]
ω
Figure 28.4: Past the pole.
Im[z]
Re[z]
ω
Figure 28.5: Far from the poles.
Im[z]
Re[z]
ω
Figure 28.6: Traversing the circle in both directions.

358
28. FILTERS AND DIFFERENCE EQUATIONS
Im[z]
Re[z]
Figure 28.7: Pole and zero placement for a simple low pass filter.
35
30
25
20
15
10
5
0
0
50
100
150
200
250
f (Hz)
|H( f )|
300
350
400
450
500
Figure 28.8: Frequency response of a simple two pole two zero low pass filter.
creases as we move away from the pole. The same happens as we move in the negative frequency
direction from 0 to  .
This filter would be called a bandpass filter—not a particularly good one but still a band-
pass filter—as it lets through a band of frequencies.
If we place a pole on the positive real axis we will be closest to it when we begin our
traversal of the unit circle so the signal will be enhanced at frequencies close to zero. A zero
placed anywhere on the unit circle will result in the blocking of the frequencies at the zero. The
placement of poles and zeros shown in Figure 28.7 results in only the lower frequencies getting
through as shown in Figure 28.8, therefore, it is called a low pass filter.
In Figure 28.9 we show a system with two zeros at z D 1 and poles at 0:9e˙j 
10 .

28.1. FREQUENCY RESPONSE AND POLES AND ZEROS
359
Im[z]
Re[z]
Figure 28.9: Pole and zero placement for a simple high pass filter.
1.2
1.0
0.8
0.6
0.4
0.2
0.0
0
50
100
150
200
250
f (Hz)
|H( f )|
300
350
400
450
500
Figure 28.10: Frequency response of a simple two pole two zero high pass filter.
Now the zero frequency or dc component is blocked and the lower frequencies are atten-
uated. The attenuation is then canceled by the poles. This behavior can be seen in the frequency
response plotted in Figure 28.10.
We have not said anything about the labels on the frequency axis in these figures. One
reason is that the sampling frequency used to generate the discrete time signal from the contin-
uous time signal has not been specified. If we know the sampling frequency !s in radians or fs
in Hz, we can map !s=2 or fs=2 to  and then divide up the span from z D 1 to z D  1 on the
unit circle in a uniform manner between 0 and !s=2 or fs=2.

360
28. FILTERS AND DIFFERENCE EQUATIONS
Example 28.1
Find the frequency response of a system with transfer function
H.z/ D 1   1
z I jzj > 1
Would you classify this as a low-pass, a high-pass, or a band-pass filter?
We can do this in two different ways. The first approach is to figure out where the poles and
zeros are and then qualitatively evaluate their effect for different frequency ranges. The second
approach is a more exact computational approach where we actually evaluate the magnitude of
the transfer function on the unit circle. Let us begin with the first approach.
We haven’t been told what the sampling frequency associated with this system is so we
will look at the behavior between ! D 0 to ! D . To find the pole zero locations let’s write
H.z/ as a rational polynomial.
H.z/ D z   1
z
This system has one pole and one zero; the pole is at the origin so it is equidistant from all points
on the unit circle, and the zero is at z D 1. The zero at z D 1 will block the zero frequency and
attenuate the frequencies close to zero. As we traverse the unit circle and ! increases the distance
from the zero will increase and the effect of the zero will be canceled out by the effect of the
pole. Thus, the low frequencies will be attenuated or blocked and the high frequencies will be
unaffected. This means the system is a high-pass filter.
The second way is to evaluate the magnitude of the transfer function on the unit circle.
For all points on the unit circle the magnitude of z is 1, so on the unit circle z D ej!. To evaluate
H.z/ on the unit circle all we need to do is to replace z with ej!.
H.ej!/ D ej!   1
ej!
To find the magnitude we take the product of this transfer function with its complex conjugate
and then take the square root of the product.
jH.ej!/j2
D
ej!   1
ej!
 e j!   1
e j!
D
1   ej!   e j! C 1
1
D
2   2 cos.!/
Therefore
jH.ej!/j D
p
2   2 cos.!/
We have plotted this in Figure 28.11. We can see that this is a high pass response even if the filter
characteristics leave something to be desired. Compare this with the high-pass filter response
shown in Figure 28.10. That was a two pole two zero system as oppose to this system which

28.2. FINDING THE INPUT OUTPUT RELATIONSHIP
361
2.0
1.8
1.6
1.4
1.2
1.0
0.8
0.6
0.4
0.2
0.0
0.0
0.5
1.0
1.5
2.0
3.0
2.5
ω
|H(e jω)|
Figure 28.11: Frequency response of a simple single pole single zero high pass filter.
consists of only a single pole and a single zero. You can see how much better the magnitude
response is when you increase the number of poles and zeros.
28.2
FINDING THE INPUT OUTPUT RELATIONSHIP
Given the pole-zero placement we cannot only obtain the frequency response, we can also obtain
the difference equation for the system which has this particular frequency response. In this
particular example the zero is at z D 1. Let’s assume the poles are at p1 and p
1. Then the transfer
function is given by
H.z/ D
z   1
.z   p1/.z   p
1/ D
z   1
z2   p1z   zp
1 C jp1j2
or
H.z/ D
z   1
z2   2Re.p1/z C jp1j2
where we have used the fact that the sum of a complex number and its complex conjugate is
equal to twice the real part of the complex number. If we now divide top and bottom by z2 we
get
H.z/ D
z 1   z 2
1   2Re.p1/z 1 C jp1j2z 2
To get the difference equation we begin with the fact that
Y.z/ D H.z/X.z/

362
28. FILTERS AND DIFFERENCE EQUATIONS
Substituting for H.z/ in this equation we get
Y.z/ D
z 1   z 2
1   2Re.p1/z 1 C jp1j2z 2 X.z/
Multiplying both sides by the denominator of the right hand side
Y.z/
 1   2Re.p1/z 1 C jp1j2z 2
D X.z/
 z 1   z 2
or
Y.z/   2Re.p1/z 1Y.z/ C jp1j2z 2Y.z/ D z 1X.z/   z 2X.z/
Using the time shift property the inverse Z-transform of both sides is
yŒn   2Re.p1/yŒn   1 C jp1j2yŒn   2 D xŒn   1   xŒn   2
or
yŒn D xŒn   1   xŒn   2 C 2Re.p1/yŒn   1   jp1j2yŒn   2
Example 28.2
Find the input output relationship for the system with transfer function
H.z/ D
z3
z3 C 2z2   3
Dividing top and bottom by z3 we get
H.z/ D
1
1 C 2z 1   3z 3
Setting this equal to Y.z/=X.z/ and cross multiplying we get
Y.z/
 1 C 2z 1   3z 3
D X.z/
or
Y.z/ C 2z 1Y.z/   3z 3Y.z/ D X.z/
Using linearity and the time shift property
Z ŒxŒn   no D z noX.z/
we get
yŒn C 2yŒn   1   3yŒn   3 D xŒn
or
yŒn D xŒn   2yŒn   1 C 3yŒn   3

28.3. DESIGNING A SIMPLE DISCRETE TIME FILTER
363
28.3
DESIGNING A SIMPLE DISCRETE TIME FILTER
What if we are interested in specifying the frequency response of the system in terms of Hz? In
order this we first have to know the sampling frequency fs. This is because all locations around
the unit circle (if we are thinking of frequency in Hz) is determined by the sampling frequency.
Let’s suppose we want our system to enhance the components of the signal at frequency fp Hz
and attenuate the signal at frequency fz Hz. In order to place the complex conjugate poles to
at fp with magnitude rp and the complex conjugate zeros at fz with magnitude rz our system
transfer function will be.
H.z/ D

z   rzej2 fz
fs
 
z   rze j 2 fz
fs


z   rpej2 fp
fs
 
z   rpe j 2 fp
fs

Note that the closer the magnitudes of the poles and zeros, rp and rz, are to 1 the more will be
their impact. For notational convenience let’s define
˛
D
rz cos

2 fz
fs

ˇ
D
rp cos

2 fp
fs

where ˛ is the real part of the zero and ˇ is the real part of the pole. Following the same procedure
as above we can find the difference equation for this filter as
yŒn D xŒn   2˛xŒn   1 C r2
z xŒn   2 C 2ˇyŒn   1   r2
pyŒn   2
A MATLAB implementation of this filter is shown below.
fs = 16000 % sampling frequency
duration = 2; % duration in seconds
Ns = duration*fs; % Number of samples
fz = 1000; %location of zero in Hz
fp = 500; % location of pole in Hz
rz = 1; % magnitude of zero
rp = .99; % magnitude of pole;
alpha
= rz*cos((fz/fs)*2*pi)
beta = rp*cos((fp/fs)*2*pi)
y = zeros(Ns,1);
t = [1:1:Ns]/fs;

364
28. FILTERS AND DIFFERENCE EQUATIONS
while(1)
fprobe = input(’Enter probe frequency: ’)
x = cos(2*pi*fprobe*t);
sound(x,fs);
pause
for i= 3:1:Ns
y(i) = x(i) - 2*alpha*x(i-1) + rz*rz*x(i-2) + 2*beta*y(i-1)
- rp*rp*y(i-2);
end
sound(y,fs);
% plot input and output
figure
subplot(2,1,1)
plot(t,x)
axis([0,.1,-2,2])
title(’input waveform’)
subplot(2,1,2)
plot(t,y)
axis([0,.1,-2,2])
title(’output waveform’)
end
We can see the frequency shaping effect of the filter if we excite the filter with random
noise instead of different probe frequencies. In the MATLAB program this would mean replac-
ing the line
x = cos(2*pi*fprobe*t);
with
x = randn(Ns,1);

28.3. DESIGNING A SIMPLE DISCRETE TIME FILTER
365
2
1
0
–1
–2
0.00
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.10
2
1
0
–1
–2
0.00
0.01
0.02
0.03
0.04
0.05
Input waveform
Normalized output waveform
0.06
0.07
0.08
0.09
0.10
Figure 28.12: Input and output signals for a two pole two zero system with the zeros at the
origin and the poles near the unit circle.
This will generate a Gauassian random number sequence of length Ns. Lets also move the zeros
to the origin by setting rz = 0 so we see just the effect of the poles. To magnify the effect of
the poles we move them close to the unit circle by setting rp = 0.99. The input and output
waveforms are shown in Figure 28.12. The output was normalized because as a result of the
poles being close to the unit circle the magnitude of the response was significantly larger than
the magnitude of the input signal. You can see how the filter is blocking most of the frequency
content except for the frequency where the poles are located.
Example 28.3
Suppose you have a signal sampled at 480 samples per second which has been
contaminated by the power line so that you have an unwanted 60 Hz component in the signal.
You want to block that component. You can block or attenuate a frequency component using a
pair of zeros in the system. Where would you place these zeros?
The zeros are going to be a complex conjugate pair. We want to block the signal not
attenuate it so the zeros will be placed on the unit circle. This means the magnitude of the zeros
will be unity. The angle of the zeros is given by
z D 60
480  2 D 
4
Therefore the zeros will be at e˙j 
4 , or in cartesian coordinates at 0:707 ˙ j 0:707.

366
28. FILTERS AND DIFFERENCE EQUATIONS
Example 28.4
Suppose you have a signal sampled at 480 samples per second with a component
at 120 Hz that you want to enhance. Given a choice of the following pole pairs which should
you pick if you want maximum enhancement of this component?
1. 0:6 ˙ j 0:6
2. ˙0:9j
3. 0:45 ˙ 0:78
4. ˙0:75j
If the component is at 120 Hz the poles we need should have an angle of
p D 120
480  2 D 
2
Choices 1 and 3 correspond to poles with angles clearly different from =2. Choices 2 and 4
both correspond to poles with angles of ˙=2. Of these two the poles in option 2, ˙0:9j are
closer to the unit circle and hence with more of an impact. Therefore, we should place poles at
˙0:9j.
We can also look at the impulse response of this filter. Here is the MATLAB code we will
be using:
clear all
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
Initialization
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
rp = 1; % Magnitude of pole
fp = 10; % pole location in Hz
fs = 500; % sampling frequency in Hz
Ns = 500; % Number of samples
t = [1:1:Ns]/fs; % time points
alpha = rp * cos(2*pi*(fp/fs)); % real part of pole
y = zeros(Ns,1);
x = zeros(Ns,1);

28.4. SUMMARY
367
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
Filtering
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
x(1) = 1;
y(1) = x(1);
y(2) = x(2) + 2*alpha*y(1);
for i= 3:1:Ns
y(i) = x(i)+2*alpha*y(i-1)-rp*rp*y(i-2);
end
plot(t,y)
In this program we have set the input to be an impulse. The value of x.1/ is set to 1 and
all other values of the input remain set to 0. As in the previous program ˛ is the real part of the
pole and rp is the magnitude of the pole. We have set the angle of the pole to be 10 Hz. As the
sampling frequency is 500 Hz the poles are at e˙j 20
500 .
Let’s first look at what happens when we place the poles on the unit circle. This is the same
as putting poles on the j! axis in the Laplace domain. Recall that the Laplace transform of a
pure sinusoid results in poles on the j! axis. Therefore, we would expect the impulse response
of this system to be a pure sinusoid. This is the case as we can see from the response shown in
Figure 28.13.
If we move the poles back a bit from the origin by setting the magnitude to be 0.99 we
obtain the impulse response shown in Figure 28.14.
We can see that after some oscillations the response dies down. If we pull the poles even
further back with a magnitude of 0.9 the damping effect is much more pronounced as evident
from the plot of the response in Figure 28.15.
28.4
SUMMARY
In this module we showed how we can figure out the frequency response of a system based on
the location of its poles and zeros. We only examined stable systems so we kept our poles inside
the unit circle.
1. Moving around the unit circle in the z-domain is like moving along the j! axis in the
Laplace domain.

368
28. FILTERS AND DIFFERENCE EQUATIONS
8
6
4
2
0
–2
–4
–6
–8
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Figure 28.13: Response of a two pole two zero system with the zeros at the origin and the poles
on the unit circle at an angle corresponding to 10 Hz.
8
6
4
2
0
–2
–4
–6
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Figure 28.14: Response of a two pole two zero system with the zeros at the origin and the poles
near the unit circle with a magnitude of 0.99 and an angle corresponding to 10 Hz.
2. The frequency demarcation depends on the sampling frequency with the angle of zero
corresponding to zero and the angle  corresponding to half the sampling frequency.
3. The points on the unit circle which are closer to poles correspond to frequencies which are
enhanced by the system. The closer the pole is to the unit circle the more pronounced the
effect.

28.4. SUMMARY
369
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
–0.5
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Figure 28.15: Response of a two pole two zero system with the zeros at the origin and the poles
near the unit circle with a magnitude of 0.9 and an angle corresponding to 10 Hz.
4. The points on the unit circle which are closer to zeros correspond to frequencies which are
attenuated by the system. The closer the zero is to the unit circle the more pronounced the
effect.
5. We can design a simple system by placing poles near points of the circle corresponding to
frequencies we want to enhance and zeros near points of the circle corresponding to fre-
quencies we want to attenuate. From the pole zero plot we can obtain the transfer function
H.z/.
6. Given a transfer function we can write it in terms of z 1 from which we can extract a
difference equation which can be easily implemented.

370
28. FILTERS AND DIFFERENCE EQUATIONS
28.5
EXERCISES
(Answers on the following page)
1. You have a real signal which has been sampled at 240 samples/second. You want to design
a two pole, two zero discrete time filter with the following specifications.
•
The filter should block the component of the signal at 60 Hz.
•
The filter should enhance the component of the signal at 30 Hz.
•
The magnitudes of the poles should be 0.9.
(a) Specify the pole zero locations for this filter.
(b) Specify the difference equation for this filter.
2. Given the input output relationship
yŒn D xŒn   xŒn   2
(a) Where are the poles and zeros of this filter?
(b) What kind of a filter is it?
3. A biological signal has been sampled at 480 samples per second. There is a component of
the signal around 120 Hz which is of particular interest and there is a 60 cycle interference
from the power line. Specify the locations of the poles and zeros of a two-zero two pole
discrete time filter which will remove this 60 cycle hum while enhancing the component
of the signal at 120 Hz.
4. Given a system with a pole at z D  0:9 and a zero at z D 0:9, would the best description
of this system be that it was a low pass filter or a high pass filter.
5. Given a system with a pole at z D 0:9 and a zero at z D  0:9, would the best description
of this system be that it was a low pass filter or a high pass filter.
6. Is the system described by the difference equation
yŒn D xŒn C xŒn   1
a low pass filter, a band pass filter, or a high pass filter?
7. Is the system described by the difference equation
yŒn D xŒn   xŒn   1
a low pass filter, a band pass filter, or a high pass filter?
8. Is the system described by the difference equation
yŒn D xŒn   0:5yŒn   2
a low pass filter, a band pass filter, or a high pass filter?

28.6. ANSWERS
371
28.6
ANSWERS
1.
(a) Zero locations: z D ˙j, Pole locations: z D 0:9e˙j 
4 .
(b)
yŒn D xŒn C xŒn   2 C 1:8
p
2
yŒn   1   0:81yŒn   2
2.
(a) Zeros at z D ˙1, poles at z D 0.
(b) Band pass filter.
3. Poles at z D ˙0:9j, Zeros at
1
p
2 ˙ j
1
p
2.
4. High pass filter.
5. Lowpass filter.
6. Low pass filter.
7. High pass filter.
8. Band pass filter.


373
M O D U L E
29
Discrete-Time Feedback
Systems
As with continuous time systems complex discrete time systems depend on
feedback for many of their characteristics. Just as in the case of continuous
time systems we can use feedback to move the poles and zeros of a system
to stabilize it or to shape its response. In practical terms there are however
differences—some of the computational aspects are different enough to
trip you up if you are not careful.
Let’s begin with obtaining the general transfer function for a feed-
back system in the z domain. Consider the system shown in Figure 29.1.
As in the case of our analysis of the feedback system in the Laplace
domain let’s define E.z/ to be the output of the summer.
E.z/ D X.z/   B.z/Y.z/
then
Y.z/
D
E.z/A.z/
D
.X.z/   B.z/Y.z//A.z/
D
X.z/A.z/   A.z/B.z/Y.z/
Therefore,
Y.z/ .1 C A.z/B.z/ D A.z/X.z//
and the transfer function is
H.z/ D
A.z/
1 C A.z/B.z/
29.1
STABILIZING A SYSTEM
Let’s take a simple example of an unstable system and see how we can use feedback to move the
poles and stabilize the system. Lets take A.z/ to be our favorite one pole system but with a pole
outside the unit circle
A.z/ D
1
1   2z 1 D
z
z   2

374
29. DISCRETE-TIME FEEDBACK SYSTEMS
x[n]
y[n]
A(z)
+
−
B(z)
Figure 29.1: A feedback system.
and let’s let B.z/ be a gain Kz 1. Notice that unlike the situation in the continuous time case
for this simple example we have used B.z/ D Kz 1 instead of B.z/ D K. There is a reason for
this. Usually in a discrete time system the feedback from the output is not instantaneous and
there is a delay between a signal appearing at the output and being fed back to appear as the
feedback component. Think in terms of the digital circuits you might have built. The z 1 in the
feedback path accounts for this delay. The overall transfer function with this B.z/ is
H.z/ D
A.z/
1 C Kz 1A.z/
Substituting the expression for A.z/ we get
H.z/ D
z
z   2
1 C Kz 1 
z
z   2
D
z
z   2 C K
from which we can see that we have a pole at 2   K.
It is at this point you have to be a bit careful. In the case of the continuous-time systems
in order for a system to be causal the region of convergence is to the right of a boundary—
where the boundary is defined by the right-most pole. For stability we want the j! axis to be
in the region of convergence. Therefore, for a causal system to be stable we need the poles to
be in the left half plane. In this particular case that would mean picking K <  1. However, we
are analyzing a discrete-time system. And for a discrete-time system to be causal the region of
convergence is to the outside of a circular boundary—the radius of the circular boundary being
defined by the poles furthest from the origin. For the system to be stable we want the region of
the convergence to include the unit circle. Therefore, for a causal system to be stable we need the
poles to be inside the unit circle. That is we need j2   Kj < 1. This will happen when 1 < K < 3.
This is a point which trips up a lot of people. Be aware of it.
Example29.1
Consider the situation we have all probably seen or participated in at one time or
another. Someone steps up to a microphone, begins speaking, and there is this annoying squeal
that has everyone wincing. You turn down the volume a bit or move the speakers further away
from the microphone and the squealing stops. Let’s model this system and see why this is so.

29.2. PRODUCING A DESIRED OUTPUT
375
Let’s make the forward path simply be the gain of the amplifier. Therefore,
A.z/ D A
In the feedback path we will have some gain or attenuation ˛ and a delay—let’s assume the delay
is one time instant. So,
B.z/ D ˛z 1
The feedback is positive so the output of the summer will be
E.z/ D X.z/ C ˛z 1Y.z/
and
Y.z/ D AE.z/ D AX.z/ C A˛Y.z/
Therefore, the transfer function for the overall system is
H.z/
D
A
1   A˛z 1
D
Az
z   A˛
This system has a pole at A˛. If the value of A˛ is greater than one then the pole will be outside
the unit circle and we will get the squealing. We can get rid of the squealing by making sure
that jA˛j < 1. We can do this in a number of different ways. We can reduce the gain of the
microphone to reduce A. Or we can move the speakers further away from the microphone so
the value of ˛ goes down and the pole moves inside the unit circle.
29.2
PRODUCING A DESIRED OUTPUT
As in the case of continuous-time systems we often want the discrete-time system to generate
a desired output. We accomplish this by driving the error between the desired response and the
actual response to zero. In the control systems literature the system whose response we are trying
to control is often called the plant while the system we use to control the system is called the
controller. The common setup for this is shown in Figure 29.2.
In this figure the block with transfer function A.z/ is the plant and the block marked B.z/
is the controller. Unlike the continuous time control system the output is not directly fed back.
Instead there is a delay in the feedback path for the reason mentioned above. As in the case of
continuous time systems the control signal can be proportional to the error signal, or it can be
the sum of a term proportional to the error and the integral of the error, or it can have a term
which is proportional to the error and a term which is the derivative of the error, or it can be the
sum of terms proportional to the error the integral of the error and the derivative of the error.
Let’s take a look at the simplest case first.

376
29. DISCRETE-TIME FEEDBACK SYSTEMS
x[n]
y[n]
A(z)
+
−
B(z)
z–1
Figure 29.2: The control system setup.
x[n]
y[n]
Hm(z)
+
−
K
z–1
Figure 29.3: A simple motor controller.
29.3
PROPORTIONAL CONTROL
Suppose we want issue commands to move a robotic arm and we have a really simple model for
the motor. At each time instant the new position yŒn is the sum of the previous position and
the input at that time.
yŒn D yŒn   1 C xŒn   1
The transfer function for this simple motor model can be obtained by taking the Z transform of
both sides.
Y.z/ D z 1Y.z/ C z 1X.z/
Moving the Y.z/ terms to the left hand side of the equation and finding the ratio of Y.z/ to
X.z/ we get the transfer function for the motor as
Hm.z/ D
z 1
1   z 1
We would like to control this motor so that we can give it a position and have the motor track
the position. That is, given a desired position xŒn we would like the difference between the
measurement of the actual position yŒn and the desired position xŒn to be zero. Let’s try to do
this using proportional control. The block diagram for the overall system is shown in Figure 29.3.
Notice the z 1 in the feedback loop. We have included that because we would need at
least one time instant to feed the measurement of the position back to the input. Let’s now find

29.3. PROPORTIONAL CONTROL
377
the transfer function for the overall system.
E.z/
D
X.z/   z 1Y.z/
Y.z/
D
E.z/  KHm.z/
D
Kz 1
1   z 1 E.z/
D
Kz 1  X.z/   z 1Y.z/

1   z 1
D
Kz 1X.z/   Kz 2Y.z/
1   z 1
Multiplying both sides by 1   z 1 and rearranging we can obtain the overall transfer function
as
H.z/ D Y.z/
X.z/ D
Kz 1
Kz 2   z 1 C 1
Multiplying top and bottom by z2 we get
H.z/ D
Kz
z2   z C K
The poles for this system are at
z1;2 D 1
2 ˙
p
1   4K
2
We can see that we will get real poles as long as K < 1=4 and complex poles otherwise. Fur-
thermore the poles migrate outside the unit circle for K > 1. So we can expect the system to go
unstable for values of K > 1.
After all of this—to paraphrase Kipling1—we still really haven’t answered that age old
question “but does it work?” We can’t build one of these right now but we could simulate it. We
can use the version of the transfer function in terms of z 1 to generate the difference equation
for this system and use that to simulate the system.
Given that
H.z/ D Y.z/
X.z/ D
Kz 1
Kz 2   z 1 C 1
Y.z/
 Kz 2   z 1 C 1

D Kz 1X.z/
or
Kz 2Y.z/   z 1Y.z/ C Y.z/ D Kz 1X.z/
Taking the inverse Z transform we get the difference equation
yŒn D KxŒn   1 C yŒn   1   KyŒn   2
1Google The Conundrum of the Workshops.

378
29. DISCRETE-TIME FEEDBACK SYSTEMS
Here is some MATLAB code to implement this difference equation.
clear all
Ns = 500; % number of samples
y = zeros(Ns,1); % motor position
x = zeros(Ns,1); % desired position
K = 0.2; % Proportional controller
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
Generate the desired signal
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
for k= 1:1:100
x(k) = 1;
end
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
Simulate the system
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
y(2) = K*x(1);
for k=3:1:Ns
y(k) = K*x(k-1) + y(k-1) - K*y(k-2);
end
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
Plot
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
figure
subplot(2,1,1)
plot(x)

29.3. PROPORTIONAL CONTROL
379
1.5
1.0
0.5
0.0
0
50
100
150
200
250
300
350
400
450
500
1.5
1.0
0.5
0.0
0
50
100
150
200
250
300
350
400
450
500
Output
Figure 29.4: A simple motor controller, with gain K D 0:2.
axis([0 Ns -0.2 1.5])
subplot(2,1,2)
title(’Control Signal’)
plot(y)
axis([0 Ns -0.2 1.5])
title(’Output’)
Let’s run the simulation for different values of K. Let’s begin with K D 0:2. We want the
output to be 1 for a while and then go to 0. The result of the simulation is shown in Figure 29.4.
Notice that it takes a few samples—about sixteen—for the system output to track the
input. The response is smooth as we would expect for a system with only real poles.
Lets increase the gain to K D 0:5. The resulting performance is shown in Figure 29.5.
Notice a couple of things about this response. First the output follows the input much
more rapidly than in the previous case. The bad news is that we have some ringing during sharp
transitions in the control signal. This is a result of the poles becoming complex. This ringing
may or may not be acceptable depending on the application. However, what is definitely not
acceptable is what happens when the gain is increased beyond 1. Figure 29.6 shows what happens
when we set K D 1:001.

380
29. DISCRETE-TIME FEEDBACK SYSTEMS
1.5
1.0
0.5
0.0
0
50
100
150
200
250
300
350
400
450
500
1.5
1.0
0.5
0.0
0
50
100
150
200
250
300
350
400
450
500
Output
Figure 29.5: A simple motor controller, with gain K D 0:5.
1.5
1.0
0.5
0.0
0
50
100
150
200
250
300
350
400
450
500
2
1
0
–1
–2
0
50
100
150
200
250
300
350
400
450
500
Output
Figure 29.6: A simple motor controller, with gain K D 1:001.

29.4. DISCRETE TIME PID CONTROLLERS
381
Clearly not a nice situation.
29.4
DISCRETE TIME PID CONTROLLERS
Just as in the continuous time case, in the discrete-time case we may want to get rid of small
steady state errors using integral control or use some level of proactive control using a derivative
term in the control. To see what some possible equivalents are in the discrete world let’s take
a look at the equivalent systems for integration and differentiation in the discrete world. The
analog to integration in continuous time is summation in discrete time. So the analog to an
integrator in continuous time with input-output relationship
y.t/ D
Z t
 1
x./d
is the system in discrete time
yŒn D
n
X
kD 1
xŒk
The impulse response of this discrete-time integrator is
hŒn D
n
X
kD 1
ıŒk D
(
1
n  0
0
otherwise
D uŒn
The Z-transform of this impulse response is
H.z/ D
1
1   z 1 I
jzj > 1
Therefore if we wanted to use a PI controller, the controller instead of simply being K would be
given by Kp C Ki=.1   z 1/.
Similarly a derivative in continuous time can be approximated with a difference in discrete
time
yŒn D xŒn   xŒn   1
We can see that the Z-transform of the derivative operator would be 1   z 1. Therefore the
PID controller would have the form Kp C Ki=.1   z 1/ C Kd.1   z 1/.
As in the continuous time case the integrator in the controller allows you to reduce steady
state error and the derivative allows you to anticipate and therefore speeds up your response.
Here is some MATLAB code for a PID controller for the simple motor.
clear all
Ns = 500; % number of samples

382
29. DISCRETE-TIME FEEDBACK SYSTEMS
y = zeros(Ns,1); % motor position
x = zeros(Ns,1); % desired position
Kp = 0.3; % Proportional controller
Ki = 0.001; % Integral controller
Kd = 0.2; % derivative term
a = Kp + Ki + Kd;
b = Kp + 2*Kd;
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
Generate the desired signal
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
for k= 1:1:100
x(k) = 1;
end
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
Simulate the system
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
y(2) = a*x(1);
y(3) = a*x(2) -b*x(1) + 2*y(2);
y(4) = a*x(3) -b*x(2) + Kd*x(1) +2*y(3) -(a+1)*y(2);
for k=5:1:Ns
y(k) = a*x(k-1) - b*x(k-2) + Kd*x(k-3) + 2*y(k-1) - (a+1)*y(k-2)
+ b*y(k-3) - Kd*y(k-4);
end
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
Plot
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

29.5. SUMMARY
383
figure
subplot(2,1,1)
plot(x,’-k’,’LineWidth’,2)
axis([0 Ns -0.2 1.5])
title(’Control Signal’)
subplot(2,1,2)
plot(x,’:k’,’LineWidth’,2)
hold on
plot(y,’-k’,’LineWidth’,2)
axis([0 Ns -0.2 1.5])
title(’Output’)
hold off
29.5
SUMMARY
In this module we introduced the idea of feedback and showed how we can use feedback to
stabilize a system, to make it more responsive, to obtain the inverse of a system, and to control
the output of a system in order to get a desired response.

384
29. DISCRETE-TIME FEEDBACK SYSTEMS
x[n]
y[n]
H1(z)
+
−
K
Figure 29.7: A simple discrete time feedback system.
x[n]
y[n]
A(z)
+
−
B(z)
Figure 29.8: Another simple feedback system.
29.6
EXERCISES
(Answers on the following page)
1. The difference equation describing the system with transfer function H1.z/ (Figure 29.7)
is given by
yŒn D xŒn C 2yŒn   1
(a) Find the difference equation relating the input and output of the overall system.
(b) Find the values of K for which the system is stable.
2. In the system shown in Figure 29.8
A.z/
D
1   z 1
B.z/
D
z 1
Find the difference equation relating xŒn and yŒn.
3. In Figure 29.7
H1.z/ D
z
z   0:9
If K D 0:8
(a) Is the system stable?
(b) What is the difference equation relating xŒn and yŒn?

29.6. EXERCISES
385
x[n]
y[n]
A(z)
+
−
B(z)
z–1
Figure 29.9: Feedback system using a control system formulation.
4. In the system shown in Figure 29.9 the plant A.z/ and the PI controller B(z) are given by
A.z/
D
z 1
1   z 1
B.z/
D
Kp C
Ki
1   z 1
Find the difference equation relating xŒn and yŒn.

386
29. DISCRETE-TIME FEEDBACK SYSTEMS
29.7
ANSWERS
1.
(a)
yŒn D
K
1 C K xŒn C
2
1 C K yŒn   1
(b) K > 1 or K <  3.
2.
yŒn D xŒn   xŒn   1   yŒn   1 C yŒn   2
3.
(a) Yes it is stable for K D 0:8
(b)
yŒn D 4
9xŒn   1
2yŒn   1
4.
yŒn D.Kp C Ki/xŒn   1   KpxŒn   2   2yŒn   1
  .K   p C Ki C 1/yŒn   2 C KpyŒn   3

387
Author’s Biography
KHALID SAYOOD
Khalid Sayood received the B.S. and M.S. degrees in electrical engineering from the University
of Rochester, Rochester, NY, in 1977 and 1979, respectively, and the Ph.D. degree in electri-
cal engineering from Texas A&M University, College Station, in 1982. From 1995 to 1996,
he served as the Founding Head of the Computer Vision and Image Processing Group at the
Turkish National Research Council Informatics Institute. From 1996 to 1997, he was a Visiting
Professor with Bogazici University, Istanbul, Turkey. Since 1982, he has been with the Uni-
versity of Nebraska–Lincoln, where he is currently serving as a Professor in the Department of
Electrical and Computer Engineering. He is the author of Introduction to Data Compression, 5th
ed., Morgan Kaufmann, 2017, Understanding Circuits: Learning Problem Solving Using Circuit
Analysis, Morgan & Claypool, 2005, Learning Programming Using MATLAB, Morgan& Clay-
pool, 2006, Joint Source Channel Coding Using Arithmetic Codes, Morgan & Claypool, 2010, and
Computational Genomic Signatures, Morgan & Claypool, 2011, and the Editor of Lossless Com-
pression Handbook, Academic Press, 2002. His research interests include bioinformatics, data
compression, and biological signal processing.


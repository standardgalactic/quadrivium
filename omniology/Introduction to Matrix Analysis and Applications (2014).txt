Universitext
Fumio Hiai
Dénes Petz
Introduction 
to Matrix 
Analysis and 
Applications

Universitext

Universitext
Series Editors
Sheldon Axler
San Francisco State University, San Francisco, CA, USA
Vincenzo Capasso
Università degli Studi di Milano, Milan, Italy
Carles Casacuberta
Universitat de Barcelona, Barcelona, Spain
Angus MacIntyre
Queen Mary University of London, London, UK
Kenneth Ribet
University of California at Berkeley, Berkeley, CA, USA
Claude Sabbah
CNRS École Polytechnique Centre de mathématiques, Palaiseau, France
Endre Süli
University of Oxford, Oxford, UK
Wojbor A. Woyczynski
Case Western Reserve University, Cleveland, OH, USA
For further volumes:
http://www.springer.com/series/223
Universitext is a series of textbooks that presents material from a wide variety of
mathematical disciplines at master’s level and beyond. The books, often well
class-tested by their author, may have an informal, personal, even experimental
approach to their subject matter. Some of the most successful and established
books in the series have evolved through several editions, always following the
evolution of teaching curricula, into very polished texts.
Thus as research topics trickle down into graduate-level teaching, ﬁrst textbooks
written for new, cutting-edge courses may make their way into Universitext.

Fumio Hiai
• Dénes Petz
Introduction to Matrix
Analysis and Applications
123

Fumio Hiai
Graduate School of Information Sciences
Tohoku University
Sendai
Japan
Dénes Petz
Alfréd Rényi Institute of Mathematics
Budapest
Hungary
Department for Mathematical Analysis
Budapest University of Technology
and Economics
Budapest
Hungary
ISSN 0172-5939
ISSN 2191-6675
(electronic)
ISBN 978-3-319-04149-0
ISBN 978-3-319-04150-6
(eBook)
DOI 10.1007/978-3-319-04150-6
Springer Cham Heidelberg New York Dordrecht London
Library of Congress Control Number: 2013957552
Mathematics Subject Classiﬁcation: 15A99, 15A60
 Hindustan Book Agency 2014. Published by Springer International Publishing Switzerland 2014
This work is subject to copyright. All rights are reserved by the Publishers’, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or
information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed. Exempted from this legal reservation are brief
excerpts in connection with reviews or scholarly analysis or material supplied speciﬁcally for the
purpose of being entered and executed on a computer system, for exclusive use by the purchaser of the
work. Duplication of this publication or parts thereof is permitted only under the provisions of
the Copyright Law of the Publishers’ location, in its current version, and permission for use must
always be obtained from Springer. Permissions for use may be obtained through RightsLink at the
Copyright Clearance Center. Violations are liable to prosecution under the respective Copyright Law.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt
from the relevant protective laws and regulations and therefore free for general use.
While the advice and information in this book are believed to be true and accurate at the date of
publication, neither the authors nor the editors nor the publisher can accept any legal responsibility for
any errors or omissions that may be made. The publisher makes no warranty, express or implied, with
respect to the material contained herein.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)
A co-publication with the Hindustan Book Agency, New Delhi, licensed for sale in all
countries outside of India. Sold and distributed within India by the Hindustan Book Agency,
P 19 Green Park Extn., New Delhi 110 016, India.
HBA ISBN 978-93-80250-60-1

Preface
The material of this book is partly based on the lectures of the authors given at the
Graduate School of Information Sciences of Tohoku University and at the
Budapest University of Technology and Economics. The aim of the lectures was to
explain certain important topics in matrix analysis from the point of view of
functional analysis. The concept of Hilbert space appears many times, but only
ﬁnite-dimensional spaces are used. The book treats some aspects of analysis
related to matrices including such topics as matrix monotone functions, matrix
means, majorization, entropies, quantum Markov triplets, and so on. There are
several popular matrix applications in quantum theory.
The book is organized into seven chapters. Chapters 1–3 form an introductory
part of the book and could be used as a textbook for an advanced undergraduate
special topics course. The word ‘‘matrix’’ was ﬁrst introduced in 1848 and
applications subsequently appeared in many different areas. Chapters 4–7 contain a
number of more advanced and less well-known topics. This material could be used
for an advanced specialized graduate-level course aimed at students who wish to
specialize in quantum information. But the best use for this part is as a reference
for active researchers in the ﬁeld of quantum information theory. Researchers in
statistics, engineering, and economics may also ﬁnd this book useful.
Chapter 1 introduces the basic notions of matrix analysis. We prefer the Hilbert
space concepts, so complex numbers are used. The spectrum and eigenvalues are
important, and the determinant and trace are used later in several applications. The
ﬁnal section covers tensor products and their symmetric and antisymmetric sub-
spaces. The chapter concludes with a selection of exercises. We point out that in
this book ‘‘positive’’ means  0; we shall not use the term ‘‘non-negative.’’
Chapter 2 covers block matrices, partial ordering, and an elementary theory of
von Neumann algebras in the ﬁnite-dimensional setting. The Hilbert space concept
requires projections, i.e., matrices P satisfying P ¼ P2 ¼ P. Self-adjoint matrices
are linear combinations of projections. Not only are single matrices required, but
subalgebras of matrices are also used. This material includes Kadison’s inequality
and completely positive mappings.
Chapter 3 details matrix functional calculus. Functional calculus provides a new
matrix f(A) when a matrix A and a function f are given. This is an essential tool in
matrix theory as well as in operator theory. A typical example is the exponential
v

function eA ¼ P1
n¼0 An=n! If f is sufﬁciently smooth, then f(A) is also smooth and
we have a useful Fréchet differential formula.
Chapter 4 covers matrix monotone functions. A real function deﬁned on an
interval is matrix monotone if A  B implies fðAÞ  fðBÞ for Hermitian matrices
A and B whose eigenvalues are in the domain of f. We have a beautiful theory of
such functions, initiated by Löwner in 1934. A highlight is the integral expression
of such functions. Matrix convex functions are also considered. Graduate students
in mathematics and in information theory will beneﬁt from having all of this
material collected into a single source.
Chapter 5 covers matrix (operator) means for positive matrices. Matrix
extensions of the arithmetic mean a þ b
ð
Þ=2 and the harmonic mean
a1 þ b1
2

1
are rather trivial, however it is nontrivial to deﬁne matrix version of the geometric
mean
ﬃﬃﬃﬃﬃ
ab
p
. This was ﬁrst done by Pusz and Woronowicz. A general theory of
matrix means developed by Kubo and Ando is closely related to operator mono-
tone functions on ð0; 1Þ. There are also more complicated means. The mean
transformation MðA; BÞ :¼ mðLA; RBÞ is a mean of the left-multiplication LA and
the right-multiplication RB, recently studied by Hiai and Kosaki. Another useful
concept is a multivariable extension of two-variable matrix means.
Chapter 6 discusses majorizations for eigenvalues and singular values of matri-
ces. Majorization is a certain order relation between two real vectors. Section 6.1
recalls classical material that can be found in other sources. There are several famous
majorizations for matrices which have strong applications to matrix norm
inequalities in symmetric norms. For instance, an extremely useful inequality is the
Lidskii–Wielandt theorem.
The last chapter contains topics related to quantum applications. Positive
matrices with trace 1, also called density matrices, are the states in quantum
theory. The relative entropy appeared in 1962, and matrix theory has found many
applications in the quantum formalism. The unknown quantum states can be
described via the use of positive operators F(x) with P
x FðxÞ ¼ I. This is called a
POVM and a few mathematical results are shown, but in quantum theory there are
much more relevant subjects. These subjects are close to the authors’ interests, and
there are some very recent results.
The authors thank several colleagues for useful communications. They are
particularly grateful to Prof. Tsuyoshi Ando for insightful comments and to Prof.
Rajendra Bhatia for valuable advice.
April 2013
Fumio Hiai
Dénes Petz
vi
Preface

Contents
1
Fundamentals of Operators and Matrices . . . . . . . . . . . . . . . . . . .
1
1.1
Basics on Matrices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Hilbert Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.3
Jordan Canonical Form . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.4
Spectrum and Eigenvalues . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
1.5
Trace and Determinant. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
1.6
Positivity and Absolute Value . . . . . . . . . . . . . . . . . . . . . . . . .
28
1.7
Tensor Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
1.8
Notes and Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
1.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
2
Mappings and Algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
2.1
Block Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
2.2
Partial Ordering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
2.3
Projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
2.4
Subalgebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
2.5
Kernel Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
2.6
Positivity-Preserving Mappings . . . . . . . . . . . . . . . . . . . . . . . .
85
2.7
Notes and Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
2.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
3
Functional Calculus and Derivation . . . . . . . . . . . . . . . . . . . . . . .
101
3.1
The Exponential Function. . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
3.2
Other Functions. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
3.3
Derivation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
3.4
Fréchet Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
3.5
Notes and Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130
3.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
4
Matrix Monotone Functions and Convexity. . . . . . . . . . . . . . . . . .
137
4.1
Some Examples of Functions . . . . . . . . . . . . . . . . . . . . . . . . .
138
4.2
Convexity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
4.3
Pick Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159
4.4
Löwner’s Theorem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
vii

4.5
Some Applications. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
172
4.6
Notes and Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182
4.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
5
Matrix Means and Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . .
187
5.1
The Geometric Mean . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
5.2
General Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
196
5.3
Mean Examples. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
5.4
Mean Transformation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
212
5.5
Notes and Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
222
5.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
223
6
Majorization and Singular Values . . . . . . . . . . . . . . . . . . . . . . . .
227
6.1
Majorization of Vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
227
6.2
Singular Values. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
6.3
Symmetric Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241
6.4
More Majorizations for Matrices . . . . . . . . . . . . . . . . . . . . . . .
253
6.5
Notes and Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
268
6.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
269
7
Some Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
273
7.1
Gaussian Markov Property . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
7.2
Entropies and Monotonicity . . . . . . . . . . . . . . . . . . . . . . . . . .
277
7.3
Quantum Markov Triplets. . . . . . . . . . . . . . . . . . . . . . . . . . . .
288
7.4
Optimal Quantum Measurements. . . . . . . . . . . . . . . . . . . . . . .
292
7.5
The Cramér–Rao Inequality . . . . . . . . . . . . . . . . . . . . . . . . . .
306
7.6
Notes and Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
320
7.7
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
321
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
323
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
327
viii
Contents

Chapter 1
Fundamentals of Operators and Matrices
A linear mapping is essentially a matrix if the vector space is ﬁnite-dimensional.
In this book the vector space is typically a ﬁnite-dimensional complex Hilbert
space. The ﬁrst chapter collects introductory materials on matrices and operators.
Section1.2 is a concise exposition of Hilbert spaces. The polar and spectral decompo-
sitions useful in studying operators on Hilbert spaces are also essential for matrices.
AﬁnerdecompositionformatricesistheJordancanonicalformdescribedinSect.1.3.
Among the most basic notions for matrices are eigenvalues, singular values, trace and
determinant, included in the subsequent sections. A less elementary but important
subject is tensor products, discussed in the last section.
1.1 Basics on Matrices
For n, m ∈N, Mn×m = Mn×m(C) denotes the space of all n×m complex matrices. A
matrix M ∈Mn×m is a mapping {1, 2, . . . , n}×{1, 2, . . . , m} →C. It is represented
as an array with n rows and m columns:
M =
⎡
⎢⎢⎢⎣
m11 m12 · · · m1m
m21 m22 · · · m2m
...
...
...
...
mn1 mn2 · · · mnm
⎤
⎥⎥⎥⎦,
where mij is the intersection of the ith row and the jth column. If the matrix is denoted
by M, then this entry is denoted by Mij. If n = m, then we write Mn instead of Mn×n.
A simple example is the identity matrix In ∈Mn deﬁned as mij = δi,j, or
F. Hiai and D. Petz, Introduction to Matrix Analysis and Applications,
1
Universitext, DOI: 10.1007/978-3-319-04150-6_1,
© Hindustan Book Agency 2014

2
1
Fundamentals of Operators and Matrices
In =
⎡
⎢⎢⎢⎣
1 0 · · · 0
0 1 · · · 0
... ... ... ...
0 0 · · · 1
⎤
⎥⎥⎥⎦.
Mn×m is a complex vector space of dimension nm. The linear operations are
deﬁned as follows:
[λA]ij := λAij,
[A + B]ij := Aij + Bij,
where λ is a complex number and A, B ∈Mn×m.
Example 1.1 For i, j = 1, . . . , n let E(ij) be the n × n matrix such that the (i, j)-
entry is equal to one and all other entries are equal to zero. Then E(ij) are called the
matrix units and form a basis of Mn:
A =
n

i,j=1
AijE(ij).
In particular,
In =
n

i=1
E(ii) .
If A ∈Mn×m and B ∈Mm×k, then the product AB of A and B is deﬁned by
[AB]ij =
m

α=1
AiαBαj,
where 1 ≤i ≤n and 1 ≤j ≤k. Hence AB ∈Mn×k. So Mn becomes an algebra.
The most signiﬁcant feature of matrices is the non-commutativity of the product
AB ̸= BA. For example,
	0 1
0 0

 	0 0
1 0

=
	1 0
0 0

,
	0 0
1 0

 	0 1
0 0

=
	0 0
0 1

.
In the matrix algebra Mn, the identity matrix In behaves as a unit: InA = AIn = A
for every A ∈Mn. The matrix A ∈Mn is invertible if there is a B ∈Mn such that
AB = BA = In. This B is called the inverse of A, and is denoted by A−1.
□
Example 1.2 The linear equations
ax + by = u

1.1 Basics on Matrices
3
cx + dy = v
can be written in a matrix formalism:
	a b
c d

 	x
y

=
	u
v

.
If x and y are the unknown parameters and the coefﬁcient matrix is invertible, then
the solution is
	x
y

=
	a b
c d

−1 	u
v

.
So the solution of linear equations is based on the inverse matrix, which is formulated
in Theorem 1.33.
□
The transpose At of the matrix A ∈Mn×m is an m × n matrix,
[At]ij = Aji
(1 ≤i ≤m, 1 ≤j ≤n).
It is easy to see that if the product AB is deﬁned, then (AB)t = BtAt. The adjoint
matrix A∗is the complex conjugate of the transpose At. The space Mn is a *-algebra:
(AB)C = A(BC),
(A + B)C = AC + BC,
A(B + C) = AB + AC,
(A + B)∗= A∗+ B∗,
(λA)∗= ¯λA∗,
(A∗)∗= A,
(AB)∗= B∗A∗.
Let A ∈Mn. The trace of A is the sum of the diagonal entries:
Tr A :=
n

i=1
Aii.
It is easy to show that Tr AB = Tr BA, see Theorem 1.28.
The determinant of A ∈Mn is slightly more complicated:
det A :=

π
(−1)σ(π)A1π(1)A2π(2) . . . Anπ(n),
(1.1)
where the sum is over all permutations π of the set {1, 2, . . . , n} and σ(π) is the
parity of the permutation π. Therefore
det
	a b
c d

= ad −bc,
and another example is the following:

4
1
Fundamentals of Operators and Matrices
det
⎡
⎣
A11 A12 A13
A21 A22 A23
A31 A32 A33
⎤
⎦
= A11 det
	A22 A23
A32 A33

−A12 det
	A21 A23
A31 A33

+ A13 det
	A21 A22
A31 A33

.
It can be proven that
det(AB) = (det A)(det B).
1.2 Hilbert Space
Let H be a complex vector space. A functional ⟨· , · ⟩: H×H →C of two variables
is called an inner product if it satisﬁes:
(1) ⟨x + y, z⟩= ⟨x, z⟩+ ⟨y, z⟩
(x, y, z ∈H);
(2) ⟨λx, y⟩= λ⟨x, y⟩
(λ ∈C, x, y ∈H);
(3) ⟨x, y⟩= ⟨y, x⟩
(x, y ∈H);
(4) ⟨x, x⟩≥0 for every x ∈H and ⟨x, x⟩= 0 only for x = 0.
Condition (2) states that the inner product is conjugate linear in the ﬁrst variable (and
it is linear in the second variable). The Schwarz inequality
⟨x, y⟩
2 ≤⟨x, x⟩⟨y, y⟩
(1.2)
holds. The inner product determines a norm for the vectors:
∪x∪:=

⟨x, x⟩.
This has the properties
∪x + y∪≤∪x∪+ ∪y∪
and
|⟨x, y⟩| ≤∪x∪· ∪y∪.
∪x∪is interpreted as the length of the vector x. A further requirement in the deﬁnition
of a Hilbert space is that every Cauchy sequence must be convergent, that is, the space
is complete. (In the ﬁnite-dimensional case, completeness always holds.)
The linear space Cn of all n-tuples of complex numbers becomes a Hilbert space
with the inner product

1.2 Hilbert Space
5
⟨x, y⟩=
n

i=1
xiyi = [x1, x2, . . . , xn]
⎡
⎢⎢⎢⎣
y1
y2
...
yn
⎤
⎥⎥⎥⎦,
where z denotes the complex conjugate of the complex number z ∈C. Another exam-
ple is the space of square integrable complex-valued functions on the real Euclidean
space Rn. If f and g are such functions then
⟨f , g⟩=

Rn f (x) g(x) dx
gives the inner product. The latter space is denoted by L2(Rn) and, in contrast to the
n-dimensional space Cn, it is inﬁnite-dimensional. Below we are mostly concerned
with ﬁnite-dimensional spaces.
If ⟨x, y⟩= 0 for vectors x and y of a Hilbert space, then x and y are called
orthogonal, denoted x ⊥y. When H ⊂H, H⊥:= {x ∈H : x ⊥h for every h ∈H}
is called the orthogonal complement of H. For any subset H ⊂H, H⊥is a closed
subspace.
A family {ei} of vectors is called orthonormal if ⟨ei, ei⟩= 1 and ⟨ei, ej⟩= 0 if
i ̸= j. A maximal orthonormal system is called a basis or orthonormal basis. The
cardinality of a basis is called the dimension of the Hilbert space. (The cardinality
of any two bases is the same.)
In the space Cn, the standard orthonormal basis consists of the vectors
δ1 = (1, 0, . . . , 0),
δ2 = (0, 1, 0, . . . , 0),
. . . ,
δn = (0, 0, . . . , 0, 1);
(1.3)
each vector has 0 coordinate n −1 times and one coordinate equals 1.
Example 1.3 The space Mn of matrices becomes a Hilbert space with the inner
product
⟨A, B⟩= Tr A∗B
which is called Hilbert–Schmidt inner product. The matrix units E(ij) (1 ≤i, j ≤
n) form an orthonormal basis.
It follows that the Hilbert–Schmidt norm
∪A∪2 :=

⟨A, A⟩=
√
Tr A∗A =
⎛
⎝
n

i,j=1
|Aij|2
⎞
⎠
1/2
(1.4)
is a norm for the matrices.
□

6
1
Fundamentals of Operators and Matrices
Assume that in an n-dimensional Hilbert space, linearly independent vectors
v1, v2, . . . , vn are given. By the Gram–Schmidt procedure an orthonormal basis
can be obtained by linear combinations:
e1 :=
1
∪v1∪v1,
e2 :=
1
∪w2∪w2
with w2 := v2 −⟨e1, v2⟩e1,
e3 :=
1
∪w3∪w3
with w3 := v3 −⟨e1, v3⟩e1 −⟨e2, v3⟩e2,
...
en :=
1
∪wn∪wn
with wn := vn −⟨e1, vn⟩e1 −· · · −⟨en−1, vn⟩en−1.
The next theorem tells us that any vector has a unique Fourier expansion.
Theorem 1.4 Let e1, e2, . . . be a basis in a Hilbert space H. Then for any vector
x ∈H the expansion
x =

n
⟨en, x⟩en
holds. Moreover,
∪x∪2 =

n
|⟨en, x⟩|2.
Let H and K be Hilbert spaces. A mapping A : H →K is called linear if it
preserves linear combinations:
A(λf + μg) = λAf + μAg
(f , g ∈H,
λ, μ ∈C).
The kernel and the range of A are
ker A := {x ∈H : Ax = 0},
ran A := {Ax ∈K : x ∈H}.
The dimension formula familiar in linear algebra is
dim H = dim(ker A) + dim(ran A).
The quantity dim(ran A) is called the rank of A and is denoted by rank A. It is easy
to see that rank A ≤dim H, dim K.
Let e1, e2, . . . , en be a basis of the Hilbert space H and f1, f2, . . . , fm be a basis of
K. The linear mapping A : H →K is determined by the vectors Aej, j = 1, 2, . . . , n.

1.2 Hilbert Space
7
Furthermore, the vector Aej is determined by its coordinates:
Aej = c1,jf1 + c2,jf2 + · · · + cm,jfm.
The numbers ci,j, 1 ≤i ≤m, 1 ≤j ≤n, form an m × n matrix, which is called the
matrix of the linear transformation A with respect to the bases (e1, e2, . . . , en) and
(f1, f2, . . . , fm). If we want to distinguish the linear operator A from its matrix, then
the latter will be denoted by [A]. We have
[A]ij = ⟨fi, Aej⟩
(1 ≤i ≤m,
1 ≤j ≤n).
Note that the order of the basis vectors is important. We shall mostly consider linear
operators of a Hilbert space into itself. Then only one basis is needed and the matrix
of the operator has the form of a square. So a linear transformation and a basis yield
a matrix. If an n × n matrix is given, then it can be always considered as a linear
transformation of the space Cn endowed with the standard basis (1.3).
The inner product of the vectors |x⟩and |y⟩will often be denoted as ⟨x|y⟩. This
notation, sometimes called bra and ket, is popular in physics. On the other hand,
|x⟩⟨y| is a linear operator which acts on the vector |z⟩as

|x⟩⟨y|

|z⟩:= |x⟩⟨y|z⟩≡⟨y|z⟩|x⟩.
Therefore,
|x⟩⟨y| =
⎡
⎢⎢⎢⎢⎣
x1
x2
.
.
xn
⎤
⎥⎥⎥⎥⎦

y1, y2, . . . , yn

is conjugate linear in |y⟩, while ⟨x|y⟩is linear in |y⟩.
The next example shows the possible use of the bra and ket.
Example 1.5 If X, Y ∈Mn(C), then
n

i,j=1
Tr E(ij)XE(ji)Y = (Tr X)(Tr Y).
Since both sides are bilinear in the variables X and Y, it is enough to check the case
X = E(ab) and Y = E(cd). Simple computation gives that the left-hand side is
δabδcd and this is the same as the right-hand side.
Another possibility is to use the formula E(ij) = |ei⟩⟨ej|. So

8
1
Fundamentals of Operators and Matrices

i,j
Tr E(ij)XE(ji)Y =

i,j
Tr |ei⟩⟨ej|X|ej⟩⟨ei|Y =

i,j
⟨ej|X|ej⟩⟨ei|Y|ei⟩
=

j
⟨ej|X|ej⟩

i
⟨ei|Y|ei⟩
and the last expression is (Tr X)(Tr Y).
□
Example 1.6 Fix a natural number n and let H be the space of polynomials of degree
at most n. Assume that the variable of these polynomials is t and the coefﬁcients are
complex numbers. The typical elements are
p(t) =
n

i=0
uiti
and
q(t) =
n

i=0
viti.
If their inner product is deﬁned as
⟨p(t), q(t)⟩:=
n

i=0
uivi,
then {1, t, t2, . . . , tn} is an orthonormal basis.
Differentiation is a linear operator on H:
n

k=0
uktk ⊃→
n

k=1
kuktk−1 .
With respect to the above basis, its matrix is
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0 1 0 . . . 0 0
0 0 2 . . . 0 0
0 0 0 . . . 0 0
...
...
... ... ... 0
0 0 0 . . . 0 n
0 0 0 . . . 0 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
This is an upper triangular matrix; the (i, j) entry is 0 if i > j.
□
Let H1, H2 and H3 be Hilbert spaces and ﬁx a basis in each of them. If B : H1 →
H2 and A : H2 →H3 are linear mappings, then the composition
f ⊃→A(Bf ) ∈H3
(f ∈H1)
is linear as well and it is denoted by AB. The matrix [AB] of the composition AB can
be computed from the matrices [A] and [B] as follows:

1.2 Hilbert Space
9
[AB]ij =

k
[A]ik[B]kj.
The right-hand side is deﬁned to be the product [A] [B] of the matrices [A] and [B],
that is, [AB] = [A] [B] holds. It is obvious that for an α × m matrix [A] and an m × n
matrix [B], their product [A] [B] is an α × n matrix.
LetH1 andH2 beHilbertspacesandﬁxabasisineachofthem.IfA, B : H1 →H2
are linear mappings, then their linear combination
(λA + μB)f ⊃→λ(Af ) + μ(Bf )
is a linear mapping and
[λA + μB]ij = λ[A]ij + μ[B]ij.
Let H be a Hilbert space. The linear operators H →H form an algebra. This
algebra B(H) has a unit, the identity operator denoted by I, and the product is non-
commutative. Assume that H is n-dimensional and ﬁx a basis. Then to each linear
operator A ∈B(H) an n × n matrix A is associated. The correspondence A ⊃→[A] is
an algebraic isomorphism from B(H) to the algebra Mn(C) of n × n matrices. This
isomorphism shows that the theory of linear operators on an n-dimensional Hilbert
space is the same as the theory of n × n matrices.
Theorem 1.7 (Riesz–Fischer theorem) Let φ : H →C be a linear mapping on a
ﬁnite-dimensional Hilbert space H. Then there is a unique vector v ∈H such that
φ(x) = ⟨v, x⟩for every vector x ∈H.
Proof: Let e1, e2, . . . , en be an orthonormal basis in H. Then we need a vector
v ∈H such that φ(ei) = ⟨v, ei⟩. The vector
v =

i
φ(ei)ei
will satisfy the condition.
□
The linear mappings φ : H →C are called functionals. If the Hilbert space
is not ﬁnite-dimensional, then in the previous theorem the boundedness condition
|φ(x)| ≤c∪x∪should be added, where c is a positive number.
Let H and K be ﬁnite-dimensional Hilbert spaces. The operator norm of a linear
operator A : H →K is deﬁned as
∪A∪:= sup{∪Ax∪: x ∈H, ∪x∪= 1} .
It can be shown that ∪A∪is ﬁnite. In addition to the common properties ∪A + B∪≤
∪A∪+ ∪B∪and ∪λA∪= |λ|∪A∪, submultiplicativity

10
1
Fundamentals of Operators and Matrices
∪AB∪≤∪A∪∪B∪
also holds.
If ∪A∪≤1, then the operator A is called a contraction.
The set of linear operators A : H →H is denoted by B(H). The convergence
An →A means ∪An −A∪→0 in terms of the operator norm deﬁned above. But
in the case of a ﬁnite-dimensional Hilbert space, the Hilbert–Schmidt norm can also
be used. Unlike the Hilbert–Schmidt norm, the operator norm of a matrix is not
expressed explicitly by the matrix entries.
Example 1.8 Let A ∈B(H) and ∪A∪< 1. Then I −A is invertible and
(I −A)−1 =
∞

n=0
An.
Since
(I −A)
N

n=0
An = I −AN+1
and
∪AN+1∪≤∪A∪N+1,
we can see that the limit of the ﬁrst equation is
(I −A)
∞

n=0
An = I.
This proves the statement, the formula of which is called a Neumann series.
□
Let H and K be Hilbert spaces. If T : H →K is a linear operator, then its adjoint
T∗: K →H is determined by the formula
⟨x, Ty⟩K = ⟨T∗x, y⟩H
(x ∈K, y ∈H).
An operator T ∈B(H) is called self-adjoint if T∗= T. An operator T is self-adjoint
if and only if ⟨x, Tx⟩is a real number for every vector x ∈H. For the self-adjoint
operators on H and the self-adjoint n × n matrices the notations B(H)sa and Msa
n are
used.
Theorem 1.9 The properties of the adjoint are:
(1) (A + B)∗= A∗+ B∗, (λA)∗= λA∗
(λ ∈C);
(2) (A∗)∗= A, (AB)∗= B∗A∗;
(3) (A−1)∗= (A∗)−1 if A is invertible;
(4) ∪A∪= ∪A∗∪, ∪A∗A∪= ∪A∪2.

1.2 Hilbert Space
11
Example 1.10 Let A : H →H be a linear operator and e1, e2, . . . , en be a basis in
the Hilbert space H. The (i, j) element of the matrix of A is ⟨ei, Aej⟩. Since
⟨ei, Aej⟩= ⟨ej, A∗ei⟩,
this is the complex conjugate of the (j, i) element of the matrix of A∗.
If A is self-adjoint, then the (i, j) element of the matrix of A is the conjugate of
the (j, i) element. In particular, all diagonal entries are real. The self-adjoint matrices
are also called Hermitian matrices.
□
Theorem 1.11 (Projection theorem) Let M be a closed subspace of a Hilbert
space H. Any vector x ∈H can be written in a unique way in the form x = x0 + y,
where x0 ∈M and y ⊥M.
Note that a subspace of a ﬁnite-dimensional Hilbert space is always closed. The
mapping P : x ⊃→x0 deﬁned in the context of the previous theorem is called the
orthogonal projection onto the subspace M. This mapping is linear:
P(λx + μy) = λPx + μPy.
Moreover, P2 = P = P∗. The converse is also true: If P2 = P = P∗, then P is an
orthogonal projection (onto its range).
Example 1.12 A matrix A ∈Mn is self-adjoint if Aji = Aij. A particular example
is a Toeplitz matrix:
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
a1
a2
a3
. . . an−1
an
a2
a1
a2
. . . an−2 an−1
a3
a2
a1
. . . an−3 an−2
...
...
...
...
...
...
an−1 an−2 an−3 . . . a1
a2
an
an−1 an−2 . . . a2
a1
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
where a1 ∈R.
□
An operator U ∈B(H) is called a unitary if U∗is the inverse of U. Then U∗U = I
and
⟨x, y⟩= ⟨U∗Ux, y⟩= ⟨Ux, Uy⟩
for any vectors x, y ∈H. Therefore the unitary operators preserve the inner prod-
uct. In particular, orthogonal unit vectors are mapped by a unitary operator onto
orthogonal unit vectors.

12
1
Fundamentals of Operators and Matrices
Example 1.13 The permutation matrices are simple unitaries. Let π be a permu-
tation of the set {1, 2, . . . , n}. The Ai,π(i) entries of A ∈Mn(C) are 1 and all others
are 0. Every row and every column contain exactly one 1 entry. If such a matrix A is
applied to a vector, it permutes the coordinates:
⎡
⎣
0 1 0
0 0 1
1 0 0
⎤
⎦
⎡
⎣
x1
x2
x3
⎤
⎦=
⎡
⎣
x2
x3
x1
⎤
⎦.
This shows the reason behind the terminology. Another possible formalism is
A(x1, x2, x3) = (x2, x3, x1).
□
An operator A ∈B(H) is called normal if AA∗= A∗A. It immediately follows that
∪Ax∪= ∪A∗x∪
for any vector x ∈H. Self-adjoint and unitary operators are normal.
The operators we need are mostly linear, but sometimes conjugate-linear oper-
ators appear. σ : H →K is conjugate-linear if
σ(λx + μy) = λ σx + μ σy
for any complex numbers λ and μ and for any vectors x, y ∈H. The adjoint σ∗of a
conjugate-linear operator σ is determined by the equation
⟨x, σy⟩K = ⟨y, σ∗x⟩H
(x ∈K, y ∈H).
(1.5)
A mapping φ : H × H →C is called a complex bilinear form if φ is linear in
the second variable and conjugate linear in the ﬁrst variable. The inner product is a
particular example.
Theorem 1.14 On a ﬁnite-dimensional Hilbert space there is a one-to-one corre-
spondence
φ(x, y) = ⟨Ax, y⟩
between the complex bilinear forms φ : H × H →C and the linear operators
A : H →H.
Proof: Fix x ∈H. Then y ⊃→φ(x, y) is a linear functional. By the Riesz–Fischer
theorem, φ(x, y) = ⟨z, y⟩for a vector z ∈H. We set Ax = z.
□
The polarization identity
4φ(x, y) = φ(x + y, x + y) + iφ(x + iy, x + iy)
−φ(x −y, x −y) −iφ(x −iy, x −iy)
(1.6)

1.2 Hilbert Space
13
shows that a complex bilinear form φ is determined by its so-called quadratic form
x ⊃→φ(x, x).
1.3 Jordan Canonical Form
A Jordan block is a matrix
Jk(a) =
⎡
⎢⎢⎢⎢⎢⎣
a 1 0 · · · 0
0 a 1 · · · 0
0 0 a · · · 0
...
...
... ... ...
0 0 0 · · · a
⎤
⎥⎥⎥⎥⎥⎦
,
where a ∈C. This is an upper triangular matrix Jk(a) ∈Mk. We also use the notation
Jk := Jk(0). Then
Jk(a) = aIk + Jk
and the sum consists of commuting matrices.
Example 1.15 The matrix Jk is
(Jk)ij =

1
if
j = i + 1,
0 otherwise.
Therefore
(Jk)ij(Jk)jk =

1 if
j = i + 1 and k = i + 2,
0 otherwise.
It follows that
(J2
k )ij =

1 if
j = i + 2,
0 otherwise.
We observe that when taking the powers of Jk, the line of the 1 entries moves upward,
in particular Jk
k = 0. The matrices Jm
k (0 ≤m ≤k −1) are linearly independent.
If a ̸= 0, then det Jk(a) ̸= 0 and Jk(a) is invertible. We can search for the inverse
via the equation
(aIk + Jk)
⎛
⎝
k−1

j=0
cjJj
k
⎞
⎠= Ik.

14
1
Fundamentals of Operators and Matrices
Rewriting this equation we get
ac0Ik +
k−1

j=1
(acj + cj−1)Jj
k = Ik.
The solution is
cj = −(−a)−j−1
(0 ≤j ≤k −1).
In particular,
⎡
⎣
a 1 0
0 a 1
0 0 a
⎤
⎦
−1
=
⎡
⎣
a−1 −a−2 a−3
0
a−1 −a−2
0
0
a−1
⎤
⎦.
Computation with Jordan blocks is convenient.
□
The Jordan canonical form theorem is the following:
Theorem 1.16 Given a matrix X ∈Mn, there is an invertible matrix S ∈Mn such
that
X = S
⎡
⎢⎢⎢⎣
Jk1(λ1)
0
· · ·
0
0
Jk2(λ2) · · ·
0
...
...
...
...
0
0
· · · Jkm(λm)
⎤
⎥⎥⎥⎦S−1 = SJS−1,
where k1 + k2 + · · · + km = n. The Jordan matrix J is uniquely determined (up to a
permutation of the Jordan blocks in the diagonal).
Note that the numbers λ1, λ2, . . . , λm are not necessarily different. The theorem
is about complex matrices. Example 1.15 showed that it is rather easy to handle a
Jordan block. If the Jordan canonical decomposition is known, then the inverse can
be computed.
Example 1.17 An essential application concerns the determinant. Since det X =
det(SJS−1) = det J, it is enough to compute the determinant of the upper-triangular
Jordan matrix J. Therefore
det X =
m

j=1
λ
kj
j .
(1.7)
The characteristic polynomial of X ∈Mn is deﬁned as

1.3 Jordan Canonical Form
15
p(x) := det(xIn −X).
From the computation (1.7) we have
p(x) =
m

j=1
(x −λj)kj = xn −
⎛
⎝
m

j=1
kjλj
⎞
⎠xn−1 + · · · + (−1)n
m

j=1
λ
kj
j .
(1.8)
The numbers λj are the roots of the characteristic polynomial.
□
The powers of a matrix X ∈Mn are well-deﬁned. For a polynomial p(x) =
m
k=0 ckxk the matrix p(X) is
m

k=0
ckXk.
A polynomial q is said to annihilate a matrix X ∈Mn if q(X) = 0.
The next result is the Cayley–Hamilton theorem.
Theorem 1.18 If p is the characteristic polynomial of X ∈Mn, then p(X) = 0.
1.4 Spectrum and Eigenvalues
Let H be a Hilbert space. For A ∈B(H) and λ ∈C, we say that λ is an eigenvalue
of A if there is a non-zero vector v ∈H such that Av = λv. Such a vector v is called
an eigenvector of A for the eigenvalue λ. If H is ﬁnite-dimensional, then λ ∈C is
an eigenvalue of A if and only if A −λI is not invertible.
Generally, the spectrum σ(A) of A ∈B(H) consists of the numbers λ ∈C such
that A −λI is not invertible. Therefore in the ﬁnite-dimensional case the spectrum
is the set of eigenvalues.
Example 1.19 We show that σ(AB) = σ(BA) for A, B ∈Mn. It is enough to prove
that det(λI −AB) = det(λI −BA). Assume ﬁrst that A is invertible. We then have
det(λI −AB) = det(A−1(λI −AB)A) = det(λI −BA)
and hence σ(AB) = σ(BA).
When A is not invertible, choose a sequence εk ∈C \ σ(A) with εk →0 and set
Ak := A −εkI. Then
det(λI −AB) = lim
k→∞det(λI −AkB) = lim
k→∞det(λI −BAk) = det(λI −BA).

16
1
Fundamentals of Operators and Matrices
(Another argument appears in Exercise 3 of Chap.2.)
□
Example 1.20 In the history of matrix theory the particular matrix
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
0 1 0 . . . 0 0
1 0 1 . . . 0 0
0 1 0 . . . 0 0
... ... ... ... ... ...
0 0 0 . . . 0 1
0 0 0 . . . 1 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
(1.9)
has importance. Its eigenvalues were computed by Joseph Louis Lagrange in 1759.
He found that the eigenvalues are 2 cos jπ/(n+1) (j = 1, 2, . . . , n).
□
The matrix (1.9) is tridiagonal. This means that Aij = 0 if |i −j| > 1.
Example 1.21 Let λ ∈R and consider the matrix
J3(λ) =
⎡
⎣
λ 1 0
0 λ 1
0 0 λ
⎤
⎦.
Now λ is the only eigenvalue and (1, 0, 0) is the only eigenvector up to a constant
multiple. The situation is similar in the k×k generalization Jk(λ): λ is the eigenvalue
of SJk(λ)S−1 for an arbitrary invertible S and there is one eigenvector (up to a constant
multiple).
If X has the Jordan form as in Theorem 1.16, then all λj’s are eigenvalues. There-
fore the roots of the characteristic polynomial are eigenvalues. When λ is an eigen-
value of X, ker(X −λI) = {v ∈Cn : (X −λI)v = 0} is called the eigenspace of X
for λ. Note that the dimension of ker(X −λI) is the number of j such that λj = λ,
which is called the geometric multiplicity of λ; on the other hand, the multiplicity
of λ as a root of the characteristic polynomial is called the algebraic multiplicity.
For the above J3(λ) we can see that
J3(λ)(0, 0, 1) = (0, 1, λ),
J3(λ)2(0, 0, 1) = (1, 2λ, λ2).
Therefore (0, 0, 1) and these two vectors linearly span the whole space C3. The
vector (0, 0, 1) is called a cyclic vector.
Assume that a matrix X ∈Mn has a cyclic vector v ∈Cn which means that the
set {v, Xv, X2v, . . . , Xn−1v} spans Cn. Then X = SJn(λ)S−1 with some invertible
matrix S, so the Jordan canonical form consists of a single block.
□
Theorem 1.22 Assume that A ∈B(H) is normal. Then there exist λ1, . . . , λn ∈C
and u1, . . . , un ∈H such that {u1, . . . , un} is an orthonormal basis of H and Aui =
λiui for all 1 ≤i ≤n.

1.4 Spectrum and Eigenvalues
17
Proof: Let us prove the theorem by induction on n = dim H. The case n = 1
triviallyholds.Supposetheassertionholdsfordimensionn−1.Assumethatdim H =
n and A ∈B(H) is normal. Choose a root λ1 of det(λI −A) = 0. As explained
before the theorem, λ1 is an eigenvalue of A so that there is an eigenvector u1 with
Au1 = λ1u1. One may assume that u1 is a unit vector, i.e., ∪u1∪= 1. Since A is
normal, we have
(A −λ1I)∗(A −λ1I) = (A∗−λ1I)(A −λ1I)
= A∗A −λ1A −λ1A∗+ λ1λ1I
= AA∗−λ1A −λ1A∗+ λ1λ1I
= (A −λ1I)(A −λ1I)∗,
that is, A −λ1I is also normal. Therefore,
∪(A∗−λ1I)u1∪= ∪(A −λ1I)∗u1∪= ∪(A −λ1I)u1∪= 0
so that A∗u1 = λ1u1. Let H1 := {u1}⊥, the orthogonal complement of {u1}. If
x ∈H1 then
⟨Ax, u1⟩= ⟨x, A∗u1⟩= ⟨x, λ1u1⟩= λ1⟨x, u1⟩= 0,
⟨A∗x, u1⟩= ⟨x, Au1⟩= ⟨x, λ1u1⟩= λ1⟨x, u1⟩= 0
so that Ax, A∗x ∈H1. Hence we have AH1 ⊂H1 and A∗H1 ⊂H1. So one can
deﬁne A1 := A|H1 ∈B(H1). Then A∗
1 = A∗|H1, which implies that A1 is also
normal. Since dim H1 = n −1, the induction hypothesis can be applied to obtain
λ2, . . . , λn ∈C and u2, . . . , un ∈H1 such that {u2, . . . , un} is an orthonormal basis
of H1 and A1ui = λiui for all i = 2, . . . , n. Then {u1, u2, . . . , un} is an orthonormal
basis of H and Aui = λiui for all i = 1, 2, . . . , n. Thus the assertion holds for
dimension n as well.
□
It is an important consequence that the matrix of a normal operator is diagonal
with respect to an appropriate orthonormal basis and the trace is the sum of the
eigenvalues.
Theorem 1.23 Assume that A ∈B(H) is self-adjoint. If Av = λv and Aw = μw
with non-zero eigenvectors v, w and the eigenvalues λ and μ are different, then
v ⊥w and λ, μ ∈R.
Proof: First we show that the eigenvalues are real:
λ⟨v, v⟩= ⟨v, λv⟩= ⟨v, Av⟩= ⟨Av, v⟩= ⟨λv, v⟩= λ⟨v, v⟩.
The orthogonality ⟨v, w⟩= 0 comes similarly:
μ⟨v, w⟩= ⟨v, μw⟩= ⟨v, Aw⟩= ⟨Av, w⟩= ⟨λv, w⟩= λ⟨v, w⟩.
□

18
1
Fundamentals of Operators and Matrices
If A is a self-adjoint operator on an n-dimensional Hilbert space, then from the
eigenvectors we can ﬁnd an orthonormal basis v1, v2, . . . , vn. If Avi = λivi, then
A =
n

i=1
λi|vi⟩⟨vi|
(1.10)
which is called the Schmidt decomposition. The Schmidt decomposition is unique
if all the eigenvalues are different, otherwise not. Another useful decomposition is
the spectral decomposition. Assume that a self-adjoint operator A has eigenvalues
μ1 > μ2 > · · · > μk. Then
A =
k

j=1
μjPj,
(1.11)
where Pj is the orthogonal projection onto the eigenspace for the eigenvalue μj.
(From the Schmidt decomposition (1.10),
Pj =

i
|vi⟩⟨vi|,
where the summation is over all i such that λi = μj.) This decomposition is always
unique. Actually, the Schmidt decomposition and the spectral decomposition exist
for all normal operators.
If λi ≥0 in (1.10), then we can set |xi⟩:= √λi|vi⟩and we have
A =
n

i=1
|xi⟩⟨xi|.
If the orthogonality of the vectors |xi⟩is not assumed, then there are several similar
decompositions, but they are connected by a unitary. The next lemma and its proof
is a good exercise for the bra and ket formalism. (The result and the proof is due to
Schrödinger [78].)
Lemma 1.24 If
A =
n

j=1
|xj⟩⟨xj| =
n

i=1
|yi⟩⟨yi|,
then there exists a unitary matrix [Uij]n
i,j=1 such that
n

j=1
Uij|xj⟩= |yi⟩
(1 ≤i ≤n).
(1.12)

1.4 Spectrum and Eigenvalues
19
Proof: Assume ﬁrst that the vectors |xj⟩are orthogonal. Typically they are not
unit vectors and several of them can be 0. Assume that |x1⟩, |x2⟩, . . . , |xk⟩are not
0 and |xk+1⟩= · · · = |xn⟩= 0. Then the vectors |yi⟩are in the linear span of
{|xj⟩: 1 ≤j ≤k}. Therefore
|yi⟩=
k

j=1
⟨xj|yi⟩
⟨xj|xj⟩|xj⟩
is the orthogonal expansion. We can deﬁne [Uij] by the formula
Uij = ⟨xj|yi⟩
⟨xj|xj⟩
(1 ≤i ≤n, 1 ≤j ≤k).
We easily compute that
k

i=1
UitU∗
iu =
k

i=1
⟨xt|yi⟩
⟨xt|xt⟩
⟨yi|xu⟩
⟨xu|xu⟩
=
⟨xt|A|xu⟩
⟨xu|xu⟩⟨xt|xt⟩= δt,u,
and this relation shows that the k column vectors of the matrix [Uij] are orthonormal.
If k < n, then we can append further columns to get an n×n unitary, see Exercise 37.
(One can see in (1.12) that if |xj⟩= 0, then Uij does not play any role.)
In the general case
A =
n

j=1
|zj⟩⟨zj| =
n

i=1
|yi⟩⟨yi|,
we can ﬁnd a unitary U from an orthogonal family to the |yi⟩’s and a unitary V from
the same orthogonal family to the |zi⟩’s. Then UV ∗maps from the |zi⟩’s to the |yi⟩’s.
□
Example 1.25 Let A ∈B(H) be a self-adjoint operator with eigenvalues λ1 ≥λ2 ≥
· · · ≥λn (counted with multiplicity). Then
λ1 = max{⟨v, Av⟩: v ∈H, ∪v∪= 1}.
(1.13)
We can take the Schmidt decomposition (1.10). Assume that
max{⟨v, Av⟩: v ∈H, ∪v∪= 1} = ⟨w, Aw⟩
for a unit vector w. This vector has the expansion

20
1
Fundamentals of Operators and Matrices
w =
n

i=1
ci|vi⟩
and we have
⟨w, Aw⟩=
n

i=1
|ci|2λi ≤λ1.
Equality holds if and only if λi < λ1 implies ci = 0. The maximizer should be an
eigenvector for the eigenvalue λ1.
Similarly,
λn = min{⟨v, Av⟩: v ∈H, ∪v∪= 1}.
(1.14)
The formulas (1.13) and (1.14) will be extended below.
□
Theorem 1.26 (Poincaré’s inequality) Let A ∈B(H) be a self-adjoint operator
with eigenvalues λ1 ≥λ2 ≥· · · ≥λn (counted with multiplicity) and let K be a
k-dimensional subspace of H. Then there are unit vectors x, y ∈K such that
⟨x, Ax⟩≤λk
and
⟨y, Ay⟩≥λk.
Proof: Let vk, . . . , vn be orthonormal eigenvectors corresponding to the eigenval-
ues λk, . . . , λn. They span a subspace M of dimension n −k + 1 which must have
intersection with K. Take a unit vector x ∈K ∩M which has the expansion
x =
n

i=k
civi.
This vector x has the required property:
⟨x, Ax⟩=
n

i=k
|ci|2λi ≤λk
n

i=k
|ci|2 = λk.
To ﬁnd the other vector y, the same argument can be used with the matrix −A. □
The next result is a minimax principle.
Theorem 1.27 Let A ∈B(H) be a self-adjoint operator with eigenvalues λ1 ≥
λ2 ≥· · · ≥λn (counted with multiplicity). Then
λk = min

max{⟨v, Av⟩: v ∈K, ∪v∪= 1} : K ⊂H, dim K = n + 1 −k

.

1.4 Spectrum and Eigenvalues
21
Proof: Let vk, . . . , vn be orthonormal eigenvectors corresponding to the eigen-
values λk, . . . , λn. They span a subspace K of dimension n + 1 −k. According to
(1.13) we have
λk = max{⟨v, Av⟩: v ∈K}
and it follows that in the statement of the theorem ≥is true.
To complete the proof we have to show that for any subspace K of dimension
n + 1 −k there is a unit vector v such that λk ≤⟨v, Av⟩, or −λk ≥⟨v, (−A)v⟩.
The decreasing eigenvalues of −A are −λn ≥−λn−1 ≥· · · ≥−λ1 where the αth
is −λn+1−α. The existence of a unit vector v is guaranteed by Poincaré’s inequality,
where we take α = n + 1 −k.
□
1.5 Trace and Determinant
When {e1, . . . , en} is an orthonormal basis of H, the trace Tr A of A ∈B(H) is
deﬁned as
Tr A :=
n

i=1
⟨ei, Aei⟩.
(1.15)
Theorem 1.28 The deﬁnition (1.15) is independent of the choice of an orthonormal
basis {e1, . . . , en} and Tr AB = Tr BA for all A, B ∈B(H).
Proof: We have
Tr AB =
n

i=1
⟨ei, ABei⟩=
n

i=1
⟨A∗ei, Bei⟩=
n

i=1
n

j=1
⟨ej, A∗ei⟩⟨ej, Bei⟩
=
n

j=1
n

i=1
⟨ei, B∗ej⟩⟨ei, Aej⟩=
n

j=1
⟨ej, BAej⟩= Tr BA.
Now let {f1, . . . , fn} be another orthonormal basis of H. Then a unitary U is
deﬁned by Uei = fi, 1 ≤i ≤n, and we have
n

i=1
⟨fi, Afi⟩=
n

i=1
⟨Uei, AUei⟩= Tr U∗AU = Tr AUU∗= Tr A,
which says that the deﬁnition of Tr A is actually independent of the choice of an
orthonormal basis.
□

22
1
Fundamentals of Operators and Matrices
When A ∈Mn, the trace of A is nothing but the sum of the principal diagonal
entries of A:
Tr A = A11 + A22 + · · · + Ann.
The trace is the sum of the eigenvalues.
Computation of the trace is very simple, however the case of the determinant (1.1)
is very different. In terms of the Jordan canonical form described in Theorem 1.16,
we have
Tr X =
m

j=1
kjλj
and
det X =
m

j=1
λ
kj
j .
Formula (1.8) shows that trace and determinant are certain coefﬁcients of the char-
acteristic polynomial.
The next example concerns the determinant of a special linear mapping.
Example 1.29 On the linear space Mn we can deﬁne a linear mapping α : Mn →
Mn as α(A) = V AV ∗, where V ∈Mn is a ﬁxed matrix. We are interested in det α.
Let V = SJS−1 be the canonical Jordan decomposition and set
α1(A) = S−1A(S−1)∗,
α2(B) = JBJ∗,
α3(C) = SCS∗.
Then α = α3 ◦α2 ◦α1 and det α = det α3 × det α2 × det α1. Since α1 = α−1
3 , we
have det α = det α2, so only the Jordan block part has inﬂuence on the determinant.
The following example helps to understand the situation. Let
J =
	λ1 x
0 λ2

and
A1 =
	1 0
0 0

,
A2 =
	0 1
0 0

,
A3 =
	0 0
1 0

,
A4 =
	0 0
0 1

.
Then {A1, A2, A3, A4} is a basis in M2. If α(A) = JAJ∗, then from the data
α(A1) = λ1λ1A1,
α(A2) = λ1xA1 + λ1λ2A2,
α(A3) = λ1xA1 + λ1λ2A3,
α(A4) = xxA1 + λ2xA2 + λ2xA3 + λ2λ2A4
we can observe that the matrix of α is upper triangular:

1.5 Trace and Determinant
23
⎡
⎢⎢⎣
λ1λ1 λ1x
λ1x
xx
0
λ1λ2
0
λ2x
0
0
λ1λ2 λ2x
0
0
0
λ2λ2
⎤
⎥⎥⎦.
So its determinant is the product of the diagonal entries:
λ1λ1 · λ1λ2 · λ1λ2 · λ2λ2 = |λ1λ2|4 = | det J|4.
Now let J ∈Mn and assume that only the entries Jii and Ji,i+1 can be non-zero.
In Mn we choose the basis of matrix units,
E(1, 1), E(1, 2), . . . , E(1, n), E(2, 1), . . . , E(2, n), . . . , E(3, 1), . . . , E(n, n).
We want to show that the matrix of α is upper triangular.
From the computation
JE(j, k)J∗= Jj−1,jJk−1,k E(j −1, k −1) + Jj−1,jJk,k E(j −1, k)
+JjjJk−1,k E(j, k −1) + JjjJk,k E(j, k)
we can see that the matrix of the mapping A ⊃→JAJ∗is upper triangular. (In the
lexicographical order of the matrix units E(j −1, k −1), E(j −1, k), E(j, k −1)
precede E(j, k).) The determinant is the product of the diagonal entries:
n

j,k=1
JjjJkk =
m

k=1
(det J)Jkk
n = (det J)ndet Jn.
Hence the determinant of α(A) = V AV ∗is (det V )n det V n = | det V |2n, since the
determinant of V is equal to the determinant of its Jordan block J. If β(A) = V AV t,
then the argument is similar, det β = (det V )2n, thus only the conjugate is missing.
Next we consider the space M of real symmetric n × n matrices. Let V be a
real matrix and set γ : M →M, γ(A) = V AV t. Then γ is a symmetric operator
on the real Hilbert space M. The Jordan blocks in Theorem 1.16 for the real V are
generally non-real. However, the real form of the Jordan canonical decomposition
holds in such a way that there is a real invertible matrix S such that
V = SJS−1,
J =
⎡
⎢⎢⎢⎣
J1 0 · · · 0
0 J2 · · · 0
...
... ... ...
0 0 · · · Jm
⎤
⎥⎥⎥⎦
and each block Ji is either the Jordan block Jk(λ) with real λ or a matrix of the form

24
1
Fundamentals of Operators and Matrices
⎡
⎢⎢⎢⎢⎢⎢⎣
C I
0 · · · 0
0 C I · · · 0
... ... ... ... ...
... ...
... I
0 0 · · · · · · C
⎤
⎥⎥⎥⎥⎥⎥⎦
, C =
	 a b
−b a

with real a, b, I =
	1 0
0 1

.
Similarly to the above argument, det γ is equal to the determinant of X ⊃→JXJt.
Since the computation by using the above real Jordan decomposition and a basis
{E(j, k) + E(k, j) : 1 ≤j ≤k ≤n} in M is rather complicated, we shall be satisﬁed
with the computation for the special case:
J =
⎡
⎢⎢⎣
α β 0 0
−β α 0 0
0
0 λ 1
0
0 0 λ
⎤
⎥⎥⎦.
For a 4 × 4 real matrix
X =
	X11 X12
Xt
12 X22

with
X11 =
	x11 x12
x12 x22

,
X12 =
	x13 x14
x23 x24

,
X22 =
	x33 x34
x34 x44

,
a direct computation gives
JXJt =
	Y11 Y12
Yt
12 Y22

with
Y11 =
	
α2x11 + 2αβx12 + β2x22
−αβx11 + (α2 −β2)x12 + αβx22
−αβx11 + (α2 −β2)x12 + αβx22
β2x11 −2αβx12 + α2x22

,
Y12 =
	 αλx13 + αx14 + βλx23 + βx24
αλx14 + βλx24
−βλx13 −βx14 + αλx23 + αx24 −βλx14 + αλx24

,
Y22 =
	λ2x33 + 2λx34 + x44 λ2x34 + λx44
λ2x34 + λx44
λ2x44

.

1.5 Trace and Determinant
25
Therefore, the matrix of X ⊃→JXJt is the direct sum of
⎡
⎣
α2
2αβ
β2
−αβ α2 −β2 αβ
β2
−2αβ
α2
⎤
⎦,
⎡
⎢⎢⎣
αλ
α
βλ β
0
αλ
0 βλ
−βλ −β αλ λ
0
−βλ 0 αλ
⎤
⎥⎥⎦,
⎡
⎣
λ2 2λ 1
0 λ2 λ
0
0 λ2
⎤
⎦.
The determinant can be computed as the product of the determinants of the above
three matrices and it is
(α2 + β2)5λ10 = (det J)5 = (det V )5.
For a general n × n real V we have det γ = (det V )n+1.
□
Theorem 1.30 The determinant of a positive matrix A ∈Mn does not exceed the
product of the diagonal entries:
det A ≤
n

i=1
Aii.
This is a consequence of the concavity of the log function, see Example 4.18 (or
Example 1.43).
If A ∈Mn and 1 ≤i, j ≤n, then in the following theorems [A]ij denotes the
(n −1) × (n −1) matrix which is obtained from A by striking out the ith row and
the jth column.
Theorem 1.31 Let A ∈Mn and 1 ≤j ≤n. Then
det A =
n

i=1
(−1)i+jAij det([A]ij).
Example 1.32 Here is a simple computation using the row version of the previous
theorem.
det
⎡
⎣
1 2 0
3 0 4
0 5 6
⎤
⎦= 1 · (0 · 6 −5 · 4) −2 · (3 · 6 −0 · 4) + 0 · (3 · 5 −0 · 0).
This theorem is useful if the matrix has several 0 entries.
□
The determinant has an important role in the computation of the inverse.
Theorem 1.33 Let A ∈Mn be invertible. Then

26
1
Fundamentals of Operators and Matrices
[A−1]ki = (−1)i+k det([A]ik)
det A
for 1 ≤i, k ≤n.
Example 1.34 A standard formula is
	a b
c d

−1
=
1
ad −bc
	 d −b
−c a

when the determinant ad −bc is not 0.
□
The next example concerns the Haar measure on some group of matrices. Math-
ematical analysis is essential here.
Example 1.35 G denotes the set of invertible real 2 × 2 matrices. G is a (non-
commutative) group and G ⊂M2(R) ∼= R4 is an open set. Therefore it is a locally
compact topological group.
The Haar measure μ is deﬁned by the left-invariance property:
μ(H) = μ({BA : A ∈H})
(B ∈G)
(H ⊂G is measurable). We assume that
μ(H) =

H
p(A) dA,
where p : G →R+ is a function and dA is the Lebesgue measure in R4:
A =
	x y
z w

,
dA = dx dy dz dw.
The left-invariance is equivalent to the condition

f (A)p(A) dA =

f (BA)p(A) dA
for all continuous functions f : G →R and for every B ∈G. The integral can be
changed:

f (BA)p(A) dA =

f (A′)p(B−1A′)

∂A
∂A′
 dA′ ,
where BA is replaced with A′. If

1.5 Trace and Determinant
27
B =
	a b
c d

then
A′ := BA =
	ax + bz ay + bw
cx + dz cy + dw

and the Jacobi matrix is
∂A′
∂A =
⎡
⎢⎢⎣
a 0 b 0
0 a 0 b
c 0 d 0
0 c 0 d
⎤
⎥⎥⎦= B ⊗I2 .
We have

∂A
∂A′
 :=
det
	 ∂A
∂A′

 =
1
| det(B ⊗I2)| =
1
(det B)2
and

f (A)p(A) dA =

f (A) p(B−1A)
(det B)2 dA.
So the condition for the invariance of the measure is
p(A) = p(B−1A)
(det B)2 .
The solution is
p(A) =
1
(det A)2 .
This deﬁnes the left invariant Haar measure, but it is actually also right invariant.
For n × n matrices the computation is similar; then
p(A) =
1
(det A)n .
(Another example appears in Exercise 61.)
□

28
1
Fundamentals of Operators and Matrices
1.6 Positivity and Absolute Value
Let H be a Hilbert space and T : H →H be a bounded linear operator. T is called a
positive operator (or a positive semideﬁnite matrix) if ⟨x, Tx⟩≥0 for every vector
x ∈H, denoted T ≥0. It follows from the deﬁnition that a positive operator is
self-adjoint. Moreover, if T1 and T2 are positive operators, then T1 + T2 is positive
as well.
Theorem 1.36 Let T ∈B(H) be an operator. The following conditions are equiva-
lent.
(1) T is positive.
(2) T = T∗and the spectrum of T lies in R+ = [0, ∞).
(3) T is of the form A∗A for some operator A ∈B(H).
An operator T is positive if and only if UTU∗is positive for a unitary U.
We can reformulate positivity for a matrix T ∈Mn. For (a1, a2, . . . , an) ∈Cn
the inequality

i

j
aiTijaj ≥0
(1.16)
should be true. It is easy to see that if T ≥0, then Tii ≥0 for all 1 ≤i ≤n. For
a special unitary U the matrix UTU∗can be diagonal Diag(λ1, λ2, . . . , λn) where
the λi’s are the eigenvalues. So the positivity of T means that it is Hermitian and the
eigenvalues are positive (condition (2) above).
Example 1.37 If the matrix
A =
⎡
⎣
a b c
b d e
c e f
⎤
⎦
is positive, then the matrices
B =
	a b
b d

,
C =
	a c
c f

are positive as well. (We take the positivity condition (1.16) for A and the choice
a3 = 0 gives the positivity of B. A similar argument with a2 = 0 shows that C is
positive.)
□
Theorem 1.38 Let T be a positive operator. Then there is a unique positive operator
B such that B2 = T. If a self-adjoint operator A commutes with T, then it commutes
with B as well.

1.6 Positivity and Absolute Value
29
Proof: We restrict ourselves to the ﬁnite-dimensional case. In this case it is enough
to ﬁnd the eigenvalues and the eigenvectors. If Bx = λx, then x is an eigenvector of T
with eigenvalue λ2. This determines B uniquely; T and B have the same eigenvectors.
AB = BA holds if for any eigenvector x of B the vector Ax is an eigenvector of B,
too. If TA = AT, then this follows.
□
B is called the square root of T and is denoted by T1/2 or
√
T. It follows from
the theorem that the product of commuting positive operators T and A is positive.
Indeed,
TA = T1/2T1/2A = T1/2AT1/2 = (A1/2T1/2)∗A1/2T1/2.
For each A ∈B(H), we have A∗A ≥0. We deﬁne the absolute value of A to be
|A| := (A∗A)1/2. The mapping
|A|x ⊃→Ax
is norm preserving:
∪|A|x∪2 = ⟨|A|x, |A|x⟩= ⟨x, |A|2x⟩= ⟨x, A∗Ax⟩= ⟨Ax, Ax⟩= ∪Ax∪2.
This mapping can be extended to a unitary U. So A = U|A| and this is called the
polar decomposition of A.
|A| := (A∗A)1/2 makes sense if A : H1 →H2. Then |A| ∈B(H1). The above
argument tells us that |A|x ⊃→Ax is norm preserving, but it is not always true that it
can be extended to a unitary. If dim H1 ≤dim H2, then |A|x ⊃→Ax can be extended
to an isometry V : H1 →H2. Then A = V |A|, where V ∗V = I.
The eigenvalues si(A) of |A| are called the singular values of A. If A ∈Mn, then
the usual notation is
s(A) = (s1(A), . . . , sn(A)),
s1(A) ≥s2(A) ≥· · · ≥sn(A).
Example 1.39 Let T be a positive operator acting on a ﬁnite-dimensional Hilbert
space such that ∪T∪≤1. We want to show that there is a unitary operator U such
that
T = 1
2(U + U∗).
We can choose an orthonormal basis e1, e2, . . . , en consisting of eigenvectors of
T and with respect to this basis the matrix of T is diagonal, say, Diag(t1, t2, . . . , tn),
0 ≤tj ≤1 from the positivity. For any 1 ≤j ≤n we can ﬁnd a real number θj such
that

30
1
Fundamentals of Operators and Matrices
tj = 1
2(eiθj + e−iθj).
Then the unitary operator U with matrix Diag(exp(iθ1), . . . , exp(iθn)) will have the
desired property.
□
If T acts on a ﬁnite-dimensional Hilbert space which has an orthonormal basis
e1, e2, . . . , en, then T is uniquely determined by its matrix
[⟨ei, Tej⟩]n
i,j=1.
T is positive if and only if its matrix is positive (semideﬁnite).
Example 1.40 Let
A =
⎡
⎢⎢⎢⎣
λ1 λ2 . . . λn
0
0 . . . 0
...
... ... ...
0
0 . . . 0
⎤
⎥⎥⎥⎦.
Then
[A∗A]i,j = λiλj
(1 ≤i, j ≤n)
and this matrix is positive:

i

j
ai[A∗A]i,jaj =

i
aiλi

j
ajλj ≥0.
Every positive matrix is the sum of matrices of this form. (The minimum number of
the summands is the rank of the matrix.)
□
Example 1.41 Take numbers λ1, λ2, . . . , λn > 0 and deﬁne the matrix A by
Aij =
1
λi + λj
.
(A is called a Cauchy matrix.) We have
1
λi + λj
=
 ∞
0
e−tλie−tλj dt
and the matrix
A(t)ij := e−tλie−tλj

1.6 Positivity and Absolute Value
31
is positive for every t ∈R by Example1.40. Therefore
A =
 ∞
0
A(t) dt
is positive as well.
The above argument can be generalized. If r > 0, then
1
(λi + λj)r =
1
(r)
 ∞
0
e−tλie−tλjtr−1 dt.
This implies that
Aij =
1
(λi + λj)r
(r > 0)
is positive.
□
The Cauchy matrix is an example of an inﬁnitely divisible matrix. If A is an
entrywise positive matrix, then it is called inﬁnitely divisible if the matrices
A(r)ij = (Aij)r
are positive for every number r > 0.
Theorem 1.42 Let T
∈B(H) be an invertible self-adjoint operator and e1,
e2, . . . , en be a basis in the Hilbert space H. T is positive if and only if for any
1 ≤k ≤n the determinant of the k × k matrix
[⟨ei, Tej⟩]k
ij=1
is positive (that is, ≥0).
An invertible positive matrix is called positive deﬁnite. Such matrices appear
in probability theory in the concept of a Gaussian distribution. The work with
Gaussian distributions in probability theory requires experience with matrices. (This
is described in the next example, but also in Example 2.7.)
Example 1.43 Let M be a positive deﬁnite n×n real matrix and x =(x1, x2, . . . , xn).
Then
fM(x) :=

det M
(2π)n exp

−1
2⟨x, Mx⟩

(1.17)

32
1
Fundamentals of Operators and Matrices
is a multivariate Gaussian probability distribution (with 0 expectation, see, for exam-
ple, III.6 in [37]). The matrix M will be called the quadratic matrix of the Gaussian
distribution.
For an n × n matrix B, the relation

⟨x, Bx⟩fM(x) dx = Tr BM−1
(1.18)
holds.
We ﬁrst note that if (1.18) is true for a matrix M, then

⟨x, Bx⟩fU∗MU(x) dx =

⟨U∗x, BU∗x⟩fM(x) dx
= Tr (UBU∗)M−1
= Tr B(U∗MU)−1
for a unitary U, since the Lebesgue measure on Rn is invariant under unitary trans-
formation. This means that (1.18) also holds for U∗MU. Therefore to check (1.18),
we may assume that M is diagonal. Another reduction concerns B, we may assume
that B is a matrix unit Eij. Then the n variable integral reduces to integrals on R and
the known integrals

R
t exp

−1
2λt2
dt = 0 and

R
t2 exp

−1
2λt2
dt =
√
2π
λ
can be used.
Formula (1.18) has an important consequence. When the joint distribution of the
random variables (ξ1, ξ2, . . . , ξn) is given by (1.17), then the covariance matrix is
M−1.
The Boltzmann entropy of a probability density f (x) is deﬁned as
h(f ) := −

f (x) log f (x) dx
if the integral exists. For a Gaussian fM we have
h(fM) = n
2 log(2πe) −1
2 log det M.
Assume that fM is the joint distribution of (real-valued) random variables ξ1, ξ2, . . . ,
ξn. Their joint Boltzmann entropy is
h(ξ1, ξ2, . . . , ξn) = n
2 log(2πe) + log det M−1
and the Boltzmann entropy of ξi is

1.6 Positivity and Absolute Value
33
h(ξi) = 1
2 log(2πe) + 1
2 log(M−1)ii.
The subadditivity of the Boltzmann entropy is the inequality
h(ξ1, ξ2, . . . , ξn) ≤h(ξ1) + h(ξ2) + · · · + h(ξn),
which is
log det A ≤
n

i=1
log Aii
in our particular Gaussian case, A = M−1. What we have obtained is the Hadamard
inequality
det A ≤
n

i=1
Aii
for a positive deﬁnite matrix A, see Theorem 1.30.
□
Example 1.44 If the matrix X ∈Mn can be written in the form
X = SDiag(λ1, λ2, . . . , λn)S−1,
with λ1, λ2, . . . , λn > 0, then X is called weakly positive. Such a matrix has n lin-
early independent eigenvectors with strictly positive eigenvalues. If the eigenvectors
are orthogonal, then the matrix is positive deﬁnite. Since X has the form

SDiag(

λ1,

λ2, . . . ,

λn)S∗ 
(S∗)−1Diag(

λ1,

λ2, . . . ,

λn)S−1
,
it is the product of two positive deﬁnite matrices.
Although this X is not positive, the eigenvalues are strictly positive. Therefore we
can deﬁne the square root as
X1/2 = SDiag(

λ1,

λ2, . . . ,

λn)S−1.
(See also Example 3.16.)
□
The next result is called the Wielandt inequality. In the proof the operator norm
will be used.
Theorem 1.45 Let A be a self-adjoint operator such that for some numbers a, b > 0
the inequalities aI ≥A ≥bI hold. Then for orthogonal unit vectors x and y the
inequality

34
1
Fundamentals of Operators and Matrices
|⟨x, Ay⟩|2 ≤
a −b
a + b
2
⟨x, Ax⟩⟨y, Ay⟩
holds.
Proof: The conditions imply that A is a positive invertible operator. The following
argument holds for any real number α:
⟨x, Ay⟩= ⟨x, Ay⟩−α⟨x, y⟩= ⟨x, (A −αI)y⟩
= ⟨A1/2x, (I −αA−1)A1/2y⟩
and
|⟨x, Ay⟩|2 ≤⟨x, Ax⟩∪I −αA−1∪2⟨y, Ay⟩.
It is enough to prove that
∪I −αA−1∪≤a −b
a + b
for an appropriate α.
Since A is self-adjoint, it is diagonal in a basis, so A = Diag(λ1, λ2, . . . , λn) and
I −αA−1 = Diag

1 −α
λ1
, . . . , 1 −α
λn

.
Recall that b ≤λi ≤a. If we choose
α = 2ab
a + b,
then it is elementary to check that
−a −b
a + b ≤1 −α
λi
≤a −b
a + b,
which gives the proof.
□
The description of the generalized inverse of an m × n matrix can be described in
terms of the singular value decomposition.
Let A ∈Mm×n with strictly positive singular values σ1, σ2, . . . , σk. (Then k ≤
m, n.) Deﬁne a matrix  ∈Mm×n as
ij =

σi
if
i = j < k,
0
otherwise.

1.6 Positivity and Absolute Value
35
This matrix appears in the singular value decomposition described in the next theo-
rem.
Theorem 1.46 A matrix A ∈Mm×n has the decomposition
A = UV ∗,
(1.19)
where U ∈Mm and V ∈Mn are unitaries and  ∈Mm×n is deﬁned above.
For the sake of simplicity we consider the case m = n. Then A has the polar
decomposition U0|A| and |A| can be diagonalized:
|A| = U1Diag(σ1, σ2, . . . , σk, 0, . . . , 0)U∗
1.
Therefore, A = (U0U1)U∗
1, where U0 and U1 are unitaries.
Theorem 1.47 For a matrix A ∈Mm×n there exists a unique matrix A† ∈Mn×m
such that the following four properties hold:
(1) AA†A = A.
(2) A†AA† = A†.
(3) AA† is self-adjoint.
(4) A†A is self-adjoint.
It is easy to describe A† in terms of the singular value decomposition (1.19).
Namely, A† = V †U∗, where
†
ij =
⎧
⎪⎪⎨
⎪⎪⎩
1
σi
if
i = j < k,
0
otherwise.
If A is invertible, then n = m and † = −1. Hence A† is the inverse of A. There-
fore A† is called the generalized inverse of A or the Moore–Penrose generalized
inverse. The generalized inverse has the properties
(λA)† = 1
λA†,
(A†)† = A,
(A†)∗= (A∗)†.
It is worthwhile to note that for a matrix A with real entries A† has real entries as
well. Another important observation is the fact that the generalized inverse of AB is
not always B†A†.
Example 1.48 If M ∈Mm is an invertible matrix and v ∈Cm, then the linear system
Mx = v

36
1
Fundamentals of Operators and Matrices
has the obvious solution x = M−1v. If M ∈Mm×n, then the generalized inverse can
be used. From property (1) a necessary condition of the solvability of the equation
is MM†v = v. If this condition holds, then the solution is
x = M†v + (In −M†M)z
with arbitrary z ∈Cn. This example justiﬁes the importance of the generalized
inverse.
□
1.7 Tensor Product
Let H be the linear space of polynomials in the variable x and with degree less than
or equal to n. A natural basis consists of the powers 1, x, x2, . . . , xn. Similarly, let
K be the space of polynomials in y of degree less than or equal to m. Its basis is
1, y, y2, . . . , ym. The tensor product of these two spaces is the space of polynomials
of two variables with basis xiyj,0 ≤i ≤n and 0 ≤j ≤m. This simple example
contains the essential ideas.
Let H and K be Hilbert spaces. Their algebraic tensor product consists of the
formal ﬁnite sums

i,j
xi ⊗yj
(xi ∈H, yj ∈K).
Computing with these sums, one should use the following rules:
(x1 + x2) ⊗y = x1 ⊗y + x2 ⊗y,
(λx) ⊗y = λ(x ⊗y) ,
x ⊗(y1 + y2) = x ⊗y1 + x ⊗y2,
x ⊗(λy) = λ(x ⊗y) .
(1.20)
The inner product is deﬁned as
$ 
i,j
xi ⊗yj,

k,l
zk ⊗wl
%
=

i,j,k,l
⟨xi, zk⟩⟨yj, wl⟩.
When H and K are ﬁnite-dimensional spaces, then we arrive at the tensor product
Hilbert space H ⊗K; otherwise the algebraic tensor product must be completed in
order to get a Hilbert space.
Example 1.49 L2[0, 1] is the Hilbert space of the square integrable functions on
[0, 1]. If f , g ∈L2[0, 1], then the elementary tensor f ⊗g can be interpreted as a
function of two variables, f (x)g(y) deﬁned on [0, 1] × [0, 1]. The computational
rules (1.20) are obvious in this approach.
□

1.7 Tensor Product
37
The tensor product of ﬁnitely many Hilbert spaces is deﬁned similarly.
If e1, . . . , en and f1, . . . , fm are bases in ﬁnite-dimensional H and K, respectively,
then {ei ⊗fj : i, j} is a basis in the tensor product space. This basis is called the
product basis. An arbitrary vector x ∈H ⊗K admits an expansion
x =

i,j
cij ei ⊗fj
(1.21)
for some coefﬁcients cij, 
i,j |cij|2 = ∪x∪2. This kind of expansion is general, but
sometimes it is not the best.
Lemma 1.50 Any unit vector x ∈H ⊗K can be written in the form
x =

k
√pk gk ⊗hk,
(1.22)
where the vectors gk ∈H and hk ∈K are orthonormal and (pk) is a probability
distribution.
Proof: We can deﬁne a conjugate-linear mapping σ : H →K as
⟨σα, β⟩= ⟨x, α ⊗β⟩
for every vector α ∈H and β ∈K. In the computation we can use the bases (ei)i in
H and (fj)j in K. If x has the expansion (1.21), then
⟨σei, fj⟩= cij
and the adjoint σ∗is determined by
⟨σ∗fj, ei⟩= cij.
(Concerning the adjoint of a conjugate-linear mapping, see (1.5).)
One can compute that the partial trace Tr2|x⟩⟨x| of the matrix |x⟩⟨x| is D := σ∗σ
(see the deﬁnition before Example 1.56). It is enough to check that
⟨x, (|ek⟩⟨eα| ⊗IK)x⟩= Tr σ∗σ|ek⟩⟨eα|
for every k and α.
Choose now the orthogonal unit vectors gk such that they are eigenvectors of D
with corresponding non-zero eigenvalues pk, Dgk = pkgk. Then
hk :=
1
√pk
|σgk⟩

38
1
Fundamentals of Operators and Matrices
is a family of pairwise orthogonal unit vectors. Now
⟨x, gk ⊗hα⟩= ⟨σgk, hα⟩=
1
√pα
⟨σgk, σgα⟩=
1
√pα
⟨gα, σ∗σgk⟩= δk,α
√pα
and we have arrived at the orthogonal expansion (1.22).
□
The product basis tells us that
dim(H ⊗K) = dim(H) × dim(K).
Example 1.51 In the quantum formalism the orthonormal basis in the two-
dimensional Hilbert space H is denoted by | ↑⟩, | ↓⟩. Instead of | ↑⟩⊗| ↓⟩,
the notation | ↑↓⟩is used. Therefore the product basis is
| ↑↑⟩,
| ↑↓⟩,
| ↓↑⟩,
| ↓↓⟩.
Sometimes ↓is replaced by 0 and ↑by 1.
Another basis
1
√
2
(|00⟩+ |11⟩),
1
√
2
(|01⟩+ |10⟩),
i
√
2
(|10⟩−|01⟩),
1
√
2
(|00⟩−|11⟩)
is often used, which is called the Bell basis.
□
Example 1.52 In the Hilbert space L2(R2) we can get a basis if the space is consid-
ered as L2(R) ⊗L2(R). In the space L2(R) the Hermite functions
ϕn(x) = exp(−x2/2)Hn(x)
form a good basis, where Hn(x) is the appropriately normalized Hermite polynomial.
Therefore, the two variable Hermite functions
ϕnm(x, y) := e−(x2+y2)/2Hn(x)Hm(y)
(n, m = 0, 1, . . . )
form a basis in L2(R2).
□
The tensor product of linear transformations can be deﬁned as well. If A : H1 →
K1 and B : H2 →K2 are linear transformations, then there is a unique linear
transformation A ⊗B : H1 ⊗H2 →K1 ⊗K2 such that
(A ⊗B)(v1 ⊗v2) = Av1 ⊗Bv2
(v1 ∈H1, v2 ∈H2).

1.7 Tensor Product
39
Since the linear mappings (between ﬁnite-dimensional Hilbert spaces) are iden-
tiﬁed with matrices, the tensor product of matrices appears as well.
Example 1.53 Let {e1, e2, e3} be a basis in H and {f1, f2} be a basis in K. If [Aij] is
the matrix of A ∈B(H1) and [Bkl] is the matrix of B ∈B(H2), then
(A ⊗B)(ej ⊗fl) =

i,k
AijBklei ⊗fk .
It is useful to order the tensor product bases lexicographically: e1 ⊗f1, e1 ⊗f2, e2 ⊗
f1, e2 ⊗f2, e3 ⊗f1, e3 ⊗f2. Fixing this ordering, we can write down the matrix of
A ⊗B and we have
⎡
⎢⎢⎢⎢⎢⎢⎣
A11B11
A11B12
A12B11
A12B12
A13B11
A13B12
A11B21
A11B22
A12B21
A12B22
A13B21
A13B22
A21B11
A21B12
A22B11
A22B12
A23B11
A23B12
A21B21
A21B22
A22B21
A22B22
A23B21
A23B22
A31B11
A31B12
A32B11
A32B12
A33B11
A33B12
A31B21
A31B22
A32B21
A32B22
A33B21
A33B22
⎤
⎥⎥⎥⎥⎥⎥⎦
.
In the block matrix formalism we have
A ⊗B =
⎡
⎣
A11B A12B A13B
A21B A22B A23B
A31B A32B A33B
⎤
⎦,
(1.23)
see Sect.2.1. The tensor product of matrices is also called the Kronecker product. □
Example 1.54 When A ∈Mn and B ∈Mm, the matrix
Im ⊗A + B ⊗In ∈Mnm
is called the Kronecker sum of A and B.
If u is an eigenvector of A with eigenvalue λ and v is an eigenvector of B with
eigenvalue μ, then
(Im ⊗A + B ⊗In)(u ⊗v) = λ(u ⊗v) + μ(u ⊗v) = (λ + μ)(u ⊗v).
So u ⊗v is an eigenvector of the Kronecker sum with eigenvalue λ + μ.
□
The computation rules of the tensor product of Hilbert spaces imply straightfor-
ward properties of the tensor product of matrices (or linear operators).
Theorem 1.55 The following rules hold:
(1) (A1 + A2) ⊗B = A1 ⊗B + A2 ⊗B.

40
1
Fundamentals of Operators and Matrices
(2) B ⊗(A1 + A2) = B ⊗A1 + B ⊗A2.
(3) (λA) ⊗B = A ⊗(λB) = λ(A ⊗B)
(λ ∈C).
(4) (A ⊗B)(C ⊗D) = AC ⊗BD.
(5) (A ⊗B)∗= A∗⊗B∗.
(6) (A ⊗B)−1 = A−1 ⊗B−1 if A and B are invertible.
(7) ∪A ⊗B∪= ∪A∪∪B∪.
For example, the tensor product of self-adjoint matrices is self-adjoint and the
tensor product of unitaries is unitary.
The linear mapping Mn ⊗Mm →Mn deﬁned as
Tr2 : A ⊗B ⊃→(Tr B)A
is called a partial trace. The other partial trace is
Tr1 : A ⊗B ⊃→(Tr A)B.
Example 1.56 Assume that A ∈Mn and B ∈Mm. Then A⊗B is an nm×nm matrix.
Let C ∈Mnm. How can we decide if it has the form of A ⊗B for some A ∈Mn and
B ∈Mm?
First we study how to recognize A and B from A ⊗B. (Of course, A and B are
not uniquely determined, since (λA) ⊗(λ−1B) = A ⊗B.) If we take the trace of all
entries of (1.23), then we get
⎡
⎣
A11Tr B A12Tr B A13Tr B
A21Tr B A22Tr B A23Tr B
A31Tr B A32Tr B A33Tr B
⎤
⎦= Tr B
⎡
⎣
A11 A12 A13
A21 A22 A23
A31 A32 A33
⎤
⎦= (Tr B)A.
The sum of the diagonal entries is
A11B + A12B + A13B = (Tr A)B.
If X = A ⊗B, then
(Tr X)X = (Tr2X) ⊗(Tr1X).
For example, the matrix
X :=
⎡
⎢⎢⎣
0 0 0 0
0 1 1 0
0 1 1 0
0 0 0 0
⎤
⎥⎥⎦
in M2 ⊗M2 is not a tensor product. Indeed,

1.7 Tensor Product
41
Tr1X = Tr2X =
	1 0
0 1

and their tensor product is the identity in M4.
□
Let H be a Hilbert space. The k-fold tensor product H ⊗· · · ⊗H is called the kth
tensor power of H, denoted by H⊗k. When A ∈B(H), then A(1) ⊗A(2) · · · ⊗A(k) is
a linear operator on H⊗k and it is denoted by A⊗k. (Here the A(i)s are copies of A.)
H⊗k has two important subspaces, the symmetric and the antisymmetric sub-
spaces. If v1, v2, · · · , vk ∈H are vectors, then their antisymmetric tensor product
is the linear combination
v1 ∧v2 ∧· · · ∧vk :=
1
√
k!

π
(−1)σ(π)vπ(1) ⊗vπ(2) ⊗· · · ⊗vπ(k)
where the summation is over all permutations π of the set {1, 2, . . . , k} and σ(π)
is the number of inversions in π. The terminology “antisymmetric” comes from the
property that an antisymmetric tensor changes its sign if two elements are exchanged.
In particular, v1 ∧v2 ∧· · · ∧vk = 0 if vi = vj for different i and j.
The computational rules for the antisymmetric tensors are similar to (1.20):
λ(v1 ∧v2 ∧· · · ∧vk) = v1 ∧v2 ∧· · · ∧vα−1 ∧(λvα) ∧vα+1 ∧· · · ∧vk
for every α and
(v1 ∧v2 ∧· · · ∧vα−1 ∧v ∧vα+1 ∧· · · ∧vk)
+ (v1 ∧v2 ∧· · · ∧vα−1 ∧v′ ∧vα+1 ∧· · · ∧vk)
= v1 ∧v2 ∧· · · ∧vα−1 ∧(v + v′) ∧vα+1 ∧· · · ∧vk .
Lemma 1.57 The inner product of v1 ∧v2 ∧· · · ∧vk and w1 ∧w2 ∧· · · ∧wk is the
determinant of the k × k matrix whose (i, j) entry is ⟨vi, wj⟩.
Proof: The inner product is
1
k!

π

κ
(−1)σ(π)(−1)σ(κ)⟨vπ(1), wκ(1)⟩⟨vπ(2), wκ(2)⟩. . . ⟨vπ(k), wκ(k)⟩
= 1
k!

π

κ
(−1)σ(π)(−1)σ(κ)⟨v1, wπ−1κ(1)⟩⟨v2, wπ−1κ(2)⟩. . . ⟨vk, wπ−1κ(k)⟩
= 1
k!

π

κ
(−1)σ(π−1κ)⟨v1, wπ−1κ(1)⟩⟨v2, wπ−1κ(2)⟩. . . ⟨vk, wπ−1κ(k)⟩
=

π
(−1)σ(π)⟨v1, wπ(1)⟩⟨v2, wπ(2)⟩. . . ⟨vk, wπ(k)⟩.

42
1
Fundamentals of Operators and Matrices
This is the determinant.
□
It follows from the previous lemma that v1 ∧v2 ∧· · · ∧vk ̸= 0 if and only if the
vectors v1, v2, · · · vk are linearly independent. The subspace spanned by the vectors
v1 ∧v2 ∧· · · ∧vk is called the kth antisymmetric tensor power of H, denoted by
H∧k. So H∧k ⊂H⊗k.
Lemma 1.58 The linear extension of the map
x1 ⊗· · · ⊗xk ⊃→
1
√
k!
x1 ∧· · · ∧xk
is the projection of H⊗k onto H∧k.
Proof: Let P be the deﬁned linear operator. First we show that P2 = P:
P2(x1 ⊗· · · ⊗xk) =
1
(k!)3/2

π
(−1)σ(π)xπ(1) ∧· · · ∧xπ(k)
=
1
(k!)3/2

π
(−1)σ(π)+σ(π)x1 ∧· · · ∧xk
=
1
√
k!
x1 ∧· · · ∧xk = P(x1 ⊗· · · ⊗xk).
Moreover, P = P∗:
⟨P(x1 ⊗· · · ⊗xk), y1 ⊗· · · ⊗yk⟩= 1
k!

π
(−1)σ(π)
k
i=1
⟨xπ(i), yi⟩
= 1
k!

π
(−1)σ(π−1)
k
i=1
⟨xi, yπ−1(i)⟩
= ⟨x1 ⊗· · · ⊗xk, P(y1 ⊗· · · ⊗yk)⟩.
So P is an orthogonal projection.
□
Example 1.59 A transposition is a permutation of 1, 2, . . . , n which exchanges the
place of two entries. For a transposition κ, there is a unitary Uκ : H⊗k →H⊗k such
that
Uκ(v1 ⊗v2 ⊗· · · ⊗vn) = vκ(1) ⊗vκ(2) ⊗· · · ⊗vκ(n).
Then
H∧k = {x ∈H⊗k : Uκx = −x for every κ}.
(1.24)

1.7 Tensor Product
43
The terminology “antisymmetric” comes from this description.
□
If e1, e2, . . . , en is a basis in H, then
{ei(1) ∧ei(2) ∧· · · ∧ei(k) : 1 ≤i(1) < i(2) < · · · < i(k)) ≤n}
is a basis in H∧k. It follows that the dimension of H∧k is
n
k

if k ≤n,
otherwise for k > n the power H∧k has dimension 0. Consequently, H∧n has dimen-
sion 1.
If A ∈B(H), then the transformation A⊗k leaves the subspace H∧k invariant. Its
restriction is denoted by A∧k which is equivalently deﬁned as
A∧k(v1 ∧v2 ∧· · · ∧vk) = Av1 ∧Av2 ∧· · · ∧Avk.
For any operators A, B ∈B(H), we have
(A∗)∧k = (A∧k)∗,
(AB)∧k = A∧k B∧k
and
A∧n = λ · identity.
(1.25)
The constant λ is the determinant:
Theorem 1.60 For A ∈Mn, the constant λ in (1.25) is det A.
Proof: If e1, e2, . . . , en is a basis in H, then in the space H∧n the vector e1 ∧e2 ∧
· · · ∧en forms a basis. We should compute A∧k(e1 ∧e2 ∧· · · ∧en).
(A∧k)(e1 ∧e2 ∧· · · ∧en) = (Ae1) ∧(Ae2) ∧· · · ∧(Aen)
=

n

i(1)=1
Ai(1),1ei(1)

∧

n

i(2)=1
Ai(2),2ei(2)

∧· · · ∧

n

i(n)=1
Ai(n),nei(n)

=
n

i(1),i(2),...,i(n)=1
Ai(1),1Ai(2),2 · · · Ai(n),nei(1) ∧· · · ∧ei(n)
=

π
Aπ(1),1Aπ(2),2 · · · Aπ(n),neπ(1) ∧· · · ∧eπ(n)
=

π
Aπ(1),1Aπ(2),2 · · · Aπ(n),n(−1)σ(π)e1 ∧· · · ∧en.

44
1
Fundamentals of Operators and Matrices
Hereweusedthefactthatei(1)∧· · ·∧ei(n) canbenon-zeroifthevectorsei(1), . . . , ei(n)
are all different, in other words, this is a permutation of e1, e2, . . . , en.
□
Example 1.61 Let A ∈Mn be a self-adjoint matrix with eigenvalues λ1 ≥λ2 ≥
· · · ≥λn. The corresponding eigenvectors v1, v2, · · · , vn form a good basis. The
largest eigenvalue of the antisymmetric power A∧k is &k
i=1 λi:
A∧k(v1 ∧v2 ∧· · · ∧vk) = Av1 ∧Av2 ∧· · · ∧Avk
=

k
i=1
λi

(v1 ∧v2 ∧· · · ∧vk).
All other eigenvalues can be obtained from the basis of the antisymmetric product
(as in the proof of the next lemma).
□
The next lemma describes a relationship between singular values and antisym-
metric powers.
Lemma 1.62 For A ∈Mn and for k = 1, . . . , n, we have
k
i=1
si(A) = s1(A∧k) = ∪A∧k∪.
Proof: Since |A|∧k = |A∧k|, we may assume that A ≥0. Then there exists an
orthonormal basis {u1, · · · , un} of H such that Aui = si(A)ui for all i. We have
A∧k(ui(1) ∧· · · ∧ui(k)) =

k
j=1
si(j)(A)

ui(1) ∧· · · ∧ui(k),
and so {ui(1) ∧· · · ∧ui(k) : 1 ≤i(1) < · · · < i(k) ≤n} is a complete set of
eigenvectors of A∧k. Hence the assertion follows.
□
The symmetric tensor product of the vectors v1, v2, . . . , vk ∈H is
v1 ∨v2 ∨· · · ∨vk :=
1
√
k!

π
vπ(1) ⊗vπ(2) ⊗· · · ⊗vπ(k),
where the summation is over all permutations π of the set {1, 2, . . . , k} again. The
linear span of the symmetric tensors is the symmetric tensor power H∨k. Similarly
to (1.24), we have
H∨k = {x ∈⊗kH : Uκx = x for every κ}.

1.7 Tensor Product
45
It follows immediately that H∨k ⊥H∧k for any k ≥2. Let u ∈H∨k and v ∈H∧k.
Then
⟨u, v⟩= ⟨Uκu, −Uκv⟩= −⟨u, v⟩
and ⟨u, v⟩= 0.
If e1, e2, . . . , en is a basis in H, then ∨kH has the basis
{ei(1) ∨ei(2) ∨· · · ∨ei(k) : 1 ≤i(1) ≤i(2) ≤· · · ≤i(k) ≤n}.
Similarly to the proof of Lemma 1.57 we have
⟨v1 ∨v2 ∨· · ·∨vk, w1 ∨w2 ∨· · ·∨wk⟩=

π
⟨v1, wπ(1)⟩⟨v2, wπ(2)⟩. . . ⟨vk, wπ(k)⟩.
The right-hand side is similar to a determinant, but the sign does not change.
The permanent is deﬁned as
per A =

π
A1,π(1)A2,π(2) . . . An,π(n)
(1.26)
similarly to the determinant formula (1.1).
1.8 Notes and Remarks
The history of matrices goes back to ancient times. Their ﬁrst appearance in applica-
tions to linear equations was in ancient China. The notion of determinants preceded
the introduction and development of matrices and linear algebra. Determinants were
ﬁrst studied by a Japanese mathematician Takakazu Seki in 1683 and by Gottfried
Leibniz (1646–1716) in 1693. In 1750 Gabriel Cramer (1704–1752) discovered his
famous determinant-based formula of solutions to systems of linear equations. From
the 18th century to the beginning of the 19th, theoretical studies of determinants were
made by Vandermonde (famous for the determinant named after him), Joseph-Louis
Lagrange (1736–1813) who characterized the maxima and minima of multivariate
functions by his method known as the method of Lagrange multipliers, Pierre-Simon
Laplace (1749–1827), and Augustin Louis Cauchy (1789–1857). Gaussian elimina-
tion to solve systems of linear equations by successively eliminating variables was
developed around 1800 by Johann Carl Friedrich Gauss (1777–1855). However, as
mentioned above, a prototype of this method appeared in important ancient Chi-
nese texts. The method is also referred to as Gauss–Jordan elimination since it was
published in 1887 in an extended form by Wilhelm Jordan.
As explained above, determinants had been more dominant than matrices in the
ﬁrst stage of the history of matrix theory up to the middle of the 19th century. The

46
1
Fundamentals of Operators and Matrices
modern treatment of matrices emerged when Arthur Cayley (1821–1895) published
his monumental work, Memoir on the Theory of Matrices, in 1858. Before that, in
1851 James Joseph Sylvester (1814–1897) introduced the term “matrix” after the
Latin word for “womb”. Cayley studied matrices in the modern style by making
connections with linear transformations. The axiomatic deﬁnition of vector spaces
was ﬁnally introduced by Giuseppe Peano (1858–1932) in 1888. The computation of
determinants of concrete special matrices has a huge literature; for example, the book
by Thomas Muir, A Treatise on the Theory of Determinants, (originally published in
1928) has more than 700 pages.
In this book matrices are mostly complex matrices, which can be studied from
three different perspectives. The ﬁrst aspect is algebraic. The n × n matrices form
a ∗-algebra with linear operations, product AB and adjoint A∗as described in the
ﬁrst section. The second is the topological/analytic aspect of matrices as described
in Sect.1.2. Since a matrix corresponds to a linear transformation between ﬁnite-
dimensional vector spaces, the operator norm is naturally assigned to a matrix. It is
also important that the n×n matrices form a Hilbert space with the Hilbert–Schmidt
inner product. In this respect, Sect.1.2 may be regarded as a concise introduction
to Hilbert spaces, though mostly restricted to the ﬁnite-dimensional case. The third
aspect is the order structure of matrices described in Sect.1.6 and in further detail in
Sect.2.2. These three structures are closely related to each other, and their interplay
is an essential feature of the study of matrix analysis.
Cauchy proved in 1829 that the eigenvalues of a symmetric matrix are all real
numbers (see the proof of Theorem 1.23). The cofactor expansion for the determinant
in Theorem 1.31 was shown by Laplace. The famous Cayley–Hamilton theorem
(Theorem1.18)for2×2and3×3matriceswascontainedinCayley’sworkmentioned
above, and later William Rowan Hamilton proved it for 4 × 4 matrices. The formula
for inverse matrices (Theorem 1.33) was also established by Cayley.
Spectral and polar decompositions are fundamental in operator theory. The phase
operator U of the polar decomposition A = U|A| cannot always be a unitary in the
inﬁnite-dimensional case. However, a unitary U can be chosen for matrices, although
it is not unique. A ﬁner decomposition for general square matrices is the Jordan
canonical form introduced in Sect.1.3, which is due to Camille Jordan (1771–1821),
not the same Jordan as that of Gauss–Jordan elimination.
The useful minimax principle in Theorem 1.27 is often called the Courant–Fisher–
Weyl minimax principle due to their contributions (see Section III.7 of 20 for details).
A similar expression for singular values will be given in (6.5), and another expression
called Ky Fan’s maximum principle for the sum k
j=1 λj of the k largest eigenvalues
of a self-adjoint matrix is also useful in matrix theory. Theorem 1.30 is the Hadamard
inequality established by Jacques Hadamard in 1893. Weakly positive matrices were
introduced by Eugene P. Wigner in 1963. He showed that if the product of two or
three weakly positive matrices is self-adjoint, then it is positive deﬁnite.
Hilbert spaces and operators are essential in the mathematical formulation of
quantum mechanics. John von Neumann (1903–1957) introduced several concepts
in connection with operator/matrix theory and quantum physics in Mathematische
GrundlagenderQuantenmechanik,1932.Nowadays,matrixtheoryplaysanessential

1.8 Notes and Remarks
47
role in the theory of quantum information as well, see [73]. When quantum theory
appeared in the 1920s, some matrices had already appeared in the work of Werner
Heisenberg. Later the physicist Paul Adrien Maurice Dirac (1902–1984) introduced
the bra-ket notation, which is sometimes used in this book. But for column vectors
x, y, matrix theorists prefer to write x∗y for the inner product ⟨x|y⟩and xy∗for the
rank one operator |x⟩⟨y|.
Note that the Kronecker sum is often denoted by A⊕B in the literature, but in this
book ⊕is the notation for the direct sum. The antisymmetric and symmetric tensor
products are used in the construction of antisymmetric (Fermion) and symmetric
(Boson) Fock spaces in the study of quantum mechanics and quantum ﬁeld theory.
The antisymmetric tensor product is a powerful technique in matrix theory as well,
as will be seen in Chap. 6.
Concerning the permanent (1.26), a famous conjecture of Van der Waerden made
in 1926 was that if A is an n × n doubly stochastic matrix then
per A ≥n!
nn ,
and equality holds if and only if Aij = 1/n for all 1 ≤i, j ≤n. (The proof was given
in 1981 by G. P. Egorychev and D. Falikman. It is included in the book [87].)
1.9 Exercises
1. Let A : H2 →H1, B : H3 →H2 and C : H4 →H3 be linear mappings. Show
that
rank AB + rank BC ≤rank B + rank ABC.
(This is called Frobenius’ inequality.)
2. Let A : H →H be a linear mapping. Show that
dim ker An+1 = dim ker A +
n

k=1
dim(ran Ak ∩ker A).
3. Show that in the Schwarz inequality (1.2) equality occurs if and only if x and y
are linearly dependent.
4. Show that
∪x −y∪2 + ∪x + y∪2 = 2∪x∪2 + 2∪y∪2
for the norm in a Hilbert space. (This is called the parallelogram law.)
5. Prove the polarization identity (1.6).

48
1
Fundamentals of Operators and Matrices
6. Show that an orthonormal family of vectors is linearly independent.
7. Show that the vectors |x1⟩, |x2, ⟩, . . . , |xn⟩form an orthonormal basis in an n-
dimensional Hilbert space if and only if

i
|xi⟩⟨xi| = I.
8. Show that the Gram–Schmidt procedure constructs an orthonormal basis e1,
e2, . . . , en. Show that ek is a linear combination of v1, v2, . . . , vk (1 ≤k ≤n).
9. Show that the upper triangular matrices form an algebra.
10. Verify that the inverse of an upper triangular matrix is upper triangular if the
inverse exists.
11. Compute the determinant of the matrix
⎡
⎢⎢⎣
1 1 1
1
1 2 3
4
1 3 6 10
1 4 10 20
⎤
⎥⎥⎦.
Give an n × n generalization.
12. Compute the determinant of the matrix
⎡
⎢⎢⎣
1 −1 0
0
x
h −1 0
x2 hx
h −1
x3 hx2 hx
h
⎤
⎥⎥⎦.
Give an n × n generalization.
13. Let A, B ∈Mn and
Bij = (−1)i+jAij
(1 ≤i, j ≤n).
Show that det A = det B.
14. Show that the determinant of the Vandermonde matrix
⎡
⎢⎢⎢⎣
1
1
· · ·
1
a1
a2
· · · an
...
...
...
...
an−1
1
an−1
2
· · · an−1
n
⎤
⎥⎥⎥⎦
is &
i<j(aj −ai).
15. Prove the following properties:

1.9 Exercises
49
(|u⟩⟨v|)∗= |v⟩⟨u|,
(|u1⟩⟨v1|)(|u2⟩⟨v2|) = ⟨v1, u2⟩|u1⟩⟨v2|,
A(|u⟩⟨v|) = |Au⟩⟨v|,
(|u⟩⟨v|)A = |u⟩⟨A∗v| for all A ∈B(H).
16. Let A, B ∈B(H). Show that ∪AB∪≤∪A∪∪B∪.
17. Let H be an n-dimensional Hilbert space. For A ∈B(H) let ∪A∪2 :=
√
Tr A∗A.
Show that ∪A + B∪2 ≤∪A∪2 + ∪B∪2. Is it true that ∪AB∪2 ≤∪A∪2 × ∪B∪2?
18. Find constants c(n) and d(n) such that
c(n)∪A∪≤∪A∪2 ≤d(n)∪A∪
for every matrix A ∈Mn(C).
19. Show that ∪A∗A∪= ∪A∪2 for every A ∈B(H).
20. Let H be an n-dimensional Hilbert space. Show that given an operator A ∈B(H)
we can choose an orthonormal basis such that the matrix of A is upper triangular.
21. Let A, B ∈Mn be invertible matrices. Show that A + B is invertible if and only
if A−1 + B−1 is invertible, and moreover
(A + B)−1 = A−1 −A−1(A−1 + B−1)−1A−1.
22. Let A ∈Mn be self-adjoint. Show that
U = (I −iA)(I + iA)−1
is a unitary. (U is the Cayley transform of A.)
23. The self-adjoint matrix
0 ≤
	a b
b c

has eigenvalues α and β. Show that
|b|2 ≤
α −β
α + β
2
ac.
(1.27)
24. Show that
	λ + z x −iy
x + iy λ −z

−1
=
1
λ2 −x2 −y2 −z2
	 λ −z
−x + iy
−x −iy
λ + z

for real parameters λ, x, y, z.
25. Let m ≤n, A ∈Mn, B ∈Mm, Y ∈Mn×m and Z ∈Mm×n. Assume that A and
B are invertible. Show that A + YBZ is invertible if and only if B−1 + ZA−1Y is
invertible. Moreover,

50
1
Fundamentals of Operators and Matrices
(A + YBZ)−1 = A−1 −A−1Y(B−1 + ZA−1Y)−1ZA−1.
26. Let λ1, λ2, . . . , λn be the eigenvalues of a matrix A ∈Mn(C). Show that A is
normal if and only if
n

i=1
|λi|2 =
n

i,j=1
|Aij|2.
27. Show that A ∈Mn is normal if and only if A∗= AU for a unitary U ∈Mn.
28. Give an example such that A2 = A, but A is not an orthogonal projection.
29. A ∈Mn is called idempotent if A2 = A. Show that each eigenvalue of an
idempotent matrix is either 0 or 1.
30. Compute the eigenvalues and eigenvectors of the Pauli matrices:
σ1 =
	 0 1
1 0

,
σ2 =
	0 −i
i 0

,
σ3 =
	 1 0
0 −1

.
(1.28)
31. Show that the Pauli matrices (1.28) are orthogonal to each other (with respect to
the Hilbert–Schmidt inner product). What are the matrices which are orthogonal
to all Pauli matrices?
32. The n × n Pascal matrix is deﬁned as
Pij =
i + j −2
i −1

(1 ≤i, j ≤n).
What is the determinant? (Hint: Generalize the particular relation
⎡
⎢⎢⎣
1 1 1
1
1 2 3
4
1 3 6 10
1 4 10 20
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
1 0 0 0
1 1 0 0
1 2 1 0
1 3 3 1
⎤
⎥⎥⎦×
⎡
⎢⎢⎣
1 1 1 1
0 1 2 3
0 0 1 3
0 0 0 1
⎤
⎥⎥⎦
to n × n matrices.)
33. Let λ be an eigenvalue of a unitary operator. Show that |λ| = 1.
34. Let A be an n × n matrix and let k ≥1 be an integer. Assume that Aij = 0 if
j ≥i + k. Show that An−k is the 0 matrix.
35. Show that | det U| = 1 for a unitary U.
36. Let U ∈Mn and u1, . . . , un be n column vectors of U, i.e., U = [u1 u2 . . . un].
Prove that U is a unitary matrix if and only if {u1, . . . , un} is an orthonormal
basis of Cn.
37. Let a matrix U = [u1 u2 . . . un] ∈Mn be described by column vectors. Assume
that {u1, . . . , uk} are given and orthonormal in Cn. Show that uk+1, . . . , un can
be chosen in such a way that U will be a unitary matrix.
38. Compute det(λI −A) when A is the tridiagonal matrix (1.9).

1.9 Exercises
51
39. Let U ∈B(H) be a unitary. Show that
lim
n→∞
1
n
n

i=1
Unx
exists for every vector x ∈H. (Hint: Consider the subspaces {x ∈H : Ux = x}
and {Ux −x : x ∈H}.) What is the limit
lim
n→∞
1
n
n

i=1
Un ?
(This is the ergodic theorem.)
40. Let
|β0⟩=
1
√
2
(|00⟩+ |11⟩) ∈C2 ⊗C2
and
|βi⟩= (σi ⊗I2)|β0⟩
(i = 1, 2, 3)
where the σi are the Pauli matrices. Show that {|βi⟩: 0 ≤i ≤3} is the Bell
basis.
41. Show that the vectors of the Bell basis are eigenvectors of the matrices σi ⊗σi,
1 ≤i ≤3.
42. Prove the identity
|ψ⟩⊗|β0⟩= 1
2
3

k=0
|βk⟩⊗σk|ψ⟩
in C2 ⊗C2 ⊗C2, where |ψ⟩∈C2 and |βi⟩∈C2 ⊗C2 is deﬁned above.
43. Write the so-called Dirac matrices in the form of elementary tensors (of two
2 × 2 matrices):
γ1 =
⎡
⎢⎢⎣
0
0
0 −i
0
0 −i 0
0 −i 0
0
−i 0
0
0
⎤
⎥⎥⎦,
γ2 =
⎡
⎢⎢⎣
0 0 0 −1
0 0 1 0
0 1 0 0
−1 0 0 0
⎤
⎥⎥⎦,
γ3 =
⎡
⎢⎢⎣
0 0 −i 0
0 0
0 i
i 0
0 0
0 −i 0 0
⎤
⎥⎥⎦,
γ4 =
⎡
⎢⎢⎣
1 0 0
0
0 1 0
0
0 0 −1 0
0 0 0 −1
⎤
⎥⎥⎦.

52
1
Fundamentals of Operators and Matrices
44. Give the dimension of H∨k if dim(H) = n.
45. Let A ∈B(K) and B ∈B(H) be operators on the ﬁnite-dimensional spaces H
and K. Show that
det(A ⊗B) = (det A)m(det B)n,
where n = dim H and m = dim K. (Hint: The determinant is the product of the
eigenvalues.)
46. Show that ∪A ⊗B∪= ∪A∪· ∪B∪.
47. Use Theorem 1.60 to prove that det(AB) = det A × det B. (Hint: Show that
(AB)∧k = (A∧k)(B∧k).)
48. Let xn + c1xn−1 + · · · + cn be the characteristic polynomial of A ∈Mn. Show
that ck = Tr A∧k.
49. Show that
H ⊗H = (H ∨H) ⊕(H ∧H)
for a Hilbert space H.
50. Give an example of an A ∈Mn(C) such that the spectrum of A is in R+ and A
is not positive.
51. Let A ∈Mn(C). Show that A is positive if and only if X∗AX is positive for every
X ∈Mn(C).
52. Let A
∈
B(H). Prove the equivalence of the following assertions:
(i) ∪A∪≤1, (ii) A∗A ≤I, and (iii) AA∗≤I.
53. Let A ∈Mn(C). Show that A is positive if and only if Tr XA is positive for every
positive X ∈Mn(C).
54. Let ∪A∪≤1. Show that there are unitaries U and V such that
A = 1
2(U + V ).
(Hint: Use Example 1.39.)
55. Show that a matrix is weakly positive if and only if it is the product of two
positive deﬁnite matrices.
56. Let V : Cn →Cn ⊗Cn be deﬁned as V ei = ei ⊗ei. Show that
V ∗(A ⊗B)V = A ◦B
for A, B ∈Mn(C). Conclude Schur’s theorem.
57. Show that
|per (AB)|2 ≤per (AA∗)per (B∗B).
58. Let A ∈Mn and B ∈Mm. Show that

1.9 Exercises
53
Tr (Im ⊗A + B ⊗In) = mTr A + nTr B.
59. For a vector f ∈H the linear operator a+(f ) : ∨kH →∨k+1H is deﬁned as
a+(f ) v1 ∨v2 ∨· · · ∨vk = f ∨v1 ∨v2 ∨· · · ∨vk.
Compute the adjoint of a+(f ), which is denoted by a(f ).
60. For A ∈B(H) let F(A) : ∨kH →∨kH be deﬁned as
F(A) v1 ∨v2 ∨· · · ∨vk =
k

i=1
v1 ∨v2 ∨· · · ∨vi−1 ∨Avi ∨vi+1 ∨· · · ∨vk.
Show that
F(|f ⟩⟨g|) = a+(f )a(g)
for f , g ∈H. (Recall that a and a+ are deﬁned in the previous exercise.)
61. The group
G =
'	a b
0 c

: a, b, c ∈R, a ̸= 0, c ̸= 0
(
is locally compact. Show that the left invariant Haar measure μ can be deﬁned
as
μ(H) =

H
p(A) dA,
where
A =
	x y
0 z

,
p(A) =
1
x2|z|,
dA = dx dy dz.
Show that the right invariant Haar measure is similar, but
p(A) =
1
|x|z2 .

Chapter 2
Mappings and Algebras
Most of the statements and deﬁnitions in this chapter are formulated in the Hilbert
spacesetting.TheHilbertspaceisalwaysassumedtobeﬁnite-dimensional,soinstead
of operators one can consider matrices. The idea of block matrices provides quite a
useful tool in matrix theory. Some basic facts on block matrices are given in Sect.2.1.
Matrices have two primary structures; one is of course their algebraic structure with
addition, multiplication, adjoint, etc., and another is the order structure coming from
the partial order of positive semideﬁniteness, as explained in Sect.2.2. Based on this
order one can consider several notions of positivity for linear maps between matrix
algebras, which are discussed in Sect.2.6.
2.1 Block Matrices
If H1 and H2 are Hilbert spaces, then H1 ∈H2 consists of all the pairs ( f1, f2),
where f1 →H1 and f2 →H2. The linear combinations of the pairs are computed
entrywise and the inner product is deﬁned as
≤( f1, f2), (g1, g2)⟩:= ≤f1, g1⟩+ ≤f2, g2⟩.
It follows that the subspaces {( f1, 0) : f1 →H1} and {(0, f2) : f2 →H2} are
orthogonal and span the direct sum H1 ∈H2.
Assume that H = H1 ∈H2, K = K1 ∈K2 and A : H ∗K is a linear
operator. A general element of H has the form ( f1, f2) = ( f1, 0)+(0, f2). We have
A( f1, 0) = (g1, g2) and A(0, f2) = (g′
1, g′
2) for some g1, g′
1 →K1 and g2, g′
2 →K2.
The linear mapping A is determined uniquely by the following four linear mappings:
Ai1 : f1 ∗gi,
Ai1 : H1 ∗Ki
(1 ≥i ≥2)
F. Hiai and D. Petz, Introduction to Matrix Analysis and Applications,
55
Universitext, DOI: 10.1007/978-3-319-04150-6_2,
© Hindustan Book Agency 2014

56
2
Mappings and Algebras
and
Ai2 : f2 ∗g′
i,
Ai2 : H2 ∗Ki
(1 ≥i ≥2).
We write A in the form
⎡A11 A12
A21 A22
⎢
.
The advantage of this notation is the formula
⎡A11 A12
A21 A22
⎢⎡f1
f2
⎢
=
⎡A11 f1 + A12 f2
A21 f1 + A22 f2
⎢
.
(The right-hand side is A( f1, f2) written in the form of a column vector.)
Assume that ei
1, ei
2, . . . , ei
m(i) is a basis in Hi and f j
1 , f j
2 , . . . , f j
n( j) is a basis in
K j, 1 ≥i, j ≥2. The linear operators Ai j : H j ∗Ki have a matrix [Ai j] with
respect to these bases. Since
{(e1
t , 0) : 1 ≥t ≥m(1)} ∪{(0, e2
u) : 1 ≥u ≥m(2)}
is a basis in H and similarly
{( f 1
t , 0) : 1 ≥t ≥n(1)} ∪{(0, f 2
u ) : 1 ≥u ≥n(2)}
is a basis in K, the operator A has an (n(1) + n(2)) × (m(1) + m(2)) matrix which
is expressed by the n(i) × m( j) matrices [Ai j] as
[A] =
⎡[A11] [A12]
[A21] [A22]
⎢
.
This is a 2 × 2 matrix with matrix entries and it is called a block matrix.
Computation with block matrices is similar to that of ordinary matrices:
⎡[A11] [A12]
[A21] [A22]
⎢⊥
=
⎡[A11]⊥[A21]⊥
[A12]⊥[A22]⊥
⎢
,
⎡[A11] [A12]
[A21] [A22]
⎢
+
⎡[B11] [B12]
[B21] [B22]
⎢
=
⎡[A11] + [B11] [A12] + [B12]
[A21] + [B21] [A22] + [B22]
⎢
and
⎡[A11] [A12]
[A21] [A22]
⎢
·
⎡[B11] [B12]
[B21] [B22]
⎢
=
⎡[A11] · [B11] + [A12] · [B21] [A11] · [B12] + [A12] · [B22]
[A21] · [B11] + [A22] · [B21] [A21] · [B12] + [A22] · [B22]
⎢
.

2.1 Block Matrices
57
In several cases we do not emphasize the entries of a block matrix
⎡A B
C D
⎢
.
However, if this matrix is self-adjoint we assume that A = A⊥, B⊥= C and D = D⊥.
(These conditions include that A and D are square matrices, A →Mn and B →Mm.)
The block matrix is used for the deﬁnition of reducible matrices. A →Mn is
reducible if there is a permutation matrix P →Mn such that
Pt AP =
⎡B C
0 D
⎢
.
A matrix A →Mn is irreducible if it is not reducible.
For a 2 × 2 matrix, it is very easy to check the positivity:
⎡a b
¯b c
⎢
⊂0 if and only if a ⊂0 and b ¯b ≥ac.
If the entries are matrices, then the condition for positivity is similar but it is a bit
more complicated. It is obvious that a diagonal block matrix
⎡A 0
0 D
⎢
is positive if and only if the diagonal entries A and D are positive.
Theorem 2.1 Assume that A is invertible. The self-adjoint block matrix
⎡A B
B⊥C
⎢
(2.1)
is positive if and only if A is positive and
B⊥A−1B ≥C.
Proof: First assume that A = I. The positivity of
⎡I
B
B⊥C
⎢
is equivalent to the condition
≤( f1, f2),
⎡I
B
B⊥C
⎢
( f1, f2)⟩⊂0

58
2
Mappings and Algebras
for all vectors f1 and f2. A computation gives that this condition is
≤f1, f1⟩+ ≤f2, C f2⟩⊂−2Re ≤B f2, f1⟩.
If we replace f1 by eiϕ f1 with real ϕ, then the left-hand side does not change, while
the right-hand side becomes 2|≤B f2, f1⟩| for an appropriate ϕ. Choosing f1 = B f2,
we obtain the condition
≤f2, C f2⟩⊂≤f2, B⊥B f2⟩
for every f2. This means that positivity implies the condition C ⊂B⊥B. The converse
is also true, since the right-hand side of the equation
⎡I
B
B⊥C
⎢
=
⎡I 0
B⊥0
⎢⎡I B
0 0
⎢
+
⎡0
0
0 C −B⊥B
⎢
is the sum of two positive block matrices.
For a general positive invertible A, the positivity of (2.1) is equivalent to the
positivity of the block matrix
⎡
A−1/2 0
0
I
⎢⎡A B
B⊥C
⎢⎡
A−1/2 0
0
I
⎢
=
⎡
I
A−1/2B
B⊥A−1/2
C
⎢
.
This gives the condition C ⊂B⊥A−1B.
□
Another important characterization of the positivity of (2.1) is the condition that
A, C ⊂0 and B = A1/2WC1/2 with a contraction W. (Here the invertibility of A
or C is not necessary.)
Theorem 2.1 has applications in different areas, see for example the Cramér–Rao
inequality, Sect.7.5.
Theorem 2.2 For an invertible A, we have the so-called Schur factorization
⎡A B
C D
⎢
=
⎡
I
0
C A−1 I
⎢
·
⎡A
0
0 D −C A−1B
⎢
·
⎡
I A−1B
0
I
⎢
.
(2.2)
The proof is simply the computation of the product on the right-hand side. Since
⎡
I
0
C A−1 I
⎢−1
=
⎡
I
0
−C A−1 I
⎢
is invertible, the positivity of the left-hand side of (2.2) with C = B⊥is equivalent to
the positivity of the middle factor of the right-hand side. This fact gives the second
proof of Theorem 2.1.

2.1 Block Matrices
59
In the Schur factorization the ﬁrst factor is lower triangular, the second factor is
block diagonal and the third one is upper triangular. This structure allows an easy
computation of the determinant and the inverse.
Theorem 2.3 The determinant can be computed as follows.
det
⎡A B
C D
⎢
= det A · det (D −C A−1B).
If
M =
⎡A B
C D
⎢
,
then D −C A−1B is called the Schur complement of A in M, and is denoted by
M/A. Hence the determinant formula becomes det M = det A · det (M/A).
Theorem 2.4 Let
M =
⎡A B
B⊥C
⎢
be a positive invertible matrix. Then
M/C = A −BC−1B⊥= sup
⎣
X ⊂0 :
⎡X 0
0 0
⎢
≥
⎡A B
B⊥C
⎢⎤
.
Proof: The condition
⎡A −X B
B⊥
C
⎢
⊂0
is equivalent to
A −X ⊂BC−1B⊥,
and this gives the result.
□
Theorem 2.5 For a block matrix
0 ≥
⎡A X
X⊥B
⎢
→Mn,
we have
⎡A X
X⊥B
⎢
= U
⎡A 0
0 0
⎢
U ⊥+ V
⎡0 0
0 B
⎢
V ⊥
for some unitaries U, V →Mn.
Proof: We can take

60
2
Mappings and Algebras
0 ≥
⎡C Y
Y ⊥D
⎢
→Mn
such that
⎡A X
X⊥B
⎢
=
⎡C Y
Y ⊥D
⎢⎡C Y
Y ⊥D
⎢
=
⎡C2 + YY ⊥CV + Y D
Y ⊥C + DY ⊥Y ⊥Y + D2
⎢
.
It follows that
⎡A X
X⊥B
⎢
=
⎡C 0
Y ⊥0
⎢⎡C Y
0 0
⎢
+
⎡0 Y
0 D
⎢⎡0 0
Y ⊥D
⎢
= T ⊥T + S⊥S,
where
T =
⎡C Y
0 0
⎢
and
S =
⎡0 0
Y ⊥D
⎢
.
When T = U|T | and S = V |S| for unitaries U, V →Mn, then
T ⊥T = U(T T ⊥)U ⊥
and
S⊥S = V (SS⊥)V ⊥.
From the formulas
T T ⊥=
⎡
C2 + YY ⊥0
0
0
⎢
=
⎡A 0
0 0
⎢
,
SS⊥=
⎡0
0
0 Y ⊥Y + D2
⎢
=
⎡0 0
0 B
⎢
,
we have the result.
□
Example 2.6 Similarly to the previous theorem we take a block matrix
0 ≥
⎡A X
X⊥B
⎢
→Mn.
For a unitary
W :=
1
√
2
⎡iI −I
iI
I
⎢
we notice that
W
⎡A X
X⊥B
⎢
W ⊥=
⎡A+B
2
+ Im X
A−B
2
+ iRe X
A−B
2
−iRe X
A+B
2
−Im X
⎢
.
So Theorem 2.5 gives
⎡A X
X⊥B
⎢
= U
⎡A+B
2
+ Im X 0
0
0
⎢
U ⊥+ V
⎡0
0
0 A+B
2
−Im X
⎢
V ⊥

2.1 Block Matrices
61
for some unitaries U, V →Mn.
□
We have two remarks. If C is not invertible, then the supremum in Theorem 2.4
is A −BC†B⊥, where C† is the Moore–Penrose generalized inverse. The supremum
of that theorem can be formulated without the block matrix formalism. Assume that
P is an ortho-projection (see Sect.2.3). Then
[P]M := sup{N : 0 ≥N ≥M,
PN = N}.
(2.3)
If
P =
⎡I 0
0 0
⎢
and
M =
⎡A B
B⊥C
⎢
,
then [P]M = M/C. The formula (2.3) makes clear that if Q is another ortho-
projection such that P ≥Q, then [P]M ≥[P]QM Q.
It follows from the factorization that for an invertible block matrix
⎡A B
C D
⎢
,
both A and D −C A−1B must be invertible. This implies that
⎡A B
C D
⎢−1
=
⎡
I −A−1B
0
I
⎢
·
⎡A−1
0
0
(D −C A−1B)−1
⎢
·
⎡
I
0
−C A−1 I
⎢
.
After multiplication on the right-hand side, we have the following:
⎡A B
C D
⎢−1
=
⎡A−1 + A−1BW −1C A−1 −A−1BW −1
−W −1C A−1
W −1
⎢
=
⎡
V −1
−V −1BD−1
−D−1CV −1 D−1 + D−1CV −1BD−1
⎢
,
(2.4)
where W = M/A := D −C A−1B and V = M/D := A −BD−1C.
Example 2.7 Let X1, X2, . . . , Xm+k be real random variables with (Gaussian) joint
probability distribution
fM(z) :=
⎥
det M
(2π)m+k exp
⎦
−1
2≤z, Mz⟩

,
where z = (z1, z2, . . . , zm+k) and M is a positive deﬁnite real (m + k) × (m +
k) matrix, see Example 1.43. We want to compute the distribution of the random
variables X1, X2, . . . , Xm.

62
2
Mappings and Algebras
Let
M =
⎡A B
B⊥D
⎢
be written in the form of a block matrix, where A is m × m and D is k × k. Let
z = (x1, x2), where x1 →Rm and x2 →Rk. Then the marginal of the Gaussian
probability distribution
fM(x1, x2) =
⎥
det M
(2π)m+k exp
⎦
−1
2≤(x1, x2), M(x1, x2)⟩

on Rm is the distribution
f1(x1) =
⎥
det M
(2π)mdet D exp
⎦
−1
2≤x1, (A −BD−1B⊥)x1⟩

.
(2.5)
We have
≤(x1, x2), M(x1, x2)⟩= ≤Ax1 + Bx2, x1⟩+ ≤B⊥x1 + Dx2, x2⟩
= ≤Ax1, x1⟩+ ≤Bx2, x1⟩+ ≤B⊥x1, x2⟩+ ≤Dx2, x2⟩
= ≤Ax1, x1⟩+ 2≤B⊥x1, x2⟩+ ≤Dx2, x2⟩
= ≤Ax1, x1⟩+ ≤D(x2 + Wx1), (x2 + Wx1)⟩−≤DWx1, Wx1⟩,
where W = D−1B⊥. We integrate on Rk as
	
exp
⎦
−1
2(x1, x2)M(x1, x2)t
dx2
= exp
⎦
−1
2(≤Ax1, x1⟩−≤DWx1, Wx1⟩)

×
	
exp
⎦
−1
2≤D(x2 + Wx1), (x2 + Wx1)⟩

dx2
= exp
⎦
−1
2≤(A −BD−1B⊥)x1, x1⟩

⎥
(2π)k
det D
and obtain (2.5).
This computation gives a proof of Theorem 2.3 (for a real positive deﬁnite matrix)
as well. If we know that f1(x1) is Gaussian, then its quadratic matrix can be obtained
from formula (2.4). The covariance of X1, X2, . . . , Xm+k is M−1. Therefore, the
covariance of X1, X2, . . . , Xm is (A −BD−1B⊥)−1. It follows that the quadratic
matrix is the inverse: A −BD−1B⊥≡M/D.
□

2.1 Block Matrices
63
Theorem 2.8 Let A be a positive n × n block matrix with k × k entries. Then A
is the sum of block matrices B of the form [B]i j = X⊥
i X j for some k × k matrices
X1, X2, . . . , Xn.
Proof: A can be written as C⊥C for some
C =


C11 C12 . . . C1n
C21 C22 . . . C2n
...
...
...
...
Cn1 Cn2 . . . Cnn

⎛⎛⎛⎝.
Let Bi be the block matrix such that its ith row is the same as in C and all other
elements are 0. Then C = B1 + B2 + · · · + Bn and for t ⊃= i we have B⊥
t Bi = 0.
Therefore,
A = (B1 + B2 + · · · + Bn)⊥(B1 + B2 + · · · + Bn) = B⊥
1 B1 + B⊥
2 B2 + · · · + B⊥
n Bn.
The (i, j) entry of B⊥
t Bt is C⊥
tiCt j; hence this matrix is of the required form.
□
Example 2.9 Let H be an n-dimensional Hilbert space and A →B(H) be a positive
operator with eigenvalues λ1 ⊂λ2 ⊂· · · ⊂λn. If x, y →H are orthogonal vectors,
then
|≤x, Ay⟩|2 ≥
⎞λ1 −λn
λ1 + λn
⎠2
≤x, Ax⟩≤y, Ay⟩,
which is called the Wielandt inequality. (It also appeared in Theorem 1.45.) The
argument presented here includes a block matrix.
We can assume that x and y are unit vectors and we extend them to a basis. Let
M =
⎡≤x, Ax⟩≤x, Ay⟩
≤y, Ax⟩≤y, Ay⟩
⎢
,
where A has a block matrix
⎡M B
B⊥C
⎢
.
(2.6)
We can see that M ⊂0 and its determinant is positive:
|≤x, Ay⟩|2 ≥≤x, Ax⟩≤y, Ay⟩.
If λn = 0, then the proof is complete. Now we assume that λn > 0. Let α and β be
the eigenvalues of M. Formula (1.27) tells us that

64
2
Mappings and Algebras
|≤x, Ay⟩|2 ≥
⎞α −β
α + β
⎠2
≤x, Ax⟩≤y, Ay⟩.
We need the inequality
α −β
α + β ≥λ1 −λn
λ1 + λn
when α ⊂β. This is true, since λ1 ⊂α ⊂β ⊂λn.
□
As an application of the block matrix technique, we consider the following result,
called the UL-factorization (or the Cholesky factorization).
Theorem 2.10 Let X be an n × n invertible positive matrix. Then there is a unique
upper triangular matrix T with positive diagonal such that X = T T ⊥.
Proof: The proof is by mathematical induction on n. For n = 1 the statement is
clear. We assume that the factorization is true for (n −1) × (n −1) matrices and
write X in the form
⎡A B
B⊥C
⎢
,
(2.7)
where A is an (invertible) (n −1) × (n −1) matrix and C is a number. If
T =
⎡T11 T12
0 T22
⎢
is written in a similar form, then
T T ⊥=
⎡T11T ⊥
11 + T12T ⊥
12 T12T ⊥
22
T22T ⊥
12
T22T ⊥
22
⎢
.
The condition X = T T ⊥leads to the equations
T11T ⊥
11 + T12T ⊥
12 = A,
T12T ⊥
22 = B,
T22T ⊥
22 = C.
If C = 0, then the positivity of (2.7) forces B = 0 so that we can apply the induction
hypothesis to A. So we may assume that C > 0. If the number T22 is positive, then
T22 =
√
C is the unique solution and moreover
T12 = BC−1/2,
T11T ⊥
11 = A −BC−1B⊥.

2.1 Block Matrices
65
From the positivity of (2.7), we have A −BC−1B⊥⊂0 by Theorem 2.1. The
induction hypothesis gives that the latter can be written in the form T11T ⊥
11 where
T11 is upper triangular. Therefore T is upper triangular, too.
□
If 0 ≥A →Mn and 0 ≥B →Mm, then 0 ≥A ∞B. More generally, if
0 ≥Ai →Mn and 0 ≥Bi →Mm, then
k

i=1
Ai ∞Bi
is positive. These matrices in Mn ∞Mm are called separable positive matrices. Is it
true that every positive matrix in Mn ∞Mm is separable? A counterexample follows.
Example 2.11 Let M4 = M2 ∞M2 and
D := 1
2


0 0 0 0
0 1 1 0
0 1 1 0
0 0 0 0

⎛⎛⎝.
D is a rank 1 positive operator, it is a projection. If D = 
i Di, then Di = λi D.
If D is separable, then it is a tensor product. If D is a tensor product, then up to a
constant factor it is equal to (Tr2D) ∞(Tr1D) (as noted in Example 1.56). We have
Tr1D = Tr2D = 1
2
⎡1 0
0 1
⎢
.
Their tensor product has rank 4 and it cannot be λD. It follows that this D is not
separable.
□
In quantum theory the non-separable positive operators are said to be entangled.
The positive operator D is maximally entangled if it has minimal rank (meaning
rank 1) and the partial traces have maximal rank. The matrix D in the previous
example is maximally entangled.
It is interesting that there is no effective procedure to decide if a positive operator
in a tensor product space is separable or entangled.
2.2 Partial Ordering
Let A, B →B(H) be self-adjoint operators. The partial ordering A ≥B holds if
B −A is positive, or equivalently
≤x, Ax⟩≥≤x, Bx⟩

66
2
Mappings and Algebras
for all vectors x. From this formulation one can easily see that A ≥B implies
X AX⊥≥X BX⊥for every operator X.
Example 2.12 Assume that for the orthogonal projections P and Q the inequality
P ≥Q holds. If Px = x for a unit vector x, then ≤x, Px⟩≥≤x, Qx⟩≥1 shows
that ≤x, Qx⟩= 1. Therefore the relation
∩x −Qx∩2 = ≤x −Qx, x −Qx⟩= ≤x, x⟩−≤x, Qx⟩= 0
gives that Qx = x. The range of Q includes the range of P.
□
Let An be a sequence of operators on a ﬁnite-dimensional Hilbert space. Fix a
basis and let [An] be the matrix of An. Similarly, the matrix of the operator A is [A].
Let the Hilbert space be m-dimensional, so the matrices are m × m. Recall that the
following conditions are equivalent:
(1) ∩A −An∩∗0.
(2) Anx ∗Ax for every vector x.
(3) ≤x, An y⟩∗≤x, Ay⟩for all vectors x and y.
(4) ≤x, Anx⟩∗≤x, Ax⟩for every vector x.
(5) Tr (A −An)⊥(A −An) ∗0.
(6) [An]i j ∗[A]i j for every 1 ≥i, j ≥m.
These conditions describe in several ways the convergence of a sequence of operators
or matrices.
Theorem 2.13 Let An be an increasing sequence of operators with an upper bound:
A1 ≥A2 ≥· · · ≥B. Then there is an operator A ≥B such that An ∗A.
Proof: Let φn(x, y) := ≤x, An y⟩be a sequence of complex bilinear functionals.
Then φn(x, x) is a bounded increasing real sequence and it is convergent. By the
polarization identity, φn(x, y) is convergent as well and the limit gives a complex
bilinear functional φ. If the corresponding operator is denoted by A, then
≤x, An y⟩∗≤x, Ay⟩
for all vectors x and y. This is the convergence An ∗A. The condition ≤x, Ax⟩≥
≤x, Bx⟩means A ≥B.
□
Example 2.14 Assume that 0 ≥A ≥I for an operator A. Deﬁne a sequence Tn of
operators by recursion. Let T1 = 0 and
Tn+1 = Tn + 1
2(A −T 2
n )
(n →N) .
Tn is a polynomial in A with real coefﬁcients. Thus these operators commute with
each other. Since

2.2 Partial Ordering
67
I −Tn+1 = 1
2(I −Tn)2 + 1
2(I −A) ,
induction shows that Tn ≥I.
We show that T1 ≥T2 ≥T3 ≥· · · by mathematical induction again. In the
recursion
Tn+1 −Tn = 1
2 ((I −Tn−1)(Tn −Tn−1) + (I −Tn)(Tn −Tn−1)) ,
I −Tn−1 ⊂0 and Tn −Tn−1 ⊂0 by the assumption. Since they commute their
product is positive. Similarly (I −Tn)(Tn −Tn−1) ⊂0. It follows that the right-hand
side is positive.
Theorem 2.13 tells us that Tn converges to an operator B. The limit of the recursion
formula yields
B = B + 1
2(A −B2) .
Therefore A = B2. This example is a constructive proof of Theorem 1.38.
□
Theorem 2.15 Assume that 0 < A, B →Mn are invertible matrices and A ≥B.
Then B−1 ≥A−1.
Proof: The condition A ≥B is equivalent to B−1/2 AB−1/2 ≥I and the statement
B−1 ≥A−1 is equivalent to I ≥B1/2 A−1B1/2. If X = B−1/2 AB−1/2, then we
have to show that X ≥I implies X−1 ⊂I. The condition X ≥I means that all
eigenvalues of X are in the interval (0, 1]. This implies that all eigenvalues of X−1
are in [1, ◦).
□
Assume that A ≥B. It follows from (1.13) that the largest eigenvalue of A is
smaller than the largest eigenvalue of B. Let λ(A) = (λ1(A), . . . , λn(A)) denote
the vector of the eigenvalues of A in decreasing order (counting multiplicities).
The next result is called Weyl’s monotonicity theorem.
Theorem 2.16 If A ≥B, then λk(A) ≥λk(B) for all k.
This is a consequence of the minimax principle, Theorem 1.27.
Corollary 2.17 Let A, B →B(H) be self-adjoint operators.
(1) If A ≥B, then Tr A ≥Tr B.
(2) If 0 ≥A ≥B, then det A ≥det B.
Theorem 2.18 (Schur’s theorem) Let A and B be positive n × n matrices. Then
Ci j = Ai j Bi j
(1 ≥i, j ≥n)
determines a positive matrix.

68
2
Mappings and Algebras
Proof: If Ai j = λiλ j and Bi j = μiμ j, then Ci j = λiμiλ jμ j and C is positive by
Example 1.40. The general case reduces to this one.
□
The matrix C of the previous theorem is called the Hadamard (or Schur) product
of the matrices A and B and is denoted by C = A ∼B.
Corollary 2.19 Assume that 0 ≥A ≥B and 0 ≥C ≥D. Then A ∼C ≥B ∼D.
Proof: The equation
B ∼D −A ∼C = (B −A) ∼D + A ∼(D −C)
implies the statement.
□
Theorem 2.20 (Oppenheim’s inequality) If 0 ≥A, B →Mn, then
det(A ∼B) ⊂
 n

i=1
Aii

det B.
Proof: For n = 1 the statement is obvious. The argument will be by induction on
n. We take the Schur complementation and the block matrix formalism
A =
⎡a A1
A2 A3
⎢
and
B =
⎡b B1
B2 B3
⎢
,
where a, b →[0, ◦). We may assume that a, b > 0. From the inductive assumption
we have
det(A3 ∼(B/b)) ⊂A2,2 A3,3 . . . An,n det(B/b).
(2.8)
From Theorem 2.3 we have det(A ∼B) = ab det(A ∼B/ab) and
A ∼B/ab = A3 ∼B3 −(A2 ∼B2)a−1b−1(A1 ∼B1)
= A3 ∼(B/b) + (A/a) ∼(B2B1b−1).
The matrices A/a and B/b are positive, see Theorem 2.4. So the matrices
A3 ∼(B/b) and (A/a) ∼(B2B1b−1)
are positive as well. Thus
det(A ∼B) ⊂ab det(A3 ∼(B/b)).
Finally the inequality (2.8) gives

2.2 Partial Ordering
69
det(A ∼B) ⊂
 n

i=1
Aii

b det(B/b).
Since det B = b det(B/b), the proof is complete.
□
A linear mapping α : Mn ∗Mn is called completely positive if it has the form
α(B) =
k

i=1
V ⊥
i BVi
for some matrices Vi. The sum of completely positive mappings is completely
positive. (More details concerning completely positive mappings can be found in
Theorem 2.49.)
Example 2.21 Let A →Mn be a positive matrix. The mapping SA : B ∗A ∼B
sends positive matrices to positive matrices. Therefore it is a positive mapping.
We want to show that SA is completely positive. Since SA is additive in A, it is
enough to prove the case Ai j = λiλ j. Then
SA(B) = Diag(λ1, λ2, . . . , λn) B Diag(λ1, λ2, . . . , λn)
and SA is completely positive.
□
2.3 Projections
Let K be a closed subspace of a Hilbert space H. Any vector x →H can be written
in the form x0 + x1, where x0 →K and x1 ⊥K, see Theorem 1.11. The linear
mapping P : x ∗x0 is called the (orthogonal) projection onto K. The orthogonal
projection P has the properties P = P2 = P⊥. If an operator P →B(H) satisﬁes
P = P2 = P⊥, then it is an (orthogonal) projection (onto its range). Instead of
orthogonal projection the terminology ortho-projection is also used.
The partial ordering is very simple for projections, see Example 2.12. If P and Q
are projections, then the relation P ≥Q means that the range of P is included in the
range of Q. An equivalent algebraic formulation is P Q = P. The largest projection
in Mn is the identity I and the smallest one is 0. Therefore 0 ≥P ≥I for any
projection P →Mn.
Example 2.22 In M2 the non-trivial ortho-projections have rank 1 and they have
the form
P = 1
2
⎡1 + a3 a1 −ia2
a1 + ia2 1 −a3
⎢
,
where a1, a2, a3 →R and a2
1 + a2
2 + a2
3 = 1. In terms of the Pauli matrices

70
2
Mappings and Algebras
σ0 =
⎡1 0
0 1
⎢
,
σ1 =
⎡0 1
1 0
⎢
,
σ2 =
⎡0 −i
i 0
⎢
,
σ3 =
⎡1 0
0 −1
⎢
(2.9)
we have
P = 1
2

σ0 +
3

i=1
aiσi

.
An equivalent formulation is P = |x⟩≤x|, where x →C2 is a unit vector. This can be
extended to an arbitrary ortho-projection Q →Mn(C):
Q =
k

i=1
|xi⟩≤xi|,
where the set {xi : 1 ≥i ≥k} is a family of orthogonal unit vectors in Cn. (k is the
rank of the image of Q, or Tr Q.)
□
If P is a projection, then I −P is a projection as well and it is often denoted by
P⊥, since the range of I −P is the orthogonal complement of the range of P.
Example 2.23 Let P and Q be projections. The relation P ⊥Q means that the
range of P is orthogonal to the range of Q. An equivalent algebraic formulation
is P Q = 0. Since the orthogonality relation is symmetric, P Q = 0 if and only if
QP = 0. (We can also arrive at this statement by taking the adjoint.)
We show that P ⊥Q if and only if P + Q is a projection as well. P + Q is
self-adjoint and it is a projection if
(P + Q)2 = P2 + P Q + QP + Q2 = P + Q + P Q + QP = P + Q
or equivalently
P Q + QP = 0.
This is true if P ⊥Q. On the other hand, the condition P Q + QP = 0 implies that
P QP + QP2 = P QP + QP = 0 and QP must be self-adjoint. We conclude that
P Q = 0, which is the orthogonality.
□
Assume that P and Q are projections on the same Hilbert space. Among the
projections which are smaller than P and Q there is a maximal projection, denoted
by P ⊗Q, which is the orthogonal projection onto the intersection of the ranges of
P and Q.
Theorem 2.24 Assume that P and Q are ortho-projections. Then
P ⊗Q = lim
n∗◦(P QP)n = lim
n∗◦(QP Q)n.

2.3 Projections
71
Proof: The operator A := P QP is a positive contraction. Therefore the sequence
An is monotone decreasing and Theorem 2.13 implies that An has the limit R. The
operator R is self-adjoint. Since (An)2 ∗R2 we have R = R2; in other words, R
is an ortho-projection. If Px = x and Qx = x for a vector x, then Ax = x and it
follows that Rx = x. This means that R ⊂P ⊗Q.
From the inequality P QP ≥P, R ≥P follows. Taking the limit of (P QP)n Q
(P QP)n = (P QP)2n+1, we have RQR = R. From this we have R(I −Q)R = 0
and (I −Q)R = 0. This gives R ≥Q.
It has been proved that R ≥P, Q and R ⊂P ⊗Q. So R = P ⊗Q is the only
possibility.
□
Corollary 2.25 Assume that P and Q are ortho-projections and 0 ≥H ≥P, Q.
Then H ≥P ⊗Q.
Proof: Since (I −P)H(I −P) = 0 implies H1/2(I −P) = 0, we have
H1/2P = H1/2 so that P H P = H, and similarly QH Q = H. These im-
ply (P QP)n H(P QP)n = H and the limit n ∗◦gives RH R = H, where
R = P ⊗Q. Hence H ≥R.
□
Let P and Q be ortho-projections. If the ortho-projection R has the property
R ⊂P, Q, then the image of R includes the images of P and Q. The smallest such
R projects to the linear subspace generated by the images of P and Q. This ortho-
projection is denoted by P ↑Q. The set of ortho-projections becomes a lattice with
the operations ⊗and ↑. However, the so-called distributivity
A ↑(B ⊗C) = (A ↑B) ⊗(A ↑C)
does not hold.
Example 2.26 We show that any operator X →Mn(C) is a linear combination of
ortho-projections. We write
X = 1
2(X + X⊥) + 1
2i(iX −iX⊥),
where X + X⊥and iX −iX⊥are self-adjoint operators. Therefore, it is enough to ﬁnd
linear combinations of ortho-projections for self-adjoint operators. This is essentially
the spectral decomposition (1.11).
Assume that ϕ0 is deﬁned on projections of Mn(C) and it has the properties
ϕ0(0) = 0,
ϕ0(I) = 1,
ϕ0(P + Q) = ϕ0(P) + ϕ0(Q) if
P ⊥Q.
It is a famous theorem of Gleason that in the case n > 2 the mapping ϕ0 has a linear
extension ϕ : Mn(C) ∗C. The linearity implies ϕ is of the form
ϕ(X) = Tr ρX
(X →Mn(C))

72
2
Mappings and Algebras
for some matrix ρ →Mn(C). However, from the properties of ϕ0 we have ρ ⊂0 and
Tr ρ = 1. Such a ρ is usually called a density matrix in the quantum applications. It
is clear that if ρ has rank 1, then it is a projection.
□
In quantum information theory the traditional variance is
Varρ(A) = Tr ρA2 −(Tr ρA)2
(2.10)
where ρ is a density matrix and A →Mn(C) is a self-adjoint operator. This is a
straightforward analogy of the variance in probability theory; a standard notation
is ≤A2⟩−≤A⟩2 in both formalisms. We note that for two self-adjoint operators the
corresponding notion is covariance:
Covρ(A, B) = Tr ρAB −(Tr ρA)(Tr ρB).
It is rather different from probability theory that the variance (2.10) can be strictly
positive even in the case where ρ has rank 1. If ρ has rank 1, then it is an ortho-
projection of rank 1, also known as a pure state.
It is easy to show that
Varρ(A + λI) = Varρ(A)
for
λ →R
and the concavity of the variance functional ρ ∗Varρ(A):
Varρ(A) ⊂

i
λiVarρi (A) if ρ =

i
λiρi.
(Here λi ⊂0 and 
i λi = 1.)
The formulation is easier if ρ is diagonal. We can change the basis of the
n-dimensional space so that ρ = Diag(p1, p2, . . . , pn); then we have
Varρ(A) =

i, j
pi + p j
2
|Ai j|2 −

i
pi Aii
2
.
(2.11)
In the projection example P = Diag(1, 0, . . . , 0), formula (2.11) gives
VarP(A) =

i⊃=1
|A1i|2
and this can be strictly positive.
Theorem 2.27 Let ρ be a density matrix. Take all the decompositions such that

2.3 Projections
73
ρ =

i
qi Qi ,
(2.12)
where Qi are pure states and (qi) is a probability distribution. Then
Varρ(A) = sup

i
qi
⎦
Tr Qi A2 −(Tr Qi A)2
,
(2.13)
where the supremum is over all decompositions (2.12).
The proof will be an application of matrix theory. The ﬁrst lemma contains a
trivial computation on block matrices.
Lemma 2.28 Assume that
ρ =
⎡ρ⊗0
0 0
⎢
,
ρi =
⎡ρ⊗
i 0
0 0
⎢
,
A =
⎡A⊗B
B⊥C
⎢
and
ρ =

i
λiρi,
ρ⊗=

i
λiρ⊗
i .
Then
⎦
Tr ρ⊗(A⊗)2 −(Tr ρ⊗A⊗)2
−

i
λi
⎦
Tr ρ⊗
i (A⊗)2 −(Tr ρ⊗
i A⊗)2
= (Tr ρA2 −(Tr ρA)2) −

i
λi
⎦
Tr ρi A2 −(Tr ρi A)2
.
This lemma shows that if ρ →Mn(C) has a rank k < n, then the computation of
a variance Varρ(A) can be reduced to k × k matrices. The equality in (2.13) is rather
obvious for a rank 2 density matrix and, by the previous lemma, the computations
will be with 2 × 2 matrices.
Lemma 2.29 For a rank 2 matrix ρ, equality holds in (2.13).
Proof: By Lemma 2.28 we can make a computation with 2 × 2 matrices. We can
assume that
ρ =
⎡p
0
0 1 −p
⎢
,
A =
⎡a1 b
b a2
⎢
.
Then
Tr ρA2 = p(a2
1 + |b|2) + (1 −p)(a2
2 + |b|2).
We can assume that

74
2
Mappings and Algebras
Tr ρA = pa1 + (1 −p)a2 = 0.
Let
Q1 =
⎡p
c e−iϕ
c eiϕ 1 −p
⎢
,
where c = √p(1 −p). This is a projection and
Tr Q1A = a1 p + a2(1 −p) + bc e−iϕ + bc eiϕ = 2c Re b e−iϕ.
We choose ϕ such that Re b e−iϕ = 0. Then Tr Q1A = 0 and
Tr Q1A2 = p(a2
1 + |b|2) + (1 −p)(a2
2 + |b|2) = Tr ρA2.
Let
Q2 =
⎡
p
−c e−iϕ
−c eiϕ 1 −p
⎢
.
Then
ρ = 1
2 Q1 + 1
2 Q2
and we have
1
2(Tr Q1A2 + Tr Q2 A2) = p(a2
1 + |b|2) + (1 −p)(a2
2 + |b|2) = Tr ρA2.
Therefore we have an equality.
□
We denote by r(ρ) the rank of an operator ρ. The idea of the proof is to reduce
the rank and the block diagonal formalism will be used.
Lemma 2.30 Let ρ be a density matrix and A = A⊥be in Mn(C). Assume the block
matrix forms
ρ =
⎡ρ1 0
0 ρ2
⎢
,
A =
⎡A1 A2
A⊥
2 A3
⎢
and r(ρ1),r(ρ2) > 1. We construct
ρ′ :=
⎡ρ1 X⊥
X ρ2
⎢
such that
Tr ρA = Tr ρ′A,
ρ′ ⊂0,
r(ρ′) < r(ρ).

2.3 Projections
75
Proof: The condition Tr ρA = Tr ρ′A is equivalent to Tr X A2 + Tr X⊥A⊥
2 = 0
and this holds if and only if Re Tr X A2 = 0.
There exist unitaries U and W such that Uρ1U ⊥and Wρ2W ⊥are diagonal:
Uρ1U ⊥= Diag(0, . . . , 0, a1, . . . , ak),
Wρ2W ⊥= Diag(b1, . . . , bl, 0, . . . , 0)
where ai, b j > 0. Then ρ has the same rank, k + l, as the matrix
⎡U 0
0 W
⎢
ρ
⎡U ⊥
0
0 W ⊥
⎢
=
⎡Uρ1U ⊥
0
0
Wρ2W ⊥
⎢
.
A possible modiﬁcation of this matrix is Y :=


Diag(0, . . . , 0, a1, . . . , ak−1)
0
0
0
0
ak
√akb1
0
0
√akb1
b1
0
0
0
0
Diag(b2, . . . , bl, 0, . . . , 0)

⎛⎛⎝
=
⎡Uρ1U ⊥
M
M
Wρ2W ⊥
⎢
and r(Y) = k + l −1. So Y has a smaller rank than ρ. Next we take
⎡U ⊥
0
0 W ⊥
⎢
Y
⎡U 0
0 W
⎢
=
⎡
ρ1
U ⊥MW
W ⊥MU
ρ2
⎢
which has the same rank as Y. If X1 := W ⊥MU is multiplied by eiα (α > 0), then
the positivity condition and the rank remain. On the other hand, we can choose α > 0
such that Re Tr eiαX1A2 = 0. Then X := eiαX1 is the matrix we wanted.
□
Lemma 2.31 Let ρ be a density matrix of rank m > 0 and A = A⊥be in Mn(C).
We claim the existence of a decomposition
ρ = pρ−+ (1 −p)ρ+
such that r(ρ−) < m, r(ρ+) < m, and
Tr Aρ+ = Tr Aρ−= Tr ρA.
Proof: By unitary transformation we can obtain the setup of the previous lemma:
ρ =
⎡ρ1 0
0 ρ2
⎢
,
A =
⎡A1 A2
A⊥
2 A3
⎢
.
With ρ′ as in the previous lemma we choose

76
2
Mappings and Algebras
ρ+ = ρ′ =
⎡ρ1 X⊥
X ρ2
⎢
,
ρ−=
⎡ρ1 −X⊥
−X
ρ2
⎢
.
Then
ρ = 1
2ρ−+ 1
2ρ+
and the requirements Tr Aρ+ = Tr Aρ−= Tr ρA also hold.
□
Proof of Theorem 2.27: For rank 2 states, the theorem is true by Lemma 2.29.
Any state with a rank larger than 2 can be decomposed into a mixture of lower rank
states, according to Lemma 2.31, that have the same expectation value for A as the
original ρ has. The lower rank states can then be decomposed into a mixture of states
with an even lower rank, until we reach states of rank ≥2. Thus, any state ρ can be
decomposed into a mixture of pure states
ρ =

pk Qk
such that Tr AQk = Tr Aρ. Hence the statement of the theorem follows.
□
2.4 Subalgebras
A unital ⊥-subalgebra of Mn(C) is a subspace A that contains the identity I and is
closed under matrix multiplication and adjoint. That is, if A, B →A, then so are AB
and A⊥. In what follows, to simplify the notation, we shall use the term subalgebra
for all ⊥-subalgebras.
Example 2.32 A simple subalgebra is
A =
⎣⎡z w
w z
⎢
: z, w →C
⎤
↓M2(C).
Since A, B →A implies AB = B A, this is a commutative subalgebra. In terms of
the Pauli matrices (2.9) we have
A = {zσ0 + wσ1 : z, w →C} .
This example will be generalized.
□
Assume that P1, P2, . . . , Pn are projections of rank 1 in Mn(C) such that Pi Pj =
0 for i ⊃= j and 
i Pi = I. Then

2.4 Subalgebras
77
A =
 n

i=1
αi Pi : αi →C

is a maximal commutative ⊥-subalgebra of Mn(C). The usual name is MASA, which
is an acronym for Maximal Abelian Sub-Algebra.
Let A be any subset of Mn(C). Then A′, the commutant of A, is given by
A′ = {B →Mn(C) : B A = AB for all A →A}.
It is easy to see that for any set A ↓Mn(C), A′ is a subalgebra. If A is a MASA,
then A′′ = A.
Theorem 2.33 If A ↓Mn(C) is a unital ⊥-subalgebra, then A′′ = A.
Proof: We ﬁrst show that for any ⊥-subalgebra A, B →A′′ and any v →Cn, there
exists an A →A such that Av = Bv. Let K be the subspace of Cn given by
K = {Av : A →A}.
Let P betheorthogonal projectionontoK inCn. Since, byconstruction, K is invariant
under the action of A, P AP = AP for all A →A. Taking the adjoint, P A⊥P = P A⊥
for all A →A. Since A is a ⊥-algebra, this implies P A = AP for all A →A. That is,
P →A′. Thus, for any B →A′′, B P = P B and so K is invariant under the action
of A′′. In particular, Bv →K and hence, by the deﬁnition of K, Bv = Av for some
A →A.
We apply the previous statement to the ⊥-subalgebra
M = {A ∞In : A →A} ↓Mn(C) ∞Mn(C) = Mn2(C).
It is easy to see that
M′′ = {B ∞In : B →A′′} ↓Mn(C) ∞Mn(C).
Now let {v1, . . . , vn} be any basis of Cn and form the vector
v =


v1
v2
...
vn

⎛⎛⎛⎝→Cn2.
Then
(A ∞In)v = (B ∞In)v

78
2
Mappings and Algebras
and Av j = Bv j for every 1 ≥j ≥n. Since {v1, . . . , vn} is a basis of Cn, this means
B = A →A. Since B was an arbitrary element of A′′, this shows that A′′ ↓A. Since
A ↓A′′ is an automatic consequence of the deﬁnitions, this proves that A′′ = A. □
Next we study subalgebras A ↓B ↓Mn(C). A conditional expectation E :
B ∗A is a unital positive mapping which has the property
E(AB) = AE(B) for every
A →A and
B →B.
Choosing B = I, we obtain that E acts identically on A. It follows from the positivity
of E that E(C⊥) = E(C)⊥. Therefore, E(B A) = E(B)A for all A →A and B →B.
Another standard notation for a conditional expectation B ∗A is EB
A.
Theorem 2.34 Assume that A ↓B ↓Mn(C). If α : A ∗B is the embedding,
then the dual E : B ∗A of α with respect to the Hilbert–Schmidt inner product is
a conditional expectation.
Proof: From the deﬁnition
Tr α(A)B = Tr AE(B)
(A →A, B →B)
of the dual, we see that E : B ∗A is a positive unital mapping and E(A) = A for
every A →A. For every A, A1 →A and B →B we further have
Tr AE(A1B) = Tr α(A)A1B = Tr α(AA1)B = Tr AA1E(B),
which implies that E(A1B) = A1E(B).
□
Note that a conditional expectation E : B ∗A has norm 1, that is, ∩E(B)∩≥∩B∩
for every B →B. This follows from Corollary 2.45.
The subalgebras A1, A2 ↓Mn(C) cannot be orthogonal since I is in A1 and in
A2. They are called complementary or quasi-orthogonal if Ai →Ai and Tr Ai = 0
for i = 1, 2 imply that Tr A1A2 = 0.
Example 2.35 In M2(C) the subalgebras
Ai := {aσ0 + bσi : a, b →C}
(1 ≥i ≥3)
are commutative and quasi-orthogonal. This follows from the facts that Tr σi = 0
for 1 ≥i ≥3 and
σ1σ2 = iσ3,
σ2σ3 = iσ1
σ3σ1 = iσ2.
So M2(C) has 3 quasi-orthogonal MASAs.
In M4(C) = M2(C) ∞M2(C) we can give ﬁve quasi-orthogonal MASAs. Each
of them is the linear span of four operators in one of the following lines:

2.4 Subalgebras
79
σ0 ∞σ0,
σ0 ∞σ1,
σ1 ∞σ0,
σ1 ∞σ1,
σ0 ∞σ0,
σ0 ∞σ2,
σ2 ∞σ0,
σ2 ∞σ2,
σ0 ∞σ0,
σ0 ∞σ3,
σ3 ∞σ0,
σ3 ∞σ3,
σ0 ∞σ0,
σ1 ∞σ2,
σ2 ∞σ3,
σ3 ∞σ1,
σ0 ∞σ0,
σ1 ∞σ3,
σ2 ∞σ1,
σ3 ∞σ2.
□
Theorem 2.36 Assume that {Ai : 1 ≥i ≥k} is a set of quasi-orthogonal MASAs
in Mn(C). Then k ≥n + 1.
Proof: The argument is rather simple. The traceless part of Mn(C) has dimension
n2 −1 and the traceless part of a MASA has dimension n −1. Therefore k ≥
(n2 −1)/(n −1) = n + 1.
□
Determining the maximal number of quasi-orthogonal MASAs is a hard problem.
For example, if n = 2m, then n + 1 MASAs is possible, but for an arbitrary n there
is no deﬁnite result.
The next theorem gives a characterization of complementarity.
Theorem 2.37 Let A1 and A2 be subalgebras of Mn(C) and denote Tr /n by τ. The
following conditions are equivalent:
(i) If P →A1 and Q →A2 are minimal projections, then τ(P Q) = τ(P)τ(Q).
(ii) The subalgebras A1 and A2 are quasi-orthogonal in Mn(C).
(iii) τ(A1 A2) = τ(A1)τ(A2) if A1 →A1, A2 →A2.
(iv) If E1 : Mn(C) ∗A1 is the trace-preserving conditional expectation, then E1
restricted to A2 is a linear functional (times I).
Proof: Note that τ((A1 −τ(A1)I)(A2 −τ(A2)I)) = 0 and τ(A1A2) =
τ(A1)τ(A2) are equivalent. If they hold for minimal projections, they hold for ar-
bitrary operators as well. Moreover, (iv) is equivalent to the property τ(A1E1(A2))
= τ(A1(τ(A2)I)) for every A1 →A1 and A2 →A2, and note that τ(A1E1(A2))
= τ(A1A2).
□
Example 2.38 A simple example of quasi-orthogonal subalgebras can be formu-
lated with tensor products. If A = Mn(C) ∞Mn(C), A1 = Mn(C) ∞CIn ↓A and
A2 = CIn ∞Mn(C) ↓A, then A1 and A2 are quasi-orthogonal subalgebras of A.
This comes from the property Tr (A ∞B) = Tr A · Tr B.
For n = 2 we give another example using the Pauli matrices. The 4-dimensional
subalgebra A1 = M2(C) ∞CI2 is the linear span of the set
{σ0 ∞σ0, σ1 ∞σ0, σ2 ∞σ0, σ3 ∞σ0}.
Together with the identity, each of the following triplets linearly spans a subalgebra
A j isomorphic to M2(C) (2 ≥j ≥4):

80
2
Mappings and Algebras
{σ3 ∞σ1, σ3 ∞σ2, σ0 ∞σ3},
{σ2 ∞σ3, σ2 ∞σ1, σ0 ∞σ2},
{σ1 ∞σ2, σ1 ∞σ3, σ0 ∞σ1} .
It is easy to check that the subalgebras A1, . . . , A4 are complementary.
The orthogonal complement of the four subalgebras is spanned by {σ0 ∞σ3, σ3 ∞
σ0, σ3 ∞σ3}. The linear span of this together with σ0 ∞σ0 is a commutative subal-
gebra.
□
The previous example describes the general situation for M4(C). This will be the
content of the next theorem. It is easy to calculate that the number of complementary
subalgebras isomorphic to M2(C) is at most (16 −1)/3 = 5. However, the next
theorem says that 5 is not possible.
If x = (x1, x2, x3) →R3, then the notation
x · σ = x1σ1 + x2σ2 + x3σ3
will be used and shall be called a Pauli triplet.
Theorem 2.39 Assume that {Ai : 0 ≥i ≥3} is a family of pairwise quasi-
orthogonal subalgebras of M4(C) which are isomorphic to M2(C). For every
0 ≥i ≥3, there exists a Pauli triplet A(i, j) ( j ⊃= i) such that A′
i ∧A j is the
linear span of I and A(i, j). Moreover, the subspace linearly spanned by
I
and
⎦3

i=0
Ai
⊥
is a maximal Abelian subalgebra.
Proof: Since the intersection A′
0∧A j is a 2-dimensional commutative subalgebra,
we can ﬁnd a self-adjoint unitary A(0, j) such that A′
0 ∧A j is spanned by I and
A(0, j) = x(0, j) · σ ∞I, where x(0, j) →R3. Due to the quasi-orthogonality of
A1, A2 and A3, the unit vectors x(0, j) are pairwise orthogonal (see (2.18)). The
matrices A(0, j) anti-commute:
A(0, i)A(0, j) = i(x(0, i) × x(0, j)) · σ ∞I
= −i(x(0, j) × x(0, i)) · σ ∞I = −A(0, j)A(0, i)
for i ⊃= j. Moreover,
A(0, 1)A(0, 2) = i(x(0, 1) × x(0, 2)) · σ

2.4 Subalgebras
81
and x(0, 1) × x(0, 2) = ±x(0, 3) because x(0, 1) × x(0, 2) is orthogonal to
both x(0, 1) and x(0, 2). If necessary, we can change the sign of x(0, 3) so that
A(0, 1)A(0, 2) = iA(0, 3) holds.
Starting with the subalgebras A′
1, A′
2, A′
3 we can similarly construct the other
Pauli triplets. In this way, we arrive at the four Pauli triplets, the rows of the following
table:
α
A(0, 1) A(0, 2) A(0, 3)
A(1, 0)
α
A(1, 2) A(1, 3)
A(2, 0) A(2, 1)
α
A(2, 3)
A(3, 0) A(3, 1) A(3, 2)
α
(2.14)
When {Ai : 1 ≥i ≥3} is a family of pairwise quasi-orthogonal subalgebras, then
the commutants {A′
i : 1 ≥i ≥3} are pairwise quasi-orthogonal as well. A′′
j = A j
and A′
i have nontrivial intersection for i ⊃= j, actually the previously deﬁned A(i, j)
is in the intersection. For a ﬁxed j the three unitaries A(i, j) (i ⊃= j) form a Pauli
triplet up to a sign. (It follows that changing sign we can always reach the situation
where the ﬁrst three columns of table (2.14) form Pauli triplets. A(0, 3) and A(1, 3)
anti-commute, but it may happen that A(0, 3)A(1, 3) = −iA(2, 3).)
1’
1
0
0’
2
3’
2’
3
A(2,3)
A(0,3)
A(1,3)
This picture shows a family {Ai : 0 ≥i ≥3} of pairwise quasi-orthogonal
subalgebras of M4(C) which are isomorphic to M2(C). The edges between two
vertices represent the one-dimensional traceless intersection of the two
subalgebras corresponding to two vertices. The three edges starting from a vertex
represent a Pauli triplet.
Let C0 := {±A(i, j)A( j, i) : i ⊃= j} ∪{±I} and C := C0 ∪iC0. We want to
show that C is a commutative group (with respect to the multiplication of unitaries).
Note that the products in C0 have factors in symmetric position in (2.14) with
respect to the main diagonal indicated by stars. Moreover, A(i, j) →A( j) and
A( j, k) →A( j)′, and these operators commute.
We have two cases for a product in C. Taking the product of A(i, j)A( j, i) and
A(u, v)A(v, u), we have
(A(i, j)A( j, i))(A(i, j)A( j, i)) = I

82
2
Mappings and Algebras
in the simplest case, since A(i, j) and A( j, i) are commuting self-adjoint unitaries.
The situation is slightly more complicated if the cardinality of the set {i, j, u, v} is 3
or 4. First,
(A(1, 0)A(0, 1))(A(3, 0)A(0, 3)) = A(0, 1)(A(1, 0)A(3, 0))A(0, 3)
= ±i(A(0, 1)A(2, 0))A(0, 3)
= ±iA(2, 0)(A(0, 1)A(0, 3))
= ±A(2, 0)A(0, 2),
and secondly,
(A(1, 0)A(0, 1))(A(3, 2)A(2, 3)) = ±iA(1, 0)A(0, 2)(A(0, 3)A(3, 2))A(2, 3)
= ±iA(1, 0)A(0, 2)A(3, 2)(A(0, 3)A(2, 3))
= ±A(1, 0)(A(0, 2)A(3, 2))A(1, 3)
= ±iA(1, 0)(A(1, 2)A(1, 3))
= ±A(1, 0)A(1, 0) = ±I.
(2.15)
So the product of any two operators in C is again in C.
Now we show that the subalgebra C linearly spanned by the unitaries {A(i, j)
A( j, i) : i ⊃= j} ∪{I} is a maximal Abelian subalgebra. Since we know the com-
mutativity of this algebra, we estimate the dimension. It follows from (2.15) and the
self-adjointness of A(i, j)A( j, i) that
A(i, j)A( j, i) = ±A(k, σ)A(σ, k)
when i, j, k and σ are different. Therefore C is linearly spanned by A(0, 1)A(1, 0),
A(0, 2)A(2, 0), A(0, 3)A(3, 0) and I. These are four different self-adjoint unitaries.
Finally, we check that the subalgebra C is quasi-orthogonal to A(i). If the cardi-
nality of the set {i, j, k, σ} is 4, then we have
Tr A(i, j)(A(i, j)A( j, i)) = Tr A( j, i) = 0
and
Tr A(k, σ)A(i, j)A( j, i) = ±Tr A(k, σ)A(k,l)A(σ, k) = ±Tr A(σ, k) = 0.
Moreover, because A(k) is quasi-orthogonal to A(i), we also have A(i, k)⊥A( j, i),
so
Tr A(i, σ)(A(i, j)A( j, i)) = ±i Tr A(i, k)A( j, i) = 0.
From this we can conclude that
A(k, σ) ⊥A(i, j)A( j, i)

2.4 Subalgebras
83
for all k ⊃= σ and i ⊃= j.
□
2.5 Kernel Functions
Let X be a non-empty set. A function ψ : X × X ∗C is often called a kernel. A
kernel ψ : X × X ∗C is called positive deﬁnite if
n

j,k=1
c jckψ(x j, xk) ⊂0
for all ﬁnite sets {c1, c2, . . . , cn} ↓C and {x1, x2, . . . , xn} ↓X.
Example 2.40 It follows from Schur’s theorem that the product of positive deﬁnite
kernels is a positive deﬁnite kernel as well.
If ψ : X × X ∗C is positive deﬁnite, then
eψ =
◦

n=0
1
n! ψm
and ˜ψ(x, y) = f (x)ψ(x, y) f (y) are positive deﬁnite for any function f : X ∗
C.
□
The function ψ : X × X ∗C is called a conditionally negative deﬁnite kernel
if ψ(x, y) = ψ(y, x) and
n

j,k=1
c jckψ(x j, xk) ≥0
for all ﬁnite sets {c1, c2, . . . , cn} ↓C and {x1, x2, . . . , xn} ↓X when n
j=1 c j = 0.
The above properties of a kernel depend on the matrices


ψ(x1, x1) ψ(x1, x2) . . . ψ(x1, xn)
ψ(x2, x1) ψ(x2, x2) . . . ψ(x2, xn)
...
...
...
...
ψ(xn, x1) ψ(xn, x2) . . . ψ(xn, xn)

⎛⎛⎛⎝.
If a kernel is positive deﬁnite, then −f is conditionally negative deﬁnite, but the
converse is not true.
Lemma 2.41 Assume that the function ψ : X ×X ∗C has the property ψ(x, y) =
ψ(y, x) and ﬁx x0 →X. Then

84
2
Mappings and Algebras
ϕ(x, y) := −ψ(x, y) + ψ(x, x0) + ψ(x0, y) −ψ(x0, x0)
is positive deﬁnite if and only if ψ is conditionally negative deﬁnite.
The proof is rather straightforward, but an interesting particular case is the fol-
lowing:
Example 2.42 Assume that f : R+ ∗R is a C1-function with the property f (0) =
f ′(0) = 0. Let ψ : R+ × R+ ∗R be deﬁned as
ψ(x, y) =



f (x) −f (y)
x −y
if
x ⊃= y,
f ′(x)
if
x = y.
(This is the so-called kernel of divided difference.) Assume that this is conditionally
negative deﬁnite. Now we apply the lemma with x0 = ε:
−f (x) −f (y)
x −y
+ f (x) −f (ε)
x −ε
+ f (ε) −f (y)
ε −y
−f ′(ε)
is positive deﬁnite and from the limit ε ∗0, we have the positive deﬁnite kernel
−f (x) −f (y)
x −y
+ f (x)
x
+ f (y)
y
= −f (x)y2 −f (y)x2
x(x −y)y
.
Assume that f (x) > 0 for all x > 0. Multiplication by xy/( f (x) f (y)) gives a
positive deﬁnite kernel
x2
f (x) −
y2
f (y)
x −y
,
which is a divided difference of the function g(x) := x2/f (x) on (0, ◦).
□
Theorem 2.43 (Schoenberg’s theorem) Let X be a non-empty set and let ψ :
X × X ∗C be a kernel. Then ψ is conditionally negative deﬁnite if and only if
exp(−tψ) is positive deﬁnite for every t > 0.
Proof:Ifexp(−tψ)ispositivedeﬁnite,then1−exp(−tψ)isconditionallynegative
deﬁnite and so is
ψ = lim
t∗0
1
t (1 −exp(−tψ)).
Assume now that ψ is conditionally negative deﬁnite. Take x0 →X and set

2.5 Kernel Functions
85
ϕ(x, y) := −ψ(x, y) + ψ(x, x0) + ψ(x0, y) −ψ(x0, x0),
which is positive deﬁnite due to the previous lemma. Then
e−ψ(x,y) = eϕ(x,y)e−ψ(x,x0)e−ψ(y,x0)eψ(x0,x0)
is positive deﬁnite. This proves the case t = 1, and the argument is similar for general
t > 0.
□
The kernel functions are a kind of generalization of matrices. If A →Mn, then the
corresponding kernel function is given by X := {1, 2, . . . , n} and
ψA(i, j) = Ai j
(1 ≥i, j ≥n).
Therefore the results of this section have matrix consequences.
2.6 Positivity-Preserving Mappings
Let α : Mn ∗Mk be a linear mapping. It is called positive (or positivity-preserving)
if it sends positive (semideﬁnite) matrices to positive (semideﬁnite) matrices. α is
unital if α(In) = Ik.
The dual α⊥: Mk ∗Mn of α is deﬁned by the equation
Tr α(A)B = Tr Aα⊥(B)
(A →Mn, B →Mk) .
It is easy to see that α is positive if and only if α⊥is positive and α is trace-preserving
if and only if α⊥is unital.
The inequality
α(AA⊥) ⊂α(A)α(A)⊥
iscalledtheSchwarzinequality.IftheSchwarzinequalityholdsforalinearmapping
α, then α is positivity-preserving. If α is a positive mapping, then this inequality holds
for normal matrices. This result is called the Kadison inequality.
Theorem 2.44 Let α : Mn(C) ∗Mk(C) be a positive unital mapping.
(1) If A →Mn is a normal operator, then
α(AA⊥) ⊂α(A)α(A)⊥.
(2)
If A →Mn is positive such that A and α(A) are invertible, then
α(A−1) ⊂α(A)−1.

86
2
Mappings and Algebras
Proof: A has a spectral decomposition 
i λi Pi, where the Pi’s are pairwise
orthogonal projections. We have A⊥A = 
i |λi|2Pi and
⎡
I
α(A)
α(A)⊥α(A⊥A)
⎢
=

i
⎡1
λi
λi |λi|2
⎢
∞α(Pi).
Since α(Pi) is positive, the left-hand side is positive as well. Reference to Theorem
2.1 gives the ﬁrst inequality.
To prove the second inequality, use the identity
⎡α(A)
I
I
α(A−1)
⎢
=

i
⎡λi
1
1 λ−1
i
⎢
∞α(Pi)
to conclude that the left-hand side is a positive block matrix. The positivity implies
our statement.
□
Corollary 2.45 A positive unital mapping α : Mn(C) ∗Mk(C) has norm 1, i.e.,
∩α(A)∩≥∩A∩for every A →Mn(C).
Proof: Let A →Mn(C) be such that ∩A∩≥1, and take the polar decomposition
A = U|A| with a unitary U. By Example 1.39 there is a unitary V such that |A| =
(V +V ⊥)/2 and so A = (UV +UV ⊥)/2. Hence it sufﬁces to show that ∩α(U)∩≥1
for every unitary U. This follows from the Kadison inequality in (1) of the previous
theorem as
∩α(U)∩2 = ∩α(U)⊥α(U)∩≥∩α(U ⊥U)∩= ∩α(I)∩= 1.
□
The linear mapping α : Mn ∗Mk is called 2-positive if
⎡A B
B⊥C
⎢
⊂0 implies
⎡α(A) α(B)
α(B⊥) α(C)
⎢
⊂0
when A, B, C →Mn.
Lemma 2.46 Let α : Mn(C) ∗Mk(C) be a 2-positive mapping. If A, α(A) > 0,
then
α(B)⊥α(A)−1α(B) ≥α(B⊥A−1B)
for every B →Mn. Hence, a 2-positive unital mapping satisﬁes the Schwarz inequal-
ity.
Proof: Since

2.6 Positivity-Preserving Mappings
87
⎡A
B
B⊥B⊥A−1B
⎢
⊂0,
the 2-positivity implies
⎡α(A)
α(B)
α(B⊥) α(B⊥A−1B)
⎢
⊂0.
So Theorem 2.1 implies the statement.
□
If B = B⊥, then the 2-positivity condition is not necessary in the previous lemma,
positivity is enough.
Lemma 2.47 Let α : Mn ∗Mk be a 2-positive unital mapping. Then
Nα := {A →Mn : α(A⊥A) = α(A)⊥α(A) and α(AA⊥) = α(A)α(A)⊥}
is a subalgebra of Mn and
α(AB) = α(A)α(B) and α(B A) = α(B)α(A)
holds for all A →Nα and B →Mn.
Proof: The proof is based only on the Schwarz inequality. Assume that α(AA⊥) =
α(A)α(A)⊥. Then
t

α(A)α(B) + α(B)⊥α(A)⊥
= α(t A⊥+ B)⊥α(t A⊥+ B) −t2α(A)α(A)⊥−α(B)⊥α(B)
≥α

(t A⊥+ B)⊥(t A⊥+ B)

−t2α(AA⊥) −α(B)⊥α(B)
= tα(AB + B⊥A⊥) + α(B⊥B) −α(B)⊥α(B)
for a real t. Divide the inequality by t and let t ∗±◦. Then
α(A)α(B) + α(B)⊥α(A)⊥= α(AB + B⊥A⊥)
and similarly
α(A)α(B) −α(B)⊥α(A)⊥= α(AB −B⊥A⊥).
Adding these two equalities we have
α(AB) = α(A)α(B).
The other identity is proven similarly.
□
It follows from the previous lemma that if α is a 2-positive unital mapping and its
inverse is 2-positive as well, then α is multiplicative. Indeed, the assumption implies
α(A⊥A) = α(A)⊥α(A) for every A.

88
2
Mappings and Algebras
A linear mapping E : Mn ∗Mk is called completely positive if
E ∞idn : Mn ∞Mn ∗Mk ∞Mn
is a positive mapping, where idn : Mn ∗Mn is the identity mapping and E ∞idn is
deﬁned by
(E ∞idn)

[Xi j]n
i, j=1

:= [E(Xi j)]n
i, j=1.
(Here, B(H) ∞Mn is identiﬁed with the n × n block matrices whose entries are
operators in B(H).) Note that if a linear mapping E : Mn ∗Mk is completely
positive in the above sense, then E ∞idm : Mn ∞Mm ∗Mk ∞Mm is positive for
every m →N.
Example 2.48 Consider the transpose mapping E : A ∗At on 2 × 2 matrices:
⎡x y
z w
⎢
∗
⎡x z
y w
⎢
.
E is obviously positive. The matrix


2 0 0 2
0 1 1 0
0 1 1 0
2 0 0 2

⎛⎛⎝
is positive. The extension of E maps this to


2 0 0 1
0 1 2 0
0 2 1 0
1 0 0 2

⎛⎛⎝.
This is not positive, so E is not completely positive.
□
Theorem 2.49 Let E : Mn ∗Mk be a linear mapping. Then the following condi-
tions are equivalent:
(1) E is completely positive.
(2) The block matrix X deﬁned by
Xi j = E(E(i j))
(1 ≥i, j ≥n)
(2.16)
is positive, where E(i j) are the matrix units of Mn.
(3) There are operators Vt : Cn ∗Ck (1 ≥t ≥k2) such that

2.6 Positivity-Preserving Mappings
89
E(A) =

t
Vt AV ⊥
t .
(2.17)
(4) For ﬁnite families Ai →Mn(C) and Bi →Mk(C) (1 ≥i ≥n), the inequality

i, j
B⊥
i E(A⊥
i A j)B j ⊂0
holds.
Proof: (1) implies (2): The matrix

i, j
E(i j) ∞E(i j) = 1
n
⎦
i, j
E(i j) ∞E(i j)
2
is positive. Therefore,

idn ∞E

⎧
⎪
i, j
E(i j) ∞E(i j)
⎨
⎩=

i, j
E(i j) ∞E(E(i j)) = X
is positive as well.
(2) implies (3): Assume that the block matrix X is positive. There are orthogonal
projections Pi (1 ≥i ≥n) on Cnk such that they are pairwise orthogonal and
Pi X Pj = E(E(i j)).
We have a decomposition
X =
nk

t=1
| ft⟩≤ft|,
where | ft⟩are appropriately normalized eigenvectors of X. Since Pi is a partition of
unity, we have
| ft⟩=
n

i=1
Pi| ft⟩
and we deﬁne Vt : Cn ∗Ck by
Vt|i⟩= Pi| ft⟩.
(|i⟩are the canonical basis vectors.) In this notation,

90
2
Mappings and Algebras
X =

t

i, j
Pi| ft⟩≤ft|Pj =

i, j
Pi

t
Vt|i⟩≤j|V ⊥
t

Pj
and hence
E(E(i j)) = Pi X Pj =

t
Vt E(i j)V ⊥
t .
Since this holds for all matrix units E(i j), we obtain
E(A) =

t
Vt AV ⊥
t .
(3) implies (4): Assume that E is of the form (2.17). Then

i, j
B⊥
i E(A⊥
i A j)B j =

t

i, j
B⊥
i Vt(A⊥
i A j)V ⊥
t B j
=

t
⎦
i
Ai V ⊥
t Bi
⊥⎦
j
A j V ⊥
t B j

⊂0
follows.
(4) implies (1): We consider
E ∞idn : Mn ∞Mn ∗Mk ∞Mn.
Since any positive operator in Mn ∞Mn is the sum of operators in the form

i, j A⊥
i A j ∞E(i j) (Theorem 2.8), it is enough to show that
Y := E ∞idn
⎦
i, j
A⊥
i A j ∞E(i j)

=

i, j
E(A⊥
i A j) ∞E(i j)
is positive. On the other hand, Y = [Yi j]n
i, j=1 →Mk ∞Mn is positive if and only if

i, j
B⊥
i Yi j B j =

i, j
B⊥
i E(A⊥
i A j)B j ⊂0.
The positivity of this operator is assumed in (4). Hence (1) follows.
□
The representation (2.17) is called the Kraus representation. The block matrix
X deﬁned by (2.16) is called the representing block matrix (or the Choi matrix).
Example 2.50 We take A ↓B ↓Mn(C) and a conditional expectation E : B ∗A.
Using condition (4) of the previous theorem we can argue that E is completely
positive. For Ai →A and Bi →B we have

2.6 Positivity-Preserving Mappings
91

i, j
A⊥
i E(B⊥
i B j)A j = E
⎦⎦
i Bi Ai
⊥⎦
j B j A j

⊂0
and this is enough.
□
The next example is slightly different.
Example 2.51 Let H and K be Hilbert spaces and ( fi) be a basis in K. For each
i deﬁne the linear operator Vi : H ∗H ∞K by Vie = e ∞fi (e →H). These
operators are isometries with pairwise orthogonal ranges and the adjoints act as
V ⊥
i (e ∞f ) = ≤fi, f ⟩e.
The partial trace Tr2 : B(H∞K) ∗B(H) introduced in Sect.1.7 can be written
as
Tr2(A) =

i
V ⊥
i AVi
(A →B(H ∞K)).
The reason for the terminology is the formula Tr2(X ∞Y) = XTr Y. The above
expression implies that Tr2 is completely positive. It is actually a conditional expec-
tation up to a constant factor.
□
Example 2.52 The trace Tr : Mk(C) ∗C is completely positive if Tr ∞idn :
Mk(C) ∞Mn(C) ∗Mn(C) is a positive mapping. However, this is a partial trace
which is known to be positive (even completely positive).
It follows that any positive linear functional ψ : Mk(C) ∗C is completely
positive. Since ψ(A) = Tr DA for some positive D, ψ is the composition of the
completely positive mappings A ∗D1/2 AD1/2 and Tr .
□
Example 2.53 Let E : Mn ∗Mk be a positive linear mapping such that E(A) and
E(B) commute for any A, B →Mn. We want to show that E is completely positive.
Any two self-adjoint matrices in the range of E commute, so we can change the
basis so that all of them become diagonal. It follows that E has the form
E(A) =

i
ψi(A)Eii,
where Eii are the diagonal matrix units and ψi are positive linear functionals. Since
the sum of completely positive mappings is completely positive, it is enough to show
that A ∗ψ(A)F is completely positive for a positive functional ψ and for a positive
matrix F. The complete positivity of this mapping means that for an m × m block
matrix X with entries Xi j →Mn, if X ⊂0 then the block matrix [ψ(Xi j)F]n
i, j=1
should be positive. This is true, since the matrix [ψ(Xi j)]n
i, j=1 is positive (due to the
complete positivity of ψ).
□
Example 2.54 A linear mapping E : M2 ∗M2 is deﬁned by the formula

92
2
Mappings and Algebras
E :
⎡1 + z x −iy
x + iy 1 −z
⎢
∗
⎡1 + γz
αx −iβy
αx + iβy
1 −γz
⎢
where α, β, γ are real parameters.
The condition for positivity is
−1 ≥α, β, γ ≥1.
It is not difﬁcult to compute the representing block matrix as follows:
X = 1
2


1 + γ
0
0
α + β
0
1 −γ α −β
0
0
α −β 1 −γ
0
α + β
0
0
1 + γ

⎛⎛⎝.
This matrix is positive if and only if
|1 ± γ| ⊂|α ± β|.
In quantum information theory this mapping E is called the Pauli channel.
□
Example 2.55 Fix a positive deﬁnite matrix A →Mn and set
TA(K) =
	 ◦
0
(t + A)−1K(t + A)−1 dt
(K →Mn).
This mapping TA : Mn ∗Mn is obviously positivity-preserving and approximation
of the integral by a ﬁnite sum also shows the complete positivity.
If A = Diag(λ1, λ2, . . . , λn), then we see from integration that the entries of
TA(K) are
TA(K)i j = log λi −log λ j
λi −λ j
Ki j.
Another integration gives that the mapping
α : L ∗
	 1
0
At L A1−t dt
acts as
(α(L))i j =
λi −λ j
log λi −log λ j
Li j.
This shows that
T −1
A (L) =
	 1
0
At L A1−t dt.

2.6 Positivity-Preserving Mappings
93
To show that T −1
A
is not positive, we take n = 2 and consider
T −1
A
⎡1 1
1 1
⎢
=


λ1
λ1 −λ2
log λ1 −log λ2
λ1 −λ2
log λ1 −log λ2
λ2

⎛⎛⎛⎝.
The positivity of this matrix is equivalent to the inequality
$
λ1λ2 ⊂
λ1 −λ2
log λ1 −log λ2
between the geometric and logarithmic means. The opposite inequality holds, see
Example 5.22, and therefore T −1
A
is not positive.
□
The next result tells us that the Kraus representation of a completely positive
mapping is unique up to a unitary matrix.
Theorem 2.56 Let E : Mn(C) ∗Mm(C) be a linear mapping which is represented
as
E(A) =
k

t=1
Vt AV ⊥
t
and
E(A) =
k

t=1
Wt AW ⊥
t
with operators Vt, Wt : Cn ∗Cm. Then there exists a k × k unitary matrix [ctu]
such that
Wt =

u
ctuVu
(1 ≥t ≥k).
Proof: Without loss of generality we may assume that m ⊂n. Indeed, we can
embed Mm = B(Cm) into a bigger Mm′ = B(Cm′) and consider E as a mapping
Mn ∗Mm′. Let xi be a basis in Cm and y j be a basis in Cn. Consider the vectors
vt :=
n

j=1
x j ∞Vt y j
and wt :=
n

j=1
x j ∞Wt y j .
We have
|vt⟩≤vt| =

j, j′
|x j⟩≤x j′| ∞Vt|y j⟩≤y j′|V ⊥
t
and
|wt⟩≤wt| =

j, j′
|x j⟩≤xi′| ∞Wt|y j⟩≤y j′|W ⊥
t .
Our hypothesis implies that

94
2
Mappings and Algebras

t
|vt⟩≤vt| =

t
|wt⟩≤wt| .
Lemma 1.24 tells us that there is a unitary matrix [ctu] such that
wt =

u
ctuvu .
This implies that
Wt y j =

u
ctuVu y j
(1 ≥j ≥n).
Hence we conclude the statement of the theorem.
□
2.7 Notes and Remarks
Theorem 2.5 is from the paper J.-C. Bourin and E.-Y. Lee, Unitary orbits of Hermitian
operators with convex or concave functions, Bull. London Math. Soc. 44(2012),
1085–1102.
The Wielandt inequality has an extension to matrices. Let A be an n ×n positive
matrix with eigenvalues λ1 ⊂λ2 ⊂· · · ⊂λn. Let X and Y be n × p and n × q
matrices such that X⊥Y = 0. The generalized inequality is
X⊥AY(Y ⊥AY)−Y ⊥AX ≥
⎞λ1 −λn
λ1 + λn
⎠2
X⊥AX,
where a generalized inverse (Y ⊥AY)−is included: BB−B = B. See Song-Gui Wang
and Wai-Cheung Ip, A matrix version of the Wielandt inequality and its applications
to statistics, Linear Algebra Appl. 296(1999), 171–181.
The lattice of ortho-projections has applications in quantum theory. The cited
Gleason theorem was obtained by A. M. Gleason in 1957, see also R. Cooke,
M. Keane and W. Moran, An elementary proof of Gleason’s theorem, Math. Proc.
Cambridge Philos. Soc. 98(1985), 117–128.
Theorem 2.27 is from the paper D. Petz and G. Tóth, Matrix variances with
projections, Acta Sci. Math. (Szeged), 78(2012), 683–688. An extension of this
result is in the paper Z. Léka and D. Petz, Some decompositions of matrix variances,
to be published.
Theorem 2.33 is the double commutant theorem of von Neumann from 1929; the
original proof was for operators on an inﬁnite-dimensional Hilbert space. (There is
a relevant difference between ﬁnite and inﬁnite dimensions; in a ﬁnite-dimensional
space all subspaces are closed.) The conditional expectation in Theorem 2.34 was ﬁrst
introduced in the paper H. Umegaki, Conditional expectation in an operator algebra,

2.7 Notes and Remarks
95
Tôhoku Math. J. 6(1954), 177–181, and it is related to the so-called Tomiyama
theorem.
The maximum number of complementary MASAs in Mn(C) is a popular subject.
If n is a prime power, then n+1 MASAs can be constructed, but n = 6 is an unknown
problematic case. (The expected number of complementary MASAs is 3 here.) It is
interesting that n MASAs cannot exist in Mn(C) for any n > 1, see the paper [83]
of M. Weiner.
Theorem 2.39 is from the paper H. Ohno, D. Petz and A. Szántó, Quasi-orthogonal
subalgebras of 4 × 4 matrices, Linear Algebra Appl. 425(2007), 109–118. It was
conjectured that in the case n = 2k the algebra Mn(C) cannot have Nk := (4k −
1)/3 complementary subalgebras isomorphic to M2, but it was proved that there
are Nk −1 copies. 2 is not a typical prime number in this situation. If p > 2 is
a prime number, then in the case n = pk the algebra Mn(C) has Nk := (p2k −
1)/(p2 −1) complementary subalgebras isomorphic to Mp, see the paper H. Ohno,
Quasi-orthogonal subalgebras of matrix algebras, Linear Algebra Appl. 429(2008),
2146–2158.
Positive and conditionally negative deﬁnite kernel functions are well discussed in
the book C. Berg, J. P. R. Christensen and P. Ressel, Harmonic Analysis on Semi-
groups. Theory of Positive Deﬁnite and Related Functions, Graduate Texts in Math-
ematics, vol. 100. Springer, New York, 1984. (It is noteworthy that conditionally
negative deﬁnite is called there ‘negative deﬁnite’.)
2.8 Exercises
1. Show that
⎡A B
B⊥C
⎢
⊂0
if and only if B = A1/2ZC1/2 for a matrix Z with ∩Z∩≥1.
2. Let X,U, V →Mn and assume that U and V are unitaries. Prove that


I
U X
U ⊥I
V
X⊥V ⊥I

⎝⊂0
if and only if X = UV .
3. Show that for A, B →Mn the formula
⎡I A
0 I
⎢−1 ⎡AB 0
B 0
⎢⎡I A
0 I
⎢
=
⎡0
0
B B A
⎢
holds. Conclude that AB and B A have the same eigenvectors.
4. Assume that 0 < A →Mn. Show that A + A−1 ⊂2I.

96
2
Mappings and Algebras
5. Assume that
A =
⎡A1 B
B⊥A2
⎢
> 0.
Show that det A ≥det A1 × det A2.
6. Assume that the eigenvalues of the self-adjoint matrix
⎡A B
B⊥C
⎢
are λ1 ≥λ2 ≥. . . λn and the eigenvalues of A are β1 ≥β2 ≥· · · ≥βm. Show
that
λi ≥βi ≥λi+n−m.
7. Show that a matrix A →Mn is irreducible if and only if for every 1 ≥i, j ≥n
there is a power k such that (Ak)i j ⊃= 0.
8. Let A, B, C, D →Mn and AC = C A. Show that
det
⎡A B
C D
⎢
= det(AD −C B).
9. Let A, B, C →Mn and
⎡A B
B⊥C
⎢
⊂0.
Show that B⊥∼B ≥A ∼C.
10. Let A, B →Mn. Show that A ∼B is a submatrix of A ∞B.
11. Assume that P and Q are projections. Show that P ≥Q is equivalent to
P Q = P.
12. Assume that P1, P2, . . . , Pn are projections and P1 + P2 + · · · + Pn = I. Show
that the projections are pairwise orthogonal.
13. Let A1, A2, · · · , Ak →Msa
n
and A1 + A2 + . . . + Ak = I. Show that the
following statements are equivalent:
(1) All operators Ai are projections.
(2) For all i ⊃= j the product Ai A j = 0 holds.
(3) rank (A1) + rank (A2) + · · · + rank (Ak) = n.
14. Let U|A| be the polar decomposition of A →Mn. Show that A is normal if and
only if U|A| = |A|U.
15. The matrix M →Mn(C) is deﬁned as
Mi j = min{i, j}.

2.8 Exercises
97
Show that M is positive.
16. Let A →Mn and deﬁne the mapping SA : Mn ∗Mn by SA : B ∗A ∼B.
Show that the following statements are equivalent.
(1) A is positive.
(2) SA : Mn ∗Mn is positive.
(3) SA : Mn ∗Mn is completely positive.
17. Let A, B, C be operators on a Hilbert space H and A, C ⊂0. Show that
⎡A B
B⊥C
⎢
⊂0
if and only if |≤Bx, y⟩| ≥≤Ay, y⟩· ≤Cx, x⟩for every x, y →H.
18. Let P →Mn be idempotent, i.e. P2 = P. Show that P is an ortho-projection if
and only if ∩P∩≥1.
19. Let P →Mn be an ortho-projection and 0 < A →Mn. Prove the following
formulas:
[P](A2) ≥([P]A)2,
([P]A)1/2 ≥[P](A1/2),
[P](A−1) ≥([P]A)†.
20. Show that the kernels
ψ(x, y) = cos(x −y),
cos(x2 −y2),
(1 + |x −y|)−1
are positive semideﬁnite on R × R.
21. Show that the equality
A ↑(B ⊗C) = (A ↑B) ⊗(A ↑C)
is not true for ortho-projections.
22. Assume that the kernel ψ : X × X ∗C is positive deﬁnite and ψ(x, x) > 0
for every x →X. Show that
¯ψ(x, y) =
ψ(x, y)
ψ(x, x)ψ(y, y)
is a positive deﬁnite kernel.
23. Assume that the kernel ψ : X × X ∗C is negative deﬁnite and ψ(x, x) ⊂0
for every x →X. Show that
log(1 + ψ(x, y))
is a negative deﬁnite kernel.
24. Show that the kernel ψ(x, y) = (sin(x −y))2 is negative semideﬁnite on R×R.
25. Show that the linear mapping Ep,n : Mn ∗Mn deﬁned as

98
2
Mappings and Algebras
Ep,n(A) = pA + (1 −p) I
n Tr A
is completely positive if and only if
−
1
n2 −1 ≥p ≥1 .
26. Show that the linear mapping E : Mn ∗Mn deﬁned as
E(D) =
1
n −1(Tr (D)I −Dt)
is a completely positive unital mapping. (Here Dt denotes the transpose of D.)
Show that E has a negative eigenvalue. (This mapping is called the Holevo–
Werner channel.)
27. Deﬁne E : Mn ∗Mn by
E(A) =
1
n −1(I Tr A −A).
Show that E is positive but not completely positive.
28. Let p be a real number. Show that the mapping Ep,2 : M2 ∗M2 deﬁned as
Ep,2(A) = pA + (1 −p) I
2Tr A
is positive if and only if −1 ≥p ≥1. Show that Ep,2 is completely positive if
and only if −1/3 ≥p ≥1.
29. Show that ∩( f1, f2)∩2 = ∩f1∩2 + ∩f2∩2.
30. Give the analogue of Theorem 2.1 when C is assumed to be invertible.
31. Let 0 ≥A ≥I. Find the matrices B and C such that
⎡A B
B⊥C
⎢
is a projection.
32. Let dim H = 2 and 0 ≥A, B →B(H). Show that there is an orthogonal basis
such that
A =
⎡a 0
0 b
⎢
,
B =
⎡c d
d e
⎢
with positive numbers a, b, c, d, e ⊂0.
33. Let
M =
⎡A B
B A
⎢

2.8 Exercises
99
and assume that A and B are self-adjoint. Show that M is positive if and only if
−A ≥B ≥A.
34. Determine the inverses of the matrices
A =
⎡a −b
b a
⎢
and
B =


a b c d
−b a −d c
−c d a b
−d c −b a

⎛⎛⎝.
35. Give the analogue of the factorization (2.2) when D is assumed to be invertible.
36. Show that the self-adjoint invertible matrix


A B C
B⊥D 0
C⊥0 E

⎝
has inverse in the form


Q−1
−P
−R
−P⊥D−1(I + B⊥P)
D−1B⊥R
−R⊥
R⊥BD−1
E−1(I + C⊥R)

⎝,
where
Q = A −BD−1B⊥−C E−1C⊥,
P = Q−1BD−1,
R = Q−1C E−1.
37. Find the determinant and the inverse of the block matrix
⎡A 0
a 1
⎢
.
38. Let A →Mn be an invertible matrix and d →C. Show that
det
⎡A b
c d
⎢
= (d −cA−1b)det A
where c = [c1, . . . , cn] and b = [b1, . . . , nn]t.
39. Prove the concavity of the variance functional ρ ∗Varρ(A) deﬁned in (2.10).
The concavity is
Varρ(A) ⊂

i
λiVarρi (A) if ρ =

i
λiρi
when λi ⊂0 and 
i λi = 1.
40. For x, y →R3 and

100
2
Mappings and Algebras
x · σ :=
3

i=1
xiσi,
y · σ :=
3

i=1
yiσi
show that
(x · σ)(y · σ) = ≤x, y⟩σ0 + i(x × y) · σ,
(2.18)
where x × y is the vectorial product in R3.

Chapter 3
Functional Calculus and Derivation
Let A ∈Mn(C) and p(x) := ⎡
i ci xi be a polynomial. It is quite obvious that by
p(A) we mean the matrix ⎡
i ci Ai. So the functional calculus is trivial for poly-
nomials. Slightly more generally, let f be a holomorphic function with the Taylor
expansion f (z) = ⎡→
k=0 ck(z −a)k. Then for every A ∈Mn(C) such that the oper-
ator norm ≤A −aI≤is less than the radius of convergence of f , one can deﬁne the
analytic functional calculus f (A) := ⎡→
k=0 ck(A −aI)k. This analytic functional
calculus can be generalized via the Cauchy integral:
f (A) :=
1
2πi
⎢
α
f (z)(zI −A)−1 dz
if f is holomorphic in a domain G containing the eigenvalues of A, where α is a
simple closed contour in G surrounding the eigenvalues of A. On the other hand,
when A ∈Mn(C) is self-adjoint and f is a general function deﬁned on an interval
containing the eigenvalues of A, the functional calculus f (A) is deﬁned via the
spectral decomposition of A or the diagonalization of A, that is,
f (A) =
k
⎣
i=1
f (αi)Pi = UDiag( f (λ1), . . . , f (λn))U ∗
for the spectral decomposition A = ⎡k
i=1 αi Pi and the diagonalization A =
UDiag(λ1, . . . , λn)U ∗. In this way, one has some types of functional calculus for
matrices (and also operators). When different types of functional calculus can be
deﬁned for one A ∈Mn(C), they yield the same result. The second half of this
chapter contains several formulas for derivatives
d
dt f (A + tT )
and Fréchet derivatives of functional calculus.
F. Hiai and D. Petz, Introduction to Matrix Analysis and Applications,
101
Universitext, DOI: 10.1007/978-3-319-04150-6_3,
© Hindustan Book Agency 2014

102
3
Functional Calculus and Derivation
3.1 The Exponential Function
The exponential function is well-deﬁned for all complex numbers. It has a convenient
Taylor expansion and it appears in some differential equations. It is also important
for matrices.
The Taylor expansion can be used to deﬁne eA for a matrix A ∈Mn(C):
eA :=
→
⎣
n=0
An
n! .
(3.1)
Here the right-hand side is an absolutely convergent series:
→
⎣
n=0
⎤⎤⎤⎤
An
n!
⎤⎤⎤⎤∗
→
⎣
n=0
≤A≤n
n!
= e≤A≤.
The ﬁrst example is in connection with the Jordan canonical form.
Example 3.1 We take
A =
⎥
⎦⎦
a 1 0 0
0 a 1 0
0 0 a 1
0 0 0 a

= aI + J.
Since I and J commute and J m = 0 for m > 3, we have
An = an I + nan−1J + n(n −1)
2
an−2J 2 + n(n −1)(n −2)
2 · 3
an−3J 3
and
→
⎣
n=0
An
n! =
→
⎣
n=0
an
n! I +
→
⎣
n=1
an−1
(n −1)! J + 1
2
→
⎣
n=2
an−2
(n −2)! J 2 + 1
6
→
⎣
n=3
an−3
(n −3)! J 3
= ea I + ea J + 1
2ea J 2 + 1
6ea J 3.
(3.2)
So we have
eA = ea
⎥
⎦⎦
1 1 1/2 1/6
0 1 1 1/2
0 0 1
1
0 0 0
1

.

3.1 The Exponential Function
103
Note that (3.2) shows that eA is a linear combination of I, A, A2, A3. (This is
contained in Theorem3.6, the coefﬁcients are speciﬁed by differential equations.) If
B = SAS−1, then eB = SeAS−1.
□
Example 3.2 It is a basic fact in analysis that
ea = lim
n→→

1 + a
n
n
for a complex number a, but we also have for matrices:
eA = lim
n→→
⎛
I + A
n
⎝n
.
(3.3)
This can be checked in a similar way to the previous example:
eaI+J = lim
n→→
⎛
I

1 + a
n

+ 1
n J
⎝n
.
From the point of view of numerical computation (3.1) is a better formula, but
(3.3) will be extended in the next theorem. (An extension of the exponential function
will appear later in (6.45).)
□
Theorem 3.3 Let
Tm,n(A) =
⎞m
⎣
k=0
1
k!
⎛A
n
⎝k⎠n
(m, n ∈N).
Then
lim
m→→Tm,n(A) = lim
n→→Tm,n(A) = eA.
Proof: The matrices B = e
A
n and
T =
m
⎣
k=0
1
k!
⎛A
n
⎝k
commute. Hence
eA −Tm,n(A) = Bn −T n = (B −T )(Bn−1 + Bn−2T + · · · + T n−1).
We can estimate:
≤eA −Tm,n(A)≤∗≤B −T ≤n × max{≤B≤i≤T ≤n−i−1 : 0 ∗i ∗n −1}.

104
3
Functional Calculus and Derivation
Since ≤T ≤∗e
≤A≤
n
and ≤B≤∗e
≤A≤
n , we have
≤eA −Tm,n(A)≤∗n≤e
A
n −T ≤e
n−1
n ≤A≤.
By bounding the tail of the Taylor series,
≤eA −Tm,n(A)≤∗
n
(m + 1)!
⎛≤A≤
n
⎝m+1
e
≤A≤
n e
n−1
n ≤A≤
converges to 0 in the two cases m →→and n →→.
□
Theorem 3.4 If AB = BA, then
et(A+B) = etAetB
(t ∈R).
(3.4)
Conversely, if this equality holds, then AB = BA.
Proof: First we assume that AB = BA and compute the product eAeB by multi-
plying term by term the series:
eAeB =
→
⎣
m,n=0
1
m!n! Am Bn.
Therefore,
eAeB =
→
⎣
k=0
1
k!Ck,
where
Ck :=
⎣
m+n=k
k!
m!n! Am Bn.
By the commutation relation the binomial formula holds and Ck = (A + B)k. We
conclude
eAeB =
→
⎣
k=0
1
k!(A + B)k
which is the statement.
Another proof can be obtained by differentiation. It follows from the expansion
(3.1) that the derivative of the matrix-valued function t →etA deﬁned on R is etAA:

3.1 The Exponential Function
105
d
dt etA = etAA = AetA.
(3.5)
Therefore, when AC = C A,
d
dt etAeC−t A = etAAeC−t A −etAAeC−t A = 0.
It follows that the function t →etAeC−t A is constant. In particular,
eAeC−A = eC.
Put A + B in place of C to obtain the statement (3.4).
The ﬁrst derivative of (3.4) is
et(A+B)(A + B) = etAAetB + etAetBB
and the second derivative is
et(A+B)(A + B)2 = etAA2etB + etAAetBB + etAAetBB + etAetBB2.
For t = 0 this is B A = AB.
□
Example 3.5 The matrix exponential function can be used to formulate the solution
of a linear ﬁrst-order differential equation. Let
x(t) =
⎥
⎦⎦⎦
x1(t)
x2(t)
...
xn(t)


and
x0 =
⎥
⎦⎦⎦
x1
x2
...
xn

.
The solution of the differential equation
x≥(t) = Ax(t),
x(0) = x0
is x(t) = etAx0, by formula (3.5).
□
Theorem 3.6 Let A ∈Mn with characteristic polynomial
p(λ) = det(λI −A) = λn + cn−1λn−1 + · · · + c1λ + c0.
Then
etA = x0(t)I + x1(t)A + · · · + xn−1(t)An−1,

106
3
Functional Calculus and Derivation
where the vector
x(t) = (x0(t), x1(t), . . . , xn−1(t))
satisﬁes the nth order differential equation
x(n)(t) + cn−1x(n−1)(t) + · · · + c1x≥(t) + c0x(t) = 0
with the initial condition
x(k)(0) = (
1
σ0 , . . . , 0,
k
σ1 , 0, . . . , 0)
for 0 ∗k ∗n −1.
Proof: We can check that the matrix-valued functions
F1(t) = x0(t)I + x1(t)A + · · · + xn−1(t)An−1
and F2(t) = etA satisfy the conditions
F(n)(t) + cn−1F(n−1)(t) + · · · + c1F≥(t) + c0F(t) = 0
and
F(0) = I, F≥(0) = A, . . . , F(n−1)(0) = An−1.
Therefore F1 = F2.
□
Example 3.7 In the case of 2 × 2 matrices, the use of the Pauli matrices
σ1 =
⎞0 1
1 0
⎠
,
σ2 =
⎞0 −i
i 0
⎠
,
σ3 =
⎞1 0
0 −1
⎠
is efﬁcient, together with I they form an orthogonal system with respect to the
Hilbert–Schmidt inner product.
Let A ∈Msa
2 be such that
A = c1σ1 + c2σ2 + c3σ3,
c2
1 + c2
2 + c2
3 = 1
in the representation with Pauli matrices. It is easy to check that A2 = I. Therefore,
for even powers A2n = I, but for odd powers A2n+1 = A. Choose c ∈R and
combine these two facts with the knowledge of the relation of the exponential to sine
and cosine:

3.1 The Exponential Function
107
eicA =
→
⎣
n=0
incn An
n!
=
→
⎣
n=0
(−1)nc2n A2n
(2n)!
+ i
→
⎣
n=0
(−1)nc2n+1A2n+1
(2n + 1)!
= (cos c)I + i(sin c)A .
A general matrix has the form C = c0I + cA and
eiC = eic0(cos c)I + ieic0(sin c)A.
(eC is similar, see Exercise 13.)
□
The next theorem gives the so-called Lie–Trotter formula. (A generalization is
Theorem 5.17.)
Theorem 3.8 Let A, B ∈Mn(C). Then
eA+B = lim
m→→

eA/meB/mn
.
Proof: First we observe that the identity
Xn −Y n =
n−1
⎣
j=0
Xn−1−j(X −Y)Y j
implies the norm estimate
≤Xn −Y n≤∗ntn−1≤X −Y≤
for the submultiplicative operator norm when the constant t is chosen such that
≤X≤, ≤Y≤∗t.
Now we choose Xn := exp((A + B)/n) and Yn := exp(A/n) exp(B/n). From
the above estimate we have
≤Xn
n −Y n
n ≤∗nu≤Xn −Yn≤,
(3.6)
if we can ﬁnd a constant u such that ≤Xn≤n−1, ≤Yn≤n−1 ∗u. Since
≤Xn≤n−1 ∗

exp((≤A≤+ ≤B≤)/n)
n−1 ∗exp(≤A≤+ ≤B≤)
and
≤Yn≤n−1 ∗

exp(≤A≤/n)
n−1
exp(≤B≤/n)
n−1 ∗exp ≤A≤· exp ≤B≤,
u = exp(≤A≤+ ≤B≤) can be chosen to have the estimate (3.6).

108
3
Functional Calculus and Derivation
The theorem follows from (3.6) if we can show that n≤Xn −Yn≤→0. The power
series expansion of the exponential function yields
Xn = I + A + B
n
+ 1
2
⎛A + B
n
⎝2
+ · · ·
and
Yn =

I + A
n + 1
2
⎛A
n
⎝2
+ · · ·
 
I + B
n + 1
2
⎛B
n
⎝2
+ · · ·

.
If Xn −Yn is computed by multiplying the two series in Yn, one can observe that all
constant terms and all terms containing 1/n cancel. Therefore
≤Xn −Yn≤∗c
n2
for some positive constant c.
□
If A and B are self-adjoint matrices, then it can be better to reach eA+B as the
limit of self-adjoint matrices.
Corollary 3.9
eA+B = lim
n→→

e
A
2n e
B
n e
A
2n
n
.
Proof: We have

e
A
2n e
B
n e
A
2n
n
= e−A
2n

eA/neB/nn
e
A
2n
and the limit n →→gives the result.
□
The Lie–Trotter formula can be extended to more matrices:
⎤⎤eA1+A2+···+Ak −(eA1/neA2/n · · · eAk/n)n⎤⎤
∗2
n

k
⎣
j=1
≤A j≤

exp
n + 2
n
k
⎣
j=1
≤A j≤

.
(3.7)
Theorem 3.10 For matrices A, B ∈Mn the Taylor expansion of the function R ∪
t →eA+tB is
→
⎣
k=0
tk Ak(1)

3.1 The Exponential Function
109
where A0(s) = es A and
Ak(s) =
⎢s
0
dt1
⎢t1
0
dt2 · · ·
⎢tk−1
0
dtke(s−t1)ABe(t1−t2)AB · · · Betk A
for s ∈R.
Proof: To make differentiation easier we write
Ak(s) =
⎢s
0
e(s−t1)AB Ak−1(t1) dt1 = es A
⎢s
0
e−t1 AB Ak−1(t1) dt1
for k ⊥1. It follows that
d
ds Ak(s) = Aes A
⎢s
0
e−t1 AB Ak−1(t1) dt1 + es A d
ds
⎢s
0
e−t1 AB Ak−1(t1) dt1
= AAk(s) + B Ak−1(s).
Therefore
F(s) :=
→
⎣
k=0
Ak(s)
satisﬁes the differential equation
F≥(s) = (A + B)F(s),
F(0) = I.
Therefore F(s) = es(A+B). If s = 1 and we write tB in place of B, then we get the
expansion of eA+tB.
□
Corollary 3.11
∂
∂t eA+tB
t=0 =
⎢1
0
eu ABe(1−u)A du.
Another important formula for the exponential function is the Baker–Campbell–
Hausdorff formula:
etAetB = exp

t(A + B) + t2
2 [A, B] + t3
12([A, [A, B]] −[B, [A, B]]) + O(t4)

in which the commutator [A, B] := AB −B A appears.
A function f : R+ = [0, →) →R is completely monotone if the nth derivative
of f has the sign (−1)n on the whole of R+ and for every n ∈N.
The next theorem is related to a conjecture.

110
3
Functional Calculus and Derivation
Theorem 3.12 Let A, B ∈Msa
n and let t ∈R. The following statements are equiv-
alent:
(i) The polynomial t →Tr (A + tB)p has only positive coefﬁcients for every
A, B ⊥0 and all p ∈N.
(ii) For every self-adjoint A and B ⊥0, the function t →Tr exp (A −tB) is
completely monotone on [0, →).
(iii) For every A > 0, B ⊥0 and all p ⊥0, the function t →Tr (A + tB)−p is
completely monotone on [0, →).
Proof: (i)⊂(ii): We have
Tr exp (A −tB) = e−≤A≤
→
⎣
k=0
1
k!Tr (A + ≤A≤I −tB)k
and it follows from Bernstein’s theorem and (i) that the right-hand side is the Laplace
transform of a positive measure supported in [0, →).
(ii)⊂(iii): By the matrix equation
(A + tB)−p =
1
α(p)
⎢→
0
exp [−u(A + tB)] u p−1du,
we can see the signs of the derivatives.
(iii)⊂(i): It sufﬁces to assume (iii) only for p ∈N. For invertible A, by Lemma
3.31 below we observe that the rth derivative of Tr (A0 + t B0)−p at t = 0 is related
to the coefﬁcient of tr in Tr (A + tB)p as given by (3.17), where A, A0, B, B0 are
related as in the lemma. The left-hand side of (3.17) has the sign (−1)r because it
is the derivative of a completely monotone function. Thus the right-hand side has
the correct sign as stated in item (i). The case of non-invertible A follows from a
continuity argument.
□
The Laplace transform of a measure μ on R+ is
f (t) =
⎢→
0
e−tx dμ(x)
(t ∈R+).
According to the Bernstein theorem such a measure μ exists if and only if f is a
completely monotone function.
Bessis, Moussa and Villani conjectured in 1975 that the function t →Tr exp(A−
tB) is a completely monotone function if A is self-adjoint and B is positive. Theorem
3.12 due to Lieb and Seiringer gives an equivalent condition. Property (i) has a very
simple formulation.

3.2 Other Functions
111
3.2 Other Functions
All reasonable functions can be approximated by polynomials and, for a polynomial
p(X), it is elementary to compute p(X) for a matrix X ∈Mn. The canonical Jordan
decomposition
X = S
⎥
⎦⎦⎦
Jk1(λ1)
0
· · ·
0
0
Jk2(λ2) · · ·
0
...
...
...
...
0
0
· · · Jkm(λm)

S−1 = SJ S−1
gives that
p(X) = S
⎥
⎦⎦⎦
p(Jk1(λ1))
0
· · ·
0
0
p(Jk2(λ2)) · · ·
0
...
...
...
...
0
0
· · · p(Jkm(λm))

S−1 = Sp(J)S−1.
The crucial point is the computation of (Jk(λ))m. Since Jk(λ) = λIn + Jk(0) =
λIn + Jk is the sum of commuting matrices, we can compute the mth power by using
the binomial formula:
(Jk(λ))m = λm In +
m
⎣
j=1
⎛m
j
⎝
λm−j J j
k .
The powers of Jk are known, see Example 1.15. Let m > 3, then the example
J4(λ)m =
⎥
⎦⎦⎦⎦⎦⎦⎦⎦⎦⎦⎦
λm mλm−1 m(m −1)λm−2
2!
m(m −1)(m −2)λm−3
3!
0
λm
mλm−1
m(m −1)λm−2
2!
0
0
λm
mλm−1
0
0
0
λm


demonstrates the point. In another formulation,

112
3
Functional Calculus and Derivation
p(J4(λ)) =
⎥
⎦⎦⎦⎦⎦⎦⎦⎦⎦⎦⎦
p(λ) p≥(λ) p≥≥(λ)
2!
p(3)(λ)
3!
0
p(λ) p≥(λ)
p≥≥(λ)
2!
0
0
p(λ)
p≥(λ)
0
0
0
p(λ)


,
which is actually correct for all polynomials and for every smooth function. We
conclude that if the canonical Jordan form is known for X ∈Mn, then f (X) is
computable. In particular, the above argument gives the following result.
Theorem 3.13 For X ∈Mn the relation
det eX = exp(Tr X)
holds between trace and determinant.
A matrix A ∈Mn is diagonalizable if
A = S Diag(λ1, λ2, . . . , λn)S−1
for some invertible matrix S. Observe that this condition means that in the Jordan
canonical form all Jordan blocks are 1 × 1 and the numbers λ1, λ2, . . . , λn are the
eigenvalues of A. In this case,
f (A) = S Diag( f (λ1), f (λ2), . . . , f (λn))S−1
(3.8)
when the complex-valued function f is deﬁned on the set of eigenvalues of A.
If the numbers λ1, λ2, . . . , λn are different, then we can have a polynomial p(x)
of order n −1 such that p(λi) = f (λi):
p(x) =
n
⎣
j=1

i√= j
x −λi
λ j −λi
f (λ j) .
(This is the so-called Lagrange interpolation formula.) Therefore we have
f (A) = p(A) =
n
⎣
j=1

i√= j
A −λi I
λ j −λi
f (λ j).
(The relevant formulations are in Exercises 14 and 15.)

3.2 Other Functions
113
Example 3.14 Consider the self-adjoint matrix
X =
⎞1 + z x −yi
x + yi 1 −z
⎠
=
⎞1 + z
w
w
1 −z
⎠
where x, y, z ∈R. From the characteristic polynomial we have the eigenvalues
λ1 = 1 + R
and
λ2 = 1 −R,
where R =

x2 + y2 + z2. If R < 1, then X is positive and invertible. The eigen-
vectors are
u1 =
⎞R + z
w
⎠
and
u2 =
⎞R −z
−w
⎠
.
Set
 =
⎞1 + R
0
0
1 −R
⎠
,
S =
⎞R + z R −z
w
−w
⎠
.
We can check that X S = S, hence
X = SS−1.
To compute S−1 we use the formula
⎞a b
c d
⎠−1
=
1
ad −bc
⎞d −b
−c a
⎠
.
Hence
S−1 =
1
2wR
⎞w
R −z
w −R −z
⎠
.
It follows that
Xt = at
⎞bt + z
w
w
bt −z
⎠
,
where
at = (1 + R)t −(1 −R)t
2R
,
bt = R (1 + R)t + (1 −R)t
(1 + R)t −(1 −R)t .
The matrix X/2 is a density matrix and has applications in quantum theory.
□

114
3
Functional Calculus and Derivation
In the previous example the function f (x) = xt was used. If the eigenvalues of A
are positive, then f (A) is well-deﬁned. The canonical Jordan decomposition is not
the only one we might use. It is known in analysis that
x p = sin pπ
π
⎢→
0
xλp−1
λ + x dλ
(x ∈(0, →))
when 0 < p < 1. It follows that for a positive matrix A we have
Ap = sin pπ
π
⎢→
0
λp−1 A(λI + A)−1 dλ.
For self-adjoint matrices A a simple formula for Ap is available, nevertheless the
previous integral formula is still useful in some situations, for example in the context
of differentiation.
Recall that self-adjoint matrices are diagonalizable and they have a spectral de-
composition. Let A = ⎡
i λi Pi be the spectral decomposition of a self-adjoint
A ∈Mn(C). (λi are the different eigenvalues and Pi are the corresponding eigen-
projections; the rank of Pi is the multiplicity of λi.) Then
f (A) =
⎣
i
f (λi)Pi .
(3.9)
Usually we assume that
f
is continuous on an interval containing the
eigenvalues of A.
Example 3.15 Consider
f+(t) := max{t, 0} and
f−(t) := max{−t, 0} for t ∈R.
For each A ∈B(H)sa deﬁne
A+ := f+(A)
and
A−:= f−(A).
Since f+(t), f−(t) ⊥0, f+(t) −f−(t) = t and f+(t) f−(t) = 0, we have
A+, A−⊥0,
A = A+ −A−,
A+A−= 0.
These A+ and A−are called the positive part and the negative part of A, respec-
tively, and A = A+ + A−is called the Jordan decomposition of A.
□
Let f be holomorphic inside and on a positively oriented simple contour α in the
complex plane and let A be an n × n matrix such that its eigenvalues are inside α.
Then

3.2 Other Functions
115
f (A) :=
1
2πi
⎢
α
f (z)(zI −A)−1 dz
(3.10)
is deﬁned by a contour integral. When A is self-adjoint, then (3.9) makes sense and
it is an exercise to show that it gives the same result as (3.10).
Example 3.16 We can deﬁne the square root function on the set
G := {reiϕ ∈C : r > 0, −π/2 < ϕ < π/2}
as
≡
reiϕ := ≡reiϕ/2 and this is a holomorphic function on G.
When X = S Diag(λ1, λ2, . . . , λn) S−1 ∈Mn is a weakly positive matrix, then
λ1, λ2, . . . , λn > 0 and to use (3.10) we can take a positively oriented simple contour
α in G such that the eigenvalues are inside α. Then
≡
X =
1
2πi
⎢
α
≡z(zI −X)−1 dz
= S
⎛1
2πi
⎢
α
≡z Diag(1/(z −λ1), 1/(z −λ2), . . . , 1/(z −λn)) dz
⎝
S−1
= S Diag(

λ1,

λ2, . . . ,

λn) S−1.
□
Example 3.17 The logarithm is a well-deﬁned differentiable function on positive
numbers. Therefore for a strictly positive operator A formula (3.9) gives log A. Since
log x =
⎢→
0
1
1 + t −
1
x + t dt,
we can use
log A =
⎢→
0
1
1 + t I −(A + t I)−1 dt.
(3.11)
If we have a matrix A with eigenvalues outside of R−= (−→, 0], then we can
take the domain
D = {reiϕ ∈C : r > 0, −π < ϕ < π}
with the function reiϕ →logr + iϕ. The integral formula (3.10) can be used for the
calculus. Another useful formula is

116
3
Functional Calculus and Derivation
log A =
⎢1
0
(A −I) (t(A −I) + I)−1 dt
(3.12)
(when A does not have eigenvalue in R−).
Note that log(ab) = log a + log b is not true for any complex numbers, so it
cannot be expected to hold for (commuting) matrices.
□
Theorem 3.18 If fk and gk are functions (α, β) →R such that for some ck ∈R
⎣
k
ck fk(x)gk(y) ⊥0
for every x, y ∈(α, β), then
⎣
k
ckTr fk(A)gk(B) ⊥0
whenever A, B are self-adjoint matrices with spectra in (α, β).
Proof: Let A = ⎡
i λi Pi and B = ⎡
j μ j Q j be the spectral decompositions.
Then
⎣
k
ckTr fk(A)gk(B) =
⎣
k
⎣
i, j
ckTr Pi fk(λi)gk(μ j)Q j
=
⎣
i, j
Tr Pi Q j
⎣
k
ck fk(λi)gk(μ j) ⊥0
by the hypothesis.
□
In the theorem assume that ⎡
k ck fk(x)gk(y) = 0 if and only if x = y. Then we
show that ⎡
k ckTr fk(A)gk(B) = 0 if and only if A = B. From the above proof
it follows that ⎡
k ckTr fk(A)gk(B) = 0 holds if and only if Tr Pi Q j > 0 implies
λi = μ j. This property yields
Q j AQ j =
⎣
i
λi Q j Pi Q j = μ j Q j,
and similarly Q j A2Q j = μ2
j Q j. Hence
(AQ j −μ j Q j)∗(AQ j −μ j Q j) = Q j A2Q j −2μ j Q j AQ j + μ2
j Q j = 0
so that AQ j = μ j Q j = BQ j for all j, which implies A = B. The converse is
obvious.
Example 3.19 In order to exhibit an application of the previous theorem, let us
assume that f is convex. Then

3.2 Other Functions
117
f (x) −f (y) −(x −y) f ≥(y) ⊥0
and
Tr f (A) ⊥Tr f (B) + Tr (A −B) f ≥(B) .
Replacing f by −η(t) = t log t we have
Tr A log A ⊥Tr B log B + Tr (A −B) + Tr (A −B) log B
or equivalently
Tr A(log A −log B) −Tr (A −B) ⊥0.
The left-hand side is the quantum relative entropy S(A≤B) of the positive deﬁnite
matrices A and B. (In fact, S(A≤B) is well-deﬁned for A, B ⊥0 if ker A ⊃ker B;
otherwise, it is deﬁned to be +→). Moreover, since −η(t) is strictly convex, we see
that S(A≤B) = 0 if and only if A = B.
If Tr A = Tr B, then S(A≤B) is the so-called Umegaki relative entropy:
S(A≤B) = Tr A(log A −log B). For this we can obtain a better estimate. If
Tr A = Tr B = 1, then all eigenvalues are in [0, 1]. Analysis tells us that for some
ξ ∈(x, y)
−η(x) + η(y) + (x −y)η≥(y) = −1
2(x −y)2η≥≥(ξ) ⊥1
2(x −y)2
when x, y ∈[0, 1]. According to Theorem 3.18 we have
Tr A(log A −log B) ⊥1
2 Tr (A −B)2.
(3.13)
The Streater inequality (3.13) has the consequence that A = B if the relative
entropy is 0. Indeed, a stronger inequality called Pinsker’s inequality is known: If
Tr A = Tr B, then
Tr A(log A −log B) ⊥1
2 ≤A −B≤2
1,
where ≤A −B≤1 := Tr |A −B| is the trace-norm of A −B, see Sect.6.3.
□
3.3 Derivation
This section introduces derivatives of scalar-valued and matrix-valued functions.
From the latter, a scalar-valued function can be obtained, for example, by taking the
trace.

118
3
Functional Calculus and Derivation
Example 3.20 Assume that A ∈Mn is invertible. Then A + tT is invertible as well
for T ∈Mn and for a small real number t. The identity
(A + tT )−1 −A−1 = (A + tT )−1(A −(A + tT ))A−1 = −t(A + tT )−1T A−1
gives
lim
t→0
1
t
⎛
(A + tT )−1 −A−1
⎝
= −A−1T A−1.
The derivative was computed at t = 0, but if A + tT is invertible, then
d
dt (A + tT )−1 = −(A + tT )−1T (A + tT )−1
by a similar computation. We can continue the derivation:
d2
dt2 (A + tT )−1 = 2(A + tT )−1T (A + tT )−1T (A + tT )−1 ,
d3
dt3 (A + tT )−1 = −6(A + tT )−1T (A + tT )−1T (A + tT )−1T (A + tT )−1 .
So the Taylor expansion is
(A + tT )−1 = A−1 −t A−1T A−1 + t2 A−1T A−1T A−1
−t3A−1T A−1T A−1T A−1 + · · ·
=
→
⎣
n=0
(−t)n A−1/2(A−1/2T A−1/2)n A−1/2.
Since
(A + tT )−1 = A−1/2(I + t A−1/2T A−1/2)−1A−1/2,
wecanalsoobtainTaylorexpansionfromtheNeumannseriesof (I+t A−1/2T A−1/2)−1,
see Example 1.8.
□
Example 3.21 There is an interesting formula which relates the functional calculus
and derivation:
f
⎛⎞A B
0 A
⎠⎝
=
⎞
f (A) d
dt f (A + tB)
0
f (A)
⎠
.
If f is a polynomial, then it is easy to check this formula.
□

3.3 Derivation
119
Example 3.22 Assume that A ∈Mn is positive invertible. Then A + tT is also
positive invertible for T ∈Msa
n and for a small real number t. Therefore log(A+tT )
is deﬁned and it is expressed as
log(A + tT ) =
⎢→
0
(x + 1)−1I −(x I + A + tT )−1 dx.
This is a convenient formula for the derivation (with respect to t ∈R):
d
dt log(A + tT ) =
⎢→
0
(x I + A)−1T (x I + A)−1 dx
from the derivative of the inverse. The derivation can be continued, yielding the
Taylor expansion
log(A + tT ) = log A + t
⎢→
0
(x + A)−1T (x + A)−1 dx
−t2
⎢→
0
(x + A)−1T (x + A)−1T (x + A)−1 dx + · · ·
= log A −
→
⎣
n=1
(−t)n
⎢→
0
(x + A)−1/2
× ((x + A)−1/2T (x + A)−1/2)n(x + A)−1/2 dx.
□
Theorem 3.23 Let A, B ∈Mn(C) be self-adjoint matrices and t ∈R. Assume that
f : (α, β) →R is a continuously differentiable function deﬁned on an interval and
assume that the eigenvalues of A + tB are in (α, β) for small t −t0. Then
d
dt Tr f (A + tB)

t=t0
= Tr (B f ≥(A + t0B)) .
Proof: One can verify the formula for a polynomial f by an easy direct compu-
tation: Tr (A + tB)n is a polynomial in the real variable t. We are interested in the
coefﬁcient of t which is
Tr (An−1B + An−2B A + · · · + AB An−2 + B An−1) = nTr An−1B.
We have the result for polynomials and the formula can be extended to a more general
f by means of polynomial approximation.
□
Example 3.24 Let f : (α, β) →R be a continuous increasing function and assume
that the spectrum of the self-adjoint matrices A and C lie in (α, β). We use the
previous theorem to show that

120
3
Functional Calculus and Derivation
A ∗C
implies Tr f (A) ∗Tr f (C).
(3.14)
We may assume that f is smooth and it is enough to show that the derivative of
Tr f (A + tB) is positive when B ⊥0. (To conclude (3.14), one takes B = C −A.)
The derivative is Tr (B f ≥(A +tB)) and this is the trace of the product of two positive
operators. Therefore, it is positive.
Another (simpler) way to show this is to use Theorem 2.16. For the eigenvalues of
A, C we have λk(A) ∗λk(C) (1 ∗k ∗n) and hence Tr f (A) = ⎡
k f (λk(A)) ∗
⎡
k f (λk(C)) = Tr f (C).
□
For a holomorphic function f , we can compute the derivative of f (A + tB) by
using (3.10), where α is a positively oriented simple contour satisfying the properties
required above. The derivation is reduced to the differentiation of the resolvent (zI −
(A + tB))−1 and we obtain
X := d
dt f (A + tB)

t=0 =
1
2πi
⎢
α
f (z)(zI −A)−1B(zI −A)−1 dz .
(3.15)
When A is self-adjoint, then there is no loss of generality in assuming that it is
diagonal, A = Diag(t1, t2, . . . , tn), and we compute the entries of the matrix (3.15)
using the Frobenius formula
1
2πi
⎢
α
f (z)
(z −ti)(z −t j) dz = f (ti) −f (t j)
ti −t j
(this means f ≥(ti) if ti = t j). Therefore,
Xi j =
1
2πi
⎢
α
f (z)
1
z −ti
Bi j
1
z −t j
dz = f (ti) −f (t j)
ti −t j
Bi j.
A C1-function, together with its derivative, can be approximated by polynomials.
Hence we have the following result.
Theorem 3.25 Assume that f : (α, β) →R is a C1-function and A = Diag(t1,
t2, . . . , tn) with α < ti < β (1 ∗i ∗n). If B = B∗, then the derivative t →
f (A + tB) is a Schur product:
d
dt f (A + tB)

t=0 = D ∞B,
(3.16)
where D is the divided difference matrix:
Di j =



f (ti) −f (t j)
ti −t j
if ti −t j √= 0,
f ≥(ti)
if ti −t j = 0.

3.3 Derivation
121
Let f : (α, β) →R be a continuous function. It is called matrix monotone if
A ∗C
implies
f (A) ∗f (C)
when the spectra of the self-adjoint matrices B and C lie in (α, β).
Theorem 2.15 tells us that f (x) = −1/x is a matrix monotone function. Matrix
monotonicity means that f (A + tB) is an increasing function when B ⊥0. The in-
creasing property is equivalent to the positivity of the derivative. We use the previous
theorem to show that the function f (x) = ≡x is matrix monotone.
Example 3.26 Assume that A > 0 is diagonal: A = Diag(t1, t2, . . . , tn). Then the
derivative of the function ≡A + t B is D ∞B, where
Di j =



1
≡ti + ≡t j
if ti −t j √= 0,
1
2≡ti
if ti −t j = 0.
This is a Cauchy matrix (see Example 1.41) and it is positive. If B is positive,
then so is the Schur product. We have shown that the derivative is positive, hence
f (x) = ≡x is matrix monotone.
An idea for another proof appears in Exercise 28.
□
A subset K ∩Mn is convex if for any A, B ∈K and for any real number
0 < λ < 1
λA + (1 −λ)B ∈K.
The functional F : K →R is convex if for A, B ∈K and for any real number
0 < λ < 1 the inequality
F(λA + (1 −λ)B) ∗λF(A) + (1 −λ)F(B)
holds. This inequality is equivalent to the convexity of the function
G : [0, 1] →R,
G(λ) := F(B + λ(A −B)).
It is well-known in analysis that convexity is related to the second derivative.
Theorem 3.27 Let K be the set of self-adjoint n × n matrices with spectrum in the
interval (α, β). Assume that the function f : (α, β) →R is a convex C2-function.
Then the functional A →Tr f (A) is convex on K.
Proof: The stated convexity is equivalent to the convexity of the numerical func-
tions

122
3
Functional Calculus and Derivation
t →Tr f (t X1 + (1 −t)X2) = Tr (X2 + t(X1 −X2))
(t ∈[0, 1]).
It is enough to prove that the second derivative of t →Tr f (A + tB) is positive at
t = 0.
The ﬁrst derivative of the functional t →Tr f (A + tB) is Tr f ≥(A + tB)B. To
compute the second derivative we differentiate f ≥(A + tB). We can assume that A is
diagonal and we differentiate at t = 0. Using (3.16) we get
 d
dt f ≥(A + tB)

t=0

i j = f ≥(ti) −f ≥(t j)
ti −t j
Bi j .
Therefore,
d2
dt2 Tr f (A + tB)

t=0 = Tr
 d
dt f ≥(A + tB)

t=0

B
=
⎣
i,k
 d
dt f ≥(A + tB)

t=0

ik Bki
=
⎣
i,k
f ≥(ti) −f ≥(tk)
ti −tk
Bik Bki
=
⎣
i,k
f ≥≥(sik)|Bik|2,
where sik is between ti and tk. The convexity of f means f ≥≥(sik) ⊥0, hence we
conclude the positivity.
□
In the above theorem one can remove the C2 assumption for f by using the so-
called regularization (or smoothing) technique; for this technique, see [20] or [36].
Note that another less analytic proof is sketched in Exercise 22.
Example 3.28 The function
η(x) =
−x log x
if
x > 0,
0
if
x = 0
is continuous and concave on R+. For a positive matrix D ⊥0
S(D) := Tr η(D)
is called the von Neumann entropy. It follows from the previous theorem that S(D)
is a concave function of D. If we are being very rigorous, then we cannot apply the

3.3 Derivation
123
theorem, since η is not differentiable at 0. Therefore we should apply the theorem to
f (x) := η(x + ε), where ε > 0 and take the limit ε →0.
□
Example 3.29 Let a self-adjoint matrix H be ﬁxed. The state of a quantum system
is described by a density matrix D which has the properties D ⊥0 and Tr D = 1.
The equilibrium state minimizes the energy
F(D) = Tr DH −1
β S(D),
where β is a positive number. To ﬁnd the minimizer, we solve the equation
∂
∂t F(D + t X)

t=0 = 0
for self-adjoint matrices X satisfying Tr X = 0. The equation is
Tr X
⎛
H + 1
β log D + 1
β I
⎝
= 0
and
H + 1
β log D + 1
β I
must be cI. Hence the minimizer is
D =
e−βH
Tr e−βH ,
which is called the Gibbs state.
□
Example 3.30 Next we restrict ourselves to the self-adjoint case A, B ∈Mn(C)sa
in the analysis of (3.15).
The space Mn(C)sa can be decomposed as MA ◦M∼
A, where MA := {C ∈
Mn(C)sa : C A = AC} is the commutant of A and M∼
A is its orthogonal complement.
If we consider the operator LA : X →i[A, X] := i(AX−X A), then MA is precisely
the kernel of LA, while M∼
A is its range.
If B ∈MA, then
1
2πi
⎢
α
f (z)(zI −A)−1B(zI −A)−1 dz = B
2πi
⎢
α
f (z)(zI −A)−2 dz = B f ≥(A)
and we have
d
dt f (A + tB)

t=0 = B f ≥(A) .

124
3
Functional Calculus and Derivation
If B = i[A, X] ∈M∼
A, then we use the identity
(zI −A)−1[A, X](zI −A)−1 = [(zI −A)−1, X]
and we conclude
d
dt f (A + ti[A, X])

t=0 = i[ f (A), X] .
To compute the derivative in an arbitrary direction B we should decompose B as
B1 ◦B2 with B1 ∈MA and B2 ∈M∼
A. Then
d
dt f (A + tB)

t=0 = B1 f ≥(A) + i[ f (A), X] ,
where X is the solution to the equation B2 = i[A, X].
□
The next lemma was used in the proof of Theorem 3.12.
Lemma 3.31 Let A0, B0 ∈Msa
n and assume A0 > 0. Deﬁne A = A−1
0
and B =
A−1/2
0
B0 A−1/2
0
, and let t ∈R. For all p,r ∈N
dr
dtr Tr (A0 + tB0)−p

t=0
=
p
p + r (−1)r dr
dtr Tr (A + tB)p+r

t=0
.
(3.17)
Proof: By induction it is easy to show that
dr
dtr (A + tB)p+r = r!
⎣
0∗i1,...,ir+1∗p
⎡
j i j =p
(A + tB)i1 B(A + tB)i2 · · · B(A + tB)ir+1 .
By taking the trace at t = 0 we obtain
κ1 := dr
dtr Tr (A + tB)p+r

t=0
= r!
⎣
0∗i1,...,ir+1∗p
⎡
j i j =p
Tr Ai1 B Ai2 · · · B Air+1 .
Moreover, by similar arguments,
dr
dtr (A0 + tB0)−p
= (−1)rr!
⎣
1∗i1,...,ir+1∗p
⎡
j i j =p+r
(A0 + t B0)−i1 B0(A0 + tB0)−i2 · · · B0(A0 + tB0)−ir+1 .
By taking the trace at t = 0 and using cyclicity, we get

3.3 Derivation
125
κ2 := dr
dtr Tr (A0 + t B0)−p

t=0
= (−1)rr!
⎣
0∗i1,...,ir+1∗p−1
⎡
j i j =p−1
Tr A Ai1 B Ai2 · · · B Air+1 .
We have to show that
κ2 =
p
p + r (−1)rκ1 .
To see this we rewrite κ1 in the following way. Deﬁne p + r matrices M j by
M j =
 B for 1 ∗j ∗r
A for r + 1 ∗j ∗r + p .
Let Sn denote the permutation group on {1, . . . , n}. Then
κ1 = 1
p!
⎣
π∈Sp+r
Tr
p+r

j=1
Mπ( j) .
Because of the cyclicity of the trace we can always arrange the product so that Mp+r
has the ﬁrst position in the trace. Since there are p + r possible locations for Mp+r
to appear in the product above, and all products are equally weighted, we get
κ1 = p + r
p!
⎣
π∈Sp+r−1
Tr A
p+r−1

j=1
Mπ( j) .
On the other hand,
κ2 = (−1)r
1
(p −1)!
⎣
π∈Sp+r−1
Tr A
p+r−1

j=1
Mπ( j) ,
so we arrive at the desired equality.
□
3.4 Fréchet Derivatives
Let f be a real-valued function on (a, b) ∩R, and denote by Msa
n (a, b) the set of all
matrices A ∈Msa
n with σ(A) ∩(a, b). In this section we discuss the differentiability
properties of the matrix functional calculus A →f (A) when A ∈Msa
n (a, b).
The case n = 1 corresponds to differentiation in classical analysis. There the
divided differences are important and will also appear here. Let x1, x2, . . . be distinct
points in (a, b). Then we deﬁne

126
3
Functional Calculus and Derivation
f [0][x1] := f (x1),
f [1][x1, x2] := f (x1) −f (x2)
x1 −x2
and recursively for n = 2, 3, . . .,
f [n][x1, x2, . . . , xn+1] := f [n−1][x1, x2, . . . , xn] −f [n−1][x2, x3, . . . , xn+1]
x1 −xn+1
.
The functions f [1], f [2] and f [n] are called the ﬁrst, the second and the nth divided
differences, respectively, of f .
From the recursive deﬁnition the symmetry is not clear. If f is a Cn-function,
then
f [n][x0, x1, . . . , xn] =
⎢
S
f (n)(t0x0 + t1x1 + · · · + tnxn) dt1dt2 · · · dtn,
(3.18)
where the integral is on the set S := {(t1, . . . , tn) ∈Rn : ti ⊥0, ⎡
i ti ∗1} and
t0 = 1 −⎡n
i=1 ti. From this formula the symmetry is clear and if x0 = x1 = · · · =
xn = x, then
f [n][x0, x1, . . . , xn] = f (n)(x)
n!
.
Next we introduce the notion of Fréchet differentiability. Assume that a mapping
F : Mm →Mn is deﬁned in a neighbourhood of A ∈Mm. The derivative ∂f (A) :
Mm →Mn is a linear mapping such that
≤F(A + X) −F(A) −∂F(A)(X)≤2
≤X≤2
−→0 as X ∈Mm and X →0,
where ≤· ≤2 is the Hilbert–Schmidt norm in (1.4). This is the general deﬁnition. In
the next theorem F(A) will be deﬁned via the matrix functional calculus as f (A)
when f : (a, b) →R and A ∈Msa
n (a, b). Then the Fréchet derivative is a linear
mapping ∂f (A) : Msa
n →Msa
n such that
≤f (A + X) −f (A) −∂f (A)(X)≤2
≤X≤2
−→0 as X ∈Msa
n and X →0,
or equivalently
f (A + X) = f (A) + ∂f (A)(X) + o(≤X≤2).
Since Fréchet differentiability implies Gâtaux (or directional) differentiability, one
can differentiate f (A + t X) with respect to the real parameter t and
f (A + t X) −f (A)
t
→∂f (A)(X) as t →0.

3.4 Fréchet Derivatives
127
This notion of Fréchet differentiability for f (A) is inductively extended to the
general higher degree. To do this, we denote by B((Msa
n )m, Msa
n ) the set of all m-
multilinear maps from (Msa
n )m := Msa
n ×· · ·×Msa
n (m times) to Msa
n , and introduce
the norm of  ∈B((Msa
n )m, Msa
n ) as
≤≤:= sup

≤(X1, . . . , Xm)≤2 : Xi ∈Msa
n , ≤Xi≤2 ∗1, 1 ∗i ∗m
⎧
. (3.19)
Now assume that m ∈N with m ⊥2 and the (m −1)th Fréchet derivative
∂m−1 f (B) ∈B((Msa
n )m−1, Msa
n ) exists for all B ∈Msa
n (a, b) in a neighborhood
of A ∈Msa
n (a, b). We say that f (B) is m times Fréchet differentiable at A if
∂m−1 f (B) is once more Fréchet differentiable at A, i.e., there exists a
∂m f (A) ∈B(Msa
n , B((Msa
n )m−1, Msa
n )) = B((Msa
n )m, Msa
n )
such that
≤∂m−1 f (A + X) −∂m−1 f (A) −∂m f (A)(X)≤
≤X≤2
−→0 as X ∈Msa
n and X →0,
with respect to the norm (3.19) of B((Msa
n )m−1, Msa
n ). Then ∂m f (A) is called the
mth Fréchet derivative of f at A. Note that the norms of Msa
n and B((Msa
n )m, Msa
n )
are irrelevant to the deﬁnition of Fréchet derivatives since the norms on a ﬁnite-
dimensional vector space are all equivalent; we can use the Hilbert–Schmidt norm
just for convenience.
Example 3.32 Let f (x) = xk with k ∈N. Then (A + X)k can be expanded and
∂f (A)(X) consists of the terms containing exactly one factor of X:
∂f (A)(X) =
k−1
⎣
u=0
Au X Ak−1−u.
To obtain the second derivative, we put A + Y in place of A in ∂f (A)(X) and again
we take the terms containing exactly one factor of Y:
∂2 f (A)(X, Y)
=
k−1
⎣
u=0
u−1
⎣
v=0
AvY Au−1−v

X Ak−1−u +
k−1
⎣
u=0
Au X
k−2−u
⎣
v=0
AvY Ak−2−u−v

.
The formulation
∂2 f (A)(X1, X2) =
⎣
u+v+w=n−2
⎣
π
Au Xπ(1)Av Xπ(2)Aw

128
3
Functional Calculus and Derivation
is more convenient, where u, v, w ⊥0 and π denotes the permutations of {1, 2}. □
Theorem 3.33 Let m ∈N and assume that f : (a, b) →R is a Cm-function. Then
the following properties hold:
(1)
f (A) is m times Fréchet differentiable at every A ∈Msa
n (a, b). If the diagonal-
ization of A ∈Msa
n (a, b) is A = UDiag(λ1, . . . , λn)U ∗, then the mth Fréchet
derivative ∂m f (A) is given by
∂m f (A)(X1, . . . , Xm) = U
⎞
n
⎣
k1,...,km−1=1
f [m][λi, λk1, . . . , λkm−1, λ j]
×
⎣
π∈Sm
(X≥
π(1))ik1(X≥
π(2))k1k2 · · · (X≥
π(m−1))km−2km−1(X≥
π(m))km−1 j
⎠n
i, j=1
U ∗
for all Xi ∈Msa
n with X≥
i = U ∗XiU (1 ∗i ∗m). (Sm is the set of permutations
on {1, . . . , m}.)
(2) Themap A →∂m f (A)isanorm-continuousmapfrom Msa
n (a, b)to B((Msa
n )m,
Msa
n ).
(3) For every A ∈Msa
n (a, b) and every X1, . . . , Xm ∈Msa
n ,
∂m f (A)(X1, . . . , Xm) =
∂m
∂t1 · · · ∂tm
f (A + t1X1 + · · · + tm Xm)

t1=···=tm=0.
Proof: (Sketch) When f (x) = xk, it is easily veriﬁed by a direct computation
that ∂m f (A) exists and
∂m f (A)(X1, . . . , Xm)
=
⎣
u0,u1,...,um⊥0
u0+u1+···+um=k−m
⎣
π∈Sm
Au0 Xπ(1)Au1 Xπ(2)Au2 · · · Aum−1 Xπ(m)Aum,
see Example 3.32. (If m > k, then ∂m f (A) = 0.) The above expression is further
written as
⎣
u0,u1,...,um⊥0
u0+u1+...+um=k−m
⎣
π∈Sm
U
⎞
⎣
k1,...,km−1=1
λu0
i λu1
k1 · · · λum−1
km−1 λum
j
× (X≥
π(1))ik1(X≥
π(2))k1k2 · · · (X≥
π(m−1))km−2km−1(X≥
π(m))km−1 j
⎠n
i, j=1
U ∗
= U
⎞
n
⎣
k1,...,km−1=1
⎛
⎣
u0,u1,...,um⊥0
u0+u1+···+um=k−m
λu0
i λu1
k1 · · · λum−1
km−1 λum
j
⎝

3.4 Fréchet Derivatives
129
×
⎣
π∈Sm
(X≥
π(1))ik1(X≥
π(2))k1k2 · · · (X≥
π(m−1))km−2km−1(X≥
π(m))km−1 j
⎠n
i, j=1
U ∗
= U
⎞
n
⎣
k1,...,km−1=1
f [m][λi, λk1, . . . , λkm−1, λ j]
×
⎣
π∈Sm
(X≥
π(1))ik1(X≥
π(2))k1k2 · · · (X≥
π(m−1))km−2km−1(X≥
π(m))km−1 j
⎠n
i, j=1
U ∗
by Exercise 31. Hence it follows that ∂m f (A) exists and the expression in (1) is valid
for all polynomials f . We can prove this and the continuity assertion in (2) for all
Cm functions f on (a, b) by a method based on induction on m and approximation
by polynomials. The details are not given here.
Formula (3) follows from the fact that Fréchet differentiability implies Gâtaux (or
directional) differentiability. One can differentiate f (A + t1X1 + · · · + tm Xm) as
∂m
∂t1 · · · ∂tm
f (A + t1X1 + · · · + tm Xm)

t1=···=tm=0
=
∂m
∂t1 · · · ∂tm−1
∂f (A + t1X1 + · · · + tm−1Xm−1)(Xm)

t1=···=tm−1=0
= · · · = ∂m f (A)(X1, . . . , Xm).
□
Example 3.34 In particular, when f is C1 on (a, b) and A = Diag(λ1, . . . , λn) is
diagonal in Msa
n (a, b), then the Fréchet derivative ∂f (A) at A is written as
∂f (A)(X) =
⎪
f [1](λi, λ j)
⎨n
i, j=1 ∞X,
where ∞denotes the Schur product. This was Theorem 3.25.
When f is C2 on (a, b), the second Fréchet derivative ∂2 f (A) at A = Diag
(λ1, . . . , λn) ∈Msa
n (a, b) is written as
∂2 f (A)(X, Y) =
⎞n
⎣
k=1
f [2](λi, λk, λ j)

XikYkj + Yik Xkj
⎠n
i, j=1
.
□
Example 3.35 If f is a holomorphic function then the Taylor expansion
f (A + X) = f (A) +
→
⎣
k=1
1
k!∂k f (A)(X, . . . , X
⎩
$%
&
m
)

130
3
Functional Calculus and Derivation
has a simple computation, see (3.10):
f (A + X) =
1
2πi
⎢
α
f (z)(zI −A −X)−1 dz .
Since
zI −A −X = (zI −A)1/2(I −(zI −A)−1/2X(zI −A)−1/2)(zI −A)1/2,
we have the expansion
(zI −A −X)−1
= (zI −A)−1/2(I −(zI −A)−1/2X(zI −A)−1/2)−1(zI −A)−1/2
= (zI −A)−1/2
→
⎣
n=0

(zI −A)−1/2X(zI −A)−1/2n
(zI −A)−1/2
= (zI −A)−1 + (zI −A)−1X(zI −A)−1
+ (zI −A)−1X(zI −A)−1X(zI −A)−1 + · · · .
Hence
f (A + X) = 1
2πi
⎢
α
f (z)(zI −A)−1 dz
+
1
2πi
⎢
α
f (z)(zI −A)−1X(zI −A)−1 dz + · · ·
= f (A) + ∂f (A)(X) + 1
2!∂2 f (A)(X, X) + · · · ,
which is the Taylor expansion.
When f satisﬁes the Cm assumption as in Theorem 3.33, we have the Taylor
formula
f (A + X) = f (A) +
→
⎣
k=1
1
k!∂k f (A)(X(1), . . . , X(k)) + o(≤X≤m
2 )
as X ∈Msa
n and X →0. The details are not given here.
□
3.5 Notes and Remarks
Formula (3.7) is due to Masuo Suzuki, Generalized Trotter’s formula and systematic
approximants of exponential operators and inner derivations with applications to
many-body problems, Commun. Math. Phys. 51(1976), 183–190.

3.5 Notes and Remarks
131
The Bessis–Moussa–Villani conjecture (or BMV conjecture) was published in
the paper D. Bessis, P. Moussa and M. Villani, Monotonic converging variational
approximations to the functional integrals in quantum statistical mechanics, J. Math.
Phys. 16 (1975), 2318–2325. Theorem 3.12 is from the paper [64] of E. H. Lieb and
R. Seiringer. A proof appeared in the paper H. R. Stahl, Proof of the BMV conjecture,
arXiv:1107.4875v3.
The contour integral representation (3.10) was found by Henri Poincaré in 1899.
Formula (3.18) is called the Hermite–Genocchi formula.
Formula (3.11) ﬁrst appeared in the work of J. J. Sylvester in 1833 and (3.12) is
due to H. Richter (1949). It is remarkable that J. von Neumann proved in 1929 that
≤A −I≤, ≤B −I≤, ≤AB −I≤< 1 and AB = BA implies log AB = log A + log B.
Theorem3.33 is essentially due to Ju. L. Daleckii and S. G. Krein, Integration and
differentiation of functions of Hermitian operators and applications to the theory of
perturbations, Amer. Math. Soc. Transl., Ser. 2 47 (1965), 1–30. There the higher
Gâteaux derivatives of the function t →f (A + t X) were obtained for self-adjoint
operators in an inﬁnite-dimensional Hilbert space. As to the version of Fréchet deriv-
atives in Theorem 3.33, the proof for the case m = 1 is in the book [20] of Rajendra
Bhatia and the proof for the higher degree case is in Fumio Hiai, Matrix Analy-
sis: Matrix Monotone Functions, Matrix Means, and Majorization, Interdisciplinary
Information Sciences 16 (2010), 139–248.
3.6 Exercises
1. Prove that
∂
∂t etA = etAA.
2. Compute the exponential of the matrix
⎥
⎦⎦⎦⎦
0 0 0 0 0
1 0 0 0 0
0 2 0 0 0
0 0 3 0 0
0 0 0 4 0


.
What is the extension to the n × n case?
3. Use formula (3.3) to prove Theorem 3.4.
4. Let P and Q be ortho-projections. Give an elementary proof of the inequality
Tr eP+Q ∗Tr ePeQ.
5. Prove the Golden–Thompson inequality using the trace inequality

132
3
Functional Calculus and Derivation
Tr (C D)n ∗Tr Cn Dn
(n ∈N)
for C, D ⊥0.
6. Give a counterexample for the inequality
|Tr eAeBeC| ∗Tr eA+B+C
with Hermitian matrices. (Hint: Use the Pauli matrices.)
7. Solve the equation
eA =
⎞cos t −sin t
sin t
cos t
⎠
where t ∈R is given.
8. Show that
exp
⎛⎞A B
0 A
⎠⎝
=
⎞
eA ' 1
0 etABe(1−t)A dt
0
eA
⎠
.
9. Let A and B be self-adjoint matrices. Show that
|Tr eA+iB| ∗Tr eA .
10. Prove the estimate
≤eA+B −(eA/neB/n)n≤2 ∗1
2n ≤AB −B A≤2 exp(≤A≤2 + ≤B≤2).
11. Show that ≤A −I≤, ≤B −I≤, ≤AB −I≤< 1 and AB = BA implies log AB =
log A + log B for matrices A and B.
12. Give an example of a pair of commuting matrices A and B such that log AB √=
log A + log B.
13. Let
C = c0I + c(c1σ1 + c2σ2 + c3σ3) with c2
1 + c2
2 + c2
3 = 1,
where σ1, σ2, σ3 are the Pauli matrices and c0, c1, c2, c3 ∈R. Show that
eC = ec0 ((cosh c)I + (sinh c)(c1σ1 + c2σ2 + c3σ3)) .
14. Let A ∈M3 have eigenvalues λ, λ, μ with λ √= μ. Show that
etA = eλt(I + t(A −λI)) + eμt −eλt
(μ −λ)2 (A −λI)2 −teλt
μ −λ(A −λI)2.

3.6 Exercises
133
15. Assume that A ∈M3 has distinct eigenvalues λ, μ, ν. Show that etA is
eλt (A −μI)(A −νI)
(λ −μ)(λ −ν)
+ eμt (A −λI)(A −νI)
(μ −λ)(μ −ν)
+ eνt (A −λI)(A −μI)
(ν −λ)(ν −μ)
.
16. Assume that A ∈Mn is diagonalizable and let f (t) = tm with m ∈N. Show
that (3.8) and (3.10) are the same matrices.
17. Prove Corollary 3.11 directly in the case B = AX −X A.
18. Let 0 < D ∈Mn be a ﬁxed invertible positive matrix. Show that the inverse of
the linear mapping
JD : Mn →Mn,
JD(B) := 1
2(DB + BD)
is the mapping
J−1
D (A) =
⎢→
0
e−t D/2 Ae−t D/2 dt .
19. Let 0 < D ∈Mn be a ﬁxed invertible positive matrix. Show that the inverse of
the linear mapping
JD : Mn →Mn,
JD(B) :=
⎢1
0
Dt BD1−t dt
is the mapping
J−1
D (A) =
⎢→
0
(D + t I)−1A(D + t I)−1 dt.
(3.20)
20. Prove (3.16) directly for the case f (t) = tn, n ∈N.
21. Let f : [α, β] →R be a convex function. Show that
Tr f (B) ⊥
⎣
i
f (Tr B pi)
(3.21)
for a pairwise orthogonal family (pi) of minimal projections with ⎡
i pi = I
and for a self-adjoint matrix B with spectrum in [α, β]. (Hint: Use the spectral
decomposition of B.)
22. Prove Theorem 3.27 using formula (3.21). (Hint: Take the spectral decomposi-
tion of B = λB1 + (1 −λ)B2 and show that
λTr f (B1) + (1 −λ)Tr f (B2) ⊥Tr f (B).)
23. Let A and B be positive matrices. Show that

134
3
Functional Calculus and Derivation
A−1 log(AB−1) = A−1/2 log(A1/2B−1 A1/2)A−1/2.
(Hint: Use (3.17).)
24. Show that
d2
dt2 log(A + t K)

t=0 = −2
⎢→
0
(A + sI)−1K(A + sI)−1K(A + sI)−1 ds.
25. Show that
∂2 log A(X1, X2) = −
⎢→
0
(A + sI)−1X1(A + sI)−1X2(A + sI)−1 ds
−
⎢→
0
(A + sI)−1X2(A + sI)−1X1(A + sI)−1 ds
for a positive invertible matrix A.
26. Prove the BMV conjecture for 2 × 2 matrices.
27. Show that
∂2 A−1(X1, X2) = A−1X1A−1X2 A−1 + A−1X2 A−1X1 A−1
for an invertible variable A.
28. Differentiate the equation
≡
A + tB
≡
A + tB = A + tB
and show that for positive A and B
d
dt
≡
A + tB

t=0 ⊥0.
29. For a real number 0 < α √= 1 the Rényi entropy is deﬁned as
Sα(D) :=
1
1 −α log Tr Dα
for a positive matrix D such that Tr D = 1. Show that Sα(D) is a decreasing
function of α. What is the limit limα→1 Sα(D)? Show that Sα(D) is a concave
functional of D for 0 < α < 1.
30. Fix a positive invertible matrix D ∈Mn and deﬁne a linear mapping Mn →Mn
by KD(A) := DAD. Consider the differential equation
∂
∂t D(t) = KD(t)T,
D(0) = ρ0,

3.6 Exercises
135
where ρ0 is positive invertible and T is self-adjoint in Mn. Show that D(t) =
(ρ−1
0
−tT )−1 is the solution of the equation.
31. If f (x) = xk with k ∈N, verify that
f [n][x1, x2, . . . , xn+1] =
⎣
u1,u2,...,un+1⊥0
u1+u2+...+un+1=k−n
xu1
1 xu2
2 · · · xun
n xun+1
n+1 .
32. Show that for a matrix A > 0 the integral
log(I + A) =
⎢→
1
A(tI + A)−1t−1 dt
holds. (Hint: Use (3.12).)

Chapter 4
Matrix Monotone Functions and Convexity
Let (a, b) ∈R be an interval. A function f : (a, b) →R is said to be monotone for
n × n matrices if f (A) ≤f (B) whenever A and B are self-adjoint n × n matrices,
A ≤B and their eigenvalues are in (a, b). If a function is monotone for every matrix
size, then it is called matrix monotone or operator monotone. (One can see by an
approximation argument that if a function is matrix monotone for every matrix size,
then A ≤B also implies f (A) ≤f (B) for operators on an inﬁnite-dimensional
Hilbert space.) On the other hand, a function f : (a, b) →R is said to be matrix
convex if
f (t A + (1 −t)B) ≤t f (A) + (1 −t) f (B)
for all self-adjoint matrices A, B with eigenvalues in (a, b) and for all 0 ≤t ≤1.
When −f is matrix convex, then f is called matrix concave.
The theory of operator/matrix monotone functions was initiated by Karel Löwner,
which was soon followed by Fritz Kraus’ theory of operator/matrix convex functions.
After further developments due to other authors (for instance, Bendat and Sherman,
Korányi), Hansen and Pedersen established a modern treatment of matrix monotone
and convex functions. A remarkable feature of Löwner’s theory is that we have sev-
eral characterizations of matrix monotone and matrix convex functions from several
different points of view. The importance of complex analysis in studying matrix
monotone functions is well understood from their characterization in terms of an-
alytic continuation as Pick functions. Integral representations for matrix monotone
and matrix convex functions are essential ingredients of the theory both theoretically
and in applications. The notion of divided differences has played a vital role in the
theory from its very beginning.
In real analysis, monotonicity and convexity are not directly related, but in matrix
analysis the situation is very different. For example, a matrix monotone function
on (0, ∞) is matrix concave. Matrix monotone and matrix convex functions have
several applications, but for a concrete function it is not so easy to verify its matrix
monotonicity or matrix convexity. Such functions are typically described in terms of
integral formulas.
F. Hiai and D. Petz, Introduction to Matrix Analysis and Applications,
137
Universitext, DOI: 10.1007/978-3-319-04150-6_4,
© Hindustan Book Agency 2014

138
4
Matrix Monotone Functions and Convexity
4.1 Some Examples of Functions
Example 4.1 Let t > 0 be a parameter. The function f (x) = −(t + x)−1 is matrix
monotone on [0, ∞).
Let A and B be positive matrices of the same order. Then At := t I + A and
Bt := t I + B are invertible, and
At ≤Bt ∗⇒B−1/2
t
At B−1/2
t
≤I ∗⇒∥B−1/2
t
At B−1/2
t
∥≤1
∗⇒∥A1/2
t
B−1/2
t
∥≤1.
Since the adjoint preserves the operator norm, the latter condition is equivalent to
∥B−1/2
t
A1/2
t
∥≤1, which implies that B−1
t
≤A−1
t
.
□
Example 4.2 The function f (x) = log x is matrix monotone on (0, ∞).
This follows from the formula
log x =
⎡∞
0
1
1 + t −
1
x + t dt ,
which is easy to verify. The integrand
ft(x) :=
1
1 + t −
1
x + t
is matrix monotone according to the previous example. It follows that
n
⎢
i=1
ci ft(i)(x)
is matrix monotone for any t(i) and positive ci ≥R. The integral is the limit of such
functions. Therefore it is a matrix monotone function as well.
There are several other ways to demonstrate the matrix monotonicity of the log-
arithm.
□
Example 4.3 The function
f+(x) =
0
⎢
n=−∞
⎣
1
(n −1/2)π −x −
nπ
n2π + 1
⎤
is matrix monotone on the interval (−π/2, +∞) and
f−(x) =
∞
⎢
n=1
⎣
1
(n −1/2)π −x −
nπ
n2π + 1
⎤

4.1 Some Examples of Functions
139
is matrix monotone on the interval (−∞, π/2). Therefore,
tan x = f+(x) + f−(x) =
∞
⎢
n=−∞
⎣
1
(n −1/2)π −x −
nπ
n2π + 1
⎤
is matrix monotone on the interval (−π/2, π/2).
□
Example 4.4 To show that the square root function is matrix monotone, consider
the function
F(t) :=
∪
A + t X
deﬁned for t ≥[0, 1] and for ﬁxed positive matrices A and X. If F is increasing,
then F(0) =
∪
A ≤∪A + X = F(1).
In order to show that F is increasing, it is enough to verify that the eigenvalues
of F⊥(t) are positive. Differentiating the equality F(t)F(t) = A + t X, we have
F⊥(t)F(t) + F(t)F⊥(t) = X.
Being a limit of self-adjoint matrices, F⊥is self-adjoint. Let F⊥(t) = ⎥
i λi Ei be its
spectral decomposition. (Of course, both the eigenvalues and the projections depend
on the value of t.) Then
⎢
i
λi(Ei F(t) + F(t)Ei) = X
and after multiplication by E j on the left and on the right, we have for the trace
2λ jTr E j F(t)E j = Tr E j X E j.
Since both traces are positive, λ j must be positive as well.
More generally, for every 0 < t < 1, matrix monotonicity holds: 0 ≤A ≤B
implies At ≤Bt. This is often called the Löwner–Heinz inequality. A proof will
be given in Example 4.45, and another approach is in Theorem 5.3 based on the
geometric mean.
Next we consider the case t > 1. Take the matrices
A =
⎦3
2 0
0 3
4

and
B = 1
2
⎦1 1
1 1

.
Then A ⊂B ⊂0 can be checked. Since B is an orthogonal projection, for each
p > 1 we have B p = B and
Ap −B p =
⎦	 3
2

p −1
2
−1
2
−1
2
	 3
4

p −1
2

.

140
4
Matrix Monotone Functions and Convexity
We can compute
det(Ap −B p) = 1
2
⎣3
8
⎤p
(2 · 3p −2p −4p).
If Ap ⊂B p then we must have det(Ap −B p) ⊂0 so that 2 · 3p −2p −4p ⊂0,
which is not true when p > 1. Hence Ap ⊂B p does not hold for any p > 1.
□
The previous example contained an important idea. To determine whether a func-
tion f is matrix monotone, one has to investigate the derivative of f (A + t X).
Theorem 4.5 A smooth function f : (a, b) →R is matrix monotone for n × n
matrices if and only if the divided difference matrix D ≥Mn deﬁned as
Di j =
 f (ti)−f (t j)
ti−t j
i f
ti −t j √= 0,
f ⊥(ti)
i f
ti −t j = 0
is positive semideﬁnite for t1, t2, . . . , tn ≥(a, b).
Proof: Let A be a self-adjoint and B be a positive semideﬁnite n × n matrix.
When f is matrix monotone, the function t ≡→f (A + t B) is an increasing function
of the real variable t. Therefore, the derivative, which is a matrix, must be positive
semideﬁnite. To compute the derivative, we use formula (3.16) of Theorem 3.25.
Schur’s theorem implies that the derivative is positive if the divided difference matrix
is positive.
To show the converse, let the matrix B have all entries equal to 1. Then the
(positive) derivative D ⊃B is equal to D.
□
The smoothness assumption in the previous theorem is not essential. At the begin-
ning of the theory Löwner proved that if a function f : (a, b) →R has the property
that A ≤B for A, B ≥M2 implies f (A) ≤f (B), then f must be a C1-function.
The previous theorem can be reformulated in terms of a positive deﬁnite kernel.
The divided difference
ψ(x, y) =

f (x)−f (y)
x−y
if
x √= y,
f ⊥(x)
if
x = y
is an (a, b) × (a, b) →R kernel function. f is matrix monotone if and only if ψ is
a positive deﬁnite kernel.
Example 4.6 The function f (x) := exp x is not matrix monotone, since the divided
difference matrix

4.1 Some Examples of Functions
141

⎛
exp x
exp x −exp y
x −y
exp y −exp x
y −x
exp y
⎝
⎞⎠
does not have positive determinant (for x = 0 and for large y).
□
Example 4.7 We study the monotone function
f (x) =
 ∪x
if
0 ≤x ≤1,
(1 + x)/2
if
1 ≤x.
This is matrix monotone in the intervals [0, 1] and [1, ∞). Theorem 4.5 can be used
to show that this is monotone on [0, ∞) for 2 × 2 matrices. We need to show that
for 0 < x < 1 and 1 < y

f ⊥(x)
f (x)−f (y)
x−y
f (x)−f (y)
x−y
f ⊥(y)

=
⎦f ⊥(x) f ⊥(z)
f ⊥(z) f ⊥(y)

(for some z ≥[x, y])
is a positive matrix. This is true, however f is not monotone for larger matrices. □
Example 4.8 The function f (x) = x2 is matrix convex on the whole real line. This
follows from the obvious inequality
⎣A + B
2
⎤2
≤A2 + B2
2
.
□
Example 4.9 The function f (x) = (x + t)−1 is matrix convex on [0, ∞) when
t > 0. It is enough to show that
⎣A + B
2
⎤−1
≤A−1 + B−1
2
,
which is equivalent to
⎣B−1/2 AB−1/2 + I
2
⎤−1
≤
	
B−1/2 AB−1/2
−1 + I
2
.
This holds, since
⎣X + I
2
⎤−1
≤X−1 + I
2
is true for any invertible matrix X ⊂0.

142
4
Matrix Monotone Functions and Convexity
Note that this convexity inequality is equivalent to the relation between the arith-
metic and harmonic means.
□
4.2 Convexity
Let V be a vector space (over the real scalars). Then u, v ≥V are called the endpoints
of the line-segment
[u, v] := {λu + (1 −λ)v : λ ≥R, 0 ≤λ ≤1}.
A subset A ∈V is convex if for any u, v ≥A the line-segment [u, v] is contained
in A. A set A ∈V is convex if and only if for every ﬁnite subset v1, v2, . . . , vn and
for every family of real positive numbers λ1, λ2, . . . , λn with sum 1
n
⎢
i=1
λivi ≥A.
For example, if ∥· ∥: V →R+ is a norm, then
{v ≥V : ∥v∥≤1}
is a convex set. The intersection of convex sets is a convex set.
In the vector space Mn the self-adjoint matrices and the positive matrices form a
convex set. Let (a, b) a real interval. Then
{A ≥Msa
n
: σ(A) ∈(a, b)}
is a convex set.
Example 4.10 Let
Sn := {D ≥Msa
n
: D ⊂0 and Tr D = 1}.
This is a convex set, since it is the intersection of convex sets. (In quantum theory
this set is called the state space.)
If n = 2, then a popular parametrization of the matrices in S2 is
1
2
⎦1 + λ3 λ1 −iλ2
λ1 + iλ2 1 −λ3

= 1
2(I + λ1σ1 + λ2σ2 + λ3σ3),
where σ1, σ2, σ3 are the Pauli matrices and the necessary and sufﬁcient condition to
be in S2 is

4.2 Convexity
143
λ2
1 + λ2
2 + λ2
3 ≤1.
This shows that the convex set S2 can be viewed as the unit ball in R3. If n > 2, then
the geometric picture of Sn is not so clear.
□
If A is a subset of the vector space V , then its convex hull, denoted by co A, is
the smallest convex set containing A, i.e.,
co A :=
 n
⎢
i=1
λivi : vi ≥A, λi ⊂0, 1 ≤i ≤n,
n
⎢
i=1
λi = 1, n ≥N

.
Let A ∈V be a convex set. The vector v ≥A is an extreme point of A if the
conditions
v1, v2 ≥A,
0 < λ < 1,
λv1 + (1 −λ)v2 = v
imply that v1 = v2 = v.
In the convex set S2 the extreme points correspond to the parameters satisfying
λ2
1 + λ2
2 + λ2
3 = 1. (If S2 is viewed as a ball in R3, then the extreme points are in the
boundary of the ball.) For extreme points of Sn, see Exercise 14.
Let J ∈R be an interval. A function f : J →R is said to be convex if
f (ta + (1 −t)b) ≤t f (a) + (1 −t) f (b)
(4.1)
for all a, b ≥J and 0 ≤t ≤1. This inequality is equivalent to the positivity of the
second divided difference
f [2][a, b, c] =
f (a)
(a −b)(a −c) +
f (b)
(b −a)(b −c) +
f (c)
(c −a)(c −b)
=
1
c −b
 f (c) −f (a)
c −a
−f (b) −f (a)
b −a

for all distinct a, b, c ≥J. If f ≥C2(J), then for x ≥J we have
lim
a,b,c→x f [2][a, b, c] = f ⊥⊥(x)
2
.
Hence the convexity is equivalent to the positivity of the second derivative. For a
convex function f the Jensen inequality
f
 ⎢
i
tiai

≤
⎢
i
ti f (ai)
holds whenever ai ≥J, ti ⊂0 and ⎥
i ti = 1. This inequality has an integral form

144
4
Matrix Monotone Functions and Convexity
f
⎣⎡
g(x) dμ(x)
⎤
≤
⎡
f ⊃g(x) dμ(x).
For a ﬁnite discrete probability measure μ this is exactly the Jensen inequality, but
it also holds for any probability measure μ on J and for a bounded Borel function g
with values in J.
Deﬁnition (4.1) makes sense if J is a convex subset of a vector space and f is a
real functional deﬁned on it.
A functional f is concave if −f is convex.
Let V be a ﬁnite-dimensional vector space and A ∈V be a convex subset. The
functional F : A →R ∞{+∞} is called convex if
F(λx + (1 −λ)y) ≤λF(x) + (1 −λ)F(y)
for every x, y ≥A and real number 0 < λ < 1. Let [u, v] ∈A be a line-segment
and deﬁne the function
F[u,v](λ) = F(λu + (1 −λ)v)
on the interval [0, 1]. F is convex if and only if all functions F[u,v] : [0, 1] →R are
convex when u, v ≥A.
Example 4.11 We show that the functional
A ≡→log Tr eA
is convex on the self-adjoint matrices, see Example 4.13.
The statement is equivalent to the convexity of the function
f (t) = log Tr (eA+t B)
(t ≥R)
(4.2)
for every A, B ≥Msa
n . To show this we prove that f ⊥⊥(0) ⊂0. It follows from
Theorem 3.23 that
f ⊥(t) = Tr eA+t B B
Tr eA+t B .
In the computation of the second derivative we use Dyson’s expansion
eA+t B = eA + t
⎡1
0
eu ABe(1−u)(A+t B) du .
In order to write f ⊥⊥(0) in a convenient form we introduce the inner product
∩X, Y◦Bo :=
⎡1
0
Tr et AX∼e(1−t)AY dt.

4.2 Convexity
145
(This is often called the Bogoliubov inner product.) Now
f ⊥⊥(0) = ∩I, I◦Bo∩B, B◦Bo −∩I, B◦2
Bo
(Tr eA)2
,
which is positive by the Schwarz inequality.
□
Let V be a ﬁnite-dimensional vector space with dual V ∼. Assume that the duality
is given by a bilinear pairing ∩· , · ◦. For a convex function F : V →R ∞{+∞} the
conjugate convex function F∼: V ∼→R ∞{+∞} is given by the formula
F∼(v∼) = sup{∩v, v∼◦−F(v) : v ≥V }.
F∼is sometimes called the Legendre transform of F. Since F∼is the supremum of
continuous linear functionals, it is convex and lower semi-continuous. The following
duality theorem is basic in convex analysis.
Theorem 4.12 If F : V →R∞{+∞} is a lower semi-continuous convex functional,
then F∼∼= F.
Example 4.13 The negative von Neumann entropy −S(D) = −Tr η(D) = Tr D
log D is continuous and convex on the density matrices. Let
F(X) =
Tr X log X
if X ⊂0 and Tr X = 1,
+∞
otherwise.
This is a lower semi-continuous convex functional on the linear space of all self-
adjoint matrices. The duality is given by ∩X, H◦= Tr X H. The conjugate functional
is
F∼(H) = sup{Tr X H −F(X) : X ≥Msa
n }
= −inf{−Tr X H −S(D) : D ≥Msa
n , D ⊂0, Tr D = 1} .
According to Example 3.29 the minimizer is D = eH/Tr eH, and therefore
F∼(H) = log Tr eH.
This is a continuous convex function of H ≥Msa
n . So Example 4.11 is recovered.
The duality theorem gives that
Tr X log X = sup{Tr X H −log Tr eH : H = H∼}
when X ⊂0 and Tr X = 1.
□

146
4
Matrix Monotone Functions and Convexity
Example 4.14 Fix a density matrix ρ = eH (with a self-adjoint H) and consider
the functional F deﬁned on the self-adjoint matrices by
F(X) :=
Tr X(log X −H)
if X ⊂0 and Tr X = 1 ,
+∞
otherwise.
F is essentially the relative entropy with respect to ρ:
S(X∥ρ) := Tr X(log X −log ρ).
The duality is ∩X, B◦= Tr X B if X and B are self-adjoint matrices. We want
to show that the functional B ≡→log Tr eH+B is the Legendre transform or the
conjugate function of F:
log Tr eB+H = max{Tr X B −S(X∥eH) : X is positive, Tr X = 1} .
We introduce the notation
f (X) = Tr X B −S(X∥eH)
for a density matrix X. When P1, . . . , Pn are projections of rank one with
⎥n
i=1 Pi = I, we write
f
⎣n
⎢
i=1
λi Pi
⎤
=
n
⎢
i=1
(λiTr Pi B + λiTr Pi H −λi log λi) ,
where λi ⊂0, ⎥n
i=1 λi = 1. Since
∂
∂λi
f
⎣n
⎢
i=1
λi Pi
⎤
λi=0
= +∞,
we see that f (X) attains its maximum at a matrix X0 > 0, Tr X0 = 1. Then for any
self-adjoint Z, Tr Z = 0, we have
0 = d
dt f (X0 + t Z)

t=0
= Tr Z(B + H −log X0) ,
so that B + H −log X0 = cI with c ≥R. Therefore X0 = eB+H/Tr eB+H and
f (X0) = log Tr eB+H by a simple computation.
On the other hand, if X is positive invertible with Tr X = 1, then
S(X∥eH) = max{Tr X B −log Tr eH+B : B is self-adjoint}

4.2 Convexity
147
by the duality theorem.
□
Theorem 4.15 Let α : Mn →Mm be a positive unital linear mapping and f :
R →R be a convex function. Then
Tr f (α(A)) ≤Tr α( f (A))
for every A ≥Msa
n .
Proof: Take the spectral decompositions
A =
⎢
j
ν j Q j
and
α(A) =
⎢
i
μi Pi.
So we have
μi = Tr (α(A)Pi)/Tr Pi =
⎢
j
ν jTr (α(Q j)Pi)/Tr Pi,
whereas the convexity of f yields
f (μi) ≤
⎢
j
f (ν j)Tr (α(Q j)Pi)/Tr Pi .
Therefore,
Tr f (α(A)) =
⎢
i
f (μi)Tr Pi ≤
⎢
i, j
f (ν j)Tr (α(Q j)Pi) = Tr α( f (A)) ,
which was to be proven.
□
It was stated in Theorem 3.27 that for a convex function f : (a, b) →R, the
functional A ≡→Tr f (A) is convex. It is rather surprising that in the equivalent
statement of the convexity of this functional the numerical coefﬁcient 0 < t < 1 can
be replaced by a matrix.
Theorem 4.16 Let f : (a, b) →R be a convex function and Ci, Ai ≥Mn be such
that
σ(Ai) ∈(a, b)
and
k
⎢
i=1
CiC∼
i = I.
Then
Tr f
 k
⎢
i=1
Ci AiC∼
i

≤
k
⎢
i=1
Tr Ci f (Ai)C∼
i .

148
4
Matrix Monotone Functions and Convexity
Proof: We prove only the case
Tr f (C AC∼+ DBD∼) ≤Tr C f (A)C∼+ Tr Df (B)D∼,
when CC∼+ DD∼= I. (The more general version can be treated similarly.)
Set F := C AC∼+ DBD∼and consider the spectral decomposition of A and B
as integrals:
X =
⎢
i
μX
i P X
i
=
⎡
λdE X(λ)
(X = A, B),
where μX
i are eigenvalues, P X
i are eigenprojections and the operator-valued measure
E X is deﬁned on the Borel subsets S of R as
E X(S) =
⎢
{P X
i
: μX
i ≥S}.
Assume that A, B, C, D ≥Mn and for a vector ξ ≥Cn we deﬁne a measure μξ:
μξ(S) = ∩(C E A(S)C∼+ DE B(S)D∼)ξ, ξ◦
= ∩E A(S)C∼ξ, C∼ξ◦+ ∩E B(S)D∼ξ, D∼ξ◦.
The motivation for deﬁning this measure is the formula
∩Fξ, ξ◦=
⎡
λdμξ(λ).
If ξ is a unit eigenvector of F (and f (F)), then
∩f (C AC∼+ DBD∼)ξ, ξ◦= ∩f (F)ξ, ξ◦= f (∩Fξ, ξ◦) = f
⎣⎡
λdμξ(λ)
⎤
≤
⎡
f (λ)dμξ(λ)
= ∩(C f (A)C∼+ Df (B)D∼)ξ, ξ◦.
(The inequality follows from the convexity of the function f .) To obtain the inequality
in the statement of the theorem we take a sum of the above inequalities where ξ ranges
over an orthonormal basis of eigenvectors of F.
□
Example 4.17 This example concerns a positive block matrix A and a concave
function f : R+ →R. The inequality
Tr f
⎣⎦A11 A12
A∼
12 A22
⎤
≤Tr f (A11) + Tr f (A22)

4.2 Convexity
149
is called the subadditivity of Tr f . We can take ortho-projections P1 and P2 such
that P1 + P2 = I and the subadditivity of Tr f
Tr f (A) ≤Tr f (P1AP1) + Tr f (P2 AP2)
follows from the previous theorem. A stronger version of this inequality is less trivial.
Let P1, P2 and P3 be ortho-projections such that P1 + P2 + P3 = I. We adopt
the notation P12 := P1 + P2 and P23 := P2 + P3. The strong subadditivity of Tr f
is the inequality
Tr f (A) + Tr f (P2 AP2) ≤Tr f (P12 AP12) + Tr f (P23AP23).
(4.3)
Further details on this will come later, see Theorems 4.50 and 4.51.
□
Example 4.18 The log function is concave. If A ≥Mn is positive deﬁnite and we
deﬁne the projections Pi := E(ii), then from the previous theorem we have
Tr log
n
⎢
i=1
Pi APi ⊂
n
⎢
i=1
Tr Pi(log A)Pi.
This means
n
⎢
i=1
log Aii ⊂Tr log A
and the exponential is
n

i=1
Aii ⊂exp(Tr log A) = det A.
This is the well-known Hadamard inequality for the determinant, see Theorem
1.30.
□
When K, L are two convex sets of matrices and F : K × L →R ∞{+∞} is a
function of two matrix variables, F is called jointly concave if
F(λA1 + (1 −λ)A2, λB1 + (1 −λ)B2) ⊂λF(A1, B1) + (1 −λ)F(A2, B2)
for every Ai ≥K, Bi ≥L and 0 < λ < 1. The function (A, B) ≥K×L ≡→F(A, B)
is jointly concave if and only if the function
A ⊕B ≥K ⊕L ≡→F(A, B)
is concave. In this way joint convexity and concavity can be conveniently studied.

150
4
Matrix Monotone Functions and Convexity
Lemma 4.19 If (A, B) ≥K × L ≡→F(A, B) is jointly concave, then
f (A) := sup{F(A, B) : B ≥L}
is concave on K.
Proof: Assume that f (A1), f (A2) < +∞. Let ε > 0 be a small number. We
have B1 and B2 such that
f (A1) ≤F(A1, B1) + ε
and
f (A2) ≤F(A2, B2) + ε.
Then
λ f (A1) + (1 −λ) f (A2) ≤λF(A1, B1) + (1 −λ)F(A2, B2) + ε
≤F(λA1 + (1 −λ)A2, λB1 + (1 −λ)B2) + ε
≤f (λA1 + (1 −λ)A2) + ε
and this gives the proof.
The case of f (A1) = +∞or f (A2) = +∞has a similar proof.
□
Example 4.20 The quantum relative entropy of X ⊂0 with respect to Y ⊂0 is
deﬁned as
S(X∥Y) := Tr (X log X −X log Y) −Tr (X −Y).
It is known (see Example 3.19) that S(X∥Y) ⊂0 and equality holds if and only
if X = Y. (We assumed X, Y > 0 in Example 3.19 but this is true for general
X, Y ⊂0.) A different formulation is
Tr Y = max {Tr (X log Y −X log X + X) : X ⊂0}.
For a positive deﬁnite D, selecting Y = exp(L + log D) we obtain
Tr exp(L + log D) = max{Tr (X(L + log D) −X log X + X) : X ⊂0}
= max{Tr (X L) −S(X∥D) + Tr D : X ⊂0}.
Since the quantum relative entropy is a jointly convex function, the function
F(X, D) := Tr (X L) −S(X∥D) + Tr D
is jointly concave as well. It follows that the maximization in X is concave and we
obtain that the functional
D ≡→Tr exp(L + log D)
(4.4)

4.2 Convexity
151
is concave on positive deﬁnite matrices. (This result is due to Lieb, but the present
proof is taken from [81].)
□
In the next lemma the operators
JD X =
⎡1
0
Dt X D1−t dt,
J−1
D K =
⎡∞
0
(t + D)−1K(t + D)−1 dt
for D, X, K ≥Mn, D > 0, are used (see Exercise 19 of Chap.3). Lieb’s concavity
theorem (see Example 7.9) says that D > 0 ≡→Tr X∼Dt X D1−t is concave for
every X ≥Mn. Thus, D > 0 ≡→∩X, JD X◦is concave. By using this we prove the
following:
Theorem 4.21 The functional
(D, K) ≡→Q(D, K) := ∩K, J−1
D K◦
is jointly convex on the domain {D ≥Mn : D > 0} × Mn.
Proof: Mn is a Hilbert space H with the Hilbert–Schmidt inner product. The
mapping K ≡→Q(D, K) is a quadratic form. If K := H ⊕H and D = λD1 + (1 −
λ)D2, then
M(K1 ⊕K2) := λQ(D1, K1) + (1 −λ)Q(D2, K2)
N(K1 ⊕K2) := Q(D, λK1 + (1 −λ)K2)
are quadratic forms on K. Note that M is non-degenerate. In terms of M and N the
dominance N ≤M is to be shown.
Let m and n be the corresponding sesquilinear forms on K, that is,
M(ξ) = m(ξ, ξ),
N(ξ) = n(ξ, ξ)
(ξ ≥K) .
There exists a positive operator X on K such that
n(ξ, η) = m(ξ, Xη)
(ξ, η ≥K)
and our aim is to show that its eigenvalues are ⊂1. If X(K ⊕L) = γ(K ⊕L) for
0 √= K ⊕L ≥H ⊕H, we have
n(K ⊕L⊥, K ⊕L) = γm(K ⊥⊕L⊥, K ⊕L)
for every K ⊥, L⊥≥H. This is rewritten in terms of the Hilbert–Schmidt inner product
as
∩λK ⊥+ (1 −λ)L⊥, J−1
D (λK + (1 −λ)L)◦= γλ∩K ⊥, J−1
D1 K◦+ γ(1 −λ)∩L⊥, J−1
D2 L◦,

152
4
Matrix Monotone Functions and Convexity
which is equivalent to the equations
J−1
D (λK + (1 −λ)L) = γJ−1
D1 K
and
J−1
D (λK + (1 −λ)L) = γJ−1
D2 L .
We infer
γJDM = λJD1 M + (1 −λ)JD2 M
with the new notation M := J−1
D (λK + (1 −λ)L). It follows that
γ∩M, JDM◦= λ∩M, JD1 M◦+ (1 −λ)∩M, JD2 M◦.
On the other hand, the concavity of D ≡→∩M, JDM◦yields the inequality
∩M, JDM◦⊂λ∩M, JD1 M◦+ (1 −λ)∩M, JD2 M◦
and we arrive at γ ≤1 if M √= 0. Otherwise, if M = 0, then we must have
γK = γL = 0 so that γ = 0.
□
Let J ∈R be an interval. As introduced at the beginning of the chapter, a function
f : J →R is said to be matrix convex if
f (t A + (1 −t)B) ≤t f (A) + (1 −t) f (B)
(4.5)
for all self-adjoint matrices A and B whose spectra are in J and for all numbers
0 ≤t ≤1. (The function f is matrix convex if the functional A ≡→f (A) is convex.)
f is matrix concave if −f is matrix convex.
The classical result concerns matrix convex functions on the interval (−1, 1).
Such a function has an integral decomposition
f (x) = β0 + β1x + β2
2
⎡1
−1
x2
1 −λx dμ(λ),
(4.6)
where μ is a probability measure and β2 ⊂0. (In particular, f must be an analytic
function.) The details will be given in Theorem 4.40.
Since self-adjoint operators on an inﬁnite-dimensional Hilbert space may be ap-
proximated by self-adjoint matrices, (4.5) holds for operators when it holds for all
matrices. The next theorem shows that in the convex combination t A + (1 −t)B the
numbers t and 1 −t can be replaced by matrices.
Theorem 4.22 Let f : [a, b] →R be a matrix convex function and Ci, Ai = A∼
i ≥
Mn be such that
σ(Ai) ∈[a, b]
and
k
⎢
i=1
CiC∼
i = I.

4.2 Convexity
153
Then
f
 k
⎢
i=1
Ci AiC∼
i

≤
k
⎢
i=1
Ci f (Ai)C∼
i .
(4.7)
Proof: We are content to prove the case k = 2:
f (C AC∼+ DBD∼) ≤C f (A)C∼+ Df (B)D∼,
when CC∼+ DD∼= I. The essential idea is contained in this case.
The condition CC∼+ DD∼= I implies that we can ﬁnd a unitary block matrix
U :=
⎦C D
X Y

when the entries X and Y are chosen properly. (Indeed, since |D∼| = (I −CC∼)1/2,
we have the polar decomposition D∼= W|D∼| with a unitary W. Then it is an
exercise to show that the choice of X = (I −C∼C)1/2 and Y = −C∼W ∼satisﬁes
the requirements.) Then
U
⎦A 0
0 B

U ∼=
⎦C AC∼+ DBD∼C AX∼+ DBY ∼
X AC∼+ Y BD∼X AX∼+ Y BY ∼

=:
⎦A11 A12
A21 A22

.
It is easy to check that
1
2 V
⎦A11 A12
A21 A22

V + 1
2
⎦A11 A12
A21 A22

=
⎦A11
0
0
A22

for
V =
⎦−I 0
0 I

.
It follows that the matrix
Z := 1
2 VU
⎦A 0
0 B

U ∼V + 1
2U
⎦A 0
0 B

U ∼
is diagonal, Z11 = C AC∼+ DBD∼and f (Z)11 = f (C AC∼+ DBD∼).
Next we use the matrix convexity of the function f :

154
4
Matrix Monotone Functions and Convexity
f (Z) ≤1
2 f
⎣
VU
⎦A 0
0 B

U ∼V
⎤
+ 1
2 f
⎣
U
⎦A 0
0 B

U ∼
⎤
= 1
2 VU f
⎣⎦A 0
0 B
⎤
U ∼V + 1
2U f
⎣⎦A 0
0 B
⎤
U ∼
= 1
2 VU
⎦f (A)
0
0
f (B)

U ∼V + 1
2U
⎦f (A)
0
0
f (B)

U ∼.
The right-hand side is diagonal with C f (A)C∼+ Df (B)D∼as (1, 1) entry. The
inequality implies the inequality between the (1, 1) entries and this is precisely the
inequality (4.7) for k = 2.
□
In the proof of (4.7) for n×n matrices, the ordinary matrix convexity was used for
(2n) × (2n) matrices. This is an important trick. The next theorem is due to Hansen
and Pedersen [40].
Theorem 4.23 Let f : [a, b] →R and a ≤0 ≤b.
If f is a matrix convex function, ∥V ∥≤1 and f (0) ≤0, then f (V ∼AV ) ≤
V ∼f (A)V holds if A = A∼and σ(A) ∈[a, b].
If f (P AP) ≤P f (A)P holds for an orthogonal projection P and A = A∼with
σ(A) ∈[a, b], then f is a matrix convex function and f (0) ≤0.
Proof: If f is matrix convex, we can apply Theorem 4.22. Choose B = 0 and W
such that V ∼V + W ∼W = I. Then
f (V ∼AV + W ∼BW) ≤V ∼f (A)V + W ∼f (B)W
holds and gives our statement.
Let A and B be self-adjoint matrices with spectrum in [a, b] and 0 < λ < 1.
Deﬁne
C :=
⎦A 0
0 B

,
U :=
⎦
∪
λ I
−
∪
1 −λ I
∪
1 −λ I
∪
λ I

,
P :=
⎦I 0
0 0

.
Then C = C∼with σ(C) ∈[a, b], U is a unitary and P is an orthogonal projection.
Since
PU ∼CU P =
⎦λA + (1 −λ)B 0
0
0

,
the assumption implies

4.2 Convexity
155
⎦f (λA + (1 −λ)B)
0
0
f (0)I

= f (PU ∼CU P)
≤P f (U ∼CU)P = PU ∼f (C)U P
=
⎦λ f (A) + (1 −λ) f (B) 0
0
0

.
This implies that f (λA + (1 −λ)B) ≤λ f (A) + (1 −λ) f (B) and f (0) ≤0.
□
Example 4.24 From the previous theorem we can deduce that if f : [0, b] →R
is a matrix convex function and f (0) ≤0, then f (x)/x is matrix monotone on the
interval (0, b].
Assume that 0 < A ≤B. Then B−1/2 A1/2 =: V is a contraction, since
∥V ∥2 = ∥V V ∼∥= ∥B−1/2 AB−1/2∥≤∥B−1/2BB−1/2∥= 1.
Therefore the theorem gives
f (A) = f (V ∼BV ) ≤V ∼f (B)V = A1/2B−1/2 f (B)B−1/2 A1/2,
which is equivalent to A−1 f (A) ≤B−1 f (B).
Now assume that g : [0, b] →R is matrix monotone. We want to show that
f (x) := xg(x) is matrix convex. By the previous theorem we need to show
P APg(P AP) ≤P Ag(A)P
for an orthogonal projection P and A ⊂0. From the monotonicity
g(A1/2P A1/2) ≤g(A)
and this implies
P A1/2g(A1/2P A1/2)A1/2P ≤P A1/2g(A)A1/2P.
Since g(A1/2P A1/2)A1/2P = A1/2Pg(P AP) and A1/2g(A)A1/2 = Ag(A), the
proof is complete.
□
Example 4.25 Heuristically we can say that Theorem 4.22 replaces all the numbers
in the Jensen inequality f (⎥
i tiai) ≤⎥
i ti f (ai) by matrices. Therefore
f
⎢
i
ai Ai

≤
⎢
i
f (ai)Ai
(4.8)

156
4
Matrix Monotone Functions and Convexity
holds for a matrix convex function f if ⎥
i Ai = I for positive matrices Ai ≥Mn
and for numbers ai ≥(a, b).
We want to show that the property (4.8) is equivalent to the matrix convexity
f (t A + (1 −t)B) ≤t f (A) + (1 −t) f (B).
Let
A =
⎢
i
λi Pi
and
B =
⎢
j
μ j Q j
be the spectral decompositions. Then
⎢
i
t Pi +
⎢
j
(1 −t)Q j = I
and from (4.8) we obtain
f (t A + (1 −t)B) = f

⎢
i
tλi Pi +
⎢
j
(1 −t)μ j Q j


≤
⎢
i
f (λi)t Pi +
⎢
j
f (μ j)(1 −t)Q j
= t f (A) + (1 −t) f (B).
This inequality was the aim.
□
An operator Z ≥B(H) is called a contraction if Z∼Z ≤I and an expansion
if Z∼Z ⊂I. For an A ≥Mn(C)sa let λ(A) = (λ1(A), . . . , λn(A)) denote the
eigenvalue vector of A in decreasing order with multiplicities.
Theorem 4.23 says that, for a function f : [a, b] →R with a ≤0 ≤b, the
matrix inequality f (Z∼AZ) ≤Z∼f (A)Z for every A = A∼with σ(A) ∈[a, b] and
every contraction Z characterizes the matrix convexity of f with f (0) ≤0. Now
we consider some similar inequalities in the weaker senses of eigenvalue dominance
under the simple convexity or concavity condition of f .
The ﬁrst theorem presents the eigenvalue dominance involving a contraction when
f is a monotone convex function with f (0) ≤0.
Theorem 4.26 Assume that f is a monotone convex function on [a, b] with a ≤0 ≤
b and f (0) ≤0. Then, for every A ≥Mn(C)sa with σ(A) ∈[a, b] and for every
contraction Z ≥Mn(C), there exists a unitary U such that
f (Z∼AZ) ≤U ∼Z∼f (A)ZU,
or equivalently,

4.2 Convexity
157
λk( f (Z∼AZ)) ≤λk(Z∼f (A)Z)
(1 ≤k ≤n).
Proof: We may assume that f is increasing; the other case is covered by taking
f (−x) and −A. First, note that for every B ≥Mn(C)sa and for every vector x with
∥x∥≤1 we have
f (∩x, Bx◦) ≤∩x, f (B)x◦.
(4.9)
Indeed, taking the spectral decomposition B = ⎥n
i=1 λi|ui◦∩ui| we have
f (∩x, Bx◦) = f
⎣n
⎢
i=1
λi|∩x, ui◦|2
⎤
≤
n
⎢
i=1
f (λi)|∩x, ui◦|2 + f (0)(1 −∥x∥2)
≤
n
⎢
i=1
f (λi)|∩x, ui◦|2 = ∩x, f (B)x◦
thanks to the convexity of f and f (0) ≤0. By the minimax expression in Theorem
1.27 there exists a subspace M of Cn with dim M = k −1 such that
λk(Z∼f (A)Z) =
max
x≥M⊗, ∥x∥=1∩x, Z∼f (A)Zx◦=
max
x≥M⊗, ∥x∥=1∩Zx, f (A)Zx◦.
Since Z is a contraction and f is non-decreasing, we apply (4.9) to obtain
λk(Z∼f (A)Z) ⊂
max
x≥M⊗, ∥x∥=1 f (∩Zx, AZx◦) = f
⎣
max
x≥M⊗, ∥x∥=1∩x, Z∼AZx◦
⎤
⊂f (λk(Z∼AZ)) = λk( f (Z∼AZ)).
In the second inequality above we have used the minimax expression again.
□
The following corollary was originally proved by Brown and Kosaki [23] in the
von Neumann algebra setting.
Corollary 4.27 Let f be a function on [a, b] with a ≤0 ≤b, and let A ≥Mn(C)sa,
σ(A) ∈[a, b], and Z ≥Mn(C) be a contraction. If f is a convex function with
f (0) ≤0, then
Tr f (Z∼AZ) ≤Tr Z∼f (A)Z.
If f is a concave function on R with f (0) ⊂0, then
Tr f (Z∼AZ) ⊂Tr Z∼f (A)Z.
Proof: Obviously, the two assertions are equivalent. To prove the ﬁrst, by approx-
imation we may assume that f (x) = αx + g(x) with α ≥R and a monotone and
convex function g on [a, b] with g(0) ≤0. Since Tr g(Z∼AZ) ≤Tr Z∼g f (A)Z by
Theorem 4.26, we have Tr f (Z∼AZ) ≤Tr Z∼f (A)Z.
□

158
4
Matrix Monotone Functions and Convexity
The next theorem is the eigenvalue dominance version of Theorem 4.23 for f
under a simple convexity condition.
Theorem 4.28 Assume that f is a monotone convex function on [a, b]. Then, for
every A1, . . . , Am ≥Mn(C)sa with σ(Ai) ∈[a, b] and every C1, . . . , Cm ≥Mn(C)
with ⎥m
i=1 C∼
i Ci = I, there exists a unitary U such that
f
⎣m
⎢
i=1
C∼
i AiCi
⎤
≤U ∼
⎣m
⎢
i=1
C∼
i f (Ai)Ci
⎤
U.
Proof: Letting f0(x) := f (x) −f (0) we have
f
⎣⎢
i
C∼
i AiCi
⎤
= f (0)I + f0
⎣⎢
i
C∼
i AiCi
⎤
,
⎢
i
C∼
i f (Ai)Ci = f (0)I +
⎢
i
C∼
i f0(Ai)Ci.
So it may be assumed that f (0) = 0. Set
A :=

⎛
A1 0 · · · 0
0 A2 · · · 0
...
...
...
...
0
0 · · · Am
⎝
⎞⎞⎞⎠
and
Z :=

⎛
C1 0 · · · 0
C2 0 · · · 0
...
... ... ...
Cm 0 · · · 0
⎝
⎞⎞⎞⎠.
For the block matrices f (Z∼AZ) and Z∼f (A)Z, we can take the (1, 1) blocks:
f
	⎥
i C∼
i AiCi

and ⎥
i C∼
i f (Ai)Ci. Moreover, all other blocks are 0. Hence
Theorem 4.26 implies that
λk
⎣
f
⎣⎢
i
C∼
i AiCi
⎤⎤
≤λk
⎣⎢
i
C∼
i f (Ai)Ci
⎤
(1 ≤k ≤n),
as desired.
□
A special case of Theorem 4.28 is that if f and A1, . . . , Am are as above,
α1, . . . , αm > 0 and ⎥m
i=1 αi = 1, then there exists a unitary U such that
f
⎣m
⎢
i=1
αi Ai
⎤
≤U ∼
⎣m
⎢
i=1
αi f (Ai)
⎤
U.
This inequality implies the trace inequality in Theorem 4.16, although monotonicity
of f is not assumed there.

4.3 Pick Functions
159
4.3 Pick Functions
Let C+ denote the upper half-plane:
C+ := {z ≥C : Im z > 0} = {reiϕ ≥C : r > 0, 0 < ϕ < π}.
Now we concentrate on analytic functions f : C+ →C. Recall that the range
f (C+) is a connected open subset of C unless f is a constant. An analytic function
f : C+ →C+ is called a Pick function.
The next examples show that this concept is related to the matrix monotonicity
property.
Example 4.29 Let z = reiθ with r > 0 and 0 < θ < π. For a real parameter p > 0
the range of the function
f p(z) = z p := r peipθ
is in P if and only if p ≤1.
This function f p(z) is a continuous extension of the real function 0 ≤x ≡→x p.
The latter is matrix monotone if and only if p ≤1. The similarity to the Pick function
concept is essential.
Recall that the real function 0 < x ≡→log x is matrix monotone as well. The
principal branch of log z deﬁned as
Log z := logr + iθ
is a continuous extension of the real logarithm function and it is also in P.
□
The next theorem, Nevanlinna’s theorem, provides the integral representation of
Pick functions.
Theorem 4.30 A function f : C+ →C is in P if and only if there exists an α ≥R,
a β ⊂0 and a positive ﬁnite Borel measure ν on R such that
f (z) = α + βz +
⎡∞
−∞
1 + λz
λ −z dν(λ),
z ≥C+.
(4.10)
The integral representation (4.10) can also be written as
f (z) = α + βz +
⎡∞
−∞
⎣
1
λ −z −
λ
λ2 + 1
⎤
dμ(λ),
z ≥C+,
(4.11)
where μ is a positive Borel measure on R given by dμ(λ) := (λ2 + 1) dν(λ) and so

160
4
Matrix Monotone Functions and Convexity
⎡∞
−∞
1
λ2 + 1 dμ(λ) < +∞.
Proof: The proof of the “if” part is easy. Assume that f is deﬁned on C+ as in
(4.10). For each z ≥C+, since
f (z + αz) −f (z)
αz
= β +
⎡
R
λ2 + 1
(λ −z)(λ −z −αz) dν(λ)
and
sup

λ2 + 1
(λ −z)(λ −z −αz)
 : λ ≥R, |αz| < Im z
2
⎧
< +∞,
it follows from the Lebesgue dominated convergence theorem that
lim
α→0
f (z + αz) −f (z)
αz
= β +
⎡
R
λ2 + 1
(λ −z)2 dν(λ).
Hence f is analytic in C+. Since
Im
⎣1 + λz
λ −z
⎤
= (λ2 + 1) Im z
|λ −z|2
,
z ≥C+,
we have
Im f (z) =
⎣
β +
⎡
R
λ2 + 1
|λ −z|2 dν(λ)
⎤
Im z ⊂0
for all z ≥C+. Therefore, we have f ≥P. The equivalence between the two
representations (4.10) and (4.11) is immediately seen from
1 + λz
λ −z = (λ2 + 1)
⎣
1
λ −z −
λ
λ2 + 1
⎤
.
The “only if” is the signiﬁcant part, the proof of which is skipped here.
□
Note that α, β and ν in Theorem 4.30 are uniquely determined by f . In fact,
letting z = i in (4.10) we have α = Re f (i). Letting z = iy with y > 0 we have
f (iy) = α + iβy +
⎡∞
−∞
λ(1 −y2) + iy(λ2 + 1)
λ2 + y2
dν(λ)
so that
Im f (iy)
y
= β +
⎡∞
−∞
λ2 + 1
λ2 + y2 dν(λ).
By the Lebesgue dominated convergence theorem this yields

4.3 Pick Functions
161
β = lim
y→∞
Im f (iy)
y
.
Hence α and β are uniquely determined by f . By (4.11), for z = x + iy we have
Im f (x + iy) = βy +
⎡∞
−∞
y
(x −λ)2 + y2 dμ(λ),
x ≥R, y > 0.
(4.12)
Thus the uniqueness of μ (hence ν) is a consequence of the so-called Stieltjes inver-
sion formula. (For details omitted here, see [36, pp.24–26] and [20, pp.139–141].)
For any open interval (a, b), −∞≤a < b ≤∞, we denote by P(a, b) the set
of all Pick functions which admit a continuous extension to C+ ∞(a, b) with real
values on (a, b).
The next theorem is a specialization of Nevanlinna’s theorem to functions in
P(a, b).
Theorem 4.31 A function f : C+ →C is in P(a, b) if and only if f is represented
as in (4.10) with α ≥R, β ⊂0 and a positive ﬁnite Borel measure ν on R \ (a, b).
Proof: Let f ≥P be represented as in (4.10) with α ≥R, β ⊂0 and a positive
ﬁnite Borel measure ν on R. It sufﬁces to prove that f ≥P(a, b) if and only if
ν((a, b)) = 0. First, assume that ν((a, b)) = 0. The function f expressed by (4.10)
is analytic in C+ ∞C−so that f (z) = f (z) for all z ≥C+. For every x ≥(a, b),
since
sup

λ2 + 1
(λ −x)(λ −x −αz)
 : λ ≥R \ (a, b), |αz| < 1
2 min{x −a, b −x}
⎧
is ﬁnite, the above proof of the “if” part of Theorem 4.30, using the Lebesgue
dominated convergence theorem, works for z = x as well, and so f is differentiable
(in the complex variable z) at z = x. Hence f ≥P(a, b).
Conversely, assume that f ≥P(a, b). It follows from (4.12) that
⎡∞
−∞
1
(x −λ)2 + y2 dμ(λ) = Im f (x + iy)
y
−β,
x ≥R, y > 0.
For any x ≥(a, b), since f (x) ≥R, we have
Im f (x + iy)
y
= Im f (x + iy) −f (x)
y
= Re f (x + iy) −f (x)
iy
−→Re f ⊥(x)
as y ↑0 and so the monotone convergence theorem yields
⎡∞
−∞
1
(x −λ)2 dμ(λ) = Re f ⊥(x),
x ≥(a, b).

162
4
Matrix Monotone Functions and Convexity
Hence, for any closed interval [c, d] included in (a, b), we have
R :=
sup
x≥[c,d]
⎡∞
−∞
1
(x −λ)2 dμ(λ) =
sup
x≥[c,d]
Re f ⊥(x) < +∞.
For each m ≥N let ck := c + (k/m)(d −c) for k = 0, 1, . . . , m. Then
μ([c, d)) =
m
⎢
k=1
μ([ck−1, ck)) ≤
m
⎢
k=1
⎡
[ck−1,ck)
(ck −ck−1)2
(ck −λ)2
dμ(λ)
≤
m
⎢
k=1
⎣d −c
m
⎤2 ⎡∞
−∞
1
(ck −λ)2 dμ(λ) ≤(d −c)2R
m
.
Letting m →∞gives μ([c, d)) = 0. This implies that μ((a, b)) = 0 and therefore
ν((a, b)) = 0.
□
Now let f ≥P(a, b). The above theorem says that f (x) on (a, b) admits the
integral representation
f (x) = α + βx +
⎡
R\(a,b)
1 + λx
λ −x dν(λ)
= α + βx +
⎡
R\(a,b)
(λ2 + 1)
⎣
1
λ −x −
λ
λ2 + 1
⎤
dν(λ),
x ≥(a, b),
where α, β and ν are as in the theorem. For any n ≥N and A, B ≥Msa
n with
σ(A), σ(B) ∈(a, b), if A ⊂B then (λI −A)−1 ⊂(λI −B)−1 for all λ ≥R\(a, b)
(see Example 4.1) and hence we have
f (A) = αI + β A +
⎡
R\(a,b)
(λ2 + 1)
⎣
(λI −A)−1 −
λ
λ2 + 1 I
⎤
dν(λ)
⊂αI + βB +
⎡
R\(a,b)
(λ2 + 1)
⎣
(λI −B)−1 −
λ
λ2 + 1 I
⎤
dν(λ) = f (B).
Therefore, f ≥P(a, b) is matrix monotone on (a, b). It will be shown in the next
section that f is matrix monotone on (a, b) if and only if f ≥P(a, b).
The following are examples of integral representations for typical Pick functions
from Example 4.29.
Example 4.32 The principal branch Log z of the logarithm in Example 4.29 is in
P(0, ∞). Its integral representation in the form (4.11) is
Log z =
⎡0
−∞
⎣
1
λ −z −
λ
λ2 + 1
⎤
dλ,
z ≥C+.

4.3 Pick Functions
163
To show this, it sufﬁces to verify the above expression for z = x ≥(0, ∞), that is,
log x =
⎡∞
0
⎣
−
1
λ + x +
λ
λ2 + 1
⎤
dλ,
x ≥(0, ∞),
which is immediate by a direct computation.
□
Example 4.33 If 0 < p < 1, then z p, as deﬁned in Example 4.29, is in P(0, ∞).
Its integral representation in the form (4.11) is
z p = cos pπ
2 + sin pπ
π
⎡0
−∞
⎣
1
λ −z −
λ
λ2 + 1
⎤
|λ|p dλ,
z ≥C+.
For this it sufﬁces to verify that
x p = cos pπ
2 + sin pπ
π
⎡∞
0
⎣
−
1
λ + x +
λ
λ2 + 1
⎤
λp dλ,
x ≥(0, ∞), (4.13)
which is computed as follows.
The function
z p−1
1 + z := r p−1ei(p−1)θ
1 + reiθ
,
z = reiθ, 0 < θ < 2π,
is analytic in the cut plane C \ (−∞, 0] and we integrate it along the contour
z =
⎪
⎨⎨⎩
⎨⎨
reiθ (ε ≤r ≤R, θ = +0),
Reiθ (0 < θ < 2π),
reiθ (R ⊂r ⊂ε, θ = 2π −0),
εeiθ (2π > θ > 0),
where 0 < ε < 1 < R. Apply the residue theorem and let ε ↑0 and R ↓∞to
show that
⎡∞
0
t p−1
1 + t dt =
π
sin pπ .
(4.14)
For each x > 0, substitute λ/x for t in (4.14) to obtain
x p = sin pπ
π
⎡∞
0
xλp−1
λ + x dλ,
x ≥(0, ∞).
Since
x
λ + x =
1
λ2 + 1 +
⎣
λ
λ2 + 1 −
1
λ + x
⎤
λ,

164
4
Matrix Monotone Functions and Convexity
it follows that
x p = sin pπ
π
⎡∞
0
λp−1
λ2 + 1 dλ + sin pπ
π
⎡∞
0
⎣
λ
λ2 + 1 −
1
λ + x
⎤
λp dλ,
x ≥(0, ∞).
Substitute λ2 for t in (4.14) with p replaced by p/2 to obtain
⎡∞
0
λp−1
λ2 + 1 dλ =
π
2 sin pπ
2
.
Hence (4.13) follows.
□
4.4 Löwner’s Theorem
The main aim of this section is to prove the primary result in Löwner’s theory, which
says that a matrix monotone (i.e., operator monotone) function on (a, b) belongs to
P(a, b).
Operator monotone functions on a ﬁnite open interval (a, b) are transformed into
those on a symmetric interval (−1, 1) via an afﬁne function. So it is essential to
analyze matrix monotone functions on (−1, 1). They are C∞-functions and f ⊥(0) >
0 unless f is constant. We denote by K the set of all matrix monotone functions on
(−1, 1) such that f (0) = 0 and f ⊥(0) = 1.
Lemma 4.20 Let f ≥K. Then:
(1) For every α ≥[−1, 1], (x + α) f (x) is matrix convex on (−1, 1).
(2) For every α ≥[−1, 1],
	
1 + α
x

f (x) is matrix monotone on (−1, 1).
(3)
f is twice differentiable at 0 and
f ⊥⊥(0)
2
= lim
x→0
f (x) −f ⊥(0)x
x2
.
Proof: (1) The proof is based on Example 4.24, but we have to change the
argument of the function. Let ε ≥(0, 1). Since f (x −1 + ε) is matrix monotone
on [0, 2 −ε), it follows that x f (x −1 + ε) is matrix convex on the same interval
[0, 2 −ε). So (x + 1 −ε) f (x) is matrix convex on (−1 + ε, 1). By letting ε ↑0,
(x + 1) f (x) is matrix convex on (−1, 1).
We repeat the same argument with the matrix monotone function −f (−x) and
get the matrix convexity of (x −1) f (x). Since
(x + α) f (x) = 1 + α
2
(x + 1) f (x) + 1 −α
2
(x −1) f (x),
this function is matrix convex as well.

4.4 Löwner’s Theorem
165
(2) (x +α) f (x) is already known to be matrix convex, so its division by x is also
matrix monotone.
(3) To prove this, we use the continuous differentiability of matrix monotone
functions. Then, by (2),
	
1 + 1
x

f (x) as well as f (x) is C1 on (−1, 1) so that the
function h on (−1, 1) deﬁned by h(x) := f (x)/x for x √= 0 and h(0) := f ⊥(0) is
C1. This implies that
h⊥(x) = f ⊥(x)x −f (x)
x2
−→h⊥(0) as x →0.
Therefore,
f ⊥(x)x = f (x) + h⊥(0)x2 + o(|x|2)
so that
f ⊥(x) = h(x) + h⊥(0)x + o(|x|) = h(0) + 2h⊥(0)x + o(|x|) as x →0,
which shows that f is twice differentiable at 0 with f ⊥⊥(0) = 2h⊥(0). Hence
f ⊥⊥(0)
2
= h⊥(0) = lim
x→0
h(x) −h(0)
x
= lim
x→0
f (x) −f ⊥(0)x
x2
and the proof is complete.
□
Lemma 4.35 If f ≥K, then
x
1 + x ≤f (x) for x ≥(−1, 0),
f (x) ≤
x
1 −x
for x ≥(0, 1),
and | f ⊥⊥(0)| ≤2.
Proof: For every x ≥(−1, 1), Theorem 4.5 implies that
⎦f [1](x, x) f [1](x, 0)
f [1](x, 0) f [1](0, 0)

=
⎦f ⊥(x)
f (x)/x
f (x)/x
1

⊂0,
and hence
f (x)2
x2
≤f ⊥(x).
(4.15)
By Lemma 4.34(1),
d
dx (x ± 1) f (x) = f (x) + (x ± 1) f ⊥(x)
is increasing on (−1, 1). Since f (0) ± f ⊥(0) = ±1, we have

166
4
Matrix Monotone Functions and Convexity
f (x) + (x −1) f ⊥(x) ⊂−1 for
0 < x < 1,
(4.16)
f (x) + (x + 1) f ⊥(x) ≤1
for −1 < x < 0.
(4.17)
By (4.15) and (4.16) we have
f (x) + 1 ⊂(1 −x) f (x)2
x2
.
If f (x) >
x
1−x for some x ≥(0, 1), then
f (x) + 1 > (1 −x) f (x)
x2
·
x
1 −x = f (x)
x
so that f (x) <
x
1−x , a contradiction. Hence f (x) ≤
x
1−x for all x ≥[0, 1). A similar
argument using (4.15) and (4.17) yields that f (x) ⊂
x
1+x for all x ≥(−1, 0].
Moreover, by Lemma 4.34(3) and the two inequalities just proved,
f ⊥⊥(0)
2
≤lim
x↑0
x
1−x −x
x2
= lim
x↑0
1
1 −x = 1
and
f ⊥⊥(0)
2
⊂lim
x↓0
x
1+x −x
x2
= lim
x↑0
−1
1 + x = −1
so that | f ⊥⊥(0)| ≤2.
□
Lemma 4.36 The set K is convex and compact if it is considered as a subset of
the topological vector space consisting of real functions on (−1, 1) with the locally
convex topology of pointwise convergence.
Proof: It is obvious that K is convex. Since { f (x) : f ≥K} is bounded for
each x ≥(−1, 1) thanks to Lemma 4.35, it follows that K is relatively compact.
To prove that K is closed, let { fi} be a net in K converging to a function f on
(−1, 1). Then it is clear that f is matrix monotone on (−1, 1) and f (0) = 0. By
Lemma 4.34(2),
	
1 + 1
x

fi(x) is matrix monotone on (−1, 1) for every i. Since
limx→0
	
1 + 1
x

fi(x) = f ⊥
i (0) = 1, we thus have
⎣
1 −1
x
⎤
fi(−x) ≤1 ≤
⎣
1 + 1
x
⎤
fi(x),
x ≥(0, 1).
Therefore,
⎣
1 −1
x
⎤
f (−x) ≤1 ≤
⎣
1 + 1
x
⎤
f (x),
x ≥(0, 1).

4.4 Löwner’s Theorem
167
Since f is C1 on (−1, 1), the above inequalities yield f ⊥(0) = 1.
□
Lemma 4.37 The extreme points of K have the form
f (x) =
x
1 −λx ,
where λ = f ⊥⊥(0)
2
.
Proof: Let f be an extreme point of K. For each α ≥(−1, 1) deﬁne
gα(x) :=
⎣
1 + α
x
⎤
f (x) −α,
x ≥(−1, 1).
By Lemma 4.34(2), gα is matrix monotone on (−1, 1). Notice that
gα(0) = f (0) + α f ⊥(0) −α = 0
and
g⊥
α(0) = lim
x→0
	
1 + α
x

f (x) −α
x
= f ⊥(0) + α lim
x→0
f (x) −f ⊥(0)x
x2
= 1 + 1
2α f ⊥⊥(0)
by Lemma 4.34(3). Since 1 + 1
2α f ⊥⊥(0) > 0 by Lemma 4.35, the function
hα(x) :=
	
1 + α
x

f (x) −α
1 + 1
2α f ⊥⊥(0)
is in K. Since
f = 1
2
⎣
1 + 1
2α f ⊥⊥(0)
⎤
hα + 1
2
⎣
1 −1
2α f ⊥⊥(0)
⎤
h−α,
the extremality of f implies that f = hα so that
⎣
1 + 1
2α f ⊥⊥(0)
⎤
f (x) =
⎣
1 + α
x
⎤
f (x) −α
for all α ≥(−1, 1). This immediately implies that f (x) = x/
	
1 −1
2 f ⊥⊥(0)x

.
□
Theorem 4.38 Let f be a matrix monotone function on (−1, 1). Then there exists
a probability Borel measure μ on [−1, 1] such that
f (x) = f (0) + f ⊥(0)
⎡1
−1
x
1 −λx dμ(λ),
x ≥(−1, 1).
(4.18)

168
4
Matrix Monotone Functions and Convexity
Proof: The essential case is f ≥K. Let φλ(x) := x/(1 −λx) for λ ≥[−1, 1].
By Lemmas 4.36 and 4.37, the Krein–Milman theorem says that K is the closed
convex hull of {φλ : λ ≥[−1, 1]}. Hence there exists a net { fi} in the convex hull of
{φλ : λ ≥[−1, 1]} such that fi(x) →f (x) for all x ≥(−1, 1). Each fi is written
as fi(x) =
% 1
−1 φλ(x) dμi(λ) with a probability measure μi on [−1, 1] with ﬁnite
support. Note that the set M1([−1, 1]) of probability Borel measures on [−1, 1]
is compact in the weak* topology when considered as a subset of the dual Banach
space of C([−1, 1]). Taking a subnet we may assume that μi converges in the weak*
topology to some μ ≥M1([−1, 1]). For each x ≥(−1, 1), since φλ(x) is continuous
in λ ≥[−1, 1], we have
f (x) = lim
i
fi(x) = lim
i
⎡1
−1
φλ(x) dμi(λ) =
⎡1
−1
φλ(x) dμ(λ).
To prove the uniqueness of the representing measure μ, let μ1, μ2 be probability
Borel measures on [−1, 1] such that
f (x) =
⎡1
−1
φλ(x) dμ1(λ) =
⎡1
−1
φλ(x) dμ2(λ),
x ≥(−1, 1).
Since φλ(x) = ⎥∞
k=0 xk+1λk is uniformly convergent in λ ≥[−1, 1] for any x ≥
(−1, 1) ﬁxed, it follows that
∞
⎢
k=0
xk+1
⎡1
−1
λk dμ1(λ) =
∞
⎢
k=0
xk+1
⎡1
−1
λk dμ2(λ),
x ≥(−1, 1).
Hence
% 1
−1 λk dμ1(λ) =
% 1
−1 λk dμ2(λ) for all k = 0, 1, 2, . . ., which implies that
μ1 = μ2.
□
The integral representation appearing in the above theorem, which we proved
directly, is a special case of Choquet’s theorem. The uniqueness of the representing
measure μ shows that {φλ : λ ≥[−1, 1]} is actually the set of extreme points of K.
Since the pointwise convergence topology on {φλ : λ ≥[−1, 1]} agrees with the
usual topology on [−1, 1], we see that K is a so-called Bauer simplex.
Theorem 4.39 (Löwner’s theorem) Let −∞≤a < b ≤∞and f be a real-valued
function on (a, b). Then f is matrix monotone on (a, b) if and only if f ≥P(a, b).
Hence, a matrix monotone function is analytic.
Proof: The “if” part was shown after Theorem 4.31. To prove the “only if” part,
it is enough to assume that (a, b) is a ﬁnite open interval. Moreover, when (a, b) is
a ﬁnite interval, by transforming f into a matrix monotone function on (−1, 1) via
a linear function, it sufﬁces to prove the “only if” part when (a, b) = (−1, 1). If f
is a non-constant matrix monotone function on (−1, 1), then by using the integral
representation (4.18) one can deﬁne an analytic continuation of f by

4.4 Löwner’s Theorem
169
f (z) = f (0) + f ⊥(0)
⎡1
−1
z
1 −λz dμ(λ),
z ≥C+.
Since
Im f (z) = f ⊥(0)
⎡1
−1
Im z
|1 −λz|2 dμ(λ),
it follows that f maps C+ into itself. Hence f ≥P(−1, 1).
□
Theorem 4.40 Let f be a non-linear matrix convex function on (−1, 1). Then there
exists a unique probability Borel measure μ on [−1, 1] such that
f (x) = f (0) + f ⊥(0)x + f ⊥⊥(0)
2
⎡1
−1
x2
1 −λx dμ(λ),
x ≥(−1, 1).
Proof: To prove this statement, we use the result due to Kraus that if f is a matrix
convex function on (a, b), then f is C2 and f [1][x, α] is matrix monotone on (a, b)
for every α ≥(a, b). Then we may assume that f (0) = f ⊥(0) = 0 by considering
f (x) −f (0) −f ⊥(0)x. Since g(x) := f [1][x, 0] = f (x)/x is a non-constant matrix
monotone function on (−1, 1), by Theorem 4.38 there exists a probability Borel
measure μ on [−1, 1] such that
g(x) = g⊥(0)
⎡1
−1
x
1 −λx dμ(λ),
x ≥(−1, 1).
Since g⊥(0) = f ⊥⊥(0)/2 is easily seen, we have
f (x) = f ⊥⊥(0)
2
⎡1
−1
x2
1 −λx dμ(λ),
x ≥(−1, 1).
Moreover, the uniqueness of μ follows from that of the representing measure for g. □
Theorem 4.41 Let f be a continuous matrix monotone function on [0, ∞). Then
there exists a positive measure μ on (0, ∞) and β ⊂0 such that
f (x) = f (0) + βx +
⎡∞
0
λx
x + λ dμ(λ),
x ≥[0, ∞),
(4.19)
where
⎡∞
0
λ
1 + λ dμ(λ) < +∞.
Proof: Consider a function ψ : (−1, 1) →(0, ∞) deﬁned by
ψ(x) := 1 + x
1 −x = −1 +
2
1 −x ,

170
4
Matrix Monotone Functions and Convexity
which is matrix monotone. Let f be a continuous matrix monotone function on R+.
Since g(x) := f (ψ(x)) is matrix monotone on (−1, 1), by Theorem 4.38 there exists
a probability measure ν on [−1, 1] such that
g(x) = g(0) + g⊥(0)
⎡
[−1,1]
x
1 −λx dν(λ)
for every x ≥(−1, 1). We may assume g⊥(0) > 0 since otherwise g and hence f are
constant functions. Since g(−1) = limx↑0 g(x) = f (0) > −∞, we have
⎡
[−1,1]
1
1 + λ dν(λ) < +∞
and in particular ν({−1}) = 0. Therefore,
g(x) −g(−1) = g⊥(0)
⎡
(−1,1]
1 + x
(1 −λx)(1 + λ) dμ(λ) =
⎡
(−1,1]
1 + x
1 −λx d ˜μ(λ),
where d ˜μ(λ) := g⊥(0)(1 + λ)−1 dμ(λ). Deﬁne a ﬁnite measure m on (0, ∞) by
m := ˜μ ⊃ψ−1. Transform the above integral expression by x = ψ−1(t) to obtain
f (t) −f (0) = t ˜μ({1}) +
⎡
(0,∞)
1 + ψ−1(t)
1 −ψ−1(ζ)ψ−1(t) dm(ζ)
= βt +
⎡
(0,∞)
t(1 + ζ)
t + ζ
dm(ζ),
where β := ˜μ({1}). With the measure dμ(ζ) := ((1 + ζ)/ζ) dm(ζ) we have the
desired integral expression of f .
□
Since the integrand
x
x + λ = 1 −
λ
x + λ
is a matrix monotone function of x (see Example 4.1), it is obvious that a function
on [0, ∞) admitting the integral expression (4.19) is matrix monotone. The theorem
shows that a matrix monotone function on [0, ∞) is matrix concave.
Theorem 4.42 If f : R+ →R is matrix monotone, then x f (x) is matrix convex.
Proof: Let λ > 0. First we consider the function f (x) = −(x + λ)−1. Then
x f (x) = −
x
λ + x = −1 +
λ
λ + x
and it is well-known that x ≡→(x + λ)−1 is matrix convex.

4.4 Löwner’s Theorem
171
For a general matrix monotone f , we use the integral decomposition (4.19) and
the statement follows from the previous special case.
□
Theorem 4.43 If f : (0, ∞) →(0, ∞), then the following conditions are equiva-
lent:
(1)
f is matrix monotone;
(2) x/f (x) is matrix monotone;
(3)
f is matrix concave.
Proof: For ε > 0 the function fε(x) := f (x + ε) is deﬁned on [0, ∞). If the
statement is proved for this function, then the limit ε →0 gives the result. So we
assume f : [0, ∞) →(0, ∞).
The implication (1) ⇒(3) has already been observed above.
The implication (3) ⇒(2) is based on Example 4.24. It says that −f (x)/x is
matrix monotone. Therefore x/f (x) is matrix monotone as well.
(2) ⇒(1): Assume that x/f (x) is matrix monotone on (0, ∞). Let α :=
limx↑0 x/f (x). Then it follows from the Löwner representation that, dividing by
x, we have
1
f (x) = α
x + β +
⎡∞
0
λ
λ + x dμ(λ).
This multiplied by −1 yields the matrix monotone −1/f (x). Therefore f (x) is
matrix monotone as well.
□
We have shown that the matrix monotonicity is equivalent to the positive deﬁ-
niteness of the divided difference kernel. Matrix concavity has a somewhat similar
property.
Theorem 4.44 Let f : R+ →R+ be a smooth function. If the divided difference
kernel function is conditionally negative deﬁnite, then f is matrix convex.
Proof: By continuity it sufﬁces to prove that the function f (x)+ε is matrix convex
on R+ for any ε > 0. So we may assume that f > 0. Example 2.42 and Theorem
4.5 give that g(x) = x2/f (x) is matrix monotone. Then x/g(x) = f (x)/x is matrix
monotone due to Theorem 4.43. Multiplying by x we have a matrix convex function
by Theorem 4.42.
□
It is not always easy to determine if a function is matrix monotone. An efﬁcient
method is based on Theorem 4.39. The theorem says that a function R+ →R is
matrix monotone if and only if it has a holomorphic extension to the upper half-plane
C+ such that its range is in the closure of C+. It is remarkable that matrix monotone
functions are very smooth and connected with functions of a complex variable.
Example 4.45 The representation
xt = sin πt
π
⎡∞
0
λt−1x
λ + x dλ

172
4
Matrix Monotone Functions and Convexity
shows that f (x) = xt is matrix monotone on R+ when 0 < t < 1. In other words,
0 ≤A ≤B
implies
At ≤Bt,
which is often called the Löwner–Heinz inequality.
We can arrive at the same conclusion by holomorphic extension as a Pick function
(see Examples 4.29 and 4.33) so that f (x) = xt is matrix monotone on R+ for these
values of the parameter but not for any other value. Another familiar example of a
matrix monotone function is log x on (0, ∞), see Examples 4.29 and 4.32.
□
4.5 Some Applications
If the complex extension of a function f : [0, ∞) →R is rather natural, then
it can be checked numerically that the upper half-plane is mapped into itself, and
the function is expected to be matrix monotone. For example, x ≡→x p has a natural
complexextension.Inthefollowingwegiveafewmoreexamplesofmatrixmonotone
functions on R+.
Theorem 4.46 Let
f p(x) :=
⎣p(x −1)
x p −1
⎤
1
1−p
(x > 0).
In particular, f2(x) = (x + 1)/2, f−1(x) = ∪x and
f1(x) := lim
p→1 f p(x) = e−1x
x
x−1 ,
f0(x) := lim
p→0 f p(x) = x −1
log x .
Then f p is matrix monotone if −2 ≤p ≤2.
Proof: Since the functions are continuous in the parameter p and the matrix
monotone functions are closed under pointwise convergence, it is enough to prove
the result for p ≥[−2, 2] such that p √= −2, −1, 0, 1, 2. By Löwner’s theorem
(Theorem 4.39) it sufﬁces to show that f p has a holomorphic continuation to C+
mapping into itself. We deﬁne log z with log 1 = 0; then in case −2 < p < 2, the
real function p(x −1)/(x p −1) has a holomorphic continuation p(z −1)/(z p −1) to
C+ since z p −1 √= 0 in C+. Moreover, it is continuous in the closed upper half-plane
Im z ⊂0. Further, since p(z −1)/(z p −1) √= 0 (z √= 1), f p has a holomorphic
continuation (denoted by the same f p) to C+ and it is also continuous in Im z ⊂0.
Assume p ≥(0, 1) ∞(1, 2). For R > 0 let K R := {z : |z| ≤R, Im z ⊂0},
σR := {z : |z| = R, Im z > 0} and K ⊃
R be the interior of K R; then the boundary of
K R is σR ∞[−R, R]. Note that f p(K R) is a compact set. Recall the well-known fact
that the image of a connected open set under a holomorphic function is a connected

4.5 Some Applications
173
open set, if it is not a single point. This yields that f p(K ⊃
R) is open and hence the
boundary of f p(K R) is included in f p(σR ∞[−R, R]). Below let us prove that for
any sufﬁciently small ε > 0, if R is sufﬁciently large (depending on ε) then
f p(σR ∞[−R, R]) ∈{z : −ε ≤arg z ≤π + ε},
which yields that f p(K R) ∈{z : −ε ≤arg z ≤π + ε}. Thus, letting R ↓∞(so
ε ↑0) we conclude that f p(C+) ∈{z : Im z ⊂0}.
Clearly, [0, ∞) is mapped into [0, ∞) by f p. If z ≥(−∞, 0), then arg(z −1) =
π and pπ ≤arg(z p −1) ≤π for 0 < p < 1 and π ≤arg(z p −1) ≤pπ for
1 < p < 2. Hence
0 ≤arg z −1
z p −1 ≤(1 −p)π
for 0 < p < 1,
(1 −p)π ≤arg z −1
z p −1 ≤0 for 1 < p < 2.
Thus, since
arg
⎣z −1
z p −1
⎤
1
1−p
=
1
1 −p arg z −1
z p −1,
it follows that 0 ≤arg f p(z) ≤π, so (−∞, 0) is mapped into Im z ⊂0.
Next, for any small ε > 0, if R is sufﬁciently large, then we have for every
z ≥σR
| arg(z −1) −arg z| ≤ε,
| arg(z p −1) −p log z| ≤ε
so that
 arg z −1
z p −1 −(1 −p) arg z
 ≤2ε.
Since
 arg
⎣z −1
z p −1
⎤
1
1−p
−arg z
 =

1
1 −p
⎣
arg z −1
z p −1 −(1 −p) arg z
⎤ ≤
2ε
|1 −p|,
we have f p(σR) ∈{z : −2ε/|1 −p| ≤arg z ≤π + 2ε/|1 −p|}. Thus, the desired
assertion follows.
The case −2 < p < 0 can be treated similarly by noting that
f p(x) =
⎣|p|x|p|(x −1)
x|p| −1
⎤
1
1+|p|
.
□

174
4
Matrix Monotone Functions and Convexity
Theorem 4.47 The function
f p(x) =
⎣x p + 1
2
⎤1
p
is matrix monotone if and only if −1 ≤p ≤1.
Proof: Observe that f−1(x) = 2x/(x +1) and f1(x) = (x +1)/2, so f p could be
matrix monotone only if −1 ≤p ≤1. We show that it is indeed matrix monotone.
The case p = 0 is well-known. Further, note that if f p is matrix monotone for
0 < p < 1 then
f−p(x) =


⎣x−p + 1
2
⎤1
p


−1
is also matrix monotone since x−p is matrix monotone decreasing for 0 < p ≤1.
So let us assume that 0 < p < 1. Then, since z p + 1 √= 0 in the upper half
plane, f p has a holomorphic continuation to the upper half plane (by deﬁning log z
as log 1 = 0). By Löwner’s theorem it sufﬁces to show that f p maps the upper half
plane into itself. If 0 < arg z < π then 0 < arg(z p + 1) < arg z p = p arg z so that
0 < arg
⎣z p + 1
2
⎤1
p
= 1
p arg
⎣z p + 1
2
⎤
< arg z < π.
Thus z is mapped into the upper half plane.
□
In the special case p = 1
n ,
f p(x) =

x
1
n + 1
2
n
= 1
2n
n
⎢
k=0
⎣n
k
⎤
x
k
n ,
and it is well-known that xα is matrix monotone for 0 < α < 1 thus f p is also matrix
monotone.
The matrix monotone functions in the next theorem play an important role in
some applications.
Theorem 4.48 For −1 ≤p ≤2 the function
f p(x) = p(1 −p)
(x −1)2
(x p −1)(x1−p −1)
(4.20)
is matrix monotone.
Proof: The special cases p = −1, 0, 1, 2 are well-known. For 0 < p < 1 we can
use an integral representation

4.5 Some Applications
175
1
f p(x) = sin pπ
π
⎡∞
0
dλ λp−1
⎡1
0
ds
⎡1
0
dt
1
x((1 −t)λ + (1 −s)) + (tλ + s)
and since the integrand is matrix monotone decreasing as a function of x, 1/f p is
matrix monotone decreasing as well. It follows that f p(x) is matrix monotone for
0 < p < 1.
The proof below is based on Löwner’s theorem. Since f p = f1−p, we may assume
p ≥(0, 1) ∞(1, 2). It sufﬁces to show that f p has a holomorphic continuation to C+
mapping into itself. It is clear that [0, ∞) is mapped into [0, ∞) by f p. First, when
0 < p < 1, f p has a holomorphic continuation to C+ in the form
f p(z) = p(1 −p)
(z −1)2
(z p −1)(z1−p −1).
For z ≥(−∞, 0), since
pπ ≤arg(z p −1) ≤π,
(1 −p)π ≤arg(z1−p −1) ≤π
and
arg f p(z) = −arg(z p −1) −arg(z1−p −1),
we have −2π ≤arg f p(z) ≤−π so that Im f p(z) ⊂0. Let K R and σR be as in the
proof of Theorem 4.46. For every ε > 0, if z ≥σR with a sufﬁciently large R > 0
(depending on ε), then we have
| arg(z −1)2 −2 arg z| ≤ε,
| arg(z p −1) −p arg z| ≤ε,
| arg(z1−p −1) −(1 −p) arg z| ≤ε,
so that
| arg f p(z) −arg z| = | arg(z −1)2 −arg(z p −1) −arg(z1−p −1) −arg z| ≤3ε,
which yields that f p(σR) ∞[−R, R]) ∈{z : −3ε ≤arg z ≤3ε}. Thus, letting
R ↓∞(so ε ↑0) we have f p(C+) ∈{z : Im z ⊂0} as in the proof of Theorem
4.46.
Next, when 1 < p < 2, f p has a holomorphic continuation to C+ in the form
f p(z) = p(p −1)
z p−1(z −1)2
(z p −1)(z p−1 −1).
For every ε > 0, if z ≥σR with a sufﬁciently large R, then
| arg f p(z) −(2 −p) log z| ≤3ε

176
4
Matrix Monotone Functions and Convexity
as above. The assertion follows similarly.
□
Theorem 4.49 If f : R+ →R is a matrix monotone function and A, B ⊂0, then
2Af (A) + 2B f (B) ⊂
∪
A + B

f (A) + f (B)
∪
A + B.
This proof is left for Exercise 8.
For a function f : (a, b) →R the notion of strong subadditivity is introduced
via the inequality (4.3). Recall that the condition for f is
Tr f (A) + Tr f (A22) ≤Tr f (B) + Tr f (C)
for every matrix A = A∼with σ(A) ∈(a, b) in the form of a 3 × 3 block matrix
A =

⎛
A11 A12 A13
A∼
12 A22 A23
A∼
13 A∼
23 A33
⎝
⎠
and
B =
⎦A11 A12
A∼
12 A22

,
C =
⎦A22 A23
A∼
23 A33

.
The next theorem tells us that f (x) = log x on (0, ∞) is a strong subadditive
function, since log det A = Tr log A for a positive deﬁnite matrix A.
Theorem 4.50 Let
S =

⎛
S11 S12 S13
S∼
12 S22 S23
S∼
13 S∼
23 S33
⎝
⎠
be a positive deﬁnite block matrix. Then
det S · det S22 ≤det
⎦S11 S12
S∼
12 S22

· det
⎦S22 S23
S∼
23 S33

and the condition for equality is S13 = S12S−1
22 S23.
Proof: Take the ortho-projections
P =

⎛
I 0 0
0 0 0
0 0 0
⎝
⎠
and
Q =

⎛
I 0 0
0 I 0
0 0 0
⎝
⎠.
Since P ≤Q, we have the matrix inequality
[P]S ≤[P]QSQ,

4.5 Some Applications
177
which implies the determinant inequality
det [P]S ≤det [P]QSQ .
According to the Schur determinant formula (Theorem 2.3), this is exactly the de-
terminant inequality of the theorem.
The equality in the determinant inequality implies [P]S = [P]QSQ which is
S11 −
&S12, S13
' ⎦S22 S23
S32 S33
−1 ⎦S21
S31

= S11 −S12S−1
22 S21.
This can be written as
&S12, S13
'
⎦S22 S23
S32 S33
−1
−
⎦
S−1
22 0
0
0
 ⎦S21
S31

= 0 .
(4.21)
For the moment, let
⎦S22 S23
S32 S33
−1
=
⎦C22 C23
C32 C33

.
Then
⎦S22 S23
S32 S33
−1
−
⎦
S−1
22 0
0
0

=
⎦
C23C−1
33 C32 C23
C32
C33

=

C23C−1/2
33
C1/2
33
 (
C−1/2
33
C32 C1/2
33
)
.
Comparing this with (4.21) we arrive at
&S12, S13
'

C23C−1/2
33
C1/2
33

= S12C23C−1/2
33
+ S13C1/2
33 = 0.
Equivalently,
S12C23C−1
33 + S13 = 0.
Since the concrete form of C23 and C33 is known, we can compute that C23C−1
33 =
−S−1
22 S23 and this gives the condition stated in the theorem.
□
The next theorem gives a sufﬁcient condition for the strong subadditivity (4.3) of
functions on (0, ∞).
Theorem 4.51 Let f : (0, ∞) →R be a function such that −f ⊥is matrix monotone.
Then the inequality (4.3) holds.

178
4
Matrix Monotone Functions and Convexity
Proof: A matrix monotone function has the representation
a + bx +
⎡∞
0
⎣
λ
λ2 + 1 −
1
λ + x
⎤
dμ(λ),
where b ⊂0, see (V.49) in [20]. Therefore, we have the representation
f (t) = c −
⎡t
1
⎣
a + bx +
⎡∞
0
⎣
λ
λ2 + 1 −
1
λ + x
⎤
dμ(λ)
⎤
dx.
By integration we have
f (t) = d −at −b
2t2 +
⎡∞
0
⎣
λ
λ2 + 1(1 −t) + log

λ
λ + 1 +
t
λ + 1
⎤
dμ(λ).
Theﬁrst quadraticpart satisﬁes strongsubadditivityandwehavetochecktheintegral.
Since log x is a strongly subadditive function by Theorem 4.50, so is the integrand.
The property is retained by integration.
□
Example 4.52 By differentiation we can see that f (x) = −(x + t) log(x + t)
with t ⊂0 satisﬁes strong subadditivity. Similarly, f (x) = −xt satisﬁes strong
subadditivity if 1 ≤t ≤2.
In some applications the matrix monotone functions
f p(x) = p(1 −p)
(x −1)2
(x p −1)(x1−p −1)
(0 < p < 1)
appear.
For p = 1/2 this is a strongly subadditivity function. Up to a constant factor, the
function is
(∪x + 1)2 = x + 2∪x + 1
and all terms are known to be strongly subadditive. The function −f ⊥
1/2 is evidently
matrix monotone.
Numerical computation indicates that −f ⊥
p seems to be matrix monotone, but no
proof is known.
□
For K, L ⊂0 and a matrix monotone function f , there is a very particular relation
between f (K) and f (L). This is described in the next theorem.
Theorem 4.53 Let f : R+ →R be a matrix monotone function. For positive
matrices K and L, let P be the projection onto the range of (K −L)+. Then
Tr PL( f (K) −f (L)) ⊂0.
Proof: From the integral representation

4.5 Some Applications
179
f (x) =
⎡∞
0
x(1 + s)
x + s
dμ(s)
we have
Tr PL( f (K) −f (L)) =
⎡∞
0
(1 + s)sTr PL(K + s)−1(K −L)(L + s)−1 dμ(s).
Hence it is sufﬁcient to prove that
Tr PL(K + s)−1(K −L)(L + s)−1 ⊂0
for s > 0. Let α0 := K −L and consider the integral representation
(K + s)−1α0(L + s)−1 =
⎡1
0
s(L + tα0 + s)−1α0(L + tα0 + s)−1 dt.
So we can make another reduction: it sufﬁces to show that
Tr PL(L + tα0 + s)−1tα0(L + tα0 + s)−1 ⊂0.
If C := L + tα0 and α := tα0, then L = C −α and we have
Tr P(C −α)(C + s)−1α(C + s)−1 ⊂0.
(4.22)
We write our operators in the form of 2 × 2 block matrices:
V = (C + s)−1 =
⎦V1 V2
V ∼
2 V3

,
P =
⎦I 0
0 0

,
α =
⎦α+
0
0 −α−

.
The left-hand side of the inequality (4.22) can then be rewritten as
Tr P(C −α)(V αV ) = Tr [(C −α)(V αV )]11
= Tr [(V −1 −α −s)(V αV )]11
= Tr [αV −(α + s)(V αV )]11
= Tr (α+V11 −(α+ + s)(V αV )11)
= Tr (α+(V −V αV )11 −s(V αV )11).
(4.23)
Because of the positivity of L, we have V −1 ⊂α+s, which implies V = V V −1V ⊂
V (α + s)V = V αV + sV 2. As the diagonal blocks of a positive operator are
themselves positive, this further implies
V1 −(V αV )11 ⊂s(V 2)11.

180
4
Matrix Monotone Functions and Convexity
Inserting this in (4.23) gives
Tr [(V −1 −α −s)(V αV )]11 = Tr (α+(V −V αV )11 −s(V αV )11)
⊂Tr (α+s(V 2)11 −s(V αV )11)
= sTr (α+(V 2)11 −(V αV )11)
= sTr (α+(V1V1 + V2V ∼
2 ) −(V1α+V1
−V2α−V ∼
2 ))
= sTr (α+V2V ∼
2 + V2α−V ∼
2 ).
This quantity is positive.
□
Theorem 4.54 Let A and B be positive operators, then for all 0 ≤s ≤1,
2Tr As B1−s ⊂Tr (A + B −|A −B|).
(4.24)
Proof: For a self-adjoint operator X, X± denotes its positive and negative parts.
Decomposing A −B = (A −B)+ −(A −B)−one gets
Tr A + Tr B −Tr |A −B| = 2Tr A −2Tr (A −B)+,
and (4.24) is equivalent to
Tr A −Tr Bs A1−s ≤Tr (A −B)+.
From
A ≤A + (A −B)−= B + (A −B)+
and B ≤B + (A −B)+ as well as the matrix monotonicity of the function x ≡→xs,
we have
Tr A −Tr Bs A1−s = Tr (As −Bs)A1−s ≤Tr ((B + (A −B)+)s −Bs)A1−s
≤Tr ((B + (A −B)+)s −Bs)(B + (A −B)+)1−s
= Tr B + Tr (A −B)+ −Tr Bs(B + (A −B)+)1−s
≤Tr B + Tr (A −B)+ −Tr Bs B1−s
= Tr (A −B)+
and the statement is obtained.
□
The following result is Lieb’s extension of the Golden–Thompson inequality.
Theorem 4.55 (Golden–Thompson–Lieb) Let A, B and C be self-adjoint matri-
ces. Then
Tr eA+B+C ≤
⎡∞
0
Tr eA(t + e−C)−1eB(t + e−C)−1 dt .

4.5 Some Applications
181
Proof: Another formulation of the statement is
Tr eA+B−log D ≤Tr eA J−1
D (eB),
where
J−1
D K =
⎡∞
0
(t + D)−1K(t + D)−1 dt
(which is the formulation of (3.20)). We choose L = −log D + A, β = eB and
conclude from (4.4) that the functional
F : β ≡→−Tr eL+log β
is convex on the cone of positive deﬁnite matrices. It is also homogeneous of order
1 so that the hypothesis of Lemma 4.56 (from below) is fulﬁlled. So
−Tr eA+B−log D = −Tr exp(L + log β) = F(β)
⊂−d
dx Tr exp(L + log(D + xβ))

x=0
= −Tr eA J−1
D (β) = −Tr eA J−1
D (eB).
This is the statement with a minus sign.
□
Lemma 4.56 Let C be a convex cone in a vector space and F : C →R be a convex
function such that F(λA) = λF(A) for every λ > 0 and A ≥C. If B ≥C and the
limit
lim
x↑0
F(A + x B) −F(A)
x
=: ∂B F(A)
exists, then
F(B) ⊂∂B F(A) .
If equality holds here, then F(A+x B) = (1−x)F(A)+x F(A+ B) for 0 ≤x ≤1.
Proof: Deﬁne a function f : [0, 1] →R by f (x) := F(A + x B). This function
is convex:
f (λx1 + (1 −λ)x2) = F(λ(A + x1B) + (1 −λ)(A + x2B))
≤λF(A + x1B) + (1 −λ)F(A + x2B))
= λ f (x1) + (1 −λ) f (x2) .
The assumption is the existence of the derivative f ⊥(0) (from the right). From the
convexity

182
4
Matrix Monotone Functions and Convexity
F(A + B) = f (1) ⊂f (0) + f ⊥(0) = F(A) + ∂B F(A).
Actually, F is subadditive:
F(A + B) = 2F(A/2 + B/2) ≤F(A) + F(B),
and the stated inequality follows.
If f ⊥(0) + f (0) = f (1), then f (x) −f (0) is linear. (This also has the alternative
description: f ⊥⊥(x) = 0.)
□
If C = 0 in Theorem 4.55, then we have
Tr eA+B ≤Tr eAeB,
which is the original Golden–Thompson inequality. If BC = C B, then on the
right-hand side, the integral
⎡∞
0
(t + e−C)−2 dt
appears. This equals eC and we have Tr eA+B+C ≤Tr eAeBeC. Without the assump-
tion BC = C B, this inequality is not true.
The Golden–Thompson inequality is equivalent to a kind of monotonicity of the
relative entropy, see [73]. An example of an application of the Golden–Thompson–
Lieb inequality is the strong subadditivity of the von Neumann entropy.
4.6 Notes and Remarks
On the subject of convex analysis, R. Tyrell Rockafellar has a famous book: Convex
Analysis, Princeton, Princeton University Press, 1970.
Theorem 4.46 as well as the optimality of the range −2 ≤p ≤2 was proved in the
paper Y. Nakamura, Classes of operator monotone functions and Stieltjes functions,
in The Gohberg Anniversary Collection, Vol. II, H. Dym et al. (eds.), Oper. Theory
Adv. Appl., vol. 41, Birkhäuser, 1989, pp. 395–404. The proof here is a modiﬁcation
of that in the paper Ádám Besenyei and Dénes Petz, Completely positive mappings
and mean matrices, Linear Algebra Appl. 435(2011), 984–997. Theorem 4.47 was
given in the paper Fumio Hiai and Hideki Kosaki, Means for matrices and comparison
of their norms, Indiana Univ. Math. J. 48(1999), 899–936.
The matrix monotonicity of the function (4.20) for 0 < p < 1 was recognized in
[74]. The proof for p ≥[−1, 2] is a modiﬁcation of that in the paper V. E. Sándor
Szabó, A class of matrix monotone functions, Linear Algebra Appl. 420 (2007),
79–85. Related discussions are in [17], and there is an extension to

4.6 Notes and Remarks
183
(x −a)(x −b)
( f (x) −f (a))(x/f (x) −b/f (b))
in the paper M. Kawasaki and M. Nagisa, Transforms on operator monotone func-
tions, arXiv:1206.5452. (a = b = 1 and f (x) = x p covers (4.20).) A shorter proof
of this is in the paper F. Hansen, WYD-like skew information measures, J. Stat. Phys.
151(2013), 974–979.
The original result of Karl Löwner appeared in 1934 (when he emigrated to the
US, he changed his name to Charles Loewner). Apart from Löwner’s original proof,
there are three other different proofs, for example by Bendat and Sherman based
on the Hamburger moment problem, by Korányi based on the spectral theorem of
self-adjoint operators, and by Hansen and Pedersen based on the Krein–Milman
theorem. In all of them, the integral representation of operator monotone functions
was obtained to prove Löwner’s theorem. The proof presented here is based on [40].
The integral representation (4.6) was obtained by Julius Bendat and Seymour
Sherman [16]. Theorem 4.22 is from the famous paper of Frank Hansen and Gert
G. Pedersen [40], and Theorem 4.16 is from [41] by the same authors. Theorems 4.28
and 4.28 are from the paper of J. S. Aujla and F. C. Silva [15], which also contains
Theorem 4.16 in a stronger form of majorization (see Theorem 6.27 in Chap.6).
Theorem 4.51 is from the paper [14]. It is an interesting question if the converse
statement is true.
Theorem 4.54 is in the paper K. M. R. Audenaert et al., Discriminating states:
the quantum Chernoff bound, Phys. Rev. Lett. 98 (2007), 160501. A quantum in-
formation application is contained in the same paper and also in the book [73]. The
present proof is due to Narutaka Ozawa, which is contained in V. Jakši´c, Y. Ogata,
Y. Pautrat and C.-A. Pillet, Entropic ﬂuctuations in quantum statistical mechanics.
an introduction, in the book Quantum Theory from Small to Large Scales (École de
Physique des Houches Session XCV 2010), J. Fröhlich et al. (eds.), Oxford Univer-
sity Press, 2012. Theorem 4.54 from [73] is an extension of Theorem 4.53 and its
proof is similar to that of Audenaert et al.
4.7 Exercises
1. Prove that the function κ : R+ →R, κ(x) = −x log x + (x + 1) log(x + 1) is
matrix monotone.
2. Prove that f (x) = x2 is not matrix monotone on any positive interval.
3. Show that f (x) = ex is not matrix monotone on [0, ∞).
4. Showthatif f : R+ →Risamatrixmonotonefunction,then−f isacompletely
monotone function.
5. Let f be a differentiable function on the interval (a, b) such that for some
a < c < b the function f is matrix monotone for 2 ×2 matrices on the intervals
(a, c] and [c, b). Show that f is matrix monotone for 2 × 2 matrices on (a,b).
6. Show that the function

184
4
Matrix Monotone Functions and Convexity
f (x) = ax + b
cx + d
(a, b, c, d ≥R,
ad > bc)
is matrix monotone on any interval which does not contain −d/c.
7. Use the matrices
A =
⎦1 1
1 1

and
B =
⎦2 1
1 1

to show that f (x) =
∪
x2 + 1 is not a matrix monotone function on R+.
8. Let f : R+ →R be a matrix monotone function. Prove the inequality
Af (A) + B f (B) ≤1
2(A + B)1/2( f (A) + f (B))(A + B)1/2
for positive matrices A and B. (Hint: Use that f is matrix concave and x f (x) is
matrix convex.)
9. Show that the canonical representing measure in (5.24) for the standard matrix
monotone function f (x) = (x −1)/ log x is the measure
dμ(λ) =
2
(1 + λ)2 dλ .
10. The function
logα(x) = x1−α −1
1 −α
(x > 0,
α > 0,
α √= 1)
is called the α-logarithmic function. Is it matrix monotone?
11. Give an example of a matrix convex function such that the derivative is not matrix
monotone.
12. Show that f (z) = tan z := sin z/ cos z is in P, where cos z := (eiz + e−iz)/2
and sin z := (eiz −e−iz)/2i.
13. Show that f (z) = −1/z is in P.
14. Show that the extreme points of the set
Sn := {D ≥Msa
n
: D ⊂0 and Tr D = 1}
are the orthogonal projections of trace 1. Show that for n > 2 not all points in
the boundary are extreme.
15. Let the block matrix
M =
⎦A B
B∼C

be positive and f : R+ →R be a convex function. Show that
Tr f (M) ⊂Tr f (A) + Tr f (C).

4.7 Exercises
185
16. Show that for A, B ≥Msa
n the inequality
log Tr eA+B ⊂log Tr eA + Tr BeA
Tr eA
holds. (Hint: Use the function (4.2).)
17. Let the block matrix
M =
⎦A B
B∼C

be positive and invertible. Show that
det M ≤det A · det C.
18. Show that for A, B ≥Msa
n the inequality
| log Tr eA+B −log Tr eA| ≤∥B∥
holds. (Hint: Use the function (4.2).)
19. Is it true that the function
ηα(x) = xα −x
1 −α
(x > 0)
is matrix concave if α ≥(0, 2)?

Chapter 5
Matrix Means and Inequalities
The study of numerical means has been a popular subject for centuries, and the
inequalities
2ab
a + b ∈
→
ab ∈a + b
2
between the harmonic, geometric and arithmetic means of positive numbers are well-
known. When we move from 1×1 matrices (i.e. numbers) to n×n matrices, then the
study of the arithmetic mean does not require any additional theory. Historically it
was the harmonic mean that was the ﬁrst non-trivial matrix mean to be investigated.
From the point of view of some applications the name parallel sum, introduced in the
late 1960s, was popular rather than ‘harmonic matrix mean’. The geometric matrix
mean appeared later in 1975, and the deﬁnition is not simple.
In the period 1791 until 1828, Carl Friedrich Gauss worked on the iteration:
a0 := a,
b0 := b,
an+1 := an + bn
2
,
bn+1 :=
⎡
anbn,
whose (joint) limit is now called the Gauss arithmetic-geometric mean AG(a, b). It
has a non-trivial characterization:
1
AG(a, b) = 2
δ
⎢≤
0
dt
⎡
(a2 + t2)(b2 + t2)
.
In this chapter, we will ﬁrst generalize the geometric mean to positive matrices
and several other means will be studied in terms of matrix (or operator) monotone
functions. There is also a natural (limit) deﬁnition for the mean of several (i.e. more
than two) matrices, but its explicit description is rather hopeless.
F. Hiai and D. Petz, Introduction to Matrix Analysis and Applications,
187
Universitext, DOI: 10.1007/978-3-319-04150-6_5,
© Hindustan Book Agency 2014

188
5
Matrix Means and Inequalities
5.1 The Geometric Mean
The deﬁnition of the geometric mean will be motivated by a property of geodesics
on Riemannian manifolds.
The positive deﬁnite matrices can be considered as the variance of multivariate
normal distributions and the information geometry of Gaussians yields a natural
Riemannian metric. Those distributions (with 0 expectation) are given by a positive
deﬁnite matrix A ∈Mn in the form
fA(x) :=
1
(2δ)n det A exp
⎣
−∗A−1x, x⟩/2
⎤
(x ∈Cn).
The set Pn of positive deﬁnite n × n matrices can be considered as an open subset
of the Euclidean space Rn2 and they form a manifold. The tangent vectors at a foot
point A ∈Pn are the self-adjoint matrices Msa
n .
A standard way to construct an information geometry is to start with an informa-
tion potential function and to introduce the Riemannian metric via the Hessian of
the potential. The information potential is the Boltzmann entropy
S( fA) := −
⎢
f A(x) log fA(x) dx = C + Tr log A
(C is a constant).
The Hessian is
λ2
λsλt S( fA+t H1+sH2)
⎥⎥⎥
t=s=0 = Tr A−1H1A−1H2
and the inner product on the tangent space at A is
gA(H1, H2) = Tr A−1H1A−1H2 .
We note here that this geometry has many symmetries, each congruence transfor-
mation of the matrices becomes a symmetry. Namely for any invertible matrix S,
gSAS∗(SH1S∗, SH2S∗) = gA(H1, H2).
(5.1)
A C1 differentiable function π : [0, 1] ≥Pn is called a curve, its tangent vector
at t is π∪(t) and the length of the curve is
⎢1
0
⎦
gπ(t)(π∪(t), π∪(t)) dt.
Given A, B ∈Pn the curve
π(t) = A1/2(A−1/2B A−1/2)t A1/2
(0 ∈t ∈1)
(5.2)

5.1 The Geometric Mean
189
connects these two points: π(0) = A, π(1) = B. The next lemma says that this is
the shortest curve connecting the two points, and is called the geodesic connecting
A and B.
Lemma 5.1 The geodesic connecting A, B ∈Pn is (5.2) and the geodesic dis-
tance is
σ(A, B) = ⊥log(A−1/2B A−1/2)⊥2 ,
where ⊥· ⊥2 denotes the Hilbert–Schmidt norm.
Proof : By property (5.1) we may assume that A = I, then π(t) = Bt. Let α(t)
be a curve in Msa
n such that α(0) = α(1) = 0. This will be used for the perturbation
of the curve π(t) in the form π(t) + φα(t).
We want to differentiate the length
⎢1
0
⎦
gπ(t) + φα(t)(π∪(t) + φα∪(t), π∪(t) + φα∪(t)) dt
with respect to φ at φ = 0. When π(t) = Bt (0 ∈t ∈1), note that
gπ(t)(π∪(t), π∪(t)) = Tr B−t Bt(log B)B−t Bt log B = Tr (log B)2
does not depend on t. The derivative of the above integral at φ = 0 is
⎢1
0
1
2

gπ(t)(π∪(t), π∪(t)
	−1/2 λ
λφgπ(t) + φα(t)(π∪(t) + φα∪(t), π∪(t) + φα∪(t))
⎥⎥⎥
φ=0 dt
=
1
2
⎡
Tr (log B)2 ×
⎢1
0
λ
λφTr
(Bt + φα(t))−1(Bt log B + φα∪(t))(Bt + φα(t))−1(Bt log B + φα∪(t))
⎥⎥⎥
φ=0 dt
=
1
⎡
Tr (log B)2
⎢1
0
Tr (−B−t(log B)2α(t) + B−t(log B)α∪(t)) dt.
To remove α∪(t), we integrate the second term by parts:
⎢1
0
Tr B−t(log B)α∪(t) dt =

Tr B−t(log B)α(t)
1
0 +
⎢1
0
Tr B−t(log B)2α(t) dt .
Since α(0) = α(1) = 0, the ﬁrst term vanishes here and the derivative at φ = 0 is 0
for every perturbation α(t). Thus we conclude that π(t) = Bt is the geodesic curve
between I and B. The distance is
⎢1
0
⎦
Tr (log B)2 dt =
⎦
Tr (log B)2 = ⊥log B⊥2.

190
5
Matrix Means and Inequalities
The lemma is proved.
□
The midpoint of the curve (5.2) will be called the geometric mean of A, B ∈Pn
and denoted by A#B, that is,
A#B := A1/2(A−1/2B A−1/2)1/2 A1/2.
(5.3)
The motivation is the fact that in the case where AB = B A, the midpoint is
→
AB.
This geodesic approach will give an idea for the geometric mean of three matrices
as well.
Let A, B ⊂0 and assume that A is invertible. We want to study the positivity of
the matrix
A X
X B

(5.4)
for a positive X. The positivity of the block matrix implies
B ⊂X A−1X,
see Theorem 2.1. From the matrix monotonicity of the square root function (Example
3.26), we obtain (A−1/2B A−1/2)1/2 ⊂A−1/2X A−1/2, or
A1/2(A−1/2B A−1/2)1/2 A1/2 ⊂X.
It is easy to see that for X = A#B, the block matrix (5.4) is positive. Therefore, A#B
is the largest positive matrix X such that (5.4) is positive, that is,
A#B = max
⎛
X ⊂0 :
A X
X B

⊂0
⎝
.
(5.5)
The deﬁnition (5.3) is for invertible A. For a non-invertible A, an equivalent
possibility is
A#B := lim
φ√0(A + φI)#B.
(The characterization with (5.4) remains true in this general case.) If AB = B A, then
A#B = A1/2B1/2(= (AB)1/2). The inequality between geometric and arithmetic
means also holds for matrices, see Exercise 5.6.
Example 5.2 The partial ordering ∈of operators has a geometric interpretation for
projections. The relation P ∈Q is equivalent to ran P ≡ran Q, that is, P projects
to a smaller subspace than Q. This implies that any two projections P and Q have
a largest lower bound denoted by P ⊃Q. This operator is the orthogonal projection
onto the (closed) subspace ran P ∞ran Q.

5.1 The Geometric Mean
191
We want to show that P#Q = P ⊃Q. First we show that the block matrix

P
P ⊃Q
P ⊃Q
Q

is positive. This is equivalent to the relation

P + φP∩P ⊃Q
P ⊃Q
Q

⊂0
(5.6)
for every constant φ > 0. Since
(P ⊃Q)(P + φP∩)−1(P ⊃Q) = P ⊃Q
is smaller than Q, the positivity (5.6) follows from Theorem 2.1. We conclude that
P#Q ⊂P ⊃Q.
The positivity of

P + φP∩X
X
Q

gives the condition
Q ⊂X(P + φ−1P∩)X = X P X + φ−1X P∩X.
Since φ > 0 is arbitrary, X P∩X = 0. The latter condition gives X = X P. Therefore,
Q ⊂X2. Symmetrically, P ⊂X2 and Corollary 2.25 tells us that P ⊃Q ⊂X2 and
so P ⊃Q ⊂X.
□
Theorem 5.3 Assumethat A1, A2, B1, B2 arepositivematricesand A1 ∈B1, A2 ∈
B2. Then A1#A2 ∈B1#B2.
Proof : The statement is equivalent to the positivity of the block matrix

B1
A1#A2
A1#A2
B2

.
This is a sum of positive matrices:

A1
A1#A2
A1#A2
A2

+
B1 −A1
0
0
B2 −A2

.
The proof is complete.
□
The next theorem is the Löwner–Heinz inequality already given in Sect.4.4. The
present proof is based on the geometric mean.

192
5
Matrix Means and Inequalities
Theorem 5.4 Assume that for matrices A and B the inequalities 0 ∈A ∈B hold
and 0 < t < 1 is a real number. Then At ∈Bt.
Proof : By continuity, it is enough to prove the case t = k/2n, that is, when t is a
dyadic rational number. We use Theorem 5.3 to deduce from the inequalities A ∈B
and I ∈I the inequality
A1/2 = A#I ∈B#I = B1/2.
A second application of Theorem 5.3 similarly gives A1/4 ∈B1/4 and A3/4 ∈B3/4.
The procedure can be continued to cover all dyadic rational numbers. The result for
arbitrary t ∈[0, 1] follows by taking limits of dyadic numbers.
□
Theorem 5.5 The geometric mean of matrices is jointly concave, that is,
A1 + A2
2
# A3 + A4
2
⊂A1#A3 + A2#A4
2
.
Proof : The block matrices

A1
A1#A2
A1#A2
A2

and

A3
A3#A4
A4#A3
A4

are positive and so is the arithmetic mean,

1
2(A1 + A3)
1
2(A1#A2 + A3#A4)
1
2(A1#A2 + A3#A4)
1
2(A2 + A4)

.
Therefore the off-diagonal entry is smaller than the geometric mean of the diagonal
entries.
□
Note that the joint concavity property is equivalent to the slightly simpler formula
(A1 + A2)#(A3 + A4) ⊂(A1#A3) + (A2#A4).
We shall use this inequality later.
The next theorem of Ando [7] is a generalization of Example 5.2. For the sake of
simplicity the formulation is given in terms of block matrices.
Theorem 5.6 Take an ortho-projection P and a positive invertible matrix R:
P =
I 0
0 0

,
R =
R11 R12
R21 R22

.
The geometric mean of P and R is the following:

5.1 The Geometric Mean
193
P#R = (P R−1P)−1/2 =

(R11 −R12R−1
22 R21)−1/2 0
0
0

.
Proof : P and R are already given in block matrix form. By (5.5) we are looking
for positive matrices
X =
X11 X12
X21 X22

such that
P X
X R

=
⎞
⎠⎠
I
0
X11 X12
0
0
X21 X22
X11 X12 R11 R12
X21 X22 R21 R22


is positive. From the positivity it follows that X12 = X21 = X22 = 0 and the
necessary and sufﬁcient condition is
I 0
0 0

⊂
X11 0
0
0

R−1
X11 0
0
0

,
or
I ⊂X11(R−1)11X11.
The latter is equivalent to I ⊂
⎣
(R−1)11
⎤1/2X2
11
⎣
(R−1)11
⎤1/2, which implies that
X11 ∈
⎣
(R−1)11
⎤−1/2.
The inverse of a block matrix is described in (2.4) and the proof is complete.
□
For projections P and Q, the theorem and Example 5.2 give
P#Q = P ⊃Q = lim
φ≥+0(P(Q + φI)−1P)−1/2.
The arithmetic mean of several matrices is simpler: for (positive) matrices
A1, A2, . . . , An it is
A(A1, A2, . . . , An) := A1 + A2 + · · · + An
n
.
Only the linear structure plays a role. The arithmetic mean is a good example illus-
trating how to move from the means of two variables to means of three variables.

194
5
Matrix Means and Inequalities
Suppose we have a device which can compute the mean of two matrices. How to
compute the mean of three? Assume that we aim to obtain the mean of A, B and C.
In the case of the arithmetic mean, we can introduce a new device
W : (A, B, C) ◦≥(A(A, B), A(A, C), A(B, C)),
which, applied to (A, B, C) many times, gives the mean of A, B and C:
W n(A, B, C) = (An, Bn, Cn)
and
An, Bn, Cn ≥A(A, B, C) as n ≥≤.
Indeed, An, Bn, Cn are convex combinations of A, B and C, so
An = ε(n)
1 A + ε(n)
2 B + ε(n)
3 C.
One can compute the coefﬁcients ε(n)
i
explicitly and show that ε(n)
i
≥1/3, and
likewise for Bn and Cn. The idea is illustrated in the following picture and will be
extended to the geometric mean (Fig.5.1).
A
B1
C
B
A1
C1
B2
A2
C2
Fig. 5.1 The triangles of An, Bn, Cn
Theorem 5.7 Let A, B, C ∈Mn be positive deﬁnite matrices and deﬁne a recursion
by
A0 = A,
B0 = B,
C0 = C,

5.1 The Geometric Mean
195
Am+1 = Am#Bm,
Bm+1 = Am#Cm,
Cm+1 = Bm#Cm.
Then the limits
G3(A, B, C) := lim
m Am = lim
m Bm = lim
m Cm
(5.7)
exist.
Proof : First we assume that A ∈B ∈C. From the monotonicity property of the
geometric mean, see Theorem 5.3, we obtain that Am ∈Bm ∈Cm. It follows that
the sequence (Am) is increasing and (Cm) is decreasing. Therefore, the limits
L := lim
m≥≤Am
and
U = lim
m≥≤Cm
exist, and L ∈U. We claim that L = U.
By continuity, Bm ≥L#U =: M, where L ∈M ∈U. Since
Bm#Cm = Cm+1,
the limit m ≥≤gives M#U = U. Therefore M = U and so U = L.
The general case can be reduced to the case of an ordered triplet. If A, B, C are
arbitrary, we can ﬁnd numbers ε and μ such that A ∈εB ∈μC and use the formula
(αX)#(βY) =
⎡
αβ(X#Y)
(5.8)
for positive numbers α and β.
Let
A∪
1 = A,
B∪
1 = εB,
C∪
1 = μC,
and
A∪
m+1 = A∪
m#B∪
m,
B∪
m+1 = A∪
m#C∪
m,
C∪
m+1 = B∪
m#C∪
m.
It is clear that for the numbers
a := 1,
b := ε
and
c := μ
the recursion provides a convergent sequence (am, bm, cm) of triplets:
(εμ)1/3 = lim
m am = lim
m bm = lim
m cm.
Since
Am = A∪
m/am,
Bm = B∪
m/bm
and
Cm = C∪
m/cm

196
5
Matrix Means and Inequalities
by property (5.8) of the geometric mean, the limits stated in the theorem must exist
and equal G(A∪, B∪, C∪)/(εμ)1/3.
□
The geometric mean of the positive deﬁnite matrices A, B, C ∈Mn is deﬁned
as G3(A, B, C) in (5.7). An explicit formula is not known and the same kind of
procedure can be used to deﬁne the geometric mean of k matrices. If P1, P2, . . . , Pk
are ortho-projections, then Example 5.2 gives the limit
Gk(P1, P2, . . . , Pk) = P1 ⊃P2 ⊃· · · ⊃Pk .
5.2 General Theory
The ﬁrst example is the parallel sum which is a constant multiple of the harmonic
mean.
Example 5.8 It is a well-known result in electronics that if two resistors with resis-
tance a and b are connected in parallel, then the total resistance q is the solution of
the equation
1
q = 1
a + 1
b.
Then
q = (a−1 + b−1)−1 =
ab
a + b
is the harmonic mean up to a factor 2. More generally, one can consider n-point
networks, where the voltage and current vectors are connected by a positive matrix.
The parallel sum
A : B = (A−1 + B−1)−1
of two positive deﬁnite matrices represents the combined resistance of two n-port
networks connected in parallel.
One can check that
A : B = A −A(A + B)−1A.
Therefore A : B is the Schur complement of A + B in the block matrix
A
A
A A + B

,

5.2 General Theory
197
see Theorem 2.4.
It is easy to see that if 0 < A ∈C and 0 < B ∈D, then A : B ∈C : D. The
parallel sum can be extended to all positive matrices:
A : B = lim
φ√0(A + φI) : (B + φI) .
Note that all matrix means can be expressed as an integral of parallel sums (see
Theorem 5.11 below).
□
Above: An n-point network with the input and output voltage vectors. Below: Two
parallelly connected networks
On the basis of the previous example, the harmonic mean of the positive matrices
A and B is deﬁned as
H(A, B) := 2(A : B).
Assume that for all positive matrices A, B (of the same size) the matrix A γ B
is deﬁned. Then γ is called an operator connection if it satisﬁes the following
conditions:
(i) 0 ∈A ∈C and 0 ∈B ∈D imply
A γ B ∈C γ D
(joint monotonicity);
(ii) if 0 ∈A, B and C = C∗, then
C(A γ B)C ∈(C AC) γ (C BC)
(transformer inequality);
(5.9)

198
5
Matrix Means and Inequalities
(iii) if 0 ∈An, Bn and An √A, Bn √B then
(An γ Bn) √(A γ B)
(upper semi-continuity).
The parallel sum is an example of an operator connection.
Lemma 5.9 Assume that γ is an operator connection. If C = C∗is invertible, then
C(A γ B)C = (C AC) γ (C BC),
(5.10)
and for every α ⊂0
α(A γ B) = (αA) γ (αB) (positive homogeneity)
(5.11)
holds.
Proof : In the inequality (5.9) A and B are replaced by C−1 AC−1 and C−1BC−1,
respectively:
A γ B ⊂C(C−1AC−1 γ C−1BC−1)C.
Replacing C with C−1, we have
C(A γ B)C ⊂C AC γ C BC.
This and (5.9) yield equality.
When α > 0, letting C := α1/2I in (5.10) implies (5.11). When α = 0, let
0 < αn √0. Then (αn I) γ (αn I) √0 γ 0 by (iii) above while (αn I) γ (αn I) =
αn(I γ I) √0. Hence 0 = 0 γ 0, which is (5.11) for α = 0.
□
The next fundamental theorem of Kubo and Ando says that there is a one-to-one
correspondence between operator connections and matrix monotone functions on
[0, ≤).
Theorem 5.10 (Kubo–Ando theorem) For each operator connection γ there exists
a unique matrix monotone function f : R+ ≥R+ such that
f (t)I = I γ (t I)
(t ∈R+)
(5.12)
and for A > 0 and B ⊂0 the formula
A γ B = A1/2 f (A−1/2B A−1/2)A1/2 = f (B A−1)A
(5.13)
holds, where the last term is deﬁned via the analytic functional calculus.
Proof : Let γ be an operator connection. First we show that if an ortho-projection
P commutes with A and B, then P commutes with A γ B and

5.2 General Theory
199
((AP) γ (B P)) P = (A γ B)P.
(5.14)
Since PAP= AP ∈A and PBP= B P ∈B, it follows from (ii) and (i) of the
deﬁnition of γ that
P(A γ B)P ∈(PAP) γ (PBP) = (AP) γ (B P) ∈A γ B.
(5.15)
Hence (A γ B −P(A γ B)P)1/2 exists so that
⎥⎥⎥
⎣
A γ B −P(A γ B)P
⎤1/2P
⎥⎥⎥
2
= P
⎣
A γ B −P(A γ B)P
⎤
P = 0.
Therefore, (A γ B −P(A γ B)P)1/2P = 0 and so (A γ B)P = P(A γ B)P. This
implies that P commutes with A γ B. Similarly, P commutes with (AP) γ (B P)
as well, and (5.14) follows from (5.15). For every t ⊂0, since I γ (t I) commutes
with all ortho-projections, it is a scalar multiple of I. Thus, we see that there is a
function f ⊂0 on [0, ≤) satisfying (5.12). The uniqueness of such a function f
is obvious, and it follows from (iii) of the deﬁnition of the operator connection that
f is right-continuous for t ⊂0. Since t−1 f (t)I = (t−1I) γ I for t > 0 thanks to
(5.11), it follows from (iii) of the deﬁnition again that t−1 f (t) is left-continuous for
t > 0 and so is f (t). Hence f is continuous on [0, ≤).
To show the operator monotonicity of f , let us prove that
f (A) = I γ A.
(5.16)
Let A = m
i=1 αi Pi, where αi > 0 and Pi are projections with m
i=1 Pi = I. Since
each Pi commutes with A, using (5.14) twice we have
I γ A =
m

i=1
(I γ A)Pi =
m

i=1
(Pi γ (APi))Pi =
m

i=1
(Pi γ (αi Pi))Pi
=
m

i=1
(I γ (αi I))Pi =
m

i=1
f (αi)Pi = f (A).
For general A ⊂0 choose a sequence 0 < An of the above form such that An √A.
By upper semi-continuity we have
I γ A = lim
n≥≤I γ An = lim
n≥≤f (An) = f (A).
So (5.16) is shown. Hence, if 0 ∈A ∈B, then
f (A) = I γ A ∈I γ B = f (B)
and we conclude that f is matrix monotone.

200
5
Matrix Means and Inequalities
When A is invertible, we can use (5.10):
A γ B = A1/2(I γ A−1/2B A−1/2)A1/2 = A1/2 f (A−1/2B A−1/2)A1/2
and the ﬁrst part of (5.13) is obtained. The rest is a general property.
□
Note that the general formula is
A γ B = lim
φ√0 Aφ γ Bφ = lim
φ√0 A1/2
φ
f (A−1/2
φ
BφA−1/2
φ
)A1/2
φ
,
where Aφ := A + φI and Bφ := B + φI. We call f the representing function of γ.
For scalars s, t > 0 we have s γ t = s f (t/s).
The next theorem follows from the integral representation of matrix monotone
functions and from the previous theorem.
Theorem 5.11 Every operator connection γ has an integral representation
A γ B = aA + bB +
⎢
(0,≤)
1 + ε
ε

(εA) : B
	
dμ(ε)
(A, B ⊂0),
where μ is a positive ﬁnite Borel measure on [0, ≤).
Using this integral expression, one can often derive properties of general operator
connections by checking them for the parallel sum.
Lemma 5.12 For every vector z,
inf{∗x, Ax⟩+ ∗y, By⟩: x + y = z} = ∗z, (A : B)z⟩.
Proof : When A, B are invertible, we have
A : B =

B−1(A + B)A−1	−1
=

(A + B) −B
	
(A + B)−1B = B −B(A + B)−1B.
For all vectors x, y we have
∗x, Ax⟩+ ∗z −x, B(z −x)⟩−∗z, (A : B)z⟩
= ∗z, Bz⟩+ ∗x, (A + B)x⟩−2Re ∗x, Bz⟩−∗z, (A : B)z⟩
= ∗z, B(A + B)−1Bz⟩+ ∗x, (A + B)x⟩−2Re ∗x, Bz⟩
= ⊥(A + B)−1/2Bz⊥2 + ⊥(A + B)1/2x⊥2
−2Re ∗(A + B)1/2x, (A + B)−1/2Bz⟩⊂0.
In particular, the above is equal to 0 if x = (A+B)−1Bz. Hence the assertion follows
if A, B > 0. For general A, B,

5.2 General Theory
201
∗z, (A : B)z⟩= inf
φ>0∗z,

(A + φI) : (B + φI)
	
z⟩
= inf
φ>0 inf
y

∗x, (A + φI)x⟩+ ∗z −x, (B + φI)(z −x)⟩

= inf
y

∗x, Ax⟩+ ∗z −x, B(z −x)⟩

.
The proof is complete.
□
The next result is called the transformer inequality, it is a stronger version of
(5.9).
Theorem 5.13 For all A, B ⊂0 and general S,
S∗(A γ B)S ∈(S∗AS) γ (S∗BS)
and equality holds if S is invertible.
Proof : For z = x + y Lemma 5.12 implies
∗z, S∗(A : B)Sz⟩= ∗Sz, (A : B)Sz⟩∈∗Sx, ASx⟩+ ∗Sy, BSy⟩
= ∗x, S∗ASx⟩+ ∗y, S∗BSy⟩.
Hence S∗(A : B)S ∈(S∗AS) : (S∗BS) follows. The statement of the theorem is true
for the parallel sum and by Theorem 5.11 we obtain it for any operator connection.
The proof of the last assertion is similar to that of Lemma 5.9.
□
A very similar argument gives the joint concavity:
(A γ B) + (C γ D) ∈(A + C) γ (B + D) .
The next theorem concerns a recursively deﬁned double sequence.
Theorem 5.14 Let γ1 and γ2 be operator connections dominated by the arithmetic
mean. For positive matrices A and B deﬁne a recursion
A1 = A,
B1 = B,
Ak+1 = Ak γ1 Bk,
Bk+1 = Ak γ2 Bk.
(5.17)
Then (Ak) and (Bk) converge to the same operator connection A γ B.
Proof : First we prove the convergence of (Ak) and (Bk). From the inequality
X γi Y ∈X + Y
2
we have
Ak+1 + Bk+1 = Ak γ1 Bk + Ak γ2 Bk ∈Ak + Bk.

202
5
Matrix Means and Inequalities
Therefore the decreasing positive sequence has a limit:
Ak + Bk ≥X
as k ≥≤.
Moreover,
ak+1 := ⊥Ak+1⊥2
2 + ⊥Bk+1⊥2
2 ∈⊥Ak⊥2
2 + ⊥Bk⊥2
2 −1
2⊥Ak −Bk⊥2
2,
where ⊥X⊥2 = (Tr X∗X)1/2, the Hilbert–Schmidt norm. The numerical sequence ak
is decreasing, it has a limit and it follows that
⊥Ak −Bk⊥2
2 ≥0
and Ak, Bk ≥X/2 as k ≥≤.
For each k, Ak and Bk are operator connections of the matrices A and B, and the
limit is an operator connection as well.
□
Example 5.15 At the end of the eighteenth century J.-L. Lagrange and C.F. Gauss
became interested in the arithmetic-geometric mean of positive numbers. Gauss
worked on this subject in the period 1791 until 1828.
As already mentioned in the introduction to this chapter, with the initial conditions
a1 = a,
b1 = b
deﬁne the recursion
an+1 = an + bn
2
,
bn+1 =
⎡
anbn.
Then the (joint) limit is the so-called Gauss arithmetic-geometric mean AG(a, b)
with the characterization
1
AG(a, b) = 2
δ
⎢≤
0
dt
⎡
(a2 + t2)(b2 + t2)
,
see [34]. It follows from Theorem 5.14 that the Gauss arithmetic-geometric mean
can also be deﬁned for matrices. Therefore the function f (x) = AG(1, x) is a matrix
monotone function.
□
It is interesting to note that (5.17) can be modiﬁed slightly:
A1 = A,
B1 = B,
Ak+1 = Ak γ1 Bk,
Bk+1 = Ak+1 γ2 Bk.
(5.18)
A similar proof gives the existence of the limit. (5.17) is called the Gaussian double-
mean process, while (5.18) is the Archimedean double-mean process.

5.2 General Theory
203
The symmetric matrix means are binary operations on positive matrices. They
are operator connections with the properties A γ A = A and A γ B = B γ A. For
matrix means we shall use the notation m(A, B). We repeat the main properties:
(1) m(A, A) = A for every A;
(2) m(A, B) = m(B, A) for every A and B;
(3) if A ∈B, then A ∈m(A, B) ∈B;
(4) if A ∈A∪and B ∈B∪, then m(A, B) ∈m(A∪, B∪);
(5) m is upper semi-continuous;
(6) C m(A, B) C∗∈m
⎣
C AC∗, C BC∗⎤
.
It follows from the Kubo–Ando theorem (Theorem 5.10) that the operator means
are in a one-to-one correspondence with matrix monotone functions R+ ≥R+ sat-
isfying conditions f (1) = 1 and t f (t−1) = f (t). Such a matrix monotone function
on R+ is said to be standard. Given a matrix monotone function f , the corresponding
mean is
m f (A, B) = A1/2 f
⎣
A−1/2B A−1/2⎤
A1/2
(5.19)
when A is invertible. (When A is not invertible, take a sequence An of invertible
operatorsapproximating A suchthat An √A andletm f (A, B) = limn m f (An, B).)
It follows from the deﬁnition (5.19) of means that if f ∈g, then m f (A, B) ∈
mg(A, B).
Theorem 5.16 If f : R+ ≥R+ is a standard matrix monotone function, then
2x
x + 1 ∈f (x) ∈x + 1
2
.
Proof : By differentiating the formula f (x) = x f (x−1), we obtain f ∪(1) = 1/2.
Since f (1) = 1, the concavity of the function f gives f (x) ∈(1 + x)/2.
If f is a standard matrix monotone function, then so is f (x−1)−1. The inequality
f (x−1)−1 ∈(1 + x)/2 gives f (x) ⊂2x/(x + 1).
□
If f (x) is a standard matrix monotone function with matrix mean m( · , · ), then
the matrix mean corresponding to x/f (x) is called the dual of m( · , · ) and is denoted
by m∩( · , · ). For instance, the dual of the arithmetic mean is the harmonic mean
and #∩= #.
The next theorem is a Trotter-like product formula for matrix means.
Theorem 5.17 For a symmetric matrix mean m and for self-adjoint A, B we have
lim
n≥≤m(eA/n, eB/n)n = exp A + B
2
.
Proof : It is an exercise to prove that
lim
t≥0
m(et A, et B) −I
t
= A + B
2
.

204
5
Matrix Means and Inequalities
The choice t = 1/n gives
exp

−n(I −m(eA/n, eB/n))
	
≥exp A + B
2
.
So it is enough to show that
Dn := m(eA/n, eB/n)n −exp

−n(I −m(eA/n, eB/n))
	
≥0
as n ≥≤. If A is replaced by A + aI and B is replaced by B + aI for some real
number a, then Dn does not change. Therefore we can assume A, B ∈0.
We use the abbreviation F(n) := m(eA/n, eB/n), so
Dn = F(n)n −exp (−n(I −F(n))) = F(n)n −e−n
≤

k=0
nk
k! F(n)k
= e−n
≤

k=0
nk
k! F(n)n −e−n
≤

k=0
nk
k! F(n)k = e−n
≤

k=0
nk
k!

F(n)n −F(n)k	
.
Since F(n) ∈I, we have
⊥Dn⊥∈e−n
≤

k=0
nk
k! ⊥F(n)n −F(n)k⊥∈e−n
≤

k=0
nk
k! ⊥I −F(n)|k−n|⊥.
Since
0 ∈I −F(n)|k−n| ∈|k −n|(I −F(n)),
it follows that
⊥Dn⊥∈e−n⊥I −F(n)⊥
≤

k=0
nk
k! |k −n|.
The Schwarz inequality gives that
≤

k=0
nk
k! |k −n| ∈
 ≤

k=0
nk
k!
1/2  ≤

k=0
nk
k! (k −n)2
1/2
= n1/2en.
So we have
⊥Dn⊥∈n−1/2⊥n(I −F(n))⊥.
Since ⊥n(I −F(n))⊥is bounded, the limit is really 0.
□

5.2 General Theory
205
For the geometric mean the previous theorem gives the Lie–Trotter formula, see
Theorem 3.8.
Theorem 5.7 concerns the geometric mean of several matrices and it can be ex-
tended for arbitrary symmetric means. The proof is due to Miklós Pálﬁa and makes
use of the Hilbert–Schmidt norm ⊥X⊥2 = (Tr X∗X)1/2.
Theorem 5.18 Let m( · , · ) be a symmetric matrix mean and 0 ∈A, B, C ∈Mn.
Deﬁne a recursion:
(1) A(0) := A,
B(0) := B,
C(0) := C;
(2) A(k+1) := m(A(k), B(k)),
B(k+1) := m(A(k), C(k))
and
C(k+1) := m(B(k),
C(k)).
Then the limits limm A(m) = limm B(m) = limm C(m) exist. This common value will
be denoted by m(A, B, C).
Proof : From the well-known inequality
m(X, Y) ∈X + Y
2
(5.20)
we have
A(k+1) + B(k+1) + C(k+1) ∈A(k) + B(k) + C(k).
Therefore the decreasing positive sequence has a limit:
A(k) + B(k) + C(k) ≥X as k ≥≤.
(5.21)
It also follows from (5.20) that
⊥m(C, D)⊥2
2 ∈⊥C⊥2
2 + ⊥D⊥2
2
2
−1
4⊥C −D⊥2
2.
Therefore,
ak+1 := ⊥A(k+1)⊥2
2 + ⊥B(k+1)⊥2
2 + ⊥C(k+1)⊥2
2
∈⊥A(k)⊥2
2 + ⊥B(k)⊥2
2 + ⊥C(k)⊥2
2
−1
4

⊥A(k) −B(k)⊥2
2 + ⊥B(k) −C(k)⊥2
2 + ⊥C(k) −A(k)⊥2
2
	
=: ak −ck.
Since the numerical sequence ak is decreasing, it has a limit and it follows that
ck ≥0. Therefore,
A(k) −B(k) ≥0,
A(k) −C(k) ≥0.

206
5
Matrix Means and Inequalities
Combining formulas with (5.21), we have
A(k) ≥1
3 X as k ≥≤.
A similar convergence holds for B(k) and C(k).
□
Theorem 5.19 The mean m(A, B, C) deﬁned in Theorem 5.18 has the following
properties:
(1) m(A, A, A) = A for every A;
(2) m(A, B, C) = m(B, A, C) = m(C, A, B) for every A, B and C;
(3) if A ∈B ∈C, then A ∈m(A, B, C) ∈C;
(4) if A ∈A∪, B ∈B∪and C ∈C∪, then m(A, B, C) ∈m(A∪, B∪, C∪);
(5) m is upper semi-continuous;
(6) D m(A, B, C) D∗∈m
⎣
DAD∗, DBD∗, DC D∗⎤
and equality holds if D is
invertible.
The above properties can be shown from convergence arguments based on
Theorem 5.18. The details are omitted here.
Example 5.20 If P1, P2, P3 are ortho-projections, then
m(P1, P2, P3) = P1 ⊃P2 ⊃P3
holds for several means, see Example 5.23.
Now we consider the geometric mean G3(A, A, B). If A > 0, then
G3(A, A, B) = A1/2G3(I, I, A−1/2B A−1/2)A1/2.
Since I, I, A−1/2B A−1/2 are commuting matrices, it is easy to compute the geomet-
ric mean. We have
G3(A, A, B) = A1/2(A−1/2B A−1/2)1/3A1/2.
This is an example of a weighted geometric mean:
Gt(A, B) = A1/2(A−1/2B A−1/2)t A1/2
(0 < t < 1).
There is a general theory of weighted geometric means.
□

5.3 Mean Examples
207
5.3 Mean Examples
Recallthatamatrixmonotonefunction f : R+ ≥R+ iscalledstandardif f (1) = 1
and x f (x−1) = f (x). Standard functions are used to deﬁne matrix means in (5.19).
Here are some familiar standard matrix monotone functions:
2x
x + 1 ∈→x ∈x −1
log x ∈x + 1
2
.
The corresponding increasing means are the harmonic, geometric, logarithmic and
arithmetic means. By Theorem 5.16 we see that the harmonic mean is the smallest
and the arithmetic mean is the largest among the symmetric matrix means.
First we study the harmonic mean H(A, B). A variational expression is ex-
pressed in terms of 2 × 2 block matrices.
Theorem 5.21
H(A, B) = max
⎛
X ⊂0 :
2A 0
0 2B

⊂
X X
X X
⎝
.
Proof : The inequality of the two block matrices is equivalently written as
∗x, 2Ax⟩+ ∗y, 2By⟩⊂∗x + y, X(x + y)⟩.
Therefore the proof is reduced to Lemma 5.12, where x + y is z and H(A, B) =
2(A : B).
□
Recall the geometric mean
G(A, B) = A#B = A1/2⎣
A−1/2B A−1/2⎤1/2 A1/2
which corresponds to f (x) = →x. The mean A#B is the unique positive solution to
the equation X A−1X = B and therefore (A#B)−1 = A−1#B−1.
Example 5.22 The function
f (x) = x −1
log x
is matrix monotone due to the formula
⎢1
0
xt dt = x −1
log x .
The standard property is obvious. The matrix mean induced by the function f (x) is
called the logarithmic mean. The logarithmic mean of positive operators A and B
is denoted by L(A, B).

208
5
Matrix Means and Inequalities
From the inequality
x −1
log x =
⎢1
0
xt dt =
⎢1/2
0
(xt + x1−t) dt ⊂
⎢1/2
0
2→x dt = →x
of the real functions we have the matrix inequality
A#B ∈L(A, B).
It can similarly be proved that L(A, B) ∈(A + B)/2.
From the integral formula
1
L(a, b) = log a −log b
a −b
=
⎢≤
0
1
(a + t)(b + t) dt
one can obtain
L(A, B)−1 =
⎢≤
0
(t A + B)−1
t + 1
dt.
□
In the next example we study the means of ortho-projections.
Example 5.23 Let P and Q be ortho-projections. It was shown in Example 5.2 that
P#Q = P ⊃Q. The inequality
2P
0
0 2Q

⊂
P ⊃Q P ⊃Q
P ⊃Q P ⊃Q

is true since
P 0
0 Q

⊂
P ⊃Q
0
0
P ⊃Q

,

P
−P ⊃Q
−P ⊃Q
Q

⊂0.
This gives that H(P, Q) ⊂P ⊃Q and from the other inequality H(P, Q) ∈P#Q,
we obtain H(P, Q) = P ⊃Q = P#Q.
The general matrix mean mf (P, Q) has the integral expression
m f (P, Q) = aP + bQ +
⎢
(0,≤)
1 + ε
ε

(εP) : Q
	
dμ(ε).
Since
(εP) : Q =
ε
1 + ε(P ⊃Q),

5.3 Mean Examples
209
we have
mf (P, Q) = aP + bQ + c(P ⊃Q).
Note that a = f (0), b = limx≥≤f (x)/x and c = μ((0, ≤)). If a = b = 0, then
c = 1 (since m(I, I) = I) and m f (P, Q) = P ⊃Q.
□
Example 5.24 The power difference means are determined by the functions
ft(x) = t −1
t
·
xt −1
xt−1 −1
(−1 ∈t ∈2),
(5.22)
where the values t = −1, 1/2, 1, 2 correspond to the well-known harmonic, geo-
metric, logarithmic and arithmetic means. The functions (5.22) are standard matrix
monotone[39] andit canbeshownthat for ﬁxed x > 0 thevalue ft(x) is anincreasing
function of t. The simple case t = n/(n −1) yields
ft(x) = 1
n
n−1

k=0
xk/(n−1)
and the matrix monotonicity is obvious.
□
Example 5.25 The Heinz mean
Ht(x, y) = xt y1−t + x1−t yt
2
(0 ∈t ∈1/2)
interpolates the arithmetic and geometric means. The corresponding standard func-
tion
ft(x) = xt + x1−t
2
is obviously matrix monotone and a decreasing function of the parameter t. Therefore
we have a Heinz matrix mean. The formula is
Ht(A, B) = A1/2 (A−1/2B A−1/2)t + (A−1/2B A−1/2)1−t
2
A1/2.
This lies between the geometric and arithmetic means:
A#B ∈Ht(A, B) ∈A + B
2
(0 ∈t ∈1/2) .
□

210
5
Matrix Means and Inequalities
Example 5.26 For x ∼= y the Stolarsky mean is
m p(x, y) =

p x −y
x p −y p

1
1−p
=

1
y −x
⎢y
x
t p−1 dt

1
p−1
,
where the case p = 1 is understood as
lim
p=1 m p(x, y) = 1
e
xx
yy

1
x−y
.
If −2 ∈p ∈2, then f p(x) = m p(x, 1) is a matrix monotone function (see Theorem
4.46), so it can deﬁne a matrix mean. The case p = 1 is called the identric mean
and the case p = 0 is the well-known logarithmic mean.
□
It is known that the following canonical representation holds for any standard
matrix monotone function R+ ≥R+.
Theorem 5.27 Let f : R+ ≥R+ be a standard matrix monotone function. Then
f admits a canonical representation
f (x) = 1 + x
2
exp
⎢1
0
(ε −1)(1 −x)2
(ε + x)(1 + εx)(1 + ε) h(ε) dε
(5.23)
where h : [0, 1] ≥[0, 1] is a measurable function.
Example 5.28 In the function (5.23) we take
h(ε) =

1 if a ∈ε ∈b,
0 otherwise
where 0 ∈a ∈b ∈1.
Then an easy calculation gives
(ε −1)(1 −x)2
(ε + x)(1 + εx)(1 + ε) =
2
1 + ε −
1
ε + x −
x
1 + εx .
Thus
⎢b
a
(ε −1)(1 −x)2
(ε + x)(1 + εx)(1 + ε) dε =

log(1 + ε)2 −log(ε + x) −log(1 + εx)
b
ε=a
= log (1 + b)2
(1 + a)2 −log b + x
a + x −log 1 + bx
1 + ax .

5.3 Mean Examples
211
So
f (x) = (b + 1)2
2(a + 1)2
(1 + x)(a + x)(1 + ax)
(b + x)(1 + bx)
.
Choosing h ≡0 yields the largest function f (x) = (1 + x)/2 and h ≡1 yields the
smallest function f (x) = 2x/(1 + x). If
⎢1
0
h(ε)
ε
dε = +≤,
then f (0) = 0.
□
The next theorem describes the canonical representation for the reciprocal 1/f
of a standard matrix monotone function f (1/f is a matrix monotone decreasing
function).
Theorem 5.29 If f : R+ ≥R+ is a standard matrix monotone function, then
1
f (x) =
⎢1
0
1 + ε
2

1
x + ε +
1
1 + xε

dμ(ε),
(5.24)
where μ is a probability measure on [0, 1].
A standard matrix monotone function f
: R+ ≥R+ is called regular if
f (0) > 0. The next theorem provides a bijection between the regular standard
matrix monotone functions and the non-regular ones.
Theorem 5.30 Let f : R+ ≥R+ be a standard matrix monotone function with
f (0) > 0. Then
˜f (x) := 1
2

(x + 1) −(x −1)2 f (0)
f (x)

is standard matrix monotone as well. Moreover, f ◦≥˜f gives a bijection between
{ f ∈F : f (0) > 0} and { f ∈F : f (0) = 0}, where F is the set of all standard
matrix monotone functions R+ ≥R+.
Example 5.31 Let A, B ∈Mn be positive deﬁnite matrices and m be a matrix mean.
The block matrix

A
m(A, B)
m(A, B)
B

is positive if and only if m(A, B) ∈A#B. Similarly,

212
5
Matrix Means and Inequalities

A−1
m(A, B)−1
m(A, B)−1
B−1

⊂0
if and only m(A, B) ⊂A#B.
If ε1, ε2, . . . , εn are positive numbers, then the matrix A ∈Mn deﬁned as
Ai j =
1
L(εi, ε j)
is positive for n = 2 according to the above argument. However, this is true for every
n by the formula
1
L(x, y) =
⎢≤
0
1
(x + t)(y + t) dt.
(Another argument appears in Example 2.55.)
From the harmonic mean we obtain the mean matrix
[H(εi, ε j)] =
 2εiε j
εi + ε j

,
which is positive since it is the Hadamard product of two positive matrices (one of
which is the Cauchy matrix).
A general description of positive mean matrices and many examples can be found
in the book [45] and the paper [46]. It is worthwhile to note that two different notions
of the matrix mean m(A, B) and the mean matrix [m(εi, ε j)] are associated with a
standard matrix monotone function.
□
5.4 Mean Transformation
If 0 ∈A, B ∈Mn, then a matrix mean m f (A, B) ∈Mn has a slightly complicated
formula expressed by the function f : R+ ≥R+ of the mean. If AB = B A, then
the situation is simpler: m f (A, B) = f (AB−1)B. The mean introduced here will
be a linear mapping Mn ≥Mn. If n > 1, then this is essentially different from
m f (A, B).
From A and B we have the linear mappings Mn ≥Mn deﬁned as
LAX := AX,
RB X := X B
(X ∈Mn).
So LA is left-multiplication by A and RB is right-multiplication by B. Obviously,
they are commuting operators, LARB = RBLA, and they can be considered as
matrices in Mn ⊗Mn = Mn2.
The deﬁnition of the mean transformation is

5.4 Mean Transformation
213
M f (A, B) := m f (LA, RB) .
Sometimes the notation J f
A,B is used for this.
For f (x) = →x we have the geometric mean, which is a simple example.
Example 5.32 Since LA and RB commute, the geometric mean is the following:
LA#RB = (LA)1/2(RB)1/2 = LA1/2RB1/2,
X ◦≥A1/2X B1/2 .
It is not true that M(A, B)X ⊂0 if X ⊂0, but as a linear mapping M(A, B) is
positive:
∗X, M(A, B)X⟩= Tr X∗A1/2X B1/2 = Tr B1/4X∗A1/2X B1/4 ⊂0
for every X ∈Mn.
Let A, B > 0. The equality M(A, B)A = M(B, A)A immediately implies that
AB = B A. From M(A, B) = M(B, A) we deduce that A = εB for some number
ε > 0. Therefore M(A, B) = M(B, A) is a very special condition for the mean
transformation.
□
The logarithmic mean transformation is
Mlog(A, B)X =
⎢1
0
At X B1−t dt.
In the next example we have a formula for general M(A, B).
Example 5.33 Assume that A and B act on a Hilbert space which has two ortho-
normal bases |x1⟩, . . . , |xn⟩and |y1⟩, . . . , |yn⟩such that
A =

i
εi|xi⟩∗xi|,
B =

j
μ j|y j⟩∗y j|.
Then for f (x) = xk we have
f (LAR−1
B )RB|xi⟩∗y j| = Ak|xi⟩∗y j|B−k+1 = εk
i μ−k+1
j
|xi⟩∗y j|
= f (εi/μ j)μ j|xi⟩∗y j| = m f (εi, μ j)|xi⟩∗y j|
and for a general f
M f (A, B)|xi⟩∗y j| = m f (εi, μ j)|xi⟩∗y j|.
This also shows that M f (A, B) ⊂0 with respect to the Hilbert–Schmidt inner
product.

214
5
Matrix Means and Inequalities
Another formulation is also possible. Let A = UDiag(ε1, . . . , εn)U ∗and B =
V Diag(μ1, . . . , μn)V ∗with unitaries U, V . Let |e1⟩, . . . , |en⟩be the standard basis
vectors. Then
M f (A, B)X = U
⎣
[m f (εi, μ j)]i j ↑(U ∗XV )
⎤
V ∗.
It is enough to check the case X = |xi⟩∗y j|. Then
U
⎣
[m f (εi, μ j)]i j ↑(U ∗|xi⟩∗y j|V )
⎤
V ∗= U
⎣
[m f (εi, μ j)]i j ↑|ei⟩∗e j|
⎤
V ∗
= m f (εi, μ j)U|ei⟩∗e j|V ∗= m f (εi, μ j)|xi⟩∗y j|.
For the matrix means we have m(A, A) = A, but M(A, A) is rather different, it
cannot be A since it is a transformation. If A = 
i εi|xi⟩∗xi|, then
M(A, A)|xi⟩∗x j| = m(εi, ε j)|xi⟩∗x j|.
(This is related to the so-called mean matrix, see Example 5.31.)
□
Example 5.34 Here we ﬁnd a very special inequality between the geometric mean
transformation MG(A, B) and the arithmetic mean transformation MA(A, B). They
are
MG(A, B)X = A1/2X B1/2,
MA(A, B)X = 1
2(AX + X B).
There is an integral formula
MG(A, B)X =
⎢≤
−≤
Ait MA(A, B)X B−it dμ(t),
(5.25)
where the probability measure is
dμ(t) =
1
cosh(δt)dt.
From (5.25) it follows that
⊥MG(A, B)X⊥∈⊥MA(A, B)X⊥
(5.26)
which is an operator norm inequality. A general comparison theorem of this kind
between mean transformations is given in [44].
□
The next theorem gives the transformer inequality.
Theorem 5.35 Let f : [0, +≤) ≥[0, +≤) be a matrix monotone function and
M( · , · ) be the corresponding mean transformation. If β : Mn ≥Mm is a 2-positive

5.4 Mean Transformation
215
trace-preserving mapping and the matrices A, B ∈Mn are positive, then
βM(A, B)β∗∈M(β(A), β(B)).
(5.27)
Proof : By approximation we may assume that A, B, β(A), β(B) > 0. Indeed,
assume that the conclusion holds under this positive deﬁniteness condition. For each
φ > 0 let
βφ(X) := β(X) + φ(Tr X)Im
1 + mφ
,
X ∈Mn,
which is 2-positive and trace-preserving. If A, B > 0, then βφ(A), βφ(B) > 0 as
well and hence (5.27) holds for β∂. Letting φ √0 implies that (5.27) for β is true for
all A, B > 0. Then by taking the limit of A + φIn, B + φIn as φ √, we have (5.27)
for all A, B ⊂0. Now assume A, B, β(A), β(B) > 0.
Based on Löwner’s theorem, we may consider f (x) = x/(ε + x) (ε > 0). Then
M(A, B) =
LA
εI + LAR−1
B
,
M(A, B)−1 = (εI + LAR−1
B )L−1
A
and similarly M(β(A), β(B))−1 = (εI + Lβ(A)R−1
β(B))L−1
β(A). The statement (5.27)
has the equivalent form
β∗M(β(A), β(B))−1β ∈M(A, B)−1,
which means
∗β(X), (εI + Lβ(A)R−1
β(B))L−1
β(A)β(X)⟩∈∗X, (εI + LAR−1
B )L−1
A X⟩
or
εTr β(X∗)β(A)−1β(X) + Tr β(X)β(B)−1β(X∗) ∈εTr X∗A−1X + Tr X B−1X∗.
This inequality is true due to the matrix inequality
β(X∗)β(Y)−1β(X) ∈β(X∗Y −1X)
(Y > 0),
see Lemma 2.46.
□
If β−1 has the same properties as β in the previous theorem, then we have equality
in formula (5.27).
Theorem 5.36 Let f : R+ ≥R+ be a matrix monotone function with f (1) = 1
and M( · , · ) be the corresponding mean transformation. Assume that 0 ∈A ∈A∪
and 0 ∈B ∈B∪in Mn. Then M(A, B) ∈M(A∪, B∪).

216
5
Matrix Means and Inequalities
Proof : By continuity we may assume that A, B > 0. Based on Löwner’s theorem,
we may consider f (x) = x/(ε + x) (ε > 0). Then the statement is
LA(εI + LAR−1
B )−1 ∈LA∪(εI + LA∪R−1
B∪)−1,
which is equivalent to the relation
εL−1
A∪+ R−1
B∪= (εI + LA∪R−1
B∪)L−1
A∪∈(εI + LAR−1
B )L−1
A = εL−1
A + R−1
B .
This is true, since L−1
A∪∈L−1
A and R−1
B∪∈R−1
B by the assumption.
□
Theorem 5.37 Let f be a matrix monotone function with f (1) = 1 and M f be the
corresponding transformation mean. Then M f has the following properties:
(1) M f (εA, εB) = εM f (A, B) for a number ε > 0.
(2)
⎣
M f (A, B)X
⎤∗= M f (B, A)X∗.
(3) M f (A, A)I = A.
(4) Tr M f (A, A)−1Y = Tr A−1Y.
(5) (A, B) ◦≥∗X, M f (A, B)Y⟩is continuous.
(6) Let
C :=
A 0
0 B

⊂0.
Then
M f (C, C)
X Y
Z W

=
M f (A, A)X M f (A, B)Y
M f (B, A)Z M f (B, B)Z

.
The proof of the theorem is an elementary computation. Property (6) is essential.
It tells us that it is sufﬁcient to know the mean transformation for two identical
matrices.
The next theorem is an axiomatic characterization of the mean transformation.
Theorem 5.38 Assume that for any n ∈N and for all 0 ∈A, B ∈Mn, the linear
operator L(A, B) : Mn ≥Mn is deﬁned. L(A, B) = M f (LA, RB) for a matrix
monotone function f if and only if L has the following properties:
(i) (X, Y) ◦≥∗X, L(A, B)Y⟩is an inner product on Mn.
(ii) (A, B) ◦≥∗X, L(A, B)Y⟩is continuous.
(iii) For a trace-preserving completely positive mapping β : Mn ≥Mm,
β L(A, B) β∗∈L
⎣
β A, βB
⎤
holds.

5.4 Mean Transformation
217
(iv) Let
C :=
A 0
0 B

> 0.
Then
L(C, C)
X Y
Z W

=
L(A, A)X L(A, B)Y
L(B, A)Z L(B, B)Z

.
The proof needs a few lemmas. We use the notation Pn := {A ∈Mn : A > 0}.
Lemma 5.39 If U, V ∈Mn are arbitrary unitary matrices, then for all A, B ∈Pn
and X ∈Mn we have
∗X, L(A, B)X⟩= ∗U XV ∗, L(U AU ∗, V BV ∗)U XV ∗⟩.
Proof : For a unitary matrix U ∈Mn deﬁne β(A) = U ∗AU. Then β : Mn ≥Mn
is trace-preserving completely positive and β∗(A) = β−1(A) = U AU ∗. Thus by
double application of (iii) we obtain
∗X, L(A, A)X⟩= ∗X, L(ββ−1 A, ββ−1 A)X⟩
⊂∗X, βL(β−1 A, β−1 A)β∗X⟩
= ∗β∗X, L(β−1 A, β−1 Aβ∗X⟩
⊂∗β∗X, β−1L(A, A)(β−1)∗β∗X⟩
= ∗X, L(A, A)X⟩,
hence
∗X, L(A, A)X⟩= ∗U AU ∗, L(U AU ∗,U AU ∗)U XU ∗⟩.
Now for the matrices
C =
A 0
0 B

∈P2n,
Y =
0 X
0 0

∈M2n
and
W =
U 0
0 V

∈M2n
it follows by (iv) that
∗X, L(A, B)X⟩= ∗Y, L(C, C)Y⟩
= ∗WY W ∗, L(WCW ∗, WCW ∗)WY W ∗⟩
= ∗U XV ∗L(U AU ∗, V BV ∗)U XV ∗⟩
and we have the statement.
□
Lemma 5.40 Suppose that L(A, B) is deﬁned by the axioms (i)–(iv). Then there
exists a unique continuous function d : R+ × R+ ≥R+ such that

218
5
Matrix Means and Inequalities
d(rε,rμ) = rd(ε, μ)
(r, ε, μ > 0)
and for every A = Diag(ε1, . . . , εn) and B = Diag(μ1, . . . , μn) in Pn,
∗X, L(A, B)X⟩=
n

j,k=1
d(ε j, μk)|X jk|2.
Proof : The uniqueness of such a function d is clear. We concentrate on the
existence.
Denote by E( jk)(n) and In the n × n matrix units and the n × n unit matrix,
respectively. We assume that A = Diag(ε1, . . . , εn) and B = Diag(μ1, . . . , μn) are
in Pn.
We ﬁrst show that
∗E( jk)(n), L(A, A)E(lm)(n)⟩= 0 if ( j, k) ∼= (l, m).
(5.28)
Indeed, if j ∼= k,l, m we let U j = Diag(1, . . . , 1, i, 1, . . . , 1) where the imaginary
unit is the jth entry and j ∼= k,l, m. Then by Lemma 5.39 we have
∗E( jk)(n), L(A, A)E(lm)(n)⟩
= ∗U j E( jk)(n)U ∗
j , L(U j AU ∗
j ,U j AU ∗
j )U j E(lm)(n)U ∗
j ⟩
= ∗iE( jk)(n), L(A, A)E(lm)(n)⟩= −i∗E( jk)(n), L(A, A)E(lm)(n)⟩,
hence ∗E( jk)(n), L(A, A)E(lm)(n)⟩= 0. If one of the indices j, k,l, m is different
from the others then (5.28) follows analogously. Finally, applying condition (iv) we
obtain that
∗E( jk)(n), L(A, B)E(lm)(n)⟩= ∗E( j, k + n)(2n), m(C, C)E(l, m + n)(2n)⟩= 0
if ( j, k) ∼= (l, m), because C = Diag(ε1, . . . , εn, μ1, . . . , μn) ∈H+
2n and one of the
indices j, k + n, l, m + n is different from the others.
Now we claim that ∗E( jk)(n), L(A, B)E( jk)(n)⟩is determined by ε j and μk.
More speciﬁcally,
⊥E( jk)(n)⊥2
A,B = ⊥E(12)(2)⊥2
Diag(ε j,μk),
(5.29)
where for brevity we have introduced the notations
⊥X⊥2
A,B = ∗X, L(A, B)X⟩
and
⊥X⊥2
A = ⊥X⊥2
A,A.
Indeed, if U j,k+n ∈M2n denotes the unitary matrix which interchanges the ﬁrst and
the jth coordinates and further the second and the (k + n)th coordinates, then by
condition (iv) and Lemma 5.39 it follows that

5.4 Mean Transformation
219
⊥E( jk)(n)⊥2
A,B = ⊥E( j, k + n)(2n)⊥2
C
= ⊥U j,k+n E( j, k + n)(2n)U ∗
j,k+n⊥2
U j,k+nCU∗
j,k+n
= ⊥E(12)(2n)⊥2
Diag(ε j,μk,ε3,...,μn) .
Thus it sufﬁces to prove
⊥E(12)(2n)⊥2
Diag(θ1,θ2,...,θ2n) = ⊥E(12)(2)⊥2
Diag(θ1,θ2).
(5.30)
Condition (iv) with X = E(12)(n) and Y = Z = W = 0 yields
⊥E(12)(2n)⊥2
Diag(θ1,θ2,...,θ2n) = ⊥E(12)(n)⊥2
Diag(θ1,θ2,...,θn).
(5.31)
Further, consider the following mappings (n ⊂4): βn : Mn ≥Mn−1,
βn(E( jk)(n)) :=

⎧
⎪
E( jk)(n−1),
if 1 ∈j, k ∈n −1,
E(n −1, n −1)(n−1), if j = k = n,
0,
otherwise,
and ˜βn : Mn−1 ≥Mn, ˜βn(E( jk)(n−1)) := E( jk)(n−1) if 1 ∈j, k ∈n −2,
˜βn(E(n −1, n −1)(n−1)) := θn−1E(n −1, n −1)(n) + θn E(nn)(n)
θn−1 + θn
and in the other cases ˜βn(E( jk)(n−1)) = 0.
Clearly, βn and ˜βn are trace-preserving completely positive mappings. Hence by
(iii)
⊥E(12)(n)⊥2
Diag(θ1,...,θn) = ⊥E(12)(n)⊥2
˜βnβnDiag(θ1,...,θn)
⊂⊥˜β∗
n E(12)(n)⊥2
βnDiag(θ1,...,θn)
⊂⊥β∗
n ˜β∗
n E(12)(n)⊥2
Diag(θ1,...,θn)
= ⊥E(12)(n)⊥2
Diag(θ1,...,θn).
Thus equality holds, which implies that
⊥E(12)(n)⊥2
Diag(θ1,...,θn−1,θn) = ⊥E(12)(n−1)⊥2
Diag(θ1,...,θn−2,θn−1+θn).
(5.32)
Now repeated application of (5.31) and (5.32) yields (5.30) and therefore (5.29) also
follows.
For ε, μ > 0 let
d(ε, μ) := ⊥E(12)(2)⊥2
Diag(ε,μ).

220
5
Matrix Means and Inequalities
Condition (ii) implies the continuity of d. We furthermore claim that d is homoge-
neous of order one, that is,
d(rε,rμ) = rd(ε, μ)
(ε, μ,r > 0).
First let r = k ∈N. Then the mappings αk : M2 ≥M2k, ˜αk : M2k ≥Mk deﬁned
by
αk(X) = 1
k Ik ⊗X
and
˜αk
⎞
⎠⎠⎠
X11 X12 . . . X1k
X21 X22 . . . X2k
...
...
...
Xk1 Xk2 . . . Xkk

= X11 + X22 + . . . + Xkk
are trace-preserving completely positive, for which ˜α∗
k = kαk. So applying condition
(iii) twice it follows that
⊥E(12)(2)⊥2
Diag(ε,μ) = ⊥E(12)(2)⊥2
˜αkαkDiag(ε,μ)
⊂⊥˜α∗
k E(12)(2)⊥2
αkDiag(ε,μ)
⊂⊥α∗
k ˜α∗
k E(12)(2)⊥2
Diag(ε,μ)
= ⊥E(12)(2)⊥2
Diag(ε,μ) .
Hence equality holds, which means that
⊥E(12)(2)⊥2
Diag(ε,μ) = ⊥Ik ⊗E(12)(2)⊥2
1
k Ik⊗Diag(ε,μ).
Thus by applying (5.28) and (5.29) we obtain
d(ε, μ) = ⊥Ik ⊗E(12)(2)⊥2
1
k Ik⊗Diag(ε,μ)
=
k

j=1
⊥E( j j)(k) ⊗E(12)(2)⊥2
1
k Ik⊗Diag(ε,μ)
= k⊥E(11)(k) ⊗E(12)(2)⊥2
1
k Ik⊗Diag(ε,μ)
= kd
ε
k , μ
k

.
If r = α/k where α, k are positive natural numbers, then
d(rε,rμ) = d
α
k ε, α
k μ

= 1
k d(αε, αμ) = α
k d(ε, μ).

5.4 Mean Transformation
221
By condition (ii), the homogeneity follows for every r > 0.
We ﬁnish the proof by using (5.28) and (5.29), obtaining
⊥X⊥2
A,B =
n

j,k=1
d(ε j, μk)|X jk|2.
□
If we require the positivity of M(A, B)X for X ⊂0, then from the formula
(M(A, B)X)∗= M(B, A)X∗
we need A = B. If A = 
i εi|xi⟩∗xi| and X = 
i, j |xi⟩∗x j| for an orthonormal
basis {|xi⟩: i}, then

M(A, A)X
	
i j = m(εi, ε j).
The positivity of this matrix is necessary.
Given the positive numbers {εi : 1 ∈i ∈n}, the matrix
Ki j = m(εi, ε j)
is called an n × n mean matrix. From the previous argument the positivity of
M(A, A) : Mn ≥Mn implies the positivity of the n × n mean matrices of the
mean M. It is easy to see that if the mean matrices of any size are positive, then
M(A, A) : Mn ≥Mn is a completely positive mapping.
If the mean matrix

ε1
m(ε1, ε2)
m(ε1, ε2)
ε1

is positive, then m(ε1, ε2) ∈→ε1ε2. It follows that to have a positive mean matrix,
the mean m should be smaller than the geometric mean. Indeed, the next general
characterization result is known.
Theorem 5.41 Let f be a standard matrix monotone function on R+ and m the
corresponding mean, i.e., m(x, y) := x f (x/y) for x, y > 0. Let M( · , · ) be the
corresponding mean transformation. Then the following conditions are equivalent:
(1) M(A, A)X ⊂0 for all 0 ∈A, X ∈Mn and every n ∈N;
(2) the mean transformation M(A, A) : Mn ≥Mn is completely positive for all
0 ∈A ∈Mn and every n ∈N;
(3) ⊥M(A, A)X⊥∈⊥A1/2X A1/2⊥for all A, X ∈Mn with A ⊂0 and every n ∈N,
where ⊥· ⊥is the operator norm;

222
5
Matrix Means and Inequalities
(4) the mean matrix [m(εi, ε j)]i j is positive semideﬁnite for all ε1, . . . , εn > 0
and every n ∈N;
(5)
f (et)e−t/2 is a positive deﬁnite function on R in the sense of Bochner, i.e., it is
the Fourier transform of a probability measure on R.
The above condition (5) is stronger than f (x) ∈→x and it is a necessary and
sufﬁcient condition for the positivity of M(A, A)X for all A, X ⊂0.
Example 5.42 The power mean or binomial mean
mt(x, y) =
xt + yt
2
1/t
is an increasing function of t when x and y are ﬁxed. The limit t ≥0 gives the
geometric mean. Therefore the positivity of the matrix mean may appear only for
t ∈0. Then for t > 0,
m−t(x, y) = 21/t
xy
(xt + yt)1/t
and the corresponding mean matrix is positive due to the inﬁnitely divisible Cauchy
matrix, see Example 1.41.
□
5.5 Notes and Remarks
The geometric mean of operators ﬁrst appeared in the paper Wieslaw Pusz and
Stanislav L. Woronowicz, Functional calculus for sesquilinear forms and the puriﬁ-
cation map, Rep. Math. Phys. 8(1975), 159–170, and was studied in detail in the
papers [2,58] of Tsuyoshi Ando and Fumio Kubo. The geometric mean for more
matrices is from the paper [10]. Another approach based on differential geometry
is explained in the book [21]. A popularization of the subject is the paper Rajendra
Bhatia and John Holbrook, Noncommutative geometric means, Math. Intelligencer
28(2006), 32–39.
Theorem 5.18 is from the paper Miklós Pálﬁa, A multivariable extension of two-
variable matrix means, SIAM J. Matrix Anal. Appl. 32(2011), 385–393. There is a
different deﬁnition of the geometric mean X of the positive matrices A1, A2, . . . , Ak
as deﬁned by the equation n
k=1 log A−1
i
X = 0. See the paper Y. Lim and M. Pálﬁa,
Matrix power means and the Karcher mean, J. Funct. Anal. 262(2012), 1498–1514
and the references therein.
The mean transformations are in the paper [44] and the book [45] of Fumio
Hiai and Hideki Kosaki. Theorem 5.38 is from the paper [18]. There are several
examples of positive and inﬁnite divisible mean matrices in the paper Rajendra
Bhatia and Hideki Kosaki, Mean matrices and inﬁnite divisibility, Linear Algebra

5.5 Notes and Remarks
223
Appl. 424(2007), 36–54. (Inﬁnite divisibility means the positivity of matrices Ai j =
m(εi, ε j)t for every t > 0.)
Lajos Molnár proved that if a bijection α : M+
n ≥M+
n preserves the geometric
mean, then, for n ⊂2, α(A) = SAS∗for a linear or conjugate linear mapping S
(Maps preserving the geometric mean of positive operators, Proc. Amer. Math. Soc.
137(2009), 1763–1770.)
Theorem 5.27 is from the paper K. Audenaert, L. Cai and F. Hansen, Inequalities
for quantum skew information, Lett. Math. Phys. 85(2008), 135–146. Theorem 5.29
is from the paper F. Hansen, Metric adjusted skew information, Proc. Natl. Acad.
Sci. USA 105(2008), 9909–9916, and Theorem 5.35 is from P. Gibilisco, F. Hansen,
T. Isola, On a correspondence between regular and non-regular operator monotone
functions, Linear Algebra Appl. 430(2009), 2225–2232.
The norm inequality (5.26) was obtained by R. Bhatia and C. Davis, A Cauchy–
Schwarz inequality for operators with applications, Linear Algebra Appl. 223/224
(1995), 119–129. The integral expression (5.25) is due to H. Kosaki, Arithmetic-
geometric mean and related inequalities for operators, J. Funct. Anal. 156(1998),
429–451. For a systematic analysis on norm inequalities and integral expressions of
this kind as well as the details on Theorem 5.41, see the papers [44,45,46].
5.6 Exercises
1. Show that for positive invertible matrices A and B the inequalities
2(A−1 + B−1)−1 ∈A#B ∈1
2(A + B)
hold. What is the condition for equality? (Hint: Reduce the general case to
A = I.)
2. Show that
A#B = 1
δ
⎢1
0
(t A−1 + (1 −t)B−1)−1
→t(1 −t)
dt.
3. Let A, B > 0. Show that A#B = A implies A = B.
4. Let 0 < A, B ∈Mm. Show that the rank of the matrix
 A
A#B
A#B
B

is smaller than 2m.
5. Show that for any matrix mean m,
m(A, B)#m∩(A, B) = A#B.

224
5
Matrix Means and Inequalities
6. Let A ⊂0 and P be a projection of rank 1. Show that A#P =
→
Tr AP P.
7. Give an argument that the natural map
(A, B) ◦−≥exp
log A + log B
2
	
would not be a good deﬁnition of the geometric mean.
8. Show that for positive matrices A : B = A −A(A + B)−1 A.
9. Show that for positive matrices A : B ∈A.
10. Show that 0 < A ∈B implies A ∈2(A : B) ∈B.
11. Show that L(A, B) ∈(A + B)/2.
12. Let A, B > 0. Show that if for a matrix mean m f (A, B) = A, then A = B.
13. Let f, g : R+ ≥R+ be matrix monotone functions. Show that their arithmetic
and geometric means are matrix monotone as well.
14. Show that the matrix
Ai j =
1
Ht(εi, ε j)
deﬁned by the Heinz mean is positive.
15. Show that
λ
λt m(et A, et B)
⎥⎥⎥
t=0 = A + B
2
for a symmetric mean. (Hint: Prove the result for arithmetic and harmonic means
and reduce the general case to these examples.)
16. Let A and B be positive matrices and assume that there is a unitary U such that
A1/2U B1/2 ⊂0. Show that A#B = A1/2U B1/2.
17. Show that
S∗(A : B)S ∈(S∗AS) : (S∗BS)
for any invertible matrix S and A, B ⊂0.
18. Prove the property
(A : B) + (C : D) ∈(A + C) : (B + D)
of the parallel sum.
19. Prove the logarithmic mean formula
L(A, B)−1 =
⎢≤
0
(t A + B)−1
t + 1
dt
for positive deﬁnite matrices A, B.

5.6 Exercises
225
20. Let A and B be positive deﬁnite matrices. Set A0 := A, B0 := B and deﬁne
recursively
An = An−1 + Bn−1
2
and
Bn = 2(A−1
n−1 + B−1
n−1)−1
(n = 1, 2, . . .).
Show that
lim
n≥≤An = lim
n≥≤Bn = A#B.
21. Show that the function ft(x) deﬁned in (5.22) has the property
→x ∈ft(x) ∈1 + x
2
when 1/2 ∈t ∈2.
22. Let P and Q be ortho-projections. What is their Heinz mean?
23. Show that
det (A#B) =
→
det A det B.
24. Assume that A and B are invertible positive matrices. Show that
(A#B)−1 = A−1#B−1.
25. Let
A :=
3/2 0
0 3/4

and
B :=
1/2 1/2
1/2 1/2

.
Show that A ⊂B ⊂0 and for p > 1 the inequality Ap ⊂B p does not hold.
26. Show that
det

G(A, B, C)
	
=

det A det B det C
	1/3
.
27. Show that
G(αA, βB, πC) = (αβπ)1/3G(A, B, C)
for positive numbers α, β, π.
28. Show that A1 ⊂A2, B1 ⊂B2, C1 ⊂C2 imply
G(A1, B1, C1) ⊂G(A2, B2, C2).
29. Show that

226
5
Matrix Means and Inequalities
G(A, B, C) = G(A−1, B−1, C−1)−1.
30. Show that
3(A−1 + B−1 + C−1)−1 ∈G(A, B, C) ∈1
3(A + B + C).
31. Show that
fπ(x) = 22π−1xπ(1 + x)1−2π
is a matrix monotone function for 0 < π < 1.
32. Let P and Q be ortho-projections. Prove that L(P, Q) = P ⊃Q.
33. Show that the function
f p(x) =
x p + 1
2
1/p
is matrix monotone if and only if −1 ∈p ∈1.
34. For positive numbers a and b
lim
p≥0
a p + bp
2
1/p
=
→
ab.
Is it true that for 0 < A, B ∈Mn(C)
lim
p≥0
 Ap + B p
2
1/p
is the geometric mean of A and B?

Chapter 6
Majorization and Singular Values
A citation from von Neumann: “The object of this note is the study of certain prop-
erties of complex matrices of nth order: A = (ai j)n
i, j=1, n being a ﬁnite positive
integer: n = 1, 2, . . . . Together with them we shall use complex vectors of nth order
(in n dimensions): x = (xi)n
i=1.” This classical subject in matrix theory is exposed in
Sects.6.2 and 6.3 after discussions on vectors in Sect.6.1. This chapter also contains
several matrix norm inequalities as well as majorization results for matrices, which
were mostly developed more recently.
Basic properties of singular values of matrices are given in Sect.6.2. This sec-
tion also contains several fundamental majorizations, notably the Lidskii–Wielandt
and Gel’fand–Naimark theorems, for the eigenvalues of Hermitian matrices and the
singular values of general matrices. Section6.3 covers the important subject of sym-
metric or unitarily invariant norms for matrices. Symmetric norms are written as
symmetric gauge functions of the singular values of matrices (von Neumann’s the-
orem). So they are closely connected with majorization theory as manifestly seen
from the fact that the weak majorization s(A) ∈w s(B) for the singular value vec-
tors s(A), s(B) of matrices A, B is equivalent to the inequality |||A||| →|||B|||
for all symmetric norms, as summarized in Theorem 6.23. Therefore, the majoriza-
tion method is of particular use to obtain various symmetric norm inequalities for
matrices.
Section6.4 further collects several more recent results on majorization (and hence,
on symmetric norm inequalities) for positive matrices involving concave or convex
functions, or operator monotone functions, or certain matrix means. For instance, the
symmetric norm inequalities of Golden–Thompson type and of its complementary
type are presented.
6.1 Majorization of Vectors
Let a = (a1, . . . , an) and b = (b1, . . . , bn) be vectors in Rn. The decreasing
rearrangement of a is a≤= (a≤
1 , . . . , a≤
n ) and b≤= (b≤
1 , . . . , b≤
n ) is similarly
deﬁned. The majorization a ∈b means that
F. Hiai and D. Petz, Introduction to Matrix Analysis and Applications,
227
Universitext, DOI: 10.1007/978-3-319-04150-6_6,
© Hindustan Book Agency 2014

228
6
Majorization and Singular Values
k
⎡
i=1
a≤
i →
k
⎡
i=1
b≤
i
(1 →k →n)
(6.1)
and equality is required for k = n. The weak majorization a ∈w b is deﬁned by the
inequality (6.1), wherethe equality for k = n is not required. These concepts were
introduced by Hardy, Littlewood and Pólya.
The majorization a ∈b is equivalent to the statement that a is a convex combi-
nation of permutations of the components of the vector b. This can be written as
a =
⎡
U
λUUb,
where the summation is over the n × n permutation matrices U and λU ≥0,
⎢
U λU = 1. The n × n matrix D = ⎢
U λUU has the property that all entries
are positive and the sums of rows and columns are 1. Such a matrix D is called
doubly stochastic. So a = Db. The proof is a part of the next theorem.
Theorem 6.1 The following conditions for a, b ∗Rn are equivalent:
(1) a ∈b;
(2) ⎢n
i=1 |ai −r| →⎢n
i=1 |bi −r| for all r ∗R;
(3) ⎢n
i=1 f (ai) →⎢n
i=1 f (bi) for any convex function f on an interval containing
all ai, bi ;
(4) a is a convex combination of coordinate permutations of b;
(5) a = Db for some doubly stochastic n × n matrix D.
Proof: (1) ⇒(4). We show that there exist a ﬁnite number of matrices D1, . . . , DN
of the form λI + (1 −λ)α where 0 →λ →1 and α is a permutation matrix
interchanging two coordinates only such that a = DN · · · D1b. Then (4) follows
because DN · · · D1 becomes a convex combination of permutation matrices. We
may assume that a1 ≥· · · ≥an and b1 ≥· · · ≥bn. Suppose a ̸= b and choose the
largest j such that a j < b j. Then there exists a k with k > j such that ak > bk.
Choose the smallest such k. Let λ1 := 1−min{b j −a j, ak −bk}/(b j −bk) and α1 be
the permutation matrix interchanging the jth and kth coordinates. Then 0 < λ1 < 1
since b j > a j ≥ak > bk. Deﬁne D1 := λ1I + (1 −λ1)α1 and b(1) := D1b. Now
it is easy to check that a ∈b(1) ∈b and b(1)
1
≥· · · ≥b(1)
n . Moreover the jth or
kth coordinates of a and b(1) are equal. When a ̸= b(1), we can apply the above
argument to a and b(1). Repeating this ﬁnitely many times we reach the conclusion.
(4) ⇒(5) trivially follows from the fact that any convex combination of permu-
tation matrices is doubly stochastic.
(5) ⇒(2). For every r ∗R we have
n
⎡
i=1
|ai −r| =
n
⎡
i=1
⎣⎣⎣⎣
n
⎡
j=1
Di j(b j −r)
⎣⎣⎣⎣→
n
⎡
i, j=1
Di j|b j −r| =
n
⎡
j=1
|b j −r|.

6.1 Majorization of Vectors
229
(2) ⇒(1). Taking large r and small r in the inequality of (2) we have ⎢n
i=1 ai =
⎢n
i=1 bi. Noting that |x| + x = 2x+ for x ∗R, where x+ = max{x, 0}, we have
n
⎡
i=1
(ai −r)+ →
n
⎡
i=1
(bi −r)+
(r ∗R).
(6.2)
We now prove that (6.2) implies that a ∈w b. When b≤
k ≥r ≥b≤
k+1, ⎢k
i=1 a≤
i →
⎢k
i=1 b≤
i follows since
n
⎡
i=1
(ai −r)+ ≥
k
⎡
i=1
(a≤
i −r)+ ≥
k
⎡
i=1
a≤
i −kr,
n
⎡
i=1
(bi −r)+ =
k
⎡
i=1
b≤
i −kr.
(4) ⇒(3). Suppose that ai = ⎢N
k=1 λkbπk(i), 1 →i →n, where λk > 0,
⎢N
k=1 λk = 1, and πk are permutations on {1, . . . , n}. Then the convexity of f
implies that
n
⎡
i=1
f (ai) →
n
⎡
i=1
N
⎡
k=1
λk f (bπk(i)) =
n
⎡
i=1
f (bi).
(3) ⇒(5) is trivial since f (x) = |x −r| is convex.
□
Note that the implication (5) ⇒(4) can be seen directly from the well-known the-
orem of Birkhoff stating that any doubly stochastic matrix is a convex combination
of permutation matrices [27].
Example 6.2 Let D AB ∗Mn ≥Mm be a density matrix which is the convex combi-
nation of tensor products of density matrices: D AB = ⎢
i λi D A
i ≥DB
i . We assume
that the matrices D A
i are acting on the Hilbert space HA and DB
i acts on HB.
The eigenvalues of D AB form a probability vector r = (r1,r2, . . . ,rnm). The
reduced density matrix D A = ⎢
i λi(Tr DB
i )D A
i
has n eigenvalues and we add
nm −n zeros to get a probability vector q = (q1, q2, . . . , qnm). We want to show
that there is a doubly stochastic matrix S which transforms q into r. This means
r ∈q.
Let
D AB =
⎡
k
rk|ek∪⊥ek| =
⎡
j
p j|x j∪⊥x j| ≥|y j∪⊥y j|
bedecompositionsofadensitymatrixintermsofunitvectors|ek∪∗HA≥HB, |x j∪∗
HA and |y j∪∗HB. The ﬁrst decomposition is the Schmidt decomposition and the
second one is guaranteed by the assumed separability condition. For the reduced
density D A we have the Schmidt decomposition and another one:

230
6
Majorization and Singular Values
D A =
⎡
l
ql| fl∪⊥fl| =
⎡
j
p j|x j∪⊥x j|,
where f j is an orthonormal family in HA. According to Lemma 1.24 we have two
unitary matrices V and W such that
⎡
k
Vkj⊂p j|x j∪≥|y j∪= ⊂rk|ek∪
⎡
l
W jl
⊂ql| fl∪= ⊂p j|x j∪.
Combining these equations we obtain
⎡
k
Vkj
⎡
l
W jl
⊂ql| fl∪≥|y j∪= ⊂rk|ek∪
and taking the squared norm:
rk =
⎡
l
⎤⎡
j1, j2
V kj1Vkj2W j1lW j2l⊥y j1, y j2∪
⎥
ql.
We verify that the matrix
Skl =
⎤⎡
j1, j2
V kj1Vkj2W j1lW j2l⊥y j1, y j2∪
⎥
is doubly stochastic.
□
The weak majorization a ∈w b is deﬁned by the inequality (6.1). A matrix S
is called a doubly substochastic n × n matrix if ⎢n
j=1 Si j →1 for 1 →i →n and
⎢n
i=1 Si j →1 for 1 →j →n.
While the previous theorem concerned majorization, the next theorem is about
weak majorization.
Theorem 6.3 The following conditions for a, b ∗Rn are equivalent:
(1) a ∈w b;
(2) there exists a c ∗Rn such that a →c ∈b, where a →c means that ai →ci,
1 →i →n;
(3) ⎢n
i=1(ai −r)+ →⎢n
i=1(bi −r)+ for all r ∗R;
(4) ⎢n
i=1 f (ai) →⎢n
i=1 f (bi) for any increasing convex function f on an interval
containing all ai, bi.
Moreover, if a, b ≥0, then the above conditions are equivalent to the following:
(5) a = Sb for some doubly substochastic n × n matrix S.

6.1 Majorization of Vectors
231
Proof: (1) ⇒(2). By induction on n. We may assume that a1 ≥· · · ≥an and
b1 ≥· · · ≥bn. Let α := min1→k→n
⎦⎢k
i=1 bi −⎢k
i=1 ai

and deﬁne ˜a := (a1 +
α, a2, . . . , an). Then a →˜a ∈w b and ⎢k
i=1 ˜ai = ⎢k
i=1 bi for some 1 →k →n.
When k = n, a →˜a ∈b. When k < n, we have (˜a1, . . . , ˜ak) ∈(b1, . . . , bk)
and (˜ak+1, . . . , ˜an) ∈w (bk+1, . . . , bn). Hence the induction assumption implies
that (˜ak+1, . . . , ˜an) →(ck+1, . . . , cn) ∈(bk+1, . . . , bn) for some (ck+1, . . . , cn) ∗
Rn−k. Then a →(˜a1, . . . , ˜ak, ck+1, . . . , cn) ∈b is immediate from ˜ak ≥bk ≥
bk+1 ≥ck+1.
(2) ⇒(4). Let a →c ∈b. If f is increasing and convex on an interval [α, β]
containing ai, bi, then ci ∗[α, β] and
n
⎡
i=1
f (ai) →
n
⎡
i=1
f (ci) →
n
⎡
i=1
f (bi)
by Theorem 6.1.
(4) ⇒(3) is trivial and (3) ⇒(1) was already shown in the proof (2) ⇒(1) of
Theorem 6.1.
We now assume a, b ≥0 and prove that (2) √(5). If a →c ∈b, then we have,
by Theorem 6.1, c = Db for some doubly stochastic matrix D and ai = αici for
some 0 →αi →1. So a = Diag(α1, . . . , αn)Db and Diag(α1, . . . , αn)D is a doubly
substochastic matrix. Conversely if a = Sb for a doubly substochastic matrix S, then
a doubly stochastic matrix D exists so that S →D entrywise, the proof of which is
left as Exercise 1, and hence a →Db ∈b.
□
Example 6.4 Let a, b ∗Rn and f be a convex function on an interval containing all
ai, bi. We use the notation f (a) := ( f (a1), . . . , f (an)) and similarly f (b). Assume
that a ∈b. Since f is a convex function, so is ( f (x) −r)+ for any r ∗R. Hence
f (a) ∈w f (b) follows from Theorems 6.1 and 6.3.
Next assume that a ∈w b and f is an increasing convex function, then f (a) ∈w
f (b) can be proved similarly.
□
Let a, b ∗Rn and a, b ≥0. We deﬁne the weak log-majorization a ∈w(log) b
when
k	
i=1
a≤
i →
k	
i=1
b≤
i
(1 →k →n)
(6.3)
and the log-majorization a ∈(log) b when a ∈w(log) b and equality holds for
k = n in (6.3). It is obvious that if a and b are strictly positive, then a ∈(log) b
(resp., a ∈w(log) b) if and only if log a ∈log b (resp., log a ∈w log b), where
log a := (log a1, . . . , log an).

232
6
Majorization and Singular Values
Theorem 6.5 Let a, b ∗Rn with a, b ≥0 and assume that a ∈w(log) b. If f is a
continuous increasing function on [0, ≡) such that f (ex) is convex, then f (a) ∈w
f (b). In particular, a ∈w(log) b implies a ∈w b.
Proof: First assume that a, b ∗Rn are strictly positive and a ∈w(log) b, so that
log a ∈w log b. Since g ⊃h is convex when g and h are convex with g increasing, the
function ( f (ex) −r)+ is increasing and convex for any r ∗R. Hence by Theorem
6.3 we have
n
⎡
i=1
( f (ai) −r)+ →
n
⎡
i=1
( f (bi) −r)+,
which implies f (a) ∈w f (b) by Theorem 6.3 again. When a, b ≥0 and a ∈w(log) b,
we can choose a(m), b(m) > 0 such that a(m) ∈w(log) b(m), a(m) ∞a, and b(m) ∞b.
Since f (a(m)) ∈w f (b(m)) and f is continuous, we obtain f (a) ∈w f (b).
□
6.2 Singular Values
In this section we discuss the majorization theory for eigenvalues and singular values
of matrices. Our goal is to prove the Lidskii–Wielandt and Gel’fand–Naimark theo-
rems for singular values of matrices. These are the most fundamental majorizations
for matrices.
When A is self-adjoint, the vector of the eigenvalues of A in decreasing order with
counting multiplicities is denoted by λ(A). The majorization relation of self-adjoint
matrices also appears in quantum theory.
Example 6.6 In quantum theory the states are described by density matrices (pos-
itive matrices with trace 1). Let D1 and D2 be density matrices. The relation
λ(D1) ∈λ(D2) has the interpretation that D1 is more mixed than D2. Among
the n × n density matrices the “most mixed” has all eigenvalues equal to 1/n.
Let f : R+ ∞R+ be an increasing convex function with f (0) = 0. We show
that
λ(D) ∈λ( f (D)/Tr f (D))
(6.4)
for a density matrix D.
Set λ(D) = (λ1, λ2, . . . , λn). Under the hypothesis on f the inequality f (y)x ≥
f (x)y holds for 0 →x →y. Hence for i →j we have λ j f (λi) ≥λi f (λ j) and
⎦
f (λ1) + · · · + f (λk)

(λk+1 + · · · + λn)
≥(λ1 + · · · + λk)
⎦
f (λk+1) + · · · + f (λn)

.

6.2 Singular Values
233
Adding to both sides the term ( f (λ1) + · · · + f (λk))(λ1 + · · · + λk) we arrive at
( f (λ1) + · · · + f (λk))
n
⎡
i=1
λi ≥(λ1 + · · · + λk)
n
⎡
i=1
f (λi) .
This shows that the sum of the k largest eigenvalues of f (D)/Tr f (D) must exceed
that of D (which is λ1 + · · · + λk).
The canonical (Gibbs) state at inverse temperature β = (kT )−1 possesses the
density e−βH/Tr e−βH. Choosing f (x) = xβ∩/β with β∩> β the formula (6.4) tells
us that
e−βH/Tr e−βH ∈e−β∩H/Tr e−β∩H
that is, at higher temperatures the canonical density is more mixed.
□
Let H be an n-dimensional Hilbert space and A ∗B(H). Let s(A) = (s1(A), . . . ,
sn(A)) denote the vector of the singular values of A in decreasing order, i.e., s1(A) ≥
· · · ≥sn(A) are the eigenvalues of |A| = (A◦A)1/2, counting multiplicities.
The basic properties of the singular values are summarized in the next theorem,
which includes the deﬁnition of the minimax expression, see Theorem 1.27. Recall
that ∼· ∼is the operator norm.
Theorem 6.7 Let A, B, X, Y ∗B(H) and k, m ∗{1, . . . , n}. Then:
(1) s1(A) = ∼A∼.
(2) sk(αA) = |α|sk(A) for α ∗C.
(3) sk(A) = sk(A◦).
(4) Minimax expression:
sk(A) = min{∼A(I −P)∼: P is a projection, rank P = k −1}.
(6.5)
If A ≥0 then
sk(A) = min

max{⊥x, Ax∪: x ∗M⊥, ∼x∼= 1} :
M is a subspace of H, dim M = k −1}.
(6.6)
(5) Approximation number expression:
sk(A) = inf{∼A −X∼: X ∗B(H), rank X < k}.
(6.7)
(6) If 0 →A →B then sk(A) →sk(B).
(7) sk(X AY) →∼X∼∼Y∼sk(A).
(8) sk+m−1(A + B) →sk(A) + sm(B) if k + m −1 →n.
(9) sk+m−1(AB) →sn(A)sm(B) if k + m −1 →n.
(10) |sk(A) −sk(B)| →∼A −B∼.
(11) sk( f (A)) = f (sk(A)) if A ≥0 and f : R+ ∞R+ is an increasing function.

234
6
Majorization and Singular Values
Proof: First, we recall some basic decompositions of A ∗B(H). Let A = U|A|
be the polar decomposition of A and write the Schmidt decomposition of |A| as
|A| =
n
⎡
i=1
si(A)|ui∪⊥ui|,
where U is a unitary and {u1, . . . , un} is an orthonormal basis of H. From the polar
decomposition of A and the diagonalization of |A| one has the expression
A = U1Diag(s1(A), . . . , sn(A))U2
(6.8)
with unitaries U1,U2 ∗B(H), called the singular value decomposition of A, see
Theorem 1.46.
(1) follows since s1(A) = ∼|A| ∼= ∼A∼. (2) is clear from |αA| = |α| |A|. Also,
(3) immediately follows since the Schmidt decomposition of |A◦| is given by
|A◦| = U|A|U ◦=
n
⎡
i=1
si(A)|Uui∪⊥Uui|.
(4) Let αk be the right-hand side of (6.5). For 1 →k →n deﬁne Pk :=
⎢k
i=1 |ui∪⊥ui|, which is a projection of rank k. We have
αk →∼A(I −Pk−1)∼=

n
⎡
i=k
si(A)|ui∪⊥ui|
 = sk(A).
Conversely, for any ε > 0 choose a projection P with rank P = k −1 such that
∼A(I −P)∼< αk + ε. Then there exists a y ∗H with ∼y∼= 1 such that Pk y = y
but Py = 0. Since y = ⎢k
i=1⊥ui, y∪ui, we have
αk + ε > ∼|A|(I −P)y∼= ∼|A|y∼=

k
⎡
i=1
⊥ui, y∪si(A)ui

=
 k
⎡
i=1
|⊥ui, y∪|2si(A)2
1/2
≥sk(A).
Hence sk(A) = αk and the inﬁmum αk is attained by P = Pk−1.
Although the second expression (6.6) is included in Theorem 1.27, we give the
proof for convenience. When A ≥0, we have
sk(A) = sk(A1/2)2 = min{∼A1/2(I −P)∼2 : P is a projection, rank P = k −1}.

6.2 Singular Values
235
Since ∼A1/2(I −P)∼2 = maxx∗M⊥, ∼x∼=1⊥x, Ax∪with M := ran P, the latter
expression follows.
(5) Let βk be the right-hand side of (6.7). Let X := APk−1, where Pk−1 is as
in the above proof of (1). Then we have rank X →rank Pk−1 = k −1 so that
βk →∼A(I −Pk−1)∼= sk(A). Conversely, assume that X ∗B(H) has rank < k.
Since rank X = rank |X| = rank X◦, the projection P onto ran X◦has rank < k.
Then X(I −P) = 0 and by (6.5) we have
sk(A) →∼A(I −P)∼= ∼(A −X)(I −P)∼→∼A −X∼,
implying that sk(A) →βk. Hence sk(A) = βk and the inﬁmum βk is attained by
APk−1.
(6) is an immediate consequence of (6.6). It is immediate from (6.5) that sn(X A) →
∼X∼sn(A). Also sn(AY) = sn(Y ◦A◦) →∼Y∼sn(A) by (3). Hence (7) holds.
Next we show (8)–(10). By (6.7) there exist X, Y ∗B(H) with rank X < k,
rank Y < m such that ∼A−X∼= sk(A) and ∼B−Y∼= sm(B). Since rank (X+Y) →
rank X + rank Y < k + m −1, we have
sk+m−1(A + B) →∼(A + B) −(X + Y)∼< sk(A) + sm(B),
implying (8). For Z := X B + (A −X)Y we get
rank Z →rank X + rank Y < k + m −1,
∼AB −Z∼= ∼(A −X)(B −Y)∼→sk(A)sm(B).
These imply (9). Letting m = 1 and replacing B by B −A in (8) we get
sk(B) →sk(A) + ∼B −A∼,
which shows (10).
(11) When A ≥0 has the Schmidt decomposition A = ⎢n
i=1 si(A)|ui∪⊥ui|, we
have f (A) = ⎢n
i=1 f (si(A))|ui∪⊥ui|. Since f (s1(A)) ≥· · · ≥f (sn(A)) ≥0,
sk( f (A)) = f (sk(A)) follows.
□
The next result is called the Weyl majorization theorem, whose proof reveals
the usefulness of the antisymmetric tensor technique.
Theorem 6.8 Let A ∗Mn and λ1(A), · · · , λn(A) be the eigenvalues of A arranged
as |λ1(A)| ≥· · · ≥|λn(A)|, counting algebraic multiplicities. Then
k	
i=1
|λi(A)| →
k	
i=1
si(A)
(1 →k →n).

236
6
Majorization and Singular Values
Proof: If λ is an eigenvalue of A with algebraic multiplicity m, then there exists
a set {y1, . . . , ym} of independent vectors such that
Ay j −λy j ∗span{y1, . . . , y j−1}
(1 →j →m).
Hence one can choose independent vectors x1, . . . , xn so that Axi = λi(A)xi + zi
with zi ∗span{x1, . . . , xi−1} for 1 →i →n. Then it is readily checked that
A⊗k(x1 ⊗· · · ⊗xk) = Ax1 ⊗· · · ⊗Axk =
⎤
k	
i=1
λi(A)
⎥
x1 ⊗· · · ⊗xk
and x1 ⊗· · · ⊗xk ̸= 0, implying that ⎛k
i=1 λi(A) is an eigenvalue of A⊗k. Hence
Lemma 1.62 yields that
⎣⎣⎣⎣
k	
i=1
λi(A)
⎣⎣⎣⎣→∼A⊗k∼=
k	
i=1
si(A).
□
Note that another formulation of the previous theorem is
(|λ1(A)|, . . . , |λn(A)|) ∈w(log) s(A).
The following majorization results are the celebrated Lidskii–Wielandt theorem
for the eigenvalues of self-adjoint matrices as well as for the singular values of general
matrices.
Theorem 6.9 If A, B ∗Msa
n , then
λ(A) −λ(B) ∈λ(A −B),
or equivalently
(λi(A) + λn−i+1(B)) ∈λ(A + B).
Proof: What we need to prove is that for any choice of 1 →i1 < i2 < · · · < ik →n
we have
k
⎡
j=1
(λi j (A) −λi j (B)) →
k
⎡
j=1
λ j(A −B).
(6.9)
Choose the Schmidt decomposition of A −B as
A −B =
n
⎡
i=1
λi(A −B)|ui∪⊥ui|

6.2 Singular Values
237
for an orthonormal basis {u1, . . . , un} of Cn. We may assume without loss of gen-
erality that λk(A −B) = 0. In fact, we may replace B by B + λk(A −B)I, which
reduces both sides of (6.9) by kλk(A−B). In this situation, the Jordan decomposition
A −B = (A −B)+ −(A −B)−is given by
(A −B)+ =
k
⎡
i=1
λi(A −B)|ui∪⊥ui|,
(A −B)−= −
n
⎡
i=k+1
λi(A −B)|ui∪⊥ui|.
Since A = B + (A −B)+ −(A −B)−→B + (A −B)+, it follows from Theorem
1.27 that
λi(A) →λi(B + (A −B)+),
1 →i →n.
Since B →B + (A −B)+, we also have
λi(B) →λi(B + (A −B)+),
1 →i →n.
Hence
k
⎡
j=1
(λi j (A) −λi j (B)) →
k
⎡
j=1
(λi j (B + (A −B)+) −λi j (B))
→
n
⎡
i=1
(λi(B + (A −B)+) −λi(B))
= Tr (B + (A −B)+) −Tr B
= Tr (A −B)+ =
k
⎡
j=1
λ j(A −B),
proving (6.9). Moreover,
n
⎡
i=1
(λi(A) −λi(B)) = Tr (A −B) =
n
⎡
i=1
λi(A −B).
The latter expression is obvious since λi(B) = −λn−i+1(−B) for 1 →i →n.
□
Theorem 6.10 For all A, B ∗Mn
|s(A) −s(B)| ∈w s(A −B)

238
6
Majorization and Singular Values
holds, that is,
k
⎡
j=1
|si j (A) −si j (B)| →
k
⎡
j=1
s j(A −B)
for any choice of 1 →i1 < i2 < · · · < ik →n.
Proof: Deﬁne
A :=
⎝0 A◦
A 0
⎞
,
B :=
⎝0 B◦
B 0
⎞
.
Since
A◦A =
⎝A◦A
0
0
AA◦
⎞
,
|A| =
⎝|A|
0
0 |A◦|
⎞
,
it follows from Theorem 6.7(3) that
s(A) = (s1(A), s1(A), s2(A), s2(A), . . . , sn(A), sn(A)).
On the other hand, since
⎝I
0
0 −I
⎞
A
⎝I
0
0 −I
⎞
= −A,
we have λi(A) = λi(−A) = −λ2n−i+1(A) for n →i →2n. Hence one can write
λ(A) = (λ1, . . . , λn, −λn, . . . , −λ1),
where λ1 ≥· · · ≥λn ≥0. Since
s(A) = λ(|A|) = (λ1, λ1, λ2, λ2, . . . , λn, λn),
we have λi = si(A) for 1 →i →n and hence
λ(A) = (s1(A), . . . , sn(A), −sn(A), . . . , −s1(A)).
Similarly,
λ(B) = (s1(B), . . . , sn(B), −sn(B), . . . , −s1(B)),
λ(A −B) = (s1(A −B), . . . , sn(A −B), −sn(A −B), . . . , −s1(A −B)).
Theorem 6.9 implies that
λ(A) −λ(B) ∈λ(A −B).

6.2 Singular Values
239
Now we note that the components of λ(A) −λ(B) are
|s1(A) −s1(B)|, . . . , |sn(A) −sn(B)|, −|s1(A) −s1(B)|, . . . , −|sn(A) −sn(B)|.
Therefore, for any choice of 1 →i1 < i2 < · · · < ik →n with 1 →k →n, we have
k
⎡
j=1
|si j (A) −si j (B)| →
k
⎡
i=1
λi(A −B) =
k
⎡
j=1
s j(A −B),
the proof is complete.
□
The following results due to Ky Fan are weaker versions of the Lidskii–Wielandt
theorem; indeed, they are consequences of the above theorems.
Corollary 6.11 If A, B ∗Msa
n , then
λ(A + B) ∈λ(A) + λ(B).
Proof: Apply Theorem 6.9 to A + B and B. Then
k
⎡
i=1
⎤
λi(A + B) −λi(B)
⎥
→
k
⎡
i=1
λi(A)
so that
k
⎡
i=1
λi(A + B) →
k
⎡
i=1
⎤
λi(A) + λi(B)
⎥
.
Moreover, ⎢n
i=1 λi(A + B) = Tr (A + B) = ⎢n
i=1
⎦
λi(A) + λi(B)

.
□
Corollary 6.12 If A, B ∗Mn, then
s(A + B) ∈w s(A) + s(B).
Proof: Similarly, by Theorem 6.10,
k
⎡
i=1
|si(A + B) −si(B)| →
k
⎡
i=1
si(A)
so that
k
⎡
i=1
si(A + B) →
k
⎡
i=1
⎤
si(A) + si(B)
⎥
.
□

240
6
Majorization and Singular Values
Another important majorization for singular values of matrices is the Gel’fand–
Naimark theorem:
Theorem 6.13 For all A, B ∗Mn
(si(A)sn−i+1(B)) ∈(log) s(AB)
(6.10)
holds, or equivalently
k	
j=1
si j (AB) →
k	
j=1
(s j(A)si j (B))
(6.11)
for every 1 →i1 < i2 < · · · < ik →n with equality for k = n.
Proof: First assume that A and B are invertible matrices and let A = U1
Diag(s1, . . . , sn)U2 be the singular value decomposition (see (6.8)) with the singular
values s1 ≥· · · ≥sn > 0 of A and unitaries U1,U2. Write D := Diag(s1, . . . , sn).
Then s(AB) = s(U1DU2B) = s(DU2B) and s(B) = s(U2B), so we may re-
place A, B by D,U2B, respectively. Hence we may assume that A = D =
Diag(s1, . . . , sn). Moreover, to prove (6.11), it sufﬁces to assume that sk = 1. In
fact, when A is replaced by s−1
k
A, both sides of (6.11) are multiplied by the same
s−k
k . Deﬁne ˜A := Diag(s1, . . . , sk, 1, . . . , 1); then ˜A2 ≥A2 and ˜A2 ≥I. We notice
from Theorem 6.7 that we have
si(AB) = si((B◦A2B)1/2) = si(B◦A2B)1/2
→si(B◦˜A2B)1/2 = si( ˜AB)
for every i = 1, . . . , n and
si( ˜AB) = si(B◦˜A2B)1/2 ≥si(B◦B)1/2 = si(B).
Therefore, for any choice of 1 →i1 < · · · < ik →n, we have
k	
j=1
si j (AB)
si j (B) →
k	
j=1
si j ( ˜AB)
si j (B) →
n
	
i=1
si( ˜AB)
si(B) = det | ˜AB|
det |B|
=
⎠
det(B◦˜A2B)
⊂det(B◦B)
= det ˜A · | det B|
| det B|
= det ˜A =
k	
j=1
s j(A),
proving (6.11). By replacing A and B by AB and B−1, respectively, (6.11) is
rephrased as

6.2 Singular Values
241
k	
j=1
si j (A) →
k	
j=1
⎤
s j(AB)si j (B−1)
⎥
.
Since si(B−1) = sn−i+1(B)−1 for 1 →i →n as readily veriﬁed, the above inequality
means that
k	
j=1
⎤
si j (A)sn−i j+1(B)
⎥
→
k	
j=1
s j(AB).
Hence (6.11) implies (6.10) and vice versa (as long as A, B are invertible).
For general A, B ∗Mn choose a sequence of complex numbers αl ∗C\(σ(A)↑
σ(B)) suchthat αl ∞0. Since Al := A−αl I and Bl := B−αl I areinvertible, (6.10)
and (6.11) hold for those. Then si(Al) ∞si(A), si(Bl) ∞si(B) and si(Al Bl) ∞
si(AB) as l ∞≡for 1 →i →n. Hence (6.10) and (6.11) hold for general A, B. □
An immediate corollary of this theorem is the following majorization result due
to Horn.
Corollary 6.14 For all matrices A and B,
s(AB) ∈(log) s(A)s(B),
where s(A)s(B) = (si(A)si(B)).
Proof: A special case of (6.11) is
k	
i=1
si(AB) →
k	
i=1
⎤
si(A)si(B)
⎥
for every k = 1, . . . , n. Moreover,
n
	
i=1
si(AB) = det |AB| = det |A| · det |B| =
n
	
i=1
⎤
si(A)si(B)
⎥
.
□
6.3 Symmetric Norms
A norm σ : Rn ∞R+ is said to be symmetric if
σ(a1, a2, . . . , an) = σ(ε1aπ(1), ε2aπ(2), . . . , εnaπ(n))
(6.12)
for every (a1, . . . , an) ∗Rn, for any permutation π on {1, . . . , n} and εi = ±1. The
normalization is σ(1, 0, . . . , 0) = 1. Condition (6.12) is equivalently written as
σ(a) = σ(a◦
1, a◦
2, . . . , a◦
n)

242
6
Majorization and Singular Values
for a = (a1, . . . , an) ∗Rn, where (a◦
1, . . . , a◦
n) is the decreasing rearrangement of
(|a1|, . . . , |an|). A symmetric norm is often called a symmetric gauge function.
Typical examples of symmetric gauge functions on Rn are the ℓp-norms σp
deﬁned by
σp(a) :=



⎤n⎢
i=1
|ai|p⎥1/p
if 1 →p < ≡,
max{|ai| : 1 →i →n}
if p = ≡.
(6.13)
The next lemma characterizes the minimal and maximal normalized symmetric
norms.
Lemma 6.15 Let σ be a normalized symmetric norm on Rn. If a = (ai), b = (bi) ∗
Rn and |ai| →|bi| for 1 →i →n, then σ(a) →σ(b). Moreover,
max
1→i→n |ai| →σ(a) →
n
⎡
i=1
|ai|
(a = (ai) ∗Rn),
which means σ≡→σ →σ1.
Proof: In view of (6.12) we have
σ(αa1, a2, . . . , an) →σ(a1, a2, . . . , an) for 0 →α →1.
This is proved as follows:
σ(αa1, a2, . . . , an)
= σ
1 + α
2
a1 + 1 −α
2
(−a1), 1 + α
2
a2 + 1 −α
2
a2, . . . , 1 + α
2
an + 1 −α
2
an

→1 + α
2
σ(a1, a2, . . . , an) + 1 −α
2
σ(−a1, a2, . . . , an)
= σ(a1, a2, . . . , an).
(6.12) and the previous inequality imply that
|ai| = σ(ai, 0, . . . , 0) →σ(a).

6.3 Symmetric Norms
243
This means σ≡→σ. From
σ(a) →
n
⎡
i=1
σ(ai, 0, . . . , 0) =
n
⎡
i=1
|ai|
we have σ →σ1.
□
Lemma 6.16 If a = (ai), b = (bi) ∗Rn and (|a1|, . . . , |an|) ∈w (|b1|, . . . , |bn|),
then σ(a) →σ(b).
Proof: Theorem 6.3 implies that there exists a c ∗Rn such that
(|a1|, . . . , |an|) →c ∈(|b1|, . . . , |bn|).
Theorem 6.1 says that c is a convex combination of coordinate permutations of
(|b1|, . . . , |bn|). Lemma 6.15 and (6.12) imply that σ(a) →σ(c) →σ(b).
□
Let H be an n-dimensional Hilbert space. A norm ||| · ||| on B(H) is said to be
unitarily invariant if
|||U AV ||| = |||A|||
for all A ∗B(H) and all unitaries U, V ∗B(H). A unitarily invariant norm on
B(H) is also called a symmetric norm.The following fundamental theorem is due
to von Neumann.
Theorem 6.17 There is a bijective correspondence between symmetric gauge func-
tions σ on Rn and unitarily invariant norms ||| · ||| on B(H) determined by the
formula
|||A||| = σ(s(A))
(A ∗B(H)).
(6.14)
Proof: Assume that σ is a symmetric gauge function on Rn. Deﬁne ||| · ||| on
B(H) by the formula (6.14). Let A, B ∗B(H). Since s(A + B) ∈w s(A) + s(B)
by Corollary 6.12, it follows from Lemma 6.16 that
|||A + B||| = σ(s(A + B)) →σ(s(A) + s(B))
→σ(s(A)) + σ(s(B)) = |||A||| + |||B|||.
Furthermore, it is clear that |||A||| = 0 if and only if s(A) = 0 or A = 0. For α ∗C
we have
|||αA||| = σ(|α|s(A)) = |α| |||A|||
by Theorem 6.7. Hence ||| · ||| is a norm on B(H), which is unitarily invariant since
s(U AV ) = s(A) for all unitaries U, V .
Conversely, assume that ||| · ||| is a unitarily invariant norm on B(H). Choose an
orthonormal basis {e1, . . . , en} of H and deﬁne σ : Rn ∞R by

244
6
Majorization and Singular Values
σ(a) :=
⎣⎣⎣
⎣⎣⎣
⎣⎣⎣
n
⎡
i=1
ai|ei∪⊥ei|
⎣⎣⎣
⎣⎣⎣
⎣⎣⎣
(a = (ai) ∗Rn).
Then it is immediate that σ is a norm on Rn. For any permutation π on {1, . . . , n}
and εi = ±1, one can deﬁne unitaries U, V on H by Ueπ(i) = εiei and V eπ(i) = ei,
1 →i →n, so that
σ(a) =
⎣⎣⎣
⎣⎣⎣
⎣⎣⎣U
⎤n
⎡
i=1
aπ(i)|eπ(i)∪⊥eπ(i)|
⎥
V ◦⎣⎣⎣
⎣⎣⎣
⎣⎣⎣=
⎣⎣⎣
⎣⎣⎣
⎣⎣⎣
n
⎡
i=1
aπ(i)|Ueπ(i)∪⊥V eπ(i)|
⎣⎣⎣
⎣⎣⎣
⎣⎣⎣
=
⎣⎣⎣
⎣⎣⎣
n
⎡
i=1
εiaπ(i)|ei∪⊥ei|
⎣⎣⎣
⎣⎣⎣
⎣⎣⎣= σ(ε1aπ(1), ε2aπ(2), . . . , εnaπ(n)).
Hence σ is a symmetric gauge function. For any A ∗B(H) let A = U|A| be the polar
decomposition of A and |A| = ⎢n
i=1 si(A)|ui∪⊥ui| be the Schmidt decomposition of
|A| for an orthonormal basis {u1, . . . , un}. We have a unitary V deﬁned by V ei = ui,
1 →i →n. Since
A = U|A| = UV
 n
⎡
i=1
si(A)|ei∪⊥ei|

V ◦,
we have
σ(s(A)) =
⎣⎣⎣
⎣⎣⎣
⎣⎣⎣
n
⎡
i=1
si(A)|ei∪⊥ei|
⎣⎣⎣
⎣⎣⎣
⎣⎣⎣=
⎣⎣⎣
⎣⎣⎣
⎣⎣⎣UV
⎤n
⎡
i=1
si(A)|ei∪⊥ei|
⎥
V ◦⎣⎣⎣
⎣⎣⎣
⎣⎣⎣= |||A|||,
and so (6.14) holds. Therefore, the theorem is proved.
□
The next theorem summarizes properties of unitarily invariant (or symmetric)
norms on B(H).
Theorem 6.18 Let ||| · ||| be a unitarily invariant norm on B(H) corresponding to
a symmetric gauge function σ on Rn and A, B, X, Y ∗B(H). Then:
(1) |||A||| = |||A◦|||.
(2) |||XAY||| →∼X∼∼Y∼|||A|||.
(3) If s(A) ∈w s(B), then |||A||| →|||B|||.
(4) Under the normalization we have ∼A∼→|||A||| →∼A∼1.
Proof: By the deﬁnition (6.14), (1) follows from Theorem 6.7. By Theorem 6.7
and Lemma 6.15 we have (2) as
|||XAY||| = σ(s(X AY)) →σ(∼X∼∼Y∼s(A)) = ∼X∼∼Y∼|||A|||.
Moreover, (3) and (4) follow from Lemmas 6.16 and 6.15, respectively.
□

6.3 Symmetric Norms
245
For instance, for 1 →p →≡, we have the unitarily invariant norm ∼·∼p on B(H)
corresponding to the ℓp-norm σp in (6.13), that is, for A ∗B(H),
∼A∼p := σp(s(A)) =



⎤n⎢
i=1
si(A)p⎥1/p
= (Tr |A|p)1/p
if 1 →p < ≡,
s1(A) = ∼A∼
if p = ≡.
The norm ∼·∼p is called the Schatten–von Neumann p-norm. In particular, ∼A∼1 =
Tr |A| is the trace-norm, ∼A∼2 = (Tr A◦A)1/2 is the Hilbert–Schmidt norm and
∼A∼≡= ∼A∼is the operator norm. (For 0 < p < 1, we may deﬁne ∼· ∼p by the
same expression as above, but this is not a norm, and is called a quasi-norm.)
Another important class of unitarily invariant norms for n × n matrices are the
Ky Fan norms ∼· ∼(k) deﬁned by
∼A∼(k) :=
k
⎡
i=1
si(A)
for k = 1, . . . , n.
Obviously,∼·∼(1) istheoperatornormand∼·∼(n) isthetrace-norm.Inthenexttheorem
we give two variational expressions for the Ky Fan norms, which are sometimes quite
useful since the Ky Fan norms are essential in majorization and norm inequalities
for matrices.
The right-hand side of the second expression in the next theorem is known as the
K-functional in real interpolation theory.
Theorem 6.19 Let H be an n-dimensional space. For A ∗B(H) and k = 1, . . . , n,
we have:
(1) ∼A∼(k) = max{∼AP∼1 : P is a projection, rank P = k},
(2) ∼A∼(k) = min{∼X∼1 + k∼Y∼: A = X + Y}.
Proof: (1) For any projection P of rank k, we have
∼AP∼1 =
n
⎡
i=1
si(AP) =
k
⎡
i=1
si(AP) →
k
⎡
i=1
si(A)
by Theorem 6.7. For the converse, take the polar decomposition A = U|A| with
a unitary U and the spectral decomposition |A| = ⎢n
i=1 si(A)Pi with mutually
orthogonal projections Pi of rank 1. Let P := ⎢k
i=1 Pi. Then
∼AP∼1 = ∼U|A|P∼1 =

k
⎡
i=1
si(A)Pi

1
=
k
⎡
i=1
si(A) = ∼A∼(k).
(2) For any decomposition A = X + Y, since si(A) →si(X) + ∼Y∼by Theorem
6.7(10), we have

246
6
Majorization and Singular Values
∼A∼(k) →
k
⎡
i=1
si(X) + k∼Y∼→∼X∼1 + k∼Y∼
for any decomposition A = X + Y. Conversely, with the same notations as in the
proof of (1), deﬁne
X := U
k
⎡
i=1
(si(A) −sk(A))Pi,
Y := U
⎤
sk(A)
k
⎡
i=1
Pi +
n
⎡
i=k+1
si(A)Pi
⎥
.
Then X + Y = A and
∼X∼1 =
k
⎡
i=1
si(A) −ksk(A),
∼Y∼= sk(A).
Hence ∼X∼1 + k∼Y∼= ⎢k
i=1 si(A).
□
The following is a modiﬁcation of the above expression in (1):
∼A∼(k) = max{|Tr (U AP)| : U a unitary, P a projection, rank P = k}.
Here we prove the Hölder inequality for matrices to illustrate the usefulness of
the majorization technique.
Theorem 6.20 Let 0 < p, p1, p2 →≡and 1/p = 1/p1 + 1/p2. Then
∼AB∼p →∼A∼p1∼B∼p2,
A, B ∗B(H).
Proof: When p1 = ≡or p2 = ≡, the result is obvious. Assume that 0 <
p1, p2 < ≡. Since Corollary 6.14 implies that
(si(AB)p) ∈(log) (si(A)psi(B)p),
it follows from Theorem 6.5 that
(si(AB)p) ∈w (si(A)psi(B)p).
Since (p1/p)−1 +(p2/p)−1 = 1, the usual Hölder inequality for vectors shows that
∼AB∼p =
⎤
n
⎡
i=1
si(AB)p⎥1/p
→
⎤
n
⎡
i=1
si(A)psi(B)p⎥1/p

6.3 Symmetric Norms
247
→
⎤
n
⎡
i=1
si(A)p1
⎥1/p1⎤
n
⎡
i=1
si(B)p2
⎥1/p2 →∼A∼p1∼B∼p2.
□
Corresponding to each symmetric gauge function σ, deﬁne σ∩: Rn ∞R by
σ∩(b) := sup

n
⎡
i=1
aibi : a = (ai) ∗Rn, σ(a) →1

for b = (bi) ∗Rn.
Then σ∩is a symmetric gauge function again, which is said to be dual to σ. For
example, when 1 →p →≡and 1/p + 1/q = 1, the ℓp-norm σp is dual to the
ℓq-norm σq.
The following is another generalized Hölder inequality, which can be proved
similarly to Theorem 6.20.
Lemma 6.21 Let σ, σ1 and σ2 be symmetric gauge functions with the correspond-
ing unitarily invariant norms ||| · |||, ||| · |||1 and ||| · |||2 on B(H), respectively. If
σ(ab) →σ1(a)σ2(b),
a, b ∗Rn,
then
|||AB||| →|||A|||1|||B|||2,
A, B ∗B(H).
In particular, if ||| · |||∩is the unitarily invariant norm corresponding to σ∩dual
to σ, then
∼AB∼1 →|||A||| |||B|||∩,
A, B ∗B(H).
Proof: By Corollary 6.14, Theorem 6.5, and Lemma 6.16, we have
σ(s(AB)) →σ(s(A)s(B)) →σ1(s(A))σ2(s(B)) →|||A|||1|||B|||2,
proving the ﬁrst assertion. For the second part, note by deﬁnition of σ∩that σ1(ab) →
σ(a)σ∩(b) for a, b ∗Rn.
□
Theorem 6.22 Let σ and σ∩be dual symmetric gauge functions on Rn with the
corresponding norms ||| · ||| and ||| · |||∩on B(H), respectively. Then ||| · ||| and
||| · |||∩are dual with respect to the duality (A, B) ↓∞Tr AB for A, B ∗B(H),
that is,
|||B|||∩= sup{|Tr AB| : A ∗B(H), |||A||| →1},
B ∗B(H).
(6.15)
Proof: First note that any linear functional on B(H) is represented as A ∗
B(H) ↓∞Tr AB for some B ∗B(H). We write |||B|||⊃for the right-hand side
of (6.15). From Lemma 6.21 we have
|Tr AB| →∼AB∼1 →|||A||| |||B|||∩

248
6
Majorization and Singular Values
so that |||B|||⊃→|||B|||∩for all B ∗B(H). On the other hand, let B = V |B| be the
polar decomposition and |B| = ⎢n
i=1 si(B)|vi∪⊥vi| be the Schmidt decomposition
of |B|. For any a = (ai) ∗Rn with σ(a) →1, let A :=
⎦⎢n
i=1 ai|vi∪⊥vi|

V ◦.
Then s(A) = s
⎦⎢n
i=1 ai|vi∪⊥vi|

= (a◦
1, . . . , a◦
n), the decreasing rearrangement of
(|a1|, . . . , |an|), and hence |||A||| = σ(s(A)) = σ(a) →1. Moreover,
Tr AB = Tr
 n
⎡
i=1
ai|vi∪⊥vi|
 n
⎡
i=1
si(B)|vi∪⊥vi|

= Tr
 n
⎡
i=1
aisi(B)|vi∪⊥vi|

=
n
⎡
i=1
aisi(B)
so that
n
⎡
i=1
aisi(B) →|Tr AB| →|||A||| |||B|||⊃→|||B|||⊃.
This implies that |||B|||∩= σ∩(s(B)) →|||B|||⊃.
□
As special cases we have ∼· ∼∩
p = ∼· ∼q when 1 →p →≡and 1/p + 1/q = 1.
The close relation between the (log-)majorization and the unitarily invariant norm
inequalities is summarized in the following proposition.
Theorem 6.23 Consider the following conditions for A, B ∗B(H):
(i) s(A) ∈w(log) s(B);
(ii) ||| f (|A|)||| →||| f (|B|)||| for every unitarily invariant norm ||| · ||| and every
continuous increasing function f : R+ ∞R+ such that f (ex) is convex;
(iii) s(A) ∈w s(B);
(iv) ∼A∼(k) →∼B∼(k) for every k = 1, . . . , n;
(v) |||A||| →|||B||| for every unitarily invariant norm ||| · |||;
(vi) ||| f (|A|)||| →||| f (|B|)||| for every unitarily invariant norm ||| · ||| and every
continuous increasing convex function f : R+ ∞R+.
Then
(i) ∧⇒(ii) =⇒(iii) ∧⇒(iv) ∧⇒(v) ∧⇒(vi).
Proof: (i) ⇒(ii). Let f be as in (ii). By Theorems 6.5 and 6.7(11) we have
s( f (|A|)) = f (s(A)) ∈w f (s(B)) = s( f (|B|)).
(6.16)
This implies by Theorem 6.18(3) that ||| f (|A|)||| →||| f (|B|)||| for any unitarily
invariant norm.
(ii) ⇒(i). Let ||| · ||| be the Ky Fan norm ∼· ∼(k), and f (x) = log(1 + ε−1x) for
ε > 0. Then f satisﬁes the condition in (ii). Since
si( f (|A|)) = f (si(A)) = log(ε + si(A)) −log ε,

6.3 Symmetric Norms
249
the inequality ∼f (|A|)∼(k) →∼f (|B|)∼(k) means that
k	
i=1
(ε + si(A)) →
k	
i=1
(ε + si(B)).
Letting ε ∨0 gives ⎛k
i=1 si(A) →⎛k
i=1 si(B) and hence (i) follows.
(i) ⇒(iii) follows from Theorem 6.5. (iii) √(iv) is trivial by deﬁnition of ∼· ∼(k)
and (vi) ⇒(v) ⇒(iv) is clear. Finally assume (iii) and let f be as in (vi). Theorem
6.7 yields (6.16) again, so that (vi) follows. Hence (iii) ⇒(vi) holds.
□
By Theorems 6.9, 6.10 and 6.23 we have:
Corollary 6.24 For A, B ∗Mn and a unitarily invariant norm |||·|||, the inequality
|||Diag(s1(A) −s1(B), . . . , sn(A) −sn(B))||| →|||A −B|||
holds. If A and B are self-adjoint, then
|||Diag(λ1(A) −λ1(B), . . . , λn(A) −λn(B))||| →|||A −B|||.
The following statements are particular cases for self-adjoint matrices:

n
⎡
i=1
|λi(A) −λi(B)|p
1/p
→∼A −B∼p
(1 →p < ≡).
The following is called Weyl’s inequality:
max
1→i→n |λi(A) −λi(B)| →∼A −B∼.
There are similar inequalities in the general case, where λi is replaced by si.
In the rest of this section we prove symmetric norm inequalities (or eigenvalue
majorizations) involving convex/concave functions and expansions. Recall that an
operato Z is called an expansion if Z◦Z ≥I.
Theorem 6.25 Let f : R+ ∞R+ be a concave function. If 0 →A ∗Mn and
Z ∗Mn is an expansion, then
||| f (Z◦AZ)||| →|||Z◦f (A)Z|||
for every unitarily invariant norm ||| · |||, or equivalently,
λ( f (Z◦AZ)) ∈w λ(Z◦f (A)Z).

250
6
Majorization and Singular Values
Proof: Note that f is automatically non-decreasing. By Theorem 6.22 it sufﬁces
to prove the inequality for the Ky Fan k-norms ∼· ∼(k), 1 →k →n. Letting f0(x) :=
f (x) −f (0) we have
f (Z◦AZ) = f (0)I + f0(Z◦AZ),
Z◦f (A)Z = f (0)Z◦Z + Z◦f0(A)Z ≥f (0)I + Z◦f0(A)Z,
which show that we may assume that f (0) = 0. Then there is a spectral projection
E of rank k for Z◦AZ such that
∼f (Z◦AZ)∼(k) =
k
⎡
j=1
f (λ j(Z◦AZ)) = Tr f (Z◦AZ)E.
When we show that
Tr f (Z◦AZ)E →Tr Z◦f (A)Z E,
(6.17)
it follows that
∼f (Z◦AZ)∼(k) →Tr Z◦f (A)Z E →∼Z◦f (A)Z∼(k)
by Theorem 6.19. For (6.17) we may show that
Tr g(Z◦AZ)E ≥Tr Z◦g(A)Z E
(6.18)
for every convex function on R+ with g(0) = 0. Such a function g can be approxi-
mated by functions of the form
αx +
m
⎡
i=1
αi(x −βi)+
(6.19)
with α ∗R and αi, βi > 0, where (x −β)+ := max{0, x −β}. Consequently, it
sufﬁces to show (6.18) for gβ(x) := (x −β)+ with β > 0. From the lemma below
we have a unitary U such that
gβ(Z◦AZ) ≥U ◦Z◦gβ(A)ZU.
We hence have
Tr gβ(Z◦AZ)E =
k
⎡
j=1
λ j(gβ(Z◦AZ)) ≥
k
⎡
j=1
λ j(U ◦Z◦gβ(A)ZU)

6.3 Symmetric Norms
251
=
k
⎡
j=1
λ j(Z◦gβ(A)Z) ≥Tr Z◦gβ(A)Z E,
that is (6.18) for g = gβ.
□
Lemma 6.26 Let A ∗M+
n , Z ∗M be an expansion, and β > 0. Then there exists
a unitary U such that
(Z◦AZ −βI)+ ≥U ◦Z◦(A −βI)+ZU.
Proof: Let P be the support projection of (A −βI)+ and set Aβ := P A. Let
Q be the support projection of Z◦Aβ Z. Since Z◦AZ ≥Z◦Aβ Z and (x −β)+ is a
non-decreasing function, for 1 →j →n we have
λ j((Z◦AZ −βI)+) = (λ j(Z◦AZ) −β)+
≥(λ j(Z◦Aβ Z) −β)+
= λ j((Z◦Aβ Z −βI)+).
So there exists a unitary U such that
(Z◦AZ −βI)+ ≥U ◦(Z◦Aβ Z −βI)+U.
It is obvious that Q is the support projection of Z◦P Z. Also, note that Z◦P Z is
unitarily equivalent to P Z Z◦P. Since Z◦Z ≥I, it follows that Z Z◦≥I and so
P Z Z◦P ≥P. Therefore, we have Q →Z◦P Z. Since Z◦Aβ Z ≥βZ◦P Z ≥βQ,
we see that
(Z◦Aβ Z −βI)+ = Z◦Aβ Z −βQ ≥Z◦Aβ Z −βZ◦P Z
= Z◦(Aβ −βP)Z = Z◦(A −βI)+Z,
which gives the conclusion.
□
When f is convex with f (0) = 0, the inequality in Theorem 6.25 is reversed.
Theorem 6.27 Let f : R+ ∞R+ be a convex function with f (0) = 0. If 0 →A ∗
Mn and Z ∗Mn is an expansion, then
||| f (Z◦AZ)||| ≥|||Z◦f (A)Z|||
for every unitarily invariant norm ||| · |||.
Proof: By approximation we may assume that f is of the form (6.19) with α ≥0
and αi, βi > 0. By Lemma 6.26 we have

252
6
Majorization and Singular Values
Z◦f (A)Z = αZ◦AZ +
⎡
i
αi Z◦(A −βi I)+Z
→αZ◦AZ +
⎡
i
αiUi(Z◦AZ −βi I)+U ◦
i
for some unitaries Ui, 1 →i →m. We now consider the Ky Fan k-norms ∼· ∼(k). For
each k = 1, . . . , n there is a projection E of rank k so that
αZ◦AZ +
⎡
i
αiUi(Z◦AZ −βi I)+U ◦
i

(k)
= Tr

αZ◦AZ +
⎡
i
αiUi(Z◦AZ −βi I)+U ◦
i

E
= αTr Z◦AZ E +
⎡
i
αiTr (Z◦AZ −βi I)+U ◦
i EUi
→α∼Z◦AZ∼(k) +
⎡
i
αi∼(Z◦AZ −βi I)+∼(k)
=
k
⎡
j=1

αλ j(Z◦AZ) +
⎡
i
αi(λ j(Z◦AZ) −βi)+

=
k
⎡
j=1
f (λ j(Z◦AZ)) = ∼f (Z◦AZ)∼(k),
and hence ∼Z◦f (A)Z∼(k) →∼f (Z◦AZ)∼(k). This implies the conclusion.
□
For the trace function the non-negativity assumption of f is not necessary so that
we have:
Theorem 6.28 Let 0 →A ∗Mn and Z ∗Mn be an expansion. If f is a concave
function on R+ with f (0) ≥0, then
Tr f (Z◦AZ) →Tr Z◦f (A)Z.
If f is a convex function on R+ with f (0) →0, then
Tr f (Z◦AZ) ≥Tr Z◦f (A)Z.
Proof: The two assertions are obviously equivalent. To prove the second, by
approximation we may assume that f is of the form (6.19) with α ∗R and αi, βi > 0.
Then, by Lemma 6.26,

6.3 Symmetric Norms
253
Tr f (Z◦AZ) = Tr

αZ◦AZ +
⎡
i
αi(Z◦AZ −βi I)+

≥Tr

αZ◦AZ +
⎡
i
αi Z◦(A −βi I)+Z

= Tr Z◦f (A)Z
and the statement is proved.
□
6.4 More Majorizations for Matrices
In the ﬁrst part of this section, we prove a subadditivity property for certain symmetric
norm functions. Let f : R+ ∞R+ be a concave function. Then f is increasing and
it is easy to show that f (a + b) →f (a) + f (b) for positive numbers a and b. The
Rotfel’d inequality
Tr f (A + B) →Tr ( f (A) + f (B))
(A, B ∗M+
n )
is a matrix extension. Another extension is
||| f (A + B)||| →||| f (A) + f (B)|||
(6.20)
for all 0 →A, B ∗Mn and for any unitarily invariant norm ||| · |||, which will be
proved in Theorem 6.33 below.
Lemma 6.29 Let g : R+ ∞R+ be a continuous function. If g is decreasing and
xg(x) is increasing, then
λ((A + B)g(A + B)) ∈w λ
⎦
A1/2g(A + B)A1/2 + B1/2g(A + B)B1/2
for all A, B ∗M+
n .
Proof: Let λ(A+B) = (λ1, . . . , λn) be the eigenvalue vector arranged in decreas-
ing order and u1, . . . , un be the corresponding eigenvectors forming an orthonormal
basis of Cn. For 1 →k →n let Pk be the orthogonal projection onto the subspace
spanned by u1, . . . , uk. Since xg(x) is increasing, it follows that
λ((A + B)g(A + B)) = (λ1g(λ1), . . . , λng(λn)).
Hence, what we need to prove is
Tr (A + B)g(A + B)Pk →Tr
⎤
A1/2g(A + B)A1/2 + B1/2g(A + B)B1/2⎥
Pk,

254
6
Majorization and Singular Values
since the left-hand side is equal to ⎢k
i=1 λig(λi) and the right-hand side is less than
or equal to ⎢k
i=1 λi
⎦
A1/2g(A+ B)A1/2+ B1/2g(A+ B)B1/2
. The above inequality
immediately follows by summing the following two inequalities:
Tr g(A + B)1/2 Ag(A + B)1/2Pk →Tr A1/2g(A + B)A1/2Pk,
(6.21)
Tr g(A + B)1/2Bg(A + B)1/2Pk →Tr B1/2g(A + B)B1/2Pk.
(6.22)
To prove (6.21), we write Pk, H := g(A + B) and A1/2 as
Pk =
⎝IK 0
0 0
⎞
,
H =
⎝H1 0
0 H2
⎞
,
A1/2 =
⎝A11 A12
A◦
12 A22
⎞
in the form of 2 × 2 block matrices corresponding to the orthogonal decomposition
Cn = K ⊕K⊥with K := PkCn. Then
Pkg(A + B)1/2 Ag(A + B)1/2Pk =
⎝
H1/2
1
A2
11H1/2
1
+ H1/2
1
A12 A◦
12H1/2
1
0
0
0
⎞
,
Pk A1/2g(A + B)A1/2Pk =
⎝A11H1A11 + A12H2 A◦
12 0
0
0
⎞
.
Since g is decreasing, we notice that
H1 →g(λk)IK,
H2 ≥g(λk)IK⊥.
Therefore, we have
Tr H1/2
1
A12 A◦
12H1/2
1
= Tr A◦
12H1A12 →g(λk)Tr A◦
12 A12
= g(λk)Tr A12 A◦
12 →Tr A12H2 A◦
12
so that
Tr
⎦
H1/2
1
A2
11H1/2
1
+ H1/2
1
A12 A◦
12H1/2
1

→Tr
⎦
A11H1 A11 + A12H2 A◦
12

,
which proves (6.21). (6.22) is proved similarly.
□
In the next result matrix concavity is assumed.
Theorem 6.30 Let f : R+ ∞R+ be a continuous matrix monotone (equivalently,
matrix concave) function. Then (6.20) holds for all 0 →A, B ∗Mn and for any
unitarily invariant norm ||| · |||.
Proof:Bycontinuitywemayassumethat A, B areinvertible.Letg(x) := f (x)/x;
then g satisﬁes the assumptions of Lemma 6.29. Hence the lemma implies that

6.4 More Majorizations for Matrices
255
||| f (A + B)||| →|||A1/2(A + B)−1/2 f (A + B)(A + B)−1/2 A1/2
+B1/2(A + B)−1/2 f (A + B)(A + B)−1/2B1/2|||.
(6.23)
Since C := A1/2(A+ B)−1/2 is a contraction, Theorem 4.23 implies from the matrix
concavity that
A1/2(A + B)−1/2 f (A + B)(A + B)−1/2 A1/2
= C f (A + B)C◦→f (C(A + B)C◦) = f (A),
and similarly
B1/2(A + B)−1/2 f (A + B)(A + B)−1/2B1/2 →f (B).
Therefore, the right-hand side of (6.23) is less than or equal to ||| f (A) + f (B)|||.
□
A particular case of the next theorem is |||(A+B)m||| ≥|||Am +Bm||| for m ∗N,
which was proved by Bhatia and Kittaneh in [23].
Theorem 6.31 Let g : R+ ∞R+ be an increasing bijective function whose inverse
function is operator monotone. Then
|||g(A + B)||| ≥|||g(A) + g(B)|||
(6.24)
for all 0 →A, B ∗Mn and ||| · |||.
Proof: Let f be the inverse function of g. For every 0 →A, B ∗Mn, Theorem
6.30 implies that
f (λ(A + B)) ∈w λ( f (A) + f (B)).
Now, replace A and B by g(A) and g(B), respectively. Then we have
f (λ(g(A) + g(B))) ∈w λ(A + B).
Since f is concave and hence g is convex (and increasing), we have by Example 6.4
λ(g(A) + g(B)) ∈w g(λ(A + B)) = λ(g(A + B)),
which means by Theorem 6.23 that |||g(A) + g(B)||| →|||g(A + B)|||.
□
The above theorem can be extended to the next theorem due to Kosem [57], which
is the ﬁrst main result of this section. The simpler proof below is from [30].
Theorem 6.32 Let g : R+ ∞R+ be a continuous convex function with g(0) = 0.
Then (6.24) holds for all A, B and ||| · ||| as above.

256
6
Majorization and Singular Values
Proof: First, note that a convex function g ≥0 on R+ with g(0) = 0 is non-
decreasing. Let  denote the set of all non-negative functions g on R+ for which
the conclusion of the theorem holds. It is obvious that  is closed under pointwise
convergence and multiplication by non-negative scalars. When f, g ∗, for the Ky
Fan norms ∼· ∼(k), 1 →k →n, and for 0 →A, B ∗Mn we have
∼( f + g)(A + B)∼(k) = ∼f (A + B)∼(k) + ∼g(A + B)∼(k)
≥∼f (A) + f (B)∼(k) + ∼g(A) + g(B)∼(k)
≥∼( f + g)(A) + ( f + g)(B)∼(k),
where the above equality is guaranteed by the fact that f and g are non-decreasing
and the latter inequality is the triangle inequality. Hence f + g ∗ by Theorem
6.23 so that  is a convex cone. Notice that any convex function g ≥0 on R+ with
g(0) = 0 is the pointwise limit of an increasing sequence of functions of the form
⎢m
l=1 clγal(x) with cl, al > 0, where γa is the angle function at a > 0 given as
γa(x) := max{x −a, 0}. Hence it sufﬁces to show that γa ∗ for all a > 0. To do
this, for a,r > 0 we deﬁne
ha,r(x) := 1
2
⎤
(x −a)2 + r + x −

a2 + r
⎥
,
x ≥0,
which is an increasing bijective function on R+ and whose inverse is
x −
r/2
2x +
⊂
a2 + r −a
+
⊂
a2 + r + a
2
.
(6.25)
Since (6.25) is operator monotone on R+, we have ha,r ∗ by Theorem 6.31.
Therefore, γa ∗ since ha,r ∞γa as r ∨0.
□
The next subadditivity inequality extending Theorem 6.30 was proved by Bourin
and Uchiyama in [30], and is the second main result.
Theorem 6.33 Let f : R+ ∞R+ be a continuous concave function. Then (6.20)
holds for all A, B and ||| · ||| as above.
Proof: Let λi and ui, 1 →i →n, be taken as in the proof of Lemma 6.29, and Pk,
1 →k →n, also be as deﬁned there. We may prove the weak majorization
k
⎡
i=1
f (λi) →
k
⎡
i=1
λi( f (A) + f (B))
(1 →k →n).
To do this, it sufﬁces to show that
Tr f (A + B)Pk →Tr ( f (A) + f (B))Pk.
(6.26)

6.4 More Majorizations for Matrices
257
Indeed, since a concave f is necessarily increasing, the left-hand side of (6.26) is
⎢k
i=1 f (λi) and the right-hand side is less than or equal to ⎢k
i=1 λi( f (A) + f (B)).
Here, note by Exercise 12 that f is the pointwise limit of a sequence of functions of
the form α + βx −g(x) where α ≥0, β > 0, and g ≥0 is a continuous convex
function on R+ with g(0) = 0. Hence, to prove (6.26), it sufﬁces to show that
Tr g(A + B)Pk ≥Tr (g(A) + g(B))Pk
for any continuous convex function g ≥0 on R+ with g(0) = 0. In fact, this is seen
as follows:
Tr g(A + B)Pk = ∼g(A + B)∼(k) ≥∼g(A) + g(B)∼(k) ≥Tr (g(A) + g(B))Pk,
where the above equality is due to the fact that g is increasing and the ﬁrst inequality
follows from Theorem 6.32.
□
ThesubadditivityinequalityofTheorem6.32wasfurtherextendedbyJ.-C.Bourin
in such a way that if f is a positive continuous concave function on R+ then
||| f (|A + B|)||| →||| f (|A|) + f (|B|)|||
for all normal matrices A, B ∗Mn and for any unitarily invariant norm ||| · |||. In
particular,
||| f (|Z|)||| →||| f (|A|) + f (|B|)|||
when Z = A + i B is the Descartes decomposition of Z.
In the second part of this section, we prove the inequality between norms of
f (|A −B|) and f (A) −f (B) (or the weak majorization for their singular values)
when f is a positive operator monotone function on R+ and A, B ∗M+
n . We ﬁrst
prepare some simple facts for the next theorem.
Lemma 6.34 For self-adjoint X, Y ∗Mn, let X = X+ −X−and Y = Y+ −Y−be
the Jordan decompositions.
(1) If X →Y then si(X+) →si(Y+) for all i.
(2) If s(X+) ∈w s(Y+) and s(X−) ∈w s(Y−), then s(X) ∈w s(Y).
Proof: (1) Let Q be the support projection of X+. Since
X+ = QX Q →QY Q →QY+Q,
we have si(X+) →si(QY+Q) →si(Y+) by Theorem 6.7(7).
(2) It is rather easy to see that s(X) is the decreasing rearrangement of the
combination of s(X+) and s(X−). Hence for each k ∗N we can choose 0 →m →k
so that

258
6
Majorization and Singular Values
k
⎡
i=1
si(X) =
m
⎡
i=1
si(X+) +
k−m
⎡
i=1
si(X−).
Hence
k
⎡
i=1
si(X) →
m
⎡
i=1
si(Y+) +
k−m
⎡
i=1
si(Y−) →
k
⎡
i=1
si(Y),
as desired.
□
Theorem 6.35 Let f : R+ ∞R+ be a matrix monotone function. Then
||| f (A) −f (B)||| →||| f (|A −B|)|||
for all 0 →A, B ∗Mn and for any unitarily invariant norm ||| · |||. Equivalently,
s( f (A) −f (B)) ∈w s( f (|A −B|))
(6.27)
holds.
Proof: First assume that A ≥B ≥0 and let C := A−B ≥0. In view of Theorem
6.23, it sufﬁces to prove that
∼f (B + C) −f (B)∼(k) →∼f (C)∼(k)
(1 →k →n).
(6.28)
For each λ ∗(0, ≡) let
hλ(x) :=
x
x + λ = 1 −
λ
x + λ,
which is increasing on R+ with hλ(0) = 0. According to the integral representation
(4.19) for f with a, b ≥0 and a positive measure μ on (0, ≡), we have
si( f (C)) = f (si(C))
= a + bsi(C) +

(0,≡)
λsi(C)
si(C) + λ dμ(λ)
= a + bsi(C) +

(0,≡)
λsi(hλ(C)) dμ(λ),
so that
∼f (C)∼(k) ≥b∼C∼(k) +

(0,≡)
λ∼hλ(C)∼(k) dμ(λ).
(6.29)
On the other hand, since

6.4 More Majorizations for Matrices
259
f (B + C) = aI + b(B + C) +

(0,≡)
λhλ(B + C) dμ(λ)
as well as the analogous expression for f (B), we have
f (B + C) −f (B) = bC +

(0,≡)
λ(hλ(B + C) −hλ(B)) dμ(λ),
so that
∼f (B + C) −f (B)∼(k) →b∼C∼(k) +

(0,≡)
λ∼hλ(B + C) −hλ(B)∼(k) dμ(λ).
By this inequality and (6.29), it sufﬁces for (6.28) to show that
∼hλ(B + C) −hλ(B)∼(k) →∼hλ(C)∼(k)
(λ ∗(0, ≡), 1 →k →n).
As hλ(x) = h1(x/λ), it is enough to show this inequality for the case λ = 1 since
we may replace B and C with λ−1B and λ−1C, respectively. Thus, what remains to
be proven is the following:
∼(B + I)−1 −(B + C + I)−1∼(k) →∼I −(C + I)−1∼(k)
(1 →k →n).
(6.30)
Since
(B+I)−1−(B+C +I)−1 = (B+I)−1/2h1((B+I)−1/2C(B+I)−1/2)(B+I)−1/2
and ∼(B + I)−1/2∼→1, we obtain
si((B + I)−1 −(B + C + I)−1) →si(h1((B + I)−1/2C(B + I)−1/2))
= h1(si((B + I)−1/2C(B + I)−1/2))
→h1(si(C)) = si(I −(C + I)−1)
by repeated use of Theorem 6.7(7). Therefore, (6.30) is proved.
Next, let us prove the assertion in the general case A, B ≥0. Since 0 →A →
B + (A −B)+, it follows that
f (A) −f (B) →f (B + (A −B)+) −f (B),
which implies by Lemma 6.34(1) that
∼( f (A) −f (B))+∼(k) →∼f (B + (A −B)+) −f (B)∼(k).
Applying (6.28) to B + (A −B)+ and B, we have

260
6
Majorization and Singular Values
∼f (B + (A −B)+) −f (B)∼(k) →∼f ((A −B)+)∼(k).
Therefore,
s(( f (A) −f (B))+) ∈w s( f ((A −B)+)).
(6.31)
Exchanging the role of A, B gives
s(( f (A) −f (B))−) ∈w s( f ((A −B)−)).
(6.32)
Here, we may assume that f (0) = 0 since f can be replaced by f −f (0). Then we
immediately see that
f ((A −B)+) f ((A −B)−) = 0,
f ((A −B)+) + f ((A −B)−) = f (|A −B|).
Hence s( f (A) −f (B)) ∈w s( f (|A −B|)) follows from (6.31) and (6.32) thanks
to Lemma 6.34(2).
□
When f (x) = xθ with 0 < θ < 1, the weak majorization (6.27) gives the norm
inequality formerly proved by Birman, Koplienko and Solomyak:
∼Aθ −Bθ∼p/θ →∼A −B∼θ
p
for all A, B ∗M+
n and θ →p →≡. The case where θ = 1/2 and p = 1 is known
as the Powers–Størmer inequality.
The following is an immediate corollary of Theorem 6.35, whose proof is similar
to that of Theorem 6.31.
Corollary 6.36 Let g : R+ ∞R+ be an increasing bijective function whose inverse
function is operator monotone. Then
|||g(A) −g(B)||| ≥|||g(|A −B|)|||
for all A, B and ||| · ||| as above.
In [13], Audenaert and Aujla pointed out that Theorem 6.35 is not true in the case
where f : R+ ∞R+ is a general continuous concave function and that Corollary
6.36 is not true in the case where g : R+ ∞R+ is a general continuous convex
function.
In the last part of this section we prove log-majorizations results, which give
inequalities strengthening or complementing the Golden–Thompson inequality. The
following log-majorization is due to Huzihiro Araki.
Theorem 6.37 For all A, B ∗M+
n ,
s((A1/2B A1/2)r) ∈(log) s(Ar/2Br Ar/2)
(r ≥1),
(6.33)

6.4 More Majorizations for Matrices
261
or equivalently
s((Ap/2B p Ap/2)1/p) ∈(log) s((Aq/2Bq Aq/2)1/q)
(0 < p →q).
(6.34)
Proof: We can pass to the limit from A + εI and B + εI as ε ∨0 by Theorem
6.7(10). So we may assume that A and B are invertible.
First we show that
∼(A1/2B A1/2)r∼→∼Ar/2Br Ar/2∼
(r ≥1).
(6.35)
It is enough to check that Ar/2Br Ar/2 →I implies A1/2B A1/2 →I which is equiv-
alent to a monotonicity: Br →A−r implies B →A−1.
We have
((A1/2B A1/2)r)⊗k = ((A⊗k)1/2(B⊗k)(A⊗k)1/2)r,
(Ar/2Br Ar/2)⊗k = (A⊗k)r/2(B⊗k)r(A⊗k)r/2,
and in place of A, B in (6.35) we put A⊗k, B⊗k:
∼((A1/2B A1/2)r)⊗k∼→∼(Ar/2Br Ar/2)⊗k∼.
This means, thanks to Lemma 1.62, that
k	
i=1
si((A1/2B A1/2)r) →
k	
i=1
si(Ar/2Br Ar/2).
Moreover,
n
	
i=1
si((A1/2B A1/2)r) = (det A · det B)r =
n
	
i=1
si(Ar/2Br Ar/2).
Hence (6.33) is proved. If we replace A, B by Ap, B p and take r = q/p, then
s((Ap/2B p Ap/2)q/p) ∈(log) s(Aq/2Bq Aq/2),
which implies (6.34) by Theorem 6.7(11).
□
Let 0 →A, B ∗Mm, s, t ∗R+ and t ≥1. Then the theorem implies
Tr (A1/2B A1/2)st →Tr (At/2B At/2)s
(6.36)
which is called the Araki–Lieb–Thirring inequality. The case s = 1 and integer t
was the Lieb–Thirring inequality.
Theorems 6.27 and 6.37 yield:

262
6
Majorization and Singular Values
Corollary 6.38 Let0 →A, B ∗Mn and|||·|||beanyunitarilyinvariantnorm.If f is
a continuous increasing function on R+ such that f (0) ≥0 and f (et) is convex, then
||| f ((A1/2B A1/2)r)||| →||| f (Ar/2Br Ar/2)|||
(r ≥1).
In particular,
|||(A1/2B A1/2)r||| →|||Ar/2Br Ar/2|||
(r ≥1).
The next corollary strengthens the Golden–Thompson inequality to the form of
a log-majorization.
Corollary 6.39 For all self-adjoint H, K ∗Mn,
s(eH+K ) ∈(log) s((er H/2er K er H/2)1/r)
(r > 0).
Hence, for every unitarily invariant norm ||| · |||,
|||eH+K ||| →|||(er H/2er K er H/2)1/r|||
(r > 0),
and the above right-hand side decreases to |||eH+K ||| as r ∨0. In particular,
|||eH+K ||| →|||eH/2eK eH/2||| →|||eHeK |||.
(6.37)
Proof: The log-majorization follows by letting p ∨0 in (6.34) thanks to the
above lemma. The second assertion follows from the ﬁrst and Theorem 6.23. Thanks
to Theorem 6.7(3) and Theorem 6.37 we have
|||eHeK ||| = ||| |eK eH| ||| = |||(eHe2K eH)1/2||| ≥|||eH/2eK eH/2|||,
which is the second inequality of (6.37).
□
The specialization of the inequality (6.37) to the trace-norm ||·||1 is the Golden–
Thompson trace inequality Tr eH+K
→Tr eHeK . It was shown in [79] that
Tr eH+K →Tr (eH/neK/n)n for every n ∗N. The extension (6.37) was given in
[61, 80] and, for the operator norm, is known as Segal’s inequality (see [77, p.
260]).
Theorem 6.40 If A, B, X ∗Mn and
⎝A X
X B
⎞
≥0,
then we have
λ
⎝A X
X B
⎞
∈λ
⎝A + B 0
0
0
⎞
.

6.4 More Majorizations for Matrices
263
Proof: By Example 2.6 and the Ky Fan majorization (Corollary 6.11), we have
λ
⎝A X
X B
⎞
∈λ
⎝A+B
2
0
0
0
⎞
+ λ
⎝0
0
0 A+B
2
⎞
= λ
⎝A + B 0
0
0
⎞
.
This is the result.
□
The following statement is a special case of the previous theorem.
Example 6.41 For all X, Y ∗Mn such that X◦Y is Hermitian, we have
λ(X X◦+ YY ◦) ∈λ(X◦X + Y ◦Y).
Since
⎝X X◦+ YY ◦0
0
0
⎞
=
⎝X Y
0 0
⎞⎝X◦0
Y ◦0
⎞
is unitarily conjugate to
⎝X◦0
Y ◦0
⎞⎝X Y
0 0
⎞
=
⎝X◦X X◦Y
Y ◦X Y ◦Y
⎞
and X◦Y is Hermitian by assumption, the above corollary implies that
λ
⎝X X◦+ YY ◦0
0
0
⎞
∈λ
⎝X◦X + Y ◦Y 0
0
0
⎞
.
So the statement follows.
□
Next we study log-majorizations and norm inequalities. These involve the
weighted geometric means
A #α B = A1/2(A−1/2B A−1/2)α A1/2,
where 0 →α →1. The log-majorization in the next theorem is due to Ando and Hiai
[8] and is considered as complementary to Theorem 6.37.
Theorem 6.42 For all A, B ∗M+
n ,
s(Ar #α Br) ∈(log) s((A #α B)r)
(r ≥1),
(6.38)
or equivalently
s((Ap #α B p)1/p) ∈(log) s((Aq #α Bq)1/q)
(p ≥q > 0).
(6.39)

264
6
Majorization and Singular Values
Proof: First assume that both A and B are invertible. Note that
det(Ar #α Br) = (det A)r(1−α)(det B)rα = det(A #α B)r.
For every k = 1, . . . , n, it is easily veriﬁed from the properties of the antisymmetric
tensor powers that
(Ar #α Br)⊗k = (A⊗k)r #α (B⊗k)r,
((A #α B)r)⊗k = ((A⊗k) #α (B⊗k))r.
So it sufﬁces to show that
∼Ar #α Br∼→∼(A#α B)r∼
(r ≥1),
(6.40)
because (6.38) follows from Lemma 1.62 by taking A⊗k, B⊗k instead of A, B in
(6.40). To show (6.40), we prove that A #α B →I implies Ar #α Br →I. When
1 →r →2, let us write r = 2 −ε with 0 →ε →1. Let C := A−1/2B A−1/2. Suppose
that A #α B →I. Then Cα →A−1 and
A →C−α,
(6.41)
so that, since 0 →ε →1,
A1−ε →C−α(1−ε).
(6.42)
Now we have
Ar #α Br = A1−ε
2 {A−1+ ε
2 B · B−ε · B A−1+ ε
2 }α A1−ε
2
= A1−ε
2 {A−1−ε
2 C A1/2(A−1/2C−1A−1/2)εA1/2C A−1−ε
2 }α A1−ε
2
= A1/2{A1−ε #α [C(A #ε C−1)C]}A1/2
→A1/2{C−α(1−ε) #α [C(C−α #ε C−1)C]}A1/2
by using (6.41), (6.42), and the joint monotonicity of power means. Since
C−α(1−ε) #α [C(C−α #ε C−1)C] = C−α(1−ε)(1−α)[C(C−α(1−ε)C−ε)C]α = Cα,
we have
Ar #α Br →A1/2Cα A1/2 = A #α B →I.
Therefore (6.38) is proved when 1 →r →2. When r > 2, write r = 2ms with m ∗N
and 1 →s →2. Repeating the above argument we have

6.4 More Majorizations for Matrices
265
s(Ar #α Br) ∈w(log) s(A2m−1s #α B2m−1s)2
...
∈w(log) s(As #α Bs)2m
∈w(log) s(A #α B)r.
For general A, B ∗B(H)+ let Aε := A + εI and Bε := B + εI for ε > 0. Since
Ar #α Br = lim
ε∨0 Ar
ε #α Br
ε
and (A #α B)r = lim
ε∨0(Aε #α Bε)r,
we have (6.38) by the above case and Theorem 6.7(10). Finally, (6.39) readily follows
from (6.38) as in the last part of the proof of Theorem 6.37.
□
By Theorems 6.42 and 6.23 we have:
Corollary 6.43 Let 0 →A, B ∗Mn and |||·||| be any unitarily invariant norm. If f
is a continuous increasing function on R+ such that f (0) ≥0 and f (et) is convex,
then
||| f (Ar #α Br)||| →||| f ((A #α B)r)|||
(r ≥1).
In particular,
|||Ar #α Br||| →|||(A #α B)r|||
(r ≥1).
Corollary 6.44 For all self-adjoint H, K ∗Mn,
s((er H#αer K )1/r) ∈w(log) s(e(1−α)H+αK )
(r > 0).
Hence, for every unitarily invariant norm ||| · |||,
|||(er H#αer K )1/r||| →|||e(1−α)H+αK |||
(r > 0),
and the above left-hand side increases to |||e(1−α)H+αK ||| as r ∨0.
Specializing to trace inequality we have
Tr (er H#αer K )1/r →Tr e(1−α)H+αK
(r > 0),
which was ﬁrst proved in [47]. The following logarithmic trace inequalities are also
known for all 0 →A, B ∗B(H) and every r > 0:
1
r Tr A log Br/2 Ar Br/2 →Tr A(log A + log B) →1
r Tr A log Ar/2Br Ar/2,
(6.43)

266
6
Majorization and Singular Values
1
r Tr A log(Ar # Br)2 →Tr A(log A + log B).
(6.44)
The exponential function has a generalization:
expp(X) = (I + pX)
1
p ,
(6.45)
where X = X◦∗Mn and p ∗(0, 1]. (If p ∞0, then the limit is exp X.) There is a
corresponding extension of the Golden–Thompson trace inequality.
Theorem 6.45 For 0 →X, Y ∗Mn and p ∗(0, 1] the following inequalities hold:
Tr expp(X + Y) →Tr expp(X + Y + pY 1/2XY 1/2)
→Tr expp(X + Y + pXY) →Tr expp(X) expp(Y) .
Proof: Let X1 := pX, Y1 := pY and q := 1/p. Then
Tr expp(X + Y) →Tr expp(X + Y + pY 1/2XY 1/2)
= Tr [(I + X1 + Y1 + Y 1/2
1
X1Y 1/2
1
)q]
→Tr [(I + X1 + Y1 + X1Y1)q]
= Tr [((I + X1)(I + Y1))q].
The ﬁrst inequality is immediate from the monotonicity of the function (1 + px)1/p
and the second follows from Lemma 6.46 below. Next we take
Tr [((I + X1)(I + Y1))q] →Tr [(I + X1)q(I + y1)q] = Tr [expp(X) expp(Y)],
which is, by the Araki–Lieb–Thirring inequality, (6.36).
□
Lemma 6.46 For 0 →X, Y ∗Mn we have the following:
Tr [(I + X + Y + Y 1/2XY 1/2)p] →Tr [(I + X + Y + XY)p] if p ≥1,
Tr [(I + X + Y + Y 1/2XY 1/2)p] ≥Tr [(I + X + Y + XY)p] if 0 →p →1.
Proof: For every A, B ∗Msa
n , let X = A and Z = (B A)k for any k ∗N. Since
X◦Z = A(B A)k is Hermitian, we have
λ(A2 + (B A)k(AB)k) ∈λ(A2 + (AB)k(B A)k).
(6.46)

6.4 More Majorizations for Matrices
267
When k = 1, by Theorem 6.1 this majorization yields the trace inequalities:
Tr [(A2 + B A2B)p] →Tr [(A2 + AB2 A)p] if p ≥1,
Tr [(A2 + B A2B)p] ≥Tr [(A2 + AB2 A)p] if 0 →p →1.
Moreover, for 0 →X, Y ∗Mn, let A = (I + X)1/2 and B = Y 1/2. Notice that
Tr [(A2 + B A2B)p] = Tr [(I + X + Y + Y 1/2XY 1/2)p]
and
Tr [(A2 + B A2B)p] = Tr [((I + X)1/2(I + Y)(I + X)1/2)p]
= Tr [((I + X)(I + Y))p] = Tr [(I + X + Y + XY)p],
where (I + X)(I +Y) has eigenvalues in (0, ≡) so that ((I + X)(I +Y))p is deﬁned
via the analytic functional calculus (3.17). The statement then follows.
□
The inequalities of Theorem 6.45 can be extended to the symmetric norm
inequality, as shown below, together with the complementary geometric mean
inequality.
Theorem 6.47 Let ||| · ||| be a symmetric norm on Mn and p ∗(0, 1]. For all
0 →X, Y ∗Mn we have
||| expp(2X)# expp(2Y)||| →||| expp(X + Y)|||
→||| expp(X)1/2 expp(Y) expp(X)1/2|||
→||| expp(X) expp(Y)|||.
Proof: We have
λ(expp(2X)# expp(2Y)) = λ((I + 2pX)1/p#(I + 2pY)1/p)
∈(log) (((I + 2pX)#(I + 2pY))1/p)
→λ(expp(X + Y)),
where the log-majorization is due to (6.38) and the inequality is due to the arithmetic-
geometric mean inequality:
(I + 2pX)#(I + 2pY) →(I + 2pX) + (I + 2pY)
2
= I + p(X + Y).
On the other hand, let A := (I + pX)1/2 and B := (pY)1/2. We can use (6.46) and
Theorem 6.37:

268
6
Majorization and Singular Values
λ(expp(X + Y)) →λ((A2 + B A2B)1/p)
∈λ((A2 + AB2 A)1/p)
= λ(((I + pX)1/2(I + pY)(I + pX)1/2)1/p)
∈(log) λ((I + pX)1/2p(I + pY)1/p(I + pX)1/2p)
= λ(expp(X)1/2 expp(Y) expp(X)1/2)
∈(log) λ((expp(X) expp(Y)2 expp(X))1/2)
= λ(| expp(X) expp(Y)|).
The above majorizations give the stated norm inequalities.
□
6.5 Notes and Remarks
The quote at the beginning of the chapter is from the paper John von Neumann, Some
matrix inequalities and metrization of matric-space, Tomsk. Univ. Rev. 1(1937), 286–
300. (The paper is also in the book John von Neumann Collected Works.) Theorem
6.17 and the duality of the ℓp norm also appeared in this chapter.
Example 6.2 is from the paper M. A. Nielsen and J. Kempe, Separable states are
more disordered globally than locally, Phys. Rev. Lett. 86(2001), 5184–5187. The
most comprehensive text on majorization theory for vectors and matrices is Marshall
and Olkin’s monograph [66]. (There is a recently reprinted version: A. W. Marshall,
I. Olkin and B. C. Arnold, Inequalities: Theory of Majorization and Its Applications,
Second ed., Springer, New York, 2011.) The content presented here is mostly based
on Fumio Hiai [43]. Two survey articles [5, 6] of Tsuyoshi Ando are the best sources
on majorizations for the eigenvalues and the singular values of matrices.
The ﬁrst complete proof of the Lidskii–Wielandt theorem (Theorem 6.9) was
obtained by Helmut Wielandt in 1955, who proved a complicated minimax repre-
sentation by induction. The proofs of Theorems 6.9 and 6.13 presented here are sur-
prisingly elementary and short (compared with previously known proofs), which are
from the paper C.-K. Li and R. Mathias, The Lidskii–Mirsky–Wielandt theorem—
additive and multiplicative versions, Numer. Math. 81(1999), 377–413.
Here is a brief remark on the famous Horn conjecture that was afﬁrmatively
solved just before 2000. The conjecture concerns three real vectors a = (a1, . . . , an),
b = (b1, . . . , bn), and c = (c1, . . . , cn). If there are two n × n Hermitian matrices
A and B such that a = λ(A), b = λ(B), and c = λ(A + B), that is, a, b, c are the
eigenvalues of A, B, A + B, then the three vectors obey many inequalities of the
form
⎡
k∗K
ck →
⎡
i∗I
ai +
⎡
j∗J
b j

6.5 Notes and Remarks
269
for certain triples (I, J, K) of subsets of {1, . . . , n}, including those coming from
the Lidskii–Wielandt theorem, together with the obvious equality
n
⎡
i=1
ci =
n
⎡
i=1
ai +
n
⎡
i=1
bi.
Horn [52] proposed a procedure which produces such triples (I, J, K) and conjec-
tured that all the inequalities obtained in this way are sufﬁcient to characterize a, b, c
that are the eigenvalues of Hermitian matrices A, B, A+ B. This long-standing Horn
conjecture was solved by a combination of two papers, one by Klyachko [55] and
the other by Knuston and Tao [56].
The Lieb–Thirring inequality was proved in 1976 by Elliott H. Lieb and Walter
Thirring in a physical proceeding. It is interesting that Bellmann proved the partic-
ular case Tr (AB)2 →Tr A2B2 in 1980 and he conjectured Tr (AB)n →Tr An Bn.
The extension was proved by Huzihiro Araki, On an inequality of Lieb and Thirring,
Lett. Math. Phys. 19(1990), 167–170.
Theorem 6.25 is from J.-C. Bourin [29]. Theorem 6.27 from [28] also appeared
in the paper of Aujla and Silva [15] with the inequality reversed for a contraction
instead of an expansion. The subadditivity inequality in Theorem 6.30 was ﬁrst
obtained by T. Ando and X. Zhan, Norm inequalities related to operator monotone
functions, Math. Ann. 315(1999), 771–780. The proof of Theorem 6.30 presented
here is simpler and is due to M. Uchiyama [82]. Theorem 6.35 is due to Ando [4].
In the papers [8, 47] there are more details on the logarithmic trace inequalities
(6.43) and (6.44). Theorem 6.45 is in the paper S. Furuichi and M. Lin, A matrix
trace inequality and its application, Linear Algebra Appl. 433(2010), 1324–1328.
6.6 Exercises
1. Let S be a doubly substochastic n × n matrix. Show that there exists a doubly
stochastic n × n matrix D such that Si j →Di j for all 1 →i, j →n.
2. Let χn denote the set of all probability vectors in Rn, i.e.,
χn :=

p = (p1, . . . , pn) : pi ≥0,
n
⎡
i=1
pi = 1

.
Prove that
(1/n, 1/n, . . . , 1/n) ∈p ∈(1, 0, . . . , 0)
(p ∗χn).
The Shannon entropy of p ∗χn is H(p) := −⎢n
i=1 pi log pi. Show that
H(q) →H(p) →log n for all p ∈q in χn and H(p) = log n if and only if
p = (1/n, . . . , 1/n).

270
6
Majorization and Singular Values
3. Let A ∗Msa
n . Prove the equality
k
⎡
i=1
λi(A) = max{Tr AP : P is a projection, rank P = k}
for 1 →k →n.
4. Let A, B ∗Msa
n . Show that A →B implies λk(A) →λk(B) for 1 →k →n.
5. Show that the statement of Theorem 6.13 is equivalent to the inequality
k	
j=1
⎤
sn+1−j(A)si j (B)
⎥
→
k	
j=1
si j (AB)
for any choice of 1 →i1 < · · · < ik →n.
6. Give an example showing that, for the generalized inverse, (AB)† = B†A† is
not always true.
7. Describe the generalized inverse of a row matrix.
8. What is the generalized inverse of an orthogonal projection?
9. Let A ∗B(H) have polar decomposition A = U|A|. Prove that
|⊥x, Ax∪| →⊥x, |A|x∪+ ⊥x,U|A|U ◦x∪
2
for x ∗H.
10. Show that |Tr A| →∼A∼1 for A ∗B(H).
11. Let 0 < p, p1, p2 →≡and 1/p = 1/p1 + 1/p2. Prove the Hölder inequality
for the vectors a, b ∗Rn:
σp(ab) →σp1(a)σp2(b),
where ab = (aibi).
12. Show that a continuous concave function f : R+ ∞R+ is the pointwise limit
of a sequence of functions of the form
α + βx −
m
⎡
ℓ=1
cℓγaℓ(x),
where α ≥0, β, cℓ, aℓ> 0 and γa is as given in the proof of Theorem 6.32.
13. Prove for self-adjoint matrices H, K the Lie–Trotter formula:
lim
r∞0(er H/2er K er H/2)1/r = eH+K .
14. Prove for self-adjoint matrices H, K that

6.6 Exercises
271
lim
r∞0(er H#αer K )1/r = e(1−α)H+αK .
15. Let f be a real function on [a, b] with a →0 →b. Prove the converse of Corollary
4.27, that is, if
Tr f (Z◦AZ) →Tr Z◦f (A)Z
for every A ∗Msa
2 with σ(A) ⊂[a, b] and every contraction Z ∗M2, then f
is convex on [a, b] and f (0) →0.
16. Prove Theorem 4.28 in a direct way similar to the proof of Theorem 4.26.
17. Provide an example of a pair A, B of 2 × 2 Hermitian matrices such that
λ1(|A + B|) < λ1(|A| + |B|) and λ2(|A + B|) > λ2(|A| + |B|).
From this, show that Theorems 4.26 and 4.28 are not true for a simple convex
function f (x) = |x|.

Chapter 7
Some Applications
Matrices are important in many areas of both pure and applied mathematics. In
particular, they play essential roles in quantum probability and quantum informa-
tion. A discrete classical probability is a vector (p1, p2, . . . , pn) of pi ∈0 with
⎡n
i=1 pi = 1. Its counterpart in quantum theory is a matrix D →Mn(C) such that
D ∈0 and Tr D = 1; such matrices are called density matrices. Thus matrix analy-
sis is central to quantum probability/statistics and quantum information. We remark
that classical theory is included in quantum theory as a special case, where the rel-
evant matrices are restricted to diagonal matrices. On the other hand, the concepts
of classical probability theory can be reformulated with matrices, for instance, the
covariance matrices typical in Gaussian probabilities and Fisher information matrices
in the Cramér–Rao inequality.
This chapter is devoted to some aspects of the applications of matrices. One of the
most important concepts in probability theory is the Markov property. This concept is
discussed in the ﬁrst section in the setting of Gaussian probabilities. The structure of
covariance matrices for Gaussian probabilities with the Markov property is clariﬁed
in connection with the Boltzmann entropy. Its quantum analogue in the setting of
CCR-algebras CCR(H) is the subject of Sect.7.3. The counterpart of the notion of
Gaussian probabilities is that of Gaussian or quasi-free states ωA induced by positive
operators A (similar to covariance matrices) on the underlying Hilbert space H. In
the setting of the triplet CCR-algebra
CCR(H1 ≤H2 ≤H3) = CCR(H1) ⊗CCR(H2) ⊗CCR(H3),
the special structure of A on H1 ≤H2 ≤H3 and equality in the strong subadditivity
of the von Neumann entropy of ωA emerge as equivalent conditions for the Markov
property of ωA.
The most useful entropy in both classical and quantum probabilities is the relative
entropy S(D1∗D2) := Tr D1(log D1 −log D2) for density matrices D1, D2, which
was already discussed in Sects.3.2 and 4.5. (It is also known as the Kullback–Leibler
divergence in the classical case.) The notion was extended to the quasi-entropy:
F. Hiai and D. Petz, Introduction to Matrix Analysis and Applications,
273
Universitext, DOI: 10.1007/978-3-319-04150-6_7,
© Hindustan Book Agency 2014

274
7
Some Applications
S A
f (D1∗D2) := ⟨AD1/2
2
, f (α(D1/D2))(AD1/2
2
)⟩
associated with a certain function f
: R+ ≥R and a reference matrix A,
where α(D1/D2)X := D1X D−1
2
= LD1R−1
D2(X). (Recall that M f (LA, RB) =
f (LAR−1
B )RB was used for the matrix mean transformation in Sect.5.4.) The orig-
inal relative entropy S(D1∗D2) is recovered by taking f (x) = x log x and A = I.
The monotonicity and the joint convexity properties are two major properties of the
quasi-entropies, which are the subject of Sect.7.2. Another important topic in this
section is the monotone Riemannian metrics on the manifold of invertible positive
density matrices.
In a quantum system with a state D, several measurements may be performed
to recover D, which forms the subject of the quantum state tomography. Here, a
measurement is given by a POVM (positive operator-valued measure) {F(x) : x →
X}, i.e., a ﬁnite set of positive matrices F(x) →Mn(C) such that ⎡
x→X F(x) = I.
In Sect.7.4 we study a few results concerning how to construct optimal quantum
measurements.
ThelastsectionisconcernedwiththequantumversionoftheCramér–Raoinequal-
ity, which is a certain matrix inequality between a kind of generalized variance and
the quantum Fisher information. This subject belongs to quantum estimation theory
and is also related to the monotone Riemannian metrics.
7.1 Gaussian Markov Property
In probability theory the matrices typically have real entries, but the content of this
section can be modiﬁed for the complex case.
Given a positive deﬁnite real matrix M →Mn(R) a Gaussian probability density
is deﬁned on Rn as
p(x) :=
⎢
det M
(2π)n exp
⎣
−1
2⟨x, Mx⟩
⎤
(x →Rn).
Obviously p(x) > 0 and the integral
⎥
Rn p(x) dx = 1
follows due to the constant factor. Since
⎥
Rn⟨x, Bx⟩p(x) dx = Tr BM−1,
the particular case B = E(i j) gives

7.1 Gaussian Markov Property
275
⎥
Rn xi x j p(x) dx =
⎥
Rn⟨x, E(i j)x⟩p(x) dx = Tr E(i j)M−1 = (M−1)i j.
Thus the inverse of the matrix M is the covariance matrix.
The Boltzmann entropy is
S(p) = −
⎥
Rn p(x) log p(x) dx = n
2 log(2πe) −1
2Tr log M.
(7.1)
(Instead of Tr log M, the formulation log det M is often used.)
If Rn = Rk × Rσ, then the probability density p(x) has a reduction p1(y) on Rk:
p1(y) :=
⎢
det M1
(2π)k exp
⎣
−1
2⟨y, M1y⟩
⎤
(y →Rk).
To describe how M and M1 are related we take the block matrix form
M =
⎦M11 M12
M∪
12 M22

,
where M11 →Mk(R). Then we have
p1(y) =
⎢
det M
(2π)mdet M22
exp
⎣
−1
2⟨y, (M11 −M12M−1
22 M∪
12)y⟩
⎤
,
see Example 2.7. Therefore M1 = M11 −M12M−1
22 M∪
12 = M/M22, which is called
the Schur complement of M22 in M. We have det M1 · det M22 = det M.
Let p2(z) be the reduction of p(x) to Rσ and denote the Gaussian matrix by
M2. In this case M2 = M22 −M∪
12M−1
11 M12 = M/M11. The following equivalent
conditions hold:
(1) S(p) ⊥S(p1) + S(p2);
(2) −Tr log M ⊥−Tr log M1 −Tr log M2;
(3) Tr log M ⊥Tr log M11 + Tr log M22.
(1) is known as the subadditivity of the Boltzmann entropy. The equivalence of (1)
and (2) follows directly from formula (7.1). (2) can be rewritten as
−log det M ⊥−(log det M −log det M22) −(log det M −log det M11)
and we have (3). The equality condition is M12 = 0. If
M−1 = S =
⎦S11 S12
S∪
12 S22

,

276
7
Some Applications
then M12 = 0 is obviously equivalent to S12 = 0. It is interesting to note that (2) is
equivalent to the inequality
(2*) Tr log S ⊥Tr log S11 + Tr log S22.
The three-fold factorization Rn = Rk ×Rσ ×Rm is more interesting and includes
essential properties. The Gaussian matrix of the probability density p is
M =


M11 M12 M13
M∪
12 M22 M23
M∪
13 M∪
23 M33

,
(7.2)
where M11 →Mk(R), M22 →Mσ(R), M33 →Mm(R). Denote the reduced probabil-
ity densities of p by p1, p2, p3, p12, p23. The strong subadditivity of the Boltzmann
entropy
S(p) + S(p2) ⊥S(p12) + S(p23)
(7.3)
is equivalent to the inequality
Tr log S + Tr log S22 ⊥Tr log
⎦S11 S12
S∪
12 S22

+ Tr log
⎦S22 S23
S∪
23 S33

,
(7.4)
where
M−1 = S =


S11 S12 S13
S∪
12 S22 S23
S∪
13 S∪
23 S33

.
The Markov property in probability theory is typically deﬁned as
p(x1, x2, x3)
p12(x1, x2) = p23(x2, x3)
p2(x2)
(x1 →Rk, x2 →Rσ, x3 →Rm).
Taking the logarithm and integrating with respect to dp, we obtain
−S(p) + S(p12) = −S(p23) + S(p2)
(7.5)
and this is the equality case in (7.3) and in (7.4). The equality case of (7.4) is described
in Theorem 4.50, so we have the following:
Theorem 7.1 The Gaussian probability density described by the block matrix (7.2)
has the Markov property if and only if S13 = S12S−1
22 S23 for the inverse.
Another condition comes from the inverse property of a 3 × 3 block matrix.

7.1 Gaussian Markov Property
277
Theorem 7.2 Let S = [Si j]3
i, j=1 be an invertible block matrix and assume that S22
and [Si j]3
i, j=2 are invertible. Then the (1, 3) entry of the inverse S−1 = [Mi j]3
i, j=1
is given by the following formula:

S11 −[S12, S13]
⎦S22, S23
S32 S33
−1 ⎦S12
S13
⎛−1
×(S12S−1
22 S23 −S13)(S33 −S32S−1
22 S23)−1.
Hence M13 = 0 if and only if S13 = S12S−1
22 S23.
It follows that the Gaussian block matrix (7.2) has the Markov property if and
only if M13 = 0.
7.2 Entropies and Monotonicity
Entropy and relative entropy are important notions in information theory. The quan-
tum versions are in matrix theory. Recall that 0 ⊥D →Mn is a density matrix if
Tr D = 1. This means that the eigenvalues (λ1, λ2, . . . , λn) form a probabilistic set:
λi ∈0, ⎡
i λi = 1. The von Neumann entropy S(D) = −Tr D log D of the density
matrix D is the Shannon entropy of the probabilistic set, −⎡
i λi log λi.
The partial trace Tr1 : Mn ⊗Mm ≥Mm is a linear mapping which is deﬁned
by the formula Tr1(A ⊗B) = (Tr A)B on elementary tensors. It is called the partial
trace, since only the trace of the ﬁrst tensor factor is taken. Tr2 : Mn ⊗Mm ≥Mn
is similarly deﬁned.
The ﬁrst example includes the strong subadditivity of the von Neumann entropy
and a condition of the equality is also included. (Other conditions will appear in
Theorem 7.6.)
Example 7.3 Here, we shall need the concept of a three-fold tensor product and
reduced densities. Let D123 be a density matrix in Mk ⊗Mσ ⊗Mm. The reduced
density matrices are deﬁned by the partial traces:
D12 := Tr3D123 →Mk ⊗Mσ,
D2 := Tr13D123 →Mσ,
D23 := Tr1D123 →Mk.
The strong subadditivity of S is the inequality
S(D123) + S(D2) ⊥S(D12) + S(D23),
(7.6)
which is equivalent to
Tr D123 (log D123 −(log D12 −log D2 + log D23)) ∈0.

278
7
Some Applications
The operator
exp(log D12 −log D2 + log D23)
is positive and can be written as λD for a density matrix D. Actually,
λ = Tr exp(log D12 −log D2 + log D23).
We have
S(D12) + S(D23) −S(D123) −S(D2)
= Tr D123 (log D123 −(log D12 −log D2 + log D23))
= S(D123∗λD) = S(D123∗D) −log λ .
Here S(X∗Y) := Tr X(log X −log Y) is the relative entropy. If X and Y are density
matrices, then S(X∗Y) ∈0, see the Streater inequality (3.13).
Therefore, λ ⊥1 implies the positivity of the left-hand side (and the strong
subadditivity). By Theorem 4.55, we have
Tr exp(log D12−log D2+log D23)) ⊥
⎥⊂
0
Tr D12(t I + D2)−1D23(t I + D2)−1 dt.
Applying the partial traces we have
Tr D12(t I + D2)−1D23(t I + D2)−1 = Tr D2(t I + D2)−1D2(t I + D2)−1
which can be integrated out. Hence
⎥⊂
0
Tr D12(t I + D2)−1D23(t I + D2)−1 dt = Tr D2 = 1
obtaining λ ⊥1, and the strong subadditivity is proved.
If equality holds in (7.6), then exp(log D12−log D2+log D23) is a density matrix
and
S(D123∗exp(log D12 −log D2 + log D23)) = 0
implies
log D123 = log D12 −log D2 + log D23.
This is the necessary and sufﬁcient condition for equality.
□
For a density matrix D one can deﬁne the q-entropy as

7.2 Entropies and Monotonicity
279
Sq(D) = 1 −Tr Dq
q −1
= Tr (Dq −D)
1 −q
(q > 1).
This is also called the quantum Tsallis entropy. The limit q ≥1 is the von
Neumann entropy.
The next theorem is the subadditivity of the q-entropy. The result has an elemen-
tary proof, but it was not known for several years.
Theorem 7.4 When the density matrix D →Mn ⊗Mm has the partial densities
D1 := Tr2D and D2 := Tr1D, the subadditivity inequality Sq(D) ⊥Sq(D1) +
Sq(D2), or equivalently
Tr Dq
1 + Tr Dq
2 = ∗D1∗q
q + ∗D2∗q
q ⊥1 + ∗D∗q
q = 1 + Tr Dq
holds for q ∈1.
Proof:
It is enough to show the case q > 1. First we use the q-norms and we prove
1 + ∗D∗q ∈∗D1∗q + ∗D2∗q.
(7.7)
Lemma 7.5 below will be used.
If 1/q + 1/q√= 1, then for A ∈0 we have
∗A∗q := max{Tr AB : B ∈0, ∗B∗q√⊥1}.
It follows that
∗D1∗q = Tr X D1
and ∗D2∗q = Tr Y D2
with some X ∈0 and Y ∈0 such that ∗X∗q√⊥1 and ∗Y∗q√⊥1. It follows from
Lemma 7.5 that
∗(X ⊗Im + In ⊗Y −In ⊗Im)+∗q√⊥1
and we have Z ∈0 such that
Z ∈X ⊗Im + In ⊗Y −In ⊗Im
and ∗Z∗q√= 1. It follows that
Tr (Z D) + 1 ∈Tr (X ⊗Im + In ⊗Y)D = Tr X D1 + Tr Y D2.
Since
∗D∗q ∈Tr (Z D),
we have the inequality (7.7).
We examine the maximum of the function f (x, y) = xq + yq in the domain

280
7
Some Applications
M := {(x, y) : 0 ⊥x ⊥1, 0 ⊥y ⊥1, x + y ⊥1 + ∗D∗q}.
Since f is convex, it is sufﬁcient to check the extreme points (0, 0), (1, 0), (1, ∗D∗q),
(∗D∗q, 1), (0, 1). It follows that f (x, y) ⊥1 + ∗D∗q
q. The inequality (7.7) gives
that (∗D1∗q, ∗D2∗q) →M and this gives f (∗D1∗q, ∗D2∗q) ⊥1 + ∗D∗q
q, which is
the statement.
□
Lemma 7.5 For q ∈1 and for the positive matrices 0 ⊥X →Mn and 0 ⊥Y →Mm
assume that ∗X∗q, ∗Y∗q ⊥1. Then the quantity
∗(X ⊗Im + In ⊗Y −In ⊗Im)+∗q ⊥1
(7.8)
holds.
Proof:
Let {xi : 1 ⊥i ⊥n} and {y j : 1 ⊥j ⊥m} be the eigenvalues of X and Y,
respectively. Then
n
⎝
i=1
xq
i ⊥1,
m
⎝
j=1
yq
j ⊥1
and
∗(X ⊗Im + In ⊗Y −In ⊗Im)+∗q
q =
⎝
i, j
((xi + y j −1)+)q.
The function a ≡≥(a + b −1)+ is convex for any real value of b:
⎞a1 + a2
2
+ b −1
⎠
+
⊥1
2(a1 + b −1)+ + 1
2(a2 + b −1)+ .
It follows that the vector-valued function
a ≡≥((a + y j −1)+ : j)
is convex as well. Since the σq norm for positive real vectors is convex and monoton-
ically increasing, we conclude that
f (a) :=

⎝
j
((a + y j −1)+)q


1/q
is a convex function. Since f (0) = 0 and f (1) = 1, we have the inequality f (a) ⊥a
for 0 ⊥a ⊥1. Actually, we need this for xi. Since 0 ⊥xi ⊥1, f (xi) ⊥xi follows
and
⎝
i
⎝
j
((xi + y j −1)+)q =
⎝
i
f (xi)q ⊥
⎝
i
xq
i ⊥1.

7.2 Entropies and Monotonicity
281
So (7.8) is proved.
□
The next theorem is stated in the setting of Example 7.3.
Theorem 7.6 The following conditions are equivalent:
(i) S(D123) + S(D2) = S(D12) + S(D23);
(ii) Dit
123D−it
23 = Dit
12D−it
2
for every real t;
(iii) D1/2
123D−1/2
23
= D1/2
12 D−1/2
2
;
(iv) log D123 −log D23 = log D12 −log D2;
(v) there are positive matrices X →Mk ⊗Mσ and Y →Mσ ⊗Mm such that
D123 = (X ⊗Im)(Ik ⊗Y).
In the mathematical formalism of quantum mechanics, instead of n-tuples of
numbers one works with n × n complex matrices. They form an algebra and this
allows an algebraic approach.
For positive deﬁnite matrices D1, D2 →Mn, for A →Mn and a function f :
R+ ≥R, the quasi-entropy is deﬁned as
S A
f (D1∗D2) := ⟨AD1/2
2
, f (α(D1/D2))(AD1/2
2
)⟩
= Tr D1/2
2
A∪f (α(D1/D2))(AD1/2
2
),
(7.9)
where ⟨B, C⟩:= Tr B∪C is the so-called Hilbert–Schmidt inner product and
α(D1/D2) : Mn ≥Mn is a linear mapping acting on matrices as follows:
α(D1/D2)A := D1AD−1
2 .
This concept was introduced by Petz in [70, 72]. An alternative terminology is the
quantum f-divergence.
For a positive deﬁnite matrix D →Mn the left and the right multiplication opera-
tors acting on matrices are deﬁned by
LD(X) := DX ,
RD(X) := X D
(X →Mn).
(7.10)
If we set
J f
D1,D2 := f (LD1R−1
D2)RD2 ,
then the quasi-entropy has the form
S A
f (D1∗D2) = ⟨A, J f
D1,D2 A⟩.
(7.11)
It is clear from the deﬁnition that
S A
f (λD1∗λD2) = λS A
f (D1∗D2)

282
7
Some Applications
for a positive number λ.
Let α : Mn ≥Mm be a mapping between two matrix algebras. The dual α∪:
Mm ≥Mn with respect to the Hilbert–Schmidt inner product is positive if and
only if α is positive. Moreover, α is unital if and only if α∪is trace preserving.
α : Mn ≥Mm is called a Schwarz mapping if
α(B∪B) ∈α(B∪)α(B)
(7.12)
for every B →Mn.
The quasi-entropies are monotone and jointly convex.
Theorem 7.7 Assume that f : R+ ≥R is a matrix monotone function with f (0) ∈
0 and α : Mn ≥Mm is a unital Schwarz mapping. Then
S A
f (α∪(D1)∗α∪(D2)) ∈Sα(A)
f
(D1∗D2)
(7.13)
holds for A →Mn and for invertible density matrices D1 and D2 in the matrix
algebra Mm.
Proof:
The proof is based on inequalities for matrix monotone and matrix concave
functions. First note that
S A
f +c(α∪(D1)∗α∪(D2)) = S A
f (α∪(D1)∗α∪(D2)) + c Tr D1α(A∪A)
and
Sα(A)
f +c (D1∗D2) = Sα(A)
f
(D1∗D2) + c Tr D1(α(A)∪α(A))
for a positive constant c. By the Schwarz inequality (7.12), we may assume that
f (0) = 0.
Let α := α(D1/D2) and α0 := α(α∪(D1)/α∪(D2)). The operator
V Xα∪(D2)1/2 = α(X)D1/2
2
(X →M0)
is a contraction:
∗α(X)D1/2
2
∗2 = Tr D2(α(X)∪α(X))
⊥Tr D2(α(X∪X) = Tr α∪(D2)X∪X = ∗Xα∪(D2)1/2∗2
since the Schwarz inequality is applicable to α. A similar simple computation gives
that
V ∪αV ⊥α0 .

7.2 Entropies and Monotonicity
283
Since f is matrix monotone, we have f (α0) ∈f (V ∪αV ). Recall that f is matrix
concave. Therefore f (V ∪αV ) ∈V ∪f (α)V and we conclude
f (α0) ∈V ∪f (α)V .
Application to the vector Aα∪(D2)1/2 gives the statement.
□
It is remarkable that for a multiplicative α (i.e., α is a ∪-homomorphism) we do
not need the condition f (0) ∈0. Moreover, since V ∪αV = α0, we do not need
the matrix monotonicity of the function f . In this case matrix concavity is the only
condition needed to obtain the result analogous to Theorem 7.7. If we apply the
monotonicity (7.13) (with −f in place of f ) to the embedding α(X) = X ≤X
of Mn into Mn ≤Mn ⊃Mn ⊗M2 and to the densities D1 = λE1 ≤(1 −λ)F1,
D2 = λE2 ≤(1 −λ)F2, then we obtain the joint convexity of the quasi-entropy:
Theorem 7.8 If f : R+ ≥R is a matrix convex function, then S A
f (D1∗D2) is
jointly convex in the variables D1 and D2.
If we consider the quasi-entropy in the terminology of means, then we obtain
another proof. The joint convexity of the mean is the inequality
f (L(A1+A2)/2R−1
(B1+B2)/2)R(B1+B2)/2 ⊥1
2 f (LA1R−1
B1 )RB1 + 1
2 f (LA2R−1
B2 )RB2,
which can be simpliﬁed as
f (LA1+A2R−1
B1+B2) ⊥R−1/2
B1+B2R1/2
B1 f (LA1R−1
B1 )R1/2
B1 R−1/2
B1+B2
+R−1/2
B1+B2R1/2
B2 f (LA2R−1
B2 )R1/2
B2 R−1/2
B1+B2
= C f (LA1R−1
B1 )C∪+ Df (LA2R−1
B2 )D∪.
Here CC∪+ DD∪= I and
C(LA1R−1
B1 )C∪+ D(LA2R−1
B2 )D∪= LA1+A2R−1
B1+B2.
So the joint convexity of the quasi-entropy has the form
f (C XC∪+ DY D∪) ⊥C f (X)C∪+ Df (Y)D∪
which is true for a matrix convex function f , see Theorem 4.22.
Example 7.9 The concept of quasi-entropies includes some important special cases.
If f (t) = tα, then
S A
f (D1∗D2) = Tr A∪Dα
1 AD1−α
2
.

284
7
Some Applications
If 0 < α < 1, then f is matrix monotone. The joint concavity in (D1, D2) is the
famous Lieb’s concavity theorem [63].
In the case where A = I we have a kind of relative entropy. For f (x) = x log x
we have the Umegaki relative entropy
S(D1∗D2) = Tr D1(log D1 −log D2).
(If we want a matrix monotone function, then we can take f (x) = log x and then
we have S(D2∗D1).) The Umegaki relative entropy is the most important example.
Let
fα(x) =
1
α(1 −α)

1 −xα
.
This function is matrix monotone decreasing for α →(−1, 1). (For α = 0, the limit
is taken, which is equal to −log x.) Then the relative entropies of degree α are
produced:
Sα(D1∗D2) :=
1
α(1 −α)Tr (I −Dα
1 D−α
2 )D2.
These quantities are essential in the quantum case.
□
Let Mn be the set of positive deﬁnite density matrices in Mn. This is a differ-
entiable manifold and the set of tangent vectors is {A = A∪→Mn : Tr A = 0}.
A Riemannian metric is a family of real inner products γD(A, B) on the tangent
vectors. For a function f : (0, ⊂) ≥(0, ⊂) with x f (x−1) = f (x), a possible
deﬁnition is similar to (7.11): for A, B →Msa
n with Tr A = Tr B = 0,
γ f
D(A, B) := Tr A(J f
D)−1(B).
(7.14)
(Here J f
D = J f
D,D.) The condition x f (x−1) = f (x) implies that (J f
D)−1(B) →Msa
n
if B →Msa
n . Hence (7.14) actually deﬁnes a real inner product.
By a monotone metric we mean a family γD of Riemannian metrics on all
manifolds Mn such that
γβ(D)(β(A), β(A)) ⊥γD(A, A)
(7.15)
for every completely positive trace-preserving mapping β : Mn ≥Mm and every
A →Msa
n with Tr A = 0. If f is matrix monotone, then γ f
D satisﬁes this monotonicity,
see [71].
Let β : Mn ⊗M2 ≥Mn be deﬁned as
⎦B11 B12
B21 B22

≡≥B11 + B22.

7.2 Entropies and Monotonicity
285
This is completely positive and trace-preserving, making it a so-called partial trace.
For
D =
⎦λD1
0
0
(1 −λ) D2

,
A =
⎦λA1
0
0
(1 −λ) A2

the inequality (7.15) gives
γλD1+(1−λ)D2(λA1 + (1 −λ)A2, λA1 + (1 −λ)A2)
⊥γλD1(λA1, λA1) + γ(1−λ)D2((1 −λ)A2, (1 −λ)A2).
Since γt D(t A, t B) = tγD(A, B), we obtain the joint convexity:
Theorem 7.10 For a matrix monotone function f , the monotone metric γ f
D(A, A)
is a jointly convex function of (D, A) for positive deﬁnite D and general A →Mn.
Now let f : (0, ⊂) ≥(0, ⊂) be a continuous function; the deﬁnition of f at 0
is not necessary here. Deﬁne g, h : (0, ⊂) ≥(0, ⊂) by g(x) := x f (x−1) and
h(x) :=
⎞f (x)−1 + g(x)−1
2
⎠−1
,
x > 0.
Obviously, h is symmetric, i.e., h(x) = xh(x−1) for x > 0, so we may call h the
harmonic symmetrization of f .
The difference between two parameters in J f
D1,D2 and one parameter in J f
D,D is
not essential if the matrix size can be changed. We need the next lemma.
Lemma 7.11 For D1, D2 > 0 and general X in Mn let
D :=
⎦D1 0
0
D2

,
Y :=
⎦0 X
0 0

,
A :=
⎦0
X
X∪0

.
Then
⟨Y, (J f
D)−1Y⟩= ⟨X, (J f
D1,D2)−1X⟩,
(7.16)
⟨A, (J f
D)−1 A⟩= 2⟨X, (Jh
D1,D2)−1X⟩.
(7.17)
Proof:
First we show that
(J f
D)−1
⎦X11 X12
X21 X22

=

(J f
D1)−1X11
(J f
D1,D2)−1X12
(J f
D2,D1)−1X21
(J f
D2)−1X22

.
(7.18)
Since continuous functions can be approximated by polynomials, it is enough to
check (7.18) for f (x) = xk, which is easy. From (7.18), (7.16) is obvious and

286
7
Some Applications
⟨A, (J f
D)−1A⟩= ⟨X, (J f
D1,D2)−1X⟩+ ⟨X∪, (J f
D2,D1)−1X∪⟩.
From the spectral decompositions
D1 =
⎝
i
λi Pi
and
D2 =
⎝
j
μ j Q j
we have
J f
D1,D2 A =
⎝
i, j
m f (λi, μ j)Pi AQ j
and
⟨X, (Jg
D1,D2)−1X⟩=
⎝
i, j
mg(λi, μ j)Tr X∪Pi X Q j
=
⎝
i, j
m f (μ j, λi)Tr X Q j X∪Pi
= ⟨X∪, (J f
D2,D1)−1X∪⟩.
(7.19)
Therefore,
⟨A, (J f
D)−1 A⟩= ⟨X, (J f
D1,D2)−1X⟩+ ⟨X, (Jg
D1,D2)−1X⟩= 2⟨X, (Jh
D1,D2)−1X⟩.
□
Theorem 7.12 In the above setting consider the following conditions:
(i)
f is matrix monotone;
(ii) (D, A) ≡≥⟨A, (J f
D)−1A⟩is jointly convex for positive deﬁnite D and general
A in Mn for every n;
(iii) (D1, D2, A) ≡≥⟨A, (J f
D1,D2)−1A⟩is jointly convex for positive deﬁnite D1, D2
and general A in Mn for every n;
(iv) (D, A) ≡≥⟨A, (J f
D)−1A⟩is jointly convex for positive deﬁnite D and self-
adjoint A in Mn for every n;
(v) h is matrix monotone.
Then (i) ∞(ii) ∞(iii) ∩(iv) ∞(v).
Proof:
(i) ∩(ii) is Theorem 7.10 and (ii) ∩(iii) follows from (7.16). We prove
(iii) ∩(i). For each ξ →Cn let Xξ := [ξ 0 · · · 0] →Mn, i.e., the ﬁrst column of Xξ
is ξ and all other entries of Xξ are zero. When D2 = I and X = Xξ, we have for
D > 0 in Mn
⟨Xξ, (J f
D,I)−1Xξ⟩= ⟨Xξ, f (D)−1Xξ⟩= ⟨ξ, f (D)−1ξ⟩.

7.2 Entropies and Monotonicity
287
Hence it follows from (iii) that ⟨ξ, f (D)−1ξ⟩is jointly convex in D > 0 in Mn and
ξ →Cn. By a standard convergence argument we see that (D, ξ) ≡≥⟨ξ, f (D)−1ξ⟩
is jointly convex for positive invertible D →B(H) and ξ →H, where B(H) is the
set of bounded operators on a separable inﬁnite-dimensional Hilbert space H. Now
Theorem 3.1 in [9] is used to conclude that 1/f is matrix monotone decreasing, so
f is matrix monotone.
(ii) ∩(iv) is trivial. Assume (iv); then it follows from (7.17) that (iii) holds for h
instead of f , so (v) holds thanks to (iii) ∩(i) for h. From (7.19) when A = A∪and
D1 = D2 = D, it follows that
⟨A, (J f
D)−1 A⟩= ⟨A, (Jg
D)−1A⟩= ⟨A, (Jh
D)−1A⟩.
Hence (v) implies (iv) by applying (i) ∩(ii) to h.
□
Example 7.13 The χ2-divergence
χ2(p, q) :=
⎝
i
(pi −qi)2
qi
=
⎝
i
⎞pi
qi
−1
⎠2
qi
was ﬁrst introduced by Karl Pearson in 1900 for probability densities p and q. Since
⎝
i
|pi −qi|
⎛2
=
⎝
i

pi
qi
−1
 qi
⎛2
⊥
⎝
i
⎞pi
qi
−1
⎠2
qi,
we have
∗p −q∗2
1 ⊥χ2(p, q).
(7.20)
A quantum generalization was introduced very recently: for density matrices
ρ and σ,
χ2
α(ρ, σ) = Tr
⎣
(ρ −σ)σ−α(ρ −σ)σα−1⎤
= Tr ρσ−αρσα−1 −1
= ⟨ρ, (J f
σ )−1ρ⟩−1,
where α →[0, 1] and f (x) = xα. If ρ and σ commute, then this formula is indepen-
dent of α.
The monotonicity of the χ2-divergence follows from (7.15). The monotonicity
and the classical inequality (7.20) imply that
∗ρ −σ∗2
1 ⊥χ2(ρ, σ).
Indeed, if E is the conditional expectation onto the commutative algebra generated
by ρ −σ, then

288
7
Some Applications
∗ρ −σ∗2
1 = ∗E(ρ) −E(σ)∗2
1 ⊥χ2(E(ρ), E(σ)) ⊥χ2(ρ, σ).
□
7.3 Quantum Markov Triplets
The CCR-algebra used in this section is an inﬁnite-dimensional C*-algebra, but
its parametrization will be by a ﬁnite-dimensional Hilbert space H. (CCR is the
abbreviation of “canonical commutation relation” and the book [69] contains the
details.)
Assume that for every f →H a unitary operator W( f ) is given so that the relations
W( f1)W( f2) = W( f1 + f2) exp(i σ( f1, f2)),
W(−f ) = W( f )∪
hold for f1, f2, f →H with σ( f1, f2) := Im⟨f1, f2⟩. The C*-algebra generated
by these unitaries is unique and denoted by CCR(H). Given a positive operator
A →B(H), a functional ωA : CCR(H) ≥C can be deﬁned as
ωA(W( f )) := exp

−∗f ∗2/2 −⟨f, Af ⟩

.
This is called a Gaussian or quasi-free state. In the so-called Fock representation of
CCR(H)thequasi-freestateωA hasthedensityoperator DA, DA ∈0andTr DA = 1.
We do not describe DA here but we remark that if the λi’s are the eigenvalues of A,
then DA has the eigenvalues

i
1
1 + λi
⎞
λi
1 + λi
⎠ni
,
where the ni’s are non-negative integers. Therefore the von Neumann entropy is
S(ωA) := −Tr DA log DA = Tr κ(A),
(7.21)
where κ(t) := −t log t + (t + 1) log(t + 1) is an interesting special function.
Assume that H = H1 ≤H2 and write the positive mapping A →B(H) in the
form of a block matrix:
A =
⎦A11 A12
A21 A22

.
If f →H1, then
ωA(W( f ≤0)) = exp

−∗f ∗2/2 −⟨f, A11 f ⟩

.

7.3 Quantum Markov Triplets
289
Therefore the restriction of the quasi-free state ωA to CCR(H1) is the quasi-free state
ωA11.
Let H = H1 ≤H2 ≤H3 be a ﬁnite-dimensional Hilbert space and consider the
CCR-algebras CCR(Hi) (1 ⊥i ⊥3). Then
CCR(H) = CCR(H1) ⊗CCR(H2) ⊗CCR(H3)
holds. Assume that D123 is a density operator in CCR(H) and we denote by
D12, D2, D23 its reductions into the subalgebras CCR(H1)⊗CCR(H2), CCR(H2),
CCR(H2)⊗CCR(H3), respectively. These subalgebras form a Markov triplet with
respect to the state D123 if
S(D123) −S(D23) = S(D12) −S(D2),
(7.22)
where S denotes the von Neumann entropy and we assume that both sides are ﬁnite
in the equation. (Note (7.22) is the quantum analogue of (7.5).)
Now we concentrate on the Markov property of a quasi-free state ωA ◦ω123 with
thedensityoperator D123,where A isapositiveoperatoractingonH = H1≤H2≤H3
and it has a block matrix form
A =


A11 A12 A13
A21 A22 A23
A31 A32 A33

.
Then the restrictions D12, D23 and D2 are also Gaussian states with the positive
operators
B =


A11 A12 0
A21 A22 0
0
0
I

,
C =


I
0
0
0 A22 A23
0 A32 A33


and
D =


I
0
0
0 A22 0
0
0
I

,
respectively. Formula (7.21) tells us that the Markov condition (7.22) is
equivalent to
Tr κ(A) + Tr κ(D) = Tr κ(B) + Tr κ(C).
(This kind of condition has appeared already in the study of strongly subadditive
functions, see Theorem 4.50.)
Denote by Pi the orthogonal projection from H onto Hi, 1 ⊥i ⊥3. Of course,
P1 + P2 + P3 = I and we also use the notation P12 := P1 + P2 and P23 := P2 + P3.
Theorem 7.14 Assume that A →B(H) is a positive invertible operator and the
corresponding quasi-free state is denoted by ωA ◦ω123 on CCR(H). Then the
following conditions are equivalent.
(a) S(ω123) + S(ω2) = S(ω12) + S(ω23);
(b) Tr κ(A) + Tr κ(P2 AP2) = Tr κ(P12 AP12) + Tr κ(P23AP23);

290
7
Some Applications
(c) there is a projection P →B(H) such that P1 ⊥P ⊥P1 + P2 and PA = AP.
Proof:
By the formula (7.21), (a) and (b) are equivalent.
Condition (c) tells us that the matrix A has a special form:
A =


A11
a 0
0
⎦a∪
0
 ⎦c 0
0 d
 ⎦0
b

0
0 b∪
A33


=


⎦A11 a
a∪c

0
0
⎦d
b
b∪A33



,
(7.23)
where the parameters a, b, c, d (and 0) are operators. This is a block diagonal matrix:
A =
⎦A1 0
0 A2

,
and the projection P is
⎦I 0
0 0

in this setting.
The Hilbert space H2 is decomposed as HL
2 ≤HR
2 , where HL
2 is the range of the
projection P P2. Therefore,
CCR(H) = CCR(H1 ≤HL
2 ) ⊗CCR(HR
2 ≤H3)
and ω123 becomes a product state ωL ⊗ωR. From this we can easily show the
implication (c) ∩(a).
The essential part is the proof of (b) ∩(c). Now assume (b), that is,
Tr κ(A) + Tr κ(A22) = Tr κ(B) + Tr κ(C).
We notice that the function κ(x) = −x log x +(x +1) log(x +1) admits the integral
representation
κ(x) =
⎥⊂
1
t−2 log(tx + 1) dt.
By Theorem 4.50 applied to t A + I we have
Tr log(t A + I) + Tr log(t A22 + I) ⊥Tr log(t B + I) + Tr log(tC + I)
(7.24)
for every t > 1. Hence it follows from (7.24) that equality holds in (7.24) for almost
every t > 1. By Theorem 4.50 again this implies that

7.3 Quantum Markov Triplets
291
tA13 = tA12(tA22 + I)−1tA23
for almost every t > 1. The continuity gives that for every t > 1 we have
A13 = A12(A22 + t−1I)−1A23.
Since A12(A22 + zI)−1A23 is an analytic function in {z →C : Re z > 0}, we have
A13 = A12(A22 + sI)−1A23
(s →R+).
Letting s ≥⊂shows that A13 = 0. Since A12s(A22 + sI)−1A23 ≥A12 A23 as
s ≥⊂, we also have A12 A23 = 0. The latter condition means that ran A23 ⊃
ker A12, or equivalently (ker A12)∼⊃ker A∪
23.
The linear combinations of the functions x ≡≥1/(s + x) form an algebra and by
the Stone–Weierstrass theorem A12g(A22)A23 = 0 for any continuous function g.
We want to show that the equality implies the structure (7.23) of the operator A.
We have A23 : H3 ≥H2 and A12 : H2 ≥H1. To deduce the structure (7.23), we
have to ﬁnd a subspace H ⊃H2 such that
A22H ⊃H,
H∼⊃ker A12,
H ⊃ker A∪
23,
or alternatively K (= H∼) ⊃H2 should be an invariant subspace of A22 such that
ran A23 ⊃K ⊃ker A12.
Let
K :=
⎧⎝
i
Ani
22 A23xi : xi →H3, ni ∈0
⎪
where the sum is always ﬁnite. K is a subspace of H2. The property ran A23 ⊃K
and the invariance under A22 are obvious. Since
A12 An
22 A23x = 0,
K ⊃ker A12 also follows. The proof is complete.
□
In the theorem it was assumed that H is a ﬁnite-dimensional Hilbert space, but the
proof also works in inﬁnite dimensions. In the theorem the formula (7.23) shows that
A should be a block diagonal matrix. There are nontrivial Markovian Gaussian states
which are not a product in the time localization (H = H1 ≤H2 ≤H3). However,
the ﬁrst and the third subalgebras are always independent.
The next two theorems give different descriptions (but they are not essentially
different).

292
7
Some Applications
Theorem 7.15 For a quasi-free state ωA the Markov property (7.22) is equivalent
to the condition
Ait(I + A)−it D−it(I + D)it = Bit(I + B)−itC−it(I + C)it
for every real t.
Theorem 7.16 The block matrix
A =


A11 A12 A13
A21 A22 A23
A31 A32 A33


gives a Gaussian state with the Markov property if and only if
A13 = A12 f (A22)A23
for any continuous function f : R ≥R.
This shows that the CCR condition is much more restrictive than the classical one.
7.4 Optimal Quantum Measurements
In the matrix formalism the state of a quantum system is a density matrix 0 ⊥ρ →
Md(C) with the property Tr ρ = 1. A ﬁnite set {F(x) : x →X} of positive matrices
is called a positive operator-valued measure (POVM) if
⎝
x→X
F(x) = I,
where F(x) ̸= 0 can be assumed. Quantum state tomography can recover the state ρ
from the probability set {Tr ρF(x) : x →X}. In this section there are arguments for
the optimal POVM set. There are a few rules from quantum theory, but the essential
part is the notion of frames in the Hilbert space Md(C).
The space Md(C) of matrices equipped with the Hilbert–Schmidt inner product
⟨A|B⟩= Tr A∪B is a Hilbert space. We use the bra-ket notation for operators:
⟨A| is an operator bra and |B⟩is an operator ket. Then |A⟩⟨B| is a linear mapping
Md(C) ≥Md(C). For example,
|A⟩⟨B|C = (Tr B∪C)A,
(|A⟩⟨B|)∪= |B⟩⟨A|,
|A1A⟩⟨A2B| = A1|A⟩⟨B|A∪
2
when A1, A2 : Md(C) ≥Md(C).

7.4 Optimal Quantum Measurements
293
For an orthonormal basis {|Ek⟩: 1 ⊥k ⊥d2} of Md(C), a linear superoperator
S : Md(C) ≥Md(C) can then be written as S = ⎡
j,k s jk|E j⟩⟨Ek| and its action
is deﬁned as
S|A⟩=
⎝
j,k
s jk|E j⟩⟨Ek|A⟩=
⎝
j,k
s jk E jTr

Ek∪A

.
We denote the identity superoperator by I, and so I = ⎡
k |Ek⟩⟨Ek|.
The Hilbert space Md(C) has an orthogonal decomposition
{cI : c →C} ≤{A →Md(C) : Tr A = 0}.
In the block matrix form under this decomposition,
I =
⎦1
0
0 Id2−1

and
|I⟩⟨I| =
⎦d 0
0 0

.
Let X be a ﬁnite set. An operator frame is a family of operators {A(x) : x →X}
for which there exists a constant a > 0 such that
a⟨C|C⟩⊥
⎝
x→X
|⟨A(x)|C⟩|2
(7.25)
for all C →Md(C). The frame superoperator is deﬁned as
A =
⎝
x→X
|A(x)⟩⟨A(x)|.
It has the properties
AB =
⎝
x→X
|A(x)⟩⟨A(x)|B⟩=
⎝
x→X
|A(x)⟩Tr A(x)∪B,
Tr A2 =
⎝
x,y→X
|⟨A(x)|A(y)⟩|2 .
(7.26)
The operator A is positive (and self-adjoint), since
⟨B|A|B⟩=
⎝
x→X
|⟨A(x)|B⟩|2 ∈0.
Since this formula shows that (7.25) is equivalent to
aI ⊥A,

294
7
Some Applications
it follows that (7.25) holds if and only if A has an inverse. The frame is called tight
if A = aI.
Let τ : X ≥(0, ⊂). Then {A(x) : x →X} is an operator frame if and only if
{τ(x)A(x) : x →X} is an operator frame.
Let {Ai →Md(C) : 1 ⊥i ⊥k} be a subset of Md(C) such that the linear span
is Md(C). (Then k ∈d2.) This is a simple example of an operator frame. If k = d2,
then the operator frame is tight if and only if {Ai →Md(C) : 1 ⊥i ⊥d2} is an
orthonormal basis up to a constant multiple.
A set {A(x) : x →X} of positive matrices is informationally complete (IC) if
for each pair of distinct quantum states ρ ̸= σ there exists an element x →X such
that Tr A(x)ρ ̸= Tr A(x)σ. When the A(x)’s are all of unit rank, A is said to be
rank-one. It is clear that for numbers λ(x) > 0 the set {A(x) : x →X} is IC if and
only if {λ(x)A(x) : x →X} is IC.
Theorem 7.17 Let {F(x) : x →X} be a POVM. Then F is informationally complete
if and only if {F(x) : x →X} is an operator frame.
Proof:
Let
A =
⎝
x→X
|F(x)⟩⟨F(x)|,
(7.27)
which is a positive operator.
Suppose that F is informationally complete and take an operator A with self-
adjoint decomposition A1 + iA2 such that
⟨A|A|A⟩=
⎝
x→X
|Tr F(x)A|2 =
⎝
x→X
|Tr F(x)A1|2 +
⎝
x→X
|Tr F(x)A2|2 = 0,
then we must have Tr F(x)A1 = Tr F(x)A2 = 0. The operators A1 and A2 are
traceless:
Tr Ai =
⎝
x→X
Tr F(x)Ai = 0
(i = 1, 2).
Take a positive deﬁnite state ρ and a small number ε > 0. Then ρ + εAi can be a
state and we have
Tr F(x)(ρ + εAi) = Tr F(x)ρ
(x →X).
The informationally complete property gives A1 = A2 = 0 and so A = 0. It follows
that A is invertible and the operator frame property follows.
For the converse, assume that for the distinct quantum states ρ ̸= σ we have
⟨ρ −σ|A|ρ −σ⟩=
⎝
x→X
|Tr F(x)(ρ −σ)|2 > 0.

7.4 Optimal Quantum Measurements
295
Then there must exist an x →X such that
Tr (F(x)(ρ −σ)) ̸= 0,
or equivalently, Tr F(x)ρ ̸= Tr F(x)σ, which means that F is informationally
complete.
□
Suppose that a POVM {F(x) : x →X} is used for quantum measurement when the
state is ρ. The outcome of the measurement is an element x →X and its probability
is p(x) = Tr ρF(x). If N measurements are performed on N independent quantum
systems (in the same state), then the results are y1, . . . , yN. The outcome x →X
occurs with some multiplicity and the estimate for the probability is
⊕p(x) =
⊕p(x; y1, . . . , yN) := 1
N
N
⎝
k=1
δ(x, yk).
(7.28)
From this information the state estimate has the form
⊕ρ =
⎝
x→X
⊕p(x)Q(x),
where {Q(x) : x →X} is a set of matrices. If we require that
ρ =
⎝
x→X
Tr (ρF(x))Q(x)
should hold for every state ρ, then {Q(x) : x →X} should satisfy some conditions.
This idea will need the concept of a dual frame.
For a frame {A(x) : x →X}, a dual frame {B(x) : x →X} is a frame such that
⎝
x→X
|B(x)⟩⟨A(x)| = I,
or equivalently for all C →Md(C) we have
C =
⎝
x→X
⟨A(x)|C⟩B(x) =
⎝
x→X
⟨B(x)|C⟩A(x).
The existence of a dual frame is equivalent to the frame inequality (7.25), but
we also have a canonical construction: The canonical dual frame is deﬁned by the
operators
| ˜A(x)⟩:= A−1|A(x)⟩.
(7.29)

296
7
Some Applications
Recall that the inverse of A exists whenever {A(x) : x →X} is an operator frame.
Note that given any operator frame {A(x) : x →X} we can construct a tight frame
as {A−1/2|A(x)⟩: x →X}.
Theorem 7.18 If { ˜A(x) : x →X} is the canonical dual of an operator frame {A(x) :
x →X} with superoperator A, then
A−1 =
⎝
x→X
| ˜A(x)⟩⟨˜A(x)|
and the canonical dual of { ˜A(x) : x →X} is {A(x) : x →X}. For an arbitrary dual
frame {B(x) : x →X} of {A(x) : x →X} the inequality
⎝
x→X
|B(x)⟩⟨B(x)| ∈
⎝
x→X
| ˜A(x)⟩⟨˜A(x)|
holds and equality holds only if B ◦˜A.
Proof:
A and A−1 are self-adjoint superoperators and we have
⎝
x→X
| ˜A(x)⟩⟨˜A(x)| =
⎝
x→X
|A−1A(x)⟩⟨A−1 A(x)|
= A−1⎣⎝
x→X
|A(x)⟩⟨A(x)|
⎤
A−1
= A−1AA−1 = A−1.
The second statement is A| ˜A(x)⟩= |A(x)⟩, which comes immediately from
| ˜A(x)⟩= A−1|A(x)⟩.
Let B be a dual frame of A and deﬁne D(x) := B(x) −˜A(x). Then
⎝
x→X
| ˜A(x)⟩⟨D(x)| =
⎝
x→X
⎣
| ˜A(x)⟩⟨B(x)| −| ˜A(x)⟩⟨˜A(x)|
⎤
= A−1 ⎝
x→X
|A(x)⟩⟨B(x)| −A−1 ⎝
x→X
|A(x)⟩⟨A(x)|A−1
= A−1I −A−1AA−1 = 0.
The adjoint gives
⎝
x→X
|D(x)⟩⟨˜A(x)| = 0 ,
and

7.4 Optimal Quantum Measurements
297
⎝
x→X
|B(x)⟩⟨B(x)| =
⎝
x→X
| ˜A(x)⟩⟨˜A(x)| +
⎝
x→X
| ˜A(x)⟩⟨D(x)|
+
⎝
x→X
|D(x)⟩⟨˜A(x)| +
⎝
x→X
|D(x)⟩⟨D(x)|
=
⎝
x→X
| ˜A(x)⟩⟨˜A(x)| +
⎝
x→X
|D(x)⟩⟨D(x)|
∈
⎝
x→X
| ˜A(x)⟩⟨˜A(x)|
with equality if and only if D ◦0.
□
We have the following inequality, which is also known as the frame bound.
Theorem 7.19 Let {A(x) : x →X} be an operator frame with superoperator A.
Then the inequality
⎝
x,y→X
|⟨A(x)|A(y)⟩|2 ∈(Tr A)2
d2
(7.30)
holds, and we have equality if and only if {A(x) : x →X} is a tight operator frame.
Proof:
By (7.26) the left-hand side is Tr A2, so the inequality holds. It is clear that
equality holds if and only if all eigenvalues of A are the same, that is, A = cI.
□
The trace measure τ is deﬁned by τ(x) := Tr F(x). The useful superoperator is
F =
⎝
x→X
|F(x)⟩⟨F(x)|(τ(x))−1.
Formally this is different from the frame superoperator (7.27). Therefore, we express
the POVM F as
F(x) = P0(x)
⎨
τ(x)
(x →X)
where {P0(x) : x →X} is called a positive operator-valued density (POVD). Then
F =
⎝
x→X
|P0(x)⟩⟨P0(x)| =
⎝
x→X
|F(x)⟩⟨F(x)|(τ(x))−1.
(7.31)
F is invertible if and only if A in (7.27) is invertible. As a corollary, we see that
for an informationally complete POVM F, the POVD P0 can be considered as a
generalized operator frame. The canonical dual frame (in the sense of (7.29)) then
deﬁnes a reconstruction operator-valued density
|R0(x)⟩:= F−1|P0(x)⟩
(x →X).

298
7
Some Applications
We also use the notation R(x) := R0(x)τ(x)−1/2. The identity
⎝
x→X
|R(x)⟩⟨F(x)| =
⎝
x→X
|R0(x)⟩⟨P0(x)| =
⎝
x→X
F−1|P0(x)⟩⟨P0(x)| = F−1F = I
(7.32)
then allows state reconstruction in terms of the measurement statistics:
ρ =
⎝
x→X
|R(x)⟩⟨F(x)|
⎛
ρ =
⎝
x→X
(Tr F(x)ρ)R(x).
(7.33)
So this state-reconstruction formula is an immediate consequence of the action of
(7.32) on ρ.
Theorem 7.20 We have
F−1 =
⎝
x→X
|R(x)⟩⟨R(x)|τ(x)
(7.34)
and the operators R(x) are self-adjoint and Tr R(x) = 1.
Proof:
From the mutual canonical dual relation of {P0(x) : x →X} and {R0(x) :
x →X} we have
F−1 =
⎝
x→X
|R0(x)⟩⟨R0(x)|
by Theorem 7.18, and this is (7.34).
The operators R(x) are self-adjoint since F and thus F−1 map self-adjoint opera-
tors to self-adjoint operators. For an arbitrary POVM, the identity operator is always
an eigenvector of the POVM superoperator:
F|I⟩=
⎝
x→X
|F(x)⟩⟨F(x)|I⟩(τ(x))−1 =
⎝
x→X
|F(x)⟩= |I⟩.
(7.35)
Thus |I⟩is also an eigenvector of F−1, and we obtain
Tr R(x) = ⟨I|R(x)⟩= τ(x)−1/2⟨I|R0(x)⟩= τ(x)−1/2⟨I|F−1P0(x)⟩
= τ(x)−1/2⟨I|P0(x)⟩= τ(x)−1⟨I|F(x)⟩= τ(x)−1τ(x) = 1 .
□
Note that we need |X| ∈d2 for F to be informationally complete. If this were
not the case then F could not have full rank. An IC-POVM with |X| = d2 is called
minimal. In this case the reconstruction OVD is unique. In general, however, there
will be many different choices.

7.4 Optimal Quantum Measurements
299
Example 7.21 Let x1, x2, . . . , xd be an orthonormal basis of Cd. Then Qi =
|xi⟩⟨xi| are projections and {Qi : 1 ⊥i ⊥d} is a POVM. However, it is not
informationally complete. The subset
A :=
⎧
d
⎝
i=1
λi|xi⟩⟨xi| : λ1, λ2, . . . , λd →C
⎪
⊃Md(C)
is a maximal abelian *-subalgebra, called a MASA.
A good example of an IC-POVM comes from d + 1 similar sets:
{Q(m)
k
: 1 ⊥k ⊥d, 1 ⊥m ⊥d + 1}
consists of projections of rank one and
Tr Q(m)
k
Q(n)
l
=
⎩
δkl
if
m = n,
1/d
if
m ̸= n.
A POVM is deﬁned by
X := {(k, m) : 1 ⊥k ⊥d, 1 ⊥m ⊥d + 1}
and
F(k, m) :=
1
d + 1 Q(m)
k
,
τ(k, m) :=
1
d + 1
for (k, m) →X. (Here τ is constant and this is a uniformity.) We have
⎝
(k,m)→X
|F(k, m)⟩⟨F(k, m)|Q(n)
l
=
1
(d + 1)2
⎣
Q(n)
l
+ I
⎤
.
This implies
FA =
⎝
x→X
|F(x)⟩⟨F(x)|(τ(x))−1
⎛
A =
1
(d + 1) (A + (Tr A)I).
So F is rather simple: if Tr A = 0, then FA =
1
d+1 A and FI = I. (Another
formulation is given in (7.36).)
This example is a complete set of mutually unbiased bases (MUBs) [54, 85].
The deﬁnition
Am :=
⎧
d
⎝
k=1
λk Q(m)
k
: λ1, λ2, . . . , λd →C
⎪
⊃Md(C)

300
7
Some Applications
gives d + 1 MASAs. These MASAs are quasi-orthogonal in the following sense.
If Ai →Ai and Tr Ai = 0 (1 ⊥i ⊥d + 1), then Tr Ai A j = 0 for i ̸= j. The
construction of d + 1 quasi-orthogonal MASAs is known when d is a prime-power
(see also [31]). But d = 6 is not a prime-power and it is already a problematic
example.
□
It is straightforward to conﬁrm that we have the decomposition
F = 1
d |I⟩⟨I| +
⎝
x→X
|P(x) −I/d⟩⟨P(x) −I/d|τ(x)
for any POVM superoperator (7.31), where P(x) := P0(x)τ(x)−1/2 = F(x)τ(x)−1
and
1
d |I⟩⟨I| =
⎦1 0
0 0

is the projection onto the subspace CI. Setting
I0 :=
⎦0
0
0 Id2−1

,
an IC-POVM {F(x) : x →X} is tight if
⎝
x→X
|P(x) −I/d⟩⟨P(x) −I/d|τ(x) = aI0.
Theorem 7.22 F is a tight rank-one IC-POVM if and only if
F = I + |I⟩⟨I|
d + 1
=
⎦1
0
0
1
d+1 Id2−1

.
(7.36)
(The latter is in the block matrix form.)
Proof:
The constant a can be found by taking the superoperator trace:
a =
1
d2 −1
⎝
x→X
⟨P(x) −I/d|P(x) −I/d⟩τ(x)
=
1
d2 −1
⎝
x→X
⟨P(x)|P(x)⟩τ(x) −1
⎛
.
The POVM superoperator of a tight IC-POVM satisﬁes the identity
F = aI + 1 −a
d
|I⟩⟨I| .
(7.37)

7.4 Optimal Quantum Measurements
301
In the special case of a rank-one POVM a takes its maximum possible value
1/(d + 1). Since this is in fact only possible for rank-one POVMs, by noting that
(7.37) can be taken as an alternative deﬁnition in the general case, we obtain the
proposition.
□
It follows from (7.36) that
F−1 =
⎦1
0
0 (d + 1)Id2−1

= (d + 1)I −|I⟩⟨I|.
This shows that Example 7.21 contains a tight rank-one IC-POVM. Here is another
example.
Example 7.23 An example of an IC-POVM is the symmetric informationally
complete POVM (SIC POVM). The set {Qk : 1 ⊥k ⊥d2} consists of projec-
tions of rank one such that
Tr Qk Ql =
1
d + 1
(k ̸= l).
Then X := {x : 1 ⊥x ⊥d2} and
F(x) = 1
d Qx,
F = 1
d
⎝
x→X
|Qx⟩⟨Qx|.
We have some simple computations: FI = I and
F(Qk −I/d) =
1
d + 1(Qk −I/d).
This implies that if Tr A = 0, then
FA =
1
d + 1 A .
So the SIC POVM is a tight rank-one IC-POVM.
SIC-POVMs are conjectured to exist in all dimensions [12, 86].
□
The next theorem tells us that the SIC POVM is characterized by the IC POVM
property.
Theorem 7.24 If a set {Qk →Md(C) : 1 ⊥k ⊥d2} consists of projections of rank
one such that
d2
⎝
k=1
λk|Qk⟩⟨Qk| = I + |I⟩⟨I|
d + 1
(7.38)

302
7
Some Applications
with numbers λk > 0, then
λi = 1
d ,
Tr Qi Q j =
1
d + 1
(i ̸= j).
Proof:
Note that if both sides of (7.38) are applied to |I⟩, then we get
d2
⎝
i=1
λi Qi = I.
(7.39)
First we show that λi = 1/d. From (7.38) we have
d2
⎝
i=1
λi⟨A|Qi⟩⟨Qi|A⟩= ⟨A|I + |I⟩⟨I|
d + 1
|A⟩
(7.40)
with
A := Qk −
1
d + 1 I.
(7.40) becomes
λk
d2
(d + 1)2 +
⎝
j̸=k
λ j
⎣
Tr Q j Qk −
1
d + 1
⎤2
=
d
(d + 1)2 .
(7.41)
The inequality
λk
d2
(d + 1)2 ⊥
d
(d + 1)2
gives λk ⊥1/d for every 1 ⊥k ⊥d2. The trace of (7.39) is
d2
⎝
i=1
λi = d.
Hence it follows that λk = 1/d for every 1 ⊥k ⊥d2. So from (7.41) we have
⎝
j̸=k
λ j
⎣
Tr Q j Qk −
1
d + 1
⎤2
= 0
and this gives the result.
□
The state-reconstruction formula for a tight rank-one IC-POVM also takes an
elegant form. From (7.33) we have

7.4 Optimal Quantum Measurements
303
ρ =
⎝
x→X
R(x)p(x) =
⎝
x→X
F−1P(x)p(x) =
⎝
x→X
((d + 1)P(x) −I)p(x)
and obtain
ρ = (d + 1)
⎝
x→X
P(x)p(x) −I .
Finally, let us rewrite the frame bound (Theorem 7.19) for the context of quantum
measurements.
Theorem 7.25 Let {F(x) : x →X} be a POVM. Then
⎝
x,y→X
⟨P(x)|P(y)⟩2τ(x)τ(y) ∈1 +

Tr F −1
2
d2 −1
,
(7.42)
with equality if and only if F is a tight IC-POVM.
Proof:
The frame bound (7.30) has a slightly improved form
Tr (A2) ∈

Tr (A)
2/D,
where D is the dimension of the operator space. Setting A = F −1
d |I⟩⟨I| and
D = d2 −1 for Md(C) ⊗CI then gives (7.42) (using (7.35)).
□
Informationally complete quantum measurements are precisely those measure-
ments which can be used for quantum state tomography. We will show that, amongst
all IC-POVMs, the tight rank-one IC-POVMs are the most robust against statistical
error in the quantum tomographic process. We will also ﬁnd that, for an arbitrary
IC-POVM, the canonical dual frame with respect to the trace measure is the optimal
dual frame for state reconstruction. These results are shown only for the case of linear
quantum state tomography.
Consider a state-reconstruction formula of the form
ρ =
⎝
x→X
p(x)Q(x) =
⎝
x→X
(Tr F(x)ρ)Q(x),
(7.43)
where Q(x) : X ≥Md(C) is an operator-valued density. If this formula is to remain
valid for all ρ, then we must have
⎝
x→X
|Q(x)⟩⟨F(x)| = I =
⎝
x→X
|Q0(x)⟩⟨P0(x)|,
(7.44)
where Q0(x) = τ(x)1/2Q(x) and P0(x) = τ(x)−1/2F(x). Equation (7.44) forces
{Q(x) : x →X} to be a dual frame of {F(x) : x →X}. Similarly {Q0(x) : x →X} is
a dual frame of {P0(x) : x →X}. Our ﬁrst goal is to ﬁnd the optimal dual frame.

304
7
Some Applications
Suppose that we take N independent random samples, y1, . . . , yN, and the out-
come x occurs with some unknown probability p(x). Our estimate for this probability
is (7.28) which of course obeys the expectation E[ ⊕p(x)] = p(x). An elementary cal-
culation shows that the expected covariance for two samples is
E

p(x) −⊕p(x)

p(y) −⊕p(y)

=
1
N
⎣
p(x)δ(x, y) −p(x)p(y)
⎤
.
(7.45)
Now suppose that the p(x)’s are the outcome probabilities for an informationally
complete quantum measurement of the state ρ, p(x) = Tr F(x)ρ. The estimate of ρ
is
⊕ρ = ⊕ρ(y1, . . . , yN) :=
⎝
x→X
⊕p(x; y1, . . . , yN)Q(x),
and the error can be measured by the squared Hilbert–Schmidt distance:
∗ρ −⊕ρ∗2
2 = ⟨ρ −⊕ρ, ρ −⊕ρ⟩=
⎝
x,y→X

p(x) −⊕p(x)

p(y) −⊕p(y)

⟨Q(x), Q(y)⟩,
which has the expectation E

∗ρ−⊕ρ∗2
2

. We want to minimize this quantity, not for an
arbitrary ρ, but for some average. (Integration will be on the set of unitary matrices
with respect to the Haar measure.)
Theorem 7.26 Let {F(x) : x →X} be an informationally complete POVM which
has a dual frame {Q(x) : x →X} as an operator-valued density. The quantum system
has a state σ and y1, . . . , yN are random samples of the measurements. Then
⊕p(x) := 1
N
N
⎝
k=1
δ(x, yk),
⊕ρ :=
⎝
x→X
⊕p(x)Q(x).
Finally let ρ = ρ(σ,U) := UσU ∪be parametrized by a unitary U. Then for the
average squared distance
⎥
U
E

∗ρ −⊕ρ∗2
2

dμ(U) ∈
1
N
⎞1
d Tr (F−1) −Tr (σ2)
⎠
(7.46)
∈1
N
⎣
d(d + 1) −1 −Tr (σ2)
⎤
.
(7.47)
Equality in the inequality (7.46) occurs if and only if Q is the reconstruction operator-
valued density (deﬁned as |R(x)⟩= F−1|P(x)) and equality in the inequality (7.47)
occurs if and only if F is a tight rank-one IC-POVM.
Proof:
For a ﬁxed IC-POVM F we have

7.4 Optimal Quantum Measurements
305
E

∗ρ −⊕ρ∗2
2

= 1
N
⎝
x,y→X

p(x)δ(x, y) −p(x)p(y)

⟨Q(x), Q(y)⟩
= 1
N
⎞⎝
x→X
p(x)⟨Q(x), Q(x)⟩−⟨
⎝
x→X
p(x)Q(x)
⎝
y→X
p(y)Q(y)⟩
⎠
= 1
N
⎣
αp(Q) −Tr (ρ2)
⎤
,
where the formulas (7.45) and (7.43) are used and moreover
αp(Q) :=
⎝
x→X
p(x) ⟨Q(x), Q(x)⟩.
Since we have no control over Tr ρ2, we want to minimize αp(Q). The IC-
POVM which minimizes αp(Q) will in general depend on the quantum state under
examination. We thus set ρ = ρ(σ,U) := UσU ∪, and now remove this dependence
by taking the Haar average μ(U) over all U →U(d). Note that
⎥
U(d)
U PU ∪dμ(U)
is the same constant C for any projection of rank 1. If ⎡d
i=1 Pi = I, then
dC =
d
⎝
i=1
⎥
U(d)
U PU ∪dμ(U) = I
and we have C = I/d. Therefore for A = ⎡d
i=1 λi Pi we have
⎥
U(d)
U AU ∪dμ(U) =
d
⎝
i=1
λiC = I
d Tr A.
This fact implies
⎥
U(d)
αp(Q) dμ(U) =
⎝
x→X
Tr
⎞
F(x)
⎥
U(d)
UσU ∪dμ(U)
⎠
⟨Q(x), Q(x)⟩
= 1
d
⎝
x→X
Tr F(x) Tr σ⟨Q(x), Q(x)⟩
= 1
d
⎝
x→X
τ(x) ⟨Q(x), Q(x)⟩=: 1
d ατ(Q) ,

306
7
Some Applications
where τ(x) := Tr F(x). We will now minimize ατ(Q) over all choices for Q, while
keeping the IC-POVM F ﬁxed. Our only constraint is that {Q(x) : x →X} remains
a dual frame to {F(x) : x →X} (see (7.44)), so that the reconstruction formula
(7.43) remains valid for all ρ. Theorem 7.18 shows that the reconstruction OVD
{R(x) : x →X} deﬁned as |R⟩= F−1|P⟩is the optimal choice for the dual frame.
Equation (7.34) shows that ατ(R) = Tr (F−1). We will minimize the quantity
Tr F−1 =
d2
⎝
k=1
1
λk
,
(7.48)
where λ1, . . . , λd2 > 0 denote the eigenvalues of F. These eigenvalues satisfy the
constraint
d2
⎝
k=1
λk = Tr F =
⎝
x→X
τ(x)Tr |P(x)⟩⟨P(x)| ⊥
⎝
x→X
τ(x) = d ,
since Tr |P(x)⟩⟨P(x)| = Tr P(x)2 ⊥1. We know that the identity operator I is an
eigenvalue of F:
FI =
⎝
x→X
τ(x)|P(x)⟩= I.
Thus we in fact take λ1 = 1 and then ⎡d2
k=2 λk ⊥d −1. Under this latter constraint it
is straightforward to show that the right-hand side of (7.48) takes its minimum value
if and only if λ2 = · · · = λd2 = (d −1)/(d2 −1) = 1/(d + 1), or equivalently,
F = 1 · |I⟩⟨I|
d
+
1
d + 1
⎞
I −|I⟩⟨I|
d
⎠
.
(7.49)
Therefore, by Theorem 7.22, Tr F−1 takes its minimum value if and only if F is a
tight rank-one IC-POVM. The minimum of Tr F−1 comes from (7.49).
□
7.5 The Cramér–Rao Inequality
The Cramér–Rao inequality belongs to estimation theory in mathematical statistics.
Assume that we have to estimate the state ρθ, where θ = (θ1, θ2, . . . , θN) lies in a
subset of RN. There is a sequence of estimates n : Xn ≥RN. In mathematical
statistics the N × N mean quadratic error matrix
Vn(θ)i, j :=
⎥
Xn
(n(x)i −θi)(n(x) j −θ j) dμn,θ(x)
(1 ⊥i, j ⊥N)

7.5 The Cramér–Rao Inequality
307
is used to express the efﬁciency of the nth estimation and in a good estimation scheme
Vn(θ) = O(n−1) is expected. Here, Xn is the set of measurement outcomes and μn,θ
is the probability distribution when the true state is ρθ. An unbiased estimation
scheme means
⎥
Xn
n(x)i dμn,θ(x) = θi
(1 ⊥i ⊥N)
and the formula simpliﬁes:
Vn(θ)i, j :=
⎥
Xn
n(x)in(x) j dμn,θ(x) −θiθ j .
(In mathematical statistics, this is sometimes called the covariance matrix of the
estimate.)
The mean quadratic error matrix is used to measure the efﬁciency of an estimate.
Even if the value of θ is ﬁxed, for two different estimates the corresponding matrices
are not always comparable, because the ordering of positive deﬁnite matrices is
highly partial. This fact has inconvenient consequences in classical statistics. In the
state estimation of a quantum system the very different possible measurements make
the situation even more complicated.
Assume that dμn,θ(x) = fn,θ(x) dx and ﬁx θ. fn,θ is called the likelihood func-
tion. Let
∂j :=
∂
∂θ j
.
Differentiating the relation
⎥
Xn
fn,θ(x) dx = 1,
we have
⎥
Xn
∂j fn,θ(x) dx = 0.
If the estimation scheme is unbiased, then
⎥
Xn
n(x)i∂j fn,θ(x) dx = δi, j.
Combining these, we conclude that
⎥
Xn
(n(x)i −θi)∂j fn,θ(x) dx = δi, j
for every 1 ⊥i, j ⊥N. This condition may be written in the slightly different form

308
7
Some Applications
⎥
Xn
⎣
(n(x)i −θi)
⎨
fn,θ(x)
⎤∂j fn,θ(x)
⎨
fn,θ(x)
dx = δi, j.
Now the ﬁrst factor of the integrand depends on i while the second one depends on
j. We need the following lemma.
Lemma 7.27 Assume that ui, vi are vectors in a Hilbert space such that
⟨ui, v j⟩= δi, j
(i, j = 1, 2, . . . , N).
Then the inequality
A ∈B−1
holds for the N × N matrices
Ai, j = ⟨ui, u j⟩and Bi, j = ⟨vi, v j⟩
(1 ⊥i, j ⊥N).
The lemma applies to the vectors
ui = (n(x)i −θi)
⎨
fn,θ(x) and v j = ∂j fn,θ(x)
⎨
fn,θ(x)
and the matrix A will be precisely the mean square error matrix Vn(θ), while in place
of B we have
In(θ)i, j =
⎥
Xn
∂i( fn,θ(x))∂j( fn,θ(x))
f 2
n,θ(x)
dμn,θ(x).
Therefore, the lemma tells us the following:
Theorem 7.28 For an unbiased estimation scheme the matrix inequality
Vn(θ) ∈In(θ)−1
(7.50)
holds (if the likelihood functions fn,θ satisfy certain regularity conditions).
This is the classical Cramér–Rao inequality. The right-hand side is called the
Fisher information matrix. The essential content of the inequality is that the lower
bound is independent of the estimate n but depends on the classical likelihood
function. The inequality is called classical because on both sides classical statistical
quantities appear.
Example 7.29 Let F be a measurement with values in the ﬁnite set X and assume
that ρθ = ρ + ⎡n
i=1 θi Bi, where the Bi are self-adjoint operators with Tr Bi = 0.
We want to compute the Fisher information matrix at θ = 0.

7.5 The Cramér–Rao Inequality
309
Since
∂iTr ρθF(x) = Tr Bi F(x)
for 1 ⊥i ⊥n and x →X, we have
Ii j(0) =
⎝
x→X
Tr Bi F(x)Tr B j F(x)
Tr ρF(x)
.
□
The essential point in the quantum Cramér–Rao inequality compared with
Theorem 7.28 is that the lower bound is a quantity determined by the family .
Theorem 7.28 allows us to compare different estimates for a given measurement but
two different measurements are not comparable.
As a starting point we give a very general form of the quantum Cramér–Rao
inequalityinthesimplesettingof asingleparameter. For θ →(−ε, ε) ⊃Rastatistical
operator ρθ is given and the aim is to estimate the value of the parameter θ close to
0. Formally ρθ is an m × m positive semideﬁnite matrix of trace 1 which describes a
mixed state of a quantum mechanical system and we assume that ρθ is smooth (in θ).
Assume that an estimation is performed by the measurement of a self-adjoint matrix
A playing the role of an observable. (In this case the positive operator-valued measure
on R is the spectral measure of A.) A is an unbiased estimator when Tr ρθ A = θ.
Assume that the true value of θ is close to 0. A is called a locally unbiased estimator
(at θ = 0) if
∂
∂θTr ρθ A

θ=0 = 1 .
(7.51)
Ofcourse,thisconditionholdsif A isanunbiasedestimatorforθ.TorequireTr ρθ A =
θ for all values of the parameter might be a serious restriction on the observable A
and therefore we prefer to use the weaker condition (7.51).
Example 7.30 Let
ρθ :=
exp(H + θB)
Tr exp(H + θB)
and assume that ρ0 = eH is a density matrix and Tr eH B = 0. The Fréchet derivative
of ρθ (at θ = 0) is
$ 1
0 et H Be(1−t)H dt. Hence the self-adjoint operator A is locally
unbiased if
⎥1
0
Tr ρt
0Bρ1−t
0
A dt = 1.
(Note that ρθ is a quantum analogue of the exponential family; in terms of physics
ρθ is a Gibbsian family of states.)
□

310
7
Some Applications
Let ϕρ[B, C] = Tr Jρ(B)C be an inner product on the linear space of self-adjoint
matrices. ϕρ[ · , · ] and the corresponding superoperator Jρ depend on the density
matrix ρ; the notation reﬂects this fact. When ρθ is smooth in θ as already assumed
above, we have
∂
∂θTr ρθB

θ=0 = ϕρ0[B, L]
(7.52)
for some L = L∪. From (7.51) and (7.52) we have ϕρ0[A, L] = 1, and the Schwarz
inequality yields:
Theorem 7.31
ϕρ0[A, A] ∈
1
ϕρ0[L, L] .
(7.53)
This is the quantum Cramér–Rao inequality for a locally unbiased estimator. It
is instructive to compare Theorem 7.31 with the classical Cramér–Rao inequality. If
A = ⎡
i λi Ei is the spectral decomposition, then the corresponding von Neumann
measurement is F = ⎡
i δλi Ei. Take the estimate (λi) = λi. Then the mean
quadratic error is ⎡
i λ2
i Tr ρ0Ei (at θ = 0) which is precisely the left-hand side of
the quantum inequality provided that
ϕρ0[B, C] = 1
2Tr ρ0(BC + C B) .
Generally, we want to interpret the left-hand side as a sort of generalized variance
of A. To do this it is useful to assume that
ϕρ[B, B] = Tr ρB2
if
Bρ = ρB .
However, in the non-commutative setting the statistical interpretation seems to be
rather problematic and thus we call this quantity the quadratic cost functional.
The right-hand side of (7.53) is independent of the estimator and provides a lower
bound for the quadratic cost. The denominator ϕρ0[L, L] appears to play the role of
Fisher information here. We call it the quantum Fisher information with respect
to the cost function ϕρ0[ · , · ]. This quantity depends on the tangent of the curve ρθ.
If the densities ρθ and the estimator A commute, then
L = ρ−1
0
dρθ
dθ

θ=0 = d
dθ log ρθ

θ=0 ,
ϕ0[L, L] = Tr ρ−1
0
⎞dρθ
dθ

θ=0
⎠2
= Tr ρ0
⎞
ρ−1
0
dρθ
dθ

θ=0
⎠2
.
The ﬁrst formula justiﬁes calling L the logarithmic derivative.
A coarse-graining is an afﬁne mapping sending density matrices into density
matrices. Such a mapping extends to all matrices and provides a positive and trace-

7.5 The Cramér–Rao Inequality
311
preserving linear transformation. A common example of coarse-graining sends a
density matrix ρ12 of a composite system Mm1 ⊗Mm2 into the (reduced) density
matrixρ1 ofcomponentMm1.Thereareseveralreasonstoassumecompletepositivity
for a coarse graining and we do so. Mathematically a coarse-graining is the same as
a state transformation in an information channel. The terminology coarse-graining
is used when the statistical aspects are focused on. A coarse-graining is the quantum
analogue of a statistic.
Assume that ρθ = ρ + θB is a smooth curve of density matrices with tangent
B := dρ/dθ at ρ. The quantum Fisher information Fρ(B) is an information quantity
associated with the pair (ρ, B). It appeared in the Cramér–Rao inequality above and
the classical Fisher information gives a bound for the variance of a locally unbiased
estimator. Now let α be a coarse-graining. Then α(ρθ) is another curve in the state
space. Due to the linearity of α, the tangent at α(ρ) is α(B). As is usual in statistics,
information cannot be gained by coarse graining, therefore we expect that the Fisher
information at the density matrix ρ in the direction B must be larger than the Fisher
information at α(ρ) in the direction α(B). This is the monotonicity property of the
Fisher information under coarse-graining:
Fρ(B) ∈Fα(ρ)(α(B)) .
(7.54)
Although we do not want to have a concrete formula for the quantum Fisher infor-
mation, we require that this monotonicity condition must hold. Another requirement
is that Fρ(B) should be quadratic in B. In other words, there exists a non-degenerate
real bilinear form γρ(B, C) on the self-adjoint matrices such that
Fρ(B) = γρ(B, B).
(7.55)
When ρ is regarded as a point of a manifold consisting of density matrices and B is
considered as a tangent vector at the foot point ρ, the quadratic quantity γρ(B, B)
may be regarded as a Riemannian metric on the manifold. This approach gives a
geometric interpretation of the Fisher information.
The requirements (7.54) and (7.55) are strong enough to obtain a reasonable but
still wide class of possible quantum Fisher informations.
We may assume that
γρ(B, C) = Tr BJ−1
ρ (C)
for an operator Jρ acting on all matrices. (This formula expresses the inner product
γρ by means of the Hilbert–Schmidt inner product and the positive linear operator
Jρ.) In terms of the operator Jρ the monotonicity condition reads as
α∪J−1
α(ρ)α ⊥J−1
ρ
(7.56)
for every coarse graining α. (α∪stands for the adjoint of α with respect to the
Hilbert–Schmidt product. Recall that α is completely positive and trace preserving

312
7
Some Applications
if and only if α∪is completely positive and unital.) On the other hand the latter
condition is equivalent to
αJρα∪⊥Jα(ρ) .
(7.57)
It is interesting to observe the relevance of a certain quasi-entropy:
⟨Bρ1/2, f (LρR−1
ρ )Bρ1/2⟩= SB
f (ρ∗ρ),
see (7.9) and (7.11), where Lρ and Rρ are in (7.10). When f : R+ ≥R is matrix
monotone (we always assume f (1) = 1),
⟨α∪(B)ρ1/2, f (LρR−1
ρ )α∪(B)ρ1/2⟩⊥⟨Bα(ρ)1/2, f (Lα(ρ)R−1
α(ρ))Bα(ρ)1/2⟩
due to the monotonicity of the quasi-entropy, see Theorem 7.7. If we set
Jρ = J f
ρ := f (LρR−1
ρ )Rρ,
then (7.57) holds. Therefore,
ϕρ[B, B] := Tr BJρ(B) = ⟨Bρ1/2, f (LρR−1
ρ )Bρ1/2⟩
can be called a quadratic cost function and the corresponding monotone quantum
Fisher information
γρ(B, C) = Tr BJ−1
ρ (C)
will be real for self-adjoint B and C if the function f satisﬁes the condition f (x) =
x f (x−1), see (7.14). This is nothing but a monotone metric, as described in Sect.7.2.
Example 7.32 In order to understand the action of the operator Jρ, assume that
ρ is diagonal, ρ = ⎡
i pi Eii. Then one can check that the matrix units Ekl are
eigenvectors of Jρ, namely
Jρ(Ekl) = pl f (pk/pl)Ekl.
The condition f (x) = x f (x−1) gives that the eigenvectors Ekl and Elk have the same
eigenvalues. Therefore, the symmetrized matrix units Ekl + Elk and iEkl −iElk are
eigenvectors as well.
Since
B =
⎝
k<l
Re Bkl(Ekl + Elk) +
⎝
k<l
ImBkl(iEkl −iElk) +
⎝
i
Bii Eii,
we have
γρ(B, B) = 2
⎝
k<l
1
pk f (pk/pl)|Bkl|2 +
⎝
i
1
pi
|Bii|2.

7.5 The Cramér–Rao Inequality
313
In place of 2 ⎡
k<l, we can write ⎡
k̸=l.
□
Any monotone cost function has the property ϕρ[B, B] = Tr ρB2 for commuting
ρ and B. The examples below show that this is not so in general.
Example 7.33 The analysis of matrix monotone functions leads to the fact that
among all monotone quantum Fisher informations there is a smallest one which
corresponds to the (largest) function fmax(t) = (1 + t)/2. In this case
Fmin
ρ
(B) = Tr BL = Tr ρL2,
where
ρL + Lρ = 2B.
For the purpose of a quantum Cramér–Rao inequality the minimal quantity seems to
be the best, since the inverse gives the largest lower bound. In fact, the matrix L has
been used for a long time under the name of the symmetric logarithmic derivative.
In this example the quadratic cost function is
ϕρ[B, C] = 1
2Tr ρ(BC + C B)
and we have
Jρ(B) = 1
2(ρB + Bρ)
and
J−1
ρ (C) = 2
$ ⊂
0
e−tρCe−tρ dt
for the operator Jρ. Since J−1
ρ is the smallest, Jρ is the largest (among all possibilities).
There is a largest among all monotone quantum Fisher informations and this
corresponds to the function fmin(t) = 2t/(1 + t). In this case
J−1
ρ (B) = 1
2(ρ−1B + Bρ−1)
and
Fmax
ρ
(B) = Tr ρ−1B2.
It is known that the function
fα(t) = α(1 −α)
(t −1)2
(tα −1)(t1−α −1)
is matrix monotone for α →(0, 1). We denote by Fα the corresponding Fisher
information. When X is self-adjoint, B = i[ρ, X] := i(ρX −Xρ) is orthogonal to
the commutator of the foot point ρ in the tangent space (see Example 3.30), and we
have
Fα
ρ (B) = −
1
α(1 −α)Tr

[ρα, X][ρ1−α, X]

.
(7.58)
Apart from a constant factor this expression is the skew information proposed by
Wigner and Yanase some time ago. In the limiting cases α ≥0 or 1 we have
f0(t) = 1 −t
log t

314
7
Some Applications
and the corresponding quantum Fisher information
γ0
ρ(B, C) = Kρ(B, C) :=
⎥⊂
0
Tr B(ρ + t)−1C(ρ + t)−1 dt
will be named here after Kubo and Mori. The Kubo–Mori inner product plays a
role in quantum statistical mechanics. In this case J is the so-called Kubo transform
K (and J−1 is the inverse Kubo transform K−1),
K−1
ρ (B) :=
⎥⊂
0
(ρ + t)−1B(ρ + t)−1 dt
and Kρ(C) :=
⎥1
0
ρtCρ1−t dt .
Therefore the corresponding generalized variance is
ϕρ[B, C] =
⎥1
0
Tr BρtCρ1−t dt .
All Fisher informations discussedinthis examplearepossibleRiemannianmetrics
of manifolds of invertible density matrices. (Manifolds of pure states are rather
different.)
□
A Fisher information appears not only as a Riemannian metric but as an informa-
tion matrix as well. Let M := {ρθ : θ →G} be a smooth m-dimensional manifold of
invertible density matrices. The quantum score operators (or logarithmic deriva-
tives) aredeﬁned as
Li(θ) := J−1
ρθ

∂θi ρθ

(1 ⊥i ⊥m),
and
Qi j(θ) := Tr Li(θ)Jρθ

L j(θ)

(1 ⊥i, j ⊥m)
is the quantum Fisher information matrix. This matrix depends on a matrix
monotone function which is involved in the superoperator J. Historically the matrix
Q determined by the symmetric logarithmic derivative (or the function fmax(t) =
(1 + t)/2) ﬁrst appeared in the work of Helstrøm. Therefore, we call this the Hel-
strøm information matrix and it will be denoted by H(θ).
Theorem 7.34 Fix a matrix monotone function f to induce quantum Fisher infor-
mation. Let α be a coarse-graining sending density matrices on the Hilbert space
H1 into those acting on the Hilbert space H2 and let M := {ρθ : θ →G} be a
smooth m-dimensional manifold of invertible density matrices on H1. For the Fisher
information matrix Q(1)(θ) of M and for the Fisher information matrix Q(2)(θ) of
α(M) := {α(ρθ) : θ →G}, we have the monotonicity relation
Q(2)(θ) ⊥Q(1)(θ).
(7.59)

7.5 The Cramér–Rao Inequality
315
(This is an inequality between m × m positive matrices.)
Proof:
Set Bi(θ) := ∂θi ρθ. Then J−1
α(ρθ)α(Bi(θ)) is the score operator of α(M).
Using (7.56), we have
⎝
i j
Q(2)
i j (θ)aia j = Tr J−1
α(ρθ)α
⎣⎝
i
ai Bi(θ)
⎤
α
⎣⎝
j
a j B j(θ)
⎤
⊥Tr J−1
ρθ
⎣⎝
i
ai Bi(θ)
⎤⎣⎝
j
a j B j(θ)
⎤
=
⎝
i j
Q(1)
i j (θ)aia j
for any numbers ai.
□
Assume that Fj are positive operators acting on a Hilbert space H1 on which the
family M := {ρθ : θ →} is given. When ⎡n
j=1 Fj = I, these operators determine
a measurement. For any ρθ the formula
α(ρθ) := Diag(Tr ρθF1, . . . , Tr ρθFn)
gives a diagonal density matrix. Since this family is commutative, all quantum Fisher
informations coincide with the classical I(θ) in the right-hand side of (7.50) and the
classical Fisher information on the left-hand side of (7.59). Hence we have
I(θ) ⊥Q(θ).
(7.60)
A combination of the classical Cramér–Rao inequality in Theorem 7.28 and (7.60)
yields the Helstrøm inequality:
V (θ) ∈H(θ)−1 .
Example 7.35 In this example, we want to investigate (7.60) which is equivalently
written as
Q(θ)−1/2I(θ)Q(θ)−1/2 ⊥Im.
Taking the trace, we have
Tr Q(θ)−1I(θ) ⊥m.
(7.61)
Assume that
ρθ = ρ +
⎝
k
θk Bk ,

316
7
Some Applications
where Tr Bk = 0 and the self-adjoint matrices Bk are pairwise orthogonal with
respect to the inner product (B, C) ≡≥Tr BJ−1
ρ (C).
The quantum Fisher information matrix
Qkl(0) = Tr BkJ−1
ρ (Bl)
is diagonal due to our assumption. Example 7.29 tells us about the classical Fisher
information matrix:
Ikl(0) =
⎝
j
Tr Bk Fj Tr Bl Fj
Tr ρFj
.
Therefore,
Tr Q(0)−1I(0) =
⎝
k
1
Tr BkJ−1
ρ (Bk)
⎝
j
(Tr Bk Fj)2
Tr ρFj
=
⎝
j
1
Tr ρFj
⎝
k

Tr
Bk
%
Tr BkJ−1
ρ (Bk)
J−1
ρ (JρFj)


2
.
We can estimate the latter sum using the fact that
Bk
%
Tr BkJ−1
ρ (Bk)
is an orthonormal system and it remains so when ρ is added to it:
(ρ, Bk) = Tr BkJ−1
ρ (ρ) = Tr Bk = 0
and
(ρ, ρ) = Tr ρJ−1
ρ (ρ) = Tr ρ = 1.
By the Parseval inequality, we have
⎣
Tr ρJ−1
ρ (JρFj)
⎤2
+
⎝
k

Tr
Bk
%
Tr BkJ−1
ρ (Bk)
J−1
ρ (JρFj)


2
⊥Tr (JρFj)J−1
ρ (JρFj)
and
Tr Q(0)−1I(0) ⊥
⎝
j
1
Tr ρFj
⎣
Tr (JρFj)Fj −(Tr ρFj)2⎤

7.5 The Cramér–Rao Inequality
317
=
n
⎝
j=1
Tr (JρFj)Fj
Tr ρFj
−1 ⊥n −1
if we show that
Tr (JρFj)Fj ⊥Tr ρFj.
To see this we use the fact that the left-hand side is a quadratic cost and it can be
majorized by the largest one (see Example 7.33):
Tr (JρFj)Fj ⊥Tr ρF2
j ⊥Tr ρFj,
because F2
j ⊥Fj.
Since θ = 0 is not essential in the above argument, we have obtained that
Tr Q(θ)−1I(θ) ⊥n −1,
which can be compared with (7.61). This bound can be smaller than the general
one. The assumption on the Bk’s is not very essential, since the orthogonality can be
attained by reparameterization.
□
Let M := {ρθ : θ →G} be a smooth m-dimensional manifold and assume that
a collection A = (A1, . . . , Am) of self-adjoint matrices is used to estimate the true
value of θ.
Given an operator J we have the corresponding cost function ϕθ ◦ϕρθ for every
θ and the cost matrix of the estimator A is a positive deﬁnite matrix, deﬁned by
ϕθ[A]i j = ϕθ[Ai, A j]. The bias of the estimator is
b(θ) =

b1(θ), b2(θ), . . . , bm(θ)

:=

Tr ρθ(A1 −θ1), Tr ρθ(A2 −θ2), . . . , Tr ρθ(Am −θm)

.
From the bias vector we form a bias matrix
Bi j(θ) := ∂θ j bi(θ)
(1 ⊥i, j ⊥m).
For a locally unbiased estimator at θ0, we have B(θ0) = 0.
The next result is the quantum Cramér–Rao inequality for a biased estimate.
Theorem 7.36 Let A = (A1, . . . , Am) be an estimator of θ. Then for the above
deﬁned quantities the inequality
ϕθ[A] ∈

I + B(θ)

Q(θ)−1
I + B(θ)∪
holds in the sense of the order on positive semideﬁnite matrices. (Here I denotes the
identity operator.)

318
7
Some Applications
Proof:
We will use the block matrix method. Let X = [Xi j]m
i, j=1 be an m×m matrix
with n × n entries Xi j, and deﬁne ˜α(X) := [α(Xi j)]m
i, j=1. For every ξ1, . . . , ξm →C
we have
m
⎝
i, j=1
ξi ¯ξ jTr (X ˜α(X∪))i j =
m
⎝
k=1
Tr
⎞⎝
i
ξi Xik
⎠
α
⎞⎞⎝
j
ξ j X jk
⎠∪⎠
∈0 ,
because
Tr Yα(Y ∪) = Tr Yα(Y)∪= ⟨α(Y), Y⟩∈0
for every n × n matrix Y. Therefore, the m × m ordinary matrix M having the (i, j)
entry Tr (X ˜α(X∪))i j is positive. In the sequel we restrict ourselves to m = 4 for the
sake of simplicity and apply the above fact to the case
X =


A1
0 0 0
A2
0 0 0
L1(θ) 0 0 0
L2(θ) 0 0 0


and α = Jρθ .
Then we have
M =


Tr A1Jρ(A1) Tr A1Jρ(A2) Tr A1Jρ(L1) Tr A1Jρ(L2)
Tr A2Jρ(A1) Tr A2Jρ(A2) Tr A2Jρ(L1) Tr A2Jρ(L2)
Tr L1Jρ(A1) Tr L1Jρ(A2) Tr L1Jρ(L1) Tr L1Jρ(L2)
Tr L2Jρ(A1) Tr L2Jρ(A2) Tr L2Jρ(L1) Tr L2Jρ(L2)

∈0 .
Now we rewrite the matrix M in terms of the matrices involved in our Cramér–
Rao inequality. The 2 ×2 block M11 is the generalized covariance, M22 is the Fisher
information matrix and M12 is easily expressed as I + B. We have
M =


ϕθ[A1, A1] ϕθ[A1, A2] 1 + B11(θ)
B12(θ)
ϕθ[A2, A1] ϕθ[A2, A2]
B21(θ)
1 + B22(θ)
1 + B11(θ)
B21(θ)
ϕθ[L1, L1] ϕθ[L1, L2]
B12(θ)
1 + B22(θ) ϕθ[L2, L1] ϕθ[L2, L2]

∈0 .
The positivity of a block matrix
M =
⎦M1 C
C∪M2

=
⎦
ϕρ[A]
I + B(θ)
I + B(θ)∪
Q(θ)

implies M1 ∈C M−1
2 C∪, which reveals precisely the statement of the theorem.
(Concerning positive block matrices, see Chap.2.)
□

7.5 The Cramér–Rao Inequality
319
Let M = {ρθ : θ →} be a smooth manifold of density matrices. The following
construction is motivated by classical statistics. Suppose that a positive functional
d(ρ1, ρ2) of two variables is given on the manifold. In many cases one can obtain a
Riemannian metric by differentiation:
gi j(θ) =
∂2
∂θi∂θ√
j
d(ρθ, ρθ√)

θ=θ√
(θ →).
To be more precise the positive smooth functional d( · , · ) is called a contrast func-
tional if d(ρ1, ρ2) = 0 implies ρ1 = ρ2.
Following the work of Csiszár in classical information theory, Petz introduced a
family of information quantities parametrized by a function F : R+ ≥R
SF(ρ1, ρ2) = ⟨ρ1/2
1 , F(α(ρ2/ρ1))ρ1/2
1 ⟩,
see (7.9); F is written here in place of f . (α(ρ2/ρ1) := Lρ2 R−1
ρ1 is the relative
modular operator of the two densities.) When F is matrix monotone decreasing, this
quasi-entropy possesses good properties, for example it is a contrast functional in
the above sense if F is not linear and F(1) = 0. In particular, for
Fα(t) =
1
α(1 −α)

1 −tα
we have the relative entropy Sα(ρ1, ρ2) of degree α in Example 7.9. We have
∂2
∂t∂u Sα(ρ + t B, ρ + uC) = −
1
α(1 −α) ·
∂2
∂t∂u Tr (ρ + t B)1−α(ρ + uC)α
= : K α
ρ (B, C)
at t = u = 0 in the afﬁne parametrization. The tangent space at ρ is decomposed into
two subspaces, the ﬁrst consists of self-adjoint matrices of trace zero commuting with
ρ and the second is {i[ρ, X] : X = X∪}, the set of commutators. The decomposition
is essential both from the viewpoint of differential geometry and from the point of
view of differentiation, see Example 3.30. If B and C commute with ρ, then
K α
ρ (B, C) = Tr ρ−1BC
is independent of α and it is the classical Fischer information (in matrix form). If
B = i[ρ, X] and C = i[ρ, Y], then
K α
ρ (B, C) = −
1
α(1 −α) Tr ([ρ1−α, X][ρα, Y]).
Thus, K α
ρ (B, B) is exactly equal to the skew information (7.58).

320
7
Some Applications
7.6 Notes and Remarks
As an introduction we suggest the book by Oliver Johnson, Information Theory and
The Central Limit Theorem, Imperial College Press, 2004. The Gaussian Markov
property is popular in probability theory for single parameters, but the vector-valued
case is less popular. Sect.7.1 is based on the chapter T. Ando and D. Petz, Gaussian
Markov triplets approached by block matrices, Acta Sci. Math. (Szeged) 75(2009),
329–345.
Classical information theory is covered in the book I. Csiszár and J. Körner,
Information Theory: Coding Theorems for Discrete Memoryless Systems, Cambridge
University Press, 2011. Shannon entropy appeared in the 1940s and it is sometimes
said that the von Neumann entropy is its generalization. However, it is a fact that
von Neumann introduced quantum entropy in 1925. Many details can be found in
the books [67, 73]. The f -entropy of Imre Csiszár is used in classical information
theory (and statistics) [35], see also the chapter F. Liese and I. Vajda, On divergences
and informations in statistics and information theory, IEEE Trans. Inform. Theory
52(2006), 4394–4412. The quantum generalization was extended by Dénes Petz in
1985, for example see Chap.7 in [67]. The strong subadditivity of the von Neumann
entropy was proved by E. H. Lieb and M. B. Ruskai in 1973. Details on f -divergence
are in the chapter [49]. Theorem 7.4 is from the chapter K. M. R. Audenaert, Sub-
additivity of q-entropies for q > 1, J. Math. Phys. 48(2007), 083507. The quantity
(Tr Dq −1)/(1 −q) is called the q-entropy or the Tsallis entropy. It is remarkable
that strong subadditivity is not true for the Tsallis entropy in the matrix case (but it
holds for probability). Useful information can be found in the chapters [38] and S.
Furuichi, Tsallis entropies and their theorems, properties and applications, Aspects
of Optical Sciences and Quantum Information, 2007.
A good introduction to CCR-algebras is the book [69]. This subject is far from
matrixanalysis,butthequasi-freestatesarereallydescribedbymatrices.Thedescrip-
tion of the Markovian quasi-free state is from the chapter A. Jenˇcová, D. Petz and
J. Pitrik, Markov triplets on CCR-algebras, Acta Sci. Math. (Szeged), 76(2010),
111–134.
Section7.4 on optimal quantum measurements is from the chapter A. J. Scott,
Tight informationally complete quantum measurements, J. Phys. A: Math. Gen.
39(2006), 13507. MUBs have a large literature. They are commutative quasi-
orthogonal subalgebras. The work of Scott motivated the chapter D. Petz, L. Ruppert
and A. Szántó, Conditional SIC-POVMs, arXiv:1202.5741. It was shown by M.
Weiner [83] that the existence of d MUBs in Md(C) implies the existence of d + 1
MUBs.
The quasi-orthogonality of non-commutative subalgebras of Md(C) also has an
extensive literature; a summary appears in the chapter D. Petz, Algebraic comple-
mentarity in quantum theory, J. Math. Phys. 51(2010), 015215. The SIC POVM is
constructed in 6 dimensions in the chapter M. Grassl, On SIC-POVMs and MUBs
in dimension 6, http://arxiv.org/abs/quant-ph/0406175.

7.6 Notes and Remarks
321
Section7.5 is taken from Sects.10.2–10.4 of D. Petz [73]. Fisher information
ﬁrst appeared in the 1920s. For more on this subject, we suggest the book of Oliver
Johnson cited above and the chapter K. R. Parthasarathy, On the philosophy of
Cramér–Rao–Bhattacharya inequalities in quantum statistics, arXiv:0907.2210. The
general quantum matrix formalism was initiated by D. Petz in the chapter [71]. A.
Lesniewski and M. B. Ruskai discovered in [62] that all monotone Fisher informa-
tions are obtained from a quasi-entropy as a contrast functional.
7.7 Exercises
1. Prove Theorem 7.2.
2. Assume that H2 is one-dimensional in Theorem 7.14. Describe the possible
quasi-free Markov triplet.
3. Show that in Theorem 7.6 condition (iii) cannot be replaced by
D123D−1
23 = D12D−1
2 .
4. Prove Theorem 7.15.
5. The Bogoliubov–Kubo–Mori Fisher information is induced by the function
f (x) = x −1
log x =
⎥1
0
xt dt
and
γBKM
D
(A, B) = Tr A(J f
D)−1B
for self-adjoint matrices. Show that
γBKM
D
(A, B) =
⎥⊂
0
Tr (D + t I)−1A(D + t I)−1B dt
= −∂2
∂t∂s S(D + t A∗D + sB)

t=s=0
.
6. Prove Theorem 7.16.
7. Show that
x log x =
⎥⊂
0
⎞
x
1 + t −
x
x + t
⎠
dt
and deduce that the function f (x) = x log x is matrix convex.
8. Deﬁne

322
7
Some Applications
Sβ(ρ1∗ρ2) := Tr ρ1+β
1
ρ−β
2
−1
β
for β →(0, 1). Show that
S(ρ1∗ρ2) ⊥Sβ(ρ1∗ρ2)
for density matrices ρ1 and ρ2.
9. The functions
gp(x) :=



1
p(1−p)(x −x p) if
p ̸= 1,
x log x
if
p = 1
can be used for quasi-entropy. For which p > 0 is the function gp matrix
concave?
10. Give an example for which condition (iv) in Theorem 7.12 does not imply con-
dition (iii).
11. Assume that
⎦A B
B∪C

∈0.
Prove that
Tr (AC −B∪B) ⊥(Tr A)(Tr C) −(Tr B)(Tr B∪).
(Hint: Use Theorem 7.4 in the case q = 2.)
12. Let ρ and ω be invertible density matrices. Show that
S(ω∗ρ) ⊥Tr (ω log(ω1/2ρ−1ω1/2)).
13. For α →[0, 1] let
χ2
α(ρ, σ) := Tr ρσ−αρσα−1 −1.
Find the value of α which gives the minimal quantity.

Bibliography
1. Ando T (1979) Generalized Schur complements. Linear Algebra Appl 27:173–186
2. Ando T (1979) Concavity of certain maps on positive deﬁnite matrices and applications to
Hadamard products. Linear Algebra Appl 26:203–241
3. Ando T (1987) Totally positive matrices. Linear Algebra Appl 90:165–219
4. Ando T (1988) Comparison of norms |∈f (A)−f (B)∈| and |∈f (|A −B|)∈|. Math Z 197:403–
409
5. Ando T (1989) Majorization, doubly stochastic matrices and comparison of eigenvalues. Linear
Algebra Appl 118:163–248
6. Ando T (1994) Majorization and inequalities in matrix theory. Linear Algebra Appl 199:17–67
7. Ando T (2009) Private communication
8. Ando T, Hiai F (1994) Log majorization and complementary Golden-Thompson type inequal-
ities. Linear Algebra Appl 197/198:113–131
9. Ando T, Hiai F (2011) Operator log-convex functions and operator means. Math Ann 350:611–
630
10. Ando T, Li C-K, Mathias R (2004) Geometric means. Linear Algebra Appl 385:305–334
11. Ando T, Petz D (2009) Gaussian Markov triplets approached by block matrices. Acta Sci Math
(Szeged) 75:265–281
12. Appleby DM (2005) Symmetric informationally complete-positive operator valued measures
and the extended Clifford group. J Math Phys 46:052107
13. Audenaert KMR, Aujla JS (2007) On Ando’s inequalities for convex and concave functions.
Preprint, arXiv:0704.0099
14. AudenaertK,HiaiF,PetzD(2010)Stronglysubadditivefunctions.ActaMathHungar128:386–
394
15. Aujla JS, Silva FC (2003) Weak majorization inequalities and convex functions. Linear Algebra
Appl 369:217–233
16. Bendat J, Sherman S (1955) Monotone and convex operator functions. Trans Am Math Soc
79:58–71
17. Besenyei Á (2012) The Hasegawa-Petz mean: properties and inequalities. J Math Anal Appl
339:441–450
18. Besenyei Á, Petz D (2012) Characterization of mean transformations. Linear Multilinear Alge-
bra 60:255–265
19. Bessis D, Moussa P, Villani M (1975) Monotonic converging variational approximations to the
functional integrals in quantum statistical mechanics. J Math Phys 16:2318–2325
20. Bhatia R (1996) Matrix analysis. Springer, New York
F. Hiai and D. Petz, Introduction to Matrix Analysis and Applications,
323
Universitext, DOI: 10.1007/978-3-319-04150-6,
© Hindustan Book Agency 2014

324
Bibliography
21. Bhatia R (2007) Positive deﬁnite matrices. Princeton University Press, Princeton
22. Bhatia R, Davis C (1995) A Cauchy-Schwarz inequality for operators with applications. Linear
Algebra Appl 223/224:119–129
23. Bhatia R, Kittaneh F (1998) Norm inequalities for positive operators. Lett Math Phys 43:225–
231
24. Bhatia R, Parthasarathy KR (2000) Positive deﬁnite functions and operator inequalities. Bull
Lond Math Soc 32:214–228
25. Bhatia R, Sano T (2009) Loewner matrices and operator convexity. Math Ann 344:703–716
26. Bhatia R, Sano T (2010) Positivity and conditional positivity of Loewner matrices. Positivity
14:421–430
27. Birkhoff G (1946) Tres observaciones sobre el algebra lineal. Univ Nac Tucuman Rev Ser A
5:147–151
28. Bourin J-C (2004) Convexity or concavity inequalities for Hermitian operators. Math Ineq
Appl 7:607–620
29. Bourin J-C (2006) A concavity inequality for symmetric norms. Linear Algebra Appl 413:212–
217
30. Bourin J-C, Uchiyama M (2007) A matrix subadditivity inequality for f (A + B) and f (A) +
f (B). Linear Algebra Appl 423:512–518
31. Calderbank AR, Cameron PJ, Kantor WM, Seidel JJ (1997) Z4-Kerdock codes, orthogonal
spreads, and extremal Euclidean line-sets. Proc Lond Math Soc 75:436
32. Choi MD (1977) Completely positive mappings on complex matrices. Linear Algebra Appl
10:285–290
33. Conway JB (1978) Functions of one complex variable I, 2nd edn. Springer, New York
34. Cox DA (1984) The arithmetic-geometric mean of Gauss. Enseign Math 30:275–330
35. Csiszár I (1967) Information type measure of difference of probability distributions and indirect
observations. Studia Sci Math Hungar 2:299–318
36. Donoghue WF Jr (1974) Monotone matrix functions and analytic continuation. Springer, Berlin
37. Feller W (1971) An introduction to probability theory with its applications, vol II. Wiley, New
York
38. Furuichi S (2005) On uniqueness theorems for Tsallis entropy and Tsallis relative entropy.
IEEE Trans Inf Theor 51:3638–3645
39. Furuta T (2008) Concrete examples of operator monotone functions obtained by an elementary
method without appealing to Löwner integral representation. Linear Algebra Appl 429:972–
980
40. Hansen F, Pedersen GK (1982) Jensen’s inequality for operators and Löwner’s theorem. Math
Ann 258:229–241
41. Hansen F, Pedersen GK (2003) Jensen’s operator inequality. Bull Lond Math Soc 35:553–564
42. Hansen F (2008) Metric adjusted skew information. Proc Natl Acad Sci USA 105:9909–9916
43. Hiai F (1997) Log-majorizations and norm inequalities for exponential operators, In: Linear
operators, vol 38. Banach Center Publications, Warsaw, pp 119–181, 1994, Polish Acad Sci
Warsaw
44. Hiai F, Kosaki H (1999) Means for matrices and comparison of their norms. Indiana Univ Math
J 48:899–936
45. Hiai F, Kosaki H (2003) Means of Hilbert space operators, vol 1820. Lecture Notes in Math-
Springer, Berlin
46. Hiai F, Kosaki H, Petz D, Ruskai MB (2013) Families of completely positive maps associated
with monotone metrics. Linear Algebra Appl 439:1749–1791
47. Hiai F, Petz D (1993) The Golden-Thompson trace inequality is complemented. Linear Algebra
Appl 181:153–185
48. Hiai F, Petz D (2009) Riemannian geometry on positive deﬁnite matrices related to means.
Linear Algebra Appl 430:3105–3130
49. Hiai F, Mosonyi M, Petz D, Bény C (2011) Quantum f -divergences and error correction. Rev
Math Phys 23:691–747

Bibliography
325
50. Hida T (1960/1961) Canonical representations of Gaussian processes and their applications,
Mem Coll Sci Univ Kyoto Ser A Math 33(1960/1961):109–155
51. Hida T, Hitsuda M (1993) Gaussian processes. Translations of mathematical monographs, vol
120. American Mathematical Society, Providence
52. Horn A (1962) Eigenvalues of sums of Hemitian matrices. Paciﬁc J Math 12:225–241
53. Horn RA, Johnson CR (1985) Matrix analysis. Cambridge University Press, Cambridge
54. Ivanovi´c ID (1981) Geometrical description of quantal state determination. J Phys A 14:3241
55. Klyachko AA (1998) Stable bundles, representation theory and Hermitian operators. Selecta
Math 4:419–445
56. Knuston A, Tao T (1999) The honeycomb model of GLn(C) tensor products I: proof of the
saturation conjecture. J Am Math Soc 12:1055–1090
57. Kosem T (2006) Inequalities between ∈f (A + B)∈and ∈f (A) + f (B)∈. Linear Algebra Appl
418:153–160
58. Kubo F, Ando T (1980) Means of positive linear operators. Math Ann 246:205–224
59. Lax PD (2002) Functional analysis. Wiley, New York
60. Lax PD (2007) Linear algebra and its applications. Wiley, New York
61. Lenard A (1971) Generalization of the Golden-Thompson inequality Tr(eAeB) →TreA+B.
Indiana Univ Math J 21:457–467
62. Lesniewski A, Ruskai MB (1999) Monotone Riemannian metrics and relative entropy on
noncommutative probability spaces. J Math Phys 40:5702–5724
63. Lieb EH (1973) Convex trace functions and the Wigner-Yanase-Dyson conjecture. Adv Math
11:267–288
64. Lieb EH, Seiringer R (2004) Equivalent forms of the Bessis-Moussa-Villani conjecture. J Stat
Phys 115:185–190
65. Löwner K (1934) Über monotone matrixfunctionen. Math Z 38:177–216
66. Marshall AW, Olkin I (1979) Inequalities: theory of majorization and its applications. Academic
Press, New York
67. Ohya M, Petz D (1993) Quantum entropy and its use. Springer, Heidelberg (2nd edn, 2004)
68. Petz D (1988) A variational expression for the relative entropy. Commun Math Phys 114:345–
348
69. Petz D (1990) An invitation to the algebra of the canonical commutation relation. Leuven
University Press, Leuven
70. Petz D (1985) Quasi-entropies for states of a von Neumann algebra. Publ. RIMS. Kyoto Univ.
21:781–800
71. Petz D (1996) Monotone metrics on matrix spaces. Linear Algebra Appl 244:81–96
72. Petz D (1986) Quasi-entropies for ﬁnite quantum systems. Rep Math Phys 23:57–65
73. Petz D (2008) Quantum information theory and quantum statistics. Springer, Berlin
74. Petz D, Hasegawa H (1996) On the Riemannian metric of α-entropies of density matrices. Lett
Math Phys 38:221–225
75. Petz D, Temesi R (2006) Means of positive numbers and matrices. SIAM J Matrix Anal Appl
27:712–720
76. Petz D (2010) From f -divergence to quantum quasi-entropies and their use. Entropy 12:304–
325
77. Reed M, Simon B (1975) Methods of modern mathematical physics II. Academic Press, New
York
78. Schrödinger E (1936) Probability relations between separated systems. Proc Cambridge Philos
Soc 31:446–452
79. Suzuki M (1986) Quantum statistical Monte Carlo methods and applications to spin systems.
J Stat Phys 43:883–909
80. Thompson CJ (1971) Inequalities and partial orders on matrix spaces. Indiana Univ Math J
21:469–480
81. Tropp JA (2012) From joint convexity of quantum relative entropy to a concavity theorem of
Lieb. Proc Am Math Soc 140:1757–1760
82. Uchiyama M (2006) Subadditivity of eigenvalue sums. Proc Am Math Soc 134:1405–1412

326
Bibliography
83. Weiner M (2013) A gap for the maximum number of mutually unbiased bases. Proc Am Math
Soc 141:1963–1969
84. Wielandt H (1955) An extremum property of sums of eigenvalues. Proc Am Math Soc 6:106–
110
85. Wootters WK, Fields BD (1989) Optimal state-determination by mutually unbiased measure-
ments. Ann Phys 191:363
86. Zauner G (1999) Quantendesigns—Grundzüge einer nichtkommutativen Designtheorie, Ph.D.
thesis, University of Vienna
87. Zhan X (2002) Matrix inequalities, vol 1790. Lecture Notes in MathSpringer, Berlin
88. Zhang F (2005) The Schur complement and its applications. Springer, New York

Index
Symbols
A ≤B, 68
A σ B, 197
⟨· , · ∗, 4
A : B, 196
A#B, 190
A∗, 3
At, 3
B(H), 10
B(H)sa, 10
E(i j), 2
Gt(A, B), 206
H(A, B), 197, 207
H⊥, 5
In, 1
L(A, B), 207
M/A, 59
[P]M, 61
AG(a, b), 187
p(Q), 305
JD, 151
, 241
p(a), 242
M f (A, B), 212
Tr A, 3
Tr1, 277
∈A∈, 9
∈A∈p, 245
∈A∈(k), 245
Msa
n , 10
Mn, 1
χ2-divergence, 287
det A, 3
ℓp-norms, 242
H, 4
Pn, 188
ker A, 6
Pn, 217
ran A, 6
σ(A), 15
|∈A∈|, 243
|∈· ∈|, 243
a ≥w b, 230
a ≥w(log) b, 231
m f (A, B), 203
s(A), 233
v1 ∪v2, 41
2-positive mapping, 86
A
Absolute value, 29
Adjoint
matrix, 3
operator, 10
Ando, 222, 268
Ando and Hiai, 263
Ando and Zhan, 269
Annihilating
polynomial, 15
Antisymmetric tensor product, 41
Arithmetic-geometric mean, 202
Audenaert, 183, 320
Aujla and Silva, 183
B
Baker–Campbell–Hausdorff
formula, 109
Basis, 5
Bell, 38
product, 37
Bernstein theorem, 110
Bessis–Moussa–Villani conjecture, 131
F. Hiai and D. Petz, Introduction to Matrix Analysis and Applications,
327
Universitext, DOI: 10.1007/978-3-319-04150-6,
© Hindustan Book Agency 2014

328
Index
Bhatia, 131, 222
Bias matrix, 317
Bilinear form, 12
Birkhoff, 229
Block matrix, 56
Boltzmann entropy, 32, 188, 275
Bourin, 269
Bourin and Uchiyama, 256
Bra and ket, 7
C
Cauchy matrix, 30
Cayley, 46
Cayley transform, 49
Cayley–Hamilton theorem, 15
Channel
Pauli, 92
Characteristic polynomial, 14
Choi matrix, 90
Coarse-graining, 310
Completely
monotone, 109
positive, 69, 88
Concave, 144
jointly, 149
Conditional
expectation, 78
Conjecture
BMV, 131
Conjugate
convex function, 145
Contraction, 10
operator, 156
Contrast functional, 319
Convex
function, 143, 144
hull, 143
set, 142
Cost matrix, 317
Covariance, 72
Cramer, 45
Csiszár, 320
Cyclic vector, 16
D
Decomposition
polar, 29
Schmidt, 18
singular value, 34
spectral, 18
Decreasing rearrangement, 227
Density matrix, 277
Determinant, 3, 25
Divided difference, 125, 143
Doubly
stochastic, 47, 228, 229
substochastic, 230
Dual
frame, 295
mapping, 85
mean, 203
E
Eigenspace, 16
Eigenvector, 15
Entangled, 65
Entropy
Boltzmann, 32, 188
quasi, 281
Rényi, 134
Tsallis, 279, 320
von Neumann, 122
Error
mean quadratic, 306
Estimator
locally unbiased, 309
Expansion operator, 156
Exponential, 102, 266
Extreme point, 143
F
Factorization
Schur, 58
UL-, 64
Family
exponential, 309
Gibbsian, 309
Fisher information, 308
quantum, 312
Formula
Baker–Campbell–Hausdorff, 109
Lie–Trotter, 107
Stieltjes inversion, 161
Fourier expansion, 6
Frame superoperator, 293
Frobenius’ inequality, 47
Furuichi, 269, 320
G
Gauss, 45, 202
Gaussian
distribution, 31

Index
329
probability, 274
Geodesic, 189
Geometric mean, 207
weighted, 263
Gibbs state, 233
Gleason, 71
Gleason theorem, 94
Golden–Thompson
–Lieb inequality, 180
inequality, 182, 262
Gram–Schmidt procedure, 6
H
Hölder inequality, 246
Haar measure, 26
Hadamard
inequality, 149
product, 68
Hansen and Pedersen, 183
Heinz mean, 209
Helstrøm inequality, 315
Hermitian matrix, 11
Hessian, 188
Hiai, 131, 222
Hilbert space, 1
Hilbert–Schmidt norm, 245
Holbrook, 222
Holevo–Werner channel, 98
Horn, 241, 269
conjecture, 269
I
Identity matrix, 1
Inequality
Araki–Lieb–Thirring, 261
classical Cramér–Rao, 308
Cramér–Rao, 306
Golden–Thompson, 182, 262
Golden–Thompson–Lieb, 180, 182
Hölder, 246
Hadamard, 33, 149
Helstrøm, 315
Jensen, 143
Kadison, 85
Löwner–Heinz, 139, 172, 191
Lieb–Thirring, 261
Pinsker’s, 117
Poincaré, 20
Powers–Størmer, 260
quantum Cramér–Rao, 310
Rotfel’d, 253
Schwarz, 4, 85
Segal’s, 262
Streater, 117
Weyl’s, 249
Wielandt, 33, 63
Information
Fisher, 308
matrix, Helstrøm, 314
skew, 313
Informationally complete, 294
Inner product, 4
Hilbert–Schmidt, 5
Inverse, 2, 25
generalized, 35
Irreducible matrix, 57
J
Jensen inequality, 143
Joint concavity, 201
Jordan block, 14
K
K-functional, 245
Kadison inequality, 85
Karcher mean, 222
Kernel, 6, 83
positive deﬁnite, 83, 140
Klyachko, 269
Knuston and Tao, 269
Kosaki, 222
Kraus representation, 90
Kronecker
product, 39
sum, 39
Kubo, 222
transform, 314
Kubo–Ando theorem, 198
Kubo–Mori
inner product, 314
Ky Fan, 239
norm, 245
L
Löwner, 164
Lagrange, 16
interpolation, 112
Laplace transform, 110
Legendre transform, 145
Li and Mathias, 268
Lie–Trotter formula, 107
Lieb, 180

330
Index
Log-majorization, 231
Logarithm, 115
Logarithmic
derivative, 310, 314
mean, 207
M
Majorization, 227
log-, 231
weak, 228
Markov property, 276
Marshall and Olkin, 268
MASA, 77, 299
Matrix
bias, 317
concave function, 137
convex function, 137
cost, 317
Dirac, 51
doubly stochastic, 228
doubly substochastic, 230
inﬁnitely divisible, 31
mean, 203
monotone function, 137
Pauli, 69, 106
permutation, 12
Toeplitz, 11
tridiagonal, 16
upper triangular, 8
Matrix-unit, 2
Maximally entangled, 65
Mean
arithmetic-geometric, 202
binomial, 222
dual, 203
geometric, 190, 207
harmonic, 197, 207
Heinz, 209
Karcher, 222
logarithmic, 207
matrix, 221
power, 222
power difference, 209
Stolarsky, 210
transformation, 212
weighted, 206
Minimax expression, 157, 233
Minimax principle, 20
Molnár, 223
Monotone metric, 284
Moore–Penrose
generalized inverse, 35
More mixed, 232
Multiplicity
algebraic, 16
geometric, 16
Mutually unbiased bases, 299
N
Neumann series, 10
Norm, 4
ℓp-, 242
Hilbert–Schmidt, 5, 245
Ky Fan, 245
operator, 9, 245
Schatten–von Neumann, 245
symmetric, 241, 243
trace-, 245
unitarily invariant, 243
Normal operator, 12
O
Ohno, 95
Operator
conjugate linear, 12
connection, 197
convex function, 152
frame, 293
monotone function, 137
norm, 245
normal, 12
positive, 28
self-adjoint, 10
Oppenheim’s inequality, 68
Ortho-projection, 69
Orthogonal complement, 5
Orthogonal projection, 11
Orthogonality, 5
Orthonormal, 5
P
Pálﬁa, 222
Parallel sum, 196
Parallelogram law, 47
Partial
ordering, 65
trace, 40, 91, 277
Pascal matrix, 50
Pauli matrix, 69, 106
Permanent, 45
Permutation matrix, 12, 229
Petz, 95, 321
Pick function, 159

Index
331
Pinsker’s inequality, 117
Polar decomposition, 29
Polarization identity, 12
Positive
mapping, 28, 85
matrix, 28
POVD, 297
POVM, 292
Powers–Størmer inequality, 260
Projection, 69
Q
Quadratic
cost function, 312
matrix, 32
Quantum
f -divergence, 281
Cramér–Rao inequality, 310
Fisher information, 312
Fisher information matrix, 314
score operator, 314
Quasi-entropy, 281
Quasi-free state, 288
Quasi-orthogonal, 300
R
Rényi entropy, 134
Rank, 6
Reducible matrix, 57
Relative entropy, 117, 146, 150, 278
Representing
block matrix, 90
function, 200
Riemannian manifold, 188
Riemannian metric, 284
Rotfel’d inequality, 253
S
Schatten–von Neumann, 245
Schmidt decomposition, 18
Schoenberg’s theorem, 84
Schrödinger, 18
Schur
complement, 59, 196, 275
factorization, 58
theorem, 67
Schwarz mapping, 282
Segal’s inequality, 262
Self-adjoint operator, 10
Separable
positive matrix, 65
Shannon entropy, 269
SIC POVM, 301
Singular
value, 29, 233
value decomposition, 234
Skew information, 313, 319
Spectral decomposition, 18
Spectrum, 15
Standard
matrix monotone function, 203
Stolarsky mean, 210
Streater inequality, 117
Strong subadditivity, 149, 177, 277
Subadditivity, 149
Subalgebra, 76
Suzuki, 130
Sylvester, 46
Symmetric
dual gauge function, 247
gauge function, 242
logarithmic derivative, 313
matrix mean, 203
norm, 241, 243
T
Taylor expansion, 129
Taylor formula, 130
Tensor product, 36
Theorem
Bernstein, 110
Cayley–Hamilton, 15
ergodic, 51
Gelfand–Naimark, 240
Jordan canonical, 14
Kubo–Ando, 198
Löwner, 168
Lidskii–Wielandt, 236
Lieb’s concavity, 284
Nevanlinna, 159
Riesz–Fischer, 9
Schoenberg, 84
Schur, 52, 67
Tomiyama, 95
Weyl majorization, 235
Weyl’s monotonicity, 67
Trace, 3, 21
Trace-norm, 245
Transformer inequality, 201, 214
Transpose, 3
Triangular, 8
Tridiagonal, 16
Tsallis entropy, 279, 320

332
Index
U
Uchiyama, 269
Umegaki relative entropy, 117, 284
Unbiased estimation scheme, 307
Unitarily invariant norm, 243
Unitary, 11
V
van der Waerden, 47
Vandermonde matrix, 48
Variance, 72
Vector
cyclic, 16
von Neumann, 46, 94, 243, 268, 320
von Neumann entropy, 122
W
Weak majorization, 228, 230
Weakly positive matrix, 33
Weighted
mean, 206
Weyl
inequality, 249
majorization theorem, 235
monotonicity, 67
Wielandt inequality, 33, 63, 94
Wigner, 46


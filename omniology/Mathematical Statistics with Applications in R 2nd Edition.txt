
Mathematical
Statistics with
Applications in R

Mathematical
Statistics with
Applications in R
Second Edition
By
Kandethody M. Ramachandran
Chris P. Tsokos
AMSTERDAM • BOSTON • HEIDELBERG • LONDON
NEW YORK • OXFORD • PARIS • SAN DIEGO
SAN FRANCISCO • SINGAPORE • SYDNEY • TOKYO
Academic Press is an imprint of Elsevier

Academic Press is an imprint of Elsevier
32 Jamestown Road, London NW1 7BY, UK
525 B Street, Suite 1800, San Diego, CA 92101-4495, USA
225 Wyman Street, Waltham, MA 02451, USA
The Boulevard, Langford Lane, Kidlington, Oxford OX5 1GB, UK
Second edition 2015
Copyright # 2015, 2009 Elsevier Inc. All rights reserved
No part of this publication may be reproduced, stored in a retrieval system or transmitted in any
form or by any means electronic, mechanical, photocopying, recording or otherwise without
the prior written permission of the publisher Permissions may be sought directly from
Elsevier’s Science & Technology Rights Department in Oxford, UK: phone (þ44) (0) 1865
843830; fax (þ44) (0) 1865 853333; email: permissions@elsevier.com. Alternatively you can
submit your request online by visiting the Elsevier web site at http://elsevier.com/locate/
permissions, and selecting Obtaining permission to use Elsevier material.
Notice
No responsibility is assumed by the publisher for any injury and/or damage to persons or
property as a matter of products liability, negligence or otherwise, or from any use or operation
of any methods, products, instructions or ideas contained in the material herein. Because of
rapid advances in the medical sciences, in particular, independent verification of diagnoses
and drug dosages should be made.
Library of Congress Cataloging-in-Publication Data
A catalog record for this book is available from the Library of Congress
British Library Cataloguing in Publication Data
A catalogue record for this book is available from the British Library
For information on all Academic Press publications
visit our web site at store.elsevier.com
This book has been manufactured using Print On Demand technology. Each copy is produced
to order and is limited to black ink. The online version of this book will show color figures
where appropriate.
ISBN: 978-0-12-417113-8

Dedicated to our families:
Usha, Vikas, Vilas, and Varsha Ramachandran and Debbie,
Matthew, Jonathan, and Maria Tsokos

Acknowledgments
We express our sincere appreciation to our late colleague, co-worker, and dear
friend, Professor A.N.V. Rao, for his helpful suggestions and ideas for the initial ver-
sion of the subject textbook. In addition, we thank Bong-jin Choi and Yong Xu for
their kind assistance in the preparation of the first edition of the manuscript. We
would like to thank the following for their help in preparation of this edition:
A.K.M.R. Bashar, Jason Burgess, Muditha Perera, Joel Negron, Hansapani Rodrigo,
Doo Young Kim, Taysseer Sharaf, Bhikhari Tharu, Ram Kafle, Dr Rebecca Wooten,
and Dr Olga Savchuk. We also, would like to thank all those commented on our book
in the internet places such as Amazon and Google sites, like Mansah Alkebu-lan for
the positive comments and especially to Dr Axel Boldt for his detailed review on the
first edition. We also wish to acknowledge the contributions of all of the editorial
staff of ELSEVIER, Jill Cetel, Anusha Sambamoorthy, and others, who at various
stages helped in completion of this book. Finally, we acknowledge our students at
the University of South Florida for their useful comments through the years. To
all of them, we are very thankful.
Kandethody M. Ramachandran
Chris P. Tsokos
Tampa, FL
xv

About the Authors
Kandethody M. Ramachandran is Professor of Mathematics and Statistics at the
University of South Florida. He received his BS and MS degrees in Mathematics
from the Calicut University, India. Later, he worked as a researcher at the Tata
Institute of Fundamental Research, Bangalore center, at its Applied Mathematics
Division. Dr Ramachandran got his PhD in Applied Mathematics from Brown
University.
His research interests are concentrated in the areas of applied probability and sta-
tistics. His research publications span a variety of areas such as control of heavy traf-
fic queues, stochastic delay equations and control problems, stochastic differential
games and applications, reinforcement learning methods applied to game theory
and other areas, software reliability problems, applications of statistical methods
to microarray data analysis, and mathematical finance. He is also co-author with
Chris Tsokos of a book titled Stochastic Differential Games theory and Applications,
Atlantis Press.
Professor Ramachandran is extensively involved in activities to improve statis-
tics and mathematics education. He is a recipient of the Teaching Incentive Program
award at the University of South Florida. He is a member of the MEME Collabora-
tive, which is a partnership among mathematics education, mathematics, and engi-
neering faculty to address issues related to mathematics and mathematics education.
He was also involved in the calculus reform efforts at the University of South Florida.
He is recipient of 2 million dollar grant from NSF, and 1.4 million grant from HHMI
to improve STEM education at USF.
Chris P. Tsokos is Distinguished University Professor of Mathematics and Statistics
at the University of South Florida. He received his BS in Engineering Sciences/
Mathematics, his MA in Mathematics from the University of Rhode Island, and
his PhD in Statistics and Probability from the University of Connecticut. Professor
Tsokos has also served on the faculties at Virginia Polytechnic Institute and State
University and the University of Rhode Island.
Dr Tsokos’s research has extended into a variety of areas, including stochastic
systems, statistical models, reliability analysis, ecological systems, operations
research, time series, Bayesian analysis, and mathematical and statistical modeling
of global warming, among others. He is the author of more than 250 research pub-
lications in these areas.
Professor Tsokos is the author of several research monographs and books, includ-
ing Random Integral Equations with Applications to Life Sciences and Engineering,
Probability Distribution: An Introduction to Probability Theory with Applications,
Mainstreams of Finite Mathematics with Applications, Probability with the Essential
xvii

Analysis, and Applied Probability Bayesian Statistical Methods with Applications to
Reliability, among others.
Dr Tsokos is the recipient of many distinguished awards and honors, including
Fellow of the American Statistical Association, USF Distinguished Scholar Award,
Sigma Xi Outstanding Research Award, USF Outstanding Undergraduate Teaching
Award, USF Professional Excellence Award, URI Alumni Excellence Award in Sci-
ence and Technology, Pi Mu Epsilon, and election to the International Statistical
Institute, among others.
xviii
About the Authors

Preface to Second Edition
In the second edition, while keeping much of the material from the first edition, there
are some significant changes and additions. Due to the popularity of R and its free
availability, we have incorporated R-codes throughout the book. This will make it
easier for students to do the data analysis. We have also added a chapter on goodness
of fit tests and illustrated their applicability with several examples. In addition we
have introduced more probability distribution functions with real world data driven
applications in global warming, brain and prostate cancer, national unemployment,
and total rain fall. In this edition, we have shortened the point estimation chapter and
merged it with interval estimation. In addition, many corrections and additions are
made to reflect the continuous feedback we have obtained.
We have created a student companion website, http://booksite.elsevier.com/
9780124171138, with solutions to selected problems and data on Global warming,
brain and prostate cancer, national unemployment, and total rain fall. We have also
posted solutions to most of the problems in the instructor site, http://textbooks.
elsevier.com/web/Manuals.aspx?isbn¼9780124171138.
PREFACE TO FIRST EDITION
This textbook is of an interdisciplinary nature and is designed for a one- or two-
semester course in probability and statistics, with basic calculus as a prerequisite.
The book is primarily written to give a sound theoretical introduction to statistics
while emphasizing applications. If teaching statistics is the main purpose of a two-
semester course in probability and statistics, this textbook covers all the probability
concepts necessary for the theoretical development of statistics in two chapters, and
goes on to cover all major aspects of statistical theory in two semesters, instead of
only a portion of statistical concepts. What is more, using the optional section on
computer examples at the end of each chapter, the student can also simultaneously
learn to utilize statistical software packages for data analysis. It is our aim, without
sacrificing any rigor, to encourage students to apply the theoretical concepts they
have learned. There are many examples and exercises concerning diverse
application areas that will show the pertinence of statistical methodology to solving
real-world problems. The examples with statistical software and projects at the end of
the chapters will provide good perspective on the usefulness of statistical methods. To
introduce the students to modern and increasingly popular statistical methods, we
have introduced separate chapters on Bayesian analysis and empirical methods.
One of the main aims of this book is to prepare advanced undergraduates and
beginning graduate students in the theory of statistics with emphasis on interdisci-
plinary applications. The audience for this course is regular full-time students from
mathematics, statistics, engineering, physical sciences, business, social sciences,
materials science, and so forth. Also, this textbook is suitable for people who work
xix

in industry and in education as a reference book on introductory statistics for a good
theoretical foundation with clear indication of how to use statistical methods. Tra-
ditionally, one of the main prerequisites for this course is a semester of the introduc-
tion to probability theory. A working knowledge of elementary (descriptive)
statistics is also a must. In schools where there is no statistics major, imposing such
a background, in addition to calculus sequence, is very difficult. Most of the present
books available on this subject contains full one-semester material for probability
and then, based on those results, continue on to the topics in statistics. Also, some
of these books include in their subject matter only the theory of statistics, whereas
others take the cookbook approach of covering the mechanics. Thus, even with two
full semesters of work, many basic and important concepts in statistics are never cov-
ered. This book has been written to remedy this problem. We fuse together both con-
cepts in order for the student to gain knowledge of the theory and at the same time
develop the expertise to use their knowledge in real-world situations.
Although statistics is a very applied subject, there is no denying that it is also a
very abstract subject. The purpose of this book is to present the subject matter in such
a way that anyone with exposure to basic calculus can study statistics without spend-
ing two semesters of background preparation. To prepare students, we present an
optional review of the elementary (descriptive) statistics in Chapter 1. All the prob-
ability material required to learn statistics is covered in two chapters. Students with a
probability background can either review or skip the first three chapters. It is also our
belief that any statistics course is not complete without exposure to computational
techniques. At the end of each chapter, we give some examples of how to use Mini-
tab, SPSS, and SAS to statistically analyze data. Also, at the end of each chapter,
there are projects that will enhance the knowledge and understanding of the materials
covered in that chapter. In the chapter on the empirical methods, we present some of
the modern computational and simulation techniques, such as bootstrap, jackknife,
and Markov chain Monte Carlo methods. The last chapter summarizes some of the
steps necessary to apply the material covered in the book to real-world problems. The
first eight chapters have been class tested as a one-semester course for more than
3 years with five different professors teaching. The audience was junior- and
senior-level undergraduate students from many disciplines who had two semesters
of calculus, most of them with no probability or statistics background. The feedback
from the students and instructors was very positive. Recommendations from the
instructors and students were very useful in improving the style and content of
the book.
AIM AND OBJECTIVE OF THE TEXTBOOK
This textbook provides a calculus-based coverage of statistics and introduces stu-
dents to methods of theoretical statistics and their applications. It assumes no prior
knowledge of statistics or probability theory, but does require calculus. Most books
at this level are written with elaborate coverage of probability. This requires teaching
xx
Preface to Second Edition

one semester of probability and then continuing with one or two semesters of statis-
tics. This creates a particular problem for nonstatistics majors from various disci-
plines who want to obtain a sound background in mathematical statistics and
applications. It is our aim to introduce basic concepts of statistics with sound theo-
retical explanations. Because statistics is basically an interdisciplinary applied sub-
ject, we offer many applied examples and relevant exercises from different areas.
Knowledge of using computers for data analysis is desirable. We present examples
of solving statistical problems using Minitab, SPSS, and SAS.
FEATURES
•
During years of teaching, we observed that many students who do well in
mathematics courses find it difficult to understand the concept of statistics.
To remedy this, we present most of the material covered in the textbook with
well-defined step-by-step procedures to solve real problems. This clearly
helps the students to approach problem solving in statistics more logically.
•
The usefulness of each statistical method introduced is illustrated by several
relevant examples.
•
At the end of each section, we provide ample exercises that are a good mix of
theory and applications.
•
In each chapter, we give various projects for students to work on. These projects
are designed in such a way that students will start thinking about how to apply the
results they learned in the chapter as well as other issues they will need to know
for practical situations.
•
At the end of the chapters, we include an optional section on computer methods
with Minitab, SPSS, and SAS examples with clear and simple commands
that the student can use to analyze data. This will help the student to learn
how to utilize the standard methods they have learned in the chapter to study
real data.
•
We introduce many of the modern statistical computational and simulation
concepts, such as the jackknife and bootstrap methods, the EM algorithms, and
the Markov chain Monte Carlo methods such as the Metropolis algorithm, the
Metropolis-Hastings algorithm, and the Gibbs sampler. The Metropolis
algorithm was mentioned in Computing in Science and Engineering as being
among the top 10 algorithms having the “greatest influence on the development
and practice of science and engineering in the 20th century.”
•
We have introduced the increasingly popular concept of Bayesian statistics and
decision theory with applications.
•
A separate chapter on design of experiments, including a discussion on the
Taguchi approach, is included.
•
The coverage of the book spans most of the important concepts in statistics.
Learning the material along with computational examples will prepare students to
understand and utilize software procedures to perform statistical analysis.
xxi
Preface to Second Edition

•
Every chapter contains discussion on how to apply the concepts and what are the
issues related to applying the theory.
•
A student’s solution manual, instructor’s manual, and data disk are provided.
•
In the last chapter, we discuss some issues in applications to clearly demonstrate
in a unified way how to check for many assumptions in data analysis and what
steps one needs to follow to avoid possible pitfalls in applying the methods
explained in the rest of this textbook.
xxii
Preface to Second Edition

Flow Chart
In this flow chart, we suggest some options on how to use the book in a one-semester
or two-semester course. For a two-semester course, we recommend coverage of the
complete textbook. However, Chapters 1, 9, and 14 are optional for both one- and
two-semester courses and can be given as reading exercises. For a one-semester
course, we suggest the following options: A, B, C, D.
Ch. 2
Ch. 5 
Ch. 3
With 
probability 
background 
Without 
probability 
background 
One semester
Ch. 6
Ch. 4
Ch. 7
Ch. 8
Ch. 5
Ch. 6
Ch. 7
Ch. 8
Ch. 10
Ch. 11
Ch. 12
A
Ch. 11
Ch. 12
B 
C 
D
Ch. 5
Ch. 6
Ch. 7
Ch. 8
Ch. 10
Ch. 12
Ch. 5
Ch. 6
Ch. 7
Ch. 8
Ch. 12
Ch. 13
Ch. 5
Ch. 6
Ch. 7
Ch. 8
Ch. 11
Ch. 13
Optional
chapters
xxiii

CHAPTER
Descriptive Statistics
1
CHAPTER CONTENTS
1.1 Introduction ........................................................................................................ 2
1.2 Basic Concepts ................................................................................................... 4
1.3 Sampling Schemes .............................................................................................. 8
1.4 Graphical Representation of Data ....................................................................... 13
1.5 Numerical Description of Data ............................................................................ 26
1.6 Computers and Statistics ................................................................................... 39
1.7 Chapter Summary .............................................................................................. 39
1.8 Computer Examples ........................................................................................... 41
Projects for Chapter 1 .............................................................................................. 51
OBJECTIVE
Review the basic concepts of elementary statistics.
Sir Ronald Aylmer Fisher
(Source: http://www-history.mcs.st-andrews.ac.uk/PictDisplay/Fisher.html)
Sir Ronald Fisher F.R.S. (1890-1962) was one of the leading scientists of the 20th
century who laid the foundations for modern statistics. As a statistician working at
the Rothamsted Agricultural Experiment Station, the oldest agricultural research
institute in the United Kingdom, he also made major contributions to Evolutionary
Mathematical Statistics with Applications in R
Copyright © 2015 Elsevier Inc. All rights reserved.
1

Biology and Genetics. The concept of randomization and the analysis of variance
procedures that he introduced are now used throughout the world. In 1922 he gave
a new definition of statistics. Fisher identified three fundamental problems in statis-
tics: (1) specification of the type of population that the data came from; (2) estima-
tion; and (3) distribution. His book Statistical Methods for Research Workers (1925)
was used as a handbook for the methods for the design and analysis of experiments.
Fisher also published the books titled The Design of Experiments (1935) and Statis-
tical Tables (1947). While at the Agricultural Experiment Station he had conducted
breeding experiments with mice, snails, and poultry, and the results he obtained led
to theories about gene dominance and fitness that he published in The Genetical
Theory of Natural Selection (1930).
1.1 INTRODUCTION
In today’s society, decisions are made on the basis of data. Most scientific or indus-
trial studies and experiments produce data, and the analysis of these data and drawing
useful conclusions from them become one of the central issues. Statistics is an inte-
gral part of the quantitative approach to knowledge. The field of statistics is con-
cerned with the scientific study of collecting, organizing, analyzing, and drawing
conclusions from data. Statistics benefits all of us because of its ability to predict
the future based on data we have previously gathered. Statistical methods help us
to transform data to information and knowledge. Statistical concepts enable us to
solve problems in a diversity of contexts, add substance to decisions, and reduce
guesswork. The discipline of statistics stemmed from the need to place knowledge
management on a systematic evidence base. Earlier works on statistics dealt only
with the collection, organization, and presentation of data in the form of tables
and charts. In order to place statistical knowledge on a systematic evidence base,
we require a study of the laws of probability. In mathematical statistics we create
a probabilistic model and view the data as a set of random outcomes from that model.
Advances in probability theory enable us to draw valid conclusions and to make rea-
sonable decisions on the basis of data.
Statistical methods are used in almost every discipline, including agriculture,
astronomy, biology, business, communications, economics, education, electronics,
geology, health sciences, and many other fields of science and engineering, and
can aid us in several ways. Modern applications of statistical techniques include sta-
tistical communication theory and signal processing, information theory, network
security and denial of service problems, clinical trials, artificial and biological intel-
ligence, quality control of manufactured items, software reliability, and survival
analysis. The first of these is to assist us in designing experiments and surveys.
We desire our experiment to yield adequate answers to the questions that prompted
the experiment or survey. We would like the answers to have good precision without
involving a lot of expenditure. Statistically designed experiments facilitate develop-
ment of robust products that are insensitive to changes in the environment and
2
CHAPTER 1 Descriptive Statistics

internal component variation. Another way that statistics assists us is in organizing,
describing, summarizing, and displaying experimental data. This is termed descrip-
tive statistics. A third use of statistics is in drawing inferences and making decisions
based on data. For example, scientists may collect experimental data to prove or dis-
prove an intuitive conjecture or hypothesis. Through the proper use of statistics we
can conclude whether the hypothesis is valid or not. In the process of solving a real-
life problem using statistics, the following three basic steps may be identified. First,
consistent with the objective of the problem, we identify the model—the appropriate
statistical method. Then, we justify the applicability of the selected model to fulfill
the aim of our problem. Last, we properly apply the related model to analyze the data
and make the necessary decisions, which results in answering the question of our
problem with minimum risk. Starting with Chapter 2, we will study the necessary
background material to proceed with the development of statistical methods for solv-
ing real-world problems.
In the present chapter we briefly review some of the basic concepts of descriptive
statistics. Such concepts will give us a visual and descriptive presentation of the
problem under investigation. Now, we proceed with some basic definitions.
1.1.1 DATA COLLECTION
One of the first problems that a statistician faces is obtaining data. The inferences that
we make depend critically on the data that we collect and use. Data collection
involves the following important steps.
GENERAL PROCEDURE FOR DATA COLLECTION
1. Define the objectives of the problem and proceed to develop the experiment or survey.
2. Define the variables or parameters of interest.
3. Define the procedures of data collection and measuring techniques. This includes sampling pro-
cedures, sample size, and data-measuring devices (questionnaires, telephone interviews, etc.).
EXAMPLE 1.1.1
We may be interested in estimating the average household income in a certain community. In this
case, the parameter of interest is the average income of a typical household in the community. To
acquire the data, we may send out a questionnaire or conduct a telephone interview. Once we have
the data, we may first want to represent the data in graphical or tabular form to better understand its
distributional behavior. Then we will use appropriate analytical techniques to estimate the parameter(s)
of interest, in this case the average household income.
Very often a statistician is confined to data that have already been collected, pos-
sibly even collected for other purposes. This makes it very difficult to determine the
quality of data. Planned collection of data, using proper techniques, is much preferred.
3
1.1 Introduction

1.2 BASIC CONCEPTS
Statistics is the science of data. This involves collecting, classifying, summarizing,
organizing, analyzing, and interpreting data. It also involves model building. Suppose
we wish to study household incomes in a certain neighborhood. We may decide to ran-
domly select, say, 50 families and examine their household incomes. As another exam-
ple, suppose we wish to determine the diameter of a rod, and we take 10 measurements
of the diameter. When we consider these two examples, we note that in the first case the
population (the household incomes of all families in the neighborhood) really exists,
whereas in the second, the population (set of all possible measurementsof the diameter)
is only conceptual. In either case we can visualize the totality of the population values,
ofwhich our sampledataareonlyasmall part. Thus, wedefineapopulationtobetheset
of all measurements or objects that are of interest and a sample to be a subset of that
population. The population acts as the sampling frame from which a sample is selected.
Now we introduce some basic notions commonly used in statistics.
Definition 1.2.1 A population is the collection or set of all objects or measure-
ments that are of interest to the collector.
EXAMPLE 1.2.1
Suppose we wish to study the heights of all female students at a certain university. The population
will be the set of the measured heights of all female students in the university. The population is not
the set of all female students in the university.
In real-world problems it is usually not possible to obtain information on the
entire population. The primary objective of statistics is to collect and study a subset
of the population, called a sample, to acquire information on some specific charac-
teristics of the population that are of interest.
Definition 1.2.2 The sample is a subset of data selected from a population. The
size of a sample is the number of elements in it.
EXAMPLE 1.2.2
We wish to estimate the percentage of defective parts produced in a factory during a given week
(5 days) by examining 20 parts produced per day. The parts will be examined each day at randomly
chosen times. In this case “all parts produced during the week” is the population and the (100)
selected parts for 5 days constitutes a sample.
Other common examples of sample and population are:
Political polls: The population will be all voters, whereas the sample will be the
subset of voters we poll.
Laboratory experiment: The population will be all the data we could have
collected if we were to repeat the experiment a large number of times (infinite
number of times) under the same conditions, whereas the sample will be the data
actually collected by the one experiment.
4
CHAPTER 1 Descriptive Statistics

Quality control: The population will be the entire batch of items produced, say,
by a machine or by a plant, whereas the sample will be the subset of items
we tested.
Clinical studies: The population will be all the patients with the same disease,
whereas the sample will be the subset of patients used in the study.
Finance: All common stock listed in stock exchanges such as the New York
Stock Exchange, the American Stock Exchanges, and over-the-counter is the
population. A collection of 20 randomly picked individual stocks from these
exchanges will be a sample.
The methods consisting mainly of organizing, summarizing, and presenting data
in the form of tables, graphs, and charts are called descriptive statistics. The methods
of drawing inferences and making decisions about the population using the sample
are called inferential statistics. Inferential statistics uses probability theory.
Definition 1.2.3 A statistical inference is an estimate, a prediction, a decision, or
a generalization about the population based on information contained in a sample.
For example, we may be interested in the average indoor radiation level in homes
built on reclaimed phosphate mine lands (many of the homes in west-central Florida
are built on such lands). In this case, we can collect indoor radiation levels for a ran-
dom sample of homes selected from this area, and use the data to infer the average
indoor radiation level for the entire region. In the Florida Keys, one of the concerns is
that the coral reefs are declining because of the prevailing ecosystems. In order to test
this, one can randomly select certain reef sites for study and, based on these data,
infer whether there is a net increase or decrease in coral reefs in the region. Here
the inferential problem could be finding an estimate, such as in the radiation problem,
or making a decision, such as in the coral reef problem. We will see many other
examples as we progress through the book.
1.2.1 TYPES OF DATA
Data can be classified in several ways. We will give two different classifications, one
based on whether the data are measured on a numerical scale or not, and the other on
whether the data are collected in the same time period or collected at different time
periods.
Definition 1.2.4 Quantitative data are observations measured on a numerical
scale. Non numerical data that can only be classified into one of the groups of cat-
egories are said to be qualitative or categorical data.
EXAMPLE 1.2.3
Data on response to a particular therapy could be classified as no improvement, partial improvement,
or complete improvement. These are qualitative data. The number of minority-owned businesses in
Florida is quantitative data. The marital status of each person in a statistics class as married or not
married is qualitative or categorical data. The number of car accidents in different US cities is quan-
titative data. The blood group of each person in a community as O, A, B, AB is qualitative data.
5
1.2 Basic Concepts

Categorical data could be further classified as nominal data and ordinal data.
Data characterized as nominal have data groups that do not have a specific order.
An example of this could be state names, or names of the individuals, or courses
by name. These do not need to be placed in any order. Data characterized as ordinal
have groups that should be listed in a specific order. The order may be either increas-
ing or decreasing. One example would be income levels. The data could have
numeric values such as 1, 2, 3, or values such as high, medium, or low.
Definition 1.2.5 Cross-sectional data are data collected on different elements or
variables at the same point in time or for the same period of time.
EXAMPLE 1.2.4
The data in Table 1.1 represent US federal support for the mathematical sciences in 1996, in millions
of dollars (source: AMS Notices). This is an example of cross-sectional data, as the data are collected
in one time period, namely in 1996.
Definition 1.2.6 Time series data are data collected on the same element or the
same variable at different points in time or for different periods of time.
EXAMPLE 1.2.5
The data in Table 1.2 represent US federal support for the mathematical sciences during the years
1995-1997, in millions of dollars (source: AMS Notices). This is an example of time series data,
because they have been collected at different time periods, 1995 through 1997.
Table 1.1 Federal Support for the Mathematical Sciences, 1996
Federal Agency
Amount
National Science Foundation
91.70
DMS
85.29
Other MPS
4.00
Department of Defense
77.30
AFOSR
16.70
ARO
15.00
DARPA
22.90
NSA
2.50
ONR
20.20
Department of Energy
16.00
University Support
5.50
National Laboratories
10.50
Total, All Agencies
185.00
6
CHAPTER 1 Descriptive Statistics

For an extensive collection of statistical terms and definitions, we can refer to
many sources such as http://www.stats.gla.ac.uk/steps/glossary/index.html. We will
give some other helpful Internet sources that may be useful for various aspects of
statistics: http://www.amstat.org/ (American Statistical Association), http://www.
stat.ufl.edu (University of Florida statistics department), http://www.statsoft.com/
textbook/ (covers a wide range of topics, the emphasis is on techniques rather than
concepts or mathematics), http://www.york.ac.uk/depts/maths/histstat/welcome.htm
(some information about the history of statistics), http://www.isid.ac.in/ (Indian Sta-
tistical Institute), http://www.isi-web.org/30-statsoc/statsoc/282-nsslist (The Inter-
national
Statistical
Institute),
http://www.rss.org.uk/
(The
Royal
Statistical
Society), http://lib.stat.cmu.edu/ (an index of statistical software and routines).
For energy-related statistics, refer to http://www.eia.doe.gov/. The Earth Observing
System Data and Information System (https://earthdata.nasa.gov/about-eosdis) is
one of the largest data source for geological data. Environmental Protection Agency
(http://www.epa.gov/datafinder/) is another great source of data on environmental
related area. If you want market data, YAHOO! Finance (http://finance.yahoo.
com/) is a good source. There are various other useful sites that you could explore
based on your particular need.
EXERCISES 1.2
1.2.1. Give your own examples for qualitative and quantitative data. Also, give
examples for cross-sectional and time series data.
1.2.2. Discuss how you will collect different types of data. What inferences do
you want to derive from each of these types of data?
Table 1.2 United States Federal Support for the Mathematical Sciences
in Different Years
Agency
1995
1996
1997
National Science Foundation
87.69
91.70
98.22
DMS
85.29
87.70
93.22
Other MPS
2.40
4.00
5.00
Department of Defense
77.40
77.30
67.80
AFOSR
17.40
16.70
17.10
ARO
15.00
15.00
13.00
DARPA
21.00
22.90
19.50
NSA
2.50
2.50
2.10
ONR
21.40
20.20
16.10
Department of Energy
15.70
16.00
16.00
University Support
6.20
5.50
5.00
National Laboratories
9.50
10.50
11.00
Total, All Agencies
180.79
185.00
182.02
7
1.2 Basic Concepts

1.2.3. Refer to the data in Example 1.2.4. State a few questions that you
can ask about the data. What inferences can you make by looking at
these data?
1.2.4. Refer to the data in Example 1.2.5. Can you state a few questions
that the data suggest? What inferences can you make by looking at
these data?
1.3 SAMPLING SCHEMES
In any statistical analysis, it is important that we clearly define the target population.
The population should be defined in keeping with the objectives of the study. When
the entire population is included in the study, it is called a census study because data
are gathered on every member of the population. In general, it is usually not possible
to obtain information on the entire population because the population is too large to
attempt a survey of all of its members, or it may not be cost-effective. A small but
carefully chosen sample can be used to represent the population. A sample is
obtained by collecting information from only some members of the population. A
good sample must reflect all the characteristics (of importance) of the population.
Samples can reflect the important characteristics of the populations from which
they are drawn with differing degrees of precision. A sample that accurately reflects
its population characteristics is called a representative sample. A sample that is not
representative of the population characteristics is called a biased sample. The reli-
ability or accuracy of conclusions drawn concerning a population depends on
whether or not the sample is properly chosen so as to represent the population
sufficiently well.
There are many sampling methods available. We mention a few commonly used
simple sampling schemes. The choice between these sampling methods depends on
(1) the nature of the problem or investigation, (2) the availability of good sampling
frames (a list of all of the population members), (3) the budget or available financial
resources, (4) the desired level of accuracy, and (5) the method by which data will be
collected, such as questionnaires or interviews.
Definition 1.3.1 A sample selected in such a way that every element of the pop-
ulation has an equal chance of being chosen is called a simple random sample.
Equivalently each possible sample of size n has same chance of being selected as
any other subset of sample of size n.
EXAMPLE 1.3.1
For a state lottery, 52 identical Ping-Pong balls with a number from 1 to 52 painted on each ball are
put in a clear plastic bin. A machine thoroughly mixes the balls and then six are selected. The six
numbers on the chosen balls are the six lottery numbers that have been selected by a simple random
sampling procedure.
8
CHAPTER 1 Descriptive Statistics

SOME ADVANTAGES OF SIMPLE RANDOM SAMPLING
1. Selection of sampling observations at random ensures against possible investigator biases.
2. Analytic computations are relatively simple, and probabilistic bounds on errors can be computed
in many cases.
3. It is frequently possible to estimate the sample size for a prescribed error level when designing
the sampling procedure.
Simple random samplingmay not be effective in all situations. For example, ina US
presidential election, it may be more appropriate to conduct sampling polls by state,
ratherthananationwiderandompoll.Itisquitepossibleforacandidatetogetamajority
of the popular vote nationwide and yet lose the election. We now describe a few other
sampling methods that may be more appropriate in a given situation.
Definition 1.3.2 A systematic sample is a sample in which every Kth element in
the sampling frame is selected after a suitable random start for the first element. We
list the population elements in some order (say alphabetical) and choose the desired
sampling fraction.
STEPS FOR SELECTING A SYSTEMATIC SAMPLE
1. Number the elements of the population from 1 to N.
2. Decide on the sample size, say n, that we need.
3. Choose K¼N/n.
4. Randomly select an integer between 1 and K.
5. Then take every Kth element.
EXAMPLE 1.3.2
If the population has 1000 elements arranged in some order and we decide to sample 10% (i.e.
N¼1000 and n¼100), then K¼1000/100¼10. Pick a number at random between 1 and K¼10
inclusive, say 3. Then select elements numbered 3, 13, 23, . . ., 993.
Systematic sampling is widely used because it is easy to implement. If the list of
population elements is in random order to begin with, then the method is similar to
simple random sampling. If, however, there is a correlation or association between
successive elements, or if there is some periodic structure, then this sampling method
may introduce biases. Systematic sampling is often used to select a specified number
of records from a computer file.
Definition 1.3.3 A sample obtained by stratifying (dividing into nonoverlapping
groups) the sampling frame based on some factor or factors and then selecting some
elements from each of the strata is called a stratified sample. Here, a population with
N elements is divided into s subpopulations. A sample is drawn from each subpop-
ulation independently. The size of each subpopulation and sample sizes in each sub-
population may vary.
9
1.3 Sampling Schemes

A stratified sample is a modification of simple random sampling and systematic
sampling and is designed to obtain a more representative sample, but at the cost of a
more complicated procedure. Compared to random sampling, stratified sampling
reduces sampling error.
STEPS FOR SELECTING A STRATIFIED SAMPLE
1. Decide on the relevant stratification factors (sex, age, income, etc.).
2. Divide the entire population into strata (subpopulations) based on the stratification criteria. Sizes
of strata may vary.
3. Select the requisite number of units using simple random sampling or systematic sampling from
each subpopulation. The requisite number may depend on the subpopulation sizes.
Examples of strata might be males and females, undergraduate students and grad-
uate students, managers and nonmanagers, or populations of clients in different racial
groups such as African Americans, Asians, Whites, and Hispanics. Stratified sam-
pling is often used when one or more of the strata in the population have a low inci-
dence relative to the other strata.
EXAMPLE 1.3.3
In a population of 1000 children from an area school, there are 600 boys and 400 girls. We divide
them into strata based on their parents’ income as shown in Table 1.3.
This is stratified data.
EXAMPLE 1.3.4
Refer to Example 1.3.3. Suppose we decide to sample 100 children from the population of 1000
(i.e. 10% of the population). We also choose to sample 10% from each of the categories. For
example, we would choose 12 (10% of 120) poor boys; 6 (10% of 60 rich girls), and so forth.
This yields Table 1.4. This particular sampling method is called a proportional stratified
sampling.
Table 1.3 Classification of School Children
Boys
Girls
Poor
120
240
Middle Class
150
100
Rich
330
60
10
CHAPTER 1 Descriptive Statistics

SOME USES OF STRATIFIED SAMPLING
1. In addition to providing information about the whole population, this sampling scheme provides
information about the subpopulations, the study of which may be of interest. For example, in a
US presidential election, opinion polls by state may be more important in deciding on the Elec-
toral College advantage than a national opinion poll.
2. Stratified sampling can be considerably more precise than a simple random sample, because the
population is fairly homogeneous within each stratum but there is a sizable variation between the
strata.
Definition 1.3.4 In cluster sampling, the sampling unit contains groups of ele-
ments called clusters instead of individual elements of the population. A cluster is an
intact group naturally available in the field. Unlike the stratified sample where the
strata are created by the researcher based on stratification variables, the clusters
naturally exist and are not formed by the researcher for data collection. Cluster sam-
pling is also called area sampling.
To obtain a cluster sample, first take a simple random sample of groups and then
sample all elements within the selected clusters (groups). Cluster sampling is con-
venient to implement. However, because it is likely that units in a cluster will be rel-
atively homogeneous, this method may be less precise than simple random sampling.
EXAMPLE 1.3.5
Suppose we wish to select a sample of about 10% from all fifth-grade children of a county. We ran-
domly select 10% of the elementary schools assumed to have approximately the same number of
fifth-grade students and select all fifth-grade children from these schools. This is an example of clus-
ter sampling, each cluster being an elementary school that was selected.
Definition 1.3.5 Multiphase sampling involves collection of some information
from the whole sample and additional information either at the same time or later
from subsamples of the whole sample. The multiphase or multistage sampling is
basically a combination of the techniques presented earlier.
Table 1.4 Proportional Stratification of School Children
Boys
Girls
Poor
12
24
Middle Class
15
10
Rich
33
6
11
1.3 Sampling Schemes

EXAMPLE 1.3.6
An investigator in a population census may ask basic questions such as sex, age, or marital status for
the whole population, but only 10% of the population may be asked about their level of education or
about how many years of mathematics and science education they had.
1.3.1 ERRORS IN SAMPLE DATA
Irrespective of which sampling scheme is used, the sample observations are prone to
various sources of error that may seriously affect the inferences about the population.
Some sources of error can be controlled. However, others may be unavoidable because
they are inherent in the nature of the sampling process. Consequently, it is necessary to
understand the different types of errors for a proper interpretation and analysis of the
sample data. The errors can be classified as sampling errors and nonsampling errors.
Nonsampling errors occur in the collection, recording, and processing of sample data.
For example, such errors could occur as a result of bias in selection of elements of the
sample, poorly designed survey questions, measurement and recording errors,
incorrect responses, or no responses from individuals selected from the population.
Sampling errors occur because the sample is not an exact representative of the popu-
lation. Sampling error is due to the differences between the characteristics of the
population and those of a sample from the population. For example, we are interested
in the average test score in a large statistics class of size, say, 80. A sample of size 10
grades from this resulted in an average test score of 75. If the average test for the entire
80 students (the population) is 72, then the sampling error is 7572¼3.
1.3.2 SAMPLE SIZE
In almost any sampling scheme designed by statisticians, one of the major issues is
the determination of the sample size. In principle, this should depend on the variation
in the population as well as on the population size, and on the required reliability of
the results, that is, the amount of error that can be tolerated. For example, if we are
taking a sample of school children from a neighborhood with a relatively homoge-
neous income level to study the effect of parents’ affluence on the academic perfor-
mance of the children, it is not necessary to have a large sample size. However, if the
income level varies a great deal in the feeding area of the school, then we will need a
larger sample size to achieve the same level of reliability. In practice, another
influencing factor is the available resources such as money and time. In later chap-
ters, we present some methods of determining sample size in statistical estimation
problems.
The literature on sample survey methods is constantly changing with new insights
that demand dramatic revisions in the conventional thinking. We know that repre-
sentative sampling methods are essential to permit confident generalizations of
results to populations. However, there are many practical issues that can arise in
real-life sampling methods. For example, in sampling related to social issues,
12
CHAPTER 1 Descriptive Statistics

whatever the sampling method we employ, a high response rate must be obtained. It
has been observed that most telephone surveys have difficulty in achieving response
rates higher than 60%, and most face-to-face surveys have difficulty in achieving
response rates higher than 70%. Even a well-designed survey may stop short of
the goal of a perfect response rate. This might induce bias in the conclusions based
on the sample we obtained. A low response rate can be devastating to the reliability
of a study. We can obtain series of publications on surveys, including guidelines on
avoiding pitfalls from the American Statistical Association (www.amstat.org). In
this book, we deal mainly with samples obtained using simple random sampling.
EXERCISE 1.3
1.3.1. Give your own examples for each of the sampling methods described in this
section. Discuss the merits and limitations of each of these methods.
1.3.2. Using the information obtained from the publications of the American
Statistical Association (www.amstat.org) or any other reference, write a short
report on how to collect survey data, and what the potential sources of error are.
1.4 GRAPHICAL REPRESENTATION OF DATA
The source of our statistical knowledge lies in the data. Once we obtain the sample
data values, one way to become acquainted with them is to display them in tables or
graphically. Charts and graphs are very important tools in statistics because they
communicate information visually. These visual displays may reveal the patterns
of behavior of the variables being studied. In this chapter, we will consider one-
variable data. The most common graphical displays are the frequency table, pie
chart, bar graph, Pareto chart, and histogram. For example, in the business world,
graphical representations of data are used as statistical tools for everyday process
management and improvements by decision makers (such as managers, and frontline
staff) to understand processes, problems, and solutions. The purpose of this section is
to introduce several tabular and graphical procedures commonly used to summarize
both qualitative and quantitative data. Tabular and graphical summaries of data can
be found in reports, newspaper articles, Web sites, and research studies, among
others.
Now we shall introduce some ways of graphically representing both qualitative and
quantitative data. Bar graphs and Pareto charts are useful displays for qualitative data.
Definition 1.4.1 A graph of bars whose heights represent the frequencies (or rel-
ative frequencies) of respective categories is called a bar graph.
EXAMPLE 1.4.1
The data in Table 1.5 represent the percentages of price increases of some consumer goods and ser-
vices for the period December 1990 to December 2000 in a certain city. Construct a bar chart for
these data.
Continued
13
1.4 Graphical Representation of Data

Solution
In the bar graph of Figure 1.1, we use the notations MC for medical care, El for electricity, RR for
residential rent, Fd for food, CPI for consumer price index, and A & U for apparel and upkeep.
Looking at Figure 1.1, we can identify where the maximum and minimum
responses are located, so that we can descriptively discuss the phenomenon whose
behavior we want to understand.
For a graphical representation of the relative importance of different factors
under study, one can use the Pareto chart. It is a bar graph with the height of the
bars proportional to the contribution of each factor. The bars are displayed from
the most numerous category to the least numerous category, as illustrated by the fol-
lowing example. A Pareto chart helps in separating significantly few factors that
have larger influence from the trivial many.
EXAMPLE 1.4.2
For the data of Example 1.4.1, construct a Pareto chart.
Solution
First, rewrite the data in decreasing order. Then create a Pareto chart by displaying the bars from
the most numerous category to the least numerous category.
100
80
60
40
20
0
MC
EI
RR
Fd
Category
Percentage
CPI
A & U
FIGURE 1.1
Percentage price increase of consumer goods.
Table 1.5 Percentages of Price Increases of Some
Consumer Goods and Services
Medical Care
83.3%
Electricity
22.1%
Residential Rent
43.5%
Food
41.1%
Consumer Price Index
35.8%
Apparel & Upkeep
21.2%
14
CHAPTER 1 Descriptive Statistics

Looking at Figure 1.2, we can identify the relative importance of each category
such as the maximum, the minimum, and the general behavior of the subject data.
Vilfredo Pareto (1848-1923), an Italian economist and sociologist, studied the dis-
tributions of wealth in different countries. He concluded that about 20% of
people controlled about 80% of a society’s wealth. This same distribution has been
observed in other areas such as quality improvement: 80% of problems usually stem
from 20% of the causes. This phenomenon has been termed the Pareto effect or
80/20 rule. Pareto charts are used to display the Pareto principle, arranging data so that
the few vital factors that are causing most of the problems reveal themselves. Focusing
improvement efforts on these few causes will have a larger impact and be more cost-
effective than undirected efforts. Pareto charts are used in business decision making as
a problem-solving and statistical tool that ranks problem areas, or sources of variation,
according to their contribution to cost or to total variation.
Definition 1.4.2 A circle divided into sectors that represent the percentages of a
population or a sample that belongs to different categories is called a pie chart.
Pie charts are especially useful for presenting categorical data. The pie “slices” are
drawn such that they have an area proportional to the frequency. The entire pie rep-
resents all the data, whereas each slice represents a different class or group within
the whole. Thus, we can look at a pie chart and identify the various percentages of
interest and how they compare among themselves. Most statistical software can create
3D charts. Such charts are attractive; however, they can make pieces at the front look
larger than they really are. In general, a two-dimensional view of the pie is preferable.
EXAMPLE 1.4.3
The combined percentages of carbon monoxide (CO) and ozone (O3) emissions from different
sources are listed in Table 1.6.
Construct a pie chart.
Solution
The pie chart is given in Figure 1.3.
Continued
100
80
60
40
20
0
MC
EI
RR
Fd
Category
Percentage increase
CPI
A&U
FIGURE 1.2
Pareto chart.
15
1.4 Graphical Representation of Data

Definition 1.4.3 A stem-and-leaf plot is a simple way of summarizing quantita-
tive data and is well suited to computer applications. When data sets are relatively
small, stem-and-leaf plots are particularly useful. In a stem-and-leaf plot, each data
value is split into a “stem” and a “leaf.” The “leaf” is usually the last digit of the
number and the other digits to the left of the “leaf” form the “stem.” Usually there is
no need to sort the leaves, although computer packages typically do. For more
details, we refer the student to elementary statistics books. We illustrate this tech-
nique by an example.
EXAMPLE 1.4.4
Construct a stem-and-leaf plot for the 20 test scores given below.
78
74
82
66
94
71
64
88
55
80
91
74
82
75
96
78
84
79
71
83
Table 1.6 Combined Percentages of CO and O3 Emissions
Transportation
(T)
Industrial
process (I)
Fuel
combustion
(F)
Solid
waste
(S)
Miscellaneous
(M)
63%
10%
14%
5%
8%
F (14.0%)
I (10.0%)
T (63.0%)
S (5.0%)
M (8.0%)
FIGURE 1.3
Pie chart for CO and O3.
16
CHAPTER 1 Descriptive Statistics

Solution
At a glance, we see that the scores are distributed from the 50s through the 90s. We use the first digit
of the score as the stem and the second digit as the leaf. The plot in Table 1.7 is constructed with
stems in the vertical position.
The stem-and-leaf plot condenses the data values into a useful display from which
we can identify the shape and distribution of data such as the symmetry, where the
maximum and minimum are located with respect to the frequencies, and whether
they are bellshaped. This fact that the frequencies are bellshaped will be of para-
mount importance as we proceed to study inferential statistics. Also, note that the
stem-and-leaf plot retains the entire data set and can be used only with quantitative
data. Examples 1.8.1 and 1.8.6 explain how to obtain a stem-and-leaf plot using
Minitab and SPSS, respectively. Refer to Section 1.8.4 for SAS commands to gen-
erate graphical representations of the data.
A frequency table is a table that divides a data set into a suitable number of cat-
egories (classes). Rather than retaining the entire set of data in a display, a frequency
table essentially provides only a count of those observations that are associated
with each class. Once the data are summarized in the form of a frequency table, a
graphical representation can be given through bar graphs, pie charts, and histograms.
Data presented in the form of a frequency table are called grouped data. A frequency
table is created by choosing a specific number of classes in which the data will be
placed. Generally the classes will be intervals of equal length. The center of each
class is called a class mark. The end points of each class interval are called class
boundaries. Usually, there are two ways of choosing class boundaries. One way is
to choose nonoverlapping class boundaries so that none of the data points will simul-
taneously fall in two classes. Another way is that for each class, except the last,
the upper boundary is equal to the lower boundary of the subsequent class. When
forming a frequency table this way, one or more data values may fall on a class
boundary. One way to handle such a problem is to arbitrarily assign it one of the
classes or to flip a coin to determine the class into which to place the observation
at hand.
Definition 1.4.4 Let fi denote the frequency of the class i and let n be sum of all
frequencies. Then the relative frequency for the class i is defined as the ratio fi/n.
The cumulative relative frequency for the class i is defined by P
k¼1
i
fk/n.
Table 1.7 Stem-and-Leaf Display of 20 Exam Scores
Stem
Leaves
5
5
6
6
4
7
8
4
1
4
5
8
9
1
8
2
8
0
2
4
3
9
4
1
6
17
1.4 Graphical Representation of Data

The following example illustrates the foregoing discussion.
EXAMPLE 1.4.5
The following data give the lifetime of 30 incandescent light bulbs (rounded to the nearest hour) of a
particular type.
872
931
1146
1079
915
879
863
1112
979
1120
1150
987
958
1149
1057
1082
1053
1048
1118
1088
868
996
1102
1130
1002
990
1052
1116
1119
1028
Construct a frequency, relative frequency, and cumulative relative frequency table.
Solution
Note that there are n¼30 observations and that the largest observation is 1150 and the smallest one
is 865 with a range of 285. We will choose six classes each with a length of 50.
When data are quantitative in nature and the number of observations is relatively
large, and there are no natural separate categories or classes, we can use a histogram
to simplify and organize the data.
Definition 1.4.5 A histogram is a graph in which classes are marked on the hor-
izontal axis and either the frequencies, relative frequencies, or percentages are
represented by the heights on the vertical axis. In a histogram, the bars are drawn
adjacent to each other without any gaps.
Histograms can be used only for quantitative data. A histogram compresses a data
setintoacompactpicturethatshowsthelocationofthemeanandmodesofthedataand
the variation in the data, especially the range. It identifies patterns in the data. This is a
good aggregate graph of one variable. In order to obtain the variability in the data, it is
always a good practice to start with a histogram of the data. The following steps can be
used as a general guideline to construct a frequency table and produce a histogram.
GUIDELINE FOR THE CONSTRUCTION OF A FREQUENCY
TABLE AND HISTOGRAM
1. Determine the maximum and minimum values of the observations. The range, R¼maximum
valueminimum value.
Class
Frequency
fi
Relative
frequency fi/fi
Cumulative relative
frequency  k¼1
i
fk/n
50-900
4
4/30
4/30
900-950
2
2/30
6/30
950-1000
5
5/30
11/30
1000-1050
3
3/30
14/30
1050-1100
6
6/30
20/30
1100-1150
10
10/30
30/30
18
CHAPTER 1 Descriptive Statistics

2. Select from five to 20 classes that in general are nonoverlapping intervals of equal length, so as to
cover the entire range of data. The goal is to use enough classes to show the variation in the data,
but not so many that there are only a few data points in many of the classes. The class width
should be slightly larger than the ratio Largest valueSmallest value
Number of classes
:
3. The first interval should begin a little below the minimum value, and the last interval should end
a little above the maximum value. The intervals are called class intervals and the boundaries are
called class boundaries. The class limits are the smallest and the largest data values in the class.
The class mark is the midpoint of a class.
4. None of the data values should fall on the boundaries of the classes.
5. Construct a table (frequency table) that lists the class intervals, a tabulation of the number of
measurements in each class (tally), the frequency fi of each class, and, if needed, a column with
relative frequency, fi/n, where n is the total number of observations.
6. Draw bars over each interval with heights being the frequencies (or relative frequencies).
Let us illustrate implementing these steps in the development of a histogram for
the data given in the following example.
EXAMPLE 1.4.6
The following data refer to a certain type of chemical impurity measured in parts per million in 25
drinking-water samples randomly collected from different areas of a county.
11
19
24
30
12
20
25
29
15
21
24
31
16
23
25
26
32
17
22
26
35
18
24
18
27
(a) Make a frequency table displaying class intervals, frequencies, relative frequencies, and
percentages.
(b) Construct a frequency histogram.
Solution
(a) We will use five classes. The maximum and minimum values in the data set are 35 and 11. Hence
the class width is (3511)/5¼4.8 ﬃ5. Hence, we shall take the class width to be 5. The lower
boundary of the first class interval will be chosen to be 10.5. With five classes, each of width 5,
the upper boundary of the fifth class becomes 35.5. We can now construct the frequency table for
the data.
Continued
Class
Class Interval
fi¼frequency
Relative Frequency
Percentage
1
10.5-15.5
3
3/25¼0.12
12
2
15.5-20.5
6
6/25¼0.24
24
3
20.5-25.5
8
8/25¼0.32
32
4
25.5-30.5
5
5/25¼0.20
20
5
30.5-35.5
3
3/25¼0.12
12
19
1.4 Graphical Representation of Data

(b) We can generate a histogram as in Figure 1.4.
From the histogram we should be able to identify the center (i.e. the location) of the data, spread
of the data, skewness of the data, presence of outliers, presence of multiple modes in the data, and
whether the data can be capped with a bell-shaped curve. These properties provide indications of the
proper distributional model for the data. Examples 1.8.2 and 1.8.7 explain how to obtain histograms
using Minitab and SPSS, respectively.
EXERCISES 1.4
1.4.1. According to the recent US Federal Highway Administration Highway
Statistics, the percentages of freeways and expressways in various road
mileage-related highway pavement conditions are as follows:
Poor 10%, Mediocre 32%, Fair 22%, Good 21%, and Very good 15%.
(a) Construct a bar graph.
(b) Construct a pie chart.
1.4.2. More than 75% of all species that have been described by biologists are
insects. Of the approximately 2 million known species, only about 30,000
are aquatic in any life stage. The data in Table 1.4.1 give proportion of total
species by insect order that can survive exposure to salt (source: http://
entomology.unl.edu/).
(a) Construct a bar graph.
(b) Construct a Pareto chart.
(c) Construct a pie chart.
1.4.3. The data in Table 1.4.2 are presented to illustrate the role of renewable
energy consumption in the US energy supply in 2007 (source: http://www.
eia.doe.gov/fuelrenewable.html). Renewable energy consists of biomass,
geothermal energy, hydroelectric energy, solar energy, and wind energy.
9
8
7
6
5
10.5
15.5
20.5
Data Interval
Frequency
25.5
30.5
35.5
FIGURE 1.4
Frequency histogram of impurity data.
20
CHAPTER 1 Descriptive Statistics

(a) Construct a bar graph.
(b) Construct a Pareto chart.
(c) Construct a pie chart.
1.4.4. A litter is a group of babies born from the same mother at the same time.
Table 1.4.3 gives some examples of different mammals and their average
litter size (source: http://www.saburchill.com/chapters/chap0032.html).
(a) Construct a bar graph.
(b) Construct a Pareto chart.
1.4.5. The following data give the letter grades of 20 students enrolled in a
statistics course.
Table 1.4.2 Renewable Energy Consumption
Source
Percentage
Coal
22%
Natural Gas
23%
Nuclear Electric Power
8%
Petroleum
40%
Renewable Energy
7%
Table 1.4.1 Percentage of Species by Insect Order
Species
Percentage
Species
Percentage
Coleoptera
26%
Odonata
3%
Diptera
35%
Thysanoptera
3%
Hemiptera
15%
Lepidoptera
1%
Orthoptera
6%
Other
6%
Collembola
5%
Table 1.4.3 Litter Size of Mammals
Species
Litter size
Bat
1
Dolphin
1
Chimpanzee
1
Lion
3
Hedgehog
5
Red Fox
6
Rabbit
6
Black Rat
11
21
1.4 Graphical Representation of Data

A
B
F
A
C
C
D
A
B
F
C
D
B
A
B
A
F
B
C
A
(a) Construct a bar graph.
(b) Construct a pie chart.
1.4.6. According to the US Bureau of Labor Statistics (BLS), the median weekly
earnings of fulltime wage and salary workers by age for the third quarter of
1998 is given in Table 1.4.4.
Construct a pie chart and bar graph for these data and interpret. Also,
construct a Pareto chart.
1.4.7. The data in Table 1.4.5 are a breakdown of 18,930 workers in a town
according to the type of work. Construct a pie chart and bar graph for these
data and interpret.
1.4.8. The data in Table 1.4.6 represent the number (in millions) of adults and
children living with HIV/AIDS by the end of 2000 according to the region
of the world (source: http://w3.whosea.org/hivaids/factsheet.htm).
Construct a bar graph for these data. Also, construct a Pareto chart and
interpret.
1.4.9. The data in Table 1.4.7 give the life expectancy at birth, in years, from 1900
through 2000 (source: National Center for Health Statistics). Construct a
bar graph for these data.
Table 1.4.4 Weekly Wage & Salary Distribution by Age
16 to 19 years
$260
20 to 24 years
$334
25 to 34 years
$498
35 to 44 years
$600
45 to 54 years
$628
55 to 64 years
$605
65 years and over
$393
Table 1.4.5 Distribution of Workers by Type of Work
Mining
58
Construction
1161
Manufacturing
2188
Transportation and Public Utilities
821
Wholesale Trade
657
Retail Trade
7377
Finance, Insurance, and Real Estate
890
Services
5778
Total
18,930
22
CHAPTER 1 Descriptive Statistics

1.4.10. Dolphins are usually identified by the shape and pattern of notches and nicks
on their dorsal fin. Individual dolphins are cataloged by classifying the fin
based on location of distinguishing marks. When a dolphin is sighted its
picture can then be compared to the catalog of dolphins in the area, and if a
match is found, the dolphin can be recorded as resighted. These methods of
mark-resight are for developing databases regarding the life history of
individual dolphins. From these databases we can calculate the levels of
association between dolphins, population estimates, and general life history
parameters such as birth and survival rates. The data in Table 1.4.8 represent
frequently resighted individuals (as of January 2000) at a particular location
(source: http://www.eckerd.edu/dolphinproject/biologypr.html).
Construct a bar graph for these data.
Table 1.4.6 Number of People Living with HIV/AIDS
Country
Adults and Children Living with
HIV/AIDS (in millions)
Sub-Saharan Africa
25.30
North Africa and Middle East
0.40
South and Southeast Asia
5.80
East Asia and Pacific
0.64
Latin America
1.40
Caribbean
0.39
Eastern Europe and Central Asia
0.70
Western Europe
0.54
North America
0.92
Australia and New Zealand
0.15
Table 1.4.7 Life Expectancy at Birth
Year
Life expectancy
1900
47.3
1960
69.7
1980
73.7
1990
75.4
2000
77.0
Table 1.4.8 Number of Dolphin Resight by Type
Hammer (adult female)
59
Mid Button Flag (adult female)
41
Luseal (adult female)
31
84 Lookalike (adult female)
20
23
1.4 Graphical Representation of Data

1.4.11. The data in Table 1.4.9 give death rates (per 100,000 population) for 10
leading causes in 1998 (source: National Center for Health Statistics, US
Department of Health and Human Services).
(a) Construct a bar graph.
(b) Construct a Pareto chart.
1.4.12. In a fiscal year, a city collected $32.3 million in revenues. City spending
for that year is expected to be nearly the same, with no tax increase
projected.
Expenditure: Reserves 0.7%, capital outlay 29.7%, operating expenses
28.9%, debt service 3.2%, transfers 5.1%, and personal services 32.4%.
Revenues: Property taxes 10.2%, utility and franchise taxes 11.3%,
licenses and permits 1%, inter-governmental revenue 10.1%, charges for
services 28.2%, fines and forfeits 0.5%, interest and miscellaneous 2.7%,
transfers and cash carryovers 36%.
(a) Construct bar graphs for expenditure and revenues and interpret.
(b) Construct pie charts for expenditure and revenues and interpret.
1.4.13. Construct a histogram for the 24 examination scores given next.
78
74
82
66
94
71
64
88
55
80
73
86
91
74
82
75
96
78
84
79
71
83
78
79
1.4.14. The following table gives radon concentration in pCi/liter (pico Curies per
liter) obtained from 40 houses in a certain area.
2.9
0.6
13.5
17.1
2.8
3.8
16.0
2.1
6.4
17.2
7.9
0.5
13.7
11.5
2.9
3.6
6.1
8.8
2.2
9.4
15.9
8.8
9.8
11.5
12.3
3.7
8.9
13.0
7.9
11.7
6.2
6.9
12.8
13.7
2.7
3.5
8.3
15.9
5.1
6.0
Table 1.4.9 Death Rate by Cause
Cause
Death rate
Accidents and Adverse Effects
34.5
Chronic Liver Disease and Cirrhosis
9.7
Chronic Obstructive Lung Diseases and Allied Conditions
42.3
Cancer
199.4
Diabetes Mellitus
23.9
Heart Disease
268.0
Kidney Disease
9.7
Pneumonia and Influenza
35.1
Stroke
58.5
Suicide
10.8
24
CHAPTER 1 Descriptive Statistics

(a) Construct a stem-and-leaf display.
(b) Construct a frequency histogram and interpret.
(c) Construct a pie chart and interpret.
1.4.15. The following data give the mean of SAT Mathematics scores by state for
1999 for a randomly selected 20 states (source: The World Almanac and
Book of Facts 2000).
558
503
565
572
546
517
542
605
493
499
568
553
510
525
595
502
526
475
506
568
(a) Construct a stem-and-leaf display and interpret.
(b) Construct a frequency histogram and interpret.
(c) Construct a pie chart and interpret.
1.4.16. A sample of 25 measurements is given here:
9
28
14
29
21
27
15
23
23
10
31
23
16
26
22
17
19
24
21
20
26
20
16
14
21
(a) Make a frequency table displaying class intervals, frequencies, relative
frequencies, and percentages.
(b) Construct a frequency histogram and interpret.
1.4.17. We may be interested in changing demographics of the US population.
Following table gives the demographics in 2010 (Overview of Race and
Hispanic Origin: 2010, http://www.census.gov/prod/cen2010/briefs/
c2010br-02.pdf). A table gives a pretty good summary understanding.
Table 1.4.10 US Population Demographics
Race/Ethnicity
Number
Percentage of
US Population
White or European American
223,553,265
72.4%
Black or African American
38,929,319
12.6%
Asian American
14,674,252
4.8%
American Indian or Alaska Native
2,932,248
0.9%
Native Hawaiian or other Pacific Islander
540,013
0.2%
Some other race
19,107,368
6.2%
Two or more races
9,009,073
2.9%
Not Hispanic nor Latino
258,267,944
83.6%
Non-Hispanic White or European American
196,817,552
63.7%
Non-Hispanic Black or African American
37,685,848
12.2%
Non-Hispanic Asian
14,465,124
4.7%
Non-Hispanic American Indian or Alaska
Native
2,247,098
0.7%
Continued
25
1.4 Graphical Representation of Data

Draw a pie chart.
1.5 NUMERICAL DESCRIPTION OF DATA
In the previous section we looked at some graphical and tabular techniques for
describing a data set. We shall now consider some numerical characteristics of a
set of measurements. Suppose that we have a sample with values x1, x2, . . ., xn.
There are many characteristics associated with this data set, for example, the
central tendency and variability. A measure of the central tendency is given by
the sample mean, median, or mode, and the measure of dispersion or variability
is usually given by the sample variance or sample standard deviation or interquar-
tile range.
Definition 1.5.1 Let x1, x2, . . ., xn be a set of sample values. Then the sample
mean (or empirical mean) x is defined by
x ¼ 1
n
X
n
i¼1
xi:
The sample variance is defined by
s2 ¼
1
n1
ð
Þ
X
n
i¼1
xi x
ð
Þ2:
The sample standard deviation is
s ¼
ﬃﬃﬃﬃ
s2
p
The sample variance s2 and the sample standard deviation s both are measures of
the variability or “scatteredness” of data values around the sample mean x.
Table 1.4.10 US Population Demographics—cont’d
Race/Ethnicity
Number
Percentage of
US Population
Non-Hispanic Native Hawaiian or other Pacific
Islander
481,576
0.2%
Non-Hispanic Some Other Race
604,265
0.2%
Non-Hispanic Two or more races
5,966,481
1.9%
Hispanic or Latino
50,477,594
16.4%
White or European American Hispanic
26,735,713
8.7%
Black or African American Hispanic
1,243,471
0.4%
American Indian or Alaska Native Hispanic
685,150
0.2%
Asian Hispanic
209,128
0.1%
Native Hawaiian or other Pacific Islander Hispanic
58,437
0.0%
Some Other Race Hispanic
18,503,103
6.0%
Two or more races Hispanic
3,042,592
1.0%
Total
308,745,538
100.0%
26
CHAPTER 1 Descriptive Statistics

Larger the variance, more is the spread. We note that s2 and s are both nonneg-
ative. One question we may ask is “why not just take the sum of the differences
xi x
ð
Þ as a measure of variation?” The answer lies in the following result which
shows that if we add up all deviations about the sample mean, we always get a
zero value.
Theorem 1.5.1 For a given set of measurements x1, x2, . . ., xn, let x be the sample
mean. Then
X
n
i¼1
xi x
ð
Þ ¼ 0:
Proof. Since x ¼ 1=n
ð
Þ
Xn
i¼1xi, we have
Xn
i¼1xi ¼ nx: Now
X
n
i¼1
xi x
ð
Þ ¼
X
n
i¼1
xi 
X
n
i¼1
x
¼ nxnx ¼ 0:
Thus although there may be a large variation in the data values,
Xn
i¼1 xi x
ð
Þ as a
measure of spread would always be zero, implying no variability. So it is not useful
as a measure of variability.
▄
Sometimes we can simplify the calculation of the sample variance s2 by using the
following computational formula:
s2 ¼
Xn
i¼1x2
i  1
n
Xn
i¼1xi

2


n1
ð
Þ
:
If the data set has a large variation with some extreme values (called outliers), the
mean may not be a very good measure of the center. For example, average salary
may not be a good indicator of the financial well-being of the employees of a com-
pany if there is a huge difference in pay between support personnel and management
personnel. In that case, one could use the median as a measure of the center, roughly
50% of data fall below and 50% above. The median is less sensitive to extreme data
values.
Definition 1.5.2 For a data set, the median is the middle number of the ordered
data set. If the data set has an even number of elements, then the median is the aver-
age of the middle two numbers. The lower quartile is the middle number of the half of
the data below the median, and the upper quartile is the middle number of the half of
the data above the median. We will denote
Q1 ¼ lower quartile
Q2 ¼ M ¼ middle quartile median
ð
Þ
Q3 ¼ upper quartile
The difference between the quartiles is called interquartile range (IQR).
IQR ¼ Q3 Q1:
27
1.5 Numerical Description of Data

A possible outlier (mild outlier) will be any data point that lies below
Q1 1:5 IQR
ð
Þ or above Q3 + 1:5 IQR
ð
Þ:
Thus, about 25% of the data lie below Q1, and about 75% of the data lie below Q3.
Note that the IQR is unaffected by the positions of those observations in the smallest
25% or the largest 25% of the data.
Mode is another commonly used measure of central tendency. A mode indicates
where the data tend to concentrate most.
Definition 1.5.3 Mode is the most frequently occurring member of the data set. If
all the data values are different, then by definition, the data set has no mode.
EXAMPLE 1.5.1
The following data give the time in months from hire to promotion to manager for a random
sample of 25 software engineers from all software engineers employed by a large telecommuni-
cations firm.
5
7
229
453
12
14
18
14
14
483
22
21
25
23
24
34
37
34
49
64
47
67
69
192
125
Calculate the mean, median, mode, variance, and standard deviation for this sample.
Solution
The sample mean is
x ¼ 1
n
X
n
i¼1
xi ¼ 83:28 months:
To obtain the median, first arrange the data in ascending order:
5
7
12
14
14
14
18
21
22
23
24
25
34
34
37
47
49
64
67
69
125
192
229
453
483
Now the median is the thirteenth number which is 34 months.
Since 14 occurs most often (thrice), the mode is 14 months.
The sample variance is
s2 ¼
1
n1
X
n
i¼1
xi x
ð
Þ2
¼ 1
24
583:28
ð
Þ2 +  + 12583:28
ð
Þ2
h
i
¼ 16,478:
and the sample standard deviation is, s ¼
ﬃﬃﬃﬃ
s2
p
¼ 128:36months. Thus, we have sample mean
x ¼ 83:28 months, median¼34 months, and mode¼14 months. Note that the mean is very much
different from the other two measures of center because of a few large data values. Also, the sample
variance s2¼16,478 months, and the sample standard deviation s¼128.36 months.
28
CHAPTER 1 Descriptive Statistics

EXAMPLE 1.5.2
For the data of Example 1.5.1, find lower and upper quartiles, median, and interquartile range (IQR).
Check for any outliers.
Solution
Arrange the data in an ascending order.
5
7
12
14
14
14
18
21
22
23
24
25
34
34
37
47
49
64
67
69
125
192
229
453
483
Then the median M is the middle (13th) data value, M¼Q2¼34. The lower quartile is the middle
number below the median, Q1¼[(14+18)/2]¼16. The upper quartile, Q3¼[(67+69)/2]¼68.
The interquartile range, (IQR)¼Q3Q1¼6816¼52.
To test for outliers, compute
Q1 1:5 IQR
ð
Þ ¼ 161:5 52
ð
Þ ¼ 62
and
Q3 + 1:5 IQR
ð
Þ ¼ 68 + 1:5 52
ð
Þ ¼ 146:
Then all the data that fall above 146 are possible outliers. None is below 62. Therefore the
outliers are 192, 229, 453, and 483.
We have remarked earlier that the mean as a measure of central location is greatly
affected by the extreme values or outliers. A robust measure of central location
(a measure that is relatively unaffected by outliers) is the trimmed mean. For
0a1, a 100a% trimmed mean is found as follows: order the data, and then dis-
card the lowest 100a% and the highest 100a% of the data values. Find the mean of
the rest of the data values. We denote the 100a% trimmed mean by xa. We illustrate
the trimmed mean concept in the following example.
EXAMPLE 1.5.3
For the data set representing the number of children in a random sample of 10 families in a neigh-
borhood, find the 10% trimmed mean (a¼0.1).
1 2 2 3 2 3 9 1 6 2
Solution
Arrange the data in ascending order.
1 1 2 2 2 2 3 3 6 9
The data set has 10 elements. Discarding the lowest 10% (10% of 10 is 1) and discarding the
highest 10% of the data values, we obtain the trimmed data set as
1 2 2 2 2 3 3 6
The 10% trimmed mean is
x0:1 ¼ 1 + 2 + 2 + 2 + 2 + 3 + 3 + 6
8
¼ 2:6:
Note that the mean for the data in the previous example without removing any observations is
3.1, which is different from the trimmed mean.
29
1.5 Numerical Description of Data

Although standard deviation is a more popular method, there are other measures of dis-
persion such as average deviation or interquartile range. We have already seen the def-
inition of interquartile range. The average deviation for a sample x1, . . ., xn is defined by
Average deviation ¼
Xn
i¼1 xi x
j
j
n
:
Calculation of average deviation is simple and straightforward.
1.5.1 NUMERICAL MEASURES FOR GROUPED DATA
When we encounter situations where the data are grouped in the form of a frequency
table (see Section 1.4), we no longer have individual data values. Hence, we cannot
use the formulas in Definition 1.5.1. The following formulas will give approximate
values for x and s2. Let the grouped data have l classes, with mi being the midpoint
and fi being the frequency of class i, i¼1, 2, ..., l. Let n¼P
i¼1
l
fi.
Definition 1.5.4 The mean for a sample of size n,
x ¼ 1
n
X
l
i¼1
f imi,
where mi is the midpoint of the class i and fi is the frequency of the class i.
Similarly the sample variance,
s2 ¼
1
n1
X
n
i¼1
f i mi x
ð
Þ2 ¼
X
m2
i f i 
X
if imi

2
n
n1
:
The following example illustrates how we calculate the sample mean for a
grouped data.
EXAMPLE 1.5.4
The grouped data in Table 1.8 represent the number of children from birth through the end of the
teenage years in a large apartment complex. Find the mean, variance, and standard deviation for
these data:
Here we use the usual convention of until the child attains next age, age will be the previous
year, for instance until child is 4-year old, we will say child is 3-year old.
Solution
For simplicity of calculation we create Table 1.9.
The sample mean is
x ¼ 1
n
X
i
f imi ¼ 515
50 ¼ 10:30:
The sample variance is
s2 ¼
X
m2
i f i 
X
if imi

2
n
n1
¼
6488:5 515
ð
Þ2
50
49
¼ 24:16:
30
CHAPTER 1 Descriptive Statistics

The sample standard deviation is s ¼
ﬃﬃﬃﬃ
s2
p
¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
24:16
p
¼ 4:92:
Using the following calculations, we can also find the median for grouped data. We
only know that the median occurs in a particular class interval, but we do not know
the exact location of the median. We will assume that the measures are spread evenly
throughout this interval. Let
L¼lower class limit of the interval that contains the median
n¼total frequency
Fb¼cumulative frequencies for all classes before the median class
fm¼frequency of the class interval containing the median
w¼interval width of the interval that contains the median
Then the median for the grouped data is given by
M ¼ L + w
f m
0:5nFb
ð
Þ:
We proceed to illustrate with an example.
EXAMPLE 1.5.5
For the data of Example 1.5.4, find the median.
Solution
First develop Table 1.10.
Table 1.9 Summary Statistics for Number of Children
Class
fi
mi
mifi
mi
2fi
0-3
7
1.5
10.5
15.75
4-7
4
5.5
22
121
8-11
19
9.5
180.5
1714.75
12-15
12
13.5
162
2187
16-19
8
17.5
140
2450
n¼50
Pmifi¼515
Pmi
2fi¼6488.5
Table 1.10 Frequency Distribution for Number of Children
Class
fi
Cumulative fi
Cumulative fi/n
0-3
7
7
0.14
4-7
4
11
0.22
8-11
19
30
0.6
12-15
12
42
0.84
16-19
8
50
1.00
Table 1.8 Number of Children and Their Age Group
Class
0-3
4-7
8-11
12-15
16-19
Frequency
7
4
19
12
8
31
1.5 Numerical Description of Data

The first interval for which the cumulative relative frequency exceeds 0.5 is the
interval that contains the median. Hence the interval 8 to 11 contains the median.
Therefore, L¼8, fm¼19, n¼50, w¼3, and Fb¼11. Then, the median is
M ¼ L + w
f m
0:5nFb
ð
Þ ¼ 8 + 3
19
0:5
ð
Þ 50
ð
Þ11
ð
Þ ¼ 10:211:
It is important to note that all the numerical measures we calculate for grouped data
are only approximations to the actual values of the ungrouped data if they are
available.
One of the uses of the sample standard deviation will be clear from the following
result, which is based on data following a bell-shaped curve. Such an indication can
be obtained from the histogram or stem-and-leaf display.
EMPIRICAL RULE
When the histogram of a data set is “bell-shaped” or “mound shaped,” and symmetric, the empirical
rule states:
1. Approximately 68% of the data are in the interval xs, x + s
ð
Þ:
2. Approximately 95% of the data are in the interval x2s, x + 2s
ð
Þ:
3. Approximately 99.7% of the data are in the interval x3s, x + 3s
ð
Þ:
The bell-shaped curve is called a normal curve and is discussed later in Chapter 3.
A typical symmetric bell-shaped curve is given in Figure 1.5.
Normal distribution
0.4
0.3
0.2
0.1
0.0
−3
−2
−1
0
x
1
2
3
3 sd
2 sd
1 sd
FIGURE 1.5
Bell-shaped curve.
32
CHAPTER 1 Descriptive Statistics

1.5.2 BOX PLOTS
The sample mean or the sample standard deviation focuses on a single aspect of the
data set, whereas histograms and stem-and-leaf displays express rather general ideas
about data. A pictorial summary called a box plot (also called box-and-whisker plots)
can be used to describe several prominent features of a data set such as the center, the
spread, the extent and nature of any departure from symmetry, and identification of
outliers. Box plots are a simple diagrammatic representation of the five number sum-
mary: minimum, lower quartile, median, upper quartile, maximum. Example 1.8.4
illustrates the method of obtaining box plots using Minitab.
PROCEDURE TO CONSTRUCT A BOX PLOT
1. Draw a vertical measurement axis and mark Q1, Q2 (median), and Q3 on this axis as shown in
Figure 1.6.
2. Construct a rectangular box whose bottom edge lies at the lower quartile, Q1 and whose upper
edge lies at the upper quartile, Q3.
Continued
Extreme outliers
Mild outliers
Mild outliers
Extreme outliers
∗
∗
Whisker
Whisker
(1.5)IQR
(3)IQR
Q3
Q2
Q1
(3)IQR
(1.5)IQR
∗
∗
FIGURE 1.6
A typical box-and-whiskers plot.
33
1.5 Numerical Description of Data

3. Draw a horizontal line segment inside the box through the median.
4. Extend the lines from each end of the box out to the farthest observation that is still within 1.5
(IQR) of the corresponding edge. These lines are called whiskers.
5. Draw an open circle (or asterisks *) to identify each observation that falls between 1.5(IQR) and
3(IQR) from the edge to which it is closest; these are called mild outliers.
6. Draw a solid circle to identify each observation that falls more than 3(IQR) from the closest edge;
these are called extreme outliers.
We illustrate the procedure with the following example.
EXAMPLE 1.5.6
The following data identify the time in months from hire to promotion to chief pharmacist for a random
sample of 25 employees from a certain group of employees in a large corporation of drugstores.
5
7
12
14
14
14
18
21
22
23
24
25
34
34
37
47
49
64
67
69
125
192
229
453
483
Construct a box plot. Do the data appear to be symmetrically distributed along the
measurement axis?
Solution
Referring to Example 1.5.2, we find that the median, Q2¼34.
The lower quartile is Q1 ¼ 14 + 18
2
¼ 16:
The upper quartile is Q3 ¼ 67 + 69
2
¼ 68:
The interquartile range is IQR¼6816¼52.
To find the outliers, compute
Q1 1:5 IQR
ð
Þ ¼ 161:5 52
ð
Þ ¼ 62
and
Q3 + 1:5 IQR
ð
Þ ¼ 68 + 1:5 52
ð
Þ ¼ 146:
Using these numbers, we follow the procedure outlined earlier to construct the box plot in
Figure 1.7. The * in the box plot represents an outlier. The first horizontal line is the first quartile,
the second is the median, and the third is the third quartile.
500
400
**
**
300
Months
200
100
0
FIGURE 1.7
Box plot for months to promotion.
34
CHAPTER 1 Descriptive Statistics

By examining the relative position of the median line (the middle line in Figure 1.7),
we can test the symmetry of the data. For example, in Figure 1.7, the median line is
closer to the lower quartile than the upper line, which suggests that the distribution is
slightly nonsymmetric. Also, a look at this box plot shows the presence of two mild
outliers and two extreme outliers.
EXERCISES 1.5
1.5.1. The prices of 12 randomly chosen homes in dollars (approximated to
nearest thousand) in a growing region of Tampa in the summer of 2002 are
given below.
176 105 133 140 305 215 207 210 173 150 78 96
Find the mean and standard deviation of the sampled home prices from
this area.
1.5.2. The following is a sample of nine mortgage companies’ interest rates for
30-year home mortgages, assuming 5% down.
7:625 7:500 6:625 7:625 6:625 6:875 7:375 5:375 7:500
(a) Find the mean and standard deviation and interpret.
(b) Find lower and upper quartiles, median, and interquartile range. Check
for any outliers and interpret.
1.5.3. For four observations, it is given that mean is 6, median is 4, and mode is 3.
Find the standard deviation of this sample.
1.5.4. The data given below pertain to a random sample of disbursements of state
highway funds (in millions of dollars), to different states.
1188
1050
2882
2802
780
1171
685
537
519
2523
316
1117
1578
261
(a) Find the mean, variance, and range for these data and interpret.
(b) Find lower and upper quartiles, median and interquartile range. Check
for any outliers and interpret.
(c) Construct a box plot and interpret.
1.5.5. Maximal static inspiratory pressure (PImax) is an index of respiratory
muscle strength. The following data show the measure of PImax (cm H2O)
for 15 cystic fibrosis patients.
105
80
115
95
100
85
90
70
135
105
45
115
40
115
95
(a) Find the lower and upper quartiles, median, and interquartile range.
Check for any outliers and interpret.
35
1.5 Numerical Description of Data

(b) Construct a box plot and interpret.
(c) Are there any outliers?
1.5.6. Compute the mean, variance, and standard deviation for the data in
Table 1.5.1 (assume that the data belong to a sample).
1.5.7. (a) For any grouped data with l classes with group frequencies fi, and class
midpoints mi, show that
X
l
i¼1
f i mi x
ð
Þ ¼ 0:
(b) Verify this result for the data given in Exercise 1.5.6.
1.5.8. (a) Given the sample values x1, x2, . . ., xn, show that
X
n
i¼1
xi x
ð
Þ2 ¼
X
n
i¼1
x2
i 
Xn
i¼1xi

2
n
:
(b) Verify the result of part (a) for the data of Exercise 1.5.5.
1.5.9. The following are the closing prices of some securities that a mutual fund
holds on a certain day:
10.25
5.31
11.25
13.13
18.00
32.56
37.06
39.00
43.25
45.00
40.06
28.56
22.75
51.50
47.00
53.50
32.00
25.44
22.50
30.00
24.75
53.37
51.38
26.00
53.50
29.87
32.00
28.87
42.19
37.50
30.44
41.37
(a) Find the mean, variance, and range for these data and interpret.
(b) Find lower and upper quartiles, median, and interquartile range. Check
for any outliers.
(c) Construct a box plot and interpret.
(d) Construct a histogram.
(e) Locate on your histogram x, xs, x2s, and x3s Count the data
points in each of the intervals xs, x2s, and x3s and compare this
with the empirical rule.
1.5.10. The radon concentration (in pCi/liter) data obtained from 40 houses in a
certain area are given below.
2.9
0.6
13.5
17.1
2.8
3.8
16.0
2.1
6.4
17.2
7.9
0.5
13.7
11.5
2.9
3.6
6.1
8.8
2.2
9.4
15.9
8.8
9.8
11.5
12.3
3.7
8.9
13.0
7.9
11.7
6.2
6.9
12.8
13.7
2.7
3.5
8.3
15.9
5.1
6.0
Table 1.5.1 Class and Frequency
Class
0-4
5-9
10-14
15-19
20-24
Frequency
5
14
15
10
6
36
CHAPTER 1 Descriptive Statistics

(a) Find the mean, variance, and range for these data.
(b) Find lower and upper quartiles, median, and interquartile range. Check
for any outliers.
(c) Construct a box plot.
(d) Construct a histogram and interpret.
(e) Locate on your histogram xs, x2s, and x3s: Count the data
points in each of the intervals x, xs, x2s, and x3s: How do
these counts compare with the empirical rule?
1.5.11. A random sample of 100 households’ weekly food expenditure represented
by x from a particular city gave the following statistics:
X
xi ¼ 11,000, and
X
xi
2 ¼ 1,900,000:
(a) Find the mean and standard deviation for these data.
(b) Assuming that the food expenditure of the households of an entire city
of 400,000 will have a bell-shaped distribution, how many households
of this city would you expect to fall in each of the intervals,
xs, x2s, and x3s?
1.5.12. The following numbers are the hours put in by 10 employees of company in
a randomly selected week:
40 46 40 54 18 45 34 60 39 42
(a) Calculate the values of the three quartiles and the interquartile range.
Also, calculate the mean and standard deviation and interpret.
(b) Verify for this data set that
X10
i¼1 xi x
ð
Þ ¼ 0:
(c) Construct a box plot.
(d) Does this data set contain any outliers?
1.5.13. For the following data:
6.3
2.9
4.5
1.1
1.8
4.0
1.2
3.1
2.0
4.0
7.0
2.8
4.3
5.3
2.9
8.3
4.4
2.8
3.1
5.6
4.5
4.5
5.7
0.5
6.2
3.7
0.9
2.4
3.0
3.5
(a) Find the mean, variance, and standard deviation.
(b) Construct a frequency table with five classes.
(c) Using the grouped data formula, find the mean, variance, and standard
deviation for the frequency table constructed in part (b) and compare it
to the results in part (a).
1.5.14. In order to assess the protective immunizing activity of various whooping
cough vaccines, suppose that 30 batches of different vaccines are tested on
groups of children. Suppose that the following data give immunity
percentage in home exposure values (IPHE values).
85
51
41
90
91
40
39
69
45
47
42
12
70
38
97
34
94
77
88
91
79
90
43
40
89
85
71
30
25
21
37
1.5 Numerical Description of Data

(a) Find the mean, variance, and standard deviation and interpret.
(b) Construct a frequency table with five classes.
(c) Using the grouped data formula, find the mean, variance, and
standard deviation for the table in part (b) and compare it to the results
in part (a).
1.5.15. The grouped data in Table 1.5.2 give the number of births by age group of
mothers between ages 10 and 39 in a certain state in 2000.
Find the median for this grouped data and interpret.
1.5.16. Table 1.5.3 gives the distribution of the masses (in grams) of 50 salmon
from a single young cohort.
(a) Using the grouped data formula, find the mean, variance, and standard
deviation
(b) Find the median for this grouped data.
1.5.17. After a pollution accident, 180 dead fish were recovered from a stream.
Table 1.5.4 gives their lengths measured to the nearest millimeter.
(a) Using the grouped data formula, find the mean, variance, and standard
deviation.
(b) Find the median for this grouped data and interpret.
Table 1.5.2 Number of Births by Mother’s Age Group
Age of Mother
Number of Births
10-14
895
15-19
55,373
20-24
122,591
25-29
139,615
30-34
127,502
35-39
68,685
Table 1.5.3 Distribution of Salmon Mass
Weight
155-164
165-174
175-184
185-194
195-204
Frequency
8
11
18
9
4
Table 1.5.4 Length of Dead Fishes
Length of Fish (mm)
1-19
20-39
40-59
60-79
80-99
Frequency
38
31
59
45
7
38
CHAPTER 1 Descriptive Statistics

1.6 COMPUTERS AND STATISTICS
With present-day technology, we can automate most statistical calculations. For
small sets of data, many basic calculations such as finding means and standard devi-
ations and creating simple charts, graphing calculators are sufficient. Students should
learn how to perform statistical analysis using their handheld calculators. For deeper
analysis and for large data sets, statistical software is necessary. Software also provides
easier data entry and editing and much better graphics in comparison to calculators.
There are many statistical packages available. Many such analyses can be performed
with spreadsheet application programs such as Microsoft Excel, but a more thorough
data analysis requires the use of more sophisticated software such as Minitab and
SPSS. For students with programming abilities, packages such as R and MATLAB
may be more appealing. For very large datasets and for complicated data analysis,
one could use SAS. SAS is one of the most frequently used statistical packages. Many
other statistical packages (such as Splus, and StatXact) are available; the utilities and
advantages of each are based on the specific application and personal taste. The soft-
ware R is free software that is being increasingly used by statisticians and can be down-
loaded from http://www.r-project.org/, and many statistical tutorial for R is freely
available in the worldwide web. For a good introduction to doing statistics with R, refer
to the book by Peter Dalgaard, Introductory Statistics, with R, Springer, 2002.
In this book, we will give some representative R, Minitab, SPSS, and SAS com-
mands at the end of each chapter just to get students started on the technology. These
examples are by no means a tutorial for the respective software. For a more thorough
understanding and use of technology, students should look at the users’ manual that
comes with the software or at references given at the end of the book. The computer
commands are designed to be illustrative, rather than completely efficient. In dealing
with data analysis for real-world problems, we need to know which statistical pro-
cedure to use, how to prepare the data sets suitable for use in the particular statistical
package, and finally how to interpret the results obtained. A good knowledge of the-
ory supplemented with a good working knowledge of statistical software will enable
students to perform sophisticated statistical analysis, while understanding the under-
lying assumptions and the limitations of results obtained. This will prevent us from
misleading conclusions when using computer-generated statistical outputs.
1.7 CHAPTER SUMMARY
In this chapter, we dealt with some basic aspects of descriptive statistics. First we
gave basic definitions of terms such as population and sample. Some sampling tech-
niques were discussed. We learned about some graphical presentations in
Section 1.4. In Section 1.5 we dealt with descriptive statistics, in which we learned
how to find mean, median, and variance and how to identify outliers. A brief discus-
sion of the technology and statistics was given in Section 1.6. All the examples given
in this chapter are for a univariate population, in which each measurement consists of
39
1.7 Chapter Summary

a single value. Many populations are multivariate, where measurements consist of
more than one value. For example, we may be interested in finding a relationship
between blood sugar level and age, or between body height and weight. These types
of problems will be discussed in Chapter 8.
In practice, it is always better to run descriptive statistics as a check on one’s data.
The graphical and numerical descriptive measures can be used to verify that the mea-
surements are sound and that there are no obvious errors due to collection or coding.
We now list some of the key definitions introduced in this chapter.
•
Population.
•
Sample.
•
Statistical inference.
•
Quantitative data.
•
Qualitative or categorical data.
•
Cross-sectional data.
•
Time series data.
•
Simple random sample.
•
Systematic sample.
•
Stratified sample.
•
Proportional stratified sampling.
•
Cluster sampling.
•
Multiphase sampling.
•
Relative frequency.
•
Cumulative relative frequency.
•
Bar graph.
•
Pie chart.
•
Histogram.
•
Sample mean.
•
Sample variance.
•
Sample standard deviation.
•
Median.
•
Interquartile range.
•
Mode.
•
Mean.
•
Empirical rule.
•
Box plots.
In this chapter, we have also introduced the following important concepts and
procedures:
•
General procedure for data collection.
•
Some advantages of simple random sampling.
•
Steps for selecting a stratified sample.
•
Procedures to construct frequency and relative frequency tables and graphical
representations such as stem-and-leaf displays, bar graphs, pie charts, histograms,
and box plots.
40
CHAPTER 1 Descriptive Statistics

•
Procedures to calculate measures of central tendency, such as mean and median,
as well as measures of dispersion such as the variance and standard deviation for
both ungrouped and grouped data.
•
Guidelines for the construction of frequency tables and histograms.
•
Procedures to construct a box plot.
1.8 COMPUTER EXAMPLES
In this section, we give some examples of how to use Minitab, SPSS, and SAS
for creating graphical representations of the data as well as methods for the com-
putation of basic statistics. Sometimes, the outputs obtained using a particular
software package may not be exactly as explained in the book; they vary from one
package to another, and also depend on the particular software version. In
fact, most of the outputs will not be shown in this book. It is important to obtain
the explanation of outputs from the help menu of the particular software package
for complete understanding. The “Computer Examples” sections of this book are
not designed as manuals for the software, nor are they written in the most efficient
way. The idea is only to introduce some basic procedures, so that the students
can get started with applying the theoretical material they have seen in each of the
chapters.
1.8.1 R INTRODUCTION AND EXAMPLES
R is a free software for statistical computing and graphics that you can download
from http://www.r-project.org/. Detailed help manuals are available from this site.
In addition, you can get R help from numerous sources. In this book, we are only
introducing the reader to basic R-programming as a starting point.
R you ready to start programming?
Introduction to R, imputing and importing data from the examples:
How to input data?
Using the following data:
66 74 79 80 69 77 78 65 79 81
we will make a single variable data set or vector named x. First manually, and
second using the scan() function for convenience.
R Code:
x¼c(66,74,79,80,69,77,78,65,79,81);
Typing the comma can be time consuming
OR
x¼scan();
1: 66
2: 74
3: 79
4: 80
This method allows you to type each number
pressing enter between each entry designed
with the number pad in mind. Notice the last
entry is blank which ends the scan function.
5: 69
6: 77
41
1.8 Computer Examples

7: 78
8: 65
9: 79
10: 81
11:
Results: Both methods obtain the same output which can be seen simply by typ-
ing x or cat(x) or print(x) however the scan method allows you type rapidly type your
numbers into the variable using a numpad and enter key.
Importing a CSV file
It is common to import CSV (comma separated value) files into R this imports
Example 7.7.1 data into variable x.
This example assumes your file is located on a D:\ drive you may need to modify
the path preceding the file name for the csv you wish to import.
R Code:
x¼read.csv(“D:\ch7_1.csv”);
Results:
You should have obtained a variable containing the data from the csv file, these
files can be opened with notepad to see their contents.
Exporting a CSV
It is common to export a CSV (comma separated value) file of data you wish to
save, backup, or share.
Using R we will export the following data:
Sample 1 (x) : 1 2 3 4 5 6 7 8 9 10
This example is writing to the path C:\Users\Admin\Documents please modify
the path to work on your computer.
R Code:
x¼c(1:10);
write.csv(x,“C:\Users\Admin\Documents\myfile.csv”);
Results: This should have created the specified file in the specified location, you
can open this file with notepad and should see the exported data.
Example 1.8.1 (Stem-and-leaf plot) Using the following data construct a stem-
and-leaf plot.
Sample X : 78 74 82 66 94 71 64 88 55 80 91 74 82 75 96 78 84 79 71 83
This assumes you have stored the data under variable x, please modify your code
appropriately.
R Code:
stem(x);
Output:
The decimal point is 1 digit(s) to the right of the j
5 j 5
6 j 46
7 j 11,445,889
8 j 022,348
9 j 146
42
CHAPTER 1 Descriptive Statistics

Example 1.8.2 (Histogram) Using the following data construct a histogram.
Sample X : 25 37 20 31 31 21 12 25 36 27 38 16 40 32 33 24 39 26 27 19
This assumes you have stored the data into variable x, please modify your code
appropriately.
R Code:
hist(x);
Output:
0.00
10
15
20
25
x
30
35
40
0.01
0.02
0.03
Density
0.04
0.05
Histogram of x
Example 1.8.3 (Descriptive Statistics) Using the following data generate
descriptive statistics.
Sample X : 5 7 229 453 12 14 18 14 18 14 14 483 22 21 25 23 24 34 37 34 49 64 47
67 69 192 125
This assumes you have stored the data into variable x, please modify your code
appropriately.
R Code:
Standard Deviation
sd(x);
Standard deviation
length(x);
Length of variable
Output:
Min. 1st Qu. Median Mean 3rd Qu. Max.
5.00 18.00
34.00
83.28 67.00
483.00
128.3649
Standard deviation
25
Length of variable
Example 1.8.4 (Box Plot) Using the following data create a box plot.
43
1.8 Computer Examples

Sample X : 870 922 1146 1120 1079 905 888 865 1112 966 1150 977 958 1088
1139 1055 1082
1053 1048 1118 866 996 1102 1028 1130 1002 990 1052 1116 1109
This assumes you have stored the data into variable x, please adjust your code
appropriately.
R Code:
boxplot(x);
Output:
1100
1000
950
900
Example 1.8.5 (Test of Randomness) Using the following data test weather or
not the sample is random (details of this test are left undisclosed):
Sample X : 24 31 28 43 28 56 48 39 52 32 38 49 51 49 62 33 41 58 63 56
This test is known as “Runs test” and assumes you have stored the data into var-
iable x, please modify your code appropriately. Additionally you will need to install
the “lawstat” package to use this test.
R Code:
install.packages(‘lawstat’);
Installs and loads the required package
library(‘lawstat’);
runs.test(x);
Output:
Runs Test - Two sided
data: x
Standardized Runs Statistic¼1.3784, p-value¼0.1681
1.8.2 MINITAB EXAMPLES
A good place to get help on Minitab is http://www.minitab.com/resources/.
There are many nice sites available on Minitab procedures; for example, Minitab
student tutorials can be obtained from http://www.minitab.com/resources/tuto
rials/. Here we illustrate only some of the basic uses of Minitab. In Minitab,
44
CHAPTER 1 Descriptive Statistics

we can enter the data in the spreadsheet and use the Windows pull-down menus,
or we can directly enter the data and commands. We will mostly give procedures
for the pull-down menus only. It is up to the user’s taste to choose among these
procedures. It should be noted that with different versions of Minitab, there will
be some differences in the pull-down menu options. It is better to consult the
Help menu for the actual procedure. Most of the time, we will not give the
output.
EXAMPLE 1.8.1 (STEM-AND-LEAF):
For the following data, construct a stem-and-leaf display using Minitab:
78
74
82
66
94
71
64
88
55
80
91
74
82
75
96
78
84
79
71
83
Solution
For the pull-down menu, first enter the data in column 1. Then follow the following sequence. The
boldface represents the actions.
Graph>Character Graphs>Stem-and-Leaf
In Variables: type C1 and click OK
EXAMPLE 1.8.2 (HISTOGRAM):
For the following data, construct a histogram:
78
74
82
66
94
71
64
88
55
80
91
74
82
75
96
78
84
79
71
83
Solution
Enter the data in C1, then use the following sequence
Graph>Histogram. . . > in Graph variables: type C1>OK
If we want to change the number of intervals, after entering Graph variables, click Options. . .
and click Number of intervals and enter the desired number, then OK.
EXAMPLE 1.8.3 (DESCRIPTIVE STATISTICS):
In this example, we will describe how to obtain basic statistics such as mean, median, and standard
deviation for the following data:
78
74
82
66
94
71
64
88
55
80
91
74
82
75
96
78
84
79
71
83
Solution
Enter the data in C1. Then use
Stat>Basic Statistics>Display Descriptive Statistics. . . > in Variables: type C1>click OK
45
1.8 Computer Examples

EXAMPLE 1.8.4 (SORTING AND BOX PLOT):
For the following data, first sort in the increasing order and then construct a box plot to check for
outliers.
78
74
82
66
94
71
64
88
55
80
91
74
82
75
96
78
84
79
71
83
Solution
After entering the data in C1, we can sort the data in increasing order as follows:
Manip>Sort. . . > in Sort column(s): type C1>in Store sorted column(s) in: type C2>in Sorted
by column: type C1>OK
If we want to draw a box plot for the data, do the following:
Graph>Box plot. . . > in Graph variables: under Y, type C1>OK
EXAMPLE 1.8.5 (TEST OF RANDOMNESS):
Almost all of the analyses in this book assume that the sample is random. How can we verify whether
the sample is really random? Project 12B explains a procedure called run test. Without going into
details, this test is simple with Minitab. All we have to do is enter the data in C1. Then click
Stat>Nonparametric>Runs Test. . . > in variables: enter C1>OK
For instance, if we have the following data:
78
74
82
66
94
71
64
88
55
80
91
74
82
75
96
78
84
79
71
83
we will get following output:
Run Test
C1
K¼44.0500
The observed number of runs¼14
The expected number of runs¼11.0000
10 Observations above K 10 below
* N Small – The following approximation may be invalid
The test is significant at 0.1681
Cannot reject at alpha¼0.05
“Cannot reject” in the output means that it is reasonable to assume that the sample is random.
For any data, it is always desirable to do a run test to determine the randomness.
1.8.3 SPSS EXAMPLES
For SPSS, we will give only Windows commands. For all the pull-down menus, the
sequence will be separated by the>symbol.
46
CHAPTER 1 Descriptive Statistics

EXAMPLE 1.8.6
Redo Example 1.8.1 with SPSS.
Solution
After entering the data in C1,
Analyze>Descriptive Statistics>Explore. . . >
At the Explore window select the variable and move to Dependent List; then click Plots. . ., select
Stem-and-Leaf, click Continue, and click OK at the Explore Window
We will get the output with a few other things, including box plots along with the stem-and-leaf dis-
play, which we will not show here.
EXAMPLE 1.8.7
Redo Example 1.8.2 with SPSS.
Solution
After entering the data:
Graphs>Histogram. . . >
At the Histogram window select the variable and move to Variable, and click OK
We will get the histogram, which we will not display here.
EXAMPLE 1.8.8
Redo Example 1.8.3 with SPSS.
Solution
Enter the data. Then:
Analyze>Descriptive Statistics>Frequencies. . . >
At the Frequencies window select the variable(s); then open the Statistics window and check
whichever
boxes
you
desire
under
Percentile,
Dispersion,
Central
Tendency,
and
Distribution>continue>OK
For example, if you select Mean, Median, Mode, Standard Deviation, and Variance, we will get the
following output and more:
Statistics
VAR00001
N
Valid
25
Missing
0
Mean
83.2800
Median
34.0000
Mode
14.00
Std. Deviation
128.36488
Variance
16,477.54333
47
1.8 Computer Examples

1.8.4 SAS EXAMPLES
We will now give some SAS procedures describing the numerical measures of a sin-
gle variable. PROC UNIVARIATE will give mean, median, mode, standard devi-
ation, skewness, kurtosis, etc. If we do not need median, mode, and so on, we could
just as well use PROC MEANS in lieu of PROC UNIVARIATE. We can use the
following general format in writing SAS programs with appropriate problem-
specific modifications. There are many good online references as well as books
available for SAS procedures. To get support on SAS, including many example
codes, refer to the SAS support Web site: http://support.sas.com/. Another helpful
site can be found at http://www.ats.ucla.edu/stat/sas/. There are many other sites that
may suit your particular application.
GENERAL FORMAT OF AN SAS PROGRAM
DATA give a name to the data set;
INPUT here we put variable names and column locations, if there are more than one variable;
CARDS; (also we can use DATALINES;)
Enter the data here;
TITLE ‘here we include the title of our analysis’;
PROC PRINT;
PROC name of procedure (such as PROC UNIVARIATE) goes here;
Options that we may want to include (such as the variables
to be used) go here;
RUN;
After writing an SAS program, to execute it we can go to the menu bar and select
run>submit, or click the “running man” icon. On execution, SAS will output the
results to the Output window. All the steps used including time of execution and any
error messages will be given in the Log window.
In order to make the SAS outputs more manageable, we can use the following
SAS command at the beginning of an SAS program:
options ls¼80 ps¼50;
ls stands for line size, and this sets each line to be 80 characters wide. ps stands for
page size and allows 50 lines on each page. This reduces the number of unnecessary
page breaks. In order to avoid date and number, we can use the option commands:
Options nodate nonumber;
EXAMPLE 1.8.9
For the data of Example 1.8.3, use PROC UNIVARIATE to summarize the data.
Solution
In the program editor window, type the following if you are entering the data directly. If you are
using the data stored in a file, the comment line (with *) should be used instead of the input and
data lines.
48
CHAPTER 1 Descriptive Statistics

Options nodate nonumber;
DATA e9;
INPUT e9 @@;
DATALINES;
5 7 229 453 12 14 18 14 14 483
22 21 25 23 24 34 37 34 49 64
47 67 69 192 125;
PROC UNIVARIATE;
TITLE;
RUN;
In this case we will get the following output:
The UNIVARIATE Procedure
Variable: ex9
Moments
N
25
Sum Weights
25
Mean
83.28
Sum Observations
2082
Std Deviation
128.364884
Variance
16,477.5433
Skewness
2.45719194
Kurtosis
5.47138396
Uncorrected SS
568,850
Corrected SS
395,461.04
Coeff Variation
154.136508
Std Error Mean
25.6729767
Basic Statistical Measures
Location
Variability
Mean
83.28000
Std Deviation
128.36488
Median
34.00000
Variance
16,478
Mode
14.00000
Range
478.00000
Interquartile Range
49.00000
Tests for Location: Mu0¼0
Test
-Statistic-
-p
Value-
Student’s t t
3.243878
Pr>jtj
0.0035
Sign
M
12.5
Pr>¼ jMj
<0.0001
Signed Rank S
162.5
Pr>¼ jSj
<0.0001
Quartiles (Definition 5)
Quartile
Estimate
100% Max
483
99%
483
95%
453
90%
229
75% Q3
67
50% Median
34
25% Q1
18
10%
12
5%
7
1%
5
0% Min
5
The UNIVARIATE Procedure
Variable: ex9
Extreme Observations
-Lowest-
-Highest-
Value
Obs
Value
Obs
5
1
125
25
7
2
192
24
12
5
229
3
Continued
49
1.8 Computer Examples

14
9
453
4
14
8
483
10
We can observe from the previous output that PROC UNIVARIATE gives much information
about the data, such as mean, standard deviation, and quartiles. If we do not want all these details,
we could use the PROC MEANS command. In the previous code, if we replace PROC UNIVAR-
IATE by the PROC MEANS statement, we will get the following:
The MEANS Procedure
Analysis Variable : ex9
N
Mean
Std Dev
Minimum Maximum
—————————————————
25 83.2800000 128.3648836 5.0000000 483.0000000
—————————————————
The output is greatly simplified.
If we use PROC UNIVARIATE PLOT NORMAL; this option will produce three plots: stem-
and-leaf, box plot, and normal probability plot (this will be discussed later in the text). In order to
obtain bar graphs at the midpoints of the class intervals, use the following commands:
PROC CHART DATA¼e9;
VBAR e9;
If we want to create a frequency table, use the following:
PROC FREQ;
table ex9;
title ‘Frequency tabulation’;
Every PROC or procedure has its own name and options. We will use different
PROCs as we need them. Always remember to enclose titles in single quotes. There
are various other actions that we can perform for the data analysis using SAS. It is
beyond the scope of this book to explain general and efficient SAS codes. For details,
we refer to books dedicated to SAS, such as the book by Ronald P. Cody and Jeffrey
K. Smith, Applied Statistics and the SAS Programming Language, 5th Edition, Pren-
tice Hall, 2006. There are many Web sites that give SAS codes. One example with
references for many aspects of SAS, including many codes, can be found at http://
www.sas.com/service/library/onlinedoc/code.samples.html.
EXERCISES 1.8
1.8.1. The following data represent the lengths (to the nearest whole millimeter) of
80 shoots from seeds of a certain type planted at the same time.
75
72
76
76
72
74
71
75
77
72
74
71
76
76
76
72
71
73
73
71
72
72
75
70
74
74
78
74
76
79
75
76
73
73
71
72
79
74
77
72
76
70
72
75
78
72
69
75
72
71
77
79
76
73
75
73
72
75
74
78
73
77
73
77
70
74
66
74
73
77
75
79
75
70
72
73
80
73
78
75
50
CHAPTER 1 Descriptive Statistics

Using one of the software packages (R, Minitab, SPSS, or SAS):
(a) Represent the data in a histogram.
(b) Find the summary statistics such as mean, median, variance, and
standard deviation.
(c) Draw box plots and identify any outliers.
1.8.2. On a particular day, asked, “How many minutes did you exercise today?” the
following were the responses of 30 randomly selected people:
15
30
25
10
30
15
10
45
20
22
18
0
45
12
15
10
17
30
30
15
10
30
20
8
18
30
27
33
15
0
Using one of the software packages (R, Minitab, SPSS, or SAS):
(a) Represent the data in a histogram.
(b) Find the summary statistics such as mean, median, variance, and
standard deviation.
(c) Draw box plots and identify any outliers.
PROJECTS FOR CHAPTER 1
1A. WORLD WIDE WEB AND DATA COLLECTION
Statistical Abstracts of the United States is a rich source of statistical data (http://
www.census.gov/prod/www/statistical-abstract-us.html).
Pick
any
category
of
interest to you and obtain data (say, Income, Expenditures, and Wealth). Represent
a section of the data graphically. Find mean, median, and standard deviation. Iden-
tify any outliers. There are many other sites, such as http://lib.stat.cmu.edu/datasets/
and http://it.stlawu.edu/rlock/datasurf.html, that we can use for obtaining real
data sets.
1B. PREPARING A LIST OF USEFUL INTERNET SITES
Prepare a list of Internet references for various aspects of statistical study.
1C. DOT PLOTS AND DESCRIPTIVE STATISTICS
From the local advertisements of apartments for rent, randomly pick 50 monthly
rents for two-bedroom apartments. For these data, first draw a dot plot and then
obtain descriptive statistics (use R, Minitab, SPSS, or SAS, or any other statistical
software).
51
Projects for Chapter 1

1D. IMPORTANCE OF STATISTICS IN OUR SOCIETY
Write a short report on the importance of statistics in our modern day society. Give
different examples to illustrate your points.
1E. USES AND MISUSES OF STATISTICS
“There are three types of lies—lies, damn lies, and statistics” Benjamin Disraeli-
Write a short report on uses and misuses of statistics.
52
CHAPTER 1 Descriptive Statistics

CHAPTER
Basic Concepts from
Probability Theory
2
CHAPTER CONTENTS
2.1 Introduction ...................................................................................................... 54
2.2 Random Events and Probability .......................................................................... 55
2.3 Counting Techniques and Calculation of Probabilities ......................................... 63
2.4 The Conditional Probability, Independence, and Bayes’ Rule ................................ 70
2.5 Random Variables and Probability Distributions .................................................. 82
2.6 Moments and Moment-Generating Functions ....................................................... 91
2.7 Chapter Summary ............................................................................................ 104
2.8 Computer Examples (Optional) ......................................................................... 105
Projects for Chapter 2 ............................................................................................ 108
OBJECTIVE
In this chapter we will review some results from probability theory that are essential
for the development of the statistical results of this book.
Andrei Nikolaevich Kolmogorov
(Source: http://www.scholarpedia.org/article/Andrey_Nikolaevich_Kolmogorov; https://www.google.com/sea
rch?q=Andrey+Nikolaevich+Kolmogorov&rlz=2T4WQIA_enUS0537US0537&source=lnms&tbm=isch&sa=X&
ei=QKK9U8j6EoWkyASA7ICAAQ&ved=0CAgQ_AUoAQ&biw=1261&bih=838)
Mathematical Statistics with Applications in R
Copyright © 2015 Elsevier Inc. All rights reserved.
53

Andrei Kolmogorov (1903-1987) laid the mathematical foundations of probabil-
ity theory and the theory of randomness. His monograph Grundbegriffe der
Wahrscheinlichkeitsrechnung, published in 1933, introduced probability theory in
a rigorous way from fundamental axioms. He later used probability theory to study
the motion of the planets and the turbulent flow of air from a jet engine. He also made
important contributions to stochastic processes, information theory, statistical
mechanics, and nonlinear dynamics. Kolmogorov had numerous interests outside
mathematics. In particular, he was interested in the form and structure of the poetry
of the Russian author Pushkin.
2.1 INTRODUCTION
Probability theory provides a mathematical model for the study of randomness and
uncertainty. The concept of probability occupies an important role in the decision-
making process, whether the problem is one faced in business, in engineering, in gov-
ernment, in sciences, or just in one’s own everyday life. Most decisions are made in
the face of uncertainty. The mathematical models of probability theory enable us to
make predictions about certain mass phenomena from the necessarily incomplete
information derived from sampling techniques. It is the probability theory that
enables one to proceed from descriptive statistics to inferential statistics. In fact,
probability theory is the most important tool in statistical inference.
The origin of probability theory can be traced to modeling of games of chances
such as dealing from a deck of cards, or spinning a roulette wheel. The earliest results
on probability arose from the collaboration of the eminent mathematicians Blaise
Pascal and Pierre Fermant and a gambler, Chevalier de Me´re´. They were interested
in what seemed to be contradictions between mathematical calculations and actual
games of chance, such as throwing dice, tossing coin, or spinning a roulette wheel.
For example, in repeated throws of a die, it was observed that each number,
1-6, appeared with a frequency of approximately 1/6. However, if two dice are rolled,
the sum of numbers showing on two dice, that is, 2-12, did not appear equally often.
It was then recognized that, as the number of throws increased, the frequency of these
possible results could be predicted by following some simple rules. Similar basic
experiments were conducted using other games of chance, which resulted in the
establishment of various basic rules of probability. Probability theory was developed
solely to be applied to games of chance until the eighteenth century, when Pierre
Laplace and Karl F. Gauss applied the basic probabilistic rules to other physical
problems. Modern probability theory owes much to the 1933 publication Founda-
tions of Theory of Probability by the Russian mathematician Andrei N. Kolmogorov.
He developed the probability theory from an axiomatic point of view.
Our objective in this chapter is to provide only a brief review of various defini-
tions and facts from probability that are needed elsewhere in the text. Proofs are
omitted in most cases. Many books are devoted solely to the study of probability
theory and we refer to them for further details and deeper understanding.
54
CHAPTER 2 Basic Concepts from Probability Theory

2.2 RANDOM EVENTS AND PROBABILITY
Any process whose outcome is not known in advance but is random is termed an exper-
iment. The term experiment is used here in a wider sense than the usual notion of a con-
trolled laboratory testing situation. Thus an experiment may include observing whether
a fuse is defective or not, or the duration of time from start to end of rain in a particular
place. Assume that the experiment can be repeated any number of times under identical
conditions. Each repetition is called a trial. A (random) experiment satisfies the follow-
ing three conditions: (1) the set of all possible outcomes are known in advance in each
trial; (2) in any particular trial, it is not known which particular outcome will happen;
and (3) the experiment can be repeated under identical conditions. We will now sum-
marize some notations and concepts for our study of probability.
BASIC DEFINITIONS
1. The sample space associated with an experiment is the set consisting of all possible outcomes
and is called the sure event in the experiment. A sample space is also referred to as a probability
space. A sample space will be denoted by S.
2. An outcome in S is also called a sample point. An event A is a subset of outcomes in S, that is,
AS. We say that an event A occurs if the outcome of the experiment is in A.
3. The null subset Ø of S is called an impossible event.
4. The event A[B consists of all outcomes that are in A or in B or in both.
5. The event A\B consists of all outcomes that are both in A and B.
6. The event Ac (the complement of A in S) consists of all outcomes not in A, but in S.
Using these concepts, we can define the following. All events are considered to be
subsets of S. For some more concepts from set theory, we refer to Appendix A.
Definition 2.2.1 Two events A and B are said to be mutually exclusive or
disjoint if A\B¼∅. Mutually exclusive events cannot happen together.
The mathematical definition of probability has changed from its earliest formu-
lation as a measure of belief to the modern approach of defining through the axioms.
We shall discuss four definitions of probability. We now give an informal definition
of probability.
INFORMAL DEFINITION OF PROBABILITY
Definition 2.2.2 The probability of an event is a measure (number) of the chance with
which we can expect the event to occur. We assign a number between 0 and 1 inclusive to the prob-
ability of an event. A probability of 1 means that we are 100% sure of the occurrence of an event, and
a probability of 0 means that we are 100% sure of the nonoccurrence of the event. The probability of
any event A in the sample space S is denoted by P(A).
From this definition, we can see that P(S)¼1. The earliest approach to measuring
uncertainty (in chance events) is the classical probability concept, which applies
55
2.2 Random Events and Probability

when all possible outcomes are equally likely or when the probabilities of outcomes
are known.
CLASSICAL DEFINITION OF PROBABILITY
Definition 2.2.3 If there are n equally likely possibilities, of which one must occur, and m of
these are regarded as favorable to an event, or as “success,” then the probability of the event or a
“success” is given by m/n.
Now we give steps that can be used to compute the probabilities of events using
this classical approach.
METHOD OF COMPUTING PROBABILITY BY THE CLASSICAL APPROACH
A. When all outcomes are equally likely
1. Count the number of outcomes in the sample space; say this is n.
2. Count the number of outcomes in the event of interest, A, and say this is m.
3. P(A)¼m/n.
B. When all outcomes are not equally likely
1. Let O1, O2, . . ., On be the outcomes of the sample space S. Let P(Oi)¼pi, i¼1, 2, . . ., n. In this
case, the probability of each outcome, pi, is assumed to be known.
2. List all the outcomes in A, say, Oi, Oj, . . ., Om.
3. P(A)¼P(Oi)+P(Oj)++P(Om)¼pi+pj++pm, the sum of the probabilities of the
outcomes in A.
EXAMPLE 2.2.1
A balanced die (with all outcomes equally likely) is rolled. Let A be the event that an even number
occurs. Then there are three favorable outcomes (2, 4, 6) in A, and the sample space has six elements,
{1, 2, 3, 4, 5, 6}. Hence P(A)¼3/6¼1/2.
EXAMPLE 2.2.2
Suppose we toss two coins. Assume that all the outcomes are equally likely (fair coins).
(a) What is the sample space?
(b) Let A be the event that at least one of the coins shows up heads. Find P(A).
(c) What will be the sample space if we know that at least one of the coins showed up heads?
Solution
(a) The sample space consists of four outcomes, namely S¼{(H, H), (H, T), (T, H), (T, T)}.
(b) The event A has three outcomes, (H, H), (H, T), and (T, H). Therefore P(A)¼3/4.
(c) Since we know that at least one of the coins showed up heads, the possible outcomes
are (H, H), (H, T), and (T, H). The sample space now has only three outcomes {(H, H),
(H, T), (T, H)}.
56
CHAPTER 2 Basic Concepts from Probability Theory

The classical probability concept is not applicable in situations where the var-
ious possibilities cannot be regarded as equally likely. Suppose we are interested
in whether or not it will rain on a given day with known meteorological condi-
tions. Clearly we cannot assume that the events of rain or no rain are equally
likely. In such cases, one could use the so-called frequency interpretation of prob-
ability. The frequentistic view is a natural extension of the classical view of
probability. This definition was developed as the result of work by R. von Mises
in 1936.
FREQUENCY DEFINITION OF PROBABILITY
Definition 2.2.4 The probability of an outcome (event) is the proportion of times the out-
come (event) would occur in a long run of repeated experiments.
For example, to find the probability of heads, H, using a biased coin, we would
imagine the coin is repeatedly tossed. Let n(H) be the number of times H appears in n
trials. Then the probability of heads is defined as P(H)¼limn!1(n(H)/n).
The frequency interpretation of probability is often useful. However, it is not
complete. Because of the condition of repetition under identical circumstances,
the frequency definition of probability is not applicable to every event. For a more
complete picture, it makes sense to develop the probability theory through axioms.
Now we will define probabilities axiomatically. This definition results from the 1933
studies of A.N. Kolmogorov.
AXIOMATIC DEFINITION OF PROBABILITY
Definition 2.2.5 Let S be a sample space of an experiment. Probability P(.) is a real-valued
function that assigns to each event A in the sample space S a number P(A), called the probability of
A, with the following conditions satisfied:
1. It is nonnegative, P(A)0.
2. It is unity for a certain event. That is, P(S)¼1.
3. It is additive over the union of an infinite number of pairwise disjoint events, that is, if A1, A2,. . .
form a sequence of pairwise mutually exclusive events (i.e. Ai\Aj¼∅, for i6¼¼j) in S, then
P([i¼1
1 Ai)¼P
i¼1
1 P(Ai).
From the previous three axioms, it can be shown that P(∅)¼0, and if A1, A2,. . .
form
a
sequence
of
pairwise
mutually
exclusive
events
in
S,
then
P([i¼1
n Ai)¼P
i¼1
n P(Ai) for a finite n. Also we could verify that 0P(A)1, for
any event A. It is important to observe that the axioms do not tell us how to assign
probabilities to events.
57
2.2 Random Events and Probability

EXAMPLE 2.2.3
A die is loaded (not all outcomes are equally likely) such that the probability that the number i shows
up is Ki, i¼1,2,. . .,6, where K is a constant. Find
(a) The value of K.
(b) The probability that a number greater than 3 shows up.
Solution
(a) Here the sample space S has six outcomes {1, 2, . . ., 6}. Hence, using axioms (2) and (3) we have
P 1
ð Þ + P 2
ð Þ +  + P 6
ð Þ ¼ 1:
Since P(i)¼Ki, we have
K
ð Þ 1
ð Þ + K
ð Þ 2
ð Þ +  + K
ð Þ 6
ð Þ ¼ 1 or
K
ð Þ 1 + 2 +  + 6
ð
Þ ¼ K
ð Þ 21
ð
Þ ¼ 1:
Hence K¼1/21.
The probability of, say, the number 5 showing up is 5/21.
(b) Let A be the event that a number greater than 3 shows up. Then the outcomes in A are {4, 5, 6}
and they are mutually exclusive. Therefore,
P A
ð Þ ¼ P 4
ð Þ + P 5
ð Þ + P 6
ð Þ
¼ 4
21 + 5
21 + 6
21 ¼ 15
21:
The following properties help us in going beyond the axioms to actually compute
various probabilities.
SOME BASIC PROPERTIES OF PROBABILITY
For two events A and B in S, we have the following:
1. P(Ac)¼1P(A), where Ac is the complement of the set A in S.
2. If AB, then P(A)P(B).
3. P(A[B)¼P(A)+P(B)P(A\B).
In particular, if A\B¼∅, then P(A[B)¼P(A)+P(B).
EXAMPLE 2.2.4
In a large university, the freshman profile for one year’s fall admission says that 40% of the students
were in the top 10% of their high school class, and that 65% are white, of whom 25% were in the top
10% of their high school class. What is the probability that a freshman student selected randomly
from this class either was in the top 10% of his or her high school class or is white?
Solution
Let E1 be the event that a person chosen at random was in the top 10% of his or her high school class,
and let E2 be the event that the student is white. We are given P(E1)¼0.40, P(E2)¼0.65, and P-
(E1\E2)¼0.25. Then the event that the student chosen is white or was in the top 10% of his or
her high school class is represented by E1[E2. Thus
P E1 [E2
ð
Þ ¼ P E1
ð
Þ + P E2
ð
ÞP E1 \E2
ð
Þ
¼ 0:40 + 0:650:25 ¼ 0:80:
58
CHAPTER 2 Basic Concepts from Probability Theory

EXAMPLE 2.2.5
A subway station in a large city has 12 gates, six inbound (entering into the subway station)
and six outbound (exiting the subway station). The number of gates open in each direction is
observed at a particular time of day. Assume that each outcome of the sample space is equally likely.
(a) Define a suitable sample space.
(b) What is the probability that at most one gate is open in each direction?
(c) What is the probability that at least one gate is open in each direction?
(d) What is the probability that the number of gates open is the same in both directions?
(e) What is the probability of the event that the total number of gates open is six?
Solution
(a) We define the sample space to be the set of ordered pairs (x, y), where x is the number of inbound
gates open and y is the number of outbound gates open. For example, (4, 5) means four gates for
inbound and five gates for outbound are open. (1, 0) means one gate is open in the inbound
direction and no gate is open in the outbound direction. Figure 2.1 represents the situation
S ¼
0, 0
ð
Þ 0,1
ð
Þ 0,2
ð
Þ 0,3
ð
Þ 0,4
ð
Þ 0,5
ð
Þ 0,6
ð
Þ
1, 0
ð
Þ 1,1
ð
Þ 1,2
ð
Þ 1,3
ð
Þ 1,4
ð
Þ 1,5
ð
Þ 1,6
ð
Þ
2, 0
ð
Þ 2,1
ð
Þ 2,2
ð
Þ 2,3
ð
Þ 2,4
ð
Þ 2,5
ð
Þ 2,6
ð
Þ
3, 0
ð
Þ 3,1
ð
Þ 3,2
ð
Þ 3,3
ð
Þ 3,4
ð
Þ 3,5
ð
Þ 3,6
ð
Þ
4, 0
ð
Þ 4,1
ð
Þ 4,2
ð
Þ 4,3
ð
Þ 4,4
ð
Þ 4,5
ð
Þ 4,6
ð
Þ
5, 0
ð
Þ 5,1
ð
Þ 5,2
ð
Þ 5,3
ð
Þ 5,4
ð
Þ 5,5
ð
Þ 5,6
ð
Þ
6, 0
ð
Þ 6,1
ð
Þ 6,2
ð
Þ 6,3
ð
Þ 6,4
ð
Þ 6,5
ð
Þ 6,6
ð
Þ
8
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
:
9
>
>
>
>
>
>
>
>
=
>
>
>
>
>
>
>
>
;
:
We see that the sample space has 49 possible outcomes. We assume that these outcomes are
equally likely.
(b) Suppose that A is the event that at most one gate is open in each direction. Then
A ¼
0, 0
ð
Þ, 0, 1
ð
Þ, 1, 0
ð
Þ, 1, 1
ð
Þ
f
g:
Hence,
P A
ð Þ ¼ 4
49 ¼ 0:082
(c) Let B be the event that at least one gate is open in each direction. Then B contains 36 elements.
Hence,
P B
ð Þ ¼ 36
49 ¼ 0:7347:
Continued
1
2
3
4
5
6
12
3
45
6
FIGURE 2.1
Inbound and outbound traffic.
59
2.2 Random Events and Probability

(d) Let
C¼ event thatnumberof opengatesis thesame bothways
¼
0, 0
ð
Þ, 1, 1
ð
Þ, 2, 2
ð
Þ, 3, 3
ð
Þ, 4, 4
ð
Þ, 5, 5
ð
Þ, 6, 6
ð
Þ
f
g:
Then P C
ð Þ ¼ 7
49 ¼ 0:1428:
(e) Let
D ¼ theevent thatthe totalnumberof gatesopenissix
¼
3, 3
ð
Þ, 2, 4
ð
Þ, 4, 2
ð
Þ, 5, 1
ð
Þ, 1, 5
ð
Þ, 6, 0
ð
Þ, 0, 6
ð
Þ
f
g:
Hence, P(D)¼7/49.
EXERCISES 2.2
2.2.1. Consider an experiment in which each of three cars exiting from a
university main entrance turns right (R) or left (L). Assume that a car will
turn right or left with equal probability of 1/2.
(a) What is the sample space S?
(b) What is the probability that at least one car will turn left?
(c) What is the probability that at most one car will turn left?
(d) What is the probability that exactly two cars will turn left?
(e) What is the probability that all three cars will turn in the same
direction?
2.2.2. A coin is tossed three times. Define an appropriate sample space for the
following cases:
(a) The outcome of each individual toss is of interest.
(b) Only the number of trials is of interest.
2.2.3. Pair of six-sided balanced dice are rolled. What are the probabilities of
getting the sum of the face values as follows?
(a) 8
(b) 6 or 9
(c) 3, 8, or 12
(d) Not an even number
2.2.4. An experiment has four possible outcomes A, B, C, and D. Check whether
the following assignments of probability are possible:
(a) P(A)¼0.20, P(B)¼0.40, P(C)¼0.09, P(D)¼0.31.
(b) P(A)¼0.41, P(B)¼0.17, P(C)¼0.12, P(D)¼0.36.
(c) P(A)¼1/8, P(B)¼1/2, P(C)¼1/4, P(D)¼1/8.
2.2.5. Suppose we toss two coins and suppose that each of the four points in the
sample space S¼{(H, H), (H, T), (T, H), (T, T)} is equally likely. Let the
events be A¼{(H, H), (H, T)} and B¼{(H, H), (T, H)}. Find P(A[B).
2.2.6. An urn contains 12 white, 5 yellow, and 13 black marbles. A marble is
chosen at random from the urn, and it is noted that it is not one of the black
marbles. What is the sample space in view of this knowledge? What is the
probability that it is yellow?
60
CHAPTER 2 Basic Concepts from Probability Theory

2.2.7. Two fair dice are rolled and face values are noted.
(a) What is the probability space?
(b) What is the probability that the sum of the numbers showing is 7?
(c) What is the probability that both dice show number 2?
2.2.8. In a city, 65% of people drink coffee, 50% drink tea, and 25% both. What is
the probability that a person chosen at random will drink at least one of
coffee or tea? Will drink neither?
2.2.9. In a fruit basket, there are five mangos, of which two are spoiled. If we were
to randomly pick two mangos:
(a) What would be our sample space?
(b) What is the probability that both mangos are good?
(c) What is the probability that no more than one mango is spoiled?
2.2.10. In a box there are three slips of paper, with one of the letters A, C, T written
on each slip. If the slips are drawn out of the box one at a time, what is the
probability of obtaining the word CAT?
2.2.11. Suppose that the genetic makeup of the population of a city is as in
Table 2.2.1.
An individual is considered to have the dominant characteristic if the
person has the AA or Aa genetic trait. If we were to choose an individual
from this city at random, what is the probability that this person has the
dominant characteristic?
2.2.12. Using the axioms of probability, show that P(∅)¼0, and if A1, . . ., An are
pairwise mutually exclusive, then P([i¼1
n Ai)¼P
i¼1
n P(Ai)
2.2.13. Using the axioms of probability, prove the following:
(a) If AB, then P(A)P(B).
(b) P(A[B)¼P(A)+P(B)P(A\B). In particular, if A\B¼∅, then
P(A[B)¼P(A)+P(B).
2.2.14. Using the axioms of probability, show that
P A[B[C
ð
Þ¼ P A
ð Þ + P B
ð Þ + P C
ð ÞP A\B
ð
ÞP A\C
ð
Þ
P B\C
ð
Þ + P A\B\C
ð
Þ:
2.2.15. Prove that
(a) P(A\B)P(A)+P(B)1
(b) P([i¼1
2 Ai)P
i¼1
2 P(Ai)
2.2.16. If A and B are mutually exclusive events, P(A)¼0.17 and P(B)¼0.46, find
(a) P(A[B)
(b) P(Ac)
(c) P(Ac[Bc)
Table 2.2.1 Genetic Makeup of a Population
Genetic Makeup
AA
Aa
aa
Probability
p
2q
r
61
2.2 Random Events and Probability

(d) P((A\B)c)
(e) P(Ac\Bc)
2.2.17. If P(A)¼0.24, P(B)¼0.67, and P(A\B)¼0.09, find
(a) P(A[B)
(b) P((A[B)c)
(c) P(Ac[Bc)
(d) P((A\B)c)
(e) P(Ac\Bc)
2.2.18. In a series of seven games, the first team to win four games wins the series.
If the teams are evenly matched, what is the probability that the team that
wins the first game will win the series?
2.2.19. In a survey, 1000 adults were asked if they would approve an increase in tax
if the revenues went to build a football stadium. It was also noted whether
the person lived in a city (C), suburb (S), or rural area (R), of the county. The
results are summarized in Table 2.2.2.
Define the following events:
A: person chosen is from the city
B: person disapproves tax increase
Find the following probabilities;
(i) P(B),
(ii) P(Ac\B),
(iii) P(A[Bc)
2.2.20. A couple has two children. Suppose we know the elder child is a boy.
(a) Determine an appropriate sample space.
(b) Find the probability that both are boys.
2.2.21. A box contains three red and two blue flies. Two flies are removed with
replacement. Let A be the event that both the flies are of the same color and
B be the event that at least one of the flies is red. Find (i) P(A), (ii) P(B), (iii)
P(A[B), and (iv) P(A\B).
2.2.22. Prove that for any n,
P
[
n
i¼1Ai


¼
X
n
i¼1
P Ai
ð
Þ
X
i1<i2
P Ai1 \Ai2
ð
Þ + 
+ 1
ð
Þm + 1
X
i1<i2<...<im
P Ai1 \Ai2 \...\Aim
ð
Þ
+  + 1
ð
Þn + 1P A1 \A2 \...An
ð
Þ:
Table 2.2.2 Survey Results for a Tax Increase
Yes (for Tax Increase)
No (Against Tax Increase)
C
150
250
S
250
150
R
50
150
62
CHAPTER 2 Basic Concepts from Probability Theory

The summation
X
i1<i2<<imP Ai1 \Ai2 \...\Aim
ð
Þ is taken over all of
the (m
n ) subsets of size m from the set {1, 2, ..., n}.
2.2.23. A sequence of events {An, n1} is said to be an increasing sequence if
A1A2 An  whereas it is said to be decreasing if
A1A2 An . If {An, n1} is increasing sequence of events, then
limn!1An ¼ [1
i¼1An Similarly, if {An, n1} is decreasing sequence of events,
then limn!1 An¼\ i¼1
1 An Show that if {An, n1} is either an increasing or a
decreasing sequence of events, then limn!1 P(An)¼P(limn!1 An).
2.3 COUNTING TECHNIQUES AND CALCULATION OF
PROBABILITIES
In a sample space with a large number of outcomes, determining the number of out-
comes associated with the events through direct enumeration could be tedious. In this
section we developsome countingtechniques and use themin probability computations.
MULTIPLICATION PRINCIPLE
Theorem 2.3.1 If the experiments A1, A2, . . ., Am contain, respectively, n1, n2, . . ., nm
outcomes, such that for each possible outcomes of A1 there are n2 possible outcomes for A2,
and so on, then there are a total of n1, n2, . . ., nm possible outcomes for the composite experiment
A1, A2, . . ., Am.
For m¼2 and n1¼2, n2¼3, the tree diagram in Figure 2.2 illustrates the
multiplication principle. If we count the total number of branches at the top of
the tree, we get the total number of possible outcomes for the composite experi-
ment. In Figure 2.2, we can see that there are total of six branches that represent
all the possible outcomes of this experiment. The tree diagrams can be utilized for
counting for any finite number of composite experiments.
A
B
B1
B2
B3
A1
A2
A3
FIGURE 2.2
Tree diagram.
63
2.3 Counting Techniques and Calculation of Probabilities

EXAMPLE 2.3.1
In how many different ways can a student club at a large university with 500 members choose its
president and vice president?
Solution
The president can be chosen 500 ways, and the vice president can be chosen from the remaining 499
ways. Hence, by the multiplication principle, there are (500)(499)¼249,500 ways in which the com-
plete choice can be made.
When a random sample of size k is taken with replacement from a total of n
objects, the total number of ways in which the random sample of size k can be
selected depends on the particular sampling method we employ. Here we will
consider four sampling methods: (i) sampling with replacement and the objects
are ordered, (ii) sampling without replacement and the objects are ordered,
(iii) sampling without replacement and the objects are not ordered, and (iv) sampling
with replacement and the objects are not ordered.
(I) Sampling with Replacement and the Objects Are Ordered
When a random sample of size k is taken with replacement from a total of n
objects and the objects being ordered, then there are nk possible ways of
selecting k-tuples.
For example, (1) if a die is rolled four times, then the sample space will
consist of 64 4-tuples. (2) If an urn contains nine balls numbered 1-9, and a
random sample with replacement of size k¼6 is taken, then the sample space S
will consist of 96 6-tuples.
(II) Sampling Without Replacement and the Objects Are Ordered
The symbol n! (read n factorial) is defined as n!¼n(n1) . . . (2)(1). Clearly
1!¼1. By definition, we take 0!¼1.
If r objects are chosen from a set of n distinct objects without replacement,
any particular (ordered) arrangement of these objects is called a permutation.
For example, CDAB is a permutation of the letters ABCD. The number of
permutations of these four letters is 4!¼24, because the first position can be
filled by any of the four letters, leaving only three possibilities for the second
position, two for the third position, and only one for the fourth position, yielding
the number of permutations to be 4.3.2.1¼24.
PERMUTATION OF n OBJECTS TAKEN m AT A TIME
Theorem 2.3.2 The number of permutations of m objects selected from a collection of n
distinct objects is
nPm ¼
n!
nm
ð
Þ!
¼ n n1
ð
Þ n2
ð
Þ... nm + 1
ð
Þ:
64
CHAPTER 2 Basic Concepts from Probability Theory

When a random sample of size k is taken without replacement from a total
of n objects and the objects being ordered, we will apply the permutation
formula.
EXAMPLE 2.3.2
How many distinct three-digit numbers can be formed using the digits 2, 4, 6, and 8 if no digit can be
repeated?
Solution
The number of distinct three-digit numbers will be the number of permutations of three numbers
from the set of four numbers {2, 4, 6, 8}. Hence the number of distinct three-digit numbers will
be 4P3¼4!/1!¼24
(III) Sampling Without Replacement and the Objects Are Not Ordered
Note that in a permutation, the order in which each object is selected becomes
important. When the order of arrangement is not important—for example, if we
donotdistinguishbetween ABandBA—thearrangementiscalled acombination.
We give the following result for number of combinations.
NUMBER OF COMBINATIONS OF n OBJECTS TAKEN m AT A TIME
Theorem 2.3.3 The number of ways in which m objects can be selected (without replace-
ment) from a collection of n distinct objects is
n
m


¼
n!
m! nm
ð
Þ!
¼ n n1
ð
Þ n2
ð
Þ... nm + 1
ð
Þ
m!
, m ¼ 0,1,2, ...,n:
The symbol (m
n ) is to be read as “n choose m.” When a random sample of size
k is taken without replacement from a total of n objects and the objects are not
ordered, we will apply combinations formula.
EXAMPLE 2.3.3
How many different ways can the admissions committee of a statistics department choose four
foreign graduate students from 20 foreign applicants and three US students from 10 US applicants?
Solution
The four foreign students can be chosen in
20
4
 
ways, and the three US students can be chosen in
10
3
 
ways. Now, by the multiplication principle, the whole selection can be made in
20
4
  10
3
 
¼581,400
ways.
65
2.3 Counting Techniques and Calculation of Probabilities

(IV) Sampling with Replacement and the Objects Are Not Ordered
In obtaining an unordered sample of size k, with replacement, from a
total of n objects, k1 replacements will be made before sampling ceases.
Thus n is increased by k1 so that sampling in this manner may be
thought of as drawing an unordered sample of size k from a population of
size n+k1. Hence, the number of possible samples can be obtained by
using the formula
n + k 1
k


¼ n + k 1
ð
Þ!
k! n1
ð
Þ! , k ¼ 0,1,2, ...:
EXAMPLE 2.3.4
An urn contains 15 balls numbered 1-15. If four balls are drawn at random, with replacement and
without regard for order, how many samples are possible?
Solution
Using the previous formula, the number of possible samples is
15 + 41
4


¼ 18!
4!14! ¼ 3060:
If we need to divide n objects into more than two groups, we can use the following result.
NUMBER OF COMBINATIONS OF n OBJECTS INTO m CLASSES
Theorem 2.3.4 The number of ways that n objects can be grouped into m classes with ni in
the ith class, i¼1, 2, . . ., m and
X
m
i¼1
ni ¼ n
is given by
n
n1n2 ...nm


¼
n!
n1!n2!...nm!
In the foregoing theorem, the numbers
n
n1n2 ...nm


are called multinomial
coefficients.
We can use the previous computational technique to compute the
probabilities of events of interest by using frequency interpretation of
probability. Suppose that there are a total of N possible outcomes for the
experiment and let nA be the number of outcomes favoring an event A. Then the
probability of this event is P(A)¼nA/N. The following is a well-known
problem that is called the birthday problem.
66
CHAPTER 2 Basic Concepts from Probability Theory

EXAMPLE 2.3.5
In a room there are n people. What is the probability that at least two of them have a common birthday?
Solution
Disregarding the leap years, assume that every day of the year is equally likely to be a birthday. Let
A be the event that there are at least two people with a common birthday. There are 365n possible
outcomes of which Ac can happen in 365364(365n+1) ways. Because the event A can hap-
pen in many more ways, it is easier to calculate P(Ac), that is, the probability that no two persons
have the same birthday or equivalently that they all have different birthdays. To count the number of
n-tuples in Ac, because there are no common birthdays, we can use the method of choosing distinct
objects without replacement for an ordered arrangement. Thus there are 365 possibilities to choose
the first person, 364 for the second person, . . ., (365(n1)) possibilities for the nth person. The
product of these numbers gives the total number of elements in Ac. Thus
P Ac
ð
Þ ¼ 365364... 365n + 1
ð
Þ
365n
and hence
P A
ð Þ ¼ 1365364... 365n + 1
ð
Þ
365n
:
For example, if n¼3, P A
ð Þ ¼ 1 365364363
3653
¼ 0:0082, and if n¼40,
P A
ð Þ ¼ 1365364... 36540 + 1
ð
Þ
365
ð
Þ40
¼ 10:1087 ¼ 0:89123:
That is, there is only a 0.82% chance of having a common birthday among three persons,
whereas if n¼40, then P(A)¼0.89123—that is, the chance of having a common birthday among
40 persons increases to 89.12%. Thus, as the number of people increases, the chance of finding
people with common birthdays also increases.
EXAMPLE 2.3.6
In a tank containing 10 fishes, there are three yellow and seven black fishes. We select three fishes at
random.
(a) What is the probability that exactly one yellow fish gets selected?
(b) What is the probability that at most one yellow fish gets selected?
(c) What is the probability that at least one yellow fish gets selected?
Solution
Let A be the event that exactly one yellow fish gets selected, and B be the event that at most one
yellow fish gets selected. There are
10
3
 
¼120 ways to select three fishes from 10.
(a) There are
3
1
 
¼3 ways to select a yellow fish and
7
2
 
¼21 ways to select two black fishes. By
multiplication rule, the probability of selecting exactly one yellow fish is
3
1
  7
2
 
10
3
  ¼ 3 21
ð
Þ
120 ¼ 0:525:
(b) The probability that at most one yellow fish gets selected is the same as the probability of select-
ing none or one, which is
3
1
  7
2
 
10
3
  +
3
0
  7
3
 
10
3
  ¼ 0:525 + 0:292 ¼ 0:817:
(c) The probability that at least one yellow fish gets selected is the same as 1P(none), which is
10.292¼0.708.
67
2.3 Counting Techniques and Calculation of Probabilities

EXAMPLE 2.3.7
Refer to Example 2.3.3. Suppose that the admission committee decides to randomly choose
seven graduate students from a pool of 30 applicants, of whom 20 are foreign and 10 are US applicants.
What is the probability that a chosen seven will have four foreign students and three US students?
Solution
As in Example 2.3.3, the number of ways of selecting four foreign and three US students is
20
4


10
3


¼ 581,400:
The number of ways of selecting seven applicants out of 30 is
30
7


¼ 2,035,800:
Hence the probability that a randomly selected group of seven will consist of four foreign and
three US students is
20
4
  10
3
 
30
7
 
¼ 581,400
2,035,800 ¼ 0:2856:
EXERCISES 2.3
2.3.1. Determine the following:
(i)
10
2
 
,
(ii)
10
0
 
,
(iii)
10
9
 
,
(iv)
10
2
  10
3
 
,
(v)
10
2 3
5


.
2.3.2. A game in a state lottery selects four numbers from a set of numbers,
{0,1,2,3,4,5,6,7,8,9}, with no number being repeated. How many possible
groups of four numbers are possible?
2.3.3. A 10-bit binary word is a sequence of 10 digits, of which each may be either a
1 or a 0. How many 10-bit words are there?
2.3.4. Insulin, a peptide hormone built from 51 amino acid residues, is one of
the smallest proteins known (note that proteins are made up of chains of
amino acids) with a molecular weight of 5808 Da. Twenty amino acids are
encoded by the standard genetic code, that is, proteins are built from a
basic set of 20 amino acids. How many possible proteins of length 51 can be
made with 20 amino acids for each position in the protein?
2.3.5. An examination is designed where the students are required to answer any 20
questions from a group of 25 questions. How many ways can a student
choose the 20 questions?
2.3.6. How many different six-place license plates are possible if the first three
places and the last place are to be occupied by letters and the fourth and fifth
places are to be occupied by numbers?
68
CHAPTER 2 Basic Concepts from Probability Theory

2.3.7.
In how many different ways can 15 tickets to a football game be distributed
among a class of 30 students if each student gets at most one ticket?
2.3.8.
How many different four-letter English words (with or without meaning)
can be written using distinct letters from the alphabet?
2.3.9.
DNA (deoxyribonucleic acid) is made from a sequence of four
nucleotides (A, T, G, or C). Suppose a region of DNA is 40 nucleotides
long. How many possible nucleotide sequences are there in this region
of DNA?
2.3.10. Show that
(a)
n
0

¼
n
n

¼ 1.
(b)
 n
m

¼
n1
m1

+
n1
m


, 1  m  n:

(c)
 n
m

¼
n
nm


:
2.3.11. A lot of 50 electrical components numbered 1-50 is drawn at random, one
by one, and is divided among five customers.
(a) Suppose that it is known that components 3, 18, 12, 26, and 46 are
defective. What is the probability that each customer will receive one
defective component?
(b) What is the probability that one customer will have drawn five
defective components?
(c) What is the probability that two customers will receive two defective
components each, two none and the other one?
2.3.12. A package of 15 apples contains two defective apples. Four apples are
selected at random.
(a) Find the probability that none of the selected apples is defective.
(b) Find the probability that at least one of the selected apples is
defective.
2.3.13. A homeowner wants to repaint her home and install new carpets (no store
where she live sells both paint and carpet). She plans to get the services
from the stores where she buys the paint and carpet. Suppose there are 12
paint stores with painting service available and 15 carpet stores with
installation services available in that city. In how many ways can she choose
these two stores?
2.3.14. From an urn containing 15 white, 7 black, and 8 yellow balls a sample of 3
balls is drawn at random. Find the probability that
(a) All three balls are yellow.
(b) All three balls are of the same color.
(c) All three balls are of different colors.
2.3.15. Refer to Example 2.3.5. Compute (A) for (a) n¼20. (b) n¼30. Estimate n if
you wish to have an approximately 50% chance of finding someone who
shares your birthday.
69
2.3 Counting Techniques and Calculation of Probabilities

2.3.16. A box of manufactured items contains 12 items, of which four are defective.
If three items are drawn at random without replacement, what is the
probability that
(a) The first one is defective and the rest are good?
(b) Exactly one of the three is defective?
2.3.17. Five white and four black balls are arranged in a row. What is the
probability that the end balls are of different colors?
2.3.18. Three numbers are chosen at random from the numbers {1, 2, . . ., 9}. What
is the probability that the middle number is 5?
2.3.19. In each of the following, find the number of elements in the resulting
sample space.
(a) If a die is rolled five times, how many elements are there in the
sample space?
(b) If 13 cards are selected from a deck of 52 playing cards without
replacement, and the order in which the cards are drawn is important,
how many elements are there in the sample space?
(c) Four players in a game of bridge are dealt 13 cards each from an
ordinary deck of 52 cards. What is the total number of ways in which we
can deal the 13 cards to the four players?
(d) If a football squad consists of 72 players, how many selections of
11-man teams are possible?
2.3.20. In Florida Lotto, an urn contains balls numbered 1 to 53. From this urn, a
machine chooses six balls at random and without replacement. The order in
which the balls are selected does not matter. For a $1 bet, a player chooses
six numbers. If all six numbers match with the six numbers chosen by the
urn, the player wins the jackpot. What is the probability of winning the
Florida Lotto jackpot?
2.3.21. The cells in our bodies receive half of their chromosomes from the father
and the other half from the mother. So for each pair of homologous
chromosomes one will be a paternal chromosome and one will be a
maternal chromosome. We have 23 pairs of homologous chromosomes.
(a) How many possible combinations of paternal and maternal
chromosomes are there?
(b) What is the probability of getting a gamete (an organism’s reproductive
cell) with nine paternal and 14 maternal chromosomes? Assume that
any ordered combination is equally likely.
2.4 THE CONDITIONAL PROBABILITY, INDEPENDENCE,
AND BAYES’ RULE
If we know that an event has already occurred or we have some partial information
about the event, then this knowledge may affect the probability of the event of inter-
est. For example, if we were to guess on the probability of rain today, the answers will
70
CHAPTER 2 Basic Concepts from Probability Theory

be different depending on whether we are sitting inside a windowless office or we are
outside and can see the formation of heavy clouds. This leads to the idea of condi-
tional probability.
Definition 2.4.1 The conditional probability of an event A, given that an event B
has occurred, denoted by P(AjB), is equal to
P AjB
ð
Þ ¼ P A\B
ð
Þ
P B
ð Þ
provided P(B)>0.
EXAMPLE 2.4.1
We toss two balanced dice, and let A be the event that the sum of the face values of two dice is 8, and
B be the event that the face value of the first one is 3. Calculate P(AjB).
Solution
The elements of the events A and B are
A ¼
2, 6
ð
Þ, 6, 2
ð
Þ, 3, 5
ð
Þ, 5, 3
ð
Þ, 4, 4
ð
Þ
f
g:
and
B ¼
3, 1
ð
Þ, 3, 2
ð
Þ, 3, 3
ð
Þ, 3, 4
ð
Þ, 3, 5
ð
Þ, 3, 6
ð
Þ
f
g:
Now A\B¼{(3, 5)}
P A
ð Þ ¼ 5
36, P B
ð Þ ¼ 6
36, and P A\B
ð
Þ ¼ 1
36:
Therefore,
P AjB
ð
Þ ¼ P A\B
ð
Þ
P B
ð Þ
¼
1
36
6
36
¼ 1
6:
It is important to note that the conditional probability P(.jB), is a probability on B.
It satisfies all the axioms of a probability.
SOME PROPERTIES OF CONDITIONAL PROBABILITY
1. If E2E1, then P(E2jA)P(E1jA).
2. P(EjA)¼1P(Ec jA).
3. P(E1[E2jA)¼P(E1jA)+P(E2jA)P(E1\E2jA).
4. Multiplication law: P(A\B)¼P(B)P(AjB)¼P(A)P(BjA).
In general,
P A1 \A2 \...\An
ð
Þ ¼ P A1
ð
ÞP A2jA1
ð
ÞP A3jA1 \A2
ð
Þ...
P AnjA1 \A2 \...\An1
ð
Þ:
71
2.4 The Conditional Probability, Independence, and Bayes’ Rule

EXAMPLE 2.4.2
A fruit basket contains 25 apples and oranges, of which 20 are apples. If two fruits are randomly
picked in sequence, what is the probability that both the fruits are apples?
Solution
Let
A ¼
event that the first fruit is an apple
f
g,
B ¼
event that the second fruit is an apple
f
g:
We need to find P(A\B). We have
P A
ð Þ ¼ 20
25, P BjA
ð
Þ ¼ 19
24:
Now using the multiplication principle for conditional probabilities,
P A\B
ð
Þ ¼ P A
ð ÞP BjA
ð
Þ ¼
20
25

 19
24


¼ 0:633:
Hence the probability that both the fruits are apples is 0.633.
Probability and statistics are proving to be very useful in the field of genetics.
Genetics is the study of heredity—traits transmitted from parent to offspring.
The starting point of the subject of genetics as presently known can be attributed
to Gregor Mendel (1822-1884), an Austrian monk. During the 1850s Mendel was
interested in plant breeding. He performed careful experiments with the garden pea,
Pisum sativum, and uncovered the basic principles of genetic inheritance. Mendel
discovered that traits are inherited in discrete units (known as genes). Mendel’s law
of independent segregation states that the parent transmits randomly one of its traits
to the offspring. Geneticists use letters to represent alleles. An allele is an alterna-
tive form of a gene that is located at a specific position on a specific chromosome.
Organisms have two alleles for each trait. A capital letter is used to represent a
dominant trait, and a lowercase letter is used to represent a recessive trait. The com-
bination pair of these traits that one inherits from parents is the genetic makeup. A
dominant allele can be observed in the organism’s appearance or physiology,
whereas a recessive allele cannot be observed unless the individual has two copies
of the recessive allele.
EXAMPLE 2.4.3
Suppose we are given a population with the following genetic distribution:
Genetic makeup
AA
Aa
aa
Probability
p
2q
r
Alleles are randomly donated from parents to offspring. Assuming random mating, what is the
probability that the mating is AaAa and the offspring is aa (recessive trait)?
72
CHAPTER 2 Basic Concepts from Probability Theory

Solution
Let B denote the event that the mating is AaAa, and C denote the event that the offspring is aa.
Then we have P(B)¼4q2. Because the alleles are randomly donated from parents to offspring,
P(CjB)¼1/4. Now, using the multiplication principle for conditional probabilities,
P B\C
ð
Þ ¼ P B
ð ÞP CjB
ð
Þ ¼ 4q2

 1
4
 
¼ q2:
Hence the probability that the mating is AaAa and the offspring is of the recessive trait is q2.
In order to compute probabilities similar to that in Example 2.4.3, we could use
Table 2.1. The distributions of the progeny (zygotes) are the predicted values from
Mendel’s law.
If the occurrence of one event has no effect on the occurrence of another event,
then those two events are said to be independent of each other. Thus, we have the
following definition.
Definition 2.4.2 Two events A and B with P(A)6¼0 and P(B)6¼0 are said to be
independent if P(AjB)¼P(A), or P(BjA)¼P(B). Otherwise, A and B are dependent.
As a consequence of the foregoing definition two events A and B are independent
if and only if P(A\B)¼P(A)P(B) and at least one of P(A) or P(B) is not zero. An
alternative definition of independence of two events A and B can be based on this
equality. That is, two events A and B are said to be independent if
P A\B
ð
Þ ¼ P A
ð ÞP B
ð Þ:
In this case it is not necessary to assume that at least one of P(A) or P(B) is
not zero.
EXAMPLE 2.4.4
Suppose that we toss two fair dice. Let E1 denote the event that the sum of the dice is 6 and E2 denote
the event that the first die equals 4. Then, P(E1\E2)¼P({4, 2})¼1/366¼P(E1)P(E2)¼5/216.
Hence E1 and E2 are dependent events.
Table 2.1 The Distribution of Zygotes.
Mating
Probability of Mating
Probability of Zygotes (Offspring)
AA
Aa
aa
AAAA
p2
1
0
0
AAAa
2pq
1
2
1
2
0
AAaa
pr
0
1
0
AaAa
4q2
1
4
1
2
1
4
Aaaa
2qr
0
1
2
1
2
aaaa
r2
0
0
1
73
2.4 The Conditional Probability, Independence, and Bayes’ Rule

Definition 2.4.3 The k events A1, A2, . . ., Ak are mutually independent if for
every j¼2, 3, . . ., k and every subset of distinct indices i1, i2, . . ., ij
P Ai1 \Ai2 \...\Aij


¼ P Ai1
ð
ÞP Ai2
ð
Þ...P Aij


:
Mutually independent events will often be called independent. In particular, if
P Aij \Aik


¼ P Aij


P Aik
ð
Þ for each j6¼k, then the events are called pairwise
independent.
Now we will discuss computation of the probability P(AjjB) (called posterior
probability) from the given prior probabilities P(Ai) and conditional probabilities
P(BjAi). First we will state the total probability rule.
LAW OF TOTAL PROBABILITY
Theorem 2.4.1 Assume S¼A1[A2[ . . . [An, where P(Ai)>0, i¼1, 2, . . ., n, and
Ai\Aj¼∅(null set) for i6¼j. Then for any event B,
P B
ð Þ ¼
X
n
i¼1
P Ai
ð
ÞP BjAi
ð
Þ:
The set A1, A2, . . ., An given in Theorem 2.4.1 is called the partition of S.
EXAMPLE 2.4.5
Assume that a noisy channel independently transmits symbols, say 0 s 60% of the time and 1 s 40%
of the time. At the receiver, there is a 1% chance of obtaining any particular symbol distorted. What
is the probability of receiving a 1, irrespective of which symbol is transmitted?
Solution
Given
P 0
ð Þ ¼ P 000istransmitted
ð
Þ ¼ 0:6
and
P 1
ð Þ ¼ P 010istransmitted
ð
Þ ¼ 0:4:
Also, given that the probability that a particular symbol is distorted is 0.01; that is,
P 1j0
ð
Þ ¼ P 1isreceivedj0istransmitted
ð
Þ
¼ 0:01 ¼ P 0j1
ð
Þ ¼ P 0 is recievedj1istransmitted
ð
Þ:
Hence, from the total probability rule, the probability of receiving a zero is
P 1
ð Þ ¼ P receiveda1
ð
Þ ¼ P 1j0
ð
ÞP 0
ð Þ + P 1j1
ð
ÞP 1
ð Þ
¼ 0:01
ð
Þ 0:6
ð
Þ + 0:99
ð
Þ 0:4
ð
Þ ¼ 0:402:
Hence, irrespective of whether a 0 or 1 is transmitted, the probability of receiving a 1 is 0.402.
74
CHAPTER 2 Basic Concepts from Probability Theory

EXAMPLE 2.4.6
During an epidemic in a town, 40% of its inhabitants became sick. Of any 100 sick persons, 10 will
need to be admitted to an emergency ward. What is the probability that a randomly chosen person
from this town will be admitted to an emergency ward?
Solution
Let
A ¼
the person is healthy
f
g
and
B ¼
the person is admitted to an emergency ward
f
g:
It is given
P Ac
ð
Þ ¼ 0:4:
Hence,
P A
ð Þ ¼ 0:6:
We want to find P(B). Now P(BjA)¼0, because a healthy person will not be admitted to an
emergency ward. Also,
P BjAc
ð
Þ ¼ 10
100 ¼ 0:1:
Hence, by the total probability rule,
P B
ð Þ ¼ P A
ð ÞP BjA
ð
Þ + P Ac
ð
ÞP BjAc
ð
Þ
¼ 0:6
ð
Þ 0
ð Þ + 0:1
ð
Þ 0:4
ð
Þ ¼ 0:04:
Sometimes it is not possible to directly calculate the conditional probability that is
needed but other probabilities related to the probability in question are available.
Bayes’ rule shows how probabilities change in the light of information and how
to calculate them. It is also an essential tool in the Bayesian inference. Bayes’ the-
orem is named after an English clergyman, Reverend Thomas Bayes, who outlined
the result in a paper published (posthumously) in 1763. This is one of those results
that we can prove relatively easily. However, the implications of this result are
profound in statistics and many other applied fields; see Chapter 11.
BAYES’ RULE
Theorem 2.4.2 Assume S¼A1[A2[ . . . [An, where P(Ai)>0, i¼1, 2, . . ., n and
Ai\Aj¼∅for i6¼j. Then for any event B, with P(B)>0
P AjjB


¼
P Aj


P BjAj


Xn
i¼1P Ai
ð
ÞP BjAi
ð
Þ
:
Continued
75
2.4 The Conditional Probability, Independence, and Bayes’ Rule

Proof. We have
P Aj B
j


¼ P Aj \B


P B
ð Þ
¼
P Aj \B


Xn
i¼1P Ai
ð
ÞP B Aj


, by total probability rule for P B
ð Þ
¼
P Aj


P BjAj


Xn
i¼1P Ai
ð
ÞP BjAi
ð
Þ
:
n
In Bayes’ theorem, the probabilities P(Ai) are called the prior or a priori
probabilities of the events Ai and the conditional probability P(AjjB) is called the
posterior probability of the event Aj. The events A1, . . ., An are sometimes called
the states of nature.
EXAMPLE 2.4.7
Suppose a statistics class contains 70% male and 30% female students. It is known that in a test, 5%
of males and 10% of females got an “A” grade. If one student from this class is randomly selected
and observed to have an “A” grade, what is the probability that this is a male student?
Solution
Let A1 denote that the selected student is a male, and A2 denote that the selected student is a female.
Here the sample space S¼A1[A2. Let D denote that the selected student has an “A” grade. We are
given P(A1)¼0.7, P(A2)¼0.3, P(DjA1)¼0.05, and P(DjA2)¼0.10. Then by the total probability
rule,
P D
ð Þ ¼ P A1
ð
ÞP DjA1
ð
Þ + P A2
ð
ÞP DjA2
ð
Þ
¼ 0:035 + 0:030 ¼ 0:065:
Now by Bayes’ rule,
P A1 D
j
ð
Þ ¼
P A1
ð
ÞP DjA1
ð
Þ
P A1
ð
ÞP DjA1
ð
Þ + P A2
ð
ÞP DjA2
ð
Þ
¼ 0:7
ð
Þ 0:05
ð
Þ
0:065
ð
Þ
¼ 7
13 ¼ 0:538:
This shows that even though the probability of a male student getting an “A” grade is smaller than
that for a female student,because of the larger number of male students inthe class, a male student with
an “A” grade has a greater probability of being selected than a female student with an “A” grade.
STEPS TO APPLY BAYES’ RULE
To find P(A1jD):
1. List all the probabilities including conditional probabilities given in the problem. That is
P(A1), . . ., P(An) and P(DjA1), . . ., P(DjAn).
2. Write the numerator as the product, P(A1)P(DjA1).
3. Using total probability rule, find the denominator probability by calculating P(D)¼P
i¼1
n P(Ai)
P(DjAi), in the Bayes’ rule.
4. The desired probability is
Numerator
Denominator:
76
CHAPTER 2 Basic Concepts from Probability Theory

EXAMPLE 2.4.8
Suppose that three types of antimissile defense systems are being tested. From the design point of
view, each of these systems has an equally likely chance of detecting and destroying an incoming
missile within a range of 250 miles with a speed ranging up to nine times the speed of sound. How-
ever, in actual practice it has been observed that the precisions of these antimissile systems are not
the same; that is, the first system will usually detect and destroy the target 10 of 12 times, the second
will detect and destroy it 9 of 12 times, and the third will detect and destroy it 8 of 12 times. We have
observed that a target has been detected and destroyed. What is the probability that the antimissile
defense system was of the third type?
Solution
Let S1, S2, and S3 be the events that the first, second, and third antimissile defense systems, respec-
tively, are used. Also let D be the event that the target has been detected and destroyed. We wish
to find P(S3jD). Given that P(S1)¼P(S2)¼P(S3)¼1/3, P(DjS1)¼10/12, P(DjS2)¼9/12, and
P(DjS3)¼8/12. By total probability rule,
P D
ð Þ ¼ P S1
ð
ÞP DjS1
ð
Þ + P S2
ð
ÞP DjS2
ð
Þ + P S3
ð
ÞP DjS3
ð
Þ
¼
1
3
  10
12


+
1
3
 
9
12


+
1
3
 
8
12


¼ 0:75:
Now using the Bayes formula, we have
P S3jD
ð
Þ ¼ P S3
ð
ÞP DjS3
ð
Þ
P D
ð Þ
¼
1
3
 
8
12
ð Þ
0:75 ¼ 8
27 ¼ 0:2963:
If the target is destroyed, then the probability that the antimissile defense system was of the third
type is 0.2963.
EXERCISES 2.4
2.4.1. Consider the portion of an electric circuit with three relays shown in
Figure 2.3. Current will flow from point a to point b if at least one of the
relays closes properly when activated. The relays may malfunction and not
close properly when activated. Suppose that the relays act independently of
one another and close properly when activated with probability 0.9.
(a) What is the probability that current will flow when the relays are
activated?
1
2
3
b
a
FIGURE 2.3
Electric circuit with three relays.
77
2.4 The Conditional Probability, Independence, and Bayes’ Rule

(b) Given that current flowed when the relays were activated, what is the
probability that relay 1 functioned?
2.4.2. If P(A)>0, P(B)>0 and P(A)<P(AjB), show that P(B)<P(BjA).
2.4.3. If P(B)>0,
(a) Show that P(AjB)+P(AcjB)¼1.
(b) Show that in general the following two statements are false: (i) P(AjB)
+P(AjBc)¼1, (ii) P(AjB)+P(AcjBc)¼1.
2.4.4. If P(B)¼p, P(AcjB)¼q, and P(Ac\Bc)¼r, find (a) P(A\Bc), (b) P(A), and
(c) P(BjA).
2.4.5. If A and B are independent, show that so are (i) Ac and B, (ii) A and Bc, and
(iii) Ac and Bc.
2.4.6. Show that two events A and B are independent if and only if P(A\B)¼P(A)
P(B) when at least one of P(A) or P(B) is not zero.
2.4.7. A card is elected at random from an ordinary deck of 52 playing cards. If E
is the event that the selected card is an ace and F is the event that it is a
spade, show that E and F are independent events.
2.4.8. A fruit basket contains 30 apples, of which five are bad. If you pick two
apples at random, what is the probability that both are good apples?
2.4.9. Two students are to be selected at random from a class with 10 girls and 12
boys. What is the probability that both will be girls?
2.4.10. Assume a population with the genetic distribution given in Example 2.4.3.
Assume random mating. What is the probability that an offspring is aa?
2.4.11. One of the most common forms of colorblindness is a sex-linked hereditary
condition caused by a defect on the X chromosome (one of the two
chromosomes that determine gender). It is known that colorblindness is
much more prevalent in males than in females. Suppose that 6% of males
are colorblind but only 0.75% of females are colorblind. In a certain
population, 45% are male and 55% are female. A person is randomly
selected from this population.
(a) Find the probability that the person is colorblind.
(b) Find the probability that the person is colorblind given that the person is
a male.
2.4.12. A survey asked a group of 400 people whether or not they were doing daily
exercise. The responses by sex and physical activity are as in Table 2.4.1.
A person is randomly selected.
(a) What is the probability that this person is doing daily exercise?
(b) What is the probability that this person is doing daily exercise if we
know that this person is a male?
Table 2.4.1 Physical Activity Survey Results by Gender
Male
Female
Daily exercise
50
61
No daily exercise
177
112
78
CHAPTER 2 Basic Concepts from Probability Theory

2.4.13. A laboratory blood test is 98% effective in detecting a certain disease if the
person has the disease (sensitivity). However, the test also yields a “false
positive” result for 0.5% of the healthy persons tested. (That is, if a healthy
person is tested, then, with probability 0.005, the test result will show
positive.) Assume that 2% of the population actually has this disease
(prevalence). What is the probability a person has the disease given that the
test result is positive?
2.4.14. In order to evaluate the rate of error experienced in reading chest X-rays, the
following experiment is done. Several people with known tuberculosis (TB)
status (through other reliable tests) are subjected to chest X-rays. A technician
who is unaware of this status reads the X-ray, and Table 2.4.2 gives the
result. Here+X-ray means the technician concluded that the person has TB.
Find (a) P(TBj+X-ray), (b) P(+X-rayjNo TB), and (c) P(No TBjX-ray).
2.4.15. Each of the 12 ordered boxes contains 12 coins, consisting of pennies and
dimes. The number of dimes in each box is equal to its order among the
boxes, that is, box number 1 contains one dime and 11 pennies, box
number 2 contains two dimes and 10 pennies, etc. A pair of fair dice is
tossed, and the total showing indicates which box is chosen to have a coin
selected at random from it.
(a) Find the probability that a coin selected is a dime.
(b) It is observed that the selected coin is a penny. Find the probability that
it came from box number 4.
2.4.16. Of 600 car parts produced, it is known that 350 are produced in one
plant, 150 parts in a second plant, and 100 parts in a third plant. Also it is
known that the probabilities are 0.15, 0.2, and 0.25 that the parts will be
defective if they are produced in the first, second, or third plants,
respectively. What is the probability that a randomly picked part from this
batch is not defective?
2.4.17. One class contains 5 girls and 10 boys and a second class contains 13 boys
and 12 girls. A student is randomly picked from the second class and
transferred to the first one. After that, a student is randomly chosen from the
first class. What is the probability that this student is a boy?
2.4.18. Consider that we have in an industrial complex two large boxes, each of
which contains 30 electrical components. It is known that the first box
contains 26 operable and 4 nonoperable components and that the second box
contains 28 operable and 2 nonoperable components. Assume that the
probability of making a selection from each of the boxes is the same.
Table 2.4.2 Chest X-ray for TB Result
Person Without TB
Person with TB
Total
+X-ray
70
27
97
X-ray
1883
20
1903
Total
1945
55
2000
79
2.4 The Conditional Probability, Independence, and Bayes’ Rule

(a) Find the probability that a component selected at random will be
operable.
(b) Suppose the component chosen at random is operable. Find the
probability that the component was chosen from box 1.
2.4.19. Urn 1 contains five white balls and three red balls. Urn 2 contains four white
and six red balls. An urn is selected at random, and a ball is drawn at random
from that urn. Find the probability that, if the ball selected is white, it came
from urn 1.
2.4.20. An urn contains two white balls and two black balls. A number is randomly
chosen from the set {1, 2, 3, 4}, and many balls are removed from the urn.
Find the probability that the number i,i¼1, 2, 3, 4, was chosen if at least one
white ball was removed from the urn.
2.4.21. A certain state groups its licensed drivers according to age into the following
categories: (1) 16 to 25; (2) 26 to 45; (3) 46 to 65; and (4) over 65. Table 2.4.3
lists, for each group, the proportion of licensed drivers who belong to the
group and the proportion of drivers in the group who had accidents.
(a) What proportion of licensed drivers had an accident?
(b) What proportion of those licensed drivers who had an accident were
over 65?
2.4.22. It is known that a rare disease, K, is present only in 0.2% of the population.
Performance of the test by a physician’s diagnostic test for the presence or
absence of the disease K is given in Table 2.4.4, where R+ denotes the
positive test result, and R denotes the negative result. Also, Kc denotes
absence of the disease.
(a) What is the probability that a patient has the disease, if the test result is
positive?
Table 2.4.3 Accident Rate and Size by Age
Group
Size
Accident Rate
1
0.250
0.086
2
0.257
0.044
3
0.347
0.056
4
0.146
0.098
Table 2.4.4 Diagnostic Test Results for Disease K
R+
R–
K
0.98
0.02
Kc
0.01
0.99
80
CHAPTER 2 Basic Concepts from Probability Theory

(b) What is the probability that a patient has the disease, if the test result is
negative?
2.4.23. A store has light bulbs from two suppliers, 1 and 2. The chance of supplier 1
delivering defective bulbs is 10%, whereas supplier 2 has a defective rate
of 3%. Suppose 60% of the current supply of light bulbs came from
supplier 1. If one of these bulbs is taken from the current supply and
observed to be defective, find the probability that it came from supplier 2.
2.4.24. The quality control chart of a certain manufacturing company shows that
45% of the defective parts produced in the company are due to mechanical
errors and 55% were caused by human error. The defective parts caused by
mechanical errors can be detected, with 95% accuracy rate, at an inspection
station. The detection rate is only 80% if the defective parts are due to
human error.
(a) Suppose a defective part was detected at the inspection station. What is
the probability that this defective part is due to human error?
(b) Suppose that a customer returned a defective part that went undetected
at the inspection station. What is the probability that the defective part
is due to human error?
2.4.25. A circuit has three major components: A, B, and C. Component A operates
independently of B and C. The components B and C are interdependent.
It is known that the component A works properly 85% of the time;
component B, 90% of the time; and component C, 95% of the time.
However, if component C fails, there is a 75% chance that B will also fail.
Assume that at least two parts must operate for the circuit to function. What
is the probability that the circuit will function properly?
2.4.26. Suppose that the data in Table 2.4.5 represent approximate distribution of
blood type frequency in percentage of total population.
Assume that the blood types are distributed the same in both male and
female populations. Also, assume that the blood types are independent of
marriage.
(a) What is the probability that in a randomly chosen couple the wife has
type B blood and the husband has type O blood?
(b) It is known that a person with type B blood can safely receive
transfusions only from persons with type B or type O blood. What is the
probability a husband has type B or type O blood? It is given that a
woman has type B blood, what is the probability that her husband is an
acceptable donor for her?
Table 2.4.5 Blood Type Frequency
Blood Type
O
A
B
AB
Frequency (%)
45
40
10
5
81
2.4 The Conditional Probability, Independence, and Bayes’ Rule

2.4.27. Suppose that there are 40 students in a statistics class and their blood type
follows the percentage distribution given in Exercise 2.4.26.
(a) If we randomly select two students from this class, what is the
probability that both will have the same blood type?
(b) If we randomly select two students from this class and it is observed
that the first student’s blood type is B+, what is the probability that the
second student’s blood type is O+?
2.4.28. A rare nonlethal disease (ND) that develops during adolescence is
believed to be associated with a certain recessive genotype (aa) at a certain
locus. It is known that in a population 5% of adults have the disease.
Suppose that among the adults with the disease ND, 85% have the aa
genotype. Also suppose that among the adults without the disease, 2% of
them have the aa genotype. We have randomly selected an adult from
this population,
(a) What is the probability that this person has the disease but not the
aa genome type?
(b) What is the probability that this person has the aa genome type the but
not the disease ND?
(c) Given that this person has the aa genotype, what is the probability
that this person has the disease ND?
2.4.29. (The gambler’s ruin problem) Two gamblers, A and B, bet on the outcomes
of successive flips of a coin. On each flip, if the coin comes up heads, A
collects from B one unit, whereas if it comes up tails, A pays to B one unit.
They continue to do this until one of them runs out of money. If it is
assumed that the successive flips of the coin are independent and each
flip results in a head with probability p, what is the probability that
A winds up with all the money if A starts with i units and B starts with
Ni units?
2.5 RANDOM VARIABLES AND PROBABILITY DISTRIBUTIONS
An experiment may contain numerous characteristics that can be measured. How-
ever, in most cases, an experimenter will focus on some specific characteristics of
the experiment. For example, a traffic engineer may focus on the number of vehi-
cles traveling on a certain road or in a certain direction rather than the brand of
vehicles or number of passengers in each vehicle. In general, each outcome of an
experiment can be associated with a number by specifying a rule of association.
The concept of a random variable allows us to pass from the experimental out-
comes to a numerical function of the outcomes, often simplifying the sample
space.
Definition 2.5.1 A random variable (r.v.) X is a function defined on a sample
space, S, that associates a real number, X(o)¼x, with each outcome o in S
(Figure 2.4).
82
CHAPTER 2 Basic Concepts from Probability Theory

EXAMPLE 2.5.1
Two balanced coins are tossed and face values are noted. Then the sample space S¼{HH, HT, TH,
TT}. Define the random variable X(o)¼n, where n is the number of heads and o represents a simple
event such as HH. Then
X o
ð Þ ¼
0, if o ¼ TT
ð
Þ
0, if o 2 HT, TH
f
g
2, if o ¼ HH
ð
Þ
8
<
:
:
It can be noted that X(o)¼0 or 2 with probability 1/4 (w.p. 1/4) and X(o)¼1 w.p. 1/2
It is important to note that in the definition of a random variable, probability plays no
role. However, as evidenced by the previous example, for each value or a set of values
of the random variable, there are underlying collections of events, and through these
events one connects the values of random variables with probability measures.
The random variable is represented by a capital letter (X, Y, Z,. . .), and any par-
ticular real value of the random variable is denoted by the corresponding lowercase
letter (x, y, z,. . .). We define two types of random variables, discrete and continuous.
In this book, we will not deal with mixed random variables.
Definition 2.5.2 A random variable X is said to be discrete if it can assume only a
finite or countably infinite number of distinct values.
Suppose an Internet business firm had 1000 hits on a particular day. Let the ran-
dom variable X be defined as the number of sales resulted on that day. Then, X can
take values 0, 1, . . ., 1000. If we are to define a random variable as the number of
telephone calls made from a large city on any given day, for all practical purposes,
this can be assumed to take values 0, 1, . . ., 1.
EXAMPLE 2.5.2
Inthetossingofthreefaircoins,lettherandomvariableXbedefinedasX¼numberoftails.ThenXcan
assume values 0, 1, 2, and 3. We can associate these values with probabilities in the following way:
P X ¼ 0
ð
Þ ¼ P
H,H,H
f
g
ð
Þ ¼ 1
8,
P X ¼ 1
ð
Þ ¼ P
H,H,T
f
g[ H,T,H
f
g[ T,H,H
f
g
ð
Þ ¼ 3
8,
P X ¼ 2
ð
Þ ¼ P
T,T,H
f
g[ T,H,T
f
g[ H,T,T
f
g
ð
Þ ¼ 3
8,
P X ¼ 3
ð
Þ ¼ P
T,T,T
f
g
ð
Þ ¼ 1
8:
Continued
R, the real line
Sample space S
X(w)
w
FIGURE 2.4
Random variable as a function.
83
2.5 Random Variables and Probability Distributions

We can write this in the tabular form
x
0
1
2
3
p(x)
1/8
3/8
3/8
1/8
Let X be a discrete random variable assuming values x1, x2, x3,. . .. We have the following.
Definition 2.5.3 The discrete probability mass function (pmf) of a discrete random var-
iable X is the function
p xi
ð Þ ¼ P X ¼ xi
ð
Þ, i ¼ 1,2,3, ...:
A probability mass function (pmf) is more simply called a probability function (pf).
The cumulative distribution function (cdf) F of the random variable X is defined by
F x
ð Þ ¼ P X  x
ð
Þ
¼
X
all yx
p y
ð Þ, for 1 < x < 1:
A cumulative distribution function is also called a probability distribution function or simply
the distribution function.
The probability function p(x) is nonnegative. In addition, because X must take on one of the values in
{x1, x2, x3,. . .}, we have P
i¼1
1 p(xi)¼1. Although the pmf p(x) is defined only for a set of discrete values x1,
x2, x3,. . ., the cdf F(x) is defined for all real numbers.
EXAMPLE 2.5.3
Suppose that a fair coin is tossed twice so that the sample space is S¼{HH, HT, TH, TT}. Let X be
number of heads.
(a) Find the probability function for X.
(b) Find the cumulative distribution function of X
Solution
(a) We have
1
4 ¼ P
HH
f
g
ð
Þ ¼ P
HT
f
g
ð
Þ ¼ P
TH
f
g
ð
Þ ¼ P
TT
f
g
ð
Þ:
Hence, the pmf is given by
p 0
ð Þ ¼ P X ¼ 0
ð
Þ ¼ 1
4 , p 1
ð Þ ¼ 1
2 , p 2
ð Þ ¼ 1
4:
(b) For example,
F 1:5
ð
Þ ¼ P X  1:5
ð
Þ ¼ P X ¼ 0or1
ð
Þ
¼ P X ¼ 0
ð
Þ + P X ¼ 1
ð
Þ
¼ 1
4 + 1
2 ¼ 3
4
:
Proceeding similarly, we obtain (as shown in Figure 2.5)
84
CHAPTER 2 Basic Concepts from Probability Theory

F x
ð Þ ¼
0,
1
4,
3
4,
1 < x < 0
0  x < 1
1  x < 2
1,
2  x < 1:
8
>
>
>
<
>
>
>
:
We have seen that a discrete random variable assumes a finite or a countably infi-
nite value. In contrast, we define a continuous random variable as one that assumes
uncountably many values, such as the points on a real line. We now give the defi-
nition of a continuous random variable.
Definition 2.5.4 Let X be a random variable. Suppose that there exists a nonneg-
ative real-valued function:
f : R![0, 1) such that for any interval [a, b],
P X
ð Þ 2 a, b
½
	Þ ¼
ðb
a
f tð Þdt:
Then X is called a continuous random variable. The function f is called the prob-
ability density function (pdf) of X.
The cumulative distribution function (cdf) is given by
F x
ð Þ ¼ P X  x
ð
Þ ¼
ðx
1
f tð Þdt:
For a given function f to be a pdf, it needs to satisfy the following two conditions:
f(x)0 for all values of x, and
ð1
1
f x
ð Þdx ¼ 1.
Also, if f is continuous, then dF x
ð Þ
dx ¼ f x
ð Þ, where F(x) is the cdf. This follows
from the fundamental theorem of calculus. If f is the pdf of a random variable
X, then
P a  X  b
ð
Þ ¼
ðb
a
f tð Þdx:
FIGURE 2.5
Graph of F(x).
85
2.5 Random Variables and Probability Distributions

Figure 2.6 represents P(aXb).
As a result, for any real number a, P(X¼a)¼0. Also,
P a  X  b
ð
Þ ¼ P a < X  b
ð
Þ ¼ P a  X < b
ð
Þ ¼ P a < X < b
ð
Þ:
If we have cdf F(x), then we have
P a  X  b
ð
Þ ¼ F b
ð ÞF a
ð Þ:
SOME PROPERTIES OF DISTRIBUTION FUNCTION
1. 0F(x)1.
2. lim
x!1 F x
ð Þ ¼ 0, and lim
x! + 1 F x
ð Þ ¼ 1:
3. F is a nondecreasing function, and right continuous.
EXAMPLE 2.5.4
Let the function
f x
ð Þ ¼
lxex,
x > 0
0,
otherwise:
	
(a) For what value of l is f a pdf?
(b) Find F(x).
Solution
(a) First note that f(x)0. Now, for f(x) to be a pdf, we need
ð1
1
f x
ð Þdx ¼ 1. Because f(x)¼0
for x0,
FIGURE 2.6
Probability as an area under a curve.
86
CHAPTER 2 Basic Concepts from Probability Theory

Therefore l¼1. See Figure 2.7.
1 ¼
ð1
1
f x
ð Þdx ¼
ð1
0
lxexdx
¼ l
ð1
1
xexdx ¼ l xexj1
0 +
ð1
0
exdx


, using integration by parts:
¼ l 0exj1
0


¼ l
(b) The cumulative distribution function is
F x
ð Þ ¼
ðx
1
f tð Þdt ¼
0,
x < 0
ðx
0
tetdt ¼ 1 x + 1
ð
Þex, x  0
8
<
:
:
Figure 2.8 represents the cumulative distribution.
FIGURE 2.7
Graph of f(x)¼xex.
FIGURE 2.8
Graph of F(x), x0.
87
2.5 Random Variables and Probability Distributions

EXAMPLE 2.5.5
Suppose that a large grocery store has shelf space for 150 cartons of fruit drink that are delivered on a
particular day of each week. The weekly sale for fruit drink shows that the demand increases steadily
up to 100 cartons and then levels off between 100 and 150 cartons. Let Y denote the weekly demand
in hundreds of cartons. It is known that the pdf of Y can be approximated by
f y
ð Þ ¼
y, 0  y  1
1, 1 < y  1:5
0, elsewhere:
8
>
<
>
:
(a) Find F(y),
(b) Find P(0Y0.5),
(c) Find P(0.5Y1.2).
Solution
(a) The graph of the density function f(y) is shown in Figure 2.9
From the definition of cdf, we have (Figure 2.10)
FIGURE 2.9
Graph of f (y).
1
0.5
1
1.
y
FIGURE 2.10
Graph of CDF.
88
CHAPTER 2 Basic Concepts from Probability Theory

F y
ð Þ ¼
ðy
1
f tð Þdt ¼
0,
y < 0
ðy
0
tdt,
0  y < 1
ð1
0
tdt +
ðy
1
dt,
1  y < 1:5
ð1
0
tdt +
ð1:5
1
dt, y  1:5
8
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
:
¼
0,
y < 0
y2
2 ,
0  y < 1
y 1
2, 1  y < 1:5
1
y  1:5:
8
>
>
>
>
>
<
>
>
>
>
>
:
(b) The probability
P 0  Y  0:5
ð
Þ ¼ F 0:5
ð
ÞF 0
ð Þ
¼ 0:5
ð
Þ2
2
¼ 1
8 ¼ 0:125:
(c) P 0:5  Y  1:2
ð
Þ ¼ F 1:2
ð
ÞF 0:5
ð
Þ
¼ 1:2 1
2
ð
Þ0:125 ¼ 0:575:
EXERCISES 2.5
2.5.1. The probability function of a random variable Y is given by
p ið Þ ¼ cli
i! ,i ¼ 0,1,2, ..., where l is a known positive value and c is a constant.
(a) Find c.
(b) Find P(Y¼0).
(c) Find P(Y>2).
2.5.2. Find k so that the function given by
p x
ð Þ ¼
k
x + 1, x ¼ 1,2,3,4
is a probability mass function. Graph the probability mass function and
cumulative distribution function.
2.5.3. A random variable X has the following probability mass function:
x
5
0
3
6
p(x)
0.2
0.1
0.4
0.3
Find the cumulative distribution function F(x) and graph it.
2.5.4. The probability mass function of a discrete random variable X is given in
the following table:
x
1
0
2
5
6
p(x)
0.1
0.15
0.4
0.8
1
89
2.5 Random Variables and Probability Distributions

(a) Find P(X¼2).
(b) Find P(X>0).
2.5.5. The cumulative distribution function F(x) of a random variable X is given by
F x
ð Þ ¼
0, 1 < x < 1
0:2, 1  x < 3
0:8, 3  x < 9
1, x  9:
8
>
>
>
<
>
>
>
:
Write down the values of the random variable X and the corresponding
probabilities, p(x).
2.5.6. The probability density function of a random variable X is given by
f x
ð Þ ¼
cx,
0 < x < 4
0,
otherwise:
(
(a) Find c.
(b) Find the distribution function F(x).
(c) Compute P(1<X<3).
2.5.7. Let the function
f x
ð Þ ¼
cx2, 0 < x < 3
0,
otherwise:
	
(a) Find the value of c so that f(x) is a density function.
(b) Compute P(2<X<3).
(c) Find the distribution function F(x).
2.5.8. Suppose that Y is a continuous random variable whose pdf is given by
f y
ð Þ ¼
K 4y2y2
ð
Þ, 0 < y < 2
0,
elsewhere:
(
(a) What is the value of K?
(b) Find P(Y>1).
(c) Find F(y).
2.5.9. The random variable X has a cumulative distribution function
F x
ð Þ ¼
0,
for x  0
x2
1 + x2 , for x > 0:
8
<
:
Find the probability density function of X.
2.5.10. A random variable X has a cumulative distribution function
F x
ð Þ ¼
0,
if x  0
ax + b, if 0  x < 3
1,
if x  3:
8
<
:
90
CHAPTER 2 Basic Concepts from Probability Theory

(a) Find the constants a and b.
(b) Find the pdf f(x).
(c) Find P(1<X<5).
2.5.11. The amount of time, in hours, that a machine functions before breakdown is
a continuous random variable with pdf
f tð Þ ¼
1
120et=120,
t  0
0,
t < 0:
(
What is the probability that this machine will function between 98 and
145 h before breaking down? What is the probability that it will function
less than 160 h?
2.5.12. The length of time that an individual talks on a long-distance telephone call
has been found to be of a random nature. Let X be the length of the talk;
assume it to be a continuous random variable with probability density
function given by
f x
ð Þ ¼
ae1
5x,
x > 0
0,
elsewhere:
	
Find
(a) The value of a that makes f(x) a probability density function.
(b) The probability that this individual will talk (i) between 8 and 12 min,
(ii) less than 8 min, (iii) more than 12 min.
(c) Find the cumulative distribution function, F(x).
2.5.13. Let T be the life length of a mechanical system. Suppose that the cumulative
distribution of such a system is given by
F tð Þ ¼
0,
t < 0
1exp  ty
ð
Þb
a


, t  0,a > 0,b,g  0:
	
Find the probability density function that describes the failure behavior
of such a system.
2.6 MOMENTS AND MOMENT-GENERATING FUNCTIONS
One of the most useful concepts in probability theory is that of expectation of a ran-
dom variable. The expected value may be viewed as the balance point of distribution
of probability on the real line, or in common language, the average.
Definition 2.6.1 Let X be a discrete random variable with pmf p(x). Then the
expected value of X, denoted by E(X), is defined by
m ¼ E X
ð Þ ¼
X
all x
xp x
ð Þ, provided
X
all x
jxjp x
ð Þ < 1:
Now we will define the expected value for a continuous random variable.
91
2.6 Moments and Moment-Generating Functions

Definition 2.6.2 The expected value of a continuous random variable X with pdf
f(x) is defined by
m ¼ E X
ð Þ ¼
ð1
1
xf x
ð Þdx, provided
ð1
1
jxjf x
ð Þdx < 1:
The expected value of X is also called the expectation or mathematical expectation of
X. We denote the expected value of X by m.
EXAMPLE 2.6.1
Let
X ¼
1, with a probability 1
2
0, with a probability 1
2
	
:
Then E(X)¼1(1/2)+0(1/2)¼1/2.
EXAMPLE 2.6.2
Let X be a discrete random variable whose probability mass function is given in the following table:
x
1
0
1
2
3
4
5
p(x)
1
7
1
7
1
14
2
7
1
14
1
7
1
7
Find E(X).
Solution
By definition,
E x
ð Þ ¼
X
xp x
ð Þ
¼ 1 1
7
 
+ 0 1
7
 
+ 1
1
14


+ 2 2
7
 
+ 3
1
14


+ 4 1
7
 
+ 5 1
7
 
¼ 2:
EXAMPLE 2.6.3
Let
X0
be
an
integer-valued
random
variable
such
that
P(X¼n)¼pn.
Show
that
EX¼P
n¼1
1 P(Xn).
Solution
Using the definition of expectation, and the fact that (0)p0¼0, we have
EX ¼
X
1
n¼1
npn ¼ 1p1 + 2p2 + 3p3 + 
¼ p1 + p2 + p3 + 
+ p2 + p3 + p4 + 
+ p3 + p4 + 
¼ P X  1
ð
Þ + P X  2
ð
Þ + 
¼
X
1
n¼1
P X  n
ð
Þ:
92
CHAPTER 2 Basic Concepts from Probability Theory

EXAMPLE 2.6.4
Suppose you are selling a car. Let X0, X1, X2,. . . be the successive offers occurring at times 0, 1, 2,
. . ., n, that you receive (assume that the offers are random, independent, and have the same distri-
bution); see Figure 2.11. Show that E(N)¼1, where N¼min{n: Xn>X0}, that is the first time an
offer exceeds the initial offer X0 at time ‘0’.
Solution
By definition,
P N  n
ð
Þ ¼ P X0 is largest of X0,X1, ...,Xn1
ð
Þ
¼ 1
n, by symmetry,
as any of the Xi’s could be more than the rest. Hence, using Example 2.6.3,
E N
ð Þ ¼
X
1
n¼1
P N  n
ð
Þ ¼
X
1
n¼1
1
n ¼ 1:
You would expect to wait a long time to receive an offer better than the first one. So, take the
first offer.
Definition 2.6.3 The variance of a random variable X is defined by
s2 ¼ Var X
ð Þ ¼ E
X m
ð
Þ2
h
i
The square root of variance, denoted by s, is called the standard deviation.
The variance is a measure of spread or variability of values of a random variable
around the mean.
The next result shows how to obtain the expectation of a function of a random
variable.
EXPECTATION OF FUNCTION OF A RANDOM VARIABLE
Theorem 2.6.1 Let g(X) be a function of X, then the expected value of g(X) is
E g X
ð Þ
½
	 ¼
X
x
g x
ð Þp x
ð Þ,
if X is discrete
ð1
1
g x
ð Þf x
ð Þdx, if X is continuous,
8
>
>
<
>
>
:
provided the sum or the integral exists.
0
1
2
3
4
n -1
X0
X1
Xn
n
FIGURE 2.11
Size of successive offerings.
93
2.6 Moments and Moment-Generating Functions

We now give some properties of the expectation of a random variable.
SOME PROPERTIES OF EXPECTED VALUE AND VARIANCE
Theorem 2.6.2 Let c be a constant and let g(X), g1(X), . . ., gn(X) be functions of a random
variable X such that E(g(X)) and E(gi(X)) for i¼1, 2, . . ., n exist. Then the following results hold:
(a) E(c)¼c.
(b) E[cg(X)]¼cE [g(X)].
(c) E[P
igi(X)]¼P
iE[gi(X)].
(d) Var(aX+b)¼a2Var(X). In particular, Var(aX)¼a2Var(X).
(e) Var(X)¼E(X2)m2.
Proof. Proof of (a) through (d) will be given as an exercise. We will prove (e).
Var X
ð Þ ¼ E X m
ð
Þ2
½
	
¼ E X2 2Xm + m2


¼ E X2


2mE X
ð Þ + m2
¼ E X2


2m2 + m2
¼ E X2


m2:
n
EXAMPLE 2.6.5
A discrete random variable X is said to be uniformly distributed over the numbers 1, 2, 3, . . ., n, if
P X ¼ i
ð
Þ ¼ 1
n, i ¼ 1,2, ...,n:
Find EX and Var(X).
Solution
By definition
EX ¼
X
n
i¼1
xipi
¼ 1 1
n
 
+ 2 1
n
 
+  + n 1
n
 
¼ 1
n
n n + 1
ð
Þ
2


¼ n + 1
2 :
Similarly, using the summation formula 12 + 22 +  + n2 ¼ n n + 1
ð
Þ 2n + 1
ð
Þ
6
, we get
EX2 ¼ 12 1
n
 
+ 22 1
n
 
+  + n2 1
n
 
¼ 1
n
n n + 1
ð
Þ 2n + 1
ð
Þ
6


¼ n + 1
ð
Þ 2n + 1
ð
Þ
6
:
94
CHAPTER 2 Basic Concepts from Probability Theory

Hence,
Var X
ð Þ ¼ EX2  EX
ð
Þ2
¼ n + 1
ð
Þ 2n + 1
ð
Þ
6

n + 1
2

2
¼ n2 1
12
:
EXAMPLE 2.6.6
To find out the prevalence of smallpox vaccine use, a researcher inquired into the number of times a
randomly selected 200 people aged 16 and over in an African village had been vaccinated. He
obtained the following figures: never, 17 people; once, 30; twice, 58; three times, 51; four times,
38; five times, 7. Assuming these proportions continue to hold exhaustively for the population of
that village, what is the expected number of times those people in the village had been vaccinated,
and what is the standard deviation?
Solution
Let X denote the random variable representing the number of times a person aged 16 or older in this
village has been vaccinated. Then, we can obtain the following distribution:
x
0
1
2
3
4
5
p(x)
17/200
30/200
58/200
51/200
38/200
7/200
Then,
E X
ð Þ ¼
X
xp x
ð Þ ¼ 1
200 0 17
ð
Þ + 1 30
ð
Þ + 2 58
ð
Þ + 3 51
ð
Þ + 4 38
ð
Þ + 5 7
ð Þ
ð
Þ ¼ 2:43:
Also,
Var X
ð Þ ¼ E X2


 E X
ð Þ
ð
Þ2
¼
X
x2p x
ð Þ 2:43
ð
Þ2 ¼ 7:52 2:43
ð
Þ2
¼ 1:6151
:
Thus, the standard deviation is
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1:6151
p
¼ 1:2709:
EXAMPLE 2.6.7
Let Y be a random variable with pdf
f y
ð Þ ¼
3
64y2 4y
ð
Þ,
0  y  4
0,
elsewhere:
(
(a) Find the expected value and variance of Y.
(b) Let X¼300Y+50. Find E(X) and Var(X), and
(c) Find P(X>750).
Continued
95
2.6 Moments and Moment-Generating Functions

Solution
(a)
E Y
ð Þ ¼
ð1
1
yf y
ð Þdy
¼ 3
64
ð4
0
yy2 4y
ð
Þdy
¼ 2:4:
and
Var Y
ð Þ ¼
ð4
0
y2:4
ð
Þ2 3
64y2 4y
ð
Þdy
¼ 0:64:
(b) Using the fact that Var(aY+b)¼a2Var(Y), we have
Var X
ð Þ ¼ 300
ð
Þ2Var Y
ð Þ
¼ 90,000 0:64
ð
Þ ¼ 57,600:
(c)
P X > 750
ð
Þ ¼ P 300Y + 50 > 750
ð
Þ
¼ P Y > 7
3


¼ 3
64
ð4
7=3
y2 4y
ð
Þdy ¼ 0:55339:
2.6.1 SKEWNESS AND KURTOSIS
Even though the mean m and the standard deviation s are significant descriptive mea-
sures that locate the center and describe the spread or dispersion of probability den-
sity function f(x), they do not provide a unique characterization of the distribution.
Two distributions may have the same mean and variance and yet could be very dif-
ferent, as in Figure 2.12.
FIGURE 2.12
Same mean and variance.
96
CHAPTER 2 Basic Concepts from Probability Theory

To better approximate the probability distribution of a random variable, we may
need higher moments.
Definition 2.6.4 The kth moment about the origin of a random variable X is
defined as EXk and denoted by m0
k0 whenever it exists. The kth moment about its
mean (also called central kth moment) of a random variable X is defined as E[-
(Xm)k] and denoted by mk, k¼2, 3, 4,..., whenever it exists.
In particular, we have E(X)¼m10 ¼m, and s2¼m2. We have seen earlier that the
second moment about mean (variance, s2) is used as a measure of dispersion about
the mean.
Definition 2.6.5 The standardized third moment about mean
a3 ¼ E X m
ð
Þ3
s3
¼ m3
m3=2
2
is called the skewness of the distribution of X. The standardized fourth moment about
mean
a4 ¼ E X m
ð
Þ4
s4
is called the kurtosis of the distribution.
Skewness is used as a measure of the asymmetry (lack of symmetry) of a density
function about its mean. Recall that a distribution, or data set, is symmetric if it looks
the same to the left and right of the center point. If a3¼0, then the distribution is
symmetric about the mean, if a3>0, the distribution has a longer right tail, and if
a3<0, the distribution has a longer left tail. Thus, the skewness of a normal distri-
bution is zero. Kurtosis is a measure of whether the distribution is peaked or flat rel-
ative to a normal distribution. Kurtosis is based on the size of a distribution’s tails.
Positive kurtosis indicates too few observations in the tails, whereas negative kurto-
sis indicates too many observations in the tail of the distribution. Distributions with
relatively large tails are called leptokurtic, and those with small tails are called pla-
tokurtic. A distribution which has the same kurtosis as a normal distribution is known
as mesokurtic. It is known that the kurtosis for a standard normal distribution a4¼3.
A sample of n values, x1,. . .,xn the skewness (g1) and kurtosis (k1) can be calcu-
lated using the following formulas.
g1 ¼
n
n1
ð
Þ n2
ð
Þ
X
n
i¼1
xi x
s

3
and
k1 ¼
n n + 1
ð
Þ
n1
ð
Þ n2
ð
Þ n3
ð
Þ
X
n
i¼1
xi x
s

4
"
#

3 n1
ð
Þ2
n2
ð
Þ n3
ð
Þ:
An important expectation is the moment-generating function for a random variable,
in a sense, this packages all the moments for a random variable in one expression.
Definition 2.6.6 For a random variable X, suppose that there is a positive number
h such that for h<t<h the mathematical expectation E(etX) exists. The moment-
generating function (mgf) of the random variable X is defined by
97
2.6 Moments and Moment-Generating Functions

MX tð Þ ¼ E etX


¼
X
etxp x
ð Þ,
ifdiscrete
Ð
etxf x
ð Þdx, ifcontinuous:
(
An advantage of the moment generating function is its ability to give the moments.
Recall that the Maclaurin series of the function etx is
etx ¼ 1 + tx + tx
ð Þ2
2!
+ tx
ð Þ3
3!
+  + tx
ð Þn
n!
+ 
By using the fact that the expected value of the sum equals the sum of the expected
values, the moment-generating function can be written as
MX tð Þ ¼ E etX


¼ E 1 + tX + tX
ð
Þ2
2!
+ tX
ð
Þ3
3!
+  + tX
ð
Þn
n!
+ 
"
#
¼ 1 + tE X
½ 	 + t2
2!E X2


+ t3
3!E X3


+  + tn
n!E Xn
½
	 + :
Note that MX(0)¼1 for all the distributions. Taking the derivative of MX(t) with
respect to t, we obtain
dMX tð Þ
dt
¼ M0
X tð Þ ¼ E X
½ 	 + tE X
½ 	 + t2
2!E X2


+ t3
3!E X3


+  + t n1
ð
Þ
n1
ð
Þ!E Xn
½
	 + :
Evaluating this derivative at t¼0, all terms except E[X] become zero. We have
M0
X 0
ð Þ ¼ E X
½ 	:
Similarly, taking the second derivative of MX(t), we obtain
M00
X 0
ð Þ ¼ E X2


:
Continuing in this manner, from the nth derivative MX
(n)(t) with respect to t, we obtain
all the moments to be
M n
ð Þ
X
0
ð Þ ¼ E Xn
½
	, n ¼ 1,2,3, ...:
We summarize these calculations in the following theorem.
Theorem 2.6.3 If MX(t) exists, then for any positive integer k,
dkMX tð Þ
dtk
t ¼ 0

¼ M k
ð Þ
X
0
ð Þ ¼ m0
k:
The usefulness of the foregoing theorem lies in the fact that, if the mgf can be found,
the often difficult process of integration or summation involved in calculating dif-
ferent moments can be replaced by the much easier process of differentiation.
The following examples illustrate this fact.
98
CHAPTER 2 Basic Concepts from Probability Theory

EXAMPLE 2.6.8
Let X be a random variable with pf
p x
ð Þ ¼
n
x
 
px 1p
ð
Þnx, x ¼ 0,1,2, ...,n:
(This random variable is called a binomial random variable, and the pmf is called a binomial
distribution.) Show that MX(t)¼[(1p)+pet]n, for all real values of t. Also obtain mean and var-
iance of the random variable X.
Solution
The moment-generating function of X is
MX tð Þ¼ E etX
ð
Þ ¼
X
n
x¼0
etx
n
x
 
px 1p
ð
Þnx
¼
X
n
x¼0
n
x
 
pet
ð
Þx 1p
ð
Þnx:
Using the binomial formula, we have
MX tð Þ ¼ pet + 1p
ð
Þ
½
	n,
1 < t < 1:
The first two derivatives of MX(t) are
M0
X tð Þ ¼ n 1p
ð
Þ + pet
½
	 n1
ð
Þ pet
ð
Þ
and
M00
X tð Þ ¼ n n1
ð
Þ 1p
ð
Þ + pet
½
	 n2
ð
Þ pet
ð
Þ2 + n 1p
ð
Þ + pet
½
	 n1
ð
Þ pet
ð
Þ:
Thus,
m ¼ E X
ð Þ ¼ M0
X 0
ð Þ ¼ np
and
s2 ¼ E X2


m2 ¼ M00 0
ð Þ np
ð
Þ2
¼ n n1
ð
Þp2 + np np
ð
Þ2 ¼ np 1p
ð
Þ:
EXAMPLE 2.6.9
Let Xbea random variablewithpmf f(x)¼ellx/(x!),x¼0,1,2,. . ..(Sucharandom variableiscalleda
Poisson r.v. and the distribution is called a Poisson distribution with parameter l.) Find the mgf of X.
Solution
By definition
MX tð Þ ¼ EetX ¼
X
1
x¼0
etxf x
ð Þ
¼
X
1
x¼0
etx ellx
x!
¼
X
1
x¼0
el etl
ð
Þx
x!
¼ elX
1
x¼0
elet e let
ð
Þ let
ð
Þx
x!
"
#
¼ el et1
ð
ÞX
1
x¼0
e let
ð
Þ let
ð
Þx
x!
"
#
:
We observe that e let
ð
Þ let
ð
Þx=x! is a Poisson pf with parameter let. Hence
X1
x¼0
e let
ð
Þ let
ð
Þx
x!
¼ 1
Thus from (1),
MX tð Þ ¼ el et1
ð
Þ:
99
2.6 Moments and Moment-Generating Functions

EXAMPLE 2.6.10
Let X be a random variable with pdf given by
f x
ð Þ ¼
1
bex=b,
x > 0
0,
otherwise:
(
Find mgf MX(t).
Solution
By definition of mgf,
Mx tð Þ ¼
ð1
1
etxf x
ð Þdx
¼
ð1
0
etx 1
bex=bdx
¼ 1
b
ð1
0
e
1
bt
ð
Þxdx, t < 1
b
¼ 1
b 
1
1=b
ð
Þt
ð
Þe
1
bt
ð
Þx
1
x¼0

"
#
¼ 1
b
b
1bt ¼
1
1bt, t < 1
b:
EXAMPLE 2.6.11
Let X be a random variable with pdf f x
ð Þ ¼ 1=
ﬃﬃﬃﬃﬃﬃ
2p
p


ex2=2, 1<x<1. (We call such random
variable a standard normal random variable.) Find the mgf of X.
Solution
By the definition of mgf, we have
E etx
ð
Þ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
ð + 1
1
etxe
x2
2 dx
¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
ð + 1
1
e
1
2 x2 2tx


dx
¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
ð + 1
1
e
1
2 x2 2tx + t2


+ t2
2 dx
¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
ð + 1
1
e
1
2 xt
ð
Þ2 + t2
2 dx
¼ e
t2
2
1ﬃﬃﬃﬃﬃﬃ
2p
p
ð + 1
1
e
1
2 xt
ð
Þ2
dx ¼ e
t2
2:
as
1=
ﬃﬃﬃﬃﬃﬃ
2p
p


e1
2 xt
ð
Þ2
is
a
normal
pdf
with
mean
t
and
variance
1
and
hence
1ﬃﬃﬃﬃ
2p
p
ð1
1
e1
2 xt
ð
Þ2
dt ¼ 1:
A random variable X with pdf
f x
ð Þ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p


e 1
2s2 xm
ð
Þ2
,
1 < x < 1
is called a normal random variable with mean m and variance s2. We will denote such random vari-
ables by X
N(m, s2).
100
CHAPTER 2 Basic Concepts from Probability Theory

PROPERTIES OF THE MOMENT-GENERATING FUNCTION
1. The moment-generating function of X is unique in the sense that, if two random variables X and Y
have the same mgf (MX (t)¼MY (t), for t in an interval containing 0), then X and Y have the same
distribution.
2. If X and Y are independent, then
MX + Y tð Þ ¼ MX tð ÞMY tð Þ:
That is, the mgf of the sum of two independent random variables is the product of the mgfs of the
individual random variables. The result can be extended to “n” random variables.
3. Let Y¼aX+b. Then
MY tð Þ ¼ ebtMX at
ð Þ:
EXAMPLE 2.6.12
Find the mgf of X
N(m, s2).
Solution
Let Y
N(0, 1) and let X¼sY+m. Then by the foregoing property (3), and the Example 2.6.11, the
mgf of X is
MX tð Þ ¼ emtMY st
ð
Þ
¼ emte
1
2s2t2 ¼ emt + 1
2s2t2:
EXAMPLE 2.6.13
Let X1
N(m1,s1
2), and X2
N(m2,s2
2). Let X1 and X2 be independent. Find the mgf of Y¼X1+X2 and
obtain the distribution of Y.
Solution
By property (2)
MX tð Þ ¼ MX1 tð ÞMX 2 tð Þ
¼ em1t + 1
2s2
1t2


em2t + 1
2s2
2t2


¼ e m1 + m2
ð
Þt + 1
2 s2
1 + s2
2
ð
Þt2:
This implies Y
N(m1+m2,s1
2+s2
2).
This result can be generalized. If X1, . . ., Xn are independent random variables such
that
Xi
N(mi,s1
2), i¼1,. . .,n,
then
we
can
show
that
P
i¼1
n aiXi
N Pn
i¼1aimi,Pn
i¼1a2
i s2
i


:
101
2.6 Moments and Moment-Generating Functions

EXERCISES 2.6
2.6.1. Find E(X) where X is the outcome when one rolls a six-sided balanced die.
Find the mgf of X. Also, using the mgf of X, compute the variance of X.
2.6.2. The grades from a statistics class for the first test are given by
xi
96
87
65
49
77
74
99
68
56
84
p(xi)
3
15
2
15
1
15
1
15
2
15
1
15
1
15
1
15
1
15
2
15
(a) Find mean m and variance s2.
(b) Find the mgf.
2.6.3. The cdf of a discrete random variable Y is given in the following table:
y
1
0
2
5
6
F(y)
0.1
0.15
0.4
0.8
1
(a) Find EY, EY2, EY3, and Var(Y).
(b) Find the mgf of Y.
2.6.4. A discrete random variable X is such that
P X ¼ n
ð
Þ ¼ 2n1
3n , n ¼ 1,2, ...
Show that EX¼3.
2.6.5. A discrete random variable X is such that
P X ¼ 2n
ð
Þ ¼ 1
2n, n ¼ 1,2, ...:
Show that EX¼1. That is, X has no mathematical expectation.
2.6.6. Let X be a random variable with pdf f(x)¼kx2 where 0x1.
(a) Find k.
(b) Find E(X) and Var(X).
(c) Find MX(t). Using the mgf, find E(X).
2.6.7. Let X be a random variable with pdf f(x)¼ax2+b, 0x1. Find a and b
such that E X
ð Þ ¼ 5
8.
2.6.8. Given that X1, X2, X3, and X4 are independent random variables with mean
2, find E(Y) and E(Z) for
Y ¼3X4 X1 + 1
5X3,
Z ¼X2 + 7X3 9X1:
2.6.9. For a random variable X, prove (a)-(d) of Theorem 2.6.2.
102
CHAPTER 2 Basic Concepts from Probability Theory

2.6.10. Let e (for “error”) be a random variable with E(e)¼0, and Var(e)¼s2.
Define the random variable, X¼m+e, where m is a constant. Find E(X),
Var(X), and E(e2).
2.6.11. A degenerate random variable is a random variable taking a constant value.
Let X¼c. Show that E(X)¼c, and Var(X)¼0. Also find the cumulative
distribution function of the degenerate distribution of X.
2.6.12. Let Y
N(m, s2). Use the mgf to find E(X2) and E(X4).
2.6.13. Using Theorem 2.6.3, show that the mean and variance of the Poisson
distribution, with parameter l, is equal to l.
2.6.14. Let X be a discrete random variable with a mass function
p x
ð Þ ¼
1
x x + 1
ð
Þ, x ¼ 1,2, ...
0,
otherwise:
8
<
:
Show that the moment-generating function does not exist for this
random variable.
2.6.15. Let X be a random variable with geometric pdf
f x
ð Þ ¼ p 1p
ð
Þx1, x ¼ 1,2,3, ....
(a) Find E(X) and Var(X).
(b) Show that MX tð Þ ¼
pet
1 1p
ð
Þet, t< ln(1p).
2.6.16. Find E(X) and Var(X) for a random variable X with pdf
f x
ð Þ ¼ 1
2e xj j,
1 < x < 1. Also find the mgf of X.
2.6.17. The probability density function of the random variable X is given by
f x
ð Þ ¼
x2
2 ,
0 < x  1
6x2x2 3
2
,
1 < x  2
x3
ð
Þ2
2
,
2 < x  3
0,
otherwise:
8
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
:
Find the expected value of the random variable X.
2.6.18. Let the random variable X be normally distributed with mean 0 and variance
s2. Show that E(X(2k+1))¼0, where k¼0, 1, 2,. . ..
2.6.19. If the kth moment of a random variable exists, show that all moments of
order less than k exist.
2.6.20. Suppose that the random variable X has an mgf
MX tð Þ ¼
a
at, t < 1
a:
Let the random variable Y have the following function for its probability
density:
103
2.6 Moments and Moment-Generating Functions

g y
ð Þ ¼
aeay, y > 0, a > 0
0,
otherwise:
	
Can we obtain the probability density of the variable X with the
foregoing information?
2.7 CHAPTER SUMMARY
In this chapter, we have introduced the concepts of random events and probability,
how to compute the probabilities of events using counting techniques. We have stud-
ied the concept of conditional probability, independence, and Bayes’ rule. Random
variables and distribution functions, moments, and moment-generating functions of
random variables have also been introduced.
The following lists some of the key definitions introduced in this chapter.
•
Sample space
•
Mutually exclusive events
•
Informal definition of probability
•
Classical definition of probability
•
Frequency interpretation of probability
•
Axiomatic definition of probability
•
Multinomial coefficients
•
Conditional probability
•
Mutually independent events
•
Pairwise independent events
•
Random variable (r.v.)
•
Discrete random variable
•
Discrete probability mass function
•
Cumulative distribution function
•
Continuous random variable
•
Expected value
•
kth moment about the origin
•
kth moment about its mean
•
Skewness and kurtosis
•
Moment-generating function
The following important concepts and procedures have been discussed in this
chapter:
•
Method of computing probability by the classical approach.
•
Some basic properties of probability.
•
Computation of probability using counting techniques.
•
Four sampling methods:
•
Sampling with replacement and the objects are ordered.
•
Sampling without replacement and the objects are ordered.
•
Sampling without replacement and the objects are not ordered.
•
Sampling with replacement and the objects are not ordered.
104
CHAPTER 2 Basic Concepts from Probability Theory

•
Permutation of n objects taken m at a time.
•
Combinations of n objects taken m at a time.
•
Number of combinations of n objects into m classes.
•
Some properties of conditional probability.
•
Law of total probability.
•
Steps to apply Bayes’ rule.
•
Some properties of distribution function.
•
Some properties of expected value.
•
Expectation of function of a random variable.
•
Properties of moment-generating functions.
2.8 COMPUTER EXAMPLES (OPTIONAL)
The three softwares packages, Minitab, SPSS, and SAS, that we are using in this
book are not specifically designed for probability computations. However, the fol-
lowing examples are given to demonstrate that we will be able to use the software
for some basic probability computations. We do not recommend using any of these
three software packages for probability calculations; they are basically designed for
statistical computations. There are many other software packages such as Maple or
MATLAB, that can be used efficiently for probability computations.
2.8.1 EXAMPLES USING R
Example 2.8.1 Calculating Cumulative Probabilities
Random variable X has the following distribution:
X
1
4
5
8
11
p(x)
0.2
0.2
0.1
0.15
0.35
Find P(X4), in this example we will use the which() statement to calculate the
cumulative probability in R however there maybe other methods available. Try using
the which() statement by itself.
R Code:
x¼c(1,4,5,8,11);
Notice p sums to 1
p¼c(0.2,0.2,0.1,0.15,0.35);
sum(p[which(x<¼4)]);
Notice we're summing p values based
on x values which meet the criterion.
Output:
0.4
i.e, P(X4)¼0.4
Example 2.8.2 Expected Value
Using the data in the previous example (2.8.1) calculate E(X) and Var(X).
Since we’re given the distribution we can calculate it using the sum of the values
multiplied by their probabilities
105
2.8 Computer Examples (Optional)

R Code:
x¼c(1,4,5,8,11);
p¼c(0.2,0.2,0.1,0.15,0.35);
Notice p sums to 1
sum(x*p);
E(X)
sum(x*x*p)-sum(x*p)^(2);
Var(X)
Output:
6.55
E(X)
14.9475
Var(X)
2.8.2 MINITAB COMPUTATIONS
In order to find the cdf of a random variable, we can use the following commands in
Example 2.8.1. We can use the mathematical expressions to find the expected value
of a discrete random variable.
EXAMPLE 2.8.1
A random variable X has the following distribution:
x
1
4
5
8
11
p(x)
0.2
0.2
0.1
0.15
0.35
Find P(X4).
Solution
Enter x values in C1 and p(x) values in C2.
Calc>Probability Distributions>Discrete. . . > click Cumulative probability, and in Values
in: enter C1, Probabilities in: enter C2, click input column: enter C1, in Optional storage: enter
C3>OK
We will get the following output in column C3.
0:20 0:40 0:50 0:65 1:00
EXAMPLE 2.8.2
For the random variable X in Example 2.8.1, find E(X).
Solution
Enter X values in column C1 (i.e., 1 4 5 8 11), and enter p(x) values in column C2. Use the following
procedure.
Calc>Calculator. . . > Store results in variable: type C3>in Expression: type (C1)*(C2)>click
OK Then to find the sum of values in column C3>Calc>Column Statistics. . . > click Sum and in
Input variable: type C3>click OK
106
CHAPTER 2 Basic Concepts from Probability Theory

We will get the output as
Column Sum
Sum of C3 ¼ 6.5500
Note that this Sum gives the E(X). In the previous procedure, if we store the expres-
sion (C1)*(C1)*(C2) in column C4 and find the sum of terms in C4, we will get
E(X2). Using this, we will be able to compute Var(X). Using a similar procedure,
we can obtain E(Xn) for any n1.
2.8.2 SPSS EXAMPLES
EXAMPLE 2.8.3
For the random variable X in Example 2.8.1, find E(X).
Solution
In column 1, enter the x values and column 2 enter the p(x) values. Then
Transform>compute. . . > in target variable: type a name, say, product. Move var00001 and
var00002 to Numeric Expression: field and put “*” in between them as (var00001)*(var00002).
Then use the SUM(., .) command to find the value of E(X)
2.8.3 SAS EXAMPLES
EXAMPLE 2.8.4
A random variable X has the following distribution:
x
2
5
6
8
9
P(X)
0.1
0.2
0.3
0.1
0.3
Using SAS, find E(X).
Solution
For discrete distributions where the random variable takes finite values, we can adapt the following
procedure:
data evalue;
input x y n;
z¼x*y*n;
cards;
2 .1 5
5 .2 5
6 .3 5
8 .1 5
9 .3 5
;
run;
proc means;
run;
107
2.8 Computer Examples (Optional)

We know that if proc means is used just for x*y, that will give us 1
n
Pxr x
ð Þ; hence,
multiplying by n, the number of values X takes will give us E(X)¼Pxp(x). We will
get the following output:
The MEANS Procedure
Variable N
Mean
Std Dev Minimum
Maximum
¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼¼
X
5 6.0000000
2.7386128
2.0000000
9.0000000
Y
5 0.2000000
0.1000000
0.1000000
0.3000000
N
5 5.0000000
0
5.0000000
5.0000000
Z
5 6.5000000
4.8476799
1.0000000
13.5000000
From this, we can see that E(X)¼6.5. A direct way to find the expected value is
by using “PROC IML.”
options nodate nonumber;
/* Finding expected value of a random variable */
proc iml;
/* defining all the variables */
x¼{2 5 6 8 9};/* a row vector */
y¼{.1 .2 .3 .1 .3};/* probabilities */
/* calculations */
z¼x*y’;
/* print statements */
print “Display the vector x and probability y and the expected value”;
print x y, z;
quit;
We will get the following output:
X
2
5
6
8
9
Y
0.1
0.2
0.3
0.1
0.3
Z
6.5
PROJECTS FOR CHAPTER 2
2A. THE BIRTHDAY PROBLEM
The famous birthday problem is to find the smallest number of people one must ask
to get an even chance that at least two people have the same birthday. To solve this
you can use the following steps.
Findthe probabilitythatina group ofk people notwo have the sameprobability.Let
q be this probability. Then p¼1q is the probability that at least two people have the
same birthday. Ignoring leap years, take the sample space S as all sequences of length k
with each element one of the 365 days in the year. Thus there are 365k elements in S.
108
CHAPTER 2 Basic Concepts from Probability Theory

(a) Find the total number of sequences with no common birthdays.
(b) Assuming that each sequence is equally likely, show that
q ¼ 365
ð
Þ 364
ð
Þ... 365k + 1
ð
Þ
365k
:
(c) Write a computer program for calculating q for k¼2 to 50, and find the first k for
which p>0.5. This will give the least number of people we should ask to make it
an even chance that at least two people will have the same birthday.
2B. THE HARDY-WEINBERG LAW
Hereditary traits in offspring depend on a pair of genes, one each contributed by the
father and the mother. A gene is either a dominant allele, denoted by A, or a recessive
allele, denoted by a. If the genotype is AA, Aa, or aA, then the hereditary trait is A,
and if the genotype is aa, then the hereditary trait is a. Suppose that the probabilities
of the mother carrying the genotypes aa, aA (same as Aa), and AA are p, q, and r,
respectively. Here p+q+r¼1. The same probabilities are true for the father.
(a) Assuming that the genetic contributions of the mother and father are independent
and the matings are random, show that the respective probabilities for the first-
generation offspring are
p1 ¼ p + q=2
ð
Þ2, q1 ¼ 2 r + q=2
ð
Þ p + q=2
ð
Þ, r1 ¼ r + q=2
ð
Þ2:
Also find P(A) and P(a)
(b) The Englishman G.H. Hardy and the German W. Weinberg could show that the
foregoing probabilities in a population stay constant for generations if certain
conditions are fulfilled. This is known as the Hardy-Weinberg law. Under the
conditions of part (a), using the induction argument, show that the Hardy-
Weinberg law is satisfied, i.e. pn¼p1, qn¼q1, and rn¼r1 for all n1. The
consequences of the Hardy-Weinberg law are that (i) no evolutionary change
occurs through the process of sexual reproduction itself, and (ii) changes in allele
and genotype frequencies can result only from additional forces on the gene pool
of a species.
109
Projects for Chapter 2

CHAPTER
Additional Topics in
Probability
3
CHAPTER CONTENTS
3.1 Introduction .................................................................................................... 112
3.2 Special Distribution Functions .......................................................................... 112
3.3 Joint Probability Distributions ........................................................................... 139
3.4 Functions of Random Variables ........................................................................ 152
3.5 Limit Theorems ............................................................................................... 159
3.6 Chapter Summary ............................................................................................ 168
3.7 Computer Examples (Optional) ......................................................................... 170
Projects for Chapter 3 ............................................................................................ 175
OBJECTIVE
In this chapter we present some special distributions, joint distributions of several
random variables, functions of random variables, and some important limit theorems.
Johann Carl Friedrich Gauss
(Source: http://tobiasamuel.files.wordpress.com/2008/06/carl_friedrich_gauss.jpg)
German mathematician and physicist Carl Friedrich Gauss (1777-1855) is some-
times called the “prince of mathematics.” He was a child prodigy. At the age of 7,
Mathematical Statistics with Applications in R
Copyright © 2015 Elsevier Inc. All rights reserved.
111

Gauss started elementary school, and his potential was noticed almost immediately.
His teachers were amazed when Gauss summed the integers from 1 to 100 instantly.
At age 24, Gauss published one of the most brilliant achievements in mathematics,
Disquisitiones Arithmeticae (1801). In it, Gauss systematized the study of number
theory. Gauss applied many of his mathematical insights in the field of astronomy,
and by using the method of least squares he successfully predicted the location of the
asteroid Ceres in 1801. In 1820 Gauss made important inventions and discoveries in
geodesy, the study of the shape and size of the earth. In statistics, he developed the
idea of the normal distribution. In the 1830s he developed theories of non-Euclidean
geometry and mathematical techniques for studying the physics of fluids. Although
Gauss made many contributions to applied science, especially electricity and mag-
netism, pure mathematics was his first love. It was Gauss who first called mathemat-
ics “the queen of the sciences.”
3.1 INTRODUCTION
In the previous chapter, we looked at the basic concepts of probability calculations,
random variables, and their distributions. There are many special distributions that
have useful applications in statistics. It is worth knowing the type of distribution that
we can expect under different circumstances, because a better knowledge of the
population will result in better inferential results. In the next section, we discuss
some of these distributions with some additional distributions presented in Appen-
dix C. We also briefly deal with joint distributions of random variables and func-
tions of random variables. Limit theorems play an important role in statistics. We
will present two limit theorems: the law of large numbers and the Central Limit
Theorem.
3.2 SPECIAL DISTRIBUTION FUNCTIONS
Random variables are often classified according to their probability distribution
functions. In any analysis of quantitative data, it is a major step to know the form
of the underlying probability distributions. There are certain basic probability distri-
butions that are applicable in many diverse contexts and thus repeatedly arise in prac-
tice. A great variety of special distributions have been studied over the years. Also,
new ones are frequently being added to the literature. It is impossible to give a com-
prehensive list of distribution functions in this book. There are many books and Web
sites that deal with a range of distribution functions. A good list of distributions can
be obtained from http://www.causascientia.org/math_stat/Dists/Compendium.pdf.
In this section, we will describe some of the commonly used probability distribu-
tions. In Appendix C, we list some more distributions with their mean, variance,
and moment-generating functions. First we discuss some discrete probability
distributions.
112
CHAPTER 3 Additional Topics in Probability

3.2.1 THE BINOMIAL PROBABILITY DISTRIBUTION
The simplest distribution is the one with only two possible outcomes. For example,
when a coin (not necessarily fair) is tossed, the outcomes are heads or tails, with each
outcome occurring with some positive probability. These two possible outcomes
may be referred to as “success” if heads occurs and “failure” if tails occurs. Assume
that the probability of heads appearing in a single toss is p; then the probability of
tails is 1p¼q. We define a random variable X associated with this experiment
as taking value 1 with probability p if heads occurs and value 0 if tails occurs with
probability q. Such a random variable X is said to have a Bernoulli probability dis-
tribution. That is, X is a Bernoulli random variable if for some p, 0p1, the prob-
ability P(X¼1)¼p and P(X¼0)¼1p. The probability function of a Bernoulli
random variable X can be expressed as
p x
ð Þ ¼ P X ¼ x
ð
Þ ¼
px 1p
ð
Þ1x,
x ¼ 0,1
0,
otherwise:
(
Note that this distribution is characterized by the single parameter p. It can be easily
verified that the mean and variance of X are E[X]¼p, Var(X)¼pq, respectively, and
the moment-generating function is MX(t)¼pet+(1p).
Even when the experimental values are not dichotomous, reclassifying the var-
iable as a Bernoulli variable can be helpful. For example, consider blood pressure
measurements. Instead of representing the numerical values of blood pressure, if
we reclassify the blood pressure as “high blood pressure” and “low blood pressure,”
we may be able to avoid dealing with a possible misclassification due to diurnal var-
iation, stress, and so forth, and concentrate on the main issue, which would be: Is the
average blood pressure unusually high?
In a succession of Bernoulli trials, one is more interested in the total number of
successes (whenever a 1 occurs in a Bernoulli trial, we term it a “success”). The prob-
ability of observing exactly k successes in n independent Bernoulli trials yields the
binomial probability distribution. In practice, the binomial probability distribution is
used when we are concerned with the occurrence of an event, not its magnitude. For
example, in a clinical trial, we may be more interested in the number of survivors
after a treatment.
Definition 3.2.1 A binomial experiment is one that has the following properties:
(1) The experiment consists of n identical trials. (2) Each trial results in one of the
two outcomes, called a success S and failure F. (3) The probability of success on a
single trial is equal to p and remains the same from trial to trial. The probability of
failure is 1p¼q. (4) The outcomes of the trials are independent. (5) The random
variable X is the number of successes in n trials.
Earlier we have seen that the number of ways of obtaining x successes in n trials is
given by
n
x
 
¼
n!
x! nx
ð
Þ!:
113
3.2 Special Distribution Functions

Definition 3.2.2 A random variable X is said to have binomial probability dis-
tribution with parameters (n, p) if and only if
P X ¼ x
ð
Þ ¼ p x
ð Þ ¼
n
x
 
pxqnx
¼
n!
x! nx
ð
Þ!pxqnx, x ¼ 0, 1, 2, ..., n, 0  p  1, and q ¼ 1p
0,
otherwise:
8
<
:
To show the dependence on n and p, denote p(x) by b(x, n, p) and the cumulative
probabilities by
B x, n, p
ð
Þ ¼
X
x
i¼0
b i, n, p
ð
Þ:
Binomial probabilities are tabulated in the binomial table.
By the binomial theorem, we have
p + q
ð
Þn ¼
X
n
x¼0
n
x
 
pxqnx:
Because (p+q)¼1, we conclude that P
i¼0
x
b(i,n,p)¼P
x¼0
n
n
x

pxqnx¼1n¼1, for
all n1 and 0p1. Hence, p(x) is indeed a probability function. The binomial
probability distribution is characterized by two parameters, the number of indepen-
dent trials n and the probability of success p.
EXAMPLE 3.2.1
It is known that screws produced by a certain machine will be defective with probability 0.01 inde-
pendently of each other. If we randomly pick 10 screws produced by this machine, what is the prob-
ability that at least two screws will be defective?
Solution
Let X be the number of defective screws out of 10. Then X can be considered as a binomial r.v. with
parameters (10, 0.01). Hence, using the binomial pf p(x), given in Definition 3.2.2, we obtain
P X  2
ð
Þ ¼
X
10
x¼2
10
x


0:01
ð
Þx 0:99
ð
Þ10x
¼ 1 P X ¼ 0
ð
Þ + P X ¼ 1
ð
Þ
½
 ¼ 0:004:
In Chapter 2, we saw Mendel’s law. In biology, the result “gene frequencies and
genotype ratios in a randomly breeding population remain constant from generation
to generation” is known as the Hardy-Weinberg law.
EXAMPLE 3.2.2
Suppose we know that the frequency of a dominant gene, A, in a population is equal to 0.2. If we
randomly select eight members of this population, what is the probability that at least six of them will
display the dominant phenotype? Assume that the population is sufficiently large that removing eight
individuals will not affect the frequency and that the population is in Hardy-Weinberg equilibrium.
114
CHAPTER 3 Additional Topics in Probability

Solution
First of all, note that an individual can have the dominant gene, A, if the person has traits AA, aA, or
Aa. Hence, if the gene frequency is 0.2, the probability that an individual is of genotype A is
P A
ð Þ ¼ P AA[Aa[aA
ð
Þ ¼ P AA
ð
Þ + 2P Aa
ð
Þ ¼ 0:2
ð
Þ2 + 2 0:2
ð
Þ 0:8
ð
Þ ¼ 0:36:
Let X denote the number of individuals out of eight that display the dominant phe-
notype. Then X is binomial with n¼8, and p¼0.36. Thus, the probability that at least
six of them will display the dominant phenotype is
P X  6
ð
Þ ¼ P X ¼ 6
ð
Þ + P X ¼ 7
ð
Þ + P X ¼ 8
ð
Þ
¼
X
8
i¼6
10
i


0:36
ð
Þi 0:64
ð
Þ10i ¼ 0:029259:
For large n, calculation of binomial probabilities is tedious. Many statistical software
packages have binomial probability distribution commands. For the purpose of this
book, we will use the binomial table that gives the cumulative probabilities B(x, n, p)
for n¼2 through n¼20 and p¼0.05, 0.10, 0.15, . . ., 0.90, 0.95. If we need the prob-
ability of a single term, we can use the relation
P X ¼ x
ð
Þ ¼ b x, n, p
ð
Þ ¼ B x, n, p
ð
ÞB x1,n,p
ð
Þ:
EXAMPLE 3.2.3
A manufacturer of inkjet printers claims that only 5% of their printers require repairs within the first
year. If of a random sample of 18 of the printers, four required repairs within the first year, does this
tend to refute or support the manufacturer’s claim?
Solution
Let us assume that the manufacturer’s claim is correct; that is, the probability that a printer will
require repairs within the first year is 0.05. Suppose 18 printers are chosen at random. Let p be
the probability that any one of the printers will require repairs within the first year. We now find
the probability that at least four of these out of the 18 will require repairs during the first year. Let X
represent the number of printers that require repair within the first year. Then X follows the binomial
pmf with p¼0.05, n¼18. The probability that four or more of the 18 will require repair within the
first year is given by
P X  4
ð
Þ ¼
X
18
x¼4
18
x


0:05
ð
Þx 0:95
ð
Þ18x
or, using the binomial table,
X
18
x¼4
b x, 18, 0:05
ð
Þ ¼ 1B 3, 18, 0:05
ð
Þ
¼ 10:9891
¼ 0:0109:
This value (approximately 1.1%) is very small. We have shown that if the manufacturer’s claim is
correct, then the chances of observing four or more bad printers out of 18 are very small. But we did
observe exactly four bad ones. Therefore, we must conclude that the manufacturer’s claim cannot be
substantiated.
115
3.2 Special Distribution Functions

MEAN, VARIANCE, AND MGF OF A BINOMIAL RANDOM VARIABLE
Theorem 3.2.1 If X is a binomial random variable with parameters n and p, then
E X
ð Þ ¼ m ¼ np,
Var X
ð Þ ¼ s2 ¼ np 1p
ð
Þ:
Also the moment-generating function
MX tð Þ ¼ pet + 1p
ð
Þ
½
n:
Proof. We derive the mean and the variance. The derivation for mgf is given in Example 2.6.5.
Using the binomial pmf, p(x)¼(n!/(x!(nx)!))pxqnx, and the definition of expectation, we have
m ¼ E X
ð Þ ¼
X
n
x¼0
xp x
ð Þ ¼
X
n
x¼0
x
n!
x! nx
ð
Þ!px 1p
ð
Þnx
¼
X
n
x¼1
n!
x1
ð
Þ! nx
ð
Þ!px 1p
ð
Þnx,
since the first term in the sum is zero, as x¼0.
Let i¼x1. When x varies from 1 through n, i¼(x1) varies from 0 through (n1). Hence,
m ¼
X
n1
i¼0
n!
i! ni1
ð
Þ!pi + 1 1p
ð
Þni1
¼ np
X
n1
i¼0
n1
ð
Þ!
i! n1i
ð
Þ!pi 1p
ð
Þn1i
¼ np,
because the last summand is that of a binomial pmf with parameter (n1) and p, hence, equals 1.
To find the variance, we first calculate E[X(X1)]
E X X 1
ð
Þ
½
 ¼
X
n
x¼0
x x1
ð
Þ
n!
x! nx
ð
Þ!px 1p
ð
Þnx
¼
X
n
x¼2
n!
x2
ð
Þ! nx
ð
Þ!px 1p
ð
Þnx,
because the first two terms are zero. Let i¼x2. Then,
E X X 1
ð
Þ
½
 ¼
X
n2
i¼0
n!
i! ni2
ð
Þ!pi + 2 1p
ð
Þni2
¼ n n1
ð
Þp2X
n2
i¼0
n2
ð
Þ!
i! n2i
ð
Þ!pi 1p
ð
Þn
¼ n n1
ð
Þp2;
because the last summand is that of a binomial pmf with parameter (n2) and p thus equals 1.
116
CHAPTER 3 Additional Topics in Probability

Note that E(X(X1))¼EX2E(X), and so we obtain
s2 ¼ Var X
ð Þ ¼ E X2


 E X
ð Þ
½
2
¼ E X X 1
ð
Þ
½
 + E X
ð Þ E X
ð Þ
½
2
¼ n n1
ð
Þp2 + np np
ð
Þ2 ¼ np2 + np
¼ np 1p
ð
Þ:
n
3.2.2 POISSON PROBABILITY DISTRIBUTION
The Poisson probability distribution was introduced by the French mathematician
Sime´on-Denis Poisson in his book published in 1837, which was entitled Recherches
sur la probabilite´ des jugements en matie`res criminelles et matie`re civile and dealt
with the applications of probability theory to lawsuits, criminal trials, and the like.
Consider a statistical experiment of which A is an event of interest. A random var-
iable that counts the number of occurrences of A is called a counting random vari-
able. The Poisson random variable is an example of a counting random variable.
Here we assume that the numbers of occurrences in disjoint intervals are independent
and the mean of the number occurrences is constant.
Definition 3.2.3 A discrete random variable X is said to follow the Poisson prob-
ability distribution with parameter l>0, denoted by Poisson (l), if
P X ¼ x
ð
Þ ¼ f x, l
ð
Þ ¼ f x
ð Þ ¼ ellx
x!
, x ¼ 0,1,2, ...:
The Poisson probability distribution is characterized by the single parameter, l,
which represents the mean of a Poisson probability distribution. Thus, in order
to specify the Poisson distribution, we only need to know the mean number of
occurrences. This distribution is of fundamental theoretical and practical impor-
tance. Rare events are modeled by the Poisson distribution. For example, the Pois-
son probability distribution has been used in the study of telephone systems. The
number of incoming calls into a telephone exchange during a unit time might be
modeled by a Poisson variable assuming that the exchange services a large number
of customers who call more or less independently. Some other problems where
Poisson representation can be used are the number of misprints in a book, radio-
activity counts per unit time, the number of plankton (microscopic plant or animal
organisms that float in bodies of water) per aliquot of seawater, or count of bacterial
colonies per petri plate in a microbiological study. In stem cell research, the Pois-
son distribution is used to analyze the redundancy of clusters in the stem cell data-
base. A Poisson probability distribution has the unique property that its mean equals
its variance.
117
3.2 Special Distribution Functions

MEAN, VARIANCE, AND MOMENT-GENERATING FUNCTION OF
A POISSON RANDOM VARIABLE
Theorem 3.2.2 If X is a Poisson random variable with parameter l, then
E X
ð Þ ¼ l,
Var X
ð Þ ¼ l:
Also the moment-generating function is
MX tð Þ ¼ el et1
ð
Þ:
The proof of this result is similar to that we used in Theorem 3.2.1 in this section.
One needs to use the Maclaurin’s expansion, el¼P
i¼0
1 (li/i!).
EXAMPLE 3.2.4
Let X be a Poisson random variable with l¼1/2. Find
(a) P(X¼0)
(b) P(X3)
Solution
(a) We have
P X ¼ 0
ð
Þ ¼ p 0
ð Þ ¼ e1=2 1=2
ð
Þ0
0!
¼ e1=2 ¼ 0:60653:
(b) Here we will use complementary event to compute the required probability. That is,
P X  3
ð
Þ ¼ 1P X  2
ð
Þ ¼ 1 p 0
ð Þ + p 1
ð Þ + p 2
ð Þ
½

¼ 1 e1=2 + e1=2 1=2
ð
Þ
1!
+ e1=2 1=2
ð
Þ2
2!
"
#
¼ 10:98561 ¼ 0:01439:
When n is large and p small, binomial probabilities are often approximated by
Poisson probabilities. In these situations, where performing the factorial and
exponential operations required for direct calculation of binomial probabilities is
a lengthy and tedious process and tables are not available, the Poisson approximation
is more feasible. The following theorem states this result.
118
CHAPTER 3 Additional Topics in Probability

POISSON APPROXIMATION TO THE BINOMIAL PROBABILITY
DISTRIBUTION
Theorem 3.2.3 If X is a binomial r.v. with parameters n and p, then for each value x¼0, 1,
2,. . . and as p!0, n!1 with np¼l constant,
lim
n!1
n
x
 
px 1p
ð
Þnx ¼ ellx
x!
:
The proof of this result is similar to that we used in Theorem 3.2.1. In the present
context, the Poisson probability distribution is sometimes referred to as “the distri-
bution of rare events” because of the fact that p is quite small when n is large. Usu-
ally, if p0.1 and n40 we could use the Poisson approximation in practice. In
general, another rule of thumb is to use Poisson approximation to binomial in the
case of n>50, and np<5.
EXAMPLE 3.2.5
If the probability that an individual suffers an adverse reaction from a particular drug is known to be
0.001, determine the probability that out of 2000 individuals, (a) exactly three and (b) more than two
individuals will suffer an adverse reaction.
Solution
Let Y be the number of individuals who suffer an adverse reaction. Then Y is binomial with n¼2000
and p¼0.001. Because n is large and p is small, we can use the Poisson approximation with
l¼np¼2.
(a) The probability that exactly three individuals will suffer an adverse reaction is
P Y ¼ 3
ð
Þ ¼ 23e2
3!
¼ 0:18:
That is, there is approximately an 18% chance that exactly three individuals of 2000 will
suffer an adverse reaction.
(b) The probability that more than two individuals will suffer an adverse reaction is
P Y > 2
ð
Þ ¼1P Y ¼ 0
ð
ÞP Y ¼ 1
ð
ÞP Y ¼ 2
ð
Þ
¼15e2 ¼ 0:323:
Similarly, there is approximately a 32.3% chance that more than two individuals will have
an adverse reaction.
Now we will discuss some continuous distributions. As mentioned earlier, if X is
a continuous random variable with pdf f(x), then
P a  X  b
ð
Þ ¼
ðb
a
f x
ð Þdx:
119
3.2 Special Distribution Functions

3.2.3 UNIFORM PROBABILITY DISTRIBUTION
The uniform probability distribution is used to generate random numbers from other
distributions and also is useful as a “first guess” if no other information about a ran-
dom variable X is known, other than that it is between a and b. Also, in real-world
problems that have uniform behavior in a given interval, we can characterize the
probabilistic behavior of such a phenomenon by the uniform distribution (see
Figure 3.1).
Definition 3.2.4 A random variable X is said to have a uniform probability dis-
tribution on (a, b), denoted by U(a, b), if the density function of X is given by
f x
ð Þ ¼
1
ba,
a  x  b
0,
otherwise:
(
The cumulative distribution function is given by
F x
ð Þ ¼
ðx
1
1
badx ¼
0,
x < a
xa
ba, a  x < b
1,
x  b:
8
<
:
EXAMPLE 3.2.6
If X is a uniformly distributed random variable over (0, 10), calculate the probability that (a) X<3,
(b) X>6, and (c) 3<X<8.
Solution
(a)
P X < 3
ð
Þ ¼
ð3
0
1
10dx ¼ 3
10:
(b)
P X > 6
ð
Þ ¼
ð10
6
1
10dx ¼ 4
10:
(c)
P 3 < X > 8
ð
Þ ¼
ð8
3
1
10dx ¼ 1
2:
f(x) = 0
a
b
f (x) = 0
f (x) = 1/(b-a)
FIGURE 3.1
Uniform probability density.
120
CHAPTER 3 Additional Topics in Probability

MEAN, VARIANCE, AND MOMENT-GENERATING FUNCTION OF
A UNIFORM RANDOM VARIABLE
Theorem 3.2.4 If X is a uniformly distributed random variable on (a, b), then
E X
ð Þ ¼ a + b
2 :
and
Var X
ð Þ ¼ ba
ð
Þ2
12
:
Also, the moment-generating function is
MX tð Þ ¼
etb eta
t ba
ð
Þ,
t 6¼ 0
1,
t ¼ 0:
8
<
:
Proof. We will obtain the mean and the variance and leave the derivation of the moment-
generating function as an exercise. By definition we have
E X
ð Þ ¼
ð1
1
x
1
badx
¼
ðb
a
x
1
badx ¼
1
ba
x2
2 j
b
a
 
!
¼ a + b
2 :
Also
E X2


¼
ðb
a
x2
1
badx ¼
1
ba
x3
3 j
b
a:
 
!
¼ 1
3
b3 a3
ba
¼ 1
3 b2 + ab + a2


as b3 a3 ¼ ba
ð
Þ b2 + ab + a2


:
Thus,
Var X
ð Þ ¼ E X2


 E X
ð Þ
ð
Þ2
¼ 1
3 b2 + ab + a2


 a + b
ð
Þ2
4
¼ 1
12 ba
ð
Þ2
:
n
EXAMPLE 3.2.7
The melting point, X, of a certain solid may be assumed to be a continuous random variable that is
uniformly distributed between the temperatures 100 and 120 C. Find the probability that such a solid
will melt between 112 and 115 C.
Continued
121
3.2 Special Distribution Functions

Solution
The probability density function is given by
f x
ð Þ ¼
1
20, 100  x  120
0,
otherwise:
8
<
:
Hence,
P 112  X  115
ð
Þ ¼
ð115
112
1
20dx ¼ 3
20 ¼ 0:15:
Thus, there is a 15% chance of this solid melting between 112 and 115 C.
3.2.4 NORMAL PROBABILITY DISTRIBUTION
The single most important distribution in probability and statistics is the normal
probability distribution. The density function of a normal probability distribution
is bell-shaped and symmetric about the mean. The normal probability distribution
was introduced by the French mathematician Abraham de Moivre in 1733. He used
it to approximate probabilities associated with binomial random variables when n is
large. This was later extended by Laplace to the so-called Central Limit Theorem,
which is one of the most important results in probability. Carl Friedrich Gauss in
1809 used the normal distribution to solve the important statistical problem of com-
bining observations. Because Gauss played such a prominent role in determining the
usefulness of the normal probability distribution, the normal probability distribution
is often called the Gaussian distribution. Gauss and Laplace noticed that measure-
ment errors tend to follow a bell-shaped curve, a normal probability distribution.
Today, the normal probability distribution arises repeatedly in diverse areas of appli-
cations. For example, in biology, it has been observed that the normal probability
distribution fits data on the heights and weights of human and animal populations,
among others.
We should also mention here that almost all basic statistical inference is based on
the normal probability distribution. The question that often arises is, when do we
know that our data follow the normal distribution? To answer this question we have
specific statistical procedures that we study in later chapters, but at this point we can
obtain some constructive indications of whether the data follows the normal distri-
bution by using descriptive statistics. That is, if the histogram of our data can be
capped with a bell-shaped curve (Figure 3.2), if the stem-and-leaf diagram is fairly
symmetrical with respect to its center, and/or by invoking the empirical rule “back-
wards,” we can obtain a good indication whether our data follow the normal prob-
ability distribution.
Definition 3.2.5 A random variable X is said to have a normal probability dis-
tribution with parameters m and s2, if it has a probability density function given by
f X
ð Þ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
s
e xm
ð
Þ2=2s2,
1 < x < 1, 1 < m < 1, s > 0:
122
CHAPTER 3 Additional Topics in Probability

If m¼0, and s¼1, we call it standard normal random variable.
For any normal random variable with mean m and variance s2, we use the notation
XN(m, s2). When a random variable X has a standard normal probability distribu-
tion, we will write XN(0, 1) (X is a normal with mean 0 and variance 1). Proba-
bilities for a standard normal probability distribution are given in the normal table.
MEAN, VARIANCE, AND MGF OF A NORMAL RANDOM VARIABLE
Theorem 3.2.5 If XN(m, s2), then E(X)¼m and Var(X)¼s2. Also the moment-generating
function is
MX tð Þ ¼ etm + 1
2 t2s2:
If XN(m, s2), then the z-transform (or z-score) of X,Z¼(Xm)/s, is an N(0, 1)
random variable. This fact will be used in calculating probabilities for normal ran-
dom variables.
EXAMPLE 3.2.8
(a) For XN(0, 1), calculate P(Z1.13).
(b) For XN(5, 4), calculate P(2.5<X<10).
Solution
(a) Using the normal table,
P Z  1:13
ð
Þ ¼ 0:50:3708 ¼ 0:1292:
Continued
0
0.0
0.1
0.2
0.3
0.4
0.5
Standard normal probabilty distribution
FIGURE 3.2
Standard normal density function.
123
3.2 Special Distribution Functions

The shaded part in the graph represents the P(Z1.13).
(b) Using the z-transform, we have
P 2:5 < X < 10
ð
Þ ¼ P 2:55
2
< Z < 105
2


¼ P 3:75 < Z < 2:5
ð
Þ
¼ P 3:75 < Z < 0
ð
Þ + P 0 < Z < 2:5
ð
Þ
¼ 0:9938:
In the following example, we will show how to find the z values when the prob-
abilities are given.
124
CHAPTER 3 Additional Topics in Probability

EXAMPLE 3.2.9
For a standard normal random variable Z, find the value of z0 such that
(a) P(Z>z0)¼0.25.
(b) P(Z<z0)¼0.95.
(c) P(Z<z0)¼0.12.
(d) P(Z>z0)¼0.68.
Solution
(a) From the normal table, and using the fact that the shaded area in the figure is 0.25, we obtain
z00.675.
0.0
Z0
0.1
0.2
0.3
0.4
0.5
(b) Because P(Z<z0)¼1P(Zz0)¼0.95¼0.5+0.45. From the normal table, z0¼1.645.
(c) From the normal table, z0¼1.175.
(d) Using the normal table, we have P(Z>z0)¼0.5+P(0<Z<z0)¼0.68.
This implies, P(z0<Z<0)¼0.18. From the normal table, z0¼0.465.
EXAMPLE 3.2.10
The scores of an examination are assumed to be normally distributed with m¼75 and s2¼64. What
is the probability that a score chosen at random will be greater than 85?
Solution
Let X be a randomly chosen score from the exam scores. Then, XN(75, 64).
P X > 85
ð
Þ ¼ P X 75
8
> 8575
8
¼ 1:25


¼ P Z > 1:25
ð
Þ ¼ 0:1056:
Continued
125
3.2 Special Distribution Functions

0.0
1.25
0.1
0.2
0.3
0.4
0.5
Thus, there is about a 10.56% chance that the score will be greater than 85.
In practice, whenever a large number of small effects are present and acting addi-
tively, it is reasonable to assume that observations will be normal. When the number
of data is small, it is risky to assume a normal distribution without a proper testing.
Apart from histogram, box-plot, and stem-and-leaf-displays, one of the most useful
tools for assessing normality is a quantile-quantile or QQ plot. This is a scatterplot
with the quantiles of the scores on the horizontal axis and the expected normal scores
on the vertical axis. The expected normal scores are calculated by taking the z-scores
of (ri0.5)/n, where ri is rank of the ith observation in increasing order. The steps in
constructing a QQ plot are as follows: First, we sort the data in an ascending order. If
the plot of these scores against the expected normal scores is a straight line, then the
data can be considered normal. Any curvature of the points indicates departures from
normality. This procedure obtaining a normal plot (QQ plot is similar to normal plot
for a normal distribution) is described in Project 4C. Figure 3.3 shows a normal
probability plot.
If plotted points do not fit the line well, but bend away from it in places, the dis-
tribution may be non-normal. The shapes in Figure 3.4 will give some indication of
the distribution of the data.
Almost all of the statistical software packages include a procedure for obtaining
the graph of a normal probability plot that can be used to test the normality of a data.
Errors in the measurements can also act in a multiplicative (rather than additive)
manner. In that case, the assumption of normality is not justified.
A closely related distribution to normal distribution is the log-normal distribu-
tion. A variable might be modeled as log-normal if it can be thought of as the mul-
tiplicative effect of many small independent factors. This distribution arises in
physical problems when the domain of the variate, X, is greater than zero and its his-
togram is markedly skewed. If a random variable Y is normally distributed, then
exp(Y) has a log-normal distribution. Thus, the natural logarithm of a log-normally
126
CHAPTER 3 Additional Topics in Probability

FIGURE 3.3
Normal probability plot.
If the layout of points starts
below the normal line, bends to
follow it, and ends above it will
indicates long tails. That is, there
is more variance than we would
expect in a normal distribution.
An S shaped-layout of points
indicates shorter than normal
tails, thus, a smaller variance is
expected.
If the layout of points bend down
and to the right of the normal line
that indicates a long tail to the
left, or left skew.
If the layout of points appear to
bend up and to the left of the
normal line that indicates a long
tail to the right, or right skew.
FIGURE 3.4
Shapes indicating distribution of the data.
127
3.2 Special Distribution Functions

distributed variable is normally distributed. That is, if X is a random variable with
log-normal distribution, then ln(X) is normally distributed. Most biological evidence
suggests that the growth processes of living tissue proceed by multiplicative, not
additive, increments. Thus, the measures of body size should at most follow a
log-normal rather than normal distribution. Also, the sizes of plants and animals
are approximately log-normal. The log-normal distribution is also useful in modeling
of claim sizes in the insurance industry.
The probability density function of a log-normal random variable, X, is given as
f x
ð Þ ¼
1
xsy
ﬃﬃﬃﬃﬃﬃ
2p
p
e lnxmy
ð
Þ
2=2s2
y, x > 0, sy > 0, 1 < my < 1
0,
otherwise:
8
<
:
where my and sy are the mean and standard deviation of Y¼ln(X). These parameters
are related to the parameters of the random variable X as follows:
my ¼ ln
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
m4
x
m2
x + s2
x
s
 
!
, sy ¼ ln
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
m2
x + s2
x
m2
x
s
 
!
:
We can verify that the expected value X is
E X
ð Þ ¼ emy + s2
y=2
ð
Þ
and the variance is
Var X
ð Þ ¼ es2
y 1


e2my + s2
y:
The question of when the log-normal distribution is applicable in a given physical
problem after a certain amount of data has been obtained can be answered by creating
a normal probability plot of ln(X) and testing for normality. Thus, if the natural log-
arithms of the data show normality, log-normal distribution may be more
appropriate.
If X is log-normally distributed with parameters my and sy, and 0<a<b, then
with Y¼ln(X)
P a  X  b
ð
Þ¼ P lna  Y  lnb
ð
Þ
¼ P
lnamy
sy
 Y my
sy
 lnbmy
sy


¼ P a0  Z  b0
ð
Þ,
where ZN(0, 1). This probability can be obtained from the standard normal table.
EXAMPLE 3.2.11
In an effort to establish a suitable height for the controls of a moving vehicle, information was gath-
ered about X, the amounts by which the heights of the operators vary from 60 in., which is the min-
imum height. It was verified that the data that were collected followed the log-normal distribution by
normal probability plot of Y¼ln X. Assume that mx¼6 in. and sx¼2 in.
128
CHAPTER 3 Additional Topics in Probability

(a) What percentage of operators would have a height less than 65.5 in.?
(b) If an operator is chosen at random, what is the probability that his or her height will be between
64 and 66 in.?
Solution
(a) Here, X¼65.560¼5.5. Also,
my ¼ ln
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
m4
x
m2
x + s2
x
s
 
!
¼ ln
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
64
62 + 22
s
¼ 1:74,
sy ¼ ln
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
m2
x + s2
x
m2
x
s
 
!
¼ ln
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
62 + 22
62
s
¼ 0:053:
Thus,
P X  5:5
ð
Þ ¼ P Y  ln5:5
ð
Þ ¼ P Z  ln5:5
ð
Þ1:74
0:053


¼ P Z  0:67
ð
Þ ¼ 0:2514
:
Hence, about 25.14% of the heights of the operators vary from 60 in.
(b) Similar to part (a), we get
P 4  X  6
ð
Þ ¼ P ln4  Y  ln6
ð
Þ
¼ P
ln4
ð
Þ1:74
0:053
 Z  ln6
ð
Þ1:74
0:053


¼ P 6:67  Z  0:98
ð
Þ ¼ 0:8365:
3.2.5 GAMMA PROBABILITY DISTRIBUTION
The gamma probability distribution has found applications in various fields. For
example, in engineering, the gamma probability distribution has been employed
in the study of system reliability. We describe the gamma function before we intro-
duce the gamma probability distribution. The gamma function, denoted by G(a), is
defined as
G a
ð Þ ¼
ð1
0
exxa1dx, a > 0:
It can be shown using the integration by parts that for a>1, G(a)¼(a1)G(a1). In
particular, if n is a positive integer, G(n)¼(n1)!.
Definition 3.2.6 A random variable X is said to possess a gamma probability
distribution with parameters a>0 and b>0 if it has the pdf given by
f x
ð Þ ¼
1
baG a
ð Þxa1ex=b,
ifx > 0
0,
otherwise:
8
<
:
The gamma density has two parameters, a and b. We denote this by Gamma(a, b).
The parameter a is called a shape parameter, and b is called a scale parameter.
Changing a changes the shape of the density, whereas varying b corresponds to
129
3.2 Special Distribution Functions

changing the units of measurement (such as changing from seconds to minutes).
Varying these two parameters will generate different members of the gamma family.
If we take a to be a positive integer, we get a special case of gamma probability dis-
tribution, known as the Erlang distribution. This is used extensively in queuing the-
ory to model waiting times. Figure 3.5 gives an indication of how a and b influence
the shape and scale of f(x).
MEAN, VARIANCE, AND MGF OF A GAMMA RANDOM VARIABLE
Theorem 3.2.6 If X is a gamma random variable with parameters a>0 and b>0, then
E X
ð Þ ¼ ab and Var X
ð Þ ¼ ab2:
Also, the moment-generating function is
MX tð Þ ¼
1
1bt
ð
Þa , t < 1
b:
EXAMPLE 3.2.12
The daily consumption of aviation fuel in millions of gallons at a certain airport can be treated as a
gamma random variable with a¼3, b¼1.
(a) What is the probability that on a given day the fuel consumption will be less than 1 million gallons?
(b) Suppose the airport can store only 2 million gallons of fuel. What is the probability that the fuel
supply will be inadequate on a given day?
0
0
0.05
0.1
0.15
0.2
0.25
0.3
5
10
15
20
25
Gam(4,3)
Gam(2,3)
Gam(3,1)
Gamma pdfs for (2,3), (3,1), (4,3), and (2,4)
Gam(2,4)
FIGURE 3.5
Gamma pdfs for different degrees of freedom.
130
CHAPTER 3 Additional Topics in Probability

Solution
(a) Let X be the fuel consumption in millions of gallons on a given day at a certain airport. Then,
XG (a¼3, b¼1) and
f x
ð Þ ¼
1
G 3
ð Þ 13

x31ex ¼ 1
2x2ex, x > 0:
Hence, using integration by parts, we obtain
P X < 1
ð
Þ ¼ 1
2
ð1
0
x2exdx ¼ 1 5
2e ¼ 0:08025:
0.00
1
0.05
0.10
0.15
0.20
0.25
0.30
Thus, there is about an 8% chance that on a given day the fuel consumption will be less than
1 million gallons.
(b) Because the airport can store only 2 million gallons, the fuel supply will be inadequate if the fuel
consumption X is greater than 2. Thus,
0.00
2
0.05
0.10
0.15
0.20
0.25
0.30
P X > 2
ð
Þ ¼ 1
2
ð1
2
x2exdx ¼ 0:677:
We can conclude that there is about a 67.7% chance that the fuel supply of 2 million gallons will
be inadequate on a given day. So, if the model is right, the airport needs to store more than 2 million
gallons of fuel.
131
3.2 Special Distribution Functions

We now describe two special cases of gamma probability distribution. In the pdf
of the gamma, we let a¼1, we get the pdf of an exponential random variable.
Definition 3.2.7 A random variable X is said to have an exponential probability
distribution with parameter b if the pdf of X is given by
f x
ð Þ ¼
1
bex=b, b > 0, 0  x < 1
0,
otherwise:
8
<
:
Exponential random variables are often used to model the lifetimes of electronic
components such as fuses, for survival analysis, and for reliability analysis, among
others. The exponential distribution (Figure 3.6) is also used in developing models of
insurance risks. The exponential distribution is related to Poisson distribution. When
the events can occur more than once within a given unit of time and the time elapsed
between two consecutive occurrences is exponentially distributed and independent
of previous occurrences of the events then the random variable defined by the num-
ber of occurrences has a Poisson distribution.
MEAN, VARIANCE, AND MGF OF AN EXPONENTIAL RANDOM VARIABLE
Theorem 3.2.7 If X is an exponential random variable with parameters b>0, then
E X
ð Þ ¼ b and Var X
ð Þ ¼ b2:
Also the moment-generating function is
MX tð Þ ¼
1
1bt
ð
Þ, t < 1
b:
10
5
0
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
exponential(3)
15
20
FIGURE 3.6
Probability density function for exponential r.v.
132
CHAPTER 3 Additional Topics in Probability

EXAMPLE 3.2.13
The time, in hours, during which an electrical generator is operational is a random variable that fol-
lows the exponential distribution with b¼160. What is the probability that a generator of this type
will be operational for
(a) Less than 40 h?
(b) Between 60 and 160 h?
(c) More than 200 h?
Solution
Let X denote the random variable corresponding to time (in hours) during which the generator is
operational. Then the density function of X is given by
f x
ð Þ ¼
1
160e
x
160
ð Þ,
x  0
0,
otherwise:
8
<
:
Thus, we have the following:
(a) P X  40
ð
Þ ¼
Ð 40
0
1
160e x=160
ð
Þdx ¼ 0:22119: There is about a 22.1% chance that a generator of this
type will be operational for less than 40 h.
(b) P 60  X  160
ð
Þ ¼ Ð 160
60
1
160e x=160
ð
Þdx ¼ 0:3194: Hence, there is about a 31.94% chance that a
generator of this type will be operational between 60 and 160 h.
(c) P X > 200
ð
Þ ¼
Ð 1
200
1
160 e x=160
ð
Þdx ¼ 0:2865: The chance that the generator will last more than
200 h is about 28.65%.
Another special case of gamma probability distribution that is useful in statistical
inference problems is the chi-square distribution.
Definition 3.2.8 Let n be a positive integer. A random variable, X, is said to have
a chi-square (w2) distribution with n degrees of freedom if and only if X is a gamma
random variable with parameters a¼n/2 and b¼2. We denote this by Xw2(n).
Hence, the probability density function of a chi-square distribution with n degrees
of freedom is given by
f x
ð Þ ¼
1
G n
2ð Þ2n=2x n=2
ð
Þ1ex=2, 0  x < 1
0,
otherwise:
8
<
:
Figure 3.7 illustrates the dependence of the chi-square distribution on n.
The mean and variance of a chi-square random variable follow directly from
Theorem 3.2.6.
MEAN, VARIANCE, AND MGF OF A CHI-SQUARE RANDOM VARIABLE
Theorem 3.2.8 If X is a chi-square random variable with n degrees of freedom, then
E(X)¼n and Var(X)¼2n. Also, the moment-generating function is given by
MX tð Þ ¼
1
12t
ð
Þn=2 , t < 1
2:
133
3.2 Special Distribution Functions

Another class of distributions that plays a crucial role in Bayesian statistics is the
beta distribution. The beta distribution is used as a prior distribution for binomial or
geometric proportions. A random variable X is said to have a beta distribution with
parameters a and b if and only if the density function of X is
f x
ð Þ ¼
xa1 1x
ð
Þb1
B a, b
ð
Þ
, a,b > 0; 0  x  1
0,
otherwise,
8
>
<
>
:
where B(a,b)¼
Ð 1
0 xa1(1x)b1dx. It can be proved that B a, b
ð
Þ ¼ G a
ð ÞG b
ð Þ
G a + b
ð
Þ , and that
E X
ð Þ ¼
a
a + b andVar X
ð Þ ¼
ab
a + b
ð
Þ2 a + b + 1
ð
Þ:
One of the questions we may have is: “How do we know which distribution to use in
a given physical problem?” There is no simple and direct answer to this question. One
intuitiveway istoconstruct a histogram fromthe informationathand; fromthe shape of
this histogram, we decide whether the random variable follows a particular distribution
such as gamma distribution. Once we decide that it follows a particular distribution,
then the parameters of this distribution, such as a and b, must be statistically estimated.
In Chapter 5, we discuss how to estimate these parameters. Then a goodness-of-fit test
(discussed in Chapter 7) can be performed to see whether the distribution model seems
to be the right one.
EXERCISES 3.2
3.2.1. A fair coin is tossed 10 times. Let X denote the number of heads obtained.
Find the following.
(a) P(X¼7)
0
2
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
n = 2
chi-square densities, n= 2, 3, 4, and 5
n = 3
n = 4
n = 5
0.45
0.5
4
6
8
10
12
FIGURE 3.7
Chi-square pdfs for different degrees of freedom.
134
CHAPTER 3 Additional Topics in Probability

(b) P(X7)
(c) P(X>0)
(d) E(X) and Var(X).
3.2.2. Let X be a Poisson random variable with l¼1/3. Find
(a) P(X¼0)
(b) P(X4).
3.2.3. For a standard normal random variable Z, find the value of z0 such that
(a) P(Z>z0)¼0.05
(b) P(Z<z0)¼0.88
(c) P(Z<z0)¼0.10
(d) P(Z>z0)¼0.95.
3.2.4. Let XN(12, 5). Find the value of x0 such that
(a) P(X>x0)¼0.05
(b) P(X<x0)¼0.98
(c) P(X<x0)¼0.20
(d) P(X>x0)¼0.90.
3.2.5. Let XN(10, 25). Compute
(a) P(X20)
(b) P(X>5)
(c) P(12X15)
(d) P(jX12j15).
3.2.6. A quarterback on a football team has a pass completion rate of 0.62. If, in a
given game, he attempts 16 passes, what is the probability that he will
complete
(a) 12 passes?
(b) More than half of his passes?
(c) Interpret your result.
(d) Out of the 16 passes, what is the expected number of completions?
3.2.7. A consulting group believes that 70% of the people in a certain county are
satisfied with their health coverage. Assuming that this is true, find the
probability that in a random sample of 15 people from the county:
(a) Exactly 10 are satisfied with their health coverage, and interpret.
(b) Not more than 10 are satisfied with their health coverage, and interpret.
(c) What is the expected number of people out of 15 that are satisfied with
their health coverage?
3.2.8. A man fires at a target six times; the probability of his hitting it each time is
independent of other tries and is 0.40.
(a) What is the probability that he will hit at least once?
(b) How many times must he fire at the target so that the probability of
hitting it at least once is greater than 0.77?
(c) Interpret your findings.
3.2.9. A certain electronics company produces a particular type of vacuum tube.
It has been observed that, on the average, three tubes of 100 are defective.
The company packs the tubes in boxes of 400. What is the probability
that a certain box of 400 tubes will contain
135
3.2 Special Distribution Functions

(a) r defective tubes?
(b) At least k defective tubes?
(c) At most one defective tube?
(d) Interpret your answers to (a), (b), and (c).
3.2.10. Suppose that, on average, in every two pages of a book there is one
typographical error, and that the number of typographical errors on a single
page of the book is a Poisson r.v. with l¼1/2. What is the probability of at
least one error on a certain page of the book? Interpret your result.
3.2.11. Show that the probabilities assigned by Poisson probability distribution
satisfy the requirements that 0p(x)1 for all x and P
x p(x)¼1.
3.2.12. In determining the range of an acoustic source using the triangulation
method, the time at which the spherical wave front arrives at a receiving
sensor must be measured accurately. Measurement errors in these times can
be modeled as possessing uniform probability distribution from 0.05 to
0.05 ms. What is the probability that a particular arrival time measurement
will be in error by less than 0.01 ms? What does your answer mean?
3.2.13. The hardness of a piece of ceramic is proportional to the firing time.
Assume that a rating system has been devised to rate the hardness of a
ceramic piece and that this measure of hardness is a random variable that is
distributed uniformly between 0 and 10. If a hardness in the interval [5, 9] is
desirable for kitchenware, what is the probability that a piece chosen at
random will be suitable for kitchen use?
3.2.14. A receiver receives a string of 0 and 1 s transmitted from a certain source.
The receiver used a majority rule. That is, if the receiver acquires five
symbols, xxxxx, x is 0 or 1, of which three or more are 1 s, it decides that a 1
was transmitted. The receiver is correct only 85% of the time. What is P(W),
the probability of a wrong decision if the probabilities of receiving 0 and 1 s
are equally likely? What can you conclude from your result?
3.2.15. The efficiency X of a certain electrical component may be assumed to be a
random variable that is distributed uniformly between 0 and 100 units.
What is the probability that X is:
(a) Between 60 and 80 units?
(b) Greater than 90 units?
(c) Interpret (a) and (b).
3.2.16. The reliability function of a system or a piece of equipment at time t is
defined by
R tð Þ ¼ P T  t
ð
Þ ¼ 1F tð Þ
where T, the failure time, is a random variable with a known distribution. A
certain vacuum tube has been observed to fail uniformly over the interval
[t1, t2].
(a) Determine the reliability of such a tube at time t, t1tt2.
(b) If 180t220, what is the reliability of such a tube at 200 h?
(c) The failure or hazard rate function r(t) is defined by
136
CHAPTER 3 Additional Topics in Probability

r tð Þ ¼
f tð Þ
1F tð Þ ¼ f tð Þ
R tð Þ ¼
dR tð Þ
dt
R tð Þ
:
Calculate the failure rate of this vacuum tube. Interpret your result.
3.2.17. An electrical component was studied in the laboratory, and it was
determined that its failure rate was approximately equal to 1
b ¼ 0:05: What is
the reliability of such a component at 10 h?
3.2.18. Suppose that the life length of a mechanical component is normally
distributed.
(a) If s¼3 and m¼100, find the reliability of such a system at 105 h.
(b) What should be the expected life of the component if it has reliability of
0.90 for 120 h?
3.2.19. A geologist defines granite as a rock containing quartz, feldspar, and small
amounts of other minerals, provided that it contains not more than 75%
quartz. If all the percentages are equally likely, what proportion of granite
samples that the geologist collects during his lifetime will contain from
50% to 65% quartz?
3.2.20. For a normal random variable with pdf,
f x
ð Þ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
s
e xm
ð
Þ2=2s2, 1 < x < 1
show that
Ð 1
1f x
ð Þdx ¼ 1: [Hint: use polar coordinates.]
3.2.21. A professor in a large statistics class has a grading policy such that only the
15% of the students with the highest scores will receive the grade A. The
mean score for this class is 72 with a standard deviation of 6. Assuming that
all the grades for this class follow a normal probability distribution, what is
the minimum score that a student in this class has to get to receive an
A grade?
3.2.22. The scores, X, of an examination may be assumed to be normally distributed
with m¼70 and s2¼49. What is the probability that:
(a) A score chosen at random will be between 80 and 85?
(b) A score will be greater than 75?
(c) A score will be less than 90?
(d) Interpret the meaning of (a), (b), and (c).
3.2.23. Suppose that the diameters of golf balls manufactured by a certain company
are normally distributed with m¼1.96 in. and s¼0.04 in. A golf ball will be
considered defective if its diameter is less than 1.90 in. or greater than
2.02 in. What is the percentage of defective balls manufactured by the
company? What did the answer indicate?
3.2.24. Suppose that the arterial diastolic blood pressure readings in a population
follow a normal probability distribution with mean 80 mm Hg and standard
deviation 6.2 mm Hg. Suppose it is recommended that a physician be
consulted if an individual has an arterial diastolic blood pressure reading of
90 mm Hg or more. If an individual is randomly picked from this
137
3.2 Special Distribution Functions

population, what is the probability that this individual needs to consult a
physician? Discuss the meaning of your result.
3.2.25. In a certain pediatric population, systolic blood pressure is normally
distributed with mean 115 mm Hg and standard deviation 10 mm Hg. Find
the probability that a randomly selected child from this population will
have:
(a) A systolic pressure greater than 125 mm Hg.
(b) A systolic pressure less than 95 mm Hg.
(c) A systolic pressure below which 95% of this population lies.
(d) Interpret (a), (b), and (c).
3.2.26. A physical fitness test was given to a large number of college freshmen. In
part of the test, each student was asked to run as far as he or she could in
10 min. The distance each student ran in miles was recorded and can be
considered to be a random variable, say X. The data showed that the random
variable X followed the log-normal distribution with my¼0.35 and sy¼0.5,
where Y¼ln X. A student is considered physically fit if he or she is able to
run 1.5 miles in the time allowed. What percentage of the college freshmen
would be considered physically fit if we consider only this part of the test?
3.2.27. An experimenter is designing an experiment to test tetanus toxoid in guinea
pigs. The survival of the animal following the dose of the toxoid is a random
phenomenon. Past experience has shown that the random variable that
describes such a situation follows the log-normal distribution with my¼0
and sy¼0.65. As a requirement of good design the experimenter must
choose doses at which the probability of surviving is 0.20, 0.50, and 0.80.
What three doses should he choose?
3.2.28. Show that G(1)¼1 and for a>1, G(a)¼(a1)G(a1).
3.2.29. (a) Find the moment-generating function for a gamma probability
distribution with parameter a>0 and b>0. [Hint: In the integral
representation of E(etX), change the variable t to u¼(1bt)x/b,
with (1bt)>0.]
(b) Using the mgf of a gamma probability distribution, find E(X) and
Var(X).
3.2.30. Let X be an exponential random variable. Show that, for numbers a>0
and b>0,
P X > a + bjX > a
ð
Þ ¼ P X > b
ð
Þ:
(This property of the exponential distribution is called the memoryless
property of the distribution.)
3.2.31. A random variable X is said to have a beta distribution with parameters a
and b if and only if the density function of X is
f x
ð Þ ¼
xa1 1x
ð
Þb1
B a, b
ð
Þ
, a,b > 0; 0  x  1
0,
otherwise,
8
<
:
where B a, b
ð
Þ ¼
Ð 1
0 xa1 1x
ð
Þb1dx:
138
CHAPTER 3 Additional Topics in Probability

(a) Show that B a, b
ð
Þ ¼ G a
ð ÞG b
ð Þ
G a + b
ð
Þ :
(b) Show that E X
ð Þ ¼
a
a + b and Var X
ð Þ ¼
ab
a + b
ð
Þ2 a + b + 1
ð
Þ:
3.2.32. The daily proportion of major automobile accidents across the United States
can be treated as a random variable having a beta distribution with a¼6 and
b¼4. Find the probability that, on a certain day, the percentage of major
accidents is less than 80% but greater than 60%. Interpret your answer.
3.2.33. Suppose that network breakdowns occur randomly and independently of
each other on an average rate of three per month.
(a) What is the probability that there will be just one network breakdown
during December? Interpret.
(b) What is the probability that there will be at least four network
breakdowns during December? Interpret.
(c) What is the probability that there will be at most seven network
breakdowns during December? Interpret.
3.2.34. Let X be a random variable denoting the number of events occurring in the
time interval (0, t]. Show that X has a gamma probability distribution with
parameters n and l.
3.2.35. In order to etch an aluminum tray successfully, the pH of the acid solution
used must be between 1 and 4. This acid solution is made by mixing a fixed
quantity of etching compound in powder form with a given volume of
water. The actual pH of the solution obtained by this method is affected by
the potency of the etching compound, by slight variations in the volume of
water used, and perhaps by the pH of the water. Thus, the pH of the solution
varies. Assume that the random variable that describes the random
phenomenon is gamma distributed with a¼2 and b¼1.
(a) What is the probability that an acid solution made by the foregoing
procedure will satisfactorily etch a tray?
(b) What would the answer to part (a) be if a¼1 and b¼2?
3.2.36. If XiPois(li), i¼1,2,. . .,k, are independent, and l¼P
i¼1
k
li, then
Y¼P
i¼1
n XiPois(l).
3.2.37. If Xiexp(b), i¼1,2,. . .,k are independent, then show that
Y¼P
i¼1
k
XiGamma(k,b).
3.3 JOINT PROBABILITY DISTRIBUTIONS
We have thus far confined ourselves to studying one-dimensional or univariate ran-
dom variables and their properties. In many practical situations, we are required to
deal with several, not necessarily independent random variables. For example, we
might be interested in a study involving the weights and heights (W, H) of a certain
group of persons. In this situation, we need the two random variables (W, H), and it is
likely that these two are related. Then it becomes important to study the joint effect of
these random variables, which will lead to finding the joint probability distributions.
In this section, we confine our studies to two random variables and their joint distri-
butions, which are called bivariate distributions. We consider the random variables
139
3.3 Joint Probability Distributions

to be either both discrete or both continuous. We now define joint distribution of two
random variables.
Definition 3.3.1 (a) Let X and Y be random variables. If both X and Y are
discrete, then
f x, y
ð
Þ ¼ P X ¼ x,Y ¼ y
ð
Þ
is called the joint probability mass function (joint pmf) of X and Y.
(b) If both X and Y are continuous then f(x, y) is called the joint probability density
function (joint pdf) of X and Y if and only if
P a  X  b, c  Y  d
ð
Þ ¼
ðb
a
ðd
c
f x, y
ð
Þdydx:
EXAMPLE 3.3.1
A probability class contains 10 African American, 8 Hispanic American, and 15 white students. If 12
students are randomly selected from this class, and if X¼number of black students, and Y¼number
of white students, find the joint probability function of the bivariate random variable (X, Y).
Solution
There are a total of 33 students. The number of ways in which x African American, and y white stu-
dents can be picked (which means, the remaining 12(x+y) students are Hispanic American) can
be obtained using the multiplication principle as
10
x


15
x


8
12xy


:
The number of ways to pick 12 students from 33 students is
33
12
 
. Hence, the joint probability
function is
P X ¼ x, Y ¼ y
ð
Þ ¼
10
x
 
15
y
 
8
12xy


33
12
 
where 0x10, 0y12, and 4x+y12. The last constraint is needed because there are only
eight Hispanic Americans, so the combined minimum number of whites and African Americans
should be at least 4.
We follow the notation: P
x,y to denote P
x
P
y. The joint distribution of two random
variables has to satisfy the following conditions.
Theorem 3.3.1 If X and Y are two random variables with joint probability func-
tion f(x, y), then
1. f(x, y)0 for all x and y.
2. If X and Y are discrete, then P
x,y f(x, y)¼1,
where the sum is over all values (x, y) that are assigned nonzero probabilities. If X
and Y are continuous, then
ðð
f x, y
ð
Þdxdy ¼ 1:
140
CHAPTER 3 Additional Topics in Probability

Given the joint probability distribution (pdf or pmf), the probability distribution
function of a component random variable can be obtained through the marginals.
Definition 3.3.2 The marginal pmf of X denoted by fX(x) (or f(x), when there is
no confusion) is defined by
f X x
ð Þ ¼
ð1
1
f x, y
ð
Þdy, if X and Y are continuous
P
ally f x, y
ð
Þ,
if X and Y are discrete:
8
>
<
>
:
Similarly, the marginal pdf of Y is defined by
f Y y
ð Þ ¼
ð1
1
f x, y
ð
Þdy, if X and Y are continuous
P
ally f x, y
ð
Þ,
if X and Y are discrete:
8
>
<
>
:
Note that
P a  X  b
ð
Þ ¼
ða
b
f x
ð Þdy, if X and Y are continuous
P f X x
ð Þ, if X and Y are discrete:
8
<
:
where summation is over all values of X from a to b.
EXAMPLE 3.3.2
Find the marginal probability density function of the random variables X and Y, if their joint prob-
ability function is given by Table 3.1.
Find the marginal densities of X and Y.
Solution
By definition, the marginal pmfs of X are given by the column sums (summands over y for fixed x),
and the marginal pmfs of Y are obtained by the row sums. Hence,
xi
–1
3
5
otherwise
yi
2
0
1
4
otherwise
fX(xi)
0.5
0.4
0.1
0
fY(yi)
0.4
0.3
0.1
0.2
0
Using the joint probability distribution and the marginals, we can now introduce the
conditional probability distribution function.
Table 3.1 Joint pmf of X and Y
y
X
2
0
1
4
Sum
–1
0.2
0.1
0.0
0.2
0.5
3
0.1
0.2
0.1
0.0
0.4
5
0.1
0.0
0.0
0.0
0.1
Sum
0.4
0.3
0.1
0.2
1.0
141
3.3 Joint Probability Distributions

Definition 3.3.3 The conditional probability distribution of the random vari-
able X given Y is given by
f xjy
ð
Þ ¼ f xjY ¼ y
ð
Þ
¼
f x, y
ð
Þ
f Y y
ð Þ ,
if X and Y are continuous, f Y y
ð Þ 6¼ 0
P X ¼ x,Y ¼ y
ð
Þ
f Y y
ð Þ
, if X and Y are discrete,
8
>
>
<
>
>
:
We note that both the marginal probability densities of X and Y as well as the con-
ditional pdf must satisfy the two important conditions of a pdf.
We know that two events A and B are independent if P(A\B)¼P(A)P(B). It is
usually more convenient to establish independence through the probability functions.
Hence, we define independence for bivariate probability distribution as follows.
Definition 3.3.4 Let X and Y have a joint pmf or pdf f(x, y). Then X and Y are
independent if and only if
f x, y
ð
Þ ¼ f X x
ð Þf Y y
ð Þ, for all x and y:
That is, for independent random variables, the joint pdf is the product of the
marginals.
EXAMPLE 3.3.3
Let
f x, y
ð
Þ ¼
3x, 0  y  x  1
0,
otherwise:
	
(a) Find P X  1
2 , 1
4 < Y < 3
4


.
(b) Find the marginals fX(x) and fY(y).
(c) Find the conditional f(xjy) (0<y<1). Also compute f xjY ¼ 1
2


:
(d) Are X and Y independent?
Solution
(a) The domain of the function f(x, y) is given in Figure 3.8. The required probability
P X  1
2 , 1
4 < Y < 3
4


is the volume over the area of the shaded region as shown by
Figure 3.9. That is,
1
1
f(x,y) = 3x in
this region
x
y
FIGURE 3.8
Domain of f(x, y).
142
CHAPTER 3 Additional Topics in Probability

P X  1
2 , 1
4 < Y < 3
4
ð
Þ ¼
ð1
2
1
4
ðx
1
4
3xdydx
¼
ð1
2
1
4
3x x 1
4
ð
Þdx
¼
3x3
3 3x2
8

 1
2
1
4





¼ 5
128:
(b) To find the marginals, we note that for each x, y varies from 0 to x (0<y<x). Therefore
f X x
ð Þ ¼
ðx
0
3xdy ¼ 3x y x
0




¼ 3x2, 0 < x < 1:
Similarly, for each y, x varies from y to 1
f Y y
ð Þ ¼
ðx
0
3xdx ¼ 3x2
2
1
y




 ¼ 3
23y2
2
¼ 3
2 1y2


, 0 < y < 1:
(c) Using the definition of conditional density
f xjy
ð
Þ ¼ f x, y
ð
Þ
f Y y
ð Þ ¼
3x
3
2 1y2
ð
Þ ¼
2x
1y2 , y  x  1:
From this we have
f xjy ¼ 1
2


¼
2x
1
1
2ð Þ2 ¼ 8
3x,
1
2  x  1:
(d) To check for independence of X and Y
f X 1
ð Þf Y
1
2
 
¼ 3
ð Þ 9
8
 
¼ 27
8 6¼ 3 ¼ f
1, 1
2


:
Hence, X and Y are not independent.
FIGURE 3.9
Region of integration.
143
3.3 Joint Probability Distributions

Recall that in the case of a univariate random variable X, with probability func-
tion f(x), we have
EX ¼
X
x
xf x
ð Þ, if
X
x
xjf x
ð Þj < 1, for discrete r:v:
ð
xf x
ð Þdx, if
ð
jxjf x
ð Þdx < 1, for continuous r:v::
8
>
<
>
:
Now we define similar concepts for bivariate distribution.
Definition 3.3.5 Let f(x, y) be the joint probability function, and let g(x, y) be such
that P
x,yjg(x,y)jf(x,y)<1 in the discrete case, or
Ð
1
1 Ð
1
1 jg(x,y)jf(x,y)
dx dy<1 in the continuous case. Then the expected value of g(X, Y) is given by
Eg X, Y
ð
Þ ¼
X
x,y
g x, y
ð
Þf x, y
ð
Þ,
if X,Y are discrete
ð1
1
ð1
1
g x, y
ð
Þf x, y
ð
Þdxdy, if X,Y are continuous:
8
>
>
<
>
>
:
In particular
E XY
ð
Þ ¼
X
x,y
xyf x, y
ð
Þ,
if X,Y are discrete
ð1
1
ð1
1
xyf x, y
ð
Þdxdy if X,Y are continuous:
8
>
>
<
>
>
:
The following properties of mathematical expectation are easy to verify.
PROPERTIES OF EXPECTED VALUE
1. E(aX+bY)¼aE(X)+bE(Y).
2. If X and Y are independent, then E(XY)¼E(X)E(Y). However, the converse is not necessarily
true.
EXAMPLE 3.3.4
Let f(x, y)¼3x, 0yx1.
(a) Find E(4X3Y),
(b) Find E(XY).
Solution
(a) E(X)¼
Ð
x fX (x)dx and E(Y)¼
Ð
y fY (y)dy.
Recall that earlier (Example 3.3.3) we have computed fX(x)¼3x2 (0<x<1) and fY(y)¼
2 (1y2), 0y1. Using these results, we have
E X
ð Þ ¼
ð1
0
x3x2dx ¼ 3
4,
E Y
ð Þ ¼
ð1
0
y3
2 1y2


dy ¼ 3
8:
Hence,
E 4X 3Y
ð
Þ ¼ 39
8 ¼ 15
8 :
144
CHAPTER 3 Additional Topics in Probability

(b)
E XY
ð
Þ ¼
ð1
0
ðx
0
xy 3x
ð
Þdydx ¼ 3
10
Conditional expectations are defined in the same way as univariate expectations, except that the
conditional density is utilized in place of the unconditional density function.
Definition 3.3.6 Let X and Y be jointly distributed with pmf or pdf f(x, y). Let g be
a function of x. Then the conditional expectation of g(x) given, Y¼y is
E g X
ð Þjy
ð
Þ ¼ E g X
ð ÞjY ¼ y
ð
Þ
¼
X
allx
g x
ð Þf xjy
ð
Þ,
if X,Y are discrete
Ð
g x
ð Þf xjy
ð
Þdx, if X,Y are continuous:
8
<
:
and
Var X yj
ð
Þ ¼ E
Y E X yj
ð
Þ
ð
Þ2 yj
h
i
¼ E X2 yj


 E X yj
ð
Þ
½
2:
Note that E(g(X)jy) is a function of y. If we let Y range over all of its possible values,
the conditional expectation E(g(X)jY) can be thought of as a function of the random
variable Y. We will then be able to find the mean and variance of E(g(X)jY), as given
in the following result, the proof of which is left as an exercise.
Theorem 3.3.2 Let X and Y be two random variables. Then
(a) E(X)¼E[E(XjY)].
(b) Var(X)¼E[Var(XjY)]+Var[E(XjY)].
We can define the conditional variance, Var(YjX)¼E([YE(YjX)]2jX).
EXAMPLE 3.3.5
Let X and Y be two random variables with joint density function given by
f x, y
ð
Þ ¼
x2 + xy
3 , 0  x  1, 0  y  2
0,
otherwise:
(
Find the conditional expectation, E XjY ¼ 1
2


:
Solution
First we will find the conditional density, f(xjy). The marginal
f Y y
ð Þ ¼
ð1
0
x2 + xy
3


dx ¼ 1
3 + 1
6y, 0 < y < 2:
Therefore,
f xjy
ð
Þ ¼ f x, y
ð
Þ
f Y y
ð Þ ¼
x2 + xy
3
1
6 y + 1
3
, 0  x  1:
Continued
145
3.3 Joint Probability Distributions

Hence,
f
xjY ¼ 1
2


¼
x2 + x
6
1
12 + 1
3
¼ 12
5
x2 + x
6


:
Thus,
E XjY ¼ 1
2


¼
ð1
0
xf xjy
ð
Þdx
¼
ð1
0
x12
5
x2 + x
6


dx ¼ 11
15 ¼ 0:733:
EXAMPLE 3.3.6
Let the joint density of two random variables X and Y be given by
f x, y
ð
Þ ¼
1
4 2x + y
ð
Þ, 0  x  1, 0  y  2
0,
otherwise:
(
(a) Find fX(x) and fY(y).
(b) Find Var(X).
(c) Find E(XjY), and Var(XjY).
Solution
(a) We have
f X x
ð Þ ¼
ð1
1
f x, y
ð
Þdy ¼
ð2
0
1
4 2x + y
ð
Þdy
¼ 1
4 4x + 2
ð
Þ, 0  x  1:
Similarly, f Y y
ð Þ ¼
ð1
0
1
4 2x + y
ð
Þdx ¼ 1
4 1 + y
ð
Þ, 0  y  2:
(b) To find the variance,
E X
ð Þ ¼
ð1
0
1
4x 4x + 2
ð
Þdx ¼ 7
12,
E X2


¼
ð1
0
1
4x2 4x + 2
ð
Þdx ¼ 5
12:
Thus, the variance of X is
Var X
ð Þ ¼ E X2


 E X
ð Þ
½
2
¼ 5
12
7
12

2
¼ 11
144:
(c) First we will find the conditional density of X given that Y¼y,
f X Y
j
x yj
ð
Þ ¼ f x, y
ð
Þ
f Y y
ð Þ ¼
1
4 2x + y
ð
Þ
1
4 1 + y
ð
Þ
¼ 2x + y
ð
Þ
1 + y
ð
Þ , 0  x  1, 0  y  2:
146
CHAPTER 3 Additional Topics in Probability

Then the conditional expectation is given by
E X Y
j
½
 ¼
ð1
0
x 2x + y
ð
Þ
1 + y
ð
Þ dx ¼
1
1 + y
ð1
0
2x2 + xy


dx
¼
1
1 + y
2
3 + 1
2y


¼
1
6
  4 + 3y
ð
Þ
1 + y
ð
Þ :
For the conditional variance, we also need to find,
E X2 Y
j


¼
ð1
0
x2 2x + y
ð
Þ
1 + y
ð
Þ dx ¼
1
1 + y
ð1
0
2x3 + x2y


dx
¼
1
6
  3 + 2y
1 + y


:
Now,
Var X Y
j
½
 ¼ E X2 Y
j


 E X Y
j
ð
Þ
½
2
¼
1
6
  3 + 2y
1 + y



1
36

 4 + 3y
ð
Þ2
1 + y
ð
Þ2
¼ 3y2 + 6y + 2
36 1 + y
ð
Þ2 :
3.3.1 COVARIANCE AND CORRELATION
We will now define the covariance and correlation coefficient of two random
variables.
Definition 3.3.7
(i) The covariance between two random variables X and Y is defined by
sXY ¼ Cov X, Y
ð
Þ ¼ E X mX
ð
Þ Y mY
ð
Þ ¼ E XY
ð
ÞmXmY,
where mX¼E(X) and mY¼E(Y).
(ii) The correlation coefficient, r¼r(X, Y) is defined by
r ¼
Cov X, Y
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Var X
ð ÞVar Y
ð Þ
p
:
Correlation is the measure of the linear relationship between the random variables
X and Y. If Y¼aX+b (a6¼0), then jr(X,Y)j¼1. If dependence on X and Y needs to be
specified, we will use the notation, rXY or r(X,Y).
From the definition of the covariance of X and Y, we note that if small values of X,
for which (XmX)<0, tend to be associated with small values of Y, for which
(YmY)<0, and similarly large values of X with large values of Y, then Cov
(X, Y)E[(XmX)(YmY)] can be expected to be positive. On the other hand, if
small values of X tend to be associated with large values of Y and vice versa so that
(XmX) and (YmY) are of opposite signs, then Cov(X, Y)<0. Thus, covariance can
be thought of as a signed measure of the variation of Y relative to X. If X and Y are
independent, then it follows from the definition of covariance that Cov(X, Y)¼0.
147
3.3 Joint Probability Distributions

The correlation coefficient of X and Y, is a dimensionless quantity that measures the
linear relationship between the random variables X and Y.
PROPERTIES OF COVARIANCE AND CORRELATION COEFFICIENT
(a) 1r1.
(b) If X and Y are independent, then r¼0. The converse is not true.
(c) If Y¼aX+b, then
r X; Y
ð
Þ ¼
1,
if a > 0
1, if a < 0:
	
Note that Cov(X, X)¼Var(X).
(d) If U¼a1X+b1 and V¼a2Y+b2, then
(i) Cov(U, V)¼a1a2Cov(X, Y), and
(ii) rUV ¼
rXY,
rXY,
if a1a2 > 0
otherwise:
	
(e) Var(aX+bY)¼a2Var(X)+b2Var(Y)+2abCov(X, Y). In particular, if X and Y are independent,
then Var(aX+bY)¼a2Var(X)+b2Var(Y).
(f) If X1,. . .,Xn are independent, then Var(P
i¼1
n Xi)¼P
i¼1
n Var(Xi).
EXAMPLE 3.3.6
The joint probability density of the random variables X and Y is given by
f x, y
ð
Þ ¼
1
64ey=8, 0  x  y  1
0,
otherwise:
(
Find the covariance of X and Y.
Solution
We can use the formula, Cov(X, Y)¼E(XY)E(X)E(Y). Now using integration by parts (three times)
we will get
E XY
ð
Þ ¼
ð1
0
ðy
0
xy
ð
Þ 1
64ey=8dxdy
¼ 1
64
ð1
0
yey=8
ðy
0
xdx


dy
¼ 1
128
ð1
0
y3ey=8dy ¼ 192:
We can also obtain
E X
ð Þ ¼
ð1
0
ðy
0
x 1
64ey=8dxdy ¼ 8
and
E Y
ð Þ ¼
ð1
0
ðy
0
y 1
64ey=8dxdy ¼ 16:
Thus, Cov(X, Y)¼192(8)(16)¼64.
148
CHAPTER 3 Additional Topics in Probability

Next we will define the moment-generating function for the bivariate distributions.
Definition 3.3.8 Let X and Y be jointly distributed. Then the joint moment-
generating function is defined by
M X, Y
ð
Þ t1, t2
ð
Þ ¼ E et1X + t2Y
ð
Þ
¼
X
y
X
x
et1x + t1yf x, y
ð
Þ,
if X and Y are discrete
ð1
1
ð1
1
et1x + t2yf x, y
ð
Þdxdy,
if X and Y are continous:
8
>
>
<
>
>
:
EXERCISES 3.3
3.3.1. An experiment consists of drawing four objects from a container, which
holds eight operable, six defective, and 10 semioperable objects. Let X be
the number of operable objects drawn and Y the number of defective objects
drawn.
(a) Find the joint probability function of the bivariate random variable
(X, Y).
(b) Find P(X¼3, Y¼0).
(c) Find P(X<3, Y¼1).
(d) Give a graphical presentation of (a), (b), and (c).
3.3.2. Let
f x, y
ð
Þ ¼
1
50 x2 + 2y


, x ¼ 0,1,2,3 and y ¼ x + 3,
0,
otherwise:
(
Show that f(x, y) satisfies the conditions of a probability mass function.
3.3.3. Let
f x, y
ð
Þ ¼ c 1x
ð
Þ 1y
ð
Þ,
1  x  1, 1  y  1:
Find the c that makes f(x, y) the joint probability density function of the
random variable (X, Y).
3.3.4. Let
f x, y
ð
Þ ¼ xexy, x  0, y  1:
Is f(x, y) a probability density function? If not, find the proper constant to
multiply with f(x, y) so that it will be a probability density.
3.3.5. Find the marginal probability mass function of the random variables X and
Y, if their joint probability mass function is given in Table 3.3.1.
3.3.6. Find the marginal density functions of the random variables X and Y if their
joint probability density function is given by
f x, y
ð
Þ ¼
1
5 3xy
ð
Þ, 1  x  2, 1  y  3
0,
otherwise:
	
3.3.7. Determine the conditional probability P(X¼1jY¼0) for the random
variables defined in Exercise 3.3.5.
149
3.3 Joint Probability Distributions

3.3.8. Find k so that f(x, y)¼kxy, 1 xy2 will be a probability density
function. Also find (i) P X  3
2 ,Y  3
2


, and (ii) P X + Y  3
2


.
3.3.9. The random variables X and Y have a joint density
f x, y
ð
Þ ¼
8
9xy, 1  x  y  2
0,
elsewhere:
8
<
:
Find:
(a) The marginal of X.
(b) P(1.5<X<1.75, Y>1).
3.3.10. The joint pdf of X and Y is
f x, y
ð
Þ ¼
1
28 4x + 2y + 1
ð
Þ, 0  x  2, 0  y  2
0,
elsewhere:
8
<
:
Find (a) fX(x) and fY(y), and (b) f(yjx).
3.3.11. Find the joint mgf of the random variables (X, Y) defined in Exercise 3.3.9.
3.3.12. The joint density of a random variable (X, Y) is given by
f x, y
ð
Þ ¼
x3y3
16 , 0  x  2, 0  y  2
0,
elsewhere:
8
<
:
(a) Find marginals of X and Y, and (b) find f(yjx).
3.3.13. The joint probability mass function of a discrete random variable (X, Y) is
given by
f x, y
ð
Þ ¼
6xy
n n + 1
ð
Þ 2n + 1
ð
Þ

2
, x,y ¼ 1,2, ...,n
0,
otherwise:
8
<
:
Find (a) f(yjx), and (b) f(yjx).
[Hint:P
i¼1
n i2¼(n(n+1)(2n+1))/6.]
3.3.14. Consider bivariate random variables with the pmf
f x, y
ð
Þ ¼
 
n
x
!
yx + a1 1y
ð
Þnx + b1, for x ¼ 0, 1, ...,n and 0 < y  1:
Verify that
f xjy
ð
Þ∝
 
n
x
!
yx 1y
ð
Þnx
Table 3.3.1 Joint pmf of X and Y
y
X
–2
0
1
4
1
0.3
0.1
0.0
0.2
3
0.0
0.2
0.1
0.0
5
0.1
0.0
0.0
0.0
150
CHAPTER 3 Additional Topics in Probability

and
f yjx
ð
Þ∝yx + a1 1y
ð
Þnx + b1:
3.3.15. The joint mass function of the discrete random variable (X, Y) is given in
Table 3.3.2.
(a) Find E(XY).
(b) Find Cov(X, Y).
(c) Find the correlation coefficient rX,Y.
3.3.16. The joint probability function of the continuous random variable (X, Y) is
given by
f x, y
ð
Þ ¼
1
28 4x + 2y + 1
ð
Þ, 0  x < 2, 0  y < 2
0,
otherwise:
8
<
:
(a) Find E(XY).
(b) Find Cov(X, Y).
(c) Find the correlation coefficient rXY.
3.3.17. Let X and Y be random variables and U¼aX+b, V¼cY+d, where a, b, c, d
are constants. Show that rUV ¼
rXY,
ifac > 0
rXY, otherwise:
	
3.3.18. Let X and Y be random variables, and let Y¼aX+b, where a and b are
constants. Show that (a) rXY¼1 if a>0, and (b) rXY¼1 if a<0.
3.3.19. If jrXYj¼1, then prove that P(Y¼aX+b)¼1.
3.3.20. Let X and Y be two random variables with joint density function
f x, y
ð
Þ ¼
8xy, 0  x  y  1
0,
otherwise:
	
(a) Find the conditional expectation, E(XjY ¼ 3/4).
(b) Find Cov(X, Y).
3.3.21. Let X and Y be two random variables with joint density function
f x, y
ð
Þ ¼
ey,
0  x  y
0,
otherwise:
	
(a) Find the conditional expectation, E(XjY¼y).
Table 3.3.2 Joint Density of (X,Y)
y
X
1
2
3
1
1
6
1
6
1
6
2
1
6
1
12
1
12
3
1
12
1
12
0
151
3.3 Joint Probability Distributions

(b) Find Cov(X, Y).
(c) Are X and Y independent? Why?
3.3.22. Let
f x, y
ð
Þ ¼
c
1 + x2
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1y2
p
,
1 < x < 1, 1 < y < 1:
Find the c that makes f(x, y) the probability density function of the
random variable (X, Y). Determine whether X and Y are independent.
3.3.23. If the random variables X and Y are independent and have equal variances,
what is the coefficient of correlation between the random variables X and
aX+Y, where a is a constant?
3.4 FUNCTIONS OF RANDOM VARIABLES
In this section we discuss the methods of finding the probability distribution of a
function of a random variable X. We are given the distribution of X, and we are
required to find the distribution of g(X). There are many physical problems that call
for the derivation of the distribution of a function of a random variable. The follow-
ing is one of the classical examples. The velocity V of a gas molecule (Maxwell-
Boltzmann law) behaves as a gamma-distributed random variable. We would like
to derive the distribution of E¼mV2, the kinetic energy of the gas molecule. Because
the value of the velocity is the outcome of a random experiment, so is the value of E.
This is a problem of finding the distribution of a function of a random variable
E¼g(V). We now illustrate various techniques for finding the distribution of g(X)
by means of examples.
3.4.1 METHOD OF DISTRIBUTION FUNCTIONS
Basically the method of distribution functions is as follows. If X is a random variable
with pdf fX(x) and if Y is some function of X, then we can find the cdf FY(y)¼P(Yy)
directly by integrating fX(x) over the region for which {Yy}. Now, by differenti-
ating FY(y), we get the probability density function fY(y) of Y. In general, if Y is a
function of random variables X1, . . ., Xn, say g(X1, . . ., Xn), then we can summarize
the method of distribution function as follows.
PROCEDURE TO FIND CDF OF A FUNCTION OF r.v. USING THE METHOD
OF DISTRIBUTION FUNCTIONS
1. Find the region {Yy} in the (x1, x2, . . ., xn) space, that is find the set of (x1, x2, . . ., xn) for which
g(x1, . . ., xn)y.
2. Find FY (y)¼P(Yy) by integrating f (x1, x2, . . ., xn) over the region {Yy}.
3. Find the density function fY (y) by differentiating FY (y).
152
CHAPTER 3 Additional Topics in Probability

EXAMPLE 3.4.1
Let XN(0, 1). Using the cdf of X, find the pdf of X2.
Solution
Let Y¼X2. Note that the pdf of X is
f x
ð Þ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
ex2=2,
1 < x < 1:
Then the cumulative distribution function of Y for a given y0 is
F y
ð Þ ¼ P Y  y
ð
Þ ¼ P X2  y


¼ P 
ﬃﬃﬃy
p  X 
ﬃﬃﬃy
p


¼
ð ﬃﬃy
p
 ﬃﬃy
p
1ﬃﬃﬃﬃﬃﬃ
2p
p
ex2=2dx
¼
ð ﬃﬃy
p
0
1ﬃﬃﬃﬃﬃﬃ
2p
p
ex2=2dx, by the symmetry of ex2=2,
Hence, by differentiating F(y), we obtain the probability density function as
f Y y
ð Þ ¼
2ﬃﬃﬃﬃﬃﬃ
2p
p
ey=2 1
2
ﬃﬃﬃy
p
¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
y1=2ey=2, 0 < y < 1
0,
otherwise:
8
<
:
This is a w2-distribution with 1 degree of freedom.
The same method can be used for the discrete case.
EXAMPLE 3.4.2
Suppose that the random variable X has a Poisson probability distribution
f x
ð Þ ¼
ellx
x!
, x ¼ 0,1,2, ...
0,
otherwise:
(
Find the cumulative distribution function of Y¼aX+b.
Solution
The cdf of Y is given by
F y
ð Þ ¼ P Y  y
ð
Þ ¼ P aX + b  y
ð
Þ
¼ P X  yb
a


¼
X
yb
a
½

x¼0
ellx
x!
,
where [x] is the largest integer less than or equal to x. Therefore,
F y
ð Þ ¼
0,
y < b
X
yb
a
½

x¼0
ellx
x!
,
y  b:
8
>
<
>
:
It should be noted here that the pmf, fY(y) of Y, can be found from the equation
f Y y
ð Þ ¼ FY y
ð ÞFY y1
ð
Þ, fory ¼ an + b, n ¼ 0,1,2, ...
The multivariate case (in particular, the bivariate case), though more difficult, can be
handled similarly.
153
3.4 Functions of Random Variables

3.4.2 THE pdf OF Y=g(X), WHERE g IS DIFFERENTIABLE AND
MONOTONE INCREASING OR DECREASING
We now consider the distribution of a random variable Y¼g(X), where X is a con-
tinuous random variable with pdf fX(x). Assume that g is differentiable and the
inverse function g1 of g exists. Let X¼g1(Y). Let fX(x) be the probability density
function of X. Then the density function of Y can be obtained using the method just
given. Thus,
f Y y
ð Þ ¼ f X g1 y
ð Þ

 d
dyg1 y
ð Þ:
This is a special case of the transformation method, which is explained later in
Section 3.4.4.
EXAMPLE 3.4.3
Let XN(0, 1). Find the pdf of Y¼eX.
Solution
Here g(x)¼ex, and hence, g1(y)¼ln(y). Thus, d
dy g1 y
ð Þ ¼ 1
y:
Also,
f X x
ð Þ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
ex2=2,
1 < x < 1:
Therefore, the pdf of Y is
f Y y
ð Þ ¼
1
y
ﬃﬃﬃﬃﬃﬃ
2p
p
e ln y
ð Þ
½
2=2,
y > 0
0,
otherwise:
8
<
:
3.4.3 PROBABILITY INTEGRAL TRANSFORMATION
Let X be a continuous random variable, with pdf f and cdf F. Let Y¼F(X). Then,
P Y  y
ð
Þ ¼ P F X
ð Þ  y
ð
Þ ¼ P X  F1 y
ð Þ


¼
ðF1 y
ð Þ
1
f X x
ð Þdx ¼ FX x
ð ÞjF1 y
ð Þ
1
¼ y:
Hence,
f y
ð Þ ¼
1,
0 < y < 1
0, otherwise:
	
Thus, Y has a U(0,1) distribution. The transformation Y¼F(X) is called a probability
integral transformation. It is interesting to note that irrespective of the pdf of X, Y is
always uniform in (0, 1).
154
CHAPTER 3 Additional Topics in Probability

EXAMPLE 3.4.4
Let X be a normal with mean m and variance s2. Thus,
f x
ð Þ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
s
e xm
ð
Þ=2s2,
1 < x < 1, 1 < m < 1, and s2 > 0:
Let Y ¼
ðX
0
1ﬃﬃﬃﬃﬃﬃ
2p
p
se xm
ð
Þ=2s2du: Then Y¼F(X), where F is the cdf of a standard normal random
variable. Therefore Y is uniform on (0, 1). That is,
f y
ð Þ ¼
1, if 0 < y < 1
0, otherwise:
	
3.4.4 FUNCTIONS OF SEVERAL RANDOM VARIABLES: METHOD
OF DISTRIBUTION FUNCTIONS
We now discuss the distribution of Y, when Y is a function of several random vari-
ables, Y¼g(X1, . . ., Xn).
EXAMPLE 3.4.5
Let X1, . . ., Xn be continuous iid random variables with pdf f(x) (cdf F(x)). Find the pdfs of
Y1 ¼ min X1, ..., Xn
ð
Þ and Yn ¼ max X1, ..., Xn
ð
Þ:
Solution
For the random variable Y1, we have
1Fy1 y
ð Þ ¼ P Y1 > y
ð
Þ
¼ P X1 > y, X2 > y, ..., Xn > y
ð
Þ
¼ X1 > y
ð
ÞP X2 > y
ð
Þ...P Xn > y
ð
Þ
because of independence
ð
Þ
¼ 1F y
ð Þ
ð
Þn:
This implies
FY1 y
ð Þ ¼ 1 1F y
ð Þ
ð
Þn
and
f Y1 y
ð Þ ¼ n 1F y
ð Þ
ð
Þn1f y
ð Þ:
Consider Yn. Its cdf is given by
FY1 y
ð Þ ¼ P Yn  y
ð
Þ ¼ F y
ð Þ
ð
Þn:
This implies that
f Yn y
ð Þ ¼ n F y
ð Þ
ð
Þn1f y
ð Þ:
3.4.5 TRANSFORMATION METHOD
A simple generalization of the method of distribution functions to functions of more
than one variable is the transformation method. We illustrate the method for bivariate
distributions. The method is similar for the multivariate case. Let the joint pdf of
155
3.4 Functions of Random Variables

(X, Y) be f(x, y). Let U¼g1(X, Y); V¼g2(X, Y). The mapping from (X, Y) to (U, V) is
assumed to be one-to-one and onto. Hence, there are functions, h1 and h2 such that
x ¼ h1
1
u, v
ð
Þ,
and
y ¼ h1
2
u, v
ð
Þ:
Define the Jacobian of the transformation J by
J ¼
@x
@u
@x
@v
@y
@u
@y
@u
















:
Then the joint pdf of U and V is given by
f u, v
ð
Þ ¼ f h1
1
u, v
ð
Þ,h1
2
u, v
ð
Þ


Jj j:
EXAMPLE 3.4.6
Let X and Y be independent random variables with common pdf f(x)¼ex, (x>0). Find the joint pdf
of U¼X/(X+Y), V¼X+Y.
Solution
We have U¼X/(X+Y)¼X/V. Hence, X¼UV and Y¼VX¼VUV¼V(1U). Thus, the
Jacobian
J ¼
v
u
v 1u








:
Then jJj¼v(1u)+uv¼v(>0). Note that 0u1, 0<v<1.
f u, v
ð
Þ ¼ f h1
1
u, v
ð
Þ,h1
2
u, v
ð
Þ


Jj j
¼ euvev 1u
ð
Þv
¼ vev, 0  u  1, 0 < v < 1:
Suppose we want the marginal fV(v) and fU(v), that is,
FV v
ð Þ ¼
ð1
0
vevdu ¼ vev, 0 < v < 1
and
f U u
ð Þ ¼
ð1
0
vevdv ¼ 1,
0  u  1:
Sometimes the expressions for two variables, U and V, may not be given. Only one
expression is available. In that case, call the given expression of X and Y as U, and
define V¼Y. Then, we can use the previous method to first find the joint density and
then find the marginal to obtain the pdf of U. The following example demonstrates
the method.
156
CHAPTER 3 Additional Topics in Probability

EXAMPLE 3.4.7
Let X and Y be independent random variables uniformly distributed on [0, 1], Find the distribution
of X+Y.
Solution
Let
U ¼ X + Y,
V ¼ Y,
f x, y
ð
Þ ¼ 1, 0  x  1, 0  y  1,
X ¼ U V,
Y ¼ V,
J ¼ 1 1
0
1








 ¼ 1:
Thus, we have
f u, v
ð
Þ ¼
1,
0,
0  uv  1,
otherwise:
0  v  1
	
Because V is the variable we introduced, to get the pdf of U, we just need to find the marginal pdf
from the joint pdf. From Figure 3.10, the regions of integration are 0u1, and 0u2. That is,
Continued
(1,0)
(2,0)
v = u -1
u=v
v
u
FIGURE 3.10
The regions of integration.
FIGURE 3.11
Graph of fU(u).
157
3.4 Functions of Random Variables

f U u
ð Þ ¼ Ð f u,v
ð
Þdv ¼ Ð 1dv
¼
ðu
0
dv ¼ u,
0  u  1
ð1
u1
dv ¼ 2u, 0  u  2:
8
>
>
<
>
>
:
EXERCISES 3.4
3.4.1. Let X be a uniformly distributed random variable over (0, a). Find the pdf of
Y¼cX+d.
3.4.2. The joint pdf of (X, Y) is
f x, y
ð
Þ ¼ 1
y2 ex + y
y , x,y > 0, y > 0:
Find the pdf of U¼XY.
3.4.3. Let f(x,y) be the probability density function of the continuous random
variable (X, Y). If U¼XY, show that the probability density function of U is
given by
f U u
ð Þ ¼
ð1
1
f u
v, v

 1
v








dv
3.4.4. The joint pdf of X and Y is
f x, y
ð
Þ ¼ ye x + yy
ð
Þ, y > 0, x > 0:
Find the pdf of XY.
3.4.5. If the joint pdf of (X, Y) is
f x, y
ð
Þ ¼
1
2ps1s2
e
1
4s2
1s2
2 x2 + y2
ð
Þ,
1 < x < 1, 1 < y < 1; s1,s2 > 0
find the pdf of X2+Y2.
3.4.6. Let X1, . . ., Xn be independent and identically distributed random variables
with pdf f(x)¼(1/y)ex/y, x>0, y>0. Find the pdf of P
i¼1
n Xi.
3.4.7. Let f(x, y) be the pdf of the continuous random variable (X, Y). If U¼X+Y,
then show that the probability density function of U is given by
f U u
ð Þ ¼
ð1
1
f uv,v
ð
Þdv:
3.4.8. Let X be uniformly distributed over (2, 2) and Y¼X2. Find the Cov(X, Y).
Are X and Y independent?
3.4.9. Let XN(m, s2). Show that
(a) Z ¼ Xm
ð
Þ
s
isN 0, 1
ð
Þ:
(b) U ¼ Xm
ð
Þ2
s2
isw2 1
ð Þ:
158
CHAPTER 3 Additional Topics in Probability

3.4.10. Let XN(m, s2). Find the pdf of Y¼eX.
3.4.11. The probability density of the velocity, V, of a gas molecule, according to
the Maxwell-Boltzmann law, is given by
f v, b
ð
Þ ¼
cv2ebv2,
0,
	
v > 0
elsewhere,
where c is an appropriate constant and b depends on the mass of the mol-
ecule and the absolute temperature. Find the density function of the kinetic
energy E, which is given by E ¼ g V
ð Þ ¼ 1
2mV2:
3.4.12. Let X and Y be two independent random variables, each normally
distributed, with parameters (m1,s1
2)and(m2,s2
2), respectively. Show that the
probability density function of U¼X/Y is given by
f U u
ð Þ ¼
s1s2
p s2
1 + s2
2u2

,
1 < u < 1:
3.4.13. Let
f x, y
ð
Þ ¼
1
2ps2 e 1=2s2
ð
Þ x2 + y2
ð
Þ,
1 < x,y < 1
be the joint pdf of (X, Y). Let
U ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
X2 + Y2
p
and V ¼ tan1
Y
X
 
, 0  V  2p:
Find the joint pdf of (U, V).
3.4.14. Let the joint pdf of (X, Y) be given by
f x, y
ð
Þ ¼
b2e
x + y
ð
Þ=b
f
g,
0,
	
x,y > 0, b > 0
elsewhere:
Let U ¼ XY
2
and V¼Y Find the joint pdf of (U, V).
3.4.15. Let X and Y be independent and identically distributed random variables
with pdf
f x
ð Þ ¼
1
2ex=2,
0,
(
x  0
otherwise:
Find the distribution of (X–Y)/2.
3.4.16. If X and Y are independent and chi-square distributed random variables with
n1 and n2 degrees of freedom, respectively. Obtain the joint distribution of
(U, V), where U¼X+Y and V¼X/Y.
3.5 LIMIT THEOREMS
Limit theorems play a very important role in the study of probability theory and in its
applications. In Chapter 2, we saw that the frequency interpretation of probability
depends on the long-run proportion of times the outcome (event) would occur
159
3.5 Limit Theorems

in repeated experiments. Also, in Section 3.2, we learned that some binomial prob-
abilities can be computed using either the Poisson probability distribution or the nor-
mal probability distribution using the limiting arguments. Many random variables
that we encounter in nature have distributions close to the normal probability distri-
bution. These modeling simplifications are possible because of various limit theo-
rems. In this section, we discuss the law of large numbers and the Central Limit
Theorem.
First we give Chebyshev’s theorem, which is a useful result for proving limit the-
orems. It gives a lower bound for the area under a curve between two points that are
on opposite sides of the mean and are equidistant from the mean. The strength of this
result lies in the fact that we need not know the distribution of the underlying pop-
ulation, other than its mean and variance. This result was developed by the Russian
mathematician Pafnuty Chebyshev (1821-1894).
CHEBYSHEV’S THEOREM
Theorem 3.5.1 Let the random variable X have a mean m and standard deviation s. Then
for K>0, a constant,
P X m
j
j < Ks
ð
Þ  1 1
K2 :
Proof. We will work with the continuous case. By definition of the variance of X,
s2 ¼ E X m
ð
Þ2 ¼
ð1
1
xm
ð
Þ2f x
ð Þdx
¼
ð1Ks
1
xm
ð
Þ2f x
ð Þdx +
ð1 + Ks
1Ks
xm
ð
Þ2f x
ð Þdx +
ð1
1 + Ks
xm
ð
Þ2f x
ð Þdx

ð1Ks
1
xm
ð
Þ2f x
ð Þdx +
ð1
1 + Ks
xm
ð
Þ2f x
ð Þdx:
Note that (xm)2K2s2 for xmKs or xm+Ks. The equation above can be rewritten as
s2  K2s2
ð1Ks
1
f x
ð Þdx +
ð1
1 + Ks
f x
ð Þdx


¼ K2s2 P X  mKs
f
g + P X  m + Ks
f
g
½

¼ K2s2P Xm
j
j  Ks
f
g:
n
This implies that
P X m
j
j  Ks
f
g  1
K2
or
P X m
j
j < Ks
ð
Þ  1 1
K2 :
160
CHAPTER 3 Additional Topics in Probability

We can also write Chebyshev’s theorem as
P X m
j
j  e
f
g 
E
X m
ð
Þ2
h
i
e2
¼ Var X
ð Þ
e2
, for some e > 0:
Equivalently,
P X m
j
j  Ks
f
g  1
K2 :
In other words, Chebyshev’s inequality states that the probability that a random var-
iable X differs from its mean by at least K standard deviations is less than or equal to
1/K2 (K2, for K¼1, the result is obvious.)
In statistics, if we do not have any idea of the population distribution,
Chebyshev’s theorem is used in the following manner. For any data set (regardless
of the shape of the distribution), at least (1(1/k2))100% of observations will lie
within k(1) standard deviations of the mean. For example, at least (1(1/22))
100%¼75% of the data will fall in the interval x2s,x + 2s
ð
Þ and at least 88.9%
of the observations will lie within three standard deviations of the mean. If the pop-
ulation distribution is bell-shaped, we have a better result than Chebyshev’s theorem,
namely, the empirical rule that states the following: (i) approximately 68% of the
observations lie within one standard deviation of the mean; (ii) approximately
95% of the observations lie within two standard deviations of the mean; and (iii)
approximately 99.7% of the observations lie within three standard deviations of
the mean.
EXAMPLE 3.5.1
A random variable X has mean 24 and variance 9. Obtain a bound on the probability that the random
variable X assumes values between 16.5 and 31.5.
Solution
From Chebyshev’s theorem
P mKs < X < m + Ks
f
g  1 1
K2 :
Equating m+Ks to 31.5 and mKs to 16.5 with m¼24 and s ¼
ﬃﬃﬃ
9
p
¼ 3, we obtain K¼2.5.
Hence,
P 16:5 < X < 31:5
f
g  1
1
2:5
ð
Þ2 ¼ 0:84:
EXAMPLE 3.5.2
Let X be a random variable that represents the systolic blood pressure of the population of 18- to
74-year-old men in the United States. Suppose that X has mean 129 mm Hg and standard deviation
19.8 mm Hg.
(a) Obtain a bound on the probability that the systolic blood pressure of this population will assume
values between 89.4 and 168.6 mm Hg.
Continued
161
3.5 Limit Theorems

(b) In addition, assume that the distribution of X is approximately normal. Using the normal table,
find P(89.4X168.6). Compare this with the empirical rule.
Solution
(a) Because we are given only the mean and standard deviation, and no distribution is specified, we
use Chebyshev’s theorem. We have
P mKs < X < m + Ks
f
g  1 1
K2 :
Equating m+Ks to 168.6 and mKs to 89.4 with m¼129 and s¼19.8, we obtain K¼2. Hence,
P 89:4  X  168:6
f
g  1 1
2
ð Þ2 ¼ 0:75:
(b) Because X is normally distributed with mean 129 and standard deviation 19.8, using the z-score,
we get
P 89:4  X  168:6
ð
Þ ¼ P 89:4129
19:8
 Z  168:6129
19:8


¼ P 2  Z  2
ð
Þ ¼ 0:9544:
Hence, approximately 95.44% of this population will have systolic blood pressure values
between 89.46 and 168.6 mm Hg. This compares well with the 95% value from the empirical rule.
We could use Chebyshev’s inequality to prove the following result, which is
called the weak law of large numbers. The law of large numbers states that if the
sample size n is large, the sample mean rarely deviates from the mean of the distri-
bution of X, which in statistics is called the population mean.
LAW OF LARGE NUMBERS
Theorem 3.5.2 Let X1, . . ., Xn be a set of pairwise independent random variables with
E(Xi)¼m, and Var(Xi)¼s2. Then for any c>0,
P mc  X  m + c


 1 s2
nc2
and as n!1, the probability approaches 1. Equivalently,
P
Sn
n m








 < e


! 1
as n!1. Here, X ¼ 1
n
Xn
i¼1Xi and Sn¼P
i¼1
n Xi.
Proof. Because X1, . . ., Xn are independent and identically distributed (iid) random variables
(random sample), we know that Var(Sn)¼ns2, and Var(Sn/n)¼s2/n. Also, E(Sn/n)¼m. By
Chebyshev’s theorem, for any e>0,
P
Sn
n m








  e


 s2
ne2 :
Thus, for any fixed e,
P
Sn
n m








  e


! 0
as n!1. Equivalently,
P
Sn
n m








 < e


! 1
as n!1.
n
162
CHAPTER 3 Additional Topics in Probability

Thus, without any knowledge of the probability distribution function of Sn, the
(weak) law of large numbers states that the sample mean, X ¼ Sn=n, will differ from
the population mean by less than an arbitrary constant, e>0, with probability that
tends to 1 as n tends to 1. Because of this, the law of large numbers is also called
the “law of averages.” This result basically states that we can start with a random
experiment whose outcome cannot be predicted with certainty, and by taking aver-
ages, we can obtain an experiment in which the outcome can be predicted with a high
degree of accuracy. The law of large numbers in its simplest form for the Bernoulli
random variables was introduced by Jacob Bernoulli toward the end of the sixteenth
century. This result in generality was first proved by the Russian mathematician A.
Khintchine in 1929. This result is widely used in its applications to insurance, sta-
tistics, and the study of heredity.
EXAMPLE 3.5.3
Let X1, . . ., Xn be iid Bernoulli random variables with parameter p. Verify the law of large numbers.
Solution
For Bernoulli random variables we know that EXi¼p, and Var(Xi)¼p(1p). Thus, by Chebyshev’s
theorem,
P pc  X  p + c


¼ P
Sn
n p








  c
	

 1 s2
nc2
¼ 1p 1p
ð
Þ
nc2
! 1, as n ! 1:
This verifies the weak law of large numbers.
EXAMPLE 3.5.4
Consider n rolls of a balanced die. Let Xi be the outcome of the ith roll, and let Sn¼P
i¼1
n Xi. Show
that, for any e>0,
P
Sn
n 7
2








  e


! 0
as n!1.
Solution
Because the die is balanced, EXi¼7/2. By the law of large numbers, for any e>0,
P
Sn
n 7
2








  e


! 0
as n!1, or equivalently,
P
Sn
n 7
2








 < e


! 1
as n!1.
One of the most important results in probability theory is the Central Limit Theorem.
This basically states that the z-transform of the sample mean is asymptotically stan-
dard normal. The amazing thing about the Central Limit Theorem is that no matter
163
3.5 Limit Theorems

what the shape of the original distribution is, the (sampling) distribution of the mean
approaches a normal probability distribution. We state one version of the Central
Limit Theorem. In a restricted case, the proof uses the idea that the moment-
generating functions of Zn converge to the moment-generating function of the stan-
dard normal random variable. The general proof is a little bit more involved. Because
the proof of the Central Limit Theorem is available in most probability books, we
will not give the proof here.
CENTRAL LIMIT THEOREM (CLT)
Theorem 3.5.3 If X1, . . ., Xn is a random sample from an infinite population with mean
m<1, and variance s2<1, then the limiting distribution of Zn ¼ X m


= s=
ﬃﬃﬃn
p
ð
Þ as n!1 is
the standard normal probability distribution. That is,
lim
n!1 P Zn  z
ð
Þ ¼
1ﬃﬃﬃ
2
p
p
ðz
1
et2=2dt:
If Sn¼P
i¼1
n
Xi, then we can rewrite Zn as
Zn ¼ X m
s=
ﬃﬃﬃn
p ¼ n X m


ns=
ﬃﬃﬃn
p
,
¼ Sn nm
s
ﬃﬃﬃn
p
, since nX ¼
X
n
i¼1
Xi:
Then the CLT states that Zn ¼ Sn nm
ð
Þ=s
ﬃﬃﬃn
p is approximately N(0, 1) for large n.
The Central Limit Theorem basically says that when we repeat an experiment a
large number of times, the average (almost always) follows a Gaussian distribution.
EXAMPLE 3.5.5
X1, X2,. . . are iid random variables such that
Xi ¼
1,
0,
with probability p,
with probability 1p:
	
Show that Zn ¼ Sn np
ð
Þ=
ﬃﬃﬃﬃﬃﬃﬃﬃ
npq
p
is approximately normal for large n, where Sn¼P
i¼1
n
Xi, and
q¼1p.
Solution
We know that
E X
ð Þ ¼ p; E X2


¼ p;Var X
ð Þ ¼ pp2 ¼ pq:
Hence, by the CLT, the limiting distribution of Zn ¼ Sn np
ð
Þ=
ﬃﬃﬃﬃﬃﬃﬃﬃ
npq
p
as n!1 is the standard
normal probability distribution.
164
CHAPTER 3 Additional Topics in Probability

EXAMPLE 3.5.6
A soft-drink vending machine is set so that the amount of drink dispensed is a random variable with a
mean of 8 ounces and a standard deviation of 0.4 ounces. What is the approximate probability that
the average of 36 randomly chosen fills exceed 8.1 ounces?
Solution
From the CLT,
X8


= 0:4=
ﬃﬃﬃﬃﬃ
36
p




 N 0, 1
ð
Þ: Hence, from the normal table,
P X > 8:1


¼ P
Z > 8:18:0
0:4ﬃﬃﬃﬃ
36
p
(
)
¼ p Z > 1:5
f
g ¼ 0:0668:
EXAMPLE 3.5.7
Numbers in decimal form are often approximated by the closest integers. Suppose n numbers X1, . . .,
Xn are approximated by their closest integers J1, J2, . . ., Jn. Let Ui¼Xi–Ji. Assume that Ui are uni-
form on (0.5, 0.5) and that Ui0s are independent.
(a) Show that
Pn
i¼1Ui
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n=12
p
 N 0, 1
ð
Þas n ! 1:
(b) For n¼300, find P
5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
300=12
p

Pn
i¼1Ui
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
300=12
p

5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
300=12
p
8
<
:
9
=
;:
(c) For n¼300, find the value of a such that P{aPUia}¼0.95
(d) For n¼106, find a such that P a  P106
i¼1Ui  a
n
o
¼ 0:99:
Solution
(a) Because Ui0s are uniform in (0.5, 0.5), PUi¼0, Var(Ui)¼1/12. Let, Sn¼Si¼1
n Xi, and
Kn¼Si¼1
n Ji. Then
P Sn Kn
j
j  a
f
g ¼ P a  P Xi Ji
ð
Þ  a
f
g
¼ P a  PUi  a
f
g:
By the CLT, Sn
i¼1Ui 0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n=12
p
 N 0, 1
ð
Þasn ! 1:
(b) For n¼300; a¼5. Using the normal table,
P
5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
300=12
p

Pn
i¼1Ui
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
300=12
p

5
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
300=12
p
(
)
¼ 0:68:
(c) Now,
0:95 ¼ P a  PUi  a
f
g
¼ P
a
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
300=12
p
 Z 
a
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
300=12
p
(
)
:
From the normal table, we get
a
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
300=12
p
¼ 1:96: This implies, a¼9.8.
(d) We have
0:99 ¼ P a 
X
106
i¼1
Ui  a
(
)
¼ P
a
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
106=12
q
 Z 
a
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
106=12
q
8
>
<
>
:
9
>
=
>
;
Now, using the normal table, we have a=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
106=12
q
¼ 2:58: Hence, a¼745.
165
3.5 Limit Theorems

EXAMPLE 3.5.8
A casino has a coin, suspected to be biased. Estimate p (probability of heads) such that they can be
99% confident that their estimate (say, ^p) is within 0.01 of p (unknown). What is the minimum num-
ber of times we need to toss this coin?
Solution
Set
Xj
1,
0,
if H as jth toss,
if T as jth toss:
	
Suppose we decided to use ^p ¼
X
Xi
n , that is,
#Heads
n


:
We want P
X p




 < 0:01


¼ 0:99:
Because
Y¼P
i¼1
n XiBin(n,p),
we
have
EY¼np,
Var(Y)¼npq.
By
the
CLT,
X p


=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pq=n
p
 N 0, 1
ð
Þ: Now,
0:99 ¼ P
0:01
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pq=n
p
< X p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pq=n
p
< 0:01
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pq=n
p
(
)
¼ P
0:01
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pq=n
p
< Z < 0:01
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pq=n
p
(
)
:
Using the normal table,
0:01=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pq=n
p


¼ 2:58, this implies that
ﬃﬃﬃn
p  2:58
ﬃﬃﬃﬃﬃ
pq
p
=0:01


:
Because the maximum of pq¼1/4, it is sufficient that
ﬃﬃﬃn
p ¼ 2:58
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1=4
ð
Þ
p


0:01
¼ 129:
Hence, n¼(129)2¼16,641, and we should choose the sample size n16,641.
Note that the method used in Example 3.5.8 can be used to estimate any unknown
probability, not just unfair coin. Also, the fact that pq¼1/4 is maximum can be
shown by calculus: let f(p)¼pq¼p(1p)¼pp2. 0¼f’(p)¼12p implies
p¼1/2, so q¼1/2.
The Central Limit Theorem is extremely important in statistics because it says
that we can approximate the distribution of certain statistics without much of the
knowledge about the underlying distribution of that statistics for a relatively “large”
sample size. How large the n should be for this normal approximation to work
depends on the distribution of the original distribution. A rule of thumb is that the
sample size n must be at least 30. We deal with these issues in Chapter 4.
EXERCISES 3.5
3.5.1. Let X be a random variable with probability density function
f x
ð Þ ¼
630x4 1x
ð
Þ4,
0,
0 < x < 1,
otherwise:
	
(a) Obtain the lower bound given by Chebyshev’s inequality for
P{0.2<X<0.8}.
(b) Compute the exact probability, P{0.2<X<0.8}.
166
CHAPTER 3 Additional Topics in Probability

3.5.2. Suppose that the number of cars arriving in 1 h at a busy intersection is a
Poisson probability distribution with l¼100. Find, using Chebyshev’s
inequality, a lower bound for the probability that the number of cars arriving
at the intersection in 1 h is between 70 and 130.
3.5.3. Prove Chebyshev’s inequality for the discrete case.
3.5.4. Suppose that number of cars arriving at a busy intersection in a given 20 min
interval in a large city has a Poisson distribution with mean 120. Determine
a lower bound for the probability that the number of cars arriving in a given
20-min period will be between 100 and 140 using Chebyshev’s inequality.
3.5.5. Find the smallest value of n in a binomial distribution for which we can
assert that
P
Xn
n p








 < 0:1


 0:90:
3.5.6. How large should the size of a random sample be so that we can be 90%
certain that the sample mean X will not deviate from the true mean by more
than s/2?
3.5.7. Let a fair coin be tossed n times and let Sn be the number of heads that turn
up. Show that the fraction of heads, Sn/n, will be near to 1/2 for large n. What
can we conclude if the coin is not fair?
3.5.8. Suppose that a failure of certain component follows the distribution
f(x)¼px(1–p)x for x¼0, 1, and zero, elsewhere. How many components
must one test in order that the sample mean X will lie within 0.4 of the true
state of nature with probability at least as great as 0.95?
3.5.9. Let X1, . . ., Xn be a sequence of mutually independent random variables,
with probability distribution
P Xi ¼
ﬃﬃ
i
p


¼ 1
2
and P Xi ¼ 
ﬃﬃ
i
p


¼ 1
2:
Show that this sequence of random variables does not satisfy the
conditions of the law of large numbers.
3.5.10. Give a proof of the Central Limit Theorem.
3.5.11. Let X1, . . ., Xn be independent discrete random variables identically
distributed as
f xi
ð Þ ¼
0:2,
0,
	
xi ¼ 0,1,2,3,4,
otherwise:
Using CTL, find the approximate value of P X100 > 2


, where
X100 ¼ 1=100
ð
ÞS100
i¼1Xi:
3.5.12. Let X1, . . ., Xn be a sequence of independent Poisson-distributed random
variables, with parameter l. Let Sn¼Si¼1
n Xi. Show that
Zn ¼
Sn nl
ð
Þ=
ﬃﬃﬃﬃﬃﬃ
nl
p


 N 0, 1
ð
Þ:
3.5.13. Let X1, . . ., Xn be a sequence of independent uniformly-distributed over
[0,1) random variables. Let Sn¼Si¼1
n Xi. Show that
Zn ¼
Sn nl
ð
Þ=
ﬃﬃﬃﬃﬃﬃ
nl
p


 N 0, 1
ð
Þ:
167
3.5 Limit Theorems

3.5.14. Suppose that 2500 customers subscribe to a telephone exchange. There are 80
trunk lines available. Any one customer has the probability of 0.03 of needing
a trunk line on a given call. Consider the situation as 2500 trials with
probability of “success” p¼0.03. What is the approximate probability that
the 2500 customers will “tie up” the 80 trunk lines at any given time?
3.5.15. Suppose a group of people have an average IQ of 122 with standard
deviation 2. Obtain a bound on the probability that IQ values of this group
will be between 104 and 120.
3.5.16. Let X be a random variable that represents the diastolic blood pressure
(DBP) of the population of 18- to 74-year-old men in the United States who
are not taking any corrective medication. Suppose that X has mean
80.7 mm Hg and standard deviation 9.2.
(a) Obtain a bound on the probability that the DBP of this population will
assumes values between 53.1 and 108.3 mm Hg.
(b) In addition, assume that the distribution of X is approximately normal.
Using the normal table, find P(53.1X108.3). Compare this with the
empirical rule.
3.5.17. Color blindness appears in 2% of the people in a certain population. How
large must a random sample be in order to be 99% certain that a color-blind
person is included in the sample?
3.5.18. A shirt manufacturer knows that, on the average, 2% of his product will not
meet quality specifications. Find the greatest number of shirts constituting a
lot that will have, with probability 0.95, fewer than five defectives.
3.5.19. A random sample of size 100 is taken from a population with mean 1 and
variance 0.04. Find the probability that the sample mean is between 0.99
and 1.
3.5.20. The lifetime X (in hours) of a certain electrical component has the pdf
f(x)¼(1/3)e(1/3)x, x>0. If a random sample of 36 is taken from these
components, find P X < 2


:
3.5.21. A drug manufacturer receives a shipment of 10,000 calibrated
“eyedroppers” for administering the Sabin poliovirus vaccine. If the
calibration mark is missing on 500 droppers, which are scattered randomly
throughout the shipment, what is the probability that, at most, two defective
droppers will be detected in a random sample of 125?
3.6 CHAPTER SUMMARY
In this chapter we looked at some special distribution functions that arise in practice.
It should be noted that we discussed only a few of the important probability distri-
butions. There many other discrete and continuous distributions that will be useful
and appropriate in particular applications. Some of them are given in Appendix C.
A larger list of probability distributions can be found at http://www.causascientia.
org/math_stat/Dists/Compendium.pdf, among many other places. For more than
168
CHAPTER 3 Additional Topics in Probability

one random variable, we learned the joint distributions. We also saw how to find the
density and cumulative distribution for the functions of a random variable. Limit the-
orems are a crucial part of probability theory. We have introduced the Chebyshev’s
inequality, the law of large numbers, and the Central Limit Theorem for the random
variables.
We now list some of the key definitions introduced in this chapter:
•
Bernoulli probability distribution
•
Binomial experiment
•
Poisson probability distribution
•
Probability distribution
•
Normal (or Gaussian) probability distribution
•
Standard normal random variable
•
Gamma probability distribution
•
Exponential probability distribution
•
Chi-square (w2) distribution
•
Joint probability density function
•
Bivariate probability distributions
•
Marginal pdf
•
Conditional probability distribution
•
Independence of two r.v.s
•
Expected value of a function of bivariate r.v.s
•
Conditional expectation
•
Covariance
•
Correlation coefficient
In this chapter, we have also learned the following important concepts and
procedures:
•
Mean, variance, and moment-generating function (mgf) of a binomial random
variable
•
Mean, variance, and mgf of a Poisson random variable
•
Poisson approximation to the binomial probability distribution
•
Mean, variance, and mgf of a uniform random variable
•
Mean, variance, and mgf of a normal random variable
•
Mean, variance, and mgf of a gamma random variable
•
Mean, variance, and mgf of an exponential random variable
•
Mean, variance, and mgf of a chi-square random variable
•
Properties of expected value
•
Properties of the covariance and correlation coefficient
•
Procedure to find the cdf of a function of r.v. using the method of distribution
functions
•
The pdf of Y¼g(X), where g is differentiable and monotone increasing or
decreasing
•
The pdf of Y¼g(X), using the probability integral transformation
169
3.6 Chapter Summary

•
The transformation method to find the pdf of Y¼g(X1, . . ., Xn)
•
Chebyshev’s theorem
•
Law of large numbers
•
Central Limit Theorem (CLT)
3.7 COMPUTER EXAMPLES (OPTIONAL)
3.7.1 THE R-EXAMPLES
Example 3.7.1 PDFs and CDFs in R
R contains functions for many distribution functions with a logical format to
access each. This example will translate to other distributions such as Poisson and
normal however the examples will be with the binomial. Specifically in R they’re
4 command prefixes (p,q,r,d) p will return the probabilities while q returns values,
r generates random values from the distribution, and d returns the density. In the case
of R and these functions everything is cumulative and you will need to adjust for this
when seeking noncumulative probabilities. Using the help() function is recom-
mended since each distribution takes different arguments. “e.g., help(pbinom)”
R Code:
pbinom(c(0:5),5,0.4);
pbinom(3,5,0.4)-pbinom(2,5,0.4);
qbinom(0.5,5,0.4);
pnorm(4.2,4,2);
qnorm(0.5,4,2);
Output:
0.07776 0.33696 0.68256 0.91296 0.98976 1.00000
CDF Where X follows the
binomial distribution
0.2304
P(X=3)
0.5398278
P(X ≤4.2)
Where
X
follows
the
normal
distribution
4
CDF(X)=0.5
Example 3.7.2 Binomial Experiment
A manufacturer of a color printer claims that only 5% of their printers require
repairs within the first year. If out of a random sample of 18 of their printers, four
required repairs within the first year, does this tend to refute or support the
manufacturer’s claim?
R Code:
1-pbinom(3,18,0.05);
Output:
0.01087322
P(X ≥4) for the sample of 18 given p =0.05.
This is a very low probability suggesting
that we refute the claim. 
170
CHAPTER 3 Additional Topics in Probability

Example 3.7.3 Binomial Experiment
Suppose that a certain drug to treat a disease has a success rate of p¼0.65. This
drug is given to n¼500 patients with the disease.
(a) What is the probability that 335 or fewer show improvement?
(b) What is the probability that more than 320 show improvement?
(c) What is the probability that exactly 300 show improvement?
(d) What is the probability that the number of improvements lies in the interval
(300,350)?
R Code:
pbinom(335,500,0.65);
1-pbinom(320,500,0.65);
pbinom(300,500,0.65)-pbinom(299,500,0.65);
pbinom(349,500,0.65)-pbinom(300,500,0.65);
Output:
0.8375342
P(X<335)
0.6648447
P(X >320)
0.002462253
P(X = 300)
0.9784924
P(300< X<350)
3.7.2 MINITAB EXAMPLES
Minitab contains subroutines that can do pdf and cdf computations. For example, for
binomial random variables, the pdf and cdf can be respectively computed using the
following comments.
MTB>pdf k;
SUBC>binomial n p.
and
MTB>cdf;
SUBC>binomial n p.
Practice: Try the following and see what you get.
MTB>pdf 3;
SUBC>binomial 5 0.40.
will give
K
P(X¼K)
3.00
0.2304
And
MTB>cdf;
SUBC>binomial 5 0.40.
will give
BINOMIAL WITH N¼5 P¼0.400000
K P(X LESS OR ¼ K)
171
3.7 Computer Examples (Optional)

0 0.0778
1 0.3370
2 0.6826
3 0.9130
4 0.9898
5 1.0000
Similarly, if we want to calculate the cdf for a normal probability distribution
with mean k and standard deviation s, use the following comments.
MTB>cdf x;
SUBC>normal k s.
will give P(Xx).
Practice: Try the following.
MTB>cdf 4.20;
SUBC>normal 4 2.
We can use the invcdf command to find the inverse cdf. For a given probability
p, P(Xx)¼F(x)¼p, we can find x for a given distribution. For example, for
a normal probability distribution with mean k and standard deviation s, use the
following.
MTB>invcdf p;
SUBC>normal k s.
We can also use the pull-down menus to compute the probabilities. The following
example illustrates this for a binomial probability distribution.
EXAMPLE 3.7.1
A manufacturer of a color printer claims that only 5% of their printers require repairs within the first
year. If out of a random sample of 18 of their printers, four required repairs within the first year, does
this tend to refute or support the manufacturer’s claim? Use Minitab.
Solution
Type the numbers 1 through 18 in C1. Then
Calc > Probability Distributions > Binomial. . . > choose Cumulative probability > in
Number of trials, enter 18 and in Probability of success, enter 0.05 > in Input column: type
C1 > Click OK
The required probability is P(X4)¼1P(X3)¼10.9891¼0.0109.
Distribution checking
In order to perform right statistical analysis, it is necessary to know the distribu-
tion of the data we are using. We can use Minitab to do this by following steps.
1. Choose Stat > Quality Tools > Individual Distribution Identification.
2. Specify the column of data to analyze and the distribution to check
it against.
3. Click OK.
172
CHAPTER 3 Additional Topics in Probability

3.7.3 SPSS EXAMPLES
EXAMPLE 3.7.2
For the data of Example 3.7.1, using SPSS, find P(X3).
Solution
Enter numbers 1 through 18 in C1. Then use the following.
Transform>Compute>type in the Target Variable: y>Use the scroll bar beside the Functions
box to find CDF.BINOM(q, n, p)>Highlight it and use the up button to load it into the Numeric
Expression: box. Set q to 3 (success, the x-value), n to 18 (total trials) and p to 0.05 (probability of
success)>OK
In the second column, we will get the y-values as 0.99. Hence, P(X3)¼0.99.
We can use this procedure for many other distributions.
3.7.4 SAS EXAMPLES
Sometimes, we can use computer calculations to find out the exact probability
of a certain event in lieu of approximations. For example, when n is large in a binomial
experiment, we can use normal approximation to calculate the probabilities. The fol-
lowing example shows how to calculate binomial probabilities using SAS codes.
EXAMPLE 3.7.3
Suppose that a certain drug to treat a disease has a success rate of p¼0.65. This drug is given to
n¼500 patients with the disease.
(a) What is the probability that 335 or fewer show improvement?
(b) What is the probability that more than 320 show improvement?
(c) What is the probability that exactly 300 show improvement?
(d) What is the probability that the number of improvements lies in the interval (300, 350)?
Solution
Let X¼number of patients showing improvement. Then X is a binomial random variable with
parameters n¼500 and p¼0.65.
(a) First three lines in the following code are comment lines. In general, it is always helpful to
include the comment lines to explain about the program.
/*This program can be used to compute probability*/
/* that a Binomial variable with parameters p*/
/*and n is less than or equal to x*/
data binomial;
p¼0.65;
n¼500;
x¼335;
y¼probbnml(p,n,x);
cards;
proc print;
run;
Continued
173
3.7 Computer Examples (Optional)

(b) To calculate P(X>320), we can use the following.
data binomial;
p¼0.65;
n¼500;
x¼320;
y¼probbnml(p,n,x);
z¼1–y;
cards;
proc print;
run;
(c) To find P(X¼300), we can use the following.
data binomial;
p¼0.65;
n¼500;
x1¼300;
y1¼probbnml(p,n,x1);
x2¼299;
y2¼probbnml(p,n,x2);
z¼y1y2;
cards;
proc print;
run;
(d) To find P(300<X<350), use the following.
data binomial;
p¼0.65;
n¼500;
x1¼300;
y1¼probbnml(p,n,x1);
x2¼349;
y2¼probbnml(p,n,x2);
z¼y2y1;
cards;
proc print;
run;
Similar procedures could be used to calculate probabilities for other distributions.
In order to test for normality of a given data set using a normal probability plot,
we can use PROC UNIVARIATE (see Chapter 1 for explanation) in the following
manner. Normal plot is called qqplot in SAS.
proc univariate data¼K noprint; /*Specify the name of data set as K*/
qqplot standard;
run;
quit;
Note that this avoids printing of all the standard output due to the univariate com-
mand, and we get only the QQ plot. If we need a straight line in the plot, we can
modify the commands as follows.
174
CHAPTER 3 Additional Topics in Probability

proc univariate data¼K noprint; /*Specify the name of data set as B*/
qqplot standard/ normal (mu¼m, sigma¼s);
run;
quit;
PROJECTS FOR CHAPTER 3
3A. MIXTURE DISTRIBUTION
In statistical modeling, if the data are contaminated by outliers or if the samples
are drawn from a population formed by a mixture of two populations, one could
use mixture distributions. Mixture distributions are used frequently in medical
applications, such as micro array analysis. Suppose a random variable X has pdf
f1(x) with probability p1 and pdf f2(x) with probability p2, where p1+p2¼1. Then
we say that the r.v. X has a mixture distribution. This can be thought of as
observing a Bernoulli random variable Z that is equal to 1 with probability p1 and
2 with probability p2. Thus,
X ¼
X1  f 1 x
ð Þ, if Y ¼ 1
X2  f 2 x
ð Þ, if Y ¼ 2:
	
(a) Show that the pdf of X is given by f(x)¼p1f1(x)+p2f2(x).
(b) If (m1,s1
2) and (m2,s2
2) are means and variances of f1(x) and f2(x), respectively,
show that
m ¼ E X
ð Þ ¼ p1m1 + p2m2,
and
s2 ¼ Var X
ð Þ ¼ p1s2
1 + p2s2
1 + p1m2
1 + p2m2
2  p1m1 + p2m2
ð
Þ2:
3B. GENERATING SAMPLES FROM EXPONENTIAL AND POISSON
PROBABILITY DISTRIBUTION
(a) Generate a sample from 1
yex=y (y is chosen). Let Y1, Y2, ..., Yn be a sample
from a U(0, 1) distribution. Let F(x)¼1–ex/y (cdf of exponential). Then
Y¼F(x) is uniform. yj¼1e–x/y implies xj¼y ln(1yi)¼y ln ui, where
u1, u2, ..., un is a sample from U(0, 1). Then X1, ..., Xn is a sample from an
exponential distribution with parameter y.
(b) Suppose we want to generate a sample from a Poisson probability
distribution with parameter l. X1, . . ., Xn is a sample from an exponential
distribution with parameter 1/l till P
i¼1
n
Xi just exceeds 1. Then yn(n1)
is a sample values form a Poisson probability distribution with
parameter l.
175
Projects for Chapter 3

Exercise 3B
Let u1, u2, . . ., un be a sample from U(0, 1). Show that
(i) X¼2P
i¼1
n
ln(ui)w2n
2 ,
(ii) X¼bP
i¼1
a
ln(ui)Gamma(a,b), and
(iii) X ¼
Xa
i¼1 ln ui
ð Þ
Xa + b
i¼1 ln ui
ð Þ
 Beta a, b
ð
Þ:
(iv) Search internet and create a list of transformations that uses uniform
random variable to generate random variables from other distributions.
Discuss computational efficiency of such methods.
3C. COUPON COLLECTOR’S PROBLEM
Suppose there are n distinct colors of coupons. Each color of coupon is equally likely
to occur. When a complete set of coupons with each color represented is assembled,
you win a prize. Let X¼# coupons for a complete set. Find (a) Distribution of X, (b)
E(X), and (c) Var(X).
3D. RECURSIVE CALCULATION OF BINOMIAL AND POISSON
PROBABILITIES
A simple way to calculate binomial probabilities is as follows: For a given n and
p, evaluate b(0,n,p) and then apply the recursive relationship
b x + 1, n, p
ð
Þ ¼ b x, n, p
ð
Þ
p nx
ð
Þ
1p
ð
Þ x + 1
ð
Þ
to obtain other binomial probabilities.
(a) Derive this recursion formula.
(b) For n¼15, p¼0.4, using the recursive formula, compute all other
probabilities starting from x¼0.
The following recursive formulas are very useful in calculating
successive Poisson probabilities:
f x1,l
ð
Þ ¼ f x, l
ð
Þ x
l
and
f x + 1,l
ð
Þ ¼ ellx + 1
x + 1!
ð
Þ ¼ f x, l
ð
Þ l
x + 1:
For example, if l¼2.5, we know that f(0,2.5)¼e2.5¼0.08208. Using
this, calculate (c) f(1, 2.5) and f(2, 2.5).
3E. SIMULATION OF POISSON APPROXIMATION OF BINOMIAL
Write and run R-code with various n and p to see how the errors compare as n
increases and p decreases, by calculation actual binomial probabilities as well as
Poisson probabilities with l¼np.
176
CHAPTER 3 Additional Topics in Probability

CHAPTER
Sampling Distributions
4
CHAPTER CONTENTS
4.1 Introduction .................................................................................................... 178
4.2 Sampling Distributions Associated with Normal Populations .............................. 184
4.3 Order Statistics ............................................................................................... 200
4.4 Large Sample Approximations .......................................................................... 205
4.5 Chapter Summary ............................................................................................ 210
4.6 Computer Examples ......................................................................................... 211
Projects for Chapter 4 ............................................................................................ 215
OBJECTIVE
In this chapter we study the probability distributions of various sample statistics such
as the sample mean and the sample variance and illustrate their usefulness.
Abraham de Moivre
(Source: http://en.wikipedia.org/wiki/Abraham_de_Moivre#mediaviewer/File:Abraham_de_moivre.jpg)
Abraham de Moivre (1667-1754) was a French mathematician known for his
work on the normal distribution and probability theory. He is famous for de Moivre’s
formula, which links complex numbers and trigonometry. He fled France and went to
Mathematical Statistics with Applications in R
Copyright © 2015 Elsevier Inc. All rights reserved.
177

England to escape the persecution of Protestants. In England he wrote a book on
probability theory, titled The Doctrine of Chances. This book was very popular
among gamblers. The normal distribution was first introduced by de Moivre in an
article in 1733 in the context of approximating certain binomial distributions for
large n, and this approximation result is now called theorem of de Moivre-Laplace.
4.1 INTRODUCTION
Sampling distributions play a very important role in statistical analysis and deci-
sion making. We begin with studying the distribution of a statistic computed from a
random sample. Based on the probabilistic foundation of Chapters 2 and 3, the pre-
sent study marks the beginning of our learning of statistics beyond the descriptive
phase. Because a random sample is a set of random variables X1, . . ., Xn, it follows
that a sample statistic that is a function of the sample is also random. We call the
probability distribution of a sample statistic its sampling distribution. Sampling
distributions provide the link between probability theory and statistical inference.
The ability to determine the distribution of a statistic is a critical part in the con-
struction and evaluation of statistical procedures. It is important to observe that
there is a difference between the distribution of population from which the sample
was taken and the distribution of the sample statistic. In general, a population has a
distribution called a population distribution, which is usually unknown, whereas a
statistic has a sampling distribution, which is usually different from the population
distribution. The sampling distribution of a statistic provides a theoretical model of
the relative frequency histogram for the likely values of the statistic that one would
observe through repeated sampling. Even though some of the terms in this section
have already been defined in Chapter 1, we now present these definitions in terms
of random variables. These abstractions are introduced to develop scientifically
based methods of analyzing the data, and one should always keep in mind the
underlying population.
Definition 4.1.1 A sample is a set of observable random variables X1, . . ., Xn. The
number n is called the sample size.
In most of the inferential procedures that we study in this book, we are dealing
with random samples. We call the random variables X1, . . ., Xn identically distributed
if every Xi has the same probability distribution.
Definition 4.1.2 A random sample of size n from a population is a set of n inde-
pendent and identically distributed (iid) observable random variables X1, . . ., Xn.
Note that in a sample (not a random sample), Xis need not be independent or
identically distributed. For the results of this book to be applicable, it is important
to ensure that the selection of a sample is at least approximately random. The
significance of random sampling is that the probability distribution of a statistic
can be easily derived. Random sampling helps us to control systematic basis. For
a finite population, one can serially number the elements of the population and then
select a random sample with the help of a table of random digits. One of the simplest
178
CHAPTER 4 Sampling Distributions

ways to select a random sample of finite size is to use a table of random numbers.
When the population size is very large, such a method can become very taxing and
sometimes practically impossible. However, there are excellent computer programs
for generating random samples from large populations, and these programs can be
used. Now we define a statistic.
Definition 4.1.3 A function T of observable random variables X1, . . ., Xn that does
not depend on any unknown parameters is called a statistic.
The sample mean X ¼ 1=n
ð
Þ
Xn
i¼1Xi is a function of X1, ..., Xn. The sample
median and sample variance S2 are also examples of statistics. It is important to
observe that even with random sampling, there is sampling variability or error. That
is, if we select different samples from the same population, a statistic will take different
values in different samples. Thus, a sample statistic is a random variable, and hence it
has a probability distribution. In order for us to study the behavior of the phenomenon
a sample statistic represents, we must identify its probability distribution.
Definition 4.1.4 The probability distribution of a sample statistic is called the
sampling distribution.
We can illustrate these definitions with the following example with a finite pop-
ulation and a finite sample size. In this case, we take all possible samples of size n
from a population of size N.
EXAMPLE 4.1.1
Let the population consist of the numbers {1, 2, 3, 4, 5}. Consider all possible samples consisting of
three numbers randomly chosen without replacement from this population. Obtain the distribution of
the sample mean.
Solution
Disregarding the order, it is clear that there are
5
3
 
¼ 10 equally likely possible samples of size 3.
They are (1,2,3), (1,2,4), (1,2,5), (1,3,4), (1,3,5), (1,4,5), (2,3,4), (2,3,5), (2,4,5), and (3,4,5).
Calculating the mean, X,for each of the samples, we will get the sampling distribution of X as
x
2
1
7
3
8
3
3
1
10
3
11
3
4
1
p x
ð Þ
1
10
1
10
2
10
2
10
2
10
1
10
1
10
For example, in the table, P X ¼ 8=3


¼ 2=10 because the two samples (1,2,5) and (1,3,4) both
give an x ¼ 8=3, which is an estimate of the population mean, m.
In general, sampling distributions are theoretical distributions that consist of possibly an infinite
number of sample statistics taken from an infinite number of randomly selected samples of a fixed
sample size. For example, if a sample of size n¼30 were taken from a large population an infinite
number of times, the combined means taken from all the samples would make up the sampling dis-
tribution of the mean. Every sample statistic has a sampling distribution. The next result states that
if one selects a random sample from a population with mean m and variance s2, then regardless of
the form of the population distribution, one can obtain the mean and standard deviation of the
statistic X in terms of the mean and standard deviation of the population. This is explained in
the following result.
179
4.1 Introduction

Theorem 4.1.1 Let X1, . . ., Xn be a random sample of size n from a population
with mean m and variance s2. Then E X
 
¼ m and Var X
 
¼ s2=n.
Proof. The mean and variance of X is given by,
E X
 
¼ E 1
n
X
n
i¼1
Xi
 
!
¼ 1
n
X
n
i¼1
E Xi
ð
Þ
¼ 1
n
X
n
i¼1
m ¼ 1
nnm ¼ m,
and
Var X
 
¼ Var 1
n
X
n
i¼1
Xi
 
!
¼ 1
n2
X
n
i¼1
Var Xi
ð
Þ besause X0
is are independent and Var aXi
ð
Þ ¼ a2Var Xi
ð
Þ


¼ 1
n2 ns2 ¼ s2
n :
We denote E X
 
¼ mX and Var X
 
¼ sX
2: Note that from the previous theorem, mX ¼
m and sX ¼ s=
ﬃﬃﬃn
p : Here, sX is called the standard error of the mean. It is important to
notice that the variance of each of the random variables X1, X2, ..., Xn is s2, whereas
the variance of the sample mean X is s2/n, which is smaller than the population var-
iance s2 for n2.
The implication of Theorem 4.1.1 is that the sample means become more and
more reliable as an estimate of m as the sample size is increased, as we would expect.
From Chebyshev’s inequality,
P jX mXj < ksX


 1 1
k2 :
Let e ¼ ks=
ﬃﬃﬃn
p
ð
Þ: Then k ¼ e
ﬃﬃﬃn
p
ð
Þ=s: Since mX ¼ m, the above inequality can be
written as
P jX mj < e


 1 s2
ne2 :
Thus, for any e>0, the probability that the difference between X and m less than e can
be made arbitrarily close to 1 by choosing the sample size n is sufficiently large. We
illustrate this result in the following example.
▄
EXAMPLE 4.1.2
A particular brand of drink has an average of 12 ounces per can. As a result of randomness, there will
be small variations in how much liquid each bottle really contains. It has been observed that the
amount of liquid in these bottles is normally distributed with s¼0.8 ounce. A sample of 10 bottles
of this brand of soda is randomly selected from a large lot of bottles, and the amount of liquid, in
ounces, is measured in each. Find the probability that the sample mean will be within 0.5 ounce of
12 ounces.
180
CHAPTER 4 Sampling Distributions

Solution
Let X1, X2, . . ., X10 denote the ounces of liquid measured for each of the bottles. We know that Xis are
normally distributed with mean m¼12 and variance s2¼0.64. From Theorem 4.1.1, X possesses a
normal distribution (actually, for the normality part, we use Corollary 4.2.2) with a mean 12 and
variance s2/n¼0.64/10¼0.064. We find
P jX 12j  0:5


¼ P 0:5  X 12


 0:5


¼ P  0:5
s=
ﬃﬃﬃn
p  X 12
s=
ﬃﬃﬃn
p
 0:5
s=
ﬃﬃﬃn
p


¼ P  0:5
0:253  Z  0:5
0:253


¼ P 1:97  Z  1:97
ð
Þ
¼ 0:9512 using standarad normal table
ð
Þ:
Hence, the chance is about 0.95% that the mean amount of drink in any 10 bottles randomly
chosen will be between 11.5 and 12.5 ounces.
4.1.1 FINITE POPULATION
Let {c1, c2, . . ., cN} be a finite population. Then the population mean
m¼(1/N)P
i¼1
N ci and the population variance s2¼(1/N)P
i¼1
N (cim)2. The follow-
ing theorem for the sample mean and variance is stated without proof.
Theorem 4.1.2 If X1, . . ., Xn is a sample of size n (chosen without replacement)
from a population {c1,c2, . . ., cN}, then
E X
 
¼ m,
Var X
 
¼ s2
n
N n
N 1


:
We remark here that the sample in the theorem is not a random sample and Xis are not
iid random variables. The factor (Nn)/(N1) in the foregoing theorem is often
called the finite population correction factor. It is close to 1 unless the sample
amounts to a significant portion of the population. Note that the sampling without
replacement causes dependence among the Xis. However, if the sample size n is
small relative to the population size N, the population correction factor is approxi-
mately 1. Hence, we will not use the finite population correlation factor in the der-
ivation of sampling distribution, unless it is absolutely necessary.
EXAMPLE 4.1.3
Obtain the mean and variance of X in Example 4.1.1.
Solution
First note that for the population in Example 4.1.1, the population mean is m¼(1/N)P
i¼1
N ci¼3 and
the population variance is s2¼(1/N)P
i¼1
N (cim)2¼2. Applying the probability distribution of X
given in Example 4.3.1, we obtain
Continued
181
4.1 Introduction

E X
 
¼ 2
1
10


+ 7
3
1
10


+ 8
3
2
10


+ 3
2
10


+ 10
3
2
10


+ 11
3
1
10


+ 4
1
10


¼ 3,
and
Var X
 
¼ E X
2


E X
2


¼ 22
1
10


+
7
3
 2
1
10


+
8
3
 2
2
10


+ 32
2
10


+
10
3

2
2
10


+
11
3

2
1
10


+ 42
1
10


32
¼ 2
31
2 ¼ 0:3333:
This is the same as (s2/n). [(Nn)/(N1)]. In this case we observe that the variance of X is
precisely one sixth of the original variance.
EXAMPLE 4.1.4
Let X1, . . ., Xn be a random sample from a population with mean m and variance s2. Consider the
sample variance
S2 ¼
1
n1
X
n
i¼1
Xi X

2:
Show that E(S2)¼s2.
Solution
It can be shown that (see Exercise 1.5.8)
1
n1
X
n
i¼1
Xi X

2 ¼
Xn
i¼1X2
i nX
2
n1
:
Hence,
E S2


¼ E
Xn
i¼1X2
i nX
2
n1
 
!
¼
1
n1
X
n
i¼1
E X2
i



n
n1E X
2


:
Using the fact that E(X2)¼Var(X)+m2 and Theorem 4.1.1, we have
E S2


¼
1
n1n s2 + m2



n
n1
s2
n + m2


¼
n
n1
1
n1


s2 +
n
n1
n
n1


m2
¼ s2:
This shows that the expected value of the sample variance is the same as the variance of the
population under consideration.
182
CHAPTER 4 Sampling Distributions

EXERCISES 4.1
4.1.1. Let the population be given by the numbers {2, 1, 0, 1, 2}. Take all
random samples of size 3.
(a) Without replacement, obtain the following in each case.
(i) The sampling distribution of the sample mean.
(ii) The sampling distribution of the sample median.
(iii) The sampling distribution of the sample standard deviation.
(iv) The mean and variance of the sample mean.
(b) How many samples of size 3 can we get, if we sample with
replacement?
4.1.2. (a) How many different samples of size n¼2 can be chosen from a finite
population of size 12 if the sampling is without replacement?
(b) What is the probability of each sample in part (a), if each sample of size
2 is equally likely?
(c) Find the value of the finite population correction factor.
4.1.3. Let the population be given by {1, 2, 3}. Let p(x)¼1/3 for x¼1, 2, 3. Take
samples of size 3 with replacement.
(a) Calculate m and s2.
(b) Obtain the sampling distribution of the sample mean.
(c) Obtain the mean and variance of the sample mean.
4.1.4. Find the value of the finite population correlation factor for (a) n¼8 and
N¼60.(b) n¼8 (c) N¼1000.(d) n¼15 and N¼60.
4.1.5. For a random sample X1, . . ., Xn, let S0
ð Þ2 ¼ 1=n
ð
Þ
Xn
i¼1 Xi X

2: Find
E[(S0)2]. Compare this with E(S2).
4.1.6. For a random sample X1, . . ., Xn with mean m and variance s2, let
Tn¼P
i¼1
n Xi, the sample total. Show that E(Tn)¼nm and Var(Tn)¼ns2.
4.1.6. A particular brand of sugar is sold in 5-lb packages. The weight of sugar in
these packages can be assumed to be normally distributed with mean
m¼5 lb and standard deviation s¼2 lb. What is the probability that the
mean weight of sugar in 15 randomly selected packages will be within
0.2 lb of 5 lb?
4.1.8. A random sample of size 150 is taken from an infinite population having the
mean m¼15 and standard deviation s¼2.5. What is the probability that X
will be between 10.5 and 18.5?
4.1.9. The distribution of heights of all students in a large university has a normal
distribution with a mean of 66 inches and a standard deviation of 2 inches.
What is the probability that the mean height of 26 randomly selected
students from this university will be more than 70 inches?
4.1.10. An image-encoding algorithm, when used to encode images of a certain
size, uses a mean of 110 milliseconds with a standard deviation of 15
milliseconds. What is the probability that the mean time (in milliseconds)
for encoding 50 randomly selected images of this size will be between 90
and 135 milliseconds? What assumptions do we need to make?
183
4.1 Introduction

4.1.11. In order to evaluate a new release of a database management system, a
database administrator runs a benchmark program several times and
measures the time to completion in seconds. Assuming that the distribution
of times is normal with mean 95 seconds and with standard deviation of
10 seconds, what proportion of measurement times will fall below
85 seconds?
4.1.12. A population of disk drives manufactured by a certain company runs with
mean seek time of 10 milliseconds with standard deviation of 0.1
milliseconds. What proportion of samples of size 250 would you expect to
result in a mean less than 9 milliseconds? What assumptions do we need
to make?
4.1.13. Suppose that the national norm of a science test for 12th graders on a
particular year has a mean of 215 and a standard deviation of 35.
(a) A random sample of 55 12th graders is selected. What is the probability
that this group will average more than 230?
(b) A random sample of 200 12th graders is selected. What is the
probability that this group will average over 230?
(c) A random sample of 35 12th graders is selected. What is the probability
that this group will average over 230?
(d) How does the sample size influence the probability?
4.1.14. Scores on the Wechsler Adult Intelligence Scale for the 20 to 34 age group
are approximately normally distributed with mean equal to 110 and standard
deviation equal to 25. If we select 100 people at random, what is the
probability that this group will have an average score of 125 or above?
4.1.15. It is known that a healthy human body has an average temperature of
98.6 F, with a standard deviation of 0.95 F. Sixty healthy humans are
selected at random. What is the probability that their temperatures average
at least 99.1 F?
4.2 SAMPLING DISTRIBUTIONS ASSOCIATED WITH
NORMAL POPULATIONS
The sampling distribution of a statistic will depend upon the population distribution
from which the samples are taken. In this section we discuss the sampling distribu-
tions of some statistics that are based on a random sample drawn from a normal dis-
tribution. These statistics are used in many statistical procedures that are very
important in solving real-world problems. The following result establishes the dis-
tribution of a linear combination of independent normal random variables.
Theorem 4.2.1 Let X1, . . ., Xn be independent random variables with the distri-
bution of Xi being normal with mean mi and variance si
2. Let a1, a2, . . ., an be real
constants. Then the distribution of Y¼P
i¼1
n aiXi is normal with mean mY¼P
i¼1
n aimi
and variance sY
2 ¼P
i¼1
n ai
2si
2.
184
CHAPTER 4 Sampling Distributions

Proof. The moment-generating function of Y is given by
MY tð Þ ¼ Ee
Xn
i¼1aiXi


t
¼
Y
i
Ee
aiXi
ð
Þt
by independence of Xi’s
½

¼
Y
i
Ee
ait
ð
ÞXi
¼
Y
iMXi ait
ð
Þ
using the definition of mgf
½

¼
Y
ie aimit + 1=2
ð
Þa2
i s2
i t2
ð
Þ
using mgf of a normal
½

¼ e Siaimi
ð
Þt + 1=2
ð
Þ Sia2
i s2
i
ð
Þt2
½

which is the mgf of a normal random variable with mean P
iaimi and variance
P
iai
2si
2.
▄
In Theorem 4.2.1 let ai¼1/n, mi¼m, and s1
2¼s2, we obtain the following result,
which provides the distribution of the sample mean.
Corollary 4.2.2 Let X1, . . ., Xn be a random sample of size n from a normal pop-
ulation with mean m and variance s2. Then
X ¼ 1=n
ð
Þ
Xn
i¼1Xi
is normally distributed with mean mX ¼ m and variance s2
X ¼ s2=n:
Recall that we have used the notation XN(m, s2) to mean that the random var-
iable X is normally distributed with mean m and variance s2. From Corollary 4.2.2,
X  N m,s2=n
ð
Þ and hence by the z-transformation we obtain the standard normal
random variable, Z ¼ X m


= s=
ﬃﬃﬃn
p
ð
Þ  N 0, 1
ð
Þ:
EXAMPLE 4.2.1
A company that manufactures cars claims that the gas mileage for its new line of hybrid cars, on the
average, is 60 miles per gallon with a standard deviation of 4 miles per gallon. A random sample of
16 cars yielded a mean of 57 miles per gallon. If the company’s claim is correct, what is the prob-
ability that the sample mean is less than or equal to 57 miles per gallon? Comment on the company’s
claim about the mean gas mileage per gallon of its cars. What assumptions did you make?
Solution
Let X represent the gas mileage for the new car (in miles per gallon). If the company’s claim is
true, then from Corollary 4.2.2, X is normally distributed with mean m¼60 and variance
s2/n¼16/16¼1. Hence,
Continued
185
4.2 Sampling Distributions Associated with Normal Populations

P X  57


¼P X 60
1
 5760
1


¼P Z  3
ð
Þ 	 10:999
¼0:001:
Therefore, if the company’s claim is correct, it is very unlikely that the mean value of the random
sample of 16 cars will be 57 miles per gallon. Because the mean is indeed 57 miles per gallon,
we conclude that the company’s claim is very likely not true. Here we have assumed that the
sample of 16 measurements comes from a normal population, so that we could apply the results
of Corollary 4.2.2.
Now we introduce some distributions that can be derived from a normal distribution.
These distributions play a very important role in inferential problems.
4.2.1 CHI-SQUARE DISTRIBUTION
A chi-square distribution is used in many inferential problems, for example, in infer-
ential problems dealing with the variance. Recall that the chi-square distribution is a
special case of a gamma distribution with a¼n/2 and b¼2. If n is a positive integer,
then the parameter n is called the degrees of freedom. However, if n is not an integer,
but b¼2, we still refer to this distribution as a chi-square. The mgf of a w2—random
variable is M(t)¼(1–2 t)n/2. The mean and variance of a chi-square distribution are
m¼n and s2¼2n, respectively. That is, the mean of a w2(n) random variable is equal
to its degree of freedom and the variance is twice the degree of freedom. We now
give some useful results for w2—random variables.
Theorem 4.2.2 Let X1, . . ., Xk be independent w2—random variables with n1, . . .,
nk degrees of freedom, respectively. Then the sum V¼P
i¼1
k
Xi is chi-square distrib-
uted with n1+n2+. . .+nk degrees of freedom.
Proof. The mgf of V is
MV tð Þ ¼
Y
k
i¼1
12t
ð
Þni=2 ¼ 12t
ð
Þ Sk
i¼1ni


2
:
This implies that Vw2 Sk
i¼1ni


.
▄
Our next result states that the difference of two chi-square random variables is a
chi-square random variable, given by the following theorem. The proof is left as an
exercise.
Theorem 4.2.3 Let X1 and X2 be independent random variables. Suppose that X1
is w2 with n1 degrees of freedom, whereas Y¼X1+X2 is chi-square with n degrees of
freedom, where n>n1. Then X2¼YX1 is a chi-square random variable with nn1
degrees of freedom.
The following result shows that we can generate a chi-square random variable
from a gamma random variable.
186
CHAPTER 4 Sampling Distributions

Theorem 4.2.4 If a random variable X has a gamma distribution with parameters
a and b, then
Y ¼ 2X
b  w2 2a
ð
Þ:
Proof. Recall that the mgf of the gamma random variable X is (1bt)a.
MY tð Þ ¼ M2X
b tð Þ ¼ E e
2X
b t


¼ E eX
2
bt
ð Þ


¼ MX
2
bt


¼ 12t
ð
Þa ¼ 12t
ð
Þ2a
2 :
Hence, Yw2(2a).
▄
The following result states that by squaring a standard normal random variable,
we can generate a chi-square random variable, with one degree of freedom.
Theorem 4.2.5 If X is a standard normal random variable, then X2 is chi-square
random variable with 1 d.f.
Proof. Because XN(0, 1) the moment-generating function of X2 is
MX2 tð Þ ¼
ð1
1
etx2
1ﬃﬃﬃﬃﬃﬃ
2p
p
ex2=2dx ¼ 12t
ð
Þ1=2:
This implies that X2w2(1). Figure 4.1 gives the probability densities of the random
variables X and X2.
▄
4
Densities of Standard normal r.v. and its square
3.5
3
2.5
2
1.5
1
0.5
0
3
2
1
0
1
2
3
pdf of X 2
pdf of X
FIGURE 4.1
Pdf of Standard Normal r.v. and the Pdf of its Square.
187
4.2 Sampling Distributions Associated with Normal Populations

The following result is a direct consequence of Theorems 4.2.2 and 4.2.5. This
result illustrates how to obtain a random sample from chi-square distribution if
we have a random sample of n measurements from a normal population.
Theorem 4.2.6 Let the random sample X1, . . ., Xn be from a N(m,s2) distributed.
Then Zi¼(Xim)/s, i¼1, . . ., n are independent standard normal random variables
and
X
n
i¼1
Z2
i ¼
X
n
i¼1
Xi m
s

2
has a w2-distribution with n degrees of freedom. In particular, if X1, . . ., Xn are inde-
pendent standard normal random variables, then Y2¼P
i¼1
n Xi
2 is chi-square distrib-
uted with n degrees of freedom.
If Xw2(n), then from the chi-square table, we can compute the values of wa
2(n)
such that
P X > w2
a n
ð Þ


¼ a,
as shown by Figure 4.2.
For example, if Xw2(15), to find w0.95
2
(15) look in the chi-square table with the
row labeled 15 d.f. and the column headed w0.950
2
and obtain the value as 7.26094.
Thus, with 15 degrees of freedom, P(X>7.26094)¼0.95. Also, if X is a chi-square
random variable with 11 degrees of freedom, from the chi-square table we have
w0.05
2
(11)¼19.675. Therefore, P(X>19.675)¼0.05.
EXAMPLE 4.2.2
Let the random variables X1,X2, . . ., X5 be from an N (5,1) distribution. Find a number a such that
P
X
5
i¼1
Xi 5
ð
Þ2  a
 
!
¼ 0:90:
FIGURE 4.2
Chi-square Probability Density.
188
CHAPTER 4 Sampling Distributions

Solution
By Theorem 4.2.6,
X5
i¼1Z2
i ¼
X5
i¼1
Xi 5
1

2
¼
X5
i¼1 Xi 5
ð
Þ2 has a chi-square distribution with
5 degrees of freedom. Because the upper tail area is 0.10, looking at the chi-square table with 5 d.f.
and the column corresponding to w0.10
2
, we obtain a¼9.23635. Thus,
P
X
5
i¼1
Xi 5
ð
Þ2  9:23635
 
!
¼ 0:90:
EXAMPLE 4.2.3
Suppose that X is w2—random variable with 20 degrees of freedom. Use the chi-square table to
obtain the following:
(a) Find x0 such that P(X>x0)¼0.95.
(b) Find P(X12.443).
Solution
(a) For 20 degrees of freedom, using the chi-square table, we have
P X > 10:851
ð
Þ ¼ 0:95:
Hence, x0¼10.851.
(b) From the chi-square table,
P X  12:443
ð
Þ ¼ 0:10:
The following result gives the probability distribution for a function of the sample variance S2.
Theorem 4.2.7 If X1, . . ., Xn is a random sample from a normal population with
the mean m and variance s2, then
(a)
Xn
i¼1 Xi X

2
s2
¼ n1
ð
ÞS2
s2
:
has a chi-square distribution with (n1) degrees of freedom.
(b) Xand S2 are independent.
Proof. We will only prove part (a). For part (b), we will give some comments on
the proof.
(a) We know from Theorem 4.2.6 that (1/s2)P
i¼1
n (Xim)2 has a chi-square
distribution with n degrees of freedom. Thus,
189
4.2 Sampling Distributions Associated with Normal Populations

1
s2
X
n
i¼1
Xi m
ð
Þ2 ¼ 1
s2
X
n
i¼1
Xi X + X m

2
¼ 1
s2
X
n
i¼1
Xi X

2 +
X
n
i¼1
X m

2
"
#
Since2
X
n
i¼1
Xi X


X m


¼ 0
 
!
¼ n1
ð
ÞS2
s2
+
X m
s=
ﬃﬃﬃn
p

2
:
The left-hand side of this equation has a chi-square distribution with n degrees
of freedom. Also, since
X m


= sj
ﬃﬃﬃn
p
ð
Þ  N 0, 1
ð
Þ by Theorem 4.2.5 we have
X m


= s=
ﬃﬃﬃn
p
ð
Þ
	

2  w2 1
ð Þ: Now from Theorem 4.2.3, (n1) S2/s2w2 (n1).
(b) We will accept the result of part (b) without proof here. A rigorous proof depends
on geometric properties of the multivariate normal distribution, which is beyond
the scope of this book. A proof based on moment-generating functions is
relatively straightforward, where essentially we can first show that the random
variable X and the vector of random variables X1 X, ..., Xn  X


are
independent. Because S2 is a function of the vector X1 X, ..., Xn  X


, it is
then independent of X.
▄
EXAMPLE 4.2.4
Let X1, X2, . . ., X10 be a random sample from a normal distribution with s2¼0.8. Find two positive
numbers a and b such that the sample variance S2 satisfies
P a  S2  b


¼ 0:90:
Solution
Because
n1
ð
ÞS2
s2
 w2 n1
ð
Þ, we have
P a  S2  b


¼ P
n1
ð
Þa
s2
 n1
ð
ÞS2
s2
 n1
ð
Þb
s2


:
The desired values can be found by setting the upper tail area and lower tail area each equal to
0.05. Using the chi-square table with n–1¼9 degrees of freedom, we have
n1
ð
Þb
s2
¼ 9b
0:8 ¼ 16:919 ¼ w2
0:05,9,
which implies b¼((16.919)(0.8)/9)¼1.50. Similarly,
n1
ð
Þa
s2
¼ 9a
0:8 ¼ 3:325 ¼ w2
0:95,9:
So we have a¼((3.325)(0.8)/9)¼0.295.
Hence,
P 0:295  S2  1:50


¼ 0:90:
It is important to note that this is not the only interval that would satisfy
P a  S2  b


¼ 0:90
but it is a convenient one.
190
CHAPTER 4 Sampling Distributions

EXAMPLE 4.2.5
A fruit-drink company wants to know the variation, as measured by the standard deviation, of the
amount of juice in 16-ounce cans. From past experience, it is known that s2¼2. The company stat-
istician decides to take a sample of 25 cans from the production line and compute the sample var-
iance. Assuming that the sample values may be viewed as a random sample from a normal
population, find a value of b such that P (S2>b)¼0.05.
Solution
To find the necessary probability, use the fact that (n1) S2/s2w2(n1), with n¼25,
0:05 ¼ P S2 > b


¼ P 24S2
2
> 24b
2


¼ P w2 > c


:
From the chi-square table we obtain, c¼36.4151. Hence, b ¼ 2
24 c ¼ 2
24 36:4151
ð
Þ ¼ 3:03 and
P S2 > 3:03


¼ 0:05:
SUMMARY OF CHI-SQUARE DISTRIBUTION
Let X1, . . ., Xn be iid N(m, s2) random variables. Then
1. X has N (m, s2/n) distribution,
2. (n1)S2/s2 has a chi-square distribution with (n1) degrees of freedom, and
3. X and S2 are independent.
4. A w2—random variable has a mean equal to its degrees of freedom and a variance equal to twice
its degrees of freedom.
4.2.2 STUDENT t-DISTRIBUTION
Let the random variables X1, . . ., Xn follow a normal distribution with mean m and
variance s2. If s is known, then we know that
ﬃﬃﬃn
p
X m


=s


is N 0, 1
ð
Þ: However,
if s is not known (as is usually the case), then it is routinely replaced by the sample
standard deviation s. If the sample size is large, one could suppose that s	s and apply
the Central Limit Theorem and obtain that
ﬃﬃﬃn
p
X m


=S


is approximately an N(0,1).
However, if the random sample is small, then the distribution of
ﬃﬃﬃn
p
X m


=S


is
given by the so-called Student t-distribution (or simply t-distribution). This was
originally developed by W.S. Gosset in 1908. Because his employers, the Guinness
brewery, would not permit him to publish this important work in his own name,
he used the pseudonym “Student.” Thus, the distribution is known as the Student
t-distribution.
Definition 4.2.2 If Y and Z are independent random variables, Y has a chi-square
distribution with n degrees of freedom, and ZN(0, 1), then
T ¼
Zﬃﬃﬃﬃﬃﬃﬃﬃ
Y=n
p
191
4.2 Sampling Distributions Associated with Normal Populations

is said to have a (Student) t-distribution with n degrees of freedom. We denote this
by TTn.
The probability density of the random variable T with n degrees of freedom is
given by
f tð Þ ¼
G n + 1
2


ﬃﬃﬃﬃﬃﬃ
pn
p
G n
2
 
1 + t2
n

n + 1
2
,
1 < t < 1:
Figure 4.3 illustrates the behavior of the t-distributions for n¼2, 10, 20, and 30. It is
clear from Figure 4.3 that as n becomes larger and larger, it is almost impossible to
distinguish the graphs. It can be shown that the t-distribution tends to a standard nor-
mal distribution as the degrees of freedom (equivalently, the sample size n) tend to
infinity. In fact, the standard normal distribution provides a good approximation to
the t-distribution for sample sizes of 30 or more. We will use this approximation in
the statistical inference problems for n30.
The t-density is symmetric about zero, and then we have E(T)¼0. If n>2, it can
be shown that Var (T)¼n/(n2). The value of ta,n is such that P (t>ta,n)¼a
(the shaded area in Figure 4.4) is obtained from the t-table. For example, if a random
variable X has a t-distribution with 9 degrees of freedom and a¼0.01, then
t0.01,9¼2.821.
If we have a random sample from a normal population, the following result
involving a t-distribution is useful in applications.
0.4
T density for n=2,
n=10, n=20, n=30
n=2
n=10
n=20
n=30
0.35
0.3
0.25
0.2
0.15
0.1
0.05
–4
–3
–2
–1
0
1
2
3
4
FIGURE 4.3
The Student t-Distribution.
192
CHAPTER 4 Sampling Distributions

Theorem 4.2.8 If X and S2 are the mean and the variance of a random sample of
size n from a normal population with the mean m and variance s2, then
T ¼ X m
S=
ﬃﬃﬃn
p
has a t-distribution with (n1) degrees of freedom.
Proof. By Corollary 4.2.2,
Z ¼ X m
s=
ﬃﬃﬃn
p  N 0, 1
ð
Þ:
By Theorem 4.2.7, we have
Y ¼ n1
ð
ÞS2
s2
¼ 1
s2
X
n
i¼1
Xi X

2  w2 n1
ð
Þ:
Hence,
T ¼
Xm
s= ﬃﬃn
p
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n1
ð
ÞS2
s2 n1
ð
Þ
q

Zﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
w2 n1
ð
Þ
n1
p
:
Also, X and S2 are independent. Thus, Y and Z are independent, and by
Definition 4.2.2, T follows a t-distribution with (n1) degrees of freedom.
▄
How can we distinguish between given degrees of freedom and the degrees of
freedom from a sample? For the t-distribution, if n is given as the degrees of freedom,
we will just use n. However, if a random sample of size n is given, then the corre-
sponding degrees of freedom will be (n–1), as given in Theorem 4.2.8.
0.4
f (t)
0.35
0.3
0.25
0.2
0.15
0.1
0.05
–4
–3
–2
–1
0
t
1
2
3
4
FIGURE 4.4
Probability of t-distribution.
193
4.2 Sampling Distributions Associated with Normal Populations

The assumption that the sample comes from a normal population is not that oner-
ous. In practice, it is necessary to check that the sampled population is approximately
bell shaped and not too much skewed. Construction of the normal-scores plot or his-
togram is a way to check for approximate normality. See Project 4C.
EXAMPLE 4.2.6
A manufacturer of fuses claims that with 20% overload, the fuses will blow in less than 10 minutes
on the average. To test this claim, a random sample of 20 of these fuses was subjected to a 20%
overload, and the times it took them to blow had the mean of 10.4 minutes and a sample standard
deviation of 1.6 minutes. It can be assumed that the data constitute a random sample from a normal
population. Do they tend to support or refute the manufacturer’s claim?
Solution
Given y ¼ 10:4,s ¼ 1:6,n ¼ 20, and m¼10. Hence
t ¼ ym
s=
ﬃﬃﬃn
p ¼ 10:410
1:6=
ﬃﬃﬃﬃﬃ
20
p
¼ 1:118:
The degree of freedom is n–1¼19. From the t-table, the probability that t exceeds 1.328 is 0.10,
and because the observed value of t¼1.118 is less than t0.10(19)¼1.328 and 0.10 is a pretty large
probability, we conclude that the data tend to agree with the manufacturer’s claim.
We will study the problems of the foregoing type in Chapter 6, where we will be
learning about hypothesis testing. Prior to Student’s work on the t-distribution, a very
large number of observations were necessary for design and analysis of experiments.
Today, the use of the t-distribution often makes it possible to draw reliable conclu-
sions from samples as small as 15 to 30 experimental units, provided that the samples
are representative of their populations and that normality could reasonably be
assumed or justified for the population.
EXAMPLE 4.2.7
The human gestation period—the period of time between conception and labor—is approximately
40 weeks (280 days), measured from the first day of the mother’s last menstrual period. For a new-
born full-term infant, the length appropriate for gestational age is assumed to be normally distributed
with m¼50 centimeters and s¼1.25 centimeters. Compute the probability that a random sample of
20 infants born at full term results in a sample mean greater than 52.5 centimeters.
Solution
Let X be length (measured in centimeters) of a newborn full-term infant. Then X  N 50,1:56=20
ð
Þ:
Hence
P X > 52:5


¼ P t > 52:550
1:25=
ﬃﬃﬃﬃﬃ
20
p
¼ 8:94


	 0:
Thus, the probability of such an occurrence is negligible.
In the previous example, it should be noted that P X > 52:5


	 0 does not imply
that the probability of observing a newborn full-term infant with length greater than
194
CHAPTER 4 Sampling Distributions

52.5 centimeters is zero. In fact, with 19 degrees of freedom, P(X>52.5)¼
P(t>2)	0.025.
4.2.3 F-DISTRIBUTION
The F-distribution was developed by Fisher to study the behavior of two variances
from random samples taken from two independent normal populations. In applied
problems we may be interested in knowing whether the population variances are
equal or not, based on the response of the random samples. Knowing the answer
to such a question is also important in selecting the appropriate statistical methods
to study their true means.
Definition 4.2.3 Let U and V be chi-square random variables with n1 and n2
degrees of freedom, respectively. Then if U and V are independent,
F ¼ U=n1
V=n2
is said to have an F-distribution with n1 numerator degrees of freedom and n2
denominator degrees of freedom. We denote this by FF (n1, n2).
The pdf for a random variable XF (n1, n2) is given by
f x
ð Þ ¼
G n1 + n2
ð
Þ=2
ð
Þ
G n1=2
ð
ÞG n2=2
ð
Þ
n1
n2

n1=2
x
n1
2 1
1 + n1
n2
x

 n1 + n2
ð
Þ=2
,
x > 0
8
>
>
<
>
>
:
0,
elsewhere:
A graph of f(x) for various values of n is given in Figure 4.5.
To find Fa (n1,n2) such that P(F>Fa (n1, n2))¼a (shaded area in Figure 4.6), we
use the F-table. For example, if F has 3 numerator and 6 denominator degrees of
freedom, then F0.01(3, 6)¼9.78.
If we know Fa(n1, n2), it is possible to find F1a(n2, n1) by using the identity
F1a n2, n1
ð
Þ ¼ 1=Fa n1, n2
ð
Þ:
Using this identity we can obtain F0.99(6, 3)¼1/F0.01 (3, 6)¼1/9.78¼0.10225.
When we need to compare the variances of two normal populations, we will use
the following result.
Theorem 4.2.9 Let two independent random samples of size n1 and n2 be drawn
from two normal populations with variances s1
2, s2
2, respectively. If the variances of
the random samples are given by S1
2, S2
2, respectively, then the statistic
F ¼ S2
1=s2
1
S2
2=s2
2
¼ s2
2S2
1
s2
1S2
2
has the F-distribution with (n11) numerator and (n21) denominator degrees of
freedom.
195
4.2 Sampling Distributions Associated with Normal Populations

F (3, 2)
F−density with n1=3, n2=2 and n1=12, n2=6
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1
2
3
4
5
6
7
F (12, 6)
FIGURE 4.5
Pdfs of F-Distribution.
0.7
0.6
0.5
0.4
0.3
0.2
0.1
f(x)
x
0
1
2
3
a
4
5
6
7
FIGURE 4.6
F-Distribution Probability.
196
CHAPTER 4 Sampling Distributions

Proof. From Theorem 4.2.8, we know that
U ¼ n1 1
ð
ÞS2
1
s2
1
 w2 n1 1
ð
Þ
and
V ¼ n2 1
ð
ÞS2
2
s2
2
 w2 n2 1
ð
Þ:
Also, U and V are independent. From Definition 4.2.3, FF(n11, n21).
▄
Corollary 4.2.11 If s1
2¼s2
2, then
F ¼ S2
1
S2
2
 F n1 1,n2 1
ð
Þ:
When s1
2¼s2
2, we refer to them as two populations that are homogeneous with
respect to their variances.
EXAMPLE 4.2.8
Let S1
2 denote the sample variance for a random sample of size 10 from Population I and let S2
2
denote the sample variance for a random sample of size 8 from Population II. The variance of
Population I is assumed to be three times the variance of Population II. Find two numbers a and
b such that P(aS1
2/S2
2b)¼0.90 assuming S1
2 to be independent of S2
2.
Solution
From the problem, we can assume that s1
2¼3s2
2 with n1¼10 and n2¼8. Thus, we can write
S2
1=s2
1
S2
2=s2
2
¼ S2
1=3s2
2
S2
2=s2
2
¼ S2
1
3S2
2
,
this has F-distribution with n11¼9 numerator and n2–1¼7 denominator degrees of freedom.
Using the F-table, F0.05 (9, 7)¼3.68. Now to find F0..95 such that
P
S2
1
3S2
2
< F0:95


¼ 0:05:
We proceed as follows:
P
S2
1
3S2
2
< F0:95


¼ P 3S2
2
S2
1
>
1
F0:95


¼ 0:05:
Indexing v1¼7 and v2¼9 in the F-table, we have 1/F0.95(7, 9)¼F0.05(9,7)¼3.68 or
F0.95¼1/3.68¼0.2717. Hence, the entire probability statement is
P 0:2717  S2
1
3S2
2
 3:68


¼ P 0:815  S2
1
S2
2
 11:04


¼ 0:90:
Thus, a¼0.815 and b¼11.04.
197
4.2 Sampling Distributions Associated with Normal Populations

EXERCISES 4.2
4.2.1. Let Y have a chi-square distribution with 15 degrees of freedom. Find the
following probabilities.
(a) P(Yy0)¼0.025
(b) P (a <Y<b)¼0.95
(c) P(Y22.307).
4.2.2. Let Y have a chi-square distribution with 7 degrees of freedom. Find the
following probabilities.
(a) P(Y>y0)¼0.025
(b) P (a<Y<b)¼0.90
(c) P(Y>1.239).
4.2.3. The time to failure T of a microwave oven has an exponential distribution
with pdf
f tð Þ ¼ 1
2et=2, t > 0:
If three such microwave ovens are chosen and t is the mean of their failure
times, find the following:
(a) Distribution of T .
(b) P T > 2


:
4.2.4. Let X1,X2, . . ., X10 be a random sample from a standard normal distribution.
Find the numbers a and b such that
P a 
X
10
i¼1
X2
i  b
 
!
¼ 0:95:
4.2.5.
Let X1,X2, . . ., X5 be a random sample from the normal distribution with
mean 55 and variance 223. Let
Y ¼
X
5
i¼1
Xi 55
ð
Þ2=223
and
Z ¼
X
5
i¼1
Xi X

2=223:
(a) Find the distribution of the random variables Y and Z.
(b) Are Y and Z independent?
(c) Find (i) P(0.62Y0.76), and (ii) P(0.77Z0.95).
4.2.6.
Let X and Y be independent chi-square random variables with 14 and 5
degrees of freedom, respectively. Find
(a) P(jXYj11.15),
(b) P(jXYj3.8).
198
CHAPTER 4 Sampling Distributions

4.2.7.
A particular type of vacuum-packed coffee packet contains an average of
16 ounces. It has been observed that the number of ounces of coffee in
these packets is normally distributed with s¼1.41 ounce. A random sample
of 15 of these coffee packets is selected, and the observations are used
to calculate s. Find the numbers a and b such that P(aS2b)¼0.90.
4.2.8.
An optical firm buys glass slabs to be ground into lenses, and it is known
that the variance of the refractive index of the glass slabs is to be no more
than 1.04103. The firm rejects a shipment of glass slabs if the sample
variance of 16 pieces selected at random exceeds 1.15103. Assuming
that the sample values may be looked on as a random sample from a normal
population, what is the probability that a shipment will be rejected even
though s2¼1.04103?
4.2.9.
Assume that T has a t-distribution with 8 degrees of freedom. Find the
following probabilities.
(a) P(T2.896)
(b) P(T1.860)
(c) The value of a such that P(a<T<a)¼0.99
4.2.10. Assume that T has a t-distribution with 15 degrees of freedom. Find the
following probabilities.
(a) P(T1.341)
(b) P(T2.131)
(c) The value of a such that P(a<T<a)¼0.95
4.2.11. A psychologist claims that the mean age at which female children start
walking is 11.4 months. If 20 randomly selected female children are found
to have started walking at a mean age of 11.5 months with standard
deviation of 2 months, would you agree with the psychologist’s claim?
Assume that the sample came from a normal population.
4.2.12. Let U1 and U2 be independent random variables. Suppose that U1 is w2 with
v1 degrees of freedom while U¼U1+U2 is chi-square with v degrees of
freedom, where v>v1. Then prove that U2 is chi-square random variable
with vv1 degrees of freedom.
4.2.13. Show that if Xw2 (v), then EX¼v and Var (X)¼2v.
4.2.14. Let X1, . . ., Xn be a random sample with Xiw2 (1), for i¼1, . . ., n. Show
that the distribution of
Z ¼ X 1
ﬃﬃﬃﬃﬃﬃﬃﬃ
2=n
p
as n!1 is standard normal.
4.2.15. Find the variance of S2, assuming the sample X1, X2, . . ., Xn is from N(m, s2).
4.2.16. Let X1, X2, . . ., Xn be a random sample from an exponential distribution with
parameter y. Show that the random variable 2y1(P
i¼1
n Xi)w2(2n).
4.2.17. Let X and Y be independent random variables from an exponential
distribution with common parameter y¼1. Show that X/Y has an
F-distribution. What is the number for degrees of freedom?
199
4.2 Sampling Distributions Associated with Normal Populations

4.2.18. Prove that if X has a t-distribution with n degrees of freedom, then
X2F (1, n).
4.2.19. Let X be F distributed with 9 numerator and 12 denominator degrees of
freedom. Find
(a) P(X3.87),
(b) P(X0.196),
(c) The value of a and b such that P (a<Y<b)¼0.95.
4.2.20. Prove that if XF(n1,n2), then 1/XF(n2,n1).
4.2.21. Find the mean and variance of F (n1, n2) random variable.
4.2.22. Let X11, X12, ..., X1n1 be a random sample with sample mean X1 from a
normal population with mean m1 and variance s1
2, and let
X21, X22, ..., X2n2 be a random sample with sample mean X2 from a
normal population with mean m2 and variance s2
2. Assume the two samples
are independent. Show that the sampling distribution of X1 X2


is normal
with mean m1–m2 and variance s1
2/n1+s2
2/n2.
4.2.23. Let X1, X2, . . ., Xn1 be a random sample from a normal population with mean
m1 and variance s2, and Y1, Y2, . . ., Yn2 be a random sample from an
independent normal population with mean m2 and variance s2. Show that
T ¼
X Y


 m1 m2
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n11
ð
ÞS2
1 + n21
ð
ÞS2
2
n1 + n22
s
1
n1 + 1
n2


 T n1 + n22
ð
Þ
4.2.24. Show that a t-distribution tends to a standard normal distribution as the
degrees of freedom tend to infinity.
4.2.25. Show that the mgf of a w2 random variable is M(t)¼(12t)–v/2. Using the
mgf, show that the mean and variance of a chi-square distribution are v and
2v, respectively.
4.2.26. Let the random variables X1, X2, . . ., X10 be normally distributed with mean
8 and variance 4. Find a number a such that
P
X
10
i¼1
Xi 8
2

2
 a
 
!
¼ 0:95
4.2.27. Let X2F(1,n). Show that Xt(n).
4.3 ORDER STATISTICS
In practice, the random variables of interest may depend on the relative magnitudes
of the observed variable. For example, we may be interested in the maximum mile-
age per gallon of a particular class of cars. In this section, we study the behavior of
ordering a random sample from a continuous distribution.
200
CHAPTER 4 Sampling Distributions

Definition 4.3.1 Let X1, . . ., Xn be a random sample from a continuous distribu-
tion with pdf f(x). Let Y1, . . ., Yn be a permutation of X1, . . ., Xn such that
Y1  Y2  


  Yn:
Then the ordered random variables Y1, . . ., Yn are called the order statistics of the
random sample X1, . . ., Xn. Here Yk is called the kth order statistic. Because of con-
tinuity, the equality sign could be ignored.
Remark. Although Xi0s are iid random variables, the random variables Yi0s are
neither independent nor identically distributed.
Thus, the minimum of Xi0s is
Y1 ¼ min X1, :: :, Xn
ð
Þ
and the maximum is
Yn ¼ max X1, : : :, Xn
ð
Þ:
The order statistics of the sample X1,X2, . . ., Xn can also be denoted by X(1), X(2), . . .,
X(n) where
X 1
ð Þ < X 2
ð Þ < 


 < X n
ð Þ:
Here X(k) is the kth order statistic and is equal to Yk in Definition 4.3.1. One of the
most commonly used order statistics is the median, the value in the middle position in
the sorted order of the values.
EXAMPLE 4.3.1
(i) The range R¼YnY1 is a function of order statistics.
(ii) The sample median M equals Ym+1 if n¼2m+1.
Hence, the sample median M is an order statistic, when n is odd. If n is even then the sample
median can be obtained using the order statistic, M¼(1/2) [Yn/2+Y(n/2)+1].
The following result is useful in determining the distribution of functions of more
than one order statistics.
Theorem 4.3.1 Let X1, . . ., Xn be a random sample from a population with pdf
f(x). Then the joint pdf of order statistics Y1, . . ., Yn is
f y1, ..., yn
ð
Þ ¼
n!f y1
ð
Þf y2
ð
Þ...f yn
ð
Þ, for y1 < 


 < yn
0,
otherwise:
(
The pdf of the kth order statistic is given by the following theorem.
Theorem 4.3.2 The pdf of Yk is
f k y
ð Þ ¼ f Yk y
ð Þ ¼
n!
k 1
ð
Þ! nk
ð
Þ!f y
ð Þ F y
ð Þ
ð
Þk1 1F y
ð Þ
ð
Þnk,
for –1<y<1, where F(y)¼P(Xiy) is the cdf of Xi.
201
4.3 Order Statistics

In particular, the pdf of Y1 is f1(y)¼nf(y)[1F(y)]n1 and the pdf of Yn is fn(y)¼
nf(y)[F(y)]n1. In the following example, we will derive pdf for Yn.
EXAMPLE 4.3.2
Let X1, . . ., Xn be a random sample from U[0,1]. Find the pdf of the kth order statistic Yk.
Solution
Since the pdf of Xi is f(x)¼1,0x1, the cdf is F(x)¼x, 0x1. Using Theorem 4.3.2, the pdf of
the kth order statistic Yk reduces to
f k y
ð Þ ¼
n!
k 1
ð
Þ! nk
ð
Þ!yk1 1y
ð
Þnk, 0  y  1
which is a beta distribution with a¼k and b¼nk+1.
The next example gives the so-called extreme (i.e. largest) value distribution,
which is the distribution of the order statistic Yn.
EXAMPLE 4.3.3
Find the distribution of the nth order statistic Yn of the sample X1, . . ., Xn from a population with
pdf f(x).
Solution
Let the cdf of Yn be denoted by Fn(y). Then
Fn y
ð Þ ¼ P Yn  y
ð
Þ ¼ P
max
1inXi  y


¼ P X1  y, ...,Xn  y
ð
Þ ¼ F y
ð Þ
½
n by independence
ð
Þ:
Hence, the pdf fn (y) of Yn is
f n y
ð Þ ¼ d
dy F y
ð Þ
½
n ¼n F y
ð Þ
½
n1 d
dyF y
ð Þ
¼n F y
ð Þ
½
n1f y
ð Þ:
In particular, if X1, . . ., Xn is a random sample from U[0, 1], then the cumulative extreme
value distribution is given by
Fn y
ð Þ ¼
0,
y < 0
yn, 0  y  1
1,
y > 1:
8
>
<
>
:
EXAMPLE 4.3.4
A string of 10 light bulbs is connected in series, which means that the entire string will not light up if
any one of the light bulbs fails. Assume that the lifetimes of the bulbs, t1, . . ., t10, are independent
random variables that are exponentially distributed with mean 2. Find the distribution of the life
length of this string of light bulbs.
202
CHAPTER 4 Sampling Distributions

Solution
Note that the pdf of ti is f(t)¼2e2t, 0<t<1, and the cumulative distribution of ti is
Fti(t)¼1e2t. Let T represent the lifetime of this string of light bulbs. Then,
T ¼ min t1, :::, t10
ð
Þ:
Thus,
FT tð Þ ¼ 1 1Fti tð Þ
½
10:
Hence, the density of T is obtained by differentiating FT(t) with respect to t, that is,
f T tð Þ ¼ 10f ti tð Þ 1Fti tð Þ
	

9
¼
2 10
ð
Þe2t e2t
ð
Þ9 ¼ 20e20t, 0 < t < 1
0,
otherwise:

The joint pdf of the order statistics is given by the following result.
Theorem 4.3.3 Let X1, . . ., Xn be a random sample with continuous probability den-
sity function f(x) and a distribution function F(x). Let Y1, . . ., Yn be the order statistics.
Then for any 1i<kn and –1<xy<1, the joint pdf of Yi and Yk is given by
f Yi,Yk x, y
ð
Þ ¼
n!
i1
ð
Þ! k i1
ð
Þ! nk
ð
Þ! F x
ð Þ
½
i1
 F y
ð ÞF x
ð Þ
½
ki1 1F y
ð Þ
½
nkf x
ð Þf y
ð Þ
EXAMPLE 4.3.5
Let X1, . . ., Xn be a random sample from U[0,1]. Find the joint pdf of Y2 and Y5.
Solution
Taking i¼2 and k¼5 in Theorem 4.3.3, we get the joint pdf of Y2 and Y5 as
f Y2,Y5 x, y
ð
Þ ¼
n!
21
ð
Þ! 521
ð
Þ! n5
ð
Þ! F x
ð Þ
½
21
F y
ð ÞF x
ð Þ
½
521  1F y
ð Þ
½
n5f x
ð Þf y
ð Þ
¼
n!
2 n5
ð
Þ!x yx
ð
Þ2 1y
ð
Þn5 0 < x  y < 1
0,
otherwise:
8
>
<
>
:
EXERCISES 4.3
4.3.1. The lifetime X of a certain electrical fuse has the following probability
density function
f x
ð Þ ¼
1
10ex=10,
x > 0
0,
otherwise:
(
Suppose two such fuses are in series and operate independently in a
system. Find the pdf of the lifetime Y of the system. (The system will work
only if both the fuses operate.)
203
4.3 Order Statistics

4.3.2. Suppose that time between two telephone calls at an office, in minutes, is
uniformly distributed on the interval [0, 20]. If there were 15 calls, (i)what is
the probability that the longest time interval between calls is less than
15 minutes? (ii)What is the probability that the shortest time interval
between calls is greater than 5 minutes?
4.3.3. Let X1, X2, X3 be three random variables of discrete type. Let X1, X2 take
values 0, 1, and X3 take values 1, 2, 3. What are the values of Y1, Y2, Y3?
4.3.4. Let X1, . . ., X10 be a random sample from U[0, 1]. Find the joint density of
Y2 and Y7, where Yi, i¼ 1, 2, . . ., 10 are order statistics of X1, . . ., X10.
4.3.5. Let X1, . . ., Xn be a random sample from exponential distribution with a
mean of y. Show that Y1¼min (X1, X2, . . ., Xn) has an exponential
distribution with mean y/n. Also, find the pdf of Yn¼max (X1,X2, . . ., Xn).
4.3.6. A string of 10 light bulbs is connected in parallel, which means that the
entire string will fail to light up only if all 10 of the light bulbs fail. Assume
that the lifetimes of the bulbs, t1, . . ., t10, are independent random variables
that are exponentially distributed with mean y. Find the distribution of the
lifetimes of this string of light bulbs.
4.3.7. Let X1, . . ., Xn be a random sample from the uniform distribution f(x)¼1/2,
0x2. Find the probability density function for the range R¼(X(n)X(1)).
4.3.8. Given a sample of 25 observations from a distribution with pdf
f x
ð Þ ¼
ex,
x > 0
0,
otherwise
(
let M be the sample median. Compute P(Mb).
[Hint: Note that M is the 13th order statistic.]
4.3.9. Let X1, . . ., Xn be a random sample from a normal population with mean 10
and variance 4. What is the probability that the largest observation is greater
than 10?
4.3.10. Let X1, . . ., Xn be a random sample from an exponential population with
parameter y. Let Y1, . . ., Yn be the ordered random variables.
(a) Show that the sampling distributions of Y1 and Yn are given by
f 1 y1
ð
Þ ¼
n
yeny1=y,
if y1 > 0
0,
otherwise,
8
<
:
and
f n yn
ð
Þ ¼
n
yeyn=y 1eyn=y
h
in1
,
if yn > 0
0,
otherwise:
8
<
:
(b) Let n¼2l+1. Show that the sampling distribution of the median, M, is
given by
f m
ð Þ ¼
n!
l!
ð Þ2y
em l + 1
ð
Þ=y 1em=y
h
il
,
for m > 0
0,
otherwise:
8
>
<
>
:
204
CHAPTER 4 Sampling Distributions

4.3.11. Let X1, . . ., Xn be a random sample from a beta distribution with a¼2 and
b¼3. Find the joint pdf of Y1 and Yn.
4.3.12. Let X1, . . ., Xn be a random sample from a geometric distribution with pmf
pi ¼ P X ¼ i
ð
Þ ¼ pqi1,
i ¼ 1, 2, :: :, 0 < p < 1,
q ¼ 1 p:
Show that
P Yk ¼ y
ð
Þ ¼
X
n
i¼k
n
i
 
q y1
ð
Þ ni
ð
Þ qni 1qy
½
i  1qy1
	

i
n
o
, y ¼ 1,2, ...:
4.4 LARGE SAMPLE APPROXIMATIONS
If the sample size is large, the normality assumption on the underlying population can
be relaxed. A useful generalization of Corollary 4.2.2 follows.
Theorem 4.4.1 Suppose that the population (not necessarily normal) from which
samples are taken has a probability distribution with mean m and variance s2. Then
the standardized variable (or z-transform) associated with X, given by
Z ¼ X m
s=
ﬃﬃﬃn
p
is asymptotically standard normal. That is,
lim
n!1P Z  z
ð
Þ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
ðz
1
eu2=2du:
Theorem 4.4.1 follows directly from the Central Limit Theorem. The consequence of
this for statistics is that, regardless of the form of the population distribution, the dis-
tribution of the z-transform of a sample mean X will be approximately a standard
normal random variable whenever n is large. This fact will be used in almost all large
sample inference problems. It is important to note that, by Corollary 4.2.2, if the ran-
dom sample came from a normal population, then sampling distribution of the mean
is normally distributed regardless of the size of the sample. We could use the fore-
going results if the population variance s2 is known or when the sample size is large.
Even though the required sample size to apply Theorem 4.4.1 will depend on the
particular distribution of the population, for practical purposes we will consider
the sample size to be large enough if n30.
EXAMPLE 4.4.1
The average SAT score for freshmen entering a particular university is 1100 with a standard devi-
ation of 95. What is the probability that the mean SAT score for a random sample of 50 of these
freshmen will be anywhere from 1075 to 1110?
Solution
The distribution of X has the mean mX ¼ 1100 and sX ¼ 95=
ﬃﬃﬃﬃﬃ
50
p
: By Theorem 4.4.3,
X  N 1100,sX ¼ 95=
ﬃﬃﬃﬃﬃ
90
p


. The z-scores corresponding to 1075 and 1110 are
z ¼
10751100
ð
Þ=95=
ﬃﬃﬃﬃﬃ
50
p
	

¼ 1:8608 and z ¼
11101100
ð
Þ=95=
ﬃﬃﬃﬃﬃ
50
p
	

¼ 0:74432:
Continued
205
4.4 Large Sample Approximations

Hence,
P 1075  X  1110


¼ P 1:8608  Z  0:74432
ð
Þ ¼ 0:739
means that we are 73.9% certain based on the given data that the mean SAT score is between 1075
and 1110, inclusive.
4.4.1 THE NORMAL APPROXIMATION TO THE BINOMIAL
DISTRIBUTION
We know that a binomial random variable Y, with parameters n and p¼P (success),
can be viewed as the number of successes in n trials and can be written as
Y ¼
X
n
i¼1
Xi
where,
Xi ¼
1
with probability p
0 with probability 1p
ð
Þ:

The fraction of successes in n trials is
Y
n ¼ 1
n
X
n
i¼1
Xi ¼ X:
Hence, Y/n is a sample mean. Since E(Xi)¼p and Var (Xi)¼p(1p), we have
E Y
n
 
¼ E 1
n
X
n
i¼1
Xi
 
!
¼ 1
nnp ¼ p
and
Var Y
n
 
¼ 1
n2
X
n
i¼1
Var Xi
ð
Þ ¼ p 1p
ð
Þ
n
:
Because Y ¼ nX, by the Central Limit Theorem, Y has an approximate normal dis-
tribution with mean m¼nm and variance s2¼np(1p). Because the calculation of
the binomial probabilities is cumbersome for large sample sizes n, the normal
approximation to the binomial distribution is widely used. A useful rule of thumb
for use of the normal approximation to the binomial distribution is to make sure n
is large enough if np5 and n(1p)5. Otherwise, the binomial distribution
may be so asymmetric that the normal distribution may not provide a good approx-
imation. Other rules, such as np10 and n(1p)10, or np(1p)10, are also
used in the literature. Because all of these rules are only approximations, for consis-
tency’s sake we will use np5 and n(1p)5 to test for largeness of sample size in
the normal approximation to the binomial distribution. If need arises, we could use
the more stringent condition np(1p)10.
206
CHAPTER 4 Sampling Distributions

Recall that discrete random variables take no values between integers, and their
probabilities are concentrated at the integers as shown in Figure 4.7. However, the
normal random variables have zero probability at these integers; they have nonzero
probability only over intervals. Because we are approximating a discrete distribution
with a continuous distribution, we need to introduce a correction factor for continuity
which is explained below.
CORRECTION FOR CONTINUITY FOR THE NORMAL APPROXIMATION TO
THE BINOMIAL DISTRIBUTION
(a) To approximate P(Xa) or P(X>a), the correction for continuity is (a+0.5), that is,
P X  a
ð
Þ ¼ P Z < a + 0:5
ð
Þnp
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
np 1p
ð
Þ
p
 
!
and
P X > a
ð
Þ ¼ P Z > a + 0:5
ð
Þnp
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
np 1p
ð
Þ
p
 
!
:
(b) To approximate P(Xa) or P(X<a), the correction for continuity is (a0.5), that is,
P X  a
ð
Þ ¼ P Z > a0:5
ð
Þnp
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
np 1p
ð
Þ
p
 
!
and
P X < a
ð
Þ ¼ P Z < a0:5
ð
Þnp
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
np 1p
ð
Þ
p
 
!
:
(c) To approximate P(aXb), treat ends of the intervals separately, calculating two distinct
z-values according to steps (a) and (b), that is,
P a  X  b
ð
Þ ¼ P
a0:5
ð
Þnp
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
np 1p
ð
Þ
p
< Z < b + 0:5
ð
Þnp
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
np 1p
ð
Þ
p
 
!
:
(d) Use the normal table to obtain the approximate probability of the binomial event.
The shaded area in Figure 4.8 represents the continuity correction for P(X¼i).
p(x)
x
FIGURE 4.7
Probability Function of Discrete r.v.
207
4.4 Large Sample Approximations

EXAMPLE 4.4.2
A study of parallel interchange ramps revealed that many drivers do not use the entire length of parallel
lanes for acceleration, but seek, as soon as possible, a gap in the major stream of traffic to merge. At
one site on Interstate Highway 75, 46% of drivers used less than one third of the lane length available
before merging. Suppose we monitor the merging pattern of a random sample of 250 drivers at this site.
(a) What is the probability that fewer than 120 of the drivers will use less than one third of the accel-
eration lane length before merging?
(b) What is the probability that more than 225 of the drivers will use less than one third of the accel-
eration lane length before merging?
Solution
First we check for adequacy of the sample size:
np ¼ 250
ð
Þ 0:46
ð
Þ ¼ 115 and n 1 p
ð
Þ ¼ 250
ð
Þ 1 0:46
ð
Þ ¼ 135:
Both are greater than 5. Hence, we can use the normal approximation. Let X be the number of
drivers using less than one third of the lane length available before merging. Then X can be con-
sidered to be a binomial random variable. Also,
m ¼ np ¼ 250
ð
Þ 0:46
ð
Þ ¼ 115:0
and
s ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
np 1p
ð
Þ
p
¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
250 0:46
ð
Þ 0:54
ð
Þ
p
¼ 7:8804:
(a) P X < 120
ð
Þ ¼ P Z < 119:5115
7:8804
¼ 0:57103


¼ 0:7157, that is, we are approximately 71.57%
certain that fewer than 120 drivers will use less than one third of the acceleration length before
merging.
(b) P X > 225
ð
Þ ¼ P Z > 225:5115
7:8804
¼ 14:02213


	 0, that is, there is almost no chance that more
than 225 drivers will use less than one third of the acceleration lane length before merging.
f (x)
i–1/2
i+1/2
x
FIGURE 4.8
Continuity Correction for P(X¼i).
208
CHAPTER 4 Sampling Distributions

EXERCISES 4.4
4.4.1. A random sample size of 150 is taken from an infinite population having
mean m¼8 and variance s2¼4. What is the probability that X will be
between 7.5 and 10?
4.4.2. A machine that is used to fill bottles with soda has been observed to have a
true standard deviation in the amounts of fill of approximately s¼1.25
ounces. However, the mean ounces of fill m may change from day to day,
because of change of operator or adjustments in the machine. If n¼55
observations on ounces of fill are taken on a given day, find the probability
that the sample mean will be within 0.5 ounce of the true population mean.
State any assumptions.
4.4.3. The times spent by customers coming to a certain gas station to fill up can be
viewed as independent random variables with a mean of 3 minutes and a
variance of 1.5 minutes. Approximate the probability that a random sample
of 75 customers in this gas station will spend a total time less than 3 hours.
Interpret your results and state any assumptions.
4.4.4. Refer to Exercise 4.4.3. Find the number of customers, m, such that the
probability that all the m customers can fill up in less than 3 hours is
approximately 0.2.
4.4.5. In the mathematics department of a certain university, in a particular
semester, 1250 students took the elementary algebra final examination. The
mean was 69% with a standard deviation of 5.4%. If a random sample of 60
students is selected from this population, what is the probability that the
average score of this sample will be at most 75.08? Interpret your results and
state any assumptions.
4.4.6. For a newborn full-term infant, the weight appropriate for gestational age is
assumed to be normally distributed with m¼3025 grams and s¼165 grams.
Compute the probability that a random sample of 50 infants born at full term
results in a sample mean of less than 3500 grams.
4.4.7. Let X1, . . ., Xn be a random sample, each with mean m1 and standard deviation
s1. Also, let Y1, Y2, . . ., Ym be a random sample, each with mean m2 and a
standard deviation s2. Assume that both the samples are from normal
populations. Verify that X Y


 N m1 m2, 1
n s2
1 + 1
ms2
2


:
4.4.8. Let X1, . . ., Xn be a random sample, each with mean m1 and standard deviation
s1. Also, let Y1 ,Y2, . . ., Yn be a random sample independent of X1, . . ., Xn, each
with mean m2 and a standard deviation s2. Prove that the random variable
Vn ¼ X Y


 m1 m2
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
s2
1 + s2
2
n
r
satisfiestheconditionsofTheorem4.4.1andhenceVnisasymptoticallynormal.
4.4.9. Suppose X is a binomial random variable with n¼20 and p¼0.2. Find the
probability that X10 using binomial tables and compare this to the
corresponding value found from normal approximation.
209
4.4 Large Sample Approximations

4.4.10. Using normal approximation, find the probability of obtaining at least 90
heads in 150 tosses of a fair coin. Is the normal approximation valid? Why?
4.4.11. A car rental company finds that each day 6% of the persons making
reservations will not show up. If the rental company reserves for 215
persons with only 200 automobiles, what is the probability that an
automobile will be available for every person who shows up holding a
reservation? (Use the normal approximation.)
4.4.12. The president of the United States is thought to have a positive approval
rating of 58% of the people at a certain time. In a random sample of 1200
people, what is the approximate probability that the number of positive
approvals will be at least 750? Interpret your results and state any
assumptions.
4.4.13. In the United States, sudden infant death syndrome (SIDS) is one of the
leading causes of postneonatal deaths (those occurring between the ages of
28 days and 1 year). Thus far, the most significant risk factor discovered for
SIDS is placing babies to sleep in a prone position (on their stomachs).
Suppose the rate of death due to SIDS is 0.00103 per year. In a random
sample of 5000 infants between the ages of 28 days and 1 year, what is the
approximate probability that the number of SIDS-related deaths will be at
least 10? Interpret your results and state any assumptions.
4.4.14. Let X and Y be independent binomial random variables with parameters
(n, p1) and (m, p2), respectively.
(a) Find E X
n Y
n


:
(b) Find Var X
n Y
n


:
(c) Show that
X
n Y
n


 N E X
n Y
n


,Var X
n Y
n




, for large n.
4.5 CHAPTER SUMMARY
In this chapter, we learned about sampling distributions. In sampling distributions
associated with normal populations, we have seen that we can generate chi-square,
t-, and F-distributions. In Section 4.3 we dealt with order statistics. Then in
Section 4.4 we looked at large sample approximations such as the normal approxi-
mation to the binomial distribution. In the following section, we will give Minitab
examples to show how the idea of sampling distribution can be explored using sta-
tistical software.
We will now list some of the key definitions introduced in this chapter.
•
Sampling distribution.
•
Sample and sample size.
•
Random sample.
210
CHAPTER 4 Sampling Distributions

•
Statistic.
•
Standard error.
•
Finite population correction factor.
•
Degrees of freedom.
•
t-Distribution.
•
F-distribution.
•
Order statistics.
In this chapter, we have also presented the following important concepts and
procedures:
•
Sampling distribution associated with normal distribution.
•
Results on chi-square distribution.
•
Results on Student t-distribution.
•
Results on F-distribution.
•
Derivation of probability density functions for order statistics.
•
Large sample approximations.
•
Normal approximation to the binomial.
•
Correction for continuity for the normal approximation to the binomial
distribution.
4.6 COMPUTER EXAMPLES
4.6.1 EXAMPLES USING R
Note: For the following problems you’re generating random samples your answers
will vary!
EXAMPLE 4.6.1 GENERATING NORMAL RANDOM SAMPLES
Create three samples of size 30 from standard normal distribution and draw histograms for each
sample.
Notice the last two arguments are the mean and standard deviation of the distribution 0, 1.
Additionally plot a density curve over the histogram. Only one output is shown for this example.
R Code:
sample1¼rnorm(30,0,1);
sample2¼rnorm(30,0,1);
sample3¼rnorm(30,0,1);
hist(sample1,prob¼T);
lines(density(sample1),col¼”red”);
hist(sample2,prob¼T);
lines(density(sample2),col¼”red”);
hist(sample3,prob¼T);
lines(density(sample3),col¼”red”);
211
4.6 Computer Examples

Output:
Histogram of sample1
Density
0.4
0.3
0.2
0.1
0.0
–3
–2
–1
0
Sample1
1
2
3
EXAMPLE 4.6.2 GENERATING A NORMAL RANDOM SAMPLE
Generate 50,000 observations from a normal distribution with mean 30 and standard deviation 8.
Obtain summary statistics for this data and draw a graph.
R Code:
sample¼rnorm(50000,30,8);
summary(sample);
sd(sample);
hist(sample,prob¼T);
lines(density(sample),col¼”red”);
Output:
Min. 1st Qu. Median Mean 3rd Qu. Max.
0.08056 24.62000 30.01000 30.03000 35.42000 60.82000
7.981699
Standard deviation
Histogram of sample1
Density
0.04
0.03
0.02
0.01
0.00
0
Sample1
10
20
30
40
50
60
212
CHAPTER 4 Sampling Distributions

EXAMPLE 4.6.3 GENERATING A RANDOM EXPONENTIAL SAMPLE
From an exponential distribution, draw 10,000 samples, each sample of size 15. Compute the mean
of each sample and draw a chart for the means. This will be an approximate sampling distribution of
x¯ for a fixed sample of size 15.
R Code:
samples_means¼c();
##Creates an empty array for us to store the
means in.
for(i in 1:10000) {
## This for loop repeats the code inside it change
variable i over the range
sample¼rexp(15,3);
##Generates a random sample of 15 from an
exponential.
mean¼mean(sample);
## calculates the mean of that sample.
samples_means¼c(sample,mean);
## store the mean inside
our array for later use.
}
hist(samples_means,prob¼T);
##Use previous methods to check the
distribution of the means.
lines(density(samples_means),col¼”red”);
summary(samples_means);
sd(samples_means);
Output:
No output given for this particular problem, please see the graph generated by R.
You have stored the samples_means in this variable use previous analysis
methods on this variable.
4.6.2 MINITAB EXAMPLES
EXAMPLE 4.6.4
Create three samples of size 30 from standard normal distribution using Minitab, and draw histo-
grams for each sample.
Solution
We can use the following procedure:
1. Open a new worksheet.
2. Choose Calc > Random Data > Normal.
3. Generate 30 rows of data.
4. Store results in C1-C3.
5. Enter a mean of 0 and a standard deviation of 1 and click OK.
6. Choose Graph > Character Graphs > Histogram and enter C1-C3 in the variable box and click
OK. We will not give the data or any of the three histograms that we will get. These histograms
are just lines containing *0s. If we need actual histograms, in step 6 use
Graph > Histogram and enter C1 in the graph variable box and click OK
If we wish to generate descriptive statistics, then
7. Choose Stat > Basic Statistics > Display Descriptive statistics. . ., enter C1-C3 in the variable
box, and click OK.
Continued
213
4.6 Computer Examples

If we would like to see the mean for the three samples,
8. Choose Calc > Row Statistics, then click Mean and in the Input variables type C1-C3. In Store
Result in: C4 and Click OK.
To see the histogram of these averages, follow step 6 with C4 in the graph variable box.
Using a similar procedure, one could generate samples from normal distributions with different
means and standard deviations, as well as from other distributions.
4.6.3 SPSS EXAMPLES
If we have the full version of SPSS, we can write code that can be used to simulate
a sampling distribution with different values of p. However, with the student version,
it is not easy to simulate. Therefore, we will not give SPSS examples in this chapter.
4.6.4 SAS EXAMPLES
EXAMPLE 4.6.5
Generate 50,000 observations from a normal distribution with mean 30 and standard deviation 8.
Obtain summary statistics for these data and draw a graph.
Solution
We could use the following program.
title ’50000 Obs Sample from a Normal Distribution’;
title2 ’with Mean¼30 and Standard Deviation¼8’;
data normaldat;
do n¼1 to 50000;
X¼8*rannor(55)+30;
output;
end;
run;
proc univariate data¼normaldat;
var x;
run;
proc chart;
vbar x / midpoints¼6 to 54 by 2;
format x msd.;
run;
In the foregoing program, rannor(55), the number 55 is just a seed number to obtain the same
series of random numbers each time we run the program. If we use ‘0’, each time we run the program
we will get a different set of random numbers. We will not give the output.
214
CHAPTER 4 Sampling Distributions

EXAMPLE 4.6.6
From an exponential distribution, draw 10,000 samples, each sample of size 15. Compute the mean
of each sample and draw a chart for the means. This will be an approximate sampling distribution of
X for a fixed sample of size 15.
Solution
Use the following program.
title ’10000 Sample Means with 15 Obs per Sample’;
title2 ’Drawn from an Exponential Distribution’;
data sample15;
do Sample¼1 to 10000;
do n¼1 to 15;
X¼ranexp(3);
output;
end;
end;
proc means data¼sample 15 noprint;
output out¼mean 15 mean¼Mean;
var x;
by sample;
run;
proc chart data¼mean 15;
vbar mean/axis¼1800
midpoints¼0.10 to 2.05 by .1;
run;
proc univariate data¼mean4 noextrobs¼0 normal
mu0¼1;
var mean;
run;
This will produce an approximate sampling distribution of X. We will not give the output.
PROJECTS FOR CHAPTER 4
4A. A METHOD TO OBTAIN RANDOM SAMPLES FROM DIFFERENT
DISTRIBUTIONS
Most of the statistical software packages contain a random number generator
that produces approximations to random numbers from the uniform distribution U
[0, 1]. To simulate the observation of any other continuous random variables, we
can start with uniform random numbers and associate these to the distribution we
want to simulate. For example, suppose we wish to simulate an observation from
the exponential distribution
F x
ð Þ ¼ 1e0:5x, 0 < x < 1:
215
Projects for Chapter 4

First produce the value of y from the uniform distribution. Then solve for x from the
equation
y ¼ F x
ð Þ ¼ 1e0:5x:
So x¼[ln (1y)]/0.5 is the corresponding value of the exponential random vari-
able. For instance, if y¼0.67, then x¼[ln (1y)]/0.5¼2.2173. If we wish to sim-
ulate a sample from the distribution F from the different values of y obtained from the
uniform distribution, the procedure is repeated for each new observation x.
(a) Simulate 10 observations of a random variable having exponential distribution
with mean and standard deviation both equal to 2.
(b) Select 1500 random samples of size n¼10 measurements from a population with
an exponential distribution with mean and standard deviation both equal to 2.
Calculate sample mean for each of these 1500 samples and draw a relative
frequency histogram. Based on Theorems 4.1.1 and 4.4.1, what can you conclude?
It should be noted that in general, if YU (0, 1) random variable, then we can
show that X ¼ 1nY
l inwill give an exponential random variable with parameter l.
Uniform random variables could also be used to generate random variables from
other distributions. For example, let Uis be iid U[0, 1] random variables. Then,
X ¼ 2
X
v
i¼1
ln Ui
ð
Þ  w2
2v,
and
Y ¼ b
X
a
i¼1
ln Ui
ð
Þ  Gamma a, b
ð
Þ:
Of course, these transformations are useful only when v and a are integers. More effi-
cient methods based on Monte Carlo simulations, such as MCMC methods, are dis-
cussed in Chapter 13.
4B. SIMULATION EXPERIMENTS
When the derivation via probability rules is too difficult or complicated to be carried
out, one can use simulation experiments to obtain information about a statistic’s sam-
pling distribution. The following characteristics of the experiment must be specified:
(i) The population distribution (normal with m¼10 and s¼2, exponential with
l¼5, etc.)
(ii) The sample size n and the statistic of interest (X, S, etc.)
(iii) The number of replications k (such as k¼300)
Then, using a computer program, obtain k different random samples, each of size
n, from the designated population distribution. Calculate the value of the statistic for
each of the k replications. Construct a histogram for this k statistic. This histogram
gives the approximate sampling distribution of the statistic. The larger the value of k,
the better will be the approximation.
216
CHAPTER 4 Sampling Distributions

(a) For your simulation study, use the population distribution as normal with m¼3.4
and s¼1.2.
For n¼8 perform k¼500 replications and draw a histogram for values of the
sample means. Repeat the experiment with n¼15, n¼25, and n¼35 and
draw the histograms. Based on this exercise, you will be able to intuitively
verify the result that X based on a large n tends to be closer to m than does X based
on a small n.
(b) Repeat the experiment of part (a) with different values of k, such as k¼200,
k¼750, and k¼1000.
(c) Repeat the simulation study with different distributions such as exponential
distribution.
4C. A TEST FOR NORMALITY
Many statistical procedures require that the population be at least approximately nor-
mal. Therefore, a procedure is needed for checking that the sampled data could have
come from a normal distribution. There are many procedures, such as the normal-
score plot, or Lilliefors test for normality, available in statistics for this purpose.
We will describe the normal-score plot, which is an effective way to detect devia-
tions from normality. The normal scores consist of values of z that divide the axes
into equal probability intervals. For a sample of size 4, the normal scores are
z0.20¼0.84, z0.40¼0.25, z0.40¼0.25, and z0.20¼0.84.
STEPS TO CONSTRUCT A NORMAL PLOT
1. Rearrange the n data points in ascending order.
2. Obtain the n normal scores.
3. Plot the kth largest observation, versus the kth normal score, for all k.
4. If the data were from a standard normal distribution, the plot would resemble a 45 degree line
through the origin.
5. If the observations were from normal (but not from standard normal), the pattern should still be a
straight line. However, the line need not pass through the origin or have a slope 1.
In applications, a minimum of 15 to 20 observations is needed to reach a more
accurate conclusion.
EXERCISES
1. For different observations, construct normal plots and check for normality of the
corresponding populations.
2. Using software (such as Minitab), generate 15 observations each from the
following distributions: (a) Normal (2, 4), (b) Uniform (0, 1), (c) Gamma (2, 4),
and (d) Exponential (2).
For each of these data sets, draw a probability plot and note the geometry of
the plots.
217
Projects for Chapter 4

CHAPTER
Statistical Estimation
5
CHAPTER CONTENTS
5.1 Introduction .................................................................................................... 220
5.2 The Methods of Finding Point Estimators .......................................................... 221
5.3 Some Desirable Properties of Point Estimators .................................................. 245
5.4 A Method of Finding the Confidence Interval: Pivotal Method ............................. 261
5.5 One Sample Confidence Intervals ..................................................................... 269
5.6 A Confidence Interval for the Population Variance ............................................. 284
5.7 Confidence Interval Concerning Two Population Parameters .............................. 289
5.8 Chapter Summary ............................................................................................ 298
5.9 Computer Examples ......................................................................................... 299
Projects for Chapter 5 ............................................................................................ 303
OBJECTIVE
In this chapter we study some statistical methods to find estimators of population
parameters and study their properties. This will include methods of finding point esti-
mation as well as interval estimation of the unknown population parameters.
C.R. Rao
(Source: http://science.psu.edu/news-and-events/2014-news/Rao4-2014)
Mathematical Statistics with Applications in R
Copyright © 2015 Elsevier Inc. All rights reserved.
219

Calyampudi Radhakrishna (C.R.) Rao (1920-) is a contemporary statistician
whose work has influenced not just statistics, but such diverse fields as anthropology,
biometry, demography, economics, genetics, geology, and medicine. Several statis-
tical terms and equations are named after Rao. He has worked with many other
famous statisticians such as Blackwell, Fisher, and Neyman and has had dozens
of theorems named after him. Rao earned an MA in mathematics and another MA
in statistics, both in India, and earned his PhD and ScD at Cambridge University.
The following was stated in the Preface to the 1991 special issue of the Journal
of Quantitative Economics in Rao’s honor: “Dr. Rao is a very distinguished scientist
and a highly eminent statistician of our time. His contributions to statistical theory
and applications are well known, and many of his results, which bear his name, are
included in the curriculum of courses in statistics at bachelor’s and master’s level all
over the world. He is an inspiring teacher and has guided the research work of numer-
ous students in all areas of statistics. His early work had greatly influenced the course
of statistical research during the last four decades. One of the purposes of this special
issue is to recognize Dr Rao’s own contributions to econometrics and acknowledge
his major role in the development of econometric research in India.” The importance
of statistics can be summarized in Rao’s own words: “If there is a problem to be
solved, seek statistical advice instead of appointing a committee of experts. Statistics
can throw more light than the collective wisdom of the articulate few.” http://www.
finse.uio.no/events/international-workshops/introduction-to-estimation/
5.1 INTRODUCTION
In statistical analysis, estimation of population parameters plays a very significant role.
In most applied problems, a certain numerical characteristic of the physical phenom-
enon may be of interest; however, its value may not be observable directly. Instead,
supposeitis possibletoobserve oneormorerandomvariables,thedistributionofwhich
depends on the characteristic of interest. Our objective will be to develop methods that
use the observed values of random variables (sample data) in order to gain information
about the unknown and unobservable characteristic of the population.
In studying a real world phenomenon we begin with a random sample of size n
taken from the totality of a population. In estimation theory, it is assumed the obser-
vations are random with probability distribution dependent on some parameters of
interest. The initial step in statistically analyzing these data is to be able to identify
the probability distribution that characterizes this information. Since the parameters
of a distribution are its defining characteristics, it becomes necessary to know the
parameters. In the present chapter, we shall assume that the form of the population
distribution is known (such as a binomial, normal, etc.) but the parameters of the dis-
tribution (p for a binomial; m and s2 for a normal, etc.) are unknown. We shall esti-
mate these parameters using the data from our random sample. It is extremely
important to have the best possible estimate of the population parameter(s). Having
such estimates will lead to a better and more accurate statistical analysis.
220
CHAPTER 5 Statistical Estimation

For example, due to phosphate mining in Florida, we may be interested in esti-
mating the average radioactivity from both uranium and radium in a clay settling area
of a mining site. Suppose that a random sample of 10 such sites resulted in a sample
average of 40 pCi/g (picocuries/gram) radioactivity. We may use this value as an
estimate of the average radioactivity for all of the settling areas of mining sites in
Florida. We may also want to know a range of values of radioactivity with certain
confidence. Since many Florida crops are grown on clay settling areas, this type of
estimates are important for accessing the risks associated with radioactivity due to
eating food from the crops grown on these clay settling areas.
There are two types of estimators, namely, point estimator and interval estimator.
First, we will introduce statistical point estimation methods, discuss their properties,
and illustrate their usefulness with a number of applications. The importance of point
estimates lies in the fact that many statistical formulas are based on them. For exam-
ple, the point estimates of mean and standard deviation are needed in the calculation
of confidence intervals and in many formulas for hypothesis testing. These topics
will be covered in subsequently. In general, the point estimates will differ from
the true parameter values by varying amounts depending on the sample values
obtained. In addition, the point estimates do not convey any measure of reliability.
To deal with these issues, we will also introduce so-called interval estimation or the
confidence intervals.
5.2 THE METHODS OF FINDING POINT ESTIMATORS
Let X1, . . ., Xn be independent and identically distributed (iid) random variables
(in statistical language, a random sample) with a probability density function or
probability mass function (pmf) f (x, y1, . . ., yl), where y1, . . ., yl are the unknown
population parameters (characteristics of interest). For example, a normal pdf has
parameters m (the mean) and s2 (the variance). The actual values of these parameters
are not known. The problem in point estimation is to determine statistics gi(X1, . . .,
Xn), i¼1, . . ., l, which can be used to estimate the value of each of the parameters—
that is, to assign an appropriate value for the parameters u¼(y1, . . ., yl) based on
observed sample data from the population. These statistics are called estimators
for the parameters, and the values calculated from these statistics using particular
sample data values are called estimates of the parameters. Estimators of yi are
denoted by ^yi, where ^yi ¼ gi X1, ..., Xn
ð
Þ, i ¼ 1, ...,l. Observe that the estimators
are random variables. As a result, an estimator has a distribution (which we called
the sampling distribution in Chapter 4). When we actually run the experiment and
observe the data, let the observed values of the random variables be X1, ..., Xn be
x1, ..., xn; then, ^y X1, ..., Xn
ð
Þ is an estimator, and its value ^y x1, ..., xn
ð
Þ is an esti-
mate. For example, in case of the normal distribution, the parameters of interest
are y1¼m, and y2¼s2, that is, u¼(m, s2). If the estimators of m and s2 are
X ¼ 1=n
ð
Þ
Xn
i¼1Xi and S2 ¼ 1= n1
ð
Þ
ð
Þ
Xn
i¼1 Xi X

2 respectively, then, the
221
5.2 The Methods of Finding Point Estimators

corresponding estimates are x ¼ 1=n
ð
Þ
Xn
i¼1xi and s2 ¼ 1=n1
ð
Þ
Xn
i¼1 xi x
ð
Þ2, the
mean and variance corresponding to the particular observed sample values. In this
book, we use capital letters such as X and S2 to represent the estimators, and lower-
case letters such as x and s2 to represent the estimates.
There are many methods available for estimating the true value(s) of the
parameter(s) of interest. Three of the more popular methods of estimation are the
method of moments, the method of maximum likelihood, and Bayes’ method. A very
popular procedure among econometricians to find a point estimator is the general-
ized method of moments (GMM). In this chapter we study only the method of
moments and the method of maximum likelihood for obtaining point estimators
and some of their desirable properties. In Chapter 11, we shall discuss Bayes’ method
of estimation.
There are many criteria for choosing a desired point estimator. Heuristically,
some of them can be explained as follows. An estimator, ^y, is unbiased if the mean
of its sampling distribution is the parameter y. The bias of ^y is given by
B ¼ E ^y
 
y. The estimator has the sufficiency property if it fully uses all the sam-
ple information. Minimal sufficient statistics are those that are sufficient for the
parameter and are functions of every other set of sufficient statistics for those same
parameters. A method due to Lehmann and Scheffe´ can be used to find a minimal
sufficient statistic. In addition, the estimator are said to satisfy the consistency prop-
erty if the sample estimator has a high probability of being close to the population
value y for a large sample size. The concept of efficiency is based on comparing var-
iances of the different unbiased estimators. If there are two unbiased estimators, it is
desirable to have the one with the smaller variance. However, some of these prop-
erties will not be discussed in this book.
How do we find a good point estimator with desirable properties? To answer this
question, we will study two methods of finding point estimators, namely, the method
of moments and the method of maximum likelihood.
5.2.1 THE METHOD OF MOMENTS
One of the oldest methods for finding point estimators is the method of moments.
This is a very simple procedure for finding an estimator for one or more popu-
lation parameters. Let mk0 ¼E[Xk] be the kth moment about the origin of a
random variable X, whenever it exists. Let mk
0 ¼(1/n)P
i¼1
n
Xi
k be the correspond-
ing kth sample moment. Then, the estimator of mk0 by the method of moments is
mk0. The method of moments is based on matching the sample moments with the
corresponding population (distribution) moments and is founded on the assump-
tion that sample moments should provide good estimates of the corresponding
population moments. Because the population moments mk0 ¼hk(y1,y2,. . .,yl) are
often functions of the population parameters, we can equate corresponding pop-
ulation and sample moments and solve for these parameters in terms of the
moments.
222
CHAPTER 5 Statistical Estimation

METHOD OF MOMENTS
Choose as estimates those values of the population parameters that are solutions of the equations
mk0 ¼mk0, k¼1,2,. . .,l. Here mk0 is a function of the population parameters.
For example, the first population moment is m10 ¼E(X), and the first sample
moment is X ¼
Xn
i¼1Xi=n. Hence, the moment estimator of m10 is X. If k¼2, then
the second population and sample moments are m20 ¼E(X2) and m20 ¼(1/n)P
i¼1
n Xi
2,
respectively. Basically, we can use the following procedure in finding point estima-
tors of the population parameters using the method of moments.
THE METHOD OF MOMENTS PROCEDURE
Suppose there are l parameters to be estimated, say y¼(y1, . . ., yl).
1. Find l population moments, mk0, k¼1,2,. . .,l. mk0 will contain one or more parameters y1, . . ., yl.
2. Find the corresponding l sample moments, mk0, k¼1,2,. . .,l. The number of sample moments
should equal the number of parameters to be estimated.
3. From the system of equations, mk0 ¼mk0, k¼1,2,. . .,l, solve for the parameter y¼(y1, . . ., yl);
this will be a moment estimator of ^y.
The following examples illustrate the method of moments for population param-
eter estimation.
EXAMPLE 5.2.1
Let X1, . . ., Xn be a random sample from a Bernoulli population with parameter p.
(a) Find the moment estimator for p.
(b) Tossing a coin 10 times and equating heads to value 1 and tails to value 0, we obtained the fol-
lowing values:
0 1 1 0 1 0 1 1 1 0
Obtain a moment estimate for p, the probability of success (head).
Solution
(a) For the Bernoulli random variable, mk0 ¼E[X]¼p, so we can use m10 to estimate p. Thus,
m0
1 ¼ ^p ¼ 1
n
X
n
i¼1
Xi:
Let
Y ¼
X
n
i¼1
Xi:
Then, the method of moments estimator for p is ^p ¼ Y=n. That is, the ratio of the total number of
heads to the total number of tosses will be an estimate of the probability of success.
(b) Note that this experiment results in Bernoulli random variables. Thus, using part (a) with Y¼6,
we get the moment estimate of p is ^p ¼ 6=10 ¼ 0:6.
Continued
223
5.2 The Methods of Finding Point Estimators

We would use this value ^p ¼ 0:6, to answer any probabilistic questions for the given problem.
For example, what is the probability of exactly obtaining eight heads out of 10 tosses of this coin?
This can be obtained by using the binomial formula, with ^p ¼ 0:6, that is,
P X ¼ 8
ð
Þ ¼
10
8


0:6
ð
Þ8 0:4
ð
Þ108:
In Example 5.2.1, we used the method of moments to find a single parameter. We
demonstrate in Example 5.2.2 how this method is used for estimating more than one
parameter.
EXAMPLE 5.2.2
Let X1, . . ., Xn be a random sample from a gamma probability distribution with parameters a and b.
Find moment estimators for the unknown parameters a and b.
Solution
For the gamma distribution (see Section 3.2.5),
E X
½  ¼ ab and E X2

	
¼ ab2 + a2b2:
Because there are two parameters, we need to find the first two moment estimators. Equating
sample moments to distribution (theoretical) moments, we have
1
n
X
n
i¼1
Xi ¼ X ¼ ab, and 1
n
X
n
i¼1
X2
i ¼ ab2 + a2b2:
Solving for a and b we obtain the estimates as a ¼ x=b
ð
Þ and b ¼
1=n
ð
Þ
Xn
i¼1x2
i x2
n
o
=x
h
i
Therefore, the method of moments estimators for a and b are
^a ¼ X
^b
and
^b ¼
1
n
Xn
i¼1X2
i X
2
X
¼
Xn
i¼1 Xi X

2
nX
,
which implies that
^a ¼ X
^b
¼
X
2
1
n
Xn
i¼1X2
i X
2 ¼
X
2
Xn
i¼1 Xi X

2 :
Thus, we can use these values in the gamma pdf to answer questions concerning the probabi-
listic behavior of the r.v. X.
Following example shows that once we find the moments estimator theoretically,
the estimate can be obtained by simply substituting sample statistic into the formula.
224
CHAPTER 5 Statistical Estimation

EXAMPLE 5.2.3
Let the distribution of X be N(m, s2).
(a) For a given sample of size n, use the method of moments to estimate m and s2.
(b) The following data (rounded to the third decimal digit) were generated using Minitab from a
normal distribution with mean 2 and a standard deviation of 1.5.
3:163 1:883
3:252
3:716 0:049 0:653 0:057 2:987
4:098 1:670
1:396
2:332
1:838
3:024
2:706 0:231
3:830 3:349 0:230 1:496
Obtain the method of moments estimates of the true mean and the true variance.
Solution
(a) For the normal distribution, E(X)¼m, and because Var(X)¼EX2m2, we have the second
moment as E(X2)¼s2+m2.
Equating sample moments to distribution moments we have
1
n
X
n
i¼1
Xi ¼ m0
1 ¼ m
and
m0
2 ¼ 1
n
X
n
i¼1
X2
i ¼ s2 + m2:
Solving for m and s2, we obtain the moment estimators as
^m ¼ X
and
^s2 ¼ 1
n
X
n
i¼1
X2
i X
2 ¼ 1
n
X
n
i¼1
Xi X

2:
(b) Because we know that the estimator of the mean is ^m ¼ X and the estimator of the variance is
^s2 ¼ 1=n
ð
ÞSn
i¼1X2
i X
2,
from
the
data
the
estimates
are
^m ¼ 2:005,
and
^s2 ¼ 6:12 2:005
ð
Þ2 ¼ 2:1. Notice that the true mean is 2 and the true variance is 2.25, which
we used to simulate the data.
In general, using the population pdf we evaluate the lower order moments, find-
ing expressions for the moments in terms of the corresponding parameters. Once we
have population (theoretical) moments, we equate them to the corresponding sample
moments to obtain the moment estimators.
EXAMPLE 5.2.4
Let X1, . . ., Xn be a random sample from a uniform distribution on the interval [a, b]. Obtain method
of moment estimators for a and b.
Solution
Here, a and b are treated as parameters. That is, we only know that the sample comes from a uniform
distribution on some interval, but we do not know from which interval. Our interest is to estimate this
interval. The pdf of a uniform distribution is
Continued
225
5.2 The Methods of Finding Point Estimators

f x
ð Þ ¼
1
ba,
a  x  b
0,
otherwise:
8
<
:
Hence, the first two population moments are
m1 ¼ E X
ð Þ ¼
ðb
z
x
badx ¼ a + b
2
and m2 ¼ E X2


¼
ðb
a
x2
badx ¼ a2 + ab + b2
3
:
The corresponding sample moments are
^m1 ¼ X and ^m2 ¼ 1
n
X
n
i¼1
X2
i :
Equating the first two sample moments to the corresponding population moments, we have
^m1 ¼ a + b
2
and ^m2 ¼ a2 + ab + b2
3
which, solving for a and b, results in the moment estimators of a and b,
^a ¼ ^m1 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
3 ^m2  ^m2
1


q
and ^b ¼ ^m1 +
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
3 ^m2  ^m2
1


q
:
In Example 5.2.4, if a¼b, that is, X1, . . ., Xn is a random sample from a uniform dis-
tribution on the interval (b, b), the problem reduces to a one-parameter estimation
problem.However,inthiscaseE(Xi)¼0,sothe first momentcannotbeusedtoestimate
b.Itbecomesnecessarytousethesecondmoment.Forthederivation,seeExercise5.2.3.
It is important to observe that the method of moments estimators need not be
unique. The following is an example of the nonuniqueness of moment estimators.
EXAMPLE 5.2.5
Let X1, . . ., Xn be a random sample from a Poisson distribution with parameter l>0. Show that both
(1/n)P
i¼1
n Xi and 1=n
ð
Þ
Xn
i¼1Xi
2 
1=n
ð
Þ
Xn
i¼1Xi

2
are moment estimators of l.
Solution
We know that E(X)¼l, from which we have a moment estimator of l as (1/n)P
i¼1
n Xi. Also,
because we have Var(X)¼l, equating the second moments, we can see that
l ¼ E X2


 EX
ð
Þ2,
so that
^l ¼ 1
n
X
n
i¼1
X2
i 
1
n
X
n
i¼1
Xi
 
!2
:
Thus,
^l ¼ 1
n
X
n
i¼1
Xi
and
^l ¼ 1
n
X
n
i¼1
X2
i 
1
n
X
n
i¼1
Xi
 
!2
:
Both are moment estimators of l. Thus, the moment estimators may not be unique. We generally
choose X as an estimator of l, for its simplicity.
226
CHAPTER 5 Statistical Estimation

It is important to note that, in general, we have as many moment conditions as the
parameters. In Example 5.2.5, we have more moment conditions than parameters,
because both the mean and variance of Poisson random variables are the same.
Given a sample, this results in two different estimates of a single parameter.
One of the questions could be, can these two estimators be combined in some
optimal way? This is done by the so-called GMM. We will not deal with this
topic. The method of moments often provides estimators when other methods fail
to do so or when estimators are harder to obtain, as in the case of a gamma dis-
tribution. Compared to other methods, method of moments estimators are easier
to compute and have some desirable properties that we will discuss in ensuing
section.
5.2.2 THE METHOD OF MAXIMUM LIKELIHOOD
Now we will present an important method for finding estimators of parameters
proposed by geneticist/statistician Sir Ronald A. Fisher around 1922 called the
method of maximum likelihood. Even though the method of moments is intuitive
and easy to apply, it usually does not yield “good” estimators. The method of
maximum likelihood is intuitively appealing, because we attempt to find the values
of the true parameters that would have most likely produced the data that we in
fact observed. For most cases of practical interest, the performance of MLEs is
optimal for large enough data. This is one of the most versatile methods for fitting
parametric statistical models to data. First, we define the concept of a likelihood
function.
Definition 5.2.1 Let f(x1, . . ., xn; y), y 2 Y  ℝk , be the joint probability (or
density) function of n random variables X1, ..., Xn with sample values x1, ..., xn.
The likelihood function of the sample is given by
L y; x1, ..., xn
ð
Þ ¼ f x1, ..., xn; y
ð
Þ, ¼ L y
ð Þ, in a briefer notation
½
:
We emphasize that L is a function of y for fixed sample values.
The likelihood of a set of parameter values u, given x1, . . ., xn, is equal to the
probability of those observed outcomes given the parameter values. If X1, . . ., Xn are
discrete iid random variables with probability function p(x, y), then, the likelihood
function is given by
L y
ð Þ ¼ P X1 ¼ x1, ...,Xn ¼ xn
ð
Þ
¼
Y
n
i¼1
P Xi ¼ xi
ð
Þ, by multiplication rule for independent random variables
ð
Þ
¼
Y
n
i¼1
p xi, y
ð
Þ
and in the continuous case, if the density is f(x, y), then the likelihood function is
L y
ð Þ ¼
Y
n
i¼1
f xi, y
ð
Þ:
227
5.2 The Methods of Finding Point Estimators

It is important to note that the likelihood function, although it depends on the
observed sample values x¼(x1, . . ., xn), is to be regarded as a function of the param-
eter y. In the discrete case, L(y; x1, . . ., xn) gives the probability of observing
x¼(x1, . . ., xn), for a given y. Thus, the likelihood function is a statistic, depending
on the observed sample x¼(x1, . . ., xn).
EXAMPLE 5.2.6
Let X1, . . ., Xn be iid N(m, s2) random variables. Let x1, . . ., xn be the sample values. Find the like-
lihood function.
Solution
The density function for the normal variable is given by f x
ð Þ ¼
1
s ﬃﬃﬃﬃ
2p
p
exp  xm
ð
Þ2
2s2
 
!
. Hence, the
likelihood
L m, s2


¼
Y
n
i¼1
1ﬃﬃﬃﬃﬃﬃﬃﬃ
2ps
p
exp  xi m
ð
Þ2
2s2
 
!
¼
1
2p
ð
Þn=2sn exp

X
n
i¼1
xi m
ð
Þ2
2s2
0
B
B
B
@
1
C
C
C
A:
A statistical procedure should be consistent with the assumption that the best expla-
nation of a set of data is provided by an estimator ^y, which will be the value of the
parameter y that maximizes the likelihood function. This value of y will be called the
MLE. The goal of maximum likelihood estimation is to find the parameter value(s)
that makes the observed data most likely.
Definition 5.2.2 The MLEs are those values of the parameters that maximize the
likelihood function with respect to the parameter y. That is,
L ^y; x1, ..., xn


¼ max
y2Y L y; x1, ..., xn
ð
Þ
where Y is the set of possible values of the parameter y.
The method of maximum likelihood extends to the case of several parameters.
Let X1, . . ., Xn be a random sample with joint pmf (if discrete) or pdf (if continuous)
L y1, ..., ym; x1, ..., xn
ð
Þ ¼ f x1, x2, ..., xn; y1, y2, ..., ym
ð
Þ
where the values of the parameters y1, . . ., ym are unknown and x1, . . ., xn are the
observed sample values. Then, the maximum likelihood estimates ^y1, ..., ^ym are
those values of the yi’s that maximize the likelihood function, so that
f x1, ..., xn; ^y1, ..., ^ym


 f x1, ..., xn; y1, ..., ym
ð
Þ for all allowable y1, ...,ym:
Note that the likelihood function conveys to us how feasible the observed sample is
as a function of the possible parameter values. Maximum likelihood estimates give
the parameter values for which the observed sample is most likely to have been gen-
erated. In general, the maximum likelihood method results in the problem of
228
CHAPTER 5 Statistical Estimation

maximizing a function of single or several variables. Hence, in most situations, the
methods of calculus can be used. In deriving the MLEs, however, there are situations
where the techniques developed are more problem specific. Sometimes we need to
use numerical methods, such as Newton’s method.
In order to find a MLE, we need only to compute the likelihood function and then
maximize that function with respect to the parameter of interest. In many cases, it is
easier to work with the natural logarithm (ln) of the likelihood function, called the
log-likelihood function. Because the natural logarithm function is increasing, the
maximum value of the likelihood function, if it exists, will occur at the same point
as the maximum value of the log-likelihood function. We now summarize the
calculus-based procedure to find MLEs.
PROCEDURE TO FIND MLE
1. Define the likelihood function, L(y).
2. Often it is easier to take the natural logarithm (ln) of L(y).
3. When applicable, differentiate ln L(y) with respect to y, and then equate the derivative to zero.
4. Solve for the parameter y, and we will obtain ^y.
5. Check whether it is a maximizer or global maximizer.
EXAMPLE 5.2.7
Suppose X1, . . ., Xn are a random sample from a geometric distribution with parameter p, 0p1.
Find MLE ^p.
Solution
For the geometric distribution, the pmf is given by
f x, p
ð
Þ ¼ p 1p
ð
Þx1, 0  p  1, x ¼ 1,2,3, ...
Hence, the likelihood function is
L p
ð Þ ¼
Y
n
i¼1
p 1p
ð
Þx1
h
i
¼ pn 1p
ð
Þn +Pn
i¼1xi:
Taking the natural logarithm of L(p),
lnL ¼ n lnp +
n +
X
n
i¼1
xi
 
!
ln 1p
ð
Þ:
Taking the derivative with respect to p, we have
d lnL
dp ¼ n
p
n +
Xn
i¼1xi
1p
:
Equating d lnL p
ð Þ
dp
to zero, we have
n
p
n +
Xn
i¼1xi
1p
¼ 0:
Solving for p,
Continued
229
5.2 The Methods of Finding Point Estimators

p ¼
n
Xn
i¼1xi
¼ 1
x:
Thus, we obtain an MLE of p as
^p ¼
n
Xn
i¼1xi
¼ 1
X:
We remark that 1=X


is the maximum likelihood estimate of p. It can be shown that ^p is a global
maximum.
EXAMPLE 5.2.8
(a) Suppose X1, . . ., Xn are random samples from a Poisson distribution with parameter l. Find
MLE ^l.
(b) Traffic engineers use Poisson distribution to model light traffic. This is based on the rational that
when rate is approximately constant in a light traffic, the distribution of counts of cars in a given
time interval should be Poisson. Following data shows the number of vehicles turns left in 15
randomly chosen 5 minute intervals at a specific intersection. Calculate maximum likelihood
estimate.
10
17
12
6
12
11
9
6
10
8
8
16
7
10
6
Solution (a)
We have the probability mass function
p x
ð Þ ¼ lxel
x!
, x ¼ 0,1,2, ..., l > 0:
Hence, the likelihood function is
L l
ð Þ ¼
Y
n
i¼1
lxiel
xi!
¼ l
Pn
i¼1xienl
Yn
i¼1xi!
:
Then, taking the natural logarithm, we have
lnL l
ð Þ ¼
X
n
i¼1
xi lnlnl
X
n
i¼1
ln xi!
ð
Þ
and differentiating with respect to l results in
d lnL l
ð Þ
dl
¼
Xn
i¼1xi
l
n
and
d lnL l
ð Þ
dl
¼ 0, implies
Xn
i¼1xi
l
n ¼ 0:
That is,
l ¼
Xn
i¼1xi
n
¼ x:
Hence, the MLE of l is
^l ¼ X:
(b) From part (a) we have the estimate as ^l ¼ x ¼ 9:8
Or approximately 10 vehicles per 5 minutes turn left at this intersection.
230
CHAPTER 5 Statistical Estimation

It can be verified that the second derivative is negative and, hence, we really have a
maximum.
Sometimes the method of derivatives cannot be used for finding the MLEs. For
example, the likelihood is not differentiable in the range space. In this case, we need
to make use of the special structures available in the specific situation to solve the
problem. The following is one such case.
EXAMPLE 5.2.9
Let X1, . . ., Xn be a random sample from U(0, y), y>0. Find the MLE of y.
Solution
Note that the pdf of the uniform distribution is
f x
ð Þ ¼
1
y,
0  x  y
0, otherwise:
8
<
:
Hence, the likelihood function is given by
L y, x1, x2, ..., xn
ð
Þ ¼
1
yn , 0  x1,x2, ...,xn  y
0,
otherwise:
8
<
:
When ymax(xi), the likelihood is (1/yn), which is positive and decreasing as a function of y
(for fixed n). However, for y<max(xi) the likelihood drops to 0, creating a discontinuity at the point
max(xi) (this is the minimum value of y that can be chosen which still satisfies the condition
0xiy), and Figure 5.1 shows that the maximum occurs at this point. Hence, we will not be able
to find the derivative. Thus, the MLE is the largest order statistic,
^y ¼ max Xi
ð
Þ ¼ X n
ð Þ:
In the previous example, because E(X)¼(y/2), we can see that y¼2E(X). Hence, the
method of moments estimator for y is ^y ¼ 2X:Sometimes the method of moments
estimator can give meaningless results. To see this, suppose we observe values 3,
5, 6, and 18 from a U(0, y) distribution. Clearly, the maximum likelihood estimate
FIGURE 5.1
Likelihood function for uniform probability distribution.
231
5.2 The Methods of Finding Point Estimators

of y is 18, whereas the method of moments estimate is 16, which is not quite accept-
able, because we have already observed a value of 18.
As mentioned earlier, if the unknown parameter y represents a vector of param-
eters, say y¼(y1, . . ., yl), then the MLEs can be obtained from solutions of the system
of equations
@
@y lnL y1, ..., yn
ð
Þ ¼ 0, for i ¼ 1, ...,l:
These are called the maximum likelihood equations and the solutions are denoted by
^y1, ..., ^yl


.
EXAMPLE 5.2.10
Let X1, . . ., Xn be N (m, s2).
(a) If m is unknown and s2¼s0
2 is known, find the MLE for m.
(b) If m¼m0 is known and s2 is unknown, find the MLE for s2.
(c) If m and s2 are both unknown, find the MLE for y¼(m, s2).
Solution
In order to avoid notational confusion when taking the derivative, let y¼s2. Then, the likelihood
function is
L m, y
ð
Þ ¼ 2py
ð
Þn=2exp 
Xn
i¼1 xi m
ð
Þ2
2y
 
!
or
lnL m, y
ð
Þ ¼ n
2 ln 2p
ð
Þn
2 lny
Xn
i¼1 xi m
ð
Þ2
2y
:
(a) When y¼y0¼s0
2 is known, the problem reduces to estimating the only one parameter, m.
Differentiating the log-likelihood function with respect to m,
@
@m lnL m, y0
ð
Þ
ð
Þ ¼
2
Xn
i¼1 xi m
ð
Þ
2y0
:
Setting the derivative equal to zero and solving for m,
X
n
i¼1
xi m
ð
Þ ¼ 0:
From this,
X
n
i¼1
xi ¼ nm or m ¼ x:
Thus, we get ^m ¼ X:
(b) When m¼m0 is known, the problem reduces to estimating the only one parameter, s2¼y.
Differentiating the log-likelihood function with respect to y,
@ lnL m, y
ð
Þ
@y
¼ n
2y +
Xn
i¼1 xi m
ð
Þ2
2y2
:
Setting the derivative equal to zero and solving for y, we get
232
CHAPTER 5 Statistical Estimation

^y ¼ ^s2 ¼
Xn
i¼1 Xi m0
ð
Þ2
n
:
(c) When both m and y are unknown, we need to differentiate with respect to both m and y
individually:
@ ln L m, y
ð
Þ
@m
¼
2
Xn
i¼1 xi m
ð
Þ
2y
and
@ ln L m, y
ð
Þ
@y
¼ n
2y +
Xn
i¼1 xi m
ð
Þ2
2y2
:
Setting the derivatives equal to zero and solving simultaneously, we obtain
^m ¼ X,
^s2 ¼ ^y ¼
Xn
i¼1 Xi X

2
n
¼ S02:
Note that in (a) and (c), the estimates for m are the same; however, in (b) and (c), the estimates
for s2 are different.
At times, the MLEs may be hard to calculate. It may be necessary to use numerical
methods to approximate values of the estimate. The following example gives one
such case.
EXAMPLE 5.2.11
Let X1, . . ., Xn be a random sample from a population with gamma distribution and parameters a and
b. Find MLEs for the unknown parameters a and b.
Solution
The pdf for the gamma distribution is given by
f x
ð Þ ¼
xa1ex=b
G a
ð Þba , x > 0, a > 0, b > 0
0,
otherwise:
8
<
:
The likelihood function is given by
L ¼ L a, b
ð
Þ ¼
1
G a
ð Þba
ð
Þn
Y
n
i¼1
xa1
i
ePn
i¼1xi=b:
Taking the logarithms gives
ln L ¼ n lnG a
ð Þna lnb + a1
ð
Þ
X
n
i¼1
lnxi 
X
n
i¼1
x
b:
Now taking the partial derivatives with respect to a and b and setting both equal to zero, we
have
@
@a ln L ¼ nG0 a
ð Þ
G a
ð Þ n lnb +
X
n
i¼1
lnxi ¼ 0
@
@b ln L ¼ na
b +
X
n
i¼1
xi
b2 ¼ 0:
Continued
233
5.2 The Methods of Finding Point Estimators

Solving the second one to get b in terms of a, we have
b ¼ x
a:
Substituting this b in the first equation, we have to solve
nG0 a
ð Þ
G a
ð Þ n ln x
a +
X
n
i¼1
lnxi ¼ 0
for a>0. There is no closed-form solution for a and b. In this case, one can use numerical methods
such as the Newton-Raphson method to solve for a, and then use this value to find b.
There are many references available on the Web. (such as http://www.mn.uio.no/
math/tjenester/kunnskap/kompendier/num_opti_likelihoods.pdf)
explaining
the
Newton-Raphson method for the gamma distribution.
In only a few cases are we able to obtain a simple form for the maximum like-
lihood equation that can be solved by setting the first derivative to zero. Often we
cannot write an equation that can be differentiated to find the MLE parameter esti-
mates. This is especially true in the situation where the model is complex and
involves many parameters. Evaluating the likelihood exhaustively for all values
of the parameters becomes almost impossible, even with modern computers. This
is why so-called optimization algorithms have become indispensable to statisticians.
The purpose of an optimization algorithm is to find as fast as possible the set of
parameter values that make the observed data most likely. There are many such algo-
rithms available. We describe the Newton-Raphson method in Project 5F, and
another powerful algorithm, known as the EM algorithm, is given in Section 13.4.
We have been introduced to several classical discrete and continuous pdf, such as
the Binomial, Poison, Gaussian (normal), Gamma, exponential pdf, among others.
Note that when we use one of these pdf to study a given set of data we refer to it
as parametric analysis, because each of the classical pdf’s contains at least one
parameter that plays a major role in the shape of the probability distribution that char-
acterizes the behavior of the phenomenon of interest.
Some Additional Probability Distributions
Now, we will introduce some additional probability distributions that play a major
role in analyzing data, or information, in health science, environmental science, engi-
neering, business and economics, among many other important areas in our society.
We shall study the three parameter gamma pdf, and the Weibull pdf. The Ray-
leigh pdf, and the power exponential pdf are other examples, that will be given in
Chapter 7. Each of these pdf’s will be applied to real data: cancer data, hurricane data,
global warming data, and environmental (rainfall) data in Chapter 7.
In Example 5.2.11, we have studied the two-parameter gamma probability distri-
bution (pdf), here we shall introduce the three-parameter version that is useful when
we analyze data that exhibits positive skewness. The three-parameter gamma
pdf is given by
f x
ð Þ ¼
1
baG a
ð Þ
xg
ð
Þa1exp xg
ð
Þ
b
,
where x>g, b>0 and G(a) ¼
Ð
0
1xa1exdx.
234
CHAPTER 5 Statistical Estimation

The corresponding cumulative distribution function (cdf) is given by
F x
ð Þ ¼ P X  x
ð
Þ
¼
ðx
g
1
baG a
ð Þ
yg
ð
Þa1exp yg
ð
Þ
b
dy
¼ Gxg
b a
ð Þ:
1
G a
ð Þ:
The expected value is given by
E X
ð Þ ¼
ð1
0
xf x
ð Þdx ¼ g + ab:
Note when the location parameter g¼0 we obtain the two-parameter gamma (pdf).
EXAMPLE 5.2.12
Given a random sample X1,. . .,Xn from a three-parameter gamma distribution, obtain the MLEs of
the parameters.
Solution
The likelihood function is given by
L a, b, g
ð
Þ ¼
Y
n
i¼1
f x
ð Þ
¼
1
baG a
ð Þ

nX
n
i¼1
xi g
ð
Þ
a1Y
n
i¼1
exp
xi g
b


,
and the log-likelihood function ‘(a,b,g) of L(a,b,g) is given by
‘ a, b, g
ð
Þ ¼ na lnbn lnG a
ð Þ + a1
ð
Þ
X
n
i¼1
ln xi g
ð
Þ
X
n
i¼1
xi g
b
:
a1
ð
Þ
X
n
i¼1
ln x1 g
ð
Þ
X
n
i¼1
x1 g
b
:
The MLE can be obtained by setting @‘
@a ¼ 0, @‘
@b ¼ 0 and @‘
@g ¼ 0.
That is,
@‘
@b ¼ na
b +
Xn
i¼1 xi g
ð
Þ
b2
¼ 0
which results in the MLE of b to be
^b ¼
Xn
i¼1 xi  ^g
ð
Þ
n^a
,
(5.1)
@‘
@a ¼ n lnbnG0 a
ð Þ
G a
ð Þ +
X
n
i¼1
ln xi g
ð
Þ ¼ 0:
Substituting ^b in the above expression we have
ln ^aG0 ^a
ð Þ
G ^a
ð Þ ¼ ln 1
n
X
n
i¼1
x1  ^g
ð
Þ
"
#
1
n
X
n
i¼1
ln xi  ^g
ð
Þ,
(5.2)
Continued
235
5.2 The Methods of Finding Point Estimators

where G0 a
ð Þ
G a
ð Þ is called the digamma function which is defined as the logarithmic derivative of the
gamma function. Now,
@‘
@a ¼  a1
ð
Þ
X
n
i¼1
1
xi g
ð
Þ +
X
n
i¼1
1
b ¼ 0
which reduces to
X
n
i¼1
1
xi  ^g ¼
n
^b ^a1
ð
Þ
:
(5.3)
Thus, we can proceed to numerically solve Equations (5.1), (5.2), and (5.3) to obtain (numer-
ically) an approximate MLE ^a, ^b,and ^g so that we can apply the subject pdf to real data.
We can also use the cumulative probability distribution of the three-parameter
gamma pdf to obtain the quantile, xp, for which F(xp)¼1p , that is,
F xp


¼ Gxp g a
ð Þ
b
 1
G a
ð Þ
¼ 1p:
Substituting the MLE for a,b,and g, that is, ^a, ^b,and ^g and proceed to obtain approx-
imate estimates of xp.
The Weibull probability distribution is very important in characterizing the
behavior of health, engineering and environmental data, among others. The Weibull
pdf is given by
f x
ð Þ ¼ a
b
xg
b

a1
exp  xg
b

a


,
where x>0, the shape parameter a, is greater than zero, the scale parameter b is
b>0 and the location parameter g is x>g. The cumulative probability distribution
of Weibull pdf is given by,
F x
ð Þ ¼ P X  x
ð
Þ ¼
ðx
g
a
b
tg
b

a1
exp  tg
b

a


dt
¼ 1exp  xg
b

a


:
When g¼0, the subject pdf is reduced to a two-parameter Weibull and it is com-
monly used because of the difficulty in estimating the three-parameters.
236
CHAPTER 5 Statistical Estimation

EXAMPLE 5.2.13
For random sample X1,. . .,Xn drawn from the three-parameter Weibull pdf, obtain MLEs for the
parameters.
Solution
The likelihood function, L(a,b,g) is given by
L a, b, g
ð
Þ ¼ anbna
Y
n
i¼1
xi g
ð
Þ
(
)a1
exp
baX
n
i¼1
xi g
ð
Þa
(
)
and the log-likelihood function ‘(a,b,g) of L(a,b,g) is given by
‘ a, b, g
ð
Þ ¼ n lnana lnb + a1
ð
Þ
X
n
i¼1
ln xi g
ð
ÞbaX
n
i¼1
xi g
ð
Þa:
Setting @‘
@a ¼ 0, @‘
@b ¼ 0 and @‘
@g ¼ 0andtakingthepartialderivativesandsubstitutinga ¼ ^a,
b ¼ ^b,
andg ¼ ^g and simplifying the resulting expression we have
^a +
X
n
i¼1
ln xi  ^g
ð
Þ ¼
n
Xn
i¼1 xi  ^g
ð
Þ^a ln xi  ^g
ð
Þ
Xn
i¼1 xi  ^g
ð
Þ^a
,
n^a
Xn
i¼1 xi  ^g
ð
Þ^a1
Xn
i¼1 xi  ^g
ð
Þ^a
¼ ^a1
ð
Þ
X
n
i¼1
1
xi  ^g
and
^b ¼
1
n
X
n
i¼1
xi  ^g
ð
Þ^a
(
)1
^a
:
The above equation cannot be analytically solved without further restrictions, so that we cannot
obtain exact values for ^a,
^b, and ^g, however, there are software packages that we can use to
obtain approximate estimates of the subject parameters.
One of the solutions for Example 5.2.13 is given in http://math.ut.ee/acta/12/
Bartkute-Sakalauskas.pdf. Thus, we can see from the previous examples is that even
though MLEs are elegent estimators, sometimes, it is not easy or possible to obtain
explicit forms. For these estimates to perform parametric analysis on a given set of
data that represents a real world phenomenon of interest, we will need numerical
approximations.
We can use the cumulative probability distribution function F(x), to the quantile
xp for which F(xp)¼1p, which reduces to
xp ¼ g + b  lnp
ð
Þ
1
a:
237
5.2 The Methods of Finding Point Estimators

Thus using the MLE of the parameters, we have
^xp ¼ ^g + ^b  lnp
ð
Þ
1
a:
The following graphs illustrate how Weibull pdf varies with the shape parameter a
(Figure 5.2a) and with scale parameter b (Figure 5.2b).
The exponential power or error probability distribution is usually applicable in
characterizing continuous data that is very nonsymmetric with respect to its mean. It
has been shown to be useful in analyzing environmental, engineering and health data,
among others. It is characterized by three parameters that offer the flexibility of
addressing different skewness behaviors. Let X be a continuous random variable that
Weibull distribution PDF (scale=1)
4
3
2
1
0
0
0.5
1
1.5
Random variable
Probability
2
2.5
3
Shape=0.75
Shape=1
Shape=5
Shape=10
(a)
Weibull distribution PDF (shape=3)
2.5
2
1
1.5
0.5
(b)
0
0
1
Random variable
Probability
2
3
Scale=0.5
Scale=1
Scale=2
FIGURE 5.2
(a) Weibull distribution with different shape parameters (scale¼1). (b) Weibull distribution
with different scale parameters (shape¼3).
238
CHAPTER 5 Statistical Estimation

characterizes the behavior of a certain problem of interest, the power exponential or
error pdf is given by
f x
ð Þ ¼ l e1elxk


elxkxk1, x > 0, l > 0, k > 0
where l and k are location and shape parameters, respectively.
The cumulative probability distribution function of the random variable X that
follows the exponential power pdf is given by
F x
ð Þ ¼ 1e1elxk
, x > 0, l > 0, k > 0:
The population mean and variance of X are mathematically intractable. Obtaining
MLE analytically is difficult.
The Rayleigh distribution characterizes the behavior of continuous random var-
iable that represents many real world problems. This pdf arise when a two-
dimensional vector, for example, wind velocity data as measured by an anemometer
and wind range that consists of speed value and direction, each of their components is
normally distributed, are not correlated and have equal variance. Let X be a contin-
uous random variable that assume such data, the Rayleigh pdf of the random variable
X is given by
f x; s
ð
Þ ¼ x
s2 e
x2
2s2


, x > 0
where the scale parameter s>0. The pdf of various values of parameters is given in
Figure 5.3.
The cdf is given by
P X  x
ð
Þ ¼ 1
s2
ðx
0
t
s2 e
t2
2s2


dt ¼ 1e
x2
2s2 , x > 0,s > 0:
1.2
s=0.5
s=1
s=2
s=3
s=4
1
0.8
0.6
0.4
0.2
0
0
2
4
6
8
10
FIGURE 5.3
Rayleigh pdg for various values of s.
239
5.2 The Methods of Finding Point Estimators

The expected value and the variance are given by
E X
ð Þ ¼ s
ﬃﬃﬃp
2
r
¼ 1:25s
and
Var X
ð Þ ¼ 4p
2
s2 ¼ 0:429s2:
For a random sample X1,. . .,Xn from Rayleigh pdf, we can verify that the MLE of s is
given by
^s ¼
1
2n
Xn
i¼1X2
i


:
Sometimes, it may be necessary to estimate a function of a parameter. The following
invariance property of MLEs is very useful in those cases.
Theorem 5.2.1 Let h(y) be a one-to-one function of y. If ^y ¼
^y1, ..., ^yl


is the
MLE of y¼(y1, ..., yl), then the MLE of a function h(y)¼(h1(y), ..., hk(y)) of these
parameters is h ^y
 
¼ h1 ^y
 
, ...,hk ^y
 


for 1kl.
As a consequence of the invariance property, in Example 5.2.10, we can obtain
the estimator of the true standard deviation as ^s ¼
ﬃﬃﬃﬃﬃ
^s2
p
¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1=n
ð
ÞSn
i¼1 XiX

2
q
.
It is also known that, under very general conditions on the joint distribution of the
sample and for a large sample size n, the MLE ^y is approximately the minimum var-
iance unbiased estimator (MVUE) (this concept is introduced in the next section) of y.
EXERCISES 5.2
5.2.1. Let X1, . . ., Xn be a random sample of size n from the geometric distribution
for which p is the probability of success.
(a) Use the method of moments to find a point estimator for p.
(b) Use the following data (simulated from geometric distribution) to find
the moment estimator for p:
2
5
7
43
18
19 16
11
22
4
34
19
21
23
6
21
7
12
How will you use this information? [The pdf of a geometric
distribution is f(x)¼p(1p)x1, for x¼1,2,. . .. Also m¼1/p.]
5.2.2. Let X1, . . ., Xn be a random sample of size n from the exponential distribution
whose pdf (by taking y¼1/b in Definition 2.3.7) is
f x, y
ð
Þ ¼
yeyx, x  0
0,
x < 0:

(a) Use the method of moments to find a point estimator for y.
240
CHAPTER 5 Statistical Estimation

(b) The following data represent the time intervals between the emissions of
beta particles.
0:9
0:1
0:1
0:8
0:9
0:1
0:1
0:7
1:0
0:2
0:1
0:1
0:1
2:3
0:8
0:3
0:2
0:1
1:0
0:9
0:1
0:5
0:4
0:6
0:2
0:4
0:2
0:1
0:8
0:2
0:5
3:0
1:0
0:5
0:2
2:0
1:7
0:1
0:3
0:1
0:4
0:5
0:8
0:1
0:1
1:7
0:1
0:2
0:3
0:1
Assuming the data follow an exponential distribution, obtain a
moment estimate for the parameter y. Interpret.
5.2.3. Let X1, . . ., Xn be a random sample from a uniform distribution on the interval
(y1, y+1).
(a) Find a moment estimator for y.
(b) Use the following data to obtain a moment estimate for y:
11:72
12:81
12:09
13:47
12:37
5.2.4. The probability density of a one-parameter Weibull distribution is given by
f x
ð Þ ¼
2axeax2,
x > 0
0,
otherwise:

(a) Using a random sample of size n, obtain a moment estimator for a.
(b) Assuming that the following data are from a one-parameter Weibull
population,
1:87
1:60
2:36
1:12
0:15
1:83
0:64
1:53
0:73
2:26
obtain a moment estimate of a.
5.2.5. Let X1, . . ., Xn be a random sample from the truncated exponential
distribution with pdf
f x
ð Þ ¼
e xy
ð
Þ,
x  y
0,
otherwise:

Find the method of moments estimate of y.
5.2.6. Let X1, . . ., Xn be a random sample from a distribution with pdf
f x, a
ð
Þ ¼ 1 + ax
2
,
1  x  1 and 1  a  1:
Find the moment estimators for a.
5.2.7. Let X1, . . ., Xn be a random sample from a population with pdf
f x
ð Þ ¼
2a2
x3 ,
x  a
0,
otherwise:
8
<
:
241
5.2 The Methods of Finding Point Estimators

Find a method of moments estimator for a.
5.2.8. Let X1, . . ., Xn be a random sample from a negative binomial distribution
with pmf
p x, r, p
ð
Þ ¼
x + r 1
r 1


px 1p
ð
Þx,
0  p  1, x ¼ 0,1,2,:...
Find method of moments estimators for r and p. [Here E[X]¼r(1p)/p and
E[X2]¼r(1p)(rrp+1)/p2.]
5.2.9. Let X1, . . ., Xn be a random sample from a distribution with pdf
f x
ð Þ ¼
y + 1
ð
Þxy, 0  x  1, y > 1
0,
otherwise:
(
Use the method of moments to obtain an estimator of y.
5.2.10. Let X1, . . ., Xn be a random sample from a distribution with pdf
f x
ð Þ ¼
2b2x
b2
, 0 < x < b
0,
otherwise:
8
<
:
Use the method of moments to obtain an estimator of b.
5.2.11. Let X1, . . ., Xn be a random sample with common mean m and variance s2.
Obtain a method of moments estimator for s.
5.2.12. Let X1, . . ., Xn be a random sample from the beta distribution with
parameters a and b. Find the method of moments estimator for a and b.
5.2.13. Let X1, X2, . . ., Xn be a random sample from a distribution with unknown
mean m and variance s2. Show that the method of moments estimators for m
and s2 are, respectively, the sample mean X and
S02 ¼ 1=n
ð
ÞPn
i¼1 X X

2:Note that S02¼[(n1)/n]S2 where S2 is the
sample variance.
5.2.14. Let X1, . . ., Xn be a random sample recorded as heads or tails resulting from
tossing a coin n times with unknown probability p of heads. Find the MLE ^p
of p. Also using the invariance property, obtain an MLE for q¼1p. How
would you use the results you have obtained?
5.2.15. Suppose X1, . . ., Xn are a random sample from an exponential distribution
with parameter y. Find the MLE of ^y. Also using the invariance property,
obtain an MLE for the variance.
5.2.16. Let X be a random variable representing the time between successive
arrivals at a checkout counter in a supermarket. The values of X in minutes
(rounded to the nearest minute) are
1
2
3
7
11
4
13
12
7
3
2
11
7
2
Assume that the pdf of X is f(x)¼(1 /y)e(x/y). Use these data to find MLE ^y.
How can you use this estimate you have just derived? Also find the method
of moment estimate.
242
CHAPTER 5 Statistical Estimation

5.2.17. Let X1, . . ., Xn be a random sample from the truncated exponential
distribution with pdf
f x
ð Þ ¼
e xy
ð
Þ,
x  y
0,
otherwise:
(
Show that the MLE of y is min(Xi;).
5.2.18. The pdf of a random variable X is given by
f x
ð Þ ¼
2x
a2 ex2=a2, x > 0
0,
otherwise:
8
<
:
Using a random sample of size n, obtain MLE ^a for a.
5.2.19. The pdf of a random variable X is given by
P X ¼ n
ð
Þ ¼ 1
n!exp anea
ð
Þ, n ¼ 0,1,2,:...
Using a random sample of size n, obtain MLE ^a for a.
5.2.20. Let X1, . . ., Xn be a random sample from a two-parameter Weibull
distribution with pdf
f x
ð Þ ¼
a
ba xa1e x=b
ð
Þa, x  0
0,
otherwise:
8
<
:
Find the MLEs of a and b.
5.2.21. Let X1, . . ., Xn be a random sample from a Rayleigh distribution with pdf
f x
ð Þ ¼
x
aex2=2a, x > 0
0,
otherwise:
8
<
:
Find the MLEs of a.
5.2.22. Let X1, . . ., Xn be a random sample from a two-parameter exponential
population with density
f x, y, u
ð
Þ ¼ 1
ye xu
ð
Þ
y , for x  u, y > 0:
Find MLEs for y and u when both are unknown.
5.2.23. Let X1, . . ., Xn be a random sample from the shifted exponential distribution
with
f x
ð Þ ¼
lel xy
ð
Þ, x  y
0,
otherwise:
(
Obtain the MLEs of y and l.
5.2.24. Let X1, . . ., Xn be a random sample on [0, 1] with pdf
f x
ð Þ ¼ G 2y
ð
Þ
G y
ð Þ2 x 1x
ð
Þ
½
y1, y > 0:
What equation does the maximum likelihood estimate of y satisfy?
243
5.2 The Methods of Finding Point Estimators

5.2.25. Let X1, . . ., Xn be a random sample with pdf
f x
ð Þ ¼
a + 1
ð
Þxa, 0  x  1
0,
otherwise:
(
Find the MLE of a.
5.2.26. Let X1, . . ., Xn be a random sample from a uniform distribution with pdf
f x
ð Þ ¼
1
3y + 2,
0  x  3y + 2
0,
otherwise:
8
<
:
Obtain the MLE of y.
5.2.27. Let X1, . . ., Xn be a random sample from a Cauchy distribution with pdf
f x
ð Þ ¼
1
p 1 + xb
ð
Þ2
h
i, 1 < x < 1:
Find the MLE for b.
5.2.28. The following data represent the amount of leakage of a fluorescent dye
from the bloodstream into the eye in patients with abnormal retinas:
1:6
1:4
1:2
2:2
1:8
1:7
1:8
6:3
2:4
2:3
18:9
22:8
Assuming that these data come from a normal distribution, find the maxi-
mum likelihood estimate of (m, s).
5.2.29. Let X1, . . ., Xn be a random sample from a population with gamma
distribution and parameters a and b. Show that the MLE of m¼ab is the
sample mean ^m ¼ X.
5.2.30. The lifetimes X of a certain brand of component used in a machine can be
modeled as a random variable with pdf f(x)¼(1/y) e(x/y). The reliability
R(x) of the component is defined as R(x)¼1F(x). Suppose X1, X2, . . ., Xn
are the lifetimes of n components randomly selected and tested. Find the
MLE of R(x).
5.2.31. Using the method explained in Project 4A, generate 20 observations of a
random variable having an exponential distribution with mean and standard
deviation both equal to 2. What is the maximum likelihood estimate of the
population mean? How much is the observed error?
5.2.32. Let X1, . . ., Xn be a random sample from a Pareto distribution (named after
the economist Vilfredo Pareto) with shape parameter a. The density
function is given by
f x
ð Þ ¼
a
xa + 1 ,
x  1
0,
otherwise:
8
<
:
244
CHAPTER 5 Statistical Estimation

(The Pareto distribution is a skewed, heavy-tailed distribution.
Sometimes it is used to model the distribution of incomes.) Show that the
MLE of a is
^a ¼
n
Xn
i¼1 ln Xi
ð
Þ
:
5.2.33. Let X1, . . ., Xn be a random sample from N (y, y),0<y<1. Find the
maximum likelihood estimate of y.
5.3 SOME DESIRABLE PROPERTIES OF POINT ESTIMATORS
Two different methods of finding estimators for population parameters have
been introduced in the preceding section. We have seen that it is possible to have
several estimators for the same parameter. For a practitioner of statistics, an im-
portant question is going to be which of many available sample statistics, such as
mean, median, smallest observation, or largest observation, should be chosen to rep-
resent all of the sample? Should we use the method of moments estimator, the MLE,
or an estimator obtained through some other method such as the least squares
(we will see this method in Chapter 8)? Now we introduce some common ways
to distinguish between them by looking at some desirable properties of these
estimators.
5.3.1 UNBIASED ESTIMATORS
It is desirable to have the property that the expected value of an estimator of a param-
eter is equal to the true value of the parameter. Such estimators are called unbiased
estimators.
Definition 5.3.1 A point estimator ^y is called an unbiased estimator of the
parameter y if E ^y
 
¼ y for all possible values of y. Otherwise ^y is said to be biased.
Furthermore, the bias of ^y is given by
B ¼ E ^y
 
y:
Note that the bias is nothing but the expected value of the (random) error, E ^yy


.
Thus, the estimator is unbiased if the bias is 0 for all values of y. The bias occurs
when a sample does not accurately represent the population from which the sample
is taken. It is important to observe that in order to check whether ^y is unbiased, it is
not necessary to know the value of the true parameter. Instead, one can use the sam-
pling distribution of ^y. We demonstrate the basic procedure through the following
example.
245
5.3 Some Desirable Properties of Point Estimators

EXAMPLE 5.3.1
Let X1,. . .,Xn be a random sample from a Bernoulli population with parameter p. Show that the
method of moments estimator is also an unbiased estimator.
Solution
We can verify that the moment estimator of p is
^p ¼
Xn
i¼1Xi
n
¼ Y
n:
Because for binomial random variables, E(Y)¼np, it follows that
E ^p
ð Þ ¼ E Y
n
 
¼ 1
nE Y
ð Þ ¼ 1
nnp ¼ p:
Hence, ^p ¼ Y=n is an unbiased estimator for p.
In fact, we have the following result, which states that the sample mean is always an
unbiased estimator of the population mean.
Theorem 5.3.1 The mean of a random sample X is an unbiased estimator of the
population mean m.
Proof. Let X1, . . ., Xn be random variables with mean m. Then, the sample mean is
X ¼ 1=n
ð
ÞSn
i¼1Xi.
EX ¼ 1
n
X
n
i¼1
EXi ¼ 1
nnm ¼ m:
Hence, X is an unbiased estimator of m.
n
How is this interpreted in practice? Suppose that a data set is collected with n
numerical observations x1, . . .,xn. The resulting sample mean may be either less than
or greater than the true population mean, m (remember, we do not know this value). If
the sampling experiment was repeated many times, then the average of the estimates
calculated over these repetitions of the sampling experiment will equal the true
population mean.
If we have to choose among several different estimators of a parameter y, it is
desirable to select one that is unbiased. The following result states that the sample
variance S2 ¼ 1=n1
ð
ÞSn
i¼1 Xi X

2 is an unbiased estimator of the population var-
iance s2. This is one of the reasons why in the definition of the sample variance,
instead of dividing by n, we divide by (n1).
Theorem 5.3.2 If S2 is the variance of a random sample from an infinite popu-
lation with finite variance s2, then S2 is an unbiased estimator for s2.
Proof. Let X1, . . ., Xn be random sample with variance s2<1. We have
E S2


¼
1
n1E
X
n
i¼1
Xi X

2 ¼
1
n1E
X
n
i¼1
Xi m
ð
Þ X m



2
"
#
¼
1
n1
X
n
i¼1
E Xi m
f
g2 nE X m

2
"
#
:
246
CHAPTER 5 Statistical Estimation

Because E{(Xim)2}¼s2 and E
X m

2
n
o
¼ s2=n, it follows that
E S2


¼
1
n1
X
n
i¼1
s2 ns2
n
"
#
¼ s2:
Hence, S2 is an unbiased estimator of s2.
n
It is important to observe the following:
1. S2 is not an unbiased estimator of the variance of a finite population.
2. Unbiasedness may not be retained under functional transformations, that is; if ^y is
an unbiased estimator of y, it does not follow that f
^y
 
is an unbiased estimator
of f(y).
3. MLEs or moment estimators are not, in general, unbiased.
4. In many cases it is possible to alter a biased estimator by multiplying by an
appropriate constant to obtain an unbiased estimator.
The following example will show that unbiased estimators need not be unique.
EXAMPLE 5.3.2
Let X1, . . ., Xn be a random sample from a population with finite mean m. Show that the sample mean
X and 1
3X + 2
3X1 are both unbiased estimators of m.
Solution
By Theorem 5.3.1, X is unbiased. Now
E 1
3X + 2
3X1


¼ 1
3m + 2
3m ¼ m:
Hence, 1
3X + 2
3X1 is also an unbiased estimator of m.
How many unbiased estimators can we find? In fact, the following example shows that
if we have two unbiased estimators, there are infinitely many unbiased estimators.
EXAMPLE 5.3.3
Let ^y1 and ^y2 be two unbiased estimators of y. Show that
^y3 ¼ a^y1 + 1a
ð
Þ^y2, 0  a  1
is an unbiased estimator of y. Note that ^y3 is a convex combination of ^y1 and ^y2. In addition, assume
that ^y1 and ^y2 are independent, Var ^y1


¼ s2
1 and Var ^y2


¼ s2
2. How should the constant a be cho-
sen in order to minimize the variance of ^y3?
Solution
We are given that E ^y1


¼ y and E ^y2


¼ y. Therefore,
E ^y3


¼ E a^y1 + 1a
ð
Þ^y2
h
i
¼ aE^y1 + 1a
ð
ÞE^y2
¼ ay + 1a
ð
Þy ¼ y:
Hence ^y3 unbiased. By independence,
Continued
247
5.3 Some Desirable Properties of Point Estimators

Var ^y3


¼ Var a^y1 + 1a
ð
Þ^y2
h
i
¼ a2Var ^y1


+ 1a
ð
Þ2Var ^y2


¼ a2s2
1 + 1a
ð
Þ2s2
2:
To find the minimum,
d
daVar ^y3


¼ 2as2
1 2 1a
ð
Þs2
2 ¼ 0,
gives us
a ¼
s2
2
s2
1 + s2
2
:
Because d2
da2 V ^y3


¼ 2s2
1 + 2s2
2 > 0,V ^y3


has a minimum at this value of 0a0. Thus, if s1
2¼s2
2,
then a¼1/2.
EXAMPLE 5.3.4
Let X1, . . ., Xn be a random sample from a population with pdf
f x
ð Þ ¼
1
bex=b, x > 0
0, otherwise:
(
Show that the method of moments estimator for the population parameter b is unbiased.
Solution
From Section 5.2, we have seen that the method of moments estimator for b is the sample mean X,
and the population mean is b. Because E X
 
¼ m ¼ b, the method of moments estimator for the pop-
ulation parameter b is unbiased.
As we have seen, there can be many unbiased estimators of a parameter y. Which one
of these estimators can we choose? If we have to choose an unbiased estimator, it will
be desirable to choose the one with the least variance. If an estimator is biased, then
we should prefer the one with low bias as well as low variance. Generally, it is better
to have an estimator that has low bias as well as low variance. This leads us to the
following definition.
Definition 5.3.2 The mean square error of the estimator ^y, denoted by MSE ^y
 
,
is defined as
MSE ^y
 
¼ E ^yy

2
:
Through the following calculations, we will now show that the MSE is a measure that
combines both bias and variance.
MSE ^y
 
¼ E ^yy

2
¼ E
^yE ^y
 


+ E ^y
 
y


h
i2
¼ E
^yE ^y
 

2
+ E ^y
 
y

2
+ 2 ^yE ^y
 


E ^y
 
y




¼ E ^yE ^y
 

2
+ E E ^y
 
y

2
+ 2E ^yE ^y
 


E ^y
 
y


¼ Var ^y
 
+ E ^y
 
y
h
i2
:
248
CHAPTER 5 Statistical Estimation

Letting B ¼ E ^y
 
y, we get
MSE ^y
 
¼ Var ^y
 
+ B2:
B is called the bias of the estimator. Also, E ^yE ^y
 


E ^y
 
y


¼ 0:
Because
the
bias
is
zero
for
unbiased
estimators,
it
is
clear
that
MSE ^y
 
¼ Var ^y
 
. Mean square error measures, on average, how close an estima-
tor comes to the true value of the parameter. Hence, this could be used as a criterion
for determining when one estimator is “better” than another. However, in general, it
is difficult to find ^y to minimize MSE ^y
 
: For this reason, most of the time, we look
only at unbiased estimators in order to minimize Var ^y
 
: This leads to the following
definition.
Definition 5.3.3 The unbiased estimator ^y that minimizes the mean square error
is called the MVUE of y.
EXAMPLE 5.3.5
Let X1, X2, X3 be a sample of size n¼3 from a distribution with unknown mean m, 1<m<1,
where
the
variance
s2
is
a
known
positive
number.
Show
that
both
^y1 ¼ X
and
^y2 ¼
2X1 + X2 + 5X3
ð
Þ=8
½
 are unbiased estimators for m. Compare the variances of ^y1 and ^y2.
Solution
We have
E ^y1


¼ E X
 
¼ 1
33m ¼ m,
and
E ^y2


¼ 1
8 2EX1 + EX2 + 5EX3
½

¼ 1
8 2m + m + 5m
½
 ¼ m:
Hence, both ^y1 and ^y2 are unbiased estimators.
However,
Var ^y1


¼ s2
3 ,
whereas
Var ^y2


¼ Var 2X1 + X2 + 5X3
8


¼ 4
64s2 + 1
64s2 + 25
64s2 ¼ 30
64s2:
Because Var ^y1


< Var ^y2


, we see that X is a better unbiased estimator in the sense that the
variance of X is smaller.
It is important to observe that the MLEs are not always unbiased, but it can be shown
that for such estimators the bias goes to zero as the sample size increases.
249
5.3 Some Desirable Properties of Point Estimators

5.3.2 SUFFICIENCY*
In the statistical inference problems on a parameter, one of the major questions is:
Can a specific statistic replace the entire data without losing pertinent information?
Suppose X1, . . ., Xn is a random sample from a probability distribution with unknown
parameter y. In general, statisticians look for ways of reducing a set of data so that
these data can be more easily understood without losing the meaning associated with
the entire collection of observations. Intuitively, a statistic U is a sufficient statistic
for a parameter y if U contains all the information available in the data about the
value of y. For example, the sample mean may contain all the relevant information
about the parameter m, and in that case U ¼ X is called a sufficient statistic for m. An
estimator that is a function of a sufficient statistic can be deemed to be a “good” esti-
mator, because it depends on fewer data values. When we have a sufficient statistic U
for y, we need to concentrate only on U because it exhausts all the information that
the sample has about y. That is, knowledge of the actual n observations does not con-
tribute anything more to the inference about y.
Definition 5.3.4 Let X1, . . ., Xn be a random sample from a probability distribu-
tion with unknown parameter y. Then, the statistic U¼g(X1, . . ., Xn) is said to be
sufficient for y if the conditional pdf or pf of X1, . . ., Xn given U¼u does not depend
on y for any value of u. An estimator of y that is a function of a sufficient statistic for y
is said to be a sufficient estimator of y.
EXAMPLE 5.3.6
Let X1, . . ., Xn be iid Bernoulli random variables with parameter y. Show that U¼Si¼1
n Xi is sufficient
for y.
Solution
The joint probability mass function of X1, . . ., Xn is
f X1, ..., Xn; y
ð
Þ ¼ y
Pn
i¼1Xi 1y
ð
ÞnPn
i¼1Xi, 0  y  1:
Because U¼Si¼1
n Xi we have
f X1, ..., Xn; y
ð
Þ ¼ yU 1y
ð
ÞnU, 0  U  n:
Also, because UB(n, y), we have
f u; y
ð
Þ ¼
n
u
 
yU 1y
ð
ÞnU:
Also,
f x1, ...,xnjU ¼ u
ð
Þ ¼ f x1, ..., xn, u
ð
Þ
f U u
ð Þ
¼
f x1, ..., xn
ð
Þ
f U u
ð Þ
, u ¼
X
xi
0,
otherwise:
8
<
:
Therefore,
f x1, ...,xnjU ¼ u
ð
Þ ¼
yu 1y
ð
Þnu
n
u
 
yu 1y
ð
Þnu
¼
1
n
u
 , ifu ¼
X
xi
0,
otherwise:
8
>
>
<
>
>
:
which is independent of y. Therefore U is sufficient for y.
250
CHAPTER 5 Statistical Estimation

EXAMPLE 5.3.7
Let X1, . . ., Xn be a random sample from U(0, y). That is,
f x
ð Þ ¼
1
y, if 0 < x < y
0, otherwise:
8
<
:
Show that U ¼ max
1inXi is sufficient for y.
Solution
The joint density or the likelihood function is given by
f x1, ..., xn; y
ð
Þ ¼
1
yn , if 0 < x1, ...,xn < y
0, otherwise:
8
<
:
The joint pdf f(x1, . . ., xn; y) can be equivalently written as
f x1, ..., xn; y
ð
Þ ¼
1
yn , if xmin > 0, xmax < y
0, otherwise:
8
<
:
Now, we can compute the pdf of U.
F u
ð Þ ¼ P U  u
ð
Þ ¼ P X1, ...,Xn  u
ð
Þ
¼
Y
n
i¼1
P Xi  u
ð
Þ, becauseof independence
¼
Y
n
i¼1
ðu
0
1
ydx
0
@
1
A ¼ un
yn , 0 < u < y:
The pdf of U may now be obtained as
f u
ð Þ ¼ d
duF u
ð Þ ¼ nun1
yn
, 0 < u < y
Moreover,
f x1, ...,xnju
ð
Þ ¼
f x1, ..., xn, u
ð
Þ
f U u
ð Þ
¼ f x1, ...xn
ð
Þ
f U u
ð Þ
, if u ¼ xmax and xmin > 0
0,
otherwise:
8
<
:
Using the expressions for f(x1, . . ., xn) and fU(u) we obtain
f x1, ...,xnju ¼ u
ð
Þ ¼
1=yn
nun1=yn ¼
1
nun1 , if u ¼ xmaxandxmin > 0
0,
otherwise:
8
<
:
f(X1, . . ., XnjU) is a function of u and xmin which is independent of y. Hence, U ¼ max
1inXi is sufficient
for y.
The outcome X1, . . ., Xn is always sufficient, but we will exclude this trivial statistic
from consideration. In the previous two examples, we were given a statistic and
asked to check whether it was sufficient. It can often be tedious to check whether
a statistic is sufficient for a given parameter based directly on the foregoing defini-
tion. If the form of the statistic is not given, how do we guess what is the sufficient
statistic? Now think of working out the conditional probability by hand for each of
our guesses! In general, this will be a tedious way to go about finding sufficient sta-
tistics. Fortunately, the Neyman-Fisher factorization theorem makes it easier to spot
251
5.3 Some Desirable Properties of Point Estimators

a sufficient statistic. The following result will give us a convenient way of verifying
sufficiency of a statistic through the likelihood function.
NEYMAN-FISHER FACTORIZATION CRITERIA
Theorem 5.3.3 Let U be a statistic based on the random sample X1, . . ., Xn. Then, U is a
sufficient statistic for y if and only if the joint pdf (or pf ) f(x1, . . ., xn; y) (which depends on the
parameter y) can be factored into two nonnegative functions.
f x1, ..., xn; y
ð
Þ ¼ g u, y
ð
Þh x1, ..., xn
ð
Þ,
forall x1, ...,xn,
where g (u, y) is a function only of u and y and h (x1, . . ., xn) is a function of only x1, . . ., xn and
not of y.
Proof. (Discrete case.) We will only give the proof in the discrete case, even though the result is
also true for the continuous case. First suppose that U (X1, . . ., Xn) is sufficient for y. Then, X1¼x1,
X2¼x2, . . ., Xn¼xn if and only if X1¼x1, X2¼x2, . . ., Xn¼xn and U (X1, . . ., Xn)¼U (x1, . . ., xn)¼u
(say). Therefore,
f x1, ..., xn; y
ð
Þ ¼ Py X1 ¼ x1,X2 ¼ x2, ...,Xn ¼ xn andU ¼ u
ð
Þ
¼ Py X1 ¼ x1,X2 ¼ x2, ...,Xn ¼ xnjU ¼ u
ð
ÞPy U ¼ u
ð
Þ:
n
Because U is assumed to be sufficient for y, the conditional probability
Py(X1¼x1,. . .,Xn¼xnjU¼u) does not depend on y. Let us denote this conditional
probability by h(x1, . . ., xn). Clearly, Py(U¼u) is a function of u and y. Let us denote
this by g(u, y).
It now follows from the equation above that
f x1, ..., xn; y
ð
Þ ¼ g u, y
ð
Þh x1, ..., xn
ð
Þ
as was to be shown.
To prove the converse, assume that
f x1, ..., xn; y
ð
Þ ¼ g u, y
ð
Þh x1, ..., xn
ð
Þ:
Define the set Au by
Au ¼
x1, ..., xn
ð
Þ : U x1, ..., xn
ð
Þ ¼ u
f
g:
That is, Au is the set of all (x1, . . ., xn) such that U maps it into u. We note that Au does
not depend on y. Now
Py X1 ¼ x1,X2 ¼ x2, ...,Xn ¼ xnjU ¼ u
ð
Þ
¼ Py X1 ¼ x1,X2 ¼ x2, ...Xn ¼ xn and U ¼ u
ð
Þ
Py U ¼ u
ð
Þ
¼
Py X1 ¼ x1,X2 ¼ x2, ...,Xn ¼ xn and U ¼ u
ð
Þ
Py U ¼ u
ð
Þ
,
if x1, ..., xn
ð
Þ 2 Au
0,
if x1, ..., xn
ð
Þ62 Au:
8
<
:
If (x1, . . ., xn)62Au, then, clearly,
f x1, ..., xn; y
ð
Þ ¼ Py X1 ¼ x1,X2 ¼ x2, ...,Xn ¼ xnjU ¼ u
ð
Þ
which is independent of y.
252
CHAPTER 5 Statistical Estimation

If (x1, . . ., xn) 62 Au, then, using the factorization criterion, we obtain
Py X1 ¼ x1,X2 ¼ x2, ...,Xn ¼ xnjU ¼ u
ð
Þ
¼ Py X1 ¼ x1,X2 ¼ x2, ...Xn ¼ xn
ð
Þ
Py U ¼ u
ð
Þ
¼ f x1, ..., xn; y
ð
Þ
Py U ¼ u
ð
Þ
¼
g u, y
ð
Þh x1, ..., xn
ð
Þ
X
x1, ..., xn
ð
Þ2 Au
g u, y
ð
Þh x1, ..., xn
ð
Þ
¼
g u, y
ð
Þh x1, ..., xn
ð
Þ
g u, y
ð
Þ
X
x1, ..., xn
ð
Þ2 Au
h x1, ..., xn
ð
Þ
¼
h x1, ..., xn
ð
Þ
X
x1, ..., xn
ð
Þ2 Au
h x1, ..., xn
ð
Þ
:
Therefore, the conditional distribution of X1, . . ., Xn given U does not depend on y,
proving that U is sufficient.
One can use the following procedure to verify that a given statistic is sufficient.
This procedure is based on factorization criteria rather than using the definition of
sufficiency directly.
PROCEDURE TO VERIFY SUFFICIENCY
1. Obtain the joint pdf or pf fy(x1, . . ., xn).
2. If necessary, rewrite the joint pdf or pf in terms of the given statistic and parameter so that one
can use the factorization theorem.
3. Define the functions g and h, in such a way that g is a function of the statistic and parameter only
and h is a function of the observations only.
4. If step 3 is possible, then the statistic is sufficient. Otherwise, it is not sufficient.
In general, it is not easy to use the factorization criterion to show that a statistic U
is not sufficient. We now give some examples using the factorization theorem.
EXAMPLE 5.3.8
Let X1, . . ., Xn denote a random sample from a geometric population with parameter p. Show that X is
sufficient for p.
Solution
For the geometric distribution, the pf is given by
f x, p
ð
Þ ¼
p 1p
ð
Þx1,x  1
0,
otherwise:

Hence, the joint pf is
f x1, ..., xn; p
ð
Þ ¼ pn 1p
ð
Þn +Pn
i¼1xi
¼
pn 1p
ð
Þnxn, if x1, ...,xn  1
0,
otherwise:

Take,
g x, p
ð
Þ ¼ pn 1p
ð
Þnxn and h x1, ..., xn
ð
Þ ¼
1, if xi  1
0, otherwise:

Thus, X is sufficient for p.
253
5.3 Some Desirable Properties of Point Estimators

EXAMPLE 5.3.9
Let X1, . . ., Xn denote a random sample from a U (0, y) with pdf
f y x
ð Þ ¼
1
y, 0 < x < y, y > 0
0,
otherwise
(
:
Show that X n
ð Þ ¼ max
1inXi is sufficient for y, using the factorization theorem.
Solution
The likelihood function of the sample is
f y x1, ..., xn
ð
Þ ¼
1
yn , if 0 < x1, ...,xn < y
0,
otherwise
(
:
We can now write fy (x1, . . .., xn) as
f y x1, ..., xn
ð
Þ ¼ h x1, ..., xn
ð
Þg y, x n
ð Þ


, forallx1, ...,xn
where
h x1, ..., xn
ð
Þ ¼
1, if x1, ...,xn > 0
0, otherwise

and
g y; x n
ð Þ


¼
1
yn , if 0 < x n
ð Þ < y
0, otherwise
(
:
From the factorization theorem, we now conclude that X(n) is sufficient for y. In the next def-
inition, we introduce the concept of joint sufficiency.
Definition 5.3.5 Two statistics U1 and U2 are said to be jointly sufficient for the
parameters y1 and y2 if the conditional distribution of X1, . . ., Xn given U1 and U2
does not depend on y1 or y2. In general, the statistic U¼(U1, . . ., Un) is jointly
sufficient for y¼(y1, . . ., yn) if the conditional distribution of X1, . . ., Xn given U
is free of u.
Now we state the factorization criteria for joint sufficiency analogous to the sin-
gle population parameter case.
THE FACTORIZATION CRITERIA FOR JOINT SUFFICIENCY
Theorem 5.4.4 The two statistics U1 and U2 are jointly sufficient for y1 and y2 if and only if
the likelihood function can be factored into two nonnegative functions,
f x1, ..., xn; y1, y2
ð
Þ ¼ g u1, u2; y1, y2
ð
Þh x1, ..., xn
ð
Þ
where g (u1, u2; y1, y2) is only a function of u1, u2; y1 and y2, and h(x1, . . ., xn) is free of y1 or y2.
254
CHAPTER 5 Statistical Estimation

EXAMPLE 5.3.10
Let X1, . . ., Xn be a random sample from N(m, s2).
(a) If m is unknown and s2¼s0
2 is known, show that X is a sufficient statistic for m.
(b) If m¼m0 is known and s2 is unknown, show that Si¼1
n (Xim0)2 is sufficient for s2.
(c) If m and s2 are both unknown, show that Si¼1
n Xi and Si¼1
n Xi
2 are jointly sufficient for m and s2.
Solution
The likelihood function of the sample is
L ¼
1
2p
ð
Þn=2sn exp 
Xn
i¼1 Xi m
ð
Þ2
2s2
"
#
¼
1
2p
ð
Þn=2sn exp
1
2s2
X
n
i¼1
x2
i 2m
X
n
i¼1
xi + nm2
 
!
"
#
¼ 2p
ð
Þn=2snexp 
Xn
i¼1x2
i
2s2
 
!
exp 2mnx
2s2


exp nm2
2s2


:
(a) When s2¼s0
2 is known, use the factorization criteria, with
g x, m
ð
Þ ¼ exp 2nmxnm2
2s2
0


and
h x1, ..., xn
ð
Þ ¼ 2p
ð
Þn=2snexp

X
n
i¼1
x2
i
2s2
0
B
B
B
@
1
C
C
C
A:
Therefore, X is sufficient for m.
(b) When m¼m0 is known, let
g
X
n
i¼1
Xi m
ð
Þ2,s2
 
!
¼ snexp 
X
n
i¼1
xi m
ð
Þ2
2s2


and
h x1, ..., xn
ð
Þ ¼
1
2p
ð
Þn=2 :
Thus Si¼1
n (Xim)2 is sufficient for s2.
(c) When both m and s2 are unknown, use
g
X
n
i¼1
xi,
X
n
i¼1
x2
i ,m,s2
 
!
¼ snexp 
Xn
i¼1x2
i 2m
Xn
i¼1xi + nm2
2s2


and
h x1, ..., xn
ð
Þ ¼
1
2p
ð
Þn=2 :
Hence, Si¼1
n Xi and Si¼1
n Xi
2 are jointly sufficient for m and s2.
255
5.3 Some Desirable Properties of Point Estimators

EXAMPLE 5.3.11
Suppose that we have a random sample X1, . . ., Xn from a discrete distribution given by
f y x
ð Þ ¼ C y
ð Þ2x=y, x ¼ y,y + 1,y + 2, ...; y > 0
whereC(y)>0isanormalizingconstant.Usingthefactorizationtheorem,findasufficientstatisticfory.
Solution
The joint density function f(x1,. . .,xn; y) of the sample X1, . . ., Xn is
f x1, ..., xn; y
ð
Þ ¼
C y
ð Þ2Pn
i¼1 xi=y
ð
Þ, x1,x2, ...,xn areintegers  y
0,
otherwise
(
:
The function f(x1, . . ., xn; y) can be written as
f x1, ..., xn; y
ð
Þ ¼ h x1, ..., xn
ð
ÞC y
ð Þ2Pn
i¼1 xi=y
ð
Þg1 y, x 1
ð Þ


where x(1)¼ mini(x1, ..., xn), and
h x1, x2, ..., xn
ð
Þ ¼
1, if xj x 1
ð Þ  0isanintegerforj ¼ 1,2, ...,n
0,
otherwise

and
g1 y, x 1
ð Þ


¼
1, if x 1
ð Þ  y
0, otherwise

:
Thus,
f x1, ..., xn; y
ð
Þ ¼ h x1, ..., xn
ð
Þg y,
X
xi,x 1
ð Þ


where g(y,Sxi,x(1))¼C(y)2 g1(y,x(1)) . Using the factorization theorem, we conclude that (Sxi,x(1))
is jointly sufficient for y. This result shows that even for a single parameter, we may need more than
one statistic for sufficiency.
When using the factorization criterion, one has to be careful in cases where the range space
depends on the parameter.
Using the factorization criterion, we can prove the following result, which says that if
we have a unique MLE, then that estimator will be a function of the sufficient statistic.
Theorem 5.3.5 If U is a sufficient statistic for y, the MLE of y, if unique, is a
function of U.
Proof. Because U is sufficient, by Theorem 5.4.1, the joint pdf can be factored as
f x1, ..., xn; y
ð
Þ ¼ g u, y
ð
Þh x1, ..., xn
ð
Þ:
This depends on y only through the statistic U. To maximize L we need to maximize
g(U,y).
n
Many common distributions such as Poisson, normal, gamma, and Bernoulli are
members of the exponential family of probability distributions. The exponential fam-
ily of distributions has density functions of the form
f x; y
ð
Þ ¼
exp k x
ð Þc y
ð Þ + S x
ð Þ + d y
ð Þ
½
, if x 2 B
0,
x62B

where B does not depend on the parameter y.
256
CHAPTER 5 Statistical Estimation

EXAMPLE 5.3.12
Write the following in exponential form.
(a) ellx
x!
(b) px(1p)1x
(c)
1ﬃﬃﬃﬃ
2p
p
e xm
ð
Þ2=2
Solution
(a) We have
ellx
x!
¼ exp x lnl lnx!l
½
:
Here k(x)¼x, c(l)¼ln l, S(x)¼ln(x!), and d(l)¼l.
(b) Similarly,
px 1p
ð
Þ1x ¼ exp x ln
p
1p


+ ln 1p
ð
Þ


, x ¼ 0or1:
(c) This is the standard normal density.
1ﬃﬃﬃﬃﬃﬃ
2p
p
e xm
ð
Þ2=2 ¼ exp xmx2
2 m2
2 1
2 ln 2p
ð
Þ


, 1 < x < 1:
Note that in the previous example, for each of the cases, Si¼1
n
Xi is a sufficient
statistic for the parameter. In the next result, we give a generalization of
this fact.
Theorem 5.3.6 Let X1, . . ., Xn be a random sample from a population with pdf or
pmf of the exponential form
f x; y
ð
Þ ¼
exp k x
ð Þc y
ð Þ + S x
ð Þ + d y
ð Þ
½
, if x 2 B
0,
x62B

where B does not depend on the parameter y. The statistic Si¼1
n k(Xi) is sufficient
for y.
Proof. The joint density
f x1, ..., xn; y
ð
Þ ¼ exp c y
ð Þ
X
n
i¼1
k xi
ð Þ +
X
n
i¼1
S xi
ð Þ + nd y
ð Þ
"
#
¼
exp c y
ð Þ
X
n
i¼1
k xi
ð Þ + nd y
ð Þ
"
#
(
)
exp
X
n
i¼1
S xi
ð Þ
"
#
(
)
:
Using the factorization theorem, the statistic Si¼1
n k(Xi) is sufficient.
n
257
5.3 Some Desirable Properties of Point Estimators

It does not follow that every function of a sufficient statistic is sufficient.
However, any one-to-one function of a sufficient statistic is also sufficient.
Every statistic need not be sufficient. When they do exist, sufficient estimators
are very important, because if one can find a sufficient estimator it is ordinarily
possible to find an unbiased estimator based on the sufficient statistic.
Actually, the following theorem shows that if one is searching for an unbiased
estimator with minimal variance, it has to be restricted to functions of a sufficient
statistics.
RAO-BLACKWELL THEOREM
Theorem 5.4.7 Let X1, . . ., Xn be a random sample with joint pf or pdf f (x1, . . ., xn; y) and let
U¼(U1, . . ., Un) be jointly sufficient for y¼(y1, . . ., yn). If T is any unbiased estimator of k(y), and if
T*¼E(T jU), then:
(a) T* is an unbiased estimator of k(y).
(b) T* is a function of U, and does not depend on y.
(c) Var(T*)Var(T) for every y, and Var(T*)<Var(T) for some y unless T*¼T with probability 1.
Proof.
(a) By the property of conditional expectation and by the fact that T is an unbiased estimator of k(y),
E T	
ð
Þ ¼ E E TjU
ð
Þ
ð
Þ ¼ E T
ð Þ ¼ k y
ð Þ:
Hence, T* is an unbiased estimator of k(y).
(b) Because U is sufficient for y, the conditional distribution of any statistic (hence, for T), given U,
does not depend on y. Thus, T*¼E(TjU) is a function of U.
(c) From the property of conditional probability, we have the following:
Var T
ð Þ ¼ E Var TjU
ð
Þ
ð
Þ + Var E TjU
ð
Þ
ð
Þ
¼ E Var TjU
ð
Þ
ð
Þ + Var T	
ð
Þ:
n
Because Var(TjU)0 for all u, it follows that E(Var(TjU))0. Hence, Var(T*)
Var(T). We note that Var(T*)¼Var(T) if and only if Var(TjU)¼0 or T is a function
of U, in which case T*¼T (from the definition of T*¼E (TjU)¼T).
In particular, if k (y)¼y, and T is an unbiased estimator of y, then T*¼E (TjU)
will typically give the MVUE of y. If T is the sufficient statistic that best summarizes
the data from a given distribution with parameter y, and we can find some function g
of T such that E (g (T))¼y, it follows from the Rao-Blackwell theorem that g(T ) is
the UMVUE for y.
EXERCISES 5.3
5.3.1. Let X1, . . ., Xn be a random sample from a population with density
f x
ð Þ ¼
e xy
ð
Þ,
forx > y
0,
otherwise

:
258
CHAPTER 5 Statistical Estimation

(a) Show that X is a biased estimator of y.
(b) Show that X is an unbiased estimator of m¼1+y.
5.3.2. The mean and variance of a finite population {a1, . . ., aN} are defined by
m ¼ 1
N
X
N
i¼1
ai and s2 ¼ 1
N
X
N
i¼1
ai m
ð
Þ2:
For a finite population, show that the sample variance S2 is a biased
estimator of s2.
5.3.3. For an infinite population with finite variance s2, show that the sample
standard deviation S is a biased estimator for s. Find an unbiased estimator of
s. [We have seen that S2 is an unbiased estimator of s2. From this exercise,
we see that a function of an unbiased estimator need not be an unbiased
estimator.]
5.3.4. Let X1, . . ., Xn be a random sample from an infinite population with finite
variance s2. Define
S02 ¼ 1
n
X
n
i¼1
Xi X

2:
Show that S02 is a biased estimator for s2, and that the bias of S02 is s2
n .
Thus, S02 is negatively biased, and so on average underestimates the variance.
Note that S02 is the MLE of s2.
5.3.5. Let X1, . . ., Xn be a random sample from a population with the mean m. What
condition must be imposed on the constants c1, c2, . . ., cn so that
c1X1 + c2X2 +  + cnXn
is an unbiased estimator of m?
5.3.6. Let X1, . . ., Xn be a random sample from a geometric distribution with
parameter y. Find an unbiased estimate of y.
5.3.7. Let X1, . . ., Xn be a random sample from U (0, y) distribution.
Let Yn¼max{X1, . . ., Xn}. We know (from Example 5.3.4) that ^y1 ¼ Yn is
a MLE of y.
(a) Show that ^y2 ¼ 2X is a method of moments estimator.
(b) Show that ^y1 is ,a biased estimator, and ^y2 is an unbiased estimator of y.
(c) Show that ^y3 ¼ n + 1
n ^y1 is an unbiased estimator of y.
5.3.8. Let X1, . . ., Xn be a random sample from a population with mean m and
variance 1. Show that ^m2 ¼ X
2 is a biased estimator of m2, and compute
the bias.
5.3.9. Let X1, . . ., Xn be a random sample from an N (m, s2) distribution. Show that
the estimator ^m ¼ X is the MVUE for m.
5.3.10. Let X1, ...,Xn1 be a random sample from an N (m1, s2) distribution and let
Y1, ...,Yn2 be a random sample from a N (m2, s2) distribution. Show that the
pooled estimator
^s2 ¼ n1 1
ð
ÞS2
1 + n2 1
ð
ÞS2
2
n1 + n2 2
is unbiased for s2, where S1
2 and S2
2 are the respective sample variances.
259
5.3 Some Desirable Properties of Point Estimators

5.3.11. Let X1, . . ., Xn be a random sample from an N (m, s2) distribution. Show that
the sample median, M, is an unbiased estimator of the population mean m.
Compare the variances of X and M. [Note: For the normal distribution, the
mean, median, and mode all occur at the same location. Even though both X
and M are unbiased, the reason we usually use the mean instead of the
median as the estimator of m is that X has a smaller variance than M.]
5.3.12. Let X1, . . ., Xn be a random sample from a Poisson distribution with
parameter l. Show that the sample mean X is sufficient for l.
5.3.13. Let X1, . . ., Xn be a random sample from a population with density function
f s x
ð Þ ¼ 1
2sexp  xj j
s


, 1 < X < 1, s > 0:
Find a sufficient statistic for the parameter s.
5.3.14. Show that if ^y is a sufficient statistic for the parameter y and if the MLE of y
is unique, then the MLE is a function of this sufficient statistic ^y.
5.3.15. Let X1, . . ., Xn be a random sample from an exponential population with
parameter y.
5.3.16. Show that Si¼1
n Xi is sufficient for y. Also show that X is sufficient for y.
5.3.17. The following is a random sample from exponential distribution.
1:5
3:0
2:6
6:8
0:7
2:2
1:3
1:6
1:1
6:5
0:3
2:0
1:8
1:0
0:7
0:7
1:6
3:0
2:0
2:5
5:7
0:1
0:2
0:5
0:4
5.3.18. What is an unbiased estimate of the mean?
5.3.19. Using part (a) and these data, find two sufficient statistics for the parameter y.
5.3.20. Let X1, . . ., Xn be a random sample from a one-parameter Weibull
distribution with pdf
f x
ð Þ ¼
2axeax2,x > 0
0,
otherwise:

5.3.19.1. Find a sufficient statistic for a.
5.3.19.2. Using part (a), find an UMVUE for a.
5.3.21. Let X1, . . ., Xn be a random sample from a population with density function
f x
ð Þ ¼
1
y, y
2  x  y
2, y > 0
0,
otherwise:
8
<
:
Show that
min
1inXi, max
1inXi


is sufficient for y.
5.3.22. Let X1, . . ., Xn be a random sample from a G(1, b) distribution.
(a) Show that U¼P
i¼1
n Xi is a sufficient statistic for b.
(b) The following is a random sample from a G(1,b) distribution.
0:3
3:4
0:4
1:8
0:7
1:0
0:1
2:3
3:7
2:0
0:3
3:7
0:1
1:3
1:2
3:3
0:2
1:3
0:6
0:4
260
CHAPTER 5 Statistical Estimation

Find a sufficient statistic for b.
5.3.23. Show that X1 is not sufficient for m, if X1, . . ., Xn is a sample from N(m, 1).
5.3.24. Let X1, . . ., Xn be a random sample from the truncated exponential
distribution with pdf
f x
ð Þ ¼
eyx, x > 0
0,
otherwise

:
Show that X(1)¼min(Xi) is sufficient for y.
5.3.25. Let X1, . . ., Xn be a random sample from a distribution with pdf
f x
ð Þ ¼
yxy1, 0 < x < 1, y > 0
0,
otherwise

:
Show that U¼X1, . . ., Xn is a sufficient statistic for y.
5.3.26. Let X1, . . ., Xn be a random sample from a Rayleigh distribution with pdf
f x
ð Þ ¼
2x
a ex2=a,x > 0
0,
otherwise:
(
Show that P
i¼1
n Xi
2 is sufficient for the parameter a.
5.4 A METHOD OF FINDING THE CONFIDENCE INTERVAL:
PIVOTAL METHOD
In the previous sections, we studied methods for finding point estimators for the pop-
ulation parameters. In general, the estimates will differ from the true parameter
values by varying amounts depending on the sample values obtained. In addition,
the point estimates do not convey any measure of reliability.
Now, we discuss another type of estimation, called an interval estimation. Although
point estimators are useful, interval estimators convey more information about the data
thatare usedtoobtainthepoint estimate.The purposeofusing anintervalestimatoris to
have some degree of confidence of securing the true parameter. For an interval estima-
tor ofa singleparameter y, wewill use the randomsampletofindtwo quantitiesL and U
such that L<y<U with some probability. Because L and U depend on the sample
values, they will be random. This interval (L, U) should have two properties: (1)
P(L<y<U) is high, that is, the true parameter y is in (L, U) with high probability,
and (2) the length of the interval (L, U) should be relatively narrow on the average.
In summary, interval estimation goes a step beyond point estimation by provid-
ing, in addition to the estimating interval (L, U), a measure of one’s confidence in
the accuracy of the estimate. Interval estimators are called confidence intervals
and the limits are called U and L, the upper and lower confidence limits, respectively.
The associated levels of confidence are determined by specified probabilities. The
width of the confidence interval reflects the amount of variability inherent in
the point estimate. Thus, our objective is to find a narrow interval with high
261
5.4 A Method of Finding the Confidence Interval: Pivotal Method

probability of enclosing the true parameter, y. We will restrict our attention to single
parameter estimation.
The probability that a confidence interval will contain the true parameter y is called
the confidence coefficient. The confidence coefficient gives the fraction of the time that
the constructed interval will contain the true parameter, under repeated sampling.
Let L and U be the lower and upper confidence limits for a parameter y based on a
random sample X1, . . ., Xn. Both L and U are functions of the sample. We can write
the interval estimate of y as
P L  y  U
ð
Þ ¼ 1a
and we read it as we are (1a) 100% confident that the true parameter y is located in
the interval (L, U). The number 1a is the confidence coefficient, and the interval
(L, U) is referred to as a (1a) 100% confidence interval ((1a) 100% CI) for y.
Thus, if we want a 95% confidence interval for, say, population mean m, then
a¼0.05. Note that for the discrete random variables, we may not be able to find
a lower bound L and an upper bound U such that the probability, P(LyU), is
exactly (1a). In such a case we can choose L and U such that P(LyU)1a.
How do we find the confidence interval? For this, we use the error structure of the
point estimator to obtain this interval. For instance, we know that the sample mean,
X, is a point estimate (MLE or unbiased estimator) of the population mean m. In this
case, we also know that the standard error of X is s=
ﬃﬃﬃn
p : If the sample came from a
normal population, then for a 95% confidence interval for the mean, multiply the
standard error by 1.96 and then add and subtract this product from the sample mean.
From this we can also observe that, if everything else remains the same, the size of
the confidence interval reduces as the sample size increases.
EXAMPLE 5.4.1
As part of a promotion, the management of a large health club wants to estimate average weight loss
for its members within the first 3 months after joining the club. They took a random sample of 45
members of this health club and found that they lost an average of 13.8 pounds within the first
3 months of membership with a sample standard deviation of 4.2 pounds. Find a 95% confidence
interval for the true mean. What if a random sample of 200 members of this health club also resulted
in the same sample mean and sample standard deviation?
Solution
Here a point estimate of the true mean m is the sample mean x ¼ 13:8 pounds. Because n¼45 is large
enough, we can use the Central Limit Theorem and use approximate normality for the distribution of
X with mean m and the approximate standard error 4:2=
ﬃﬃﬃﬃﬃ
45
p


¼ 0:626: Thus a 95% confidence
interval is 13.8
(1.96)(0.626), resulting in the interval (12.57, 15.03). Thus, on average, with
95% confidence, one can expect the true mean to lie in this interval.
For n¼200, the standard error is
4:2=
ﬃﬃﬃﬃﬃﬃﬃﬃ
200
p


 0:297: Thus a 95% confidence interval is
13.8
(1.96)(0.297) resulting in the interval (13.22, 14.38). Thus the more sample values (that
is, the more information) we have, the tighter (smaller width) the interval.
The previous example was built on our knowledge of the sampling distribution of the sample
mean. What if the sampling distribution of the statistic we are interested in is not readily available?
More generally, our success in building confidence intervals for an estimate of a parameter depends
on identifying a quantity known as the pivot. We now describe this method.
262
CHAPTER 5 Statistical Estimation

The pivotal method is a general method of constructing a confidence interval
using a pivotal quantity. This relies on our knowledge of sampling distributions. Here
we have to find a pivotal quantity with the following two characteristics:
(i) It is a function of the random sample (a statistic or an estimator ^y) and the
unknown parameter y, where y is the only unknown quantity, and
(ii) It has a probability distribution that does not depend on the parameter y.
Suppose that ^y ¼ ^y X
ð Þ is a point estimate of y, and let p ^y, y


be the pivotal quan-
tity. Now, for a given value of a, (0<a<1), and constants a and b, with (a<b), let
P a  p ^y, y


 b


¼ 1a:
Hence, given ^y, the inequality is solved for y to obtain a region of y values, usually an
interval corresponding to the observed ^y-value. This will be a desired confidence
interval.
From (i) and (ii), it is important to note that the pivotal quantity depends on the
parameter, but its distribution is independent of the parameter. Let X1, . . ., Xn be a
random sample and let ^y be a reasonable point estimate of y. For instance, ^y could be
the maximum likelihood (or some other) estimator of y. In general, finding a pivotal
quantity may not be easy. However, if ^y is the sample mean X or sample variance S2,
we could find a pivotal quantity with known sampling distributions. Suppose p ^y, y


is a pivotal quantity with known probability distribution that is independent of y.
(Usually, the probability distribution of the pivotal quantity will be standard normal,
t, w2, or F-distribution.) The following are some of the standard pivotal quantities: If
the sample X1, ..., Xn is from N(m, s2)
With m unknown and s known, let X be the sample mean. Then the pivot is
X m


= s=
ﬃﬃﬃn
p
ð
Þ, which has an N(0, 1) distribution (see comments after Corollary
4.2.2).
With m unknown and s unknown, then the pivot is X m


= S=
ﬃﬃﬃn
p
ð
Þ, which has a
t-distribution with (n1) degrees of freedom (see Theorem 4.2.9). If n is large, using
CLT, the distribution of the pivot is approximately N(0, 1).
If s2 is unknown, then the pivot is (n1)S2/s2, which has a w2-distribution with
(n1) degrees of freedom (see Theorem 4.2.8).
The following examples illustrate the pivotal method.
EXAMPLE 5.4.2
Suppose we have a random sample X1, . . ., Xn from N(m, 1). Construct a 95% confidence interval for m.
Solution
Here the confidence coefficient is 0.95. We know that the MLE of m is X which has an N(m,1/n)
distribution. Note that this distribution depends on the unknown value of m, and hence X cannot
be a pivot. However, taking the z-transform of X we obtain the pivotal quantity as
Z ¼ X m
s=
ﬃﬃﬃn
p ¼ Xm
1=
ﬃﬃﬃn
p
Continued
263
5.4 A Method of Finding the Confidence Interval: Pivotal Method

which has an N(0, 1) distribution that is a function of the sample measurements and does not depend
on m. Hence, this Z can be taken as a pivot p ^y, y


: Now to find a and b such that
P a  Z
ð
Þ ¼ p ^y, y


 bÞ ¼ 0:95: One such choice is to find the value of a such that P
(aZa)¼0.95. From the normal table,
P za=2  Z  za=2


¼ 0:95,
where za/2 represents the value of z with tail area a/2. This implies a¼za/2¼1.96. Hence,
P 1:96  Z  1:96
ð
Þ ¼ 0:95
or, using the definition of Z and solving for m, we obtain
P X1:96ﬃﬃﬃn
p
 m  X + 1:96ﬃﬃﬃn
p


¼ 0:95:
Hence, a 95% confidence interval for m is X  1:96=
ﬃﬃﬃn
p
ð
Þ,X + 1:96=
ﬃﬃﬃn
p
ð
Þ


: Thus, the lower
confidence limit L is X 1:96=
ﬃﬃﬃn
p
ð
Þ and the upper confidence limit U is X + 1:96=
ﬃﬃﬃn
p
ð
Þ:
From the derivation of Example 5.4.1, it follows that
P
X m

 < za=2
sﬃﬃﬃn
p


¼ 1a:
Thus, for a normal population with known variance s2, if X is used as an estimator of the
truemean m, theprobability that theerrorwill beless than za=2s=
ﬃﬃﬃn
p is 1a. Itis impor-
tant to note that there is some arbitrariness in choosing a confidence interval for a given
problem. There may be several pivotals for ^y that could be used. Also, it is not necessary
to allocate equal probability to the two tails of the distribution; however, doing so may
result in the shortest length confidence interval for a given confidence coefficient.
When we make the statement of the form
P X 1:96ﬃﬃﬃn
p
 m  X + 1:96ﬃﬃﬃn
p


¼ 0:95,
we mean that, in an infinite series of trials in which repeated samples of size n are
drawn from the same population and 95% confidence intervals for m are calculated
by the same method for each of the samples, the proportion of intervals that actually
include m will be 0.95. Figure 5.4 illustrates this idea, where the vertical line repre-
sents the position of true mean m and each of the horizontal lines represents a 95%
confidence interval of the sample, 20 samples of size n are taken.
A statement of the type P x 1:96=
ﬃﬃﬃn
p
ð
Þ  m  x + 1:96=
ﬃﬃﬃn
p
ð
Þ
ð
Þ ¼ 0:95, where x
is the observed sample mean, is misleading. Once we calculate this interval using a
particular sample, then either this interval contains the true mean m or not, and hence
the probability will be either 0 or 1. Thus, the correct interpretation of confidence
interval for the population mean is that if samples of the same size, n, are drawn
repeatedly from a population, and a confidence interval is calculated from each
sample, then 95% of these intervals should contain the population mean. This is often
stated as “We are 95% confident that the true mean is in the interval
X za=2 s=
ﬃﬃﬃn
p
ð
Þ,X + za=2 s=
ﬃﬃﬃn
p
ð
Þ


:” Thus, the correct interpretation requires the
confidence limits to be variables. This concept of confidence interval is attributed
to Neyman.
264
CHAPTER 5 Statistical Estimation

We can follow the accompanying procedure to find a confidence interval for the
parameter y.
PROCEDURE TO FIND A CONFIDENCE INTERVAL FOR u USING THE PIVOT
1. Find an estimator ^y of y: usually MLE of y works.
2. Find a function of y and ^y,p y, ^y


(pivot), such that the probability distribution of p(.,.) does not
depend on y.
3. Find a and b such that P a  p y, ^y


 b


¼ 1a:. Choose a and b such that P p y, ^y


 a


¼
a=2 and P p y, ^y


 b


¼ a=2 (see Figure 5.5 where the shaded area in each side is a/2).
4. Now, transform the pivot confidence interval to a confidence interval for the parameter y. That is,
work with the inequality in step 3 and rewrite it as P(LyU)¼1a, where L is the lower
confidence limit and U is the upper confidence limit.
m
FIGURE 5.4
95% confidence intervals for m.
FIGURE 5.5
Probability density of the pivot.
265
5.4 A Method of Finding the Confidence Interval: Pivotal Method

The following example is given to show that the success of finding a pivotal
quantity depends on our ability to find the right transformation of the statistic and
its distribution so that the transformed variable is a pivot.
EXAMPLE 5.4.3
Suppose the random sample X1, . . ., Xn has U(0,y) distribution. Construct a 90% confidence interval
for y and interpret. Identify the upper and lower confidence limits.
Solution
From Example 5.3.4, we know that
U ¼ max Xi
1in
is the MLE of y. The random variable U has the pdf
f U u
ð Þ ¼ nun1=yn, 0  u  y:
This is not independent of the parameter y. Let Y¼U/y, then (using the Jacobians described in
Chapter 3) the pdf of Y is given by
f Y y
ð Þ ¼ nyn1, 0  y  1:
Hence, Y satisfies the two characteristics of the pivotal quantity. Thus, Y¼U/y is a pivot. Now,
we have to find a and b such that
p a  U
y  b


¼ 0:90:
PDF of Y
0.05
0.05
0
1
y
To find a and b we use the cdf of Y, FY(y)¼yn, 0y1, as follows.
266
CHAPTER 5 Statistical Estimation

FY a
ð Þ ¼ 0:05 and FY b
ð Þ ¼ 0:95
which implies that
an ¼ 0:05 and bn ¼ 0:95
resulting in
a ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:05
np
and b ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:95
np
:
Write
P
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:05
np
< U
y <
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:95
np


¼ 0:90:
Solving, the 90% confidence interval for y is
U
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:95
np
,
U
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:05
np


or
P
U
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:95
np
 y 
U
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:95
np


¼ 0:90:
Thus, the lower confidence limit is U=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:95
np
and the upper confidence limit is U=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:05
np
, and
the 90% confidence interval is U=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:95
np
,U=
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:05
np
Þ:

We can interpret this in the following manner. In a large number of trials in which
repeated samples are taken from a population with uniform pdf with parameter y,
approximately 90% of the intervals will contain y. For instance, if we observed
n¼20 values from a uniform distribution with the maximum observed value being
15, then a 90% confidence interval for y is (15.04, 17.42). Thus, we are 90% con-
fident that these data came from a uniform distribution upper limit falling somewhere
in this interval.
It is important to note that the pivotal method may not be applicable in all
situations. For example, in the binomial case, to find a confidence interval for p,
there is no quantity that satisfies the two conditions of a pivot. However, if sample
size is large, then the z-score of sample proportion can be used as a pivot with ap-
proximate standard normal distribution. For pivotal method to work, there is the
practical necessity that the distribution of the pivotal quantity make it easy to
compute the probabilities. In cases where the pivotal method does not work, we
may need to use other techniques such as the method based on sampling distributions
(see Project 4A). A proper discussion of these methods is beyond the level of
this book.
267
5.4 A Method of Finding the Confidence Interval: Pivotal Method

EXERCISES 5.4
5.4.1. (a) Suppose we construct a 99% confidence interval. What are we 99%
confident about?
(b) Which of the confidence intervals is wider, 90% or 99%?
(c) In computing a confidence interval, when do you use the t-distribution
and when do you use z, with normal approximation?
(d) How does the sample size affect the width of a confidence interval?
5.4.2. Suppose X is a random sample of size n¼1 from a uniform distribution
defined on the interval (0, y). Construct a 98% confidence interval for y
and interpret.
5.4.3. Consider the probability statement
P 2:81  Z ¼ X m
s=
ﬃﬃﬃn
p  2:75


¼ k
where X is the mean of a random sample of size n from N(m, s2) distribution
with known s2.
(a) Find k.
(b) Use this statement to find a confidence interval for m.
(c) What is the confidence level of this confidence interval?
(d) Find a symmetric confidence interval for m.
5.4.4. A random sample of size 50 from a particular brand of 16-ounce tea packets
produced a mean weight of 15.65 ounces. Assume that the weights of these
brands of tea packets are normally distributed with standard deviation of
0.59 ounce. Find a 95% confidence interval for the true mean m.
5.4.5. Let X1, . . ., Xn be a random sample from an N(m, s2), where the value of s2
is unknown.
(a) Construct a (1a) 100% confidence interval for s2, choosing an
appropriate pivot. Interpret its meaning.
(b) Suppose a random sample from a normal distribution gives the
following summary statistics: n¼21, x ¼ 44:3, and s¼3.96. Using part
(a), find a 90% confidence interval for s2. Interpret its meaning.
5.4.6. Let X1, . . ., Xn be a random sample from a gamma distribution with a¼2
and unknown b. Construct a 95% confidence interval for b.
5.4.7. Let X1, . . ., Xn be a random sample from an exponential distribution with pdf
f(x)¼(1/y)ex/y, y>0, x>0. Construct a 95% confidence interval for y and
interpret.[Hint:RecallthatP
i¼1
n Xihasagammadistributionwitha¼n,b¼y.]
5.4.8. Let X1, . . ., Xn be a random sample from a Poisson distribution with
parameter l.
(a) Construct a 90% confidence interval for l.
(b) Suppose that the number of raisins in a bowl of a particular brand of
cereal is observed to be 25. Assuming that the number of raisins in a
bowl is Poisson distributed, estimate the expected number of raisins per
bowl with a 90% confidence interval.
268
CHAPTER 5 Statistical Estimation

(c) How many bowls of cereal need to be sampled in order to estimate the
expected number of raisins per bowl with a standard error of less than 0.2?
5.4.9. Let X1, . . ., Xn be a random sample from an N(m, s2).
(a) Construct a (1a) 100% confidence interval for m when the value of s2
is known.
(b) Construct a (1a) 100% confidence interval for m when the value of s2
is unknown.
5.4.10. Let X1, . . ., Xn be a random sample from an N(m1, s2) population and Y1, . . .,
Yn be an independent random sample from an N(m2, s2) distribution where
s2 is assumed to be known. Construct a (1a) 100% interval for (m1m2).
Interpret its meaning.
5.4.11. Let X1, . . ., Xn be a random sample from a uniform distribution on [y, y+1].
Find a 99% confidence interval for y, using an appropriate pivot.
5.5 ONE SAMPLE CONFIDENCE INTERVALS
In this section, we will find confidence intervals for one sample case for both large
and small sample situations.
5.5.1 LARGE SAMPLE CONFIDENCE INTERVALS
If the sample size is large, then by the Central Limit Theorem, certain sampling dis-
tributions can be assumed to be approximately normal. That is, if y is an unknown
parameter (such as m, p, (m1m2), (p1p2)), then for large samples, by the Central
Limit Theorem, the z-transform
z ¼
^yy
s^y
possesses an approximately standard normal distribution, where ^y is the MLE of y
and s^y is its standard deviation. Then as in Example 5.4.1, the pivotal method can be
used to obtain the confidence interval for the parameter y. For y¼m, n30 will be
considered large; for the binomial parameter p, n is considered large if np, and
n(1p) are both greater than 5. Note that these numbers are only a rule of thumb.
PROCEDURETO CALCULATELARGESAMPLE CONFIDENCEINTERVALFORu
1. Find an estimator (such as the MLE) of y, say ^y.
2. Obtain the standard error, s^yof ^y:
3. Find the z-transform z ¼
^yy


=s^y: Then z has an approximately standard normal distribution.
4. Using the normal table, find two tail valuesza/2 and za/2.
5. An approximate (1a) 100% confidence interval for y is
^yza=2s^y, ^y + za=2s^y


, that is,
P ^yza=2s^y  y  ^y + za=2s^y


¼ 1a:
6. Conclusion: We are (1–a) 100% confident that the true parameter y lies in the interval
^yza=2s^y, ^y + za=2s^y


:
269
5.5 One Sample Confidence Intervals

EXAMPLE 5.5.1
Let ^y be a statistic that is normally distributed with mean y and standard deviation s^y, where s is
assumed to be known. Find a confidence interval for y that possesses a confidence coefficient equal
to 1–a.
Solution
The z-transform of ^y is
Z ¼
^yy
s^y
and has a standard normal distribution. Select two tail valuesza/2 and za/2 such that
P za=2  Z  za=2


¼ 1a:
Because of symmetry, this is the shortest interval that contains the area 1a. Then,
P ^yza=2s^y  y  ^y + za=2s^y


¼ 1a:
Therefore, the confidence limits of y are ^yza=2s^y and ^y + za=2s^y: Hence, (1a)100% confi-
dence interval for y is given by ^y
za=2s^y:
If in particular for a large sample of size n, let ^y ¼ X be the sample mean. Then the
large sample (1a) 100% confidence interval for the population mean m is
X 
za=2
sﬃﬃﬃn
p ’ X 
za=2
Sﬃﬃﬃn
p
where S is a point estimate of s. That is,
P X za=2
Sﬃﬃﬃn
p  m  X 
za=2
Sﬃﬃﬃn
p


¼ 1a:
As we have seen in Section 5.4, the correct interpretation of this confidence interval
is that in a repeated sampling, approximately (1a) 100% of all intervals of the form
X 
za=2 S=
ﬃﬃﬃn
p
ð
Þ include m, the true mean. Suppose x and s are the sample mean and
the sample standard deviation, respectively, for a particular set of n observed sample
values x1, ..., xn. Then we do not know whether the particular interval
xza=2 s=
ﬃﬃﬃn
p
ð
Þ,xza=2 s=
ﬃﬃﬃn
p
ð
Þ


contains m. However, the procedure that produced
this interval does capture the true mean in approximately (1a) 100% of cases. This
interpretation will be assumed hereafter, when we make a statement such as, “We are
95% confident that the true mean will lie in the interval (74.1, 79.8).”
EXAMPLE 5.5.2
Two statistics professors want to estimate average scores for an elementary statistics course that has
two sections. Each professor teaches one section and each section has a large number of students. A
random sample of 50 scores from each section produced the following results:
(a) Section I: x1 ¼ 77:01,s1¼10.32.
(b) Section II: x2 ¼ 72:22,s2¼11.02.
Calculate 95% confidence intervals for each of these three samples.
270
CHAPTER 5 Statistical Estimation

Solution
Because n¼50 is large, we could use normal approximation. For a¼0.05, from the normal table:
za/2¼z0.025¼1.96. The confidence intervals are:
(a) We have
x1 
za=2
s1ﬃﬃﬃn
p ¼ 77:01
1:96 10:32
ﬃﬃﬃﬃﬃ
50
p


which gives a 95% confidence interval (74.149, 79.871).
(b) We can compute
x2 
za=2
s2ﬃﬃﬃn
p ¼ 72:22
1:96 11:02
ﬃﬃﬃﬃﬃ
50
p


which gives the interval (69.165, 75.275).
It may be noted that if the population is normal with a known variance s2, we
can use X 
za=2 s=
ﬃﬃﬃn
p
ð
Þ as the confidence interval for the population mean m, irre-
spective of the sample size. However, if s2 is unknown, in order to use
X 
za=2 s=
ﬃﬃﬃn
p
ð
Þ as an approximate confidence interval for m, the sample size has
to be large for the Central Limit Theorem to hold. However to use this approximate
procedure, we do not need the condition that samples arise from a normal distribu-
tion. We will consider sample size to be large if n30 (applicable to estimators of
the mean). If not, we shall use the small sample procedure discussed in the next
section.
EXAMPLE 5.5.3
Fifteen vehicles were observed at random for their speeds (in mph) on a highway with speed limit
posted as 70 mph, and it was found that their average speed was 73.3 mph. Suppose that from past
experience we can assume that vehicle speeds are normally distributed with s¼3.2. Construct a
90% confidence interval for the true mean speed m, of the vehicles on this highway. Interpret the
result.
Solution
Because the population is given to be normal with standard deviation s¼3.2, sample size need not
be large given x ¼ 73:3 and s¼3.2. Here, n¼15, and a¼0.10. Thus, za/2¼z0.05¼1.645. Hence, a
90% confidence interval for m is given by
73:31:645 3:2ﬃﬃﬃﬃﬃ
15
p
< m < 73:3 + 1:645 3:2ﬃﬃﬃﬃﬃ
15
p
or
71:681 < m < 74:919:
Interpretation: We are 90% confident that the true mean speed m of the vehicles on this highway
is between 71.681 and 74.919.
271
5.5 One Sample Confidence Intervals

5.5.2 CONFIDENCE INTERVAL FOR PROPORTION, p
Consider a binomial distribution with parameter p. Let X be the number of successes
in n trials. Then the MLE ^p of p is ^p ¼ X=n: It can be shown, using the procedure
outlined at the beginning of this section, that an approximate large sample (1a)
100% confidence interval for p is
^pza=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p 1 ^p
ð
Þ
n
r
, ^p + za=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p 1 ^p
ð
Þ
n
r
 
!
:
That is,
P
^pza=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p 1 ^p
ð
Þ
n
r
< p < ^p + za=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p 1 ^p
ð
Þ
n
r
 
!
¼ 1a:
A natural question is: “How do we determine the sample size that we have is suffi-
cient for the normal approximation that is used in the foregoing formula?” There are
various rules of thumb that are used to determine the adequacy of the sample size for
normal approximation. Some of the popular rules are that np and n(1–p) should be
greater than 10, or that ^p
2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p 1 ^p
ð
Þ=n
p
should be contained in the interval (0,1), or
np(1p)10, etc. All of these rules perform poorly when p is nearer to 0 or 1.
Recently, there have been many works on coverage analysis for confidence intervals.
We refer to a survey article by Lee et al. for more details on this topic. For simplicity
of calculations, we will use the rule that np and n(1p) are both greater than 5.
EXAMPLE 5.5.4
An auto manufacturer gives a bumper-to-bumper warranty for 3 years or 36,000 miles for its new
vehicles. In a random sample of 60 of its vehicles, 20 of them needed five or more major warranty
repairs within the warranty period. Estimate the true proportion of vehicles from this manufacturer
that need five or more major repairs during the warranty period, with confidence coefficient 0.95.
Interpret.
Solution
Here we need to find a 95% confidence interval for the true proportion, p. Here, ^p ¼ 20=60 ¼ 1=3:
For a¼0.05, za/2¼z0.025¼1.96. Hence, a 95% confidence interval for p is
^p
za=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p 1 ^p
ð
Þ
n
r
¼ 1
3
1:96
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
3
 
2
3
 
60
s
which gives the confidence interval as (0.21405, 0.45262). That is, we are 95% confident that the
true proportion of vehicles from this manufacturer that need five or more major repairs during the
warranty period will lie in the interval (0.21405, 0.45262).
5.5.2.1 MARGIN OF ERROR AND SAMPLE SIZE
In real-world problems, the estimates of the proportion p are usually accompanied by
a margin of error, rather than a confidence interval. For example, in the news media,
especially leading up to election time, we hear statements such as “The CNN/USA
Today/Gallup poll of 818 registered voters taken on June 27-30 showed that if the
272
CHAPTER 5 Statistical Estimation

election were held now, the president would beat his challenger 52% to 40%, with
8% undecided. The poll had a margin of error of plus or minus four percentage
points.” What is this “margin of error”? According to the American Statistical Asso-
ciation, the margin of error is a common summary of sampling error that quantifies
uncertainty about a survey result. Thus, the margin of error is nothing but a confi-
dence interval. The number quoted in the foregoing statement is half the maximum
width of a 95% confidence interval, expressed as a percentage.
Let b be the width of a 95% confidence interval for the true proportion, p. Let
^p ¼ x=n be an estimate for p where x is the number of successes in n trials. Then,
b ¼x
n + 1:96
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x=n
ð
Þ 1 x=n
ð
Þ
ð
Þ
n
r

x
n1:96
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x=n
ð
Þ 1 x=n
ð
Þ
ð
Þ
n
r
 
!
¼3:92
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x=n
ð
Þ 1 x=n
ð
Þ
ð
Þ
n
r
 3:92
ﬃﬃﬃﬃﬃ
1
4n
r
,
because x=n
ð
Þ 1 x=n
ð
Þ
ð
Þ ¼ ^p 1 ^p
ð
Þ  1
4:
Thus, the margin of error associated with ^p ¼ x=n
ð
Þ is 100d%, where
d ¼ maxb
2
¼ 3:92
ﬃﬃﬃ
1
4n
p
2
¼ 1:96
2
ﬃﬃﬃn
p :
From the foregoing derivation, it is clear that we can compute the margin of error for
other values of a by replacing 1.96 by the corresponding value of za/2.
A quick look at the formula for the confidence interval for proportions reveals
that a larger sample would yield a shorter interval (assuming other things being
equal) and hence a more precise estimate of p. The larger sample is more costly
in terms of time, resources, and money, whereas samples that are too small may result
in inaccurate inferences. Then, it becomes beneficial for finding out the minimum
sample size required (thus less costly) to achieve a prescribed degree of precision
(usually, the minimum degree of precision acceptable). We have seen that the large
sample (1a) 100% confidence interval for p is
^pza=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p 1 ^p
ð
Þ
n
r
< p < ^p + za=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p 1 ^p
ð
Þ
n
r
:
Rewriting it, we have
^pp
j
j  za=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p 1 ^p
ð
Þ
n
r
¼ za=2ﬃﬃﬃn
p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p 1 ^p
ð
Þ
p
which shows that, with probability (1a), the estimate ^p is within za=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p 1 ^p
ð
Þ=n
p
units of p. Because ^p 1 ^p
ð
Þ  1=4, for all values of ^p, we can write the foregoing
inequality as
^pp
j
j  za=2ﬃﬃﬃn
p
ﬃﬃﬃ
1
4
r
¼ za=2
2
ﬃﬃﬃn
p :
273
5.5 One Sample Confidence Intervals

If we wish to estimate p at level (1a) to within d units of its true value, that is
j^ppj  d, the sample size must satisfy the condition za=2= 2
ﬃﬃﬃn
p
ð
ÞÞ  d,

or
n 
z2
a=2
4d2 :
Thus, to estimate p at level (1a) to within d units of its true value, take the minimal
sample size as n¼za/2
2 /4d2, and if this is not an integer, round up to the next integer.
Sometimes, we may have an initial estimate ep of the parameter p from a similar
process or from a pilot study or simulation. In this case, we can use the following
formula to compute the minimum required size of the sample to estimate p, at level
(1a), to within d units by using the formula
n ¼
z2
a=2ep 1ep
ð
Þ
d2
and, if this is not an integer, rounding up to the next integer.
A similar derivation for calculation of sample size for estimation of the popula-
tion mean m at level (1a) with margin of error E is given by
n ¼
z2
a=2s2
E2
and, if this is not an integer, rounding up to the next integer. This formula can be used
only if we know the population standard deviation, s. Although it is unlikely to know
s when the population mean itself is not known, we may be able to determine s from
an earlier similar study or from a pilot study/simulation.
EXAMPLE 5.5.5
A dendritic tree is a branched formation that originates from a nerve cell. In order to study brain
development, researchers want to examine the brain tissues from adult guinea pigs. How many cells
must the researchers select (randomly) so as to be 95% sure that the sample mean is within 3.4 cells
of the population mean? Assume that a previous study has shown s¼10 cells.
Solution
A 95% confidence corresponds to a¼0.05. Thus, from the normal table, za/2¼z0.025¼1.96. Given
that E¼3.4 and s¼10, and using the sample size formula, the required sample size n is
n ¼
z2
a=2s2
E2
¼ 1:96
ð
Þ2 10
ð
Þ2
3:4
ð
Þ2
¼ 33:232:
Thus, take n¼34.
EXAMPLE 5.5.6
Suppose that a local TV station in a city wants to conduct a survey to estimate support for the pres-
ident’s policies on economy within 3% error with 95% confidence.
(a) How many people should the station survey if they have no information on the support level?
(b) Suppose they have an initial estimate that 70% of the people in the city support the economic
policies of the president. How many people should the station survey?
274
CHAPTER 5 Statistical Estimation

Solution
Here a¼0.05, and thus za/2¼1.96. Also, d¼0.03.
(a) With no information on p, we use the sample size formula:
n ¼
z2
a=2
4d2 ¼
1:96
ð
Þ2
4 0:03
ð
Þ2 ¼ 1067:1:
Hence, the TV station must survey 1068 people.
(b) Because ep ¼ 0:7, the required sample size is calculated from
n ¼
z2
a=2ep 1ep
ð
Þ
d2
¼ 1:96
ð
Þ2 0:70
ð
Þ 0:30
ð
Þ
0:03
ð
Þ2
¼ 896:37:
Thus, the TV station must survey at least 897 people.
In practice, we should realize that one of the key factors of a good design is not
sample size by itself; it is getting representative samples. Even if we have a very large
sample size, if the sample is not representative of our target population, then sample
size means nothing. Therefore, whenever possible, we should use random sampling
procedures (or other appropriate sampling procedures) to ensure that our target pop-
ulation is properly represented.
5.5.3 SMALL SAMPLE CONFIDENCE INTERVALS FOR m
Now we will consider the problem of finding a confidence interval for the true mean
m of a normal population when the variance s2 is unknown and obtaining a large sam-
ple is either impossible or impractical. Let X1, . . ., Xn be a random sample from a
normal population. We know that
T ¼
ﬃﬃﬃn
p
Xm
s
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n1
ð
ÞS2= s2 n1
ð
Þ
½

q
¼ X m
S=
ﬃﬃﬃn
p
has a t-distribution with (n1) degrees of freedom, irrespective of the value of s2.
Thus, X m


= S=
ﬃﬃﬃn
p Þ
ð
can be used as a pivot. Hence, for n small (n<30) and s2
unknown, we have the following result.
Theorem 5.5.1 If X and S are the sample mean and the sample standard deviation
of a random sample of size n from a normal population, then
X ta=2:n1
Sﬃﬃﬃn
p < m < X + ta=2,n1
Sﬃﬃﬃn
p
is a (1a) 100% confidence interval for the population mean m.
Note that if the confidence coefficient, 1a, and X and S remain the same, the
confidence range CR ¼ ^yU  ^yL decreases as the sample size n increases, which
means that we are closing in on the true parameter value of y.
275
5.5 One Sample Confidence Intervals

One can use the following procedure to find the confidence interval for the mean
when a small sample is from an approximately normal distribution.
PROCEDURE TO FIND SMALL SAMPLE CONFIDENCE INTERVAL FOR m
1. Calculate the values of X and S, from the sample X1, ..., Xn.
2. Using the t-table, select two tail valuesta/2 and ta/2.
3. The (1a) 100% confidence interval for m is
X ta=2,n1
Sﬃﬃﬃn
p , X + ta=2,n1
Sﬃﬃﬃn
p


that is,
P X ta=2,n1
Sﬃﬃﬃn
p  m  X + ta=2,n1
Sﬃﬃﬃn
p


¼ 1a:
4. Conclusion: We are (1a) 100% confident that the true parameter m lies in the interval
X ta=2,n1 S=
ﬃﬃﬃn
p
ð
Þ,X + ta=2,n1 S=
ﬃﬃﬃn
p
ð
Þ


:
5. Assumption: The population is normal.
In practice, the first step in the previous procedure should include a test of nor-
mality (see Project 4C). A built-in test of normality is available in most of the sta-
tistical software packages. In Example 5.5.9, we show how this test is utilized. Even
when the data fail the normality test, most statistical software will produce a confi-
dence interval based on normality or give an error report. We should understand that
generally such answers are meaningless. In those cases, nonparametric methods
(Chapter 12) such as the Wilcoxon rank sum method or bootstrap methods
(Chapter 13) will be more appropriate. For more discussion, refer to Section 14.4.1.
EXAMPLE 5.5.7
The following is a random data from a normal population.
7:2 5:7 4:9 6:2 8:5 2:8
Construct a 95% confidence interval for the population mean m. Interpret.
Solution
The first step is to calculate mean and standard deviation of the sample. We compute as the mean
x ¼ 5:883 and standard deviation, s¼1.959. For 5 degrees of freedom, and for a¼0.05, from the
t-table, t0.025¼2.571. Hence, a 95% confidence interval for m is
xta=2,n1
2ﬃﬃﬃn
p , x + ta=2,n1
2ﬃﬃﬃn
p


¼
5:8832:571 1:959
ﬃﬃﬃ
6
p


,5:5883 + 2:571 1:959
ﬃﬃﬃ
6
p




¼ 3:827, 7:939
ð
Þ
:
This can be interpreted as that we are 95% confident that the true mean m will be between 3.827
and 7.939.
276
CHAPTER 5 Statistical Estimation

EXAMPLE 5.5.8
The scores of a random sample of 16 people who took the TOEFL (Test of English as a Foreign
Language) had a mean of 540 and a standard deviation of 50. Construct a 95% confidence interval
for the population mean m of the TOEFL score, assuming that the scores are normally distributed.
Solution
Because n¼16 is small, using Theorem 6.3.1 with degrees of freedom 15, a 95% confidence interval
for m is
x
ta=2,n1
sﬃﬃﬃn
p ¼ 540
2:131
50ﬃﬃﬃﬃﬃ
16
p


:
So the 95% confidence interval for the population mean m of the TOEFL scores is (513.36,
566.64).
A Dobson unit is the most basic measure used in ozone research. The unit is named
after G.M.B. Dobson, one of the first scientists to investigate atmospheric ozone
(between 1920 and 1960). He designed the Dobson spectrometer—the standard
instrument used to measure ozone from the ground. The data in Example 5.5.9 rep-
resent the total ozone levels at randomly selected points on the earth (represented by
the pair (Latitude, Longitude)) on a particular day.
EXAMPLE 5.5.9
The following data represent the total ozone levels measured in Dobson units at randomly selected
locations of earth on a particular day.
269 246 388 354 266 303
295 259 274 249 271 254
Can we say that the data are approximately normally distributed? Construct a 95% confidence
interval for the population mean m of ozone levels on this day.
Solution
The following is the probability plot of these data created using Minitab.
200
300
400
ML Estimates
Mean : 285.667
Std Dev: 42.0086
99
95
90
80
70
60
50
Percent
40
30
20
10
5
1
Data
Normal probability plot for ozone data
Continued
277
5.5 One Sample Confidence Intervals

Because all the data values lie within the bounds on the normal probability plot (see the
discussion in Section 3.2.4), we can assume that the data have approximate normality. We have
x ¼ 285:7 and s¼43.9. Also n¼12. For a¼0.05, t0.025,11¼2.201. A 95% confidence interval
for m is
x
ta=2, n1
ð
Þ
sﬃﬃﬃn
p ¼ 285:7
2:201 43:9
ﬃﬃﬃﬃﬃ
12
p


:
Hence, a 95% confidence interval for m, the average ozone level over the earth, lies in (257.81,
313.59).
EXERCISES 5.5
5.5.1. A survey indicates that it is important to pay attention to truth in political
advertising. Based on a survey of 1200 people, 35% indicated that they
found political advertisements to be untrue; 60% say that they will not vote
for candidates whose advertisements are judged to be untrue; and of this
latter group, only 15% ever complained to the media or to the candidate
about their dissatisfaction.
(a) Find a 95% confidence interval for the percentage of people who find
political advertising to be untrue.
(b) Find a 95% confidence interval for the percentage of voters who will not
vote for candidates whose advertisements are considered to be untrue.
(c) Find a 95% confidence interval for the percentage of those who avoid
voting for candidates whose advertisements are considered untrue and
who have complained to the media or to the candidate about the
falsehood in commercials.
(d) For each case above, interpret the results and state any assumptions you
have made.
5.5.2. Many mutual funds use an investment approach involving owning stocks
whose price/earnings multiples (P/Es) are less than the P/E of the S&P
500. The following data give P/Es of 49 companies a randomly selected
mutual fund owns in a particular year.
6:8
5:6
8:5
8:5
8:4
7:5
9:3
9:4
7:8
7:1
9:9
9:6
9:0
9:4 13:7 16:6
9:1 10:1 10:6 11:1
8:9 11:7 12:8 11:5 12:0 10:6 11:1
6:4 12:3 12:3
11:4
9:9 14:3 11:5 11:8 13:3 12:8 13:7 13:9 12:9
14:2 14:0 15:5 16:9 18:0 17:9 21:8 18:4 34:3
Find a 98% confidence interval for the mean P/E multiples. Interpret the
result and state any assumptions you have made.
5.5.3. Let X1, . . ., Xn be a random sample from N(m, s2) distribution, s2 known.
(a) Show that ^m ¼ X is a MLE of the population mean m.
(b) Show that
P X  2sﬃﬃﬃn
p < m < X + 2sﬃﬃﬃn
p


¼ 0:954:
278
CHAPTER 5 Statistical Estimation

(c) Let
P X  ksﬃﬃﬃn
p < m < X + ksﬃﬃﬃn
p


¼ 0:90:
Find k.
5.5.4. Let the observed mean of a sample of size 45 be x ¼ 68:51 from a
distribution having variance 110. Find a 95% confidence interval for the
true mean m and interpret the result and state any assumptions you
have made.
5.5.5. In a random sample of 50 college seniors, 18 indicated that they were
planning to pursue a graduate degree. Find a 98% confidence interval
for the true proportion of all college seniors planning to pursue a
graduate degree, and interpret the result, and state any assumptions you
have made.
5.5.6. DVD players coming off an assembly line are automatically checked to
make sure they are not defective. The manufacturer wants an interval
estimate of the percentage of DVD players that fail the testing procedure.
Compute a 90% confidence interval, based on a random sample of size 105
in which 17 DVD players failed the testing procedure. Also, interpret the
result and state any assumptions you have made.
5.5.7. Studies have shown that the risk of developing coronary disease increases
with the level of obesity, or accumulation of body fat. A study was
conducted on the effect of exercise on losing weight. Fifty men who
exercised lost an average of 11.4 lb, with a standard deviation of 4.5 lb.
Construct a 95% confidence interval for the mean weight loss through
exercise. Interpret the result and state any assumptions you have made.
5.5.8. Basing findings on 60 successful pregnancies involving natural birth, an
experimenter found that the mean pregnancy term was 274 days, with a
standard deviation of 14 days. Construct a 99% confidence interval for the
true mean pregnancy term m.
5.5.9. Let Y be the binomial random variable with parameter p and n¼400. If the
observed value of Y is y¼120, find a 95% confidence interval for p.
5.5.10. For a health screening in a large company, the diastolic and systolic blood
pressures of all the employees were recorded. In a random sample of 150
employees, 12 were found to suffer from hypertension. Find 95% and 98%
confidence intervals for the proportion of the employees of this company
with hypertension.
5.5.11. In a random sample of 500 items from a large lot of manufactured items,
there were 40 defectives.
(a) Find a 90% confidence interval for the true proportion of defectives in
the lot.
(b) Is the assumption of normal approximation valid?
(c) Suppose we suspect that another lot has the same proportion of
defectives as in the first lot. What should be the sample size if we want
to estimate the true proportion within 0.01 with 90% confidence?
279
5.5 One Sample Confidence Intervals

5.5.12. Pesticide concentrations in sediment from irrigation areas can provide
information required to assess exposure and fate of these chemicals in
freshwater ecosystems and their likely impacts to the marine environment.
In a study (Jochen et al., 2000), 103 sediment samples were collected from
irrigation channels and drains in 11 agricultural areas of Queensland. In 74
of these samples, they detected DDTs with concentration levels up to
840 ngg1 dw. Obtain a 95% confidence interval for the proportion of total
number of sediments with detectable DDTs.
5.5.13. Let X be the mean of a random sample of size n from an N(m, 16)
distribution. Find n such that p X 2 < m < X + 2


¼ 0:95:
5.5.14. Let X be a Poisson random variable with parameter l. A sample of 150
observations from this population has a mean equal to 2.5. Construct a
98% confidence interval for l.
5.5.15. An opinion poll conducted in March of 1996 by a newspaper (Tampa
Tribune) among eligible voters with a sample size 425 showed that the
president, who was seeking reelection, had 45% support. Give a 95% and a
98% confidence interval for the proportion of support for the president.
5.5.16. A random sample of 100 households located in a large city recorded the
number of people living in the household, Y, and the monthly
expenditure for food, X. The following summary statistics are given.
X
100
i¼1
Yi ¼ 340
X
100
i¼1
Y2
i ¼ 1650
X
100
i¼1
Xi ¼ 40,000
X
100
i¼1
X2
i ¼ 44,000,000
:
(a) Form a 95% confidence interval for the mean number of people living
in a household in this city.
(b) Form a 95% confidence interval for the mean monthly food expenses.
(c) For each case just given, interpret the results and state any assumptions
you have made.
5.5.17. Let X1, . . ., Xn be a random sample from an exponential distribution with
parameter y. A sample of 350 observations from this population has a mean
equal to 3.75. Construct a 90% confidence interval for y.
5.5.18. Suppose a coin is tossed 100 times in order to estimate p¼p (Head). It is
observed that head appeared 60 times. Find a 95% confidence interval for p.
5.5.19. Suppose the population is women at least 35 years of age who are pregnant
with a fetus affected by Down syndrome. We are interested in testing
positive on a noninvasive screening test for fetuses affected by Down
280
CHAPTER 5 Statistical Estimation

syndrome in women at least 35 years of age. In an experiment, suppose 52
of 60 women tested positive. Obtain a 95% confidence interval for the true
proportion of women at least 35 years of age who are pregnant with a fetus
affected by Down syndrome who will receive positive test results from this
procedure.
5.5.20. (a) Let X1, . . ., Xn be a random sample from a Poisson distribution with
parameter l. Derive a (1a) 100% large sample confidence interval
for l.
(b) To date nodes in a phylogenetic tree, the mean path length (MPL) is
used in estimating the relative age of a node. The following data
represent the MPL for 39 nodes (source: TomBritton, Bengt Oxelman,
Annika Vinnersten, and Ka˚re Bremer, “Phylogenetic dating with
confidence intervals using mean path-lengths”). Assume that the data
(given in centimeters) follow a Poisson distribution with parameter l.
65:2 47:0 38:2 13:5 18:0 25:6 16:3 14:0 23:2 18:8
7:5 13:3 11:0 54:9 22:0 50:1 32:6 26:0 13:0
9:0
7:2
4:7
4:5 41:1 45:8 37:0
8:5 30:5 29:3 13:8
7:7
5:5 24:1 12:5 22:3 19:0
9:5
4:7
3:0
Obtain a 95% confidence interval for l and interpret.
5.5.21. A person plans to start an Internet service provider in a large city. The plan
requires an estimate of the average number of minutes of Internet use of a
household in a week. How many households must be (randomly) sampled
to be 95% sure that the sample mean is within 15 minutes of the population
mean? Assume that a pilot study estimated the value of s¼35 minutes.
5.5.22. The fruit fly Drosophila melanogaster normally has a gray color. However,
because of mutation a good portion of them are black. A biologist eager to
learn about the effect of mutation wants to collect a random sample to
estimate the proportion of black fruit flies of this type within 1% error with
95% confidence.
(a) How many individual flies should the researcher capture if there is no
information on the population proportion of black flies?
(b) Suppose the researcher has the initial estimate that 25% of the fruit fly
Drosophila melanogaster have been affected by this mutation. What is
the sample size?
5.5.23. In a pharmacological experiment, 35 lab rats were not given water for
11 hours and were then permitted access to water for 1 hour. The
amounts of water consumed (mL/hour) are given in the following table.
10:6 13:3 15:5 10:7
9:6 12:1 11:8 10:9
9:9 13:2
9:3 11:7
9:9 13:0 12:3 11:0 13:1 11:0 12:5 13:9
14:1 14:8 15:1 12:8 14:0
7:1 14:1 12:7
9:6 12:5
9:0 12:7 13:6 12:5 12:6
Obtain a 98% confidence interval for the mean amount of water
consumed.
281
5.5 One Sample Confidence Intervals

5.5.24. In sociology, a social network is defined as the people you make frequent
contact with, say through Facebook. The personal network size for each
adult in a random sample of 3000 adults was calculated. The sample had a
mean personal network size of 190 with a known population standard
deviation of 25. Find a 95% confidence interval for the mean personal
network size of all adults in order to see if we have normal amount of friends
in our network.
5.5.25. (a) How does the t-distribution compare with the normal distribution?
(b) How does the difference affect the size of confidence intervals
constructed using z (normal approximation) relative to those
constructed using the t-distribution?
(c) Does sample size make a difference?
(d) What assumptions do we need to make in using the t-distribution for
the construction of a confidence interval?
5.5.26. Use the t-table to determine the values of ta/2 that would be used in the
construction of a confidence interval for a population mean in each of
the following cases:
(a) a¼0.99, n¼20
(b) a¼0.95, n¼18
(c) a¼0.90, n¼25
5.5.27. Let X1, . . ., Xn be a random sample from a normal population. A particular
realization resulted in a sample mean of 20 with the sample standard
deviation 4. Construct a 95% confidence interval for m when:
(a) n¼5, (b) n¼10, and (c) n¼25. What happens to the length of the
confidence interval as n changes?
5.5.28. In a large university, the following are the ages of 20 randomly chosen
employees:
24 31 28 43 28 56 48 39 52 32
38 49 51 49 62 33 41 58 63 56
Assuming that the data came from a normal population, construct a 95%
confidence interval for the population mean m of the ages of the employees
of this university. Interpret your answer.
5.5.29. A random sample of size 26 is drawn from a population having a normal
distribution. The sample mean and the sample standard deviation from the
data are given, respectively, as x ¼ 2:22 and s¼1.67. Construct a 98%
confidence interval for the population mean m and interpret.
5.5.30. A drug is suspected of causing an elevated heart rate in a certain group of
high-risk patients. Twenty patients from the group were given the drug. The
changes in heart rates were found to be as follows.
1 8 5 10
2 12
7
9 1 3
4 6 4 12 11
2 1 10 2 8
Construct a 98% confidence interval for the mean change in heart rate.
Assume that the population has a normal distribution. Interpret your answer.
282
CHAPTER 5 Statistical Estimation

5.5.31. Ten bearings made by a certain process have a mean diameter of 0.905 cm
with a standard deviation of 0.0050 cm. Assuming that the data may be
viewed as a random sample from a normal population, construct a 95%
confidence interval for the actual average diameter of bearings made by
this process and interpret.
5.5.32. Air pollution in large US cities is monitored to see whether it conforms to
requirements set by the Environmental Protection Agency. The following
data, expressed as an air pollution index, give the air quality of a city for 10
randomly selected days.
57:3 58:1 58:7 66:7 58:6 61:9 59:0 64:4 62:6 64:9
Assuming that the data may be looked upon as a random sample from a
normal population, construct a 95% confidence interval for the actual
average air pollution index for this city and interpret.
5.5.33. In order to find out the average hemoglobin (Hb) level in children with
chronic diarrhea, a random sample of 10 children with chronic diarrhea
is selected from a city and their Hb levels (g/dL) are obtained as follows:
12:3 11:4 14:2 15:3 14:8 13:8 11:1 15:1 15:8 13:2
Assuming that the data may be looked upon as a random sample from a
normal population, construct a 99% confidence interval for the actual
average Hb level in children with chronic diarrhea for this city and
interpret. Draw a box plot and normal plot for this data, and comment.
5.5.34. Suppose that you need to estimate the mean number of typographical errors
per page in the rough draft of a 400-page book. A careful examination of 10
pages gives an average of 6 errors per page with a standard deviation of 2
errors. Assuming that the data may be looked upon as a random sample
from a normal population, construct a 99% confidence interval for the
actual average number of errors per page in this book and interpret. In this
problem, is the normal model appropriate?
5.5.35. Creatine kinase (CK) is found predominantly in muscle and is released into
the circulation during muscular lesions. Therefore, serum CK activity has
been theoretically expected to be useful as a marker in exercise physiology
and sports medicine for the detection of muscle injury and overwork. The
following data represent the peak CK activity (measured in IU/L) after
90 minutes of exercise in 15 healthy young men. (Source: Manabu Totsuka,
Shigeyuki Nakaji, Katsuhiko Suzuki, Kazuo Sugawara, and Koki Sato,
Break point of serum creatine kinase release after endurance exercise,
http://jap.physiology.org/cgi/content/full/93/4/1280.)
1112 722 689 251 196 185 128 102 166 178
775 694 514 244 208
Construct a 95% confidence interval for the mean peak CK activity.
283
5.5 One Sample Confidence Intervals

5.5.36. A random sample of 20 observations gave the following summary
statistics: Pxi¼234 and Pxi
2¼3048. Assuming that the data may be
looked upon as a random sample from a normal population, construct a
95% confidence interval for the actual average, m.
5.5.37. Let a random sample of size 17 from a normal population for which both
mean m and variance s2 are unknown yield x ¼ 3:12 and s2¼1.04.
Determine a 99% confidence interval for m.
5.5.38. A random sample from a normal population yields the following 25 values:
90 87 121
96 106 107 89 107 83 92
117 93
98 120
97 109 78
87 99 79
104 85
91 107
89
(a) Calculate an unbiased estimate ^y of the population mean.
(b) Give approximate 99% confidence interval for the population mean.
5.5.39. The following are random data from a normal population.
3:3 3:3 4:7 2:6 6:4 4:7 1:7 4:5 5:0 3:0
Construct a 98% confidence interval for the population mean m.
5.5.40. The following data represent the rates (micrometers per hour) at which a
razor cut made in the skin of anesthetized newts is closed by new cells.
28 20 21 39 32 23 18 31 14 23
18 22 28 24 33 12 23 21 25 25
(a) Can we say that the data are approximately normally distributed?
(b) Find a 95% confidence interval for population mean rate m for the new
cells to close a razor cut made in the skin of anesthetized newts.
(c) Find a 99% confidence interval for m.
(d) Is the 95% CI wider or narrower than the 99% CI? Briefly explain why.
5.5.41. For a particular car, when the brake is applied at 62 mph, the following data
give stopping distance (in feet) for 10 random trials on a dry surface.
(Source: http://www.nhtsa.dot.gov/cars/testing/brakes/b.pdf.)
146:9 148:4 149:4 148:6 150:3
147:5 147:5 149:3 148:4 145:5
(a) Can we say that the data are approximately normally distributed?
(b) Find a 95% confidence interval for population mean stopping distance m.
5.6 A CONFIDENCE INTERVAL FOR THE POPULATION
VARIANCE
In this section we derive a confidence interval for the population variance s2 based
on the chi-square distribution (w2-distribution). Recall that the w2-distribution, like
the Student t-distribution, is indexed by a parameter called the degrees of freedom.
284
CHAPTER 5 Statistical Estimation

However, the w2-distribution is not symmetric and covers positive values only, and
hence it cannot be used to describe a random variable that assumes negative values.
Let X1, . . ., Xn be normally distributed with mean m and variance s2, with both m and
s unknown. We know that
Xn
i¼1 Xi X

2
s2
¼ n1
ð
ÞS2
s2
has a w2-distribution with (n1) degrees of freedom irrespective of s2. Hence it can
be used as a pivot. We now find two numbers wL
2 and wU
2 such that
P w2
L  n1
ð
ÞS2
s2
 w2
U


¼ 1a:
The foregoing inequality can be rewritten as
P
n1
ð
ÞS2
w2
U
 s2  n1
ð
ÞS2
w2
L


¼ 1a:
Hence, a (1a) 100% confidence interval for s2 is given by ((n1)S2/wU
2 , (n1)
S2/wL
2). For convenience, we take the areas to the right of wU
2 ¼wa/2
2
and to the left of
wL
2 ¼w1a/2
2
to be both equal to a/2; see Figure 5.6. Using the chi-square table we can
find the values of wa/2
2
and w1a/2
2
. Then, we have the following result.
Theorem 5.6.1 If X and S are the mean and standard deviation of a random
sample of size n from a normal population, then
P
n1
ð
ÞS2
w2
a=2
 s2  n1
ð
ÞS2
w2
1a=2
 
!
¼ 1a
where the w2-distribution has (n1) degrees of freedom.
That is, we are (1a) 100% confident that the population variance s2 falls in
the interval ((n1)S2/wa/2
2 , (n1)S2/w1a/2
2
).
FIGURE 5.6
Chi-square density with equal area on both sides of the CI.
285
5.6 A Confidence Interval for the Population Variance

EXAMPLE 5.6.1
A random sample of size 21 from a normal population gave a standard deviation of 9. Determine a
90% confidence interval for s2.
Solution
Here n¼21 and s2¼81. From the w2-table with 20 degrees of freedom, w0.05
2
¼31.4104 and
w0.95
2
¼10.8508. Therefore, a 90% confidence interval for s2 is obtained from
n1
ð
ÞS2
w2
a=2
,
n1
ð
ÞS2
w2
1a=2
 
!
:
Thus, we get
20
ð
Þ 81
ð
Þ
31:4104 < s2 < 20
ð
Þ 81
ð
Þ
10:8508
or, we are 90% confident that 51.575<s2<149.298.
We can summarize the steps for obtaining the confidence interval for the true var-
iance as follows.
PROCEDURE TO FIND CONFIDENCE INTERVAL FOR s2
1. Calculate x and s2 from the sample x1, ..., xn.
2. Find wU
2 ¼wa/2
2 , and wL
2 ¼w1a/2
2
using the w2-square table with (n1) degrees of freedom.
3. Compute the (1a) 100% confidence interval for the population variance s2 as
((n1)s2/wa/2
2 , (n1)s2/w1a/2
2
), where w2-values are with (n1) degrees of freedom.
Assumption: The population is normal.
EXAMPLE 5.6.2
The following data represent cholesterol levels (in mg/dL) of 10 randomly selected patients from a
large hospital on a particular day.
360 352 294 160 146 142 318 200 142 116
Determine a 95% confidence interval for s2.
Solution
From the data, we can get x ¼ 223 and standard deviation s¼96.9. The following probability graph
is obtained by Minitab.
286
CHAPTER 5 Statistical Estimation

99
95
90
80
70
60
50
Percent
40
30
20
10
5
0
100
100
200
300
500
400
1
Data
Normal probability plot for cholesterol levels
Even though the scattergram does not appear to follow a straight line, the data are still within
the band, so we can assume approximate normality for the data. (In situations like this, it is more
appropriate to use nonparametric tests explained in Chapter 12.) A box plot of the data shows that
there are no outliers. From the w2-table, w0.025
2
(9)¼19.023 and w0.975
2
(9)¼2.70. Therefore a 90%
confidence interval for s2 is obtained from
n1
ð
ÞS2
w2
a=2 n1
ð
Þ,
n1
ð
ÞS2
w2
1a=2 n1
ð
Þ
 
!
:
Thus, we get
9
ð Þ 96:9
ð
Þ2
19:023
< s2 < 9
ð Þ 96:9
ð
Þ2
2:70
or, we are 95% confident that 4442.3<s2<31,299. Note that the numbers look very large, but it is
the value of variance. By taking the square root of the numbers on the both sides, we can also get a
confidence interval for the standard deviation s.
As remarked in the previous exercise, in general to find a (1a)100% confidence interval for
the true population standard deviation, s, take the square roots of the end points of the confidence
interval of the variance.
EXERCISES 5.6
5.6.1. A random sample of size 20 is drawn from a population having a normal
distribution. The sample mean and the sample standard deviation from the
data are given, respectively, as x ¼ 2:2 and s¼1.42. Construct a 90%
confidence interval for the population variance s2 and interpret.
5.6.2. A drug is suspected of causing an elevated heart rate in a certain group of
high-risk patients. Twenty patients from the group were given the drug. The
changes in heart rates were found to be as follows.
287
5.6 A Confidence Interval for the Population Variance

1 8 5 10
2 12
7
9 1 3
4 6 4 12 11
2 1 10 2 8
Construct a 95% confidence interval for the variance of change in heart
rate. Assume that the population has a normal distribution and interpret.
5.6.3. Air pollution in large US cities is monitored to see whether it conforms to
requirements set by the Environmental Protection Agency. The following
data, expressed as an air pollution index, give the air quality of a city for 10
randomly selected days.
56:23 57:12 57:7
65:80 59:40
62:90 58:00 64:56 63:92 63:45
Assuming that the data may be viewed as a random sample from a
normal population, construct a 99% confidence interval for the actual
variance of the air pollution index for this city and interpret.
5.6.4. A random sample of 25 observations gave the following summary
statistics: Pxi¼234 and Pxi
2¼3048. Assuming that the data can be
looked upon as a random sample from a normal population, construct a
95% confidence interval for the actual variance, s2.
5.6.5. Let a random sample of size 18 from a normal population with both mean m
and variance s2 unknown yield x ¼ 2:27 and s2¼1.02. Determine a 99%
confidence interval for s2.
5.6.6. Suppose we want to study contaminated fish in a river. It is important for
the study to know the size of the variance s2 in the fish weights. The 25
samples of fish in the study produced the following summary statistics: x ¼
1030:5g, and the standard deviation s¼200.6 g. Construct a 95%
confidence interval for the true variation in weights of contaminated fish in
this river.
5.6.7. A random sample from a normal population yields the following 25 values:
90 87 121
96 106 107 89 107 83 92
117 93
98 120
97 109 78
87 99 79
104 85
91 107
89
(a) Calculate an unbiased estimate ^s2 of the population variance.
(b) Give approximate 99% confidence interval for the population variance.
(c) Interpret your results and state any assumptions you made in order to
solve the problem.
5.6.8. It is known that some brands of peanut butter contain impurities within an
acceptable level. A test conducted on randomly selected 12 jars of a certain
brand of peanut butter resulted in the following percentages of impurities:
1:9 2:7 2:1 2:8 2:3 3:6 1:4 1:8 2:1 3:2 2:0
Construct a 95% confidence interval for the average percentage of
impurities in this brand of peanut butter.
288
CHAPTER 5 Statistical Estimation

Giveanapproximate95%confidenceintervalforthepopulationvariance.
Interpret your results and test for normality.
5.6.9. The following data represent the maximal head measurements (across the top
of the skull) in millimeters of 15 Etruscans (inhabitants of ancient Etruria).
152 147 126 140 135 139 149 140
142 147 132 148 146 143 137
Calculate an unbiased estimate ^s2 of the population variance.
Give approximate 95% confidence interval for the population variance.
Interpret your results and test for normality.
5.6.10. A pharmaceutical company tested a new drug to be marketed for the
treatment of a particular type of virus. In order to obtain an estimate on
the mean recovery time, this drug was tested on 15 volunteer patients, and the
recovery time (in days) was recorded. The following data were obtained.
8 17 10
6 34 11 13 6 9 8
19
4 12 17
7
(a) Obtain a 95% confidence interval estimate of the mean recovery.
(b) What assumptions do we need to make? Test for these assumptions.
5.6.11. The rates of return (rounded to the nearest percentage) for 25 clients of a
financial firm are given in the following table.
13 11 28 6 4 15 13 6 11 11
3 12 20 3 16 16 15 8 20 15
4
1 12 2 9
Find a 98% confidence interval for the variance s2 of rates of return. Use
this to find the confidence interval for the population standard deviation, s.
5.6.12. In order to test the precision of a new type of blood sugar monitor for
diabetic patients, 20 randomly selected monitors of this type were used.
A blood sample with 120 mg/dL was tested in each of these monitors, and
the resulting readings are given in the following table.
117 116 121 120 122 117 120 120 118 119
118 123 119 123 119 122 118 122 121 120
(a) Obtain a 99% confidence interval for the variance s2.
(b) Is it reasonable to assume that the data follow a normal distribution?
5.7 CONFIDENCE INTERVAL CONCERNING TWO POPULATION
PARAMETERS
In the earlier sections we studied the confidence limits of true parameters from sam-
ples from single populations. Now, we consider the interval estimation based on sam-
ples from two populations. Our interest is to obtain a confidence interval for the
289
5.7 Confidence Interval Concerning Two Population Parameters

parameters of interest based on two independent samples taken from these two
populations.
Let X11, ..., X1n1 be a random sample from a normal distribution with mean m1
and variance s1
2, and let X21, ..., X2n2 be a random sample from a normal distribution
with mean m2 and variance s2
2, Let X1 ¼ 1=n1
ð
Þ
Xn1
i¼1X1i and X2 ¼ 1=n2
ð
Þ
Xn2
i¼1X2i:
We will assume that the two samples are independent. Then X1 and X2 are indepen-
dent. The distribution of X1 X2 is N(m1m2,(1/n1)s1
2+(1/n2)s2
2). Now as in the one-
sample case, the confidence interval for m1m2 is obtained as follows.
LARGE SAMPLE CONFIDENCE INTERVAL FOR THE DIFFERENCE OF
TWO MEANS
(a) s1, s2 are known. The (1a) 100% large sample confidence interval for m1m2 is given by
X1 X2



za=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
s2
1
n1
+ s2
2
n2


s
:
(b) If s1 and s2 are not known, s1 and s2 can be replaced by the respective sample standard devi-
ations S1 and S2 when ni30, i¼1,2. Thus, we can write
p
X1 X2


za=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
S2
1
n1
+ S2
2
n2


s
 m1 m2
 
 X1 X2


+ za=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
S2
1
n1
+ S2
2
n2


s
!
¼ 1a:
Assumption: The population is normal, and the samples are independent.
EXAMPLE 5.7.1
A study of two kinds of machine failures shows that 58 failures of the first kind took on the average
79.7 minutes to repair with a standard deviation of 18.4 minutes, whereas 71 failures of the second
kind took on average 87.3 minutes to repair with a standard deviation of 19.5 minutes. Find a 99%
confidence interval for the difference between the true average amounts of time it takes to repair
failures of the two kinds of machines.
Solution
Here, n1¼58, n2¼71, x1 ¼79.7, s1¼18.4, x2 ¼87.3, and s2¼19.5. Then the 99% confidence inter-
val for m1m2 is given by
79:787:3
ð
Þ
2:575
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
18:4
ð
Þ2
58
+ 19:5
ð
Þ2
71
s
:
That is, we are 99% certain that m1m2 is located in the interval (16.215, 1.0149). Note that
16.215<m1m2<1.0149 means that more than 90% of the length of this interval is negative.
Thus, we can conclude that m2 dominates m1, that is, m2>m1 more than 90% of the time.
290
CHAPTER 5 Statistical Estimation

In the small sample case, the problem of constructing confidence intervals for the
difference of the means from the two normal populations with unknown variances
can be a difficult one. However, if we assume that the two populations have a com-
mon but unknown variance, say s1
2¼s2
2¼s2, we can obtain an estimate of the var-
iance by pooling the two sample data sets. Define the pooled sample variance Sp
2 as
S2
p ¼
Xn1
i¼1 X1i X1

2 +
Xn2
i¼1 X2i X2

2
n1 + n2 2
¼ n1 1
ð
ÞS2
1 + n2 1
ð
ÞS2
2
n1 + n2 2
:
Now, when the two samples are independent,
T ¼ X1 X2


 m1 m2
ð
Þ
Sp
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n1 + 1
n2
p
has a t-distribution with n1+n22 degrees of freedom. We summarize the CI for
m1m2 below.
SMALL SAMPLE CONFIDENCE INTERVAL FOR THE DIFFERENCE OF TWO
MEANS (s1
2¼s2
2)
The small sample (1a)100% confidence interval for m1m2 is
X1 X2



ta=2, n1 + n22
ð
ÞSp
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n1
+ 1
n2
r
:
Assumption: The samples are independent from two normal populations with equal variances.
EXAMPLE 5.7.2
Independent random samples from two normal populations with equal variances produced the fol-
lowing data.
Sample1 : 1:2 3:1 1:7 2:8 3
Sample2 : 4:2 2:7 3:6 3:9
(a) Calculate the pooled estimate of s2.
(b) Obtain a 90% confidence interval for m1m2.
Solution
(a) We have n1¼5 and n2¼4. Also,
x1 ¼ 2:36, s2
1 ¼ 0:733,
x2 ¼ 3:6,
s2
2 ¼ 0:42:
Hence,
s2
p ¼ n1 1
ð
Þs2
1 + n2 1
ð
Þs2
2
n1 + n2 2
¼ 0:5989:
Continued
291
5.7 Confidence Interval Concerning Two Population Parameters

(b) For the confidence coefficient 0.90, a¼0.10 and from the t-table, t0.05,7¼1.895. Thus, a 90%
confidence interval for m1m2 is
X1 X2



ta=2, n1 + n22
ð
Þsp
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n1
+ 1
n2
r
¼ 2:363:6
ð
Þ
1:895
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:5989 1
5 + 1
4


s
¼ 1:24
0:98 ¼ 2:22, 0:26
ð
Þ:
Here, m2 dominates m1 uniformly. Note that we can decrease the confidence range 2.22 to
0.26, by increasing n1 and n2 with 1a¼0.90 to remain the same. This means that we are closing
on the unknown true value of m1m2.
In the small sample case, if the equality of the variances cannot be reasonably
assumed, that is s1
26¼s2
2, we can still use the previous procedure, except that we
use the following degrees of freedom in obtaining the t-value from the table. Let
v ¼
s2
1
n1 + s2
2
n2

2
s2
1
n1
 2
n1 1 +
s2
2
n2
 2
n2 1
:
The number given in this formula is always rounded down for the degrees of free-
dom. Hence, in this case, a small sample (1a) 100% confidence interval for m1m2
is given by
X1 X2



ta=2,v
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
S2
1
n1
+ S2
2
n2
s
,
where the t-distribution has v degrees of freedom as given previously.
EXAMPLE 5.7.3
Assuming that two populations are normally distributed with unknown and unequal variances. Two
independent samples are taken with the following summary statistics:
n1 ¼ 16 x1 ¼ 20:17 s1 ¼ 4:3
n2 ¼ 11 x2 ¼ 19:23 s2 ¼ 3:8
Construct a 95% confidence interval for m1m2.
Solution
First let us compute the degrees of freedom,
v ¼
s2
1
n1 + s2
2
n2

2
s2
2
n2
 2=n2 1


+
s2
2
n2
 2=n2 1

 ¼
4:3
ð
Þ2
16
+ 3:8
ð
Þ2
11


4:3
ð
Þ2
16

2=15


+
3:8
ð
Þ2
11

2=110

 ¼ 23:312:
Hence, v¼23, and t0.025,23¼2.069.
Now a 95% confidence interval for m1m2 is
292
CHAPTER 5 Statistical Estimation

x1 x2
ð
Þ
ta=2,v
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
s2
1
n1
+ s2
2
n2
s
¼ 20:1719:23
ð
Þ

 2:069
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
4:3
ð
Þ2
16
+ 3:8
ð
Þ2
11
s
,
which gives the 95% confidence interval as
2:3106 < m1 m2 < 4:1906:
In a real-world problem, how do we determine if s1
2¼s2
2, or s1
26¼s2
2, so that we can
select one of the two methods just given? In Chapter 14, we discuss a procedure that
determines the homogeneity of the variances (i.e. whether s1
2¼s2
2). For the time
being a good indication is to look at the point estimators of s1
2 and s2
2, namely, S1
2
and S2
2. If the point estimators are fairly close to each other, then we can select
s1
2¼s2
2. Otherwise, s1
26¼s2
2. For a more general method of testing for equality of var-
iances, we refer to Section 14.4.3.
We now give a procedure for a large sample confidence interval for the difference
of the true proportions, p1p2, in two binomial distributed populations.
LARGE SAMPLE CONFIDENCE INTERVAL FOR p1p2
The
(1a)100%
large
sample
confidence
interval
for
p1p2
is
given
by
^p1  ^p2
ð
Þ
za=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p1 1^p1
ð
Þ
n1
+ ^p2 1^p2
ð
Þ
n2


r
,
where ^p1 and ^p2 are the points estimators of p1 and p2. This approximation is applicable if ^pini 
5, i ¼ 1,2 and 1 ^pi
ð
Þni  5, i ¼ 1,2: The two samples are independent.
EXAMPLE 5.7.4
Iron deficiency, the most common nutritional deficiency worldwide, has negative effects on work
capacity and on motor and mental development. In a 1999-2000 survey by the National Health
and Nutrition Examination Survey (NHANES), iron deficiency was detected in 58 of 573 white,
non-Hispanic females (10% rounded to whole number) and 95 of 498 (19% rounded to whole
number) black, non-Hispanic females (source: http://www.cdc.gov/mmwr/preview/mmwrhtml/
mm5140a1.htm). Let p1 be the proportion of black, non-Hispanic females with iron deficiency
and let p2 be the proportion of black, non-Hispanic females with iron deficiency. Obtain a 95%
confidence interval for p1p2.
Solution
Here, n1¼573 and n2¼498. Also, ^p1 ¼ 58
573 ¼ 0:10122  0:1, and ^p2 ¼ 95
498 ¼ 0:1907  0:19: For
a¼0.05, z0.015¼1.96. Hence, a 95% confidence interval for p1p2 is
^p1  ^p2
ð
Þ
za=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p1 1 ^p1
ð
Þ
n1
+ ^p2 1 ^p2
ð
Þ
n2


s
¼ 0:10:19
ð
Þ
 1:96
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:1
ð
Þ 0:9
ð
Þ
573
+ 0:19
ð
Þ 0:81
ð
Þ
498
r
¼ 0:13232, 0:047685
ð
Þ:
Continued
293
5.7 Confidence Interval Concerning Two Population Parameters

Here, the true difference of p1p2 is located in the negative portion of the real line, which tells
us that the true proportion of black, non-Hispanic females with iron deficiency is larger than the
proportion of white, non-Hispanic females with iron deficiency.
There are situations in applied problems that make it necessary to study and compare
the true variances of two independent normal distributions. For this purpose, we will
find a confidence interval for the ratio s1
2/s2
2 using the F-distribution. Let X1, ..., Xn1
and Y1, ..., Yn2 be independent samples of size n1 and n2 from two normal distri-
butions N(m1,s1
2) and N(m2,s2
2), respectively. Let S1
2 and S2
2 be the variances of the
two random samples. The confidence interval for the ratio s1
2/s2
2 is given as follows.
A (1a)100% CONFIDENCE INTERVAL FOR s1
2/s2
2
A (1a)100% confidence interval for s1
2/s2
2 is given by
S2
1
S2
2


1
Fn11,n21,1a=2


,
S2
1
S2
2


1
Fn11,n21, a=2
ð
Þ




:
That is,
P
S2
1
S2
2


1
Fn11,n21,1a=2


 s2
1
s2
2

S2
1
S2
2


1
Fn11,n21, a=2
ð
Þ




¼ 1a
:
Assumptions: This two populations are normal, and the sample are independent.
Note that we can also write a (1a)100% confidence interval for s1
2/s2
2 in the form
S2
1
S2
2


1
Fn11,n21,1a=2


,
S2
1
S2
2


Fn21,n11,1a=2


:
The following example illustrates how to find the confidence interval for s1
2/s2
2.
EXAMPLE 5.7.5
Assuming that two populations are normally distributed, two independent random samples are taken
with the following summary statistics:
n1 ¼ 21 x1 ¼ 20:17 s1 ¼ 4:3
n2 ¼ 16 x2 ¼ 19:23 s2 ¼ 3:8
Construct a 95% confidence interval for s1
2/s2
2.
Solution
Here, n1¼21, n2¼16, and a¼0.05. Using the F-table, we have
Fn11, n21, 1a=2 ¼ F 20, 15, 0:975
ð
Þ ¼ 2:76
and
Fn21, n11, 1a=2 ¼ F 15, 20, 0:975
ð
Þ ¼ 2:57:
A 95% confidence interval for s1
2/s2
2 is
294
CHAPTER 5 Statistical Estimation

S2
1
S2
2


1
Fn11,n21,1a=2


,
S2
1
S2
2


Fn21,n11,1a=2


¼
4:3
ð
Þ2
3:8
ð
Þ2
 
!
1
2:76


,
4:3
ð
Þ2
3:8
ð
Þ2
 
!
2:57
ð
Þ
 
!
¼ 0:46394, 3:2908
ð
Þ:
That is, we are 95% confident that the ratio of true variance, s1
2/s2
2, is located in the interval that
implies a 95% confidence interval (0.46394, 3.2908).
EXERCISES 5.7
5.7.1. A study was conducted to compare two different procedures for assembling
components. Both procedures were implemented and run for a month to
allow employees to learn each procedure. Then each was observed for
10 days with the following results. Values are number of components
assembled per day.
Procedure I 115 101 113 64 104
97 114 96 87 93
Procedure II 86
99 100 78
97 111 102 94 88 99
Construct a 98% confidence interval for the difference in the mean
number of components assembled by the two methods. Assume that the
data for each procedure are from approximately normal populations with a
common variance. Interpret the result.
5.7.2. A study was conducted to see the differences between oxygen consumption
rates for male runners from a college who had been trained by two different
methods, one involving continuous training for a period of time each day
and the other involving intermittent training of about the same overall
duration. The means, standard deviations, and sample sizes are shown in
the following table.
Continuous training n1 ¼ 15 x1 ¼ 46:28 s1 ¼ 6:3
Intermittent training n2 ¼ 7 x2 ¼ 42:34 s2 ¼ 7:8
If the measurements are assumed to come from normally distributed
populations with equal variances, estimate the difference between the
population means, with confidence coefficient 0.95, and interpret.
5.7.3. Studies have shown that the risk of developing coronary disease increases
with the level of obesity. A study comparing two methods of losing weight:
diet alone and exercise alone were conducted on 82 men over 1-year period.
Forty-two men dieted and lost an average of 16.0 lb over the year, with a
standard deviation of 5.6 lb. Forty-five men who exercised lost an average
of 10.6 lb, with a standard deviation of 7.9 lb. Construct a 99% confidence
interval for the difference in the mean weight loss by these two methods.
State any assumptions you made and interpret the result you obtained.
5.7.4. The following information was obtained from two independent samples
selected from two normally distributed populations with unknown but
equal variances.
295
5.7 Confidence Interval Concerning Two Population Parameters

Sample 1 14 15 12 13
6 14 11 12 17 19 23
Sample 2 16 18 12 20 15 19 15 22 20 18 23 12 20
Construct a 95% confidence interval for the difference between the
population means and interpret.
5.7.5. In the academic year 2001-2002, two random samples of 25 male
professors and 23 female professors from a large university produced a
mean salary for male professors of $58,550 with a standard deviation of
$4000; the mean for female professors was $53,700 with a standard
deviation of 3200. Construct a 90% confidence interval for the difference
between the population mean salaries. Assume that the salaries of male and
female professors are both normally distributed with equal standard
deviations. Interpret the result.
5.7.6. Let the random variables X1 and X2 follow binomial distributions that have
parameters n1¼100, n2¼75, Let x1¼35 and x2¼27 be observed values of
X1 and X2. Let p1 and p2 be the true proportions. Determine an appropriate
95% confidence interval for p1p2.
5.7.7. The following information is obtained from two independent samples
selected from two populations.
n1 ¼ 40 x1 ¼ 28:4 s1 ¼ 4:1
n2 ¼ 32 x2 ¼ 25:6 s2 ¼ 4:5
(a) What is the MLE of m1m2?
(b) Construct a 99% confidence interval for m1m2.
5.7.8. In order to compare the mean hemoglobin (Hb) levels of well-nourished
and undernourished groups of children, random samples from each of
these groups yielded the following summary.
Construct a 95% confidence interval for the true difference of means,
m1m2.
5.7.9. In a certain part of a city, the average price of homes in 2000 was $148,822,
and in 2001 it was $155,908. Suppose these means were based on a random
sample of 100 homes in 1997 and 150 homes in 1998 and that the sample
standard deviations of sale prices were $21,000 for 2000 and $23,000 for
2001. Find a 98% confidence interval for the difference in the two
population means.
5.7.10. Two independent samples from a normal population are taken with the
following summary statistics:
Number of
Children
Sample
Mean
Sample Standard
Deviation
Well nourished
95
11.2
0.9
Undernourished
75
9.8
1.2
296
CHAPTER 5 Statistical Estimation

n1 ¼ 16 x1 ¼ 2:4 s1 ¼ 0:1
n2 ¼ 11 x2 ¼ 2:6 s2 ¼ 0:5
Construct a 95% confidence interval for s1
2/s2
2.
5.7.11. The following information was obtained from two independent samples
selected from two normally distributed populations.
Sample 1
35
36
33
34
27
35
32
33
38
40
44
Sample 2
37
39
33
41
36
40
36
43
41
39
44
33
41
Construct a 90% confidence interval for s1
2/s2
2.
5.7.12. The management of a supermarket wanted to study the spending habits of
its male and female customers. A random sample of 16 male customers
who shopped at this supermarket showed that they spent an average of $55
with a standard deviation of $12. Another random sample of 25 female
customers showed that they spent $85 with a standard deviation of $20.50.
Assuming that the amounts spent at this supermarket by all its male and
female customers were approximately normally distributed, construct a
90% confidence interval for the ratio of variance in spending for males and
females, s1
2/s2
2.
5.7.13. An experiment is conducted comparing the effectiveness of a new method
of teaching algebra for eighth-grade students. Twelve gifted and 12 regular
students are taught using this method. Their scores on a final exam are
shown in the following table.
Average
58
69
55
65
88
52
99
76
45
86
55
79
Gifted
77
86
84
93
77
91
87
95
68
78
74
58
(a) Compute the 95% confidence interval on the difference between the
mean of the students being taught by this new method.
(b) Construct a 95% confidence interval for the ratio of variance in test
scores for regular and gifted students, s1
2/s2
2.
(c) What are the assumptions you made in parts (a) and (b)? Are these
assumptions justified?
5.7.14. Assume that two populations have the same variance s2. If a sample of size
n1 produced a variance S1
2 from population I and a sample of size n2
produced a variance S2
2 from population II, show that the pooled variance
S2
p ¼ n1 1
ð
ÞS2
1 + n2 1
ð
ÞS2
2
n1 + n2 2
,
is an unbiased estimator of s2. Show that (S1
2+S2
2)/2 is also an unbiased esti-
mator of s2. Which of the two estimators would you prefer? Give reasons
for your choice.
297
5.7 Confidence Interval Concerning Two Population Parameters

5.8 CHAPTER SUMMARY
In this chapter we have discussed the basic concepts of estimation, both point
estimation and interval estimation. Two methods of finding point estimators were
described—the method of moments and the method of maximum likelihood. Some
desirable properties of the point estimators that we have discussed are unbiasedness,
and sufficiency. Unbiasedness guards against consistently producing under- or
overestimates of the parameter in repeated sampling. A sufficient estimator is a
“good” estimator of the population parameter y in the sense that it depends on fewer
data values. Later, this chapter discusses the concept of interval estimation. A (1a)
100% confidence interval (CI) for an unknown parameter y is computed from
sample data. The so-called pivotal method is introduced for deriving a confidence
interval. Large sample and small sample confidence intervals are derived for
population mean m. Confidence intervals in the case of two samples are also
discussed. Additionally, confidence intervals for variance and ratio of variances
are derived.
We will now list some of the key definitions introduced in this chapter.
•
Method of moments.
•
Likelihood function.
•
Maximum likelihood equations.
•
Unbiased estimator.
•
Mean square error.
•
Minimum variance unbiased estimator.
•
Sufficient estimator.
•
Jointly sufficient.
•
Upper and lower confidence limits.
•
Confidence coefficient.
•
100 (1a)% confidence interval for y.
•
Interval estimation.
•
Confidence interval.
In this chapter, we have also learned the following important concepts and
procedures.
•
The method of moments procedure.
•
Procedure to find MLE.
•
Procedure to verify.
•
Pivotal method.
•
Procedure to find a confidence interval for y using the pivot.
•
Procedure to find a large sample confidence interval for y.
•
Procedure to find a small sample confidence interval for m.
•
Procedure to find a confidence interval for the population variance s2.
298
CHAPTER 5 Statistical Estimation

•
Large sample confidence interval for the difference of the means.
•
Small sample confidence interval for the difference of two means (s1
2¼s2
2).
•
Small sample confidence interval for the difference of two means (s1
26¼s2
2).
•
Large sample confidence interval for p1p2.
•
A (1a) 100% confidence interval for s1
2/s2
2
5.9 COMPUTER EXAMPLES
5.9.1 EXAMPLES USING R
It should be noted that for the problems where you’re generating random samples
your answers will vary!
EXAMPLE 5.9.1 DESCRIPTIVE POINT ESTIMATES
Generate 50 sample points from an N(4, 4) distribution and find the descriptive statistics. Obtain an
unbiased and sufficient estimate of m.
R Code:
sample¼rnorm(50,4,4);
summary(sample);
sd(sample);
sd(sample)/sqrt(length(sample));
Standard error of the mean
Output:
Your output will be unique since the samples are generated randomly, take notice of standard
error.
Min.
1st Qu. Median Mean 3rd Qu. Max.
-4.292 1.105
4.012
3.865
Notice this is an estimate
we know that the population
mean is 4 as we defined it.
6.478
14.790
4.288085
Standard deviation
0.6064268
Standard error of the mean
EXAMPLE 5.9.2 UNIFORM MAXIMUM LIKELIHOOD
Generate 35 samples from a U(0,5) distribution and using the descriptive statistics command, find
the maximum likelihood estimate for this data.
Solution
We know that for a random sample X1,. . .,Xn from U(0,y) the MLE, ^y ¼ max Xi
ð
Þ ¼ X n
ð Þ, the nth
order statistic. We can use the following steps to obtain the estimate.
R Code:
sample¼runif(35,0,5);
summary(sample);
Continued
299
5.9 Computer Examples

Output:
Your output will be unique since the
samples are generated randomly.
Min.
1st Qu. Median Mean 3rd Qu. Max.
0.1155 1.5710 2.9520 2.7620 4.0920 4.9900
The MLE of the data 4.99
EXAMPLE 5.9.3 CONFIDENCE INTERVAL
Obtain a 95% confidence interval for m using the following data:
Sample (x) : 7.227 5.7383 4.9369 6.238 8.4876 2.7618
This example assumes you’ve stored your data into variable x. Please modify code
appropriately.
R Code:
t.test(x,conf.level¼0.95);
Output:
One Sample t-test
data: x
t¼7.3399, df¼5, p-value¼0.0007365
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
3.832566 7.963967
sample estimates:
Notice the interval
rather than point estimate.
mean of x
5.898267
EXAMPLE 5.9.4 CONFIDENCE INTERVAL
For the following data obtain a 98% confidence interval for m
Sample (x) : 6.8 5.6 8.5 8.5 8.4 7.5 9.3 9.4 7.8 7.1 9.9 9.6 9.0 13.7 9.4 16.6 9.1 10.1 10.6 11.1 8.9
11.7 12.8
11.5 10.6 12.0 11.1 6.4 12.3 12.3 11.4 9.9 15.5 14.3 11.5 13.3 11.8 12.8 13.7 13.9 12.9 14.2 14.0
This example assumes you’ve stored the data into variable x. Please modify your code
appropriately.
R Code:
t.test(x,conf.level¼0.98);
Output:
One Sample t-test
data: x
t¼27.7762, df¼42, p-value<2.2e-16
alternative hypothesis: true mean is not equal to 0
98 percent confidence interval:
9.910598 11.801030
sample estimates:
Notice the interval
rather than point estimate.
mean of x
10.85581
300
CHAPTER 5 Statistical Estimation

EXAMPLE 5.9.5 CONFIDENCE INTERVAL
For the following data, find a 90% confidence interval for m1m2 using the following data.
Sample (x) : 1.2 3.1 1.7 2.8 3.0
Sample (y) : 4.2 2.7 3.6 3.9
This example assumes you’ve stored your data into variables x and y. Please modify your code
appropriately.
R Code:
t.test(x,y,conf.level¼0.90);
Output:
Welch Two Sample t-test
data: x and y
t¼-2.4721, df¼6.996, p-value¼0.04272
alternative hypothesis: true difference in means is not equal to 0
90 percent confidence interval:
-2.1903896 -0.2896104
90% Confidence Interval
sample estimates:
mean of x mean of y
2.36 3.60
5.9.2 MINITAB EXAMPLES
EXAMPLE 5.9.6
Generate 50 sample points from an N(4, 4) distribution and find the descriptive statistics. Obtain an
unbiased and sufficient estimate of m.
Solution
Because we know that the sample mean x is an unbiased and sufficient estimate of the population
mean m, we only need to find the sample mean of the generated data.
Calc>Random Data>Normal . . .>Type 50 in Generate __ rows of data>Store in column(s):
type C1
>type in Mean: 4.0 and in Standard deviation: 2.0>click OK
EXAMPLE 5.9.7
Generate 35 samples from a U(0, 5) distribution and using the descriptive statistics command, find
the maximum likelihood estimate for this data.
Solution
We know that for a random sample X1, . . ., Xn from U(0, y), the MLE, ^y ¼ max(Xi)¼X(n), the nth
order statistic. We can use the following steps to obtain the estimate.
Calc>Random Data>Uniform. . .>Type 35 in Generate __ rows of data>Store in
column(s): type C1>type in Lower end point: 0.0 and in Upper end point: 5.0>
click OK.
301
5.9 Computer Examples

EXAMPLE 5.9.8
(Small Sample): Using Minitab, obtain a 95% confidence interval for m using the following data
7:227 5:7383 4:9369 6:238 8:4876 2:7618
Solution
Use the following commands.
Enter the data in C1. Then
Stat>Basic Statistics>1-sample t. . . , in variables: enter C1, click Confidence
interval, in Level default value is 95, if any other value, enter that value, and click OK.
EXAMPLE 5.9.9
(Large Sample): For the data
6:8
5:6
8:5
8:5
8:4
7:5
9:3
9:4
7:8
7:1
9:9
9:6
9:0 13:7
9:4 16:6
9:1 10:1 10:6 11:1
8:9 11:7
12:8 11:5 10:6 12:0 11:1
6:4 12:3 12:3 11:4
9:9 15:5
14:3 11:5 13:3 11:8 12:8 13:7 13:9 12:9 14:2 14:0
obtain a 98% confidence interval for m.
Solution
Enter the data in C1. Then click
Stat>Basic Statistics>1-Sample Z. . . >, in Variables: type C1>click Confidence
interval, and enter 98 in Level:> enter 5 in Sigma:>OK.
EXAMPLE 5.9.10
For the following data, find a 90% confidence interval for m1m2
Sample 1
1.2
3.1
1.7
2.8
3.0
Sample 2
4.2
2.7
3.6
3.9
Solution
Enter sample 1 in C1 and sample 2 in C2. Then click
Stat>Basic Statistics>2-Sample t. . .>click Sample in different columns>in
First: enter C1 and in Second: enter C2>enter 90 in Confidence Level: (if equality of
variance can be assumed, click Assume equal variances)>OK.
5.9.3 SPSS EXAMPLES
EXAMPLE 5.9.11
Consider the data
66 74 79 80 77 78 65 79 81 69
Using SPSS, obtain a 99% confidence interval for m.
Solution
One easy way to obtain the confidence interval in SPSS is to use the hypothesis testing procedure.
The procedure is as follows: First enter the data in C1. Then click
302
CHAPTER 5 Statistical Estimation

Analyze>Compare Means>One-sample t Test. . ., > Move var00001 to Test
Variable(s), and Click Options. . . , and enter 99 in Confidence interval:, click
Continue, and OK.
Note that the default value is 95%.
5.9.4 SAS EXAMPLES
We will not give the output in this section.
EXAMPLE 5.9.12
The following data give P/E for a particular year of 49 mutual fund companies owned by a randomly
selected mutual fund.
6:8
5:6
8:5
8:5
8:4
7:5
9:3
9:4
7:8
7:1
9:9
9:6
9:0 16:6
9:1 10:1 10:6 11:1
8:9 11:7
12:8 11:5 12:0 10:6 11:1
6:4 11:4
9:9 14:3 11:5
11:8 13:3 13:9 12:9 14:2 14:0 15:5 17:9 21:8 18:4
34:3 13:7 12:3 18:0
9:4 12:3 16:9 12:8 13:7
Find a 98% confidence interval for the mean P/E multiples. Use SAS procedures.
Solution
We could use the following procedure.
DATA peratio;
INPUT patio @@;
DATALINES;
6.8
5.6
8.5
8.5
8.4
7.5
9.3
9.4
7.8
7.1
9.9
9.6
9.0
9.4
13.7
16.6
9.1
10.1
10.6
11.1
8.9
11.7
12.8
11.5
12.0
10.6
11.1
6.4
12.3
12.3
11.4
9.9
14.3
11.5
11.8
13.3
12.8
13.7
13.9
12.9
14.2
14.0
15.5
16.9
18.0
17.9
21.8
18.4
34.3
;
PROC MEANS data¼peratio lclm uclm alpha¼0.02;
var peratio;
RUN;
EXERCISES 5.9
5.9.1. Using any of the software packages (R, Minitab, SPSS, or SAS), obtain
confidence intervals for at least one data set taken from each section of this
chapter.
PROJECTS FOR CHAPTER 5
5A. ASYMPTOTIC PROPERTIES
In general, we do not have a single sample with one estimator of the unknown param-
eter y. Rather, we will have a general formula that defines an estimator for any sam-
ple size. This gives a sequence of estimators of y:
^y ¼ hn X1, ..., Xn
ð
Þ, n ¼ 1, 2, ... ..
303
Projects for Chapter 5

In this case, we can define the following asymptotic properties:
(i) The sequence of estimators ^yn is said to be asymptotically unbiased for y if
bias ^yn


! 0 as n!1.
(ii) Suppose
^yn


and ^yn are two sequences of estimators that are asymptotically
unbiased for y. The asymptotic relative efficiency of ^yn to ^yn is defined by
lim
n
Var ^yn


Var ^yn
ð
Þ :
(a) Show that ^yn is asymptotically unbiased if and only if
E ^yn


! yas n ! 1:
(b) Let X1, . . ., Xn be a random sample from a distribution with unknown mean
m and variance s2. It is known that the method of moments estimators for m
and s2 are, respectively, the sample mean X and
S02
n ¼ 1=n
ð
Þ
Xn
i¼1 Xi X

2 ¼
n1
ð
Þ=n
ð
ÞS2
n, where Sn
2 is the sample
variance.
(i) Show that S0n
2 is an asymptotically unbiased estimator of s2.
(ii) Show that the asymptotic relative efficiency of S0n
2 to Sn
2 is 1.
(iii) Show that MSE (S0n
2)<MSE (Sn
2). Thus, (Sn
2) is unbiased but (S0n
2)
has a smaller mean square error. However, it should be noted
that the difference is very small and approaches zero as n
becomes large.
5B. ROBUST ESTIMATION
The estimators derived in this chapter are for particular parameters of a presumed
underlying family of distributions. However, if the choice of the underlying family
of distributions is based on past experience, there is a possibility that the true pop-
ulation will be slightly different from the model used to derive the estimators. For-
mally, a statistical procedure is robust if its behavior is relatively insensitive to
deviations from the assumptions on which it is based. If the behavior of an estimator
is taken as its variance, a given estimator may have minimum variance for the dis-
tribution used, but it may not be very good for the actual distribution. Hence, it is
desirable for the derived estimators to have small variance over a range of distribu-
tions. We call such estimators robust estimators. The following illustrates how the
variance of an estimator can be affected by deviations from the presumed underlying
population model.
Consider estimating the mean of a standard normal distribution. Let X1, . . ., Xn be
a random sample from a standard normal distribution. Suppose the population
304
CHAPTER 5 Statistical Estimation

actually follows a contaminated normal distribution. That is, for 0d1, 100
(1d) % of the observations come from an N(0, 1) distribution and the remaining
100d% of observations come from an N(0, 5) distribution. We already know that the
MVUE of the mean m of an uncontaminated normal distribution is the sample mean.
A less effective alternative would be the sample median.
(a) Conduct a simulation study with sample size n that takes, say, 5000 random
samples of 100 observations each. Find the mean and median. Also find the
sample variance of each. For various values of d, say 0.0, 0.01, 0.05, 0.1, 0.2, 0.3,
and 0.4, create a table of variances of sample mean and sample variance.
Compare the variances as the value of d increases.
(b) The aim of robust estimation is to derive estimators with variance near that of the
sample mean when the distribution is standard normal while having the variance
remain relatively stable as d increases. One such estimator is the a-trimmed
mean. Let 0a0.5, and define k¼[na], where [x] is the greatest integer that is
less than or equal to x. For the ordered sample, discard the k highest and lowest
observations and find the mean of the remaining nk observations. That is, let
X(1)X(2)  <X(n) be the ordered sample, and define
Xa ¼ X 1 + k
ð
Þ  X 2 + k
ð
Þ    X n + k
ð
Þ
n2k
:
For the values of d and the samples in part (a), compute the mean and the 0.05-,
0.1-, 0.25-, and 0.5-trimmed means. Discuss the robustness.
5C. NUMERICAL UNBIASEDNESS AND CONSISTENCY
(a) Run the simulation of a normal experiment with increasing sample size.
Numerically show the unbiased and consistent properties of the sample mean.
Run the experiment at least up until n¼1000.
(b) Repeat the experiment of part (a), now with an exponential distribution.
5D. AVERAGED SQUARED ERRORS (ASES)
Generate 25 samples of size 40 from a normal population with m¼10, and s2¼4. For
each of the 25 samples:
(a) Compute: x, s2 ¼
X40
i¼1 xi x
ð
Þ2
39
, s2
1 ¼
X40
i¼1 xi x
ð
Þ2
40
, and s2
2 ¼
X40
i¼1 xi x
ð
Þ2
41
.
(b) Compute the average squared error (ASE) for each of the estimates s2, s1
2, s2
2 as
follows.
Let Ks2 ¼
XK
i¼1 xi x
ð
Þ2
h
i
=39
h
i
for K¼1, 2, ..., 25; and Ks2 be the sample
variance for the Kth sample. Then, the average squared error is
ASE ¼
X25
i¼1 KS2 s2

2
25
:
305
Projects for Chapter 5

Repeat this procedure for the other two estimators. Compare the three ASEs
and check which has the least ASE.
(c) Repeat (a) and (b) with a sample size of 15.
5E. ALTERNATE METHOD OF ESTIMATING THE MEAN AND VARIANCE
(a) Consider the following alternative method of estimating m and s2. We sample
sequentially, and at each stage we compute the estimates of m and s2 as follows.
Let X1, . . ., Xn, Xn+1 be the sample values.
Compute
Xn ¼
Xn
i¼1Xi
n
, Xn + 1 ¼
Xn + 1
i¼1 Xi
n + 1
,
S2
n ¼
Xn
i¼1 Xi Xn

2
n1
, and S2
n + 1 ¼
Xn + 1
i¼1 Xi Xn

2
n
:
The sequential procedure is stopped when
S2
n S2
n + 1

  0:01:
This will also determine the sample size.
(b) Compare the sample sizes and estimates in 5D and 5E.
5F. NEWTON-RAPHSON IN ONE DIMENSION
For a given function g(x), suppose we need to solve g(y)¼0. Using the first-order
Taylor expansion, g(y)g(x)+(yx)g0(x), where g0 x
ð Þ ¼ dg
dx, and setting g(y)¼0,
we get y  x g x
ð Þ
g0 x
ð Þ. Thus, starting with an initial guess solution x, the guess is updated
by y using the previous formula. This derivation is the basis for the Newton-Raphson
iterative method for obtaining the solution of g(y)¼0. This is given by
y n + 1
ð
Þ ¼ yn  g yn
ð
Þ
g0 yn
ð
Þ, n  0,
where yn is the value of y at the nth iteration, starting with the initial guess, y0. For a
good approximation of the solution, the choice of y0 is important. The convergence
of this algorithm cannot be guaranteed.
For the MLE, we want to find a solution of
g y
ð Þ ¼ dL
dy ¼ 0,
where L¼L(y) is the likelihood function of the random sample X1, . . ., Xn . An iter-
ative algorithm for finding the MLE can be given by
y n + 1
ð
Þ ¼ yn 
dL
dy yn
ð
Þ
d2L
dy2 yn
ð
Þ
, n  0:
306
CHAPTER 5 Statistical Estimation

Write a computer program to find the MLE of a for a gamma distribution with param-
eters a and b.
5G. THE EMPIRICAL DISTRIBUTION FUNCTION
The estimators in this chapter yield a single real value (point estimate) for each
parameter. In Chapter 6, we will learn about so-called interval estimates. In this pro-
ject, we use an estimation procedure that estimates the whole distribution function, F,
of a random variable X. We now define the empirical distribution.
The empirical distribution function for a random sample X1, . . ., Xn from a dis-
tribution F is the function defined by
Fn x
ð Þ ¼ 1
n# i,1  i  n : Xi  x
f
g:
It can be shown that nFn(x) is a binomial random variable with
E Fn x
ð Þ
½
 ¼ F x
ð Þ and Var Fn x
ð Þ
½
 ¼ 1
nF x
ð Þ 1F x
ð Þ
½
:
Also, by the strong law of large numbers, for each real number x,
lim
n!1 Fn x
ð Þ ¼ F x
ð Þ with probability 1:
One of the tests to determine whether a random sample comes from a specific dis-
tribution is the Kolmogorov-Smirnov (K-S) test. The K-S test is based on the max-
imum distance between the empirical distribution function and the actual cumulative
distribution function of this specific distribution (such as, say, the normal
distribution).
Using the method of Project 4A (or using any statistical software), generate 100
sample points from a normal distribution with mean 2 and variance 9. Graph the
empirical distribution function for this sample. Compare this graph with the graph
of the N(2, 9) distribution.
5H. SIMULATION OF COVERAGE OF THE SMALL CONFIDENCE
INTERVALS FOR m
(a) Generate 25 samples of size 15 from a normal population with m¼10 and s2¼4.
Using a statistical package (such as Minitab), compute the 95% confidence
intervals for each of the samples using the small sample formula. From your
output, determine the proportion of the 25 intervals that cover the true mean
m¼10.
(b) What would you expect if the sample size is increased to 100? Would the width
of the interval increase or decrease? Would you expect more or fewer of these
intervals to contain the true mean 10? Check your answers with actual
computation.
(c) Repeat with 20 samples of size 10.
307
Projects for Chapter 5

5I. CONFIDENCE INTERVALS BASED ON SAMPLING DISTRIBUTIONS
If we want to obtain a (1a) 100% confidence interval for y, begin with an estimator
^y of y and determine its sampling distribution. Now select two probability levels, a1
and a2, so that a¼a1+a2. Generally we let a1¼a2. Take a sample and calculate the
value of ^y, say ^y ¼ k: Now we need to determine the values of the upper and lower
confidence limits. Find a value yL such that
p ^y  k


¼ a1
and yU such that
p ^y  k


¼ a2:
Then a (1a) 100% confidence interval for y will be
yL < y < yU:
(a) Let X1, . . ., Xn be a random sample from U(0,y) distribution. Obtain a (1a)
100% confidence interval for y, using the method of sampling distribution.
(b) Let X have a binomial distribution with parameters n and p. First show that there is
no quantity that satisfies the conditions of a pivotal quantity. Then using the
method of sampling distributions, obtain a (1a) 100% confidence interval for p.
5J. LARGE SAMPLE CONFIDENCE INTERVALS: GENERAL CASE
The method of finding a confidence interval for a parameter y that we described in
this chapter depends on our ability to find the pivotal quantity. We have seen that
such a quantity may not exist. In those cases, the method of sampling distribution
described in the previous project could be used. However, this method can involve
some difficult calculations. For large samples, we can utilize the following proce-
dure, which is based on the asymptotic distribution of MLEs. Under fairly general
conditions, the MLEs have a limiting distribution that is normal. Also, MLEs are
asymptotically efficient. Hence, for a large sample the MLE ^y of y will have approx-
imately normal distribution with mean y. Also, if the Crame´r-Rao lower bound
exists, the limiting variance of ^y will be
s2
^y ¼
1
E
@1nL
@y

2
h
i:
Hence,
Z ¼
^yy
s^y
 N 0, 1
ð
Þ:
Then a large sample (1a) 100% confidence interval is obtained from the probabil-
ity statement
308
CHAPTER 5 Statistical Estimation

P za=2 <
^yy
s^y
< za=2
 
!
 1a:
We summarize the procedure to construct large sample confidence intervals.
1. Determine the MLE, ^y, of y. Also find the MLEs of all other unknown
parameters.
2. Obtain the variance s^y, (if possible directly, otherwise by using the Crame´r-Rao
lower bound).
3. In the expression for s^y, substitute ^y for y. Replace all other unknown parameters
by its MLEs. Let the resulting quantity be denoted by s^y:
4. Now construct a (1a) 100% confidence interval for y from
^yza=2s^q < y < ^y + za=2s^q:
(a) Using the foregoing procedure, show that a large sample (1a) 100%
confidence interval for the parameter p in a binomial distribution based on n
trials is
^pza=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p 1 ^p
ð
Þ
n
r
< p < ^p + za=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p 1 ^p
ð
Þ
n
r
:
(b) Let X1, . . ., Xn be a random sample from a normal population with parameters
m and s2. Derive a large sample confidence interval for s2 using the above
procedure.
(c) Let X1, . . ., Xn be a random sample from a population with a pdf
f x
ð Þ ¼
1
yex=y,
x > 0
0,
otherwise

:
Derive a large sample confidence interval for y.
5K. PREDICTION INTERVAL FOR AN OBSERVATION FROM A
NORMAL POPULATION
In many cases, we may be interested in predicting future observations from a pop-
ulation, rather than making an inference. A (1a) 100% prediction interval for a
future observation X is an interval of the form (XL, XU) such that P-
(XL<X<XU)¼1a. Similarly to confidence intervals, we can also define
one-sided prediction intervals. Assume that the population is normal with known
variance s2. Let X1, . . ., Xn be a random sample from this population. Then the sam-
pling distribution of the difference X X (we use X to denote Xn) is normal with
mean zero and variance s2+s(2/X)¼(1+(1/n))s2. Then a (1a) 100% prediction
interval for X is given by
309
Projects for Chapter 5

X za=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1
n


s2
s
, X + za=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1
n


s2
s
 
!
:
Thus, we are (1a) 100% confident that the next observation, Xn+1, will lie in this
interval. As in confidence intervals, if the sample size is large, replace s by sample
standard deviation s.
In case, where both m and s are not known, and the sample size is small (so that
the
Central
Limit
Theorem
cannot
be
applied),
it
can
be
shown
that
Xn + 1 Xn


= Sn
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1=n
ð
Þ
p



	
has a t-distribution with (n1) degrees of freedom.
Thus, a (1a) 100% prediction interval for Xn+1 is given by
X ta=2,n1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1=n
ð
Þ
ð
ÞS2
q
, X + ta=2,n1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1=n
ð
Þ
ð
ÞS2
q


:
A standard measure of the capacity of lungs to expel air in breathing is called forced
expiratory volume (FEV). The FEV1 is the volume exhaled during the first second of
a forced expiratory maneuver started from the level of total lung capacity. The fol-
lowing data (source: M. Bland, An Introduction to Medical Statistics, Oxford Uni-
versity Press, 1995) represents FEV measurements (in liters) from 57 male medical
students.
4:47
3:10
4:50
4:90
3:50
4:14
4:32
4:80
3:10
4:68
4:47
3:57
2:85
5:10
5:20
4:80
5:10
4:30
4:70
4:08
3:48
4:20
3:70
5:30
4:71
4:10
4:30
3:39
3:69
4:44
5:00
4:50
4:20
4:16
3:70
3:83
3:90
4:47
3:30
5:43
3:42
3:60
3:20
4:56
4:78
3:60
3:96
3:19
2:85
3:04
3:78
3:75
4:05
3:54
4:14
2:98
3:54
Obtain a 95% prediction interval for a future observation Xn+1.
310
CHAPTER 5 Statistical Estimation

CHAPTER
Hypothesis Testing
6
CHAPTER CONTENTS
6.1 Introduction .................................................................................................... 312
6.2 The Neyman-Pearson Lemma ........................................................................... 323
6.3 Likelihood Ratio Tests ..................................................................................... 328
6.4 Hypotheses for a Single Parameter ................................................................... 333
6.5 Testing of Hypotheses for Two Samples ............................................................ 345
6.6 Chapter Summary ............................................................................................ 359
6.7 Computer Examples ......................................................................................... 360
Projects for Chapter 6 ............................................................................................ 368
OBJECTIVE
In this chapter, various methods of testing hypotheses will be discussed.
Jerzy Neyman
(Source: http://www.learn-math.info/history/photos/Neyman.jpeg)
Jerzy Neyman (1894-1981) was a Polish statistician and mathematician who after
spending time in various institutions in Warsaw, Poland, came to the University of
California, Berkeley. He had made far-reaching contributions in hypothesis testing,
confidence intervals, probability theory, and other areas of mathematical statistics.
Mathematical Statistics with Applications in R
Copyright © 2015 Elsevier Inc. All rights reserved.
311

His work with Egon Pearson gave logical foundation and mathematical rigor to the
theory of hypothesis testing. Neyman made a broader impact in statistics throughout
his lifetime.
6.1 INTRODUCTION
Statistics plays an important role in decision making. In statistics, one utilizes
random samples to make inferences about the population from which the samples
were obtained. Statistical inference regarding population parameters takes two
forms: estimation and hypothesis testing, although both may be viewed as different
aspects of the same general problem of arriving at decisions on the basis of observed
data. We already saw several estimation procedures in earlier chapters. Hypothesis
testing is the subject of this chapter. This has an important role in the application of
statistics to real-life problems. Here we utilize the sampled data to make decisions
concerning the unknown distribution of a population or its parameters. Pioneering
work on the explicit formulation as well as the fundamental concepts of the theory
of hypothesis testing is due to J. Neyman and E.S. Pearson.
A statistical hypothesis is a statement concerning the probability distribution of a
random variable or population parameters that are inherent in a probability distribu-
tion. The following example illustrates the concept of hypothesis testing. An impor-
tant industrial problem is that of accepting or rejecting lots of manufactured products.
Before releasing each lot for the consumer, the manufacturer usually performs some
tests to determine whether the lot conforms to acceptable standards. Let us say that
both the manufacturer and the consumer agree that if the proportion of defectives in a
lot is less than or equal to a certain number p, the lot will be released. Very often,
instead of testing every item in the lot, we may test only a few at random from
the lot and make decisions about the proportion of defectives in the lot; that is,
we make the decisions about the population on the basis of sample information. Such
decisions are called statistical decisions. In attempting to reach decisions, it is useful
to make some initial conjectures about the population involved. Such conjectures are
called statistical hypotheses. Sometimes the results from the sample may be mark-
edly different from those expected under the hypothesis. Then we can say that the
observed differences are significant and we would be inclined to reject the initial
hypothesis. The procedures that enable us to decide whether to reject hypotheses
or to determine whether observed samples differ significantly from expected results
are called tests of hypotheses, tests of significance, or rules of decision.
In any hypothesis testing problem, we formulate a null hypothesis and an alter-
native hypothesis such that if we reject the null, then we have to accept the alterna-
tive. The null hypothesis usually is a statement of either the “status quo” or “no
effect.” A guideline for selecting a null hypothesis is that when the objective of
an experiment is to establish a claim, the nullification of the claim should be taken
as the null hypothesis. The experiment is often performed to determine whether the
null hypothesis is false. For example, suppose the prosecution wants to establish that
312
CHAPTER 6 Hypothesis Testing

a certain person is guilty. The null hypothesis would be that the person is innocent
and the alternative would be that the person is guilty. Thus, the claim itself becomes
the alternative hypothesis. Customarily, the alternative hypothesis is the statement
that the experimenter believes to be true. For example, the alternative hypothesis
is the reason a person is arrested (police suspect the person is not innocent). Once
the hypotheses have been stated, appropriate statistical procedures are used to deter-
mine whether to reject the null hypothesis. For the testing procedure, one begins with
the assumption that the null hypothesis is true. If the information furnished by the
sampled data strongly contradicts (beyond a reasonable doubt) the null hypothesis,
then we reject it in favor of the alternative hypothesis. If we do not reject the null,
then we automatically reject the alternative. Note that we always make a decision
with respect to the null hypothesis. Failure to reject the null hypothesis does not nec-
essarily mean that the null hypothesis is true. For example, a person being judged
“not guilty” does not mean the person is innocent. This basically means that there
is not enough evidence to reject the null hypothesis (presumption of innocence)
beyond “a reasonable doubt.”
We summarize the elements of a statistical hypothesis in the following.
THE ELEMENTS OF A STATISTICAL HYPOTHESIS
1. The null hypothesis, denoted by H0, is usually the nullification of a claim. Unless evidence from
the data indicates otherwise, the null hypothesis is assumed to be true.
2. The alternate hypothesis, denoted by Ha (or sometimes denoted by H1), is customarily the claim
itself.
3. The test statistic, denoted by TS, is a function of the sample measurements upon which the
statistical decision, to reject or not reject the null hypothesis, will be based.
4. A rejection region (or a critical region) is the region (denoted by RR) that specifies the values of
the observed test statistic for which the null hypothesis will be rejected. This is the range
of values of the test statistic that corresponds to the rejection of H0 at some fixed level of
significance, a, which will be explained later.
5. Conclusion: If the value of the observed test statistic falls in the rejection region, the null hypoth-
esis is rejected and we will conclude that there is enough evidence to decide that the alternative
hypothesis is true. If the TS do not fall in the rejection region, we conclude that we cannot reject
the null hypothesis.
In practice one may have hypotheses such as H0:m¼m0 against one of the
following alternatives:
Ha : m 6¼ m0,
called atwotailedalternative
or Ha : m < m0,
called alower orleft
ð
Þtailed alternative
or Ha : m > m0,
called anupper orright
ð
Þ tailedalternative:
8
<
:
A test with a lower or upper tailed alternative is called a one-tailed test. One of the
issues in hypothesis testing is the choice of the form of alternative hypothesis. Note
that as we discussed earlier, the null hypothesis is always concerned with the ques-
tion posed—the claim. The alternative hypothesis must reflect the aim of the claim
when in fact we reject the claim; we want to know why we rejected. For example,
313
6.1 Introduction

suppose that a pharmaceutical company claims that drug A is 80% effective (i.e.
p¼0.8). We conduct an experiment, clinical trials to test this claim. Thus, the null
hypothesis is that the claim is true. Now if we don’t want to reject the null hypothesis,
no problem, but if we reject the null hypothesis, we want to know why. Thus, the
alternative must be one tail test, p<0.8, that is, the claim is not true. If we were
to use a two tail test, we wouldn’t know whether the rejection was due because
p<0.8 or p>0.8 in this case is actually part of the null hypothesis. It is important
to note that when using one-sided test in a certain direction, if the consequence of
missing an effect in the other direction is not negligible, it is better to use two-sided
test. Also choosing a one-tailed test after doing a two-tailed test that failed to reject
the null hypothesis is not appropriate. Therefore, choice of the alternative is based
on what happens if we reject the null hypothesis. In an applied hypothesis testing
problem, we can use the following general steps.
GENERAL METHOD FOR HYPOTHESIS TESTING
1. From the (word) problem, determine the appropriate null hypothesis, H0, and the alternative, Ha.
2. Identify the appropriate test statistics and calculate the observed test statistic from the data.
3. Find the rejection region by looking up the critical value in the appropriate table.
4. Draw the conclusion: Reject or fail to reject the null hypothesis, H0.
5. Interpret the results: State in words what the conclusion means to the problem we started with.
It is always necessary to state a null and an alternate hypothesis for every statis-
tical test performed. All possible outcomes should be accounted for by the two
hypotheses. Note that, a critical value is the value that a test statistic must surpass
in order for the null hypothesis to be rejected, and is derived from the level of sig-
nificance a of the test. Thus, the critical values are the boundaries of the rejection
region. It is important to observe that both null and alternate hypotheses are stated
in terms of parameters, not in terms of statistic.
EXAMPLE 6.1.1
In a coin-tossing experiment, let p be the probability of heads. We start with the claim that the coin is
fair, that is, H0:p¼1/2. We test this against one of the following alternatives:
(a) Ha: The coin is not fair (p6¼1/2). This is a two-tailed alternative.
(b) Ha: The coin is biased in favor of heads (p>1/2). This is an upper tailed alternative.
(c) Ha: The coin is biased in favor of tails (p<1/2). This is a lower tailed alternative.
It is important to observe that the test statistic is a function of a random sample.
Thus, the test statistic itself is a random variable whose distribution is known under
the null hypothesis. The value of a test statistic when specific sample values are
substituted is called the observed test statistic or simply test statistic.
For example, consider the hypothesis H0:m¼m0 versus Ha:m6¼m0, where m0 is
known. Assume that the population is normal with a known variance s2. Consider
314
CHAPTER 6 Hypothesis Testing

X, an unbiased estimator of m based on the random sample, X1,...,Xn. Then
Z ¼ X m0


= s=
ﬃﬃﬃn
p
ð
Þ is a function of the random sample X1,...,Xn, and has a known
distribution, say a standard normal, under H0. If x1, x2,...,xn are specific sample
values, then z ¼ xm0
ð
Þ= s=
ﬃﬃﬃn
p
ð
Þ is called the observed sample statistic or simply
sample statistic.
Definition 6.1.1 A hypothesis is said to be a simple hypothesis if that hypothesis
uniquely specifies the distribution from which the sample is taken. Any hypothesis
that is not simple is called a composite hypothesis.
EXAMPLE 6.1.2
Refer to Example 6.1.1. The null hypothesis p¼1/2 is simple, because the hypothesis completely
specifies the distribution, which in this case will be a binomial with p¼1/2 and with n being the
number of tosses. The alternative hypothesis p6¼1/2 is composite because the distribution now is
not completely specified (we do not know the exact value of p).
Because the decision is based on the sample information, we are prone to commit
errors. In a statistical test, it is impossible to establish the truth of a hypothesis with
100% certainty. There are two possible types of errors. On the one hand, one can
make an error by rejecting H0 when in fact it is true. On the other hand, one can also
make an error by failing to reject the null hypothesis when in fact it is false. Because
the errors arise as a result of wrong decisions, and the decisions themselves are based
on random samples, it follows that the errors have probabilities associated with them.
We now have the following definitions.
The decision and the errors are represented in Table 6.1.
Definition 6.1.2 (a) A type I error is made if H0 is rejected when in fact H0 is
true. The probability of type I error is denoted by a. That is,
P rejectingH0jH0istrue
ð
Þ ¼ a:
The probability of type I error, a, is called the level of significance.
(b) A type II error is made if H0 is accepted when in fact Ha is true. The probability
of a type II error is denoted by b. That is,
P notrejectingH0jH0isfalse
ð
Þ ¼ b:
It is desirable that a test should have a¼b¼0 (this can be achieved only in trivial
cases), or at least we prefer to use a test that minimizes both types of errors.
Table 6.1 Statistical Decision and Error Probabilities
Statistical Decision
True State of Null Hypothesis
H0 True
H0 False
Do not reject H0
Correct decision
Type II error (b)
Reject H0
Type I error (a)
Correct decision
315
6.1 Introduction

Unfortunately, it so happens that for a fixed sample size, as a decrease, b tends to
increase and vice versa. There are no hard and fast rules that can be used to make
the choice of a and b. This decision must be made for each problem based on quality
and economic considerations. However, in many situations it is possible to determine
which of the two errors is more serious. It should be noted that a type II error is only
an error in the sense that a chance to correctly reject the null hypothesis was lost. It is
not an error in the sense that an incorrect conclusion was drawn, because no conclu-
sion is made when the null hypothesis is not rejected. In the case of type I error, a
conclusion is drawn that the null hypothesis is false when, in fact, it is true. There-
fore, type I errors are generally considered more serious than type II errors. For
example, it is mostly agreed that finding an innocent person guilty is a more serious
error than finding a guilty person innocent. Here, the null hypothesis is that the per-
son is innocent, and the alternate hypothesis is that the person is guilty. “Not rejecting
the null hypothesis” is equivalent to acquitting a defendant. It does not prove that the
null hypothesis is true, or that the defendant is innocent. In statistical testing, the sig-
nificance level a is the probability of wrongly rejecting the null hypothesis when it is
true (i.e. the risk of finding an innocent person guilty). Here the type II risk is acquit-
ting a guilty defendant. The usual approach to hypothesis testing is to find a test pro-
cedure that limits a, the probability of type I error, to an acceptable level while trying
to lower b as much as possible.
The consequences of different types of errors are, in general, very different. For
example, if a doctor tests for the presence of a certain illness, incorrectly diagnosing
the presence of the disease (type I error) will cause a waste of resources, not to
mention the mental agony to the patient. On the other hand, failure to determine
the presence of the disease (type II error) can lead to a serious health risk.
To formulate a hypothesis testing problem, consider the following situation. Sup-
pose a toy store chain claims that at least 80% of girls under 8 years old prefer dolls
over other types of toys. We feel that this claim is inflated. In an attempt to dispose of
this claim, we observe the buying pattern of 20 randomly selected girls under 8 years
old, and we observe X, the number of girls under 8 years old who buy stuffed toys or
dolls. Now the question is, how can we use X to confirm or reject the store’s claim? Let
p be the probability that a girl under 8 chosen at random prefers stuffed toys or dolls.
The question now can be reformulated as a hypothesis testing problem. Is p0.8 or
p<0.8? Because we would like to reject the store’s claim only if we are highly cer-
tain of our decision, we should choose the null hypothesis to be H0:p0.8, the rejec-
tion of which is considered to be more serious. The null hypothesis should be H0:
p0.8, and the alternative Ha:p<0.8. In order to make the null hypothesis simple,
we will use H0:p¼0.8, which is the boundary value with the understanding that it
really represents H0:p0.8. We note that X, the number of girls under 8 years old
who prefer stuffed toys or dolls, is a binomial random variable. Clearly a large sam-
ple value of X would favor H0. Suppose we arbitrarily choose to accept the null
hypothesis if X>12. Because our decision is based on only a sample of 20 girls under
8, there is always a possibility of making errors whether we accept or reject the store
chain’s claim. In the following example, we will now formally state this problem and
calculate the error probabilities based on our decision rule.
316
CHAPTER 6 Hypothesis Testing

EXAMPLE 6.1.3
A toy store chain claims that at least 80% of girls under 8 years old prefer dolls over other
types of toys. After observing the buying pattern of many girls under 8 years old, we feel that
this claim is inflated. In an attempt to dispose of this claim, we observe the buying pattern of
20 randomly selected girls under 8 years old, and we observe X, the number of girls who buy
stuffed toys or dolls. We wish to test the hypothesis H0:p¼0.8 against Ha:p<0.8. Suppose we
decide to accept the H0 if X>12 (i.e. X13). This means that if {X12} (i.e. X<13) we will
reject H0.
(a) Find a.
(b) Find b for p¼0.6.
(c) Find b for p¼0.4.
(d) Find the rejection region of the form {XK} so that (i) a¼0.01; (ii) a¼0.05.
(e) For the alternative Ha:p¼0.6, find b for the values of a in part (d).
Solution
The TS X is the number of girls under 8 years old who buy dolls. X follows the binomial distribution
with n¼20 and p, the unknown population proportion of girls under 8 who prefer dolls. We now
calculate a and b.
(a) For p¼0.8, the probability of type I error is
a¼ P reject H0 H0 is true
j
f
g
¼ P X  12 p ¼ 0:8
j
f
g
¼
X
12
x¼0
20
x


0:8
ð
Þx 0:2
ð
Þ20x
¼ 0:0321:
If we calculate a for any other value of p>0.8, then we will find that it is smaller than 0.0321.
Hence, there is at most a 3.21% chance of rejecting a true null hypothesis. That is, if the
store’s claim is in fact true, then the chance that our test will erroneously reject that claim
is at most 3.21%.
(b) Here p¼0.6. The probability of type II error is
b¼P accept H0 H0 false
j
f
g
¼P X > 12 p ¼ 0:6
j
f
g
¼1P X  12jp ¼ 0:6
f
g
¼10:584
¼0:416:
so there is a 41.6% chance of accepting a false null hypothesis. Thus, in casethe store’s claim is not
true, and the truth is that only 60% of girls under 8 years old prefer dolls over other types of toys,
then there is a 41.6% chance that our test will erroneously conclude that the store’s claim is true.
(c) If p¼0.4, then
b ¼ P accept H0jH0 false
f
g
¼ P X > 12jp ¼ 0:4
f
g
¼ 1P X  12jp ¼ 0:4
f
g
¼ 10:979
¼ 0:021:
That is, there is a 2.1% chance of not rejecting a false null hypothesis.
Continued
317
6.1 Introduction

(d) (i) To find K such that
a ¼ P X  Kjp ¼ 0:8
f
g ¼ 0:01
from the binomial table, K¼11. Hence, the rejection region is: Reject H0 if {X11}.
(ii) To find K such that
a ¼ P X  Kjp ¼ 0:8
f
g ¼ 0:05
from the binomial table, a¼0.05 falls between K¼12 and K¼13. However, for K¼13, the value
for a is 0.087, exceeding 0.05. If we want to limit a to be no more than 0.05, we will have to take
K¼12. That is, we reject the null hypothesis if X12, yielding an a¼0.0321 as shown in (a).
(e) (i) When a¼0.01, from (d), the rejection region is of the form {X11}. For p¼0.6,
b ¼ P accept H0jH0 false
f
g
¼ P Y > 11jp ¼ 0:6
f
g
¼ 1P Y  11jp ¼ 0:6
f
g
¼ 10:404
¼ 0:596:
(ii) From (a) and (b) for testing the hypothesis H0:p¼0.8 against Ha:p<0.8 with n¼20. We see
that when a is 0.0321, b is 0.416. From (d)(i) and (e)(i) for the same hypothesis, we see that
when a is 0.01, b is 0.596. This holds in general. Thus, we observe that for fixed n as a
decrease, b increases and vice versa.
In the next example, we explore what happens to b as the sample size increases,
with a fixed.
EXAMPLE 6.1.4
Let X be a binomial random variable. We wish to test the hypothesis H0:p¼0.8 against Ha:p¼0.6.
Let a¼0.03 be fixed. Find b for n¼10 and n¼20.
Solution
For n¼10, using the binomial tables, we obtain P{X5jp¼0.8}ﬃ0.03. Hence the rejection region
for the hypothesis H0:p¼0.8 versus Ha:p¼0.6 is given by reject H0 if X5. The probability of type
II error is
b ¼ P acceptH0jp ¼ 0:6
f
g,
b ¼ P X > 5jp ¼ 0:6
f
g ¼ 1P X  5jp ¼ 0:6
f
g ¼ 0:733:
For n¼20, as shown in Example 7.1.3, if we reject H0 for X12, we obtain
P X  12jp ¼ 0:8
ð
Þ ﬃ0:03
and
b ¼ P X > 12jp ¼ 0:6
ð
Þ ¼ 1P X  12jp ¼ 0:6
f
g ¼ 0:416:
We see that for a fixed a, as n increases b decreases and vice versa. It can be shown that this result
holds in general.
In order for us to compute the value of b, it is necessary that the alternate hypoth-
esis is simple. Now we will discuss a three-step procedure to calculate b.
318
CHAPTER 6 Hypothesis Testing

STEPS TO CALCULATE b
1. Decide an appropriate test statistic (usually this is a sufficient statistic or an estimator for the
unknown parameter, whose distribution is known under H0).
2. Determine the rejection region using a given a, and the distribution of the test statistic (TS).
3. Find the probability that the observed test statistic does not fall in the rejection region assuming
Ha is true. This gives b. That is,
b ¼ P TSfallsin thecomplementof the rejectionregionjHa is true
ð
Þ:
EXAMPLE 6.1.5
A random sample of size 36 from a population with known variance, s2¼9, yields a sample mean of
x ¼ 17. For the hypothesis H0:m¼15 versus Ha:m>15, find b when m¼16. Assume a¼0.05.
Solution
Here n¼36, x ¼ 17, and s2¼9. In general, to test H0:m¼m0 versus Ha:m>m0, we proceed as
follows. An unbiased estimator of m is X. Intuitively we would reject H0 if X is large, say X > c.
Now using a¼0.05, we will determine the rejection region. By the definition of a, we have
P X > cjm ¼ m0


¼ 0:05 or
P X m0
s=
ﬃﬃﬃn
p > cm0
s=
ﬃﬃﬃn
p jm ¼ m0


¼ 0:05:
But if m¼m0, because the sample size n30,
X m0


= s=
ﬃﬃﬃn
p
ð
Þ


 N 0, 1
ð
Þ:
Therefore,
P X m0
s=
ﬃﬃﬃn
p
> cm0
s=
ﬃﬃﬃn
p


¼ 0:05 is equivalent to P Z > cm0
s=
ﬃﬃﬃn
p


¼ 0:05: From standard normal tables,
we obtain P(Z>1.645)¼0.05. Hence cm0
s=
ﬃﬃﬃn
p ¼ 1:645 or c ¼ m0 + 1:645 s=
ﬃﬃﬃn
p
ð
Þ:
Therefore, the rejection region is the set of all sample means x such that
x > m0 + 1:645
sﬃﬃﬃn
p


:
Substituting m0¼15, and s¼3, we obtain
m0 + 1:645
sﬃﬃﬃn
p


¼ 15 + 1:645
3
36


¼ 15:8225:
The rejection region is the set of x such that x  15:8225.
Then by definition,
b ¼ P X  15:8225, whenm ¼ 16


:
Consequently, for m¼16,
b ¼ P X16
s=
ﬃﬃﬃn
p
 15:822516
3=
ﬃﬃﬃﬃﬃ
36
p
 
!
¼ P Z  0:36
ð
Þ
¼ 0:3594:
That is, under the given information, there is a 35.94% chance of not rejecting a false null
hypothesis.
319
6.1 Introduction

6.1.1 SAMPLE SIZE
It is clear from the preceding example that once we are given the sample size n, an a,
a simple alternative Ha, and a test statistic, we have no control over b and it is exactly
determined. Hence, for a given sample size and test statistic, any effort to lower b
will lead to an increase in a and vice versa. This means that for a test with fixed sam-
ple size it is not possible to simultaneously reduce both a and b. We also notice from
Example 6.1.4 that by increasing the sample size n, we can decrease b (for the same
a) to an acceptable level. The following discussion illustrates that it may be possible
to determine the sample size for a given a and b.
Suppose we want to test H0:m¼m0 versus Ha:m>m0. Given a and b, we want to
find n, the sample size, and K, the point at which the rejection begins. We know that
a ¼ P X > K, when m ¼ m0


¼ P X m0
s=
ﬃﬃﬃn
p > K m0
s=
ﬃﬃﬃn
p , when m ¼ m0


¼ P Z > za
ð
Þ
(6.1)
and for some particular value m¼ma>m0,
b ¼ P X  K, when m ¼ ma


¼ P X ma
s=
ﬃﬃﬃn
p  K ma
s=
ﬃﬃﬃn
p , when m ¼ ma


¼ P z  zb


:
(6.2)
From Equations (6.1) and (6.2),
za ¼ K m0
s=
ﬃﬃﬃn
p
and
zb ¼ K ma
s=
ﬃﬃﬃn
p :
This gives us two equations with two unknowns (K and n), and we can proceed to
solve them. Eliminating K, we get
m0 + za
sﬃﬃﬃn
p


¼ ma zb
sﬃﬃﬃn
p


:
From this we can derive
ﬃﬃﬃn
p ¼ za + zb


s
ma m0
:
Thus, the sample size for an upper tail alternative hypothesis is
n ¼ za + zb

2s2
ma m0
ð
Þ2 :
320
CHAPTER 6 Hypothesis Testing

The sample size increases with the square of the standard deviation and decreases
with the square of the difference between mean value of the alternative hypothesis
and the mean value under the null hypothesis. Note that in real-world problems, care
should be taken in the choice of the value of ma for the alternative hypothesis. It may
be tempting for a researcher to take a large value of ma in order to reduce the required
sample size. This will seriously affect the accuracy (power) of the test. This alterna-
tive value must be realistic within the experiment under study. Care should also be
taken in the choice of the standard deviation s. Using an underestimated value of the
standard deviation to reduce the sample size will result in inaccurate conclusions
similar to overestimating the difference of means. Usually, the value of s is estimated
using a similar study conducted earlier. The problem could be that the previous study
may be old and may not represent the new reality. When accuracy is important, it
may be necessary to conduct a pilot study only to get some idea on the estimate
of s. Once we determine the necessary sample size, we must devise a procedure
by which the appropriate data can be randomly obtained. This aspect of the design
of experiments is discussed in Chapter 9.
EXAMPLE 6.1.6
Let s¼3.1 be the true standard deviation of the population from which a random sample is chosen.
How large should the sample size be for testing H0:m¼5 versus Ha:m¼5.5, in order that a¼0.01 and
b¼0.05?
Solution
We are given m0¼5 and ma¼5.5. Also, za¼z0.01¼2.33 and zb¼z0.05¼1.645. Hence, the sample size
n ¼ za + zb

2s2
ma m0
ð
Þ2 ¼ 2:33 + 1:645
ð
Þ2 3:1
ð
Þ2
0:5
ð
Þ2
¼ 607:37:
So, n¼608 will provide the desired levels. That is, in order for us to test the foregoing hypothesis, we
must randomly select 608 observations from the given population.
From a practical standpoint, the researcher typically chooses a and the sample
size, b is ignored. Because a trade-off exists between a and b, choosing a very small
value of a will tend to increase b in a serious way. A general rule of thumb is to pick
reasonable values of a, possibly in the 0.05-0.10 range so that b will remain
reasonably small.
EXERCISES 6.1
6.1.1. An appliance manufacturer is considering the purchase of a new machine for
stamping out sheet metal parts. If m0 (given) is the true average of the number
of good parts stamped out per hour by their old machine and m is the
corresponding true unknown average for the new machine, the manufacturer
wants to test the null hypothesis m¼m0 versus a suitable alternative. What
321
6.1 Introduction

should the alternative be if he does not want to buy the new machine unless it
is (a) more productive than the old one? (b) At least 20% more productive
than the old one?
6.1.2. Formulate an alternative hypothesis for each of the following null
hypotheses.
(a) H0: Support for a presidential candidate is unchanged after the start of the
use of TV commercials.
(b) H0: The proportion of viewers watching a particular local news channel
is less than 30%.
(c) H0: The median grade point average of undergraduate mathematics
majors is 2.9.
6.1.3. It is suspected that a coin is not balanced (not fair). Let p be the probability of
tossing a head. To test H0:p¼0.5 against the alternative hypothesis Ha:
p>0.5, a coin is tossed 15 times. Let Y equal the number of times a head is
observed in the 15 tosses of this coin. Assume the rejection region to be
{Y10}.
(a) Find a.
(b) Find b for p¼0.7.
(c) Find b for p¼0.6.
(d) Find the rejection region for {YK} for a¼0.01, and a¼0.03.
(e) For the alternative Ha:p¼0.7, find b for the values of a given in (d).
6.1.4. In Exercise 6.1.3:
(a) Assume that the rejection region is {Y8}. Calculate a and b if p¼0.6.
Compare the results with the corresponding values obtained in
Exercise 6.1.3. (This gives the effect of enlarging the rejection region
on a and b.)
(b) Assume that the rejection region is {Y8}. Calculate a and b if p¼0.6
and (i) the coin is tossed 20 times, or (ii) the coin is tossed 25 times.
(This shows the effect of increasing the sample size on a and b for a
fixed rejection region.)
6.1.5. Suppose we have a random sample of size 25 from a normal population with
an unknown mean m and a standard deviation of 4. We wish to test the
hypothesis H0:m¼10 versus Ha:m>10. Let the rejection region be defined
by: reject H0 if the sample mean X > 11:2:
(a) Find a.
(b) Find b for Ha:m¼11.
(c) What should the sample size be if a¼0.01 and b¼0.2?
6.1.6. A process for making steel pipe is under control if the diameter of the
pipe has mean 3.0 in. with standard deviation of no more than 0.0250 in.
To check whether the process is under control, a random sample of size
n¼30 is taken each day and the null hypothesis m¼3.0 is rejected if X is
less than 2.9960 or greater than 3.0040. Find (a) the probability of type I
error; (b) the probability of type II error when m¼3.0050 in. Assume
s¼0.0250 in.
322
CHAPTER 6 Hypothesis Testing

6.1.7. A bowl contains 20 balls, of which x are green and the remainder red. To test
H0:x¼10 versus Ha:x¼15, three balls are selected at random without
replacement, and H0 is rejected if all three balls are green. Calculate a and b
for this test.
6.1.8. Suppose we have a sample of size 6 from a population with probability
density function (pdf) f(x)¼(1/y)ex/y, x>0, y>0. We wish to test
H0:y¼1 versus Ha:y>1. Let the rejection region be defined by reject H0 if
P
i¼1
6 Xi>8. (a) Find a. (b) Find b for Ha:y¼2.
6.1.9. Let s2¼16 be the variance of a normal population from which a random
sample is chosen. How large should the sample size be for testing H0:m¼25
versus Ha:m¼24, in order that a¼0.05 and b¼0.05?
6.2 THE NEYMAN-PEARSON LEMMA
In practical hypothesis testing situations, there are typically many tests possible with
significance level a for a null hypothesis versus alternative hypothesis (see Project
7A). This leads to some important questions, such as (1) how to decide on the test
statistic and (2) how to know that we selected the best rejection region. In this sec-
tion, we study the answer to these questions using the Neyman-Pearson approach.
Definition 6.2.1 Suppose that W is the test statistic and RR is the rejection region
for a test of hypothesis concerning the value of a parameter y. Then the power of the
test is the probability that the test rejects H0 when the alternative is true. That is,
p ¼ Power y
ð Þ
¼ P W in RR when the parameter value is an alternative y
ð
Þ:
If H0:y¼y0 and Ha:y6¼y0, then the power of the test at some y¼y16¼y0 is
Power y1
ð
Þ ¼ P rejectH0 y ¼ y1
j
ð
Þ:
But, b(y1)¼P(accept H0jy¼y1). Therefore,
Power y1
ð
Þ ¼ 1b y1
ð
Þ:
In otherwords, power refers to the probability that the test will find a statistically
significant difference when such a difference actually exists. A good test will have
high power. In statistical tests, it is generally accepted that power should be 0.8
or greater.
Note that the power of a test H0 cannot be found until some true situation Ha is
specified. That is, the sampling distribution of the test statistic when Ha is true must
be known or assumed. Because b depends on the alternative hypothesis, which being
composite most of the time does not specify the distribution of the test statistic, it is
important to observe that the experimenter cannot control b. For example, the alter-
native Ha:m<m0 does not specify the value of m, as in the case of the null hypothesis,
H0:m¼m0.
323
6.2 The Neyman-Pearson Lemma

EXAMPLE 6.2.1
Let X1, . . ., Xn be a random sample from a Poisson distribution with parameter l, that is, the pdf is
given by f(x)¼ellx/(x!). Then the hypothesis H0:l¼1 uniquely specifies the distribution, because
f(x)¼e1/(x!) and hence is a simple hypothesis. The hypothesis Ha:l>1 is composite, because f(x)
is not uniquely determined.
Definition 6.2.2 A test at a given a of a simple hypothesis H0 versus the simple
alternative Ha that has the largest power among tests with the probability of type I
error no larger than the given a is called a most powerful test.
Consider the test of hypothesis H0:y¼y0 versus Ha:y¼y1. If a is fixed, then our
interest is to make b as small as possible. Because b¼1Power(y1), by minimizing
b we would obtain a most powerful test. The following result says that among all tests
with given probability of type I error, the likelihood ratio test given later minimizes
the probability of a type II error, in other words, it is most powerful.
Theorem 6.2.1 (Neyman-Pearson Lemma) Suppose that one wants to test a
simple hypothesis H0:y¼y0 versus the simple alternative hypothesis Ha:y¼y1 based
on a random sample X1, . . ., Xn from a distribution with parameter y. Let L(y)L(y;
X1, . . ., Xn)>0 denote the likelihood of the sample when the value of the parameter
is y. If there exist a positive constant K and a subset C of the sample space ℝn
(the Euclidean n-space) such that
1. L y0
ð
Þ
L y1
ð
Þ  K, for x1, x2, ..., xn
ð
Þ 2 C,
2. L y0
ð
Þ
L y1
ð
Þ  K, for x1, x2, ..., xn
ð
Þ 2 C0, where C0 is the complement of C, and
3. P[(X1,. . .,Xn)2C;y0] ¼a
Then the test with critical region C will be the most powerful test for H0 versus Ha. We
call a the size of the test and C the best critical region of size a.
Proof. We prove this theorem for continuous random variables. For discrete ran-
dom variables, the proof is identical with sums replacing the integral. Let S be some
region in ℝn, an n-dimensional Euclidean space. For simplicity we will use the fol-
lowing notation:
ð
S
L y
ð Þ ¼
ð
S
...
ð
S
L y; x1, x2, ..., xn
ð
Þdx1dx2, ...,dxn
Note that
P X1, ..., Xn
ð
Þ 2 C;y0
ð
Þ ¼
ð
C
f x1, ..., xn; y0
ð
Þdx1, ...,dxn
¼
ð
C
L y0; x1, ..., xn
ð
Þdx1, ...,dxn:
Suppose that there is another critical region, say B, of size less than or equal to a, that
is
Ð
BL(y0)a. Then
324
CHAPTER 6 Hypothesis Testing

0 
ð
C
L y0
ð
Þ
ð
B
L y0
ð
Þ, because
ð
C
L y0
ð
Þ ¼ a by assumption3:
Therefore,
0 
ð
C
L y0
ð
Þ
ð
B
L y0
ð
Þ
¼
ð
C\B
L y0
ð
Þ +
ð
C\B0L y0
ð
Þ
ð
C\B
L y0
ð
Þ
ð
C0\B
L y0
ð
Þ
¼
ð
C\B0L y0
ð
Þ
ð
C0\B
L y0
ð
Þ:
Using assumption 1 of Theorem 6.2.1, KL(y1)L(y0) at each point in the region C
and hence in C\B0. Thus
ð
C\B
L y0
ð
Þ  K
ð
C\B0L y1
ð
Þ:
By assumption 2 of the theorem, KL(y1)L(y0) at each point in C0, and hence in
C0 \B. Thus,
ð
C0\B
L y0
ð
ÞK 
ð
C0\B
L y1
ð
Þ:
Therefore,
0 
ð
C\B0L y0
ð
Þ
ð
C0\B
L y0
ð
Þ
 K
ð
C\B0L y1
ð
Þ
ð
C0\B
L y1
ð
Þ
	

:
That is,
0  K
ð
C\B
L y1
ð
Þ +
ð
C\B0L y1
ð
Þ
ð
C\B
L y1
ð
Þ
ð
C\B
L y1
ð
Þ
	

¼ K
ð
C
L y1
ð
Þ
ð
B
L y1
ð
Þ
	

:
As a result,
ð
C
L y1
ð
Þ 
ð
B
L y1
ð
Þ
Because this is true for every critical region B of size a, C is the best critical region
of size a, and the test with critical region C is the most powerful test of size a.
When testing two simple hypotheses, the existence of a best critical region is
guaranteed by the Neyman-Pearson lemma. In addition, the foregoing theorem pro-
vides a means for determining what the best critical region is. However, it is impor-
tant to note that Theorem 6.2.1 gives only the form of the rejection region; the actual
rejection region depends on the specific value of a.
In real-world situations, we are seldom presented with the problem of testing two
simple hypotheses. There is no general result in the form of Theorem 6.4.1 for
325
6.2 The Neyman-Pearson Lemma

composite hypotheses. However, for hypotheses of the form H0:y¼y0 versus
Ha:y>y0, we can take a particular value y1>y0 and then find a most powerful test
for H0:y¼y0 versus Ha:y>y1. If this test (i.e. the rejection region of the test) does not
depend on the particular value y1, then this test is said to be a uniformly most powerful
(UMP) test for H0:y¼y0 versus Ha:y>y0.
The following example illustrates the use of the Neyman-Pearson lemma.
EXAMPLE 6.2.2
Let X1, . . ., Xn denote an independent random sample from a population with a Poisson distribution
with mean l. Derive the most powerful test for testing H0:l¼2 versus Ha:l¼1/2.
Solution
Recall that the pdf of Poisson variable is
p x
ð Þ ¼
ellx
x!
, l > 0, x ¼ 0,1,2, ...
0,
otherwise:
(
Thus, the likelihood function is
L ¼
l
Pn
i¼1xi
ð
Þeln
h
i
Yn
i¼1 xi!
ð
Þ
:
For l¼2,
L y0
ð
Þ ¼ L l ¼ 2
ð
Þ ¼
2
Pn
i¼1xi
ð
Þe2n
h
i
Yn
i¼1 xi!
ð
Þ
:
and for l¼1/2,
L y1
ð
Þ ¼ L l ¼ 1
2


¼
1
2
Pn
i¼1xi
ð
Þe 1=2
ð
Þn
h
i
Yn
i¼1 xi!
ð
Þ
:
Thus,
L y0
ð
Þ
L y1
ð
Þ ¼
2
P
xi
ð
Þe2n


1
2
P
xien=2
< K
which implies
4
ð Þ
P
xi e3n=2


< K
or, taking natural logarithm,
X
xi


ln 43n
2 < lnK:
Solving for Pxi
ð
Þ and letting {[ln K+(3n/2)]/ln 4}¼K0, we will reject H0 whenever Pxi
ð
Þ<K0.
326
CHAPTER 6 Hypothesis Testing

A step-by-step procedure in applying the Neyman-Pearson lemma is now given.
PROCEDURE FOR APPLYING THE NEYMAN-PEARSON LEMMA
1. Determine the likelihood functions under both null and alternative hypotheses.
2. Take the ratio of the two likelihood functions to be less than a constant K.
3. Simplify the inequality in step 2 to obtain a rejection region.
EXAMPLE 6.2.3
Suppose X1, . . ., Xn is a random sample from a normal distribution with a known mean of m and an
unknown variance of s2. Find the most powerful a-level test for testing H0:s2¼s0
2 versus
Ha:s2¼s1
2, (s1
2>s0
2). Show that this test is equivalent to the w2-test. Is the test UMP for
Ha:s2>s0
2?
Solution
To test H0:s2¼s0
2 versus Ha:s2>s1
2. We have
L s2
0


¼
Y
n
i¼1
1
ﬃﬃﬃﬃﬃﬃ
2p
p
sn
0
e xim
ð
Þ2
2s2
0
¼
1
ﬃﬃﬃﬃﬃﬃ
2p
p

nsn
0
e
P
xim
ð
Þ2
2sn
0
:
Similarly,
L s2
1


¼
1
ﬃﬃﬃﬃﬃﬃ
2p
p

nsn
1
e
P
xim
ð
Þ2
2s2
1
:
Therefore, the most powerful test is, reject H0 if,
L s2
0


L s2
1

 ¼
s2
1
s2
0

n
e

s2
1s2
0
ð
Þ
2
2s2
1s2
0
P
xim
ð
Þ2


 K
for some K.
Taking the natural logarithms, we have
n ln
s1
s0


 s2
1 s2
0


2s2
1s2
0
X
xi m
ð
Þ2  lnK
or
X
xi m
ð
Þ2  n ln
s1
s0


 lnK


2s2
1s2
0
s2
1 s2
0


C:
To find the rejection region for a fixed value of a, write the region as
X
xi m
ð
Þ2
s2
0
 C
s2
0
¼ C0:
Note that by Theorem 4.2.7, P(xim)2/s0
2 has a w2-distribution with n degrees of freedom. Thus,
this test is equivalent to the w2-test. Under the H0, because the same rejection region (does not
depend upon the specific value of s1
2 in the alternative) would be used for any s1
2>s0
2, the test
is UMP.
327
6.2 The Neyman-Pearson Lemma

The foregoing example shows that, in order to test for variance using a sample
from a normal distribution, we could use the chi-square table to obtain the critical
value for the rejection region given a.
EXERCISES 6.2
6.2.1. Suppose X1, . . ., Xn is a random sample from a normal distribution with a
known variance of s2 and an unknown mean of m. Find the most powerful
a-level test of H0:m¼m0 versus Ha:m¼ma if (a) m0>ma, and (b) ma>m0.
6.2.2. Show that the most powerful test obtained in Example 6.2.1 is UMP for
testing H0:mm0 versus Ha:m>ma, but not UMP for testing H0:m¼m0
versus Ha:m6¼m0.
6.2.3. Suppose X1, . . ., Xn is a random sample from a U(0, y) distribution. Find the
most powerful a-level test for testing H0:y¼y0 versus Ha:y¼y1, where
y0<y1.
6.2.4. Let X1, . . ., Xn be a random sample from a geometric distribution with
parameter p. Find the most powerful test of H0:p¼p0 versus Ha:p¼pa (>p0).
Is this UMP test for H0:p¼p0 versus Ha:p>p0?
6.2.5. Let X1, . . ., Xn be a random sample from a distribution having a pdf of
f y
ð Þ ¼
2y
n2 ey2=n2 if x > 0
0, otherwise:
8
<
:
Find a UMP test for testing H0:¼0 versus Ha:<0.
6.2.6. Let X be a single observation from the pdf
f x
ð Þ ¼
yxy1, 0 < x < 1
0, otherwise:
	
Find the most powerful test with a level of significance a¼0.01 to test
H0:y¼3 versus Ha:y¼4.
6.2.7. Let X1, . . ., Xn be a random sample from a Bernoulli distribution with
parameter p. Find the most powerful test of H0:p¼p0 versus Ha:p¼pa, where
pa>p0.
6.2.8. Let X1, . . ., Xn be a random sample from a Poisson distribution with mean l.
Find a best critical region for testing H0:l¼3 against Ha:l¼6.
6.3 LIKELIHOOD RATIO TESTS
The Neyman-Pearson lemma provides a method for constructing most powerful tests
for simple hypotheses. We also have seen that in some instances when a hypothesis is
not simple, it is possible to find UMP tests. In general, UMP tests do not exist for
composite hypotheses. As an example, consider the two-sided hypothesis, at level
a, given by
H0 : m ¼ m0 versus Ha : m 6¼ m0
328
CHAPTER 6 Hypothesis Testing

where m is the mean of a normal population with known variance s2. If X is the
sample mean of a random sample of size n, then as shown earlier, we can use the
test statistic
Z ¼ X m0
s=
ﬃﬃﬃn
p :
For Ha:m¼m1>m0, the rejection region for the most powerful test would be
Reject H0 if z > za:
On the other hand for Ha:m¼m2<m0, the rejection region for the most powerful test
would be
Reject H0 if z < za:
Thus, the rejection region depends on the specific alternative. Consequently, the
two-sided hypothesis just given has no UMP test.
In this section, we shall study a general procedure that is applicable when one or
both H0 and Ha are composite. In fact, this procedure works for simple hypotheses as
well. This method is based on the maximum likelihood estimation and the ratio of
likelihood functions used in the Neyman-Pearson lemma. We assume that the pdf
or probability mass function (pmf) of the random variable X is f(x,y), where y
can be one or more unknown parameters. Let Y represent the total parameter space
that is the set of all possible values of the parameter y given by either H0 or Ha.
Consider the hypotheses
H0 : y 2 Y0 versus Ha : y 2 Ya ¼ YY0:
where y is the unknown population parameter (or parameters) with values in Y, and
Y0 is a subset of Y.
Let L(y) be the likelihood function based on the sample X1, . . ., Xn. Now we
define the likelihood ratio corresponding to the hypotheses H0 and Ha. This ratio will
be used as a test statistic for the testing procedure that we develop in this section. This
is a natural generalization of the ratio test used in the Neyman-Pearson lemma when
both hypotheses were simple.
Definition 6.3.1 The likelihood ratio l is the ratio
l ¼
max
y2Y0 L y; x1, ..., xn
ð
Þ
max
y2Y L y; x1, ..., xn
ð
Þ ¼ L
0
L:
We note that 0l1. Because l is the ratio of nonnegative functions, we have l0.
Because Y0 is a subset of Y, we know that max
y2Y0L y
ð Þ  max
y2Y L y
ð Þ. Hence, l1.
If the maximum of L in Y0 is much smaller as compared with the maximum of L
in Y, that is, if l is small, it would appear that the data X1, . . ., Xn do not support the
null hypothesis y2Y0. Thus, there are some parameter values in Ha for which
observed sample more likely came from than for any parameter values in H0. On
the other hand, if l is close to 1, one could conclude that the data support the null
329
6.3 Likelihood Ratio Tests

hypothesis, H0. Therefore, small values of l would result in rejection of the
null hypothesis, and large values nearer to 1 will result a decision in support of
the null hypothesis.
For the evaluation of l, it is important to note that maxy2YL y
ð Þ ¼ L ^yml:


, where
^yml: is the maximum likelihood estimator of y2Y, and maxy2Y0L y
ð Þ is the likeli-
hood function with unknown parameters replaced by their maximum likelihood esti-
mators subject to the condition that y2Y0. We can summarize the likelihood ratio
test as follows.
LIKELIHOOD RATIO TESTS
To test
H0 : y 2 Y0 versus Ha : y 2 Ya,
l ¼
max
y2Y0 L y; x1, ..., xn
ð
Þ
max
y2Y L y; x1, ..., xn
ð
Þ ¼ L
0
L
will be used as the test statistic.
The rejection region for the likelihood ratio test is given by
Reject H0 if l  K:
K is selected such that the test has the given significance level a.
Note that different choices of K2[0,1] will give different tests and rejection
regions. Smaller values of K will result in smaller values of Type I error probabilities
and the larger values of K will result in smaller Type II error probabilities.
EXAMPLE 6.3.1
Let X1, . . ., Xn be a random sample from an N(m, s2). Assume that s2 is known. At level a, we wish to
test, H0:m¼m0 versus Ha:m6¼m0. Find an appropriate likelihood ratio test.
Solution
We have seen that to test
H0 : m ¼ m0 versus Ha : m 6¼ m0
there is no UMP test. The likelihood function is
L m
ð Þ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
s

n
e
Pn
i¼1 xim
ð
Þ2
2s2
:
Here, Y0¼{m0} and Ya¼ℝ{m0}.
Hence,
L
0 ¼ max
m¼m0
1
s
ﬃﬃﬃﬃﬃﬃ
2p
p

n
e
Pn
i¼1 xim
ð
Þ2
2s2
¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
s

n
e
Pn
i¼1 xim0
ð
Þ2
2s2
:
Similarly,
L* ¼
max
1<m<1
1ﬃﬃﬃﬃﬃﬃ
2p
p
s

n
e
Pn
i¼1 xim
ð
Þ2
2s2
:
330
CHAPTER 6 Hypothesis Testing

Because the only unknown parameter in the parameter space Y is m, 1<m<1, the maxi-
mum of the likelihood function is achieved when m equals its maximum likelihood estimator, that is,
^mml ¼ X:
Therefore, with a simple calculation we have
l ¼ e Pn
i¼1 xim0
ð
Þ2
ð
Þ=2s2
e Pn
i¼1 xix
ð
Þ2
ð
Þ=2s2 ¼ en xm0
ð
Þ2=2s2:
Thus, the likelihood ratio test has the rejection region
Reject H0ifl  K
which is equivalent to
 n
2s2 X m0

2  lnK ,
Xm0

2
s2=n
 2 lnK ,
Xm0
s=
ﬃﬃﬃn
p

  2 lnK ¼ c1, say:
Note that we use the symbol , to mean “if and only if.” We now compute c1. Under H0,
Xm0


= s=
ﬃﬃﬃn
p
ð
Þ


 N 0, 1
ð
Þ:
Observe that
a ¼ P
X m0
s=
ﬃﬃﬃn
p

  c1
	

This gives a possible value of c1 as c1¼za/2. Hence, likelihood ratio test (LRT) for the given hypoth-
esis is
Reject H0 if X m0
s=
ﬃﬃﬃn
p

  za=2:
Thus, in this case, the LRT is equivalent to the z-test for large random samples.
In fact, when both the hypotheses are simple, the LRT is identical to the Neyman-
Pearson test. We can now summarize the procedure for the LRT.
PROCEDURE FOR THE LRT
1. Find the largest value of the likelihood L(y) for any y02Y0 by finding the maximum likelihood
estimate within Y0 and substituting back into the likelihood function.
2. Find the largest value of the likelihood L(y) for any y2Y by finding the maximum likelihood
estimate within Y and substituting back into the likelihood function.
3. Form the ratio
l ¼ l x1, x2, ..., xn
ð
Þ ¼ L y
ð ÞinY0
L y
ð ÞinY :
4. Determine a K so that the test has the desired probability of type I error, a.
5. Reject H0 if lK.
In the next example, we find a LRT for a testing problem when both H0 and Ha are
simple.
331
6.3 Likelihood Ratio Tests

EXAMPLE 6.3.2
Machine 1 produces 5% defectives. Machine 2 produces 10% defectives. Ten items produced by
each of the machines are sampled randomly; X¼number of defectives. Let y be the true proportion
of defectives. Test H0:y¼0.05 versus Ha:y¼0.1. Use a¼0.05.
Solution
We need to test H0:y¼0.05 versus Ha:y¼0.1. Let
L y
ð Þ ¼
10
x


0:05
ð
Þx 0:95
ð
Þ10x, if y ¼ 0:05
10
x


0:1
ð
Þx 0:90
ð
Þ10x, if y ¼ 0:10:
8
>
>
<
>
>
:
and
L1 ¼ L 0:05
ð
Þ ¼
10
x


0:05
ð
Þx 0:95
ð
Þ10x
and
L2 ¼ L 0:1
ð
Þ ¼
10
x


0:1
ð
Þx 0:90
ð
Þ10x:
Thus, we have
L1
L2
¼ 0:05x
0:1x
0:95
ð
Þ10x
0:9
ð
Þ10x ¼
1
2
 x 19
18

10x
:
The ratio
l ¼
L1
max L1, L2
ð
Þ:
Note that if max(L1, L2)¼L1, then l¼1. Because we want to reject for small values of l,
max(L1, L2)¼L2, and we reject H0 if (L1/L2)K or (L2/L1)>K (note that L2
L1
¼ 2x 18
19

10x
Þ:
That is, reject H0 if
2x 18
19

10x
> K
,
2
18
19
0
B
@
1
C
A
x
> K1
,
19
9

x
> K1:
Hence, reject H0 if X>C; P(X>CjH0:y¼0.05)0.05.
Using the binomial tables, we have
P X > 2jy ¼ 0:05
ð
Þ ¼ 0:0116
and
P X  2jy ¼ 0:05
ð
Þ ¼ 0:0862:
Reject H0 if X>2. If we want a to be exactly 0.05, we have to use randomized test. Reject with
probability 0:0384
0:0762 ¼ 0:5039 if X¼2.
The LRTs do not always produce a test statistic with a known probability distribution
such as the z-statistic of Example 6.3.1. If we have a large sample size, then we can
obtain an approximation to the distribution of the statistic l, which is beyond the
level of this book.
332
CHAPTER 6 Hypothesis Testing

EXERCISES 6.3
6.3.1. Let X1, . . ., Xn be a random sample from an N(m,s2). Assume that s2 is
unknown. We wish to test, at level a, H0:m¼m0 versus Ha:m<m0. Find an
appropriate LRT.
6.3.2. Let X1, . . ., Xn be a random sample from an N(m,s2). Assume that both m and
s2 are unknown. We wish to test, at level a, H0:s2¼s0
2 versus Ha:s2>s0
2.
Find an appropriate LRT.
6.3.3. Let X1, . . ., Xn be a random sample from an N(m1, s2) and let Y1,Y2, . . ., Yn be
an independent sample from an N(m2, s2), where s2 is unknown. We wish to
test, at level a, H0:m1¼m2 versus Ha:m16¼m2. Find an appropriate LRT.
6.3.4. Let X1, . . ., Xn be a sample from a Poisson distribution with parameter l.
Show that a LRT of H0:l¼l0 versus Ha:l6¼l0 rejects the null hypothesis if
X  m1 or X  m2:
6.3.5. Let X1, . . ., Xn be a sample from an exponential distribution with parameter y.
Show that an LRT of H0:y¼y0 versus Ha:y6¼y0 rejects the null hypothesis if
P
i¼1
n Xim1 or P
i¼1
n Xim2.
6.3.6. A clinical oncology program developed a set of guidelines for their cancer
patients to follow. It is believed that the proportion of patients who are still
living after 24 months is greater for those who follow the guidelines. Of the 40
patients who followed the guidelines, 30 are still living after 24 months,
whereas of 32 patients who did not follow the guidelines, 21 are living after
24 months. Find an LRT at a¼0.01 to decide whether the program is effective.
6.4 HYPOTHESES FOR A SINGLE PARAMETER
In this section, we first introduce the concept of p-value. After that, we study hypoth-
esis testing concerning a single parameter.
6.4.1 THE p-VALUE
In hypothesis testing, the choice of the value of a is somewhat arbitrary. For the same
data, if the test is based on two different values of a, the conclusions could be dif-
ferent. Many statisticians prefer to compute the so-called p-value, which is calcu-
lated based on the observed test statistic. For computing the p-value, it is not
necessary to specify a value of a. We can use the given data to obtain the p-value.
Definition 6.4.1 Corresponding to an observed value of a test statistic, the p-
value (or attained significance level) is the lowest level of significance at which
the null hypothesis would have been rejected.
For example, if we are testing a given hypothesis with a¼0.05 and we make a
decision to reject H0 and we proceeded to calculate the p-value equal to 0.03, this
means that we could have used an a as low as 0.03 and still maintain the same deci-
sion, rejecting H0.
333
6.4 Hypotheses for a Single Parameter

Based on the alternative hypothesis, one can use the following steps to compute
the p-value.
STEPS TO FIND THE p-VALUE
1. Let TS be the test statistic.
2. Compute the value of TS using the sample X1, . . ., Xn. Say it is a.
3. The p-value is given by
pvalue ¼
P TS < ajH0
ð
Þ, if lowertailtest
P TS > ajH0
ð
Þ, if lowertailtest
P jTSj > jajjH0
ð
Þ, if lowertailtest:
8
<
:
EXAMPLE 6.4.1
To test H0:m¼0 versus Ha:m6¼0, suppose that the test statistic Z results in a computed value of 1.58.
Then, the p-value¼P(jZj>1.58)¼2(0.0571)¼0.1142. That is, we must have a type I error of
0.1142 in order to reject H0. Also, if Ha:m>0, then the p-value would be P(Z>1.58)¼0.0582.
In this case we must have an a of 0.0582 in order to reject H0.
The p-value can be thought of as a measure of support for the null hypothesis: The
lower its value, the lower the support. Typically one decides that the support for H0 is
insufficient when the p-value drops below a particular threshold, which is the signif-
icance level of the test.
REPORTING TEST RESULT AS p-VALUES
1. Choose the maximum value of a that you are willing to tolerate.
2. If the p-value of the test is less than the maximum value of a, reject H0.
If the exact p-value cannot be found, one can give an interval in which the p-value
can lie. For example, if the test is significant at a¼0.05 but not significant for
a¼0.025, report that 0.025p-value0.05. So for a>0.05, reject H0, and for
a<0.025, do not reject H0.
In another interpretation, 1(p-value) is considered as an index of the strength of
the evidence against the null hypothesis provided by the data. It is clear that the value
of this index lies in the interval [0, 1]. If the p-value is 0.02, the value of index is 0.98,
supporting the rejection of the null hypothesis. Not only do p-values provide us with
a yes or no answer, they provide a sense of the strength of the evidence against the
null hypothesis. The lower the p-value, the stronger the evidence. Thus, in any test,
reporting the p-value of the test is a good practice.
334
CHAPTER 6 Hypothesis Testing

Because most of the outputs from statistical software used for hypothesis testing
include the p-value, the p-value approach to hypothesis testing is becoming more and
more popular. In this approach, the decision of the test is made in the following way.
If the value of a is given, and if the p-value of the test is less than the value of a, we
will reject H0. If the value of a is not given and the p-value associated with the test is
small (usually set at p-value <0.05), there is evidence to reject the null hypothesis in
favor of the alternative. In other words, there is evidence that the value of the true
parameter (such as the population mean) is significantly different (greater, or lesser)
than the hypothesized value. If the p-value associated with the test is not small
(p>0.05), we conclude that there is not enough evidence to reject the null hypoth-
esis. In most of the examples in this chapter, we give both the rejection region and
p-value approaches.
EXAMPLE 6.4.2
The management of a local health club claims that its members lose on the average 15 pounds
or more within the first 3 months after joining the club. To check this claim, a consumer agency
took a random sample of 45 members of this health club and found that they lost an average of
13.8 pounds within the first 3 months of membership, with a sample standard deviation of 4.2
pounds.
(a) Find the p-value for this test.
(b) Based on the p-value in (a), would you reject the null hypothesis at a¼0.01?
Solution
(a) Let m be the true mean weight loss in pounds within the first 3 months of membership in this club.
Then we have to test the hypothesis
H0 : m ¼ 15 versus Ha : m < 15
Here n¼45, x ¼ 13:8, and s¼4.2. Because n¼45>30, we can use normal approximation.
Hence, the test statistic is
z ¼ 13:815
4:2=
ﬃﬃﬃﬃﬃ
45
p
¼ 1:9166
and
pvalue ¼ P Z < 1:9166
ð
Þ  P Z < 1:92
ð
Þ ¼ 0:0274:
Thus, we can use a as small as 0.0274 and still reject H0
(b) No. Because the p-value¼0.0274 is greater than a¼0.01, one cannot reject H0.
In any hypothesis testing, after an experimenter determines the objective of an
experiment and decides on the type of data to be collected, we recommend the fol-
lowing step-by-step procedure for hypothesis testing.
335
6.4 Hypotheses for a Single Parameter

STEPS IN ANY HYPOTHESIS TESTING PROBLEM
1. State the alternative hypothesis, Ha (what is believed to be true).
2. State the null hypothesis, H0 (what is doubted to be true).
3. Decide on a level of significance a.
4. Choose appropriate TS and compute the observed test statistic.
5. Using the distribution of TS and a, determine the rejection region(s) (RR).
6. Conclusion: If the observed test statistic falls in the RR, reject H0 and conclude that based on the
sample information, we are (1a)100% confident that Ha is true. Otherwise, conclude that there
is not sufficient evidence to reject H0. In all the applied problems, interpret the meaning of your
decision.
7. State any assumptions you made in testing the given hypothesis.
8. Compute the p-value from the null distribution of the test statistic and interpret it.
6.4.2 HYPOTHESIS TESTING FOR A SINGLE PARAMETER
Now we study the testing of a hypothesis concerning a single parameter, y, based on a
random sample X1, . . ., Xn. Let ^y be the sample statistic. First, we deal with tests for
the population mean m for large and small samples. Next, we study procedures
for testing the population variance s2. We conclude the section by studying a test
procedure for the true proportion p.
To test the hypothesis H0:m¼m0 concerning the true population mean m, when we
have a large sample (n30) we use the test statistic Z given by
Z ¼ X m0
S=
ﬃﬃﬃn
p
where S is the sample standard deviation and m0 is the claimed mean under H0 (if the
population variance is known, we replace S with s).
For a small random sample (n<30), the test statistic is
T ¼ X m0
S=
ﬃﬃﬃn
p
where m0 is the claimed value of the true mean, and X and S are the sample mean
and standard deviation, respectively. Note that we are using the lowercase letters,
such as z and t, to represent the observed values of the test statistics Z and T,
respectively.
In practice, with raw data, it is important to verify the assumptions. For
example, in the small sample case, it is important to check for normality by using
normal plots. If this assumption is not satisfied, the nonparametric methods
described in Chapter 12 may be more appropriate. In addition, because the sam-
ple statistic such as X and S will be greatly affected by the presence of outliers,
drawing a box plot to check for outliers is a basic practice we should incorporate
in our analysis.
We now summarize the typical test of hypothesis for tests concerning population
(true) mean.
336
CHAPTER 6 Hypothesis Testing

In order to compute the observed test statistic, z in the large sample case and t in
the
small
sample
case,
calculate
the
values
of
z ¼ xm0
ð
Þ= s=
ﬃﬃﬃn
p
ð
Þ
and
t ¼
xm0
ð
Þ= s=
ﬃﬃﬃn
p
ð
Þ
½
	, respectively.
SUMMARY OF HYPOTHESIS TESTS
For m
Large Sample (n30)
Small Sample (n<30)
To test
To test
H0:m¼m0
H0:m¼m0
versus
versus
Ha :
m > m0, uppertail test
m < m0, lowertail test
m 6¼ m0, two-tailed test
Ha :
m > m0, uppertailtest
m < m0, lowertailtest
m 6¼ m0, two-tailedtest
Test statistic: Z ¼ Xm0
s= ﬃﬃn
p
Test statistic: T ¼ Xm0
S= ﬃﬃn
p
Replace s by S, if s is unknown.
Rejectionregion :
z < za, uppertailRR
z < za, lowertailRR
zj j > za=2, twotailRR
8
<
:
RR :
t < ta,n1, uppertailRR
t < ta,n1, lowertailRR
tj j > ta= 2n1
ð
Þ, two tailRR
8
<
:
Assumption: n30 and s2<1.
Assumption: Random sample comes from a
normal population
Decision: Reject H0, if the observed test statistic falls in the RR and conclude that Ha is true with
(1a)100% confidence. Otherwise, keep H0 so that there is not enough evidence to conclude that Ha
is true for the given a and more experiments may be needed.
EXAMPLE 6.4.3
It is claimed that sports-car owners drive on the average 18,000 miles per year. A consumer firm
believes that the average mileage is probably lower. To check, the consumer firm obtained infor-
mation from 40 randomly selected sports-car owners that resulted in a sample mean of 17,463 miles
with a sample standard deviation of 1348 miles. What can we conclude about this claim? Use
a¼0.01. What is the p-value?
Solution
Let m be the true population mean. We can formulate the hypotheses as H0:m¼18,000 versus
Ha:m<18,000.
The observed test statistic (for n30) is
z ¼ xm0
s=
ﬃﬃﬃn
p ﬃ17,46318,000
1348=
ﬃﬃﬃﬃﬃ
40
p
¼ 2:52:
Rejection region is {z<z0.01}¼{z<2.33}.
Decision:Becausez¼2.52islessthan2.33,thenullhypothesisisrejectedata¼0.01.Thereis
sufficient evidence to conclude that the mean mileage on sport cars is less than 18,000 miles per year.
p-Value¼P(z<2.52)¼0.0059. This p-value is less than 0.01 also supports rejection of the
null hypothesis.
337
6.4 Hypotheses for a Single Parameter

EXAMPLE 6.4.4
In a frequently traveled stretch of the I-75 highway, where the posted speed is 70 mph, it is thought
that people travel on the average of at least 70 mph. To check this claim, the following radar
measurements of the speeds (in mph) are obtained for 10 vehicles traveling on this stretch of the
interstate highway
66 74 79 80 69 77 78 65 79 81
Do the data provide sufficient evidence to indicate that the mean speed at which people travel on
this stretch of highway is at least 70 mph (the posted speed limit)? Test the appropriate hypothesis
using a¼0.01. Draw a box plot and normal plot for this data, and comment.
Solution
We need to test
H0 : m ¼ 70 versus Ha : m > 70
For this sample, the sample mean is x ¼ 74:8mph and the standard deviation is s¼5.9963 mph.
Hence, the observed test statistic is
t ¼ xm0
s=
ﬃﬃﬃn
p ¼
74:870
5:9963=
ﬃﬃﬃﬃﬃ
10
p
¼ 2:5314:
From the t-table, t0.01,9¼2.821. Hence, the rejection region is {t>2.821}.
Because, t¼2.5314 does not fall in the rejection region, we do not reject the null hypothesis at
a¼0.01. This can also be verified by the fact that the p-value of 0.01608 is larger than a¼0.01. this
p-value is obtained from R (if we use the t-table, we will see that 0.01<p-value<0.025.) Note that
we assumed that the vehicles were randomly selected and that collected data follow the normal
distribution, because of the small sample size, n<30, we use the t-test.
Figures 6.1 and 6.2 are the box plot and the normal plot of the data, respectively.
The box plot suggests that there are no outliers present. However, the normal plot indicates that
the normality assumption for this data set is not justified. Hence, it may be more appropriate to do a
nonparametric test.
65
70
75
80
Speed
FIGURE 6.1
Box plot of speed data.
338
CHAPTER 6 Hypothesis Testing

EXAMPLE 6.4.5
In attempting to control the strength of the wastes discharged into a nearby river, an industrial firm
has taken a number of restorative measures. The firm believes that they have lowered the oxygen
consuming power of their wastes from a previous mean of 450 manganate in parts per million. To
test this belief, readings are taken on n¼20 successive days. A sample mean of 312.5 and the sample
standard deviation 106.23 are obtained. Assume that these 20 values can be treated as a random
sample from a normal population. Test the appropriate hypothesis. Use a¼0.05.
Solution
Here we need to test the following hypothesis:
H0 : m ¼ 450 versus Ha : m < 450
Given n¼20, x ¼ 312:5, and s¼106.23. The observed test statistic is
t ¼ 312:5450
106:23=
ﬃﬃﬃﬃﬃ
20
p
¼ 5:79:
The rejection region for a¼0.05 and with 19 degrees of freedom is the set of t-values such that
t < t0:05:19
f
g ¼ t < 1:729
f
g:
Decision: Because t¼5.79 is less than 1.729, reject H0. There is sufficient evidence to
confirm the firm’s belief.
For large random samples, the following procedure is used to perform tests of
hypotheses about the population proportion, p.
99
95
90
80
70
60
50
40
Percent
Data
30
20
10
5
55
65
85
95
ML Estimates
Mean :  74.8
Std Dev: 5.68858
75
1
FIGURE 6.2
Normal probability plot for speed.
339
6.4 Hypotheses for a Single Parameter

EXAMPLE 6.4.6
A machine is considered to be unsatisfactory if it produces more than 8% defectives. It is suspected
that the machine is unsatisfactory. A random sample of 120 items produced by the machine contains
14 defectives. Does the sample evidence support the claim that the machine is unsatisfactory? Use
a¼0.01.
Solution
Let Y be the number of observed defectives. This follows a binomial distribution. However, because
np0 and nq0 are greater than 5, we can use a normal approximation to the binomial to test the
hypothesis. So we need to test H0:p¼0.08 versus Ha:p>0.08. Let the point estimate of p be
^p ¼ Y=n
ð
Þ ¼ 0:117, the sample proportion. Then the value of the TS is
z ¼ ^pp0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p0q0
n
r
¼ 0:1170:08
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:08
ð
Þ 0:92
ð
Þ
120
r
¼ 0:137:
For a¼0.01, z0.01¼2.33. Hence, the rejection region is {z>2.33}.
Decision: Because 0.137 is not greater than 2.33, we do not reject H0. We conclude that the
evidence does not support the claim that the machine is unsatisfactory.
SUMMARY OF LARGE SAMPLE HYPOTHESIS TEST FOR p
To test
H0 : p ¼ p0
versus
Ha : p > p0, upper tail test
p < p0, lower tail test:
Test statistic:
Z ¼ ^pp0
s^p
, where s^p ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p0q0
n
r
, where q0 ¼ 1p0:
Rejectionregion :
z > za, upper tail RR
z < za, lower tail RR
zj j > za=2, two tail RR,
8
<
:
where z is the observed test statistic.
Assumption: n is large. A good rule of thumb is to use the normal approximation to the bino-
mial distribution only when np0 and n(1p0) are both greater than 5.
Decision: Reject H0, if the observed test statistic falls in the RR and conclude that Ha is true with
(1a)100% confidence. Otherwise, do not reject H0 because there is not enough evidence to
conclude that Ha is true for given a and more data are needed.
Note that this an approximate test, and the test can be improved by increasing the
sample size.
Now we give the procedure for testing the population variance when the samples
come from a normal population.
340
CHAPTER 6 Hypothesis Testing

SUMMARY OF HYPOTHESIS TEST FOR THE VARIANCE s2
To test
H0 : s2 ¼ s2
0
versus
Ha :
s2 > s2
0, uppertailtest
s2 < s2
0, lowertailtest
s2 6¼ s2
0, two-tailedtest:
Test statistic:
w2 ¼ n1
ð
ÞS2
s2
0
where S2 is the sample variance.
Observed value of test statistic:
n1
ð
Þs2
s2
0
Rejectionregion :
w2 > w2
a,n1, upper tail RR
w2 < w2
1a,n1, lower tail RR
w2 > w2
a=2,n1 or w2 < w2
1a=2,n1, two tail RR
8
>
<
>
:
where wa,n1
2
is such that the area under the chi-square distribution with (n1) degrees of freedom to
its right is equal to a.
Assumption: Sample comes from a normal population.
Decision: Reject H0, if the observed test statistic falls in the RR and conclude that Ha is true with
(1a)100% confidence. Otherwise, do not reject H0 because there is not enough evidence to con-
clude that Ha is true for given a and more data are needed.
Because the chi-square distribution is not symmetric, the “equal tails” used for
the two-sided alternative may not be the best procedure. However, in real-world
problems we seldom use a two tail test for the population variance.
EXAMPLE 6.4.7
A physician claims that the variance in cholesterol levels of adult men in a certain laboratory is at least
100 mg/dL. A random sample of 25 adult males from this laboratory produced a sample standard devi-
ation of cholesterol levels as 12 mg/dL. Test the physician’s claim at 5% level of significance.
Solution
To test
H0 : s2 ¼ 100 versus Ha : s2 < 100
for a¼0.05, and 24 degrees of freedom, the rejection region is
RR ¼ w2 < w2
1a,n1


¼ w2 < 13:484


:
The observed value of the TS is
w2 ¼ n1
ð
ÞS2
s2
0
¼ 24
ð
Þ 144
ð
Þ
100
¼ 34:56:
Because the value of the test statistic does not fall in the rejection region, we cannot reject H0 at
5% level of significance. Here, we assumed that the 25 cholesterol measurements follow the normal
distribution.
341
6.4 Hypotheses for a Single Parameter

EXERCISES 6.4
6.4.1. A random sample of 50 measurements resulted in a sample mean of 62 with a
sample standard deviation 8. It is claimed that the true population mean is at
least 64.
(a) Is there sufficient evidence to refute the claim at the 2% level of
significance?
(b) What is the p-value?
(c) What is the smallest value of a for which the claim will be rejected?
6.4.2. A machine in a certain factory must be repaired if it produces more than 12%
defectives among the large lot of items it produces in a week. A random
sample of 175 items from a week’s production contains 35 defectives, and it
is decided that the machine must be repaired.
(a) Does the sample evidence support this decision? Use a¼0.02.
(b) Compute the p-value.
6.4.3. A random sample of 78 observations produced the following sums:
X
78
i¼1
xi ¼ 22:8,
X
78
i¼1
xi x
ð
Þ2 ¼ 2:05:
(a) Test the null hypothesis that m¼0.45 against the alternative hypothesis
that m<0.45 using a¼0.01. Also find the p-value.
(b) Test the null hypothesis that m¼0.45 against the alternative hypothesis
that m<0.45 using a¼0.01. Also find the p-value.
(c) What assumptions did you make for solving (a) and (b)?
6.4.4. Consider the test H0:m¼35 versus Ha:m>35 for a population that is normally
distributed.
(a) A random sample of 18 observations taken from this population
produced a sample mean of 40 and a sample standard deviation of 5.
Using a¼0.025, would you reject the null hypothesis?
(b) Another random sample of 18 observations produced a sample mean of
36.8 and a sample standard deviation of 6.9. Using a¼0.025, would you
reject the null hypothesis?
(c) Compare and discuss the decisions of parts (a) and (b).
6.4.5. According to the information obtained from a large university, professors
there earned an average annual salary of $55,648 in 1998. A recent random
sample of 15 professors from this university showed that they earn an average
annual salary of $58,800 with a sample standard deviation of $8300. Assume
that the annual salaries of all the professors in this university are normally
distributed.
(a) Suppose the probability of making a type I error is chosen to be zero.
Without performing all the steps of test of hypothesis, would you accept
or reject the null hypothesis that the current mean annual salary of all
professors at this university is $55,648?
(b) Using the 1% significance level, can you conclude that the current mean
annual salary of professors at this university is more than $55,648?
342
CHAPTER 6 Hypothesis Testing

6.4.6. Acheck-cashingservicecompanyfoundthatapproximately7%ofallchecks
submitted to the service were without sufficient funds. After instituting a
random check verification system to reduce its losses, the service company
found that only 70 were rejected in a random sample of 1125 that were
cashed. Is there sufficient evidence that the check verification system
reduced the proportion of bad checks at a¼0.01? What is the p-value
associated with the test? What would you conclude at the a¼0.05 level?
6.4.7. Preliminary results of a study (The journal Environmental News reported in
April 1975that“Thecontinuinganalysisofleadlevelsinthe drinkingwater of
several Boston communities has verified elevated lead concentrations in the
water supplies of Somerville, Brighton and Beacon Hill.”) found that “20% of
the 248 randomlychosenhouseholdstestedin these communitiesshowedlead
levels exceeding the U.S. Public Health Service standard of 50 parts per
million.” In contrast, in Cambridge, which adds anticorrosive to its water in an
attempt to keep the lead from leaching out of the pipes, “only 5% of the 100
randomly sampled households showed lead levels exceeding the standard.”
Find a 95% confidence interval for the difference in the proportions of
households in Somerville, Brighton and Beacon Hill on the one hand and
Cambridge on the other that had lead levels exceeding the government
standard, and carry out a test of the hypothesis of no difference at a¼0.05.
6.4.8. A manufacturer of washers provides a particular model in one of three
colors, white, black, or ivory. Of the first 1500 washers sold, it is noticed
that 550 were of ivory color. Would you conclude that customers have a
preference for the ivory color? Justify your answer. Use a¼0.01.
6.4.9. Atestofthebreakingstrengthofsixropesmanufacturedbyacompanyshowed
a mean breaking strength of 7225 lb and a standard deviation of 120 lb.
However, the manufacturer claimed a mean breaking strength of 7500 lb.
(a) Canwesupportthemanufacturer’sclaimatalevelofsignificanceof0.10?
(b) Computethe p-value. Whatassumptions did you make for this problem?
6.4.10. A sample of 10 observations taken from a normally distributed population
produced the following data:
44 31 52 48 46 39 43 36 41 49
(a) Test the hypothesis that H0:m¼44 versus Ha:m6¼44 using a¼0.10.
Draw a box plot and normal plot for this data, and comment.
(b) Find a 90% confidence interval for the population mean m.
(c) Discuss the meanings of (a) and (b). What can we conclude?
6.4.11. The principal of a charter school in Tampa believes that the IQs of its
students are above the national average of 100. From the past experience,
IQ is normally distributed with a standard deviation of 10. A random sample
of 20 students is selected from this school and their IQs are observed.
The following are the observed values.
95
91
110
93
133
119
113
107
110
89
113
100
100
124
116
113
110
106
115
113
343
6.4 Hypotheses for a Single Parameter

(a) Test for the normality of the data.
(b) Do the IQs of students at the school run above the national average
at a¼0.01?
6.4.12. In order to find out whether children with chronic diarrhea have the
same average hemoglobin level (Hb) that is normally seen in healthy
children in the same area, a random sample of 10 children with
chronic diarrhea are selected and their Hb levels (g/dL) are obtained as
follows.
12:3
11:4
14:2
15:3
14:8
13:8
11:1
15:1
15:8
13:2
Do the data provide sufficient evidence to indicate that the mean Hb
level for children with chronic diarrhea is less than that of the normal value
of 14.6 g/dL? Test the appropriate hypothesis using a¼0.01. Draw a box
plot and normal plot for this data, and comment.
6.4.13. A company that manufactures precision special-alloy steel shafts claims
that the variance in the diameters of shafts is no more than 0.0003. A
random sample of 10 shafts gave a sample variance of 0.00027. At the 5%
level of significance, test whether the company’s claim can be
substantiated.
6.4.14. It was claimed that the average annual expenditures per consumer unit had
continued to rise, as measured by the Consumer Price Index annual
averages (Bureau of Labor Statistics report, 1995). To test this claim, 100
consumer units were randomly selected in 1995 and found to have an
average annual expenditure of $32,277 with a standard deviation of $1200.
Assuming that the average annual expenditure of all consumer units
was $30,692 in 1994, test at the 5% significance level whether the
annual expenditure per consumer unit had really increased from 1994
to 1995.
6.4.15. It is claimed that two of three Americans say that the chances of world peace
are seriously threatened by the nuclear capabilities of other countries. If in a
random sample of 400 Americans, it is found that only 252 hold this view,
do you think the claim is correct? Use a¼0.05. State any assumptions you
make in solving this problem.
6.4.16. According to the Bureau of Labor Statistics (1996), the average
price of a gallon of gasoline in all US cities in the United States in January
1996 was $1.129. A later random sample in 24 cities found the mean
price to be $1.14 with a standard deviation of 0.01. Test at a¼0.05 to see
whether the average price of a gallon of gas in the cities had recently
changed.
6.4.17. A manufacturer claims that the mean life of batteries manufactured by his
company is at least 44 months. A random sample of 40 of these batteries
was tested, resulting in a sample mean life of 41 months with a sample
standard deviation of 16 months. Test at a¼0.01 whether the
manufacturer’s claim is correct.
344
CHAPTER 6 Hypothesis Testing

6.5 TESTING OF HYPOTHESES FOR TWO SAMPLES
In this section we study the hypothesis testing procedures for comparing the means
and variances of two populations. For example, suppose that we want to determine
whether a particular drug is effective for a certain illness. The sample subjects will be
randomly selected from a large pool of people with that particular illness and will be
assigned randomly to the two groups. To one group we will administer a placebo; to
the other we will administer the drug of interest. After a period of time, we measure a
physical characteristic; say the blood pressure, of each subject that is an indicator of
the severity of the illness. The question is whether the drug can be considered effec-
tive on the population from which our samples have been selected. We will consider
the cases of independent and dependent samples.
6.5.1 INDEPENDENT SAMPLES
Two random samples are drawn independently of each other from two populations,
and the sample information is obtained. We are interested in testing a hypothesis
about the difference of the true means. Let X11, ...,X1n1 be a random sample from
population 1 with mean m1 and variance s1
2, and X11, ...,X1n2 be a random sample
from population 2 with mean m2 and variance s2
2. Let Xi, i¼1, 2, represent the respec-
tive sample means and Si
2,i¼1, 2, represent the sample variances. In this case, we
shall consider following three cases in testing hypotheses about m1 and m2: (i) when
s1
2 and s2
2 are known, (ii) when s1
2 and s2
2 are unknown and n130 and n230, and
(iii) when s1
2 and s2
2 are unknown and n1<30 and n2<30. In case (iii) we have the
following two possibilities, (a) s1
2¼s2
2, and (b) s1
26¼s2
2.
In the large sample case, knowledge of population variances s1
2 and s2
2 does not
make much difference. If the population variances are unknown, we could replace
them with sample variances as an approximation. If both n130 and n230 (large
sample case), we can use normal approximation. The following box sums up a
large sample hypothesis testing procedure for the difference of means for the large
sample case.
SUMMARY OF HYPOTHESIS TEST FOR m1–m2 FOR LARGE SAMPLES
(n1 AND n2‡30)
To test
H0 : m1 m2 ¼ D0
versus
Ha :
m1 m2 > D0, uppertailedtest
m1 m2 < D0, lowertailedtest
m1 m2 6¼ D0, two-tailedtest:
8
<
:
Continued
345
6.5 Testing of Hypotheses for Two Samples

The test statistic is
Z ¼ X1 X2 D0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
s2
1
n1
+ s2
2
n2
s
:
Replace si by Si, if si, i¼1,2 are not known.
Rejection region is
RR :
z > za, lower tail RR
zj j > za=2, two tail RR
	
,
where z is the observed test statistic given by
z ¼ x1 x2 D0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
s2
1
n1
+ s2
2
n2
s
:
Assumption: The samples are independent and n1 and n230.
Decision: Reject H0, if test statistic falls in the RR and conclude that Ha is true with (1a)
100% confidence. Otherwise, do not reject H0 because there is not enough evidence to conclude
that Ha is true for given a and more experiments are needed.
EXAMPLE 6.5.1
In a salary equity study of faculty at a certain university, sample salaries of 50 male assistant pro-
fessors and 50 female assistant professors yielded the following basic statistics.
Test the hypothesis that the mean salary of male assistant professors is more than the mean
salary of female assistant professors at this university. Use a¼0.05.
Solution
Let m1 be the true mean salary for male assistant professors and m2 be the true mean salary for female
assistant professors at this university. To test
H0 : m1 m2 ¼ 0 versus Ha : m1 m2 > 0
the test statistic is
z ¼ x1 x2 D0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
s2
1
n1
+ s2
2
n2
s
¼ 46,40046,000
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
360
ð
Þ2
50
+ 220
ð
Þ2
50
s
¼ 6:704:
The rejection region for a¼0.05 is {z>1.645}.
Because z¼6.704>1.645, we reject the null hypothesis at a¼0.05. We conclude that the salary
of male assistant professors at this university is higher than that of female assistant professors for
a¼0.05. Note that even though s1
2 and s2
2 are unknown, because n130 and n230, we could
replace s1
2 and s2
2 by the respective sample variances. We are assuming that the salaries of male
and female are sampled independently of each other.
Sample Mean
Salary
Sample Standard
Deviation
Male assistant professor
$46,400
360
Female assistant
professor
$46,000
220
346
CHAPTER 6 Hypothesis Testing

Given next is the procedure we follow to compare the true means from two indepen-
dent normal populations when n1 and n2 are small (n1<30 or n2<30) and we can
assume homogeneity in the population variances, that is, s1
2¼s2
2. In this case, we
pool the sample variances to obtain a point estimate of the common variance.
COMPARISON OF TWO POPULATION MEANS, SMALL SAMPLE CASE
(POOLED t-TEST)
To test
H0 : m1 m2 ¼ D0
versus
Ha :
m1 m2 > D0, upper tailed test
m1 m2 < D0, lower tailed test
m1 m2 6¼ D0, two-tailed test:
The test statistic is
T ¼ X1 X2 D0
Sp
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n1
+ 1
n2
r
Here the pooled sample variance is
S2
p ¼ n1 1
ð
ÞS2
1 + n2 1
ð
ÞS2
2
n1 + n2 2
:
Then the rejection region is
RR :
t > ta, upper tailed test
t < ta, lower tail test
tj j > ta=2, two-tailed test
8
<
:
where t is the observed test statistic and ta is based on (n1+n22) degrees of freedom, and such that
P(T>ta)¼a.
Decision: Reject H0, if test statistic falls in the RR and conclude that Ha is true with (1a)100%
confidence. Otherwise, do not reject H0 because there is not enough evidence to conclude that Ha is
true for given a.
Assumptions: The samples are independent and come from normal populations with means m1
and m2, and with the (unknown) but equal variances, that is, s1
2¼s2
2.
Now we shall consider the case where s1
2 and s2
2 are unknown and cannot be
assumed to be equal. In such a case the following test is often used. For the
hypothesis
H0 : m1 m2 ¼ D0 versus H0 :
m1 m2 > D0
m1 m2 < D0
m1 m2 ¼ D0
8
<
:
define the test statistic Tv as
Tv ¼ X1 X2 D0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
S2
1
n1
+ S2
2
n2
s
347
6.5 Testing of Hypotheses for Two Samples

where Tv has a t-distribution with v degrees of freedom, and
v ¼
s2
1
n1 + s2
2
n2
h
i2
s2
1=n1

2
n1 1
+ s2
2=n2

2
n2 1
:
The value of v will not necessarily be an integer. In that case, we will round it down
to the nearest integer. This method of hypothesis testing with unequal variances
is called the Smith-Satterthwaite procedure. Even though this procedure is not
widely used, some simulation studies have shown that the Smith-Satterthwaite pro-
cedure perform well when variances are unequal and it gives results that are more
or less equivalent to those obtained with the pooled t-test when the variances are
equal. However, when the sample sizes are approximately equal, the pooled t-test
may still be used. Note that in addressing the question which of the cases (iii) (a)
or (iii) (b) to use in a given problem, we suggest that if the point estimates S1
2 of
s1
2, and S2
2 of s2
2 are approximately the same, then it is logical to assume homogeneity,
s1
2¼s2
2 and use (iii)(a), whereas if S1
2 and S2
2 are significantly different we use (iii)(b).
More appropriately, we have tests that can be used to test hypotheses concerning
s1
2¼s2
2 or s1
26¼s2
2, known as the F-test, which we discuss at the end of this
subsection.
EXAMPLE 6.5.2
The intelligence quotients (IQs) of 17 students from one area of a city showed a sample mean of 106
with a sample standard deviation of 10, whereas the IQs of 14 students from another area chosen
independently showed a sample mean of 109 with a sample standard deviation of 7. Is there a
significant difference between the IQs of the two groups at a¼0.02? Assume that the population
variances are equal.
Solution
We test
H0 : m1 m2 ¼ 0 versus Ha : m1 m2 6¼ 0:
Here n1¼17, x1 ¼ 106, and s1¼10. Also, n2¼14, x2 ¼ 109, and s2¼7.
We have
s2
p ¼ n1 1
ð
Þs2
1 + n2 1
ð
Þs2
2
n1 + n2 2
¼ 16
ð
Þ 10
ð
Þ2 + 13
ð
Þ 7
ð Þ2
29
¼ 77:138:
The test statistic is
T ¼ X1 X2 D0
sp
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n1
+ 1
n2
r
¼
106109
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
77:138
p


ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
17 + 1
14
r
¼ 0:94644:
For a¼0.02, t0.01,29¼2.462. Hence, the rejection region is t< 2.462 or t>2.462.
Because the observed value of the test statistic, T¼0.94644, does not fall in the rejection
region, there is not enough evidence to conclude that the mean IQs are different for the two groups.
Here we assume that the two samples are independent and taken from normal populations.
348
CHAPTER 6 Hypothesis Testing

EXAMPLE 6.5.3
Assume that two populations are normally distributed with unknown and unequal variances.
Two independent samples were drawn from these populations and the data obtained resulted in
the following basic statistics:
n1 ¼ 18 x1 ¼ 20:17 s1 ¼ 4:3
n2 ¼ 12 x2 ¼ 19:23 s2 ¼ 3:8
Test at the 5% significance level whether the two population means are different.
Solution
We need to test the hypothesis
H0 : m1 m2 ¼ 0 versus Ha : m1 m2 6¼ 0:
Here n1¼18, x1 ¼ 20:17, and s1¼4.3. Also, n2¼12, x2 ¼ 19:23, and s2¼3.8. The degrees of
freedom for the t-distribution are given by
v ¼
s2
1
n1 + s2
2
n2

2
s2
1=n1

2
n1 1
+ s2
2=n2

2
n2 1
¼
4:3
ð
Þ2
18
+ 3:8
ð
Þ2
12

2
4:3
ð
Þ2=18

2
17
+
3:8
ð
Þ2=12

2
11
¼ 25:685:
Hence, we have v¼25 degrees of freedom. For a¼0.05, t0.025,25¼2.060. Thus, the rejection
region is t<2.060 or t>2.060.
The test statistic is given by
Tv ¼ x1 x2 D0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
S2
1
n1
+ S2
2
n2
s
¼
20:1719:23
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
4:3
ð
Þ2
18
s
+ 3:8
ð
Þ2
12
¼ 0:62939:
Because the observed value of the test statistic, Tu¼0.62939, does not fall in the rejection
region, we do not reject the null hypothesis. At a¼0.05 there is not enough evidence to conclude
that the population means are different. Note that the assumptions we made are that the samples are
independent and came from two normal populations. No homogeneity assumption is made.
EXAMPLE 6.5.4
Infrequent or suspended menstruation can be a symptom of serious metabolic disorders in women. In
a study to compare the effect of jogging and running on the number of menses, two independent
subgroups were chosen from a large group of women, who were similar in physical activity (aside
from running), heights, occupations, distribution of ages, and type of birth control methods being
used. The first group consisted of a random sample of 26 women joggers who jogged “slow and
easy” 5-30 miles per week, and the second group consisted of a random sample of 26 women runners
who ran more than 30 miles per week and combined long, slow distance with speed work. The
following summary statistics were obtained (Dale et al., 1979).
Continued
349
6.5 Testing of Hypotheses for Two Samples

Joggers x1 ¼ 10:1, s1 ¼ 2:1
Runners x2 ¼ 9:1, s2 ¼ 2:4
Using a¼0.05, (a) test for differences in mean number of menses for each group assuming
equality of population variances, and (b) test for differences in mean number of menses for each
group assuming inequality of population variances.
Solution
Here we need to test
H0 : m1 m2 ¼ 0 versus Ha : m1 m2 6¼ 0:
Here, n1¼26, x1 ¼ 10:1, and s1¼2.1. Also, n2¼26, x2 ¼ 9:1, and s2¼2.4.
(a) Under the assumption s1
2¼s2
2, we have
s2
p ¼ n1 1
ð
Þs2
1 + n2 1
ð
Þs2
2
n1 + n2 2
¼ 25
ð
Þ 2:1
ð
Þ2 + 25
ð
Þ 2:4
ð
Þ2
50
¼ 5:085:
The test statistic is
T ¼ X1 X2 D0
sp
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n1
+ 1
n2
r
¼
10:19:1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
5:085
p


ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
26 + 1
26
r
¼ 1:5989:
For a¼0.05, t0.025,50
1.96. Hence, the rejection region is t<1.96 and t>1.96. Because
T¼1.589 does not fall in the rejection region, we do not reject the null hypothesis. At a¼0.05 there
is not enough evidence to conclude that the population mean number of menses for joggers and
runners are different.
(b) Under the assumption s1
26¼s2
2, we have
v ¼
s2
1
n1
+ s2
2
n2

2
s2
1=n1

2
n1 1
+ s2
2=n2

2
n2 1
¼
2:1
ð
Þ2
26
+ 2:4
ð
Þ2
26
 
!2
2:1
ð
Þ2=26

2
25
+
2:4
ð
Þ2=26

2
25
¼ 49:134:
Hence, we have v¼49 degrees of freedom. Because this value is large, the rejection region is
still approximately t<1.96 and t>1.96. Hence, the conclusion is the same as that of part (a). In
both parts (a) and (b), we assumed that the samples are independent and came from two normal
populations.
Now we present the summary of the test procedure for testing the difference of
two proportions, inherent in two binomial populations. Here, again we assume that
the binomial distribution is approximated by the normal distribution and thus it is an
approximate test.
350
CHAPTER 6 Hypothesis Testing

SUMMARY OF HYPOTHESIS TEST FOR (p1–p2) FOR LARGE SAMPLES
(nipi>5 AND niqi>5, FOR i=1, 2)
To test
H0 : p1 p2 ¼ D0
versus
Ha :
p1 p2 < D0, uppertailedtest
p1 p2 > D0, lowertailedtest
p1 p2 6¼ D0, two-tailedtest
at significance level a, the test statistic is
Z ¼ ^p1  ^p2 D0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p1^q1
n1
+ ^p2^q2
n2
r
where z is the observed value of Z.
The rejection region is
RR :
z > za, uppertailedRR
z < za, lowertailedRR
zj j > za=2, two-tailedRR:
8
<
:
Assumption: The samples are independent and
nipi > 5 and niqi > 5, for i ¼ 1,2:
Decision: Reject H0 if the test statistic falls in the RR and conclude that Ha is true with (1a)
100% confidence. Otherwise, do not reject H0, because there is not enough evidence to conclude that
Ha is true for given a and more experiments are needed.
EXAMPLE 6.5.5
Because of the impact of the global economy on a high-wage country such as the United States, it is
claimed that the domestic content in manufacturing industries fell between 1977 and 1997. A survey
of 36 randomly picked US companies gave the proportion of domestic content total manufacturing
in 1977 as 0.37 and in 1997 as 0.36. At the 1% level of significance, test the claim that the domestic
content really fell during the period 1977-1997.
Solution
Let p1 be the domestic content in 1977 and p2 be the domestic content in 1997.
Given n1¼n2¼36, ^p1 ¼ 0:37 and ^p2 ¼ 0:36. We need to test
H0 : p1 p2 ¼ 0 versus Ha : p1 p2 > 0:
The test statistic is
z ¼
^p1  ^p2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
^p1^q2
n1
+ ^p1^q2
n2
r
¼
0:370:36
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:37
ð
Þ 0:63
ð
Þ
36
+ 0:36
ð
Þ 0:64
ð
Þ
36
r
¼ 0:08813:
For a¼0.01, z0.01¼2.325. Hence, the rejection region is z>2.325. Because the observed value
of the test statistic does not fall in the rejection region, at a¼0.01, there is not enough evidence to
conclude that the domestic content in manufacturing industries fell between 1977 and 1997.
351
6.5 Testing of Hypotheses for Two Samples

Let X1, . . ., Xn and Y1, . . .,Yn be two independent random samples from two normal
populations with sample variances s1
2 and s2
2, respectively. The problem here is of
testing for the equality of the variances, H0:s1
2¼s2
2. We have already seen in
Chapter 4 that
F ¼ S2
1=s2
1
S2
2=s2
2
follows the F-distribution with v1¼n11 numerator and v2¼n21 degrees of
freedom. Under the assumption H0:s1
2¼s2
2, we have
F ¼ S2
1
S2
2
which has an F-distribution with (v1, v2) degrees of freedom. We summarize the test
procedure for the equality of variances.
TESTING FOR THE EQUALITY OF VARIANCES
To test
H0 : s2
1 ¼ s2
2
versus
Ha :
s2
1 > s2
2, lowertailedtest
s2
1 < s2
2, uppertailedtest
s2
1 6¼ s2
2, two-tailed test
at significance level a, the test statistic is
F ¼ S2
1
S2
2
:
The rejection region is
RR :
f > Fa v1, v2
ð
Þ, uppertailedRR
f < F1a v1, v2
ð
Þ, lowertailedRR
f > Fa=2 v1, v2
ð
Þor f < F1a=2 v1,v2
ð
Þ, two-tailedRR
8
<
:
where f is the observed test statistic given by f¼s1
2/s2
2.
Decision: Reject H0 if the test statistic falls in the RR and conclude that Ha is true with (1a)
100% confidence. Otherwise, keep H0, because there is not enough evidence to conclude that Ha is
true for a given a and more experiments are needed.
Assumption:
(i) The two random samples are independent.
(ii) Both populations are normal.
Recall from Section 4.2 that in order to find F1a(v1, v2), we use the identity
F1–a(v1,v2) ¼ (1/Fa(v2,v1)).
352
CHAPTER 6 Hypothesis Testing

EXAMPLE 6.5.6
Consider two independent random samples X1, . . ., Xn from an N(m1,s1
2) distribution and
Y1, . . ., Yn from an N(m2,s2
2) distribution. Test H0:s1
2¼s2
2 versus Ha:s1
26¼s2
2 for the following
basic statistics:
n1 ¼ 25, x1 ¼ 410, s2
1 ¼ 95, and n2 ¼ 16, x2 ¼ 390, s2
2 ¼ 300
Use a¼0.20.
Solution
Test H0:s1
2¼s2
2 versus Ha:s1
26¼s2
2. This is a two-tailed test.
Here the degrees of freedom are v1¼24 and v2¼15. The test statistic is
F ¼ s2
1
s2
2
¼ 95
300 ¼ 0:317:
From the F-table, F0.10(24, 15)¼1.90 and F0.90(24, 15)¼(1/F0.10(15, 24))¼1/1.78¼0.56.
Hence, the rejection region is F>1.90 or F<0.56. Because the observed value of the test
statistic, 0.317, is less than 0.56, we reject the null hypothesis. There is evidence that the population
variances are not equal.
6.5.2 DEPENDENT SAMPLES
We now consider the case where the two random samples are not independent. When
two samples are dependent (the samples are dependent if one sample is related to the
other), then each data point in one sample can be coupled in some natural, nonran-
dom fashion with each data point in the second sample. This situation occurs when
each individual data point within a sample is paired (matched) to an individual data
point in the second sample. The pairing may be the result of the individual observa-
tions in the two samples: (1) representing before and after a program (such as weight
before and after following a certain diet program), (2) sharing the same characteris-
tic, (3) being matched by location, (4) being matched by time, (5) control and exper-
imental, and so forth. Let (X1i, X2i), for i¼1, 2, . . ., n, be a random sample. X1i, and
X2j (i6¼j) are independent. To test the significance of the difference between two
population means when the samples are dependent, we first calculate for each pair
of scores the difference, Di¼X1iX2i, i¼1, 2, . . ., n, between the two scores. Let
mD¼E(Di), the expected value of Di. Because pairs of observations form a random
sample D1, . . ., Dn are independent and identically distributed random variables, if
d1, . . ., dn are the observed values of D1, . . ., Dn, then we define
d ¼ 1
n
X
n
i¼1
di and s2
d ¼
1
n1
X
n
i¼1
di d

2 ¼
Xn
i¼1did2
i 1
n
Xn
i¼1di

2
n1
:
Now the testing for these n observed differences will proceed as in the case of a single
sample. If the number of differences is large (n30), large sample inferential
methods for one sample case can be used for the paired differences. We now sum-
marize the hypothesis testing procedure for small samples.
353
6.5 Testing of Hypotheses for Two Samples

SUMMARY OF TESTING FOR MATCHED PAIRS EXPERIMENT
To test
H0 : mD ¼ d0 versus Ha :
mD > d0, upper tail test
mD < d0, lower tail test
mD 6¼ d0, two-tailed test
the test statistic: T ¼ DD0


= SD=
ﬃﬃﬃn
p
ð
Þ (this approximately follows a Student t-distribution with
(n1) degrees of freedom).
t > ta,n1, uppertail RR
t < ta,n1, lowertail RR
tj j > ta=2,n1, two-tailed RR
8
<
:
where t is the observed test statistic.
Assumptions: The differences are approximately normally distributed.
Decision: Reject H0 if the test statistic falls in the RR and conclude that Ha is true with (1a)
100% confidence. Otherwise, do not reject H0, because there is not enough evidence to conclude that
Ha is true for a given a and more data are needed.
EXAMPLE 6.5.7
A new diet and exercise program has been advertised as remarkable way to reduce blood glucose
levels in diabetic patients. Ten randomly selected diabetic patients are put on the program, and the
results after 1 month are given by the following table:
Before
268
225
252
192
307
228
246
298
231
185
After
106
186
223
110
203
101
211
176
194
203
Do the data provide sufficient evidence to support the claim that the new program reduces blood
glucose level in diabetic patients? Use a¼0.05.
Solution
We need to test the hypothesis
H0 : mD ¼ 0 versus Ha : mD < 0:
First we calculate the difference of each pair given in the following table.
Before
268
225
252
192
307
228
246
298
231
185
After
106
186
223
110
203
101
211
176
194
203
Difference (afterbefore)
162
39
29
82
104
127
35
122
37
18
From the table, the mean of the differences is d ¼ 71:9 and the standard deviation sd¼56.2.
The test statistic is
t ¼ d d0
sd=
ﬃﬃﬃn
p ¼
71:9
56:2=
ﬃﬃﬃﬃﬃ
10
p
¼ 4:0457 
 4:05:
From the t-table, t0.05,9¼1.833. Because the observed value of t¼4.05<t0.05,9¼1.833,
we reject the null hypothesis and conclude that the sample evidence suggests that the new diet and
exercise program is effective.
354
CHAPTER 6 Hypothesis Testing

We can also obtain a (1a) 100% confidence interval for mD using the formula
Dta=2
Sdﬃﬃﬃn
p ,D + ta=2
Sdﬃﬃﬃn
p


where ta/2 is obtained from the t-table with (n1) degrees of freedom. The interpre-
tation of the confidence interval is identical to the earlier interpretation.
EXAMPLE 6.5.8
For the data in Example 6.5.7, obtain a 95% confidence interval for mD and interpret its meaning.
Solution
We have already calculated d ¼ 71:9 and sd¼56.2. From the t-table, t0.025,9¼2.262. Hence, a
95% confidence interval for mD is (112.1, 31.7). That is, P(112.1mD31.7)¼0.95. Note
that mD¼m1m2, and from the confidence limits we can conclude with 95% confidence that m2 is
always greater than m1, that is, m2>m1.
It is interesting to compare the matched pairs test with the corresponding
two independent sample test. One of the natural questions is, why must we take
paired differences and then calculate the mean and standard deviation for the
differences—why can’t we just take the difference of means of each sample, as
we did for independent samples? The answer lies in the fact that s 2
D need not be equal
to s2
X1X2
ð
Þ. Assume that
E Xji


¼ mj, Var Xji


¼ s2
j , for j ¼ 1,2,
and
Cov X1i, X2i
ð
Þ ¼ rs1s2
where r denotes the assumed common correlation coefficient of the pair (X1i, X2i) for
i¼1, 2, . . ., n. Because the values of Di, i¼1, 2, . . ., n, are independent and identi-
cally distributed,
mD ¼ E Di
ð
Þ ¼ E X1i
ð
ÞE X2i
ð
Þ ¼ m1 m2
and
s2
D ¼ Var Di
ð
Þ ¼ Var X1i
ð
Þ + Var X2i
ð
Þ2Cov X1i,X2i
ð
Þ
¼ s2
1 + s2
2 2rs1s2:
From these calculations,
E D
 
¼ mD ¼ m1 m2
and
sD
2 ¼ Var D
 
¼ s2
D
n ¼ 1
n s2
1 + s2
2 2rs1s2


:
355
6.5 Testing of Hypotheses for Two Samples

Now, if the samples were independent with n1¼n2¼n,
E X1 X2


¼ m1 m2
and
s2
X1X2
ð
Þ ¼ 1
n s2
1 + s2
2


:
Hence, if r>0, then s 2
D < s2
X1X2
ð
Þ. As a result, we can see that the matched pairs
test reduces any variability introduced by differences in physical factors in compar-
ison to the independent samples test when r>0. It is also important to observe that
normality assumption for the difference does not imply that the individual samples
themselves are normal. Also, in a matched pairs experiment, there is no need to
assume the equality of variances for the two populations. Matching also reduces
degrees of freedom, because in case of two independent samples, the degrees
of freedom is (n1+n22), whereas for the case of two dependent samples it is
only (n1).
EXERCISES 6.5
6.5.1. Two sets of elementary school children were taught to read by different
methods, 50 by each method. At the conclusion of the instructional period, a
reading test gave results y1 ¼ 74, y2 ¼ 71, s1¼9, and s2¼10. What is the
attained significance level if you wish to see if there is evidence of a real
difference between the two population means? What would you conclude if
you desired an a-value of 0.05?
6.5.2. The following information was obtained from two independent samples
selected from two normally distributed populations with unknown but equal
variances.
Sample 1
14
15
11
14
10
8
13
10
12
16
15
Sample 2
17
16
21
12
20
18
16
14
21
20
13
20
13
Test at the 2% significance level whether m1 is lower than m2.
6.5.3. In the academic year 1997-1998, two random samples of 25 male professors
and 23 female professors from a large university produced a mean salary for
male professors of $58,550 with a standard deviation of $4000 and an
average for female professors of $53,700 with a standard deviation of $3200.
At the 5% significance level, can you conclude that the mean salary of all
male professors for 1997-1998 was higher than that of all female professors?
Assume that the salaries of male and female professors are both normally
distributed with equal standard deviations.
356
CHAPTER 6 Hypothesis Testing

6.5.4. It is believed that the effects of smoking differ depending on race. The
following table gives the results of a statistical study for this question.
Do the data indicate that African Americans are more likely to develop
lung cancer due to smoking? Use a¼0.05.
6.5.5. A supermarket chain is considering two sources A and B for the
purchase of 50-pound bags of onions. The following table gives the results
of a study.
Test at a¼0.05 whether there is a difference in the mean weights.
6.5.6. In order to compare the mean Hemoglobin (Hb) levels of well-nourished and
undernourished groups of children, random samples from each of these
groups yielded the following summary.
Test at a¼0.01 whether the mean Hb levels of well-nourished children
were higher than those of undernourished children.
6.5.7. An aquaculture farm takes water from a stream and returns it after it has
circulated through the fish tanks. In order to find out how much organic
matter is left in the waste water after the circulation, some samples of the
water are taken at the intake and other samples are taken at the
downstream outlet and tested for biochemical oxygen demand (BOD).
BOD is a common environmental measure of the quantity of oxygen
consumed by microorganisms during the decomposition of organic
matter. If BOD increases, it can be said that the waste matter contains
Number of
Children
Sample
Mean
Sample Standard
Deviation
Well nourished
95
11.2
0.9
Undernourished
75
9.8
1.2
Number in
the Study
Average Number of
Cigarettes per Day
Number of Lung
Cancer Cases
Whites
400
15
78
African
Americans
280
15
70
Source A
Source B
Number of bags weighed
80
100
Mean weight
105.9
100.5
Sample variance
0.21
0.19
357
6.5 Testing of Hypotheses for Two Samples

more organic matter than the stream can handle. The following table gives
data for this problem.
Upstream
9.0
6.8
6.5
8.0
7.7
8.6
6.8
8.9
7.2
7.0
Downstream
10.2
10.2
9.9
11.1
9.6
8.7
9.6
9.7
10.4
8.1
Assuming that the samples come from a normal distribution,
(a) Test that the mean BOD for the downstream samples is more than for the
samples upstream at a¼0.05. Assume that the variances are equal.
(b) Test for the equality of the variances at a¼0.05.
(c) In parts (a) and (b), we assumed samples are independent. Now, we feel
this assumption is not reasonable. Assuming that the difference of each
pair is approximately normal, test that the mean BOD for the
downstream samples is more than for the upstream samples at a¼0.05.
6.5.8. Suppose we want to know the effect on driving of a drug for cold and allergy,
in a study in which the same people were tested twice, once after 1 h of
taking the drug and once when no drug is taken. Suppose we obtain the
following data, which represent the number of cones (placed in a certain
pattern) knocked down by each of the nine individuals before taking the drug
and after an hour of taking the drug.
No drug
0
0
3
2
0
0
3
3
1
After drug
1
5
6
5
5
5
6
1
6
Assuming that the difference of each pair is coming from an approximately
normal distribution, test if there is any difference in the individuals’ driving
ability under the two conditions. Use a¼0.05. What is the p-value?
6.5.9. Suppose that we want to evaluate the role of intravenous pulse
cyclophosphamide (IVCP) infusion in the management of nephrotic
syndrome in children with steroid resistance. Children were given a monthly
infusion of IVCP in a dose of 500-750 mg/m2. The following data
(source: S. Gulati and V. Kher, “Intravenous pulse cyclophosphamide—a
new regime for steroid resistant focal segmental glomerulosclerosis,” Indian
Pediatr. 37, 2000) represent levels of serum albumin (g/dL) before and
after IVCP in 14 randomly selected children with nephrotic syndrome.
Pre-IVCP
2.0
2.5
1.5
2.0
2.3
2.1
2.3
1.0
2.2
1.8
2.0
2.0
1.5
3.4
Post-IVCP
3.5
4.3
4.0
4.0
3.8
2.4
3.5
1.7
3.8
3.6
3.8
3.8
4.1
3.4
Assuming that the samples come from a normal distribution:
(a) Here, we cannot assume that samples are independent. Assuming that
the difference of each pair is approximately normal, test that the mean
Pre-IVCP is less than the Post-IVCP at a¼0.05.
(b) Test for the equality of the variances at a¼0.05.
358
CHAPTER 6 Hypothesis Testing

6.5.10. Show that SD
2 is an unbiased estimator of sD
2 .
6.5.11. Test H0:s1
2¼s2
2 versus Ha:s1
26¼s2
2 for the following data.
n1 ¼ 10, x1 ¼ 71, s2
1 ¼ 64 and n2 ¼ 25, x2 ¼ 131, s2
2 ¼ 96:
Use a¼0.10.
6.5.12. The IQs of 17 students from one area of a city showed a mean of 106 with a
standard deviation of 10, whereas the IQs of 14 students from another area
showed a mean of 109 with a standard deviation of 7. Test for equality of
variances between the IQs of the two groups at a¼0.02.
6.5.13. The following data give SAT mean scores for math by state for 1989 and
1999 for 20 randomly selected states (source: The World Almanac and Book
of Facts 2000).
State
1989
1999
Arizona
523
525
Connecticut
498
509
Alabama
539
555
Indiana
487
498
Kansas
561
576
Oregon
509
525
Nebraska
560
571
New York
496
502
Virginia
507
499
Washington
515
526
Illinois
539
585
North Carolina
469
493
Georgia
475
482
Nevada
512
517
Ohio
520
568
New Hampshire
510
518
Assuming that the samples come from a normal distribution:
(a) Test that the mean SAT score for math in 1999 is greater than that in
1989 at a¼0.05. Assume the variances are equal.
(b) Test for the equality of the variances at a¼0.05.
6.6 CHAPTER SUMMARY
In this chapter, we have learned various aspects of hypothesis testing. First, we dealt
with hypothesis testing for one sample where we used test procedures for testing
hypotheses about true mean, true variance, and true proportion. Then we discussed
the comparison of two populations through their true means, true variances, and true
359
6.6 Chapter Summary

proportions. We also introduced the Neyman-Pearson lemma and discussed LRTs
and chi-square tests for categorical data.
We now list some of the key definitions in this chapter.
•
Statistical hypotheses.
•
Tests of hypotheses, tests of significance, or rules of decision.
•
Simple hypothesis.
•
Composite hypothesis.
•
Type I error.
•
Type II error.
•
The level of significance.
•
The p-value or attained significance level.
•
The Smith-Satterthwaite procedure.
•
Power of the test.
•
Most powerful test.
•
Likelihood ratio.
In this chapter, we also learned the following important concepts and procedures:
•
General method for hypothesis testing.
•
Steps to calculate b.
•
Steps to find the p-value.
•
Steps in any hypothesis testing problem.
•
Summary of hypothesis tests for m.
•
Summary of large sample hypothesis tests for p.
•
Summary of hypothesis tests for the variance s2.
•
Summary of hypothesis tests for m1m2 for large samples (n1 and n230).
•
Summary of hypothesis tests for p1p2 for large samples.
•
Testing for the equality of variances.
•
Summary of testing for a matched pairs experiment.
•
Procedure for applying the Neyman-Pearson lemma.
•
Procedure for the LRT.
6.7 COMPUTER EXAMPLES
In the following examples, if the value of a is not specified, we will always take it
as 0.05.
6.7.1 R-EXAMPLES
EXAMPLE 6.7.1
One-sample t-test:
Using the following data:
Sample x: 66 74 79 80 69 77 78 65 79 81
360
CHAPTER 6 Hypothesis Testing

Test H0:m¼70 versus Ha:m>70
This example assumes you’ve stored the data in variable x, please modify the code
appropriately.
R Code:
t.test(x, mu¼70, alternative¼“greater”);
Output:
One Sample t-test
data: x
t¼2.5314, df¼9, p-value¼0.01608
alternative hypothesis: true mean is greater than 70
95 percent confidence interval:
71.32406 Inf
sample estimates:
mean of x
74.8
Conclusion: since the p-value¼0.01608>0.01, we will not reject H0 at a¼0.01. However, if a
is greater than 0.01608, then we will reject the null hypothesis.
EXAMPLE 6.7.2
The management of a local health club claims that its members lose on the average 15 pounds or
more within the first 3 months after joining the club. To check this claim, a consumer agency
took a random sample of 45 members of this health club and found that they lost an average of
13.8 pounds within the first 3 months of membership, with a sample standard deviation of 4.2
pounds.
(a) Find the p-value for this test.
(b) Based on the p-value in (a), would you reject the null hypothesis at a¼0.01?
R-code:
>xbar¼13.8 #sample mean
>mu0¼15 #hypothesized value
>sigma¼4.2
>n¼45
>z¼(xbar-mu0)/(sigma/sqrt(n))
>z
[1] -1.91663
>alpha¼.01
>z.alpha¼qnorm(1-alpha)
>-z.alpha
[1] -2.326348
Since observed z- doesnot fall in the rejection region, we do not reject the null hypothesis at
a¼0.01.
If we need p-value approach, then:
>pval¼pnorm(z)
>pval
Output:
[1] 0.0276425
Again since p-value is larger that a¼0.01, we do not reject the null hypothesis.
361
6.7 Computer Examples

EXAMPLE 6.7.3 R-CODE FOR EXERCISE 6.4.9
>xbar¼7225
>mu0¼7500
>s¼120
>n¼6
>t¼(xbar-mu0)/(s/sqrt(n))
>t
[1] -5.613414
>alpha¼0.01
>t.alpha¼qt(1-alpha, df¼n-1)
>-t.alpha
[1] -3.36493
> pval¼pt(t, df¼n1)
>pval
[1] 0.001240944
EXAMPLE 6.7.4 TWO SAMPLE t-TEST:
Using the following data:
Sample x: 16 18 21 13 19 16 18 15 20 19 14 21 14
Sample y: 14 15 10 13 11 7 12 11 12 15 14
Test H0:mx¼my versus Ha:mx<my using a¼0.02.
This example assumes you’ve stored the data in variables x and y. Please modify your code
appropriately.
R Code:
t.test (x, y, alternative¼”less”);0
Output:
Welch Two Sample t-test
data: x and y
t¼4.8077, df¼21.963, p-value¼1
alternative hypothesis: true difference in means is less than 0
95 percent confidence interval:
-Inf 6.852384
sample estimates:
mean of x mean of y
17.23077 12.18182
EXAMPLE 6.7.5 ONE-SAMPLE t-TEST (TWO-TAILED):
Using the following data:
Sample X: 6.8 5.6 8.5 8.5 8.4 7.5 9.3 9.4 7.8 7.1 9.9 9.6 9.0 9.4 13.7 16.6 9.1 10.1 10.6 11.1 8.9
11.7 12.8 11.5 12.0 10.6 11.1 6.4 12.3 12.3 11.4 9.9 14.3 11.5 11.8 13.3 12.8 13.7 13.9 12.9 14.2 14.0
15.5 16.9 18.0 17.9 21.8 18.4 34.3
Test H0:mx¼12 versus Ha:mx6¼12 using a¼0.05.
This example assumes you’ve stored the data in variable x. Please modify your code appropriately.
R Code:
t.test (x, mu¼12);
Since our p-value is greater than
0.02, we fail to reject the null.
362
CHAPTER 6 Hypothesis Testing

Output:
One Sample t-test
data: x
t¼0.1854, df¼48, p-value¼0.8537
alternative hypothesis: true mean is not equal to 12
95 percent confidence interval:
10.77437 13.47461
sample estimates:
mean of x
12.12449
EXAMPLE 6.7.6 PAIRED SAMPLES t-TEST:
Using data:
Upstream (x)
9.0
6.8
6.5
8.0
7.7
8.6
6.8
8.9
7.2
7.0
Downstream (y)
10.2
10.2
9.9
11.1
9.6
8.7
9.6
9.7
10.4
8.1
Test Ha:md¼0 versus Ha:md<0 using a¼0.05.
This is a paired t-test and assumes you’ve stored the data in variables x and y. Please modify
code appropriately.
R Code:
t.test (x, y, paired¼TRUE, alternative¼”less”);
Output:
Paired t-test
data: x and y
t¼5.3982, df¼9, p-value¼0.000217
alternative hypothesis: true difference in means is less than 0
95 percent confidence interval:
-Inf1.38689
sample estimates:
mean of the differences
-2.1
6.7.2 MINITAB EXAMPLES
EXAMPLE 6.7.7
(t-Test): Consider the data
66 74 79 80 69 77 78 65 79 81
Using Minitab, test H0:m¼75 versus H1:m>75.
Solution
Enter the data in C1. Then
Stat>Basic Statistics>1-sample. . .>In Variables: enter C1>choose Test Mean>enter 75>in
Alternative: choose greater than and click OK
Since the p-value is greater than 0.05,
we fail
to reject the null hypothesis
We reject the null hypothesis since our
p-value is less than 0.05 suggesting 
than the mean difference is less than 0.
363
6.7 Computer Examples

EXAMPLE 6.7.8
For the following data:
Sample1: 16 18 21 13 19 16 18 15 20 19 14 21 14
Sample 2: 14 15 10 13 11 7 12 11 12 15 14
Test H0:m1¼m2 versus H1:m1<m2. Use a¼0.02.
Solution
Enter sample 1 data in C1 and sample 2 data in C2. Then
Stat>Basic Statistics>2-sample t. ..>Choose Samples in different columns>in Alternative:
choose less than>in Confidence level: enter 98>click Assumed equal variances and click OK
We obtain the following output.
Two Sample T-test and Confidence Interval
Two sample T for C1 vs C2
N Mean StDev SE Mean
C1 13 17.23 2.74 0.76
C2 11 12.18 2.40 0.76
98% CI for mu C1mu C2: (2.38, 7.71)
T-Test mu C1¼mu C2 (vs <): T¼4.75, P¼1.0, DF¼22
Both use Pooled StDev¼2.59
If we did not select Assumed equal variances, we will obtain the following
output.
Two Sample T-Test and Confidence Interval
Two sample T for C1 vs C2
N
Mean
StDev
SE Mean
C1
13
17.23
2.74
0.76
C2
11
12.18
2.40
0.72
98% CI for mu C1 - mu C2: (2.40, 7.69)
T-Test mu C1¼mu C2 (vs <): T¼4.81 P¼1.0 DF¼21
EXAMPLE 6.7.9
For the following data:
6:8
5:6
8:5
8:5
8:4
7:5
9:3
9:4
7:8
7:1
9:9
9:6
9:0
9:4
13:7 16:6
9:1 10:1 10:6 11:1
8:9
11:7
12:8
11:5 12:0 10:6 11:1
6:4 12:3 12:3
11:4
9:9
14:3 11:5
11:8 13:3 12:8 13:7 13:9 12:9
14:2
14:0
15:5 16:9
18:0 17:9 21:8 18:4 34:3
Test H0:m¼12 versus H1:m6¼12. Use a¼0.05.
Solution
Enter the data in C1. Then
Stat>Basic Statistics>1-sample z. . .>in Variables: Type C1>choose Test Mean and enter
12>choose not equal in Alternative, and Type 4.7 for sigma>Click OK
364
CHAPTER 6 Hypothesis Testing

EXAMPLE 6.7.10
(Paired t-Test): Consider the data of Example 7.5.7. Using Minitab, perform a paired t-test.
Solution
Enter sample 1 in column C1 and sample 2 in column C2. Then:
Stat>Basic Statistics>Paired t. . .>in First Sample: Type C2, and in the Second sample: Type
C1>click options>and click less than (if a is other than 0.05, enter appropriate percentage in
Confidence level: and enter appropriate number if it is not zero in Test mean:)>click OK>OK
6.7.3 SPSS EXAMPLES
EXAMPLE 6.7.11
Consider the data
66747980 69777865 7981
Using SPSS, test H0:m¼75 versus H1:m>75.
Solution
Use the following procedure:
1. Enter the data in column 1.
2. Click Analyze>Compare Means>One-sample t-Test. . ., Move var00001 to Test Variable(s),
and change Test Value: 0 to 75. Click OK
If we want the computer to calculate the p-value in the previous example, use the
following procedure.
1. Enter the test statistic (0.105) in the data editor using ‘teststat’.
2. Click Transform>compute. . .
3. Type ‘p-value’ in the box called Tarobtain value. In the box called Functions: scroll and click
on CDF.T(q,df) and move to Numeric Expressions.
4. The CDF(q,df) will appear as CDF(?,?) in the Numeric Expressions box. Replace teststat for q
and 9 for df (the degree of freedom in this example is 9). Click OK
EXAMPLE 6.7.12
For the following data
Sample1 : 16 18 21 13 19 16 18 15 20 19 14 21 14
Sample2 : 14 15 10 13 11
7 12 11 12 15 14
Test H0:m1¼m2 versus H1:m1<m2. Use a¼0.02.
Solution
In column 1, under the title “group” enter 1 s to identify the sample 1 data and 2 s to identify sample
2 data. In column C2, under the title “data” enter the data corresponding to samples 1 and 2. Then:
Analyze>Compare Means>Independent Samples t-test. . .>bring Data to Test Variable(s): and
group to Grouping Variable:, click Define Groups. . ., and enter 1 for sample 1, 2 for sample
2>click continue>click Options. . . Enter 98 in Confidence interval: >click continue>OK
365
6.7 Computer Examples

EXAMPLE 6.7.13
(Paired t-Test) For the data of Example 7.5.7, use SPSS to test whether the data provide sufficient
evidence for the claim that the new program reduces blood glucose level in diabetic patients. Use
a¼0.05.
Solution
Enter after data in column C1 and before data in column C2. Then:
Analyze>Compare Means>Paired-Sample T-Test>bring after and before to Paired Variables:
so that it will look after-before>click OK
6.7.4 SAS EXAMPLES
To conduct a hypothesis test using SAS, we could use proc ttest, or proc means with
option of computing the t-value and corresponding probability. However, to use
this, we need a hypothesis of the form H0:m¼0. For testing nonzero values,
H0:m¼m0, we must create a new variable by subtracting m0 from each observation,
and then use the test procedure for this new variable. The following example illus-
trates this concept.
EXAMPLE 6.7.14
(t-Test): The following radar measurements of speed (in miles per hour) are obtained for 10 vehicles
traveling on a stretch of interstate highway.
66 74 79 80 69 77 78 65 79 81
Do the data provide sufficient evidence to indicate that the mean speed at which people travel on
this stretch ofhighwayis at least 75 mph? Test using a¼0.01. Use anSAS procedure to do the analysis.
Solution
In the SAS editor, type in the following commands.
data speed;
title ’Test on highway speed’;.
input X @@;
Y¼X-75;
datalines;
66 74 79 80 69 77 78 65 79 81
;
PROC TTEST data¼speed;
run;
We obtain the following output.
Test on highway speed
The TTEST Procedure Statistics
Statistics
Lower CL
Upper CL
Lower
Upper
CL
CL
Variable N
Mean
Mean
Mean
Std
Std
Std
Std
Dev
Dev
Dev
Err
Continued
366
CHAPTER 6 Hypothesis Testing

X 10
70.511
74.8
79.089
4.1245 5.9963 10.947 1.8962
Y 10
4.489
0.2
4.0895
4.1245 5.9963 10.947
T-Tests
Variable
DF
t Value
Pr>jtj
X
9
39.45
<.0001
Y
9
0.11
0.9183
To test H0:m¼75, we need to look at the Y-values. The corresponding t-value is 0.11, and
because this is a one-sided test, we need to divide 0.9183 by 2 to obtain the p-value as
p¼0.45915. Because the p-value is larger than 0.01¼a, we cannot reject the null hypothesis.
One of the easier ways to conduct large sample hypothesis testing using SAS pro-
cedures is through the computation of the p-value. The following example illustrates
the procedure.
EXAMPLE 6.7.15
(z-Test): It is claimed that the average miles driven per year for sports cars is at least 18,000 miles.
To check the claim, a consumer firm tests 40 of these cars randomly and obtains a mean of 17,463
miles with standard deviation of 1348 miles. What can it conclude if a¼0.01?
Solution
Here we will find the p-value and compare that with a to test the hypothesis. We use the following
SAS procedure:
Data ex888;
z¼(17463-18000)/(1348/(SQRT(40)));
pval¼probnorm(z);
run;
proc print data¼ex888;
title ’Test of mean, large sample’;
run;
We obtain the following output.
Test of mean
large sample
Obs
z
pval
1
2.51950
.005876079
Because the p-value of 0.005876079 is less than a¼0.01, we reject the null hypothesis.
There is sufficient evidence to conclude that the mean miles driven per year for sport cars is less
than 18,000.
Note that in the previous example, the value of z was negative. If the value of z is
positive, use pval¼probnorm(z);, also, if it is a two-sided hypothesis, we need
to multiply by 2, so use pval¼probnorm(z)*2; to obtain the p-value.
367
6.7 Computer Examples

EXAMPLE 6.7.16
(Paired t-Test): For the data of Example 7.5.7, use SAS to test whether the data provide sufficient
evidence for the claim that the new program reduces blood glucose level in diabetic patients. Use
a¼0.05.
Solution
We can use the following commands.
data dietexr;
input before after;
diff¼after - before;
datalines;
268 106
225 186
252 223
192 110
307 203
228 101
246 211
298 176
231 194
185 203
;
run;
proc means data¼dietexr t prt;
var diff;
title ’Test of mean, Paired difference’;
run;
PROJECTS FOR CHAPTER 6
6A. TESTING ON COMPUTER-GENERATED SAMPLES
(a) Small sample test:
Generate a sample of size 20 from a normal population with m¼10, and
s2¼4.
(i)
Perform a t-test for the test H0:m¼10 versus Ha:m6¼10 at level a¼0.05.
(ii) Perform the test H0:s2¼4 versus Ha:s26¼4 at level a¼0.05.
Repeat the procedure 10 times, and comment on the results.
(b) Large sample test:
Generate a sample of size 50 from a normal population with m¼10, and
s2¼4. Perform a z-test for the test H0:m¼10 versus Ha:m6¼10 at level a¼0.05.
Repeat the procedure 10 times and comment on the results.
368
CHAPTER 6 Hypothesis Testing

6B. CONDUCTING A STATISTICAL TEST WITH CONFIDENCE INTERVAL
Let y be any population parameter. Consider the three tests of hypotheses
H0 : y ¼ y0 versus Ha : y > y0,
(1)
H0 : y ¼ y0 versus Ha : y < y0,
(2)
H0 : y ¼ y0 versus Ha : y 6¼ y0:
(3)
The following procedure can be exploited to test a statistical hypothesis utilizing the
confidence intervals.
Procedure to Use Confidence Interval for Hypothesis Testing
Let y be any population parameter.
(a) For test (1), that is,
H0 : y ¼ y0 versus Ha : y > y0
choose a value for a. From a random sample, compute a confidence interval for y
using a confidence coefficient equal to 1–2a. Let L be the lower end point of this
confidence interval.
Reject H0ify0 < L:
That is, we will reject the null hypothesis if the confidence interval is completely
to the right of y0.
(b) For test (2), that is,
H0 : y ¼ y0 versus Ha : y < y0
choose a value for a. From a random sample, compute a confidence interval for y
using a confidence coefficient equal to 1–2a. Let U be the upper end point of this
confidence interval.
Reject H0ifU < y0:
That is, we will reject the null hypothesis if the confidence interval is
completely to the left of y0.
(c) For test (3), that is,
H0 : y ¼ y0 versus Ha : y 6¼ y0
choose a value for a. From a random sample, compute a confidence interval
for y using a confidence coefficient equal to 1–a. Let L be the lower end point
and U be the upper end point of this confidence interval.
Reject H0 if y0 < L or U < y0:
That is, we will reject the null hypothesis if the confidence interval does not
contain y0.
(i)
For any large data set, conduct all three of these hypothesis tests using a
confidence interval for the population mean.
(ii) For any small data set, conduct all three of these hypothesis tests using a
confidence interval for the population mean.
369
Projects for Chapter 6

CHAPTER
Goodness-of-Fit Tests
Applications
7
CHAPTER CONTENTS
7.1 Introduction .................................................................................................... 372
7.2 The Chi-Square Tests for Count Data ................................................................ 372
7.3 Goodness-of-Fit Tests to Identify the Probability Distribution .............................. 381
7.4 Applications: Parametric Analysis .................................................................... 392
7.5 Exercises ........................................................................................................ 402
7.6 Chapter Summary ............................................................................................ 406
7.7 Computer Examples ......................................................................................... 406
Projects for Chapter 7 ............................................................................................ 408
OBJECTIVE
In this chapter, we will study various methods of testing, goodness-of-fit, to deter-
mine if a given set of data follows a particular probability distribution. In addition,
we will perform parametric analysis using real data, from economics, environment,
and health sciences.
Karl Pearson
(Source: http://www-history.mcs.st-and.ac.uk/~history/PictDisplay/Pearson.html)
Karl Pearson (1857-1936) is considered the founder of the twentieth-century sci-
ence of statistics. Pearson has contributed in several different fields such as anthro-
pology, biometry, genetics, scientific methods, and statistical theory. He applied
Mathematical Statistics with Applications in R
Copyright © 2015 Elsevier Inc. All rights reserved.
371

statistics to biological problems of heredity and evolution. In 1911, he founded the
world’s first university statistics department at the University College London.
He is the author of The Grammar of Science, the three volumes of The Life, Let-
ters and Labors of Francis Galton, and The Ethic of Free Thought. Pearson was the
founder of the statistical journal Biometrika.
In 1900, he published a paper on the chi-square goodness-of-fit test that we will
study in this Chapter. This is one of Pearson’s most significant contributions to sta-
tistics. In 1893, Pearson coined the term “standard deviation.”
7.1 INTRODUCTION
In studying various real world phenomena, we begin with a random sample of data
X1,. . .,Xn that represents values of some sort of a subject of interest. These measure-
ments could represent the amount of carbon dioxide, CO2, in the atmosphere on a
daily basis, the sizes of cancerous breast tumors, the monthly average rainfall in
the State of Florida, the average monthly unemployment rate in the United States,
the hourly wind forces of a hurricane, etc. In order for us to probabilistically
understand the behavior of these phenomena, we need to identify the probability
distribution that follows or the given data are drawn from. For example, at a certain
time point we say that these data follow or come from the normal or exponential
probability distribution. One of the important questions then is whether observed
data are representative or follow a particular probability distribution. In fact, there
is nothing we can do parametrically or statistically unless through goodness-of-fit
testing identifies the probability density functions, pdf, that probabilistically charac-
terizes the behavior of the given data, the phenomenon of interest.
To accomplish this objective of identifying the underlying probability distribution,
we will discuss four statistical tests (methods), that we can use to determine how good
the data fits a particular probability distribution. These four tests are as follows:
Pearson’s chi-square test
Kolmogorov-Smirnov test
Anderson-Darling test
Shapiro-Wilk test
P-P plots
Q-Q plots
There are other methods we can follow, if we are not able to identify the appro-
priate pdf, such as nonparametric or probability distribution free analysis, which will
be discussed in Chapter 12.
7.2 THE CHI-SQUARE TESTS FOR COUNT DATA
In this section, we will study several commonly used tests for count data, where obser-
vations are given by counting and assume non-negative integer values {0, 1, 2,. . .}.
372
CHAPTER 7 Goodness-of-Fit Tests Applications

These are basically large sample tests based on a w2-approximation. A chi-square test
is designed to analyze categorical data and it is intended to test how likely it is that an
observed probability distribution is due to chance. Suppose that we have outcomes of
a multinomial experiment that consists of k mutually exclusive and exhaustive
events A1, . . .,Ak. Let P(Ai)¼pi, i¼1,2,. . .,k. Then P
i¼1
n
pi¼1. Let the experiment
be repeated n times, and let Xi (i¼1, 2, . . ., k) represent the number of times the event
Ai occurs. Then (X1, . . ., Xk) have a multinomial distribution with parameters n,
p1,. . .,pk.
Let
Q2 ¼
X
k
i¼1
Xi npi
ð
Þ2
npi
:
It can be shown that for large n, the random variable Q2 is approximately w2-
distributed with (k1) degrees of freedom. It is required that npi5 (i¼1, 2, . . .,
k) for the approximation to be valid, although the approximation generally works
well if we only have a few values of i (about 20%), npi1 and the rest (about
80%) satisfy the condition that npi5. This statistic was proposed by Karl Pearson
in his 1900 paper.
It should be noted that the w2-tests that we are studying in this section is an
approximate tests valid for large samples. Often Xi is called the observed frequency
and is denoted by Oi (this is the observed value in class i), and npi is called the
expected frequency and is denoted by Ei (this is the theoretical distribution frequency
under the null hypothesis). Thus, with these notations, we can calculate
Q2 ¼
X
k
i¼1
Oi Ei
ð
Þ2
Ei
:
The example given below, illustrates how we apply this goodness-of-fit test.
EXAMPLE 7.2.1
A plant geneticist grows 200 progeny from a cross that is hypothesized to result in a 3:1 phenotypic
ratio of red-flowered to white-flowered plants. Suppose the cross produces 170 red- to 30 white-
flowered plants. (a) Calculate Q2 for this experiment. (b) Does the given data support the 3:1 ratio
at a¼0.05?
Solution
There are two categories of data totaling n¼200. Hence, k¼2. Let i¼1 represent red-flowered and
i¼2 represent white-flowered plants. Then O1¼170, and O2¼30.
Here, we want to test the hypothesis to answer the posed question.
H0 : Theflowercolorpopulationratioisnotdifferentfrom3 : 1,
versus
Ha : Theflowercolorpopulationsampledhasaflowercolorratiothatisnot3red : 1white:
(a) We are given that p1¼ 3
4 , and p2¼ 1
4 and the condition that np15 and np25 are satisfied.
Thus, we proceed to calculate Q2 for the information that is given.
Thus,
Continued
373
7.2 The Chi-Square Tests for Count Data

E1 ¼ np1 ¼ 200
ð
Þ 3
4
 
¼ 150, and E2 ¼ np2 ¼ 200
ð
Þ 1
4
 
¼ 50
and
Q2 ¼
X
2
i¼1
Oi Ei
ð
Þ2
Ei
¼ 170150
ð
Þ2
150
+ 3050
ð
Þ2
50
¼ 10:667:
(b) Since k¼2, from the w2-table with 1 degree of freedom and a¼0.05, the rejection region is
{x1,0.05
2
>3.841}. Since 10.667 is greater than 3.841, we reject the null hypothesis and conclude that
the color ratio is not 3:1. The data support the alternative hypothesis that the ratio is not 3 red:
1 white
The type of calculation in Example 7.2.1 gives a measure of how close our observed
frequencies come to the expected frequencies and is referred to as a measure of
goodness-of-fit. Smaller values of Q2 indicate better fit.
Unless the sample size is exactly 100, percentages cannot be used. These tests
that we will study are approximate test, but very useful in performing statistical anal-
ysis. Let the random variables (X1, . . ., Xk) have a multinomial distribution with
parameters n, p1, . . ., pk. Let n be known. We will now present some important tests
based on the chi-square w2-statistic.
7.2.1 TESTING THE PARAMETERS OF A MULTINOMIAL
DISTRIBUTION: GOODNESS-OF-FIT TEST
One of the most frequent uses of the w2-test is in comparison of observed fre-
quencies. The test is also called a “goodness-of-fit” test statistic, because this
measure how well the observed distribution of data fits with the distribution that
is expected if the variables are independent. Let an experiment have k mutually
exclusive and exhaustive outcomes A1, A2, . . ., Ak. We would like to test the null
hypothesis that all the pi¼p(Ai), i¼1, 2, . . ., k are equal to known numbers pi0,
i¼1,2,. . .,k.
The test procedure that we use to test the subject hypothesis is highlighted below.
TESTING THE PARAMETERS OF A MULTINOMIAL
DISTRIBUTION (SUMMARY)
To test
H0 : p1 ¼ p10, ...,pk ¼ pk0
versus
Ha : Atleastoneoftheprobabilitiesisdifferentfromthehypothesizedvalue:
The test is always a one-sided upper tail test.
Let Oi be the observed frequency, Ei¼npi0 be the expected frequency (frequency under the null
hypothesis), and k be the number of classes. The test statistic is
Q2 ¼
X
k
i¼1
Oi Ei
ð
Þ2
Ei
:
374
CHAPTER 7 Goodness-of-Fit Tests Applications

The test statistic Q2 has an approximate chi-square probability distribution with k1 degrees of
freedom.The rejection region is given by
Q2  w2
a,k1:
Assumption: Ei5.
This test is known as the w2-goodness-of-fit test. It implies that if the observed
data are very close to the expected data, we have a very good fit and we do not reject
the null hypothesis. That is, for small Q2 values, we do not have enough evidence to
reject H0 and hence we will accept H0.
The following examples illustrate how we apply the subject goodness-of-fit test.
EXAMPLE 7.2.2
A TV station broadcasts a series of programs on the ill effects of smoking marijuana. After the series,
the station wants to know whether people have changed their opinion about legalizing marijuana.
Given in the following tables are the data based on a survey of 500 randomly chosen individuals:
Before the series was shown
After the series was shown
Here, k¼4, and we wish to test the following hypothesis:
H0 : p1 ¼ 0:07; p2 ¼ 0:18; p3 ¼ 0:65; p4 ¼ 0:1
versus
Ha : Atleastoneoftheprobabilitiesisdifferentfromthehypothesizedvalue:
The test is always an upper tail test. Test this hypothesis using a¼0.01.
Solution
We have
E1 ¼ 500
ð
Þ 0:07
ð
Þ ¼ 35; E2 ¼ 90; E3 ¼ 325; E4 ¼ 50:
The observed frequencies are
O1 ¼ 500
ð
Þ 0:39
ð
Þ ¼ 195; O2 ¼ 45; O3 ¼ 180; O4 ¼ 80:
The value of the test statistic is
Q2 ¼
X
4
i¼1
Oi Ei
ð
Þ2
Ei
¼
19535
ð
Þ2
35
+ 4590
ð
Þ2
90
+ 180325
ð
Þ2
325
+ 8050
ð
Þ2
50
"
#
¼ 836:62:
From the w2-table, w0.01,3
2
¼11.3449. Because the test statistic Q2¼836.62>11.3449, we reject
H0 at a¼0.01. Hence, the data suggest that people have changed their opinion after the series on the
ill effects of smoking marijuana was shown.
For
Legalization
Decriminalization
Existing Law (Fine or
Imprisonment)
No
Opinion
39%
9%
36%
16%
For
Legalization
Decriminalization
Existing Law (Fine or
Imprisonment)
No
Opinion
7%
18%
65%
10%
375
7.2 The Chi-Square Tests for Count Data

EXAMPLE 7.2.3
A die is rolled 60 times and the face values are recorded. The results are as follows.
Up face
1
2
3
4
5
6
Frequency
8
11
5
12
15
9
Is the die balanced fair? Test this question using a¼0.05.
Solution
If the die is fair, we must have
p1 ¼ p2 ¼  ¼ p6 ¼ 1
6
where pi¼P(face value on the die is i), i¼1, 2, . . ., 6. This has the discrete uniform distribution.
Hence,
H0 : p1 ¼ p2 ¼  ¼ p6 ¼ 1
6
versus
Ha : At least one of the probabilities is different from the hypothesized value of 1
6
Note that E1¼n1p1¼(60)(1/6)¼10, . . ., E6¼10, and the assumption of using the test is
satisfied.
We summarize the calculations in the following table:
Face value
1
2
3
4
5
6
Frequency, Oi
8
11
5
12
15
9
Expected value, Ei
10
10
10
10
10
10
The test statistic value is given by
Q2 ¼
X
6
i¼1
Oi Ei
ð
Þ2
Ei
¼ 6:
From the chi-square table with 5 degrees of freedom., w0.05,5
2
¼11.070.
Thus, w0.05,5
2
¼11.070¼11.07>Q2¼6 and since the value of the test statistic does not fall in the
rejection region, we do not reject H0. Therefore, we do not have enough evidence to conclude that
the die is not fair.
7.2.2 CONTINGENCY TABLE: TEST FOR INDEPENDENCE
Another important use of the w2-statistic is testing for dependencies between the rows
and columns in a contingency table. Here, we have n randomly selected items are
classified according to two different criteria, or two factors (row factor and column
factor) where the row factor has r levels and the column factor has c levels. The
obtained data are displayed as shown in the following table, where nij represents
the number of data values in row i and column j. This display of the data is called
a contingency table. Our interest here is to test for independence of two methods of
376
CHAPTER 7 Goodness-of-Fit Tests Applications

classifications of observed events. For example, we might classify a sample of stu-
dents by male or female and by their grade on a statistics course in order to test the
hypothesis that the grades are independent of gender. More generally the problem is
to investigate a dependency (or contingency) between two classification criteria.
In the present study, the given data of a problem are presented in a tabular form as
illustrated by the table below:
Levels of column factor
1
2
. . .
c
Row total
Row 1
n11
n12
n1c
n1
Levels 2
n21
n21
n2c
n2


r
nr1
nr2
nrc
nr
Column totals
n.1
n.2
n.c
N
where N¼P
j¼1
c n. j¼P
i¼1
r
ni.¼P
i¼1
r
P
j¼1
c nij is the grand total.
Here, we wish to test the hypothesis that the two factors (rows and columns) are
independent. We summarize the procedure in the following table for testing that
the factors represented by the rows are independent with those represented by
the columns.
TESTING FOR THE INDEPENDENCE OF TWO FACTORS
To Test
H0 : The factors are independent
versus
Ha : The factors are dependent
the test statistic is,
Q2 ¼
X
r
i¼1
X
c
j¼1
Oij Eij

2
Eij
,
where
Oij ¼ nij
and
Eij ¼ ninj
N :
Then under the null hypothesis the test statistic Q2 has an approximate chi-square probability
distribution with (r1)(c1) degrees of freedom.
Hence, the rejection region is Q2>wa,(r1)(c1)
2
.
Assumption: Eij5.
377
7.2 The Chi-Square Tests for Count Data

EXAMPLE 7.2.4
The following table gives a classification according to religious affiliation and marital status for 500
randomly selected individuals (Table 7.1).
Using a level of significance, a¼0.01, test the null hypothesis that marital status and religious
affiliation are independent.
Solution
We need to test the hypothesis
H0 : Maritalstatusandreligiousaffiliationareindependent
versus
Ha : Maritalstatusandreligiousaffiliationaredependent:
Here, c¼5, and r¼2. For a¼0.01, and for (c–1)(r–1)¼4 degrees of freedom, we have
w2
0:01,4 ¼ 13:2767:
Hence, the rejection region is Q2>13.2767.
We have Eij ¼ ninj
N : Thus,
E11 ¼ 116
ð
Þ 211
ð
Þ
500
¼ 48:952; E12 ¼ 116
ð
Þ 80
ð
Þ
500
¼ 18:5;
E13 ¼ 116
ð
Þ 56
ð
Þ
500
¼ 12:992; E14 ¼ 116
ð
Þ 98
ð
Þ
500
¼ 22:736;
E15 ¼ 116
ð
Þ 55
ð
Þ
500
¼ 12:76; E21 ¼ 384
ð
Þ 211
ð
Þ
500
¼ 162:05;
E22 ¼ 384
ð
Þ 80
ð
Þ
500
¼ 61:44; E23 ¼ 384
ð
Þ 56
ð
Þ
500
¼ 43:008;
and
E24 ¼ 384
ð
Þ 98
ð
Þ
500
¼ 75:264; E25 ¼ 384
ð
Þ 55
ð
Þ
500
¼ 42:24:
The value of the test statistic is
Q2 ¼
X
r
i¼1
X
c
j¼1
Oij Eij

2
Eij
¼
3948:952
ð
Þ2
48:952
+ 1918:5
ð
Þ2
18:5
+ 1212:992
ð
Þ2
12:992
+ 2822:736
ð
Þ2
22:736
"
#
+ 1812:76
ð
Þ2
12:76
+ 172162:05
ð
Þ2
162:05
+ 6161:44
ð
Þ2
61:44
+ 4443:08
ð
Þ2
43:08
+ 7075:264
ð
Þ2
75:264
+ 3742:24
ð
Þ2
42:24
¼ 7:1351:
Table 7.1 Marital Status and Religious Affiliation
Religious Affiliation
Total
A
B
C
D
None
Marital Status
Single
39
19
12
28
18
116
With spouse
172
61
44
70
37
384
Total
211
80
56
98
55
500
378
CHAPTER 7 Goodness-of-Fit Tests Applications

Because the observed value of Q2 does not fall in the rejection region, we do not reject the null
hypothesis at a¼0.01. Therefore, based on the given data, the marital status and religious affiliation
are independent. Note that the assumption of Eij5 is satisfied.
EXERCISES 7.2
7.2.1. If we toss a coin few times, we expect it half heads and half tails. Suppose we
tossed a coin 200 times and obtained 104 heads. Can we assume the coin
is fair?
7.2.2. The following table gives the opinion on collective bargaining by a random
sample of 200 employees of a school system, belonging to a teachers’ union.
Opinion on Collective Bargaining by Teachers’ Union
For
Against
Undecided
Total
Staff
30
15
15
60
Faculty
50
10
40
100
Administration
10
25
5
40
Column totals
90
50
60
200
Test the hypotheses
H0 : Opinion on collective bargaining is independent of employee classification
versus
Ha : Opinion on collective bargaining is dependent on employee classification
using a ¼ 0:05:
7.2.3. A random sample was taken of 300 undergraduate students from a university.
The students in the sample were classified according to their gender and
according to the choice of their major. The result is given in the
following table.
College
Gender
Arts and sciences
Engineering
Business
Other
Total
Male
75
40
24
66
205
Female
45
12
15
23
95
Total
120
52
39
89
300
Test the hypothesis that the choice of the major by undergraduate students in
this university is independent of their gender. Use a¼0.01.
7.2.4. A presidential candidate advertises on TV by comparing his positions on
some important issues with those of his opponent. After a series of
advertisements, a pollster wants to know whether people have changed their
opinion about the candidate. The following are the data based on a survey of
950 randomly chosen people:
379
7.2 The Chi-Square Tests for Count Data

Before the Advertisement Was Shown
Support the
Candidate
Oppose the
Candidate
Need to Know More
About the Candidate
Undecided
40%
20%
5%
35%
After the Advertisement Was Shown
Support the
Candidate
Oppose the
Candidate
Need to Know More
About the Candidate
Undecided
45%
25%
2%
28%
Let pi, i¼1, 2, 3, 4, represent the respective true proportions.
Test
H0 : p1 ¼ 0:35; p2 ¼ 0:20; p3 ¼ 0:15; p4 ¼ 0:3
versus
Ha : At least one of the probabilities is different from the hypothesized value:
Test this hypothesis using a¼0.05.
7.2.5. A survey of footwear preferences of a random sample of 100 undergraduate
students (50 females and 50 males) from a large university resulted in the
following data.
Boots
Leather shoes
Sneakers
Sandals
Other
Female
12
9
12
10
7
Male
10
12
17
7
4
(a) Let pi, i¼1, 2, 3, 4, 5, represent the respective true proportions of
students with a particular footwear preference, and let
H0 : p1 ¼ 0:20; p2 ¼ 0:20; p3 ¼ 0:30; p4 ¼ 0:20; p5 ¼ 0:10
versus
Ha : At least one of the probabilities is different from the hypothesized value:
Test this hypothesis using a¼0.05.
(b) Test the hypothesis that the choice of footwear by undergraduate
students in this university is independent of their gender, using a¼0.05.
7.2.6 A casino game involves rolling three dice. The winning are directly
proportional to the total number of sixes rolled. Suppose a gambler plays the
game 150 times, with the following observed counts:
Number of sixes
0
1
2
3
Number of rolls
72
51
21
6
Assuming that roll of one die does not affect the roll of others, test if the
dice are fair.
380
CHAPTER 7 Goodness-of-Fit Tests Applications

7.3 GOODNESS-OF-FIT TESTS TO IDENTIFY THE
PROBABILITY DISTRIBUTION
In this section, we will explain a few popular goodness-of-fit tests used to identify the
pdf that characterizes the behavior or fits a given set of data.
7.3.1 PEARSON’S CHI-SQUARE TEST
When we are interested in studying the behavior of a given unknown phenomenon,
we begin by obtaining thorough experimentation or other means a set of data, the
random sample. The initial step of studying this phenomenon is to try to identify
the probability distribution that characterizes the behavior of the given data. The
methods that we use are called goodness-of-fit test. That is, if we assume that a given
set of data follows the normal or Gaussian probability distribution, the data must be a
good fit to this distribution with a high degree of assurance. Historically the first sta-
tistical method to identify the pdf for a given set of data was Person’s Chi-Square
Goodness-of-fit tests.
In hypothesis testing problems we often assume that the form of the population
distribution is known. For example, in a w2-test for variance, we assume that the pop-
ulation is normal. The goodness-of-fit tests examine the validity of such an assump-
tion if we have a large enough sample. We now describe the goodness-of-fit test
procedure for such an application. This test uses a measure of goodness-of fit which
is the mean of the differences between the observed and expected outcome frequen-
cies (counts of observations), each squared and divided by the expectation. That is,
the test statistic is given by:
Q2 ¼
X
k
i¼1
Oi Ei
ð
Þ2
Ei
:
Here, Oi is the ith observed outcome frequency (in class i), Ei is the ith expected
(theoretical) frequency and i¼1,2,. . .,k is the number of classes. The expected
frequency, Ei is calculated by,
Ei ¼ F0 yu
ð
ÞF0 yl
ð Þ
½
n,
where F0 is the cumulative probability distribution that is being tested (assumed) to
determine if the given data follows (fits) this probability distribution, Yu and Yl are
the upper and lower limit of class i, respectively and n is the sample size. Thus we
proceed to setup the hypothesis,
H0 : The given data follow a specific probability distribution F
ð Þ
versus
Ha : The data do not follow the specified probability distribution
We proceed to calculate the value of the Q2 statistic and if it is greater than the value
we obtain from the wa,k1
2
tables for a given level of significance a and k1 degrees
381
7.3 Goodness-of-Fit Tests to Identify the Probability Distribution

of freedom, we reject the hypothesis. That is, the data does not follow or fit the spec-
ified probability distribution. Thus, if the calculated value of the chi-square test sta-
tistic is less than the wa,k1
2
value that we obtain from the tables, indeed the specified
data fits the specified probability distribution at a level of significance a. That is, the
critical region is given by
P Q2  w2
a,k1


¼ a
The basic assumptions for applying this test are
i. The observed frequencies in the k classes should be independent.
ii. P
i¼1
k
Ei ¼P
i¼1
k
Oi ¼n
iii. The total frequency, n, should be approximately more than 50.
iv. Each expected frequency, Ei, in each class should be at least 5.
In testing the above hypothesis we usually assume a value of the level of signif-
icance a, like a¼0.01,0.05, 0.1, etc. and proceed to make the decision of accepting
or rejecting the null hypothesis based on the assumed a. However, by using statistical
packages such as R it gives you a p-value, in contrast to fixed a value, that is calcu-
lated based on the test statistic, and denotes the threshold value of the significance
level in the sense that the null hypothesis, will be accepted at all significance a level
less than the calculated p-value. For example, if p¼0.05, the null hypothesis will not
be rejected for all values of assumed a<p-value of 0.05, and will be rejected for
higher levels. Given below is a summary of a step-by-step procedure for applying
the subject test.
GOODNESS-OF-FIT TEST PROCEDURES FOR PROBABILITY
DISTRIBUTIONS
Let X1,. . .,Xn be a sample from a population with cumulative distribution function (cdf) F(x), which
may depend on the set of unknown parameter y. We wish to test H0:F(x)¼F0(x), where F0(x) is
completely specified (assumed) pdf.
1. Divide the range of values of the random variables X1 into k non-overlapping intervals I1,I2,. . .,
Ik. Let Oj be the number of sample values that fall in the interval Ij (j¼1, 2, . . ., k).
2. Assuming the distribution of X to be F0(x), find P(X2Ij). Let P(X2Ij)¼pi. Let ej¼npj be the
expected frequency.
3. Compute the test statistic Q2 given by
Q2 ¼
X
k
i¼1
Oi Ei
ð
Þ2
Ei
:
The test statistic Q2 has an approximate w2- distribution with (k–1) degrees if freedom.
4. Reject the H0 if Q2 xa(k1)
2
.
5. Assumptions: Ej 5, j¼1, 2, . . ., k.
Note that if the observed data, Oi is very close to the expected value Ei, the dif-
ference OiEi is going to be very small which implies the Q2 statistics will be small
and thus, a good fit of the given data to the assumed pdf.
382
CHAPTER 7 Goodness-of-Fit Tests Applications

EXAMPLE 7.3.1
We are given a random sample of n¼30 observations of a given experiment of a certain phenom-
enon of interest, that is,
1.79
2.62
7.92
9.77
12.13
15.04
16.14
20.74
22.73
23.29
24.97
26.12
27.06
29.60
32.47
36.32
42.18
45.06
45.64
48.34
48.87
64.99
66.28
68.00
68.60
75.34
99.32
162.48
164.38
235.95
We believe that this data may follow the exponential pdf. Test our belief at a¼0.05.
Solution
We need to test
H0 : The given data follow an expontential probability distribution
versus
Ha : The data do not follow the specified probability distribution
We shall use the w2 goodness-of-fit test given above to test the stated hypothesis. By using the
following R-code we can perform the subject hypothesis
R-code
x¼c(1.79,2.62,7.92,9.77,12.13,15.04,16.14,20.74,22.73,23.29,24.97,26.12,
+ 27.06,29.60,32.47,36.32,42.18,45.06,45.64,48.34,48.87,64.99,66.28,68.00,
+ 68.60,75.34,99.32,162.48,164.38,235.95)
chisq.test(cbind(x,dexp(x)))
Output
Pearson’s Chi-squared test
X-squared¼104.0212, df¼29, p-value¼2.201e-10
Thus, for a p-value of 2.201e-10, we reject the null hypothesis and we conclude that the given
data does not follow the exponential pdf.
EXAMPLE 7.3.2
The grades of students in a class of 200 are given in the following table. Test the hypothesis that the
grades are normally distributed with a mean of 75 and a standard deviation of 8. Use a¼0.05.
Range
0-59
60-69
70-79
80-89
90-100
Number of students
12
36
90
44
18
Solution
To test the hypothesis,
H0 : Student grades are normally distributed:
versus
Ha : Student grades are not normally distributed:
We have O1¼12, O2¼36, O3¼90, O4¼44, O5¼18.
We now compute pi(i¼1, 2, . . ., 5), using the continuity correction factor,
p1 ¼ P X  59:5jH0
f
g ¼ P z  59:575
8


¼ 0:0262,
p2 ¼ 0:2189, p3 ¼ 0:4722, p4 ¼ 0:2476, p5 ¼ 0:0351,
and
Continued
383
7.3 Goodness-of-Fit Tests to Identify the Probability Distribution

E1 ¼ 5:24, E2 ¼ 43:78, E3 ¼ 94:44, E4 ¼ 49:52, E5 ¼ 7:02:
The test statistic results in
Q2 ¼
X
n
i¼1
Oi ei
ð
Þ2
ei
¼ 125:74
ð
Þ2
5:74
+ 3643:78
ð
Þ2
43:78
+ 9094:44
ð
Þ2
94:44
+ 4449:52
ð
Þ2
49:52
+ 187:02
ð
Þ2
7:02
¼ 26:22:
Q2 has a chi-square distribution with (5–1)¼4 degrees of freedom. The critical value is
w0.05,4
2
¼7.11. Hence, the rejection region is Q2>7.11. Because the observed value of
Q2¼26.22>7.11, we reject H0 at a¼0.05. Thus, we conclude that the given data do not follow
(or drawn) from the normal pdf.
7.3.2 THE KOLMOGOROV-SMIRNOV TEST: (ONE POPULATION)
Let Xi,i¼1,2,. . .,n be a random sample of n observations and we will assume is
drawn (it-follows) a probability distribution whose cumulative distribution is spec-
ified to be F0(x). Our objective now is to determine if the actual (correct) cumulative
probability is F(x) based on the assumed F0(x). That is, we wish to test the following
hypothesis:
H0 : The true probability distribution that follows the given data, F x
ð Þ,
is actually the assumed distribution F0 x
ð Þ
versus
Ha : The actual cumulative distribution,F x
ð Þ is not F0 x
ð Þ,
based on level of significance a.
The Kolmogorov-Smirnov goodness-of-fit test to test the above hypothesis is
based on the following test statistic
D ¼ Max F0 x
ð ÞFn x
ð Þ
j
j:
PROCEDURE TO CALCULATE D
To calculate the value of the test statistic D, we follow the following three steps.
1. We calculate the assumed cumulative distribution, F0(x), based on the given data of observations
and the specified population distribution.
2. We proceed to obtain the cumulative distribution of the sample, Fn(x) is the empirical distribu-
tion function defined as a step function,
Fn x
ð Þ ¼ Xi x
n
,
the number of observations Xix divided by n.
3. We find the absolute difference
F0 x
ð ÞFn x
ð Þ
j
j:
Thus, we have a value of the test statistic D and if:
D  Da:
384
CHAPTER 7 Goodness-of-Fit Tests Applications

We accept the hypothesis, where Da is the critical value from the Kolmogorov-
Smirnov tables that is based on a given a and n. The following example illustrates
how we apply this test.
EXAMPLE 7.3.3
From a large statistics class, we have taken a random sample of fifty five students, n¼55 and
recorded their ages. The resulted data are:
27
25
24
24
22
20
21
22
21
25
24
26
25
24
23
22
20
21
19
21
25
24
26
25
22
23
22
22
21
19
21
23
21
26
24
22
23
22
22
20
19
21
23
21
26
24
22
23
21
19
20
18
20
20
18
We believe that this data follows the normal pdf and wish to use the Kolmogorov-Smirnov
goodness-of-fit test, given above to test our believe. That is, test,
H0 : The ages of the students follow the normal probability distribution
versus
Ha : The ages of students do not follow the normal probability distribution:
Solution
It usually helps to obtain a possible visual indication of the pdf by structuring a histogram of the
given data (Figure 7.1). That is, visually, it seems that the normal pdf is a good possibility. We shall
now test it statistically.
The sample mean is x ¼ 22 and the sample standard deviation is s¼2.08. The three steps pro-
cedure of the subject test to obtain the value of the test statistic D can be easily calculated using the
following table,
Continued
26
24
22
20
18
10
8
6
4
2
0
Age
Frequency
Histogram of age
FIGURE 7.1
Histogram of the ages.
385
7.3 Goodness-of-Fit Tests to Identify the Probability Distribution

Row
Age
F0(x)
Fn(x)
jF0(x)Fn(x)j
D
Critical Value
1
18
0.028
0.018
0.010
0.127
0.183
2
18
0.028
0.036
0.009
3
19
0.071
0.055
0.017
4
19
0.071
0.073
0.001
5
19
0.071
0.091
0.019
6
19
0.071
0.109
0.038
7
20
0.155
0.127
0.028
8
20
0.155
0.145
0.010
9
20
0.155
0.164
0.009
10
20
0.155
0.182
0.027
11
20
0.155
0.200
0.045
12
20
0.155
0.218
0.063
13
21
0.286
0.236
0.050
14
21
0.286
0.255
0.032
15
21
0.286
0.273
0.013
16
21
0.286
0.291
0.005
17
21
0.286
0.309
0.023
18
21
0.286
0.327
0.041
19
21
0.286
0.345
0.059
20
21
0.286
0.364
0.078
21
21
0.286
0.382
0.096
22
21
0.286
0.400
0.114
23
22
0.454
0.418
0.036
24
22
0.454
0.436
0.018
25
22
0.454
0.455
0.000
26
22
0.454
0.473
0.018
27
22
0.454
0.491
0.037
28
22
0.454
0.509
0.055
29
22
0.454
0.527
0.073
30
22
0.454
0.545
0.091
31
22
0.454
0.564
0.109
32
22
0.454
0.582
0.127
33
23
0.631
0.600
0.031
34
23
0.631
0.618
0.013
35
23
0.631
0.636
0.005
36
23
0.631
0.655
0.023
37
23
0.631
0.673
0.041
38
23
0.631
0.691
0.059
39
24
0.784
0.709
0.075
40
24
0.784
0.727
0.057
41
24
0.784
0.745
0.039
42
24
0.784
0.764
0.020
43
24
0.784
0.782
0.002
44
24
0.784
0.800
0.016
45
24
0.784
0.818
0.034
46
25
0.892
0.836
0.055
47
25
0.892
0.855
0.037

Since the D-statistic¼0.127<Da¼0.05¼0.183, we fail to reject the null hypothesis at the level
of significance a¼0.05. Thus, the ages of the students in the class indeed follows the normal pdf.
Also, we can easily calculate the Kolmogorov-Smirnov test statistics and the p-value using
R-code and the output is given below:
Ks.test(x,pnorm,mean(x),sd(x));
Output
One-sample Kolmogorov-Smirnov test
Data: x
D¼0.1271, p-value¼0.7085
Alternative hypothesis: two-sided
Since p-value is large, we cannot reject the null hypothesis.
7.3.3 THE ANDERSON-DARLING TEST
The Anderson-Darling goodness-of-fit test is also used to determine if a given set
of data is drawn from a population that follows a specific probability distribution.
Let Xi, i¼1,2,. . .,n, be a random sample of observations and Yi, i¼1,2,. . .,n
is the corresponding ordered value according to size. The hypothesis that we wish
to test is
H0 : The given data follow a specific probability distribution
versus
Ha : The given data do not follow the specified probability distribution:
The Anderson-Darling test statistic for testing the above hypothesis is given by
A2 ¼ ns,
where s ¼
Xn
i¼1
2i1
ð
Þ
n
lnF Yi
ð
Þ + ln 1F Yn + 1i
ð
Þ
ð
Þ
½
 n is the random sample size,
Yi the ordered data and F the specified probability distribution that we are testing.
For a given level of significance a, the hypothesis is rejected if the value of the test
statistic A is greater than the critical value Aa, that is, if
A > Aa:
Thus, we reject the null hypothesis in favor of the alternative hypothesis; the spec-
ified probability distribution does not fit the distribution of the drawn data from the
population. The Aa is obtained from the Anderson-Darling tables for a given a. The
following example illustrates how we apply the subject test.
48
25
0.892
0.873
0.019
49
25
0.892
0.891
0.001
50
25
0.892
0.909
0.017
51
26
0.954
0.927
0.027
52
26
0.954
0.945
0.009
53
26
0.954
0.964
0.010
54
26
0.954
0.982
0.028
55
27
0.984
1.000
0.016
387
7.3 Goodness-of-Fit Tests to Identify the Probability Distribution

EXAMPLE 7.3.4
Use ages of the 55 students given in Example 7.3.3 to illustrate the applicability of the Anderson-
Darling goodness-of-fit test.
Solution
The data is given in Example 7.3.3 and proceed to test our belief that the students’ ages follow the
normal pdf.
The Anderson-Darling statistic is A¼0.646 with a p-value of 0.087. Thus, at 5% level of sig-
nificance we fail to reject the null hypothesis. The data fits normal distribution with mean 22 and
standard deviation 2.
7.3.4 SHAPIRO-WILK NORMALITY TEST
The Shapiro-Wilk goodness-of-fit test is used to determine if a random sample Xi,
i¼1,2,. . .,n, is drawn from a Normal Gaussian probability distribution with true
mean and variance, m and s2, respectively. That is, XN(m,s2). Thus, we wish to
test the following hypothesis:
H0 : The random sample was drawn from a normal population, N m, s2


versus
Ha : The random sample does not follow N m, s2


:
To test this hypothesis we use the Shapiro-Wilk test statistic which is given by
W ¼
Xn
i¼1aix ið Þ

2
Xn
i¼1 xi x
ð
Þ2 ,
where x(i) are the ordered sample values and ai are constants generated by the
expression
a1, a2, ..., an
ð
Þ ¼
mT V1
mT V1m

1=2
with m¼(m1,m2,. . ..,mn)T are the expected values of the ordered statistics that are
independent and identically distributed random variables that follow the standard
normal, N(0,1), and V is the covariance matrix of the order statistics.
EXAMPLE 7.3.5
Proceed to use the Shapiro-Wilk normality test for the data of Example 7.3.3 that we used the
Anderson-Darling goodness-of-fit test to see if the ages of the students follow the normal pdf.
Use a¼0.05.
Solution
The R-code for the subject test is
Shapiro.test(x)
Output
Shapiro-Wilk normality test
388
CHAPTER 7 Goodness-of-Fit Tests Applications

Data: x
W¼0.9683, p-value¼0.1551
Thus, since the p-value is larger than 0.05, we fail to reject the null hypothesis and the ages of
the students indeed follow the normal pdf.
7.3.5 THE P-P PLOTS AND Q-Q PLOTS
We commonly use a visual interpretation of graphs (plots) to determine if a given
random sample of data follows or is drawn from a well-known probability distribu-
tion. These graphs are the probability, P-P plots and the quantile, Q-Q plots.
The P-P plot is a graphical tool used to determine how well a given data set fits a
specific probability that we are testing. This plot compares the empirical cdfs of the
given data with that of the assumed true cumulative probability distribution functions.
If the plot of these two distributions is approximately linear; it indicates that the assumed
true pdf gives a reasonably good fit to the given data that we seek to find its true pdf.
Steps to construct the P-P plot
Let F(x) be the cumulative pdf of the random variable, X with a random sample
x(1),x(2),. . .,x(n) of ordered data values with associated probabilities ^C ið Þ ¼
i
n + 1 , the
scattered P-P plot is the plot of ^C ið Þ versus C(i)¼F[X¼x(i)], of the possibly true
cumulative pdf that we are testing.
STEPS FOR P-P PLOT
Step 1. Given a random sample x1,x2,. . .,xn sort the data in ascending order,
x 1
ð Þ,x 2
ð Þ, ...,x n
ð Þ:
Step 2. Associate with each of the order data value x(1) a cumulative probability,
^C ið Þ ¼
i
n + 1:
Step 3. Determine the hypothetical probabilities associated with the probability distribution we are
testing
C ið Þ ¼ F X ¼ x ið Þ

	
,
F x
ð Þ ¼ P X  x
½
,
where F(x) is the cumulative pdf.
Step 4. Construct the scatter plot of ^C ið Þ versus C(i)¼F[X¼x(i)].
Step 5. Interpret the plot, if the overall pattern follows approximately a straight line, then the data fol-
low the assumed probability distribution, and if the overall pattern has curvature or shelves,
then the data have skewed behavior and therefore it does not follow the assumed pdf.
EXAMPLE 7.3.6
Using the data of Example 7.3.3, obtain the P-P plot using R (Figure 7.2). The R-code is simply: PP
line (data).
Thus, the data fall on a straight line and we can conclude that the information of the ages of the
students follow the normal pdf.
Continued
389
7.3 Goodness-of-Fit Tests to Identify the Probability Distribution

The Q-Q plot is another graphical method that is commonly used to obtain a graphical indication
of the true pdf that the given data come from. This method is a graph of the quantiles of the empirical
distribution of the given data versus the quantiles of the assumed true pdf that we are testing. If the
resulting graph of these two distributions follows a linear pattern it indicates that the assumed pdf fits
the given data reasonably well.
STEPS TO OBTAIN Q-Q PLOTS
The Q-Q plot is obtained by following the step-by-step procedure given below:
Let F(x) be the assumed cumulative pdf of the random variable X, with a random sample x(1),
x(2),. . .,x(n) of ordered data values with associated probabilities ^C ið Þ ¼
i
n + 1:, the Q-Q plot is the
x ið Þ ¼ F1
^C ið Þ


, the inverse function of F(x).
Step 1. Given a random sample x1,x2,. . .,xn sort the data in ascending order,
x 1
ð Þ,x 2
ð Þ, ...,x n
ð Þ:
Step 2. Associate with each of the order data value x(1) a cumulative probability,
^C ið Þ ¼
i
n + 1:
Step 3. Determine the estimated value of the random variable associated with the assumed
probability distribution
x ið Þ ¼ F1 ^C ið Þ


where F(x) is the cumulative pdf.
Step 4. Construct the scatter plot of x(i) versus x^ið Þ ¼ F1 ^C ið Þ

	
.
Step 5. If the overall pattern follows approximately a straight line, then the data follows the
assumed probability distribution. If the overall pattern has curvature or shelves, then
thedatahasskewedbehaviorandit doesnotfollowtheassumedprobabilitydistribution.
0.0
0.0
0.2
0.4
0.6
0.8
Observed cum prob
Normal P-P plot of age
Expected cum prob
1.0
0.2
0.4
0.6
0.8
1.0
FIGURE 7.2
P-P plot of the ages.
390
CHAPTER 7 Goodness-of-Fit Tests Applications

EXAMPLE 7.3.7
We shall use the data given in Example 7.3.3, the ages of 55 students to construct the Q-Q plot with
R. The R-code is simply qq line (data).
Solution
The results are given in Figure 7.3. Note that the plot follows approximately a straight line, which
suggest that the data follow the normal pdf, which we have also proven using two other goodness-
of-fit tests.
EXERCISES 7.3
7.3.1. The speeds of vehicles (in mph) passing through a section of Highway 75 are
recorded for a random sample of 150 vehicles and are given below. Test the
hypothesis that the speeds are normally distributed with a mean of 70 and a
standard deviation of 4. Use a¼0.01.
Range
40-55
56-65
66-75
76-85
>85
Number
12
14
78
40
6
7.3.2. Based on the sample data of 50 days contained in the following table, test the
hypothesis that the daily mean temperatures in the City of Tampa are
normally distributed with mean 77 and variance 6. Use a¼0.05.
City of Tampa
Temperature
46-55
56-65
66-75
76-85
86-95
Number of days
4
6
13
23
4
7.3.3. A sample of 30 electronic circuit components is randomly selected from a
production process. The life time, in hours, of each component is precisely
measured by testing it until it fails. The times in hours that it took the
component to fail is given below:
0.0
0.0
0.2
0.4
Cumulative distribution of age
Normal (Mu-22.255 Sigma-2.2213)
0.6
0.8
1.0
0.2
0.4
0.6
0.8
1.0
FIGURE 7.3
Q-Q plot for the ages.
391
7.3 Goodness-of-Fit Tests to Identify the Probability Distribution

268.276 420.559 6.590 78.389 14.123 85.507 216.594 39.892 9.468
83.088
519.682 315.754 139.046 4.522 81.480 209.099 170.128 77.794 115.778
108.640
226.053 443.029 35.662 115.668 5.032 17.357 331.462 184.734 79.502
67.019
Using the Pearson’s chi-square goodness-of-fit test, test the hypothesis that
the life times of the components follow an exponential distribution with
mean of 200 hours. Use a¼0.05.
7.3.4. For the data given in Example 7.3.3, test the goodness-of-fit that the data
follows:
(a) the gamma pdf and
(b) the Weibull pdf.
7.3.5. Using the data given in Example 7.3.1, construct the P-P plot and interpret
the meaning of the graph.
7.3.6. For the data given in Example 7.3.2, construct the P-P plot and interpret its
meaning.
7.3.7. Using the data given in Example 7.3.1, construct the graph of the Q-Q plot
and interpret its meaning.
7.3.8. For the data given in Example 7.3.2, construct the graph of the Q-Q plot and
interpret its meaning.
7.4 APPLICATIONS: PARAMETRIC ANALYSIS
In this section, we will use the goodness-of-fit methods discussed in the previous
sections to identify the probability distribution that characterizes the behavior of
some real world problems that our society is facing. All the data sets used in this
section are available at http://booksite.elsevier.com/9780124171138.
7.4.1 GLOBAL WARMING
The concept of “Global Warming” consists of two interacting entities, the atmo-
spheric temperature and carbon dioxide, CO2, in the atmosphere. The United States
collects annual data for both of these variables in our observatories in Alaska and
Hawaii. The actual data can be found on the website: http://scrippsco2.ucsd.edu/
data/atmosperic-co2.html.
Our objective is to identify the probability distribution function, pdf, that follows
the CO2 data that is given in thousands in metric tons annually for 31 years. Once we
know the pdf that fits the CO2 data, we can obtain useful information, such as, prob-
abilistic characterization of its behavior, the expected value of CO2 (theoretical aver-
age), obtain confidence limits on the true amount of CO2, among other interesting
information.
We begin our process of identifying the pdf by structuring a histogram of the 31
randomly selected measurements of CO2. The histogram of the subject data will
392
CHAPTER 7 Goodness-of-Fit Tests Applications

give us some idea about the possible pdf that we should be testing. After some pre-
liminary testing of some pdf’s we proceeded to test the following hypothesis:
H0 : The CO2 data follow the gamma pdf
versus
Ha : The CO2 data do not follow the gamma pdf:
To test this hypothesis we applied the Kolmogorov-Smirnov, Anderson-Darling, and
the Chi-Square tests with a level of significance a¼0.05. The test statistic results of
the three goodness-of-fit tests are given below:
Kolmogorov-Smirnov Test
:
D¼0.08771, p-value 0.954
Anderson-Darling Test
:
A¼0.3627, p-value 0.883
Chi-Square Test
:
w2¼0.95844, p-value 0.811
All three goodness-of-fit tests strongly support the null hypothesis that the CO2 mea-
surements follow the gamma pdf. We obtained the maximum likelihood estimates of
the two parameters a and b of the gamma pdf as a¼635.29 and ^b¼0.557. Thus, we
can write the gamma pdf for the subject data. That is,
f x
ð Þ ¼
x635:291
0:557
ð
Þ635:29 635:29
ð
Þ
exp
x
0:557, 0 < x


:
We can use f(x) to determine various probabilities of interest concerning the behavior
of X, the amount of CO2 in the atmosphere. Also, we can calculate F(x), the expected
amount that we would find in the atmosphere, confidence limits, among other inter-
esting questions about the behavior of CO2.
7.4.2 HURRICANE KATRINA
One of the most devastating hurricanes in the last 100 years to hit the United States
was Hurricane Katrina. The Atlantic based hurricane, category 5, (most devastating)
lasted nine (9) days, August 23-31, 2005. The pressure wind velocity of Katrina in
one of the most important variables and we wish to identify the pdf that characterizes
its behavior. That is, perform goodness-of-fit testing to determine the pdf that follows
the pressure wind data that was obtained from: http://weather.unisys.com/hurricane/
atlantic/2005H/KATRINA/track.dat
We have 63 observations of the wind velocity, mph, that reached a maximum
wind velocity of 150 miles per hour. After looking at the histogram of the data,
we believe that the wind velocity of Katrina, follows the two parameter (d¼0), Wei-
bull pdf. Thus, we proceeded to test the following hypothesis:
H0 : The wind velocity data of Hurricane Katrina follow the two parameter Weibull pdf
versus
Ha : The wind velocity data of Hurricane Katrina do not follow the Weibull pdf:
To test this hypothesis we applied the Kolmogorov-Smirnov, Anderson-Darling, and
the Chi-Square tests.
393
7.4 Applications: Parametric Analysis

All these tests strongly support the acceptance of the null hypothesis. The test
results are given below:
Kolmogorov-Smirnov Test
:
D¼0.0792, p-value 0.795
Anderson-Darling Test
:
A¼0.5949, p-value 0.863
Chi-Square Test
:
w2 ¼ 3.4031, p-value 0.638
Thus, the wind velocity measurements of Hurricane Katrina follows the two param-
eter Weibull pdf with the maximum likelihood estimates of the parameter given by
^a¼2.1281 and ^b¼86.376. The pdf of the subject data is given by:
f x
ð Þ ¼ 86:376
2:1281
x
2:1281

85:376
exp
x
2:1281

86:376
for x>0 and zero elsewhere, a graphical display of f(x) is given in Figure 7.4.
Knowing the pdf that characterize the two probabilistic behavior of the wind
velocity of Katrina, we can calculate the expected wind velocity and confidence
limits. That is,
E X
ð Þ ¼ 76miles per hour
and the 95% confidence limits of the true mean of the wind velocity is between 23.8
and 150.6 miles per hour.
That is, we are at least 95% certain that the true wind velocity of Hurricane
Katrina or similar hurricanes will between 23.8 and 150.6 miles per hour.
200
150
100
50
0
0.010
0.008
0.006
0.004
0.002
0.000
Wind Velocity
Density
Weibull Probability Denisty Function of Wind Velocity
Shape=2.1281, Scale=86.376
FIGURE 7.4
Weibull pdf of wind velocity of Hurricane Katrina.
394
CHAPTER 7 Goodness-of-Fit Tests Applications

Also, the cumulative probability distribution, F(x), of the wind velocity in its ana-
lytical and graphical form is given below:
F x
ð Þ ¼ P X  x
ð
Þ ¼
ðx
0
b
a
t
a
 b1
e t
a
 b
dt, 0 < x
and for the given data,
F x
ð Þ ¼
ðx
0
86:37
2:128
t
2:128

85:37

b1
e
t
2:128

86:37
dt:
Thus, we can use the graph (see Figure 7.5) to obtain various probabilities, for exam-
ple if we are interested in the probability that the wind velocity of category 5 hur-
ricane is less than 150 miles per hour we can obtain an approximate estimate
from the graph above, that is,
F 150
ð
Þ ¼ P X  150
½
  0:93:
This means that based on the given data we are approximately 93% certain that the
wind velocity will be less than 150 miles per hour.
Given below in the R-code for the goodness-to-fit tests for the Katarina data.
HK<read.delin(“/Documents/Hurricane Katrina.txt”)
View(HK)
summary(HK) #### descriptive Stat###
xk¼HK$WIND
hist(xk) ### Histogram ####
library(lessR)
dens(xk,type¼c(“both”,”normal”),xlab¼”Wind”,ylab¼”f(x)”)
200
150
100
50
0
100
80
60
40
20
0
x
F(x)
Weibull 
Weibull CDF of Wind Velocity
FIGURE 7.5
Weibull cdf of wind velocity of Hurricane Katrina.
395
7.4 Applications: Parametric Analysis

color.density(xk)
m¼mean(xk);m
std¼sqrt(var(xk));std hist(xk),density¼12,breaks¼8,prob¼T,col¼”
plum4”,xlab¼”Wind”,xlim¼c(0,200),main¼”Hi
stogram of Wind velocity of Hurricane Katrina”)
library(vcd) ## Goodness of fit test
fitdistr(xk,’weibull’) ### estimate the parameters using MLE ks.test
(xk,”pweibull”,shape¼1.805,scale¼52.323) #Kolmogorov-Smirnov test
ad.test(xk)#Anderson-Darling
7.4.3 NATIONAL UNEMPLOYMENT
The aim in the present problem is to identify the probability distribution that char-
acterizes the rates of unemployment in the US. The subject data was obtained from
the U.S. Bureau of Labor Statistics, www.bls.gov/, under Database & Tools. The data
in the annual average of unemployment rate in the US from 1957 to 2008. Initially,
we looked at the histogram of the data and it gave us a visual interpretation that it may
follow the gamma pdf. Initially we tested for the two parameter gamma pdf and
obtained a fairly good fit, but when we tried the three parameter gamma pdf, we
obtained a better fit. That is,
H0 : The annual average rates of unemployment in the U:S:follow the three
parameter gamma pdf:
versus
Ha : The subject data do not fit the three parameter gamma pdf:
Given below is the value of the goodness-of-fit test statistics for a sample of 51 data
points:
Kolmogorov-Smirnov Test
:
D¼0.0847, p-value 0.8276
Anderson-Darling Test
:
A¼0.3424, p-value 0.7916
Chi-Square Test
:
w2 ¼ 2.2353, p-value 0.8172
All three goodness-of-fit tests strongly support that the three parameter gamma pdf is
probabilistically the best to characterize the behavior of the U.S. annual average of
unemployment with the maximum likelihood estimate of the parameter ^a¼5.5871,
^b¼0.5954, and ^g¼2.5008. Thus, the subject pdf is given by
f x
ð Þ ¼
1
0:5954
ð
Þ5:5871 G 0:5954
ð
Þ
x2:5008
ð
Þ4:5871exp x2:5008
0:5954


, 0 < x:
The expected value of the subject pdf is
E X
½  ¼ ^d + ^a ^b
 
¼ 2:5008 + 5:5871
ð
Þ 0:5954
ð
Þ ¼ 5:83:
Thus, one will expect that the unemployment rate to be approximately 5.83% based
on the actual date we analyzed. A graphical form of f(x) over the initial histogram that
guided us to the three parameter gamma pdf is shown in Figure 7.6.
396
CHAPTER 7 Goodness-of-Fit Tests Applications

We can use the pdf to obtain confidence limits on the true rate of unemployment,
the cumulative pdf, F(x), to obtain various probabilities of interest on the subject
problem, among other useful information.
7.4.4 BRAIN CANCER
A brain tumor is an abnormal growth of cells within the brain, which can be cancerous
(malignant) or benign. It is estimated that there are more than 43,800 new cases of brain
cancerous tumors in the U.S. during the last few years. In this application we are inter-
ested instudyingthe behaviorofthemalignant tumorsizesinthebrain.Thesubject data
was obtained from the Surveillance Epidemiology and End Results (SEER) data base.
Wehave takena randomsampleof200 brain cancerpatientsfroma large databasewith
their cancerous tumor size measured in millimeters. Our aim is to find the probability
distributionthat characterizesthebehaviorofthetumor sizes.Thus,after testing several
pdf’s and looking at the histogram we believe that the three parameter Weibull pdf is
a prime candidate. The data we have provided for this book in the internet site http://
booksite.elsevier.com/9780124171138 contains another 250 random data values.
It should be noted that you will get different parameters and pdf for that data set.
Now, we proceed to test our belief.
H0 : The sizes of the malignant tumor in the brain fits the three parameter Weibull pdf:
Versus
Ha : The subject data do not follow the three parameter Weibull pdf:
We are applying the most commonly used goodness-of-fit tests to make a decision
concerning accepting or rejecting the stated hypothesis for say, a¼.01, 05, 0.10. The
results of the three tests are given below:
Kolmogorov-Smirnov Test
:
D¼0.0502, p-value 0.6746
Anderson-Darling Test
:
A¼0.6948, p-value 0.7321
Chi-Square Test
:
w2¼9.6143, p-value 0.2115
3.6
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.22
0.24
0.26
0.28
0.3
4
4.4
4.8
5.2
5.6
6
x
6.4
f(x)
6.8
7.2
7.6
8
8.4
8.8
9.2
9.6
a = 5.5871
ˆ
b = 0.5954
ˆ
g = 2.5008
ˆ
FIGURE 7.6
Three-parameter gamma pdf for unemployment in the United States.
397
7.4 Applications: Parametric Analysis

Thus, all three goodness-of-fit tests, for all level of significance, support the null
hypothesis that the cancerous tumor sizes of the brain follows the three parameter
Weibull pdf. The approximate maximum likelihood estimates of the three parameters
used are ^a¼9.4826E+7, ^b¼1.4060E+9, and ^g¼1.3940E+9. Thus, we can write the
pdf that characterizes probabilistically the malignant tumor sizes in the brain by
f x
ð Þ ¼ 9:4826E + 7
1:4060E + 9
x + 1:3940E + 9
1:4060E + 9

9:4826E + 71
exp  x + 1:3940E + 9
1:4060E + 9

9:4826E + 7
"
#
and the cumulative pdf is gen by
F x
ð Þ ¼ 1exp  x + 1:3940E + 9
1:4060E + 9

9:4826E + 7
"
#
, 0 < x:
A graphical illustration of the three parameter Weibull pdf along with a frequency
histogram of the data is shown in Figure 7.7.
We can use the above diagram to obtain approximate probabilities of the behavior
of the cancerous tumor sizes. For example, the probability that the tumor size is less
60 mm is approximately 25%, that is,
P X  60mm
½
 ¼ 0:25,
the probability that the tumor size is larger than 48 mm is approximately 74%, that is,
P X  48mm
½
 ¼ 1P X < 48mm
½
  0:74:
We also can proceed to obtain the expected value of the tumor size and approximate
confidence limits on the true size of the tumor, among other interesting information.
240
200
160
120
80
40
0
80
70
60
50
40
30
20
10
0
tumor size
Frequency
Histogram of tumor size
3-Parameter Weibull 
FIGURE 7.7
Three-parameter Weibull pdf for tumor size data.
398
CHAPTER 7 Goodness-of-Fit Tests Applications

7.4.5 RAINFALL
For the Southern Region of the U.S., we have the average annual rainfall data in
inches from 1975 to 2007. The actual data are given below in Table 7.2:
Using 33 measurements (n¼33), we wish, if possible to identify the probability
distribution function that probabilistically characterize the behavior of the average
annual rainfall of the Southern District of U.S. Having such pdf we can calculate
the amount of rain we will expect in the region, obtain confidence limits of the true
amount of annual rainfall, among other interesting questions.
From a preliminary view of the histogram of the data we believe that the rainfall
data follow the beta pdf.
Thus, let us proceed to test our belief:
H0 : We believe that the rainfall data follow the beta pdf:
versus
Ha : The subject data do not follow the beta pdf:
Given below are the goodness-of-fit results applying the three commonly used sta-
tistical tests:
Kolmogorov-Smirnov Test
:
D¼0.0773, p-value 0.9806
Anderson-Darling Test
:
A¼0.2098, p-value 0.8836
Chi-Square Test
:
w2 ¼ 0.2888, p-value 0.9905
For all commonly used level of significance, a¼0.01, 0.05, and 0.10, we strongly
accept the null hypothesis that our belief is true, that is, the given rainfall data follow
the beta pdf. The maximum likelihood estimates of the parameter a and b of the beta
pdf are ^a¼2.2823 and ^b¼1.8754. Thus, the beta pdf of the rainfall data is
f x
ð Þ ¼
G 2 + ^b


G 2
ð ÞG ^b
 x^a1 1x
ð
Þ
^b1, 0  x
or
f x
ð Þ ¼ 2:5237
5:7593x0:2823 1x
ð
Þ0:8754,
where
G 2:2823 + 1:8754
ð
Þ ¼ 2:5237 and G 2:2823
ð
ÞG 1:8754
ð
Þ ¼ 5:7593:
The graph of the subject pdf over the histogram of the data is shown in Figure 7.8.
The expected amount of average rainfall in the Southern Region is 4.2998 inches,
that is,
E x
ð Þ ¼
ð∝
0
xf x
ð Þdx ¼ 4:2998inches:
We can also calculate confidence limits around the true value of the annual average
rainfall. For example, we are at least 95% confident that the true annual average rain-
fall in the Southern District in between 4.0579 and 4.5167 inches.
399
7.4 Applications: Parametric Analysis

Table 7.2 Rainfall Data
Year
Rain
Year
Rain
Year
Rain
Year
Rain
Year
Rain
Year
Rain
1975
3.957
1981
3.68
1987
4.465
1993
4.175
1999
4.103
2005
5.22
1976
4.031
1982
5.224
1988
4.487
1994
4.889
2000
2.737
2006
3.526
1977
3.918
1983
5.639
1989
3.612
1995
5.468
2001
4.104
2007
3.211
1978
4.299
1984
3.563
1990
3.322
1996
3.668
2002
5.037
1979
4.942
1985
3.592
1991
4.463
1997
5.029
2003
4.633
1980
3.921
1986
4.307
1992
4.514
1998
4.73
2004
5.219
400
CHAPTER 7 Goodness-of-Fit Tests Applications

7.4.6 PROSTATE CANCER
In this application, we will study the behavior of the cancerous tumor in Prostate
Cancer patients. We shall use real prostate cancer data for white men from 1973
to 2007 from the Surveillance Epidemiology and End Results known as the SEER
Program. The tumor size is the random variable of interest for 20,645 prostate cancer
patients. Our primary objective is to identify the pdf that characterizes probabilisti-
cally the behavior of the cancerous tumor size in mm. From the initial structure of the
histogram we believe that the two-parameter Weibull pdf may fit the subject data.
Thus, we set up our hypothesis to test our belief,
H0 : The prostate cancerous tumorsizes followstheWeibull pdf:
versus
Ha : Thesubject data do not followtheWeibull pdf:
Applying the Kolmogorov-Smirnov, Anderson-Darling, and Chi-Squared tests, all
support the null hypothesis that the subject data follow the two-parameter Weibull
pdf. The maximum likelihood estimates of the parameter a and b that drive the Wei-
bull pdf are ^a¼0.8704 and ^b¼12.4403.
Thus, the two-parameter Weibull pdf is given by:
f x
ð Þ ¼ 0:8704
12:4403
x
12:4403

0:1296
exp 
x
12:4403

0:8704

, 0  x,
where x represents the size of the cancerous tumor in mm. The cumulative Weibull
pdf is useful in obtaining various probabilities of the size of the tumor and is given by
F x
ð Þ ¼ P X  x
½
 ¼ 1exp 
x
12:4403

0:8704

, 0  x:
The Weibull pdf over the initial histogram along the cumulative pdf is given in
Figure 7.9. Thus, an individual patient that falls in the subject population we expect
his cancer tumor size to be 13.341 mm, that is,
2.8
0
0.02
0.04
0.06
0.08
0.1
0.12
f(x)
0.14
0.16
0.18
0.2
0.22
3
3.2
3.4
3.6
3.8
4
x
4.2
4.4
4.6
4.8
5
5.2
5.4
5.6
a = 2.2823
ˆ
b = 1.8754
ˆ
FIGURE 7.8
Beta pdf for rainfall data.
401
7.4 Applications: Parametric Analysis

E x
ð Þ ¼
ð1
0
x f x
ð Þdx ¼ 13:341mm:
Furthermore, we can calculate confidence limits around the true unknown size of the
prostate tumor, that is, a 90% confidence interval for the true mean size is (0.410,
43.81). We can conclude that we are at least 90% certain that the true size of the
tumor will be between 0.410 and 43.81 mm for an individual that falls in the subject
population.
EXERCISES 7.5
7.5.1. Global Warming
Carbon Dioxide CO2 data in the United States in collected in two
locations in the Inland of Hawaii, Point Barrow and Mauna Lao.
These data are given in http://booksite.elsevier.com/9780124171138.
Using the CO2 data collected in Point Barrow from 1974 to 2004, perform
the following analysis:
(a) Structure a histogram of the data and interpret its visual behavior
(b) Apply the Chi-Square goodness-of-fit test to prove or disprove that the
CO2 data follows the exponential power probability distribution, using
a¼0.05.
(c) If you have proven that the CO2 data follow the exponential power pdf,
proceed to calculate and interpret the expected vale of the subject pdf.
7.5.2. Answer the same questions stated in Exercise 7.4.1 using the CO2 data that
was collected at Mauna Lao.
7.5.3. Rainfall Data
In http://booksite.elsevier.com/9780124171138., you will find the
average yearly rainfall data in inches for the North, Central and Southern
FIGURE 7.9
Weibull pdf and cdf for prostate tumor sizes.
402
CHAPTER 7 Goodness-of-Fit Tests Applications

Region of the United States from 1975 to 2007. Using the North Region
data, perform the following analysis:
(a) Construct a histogram of the yearly average rainfall for the Northern
Region. Does the histogram give you any visual indication of the type
of pdf that the data follows?
(b) Using the Kolmogorov-Smirnov goodness-of-fit test, verify if the
subject data follows the normal pdf for a¼0.05.
(c) If you have proven that the data follows the normal pdf, what is the
expected rainfall for a given year? Also, calculate a 95% confidence
limits for the true average rainfall and interpret its meaning.
(d) Calculate P-P plot and interpret its visual meaning with respect to part b).
7.5.4. Using the average yearly rainfall from the Central Region of the United
States, perform the same analysis as the Northern Region and in place of the
normal pdf use the gamma pdf.
7.5.5. Use the data given for the Southern Region of the United States perform the
same analysis as the Northern Region, Exercise 7.5.3, with the normal pdf
replaced with the beta pdf.
7.5.6. Hurricane Katrina
Hurricane Katrina was the most devastating hurricane to hit the United
States in the last 100 years. Katrina was an Atlantic based hurricane
category 5 that reached wind velocity of more than 160 miles per hour. In
http://booksite.elsevier.com/9780124171138, you will find 63
measurements of the wind velocity of Katrina. Using the subject data
perform the following analysis:
(a) We believe that the measurements of the wind velocity measurements
of hurricane Katrina follow the three parameter Weibull pdf. Test this
belief using:
(i) The Kolmogorov-Smirnov goodness-of-fit test.
(ii) The Anderson-Darling goodness-of-fit test
Using a¼0.05
(b) Discuss the results of (i) and (ii) above. What conclusion have you
reached about our belief?
(c) If our belief is correct, write the complete form of the pdf that
characterizes the behavior of the wind velocity of hurricane Katrina.
(d) If in the future we experience a category 5 hurricane, what would the
expected velocity be of such a hurricane?
7.5.7. With respect to Exercise 7.4.6, there is a group of scientists that believe that
the Rayleigh pdf is a better fit of the wind speed measurements of Hurricane
Katrina, follow the same questions posed in Exercise 7.4.6 using the
Rayleigh pdf. What do you conclude in comparing the results of Exercise
7.4.6 with those of Exercise 7.4.7?
7.5.8. National Unemployment
In http://booksite.elsevier.com/9780124171138., you will find
the annual percent average of unemployment data for the
403
7.4 Applications: Parametric Analysis

United States from 1957 to 2007. Using this data perform the following
analysis:
(a) Structure a histogram of the data. Does this histogram convey any
useful information concerning the behavior of the data?
(b) Using the goodness-of-fit test of your choice, can you identify the pdf
that characterizes the behavior of the data, that is, the pdf that the
subject data were drawn from using a¼0.05?
(c) Once you have found the subject pdf of the unemployment data,
calculate the expected value of the annual average percentage of
unemployment rate.
7.5.9. Breast Cancer
In http://booksite.elsevier.com/9780124171138, we have the malignant
breast tumor size in mm of 250 breast cancer patients. In the data base, draw
a random sample of the tumor sizes of n¼50 breast cancer patients. For the
50 tumor sizes in mm perform the following analysis:
(a) Structure a histogram of the 50 tumor sizes. Discuss any visual
information you might obtain concerning the possible pdf that
characterizes the data behavior.
(b) Identify if possible a pdf that you believe may characterize the given
data, using one or more of the goodness-of-fit test, using a¼0.05
(c) If you were not able to identify the pdf, why not? If you were successful,
identify completely the pdf, with appropriate parameter estimates.
(d) If you have identified correctly the pdf, calculate and interpret the
expected value of the subject data.
7.5.10. In http://booksite.elsevier.com/9780124171138, we have the survival times
(in years) of 250 breast cancer patients, that is, the age that they died due to
breast cancer. From this data base, draw a random sample of n¼50 survival
times. Use these survival times to perform the following analysis:
(a) Structure a histogram to possibly guide you in identifying the pdf of the
subject data.
(b) Use any of thegoodness-of-fittestto search in identifying the correct pdf
that characterizes the behavior of the given survival times for a¼0.05
(c) State completely the pdf you have identified and discuss its usefulness
in obtaining information about the subject data.
(d) Obtain the cdf F(t) of the pdf f(t) you have found. If you take one minus
the F(t) you will obtain the survival function, S(t), of the given data.
That is, S(t)¼1–F(t). The survival function, S(t), gives you the
probability that a given patients drawn the data base of 250 breast
cancer patients will survive a specified year.
(e) Write the survival function of the given data set and graph it, that is, S(t)
versus t. Discuss the useful information that the graph gives concerning
breast cancer patients.
7.5.11. Lung Cancer
In http://booksite.elsevier.com/9780124171138, we have the
malignant tumor sizes for male and female lung cancer patients. We also
404
CHAPTER 7 Goodness-of-Fit Tests Applications

include the survival times of both genders; that is, the age in years that
they died due to lung cancer. From the male data base draw a random
sample of n¼60, malignant tumor sizes and perform the following
analysis:
(a) Structure a histogram of the 60 measurement of the tumor sizes
in mm.
(b) Let part a) guide you, if possible in performing goodness-of-fit testing
at a¼0.05 to identify the best possible pdf that characterizes the
probabilistic behavior of the tumor sizes.
(c) Write the pdf completely with appropriate parameter estimates and
obtain and interpret its expected value.
7.5.12. Proceed to obtain a random sample of n¼60 from the female data base and
perform the same analysis (a)-(c) as in Exercise 7.4.11.
7.5.13. Give a precise comparison of males and females for each of the analysis you
performed (a)-(c) in Exercises 7.4.11 and 7.4.12. Discuss your comparison
findings.
7.5.14. In the lung cancer data base, we have also given information about
the survival times of male and female lung cancer patients. Take a
random sample of n¼50 of the survival times of male lung patients and
proceed to perform the same analysis for the survival times as in Exercise
7.4.10.
7.5.15. Similarly as in Exercise 7.4.4 proceed to take a random sample of n¼50 of
the survival times of female lung patients and proceed to perform the same
analysis as you did for the male patients in Exercise 7.5.14.
7.5.16. Give a precise comparison of the analysis of the findings of male and female
lung patients that you have found in Exercises 7.4.14 and 7.4.15,
respectively. Discuss your comparison findings.
7.5.17. Colon Cancer
In http://booksite.elsevier.com/9780124171138, we have the malignant
tumor sizes of male and female colon cancer patients. From this data base
draw a random sample of n¼50 tumor sizes of the male colon cancer
patients. Using this data proceed to perform the same analysis that you did
for the lung cancer data in Exercise 7.4.11.
7.5.18. Proceed to draw a random sample of n¼50 from the female data base that
gives the malignant colon tumor size. Perform the same analysis for the
females that you did for the males in Exercise 7.4.17.
7.5.19. In the colon cancer data base, we also give the survival times for both male
and female patients. From the male data base draw a random sample of
n¼60 survival times and proceed to perform the same analysis as you did in
Exercise 7.5.11.
7.5.20. From the survival times of female colon cancer patients draw a random
sample of n¼60 and proceed with the same analysis that you did for the
male patients in Exercise 7.4.19.
7.5.21. Give a precise comparison of the survival times analysis (a)-(c) for males
and females.
405
7.4 Applications: Parametric Analysis

7.6 CHAPTER SUMMARY
In this chapter, we learned different goodness-of-fit methods and how we use them
to attempt to identify the pdf that characterizes the behavior of a given set of data.
These are the methods; Chi-square, Kolmogorov-Smirnov, Anderson-Darling, and
Shapiro-Wilk tests. We used these tests to perform parametric analysis of real data
on some very important problems that our global society is facing in unemployment,
global warming, various types of cancers, environment, among others.
A list of some of the key definitions introduced in this chapter is given below:
•
Chi-square tests for count data.
•
Goodness-of fit.
•
Test for independence.
•
Contingency table.
•
P-P plot.
•
Q-Q plot.
•
Shapiro-Wilk normality test.
•
CO2 data.
•
Wind velocity data.
•
National unemployment data.
•
Brain cancer data.
•
Rain fall data.
•
Prostate cancer data.
In this chapter, we have also learned the following important concepts and
procedures.
•
Pearson’s Chi-square test procedure.
•
Kolmogorov-Smirnov test procedure.
•
Anderson-Darling test procedure.
•
Shapiro-Wilk test procedure.
•
P-P plots construction procedure.
•
Q-Q plots construction procedure.
7.7 COMPUTER EXAMPLES
7.7.1 R-COMMANDS
Since most of the R-codes are already given in the chapter, we will only give the R-
code for selecting a random sample from a large data set.
In R, sample() function can be used to take a random sample of size n. Suppose
we want to take a random sample of size 40 from a data set named mydata without
replacement.
R-code:
Mysample< mydata[sample(1:nrow(mydata), 40, replace¼FALSE),]
406
CHAPTER 7 Goodness-of-Fit Tests Applications

R can also be used to calculate log likelihood of a data set. The distribution with
smallest log likelihood can be chosen as best fit. Download package ‘MASS’. Then
do the following:
library(MASS)
fitdistr(mydata, ’t’)$loglik
> fitdistr(x, ’normal’)$loglik
> fitdistr(mydata, ’logistic’)$loglik
> fitdistr(mydata, ’weibull’)$loglik
> fitdistr(mydata, ’gamma’)$loglik
> fitdistr(mydata, ’lognormal’)$loglik
> fitdistr(mydata, ’exponential’)$loglik
Some other distributions such as beta may need specification of additional parame-
ters. We suggest you look at R-help.
7.7.2 MINITAB EXAMPLES
EXAMPLE 7.7.1
(Contingency Table): Consider the following data with five levels and two factors. Test for depen-
dence of the factors.
Factors
Levels
1
2
3
4
5
1
39
19
12
28
18
2
172
61
44
70
37
Solution
In C1 enter the data in column 1 (39 and 172), and continue to C5. Then
Stat>Tables>Chi-Square-Test. . .>in Columns containing the table: Type C1 C2 C3 C4
C5>click OK
We will obtain the following output.
Chi-Square Test
Expected counts are printed below observed counts
C1
C2
C3
C4
C5
Total
1
39
19
12
28
18
116
48.95
18.56
12.99
22.74
12.76
2
172
61
44
70
37
384
162.05
61.44
43.01
75.26
42.24
Total
211
80
56
98
55
500
Chi-Sq¼2.023+0.010+0.076+1.219+2.152 +
0.611 + 0.003+0.023+0.368+0.650¼7.135
DF¼4, p-value¼0.129
407
7.7 Computer Examples

PROJECTS FOR CHAPTER 7
7A. FITTING A DISTRIBUTION TO DATA
A common problem is statistical modeling is fitting a probability distribution to a set
of observations (data set) for a variable. By graphically (like histogram) we may have
some rough idea. If we do goodness-of-fit tests, with say two different distributions,
it can happen that both hypothesis may not be rejected. So which one should we
choose? This is mainly important in forecasting. Do a short paper on fitting a distri-
bution to data and apply your results to each of the data in Section 7.4 to check if the
chosen distributions are best possible. Some references are:
(1) FITTING DISTRIBUTIONS WITH R, http://cran.r-project.org/doc/contrib/
Ricci-distributions-en.pdf
(2) Fitting distributions to data and why you are probably doing it wrong, By David
Vose,http://www.vosesoftware.com/whitepapers/Fitting%20distributions%
20to%20data.pdf.
408
CHAPTER 7 Goodness-of-Fit Tests Applications

CHAPTER
Linear Regression Models 8
CHAPTER CONTENTS
8.1 Introduction .................................................................................................... 410
8.2 The Simple Linear Regression Model ................................................................ 411
8.3 Inferences on the Least-Squares Estimators ...................................................... 425
8.4 Predicting a Particular Value of Y .................................................................... 433
8.5 Correlation Analysis ........................................................................................ 436
8.6 Matrix Notation for Linear Regression ............................................................... 440
8.7 Regression Diagnostics ................................................................................... 446
8.8 Chapter Summary ............................................................................................ 449
8.9 Computer Examples ......................................................................................... 450
Project for Chapter 8 .............................................................................................. 456
OBJECTIVE
In this chapter we will study linear relationships in sample data and use the method of
least squares to estimate the necessary parameters.
Sir Francis Galton
(Source: http://en.wikipedia.org/wiki/Francis_Galton)
Mathematical Statistics with Applications in R
Copyright © 2015 Elsevier Inc. All rights reserved.
409

English scientist Sir Francis Galton (1822-1911), a cousin of Charles Darwin, made
significantcontributionstobothgeneticsandpsychology.Heistheinventorofregression
and a pioneer in applying statistics to biology. One of the data sets that he considered
consistedoftheheightsoffathersandfirstsons.Hewasinterestedinpredictingtheheight
of son based on the height of father. Looking at the scatterplots of these heights, Galton
saw that the trend was linear and increasing. After fitting a line to these data (using the
techniques described in this chapter), he observed that for fathers whose heights were
taller than the average, the regression line predicted that taller fathers tended to have
shorter sons and shorter fathers tended to have taller sons. There is a regression toward
the mean. That is how the method of this chapter got its name: regression.
8.1 INTRODUCTION
In earlier chapters, we were primarily concerned about inferences on population
parameters. In this chapter, we examine the relationship between one or more vari-
ables and create a model that can be used for predictive purposes. For example, con-
sider the question “Is there statistical evidence to conclude that the countries with the
highestaverageblood-cholesterollevels havethegreatest incidenceofheartdisease?”
It is important to answer this if we want to make appropriate lifestyle and medical
choices. We will study the relationship between variables using regression analysis.
Our aim is to create a model and study inferential procedures when one dependent and
several independent variables are present. We denote by Y the random variable to be
predicted, also called the dependent variable (or response variable) and by xi the inde-
pendent (or predictor) variables used to model (or predict) Y. For example, let (x, y)
denote the height and weight of an adult male. Our interest may be to find the relation-
ship between height and weight from a sample measurement of n individuals. The pro-
cess of finding a mathematical equation that best fits the noisy data is known as
regression analysis. In his book Natural Inheritance, Sir Francis Galton introduced
the word regression in 1889 to describe certain genetic relationships. The technique
of regression is one of the most popular statistical tools to study the dependence of one
variable with respect to another. There aredifferentforms of regression: simple linear,
nonlinear, multiple, and others. The primary use of a regression model is prediction.
When using a model to predict Y for a particular set of values of x1, . . ., xk, one may
want to know how large the error of prediction might be. Regression analysis, in gen-
eral after collecting the sample data, involves the following steps.
PROCEDURE FOR REGRESSION MODELING
1. Hypothesize the form of the model as Y¼f (x1, . . ., xk ; b0, b1, . . ., bk)+e. Here e represents the
random error term. We assume that E(e)¼0 but Var(e)¼s2 is unknown. From this we can obtain
E(Y)¼f (x1, . . ., xk ; b0, b1, . . ., bk).
2. Use the sample data to estimate unknown parameters in the model.
3. Check for goodness of fit of the proposed model.
4. Use the model for prediction.
410
CHAPTER 8 Linear Regression Models

The function f(x1, . . ., xk; b0, b1, . . ., bk) (k1) contains the independent or pre-
dictor variables x1, . . ., xn (assumed to be nonrandom) and unknown parameters or
weights b0, b1, . . ., bk and e representing the random or error variable. We now pro-
ceed to introduce the simplest form of a regression model, called simple linear
regression.
8.2 THE SIMPLE LINEAR REGRESSION MODEL
Consider a random sample of n observations of the form (x1, y1), (x2, y2), . . ., (xn, yn),
where X is the independent variable and Y is the dependent variable, both being
scalars. A preliminary descriptive technique for determining the form of relationship
between X and Y is the scatter diagram. A scatter diagram is drawn by plotting the
sample observations in Cartesian coordinates. The pattern of the points gives an
indication of a linear or nonlinear relationship between the variables.
In Figure 8.1a, the relationship between x and y is fairly linear, whereas the
relationship is somewhat like a parabola in Figure 8.1b, and in Figure 8.1c there
is no obvious relationship between the variables.
Once the scatter diagram reveals a linear relationship, the problem then is to find
the linear model that best fits the given data. To this end, we will first give a general
definition of a linear statistical model, called a multiple linear regression model.
Definition 8.2.1 A multiple linear regression model relating a random response
Y to a set of predictor variables x1, . . ., xk is an equation of the form
Y ¼ b0 + b1x1 + b2x2 +  + bkxk + e
+
+
+
+
+
+
+
+
+
+
+
+
Linear relationship
No relationship
(c)
(a)
(b)
X
y
X
X
y
y
Quadratic relationship
FIGURE 8.1
Scatter diagram.
411
8.2 The Simple Linear Regression Model

where b0, . . ., bk are unknown parameters, x1, . . ., xk are the independent nonrandom
variables, and e is a random variable representing an error term. We assume that
E(e)¼0, or equivalently,
E Y
ð Þ ¼ b0 + b1x1 + b2x2 +  + bkxk:
To understand the basic concepts of regression analysis we shall consider a single
dependent variable Y and a single independent nonrandom variable x. We assume
that there are no measurement errors in xi. The possible measurement errors in y
and the uncertainties in the assumed model are expressed through the random error
e. Our inability to provide an exact model for a natural phenomenon is expressed
through the random term e, which will have a specified probability distribution (such
as a normal) with mean zero. Thus, one can think of Y as having a deterministic com-
ponent, E(Y), and a random component, e. If we take k¼1 in the multiple linear
regression model, we have a simple linear regression model.
Definition 8.2.2 If Y ¼ b0 + b1x + e, this is called a simple linear regression
model. Here, b0 is the y-intercept of the line and b1 is the slope of the line. The term
e is the error component.
This basic linear model assumes the existence of a linear relationship between the
variables x and y that is disturbed by a random error e. The known data points are the
pairs (x1, y2), (x2, y2), . . ., (xn, yn); the problem of simple linear regression is to fit a
straight line optimal in some sense to the set of data, as shown in Figure 8.2.
Now, the problem becomes one of finding estimators for b0 and b1. Once we
obtain the “good” estimators ^b0 and ^b1, we can fit a line to the data given by the
prediction equation ^Y ¼ ^b0 + ^b1x: The question then becomes whether this predicted
line gives the “best” (in some sense) description of the data. We now describe the
most widely used technique, called the method of least squares, to obtain the estima-
tors or weights of the parameters.
15
10
5
−20
−10
10
20
30
40
50
60
FIGURE 8.2
Scatterplot and least-squares regression line.
412
CHAPTER 8 Linear Regression Models

8.2.1 THE METHOD OF LEAST-SQUARES
As stated (x1, y1), (x2, y2), . . ., (xn, yn) are the n observed data points, with correspond-
ing errors ei, i¼1, . . ., n. That is,
Yi ¼ b0 + b1xi + ei, i ¼ 1,2, ...,n:
We assume that the errors ei, i ¼ 1,. . .,n. are independent and identically distributed
with E(ei) ¼ 0, i ¼ 1,. . .,n, and Var(ei) ¼ s2, i ¼ 1,. . .,n. One of the ways to
decide on how well a straight line fits the set of data is to determine the extent to
which the data points deviate from the line. The straight line model for the response
Y for a given x is
Y ¼ b0 + b1x + e:
Because we assumed that E(e)¼0, the expected value of Y is given by
E Y
ð Þ ¼ b0 + b1x:
The estimator of the E(Y), denoted by ^Y, can be obtained by using the estimators ^b0
and ^b1 of the parameters b0 and b1, respectively. Then, the fitted regression line we
are looking for is given by
^Y ¼ ^b0 + ^b1x:
For observed values (xi,yi), we obtain the estimated value of yi as
^yi ¼ ^b0 + ^b1xi:
The deviation of observed yi from its predicted value ^yi, called the ith residual, is
defined by
ei ¼ yi  ^yi
ð
Þ ¼ yi 
^b0 + ^b1xi


h
i
:
The residuals, or errors ei, are the vertical distances between observed and predicted
values of yi0s (Figure 8.3).
y
x
ei
FIGURE 8.3
Illustration of ei.
413
8.2 The Simple Linear Regression Model

Definition 8.2.3 The sum of squares for errors (SSE) or sum of squares of the
residuals for all of the n data points is
SSE ¼
X
n
i¼1
e2
1 ¼
X
n
i¼1
yi 
^b0 + ^b1xi


h
i2
The least-squares approach to estimation is to find ^b0 and ^b1 that minimize the sum
of squared residuals, SSE. Thus, in the method of least squares, we choose b0 and b1
so that SSE is a minimum. The quantities ^b0 and ^b1 that make the SSE a minimum are
called the least-squares estimates of the parameters b0 and b1, and the corresponding
line ^y ¼ ^b0 + ^b1x is called the least-squares line.
Definition 8.2.4 The least-squares line ^y ¼ ^b0 + ^b1x is one that satisfies the
following property:
SSE ¼
X
n
i¼1
yi  ^yi
ð
Þ2
is a minimum for any other straight line model with
SE ¼
X
n
i¼1
yi  ^yi
ð
Þ ¼ 0
Thus, the least-squares line is a line of the form y¼b0+b1x for which the error sum of
squares P
i¼1
n (yib0b1x)2 is a minimum. The minimum is taken over all values of
b0 and b1, and (x1, y1), (x2, y2), . . ., (xn, yn) are observed data pairs.
The problem of fitting a least-squares line now reduces to finding the quantities
^b0 and ^b1 that minimize the error sum of squares.
8.2.2 DERIVATION OF ^b0 AND ^b1
Now we derive expressions for ^b0 and ^b1. If SSE attains a minimum, then the partial
derivatives of SSE with respect to b0 and b1 are zeros. That is,
@SSE
@b0
¼
@
Xn
i¼1 yi  b0 + b1xi
ð
Þ
½
2
n
o
@b0
¼ 
X
n
i¼1
2 yi  b0 + b1xi
ð
Þ
½

¼ 2
X
n
i¼1
yi nb0 b1
X
n
i¼1
xi
 
!
¼ 0
(8.1)
and
@SSE
@b1
¼
@
Xn
i¼1 yi  b0 + b1xi
ð
Þ
½
2
n
o
@b1
¼ 
X
n
i¼1
2 yi  b0 + b1xi
ð
Þ
½
xi
¼ 2
X
n
i¼1
xiyi b0
X
n
i¼1
xi b1
X
n
i¼1
x2
i
 
!
¼ 0:
(8.2)
414
CHAPTER 8 Linear Regression Models

Equations (8.1) and (8.2) are called the least-squares equations for estimating the
parameters of a line. From Equations (8.1) and (8.2) we obtain a set of linear equa-
tions called the normal equations,
X
n
i¼1
yi ¼ nb0 + b1
X
n
i¼1
xi
(8.3)
and
X
n
i¼1
xiyi ¼ b0
X
n
i¼1
xi + b1
X
n
i¼1
x2
i :
(8.4)
Solving for b0 and b1 from Equations (8.3) and (8.4), we obtain
^b1 ¼
Xn
i¼1 xi x
ð
Þ yi y
ð
Þ
Xn
i¼1 xi x
ð
Þ2
¼
n
Xn
i¼1xiyi
Xn
i¼1xi
Xn
i¼1yi
n
Xn
i¼1x2
1 
Xn
i¼1xi

2
¼
Xn
i¼1xiyi 
Xn
i¼1xi
Xn
i¼1yi
n
Xn
i¼1x2
1 
Xn
i¼1xi

2
n
(8.5)
and
^b0 ¼ y ^b1x:
(8.6)
To simplify the formula for ^b1, set
Sxx ¼
X
n
i¼1
x2
i 
Xn
i¼1xi

2
n
, Sxy ¼
X
n
i¼1
xiyi 
Xn
i¼1xi

 Xn
i¼1yi


n
we can rewrite Equation (8.5) as
^b1 ¼ Sxy
Sxx
:
It can be shown (by using the second derivatives) that Equations (8.5) and (8.6) do
indeed minimize SSE. Now we will summarize the procedure for fitting a least-
squares line.
PROCEDURE FOR FITTING A LEAST-SQUARES LINE
1. Form the n data points (x1, y1), (x2, y2), . . ., (xn, yn), and compute the following quantities:
P
i¼1
n xi,P
i¼1
n xi
2,P
i¼1
n yi,P
i¼1
n yi
2,andP
i¼1
n xiyi.
Also
compute
the
sample
means,
x ¼ 1=n
ð
Þ
Xn
i¼1xi and y ¼ 1=n
ð
Þ
Xn
i¼1yi:
2. Compute
Sxx ¼
X
n
i¼1
x2
i 
Xn
i¼1xi

2
n
¼
X
n
i¼1
xi x
ð
Þ2
and
Sxy ¼
X
n
i¼1
xiyi 
Xn
i¼1xi

 Xn
i¼1yi


n
¼
X
n
i¼1
xi x
ð
Þ yi y
ð
Þ:
Continued
415
8.2 The Simple Linear Regression Model

3. Compute ^b0 and ^b1 by substituting the computed quantities from step 1 into the equations
^b1 ¼ Sxy
Sxx
and
^b0 ¼ y ^b1x:
4. The fitted least-squares line is
^y ¼^b0 + ^b1x:
For a graphical representation, in the xy-plane, plot all the data points and draw the least-squares
line obtained in step 4.
Once we have accomplished the best-fit combination of the two parameters
b0 and b1, any deviation of either parameter away from its optimum value will cause
the sum of squares error to increase. Thus, the optimum combination of the pairs
^b0, ^b1


forms a global minimum point of the error sum of squares among all pos-
sible values of b0 and b1 for the given data set.
EXAMPLE 8.2.1
Use the method of least squares to fit a straight line to the accompanying data points. Give the esti-
mates of b0 and b1. Plot the points and sketch the fitted least-squares line. The observed data values
are given in the following table.
x
1
0
2
2
5
6
8
11
12
3
y
5
4
2
7
6
9
13
21
20
9
Solution
Form a table to compute various terms
xi
yi
xiyi
xi
2
1
5
5
1
0
4
0
0
2
2
4
4
2
7
14
4
5
6
30
25
6
9
54
36
8
13
104
64
11
21
231
121
12
20
240
144
3
9
27
9
Pxi¼38
Pyi¼46
Pxiyi¼709
Pxi
2¼408
416
CHAPTER 8 Linear Regression Models

Sxx ¼
X
n
i¼1
x2
i 
Xn
i¼1xi

2
n
¼ 408 38
ð
Þ2
10 ¼ 263:6,
Sxy ¼
X
n
i¼1
xiyi
Xn
i¼1xi

 Xn
i¼1yi


n
¼ 709 38
ð
Þ 46
ð
Þ
10
¼ 534:2,
x ¼3:8 and y ¼ 4:6:
Therefore,
^b1 ¼ Sxy
Sxx
¼ 534:2
263:6 ¼ 2:0266
and
^b0 ¼ y ^b1x
¼ 4:6 2:0266
ð
Þ 3:8
ð
Þ ¼ 3:1011:
Hence, the least-squares line for these data is
^y ¼ ^b0 + ^b1x ¼ 3:1011 + 2:0266x
and its plot is shown in Figure 8.4.
Recall that for the regression line ^y ¼ ^b0 + ^b1x: We have defined SSE to be
SSE ¼
X
n
i¼1
yi  ^yi
ð
Þ2 ¼
X
n
i¼1
yi  ^b0  ^b1xi

2
:
12.5
10.0
7.5
5.0
2.5
0.0
−2.5
−5.0
25
20
15
10
5
0
−5
−10
S
0.988314
R-Sq
99.3%
R-Sq(adj)
99.2%
x
y
Fitted line plot
y = −3.101 + 2.027 x
FIGURE 8.4
Simple regression line.
417
8.2 The Simple Linear Regression Model

We now show that
SSE ¼ Syy  ^b1Sxy, where Syy ¼
X
n
i¼1
y2
i 
Xn
i¼1yi

2
n
¼
X
n
i¼1
yi y
ð
Þ2:
We know that
SSE ¼
X
n
i¼1
yi  ^b0  ^b1xi

2
¼
X
n
i¼1
yi y + ^b1x ^b1xi

2
¼
X
n
i¼1
yi y
ð
Þ ^b1 xi x
ð
Þ
h
i2
¼
X
n
i¼1
yi y
ð
Þ2 + ^b
2
1
X
n
i¼1
xi x
ð
Þ2 2^b1
X
n
i¼1
xi x
ð
Þ yi y
ð
Þ
¼ Syy + ^b
2
1Sxx 2^b1Sxy:
Recall that ^b1 ¼ Sxy
Sxx:
Substituting for ^b1, we obtain
SSE ¼ Syy 
Sxy
Sxx

2
Sxx 2Sxy
Sxx
Sxy
¼ Syy Sxy
Sxx
Sxy
¼ Syy  ^b1Sxy:
8.2.3 QUALITY OF THE REGRESSION
Once we obtain the linear model, the question is, how well does this line fit the data?
We could make use of the residuals
^ei ¼ yi  ^b0  ^b1xi
to answer the question and to assess the quality of the fit. If our model is good, then
the residual ^ei should be close to the random error e with mean zero. Furthermore,
the residuals should contain little or no information about the model, and there
should be no recognizable pattern. If we plot the residuals versus the independent
variables on the x-axis, ideally, the plot should look like a horizontal blur, the resid-
uals showing no relationship to the x-values, as shown by Figure 8.5. Otherwise,
these plots reveal a not very good fit of the given data, as shown by Figure 8.6,
and we need to improve our model specifications. Thus, a symmetric trend in
the plot of residuals ei versus xi or ^yi i ¼ 1, ...,n
ð
Þ indicates that the assumed regres-
sion model is not correct.
418
CHAPTER 8 Linear Regression Models

Whereas the residual plots give us a visual representation of the quality of fit, a
numerical measure of how well the regression explains the data is obtained by
calculating the coefficient of determination, also called the R2 of the regression.
Particular (observed) value of realized R2 is
r2 ¼ Syy SSE
Syy
¼ 1
Xn
i¼1 yi  ^yi
ð
Þ2
Xn
i¼1 yi y
ð
Þ2 :
Further discussion is given in Project 8B. Regression analysis with any of the stan-
dard statistical software packages will contain an output value of the R2. This value
will be between 0 and 1; closer to 1 means a better fit. For example, if the value of R2
is 0.85, the regression captures 85% of the variation in the dependent variable. This is
generally considered good regression.
.
.
.
. .
.
.
.
.
. .
. . . .
..
. . .
.
y 
e
^
FIGURE 8.5
Good fit.
FIGURE 8.6
Not a good fit.
419
8.2 The Simple Linear Regression Model

8.2.4 PROPERTIES OF THE LEAST-SQUARES ESTIMATORS
FOR THE MODEL Y5b0+b1x+«
We discussed in Chapter 4 the concept of sampling distribution of sample statistics
such as that of X. Similarly, knowledge of the distributional properties of the least-
squares estimators ^b0 and ^b1 is necessary to allow any statistical inferences to be
made about them. The following result gives the sampling distribution of the
least-squares estimators.
Theorem 8.2.1 Let Y¼b0+b1x+e be a simple linear regression model with eN
(0, s2), and let the errors ei associated with different observations yi (i¼1, . . ., N) be
independent. Then
(a) ^b0 and ^b1 have normal distributions.
(b) The mean and variance are given by
E ^b0


¼ b0, Var ^b0


¼
1
n + x2
Sxx


s2,
and
E ^b1


¼ b1, Var ^b1


¼ s2
Sxx
,
where Sxx ¼
Xn
i¼1x2
i 1
n
Xn
i¼1xi

2
: In particular, the least-squares estimators ^b0
and ^b1 are unbiased estimators of b0 and b1, respectively.
Proof. We know that
^b1 ¼Sxy
Sxx
¼ 1
Sxx
X
n
i¼1
xi x
ð
Þ Yi Y


¼ 1
Sxx
X
n
i¼1
xi x
ð
ÞYi Y
X
n
i¼1
xi x
ð
Þ
"
#
¼ 1
Sxx
X
n
i¼1
xi x
ð
ÞYi
where the last equality follows from the fact that
Xn
i¼1 xi x
ð
Þ ¼
Xn
i¼1xi nx ¼ 0.
Because Yi is normally distributed, the sum
1
Sxx
Xn
i¼1 xi x
ð
ÞYi is also normal.
Furthermore,
E ^b1
h
i
¼ 1
Sxx
X
n
i¼1
xi x
ð
ÞE Yi
½

¼ 1
Sxx
X
n
i¼1
xi x
ð
Þ b0 + b1xi
ð
Þ
¼ b0
Sxx
X
n
i¼1
xi x
ð
Þ + b1
Sxx
X
n
i¼1
xi x
ð
Þxi
¼ b1
1
Sxx
X
n
i¼1
xi x
ð
Þxi
420
CHAPTER 8 Linear Regression Models

¼ b1
1
Sxx
X
n
i¼1
x2
i x
X
n
i¼1
xi
"
#
¼ b1
1
Sxx
X
n
i¼1
x2
i 
X
n
i¼1
xi
 
! Xn
i¼1xi
n
 
!
"
#
¼ b1
1
Sxx
X
n
i¼1
x2
i 
Xn
i¼1xi

2
n
2
64
3
75
¼ b1
1
Sxx
Sxx ¼ b1:
For the variance we have,
Var ^b1
h
i
¼ Var
1
Sxx
X
n
i¼1
xi x
ð
ÞYi
"
#
¼ 1
S2
xx
X
n
i¼1
xi x
ð
Þ2Var Yi
½
, since the Yi’s are independent
¼ s2 1
S2
xx
X
n
i¼1
xi x
ð
Þ2 Var Yi
ð
Þ ¼ Var b0 + b1 + ei
ð
Þ ¼ Var ei
ð Þ ¼ s2


¼ s2
Sxx
:
Note that both Y and ^b1 are normal random variables. It can be shown that they are
also independent (see Exercise 8.3.3). Because ^b0 ¼ y ^b1x is a linear combination
of Y and ^b1, it is also normal. Now,
E ^b0
h
i
¼ E Y  ^b1x
h
i
¼ E Y
 	
xE ^b1
h
i
¼ E 1
n
X
n
i¼1
Yi
"
#
xb1 ¼ 1
n
X
n
i¼1
b0 + b1x
ð
Þxb1
¼ b0 + xb1 xb1 ¼ b0:
The variance of ^b0 is given by
Var ^b0
h
i
¼ Var Y  ^b1x
h
i
¼ Var Y
 	
+ x2Var ^b1
h
i
, since Y and ^b1 are independent
¼ s2
n + x2s2
Sxx
¼
1
n + x2
Sxx


s2:
z
If an estimator ^y is a linear combination of the sample observations and has a variance
that is less than or equal to that of any other estimator that is also a linear combination
of the sample observations, then ^y is said to be a best linear unbiased estimator
(BLUE) for y. The following result states that among all unbiased estimators for
b0 and b1 which are linear in Yi, the least-square estimators have the smallest variance.
421
8.2 The Simple Linear Regression Model

GAUSS-MARKOV THEOREM
Theorem 8.2.2 Let Y¼b0+b1x+e be the simple regression model such that for each xi
fixed, each Yi is an observable random variable and each e¼ei, i¼1, 2, . . ., n is an unobservable
random variable. Also, let the random variable ei be such that E[ei]¼0, Var(ei)¼s2 and Cov(ei,
ej)¼0, if i6¼j. Then the least-squares estimators for b0 and b1 are best linear unbiased estimators.
It is important to note that even when the error variances are not constant, there
still can exist unbiased least-square estimators, but the least-squares estimators do
not have minimum variance.
8.2.5 ESTIMATION OF ERROR VARIANCE s2
The greater the variance, s2, of the random error e, the larger will be the errors in the
estimation of model parameters b0 and b1. We can use already-calculated quantities
to estimate this variability of errors. It can be shown that (see Exercise 8.2.1(b)) that
E SSE
ð
Þ ¼ n2
ð
Þs2:
Thus, an unbiased estimator of the error variance, s2, is ^s2 ¼ SSE
ð
Þ= n2
ð
Þ: We will
denote (SSE)/(n2) by MSE (Mean Square Error).
EXERCISES 8.2
8.2.1 For a random sample of size n,
(a) Show that the error sum of squares can be expressed by
SSE ¼ Syy  ^b1Sxy:
(b) Show that E[SSE]¼(n2)s2.
8.2.2 The following are midterm and final examination test scores for 10 students
from a calculus class, where x denotes the midterm score and y denotes the
final score for each student.
x
68
87
75
91
82
77
86
82
75
79
y
74
79
80
93
88
79
97
95
89
92
(a) Calculate the least-squares regression line for these data.
(b) Plot the points and the least-squares regression line on the same graph.
8.2.3. The following data give the annual incomes (in thousands of dollars) and
amounts (in thousands of dollars) of life insurance policies for eight persons.
Annual income
42
58
27
36
70
24
53
37
Life insurance
150
175
25
75
250
50
250
100
422
CHAPTER 8 Linear Regression Models

(a) Calculate the least-squares regression line for these data.
(b) Plot the points and the least-squares regression line on the same graph.
8.2.4. Consider a simple linear model Y¼b0+b1x+e, with eN(0, s2). Show that
Covð^b0, ^b1Þ ¼
s2Xn
i¼1xi
n
Xn
i¼1x2
i 
Xn
i¼1xi

2 :
8.2.5. (a) Show that the least-squares estimates of b0 and b1 of a line can be
expressed as
^b0 ¼ y ^b1x
and
^b1 ¼
Xn
i¼1 xi x
ð
Þ yi y
ð
Þ
Xn
i¼1 xi x
ð
Þ2
:
(b) Using part (a), show that the line fitted by the method of least squares
passes through the point x, y
ð
Þ:
8.2.6. Crickets make their chirping sounds by rapidly sliding one wing over the
other. The faster they move their wings, the higher the number of chirping
sounds that are produced. Scientists have noticed that crickets move their
wings faster in warm temperatures than in cold temperatures (they also do
this when they are threatened). Therefore, by listening to the pitch of the
chirp of crickets, it is possible to tell the temperature of the air. The following
table gives the number of cricket chirps per 13 s recorded at 10 different
temperatures. Assume that the crickets are not threatened.
Temperature
60
66
70
73
78
80
82
87
90
92
Number of chirps
20
25
31
33
36
39
42
48
49
52
Calculate the least-squares regression line for these data and discuss its
usefulness.
8.2.7. Consider the regression model
Y ¼ b1x + e
where eN(0, s2). Show that
^b1 ¼
Xn
i¼1xiyi
Xn
i¼1x2
i
:
8.2.8. A farmer collected the following data, which show crop yields for various
amounts of fertilizer used.
423
8.2 The Simple Linear Regression Model

Fertilizer (pounds/100 sq. ft)
0
4
8
10
15
18
20
25
Yield (bushels)
6
7
10
13
17
18
22
23
(a) Calculate the least-squares regression line for these data.
(b) Plot the points and the least-squares regression line on the same graph.
8.2.9. An economist desires to estimate a line that relates personal disposable
income (DI) to consumption expenditures (CE). Both DI and CE are in
thousands of dollars. The following gives the data for a random sample of
nine households of size four.
DI
25
22
19
36
40
47
28
52
60
CE
21
20
17
28
34
41
25
45
51
(a) Calculate the least-squares regression line for these data.
(b) Plot the points and the least-squares regression line on the same graph.
8.2.10. The following data represent systolic blood pressure readings on 10
randomly selected females between ages 40 and 82.
Age (x)
63
70
74
82
60
44
80
71
71
41
Systolic (y)
151
149
164
157
144
130
157
160
121
125
(a) Calculate the least-squares regression line for these data.
(b) Plot the points and the least-squares regression line on the same graph.
8.2.11. It is believed that exposure to solar radiation increases the pathogenesis of
melanoma. Suppose that the following data give sunspot relative number
and age-adjusted total incidence (incidence is the number of cases per
100,000 population) for eight different years in a certain region.
Sunspot relative number
104
12
40
75
110
180
175
30
Incidence total
4.7
1.9
3.8
2.9
0.9
2.7
3.9
1.6
(a) Calculate the least-squares regression line for these data.
(b) Plot the points and the least-squares regression line on the same graph.
8.2.12. It is believed that the average size of a mammal species is a major factor in
the period of gestation (the period of development in the uterus from
conception until birth). In general, it is observed that the bigger the mammal
is, the longer the gestation period. Table 8.2.1 gives adult mass in kilograms
and gestation period in weeks of some species (source: http://www.
saburchill.com/chapters/chap0037.html).
(a) Calculate the least-squares regression line for these data with adult
mass as the independent variable.
(b) Plot the points and the least-squares regression line on the same graph.
424
CHAPTER 8 Linear Regression Models

(c) Calculate the least-squares regression line for these data with gestation
period as the independent variable.
(d) Assuming that the regression model of part (c) holds for all mammals,
estimate the adult mass in kilograms for the mammals given in
Table 8.2.2.
8.2.13. Using internet, obtain home sales data for your area of interest and obtain a
least-squares regression line for these data. Test for all the assumptions for
this analysis and see if your data satisfies these assumptions.
8.3 INFERENCES ON THE LEAST-SQUARES ESTIMATORS
Once we obtain the estimators of the slope b1 and intercept b0 of the model regression
line, we are in a position to use Theorem 8.2.1 to make inferences regarding these
model parameters. Using the properties of ^b0 and ^b1, in this section we study the
confidence intervals and hypothesis tests concerning these parameters.
From Theorem 8.2.1, we can write
Z1 ¼
^b1 b1
s=
ﬃﬃﬃﬃﬃﬃ
Sxx
p
 N 0, 1
ð
Þ:
Table 8.2.1 Adult Mass and Gestation Period of Mammals
Species
Adult Mass (kg)
Gestation Period
(weeks)
African elephant
6000
88
Horse
400
48
Grizzly bear
400
30
Lion
200
17
Wolf
34
9
Badger
12
8
Rabbit
2
4.5
Squirrel
0.5
3.5
Table 8.2.2 Gestation Period of Mammals
Species
Gestation Period
(weeks)
Indian elephant
89.0
Camel
57.0
Sea lion
51.4
Dog
8.7
Rat
3.0
Hamster
2.3
425
8.3 Inferences on the Least-Squares Estimators

Also, it can be shown that SSE/s2 is independent of ^b1 and has a chi-square
distribution with n2 degrees of freedom. Let the mean square error be defined by
MSE ¼ SSE
n2 ¼
1
n2 ¼
X
n
i¼1
yi 
^b0 + ^b1xi


h
i2
:
Then using Definition 4.2.2, we have
tb1 ¼
Z
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
SSE=s2
ð
Þ
n2
r
¼
^b1 b1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
MSE
Sxx
r
which follows the t-distribution with n2 degrees of freedom.
Similarly, let
Z0 ¼
^b0 b0
s 1
n + x2
Syy

  N 0, 1
ð
Þ:
Also, it can be shown that ^b0 and SSE are independent. Hence,
tb0 ¼
z0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
SSE=s2
n2
r
¼
^b0 b0
MSE
1
n + x2
Sxx


h
i1=2
follows the t-distribution with n2 degrees of freedom.
From these derivations, we can obtain the following procedure about the confi-
dence intervals for the slopes b1 and for the intercept b0.
PROCEDURE FOR OBTAINING CONFIDENCE INTERVALS FOR b0 AND b1
1. Compute Sxx, Sxy, Syy, y, and x as in the procedure for fitting a least-squares line.
2. Compute ^b1, ^b0 using equations ^b1 ¼ Sxy=Sxx and ^b0 ¼ y ^b1x, respectively.
3. Compute SSE by SSE ¼ Syy  ^b1Sxy.
4. Define MSE to be
MSE ¼ SSE
n2,
where n¼Number of pairs of observations (x1,y1), . . ., (xn,yn)
5. A(1a)100% confidence interval for b1 is given by
^b1 ta=2,n2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
MSE
Sxx
,
r
^b1 + ta=2,n2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
MSE
Sxx
r


where ta/2 is the upper tail a/2-point based on a t-distribution with (n2) degrees of freedom.
6. A(1a)100% confidence interval for b0 is given by
^b0 ta=2,n2 MSE 1
n + x2
Sxx



1=2
,
^b0 + ta=2,n2 MSE 1
n + x2
Sxx



1=2
 
!
:
We illustrate this procedure for obtaining confidence limits with an example.
426
CHAPTER 8 Linear Regression Models

EXAMPLE 8.3.1
For the data of Example 8.2.1:
(a) Construct a 95% confidence interval for b0 and interpret.
(b) Construct a 95% confidence interval for b1 and interpret.
Solution
The following calculations were obtained in Example 8.2.1:
Sxx ¼ 263:6, Sxy ¼ 534:2, y ¼ 4:6 and x ¼ 3:8:
Also,
^b1 ¼ 2:0266,
^b0 ¼ 3:1011:
In addition to those calculations, we can compute
X
n
i¼1
y2
i ¼ 1302 and Syy ¼
X
n
i¼1
y2
i 
Xn
i¼1yi

2
n
¼ 1302 46
ð
Þ2
10 ¼ 1090:4:
Now,
SSE ¼ Syy  ^b1Sxy
¼ 1090:4 2:0266
ð
Þ 534:2
ð
Þ
¼ 7:79028
:
Hence,
MSE ¼ SSE
n2 ¼ 7:79028
8
¼ 0:973785:
Now from the t-table, we have t0.025,8¼2.306.
(a) A 95% confidence interval for b0 is given by
^b0 taj2,n2 MSE 1
n + x2
Sxx



1j2
,
^b0 + taj2,n2 MSE 1
n + x2
Sxx



1j2
 
!
¼
3:1011 2:306
ð
Þ
0:973785
ð
Þ
1
10 + 3:8
ð
Þ2
263:6
 
!
"
#1j2
0
@
3:1011 + 2:306
ð
Þ
0:973785
ð
Þ
1
10 + 3:8
ð
Þ2
263:6
 
!
"
#1j21
A:
From which we obtain a 95% confidence interval for b0 as (3.9846, 2.2176). Thus, we can con-
clude with 95% confidence that the true value of the intercept, b0, is between 3.9846 and 2.2176.
(b) A 95% confidence interval for b1 is given by
^b1 taj2,n2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
MSE
Sxx
r
,
^b1 + taj2,n2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
MSE
Sxx
r


¼
2:0266 2:306
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:973785
236:6
r
, 2:0266 + 2:306
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
0:973785
236:6
r
 
!
from which we obtain a 95% confidence interval for b1 as (1.8864, 2.1668). Thus, we can conclude
with 95% confidence that the true value of the slope of the linear regression model is between 1.8864
and 2.1663.
427
8.3 Inferences on the Least-Squares Estimators

One of the assumptions for linear regression model that we have made is that the
variance of the errors is a constant and independent of x. Errors with this property
are called homoscedastic. If the variance of the errors is not constant, the errors are
called heteroscedastic. In the heteroscedastic case, standard errors and confidence
intervals based on the assumption that s2 is an estimate of s2 may be somewhat
deceptive.
Now we introduce hypothesis testing concerning the slope and intercept of the
fitted least-squares line. We use tb0 and tb1 defined earlier as the test statistic for
testing hypotheses concerning b0 and b1, respectively. The usual one- and two-sided
alternatives apply. We proceed to summarize these test procedures.
Hypothesis Test for b0
One-sided test
Two-sided test
H0: b0¼b00 (b00 is a specific value of b0)
H0: b0¼b00
Ha: b0>b00 or b0<b00
Ha: b06¼b00
Test statistic:
Test statistic:
tb0 ¼
^b0 b00
MSE
1
n +
x
Sxx


h
i1=2
tb0 ¼
^b0 b00
MSE
1
n +
x
Sxx


h
i1=2
Rejection region:
Rejection region:
t>ta,(n2), (upper tail region)
jtj>ta/2,(n2)
t<ta,(n2), (lower tail region)
Decision: If tb0 falls in the rejection region, reject the null hypothesis at level of significance a.
Assumptions: Assume that the errors ei , i¼1, . . ., n are independent and normally distributed
with E (ei)¼0, i¼1, . . ., n, and Var(ei)¼s2, i¼1, . . ., n.
We now illustrate this procedure with the following example.
EXAMPLE 8.3.2
Using the data given in Example 8.2.1, test the hypothesis H0: b0¼3 versus Ha: b06¼3 using the
0.05 level of significance.
Solution
We test H0 : b0¼3 versus Ha: b06¼3.
Here b00¼3. The rejection region is t<2.306 or t>2.306.
From the calculations of the previous example, we have
tb0 ¼
^b0 b00
MSE 1
n + x2
Sxx



1j2
¼
3:1011 3
ð
Þ
0:973785
ð
Þ
1
10 + 3:8
ð
Þ2
263:2
 
!
"
#1j2
¼ 0:26041:
Because the test statistic does not fall in the rejection region, at a¼0.05, we do not reject H0.
428
CHAPTER 8 Linear Regression Models

Hypothesis Test For b1
One-sided test
Two-sided test
H0: b1¼b10 (b10 is a specific value of b1)
H0: b1¼b10
Ha: b1>b10 or b1<b10
Ha: b16¼b10
Test statistic:
Test statistic:
tb1 ¼
^b1 b10
ﬃﬃﬃﬃﬃﬃﬃﬃ
MSE
Sxx
r
tb1 ¼
^b1 b10
ﬃﬃﬃﬃﬃﬃﬃﬃ
MSE
Sxx
r
Rejection region:
Rejection region:
t>ta,(n2) (upper tail region)
jtj>ta/2, (n2)
t<ta,(n2) (lower tail region)
Decision: If tb1 falls in the rejection region, reject the null hypothesis at level of significance a.
Assumptions: Assume that the errors ei , i¼1, ..., n are independent and normally distributed
with E (ei)¼0, i¼1, ..., n, and Var(ei)¼s2, i¼1, ..., n.
The test of hypothesis H0: b1¼0 answers the question, is the regression significant? If
b1¼0,weconcludethatthereisnosignificantlinearrelationshipbetweenXandY,and
hence, the independent variable X is not important in predicting the values of Y if the
relationship of Y and X is not linear. Note that if b1¼0, then the model
becomes y¼b0+e. Thus, the question of the importance of the independent variable
in the regression model translates into a narrower question of the test of hypothesis H0:
b1¼0.That is,theregressionlineis actually ahorizontalline through the intercept,b0.
EXAMPLE 8.3.3
Using the data given in Example 8.2.1, test the hypothesis H0: b1¼2 versus Ha: b16¼2 using the 0.05
level of significance.
Solution
We test
H0 : b1 ¼ 2 versus Ha : b1 6¼ 2:
We know that ^b1 ¼ 2:0266:
For a¼0.05 and n¼10, the rejection region is t<2.306 or t>2.306. The test statistic is
tb1 ¼
^b1 b10
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
MSE
Sxx
r
¼ 2:02662
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2:02662
263:6
r
¼ 0:4376:
Because the test statistic does not fall in the rejection region, at a¼0.05, we do not reject H0.
Thus, for a¼0.05, the given data support the null hypothesis that the true value of the slope, b1, of
the regression line is equal to 2.
Another problem closely related to the problem of estimating the regression coeffi-
cients b0 and b1 is that of estimating the mean of the distribution of Y for a given
value of x, that is, estimating b0+b1x. For a fixed value of x, say x0, we have the
following confidence limits.
429
8.3 Inferences on the Least-Squares Estimators

A (1a)100% confidence interval for b0+b1x is given by
^b0 + ^b1x


ta=2se
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n + x0 x
ð
Þ2
Sxx
s
where
se ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Syy  Sxy

2
n2
ð
ÞSxx
:
s
We could use the data from the previous example to easily calculate a confidence
interval for b0+b1x.
8.3.1 ANALYSIS OF VARIANCE (ANOVA) APPROACH
TO REGRESSION
Another approach to hypothesis testing is based on ANOVA. A detailed explanation of
thisapproachisgiveninChapter10.Herewepresentnecessarystepsforregression.The
main reason for this presentation is the fact that most of the major statistical software
outputsforregressionanalysis(seeSection8.9)aregivenintheformofANOVAtables.
It can be verified that (see Exercise 8.3.7)
X
n
i¼1
yi y
ð
Þ2 ¼
X
n
i¼1
yi  ^yi
ð
Þ2 +
X
n
i¼1
^yi y
ð
Þ2:
Denoting
SST ¼
X
n
i¼1
yi y
ð
Þ2, SSE ¼
X
n
i¼1
yi  ^yi
ð
Þ2, and SSR ¼
X
n
i¼1
^yi y
ð
Þ2,
the foregoing equation can be written as
SST ¼ SSR + SSE:
Note that the total sum of squares (SST) is a measure of the variation of yi
0s around
the mean y, and SSE is the residual or error sum of squares that measures the lack of
fit of the regression model. Hence, sum of squares of regression or model (SSR) mea-
sures the variation that can be explained by the regression model.
We saw that to test the hypothesis H0: b1¼0 versus Ha: b16¼0, the statistic
tb1 ¼
^b1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
MSE
Sxx
r
was used, where tb1 follows a t-distribution with (n2) degrees of freedom. From
Exercise 4.2.18, we know that
t2
b1 ¼
^b
2
1
MSE
Sxx


430
CHAPTER 8 Linear Regression Models

follows an F-distribution with numerator degrees of freedom 1 and denominator
degrees of freedom (n2). We can also verify that
t2
b1 ¼ MSR
MSE,
where MSR is the mean sum of squares of regression.
Thus, to test H0: b1¼0 versus Ha: b16¼0, we could use the statistic
MSR
MSE  F 1,n2
ð
Þ
and reject H0 if
MSR
MSE  Fa 1,n2
ð
Þ:
These can be summarized by Table 8.1, known as the ANOVA table.
The last column in the ANOVA table gives the statistic (MSR)/(MSE). It is also
customary to give another column with the p-value of the test.
EXAMPLE 8.3.4
In a study of baseline characteristics of 20 patients with foot ulcers, we want to see the relationship
between the stage of ulcer (determined using the Yarkony-Kirk scale, a higher number indicating a
more severe stage, with range 1 to 6), and duration of ulcer (in days). Suppose we have the data
shown in Table 8.2.
(a) Give an ANOVA table to test H0: b1¼0 versus Ha: b16¼0. What is the conclusion of the test
based on a¼0.05?
(b) Write down the expression for the least-squares line.
Solution
(a) We test H0: b1¼0 versus Ha: b16¼0. We will use Minitab to generate the ANOVA table
(Table 8.3). Because the p-value is less than 0.001, for a¼0.05, we reject the null hypothesis
that b1¼0 and conclude that there is a relationship between the stage of ulcer and its
duration.
(b) Again, using the Minitab output, we get the least-squares line as
d ¼ 4:61x2:40:
Continued
Table 8.1 ANOVA Table for Simple Regression
Source of
Variation
Degrees of
Freedom
Sum of
Squares
Mean Sum
of Squares
F-
Ratio
Regression
(model)
1
SSR
MSR ¼ SSR
d:f:
MSR
MSE
Error (residuals)
n2
SSE
SSE
d:f:
Total
n1
SST
431
8.3 Inferences on the Least-Squares Estimators

EXERCISES 8.3
8.3.1. An experiment was conducted to observe the effect of an increase in
temperature on the potency of an antibiotic. Three one ounce portions of the
antibiotic were stored for equal lengths of time at each of the following
Fahrenheit temperatures: 40, 55, 70, and 90 .The potency readings
observed at the end of the experimental period were
Potency reading, y
49
38
27
24
38
33
19
28
16
18
23
Temperature, x
40
55
70
90
(a) Find the least-squares line appropriate for these data.
(b) Plot the points and graph the line as a check on your calculations.
(c) Calculate the 95% confidence intervals for b0 and b1, respectively.
8.3.2. Consider the data
x
38
26
48
22
40
15
30
33
y
10
11
16
8
12
5
10
11
(a) Find the least-squares line appropriate for these data.
(b) Plot the points and graph the line as a check on your calculations.
(c) Calculate the 95% confidence intervals for b0 and b1, respectively.
8.3.3. Show that Y and ^b1 are independent, under the usual assumptions of a simple
linear regression model.
Table 8.3 ANOVA Table for Foot Ulcer Data
Source of
Variation
Degrees of
Freedom
Sum of
Squares
Mean Sum
of Squares
F-Ratio
p-Value
Regression
(model)
1
570.04
570.04
77.05
0.000
Error
(residuals)
18
133.16
7.40
Total
19
703.20
Table 8.2 Stage and Duration of Foot Ulcers
Stage of Ulcer (x)
4
3
5
4
4
3
3
4
6
3
Duration (d)
18
6
20
15
16
15
10
18
26
15
Stage of Ulcer (x)
3
4
3
2
3
2
2
3
5
6
Duration (d)
8
16
17
6
7
7
8
11
21
24
432
CHAPTER 8 Linear Regression Models

8.3.4. Using the data of Exercise 8.2.10, calculate the 95% confidence intervals for
b0 and b1, respectively.
8.3.5. The following data represent survival time in days after a heart transplant and
patient age in years at the time of transplant for 10 randomly selected patients.
Age at transplant
28
41 46 53
39
36
47
29
48
44
Survival time, in days
7 278 44 48 406 382 1995 176 323 1846
(a) Find the least-squares line appropriate for these data.
(b) Plot the points and graph the line.
(c) Calculate the 95% confidence intervals for b0 and b1, respectively.
8.3.6. The following data represent weights of cigarettes (g) from different
manufacturers and their nicotine contents (mg).
Weight
15.8
14.9
9.0
4.5
15.0
17.0
8.6
12.0
4.1
16.0
Nicotine
0.957
0.886 0.852 0.911
0.889
0.919 0.969
1.118 0.946
1.094
(a) Find the least-squares line appropriate for these data.
(b) Plot the points and graph the line. Do you think the linear regression is
appropriate?
(c) Calculate the 95% confidence intervals for b0 and b1, respectively.
8.3.7. The following data represents total CO2 emissions per vehicle (in Metric tons
per vehicle) [http://corporate.ford.com/microsites/sustainability-report-
2012-13/environment-data-energy].
Year
2007
2008
2009
2010
2011
2012
Total
1.01
1.09
1.07
1.01
0.91
0.90
(a) Find the least-squares line appropriate for these data.
(b) Plot the points and graph the line.
(c) Calculate the 95% confidence intervals for b0 and b1, respectively.
8.3.8. Show that
X
n
i¼1
yi y
ð
Þ2 ¼
X
n
i¼1
yi  ^yi
ð
Þ2 +
X
n
i¼1
^yi y
ð
Þ2:
8.4 PREDICTING A PARTICULAR VALUE OF Y
In the earlier sections, we have seen how to fit a least-squares line for a given set of
data. Also using this line, we could find E(Y), for any given value of x. Instead of
obtaining this mean value, we may be interested in predicting the particular value
of Y for a given x. In fact, one of the primary uses of the estimated regression line
is to predict the response value of Y for a given value of x. Prediction problems are
433
8.4 Predicting a Particular Value of Y

very important in several real-world problems; for example, in economics one may
be interested in a particular gain associated with an investment.
Let ^Y0 denote a predictor of a particular value of Y¼Y0 and let the corresponding
values of x be x0. We shall choose ^Y0 to be E ^Yjx0


: Let ^Y denote a predictor of a
particular value of Y. Then the error  of the predictor in comparison to a particular
value of Y is
 ¼ Y  ^Y0:
Both Y and ^Y are normal random variables, and the error is a linear function of Y and ^Y.
This means that  itself is normally distributed. Also, because E ^Y
 
¼ E Y
ð Þ, we have
E 
ð Þ ¼ E Yjx0
ð
ÞE ^Y
 
¼ 0:
Furthermore,
Var 
ð Þ ¼ Var Y  ^Y


¼ Var Y
ð Þ + Var ^Y
 
2Cov Y, ^Y


:
We can consider Y and ^Y as independent, because we are predicting a different value
of Y, not used in the calculation of ^Y. Therefore, Cov Y, ^Y


¼ 0. In that case
Var 
ð Þ ¼ Var Y0
ð
Þ + Var ^Y0


¼ s2 + s2 1
n + xx
ð
Þ2
Sxx
"
#
¼ 1 + 1
n + xx
ð
Þ2
Sxx
"
#
s2
:
Hence, the error of predicting a particular value of Y, given x, is normally distributed
with mean zero and variance
1 + 1
n + xx
ð
Þ2
Sxx
"
#
s2:
That is,
  N
0, 1 + 1
n + xx
ð
Þ2
Sxx
"
#
s2
 
!
,
and
Z ¼
Y  ^Y
s
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1
n + xx
ð
Þ2
Sxx


s
 N 0, 1
ð
Þ:
If we substitute the sample standard deviation S for s, then we can show that
T ¼
Y  ^Y
S
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1
n + xx
ð
Þ2
Sxx


s
follows the t-distribution with [n(k+1)] degrees of freedom. Using this fact, we now
give a prediction interval for the random variable Y, the response of a given situation.
434
CHAPTER 8 Linear Regression Models

We know that
P ta=2 < T < ta=2


¼ 1a:
Substituting for T, we have
P
ta=2 <
Y  ^Y
S
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1
n + xx
ð
Þ2
Sxx
"
#
v
u
u
t
< ta=2
0
B
B
B
B
B
B
@
1
C
C
C
C
C
C
A
¼ 1a
which implies that
P ^Y ta=2S
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1
n + xx
ð
Þ2
Sxx
"
#
v
u
u
t
< Y < ^Y + ta=2S
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1
n + xx
ð
Þ2
Sxx
"
#
v
u
u
t
2
4
3
5 ¼ 1a:
Hence, we have the following.
A (1a)100% prediction interval for Y is
^Y ta=2S
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1
n + xx
ð
Þ2
Sxx
"
#
v
u
u
t
where ta/2 is based on (n2) degrees of freedom and S2 ¼ SSE
n2 ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
MSE
p
:
We illustrate this statistical procedure with the following example.
EXAMPLE 8.4.1
Using the data given in Example 8.2.1, obtain a 95% prediction interval at x¼5.
Solution
We have shown that ^y ¼ 3:1011 + 2:0266x Hence, at x¼5, ^y ¼ 7:0319:
Also x ¼ 3:8, Sxx¼263.6, SSE¼7.79028, and S ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
7:79028
8
q
¼ 2:306:
From the t-table, t0.025,8¼2.306.
Thus, we have
7:0319 2:306
ð
Þ 0:98681
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1
10 + 53:8
ð
Þ2
263:6
"
#
v
u
u
t
which gives the 95% prediction interval as (4.6393, 9.4245).
We can conclude with 95% confidence that the true value of Y at the point x¼5 will be some-
where between 4.6393 and 9.4245.
435
8.4 Predicting a Particular Value of Y

EXERCISES 8.4
8.4.1. The following are midterm and final examination test scores for 10 calculus
students, where x denotes the midterm score and y denotes the final score for
each student.
x
68
87
75
91
82
77
86
82
75
79
y
74
89
80
93
88
79
97
95
89
92
Obtain a 95% prediction interval for x¼92 and interpret its meaning.
8.4.2. The following data give the annual incomes (in thousands of dollars) and
amounts (in thousands of dollars) of life insurance policies for eight persons.
Annual income
42
58
27
36
70
24
53
37
Life insurance
150
175
25
75
250
50
250
100
Obtain a 90% prediction interval for x¼59 and interpret its meaning.
8.4.3. For the following data, construct a 95% prediction interval for x¼12.
x
1
3
5
7
9
11
y
16
36
43
65
80
88
8.4.4. The data given below are from a random sample of height (in inches) and
weight (in pounds) of seven basketball players.
Height
73
83
77
80
85
71
80
Weight
186
234
208
237
265
190
220
Construct a 99% prediction interval for height equal to 90. Interpret the
result and state any assumptions.
8.4.5. For the data in Exercise 8.2.10, obtain a 95% prediction interval for the age,
x¼85, interpret and state any assumptions.
8.4.6. For the CO2 emission data of Exercise 8.3.7, construct a 95% prediction
interval for the year 2013 emission.
8.5 CORRELATION ANALYSIS
Using the regression model, we can evaluate the magnitude of change in the depen-
dent variable due to certain changes in the independent variables. One of the main
assumptions we have used is that the independent variables are known. However,
436
CHAPTER 8 Linear Regression Models

there are problems where the x-values as well as the y-values are assumed by random
variables. This would be the case, for example, if we study the relationship between
secondhand smoking and the incidence of a certain disease. Here, basically, one
treats X as random, and hence the simple linear regression model is
Y ¼ b0 + b1X + e
This implies that
E YjX ¼ x
ð
Þ ¼ b0 + b1x
and one looks for dependence of X and Y. Once we have determined that there is a rela-
tionship between the variables, the next question that arises is how closely the vari-
ables are associated. A measure of the amount of linear dependency of two random
variables is the correlation. The correlation coefficient tells us how strongly two vari-
ables are linearly related. The statistical method used to measure the degree of corre-
lation is referred to as correlation analysis. We will assume that the vector random
variable (X, Y) has a bivariate normal distribution. In this case, it can be shown that
E YjX ¼ x
ð
Þ ¼ b0 + b1x:
At times, our interest may not be in the linear relationship; rather, we may merely
want to know whether X and Y are independent random variables. If (X, Y) has a
bivariate normal distribution, then testing for independence is equivalent to testing
that the correlation coefficient, r¼sxy/(sxsy), is equal to zero. Note that r is positive
if X and Y increase together and r is negative if Y decreases as X increases. If r¼0,
there is no relation between X and Y; if r>0, there is a positive relation between X
and Y (increasing slope); and when r<0, we have a negative relationship (decreas-
ing slope). Thus, the correlation coefficient can be used to measure how well the lin-
ear regression model fits the data.
Let (X1, Y1), (X2, Y2), . . ., (Xn, Yn) be a random sample from a bivariate normal
distribution. The maximum likelihood estimator of r is the sample correlation coef-
ficient defined by ^r or r,
r ¼
Xn
i¼1 Xi X


Yi Y


ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Xn
i¼1 Xi X

2Xn
i¼1 Yi Y

2
q
¼
Sxy
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
SxxSyy
p
:
(8.7)
Equivalently, we can rewrite Equation (8.7) by
r ¼
n
Xn
i¼1XiYi
Xn
i¼1Xi
Xn
i¼1Yi
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n
Xn
i¼1X2
i 
Xn
i¼1Xi

2


n
Xn
i¼1Y2
i 
Xn
i¼1Yi

2


s
:
We can see that 1	r	1. The value of r could readily be obtained by the calcu-
lations one already has performed for the regression analysis. Observe that the
numerator of r is exactly the same as the numerator of ^b1 derived in Section 8.2.
437
8.5 Correlation Analysis

Because the denominators of both ^b1 and r are nonnegative, they have the same sign.
It can be shown that this estimator is not unbiased. If the value of r is near or equal to
zero, this implies little or no linear relationship between x and y. On the other hand,
the closer r is to 1 or 1, the stronger the linear relationship between x and y. When
r>0, values of y increase as the values of x increase, and the data set is said to be
positively correlated. When r<0, values of y decrease as the values of x increase,
and the data set is said to be negatively correlated. In this book, we use the term cor-
relation only when referring to linear relationships. In actual practice we can use the
value of r to decide whether it is appropriate to develop linear regression models in a
given situation. As a rule of thumb, if r>0.30 or r<0.30, we proceed with devel-
oping a linear regression model. However, a much higher or lower value is desirable.
For example, if in a given problem where r¼0.77, it conveys to us that approxi-
mately 77% of the data we have are linearly related.
The probability distribution for r is difficult to obtain. For large samples, this dif-
ficulty could be overcome by using the fact that the Fisher z-transform, given by
z ¼ 1
2 ln 1 + r
ð
Þ= 1r
ð
Þ
½

is approximately normally distributed with mean mz¼ 1
2ln[(1+r)(1r)] and variance
sz¼1/(n3). Thus, for large random samples, we can test hypotheses about r using
the approximate test statistic:
Z ¼ zmz
sz
¼
1
2 ln
1 + r
1r


1
2 ln
1 + r
1r


1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n3
p
:
For example, suppose we are interested in testing the hypothesis that the true value of
r is a specific number, say, r0, with a certain value of a. We can proceed to make a
decision by following the procedure given next.
HYPOTHESIS TEST FOR r
One-sided test
H0: r¼r0
Ha: r>r0 or
Ha: r<r0
Test statistic:
Z ¼
1
2 ln
1 + r
1r


 1
2 ln
1 + r0
1r0


1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n3
p
Rejection region:
z>za (upper tail region)
z<za (lower tail region)
Two-sided test
H0: r¼r0
Ha: r¼r0
Test statistic:
Z ¼
1
2 ln
1 + r
1r


 1
2 ln
1 + r0
1r0


1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n3
p
Rejection region:
jzj>za/2
Decision: If z falls in the rejection region, reject the null hypothesis at confidence level a.
Assumption: (X, Y) follow the bivariate normal, and this test procedure is approximate.
438
CHAPTER 8 Linear Regression Models

EXAMPLE 8.5.1
For the data given in Example 8.2.1, would you say that the variables X and Y are independent? Use
a¼0.05.
Solution
We test
H0 : r ¼ 0 versus Ha : r 6¼ 0:
From Example 8.2.1, we have the following summary:
X
n
i¼1
xi ¼ 38;
X
n
i¼1
yi ¼ 46;
X
n
i¼1
xiyi ¼ 709
and
X
n
i¼1
x2
i ¼ 408;
X
n
i¼1
y2
i ¼ 1302; n ¼ 10:
Hence,
r ¼
n
Xn
i¼1XiYi
Xn
i¼1Xi
Xn
i¼1Yi
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n
Xn
i¼1X2
i 
Xn
i¼1Xi

2


n
Xn
i¼1Y2
i 
Xn
i¼1Yi

2


s
¼
10
ð
Þ 709
ð
Þ 38
ð
Þ 46
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
10
ð
Þ 408
ð
Þ 38
ð
Þ2
h
i
10
ð
Þ 1302
ð
Þ 46
ð
Þ2
h
i
r
¼ 0:99641:
The test statistic is
z ¼
1
2 ln
1 + r
1r


1
2 ln
1 + r0
1r0


1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n3
p
¼
1
2 ln
1 + 0:99641
10:99641


1
2 ln
1 + 0
10


1ﬃﬃﬃ
7
p
¼ 8:3618:
For za/2¼z0.025¼1.96, the rejection region is jzj>1.96. Because the observed value of the test
statistic falls in the rejection region, we reject the null hypothesis and conclude that at a¼0.05, the
variables X and Y are dependent.
EXERCISES 8.5
8.5.1. The table shows the midterm and final examination test scores for 10 students
from a differential equations class, where x denotes the midterm scores and y
denotes the final scores.
x
68
87
75
91
82
77
86
82
75
79
y
74
89
80
93
88
79
97
95
89
92
439
8.5 Correlation Analysis

(a) At 95% confidence level, test whether X and Y are independent.
(b) Find the p-value.
(c) State any assumptions you have made in solving the problem.
8.5.2. The following table gives the annual incomes (in thousands of dollars) and
amounts (in thousands of dollars) of life insurance policies for eight persons.
Annual income
42
58
27
36
70
24
53
37
Life insurance
150
175
25
75
250
50
250
100
(a) At the 98% confidence level, test whether annual income and the amount
of life insurance policies are independent.
(b) Find the attained significance level.
(c) State any assumptions you have made in solving the problem.
8.5.3. Show that
r ¼
n
Xn
i¼1XiYi
Xn
i¼1Xi
Xn
i¼1Yi
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n
Xn
i¼1X2
i 
Xn
i¼1Xi

2


n
Xn
i¼1Y2
i 
Xn
i¼1Yi

2


s
is not an unbiased estimator of the population coefficient, r.
8.5.4 Using the data in Example 8.2.1:
(a) Compute r, the coefficient of correlation.
(b) Would you say that the variables X and Y are independent? Use a¼0.05.
(c) State any assumptions you have made in solving the problem.
8.5.5 A new drug is tested for serum cholesterol-lowering properties on six
randomly selected volunteers. The serum cholesterol values are given in the
following table.
Before treatment:
232
254
220
200
213
222
After treatment:
212
240
225
205
204
218
(a) At 95% confidence level, test whether X and Y are independent.
(b) Find the p-value.
(c) Calculate the least-squares regression line for these data.
(d) Interpret the usefulness of the model.
(e) State any assumptions you have made in solving the problem.
8.6 MATRIX NOTATION FOR LINEAR REGRESSION
Most real-life applications of regression analysis use models that are more complex
than the simple straight-line model. For example, a person’s body weight may
depend not just on the person’s eating habits; it may depend on additional factors
such as heredity, exercise, and type of work. Hence, we may want to incorporate
440
CHAPTER 8 Linear Regression Models

other potential independent variables in the modeling. We now study the situation
where k (>1) independent variables are used to predict the dependent variable.
The model to be studied is of the form
Y ¼ b0 + b1x1 + b1x2 +  + bkxk + e:
Here, eN(0, s2). This model is called a multiple regression model.
Let y1, y2, . . ., yn be n independent observations on Y. Then each observation yi
can be written as
yi ¼ b0 + b1xi1 + b2xi2 +  + bkxik + e
where xij is the jth independent variable for the ith observation, i¼1, 2, . . ., n, and ei0s
are independent as in the simple linear regression case. It is sometimes advantageous
to introduce matrices to study the linear equations. Let x0¼1. Define the following
matrices:
X ¼
x0 x11 x12 : : x1k
x0 x21 x22 : : x2k
:
:
:
: :
:
:
:
:
: :
:
:
:
:
: :
:
x0 xn1 xn2 : : xnk
2
666666664
3
777777775
, Y ¼
y1
y2
:
:
:
yn
2
666666664
3
777777775
,
b ¼
b0
b1
:
:
:
bn
2
666666664
3
777777775
, and e ¼
e1
e2
:
:
:
en
2
666666664
3
777777775
:
(8.8)
Thus the n equations representing the linear equations can be rewritten in the matrix
form as
Y ¼ Xb + e:
In particular, for the n observations from the simple linear model of the form
Y ¼ b0 + b1x + e
we can write
Y ¼ Xb + e,
where
Y ¼
y1
y2
:
:
:
yn
2
6666664
3
7777775
, X ¼
1 x1
1 x2
1
:
1
:
1
:
1 xn
2
6666664
3
7777775
, e ¼
e1
e2
:
:
:
en
2
6666664
3
7777775
, and b ¼ b0
b1


:
441
8.6 Matrix Notation for Linear Regression

We can see that
X0X ¼
1
1 : : : 1
x1 x2 : : : xn
"
#
1 x1
1 x2
1
:
1
:
1
:
1 xn
2
66666666664
3
77777777775
¼
n
Xn
i¼1xi
Xn
i¼1xi
Xn
i¼1x2
1
2
4
3
5,
where 0 denotes the transpose of a matrix.
Also,
X0Y ¼
Xn
i¼1yi
Xn
i¼1xiyi
2
4
3
5:
Let us now go back to the multiple regression model
Y ¼ b0 + b1x1 + b1x2 +  + bkxk + e:
The least-squares estimators ^bi of bi for i¼0, 1, 2, ..., k are the ones that
minimize the sum of squares
SSE ¼
X
n
i¼1
e2
i ¼
X
n
i¼1
yi 
^b0 + ^b1x1 + ^b2x2 +  + ^bkxk


h
i2
¼ yX^b


0 yX^b


¼ y0yy0X^b X^b


0y +
^bX


0X^b:
To minimize SSE with respect to b, we differentiate SSE with respect to b and
equate it to zero. Thus,
@
@b y0yy0X0bb0X0y + X0b0Xb
ð
Þ ¼ 0
yielding
X0X
ð
Þ^b ¼ X0Y:
Assuming the matrix (X0X) is invertible, we obtain
^b ¼ X0X
ð
Þ1X0Y:
Now we summarize the procedure to obtain a multiple linear regression equation.
442
CHAPTER 8 Linear Regression Models

PROCEDURE TO OBTAIN A MULTIPLE LINEAR REGRESSION EQUATION
1. Rewrite the n observations
Yi ¼ b0 + b1x1i + b1x2i +  + bkxki, i ¼ 1,2, ...,n
in the matrix notation as
Y ¼ Xb + e
where X, Y , and b are defined in (1).
2. Compute (X0X)1 and obtain the estimators of b as
^b ¼ X0X
ð
Þ1X0Y:
3. Then the regression equation is
^Y ¼ X^b:
EXAMPLE 8.6.1
Using the data given in Example 8.2.1, use the matrix approach to solve the problem of operations.
Solution
From the data of Example 8.2.1 we have
Y ¼
9
7
5
4
2
6
9
13
21
20
2
666666666666664
3
777777777777775
and X ¼
1 3
1 2
1 1
1
0
1
2
1
5
1
6
1
8
1
11
1
12
2
666666666666664
3
777777777777775
:
Thus, we can write
X0X ¼ 10
38
38 408


X0Y ¼
46
709


X0X
ð
Þ1 ¼
0:1548 0:0144
0:0144
0:0038


:
Hence,
^b ¼ X0X
ð
Þ1 X0Y
ð
Þ ¼
0:1548 0:0144
0:0144
0:0038


46
709


¼ 3:1009
2:0266


¼
^b0^b1


:
Thus, the least-squares line is given by
^y ¼ 3:1009 + 2:0266X,
which is identical to the regression line we obtained in Example 8.2.1.
443
8.6 Matrix Notation for Linear Regression

EXAMPLE 8.6.2
The following data relate to the prices (Y) of five randomly chosen houses in a certain neighborhood,
the corresponding ages of the houses (x1), and square footage (x2).
Price y in thousands
of dollars
Age x1 in years
Square footage x2 in
thousands of square feet
100
1
1
80
5
1
104
5
2
94
10
2
130
20
3
Fit a multiple linear regression model
Y ¼ b0 + b1x1 + b2x2 + e
to the foregoing data.
Solution
We have
Y ¼
100
80
104
94
130
2
6666664
3
7777775
, X ¼
1
1
1
1
5
1
1
5
2
1
0
2
1 20 3
2
6666664
3
7777775
,
X0X ¼
5
41
9
41 551 96
9
96
19
2
64
3
75, and X0Y ¼
508
4560
966
2
64
3
75:
and
X0X
ð
Þ1 ¼
2:3076
0:1565 1:8840
0:1565
0:0258 0:2044
1:8840 0:2044
1:9779
2
4
3
5:
Hence,
X0X
ð
Þ1 X0Y
ð
Þ ¼
66:1252
0:3794
21:4365
2
4
3
5:
Thus, the regression model is
y ¼ 66:120:3794x1 + 21:4365x2:
8.6.1 ANOVA FOR MULTIPLE REGRESSION
As in Section 8.3, we can obtain an ANOVA table for multilinear regression (with k
independent or explanatory variables) to test the hypothesis
H0 : b1 ¼ b2 ¼  ¼ bk ¼ 0
444
CHAPTER 8 Linear Regression Models

versus
Ha : At least one of the parameters bj 6¼ 0, j ¼ 1, ...,k:
The calculations for multiple regression are almost identical to those for simple
linear regression, except that the test statistic (MSR)/(MSE) has an F(k, nk1)
distribution. Note that the F-test does not indicate which of the parameters bj6¼0,
except to say that at least one of them is not zero. The ANOVA table for multiple
regression is given by Table 8.4.
EXAMPLE 8.6.3
For the data of Example 8.6.2, obtain an ANOVA table and test the hypothesis
H0 : b1 ¼ b2 ¼ 0 versus Ha : Atleastoneofthebi 6¼ 0,i ¼ 1,2:
Use a¼0.05.
Solution
We test H0: b1¼b2¼0 versus Ha: At least one of the bi6¼0, i¼1, 2. Here n¼5, k¼2. Using Minitab,
we obtain the ANOVA table (Table 8.5). Based on the p-value, we cannot reject the null hypothesis at
a¼0.05.
EXERCISES 8.6
8.6.1. Given the data
X1
X2
y
3
1
4
2
5
3
3
3
6
1
2
5:
Table 8.4 ANOVA Table for Multiple Regression
Source of Variation
Degrees of
Freedom
Sum of
Squares
Mean Sum of
Squares
F-Ratio
Regression (Model)
k
SSR
MSR ¼ SSR
df
MSR
MSE
Error (Residuals)
nk1
SSE
SSE
df
Total
n1
SST
Table 8.5 ANOVA Table for Home Price Data
Source of
Variation
Degrees
of
Freedom
Sum of
Squares
Mean
Sum of
Squares
F-Ratio
p-Value
Regression (Model)
2
956.5
478.2
2.50
0.286
Error (Residuals)
2
382.7
191.4
Total
4
1339.2
445
8.6 Matrix Notation for Linear Regression

(a) Write the multiple regression model in matrix form.
(b) Find X0X, (X0X)1, and X0y.
(c) Estimate b.
8.6.2. A study is conducted to estimate the demand for housing (y) based on current
interest rate X1 and the rate of unemployment. The data in Table 8.6.1 are
obtained.
(a) Fit the multiple regression model
y ¼ b0 + b1x1 + b1x2 + e:
(b) Test whether the model is significant.
8.6.3. The following data give the annual incomes (in thousands of dollars) and
amounts (in thousands of dollars) of life insurance policies for eight persons.
Annual income
42
58
27
36
70
24
53
37
Life insurance
150
175
25
75
250
50
250
100
Calculate the least-squares regression line for these data using matrix
operations.
8.6.4. The following is a random sample of height (in inches) and weight (in
pounds) of seven basketball players.
Height
73
83
77
80
85
71
80
Weight
186
234
208
237
265
190
220
Calculate the least-squares regression line for these data using matrix
operations.
8.7 REGRESSION DIAGNOSTICS
In the previous sections, we derived least-squares estimators for the parameters in the
linear regression model. These estimators are useful as long as we can determine (1)
how well the model fits the data and (2) how good our estimates are in providing
possible relationships between variables of interest. Some of these problems are
Table 8.6.1 Housing Demand, Interest Rate, and Unemployment Rate
Units Sold
Interest Rate (%)
Unemployment Rate (%)
65
9.0
10.0
59
9.3
8.0
80
8.9
8.2
90
9.1
7.7
100
9.0
7.1
105
8.7
7.2
446
CHAPTER 8 Linear Regression Models

discussed in Chapter 14 in a unified manner. We now briefly discuss some aspects of
the adequacy of the simple linear regression model. In multiple regression, in addi-
tion to the problems discussed here, there are other problems, such as collinearity and
model specification (inclusion of all relevant variables, as well as exclusion of
irrelevant variables), that need to be examined. They are beyond the level of this text.
Many graphical methods and numerical tests dealing with these problems are
available in the literature and are often called regression diagnostics. Most of the
major statistical software packages incorporate these tests, making it easier to
perform regression diagnostics so as to detect potential problems.
We have seen that the (ordinary) least-squares regression model must meet the
following assumptions.
1. Linearity. The existence of a linear relationship between x and y is the basis of
the simple linear regression model. A simple method to test for linearity is to
draw a scatterplot of data points. As we explained in Section 8.2, we could also
plot residual ei versus xi or ^Yi. A symmetric trend in the plot of the residuals
versus the explanatory variable or the fitted values indicates there is a problem
with the obtained regression model. For a correct model, the residuals should
center around zero across the explanatory variables and the fitted values. The
degree of linear relationship can be ascertained by the correlation coefficient, r,
given in Section 8.5 or by using the value of the coefficient of determination r2,
explained in Project 8B. Most statistical software packages give the value of r2
(refer to outputs given in Section 8.9). The closer the value of r2 is to 1, the better
the least-squares equation ^y ¼ ^b1x + ^b0 performs as a predictor of y.
2. Homoscedasticity (homogeneity of variance). This assumption says that the
variance of the error term remains constant across all values of x. In this case we
know by the Gauss-Markov theorem that the least-squares estimators ^b0 and ^b1
are the best linear unbiased estimators of b0 and b1. A frequently used graphical
method is to draw the residuals versus a fitted plot. This can be easily done using
statistical software packages. The graph of residuals ei versus fitted values ^Yi or
explanatory variable xi indicates a change in the spread of residuals as ^Y or x
changes. It may look like Figure 8.7.
If the variances of yi values are not constant, the inferences we made, such as
confidence intervals on means, prediction, and so forth, are off. The severity of this
discrepancy depends on the degree of the assumption violation. If we see that the
patternofdatapointsonlychangesslightly,thatwillindicateamildheteroscedasticity.
Two numerical tests for heteroscedasticity are explained in Section 14.4.3.
3. Independence of ei and ej, for i6¼j. This assumption specifies that the errors
associated with one observation should not be correlated with the errors of any
other observation. In general, whether the two samples are independent of each
other is decided by the structure of the experiment from which they arise.
Violation of the independence assumption can occur in a variety of situations.
For example, if we take a survey on a certain issue on children’s education from
one particular school, these observations may reflect some pattern, thus violating
447
8.7 Regression Diagnostics

the independence assumption. If data are collected on the same variable over
time, then the assumption of independence will be violated. Project 12B explains
a run test for check of this assumption. Also, see Section 14.4.4.
4. Normality of the errors. This assumption specifies that the distribution of the ei
valuesshouldbenormal.Thisassumptioniscrucialwhensamplesizeissmallifthep-
value for the test is to be valid. For large samples, by the Central Limit Theorem this
assumption becomes less important unless the prediction of a single value of y is
involved. Thus a test of normality is necessary mainly when the t-test is used.
Section 14.4.1 explains some of the tests for normality. A simple way is to draw a
probabilityplotfortheerrorstoconformtotheassumptionofnormality.Ifweobserve
nonnormality, one of the ways to overcome the problem is to use data transformation
such as logarithmic transformation, as explained in Section 14.4.2, and perform the
regression analysis on the transformed data. Sometimes nonparametric methods may
be more appropriate, but we will not deal with this topic in this book.
Another important issue is the existence of influential observations, individual obser-
vations that have a strong influence on estimated coefficients. If a single observation
substantially changes our results, we need to do further investigation. The ordinary
least-squares method is quite sensitive for outlying observations, both for indepen-
dent variables and for dependent variables, and can have an adverse effect on the
estimate. In higher dimensional data, these outlying observations can remain unno-
ticed. This aspect in one explanatory variable case is discussed in Project 8C. One of
the simple ways to identify such observations is to draw a scatterplot. In the scatter-
plot, if we see a data point that is farther away from the rest of the data points, that is
an indication of possible influential points.
0
10
20
30
40
50
60
70
80
90
−20
−10
0
10
20
Fitted value
Residual
Residuals versus the Fitted values
(response is C2)
FIGURE 8.7
Scatterplot of fitted values versus residuals.
448
CHAPTER 8 Linear Regression Models

The natural question is, if we find that the data violate one or more of the assump-
tions, what can we do about it? We have already explained that violation of the nor-
mality assumption in large samples is not an issue unless prediction is involved,
because prediction depends on normality of an individual observation. Thus, if the
inferences are based on the t- or F-tests or prediction is involved, we may be able
to transform Y to Y0 to achieve normality. If we have predicted Y0, then back-transform
to predict Y. If we observe nonlinearity of data, we may be able to transform x to x0 ¼
h(x) such that Y is linear in x0, or consider a polynomial model in x, in which case the
ideas of multiple linear regression may be utilized. Robust estimates of variances of
b0 and b1 or the method of weighted least squares may be used to deal with the case of
nonconstantvariance.Oftencarefulexperimentaldesigncouldbedonetoremovepos-
siblecorrelationinerrors.Therearealsorobustmethodsavailableforcorrelation anal-
ysis. We refer to specialized books on regression methods for further details on these
issues. If we detect influential observations, there are statistical techniques available,
such as least trimmed squares estimators, to deal with outlying observations.
8.8 CHAPTER SUMMARY
In this chapter, we first derived the least-squares line and its properties. Then we
learned about the confidence intervals for the coefficients in the regression model
and did hypothesis tests on the values of the coefficients. We introduced the matrix
notation for linear regression as well as for multiple regression. We discussed how to
predict a particular value of Y for a given value of X. In order to study the dependence
of X and Y, we presented correlation analysis.
The following are some of the key definitions we have used in this chapter.
•
Predictors.
•
Response variable.
•
Regression analysis.
•
Multiple linear regression model.
•
Simple linear regression model.
•
Sum of squares for errors (SSE).
•
Sum of squares of the residuals.
•
Least-squares line.
•
Least-squares equations.
•
Normal equations.
•
Best linear unbiased estimator (BLUE).
•
Correlation analysis.
The following important concepts and procedures were discussed in this chapter:
•
Procedure for regression modeling.
•
Procedure for fitting a least-squares line.
•
Properties of the least-squares estimators for the model Y¼b0+b1x+e.
•
The Gauss-Markov theorem.
449
8.8 Chapter Summary

•
Procedure for obtaining confidence intervals of b0 and b1.
•
Procedure to obtain a multiple linear regression equation.
•
Prediction interval for the response variable Y.
•
Hypothesis testing for correlation, r.
•
Linearity.
•
Homoscedasticity.
•
Independence of ei and ej, for i6¼j.
•
Normality of the errors.
•
Influential observations.
8.9 COMPUTER EXAMPLES
8.9.1 EXAMPLES USING R
EXAMPLE 8.9.1
For the following data, use the method of least squares regression to fit a straight line to the accompa-
nying datapoints. Givetheestimatesofb0 and b1. Plot thepointsand sketchthefittedleast-squaresline.
Sample (x)
1
0
2
2
5
6
8
11
12
3
Sample (y)
5
4
2
7
6
9
13
21
20
9
This example assumes you put the data into variables x and y. Please modify your code
appropriately.
R Code:
model¼lm(yx);
summary(model);
Solution
From the output below the estimate of b0 is 3.10091, and the estimate of b1 is 2.02656. Hence, the
regression line is ^y ¼ 3:10091 + 2:02656x
Output:
Residuals:
Min
1Q
Median
3Q
Max
1.21775
-0.70220
0.03452
0.17394
1.80880
Coefficients:
Estimate Std. Error t value Pr(>jtj)
(Intercept)
3.10091
0.38882 7.975
4.47e-05
***
x
2.02656
0.06087
33.292
7.23e-10
***
—
Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Residual standard error: 0.9883 on 8 degrees of freedom
Multiple R-squared: 0.9928, Adjusted R-squared: 0.9919
F-statistic: 1108 on 1 and 8 DF, p-value: 7.232e-10
450
CHAPTER 8 Linear Regression Models

EXAMPLE 8.9.2
Now obtain the fitted regression line, using results from the previous example.
This example assumes you have your linear model stored in the variable model from the previous
example. This example also assumes you have the data from the previous example stored in x and y.
Please modify your code appropriately.
R Code:
Yhat¼predict(model,data¼x);
plot(x,y);
lines(x,yhat);
c¼confint(model);
New command for confidence interval of model estimates
m¼model;
m$coefficients[1]¼c[1];
lines(x,predict(m,data¼x),col¼”blue”);
m$coefficients[1]¼c[3];
lines(x,predict(m,data¼x),col¼”blue”);
m¼model;
m$coefficients[2]¼c[2];
lines(x,predict(m,data¼x),col¼”red”);
m$coefficients[2]¼c[4];
lines(x,predict(m,data¼x),col¼”red”);
Output:
We obtain a graph with confidence intervals for the intercept in blue and confidence intervals for the
slope in red. The coefficient of determination r2 is 0.9928, and the p-value is small suggesting the
model fit pretty good.
20
15
10
5
y
0
0
5
x
10
−5
−10
451
8.9 Computer Examples

EXAMPLE 8.9.3
In this example we’ll be using matrix multiplication to preform linear regression. The follow-
ing is a random sample of height (in inches) and weight (in pounds) of several basketball
players.
Sample (x) ¼
73
83
77
80
85
71
80
Sample (y) ¼
186
234
208
237
265
190
220
Calculate the least-squares regression line for this data. This example assumes you have placed
the data into variables x and y. Please modify your code appropriately.
R Code:
library(’MASS’);
Required for ginv() function
x¼cbind(c(1:length(x))*0+1,x);
Creates a matrix with a column of 1's for the intercept
b¼ginv(t(x)%*%x)%*%t(x)%*%y;
Store coefficients into b
yhat¼x%*%b;
Calculate yhat using the regression equation
plot(x[,2],y);
lines(x[,2],yhat);
Output:
Looking at the coefficients, we see that ^b0 ¼ 188:476 and ^b1 ¼ 5:208. Hence, the regression line is
given by ^y ¼ 188:476 + 5:208x. It is more difficult to preform confidence intervals and other tasks
since this is done using matrices instead of model objects.
260
240
220
y
200
72
74
76
78
x
80
82
84
452
CHAPTER 8 Linear Regression Models

EXAMPLE 8.9.4
Consider the following advertisement expense versus total sales data.
Year
Advertising
Cost ($)
Yearly Sales
Volume (Units)
1999
20,210
112,485
2000
22,469
118,332
2001
23,982
122,435
2002
24,645
125,569
2003
24,988
125,880
2004
25,250
127362
2005
25,978
125,967
2006
26,556
127,252
2007
26,978
127,456
2008
27,125
127,789
2009
27,461
128,313
2010
28,120
128,662
2011
28,888
128,879
2012
29,200
129,290
Use the method of least-squares regression to fit a straight line to the accompanying data points.
Plot the points and sketch the fitted least-squares line. Interpret the output.
R-code
>x<c(22469,23982,24645,24988,25250,25978,26556,26978,27125,27461,28120,28888,29200)
>y<c(118332,122435,125569,125880,127362,125967,127252,127456,127789,128313,
128662,128879,129290)
> model¼lm(yx)
> summary(model)
8.9.2 MINITAB EXAMPLES
EXAMPLE 8.9.5
For the data in Example 8.2.1, use the method of least squares to fit a straight line to the accompanying
data points. Give the estimates of b0 and b1. Plot the points and sketch the fitted least-squares line.
Solution
Enter independent variable, x, in C1 and the response variable, y, in C2. Then:
Stat > Regression > Regression. . . > in Response: type C2, and in Predictors: type C1 > click
OK.
Now to obtain the fitted regression line, use the following procedure:
Stat > Regression > Fitted Line Plot. . . > in Response(Y): type C2, and in Predictors(X): type C1
> click Linear OK.
If in addition, we need, say, 95% confidence and predictor bands, then use
Stat > Regression > Fitted Line Plot. . . > in Response(Y): type C2, and in Predictor(X): type C1
> click
Linear > click options. . . > click Display confidence bands and Display predictor bands > in
Title: type a title for the graph and OK > OK.
453
8.9 Computer Examples

8.9.3 SPSS EXAMPLES
A detailed explanation of regression methods including diagnostics using SPSS can
be obtained at the site: http://www.ats.ucla.edu/stat/spss/webbooks/reg/. We will just
demonstrate a simple case with an example.
EXAMPLE 8.9.6
The following is a random sample of height (in inches) and weight (in pounds) of seven basketball
players.
Height
73
83
77
80
85
71
80
Weight 186 234 208 237 265 190 220
Calculate the least-squares regression line for these data using SPSS
Solution
Enter height in column 1 and weight in column 2. Then
Analyze > Regression > Linear. . . > move var00002 to dependent:, and var00001 to
Independent(s): > click OK.
8.9.4 SAS EXAMPLES
For regression analysis, we can use the SAS procs called GLM, which stands for gen-
eral linear model, and REG, which stands for regression. In the following example,
we will give a simplified version of the foregoing procedure. A good explanation of
regression methods including diagnostics using SAS can be obtained at http://www.
ats.ucla.edu/stat/sas/webbooks/reg/.
EXAMPLE 8.9.7
Using the SAS commands, redo Example 8.9.1.
Solution
We can use the following commands.
options nodate nonumber;
data exreg;
INPUT x y @@;
datalines;
1 5
0 4
2
2
2 7
5
6
6
9
8 13
11 21
12 20
3 9
;
proc reg data¼exreg;
title ‘Regression of Y on X’;
model y¼x / p clm;
run;
454
CHAPTER 8 Linear Regression Models

We obtain the following output.
Regression of Y on X
The REG Procedure
Model: MODEL1
Dependent Variable: y
Analysis of Variance
Source DF Sum of Squares Mean Square F Value Pr > F
Model
1
1082.58589
1082.58589
1108.34
<.0001
Error
8
7.81411
0.97676
Corrected Total 9
1090.40000
Root MSE
0.98831
R-Square
0.9928
Dependent Mean
4.60000
Adj R-Sq
0.9919
Coeff Var
21.48508
Parameter Estimates
Parameter
Standard
Variable
DF
Estimate
Error
t Value
Pr > jtj
Intercept
1
–3.10091
0.38882
7.98
<.0001
x
1
2.02656
0.06087
33.29
<.0001
Regression of Y on X
The REG Procedure
Model: MODEL1
Dependent Variable: y
Output Statistics
Dep Var Predicted Std Error
Obs
y
Value
Mean
Predict
95% CL Mean
Residual
1
–5.0000
–5.1275
0.4278
–6.1141
–4.1409
0.1275
2
–4.0000
–3.1009
0.3888
–3.9975
–2.2043
0.8991
3
2.0000
0.9522
0.3312
0.1885
1.7159
1.0478
4
–7.0000
–7.1540
0.4715
–8.2413
–6.0667
0.1540
5
6.0000
7.0319
0.3210
6.2917
7.7720
–1.0319
6
9.0000
9.0584
0.3400
8.2743
9.8425
–0.0584
7
13.0000
13.1115
0.4038
12.1804
14.0427
–0.1115
8
21.0000
19.1912
0.5383
17.9499
20.4325
1.8088
9
20.0000
21.2178
0.5889
19.8597
22.5758
–1.2178
10
–9.0000
–9.1806
0.5187
–10.3766
–7.9845
0.1806
Sum of Residuals
0
Sum of Squared Residuals
7.81411
Predicted Residual SS (PRESS)
14.18340
By looking at the parameter estimates in the foregoing output, we see that an
intercept value of 3.10091 is the estimate of b0, and the estimate of b1 is
2.02656, corresponding to the variable x. For each value of x, the actual value
and predicted value of y are given as the output statistics.
It is important to note that the presentation of results of analysis in a simple way is
as important as the analysis itself. For example, if one is interested only in a simple
linear regression, most of the output values in the foregoing output may not be
necessary. All the values until the parameter estimates are giving us the analysis
of variance results, and all the values in the REG procedure are dealing with
455
8.9 Computer Examples

prediction and confidence intervals. For clarity and simplicity of report, we may only
need to report the regression line, and perhaps the graph of the line.
If we need the plot of the points (x, y), add the following commands to the pre-
vious program. We will not give the corresponding graph.
proc plot data¼exreg;
title ‘Plot of Y Vs. X’;
plot y*x;
run;
If we need the graph of the regression line along with, say, 95% prediction and
confidence intervals, we add the following.
proc gplot data¼exreg;
plot y*x
y*x
y*x / overlay frame vaxis¼axis1 haxis¼axis2;
symbol1 v¼h¼1.5 i¼none c¼black;
symbol2 v¼none i¼rlclm95 c¼red;
symbol3 v¼none i¼rlcli95 c¼blue;
axis1 order ¼ (5 to 14 by 1)
offset¼(1)
label¼(h¼1.5 f¼duplex);
axis2 order¼(10 to 20 by 1)
offset¼(1)
label¼(h¼1.5 f¼duplex);
title h¼1.5
’Effect of X on Y’;
title2 h¼1.2 f¼duplex
’Common regression line with 95% confidence
interval’;
title3 h¼1.5 f¼duplex
’Regression line is predicted Y¼3.1011
+2.0266X’;
run;
PROJECT FOR CHAPTER 8
8A. CHECKING THE ADEQUACY OF THE MODEL BY SCATTERPLOTS
If the regression model is adequate, then the fitted equation can be used to make
inferences. Otherwise, the inferences made will be practically useless. Note that
the residuals give all the information on lack of fit. Figures 8.5 and 8.6 give an indi-
cation of good fit and misfit.
(i) Collect a couple of real-life data and find a regression line for each.
(ii) Draw the scatterplot for the residuals ei versus x and determine whether the
regression lines obtained in (i) are a good fit or not.
456
CHAPTER 8 Linear Regression Models

8B. THE COEFFICIENT OF DETERMINATION
One of the ways to measure the contribution of x in predicting y is to consider how
much the prediction errors were reduced by using the information provided by the
variable x. The quantity called the coefficient of determination measures how well
the least-squares equation ^y ¼ ^b1x + ^b0 performs as a predictor of y. If x contributes
no information for predicting y, then the best prediction for values of y is simply the
sample mean y. The resulting sum of squares of deviation for this model ^y ¼ y is Syy ¼
Pn
i¼1 yi y
ð
Þ2: In the case where x contributes information for predicting y, then we
have seen that the sum of squares of deviation for the model ^y ¼ ^b1x + ^b0 is
Syy ¼ Pn
i¼1 yi  ^yi
ð
Þ2: It can be shown that Pn
i¼1 yi  ^yi
ð
Þ2 	 Pn
i¼1 yi y
ð
Þ2:
The coefficient of determination is the proportion of the sum of squares of devi-
ations of the y-values that can be credited to a linear relationship between x and y.
This is defined by
r2 ¼ Syy SSE
Syy
¼ 1SSE
Syy
¼ 1
Xn
i¼1 yi  ^yi
ð
Þ2
Xn
i¼1 yi y
ð
Þ2:
We can see that 0	r2	1. We can interpret r2 to be the proportion of variability
explained by the regression line. When x contributes no information for predicting
y, Syy and SSE will be nearly equal, and hence r2 will be near to zero. If x contributes
information for predicting y, Syy will be larger than SSE, and hence r2 will be greater
than zero. Thus, r2¼0.75 means that use of ^y instead of y to predict y reduced the sum
of squares of deviations of the y-values about their predicted values ^y by 75%. This
can also be interpreted as meaning that nearly 75% of the variation is explained by
the independent variable x. In general, about (r2
100)% of the sample variation in y
can be attributed to using x to predict y in the linear model. The coefficient of
nondetermination is the percent of variation that is unexplained by the regression
equation and is given by 1r2.
(i) For Exercises 8.2.2 and 8.2.3, find the coefficient of determination, and discuss
the information contributed by x in predicting y.
(ii) Collect a couple of real-life data and find the corresponding regression lines.
Also draw the scatterplot for ei versus ^y and determine whether the regression
line obtained is a good fit or not based on the coefficient of determination.
8C. OUTLIERS AND HIGH LEVERAGE POINTS
One of the important aspects of residual analysis is to identify any existence of
unusual observations in a data set. There are two possibilities for a data point to
be unusual. It could be in the response variable (i.e. in the horizontal direction) repre-
senting model failure, or in the predictor variable (i.e. in the vertical direction). It
457
Project for Chapter 8

should be noted that unusual observations in the horizontal direction occur when we
assume that the independent variable X in the linear model is random. An observation
that is unusual in the vertical direction is called an outlier. An observation that is
unusual in the horizontal direction is called a high leverage point (or just leverage
point).
Consider the following 10 points, which we will call base points, and three
additional points representing an outlier (O), a high leverage point (H), and both
(OH), respectively.
10 Base points
O
H
OH
x
1
0
2
2
5
6
8
11
12
3
6
19
19
y
5
4
2
7
6
9
13
21
20
9
30
13
30
Investigate the effect of adding a single aberrant point by running four separate
regressions: (i) regression for 10 base points; (ii) regression for 10 base points plus
O; (iii) regression for 10 base points plus H; and (iv) regression for 10 base points
plus OH. For each of them, find ^b0 and ^b1 as well as the coefficient of determination.
Discuss the effects of each type of outlier on the regression line.
458
CHAPTER 8 Linear Regression Models

CHAPTER
Design of Experiments
9
CHAPTER CONTENTS
9.1 Introduction .................................................................................................... 460
9.2 Concepts from Experimental Design .................................................................. 461
9.3 Factorial Design .............................................................................................. 477
9.4 Optimal Design ................................................................................................ 481
9.5 The Taguchi Methods ...................................................................................... 484
9.6 Chapter Summary ............................................................................................ 488
9.7 Computer Examples ......................................................................................... 489
Projects for Chapter 9 ............................................................................................ 493
OBJECTIVE
To study the basic design concepts for experiments and through which we can make
comparisons of treatments with respect to the observed responses.
Genichi Taguchi
(Source: http://www.amsup.com/BIOS/g_taguchi.html)
Mathematical Statistics with Applications in R
Copyright © 2015 Elsevier Inc. All rights reserved.
459

Genichi Taguchi (1924-2012) acquired his statistical skills under the guidance of
Prof. Motosaburo Masuyama, one of the best statisticians of his time. After World
War II, Japanese manufacturers were struggling to survive with very limited
resources. Taguchi revolutionized the manufacturing process in Japan through cost
savings. He understood that all manufacturing processes are affected by outside
influences—noise. However, Taguchi realized methods of identifying those noise
sources that have the greatest effects on product variability. Isolating these factors
to determine their individual effects can be a very costly and time-consuming
process. Taguchi devised a way to use the so-called orthogonal arrays to isolate these
noise factors from all others in a cost-effective manner. He introduced the loss func-
tion to quantify the decline of a customer’s perceived value of a product as its quality
declines. Taguchi referred to the ability of a process or product to work as intended
regardless of uncontrollable outside influences as robustness. This was a novel
concept in the DOEs with profound influence in manufacturing. His ideas have been
adopted by successful manufacturers around the globe because of their results
in creating superior production processes at much lower costs.
9.1 INTRODUCTION
In statistics, we are concerned with the analysis of data generated from an experi-
ment. How do we collect data to answer our research questions? What should our
design be? It is desirable to take the necessary time and effort to organize the exper-
iment appropriately so that we have the right type of data and sufficient amount of
data to answer the questions of interest as clearly and efficiently as possible. This
process is called experimental design. We can trace the roots of modern experimental
design to the 1935 publication of the book The Design of Experiments, written by Sir
Ronald A. Fisher. He showed how one could conduct credible experiments in the
presence of many naturally fluctuating conditions such as the soil condition, temper-
ature, and rainfall, in an agricultural experiment. The design principles that were
developed for agricultural experiments were successfully modified and adapted to
industrial, military, and other applications. In modern industry it is essential to man-
ufacture parts efficiently and with practically no defects. As a result, variation reduc-
tion in quality characteristics of these parts has become a major focus of quality and
productivity improvement. Dr Genichi Taguchi pioneered the use of design of exper-
iments (DOE) in designing robust products—those relatively insensitive to changes
in design parameters. Presently, DOE is used as an essential tool for improving the
quality of goods and services. It is important to note that, unless a sound design is
employed, it may be very difficult or even impossible to obtain valid conclusions
from the resulting data. Also, properly designed experiments will generate more pre-
cise data while using substantially fewer experimental runs than ad hoc approaches.
In industrial manufacturing, some of the major benefits of DOE are lower costs,
simultaneous optimization of several factors, fast generation and organization of
quantitative information, and overall quality improvement.
460
CHAPTER 9 Design of Experiments

It is important to clearly identify the particular questions that an experiment is
intended to answer (i.e. the major objective of the experiment) before performing
the experiment. These objectives may be to estimate or predict some unknown
parameters, to explore relationships among various factors, to compare a collec-
tion of effects or parameters, or any combinations of these. When the intention is
to compare parameters, the objective may be to corroborate a hypothesis, or to
explore some simple relationships. In any design, it is necessary to identify the
populations that are to be studied and the type of information about these popula-
tions that will be needed to answer the desired questions. While planning an
experiment to investigate the primary objectives of the investigation, we need
to ensure that the measurement process is simple, the cost of the study is reason-
able, the study can be concluded in a reasonable time frame, and the study pro-
duces reliable data. Because of the complex nature of real-world problems,
planning an effective experiment is not an easy task. The important issues con-
fronting one area, say engineering, will be different from those for another area
such as biology or medicine. As a result, the DOEs can take several forms. In this
chapter, we will follow a general framework. Two of the major distinguishing
elements of DOE are (1) simultaneous variation and evaluation of various factors,
and (2) systematic removal of some of the possible test combinations to cut back
experimental time and cost. Thus, a researcher should ensure that the statistical
design is as simple as possible given the objectives of the experiment and within
the practical constraints such as material, labor, and cost. Some other desirable
criteria of a good design are that it provides unbiased estimates of treatment
effects and the experimental error. In addition, it should be able to detect impor-
tant small differences with sufficient precision, and it should provide an estima-
tion of uncertainty in the conclusions and the confidence with which the result can
be extended to other analogous situations. The experimental design determines
the basic characteristics of the data collected. These data are then processed using
statistical analysis techniques, with the goals of these analyses being determined
by the experimental objectives. Conclusions are obtained by looking at the results
of the statistical analyses.
9.2 CONCEPTS FROM EXPERIMENTAL DESIGN
In this section we introduce some of the basic definitions, methods, and procedures
used in the experimental design. Many of the terms used have an agricultural basis,
because the early development and applications of DOE were in the field of
agriculture.
9.2.1 BASIC TERMINOLOGY
The first step in planning an experiment is to formulate a clear statement of objec-
tives of the test program. The purpose of most statistical experiments is to determine
461
9.2 Concepts from Experimental Design

the effect of one or more independent variables on the response variable. The main
variable of interest in a study is the response variable, also called an output variable.
These are the dependent variables (also referred to as criteria, effect, or predicted
variable) in an experiment that describes the factors we are interested in predicting
or comparing. The response variable is measured with different values of indepen-
dent variables (representing those factors that are assumed to be the causes of the
outcome) and analyzed to determine whether the independent variables have any
effect. For example, in an agricultural experiment, the crop yield could be the
response variable, whereas the type of soil, temperature, and rainfall could be the
independent variables. We would like also to identify known or expected sources
of variability in the experimental units, because one of the main aims of a designed
experiment is to reduce the effect of these sources of variability on the answers to
questions of interest. Hence, we must make a list of the factors that may affect
the value of the response variable. We must also decide how many observations
should be taken and what values should be chosen for each independent variable
in each individual test run.
Definition 9.2.1 The variables that an experimenter is able to completely control
in the DOE are called independent variables or treatment variables. These are
also called input variables, explanatory variables, or factors.
Basically, factors are independent variables whose effect on the response vari-
able is a main objective of the study. These are control variables selected by the ana-
lyst for comparison. A factor is a general category or type of treatment. Factors can
be either quantitative or qualitative based on whether the variable is measured on a
numerical scale or not. For example, a rice field is divided into six parts, and each
part is treated with a different fertilizer to see which produces the most rice. Here the
response variable is the amount of rice output. The objective of the study is to com-
pare the effects of different fertilizers on the rice output. Thus, the type of fertilizer is
the factor.
Definition 9.2.2 Independent variables that are unknown or known but nonma-
nipulable are called nuisance variables.
The factors, that we could change but we deliberately keep fixed, are called the
constants in the experiment. A factor can have different levels referred to as the treat-
ment or factor levels. Different treatments constitute different levels of a factor.
Levels are the values at which the factors are set in an experiment. The level of a
variable or treatment means its amount or magnitude. For example, if the experimen-
tal units of a medication were given as 2.5, 5, and 10 mg, those amounts would be
three levels of the treatment. Level is also used for categorical variables, such as
drugs I, II, and III, where the three are different kinds of drugs, not different amounts
of the same thing. Suppose four different groups of students are subjected to four
different teaching methods. The students are the experimental units, the teaching
methods are the treatments, and the four types of teaching methods constitute four
levels of the factor “type of teaching.” Note that this is a single-factor experiment, the
factor being the method of teaching.
462
CHAPTER 9 Design of Experiments

Definition 9.2.3 Noise is the effect of all the uncontrolled factors in an
experiment.
In some experiments, all the noise factors are known; however, in most cases
only some of them are known. When an analyst controls the specification of the
treatments and the method of allocating the experimental units to each of the treat-
ments, the experiment is called designed. For example, n rats are randomly
assigned to one of the five dose levels of an experimental drug under investigation.
The analyst can also decide on the number ni of rats for each dose level such that
P
i¼1
5
ni¼n.
Sometimes, conducting a designed experiment may not be practical or ethical.
For example, if an analyst wants to know the relationship between fat content in
a diet and the cholesterol level, it would be unethical and costly as well as time con-
suming to subject human volunteers to different fat-content diets. However, it is pos-
sible to observe the cholesterol levels of people who consume different diets. Care
must be taken to record various other factors, such as exercise habits, age, and gen-
der, before reporting any association between cholesterol levels and fat content of
diets. The experiment is called observational, if the analyst is just an observer of
the treatments on a sample of experimental units. Note that the experimental units
are objects to which treatments are applied.
The crucial difference between an experiment and an observational study for
comparing the effects of treatments is that, in an experiment, the researcher decides
which experimental units receive which treatments, whereas in an observational
study, the researcher simply compares experimental units that happen to be there that
have received each of the treatments. Observational studies are often useful for iden-
tifying possible causes of treatment effects, and they are often cheaper. Their main
disadvantage is that they are less conclusive. Only properly designed and executed
experiments can lead to reliable conclusions. Hence, in general designed experi-
ments are preferred over observational experiments. In designing the experiment,
there are almost always going to be constraints such as budget, time, and availability
of experimental units.
The following example illustrates an observational experiment, where the analyst
has control over the random sampling from the treatment populations as well as the
size of each sample, but has no control over the assignment of the experimental units
to the treatments.
EXAMPLE 9.2.1
In order to compare the risk-taking tendency of the people that invest in mutual funds, samples are
taken of individuals from three income groups—low income class, middle income class, and high
income class. A score is given based on the percentage of their investment allocation on different
types of mutual funds, such as large-cap, mid-cap, small-cap, hybrid, and specialty. The mean score
for each income group is calculated. Identify each of the following elements: response, factors and
factor type(s), treatments, and experimental units.
Continued
463
9.2 Concepts from Experimental Design

Solution
The response is the variable of interest, which is the score given to each individual investor. The only
factor investigated is the income class. This is a qualitative variable. The three income classes rep-
resent the levels of this factor. The treatment is the percentage investments in different types of
mutual funds, such as large-cap, mid-cap, small-cap, hybrid, and specialty. The experimental unit
is the individual investor.
There are single-factor experiments and multifactor experiments. The previous
example was a case of a single-factor experiment. Single-factor experiments have only
one independent variable. Another example of a single-factor experiment is when we
are interested in the effect of size of the screen of a computer monitor on the reading
speed. In this case, the size of the screen is the single factor. If there are only two sizes,
say 15 and 17-in. monitors, that we wish to compare, tests such as the two-sample t-test
could be used to compare average reading speed. If there are more than two sizes of
monitors, then the one-way analysis of variance (ANOVA) methods described in
Chapter 10 could be used for analysis of the resulting data.
Even though the single-factor experiments are simple and elegant, they are costly
and not very effective when there is more than one independent variable. Efficient
use of resources is achieved through multifactor experiments in comparison to con-
ducting many single-factor experiments. A multifactor experiment involves two or
more independent variables and a dependent variable. Also, a greater range of ques-
tions could be answered using multifactor experiments. The resulting data are ana-
lyzed using ANOVA as described in Chapter 10. The following is an example of a
multifactor experiment.
EXAMPLE 9.2.2
In order to study the conditions under which a particular type of commercially raised fish reach
maximum weight, an experiment is conducted at four water temperatures (60, 70, 80, and 90 F)
and four water salinity levels (1%, 5%, 10%, and 15%). Fish are raised in tanks with specific salinity
levels and temperature levels. There are 32 tanks and one of the four temperatures and one of the four
salinity levels are assigned randomly to each tank. The weights are recorded at the beginning of the
experiment and after two months. Identify each of the following elements: response, and factors and
factor type(s). Write all the treatments from the factor-level combinations.
Solution
The response is the variable of interest, which is the weight gain of a fish. This experiment has two
factors: water temperatures at four levels and water salinity at four levels. There are 44¼16 pos-
sible treatments:
60F,1%
ð
Þ
60F,5%
ð
Þ
60F,10%
ð
Þ
60F,15%
ð
Þ
70F,1%
ð
Þ
70F,5%
ð
Þ
70F,10%
ð
Þ
70F,15%
ð
Þ
80F,1%
ð
Þ
80F,5%
ð
Þ
80F,10%
ð
Þ
80F,15%
ð
Þ
90F,1%
ð
Þ
90F,5%
ð
Þ
90F,10%
ð
Þ
90F,15%
ð
Þ:
It should be noted that there may be other factors, such as the density of the fish pop-
ulation, the initial size of the fish, and the type of feeding that may affect weight gain
of fish.
464
CHAPTER 9 Design of Experiments

Definition 9.2.4 The experimental error explains the variation in the responses
among experimental units that are assigned the same treatment and observed under
identical experimental conditions.
Experimental error can occur for many reasons, among them (1) the difference in
the devices that record the measurements, (2) the natural dissimilarities in the exper-
imental units prior to their receiving the treatment, (3) the variation in setting the
treatment conditions, and (4) the effect on the response variable of all extraneous
factors other than the treatment factors.
In order to construct confidence intervals on the treatment population means
and to test hypotheses, it is necessary to obtain an estimate of the variance of exper-
imental design. In a single-factor experiment with k levels, the estimate of the var-
iance of experimental design could be taken as the pooled variance of responses
from experimental units receiving the identical treatments. A large variance of
experimental error will compromise the accuracy of inferences made from
the experiments. Also, large amounts of experimental error make it difficult to
determine whether the treatment has produced an effect or not, so one of the design
goals is to reduce the experimental error. Bad execution of a design can lead to
the whole experiment becoming a waste of time and resources. It is necessary to
implement techniques to reduce experimental error in order to obtain more acc-
urate inferences. One approach to reducing experimental error is to take extra care
in conducting the experiment. The effect of experimental error can be reduced
by using more homogeneous experimental materials (if available), and using
the fundamental principles of replication, randomization, and blocking (see
Section 9.2.2).
The one-way ANOVA (in a single-factor experiment at several levels) enables one
to compare several groups of observations, all of which are independent with the pos-
sibility of a different mean for each group. A test of significance is whether or not all
the means are equal. Two-way ANOVA is a method of studying the effects of two
factors on the response variable.
There are other terms that are important in different applications. For example,
in the medical field, the terms blinding, double-blind, and placebo are used. In a
medical experiment, the comparison of treatments may be distorted if the patient,
the person administering the treatment, and those evaluating it know which treatment
is being allocated to which patient. It is therefore necessary to ensure that the patient,
and/or the person administering the treatment, and/or the trial evaluators do not
know (are blind to) which treatment is allocated to whom. If only the patient is
unaware of the treatment, it is called blinding, and if both the patient and the person
administering the treatment are blind to which treatment is being allocated, it is
called double-blinding. In order to study the effect of a particular drug, experi-
menters divide the study population into two groups and treat one group with the
drug and the other group with a so-called placebo, which could be just sugar pills.
In order to clarify the objective of a design, it is necessary for an experimental
designer to consult a wide range of people, especially those affected by the problem
to be solved.
465
9.2 Concepts from Experimental Design

9.2.2 FUNDAMENTAL PRINCIPLES: REPLICATION, RANDOMIZATION,
AND BLOCKING
A good design of an experiment makes efficient use of resources to gather the data
needed to meet the goals of the study. There are three fundamental principles that
need to be considered in a good experimental design. They are replication, random-
ization, and blocking. Replication and blocking increase precision in the experiment,
whereas the randomization reduces the bias.
Definition 9.2.5 Replication means that the same treatment is applied (i) several
times to the same experimental units, or (ii) one time to several similar experimental
units, called replicate units.
Replications are necessary for the estimation of the error variance in an experi-
ment against which the differences among treatments are assessed. If an experiment
is intended to test whether or not a number of treatments differ in their effects, these
treatments must be applied to replicate units of the experiment. In order to show that
two treatments have different mean effects, we need to measure several samples
given the same treatment. For example, observing that one plant of a particular geno-
type is more resistant to a disease than another plant of a different genotype does not
convey anything about the difference between the mean disease resistances of the
two genotypes. This difference could have been caused by the environment or the
inoculation procedure affecting the two plants differently. Hence, to make any infer-
ence about the mean difference between the genotypes, we have to test several plants
of each type. Thus, increasing the number of replications increases the reliability of
inferences drawn from the observed data. It is necessary to increase the number of
replications with varied experimental conditions to decrease the variance of the treat-
ment effect estimates and also to provide more power for detecting differences in
treatment effects. We should not confuse multiple observations of the same exper-
imental unit with replication. Replication involves applying the treatment to a num-
ber of experimental units.
Definition 9.2.6 A block is a portion of the experimental unit that is more likely
to be homogeneous within itself than with other units.
Blocking refers to the distribution of the experimental units into blocks in such a
way that the units within each block are more or less homogeneous. The experi-
menter uses information of the possible variability among units to group them in such
a way that most of the unwanted experimental error can be removed through the
block effect.
For blocking to be effective, the units should be arranged so that within-block
variation is much smaller than between-block variation. As an example, suppose
a researcher wishes to compare the yields of rice for four different kinds of fertilizers.
In order to minimize the effect of environmental and soil conditions, the field may be
divided into smaller blocks and each block is further parceled into four plots. Each
variety of fertilizer is applied in each block with one in each parcel. This method
ensures that the external conditions from plot to plot within a block will be relatively
uniform. Then we can use the ANOVA methods to pool from block to block to obtain
466
CHAPTER 9 Design of Experiments

the within-block information about the treatment differences while avoiding
between-block differences. The relevant analysis is given in Section 10.5. Time
could also be a block factor, because the concentration or expertise could alter as
one carries out a task, such as determining disease levels or scoring microscope
slides.
Definition 9.2.7 Randomization is the process of assigning experimental units
to treatment conditions in an entirely chance manner.
The main objective of randomization is to negate the effects of all uncontrolled
extraneous variables. Usually, randomization is associated with design functions
such as random sampling or selection, random assignment, and random order. Ran-
dom assignment of experimental units to groups tends to spread out differences
between subjects in unsymmetric or random ways so that there is no tendency to give
an edge to any group. In any well-conducted experiment, randomization eliminates
bias from the experiment, enables us to use statistical tests of significance, and cre-
ates valid estimates of experimental error. For instance, suppose we are measuring
the time of flowering of plants in a glass house or in a growth cabinet. If the pots are
arranged so that all the plants of one variety are next to each other, and we observe
that one variety flowers earlier than the rest, does this imply that this variety is inher-
ently earlier-flowering, or does it suggest that the light and temperature conditions in
that part of the cabinet or glass house cause plants to flower early? It is not possible to
tell from an experiment designed in this manner. Randomizing the treatments in time
or space is an insurance policy, to take account of variation that we may or may not
know to exist under the conditions of our experiment. For instance, the levels of light
in growth cabinets vary considerably, so randomizing the layout of the plants of dif-
ferent types is essential to make sure that no one type is consistently exposed to light
and temperature levels that are particularly high or low. Another way of selecting
experimental units is simply to use intact groups, such as all students in a particular
statistics classroom. Results obtained this way may be highly biased and hence not
desirable. In general the process of randomization ensures independent observations,
it should be noted that random assignment does not completely eliminate the prob-
lem of correlated data values.
Now we study some steps that can be used for randomization. Suppose there are
N homogeneous experimental units and k treatments. In order to randomly assign
ri experimental units to the ith treatment with P
i¼1
k
ri¼N, we could use the following
steps.
PROCEDURE FOR RANDOM ASSIGNMENT
1. Number the experimental units from 1 to N.
2. Use a random number table or statistical software to get a list of numbers that are random per-
mutations of the numbers 1 to N.
3. Give treatment 1 to the experimental units having the first r1 numbers in the list. Treatment 2 will
be given to the next r2 numbers in the list, and so on; give treatment k to the last rk units in the list.
467
9.2 Concepts from Experimental Design

The following example illustrates the random assignment procedure.
EXAMPLE 9.2.3
In order to study the number of hours to relief provided by five different brands (A, B, C, D, and E) of
pain reliever, doses are administered to 25 subjects numbered 1-25 with each brand administered to
five subjects. Develop a design using the random assignment procedure.
Solution
Using Minitab, we obtained the following random permutations of the numbers from 1 to 25.
1
8
7 12 10 25 23
4
6
3
9
21 5 24 18 16 22 14 17 15
20 13 2 11 19
Using the randomized procedure, we obtain the design given in Table 9.1.
That is, subject number 8 will get brand A pain reliever, subject 23 will get brand B pain
reliever, and so forth. We can rewrite Table 9.1 as shown in Table 9.2.
It should be noted that once we create the design, the actual data will contain the number of
hours to relief for each individual.
It is important to note that randomization may not be possible in some cases. Obser-
vational studies may be necessary whenever the researcher cannot use controlled ran-
domized experiments. For example, if we want to study the effect of smoking on lung
cancer, randomization will mean that we should be able to select a group of people
and tell a randomly selected subgroup to smoke and the other subgroup not to smoke.
This is not only practically impossible; it is also unethical to deliberately expose
people to a potentially hazardous substance.
Table 9.2 Random Permutation of Numbers by Brand
Brand
Subject
A
1
8
7
12
10
B
25
23
4
6
3
C
9
21
5
24
18
D
16
22
14
17
15
E
20
13
2
11
19
Table 9.1 Random Permutation of Numbers 1 to 25
Subject
1
8
7
12
10
25
23
4
6
3
9
21
Brand
A
A
A
A
A
B
B
B
B
B
C
C
Subject
5
24
18
16
22
14
17
15
20
13
2
11
19
Brand
C
C
C
D
D
D
D
D
E
E
E
E
E
468
CHAPTER 9 Design of Experiments

9.2.3 SOME SPECIFIC DESIGNS
In this subsection, we will introduce three specific designs: completely randomized
design, randomized complete block design, and Latin square design. The structure of
the experiment in a completely randomized design is presumed to be such that the
treatments are assigned to the experimental units completely at random.
Example 9.2.1 is one such a design. In order to create a completely randomized
design, follow the procedure given in Section 9.2.2.
The randomized complete block design is a design in which the subjects are
matched according to a variable that the experimenter wants to control. The subjects
are put into groups (blocks) of the same size as the number of treatments. The ele-
ments of each block are then randomly assigned to different treatment groups so as to
reduce the influence of unknown variables. For example, a researcher is carrying out
a study of three different drugs for the treatment of high cholesterol. Suppose she has
45 patients and divides them into three treatment groups of 15 patients each. Using a
randomized block design, the patients are rated and put in blocks of three, based on
the cholesterol level: the three patients with the highest cholesterol are put in the first
block, those with the next highest levels are put in the second block, and so on. The
three members of each block are then randomly assigned, one to each of the three
treatment groups. If there is very little extraneous, systematic variation, complete
randomization allows differences between the mean effects of the treatments to
be estimated with higher precision than other designs. However, it does not allow
for the possibility that there could be some unknown extraneous factors, so if in
doubt, use a randomized complete block design.
Suppose we have k treatments and N experimental units. Further, assume that the
experimental units can be grouped into b groups containing k experimental units, so
that N¼bk. We could use the following steps for a randomized complete block design.
PROCEDURE FOR RANDOMIZATION IN A RANDOMIZED COMPLETE
BLOCK DESIGN
1. Group
the
experimental
units
into
b
groups
(blocks)
containing
k
homogeneous
experimental units.
2. In group 1, number the experimental units from 1 to k and obtain a random permutation of num-
bers 1 to k using a random number generator.
3. In group 1, the experimental unit corresponding to the first number in the permutation receives
treatment 1, the experimental unit corresponding to the second number in the permutation
receives treatment 2, and so on.
4. Repeat steps 2 and 3 for each of the remaining blocks.
We illustrate the step-by-step procedure just given in the following example.
EXAMPLE 9.2.4
In order to study the number of hours to relief provided by five different brands (A, B, C, D, and E) of
pain relievers for pain resulting from different causes [headache (H), muscle pain (M), pain due to
Continued
469
9.2 Concepts from Experimental Design

cuts and bruises (CB)], doses are administered to five subjects each having similar types of pain.
Create a randomized complete block design. Choose, as blocks, the different types of pain (H,
M, or CB).
Solution
Using Minitab with k¼5 we have generated the random permutations shown in Table 9.3 for each of
the b¼3 blocks of five numbers and assigned the treatments according to the procedure just
explained. As the table indicates, among persons with headache, subject number 3 is treated with
brand A pain killer, and so forth.
In the previous example, we had only one replication of each treatment per block.
This idea can be generalized to have r replications of each treatment per block. Then
the generalized randomized complete block design with k treatments, b blocks, and
r replications with N¼kbr which has kr homogeneous experimental units, can be
randomized as follows.
PROCEDURE FOR A RANDOMIZED COMPLETE BLOCK DESIGN
WITH r REPLICATIONS
1. Group the experimental units into b groups (called blocks), each containing rk homogeneous
experimental units.
2. In group 1, number the experimental units from 1 to rk and generate a list of numbers that are
random permutations of the numbers 1 to rk.
3. In group 1, assign treatment 1 to the experimental units having numbers given by the first r num-
bers in the list. Assign treatment 2 to the experiments having next r numbers in the list, and so on
until treatment k receives r experimental units.
4. Repeat steps 2 and 3 for the remaining blocks of experimental units.
The following example illustrates this procedure.
EXAMPLE 9.2.5
With the following modifications, consider Example 9.2.2. Three groups of subjects are considered,
with each group having 15 subjects. Group I consists of subjects with only headache (H), group II of
subjects only with muscle pain (M), and group III of subjects only pain due to cuts and bruises (CB).
Of the 15 with headache (group I), three are treated with brand A pain killer, three with brand B, and
so forth. Subjects with other types of pain are treated similarly. Here the number of replications is
Table 9.3 Random Permutation of Numbers by Block
H
M
CB
3(A)
5(A)
1(A)
1(B)
4(B)
2(B)
2(C)
3(C)
4(C)
5(D)
1(D)
3(D)
4(E)
2(E)
5(E)
470
CHAPTER 9 Design of Experiments

three for each type of drug and for each type of pain. Create a randomized complete block design
with three replications.
Solution
Using Minitab, for the group with headache (H), we generate a random permutation of numbers
1 to 15. The first three are given pain killer A, the next three B, and so forth. The process is repeated
for other types of pain killers. The design is given in Table 9.4 where “2(A)” means that patient
2 is given brand A pain killer.
By increasing the number of replications, we can increase the accuracy of esti-
mators of treatment means and the power of the tests of hypotheses regarding differ-
ences between treatment means. However, because of constraints such as cost, time
needed to handle a large number of experimental units, and even availability of
experimental units, it is not realistic to have a large number of replications. It is
then necessary to determine the minimum number of replications needed to meet
reasonable specifications on the accuracy of estimators or on the power of tests of
hypotheses. We give a simple procedure for determining the number of
replications needed.
Let r be the number of replications that we need to determine. Let s be the exper-
imental standard deviation, and E be the desired accuracy of the estimator. Then the
sample size required to be (1a) 100% confident that the estimator is within E units
of the true treatment mean, m, is
r ¼ za=2

2 ^s2
E2
:
The values of ^s could be obtained from past experiments, from a pilot study, or by
using a rough estimator
^s ¼ largest observationsmallest observation
ð
Þ=4:
Following is an example for determining the appropriate number of replications.
Table 9.4 Random Permutation of Numbers by Brand and Block
H
M
CB
H
M
CB
2(A)
8(A)
3(A)
15(C)
9(C)
11(C)
14(A)
13(A)
8(A)
7(D)
4(D)
2(D)
10(A)
5(A)
14(A)
5(D)
11(D)
13(D)
8(B)
2(B)
6(B)
6(D)
15(D)
5(D)
12(B)
1(B)
15(B)
3(E)
7(E)
1(E)
11(B)
10(B)
12(B)
9(E)
12(E)
4(E)
4(C)
3(C)
10(C)
13(E)
6(E)
9(E)
1(C)
14(C)
7(C)
471
9.2 Concepts from Experimental Design

EXAMPLE 9.2.6
A researcher wants to know the effect of class sizes on the mean score in a standardized test. She
wants to estimate the treatment means m1, m2, m3, and m4 such that she will be 95% confident that the
estimates are within 10 points of the true mean score. What is the necessary number of replications to
achieve this goal? It is known from the previous experiments that scores have ranged from 46 to 98.
Solution
A rough estimator of s is
^s ¼ Range
4
¼ 9846
4
¼ 13:
From the normal table, z0.025¼1.96. The value of E¼10. Thus, the number of replications nec-
essary is
r ¼ za=2

2 ^s2
E2
¼ 1:96
ð
Þ2 13
ð
Þ2
10
ð
Þ2
¼ 6:4923 ﬃ7:
Thus, the researcher should use seven replications of each of the treatments to obtain the
desired precision.
We have used the randomized complete block design when we wanted to control a
single source of extraneous variation and there is only one factor of interest. When
we need to compare k treatment means and there are two possible sources of extra-
neous variation, a Latin square design is the appropriate DOE.
Definition 9.2.8 A kk Latin square design contains k rows and k columns. The
k treatments are randomly assigned to the rows and columns so that each treatment
appears in every row and column of the design.
It was the famous mathematician Leonhard Euler who introduced Latin squares
in 1783 as a new kind of magic squares. Even though the idea is fairly elementary, a
systematic use of Latin squares to the DOEs was advanced by Ronald A. Fisher only
around 1921. Fisher realized that in a two-dimensional plot of land, the systematic
error due to variation in soil and other factors could be minimized by a suitable Latin
square partition of the plot.
The following example illustrates a case in which the experimental problems
are affected by two sources of extraneous variation, the type of car and type of
driver used.
EXAMPLE 9.2.7
A gasoline company is interested in comparing the effect of four gasoline additives (A, B, C, and D)
on the gas mileage achieved per gallon. Four cars (I, II, III, and IV) and four drivers (1, 2, 3, and 4)
will be used in the experiment. Create a Latin square design.
Solution
We can filter out the variability due to type of cars used by ensuring that in each row only one of the
additive types appears. Also, to filter the driver effect, use each additive only once for each driver.
One such randomization results in the Latin square design given in Table 9.5.
To construct a basic Latin square, one can use the following method, which we will present only
for the 44 Latin square of Example 9.2.7.
472
CHAPTER 9 Design of Experiments

PROCEDURE FOR CONSTRUCTING A 4×4 LATIN SQUARE
1. Begin with the first row as A, B, C, and D.
2. Generate each succeeding row by taking the first letter of the preceding row and placing it last,
which has the effect of moving the other letters one position to the left.
3. Randomly assign one block factor to the rows and the other to the columns.
4. Randomly assign levels of the row factor, column factor, and treatment to row positions, column
positions, and letters, respectively.
In step 2 of the foregoing procedure, instead of using the cyclic placement of
rows, we can perform a cyclic placements for the columns. Accordingly, change
the procedures in steps 3 and 4.
The following example illustrates a 44 Latin square design.
EXAMPLE 9.2.8
Using the previous procedure, construct a Latin square for the case of Example 9.2.7.
Solution
Following the procedure just given, the Latin square in Example 9.2.7, the basic Latin square is
represented by Table 9.6.
Now one random assignment of cars, I, II, III, and IV, is to the rows 4, 3, 2, and 1 (this is a
random order of numbers 1, 2, 3, and 4) of Table 9.6. This gives Table 9.7.
Continued
Table 9.5 Latin Square Design of Gasoline Additives
Cars
Drivers
1
2
3
4
I
D
B
A
C
II
C
A
D
B
III
B
D
C
A
IV
A
C
B
D
Table 9.6 Latin Square Design of Cars and Drivers
Cars
Drivers
1
2
3
4
I
A
B
C
D
II
B
C
D
A
III
C
D
A
B
IV
D
A
B
C
473
9.2 Concepts from Experimental Design

Now one random assignment of the drivers 1, 2, 3, and 4 is to the columns 1, 2, 4, 3 (this is a
random order of numbers 1, 2, 3, and 4) of Table 9.7, resulting in the Latin square shown in
Table 9.8.
Now along with this Latin square, we can represent the corresponding observations (numbers in
parentheses are the gas mileage in miles per gallon) as shown in Table 9.9.
Note that if we use the notation 1 for additive A, 2 for additive B, 3 for additive C,
and 4 for additive D, the Latin square in the previous example can be rewritten as
shown in Table 9.10.
This representation will be convenient if we need to write down a model. In order
to test for the treatment effects, one could use the ANOVA methods discussed in
Chapter 10.
Table 9.7 Latin Square Design of Drivers and Random
Order of Cars
Cars
Drivers
1
2
3
4
I
D
A
B
C
II
C
D
A
B
III
B
C
D
A
IV
A
B
C
D
Table 9.8 Latin Square Design of Cars and Random
Order of Drivers
Cars
Drivers
1
2
3
4
I
D
A
C
B
II
C
D
B
A
III
B
C
A
D
IV
A
B
D
C
Table 9.9 Latin Square Design of Cars and Drivers with
Gasoline Additive
Cars
Drivers
1
2
3
4
I
D(18)
A(22)
C(25)
B(19)
II
C(22)
D(24)
B(26)
A(24)
III
B(21)
C(20)
A(22)
D(23)
IV
A(17)
B(24)
D(23)
C(21)
474
CHAPTER 9 Design of Experiments

For Latin square experiments involving k treatments, it is necessary to include k
observations for each treatment resulting in a total of k2 observations. Table 9.11
shows two examples of Latin squares for n¼3, and n¼5.
We have used the Latin square design to eliminate two extraneous sources of var-
iability. In order to eliminate three extraneous sources of variability, we can use a
design called the Greco-Latin square. Greco-Latin squares are also called orthogo-
nal Latin squares. This design consists of k Latin and k Greek letters. In this design,
we take a Latin square and superimpose upon it a second square with treatments
denoted by Greek letters. In this superimposed square, each Latin letter coincides
with exactly one of each Greek letter. In our gasoline example, if we introduce
the effect of, say, four different days, represented by Greek letters, then
Table 9.12 shows the 44 Greco-Latin square.
We will not go into more detail on this design, or on the many other similar designs.
When developing an experimental design, it is important for the researcher to
learn more about the terminology as well as the intricacies of the field in which
Table 9.10 Latin Square Design of Cars and Drivers
with Gasoline Additive in Numbers
Cars
Drivers
1
2
3
4
I
4
1
3
2
II
3
4
2
1
III
2
3
1
4
IV
1
2
4
3
Table 9.11 Latin Square for n=5 and n=3
A
B
C
D
E
B
A
E
C
D
C
D
A
E
B
D
E
B
A
C
E
C
D
B
A
55
A
B
C
C
A
B
B
C
A
33
Table 9.12 Greco-Latin Squares
Aa
Bb
Cg
Dd
Bd
Ag
Db
Ca
Cb
Da
Ad
Bg
Dg
Cd
Ba
Ab
475
9.2 Concepts from Experimental Design

the experiment will be performed. It is also important to observe that there are many
other practical constraints affecting the DOEs. For example, experiments are done by
organizations and individuals that have limited resources of money and time. Appro-
priating these resources within the constraints is an integral part of planning an
experiment. Also, many problems are approached sequentially in several stages.
Planning for each stage is built on what has been learned before. Dealing with these
types of issues is beyond the scope of this book.
EXERCISES 9.2
9.2.1. In order to study the conditions under which hash-brown potatoes will absorb
the least amount of fat, an experiment is conducted with four frying durations
(2, 3, 4, and 5 min) and using four different types of fats (animal fat I, animal
fat II, vegetable fat I, and vegetable fat II). The amount of fat absorbed is
recorded. Identify each of the following elements: response, factors, and
factor type(s). Write all the treatments from the factor-level combinations.
9.2.2. A team of scientists is interested in the effects of vitamin A, vitamin C, and
vitamin D on the number of offspring born for a specific species of mice. An
experiment is set up using the same species of mice. The mice are randomly
assigned to three groups. Each mouse in the study gets the same amount of
food and daily exercise and is kept at the same temperature. One group of
mice gets extra vitamin A, another group gets extra vitamin C, and the
remaining group gets extra vitamin D. The supplements are added to their
food. The number of offspring are counted and recorded for each group.
(a) What is the response variable?
(b) What is the factor?
9.2.3. Thirty rose bushes are numbered 1-30. Three different fertilizers are to be
applied to 10 bushes each. Develop a design using the random assignment
procedure.
9.2.4. Three different fertilizers are to be applied to five bushes each for three
varieties of flower plants: gardenia (G), rose (R), and jasmine (J). Create a
randomized complete block design. Choose as blocks the different types of
plants (G, R, or J).
9.2.5. With the following modifications, consider Exercise 9.2.4. Three groups of
flower plants are considered, with each group having nine plants. Group I
consists of gardenia (G), group II consists of rose (R), and group III consists
of jasmine (J). Of the nine gardenias (group I), three are treated with brand A
fertilizer, three with brand B, and three with brand C fertilizer. Other plant
types are treated similarly. Here the number of replications is three for each
type of fertilizer and for each type of plants. Create a randomized complete
block design with three replications.
9.2.6. What are the reasons for using randomization in Exercises 9.2.3–9.2.5?
9.2.7. Suppose a food processing company wants to package sliced pineapples in
cans. They have four different processing plants, say, A, B, C, and D.
476
CHAPTER 9 Design of Experiments

Suppose they have 56 truckloads (numbered 1-56) of pineapples collected
from different parts of the country. In order to get some uniformity in taste,
it is better to randomly assign the trucks to the four plants. Develop a
design using the random assignment procedure.
9.2.8. In Exercise 9.2.1, suppose there are four pans and 25 packets of hash-brown
potatoes. Randomly select six of the 25 packets to be fried with each of
the fat types.
(a) Create a randomized complete block design.
(b) Create a Latin square design.
9.2.9. A chemist is interested in the effects of five different catalysts (A, B, C, D,
and E) on the reaction time of a chemical process. There are five batches of
new material (1, 2, 3, 4, and 5). She decides to study the effect of each
catalyst on each material for five days (1, 2, 3, 4, and 5). Construct a Latin
square design for this experiment.
9.2.10. Suppose a dating service wants to schedule dates for four women, Anna,
Carol, Judy, and Nancy, with Ed, John, Marcus, and Richard on Thursday,
Friday, Saturday, and Sunday in such a way that each man dates each
woman in the four days. Create a Latin square design displaying a schedule
that the dating service could follow.
9.2.11. In order to test the relative effectiveness of four different fertilizer
mixtures on an orange crop, a Florida farmer applies the fertilizer and
measures the yield per unit area when it harvests. The four experiments
cannot be carried out on the same plot of land. Devise a Latin square
arrangement of dividing a single plot into a 44 grid of subplots for
administering the fertilizers (labeled randomly A, B, C, and D).
9.2.12. A researcher wants to know the effect of four different types of fertilizers
on the mean number of tomatoes produced. He wants to estimate the
treatment means m1, m2, m3, and m4 such that he will be 90% confident that
the estimates are within five tomatoes of the true mean number of tomatoes.
What is the necessary number of replications to achieve this goal? It is
known from previous experiments that the numbers of tomatoes per plant
have ranged from 20 to 60.
9.3 FACTORIAL DESIGN
In this section, we introduce a treatment design where the treatments are constructed
from several factors rather than just being k levels of a single factor. The treatments
are combinations of levels of the factors. A factorial experiment can be defined as an
experiment in which the response variable is observed at all factor-level combina-
tions of the independent variables. A factorial design is used to evaluate two or more
factors simultaneously. In general, there are three ways to obtain experimental data:
one-factor-at-a-time, full factorial, and fractional factorial. The most efficient design
is the fractional factorials. A simple approach for examining the effect of multiple
477
9.3 Factorial Design

factors is the one-at-a-time approach. The advantages of factorial designs over
one-factor-at-a-time experiments is that they allow interactions to be spotted.
An interaction occurs when the effect of one factor varies with the level of another
factor or with some combination of levels of other factors when there are multiple
factors.
The one-way ANOVA, discussed in the next chapter, enables us to compare
several groups of observations, all of which are independent with the possibility
of a different mean for each group. A test of significance is whether or not all the
means are equal. Two-way ANOVA is a way of studying the effects of two factors
separately, such as their main effects, and together, with their interaction effect.
9.3.1 ONE-FACTOR-AT-A-TIME DESIGN
In one-factor-at-a-time design, one conducts the experiment with one factor at a time.
Here we hold all factors constant except one and take measurements on the response
variable for several levels of this one factor, then choose another factor to vary, keep-
ing all others constant, and so forth. We are familiar with this type of experiment
from undergraduate chemistry or physics labs. One of the drawbacks of this method
is that all factors are evaluated while the other factors are at a single setting. For
example, in the case of Example 9.2.2, we would set a fixed temperature and study
the effect of water salinity on fish weight gains, and then set a fixed water salinity and
vary temperature. All these are time consuming.
EXAMPLE 9.3.1
Consider the following hypothetical data, in which two types of diet (fat, carbohydrates) in two
levels (high, medium) were administered for a week for a sample of individuals. At the end of
the week, each subject was put on a treadmill and time of exhaustion, in seconds, was measured.
The objective was to determine the factor-level combination that will give maximum time of
exhaustion. Table 9.13 gives average time to exhaustion for each combination of diet.
Discuss this as a one-factor-at-a-time experiment to predict average time of exhaustion.
Solution
We can see that the average time of exhaustion decreases when fat content is increased from medium
to high while holding carbohydrate at medium. The average time of exhaustion also decreases when
carbohydrate content is increased from medium to high while holding fat at medium. Thus, it is
tempting to predict that increasing both fat and carbohydrate consumption will result in a lower
average time of exhaustion. The problem with this reasoning is that the prediction is based on
Table 9.13 Average Time to Exhaustion
Average Time to Exhaustion
Fat
Carbohydrate
88
High
Medium
98
Medium
Medium
77
Medium
High
74
High
High
478
CHAPTER 9 Design of Experiments

the assumption that the effect of one factor is the same for both levels of the other factor. Changing
the fat content from medium to high, keeping carbohydrate at medium, and the carbohydrate content
from medium to high, keeping fat at medium, reduced the average time of exhaustion by approxi-
mately 10 s. The question then is, can we predict that increasing both fat and carbohydrate content
to high will lower the average time of exhaustion to approximately 67 s? To answer this question, we
need to administer high levels of both diets to a sample and observe the average time of exhaustion.
If it is 67 s, then our observation is correct. However, what if the observation is 74 s? The average
time of exhaustion has been lowered, but not as much. If this happens, we say that the two factors
interact. When factors interact, the effect of one factor on the response is not the same for different
levels of the other factor. Hence, the information obtained from the one-factor-at-a-time approach
would lead to an invalid prediction.
The
factor-level
combination
for
a
one-factor-at-a-time
approach
of
Example 9.3.1 can be seen from Figure 9.1.
If there is no interaction, we get Figure 9.2, which shows average time to exhaus-
tion with three given points and a possible point of around 67 s.
M
Fat
Carbohydrate
M
H
H
FIGURE 9.1
One-factor-at-a-time approach.
40
50
60
70
80
90
100
M
H
Fat = M
Fat = H
Ave. time to exhaustion
Carbohydrate
FIGURE 9.2
No interaction.
479
9.3 Factorial Design

Definition 9.3.1 Two factors I and II are said to interact if the difference in mean
responses for different levels of one factor is not constant across levels of the second
factor.
If there is interaction, the lines in Figure 9.2 might cross each other, in which case
a one-factor-at-a-time approach may not be the appropriate design. In that case, the
following alternative designs will give more accurate data.
9.3.2 FULL FACTORIAL DESIGN
One way to get around the problem of interaction in one-factor-at-a-time design is to
evaluate all possible combinations of factors in a single experiment. This is called a
full factorial experiment. The main benefit of a full factorial design is that every pos-
sible data point is collected. The choice of optimum condition becomes easy. For
example, in an experiment such as the one in Example 9.2.2, one could conduct a
full factorial design. The simplest form of factorial experiment involves two factors
only and is called a two-way layout. A full factorial experiment with n factors and
two levels for each factor is called a 2n factorial experiment. A full factorial exper-
iment is practical if only a few factors (say, fewer than five) are being investigated.
Beyond that, this design becomes time consuming and expensive.
9.3.3 FRACTIONAL FACTORIAL DESIGN
In a fractional factorial experiment, only a fraction of the possible treatments are
actually used in the experiment. A full factorial design is the ideal design, through
which we could obtain information on all main effects and interactions. But because
of the prohibitive size of the experiments, such designs are not practical to run. For
instance, consider Example 9.2.2. Now if we were to add say, two different densities,
three sizes of fish, and three types of food, the number of factors becomes five, and
total number of distinct treatments will be 44233¼288. This method
becomes very time consuming and expensive. The number of relatively significant
effects in a factorial design is relatively small. In these types of situations, fractional
factorial experiments are used in which trials are conducted on only a well-balanced
subset of the possible combinations of levels of factors. This allows the experimenter
to obtain information about all main effects and interactions while keeping the size of
the experiment manageable. The experiment is carried out in a single systematic
effort. However, care should be taken in selection of treatments in the experiment
so as to be able to answer as many relevant questions as possible. The fractional fac-
torial design is useful when the number of factors is large. Because we are reducing
the number of factors, a fractional factorial design will not be able to evaluate the
influence of some of the factors independently. Of course, the question is how to
choose the factors and levels we should use in a fractional factorial design. The ques-
tion of how fractional factorial designs are constructed is beyond the scope of
this book.
480
CHAPTER 9 Design of Experiments

EXERCISES 9.3
9.3.1. Suppose a large retail chain decides to introduce clothing in two types of
materials’ (ordinary, fine) qualities. Each store will have two different
proportions (40, 60%) displayed. At the end of the month, profits from each
store for these two types of clothing are recorded. Table 9.3.1 represents the
average profits for each of the quality-proportion combinations.
Discuss this as a one-factor-at-a-time experiment to predict the average
amount of profit.
9.3.2. Draw graphs for the data to represent quality-proportion combinations (a) for
the one-factor-at-a-time approach and (b) for the case where there is no
interaction.
9.3.3. Discuss how a fractional factorial design can be performed for the problem in
Exercise 9.3.1.
9.3.4. Suppose a researcher wants to conduct a series of experiments to study the
effect of fertilizer and temperature on plant growth. She uses four different
brands of fertilizers in three different settings for the rose plants of the same
age and of similar growth.
(a) How many factor-level combinations are possible in this experiment?
(b) Each experiment makes use of one fertilizer-temperature combination
(one-factor-at-a-time design). How should she implement randomization
in this experiment?
9.4 OPTIMAL DESIGN
In 1959, J. Kiefer presented a paper to the Royal Statistical Society about his work on
the theory of optimal design. He was trying to answer the major question, “How do
we find the best design?” This work initiated a whole new field of optimal design.
Optimal designs are a class of experimental design, which are optimal with respect to
certain statistical criterion. For instance, in estimation problems, these designs allow
parameters to be estimated without bias and minimal variance. The methods of opti-
mal experimental design provide the technical tools for building experimental
designs to attain well-defined objectives with efficiency and with minimum cost.
The cost can be the monetary cost, time, number of experimental runs, and so on.
Table 9.3.1 Two Types of Clothing and Profit
Average Profit
Quality
Proportion (%)
$10,000
Fine
40
$25,000
Ordinary
40
$9500
Ordinary
60
?
Fine
60
481
9.4 Optimal Design

There are many methods of achieving optimal designs such as sequential (simplex)
or simultaneous experiment designs. In sequential design, experiments are per-
formed in succession in a direction of improvement until the optimum is reached.
Simultaneous experiment designs such as response surface designs are used to build
empirical models. A survey by Atkinson in 1988 contains many references on opti-
mal design.
In this section, we focus only on one simple example to illustrate the ideas of
optimal design in terms of choosing appropriate sample size. It is not possible to have
a single design that is best for securing information concerning all types of popula-
tion parameters. Indeed, it is beyond the scope of this section to present a general
theory of optimal design.
9.4.1 CHOICE OF OPTIMAL SAMPLE SIZE
The sample size estimation is an essential part of experimental design; otherwise,
sample size may be very high or very low. If sample size is too low, the experiment
will lack the accuracy to provide dependable answers to the questions we are
investigating. If sample size is too large, time and resources will be wasted, often
for insignificant gain. We now illustrate a simple case of optimal sample size
determination.
Let X11, ...,X1n1 be a random sample from population 1 with mean m1 and var-
iance s1
2 and X21, ...,X2n2 be random samples from population 2 with mean m2
and variance s2
2. Assume that the two samples are independent. Then we know that
X1 X2 is an unbiased estimator of m1m2 with standard error
s2
X1X2
ð
Þ ¼ Var X1 X2


¼ s2
1
n1
+ s2
2
n2
:
Suppose that there is a restriction that the total observations should be n, that is,
n1+n2¼n. Such a restriction may be due to cost factors or to a shortage of available
subjects. An important design question is how to choose the sample sizes n1 and n2 so
as to maximize the information in the data relevant to the parameter m1m2. We
know that the samples contain maximum information when the standard error is min-
imum. Hence, the problem reduces to minimization of Var X1 X2


. Let a ¼ n1
n be
the fraction on n observations that is assigned to sample 1. Then n1¼na and n2¼n
(1a), and we have
Var X1 X2


¼ s2
1
n1
+ s2
2
n2
¼ s2
1
na +
s2
2
n 1a
ð
Þ
482
CHAPTER 9 Design of Experiments

The problem is now reduced to finding an a that minimizes the function
g a
ð Þ ¼ s2
1
na +
s2
2
n 1a
ð
Þ. This problem can be solved using calculus. By taking the
derivative with respect to a, d
dag a
ð Þ and equating it to zero, we have
 s2
1
na2 +
s2
2
n 1a
ð
Þ2 ¼ 0:
Multiplying throughout by na2(1a)2, we have
s2
1 1a
ð
Þ2 + s2
2a2 ¼ 0,
which results in the quadratic equation
s2
2 s2
1


a2 + 2s2
1as2
1 ¼ 0:
Using the quadratic formula, we obtain the two roots as
a1 ¼
s1
s1 + s2
and
a2 ¼
s1
s1 s2
:
However, a2 cannot be the solution because, if s1>s2, then a2>1, otherwise a2<0;
both are not admissible because a is a fraction. Hence,
a ¼
s1
s1 + s2
and 1a ¼
s2
s1 + s2
:
Using the second derivative test, we can verify that this indeed is a minimum for
Var X1 X2


. From this analysis we can see that the sample sizes that maximize
the information in the data relevant to the parameter m1m2 subject to the constraint
n1+n2¼n are
n1 ¼
s1
s1 + s2
n and n2 ¼
s2
s1 + s2
n:
As a special case, we can see that when s1
2¼s2
2, the optimal design is to take n1¼n2.
EXERCISES 9.4
9.4.1. A total of 100 sample points were taken from two populations with variances
s1
2¼4 and s2
2¼9. Find n1 and n2 that will result in the maximum amount
of information about (m1m2).
9.4.2. Suppose in Exercise 9.4.1, we want to take n¼n1¼n2. How large should
n be to obtain the same information as that implied by the solution of
Exercise 9.4.1?
483
9.4 Optimal Design

9.5 THE TAGUCHI METHODS
Taguchi methods were developed by Genichi Taguchi to improve the implementa-
tion of total quality control in Japan. These methods are claimed to have provided as
much as 80% of Japanese quality gains. They are based on the DOEs to provide near-
optimal quality characteristics for a specific objective. A special feature of Taguchi
methods is that they integrate the methods of statistical DOEs into a powerful engi-
neering process. The Taguchi methods are in general simpler to implement.
Taguchi methods are often applied on the Japanese manufacturing floor by tech-
nicians to improve their processes and their product. The goal is not just to optimize an
arbitrary objective function, but also to reduce the sensitivity of engineering designs to
uncontrollable factors or noise. The objective function used is the signal-to-noise ratio,
which is then maximized. This moves design targets toward the middle of the design
space so that external variation affects the behavior of the design as little as possible.
This permits large reductions in both part and assembly tolerances, which are major
drivers of manufacturing cost. Linking quality characteristics to cost through the
Taguchi loss function (Taguchi and Yokoyama, 1994) was a major advance in quality
engineering, as well as in the ability to design for cost. Taguchi methods are also called
robust design. In 1982, the American Supplier Institute introduced Dr Taguchi and his
methods to the US market.
Using a well-planned experimental design, such as a fractional factorial design, it
is possible to efficiently obtain information about the model and the underlying pro-
cess. Clearly, the purpose of these methods is to control and ensure the quality of the
end product. In the conventional approach, this is achieved by further testing a few
end products that are randomly chosen or using control charts and making decisions
based on certain preset criteria, such as acceptable or unacceptable. Thus, “quality”
of the product is thought of as inside or outside of specifications. Instead, Taguchi
suggested that we should specify a target value, and the quality should be thought of
as the variation from the target.
As an example, suppose we make n observations of the output x1,. . .,xn of a pro-
cess at times 1, 2, . . ., n, as shown in Figure 9.3.
. 
TU
x
. 
T = Target value
. 
TL
1
2
n
Time
FIGURE 9.3
Control plot of processing times and outputs.
484
CHAPTER 9 Design of Experiments

The control chart consists of a plot of observed output values (xi0s) on the y-axis
and the times of observation, 1, 2, . . ., n on the x-axis, as shown in the figure. The
letter T represents the target value. If the output value is between TL and TU, the pro-
cess is deemed to be operating satisfactorily; otherwise the process is said to be out of
control and the output value is considered unsatisfactory.
Some other examples are (1) defining specification limits for acceptance, such as
stating that the diameter of bolts must be between 9.8 and 10.2 mm with mean 10 mm
and (2) that the waiting time in a line should be less than 30 min for at least 90% of
customers.
In all these situations, the specifications partition the state of the process as
acceptable or unacceptable, that is, it classifies the state as a dichotomy. This is often
called the “goal post mentality.”
The basic idea of the Taguchi approach is a shift in mindset from demarking the
quality as acceptable or unacceptable to a more flexible and realistic classification.
The traditional approach to quality control does not take into account the size of
departure from the target value. To accommodate the size of such departure as a sig-
nificant factor in quality control, let us introduce the concept of loss function (see
Chapter 11). If an output value x differs from the target value T, let L(T, x) denote
the loss incurred, say in dollars. Other possible losses could also be reputation or cus-
tomer satisfaction.
For the control chart example, we can assign the loss function
L T, x
ð
Þ ¼
0, ifTU < x < TL
L, ifx > TL or x < TU

,
where L is a constant and x is the measured value. This is schematically shown in
Figure 9.4.
From Figure 9.4, it is seen that we view outputs x1 and x2 as having equal quality,
whereas x2 and x3 are considered to have vastly differing quality (x2 is acceptable and
x3 is not acceptable). A more reasonable conclusion would be that x1 has excellent
quality, whereas x2 and x3 are similar, both being poor.
In Taguchi’s approach, the loss function takes into account the size of departure
from the target value. For example, a popular choice for the loss function is
L T, X
ð
Þ ¼ k X T
ð
Þ2,
where L is the loss incurred, k is constant, X is actual value of the measured output,
and T is target value.
L
L
TL
T      x1
x2
TU
x3
FIGURE 9.4
Loss function.
485
9.5 The Taguchi Methods

We can schematically represent the behavior as shown by Figure 9.5.
This form of loss function is called the quadratic loss function. The choice of k
depends on the particular problem. For example, the scaling factor k can be used
to convert loss into monetary units to accommodate comparisons of systems with
different capital loss. Or, in product manufacturing, let D denote the allowed devi-
ation from the target, and let A denote the loss due to a defective product. Then a
choice of k can be k¼(A/D)2. As shown earlier, the average loss is E(L) and is
given by
E L
ð Þ ¼ k
E X
ð ÞT
ð
Þ2 + s2
h
i
¼ k
bias
ð
Þ2 + variance
h
i
,
where s2 is the variance of X (measured quality, which is assumed to be random). In
Taguchi, the variation from the target can be broken into components containing
bias and product variation. Thus, if our aim is to minimize the expected loss, E(L),
we should not only require E(X)¼m to be close to T but also should reduce the var-
iance. It turns out that often these requirements are contradictory. The objective is
to choose the design parameters (the factors that influence the quality) optimally to
obtain the best quality product. In practice, the parameters m and s2 are not known
and are being estimated by X and S2, respectively. This results in the Taguchi loss
function
L ¼ k
X T

2 + S2
h
i
:
This loss function penalizes small deviations from T only slightly, while assessing a
larger penalty for responses far from the target. The expected loss is similar to a mean
squared error loss, which we have seen in regression analysis in the form of least
squares.
Why is controlling both bias and variance important? Suppose you want your
community swimming pool temperature at 80 F, which is the T here. Suppose
the temperature varies between 60 and 100 F. Clearly the average (bias) is zero;
however, it will be pretty uncomfortable to swim at 60 F or 100 F. Here the bias
takes the ideal value of zero, but the variance is large. In another scenario, the
L(T , x)
L
TL
x
T 
TU
FIGURE 9.5
Quadratic loss function.
486
CHAPTER 9 Design of Experiments

variance may be small, but the average temperature may be farther away from the
target value of 80 F (e.g. the temperature is constant at 60 F). Hence, we want
the pool temperature to be near to the target value of 80 F, with as small variance
as possible (say, within 1-2 F).
Taguchi coined the term design parameters as the generic description for fac-
tors that may influence the quality and whose levels we want to optimize. Tagu-
chi’s philosophy is to “design quality in” rather than to weed out the defective
items after manufacturing. In order to obtain an optimal set of design parameters
that affect the quality of the end product, the Taguchi method utilizes appropriately
designed experiments. More specifically, orthogonal arrays are used for fractional
factorial designs. Orthogonal arrays provide a set of well-balanced experiments.
Taguchi provides tables for these designs so that even a nonspecialist can use them.
For two-level designs (high, low), we have a table for an L4 orthogonal array up to
three factors; a table for an L8 orthogonal array up to seven factors; and so forth.
Similar tables are available for three-level designs. We will not describe these
design issues in this section. We refer the reader to specialized books on the subject
for further details.
We can summarize the Taguchi approach to quality design as follows:
1. Taguchi’s methods for experimental design are ready made and simple to use in
the design of efficient experiments, even by nonexperts.
2. Taguchi’s approach to total quality management is holistic and tries to design
quality into a product rather than inspecting defects in the final product.
3. Taguchi’s techniques can readily be applied to other fields such as management
problems.
EXERCISES 9.5
9.5.1. Suppose the following data represent thickness between and within silicon
wafers (in microns), with a target value of 14.5 microns
13:688 13:788 14:173 14:557
13:925 14:545 13:797 14:778:
Compute the Taguchi loss function.
9.5.2 One of the commonly used performance measures in the Taguchi method is
log
mean
ð
Þ2
s2
 
!
,
where s2 is the sample variance. In general, the higher the performance mea-
sure, the better the design. This measure is called robustness statistics. For the
problem of Exercise 9.5.1, suppose that we run the experiment by controlling
various factors affecting the thickness. Table 9.5.1 shows the data obtained in
four different runs.
487
9.5 The Taguchi Methods

(a) Using the robustness statistics given earlier, which of the processes gives
us an improved performance?
(b) Another commonly used performance statistic is
log s2
 
:
Using this robustness statistic, which of the processes gives us an
improved performance? Compare this with the results of part (a).
9.6 CHAPTER SUMMARY
In this chapter, we have learned some basic aspects of experimental design. Some
fundamental definitions and tools for developing experimental designs such as ran-
domization, replication, and blocking were introduced in Section 9.2. Basic concepts
of factorial design were given in Section 9.3. In Section 9.4, we saw an example of
optimal design. The Taguchi method was introduced in Section 9.5. In the next chap-
ter, we introduce the analysis component. We have discussed only a very small col-
lection of experimental designs in this chapter. There exist a wide variety of
experimental designs to deal with a large number of treatments and to suit specific
needs of research experiments in diverse fields. It is an exciting and growing area for
the interested student to apply and explore.
We list some of the key definitions introduced in this chapter:
•
Response variable (output variable).
•
Independent variables (treatment variables or input variables or factors).
•
Nuisance variables.
•
Noise.
•
Observational.
•
Experimental units.
•
Single-factor experiments
•
Multifactor experiments.
•
Experimental error.
•
Blinding, double-blinding, and placebo.
•
Replication.
•
Block.
Table 9.5.1
Run 1:
14.158
14.754
14.412
14.065
13.802
14.424
14.898
14.187
Run 2:
13.676
14.177
14.201
14.557
13.827
14.514
13.897
14.278
Run 3:
13.868
13.898
14.773
13.597
13.628
14.655
14.597
14.978
Run 4:
13.668
13.788
14.173
14.557
13.925
14.545
13.797
14.778
488
CHAPTER 9 Design of Experiments

•
Randomization.
•
Completely randomized design
•
Randomized complete block design.
•
kk Latin square design.
•
Greco-Latin square.
•
Design parameters.
In this chapter, we have also learned the following important concepts and
procedures.
•
Procedure for random assignment.
•
Procedure for randomization in a randomized complete block design.
•
Procedure for a randomized complete block design with r replications.
•
Procedure for constructing a 44 Latin square.
•
One-factor-at-a-time design.
•
Full factorial design.
•
Fractional factorial design.
•
Choice of optimal sample size.
•
The Taguchi methods.
9.7 COMPUTER EXAMPLES
In this chapter, we present R, Minitab and SAS commands only. SPSS commands
can be performed similarly to Minitab.
9.7.1 EXAMPLES USING R
EXAMPLE 9.7.1
Permutation
Obtain a random perturbation of the numbers from 1 to n. Where n¼10.
R Code:
sample(c(1:10));
Frequently used in R, c(1:10) is similar
to c(1,2,3,4,5,6,7,8,9,10). Anywhere
you want a range of values use 1:n.
Output:
This output will be a random sample without replacement, your output will look similar.
6 7 9 2 10 1 5 3 8 4
489
9.7 Computer Examples

EXAMPLE 9.7.2
Randomized Block Design
In order to study the number of hours to relief provided by five different brands (A, B, C, D, and
E) of pain relievers for pain resulting from different causes [headache (H), muscle pain (M), pain due
to cuts and bruises (CB)], doses are administered to five subjects each having similar types of pain.
Create a randomized complete block design. Choose the different types of pain (H, M, and CB) as
the blocks.
R Code:
h¼sample(c(1:5));
m¼sample(c(1:5));
cb¼sample(c(1:5));
table¼cbind(h,m,cb);
table¼as.data.frame(table);
colnames(table)¼c(“H”,“M”,“CB”);
rownames(table)¼c(“A”,“B”,“C”,“D”,“E”);
print(table);
Output:
This output will be a random sample without replacement, your output will look similar.
H, M, CB
A 4 2 1
B 2 1 4
C 3 4 5
D 5 5 3
E 1 3 2
EXAMPLE 9.7.3
Latin Squares
A gasoline company is interested in comparing the effect of four gasoline additives (A, B, C,
and D) on the gas
mileage achieved per gallon. Four cars (1, 2, 3, and 4) and four drivers (I, II, III, and IV) will
be used in the experiment. Create a Latin square design.
R Code:
gasadd¼c(“A”,“B”,“C”,“D”);
table¼c();
for(i in 1:4) {
table¼cbind(table,c(gasadd[i:4],gasadd[0:(i1)]));
} table¼as.data.frame(table);
colnames(table)¼c(1:4);
rownames(table)¼c(“I”,“II”,“III”,“IV”);
print(table[,sample(c(1:4))]);
Random samples from 5 subjects 
Without replacement are generated for
each type of pain.
For the sake of formatting we create 
a matrix using cbind() to bind columns.
Notice we added a column of row titles.
490
CHAPTER 9 Design of Experiments

Output:
This output will be a random sample without replacement, your output will look similar.
4 1 3 2
I D A C B
II A B D C
III B C A D
IV C D B A
9.7.2 MINITAB EXAMPLES
EXAMPLE 9.7.4
Obtain a random permutation of numbers 1 to n.
Solution
Enter in C1 the numbers 1 to n, say n¼10. Then
calc>random data>samples from column. . . >
enter sample 10>rows from column(s) C1>Store samples in: C2>OK
The result is a random permutation of numbers 1 to n(¼10). Now if we need to generate blocks
of random permutations of numbers from 1 to n(¼10), in the foregoing steps, just store samples in
C3, C4, . . .
9.7.3 SAS EXAMPLES
EXAMPLE 9.7.5
For the data of Example 9.2.4, conduct a randomized complete block design using SAS.
Solution
We represent blocks that are reasons for pain by H¼1, M¼2, and CB¼3. Similarly five brands
which are treatments by A¼1, B¼2, C¼3, D¼4, and E¼5. Then we can use the following code
to generate a randomized complete block design.
options nodate nonumber;
data a;
do block ¼ 1 to 3;
do subject ¼ 1 to 5;
x ¼ ranuni(0);
output;
end;
end;
proc sort; by block x;
data c; set a;
trt ¼ 1 + mod(N  1, 5); /* mod ¼ remainder of N/5 */
proc sort; by block subject;
proc print;
var block subject trt;
run;
491
9.7 Computer Examples

We get the following output.
Completely randomized 23 design, 4 subjects per cell
Obs
Block
Subject
trt
1
1
1
5
2
1
2
4
3
1
3
3
4
1
4
2
5
1
5
1
6
2
1
2
7
2
2
5
8
2
3
3
9
2
4
4
10
2
5
1
11
3
1
4
12
3
2
5
13
3
3
1
14
3
4
2
15
3
5
3
Note that the numbers in the column corresponding to a block identify the type of
pain, the numbers in the subject column correspond to the subjects, and the numbers
in the column corresponding to trt identify the brands. Using the corresponding let-
ters, we can rewrite the foregoing table in the familiar form shown in Table 9.14.
The PLAN procedure constructs experimental designs. The PLAN procedure
does not have a DATA¼option in the PROC statement; in this procedure, both
the input and output data sets are specified in the OUTPUT statement. We will
use this to construct a Latin square design.
EXAMPLE 9.7.6
A gasoline company is interested in comparing the effect of four gasoline additives (A, B, C, and D)
on the gas mileage achieved per gallon. Four cars (1, 2, 3, and 4) and four drivers (I, II, III, and IV)
will be used in the experiment. Create a Latin square design.
Solution
We can use the following program, where we represent the additives by 1¼A, 2¼B, 3¼C, and 4¼D.
Table 9.14
H
M
CB
1(E)
1(B)
1(D)
2(D)
2(E)
2(E)
3(C)
3(C)
3(A)
4(B)
4(D)
4(B)
5(A)
4(A)
5(C)
492
CHAPTER 9 Design of Experiments

Options nodate nonumber;
title ’Latin Square design for 4 additives’;
proc plan seed¼37432;
factors rows¼4 ordered cols¼4 ordered/NOPRINT;
treatments tmts¼4 cyclic;
output out¼g
rows cvals¼(’car 1’ ’car 2’ ’car 3’ ’car 4’)
random
cols cvals¼(’Driver 1’ ’Driver 2’ ’Driver 3’
’Driver 4’) random
tmts nvals¼(1 2 3 4) random;
run;
proc tabulate;
class rows cols;
var tmts;
table rows, cols*(tmts*f¼1.);
keylabel sum¼’ ’;
run;
PROJECTS FOR CHAPTER 9
9A. SAMPLE SIZE AND POWER
Suppose that the experimenter is interested in comparing the true means of two inde-
pendent populations. If two similar treatments are to be compared, the assumption of
equality of variances is not unreasonable. Hence, assume that the common variance
of the two populations is s2, and the experimenter has a prior estimate of the vari-
ance. We have learned in Section 9.4 that in this case, the optimal design will be to
take sample sizes n1 and n2 to be equal. Let n¼n1¼n2 be the size of the random
sample that the experimenter should take from each population.
Now, suppose that the experimenter has decided to use the one-sided large sam-
ple test, H0: m1¼m2 versus Ha: m1>m2 with a fixed a¼P(Type I error). He wants to
choose n to be so large that if m1¼m2+ks, he will get a fixed power (1–b) of deciding
m1>m2. Recall that power of a test is the probability of (correctly) rejecting H0 when
H0 is false. Find the approximate value of n. Note that, for a given a, this will be an
optimal sample size with a desired value of the power.
In particular, what should be the sample size in the hypothesis testing problem,
H0: m1–m2¼0 vs. Ha: m1–m2¼3, if a¼b¼0.05. Assume that s¼7.
9B. EFFECT OF TEMPERATURE ON SPOILAGE OF MILK
Suppose you have observed that milk in your refrigerator spoils very fast. You may
be wondering whether it has anything to do with the temperature settings. Design an
experiment to study the effect of temperature on spoiled milk, with at least three
493
Projects for Chapter 9

meaningful settings of the temperature. (i) Write a possible hypothesis for your
experiment. (ii) What are the independent and dependent variables? (iii) Which vari-
ables are being controlled in this experiment? (iv) Discuss how you used the three
basic principles of replication, blocking, and randomization. (v) What conclusions
can you make? Think through any possible flaws in the design that may affect the
integrity of your findings.
494
CHAPTER 9 Design of Experiments

CHAPTER
Analysis of Variance 10
CHAPTER CONTENTS
10.1 Introduction .................................................................................................. 496
10.2 ANOVA Method for Two Treatments (Optional) ................................................. 498
10.3 ANOVA for Completely Randomized Design ...................................................... 505
10.4 Two-Way ANOVA, Randomized Complete Block Design .................................... 521
10.5 Multiple Comparisons .................................................................................... 528
10.6 Chapter Summary .......................................................................................... 535
10.7 Computer Examples ....................................................................................... 535
Projects for Chapter 10 .......................................................................................... 545
OBJECTIVE
To analyze the means of several populations by identifying sources of variability of
the data.
John Wilder Tukey
(Source: http://en.wikipedia.org/wiki/John_Tukey)
John W. Tukey (1915-2000), a chemist-turned-topologist-turned statistician, was
one of the most influential statisticians of the past 50 years. He is credited with
inventing the word software. He worked as a professor at Princeton University
Mathematical Statistics with Applications in R
Copyright © 2015 Elsevier Inc. All rights reserved.
495

and a senior researcher at AT&T’s Bell Laboratories. He made significant contribu-
tions to the fields of exploratory data analysis and robust estimation. His works on the
spectrum analysis of time series and other aspects of digital signal processing have
been widely used in engineering and science. He coined the word bit, which refers to
a unit of information processed by a computer. In collaboration with Cooley, in 1965,
Tukey introduced the fast Fourier transform (FFT) algorithm that greatly simplified
computation for Fourier series and integrals. Tukey authored or coauthored many
books in statistics and wrote more than 500 technical papers. Among Tukey’s most
far-reaching contributions was his development of techniques for “robust analysis,”
an approach to statistics that guards against wrong answers in situations where a ran-
domly chosen sample of data happens to poorly represent the rest of the data set.
Tukey also made significant contributions to the ANOVA.
10.1 INTRODUCTION
Suppose that we are interested in the effect of four different types of chemical fer-
tilizers on the yield of rice, measured in pounds per acre. If there is no difference
between the different types of fertilizers, then we would expect all the mean yields
to be approximately equal. Otherwise, we would expect the mean yields to differ.
The different types of fertilizers are called treatments and their effects are the treat-
ment effects. The yield is called the response. Typically, we have a model with a
response variable that is possibly affected by one or more treatments. The study
of these types of models falls under the purview of design of experiments, which
we discussed in Chapter 9. In this chapter, we concentrate on the analysis aspect
of the data obtained from the designed experiments. If the data came from one or
two populations, we could use the techniques learned in Chapters 6 and 7. Here,
we introduce some tests that are used to analyze the data from more than two popu-
lations. These tests are used to deal with treatment effects, including tests that take
into account other factors that may affect the response. The hypothesis that the pop-
ulation means are equal is considered equivalent to the hypothesis that there is no
difference in treatment effects. The analytical method we will use in such problems
is called the analysis of variance (ANOVA). Initial development of this method
could be credited to Sir Ronald A. Fisher who introduced this technique for the anal-
ysis of agricultural field experiments. The “green revolution” in agriculture would
have been impossible without the development of theory of experimental design
and the methods of ANOVA.
ANOVA is one of the most flexible and practical techniques for comparing sev-
eral means. It is important to observe that ANOVA is not about analyzing the pop-
ulation variance. In fact, we are analyzing treatment means by identifying sources of
variability of the data. In its simplest form, ANOVA can be considered as an exten-
sion of the test of hypothesis for the equality of two means that we learned in
Chapter 7. Actually, the so-called one-way ANOVA is a generalization of the
496
CHAPTER 10 Analysis of Variance

two-means procedure to a test of equality of the means of more than two independent,
normally distributed populations.
Recall that the methods of testing H0:m1  m2 ¼ 0, such as the t-test, were dis-
cussed earlier. In this chapter, we are concerned with studying situations involving
the comparison of more than two population or treatment means. For example, we
may be interested in the question “Do the rates of heart attack and stroke differ for
three different groups of people with high cholesterol levels (borderline high such as
150-199 mg/dL, high such as 200-239 mg/dL, very high such as greater than 240 mg/
dL) and a control group given different dosage levels of a particular cholesterol-
lowering drug (say, a particular statin drug)?” Let us consider four populations with
means m1, m2, m3, and m4, and say that we wish to test the hypotheses m1 ¼ m2 ¼ m3 ¼
m4. That is, the mean rate is the same for all the four groups. The question here is: why
do we need a new method to test for differences among the four procedure population
means? Why not use z- or t-tests for all possible pairs and test for differences in each
pair? If any one of these tests leads to the rejection of the hypothesis of equal means,
then we might conclude that at least two of the four population means differ. The
problem with this approach is that our final decision is based on results of
4
2
 
¼6 different tests, and any one of them can be wrong. For each of the six tests,
let a¼0.10 be the probability of being wrong (type I error). Then the probability that
at least one of the six tests leads to the conclusion that there is a difference leads to an
error 1  (0.9)6 ¼ 0.46856, which clearly is much larger than 0.10, thus resulting in a
large increase in the type I error rate. Hence, if an ordinary t-test is used to make
several treatment comparisons from the same data, the actual a-value applying to
the tests taken as a group will be larger than the specified value of a, and one is likely
to declare significance when there is none.
ANOVA procedures were developed to eliminate the increase in error rates
resulting from multiple t-tests. With ANOVA, we are able to set one alpha level
and test whether any of the group means differ from one another. Given a sample
from each of the populations, our interest is to answer the question: are the observed
discrepancies among the different sample means (SMs) merely due to chance fluc-
tuations, or are they due to inherent differences among the populations? ANOVA
separates the effect of purely random variations from those caused by existing dif-
ferences among population means: the phrase “analysis of variance” springs from the
idea of analyzing variability in the data to see how much can be attributed to differ-
ences in m and how much is due to variability in the individual populations. The
ANOVA method incorporates information on variability from all of the samples
simultaneously. At the heart of ANOVA is the fact that variances can be partitioned,
with each partition attributable to a specific source. The method inspects various
sums of squares (which are measures of variation in a sample) calculated from
the data. ANOVA looks at two types of sums of squares: sums of squares within
groups and sums of squares between groups. That is, it looks at each of the distribu-
tions and compares the between-group differences (variation in group means) with
the within-group differences (variation in individuals’ scores within groups).
497
10.1 Introduction

10.2 ANOVA METHOD FOR TWO TREATMENTS (OPTIONAL)
In this section, we present the simplest form of the ANOVA procedure, the case of
studying the means of two populations I and II. For comparing only two means, the
ANOVA will result in the same conclusions as the t-test for independent random
samples. The basic purpose of this section is to introduce the concept of ANOVA
in simpler terms. Let us consider two random samples of size n1 and n2, respectively.
That is, y11, y12, : : : , y1n1 from population I and y21, y22, : : : , y2n2 from popula-
tion II. Let
y1 ¼ y11 + y12 +  + y1n1
n1
sample mean from population I
ð
Þ
and
y2 ¼ y21 + y22 +  + y2n2
n2
sample mean from populationII
ð
Þ:
These samples are assumed to be independent and come from normal populations
with respective means m1, m2, and variances s1
2¼s2
2. We wish to test the hypothesis
H0 : m1 ¼ m2 versus Ha : m1 6¼ m2:
The total variation of the two combined response measurements about y (the SM of
all n ¼ n1 + n2 observations) is (SS is used for sum of squares) defined by
Total SS ¼
X
2
i¼1
X
ni
j¼1
yij y

2
:
(10.1)
That is,
y ¼ y11 + y12 +  + y1n1 + y21 + y22 +  + y2n2
n
¼ 1
n
X
ij
yij:
The total sums of squares measures the total spread of scores around the grand mean
(GM), y. We can rewrite Equation (10.1) as
Total SS ¼
X
2
i¼1
X
ni
j¼1
yij y

2
¼
X
n1
j¼1
y1j y

2
+
X
n2
j¼1
y2j y

2
¼
X
n1
j¼1
y1j y1 + y1 y

2
+
X
n2
j¼1
y2j y2 + y2 y

2
¼
X
n1
j¼1
y1j y1

2
+ n1 y1 y
ð
Þ2 + 2 y1 y
ð
Þ
X
n1
j¼1
y1j y1


+
X
n2
j¼1
y2j y2

2
+ n2 y2 y
ð
Þ2 + 2 y2 y
ð
Þ
X
n2
j¼1
y2j y2


:
Note that
Xn1
j¼1 y1j y1


¼ 0 ¼
Xn2
j¼1 y2j y2


: We obtain
498
CHAPTER 10 Analysis of Variance

Total SS ¼
X
n1
j¼1
y1j y1

2
+
X
n2
j¼1
y2j y2

2
+ n1 y1 y
ð
Þ2 + n2 y2 y
ð
Þ2
¼
X
2
i¼1
X
ni
j¼1
yij yi

2
+
X
2
i¼1
ni yi y
ð
Þ2:
(10.2)
Define SST, the sum of squares for treatment by
SST ¼
X
2
i¼1
ni yi y
ð
Þ2:
The SST measures the total spread of the group means yi with respect to the GM, y.
Also, SSE represents the sum of squares of errors given by
SSE ¼
X
2
i¼1
X
ni
j¼1
yij yi

2
¼
X
n1
j¼1
y1j y1

2
+
X
n2
j¼1
y2j y2

2
¼ n1 1
ð
Þs2
1 + n2 1
ð
Þs2
2
where s1
2 and s2
2 are the unbiased sample variances of the two random samples. Note
that this connects the sum of squares to the concept of variance we have been using in
previous chapters. We can now rewrite Equation (10.2) as
Total SS ¼ SSE + SST:
It should be clear that the SSE measures the within-sample variation of the y-values
(effects), whereas SST measures the variation among the two SMs. The logic by
which the ANOVA tests is as follows: if the null hypothesis is true, then SST as com-
pared to SSE should be about the same, or less. The larger SST, the greater will be the
weight of evidence to indicate a difference in the means m1 and m2. The question then
is, how large?
To answer this question, let us suppose we have two populations that are normal.
That is, let Yij be N (mi, s2) distributed with values yij. Then the pooled unbiased esti-
mate of s2 is given by
s2
p ¼ n1 1
ð
Þs2
1 + n2 1
ð
Þs2
2
n1 + n2 2
¼
SSE
n1 + n2 2:
Hence,
s2 ¼ E S2
p


¼ E
SSE
n1 + n2 2


:
Also, we can write
SSE
s2 ¼
X
n1
j¼1
Y1j Y1

2
s2
+
X
n2
j¼1
Y2j Y2

2
s2
499
10.2 ANOVA Method for Two Treatments (Optional)

which has a w2-distribution with (n1 + n2  2) degrees of freedom.
Under the hypothesis that m1¼m2, E(SST)¼s2. Furthermore,
Z ¼
Y1 Y2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
s2
1
n1 + 1
n2


r
 N 0, 1
ð
Þ:
This implies that
Z2 ¼
1
n1
+ 1
n2

 Y1 Y2
s2
	

¼ SST
s2
has a w2-distribution with 1 degree of freedom. It can be shown that SST and SSE are
independent. From Chapter 4, we restate the following result.
Theorem 10.2.1 If w1
2 has u1 degrees of freedom w2
2 has u2 degrees of freedom, and
w1
2 and w2
2 are independent, then F ¼ w2
1=u1
w2
2=u2
has an F-distribution with u1 numerator
degrees of freedom and u2 denominator degrees of freedom.
Using the foregoing result, we have
SST= 1
ð Þs2
SSE= n1 + n2 2
ð
Þs2 ¼
SST=1
SSE= n1 + n2 2
ð
Þ
which has an F-distribution with u1¼1 numerator degrees of freedom and u2 ¼
(n1 + n2  2) denominator degrees of freedom.
Now, we introduce the mean square error (MSE), defined by
MSE ¼
SSE
n1 + n2 2
ð
Þ
¼ n1 1
ð
Þs2
1 + n2 1
ð
Þs2
2
n1 + n2 2
ð
Þ
and the mean square treatment (MST) given by
MST ¼ SST
1
¼ n1 y1 y
ð
Þ2 + n2 y2 y
ð
Þ2
h
i
:
Under the null hypothesis, H0: m1¼m2, both MST and MSE estimate s2 without bias.
When H0 is false and m16¼m2, MST estimates something larger than s2 and will be
larger than MSE. That is, if H0 is false, then E(MST)>E(MSE) and the greater the
differences among the values of m, the larger E(MST) will be relative to E(MSE).
Hence, to test H0:m1¼m2 versus Ha:m16¼m2, we use the F-test given by
F ¼ MST
MSE
as the test statistic. Thus, for given a, the rejection region is {F>Fa}. It is important
to observe that compared to the small sample t-test, here we work with variability.
Now we summarize the ANOVA procedure for the two-sample case.
500
CHAPTER 10 Analysis of Variance

ANOVA PROCEDURE FOR TWO TREATMENTS
For equal sample sizes n¼n1¼n2, assume s1
2¼s2
2.
We test
H0 : m1 ¼ m2 versus Ha : m1 6¼ m2:
1. Calculate: y1,y2,
X
ijy2
ij,
X
ijyij and find
SST ¼
X
2
i¼1
ni yi y
ð
Þ2:
Also calculate
Total SS ¼
X
i
X
j
y2
ij 
X
i
X
jyij

2
n1 + n2
:
Then
SSE ¼ Total SSSST:
2. Compute
MST ¼ SST
1
MSE ¼
SSE
n1 + n2 2:
3. Compute the test statistic,
F ¼ MST
MSE:
4. For a given a, find the rejection region as
RR : F > Fa,
based on 1 numerator and (n1+n22) denominator degrees of freedom.
5. Conclusion: If the test statistic F falls in the rejection region, conclude that the sample evidence
supports the alternative hypothesis that the means are indeed different for the two treatments.
Assumptions: Populations are normal with equal but unknown variances.
EXAMPLE 10.2.1
The following data represent a random sample of end-of-year bonuses for lower-level managerial
personnel employed by a large firm. Bonuses are expressed in percentage of yearly salary.
Female
6.2
9.2
8.0
7.7
8.4
9.1
7.4
6.7
Male
8.9
10.0
9.4
8.8
12.0
9.9
11.7
9.8
The objective is to determine whether the male and female bonuses are the same. We can answer
this question by connecting the following.
Continued
501
10.2 ANOVA Method for Two Treatments (Optional)

(a) Use the ANOVA approach to test the appropriate hypotheses. Use a¼0.05.
(b) What assumptions are necessary for the test in part (a)?
(c) Test the appropriate hypothesis by using the two-sample t-test for comparing population means.
Compare the value of the t-statistic to the value of the F-statistic calculated in part (a).
Solution
(a) We need to test
H0 : m1 ¼ m2 versus Ha : m1 6¼ m2
From the random sample, we obtain the following needed estimates, n1¼n2¼8:
y1 ¼ 7:8375, y2 ¼ 10:0625,
X
ij
y2
ij ¼ 1319:34,
X
ij
yij ¼ 143:20, y ¼ 8:95
SST ¼
X
2
i¼1
ni yi y
ð
Þ2 ¼ 19:8025:
Therefore,
Total SS ¼
X
i
X
j
y2
ij 
X
i
X
jyij

2
n1 + n2
ð
Þ
¼ 1391:34 143:2
ð
Þ2
16
¼ 109:70:
Then
SSE ¼ TotalSSSST
¼ 109:719:8025
¼ 89:8975,
MST ¼ SST
1 ¼ 19:8025
and
MSE ¼ SSE
2n1 2
¼ 89:8975
14
¼ 6:42125:
Hence, the test statistic
F ¼ MST
MSE
¼ 19:8025
6:42125
¼ 3:0839:
For a¼0.05, F0.05,1,14¼4.60. Hence the rejection region is {F>4.60}. Because 3.0839 is not
greater than 4.60, H0 is not rejected. There is not enough evidence to indicate that the average
bonuses are different for men and women at a¼0.05.
(b) To solve the problem, we assumed that the samples are random and independent with n1 ¼ n2 ¼ 8,
drawn from two normal populations with means m1 and m2 and common variance s2.
502
CHAPTER 10 Analysis of Variance

(c) The value of MSE is the same as s2¼sp
2¼6.42125. Also, y1 ¼ 7:8375 and y2 ¼ 10:0625. Then,
the t-statistic is
t ¼
y1 y2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
s2
1
n1
+ 1
n2


s
¼ 7:837510:0625
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
6:42125 1
8 + 1
8


s
¼ 1:756:
Now, t0.025, 14¼2.145 and the rejection region is {t<2.145}.
Because 1.756 is not less than 2.145, H0 is not rejected, which implies that there is no significant
difference between the bonuses for the males and the females.
Note also that t2¼F, that is, (1.756)2¼3.083 implying that in the two-sample case, the t-test and
F-test lead to the same result.
It is not surprising that in the previous example, the conclusions reached using
ANOVA and two-sample t-tests are the same. In fact, it can be shown that for
two sets of independent and normally distributed random variables, the two proce-
dures are entirely equivalent for a two-sided hypothesis. However, a t-test can also be
applied to a one-sided hypothesis, whereas ANOVA cannot. The purpose of this sec-
tion is only to illustrate the computations involved in the ANOVA procedures as
opposed to simple t-tests. The ANOVA procedure is effectively used for three or
more populations, which is described in the next section.
EXERCISES 10.2
10.2.1. The following information was obtained from two independent samples
selected from two normally distributed populations with unknown but equal
standard deviations. Do the data present sufficient evidence to indicate that
there is a difference in the mean for the two populations?
Sample 1
1
2
3
3
1
2
1
3
1
Sample 2
2
5
2
4
3
1
2
3
3
(a) Use the ANOVA approach to test the appropriate hypotheses. Use
a¼0.05.
(b) Test the appropriate hypothesis by using the two-sample t-test for
comparing population means. Compare the value of the t-statistic to the
value of the F-statistic calculated in part (a).
10.2.2. The following information was obtained from two independent samples
selected from two normally distributed populations with unknown but equal
standard deviations. Do the data present sufficient evidence to indicate that
there is a difference in the mean for the two populations?
Sample 1:
15
13
11
14
10
12
7
12
11
14
15
Sample 2:
18
16
13
21
16
19
15
18
19
20
21
14
503
10.2 ANOVA Method for Two Treatments (Optional)

(a) Use the ANOVA approach to test the appropriate hypotheses. Use
a¼0.01.
(b) Test the appropriate hypothesis by using the two-sample t-test for
comparing population means. Compare the value of the t-statistic to the
value of the F-statistic calculated in part (a).
10.2.3. A company claims that its medicine, brand A, provides faster relief from
pain than another company’s medicine, brand B. A random sample from
each brand gave the following times (in minutes) for relief. Do the data
present sufficient evidence to indicate that there is a difference in the mean
time to relief for the two populations?
Brand A:
47
51
45
53
41
55
50
46
45
51
53
50
48
Brand B:
44
48
42
45
44
42
49
46
45
48
39
49
(a) Use the ANOVA approach to test the appropriate hypotheses. Use
a¼0.01.
(b) What assumptions are necessary for the conclusion in part (a)?
(c) Test the appropriate hypothesis by using the two-sample t-test for
comparing population means. Compare the value of the t-statistic to the
value of the F-statistic calculated in part (a).
10.2.4. Table 10.2.1 gives mean SAT scores for math by state for 1989 and 1999 for
20 randomly selected states (source: The World Almanac and Book of Facts
2000).
Table 10.2.1
State
1989
1999
Arizona
523
525
Connecticut
498
509
Alabama
539
555
Indiana
487
498
Kansas
561
576
Oregon
509
525
Nebraska
560
571
New York
496
502
Virginia
507
499
Washington
515
526
Illinois
539
585
North Carolina
469
493
Georgia
475
482
Nevada
512
517
Ohio
520
568
New Hampshire
510
518
504
CHAPTER 10 Analysis of Variance

Using the ANOVA procedure, test that the mean SAT score for math in
1999 is greater than that in 1989 at a¼0.05. Assume that the variances are
equal and the samples come from a normal distribution.
10.2.5. Let X1, ...,Xn1 and Y1, ...,Yn2 be two sets of independent, normally
distributed random variables with means m1 and m2, and the common
variance s2. Show that the two-sample t-test and the ANOVA are
equivalent for testing H0:m1 ¼ m2 versus Ha:m1 > m2.
10.3 ANOVA FOR COMPLETELY RANDOMIZED DESIGN
In this section, we study the hypothesis testing problem of comparing population
means for more than two independent populations, where the data are about several
independent groups (different treatments being applied, or different populations
being sampled). We have seen in Chapter 9 that the random selection of independent
samples from k populations is known as a completely randomized experimental
design or one-way classification.
Let m1,. . .,mk be the means of k normal populations with unknown but equal
variance s2. The question is whether the means of these groups are different or
are all equal. The idea is to consider the overall variability in the data. We partition
the variability into two parts: (1) between-groups variability and (2) within-groups
variability. If between groups is much larger than that within groups, this will indi-
cate that differences between the groups are real, not merely due to the random
nature of sampling. Let independent samples be drawn of sizes ni, i¼1, 2,. . .,k
and let N ¼ n1 +   + nk. Let yij be the measured response on the jth experimental
unit in the ith sample. That is, Yij is the jth observation from population
i, i ¼ 1,2,. . .,k,
and
j ¼ 1,2,. . .,ni. Let y be the overall mean of all observa-
tions. The problem can be formulated as a hypothesis testing problem, where
we need to test
H0 : m1 ¼ m2 ¼  ¼ mk versus Ha : Not all the mis are equal:
The method of ANOVA tests the null hypothesis H0 by comparing two unbiased esti-
mates of the variance, s2, an estimate based on variations from sample to sample and
the other one based on variations within the samples. We will be rejecting H0 if the
first estimate is significantly larger than the second, so that the samples cannot be
assumed to come from the same population.
We can write the total sum of squares of deviations of the response measurements
about their overall mean for the k samples into two parts, from the treatment (SST)
and from the error (SSE). This partition gives the fundamental relationship in
ANOVA, where total variation is divided into two portions: between-sample varia-
tion and within-sample variation. That is,
Total SS ¼ SST + SSE:
505
10.3 ANOVA for Completely Randomized Design

The following derivations will make computation of these quantities simpler. The
total SS can be written as
Total SS ¼
X
k
i¼1
X
ni
j¼1
yij y

2
¼
X
k
i¼1
X
ni
j¼1
y2
ij 2y
X
k
i¼1
X
ni
j¼1
yij + Ny2:
Note that y ¼
Xk
i¼1
Xni
j¼1yij
N
, and then we have
Total SS ¼
X
k
i¼1
X
ni
j¼1
y2
ij CM
where CM is the correction factor for the correction for the means and is given by
CM ¼
Xk
i¼1
Xni
j¼1yij

2
N
¼ Ny2:
Let
Ti ¼
X
ni
j¼1
yij, be the sum of all the observations in the ith sample
and
Ti ¼
Xni
j¼1yij
ni
, the mean of the observations in the ith sample:
We can rewrite y as
y ¼
Xk
i¼1
Xni
j¼1yij
N
¼
Xk
i¼1niTi
N
Now, we introduce SST, the sum of squares for treatment (sometimes known as
between group sum of squares, SSB) by
SST ¼
X
k
i¼1
ni Ti y

2:
We note that Ti


is the mean response due to its ith treatment and y is the overall
mean. A large value of Ti y


is likely to be caused by the ith treatment effect being
much different from the rest. Hence SST can be used to measure the differences in
the treatment effects.
Thus, the sum of squares of errors (SSE) is
SSE ¼ Total SSSST:
We must state that the SSE is the sum of squares within groups (thus, sometimes SSE
is referred to as within group sum of squares, SSW) and this can be seen from rewrit-
ing the expression as
506
CHAPTER 10 Analysis of Variance

SSE ¼
X
k
i¼1
X
ni
j¼1
yij Ti


2:
The decomposition of total sum of squares can be easily seen in Figure 10.1.
Figure 10.2 represents one point for each observation against each sample, with
SM representing the sample means and GM representing the grand mean. The dotted
line between SMs and GM is the distance between them. Taking this distance,
SST (or between
group sum of squares)
SSE (or within group sum
of squares)
Total sum of
squares
(yij – Ti)2
i=1
k
ni
j=1
ΣΣ
−
=
Ti  – y  2
i =1
k
ni
Σ
−
−
⎛
⎝
⎛
⎝
=
FIGURE 10.1
Decomposition of total SS.
I
II
III
Sample
SM
SM
–
o
o
o
GM
SM
Observations
FIGURE 10.2
ANOVA decomposition.
507
10.3 ANOVA for Completely Randomized Design

squaring, multiplying by the corresponding sample sizes, and summing, we get SST.
To obtain SSE, we take the distance from each group mean, SM, to each member of
the group, square them, and add them. In addition, to give an idea of within-group
variations, it is customary to draw side-by-side box plots.
As mentioned earlier, SST estimates the variation among the mi’s, and hence if all
the mi’s were equal, the Ti s would be similar and the SST would be small. It can be
verified that the unbiased estimator of s2 based on (n1 + n2 ++ nk–k) degrees of
freedom is
S2 ¼ MSE
¼
SSE
n1 + n2 +  + nk k
ð
Þ
¼ SSE
N k
:
Note that the quantity MSE is a measure of variability within the groups. If there
were only one group with n observations, then the MSE is nothing but the sample
variance, s2. The fact that ANOVA deals simultaneously with all the k groups can
be seen by rewriting MSE in the following form:
MSE ¼ n1 1
ð
Þs2
1 + n2 1
ð
Þs2
2 +  + nk 1
ð
Þs2
k
n1 1
ð
Þ + n2 1
ð
Þ +  + nk 1
ð
Þ
:
The mean square for treatments with (k1) degrees of freedom is
MST ¼ SST
k 1:
The MST is a measure of the variability between the SMs of the groups. We now
summarize the ANOVA hypothesis testing method for two or more populations.
ONE-WAY ANOVA FOR k ‡2 POPULATIONS
We test
H0 : m1 ¼ m2 ¼  ¼ mk versus Ha : At leasttwoof themi
0sare different:
When H0 is true, we have
E MST
ð
Þ ¼ E MSE
ð
Þ
The greater the differences among the m’s, the larger the E(MST) will be relative to E(MSE).
Test statistic:
F ¼ MST
MSE:
Rejection region is
RR : F > Fa
with u1¼(k1) numerator degrees of freedom and u2¼P
i¼1
k
nik¼Nk denominator degrees of
freedom, where N¼P
i¼1
k
ni.
Assumptions: The observations Yij0s are assumed to be independent and normally distributed
with mean mi, i¼1, 2, . . . , k, and variance s2.
508
CHAPTER 10 Analysis of Variance

Now we give a five-step computational procedure that we could follow for
ANOVA for the completely randomized design.
ONE-WAY ANOVA PROCEDURE FOR k ‡2 POPULATIONS
We test
H0 : m1 ¼ m2 ¼  ¼ mk versusHa : At leasttwo of themi
0sare different:
1. Compute
Ti ¼
X
ni
j¼1
yij,T ¼
X
k
i¼1
X
ni
j¼1
yij, and
X
k
i¼1
X
ni
j¼1
y2
ij,
CM ¼
Xk
i¼1
Xni
j¼1yij

2
N
¼ T2
N , where N ¼
X
k
i¼1
ni, and
Ti ¼ Ti
ni
,
Total SS ¼
X
k
i¼1
X
ni
j¼1
y2
ij CM:
2. Compute the sum of squares between samples (treatments),
SST ¼
X
k
i¼1
T2
i
ni
CM:
and the sum of squares within samples,
SSE ¼ TotalSSSST
Let
MST ¼ SST
k 1,
and
MSE ¼ SSE
nk:
3. Compute the test statistic:
F ¼ MST
MSE:
4. For a given a, find the rejection region as
RR : F > Fa
with u1¼(k1) numerator degrees of freedom and u2¼(P
i¼1
k
ni)k¼Nk denominator
degrees of freedom, where N¼P
i¼1
k
ni.
Continued
509
10.3 ANOVA for Completely Randomized Design

5. Conclusion: If the test statistic F falls in the rejection region, conclude that the sample evidence
supports the alternative hypothesis that the means are indeed different for the k treatments and
are not all equal.
Assumptions: The samples are randomly selected from the k populations in an independent manner.
The populations are assumed to be normally distributed with equal variances s2 and means
m1,. . .,mk .
Even though the completely randomized design is extremely easy to construct
and the calculations described above are relatively easy, the homogeneousness of
the treatments is crucial. Any extraneous sources of variability will make it more
difficult to detect differences among treatment means due to inflation of the
error term.
10.3.1 THE p-VALUE APPROACH
Note that if we are using statistical software packages, the p-value approach can be
used for the testing. Just compare the p-value and a to arrive at a conclusion. Refer to
the computer examples in Section 10.7.
The following example illustrates the ANOVA procedure.
EXAMPLE 10.3.1
The three random samples in Table 10.1 represent test scores from three classes of statistics taught
by three different instructors and are independently obtained. Assume that the three different popu-
lations are normal with equal variances.
At the a¼0.05 level of significance, test for equality of population means.
Solution
We test
H0 : m1 ¼ m2 ¼ m3 versus Ha : At least two of the m0s are different:
Here, k¼3, n1¼5, n2¼3, and N¼n1+n2+n3¼11.
Also,
Ti
380
199
257
ni
5
3
3
Ti
76
66.33
85.67
Table 10.1
Sample 1
Sample 2
Sample 3
64
56
81
84
74
92
75
69
84
77
80
510
CHAPTER 10 Analysis of Variance

Clearly, the SMs are different. The question we are going to answer is: Is this difference due to
just chance, or is it due to a real difference caused by different teaching styles? For this, we now
compute the following:
CM ¼
X
i
X
jyij

2
N
¼ 836
ð
Þ2
11
¼ 63,536,
TotalSS ¼
X
i
X
j
y2
ij CM
¼ 64,56063,536 ¼ 1024,
SST ¼
X
i
T2
i
ni
CM
¼ 380
ð
Þ2
5
+ 199
ð
Þ2
3
+ 257
ð
Þ2
3
CM
¼ 64,096:6663,536 ¼ 560:66,
SSE ¼ Total SSSST
¼ 1024560:66 ¼ 463:34:
Hence,
MST ¼ SST
k 1 ¼ 560:66
2
¼ 280:33,
and
MSE ¼ SSE
N k ¼ 463:34
8
¼ 57:9175:
The test statistic is
F ¼ MST
MSE ¼ 280:33
57:9175 ¼ 4:84:
From the F-table, F0.05,2,8¼4.46
Therefore, the rejection region is given by
RR : F > 4:46:
Decision: Because the observed value of F¼4.84 falls in the rejection region, we do reject H0
and conclude that there is sufficient evidence to indicate a difference in the true means.
If we want the p-value, we can see from the F-table that 0.025<p-value<0.05, indicating the
rejection of the null hypothesis with a¼0.05. Using statistical software packages, we can get the
exact p-value.
The calculations obtained in analyzing the total sum of squares into its components
are usually summarized by the analysis-of-variance table (ANOVA table), given in
Table 10.2.
Sometimes, one may also add a column for the p-value, P(Fk–1,n–kobserved F),
in the ANOVA table.
For the previous example, we can summarize the computations by the ANOVA
table shown in Table 10.3.
511
10.3 ANOVA for Completely Randomized Design

10.3.2 TESTING THE ASSUMPTIONS FOR ONE-WAY ANOVA
The randomness assumption could be tested using the Wald-Wolfowitz test (see Pro-
ject 12B). The assumption of independence of the samples is hard to test without
knowing how the data are collected and should be implemented during collection
of data in the design stage. Normality can be tested (this should be performed sep-
arately for each sample, not for the total data set) using probability plots or other tests
such as the chi-square goodness-of-fit test. ANOVA is fairly robust against violation
of this assumption if the sample sizes are equal. Also, if the sample sizes are fairly
large, the central limit theorem helps. The presence of outliers is likely to increase the
sample variance, thus decreasing the value of the F-statistic for ANOVA, which will
result in a lower power of the test. Box plots or probability plots could be used to
identify the outliers. If the normality test fails, transforming the data (see
Section 14.4.2) or a nonparametric test such as the Kruskal-Wallis test described
in Section 12.5.1 may be more appropriate. If the sample sizes of each sample are
equal, ANOVA is mostly robust for violation of homogeneity of the variances.
A rule of thumb used for robustness for this condition is that the ratio of sample var-
iance of the largest sample variance s2 to the smallest sample variance s2 should be no
more than 3:1. Another popular rule of thumb used in one-way ANOVA to verify the
requirement of equality of variances is that the largest sample standard deviation not
be larger than two times the smallest sample standard deviation. Graphically, repre-
senting side-by-side box plots of the samples can also reveal lack of homogeneity of
variances if some box plots are much longer than others (see Figure 10.3e). For a
significance test on the homogeneity of variances (Levene’s test), refer to
Section 14.4.3. If these tests reveal that the variances are different, then the
Table 10.2
Source of
Variation
Degree of
Freedom
Sum of Squares
Mean
Squares
F-
statistic
Treatments
k–1
SST ¼ Pk
i¼1
Ti
2
ni CM
MST ¼ SST
k 1
MST
MSE
Error
N–k
SSE¼Total SS–SST
MSE ¼ SSE
Nk
Total
N–1
Total SS ¼ Pk
i¼1
Pni
i¼1 yij y

2
Table 10.3
Source of
variation
Degree of
freedom
Sum of
squares
Mean
square
F-
statistic
p-
Value
Treatments
2
560.66
280.33
4.84
0.042
Error
8
463.34
57.917
Total
10
1024
512
CHAPTER 10 Analysis of Variance

populations are different, in spite of what ANOVA concludes about differences of
the means. But this itself is significant, because it shows that the treatments had
an effect.
EXAMPLE 10.3.2
In order to study the effect of automobile size on the noise pollution, the following data are randomly
chosen from the air pollution data (source: A.Y. Lewin and M.F. Shakun, Policy Sciences: Meth-
odology and Cases, Pergamon Press, 1976, p. 313). The automobiles are categorized as small,
medium, large, and noise level reading (decibels) are given in Table 10.4.
At the a¼0.05 level of significance, test for equality of population mean noise levels for dif-
ferent sizes of the automobiles. Comment on the assumptions.
Solution
Let m1, m2, and m3 be population mean noise levels for small, medium, and large automobiles, respec-
tively. First we test for the assumptions. Using Minitab, run tests for each of the samples; we can
justify the assumption of randomness of the sample values. A normality test for each column gives the
graphs shown in Figures 10.3a through 10.3c, through which we can reasonably assume the nor-
mality. Because the sample sizes are equal, we will use the one-way ANOVA method to analyze these
data.
Figure 10.3d indicates that the relative positions of the SMs are different, and Figure 10.3e
(Minitab steps for creating side-by-side box plots are given at the end of Example 10.7.1) gives
an indication of within-group variations; perhaps the group 2 (medium-size) variance is larger.
Now, we will do the analytic testing.
We test
H0 : m1 ¼ m2 ¼ m3 versus Ha : At least two of the m0s are different:
Here, k ¼ 3,n1 ¼ 5,n2 ¼ 5,n3 ¼ 5 and N ¼ n1 + n2 + n3 ¼ 15.
Also,
Ti
4125
4175
3860
ni
5
5
5
Ti
825
835
772
In the following calculations, for convenience we will approximate all values to the nearest
integer.
Continued
Table 10.4
Size of Automobile
Small
Medium
Large
Noise Level (decibels)
820
840
785
820
825
775
825
815
770
835
855
760
825
840
770
513
10.3 ANOVA for Completely Randomized Design

840
835
830
825
820
815
810
(a)
99
95
90
80
70
60
50
40
30
20
10
5
1
Mean
St dev
AD
P-value
825
6.124
N
5
0.464
0.135
Small
Percent
Noise level for small size automobiles
Normal
880
870
860
850
840
830
820
810
800
790
99
95
90
80
70
60
50
40
30
20
10
5
1
Medium
Percent
(b)
Noise level for medium size automobiles
Normal
Mean
St dev
AD
P-value
835
15.41
N
5
0.229
0.630
FIGURE 10.3
(a) Normal plot for noise level of small automobiles. (b) Normal plot for noise level of medium-
sized automobiles.
514
CHAPTER 10 Analysis of Variance

790
780
770
760
750
(c)
99
95
90
80
70
60
50
40
30
20
10
5
1
Large
Percent
Noise level for large size automobiles
Normal
Mean
St dev
AD
P-value
772
9.083
N
5
0.245
0.570
840
Chart title
830
820
810
800
790
780
770
760
(d)
0
0.5
1
1.5
2
2.5
3
3.5
FIGURE.10.3, cont’d (c) Normal plot for noise level of large automobiles. (d) Mean decibel
levels for three sizes of automobiles.
Continued
515
10.3 ANOVA for Completely Randomized Design

CM ¼
X
i
X
jyij

2
N
¼ ð12,160Þ2
15
¼ 9,857,707,
TotalSS ¼
X
i
X
j
y2
ij CM
¼ 12,893,
SST ¼
X
i
T2
i
ni
CM
¼ 11,463,
SSE ¼ TotalSSSST
¼ 1430:
Hence,
MST ¼ SST
k 1 ¼ 11,463
2
¼ 5732
and
MSE ¼ SSE
N k ¼ 1430
12 ¼ 119:
The test statistic is
F ¼ MST
MSE ¼ 5732
119 ¼ 48:10:
From the table, we get F0.05,2,12¼3.89. Because the test statistic falls in the rejection region, we
reject at a¼0.05 the null hypothesis that the mean noise levels are the same. We conclude that size of
the automobile does affect the mean noise level.
It should be noted that the alternative hypothesis Ha in this section covers a wide
range of situations, from the case where all but one of the population means are equal
3
2
1
860
850
840
830
820
810
800
790
780
770
760
Size of auto
(e)
Decibles
FIGURE.10.3, cont’d (e) Side-by-side box plots for decibel levels for three sizes of automobiles.
516
CHAPTER 10 Analysis of Variance

to the case where they are all different. Hence, with such an alternative, if the samples
lead us to reject the null hypothesis, we are left with a lot of unsettled questions about
the means of the k populations. These are called post hoc testing. This problem of
multiple comparisons is the topic of Section 10.5.
10.3.3 MODEL FOR ONE-WAY ANOVA (OPTIONAL)
We conclude this section by presenting the classical model for one-way ANOVA.
Because the variables Yij values are random samples from normal populations with
E(Yij)¼mi and with common variance Var(Yij)¼s2, for i¼1,. . .,k and j¼1,. . ., ni, we
can write a model as
Yij ¼ mi + eij, j ¼ 1, ....,ni
where the error terms eij are independent normally distributed random variables with
E(eij)¼0 and Var(eij)¼s2. Let ai¼mim be the difference of mi (ith population
mean) from the GM m. Then ai can be considered as the ith treatment effect. Note
that the ai values are nonrandom. Because m¼Si (ni mi/N), it follows that P
i¼1
k
ai¼0.
This will result in the following classical model for one-way layout:
Yij ¼ m + ai + Eij, i ¼ 1, ...,k, j ¼ 1, ...,ni:
With this representation, the test H0: m1 ¼ m2 ¼    ¼ mk reduces to testing the null
hypothesis that there is no treatment effect, H0:ai ¼ 0, for i ¼ 1,. . .,k.
EXERCISES 10.3
10.3.1. In an effort to investigate the premium charged by insurance companies
for auto insurance, an agency randomly selects a few drivers who are
insured by one of three different companies. These individuals have
similar cars, driving records, and levels of coverage. Table 10.3.1 gives the
premiums paid per six months by these drivers with these three companies.
(a) Construct an ANOVA table and interpret the results.
(b) Using the 5% significance level, test the null hypothesis that the mean
auto insurance premium paid per six months by all drivers insured for
each of these companies is the same. Assume that the conditions of
completely randomized design are met.
Table 10.3.1
Company I
Company II
Company III
396
348
378
438
360
330
336
522
294
318
474
432
517
10.3 ANOVA for Completely Randomized Design

10.3.2. Three classes in elementary statistics are taught by three different persons:
a regular faculty member, a graduate teaching assistant, and an adjunct
from outside the university. At the end of the semester, each student is
given a standardized test. Five students are randomly picked from each of
these classes, and their scores are as shown in Table 10.3.2.
(a) Construct an ANOVA table and interpret your results.
(b) Test at the 0.05 level whether there is a difference between the mean
scores for the three persons teaching. Assume that the conditions of
completely randomized design are met.
10.3.3. Let n1 ¼ n2 ¼   ¼ nk ¼ n 0. Show that
X
k
i¼1
X
n0
j¼1
yij y

2
¼
X
k
i¼1
X
n0
j¼1
yij Ti

2
+ n
X
k
i¼1
Ti y

2:
10.3.4. For the sum of squares for treatment
SST ¼
X
k
i¼1
ni Ti y

2
show that
E SST
ð
Þ ¼ k 1
ð
Þs2 +
X
k
i¼1
ni mi m
ð
Þ2
where m ¼ 1
N
Xk
i¼1nimi:
[This exercise shows that the expected value of SST increases as the dif-
ferences among the mi’s increase.]
10.3.5. (a) Show that
SSE ¼
X
k
i¼1
ni 1
ð
ÞS2
i ¼
X
k
i¼1
X
ni
j¼1
Yij Ti

2,
where S2
i ¼
1
n1Sni
j¼1 Yij Ti

2 provides an independent, unbiased
estimator for s2 in each of the k samples.
(b) Show that SSE/s2 has a chi-square distribution with N – k degrees of
freedom, where N¼Si¼1
k
ni.
Table 10.3.2
Faculty
Teaching Assistant
Adjunct
93
88
86
61
90
56
87
76
73
75
82
90
92
58
47
518
CHAPTER 10 Analysis of Variance

10.3.6. Let each observation in a set of k independent random samples be
normally distributed with means m1,. . .,mk and common variance s2.
If H0 ¼ m1 ¼ m2 ¼   ¼ mk is true, show that
F ¼ SST= k 1
ð
Þ
SSE= nk
ð
Þ ¼ MST
MSE
has an F-distribution with k–1 numerator and n–k denominator degrees of
freedom.
10.3.7. The management of a grocery store observes various employees for work
productivity. Table 10.3.3 gives the number of customers served by each
of its four checkout lanes per hour.
(a) Construct an ANOVA table and interpret the results. Indicate any
assumptions that were necessary.
(b) Test whether there is a difference between the mean numbers of
customers served by the four employees at the 0.05 level. Assume that
the conditions of completely randomized design are met.
10.3.8. Table 10.3.4 represents immunoglobulin levels (with each observation
being the IgA immunoglobulin level measured in international units) of
children under 10 years of age of a particular group. The children are
grouped as follows: A: ages 1 to less than 3, B: ages 3 to less than 6, C: ages
6 to less than 8, and D: ages 8 to less than 10. Test whether there is a
difference between the means for each of the age groups. Use a¼0.05.
Interpret your results and state any assumptions that were necessary to
solve the problem.
10.3.9. Table 10.3.5 gives rental and homeowner vacancy rates by US region
(source: US Census Bureau) for 5 years.
Table 10.3.4
A
35
8
12
19
56
64
75
25
B
31
79
60
45
39
44
45
62
20
66
C
74
56
77
35
95
81
28
D
80
42
48
69
95
40
86
79
51
Table 10.3.3
Lane 1
Lane 2
Lane 3
Lane 4
16
11
8
21
18
14
12
16
22
10
17
17
21
10
10
23
15
14
13
17
10
15
519
10.3 ANOVA for Completely Randomized Design

Test at the 0.01 level whether the true rental and homeowner vacancy
rates by area are the same for all 5 years. Interpret your results and state
any assumptions that were necessary to perform the analysis.
10.3.10. Table 10.3.6 gives lower limits of income (approximated to the nearest
$1000 and calculated as of March of the following year) of the top
5% of US households by race from 1994 to 1998 (Source: US Census
Bureau).
Test at the 0.05 level whether the true lower limits of income for the
top 5% of US households for each race are the same for all 5 years.
10.3.11. Table 10.3.7 gives mean serum cholesterol levels (given in milligrams per
deciliter) by race and age for the adult population in the United States
between 1978 and 1980 (source: “Report of the National Cholesterol
Education Program Expert Panel on Detection, Evaluation, and Treatment
of High Blood Cholesterol in Adults,” Arch. Intern. Med. 148,
January 1988).
Test at the 0.01 level whether the true mean cholesterol levels for adult
population in the United States between 1978 and 1980 are the same.
Table 10.3.5
Rental Units
1995
1996
1997
1998
1999
Northeast
7.2
7.4
6.7
6.7
6.3
Midwest
7.2
7.9
8.0
7.9
8.6
South
8.3
8.6
9.1
9.6
10.3
West
7.5
7.2
6.6
6.7
6.2
Table 10.3.6
Race
Year
1994
1995
1996
1997
1998
All Races
110
113
120
127
132
White
113
117
123
130
136
Black
81
80
85
87
94
Hispanic
82
80
86
93
98
Table 10.3.7
Race
Age
20-24
25-34
35-44
45-54
55-64
65-74
All Races
180
199
217
227
229
221
White
180
199
217
227
230
222
Black
171
199
218
229
223
217
520
CHAPTER 10 Analysis of Variance

10.4 TWO-WAY ANOVA, RANDOMIZED COMPLETE
BLOCK DESIGN
A randomized block design, or the two-way ANOVA, consists of b blocks of k exper-
imental units each. In many cases we may be required to measure response at com-
binations of levels of two or more factors considered simultaneously. For example,
we might be interested in gas mileage per gallon among four different makes of cars
for both in-city and highway driving, or to examine weight loss comparing five dif-
ferent diet programs among whites, African Americans, Hispanics, and Asians
according to their gender. In studies involving various factors, the effect of each fac-
tor on the response variable may be analyzed using one-way classification. However,
such an analysis will not be efficient with respect to time, effort, and cost. Also, such
a procedure would give no knowledge about the likely interactions that may exist
among different factors. In such cases, the two-way ANOVA is an appropriate sta-
tistical method to use.
In a randomized block design, the treatments are randomly assigned to the units in
each block, with each treatment appearing exactly once in every block (that is, there
is no interaction between factors). Thus, the total number of observations obtained in
a randomized block design is n¼bk. The purpose of subdividing experiments into
blocks is to eliminate as much variability as possible, that is, to reduce the experi-
mental error or the variability due to extraneous causes. Refer to Section 9.2.3 for
a procedure to obtain completely randomized block design. The goal of such an
experiment is to test the equality of levels for the treatment effect. Sometimes, it
may also be of interest to test for a difference among blocks. We proceed to give
a formal statistical model for the completely randomized block design.
For i¼1, 2, . . ., k and j¼1, 2, . . ., b, let Yij¼m+ai+bj+eij, where Yij is the obser-
vation on treatment i in block j, m is the overall mean, ai is the nonrandom effect of
treatment i, bj is the nonrandom effect of block j, and eij are the random error terms
such that Eij are independent normally distributed random variables with E (eij)¼0 and
Var (eij)¼s2. In this case, Sai¼0, and Sbj¼0.
The ANOVA for a randomized block design proceeds similarly to that for a
completely randomized design, the main difference being that the total sum of squares
of deviations of the response measurements from their means may be partitioned into
three parts: the sum of squares of blocks (SSB), treatments (SST), and error (SSE).
Let Bj¼Si¼1
k
yij and Bj denote, respectively, the total sum and mean of all obser-
vations in block j. Represent the total for all observations receiving treatment i by
Ti¼Sj¼1
b
yij , and mean and Ti , respectively. Let
y ¼ average of n ¼ bk observations ¼ 1
n
X
b
j¼1
X
k
i¼1
yij
and
CM ¼ 1
n total of all observations
ð
Þ2
¼ 1
n
X
b
j¼1
X
k
i¼1
yij
 
!2
:
521
10.4 Two-Way ANOVA, Randomized Complete Block Design

For convenience, we can represent the two-way classification as in Table 10.5.
Note that from Table 10.5, we can obtain P
j¼1
b
P
i¼1
k
yij¼P
j¼1
b Bj. Hence,
CM¼(1/n) (P
j¼1
b Bj)2.
Then for a randomized block design with b blocks and k treatments, we need to
compute the following sums of squares. They are
Total SS ¼ SSB + SST + SSE
¼
X
b
j¼1
X
k
i¼1
yij y

2
¼
X
b
j¼1
X
k
i¼1
y2
ij CM
SSB ¼ k
X
b
j¼1
Bj y

2 ¼
Xb
j¼1B2
j
k
CM
and
SSB ¼ b
X
k
i¼1
Ti y

2 ¼
Xk
i¼1T2
i
b
CM
SSE ¼ TotalSSSSBSST:
We define
MSB ¼ SSB
b1,
MST ¼ SST
k 1,
and
MSE ¼
SSE
nbk + 1:
Table 10.5
Blocks
1
2
. . .
j
. . .
b
Total Ti
Mean Ti
Treatment 1
y11
y12
. . .
y1j
. . .
y1b
T1
T1
Treatment 2
y21
y22
. . .
y2j
. . .
y2b
T2
T2









Treatment i
yi1
yi2
. . .
yij
. . .
yib
Ti
Ti






Treatment k
yk1
yk2
. . .
ykj
. . .
ykb
Tk
Tk
Total Bj
B1
B2
. . .
Bj
. . .
Bb
Mean Bj
B1
B2
. . .
Bj
. . .
Bb
y
522
CHAPTER 10 Analysis of Variance

The ANOVA for the randomized block design is presented in Table 10.6. The col-
umn corresponding to d.f. represents the degrees of freedom associated with each
sum of squares. MS denotes the mean square.
To test the null hypothesis that there is no difference in treatment means, that is,
to test
H0 : ai ¼ 0, i ¼ 1, ...,k versus Ha : Not all ais are zero
We use the F-statistic
F ¼ MST
MSE
and reject H0 if F>Fa based on (k–1) numerator and (n–b–k+1) denominator
degrees of freedom.
Although blocking lowers the experimental error, it also furnishes a chance to see
whether evidence exists to indicate a difference in the mean response for blocks. In
this case we will be testing the hypothesis
H0 : bj ¼ 0, j ¼ 1, ...,b versus Ha : Not all bj
0s are zero:
Under the assumption that there is no difference in the mean response for blocks,
MSB provides an unbiased estimator for s2 based on (b–1) degrees of freedom.
If there is a real difference that exists among block means, MSB will be larger in
comparison with MSE and
F ¼ MSB
MSE
will be used as a test statistic. The rejection region will be if F>Fa based on (b–1)
numerator and (n–b–k+1) denominator degrees of freedom.
We now summarize the foregoing methodology in a step-by-step computational
procedure. For a reasonable data size, we could use scientific calculators for handling
the ANOVA calculations. For larger data sets, the use of statistical software packages
is recommended.
Table 10.6
Source
d.f.
SS
MS
Blocks
b–1
SSB
SSB
b1
Treatments
k–1
SST
SST
k 1
Error
(b–1)(k–1)¼n–b–k+1
SSE
SST
nbk + 1
Total
n–1
Total SS
523
10.4 Two-Way ANOVA, Randomized Complete Block Design

COMPUTATIONAL PROCEDURE FOR RANDOMIZED BLOCK DESIGN
1. Calculate the following quantities:
(i) Sum the observations for each row to form row totals:
T1,T2, ...,Tk, where Ti ¼
X
b
j¼1
yij:
(ii) Sum the observations for each column to form column totals:
B1,B2, ...,Bb, where Bj ¼
X
k
i¼1
yij:
(iii) Find the sum of all observations:
X
b
j¼1
X
k
i¼1
yij ¼
X
b
j¼1
Bj:
2. Calculate the following quantities:
(i) Square the sum of the totals for each column and divide it by n¼bk to obtain
CM ¼ 1
n
X
b
j¼1
Bj
 
!2
:
(ii) Find the sum of squares of the totals of each column and divide it by k to obtain
1
k
X
b
j¼1
B2
j
and
SSB ¼
Xb
j¼1B2
j
k
CM and MSB ¼ SSB
b1:
(iii) Find the sum of squares of the totals of each row and divide it by b to obtain
Xk
i¼1T2
i
b
and
SST ¼
Xk
i¼1T2
j
b
CM and MSB ¼ SST
k1:
(iv) Find the sum of squares of individual observations:
X
b
j¼1
X
k
i¼1
y2
ij
Also compute
TotalSS ¼
X
b
j¼1
X
k
i¼1
y2
ij CM:
524
CHAPTER 10 Analysis of Variance

(v) Using (ii), (iii), and (iv), find
SSE ¼ TotalSSSSBSST and MSE ¼
SSE
nbk + 1:
3. To test the null hypothesis that there is no difference in treatment means:
(i) Compute the F-statistic,
F ¼ MST
MSE:
(ii) From the F-table, find the value of Fa,u1,u2, where u1¼(k1) is the numerator and
u2¼(nbk+1) the denominator degrees of freedom.
(iii) Decision: Reject H0 if F > Fa,u1,u2 and conclude that there is evidence to conclude that there
is a difference in treatment means at level a.
4. To test the null hypothesis that there is no difference in the mean response for blocks,
(i) Compute the F-statistic,
F ¼ MSB
MSE:
(ii) From the F-table, find the value of Fa,u1,u2, where u1¼(b1) is the numerator and
u2¼(nbk+1) the denominator degrees of freedom.
(iii) Decision: Reject H0 if F > Fa,u1,u2 and conclude that there is evidence to conclude there is a
difference in the mean response for blocks at level a.
Assumptions: The samples are randomly selected in an independent manner from n¼bk popu-
lations. The populations are assumed to be normally distributed with equal variances s2. Also, there
are no interactions between the variables (two factors).
We have already discussed the assumptions and how to verify those assumptions
in one-way analysis. The only new assumption in the randomized blocked design is
about the interactions. One of the ways to verify the assumption of no interaction is to
plot the observed values against the sample number. If there is no interaction, the line
segments (one for each block) will be parallel or nearly parallel; see Figure 9.2. If the
lines are not approximately parallel, then there is likely to be interaction between
blocks and treatments. In the presence of interactions, the analysis of this section
needs to be modified. For details on those procedures, refer to more specialized
books on ANOVA methods.
We illustrate the randomized block design procedure with the following example.
EXAMPLE 10.4.1
A furniture company wants to know whether there are differences in stain resistance among the four
chemicals used to treat three different fabrics. Table 10.7 shows the yields on resistance to stain (a
low value indicates good stain resistance).
At the a¼0.05 level of significance, is there evidence to conclude that there is a difference in
mean resistance among the four chemicals? Is there any difference in the mean resistance among the
materials? Give bounds for the p-values in each case.
Continued
525
10.4 Two-Way ANOVA, Randomized Complete Block Design

Solution
Here T1¼16, T2¼28, T3¼14, and T4¼24. Also, B1¼21, B2¼32, and B3¼29. In addition, b¼3,
k¼4, and n¼bk¼12. Now
CM ¼ 1
n
X
b
j¼1
Bj
 
!2
¼ 1
12 82
ð
Þ2 ¼ 560:3333:
We can compute the following quantities:
SSB ¼
Xb
j¼1B2
j
k
CM ¼ 2306
4
560:3333 ¼ 16:1667,
MSB ¼ SSB
b1 ¼ 16:1667
2
¼ 8:0834,
SST ¼
Xk
i¼1T2
i
b
CM ¼ 1812
3
560:3333 ¼ 43:6667,
and
MST ¼ SST
k1 ¼ 43:6667
3
¼ 14:5556:
We have P
j¼1
b
P
i¼1
k
yij
2 ¼632. From this
TotalSS ¼
X
b
j¼1
X
k
i¼1
y2
ij CM ¼ 632560:3333 ¼ 71:666,
SSE ¼ TotalSSSSBSST ¼ 71:666716:166743:6667
¼ 11:8333
and
MSE ¼
SSE
nbk + 1 ¼ 11:8333
6
¼ 1:9722:
The F-statistic is
F ¼ MST
MSE ¼ 14:5556
1:9722 ¼ 7:3804
From the F-table, F0.05,3,6¼4.76. Because the observed value F¼7.3804>4.76, we reject the
null hypothesis and conclude that there is a difference in mean resistance among the four chemicals.
Because the F-value falls between a¼0.025 and a¼0.01, the p-value falls between 0.01 and 0.025.
To test for the difference in the mean resistance among the materials,
F ¼ MSB
MSE ¼ 8:0834
1:9722 ¼ 4:0987:
From the F-table, F0.05,2,6¼5.14. Because the observed value of F¼4.098<5.14, we conclude
that there is no difference in the mean resistance among the materials. Because the F-value falls
between a¼0.10 and 0.05, the p-value falls between 0.05 and 0.10.
Table 10.7
Chemical
Material
I
II
III
Total
C1
3
7
6
16
C2
9
11
8
28
C3
2
5
7
14
C4
7
9
8
24
Total
21
32
29
82
526
CHAPTER 10 Analysis of Variance

EXERCISES 10.4
10.4.1. Show that
X
b
j¼1
X
k
i¼1
yij y

2
¼
X
k
i¼1
X
b
j¼1
yij Ti Bj y

2
+ b
X
k
i¼1
Ti y

2 + k
X
b
j¼1
Bj y

2:
[Hint: Use the identity yij y ¼ yij Ti Bj y


+ Ti y


+ Bj y


:
10.4.2. Show the following:
(a) E(MSE) ¼ s2,
(b) E MSB
ð
Þ ¼
k
b1
X
b
j¼1
B2
j + s2,
(c) E MST
ð
Þ ¼
b
k 1
X
k
i¼1
t2
i + s2:
10.4.3. The least-square estimators of the parameters m, ti’s, and bj’s are obtained
by minimizing the sum of squares
W ¼
X
k
i¼1
X
b
j¼1
yij mti bj

2
with
respect
to
m,
ti’s,
and
bj’s;
subject
to
the
restrictions:
P
i¼1
k
ti¼P
j¼1
b bj¼0. Show that the resultant estimators are
^m ¼ y,
^ti ¼ Ti y, i ¼ 1,2, ...,k,
and
^bj ¼ Bj y, j ¼ 1, ...,b:
10.4.4. In order to test the wear on four hyperalloys, a test piece of each alloy was
extracted from each of the three positions of a test machine. The reduction
of weight in milligrams due to wear was determined on each piece, and the
data are given in Table 10.4.1.
Table 10.4.1 Loss in Weights Due to Wear Testing of Four Materials (in mg)
Position
Type of alloy
1
2
3
1
241
270
274
2
195
241
218
3
235
273
230
4
234
236
227
527
10.4 Two-Way ANOVA, Randomized Complete Block Design

At a¼0.05, test the following hypotheses, regarding the positions as
blocks:
(a) There is no difference in average wear for each material.
(b) There is no difference in average wear for each position.
(c) Interpret your final result and state any assumptions that were necessary
to solve the problem.
10.4.5. For the data of Exercise 10.3.10, test at the 0.05 level that the true income
lower limits of the top 5% of US households for each race are the same for
all 5 years. Also, test at the 0.05 level that the true income lower limits of
the top 5% of US households for each year between 1994 and 1998
are the same.
10.4.6. For the data of Exercise 10.3.11, test at the 0.01 level that the true mean
cholesterol levels for all races in the United States during 1978–1980 are the
same. Also, test at the 0.01 level that the true mean cholesterol levels for all
ages in the United States during 1978-1980 are the same.
10.4.7. In order to see the effect of hours of sleep on tests of different skill
categories (vocabulary, reasoning, and arithmetic), tests consisting of 20
questions each in each category were given to 16 students, four each based
on the hours of sleep they had on the previous night. Each right answer is
given one point. Table 10.4.2 gives the cumulative scores of the each of the
four students in each category.
Test at the 0.05 level whether the true mean performance for different
hours of sleep is the same. Also, test at the 0.05 level whether the true mean
performance for each category of the test is the same.
10.5 MULTIPLE COMPARISONS
The ANOVA procedures that we have used so far showed whether differences
among several means are significant. However, if the equality of means is rejected,
the F-test did not pinpoint for us which of the given means or group of means differs
significantly from another given mean or group of means. With ANOVA, when the
null hypothesis of equality of means is rejected, the problem is to see whether there is
some way to follow up (post hoc) this initial test H0:m1 ¼ m2 ¼   ¼ mk by looking at
subhypotheses, such as H0:m1 ¼ m2 .
Table 10.4.2
Hours of sleep
Category
Vocabulary
Reasoning
Arithmetic
0
44
33
35
4
54
38
18
6
48
42
43
8
55
52
50
528
CHAPTER 10 Analysis of Variance

This involves multiple tests. However, the solution is not to use a simple t-test
repeatedly for every possible combination taken two at a time. That, apart from intro-
ducing many tests, will considerably increase the significance level, the probability
of type I error. For example, to test four samples we will need (2
4)¼6 tests. If each one
of the comparisons is tested with the same value of a¼P (type I error), and if all the
null hypotheses involving six comparisons are true, then the probability of rejecting
at least one of them is
P at least one type I error
ð
Þ ¼ 1 1a
ð
Þ6:
In particular, if a¼0.01, then P (at least one type I error)¼0.077181, which is sig-
nificantly higher than the original error value of 0.01.
One way to investigate the problem is to use a multiple comparison procedure.
A good deal of work has been done on problems of multiple comparisons. There are a
variety of techniques available in the literature, such as the Bonferroni procedure,
Tukey’s method, and Scheffe’s method. We now describe one of the more popular
procedures called Tukey’s method for completely randomized, one-factor design.
In this multiple comparison problem, we would like to test H0:mi ¼
mj versus Ha:mi 6¼ mj, for all i 6¼ j. Tukey’s method will be used to test all possible
differences of means to decide whether at least one of the differences mi–mj is con-
siderably different from zero. In this comparison problem, Tukey’s method makes
use of confidence intervals for mi–mj. If each confidence interval has a confidence
level 1–a, then the probability that all confidence intervals include their respective
parameters is less than 1–a. We now describe this method where each of the k SMs is
based on the common number of observations, n.
Let N¼kn be the total number of observations and let
S2 ¼
1
N k
X
k
i¼1
X
ni¼n
j¼1
Yij Ti

2:
Let Tmax ¼ max T1, ..., Tk


and Tmin ¼ min T1, ..., Tk


: Define the random
variable
Q ¼ Tmax Tmin
S
ﬃﬃﬃn
p
:
The distribution of Q under the null hypothesis H0:m1 ¼   ¼ mk. is called the
Studentized range distribution, which depends on the number of samples k and
the degrees of freedom u¼N–k¼(n–1)k. We denote the upper a critical value
by qa,k,u. The Studentized range distribution table gives values for selected values
of k, u, and a¼0.01, 0.05, and 0.10. The following theorem, due to Tukey, defines
the test procedure.
Theorem 10.5.1 Let Ti, i ¼ 1,2, ...,k be the k SMs in a completely randomized
design. Let mi, i ¼ 1,2,...,k be the true means and let ni¼n be the common sample
size. Then the probability that all (2
k) differences mi–mj will simultaneously satisfy the
inequalities
529
10.5 Multiple Comparisons

Ti Tj


qa,k,u
sﬃﬃﬃn
p  mi mj  Ti Tj


+ qa,k,u
sﬃﬃﬃn
p ,
is (1–a), where qa,k,u is the upper a critical value of the Studentized range distribu-
tion. If, for a given i and j, zero is not contained in the preceding inequality, H0:
mi¼mj can be rejected in favor of Ha : mi6¼mj, at the significance level of a.
Now we give a step-by-step approach to implementing the Tukey’s method dis-
cussed earlier.
PROCEDURE TO FIND (1–a)100% CONFIDENCE INTERVALS FOR
DIFFERENCE OF MEANS WITH COMMON SAMPLE SIZE N:
TUKEY’S METHOD
1. There are
k
2
 
comparisons of mi versus mj.
2. Compute the following quantities:
Ti ¼
Xni
j¼1yij
ni
, i ¼ 1,2, ...,k,
and
s2 ¼
1
N k
X
k
i¼1
X
ni¼n
j¼1
yij Ti

2
, whereN ¼ kn:
3. From the Studentized range distribution table, find the upper a critical value, qa, k, u, where
u¼Nk¼(n1)k.
4. For each of
k
2
 
pair (i, j), i6¼j, compute the Tukey’s interval
Ti Tj


qa,k,u
sﬃﬃﬃn
p , Ti Tj


+ qa,k,u
sﬃﬃﬃn
p


:
5. Let NR denote insufficient evidence for rejecting H0. Create the following table for each of (2
k)
pairwise difference mimj, i6¼j, and do not reject if the Tukey interval contains the number 0.
Otherwise reject.
Table 10.8 is used to summarize the final calculations of the Tukey method.
In practice, there are now numerous statistical packages available for Tukey’s
purpose. The following example is solved using Minitab. The necessary Minitab
commands are given in Example 10.7.3.
Table 10.8
mi–mj
TiTj
Tukey Interval
Observation
Conclusion
m1–m2
T1 T2
. . .
Doesn’t contain 0
Reject
m1–m3
T1 T3
. . .
Contains 0
Do not reject
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
530
CHAPTER 10 Analysis of Variance

EXAMPLE 10.5.1
Table 10.9 shows the one-year percentage total return of the top five stock funds for five different
categories (source: Money, July 2000). Which categories have similar top returns and which are dif-
ferent? Use 95% Tukey’s confidence intervals.
Solution
For simplicity of computation, we will use SPSS (Minitab steps are given in Example 10.7.2). The
following is the output.
One-way
ANOVA
RETURN
Note that since the p-value is 0.001, we are rejecting the null hypothesis that all means are
equal. To find out which of the means might be different, we use the multiple comparison output.
Post Hoc Tests
Multiple Comparisons
Dependent Variable: RETURN
Tukey HSD
Continued
Table 10.9
Large-cap
Mid-cap
Small-cap
Hybrid
Specialty
110.1
299.8
153.8
68.3
181.6
102.9
139.0
139.8
67.1
159.3
93.1
131.2
138.3
42.5
138.3
83.0
110.5
121.4
40.0
132.6
83.3
129.2
135.9
41.0
135.7
Sum of Squares
df
Mean Square
F
Sig.
Between Groups
41,243.698
4
10,310.925
7.397
0.001
Within Groups
27,877.580
20
1393.879
Total
69,121.278
24
(I)
Fund
(J)
Fund
Mean
Difference
(IJ)
Std.
Error
Sig.
95% Confidence
Interval
Lower
Bound
Upper
Bound
1.00
2.00
–67.4600
23.61253
0.066
–138.1175
3.1975
3.00
–43.3600
23.61253
0.382
–114.0175
27.2975
4.00
42.7000
23.61253
0.396
–27.9575
113.3575
5.00
–55.0200
23.61253
0.177
–125.6775
15.6375
2.00
1.00
67.4600
23.61253
0.066
–3.1975
138.1175
3.00
24.1000
23.61253
0.843
–46.5575
94.7575
531
10.5 Multiple Comparisons

Homogeneous Subsets
RETURN
Tukey HSDa
The Tukey intervals for pairwise differences (mi–mj) are in the foregoing computer printout.
For example, the Tukey interval for (m1–m2) is (138.1, 3.2) and for (m2–m4) is (39.5, 180.8). Also,
SM and standard deviation are given in the output. For example, 94.48 is the SM of the five data points
of large-cap funds, and 11.97 is the sample standard deviation of the five data points of large-cap
funds.
If the Tukey interval for a particular difference (mj–mi) contains the number zero, we do
not reject the H0 : mi¼mj. Otherwise, we reject the H0 : mi¼mj. For example the interval for
(m4–m2) is (39.5–180.8) and does not contain zero. Hence we reject H0 : m4¼m2.
The complete table corresponding to step 5 is produced in Table 10.10, where N.R. represents
“not reject.”
(I)
Fund
(J)
Fund
Mean
Difference
(IJ)
Std.
Error
Sig.
95% Confidence
Interval
Lower
Bound
Upper
Bound
4.00
110.1600*
23.61253
0.001
39.5025
180.8175
5.00
12.4400
23.61253
0.984
–58.2175
83.0975
3.00
1.00
43.3600
23.61253
0.382
–27.2975
114.0175
2.00
–24.1000
23.61253
0.843
–94.7575
46.5575
4.00
86.0600*
23.61253
0.012
15.4025
156.7175
5.00
–11.6600
23.61253
0.987
–82.3175
58.9975
4.00
1.00
–42.7000
23.61253
0.396
–113.3575
27.9575
2.00
– 110.1600*
23.61253
0.001
–180.8175
–39.5025
3.00
–86.0600*
23.61253
0.012
–156.7175
–15.4025
5.00
– 97.7200*
23.61253
0.004
–168.3775
–27.0625
5.00
1.00
55.0200
23.61253
0.177
–15.6375
125.6775
2.00
–12.4400
23.61253
0.984
–83.0975
58.2175
3.00
11.6600
23.61253
0.987
–58.9975
82.3175
4.00
97.7200*
23.61253
0.004
27.0625
168.3775
*The mean difference is significant at the 0.05 level.
Subset for alpha¼0.05
FUND
N
1
2
4.00
5
51.7800
1.00
5
94.4800
94.4800
3.00
5
137.8400
5.00
5
149.5000
2.00
5
161.9400
Sig.
0.396
0.066
Means for groups in homogeneous subsets are displayed.
aUses Harmonic Mean Sample Size¼5.000.
532
CHAPTER 10 Analysis of Variance

Based on the 95% Tukey intervals, the average top return of hybrid funds is different from those
for mid-cap, small-cap, and specialty funds. All other returns are similar.
In Tukey’s method, the confidence coefficient for the set of all pairwise comparisons
{mi–mj} is exactly equal to 1–a when all sample sizes are equal. For unequal sample
sizes, the confidence coefficient is greater than 1–a. In this sense, Tukey’s procedure is
conservative when the sample sizes are not equal. In the case of unequal sample sizes,
one has to estimate the standard deviation for each pairwise comparison. Tukey’s pro-
cedure for unequal sample sizes is sometimes referred to as the Tukey-Kramer method.
EXERCISES 10.5
10.5.1. A large insurance company wants to determine whether there is a difference
in the average time to process claim forms among its four different
processing facilities. The data in Table 10.5.1 represent weekly average
number of days to process a form over a period of four weeks.
(a) Test whether there is a difference in the average processing times at the
0.05 level.
(b) Test whether there is a difference, using Tukey’s method to find which
facilities are different.
(c) Interpret your results and state any assumptions you have made in
solving the problem.
Table 10.5.1
Facility 1
Facility 2
Facility 3
Facility 4
1.50
2.25
1.30
2.0
0.9
1.85
2.75
1.5
1.12
1.45
2.15
2.85
1.95
2.15
1.55
1.15
Table 10.10
mi–mj
TiTj
Tukey interval
Reject or N.R.
Conclusion
m1–m2
161.94–94.48
(138.1, 3.2)
N.R.
m1¼m2
m1–m3
137.84–94.48
(114.0, 27.3)
N.R.
m1¼m3
m2–m3
137.84–161.94
(46.6, 94.8)
N.R.
m3¼m2
m1–m4
51.78–94.48
(27.9, 113.3)
N.R.
m4¼m1
m2–m4
51.78–161.94
(39.5, 180.8)
R
m46¼m1
m3–m4
51.78–137.84
(15.4, 156.7)
R
m46¼m3
m1–m5
149.50–94.98
(125.6, 15.6)
N.R.
m5¼m1
m2–m5
149.50–161.94
(58.2, 83.1)
N.R.
m5¼m2
m3–m5
149.50–137.84
(82.3, 59.0)
N.R.
m5¼m3
m4–m5
149.50–51.78
(168.3,–27.1)
R
m56¼m4
533
10.5 Multiple Comparisons

10.5.2. Table 10.5.2 gives the rental vacancy rates by US region (source: US
Census Bureau) for 5 years.
(a) Test at the 0.01 level whether the true rental vacancy rates by region are
the same for all 5 years.
(b) If there is a difference, use Tukey’s method to find which regions are
different.
10.5.3. Table 10.5.3 gives lower limits of income (approximated to nearest
$1000 and calculated as of March of the following year) by race for
the top 5% of US households from 1994 to 1998. (Source: US Census
Bureau.)
(a) Test at the 0.05 level whether the true lower limits of income
for the top 5% of US households for each race are the same for all
5 years.
(b) If there is a difference, use Tukey’s method to find which is different.
(c) Interpret your results and state any assumptions you have made in
solving the problem.
10.5.4. The data in Table 10.5.4 represent the mean serum cholesterol levels (given
in milligrams per deciliter) by race and age in the United States from 1978
to 1980 (source: “Report of the National Cholesterol Education Program
Table 10.5.3
Race
1994
1995
1996
1997
1998
All Races
110
113
120
127
132
White
113
117
123
130
136
Black
81
80
85
87
94
Hispanic
82
80
86
93
98
Table 10.5.4
Race
Age
20-24
25-34
35-44
45-54
55-64
65-74
All races
180
199
217
227
229
221
White
180
199
217
227
230
222
Black
171
199
218
229
223
217
Table 10.5.2
Rental units
1995
1996
1997
1998
1999
Northeast
7.2
7.4
6.7
6.7
6.3
Midwest
7.2
7.9
8.0
7.9
8.6
South
8.3
8.6
9.1
9.6
10.3
West
7.5
7.2
6.6
6.7
6.2
534
CHAPTER 10 Analysis of Variance

Expert Panel on Detection, Evaluation, and Treatment of High Blood
Cholesterol in Adults,” Arch. Intern. Med. 148, Jan. 1988).
(a) Test at the 0.01 level whether the true mean cholesterol levels for all
races in the United States during 1978–1980 are the same.
(b) If there is a difference, use Tukey’s method to find which of the races
are different with respect to the mean cholesterol levels.
10.6 CHAPTER SUMMARY
In this chapter, we have introduced the basic idea of analyzing various experimental
designs. In Section 10.3, we explained the one-way ANOVA for the hypothesis test-
ing problem for more than two means (different treatments being applied, or different
populations being sampled). The two-way ANOVA, having b blocks and k treat-
ments consisting of b blocks of k experimental units each, is discussed in
Section 10.5. We also describe one popular procedure called Tukey’s method for
completely randomized, one-factor design for multiple comparisons. We saw in
Chapter 9 that there are other possible designs, such as the Latin square design or
Taguchi methods. We refer to specialized books on experimental design (Hicks
and Turner) for more details on how to conduct ANOVA on such designs. In the final
section, we give some computational examples.
We now list some of the key definitions introduced in this chapter:
•
Completely randomized experimental design
•
Randomized block design
•
Studentized range distribution
•
Tukey-Kramer method
In this chapter, we also learned the following important concepts and procedures:
•
ANOVA procedure for two treatments
•
One-way ANOVA for k2 populations
•
One-way ANOVA procedure for k2 populations
•
Procedure to find (1–a) 100% confidence intervals for difference of means with
common sample size n; Tukey’s method
•
Computational procedure for randomized block design
10.7 COMPUTER EXAMPLES
Minitab, SPSS, SAS, and other statistical programming packages are especially use-
ful when we perform an ANOVA. As we have experienced in earlier sections, an
ANOVA computation is very tedious to complete by hand.
535
10.7 Computer Examples

P-values suggest that the
chemical is a significant factor
but the material is not.
10.7.1 EXAMPLES USING R
EXAMPLE 10.7.1 ONE-WAY ANOVA
The three following random samples in the table are independently obtained from three different nor-
mal populations with equal variances. At the a¼0.05 level of significance, test for equality of means.
Sample
64
84
75
77
80
56
74
69
81
92
84
Group
1
1
1
1
1
2
2
2
3
3
3
This example assumes you have stored the data into two variables sample and group. Please
modify your code appropriately.
R Code:
model¼lm(sampleas.factor(group));
Notice we must use as.factor() to
get the proper degrees of freedom.
anova(model);
Output:
Analysis of Variance Table
Response: sample
Df Sum
Sq Mean Sq F value Pr(>F)
as.factor(group) 2
560.67 280.333 4.8403
0.04192*
Residuals
8
463.33
57.917
—
Signif. codes:
0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
Since p-value is less than 0.05, we reject H0 of equal means.
EXAMPLE 10.7.2 TWO-WAY ANOVA
A furniture company wants to know whether there are differences in stain resistance among the four
chemicals used to treat three different fabrics. The following table shows the yields on resistance to
stain (a low value indicates good stain resistance). At the a¼0.05 level of significance, is there evi-
dence to conclude that there is a difference in mean resistance among the four chemicals? Is there
any difference in the mean resistance among the materials?
Chemical
1
2
3
4
1
2
3
4
1
2
3
4
Resistance
3
9
2
7
7
11
5
9
6
8
7
8
Material
1
1
1
1
2
2
2
2
3
3
3
3
This example assumes you have stored data into three variables chemical, resistance, and mate-
rial. Please modify your code appropriately.
R Code:
model¼lm(resistanceas.factor(chemical)+as.factor(material));
anova(model);
Output:
Analysis of Variance Table
Response: resistance
Df Sum Sq Mean Sq F value
Pr(>F)
as.factor(chemical)
3 43.667 14.5556
7.3803 0.01943 *
as.factor(material)
2 16.167
8.0833
4.0986 0.07548
Residuals
6 11.833
1.9722
—
Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
536
CHAPTER 10 Analysis of Variance

EXAMPLE 10.7.3
Tukey’s Method
The following table shows the one-year percentage total return of the top five stock funds for
five different categories (source: Money, July 2000). Which categories have similar top returns and
which are different? Use 95% Tukey’s confidence intervals. This example assumes you have stored
your data into “stocks” and “groups” variable that pair.
Large-cap
Mid-cap
Small-cap
Hybrid
Specialty
110.1
299.8
153.8
68.3
181.6
102.9
139.0
139.8
67.1
159.3
93.1
131.2
138.3
42.5
138.3
83.0
110.5
121.4
40.0
132.6
83.3
129.2
135.9
41.0
135.7
R Code:
groups¼c(rep(“Large-cap”,5),rep(“Mid-cap”,5),rep(“Small-
cap”,5),rep(“Hybrid”,5),rep(“Specialty”,5));
This assumes your “stocks”
variable is typed in column
by column, top to bottom.
model¼aov(stocksas.factor(groups));
TukeyHSD(model);
Output:
Tukey multiple comparisons of means
95% family-wise confidence level
Fit: aov(formula¼stocksas.factor(groups))
$‘as.factor(groups)‘
diff
lwr
upr
p adj
Large-cap-Hybrid
42.70
27.957536
113.35754
0.3963504
Mid-cap-Hybrid
110.16
39.502464
180.81754
0.0012546*
Small-cap-Hybrid
86.06
15.402464
156.71754
0.0124242*
Specialty-Hybrid
97.72
27.062464
168.37754
0.0041271*
Mid-cap-Large-cap
67.46
3.197536
138.11754
0.0657451
Small-cap-Large-cap
43.36
27.297536
114.01754
0.3816028
Specialty-Large-cap
55.02
15.637536
125.67754
0.1765264
Small-cap-Mid-cap
24.10
94.757536
46.55754
0.8429013
Specialty-Mid-cap
12.44
83.097536
58.21754
0.9835150
Specialty-Small-cap
11.66
58.997536
82.31754
0.9870429
This shows that mean returns for Hybrid to mid-cap, small-cap, and specialty are different.
10.7.2 MINITAB EXAMPLES
EXAMPLE 10.7.4
(One-way ANOVA) The three random samples in Table 10.11 are independently obtained from
three different normal populations with equal variances.
At the a¼0.05 level of significance, test for equality of means.
Solution
Enter sample 1 data in C1, sample 2 in C2, and sample 3 in C3.
Stat>ANOVA>One-way (unstacked) . . .>in Responses (in separate columns): type C1 C2 C3
and click OK
Continued
537
10.7 Computer Examples

We get the following output:
One-Way Analysis of Variance
Analysis of Variance
Source
DF
SS
MS
F
P
Factor
2
560.7
280.3
4.84
0.042
Error
8
463.3
57.9
Total
10
1024.0
Individual 95% CIs For Mean
Based on Pooled StDev
Level N
Mean
StDev -----+--------+---------+---------+--
C1
5
76.000
7.517
(-----*------)
C2
3
66.333
9.292
(-----*------)
C3
3
85.667
5.686
(-----*------)
----+---------+--------+---------+--
Pooled StDev ¼ 7.610 60 72 84 96
We can see that the output contains, SS, MS, individual column means, and standard deviation
values. Also, the F-value gives the value of the test statistic, and the p-value is obtained as 0.042.
Comparing this p-value of 0.042 with a¼0.05, we will reject the null hypothesis.
If we want to create side-by-side box plots to graphically test homogeneity of variances, we can
do the following.
Enter all the data (from all three samples) in C1, and enter the sample identifier number in C2
(that is, 1 if the data belong to sample 1, 2 for sample 2, and 3 for sample 3).
Graph>Boxplot>in Y column, type C1 and in X column, type C2>click OK
Then as in Example 10.3.2, interpret the resulting box plots.
EXAMPLE 10.7.5
Give Minitab steps for randomized block design for the data of Example 10.4.1.
Solution
To put the data into the format for Minitab, place all the data values in one column (say, C2). Let
numbers 1, 2, 3, 4 represent the chemicals and numbers 1, 2, 3 represent the fabric material. In one
column (say, C1) place numbers 1 through 4 with respect to the data values identifying the factor
(chemical) used. In another column (say, C3) place corresponding numbers 1 through 3 to identify
the second factor (material) used. See Table 10.12.
Then do the following:
Stat>ANOVA>Two-way. . .>in Response: type C2, in Row Factor: type C1, and in Column fac-
tor: type C3>OK
We will get the following output.
Table 10.11
Sample 1
Sample 2
Sample 3
64
56
81
84
74
92
75
69
84
77
80
538
CHAPTER 10 Analysis of Variance

Two-Way Analysis of Variance
Analysis of variance for Response
Source
DF
SS
MS
F
P
Chemical
3
43.67
14.56
7.38
0.019
Material
2
16.17
8.08
4.10
0.075
Error
6
11.83
1.97
Total
11
71.67
Note that the output contains p-values for the effect both of the chemicals and of the materials.
Because the p-value of 0.019 is less than a¼0.05, we reject the null hypothesis and conclude that
there is a difference in mean resistance among the four chemicals. For the materials, the p-value of
0.075 is greater than a¼0.05, so we cannot reject the null hypothesis and conclude that there is no
difference in the mean resistance among the materials.
EXAMPLE 10.7.6
Give the Minitab steps for using Tukey’s method for the data of Example 10.5.1.
Solution
In order to use Tukey’s method, it is necessary to enter the data in a particular way. Enter all the
data points in column C1; first five from large-cap, next five from mid-cap, and so on, with the last
five from specialty. In column C2, enter the number identifying the data points; the first four num-
bers are 1 (identifying 1 as the data belonging to large-cap), next five numbers are 2, and so on; the
last five numbers are 5. Then:
Stat>ANOVA>One-way. . .>Comparisons. . .>click Tukey’s, family error rate: and type 5 (to
represent 100a% error)>OK>in Response: type C1, and in Factor: type C2>OK
We will get the output similar to that given in the solution part of Example 10.5.1. For discussion of
the output, refer to Example 10.5.1.
Table 10.12
C1 Chemical
C2 Response
C3 Material
1
3
1
2
9
1
3
2
1
4
7
1
1
7
2
2
11
2
3
5
2
4
9
2
1
6
3
2
8
3
3
7
3
4
8
3
539
10.7 Computer Examples

10.7.3 SPSS EXAMPLES
EXAMPLE 10.7.7
Conduct a one-way ANOVA for the data of Example 10.7.1. Use a¼0.05 level of significance, and
test for equality of means.
Solution
In SPSS, we need to enter the data in a special way. First name column C1 as Sample, and column
C2 as Values. In the Sample column, enter the numbers to identify from which group the data comes.
In this case, enter 1 in the first five rows, 2 in the next three rows, and 3 in the last three rows. In the
Values column, enter sample 1 data in the first five rows, sample 2 data in the next five rows, and
sample 3 data in the last three rows. Then:
Analyze>Compare Means>One-way ANOVA. . .>Bring Values to Dependent List: and Sample
to Factor: > OK
EXAMPLE 10.7.8
Give the SPSS steps for using Tukey’s method for the data of Example 10.5.1.
Solution
First name column C1 as Fund and column C2 as Return. In the Fund column, enter the numbers to
identify from which group the data comes. In this case, the first four numbers are 1 (identifying 1 as
the data belonging to large-cap), the next four numbers are 2, and so on, until the last four numbers
are 5. In the Return column, enter large-cap return data in the first four rows, mid-cap data in the
next four rows, and so on; the last four from speciality. Then:
Analyze>Compare Means>One-way ANOVA. . .>Bring Return to Dependent List: and Fund to
Factor: > Click Post-Hoc. . .>click Tukey>click Continue>OK
We will get the output as in Example 10.5.1.
Interpretation of output is given in Example 10.5.1. When the treatment effects are significant, as in
this example where the p-value is 0.001, the means must then be further examined to determine the
nature of the effects. There are procedures called post hoc tests to assist the researcher in this task.
For example, looking at the output column Sig., we could observe that there are significant differ-
ences in the mean returns between funds 2 and 4, and funds 4 and 5.
10.7.4 SAS EXAMPLES
EXAMPLE 10.7.9
Using SAS, conduct a one-way ANOVA for the data of Example 10.7.1. Use a¼0.05 level of sig-
nificance, and test for equality of means.
Solution
We could use the following code.
Options nodate nonumber;
options ls¼80 ps¼50;
DATA Scores;
INPUT Sample Value @@;
DATALINES;
540
CHAPTER 10 Analysis of Variance

1
64
1
84
1
75
1
77
1
80
2
56
2
74
2
69
3
81
3
92
3
84
;
PROC ANOVA DATA¼Scores;
TITLE ‘ANOVA for Scores’;
CLASS Sample;
MODEL Value¼Sample;
MEANS Sample;
RUN;
We could have used PROC GLM instead of PROC ANOVA to perform the ANOVA procedure.
Usually, PROC ANOVA is used when the sizes of the samples are equal; otherwise PROC GLM is
more desirable. The next example will show how to do the multiple comparison using Tukey’s
procedure.
EXAMPLE 10.7.10
Give the SAS commands for using Tukey’s method for the data of Example 10.5.1.
Solution
We could use the following code:
Options nodate nonumber;
options ls¼80 ps¼50;
DATA Mfundrtn;
INPUT Fund Return @@;
DATALINES;
1 110.1
2 299.8
3 153.8
4 68.3
5 181.6
1 102.9
2 139.0
3 139.8
4 67.1
5 159.3
1
93.1
2 131.2
3 138.3
4 42.5
5 138.3
1
83.3
2 129.2
3 135.9
4 41.0
5 135.7
1
83.0
2 110.5
3 121.4
4 40.0
5 132.6
;
PROC GLM DATA¼Mfundrtn;
TITLE ‘ANOVA for Mutual fund returns’;
CLASS Fund;
MODEL Return¼Fund;
MEANS Fund / tukey;
RUN;
ANOVA for Mutual fund returns
The GLM Procedure
Class Level Information
Class Levels Values
Fund
5
1 2 3 4 5
Number of observations 25
ANOVA for Mutual fund returns
The GLM Procedure
Dependent Variable: Return
Sum of
Source
DF
Squares
Mean Square
F Value
Pr > F
Model
4
41,243.69840
10,310.92460
7.40
0.0008
Continued
541
10.7 Computer Examples

Error
20
27,877.58000
1393.87900
Corrected Total 24 69,121.27840
R-Square
Coeff Var
Root MSE
Return Mean
0.596686
31.34524
37.33469
119.1080
Source
DF
Type I SS
Mean Square
F Value
Pr > F
Fund
4
41,243.69840
10,310.92460
7.40
0.0008
Source
DF
Type III SS
Mean Square
F Value
Pr > F
Fund
4
41,243.69840
10,310.92460
7.40
0.0008
ANOVA for Mutual fund returns
The GLM Procedure
Tukey’s Studentized Range (HSD) Test for Return
NOTE: This test controls the Type I experiment wise error rate, but it
generally has a higher Type II error rate than REGWQ.
Alpha
0.05
Error Degrees of Freedom
20
Error Mean Square
1393.879
Critical Value of Studentized Range
4.23186
Minimum Significant Difference
70.658
Means with the same letter are not significantly different.
Tukey Grouping
Mean
N
Fund
A
161.94
5
2
A
A
149.50
5
5
A
A
137.84
5
3
A
B
A
94.48
5
1
B
B
51.78
5
4
The GLM Procedure
Tukey’s Studentized Range (HSD) Test for Value
NOTE: This test controls the Type I experiment wise error rate, but it
generally has a higher Type II error rate than REGWQ.
Alpha
0.05
Error Degrees of Freedom
20
Error Mean Square
1393.879
Critical Value of Studentized Range
4.23186
Minimum Significant Difference
70.658
Means with the same letter are not significantly different.
TurkeyGrouping
Mean
N
Sample
A
161.94
5
2
A
A
149.50
5
5
A
A
137.84
5
3
A
B
A
94.48
5
1
B
B
51.78
5
4
Looking at the p-value of 0.008, which is less than a¼0.05, we conclude that there is a differ-
ence in mutual fund returns.
In the previous example, we used the post hoc test Tukey. We could have used other options such
as DUNCAN, SNK, LSD, and SCHEFFE. The test is performed at the default value of a¼0.05. If we
542
CHAPTER 10 Analysis of Variance

want to specify, say, a¼0.01, or 0.1, we could have done so by using the command MEANS Fund /
Tuckey ALPHA¼0.01;.
If we need all the confidence intervals in the Tukey method, in the code just given, we have to
modify ‘MEANS Fund / Tukey;’ to ‘MEANS Fund / LSD TUKEY CLDIFF;’ which will result in the
following output.
ANOVA for Mutual fund returns
The GLM Procedure
Class Level Information
Class levels Values
Fund
5
1
2
3
4
5
Number of observations 25
ANOVA for Mutual fund returns
The GLM Procedure
Dependent Variable: Return
Sum of
Source
DF
Squares
Mean Square
F Value
Pr > F
Model
4
41,243.69840
10,310.92460
7.40
0.0008
Error
20
27,877.58000
1393.87900
Corrected Total 24
69,121.27840
R-Square
Coeff Var
Root MSE
Return Mean
0.596686
31.34524
37.33469
119.1080
Source
DF
Type I SS
Mean Square
F Value
Pr > F
Fund
4
41,243.69840
10,310.92460
7.40
0.0008
Source
DF
Type III SS
Mean Square
F Value
Pr > F
Fund
4
41,243.69840
10,310.92460
7.40
0.0008
ANOVA for Mutual fund returns
The GLM Procedure
t-tests (LSD) for Return
NOTE: This test controls the Type I comparison wise error rate, not the
experiment wise error rate.
Alpha
0.05
Error Degrees of Freedom
20
Error Mean Square
1393.879
Critical Value of t
2.08596
Least Significant Difference
49.255
Comparisons significant at the 0.05 level are indicated by ***.
Difference
Fund
Between
95% Confidence
Comparison
Means
Limits
2 – 5
12.44
36.81
61.69
2 – 3
24.10
25.15
73.35
2 – 1
67.46
18.21
116.71
***
2 – 4
110.16
60.91
159.41
***
5 - 2
 12.44
61.69
36.81
5 – 3
11.66
37.59
60.91
5 – 1
55.02
5.77
104.27
***
5 – 4
97.72
48.47
146.97
***
3 - 2
24.10
73.35
25.15
3 - 5
11.66
60.91
37.59
3 – 1
43.36
5.89
92.61
3 – 4
86.06
36.81
135.31
***
1 – 2
67.46
116.71
18.21
***
Continued
543
10.7 Computer Examples

1 – 5
55.02
104.27
5.77
***
1 – 3
43.36
92.61
5.89
1 – 4
42.70
6.55
91.95
4 – 2
110.16
159.41
60.91
***
4 – 5
97.72
146.97
48.47
***
4 – 3
86.06
135.31
36.81
***
4 – 1
42.70
91.95
6.55
ANOVA for Mutual fund returns
The GLM Procedure
Tukey’s Studentized Range (HSD) Test for Return
NOTE: This test controls the Type I experiment wise error rate.
Alpha
0.05
Error Degrees of Freedom
20
Error Mean Square
1393.879
Critical Value of Studentized Range
4.23186
Least Significant Difference
70.658
Comparisons significant at the 0.05 level are indicated by ***.
Difference
Fund
Between
Simultaneous 95%
Comparison
Means
Confidence Limits
2 – 5
12.44
58.22
83.10
2 – 3
24.10
46.56
94.76
2 – 1
67.46
3.20
138.12
2 – 4
110.16
39.50
180.82
***
5 – 2
12.44
83.10
58.22
5 – 3
11.66
59.00
82.32
5 – 1
55.02
15.64
125.68
5 – 4
97.72
27.06
168.38
***
3 – 2
24.10
94.76
46.56
3 – 5
11.66
82.32
59.00
3 – 1
43.36
27.30
114.02
3 – 4
86.06
15.40
156.72
***
1 – 2
67.46
138.12
3.20
1 – 5
55.02
125.68
15.64
1 – 3
43.36
114.02
27.30
1 – 4
42.70
27.96
113.36
4 – 2
110.16
180.82
39.50
***
4 – 5
97.72
168.38
27.06
***
4 – 3
86.06
156.72
15.40
***
4 – 1
42.70
113.36
27.96
EXERCISES 10.7
10.7.1. For the data of Exercise 10.5.4, perform a one-way ANOVAusing any of the
software (R, Minitab, SPSS, or SAS).
10.7.2. For the data of Exercise 10.5.2, perform Tukey’s test using any of the
software (R, Minitab, SPSS, or SAS).
10.7.3. For the data of Exercise 10.5.4, perform Tukey’s test using any of the
software (R, Minitab, SPSS, or SAS).
544
CHAPTER 10 Analysis of Variance

PROJECTS FOR CHAPTER 10
10A. TRANSFORMATIONS
The basic model for the ANOVA requires that the independent observations come
from normal populations with equal variances. These requirements are rarely met
in practice, and the extent to which they are violated affects the validity of the sub-
sequent inference. Therefore, it is important for the investigator to decide whether
the assumptions are at least approximately satisfied and, if not, what can be done
to rectify the situation. Hence it is necessary to (a) examine the data for marked
departures from the model and, if necessary, (b) apply an appropriate transformation
to the data to bring it more in line with the basic assumptions.
A simple way to check for the equality of the population variances is to calculate
the sample variances and plot against mean as in Figure 10.3. If the graph suggests a
relation between SM and variance, then the relation very likely exists between pop-
ulation mean and variance, and hence the population from which the samples are
taken may very well be nonnormal.
If a study of SMs and variances reveals a marked departure from the model, the
observations maybe transformed into a new set to which the methods of ANOVA are
better suited. Three commonly used transformations are the following:
(a) The logarithmic transformation: If the graph of SMs against sample variance
suggests a relation of the form
s2 ¼ C X
2


,
replace each observation X by its logarithm to the base 10,
Y ¼ log10 X;
or, if some X-values are zero, by Y¼log10 (X+1).
(b) The square root transformation: If the relation is of the form
s2 ¼ CX
replace X by its square root,
Y ¼
ﬃﬃﬃﬃ
X
p
or, if the values of X are very close to zero, by the square root of (X+½). This
relation is found in data from Poisson populations, where the variance is equal to
the mean.
(c) The angular transformation: If the observations are counts of a binomial
nature, and ^p is the observed proportion, replace ^p by
y ¼ arcsin
ﬃﬃﬃ
^p
p
,
which is the principal angle (in degrees or radians) whose sine is the square root of ^p.
(i) To check for the equality of the population variances, calculate the sample
variances for each of the data sets given in the exercises of Section 10.3 and
plot against the corresponding mean.
(ii) If there is assumption violation, perform one of the transformations
described earlier and do the ANOVA procedure for the transformed data.
545
Projects for Chapter 10

10B. ANOVA WITH MISSING OBSERVATIONS
In the two-way ANOVA, we assumed that each block cell has one treatment value.
However, it is possible that some observations in some block cells may be missing
for various reasons, such as that the investigator failed to record the observations, the
subject discontinued participation in the experiment, or the subject moved to a dif-
ferent place or died prior to completion of the experiment. In those cases, this project
gives a method of inserting estimates of the missing values.
Let y.. denote the total of all kb observations. If the observation corresponding to
the ith row and the jth column, which is denoted by yij., is missing, then all the sums
of squares are calculated as before, except that the yij term is replaced by
^yij ¼
bB0
j + kT0
i y0 ..
k 1
ð
Þ b1
ð
Þ ,
where T0i denotes the total of b–1 observations in the ith row, B0j denotes the total of
k–1 observations in the jth column, and y0.. denotes the sum of all kb–1 observa-
tions. Using calculus, one can show that ^yij minimizes the error sum of squares.
One should not include these estimates when computing relevant degrees of free-
dom. With these changes, proceed to perform the analysis as in Section 10.4. For
more details on the method, refer to Sahai and Ageel (2000), p. 145.
Perform the test of Example 10.4.1, now with a missing value for material III and
chemical C4. Does the conclusion change?
10C. ANOVA IN LINEAR MODELS
In order to determine whether the multiple regression model introduced in
Section 8.5 is adequate for predicting values of dependent variable y, one can use
the ANOVA F-test. The model is
Y ¼ b0 + b1x1 + b2x2 +  + bkxk + e,
where e¼(e1, e2,. . .,en)N (0, s2) and ei and ej are uncorrected if i6¼j. Define the
multiple coefficient of determination, R2, by
R2 ¼ 1
X
yi  ^yi
ð
Þ2
X
yi y
ð
Þ2 :
The ANOVA F-Test
H0 : b1 ¼ b2 ¼  ¼ bk ¼ 0 versus
Ha : At least one of theparameters, b1,b2, ..., bk, differs from 0:
Test statistic:
F ¼ Mean square formodel
Mean square forerror
¼
SS model
ð
Þ=k
SSE= n k + 1
ð
Þ
½

¼
R2=k
1R2


= n k + 1
ð
Þ
½
,
546
CHAPTER 10 Analysis of Variance

where
n ¼ number of observations
k ¼ number of parameters in the model excluding b0:
From the F-table, determine the value of Fa with k numerator d.f. and n–(k+1)
denominator d.f. Then the rejection region is {F>Fa}.
If we reject the null hypothesis, then the model can be taken as useful in predict-
ing values of y.
For the data of Example 8.5.1, test the overall utility of the fitted model
y ¼ 66:120:3794X1 + 21:4365X2
using the F-test described earlier.
547
Projects for Chapter 10

CHAPTER
Bayesian Estimation
Inference
11
CHAPTER CONTENTS
11.1 Introduction .................................................................................................. 550
11.2 Bayesian Point Estimation .............................................................................. 552
11.3 Bayesian Confidence Interval or Credible Intervals .......................................... 568
11.4 Bayesian Hypothesis Testing .......................................................................... 573
11.5 Bayesian Decision Theory .............................................................................. 576
11.6 Chapter Summary .......................................................................................... 583
11.7 Computer Examples ....................................................................................... 584
Projects for Chapter 11 .......................................................................................... 587
OBJECTIVE
To study Bayesian analysis methods and procedures that are becoming very popular
in building statistical models for real-world problems.
The Reverend Thomas Bayes
(Source: http://en.wikipedia.org/wiki/Thomas_Bayes)
The Reverend Thomas Bayes (1702-1761) was a Nonconformist minister. In the
1720s Bayes started working on the theory of probability. Even though he did not
publish any of his works on mathematics during his lifetime, Bayes was elected a
Fellow of the Royal Society in 1742. His famous work titled “Essay toward solving
a problem in the doctrine of chances” was published in the Philosophical
Mathematical Statistics with Applications in R
Copyright © 2015 Elsevier Inc. All rights reserved.
549

Transactions of the Royal Society of London in 1764, after his death. The paper was
sent to the Royal Society by Richard Price, a friend of Bayes. Another mathematical
publication on asymptotic series also appeared after his death.
11.1 INTRODUCTION
Bayesian procedures are becoming increasingly popular in building statistical
models for real-world problems. In recent years, the Bayesian statistical methods
have been increasingly used in scientific fields ranging from archeology to comput-
ing. Bayesian inference is a method of analysis that combines information collected
from experimental data with the knowledge one has prior to performing the exper-
iment. Bayesian and classical (frequentist) methods take basically different outlooks
toward statistical inference. In this approach to statistics, the uncertainties are
expressed in terms of probabilities. In the Bayesian approach, we combine any
new information that is available with the prior information we have, to form the
basis for the statistical procedure. The classical approach to statistical inference that
we have studied so far is based on the random sample alone. That is, if a probability
distribution depends on a set of parameters y, the classical approach makes infer-
ences about y solely on the basis of a sample X1, . . ., Xn. This approach to inference
is based on the concept of a sampling distribution. To correctly interpret traditional
inferential procedures, it is necessary to fully understand the notion of a sampling
distribution. In this approach, we analyze only one set of sample values. However,
we have to imagine what could happen if we drew a large number of random samples
from the population. For example, consider a normal sample with known variance.
We have seen that a 95% confidence interval for the population mean m is given by
the random interval X 1:96s=
ﬃﬃﬃn
p , X + 1:96s=
ﬃﬃﬃn
p


: This means that when samples
are repeatedly taken from the population, at least 95% of the random intervals con-
tain the true mean m. The classical inferential approach does not use any of the prior
information we might have as a result of, say, our familiarity with the problem, or
information from earlier studies. Scientists and engineers are faced with the problem
that there is typically only a single data set, and they need to determine the value of
the parameter at the time the data are taken. The basic question then is, “What is the
best estimate of a parameter one can make from the data using one’s prior informa-
tion?” Statistical approaches that use prior knowledge, possibly subjective, in addi-
tion to the sample evidence to estimate the population parameters are known as
Bayesian methods.
Bayesian statistics provides a natural method for updating uncertainty in the light
of evidence. Data are still assumed to come from a distribution belonging to a known
parametric family. However, the Bayesian outlook toward inference is founded on
the subjective interpretation of probability. Subjective probability is a way of stating
our belief in the validity of a random event. The following example will illustrate the
idea. Suppose we are interested in the proportion of all undergraduate students at a
particular university who take on out-of-campus jobs for at least 20 hours a week.
550
CHAPTER 11 Bayesian Estimation Inference

Suppose we randomly select, say, 50 students from this university and obtain the pro-
portion of students who have out-of-campus jobs for at least 20 hours a week. Let us
assume that the sample proportion is 30/50¼0.6. In a frequentist approach, all of the
inferential procedures, such as point estimation, interval estimation, or hypothesis
testing, are based on the sampling distribution.
That is, even though we are analyzing only one data set, it is necessary to have the
knowledge of the mean, standard deviation, and shape of this sampling distribution
of the proportion for the correct interpretation in classical inferential procedures. In
the subjective interpretation of probability, the proportion of undergraduates who
work on an out-of-campus job for at least 20 hours a week is assumed to be unknown
and random. A probability distribution, called the prior, that represents our knowl-
edge or belief about the location of this proportion before any data collected is used.
For instance, the college placement office already may have an opinion on this pro-
portion based on its earlier experience. The classical approach ignores this prior
knowledge, whereas the Bayesian approach incorporates this knowledge with the
current observed data to update the value of this proportion. That is, after the data
are collected our opinion about the proportion may change. Using Bayes’ rule, we
will compute the posterior probability distribution for the proportion, based on
our prior belief and evidence from the data. All of our inferences about the proportion
are made by computing appropriate statistics of the posterior distribution.
The Bayesian approach seeks to optimally merge information from two sources:
(1) knowledge that is known from theory or opinion formed at the beginning of the
research in the form of a prior, and (2) information contained in the data in the form
of likelihood functions. Basically, the prior distribution represents our initial belief,
whereas the information in the data is expressed by the likelihood function. Combin-
ing prior distribution and likelihood function, we can obtain the posterior distribu-
tion. This expresses our revised uncertainty in light of the data. The main
difference between the Bayesian approach and the classical approach is that in
the Bayesian setting, the parameter is viewed as random variables, whereas the clas-
sical approach considers the parameter to be fixed but unknown. The parameter is
random in the sense that we can assign to it a subjective probability distribution that
describes our confidence about the actual value of the parameter.
Some of the reasons for Bayesian approaches are as follows: (1) Most Bayesian
inferential conclusions are made conditional on the observed data. Unlike the tradi-
tional approach, one need not be concerned with data sets other than the one that is
observed. There is no need to discuss sampling distributions using the Bayesian
approach. Also, (2) from a Bayesian viewpoint, it is legitimate to talk about the prob-
ability that the proportion falls in a specific interval, say (0.2, 0.6), or the probability
that a hypothesis is true. Too often, traditional inferential conclusions are misstated;
for example, if a confidence interval computed from a sample for a parameter is (0.2,
0.6), it is common for the student to incorrectly state that the population parameter
falls in the interval (0.2, 0.6) with probability at least 0.90. The Bayesian viewpoint
provides a convenient model for implementing the scientific method. The prior prob-
ability distribution can be used to state initial beliefs about the population of interest,
551
11.1 Introduction

relevant sample data are collected, and the posterior probability distribution reflects
one’s new updated beliefs about the population parameter in light of the new data that
were collected. All inferences about the parameter are made by computing appropri-
ate summaries of the posterior probability distribution. Because of formidable the-
oretical and computational challenges, the Bayesian approach has found relatively
limited use. Recent advances in Bayesian analysis combined with the growing power
of computers are making Bayesian methods practical and increasingly popular. The
Markov chain Monte Carlo (MCMC) method described in Section 13.5 is one of the
computationally intensive methods that is often useful in Bayesian estimation.
11.2 BAYESIAN POINT ESTIMATION
The cornerstone of Bayesian methodology is the Bayes theorem. It helps us to update
our beliefs in the form of probability statements about the parameters after the sam-
ple has been taken. The conditional distribution of the parameters after observing the
data is called the posterior distribution that integrates the prior and the sample infor-
mation. Suppose we have two discrete random variables, X and Y. Then the joint
probability function (pmf) can be written as p(x, y)¼p(xjy)pY(y), and the marginal
probability density function of X is pX(x)¼P
yp(x,y)¼P
yp(xjy)pY(y). Then Bayes’
rule for the conditional p(yjx) is
p yjx
ð
Þ ¼ p x, y
ð
Þ
pX x
ð Þ ¼ p xjy
ð
ÞpY y
ð Þ
pX x
ð Þ
¼
p xjy
ð
ÞpY y
ð Þ
X
yp xjy
ð
ÞpY y
ð Þ
:
The denominator in this expression is a fixed normalizing factor that ensures that the
P
yp(yjx)¼1. If Y is continuous, the Bayes theorem can be stated as
p yjx
ð
Þ ¼
p xjy
ð
ÞpY y
ð Þ
ð
p xjy
ð
ÞpY y
ð Þdy
,
where the integral is over the range of values of y. These two equations are the Bayes
formulas for random variables.
In Bayesian terminology, pY(y) represents the probability statement of our prior
belief, p(xjy) is the probability of the data x given our prior beliefs, which is called the
likelihood, and the updated probability p(xjy) is the posterior. Because pX(x) (which
is the likelihood accumulated over all possible prior values) is independent of y, we
can express the posterior distribution as proportional (∝) to [(likelihood)(prior
distribution)], that is,
p yjx
ð
Þ∝p xjy
ð
Þp y
ð Þ:
We use the notation f(xjy) to represent a probability distribution whose population
parameter is considered to be a random variable. Now one of the problems is of find-
ing a point estimate of the parameter y (possibly a vector) for the population with
distribution f(xjy), given y. Since y is assumed to be a random variable, we can talk
552
CHAPTER 11 Bayesian Estimation Inference

of the distribution of y. Assume that p(y) is the prior distribution of y, which reflect
the experimenter’s prior belief about y. We will not distinguish between the scalars
and vectors, which will be clear based on the specific situation. Suppose that we have
a random sample X¼(X1, . . ., Xn) of size n from f(xjy). Then the posterior distribution
of y can be written as
f yjX1, ...,Xn
ð
Þ ¼f y, X1, ..., Xn
ð
Þ
f X1, ..., Xn
ð
Þ ¼L X1, ...,Xnjy
ð
Þp y
ð Þ
f X1, ..., Xn
ð
Þ
,
where L(X1, . . ., Xnjy) is the likelihood function. Letting C represent all terms that do
not involve y (in this case, C¼1/f(X1, . . ., Xn)), we have
f yjX1, ...,Xn
ð
Þ ¼ CL X1, ...,Xnjy
ð
Þp y
ð Þ,
For specific sample values X1¼x1, X2¼x2, . . ., Xn¼xn, the foregoing equation can
be written in a compact form as
f yjx
ð
Þ∝f xjy
ð
Þp y
ð Þ, where x ¼ x1, x2, ..., xn
ð
Þ:
This can be expressed as
posterior distribution
ð
Þ∝prior distribution
ð
Þ likelihood
ð
Þ:
The full result including the normalization can be written as
posterior distribution
ð
Þ ¼
prior distribution
ð
Þ likelihood
ð
Þ
½
=
X
priorlikelihood
ð
Þ
h
i
where the denominator is a fixed normalizing factor obtained by the likelihood accu-
mulated over all possible prior values. We can now give a formal definition.
Definition 11.2.1 The distribution of y, given data x1, . . ., xn, is called the pos-
terior distribution, which is given by
p yjx
ð
Þ ¼ f xjy
ð
Þp y
ð Þ
g x
ð Þ
,
(11.1)
where g (x) is the marginal distribution of X. The Bayes estimate of the parameter y
is the posterior mean.
The marginal distribution g(x) can be calculated using the formula
g x
ð Þ ¼
X
y
f xjy
ð
Þp y
ð Þ,
in discrete case
ð1
1
f xjy
ð
Þp y
ð Þdy, in continuous case
8
>
<
>
:
where p(y) is the prior distribution of y. Here, the marginal distribution g (x) is also
called the predictive distribution of X, because it represents our current predictions of
the values of X taking into account both the uncertainty about the value of y and the
residual uncertainty about the random variable X when y is known.
In a Bayesian setting, all the information about y from the observed data and from
the prior knowledge is contained in the posterior distribution, p(yjx). In almost all
practical cases, because we are combining our prior information with the information
contained in the data, the posterior distribution provides a more refined estimation
553
11.2 Bayesian Point Estimation

of y than the prior. All inferences from Bayesian methods are based on the posterior
probability distribution of the parameter y. Using the explanation given later, we will
take the Bayes estimate of a parameter as the posterior mean.
Furthermore, consider a Bayesian statistical inference problem where the param-
eter is a population proportion. In the Bernoulli trials, the population contains two
types called “successes” and “failures.” The proportion of successes in the popula-
tion is denoted by y. We take a random sample of size n from the population and
observe s successes and f failures. The goal is to learn about the unknown proportion
y on the basis of these data.
In this situation, a model is represented by the population proportion y. We do not
know its value. In Chapter 5, we have seen that we could use the maximum likelihood
estimator (MLE) for estimating y, which did not use any prior knowledge we may
have about y. Note that the maximum likelihood estimate is broadly equivalent to
finding the mode of the likelihood. In a Bayesian setting, we represent our beliefs
about location of y in terms of a prior probability distribution. We introduce propor-
tion inference by using a discrete prior distribution for y. We can construct a prior by
specifying a list of possible values for the proportion y, and then assigning probabil-
ities to these values that reflect our knowledge about y. Then the posterior probabil-
ities can be computed using the Bayes theorem. The following example illustrates
this concept.
EXAMPLE 11.2.1
It is believed that cross-fertilized plants produce taller offspring than the self-fertilized plants. In
order to obtain an estimate on the proportion y of cross-fertilized plants that are taller, an experi-
menter observes a random sample of 15 pairs of plants that are exactly the same age. Each pair
is grown in the same conditions with some cross-fertilized and the others self-fertilized. Based
on previous experience, the experimenter believes that the following are possible values of y and
that the prior probability for each value of y (prior weight) is p(y).
y : 0:80 0:82 0:84 0:86 0:88 0:90
p y
ð Þ : 0:13 0:15 0:22 0:25 0:15 0:10
From the experiment, it is observed that in 13 of 15 pairs, cross-fertilized is taller. Create a table with
columns of the prior p(y), likelihood of L(X1, X2, . . ., Xnjy) for different values of y and for the given
sample, prior times likelihood, and posterior probability of y. Based on the posterior probabilities,
what value of y has the highest support? Also, find E(y) based on the posterior probabilities.
Solution
The likelihood of obtaining 13 of 15 taller plants to the different prior values of p are given using the
binomial pdf
15
13


y13 1y
ð
Þ2. For example, if the prior value of y is 0.80, then the likelihood of y
given in the sample is
f xjy
ð
Þ ¼
15
13


0:8
ð
Þ13 0:2
ð
Þ2 ¼ 0:2309:
From Table 11.1 we obtain P(priorlikelihood)¼0.27217. Hence, the normalized value cor-
responding to y¼0.80 is the posterior probability f(yjx), which is equal to (0.030017/0.27217)¼
0.11029. Now, we can obtain the table of posterior distribution of a proportion p using the discrete
554
CHAPTER 11 Bayesian Estimation Inference

prior given in Table 11.1. When we substitute in Bayes’ rule, the factor
15
13


would be canceled.
Hence, in the calculation of the likelihood function, we could have just used y13(1y)2 instead of the
full expression
15
13


y13 1y
ð
Þ2.
Thus, the Bayesian estimate of y is
E y
ð Þ¼ 0:8
ð
Þ 0:11029
ð
Þ + 0:82
ð
Þ 0:14028
ð
Þ + 0:84
ð
Þ 0:22528
ð
Þ
+ 0:86
ð
Þ 0:2661
ð
Þ + 0:88
ð
Þ 0:15817
ð
Þ + 0:9
ð
Þ 0:098065
ð
Þ
¼ 0:84879  0:85:
It may be noted that the MLE of y is 13/15¼0.867.
In Example 11.2.1, the priors are called informative priors, because it favored certain
values of y; for example for the value y¼0.86, the prior value of p (y) is 0.25, which
is higher than all the rest of the values. If there was no information or no strong prior
opinions, then we could select a noninformative prior, which would have assigned
equal prior probability of 1/6 to each of the possible values of y. A noninformative
prior (also called a flat or uniform prior) provides little or no information. Based on
the situation, noninformative priors may be quite disperse, may avoid only impos-
sible values of the parameter, and oftentimes give results similar to those obtained
by classical frequentist methods.
EXAMPLE 11.2.2
Repeat the Example 11.2.1 using a noninformative prior, p(y)¼1/6, for each given value of y.
Solution
Here p(y)¼1/6 for each value of y. See Table 11.2.
The Bayesian estimate for the noninformative prior is
E y
ð Þ ¼ 0:8
ð
Þ 0:14333
ð
Þ + 0:82
ð
Þ 0:16003
ð
Þ + 0:84
ð
Þ 0:173
ð
Þ
+ 0:86
ð
Þ 0:17982
ð
Þ + 0:88
ð
Þ 0:17815
ð
Þ
+ 0:9
ð
Þ 0:16567
ð
Þ ¼ 0:85173:
Continued
Table 11.1 Summary of Prior and Posterior Probabilities
Prior
Values
of u
Prior
Probability
p(u)
Likelihood of u
Given Sample
Prior Times
Likelihood
Posterior
Probability
of u
0.80
0.13
0.2309
3.0017102
0.11029
0.82
0.15
0.2578
0.03867
0.14208
0.84
0.22
0.2787
6.1314102
0.22528
0.86
0.25
0.2897
7.2425102
0.2661
0.88
0.15
0.2870
0.4305
0.15817
0.90
0.10
0.2669
0.02669
0.098064
Total 0.27217
0.99981.0
555
11.2 Bayesian Point Estimation

It should be noted that because the choice of priors in Example 11.2.1 is only
mildly informative, we do not see much difference in the values of Bayesian esti-
mates. In general, it is difficult to construct an acceptable prior, because most often
it has to be based on subjective experiences. Therefore, it is relatively easy to use a
“noninformative” prior. For example, if we have no information on the values of pro-
portion y, then one type of standard “noninformative” prior is to take the proportion y
as one of the equally spaced values 0, 0.1, 0.2,. . ., 0.9, 1. We can assign for each value
of y the same probability, p(y)¼1/11. This prior is convenient and may work rea-
sonably well when we do not have much data. It is fairly easy to construct a prior
when there exists considerable prior information about the proportion of interest.
The posterior distribution gives us information regarding the likelihood of values
of y given sample data. Then the question is how to use this information to estimate y.
Instead of having explicit probabilities, the prior may be given through an assumed
probability distribution. We illustrate the calculations involved to find the posterior
distribution in the following example.
EXAMPLE 11.2.3
Let X be a binomial random variable with parameters n and p. Assume that the prior distribution of p
is uniform on [0,1]. Find the posterior distribution, f(pjx).
Solution
Because X is binomial, the likelihood function is given by
f xjp
ð
Þ ¼
n
x
 
px 1p
ð
Þnx:
Because p is uniform on [0,1], p(p)¼1, 0p1.
Then the posterior distribution is given by
f pjx
ð
Þ∝f xjp
ð
Þp p
ð Þ ¼
n
x
 
px 1p
ð
Þnx, x ¼ 0,1, ...,n
which is the same as the likelihood.
Table 11.2 Prior and Posterior Probabilities with Noninformative Prior
Prior
Values
of u
Prior
Probability
p(u)
Likelihood of u
Given Sample
Prior Times
Likelihood
Posterior
Probability
of u
0.80
1/6
0.2309
3.8483102
0.14333
0.82
1/6
0.2578
4.2967102
0.16003
0.84
1/6
0.2787
0.04645
0.173
0.86
1/6
0.2897
4.8283102
0.17982
0.88
1/6
0.2870
4.7833102
0.17815
0.90
1/6
0.2669
4.4483102
0.16567
Total 0.2685
1.0
556
CHAPTER 11 Bayesian Estimation Inference

Note that in the previous example, the form of the pmf in both f(xjp) and f(pjx) are
same, however, in f(pjx), p is considered random and in f(xjp), p is not random. This
particular form of f(pjx) is also called beta-binomial distribution for p with param-
eters a¼x+1 and b¼nx+1. This example illustrates that if the prior is noninfor-
mative (uniform), then the posterior is essentially the likelihood function. In the case
where the prior and posterior are of the same functional form, we call it a conjugate
prior. Bayesian inference becomes simpler when the prior density has the same func-
tional form as the likelihood (which is the case for the conjugate prior) or when data
are an independent sample from an exponential family (such as normal, Poisson, or
binomial). Bayesian priors act just like adding pseudo observations to the data.
The following example demonstrates the method of finding posterior distribution
for a continuous random variable.
EXAMPLE 11.2.4
Suppose that X is a normal random variable with mean m and variance s2, where s2 is known and m is
unknown. Suppose that m behaves as a random variable whose probability distribution (prior) is p(m)
and is also normally distributed with mean mp and variance sp
2, both assumed to be known or esti-
mated. Find the posterior distribution f(mjx).
Solution
Using the Bayes theorem, we have
f mjx
ð
Þ ¼
f xjm
ð
Þp m
ð Þ
Ð
f xjm
ð
Þp m
ð Þdm
¼
1ﬃﬃﬃﬃﬃﬃ
2ps
p
e xm
ð
Þ2=2s2
1ﬃﬃﬃﬃﬃﬃﬃ
2psp
p
e mmp
ð
Þ
2=2s2
p
Ð
1ﬃﬃﬃﬃﬃﬃ
2ps
p
e xm
ð
Þ2=2s2
1ﬃﬃﬃﬃﬃﬃ
2ps
p
pe mmp
ð
Þ
2=2s2pdm
¼
1
2pssp
e

xm
ð
Þ2
2s2
+
mmp
ð
Þ2
2s2p
h
i
:
(11.2)
Consider the exponential term in (11.2), namely,
xm
ð
Þ2
2s2
+
mmp
ð
Þ
2
2s2p :
xm
ð
Þ2
2s2
+ mmp

2
2s2
p
¼ 1
2
xm
ð
Þ2
s2
+ mmp

2
s2
p
"
#
¼ 1
2
1
s2 + 1
s2
p
 
!
m2 2 mp
s2
p
+ x
s2
 
!
m +
x2
s2 +
m2
p
s2
p
 
!
"
#
¼ 1
2
s2
p + s2
s2s2
p
m2 2 mp
s2
p
+ x
s2
 
!
m +
x2
s2 +
m2
p
s2
p
 
!
"
#
¼ 1
2
s2
p + s2
s2s2
p
m2 2
s2s2
p
s2
p + s2
mp
s2
p
+ x
s2
 
!
m
"
+
s2s2
p
s2
p + s2
x2
s2 +
m2
p
s2
p
 
!#
¼ 1
2
s2
p + s2
s2s2
p
m2 2
s2
s2
p + s2 mp +
s2
p
s2
p + s2 x
 
!
"
m
+
s2
s2
p + s2 mp +
s2
p
s2
p + s2 x
 
!23
5
Continued
557
11.2 Bayesian Point Estimation

+ 1
2
s2
p + s2
s2s2
p
x2
s2 +
m2
p
s2
p

s2
p
s2
p + s2 x +
s2
s2
p + s2 mp
 
!2
2
4
3
5
¼ 1
2
s2
p + s2
s2s2
p
m
s2
s2
p + s2 mp +
s2
p
s2
p + s2 x
 
!
"
#2
+ eK,
where
eK ¼ 1
2
s2
p + s2
s2s2
p
x2
s2 +
m2
p
s2
p

s2
s2 + s2
p
mp +
s2
p
s2 + s2
p
x
 
!2
2
4
3
5:
From the foregoing derivation, we obtain
f mjx
ð
Þ ¼ Ke1
2
s2
p + s2
s2s2
p
m
s2
s2
p + s2 mp +
s2
p
s2
p + s2 x
 
!
"
#2
,
where K does not contain m.
This implies that the posterior density f(mjx) is the pdf of normal random variable with mean
s2
s2
p + s2 mp +
s2
p
s2
p + s2 x
 
!
and variance
s2s2
p
s2
p + s2 :
If we let tp ¼ 1
s2p and t ¼ 1
s2, then the posterior density can be rewritten as the pdf of normal ran-
dom variable with mean
1
tp + t tpmp + tx


and variance
1
tp + t:
As an example, suppose that mp¼100, sp¼15, and s¼10, x¼115. Then f(mjx) is the pdf of a
normal random variable with
Mean ¼
100
100 + 225 100
ð
Þ +
225
100 + 225 115
ð
Þ ¼ 110:4
and
Variance ¼ 100
ð
Þ 225
ð
Þ
100 + 225 ¼ 69:2:
11.2.1 CRITERIA FOR FINDING THE BAYESIAN ESTIMATE
In the Bayesian approach to parameter estimation, we use both the prior and obser-
vations. This leads to an estimation strategy based on the posterior distribution. How
do we know that the estimate thus obtained is “good”? To assess the quality of likely
estimators, we use a loss function L(y,a) that measures the loss incurred by using a as
an estimate of y. Here y is the parameter being estimated (in real-world problems it is
not known), and a is the estimate of y. Then the “optimal” or “best” estimate a ¼ ^y
is chosen so as to minimize the expected loss E L y, ^y


h
i
, where the expectation is
taken over y with respect to the posterior distribution f(yjx). Here we mention two
types of commonly used loss functions: quadratic and absolute error loss functions
and the resulting estimates.
558
CHAPTER 11 Bayesian Estimation Inference

(1) A quadratic (or squared error) loss function is of the form L(y,a)¼(ay)2. In
this case,
E L y, a
ð
Þ
½
 ¼
ð
L y,a
ð
Þf yjx1, ...,xn
ð
Þdy
¼
ð
ay
ð
Þ2f yjx1, ...,xn
ð
Þdy:
Differentiating with respect to a and equating to zero, we obtain
2
ð
ay
ð
Þf yjx1, ...,xn
ð
Þdy ¼ 0
This implies
a ¼
ð
yf yjx1, ...,xn
ð
Þdy:
This is the posterior mean (expected value) of y, E(yjx1, . . ., xn). Hence the quadratic
loss function is minimized by taking the estimate of y, that is, ^y, to be the posterior
mean. In previous examples in this section, we used this value as the estimate ^y. Note
that what the quadratic loss function displays is that if the estimate ^y and the true
parameter y are close to each other, the loss we expect is very small. Likewise, if
the difference is larger, the expected loss in estimating y with ^y is going to be large.
(2) An absolute error loss function is of the form L(y, a)¼jayj. In this case,
E L y, a
ð
Þ
½
 ¼
ð
L y,a
ð
Þf yjx1, ...,xn
ð
Þdy
¼
ða
y¼1
ay
ð
Þf yjx1, ...,xn
ð
Þdy
+
ð1
y¼a
ya
ð
Þf yjx1, ...,xn
ð
Þdy:
Differentiating with respect to a and equating to zero, we obtain
ða
y¼1
f yjx1, ...,xn
ð
Þdy
ð1
y¼a
f yjx1, ...,xn
ð
Þdy ¼ 0:
The minimum loss is attained when the values of both integrals are equal to 1/2. This
can be achieved by taking ^y to be the posterior median.
There are other loss functions such as the all or nothing (or 0-1) loss function
given by
L a, y
ð
Þ ¼ 1day ¼
0, if y ¼ a
1, otherwise
	
where d is the Kronecker Delta function. This loss function is used mostly when
values of y is assumed to be discrete. In this case, it can be shown that expected loss
is minimized when ^y is the maximum of the posterior (MAP) distribution, or
the mode.
The following can be considered as a general Bayesian procedure for point
parameter estimation.
559
11.2 Bayesian Point Estimation

BAYESIAN PARAMETER ESTIMATION PROCEDURE
1. Consider the unknown parameter y as a random variable.
2. Use a probability distribution (prior) to describe the uncertainty about the unknown parameter.
3. Update the parameter distribution using the Bayes theorem:
P yjData
ð
Þ∝P y
ð ÞP Datajy
ð
Þ,
that is,
posterior of y
ð
Þ∝prior of y
ð
Þ likelihood
ð
Þ:
4. The Bayes estimator of y is set to be the expected value of the posterior distribution P(y jData)
under quadratic loss function.
From the procedure of Bayesian estimation, it is clear that a bad choice of prior
may result in a bad estimate. Generally, if the priors are based on a previous and trust-
worthy sample, Bayesian estimation methods are desirable. A schematic figure of
steps involved in the Bayesian estimate is given in Figure 11.1.
In this chapter, we use only the quadratic loss function unless it is explicitly stated
otherwise. We also mention that this loss function is very popular because of its ana-
lytic tractability. We now derive Bayesian point estimates for some specific
distributions.
Whereas uniform priors are useful in the noninformative situations, the beta fam-
ily of distributions is one of the commonly taken informative priors. Distributions in
the beta family take values in the interval (0, 1). Recall that if Xbeta(a, b), then the
pdf of X is given by
f x
ð Þ ¼
G a + b
ð
Þ
G a
ð ÞG b
ð Þxa1 1x
ð
Þb1, 0  x < 1
0,
otherwise, a > 0, b > 0
	
:
The beta pdf can be written as
f x
ð Þ ¼ Cxa1 1x
ð
Þb1∝xa1 1x
ð
Þb1,
Prior  info, 
P(θ)
Likelihood
P(Data | θ)
Posterior
P(θ | Data)
Loss 
Function
Updated
FIGURE 11.1
Bayesian estimation procedure.
560
CHAPTER 11 Bayesian Estimation Inference

where C ¼ G a + b
ð
Þ
G a
ð ÞG b
ð Þ: We also know that
E X
ð Þ ¼
a
a + b, and Var X
ð Þ ¼
ab
a + b
ð
Þ2 a + b + 1
ð
Þ
:
When using beta prior, we will take number of successes as a1 and the number of
failures as b1.
EXAMPLE 11.2.5
Let X1, . . ., Xn be a sample from geometric distribution with parameter p, 0p1. Assume that the
prior distribution of p is beta with a¼4, and b¼4.
(a) Find the posterior distribution of p.
(b) Find the Bayes estimate under quadratic loss function.
Solution
(a) Because p is Beta(4,4), the prior density is
G 8
ð Þ
G 4
ð ÞG 4
ð Þp3 1p
ð
Þ3 ¼ 140p3 1p
ð
Þ3:
Because the r.v.’s Xi’s have geometric distribution with parameter p, the likelihood is given by
L X1, ...,Xnjy
ð
Þ ¼
Y
n
i¼1
p 1p
ð
Þxi1 ¼ pn 1p
ð
Þ
Pn
i¼1xin:
The product of the likelihood function and the prior is given by
pn 1p
ð
Þ
Pn
i¼1xin 140p3 1p
ð
Þ3
h
i
¼ 140pn + 3 1p
ð
Þ
Pn
i¼1xin + 3:
Because, (posterior of p)∝(prior of p).(likelihood), rewriting the normalizing constant in the
denominator of Equation (11.1) as C, and letting C1¼140C, the posterior distribution (because
a1¼n+3, and b1¼P
i¼1
n xin+3) is Beta n + 4,
X
n
i¼1
xi n + 4
 
!
:
(b) Recall that for a Beta(a, b) random variable, the mean is [a/(a+b)]. Because the Bayes estimate
is the posterior mean, the mean of Beta(n+4, P
i¼1
n xin+4) is
n + 4
Xn
i¼1xi n + 4
h
i
+ n + 4
ð
Þ
¼
n + 4
Xn
i¼1xi + 8
Note that for large n, the Bayes estimate is approximately n/P
i¼1
n xi, which is the MLE of p.
In general, for a Bernoulli random variable with unknown probability of success p in [0,1], the
usual conjugate prior is the beta distribution, where the parameters of the beta distribution are cho-
sen to reflect any prior information that we have.
We will follow the idea of the previous example in a binomial experiment of tossing a coin.
EXAMPLE 11.2.6
Suppose we are flipping a biased coin, where the probability of heads p could be any value between
0 and 1. Given a sequence of toss samples x1, . . ., xn, we want to estimate P (H)¼p. We may have
two sources of information: our prior belief, which we will express as a beta distribution, and the
Continued
561
11.2 Bayesian Point Estimation

data, which could come from counts of heads x in n¼20 independent flips of the coin, say x¼13.
Suppose that in six prior tosses, we observed three heads and three tails, which lead us to believe that
the value of p is near 0.5. Obtain the posterior distribution of p.
Solution
Here our prior belief or assumption can be written in terms of beta distribution as
p p
ð Þ ¼ G a + b
ð
Þ
G a
ð ÞG b
ð Þpa1 1p
ð
Þb1
where a¼4 and b¼4. That is (noting G(n)¼(n1)!)
p p
ð Þ ¼
7!
3!
ð Þ 3!
ð Þp3 1p
ð
Þ3:
Hence, p(p)∝p3(1p)3. Because the mean of a beta distribution is a/(a+b) and the variance is
ab/((a+b)2 (a+b+1)), for the prior,
Mean p
ð Þ ¼
4
4 + 4 ¼ 0:5,
and
Var p
ð Þ ¼
4
ð Þ 4
ð Þ
4 + 4
ð
Þ2 4 + 4 + 1
ð
Þ
¼ 0:028:
Let X denote the number of heads in 20 flips of this coin. Then X has a binomial distribution, and
the pmf is given by
f xjp
ð
Þ ¼
20
x


px 1p
ð
Þ20x, x ¼ 0,1, ...,20:
This we can write as
f xjp
ð
Þ∝px 1p
ð
Þ20x:
In the 20 flips we have observed 13 heads. Then fix x¼13, and we are interested in the like-
lihood, which is the relative value of the function at different values of p:
f 13jp,20
ð
Þ∝p13 1p
ð
Þ7:
The posterior probability of p, given x¼13, is
p pjx ¼ 13
ð
Þ∝f xjp
ð
Þp p
ð Þ
¼ p13 1p
ð
Þ2013


p3 1p
ð
Þ3
¼ p16 1p
ð
Þ10:
Thus, the posterior is a beta distribution with a¼17 and b¼11. Consequently, we can now
obtain the mean and variance of p as
Mean p
ð Þ ¼
17
17 + 11 ¼ 0:607
and
Var p
ð Þ ¼
17
ð
Þ 11
ð
Þ
17 + 11
ð
Þ2 17 + 11 + 1
ð
Þ
¼ 0:008:
Note that the prior was beta distribution with mean 0.5 and variance 0.028. Figure 11.2 gives
the prior and posterior densities.
Note that if we had ignored the prior and just took the point estimation, then the MLE of p is
MLE p
ð Þ ¼ ^p ¼ 13
20 ¼ 0:65: Compare this with the Bayesian estimate of p¼0.607. Because Beta(1,1) is
the Uniform [0, 1], the method of the previous example can be used for noninformative priors. The
562
CHAPTER 11 Bayesian Estimation Inference

method could also be used in many applications. For example, suppose p represents the proportion
of infected individuals in a population, and x is the number of infected individuals in a sample of size
n. Then with a noninformative prior, we can show that the posterior of p is Beta(x+1, nx+1). This
type of setting can be used for estimating the true proportion of infected individuals in the
population.
EXAMPLE 11.2.7
Suppose for the past million days we have been predicting whether the sun will rise the next morning
or not. Each evening we say that the sun will rise the next morning
^R
 
, and we were right (R) all
these days. Suppose on the 106 evenings we predicted that the sun will rise on the next day. What is
the probability that the sun will rise the next day?
Solution
The problem can be cast in the following table form.
1
2
. . .
106
106+1
^R
^R
. . .
^R
^R
R
R
. . .
R
?
P Rj^R


¼ 1 if we use the frequency method of estimation (for example the MLE). Let us now consider
the Bayes method. Suppose the prior is uniform on [0,1]. That is,
p p
ð Þ ¼
1, if 0  p  1
0, otherwise:
	
Continued
4.5
4
3.5
3
2.5
prior
posterior
P (X)
2
1.5
1
0.5
0 0
0.2
0.4
p
0.6
0.8
1
FIGURE 11.2
Prior and posterior distributions for the proportions.
563
11.2 Bayesian Point Estimation

Suppose we predict n times and we succeed x times. Then
f xjp
ð
Þ ¼
n
x
 
px 1p
ð
Þnx:
The joint pdf is given by
f x, p
ð
Þ¼ f xjp
ð
Þp p
ð Þ
¼
n
x
 
px 1p
ð
Þnx, x ¼ 0,1, ...,n; 0  p  1:
By the Bayes theorem, the posterior pdf p(pjx) is
p pjx
ð
Þ¼ f xjp
ð
Þp p
ð Þ
ð1
0
xjp
ð
Þp p
ð Þdp
¼ K n, x
ð
Þpx 1p
ð
Þnx, 0  p  1, 0  x  n,
which is a beta probability distribution. Recall that the beta density is given by
f y
ð Þ ¼
1
B a, b
ð
Þya1 1y
ð
Þb1
and E Y
ð Þ ¼
a
a + b: Thus,
E p pjx
ð
Þ
½
 ¼
x + 1
x + 1
ð
Þ + nx
ð
Þ + 1 ¼ x + 1
n + 2:
In our example, x¼106, n¼106, which implies that the posterior mean is given by
^pb ¼ 106 + 1
106 + 2  1:
EXAMPLE 11.2.8
Let X1, X2, . . ., Xn be N(m,s2) random variables with prior p(m) having N(m0,s0
2) distribution with
known s2.
(a) Obtain the posterior distribution of m.
(b) Suppose it is known from past experience that the weight loss for a particular combination of
diet and exercise program (if followed for a month) is normally distributed with mean 10 lb and
standard deviation of 2 lb. A random sample of five persons who went through this program for
a month produced the following weight loss in pounds:
14 8 11 7 11
What is the point estimate of the mean, m? Assume s2¼4.
Solution
(a) Because p(m)N(m0,s0
2),p(m)∝exp[(mm0)2/s0
2] and we omit the terms that do not depend on
m. We have from the data x¼(x1, . . ., xn), the likelihood function,
L x1, ...,xnjm
ð
Þ ¼ f xjm
ð
Þ∝
Y
n
i¼1
exp  xi m
ð
Þ2
2s2
(
)
¼ exp

X
n
i¼1
xi m
ð
Þ2j2s2
h
i
(
)
,
564
CHAPTER 11 Bayesian Estimation Inference

where m is determined by the posterior distribution. The product of the likelihood function and
the prior gives the posterior, which is obtained (after some algebra) as follows:
f mjx
ð
Þ∝p m
ð Þ∝exp  mm1
ð
Þ2=2s2
1
h
i
where
m1 ¼ n=s2
ð
Þx + 1=s2
0


m0
n
s2 + 1
s2
0
and
s2
1 ¼
1
n
s2 + 1
s2
0
:
Thus, the posterior distribution of m is N(m1,s1
2).
(b) Note that the sample mean x ¼ 10:2lb, and sample standard deviation s¼2.77 lb. Now from
part (a), the posterior distribution of m is normal with mean
m1 ¼ n=s2
ð
Þx + 1=s2
0


m0
n
s2 + 1
s2
0
¼ 5=22


10:2 + 1=22


10
5
22 + 1
22
¼ 10:167
and variance
s2
1 ¼
1
n
s2 + 1
s2
0
¼
1
5
22 + 1
22
¼ 0:66667:
Thus, the point estimate of m is the posterior mean, 10.167. Figure 11.3 represents
the prior and posterior densities of m.
Sometimes, the inverse of variance in the normal distribution is called the pre-
cision of the normal distribution and denoted by t¼1/s2. Also note that in part
(a) of the previous example, if the prior variance s0
2!1, then the prior flattens
out, p(m)∝c, a constant. This basically amounts to saying that prior information
on m decreases, that is, all m are equally probable. This corresponds to a noninforma-
tive prior. Also, in this case as s2
0 ! 1, s2
1 ! s2
n and m1 ! x. Hence, in the limit (i.e.
for noninformative priors), the posterior f(mjx) will have an N x,s2=n
ð
Þ distribution,
which is exactly the same inference as in classical statistics.
In Bayesian inference problems, one of the questions is, which will have rela-
tively more influence, prior or likelihood? As we observe a large amount of data,
it can be shown that the posterior distribution is almost exclusively determined by
the data. That is, asymptotically, observed data will have a larger influence compared
to the choice of prior, and thus the prior will be irrelevant. Hence, we can make the
following general observations. If the prior is noninformative and we have a large
data set, then we can expect that the likelihood will have greater influence. Whereas,
if we have a small data set and an informative prior, then the prior will have a larger
influence on the updated posterior distribution. Bayesian estimators are more
565
11.2 Bayesian Point Estimation

complicated to compute than calculating the maximum likelihood estimates in sim-
ple cases. However, in complex settings Bayesian statistics are often relatively easier
to compute.
One of the problems in using Bayesian analysis is choosing an appropriate prior.
There are no specific rules available for this purpose. For instance, the following
priors are commonly used in the literature. If data are in [0,1], we could use uniform
or beta distribution. If the data are in [0, 1), normal (with nonnegative and relatively
large m), gamma, or log-normal distributions are used. If the data are in (1, 1),
normal or t-distributions are commonly used.
EXERCISES 11.2
11.2.1. Suppose in a casino, two kinds of dice are used, one kind of which 98% are
fair, and 2% are loaded such that five comes up 60% of the time and
the rest of the numbers are equally probable. We pick a die at random
and roll it three times. We get three consecutive fives. What is the
probability that the dice is loaded?
11.2.2. It is believed that cross-fertilized plants produce taller offspring than
self-fertilized plants. In order to obtain an estimate on the proportion y
of cross-fertilized plants that are taller, an experimenter observes a random
sample of 15 pairs of plants exactly the same age, with each pair grown
in the same conditions with one cross-fertilized and the other self-
fertilized. Based on previous experience, the experimenter believes that
0.5
0.45
0.4
0.35
0.3
0.25
0.2
prior
posterior
0.15
0.1
0.05
0 4
6
8
10
m
P(m)
12
14
16
FIGURE 11.3
Prior and posterior densities of m.
566
CHAPTER 11 Bayesian Estimation Inference

the following are possible values of p and prior probabilities for each value
(prior weight), p(y):
y:
0.80
0.82
0.84
0.86
0.88
0.90
p(y):
0.03
0.40
0.22
0.15
0.15
0.05
From the experiment, it is observed that in 13 of 15 pairs, the cross-
fertilized is taller
(a) Create a table with columns for prior, likelihood of y given sample,
prior times likelihood, and posterior probability of y. Based on the
posterior probabilities, what value of y has the highest support? Also,
find E(y) based on the posterior probabilities.
(b) Redo part (a) with a completely noninformative prior, that is, take the
prior for the proportion y as one of the equally spaced values 0, 0.1,
0.2, . . ., 0.9, 1. Also assign for each value of y the same probability,
p(y)¼1/11.
(c) Calculate the MLE of y and compare it with the Bayesian estimate.
11.2.3. Consider the problem of estimating p in a binomial distribution. Let X be
number of successes in a sample of size n.
(a) Let the prior distribution of p be given by Beta(3,1), that is
p p
ð Þ ¼
3p2, 0 < p < 1
0,
otherwise:
	
Find the posterior distribution of p.
Hint : f xjp
ð
Þ ¼
n
x
 
px 1p
ð
Þnx, x ¼ 0,1,2, ...,n
0,
otherwise:
8
<
:
2
4
3
5
(b) Let the prior distribution of p be given by Beta(a,b) (that is,
p(p)∝pa1 (1p)b1). Find the posterior distribution of p.
11.2.4. A biased coin is tossed n times. Let xi be 1 if the ith toss is heads and 0 if it
is tails. Assume a noninformative prior, p(y)¼1, 0y1. Let t be the
number of heads obtained. Show that the posterior distribution of y is Beta
(t+1, nt+1).
11.2.5. Let X1, X2, . . ., Xn be exponential random variables with parameter l. Let
the prior p(l) be exponentially distributed with parameter m, which is a
fixed and known constant.
(a) Show that the posterior distribution of l is Gamma (n+1,
m+P
i¼1
n xi).
(b) Obtain the Bayes estimate of l.
11.2.6. Let X1, X2, . . ., Xn be Poisson random variables with parameter l. Assume
that l has a Gamma (a, b) prior.
(a) Compute the posterior distribution of l.
567
11.2 Bayesian Point Estimation

(b) Obtain the Bayes estimate of l.
(c) Compare the MLE of l with the Bayes estimate of l.
(d) Which of the two estimates is better? Why?
11.2.7. Let X1, X2, . . ., Xn be Poisson random variables with parameter l. Assume
that l has an exponential distribution with y¼1 prior.
(a) Compute the posterior distribution of l and show that it is Gamma
((P
i¼1
n xi+1),(n+1)).
(b) Find the Bayes estimate of l.
11.2.8. It is known that a certain disease has affected 10% of a population. In a
random sample of 50 patients typical of the disease group who are exposed
to a new treatment, we observe that 12 patients were hospitalized in a year.
Let m be the rate of population that need hospitalization. Assume that
m  Gamma 0:1, 2
ð
Þ and f xjm
ð
Þ  Poi 50m
ð
Þ:
Given that 0.24 is an observation from f(xjm), find the Bayesian
estimator of m (that is, obtain E(mjx)).
11.2.9. Let X1, . . ., Xn be an N(m, 2) random sample with prior p(m) having N(0, s2)
distribution with known s2. Obtain the posterior distribution of m.
11.2.10. Let X1, . . ., Xn be an N(m, 1) random sample with prior p(m) having the pdf
[1/p (1+m2)]. Show that the posterior
p mjx
ð
Þ∝exp n mx
ð
Þ2
2
(
)
1
1 + m2 :
11.3 BAYESIAN CONFIDENCE INTERVAL OR CREDIBLE
INTERVALS
In this section, we want to study the question, “Can we construct an interval where
we are confident that the interval contains the unknown true value of y?” We have
seen how in many situations it may be preferable to use an interval estimate instead of
a point estimate for a population parameter y. Such intervals in classical statistics
were called confidence intervals. We can extend the concept of interval estimation
to a Bayesian setting. The Bayesian analog of a confidence interval is called a cred-
ible interval and is defined as follows.
Definition 11.3.1 A 100(1a)% credible interval for y is an interval (a, b) such
that
p a  y  bjx1, ...,xn
ð
Þ  1a
ð
Þ
Here a is given as a small positive number between 0 and 1, and x1, . . ., xn are the
sample values.
Note that we read this definition backwards, that is, we are at least (1a) 100%
confident that the true value of y is between a and b, given the sampled information.
568
CHAPTER 11 Bayesian Estimation Inference

Because the conditional distribution of y given X1, . . ., Xn is actually a probability
distribution, it makes sense to talk about the probability that y is in the interval (a, b).
Once we have observed data, the credible interval is fixed while y is random. This is
incontrasttotheclassicalconfidenceintervalwheretheintervalisrandombutyisafixed
parameter. In the classical case, we would say, “In the long run, 100(1a)% of all such
intervals will contain the true parameter y.” In the Bayesian approach, we would say,
“The probability is at least (1a) that y lies within the specified interval (a, b).”
As in the classical case, it would be desirable to minimize the length of the cred-
ible interval. This entails choosing only those points with highest values in the pos-
terior density of f(yjx1, . . ., xn), as shown in Figure 11.4. This will be better
especially if the density is not symmetric.
Definition 11.3.1 can be rephrased as follows using the posterior distribution of y.
Definition 11.3.2 A100(1a)%credibleintervalforyisaninterval(a,b)suchthat
1.
Ð
a
bf(yjx1,. . .,xn)dy1a if y is continuous, and the posterior pdf of y is
f(yjx1, . . ., xn).
2.
X
b
f yjx1, ...,xn
ð
Þ  1a if y is discrete.
We will now give some examples for computing credible intervals.
EXAMPLE 11.3.1
Suppose X1, . . ., Xn is a random sample from N(m, s2) with s2¼4. Suppose the prior pdf of m is
N(0, 1), that is, p(m)N (0, 1). Find a 95% credible interval for m.
Solution
We have seen from Example 11.2.8 that the posterior distribution of m given x1, . . ., xn, is normally
distributed with
Mean ¼ 1
1 + 4
n
x
and
Variance ¼ 1
1 + n
4
:
Continued
1-a
q
a
b
0
f(q |x1..., xn)
a/2
FIGURE 11.4
Credible interval for y.
569
11.3 Bayesian Confidence Interval or Credible Intervals

Figure 11.5 represents the posterior distribution of m.
To find the 95% credible interval for m, we have to find two numbers a and b such that
p a  X  b
ð
Þ ¼ 0:95
where
X  N m ¼ x
1 + 4
n
,s2 ¼ 1
1 + n
4


:
We choose a to be b (b is positive). Using z-scores, we get (X is continuous),
p za=2 < m 1= 1 + 4
n




x
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1= 1 + n
4
ð
Þ
p
< za=2
 
!
¼ 1a
which can be rearranged as
p
1
1 + 4
n
x
1ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + n
4
p
za=2 < m < 1
1 + 4
n
x +
1ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + n
4
p
za=2
 
!
¼ 1a:
Thus, a 95% credible interval for m is
1
1 + 4
n
x
1ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + n
4
p
za=2,
1
1 + 4
n
x +
1ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + n
4
p
za=2
 
!
:
For convenience, we summarize this procedure in the following steps.
Bayesian Credible Interval Procedure
1. Consider y as a random variable with prior pdf (or pmf) p(y).
2. Update the prior distribution p(y) using the Bayes theorem. That is find the posterior distribution
of y by the formula
p yjdata
ð
Þ ¼
f datajy
ð
Þp y
ð Þ
ð
f datajy
ð
Þp y
ð Þdy
, if continuous
f datajy
ð
Þp y
ð Þ
X
f datajy
ð
Þp y
ð Þ
,
if discrete:
8
>
>
>
>
>
<
>
>
>
>
>
:
3. Find two numbers a and b such that
(1+ −)−1x−
n
4
1+ −n
4
1
Var =
m
p (m)
FIGURE 11.5
Posterior distribution of m.
570
CHAPTER 11 Bayesian Estimation Inference

ðb
a
p yjdata
ð
Þdy  1a, if continuous,
X
b
y¼a
p yjdata
ð
Þ  1a, if discrete:
Note: The numbers a and b are found such that
ða
1
p yjdata
ð
Þdy ¼ a=2, if continuous,
X
ya
p yjdata
ð
Þ ¼ a=2, if discrete:
And
ð1
b
p yjdata
ð
Þdy ¼ a=2, if continuous,
X
yb
p yjdata
ð
Þ ¼ a=2, if discrete:
4. The (1a)100% credible interval for y is the interval (a, b).
In the discrete case, an easy way of finding a credible interval of smallest length is
to arrange the values of y from most likely to least likely (that is, in the order of the
magnitude of the posterior probabilities), and then put values of y into the interval
until the cumulative posterior probability of the set exceeds (1a)100%. Such an
interval is called a highest posterior density (HPD) interval. It can be shown that
the HPD interval always exists, and it is unique, so long as for all intervals of prob-
ability (1a), the posterior density is never uniform in any interval of values of y.
EXAMPLE 11.3.2
For the data of Example 11.2.1, find a 90% credible interval for y.
Solution
Arranging the values of y from most likely to least likely, we have Table 11.3. Looking at the “cumu-
lative probability” column, we see that the probability that y is in the set {0.86, 0.84, 0.88, 0.82,
0.80} is 0.90192. So this set is a 90% probability (or credible) interval for y.
Table 11.3 Posterior and Cumulative Probability
Prior Values of u
Posterior Probability of u
Cumulative Probability
0.86
0.2661
0.2661
0.84
0.22528
0.49138
0.88
0.15817
0.64955
0.82
0.14208
0.79163
0.80
0.11029
0.90192
0.90
9.8064102
0.99984
571
11.3 Bayesian Confidence Interval or Credible Intervals

EXERCISES 11.3
11.3.1. (a) Suppose X1, . . ., Xn is a random sample from N(m, s2) with s2¼9.
Suppose the prior pdf of m is N(0,1); that is p(m)N(0,1). Find a 95%
credible interval for m.
(b) The following is a set of random data from a normal distribution with
variance 9.
0:92 1:05 5:53 3:64 4:47 2:60 0:71 3:66
1:38
3:87
7:42 1:76 0:01 2:69
1:54
3:97 1:34 1:63 1:24 4:78
Using the results of part (a), compute a 95% credible interval for m, interpret
its meaning, and state any assumptions you have made.
11.3.2. Suppose that a person believes that his last year’s weight was normally
distributed with mean of 165 lb and standard deviation of 5 lb. That is, the
prior pdf of m is N(165, 25), or p(m)N(165, 25). He expects his current
weight X is normally distributed with mean m and standard deviation 7 lb.
Following are 10 random measurements (in pounds) from this year.
176 165 180 172 175
179 166 177 184 183
Find a 95% credible interval for m.
11.3.3. It is known that a certain disease affects 10% of a population. In a random
sample of 50 patients in the disease group who are exposed to a new
treatment, we observe that 12 patients were hospitalized in a year. Let m be
the population rate that needs hospitalization in a year. Assume m has a
Gamma (0.1, 2) prior. Let mGamma (0.1,2) and f(xjm)Poi (50m). Given
that x¼0.24 is an observation of X, find 95% credible internal for m. Obtain
a Bayesian credible interval for m. (If X is the number of patients admitted in
a year, assume XPoi (50m), the Poisson approximation of the binomial.)
How can we improve on this estimate?
11.3.4. For an upcoming congressional election, suppose we want to estimate the
amount of support for a particular candidate in a district. By previous
experience and voter registration data, we can assume that the prior
distribution of the proportion of support, p, is a beta distribution with
m¼10, and b¼8 (i.e. p (p)Beta (10, 8)). We conducted a survey of 1000
randomly selected voters, of whom 600 support the candidate. Obtain a
95% credible interval for p. What will happen to the credible interval if we
reduce the confidence interval? What will happen to the 95% credible
interval if we increase the sample size?
11.3.5. It is recommended that the daily intake of sodium be 2400 mg per day.
From a previous study on a particular ethnic group, the prior distribution of
sodium intake is believed to be normal with mean 2700 mg and standard
deviation 250 mg. If a recent survey for this group resulted in a mean of
3000 mg and standard deviation of 300 mg, obtain a 95% credible interval
for the mean intake of sodium for this ethnic group.
572
CHAPTER 11 Bayesian Estimation Inference

11.3.6. Suppose we have a coin (not necessarily balanced) with p being the
probability of heads. Assume a uniform prior for p. Suppose in
20 tosses of this coin, we obtained 12 heads. Obtain a 90% credible
interval for p.
11.3.7. Suppose that in a particular telephone exchange, the number of
calls received per minute has a Poisson distribution with parameter l.
Assume an exponential prior for l with parameter 2. Suppose this
exchange had received 25 calls in 5 min. Obtain a 95% credible
interval for l.
11.4 BAYESIAN HYPOTHESIS TESTING
The Bayesian approach to hypothesis testing for simple hypotheses is pretty straight-
forward. Deciding between two hypotheses for a given set of data x reduces to com-
puting their posterior probabilities. If an explicit loss function is available, the Bayes
rule is chosen to minimize the expected value of the loss function with respect to the
posterior distribution. In the absence of a loss function, the probabilities of type I and
type II errors are of little interest to the Bayesian.
In the classical hypothesis testing, we test a null hypothesis (denoted by H0)
against an alternative hypothesis (denoted by H1 or Ha). The test procedure is based
on controlling the two types of errors—type I and type II. The classical test proce-
dures limit the type I error to a and minimize the type II error. If the type II error is
unacceptably high, it is reduced by increasing the sample size.
In the Bayesian approach, the problem of deciding between the null and alterna-
tive is rather straightforward. Consider the problem of hypothesis testing with
H0 : y 2 Y0 vs: H1 : y 2 Y1
(11.3)
where Y0, Y1 are subsets of the real line. Let X1, . . ., Xn be the sample from a pop-
ulation with pdf fy(x).
In the Bayesian hypothesis testing approach we compute the following posterior
probabilities:
a0 ¼ p y 2 Y0jx1, ...,xn
ð
Þ
(11.4)
and
a1 ¼ p y 2 Y1jx1, ...,xn
ð
Þ:
(11.5)
If a0>a1, we accept the null hypothesis, and if a0<a1, we reject the null hypothesis.
We now outline the Bayes hypothesis testing procedure for testing hypothesis (11.3).
Let p(y) be the prior. Also,
p0 ¼ p y 2 Y0
ð
Þ
and
p1 ¼ p y 2 Y1
ð
Þ
573
11.4 Bayesian Hypothesis Testing

Definition 11.4.1 The ratio p0/p1 is called the prior odds ratio. The ratio a0/a1
(see Equations (11.4) and (11.5)) is called the posterior odds ratio.
The posterior odds ratio is the ratio of the posterior probabilities, given the data,
of the null and alternate hypotheses. The posterior odds ratio will be used in decision
making for testing the hypotheses. We now compute a0 and a1 using the Bayes the-
orem. That is,
a0 ¼p y 2 Y0jx1, ...,xn
ð
Þ
¼
ð
Y0
f yjx1, ...,xn
ð
Þdy, if continuous
X
y2Y0
f yjx1, ...,xn
ð
Þ,
if discrete:
8
>
>
>
>
<
>
>
>
>
:
Similarly,
a1 ¼p y 2 Y1jx1, ...,xn
ð
Þ
¼
ð
Y1
f yjx1, ...,xn
ð
Þdy, if continuous
X
y2Y1
f yjx1, ...,xn
ð
Þ,
if discrete:
8
>
>
>
>
<
>
>
>
>
:
We reject H0 if the odds ratio (a0/a1)<1 and accept H0 if (a0/a1)>1.
This method of hypothesis testing is called Jeffreys’ hypothesis testing criterion.
It basically says that if the posterior odds ratio is greater than 1, we accept the null
hypothesis; otherwise, we reject the null in favor of the alternative hypothesis.
Because we cannot determine the probability of a single value in the continuous
variable case, it should be noted that for a simple null hypothesis of the form y equals
some specified value cannot be dealt with easily in the Bayesian framework. Hence,
unlike the classical framework, here we mostly deal with the composite hypotheses
for both null and alternative.
EXAMPLE 11.4.1
A student taking a standardized test is classified as gifted if he or she scores at least 100 out of a
possible score of 150. Otherwise the student is classified as not gifted. Suppose the prior distribution
of the scores of all students is a normal with mean 100 and standard deviation 15. It is believed that
scores will vary each time the student takes the test and that these scores can be modeled as a normal
distribution with mean m and variance 100. Suppose the student takes the test and scores 115. Test
the hypothesis that the student can be classified as a gifted student.
Solution
The hypothesis testing problem can be phrased as
H0 : y < 100 vs: Ha : y  100:
Referring to Example 11.2.8, we know that the posterior distribution f(yjx) is a normal with
mean 110.4 and variance 69.2. Because the prior is an N(100,225), we have p0¼P(y<100)¼
1/2 and p1¼P(y100)¼1/2.
574
CHAPTER 11 Bayesian Estimation Inference

We can now compute
a0 ¼ p y < 100jx ¼ 115
ð
Þ
¼ p y110:4
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
69:2
p
< 100110:4
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
69:2
p


¼ p z   10:4
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
69:2
p


¼ 0:106
and
a1 ¼ p y  100jx ¼ 115
ð
Þ
¼ 1p y < 100jx ¼ 115
ð
Þ
¼ 10:106 ¼ 0:894:
Thus, a0/a1¼(0.106/0.894)¼0.119<1, and we reject H0.
Bayesian Hypothesis Testing Procedure
To test H0: y 2Y0 versus H1: y 2Y1, where Y0 and Y1 are given sets:
1. Consider y as a random variable with prior distribution p(y).
2. Compute the posterior distribution f(yjx1,. . .,xn) of y given x1, . . ., xn, using Bayes’ theorem.
3. Compute a0 and a1 using the following formulas:
a0 ¼ p y 2 Y0jx1, ...,xn
ð
Þ
¼
ð
Y0
f yjx1, ...,xn
ð
Þdy, if continuous
X
y2Y0
f yjx1, ...,xn
ð
Þ,
if discrete
8
>
>
>
>
<
>
>
>
>
:
and
a1 ¼ p y 2 Y1jx1, ...,xn
ð
Þ
¼
ð
Y1
f yjx1, ...,xn
ð
Þdy, if continuous
X
y2Y1
f yjx1, ...,xn
ð
Þ,
if discrete:
8
>
>
>
>
<
>
>
>
>
:
4. Reject H0 if the posterior odds ratio, a0
a1 < 1: Otherwise accept.
In the foregoing procedure, we assume that P(y2Y0) and P(y2Y1) are both
greater than zero.
EXERCISES 11.4
11.4.1. The following is random data from a normal distribution with variance 9.
0:92 1:05 5:53 3:64 4:47 2:60 0:71 3:66
1:38
3:87
7:42 1:76 0:01 2:69
1:54
3:97 1:34 1:63 1:24 4:78
575
11.4 Bayesian Hypothesis Testing

(a) Test the hypothesis, H0: m0 versus Ha: m>0. Assume that the prior is
N(0, 4), so that m0 and m>0 are equally probable.
(b) Compare your decision with classical hypothesis testing, with a¼0.05.
11.4.2. (a) For the data of Exercise 11.3.2, using the Bayesian method, test the
hypothesis H0: m170 versus Ha: m>170.
(b) Compare your decision with classical hypothesis testing, with a¼0.05.
11.4.3. It is known that a certain disease affects 10% of a population. Of a random
sample of 50 patients in the disease group who are exposed to a new
treatment, we observe that 12 patients were hospitalized in a year. Let m be
the population rate that needs hospitalization in a year. Assume m has a
Gamma(0.1,2) prior. Let mGamma(0.1,2) and f(xjm)Poi(50m). Given
that x¼0.24 is an observation of X, test the hypothesis H0: p0.10 versus
Ha: p>0.10. (If X is the number of patients admitted in a year, assume
XPoi (50m), the Poisson approximation of the binomial.)
11.4.4. For an upcoming congressional election, suppose we want to estimate the
amount of support for a particular candidate in a district. By previous
experience and voter registration data, we can assume that the prior
distribution, the proportion of support, p, is a beta distribution with a¼10,
and b¼8 (i.e. p(p)Beta (10, 8)). We conducted a survey of 1000
randomly selected voters, of whom 600 support the candidate. Test the
hypothesis H0: p0.60 versus Ha: p<0.60.
11.4.5. For the data of Exercise 11.3.5, test the hypothesis H0: m2400 mg versus
Ha: m>2400 mg for this ethnic group.
11.4.6. Suppose we have a coin (not necessarily balanced) with p being the
probability of heads. Assume a uniform prior for p. Suppose in 20 tosses of
this coin, we obtained 12 heads. Test the hypothesis H0: p0.50 versus Ha:
p>0.50.
11.5 BAYESIAN DECISION THEORY
Bayesian methods in general are more concerned with problems of decision making
than with problems of inference. Decision theory, as the name implies, is concerned
with the problem of making decisions. Statistical decision theory is concerned with
optimal decision making under uncertainty or when statistical knowledge is available
only on some of the uncertainties involved in the decision problem. Uncertainty
could be about the true value related to the decision, or, uncertainty could be about
the actual state of the nature. Abraham Wald (1902-1950) laid the foundation for
statistical decision theory. Original works on the decision theory emerged out of
game theory considerations. Many books and articles have been written on the var-
ious aspects of decision theory. The Bayesian approach to the decision theory was
introduced by Leonard Jimmie Savage in 1954. In this section, we introduce the gen-
eral idea of decision theory. We basically deal with analytical procedures for the
decision-making process. This will involve selection of an optimum decision from
576
CHAPTER 11 Bayesian Estimation Inference

a choice of courses of action among two or more alternatives. The Bayesian decision
theory quantifies the trade-offs between different decisions using costs and probabil-
ities that accompany such decisions.
Consider, as an example, a company deciding whether or not to market a new
brand of toothpaste with a whitening agent. Clearly many factors will affect the deci-
sion (for example, the proportion of people who are likely to switch to the new brand,
and the likelihood of other competing companies introducing similar toothpastes).
These factors are generally unknown, but estimates can be obtained from statistical
investigations.
The classical statistical approach relies exclusively on the data obtained from
these statistical investigations, ignoring other relevant information such as the com-
pany’s past experiences in marketing similar products. Statistical decision theory
tries to combine other relevant information with the sample information to arrive
at the optimal decision. Therefore, a Bayesian setting seems to be more appropriate
for decision theory.
One piece of relevant information that decision theory considers is the possible
consequences of the decisions. Often these consequences can be quantified. That is,
the loss or gain of each decision can be expressed as a number (called the loss or
utility). A loss or utility to a decision maker is the effect of the interaction of two
factors: (1) the decision or action selected by the decision maker; and (2) the event
or state of the world that actually occurs. Classical statistics does not explicitly use a
loss function or a utility (payoff) function.
A second source of information that decision theory utilizes is the prior informa-
tion. Prior information could be based on past experiences of similar situations or on
expert opinion. We can follow the procedure explained next as a guideline for deci-
sion making.
General Decision Theory Procedure
1. Identify the objectives of the decision-making process.
2. Identify the set of actions and set of possible events (states of nature).
3. Assign probabilities to the occurrence of each possible state of nature (prior). If more observa-
tions are available, calculate the posterior probabilities to the occurrence of each possible state of
nature.
4. For each possible event, assign a numerical value to the anticipated payoff (or loss) of each
course of action.
5. Compute the expected value of the payoffs (utility or loss function). This could be done by either
using the prior probabilities if there are no observations, or using the posterior probabilities.
6. Select the optimum decision among the available alternative courses of action that maximizes
the expected value of the payoffs.
There are many other decision criterion available in literature. In this section, we
only consider the expected utility or loss function approach. We now consider an
example to illustrate the idea of statistical decision making.
577
11.5 Bayesian Decision Theory

EXAMPLE 11.5.1
Suppose you own a small stall at a flea market that is open only on weekends. If the weather is good,
you make a profit of $200, and if it is bad, you close your stall and you make no (zero) profit. How-
ever, you have the option of buying, from an insurance company, weather insurance that costs $75.
The company pays you $210 if the weather is bad. Suppose you believe that the probability of good
weather on a particular weekend is p. Compute the expected gain if you insure and if you do not.
What is the best course of action? Arrive at a decision.
Solution
From the information in the problem, we can obtain the utility gain or profit table shown in
Table 11.4, based on our decision to insure or not insure. Suppose that we model the state of weather
as good or bad by means of a random variable defined as follows.
y ¼
1, if the weather is good
0, if the weather is bad:
	
Suppose for our example we believe that during a particular weekend P(y¼1)¼p, and
P(y¼0)¼1p. This can be considered as prior information. The different values of y are called
states of nature. We assign (perhaps subjectively) a probability structure for the states of nature
defined by a prior distribution p(y). Now we can compute the expected gain when we insure and
when we do not.
Using the values in the table,
Expected gain given we insure ¼ 125
ð
Þp + 135
ð
Þ 1p
ð
Þ
¼ 13510p,
Expected gain when do not insure ¼ 200
ð
Þp + 0
ð Þ 1p
ð
Þ
¼ 200p:
Hence, insurance is preferable if
13510p > 200p
or
p < 135
210 ¼ 0:643:
That is, we should take the insurance if we believe the probability of good weather is less than 0.643.
In general the states of the nature are represented by y1, . . ., yn and the possible
decisions (actions) are represented by d1, . . ., dm. Let U (dj,yi) represent the net gain
when the true states of nature is yi and the decision dj is made. Then we can construct
the general utility table shown in Table 11.5.
Table 11.4 Weather Insurance
Weather
Parameter space!
Good
Bad
Decision space #D
(y1)
(y2)
Insurance (I)(d1)
$125 (200-75)
$135(210-75)
No insurance (NI)(d2)
$200
$0
578
CHAPTER 11 Bayesian Estimation Inference

In Bayesian decision theory, we assume a probability distribution on the states of
nature called the prior distribution. Using this probability distribution, we can find
the decision that maximizes the expected utility. That is, let the states of nature
be initially modeled by a random variable y with probability function p(y) such that
P(y¼yi)¼p(yi), i¼1, . . . ,n. Let U denote the utility. Then the expected utility for
decision dj is given by
E Ujdj


¼
X
n
i¼1
U dj, yi


p yi
ð Þ:
The optimal decision, called the Bayes decision, denoted by d*, is that which max-
imizes the expected utility. That is, d* satisfies the following equation:
max
dj
X
n
i¼1
U dj, yi


p yi
ð Þ ¼
X
n
i¼1
U d	, yi
ð
Þp yi
ð Þ:
This procedure is called the Bayes decision procedure with respect to the assumed or
given prior p(yi), i¼1, 2, . . ., n.
PROCEDURE TO FIND OPTIMAL DECISION
1. For each decision di, compute P
i1
n U(dj,yi)p(yi)
2. Find a decision d* from the decision space that maximizes the sum in step 1. This is the Bayes
decision.
In determining the Bayes decision, we have assumed a prior distribution p (y) for
the states of nature {yi}. Naturally the question arises: can there be information or
observations that will help us to determine p (y)?
Definition 11.5.1 Observations that can aid us in determining the relative likeli-
hoods of the possible states of nature are called observables.
Table 11.5 General Utility Table
States of Nature
u1
u2
. . .
ui
. . .
un
d1
U (d1, y1)
U (d1, y2)
U (d1, yi)
U (d1, yn)
d2
Decision

States
dj
U (dj, yi)

dm
U (d1, y1)
U (dm, yn)
579
11.5 Bayesian Decision Theory

We remark that observables enable us to refine and update our initial prior p(y).
The updated prior is the conditional distribution p(yjobservables), which clearly
depends on the observables as well as the initial prior p(y). The updated prior is also
called the posterior.
For example, to determine the nature of weather we may hear the weather forecast
(80% chance of rain), in which case we may assume P(G)¼0.2, and P(B)¼0.8.
However, the weather forecast is not perfect. Let ^G and ^B denote the meteorologist’s
prediction. We may like to know P Gj ^G


and P Gj^B


. That is, what is the proba-
bility of the weather being good when the meteorologist predicts the weather will
be good, and what is the probability that the weather is good when the meteorologist
predicts the weather will be bad?
It may be noted that there is no direct cause-effect relation in Gj ^G. That is, the
prediction of the weather forecast does not influence the weather. If a probability
distribution depends on a set of parameters y, the classical approach estimates y
on the basis of an observed sample X1, ..., Xn. The samples X1, ..., Xn are the observ-
ables. Thus, observables are used to estimate the parameters, that is, we want the
distribution of y given X1, ..., Xn or p(yjX1, ..., Xn). In our weather situation, the
observable is the weather forecast, whereas the parameter is one of the weather con-
ditions, good or bad. In P ^GjG


we are asking, “Given that the weather is good, what
is the probability that the weather forecast is correct?” We can imagine that meteo-
rological conditions such as the barometric pressure determine the weather (that is,
G¼f(m1, ..., mk), mi¼meterological factor), and in this sense we can consider that G
is a parameter. We thus want P Gj ^G


:
To compute the posterior P Gj ^G


, we use the Bayes theorem (which needs a
prior distribution, P(G)). That is,
P Gj ^G


¼
P ^GjG


P G
ð Þ
P ^GjG


P G
ð Þ + P ^GjB


P B
ð Þ
:
Similarly, we can compute P Bj^B


:
Coming back to our weather situation, if P(G) is known and P ^GjG


,P ^BjB


are
known, we could obtain the required posterior distributions P Gj ^G


and P Bj^B


.
We can now use this distribution to calculate the expected utilities and choose the
decision that maximizes the expected utility.
We now consider an example.
EXAMPLE 11.5.2
Let us initially assume P(y¼1)¼P(y¼0)¼ 1
2. That is,
P good weather
ð
Þ ¼ P bad weather
ð
Þ ¼ 1
2:
Suppose we have the following record on the meteorologist’s predictions. The meteorologist
predicts good weather
^G
 
, given the weather is good, 2
3 of the time, that is, P ^GjG


¼ 2
3, and predicts
bad weather, given the weather is bad, 3
4 of the time, that is, P ^BjB


¼ 3
4. Thus, given that the mete-
orologist predicts good weather, what is the probability that the weather will turn out to be good, and
580
CHAPTER 11 Bayesian Estimation Inference

given the meteorologist predicts bad weather, what is the probability that the weather will turn out to
be bad?
Solution
To compute the true probabilities, we use the Bayes theorem.
We are given P ^GjG


¼ 2
3 and P ^BjB


¼ 3
4, which imply P ^BjG


¼ 1
3 and P ^GjB


¼ 1
4. Using the
Bayes theorem, we obtain the likelihood of G as
P Gj ^G


¼
P ^GjG


P G
ð Þ
P ^GjG


P G
ð Þ + P ^GjB


P B
ð Þ
¼
2
3
 
1
2ð Þ
2
3
 
1
2ð Þ +
1
4ð Þ 1
2ð Þ ¼ 8
11,
and the likelihood of B is
P Bj^B


¼
P ^BjB


P B
ð Þ
P ^BjB


P B
ð Þ + P ^BjG


P G
ð Þ
¼
3
4ð Þ 1
2ð Þ
3
4ð Þ 1
2ð Þ +
1
3
 
1
2ð Þ ¼ 9
13:
Thus, we have the following updated prior depending upon the meteorologist’s prediction. The
updated prior when the meteorologist predicts good weather is
p G
ð Þ ¼ P Gj ^G


¼ 8
11; p B
ð Þ ¼ 1p G
ð Þ ¼ 3
11:
Thus, the updated p(G) is actually p ^G G
ð Þ: Similarly, the updated prior when the meteorologist pre-
dicts bad weather that is, p^B G
ð Þ


is
p G
ð Þ ¼ P Gj^B


¼ 4
13; p B
ð Þ ¼ P Bj^B


¼ 9
13:
That is, if the meteorologist predicts good weather, he will be right about 72.7% of the time, and if he
predicts bad weather, he will be right about 69.2% of the time.
EXAMPLE 11.5.3
Consider Example 11.5.2, with the additional information that the meteorologist has predicted that
the weather will be good on a given weekend. Referring to the utility table (Table 11.5) given in
Example 11.5.1, we ask, what should be our decision—to insure or not to insure—in light of this
prediction?
Solution
From Example 11.5.2, we know that the updated prior, given that the meteorologist predicts good
weather, is
p G
ð Þ ¼ P Gj ^G


¼ 8
11 and p B
ð Þ ¼ P Bj ^G


¼ 3
11:
Using the foregoing prior and the utility table in Example 11.5.2, we can compute the following
expected gains:
Expected gain if we insure ¼ 125
ð
Þp G
ð Þ + 135
ð
Þp B
ð Þ
¼ 125
ð
Þ 8
11 + 135
ð
Þ 3
11 ¼ 127:73:
Continued
581
11.5 Bayesian Decision Theory

and
Expected gain if we do not insure ¼ 200
ð
Þ 8
11 ¼ 145:45:
Therefore our decision, given that the meteorologist predicts good weather, is not to insure.
EXERCISES 11.5
11.5.1. Suppose that we will receive $25 if we get two consecutive heads (H) on
two flips of a balanced coin. If only one head appears, we will get $10. On
the other hand, if there is no heads, we will lose $15. If monetary return is
the only concern, should we play this game? Why?
11.5.2. In the previous problem, suppose we suspect the coin is not balanced.
We feel that P(H) is only 0.4. In our last 10 observations, we counted
three heads and seven tails. Should we play the game? Defend your
answer.
11.5.3. The owner of a small structural engineering firm in Tampa wants to open a
new branch office in Orlando. The single most influential factor is the
projected state of the economy for the next 4 years. If the economy keeps
expanding or at least does not take a turn for the worse, the owner expects an
annual profit of $300,000 by opening the new office. If the economy
experiences a downward trend, then the owner forecasts an annual loss of
$200,000. If he just continues to operate his business in Tampa, he expects a
$50,000 annual profit. Suppose a government forecast indicates that there is
a 70% chance of economic expansion or status quo in the next 4 years and
there is a 30% chance that the economy will show a decline. What is the
optimal decision in this problem? Did you make any assumption in
obtaining this optimal decision?
11.5.4. In Exercise 11.5.3, suppose the owner decides to look at the accuracy of past
forecasts by the government. Suppose his study indicates that a forecast of
economic expansion came true only 2/3 of the time, whereas an economic
downturn came true 4/5 of the time. Now based on this new evidence, what
is the optimal option for the owner?
11.5.5. Consider the weather Example 11.5.1, discussed earlier. The
meteorologist’s prediction record over the past 15 days is as follows:
Weather person’s prediction
G
B
B
G
G
G
B
G
G
B
B
G
B
G
G
How the weather turned out to be
B
B
B
G
G
B
B
G
B
G
B
G
G
G
G
(a) Assuming a uniform distribution for the states of nature, obtain an
updated prior (posterior) based on the meteorologist’s record.
(b) Obtain the Bayes decision.
11.5.6. A coin (not necessarily fair) will be tossed once, and you have to predict the
outcome. If you predict the outcome correctly you win $1000. Otherwise,
you lose $5.
582
CHAPTER 11 Bayesian Estimation Inference

(a) What are the states of nature? What is the decision space? Write the
utility table.
(b) Suppose that you believe that the probability of heads is 2/3. What is
your price for the states of nature? Find the expected gains.
(c) Suppose that you are allowed to toss the coin twice and you find that the
firsttossresultsinheadsandthesecondintails.Whataretheobservables?
(d) Assume the situation in (c). The coin is going to be tossed again and you
have to predict the outcome. What is your updated prior?
(e) What are your expected gains, and what is your decision for the
situation in (d)?
11.5.7. We are given the following utility table:
States of nature
y1
y2
y3
d1
0
10
4
d2
2
5
1
Determine the Bayes decision assuming a uniform prior for the states of
nature.
11.5.8. Suppose that we have an observable X that can take only two values, X1 and
X2, for the situation in Exercise 11.5.7. The distribution of X depends on the
states of nature and is as follows:
y1
y2
y3
X1
0.1
0.5
0.6
X2
0.9
0.5
0.4
That is, P(X¼x1jy1)¼0.1 or P(X¼x2jy3)¼0.4, and so forth.
Suppose you observe X1; what is the updated prior? What is the Bayes
decision?
11.5.9. A large lot has p% defectives and you have to predict p. If you predict p
correctly you gain $g, and if the prediction is wrong, you lose $l. It is known
that the possible values of p are p1, p2, . . ., pk.
(a) Set up a utility table.
(b) Suppose you assume a uniform prior for p. That is
p pi
ð Þ ¼ 1
k, i ¼ 1,2, ...,k . Find an expression for the Bayes decision.
(c) Suppose you have an observable X such that P(X¼x1jpi)¼ai, i¼1, 2
. . ., k and P(X¼x1jpi)¼1ai, i¼1, 2,. . ., k. Find the updated prior for
p. What is the Bayes decision in this case?
11.6 CHAPTER SUMMARY
In this chapter we introduced the basic philosophy, definitions, and methods of per-
forming statistical analysis in a Bayesian setting. The treatment of unknown param-
eters as if they are random variables provides a feedback mechanism to update our
583
11.6 Chapter Summary

original beliefs about the parameter(s). The posterior distribution of the parameter(s)
represents our revised belief and is calculated by combining data and prior knowl-
edge. We also saw a brief explanation of Bayesian decision theory. It should be noted
that thereare various other aspects ofBayesian analysis, such asBayesian regression, in
which priors are used about the regression coefficients as well as about the error vari-
ance. It is beyond the scope of one chapter to deal with all aspects of Bayesian analysis.
There are many publications on Bayesian statistics. We have also briefly studied some
elements of decision theory, which has a natural base in the Bayesian approach.
We now list some of the key definitions introduced in this chapter:
•
Posterior distribution.
•
Quadratic loss function.
•
Absolute error loss function.
•
100 (1a)% credible interval.
•
Prior odds ratio.
•
Posterior odds ratio.
•
Observable.
In this chapter, we have also learned the following important concepts and
procedures:
•
Bayesian parameter estimation procedure.
•
Bayesian credible interval procedure.
•
General decision theory procedure.
•
Procedure to find optimal decision.
11.7 COMPUTER EXAMPLES
A very popular software (and it is free) for the Bayesian computation is WinBUGS,
which can be obtained from http://www.mrc-bsu.cam.ac.uk/bugs/. Computing pos-
terior probability for proportions using the steps we learned in Section 11.2 can be
performed using Minitab. Refer to the book, Bayesian Computation Using Minitab,
by Jim Albert (Wadsworth, 1996). For R help, we suggest the book, Bayesian Com-
putation with R (Second Edition), by Jim Albert, Springer, 2009. The methods
explained in this book can also be used in Chapter 13.
11.7.1 EXAMPLES WITH R
In order to do the R-codes in this section, download the R-package “LearnBayes”.
EXAMPLE 11.7.1
For the data of Example 11.2.1 write an R-code to obtain posterior.
Solution: We use p¼y.
p¼seq(0.8, 0.9, by¼0.02)
584
CHAPTER 11 Bayesian Estimation Inference

prior¼c(0.13, 0.15, 0.22, 0.25, 0.15, 0.10)
prior¼prior/sum(prior)
plot(p, prior, type¼“h”, ylab¼“Prior Probability”)
data¼c(13, 2)
post¼pdisc(p, prior, data)
post¼pdisc(p, prior, data)
round(cbind(p, prior, post), 2)
Output:
0.25
0.20
Prior Probability
0.15
0.10
0.80
0.82
0.84
p
0.86
0.88
0.90
Discrete prior distribution for a proportion p.
p prior post
[1,] 0.80 0.13 0.11
[2,] 0.82 0.15 0.14
[3,] 0.84 0.22 0.23
[4,] 0.86 0.25 0.27
[5,] 0.88 0.15 0.16
[6,] 0.90 0.10 0.10
EXAMPLE 11.7.2 (POSTERIOR CALCULATION)
Consider Example 11.2.4 with mp¼100, sp¼15, and x¼115. Write an R-code to find posterior.
Solution
R-code:
library (LearnBayes)
Continued
585
11.7 Computer Examples

mup¼100
sigmp¼15
sigma¼10
x¼115
post¼rnorm(1000,((sigma^2*mup/(sigmp^2+sigma^2))+(sigma^2*x/(sigmp^2+sigma^2))),
(sigma^2*sigmp^2/(sigmp^2+sigma^2)))
post
hist(post)
Output
Along with many posterior sample values, we will get following histogram for the posterior.
250
200
150
100
50
0
-100
0
100
post
Histogram of post
Frequency
200
300
EXAMPLE 11.7.3
(Credible interval) Obtain a 95% credible interval for the posterior obtained in Example 11.7.2.
Solution
Once we have posterior stored in post, then following will give us the credible interval.
R-code
quantile(post, c(0.025,0.5,0.975))
Output
2.5% 50% 97.5%
76.84277 66.83870 207.86700
586
CHAPTER 11 Bayesian Estimation Inference

Post. Odds <1
reject the null hypothesis
EXAMPLE 11.7.4 (BAYESIAN HYPOTHESIS TESTING)
The following is random data from a normal distribution with variance 9.
0:92 1:05 5:53 3:64 4:47 2:60 0:71 3:66
1:38
3:87
7:42 1:76 0:01 2:69
1:54
3:97 1:34 1:63 1:24 4:78
Test the hypothesis, H0: m0 versus Ha: m>0. Assume that the prior is N(0, 4), so that m0 and
m>0 are equally probable.
Solution
R-code
y¼c(0.92, 7.42, 1.05, 1.76, 5.53, 0.01, 3.64, 2.69, 4.47, 1.54,
+ 2.60, 3.97, 0.71, 1.34, 3.66, 1.63, 1.38, 1.24, 3.87, 4.78)
pop.s¼3
norpar¼c(0,4) # vector of mean and standard deviation of the normal prior distribution
m0¼0 # value of the normal mean to be tested
mnormt.onesided(m0,normpar,data)
Output
$BF (Bayes factor in support of the null hypothesis)
[1] 0
$prior.odds (prior odds of the null hypothesis)
[1] 0.7621303
$post.odds (posterior odds of the null
hypothesis)
[1] 0
$postH (posterior probability of null hypothesis)
[1] 0
PROJECTS FOR CHAPTER 11
11A. PREDICTING FUTURE OBSERVATIONS
Suppose we want to predict the value of future observations based on the prior and
observed data. In addition to the posterior distribution f(yjx), in Bayesian statistics we
are interested in the marginal density of the observations (note that because both y
and x are random, it makes sense to speak about their joint, marginal, and conditional
densities). Using the Bayes theorem, we have seen that g (x) is at x¼(x1, . . ., xn) (for
the continuous case) to be
g x
ð Þ ¼
ð
f xjy
ð
Þp y
ð Þdy
where f (xjy)p (y) is the joint density of x and y. This also can be written as
g x
ð Þ ¼ E f xjy
ð
Þ
½
,
the expected density of observations with respect to the prior distribution p (y). With
the help of g (x), we can predict observations.
We are more interested in the density of future observations y, given present data
x. However, because we have already updated the value of y using the posterior den-
sity, this should be reflected in our prediction:
587
Projects for Chapter 11

f yjx
ð
Þ ¼
ð
f y,yjx
ð
Þdy
¼
ð
f yjy,x
ð
Þ
p yjx
ð
Þdy
¼
ð
f yjy
ð
Þp yjx
ð
Þdy,
if y and x are conditionally independent given y. Conditional independence is
achieved, for example, when x¼(x1, . . ., xn)0 and y¼(xn+1, . . ., xn+m)0 both are sam-
ples from f (xjy).
We see that the density of future observations is the expected density of obser-
vations with respect to posterior distribution. Consider two different priors for y.
Uniform [0,2], (2) N (1, 1/16). Assume f (xjy)N (y, 1). Find the predictive dis-
tributions given the sample X1, X2, . . ., Xn.
588
CHAPTER 11 Bayesian Estimation Inference

CHAPTER
Nonparametric Tests 12
CHAPTER CONTENTS
12.1 Introduction .................................................................................................. 590
12.2 Nonparametric Confidence Interval ................................................................. 592
12.3 Nonparametric Hypothesis Tests for One Sample ............................................. 597
12.4 Nonparametric Hypothesis Tests for Two Independent Samples ........................ 609
12.5 Nonparametric Hypothesis Tests for k  2 Samples ........................................ 618
12.6 Chapter Summary .......................................................................................... 627
12.7 Computer Examples ....................................................................................... 627
Projects for Chapter 12 .......................................................................................... 635
OBJECTIVE
In this chapter we shall introduce several classical Nonparametric or distribution-free
tests. These tests do not require distributional assumptions about the population such
as the normality.
Jacob Wolfowitz
(Source: http://apprendre-math.info/anglais/historyDetail.htm?id=Wolfowitz)
Jacob Wolfowitz was born on 19 March 1910 in Warsaw, Russian Empire (now
Poland), and died on 16 July 1981 in Tampa, Florida, USA. Wolfowitz’s earliest
Mathematical Statistics with Applications in R
Copyright © 2015 Elsevier Inc. All rights reserved.
589

interest was nonparametric inference, and the first joint paper he wrote with Abra-
ham Wald introduced methods of calculating confidence intervals that are not nec-
essarily of fixed width. It is in this paper by Wolfowitz in 1942 that the term
nonparametric appears for the first time. Later, he worked on the area of sequential
analysis and published work on sequential estimators of a Bernoulli parameter and
results on the efficiency of certain sequential estimators. He also studied asymptotic
statistical theory and worked on many aspects of the maximum likelihood method.
Information theory pioneered by Shannon was another area to which Wolfowitz
made important contributions, culminating in a classic book titled Coding Theorems
of Information Theory (3rd ed. 1978). After working at different places such as the
Statistical Research Group at Columbia University, the University of North Carolina,
and the University of Illinois at Urbana, in 1978 he joined the faculty of the Univer-
sity of South Florida at Tampa. Wolfowitz was elected to the National Academy of
Sciences and the American Academy of Arts and Sciences. He was also elected a
Fellow of the Econometric Society, the International Statistics Institute, and the
Institute of Mathematical Statistics. In 1979, he was Shannon Lecturer of the Institute
of Electrical and Electronic Engineers.
12.1 INTRODUCTION
Most of the tests that we have learned up to this point are based on the assumption
that the sample(s) came from a normal population, or at the least that the population
probability distribution(s) is specified except for a set of free parameters. Such tests
are called parametric tests. In general, a parametric test is known to be generally
more powerful than other procedures when the underlying assumptions are met. Usu-
ally the assumption of normality or any other distributional assumption about the
population is hard to verify, especially when the sample sizes are small or the data
are measured on an ordinal scale such as the letter grades of a student, in which case
we do not have a precise measurement. For example, incidence rates of rare diseases,
data from gene-expression microarrays, and the number of car accidents in a given
time interval are not normally distributed. Nonparametric tests are tests that do not
make such distributional assumptions, particularly the usual assumption of normal-
ity. In situations where a distributional model for a set of data is unavailable, non-
parametric tests are ideal. Even if the data are distributed normally, nonparametric
methods are frequently almost as powerful as parametric methods. These tests
involve only order relationships among observations and are based on ranks of
the variables and analyzing the ranks instead of the original values. Nonparametric
methods include tests that do not involve population parameters at all, such as testing
whether the population is normal. Distribution-free tests generally do make some
weak assumptions, such as equality of population variances and/or the distribution,
and are of the continuous type.
Sometimes we may be required to make inferences about models that are difficult
to parameterize, or we may have data in a form that make, say, the normal theory tests
590
CHAPTER 12 Nonparametric Tests

unsuitable. For example, incomes of families generally follow a skewed distribution.
If we do a sample survey of a large number of the families in a feeder area, the
income distribution may look as in Figure 12.1.
This distribution is clearly difficult to parameterize, that is, to identify a classical
probability distribution that will characterize the data’s behavior. Moreover, the
mean income of this sample may be misleading. A better measure of the central ten-
dency is the median income. At least we know that 50% of the families are below the
median and 50% above. Appropriate techniques of inference in these situations are
based on distribution-free methods. Most of the nonparametric methods use only the
order of magnitude of observations, known as order statistics, in a sample, rather than
the observed values of the random variables.
In general, nonparametric methods are appropriate to estimation or hypothesis
testing problems when the population distributions could only be specified in general
terms. The conditions may be specified as being continuous, symmetric, or identical,
differing only in median or mean.
The distributions need not belong to specific families such as normal or gamma.
Because most of the nonparametric procedures depend on a minimum number of
assumptions, the chance of their being improperly used is relatively small. Most
of the nonparametric procedures involve ranking data values and developing testing
methods based on the ranks. Because of this, nonparametric procedures may be used
when the data are measured on a weak scale such as only count data or rank data. We
may ask: why not use nonparametric methods all the time? The answer lies in the fact
that when the assumptions of the parametric tests can be verified as true, parametric
tests are generally more powerful than nonparametric tests. Because only ranks are
used in nonparametric methods, and even though the ranks preserve information
about the order of the data, because the actual values are not used some information
is lost. Because of this, nonparametric procedures cannot be as powerful as their
parametric counterparts when parametric tests can be used. For brevity and clarity,
this chapter is presented without much theoretical explanation to focus on the
Frequency
Unemployed
Part time
Full time
People with
large assets
Income
FIGURE 12.1
Income distribution of families.
591
12.1 Introduction

methods. Theoretical developments can be found in many specialized books on the
subject.
In this chapter, we study some of the commonly used classical nonparametric
methods that are based on ordering, ranking, and permutations. The modern
approaches are based on resampling methods such as bootstrap and will be discussed
in Chapter 13.
12.2 NONPARAMETRIC CONFIDENCE INTERVAL
We have seen that for a large sample, using the Central Limit Theorem, we can
obtain a confidence interval for a parameter within a well-defined probability distri-
bution. However, for small samples, we need to make distributional assumptions that
are often difficult to verify. For this reason, in practice it is often advisable to con-
struct confidence intervals or interval estimates of population quantities that are not
parameters of a particular family of distributions. In a nonparametric setting, we need
procedures where the sample statistics used have distributions that do not depend on
the population distribution. The median is commonly used as a parameter in nonpara-
metric settings. We assume that the population distribution is continuous.
Let M denote the median of a distribution and X (assumed to be continuous) be
any observation from that distribution. Then
P X  M
ð
Þ ¼ P X  M
ð
Þ ¼ 1
2:
This implies that, for a given random sample X1, . . ., Xn from a population
with median M, the distribution of the number of observations falling below M will
follow a binomial distribution with parameters n and p¼ 1
2, irrespective of the pop-
ulation distribution. That is, let N be the number of observations less than M. Then
the distribution of N is binomial with parameters n and p¼ 1
2 for a sample of size n.
Hence, we can construct a confidence interval for the median using the binomial
distribution.
For a given probability value a, we can determine a and b such that
P N  a
ð
Þ ¼
X
a
i¼0
n
i
  1
2
 i 1
2
 ni
¼
X
a
i¼0
n
i
  1
2
 n
¼ a
2
and
P N  b
ð
Þ ¼
X
n
i¼b
n
i
  1
2
 i 1
2
 ni
¼
X
n
i¼b
n
i
  1
2
 n
¼ a
2:
592
CHAPTER 12 Nonparametric Tests

If exact probabilities cannot be achieved, choose a and b such that the probabilities
are as close as possible to the value of a/2. Furthermore, let X(1), X(2), . . ., X(a), . . .,
X(b), . . ., X(n) be the order statistics of X1, . . ., Xn as in Figure 12.2.
Then the population median will be above the order statistic, X b
ð Þ, a
2
 
100% of the
time and below the order statistic, X a
ð Þ, a
2
 
100% of the time. Hence, a (1–a)100%
confidence interval for the median of a population distribution will be
X a
ð Þ < M < X b
ð Þ:
We can write this result as P(X(a)<M<X(b))¼1–a.
By dividing the upper and lower tail probabilities equally, we find that b¼n+1–a.
Therefore, the confidence interval becomes
X a
ð Þ < M < X n + 1a
ð
Þ:
In practice, a will be chosen so as to come as close to attaining a
2 as possible.
We can summarize the nonparametric procedure for finding the confidence inter-
val for the population median as follows.
PROCEDURE FOR FINDING (1a) 100% CONFIDENCE INTERVAL FOR
THE MEDIAN M
For a sample of size n:
1. Arrange the data in ascending order.
2. From the binomial table with n and p ¼ 1
2, find the value of a such that
p X  a
ð
Þ ¼ a
2 or nearest to a
2:
3. Set b¼n+1a.
4. Then the confidence interval is such that the lower limit is the ath value and the upper limit is the
bth value of the observations in step 1.
Assumptions: Population distribution is continuous; the sample is a simple random sample.
We illustrate this four-step procedure with an example.
EXAMPLE 12.2.1
In a large company, the following data represent a random sample of the ages of 20 employees.
24 31 28 43 28 56 48 39 52 32
38 49 51 49 62 33 41 58 63 56
Continued
X(1)
X(2)
X(a)
X(b)
X(n−1)
X(n)
M
FIGURE 12.2
Ordered sample.
593
12.2 Nonparametric Confidence Interval

Construct a 95% confidence interval for the population median M of the ages of the employees
of this company.
Solution
For a 95% confidence interval, a¼0.05. Hence, a
2¼0.025. The ordered data are
24 28 28 31 32 33 38 39 41 43
48 49 49 51 52 56 56 58 62 63
Looking at the binomial table with n¼20 and p ¼ 1
2, we see that P(X5)¼0.0207. Hence, a¼5
comes closest to achieving a
2 ¼0.025. Hence, in the ordered data, we should use the fifth observation,
32, for the lower confidence limit and the 16th observation (n+1–a¼21–5¼16), 56, for the
upper confidence limit. Therefore, an approximate 95% confidence interval for M is
32 < M < 56:
That is, we are at least 95% certain that the true median of the employee ages of this company
will be greater than 32 and less than 56.
The data of Example 12.2.1 passes the normality test and we can calculate the 95% parametric
confidence interval as (38.40, 49.70). Comparing this to the nonparametric confidence interval,
length of parametric confidence interval, in general, is smaller whenever parametric assumption
can be made.
EXAMPLE 12.2.2
A drug is suspected of causing an elevated heart rate in a certain group of high-risk patients. Twenty
patients from this group were given the drug. The changes in heart rates were found to be as follows.
1 8 5 10
2 12
7
9
1 3
4 6 4 20 11
2 1 10 2 8
Construct a 98% confidence interval for the mean change in heart rate. Can we assume that the
population has a normal distribution? Interpret your answer.
Solution
First testing for normality, we get the probability plot shown in Figure 12.3.
This shows that the normality assumption may not be satisfied, and thus the nonparametric
method is more suitable (this conclusion is based strictly on the normal probability plot). Using
a box plot, we could also test for outliers. The ordered data are
1 1 1 2 2
2
3
4
4
5
6
7 8 8 9 10 10 11 12 20
Looking at the binomial table with n¼20 and p ¼ 1
2, we see that P(X4)¼0.006. Hence, a¼4
comes closest to achieving a
2¼0.01. Hence, in the ordered data, we should use the fourth observation,
2, for the lower confidence limit and the 17th observation (n+1–a¼21–4¼17), 10, for the upper
confidence limit. Therefore, an approximate 98% confidence interval for M is
2 < M < 10:
That is, we are at least 98% certain that the true median of the mean change in heart rate will be
greater than 2 and less than 10.
If we perform the usual t-test, we will get the 98% confidence interval as (3.20, 9.0). However,
such an interval is not valid, because the normality assumptions are not satisfied.
594
CHAPTER 12 Nonparametric Tests

EXERCISES 12.2
12.2.1. For the following random sample values, construct a 95% confidence
interval for the population median M:
7:2 5:7 4:9 6:2 8:5 2:7 5:9 6:0 8:2
12.2.2. The following data represent a random sample of end-of-year bonuses for
the lower-level managerial personnel employed by a large firm. Bonuses
are expressed in percentage of yearly salary.
6:2
9:2
8:0
7:7
8:4
9:1
7:4
6:7
8:6
6:9
8:9
10:0
9:4
8:8
12:0
9:9
11:7
9:8
3:2
4:6
Construct a 98% confidence interval for the median bonus expressed in
percentage of yearly salary of this firm. Also, draw a probability plot and
test for normality. Can this be considered a random sample?
12.2.3. Air pollution in large U.S. cities is monitored to see if it conforms to
requirements set by the Environmental Protection Agency. The following
data, expressed as an air pollution index, give the air quality of a city for 10
randomly selected days.
57:3 58:1 58:7 66:7 58:6 61:9 59:0 64:4 62:6 64:9
1
10
0
10
Data
20
5
Percent
Normal probability plot for heart rate
ML Estimates
Mean: 6.1
Std Dev: 4.97896
10
20
30
40
50
60
70
80
90
95
99
FIGURE 12.3
Normal probability plot for heart rate.
595
12.2 Nonparametric Confidence Interval

(a) Draw a probability plot and test for normality.
(b) Construct a 95% confidence interval for the actual median air
pollution index for this city and interpret its meaning.
12.2.4. A random sample from a population yields the following 25 values:
90 87 121
96 106 107 89 107 83 92
117 93
98 120
97 109 78
87 99 79
104 85
91 107
89
Give a 99% confidence interval for the population median.
12.2.5. In an experiment on the uptake of solutes by liver cells, a researcher found
that six determinations of the radiation, measured in counts per minute
after 20 minutes of immersion, were:
2728 2585 2769 2662 2876 2777
Construct a 99% confidence interval for the population median and
interpret its meaning.
12.2.6. The nominal resistance of a wire is 0.20 ohm. A testing of the wire
randomly chosen from a large collection of such wires yields the following
resistance data.
0:199 0:211 0:198 0:201 0:197 0:200 0:198 0:208
Obtain a 95% confidence interval for the population median.
12.2.7. In order to measure the effectiveness of a new procedure for pruning
grapes, 15 workers are assigned to prune an acre of grapes. The
effectiveness is measured in worker-hours per acre for each person.
5:2 5:0 4:8 4:5 3:9 6:1 4:2 4:4 5:5 5:8
4:2 5:3 4:9 4:7 4:9
Obtain a 99% confidence interval for the median time required to
prune an acre of grapes for this procedure and interpret its meaning.
12.2.8. The following data give the exercise capacity (in minutes) for 10 randomly
chosen patients being treated for chronic heart failure.
15 27 11 19 12 21 11 17 13 22
Obtain a 95% confidence interval for the median exercise capacity for
patients being treated for chronic heart failure.
12.2.9. The data given below refer to the in-state tuition costs (in dollars) of 15
randomly selected colleges from a list of the 100 best values in public
colleges (source: Kiplinger’s Magazine, October 2000).
3788 4065 2196 7360 5212 4137 4060 3956
3975 7395 4058 3683 3999 3156 4354
Obtain a 95% confidence interval for the median in-state tuition costs
and interpret its meaning.
12.2.10. Sepsis is an extreme immune system response to an infection that has
spread throughout the blood and tissues. Sepsis can reduce blood flow to
596
CHAPTER 12 Nonparametric Tests

kidneys resulting in acute renal failure (also called acute kidney injury).
Relative risk of mortality associated with developing acute renal failure as
of sepsis in 16 studies is given below (Crit Care. 2002: 6(6): 509–513).
0.75
2.03
2.29
2.11
0.80
1.50
0.79
1.01
1.23
1.48
2.45
1.02
1.03
1.30
1.54
1.27
Obtain a 95% confidence interval for the median relative risk of
mortality.
12.3 NONPARAMETRIC HYPOTHESIS TESTS FOR
ONE SAMPLE
In this section, we study two popular tests for testing hypotheses about the population
location, or median using the sign test and the Wilcoxon signed rank test. The com-
parison of medians rather than means is a technicality that is not important unless the
data are skewed substantially. In such cases, medians are somewhat more accurate
than means for comparing the locations of probability distributions. Further discus-
sions on nonparametric tests can be found in many references, such as those by W. J.
Conover and by E. L. Lehmann. Before using nonparametric tests, it is desirable to test
for normality of the data using normal probability plots, and for the existence of outliers
using box plots, and run tests for test of randomness of the data. When we make any
particular choice of method, test for the assumptions made. These assumption checks
are relatively easier using statistical software packages. Many of the examples in this
chapteraregivenmoreforillustrationofthenonparametricmethodsthanforassumption
violationsofparametrictestsorforcomprehensiveassumptiontestingtechniques.Also,
whenweusestatisticalsoftwarepackages,generally,thep-valueofthetestwillbegiven
intheoutput.Inordertomakeadecisiononaparticularhypothesis,wejustneedtocom-
pare the p-value with the chosen value of a. We are going to explain a more traditional
approachinsteadofusingthe p-valueapproachinthediscussion; howeverthecomputer
example section will illustrate the p-value approach.
12.3.1 THE SIGN TEST
In this section, we describe a test that is the nonparametric alternative to the one-
sample t-test and to the paired-sample t-test. Let M be the median of a certain pop-
ulation. Then we know that
P X  M
ð
Þ ¼ 0:5 ¼ P X > M
ð
Þ:
We consider the problem of testing the null hypothesis
H0 : M ¼ m0 versus Ha : M > m0:
Assume that the underlying population distribution is continuous. Let Xi be the ith
observation and let N+ be the number of observations that are greater than m0.
597
12.3 Nonparametric Hypothesis Tests for One Sample

N+ will be our test statistic. We will reject H0 if, n+ the observed value of N+, is too
large. This test is called the sign test. A test at significance level a will reject H0 if
n+k, where k is chosen such that
P N +  k when M ¼ m0
ð
Þ ¼ a:
Similarly, if the alternative is of the form Ha: M6¼m0, the critical region is of the form
N+k or N+k1, where P(N+k)+P(N+k1)¼a.
In order to determine such a k and k1, we need to determine the distribution of N+.
The test works on the principle that if the sample were to come from a population
with a continuous distribution, then each of the observations falls above the median
or below the median with probability 1
2. Hence, the number of sample values falling
below the median follows a binomial distribution with parameters n and p ¼ 1
2, n
being the sample size. If a sample value equals the hypothesized median m0, that
observation will be discarded and the sample size will be adjusted accordingly
(we remark that such values should be very few). Thus, when H0 is true, N+ will have
a binomial distribution with parameters n and p ¼ 1
2. For this reason, some authors
call this test the binomial test. The following box summarizes the test procedure
and the corresponding critical regions.
SIGN TEST
H0 : M ¼ m0
Alternative Hypothesis
Critical Region
Ha: M>m0
N +  k, where
Xn
i¼k
n
i
  1
2
 n
¼ a
Ha: M<m0
N +  k, where
Xk
i¼0
n
i
  1
2
 n
¼ a
Ha: M6¼m0
N +  k1,where
Xn
i¼k1
n
i
  1
2
 n
¼ a
2
or
N +  k,where
Xk
i¼0
n
i
  1
2
 n
¼ a
2
If a or a
2 cannot be achieved exactly, choose k (or k and k1) so that the probability comes as close
to a (or a/2) as possible.
We now summarize the procedure of the sign test in the case of an upper tail alter-
native. The other two cases are similar.
HYPOTHESIS TESTING PROCEDURE BY SIGN TEST
We test
H0 : M ¼ m0 versus H1 : M > m0:
598
CHAPTER 12 Nonparametric Tests

1. Replace each value of the observation that is greater than m0 by a plus sign and each sample value
less than m0 by a minus sign. If the sample value is equal to m0, discard the observation and adjust
the sample size n accordingly.
2. Let n+ be the number of +’s in the sample. For n and p ¼ 1
2, from the binomial table, find
g ¼ P N +  n +
ð
Þ:
3. Decision: If g is less than a, H0 must be rejected. Based on the sample, we will conclude that
the median of the population is greater than m0 at the significance level a. Otherwise do not
reject H0.
Assumptions: The population distribution is continuous. The number of ties is small (less than
10% of the sample).
Note that the approach described in the foregoing procedure is nothing but the p-
value method for hypothesis testing regarding a median using the sign test. Recall
that the p-value is the probability of observing a test statistic as extreme or more
extreme than what was really observed, under the assumption that the null hypothesis
is true. In the sign test, we had assumed that the median is M¼m0, so 50% of the data
should be less than m0 and 50% of the data greater than m0. Thus, we expect half of
the data to result in plus signs and half to result in minus signs. Hence, we can think of
the data as following a binomial distribution with p ¼ 1
2 under the null hypothesis. The
p-value is computed from its definition given by the formula
p-value ¼ P N +  n +
ð
Þ ¼
X
n
i¼k
n
i
  1
2
 n
¼ g:
The p-value method is to reject the null hypothesis if the computed p-value is greater
than a. These binomial probabilities can be obtained from the binomial tables, or
statistical software packages. The following example illustrates how we apply the
three-step procedure.
EXAMPLE 12.3.1
For the given data from an experiment
1:51 1:35 1:69 1:48 1:29 1:27 1:54 1:39 1:45
test the hypothesis that H0: M¼1.4 versus Ha: M>1.4 at a¼0.05.
Solution
We test
H0 : M ¼ 1:4 versus Ha : M > 1:4:
Replacing each value greater than 1.4 with a plus sign and each value less than 1.4 with a minus
sign, we have
+  + +  +  + :
Thus, n+¼5. From the binomial table with n¼9 and p ¼ 1
2, we have
P N +  5
ð
Þ ¼ 0:50:
Thus, the p-value is 0.5. Because a¼0.05<0.50, the null hypothesis is not rejected. We con-
clude that the median does not exceed 1.4.
599
12.3 Nonparametric Hypothesis Tests for One Sample

When the sample size n is large, we can apply the normal approximation to the bino-
mial distribution. That is, the test statistic N+ is approximately normally distributed.
Thus, under H0, N+ will have approximate normal distribution with mean np ¼ n
2 and
variance of np 1p
ð
Þ ¼ n
4. By the z-transform, we have
Z ¼ N + n=2
ﬃﬃﬃﬃﬃﬃﬃﬃ
n=4
p
¼ 2N + n
ﬃﬃﬃn
p
 N 0, 1
ð
Þ:
We could utilize this test if n is large, that is, if np5 and n(1–p)5. Hence, under
H0, because p ¼ 1
2, if n10, we could use the large sample test. The following table
summarizes the large sample sign test.
A SIGN TEST FOR A LARGE RANDOM SAMPLE
When the sample size is large (n10), we can use the normal approximation to a binomial. This
leads to the large sample sign test:
H0 : M ¼ m0
versus
Alternative Hypothesis
Rejection Region
Ha: M>m0
zza
Ha: M<m0
zza
Ha: M6¼m0
jzjza/2
The test statistic is
Z ¼ 2N + n
ﬃﬃﬃn
p
:
Decision: Reject H0, if the test statistic falls in the rejection region, and conclude that Ha is true
with (1a)100% confidence. Otherwise, do not reject H0 because there is not enough evidence to
conclude that Ha is true for a given a, and more experiments are needed.
Assumptions: (i) Population distribution is continuous. (ii) Sample size greater than or equal to
10 (after the removal of ties). (iii) The number of ties is small (less than 10% of the sample size).
We illustrate this procedure with the following example.
EXAMPLE 12.3.2
In order to measure the effectiveness of a new procedure for pruning grapes, 15 workers are assigned
to prune an acre of grapes. The effectiveness is measured in worker-hours/acre for each person.
5:2 5:0 4:8 3:9 6:1 4:2 4:4 5:5 5:8 4:5
4:2 5:3 4:9 4:7 4:9
Test the null hypothesis that the median time to prune an acre of grapes with this method is 4.5 h
against the alternative that it is larger. Use a¼0.05.
Solution
We test
H0 : M ¼ 4:5 versus H0 : M > 4:5:
600
CHAPTER 12 Nonparametric Tests

Replacing each value greater than 4.5 with a plus sign and each value less than 4.5 with a minus
sign, we have
+ + +  +  + +  + + + + :
Because there is one observation that is equal to 4.5, we must discard it and take n¼14. Thus
N+¼10, using the large sample approximation, the test statistic is
Z ¼ 2N + n
ﬃﬃﬃn
p
¼ 2014
ﬃﬃﬃﬃﬃ
14
p
¼ 1:6:
For a¼0.05, from the standard normal table, the value of z0.05¼1.645. Hence, the rejection
region is z¼1.645. Because the observed value of the test statistic does not fall in the rejection
region, we do not reject the null hypothesis at a¼0.05 and conclude that the median time to prune
an acre of grapes is 4.5 h.
12.3.2 WILCOXON SIGNED RANK TEST
In the sign test, we have considered only whether each observation is greater than m0
or less than m0 without giving any importance to the magnitude of the difference
from m0. Neglecting information on the magnitude of the observations is rather inef-
ficient and may reduce the statistical power of the test. An improved version of the
sign test is the Wilcoxon signed rank test, in which one replaces the observations by
their ranks of the ordered magnitudes of differences, jxim0j. The smallest obser-
vation is ranked as 1, the next smallest will be 2, and so on. However, the Wilcoxon
signed rank test requires an additional assumption that the continuous population
distribution is symmetric with respect to its center. Thus, if the data are ordinal,
the Wilcoxon test cannot be used.
HYPOTHESIS TESTING PROCEDURE BY WILCOXON SIGNED RANK TEST
We test
H0 : M ¼ m0 versus H1 : M 6¼ m0:
1. Compute the absolute differences zi¼jxim0j for each observation. Replace each value of the
observation that is greater than m0 by a plus sign and each sample value that is less than m0 by a
minus sign. If the sample value is equal to m0, discard the observation and adjust the sample size
n accordingly.
2. Assign each zi a value equal to its rank. If two values of zi are equal, assign each zi a rank equal to
the average of ranks each should receive if there were not a tie.
3. Let W+ be the sum of the ranks associated with plus signs and W be the sums of ranks with
negative signs.
4. Decision: If m0 is the true median, then the observations should be evenly distributed about m0.
For a size a critical region, reject H0 if
W +  c1,whereP W +  c1
ð
Þ ¼ a
2,
or
W +  c2,whereP W +  c2
ð
Þ ¼ a
2,
Assumptions: The population distribution is continuous and symmetrical. The number of ties is
small, less than 10% of the sample size.
601
12.3 Nonparametric Hypothesis Tests for One Sample

The exact distribution of W+ is considerably complicated and we will not derive
it. However, for certain values of n, the distribution is given in the Wilcoxon signed
rank test table.
For the Wilcoxon signed rank test, the rejections region based on the alternative
hypothesis is given next.
For
Ha : M > m0, rejectionregion is W +  c, where P W +  c
ð
Þ ¼ a:
and for
Ha : M < m0, rejectionregion is W +  c, where P W +  c
ð
Þ ¼ a:
We illustrate the Wilcoxon signed rank test with the following examples.
EXAMPLE 12.3.3
For the given data that resulted from an experiment
1:51 1:35 1:69 1:48 1:29 1:27 1:54 1:39 1:45
test the hypothesis that H0: M¼1.4 versus Ha: M 6¼ 1.4. Use a¼0.05.
Solution
We test
H0 : M ¼ 1:4 versus Ha : M 6¼ 1:4:
Here, a¼0.05, and m0¼1.4. The results of steps 1-3 are given in Table 12.1.
Thus, we have W+¼29 and n¼9. From the Wilcoxon signed rank test table in the appendix, we
should reject H0 if W+6 or W+38 with actual size of a¼0.054. Because W+¼29 does not fall in
the rejection region, we do not reject the null hypothesis that M¼1.4.
Table 12.1 Data Summary for Wilcoxon Signed Rank Test
xi
zi¼jxi–1.4j
Sign
Rank
1.51
0.11
+
5.5
1.35
0.05
–
3
1.69
0.29
+
9
1.48
0.08
+
4
1.29
0.11
–
5.5
1.27
0.13
–
7
1.54
0.14
+
8
1.39
0.01
–
1.5
1.45
0.01
+
1.5
602
CHAPTER 12 Nonparametric Tests

EXAMPLE 12.3.4
Air pollution in large US cities is monitored to see whether it conforms to requirements set by the
Environmental Protection Agency. The following data, expressed as an air pollution index, give the
air quality of a city for ten randomly selected days.
57:3 58:1 58:7 66:7 58:6 61:9 59:0 64:4 62:6 64:9
Test the hypothesis that H0: M¼65 versus Ha: M<65. Use a¼0.05.
Solution
We test
H0 : M ¼ 65 versus Ha : M < 65:
Here, a ¼ 0.05,
and
m0 ¼ 65.
The results of steps 1-3 are given in Table 12.2.
Thus, W+¼3, and n¼10. Using the Wilcoxon signed rank test table, we should reject H0 if
W+10 with actual size of a¼0.042. Because the observed value of W+ falls in the rejection region,
we reject H0 and conclude that the sample evidence suggests that we conclude the median air pol-
lution index is less than 65.
The Wilcoxon signed rank test is a nonparametric alternative to the one-sample t-test.
The question then is, how do we decide which one to choose? Choose the one-sample t-
test if it is reasonable to assume that the population follows a normal distribution. Oth-
erwise, choose the Wilcoxon nonparametric test. However, the Wilcoxon test will have
less power. For example, a normal probability plot of the data of Example 12.3.4 is
given in Figure 12.4. Looking at this figure, we can see that the normality assumption
is a suspect. It may make more sense to use the nonparametric method.
When sample size n is sufficiently large, under the assumption of H0 being true,
the distribution of W+ is approximately normal with mean
E W +
ð
Þ ¼ 1
4n n + 1
ð
Þ
and variance
Var W +
ð
Þ ¼ n n + 1
ð
Þ 2n + 1
ð
Þ
24
:
Table 12.2 Summary Calculations for Air Pollution Data
xi
zi¼jxi–65j
Sign
Rank
57.3
7.7
–
10
58.1
6.9
–
9
58.7
6.3
–
8
66.7
1.7
+
3
58.8
6.2
–
7
61.9
4.1
–
5
59.0
6.0
–
6
64.4
0.6
–
2
62.6
2.4
–
4
64.9
0.1
–
1
603
12.3 Nonparametric Hypothesis Tests for One Sample

Hence, the test statistic is given by
Z ¼
W + 1
4n n + 1
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n n + 1
ð
Þ 2n + 1
ð
Þ=24
p
which is approximately the standard normal distribution. This approximation can be
used when n>20.
SUMMARY OF THE WILCOXON SIGNED RANK TEST FOR LARGE
SAMPLES (n>20)
We test
H0 : M ¼ m0
versus
M > m0,upper tailed test
Ha : M < m0, lower tailed test
M 6¼ m0, twotailed test:
The test statistic:
Z ¼
W +  1
4n n + 1
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n n + 1
ð
Þ 2n + 1
ð
Þ=24
p
:
Rejection region:
z > za,
upper tail RR
z < za,
lower tail RR
jzj > za=2, two tail RR:
8
<
:
57
58
59
60
61
62
Index
Probability
Normal probability plot
Average: 61.22
Std Dev: 3.32158
N: 10
Kolmogorov-Smirnov Normality Test
D: 0.248 D: 0.131 D: 0.248
Approximate P-Value: 0.081
0.001
0.01
0.05
0.20
0.50
0.80
0.95
0.99
0.999
63
64
65
66
67
FIGURE 12.4
Normal probability for air pollution index.
604
CHAPTER 12 Nonparametric Tests

Decision: Reject H0, if the test statistic falls in the RR, and conclude that Ha is true with (1a)
100% confidence. Otherwise, do not reject H0, because there is not enough evidence to conclude that
Ha is true for a given a and more experiments are needed.
Assumptions: (i) The population distribution is continuous and symmetric about 0. (ii) Sample
size is greater than or equal to 20. (iii) The number of ties is small, <10% of the sample size.
We illustrate the Wilcoxon signed rank test with the following example.
EXAMPLE 12.3.5
The following data give the monthly rents (in dollars) paid by a random sample of 25 households
selected from a large city.
425 960 1450 655
1025 750
670
975
660
880
1250 780
870 930
550
575
425
900
525
1800
545 840
765 950 1080
Using the large sample Wilcoxon signed rank test, test the hypotheses that the median rent in
this city is $750 against the alternative that it is higher with a¼0.05.
Solution
We test
H0 : M ¼ 750 versus Ha : M > 750:
Here a¼0.05, and m0¼750. The results of steps 1–3 are given in Table 12.3 (where the asterisk
indicates zi¼0).
Continued
Table 12.3 Summary Calculations for
Monthly Rent Data
xi
zi¼jxi–750j
Sign
Rank
425
325
–
19.5
960
210
+
15
1450
700
+
23
655
95
–
6
1025
302
+
18
750
0
*
ignore
670
80
–
3
975
225
+
16.5
660
90
–
4.5
880
130
+
8
1250
500
+
22
780
30
+
2
870
120
+
7
930
180
+
11
550
200
–
12.5
575
175
–
10
425
325
–
19.5
605
12.3 Nonparametric Hypothesis Tests for One Sample

Here, for n¼24, W+¼172.5, and the test statistic is
Z ¼
W + 1
4n n + 1
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n n + 1
ð
Þ 2n + 1
ð
Þ=24
p
¼
172:5
1
4
 
24
ð
Þ 25
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
24
ð
Þ 25
ð
Þ 49
ð
Þ
24
r
¼ 0:64286:
For a¼0.05, the rejection region is z>1.645. Because the observed value of the test statistic
does not fall in the rejection region, we do not reject the null hypothesis. There is not enough evi-
dence to conclude that the median rent in this city is more than $750.
The rank tests are useful for situations when you suspect that the data do not follow
the normal population. It is important to note that ignoring the tied observations
reduces the effective sample size, which in turn reduces the power of the test (see
Example 7.1.4 for the effect of n on the value of b). This loss is not significant if
there are only a few ties. However, if the ties are 10% or more, hypothesis testing
using rank tests becomes considerably conservative. That is, they yield error prob-
abilities that are significantly high.
12.3.3 DEPENDENT SAMPLES: PAIRED COMPARISON TESTS
The sign test and the Wilcoxon signed rank test can also be used for paired compar-
isons. The experimental procedure typically consists of taking “before” and “after”
type or otherwise matched as in the paired t-test case readings for each unit. Suppose
there are n pairs of before and after observations and we are interested in testing the
equality of the two medians. One way to test such observations is to consider the
difference between the two observations for a unit to be a single observation on that
unit. Thus, we can treat the sample as being n observations on a population of dif-
ferences. For this new sample of differences, the testing problem becomes
Table 12.3 Summary Calculations for
Monthly Rent Data—cont’d
xi
zi¼jxi–750j
Sign
Rank
900
150
+
9
525
225
–
16.5
1800
1050
+
24
545
205
–
14
840
90
+
4.5
765
15
+
1
950
200
+
12.5
1080
330
+
21
606
CHAPTER 12 Nonparametric Tests

H0 : M ¼ 0 versus Ha : M > 0 or M < 0, or M 6¼ 0
ð
Þ:
Hence, the basic procedure could be summarized to first find the difference between
the two units for each of the observations, and then follow the testing procedures
explained earlier for the sign test or the Wilcoxon signed rank test. Both small sample
and large sample cases can be handled as before. In the following example, we illus-
trate this concept for a large sample sign test.
EXAMPLE 12.3.6
A dietary program claims that three months of its diet will reduce weight. In order to test this claim, a
random sample of eight individuals who went through this program for three months is taken. The
following table gives weight in pounds.
Before 180
199
175
226
189
205
169
211
After
172
191
172
230
178
199
171
201
Using a 5% significance level, is there evidence to conclude that the program really reduces the
population median weight?
Solution
Let M denote the median of the population of difference of weights. We will use the difference as
“after”–“before.” Then we will test
H0 : M ¼ 0 versus Ha : M < 0:
We will use the large sample sign test. Replacing each value of the difference that is greater than
zero by a ‘+’ sign and less than zero by a ‘–’ sign, we have
Difference
8
8
3
4
11
6
2
10
Sign
–
–
–
+
–
–
+
–
For n¼8 and N+¼2, the test statistic is given by
Z ¼ 2N + n
ﬃﬃﬃn
p
¼ 48
ﬃﬃﬃ
8
p
¼ 1:414:
For a¼0.05, z0.05¼1.645, and the rejection region is z<1.645. Because the observed value
of the test statistic does not fall in the rejection region, we do not reject the null hypothesis. Thus,
there is not enough evidence to conclude that the new program reduces the weight. Note that even
though n¼8 is small, here we are using the large sample test only for the demonstration purpose.
EXERCISES 12.3
12.3.1. It was reported that the median interest rate on 30-year fixed mortgages
in a certain large city is 7.75% on a particular day, with zero points.
A random sample of nine lenders produced the following data of interest
rates in percentage.
7:625 7:375 8:00 7:50 7:875 8:00 7:625 7:75 7:25
Test the hypothesis that the median interest rate in this city is different
from 7.75%, using (a) the sign test, and (b) the Wilcoxon signed rank test.
Use a¼0.01. Compare the two results.
607
12.3 Nonparametric Hypothesis Tests for One Sample

12.3.2. It is believed that a typical family spends 35% of its income on food and
groceries. A sample of eight randomly selected families yielded the
following data.
30 29 39 49 36 33 37 35
Test the hypothesis that the median percentage of family income spent
for food and groceries is 35 against the alternative that it is less than 35.
Use a¼0.05.
12.3.3. The SAT scores (out of a maximum possible score of 1600) for a random
sample of 10 students who took this test recently are:
1355 765 890 1089 986 1128 1157 1065 1224 567
Test the hypothesis that the median SAT score is 1000 against the
alternative that it is greater using a¼0.05. Use both the sign test and the
Wilcoxon signed rank test. Explain if the conclusions are different.
12.3.4. The regulatory board of health in a particular state specifies that the
fluoride levels in water must not exceed 1.5 parts per million (ppm). The
20 measurements given here represent the randomly selected daily early
morning readings on fluoride levels in water at a certain city.
0:88 0:82 0:97 0:95 0:84 0:90 0:87 0:78 0:75 0:83
0:71 0:92 1:11 0:81 0:97 0:85 0:97 0:91 0:78 0:87
Test the hypothesis that the median fluoride level for this city is 0.90
against the alternative that the median is different from 0.9 at a¼0.01,
using (a) the large sample sign test, and (b) the Wilcoxon signed rank test.
Interpret the results.
12.3.5. The following data give the weights (in pounds) for a random sample of 20
NFL players.
285
178
311
276
192
232
259
189
298
211
269
285
296
193
288
254
246
234
274
229
Test the hypothesis that the median weight of NFL players is 250
pounds against the alternative that it is greater at a¼0.05, using (a) the
large sample sign test and (b) the Wilcoxon signed rank test.
12.3.6. The following data give the amount of money (in dollars) spent on
textbooks by 18 students for the last academic year at a large university.
510
425
190
298
157
260
320
615
455
490
188
115
230
610
220
155
315
110
Test the hypothesis that the median amount spent on books at this
university is $325 against the alternative that it is different using the
large-sample sign test. Use a¼0.05.
12.3.7. It is desired to study the effect of a special diet on systolic blood pressure.
The following sample data are obtained for eight adults over 40 years of
age before and after six months of this diet.
608
CHAPTER 12 Nonparametric Tests

Before 185
222
235
198
224
197
228
234
After
188
217
229
190
226
185
225
231
At 95% confidence level, is there evidence to conclude that the new
diet reduces the systolic blood pressure in individuals of over 40 years old?
Test (a) using the sign test, and (b) using the Wilcoxon signed rank test.
Interpret the results.
12.3.8. In an effort to study the effect on absenteeism of having a day-care facility
at the workplace for women with newborn babies (less than 1 year old), a
large company compared the number of absent days for a year for seven
women with newborn children before and after instituting a day-care
facility.
Before 20 18 35 22 17 24 15
After
16
9 22 28 19 13 10
At 99% confidence level, is there evidence to conclude that having a
day-care facility at the workplace reduces absenteeism for women with
newborn children?
12.3.9. For a popular computer tablet, the user ratings (1 through 5 stars, with 5
start being highest rating) of 10 randomly selected are given as follows
5, 5, 1, 4, 3, 5, 4, 4, 5, 4
At the 0.05 level, is there evidence that the median rating is at least 4?
12.3.10. For the data of Exercise 12.2.10, does the combined evidence from all 16
studies suggest that developing acute renal failure as a complication of
sepsis impacts on mortality? Use a¼0.05. Do both sign test and Wilcoxon
signed rank test.
12.4 NONPARAMETRIC HYPOTHESIS TESTS FOR TWO
INDEPENDENT SAMPLES
In this section, we learn how to test the equality of the medians of two independent
samples from two populations. This is especially useful when one studies the treat-
ment effects, such as the effect of a certain drug to treat a given medical condition
when we have two groups—an experimental group and a control group—or the
effect of a particular type of teaching method. Even though this test can be used
for more than two samples, here, we will restrict to two samples. We will describe
the median test, which corresponds to the sign test, and the Wilcoxon rank sum test.
12.4.1 MEDIAN TEST
Let m1 and m2 be the medians of two populations 1 and 2, respectively, both with
continuous distributions. Assume that we have a random sample of size n1 from pop-
ulation 1 and a random sample of size n2 from population 2. The median test can be
summarized as follows.
609
12.4 Nonparametric Hypothesis Tests for Two Independent Samples

HYPOTHESIS TESTING PROCEDURE USING MEDIAN TEST
We test
H0 : m1 ¼ m2 versus
Ha :
m1 > m2, uppertailedtest
m1 < m2, lowertailedtest
m1 6¼ m2, twotailedtest:
1. Combine the two samples into a single sample of size n¼n1+n2, keeping track of each obser-
vation’s original population. Arrange the n1+n2 observations in increasing order and find the
median of this combined sample. If the median is one of the sample values, discard those obser-
vations and adjust the sample size accordingly.
2. Define N1b to be the number of observations of a sample from population 1.
3. Decision: If H0 is true, then we would expect N1b to be equal to some number around n1/2. For
Ha: m1>m2, rejection region is N1bc, where P(N1bc )¼a, for Ha: m1<m2, rejection region
is N1bc, where P(N1bc )¼a, and for Ha: m1¼m2, rejection region is N1bc1, or N1bc2,
where
P N1b  c1
ð
Þ ¼ a
2
and P N1b  c2
ð
Þ ¼ a
2:
Assumptions: (i) Population distribution is continuous. (ii) Samples are independent.
Note that since some observations can be equal to the overall median, and those
values will be discarded, N1b need not be equal to n1. Let n1+n2¼2k. Under H0, N1b
has a hypergeometric distribution given by
P N1b ¼ n1b
ð
Þ ¼
n1
n1b


n2
k n1b


n1 + n2
k


, n1b ¼ 0,1,2, ...,n1:
with the assumption that
i
j
 
¼ 0, ifj > i: Note that the hypergeometric distribution
is a discrete distribution that describes the number of “successes” in a sequence of n
draws from a finite population without replacement. Thus, we can find the values of c,
c1, and c2, required earlier. This calculation can be tedious. To overcome this, we can
use the following large sample approximation valid for n1>5 and n2>5. First classify
each observation as above or below the sample median as shown in Table 12.4.
It can be verified that the expected value and variance of N1a (similarly for N1b)
are given by
Table 12.4 Data Classification with Respect to Median
Below
Above
Totals
Sample 1
N1b
N1a
n1
Sample 2
N2b
N2a
n2
Total
Nb
Na
n1+n2¼n
610
CHAPTER 12 Nonparametric Tests

E N1a
ð
Þ ¼ Nan1
n
, andVar N1a
ð
Þ ¼ Nan1n2Nb
n2 n1
ð
Þ :
Thus, for a large sample we can write
:
z ¼ N1a E N1a
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Var N1a
ð
Þ
p
 N 0, 1
ð
Þ:
Hence, we can follow the usual large sample rejection region procedure, which is
summarized next.
SUMMARY OF LARGE SAMPLE MEDIAN SUM TEST (n1>5 AND n2>5)
We test
H0 : m1 ¼ m2 versus Ha :
m1 > m2, upper tailed test
m1 < m2, lower tailed test
m1 6¼ m2, twotailed test:
8
<
:
The test statistic:
Z ¼ N1a E N1a
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Var N1a
ð
Þ
p
,
where
E N1a
ð
Þ ¼ Nan1
n
and
Var N1a
ð
Þ ¼ Nan1n2Nb
n2 n1
ð
Þ :
Rejection region:
z > za,
upper tail RR
z < za, lower tail RR
jzj > za=2, two tail RR:
8
<
:
Decision: Reject H0, if the test statistic falls in the RR, and conclude that Ha is true with (1a)
100% confidence. Otherwise, do not reject H0, because there is not enough evidence to conclude that
Ha is true for a given a and more experiments are needed.
Assumptions: (i) Population distributions are continuous. (ii) n1>5 and n2>5.
We illustrate this procedure with the following example.
EXAMPLE 12.4.1
Given below are the mileages (in thousands of miles) of two samples of automobile tires of two
different brands, say I and II, before they wear out.
TireI : 34 32 37 35 42 43 47 58
59
62
69
71 78 84
TireII : 39 48 54 65 70 76 87 90 111 118 126 127
Use the median test to see whether tire II gives more median mileage than tire I. Use a¼0.05.
Continued
611
12.4 Nonparametric Hypothesis Tests for Two Independent Samples

Solution
We test
H0 : m1 ¼ m2 versus H0 : m1 < m2:
Because the sample size assumption is satisfied, we will use the large sample normal approx-
imation. The results of steps 1 and 2, using the notation A for above the median and B for below the
median, are given in Table 12.5.
The median is 63.5. Thus, we obtain Table 12.6.
Table 12.5 Data Classification of Mileage Data
Sample Values
Population
Above/Below the Median
32
I
B
34
I
B
35
I
B
37
I
B
39
II
B
42
I
B
43
I
B
47
I
B
48
II
B
54
II
B
58
I
B
59
I
B
62
I
B
65
II
A
69
I
A
70
II
A
71
I
A
76
II
A
78
I
A
84
I
A
87
II
A
90
II
A
111
II
A
118
II
A
126
II
A
127
II
A
Table 12.6 Summary of Mileage Data for Automobile Tires
Below
Above
Totals
Sample 1
N1b¼10
N1a¼4
n1¼14
Sample 2
N2b¼3
N2a¼9
n2¼12
Total
Nb¼13
Na¼13
n1+n2¼n¼26
612
CHAPTER 12 Nonparametric Tests

Also,
EN1a ¼ Nan1
n
¼ 13
ð
Þ 14
ð
Þ
26
¼ 7
and
Var N1a
ð
Þ ¼ Nan1n2Nb
n2 n1
ð
Þ ¼ 13
ð
Þ 13
ð
Þ 14
ð
Þ 12
ð
Þ
16,900
¼ 1:68:
Hence, the test statistic is
z ¼ N1a E N1a
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Var N1a
ð
Þ
p
¼ 47
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1:68
p
¼ 2:31:
For a¼0.05, z0.05¼1.645. Hence, the rejection region is {z<1.645}. Because the observed
value of z does fall in the rejection region, we reject H0 and conclude that there is enough evidence to
conclude that there is difference in the median mileage for the two types of tires.
12.4.2 THE WILCOXON RANK SUM TEST
The Wilcoxon rank sum test is used for comparing the medians of two independent
populations, as in the two-sample t-test in the parametric case. For accurate results,
it is necessary to assume that the variances of the populations are equal. This test is quite
similar totheWilcoxonsignedranktest.Whereastheone-sampleWilcoxon signedrank
test requires an additional assumption that the population distribution is symmetric,
such an assumption is not necessary for the two-sample Wilcoxon rank sum test. This
test can be applied for skewed distributions. The test is almost as powerful as the para-
metric version when the population distributions are close to normal. Many statistical
software packages do not give the Wilcoxon rank sum test; instead the Mann-Whitney
test is given. It should be noted that the Wilcoxon rank sum test is equivalent to the
Mann-Whitney U-test. We will not separately describe the Mann-Whitney test; how-
ever, in practice just perform the Mann-Whitney test if the software has only that test.
Assume that we have n1 observations randomly sampled from population I and n2
observations randomly sampled from population II with n1n2. The Wilcoxon rank
sum test procedure can be summarized as follows.
HYPOTHESIS TESTING PROCEDURE BY WILCOXON RANK SUM TEST
We test
H0 : m1 ¼ m2 versus H1 : m1 6¼ m2:
1. Combine the two samples into a single sample of size n1+n2, keeping track of each observation’s
original population. Arrange the n1+n2 observations in ascending order and assign ranks.
2. Sum the ranks of observations from population II and call it R.
3. Let the test statistic be W ¼ R 1
2n2 n2 + 1
ð
Þ:
4. Decision: If H0 is false, one would expect that the value of W would be very small or very large.
For a size a critical region, reject H0 if
W  c1,whereP W  c1
ð
Þ ¼ a
2,
Continued
613
12.4 Nonparametric Hypothesis Tests for Two Independent Samples

or
W  c2,whereP W  c2
ð
Þ ¼ a
2:
Note: The exact distribution of W is given in the Wilcoxon rank sum test table in the appendix
for small values of n1 and n2.
In the Wilcoxon rank sum test, based on the alternative hypothesis, we have the
following rejection regions.
For
Ha : m1 > m2, rejection regionis W  c, where P W  c
ð
Þ ¼ a:
and for
Ha : m1 < m2, rejection regionis W  c, where P W  c
ð
Þ ¼ a:
We will illustrate the foregoing procedure with the following example.
EXAMPLE 12.4.2
Comparison of the prices (in dollars) of two brands of similar automobile tires resulted in the data in
Table 12.7.
Use the Wilcoxon rank sum test with a¼0.05 to test the null hypothesis that the two population
medians are the same against the alternative hypothesis that the population medians are different.
Solution
Here we need to test
H0 : m1 ¼ m2 versus Ha : m1 6¼ m2:
The sample sizes are n1¼6, and n2¼8. Combining steps 1 and 2, we have the results shown in
Table 12.8.
The sum of ranks of observations from population II is R¼56. Hence, the test statistic is
W ¼ R1
2n2 n2 + 1
ð
Þ
¼ 561
2 8
ð Þ 9
ð Þ ¼ 20:
Table 12.7 Prices of Two Brands of Tires
Tire I:
85
99
100
110
105
87
Tire II:
67
69
70
93
105
90
110
115
Table 12.8 Ranking of Prices of Tires
Value
67
69
70
85
87
90
93
99
100
105
105
110
110
115
Population
II
II
II
I
I
II
II
I
I
I
II
I
II
II
Rank
1
2
3
4
5
6
7
8
9
10.5
10.5
12.5
12.5
14
614
CHAPTER 12 Nonparametric Tests

For a¼0.05, the rejection region is W9 or W>38, with the actual a being 0.0592. Because
the observed value of the test statistic does not fall in the rejection region, H0 is not rejected. Thus,
we do not have enough evidence to conclude that the median prices are different for these two brands
of automobile tires.
When the sample sizes are large and when H0 is true, the distribution of the Wilcoxon
rank sum test can be approximated by the normal distribution. It can be shown that
under H0, when both n1 and n2 are greater than 10, the distribution of W is approx-
imately normal with
E W
ð
Þ ¼ n1n2
2
and Var W
ð
Þ ¼ n1n2 n1 + n2 + 1
ð
Þ
12
:
For a large random sample, we can summarize the test procedure as follows.
SUMMARY OF LARGE SAMPLE MEDIAN SUM TEST (n1>10
AND n2>10)
We test
H0 : m1 ¼ m2 versus Ha :
m1 > m2, uppertailed test
m1 < m2, lowertailedtest
m1 6¼ m2, twotailedtest:
8
<
:
The test statistic:
Z ¼
W n1n2=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n1n2 n1 + n2 + 1
ð
Þ=12
p
:
Rejection region:
z > za,
uppertailRR
z < za, lowertailRR
jzj > za=2, two tailRR:
8
<
:
Assumption: The samples are independent and n1>10 and n2>10.
Decision: Reject H0, if the test statistic falls in the RR, and conclude that Ha is true with (1–a)
100% confidence. Otherwise, do not reject H0, because there is not enough evidence to conclude that
Ha is true for a given a and more data are needed.
We will use the foregoing procedure to solve the following problem.
EXAMPLE 12.4.3
In an effort to determine the immunoglobulin D (IgD) levels of a certain ethnic group, a large num-
ber of blood samples representing both sexes for 12-year-olds were taken. The following sample
data give the IgD levels (in mg/100 mL).
Male:
9.3
0.0
12.2
8.1
5.7
6.8
3.6
9.4
8.5
7.3
9.7
Female:
7.1
0.0
5.9
7.6
2.8
5.8
7.2
7.4
3.5
3.3
7.5
7.0
Continued
615
12.4 Nonparametric Hypothesis Tests for Two Independent Samples

Use the large sample Wilcoxon rank sum test with the significance level a¼0.01 to test the
hypothesis that there is no difference between the sexes in the median level of IgD.
Solution
We need to test
H0 : m1 ¼ m2 versus Ha : m1 6¼ m2:
Here, n1¼11, and n2¼12, and the results of step 1 and step 2 are given in Table 12.9, where we
use M or F to identify the population from which the data are coming.The sum of the ranks for
females is R¼114.5, and
W ¼ R1
2n2 n2 + 1
ð
Þ
¼ 114:51
2 12
ð
Þ 13
ð
Þ ¼ 36:5
Therefore, the test statistic results in
Z ¼
W n1n2=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n1n2 n1 + n2 + 1
ð
Þ=12
p
¼ 36:5 11
ð
Þ 12
ð
Þ=2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
11
ð
Þ 12
ð
Þ 24
ð
Þ=12
p
¼ 1:815  1:82:
For a¼0.01, we have za/2¼z0.005¼2.575. Hence, the rejection region is z<2.575 or
z>2.575. Because the test statistic does not fall in the rejection region, we do not reject H0 at
a¼0.01 and conclude that there is not enough evidence to conclude that there is any difference
between the sexes in the median level of IgD.
WithaslightmodificationoftherankingsystemintheWilcoxonranksumtest,wecould
test for the equality of variances when the normality assumption of the F-test fails.
EXERCISES 12.4
12.4.1. The following data give the winning proportions of the top six football
teams from each of the two conferences of the NFL.
American Conference
0.818
0.727
0.909
0.818
0.727
0.545
National Conference
0.636
0.545
0.636
0.636
0.818
0.455
Use the Wilcoxon rank sum test at the significance level of 0.05 to test
the null hypothesis that the two samples contain populations with identical
Table 12.9 Ranking of Immunoglobulin D (IgD) Levels
Value
0
0
2.8
3.3
3.5
3.6
5.7
5.8
5.9
6.8
7
7.1
M or F
M
F
F
F
F
M
M
F
F
M
F
F
Rank
1.5
1.5
3
4
5
6
7
8
9
10
11
12
Value
7.2
7.3
7.4
7.5
7.6
8.1
8.5
9.3
9.4
9.7
12.2
M or F
F
M
F
F
F
M
M
M
M
M
M
Rank
13
14
15
16
17
18
19
20
21
22
23
616
CHAPTER 12 Nonparametric Tests

medians against the alternative hypothesis that the medians are not equal.
State any assumptions you have made to solve the problem.
12.4.2. Comparison of two protective methods against corrosion yielded the
following maximum depths of pits (in thousandths of an inch) in pieces of
similar metals subjected to the respective treatments:
Method I:
68
75
69
75
70
69
72
Method II:
61
65
57
63
58
Use the Wilcoxon rank sum test at the significance level of 0.01 to test
the null hypothesis that the two samples have identical medians against the
alternative hypothesis that the medians are not equal.
12.4.3. Show that when H0 is true, the mean and variance of the Wilcoxon rank sum
test with sample sizes n1 and n2 are
E W
ð
Þ ¼ n1n2
2
and Var W
ð
Þ ¼ n1n2 n1 + n2 + 1
ð
Þ
12
:
12.4.4. In order to make inferences about the temporal muscles of the cat, a certain
dose of tubocurarine is injected into a random sample of nine cats. The
following data give the tetanus frequency (in hertz) in the temporal (T)
muscles before and after injection of tubocurarine.
T before
24
33
27
23
31
28
31
24
19
T after
27
38
34
32
37
28
35
28
41
Use the Wilcoxon rank sum test at the significance level of 0.05 to test
the null hypothesis that the median tetanus frequency (in hertz) in the
temporal (T) muscles is larger after injection of tubocurarine. State any
assumptions you made to solve the problem.
12.4.5. In a study of the net conversion of progesterone in rat liver, the following
samples were attained for the net conversion in rats 3-4 weeks old:
Male:
16.9 16.0 13.5 13.1 14.2 11.6 12.8 17.3 13.8 9.8 16.0 15.9 16.7 15.1
Female: 13.8 11.2 7.5
10.4 15.8 14.5 9.5
9.8
5.1
5.5 6.5
7.2
Use the large sample Wilcoxon rank sum test at the significance level of
0.05 to test the hypothesis that the median net conversion of progesterone in
male rats is larger than that in female rats. What would be your conclusion if
you were to use the median test?
12.4.6. Two groups of randomly selected 1-acre plots were treated with two
different brands of fertilizer. The following data give the yields of corn (in
bushels) from each of these plots.
Fertilizer I:
89
93
105
94
92
96
93
101
Fertilizer II:
85
88
94
87
86
91
617
12.4 Nonparametric Hypothesis Tests for Two Independent Samples

Use the data to determine whether there is a difference in yields for two
brands of fertilizers. Use a¼0.01. State any assumptions you made to solve
the problem.
12.4.7. The following information is obtained from two independent samples.
Sample 1:
15
8
12
4
10
8
13
7
12
6
14
11
Sample 2:
18
13
15
19
17
13
17
16
Test at 1% significance level that the median for sample 1 is less than the
median for sample 2 and interpret the meaning of your result.
12.4.8. In order to determine if a new hybrid seeding produces a bushier flowering
plant, data are collected on shrub girth (in inches) for both current variety
and hybrid plants resulted in the following values.
Current variety
27.7
25.1
35.4
36.5
22.0
30.5
hybrid
35.8
30.0
34.6
37.5
31.9
32.6
39.7
Test at 1% significance level that the median for sample 1 is different
from the median for sample 2 and interpret the meaning of your result.
12.5 NONPARAMETRIC HYPOTHESIS TESTS FOR k‡2
SAMPLES
In this section, we learn how to compare the medians of more than two independent
samples and to determine whether medians of the groups differ. These tests are non-
parametric alternatives to the analysis of variance (ANOVA) methods discussed in
Chapter 10. We study the Kruskal-Wallis test and Friedman test. Both of these
methods test the equality of the treatment medians.
12.5.1 THE KRUSKAL-WALLIS TEST
The Kruskal-Wallis test is a generalization of the Wilcoxon rank sum test for two
independent samples to several independent samples. This test is a nonparametric
alternative to one-way ANOVA. The Kruskal-Wallis test is almost as powerful as
the one-way ANOVA when the data are from a normal distribution, and more pow-
erful in case of nonnormality or in the presence of outliers. We now describe this test.
Suppose that we have k populations, with yi being the median of the population i
and k independent random samples from these populations. Let the samples from the
ith population be ni. We wish to test the equality of the medians of different groups—
that is, to test the hypothesis
H0 : y1 ¼ y2 ¼  ¼ yk ¼ 0 versus Ha : not all y’s equal 0:
We shall show that the hypothesis y1¼  ¼yk is equivalent to the hypothesis H0:
y1¼y2¼  ¼yk¼0. Let y1¼  ¼yk¼t (same number). Then the observations
yijt (i¼1,2,. . .,k) will be from a population with median zero. Because the
618
CHAPTER 12 Nonparametric Tests

Kruskal-Wallis test procedure depends only on the ranks of yij values in the com-
bined sample and the ranks of (yij–t) values are identical to those of yij values,
the two hypotheses are equivalent.
We summarize the Kruskal-Wallis procedure to solve this type of problem in the
following steps.
KRUSKAL-WALLIS TEST PROCEDURE
1. Combine and rank all N¼P
i¼1
n ni observations yij in ascending order. Also keep track of the
groups from which the observations came. Assign average ranks in case of ties. Let
rij ¼ rank yij


:
2. Calculate the group sum,
ri ¼
X
ni
i¼1
rij, i ¼ 1,2, ..., k:
and the group averages
ri ¼ ri
ni
, i ¼ 1,2, ..., k:
3. Let
r ¼
X
k
i¼1
ri ¼ N N + 1
ð
Þ
2
(this can be used as a check for accuracy of your calculation of ri0 s ) and let
r ¼ r
N ¼ N + 1
2
:
4. Calculate the Kruskal-Wallis test statistic
H ¼
12
N N + 1
ð
Þ
X
k
i¼1
ni ri r
ð
Þ2
or the convenient computational form of H,
H ¼
12
N N + 1
ð
Þ
X
k
i¼1
r2
i
ni
3 N + 1
ð
Þ:
Note that to compute the convenient form of H, there is no need to calculate ri and r.
5. Reject H0 if
H  c,
where the constant c is chosen to achieve a specified value for a.
The exact distribution of H is complicated. It depends on the sample sizes, n1, n2,
. . ., nk, and so it is not practical to tabulate its values beyond a small number of cases.
When k or N is large, the exact distribution of H under the null hypothesis can be
approximated by the chi-square distribution with (k–1) degrees of freedom. To this
effect, we state the Kruskal-Wallis theorem without proof.
619
12.5 Nonparametric Hypothesis Tests for k2 Samples

Theorem 12.5.1 When H0:y1¼y2¼  ¼yk is true, then as N becomes large, the
statistic
H ¼
12
N N + 1
ð
Þ
X
k
i¼1
ni ri r
ð
Þ2
has an asymptotic distribution that is chi-square with (k–1) degrees of freedom.
Thus, for approximate large samples the Kruskal-Wallis test for a given a is to
reject H0 if
H > w2
a k 1
ð
Þ:
The chi-square approximation is acceptable when the group sample sizes ni>5 with
k3. However, for convenience, we will use the chi-square approximation for all
values of ni. For this test, we follow the procedure described earlier except that
for finding the rejection region, we use the chi-square table.
The following example illustrates how we use the foregoing procedure to test the
appropriate hypothesis for three populations.
EXAMPLE 12.5.1
In an effort to investigate the premium charged by insurance companies for auto insurance, an
agency randomly selects a few drivers who are insured with three different companies. Assume that
these persons have similar autos, driving records, and level of coverage. Table 12.10 gives the pre-
miums paid per six months by these drivers with these three companies. Using the 5% level of sig-
nificance, test the null hypothesis that the median auto insurance premium paid per six months by all
drivers insured with each of these companies is the same.
Solution
Here we need to test
H0 : M1 ¼ M2 ¼ M3 ¼ 0 versus Ha : not all Mi’s equal 0,
where Mi is the true median of the auto insurance premium paid to company i, i¼1, 2, 3.
Here n1¼4, n2¼3, and n3¼5. Hence, there are N¼P
i¼1
3 ni¼12 observations. Let Y denote the
observations in ascending order. Table 12.11 gives the combined data in ascending order while
keeping track of the groups and their ranks.
Thus, the group rank sums are
r1 ¼ 24, r2 ¼ 23, and r3 ¼ 31:
Table 12.10 Auto Insurance Premium by Company
Company I
Company II
Company III
396
348
378
438
360
330
336
522
294
318
474
432
620
CHAPTER 12 Nonparametric Tests

As a check for accuracy of these calculations, note that
r1 + r2 + r3 ¼ 78 ¼ N N + 1
ð
Þ
2
¼ 12
ð
Þ 13
ð
Þ
2
:
The test statistic is given by
H ¼
12
N N + 1
ð
Þ
X
k
i¼1
r2
i
ni
3 N + 1
ð
Þ
¼
12
12
ð
Þ 13
ð
Þ
24
ð
Þ2
4
+ 23
ð
Þ2
3
+ 31
ð
Þ2
5
 
!
3 13
ð
Þ
¼ 0:42564:
From the chi-square table, w0.05
2
(2)¼5.991, and hence the rejection region is H5.991.
Because the observed value of H does not fall in the rejection region, we do not reject H0 and con-
clude that there is no evidence to show that the median auto insurance premiums paid per six months
by all drivers insured in each of these companies are different.
12.5.2 THE FRIEDMAN TEST
The Friedman test, named after the Nobel laureate economist Milton Friedman, tests
whether several treatment effects (measured as locations) are equal for data in a two-
way layout. We will assume that there are k different treatment levels and l blocks. In
each block, assign one experimental unit to each treatment level. We want to test
whether the true medians for different treatment levels are the same in each
block—that is, to test
H0 : True mediansat different levelsare all equal
versus
Ha : Not all themediansare equal:
Rather than combine the entire sample as in the Kruskal-Wallis statistic, here we
order the y-values within each block and then assign each its rank. In order to elim-
inate the differences due to blocks, we take the sum of ranks for each treatment level.
The following gives a summary of the procedure.
THE FRIEDMAN TEST PROCEDURE
1. Rank observations from k treatments separately within each block. Assign average ranks in case
of ties. Let Rij¼rank(Yij ), the rank of the observation for treatment level i in block j.
2. Calculate the rank sums
Ri ¼
X
l
j¼1
Rij, i ¼ 1,2, ..., k:
Continued
Table 12.11 Ranking of Auto Insurance Premium
Premium
294
318
330
336
348
360
378
396
432
438
474
522
Group
3
1
3
1
2
2
3
1
3
1
3
2
Rank
1
2
3
4
5
6
7
8
9
10
11
12
621
12.5 Nonparametric Hypothesis Tests for k2 Samples

3. Calculate the Friedman statistic
S ¼
12
lk k + 1
ð
Þ
X
k
i¼1
Ri l k + 1
ð
Þ
2

2
or a convenient computational form,
S ¼
12
lk k + 1
ð
Þ
X
k
i¼1
R2
i 3l k + 1
ð
Þ:
4. Reject H0 if Sc, where the constant c is chosen to achieve a specified value for a.
The exact distribution of S is complicated. Here, for k¼3, 4, 5, and for various
values of l, the Friedman distribution has been calculated and its values are given in
the table in the Table E.8. We will illustrate this four-step procedure with an
example.
EXAMPLE 12.5.2
Three classes in elementary statistics are taught by three different persons, a regular faculty member,
a graduate teaching assistant, and an adjunct from outside the university. At the end of the semester,
each student is given a standardized test. Five students are randomly picked from each of these clas-
ses, and their scores are given in Table 12.12. Test whether there is a difference between the scores
for the three persons teaching with a¼0.05.
Solution
Here we need to test
H0 : Median for the three persons scores are all equal
Ha : The medians are not equal
We are given a¼0.05, k¼3, and l¼5. To compute the value of the statistic S, we first assign
ranks for each student as shown in Table 12.13. Ha: Note that they are not all equal.
Thus, we have
R1 ¼ 12, R2 ¼ 11, and R3 ¼ 7,
and the test statistic is given by
S ¼
12
lk k + 1
ð
Þ
X
k
i¼1
R2
i 3l k + 1
ð
Þ
¼
12
5
ð Þ 3
ð Þ 4
ð Þ
12
ð
Þ2 + 11
ð
Þ2 + 7
ð Þ2


 3
ð Þ 5
ð Þ 4
ð Þ ¼ 2:8:
Table 12.12 Test Grades by Instructor
Faculty
Teaching Assistant
Adjunct
93
88
86
61
90
56
87
76
73
75
82
90
92
58
47
622
CHAPTER 12 Nonparametric Tests

From the Friedman table, the rejection region is S5.20 at an exact significance level of
0.092. Because the computed value of the test statistic does not fall in the rejection region, we
do not reject H0 and conclude that there is no difference in scores based on who teaches the course.
When the number of blocks, l, becomes large, the Friedman test statistic has an
approximate chi-square distribution under the null hypothesis. That is:
Theorem 12.5.2 When H0: y1¼y2¼  ¼yk is true then, as l becomes large,
S ¼
12
lk k + 1
ð
Þ
X
k
i¼1
Ri l k + 1
ð
Þ
2

2
has an asymptotic distribution that is chi-squared with (k–1) degrees of freedom.
Thus, for an approximate large random sample, the Friedman test for given a is to
reject H0 if S>wa
2(k1).
When the values of k and l exceed the values given in the Friedman table, we
could use the chi-square approximation, which gives acceptable results. We proceed
to illustrate the Friedman test with the following example.
EXAMPLE 12.5.3
In the previous example, we now randomly select 10 student grades from each class, resulting in the
data shown in Table 12.14.
Continued
Table 12.13 Ranks of Test Scores by Instructor
Faculty
Teaching Assistant
Adjunct
3
2
1
2
3
1
3
2
1
1
2
3
3
2
1
Table 12.14 Test Grades of 10 Random Students from
Each Instructor
Faculty
Teaching Assistant
Adjunct
93
88
86
61
90
56
87
76
73
75
82
90
92
58
47
45
74
88
99
23
77
86
61
18
82
60
66
74
77
55
623
12.5 Nonparametric Hypothesis Tests for k2 Samples

Test whether there is a difference between the scores for the three persons teaching. Use
a¼0.05.
Solution
Here we need to test
H0 : The true median scores for the three instructors are all equal
versus
Ha : They are not all equal:
We are given a¼0.05, k¼3, and l¼10. We use the chi-square approximation to solve the prob-
lem. To compute the value of the statistic S we first assign ranks for each student as shown in
Table 12.15. The Friedman test statistic is
S ¼
12
lk k + 1
ð
Þ
X
k
i¼1
R2
i 3l k + 1
ð
Þ
¼
12
10
ð
Þ 3
ð Þ 4
ð Þ
24
ð
Þ2 + 20
ð
Þ2 + 16
ð
Þ2


 3
ð Þ 10
ð
Þ 4
ð Þ ¼ 3:2:
From the chi-square table, w0.05
2
(2)¼5.992. Hence, the rejection region is S5.992. The com-
puted value of the test statistic does not fall in the rejection region, and we do not reject H0. We
conclude that there is no difference in scores based on who teaches the course.
Friedman’s test is an alternative to the repeated measures ANOVA, when assump-
tions such as that of normality or equality of variance are not satisfied. Because this
test, like many other nonparametric tests, does not make a distribution assumption, it
is not as powerful as the ANOVA.
EXERCISES 12.5
12.5.1 Table 12.16 shows a random sample of observations on children under
10 years of age, each observation being the IgA immunoglobulin level
Table 12.15 Ranks of Test Scores of 10 Random Students
Faculty
Teaching Assistant
Adjunct
3
2
1
2
3
1
3
2
1
1
2
3
3
2
1
1
2
3
3
1
2
3
2
1
3
1
2
2
3
1
Total
24
20
16
624
CHAPTER 12 Nonparametric Tests

measured in international units from a large number of blood samples, and
the population is studied in blocks in terms of age groups (the upper value is
not included) as I: (1 to 3), II: (3 to 6), III: (6 to 8), and IV: (8 to 10). Test for
the hypothesis of equality of true medians for IgA level in each block (age
level), (a) with the 5% level and (b) with the 1% level of significance.
Compare the results obtained.
12.5.2. In an effort to study the effect of four different preventive maintenance
programs on downtimes (in minutes) for a certain period of time in a
production line, a factory runs four parallel production lines, and each line
has five different types of machine. The different maintenance programs are
randomly assigned to each of the four production lines so as to treat the
various machines as blocks. Results are shown in Table 12.17.
Test the hypothesis, H0: True medians of the four maintenance programs
are equal versus Ha: Not all are equal. [Hint: In the Friedman test, k¼4, and
l¼6.] State any assumptions you have made to solve this problem.
12.5.3. Show that, when k¼2, the Kruskal-Wallis statistics,
H ¼
12
N N + 1
ð
Þ
X
k
i¼1
r2
i
ni
3 N + 1
ð
Þ
becomes equivalent to the Wilcoxon rank sum test.
12.5.4. A consumer testing agency is interested in determining whether there is a
difference in the mileage for three brands of gasoline. To test this, four
different vehicles are driven with each of these gasoline. Results are shown
in Table 12.18.
Test whether there is a difference between the three gasoline medians at
the 0.05 level.
Table 12.17 Downtimes by Program
Machine
Method 1
Method 2
Method 3
Method 4
I
181
124
126
181
II
185
122
125
160
III
67
65
68
69
IV
121
66
120
68
V
62
60
62
65
Table 12.16 IgA Immunoglobulin Level of Children
I
6
37
19
14
51
68
27
75
II
32
65
76
42
45
41
38
63
III
73
75
59
90
37
32
63
80
IV
81
42
48
60
98
100
79
45
625
12.5 Nonparametric Hypothesis Tests for k2 Samples

12.5.5. In order to study the effect of fertilizers, five groups of 1-acre plots were
randomly selected. One group was not treated with any fertilizers and the
remaining four groups were treated with four different brands of fertilizers.
Table 12.19 gives the yields of corn (in bushels) from each of these plots.
Use the data to determine whether there is a difference in yields for
different fertilizers. Use a¼0.01.
12.5.6. In order to compare grocery prices of four different grocery stores on a
particular day in November 1999, 11 randomly selected items with same
brands are given in Table 12.20.
Table 12.19 Yield by Fertilizer
None:
58
27
36
41
48
36
50
50
39
Fertilizer I:
69
67
57
63
49
65
78
69
Fertilizer II:
95
92
92
89
100
88
79
97
75
Fertilizer III:
102
111
92
103
102
94
100
112
96
Fertilizer IV:
127
115
112
122
114
107
116
112
108
Table 12.20 Grocery Prices by Store
Product
Store A
Store B
Store C
Store D
Bread (20 oz)
$1.39
$1.39
$1.39
$1.39
Red apple (1 lb)
1.29
1.29
0.99
0.68
Large eggs (1 dozen)
0.69
0.88
0.89
0.89
Orange Juice (64 oz)
3.29
2.99
2.79
2.69
Cereal (15 oz)
3.59
3.19
3.19
3.58
Canned corn (15.25 oz)
0.50
0.53
0.50
0.49
Crystals sugar (5 lb)
1.99
2.09
1.99
1.89
2% milk (1 gal)
3.19
3.19
3.09
3.09
Frozen pizza (21.5 oz)
3.00
4.59
3.50
3.50
Puppy Chow (4.4 lb)
4.59
3.69
3.69
3.99
Diapers (56-pack)
12.99
12.99
12.99
11.88
Table 12.18 Mileage by Gasoline Type
Vehicle
Gasoline
A
B
C
I
19
25
22
II
26
33
39
III
20
28
25
IV
18
30
21
626
CHAPTER 12 Nonparametric Tests

Use the data to determine whether there is a difference in prices at these
four grocery store chains. Use a¼0.01. State any assumptions you have
made to solve this problem.
12.6 CHAPTER SUMMARY
In this chapter, we first learned about nonparametric approaches to interval estima-
tion and nonparametric hypothesis tests for one sample, such as the sign test, the Wil-
coxon signed rank test, and dependent sample paired comparison tests. Then
nonparametric hypothesis tests for two independent samples such as the median test
and Wilcoxon rank sum test were considered. Later the Kruskal-Wallis test and the
Friedman test were explained for more than two samples.
It is natural to ask, “Why do we substitute a set of nonnormal numbers, such as
ranks, for the original data?” Few data are truly normal. Rank tests are sometimes
called “approximate” tests. They are most useful in instances when we suspect that
the data are not normal, and we either cannot transform the data to make them more
normal, or do not like to do so. One of the simple ways to check for appropriateness of
use of nonparametric tests is to simply construct a stem-and-leaf display or a histogram
for the sample data and see whether they look symmetric and approximately bell
shaped. If this is not so, we may often be better off using a nonparametric approach.
Since the 1940s, many nonparametric procedures have been introduced, and the
number of procedures continues to grow. The nonparametric tests presented in this
chapter represent only a small portion of available nonparametric tests. There are
many references available in the bibliography for further reading on the subject.
In this chapter, we have also learned the following important concepts and
procedures.
•
Procedure for finding (1–a)100% confidence interval for the median M
•
Hypothesis testing procedure by sign test
•
A large sample sign test
•
Hypothesis testing procedure by Wilcoxon signed rank test
•
Summary of large sample Wilcoxon signed rank test (n>20)
•
Summary of large sample median sum test (n1>5 and n2>5)
•
Hypothesis testing procedure by Wilcoxon rank sum test
•
Summary of large sample Wilcoxon rank sum test (n1>10 and n2>10)
•
Kruskal-Wallis test procedure
•
Friedman test procedure
12.7 COMPUTER EXAMPLES
In this section, we illustrate some nonparametric procedures using statistical soft-
ware packages.
627
12.7 Computer Examples

Fail to reject the null hypothesis since
the p-value is larger than our alpha.
This suggests the median difference is zero.
We fail to reject the null hypothesis 
suggesting that the true mean is equal to 1.4
for any reasonable level of significance. 
Our p-value suggests that we fail to
reject the null hypothesis for any 
reasonable level of significance and
that the medians are equal.
12.7.1 EXAMPLES USING R
EXAMPLE 12.7.1
(Sign test): Using the following data test H0:M¼1.4 versus Ha:M>1.4,using the sign test.
Sample (x): 1.51 1.35 1.69 1.48 1.29 1.27 1.54 1.39 1.45
R Code:
y¼length(which(x>1.4));
n¼length(x);
binom.test(y,n,alternative¼“greater”);
Output:
Exact binomial test
data: y and n
number of successes¼5, number of trials¼9, p-value¼0.5
alternative hypothesis: true probability of success is greater than 0.5
95 percent confidence interval:
0.2513676 1.0000000
sample estimates:
probability of success
0.5555556
EXAMPLE 12.7.2
(Wilcoxon Test): Using the data from the previous example test H0:M¼1.4 versus Ha:M6¼1.4,
using one-sample Wilcoxon test.
R Code:
wilcox.test(x,mu¼1.4);
Output:
Wilcoxon signed rank test
data: x
V¼30, p-value¼0.4258
alternative hypothesis: true location is not equal to 1.4
EXAMPLE 12.7.3
(Two-sample Sign Test): Using the following data, test H0: M¼0 versus Ha: M<0, using the
two-sample sign test, where M is the median difference. Use a¼0.05.
Sample (x) : 180 199 175 226 189 205 169 211
Sample (y) : 172 191 172 230 178 199 171 201
R Code:
z¼x-y;
y¼length(which(z<0));
n¼length(z);
binom.test(y,n,alternative¼”less”);
Output:
Exact binomial test
data: y and n
number of successes¼2, number of trials¼8, p-value¼0.1445
alternative hypothesis: true probability of success is less than 0.5
628
CHAPTER 12 Nonparametric Tests

Fail to reject the null 
hypothesis 
95 percent confidence interval:
0.0000000 0.5996894
sample estimates:
probability of success
0.25
EXAMPLE 12.7.4
(Wilcoxon Two-sample Test): Use the Wilcoxon rank sum test with a¼0.05 to test the null hypoth-
esis that the two population medians are the same against the alternative hypothesis that the pop-
ulation medians are different.
Sample (x) : 85 99 100 110 105 87
Sample (y) : 67 69 70 93 105 90 110 115
R Code:
wilcox.test(x,y);
Output:
Wilcoxon rank sum test with continuity correction
data: x and y
W¼28, p-value¼0.6507
alternative hypothesis: true location shift is not equal to 0
EXAMPLE 12.7.5
(Kruskall-Wallis Test): In an effort to investigate the premium charged by insurance companies for
auto insurance, an agency randomly selects a few drivers who are insured by three different com-
panies. Assume that these persons have similar cars, driving records, and levels of coverage. The
following data is the premiums paid per 6 months by these drivers with these three companies. Using
a¼0.05, test the null hypothesis that the median auto insurance premium paid per 6 months by all
drivers insured in each of these companies is the same.
Company
C1
C1
C1
C1
C2
C2
C2
C3
C3
C3
C3
C3
Value
396
438
336
318
348
360
522
378
330
294
474
432
R Code:
kruskal.test(value, company);
Output:
Kruskal-Wallis rank sum test
data: value and company
Kruskal-Wallis chi-squared¼0.4256, df¼2, p-value¼0.8083
A large p -value suggests we
fail to reject thefnull hypothesis
EXAMPLE 12.7.6
(Friedman test): Using the following data conduct a Friedman test.
C1
93
61
87
75
92
45
99
86
82
74
C2
88
90
76
82
58
74
23
61
60
77
C3
86
56
73
90
47
88
77
18
66
55
Continued
629
12.7 Computer Examples

R Code:
blocks¼c(c(1:10), c(1:10), c(1:10));
A data set called blocks contains
matching block data ranged 1 to 10
friedman.test(values, groups, blocks);
Output:
Friedman rank sum test
data: values, groups and blocks
Friedman chi-squared¼3.2, df¼2, p-value¼0.2019
12.7.2 MINITAB EXAMPLES
EXAMPLE 12.7.7
(One-sample sign): For the data
1:51 1:35 1:69 1:48 1:29 1:27 1:54 1:39 1:45
test H0: M¼1.4 versus Ha: M>1.4, using sign test.
Solution
Enter data in C1. Then
Stat>Nonparametric>1-Sample Sign . . .>in Variables: type C1>click Test median: type
1.4>in Alternative: click greater than>click OK
We can obtain the nonparametric confidence interval using the following procedure.
Enter in variable, C1, and then
Stat>Nonparametric>1-Sample Sign . . .>in Variables: type C1>click Confi-
dence interval>in level: enter appropriate, say, 95.0>Click OK
EXAMPLE 12.7.8
(One-sample Wilcoxon): For the data
1:51 1:35 1:69 1:48 1:29 1:27 1:54 1:39 1:45
test H0: M¼1.4 versus Ha: M6¼1.4, using one-sample Wilcoxon test.
Solution
We will give only Sessions commands; the Windows procedure is similar to the previous example.
Stat>Nonparametric>1-Sample Wilcoxon. . .>in Variables: type C1>click Test median:
type 1.4>in Alternative: click not equal>click OK
EXAMPLE 12.7.9
(Two-sample Sign Test): For the data
Sample 1
180
199
175
226
189
205
169
211
Sample 2
172
191
172
230
178
199
171
201
test H0: M¼0 versus Ha: M<0, using the two-sample sign test, where M is the median of the dif-
ference. Use a¼0.05.
630
CHAPTER 12 Nonparametric Tests

Solution
After entering sample 1 data in C1 and sample 2 data in C2, we can use the following sequence:
Calc>Calculator...>in Store result in variable: type C3>in Expression: type C2-C3>click OK
We will get the pairwise difference of the two samples. For these values, we will apply the one-
sample sign test.
Stat>Nonparametric>1-sample sign. . .>in Variables: type C3>click Test median : and in
Alternative: choose less than>click OK
EXAMPLE 12.7.10
(Kruskal-Wallis test): In an effort to investigate the premium charged by insurance companies for
auto insurance, an agency randomly selects a few drivers who are insured by three different com-
panies. Assume that these persons have similar cars, driving records, and levels of coverage.
Table 12.21 gives the premiums paid per 6 months by these drivers with these three companies.
Using the 5% significance level, test the null hypothesis that the median auto insurance pre-
mium paid per 6 months by all drivers insured in each of these companies is the same. Use Minitab.
Solution
Enter data for company I in C1, for company II in C2, and for company III in C3. First stack the data
while keeping track of the companies in the following way.
Manip>Stack/Unstack>Stack Columns. . .>in Stack the following columns: type C1 C2 C3>in
Stored data in: type C4>in Store subscripts in: type C5>Click OK
Now we can use Kruskal-Wallis as follows.
Stat>Nonparametric>Kruskall-Wallis.. .>in Response: type C4>in Factor: type C5>click OK
We will get the output shown in Table 12.22.
Because the p-value of 0.808 is larger than a¼0.05, we cannot reject the null hypothesis.
Table 12.21 Auto Insurance Premium by Company
Company I
Company II
Company III
396
348
378
438
360
330
336
522
294
318
474
432
Table 12.22 Kruskal-Wallis Test
Kruskal-Wallis Test on C4
C5
N
Median
Ave rank
Z
1
4
366.0
6.0
0.34
2
3
360.0
7.7
0.65
3
5
378.0
6.2
0.24
Overall
12
6.5
H¼0.43 DF¼2 P¼0.808
*NOTE*One or more small samples
631
12.7 Computer Examples

EXAMPLE 12.7.11
(Friedman test): For the following data, conduct a Friedman test.
93 61 87 75 92 45 99 86 82 74
88 90 76 82 58 74 23 61 60 77
86 56 73 90 47 88 77 18 66 55
Solution
Enter each row of data in C1, C2, and C3 respectively. Then stack the data in C1, C2, and C3 in the
following way.
Manip>Stack/Unstack>Stack Columns. . .>in Stack the following columns: type C1 C2 C3>in
Stored data in: type C4>in Store subscripts in: type C5>Click OK
In C6, enter numbers 1 through 10 in the first 10 rows, enter numbers 1 through 10 in the next 10
rows, and enter numbers 1 through 10 in the following 10 rows. Now we can use the Friedman test as
follows.
Stat>Nonparametric>Friedman. . .>in Response: type C4>in Treatment: C5>in Blocks: type
C6>click OK
We will get the output shown in Table 12.23.
Because the p-value is 0.202, for any value of a<0.202, we cannot reject the null hypothesis.
12.7.3 SPSS EXAMPLES
EXAMPLE 12.7.12
(Wilcoxon rank sum test): For the data of Example 12.4.2, use the Wilcoxon rank sum test at the
significance level of 0.05 to test the null hypothesis that the two population medians are the same
against the alternative hypothesis that the population medians are different. Use an SPSS procedure.
Solution
Because the SPSS pull-down menu does not have the Wilcoxon rank sum test, we will use the Mann-
Whitney U-test. The Mann-Whitney U-test is equivalent to the Wilcoxon rank sum test, although we
calculate it in a slightly different way. For the same data set, any p-values generated from one test
will be identical to those generated from the other. The following gives the steps to follow. Enter tire
brands as 1 to identify brand 1 and 2 to identify brand 2, in C1. Enter the corresponding prices in
C2. Name C1 as Brand and C2 as Price. Then click
Analyze>Nonparametric Tests>2 Independent Samples. . .>move Brand to Grouping Variable:
and Price to Test Variable list: > click Define Groups. . .>enter 1 in Group 1:, and 2 in Group 2: >
click continue>choose Mann-Whitney U>OK
Table 12.23 Friedman Test for C4 by C5 Blocked by C6 S¼3.20 DF¼2
P¼0.202
C5
N
Est median
Sum of Ranks
1
10
81.500
24.0
2
10
72.000
20.0
3
10
68.000
16.0
Grand median¼73.833
632
CHAPTER 12 Nonparametric Tests

We get the following output:
Mann-Whitney Test
Ranks
BRAND
N
Mean Rank
Sum of Ranks
PRICE
1.00
6
8.17
49.00
2.00
8
7.00
56.00
Total
14
Test Statistics
PRICE
Mann-Whitney U
20.000
Wilcoxon W
56.000
Z
–0.518
Asymp. Sig. (2-tailed)
0.605
Exact Sig. [2*(1-tailed Sig.)]
0.662
(a) Not corrected for ties.
(b) Grouping Variable: BRAND
In the first table just shown, ranks show the mean ranking of tire brand I and tire brand II. The
Mann-Whitney test is used to assess whether the distribution of ranks is statistically significant.
Under the null hypothesis, the distribution of ranks should be the same for both groups. Looking
at the second table, the calculated value of the Mann-Whitney U is 20. The value U represents
the amount by which the ranks for tire brand I and tire brand II deviate from what we would expect
under the null hypothesis. For a 0.05 significance level, we can reject the null hypothesis if the 2-
tailed significance (see Asymp. sig in the second table) is less than 0.05. In this case, because Asymp.
Sig. (2-tailed)¼0.605, we do reject the null hypothesis.
EXAMPLE 12.7.13
(Kruskal-Wallis test): For the data of Example 12.5.1, conduct the Kruskal-Wallis test using SPSS.
Solution
Enter insurance companies as 1 to identify company I, 2 to identify company II, and 3 to identify
company III, in C1. Enter the corresponding premiums in C2. Name C1 as Company, and C2
as Premium. Then:
Analyze>Nonparametric Tests>K Independent Samples. . .>move Premium to Test Variable
List: and Company to Grouping variable: > click Define Rage. . .>enter 1 in Minimum, and 3
in Maximum>click continue>click Kruskal-Wallis H>OK
If we need to do a Friedman test, say for the data of Example 12.7.5, enter each row of data in C1,
C2, and C3, respectively. Then use the following sequence to obtain the appropriate output.
Analyze>Nonparametric Tests>K Related Samples. . .>move each of the three columns to Test
Variables: > check in Test Type Friedman>OK
12.7.4 SAS EXAMPLES
To perform the nonparametric tests, use the SAS statement PROC NPAR1WAY. In
the procedure, if we include the EXACT statement, the program will compute the
exact p-value computations for the Wilcoxon rank sum test.
633
12.7 Computer Examples

EXAMPLE 12.7.14
(Wilcoxon rank sum test): Comparison of the prices (in dollars) of two brands of similar tires gave
the following data.
Tire I:
85
99
100
110
105
87
Tire II:
67
69
70
93
105
90
110
115
Use the Wilcoxon rank sum test at the significance level of 0.05 to test the null hypothesis that
the two population medians are the same against the alternative hypothesis that the population
medians are different. Use the SAS procedure.
Solution
We can use the following procedure.
options nodate nonumber;
DATA tprice;
INPUT Brand Price @@;
CARDS;
1 85 1 99 1 100 1 110 1 105 1 87
2 67 2 69 2 70 2 93 2 105 2 90 2 110 2 115
;
/* Nonparametric statistics/Wilcoxon Rank-Sum */
PROC NPAR1WAY DATA¼tprice WILCOXON;
CLASS Brand;
VAR Price;
EXACT WILCOXON;
run;
EXAMPLE 12.7.15
(Kruskal-Wallis test): For the data of Example 12.7.4, perform the Kruskal-Wallis test using SAS.
Solution
We can use the following code.
options nodate nonumber;
DATA insprice;
INPUT Company Price @@;
CARDS;
1 396 1 438 1 336 1 318
2 348 2 360 2 522
3 378 3 330 3 294 3 474 3 432
;
proc npar1way data¼insprice;
class company;
var Price;
run;
634
CHAPTER 12 Nonparametric Tests

PROJECTS FOR CHAPTER 12
12A. COMPARISON OF WILCOXON TESTS WITH NORMAL
APPROXIMATION
(i) For the Wilcoxon signed rank test, compare the results from the Wilcoxon
signed rank test table with the normal approximation using several sets of
data of various sample sizes. Also, if the sample size is very small, compare the
results from the Wilcoxon signed rank test with a small sample t-test.
(ii) For the Wilcoxon rank sum test, compare the results from the Wilcoxon rank
sum test table with the normal approximation using several sets of data
(from pairs of samples) of various sample sizes. Also, if the sample sizes are
very small, compare the results from the Wilcoxon rank sum test with small
sample t-test for two samples.
12B. RANDOMNESS TEST (WALD-WOLFOWITZ TEST)
When we have no control over the way in which the data are selected, it is useful to
have a technique for testing whether the sample may be looked on as random. The
condition of randomness is essential for all of the analysis explained in this book: that
is, whether a sequence of random variables X1, . . ., Xn are independent based on a set
of observations x1, . . ., xn of these random variables. Here we will give a method
based on the number of runs displayed in the sample events. This is a nonparametric
procedure. The run test is used to test the randomness of a sample at 100(1–a)%
confidence level.
Given a sequence of two symbols, say H and T, a run is defined as a succession of
identical symbols contained between different symbols or none at all. The total num-
ber of runs in a sequence of n trials serves as an indication whether the arrangement is
random or not. If a sequence contains n1 symbols of one kind and n2 symbols of
another kind and both n1 and n2 are greater than 10 (this is a rule of thumb; for more
accuracy we can also take both n1 and n2 as greater than 20), then the sampling
distribution of the total number of runs, R, has an asymptotic normal distribution with
mean
mR ¼ 2n1n2
n1 + n2
+ 1
and variance
s2
R ¼ 2n1n2 2n1n2 n1 n2
ð
Þ
n1 + n2
ð
Þ2 n1 + n2 1
ð
Þ
:
For example, if we have the following symbols
HHH T HH TTTT HH TTT
635
Projects for Chapter 12

there are six runs indicated by the underlines and n1¼7 and n2¼8. If the sample
contains numerical data, the run test is used by counting runs above and below
the median. Denoting the observations above the median by the letter A and obser-
vations below the median by the letter B, we can determine the run as before. For
example, if we have data values
2 5 11 13 7 22 6 8 15 9
then the median is 8.5. Hence, we get the following arrangement of values above and
below the median:
BB AA B A BB AA:
Hence, there are six runs with n1¼5 and n2¼5.
Now we can formulate the test of randomness as a hypothesis testing problem as
described in the following procedure.
PROCEDURE FOR TEST OF RANDOMNESS USING RUN TEST
To test
H0 : Arrangement of samplevaluesis random
versus
Ha : Dataare notrandom:
1. Compute the median of the sample.
2. Going through the sample values, replace any observation with A if the value is above the
median, or B if the value is below the median. Discard any ties.
3. Compute n1, n2, and R. Also, compute the mean and variance of R.
mR ¼ 2n1n2
n1 + n2
+ 1,
s2
R ¼ 2n1n2 2n1n2 n1 n2
ð
Þ
n1 + n2
ð
Þ2 n1 + n2 1
ð
Þ
:
4. Compute the test statistic:
Z ¼ RmR
sR
:
5. Rejection region:
Z
j j > za=2:
6. Decision: If the test statistic falls in the rejection region, reject H0 and conclude that the sample is
not random with (1–a) 100% confidence.
Assumption: n110 and n210.
Note 1: Sometimes the same procedure is used with the median replaced by the
mean of the sample. That is, if the observation is above the sample, use A, and if it is
below the sample, use B. We use this procedure for large samples. For small sample
sizes, to determine the upper and lower critical values, a special table is needed.
Some statistical software packages have the ability to use the run test for randomness.
For example, in Minitab we can use following procedure.
636
CHAPTER 12 Nonparametric Tests

Enter the data that we want to test for randomness in C1. Then:
Stat>Nonparametric>Runs Test. . .>In variables: enter C1>OK
Default in Minitab is a run test with the mean. If we prefer median, type the value
of the median by first clicking Above and below:.
EXAMPLE 12.B.1
The following table gives radon concentration in pCi/L obtained from 40 houses in a certain area.
2:9
0:6
13:5
17:1
2:8
3:8
16:0
2:1
6:4
17:2
7:9
0:5
13:7
11:5
2:9
3:6
6:1
8:8
2:2
9:4
15:9
8:8
9:8
11:5
12:3
3:7
8:9
13:0
7:9
11:7
6:2
6:9
12:8
13:7
2:7
3:5
8:3
15:9
5:1
6:0
Test using Minitab (or some other software) whether the data are random at 95%
confidence level.
Solution
Running the data with Minitab, we get the following output.
radon
K¼8.3400
The observed number of runs¼17
The expected number of runs¼20.9500
19 Observations above K 21 below
The test is significant at 0.2046
Cannot reject at alpha¼0.05
Thus the data set is a random sample at 95% confidence level.
Note 2: If the large samples assumption is not satisfied (that is, n1<10 and
n2<10, for more accuracy use 20 instead of 10), then use the total number of runs,
R, itself as the test statistic and we can find lower and upper critical values for a given
a from Frieda S. Swed and C. Eisenhart Tables for testing randomness of grouping in
a sequence of alternatives, Annals of Mathematical Statistics, 14, 83–86, 1943. We
will not be giving this table in this book.
Exercise
Pick a couple of data sets from this book or your own and test for randomness
using (i) hand calculations, and (ii) a statistical software package.
637
Projects for Chapter 12

CHAPTER
Empirical Methods
13
CHAPTER CONTENTS
13.1 Introduction .................................................................................................. 640
13.2 The Jackknife Method .................................................................................... 640
13.3 An Introduction to Bootstrap Methods ............................................................. 645
13.4 The Expectation Maximization Algorithm ......................................................... 651
13.5 Introduction to Markov Chain Monte Carlo ...................................................... 662
13.6 Chapter Summary .......................................................................................... 678
13.7 Computer Examples ....................................................................................... 679
Projects for Chapter 13 .......................................................................................... 686
OBJECTIVE
In this chapter, we introduce several empirical methods that are being increasingly
used in statistical computations as an alternative or as an improvement to classical
statistical methods.
Stanislaw Ulam
(Source: http://scienceworld.wolfram.com/biography/Ulam.html)
Stanislaw Ulam (1909-1986) was a Polish-American mathematician who was
born in Lwo´w, Poland, and came to the United States in 1936. He worked at Prin-
ceton University. He was involved with the Manhattan Project to build the first
atomic bomb. Ulam solved the problem of how to initiate fusion in the hydrogen
Mathematical Statistics with Applications in R
Copyright © 2015 Elsevier Inc. All rights reserved.
639

bomb. Ulam was interested in astronomy, physics, and mathematics from an early
age. He obtained his Ph.D. from the Polytechnic Institute in Lwo´w in 1933, where
he studied under a famous mathematician named Banach. Ulam’s writing included A
Collection of Mathematical Problems (1960), Sets, Numbers and Universes (1974),
and Adventures of a Mathematician (1976). His major contribution to statistics is
through the introduction of the Monte Carlo methods along with Metropolis in
1949. These methods are widely used in solving mathematical problems using sta-
tistical sampling. Monte Carlo methods became widely popular with the ever-
increasing power of computers and the development of specialized mathematical
and statistical software.
13.1 INTRODUCTION
In statistics, major efforts are made to develop and study accurate statistical models
that are able to describe natural phenomena. The dilemma is whether to use the stan-
dard model that may allow closed-form solutions, or to describe the phenomenon
more accurately, which would often preclude the computation of explicit answers.
Obtaining methods that result in useful qualitative and quantitative understanding
of realistic complex systems is difficult, and obtaining exact analytical tools is
not practical either. Because of this problem, practitioners have relied on
simulation-based methods. Computer simulation methods are becoming tools of
choice for problems in statistics. Most of the empirical methods discussed in this
chapter had been in existence in the statistical literature as possible numerical
methods for some time. Because of the difficulty of computing by hand, these
methods did not gain much popularity. These numerical techniques became popular
and practical with the advent of high-quality pseudorandom number generators and
high-speed computers. Modern statistics is increasingly being equipped with
theoretical concepts complemented with effective computational tools to handle
the challenges that arise in science and technology. The methods presented in this
chapter could be effectively used for Bayesian computation and for problems arising
in such diverse areas as environmental modeling, epidemiology, finance, genetics,
image analysis, and statistical physics.
It is important to note that the literature on these simulation methods is growing,
and it is impossible to present the whole picture in a single chapter. The purpose of
this chapter is only to introduce some basic and popular computational methods.
There are many specialized books for further study.
13.2 THE JACKKNIFE METHOD
It was Tukey who in 1958 gave the name “jackknife” (sometimes also known as the
Quenouille-Tukey jackknife) to a general statistical method, invented by Maurice
Quenouille in 1956, for testing hypotheses and finding confidence intervals where
640
CHAPTER 13 Empirical Methods

traditional methods are not applicable or not well suited. In general usage, a jackknife
is a large clasp knife that has a multitude of small pull-out tools. Because this method
could be used for small tasks without resorting to other tools, it was named the jack-
knife. The jackknife method could also be used with multivariate data. However,
here we will only present the method for univariate data. The jackknife procedure
is very useful when outliers are present in the data or the dispersion of the distribution
is wide. In the jackknife method, we systematically recompute the statistic, leaving
out one observation at a time from the observed sample. This is used to estimate the
variability of statistic from the variability of that statistic between subsamples. This
avoids the parametric assumptions that we used in obtaining the sampling distribu-
tion of the statistic to calculate standard error. Thus, this can be considered as a non-
parametric estimate of the parameter. Initially, the jackknife method was introduced
for bias reduction (thus improving a given estimator) and is a useful method for var-
iance estimation. In this section, we study only how to compute a jackknife estimate
and a confidence interval. We do not discuss how it reduces bias or any other the-
oretical properties. Jackknife methods predates the bootstrap method discussed in the
next section.
Let X1, . . ., Xn be a random sample from a population with finite variance. Then
the sample mean is
X ¼ 1
n
X
n
i¼1
Xi:
If one of the observations, say, the kth observation, is taken out (or missing), then
Xk ¼
1
n1
X
n
i¼1
Xi Xk
 
!
¼
1
n1
X
n
k6¼i¼1
Xi:
Now, if we know the overall sample mean X and we calculated Xk, then we can
obtain the deleted observation Xk by using the formula
Xk ¼ nX  n1
ð
ÞXk:
In general, suppose that the population parameter y is estimated by a function of
the sample values ^y (X1, ..., Xn), represented by ^y, and let ^yk be the corresponding
estimate by removing the kth observation. Note that here y is any parameter; it need
not be the population mean. Then the set of “pseudovalues” ^y

k, k ¼ 1,2, ...,n is
obtained by
^y

k ¼ n^y n1
ð
Þ^y

k:
The average of these pseudovalues
^y
 ¼ 1
n
X
n
k¼1
^y

k,
is the jackknife estimate of the parameter y.
641
13.2 The Jackknife Method

Let s2 be the sample variance of these pseudovalues. Then, the variance of ^y
 is
estimated by s2=n, and a (1a) 100% jackknife confidence interval for y is given by
^y
 ta=2
s
ﬃﬃﬃn
p
where ta/2 is evaluated with (n1) degrees of freedom.
A PROCEDURE FOR JACKKNIFE POINT AND INTERVAL ESTIMATION
1. Generate a random sample X1, . . ., Xn from a population.
2. First remove X1 from the sample (so the new sample will be X2, . . ., Xn) and compute the esti-
mator ^y1 (such as the sample mean); then remove X2 (the resulting sample will be X1, X3, ..., Xn)
and compute the estimator ^y2, and so on until the last sample is X1, ..., Xn1, with the estimator
being ^yn.
3. The jackknife point estimate of y is
^y
 ¼ 1
n
X
n
k¼1
^y

k:
4. Calculate the sample variance of the values ^yi, i ¼ 1, ...,n, and denote the variance by s2.
5. A (1a)100% jackknife confidence interval for y is given by
^y
 ta=2
s
ﬃﬃﬃn
p :
EXAMPLE 13.2.1
A random sample of n¼6 from a given population resulted in the following data:
7:2 5:7 4:9 6:2 8:5 2:8
(a) Find a jackknife point estimate of the population mean m.
(b) Construct a 95% jackknife confidence interval for the population mean m.
Solution
(a) Here n¼6. Table 13.1 represents the original sample and the six jackknife samples.
Table 13.1 Jackknife Samples
Original
Sample 1
Sample 2
Sample 3
Sample 4
Sample 5
Sample 6
7.2
5.7
7.2
7.2
7.2
7.2
7.2
5.7
4.9
4.9
5.7
4.9
4.9
4.9
4.9
6.2
6.2
6.2
5.7
6.2
6.2
6.2
8.5
8.5
8.5
8.5
5.7
8.5
8.5
2.8
2.8
2.8
2.8
2.8
5.7
2.8
642
CHAPTER 13 Empirical Methods

Using Minitab descriptive statistics, we obtained the summary of the analysis given in Table 13.2.
Nowtakingthemeanandstandarddeviationofthemeansofthesixjackknifesamples,weget ^m ¼ 5:883,
and the standard deviation s*¼0.392. Thus the jackknife point estimate of m is ^m ¼ 5:883, that is the
same as the mean of the original sample.However, we can seethat the standard deviation resulting from
the jackknife is reduced to only 0.392, compared to 1.959 for the original sample.
(b) A 95% jackknife confidence interval for m is
^m ta=2
s
ﬃﬃﬃn
p ¼ 5:8832:5710:392
ﬃﬃﬃ
6
p
resulting in (5.471, 6.2944). Compare this with Example 5.5.7, where we got the confidence interval
as (3.827, 7.939). Thus, through the jackknife method, we get a much tighter confidence interval
for m.
The jackknife method of resampling is also known as the “leave-one-out” method
because it uses all observations but one in each subsample. Here, every observation is
left out exactly once. Note that in the jackknife method, sampling is done without
replacement. This procedure can also be used for other statistical procedures such
as hypothesis testing and regression.
EXERCISES 13.2
13.2.1. The following data represent the total ozone levels measured in Dobson
units at randomly selected locations on earth on a particular day.
269
246
388
354
266
303
295
259
274
249
271
254
(a) Find a jackknife point estimate of the population mean m ozone level.
(b) Construct a 95% jackknife confidence interval for the population mean m.
(c) Compare the confidence interval obtained in part (b) with that in
Example 6.3.3.
Table 13.2 Summary Statistics for Jackknife Samples
Variable
N
Mean
Median
Tr Mean
St. Dev
SE Mean
Original
6
5.883
5.950
5.883
1.959
0.800
Sample 1
5
5.620
5.700
5.620
2.068
0.925
Sample 2
5
5.920
6.200
5.920
2.188
0.978
Sample 3
5
6.080
6.200
6.080
2.123
0.949
Sample 4
5
5.820
5.700
5.820
2.183
0.976
Sample 5
5
5.360
5.700
5.360
1.656
0.741
Sample 6
5
6.500
6.200
6.500
1.395
0.624
643
13.2 The Jackknife Method

13.2.2. A drug is suspected of causing an elevated heart rate in a certain group of
high-risk patients. Twenty patients from those groups were given the drug.
The changes in heart rates were found to be as follows:
1 8 5 10
2 12
7
9 1 3
4 6 4 12 11
2 1 10 2 8
Construct a 98% jackknife confidence interval for the mean change in heart
rate. Interpret your answer.
13.2.3. Air pollution in large U.S. cities is monitored to see whether it conforms to
requirements set by the Environmental Protection Agency. The following
data, expressed as an air pollution index, give the air quality of a city for 10
randomly selected days.
57:3 58:1 58:7 66:7 58:6 61:9 59:0 64:4 62:6 64:9
Construct a 95% jackknife confidence interval for the actual average air
pollution index for this city and interpret.
13.2.4 The mileage (in thousands) for a random sample of 10 rental cars from a
large rental company’s fleet is listed.
7 13 5 5 11 15 7 9 13 8
Find a 95% jackknife confidence interval for the population mean mileage
of the rental cars of this company.
13.2.5. The following data represent cholesterol levels (in mg/dL) of 10 randomly
selected patients from a large hospital on a particular day.
360 352 294 160 146 142 318 200 142 116
Determine a 95% jackknife confidence interval for s2. Compare this with
the confidence interval obtained in Example 6.4.2.
13.2.6. Air pollution in large U.S. cities is monitored to see whether it conforms to
requirements set by the Environmental Protection Agency. The following
data, expressed as an air pollution index, give the air quality of a city for five
randomly selected days.
56:23 57:12 57:7 63:92 59:40
Construct a 99% jackknife confidence interval for the actual variance of the
air pollution index for this city and interpret.
13.2.7. It is known that some brands of peanut butter contain impurities within an
acceptable level. A test conducted on 12 randomly selected jars of a certain
brand of peanut butter resulted in the following percentages of impurities:
1:9 2:7 2:1 2:8 2:3 3:6
1:4 1:8 2:1 3:2 2:0 1:9
(a) Construct a 95% jackknife confidence interval for the average
percentage of impurities in this brand of peanut butter.
(b) Give an approximate 95% jackknife confidence interval for the
population variance.
(c) Interpret your results.
644
CHAPTER 13 Empirical Methods

13.2.8. The following is a random sample taken from the data that represents the
time intervals in days between earthquakes that either registered
magnitudes greater than 7.5 on the Richter scale or produced more than
1000 fatalities during the time period December 1902 to March 1977.
263 1901 121 832 150 99
(a) Construct a 95% jackknife confidence interval for the average number
of days between earthquakes of this type.
(b) Give an approximate 95% jackknife confidence interval for the
population variance of number of days between earthquakes of
this type.
13.3 AN INTRODUCTION TO BOOTSTRAP METHODS
In this section, we describe some aspects of a relatively recent statistical technique
known as the bootstrap method that can be used when the statistical distribution is
unknown or the assumptions of normality are not satisfied. This is a general method
for estimating sampling distributions. The concept of the bootstrap was introduced
by Bradley Efron in 1979 and further developed by Efron and Tibishirani in 1993.
We often try to determine the exact (sampling) distribution in an inferential proce-
dure, such as the sampling distribution of the sample mean, the median, or the
variance, to be used in computing confidence intervals and for testing hypotheses.
However, as we have seen, this is often the most difficult part of the work, because
the sampling distribution depends on the population distribution, which is often
unknown. This is the reason why asymptotic methods are quite frequently used
for hypothesis testing and interval estimation. The bootstrap procedure provides
us with a simple method for obtaining an approximate sampling distribution of
the statistic, conditional on the observed data. However, it should be noted that
the distribution thus obtained is only approximate. It is not as “good” as the exact
distribution, because we have only a sample from the population. However, often,
a bootstrap sampling distribution is easier to compute. Bootstrap methods are
computer-intensive methods that use simulation to calculate standard errors, confi-
dence intervals, and significance tests. The methods are applied by researchers in
business, econometrics, life sciences, medical sciences, social sciences, and other
areas where statistics is being utilized. The bootstrap method uses computer-
generated pseudorandom numbers. So the same situations might give similar but
possibly different results. Also, it is computationally more involved to obtain results
than by using the asymptotic distribution. The advantage is that the results are con-
ditional on observed data, not based on large sample approximations. How does
bootstrap help in reality? For instance, suppose we have 10 years of monthly return
data on a particular stock. If we were to use these data to predict the future return, say
through linear regression, we would be assuming that the future is going to behave
645
13.3 An Introduction to Bootstrap Methods

similarly to what happened in the past. We know from experience that such an
assumption may not give us a good prediction and the underlying parametric assump-
tions may not hold. By creating bootstrap samples from these available data, what we
are creating is not what happened, but rather what could have happened in the past
from what did happen. For example, to see how resampling affects sample mean, a
particular mutual fund had the following total return (in percentage) for the past
5 years:
Year
1
2
3
4
5
Total return
40.7
10.8
29.2
9.9
0.7
In this case, the average return for the past 5 years is 18.26%. A two-times resampling
(what could have happened) resulted in the following outcomes.
Year
1
2
3
4
5
Total return
29.2
40.7
9.9
10.8
10.8
Here, the average is 20.28%. The next one gave the following:
Year
1
2
3
4
5
Total return
0.7
0.7
40.7
0.7
9.9
The resulting average return is 10.54%. A realistic future prediction method should
depend on these possible fluctuations that could have happened in different
scenarios.
Most of the inferential procedures we learned are based on a single sample drawn
from the population. Bootstrap methods, in contrast, generate repeated subsamples
from the single original sample itself and make inferences without assuming any par-
ticular functional form for the population distribution. Because this has the effect of
sampling with replacement, we can create as many subsamples as we wish. These
subsamples will have the same sample size and values as the original sample, except
that many values in each of the subsamples will be repeated because of sampling with
replacement. It should be noted that the effectiveness of a bootstrap procedure
depends on the original sample being representative of the population. If the original
sample is not representative, the conclusions drawn from the bootstrap methods will
be completely inappropriate.
Using the jackknife method, the size of resamples is confined to (n1), and the
number of total possible samples is only n, the original sample size. The resampling
strategy based on bootstrap has no such limitations in terms of the number and mag-
nitude of replications possible. The only limitation comes from the computing
resources, and these new sets of samples can be treated as a virtual population.
646
CHAPTER 13 Empirical Methods

EXAMPLE 13.3.1
Suppose that the population distribution is an N (1, s2). Estimate s2.
Solution
Because we know the functional form of the distribution, we could use the estimation procedures
discussed in Chapter 5. There is no need for the bootstrap method. These steps are as follows.
Step 1. If we have a random sample from N(1, s2) of size n use it. Otherwise, generate a random
sample X1, . . ., Xn from N(m, s2). This could be done using the method described in Project
4A of Chapter 4.
Step 2. Estimate s2 by using, the method of maximum likelihood, yielding
^s2
ml ¼ 1
n
X
n
i¼1
Xi X

2:
Note that the maximum likelihood procedure requires the knowledge of the func-
tional form of the distribution; see the derivation in Chapter 5. Suppose the form of
the population distribution is not known but we do have a random sample X1, . . ., Xn
from a distribution. Now we will describe how we can estimate s2 using the
bootstrap method.
Let X1, . . ., Xn be a random sample from a probability distribution F with
m¼E(Xi) and s2¼Var(Xi). Then the standard error of X is defined as s2/n. In
general, the population distribution F is unknown. A simple estimate of F is the
empirical (or sample) cumulative distribution function defined by
^F x
ð Þ ¼ # Xi  x
f
g
n
¼ Proportion of X0
is  x:
This ^F is a step function with the size of the jump being 1/n at each ordered Xi.
SUMMARY OF BOOTSTRAP METHOD OF ESTIMATING THE STANDARD
ERROR OF X
Step 1. Use the sample X1, . . ., Xn and find ^F, the empirical cumulative distribution function of F.
Step 2. Generate a sample {X11*, X12*,. . ., X1n*} from ^F. From this sample, compute X

1.
Step 3. Repeat step 2, (N1) times to obtain samples {Xi1*, Xi2*,...., Xin*}, i¼1, 2, . . ., N and find
X

2, X

3, ..., X

N. Now calculate X
 ¼ 1
N
XN
i¼1X

i . This is the bootstrap mean.
Step 4. Then the bootstrap estimate of Var X
 
, denoted by ^s2
bs0, is given by
^s2
bs ¼
1
N 1
X
N
i¼1
X

i X


2:
Observe that once we have the subsample means X

1, ..., X

N, the formulas for
calculating the bootstrap mean and bootstrap variance are the same as those for cal-
culating the mean and variance of a given sample.
647
13.3 An Introduction to Bootstrap Methods

Note that when ^F is taken to be the empirical cumulative distribution function,
generating a sample from ^F is equivalent to generating a sample from {X1, ...,
Xn} with replacement. As a result, we obtain the following algorithm.
BOOTSTRAP ALGORITHM FOR ESTIMATING THE STANDARD ERROR OF X
1. Draw N random samples with replacement from the original sample X1, . . ., Xn, with each
observation having the same probability of being drawn (1/n). Let these bootstrap samples be
denoted by {{Xi1*,Xi2*,. . .,Xin*}, i¼1,2,. . .,N}.
2. Calculate the sample means of each of these bootstrap samples and the overall sample mean by
X

i ¼ 1
n
X
n
j¼1
X
ij and X
 ¼ 1
N
X
N
i¼1
X

ij:
3. Compute
^s2
bs ¼
1
N 1
X
N
i¼1
X

i X


2:
4. Then the bootstrap estimate of Var X
 
is ^s2
bs or equivalently, the standard error of X is
ﬃﬃﬃﬃﬃﬃ
^s2
bs
q
:
It is not necessary that the size of the bootstrap sample also must be n or the
samples have to be obtained with replacement. However, it is suggested that the best
results are obtained when the repeated samples are the same size n as the original
sample and when the samples are obtained with replacement. The number of
bootstrap samples N could be in the hundreds or more, depending only on the capac-
ity of the software that we are using to generate these samples.
EXAMPLE 13.3.2
The following data represent the total ozone levels measured in Dobson units at randomly selected
locations on Earth on a particular day.
269 246 388 354 266 303
295 259 274 249 271 254
Generate N¼6 bootstrap samples of size 12 each and find the bootstrap mean and standard deviation
(standard error).
Solution
Using Minitab (see Example 13.7.1 for the steps) we have created 200 bootstrap samples of size 12.
We obtain the following summary results.
X
 ¼ 285:74
and
^s2
bs ¼ 153:02 and ^sbs ¼ 12:37:
Note that the mean of the original sample is 285.7, but the standard deviation is 43.9 (see
Example 5.5.9). Even though the means of the original sample and the bootstrap means are very
close, their standard deviations are substantially different.
648
CHAPTER 13 Empirical Methods

In real applications, one of the difficulties is to estimate the standard errors of more
complicated statistics. We can now generalize the bootstrap method for those situ-
ations. Let ^y ¼ ^y X1, ..., Xn
ð
Þ be a sample statistic that estimates of the parameter y of
an unknown distribution F using some procedure. We wish to estimate the standard
error of ^y using the bootstrap procedure, which is summarized next.
GENERAL BOOTSTRAP PROCEDURE TO ESTIMATE THE STANDARD
ERROR OF ^u
1. Draw N samples with replacement from the original sample, (X1, . . ., Xn). Denote these bootstrap
samples by {Xi1*,Xi2*,. . .,Xin*}, i¼1, 2, . . ., N.
2. Compute ^y1, ^y2, ..., ^yN, where
^y

i ¼ ^yi Xi1, Xi2, ..., Xin
ð
Þ:
The procedure for computing ^y

i is the same procedure as that used to compute ^y original sample
X1, ..., Xn. Also, compute
^y
 ¼ 1
N
X
N
i¼1
^y

i :
3. The bootstrap estimator of standard error (BSE) of ^y is given by
^
BSE ^y
 


¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
XN
i¼1 ^y

i  ^y


2
N 1
v
u
u
t
:
It is clear that these algorithms are considerably computer intensive and it is nec-
essary to have suitable software to implement them. The accuracy of the bootstrap
approximation depends on the accuracy of ^F as an estimate of F and how large a
bootstrap sample is used to estimate the standard error of ^y. We will leave the com-
putation to Project 13A. We now give a theoretical example.
EXAMPLE 13.3.3
Let X1, . . ., Xn be a sample from a Poisson distribution with parameter l. Let
y ¼ P X  1
f
g ¼ el 1 + l
ð
Þ:
Obtain a bootstrap estimate of y.
Solution
It can be shown that the maximum likelihood estimator (MLE) of y is
^yml ¼ eX 1 + X
ð
Þ:
In order to estimate the bias of y, take N bootstrap samples from {X1, . . ., Xn}. Let
^yi ¼ eXi 1 + Xi


 #X0
is  1
	

n
:
Continued
649
13.3 An Introduction to Bootstrap Methods

Then the bootstrap estimate of the bias of y is
^ybias ¼
^y1 +  + ^yN
N
:
One might now use
eXi 1 + Xi


 ^ybias
as an estimator of y.
13.3.1 BOOTSTRAP CONFIDENCE INTERVALS
We could use the repeated sampling method to construct bootstrap confidence inter-
vals. We now give a procedure to obtain this.
PROCEDURE TO FIND BOOTSTRAP CONFIDENCE INTERVAL FOR
THE MEAN
1. Draw N samples (N will be in the hundreds, and if the software allows, in the thousands) from the
original sample with replacement.
2. For each of the samples, find the sample mean.
3. Arrange these sample means in order of magnitude.
4. To obtain, say, a 95% confidence interval, we will find the middle 95% of the sample means. For
this, find the means at the 2.5% and 97.5% quartile. The 2.5th percentile will be at the position
(0.025)(N+1), and the 97.5th percentile will be at the position (0.975)(N+1). If any of these
numbers are not integers, round to the nearest integer. The values of these positions are the lower
and upper limits of the 95% bootstrap interval for the true mean.
It should be noted that every time we do this procedure, we may get a slightly
different bootstrap interval. We now give an example.
EXAMPLE 13.3.4
For the data given in Example 13.3.2, obtain a 95% bootstrap confidence interval for m.
Solution
We took N¼200 samples of size 12. Thus 0.025201¼5.0255 and 0.975201¼195.975196.
Thus, taking the 5th and 196th values of sorted (in ascending order) sample means, we get the 95%
bootstrap confidence interval for m as
263:8, 311:5
ð
Þ:
1. Comparing the classical confidence interval we obtained in Example 6.3.3, which is (257.81,
313.59), the bootstrap confidence interval of Example 13.3.4 has smaller length, and thus less
variability. In addition, we saw in Example 6.3.3 that the normality assumption necessary for the
confidence interval there was suspect. In the bootstrap method, we did not have any distribu-
tional assumptions.
2. Because the bootstrap methods are more in tune with nonparametric methods, sometimes it
makes sense to obtain a confidence interval about the median rather than the mean. With a slight
modification of the procedure that we have described for the bootstrap confidence interval for
the mean, we can obtain the bootstrap confidence interval for the median.
650
CHAPTER 13 Empirical Methods

PROCEDURE TO FIND BOOTSTRAP CONFIDENCE INTERVAL FOR
THE MEDIAN
1. Draw N samples (N will be in the hundreds, and if the software allows, in the thousands) from the
original sample with replacement.
2. For each of the samples, find the sample median.
3. Arrange these sample medians in order of magnitude.
4. To obtain, say, a 95% confidence interval we will find the middle 95% of the sample medians.
For this, find the medians at the 2.5% and 97.5% quartile. The 2.5th percentile will be at the
position (0.025)(N+1), and the 97.5th percentile will be at the position (0.975)(N+1). If any
of these numbers are not integers, round to the nearest integer. The values of these positions
are the lower and upper limits of the 95% bootstrap interval for the median.
In practice, how many bootstrap samples should be taken? The answer depends
on two things: how much the result matters, and what type of computing power is
available. In general, it is better to start with 1000 subsamples. With the computa-
tional power available now, even taking 10,000 replications is not much of a prob-
lem. There are many works in the literature on bootstrap hypothesis testing and
regression. These are beyond the scope of this chapter.
EXERCISES 13.3
13.3.1. For the data of Exercise 13.2.2, generate N¼8 bootstrap samples of size 20
each and find the bootstrap mean and standard deviation (standard error).
13.3.2. For the data of Exercise 13.2.5, generate N¼12 bootstrap samples of size
10 each and find the bootstrap mean and standard deviation (standard error).
13.3.3. For the data of Exercise 13.3.3, obtain a 95% bootstrap confidence interval
for m.
13.3.4. For the data of Exercise 13.2.6, (a) obtain a 95% bootstrap confidence
interval for m, and (b) obtain a 95% bootstrap confidence interval for the
population median.
13.3.5. For the data of Exercise 13.2.8, (a) obtain a 95% bootstrap confidence
interval for m, and (b) obtain a 95% bootstrap confidence interval for the
population median.
13.4 THE EXPECTATION MAXIMIZATION ALGORITHM
In this section, we introduce an algorithm, called the expectation maximization
(EM) algorithm that is widely used to compute maximum likelihood estimates
when some elements of the data set are either missing, unobservable or incomplete.
In real-life problems, observing the complete data is the exception rather than the
rule. For example, in lifetime studies, when n items are placed on a given test, we
may have the failure times of only n1<n items while for the rest of (nn1) items
651
13.4 The Expectation Maximization Algorithm

we only know the censored failure time, that they survived a particular failure time
T (fixed beforehand). For example, we may want to know whether the lifetime of a
certain brand of fluorescent light bulbs is at least 24 months. For this purpose, let us
say we randomly test 100 light bulbs of this brand. In this case, our data will contain
all the months within which the bulbs burned out, and some that survived for
24 months. After 24 months, we may not follow when these bulbs burn out; all
we know is that these bulbs lasted for 24 months. Such a data is an example of
censored data. We can consider the censored failure times of (nn1) items as
the unobservable data values.
Another common problem is of missing data. For example, suppose we were to
take a survey on some socioeconomic problems from a random sample of families
from a city in 2009 and then a follow-up study on the same families in 2014. This
may result in many missing values in the follow-up study, because it is possible that
we may not be able to locate some of the families. Missing values can also occur if
some of the respondents refuse to answer certain questions. We have seen in
Section 5.3 that sometimes it is not possible to obtain closed-form solutions for
MLE. In the completely observed case, there are other algorithms, such as
Newton-Raphson, that can be used to numerically obtain the estimates. With
missing values, those algorithms cannot be used. The name EM algorithm was
coined by Dempster, Laird, and Rubin in 1977. This is a general iterative algorithm
to obtain the MLE when the data set is incomplete. The EM algorithm is a
formalization of an intuitive idea of estimating parameters with missing data:
(i) replace missing values with estimated values as true values, (ii) estimate
parameters, (iii) repeat.
Let X1, ...,Xn1 be the n1 observed data values, and let y1, ...,ynn1 be the (nn1)
unobserved data values. Assume that Xi0s are iid random variables with pdf f (xjy) and
Xi0s and Yi0s are independent, that is, data are missing at random.
We denote the random vector by X and the corresponding data vector by x.
The joint pdf of X1, ...,Xn1 is represented by f(xjy), where y is the parameter
vector with values in Y	Rp, a p-dimensional Euclidean space. Let g(x, yjy) denote
the pdf of the complete data set x and y, that is, the vector (x, y) represents the
conceptualized complete data set. Let h(yjy, x) be the conditional pdf of the
unobserved data y given y and the observed data x. The likelihood function for
the observed data x is, by definition,
L y; x
ð
Þ ¼ f x yj
ð
Þ:
The likelihood function for the combined data (x, y) is again by definition given by
Lc y; x, y
ð
Þ ¼ g x,y yj
ð
Þ:
The problem is to find the MLE that maximizes the likelihood function L(y, x), at the
same time using Lc(y; x, y).
From the foregoing definitions, we know that
g x,y yj
ð
Þ ¼ f x yj
ð
Þh y yj ,x
ð
Þ:
652
CHAPTER 13 Empirical Methods

Thus, we have the conditional pdf of the missing (or unobserved) data y, given x:
h yjy,x
ð
Þ ¼ g x,y yj
ð
Þ
f xjy
ð
Þ
or equivalently
f xjy
ð
Þ ¼ g x,y yj
ð
Þ
h yjy,x
ð
Þ:
(13.1)
Let y02Y be a given y-value. Because h(yjy0, x) is a pdf, we have
ð
h yjy0,x
ð
Þdy ¼ 1:
Thus ln of observed likelihood,
lnL y; x
ð
Þ ¼ ln L y; x
ð
Þ
ð
h yjy0,x
ð
Þdy
¼
ð
lnL y; x
ð
Þh yjy0,x
ð
Þdy as ln L y; x
ð
Þ is independent of y
ð
Þ:
Because L(y, x)¼f(xjy), we have
lnL y; x
ð
Þ ¼
ð
ln f x yj
ð
Þh y y0
j
,x
ð
Þdy
¼ lng x,y yj
ð
Þ lnh yjy,x
ð
Þ
½

h yjy0,x
ð
Þdy from 1
ð Þ
ð
Þ
¼
ð
lng x,y yj
ð
Þh yjy0,x
ð
Þdy
ð
lnh yjy,x
ð
Þh yjy0,x
ð
Þdy
¼ Ey0 lng x,y yj
ð
Þ
½

Ey0 lnh y y, x
j
ð
Þ
½

,
(13.2)
where the expectation is taken with respect to the conditional distribution of y given
y0 and x. Let us now consider maximizing this with respect to y. This maximization is
the maximization step (M-step) in the EM algorithm.
Let y0 be an initial estimate of y. The choice of this initial value y0 could be
done randomly or heuristically based on any prior knowledge about the optimal
value of the parameter. For instance, suppose we have to estimate mean and var-
iance of a normal distribution. One good starting point could be to take the sample
mean x and sample variance s2 based on a subset of data containing no missing
values.
Let
Q y y0
j
,x
ð
Þ ¼ Ey0 lnLc y; x, y
ð
Þ
½

¼ Ey0 lng x,y yj
ð
Þ
½

Here, y0 is used only to compute the expectation; we should not substitute for y in the
complete data log-likelihood. Let ^y 1
ð Þ be the maximizer that maximizes Q(y jy0, x)
with respect to y. That is, Q ^y 1
ð Þ y0
j
,x


 Q y y0
j
,x
ð
Þ for all y02Y. Then ^y 1
ð Þ is the
first-step estimator of y. Continuing the procedure we obtain a sequence of estima-
tors ^y m
ð Þ, which under appropriate conditions converges to the maximum likelihood
estimate with likelihood Lc(y; x, y).
653
13.4 The Expectation Maximization Algorithm

STEPS FOR EM ALGORITHM
1. ^y n
ð Þ is the estimate of the parameter y on the nth step.
2. Expectation step (E-step). Compute
Q y ^y n
ð Þ, x



¼ E^y n
ð Þ lng x;y yj
ð
Þ
½

where the expectation is with respect to the conditional pdf of y given ^y n
ð Þ and x (i.e. with respect
to h y ^y n
ð Þ, x



).
3. Maximization step (M-step). Find ^y n + 1
ð
Þ 2 Y such that
^y n + 1
ð
Þ ¼ max
y
Q y ^y n
ð Þ, x



:
4. Repeat until convergence criteria are met.
Thus, in the EM algorithm, each iteration involves two steps: the expectation step
(E-step), followed by the maximization step (M-step). In the E-step, we find the con-
ditional expectation of the unobserved or missing data given the observed data and
the current estimated parameters. That is, the E-step constitutes the calculation of
Q yj^y n
ð Þ,x


¼ E^y n
ð Þ lng x,y yj
ð
Þ
½

¼
ð
lng x,y yj
ð
Þh y y n
ð Þ

,x


dy
(which is the sum if discrete), where the integration is over the range of values that y
can take. The M-step constitutes maximization of Q yj^y n
ð Þ,x


with respect to y. This
procedure improves the log-likelihood at every iteration, that is, the log-likelihood is
nondecreasing for every iteration. Thus, for the sequence
^y n
ð Þ


obtained through the
EM algorithm, we have L ^y n + 1
ð
Þ; x


 L ^y n
ð Þ; x


with equality holding if and only
if Q ^y n + 1
ð
Þj^y n
ð Þ,x


¼ Q ^y n
ð Þj^y n
ð Þ,x


: When we have filled the completed data set,
the parameter y can be estimated by maximizing the log-likelihood estimating
procedure (M-step). It can be shown that under some conditions (such as that ln f(xjy)
is bounded, or that Q(yjy0,x) is continuous in both y and y0), ^y n
ð Þ converges in prob-
ability as n!1 to the maximum likelihood estimate based on the complete likeli-
hood Lc(y; x, y).
For computational convergence purposes, the E-step and M-step are alternated
repeatedly until the difference L ^y n + 1
ð
Þ, x


L ^y n
ð Þ, x


is less than d, a small but
prescribed quantity. Another possible convergence criterion is to stop the iteration
when the distance between ^y n + 1
ð
Þ and ^yn becomes arbitrarily small. In practice, it
may be necessary to run the EM algorithm a number of times with different (random)
starting points to ensure that the global maximum is obtained.
In general, the E-step and M-step could be complex. Even though the EM
algorithm is applicable to any model, it is particularly effective if the data come from
an exponential family. It turns out that, in this case, the log-likelihood is linear in the
654
CHAPTER 13 Empirical Methods

sufficient statistic for y. For the E-step, simply compute the expectation of the
complete data sufficient statistic given the observed data. By substituting the condi-
tional expectations of the sufficient statistics computed in the E-step for the sufficient
statistics that occurs in the expression obtained for the complete data MLEs of y, we
can obtain the next iterate in the M-step. Thus, when the complete data set is from an
exponential family, both the E-step and the M-step are simplified.
Let z¼(x, y) be the complete observation vector. A particular case in which
g(x, yjy)¼g(z, y) is from an exponential family:
g z, y
ð
Þ ¼ a x
ð Þexp k0 y
ð Þt x
ð Þ
f
g=c y
ð Þ
where t(x) is a vector of sufficient statistics with complete data, k0(y) is a vector
function of the parameter vector y, and a(x) and c(y) are scalar functions. Recall that
the members of the exponential family include many popular distributions, such
as the normal, multivariate normal, Poisson, and multinomial distributions. In this
case, the E-step can be written as
Q yjy n
ð Þ,x


¼ Ey n
ð Þ lna x
ð Þ xj
½

 + k0 y
ð Þt n
ð Þ  lnc y
ð Þ
where t(n)¼Ey(n) [t(Z)jx] is an estimator of the sufficient statistic. The M-step max-
imizes the Q-function with respect to y. Because Ey(n) [ln a(x)jx] does not depend on
y, we can rewrite the steps as follows:
E-step: Compute t(n)¼Ey(n) [t(Z)jx].
M-step: Find ^y n + 1
ð
Þ 2 Y such that
^y n + 1
ð
Þ ¼ max
y
k0 y
ð Þt n
ð Þ  lnc y
ð Þ


:
The following example gives an EM algorithm for a special case of censored survival
times. In the following example, the survival function is defined as the probability
that an individual survives beyond time y, that is, S(y)¼P(Y>y).
EXAMPLE 13.4.1
Let x ¼ x1, ..., xn1
ð
Þ be observed data and the censored observations at T are y ¼ y1, ..., yn2


(that
is, the survival time is at least T). Let the mean survival time be y, and the probability density be
given by
f x yj
ð
Þ ¼ y1exp x=y
ð
Þ, x > 0:
(a) Obtain the MLE, ^yML:
(b) Obtain an EM algorithm.
(c) Consider the following censored data, which represent the number of years 20 patients survived
after a major surgery, where a+ symbol represents that we know only that this patient survived
for 4 years and no further information.
4+
12
12
1
4+
3
3
5
2
0
5
1
4+
0
3
13
13
1
0
4
Continued
655
13.4 The Expectation Maximization Algorithm

Using the algorithm developed in part (b), run for 50 iterations with initial value of y0 being the
observed sample mean, x, and with y0¼0. Comment on the results.
Solution
The joint pdf of the uncensored observation, x, is
f xjy
ð
Þ ¼ 1
yn exp 
X
n1
i¼1
xi=y
 
!
For the right censored (at T) observations yi, i¼1, . . ., n2, the pdf can be calculated as follows:
K
ð1
T
1
yey=ydy ¼ 1
implies that K¼eT/y. Thus, the pdf of yi is given by
h y y, x
j
ð
Þ ¼ eT=y
y ey=y ¼ 1
ye
1
y Ty
ð
Þ, y  T:
(a) The likelihood, Lc(y, x, y), can also be written in the form
Lc y, x, y
ð
Þ ¼ 1
yn1 ePn1
i¼1 xi=y
ð
Þ 1F T
ð Þ
½

n2
¼ 1
yn1 ePn1
i¼1 xi=y
ð
Þe n2T=y
ð
Þ
Thus,
ln Lc y, x, y
ð
Þ ¼ n1 lny
Xn1
i¼1xi
y
n2T
y :
Differentiating with respect to y, and equating to zero,
@
@y ln Lc y, x, y
ð
Þ ¼ n1
y +
Xn1
i¼1xi
y2
+ n2T
y2 ¼ 0:
This implies
n1y ¼
X
n1
i¼1
xi + n2T
or
^y ¼ 1
n1
X
n1
i¼1
xi + n2
n1
T ¼ x + n2
n1
T:
Hence, the MLE is
^yML ¼ X + n2
n1
T:
(b) Because g(X, Yjy) denote the pdf of the complete data, and we assumed the pdf of all the data
(censored or not) follows exponential distribution, we have
g x,y yj
ð
Þ ¼ 1
yn1 ePn1
i¼1 xi=y
ð
Þ 1
yn2 ePn1
i¼1 yi=y
ð
Þ,
we get
lng x,y yj
ð
Þ ¼ n1 lny
X
n1
i¼1
xi
y n2 lny
X
n2
i¼1
yi
y:
For the E-step of the EM algorithm, we first compute
656
CHAPTER 13 Empirical Methods

Ey0Y ¼ eT=y0
ð1
T
y 1
y0
ey=y0 dy
¼ T + y0 using the integration by parts
ð
Þ:
So, we get
Q y y0
j
,x
ð
Þ ¼ Ey0 g x,y yj
ð
Þ
½

¼ Ey0 n1 lny
X
n1
i¼1
xi
y n2 lny
X
n2
i¼1
yi
y
"
#
¼ n1 lny
X
n1
i¼1
xi
y n2 lny1
y
X
n2
i¼1
Ey0 yi
ð Þ
¼ n1 lny
X
n1
i¼1
xi
y n2 lny1
yn2 T + y0
ð
Þ
¼ n1 lny
X
n1
i¼1
xi
y n2 lnyn2T + n2y0
y
:
For the M-step, we differentiate Q(yjy0,x) with respect to y,
@
@yQ y y0
j
,x
ð
Þ ¼ @
@y n1 lny
X
n1
i¼1
xi
y n2 lnyn2T + n2y0
y
"
#
¼ n1
y +
Xi¼1
n1 xi
y2
n2
y + n2T + n2y0
y2
¼ 0
n1 + n2
½

y ¼
X
n1
i¼1
xi + n2T + n2y0
^y1 ¼
1
n1 + n2
½

X
n1
i¼1
xi +
n2T
n1 + n2
½

 +
n2
n1 + n2
½

y0
¼
n1
n1 + n2
½

x +
n2T
n1 + n2
½

 +
n2
n1 + n2
½

y0:
Thus, for the general n, the algorithm is
^y n + 1
ð
Þ ¼
n1
n1 + n2
½

x +
n2T
n1 + n2
½

 +
n2
n1 + n2
½

^y n
ð Þ:
Now putting y(k+1)¼y(k)¼y* in the previous equation and solving for y*, we have that the EM
sequence {y(k)} has the MLE ^yML as its unique limit point, as k!1. That is, y ¼ ^yML:
(c) We used the following MATLAB code to run the algorithm with starting value y0 as the sample
mean, that is 4.5. Here T¼4. We run it for 50 iterations
A(1) ¼ 4.5
for n¼2: 50
A(n) ¼ 4.41*(17./20)+3*4/20+(3./20)*A(n1)
End
Following is the output.
4.5000
5.0235
5.1020
5.1138
5.1156
5.1158
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
Continued
657
13.4 The Expectation Maximization Algorithm

Thus ^y ¼ 5:1159:
To run with y0¼0, in the previous code, just change A(1)¼0. We get the following output.
0.0000
4.3485
5.0008
5.0986
5.1133
5.1155
5.1158
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
5.1159
With y0 ¼ x ¼ 4:5, it took six iteration steps to converge, whereas with y0¼0, it took seven steps
to converge. Note that in both cases, ^y ¼ 5:1159 ¼ ^yML:
Example 13.4.1 is a simple case, where there is no need for iterative computation
of ^yML: However, this demonstrates how the EM algorithm would work. These types
of problems are abundant in the medical field. For example, we may be interested in
the survival times of n patients after a treatment. For practical reasons, we may be
observing only for a fixed duration, such as 10 years. In Example 13.4.1, the vector x
will represent the time of death for the n1 individuals. For the remaining n2¼nn1
individuals, the only data we have state that they survived for more than 4 years.
Thus the value of T is 4. There is a possibility that during these experimental times,
we may lose contact with some individuals, perhaps because they moved to some
other place or they simply refused to participate in this experiment. In those cases,
we will know only that the individual survived until we lost contact. This gen-
eralization of Example 13.4.1 to where the survival time data are different for each
observation is given in Exercise 13.4.5. We now give a similar example with a
normal sample.
EXAMPLE 13.4.2
Let x ¼ x1, ..., xn1
ð
Þ be observed data from a normal population with mean y and variance 1. Let the
censored observations at T be y ¼ y1, ..., yn2


(that is, the survival time is at least T) from the same
population. Assume that the two sets of observations {xi} and {yi} are independent. Write down an
EM algorithm to estimate y.
Solution
For the uncensored observed sample x1, ...,xn1, the likelihood function is
L y xj
ð
Þ ¼ f x x yj
ð
Þ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p

n1 e1
2
Pn1
i¼1 xi y
ð
Þ2
:
Furthermore, the complete likelihood for both the samples is
L y x, y
j
ð
Þ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p

n1 e1
2
Pn1
i¼1 xi y
ð
Þ2
1ﬃﬃﬃﬃﬃﬃ
2p
p

n2 e1
2
Pn2
i¼1 yi y
ð
Þ2
:
(13.3)
658
CHAPTER 13 Empirical Methods

From the definition of Q(yjy0, x), we obtain
Q y y0
j
,x
ð
Þ ¼ Ey0 lnLc y x, y
j
ð
Þ
½

(13.4)
where the expectation is taken with respect to the conditional pdf
h yjy0,x,T
ð
Þ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
e yy0
ð
Þ2=2
1
1FY T, y0
ð
Þ
¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
e yy0
ð
Þ2=2
1
1F T y0
ð
Þ
,
where
FY T, y0
ð
Þ ¼
ðT
1
1ﬃﬃﬃﬃﬃﬃ
2p
p
e yy0
ð
Þ2=2dy ¼
ðTy0
1
1ﬃﬃﬃﬃﬃﬃ
2p
p
eu2=2du ¼ F T y0
ð
Þ:
Thus, from Equations (13.4) and (13.5),
Q yjy0,x
ð
Þ ¼ Ey0
X
n1
i¼1
ln
1ﬃﬃﬃﬃﬃﬃ
2p
p
e
xiy
ð
Þ2
2


+ Ey0 ln
1ﬃﬃﬃﬃﬃﬃ
2p
p
n2 e
yiy
ð
Þ2
2


¼ n1
2 ln 2p
ð
Þ
X
n1
i¼1
xi y
ð
Þ2
2
+ n2
ð1
T
ln
1ﬃﬃﬃﬃﬃﬃ
2p
p

n2 e
yiy
ð
Þ2
2
"
#

1ﬃﬃﬃﬃﬃﬃ
2p
p
e yy0
ð
Þ2=2
1
1F T y0
ð
Þdy:
Now taking the derivative with respect to y,
@Q
@y ¼
X
n1
i¼1
xi y
ð
Þ2 + n2ﬃﬃﬃﬃﬃﬃ
2p
p
ð1
T
yy
ð
Þ
e yy0
ð
Þ2=2
1F T y0
ð
Þdy
¼
X
n1
i¼1
xi n1y +
n2
1F T y0
ð
Þ
½

F T y0
ð
Þn2 yy0
ð
Þ
:
Solving @Q
@y ¼ 0, and letting n¼n1+n2, we obtain
y ¼
Xn1
i¼1xi
n
+ n2
n y0 + n2F T y0
ð
Þ
1F T y0
ð
Þ:
(13.5)
From Equation (13.5), we obtain the EM algorithm as
^ym + 1 ¼
Xn1
i¼1xi
n
+ n2
n
^ym +
n2F T  ^ym


1F T  ^ym


where F is the cumulative distribution function of a standard normal random variable.
We have seen that the incomplete data could occur as a result of missing data, or the
complete data may contain variables that are not observable (hidden). The following
is an example of the latter situation.
659
13.4 The Expectation Maximization Algorithm

EXAMPLE 13.4.3
Suppose that in a set of n twin pairs of children, n1 are male twin pairs, n2 are female twin pairs, and
n3¼n(n1+n2) are opposite-sex twin pairs. Let p be the probability that a twin pair is identical and
q be the probability that a child is male. It is not known which pairs of same-sex twins are identical.
Obtain an EM sequence for y¼(p, q).
Solution
We have n¼(n1+n2+n3), and y¼(p, q) is the parameter vector. Let x¼(n1, n2, n3) be the observed
data. Because we don’t know which pairs of the same sex are identical, postulate the complete data
set as z¼(n11, n12, n21, n22, n3), where n11 is the number of male identical pairs, n21 is the number of
female identical pairs, and n12 and n22 are the nonidentical pairs for males and females, respec-
tively. Here, the complete data, z, has a multinomial distribution with the likelihood given by
L z, y
ð
Þ ¼ f z yj
ð
Þ
¼
n
n11,n12,n21,n22,n3


pq
ð
Þn11 1p
ð
Þq2
½

n12 p 1q
ð
Þ
½

n21

1p
ð
Þ 1q
ð
Þ2
h
in22 2 1p
ð
Þ 1q
ð
Þq
½

n3
where the identical twins involve one choice of sex and the nonidentical twins involve two choices of
sex. The log-likelihood for the complete data is
lnf x yj
ð
Þ ¼ n11 + n21
ð
Þ lnp + n12 + n22 + n3
ð
Þ ln 1p
ð
Þ
+ n11 + 2n12 + n3
ð
Þ lnq + n21 + 2n22 + n3
ð
Þ
 ln 1q
ð
Þ + constant:
For the E-step, use Bayes’ rule to obtain the following:
n k
ð Þ
11 ¼ E n11 x, y k
ð Þ



¼ n1
p k
ð Þq k
ð Þ
p k
ð Þq k
ð Þ + 1p k
ð Þ


q k
ð Þ

2 ,
n k
ð Þ
12 ¼ E n12 x, y k
ð Þ



¼ n1
1p k
ð Þ


q k
ð Þ

2
p k
ð Þq k
ð Þ + 1p k
ð Þ


q k
ð Þ

2 ,
n k
ð Þ
21 ¼ E n21 x, y k
ð Þ



¼ n2
p k
ð Þ 1q k
ð Þ


p k
ð Þ 1q k
ð Þ


+ 1p k
ð Þ


1q k
ð Þ

2 ,
n k
ð Þ
22 ¼ E n22 x, y k
ð Þ



¼ n2
1p k
ð Þ


1q k
ð Þ

2
p k
ð Þ 1q k
ð Þ


+ 1p k
ð Þ


1q k
ð Þ

2 :
Thus, the Q-function is given by
Q y, y k
ð Þ


¼ n k
ð Þ
11 + n k
ð Þ
21


lnp + n k
ð Þ
12 + n k
ð Þ
22 + n3


ln 1p
ð
Þ
+ n k
ð Þ
11 + 2n k
ð Þ
21 + n3


lnq + n k
ð Þ
21 + 2n k
ð Þ
22 + n3


 ln 1q
ð
Þ + constant:
It can be verified that the M-step gives the following:
p k + 1
ð
Þ ¼ n k
ð Þ
11 + n k
ð Þ
21
n
,
q k + 1
ð
Þ ¼ n k
ð Þ
11 + 2n k
ð Þ
12 + n3
n + n k
ð Þ
12 + n k
ð Þ
22
:
660
CHAPTER 13 Empirical Methods

Substituting for the log-likelihoods by log-posteriors, the EM algorithm can also be
used for computations related to Bayesian analysis to find the posterior mode of y. In
the context of incomplete data coming from mixtures of parametric families, the EM
algorithm provides a very powerful numerical technique. In this book, we will not go
into the mixture models. The steps necessary to compute the required quantities
depend on the particular application, and thus in general how to code the EM algo-
rithm is not clear. There are special cases available in some software packages such
as SAS using PROC MI with EM option when the data come from a multivariate
normal distribution. It is desirable to search the literature on the particular software
you are using to find out the availability of “EM codes” to suit the particular appli-
cation in which you are interested. Also, another difficulty with implementation of
EM algorithm is that in each E-step, we require computation of the conditional
expectation. To overcome this difficulty, Wei and Tanner in 1990 proposed an algo-
rithm called MCEM (Monte Carlo EM) based on the Monte Carlo approach
explained in Section 13.5. This basically involves simulating m variables, Y1, . . .,
Ym, from the conditional distribution h(yjy(n), x) and then maximizing the approxi-
mate complete data likelihood
^Q y ^y n
ð Þ, x



¼ 1
m
X
m
i¼1
lng x,y yj
ð
Þ
½

:
We will not go into the details of these methods. The student may refer to Wei and
Tanner’s paper for further details.
EXERCISES 13.4
13.4.1. Suppose that Y is a noise-corrupted observation of a signal S. That is, Y¼S
+N, where S is independent of N. Assume that for a known s, NN(0, s2)
and SN (0, y2), where y is unknown. Given the observation Y¼y:
(a) Obtain the MLE, ^yML:
(b) Obtain an EM algorithm.
13.4.2. Let X1, . . ., Xn be an observed random sample and X n1 + 1
ð
Þ, ...,Xn be the
missing (at random) observations. Assume that Xi are iid from an N(m, s2)
distribution.
(a) Show that (Si¼1
n Xi, S i¼1
n Xi
2 ) are sufficient statistics for y¼(m, s2).
(b) Obtain the EM sequence for y¼(m, s2).
(c) Consider a censored normal sample with n¼10, with the largest three
being censored [Gupta].
1:613 1:644 1:663 1:732 1:740 1:763 1:778
Using the results of part (a), obtain an EM estimate of y¼(m, s2) with an
arbitrary starting point.
13.4.3 In Example 13.4.3, suppose that q is the probability that a child is a female.
Obtain an EM sequence for y¼(p, q).
661
13.4 The Expectation Maximization Algorithm

13.4.4 Let x ¼ x1, ..., xn1
ð
Þ and censored observations xn1 + 1, ...,xn
ð
Þ (that is, ith
experiment, if i>n1, the survival time is at least yi). Let the new complete
censored data yi be such that
yi ¼
xi, i  n1
yi, i > n1

:
Let the mean survival time be y and the probability density of y be
f y yj
ð
Þ ¼ y1exp y=y
ð
Þ, y > 0
and let the survival function be defined as the probability that an individual
survives beyond time y, that is, S(y)¼P(Y>y). Thus,
S y
ð Þ ¼ exp y=y
ð
Þ, y > 0:
(a) Obtain the MLE, ^yML:
(b) Obtain an EM algorithm.
13.4.5. Let x ¼ x1, ..., xn1
ð
Þ be observed data and the censored observations be y ¼
y1, ..., yn2


(that is, in the ith experiment, if i>n1, the survival time is at
least yi). Let the mean survival time be 9, and the probability density be
given by
f x yj
ð
Þ ¼
1ﬃﬃﬃﬃﬃﬃ
2p
p
exp 1
2 xy
ð
Þ2


:
(a) Obtain the MLE, ^yML:
(b) Obtain an EM algorithm.
13.5 INTRODUCTION TO MARKOV CHAIN MONTE CARLO
In this section, we give a brief introduction to Markov chain Monte Carlo (MCMC)
methods. Among the computational simulation methods, MCMC is enormously use-
ful for realistic statistical modeling. MCMC methods were initially developed and
used in physics. These methods have had profound influence on statistics over the
past two decades, especially in Bayesian inference. MCMC methods are used to
solve problems in many diverse areas such as archeology, biology, biophysics, com-
putational chemistry, computer graphics, finance, nuclear medicine, transport the-
ory, and zoology. These methods have enabled researchers to exploit a degree of
complexity and realism in modeling and analysis of problems in these areas that were
previously beyond reach. The name Monte Carlo method was coined by Stan Ulam
and John von Neumann, who introduced this method to solve neutron shielding and
other related problems at Los Alamos in the early 1940s.
The popular MCMC procedures make use of two standard algorithms: the
Metropolis algorithm, and the Gibbs sampler. In the Metropolis approach, all the
662
CHAPTER 13 Empirical Methods

parameters are varied at once. In the Gibbs method, each variable of the target pdf is
changed one at a time. An improvement on Metropolis, called the Metropolis-
Hastings algorithm, was introduced by Hastings in 1970. There are other hybrid
methods, such as the Hamiltonian method that alternates between Gibbs and Metrop-
olis procedures. In our present study, we will explain only the first three methods,
namely, the Metropolis algorithm, the Metropolis-Hastings algorithm, and the Gibbs
sampler.
The objective of MCMC techniques is to generate random variables having cer-
tain distributions called target distributions with pdf p(x). The simulation of standard
distributions is readily available in many statistical software packages, such as Mini-
tab. In cases where the functional form of p(x) is not known, MCMC techniques
become very useful. The basic idea of MCMC methods is to find a Markov chain
with a stationary distribution that is the same as the desired probability distribution
p(x); this is the target distribution. Run the Markov chain for a long time (say, K iter-
ations) and observe in which state the chain is after these K iterations. The probability
that the chain is in state x will be approximately the same as the probability that the
discrete random variable equals x.
In Bayesian analysis, whether we are finding a posterior distribution or a Bayesian
estimate (usually, the posterior mean), integration is involved. We know from calculus
that obtaining closed-form solutions for integrations becomes almost impossible (too
difficult) for all but some simple functions. A standard approach to numerical integra-
tion of a function f(x) is to first divide the range of integration R into n segments
x1, . . ., xn, calculate the value of f(x) at each of these points f(x1), . . ., f(xn), multiply
the values by the length of each segment, and sum these rectangles to approximate the
integral, which is the area under the curve. The error in this approximation is reduced
by increasing the number of segments n.
In Monte Carlo integration, instead of taking x1, . . ., xn as fixed deterministic
numbers, we proceed to draw a random sample from a uniform distribution over
the range of integration R, then evaluate f(xi) for each xi and take the average. This
assumes that the range R is bounded. If R is not bounded, then f(x) can be integrated
when it can be written as the product of another function h(x) and a distribution func-
tion p(x) from which we can draw values of x (that is, x1, . . ., xn is drawn from the
distribution p(x)). That is,
ð
f x
ð Þdx ¼
ð
h x
ð Þp x
ð Þdx
where integration is over the range R. Then, the integral can be approximated with
averaging the f(xi) values, that is,
ð
f x
ð Þdx  1
n
X
n
i¼1
h xi
ð Þ,
where we assume that xi values are a random sample from p(x) and in the range R.
When p(x) is a standard distribution, many statistical software packages, such as
663
13.5 Introduction to Markov Chain Monte Carlo

Minitab, can generate random samples from this distribution. In those cases, a gen-
eral coding to evaluate this integral can be written as
sum¼: 0
For i¼1 to n
{Draw xi from p(x)
sum¼: sum+h(xi)}
return sum/n
In the preceding coding, by multiplying h(xi) by the indicator function of R (that
is, IR (xi)¼1, if xi2R, and zero otherwise), we can avoid the assumption that xi values
are in the range R. For instance, let X1, . . ., Xn be a random sample generated from a
target pdf, p(x). Then the expectation of any function f(X) can be estimated using the
Monte Carlo method by
Epf X
ð Þ ¼
ð
f x
ð Þp x
ð Þdx  1
n
X
n
i¼1
f xi
ð Þ ¼ f
where Ep denotes the expectation with respect to the pdf p(x). By the law of large
numbers, it follows that
1
n
X
n
i¼1
f Xi
ð
Þ ! Ep f X
ð Þ
½

asn ! 1
provided X1, . . ., Xn are independent. We can verify that f is an unbiased estimate of
Ep f. In addition, the sampling distribution of f is approximately normal, with var-
iance s2/n, where s2 is estimated by
s2 ¼ 1
n
X
n
i¼1
f xi
ð Þf

2:
For example, in a Bayesian setting, an estimate of the posterior mean can be obtained
by taking f(x)¼x, and the variance can be obtained by taking f x
ð Þ ¼ xx
ð
Þ2, if p(x)
is the posterior distribution (recall that in Chapter 11, we used the notation p (yjx) for
the posterior distribution). Using the sampling distribution of f, we can also construct
point and interval estimates for Ep f.
Observe that the heart of the Monte Carlo method is to obtain random samples
from the target distribution p(x). One of the problems encountered using this
approach is that, while it is easy to generate samples from standard distributions
using popular statistical software packages, it is very difficult (sometimes not feasi-
ble) to do so from any distribution that is not standard (see Project 4A for a method of
generating random samples from a given distribution). For these reasons, the ordi-
nary Monte Carlo method can be implemented in only a very few cases for Bayesian
inference. That is where the MCMC method plays a crucial role. MCMC methods
allow the data analyst to build and analyze more realistic statistical models that
may be more complex than standard formulations.
Using the MCMC methods, we will construct a Markov chain {Xn} with a
limiting distribution as the target distribution, p(x). Let us first introduce the
concept of Markov chains. For a brief description of Markov chains, refer to
664
CHAPTER 13 Empirical Methods

Appendix B. We call a sequence of random variables {Xn} a Markov chain (MC)
with state space S if
P Xn ¼ xn Xn1
j
¼ xn1, ...,X1 ¼ x1
ð
Þ ¼ P Xn ¼ xn Xn1
j
¼ xn1
ð
Þ:
That is, the probability distribution of future states of a MC depends only on the pre-
sent state and not on the past states. However, it is important to note that a Markov
chain {Xn} is a dependent sequence of random variables; thus, the independence
assumption inherent in a random sample cannot be used. The transition probability
function of a discrete parameter Markov chain is defined as
pm,n x, y
ð
Þ ¼ P Xn ¼ y Xm
j
¼ x
ð
Þ, x,y in S:
We simply denote this transition probability by p(x, y). When the number of elements
in the state space S is finite, we can form a matrix P with the (x, y)th element being
p(x, y). This matrix is called a one-step transition probability matrix. p(x) is called an
invariant (limiting) distribution if it satisfies the equation
p x
ð Þ ¼
X
y2S
p y
ð Þp y, x
ð
Þ:
We say that the chain satisfies the reversibility or detailed balanced condition if p(x)
p(x, y)¼p(y)p(y, x) holds for some p(). It can be shown that such a p(x) that satisfies
the reversibility condition is invariant. Basically, if a Markov chain is reversible and
its limiting distribution exists, then the limiting distribution is the invariant
distribution.
The results explained for discrete Markov chains can be extended to continuous
time defined in a continuous state space. The stationary or the equilibrium distribu-
tion p(x) of a continuous Markov chain satisfies
p x
ð Þ ¼
ð
p y, x
ð
Þp y
ð Þdy:
Assume that the samples are generated from a Markov chain whose equilibrium dis-
tribution is the target distribution, p(x). We know by the law of large numbers that
1
n
X
n
i¼1
f Xi
ð
Þ ! Ep f X
ð Þ
½

as n ! 1
provided X1, . . ., Xn are independent. It turns out that, if we generate a Markov chain
X1, . . ., Xn from the target distribution p(x), the result
1
n
X
n
i¼1
f Xi
ð
Þ ! Ep f X
ð Þ
½

as n ! 1
still holds. In this sense, the chain {Xi} resulting from an MCMC algorithm with sta-
tionary distribution p is similar to the use of a random sample from p. The analytical
details are beyond the scope of this book. Instead, we focus on the question: How do
we construct a Markov chain whose stationary distribution is our target distribution,
p(x)? The answer is given by the Metropolis-Hastings algorithm, and the two special
cases: the Metropolis algorithm, and the Gibbs sampler. A MCMC method for
665
13.5 Introduction to Markov Chain Monte Carlo

simulating a distribution p can be defined as any method that produces an ergodic
(thus, forgets the initial starting point x0). Markov chain {Xi} whose stationary dis-
tribution is p. We start with the Metropolis algorithm. Subsequently, we will explain
both the Metropolis-Hastings algorithm and the Gibbs sampler. MCMC methods are
increasingly being used for simulation of complex probability models, for computa-
tion of integrals, and optimization.
13.5.1 METROPOLIS ALGORITHM
One of the simplest algorithms in MCMC calculations is the Metropolis algorithm,
introduced by the Greek-American mathematician Nicholas Constantine Metropolis
and colleagues in 1953. This work was mentioned in Computing in Science and Engi-
neering as being among the top 10 algorithms having the “greatest influence on the
development and practice of science and engineering in the twentieth century.” In
this case, we make a trial perturbation from the current position in a parameter space
by randomly selecting a trial step from a symmetric probability distribution called
candidate-generating density or proposal density q(x, y) (in the discrete case, it is
a symmetric matrix called the nominating matrix A¼(aij), with aij¼aji, where i,
j2S, the state space of the Markov chain). The q(x, y) depends only on the current
state x and the new proposed state y (that is, q(x, y)¼qx(y) is a function of the next
proposed state y that is allowed to depend on the current state x). Thus, starting at x,
q(x, y) can be regarded as the conditional density of landing at y in one transition step.
The trial step is either accepted or rejected on the basis of the probability of the new
position relative to the previous one. The Metropolis Algorithm is formulated as an
instance of the rejection method used for generating steps in a Markov chain. Idea of
the rejection algorithm is that if we want to sample from a specific distribution, sim-
ply sample from any distribution that is convenient, but keep only the good samples.
We now give the Metropolis algorithm for a discrete distribution. We want to
obtain a sample from a distribution {pj}, where p(j)¼P(Xk+1¼j), and we have a
symmetric nominating matrix A; then we can write the Metropolis algorithm in five
steps as follows.
METROPOLIS ALGORITHM (DISCRETE CASE)
For k¼0, start with an arbitrary point, xk¼i.
1. Generate j from the probability distribution {aij, j¼1, 2, . . .}.
2. Set
r ¼ p j
ð Þ
p ið Þ :
3. If r1 set xk+1¼j (acceptance),otherwise generate u from Uniform (0, 1),if u<r set xk+1¼j
(acceptance), else xk+1¼xk (rejection); (note that the value of xk+1 becomes the next state).
4. Set k¼k+1, go to step 1.
666
CHAPTER 13 Empirical Methods

Each of the accepted points is considered to be a sample value from the target
distribution {pj}.
The continuous case of the Metropolis algorithm is given next.
METROPOLIS ALGORITHM (CONTINUOUS CASE)
1. Start with an arbitrary point, x0.
2. Select a new position x*¼xk+Dx, where Dx is randomly chosen from a symmetric distribution.
3. Calculate the ratio
r ¼ p x
ð
Þ
p xk
ð
Þ
where p(x) is the target distribution.
4. Accept the trial position, that is, set
xk + 1 ¼ x, ifr  1:
Otherwise generate u from Uniform (0, 1) If u<r set xk+1¼x* else set xk+1¼xk.
5. Set k¼k+1, go to step 2.
If the proposal step size is dx, we could use the proposal distribution as U(dx, dx);
for example, if the step size is 1, then randomly choose DxU(1, 1). For further dis-
cussion on selection of the proposal distribution, read Section 13.5.4. The Metropolis
algorithm generates a set of states that is a Markov chain because each state xk+1
depends only on the previous state xk. Using Markov chain techniques, it can be shown
that the equilibrium distribution of the chain constructed by the Metropolis algorithm
is indeed p(x*). Note that in the Metropolis algorithm, it is not necessary to have the
pdf; instead, all that is necessary is to know the ratio p(x*)/p(xk). Thus, none of the
multiplicative constants in the pdf p plays a role in the algorithm.
This algorithm works well in most applications. Following is a simple example to
show how the Metropolis algorithm works.
EXAMPLE 13.5.1
Using the Metropolis algorithm, generate a random sample from a Poisson distribution with mean l.
For the nominating matrix, use the symmetric matrix with elements
a00 ¼ 1=2, aij ¼
1=2,
j ¼ i1
1=2,
j ¼ i + 1
0,
otherwise:
8
<
:
Solution
The nominating probability matrix is a one-step transition matrix (see Appendix B),
A ¼
1=2 1=2
0
0
0
...
1=2
0
1=2
0
0
...
0
1=2
0
1=2
0
...
0
0
1=2
0
1=2 ...
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
:
2
666666664
3
777777775
:
Continued
667
13.5 Introduction to Markov Chain Monte Carlo

Now we apply the Metropolis algorithm for generating samples from Poisson (l) in the follow-
ing steps.
Step 1. Start with xn1¼i.
Step 2. Generate j from A¼{aij}. How do we do it? We can do this using the following procedure:
For i6¼0,
Generate u1 from U(0, 1).
If u11/2, set j¼i+1 else set j¼i1.
For i¼0,
If u1<1/2, set j¼0
else set j¼1.
Step 3. Set
r ¼ p ið Þ
p ið Þ ¼ ellij=j!
elli=i! ¼ i!lij
j!lii ¼ i!liji
j!
:
Set
r ¼
1, ifi ¼ 0, j ¼ 0
l
j,
ifj ¼ i + 1
i
l,
ifj ¼ i1
:
8
<
:
Step 4. Acceptance/rejection:
If r1, set xn¼j (i.e. accept the new state j).
Otherwise, generate u2 from U(0, 1)
if u2<r, set xn¼j (i.e. accept the new state j)
else set xn¼xn1 (i.e. reject the new state j and keep the current state i).
Step 5. Set n¼n+1, go to step 2.
In Example 13.5.1, let us say we want to generate a random sample from Poisson
withl¼2andweareatstate i¼3intheiterationstep (n1). Ifourproposednewstate
is j¼4, then r¼2/4¼1/2. Suppose we obtained the value of u2 as 0.672772. Because
thisvalueis larger than 1/2,we reject theproposed newstate 4and stayat state 3for the
iteration step n (if you generate a new u2, your decision might be different). Instead,
suppose our proposed step was j¼2; then r¼i/l¼3/2>1, and we will immediately
acceptour newstate asj¼2(no need togenerate auniformrandom number; ifyoudid,
it would have been smaller than 3/2 anyway) for the iteration step n.
EXAMPLE 13.5.2
Let p(x)¼c exp (f(x)) be the form of the target distribution function. Write a general Metropolis
algorithm to generate a sample from p.
Solution
Let q(x, y) be any symmetric distribution. Starting from an arbitrary x(0), we can write the Metropolis
algorithm through the following steps.
Step 1. Let x(t) be the current state.
Step 2. Generate y from the distribution q(x, y). Because,
r ¼ p y
ð Þ
p x tð Þ

 ¼ c exp f y
ð Þ
ð
Þ
c exp f x tð Þ



 ¼ exp f y
ð Þf x tð Þ




,
calculate the change in f, D f¼f(y)f (x(t)).
668
CHAPTER 13 Empirical Methods

Step 3. Generate a random number from the uniform distribution, U(0, 1). If uexp(Df), set
x(t+1)¼y (accept the proposed new state), otherwise set x(t+1)¼x(t) (reject the proposed
new state).
Step 4. Continue (i.e. go to step 1).
Note that in the previous example, the normalizing constant in p(x) is not
important, because it cancels in the ratio. In fact this is true in all Metropolis
and Metropolis-Hastings algorithms. In the special case, where q(x, y)¼q (jyxj),
the Metropolis algorithm is also called the random-walk Metropolis. Another
special choice is q(x, y)¼q(y); this is called the independence sampler. In all of
these cases, it is important to observe that whereas the target distribution is
independent of the positions, the proposal functions depend on where we are.
For example, let p(x) be standard normal density, and let the proposal density
be of the form
q x, y
ð
Þ∝exp  yx
ð
Þ2
2 0:25
ð
Þ2
 
!
:
Figure 13.1 gives a representation of the target distribution and some representative
proposals. For each point x of the target distribution, we generate a y from the cor-
responding proposal distribution. Then, according to the accept/reject rule that we
specified earlier, we will make a decision whether to treat this new value y as being
from the target distribution.
1
0
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
−1
−2
−3
−4
2
Target
Proposals
3
4
FIGURE 13.1
Target and proposal densities.
669
13.5 Introduction to Markov Chain Monte Carlo

13.5.2 THE METROPOLIS-HASTINGS ALGORITHM
The Metropolis-Hastings (M-H) algorithm is a generalization of the Metropolis
algorithm, in which we need not assume symmetry of the nominating matrix A
(or for proposal density q(x, y)). The acceptance probability is given by
a i, j
ð
Þ ¼ min
p j
ð Þaji
p ið Þaij
, 1


:
This algorithm is the basic building block of MCMC methods. The Metropolis-
Hastings algorithm is widely used in applied statistics and is very useful for sampling
from complicated, high-dimensional probability distributions. Now we present the
steps involved in the Metropolis-Hastings algorithm in the discrete case.
METROPOLIS-HASTINGS ALGORITHM (DISCRETE CASE)
For k¼0, start with an arbitrary point, xk¼i.
1. Generate j from the nominating distribution {aij, j¼1, 2, . . .}.
2. Set
r ¼ p j
ð Þaji
p ið Þaij
:
3. If r1 set xk+1¼j.
Otherwise generate u from U(0, 1)
if u<r, set xk+1¼j
else set xn¼xn1.
4. Set k¼k+1, go to step 1.
In the preceding algorithm, if we calculate a(i, j)¼min{r, 1}, basically, we
accept the proposed new step j if u<a(i, j); otherwise we stay at the current step
i. The resulting Markov chain from both Metropolis and Metropolis-Hastings algo-
rithms would have the transition probability matrices defined by
p i, j
ð
Þ ¼ aija i, j
ð
Þ fori 6¼ j,
p i, i
ð
Þ ¼ 1
X
j6¼i
aija i, j
ð
Þ:
In the continuous case, for any given p(x), the Metropolis-Hastings algorithm takes
the following form. To start the algorithm, we choose an arbitrary proposal distribu-
tion q(x, y) so that it is easy to obtain a sample from this distribution. Define the
acceptance/rejection function as
a x, y
ð
Þ ¼ min
p y
ð Þq y, x
ð
Þ
p x
ð Þq x, y
ð
Þ, 1


:
If both p(x) and p(y) are zero, set a(x, y)¼0.
670
CHAPTER 13 Empirical Methods

METROPOLIS-HASTINGS ALGORITHM (CONTINUOUS CASE)
Step 1. Start with an arbitrary point, x0.
Step 2. Given a current state x(t), draw y from the proposal distribution q(x, y).
Step 3. Draw u from U[0, 1].
Step 4. If u<a (x(t), y), set x(t+1)¼y, otherwise set x(t+1)¼x(t).
Step 5. Set t¼t+1, go to step 2.
Note that if the q(x, y) is symmetric (i.e. q(x, y)¼q(y, x)), then the Metropolis-
Hastings algorithm reduces to the Metropolis algorithm. In practice, there are other
forms of acceptance/rejection functions suggested. Observe that in the Metropolis-
Hastings algorithm, as in the Metropolis algorithm, it is not necessary to have the pdf;
instead, all that is necessary is to know the ratio p(y)/p(x). Thus, none of the multi-
plicative constants in the pdf, p, plays a role in the algorithm.
Because of the versatility of this method, there are many generalizations of the
Metropolis-Hastings algorithm in the literature. It is also necessary to impose some
conditions both on p and on the proposal distribution q for p to be the limiting dis-
tribution of the Markov chain {X(t)} produced by the M-H algorithm. We do not want
a large ratio of the proposed new values to be rejected. Discussion of these issues is
beyond the scope of this book.
EXAMPLE 13.5.3
Using the Metropolis-Hastings algorithm, generate a sample from the following distribution. Let
O¼{2, 3, . . ., 11, 12}, which represents the sum of the up faces of two balanced dice, and let
the distribution be given by
Sum i
2
3
4
5
6
7
8
9
10
11
12
p(i)
1/36
2/36
3/36
4/36
5/36
6/36
5/36
4/36
3/36
2/36
1/36
Using the nominating matrix
a22 ¼ a 12
ð
Þ 12
ð
Þ ¼ 1=2, aij ¼
1=2, j ¼ i1
1=2, j ¼ i + 1, i, j 2 O
0,
otherwise
8
<
:
write the M-H algorithm to generate samples from the distribution p.
Solution
Suppose we start with state i∊O, say at 5 (starting at any other state is ok).
Step 1. Generate j from the nominating distribution {aij, j¼1, 2, . . .}. Thus, j¼i1 or i+1, and in
this case j has to be 4 or 6. We can follow the same procedure as in Example 13.5.1 to choose
between i1 and i+1. Let us say, we got j¼i+1, here 6.
Step 2. Set r ¼ p j
ð Þaji
p j
ð Þaij . In this case, r ¼ p 6
ð Þ
p 5
ð Þ ¼ 5=36
4=36 ¼ 5
4. (if we had chosen 4, then, r ¼ p 4
ð Þ
p 5
ð Þ ¼ 3
4)
Step 3. If r1 set xn¼j. Here r>1; hence, we accept the new state, xn¼6. Otherwise generate u
from U(0, 1) if u<r, set xn¼j else set xn¼xn1.
Step 4. Set n¼n+1, and go to step 1.
671
13.5 Introduction to Markov Chain Monte Carlo

EXAMPLE 13.5.4
Write a Metropolis-Hastings algorithm to generate samples from N(0, 1) based on the proposal
U[1, 1].
Solution
Note that in order for y to be generated based on U[1, 1], we need y – x(t)U[1, 1]. Thus, yU
[x(t) 1, x(t)+1]. Figure 13.2 shows the target distribution as the standard normal, and the repre-
sentative proposals that are uniform at points x(t)¼2 and 2.
Now, the M-H algorithm can be obtained in the following way.
Set
a x tð Þ, y


¼ min
p y
ð Þq y, x
ð
Þ
p x
ð Þq x, y
ð
Þ, 1


¼ min
exp x tð Þ2 y2=2
n
o

x + 1
y + 1,1


Generate uU[0, 1]
If u<a (x(t), y), set x(t+ 1)¼y, otherwise set x(t+1)¼x(t). Continue.
Observethatinordertogeneratenormalrandomvariables,itisnotnecessarytouse
M-H algorithms. Most of the statistical software packages will give us a random
sample from the normal distribution. Example 6.5.2 (originally suggested by Hastings
in1970)isgiven fordemonstrationof theM-Halgorithm.Thealgorithmiseffectivein
general cases, for instance, to generate a sample from a gamma distribution. In
Gamma(a, b), if a is an integer, we can use the method of Project 4A to generate a
random sample. However, if a is not an integer, we could use Gamma([a], b) (here
[a] denotes the integer part of a) as the proposal distribution, and follow the steps
of the M-H algorithm to generate a sample from Gamma(a, b) (see Exercise 13.5.3).
Target
Proposals
6
4
2
0
1
−2
−4
−6
FIGURE 13.2
Normal target and uniform proposal distributions.
672
CHAPTER 13 Empirical Methods

13.5.3 GIBBS ALGORITHM
The name Gibbs algorithm (or Gibbs sampler) was coined by the brothers Stuart
Geman and Donald Geman in 1984 and refers to Gibbs distributions in statistical
physics. This is very useful in obtaining a sequence of observations from a specified
multivariate probability distribution, when direct sampling is hard or the joint distri-
bution is not known explicitly. Gibbs sampler can be used in those situations when
the conditional distribution of each variable is known and is relatively easier to sam-
ple from. In the Gibbs sampler, only one parameter is varied at a time, while all others
are held fixed. The parameter then is randomly drawn from a conditional probability
density function, the probability distribution of one parameter, given all other param-
eters; p (xijxi), where xi is the full set of parameters excluding only the single com-
ponent xi. Let x¼(x1, . . ., xk) be k(2)-dimensional. Recall from Chapter 3 that these
conditional densities can be obtained as follows:
p xi xi
j
ð
Þ ¼ p xi x1
j
, ...,xi1, xi + 1, ...,xk
ð
Þ
¼
p x1, ...,xi1, xi, xi + 1, ...,xk
ð
Þ
ð
p x1, ...,xi1, xi, xi + 1, ...,xk
ð
Þdxi
:
The basic assumption under which the Gibbs algorithm works is that we could easily
draw a random sample from these conditional pdfs. Thus, the Gibbs algorithm is a
particular case of Metropolis-Hastings algorithms. For example, at the ith step, yi is
generated from the nominating density qi (xi, yi) where qi depends on the current state
xi. The candidate yi is accepted with probability
ai xi, yi
ð
Þ ¼ min
pi yi
ð Þqi yi, xi
ð
Þ
pi xi
ð Þqi xi, yi
ð
Þ, 1


:
If yi is accepted, we will set the ith component of xn, xn, i¼yi; otherwise set xn, i¼xn i.
The remaining components of xn are not changed in step i. This is repeated for each i,
at the end of which the entire vector xn would have been updated. Thus, if we are in
state x at time t, at time t+1 we either remain at x or go to y by modifying only one
component of x. It is important to use the most recent values of updated components
to update the next component. That is, given x(t)¼(x1
(t),. . .,xk
(t)) at time t, generate
x t + 1
ð
Þ
1
 p x1 x tð Þ
2 , x tð Þ
3 , ..., x tð Þ
k



x t + 1
ð
Þ
2
 p x2 x t + 1
ð
Þ
1
, x tð Þ
3 , ..., x tð Þ
k



x t + 1
ð
Þ
3
 p x3 x t + 1
ð
Þ
1
, x t + 1
ð
Þ
2
, x tð Þ
4 , ..., x tð Þ
k



:
:
:
x t + 1
ð
Þ
k
 p xk x t + 1
ð
Þ
1
, x t + 1
ð
Þ
2
, ..., x t + 1
ð
Þ
k1



:
For instance, let k¼2. The Gibbs sampler updates in the following manner. Start at
x(0)¼(x1
(0),x2
(0)); first update x1
(0) to x1
(1), using this updated value x1
(1) and x2
(0), update
673
13.5 Introduction to Markov Chain Monte Carlo

x2
(0) to x2
(1), resulting in the updated vector x(1). Repeat this procedure to obtain x(2),
x(3), . . .. Figure 13.3 depicts this updating procedure.
The conditional densities f1, . . ., fk are called the full conditionals. In the Gibbs
sampler, only these conditional densities are needed for simulation. Thus, this pro-
cedure becomes very efficient when the vector x is large, because all of the simula-
tions can be done as univariate.
The following example of bivariate density is popularly used in the literature to
illustrate the Gibbs sampler. It is the case where the joint density is complex, because
one variable (x) is discrete, while the other variable (y) is continuous. However, the
conditional densities are simple known distributions, binomial and beta distributions,
respectively. It is then easier to simulate these distributions, thus demonstrating the
power of the Gibbs sampler.
EXAMPLE 13.5.5
(a) Write a Gibbs sampler for generating samples from the following bivariate density:
f x, y
ð
Þ ¼
n
x
 
yx + a1 1y
ð
Þnx + b1, for x ¼ 0,1, ...,n
and 0  y  1:
(b) Starting with y0¼1/4, n¼15, and a¼1, b¼2, obtain the first three realizations of the Gibbs
sequence.
Solution
(a) From Exercise 3.3.14, we know that
f x yj
ð
Þ∝
n
x
 
yx 1y
ð
Þnx:
updated
X(1)
X(3)
x2
(0)
x2
x1
x1
(0)
X(0)
X(2)
updated
FIGURE 13.3
Gibbs updating procedure.
674
CHAPTER 13 Empirical Methods

That is, the conditional distribution of x (treating y as a constant) is binomial with parameters n
and y, 0y1. Also,
f x yj
ð
Þ∝yx + a1 1y
ð
Þnx + b1:
Thus, the conditional distribution of y given x is a beta distribution with parameters x+a and
n–x+b. The Gibbs sampler for generating bivariate samples from f (x, y) is then given as follows:
For i¼1, . . ., n, repeat:
1. Generate yi from fYjX(.jx(i1)), that is from Beta (xi–1+a, nxi1+b).
2. Generate xi from fXjY(.jy(i)), that is from binomial (n, yi).
3. Return (xi, yi).
(b) We proceed with the following steps.
(i) For y0¼1/4, x0 is obtained from generating a random variable from binomial with n¼15,
y0¼1/4, that is, from B(15, 1/4), resulting in a value of 4 (generated using Minitab; you
may get a different value when you do it). Thus, x0¼4.
(ii) Generate y1 randomly from
Beta x0 + a,nx0 + b
ð
Þ ¼ Beta 4 + 1,154 + 2
ð
Þ
¼ Beta 5, 13
ð
Þ
resulting in y1¼0.53 (approximated to second digit). Now x1B(15, 0.53), resulting in x1¼6.
(iii) Generate y2 randomly from
Beta x1 + a,nx1 + b
ð
Þ ¼ Beta 7, 11
ð
Þ
resulting in y2¼0.30. Now x2B(15, 0.30), resulting in x2¼3.
Thus, a particular realization of the Gibbs sampler for the first three iterations is
(4, 0.25), (6, 0.53), and (3, 0.30).
From Exercise13.5.8, it can be observed that at thebeginning, thevalues ofthechain
are highlydependent onthe choice oftheinitial valuey0.Inpractice,itisnecessarytorun
a sufficient number of iterations to remove the effect of the starting values. Even though
theGibbssamplerisaspecialcaseoftheMetropolis-Hastingsalgorithm,itisimportantto
observethatunliketheM-Halgorithm,everysamplegeneratedbytheGibbsalgorithmis
accepted.Also,weshouldhaveatleastatwo-dimensionalproblemfortheGibbssampler
tobeused.SinceGibbssampling(likeotherMCMCsampling)generatesaMarkovchain
ofsamples,eachsampleiscorrelatedwithneighboringsamples,toobtainarandom sam-
ple, one need to perform thinning the resulting chain by only taking every kth value (like
takingevery50thvalue).Therearesomeprosandconsinthepracticeofthinning,forthat
and some nice applications of Gibbs method, we refer reader to specialized books.
From the previous discussions, we can see that a general description of an MCMC
method can be summarized in the following algorithm.
Initialize X0
For i¼1; . . .; N repeat
x¼Xi1;
Generate Y from a nominating density, q(x; y);
Calculate the acceptance rate, a(x; y);
Generate U from the uniform U(0; 1);
If (U<a(x; y)) set X(i)¼y,
Else set X(i)¼x;
End;
675
13.5 Introduction to Markov Chain Monte Carlo

If we choose a nominating density q(x, y) and an acceptance rate a(x, y) such that
the reversibility condition
p x
ð Þa x, y
ð
Þq x, y
ð
Þ ¼ p y
ð Þa y, x
ð
Þq y, x
ð
Þ,
is satisfied, then the foregoing procedure generates a Markov chain with limiting dis-
tribution p(x). In order to use Gibbs sampling for Bayesian analysis, we must have an
explicit analytical posterior conditional distribution.
13.5.4 MCMC ISSUES
Two major issues in MCMC are convergence and burn-in. Because in all three
MCMC algorithms we start the sequence from an arbitrary point, any particular
sequence may take some time to pass through the transient stage, and the effect
of the starting value is very small and can be ignored—that is, it attains convergence.
In practice, we will have to run the algorithm for a few thousand iterations so that the
effect of this initial state is negligible. The samples obtained during this burn-in
period should be discarded for the subsequent analysis as they do not represent
the target pdf. By monitoring the sequence itself, we can determine whether the
sequence has reached the convergence. A simple way to decide how much burn-
in is necessary is to create scatterplots of Xi versus Xj, i6¼j. When the wild variations
stop, then it is safe to assume that the chain has reached stationarity.
Another major issue in the implementation of MCMC algorithms is the choice of
proposal density. In the continuous case, popular choices among others are the mul-
tivariate normal density and the multivariate t with specified parameters. Even in
these cases, there is the question of appropriate size of the spread, or scale of the
proposal density. The size of the acceptance ratio is another issue. If the ratio is
too small, the samples will get stuck (because almost all proposed new states will
be rejected), and if the ratio is too high, the samples will show tracking. A general
rule of thumb is that the acceptance ration should be within 30% to 60%. If not, adjust
the step size (for a small ratio, decrease the step size, and for a high ratio, increase the
step size). There are many pulications devoted to these issues.
For the Bayesian computation, MCMC allows us to sample from any posterior.
Because of the availability of specialized software packages, such as BUGS, it is
practical to code up for a particular problem.
There are many references including books on MCMC methods; some of these
are listed in the references at the end of this book. For a good discussion including
some
technical
details,
refer
to
http://vcla.stat.ucla.edu/old/MCMC/MCMC_
tutorial.htm.
EXERCISES 13.5
13.5.1. For Example 13.5.1, let l¼3. Starting with initial state x0¼6, compute
relevant quantities performing 10 iterations of the algorithm.
676
CHAPTER 13 Empirical Methods

13.5.2. Using the Metropolis-Hastings algorithm, generate a random sample from
a geometric distribution with mean y. Use the nominating distribution
{aij, j¼1, 2, . . .} such that
aij ¼
1
2 j ¼ i1,i + 1,andi ¼ 1,2,3, ...
1
2 j ¼ 0,1and i ¼ 0
0 otherwise:
8
<
:
[Recall that if X is geometric with parameter y, then P(X¼x)¼
(1–y)xy, for x¼0, 1, 2, . . .]
13.5.3. Write down the Metropolis-Hastings algorithm to generate a sample from
Gamma(a, b) using the proposal density as Gamma([a], [a]/a).
13.5.4. Write down the Metropolis-Hastings algorithm for simulating a Markov
chain with stationary distribution p¼(1/6, 2/3, 1/6), using the “proposal”
transition matrix
Q ¼
1=2 1=2
0
1=2
0
1=2
0
1=2 1=2
0
@
1
A:
13.5.5. In tossing three fair coins, let the random variable X be defined as
X¼number of tails. Then the distribution of X is given by Write down
the Metropolis or Metropolis-Hastings algorithm for simulating a
Markov chain with stationary distribution p(x). Use any nominating
matrix.
x
0
1
2
3
p(x)
1/8
3/8
3/8
1/8
13.5.6. Write a Metropolis algorithm to generate samples from a target
distribution, p x
ð Þ∝exp x2
2
ð
Þ, based on the proposal
qx y
ð Þ ¼ exp  yx
ð
Þ2
2 0:4
ð
Þ2
 
!
:
13.5.7. Write a general Metropolis or Metropolis-Hastings algorithm to generate a
sample from a target distribution p, where p is an exponential random
variable with parameter y.
13.5.8. Write a general Metropolis or Metropolis-Hastings algorithm to generate a
sample from a target distribution p, where p(x) a x34(1x)38(2+x)125. Use
the proposal density as q(x, y)¼1 on the interval [0, 1].
13.5.9. For the bivariate density given in Example 13.5.5, starting with three
different values of y0, say, 1/3, 1/2, and 2/3 n¼15, and a¼1, b¼2, obtain
the first three realizations of the Gibbs sequence. Comment on the
influence of the initial values.
677
13.5 Introduction to Markov Chain Monte Carlo

13.5.10. Consider a problem of sampling bivariate random variables with joint
density given by
f x, y
ð
Þ ¼
ce x + y + 4xy
ð
Þ, x  0, y  0
0,
otherwise:

(a) Find f (xjy) and f (yjx).
(b) Write a Gibbs procedure to generate samples from this distribution.
Discuss why it is easier to use the Gibbs sampler for this case.
(c) Starting from an arbitrary point, obtain the first three sample points.
13.5.11. Suppose the target distribution is
X, Y
ð
Þ  N
0
0
 
,
1 r
r 1




:
Then write theGibbssamplertogenerate asample from thisdistribution.In
particular,say, westart with (X,Y)¼(12, 12)and r¼0.7.What istheGibbs
procedure to generate a sample from a binormal distribution? [The pdf of a
bivariate normal distribution with
x ¼
x
y
 
m ¼
mx
my


S ¼
s2
X
rsXsY
rsXsY
s2
Y


is given by
f x
ð Þ ¼ 1
2p detS
ð
Þ1=2exp 1
2 xm
ð
Þ0S1 xm
ð
Þ


,
where 0 denotes the vector transpose.]
13.5.12 Suppose the target distribution is
X, Y
ð
Þ  N
m
m
 
,
2 1
1 1




:
Then write the Gibbs sampler to generate a sample from this distribution.
13.6 CHAPTER SUMMARY
In this chapter, we introduced some empirical methods that are becoming increas-
ingly popular in modern statistical analysis. The methods presented must be viewed
as introductory in nature and by no means most efficient or general. Because of ever-
evolving applications and advancements in technology, most of the methods pre-
sented here also evolve. Also, based on the situation, it is necessary to write computer
codes to run the algorithms introduced in this chapter. Our hope is that students will
explore these topics in more detail by referring to specialized books and publications.
In this chapter, we also learned the following important concepts and procedures:
•
The jackknife method.
•
General bootstrap procedure to estimate the standard error of ^y:
678
CHAPTER 13 Empirical Methods

Create a function returning your parameter
to be estimated. Notice this requires an index.
•
Bootstrap confidence intervals.
•
EM algorithm.
•
MCMC methods.
•
Metropolis algorithm.
•
Metropolis-Hastings algorithm.
•
Gibbs sampler.
13.7 COMPUTER EXAMPLES
Most of the procedures described in this chapter could be implemented using
Minitab, SAS, or SPSS. There are other specialized programs that will do a good
job of implementing the methods discussed in this chapter. BUGS (Bayesian
inference using Gibbs sampling) is free software that has proven to be effective
in MCMC computations, and the details are at the Web site: http://www.mrc-bsu.
cam.ac.uk/bugs/. Most of the procedures discussed in this chapter can also be
implemented in “R,” which is also free software that can be downloaded
from http://www.rproject.org/. Few examples in R are given. We will also
present an example in Minitab. However, we will not discuss SAS or SPSS
examples.
13.7.1 EXAMPLES USING R
EXAMPLE 13.7.1 BOOTSTRAP
Using the following data, perform a bootstrap point and interval estimate for the median. Generate
six replications or bootstrap samples of size 12 each.
Sample (x) : 269 246 388 354 266 303 295 259 274 249 271 254
R Code:
library(’boot’);
Load the boot library
mystatfun¼function(data,index) {
return(median(data[index]));
}
mybs¼boot(x,mystatfun,R¼6);
print(mean(mybs$t));
mybs$t contains the values generated by
your function from each bootstrap replication
print(sd(mybs$t));
boot.ci(mybs,type¼"basic");
Output:
269.6667
BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS
Based on 6 bootstrap replicates
CALL :
boot.ci(boot.out¼mybs, type¼"basic")
Continued
679
13.7 Computer Examples

Intervals :
Level Basic
95% (241, 286)
Calculations and Intervals on Original Scale
Warning : Basic Intervals used Extreme Quantiles
Some basic intervals may be unstable
EXAMPLE 13.7.2
Jackknife
Using the Data from the previous example, perform a jackknife point estimate for the mean, and
standard deviation. Notice the jackknife computation is not simulated like the bootstrap and will
have one answer.
R Code:
tmp¼c();
for(i in 1:12) {
tmp¼c(tmp,mean(x[-i]));
}
mean(tmp);
sd(tmp);
Output:
285.6667
Mean
3.988777
Standard deviation
EXAMPLE 13.7.3 MCMC
MCMC is used to simulate random variables from distributions we cannot sample from. In this
example our target distribution is the chisq(4) and our proposal distribution is normal(i,1). Notice
we can use the rchisq() function to do this and obtain a better result, however we are going to
compare the results of rchisq() to MCMC for learning purposes. Another important note, our
proposal distribution’s mean is the previous value in the Markov Chain.
The chain variable maybe treated as a generated random sample from our target distribution.
Notice we can evaluate the target distribution but perhaps we cannot sample or integrate the
target.
Your means will be unique for both your rchisq() and chain but they should be close. Observe
the density curves over the histogram.
R Code:
i¼10;
#Step 1
chain¼c();
680
CHAPTER 13 Empirical Methods

for(c in 1:100000) {
j¼rnorm(1,i,1); #Step 2
u¼runif(1,0,1); #Step 3
r¼(dchisq(j,df¼4)*dnorm(j,i,1))/(dchisq(i,df¼4)*dnorm(i,j,1));
a¼min(c(r,1),na.rm¼TRUE);
if(u<a) {
chain¼c(chain,j);
i¼j;
} else {
chain¼c(chain,i);
}
}
mean(chain);
mean(rchisq(3000,df¼4));
plot(density(chain),col¼"blue",type¼"l");
lines(density(rchisq(3000,df¼4)),col¼”red”);
lines(seq(0,25,by¼0.1),dchisq(seq(0,25,by¼0.1),df¼4),col¼”black”);
Output:(Figure 13.4)
0.15
0.10
0.05
0.00
0
Density.default (x = chain)
5
10
N = 100000 Bandwidth = 0.2305
Density
15
20
25
FIGURE 13.4
Simulated kernel densities.
Step 4
Step 5
681
13.7 Computer Examples

EXAMPLE 13.7.4 (EM ALGORITHM)
For the data of Exercise 13.4.2 (c) give R-code.
Solution
We take arbitrary Initial values for the parameters m and s.
#Change the values in (*) and (**) and put any arbitrary values as follows:
em.norm<- function(Y){
Yobs<- Y[!is.na(Y)]
Ymis<- Y[is.na(Y)]
n<- length(c(Yobs, Ymis))
r<- length(Yobs)
# initial values
mut<- 1 # (*)put arbitrary value for m
sit<- 0.1 # (**)put arbitrary value for s
# Define log-likelihood function
ll<- function(y, mu, sigma2, n){
-0.5*n*log(2*pi*sigma2)-0.5*sum((y-mu)^2)/sigma2
}
# Compute the log-likelihood for the initial values, and ignoring the missing data mechanism
lltm1<- ll(Yobs, mut, sit, n)
repeat{
# E-step
EY<- sum(Yobs)+(n-r)*mut
EY2<- sum(Yobs^2)+(n-r)*(mut^2+sit)
# M-step
mut1<- EY//n
sit1<- EY2//n - mut1^2
# Update parameter values
mut<- mut1
sit<- sit1
# compute log-likelihood using current estimates, and igoring the missing data mechanism
llt<- ll(Yobs, mut, sit, n)
# Print current parameter values and likelihood
cat(mut, sit, llt, "\n")
# Stop if converged
if (abs(lltm1 - llt)<0.001) break
lltm1<- llt
}
# fill in missing values with new mu.
return(mut,sit)
}
682
CHAPTER 13 Empirical Methods

EXAMPLE 13.7.5 (MCMC)
Write MCMC algorithm for Example 13.5.4.
Solution
metrop3¼function(n¼1000,eps¼0.5)
{
vec¼vector("numeric", n)
x¼0
oldll¼dnorm(x,log¼TRUE)
vec[1]¼x
for (i in 2:n) {
can¼x+runif(1,-eps,eps)
loglik¼dnorm(can,log¼TRUE)
loga¼loglik-oldll
if (log(runif(1)) < loga) {
x¼can
oldll¼loglik
}
vec[i]¼x
}
vec
}
In addition, if we want to plot the results, use following code:
plot.mcmc<-function(mcmc.out)
{
op¼par(mfrow¼c(2,2))
plot(ts(mcmc.out),col¼2)
hist(mcmc.out,30,col¼3)
qqnorm(mcmc.out,col¼4)
abline(0,1,col¼2)
acf(mcmc.out,col¼2,lag.max¼100)
par(op)
}
metrop.out<-metrop3(10000,1)
plot.mcmc(metrop.out)
With the plot, we get the following output
Continued
683
13.7 Computer Examples

EXAMPLE 13.7.6 (GIBBS SAMPLER)
Write an R code for Example 13.5.5 (b).
Solution
#R program for Gibbs sampling
>
>n¼15
>y0¼1/4
>p¼y0
>x0¼rbinom(1,n,p)
>
>a¼1
>b¼2
>A¼x0+a
>B¼n-x0+b
>X¼matrix(x0,3);Y¼matrix(y0,3)
>
3
2
1
0
−1
−2
−2
0
200
400
600
800
−3
−1
0
mcmc.out
Frequency
ts(mcmc.out)
1
2
3
−3
0
2000
4000
6000
Time
Histogram of mcmc.out
8000
10000
0.2
0.0
0
20
40
60
Lag
ACF
Sample quantiles
80
100
0.4
0.6
0.8
1.0
−2
−4
0
Theoretical quantiles
2
4
3
2
1
0
−1
−2
−3
Normal Q–Q Plot
Series mcmc.out
684
CHAPTER 13 Empirical Methods

>for(i in 2:3){#sample from f(y/x)
+Y[i]¼rbeta(1,A,B)
+#sample from f(x/y)
+X[i]¼rbinom(1,n,Y[i])
}
>print(matrix(c(X,Y),3,2))
Output
[,1] [,2]
[1,] 5 0.2500000
[2,] 5 0.4011747
[3,] 4 0.2047587
It should be noted that each time we run the code, we may get different output.
13.7.2 EXAMPLES WITH MINITAB
EXAMPLE 13.7.7
For the data of Example 13.3.2, give the Minitab steps.
Solution
Enter the data in C1. Enter 0.08 (1/12) 12 times in C2. Then
Calc>Random Data>Discrete. . .>Generate [enter 200] rows of data>Store in column(s):
enter C3-C14>values in: enter C1>Probabilities in: enter C2>click OK
We will get 200 rows of data stored in 12 columns. Because the data are generated randomly from
the original data with replacement, we will consider the row data (C3-C14) as the sample size and
the 200 columns as the number of samples. Thus N¼200, and n¼12. Now for each row we can find
the mean, X

i by doing the following.
Calc>Row Statistics. . .>click Mean>in Input variables: enter C3-C14>store results in: enter
C15>click OK
We will get 200 values representing the sample means. To get the bootstrap mean,
Stat>Basic Statistics>Display Descriptive Statistics. . .>Variables: enter C15>click OK
The value in the mean is the bootstrap mean, and the value in the standard deviation is the bootstrap
standard deviation.
If we want to get say, a 95% bootstrap confidence interval, first sort the sample means in
ascending order:
Manip>Sort. . .>Sort column(s): enter C15>store sorted column(s) in: enter C16>sorted by
column: enter C15>click OK
Calculate
the
values
of
0.025(N+1)¼0.025
x
201¼5.025
and
0.975(N+1)¼
0.975201¼195.975. Approximating these values to the nearest integer, we get 5 and 196,
respectively. The lower confidence limit will be the fifth entry in the sorted means, and upper
confidence limit will be the 196th value in the sorted means.
685
13.7 Computer Examples

If we want to obtain a confidence interval for the median, we follow very much
the same steps as before, but instead of using the mean in the procedure, we substitute
the median. For example:
Calc>Row Statistics. . .>click Median>in Input variables: enter C3-
C14>store results in: enter C15>click OK
The rest of the steps are similar.
13.7.1 SAS EXAMPLES
There are %JACK and %BOOT macros available to do jackknife and bootstrap com-
putations. A good site with example programs from SAS institute is http://ftp.sas.
com/techsup/download/ stat/jackboot.html. Sometimes, PROC IML could also be
used to bootstrap. In the case of multivariate normal data, PROC MI with the EM
option will perform the EM algorithm in SAS. Refer to http://support.sas.com/docu
mentation/cdl/en/statug/63033/HTML/default/viewer.htm#mcmc_toc.htm for the
options available for the MCMC procedure. Example SAS codes could be obtained
from a simple search of the Web for almost all the procedures explained in
this chapter.
PROJECTS FOR CHAPTER 13
13A. BOOTSTRAP COMPUTATION
Use any statistical computer programs to generate random numbers. By specifying
a particular distribution, such as normal with mean 0 and variance 1 or other similar
distributions, we can then generate numbers that follow this distribution. (This can
be done either directly, if your software allows, or by the method described in
Project 4A.)
(a) Use such a package to generate 200 numbers from an N (0, 1) distribution. Then
calculate the sample mean and sample variance. (They will be slightly off from
the actual mean and variance. From this, we can draw the conclusion that the
estimates of data parameters which are computed using the data set are not
necessarily the true parameters, but often are reasonable guesses.) Using these
values, calculate an estimate of the standard error.
(b) Now for the same data, pretend that we are not really sure what the distribution
is. Then, we could consider letting the observed data specify what the
distribution is. This is the essence of bootstrapping. In particular, sample, with
replacement from a distribution that we have observed (the empirical
distribution of the data), in order to study the possible estimates that might have
resulted from a similar sample (same data observations, but in possibly different
quantities). Using the bootstrap algorithm described in Section 13.3, obtain a
bootstrap estimate of the standard error and compare this with the estimate
obtained in part (a).
686
CHAPTER 13 Empirical Methods

CHAPTER
Some Issues in Statistical
Applications: An Overview14
CHAPTER CONTENTS
14.1 Introduction .................................................................................................. 688
14.2 Graphical Methods ........................................................................................ 689
14.3 Outliers ........................................................................................................ 694
14.4 Checking Assumptions ................................................................................... 699
14.5 Modeling Issues ............................................................................................ 712
14.6 Parametric Versus Nonparametric Analysis ..................................................... 719
14.7 Tying it All Together ...................................................................................... 721
14.8 Conclusion .................................................................................................... 731
OBJECTIVE
In this chapter we discuss some general concepts and useful methods with applica-
tions to real-world problems.
Florence Nightingale
(Source: http://commons.wikimedia.org/wiki/File:Florence_Nightingale_1920_reproduction.jpg)
Mathematical Statistics with Applications in R
Copyright © 2009 Elsevier Inc. All rights reserved.
687

Florence Nightingale (1820-1910) is most remembered as a pioneer of nursing
and a reformer of hospital sanitation methods. Her statistical contributions caused
Karl Pearson to acknowledge Nightingale as a “prophetess” in the development
of applied statistics. Nightingale used data as a tool for improving medical and sur-
gical practices. During the Crimean War, she plotted the incidence of preventable
deaths in the military and introduced polar-area charts to demonstrate the unneces-
sary deaths due to unsanitary conditions. With her analysis, Florence Nightingale
showed the need for reform and revolutionized the idea that social phenomena could
be objectively measured and subjected to mathematical analysis. In addition, she
developed a Model Hospital Statistical Form for hospitals to collect and generate
data and statistics. She became a Fellow of the Royal Statistical Society in 1858
and an honorary member of the American Statistical Association in 1874.
14.1 INTRODUCTION
Basically, there can be three major problems in applying the statistical methods that
we have studied in the previous chapters to real-world problems. These involve
sources of bias, errors in methodology, and the interpretation of the analytical
results. Bias occurs in situations or conditions that affect the validity of statistical
results. In order for the statistical inferences to be valid, the observed sample must
be representative of the target population, and the observed variables must conform
to assumptions that underlie the statistical procedures to be used. Of course the sta-
tistical methodology chosen must be also appropriate for the problem under study.
We must be careful with the interpretation of the statistical results. For example, in a
regression problem, a cause-and-effect relationship may not be warranted, or in a
hypothesis testing problem, we may not accept the null hypothesis, without exploring
the probability of type II error. If we present the results graphically, the graphs should
be accurate and should reflect the data variations clearly.
In this textbook, we have assumed that a data set is available to us: Either it is a
small data set that we can handle without much effort, or it is in a computer-readable
file. In practical situations, the proper handling of a statistical data set is not an easy
task. Going from a stack of disorganized hard copy to online data that are trustwor-
thy, that is, to input, debug, and manipulate the data, is a problem one will face even
before one starts the statistical analysis. Here, we will not be dealing with these
issues. Interested readers should refer to the references at the end of this book for
further study on these aspects.
It is not our aim to discuss comprehensively all the problems that come up in
applications. Most of the material presented in this chapter has already been dis-
cussed in various parts of the book. One of the problems we face when we study
a book of this sort is that the problems of each chapter, say, Chapter 6 on hypothesis
testing, we know that we only need to use the techniques of that section, at most of
that chapter. For the parametric analysis, in Chapter 7, we gave ways to do goodness-
of-fit for choosing particular distribution. In a real-world situation, we will not be
able to look at the data analysis in a chapter by chapter manner. The purpose of this
688
CHAPTER 14 Some Issues in Statistical Applications: An Overview

chapter is to present some methods in a unified way and to discuss generally the
various ways in which the techniques developed in previous chapters could be
applied to real-world data. Because the material in this chapter is a collection of
available techniques, we will not follow the more rigorous pattern of previous
chapters, and no proofs will be given.
14.2 GRAPHICAL METHODS
We first present some useful graphical methods that were not introduced in Chapter 1
on descriptive statistics. Graphical analysis is a very important aspect of any statis-
tical study. Before attempting a complex statistical analysis, summarize the data with
a graph. Graphical displays of data analysis help in data exploration, analysis, and
presentation and in communication of results. In data analysis, one of the significant
steps is to summarize and plot the data. Graphs help in the communication of final
results and recommendations inferred from quantitative models. A statistical model
is often suggested by an initial graphical analysis. Adequacy of statistical models
depends on the model conditions. Because the violations of these model assumptions
may sometimes occur as nonlinearities, graphical methods provide an easy and
perhaps very effective method of detection. Some examples of graphical displays
are the histograms, dotplots, box plots, and scatterplots. Methods of graphing
multivariate data are more complex and include scatterplot matrices, and icon plots.
These are beyond the level of this book.
If we have a data set with one variable (univariate), we first create a dotplot and
summary of basic statistics. In a dotplot, we plot the data as dots (one dot for each
observation) above the horizontal axis that covers the entire range of observations
(see Figure 14.1). The dotplot will provide us with an idea of the distribution of
the data and any unusual behavior of the data that may not be apparent from summary
statistics such as mean, median, or standard deviation. The dotplots allow us to
visualize the entire distribution of the data set by listing each possible outcome
and the frequency of the variable. Other ways of summarizing univariate data, such
as histograms, have been discussed in Chapter 1. The histogram differs from the
dotplot in that it groups data into categories. We illustrate these problems with sev-
eral examples.
EXAMPLE 14.2.1
The following data give the lifetime of 30 light bulbs (rounded to nearest hour) of a particular type.
1122 922
1146 1120 1079
905 1095
977 1138
966
1150 977
1137 1088 1139 1055 1082 1053 1048 1132
1088 996
1102 1028 1130 1002
990 1052 1116 1135
Construct a dotplot.
Solution
Figure 14.1 is the dotplot for these data.
Continued
689
14.2 Graphical Methods

The dotplot suggests a distribution that is skewed toward the right, because most of the obser-
vations are located to the right.
Some of the graphing methods can also be applied to compare two variables—for
example, their frequency distributions. For instance, dotplots could also be used
to compare bivariate (two variables) or multivariate (many variables) data. When
we have independent samples, side-by-side box plots could be used for comparing
two sample distributions in terms of their centers, dispersions, and skewnesses.
When there are two variables, a scatter plot is used as one of the basic graphic
tools to examine the relationship between two variables.
The scatterplot in Figure 14.2 for two variables x and y indicates a possible linear
relation between x and y. The strength of the relationship between two variables is
often represented through a correlation statistic. It should be noted that the correla-
tion coefficient is a single number that is easy to calculate and comprehend, though it
only measures the strength of a linear relationship and hence is often used as the
primary statistic of interest. However, scatterplots provide information about the
strength of association, not necessarily linear, between variables. In addition,
scatterplots help us understand other aspects of the data, such as the range. Given
350
300
250
200
150
150
175
200
225
250
275
300
FIGURE 14.2
Scatterplot.
1155
1120
1085
1050
1015
980
945
910
FIGURE 14.1
Dotplot for lifetime of light bulbs.
690
CHAPTER 14 Some Issues in Statistical Applications: An Overview

n observations on two variables, X and Y, we plot a character or symbol at n points
representing (xi, yi). If two or more observations in a scatterplot are identical, the
plotted symbols will coincide, masking possibly important information.
EXAMPLE 14.2.2
The following data give the cholesterol levels before a certain treatment and after four months of the
treatment.
Before
235
212
277
262
162
212
226
252
185
276
216
315
289
283
234
223
275
282
311
285
After
233
214
200
266
146
212
238
284
191
247
244
268
241
289
220
202
221
196
212
247
Draw a scatterplot. Also find the correlation between before- and after-treatment values.
Solution
Figure 14.3 is a scatterplot of the data.
Looking at the scatterplot in Figure 14.3, we see a trend in the cholesterol levels before and
after the treatment. Correlation of before- and after-treatment data is measured by r, where
r ¼
Xn
i¼1 xi x
ð
Þ yi y
ð
Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
P xi x
ð
Þ2P yi y
ð
Þ2
q
:
The quantile-quantile (QQ) plot is another useful technique in comparing bivariate
data. In a QQ plot, the quantiles of the two samples are plotted against each other. For
two distributions that are almost the same, their quantiles would be nearly equal. As a
275
300
250
225
200
175
150
150
175
200
225
250
Before
After
275
300
325
FIGURE 14.3
Scatterplot for cholesterol levels.
691
14.2 Graphical Methods

result, the quantiles would plot along the 45-degree line. Deviation of plots from this
line can be used to draw inferences about how the two samples differ from one
another. If the two sample sizes n1 and n2 are equal, then we can draw the QQ plot
by graphing the order statistics x(i) and y(i) against each other. If the two samples are
not of the same size, then we can use the following procedure to create the QQ plot. If
n1>n2, then draw the (1/(ni+1))th quantiles of the two samples against each other.
For a large sample, they are the order statistics, x 1
ð Þ <  < x n1
ð
Þ. For the smaller sam-
ple sizes, the pth quantile value is obtained by using the following formula:
exp ¼
xp n + 1
ð
Þ,
if p n + 1
ð
Þ, is an integer
x m
ð Þ + p n + 1
ð
Þm
½
 x m + 1
ð
Þ x m
ð Þ


, if p n + 1
ð
Þ, is a fraction

(14.1)
where m denotes the integer part of p(n+1). It should be noted that a QQ plot is not
useful for paired data because the same quantiles based on the ordered observations
do not, in general, come from the same pair.
EXAMPLE 14.2.3
Draw a QQ plot for the data given in Example 14.2.2.
Solution
Here n1¼n2¼20. First sort the data in ascending order.
Before
162
185
212
212
216
223
226
234
235
252
262
275
276
277
282
283
285
289
311
315
After
146
191
196
200
202
212
212
214
220
221
233
238
241
244
247
247
266
268
284
289
Because the QQ plot points lie mostly below the 45-degree line, we may conjecture that the
cholesterol level before is generally higher than that after (Figure 14.4).
0
50
100
150
200
250
300
350
100
250
300
150
200
50
.
.
... ....
.
.......
..
FIGURE 14.4
Q-Q plot for cholesterol levels.
692
CHAPTER 14 Some Issues in Statistical Applications: An Overview

We saw in Chapter 1 that box plots could be used for identification of outliers. To
summarize, we emphasize that graphical procedures, although preliminary, are an
integral part of any statistical analysis.
14.2 EXERCISES
14.2.1 In order to study any possible relationship between expense and return, the
following data give percentage of expense ratio and total one-year return for
randomly selected stock mutual funds for the year 2000 (source: Money,
February 2000).
% Expense ratio
1.03
1.80
1.90
1.53
1.03
2.06
3.20
0.49
1.10
1.07
1.48
1.30
1.23
1.22
1.60
1.50
1.81
1.75
0.97
1.28
% Return
7.3
9.5
32.2
11.0
19.5
7.3
25.1
10.2
1.5
7.9
18.9
26.1
3.4
3.7
23.5
2.9
14.5
14.9
22.7
21.9
Draw a scatterplot. Also find the sample correlation of percent expense
ratio and percent return.
14.2.2 In order to study any possible relationship between age and change in
systolic blood pressure (BP) (mm Hg) in 24 h in response to a treatment, the
following data were obtained from 11 individuals.
Age
70
51
65
70
48
70
45
48
35
48
30
Systolic BP
change
28
10
8
15
8
10
12
3
1
5
5
(a) Draw a scatterplot.
(b) Find the sample correlation of age and systolic BP.
(c) Fit a least-squares regression line.
(d) Interpret (a), (b), and (c).
14.2.3 The following data represent 15 randomly selected state finances: revenue
and expenditures (in millions of dollars) for the fiscal year 1997 (source: The
World Almanac and Book of Facts 2000).
Revenue:
9439
26,538
8845
5,537
14,520
6,494
24,028
2818
39,038
49,318
5215
4229
20,128
7724
7467
Expenditure:
5722
25,791
7685
4808
13,862
5130
21,975
2426
35,302
39,296
4441
4002
16,200
6818
7145
(a) Draw a scatterplot.
(b) Find the sample correlation between revenue and expenditure.
(c) Draw a QQ plot.
(d) Interpret (a), (b), and (c).
693
14.2 Graphical Methods

14.2.4 The following data give birth rates (per 1000 population) for 20 selected
states in 1998 (source: The World Almanac and Book of Facts 2000).
14:4 16:3 13:5 14:6 13:7 15:6 10:9 12:8 13:0 14:2
13:4 13:9 15:9 13:3 14:1 15:7 15:2 13:9 15:4 11:3:
14.2.5 Construct a dotplot and interpret.
The following data give the median prices (rounded to nearest $1000) of
single-family homes for 18 randomly selected US cities in 1998 (source: The
World Almanac and Book of Facts 2000).
128 146 109
90
105 152 79
89
109
93
108 128 188 158
93
78 123 137
Construct a dotplot and interpret.
14.3 OUTLIERS
All statistical procedures make assumptions about a population and the sample
values obtained from the population. Before we proceed to analyze the data, we must
check to see if there are any outliers, that is, data points that do not belong in the data
set or are not in line with the rest of the data.
Outliers are observations that appear to have an abnormal value as compared with
the rest of the values in the data set; that is, the value of an outlier is either much
higher or significantly lower than any other value in the data set. An outlier could
be a discordant observation or a contaminant. A discordant observation is one that
appears surprising or discrepant to the investigator and is to some extent subjective.
A contaminant is an observation that is from a different distribution than the rest of
the data. Outliers may occur as a result of some limitations on measuring techniques
or recording errors. They may also be due to the sample not being entirely from the
same population. Extreme values in a data set could also be due to a skewed
population. It should be noted that sometimes a data point that is labeled as an outlier
may really be indicative of a novel phenomenon. In these cases, an extreme
observation may not be classified as an outlier.
The presence of outliers can dramatically affect the estimate of the mean and
variance of the sample, especially if the sample size is small. As a result, any test
statistic computed from such data would be unreliable, and so would be the statistical
inferences. For example, presence of outliers might lead to an incorrect conclusion
that the variances of two samples are not equal if the outlier is the result of a record-
ing or measurement error.
In a controlled experiment, such as in a laboratory setting, good record
keeping with a clear understanding of the phenomenon under investigation and
information about all the data will minimize the occurrence of outliers due to
recording errors.
694
CHAPTER 14 Some Issues in Statistical Applications: An Overview

What to do with outliers? As long as these points remain observations, we cannot
throw them out on a whim. There are basically two methods that are employed in
dealing with outliers. One method is to use statistical testing procedures to detect
outliers, possibly removing them from the data set if we know that these are
measurement errors, incorrectly entered values, or impossible values in real life,
and letting the analysis deal only with the rest of the data. The second method is
to use statistical procedures, such as nonparametric tests or data transformations, that
are immune or only minimally sensitive to the presence of outliers. Of course, we
could run the analysis both with and without the outliers and report both results.
We now present some commonly used tests for labeling outliers.
In data analysis, it is necessary to label suspected outliers for further study. For
normally distributed data, we give three simple methods to identify an outlier:
z-score, modified z-score, and box plot.
In a z-test, first find the z-scores of the entire data set and label any observation
with a z-score greater than 3 or less than 3 as an outlier. Recall that for the observed
values x1, . . ., xn, the z-score is defined by
zi ¼ xi x
s
where s is the sample standard deviation of the sample, that is,
s ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n1
X
n
i¼1
xi x
ð
Þ2
s
:
Because both the sample mean and the sample standard deviation are affected by the
outliers, this labeling method is not very reliable.
In a modified z-score test, the median of absolute deviation (MAD) about the
median is used. Let
MAD ¼ median
xi m
j
j
ð
Þ
where m is the median of the observations. Then
zi ¼ xi x
ð
Þ
MAD :
An observation is labeled as an outlier if the corresponding modified z-score is
greater than 3.5. A normal plot may be used for testing normality for the data.
If we want a reasonably robust distribution-free test, an observation x0 is labeled
as an outlier if
x0 m
j
j
MAD > 5:
Here, the choice of 5 is somewhat arbitrary.
A box plot (also called box-and-whisker plot) gives a method of labeling outliers
through a graphical representation. We have seen the method of construction of box
plots in Chapter 1. A box plot consists of a box, whiskers, and outliers. We draw a
695
14.3 Outliers

line across the box at the median. For example, in Minitab, the bottom of the box is at
the first quartile (Q1) and the top is at the third quartile (Q3). The whiskers are the
lines that extend from the top and bottom of the box to the adjacent values, the lowest
and highest observations still inside the region defined by the lower limit Q11.5
(Q3Q1) and the upper limit Q1+1.5(Q3Q1). Outliers are points outside the
lower and upper limits, plotted with asterisks (*).
EXAMPLE 14.3.1
The following data give the hours worked by 25 employees of a company in a randomly selected
week.
45 40 39 36 42 40 55 58 42 41
48 50 47 54 40 34 18 40 60 56
42 43 46 43 54
Label all possible outliers using:
(a) z-Score test, distribution-free test, and modified z-score test.
(b) Box plot.
Solution
(a) We can create Table 14.1, where dfree z stands for the distribution-free scores, and modified
stands for the modified z-scores. By the z-score test, there are no outliers. Using the
distribution-free test, the 18 is the only outlier. By the modified z-score test, 18 and 60 are pos-
sible outliers.
(b) The box plot is given in Figure 14.5.
Hence the observation 18 is identified as an outlier using the box plot.
Table 14.1 Hours Worked and Modified Scores
Data
z-Score
dfree z
Modified
45
0.05355
0.12
0.12
40
0.50427
1.13
1.13
39
0.61583
1.38
1.38
36
0.95053
2.13
2.13
42
0.28114
0.63
0.63
40
0.50427
1.13
1.13
55
1.16919
2.62
2.62
58
1.50389
3.75
3.37
42
0.28114
0.63
0.63
41
0.39271
0.88
0.88
48
0.38824
0.87
0.87
50
0.61137
1.37
1.37
47
0.27668
0.62
0.62
54
1.05763
2.37
2.37
40
0.50427
1.13
1.13
696
CHAPTER 14 Some Issues in Statistical Applications: An Overview

Once we identify the outliers, then the question is what to do with them. If we can
rule out recording errors as the source of outliers, the situation becomes more diffi-
cult. It is often impossible to say whether an outlier is really an extreme value within
a skewed population or whether it represents a value drawn from a different popu-
lation. As we indicated earlier, an outlier can be a legitimate observation representing
special feature of the sample population. In those cases, discarding the outliers may
simplify the statistical analysis, although it also reduces the usefulness of such anal-
ysis. Understanding the experiment that generated the data might help in determining
whether to discard or keep the outliers.
Once we decide to include the outliers, there are two possible ways to deal with
them. One is to transform the data, such as by taking the natural logarithm, so as to
reduce the undue influence of the outliers. Another possibility is to perform the anal-
ysis twice, with and without outliers, and report both results.
If we have bivariate data, a scatterplot may reveal any possible outliers; see
Figure 14.27. There are other methods available to detect multivariate data.
34
1.17366
2.63
2.63
18
2.95868
6.63
6.63
40
0.50427
1.13
1.13
60
1.72701
3.87
3.87
56
1.28076
2.87
2.87
42
0.28114
0.63
0.63
43
0.16958
0.38
0.38
46
0.16512
0.37
0.37
43
0.16958
0.38
0.38
54
1.05763
2.37
2.37
60
50
40
30
20
C1
FIGURE 14.5
Box plot for hours of work per week.
697
14.3 Outliers

14.3 EXERCISES
14.3.1 Motor vehicle thefts are a big problem in cities. Table 14.3.1 displays data
on motor vehicle thefts per 100,000 population in the year 1997 for 15
randomly selected large US cities (source: Statistical Abstracts of the United
States, 1999).
Label all possible outliers using:
(a) (i) z-Score test, (ii) distribution-free test, and (iii) modified z-score test.
(b) Box plot.
14.3.2 For the data of Example 14.2.1, label all possible outliers using:
(a) (i) z-Score test, (ii) distribution-free test, and (iii) modified z-score test.
(b) Box plot.
14.3.3 The following data represent test scores of 36 randomly selected students
from a large mathematics class.
67 63 39 80 64 95 90 93 21 36 44
66
100 66 72 34 78 66 68 98 74 81 71 100
60 50 81 66 90 89 86 49 77 63 58
43
Label all possible outliers using:
(a) (i) z-Score test, (ii) distribution-free test, and (iii) modified z-score test.
(b) Box plot.
14.3.4 The following data represent the number of days in 1997 on which selected
US metropolitan areas failed to meet acceptable air-quality standards at
trend sites (source: The World Almanac and Book of Facts 2000).
26 55 30 8
9 15
0 12
3 50 16
47
0 63 3
0 19 23
3 32 15 20
106
2 15 1 14
0
1 44 28
Label all possible outliers using:
(a) (i) z-Score test, (ii) distribution-free test, and (iii) modified z-score test.
(b) Box plot.
Table 14.3.1 Motor Vehicle Thefts per 100,000 Population
Chicago, IL
1215.1
San Antonio, TX
830.0
Columbus, OH
1109.9
Charlotte, NC
780.1
Nashville, TN
1536.5
Tucson, AZ
1403.3
Albuquerque, NM
1797.8
Atlanta, GA
1869.7
Sacramento, CA
1630.5
St. Louis, MO
2152.8
Toledo, OH
939.7
Tampa, FL
1410.0
Birmingham, AL
1219.7
Anchorage, AK
532.8
Norfolk, VA
519.9
698
CHAPTER 14 Some Issues in Statistical Applications: An Overview

14.4 CHECKING ASSUMPTIONS
With some exceptions, checking data for agreement with assumptions is not a topic
that is strongly emphasized in other textbooks at this level. Even in more advanced
books, this step is frequently omitted. In order for the inferences to work correctly,
the measured variables must conform to assumptions that underlie the statistical
procedures to be applied. In hypothesis testing such as the t-tests and ANOVA,
we made some fundamental assumptions that the random samples need to satisfy
for the tests to yield correct results.
As an example the basic assumptions underlying a t-test are:
(i) The sample comes from a normal population.
(ii) The sample is random. In case of two sample tests (excluding paired tests),
the measurements in one sample are independent of those in the other
sample.
(iii) When we are given two random samples, most of the results assume the
equality of population variances, that is, s1
2¼s2
2. This assumption is called
the homogeneity of variances. The test for equality of variance may have to be
performed first if we doubt the equality of the variance.
Likewise, analysis of variance is based on a model that requires the following
three primary assumptions:
(i) The samples come from normal populations.
(ii) Each of the samples is randomly selected from each group, and the samples are
independent of each other.
(iii) The population variances for all the samples are equal. That is, if we have k
populations with variances s1
2, i¼1, 2, . . ., k, then s1
2¼s2
2¼  ¼sk
2.
When we say we have a random sample, we implicitly assume that the data are
identically distributed. The presence of outliers in an observed sample may affect
such an assumption. We now explain a few tests for checking these assumptions such
as the assumptions of normality, data transformations, and equality of variances.
14.4.1 CHECKING THE ASSUMPTION OF NORMALITY
We start with the assumption of normality. Let us consider the example of randomly
selected scores of 28 calculus students.
EXAMPLE 14.4.1
Giveninthefollowingtablearethetestscoresof28randomlyselectedstudentsfromacalculus1class.
86 95 82 53 98 85 87 80 49 71 99 40 96 97
94 89 69 23 72 76 78 91 96 77 77 91 35 47
Continued
699
14.4 Checking Assumptions

Construct a dotplot and a histogram, and compute the percentage of observations that fall in the
intervals xs, x2s, and x3s:
Solution
The dotplot is shown in Figure 14.6.
The histogram is shown in Figure 14.7.
We have x¼71.18 and s¼20.99. Also, 57% of the random sample (i.e. 16 observations) fall in
the interval 71.1820.99¼(50.19, 92.17). There are 27 observations, or about 96%, that fall in
71.1841.98¼(29.2, 113.16), and all the observations fall in 71.1862.97¼(8.21, 134.94). This
suggests that the data set is approximately normally distributed. This procedure is the
empirical rule.
For the previous example, we have seen that the dotplot does not suggest any
normality. A histogram also does not suggest any normality (see Figure 14.7).
However, if we used the empirical rule as a test for normality, the data suggest
normality. Clearly this leads to a conflicting situation with a simple theoretical check
suggesting normality, while visual displays suggest nonnormality. In this case more
sophisticated procedures are warranted.
24
36
48
60
72
84
96
FIGURE 14.6
Dotplot of student scores.
20
0
1
2
3
4
Frequency
5
6
7
40
60
80
100
FIGURE 14.7
Histogram for student scores.
700
CHAPTER 14 Some Issues in Statistical Applications: An Overview

Sometimes, skewness and kurtosis can be used to test for tilt in and peakedness of
a distribution. After getting skewness and kurtosis from the descriptive statistics,
divide these by the standard errors. If both skew and kurtosis are within the 2 range,
the data can be considered normal.
We mention some sophisticated testing procedures for two of the most important
of the parametric assumptions when running single-factor trials, namely, normality
and homogeneity of variance. We have already seen in Project 4C how to construct a
normal probability plot and to check for normality. In this chapter, we will use the
Minitab normal plot to check for normality. Figure 14.8 graphs a normal probability
plot (using Minitab) for Example 14.4.1.
We see that the test scores follow the straight line on the normal probability plot
pretty well. The serious departures occur for the last four scores, because the values
fall well above the line. This suggests normality with possible outliers.
It should be noted that for skewed data, in the normal probability plot, positively
skewed data fall below the straight line, whereas the negatively skewed data rise
above the straight line. A normal probability plot for the lifetime of 30 light bulbs
in Example 14.2.1 is given in Figure 14.9.
This graph suggests that the data may not be normal and are more toward
negatively skewed. Figure 14.10 is a normal probability plot for 30 data points
generated from a standard normal distribution.
In this textbook, we have presented only simple graphical tests for testing of
normality. We should mention that in the literature, a variety of procedures for
0.001
20
30
40
50
60
C1
70
80
90
100
Probability
Normal probability plot
0.01
0.05
0.20
0.50
0.80
0.95
0.99
0.999



 

  

Anderson Darling Normality Test
A-Squared: 1.256
p-value: 0.002
Average: 76.1756
Std Dev: 20.5979
N of data: 2.5
FIGURE 14.8
Normal probability plot of student scores.
701
14.4 Checking Assumptions

testing for normality are available, including the Kolmogorov-Smirnov test, the
Shapiro-Wilks W test, and the Lilliefors test. Some of these tests are incorporated
in statistical software packages such as Minitab and could be performed as easily as
the graphical tests. If the sample size is very small, with any of these tests it may be
0.001
C4
860
Average: 1021.07
Std Dev: 91.7557
N of data: 2.0
Anderson-Darling Normality Test
A-Squared: 0.510
p-value: 0.022
960
1060
1160
Probability
Normal probability plot
0.01
0.05
0.20
0.50
0.80
0.95
0.99
0.999











  

FIGURE 14.9
Normal probability plot for the lifetime of light bulbs.
0.001
C5
1
0
1
2
Probability
Normal probability plot
0.01
0.05
0.20
0.50
0.80
0.95
0.99
0.999

































Average: 0.0567849
Std Dev: 0.901967
N of data: 2.0
Anderson-Darling Normality Test
A-Squared: 0.562
p-value: 0.124
FIGURE 14.10
Normal probability plot of data from a standard normal distribution.
702
CHAPTER 14 Some Issues in Statistical Applications: An Overview

difficult to detect assumption violations. It is important to keep in mind that these
tests are only rough indicators of assumption violations. For small sample sizes,
even when the tests show that none of the test assumptions is violated, a normality
test may not have sufficient power to detect a significant departure from normality,
though it is present.
14.4.2 DATA TRANSFORMATION
Many data in real life do not meet the assumptions of parametric statistical tests: they
may not be normally distributed, the variances may not be homogeneous, or both.
Using most of the parametrical tests on those data may give a misleading result. Data
transformation uses mathematical operations (filters) on each of the observations,
transforming the original scores into a new set of scores. An appropriate transforma-
tion may (i) reduce the influence of outliers, (ii) make data, from a nonnormal
distribution, more normal, and/or (iii) make the variances of different data sets more
homogeneous. Some of the more commonly used transformations are (i) power
transformations such as square root, (ii) logarithm, (iii) reciprocal, and (iv) arcsine.
Used correctly, data transformation can be a useful tool for the practitioner. Some of
these transformations can be put into a popular class of transformations called the
Box-Cox power law transformation
y ¼ xl 1
l
where l can be optimally adjusted from 0 to 1. For example, as l!0, we obtain the
y¼ln x (logarithmic filter) transformation, and when l¼1/2, we get the square root
transformation.
Even though we have done a statistical test on a transformed variable, it is not a
good idea to report the summary statistics such as mean, standard errors, etc. in trans-
formed units. We should back transform by doing the opposite of the mathematical
function we used in the data transformation. For instance, if we had originally used
the natural logarithm, we should use exponential transformation as the back
transformation. For instance, if we got a symmetric confidence interval for
transformed mean as in Chapter 5 which is symmetric for a ln transformed data,
we should take exponentials of the lower and upper limits. In the process, we
may lose the symmetry of the confidence interval.
As we have seen in Project 9A, it is sometimes possible to use appropriate data
transformations to transform nonnormal data into approximately normal data. Then
we can use this normality property to perform statistical analysis on these trans-
formed values. For instance, if the distribution of data has a long tail (which could
be seen by drawing a histogram of observations) or a few laggards on the right (which
could be seen by drawing a dotplot of observations), the
ﬃﬃﬃx
p or ln x transforms will
pull larger values down further than they pull the smaller or center values. Sometimes
it is necessary to try several different transformations (trial and error) in order to find
one that is more appropriate.
703
14.4 Checking Assumptions

EXAMPLE 14.4.2
Consider the following data from an experiment.
1:15 3:84 0:01 2:06 3:28 2:61 0:59 3:19 1:32 1:07
7:80 1:74 0:25 0:21 3:42 4:52 0:43 0:38 0:07 1:26
4:03 7:28 0:85 3:24 0:62
(a) Draw a histogram and normal plot.
(b) Take the transform y ¼
ﬃﬃﬃx
p and draw a histogram and normal plot for the transformed data.
Solution
(a) The histogram and normal plots for the data are shown in Figures 14.11 and 14.12.These graphs
clearly show that the data do not follow a normal distribution.
8
6
4
2
0
9
8
7
6
5
4
3
2
1
0
C3
Frequency
FIGURE 14.11
A histogram of the data.
0.001
C3
1
2
0
3
4
5
6
7
8
Probability
Normal probability plot
0.01
0.05
0.20
0.50
0.80
0.95
0.99
0.999





 








Average: 2.21297
Std Dev: 2.12252
N of data: 25
Anderson-Darling Normality Test
A-Squared: 1.033
p-value: 0.003
FIGURE 14.12
Normal probability plot of the data.
704
CHAPTER 14 Some Issues in Statistical Applications: An Overview

(b) The histogram and normal plot for the transformed data are shown in Figures 14.13
and 14.14. With this transformation (filter), we can see that the filtered data follow normality
We have only pointed out transformations in single-variable cases. The transfor-
mation methods are also useful in multivariable and multi-factor studies; however,
these involve more difficult analysis.
0
0.001


  


 



Probability
Normal probability plot
0.01
0.05
0.20
0.50
0.80
0.95
0.99
0.999
1
C5
2
Average  : 1.20944
Std Dev  : 0.7242
N of data: 25
Anderson-Darling Normality Test
A-Squared: 0.289
p-value: 0.040
FIGURE 14.14
Normal probability plot of the transformed data.
3
2
1
0
10
5
0
C5
Frequency
FIGURE 14.13
Histogram of the transformed data.
705
14.4 Checking Assumptions

14.4.3 TEST FOR EQUALITY OF VARIANCES
Now we discuss the tests for equality of variances, that is, the tests for heteroscedas-
ticity. Our recommendation is that, in a real-world problem, after accounting for out-
liers one should conduct tests for normality and heterogeneity of variance routinely
before analyzing any data. Here, we give two tests. One, for the two-sample case, is
based on the F-test, and for the multisampling case we give Levene’s test based on
analysis of variance procedures. Albert Madansky’s book Prescriptions for Working
Statisticians (Springer-Verlag, 1988) gives various other tests for normality and
heteroscedasticity.
(a) Testing Equality of Variances for Two Normal Populations
The following procedure has already been discussed in the hypothesis testing
chapter. For the sake of completeness, here we again briefly discuss this procedure.
Let X11, ..., X1n1 be a random sample from an N(m1,s1
2) distribution and
X21, ..., X2n2 be a random sample from an N(m2,s2
2) distribution. Assume that the
X1i’s and X2j’s are independent of each other for all i, j. Let
xi ¼ 1
ni
X
ni
j¼1
xij, i ¼ 1,2:
Assuming that m1 and m2 are unknown, we can test the hypothesis that s1
2¼s2
2 based
on the ratio
F ¼ s2
1
s2
2
¼
Xn1
j¼1 x1j x1

2= n1 1
ð
Þ
Xn2
j¼1 x2j x2

2= n2 1
ð
Þ
:
We know that (n11)s1
2/s1
2 has a w2(n11) distribution and (n21)s2
2/s2
2 has a
w2(n21) distribution. Therefore, under the null hypothesis H0: s1
2¼s2
2, the statistic
F has an F(n11, n21) distribution.
Based on the alternate hypothesis, we will reject the equality of variance
assumption if the test statistic falls into the appropriate tail of the F-distribution.
For example, if Ha: s1
2>s2
2 with a¼0.05, we would reject H0 when F>F0.95
(n11, n21), and if Ha:s1
2<s2
2 with a¼0.05, we would reject H0 when
FF0.05(ni1, n21). When Ha: s1
26¼s2
2 with a¼0.05, we would reject H0 when
FF0 975(n11, n21) or FF0 025(n11, n21). It should be noted that in the
case of a two-sided alternative, this procedure is not the best one in the sense of
minimizing the type II error. However, for simplicity, we will not discuss the opti-
mal two-tailed procedure.
EXAMPLE 14.4.3
An aquaculture farm takes water from a stream and returns it after it has circulated through the fish
tanks. Suppose the owner thinks that, because the water circulates rather quickly through the tank,
there is little organic matter in the effluent. To find out, some samples of the water are taken at the
intake and other samples are taken at the downstream outlet, and tests are performed for biochemical
oxygen demand (BOD). If BOD increases, it can be said that the effluent contains more organic
matter than the stream can handle. Table 14.2 gives the data for this problem.
706
CHAPTER 14 Some Issues in Statistical Applications: An Overview

(a) Using normal plots, check for normality of each sample.
(b) Test for the equality of variances of the BOD for the downstream and upstream samples at
a¼0.05.
Solution
(a) The normal plots are shown in Figures 14.15 and 14.16.The BOD data for the downstream and
upstream samples are approximately normal.
(b) We test H0: s1
2¼s2
2 versus Ha: s1
26¼s2
2. We have n1¼n2¼10, and a¼0.05. Because the nor-
mal plots of each sample conform with the normality assumption, we can use the F-statistic:
F ¼ s2
1
s2
2
¼ 0:729
ð
Þ2
0:654
ð
Þ2 ¼ 1:2425:
Continued
0.001
C1
5.8
6.8
7.8
Probability
Upstream
0.01
0.05
0.20
0.50
0.80
0.95
0.99
0.999




 




Average : 6.6147
Std Dev : 0.725827
N of data : 10
Anderson-Darling Normality Test
A-Squared : 0.236
p-value : 0.716
FIGURE 14.15
Normal plot of upstream data.
Table 14.2 Biochemical Oxygen Demand
Upstream
Downstream
7.863
8.132
5.714
9.128
5.871
7.574
6.479
8.678
7.124
9.336
7.539
8.798
6.682
8.457
5.877
9.756
6.227
8.548
6.771
7.992
707
14.4 Checking Assumptions

From the F-table, the rejection region is {FF0.025(9, 9)¼0.248} or {F>F0.975(9, 9)¼4.03}.
Because the observed value of the test statistic does not fall in the rejection region, we conclude
based on the sample evidence that the variances of the two populations are equal.
(b) Test for Equality of Variances, k2 Populations
Generalizing to k populations, let Xi1, Xi2, ..., Xini be a random sample from
N(mi,si
2) distributions for i¼1, 2, with both mi0s and si0s unknown. Also assume that
Xij, Xkl are independent for all (i, j), (k, l). We wish to test the hypothesis
H0:s1
2¼s2
2¼¼sk
2 against Ha: At least one of the si
2 is different. There are many
tests available. One of the basic graphical procedures is to use a side-by-side box
plots (see Example 10.3.1). We describe Levene’s test based on the analysis of
variance (source: Levene, 1960).
Let yij ¼ jxij xij: Now perform an analysis of variance test for equality of the
means of the yij. Let
n ¼
X
k
i¼1
ni, yi: ¼
X
ni
j¼1
yij=ni and y.. ¼
X
k
i¼1
X
ni
j¼1
yij
X
k
i¼1
ni:
The analysis of variance statistic is
z ¼
Xk
i¼1ni yi: y..

2= k 1
ð
Þ
Xk
i¼1
Xni
j¼1 yij yi:

2
= nk
ð
Þ
¼ MST
MSE:
Recall that MST (mean square for treatments) and MSE (mean square error) were
defined in Section 10.3; the MST is a measure of the variability between the sample
0.001
C2
7.5
8.5
9.5
Probability
Down stream
0.01
0.05
0.20
0.50
0.80
0.95
0.99
0.999



   



Average : 6.6299
Std Dev : 0.654107
N of data : 10
Anderson-Darling Normality Test
A-Squared : 0.108
p-value : 0.883
FIGURE 14.16
Normal plot of downstream data.
708
CHAPTER 14 Some Issues in Statistical Applications: An Overview

means of the groups and the MSE is a measure of variability within the groups. For a
95% confidence level, the rejection region is {z>F0.95 (k1, nk)}.
Itshouldbenotedthattheyijisnotindependent,buttheanalysisofvariancemethod
is found to be robust against the deviation from this assumption of independence.
EXAMPLE 14.4.4
The three random samples in Table 14.3 are independently obtained from three different normal
populations.
At the a¼0.05 level of significance, test for equality of variances.
Solution
We test H0: s1
2¼s2
2¼s3
2 versus Ha: Not all the si
2 are equal. For this sample, x1 ¼ 76, x2 ¼ 66:33,
and x3 ¼ 85:67. Also n¼11, and k¼3. Letting yij ¼ jxij xij, we obtain the following yij values:
12
10.33
4.67000
8
7.67
6.33000
1
2.67
1.67000
1
4
The test statistic is
z ¼
Xk
i¼1ni yi: y..

2= k1
ð
Þ
Xk
i¼1
Xni
j¼1 yij yi:

2
= nk
ð
Þ
¼ MST
MSE ¼ 5:5
16:5 ¼ 0:33:
From the F-table, the 95% point is F0.05(2, 8)¼4.46. Hence the rejection region is {z>4.46}.
Because the observed value of z¼0.33 does not fall in the rejection region, the null hypothesis is not
rejected, and we conclude that the assumption of equality of variances seems to be justified.
Through our tests, if we find that the homogeneity of variance of the data is violated
significantly, then nonparametric tests are more appropriate. Another popular test for
equality of variance is Bartlett’s test.
14.4.4 TEST OF INDEPENDENCE
Almost all the results in this book assume that we have independent random samples.
In the situation where we suspect that the sample data may not be independent, per-
form a run test as described in Project 12B to test for independence. There are
Table 14.3 Three Independent Samples from Normal Population
Sample 1
Sample 2
Sample 3
64
56
81
84
74
92
75
69
84
77
80
709
14.4 Checking Assumptions

parametric procedures available to test independence; however, the run test is inde-
pendent of the distributional assumptions and simpler to perform. In general, whether
the two samples are independent of each other is decided by the structure of the
experiment from which they arise. In case of correlated samples, such as a set of
pre- and post-test observations on the same subject that are not independent, a
two-sample paired test may be more appropriate. Another popular method used to
check for independence is the chi-squared test of independence; see Section 7.6.2.
For time series data, the Durbin-Watson test (http://www.alchemygroup.net/Permu
tation%20Durbin-Watson%20Final.pdf) is effective.
In practical sampling situations, the underlying populations are unlikely to be
exactly normally distributed with homogeneity of variances. Both t-tests and
ANOVA are robust for reasonable departures in some of these assumptions.
However, these tests may not be robust with respect to certain other assumption
violations. For example, ANOVA is quite sensitive to the violation of independence
assumption. These factors need to be given special attention in data analysis.
14.4 EXERCISES
14.4.1 The scores of 25 randomly selected students from a large calculus class are
given below
47 73 90 22 68 86 94 32 88 86
80 97 48 70 61 82 67 73 78 55
63 59 42 46 90
(a) Test the data for normality.
(b) If the data are not normal, try a suitable transformation (filter) to make
the transformed data normal.
14.4.2 Refer to Example 14.3.1. Suppose we use the transformation yi¼ln xi for
each observation.
(a) Test whether the transformed data are normal.
(b) Determine whether the data value 18 is still an outlier in the
transformed data set.
14.4.3 The data shown in the following table related to the concealed weapons
permits issued in 13 randomly selected Florida counties in 1996.
31,603 20,873 15,963 10,294 8956 7901 6820
5695
5485
4827
3969 3278 1731
(a) Test whether the data are normal.
(b) If not, try a suitable transformation to make the transformed data normal.
14.4.4 The following table represents a summary by state for Medicare enrollment
(in thousands) for 15 randomly selected states in 1998 (source: Statistical
Abstracts of the United States, 1999).
665 3,757 623
757 541 448 478 2,728 103 771
224
86 623 1,373 713
710
CHAPTER 14 Some Issues in Statistical Applications: An Overview

(a) Test to determine whether the data are normal.
(b) If not, try a suitable transformation to make the transformed data
approximately normal.
(c) Test for outliers. If an observation is extreme, would you classify it as
an outlier?
14.4.5 Given in the following table are 15 randomly selected state expenditures
(in millions of dollars) for the fiscal year 1997 (source: The World Almanac
and Book of Facts 2000).
5722 7685 13,862 21,975 35,302 4441 16,200 25,791
4808 5130
2426 39,296
4002 6818
7145
(a) Test the data for normality.
(b) If the data are not normal, try a suitable transformation to make the
transformed data approximately normal.
14.4.6 For the data of Exercise 14.3.4.
(a) Test whether the data are normal.
(b) If not, try a suitable transformation to make the transformed data
approximately normal.
14.4.7 The following data give in-city mileage per gallon for 25 small and midsize
cars (source: Money Magazine, March 2001).
25 23 20 20 27 26 20 32 25 22
24 21 28 20 22 19 21 29 23 32
23 52 24 24 22
(a) Test to determine whether the data are normal.
(b) If not, try a suitable transformation to make the transformed data
approximately normal.
(c) Test for outliers. If an observation is extreme, would you classify it as
an outlier?
14.4.8 The following table gives in-state tuition costs (in dollars) for 15 randomly
selected colleges taken from a list of the 100 best values in public colleges
(source: Kiplinger’s Magazine, October 2000).
3788 4065 2196 7360 5212 4137 4060 3956 3975 7395
4058 3683 3999 3156 4354
(a) Test for outliers.
(b) Test whether the data are normal.
14.4.9 For the data of Exercise 14.2.1, test for equality of variances.
14.4.10 For the data of Exercise 14.2.3, test for equality of variances.
14.4.11 The following data represent a random sample of end-of-year bonuses for
lower-level managerial personnel employed by a large firm. Bonuses are
expressed in percentage of yearly salary.
Female
6.2
9.2
8.0
7.7
8.4
9.1
7.4
6.7
Male
8.9
10.0
9.4
8.8
12.0
9.9
11.7
9.8
711
14.4 Checking Assumptions

Test for equality of variances. State any assumptions you have made, and
interpret your result.
14.4.12 In an effort to investigate the premium charged by insurance companies for
auto insurance, an agency randomly selects a few drivers who are insured
by three different companies. These individuals have similar cars, driving
records, and level of coverage. Table 14.4.1 gives the premiums paid per
six months by these drivers with these three companies.
Test for equality of variances. State any assumptions you have made,
and interpret your result.
14.4.13 Three classes in elementary statistics are taught by three different persons,
a regular faculty member, a graduate teaching assistant, and an adjunct
from outside the university. At the end of the semester, each student is
given a standardized test. Five students are randomly picked from each of
these classes, and their scores are as shown in Table 14.4.2.
Test for equality of variances. State any assumptions you have made,
and interpret your result.
14.5 MODELING ISSUES
A model is a theoretical description in the language of mathematical statistics of a
physical phenomenon. Even though interpretations can be developed by analogy,
past experience, or intuition, the scientific approach requires a model for the
Table 14.4.1 Auto Insurance Premiums
Company I
Company II
Company III
396
348
378
438
360
330
336
522
294
318
474
432
Table 14.4.2 Exam Scores
Faculty
Teaching Assistant
Adjunct
93
88
86
61
90
56
87
76
73
75
82
90
92
58
47
712
CHAPTER 14 Some Issues in Statistical Applications: An Overview

phenomenon of interest. Models are simplifications (or approximations) of real-
world situations and are designed to make it easier to identify and to understand
relationships among variables. A good model is crucial for accurate estimation,
forecasting, or predicting. If the observed data show a good fit to the estimates
obtained through the model, we consider the model to be an adequate representa-
tion of the real-world phenomenon. If not, the model must be improved, to incor-
porate additional variables or modify the equations defining the relationships. In
statistical modeling, it is important not to lose perspective on the essential purpose
of the modeling effort. The emphasis should be on making these models work on
real data sets in lieu of spending a large amount of time on the capabilities of the
models. Even though the study of properties and abilities of models is important,
equally important is an ability to know when and how to fit models to a particular
data set. A regression line is a two-parameter model that depicts a linear depen-
dence of one variable on another. Again, it is not our objective to discuss all the
issues related to statistical modeling. We will only discuss briefly some simple
issues relevant to modeling.
14.5.1 A SIMPLE MODEL FOR UNIVARIATE DATA
Suppose that we have a data set that characterizes a phenomenon of interest. Sup-
pose our problem is to create a statistical model for the data set in the form of a
probability distribution from which the data set came. First we create a dotplot
and summary of the basic statistics. The dotplot will provide us with an idea of
the probability distribution of the data and any unusual behavior of the data that
will not be apparent from the basic statistics such as sample mean and sample stan-
dard deviation. Having identified the probability distribution of the sample statistic,
we can proceed to obtain 95% confidence limits on parameters such as the mean
and variance. In addition, we can obtain a 95% prediction interval of the next obser-
vation using the expression
y tvalue
ð
Þs
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1
n
r
:
Note that the prediction interval is always wider than the corresponding confidence
interval. The confidence interval provides a measure of reliability for estimating a
parameter. The prediction interval provides a measure of reliability for the prediction
of an observation. Thus, the prediction interval needs to account for estimation error
as well as the natural variability of a single observation. These steps can be consid-
ered as the first modeling effort for univariate data. Note that if we have a small sam-
ple size, using a t-value in the confidence interval and/or prediction interval supposes
a modeling assumption of normality for the corresponding population. The prelim-
inary verification of this is done by the dotplot. For more detailed verification of this
modeling assumption, use the normal plots.
713
14.5 Modeling Issues

EXAMPLE 14.5.1
Consider the following data from an experiment:
0.15
0.14
0.15
0.14
0.26
0.00
0.00
0.47
0.35
0.16
0.15
0.15
0.23
0.13
0.19
0.15
0.22
0.53
0.17
0.23
0.22
0.16
0.12
0.13
0.11
0.14
0.18
0.15
0.14
0.21
0.13
0.12
0.13
0.13
0.21
0.22
0.18
0.20
0.22
0.16
0.17
0.00
0.23
0.21
0.18
0.05
0.16
0.13
0.23
0.18
0.14
0.29
0.21
0.22
0.11
0.16
0.23
0.13
0.07
0.17
0.08
0.14
0.06
0.08
0.07
0.11
0.12
0.14
0.16
0.12
0.10
0.27
0.19
0.13
0.27
0.16
0.07
0.09
0.04
0.53
0.29
0.15
0.12
0.11
0.10
0.14
0.14
0.16
0.16
0.17
0.36
0.46
1.21
0.39
0.01
0.52
0.09
0.18
0.16
0.16
0.14
0.15
0.09
0.09
0.13
0.13
0.08
0.14
0.20
0.09
0.09
0.16
0.08
0.10
0.34
0.24
0.15
0.44
0.08
0.08
0.16
0.14
0.18
0.23
0.19
0.11
0.19
0.10
0.14
0.11
0.14
0.17
0.17
0.17
0.05
0.12
0.14
0.11
0.20
0.14
0.23
0.03
0.10
0.29
0.13
0.26
0.13
0.15
0.27
0.14
0.50
0.16
0.15
0.18
0.16
0.14
0.13
0.08
0.20
0.17
0.17
0.16
0.15
0.11
0.13
0.76
0.18
0.19
0.09
0.12
0.11
0.12
0.08
0.26
0.23
0.20
0.19
0.19
0.16
0.11
0.12
0.13
0.32
0.05
0.18
0.12
0.13
0.50
0.13
0.04
0.00
0.11
0.18
0.15
0.14
0.15
0.02
0.20
(a) Obtain a dotplot.
(b) Calculate the basic statistics, sample mean, sample median, and sample standard deviation.
(c) Obtain a 95% confidence interval for the true mean.
(d) Obtain a 95% prediction interval.
Solution
(a) Each dot in Figure 14.17 represents three points.
(b) We can use Minitab’s describe command to obtain the following.
N
Mean
Median
TR mean
St. dev
SE mean
C1
198
0.17038
0.15121
0.15982
0.13610
0.00967
Min
Max
Q1
Q3
0.39575
1.22076
0.12059
0.19284
(c) Again using Minitab commands, we can obtain (where data are stored in C1), MTB>ZInterval
95.0 0.136 c1.
The assumed Sigma=0.136
N
Mean
STdev
SE mean
95.0% C.I.
C1
198
0.17038
0.13610
0.00967
(0.15143, 0.18933)
(d) For the prediction interval use the large sample formula y za=2


s
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 + 1
n0
q
to obtain the 95%
prediction interval for the true mean as (0.097, 0.4387).
714
CHAPTER 14 Some Issues in Statistical Applications: An Overview

14.5.2 MODELING BIVARIATE DATA
When a scatterplot of bivariate data exhibits a linear pattern, the modeling is usu-
ally done using linear regression to study their linear relationship as explained in
Chapter 8. Clearly a linear relationship is desirable because it is easy to interpret,
departure from linearity is easy to detect, and predicting dependent values from
independent variables is straightforward. However, when a scatterplot shows a
curved nonlinear pattern, then finding a “good” model that fits the observed data
may not be very easy. Sometimes, instead of fitting a curve we may be able to
transform the data so as to make the scatterplots of the transformed data look
more linear.
A popular statistical method used to straighten a plot is the so-called power
transformation. The power transformation is defined by specifying an exponent,
k, which could be a positive or negative real number, then computing each trans-
formed value as the original value to the power k. Note that k¼1/2 gives the square
root transform. When k¼0, every transformed value is equal to 1. Instead it is cus-
tomary to think of k¼0 as corresponding to a logarithmic transformation so as to
unify the transformation concept. The power k¼1 corresponds to no transformation
at all. Observe that these are the same transformations we have explained in
Section 14.4.2 to transform nonnormal data into normal transformed data. The
shape of the scatterplots should suggest an appropriate transformation. The four
curves in Figure 14.18 represent possible shapes of scatterplots that are usually
encountered in practice.
0.00
0.18
0.36
0.54
0.72
0.90
1.08
FIGURE 14.17
Dotplot of the data.
715
14.5 Modeling Issues

We can use the following as a general guideline for making transformations.
If we have a scatterplot that looks like plot 1 of Figure 14.18, then to straighten
the plot, we should use a power k<1 for x (the independent variable) and/or use
a power k>1 for y (the dependent variable). Similarly, for curve 2, k>1 for x
and/or k<1 (such as
ﬃﬃﬃy
p
or ln y) for y. For curve 3, take k>1 for x (such as x2
or x3) and/or k>1 for y. Finally, for curve 4, take k<1 for x and/or k>1 for y. Once
we straighten the data through transformations, obtain the least-squares equation of
the line as explained in Chapter 8. By reversing the transformation (or solving for y
in the transformed equation) we can obtain the original nonlinear relationship
between x and y.
EXAMPLE 14.5.2
For the following bivariate data:
x
0
4
8
10
15
18
20
25
y
2.4
2.6
3.1
3.6
4.1
4.2
4.6
4.7
(a) Draw a scatterplot.
(b) Use appropriate transformation (if necessary) to linearize the scatterplot.
(c) Fit the data to an appropriate curve.
Solution
(a) The scatterplot is shown in Figure 14.19.
This looks more like curve 4.
(b) Let us use the transformation x0 ¼ln x and y0 ¼y2. We will get the scatterplot shown in
Figure 14.20.
This looks more linear.
(c) The regression line for the transformed data is y0 ¼8.86x0 6.96. Therefore, for the original
data, y2¼8.86 ln x6.96. The fitted curve is shown in Figure 14.21.
Looking at Figure 14.21, we can see that the data are only slightly nonlinear. In addition, using
the equation, for a given value of x we can predict the value of the response variable y. For instance,
if x¼1.5, we estimate y2 to be 3.3676.
y
y
x
x
x
x
4
3
2
1
y
y
FIGURE 14.18
Possible shapes of a scatterplot.
716
CHAPTER 14 Some Issues in Statistical Applications: An Overview

Continued
2.5
0
5
10
15
20
25
3.0
3.5
4.0
4.5
5.0
FIGURE 14.19
Scatterplot of the data.
22.5
20.0
17.5
15.0
12.5
10.0
7.5
5.0
1.50
1.75
2.00
2.25
2.50
2.75
3.00
3.25
FIGURE 14.20
Scatterplot of the transformed data.
717
14.5 Modeling Issues

There are various other modeling issues that one may encounter in applications. For
example, in multiple regression modeling, an investigator may have data on number of
predictor variables that mightbeincorporated into a model. Some of these variablesmay
be irrelevant or may duplicate the information provided by other variables. The problem
then is how to detect and eliminate the duplicating variables. However, for the sake of
brevity and level of presentation, we will not go into these issues of model selection.
EXERCISES 14.5
14.5.1 For the data of Exercise 14.4.5:
(a) Obtain a dotplot.
(b) Describe the data, such as mean, median, and standard deviation.
(c) Obtain a 95% confidence interval for the mean.
(d) Obtain a 95% prediction interval.
(e) Explain your solutions and state any assumptions.
14.5.2 For the gas mileage data of Exercise 14.4.7
(a) Obtain a dotplot.
(b) Describe the data, such as mean, median, and standard deviation.
(c) Obtain a 95% confidence interval for the mean.
(d) Obtain a 95% prediction interval.
14.5.3 The following represents the midterm and final exam scores for 35 randomly
selected students from a large mathematics class.
Midterm:
67
63
39
80
64
95
90
93
21
36
44
66
66
72
34
78
66
68
98
43
74
81
71
100
60
50
81
66
90
89
86
49
77
63
58
Final:
29
33
100
33
55
20
10
5
67
64
71
25
34
66
28
34
16
27
32
20
14
21
16
62
50
14
61
11
14
41
52
35
37
51
43
2
1
0
−1
−2
1.4
2.4
C3
C5
3.4
FIGURE 14.21
Fitted curve.
718
CHAPTER 14 Some Issues in Statistical Applications: An Overview

(a) Draw a scatterplot.
(b) Use appropriate transformation (if necessary) to linearize the
scatterplot.
(c) Fit the data to an appropriate curve and explain the usefulness.
14.5.4 For the state finance data of Exercise 14.2.3:
(a) Draw a scatterplot.
(b) Fit a least-squares line.
(c) Explain your solutions and state any assumptions.
14.5.5 Table 14.5.1 gives in-state tuition costs (in dollars) and four-year graduation
rate (%) for 15 randomly selected colleges taken from a list of the 100 best
values in public colleges (source: Kiplinger’s Magazine, October 2000).
(a) Draw a scatterplot.
(b) Fit a least-squares line and graph it.
(c) Looking at the scatterplot of part (a), do you think the least-squares line
is a good choice? Discuss.
14.6 PARAMETRIC VERSUS NONPARAMETRIC ANALYSIS
Up until Chapter 11, we basically assumed that random variables belong to specific
probability distributions, such as a normal distribution or binomial distribution.
The members of those distributions are associated by different parameters such as
means or variances. Most of our efforts were concentrated on making some inferences
about the unknown parameters. In this vein, we looked at point estimators, confidence
intervals, and hypothesis testing problems. In practice the assumption that observa-
tions come from a particular family of distributions such as normal or exponential
may be quite sensible. As we have already mentioned, slight violations of these
assumptions inmanypractical casesmay not significantlyaffect statisticalinferences.
However, this is not always true. Furthermore, sometimes we may want to make infer-
ences that have nothing to do with parameters. We may not even have precise mea-
surement data, but only the rank order of observations. For example, if we want to
study the performance of students at an institution, we may not have the precise scores
the students obtained; instead we may only have their letter grades such as A, B, C, D,
and F. Even if we have precise measurements, we may not be able to assume a distri-
bution, such as normality. Still, we may be able to say that the distribution is symmet-
ric, or skewed, or has some other characteristics. Basically, if there is doubt about the
Table 14.5.1 Tuition Amount Versus Graduation Rate
In-state Tuition:
3788
4065
2196
7360
5212
4137
4060
4354
Graduation Rate:
45
64
40
58
38
20
39
48
In-state Tuition:
3956
3975
7395
4058
3683
3999
3156
Graduation Rate:
40
20
45
39
39
20
9
48
719
14.6 Parametric Versus Nonparametric Analysis

parametric assumptions, or the data are not suitable for parametric inference, or we are
not interested in inference about parameters, a nonparametric test that is valid under
weaker assumptions is preferable. It should be noted that weaker assumptions do not
meanthatnonparametric methods areassumptionfree.Theinferencethatcanbemade
depends on valid assumptions that are made.
When using nonparametric tests, a common question is “Why substitute a set of
nonnormal numbers, such as ranks, for the original data?” Rank tests are often useful
in circumstances when we have no idea about the population distribution. We suspect
that the data are not normal, and either we cannot transform the data to make them
more normal, or we do not wish to do so. Few data are truly normal, despite the
robustness of common parametric tests; unless we are quite sure that the nonnorm-
ality is a minor problem and would not affect the conclusions, we may often be better
off using a rank test. However, there is a small penalty for using delete rank tests. If
the original data are really normal, in the long run, the rank tests will be about 95.5%
as efficient as a Student t-test would have been. This means that in such situations,
the t-test will require about 95 samples compared to 100 for the rank test. But when
data are far from normal, the rank tests will require fewer samples than the t-test; in
fact, we should not use the t-test in such cases.
Basically, if we know the distribution of the underlying population, we can use
parametric tests. Otherwise, for a given data set, we first perform the normality test as
explained in Section 14.3. If normality fails, in general, we can use nonparametric
methods for data analysis.
Another situation in which we can use nonparametric tests is when the data con-
tain some outliers. A box plot or a normal plot, as explained in Section 14.3, will
reveal the existence of outliers. However, in many applied areas such as in most bio-
availability data, there will appear to be outliers. It is not feasible to determine
whether these are skewed or contaminated distributions. They are not errors. In those
situations, a conservative approach will be to use nonparametric methods. For exam-
ple, because the statistic for the rank sum test is resistant to outliers, it will not be
seriously affected by the presence of outliers unless the number of outliers becomes
large relative to the sample size.
It should be noted that we ought to be careful even when we use nonparametric
tests. For example, if the data for one or both of the samples to be analyzed by a rank
sum test come from a population whose distribution violates the assumption that the
distributional shapes are the same, then the rank sum test on the original data may
provide misleading results or may not be the most powerful test available. Trans-
forming the data (for example, a logarithmic transformation pulls in long tails) to
obtain normality and then performing a two-sample t-test, or using another nonpara-
metric test, may be more appropriate for the analysis. In general, nonparametric
methods are appropriate when the sample sizes are small. When the data set is large,
say n>100, it often makes little sense to use nonparametric statistics.
Finally, we must conclude that we do not perform nonparametric tests on a given set
of data unless it is necessary, that is, if we cannot assume a classical probability distri-
bution that characterizes the given data. Also, parametric statistical analysis is, in
720
CHAPTER 14 Some Issues in Statistical Applications: An Overview

general, more powerful than the nonparametric analysis. We willend this section with a
quote from W.J. Conover: “Nonparametric methods use approximate solutions to exact
problems, while parametric methods use exact solutions to approximate problems.”
EXERCISES 14.6
14.6.1 Consider the following data.
0:01
0:012 0:016 0:018 0:036 0:042 0:036 0:048
0:072 0:042 0:22
0:096 0:76
0:055 0:13
0:016
(a) Test for normality and comment whether a parametric or nonparametric
test is appropriate.
(b) Try a suitable transformation (filter) to make the transformed data
normal, if possible, and then use a parametric procedure.
14.6.2 For the Medicare data of Exercise 14.4.4, if parametric procedures are not
appropriate, use a nonparametric procedure.
14.7 TYING IT ALL TOGETHER
Now we will give some real data on which we will use standard methods to analyze
the given data. Software reliability is a major aspect in any kind of software devel-
opment. One of the ways to do this is to observe time to failure and/or time between
failure (TBF). If the defects are fixed, we would expect, on average, the TBF to
increase. Based on that data, one studies reliability of the software. There are a vari-
ety of methods to analyze the software reliability problems. Here we will not dwell
on the reliability issues. We will only do some simple data analysis on a set of soft-
ware failure data. The following data represent software failure times in the Apollo
8 software system. They were obtained from www.dacs.dtic.mil/databases/sled/
swrel.shtml. It is assumed that these failure times are random.
EXAMPLE 14.7.1
The following data set consists of 26 software failure times taken from testing of the Apollo 8 soft-
ware system.
T :
9
21
32
36
43
45
50
58 63
70
71
77
78
87
91
92
95 98
104 105 116 149 156 247 249 250
TBF : 9 12 11
4 7
2 5 8 5
7
1
6
1 9
4 1 3 3
6
1 11 33 7 91 2 1
(a) Obtain a dotplot and describe the TBF data.
Continued
721
14.7 Tying it All Together

(b) Identify any outliers and test for normality with and without outliers for TBF data. If the data are
not normal, does any simple transformation make the data normal?
(c) Obtain a 95% confidence interval for TBF.
(d) For estimation problems, does a parametric or nonparametric method seem more appropriate for
the data?
(e) Obtain a scatterplot between T and TBF and discuss its usefulness.
Solution
(a) The dotplot for the TBF data is shown in Figure 14.22. The following is the result of the describe
command from Minitab.
TBF
N
Mean
Median
TR mean
St. dev
SE mean
26
9.62
5.50
6.58
17.79
3.49
TBF
Min
Max
Q1
Q3
1.00
91.00
2.00
9.00
(b) We will use the box plot shown in Figure 14.23 to identify the outliers.
From the box plot the observations 33 and 91 are outliers.
Figures 14.24 and 14.25 show the normal plots with and without outliers.
It is clear that the data with outliers are not normal, whereas if we remove the outliers, the data
become normal.
Figure 14.26 gives the normal plot by taking the natural log of the TBF data with outliers. The
figure shows that the data become approximately normal.
(c) It is clear that to obtain a small sample confidence interval, to satisfy the assumption of nor-
mality, we need to take the data without the outliers. Hence a 95% confidence interval for
TBF with the outliers removed is (3.77, 6.73). Running a nonparametric Wilcoxon test in Mini-
tab for the 95% confidence interval with outliers gave the following.
0
12
24
36
48
60
72
84
FIGURE 14.22
Dotplot of TBF data.
90
80
70
60
50
40
30
20
10
0
TBF
FIGURE 14.23
Box plot of TBF data.
722
CHAPTER 14 Some Issues in Statistical Applications: An Overview

Estimated
Achieved
TBF
N
Median
Confidence
Confidence interval
26
6.00
94.9
(4.00, 8.00)
(d) If we are analyzing the data without outliers or the log-transformed data, parametric methods
are better. With the original data, because the normality assumption may not be appropriate, we
need to use nonparametric methods.
(e) Figure 14.27 gives the scatterplot of T and TBF.
Continued
Anderson-Darling Normality Test
A-Squared: 5.075
p-value: 0.000
Average: 9.61539
Std Dev: 17.7878
N of data: 26
90
80
70
60
50
40
30
20
10
0
.999
.99
.95
.80
.50
.20
.05
.01
.001
Probability
TBF
Normal plot with outliers
FIGURE 14.24
Normal probability plot of TBF data with outliers.
A-Squared: 0.504
p-value: 0.184
Anderson-Darling Normality Test
Average: 5.25
Std Dev: 3.50466
N of data: 24
12
7
2
.999
.99
.95
.80
.50
.20
.05
.01
.001
Probability
TBF-OL
Normal plot without outliers
FIGURE 14.25
Normal probability plot of TBF data without outliers.
723
14.7 Tying it All Together

A-Squared: 0.576
p-value: 0.121
Anderson-Darling Normality Test
Average: 1.56762
Std Dev: 1.10478
N of data: 26
4
3
2
1
0
.999
.99
.95
.80
.50
.20
.05
.01
.001
Probability
C4
Normal plot of ln (TBF)
FIGURE 14.26
Normal probability plot of transformed TBF data with outliers.
0
0
10
20
30
40
50
TBF
60
70
80
90
50
100
150
T
200
250
FIGURE 14.27
Scatterplot of T and TBF.
724
CHAPTER 14 Some Issues in Statistical Applications: An Overview

EXAMPLE 14.7.2
Table 14.4 gives dealer cost and sticker price for four-door base models of 25 small and midsize cars
(source: Money Magazine, March 2001).
(a) Obtain a dotplot and describe the sticker price data.
(b) Identify any outliers and test for normality with and without outliers for sticker price data. If the
data are not normal, does any simple transformation make the data normal?
(c) Obtain a 95% confidence interval for sticker price.
(d) For estimation problems, do parametric or nonparametric methods seem more appropriate for
the data?
(e) Obtain a scatterplot between dealer cost and sticker price.
(f) Fit a least-squares regression line and run a residual model diagnostic using Minitab.
Solution
(a) The dotplot for the sticker price is shown in Figure 14.28.
The following summary statistics are obtained by the describe command in Minitab.
Continued
Table 14.4 Dealer Cost and Sticker Price
Model
Dealer Cost (in dollars)
Sticker Price (in dollars)
Acura Integra GS
19,479
21,600
Chevy Cavalier
12,398
13,260
Chevy Impala LS
21,251
23,225
Chrysler Concord LX
20,834
22,510
Dodge Neon SE
11,856
12,715
Ford Escort
12,277
12,970
Ford Taurus SE
17,606
19,035
Honda Civic DX
11,723
12,960
Honda Accord 2.3 LX
16,727
18,790
Hyundai Sonata
13,805
14,999
Kia Sephia
9914
10,595
Mazda 626 LX V6
18,181
19,935
Mitsubishi Mirage ES
Mercury Sable GS
12,534
17,777
13,627
19,185
Nissan Maxima GXE
19,430
21,249
Oldsmobile Intrigue GL
22,097
24,150
Pontiac Grand Am GT
18,790
20,535
Saturn SL
9936
10,570
Subaru Impreza L
14,695
15,995
Toyota Corolla LE
12,042
13,383
Toyota Camry LE
18,169
20,415
Toyota Prius
18,793
19,995
VW Jetta GLS
15,347
16,500
VW Passat GLS
19,519
21,450
Volvo S40
22,090
23,500
725
14.7 Tying it All Together

N
Mean
Median
TR mean
St. dev
SE mean
St. price
25
17726
19035
17758
4278
856
Min
Max
Q1
Q3
St. price
10570
24150
13322
21350
(b) The box plot for the sticker price is shown in Figure 14.29.
According to this, there are no outliers. The normal plot is shown in Figure 14.30. This is
approximately normal.
(c) The 95% confidence interval for the sticker price is
N
Mean
St. dev
SE mean
95.0% C.I.
St. price
25
17,726
4278
856
(15,960, 19,492)
(d) Because there are no outliers and the data look approximately normal, parametric tests seems to
be appropriate for these data.
(e) The scatterplot for dealer cost versus sticker price is shown in Figure 14.31.
(f) Figure 14.32 shows the fitted regression line.
An analysis of residuals by Minitab gives Figure 14.33.
By looking at the residuals versus fits, we can see that we have a good fit, and hence the model
looks appropriate.
25000
20000
15000
10000
St.price
FIGURE 14.29
Box plot for the sticker price.
10800
12600
14400
16200
18000
19800
21600
23400
FIGURE 14.28
Dotplot for the sticker price.
726
CHAPTER 14 Some Issues in Statistical Applications: An Overview

Continued
Probability
Normal plot for the sticker price
Sticker price
0.999
0.99
0.95
0.80
0.50
0.20
0.05
0.01
0.001
10000
Average: 17.7259
Std Dev: 42.7781
N of data: 2.5
Anderson-Darling Normality Test
A-squared: 0.721
p-value: 0.052
12000
14000
16000
18000
20000
22000
24000
FIGURE 14.30
Normal plot for the sticker price.
24000
22000
22000
20000
20000
18000
18000
16000
16000
Dealer Cost
Sticker price
14000
14000
12000
12000
10000
10000
FIGURE 14.31
Scatterplot for dealer cost versus sticker price.
727
14.7 Tying it All Together

20000
15000
10000
25000
20000
15000
10000
Del.cost
St.price
Y = −191.308 + 1.09984X
R-squared =    0.995
95.0% Prediction bands
95.0% Confidence bands
Delear cost vs. Sticker price
FIGURE 14.32
Regression line for dealer cost versus sticker price.
2
1
0
-1
-2
500
0
-500
Z-score
Residual
Normal plot of residuals
650
400
150
-100
-350
-600
10
5
0
Residual
Frequency
Histogram of residuals
25
20
15
10
5
0
1000
0
-1000
Observation number
Residual
I chart of residuals
X =-1.6E-04
UCL=815.9
LCL=-815.9
25,000
20,000
15,000
10,000
500
0
-500
Fit
Residual
Residuals vs. Fits
Residual model diagnostics
FIGURE 14.33
Residuals versus fit.
728
CHAPTER 14 Some Issues in Statistical Applications: An Overview

EXERCISES 14.7
14.7.1 Table 14.7.1 gives revenue (in thousands) for public elementary and
secondary schools, by state, for 1997-1998 and corresponding pupils per
teacher for that state for 20 randomly selected states (source: The World
Almanac and Book of Facts 2000).
(a) Obtain a dotplot and describe the pupils per teacher data.
(b) Identify any outliers and test for normality with and without outliers for
the pupils per teacher data. If the data are not normal, does any simple
transformation make the data normal?
(c) Obtain a 95% confidence interval for pupils per teacher.
(d) Obtain a scatterplot between total revenue and pupils per teacher.
(e) Fit a regression line between total revenue and pupils per teacher.
14.7.2. Table 14.7.2 gives the dealer cost and sticker price for luxury cars and sports
utility vehicles with popular options (source: Money Magazine, March 2001).
(a) Obtain a dotplot and describe the sticker price data.
(b) Identify any outliers and test for normality with and without outliers for
sticker price data. If the data are not normal, does any simple
transformation make the data normal?
(c) Obtain a 95% confidence interval for sticker price.
(d) Do parametric or nonparametric methods seem more appropriate for
the data?
(e) Obtain a scatterplot between dealer cost and sticker price.
(f) Fit a least-squares regression line and run a residual model diagnostics
using Minitab.
Table 14.7.1 School Revenue and Number of Pupils per Teacher
State
Total Revenue
Pupils Per Teacher
Arizona
4,388,915
19.8
Connecticut
5,112,950
14.2
Alabama
4,030,356
16.3
Indiana
7,006,752
17.2
Kansas
3,090,829
14.9
Oregon
3,119,028
20.1
Nebraska
1,688,662
14.5
New York
27,690,556
15.0
Virginia
6,661,612
14.7
Washington
6,722,916
20.2
Illinois
13,649,628
16.8
North Carolina
7,127,549
15.9
Georgia
8,579,628
16.2
Nevada
1,754,717
18.5
Ohio
12,694,407
16.7
New Hampshire
1,365,391
15.6
729
14.7 Tying it All Together

14.7.3. For the college tuition data of Exercise 14.5.5, fit a least-squares regression
line and run a residual model diagnostics using Minitab
14.7.4. The following data give the area (in square feet) and the sale prices
(approximated to the nearest $1000) of homes that were sold in a particular
city in a six-week period of 2003.
Area:
1123
1028
1490
2172
2300
1992
3200
3063
3720
7228
720
943
904
912
1031
1152
1482
1426
1491
1184
1650
1392
1755
2062
2495
3253
5152
1270
1723
1161
1220
837
1446
2442
2300
2518
Price:
75
75
102
149
152
154
327
425
625
775
775
57
66
68
75
86
90
93
95
95
104
105
135
159
169
253
725
67
67
110
65
74
95
156
183
207
Table 14.7.2 Dealer Cost and Sticker Price for Luxury and Sport Cars
Model
Dealer Cost (in
dollars)
Sticker Price (in
dollars)
Acura TL 3.2
26,218
29,030
Audi A6 4.2
45,385
50,754
BMW 525i
33,800
37,245
Cadillac DeVille DHS4
43,825
47,603
Infiniti I30 Touring
28,604
32,065
Jaguar XJ8
52,535
58,171
Lexus GS430
41,881
48,581
Mercedes-Benz C320
35,067
36,950
SAAB 9-3 Viggen
35,270
38,690
Volvo S80T-6
39,315
41,768
BMW X5 4.4i
45,994
50,774
Chevrolet Blazer LT
26,958
29,725
Dodge Durango
26,845
29,370
GMC Jimmy SLE
26,637
29,370
Honda CR-V LX
17,578
19,190
Isuzu Trooper LS
27,901
31,285
Jeep Cherokee SE
21,392
23,130
Lexus LX470
54,785
63,474
Mercedes-Benz ML430
42,243
45,337
Nissan Pathfinder SE
27,203
29,869
Pontiac Aztek GT
22,912
24,995
Subaru Forester S
21,990
24,190
Suzuki Vitara JS
16,063
17,079
Toyota RAV4
18,786
20,630
730
CHAPTER 14 Some Issues in Statistical Applications: An Overview

(a) Obtain a dotplot and describe the home price data.
(b) Identify any outliers and test for normality with and without outliers for
home price data. If the data are not normal, does any simple
transformation make the data normal?
(c) Obtain a 95% confidence interval for home price.
(d) Do parametric or nonparametric methods seem more appropriate for
the data?
(e) Obtain a scatterplot between the square-foot area of a home and
its price.
(f) Fit a least-squares regression line and run a residual model diagnostics
using Minitab.
14.8 CONCLUSION
We have briefly discussed some of the problems that arise in applied data analysis.
However, this discussion is not exhaustive. There are various other special problems
that can arise in applied data analysis. For example, if one or both of the sample sizes
are small, it may be hard to detect violations of some of the assumptions. For small
samples, violation of assumptions such as inequalities of variances is hard to dis-
cover. Also, for small sample sizes, possible outliers whose detection may be in
doubt may have undue influence on the inferences. It is better to avoid such problems
in the design stage of an experiment, when suitable sample sizes can be determined
before we start collecting data.
Differences in distributional shapes can influence the testing procedures of two or
more samples. In those cases, utilizing a transformation may settle that problem and
may also promote normality as well as correct the problem of inequality of variances.
There are also many issues related to simulation that are discussed in Chapter 13 in
the utilization of empirical methods—for instance, in the application of Markov
chain Monte Carlo (MCMC) methods, the issues of burn-in, choice of the correct
proposal function, and convergence. These are beyond the scope of this book.
Combining the issues discussed in this chapter with the rest of the material of this
textbook should give the student a good footing in the theory of statistics as well as
the ability to deal with many real-world problems.
731
14.8 Conclusion

APPENDIX
Set Theory
A
In this appendix, we present some of the basic ideas and concepts of set theory that
are essential for a modern introduction to probability and statistics. The origin of set
theory is credited to Georg Cantor, when he proved the uncountability of the real line
in 1873. A set is defined as a collection of well-defined distinct objects. These objects
of a set are called elements or members. The elements of a set can be anything: the
alphabet, numbers, people, other sets, and so forth. Sets are conventionally denoted
with capital letters, A, B, C, and so on. A universal set, denoted by S, is the collection
of all possible elements under consideration. If a is an element of a set A, we write
a2A. If a is not an element of A, we write a62A.
A set is described either by listing its elements or by stating the properties that
characterize the elements of the set. For example, to specify the set A of all positive
integers less than 12, we may write
A ¼
1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11
f
g
all positive integres less than 12
f
g
x : x < 12, x a positive integer
f
g
8
<
:
:
Sets are classified as finite or infinite. A set is finite if it contains exactly n objects,
where n is a nonnegative integer. A set is infinite if it is not finite. For example, if A is
a set containing all positive integers less than or equal to 50, then A is a finite set. If B
is a set containing all the positive integers, it is an infinite set.
Describing a set by stating its properties is the practical way to represent a set with
a large or infinite number of elements.
A set B is a subset of a set A if every element of B is also an element of A. We
denote this by writing BA, which is read “A contains B” or “B is contained in A.”
For example, if A is the set of real numbers and
B ¼ x : x  5, x a positive integer
f
g,
it is clear that B is a subset of A. Also, every subset is a subset of itself. Two sets A and
B are equal, A¼B, if and only if AB and BA. Thus, two sets A and B are said to
be equal if they have the same members. A set B is a proper subset of a set A if every
element of B is an element of A and A contains at least one element that is not an
element of B. We denote this relationship by BA. In the previous example, we have
BA. The set, which contains no elements, is called the empty set (or null set) and is
denoted by ∅. The null set ∅is a subset of every set.
A Venn diagram is used for visual representation of sets (Figure A.1). In the
Venn diagram, the universal set, S, is represented by a rectangle. The subsets are
represented by circles inside this rectangle.
733

A.1 SET OPERATIONS
Union, »: The union of two sets A and B (Figure A.2) is the set of all elements that
belong to A or B (or both; elements that belong to both sets are included only once)
and is denoted by A[B,
A[B ¼ x : x 2 A orx 2 B
f
g:
Intersection, «: The intersection of two sets A and B (Figure A.3) is the set of all
elements that belong to both A and B and is denoted by A\B ,
A\B ¼ x 2 S : x 2 A and x 2 B
f
g:
FIGURE A.1
A Venn diagram.
FIGURE A.2
Union of two sets.
FIGURE A.3
Intersection of two sets.
734
APPENDIX A Set Theory

If A\B¼∅, then the sets A and B are said to be disjoint or mutually exclusive sets.
Complement: The complement of a set A (Figure A.4) is the set of all elements
that belong to S but not to A,
Ac ¼ x : x 2 S; x62A
f
g:
The difference of any two sets, A and B, denoted by A\B, is equal to A\Bc. Thus,
Ac¼S\A. It should be noted that(Ac)c¼A. The symmetric difference between any
two sets, A and B, denoted by ADB, is the set of elements in A or B, but not both,
that is, (A\B)[(B\A).
PROPERTIES OF SETS
If A, B, and C are the subsets of the universal set S, then they satisfy the following properties.
Commutative law
A[B ¼ B[A:
A\B ¼ B\A:
Associative law
A[ B[C
ð
Þ ¼ A[B
ð
Þ[C ¼ A[B[C:
A\ B\C
ð
Þ ¼ A\B
ð
Þ\C:
Distributive law
A[ B\C
ð
Þ ¼ A[B
ð
Þ\ A[C
ð
Þ,
A\ B[C
ð
Þ ¼ A\B
ð
Þ[ A\C
ð
Þ,
Idempotent law
A[A ¼ A, A\A ¼ A
Identity law
A[S ¼ S, A\S ¼ A;
A[∅¼ A, A\∅¼ ∅
Continued
S
Ac
FIGURE A.4
Complement of a set.
735
APPENDIX A Set Theory

Complement law
A[Ac ¼ S, A\Ac ¼ ∅
De Morgan’s laws
A[B
ð
Þc ¼ Ac \Bc
A\B
ð
Þc ¼ Ac [Bc
The two sets A and B are said to be in one-to-one correspondence (denoted by
1:1) if each element a2A is paired with one and only one element b2B in such a
manner that each element of B is paired with exactly one element of A. For example,
if A¼{a1,a2,a3,a4} and B¼{1,2,3,4}, then A and B in a 1:1 correspondence.
A set whose elements can be put into a one-to-one correspondence with the set of
all positive integers is referred to as being a countably infinite set. Also, a set is said
to be countable, denumerable, or enumerable if it is finite or countably infinite. The
product or Cartesian product of sets A and B is denoted by AB and consists of all
ordered pairs (a,b), where a2A and b2B, that is,
AB ¼
a, b
ð
Þ : a 2 A, b 2 B
f
g:
For example, if A¼{a1,a2,a3} and B¼{1,2}, then
AB ¼
a1, 1
ð
Þ, a1, 2
ð
Þ, a2, 1
ð
Þ, a2, 2
ð
Þ, a3, 1
ð
Þ, a3, 2
ð
Þ
f
g:
The notion of a Cartesian product can be extended to any finite number of sets; that
is, A1A2. . .An is the set of all ordered n-tuples, (a1,a2,. . .,an), where
a1 2 A1,a2 2 A2, ...,an 2 An:
736
APPENDIX A Set Theory

APPENDIX
Review of Markov Chains B
A stochastic or random process is defined as a family of random variables, {X(t)},
describing an empirical process, the development of which in time is governed by
probabilistic laws. The state space, S, of the stochastic process is the set of all pos-
sible values that the random variable X(t) can take. The parameter t is often inter-
preted as time and may be either discrete or continuous. When the set of possible
values of t forms a countable set, the process {X(t), t¼0,1,2,. . .}, is discrete. If t
forms an interval of real line, the process {X(t), t0} is said to be continuous.
In the discrete case, the state space can be finite or infinite.
Among many different discrete stochastic processes, we are interested in a spe-
cial class called Markov chains. The basic concepts of Markov chains were intro-
duced in 1907 by the Russian mathematician A.A. Markov.
Let i1,i2,. . . represent the states of the chain. The sequence of random variables
X1,X2,. . . is called a Markov chain if
P Xn ¼ ikn
ð
X1 ¼ ik1, ...,Xn1 ¼ ikn1
j
Þ ¼ P Xn ¼ ikn
ð
Xn ¼ ikn1
j
Þ
An intuitive interpretation is that a stochastic process {X(t)} has the Markov property
if the conditional probability of any future state, given the present and past states, is
independent of the past states and depends only on the present state. Thus, a Markov
chain can be used to model the position of an object in a discrete set of possible states
over time, in which the subsequent position is chosen at random from a distribution
that depends only on the current location of the chain and not on any previous loca-
tions of the chain.
The conditional probabilities that the chain moves to state j at time n, given that it
is in state i at time n1, are called transition probabilities and are denoted by pij,
pij ¼ P Xn ¼ j
ð
Xn1 ¼ i
j
Þ
with the subscript ij of p indicating the direction of transition i!j. Sometimes, pij
may also be represented by p(i,j), and if we need to represent the time points, then we
use the notation, pn1,n(i,j)¼P(Xn¼jjXn1¼i).
Two basic assumptions we make are that (i) pij0 for all i and j; the transition
probabilities are nonnegative. Also, (ii) for every i,
X
1
j¼1
pij ¼ 1
X
n
j¼1
pij ¼ 1 if thestate space isfinite
 
!
that is, the chain makes a transition to some state in the state space.
737

If the transition probabilities pij depend only on the states i and j and not on the
time n, then the conditional probabilities are called stationary. Markov chains with
stationary probabilities are called (time) homogeneous Markov chains. We shall
consider only homogeneous Markov chains.
The behavior of homogeneous Markov chains is described by the transition or
stochastic matrices of the processes where the transition probabilities are arranged
as elements of a matrix. The transition or stochastic matrix of a chain having tran-
sition probabilities i,j¼1,2,. . .,n is
P ¼
p11 ... p1n
...
..
.
...
pn1  pnn
0
B
@
1
C
A
In the infinite state space case, we represent the transition matrix in the following
manner:
p11  p1n 
..
.
..
.
..
.
..
.
pm1  pmn 
...
...
...
...
0
B
B
B
B
@
1
C
C
C
C
A
Each element of the matrix is nonnegative, and each row sums to 1. If we look at any
particular row, say the mth row, then we can see the probabilities of going from state
m to the various other states including the state m.
EXAMPLE B.1
Four quarterbacks are warming up by throwing a football to one another. Let 1, 2, 3, and 4 denote the
four quarterbacks. It has been observed that 1 is as likely to throw the ball to 2 as to 3 and 4. Player 2
never throws to 3 but splits his throws between 1 and 4. Quarterback 3 throws twice as many passes
to 1 as to 4 and never to 2, but 4 throws only to 1. This process forms a Markov chain because the
player who is about to throw the ball is not influenced by the player who had the ball before him. The
one-step transition matrix is
0
1
3
1
3
1
3
1
2 0 0
1
2
2
3 0 0
1
3
1 0 0 0
0
B
B
@
1
C
C
A
Following is a standard example of a chain with infinite state space.
EXAMPLE B.2
Consider a chain with state space S¼(0,1,2,3,. . .) and transition matrix
P ¼
r0 p0
0
0
0

q1 r1 p1
0
0

0
q2 r2 p2
0

0
0
q3 r3 p3 
...
...
...
...
...
...
0
B
B
B
B
B
@
1
C
C
C
C
C
A
738
APPENDIX B Review of Markov Chains

where pi,qi,ri0 for all i0, p0+r0¼1, and pi+qi+ri¼1 for all i1. Thus, for this Markov chain,
the transition probabilities are: p00¼r0, p01¼p0, and for i,j6¼0,
pij ¼
pi, j ¼ i + 1,
ri, j ¼ i,
qi, j ¼ i1,
0,
otherwise:
8
>
>
>
<
>
>
>
:
This chain is known as the random walk chain (with barrier at 0).
The following example gives a transition matrix for the random walk chain in a
special case. We can think of this as a chain resulting from tossing of a fair coin. If we
are not at state zero, then if heads comes up, we take a step to the right and if tails
comes up, we take a step to the left. If at state 0, we remain at zero for a tails outcome
and move a step to the right for heads.
EXAMPLE B.3
Consider a Markov chain with state space S¼(0,1,2,3,. . .) and the transition probabilities given by
p00 ¼ 1
2, pij ¼
1
2,
j ¼ i1
1
2,
j ¼ i + 1
0, otherwise
8
<
:
:
This results in the symmetric transition matrix with elements
A ¼
1
2
1
2 0 0 0 
1
2 0
1
2 0 0 
0
1
2 0
1
2 0 
0 0
1
2 0
1
2 


















2
66666666664
3
77777777775
:
The n-step transition probability, pij
(n), is defined as the probability that the chain is in state i
and will go to state j in n steps. If pij is the one-step transition probability, pij
(n) can be obtained
as follows. Let i be the state of the process at time, m, that is Xm¼i. Then, the n -step transition
probability is
p n + m
ð
Þ
ij
¼ P Xn + m ¼ j
ð
X0 ¼ i
j
Þ
¼
X
1
k¼0
P Xn + m ¼ j,Xn ¼ k
ð
X0 ¼ i
j
Þ
¼
X
1
k¼0
P Xn + m ¼ j
ð
Xn ¼ k,X0 ¼ i
j
ÞP Xn ¼ k
ð
X0 ¼ i
j
Þ
¼
X
1
k¼0
pm
kjpn
ik
:
This can be rewritten in the matrix notation as
P n + m
ð
Þ ¼ P m
ð ÞP n
ð Þ ¼ P n
ð ÞP m
ð Þ
This is known as the Chapman–Kolmogorov equation.
739
APPENDIX B Review of Markov Chains

The following example shows how to compute an n-step transition matrix.
EXAMPLE B.4
Consider the one-step transition matrix given in Example B.1,
0
1
3
1
3
1
3
1
2 0 0
1
2
2
3 0 0
1
3
1 0 0 0
0
B
B
B
@
1
C
C
C
A:
The two-step transition matrix, P2, is
P2 ¼ P:P ¼
0
1
3
1
3
1
3
1
2 0 0
1
2
2
3 0 0
1
3
1 0 0 0
0
B
B
B
B
@
1
C
C
C
C
A
0
1
3
1
3
1
3
1
2 0 0
1
2
2
3 0 0
1
3
1 0 0 0
0
B
B
B
@
1
C
C
C
A
¼
13
18 0 0
5
18
1
2
1
6
1
6
1
6
1
3
2
9
2
9
2
9
0
1
3
1
3
1
3
0
B
B
B
B
@
1
C
C
C
C
A
:
The three-step transition matrix, P3, is
P3 ¼ P2P ¼
5
18
13
54
13
54
13
54
13
36
1
6
1
6
11
36
13
27
1
9
1
9
8
27
13
18 0 0
5
18
0
B
B
B
@
1
C
C
C
A:
For instance, the third row of P3,
13
27
1
9
1
9
8
27
ð
Þ,
denotes that, after three throws, the ball is in the hands of players 1, 2, 3, and 4, with respective
probabilities 13/27, 1/9, 1/9, and 8/27.
A transition matrix, P, all entries of which are positive, is called a positive tran-
sition matrix. A state j of a Markov chain is accessible from a state i if pij
(n)>0 for
some n0. If state j is accessible from state i, and state i is accessible from state j, the
states are said to communicate. If all the states communicate, then the Markov chain
is called irreducible. A state i is periodic (of period d) if the only way to revisit it is
through steps of length k.d for some value of k and a fixed value of d>1. Thus, the
period, d, is the greatest common divisor of the number of steps n needed for the
chain, starting at state i, to revisit the state i:
d ¼ GCD n  1
f
pn
ii > 0


If a state is not periodic, then it is called aperiodic. A state i is recurrent if it will be
revisited by the chain with probability 1. That is,
P Xn ¼ i for infinitelymany n
ð
X0 ¼ i
j
Þ ¼ 1
740
APPENDIX B Review of Markov Chains

If a state is not recurrent, it is called transient. Recurrent, aperiodic states are called
ergodic. It is necessary to impose an extra condition for ergodicity, that the expected
recurrence time be finite. This is satisfied for recurrent states in a finite-state Markov
chain. A Markov chain is called ergodic if every state is ergodic. It is clear that a
finite state Markov chain with a positive transition matrix is ergodic.
The following result is of fundamental importance.
Theorem B.1 For an ergodic Markov chain, limn!1pij
(n)¼pj exists, and this limit
is independent of the initial state i. Let the vector p with elements (pj) be the limiting
or the stationary distribution of the chain. Then, this stationary probability vector is
the unique solution of the equation
p ¼ pP
and satisfies the normalization condition
X
j2S
pj ¼ 1
If, at any transition step n, the distribution of the chain is same as p obtained in
Theorem B.1, we say that the chain has reached the steady state. Thus, the vector
p would be the unique steady-state probability vector of the Markov chain.
Analogous to the law of large numbers for a sequence of independent random
variables, for Markov chains we can obtain the following so called ergodic theorem.
Theorem B.2 For any ergodic Markov chain {Xn} with stationary distribution p:
1
n
X
n
k¼1
f Xk
ð
Þ !
X
i2S
f ið Þpi ¼ Ef X
ð Þw:p:1:
The validity of the Markov chain Monte Carlo method lies in this ergodic theorem.
741
APPENDIX B Review of Markov Chains

APPENDIX
Common Probability
Distributions
C
In this appendix, we present some common probability distributions that are useful in
statistical methods that we have used in this book. There is a much greater variety of
distributions that are very important in particular area of applications. A good refer-
ence can be found at http://www.causascientia.org/math_stat/Dists/Compendium.
pdf. We give the density function, mean, variance, and moment-generating function
(mgf). For some distribution functions, if the mgf is complicated, we just leave it out
and refer the reader to one of the references in the book.
743

Name
Pfd
Mean
Variance
mgf
Bernoulli distribution
f x, p
ð
Þ ¼
p,
x ¼ 1
1p,
x ¼ 0
0,
otherwise
8
<
:
0<p<1
p
p(1p)
q+pet, q¼1p
Binomial
f x, n, p
ð
Þ ¼
n
x
 
pxqnx, x¼0,1,...,n
np
npq
(q+pet)n
Geometric
f(x,p)¼qx1p,x¼1,2,. . .0<p1.
1
p
q
p2
pet
1qet
Hyper-geometric
f x, N, m, n
ð
Þ ¼
m
x
  Nm
nx


N
n
 
,
N ¼ 0,1,2, ..., m ¼ 0,1, ...,N,
n ¼ 0,1, ...,N
nm
N
n m
N
ð Þ 1 m
N
ð
Þ Nn
ð
Þ
N1
Negative binomial
f x, r, p
ð
Þ ¼
x + r1
x


prqx
x ¼ 0,1,2, ...
rq
p
r q
p2
p
1qet

r
Poisson
f x, l
ð
Þ ¼ lxel
x!
,
x¼0,1,2,...
l
l
exp(l(et1))
Beta
f x, a, b
ð
Þ ¼
G a + b
ð
Þ
G a
ð ÞG b
ð Þ


xa1 1x
ð
Þb1,
0 < x < 1
a
a + b
ab
a + b
ð
Þ2 a + b + 1
ð
Þ
Chi-square
f x, n
ð
Þ ¼ 2n=2xv1ex2=2
G x=2
ð
Þ
,
x  0,n > 0
degrees of freedom
ð
Þ
ﬃﬃﬃ
2
p
G
n + 1
ð
Þ=2
ð
Þ
G n=2
ð
Þ
nm2
Exponential
f x, l
ð
Þ ¼
lelx,
x  0,
0,
otherwise,

l > 0
1
l
1
l2
1 t
l

1
Gamma
f x, a, b
ð
Þ ¼ xa1 baebx
G a
ð Þ ,
x > 0,a > 0,b > 0
a
b
a
b2
1 t
b

	a
,
t<b.
Laplace
f x, m, s
ð
Þ ¼ 1
2sexp  x m
j
j
s


,
1 < x,m
m
2s2
Normal
f x, m, s2


¼
1
s
ﬃﬃﬃﬃﬃﬃ
2p
p
exp
x m
ð
Þ2
2s2
 
!
,
1 < x, m < 1,s > 0
m
s2
exp mt + s2t2
2


Uniform
f x, a, b
ð
Þ ¼
1
ba,
a  x  b
a + b
2
ba
ð
Þ2
12
etb eta
t ba
ð
Þ
744
APPENDIX C Common Probability Distributions

APPENDIX
What is R?
D
R is a language and environment for statistical computing and graphics. It provides a
broad variety of statistical methods including basic statistical tests, regression
models, among many other classical and graphical methods/techniques. R can be
easily used to produce technical plots of quality along with mathematical symbols
and formulas. R is available as free software under the terms of Free Software Foun-
dation’s GNU general public license in source code form.
R is similar to S language and environment. The language S is considered the
choice in statistical methods and R provides an open source route to work with in
statistical methods, among others.
R is also designed like S around a true computer language with the flexibility in
that it allows users to add additional functions.
Finally, some users of R think it as a statistics system, others think of R as an
environment within which statistical methods and graphics are implemented.
745

APPENDIX
Probability Tables
E
Table E.1 Cumulative Binomial Probabilities, P(Xx)¼P
i¼0
x
p(i)
747

Table E.1 Cumulative Binomial Probabilities, P(Xx)¼P
i ¼ 0
xp(i)—cont’d
748
APPENDIX E Probability Tables

Table E.1 Cumulative Binomial Probabilities, P(Xx)¼P
i ¼ 0
xp(i)—cont’d
Table E.2 Standard Norms Table
749
APPENDIX E Probability Tables

Table E.2 Standard Norms Table—cont’d
Table E.3 t-Table
750
APPENDIX E Probability Tables

Table E.4 Chi-Square Probabilities
751
APPENDIX E Probability Tables

Table E.5 Percentage Point of F-Distributions
752
APPENDIX E Probability Tables

Table E.5 Percentage Point of F-Distributions—cont’d
753
APPENDIX E Probability Tables

Table E.5 Percentage Point of F-Distributions—cont’d
754
APPENDIX E Probability Tables

Table E.5 Percentage Point of F-Distributions—cont’d
755
APPENDIX E Probability Tables

Table E.5 Percentage Point of F-Distributions—cont’d
756
APPENDIX E Probability Tables

Table E.5 Percentage Point of F-Distributions—cont’d
757
APPENDIX E Probability Tables

Table E.5 Percentage Point of F-Distributions—cont’d
758
APPENDIX E Probability Tables

Table E.5 Percentage Point of F-Distributions—cont’d
759
APPENDIX E Probability Tables

Table E.5 Percentage Point of F-Distributions—cont’d
760
APPENDIX E Probability Tables

Table E.5 Percentage Point of F-Distributions—cont’d
761
APPENDIX E Probability Tables

Table E.6 Wilcoxon Signed Rank Test: P(W+ c)
762
APPENDIX E Probability Tables

Table E.6 Wilcoxon Signed Rank Test: P(W+ c)—cont’d
763
APPENDIX E Probability Tables

Table E.6 Wilcoxon Signed Rank Test: P(W+ c)—cont’d
764
APPENDIX E Probability Tables

Table E.6 Wilcoxon Signed Rank Test: P(W+ c)—cont’d
765
APPENDIX E Probability Tables

Table E.6 Wilcoxon Signed Rank Test: P(W+ c)—cont’d
766
APPENDIX E Probability Tables

Table E.7 Wilcoxon Rank Sum Test
767
APPENDIX E Probability Tables

Table E.7 Wilcoxon Rank Sum Test—cont’d
768
APPENDIX E Probability Tables

Table E.7 Wilcoxon Rank Sum Test—cont’d
769
APPENDIX E Probability Tables

Table E.7 Wilcoxon Rank Sum Test—cont’d
770
APPENDIX E Probability Tables

Table E.7 Wilcoxon Rank Sum Test—cont’d
771
APPENDIX E Probability Tables

Table E.7 Wilcoxon Rank Sum Test—cont’d
772
APPENDIX E Probability Tables

Table E.7 Wilcoxon Rank Sum Test—cont’d
773
APPENDIX E Probability Tables

Table E.7 Wilcoxon Rank Sum Test—cont’d
774
APPENDIX E Probability Tables

Table E.8 Friedman Test
775
APPENDIX E Probability Tables

Table E.8 Friedman Test—cont’d
776
APPENDIX E Probability Tables

Table E.8 Friedman Test—cont’d
777
APPENDIX E Probability Tables

Table E.8 Friedman Test—cont’d
778
APPENDIX E Probability Tables

Table E.8 Friedman Test—cont’d
779
APPENDIX E Probability Tables

Table E.8 Friedman Test—cont’d
780
APPENDIX E Probability Tables

Table E.9 Studentized Range q Table
781
APPENDIX E Probability Tables

Table E.9 Studentized Range q Table—cont’d
782
APPENDIX E Probability Tables

Table E.9 Studentized Range q Table—cont’d
783
APPENDIX E Probability Tables

Table E.10 Critical Values of the Kolmogorov-Smirnov One Sample Test
Statistics
784
APPENDIX E Probability Tables

References
Aggarwal, C.C., 2013. Outlier Analysis. Springer, New York.
Albert, J., 2009. Bayesian Computation with R. Springer, Dordrecht.
Atkinson, A.C., 1988. Recent developments in the methods of optimum and related experi-
mental design. Int. Stat. Rev. 56, 99–116.
Bain, Lee J., Engelhardt, Max, 2000. Introduction to Probability and Mathematical Statistics
(Duxbury Classic). Duxbury, Boston.
Balakrishnan, N., Nevzorov, V., 2003. A Primer on Statistical Distributions. Wiley, New
York.
Barker, T.B., 1986. Quality Engineering by Design: Taguchi’s Philosophy. Quality Progress,
New York, pp. 32–42.
Barnett, V., Lewis, T., 1995. Outliers in Statistical Data, third ed. Wiley, New York.
Berinstein, Paula, 1998. In: Bjorner, Susanne (Ed.), Finding Statistics Online. Information
Today, Independent Pub Group, Medford, NJ.
Berk, K.N., Carey, P., 2009. Data Analysis with Microsoft Excel. Brooks/Cole Cengage
Learning, London.
Box, G.E.P., Hunter, W.G., Hunter, J.S., 2005. Statistics for Experiments, second ed. Wiley,
New York.
Bratcher, T.L., Moran, M.A., Zimmer, W.J., 1970. Tables of sample sizes in the analysis of
variance. J. Qual. Technol. 2, 156–164.
Brereton, R.G., 1990. Chemometrics—Application of Mathematics and Statistics to Labora-
tory Systems. Ellis Horwood, Chichester, UK.
Bryne, D.M., Taguchi, S., 1986. The Taguchi Approach to Parameter Design. ASQC Quality
Congress Transactions, Anaheim, CA.
Bush, L.B., Unal, R., 3-6 February 1992. Preliminary structural design of a lunar transfer vehi-
cle aerobrake. Paper Presented at the AIAA 1992 Aerospace Design Conference, Irvine,
CA, AIAA-92-1108.
Chiang, C.L., 2003. Statistical Methods of Analysis. World Scientific, Singapore.
Chou, Ya-lun, 1989. Statistical Analysis for Business and Economics. Elsevier, New York.
Cobb, B.D., Clarkson, J.M., 1994. A simple procedure for optimizing the polymerase chain
reaction (PCR) using modified Taguchi methods. Nucleic Acids Res. 22 (18), 3801–3805.
Cochran, W.G., 1977. Sampling Techniques. Wiley, New York.
Cody, Ron, 2005. Applied Statistics & the SAS Programming Language, fifth ed. Prentice
Hall, Upper Saddle River, NJ.
Condra, Lloyd W., 2001. Reliability Improvement with Design of Experiments, second ed.
Marcel Dekker, New York.
Conover, W.J., 1998. Practical Nonparametric Statistics. Wiley, New York.
Converse, P., Traugott, M., 1986. Assessing the accuracy of polls and surveys. Science
234, 1094–1098.
Crossfield, R.T., Dale, B.G., 1991. Applying Taguchi methods to the design improvement pro-
cess of turbochargers. Qual. Eng. 3 (4), 501–516.
Dalgard, Peter, 2008. Introductory Statistics with R. Springer, New York.
Dale, E., Gerlach, D.H., Wilhite, A.L., 1979. Menstrual Dysfunction in Distance Runners.
Obstet. Gynecol. 54, 47–53.
Daniel, Wayne W., 1978. Applied Nonparametric Statistics. Houghton Mifflin, New York.
785

Davidson, Fred, 1996. Principles of Statistical Data Handling. Sage, Thousand Oaks, CA.
Davies, O., 1960. The Design and Analysis of Industrial Experiments. Oliver and Boyd,
London.
Davison, Anthony, Hinkley, David, 2003. Bootstrap Methods and Their Application. Cam-
bridge University Press, Cambridge, UK.
Dempster, A.P., Laird, N.M., Rubin, D.B., 1977. Maximum likelihood from incomplete data
via the EM algorithm. J. R. Stat. Soc. Ser. B, Methodol. 39, 1–38.
Dertouzos, M.L., Lester, R.S., Solow, R.M., 1989. Made in America: Regaining the Produc-
tive Edge. Harper Perennial, New York.
Edgington, Eugene S., 1987. Randomization Tests, second ed. Marcel Dekker, New York.
Efron, B., 1979. Bootstrap methods. Another look at jackknife. Ann. Stat. 7 (1), 1–26.
Efron, B., Tibshirani, R.J., 1993. An Introduction to the Bootstrap. Chapman & Hall, New
York.
Fisher, R.A., 1926. The arrangement of field experiments. J. Minist. Agric. G. B. 33, 503–513.
Fisher, R.A., 1971. The Design of Experiments, ninth ed. Hafner Press (Macmillan), New
York.
Fisher, R.A., Balmukand, B., 1928. The estimation of linkage from the offspring of selfed het-
erozygotes. J. Genet. 20, 70–92.
Fisher, R.A., Yates, Frank, 1963. Statistical Tables for Biological, Agricultural and Medical
Research, sixth ed. Hafner Press (Macmillan), New York.
Frees, Edward W., 1996. Data Analysis Using Regression Models: The Business Perspective.
Prentice Hall, Upper Saddle River, NJ.
Freund, John E., 1992. Mathematical Statistics, fifth ed. Prentice Hall, Upper Saddle River,
NJ.
Geman, S., Geman, D., 1984. Stochastic relaxation, Gibbs distributions and Bayesian resto-
ration of images. IEEE Trans. Pattern Anal. Mach. Intell. 6, 721–741.
George, Casella, George, Edward I., 1992. Explaining the Gibbs sampler. Am. Stat. 46 (3),
167–174.
Gilbert, R.O., 1987. Statistical Methods for Environmental Pollution Monitoring. Van
Nostrand Reinhold, New York.
Gilks, W.R., Richardson, S., Spiegelhalter, D.J. (Eds.), 1996. Markov Chain Monte Carlo in
Practice. Chapman & Hall, London.
Hald, A., 2003. A History of Probability and Statistics and Their Applications before 1750.
Wiley, New York.
Hand, D.J., Daly, F., Lunn, A.D., McConway, K.J., Ostrowski, E., 1993. A Handbook of Small
Data Sets. Chapman & Hall, London.
Harwell, M.R., 1988. Choosing between parametric and nonparametric tests. J. Couns. Dev.
67, 35–38.
Hastings, W.K., 1970. Monte Carlo sampling methods using Markov chains and their appli-
cations. Biometrika 57, 97–109.
Hegsted, D.M., Nicolosi, R.J., 1987. Individual variation in serum cholesterol levels. Proc.
Natl. Acad. Sci. USA 84, 6259–6261.
Hinkley, D.V., 1983. Jackknife methods. Encycl. Stat. Sci. 4, 280–287.
Hoenig, John M., Heisey, Dennis M., 2001. The abuse of power: the pervasive fallacy of power
calculations for data analysis. Am. Stat. 55, 19–24.
Hogg, R.V., Craig, A.T., 1978. Introduction to Mathematical Statistics, fourth ed. Macmillan,
New York.
786
References

Hogg, Robert V., Tanis, Elliot A., 1993. Probability and Statistical Inference. Macmillan, New
York.
Hollander, M., Wolfe, D.A., 1999. Nonparametric Statistical Methods. Wiley, New York.
Iglewicz, B., Hoaglin, D.C., 1993. How to Detect and Handle Outliers. American Society for
Quality Control, Milwaukee, WI.
Inman, Ronald L., 1994. A Data-Based Approach to Statistics. Duxbury, Pacific Grove, CA.
Jiju, A., 2014. Design of Experiments for Engineers and Scientists, second ed. Elsevier, London.
Jochen, F.M., et al., 2000. Pesticides in sediments from Queensland irrigation channels and
drains. Mar. Pollut. Bull. 41 (7–12), 294–301.
Johnson, Richard A., 2010. Miller and Freund’s Probability and Statistics for Engineers, eighth
ed. Prentice Hall, Upper Saddle River, NJ.
Kiefer, J., 1959. Optimal experimental designs (with discussions). J. R. Stat. Soc. Ser. B
21, 272–319.
Kuehl, Robert O., 2000. Design of Experiments: Statistical Principles of Research Design and
Analysis, second ed. Brooks/Cole, Pacific Grove, CA.
Larsen, R.J., Marx, M.L., 2011. An Introduction to Mathematical Statistics and Its Applica-
tions, fifth ed. Prentice Hall, Upper Saddle River, NJ.
Lee Jong-Suk, R., McNickle, Donald, Pawlikowski, Krzyszto, 1998. A survey of confidence
interval formulae for coverage analysis. Retrieved from: http://citeseerx.ist.psu.edu/
viewdoc/download?doi¼10.1.1.32.9485&rep¼rep1&type¼pdf.
Lehmann, E.L., 1975. Nonparametrics: Statistical Methods Based on Ranks. Holden-Day, San
Francisco.
Lenth, R.V., 2001. Some practical guidelines for effective sample size determination. Am.
Statistician 55, 187–193.
Levene, H., 1960. Robust tests for the equality of variances. In: Olkin, I. (Ed.), Contributions to
Probability and Statistics. Stanford University Press, Palo Alto, CA, pp. 278–292.
Liang, F., Liu, C., Carroll, R.J., 2010. Advanced Markov Chain Monte Carlo Methods. Wiley,
West Sussex.
Madansky, A., 2012. Prescriptions for Working Statisticians. Springer, New York.
Maghsoodloo, S., 1990. The exact relation of Taguchi’s signal-to-noise ratio to his quality loss
function. J. Qual. Technol. 22 (1), 57–67.
Manly, Brayan F.J., 1991. Randomization and Monte Carlo Methods in Biology. Chapman &
Hall, London.
McLachlan, G.J., Krishnan, T., 1997. The EM Algorithm and Extensions. Wiley, New York.
Mendenhall, W., Sincich, T., 1996. A Second Course in Statistics: Regression Analysis, fifth
ed. Prentice Hall, Upper Saddle River, NJ.
Mendenhall, W., Wackerly, D.D., Scheaffer, R.L., 2007. Mathematical Statistics with Appli-
cations, seventh ed. PWS-Kent, Boston.
Metropolis, N., Rosenbluth, A.W., Teller, A.H., Teller, E., 1953. Equations of state calcula-
tions by fast computing machines. J. Chem. Phys. 21, 1087–1092.
Miller, B., 2001. Beyond Statistics: A Practical Guide to Data Analysis. Allyn & Bacon, Boston.
Murphy, K., Myors, B., 1998. Statistical Power Analysis. L. Erlbaum Associates, London.
Nair, V.N., 1992. Taguchi’s parameter design: a panel discussion. Technometrics 34 (2),
127–161.
Nguyen, H.T., Rogers, G.S., 1989. Fundamentals of Mathematical Statistics. Springer, New
York.
Peace, G.S., 1993. Taguchi Methods: A Hands-On Approach. Addison-Wesley, Reading, MA.
787
References

Pearson, E.S., 1939. “Student” as statistician. Biometrika 30, 210–250.
Peterson, G.E., St. Clair, D.C., Aylward, S.R., Bond, W.E., 1995. Using Taguchi’s method of
experimental design to control errors in layered perceptrons. IEEE Trans. Neural Netw.
6 (4), 949–961.
Phadke, M.S., 1989. Quality Engineering Using Robust Design. Prentice Hall, Englewood
Cliffs, NJ.
Porter, T., 1986. The Rise of Statistical Thinking, 1820–1900. Princeton University Press,
Princeton, NJ.
Raudenbush, S.W., 1997. Statistical analysis and optimal design for cluster randomized trials.
Psychol. Methods 2 (2), 173–185.
Rinaman, William C., 1993. Foundations of Probability and Statistics. Saunders College Pub-
lishing, Fort Worth.
Robert, C.P., Casella, G., 2004. Monte Carlo Statistical Methods, second ed. Springer, New
York.
Robert, C.P., Casella, G., 2009. Introducing Monte Carlo Methods with R. Springer, New
York.
Robinson, G.K., 2000. Practical Strategies for Experimenting. Wiley, New York.
Ross, P.J., 1988. Taguchi Techniques for Quality Engineering: Loss Function, Orthogonal
Experiments, Parameter and Tolerance Design. McGraw-Hill, New York.
Rousseeuw, P.J., Leroy, A.M., 1987. Robust Regression and Outlier Detection. Wiley, New
York.
Roy, R.K., 1990. A Primer on the Taguchi Method. Van Nostrand Reinhold, New York.
Ryan, B., Joiner, B.L., 2013. Minitab Handbook. Thomson Brooks/Cole, Belmont.
Sahai, H., Ageel, M.I., 2000. The Analysis of Variance. Birkhauser, Boston.
Salsburg, David, 2001. The Lady Tasting Tea. W.H. Freeman, New York.
Savage, L.J., 1954. The Foundations of Statistics. Wiley, New York.
Shao, J., Tu, D., 1995. The Jackknife and Bootstrap. Springer, New York.
Shina, S.G., 1991. The successful use of the Taguchi method to increase manufacturing
process capability. Qual. Eng. 3 (3), 333–349.
Snedecore, George W., Cocharan, William G., 1980. Statistical Methods, seventh ed. Iowa
State University Press, Ames.
Sprent, Peter, 1998. Data Driven Statistical Methods. Chapman & Hall, New York.
Sullivan, L.P., 1987. The Power of Taguchi Methods. Quality Progress 20 (6), 76–79.
Taguchi, G., 1986. Introduction to Quality Engineering: Designing Quality into Products and
Processes. Asian Productivity Organization. American Supplier Institute, Dearborn, MI.
Taguchi, G., 1987. System of Experimental Design: Engineering Methods to Optimize Quality
and Minimize Costs, vols. 1 and 2. UNIPUB/Kraus International, White Plains, NY.
Taguchi, G., 1993. Taguchi on Robust Technology Development: Bringing Quality Engineer-
ing Upstream. ASME, New York.
Taguchi, G., Clausing, D., 1990. Robust quality. Harv. Bus. Rev.
Taguchi, G., Konishi, S., 1987. Taguchi Methods, Orthogonal Arrays and Linear Graphs.
American Supplier Institute, Dearborn, MI.
Taguchi, G., Yokoyama, Y., 1994. Taguchi Methods: Design of Experiments. American Sup-
plier Institute, Dearborn, MI, in conjunction with the Japanese Standards Association,
Tokyo.
Tamhane, A.C., Dunlup, D.D., 2000. Statistics and Data Analysis from Elementary to Inter-
mediate. Prentice Hall, Upper Saddle River, NJ.
Tankard, J., 1984. The Statistical Pioneers. Schenkman Books, New York.
788
References

Tanur, J.M., Mosteller, F., Kruscall, W., Link, R., Pieters, R., Rising, G., 1972. Statistics:
A Guide to the Unknown. Holden-Day, New York.
Thompson, Steven K., 1992. Sampling. Wiley, New York.
Thompson, Steven K., Seber, George A.F., 1996. Adaptive Sampling. Wiley, New York.
Tsao, H., Wright, T., 1983. On the maximum ratio: a tool for assisting inaccuracy assessment.
Am. Stat. 37 (4), 339–342.
Walters, F.H., Parker Jr., L.R., Morgan, S.L., Deming, S.N., 1993. Sequential Simplex Opti-
mization: A Technique for Improving Quality & Productivity in Research, Development,
& Manufacturing. CRC Press, Boca Raton, FL.
Wei, G.C.G., Tanner, M.A., 1990. A Monte Carlo implementation of EM algorithm and the
poor man’s data augmentation algorithm. J. Am. Stat. Assoc. 85, 699–704.
Wooding, William M., 1994. Planning Pharmaceutical Clinical Trials: Basic Statistical Prin-
ciples. Wiley, New York.
Yang, M.C.K., Robinson, D.H., 1986. Understanding and Learning Statistics by Computers.
World Scientific, Singapore.
Zar, Jerrold H., 1996. Biostatistical Analysis, third ed. Prentice Hall, Upper Saddle River, NJ.
789
References

Index
Note: Page numbers followed by b indicate boxes, f indicate figures and t indicate tables.
A
Absolute error loss function, 559
a-trimmed mean, 305
Alternate hypothesis, 313
Analysis of variance (ANOVA)
angular transformation, 545
assumptions, 699
completely randomized design (see Completely
randomized design)
F-test, 546
linear models, 546–547
logarithmic transformation, 545
Minitab, 537–539
missing observations, 546
multiple comparisons, 528–535
multiple regressions, 444–445
one-way (see One-way ANOVA)
R code, 536–537
regression, 430–432
SAS, 540–544
simple regression, 430–432
SPSS, 540
square root transformation, 545
treatments, 498–505
two-way (see Two-way ANOVA)
Anderson–Darling test, 387–388
Angular transformation, ANOVA, 545
ANOVA. See Analysis of variance (ANOVA)
Area sampling, 11
Asymptotic properties, 303–304
Asymptotic relative efficiency, 304
Average deviation, 30
Averaged squared errors (ASES), 305–306
Average weight loss estimation, 262
Axiomatic definition of probability, 57b
B
Bar graph
definition, 13
Pareto chart, 14–17, 15f
Bayes, Thomas, 549
Bayes decision, 579
Bayesian confidence interval. See Credible intervals
Bayesian decision theory
decision-making process, 576–577
statistical theory, 577
Bayesian hypothesis testing
Jeffreys’ hypothesis testing criterion, 574
null hypothesis, 573
posterior odds ratio, 574
posterior probability, 573
prior odds ratio, 574
Bayesian inference, 550
Bayesian point estimation
Bayes’ rule, 552
criteria for finding Bayesian estimate, 558–566
likelihood function, 552–553
marginal distribution, 553
population proportion, 554–555
posterior distribution, 552
probability distribution, 552–553
Bayes’ rule, 75b
Bell-shaped curve, 32
Bernoulli population, 246
Bernoulli random variable, 223, 250
probability function of, 113
Best linear unbiased estimator (BLUE), 421–422
Beta-binomial distribution, 557
Beta PDF, 399, 401f
Binomial distribution, normal approximation,
206–208
correction for continuity, 207b
Binomial experiment, 113
Binomial formula, 224
Binomial probability distribution, 113–117
Poisson approximation, 119b
Binomial random variables, 246
expected value of, 128
mean of, 116b
moment-generating function of, 116b
variance of, 116b
Binomial theorem, 114
Birthday problem, 108–109
Bivariate data, 715–718
Bivariate probability distributions, 139–140
Blinding, 465
Blocking, 466
Bootstrap methods, 645–651
R code, 679
SAS, 686
Box plot, 33–35, 33b
outliers, 695–697
791

C
Cauchy distribution, 244
Central limit theorem (CLT), 164b, 262, 269–270,
271–272
Chapman–Kolmogorov equation, 739
Chebyshev’s theorem, 160b
Chi-square distribution, 186–191, 284–285
degrees of freedom, 186
density, 285–286, 285f
probabilities, 751t
probability density, 188f
random variable, 133, 133b
Chi-square tests
contingency table, 376–379
multinomial distribution, 374–376
Pearson’s, 381–384
Cluster sampling, 11
Coefficient of determination, 419, 457
Complement set, 735, 735f
Completely randomized design
ANOVA decomposition, 507–508, 507f
between-groups variability, 505
correction factor, 506
decomposition of SS, 507, 507f
null hypothesis, 505
one-way ANOVA, 508b, 509b, 512–517
population means, 505
p-value approach, 510–511
SSE, 506
unbiased estimator, 508–509
within-groups variability, 505
Composite hypothesis testing, 315
Computers and statistics, 39
Conditional probability
definition, 71
law of total probability, 74b
properties of, 71b
Conditional probability distributions, 145
Confidence intervals
computer examples, 300, 301
confidence coefficient, 262
degrees of freedom, 263
interval estimation, 261
normal population, 264
one sample, 269–284
pivotal quantity, 263
population variance, 284–289
probability density, pivot, 265, 265f
proportion, 272
sample mean, 262–263
sampling distributions, 267, 308
Tukey’s method, 530b
two population parameters, 289–297
upper and lower confidence limits, 261–262
Conjugate prior, 557
Contingency table, chi-square tests
definition, 376–377
independent factors, 377b
Minitab, 407
Continuity correction factor, 383–384
Control plot, Taguchi methods, 484, 484f
Correction factor, 506
Correlation analysis
Fisher z-transform, 438
independent variables, 436–437
maximum likelihood estimator, 437
simple linear regression model, 436–437
Correlation coefficient, 147–149, 436–437
Counting random variable, 117
Covariance, 147–149
Credible intervals
conditional distribution, 569
definition, 568
posterior distribution, 569, 570f
Cross-sectional data, 6
Cumulative binomial probabilities, 747t
Cumulative distribution function (cdf), 84, 85
Cumulative probability distribution, 236
D
Data
bivariate, 715–718
collection, 3, 3b
cross-sectional, 6
graphical representation, 13–26 (see also
Graphical representation)
nominal, 6–7
numerical description, 26–38
ordinal, 6–7
quantitative, 5
time series, 6
transformation, 703–705
types of, 5–7
de Moivre, Abraham, 177
Degrees of freedom, 186, 284–285
Descriptive point estimates, 299
Descriptive statistics, 5
Design of experiments (DOE)
basic terminology, 461–465
factorial design, 477–481
Minitab, 491
optimal design, 481–483
R code, 489–491
792
Index

replication, randomization, and blocking,
466–468
sample size and power, 493
SAS, 491–493
specific designs, 469–476
Taguchi methods, 484
temperature effect, 493–494
Digamma function, 235–236
Discrete distribution, 256
Discrete random variable, 117
Discrete uniform distribution, 376
Distribution-free tests, 695
income distribution of families, 590–591, 591f
nonparametric confidence interval, 592–597
nonparametric hypothesis tests
(see Nonparametric hypothesis tests)
outliers, 695
parametric tests, 590
projects for, 635–637
Dobson units, 277–278
DOE. See Design of experiments (DOE)
Dotplot, 689–690, 690f, 700f
Double-blind treatment method, 465
E
Elementary statistics. See also Statistics
course, 270
Empirical distribution function, 307
Empty set (null set), 733
Equality of variances, 706–709
testing, 352b
Ergodic theorem, 741
Error probability distribution, 238–239
Error variance estimation, 422
Estimation theory, 220
Expectation maximization (EM) algorithm,
651–662
R code, 682
Experimental error, 465
Exponential family of probability distributions,
256–257
Exponential power, 238–239
Exponential probability distribution, 132
F
Factorial design
fractional, 480
full, 480
one-factor-at-a-time design, 478–480
Factorization criteria, joint sufficiency, 254b
F-distribution, 195–197
Finite population correction factor, 181
Finite set, 733
Finite variance, 259
Fisher, Ronald Aylmer, 1
Fisher z-transform, 438
Fractional factorial design, 480
Friedman test, 775t
Minitab, 630–632
R code, 630
treatment effects, 621
Full conditionals, 674
Full factorial design, 480
G
Galton, Francis, 409
Gamma probability distribution, 129–134, 224, 233
Gauss, Carl Friedrich, 111
Gaussian distribution, 122
Gaussian probability distribution, 381, 388
Gauss–Markov theorem, 422b
Geometric distribution, 229, 253
Gibbs algorithm (Gibbs sampler), 673–676
Goodness-of-fit tests
Anderson–Darling test, 387–388
brain cancer, 397–398
chi-square tests, 372–380, 381–384
contingency table, 376–379
global warming, 392–393
hurricane Katrina, 393–396
Kolmogorov–Smirnov test, 384–387
Minitab, 407
multinomial distribution, 372–373, 374–376
national unemployment, 396–397
P-P plots, 389–391
prostate cancer, 401–402
Q-Q plots, 389–391
rainfall, 399–400
R-commands, 406–407
Shapiro–Wilk test, 388–389
Graphical representation
bar, 13
dotplot, 689–690, 690f, 700f
frequency table, 17–18
grouped data, 17–18
histogram, 18
pie chart, 15, 16f
quantile-quantile (QQ) plot, 691–693
relative frequency, 17
scatter plot, 690, 690f, 691
side-by-side box plots, 690
stem-and-leaf plot, 16, 17
Greco–Latin square, 475
Grouped data, numerical measures, 30–32
793
Index

H
Hardy–Weinberg law, 109, 114–115
Highest posterior density (HPD) interval, 571–572
Histogram, 385, 385f, 700f
of data, 704f
definition, 18
guideline, 18b
Homoscedasticity, 447
Hypothesis testing
composite, 315
level of significance, 315
likelihood ratio tests, 328–333, 330b
method for, 314b
Neyman–Pearson Lemma, 323–328
p-value, 333–336
sample size, 320–321
simple, 315
single parameter, 333–344
two samples, 345–359
type I error, 315
type II error, 315
I
Independent variables, 462
Inferential statistics, 5
Infinite set, 733
Informative priors, 555–556
Interquartile range (IQR), 27–28
Invariance property, 242
J
Jackknife method, 640–645
R code, 680
SAS, 686
Jeffreys’ hypothesis testing criterion, 574
Joint density function, 251, 256
Joint probability distributions, 139–152
bivariate distributions, 139–140
conditional expectation, 145
covariance and correlation, 147–149
marginal pmf, 141
Joint probability mass function, 250
K
Kolmogorov, Andrei Nikolaevich, 53
Kolmogorov–Smirnov test, one sample test
statistics, 783t
Kronecker Delta function, 559
Kruskal–Wallis test
asymptotic distribution, 620
chi-square distribution, 619–620
description, 618
Minitab, 631
R code, 629
SAS, 634
SPSS, 633
L
Large sample approximations, 205–210
binomial distribution, normal approximation,
206–208
Large sample confidence intervals
difference of two means, 290b
Minitab, 302
p1-p2, 269b
Latin square design
definition, 472
Greco–Latin square, 475
R code, 490
Least-squares equations, 414–416
Least-squares estimators
definition, 414
Gauss–Markov theorem, 447
inferences, 425–433
properties of, 420–422
Least-squares line, 414
Least-squares, method of, 413–414
Least-squares regression line, 412, 412f
Least-squares regression model, 447–448
Level of significance, hypothesis testing, 315
Likelihood ratio tests (LRT), 328–333, 330b
Limit theorems, 159–168
central limit theorem (CLT), 164b
Chebyshev’s theorem, 160b
law of large numbers, 162b
Linear regression models
ANOVA, 546–547
coefficient of determination, 457
correlation analysis, 436–440
least-squares estimators, 425–433
matrix notation, 440–446
Minitab, 453
outliers and high leverage points, 457–458
particular value prediction, 433–436
regression diagnostics, 446–449
SAS, 454–456
scatterplots, 456
simple, 411–425
SPSS, 454
Logarithmic transformation, ANOVA, 545
Log-likelihood function, 229–231
Loss function, Taguchi methods, 485, 485f
Lower confidence limit, 261–262, 264,
266, 267
794
Index

M
Maclaurin’s expansion, with Poisson random
variable, 118
Marginal pmf/pdf, 141
Margin of error and sample size, 272–275
Markov chain Monte Carlo (MCMC) methods,
662–678
issues in, 676
Metropolis algorithm, 666–669
R code, 680
Markov chains, 737
aperiodic, 740–741
ergodic, 741
homogeneous, 738
irreducible, 740
periodic, 740
random walk chain, 739
steady state, 741
stochastic/random process, 737
transient, 741
transition probabilities, 737
transition/stochastic matrix, 738
Matrix notation
independent observations, 441
least-squares estimators, 442
linear equations, 441
multiple regression model, 441
Maximum likelihood equations (MLE), 231, 233
definition, 227
log-likelihood function, 229–231
optimization, 234
parameter values, 227
probability distributions, 234–240
Mean
of binomial random variable, 116b
chi-square random variable, 133b
exponential random variable, 132b
gamma probability distribution, 130b
normal probability distribution, 123b
of Poisson random variable, 118b
uniform probability distribution, 121b
and variance estimation, 306
Mean square error (MSE), 248, 500
Mean square treatment (MST), 500
Median test
hypergeometric distribution, 610
hypothesis testing procedure, 610b
large sample, 611b
Minitab, 630–632
sample median, 610, 610t
Method of moments, 222–227
Metropolis algorithm
continuous case, 667b
discrete case, 666b
random-walk, 669
Metropolis-Hastings (M-H) algorithm,
670–672
continuous case, 671b
discrete case, 670b
Minimal sufficient statistics, 222
Minimum variance unbiased estimator (MVUE),
240
Minitab, 301–302
ANOVA, 537–539
design of experiments, 491
goodness-of-fit tests, 407
linear regression models, 453
nonparametric tests, 630–632
statistical estimation, 301–302
t-test, 363
Model
issues in, 712–719
for univariate data, 713–715
Moment-generating function (MGF)
of Bernoulli random variable, 113
of binomial random variable, 116b
chi-square random variable, 133b
exponential random variable, 132b
gamma probability distribution, 130b
moments and, 91–104
normal probability distribution, 123b
of Poisson random variable, 118b
properties of, 101b
uniform probability distribution, 121b
Multifactor experiments, 464
Multinomial distribution, 374–376
Multiple comparisons, ANOVA
studentized range distribution, 529–530
Tukey’s method, 530b
Multiple linear regression model
ANOVA table, 445, 445t
definition, 411–412
N
Negative binomial distribution, 242
Newton–Raphson iterative method, 306
Neyman, Jerzy, 311
Neyman–Fisher factorization theorem, 252b
Neyman–Pearson Lemma, 323–328
Nightingale, Florence, 687
Noise, 463
Nominal data, 6–7
Noninformative priors, 555–556
Nonparametric analysis vs. parametric, 719–721
795
Index

Nonparametric confidence interval
binomial distribution, 592
central limit theorem, 592
ordered sample, 593, 593f
population median, 593
Nonparametric hypothesis tests
for one sample, 597–609
for two samples, 609–618
Nonparametric tests. See Distribution-free tests
Normal distribution, 225
Normality, assumption, 699–700
Normal probability distribution, 122–129
Normal probability plots, 701, 701f, 702f, 704f,
705f, 723f, 724f
for ANOVA, 514f, 545
Nuisance variables, 462
Null hypothesis, 312–313
Numerical description, data
average deviation, 30
bell-shaped curve, 32
empirical rule, 32b
grouped data, numerical measures, 30–32
interquartile range, 27–28
lower quartile, 27
median, 27
sample mean (empirical mean), 26
sample standard deviation, 26
sample variance, 26
upper quartile, 27
Numerical unbiasedness and consistency, 305
O
Observables
for Bayesian decision theory, 576–583
definition, 579
predicting future, 587–588
Observational experiment, 463
One-factor-at-a-time design, 478–480
One-parameter Weibull distribution, 260
One sample confidence intervals
large sample, 269–271
proportion, 272
small sample, 275–278
One-tailed test, 313–314
One-way ANOVA, 465
assumption testing, 512–517
k32 populations, 508b, 509b
Minitab, 537–539
model for, 517
R code, 513
SAS, 540
SPSS, 540
Optimal design
choice of optimal sample size, 482–483
sequential design, 481–482
simultaneous experiment design, 481–482
Optimization, 234
Order statistics, 200–205
Ordinal data, 6–7
Orthogonal Latin squares, 475
Outliers
box plot, 695–697
distribution-free test, 695
and high leverage points, 457–458
modified z-score, 695
value, 694
z-score, 695
P
Paired comparison tests, 606–607
Parametric analysis
brain cancer, 397–398
global warming, 392–393
hurricane Katrina, 393–396
national unemployment, 396–397
vs. nonparametric analysis, 719–721
prostate cancer, 401–402
rainfall, 399–400
Pareto chart, 14–17
Pareto distribution, 244
Pearson, Karl, 371
Pearson’s chi-square tests
cumulative probability distribution, 381
Gaussian probability distribution, 381
Percentage point of F-distributions, 752t
Pie chart, 15, 16f
Pivotal method. see Confidence intervals
Placebo, 465
Point estimators
method of maximum likelihood, 227–234
method of moments, 222–227
sufficiency, 250–258
unbiased estimators, 245–249
Poisson, Sime´on-Denis, 117
Poisson distribution, 226, 230, 260
Poisson probability distribution, 117–119
discrete random variable and, 117
Poisson random variables, 227
definition of, 117
mean, 118b
moment-generating function of, 118b
variance, 118b
Pooled sample variance, 291
Pooled t-test, 347
796
Index

Population
defined, 4
standard deviation, 274–275
Population parameters, confidence interval
large sample, 290b
small sample, 290b
Population variance, confidence interval
chi-square density, 285–286, 285f
chi-square distribution, 284–285
Posterior distribution
Bayesian point estimation, 552–568
definition, 552
Posterior mean, 559
Posterior odds ratio, 574
Power transformation, 715
P-P plots, 389–391
Prediction interval, normal population, 309–310
Prior odds ratio, 574
Probabilistic model. See Statistics
Probability density, 241, 265, 265f
Probability distribution, 381–392
common, 743
Probability distribution function (PDF), 112–139,
220, 221–222, 234–240
references for, 112
Probability function (pf), Bernoulli random
variable, 113
Probability mass function, 221–222, 230
Probability tables
chi-square probabilities, 751t
cumulative binomial probabilities, 747t
Friedman test, 775t
Kolmogorov–Smirnov one sample test statistics,
783t
percentage point of F-distributions, 752t
standard norms table, 749t
studentized range q table, 781t
t-table, 750t
Wilcoxon rank sum test, 767t
Wilcoxon signed rank test, 762t
Probability theory
axiomatic definition, 57b
concept of, 54
counting techniques and calculation of, 63–70
experiment, defined, 55
impossible event, 55b
mutually exclusive/ disjoint, 55
origin of, 54
probability, defined, 55, 57
probability space, 55b
properties of, 58b
sample point, 55b
sample space, 55b
special distribution functions, 112–139
trial, 55
p-value
approach, 510–511
hypothesis testing, 333–336
Q
Quadratic loss function, 486, 486f, 559
Quantile-quantile (QQ) plot, 389–391, 691–693
Quantitative data, 5
R
Random assignment procedure, 468
Randomization, 467
Randomized complete block design.
See also Two-way ANOVA
definition, 469
R code, 490
replications, 470b
SAS, 491
Randomness test
asymptotic normal distribution, 635
Minitab, 637
nonparametric procedure, 635
Random variables
Bernoulli (see Bernoulli random variable)
binomial (see Binomial random variables)
counting, 117
and probability distributions, 82–91
Random variables functions, 152–159
distribution functions method, 152–153
functions of, 155
pdf, 154
probability integral transformation,
154–155
transformation method, 155–158
Random-walk metropolis, 669
Rao, C.R., 219
Rao–Blackwell Theorem, 258b
Rayleigh distribution, 239, 239f, 261
R code
Bayesian estimation inference, 584–587
design of experiments, 489–491
goodness-of-fit tests, 289–297
linear regression models, 450–453
nonparametric tests, 628–630
one-way ANOVA, 513
two-way ANOVA, 536
Regression diagnostics, 446–449
Rejection region (Critical region), 313
Relative frequency, 17, 18
797
Index

Replication
definition, 466
procedure for randomized complete block design,
470b
Response variable, 461–462
R language, 745
Robust estimation, 304–305
S
Sampling
area, 11
biased, 8
chi-square distribution, 186–191
cluster, 11
defined, 4
distribution, 179
errors in, 12
F-distribution, 195–197
finite population correction factor, 181
large sample approximations, 205–210
Minitab, 213–214
multiphase, 11
order statistics, 200–205
population distribution, 184–200
R code, 211–213
representative, 8
sample, defined, 178
SAS, 214–215
simple random, 8
size, 12–13
SPSS, 214
standard error, 180
stratified, 9, 10, 10b
student t-distribution, 191–195
systematic, 9
SAS, 303
ANOVA, 540–544
design of experiments, 491–493
linear regression models, 454–456
Nonparametric tests, 633–634
statistical estimation, 303
Scatter diagram, 287, 411, 411f
Scatter plot, 412, 412f, 456, 690, 690f, 691
Set theory
complement, 735, 735f
countably infinite, 736
difference, 735–736
disjoint/mutually exclusive, 735
elements/members, 733
empty set (null set), 733
finite, 733
infinite, 733
intersection, 734, 734f
one-to-one correspondence, 736
properties, 735–736
set, defined, 733
subset, 733
symmetric difference, 735–736
union, 734, 734f
universal set, 733
Venn diagram, 733, 734f
Shapiro–Wilk test, 388–389
Shortest length confidence interval, 264
Side-by-side box plots, 690
one-way ANOVA, 512–516, 514f
Sign test
binomial distribution, 598
hypothesis testing procedure, 598b
large random sample, 600b
Minitab, 630
null hypothesis testing, 597
population distribution, 597–598
R code, 628
z-transform, 600
Simple hypothesis testing, 315
Simple linear regression models
definition, 412
derivation of b0 and b1, 414–418
error variance estimation, 422
least-squares estimators, 420–422
least-squares regression line, 412, 412f
method of least-squares, 413–414
quality of regression, 418–419
scatter diagram, 411, 411f
Simple random sampling
advantages, 9b
definition, 8
Simple regression line, 417, 417f
Single-factor experiments, 464
Skewness and Kurtosis, 96–101, 701
Small sample confidence intervals, 307
difference of two means, 291b
Minitab, 302
simulation of coverage, 307
Smith-Satterthwaite procedure, 348–350
SPSS
ANOVA, 540
linear regression models, 454
Nonparametric tests, 632–633
statistical estimation, 302–303
Squared error loss function, 559
Square root transformation, ANOVA, 545
Standard error, 180
Standard normal density, 257
Standard normal random variable, 122–123
Standard norms table, 749t
798
Index

Standard pivotal quantity, 263
Statistic(s)
concepts of, 4
descriptive, 5
inferential, 5
population, 4
sampling, 4
Statistical decision, 312
making, 577–578
Statistical hypotheses, 312
Stem-and-leaf plot, 16, 17
Sticker price, 726f, 727f, 728f
Stratified sample
definition, 9
selection procedure, 10b
uses, 11b
Studentized range distribution, 529–530
Studentized range q table, 781t
Student t-distribution, 191–195, 284–285
Subjective probability, 550–551
Subset, 733
proper subset, 733
Sufficient estimator
conditional probability, 252
definition, 250
density functions, 256–257
factorization criterion, 253, 254b
Neyman–Fisher factorization criteria, 252b
Rao–Blackwell theorem, 258b
Sum of squares of errors (SSE), 499, 506
Systematic sampling
definition, 9
selection procedure, 9b
T
Taguchi, Genichi, 459
Taguchi methods
control plot, 484, 484f
design parameters, 487
engineering designs, 484
goal post mentality, 485
loss function, 485, 485f
quadratic loss function, 486, 486f
quality control, 484
Test of independence, 709–710
Test statistics (TS), 313
Three-parameter gamma PDF, 234, 235,
396, 397f
Three-parameter Weibull PDF, 398, 398f
Time series data, 6
Time to failure and/or time between failure (TBF),
721–731
Transformation(s)
for ANOVA, 545
power, 715
Transition probabilities, 737
function, 665
n-step, 739
positive transition matrix, 740
Treatment variables, 462
Truncated exponential distribution, 243
t-table, 750t
t-test
assumptions, 699
Minitab, 363
one-sample, 360, 362
paired samples, 363
pooled, 347
SAS, 366–368
SPSS, 365–366
two sample, 362
Tukey, John W., 495
Tukey–Kramer method, 533
Tukey’s method
calculations of, 530, 530t
confidence intervals, 529
Minitab, 539
R code, 537
SAS, 541
SPSS, 540
Two random samples, hypothesis testing, 345–359
dependent samples, 353–356
independent samples, 345–353
Two-way ANOVA, 465
computational procedure for, 524b
Minitab, 538
nonrandom effect, 521
null hypothesis, 523
R code, 536
step-by-step computational procedure, 523–525
sums of squares, 522
two-way classification, 521–522, 522t
unbiased estimator, 523
Type I error, hypothesis testing, 315
Type II error, hypothesis testing, 315
U
Ulam, Stanislaw, 639
Unbiased estimators
definition, 245
mean square error, 248
sample mean, 246
variance, 246–247
Uniform maximum likelihood estimation, 299
Uniform probability distribution, 120–122, 231, 231f
799
Index

Univariate data, 713–715
Upper confidence limit, 261–262, 264, 266, 267
V
Variance
of binomial random variable, 116b
chi-square random variable, 133b
exponential random variable, 132b
gamma probability distribution, 130b
normal probability distribution, 123b
of Poisson random variable, 118b
uniform probability distribution, 121b
Venn diagram, 733, 734f
W
Wald–Wolfowitz test. See Randomness test
Weibull cumulative probability distribution, 395,
395f, 401–402, 402f
Weibull PDF, 234, 236, 238, 238f, 394, 394f,
401–402, 402f
Wilcoxon rank sum test, 767t
hypothesis testing procedure, 613b
large sample, 615b
R code, 629
SAS, 634
SPSS, 632
Wilcoxon signed rank test, 762t
hypothesis testing procedure, 601b
large samples, 604b
Minitab, 630–632
R code, 628
Wilcoxon tests vs. normal approximation, 635
Wolfowitz, Jacob, 589
Z
z-score test, 695
Z-transform, 270
800
Index


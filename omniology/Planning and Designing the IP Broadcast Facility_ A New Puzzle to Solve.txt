
 Planning and Designing the IP 
Broadcast Facility 
The transition to computer-based technologies and ﬁ le-based workﬂ ows is one 
of the most signiﬁ cant changes the broadcast and production industry has seen. 
Media is produced for multiple delivery platforms: Over-the-Air, Over-the-Top, 
large screen displays, cable, satellite, web, digital signage, tablets, and smartphones. 
These changes impact all aspects of creation, production, media management, 
technical operations, business processes, and distribution to end users. Of all the 
books and papers discussing storage mapping, packet transport, and compression 
algorithms, none puts all the pieces together and explains where these ﬁ t into the 
whole environment. Planning and Designing the IP Broadcast Facility is the ﬁ rst 
to provide a comprehensive understanding of the technology architecture, physi-
cal facility changes, and—most importantly—the new media management work-
ﬂ ows and business processes to support the entire lifecycle of the IP broadcast 
facility from an engineering and workﬂ ow perspective.
Key features:
 This beginning-to-end perspective gives you the necessary knowledge to make 
the decisions to implement a cost-effective ﬁ le-based production and distri-
bution system. 
 The cohesive, big-picture viewpoint helps you identify the differences in a 
tape-based facility, then how to overcome the unique challenges of upgrading 
your plant. 
 Case studies throughout the book serve as recommendations and examples of 
use, helping you weigh the pros and cons of various approaches.
Gary Olson is an advisor specializing in the transition of traditional media 
workﬂ ows and business processes. As a designer, he has provided his knowledge 
to organizations implementing IP and ﬁ le-based technology. His focus is the 
adaptation of organizational structure, stafﬁ ng models, and workﬂ ows to imple-
ment digital media technology. Gary is a recognized industry leader with practical 
experience in the analysis, selection, and uses of technology and as an innovator in 
media technologies and broadcast design. He designed the ﬁ rst commercial televi-
sion networks for countries in Central Europe, the Caribbean, and South America. 
Gary holds a US patent in streaming media automation and distribution.

This page intentionally left blank

 Planning and Designing the IP 
Broadcast Facility 
 A New Puzzle to Solve 
 GARY OLSON 

 First published 2015 
 by Focal Press 
 70 Blanchard Road, Suite 402, Burlington, MA 01803 
 and by Focal Press 
 2 Park Square, Milton Park, Abingdon, Oxon OX14 4RN 
 Focal Press is an imprint of the Taylor & Francis Group, 
an informa business 
 © 2015 Gary Olson 
 The right of Gary Olson to be identiﬁ ed as the author of this work 
has been asserted by him in accordance with sections 77 and 78 of 
the Copyright, Designs and Patents Act 1988. 
 All rights reserved. No part of this book may be reprinted or 
reproduced or utilized in any form or by any electronic, mechanical, 
or other means, now known or hereafter invented, including 
photocopying and recording, or in any information storage or 
retrieval system, without permission in writing from the publishers. 
 Notices 
 Knowledge and best practice in this ﬁ eld are constantly changing. 
As new research and experience broaden our understanding, 
changes in research methods, professional practices, or medical 
treatment may become necessary. 
 Practitioners and researchers must always rely on their own 
experience and knowledge in evaluating and using any 
information, methods, compounds, or experiments described 
herein. In using such information or methods they should be 
mindful of their own safety and the safety of others, including 
 parties for whom they have a professional responsibility. 
 Product or corporate names may be trademarks or registered 
trademarks, and are used only for identiﬁ cation and explanation 
without intent to infringe. 
 Library of Congress Cataloging-in-Publication Data 
 Olson, Gary (Broadcast engineer) 
 Planning and designing the IP broadcast facility : a new puzzle 
to solve / Gary Olson. 
   pages cm 
 1. Webcasting. 2. Internet radio broadcasting. 3. Digital 
media. 4. Communication and technology. I. Title. 
  TK5105.887.O47 2014 
  006.7ʹ876—dc23 
  2014019007 
 ISBN: 978-1-138-79895-3 (hbk) 
 ISBN: 978-1-138-79896-0 (pbk) 
 ISBN: 978-1-315-75630-1 (ebk) 
Typeset in Minion
By Apex CoVantage, LLC

   This is dedicated to my wife Ellen who has given me unwavering love 
and support through all my changes and adventures. 
Dedication


 About the Author  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .x
 Acknowledgments  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi 
 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xii 
 CHAPTER 1 —Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 
 New Terminology  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
 New Vocabulary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
 Workﬂ ow, Processes, and Integration  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
 Business Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
 Technology Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
 Total Cost of Ownership  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
 Facility Infrastructure—Physical Plant . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
 Heat and Power Loads and HVAC Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
 Operating Costs—Space/Power/HVAC  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
 Changes in Workﬂ ow  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
 Beginning-to-End Planning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
 Media Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
 An Example of IP Architecture for Media Management  . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
 Engineering in an IP world  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
 Quality Assurance and Quality Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
 Contents 

viii
Contents
 CHAPTER 2—Ingest/Acquisition/Capture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 
 Acquire, Ingest, and Manage  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 
 Compression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 
 Quality Control . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 
 Manage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 
 Standards and Formats  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 
 Ingest, Acquisition, and Capture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 
 Ingest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 
 Case Studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 
 Case Study 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 
 Case Study 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 
 Studio . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 
 What is the “Studio in a Box”?  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 
 Automation and Metadata. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 
 Craft Production . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 
 Removable Media  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 
 Streaming Contribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 
 Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 
 CHAPTER 3—Workﬂ ow and Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 
 Workﬂ ows and Business Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 
 Integration between Broadcast and Enterprise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 
 Business Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 
 Business Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 
 Operations Workﬂ ows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 
 Automation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 
 Trafﬁ c Workﬂ ow . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 
 Roles and Responsibilities Redeﬁ ned . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 
 CHAPTER 4—Media Management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
 Metadata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 
 Metadata Standards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62 
 Storytelling and the Value of Metadata . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 
 Storage Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 
 Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 
 Case Study 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 
 Case Study 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 
 Retention, Preservation, and Migration  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76 
 Governance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77 
 Roles and Responsibilities  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 78 
 CHAPTER 5—Technology Infrastructure and Engineering . . . . . . . . . . . . . . . . . . . . . . . . 79
 Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 
 Changes in Engineering and Maintenance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83 

Contents
ix
 Planning for the Future . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 
 Total Cost of Ownership . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87 
 CHAPTER 6—Transmission and Delivery   . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
 Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100 
 Cloud Services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103 
 Cloud Services in Broadcast . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105 
 Case Study 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 
 Case Study 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 
 Cloud Decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107 
 Public vs. Private Cloud  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 
 Private Build vs. Outsource . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108 
 CHAPTER 7—Facility Planning and Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
 Operating Costs—Space/Power/HVAC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112 
 Master Control—Program Origination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115 
 Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 
 Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 
 CHAPTER 8—Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 
 Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133 
Deﬁ nitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
Glossary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135
Index  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151

 GARY OLSON  is a Technology Architect and Advisor for Digital Media Strategies, 
who has developed a wide range of innovative broadcast, digital media, content 
management, and information products and services. 
 Gary has designed broadcast centers for major US networks and for emerg-
ing countries introducing their ﬁ rst commercial television channels. His designs 
have always incorporated a “future proof” philosophy, using innovative technology 
solutions. 
 He is an evangelist for the transition to an IP in the broadcast and production 
industry, creating media management system designs. 
 Gary advises on the best use of technology in the migration and transition from 
traditional media technology to digital platforms for broadcast, broadband, wire-
less, portable multimedia, and On-Demand media. 
 His substantial experience in working with project sponsors and stakeholders 
helps them understand the changes associated with digital media platforms and 
ﬁ le-based workﬂ ows. 
 Gary has advised major US and international broadcasters, corporations, and 
institutions on new digital production and distribution. He is a recognized industry 
leader with practical experience in the analysis, selection, and uses of technology and 
as an innovator in media technologies and broadcast design. 
 About the 
Author 

 I need to thank Peter Cresse and Andrea Cummis for pushing me to write my ﬁ rst 
whitepaper that started this process. I hadn’t thought of myself as a writer. The idea 
for the book came from conversations and discussions with so many people in the 
industry about all the end-to-end solutions that didn’t make a complete system. 
Capturing that into the puzzle concept was in collaboration with my niece Jordan 
and turning the concept to illustrations was thanks to Paul and Judy. There are so 
many friends and colleagues I need to thank for giving me their knowledge and 
support on this adventure. I want to acknowledge TBC Consoles for allowing me 
to use their images to demonstrate some of the concepts. Other images are from my 
own projects. All of the standards and protocols shown are in the public domain 
published by all the different Standards organizations. 
 
 Acknowledgments 

 The transition occurring in the broadcast and media production industry is a 
profound  game changer . It is one of the most dramatic and signiﬁ cant changes 
that impacts every aspect of creation, production, management, distribution, and 
monetization. It is more than technology, it is changing the way people work, the 
interaction between business units and how content is consumed. 
 The IP- and ﬁ le-based architecture and the new concepts in workﬂ ows are very 
different from the proprietary tape–based SDI technology and legacy workﬂ ows 
that have been inherited from the early days of television. 
 This book examines the spectrum of differences in the planning and design of 
an IP- and ﬁ le-based infrastructure from a technical, operational, and architec-
tural perspective. Whether planning and designing for a new facility or updating 
and evolving an existing one, there will be a signiﬁ cant impact on the lifecycle of 
media. The changes in the complete lifecycle also have had a considerable effect 
on the workﬂ ow, relationships between business and broadcast, plus the change 
to business processes. 
 The book focuses on the “beginning-to-end” entire architecture and media 
lifecycle, including all of the workﬂ ows and processes—rather than on the over-
used statement of “end-to-end.” There is much discussion in the broadcast and 
production industry that the complete architecture is an end-to-end problem to 
solve. However, there is no silver bullet or single solution that solves all the issues 
of integration, business, and workﬂ ow or technology associated with the entire 
broadcast and production lifecycle of media and its value chain. The design con-
siderations need to start from the very beginning (acquisition) and continue right 
through to the end user experience (distribution and delivery). 
  

Introduction
xiii
 The goal of this book is to provide some useful information and guidelines to 
the broadcast engineer, system designer, and technical management of the changes 
to the technology architecture and workﬂ ows within the IP- and ﬁ le-based archi-
tecture that impact the entire lifecycle and value chain of media. 
 There are a lot of facets to this, so to assist with a little structure, the book is 
organized in a way that will provide a comprehensive understanding of all the 
aspects that need consideration when designing an IP ecosystem and transitioning 
into IP. The book starts with an overview and then each chapter delves deeper into 
each of the different areas. 
 There have been signiﬁ cant milestones in the growth and evolution of broad-
cast and production technologies and processes. This examines the profound and 
substantial changes that the transition to IP is creating. It can certainly be looked 
at as a new puzzle to solve. 
 

This page intentionally left blank

 To begin solving this new puzzle, ﬁ rst it is necessary to identify all the pieces. Then 
they can be assembled into the complete new picture. There are many familiar 
elements to this new architecture and the workﬂ ows. 
 
 Ingest, Acquisition, and Capture 
 
 Workﬂ ow and Business Processes 
 
 Media Management 
 
 Technology Infrastructure and Engineering 
 
 Transmission, Delivery, and Distribution 
 
 Facility Planning and Design 
 This new picture will provide an understanding of what goes into the ﬁ le-based 
architecture in the IP ecosystem. 
 The core infrastructure has changed substantially in broadcast and production 
technology in the world of IP. The design and implementation of IP- and ﬁ le-
based technologies and infrastructure also has to cover new workﬂ ows, processes, 
and facilities design. 
 One of the critical puzzle pieces is the integration of business units. The intro-
duction of new workﬂ ow integration between business units and media is the 
need to work more tightly together. This means that many of the operational silos 
that existed before are now being broken down. Traditionally, media and business 
units operated largely autonomously from each other, as in silos. This “silo” orga-
nizational structure existed even within each of the business and media groups. 
 one 
 

2
Overview
Within the broadcast, production, and technical operations, ﬁ nance, business, 
and legal departments, they all operated independently. Now in the new business 
model, everyone in the organization needs, will, and can access the media and 
metadata. Metadata in the context of media management has critical implications 
and is core to what the IP- and the ﬁ le-based world is all about. 
 One of the many changes that occurs is that the relationship between technical 
operations, engineering, and business demands tighter integration. The broadcast 
center has now become a media data center. Governance, once the province of 
enterprise IT, now plays a signiﬁ cant role in the rules and policies that control 
and manage workﬂ ows and processes of media. These rules and policies are new 
and different when applied to what IP means and how IP now exists in the broad-
cast space and beyond. Program delivery is across many platforms and devices, 
in a time when the consumer watches media on multiple devices and platforms 
(phones, tablets, and PCs). 
 This signiﬁ cant change in the way media is delivered impacts production as 
each platform can have a different format and its own delivery speciﬁ cations. 
 Not only has the technology changed, but there has also been signiﬁ cant impact 
on the design and planning of the physical plant—the actual bricks, mortar, and 
electrical and mechanical infrastructures. 
 These new puzzle pieces ﬁ t together into a completely different picture, in 
terms of the changes in technology, workﬂ ows, the design of the facility as well as 
the design of the infrastructure. 
 That new picture is what this book addresses and represents the entirety of the 
now-changed landscape of producing, managing, and delivering media. 
 New Terminology 
 IP is a term that is used to describe many different things. In the technology world, 
by deﬁ nition, it started out as TCP/IP, which means: Transmission Control Proto-
col (TCP) and Internet Protocol (IP). Then it was shortened to just IP. TCP is now 
only one of a few types of transport protocols. 
 In the media industry, the transition from tape-based or traditional baseband 
or SDI technologies to computer-based or “IT” technologies has been coined 
as IP. The term IP is now used as a more generic deﬁ nition used to describe the 
encoding of media using an application running on a server that is transported 
over a network into a storage environment. It is used to describe how informa-
tion moves within a media organization’s business and media processes across 
the network architecture. The media industry is using the term IP as a mod-
est differentiator to the IT industry, which uses IP to describe a protocol. The 
media industry has adopted “IP” to describe more than a protocol or a tech-
nology. It refers to the overall environment, including computers, applications, 
middleware, servers, storage, and network—plus all the workﬂ ows and business 
processes. 

Overview
3
 In the media management ecosystem, IP is the way media moves and how dif-
ferent systems and subsystems can be connected. This infrastructure has to be 
designed to support the different layers within the IP architecture, and there are 
new tools known as middleware to integrate applications. These tools address 
many of the layers, such as command and control, interfaces between systems, 
media transport, production, metadata in its many formats (e.g. descriptive, 
administrative, and structural), management, communications, and distribution. 
 What does the entire architecture look like from beginning to end? And what are 
the changes that have taken place? Media management includes the business pro-
cesses and workﬂ ows. In many ways the lifecycle of media itself has not changed, 
the ecosystem and technology architecture have. The value chain has evolved to 
more closely integrate many of the processes that were either manually intensive 
or in systems that did not communicate with each other. 
 The book will explore the changes to the core processes in the new lifecycle: 
Creation, Management, Media Movement, Handling, Retention, and Delivery.   
   Figure 1-1   shows at a very high level the core processes that represent the entire 
lifecycle of media movement in the IP world. 
 When raw media is ﬁ rst recorded (ingested, acquired), it is considered  essence . 
 As we  ingest  the  essence  and add  metadata  through the  logging process , the 
media becomes  content. 
 Media is acquired on ﬂ ash drives, Solid State Drives, hard drives, and optical 
disks, or it can be transported ﬁ rst as a stream then captured to a ﬁ le. As SD/HD-SDI 
is brought into the new  ecosystem , it gets encoded to IP as a ﬁ le and then transferred 
to storage. Once in the storage environment, it enables all users to have access to, 
edit, distribute, archive, and manage the media. 
 Studio and ﬁ eld production has changed. For ﬁ eld or remote productions the 
media can now be viewed immediately, rough cut editing can be performed in the 
 FIGURE 1-1 

4
Overview
ﬁ eld, proxies can be posted online for review and approval. Many production and 
business decisions can be made instantly. 
 Computer- and server-based systems have changed studio production. There is 
automation for camera control, video switchers are more powerful with built-in 
image storage, multi-channel 2D and 3D effects, and clip players for animations. 
The studio is a virtual set, tied to the camera robotics and lens telemetry. All of this 
is based on the IP infrastructure. The production consoles are now work surfaces 
connected via the Ethernet switch topology. Media moves between systems over 
the same network as command and control. 
 Craft production, editing, and graphics are all computer-based and add layers 
of complexity with metadata, media management, and version control. 
 A different delivery format is required to send content to a phone, tablet, or 
computer as a stream or a ﬁ le. 
 The term IP covers a fairly broad spectrum of meanings, not all of which are 
related to broadcast. 
 New Vocabulary 
 The broadcast and media industry like most others has always had its own 
lexicon complete with acronyms and abbreviations. The transition to IP- and ﬁ le-
based technologies, workﬂ ows, and processes has brought with it new and excit-
ing acronyms and abbreviations with shiny new terminology and a whole new 
vocabulary. Even the word “vocabulary” has a new application. 
 In the production world, the term “shoot” is still used while “ﬁ lm” has been 
largely replaced by “video,” the same applies to how we “ record ”; now it’s “ acquire 
or  capture ” content “ and ingest ” to a “ ﬁ le or stream ” project, script and location 
notes are now “ metadata that is logged and tagged .” Here again, and as mentioned 
earlier, the raw material is called “ essence ,” and once it’s associated with “ metadata ” 
it becomes “ content .” 
 SD/HD-SDI is “ Encoded ” and “ﬁ les and streams” are “ Transcoded ,” “ Trans-
muxed ,” for ingest. There are multiple formats, and for production and craft, the 
audio and video needs to be “ Embedded ,” “ DeBedded ,” “ Muxed ,” or “ DeMuxed .” 
Then for delivery and distribution, they are “ decoded”  or once again “ transcoded or 
transmuxed ” into many different formats before they are “ spliced ” and “ groomed ” 
before being “uploaded or streamed” to a “content distribution network (CDN)” or 
“cloud” using a “Content Management System (CMS).” 
 Editing is done on a “ reference timeline ” using “ proxies or a mezzanine resolu-
tion ” and is only “ rendered ” to high resolution for ﬁ nishing and delivery. 
 When SDI was introduced there were new tools for Measurement, Test, and 
Monitoring that provided better information on the quality and integrity of audio 
and video. The Waveform Monitor and VectorScope for video expanded to include 
Jitter, Gamut, and Eye Patterns. For audio, Level and Phase metering has expanded 

Overview
5
to Sample Frequency Accuracy, Channel Compliance, Phase-Lock, Fs Jitter, and 
Bitrate. File- and stream-based media has new quality control test and measure-
ment tools and analytics. The new measurements are “ bitrate error ,” “ CRC check-
ing ,” “ packet loss detection ,” “ frame rate ,” and “ GOP length ” to list a few. And now 
“ Quality of Service ” and “ Quality of Experience ” are the new metrics. 
 Information about media is now “ Metadata ” or “ Data about Data .” 
 Metadata, too, has its own terminology that includes a “ controlled vocabulary ” 
with terms such as “ taxonomy ” and “ ontology ,” with “ faceted ” and “ contextual ” 
searching. 
 Workﬂ ow, Processes, and Integration 
       The lifecycle of media begins with the creative process and (essence) acquisition. 
Media workﬂ ows, business processes, and systems have become more tightly 
integrated. 
 Once the  essence  together with its associated  metadata  has been acquired, 
ingested, and logged (now “Content”), it must be made available and accessible 
over the network and throughout the organization. Various business units, such as 
craft production, media management, library, and business operations—including 
legal, ﬁ nance, marketing, rights management, and others—must all work in paral-
lel and in concert to turn this content into an asset, something that has value, is 
managed and tagged, and can be tracked. 
 The entire organization has to create and integrate metadata, and also set the 
rules and rights of the asset. This will position the asset for its journey as it is pre-
pared for distribution and monetization. 
FIGURE 1-2

6
Overview
 While the craft of broadcast and production is the same, the technologies and 
processes have changed a lot. They now include: 
 
 Acquisition/Ingest/Capture 
 
 Craft Production 
 
 Editing 
 
 Graphics 
 
 Virtual Sets 
 Motion Capture 
 Media Management 
 File Movement 
 Library & Archive 
 Metadata 
 Technology Integration – IT 
 Test and Measurement 
 Stream Monitoring 
 File Error Checking 
 Network Monitoring 
 Technical Operations Workﬂ ows & Processes 
 Automation 
 File Movement 
 Business Integration 
 Legal 
 Finance 
 Marketing and Business Intelligence 
 Digital Asset Management (DAM) 
 Digital Rights Management (DRM) 
 Controlled Access, i.e. subscription 
 Protection, i.e. Watermarking 
 Multiple Delivery and Distribution Platforms 
 Over-the-Air (OTA) 
 Over-the-Top (OTT) 
 Cable, Satellite, IPTV 
 Broadband 
 Mobile and Wireless 
 Gaming Console 
 Business Integration 
 The business or enterprise side of a media organization has operated on an IT back-
bone for many years, using a variety of applications (e.g. ofﬁ ce productivity tools, 
customer management, accounting, databases, etc.), servers, storage and network 
systems. Now the broadcast and media production side of the organization has 

Overview
7
moved to a similar backbone, but with the additional considerations and require-
ments such as Quality of Service (QoS), low latency, security, and access. In this 
new relationship, the technical operations group supports media operations and 
ties the business units to media services. There is a need for close collaboration 
between enterprise IT and broadcast IT and both must respect the differences in 
technology considerations that result from this change. 
 As the media is delivered to so many different platforms, there are new concerns 
about access and protection. 
 
 Over-the-Top (OTT) 
 
 Over-the-Air (OTA) 
 
 Video On-Demand (VOD) 
 
 Streaming—Phones, PCs, tablets, or game consoles 
 These are managed through metadata and media management tools. 
 Technology integration between media and enterprise IT needs to protect both 
the business network and the media network. 
 The enterprise and broadcast networks now intersect and media management 
crosses over to the enterprise while both networks need to be protected from each 
other and from the outside. 
 In the media IP- and ﬁ le-based ecosystem, managing the library and archive is 
different from just storing tapes on a shelf while using yellow pads with segment and 
timecode information; and tracking them in a database. A digital library is a reposi-
tory of media that is dependent on metadata, asset management tools, and a storage 
architecture that includes removable media for long-term archiving. The metadata 
is associated with the media by using asset management tools and databases. 
 Technology Infrastructure 
 The core infrastructure is now comprised of applications, servers, storage, and net-
work. When planning and designing any facility it is critical to anticipate growth. 
The core technologies in the IP infrastructure are different from tape-based SDI 
technologies and have other considerations: 
 
 Extensibility 
 
 Scalability 
 
 Sustainability 
 Extensibility : This is a design principle where the implementation takes into con-
sideration planning for future growth. 
 With technology changing as rapidly as it does, it is essential in the planning 
and design of a facility to assure that extensibility provides for and  anticipates 
future growth. 

8
Overview
 Scalability : The ability of the hardware and software systems to expand to han-
dle the growth without requiring replacement. This growth can be an increase in 
the volume of material acquired, produced, delivered, or archived. It could be the 
number of users needing access or an increase in delivery platforms. What about 
the growing amount of proxies and metadata? 
 Scaling, growing, or expanding in the IP ecosystem is accomplished by adding 
servers, network, storage, and applications. Are there enough network ports? Is there 
enough bandwidth? If more storage capacity is added, what about throughput? 
 Sustainability : Maintaining hardware and software is all about service con-
tracts, software updates, and patches. Sustainability includes keeping spares or 
redundant systems as hot spares or backup. 
 Total Cost of Ownership 
 Planning a new facility or making a signiﬁ cant capital investment in an existing 
facility, one of the critical elements of budgeting and decision-making is the  total 
cost of ownership  of equipment and infrastructure. It is important to know not just 
what it costs to build, but also the cost to operate and maintain.  What does it take 
to run ? For the IP architecture, there are a number of new considerations. 
 Facility Infrastructure—Physical Plant 
 Infrastructure is about more than just the technology. It is also about the physical 
facility, and with that there are a considerable number of changes to consider in 
planning and design. 
 It is generally accepted that building a new facility is always easier than upgrading 
an existing one. Many of the considerations are the same, but it’s always a little easier 
to build from scratch, rather than modify existing infrastructure. This is as true with 
the physical aspect of the project (bricks and mortar) as it is with the technology. 
It’s also much easier for business continuity if the upgrade does not involve working 
around live systems. It is something like changing the tires while the car is moving. 
 When thinking about making building changes to accommodate IP- and ﬁ le-
based systems, space planning is completely different. New control surfaces do 
not require the same proximity to a mainframe for the proprietary connection 
requirements because they are now IP and connected over the Ethernet network. 
Even the server controls—screens, keyboards, and mice—are accessed using 
extenders with a switch matrix that allows a single screen, keyboard, and mouse to 
operate multiple servers from many locations. 
 Core space planning includes: 
 
 Room Adjacencies 
 
 Spatial Allocations 

Overview
9
 
 Room Interdependencies 
 
 Ergonomics 
 Production control rooms no longer accommodate as many dedicated control 
surfaces or have as many individual monitors by using multi-viewers, so they’re 
more efﬁ cient. 
 A typical console and room layout still has the same design considerations. 
These include: 
 
 The quantity and type of control surfaces 
 
 How much support equipment 
 
 How many screens or windows on the multi-viewer 
 
 Ergonomics—the placement and proximity of primary control surfaces 
 Heat and Power Loads and HVAC Design 
 Power-wise, spaces today need a lot less power, and ergonomically they are easier 
to manage and operate. In the control rooms there are mostly control surfaces not 
the equipment frames or servers. In the equipment room the production systems 
have more of the features and functionality embedded in fewer physical devices. 
This signiﬁ cantly reduces the heat load. An example of this is the production 
switcher, it is a composite of functionality, encompassing a video switcher, still 
store, digital effects, and clip animation player. 
 Operating Costs—Space/Power/HVAC 
 The amount of physical space required factors into the total cost of ownership, 
including basic overhead costs such as space, power, and mechanical systems. It 
is a reasonable assumption that if there is less space in an IP-based architecture, 
it’s possible that the cost of overhead is lower. The amount of mechanical systems 
that are needed for the control rooms and support spaces is reduced signiﬁ cantly 
and that becomes a reduction in capital costs that can also translate into potential 
cost-savings on power. 
 In the IP facility, there are considerable changes to all control room designs. 
There are fewer control surfaces because of the greater feature sets and consolida-
tion of functionality built into the hardware and software. This creates a signiﬁ -
cant change in console layouts. 
 Media management and metadata (for description, business, and automation) 
can be entered at the same time that craft production is taking place. 
 The media manager and automation system are applications that handle 
the transport of the file as it moves through the system. As content goes 
through the encoder and the media manager, and then into production and 

 FIGURE 1-3B 
 FIGURE 1-3A 

Overview
11
 Changes in Workﬂ ow 
 The interaction between business units and the handling of media has changed 
many of the workﬂ ows and business processes. Business models have also signiﬁ -
cantly changed for creating and distributing media. As media moves through-
out the entire infrastructure, it needs to be accessible to all departments/units 
including production, distribution, library, legal, ﬁ nance, marketing, and busi-
ness intelligence. These groups need access to the media to input and export 
metadata and to ensure that the appropriate business rules, policies, and permis-
sions are in place. This metadata will protect the content as it is distributed into 
the marketplace. 
 Beginning-to-End Planning 
 ‘Beginning-to-End’ is the concept of planning and design that must include busi-
ness operations, technical operations, and workﬂ ow management. 
 Workﬂ ow is a term often used and sometimes abused. It is deﬁ ned as a series of 
connected activities and processes. For example, editing workﬂ ow is the process 
of editing plus all the different pieces and parts associated with editing. Account-
ing workﬂ ow relates to all the things associated with payables, receivables, and 
accounting work. 
 FIGURE 1-4 
distribution systems while insuring a copy goes into archive, automation plays 
a much larger role.   

12
Overview
 There are many workﬂ ows within the world of media, technical operations, and 
their association with business operations. Some new processes are made possible 
by adding technology and achieved by changing the way people work. 
 During  acquisition, essence  is  captured  and  ingested  using encoders and 
transcoders. In the  craft workﬂ ow there is  Remote  and  Studio  production,  edit-
ing , and  graphics . These are all workﬂ ows that change in the IP ecosystem. Each 
of these processes needs simultaneous access to the same media ﬁ les in the shared 
storage over the IP architecture. 
 Media Management 
 Media management means many things to different people, and it is often con-
fused with only being asset management. Media management is the entire col-
lection of processes and technologies that handle media throughout its lifecycle. 
 Historically, media management was considered a trafﬁ c and library function. 
Now that the business process is tightly integrated with the handling of media,   it 
could be easily said that the entire new lifecycle from beginning to end is called 
media management . 
 Asset management, speciﬁ cally digital asset management, assumes that we have 
something digital, we've applied some value to it, have started calling it an “asset,” 
and that this asset needs to be managed. The term for raw media is  essence . A video 
that is captured, given a title, and possesses a reason for being (value) becomes “an 
asset.” By assigning a title and other descriptive data to the asset now allows it to 
be indexed, organized, and managed in a way that is easier for users to search and 
ﬁ nd it without needing to know very much about it. 
 The  asset management  system is the master repository for the  metadata that 
has to be  logged , indexed, and managed. Metadata has to be associated with the 
media for the media to have any value. Media without metadata cannot be man-
aged or monetized. Asset management assumes that enough metadata has been 
applied to the content that has become an asset. 
 Asset management is the process of attaching useful information to media that 
allows the asset to be tracked and searched as it moves throughout the value chain. 
This includes saving a copy to the library for  archiving , thereby enabling access 
for future use, say in a week, a year, or a decade. It’s important that when an asset 
is requested again, there is enough information to tell what it is, why and when it 
was made, and who has the rights to use and access it. 
 The media management system creates and maintains the rules, vocabulary, 
and structure for the metadata that are applied to the asset in order for it to be 
useful. Files are archived in a digital library so users will be able to access them in 
the future. For example, a sports program with a great catch, an amazing pitch or a 
signiﬁ cant news event needs to be tagged with metadata (i.e. keywords, who, what, 
and where) so the content can become searchable and retrievable. 

Overview
13
 Metadata is not only used for management and automation. It has an impor-
tant use in driving electronic program guides (EPGs), enabling end users to ﬁ nd 
their programs. It’s also increasingly used to drive monetization as part of new 
business models for second and third screen revenue. 
 Assuming all media is being archived, what format is it archived in? If it’s going 
to be archived on removable media, there are decisions to make. Is it optical disk, 
ﬂ ash, digital tape or does it stay on spinning disk? 
 An Example of IP Architecture for 
Media Management 
 Imagine that there are different LANs, with different application servers, and 
different users that will access the information across various networks. On the 
broadcast network (BLAN), user workstations manage tasks on application serv-
ers with systems that move media around, handle, ingest, archive, and play media. 
There are database servers to manage the metadata and video servers to handle the 
audio and video. These applications can be on dedicated servers and/or on virtual 
servers (i.e. VMs or virtual machines). 
 The next questions are about the storage architecture: 
 
 Where are the video and audio coming in from? 
 
 Where is it landing? 
 Is there a remote access storage system to receive incoming media over FTP and is 
there proxy storage that handles the media formatted for searching and browsing? 
There should be separate storage for craft and production so work in progress 
doesn’t consume valuable real estate on the main media storage. The interaction 
between the internal networks as well as allowing access to outside users is gov-
erned by a set of rules and policies. 
 Another design component of media management is  automation , which is 
how the systems and subsystems interact with each other. Media has to move 
seamlessly throughout the IP infrastructure among different application serv-
ers, over the network through various storage systems, from ingest to play-out. 
In  distribution , the automation is responsible for controlling the delivery to a 
number of distribution channels each with a different requirement. Content 
is sent to a transmitter for Over-the-Air and to a cable or satellite provider for 
traditional television distribution. And it is delivered to a content distribution 
network (CDN) for delivery to the web, a tablet, a phone, and Over-the-Top. Auto-
mation is a bigger concept than just play-out, and there are new tools to manage 
automation. 
 Therefore, to manage media and how it's handled, the whole architecture must 
be viewed holistically. 

14
Overview
 Engineering in an IP world 
 The role of the broadcast engineer has changed dramatically. There are now new 
skills and knowledge required to maintain the IP media environment. The long-
standing broadcast engineers’ philosophy that media needs to be the highest qual-
ity with the highest service priority has NOT changed. 
 Quality Assurance and Quality Control 
 A broadcast center needs to be designed with the ability to ingest all these different 
formats and insure their integrity, while making them available for inclusion in 
live programs or to be used as captured content in craft production. 
 One of the greatest challenges with all these different acquisition and contribu-
tion formats is  quality control. 
 Quality control begins in the ﬁ eld before a ﬁ le is transferred. As live streams 
are transported, they too need evaluation and analysis. The Waveform monitor, 
VectorScope, audio metering, and spectrum analyzers are still part of the engi-
neer’s tool set with SD/HD-SDI. In the IP environment, there are new ways to 
monitor and measure the quality of the signal in real-time. If an artifact needs to 
be corrected or ﬁ xed, it shouldn’t have to wait until a recording is complete (when 
it’s too late). In the IP and ﬁ le world, there are new tools such as bitrate analyzers, 
packet loss detection, and bit error detectors for ﬁ le and stream analysis. There are 
also different types of analyzers to determine the integrity of the network, such 
as bandwidth, port speeds, VLAN trafﬁ c, and many more. The conﬁ guration of 
the network has a signiﬁ cant impact on both performance and quality control for 
media movement. 
 Another thing to monitor is the integrity of the metadata in the database. 
When metadata moves along, it moves between different databases and across 
systems. 
 In the IP architecture, quality control also applies to the applications and serv-
ers. As ﬁ les and streams move back and forth across IP networks, bandwidth 
becomes a major consideration. 
 Which of these must be prioritized? What about Quality of Service (QoS)? Net-
work monitoring is required to ensure it’s up all the time, in terms of the integrity 
of the data and storage monitoring. Disks also require routine maintenance. 
 The acquisition process captures content to removable media encoded from 
AES/SDI, and then brings it in as a stream and/or ﬁ le. It lands on a server and 
is then moved to storage, where it’s managed. During the evolution to SDI, 
the audio was embedded into the video for transport and distribution. In the 
transition to IP, the next step of the encoding process is to embed audio, video, 
control, and management data into the ﬁ le or stream using different acquisi-
tion, transport, media storage, and distribution protocols.  This all begins with 
acquisition . 

Overview
15
 During acquisition, the focus is on what has changed with how media is cap-
tured and ingested, and that many of the rules are different from before. As a 
result, there are numerous format and codec decisions to be made. 
 IP has different tools and metrics for use in quality control, format control, and 
metadata management. These tools assure that all of the critical elements are pres-
ent, so that when production, business, and distribution people look for media 
and metadata they can access it easily based on the policies and rules. 
 Broadcast now shares an IT-centric world that uses Ethernet network switches, 
servers running media applications, and complex storage systems that all are han-
dling high bandwidth streams plus large bulky ﬁ les. The concept of quality control 
monitoring with test and measurement hasn’t changed; there are just new param-
eters that need monitoring. Some of these new parameters deal with priorities in 
the network—like bandwidth utilization and quality of service (QoS)—and some 
are more subjective, like quality of experience (QoE). These parameters are mea-
surable against a set of metrics determined by the engineer. 
 In the IP environment, there are new challenges. One of these is latency, previ-
ously known as delay. Delay has always been a serious problem in media process-
ing and delivery. In the IP-based architecture command and control is on the same 
network as media transport. If the network is not carefully conﬁ gured there can be 
latency. One example of latency is when the play-out system requests a program 
to air, and the network trafﬁ c introduces a delay that prevents the request from 
arriving; there will be dead air until the request is executed. There are a number 
of areas within the ﬂ ow of digital media where latency can be introduced. One of 
the more noticeable aspects of latency is the synchronization of audio and video 
within a program element, also known as lip sync. In the world of digital media, 
there is an interesting phenomenon where broadcasters have compromised their 
level of quality and allowed a certain amount of acceptable latency when it comes 
to lip sync. 
 In the broadcast world, there is no such thing as “off the air.” All systems have 
to be up 24/7/365 and they have to work. Routine and preventative maintenance is 
something that has to happen in real-time while systems are running. This would 
be something like changing the tires while the car is driving on the highway. 
 IP networks have to be protected and security is both of deep concern and also 
presents many challenges. There is internal security, external security, and opera-
tional security. The business side wants to protect their enterprise data because 
large media ﬁ les could slow the business down and congest the network. Broadcast 
engineers have to protect their network because they have to be on the air. There-
fore, bandwidth management is critical in the IP infrastructure because it ensures 
that nothing prevents the media from moving. Bringing media into the system 
from removable media can introduce viruses or Trojans. 
 There are differences in providing technical support in broadcast and produc-
tion vs. enterprise. Solving technical problems is  not  calling the help desk! The 
broadcast engineer  is  the help desk and has to be equally responsive to IP issues 
and staying on the air. It’s no different than SDI. 

16
Overview
 This overview has taken a high-altitude look at the entire media lifecycle from 
 beginning through to the end.  It introduces what constitutes an IP infrastructure 
and the multitude of elements that need to be considered when designing, plan-
ning, and building an IP infrastructure. 
 There are new considerations in the operation of an IP-based ecosystem that 
range from the acquisition process to media management, craft production, busi-
ness unit integration, library, archive, and retention. There are accessibility issues 
and a new breed of multiple stakeholders and business units that search through 
and access content. Media is delivered to all platforms in different forms. 

 There are many business processes that are part of the lifecycle of media and 
media management. The creation or acquisition process begins the media han-
dling part of the media management process. Acquisition is the capture of essence 
and adding metadata to make it into content so it can be ingested into the IP- and 
ﬁ le-based workﬂ ow. The acquisition piece of the puzzle is going to address the 
technologies, workﬂ ows, and processes related to remote and studio production, 
as well as all the various forms of contribution that bring media into the broadcast 
production facility. 
 two 
 
 FIGURE 2-1 

18
Ingest/Acquisition/Capture
 The lifecycle of media actually begins before any media is actually created. Early 
in the process contracts are signed, distribution rights negotiated, and production 
is planned. Each of these processes creates metadata. Next in the media manage-
ment ecosystem is the acquisition process, which includes studio and remote pro-
duction plus contribution. This is the critical stage where format and removable 
media type decisions are made prior to ingest. Essence is the term used to describe 
the raw audio, video, and still images. It is acquired through cameras and micro-
phones and then encoded into ﬁ les or transport streams. This is also the point 
where tagging and logging of metadata begins. 
 When the metadata is associated with the essence it becomes content that can be 
created as a ﬁ le or stream. If it is created directly to a ﬁ le and transferred to storage, 
is the storage removable media? There are a number of different kinds of storage 
media: a hard drive, solid state or ﬂ ash drives, optical disk or digital tape. If it is 
encoded to a stream for transport then there are different formats for streams before 
it is captured to a ﬁ le. There are different formats used for sending content from the 
ﬁ eld as remote production or as contribution depending on if it is a ﬁ le or a stream. 
When the content is captured to ﬁ le, the ﬁ les need to be transported back to the 
broadcast center. If there is enough bandwidth available, the ﬁ les can be transferred 
(uploaded) to the broadcast center or, if not, then they need to be transferred to 
removable media and then physically transported (courier or hand carried). 
 If the content is going to be transported as a stream there are also a number of 
options and decisions. They can be sent in their native SDI form or encoded into 
multiple kinds of IP streams, such as ASI, MPEGTS, JPEG2000, MPEG4 H.264, 
HTML5, and HTTPLive. Most people think that by sending SDI Uncompressed 
(SD-SDI is 275Mb/s and HD-SDI is 1.4Gb/s) is using an uncompressed trans-
port circuit and that there is no compression. Actually most video circuits are still 
based on some form of compression so while the input and output of the circuit 
may be SD/HD-SDI, it is still compressed. Once it is received at the broadcast 
facility, it is encoded to a ﬁ le and transferred to storage. If the SD/HD-SDI is 
encoded to a stream in the ﬁ eld, then the type of transport changes, fast Ethernet 
circuits are 1Gb/s up to 100Gb/s. The stream will still be received by servers, cap-
tured as a ﬁ le, and transferred into the media storage system. It can also be passed 
directly through to distribution as a live stream. When it is live studio production, 
it is encoded to a ﬁ le into media storage and is also streamed live to distribution. 
Streaming media live for distribution is a different workﬂ ow and process from 
delivering ﬁ les for distribution. 
 Whether the content is a stream or a ﬁ le, quality assurance is critical, but the 
methods for handling the two differ signiﬁ cantly. This will be discussed in greater 
detail in the engineering chapter. As we are capturing the essence, there is a logging 
and tagging process to enter metadata that is a core component in media manage-
ment, whether for a stream or a ﬁ le in the IP ecosystem. 
 The ﬁ gure below shows when media (or essence) is imported into an IP infra-
structure, the SDI video and/or AES audio must ﬁ rst be encoded. The encoding 
process involves multiple format options for ﬁ les and streams. 

Ingest/Acquisition/Capture
19
 Acquire, Ingest, and Manage 
 What used to be called baseband in analog is now called Wideband (SD) or Ultra 
Wideband (HD) in SDI and now there is UltraHD (4K) which is four (4) times 
the number of pixels than HD-SDI, each of these are the terms used to deﬁ ne the 
content as uncompressed. In the acquisition workﬂ ow, the SDI and 4K has to be 
captured and encoded to a ﬁ le or stream. 
 If the content is already in ﬁ le form, then the ingest process is handled by ﬁ le 
transfer and if the ﬁ le format is not in the house standard format then ﬁ le conver-
sion is needed. 
 This now becomes a  transcoding  or  trans-muxing  process. What’s the difference 
between  transcoding and  transmuxing ? 
 Transcoding  is the process of changing the format of audio or video through a 
partial decode and re-encode. This is considered a very lossy process. 
 Transmuxing is the process of changing the format of a video or audio ﬁ le while 
preserving some or all of the streams from the original ﬁ le. Transmuxing is a more 
efﬁ cient way to convert a ﬁ le, and involves little or no loss. 
 If the content originates as SDI, then it needs to be encoded. The encoding 
process and format decision includes a decision on compression. If the content 
is encoded to a stream then there are multiple decisions. What is the transport 
format; and then as it arrives in the server as a stream, how will the stream be 
captured to a ﬁ le? 
 Compression 
 When media is compressed, sophisticated algorithms are used to calculate what 
bits can be removed during the encoding process without affecting or minimally 
affecting the quality of the content when it is decoded. There are many different 
encoding schemas. MPEG2 follows an I Frame and Long GOP (Long Group of 
Pictures) format, JPEG2000 is based on the Discreet Cosine Transform (DCT) 
 FIGURE 2-2 

20
Ingest/Acquisition/Capture
methodology. MPEG4 H.264 follows closely to MPEG2 and the open source 
formats like ON2 follow a number of different compression formats. 
 The basic rules are simple, the higher the compression rate, the more content 
gets removed and the more apparent it is when viewing and listening to the video 
and audio. Lossless compression suggests that while there is content removed, there 
is no perceptible change to the quality of the content. Lossy compression suggests 
that in the compression process there will be discernible quality loss in the content. 
The compression process evaluates key frames, groups of pictures (GOP), white and 
black spaces, colorimetry, motion, and silence and then the algorithms make deter-
minations of what can be reliably reproduced without compomrising the quality of 
the content during the decoding process. In the decoding process, one of these tech-
niques is called interpolation. This is mostly used in sports slow motion for instant 
replay. What is interpolation? If there is a gap between two frames, the algorithm 
makes an assumption based on the frames that bracket the gap, the before and after 
frame. It creates a ﬁ ller that replaces the gap. This has caused some interesting dis-
cussions when ofﬁ cials using slow motion instant replay are making judgment calls 
for goals or infractions. For example, if the replay is to determine if a player is out of 
bounds and the ofﬁ cials look at a replay, if the point where the player’s toe is too close 
to the boundary is in one frame and the next frame is missing, but the following 
frame is on the line, the algorithm in interpolating the missing frame could put the 
toe in or out which can change the outcome of the game. If the interpolation is not 
done, then the game ofﬁ cial has to make the call based on essentially missing video. 
It presents a challenging question of allowing the algorithm to add a best guess. 
 When the content is decompressed, the algorithms track the compression 
schema and, during the decoding process, replace missing content by making theo-
retically intelligent decisions where either duplicate pieces of the retained content 
or interpolative assumptions can be used to restore the full content without any 
perceptible change or loss. 
 Since each compression process is unique and there is no catalogue of what the 
ﬁ rst compression process removed, the next stage of compression removes some-
thing completely different. This means that each time media is compressed, encoded, 
decompressed, and recompressed, it is impossible to get back what was originally 
compressed—meaning what was removed. Therefore, each compression process is 
forced to remove new things. In the world of digital media theoretically there is NO 
loss in transfer or copy—or maybe not. Each iteration of compression adds more loss. 
This is called  concatenative  loss, a fancy word, but it still just means loss. This is essen-
tially the same as degenerative loss during the tape duplication and transfer process. 
 Quality Control 
 The concern about quality control has not changed. There are new tools and qual-
ity control processes to maintain consistency for a stream or a ﬁ le. One of the 
issues that ﬁ le-based production introduces is that before a quality control tool 

Ingest/Acquisition/Capture
21
can analyze a ﬁ le, the ﬁ le ﬁ rst needs to be fully rendered. File analyzers look at 
the integrity of the ﬁ le and a stream analyzer looks at the packets and compares 
them to the parameters of the protocol selected. Speed becomes an issue here. If a 
program is 1 hour long and the ﬁ le analysis is in real-time and takes an hour, the 
program is not available for craft or distribution for 2 hours. The manufacturers 
of the testing tools are working to improve this by having the process happen faster 
than real-time or check the media as it is being written to a ﬁ le. 
 Manage 
 Once media is encoded as part of the ingest process, it moves to shared storage 
based on the architecture. This may be production storage or centralized storage. 
Once in the storage infrastructure, it can be concurrently accessed through edit-
ing, management, library, distribution, and metadata processes. 
 In production and contribution, which storage and removable media format to 
use is just one of many questions. How many formats are there? 
 
 Optical disk 
 
 Flash or Solid State Drives 
 
 Digital tape 
 
 Hard disk 
 Cloud storage is not a unique type of storage; it is a place. Instead of building a storage 
architecture in the broadcast center it is built in a separate datacenter. It depends on 
the same storage technologies, it is a different way to access, store, and manage media. 
 Streams have their own formats. Now is a good time to remember that SDI is 
the acronym for Serial Digital Interface. There is Standard Deﬁ nition (SD), High 
Deﬁ nition (HD), and Ultra High Deﬁ nition (UltraHD; 4K). Each of these is a 
protocol based on published standards or is it a standard based on published 
protocols? This is a topic for another chapter! SD/HD-SDI are MPEG2 formats 
that are typically considered uncompressed in the broadcast architecture. 
 
 HD-SDI can be multiple formats i.e. 1080i, 1080p, 720p, or 480p. These num-
bers determine vertical and horizontal resolution, aspect ratio, pixel aspect 
ratio, scanning and frame rate of the content. 
 
 The introduction of 4K as UltraHD raises the horizontal and vertical, aspect 
ratio, pixel aspect ratio, and frame rate which all translate into a considerable 
increase in bitrate. This introduces challenges in production, storage, trans-
port and delivery for distribution including to displays. 
 In the IP ecosystem, SDI can be encoded to a broad spectrum of formats. One of 
the primary considerations with respect to selecting a format is its intended use or 
delivery platform. This can be production, archive, OTT, mobile, or web. 

22
Ingest/Acquisition/Capture
 In addition to encoding, media can be ingested directly from other IP sources 
as ﬁ les or streams and may require transcoding. Managing this process starts get-
ting complex. It may be packaged into a container to move across systems and 
devices throughout the architecture. There are a number of standards and proto-
cols when media moves through the IP environment whether as a ﬁ le or wrapped 
in a container: 
 
 MXF – is the SMPTE standard for craft production and archive 
 
 GXF – is primarily used for delivery 
 
 BXF – has superseded GXF with more efﬁ cient packaging 
 
 ProRes – is a proprietary Apple technology 
 These are all container formats. The ﬁ le itself within the container is a different 
format. Some of the ﬁ le formats are MPEG2, MPEG4, JPEG2000 and Quicktime 
which are usually associated with the video, and AAC, WAV, and PCM are the 
formats associated with audio. The containers are a way the audio and video 
can be packaged together in an interoperable transport ﬁ le to move between 
systems. If the media is a stream, it can be delivered over different transport 
systems using SDI, IP, or ASI. Media can be transported compressed or uncom-
pressed and both ﬁ les and streams are handled in the same way. SDI is consid-
ered uncompressed. There are different formats, standards, and protocols used 
to move media. 
 All of these are transport protocols: 
 
 SMPTE – SD-SDI SMPTE 259M 
 
 HD-SDI SMPTE 292M 
 
 ETSI – ASI- TR 101 891 Technical Report 
 
 MPEG2 – ISO/IEC 13818 
 
 MPEGTS – ISO/IEC 13818-1 
 
 MPEG4 – ISO/IEC 14496 
 
 MPEG4 H.264 – ISO/IEC 14496-10 
 
 JPEG2000 – ISO/IEC 15444-12 
 Then there are the other questions such as: 
 
 Is the media being captured to removable media, or directly to a server and 
then ﬁ le transfer? 
 
 Is it getting encoded to a stream and transported over an IP link to the broad-
cast production center, where it will be captured as a ﬁ le? 
 
 What about stream format? Is it ASI or IP—is it J2000, MPEG2-TS, or MPEG4 
H.264? 
 
 In the case of JPEG2000, it is deﬁ ned as both a ﬁ le and stream format. 
 
 What ﬁ le format is the broadcast center standard? 
 
 What format is the original recording? 
 
 What is the bitrate? 

Ingest/Acquisition/Capture
23
 
 How much compression should be done in the ﬁ eld? 
 
 Which audio format is used? Compressed or uncompressed? 
 
 How much storage is needed? 
 These are just some of the questions that need to be considered when looking at 
acquisition. 
 What factors are involved in deciding how much storage is needed? Here is one 
example: 1 hour of DVCPro100 creates a 60GB ﬁ le. However, with 8–16 channels 
of audio, the ﬁ le size is closer to 72GB. 
 There are a number of storage options when the essence is being captured on 
removable media. Optical disks are now four layers and can handle up to 128GB. 
Flash drives hold up to 128GB. Hard disks go into the terabyte range, with Solid 
State Drives (SSD) quickly replacing spinning disks. Speed is very important when 
making a decision on solid state and hard drives, since higher bitrates need faster 
drives. One of the major considerations when selecting storage is that bandwidth 
throughput and drive speed have as much of an impact on design as the  amount 
of storage. This will be covered in more depth. 
 Standards and Formats 
 Articulating the difference between standards and formats makes for an interest-
ing discussion. While the industry standards organizations have agreed on the 
standards and protocols, each lists them with their own catalogue numbers or the 
standard is listed as a joint effort: 
 
 SDI is SMPTE and ITU 
 
 MPEG is ISO and IEC 
 
 JPEG2000 is ISO/IEC 
 
 ON2 is Open Source 
 Once one group establishes a standard, the other groups may adopt it and assign 
its own designation. For example, the main MPEG2 standard is both ISO/IEC 
13818-1 and ITU-T Rec. H.222.0. While these organizations publish them as 
standards, in reality they are considered protocols. This is something akin to a 
“polite” suggestion of a standard that allows many variations, formats, or “pro-
ﬁ les.” Whether it’s a stream or ﬁ le, there are many formats that are permissible 
under each standard. For each published standard, there are numerous proﬁ les—
and proﬁ les are not necessarily compatible with each other. 
 MPEG Proﬁ les 
 
 MPEG2 4:2:2 @ MP/HL 
 
 MPEG2 4:2:0 @ MP 
 
 MPEG4 Part 2 
 
 MPEG4 Part 10 (H.264) 

24
Ingest/Acquisition/Capture
 In an SD/HD-SDI infrastructure, media transport is well deﬁ ned. In the IP infra-
structure and the transport of streams and ﬁ les,   bandwidth   is king. When it comes 
to ﬁ le standards and formats, there are many other decisions to be made. The basis 
for these decisions is determined by these questions: 
 
 Where is the ﬁ le going and how many resolutions are needed? 
 
 Is it the library copy? 
 
 Is it the production or proxy version? 
 This drives the decisions surrounding the following: 
 
 The ﬁ le size 
 
 The amount of compression 
 
 The format compatibility between systems 
 
 How much bandwidth is available 
 
 How much storage will be needed 
 Some ﬁ le formats are used for encoding, while others are used for transport. When 
content is encoded, there is a separate codec for audio and video. When content 
needs to move across systems, it is wrapped into containers. All the codecs and 
containers are based on the published standards. 
 Containers: 
 
 MXF – SMPTE 377M 
 
 GXF – SMPTE 360M 
 
 ProRes – Apple 
 When a stream is created for broadcast contribution, there typically are only a few 
standards used: MPEG2, MPEG4 H.264, and JPEG2000. These can also be encap-
sulated into DVB S, DVB S2, DVB-T, or as ASI or IP. 
 There are other formats used for delivery to online and mobile platforms, such 
as Flash, ON2, HTTPLive, and HTML5. 
 When a ﬁ le is being created, there are literally hundreds of codec options 
for encoding. Once the audio and video is encoded, it is then packaged into 
containers or wrappers. The codec may be AVCi. XDCAM, DVCPro, and ProRes 
for video and PCM, AAC, or MP3 for audio, but the wrapper is MXF, GXF, and 
QuickTime. 
 When making a decision on compression formats, is there really such a thing 
as lossless compression? With compression something is always given away! In 
the great codec wars, there are so many different formats and more coming 
every day. The codec decision is based on what the best format is for library, 
production, delivery, and proxy. The multiple delivery formats are speciﬁ ed 
for each of the distribution channels. While in production, interoperability is 

Ingest/Acquisition/Capture
25
crucial if more than one edit system is going to be used. It is fairly typical to 
create the content in the multiple formats required for wireless, mobile, and 
web delivery. 
 Consider the subject of bitrates. All bitrates are not created equally; there are 
different formats and bitrates for ﬁ les and streams. Quality is  not  only determined 
by the actual bitrate. There are other factors, such as the color space, bit depth, and 
inter-frame vs. intra-frame. Is it LongGOP? When choosing the bitrate, the higher 
the bitrate, the larger the ﬁ le, which means more storage and more bandwidth. 
With a lower bitrate, there is a reduction in bandwidth and the ability to make 
more efﬁ cient use of storage. At the same time, you have to determine whether 
quality is compromised. 
 So what is the best choice? What’s the best resolution to start with? One sug-
gestion is to create one house format and look no further (a decision that can 
just as well be made in the tape-based world). Historically, it may have been 
1” Tape, BetaSP, DigiBeta, or D5. There was one format for acquisition and one 
for in-house. Once a tape came in, a dub was made, and then the master was ﬁ led 
in the library. 
 In the IP- and ﬁ le-based space, there are different copies of the ﬁ le: a high reso-
lution for the library copy, mezzanine resolution for production, and lower proxy 
resolution for searching and browsing. 
Codecs 
So Many to Choose from
Audio Video Standard (AVS); OpenAVS Blackbird FORscene video codec; Cineform; 
Cinepak; Dirac; Schrödinger; dirac-research; DV; Firebird [1] Original FORscene 
video codec; H.261; FFmpeg H.261; libavcodec); MPEG1 Part 2 (MPEG1 Video); Cinema 
Craft Encoder; FFmpeg; Ligos LSX MPEG1; MainConcept MPEG1; TMPGEnc; 
H.262/MPEG2 Part 2 (MPEG2 Video); Canopus ProCoder; Cinema Craft 
Encoder; Elecard MPEG2 Video Decoder; FFmpeg; InterVideo Video Decoder; Ligos 
LSX MPEG2; MainConcept MPEG2; TMPGEnc; H.263; FFmpeg H.263 (libavcodec); 
MPEG4 Part 2 (MPEG4 Advanced Simple Proﬁ le); 3ivx; DivX; FFmpeg MPEG4; HDX4; 
H.264/MPEG4 AVC or MPEG4 Part 10 (MPEG4 Advanced Video Coding), approved for 
Blu-ray; CoreAVC; MainConcept l; QuickTime H.264; Sorenson AVC Pro codec, Sorenson’s 
new implementation; Vanguard Software Solutions; x264; Indeo 3/4/5; MJPEG; FFmpeg; 
Morgan Multimedia M-JPEG; Pegasus PICVideo M-JPEG; JPEG 2000 intra frame video 
codec; OMS Video; On2 Technologies TrueMotion VP3/VP4, VP5, VP6, VP7, VP8; 
under the name The Duck Corporation: TrueMotion S, TrueMotion 2; Pixlet; Apple ProRes 
422; RealVideo; Snow Wavelet Codec; Sorenson Video, Sorenson Spark; Tarkin; Theora; 
FFmpeg; libtheora; VC-1 (SMPTE standard, subset of Windows Media Video); VC-3 SMPTE 
standard; Avid DNxHD; FFmpeg; Windows Media Video (WMV); WAX (Part of the Windows 
Media Series)…………..and so on; and so on.
 FIGURE 2- 3 

26
Ingest/Acquisition/Capture
 So Many Formats, So Many Choices 
 Not to stir up confusion, but indeed there are probably between 800 and 1,000 
different codecs available. And it seems like more and more are introduced 
every day. Arguably, there may be only a half dozen or so that are mainstream 
and that matter, but there are many other formats out there, as well as some-
thing that is of greater concern: old formats don’t die. When planning and 
designing a facility, it is strongly recommended to standardize on one format 
and maintain that throughout the entire facility and workﬂ ow.  THE GOLDEN 
RULE IS, “PICK ONE.”  
 Pick any one. Choose a bitrate, resolution, and ﬁ le format compatible with 
production systems. Next, select the container, and then   STANDARDIZE ON IT . 
 Then, communicate this standard to those creating content and keep it in that 
format while it’s in the production environment. This way, while it is in-house, 
everybody’s handling the same format. 
 Handling ﬁ les and streams philosophically is no different than with tape. 
While it is important in the tape-based world to avoid creating a large number 
of copies to minimize degradation, in the IP- and ﬁ le-based world, it is equally 
important to avoid multiple transcoding or encoding processes to minimize any 
loss of quality. 
 The same is true for library and production resolution. On a ﬁ le transfer, ﬁ les 
that are born digitally (e.g. XDCAM, P2, and HDV) are best left in the ﬁ le format 
in which they were created. Taking a 25MB ﬁ le and turning it into a 100MB ﬁ le 
for archive, doesn’t make it any better, and in many cases makes it worse. Keeping 
content at the original bitrate and format preserves the integrity. 
 How many formats are really necessary? 
 To begin, consider the internal formats for library (archive), production (mez-
zanine), and proxy (browse). These are the fundamental house masters. Then, 
there are delivery formats. These are different for each platform and delivery 
network. 
 An example of this would be On-Demand delivery, which requires a higher 
bitrate, and depending on which On-Demand provider, they have a format speci-
ﬁ cation. For example, iTunes requires ProResHQ at a bitrate of 88–220Mb. 
 As far as streaming, the format depends upon the platform, so the format and 
bitrate that go to a phone, tablet, or PC will all be different. Moreover, when we 
consider mobile and what is available on broadband, the bitrate is going to be 
determined by the carrier. 
 There are a lot of format discussions and competition in the mobile market. 
While ﬂ ash has been the dominant format for some time, HTML5 and HTTPlive 
are now becoming the new standards. 
 It’s all in ﬂ ux 
 MPEG4 H.264 has been the mainstay of mobile media, and H.265, MPEG7, and 
MPEG21 are on the horizon. The primary reason that formats are in ﬂ ux is that 

Ingest/Acquisition/Capture
27
MPEGx x.xxx is a licensed technology, and the Open Source community is push-
ing other formats that are royalty-free. When it comes to choosing formats and 
compression, what looks good on a 65" TV is very different from what looks good 
on an 11" tablet, 3" phone, 1" wrist device or heads up glasses. The delivery plat-
form plays a large role in this decision. 
 Ingest, Acquisition, and Capture  
   Table 2-4   compares the different format standards, their bitrates, and the aver-
age ﬁ le size for an hour of content. As 4K becomes mainstream, it will have a 
considerable impact on production. One hour of 4K @ 3.82Gb/s is 1.72 TB/hr 
taking into account how much storage 4K uses, ﬂ ash and optical become very 
limiting. Even at MPEG2 bitrates, the ﬁ les are large and need a lot of bandwidth 
for transfers. 
Format
Name
Bitrates
File Size
Platform
MPEG2 4:2:2) 
@ MP/HL
XDCAM
25, 35, 50Mbit/s
18 30 GB/
Hr.
Production
MPEG2 4:2:2)@ 
MP/HL
DVCPRO
50, 100 Mbit/s
30- 60GB/
Hr.
Production/Library
MPEG2 4:2:2)@ 
MP/HL
ProRes 422HQ/
DNxHD
147, 220 Mbit/s
100 GB/Hr.
iTunes/Production/
Library
MPEG2 4:2:0 
@MP
DVCAM/Firewire
25 Mbit/s
15GB/Hr.
Production
JPEG2000
J2K
250Mbit/s
137.2GB/Hr.
Production
4K RAW 
4K
3.82 Gbits/s
1.72 TB/Hr.
Production/Cinema
 
MPEG4 
Blu-Ray
40Mbit/s
6GB/Hr.
DVD
MPEG4 Part 2
H.263
700K-3Mbits/s
N/A
Video Conference/
Web
MPEG4 Part 2
H. 263
700K-3Mbits/s
300MB
You Tube
MPEG4 Part 10 
H.264
700K-3Mbits/s
300MB
Web, Mobile, Flash
MPEG4 
H.264/AVCHD
100Mbits/s
16GB/Hr.
Production
HTML5-Open 
Source
H. 264
700Kbits/s-
3Mbits/s
Apple iPhone, iPad,
HTML5-Open 
Source
Web/VP8
700K-3Mbits/s
Web, Mobile
 FIGURE 2-4 

28
Ingest/Acquisition/Capture
 The table shows how big an MPEG2 ﬁ le can be for an hour’s worth of media 
based on different encoding formats and bitrates. It’s easy to see that with MPEG4 
there’s a substantial difference. One thing that is not well publicized is that actual 
ﬁ le size will vary based on the audio encoding. Each HD-SDI stream can have 
eight (8) pairs of embedded AES, plus the ancillary data channels. One example is, 
an XDCAMHD 50Mb/s that claims 30GB for an hour with uncompressed audio is 
really 62Mb/s, so the total ﬁ le size is really 37.5GB. Using JPEG2000 at 250Mbit/s 
and adding audio and data brings the true bitrate to 318Mbit/s. 
 The transition from analog to digital maintained a similar architecture of video 
and audio routing and distribution. In the analog world, there was tape-based A/V 
record and playback, serial control (RS422 and 232), and IP was in the province 
of the IT department. 
 As tape evolved to SDI, the audio is now embedded in the video, and digital sig-
nals have different requirements for routing and distribution, such as re-clocking. 
With the introduction of workstation-based editing (NLE), IP appeared as a con-
trol layer in addition to serial protocols. 
 Tape machines have evolved into servers, and IP has changed from being only 
a control layer to fully encompassing media and management. Before, there were 
separate paths and layers for audio, video, and control, with a completely sepa-
rate ecosystem for management. Now everything is fully integrated—audio, video, 
control, and data for management all travel in the same IP stream or ﬁ le. The 
stream is transported and received by a variety of applications and then parsed for 
craft, distribution, and management. 
 FIGURE 2-5A 
Analog
Analog
Video
Audio
Communications
Control RS422
Analog
SDI
Analog
Video
Audio
Communications
Control RS422
Digital
Video/Audio Embedded
Communications
Control RS422
Control IP
 FIGURE 2-5B 

Ingest/Acquisition/Capture
29
 Ingest 
 Let’s take a look at Ingest in the ﬁ le-based architecture. 
 There are different ways to encode an IP ﬁ le. Studio cameras output HD-SDI, 
4K, and—coming soon—8K and 16K. HD-SDI enters the router, production 
switcher, and/or goes directly to a production server. Once there, it is captured and 
encoded to a full-resolution ﬁ le while a proxy is being created. Both are placed in 
storage. All these processes are handled by the media manager. The higher resolu-
tion 4K, 8K, and 16K use multiple outputs at the camera and need to be recom-
piled and synchronized at the router, production switcher, and encoder. These 
higher resolution formats have different transport and handling requirements, 
and the higher bitrates impact editing and storage. 
 If there are lot of Isolated (ISO) camera shots, then a larger number of ingest 
channels are needed. How many concurrent encoding processes can one server 
handle? As the master ﬁ le at high resolution is being created how many proxy 
versions are needed and do they need to be created simultaneously? This will 
impact application servers, network trafﬁ c, and storage design considerations. 
The impact on storage will be in throughput and bandwidth, not so much actual 
storage capacity. The design consideration becomes how many concurrent read/
writes the storage architecture can support. How many can the disks handle at the 
same time before requiring additional disks (spindles) just to support the load? 
Solid state storage has the same issue. 
 Case Studies 
 Sometimes the number of concurrent operations are not based solely on the number 
of camera ISOs but on concurrent events. Here are two different case studies show-
ing the challenges and considerations when there are a number of concurrent events. 
 Case Study 1 
 A large global humanitarian organization modernized its meeting facilities. There 
are seventeen (17) meeting rooms that operate at the same time with three (3) to 
ﬁ ve (5) hour meetings in each room every day. Each meeting and therefore each 
video channel has multiple audio channels for different languages embedded. There 
are two (2) high resolution recordings of each meeting plus three (3) different 
Analog
SDI
IP & File-Based
Analog
Video
Audio
Communications
Control RS422
Digital
Video/Audio Embedded
Communications
Control RS422
Control IP
IP
Video/Audio/Control/Communications
Embedded
 FIGURE 2-5C 

30
Ingest/Acquisition/Capture
proxies for each recording, with automated and manually entered metadata to log 
the meeting events. The encoding processes start/stop are triggered automatically 
from a scheduling system sending the information as an XML ﬁ le. There are over 
100 real-time editing processes on the ﬁ les as they are being written. Separate pro-
cesses take the live content and encode it to the web and mobile media, carrying 
the language channels and inserting captioning. 
 In this example there are multiple simultaneous encoding processes creating 
multiple bitrate ﬁ les for each instance of encoding. There are multiple streams 
transported to storage where ﬁ les are being written as multiple users are using 
proxies on a timeline to create subclips and rendering (processing) new ﬁ les that 
add to the writing demand of the storage and the management demand of the 
application software. At the same time metadata is added and associated correctly 
to each ﬁ le. This is one demonstration of scale and the amount of concurrent 
processing, reading, writing, and network trafﬁ c of large ﬁ les that can present 
a considerable challenge in designing the architecture to support this business 
requirement of the organization. 
 Case Study 2 
 Most American sports leagues are centralizing their replay review process to provide 
greater integrity and accuracy to the ofﬁ ciating of the games. To accomplish this, 
American sports leagues have instituted centralized game archiving and instant 
replay review systems. They are transporting all the cameras live from every venue 
to a centralized production center, encoding them individually to ﬁ le for archive 
and production while having real-time access for replay review of an infraction. 
 One such example is having as many as nine (9) cameras from over twenty ﬁ ve 
(25) venues individually transmitted into encoders at as many as ﬁ ve (5) bitrates 
for archive, production, management, and distribution. There may be as many as 
nine (9) concurrent live games on any one (1) day. This translates into eighty-one (81) 
ingest channels simultaneously encoding and producing four hundred and ﬁ ve 
(405) ﬁ les concurrently. All while any or all of these ﬁ les are being accessed by 
production and ofﬁ cial review in real-time. Once a replay has started, another 
ﬁ le is created only for the replay segment with all nine (9) cameras in sync. This 
translates into nine (9) x ﬁ ve (5) ﬁ les being read and controlled with the same 
metadata to track plus the unique information for each ﬁ le and then writing 
forty ﬁ ve (45) separate ﬁ les, all while the master ﬁ le for each of the nine (9) cam-
eras continues to be encoded and written. 
 While all this is going on, there is a steady stream of statistics and tournament 
data automatically being ingested in addition to manual metadata being entered 
in real-time. All this metadata has to be assigned correctly to each master, proxy, 
and sub-ﬁ le (parent-child inherited metadata properties). 
 This level of ingest puts an incredible burden on all the systems. The storage 
has to perform multiple read/writes on the same ﬁ les as new ﬁ les are created while 
the master ﬁ les continue to be written. The applications that manage this move 
ﬁ les and streams, plus in separate layers perform processes that enable a user to 

Ingest/Acquisition/Capture
31
display and work with the ﬁ les. Other applications or middleware are performing 
automated processes to integrate data or trigger other processes with metadata. All 
while management applications are indexing and registering the media and meta-
data for searching and browsing. Now add in quality control with management 
of all the processes, trafﬁ c, and bandwidth. It is easy to see how the systems can be 
stressed and why the planning and design of the entire architecture is so critical. 
 Studio 
 Studio production has a different set of requirements whether it is live or recorded. 
Studio production allows for greater control and management of the processes. 
Many if not all of the encoding and format decisions have already been made. 
 When media is contributed from the ﬁ eld as SDI, it arrives in the facility and 
is encoded to a ﬁ le in the house format and bitrate. If it’s an IP stream it will be 
encoded in the ﬁ eld, streamed to the broadcast center, and captured to a ﬁ le as it’s 
received. 
 In the IP architecture, SDI video and analog/AES audio are still part of the core 
in studio production. Cameras output SDI while microphones and speakers are 
analog. Once the output of a microphone enters the audio production console, it 
becomes AES. Another possibility is when the microphone is wireless, the receiver 
encodes the analog to AES prior to connection to the production console. 
 One of the reasons that the output of cameras and live audio remain as SDI and 
AES, when compared with the transition to IP- and ﬁ le-based production for edit-
ing, graphics, distribution, is that the real-time visual transitions (i.e. crossfades, 
wipes, and keying) and live audio mixing with processing still heavily rely on SDI 
and AES technology. The industry is reluctant to adopt IP for  live  production. IP 
has not overcome the hurdle of the seamless transition between SDI sources, or 
AES audio that has to be mixed, equalized, and processed, cross fades or the type 
of on-air keying, layering, and effects that wideband—SDI and AES—production 
devices do. Some of the same challenges are present in 4K and 8K production. 
 Another major change in the evolution to IP is in the command and control sys-
tems. This has brought signiﬁ cant beneﬁ t and change to studio automation. Auto-
mation is so much more than master control program origination, however it did 
become critical in multi-channel and multi-platform program origination. Auto-
mation is also a major component in the studio production control room. There are 
different levels of studio automation, from using automation controllers that man-
age production equipment to complete “Studio in a Box” systems and everything in 
between. And all of these are managed and controlled over an IP architecture. 
 In the studio, there are robotic camera pedestals and mounts that integrate with 
virtual sets controlled by dynamic touch panel graphic devices and image displays 
that tie into the virtual set using telemetry integrated with the camera mount and 
lenses. This is all managed in the production control room. 
 The camera robotics are controlled over the IP network and using an application 
running on a server that manages and controls the camera motion with the ability 

32
Ingest/Acquisition/Capture
to memorize all the parameters of the shot that includes motion, pedestal position, 
pan, tilt, zoom, and focus. All of these can be pre-programmed and embedded in a 
script. When the script runs it automatically recalls the metadata and triggers com-
mands or it can be manually triggered by an operator using a touch panel. 
 In a fully automated control room, the system operates the switcher, mixer, 
triggers pre-programmed effects, triggers and inserts graphics, and plays program 
clips. The graphics and clips are managed with metadata that has embedded con-
trol information. In this scenario, all the command and control data is over IP, 
with the automation system as an application on a server based on a database. The 
graphics, animations, and packaged program clips are sent to the play-out server 
that runs under the automation system for production. The integrated produc-
tion program is streamed to the delivery server for capture to ﬁ le as well as sent as 
a live stream directly to distribution and will also be converted to many different 
formats for each of the distribution platforms. 
 What is the “Studio in a Box”? 
 The “Studio in a Box” concept is a server-based system that has all the capability of 
a production studio integrated in a suite of software applications—virtual audio 
mixers, video switchers, still store, digital effects, clip players, and computer graph-
ics. Most of these offer a physical control surface that while it is still operating 
a software application, the control surface has the look, the feel, and acts like a 
dedicated audio mixer and video switcher. These “Studio in a Box” systems are all 
in the IP infrastructure, and while they are all server-based, they are technically 
hybrid since camera and audio mixing is handled as AES/SDI. 
 In a “Studio in a Box,” the production can be highly automated. Thus enabling 
the director/producer to build the script with a shot list for the cameras includ-
ing the positioning for robotics, and create a clip play list that incorporates the 
graphics, integrates program elements like transitions, switches between the live 
cameras, and creates a complete studio production program with very few peo-
ple. Today’s studio operations include robotic camera operations based on server 
applications, encoder control and management, metadata tagging and logging. 
 The director/producer can create templates that will allow for pre-programmed 
production scenarios. 
 There are also downsides to a fully automated “Studio in a Box,” such as the 
fact that the on-air production scenario cannot change dynamically. If an on-air 
presenter speaks out of turn or is out of place, they are off camera and potentially 
off mic. The cameras and microphones track the script and blocking. 
 This changes the design of the control room and studio. Going back to a studio 
that’s not in a box. The control room and studio are still more efﬁ cient to manage 
and operate. The control room monitoring uses multi-viewers that are ﬂ exible and 
integrated with the router. These multi-viewer displays include source identiﬁ cation, 
clock, countdown timers, audio levels, and tally that are all fully integrated using 

Ingest/Acquisition/Capture
33
metadata for onscreen information. There is a lot more information available to 
them and it’s all easily accessible. Production switchers now include sophisticated 
multi-channel visual effects, store and play still images, and have the ability to play 
animations and program clips. The production switcher is integrated with the router 
and reads the router table for input labeling using metadata. The audio console con-
nects to a server that has all the inputs and outputs on a frame. There can be many 
more inputs and outputs that the console can support in any one session, however it 
can also function like a router or interface to core router and populate input chan-
nels based on a speciﬁ c production requirement. The operators can store a show’s 
proﬁ le on a removable drive so as the studio is used for different productions, they 
can easily restore the conﬁ guration for any show. All of this is controlled over net-
worked devices that communicate using the command and control metadata. 
 Automation and Metadata 
 Within the IP architecture and production workﬂ ow, ﬁ les need to be accessible 
to various applications and processes and possibly need to move between stor-
age locations. All of these processes are applications on servers and where the ﬁ le 
may never move from its location for an application to perform a process or have 
a user access it, there are also processes that require the ﬁ le to transfer between 
application servers and storage locations. There are different layers of automation 
to manage these processes and handle the ﬁ le movement. 
 Metadata, which is discussed in detail in a later chapter, plays a crucial role in 
all aspects of the IP- and ﬁ le-based architecture. In studio production, metadata is 
the control layer integrating all the devices, it triggers the automation systems, it 
manages the Studio in Box, and provides the tracking information for the camera 
robotics and virtual sets telemetry. The control or structural metadata from the 
automation system will be associated with the stream or ﬁ le and includes all the 
content elements, control activities (e.g. camera switching), as well as the admin-
istrative metadata, usage rights, permission, tracking, and descriptive metadata. 
 Craft Production 
 Craft production or editing no longer has to wait for the recording or “ingest” 
of a production to complete. In the ﬁ le-based environment, as the master ﬁ le is 
being captured and ingested craft editing can begin. All the systems are networked 
together and can access the storage. Editing can use a proxy and timeline reference 
that does not require the ﬁ le to be complete and fully rendered. As the editor starts 
working, they may save their work in progress in a separate work area in the stor-
age environment. Sometimes this is completely separate storage, however, it is still 
connected to the core storage architecture. There are areas sometimes referred to 
as “scratch bins” and these store interim ﬁ les before the full program is completed. 
This enables an editor to move between systems and access their work. The edit 

34
Ingest/Acquisition/Capture
systems can also access content from the media library and archive using browsing 
tools. This is critical in news and sports. While a live event is still in progress and 
being recorded, a news or sports editor can be creating a story or trailer clip and 
sending it to air while the live event continues. 
 Where graphic production is not typically real-time to an event, the graphics 
team still can access any content as it is being ingested. Additionally the graphic 
workstations can access the media library for any content and then place the ﬁ n-
ished product as ﬁ les for inclusion in a production or as a clip for play-out on the 
play-out storage. One of the imperative changes in workﬂ ow is that each time a 
ﬁ le is created or edited new metadata needs to be added. 
 Removable Media 
 Removable media was ﬁ rst used to refer to analog tape and later to digital tape. 
Remote production still records to digital tape, but now also records to optical 
disk, ﬂ ash, solid state storage, camera attached hard drives, and portable digital 
video recorders (DVRs). These portable DVRs are either hard drives or ﬂ ash, 
sometimes with onboard controls or controlled via a computer interface. 
 Removable media is the primary method of capturing in the ﬁ eld. While ENG/
SNG typically have had attached storage onboard the camera, sports and news 
remotes that use big production vehicles that have large servers and vast amounts 
of storage. Some have removable hard drives that once the event is over are put 
into a suitcase and hand carried back to the broadcast center.  Therefore, the volume 
or capacity of the removable media becomes a critical decision.  In making this deci-
sion about which is the best type of removable media to use for a speciﬁ c produc-
tion there are several considerations: 
 
 What is the availability of a speciﬁ c removable media type? Can it be found in 
the ﬁ eld or is it necessary to bring enough? 
 
 Can decisions be made in the ﬁ eld that would allow content to be deleted and 
disk space recovered? 
 
 The ease of use—is it hot swappable? Does anything need to be formatted or reset? 
 
 How transportable is it? Will it ﬁ t in a pocket or backpack, or does it need a 
hardened suitcase? 
 
 Capacity: how much content can it capture? 
 
 How much additional storage do I need to bring with me? 
 
 Will it immediately be transferred to other storage? 
 
 What is the cost of the media per hour of content? 
 When it comes to removable media, reliability and durability are big concerns. 
When recordable media was tape-based, there were issues of physical damage, 
humidity, delamination, tearing, or getting caught in the machine. While DLT 
or LTO tapes are still susceptible to the same issues, they are typically used in 
a controlled environment. Optical disks are pretty resilient, however they can’t 

Ingest/Acquisition/Capture
35
withstand being run over by a car or getting scratched, but other than that appear 
to be sturdy. Hard disks have moving parts and need an interface (USB, IEEE1394, 
Thunderbolt, Ethernet, etc.) and since hard disks are still magnetic media, it is 
important to consider how they will be transported and handled. Will these disks 
remain viable indeﬁ nitely? This will likely require a migration strategy (see Storage 
and Media Management). 
 The introduction of Solid State Drives (SSD) as a replacement for hard disks 
solves some of these issues, however, present some of their own. There are con-
cerns about their durability. How long will they last? What’s their sensitivity? Are 
they fragile? Solid State Drives require specialized card readers, and these are fre-
quently ungraded and changing. The continuing evolution of these technologies 
creates compatibility challenges, so it is reasonable to question their backwards 
compatibility. There are many considerations to be taken into account for even 
the simplest of storage decisions. 
 Streaming Contribution 
 In addition to capturing content to ﬁ les and transporting the media back to the 
broadcast center, the output of a camera or production switcher is SDI, UltraHD, 
4K, and 8K (multiple HD-SDI outputs) that is transported as a live stream directly 
to the broadcast center where it can be integrated for delivery and distribution while 
being ingested and captured to a ﬁ le for media management and archive. Contribu-
tion from remotes back to the studio has evolved from a dedicated path satellite, 
ﬁ ber, or microwave feed. There are new mesh or fabric networks using Fast Ethernet, 
Synchronous Optical Networking (SONET), and Multiprotocol Label Switching 
(MPLS) that provide private network high bandwidth transport enabling full reso-
lution (uncompressed) or compressed content to be streamed. Many of the broad-
cast occasional service interconnect providers (Encompass, The Switch, Vyvx, etc.) 
are providing new tiers of managed services over these new mesh networks. Where 
in the past their services provided transport of a single SDI signal per ﬁ ber, they are 
using multiplexing technology that can send multiple signals on fewer ﬁ bers. There 
are many other technologies used for contribution such as open Internet, VSAT, 
BeGan Sat Phone, bonded 4G modems, cellphone, smartphones, and Skype. While 
these tend to transmit at lower bitrates and lower quality, receiving content from 
them has become more than acceptable if the story is interesting enough or there is 
no other way of getting the remote feed back to the broadcast center. 
 The open Internet presents a number of issues, there is no guarantee of consis-
tent bandwidth from the origination point to the receive location; the second issue 
is security; many countries have policies that block moving video over the Internet 
and Internet transmission typically has considerable network latency. 
 Another alternative available for contribution is cloud services, which is ﬁ nding 
its way into ﬁ le and stream delivery for media. A cloud provider relies on private 
network internally and either private network or open Internet for the ﬁ rst and 
last mile. Some cloud services are used for simple ﬁ le transport, however there are 

36
Ingest/Acquisition/Capture
some service providers that provide optimized bandwidth products in the cloud 
that improve the speed of transfer. Internet-based cloud services have the same 
challenges of bandwidth limitations as direct transmission over the open Internet. 
If the cloud service uses private networks like Fast Ethernet or MPLS then it is 
more reliable and more secure. While the cloud is not completely private, it is also 
not the fully open Internet. 
 The cloud is more practical for ﬁ le transfers than live streams. There are a few 
services that are starting to offer live delivery in the cloud including encoding. This 
becomes more distribution than contribution. There are also production services 
that are cloud based (this will be covered in a later chapter). Using the cloud for live 
streaming will have a few drawbacks. Bandwidth is a major issue, and if it is over 
open Internet then there are network latency issues that may impact delivery. If the 
content being delivered is time sensitive, i.e. live sports, then latency is a problem. 
 The cloud is still dependent on the famous “ last mile ,” meaning that reasonable 
bandwidth in the ﬁ eld is required to transfer or stream the captured media in a timely 
fashion. Remember bitrates! The amount of bandwidth determines the speed of trans-
fer and the size of the ﬁ le or the bitrate of the stream will determine the deliver time. 
 In remote production and live contribution there have been changes, instead 
of using the remote truck as a production control room, the cameras and micro-
phones are encoded directly to IP, multiplexed, and sent to the broadcast center. 
There, they are decoded and brought into a production control room, which does 
the switching and mixing. Communications and return monitoring is sent back 
over IP to the venue. In this scenario, IP enables multi-camera production to be 
controlled from the broadcast center. The broadcast center now uses its own stu-
dio control room without needing a production vehicle in the ﬁ eld. It is easy to see 
how latency can affect production in this scenario. 
 Case Study 
 Beginning in 1996 for the Atlanta Olympics, NBC made an interesting decision. 
Instead of building an extensive and expensive master control and commercial 
integration facility on site, they had a signiﬁ cant amount of ﬁ ber connectivity 
established from Atlanta to their Rockefeller Center New York City broadcast cen-
ter. They built their International Broadcast Center to handle production locally, a 
high percentage of editing and graphics were produced in NY and then transferred 
to Atlanta for insertion to production. Then the fully integrated program was sent 
back to NY for commercial integration and broadcast. 
 In each subsequent Olympics, they added more capability, over time they were 
able to change the workﬂ ow and production on site by using high bandwidth con-
nections and transferring content to their broadcast and production facilities in 
the US. Using their media management infrastructure, all their production teams 
were able to access the ﬁ les for production and distribution. Where the bandwidth 
allows, the camera feeds and audio are sent to their US production facilities and 
the program is switched and produced there. 

 Setting up new workﬂ ows and processes is the next piece of the puzzle, and this 
chapter looks at these changes. The changes in technology in the move to an IP 
architecture are not the only ones and it requires many other considerations. 
 This chapter focuses on the new and tighter integration between business and 
production operations, which includes workﬂ ows and processes within the busi-
ness organization. One of the biggest challenges in the transition to IP- and ﬁ le-
based technology is change. The adoption of new technologies that replace long 
standing technology solutions coupled with the resultant workﬂ ow and process 
change is intimidating. Many manual processes are now performed by automa-
tion. The processes and workﬂ ows that do require manual support are different. 
 Adding to that is the introduction of a new type of governance. Governance is 
a principle that has been in the IP world for a long time. While the media industry 
does have rules and policies, it was not structured and characterized as governance. 
Governance is the rules and policies that are critical to the smooth functioning of 
the business. All of these elements are critical when planning or evolving to the 
IP- and ﬁ le-based architecture and must be tightly integrated. In the IP ecosys-
tem, governance is the business rules and policies that are programmed as part of 
the conﬁ guration of the applications that manage and control the movement of 
media. 
 Which databases can be integrated? 
 
 What ﬁ elds are protected and not accessible by a group of users, another 
application or system? 
 
 Do these databases need middleware to communicate with each other? 
 three 

38
Workﬂ ow and Process
 These are only some of the decisions to be made before any application conﬁ gura-
tion can be done. 
 Workﬂ ows and Business Process 
 The word  workﬂ ow  is much used and abused in the broadcast industry, where it 
is used to describe almost everything. The deﬁ nition of workﬂ ow is “a sequence 
of connected steps”: 
 
  Work  is deﬁ ned as a depiction of a sequence of operations by a person or a 
group. 
 
  Flow  refers to content, a document, media, or a product being transferred 
from one step to another. 
 When applying it to handling IP- and ﬁ le-based media,  workﬂ ow  is the process of 
an individual, a group, a software application, or a dedicated device that is doing 
something with content in sequential steps as it moves through a series of pro-
cesses within the ecosystem. These operations are handled by both automation 
systems and human interaction. 
 The core to most media workﬂ ows can be quantiﬁ ed by three (3) things: 
 People:   These are the champions, sponsors, stakeholders, and users that rely on 
the workﬂ ow to do their job or manage the business. 
 Process: This is the stewardship that guides and supports the successful execution 
of the workﬂ ow. 
 Technology: These are the devices, applications, and systems which facili-
tate and enable the workﬂ ow. Each of these is a critical element when 
changing the workﬂ ow from tape-based to ﬁ le-based, from manual to 
automated, and the conversion to an IP architecture. 
 Integration between Broadcast and Enterprise 
 In the IP ecosystem, production and business processes, operations, networks, and 
systems are more tightly integrated and use automated processes to manage the 
media workﬂ ow. Business systems such as legal, ﬁ nance, marketing, rights man-
agement, and business intelligence all create and need access to metadata. 
 Metadata is the foundation of the IP architecture, and each business depart-
ment contributes metadata that controls media movement and manages access. 
All users within the organization will search and browse the library for different 
reasons and operations. There are a number of touch points between the business 
enterprise and the broadcast network for managing media trafﬁ c. The business 
units need to establish working policies that allow the permissible data to move 

Workﬂ ow and Process
39
between systems across networks, while the IT and broadcast IT groups need to 
work closely together to create seamless workﬂ ows for the users. 
 In the IP- and ﬁ le-based ecosystem it is harder to segregate how the technology 
and workﬂ ows are used throughout the entire ecosystem and architecture. There 
is such tight integration that in describing the overall technology architecture and 
design requirements, it is difﬁ cult not to constantly refer to where workﬂ ow ﬁ ts. 
 Not only do the technology systems integrate differently in the IP architecture, the 
workﬂ ows integrate differently as well. And how they integrate is very important.   Fig-
ure 3-1   shows that media management is one of the core processes in the IP workﬂ ow. 
 FIGURE 3-1 
   This demonstrates that while each of the business and production processes are 
interdependent, they all integrate to the media management system. There is a mix 
of media and business departments that each can add permissions, rights control, 
management metadata along with the descriptive and automation control meta-
data. More importantly, the IP ecosystem brings new operational and production 
processes; and new workﬂ ows. 

40
Workﬂ ow and Process
 Media Management  – This is the heart of the environment and ecosystem. It 
encompasses all the technologies, the complete workﬂ ow and set of business pro-
cesses that manage the movement of media from the time of its creation through-
out its entire lifecycle. Each cog in this wheel are the workﬂ ows and processes that 
represent the core groups that are accessed directly or indirectly by a user or are 
involved in media management. 
 Creative  – This is the beginning of the media lifecycle. All other processes stem 
from the creative process. The descriptive and administrative information devel-
oped in the creative process is the ﬁ rst set of metadata entered into the media man-
agement system and could be entered prior to any media being actually produced. 
This may include the script, technical resources, locations, contracts, usage rights, 
and other early production information. 
 Acquisition  – Once production begins, titles, location information, media log-
ging details, production notes, contribution information, and technical details are 
entered. The acquisition process includes the ingest and encoding process and, of 
course, requires media handling. During the acquisition process additional control 
and management metadata is entered, as well as the application of retention rules 
set by the business groups. 
 Craft  – These are the processes that transform the content into the ﬁ nished 
asset. This can be accomplished in a studio control room, with editing and/or 
graphics operations. This step encompasses all of the different craft operations 
that provide the ﬁ nishing details before the asset moves to distribution. In the 
craft process, any content that’s created or acquired and will be included as an 
element to the program or separate asset and need metadata to become associ-
ated with main asset under a set of ‘parent–child’ policies in the media manage-
ment architecture. The element (child) inherits the properties of the main asset 
(parent) while retaining its own identity. In the IP environment each asset can be 
both a parent and a child. 
 Library  – In the IP architecture and workﬂ ow, the digital library serves a similar 
and more expansive role than the tape librarian or archivist. This is the command 
and control center for metadata and media management. The librarian oversees 
ingest and ﬁ le movement processes, ensuring the integrity of the metadata. Man-
aging the media archive is another of the workﬂ ows the library manages. In the 
ﬁ le-based ecosystem, media in the library is available instantly for production, 
however, once it’s archived the retention policies take effect. 
 Business Operations  – The legal, ﬁ nance, rights, marketing, and business intel-
ligence departments are the primary business groups that are directly connected 
to the media lifecycle and ecosystem. These departments contribute and extract 
metadata for contracts, sales, marketing, rights management, clearance, and mar-
ket analysis. 
 Rights Management  – Rights management has a larger purview than just usage 
and copyright protection. User access is also  an internal  management issue (e.g. 
setting permissions to access content). It’s not uncommon for certain assets to 
need encryption or protection from speciﬁ c users for production or distribution 

Workﬂ ow and Process
41
reasons. Rights management includes content expiration rules and consumer 
access control. Rights management metadata controls what delivery platforms 
have access to the content. 
 Technical Operations  – Technical operations include engineering and encom-
pass all of the ﬁ le handling and movement throughout all the systems and pro-
cesses of the IP infrastructure. While most of the content handling operations are 
automated, there still needs to be management and quality control. With all the 
different delivery platforms, technical operations include insuring that the vari-
ous complements of metadata as required by numerous distribution networks 
are included and correct. Quality control is part of technical operations and is 
responsible for assuring the integrity of the media from the moment it is ingested 
to media movement, ﬁ le management, and play-out. All of the program origina-
tion center functions are typically considered part of technical operations (e.g. 
error logging and discrepancy reporting). 
 Distribution –  Distribution ﬁ rst changed from single channel to multi-channel 
origination. The next change was to add in multi-platform delivery and multi-
channel origination. Each distribution platform has a different format, requires 
different metadata, and has different rights and protection rules. Program origina-
tion is typically fully automated. 
 The workﬂ ows and business processes come down to the relationship between 
people, process, and technology as it applies to all the stakeholders and to the core 
processes that are integrated within the ﬁ le-based and IP technology architecture. 
 FIGURE 3-2 
 People  – These are the stakeholders and executives that want access to the 
media by using search and browse functions for presentations, creative ideas, 

42
Workﬂ ow and Process
and promotion. The creative teams begin the media lifecycle by creating the asset 
through the production and craft processes. The operation units control and man-
age the media across the entire ecosystem, while the business units establish the 
governance and monetization policies. The overall responsibilities of engineering 
include ensuring that all systems are operating properly at all times, protecting 
operations from any interruptions in services and providing quality control for 
the ﬁ les and streams as they move throughout the infrastructure. 
 Process  – The area of process integration must cater to a new alignment of the 
primary workﬂ ows and processes that support business, technical, and produc-
tion operations. Governance is the rules and policies that manage, control, and 
protect the asset though its entire lifecycle. One of the critical processes is security, 
protecting the asset, and controlling access. This protects the value of the asset and 
manages the integrity of the systems within the infrastructure. The management 
process handles ﬁ le movement, metadata, library, and distribution. Craft produc-
tion is no longer an autonomous operation from the rest of the media lifecycle. 
Content is registered and indexed before being accessible by craft production. The 
craft process includes updating metadata and authorizing the content to move 
within the media management workﬂ ow to library and distribution. Quality con-
trol is one of the most critical processes. This monitors and insures the integrity 
of the ﬁ le or stream for library and distribution. With the number of formats and 
ﬁ le types, maintaining quality control is key to monetizing the asset. 
 Technology –  This is a good place to remember that this is really all about 
technology; Applications, Servers, Network, and Storage. This is looking at the 
technologies and all the other requirements that need consideration in the design 
and planning of a facility. The technology is the support systems that enable the 
new processes, workﬂ ows platforms to exist. There are new technologies for pro-
duction, MAM is a common place acronym and applications and network are 
part of our normal lexicon. The IP- and ﬁ le-based media technologies are the core 
foundation to the new lifecycle of media. 
 Business Integration 
 Business integration is the newest component in the broadcast and production 
workﬂ ow. The IP ecosystem opens up a new opportunity to integrate the business 
technologies and workﬂ ows responsible for monetizing the assets with the actual 
media workﬂ ows. These groups previously worked in operational and technology 
silos with each system isolated from the others and interconnectivity limited if at all. 
 Business operations is comprised of a group of departments that, in the IP 
architecture, is integrated across the network and allows applications and data-
bases to communicate and share data governed with rules and policies to control 
access. Let’s take a closer look to see where the integration occurs and which design 
considerations come into play on the application and network side required to 
make this work. 

Workﬂ ow and Process
43
 The legal department can interface directly with the entire media process, 
beginning at the creative process where production contracts are created and 
managed and then extending all the way to delivery, where distribution contracts 
with restrictions and rights management are imperative. The legal department 
needs access to the media management system to input management metadata 
and export metadata into the applications that support the legal processes. 
 The ﬁ nance group retrieves records from the media management system in 
order to track costs and resource utilization. One of the roles ﬁ nance performs 
is to assign monetary value and track resource costs associated with media cre-
ation and production. The ﬁ nance applications provide the monetization rules 
and policies to the media manager as metadata. This metadata will stay with the 
content as it travels to each of the delivery platforms. When a consumer attempts 
to access the content by either paid subscription or conditional access, this is con-
trolled by user authentication and validation rules and policies that are managed 
by the ﬁ nance department. 
 The marketing group is another major stakeholder. From the outset of produc-
tion, marketing has data that tracks and stays with the content as it travels through 
 FIGURE 3-3 

44
Workﬂ ow and Process
its lifecycle. Marketing creates promotional products that need to be assigned as 
“children” of the main asset. This is done in the media manager by adding associ-
ated metadata. The metadata that travels with the asset to the different delivery 
platforms and independent program guide services (data aggregators) is also cre-
ated by the marketing group. 
 Marketing manages the tracking data embedded in the media (e.g. watermarks 
and encryption keys). The entire value chain of media in the multi-platform world 
broadens out to include metadata for clearance, subscription access, tracking, plus 
provides the necessary information to Electronic Program Guides (EPG) and Pro-
gram and System Information Protocol (PSIP) (for Over-the-Air). One of the 
most important applications of metadata is when it is sent to the delivery plat-
forms to enable indexing and search by all the different recommendation engines 
and devices in media enabled devices, players, and DVR/PVRs. 
 Bringing this back to the design of the IP architecture, the marketing applications 
need to place this metadata into the media management system and have it travel 
between applications and across networks as an element of the asset’s metadata. The 
accessibility of media to the consumer is fully dependent on the metadata—as is the 
monetization of the asset. 
 Business Intelligence 
 Business Intelligence (BI) grew out of the requirement and ability to gather data 
from all types of online services and the different social networks. It is the process 
of retrieving data from various content distribution platforms and bringing it 
back into the business for analysis. There are specialized application and data-
base tools that can analyze the data to create a set of recommendations that can 
be used for critical business decisions. Business Intelligence has moved into the 
media industry and become a valuable tool for marketing and production. There 
is a considerable volume of data that can be gathered or mined from content users 
across all the platforms. 
 Business Intelligence is deﬁ ned as the process of seeking information and knowledge 
for the purpose of making quality decisions that are in line with organizational and 
business goals. It is having factual information, analytics, and statistics to do predictive 
modeling based on historical facts that are available immediately (in seconds instead of 
hours, days or weeks). This is the metadata mined by all the delivery platforms and 
returned to the broadcaster for analysis. It can be an enormous amount of data, in 
which case it is referred to as Big Data. Business Intelligence uses this data to gain power-
ful insights, obtain knowledge, and use it to predict the patterns of the end user. 
 Business Intelligence is all about the quality of decisions made within the 
organization AND can be a primary competitive differentiator. It is designed 
to support the decision-making process and facilitate informed actions to meet 
business goals. Leveraging analytics and the knowledge gained from it makes 
business intelligence an essential new weapon for competitive media companies. 

Workﬂ ow and Process
45
 Business Intelligence also refers to the software applications and techniques 
that are used to identify, extract, and analyze data. This is data such as consumer 
information, search results, and appearances in recommendation engines. These 
statistics and data sets are a result of the metadata that is sent to the Electronic 
Program Guide (EPG) aggregators, other program guide service providers, and by 
tracking the interaction with end users. 
 Business integration has implications across the workﬂ ow and business pro-
cesses and greatly depends on technology. Business Intelligence relies on large 
databases with algorithms that analyze the data and is used to develop rules and 
policies for other data sets that manage the content. Database integration is at the 
core of media management. 
 Operations Workﬂ ows 
 The Production and Technical Operations groups need to adopt new workﬂ ows 
and processes driven by the integration of the IP technologies and infrastructure. 
These are some of the groups that need to adapt to the IP- and ﬁ le-based technolo-
gies. This will have a large impact on how they achieve their program creation and 
origination requirements. 
 In the production workﬂ ow, as the media is captured in ﬁ le form or encoded to 
a stream, there needs to be a process to assure that the media is properly indexed, 
logged, and tagged. Some of this is handled by automated processes moving data 
between systems while some still requires a person watching and entering meta-
data as a production or event is taking place. 
 During live sports, there is a considerable amount of data that is created prior 
to an event. The team rosters or list of competitors, their numbers, statistics about 
them. Once the tournament starts both automated and manual metatags are 
entered. Car racing has many sensors in the cars and around the track. In other 
sports where embedding sensors on a person or in their clothing is impractical, a 
logging person needs to watch the event with the ability to enter tags with more 
than a timestamp. The logging systems use “Hot-Keys,” touch panels with cus-
tom interface speciﬁ c to that sport or dedicated controllers. Each of these inter-
faces have preprogrammed buttons with embedded metadata so when the logger 
touches the button or key the tag has the type of the event, possibly the player 
number, court position, and other data. This allows editing systems to automati-
cally retrieve clips and elements for production. 
 Another of the efﬁ ciencies in ﬁ le-based production is that during the encoding 
process while the media is being indexed, logged, and tagged, craft editing begins 
simultaneously using metadata to assemble clips. This provides a huge beneﬁ t 
to live production such as sports and news. Within the craft production process, 
producers, directors, or non-technical managers can browse the digital library and 
select content. The identiﬁ ed elements can be assembled as a list or even into an 
edit decision list (EDL) and sent as direction to an editor to conform or include 

46
Workﬂ ow and Process
in a ﬁ nished program. These can all be concurrent processes; however, each time a 
different user accesses the content for inclusion, there should be an entry made in 
the metadata record. There are new rules and policies to govern usage. Governance 
is a larger subject that is covered in greater detail with media management. 
 In the production workﬂ ow, the ingest or editing process needs to allow time 
for rendering to ﬁ nish a program for delivery and archive. During ingest, render-
ing is the process that takes the temporary ﬁ le created and ﬁ nalizes it for use in 
production or delivery. In the editing process, once all the decisions are made 
using timeline-based editing, rendering is the process that assembles the elements 
into a single ﬁ nished ﬁ le for delivery and archive. When rendering during an edit 
session, the original ﬁ le is untouched and a new ﬁ le is created with its own meta-
data, also inheriting metadata from the original content. Rendering is similar to 
what was called a conform in linear editing that took an Edit Decision List and 
created the ﬁ nal program for distribution. Rendering is necessary with ﬁ le-based 
systems, because for editing they use proxy or mezzanine resolutions and then 
need to render the ﬁ nished program drawing the elements from the original high-
resolution content into a new ﬁ le which is the ﬁ nished program. For ingest, typi-
cally a temporary ﬁ le is created until the ingest is complete, and then the ﬁ le is 
rendered to enter the media management system. The rendering process can be 
both slower and faster than real-time based on the parameters of the ﬁ nished ﬁ le. 
 There are many changes in the production workﬂ ows. Post-production is inte-
grated on the media network to local and shared storage that is managed by the library. 
There are changes in the edit process as to how programs are ﬁ nished. The editing 
workstations are craft devices and there are dedicated servers to render the ﬁ nal pro-
gram without burdening the edit workstation, thereby freeing it to continue working. 
The format differences for content on the various platforms change the editing and 
post-production process. Editing for web, tablet, and phone is different from a large 
screen display. Each delivery platform has its own production requirements. 
 Technical operations is all of the processes and workﬂ ows that manage media 
movement throughout the infrastructure from ingest to distribution and is 
responsible for quality assurance. Technical operations include transmission, 
master control, trafﬁ c, library, archive, and engineering. It is responsible for the 
integrity of signal quality and the quality of all content in archive. The transi-
tion to IP- and ﬁ le-based technologies has had a signiﬁ cant impact on technical 
operations and to most of these processes. One major change is that many of the 
processes are handled by automation. 
 In the IP ecosystem and value chain, the digital library functions as the pri-
mary media manager, this system owns the governance and retention policies that 
move media between different storage locations and controls access. The library 
is responsible for conﬁ rming the integrity of the metadata, tracking missing data, 
and validating it so that the asset has all the appropriate metadata it needs for 
archiving, distribution, and browsing. Technical operations handles format con-
version for both ingest and delivery. And, of course, quality control is key. Quality 
control has different processes depending on if the media is a ﬁ le or a stream. 

Workﬂ ow and Process
47
 Technical operations has an expanded dimension of workﬂ ow in the IP eco-
system, providing users with remote access. This encompasses a variety of user 
groups; they can be managers, production personnel, editors, media loggers, exec-
utives, and many others. The design of the IP infrastructure has to account for 
these users, not only in the physical and technical design but in the operational 
workﬂ ow design as well. Looking at the different workﬂ ows across production, 
technical, and business operations involved in media handling and management, 
many of these can be done using remote access. 
 Network management is core to the IP infrastructure and there are workﬂ ows 
associated with the conﬁ guration of the network to support technical operations. 
   Figure 3-4   shows how the main network segments (enterprise LAN and broadcast 
LAN) are segregated yet connected. A ﬁ rewall is used to protect each segment and 
the ﬁ rewall enables and controls protected Internet access to both segments. On 
the enterprise is legal, ﬁ nance, marketing accessing application servers, databases, 
and ﬁ le storage. The broadcast LAN has media, command and control, media 
management, communications, production accessing application servers, data-
bases, and storage. The systems on the enterprise LAN and broadcast LAN need 
controlled access to each other. 
 FIGURE 3-4 

48
Workﬂ ow and Process
 The search and browse engines of the media manager make media and meta-
data easily available using remote access and this enables a user to assemble an 
EDL and leave only the rendering to be processed in the main infrastructure by 
automation. In this scenario, an edit can be initiated and mostly executed remotely. 
The media can be easily accessed using search functionality for all operations via 
the browse mode. 
 Media management is based on rules set in all IP-based systems. This too can 
be managed through a remote interface. Setting up this access is a joint operation 
between the IT and Broadcast Engineering groups, and is based on access rules 
and policies under the overall governance schema. 
 Automation 
 In the IP- and ﬁ le-based ecosystem, automation is used to manage and move the 
media and metadata as it moves between servers, applications, and processes. 
 Introducing this level of automation changes the workﬂ ows at each production 
and technical operational process. Automation is more than just play-out; auto-
mation manages the movement of media throughout the infrastructure. Systems 
monitor other systems waiting for media to arrive. Once the media appears in a 
folder or as a stream, automated processes recognize its arrival and trigger the next 
event. This could be an encode or transcode, or it could alert a producer or editor 
that there is media available for editing. The digital library is alerted that new media 
is in the system and to check metadata. The quality control tools monitor incoming 
content locations and begin testing the integrity of the media and metadata. 
 Moving media requires rules and policies and systems to manage them. In the 
IT enterprise industry these applications are called Business Process Managers and 
they control the movement of ﬁ les, the integration of databases, and the interac-
tion between applications sharing data. In the broadcast IP infrastructure these sys-
tems are called conductors or orchestration applications. One of the key differences 
between enterprise and broadcast ﬁ le management is that in the enterprise docu-
ments and data do not move throughout different processes and applications. A 
word document stays with word processing, a spreadsheet is not converted back and 
forth between applications and data sets that may be shared by multiple applications 
don’t change in structure. In the broadcast architecture, media and metadata moves 
and changes as it travels between different applications and processes. There are ﬁ le 
movers and data directors. This is where orchestration becomes a critical piece of the 
puzzle. As the media enters the system, it needs guidance to go from acquisition to 
ingest, get encoded, get registered to the management system, possibly transcoded, 
and converted before being moved to the appropriate storage location. 
 Orchestration is automation on steroids. Each system needs to know what to 
do, when to do it, and what to do with the ﬁ le after it is done with it. Systems have 
proﬁ les for performing their tasks. Different ﬁ les have different priorities and need 
to be tracked as they move through the entire media management system out to 

Workﬂ ow and Process
49
distribution. What if there is a problem with the ﬁ le or a process, something needs 
to be aware that there is an error and alert an operator or engineer. What about 
resolving conﬂ icts when there are a number of concurrent processes all happening 
requiring the same resources? 
 As the media moves through its lifecycle, trafﬁ c information is automatically 
pushed into the system, directing the media to all the play-out destinations with 
the associated metadata. The trafﬁ c system needs to manage and produce formats 
and schedules for each platform and distribution channel. 
 We took a brief look at studio automation and some of the changes driven by 
IP-based technologies. Now, let’s look a little more closely at these technologies and 
how automation changes the studio production workﬂ ow. Both the studio and studio 
control room have changed based on IP technology. In the studio, camera robotic 
control is all IP, the pedestal and pan/tilt head receive command and control data 
of the IP network. LED lighting instruments have integrated dimmers receiving 
command and control from the server-based dimmer control over the IP network. 
The camera lens sends and receives data for the robotic telemetry. The studio set 
may have a large image display that is server controlled showing video and graph-
ics or the studio can be all green screen and use a virtual set controlled by automa-
tion running on a server. The virtual set allows the real camera to move in the 3D 
space, while the graphic image from the virtual set is rendered in real-time from 
the same perspective. This virtual scene adapts to all the camera settings (i.e. pan, 
tilt, zoom, track) and it supports multi-camera production. 
 Most if not all of production devices are servers with their control surfaces con-
nected over an Ethernet network. Most if not all of the devices in the studio and 
studio control room have an IP management and control layer. Setting up for a 
production involves accessing the server to conﬁ gure the production system. The 
entire control room can have a “soft conﬁ guration” which essentially means the 
operator can recall a “proﬁ le” from a database or ﬂ ash drive at the control surface 
to set up the control room for the speciﬁ c production. The production system may 
use a server-based browser or application to recall the conﬁ guration. 
 The introduction of automated systems into studio production has deﬁ ned 
new workﬂ ows. There can be a robotic camera operator or the TD (technical 
director) can use a touch panel interface pre-programmed with camera positions 
and moves. In live studio production like news, the camera position and shots are 
pre-programmed based on the scripts for the program. This includes recalling and 
playing clips, inserting graphics and visual effects, and running the teleprompter, 
all from a uniﬁ ed control screen that is either a touch panel or by using a keyboard 
and mouse. It’s not uncommon to have a single operator in a control room over-
seeing an automation system running the show. 
 There are new workﬂ ows in the post-production area. The introduction of 
Non Linear Editing (NLE) technology completely changed the workﬂ ow of post-
production. Now, with all these systems integrated over a network with shared 
storage, ﬁ le movement and media management play a larger role from when the 
editing system was a closed architecture. The producer and editor use the media 

50
Workﬂ ow and Process
management tools to locate content, and content is instantly accessible during the 
ingest process. From a workﬂ ow perspective, using the media manager a producer 
can create a storyboard complete with an EDL. Depending on the complexity of 
the ﬁ nished program, automated processes can use the EDL and assemble a ﬁ n-
ished program. Or an editor can recall it and ﬁ nish the program adding new ele-
ments and transitions while at the same time entering more metadata. Adding 
metadata makes ﬁ nding and using the content again easier. 
 Production workﬂ ows and processes are continuing to evolve and the next gen-
eration of these processes will be cloud based. There are a number of editing and 
graphic systems being offered as cloud services and how these will integrate into 
the post-production workﬂ ow and processes is also evolving. Does the content 
live in the cloud or are the proxies in the cloud making the content accessible from 
anywhere and the EDL is created in the cloud while the full resolution content 
stays in the broadcast center storage? Once the EDL is created in the cloud, is there 
an automated function that retrieves it and pushes it to the rendering engine to 
create the ﬁ nal program? 
 Trafﬁ c Workﬂ ow 
 Taking a step back and looking at the entire lifecycle again, the content travels through 
the infrastructure towards distribution. It’s time to explore the changes the networked 
and integrated environment has created with trafﬁ c and scheduling. Since trafﬁ c and 
scheduling have been using databases for a long time, the ﬁ rst evolution was from the 
connection between trafﬁ c and master control as a device-to-device RS232 serial con-
nection to an Ethernet connection that transfers data over a network to many devices 
simultaneously. Trafﬁ c systems have continued to evolve to having metadata commu-
nicating between devices and systems while being managed by automation. 
 Multi-channel distribution was one of the ﬁ rst big changes in program origina-
tion workﬂ ows. In the ﬁ rst generation of multi-channel origination, the delivery 
format was the same to each platform (cable, satellite, and DTV) and different 
programs were delivered via different channels. Now in addition to the same for-
mat for multiple programs that are delivered for delivery to multiple channels, 
there are many different formats that are delivered to the many different delivery 
platforms and there are multiple transmission paths that also are in many differ-
ent formats. 
 This has signiﬁ cantly changed the workﬂ ow and processes of trafﬁ c operations. 
For commercial networks the integration of commercials is substantially different 
for each platform. Linear program channels have one type of commercial inte-
gration, On-Demand has a different one and streaming to devices even another. 
There are differences streaming over broadband vs. to mobile. The trafﬁ c system 
has to manage and control the delivery to each of these platforms, and each plat-
form is different: different in what it receives, how it receives it, and how the asset is 
formatted speciﬁ cally for each distribution channel. The scheduling group has to 
produce multiple schedules. There is one schedule for linear distribution for more 

Workﬂ ow and Process
51
traditional viewing, and a different one for On-Demand delivery to all devices 
and DVRs that are programmed by show rather than linear channel. Programs 
are watched based on the viewer’s interest and a user-created schedule regardless 
whether the origination is still linear. 
 These transmission and distribution processes are all managed and controlled 
via automation. The trafﬁ c and scheduling systems create the metadata that is sent 
over the network in a common data format and are instructions to the automa-
tion system. 
 The trafﬁ c department manages playlists not only for multiple channels and 
time zones, but also across multiple platforms. The trafﬁ c logs being fed to the 
automation systems have to include the instructions that specify which metadata 
ﬁ elds accompany the content to each platform. There is no standard for metadata 
for the different content distribution channels. The trafﬁ c and scheduling process 
provides the command and control metadata that are the instructions to the auto-
mation system for what type of processing the content requires for distribution. 
The scheduling department needs to manage these instructions. 
 FIGURE 3-5 
 Roles and Responsibilities Redeﬁ ned 
 The roles and responsibilities of support personnel within the organization 
change as part of the transition to an IP architecture and workﬂ ow. First there 
are expanded responsibilities and a need for new skills and knowledge base. Then 

52
Workﬂ ow and Process
there are new positions created as a result of IP stream- and ﬁ le-based processes. 
The ingest manager is responsible for bringing media and metadata into the sys-
tem. This can include ﬁ le transfers, encoding, and transcoding. The ingest point 
is when metadata tagging and media logging begins. In the ﬁ le-based ecosystem, 
the digital librarian ensures that the asset management system logs the media and 
creates the proxies, and that the metadata is entered properly for users to be able 
to search and browse. 
 The “master control” operators now manage delivery to multiple platforms 
that have their own speciﬁ cations for media format, resolution, and metadata 
requirements. 
 Engineering has expanded responsibilities that add additional workﬂ ows 
and processes in the IP ecosystem. Problem solving, maintenance, and support 
for the IP infrastructure are mostly software centric. Even the hardware systems 
are maintained with software tools. Routine maintenance on software systems 
includes updates and service patches that occur on a frequent basis. Maintaining 
hardware includes ﬁ rmware updates, storage, and bandwidth management. The 
maintenance process for servers, storage, and network or proprietary IP devices is 
not pulling it out of a rack and putting it on a test bench. That may apply only if 
a part needs replacing and not even then. There is less bench work in the IP- and 
ﬁ le-based ecosystem. Keeping track of software and ﬁ rmware versions plus device 
conﬁ gurations is as much a part of engineering as designing, building, and main-
taining in the more traditional sense. 
 The entire architecture and infrastructure resembles an IT datacenter and an IT 
backbone. The broadcast engineer needs to be skilled in network routing, switch-
ing, servers, applications, and storage. Understanding the interfaces between dif-
ferent software systems and middleware is an essential knowledge base. 
 There are many pieces to the puzzle that make up the complete IP- and ﬁ le-
based architecture. Understanding how all the business units work together is just 
one more piece of this complicated puzzle. There are new workﬂ ows and processes 
as a result of the technologies that are integrated within the IP architecture. There 
is a need for media and metadata to be accessible across all the business units that 
rely on database integration. 
 Workﬂ ow and process are an important design consideration. User access and 
the interfaces between software applications are all based on the way business pro-
cesses interact. 
 

 Media management is probably the most important piece to this puzzle. In the 
IP- and ﬁ le-based ecosystem, media management is the core of how technology, 
operations, workﬂ ow, and business processes all interact and function. AND, at 
the heart of media management is metadata. The logging and entering of meta-
data is essential because it includes the critical information used to package, sell, 
and distribute a show. Metadata is associated with the media at all stages of the 
media management lifecycle. 
 Creating and managing assets in the IP- and ﬁ le-based ecosystem is depen-
dent on using descriptive and administrative metadata that stays with the asset 
as it travels through the value chain and into distribution. It is imperative to have 
well-organized and well-structured metadata associated with every asset. This 
metadata is what content delivery networks use to enable program guides, recom-
mendation engines, and search tools to locate the asset, with the metadata serving 
double time for business intelligence requirements. In the ﬁ le-based ecosystem, it 
is critical to have well-deﬁ ned policies that are rigorously and consistently adhered 
to, thus enabling the creation of quality metadata that is correct and complete. 
Without this, deploying the latest in Digital Asset Management (DAM) technol-
ogy becomes an empty vessel. 
 Media management encompasses the management and movement of media 
throughout the infrastructure, from ingest out to delivery and distribution while 
four 

54
Media Management
providing metadata for search. Media management includes tracking viewer 
access and managing the return data from the distribution networks on usage 
statistics. 
 On the business side, media management and metadata are central to the mon-
etization of ﬁ le- and stream-based media. 
 Even before the ﬁ rst element of media is created, there is metadata, administra-
tive and descriptive metadata. As media moves through the entire system, more 
metadata is continually added, and the metadata also controls how it moves and 
where it goes. 
 In the ﬁ le-based lifecycle, there needs to be a management structure, complete 
with standards and policies to govern the media ﬂ ow. 
 Media management encompasses more than asset management and metadata. 
Indeed, it is the entire movement of media within the IP- and ﬁ le-based ecosys-
tem and value chain. Media management is an overarching term that describes 
the handling and control of streams and ﬁ les across the media lifecycle and value 
chain. 
 Media management is not a new concept; however, in the IP- and ﬁ le-based 
ecosystem, it has become the core and foundation of the architecture. From the 
moment a production is planned, there is valuable information—data—that fol-
lows the idea into creation. Once the essence is created, if it doesn’t have something 
describing it, identifying it, and managing it, it has no value. Metadata deﬁ nes the 
essence and turns it into content that, when associated with a program that will 
generate revenue, becomes an asset. 
 Figure 4-1 works from the top down, it shows how media management func-
tions are the core processes as content moves from acquisition through creative 
and out to distribution. 
 Media management starts with business drivers and user groups. This drives 
the policies that create the workﬂ ow and processes. This is the entire basis for the 
metadata that travels through applications and databases, that runs on servers and 
dedicated devices over the network where the content ultimately lives in storage 
and archive. 
 All of these elements are part of the media management system that ensures the 
media ﬂ ows through the infrastructure: 
 
 It creates the relationships between assets, contracts and rights 
 
 It controls the movement and processing of media with automation 
 
 It manages the library and retention policies 
 
 It organizes media for search and browsing 
 
 It provides the structure for the organization and cataloging of media 
 
 It manages the formatting and delivery to distribution platforms 
 
 It delivers program data to distribution channels for guides and searching 
 
 It imposes the usage and access rights to the asset 
 
 It analyzes data aggregated from the various distribution networks 
 
 It governs the entire media ﬂ ow with standards, rules, and policies 

Media Management
55
 Metadata 
 The term “metadata” has become almost a generic term almost like using Kleenex 
to describe facial tissue. What is metadata? Metadata is often called “data about 
data” or “information about information.” In telecommunications metadata is the 
personal information on a mobile device, metadata is the embedded key terms 
that search and recommendation engines use to ﬁ nd content. In broadcast, meta-
data takes on the responsibility of managing and controlling every aspect of media 
from before it’s created to when it goes to archive forever! 
FIGURE 4-1

56
Media Management
 In broadcast, metadata is structured information that describes, explains, or 
makes it easier to ﬁ nd, use, control, or manage digital stills, video, or audio. 
 The word “metadata” was originally coined in the library world, and is now 
commonly used for any formal scheme of resource description, applying to any 
type of object, digital or non-digital. 
 The following are some of the ways metadata is used, as well as a few terms used 
to describe metadata: 
 Digital ID —This is a unique identiﬁ er of the asset or object, which could be 
part of a ﬁ le name. 
 Discovery —Metadata allows resources to be found by: 
 
 Deﬁ ning multiple criteria 
 
 Identifying resources 
 
 Bringing similar resources together 
 
 Distinguishing between dissimilar resources 
 
 Giving location information 
 Organize— Metadata is used to index, organize, and catalog media based on 
taxonomies and ontologies. 
 Interoperability —This is the ability to have multiple systems operating different 
hardware and software platforms, data structures, and interfaces to exchange 
data with minimal loss. Assets can be searched across the network more seam-
lessly by using well-deﬁ ned metadata schemes and shared transfer protocols. 
 Automation —Automation uses metadata to provide instructions and com-
municate between different systems for moving ﬁ les throughout the ecosys-
tem, controlling applications and devices that manage and process media. 
 Library and Preservation —Metadata typically centers on the discovery of recently 
created assets. There is always the need to retain programs, B-Rolls, and ele-
ments. However, there is also concern that digital assets are fragile and they can 
be corrupted or altered, intentionally or unintentionally. The assets can become 
unusable as storage media, hardware, and software technologies change. 
 Metadata is the key to ensuring that assets will survive and continue to be acces-
sible into the future. The archiving and preservation processes require special 
metadata elements to track the lineage of a digital object. 
 Metadata not unlike “regular” data is managed in a database. The ﬁ eld structure 
of the database and the relationship of the ﬁ elds are commonly called the “meta-
data schema.” 
 When a metadata schema is being planned, the discussion usually centers on the 
tags and indexing that will assist in ﬁ nding an asset. For retention, the key question 
is, “What needs to be preserved for the future?” Metadata is much broader than that. 
 There are six main types of metadata: 
 
 Descriptive 
 
 Structural 

Media Management
57
 
 Administrative 
 
 Rights Management 
 
 Preservation 
 Technical
 Descriptive metadata  is the most familiar. This is the metadata used to browse 
and search for an asset. It can include elements such as title, abstracts, 
descriptions, author, and keywords. Most of the catalog and organizational 
tools are used with the descriptive and preservation metadata. This is the 
metadata that search engines and program guides use to locate media. 
 Structural metadata  is the control layer that handles the movement of media, 
automation, and interaction between applications and databases. An exam-
ple of structural metadata is recognizing a ﬁ le in one format, sending it to 
a transcoder, and giving the transcoder a set of instructions to convert the 
ﬁ le to a different format using a speciﬁ c proﬁ le. Once the ﬁ le is converted, 
it triggers a QC process to validate the ﬁ le. This process controls how com-
pound objects are put together or which instructions are sent to transcoders 
to format for cable, tablet, and web. 
 Administrative metadata   is the management layer—deﬁ ning the media by ﬁ le 
type, ownership, location in the storage network, internal usage, and access 
rights. Additionally, this is the metadata that integrates with business sys-
tems. Essentially, the administrative metadata captures the technical details 
of the asset, making it easier to manage. 
 Rights management metadata  is different than administrative usage per-
mission and access rights. This is the protection layer of distribution, and 
includes copyright, intellectual property, and distribution rights. It carries 
expiration rules, encryption policies, and tracking metadata used in water-
marking for piracy protection. 
 Library & preservation metadata  is used for archiving purposes.  When meta-
data is part of any discussion, it typically centers on archiving. In the IP- and 
ﬁ le-based architecture, the digital library takes on greater roles and respon-
sibilities. The library retains and manages the “master copy” of all content 
ingested to the system. There are core elements that need to travel with the 
asset for preservation, ensuring that when it is accessed or restored in the 
future, there is adequate information (both descriptive and administrative) 
that identiﬁ es it. 
Technical metadata describes the creation and technical characteristics of digi-
tal and physical objects, such as ﬁ le format, resolution, size, duration, and 
track conﬁ guration. The automated capture of detailed technical metadata 
is central to obsolescence planning and preservation strategy.
 Creating, managing, and integrating metadata is at the core of a well-planned 
and designed IP architecture. These functions must exist from the origin of media 
during the creative process all the way through to production, acquisition, craft 
production, distribution, and monetization. 

58
Media Management
 Media management relies on the integrity and consistency of metadata. The 
amount of metadata expands as the content travels across the architecture, with 
each system and operation adding more. 
 Media management is not a single application, it is a collection of integrated 
applications. There are a number of different technologies and systems within 
the IP architecture that manage and handle media. The metadata moves with the 
media. The systems and devices need to be able to receive and communicate infor-
mation about the ﬁ le or stream. 
 Many of these systems use different database applications and structures. There 
needs to be middleware, a connector application that facilitates interoperability. 
It functions almost like a trafﬁ c controller and translator. There are a number of 
these types of applications in the marketplace. They operate on a rules-basis that 
knows which data is permissible for extraction or loading to each of the applica-
tions and databases that are part of the media management system. 
 Media and metadata move throughout the entire media management infra-
structure .  It is a critical requirement that there is interoperability between devices 
and systems. It is not uncommon for different systems to use different databases 
and database structures, which means that there needs to be controlled integration 
among databases. In the IP ecosystem, the term “governance” is used to deﬁ ne the 
rules and policies that protect access to sensitive data while enabling integration. 
 “Interoperability” is one of those words that instills fear in the hearts of all sys-
tem engineers. System and device interoperability is a myth that has been passed 
around the broadcast and IT industry for many years. First, there were little black 
boxes and software widgets to deal with device interface. Then, as IP- and ﬁ le-
based technologies arrived, so did even more devices, APIs, SDKs, and middleware 
to deal with “interoperability.” Published standards do not resolve this! 
 Media management doesn’t solve it, either; it only adds another layer. In the 
IT industry, interoperability keeps programmers employed writing interfaces and 
creating special mapping or custom translators to move data between applications 
and databases. 
 Media management is all about encoding, transcoding, and transmuxing in 
order to allow the media to move “seamlessly” among systems and devices. Meta-
data faces the same challenges. While the eXtensible Markup Language (XML) 
has become one of the accepted standards as a ﬁ le transfer protocol for interoper-
ability, the format and ﬁ eld structure of the XML is not part of the standard. The 
formats CAN be different in each system that uses metadata, and depending on 
the role of the device and/or system, the metadata ﬁ elds WILL be different. 
 Another consideration is how the XML is delivered: 
 
  Synchronous— The systems need to communicate dynamically in both 
directions, ensuring that the metadata is the same in both systems. 
 
  Push— One system sends the XML ﬁ le to another system programmed to 
receive it. 
 
  Pull— One system has a folder or IP address conﬁ gured, and under a schedule 
or trigger, it retrieves the metadata and updates itself. 

Media Management
59
 When a metadata interchange happens between databases with dissimilar ﬁ eld 
structures, the metadata has to be re-mapped to match the structure of the receiv-
ing database. This is sometimes done manually, with custom code or by using mid-
dleware specially designed to interface disparate databases. In the IT community, 
there is a process called ETL, or “Extract-Transfer-Load.” These are middleware 
applications that can be considerable problem solvers in the media architecture 
across databases.  
 These tools operate under a set of policies programmed into the application 
that control the access to only the metadata that is allowable for transfer, thus 
creating a seamless transfer. 
 In the media management system, there a number of applications, dedicated 
devices, databases, and storage that are organized and structured in a multi-
segmented layered network. The metadata ﬁ les moving in an XML format travel 
across these networks. 
 FIGURE 4-2 
 Figure 4-2 oversimpliﬁ es the number of layers within the network architecture, 
but it clearly demonstrates that there exists an application layer, database layer, 
and storage layer. 

FIGURE 4-3

Media Management
61
 The ﬁ gure also shows the different applications that potentially share metadata 
across databases while the media is organized in a federated storage architecture—
possibly by department, workﬂ ow (ingest, production, distribution), or simply 
application. 
 There is a management layer to support the interoperability between applica-
tions and media. One of the critical design considerations (and one that demands 
constant monitoring) is the trafﬁ c and performance requirements of each of the 
applications and systems. Metadata and media move across the network differently 
and to various locations. The media management systems control and manage this 
movement, locating and integrating the appropriate metadata to each of the appli-
cations and delivering the media to the next stop in the lifecycle. At the same time, 
it maintains the relationship of the media and its associated metadata. 
 There is a dynamic interaction of XML ﬁ les moving the metadata, sometimes 
requiring middleware to enable this. 
 “Integration” is a broad term that boils down to which systems really need to 
interact, have access to each other and why. Here are some of the business systems 
needed to interact with media management: 
 
 Trafﬁ c and Scheduling 
 
 Automation and Control 
 
 Marketing and Business Intelligence 
 
 Executive Management 
 
 Legal and Finance 
 Trafﬁ c and scheduling are somewhat obvious. The trafﬁ c system will handle mul-
tiple schedules going to different platforms, so it is easy to see how the delivery 
metadata plays a key role. The trafﬁ c system sends a control set of XML along with 
the trafﬁ c information to the play-out server, and gives an instruction set to the 
automation system. The scheduling system has the electronic program guide (EPG) 
and search metadata that may be different for each platform and must be packaged 
with the asset for distribution. 
 The automation system controls ﬁ le and stream movement throughout the 
infrastructure and into the different storage areas for access by each of the differ-
ent production and business systems. It receives its instructions in XML from the 
trafﬁ c system, ﬁ nance, marketing, and legal. 
 Marketing has a number of roles and responsibilities. It needs access to the 
content to create promos and ads, and it adds metadata for tags and indexes for 
search and recommendation engines. Business intelligence compares the return 
metadata with the original metadata sent to the distribution channels and pro-
vides direction for the tags and keywords that marketing includes. 
 There are a number of user groups that will only use the search and browse 
features. Executive management will research assets for many different reasons. 
 Legal and business affairs require access to oversee usage and access rights. They 
will enter contractual metadata that controls which internal and external users can 
access, handle, and use the asset. The ﬁ nance group accesses the metadata to see 

62
Media Management
what resources have been used to create and manage the asset. Finance may also 
add metadata of other associated costs not captured by other systems. 
 Metadata Standards 
 Just as there are media standards, there are also metadata standards: 
 
 Media Formats – MXF, GXF, ProRes, DVC, AVCi, DNx 
 
 Metadata Schemas – Dublin Core, EBUCore, PBSCore, ISO/IEC, SMPTE 
 
 Data Interoperability Formats – XML, ODBC, CSV, J2E 
 In the previous discussion on codecs and format standards, it was important to 
ensure the container for audio, video, and metadata conformed to an interoper-
able standard since this is essential for delivery across multiple platforms. The 
same is true about metadata. With metadata there are schemas; this is the ﬁ eld and 
data record structure in the database that will host the metadata. For a title, there 
is the structure of the title, it will be considered a text ﬁ eld, it will be allowed to 
be “X” characters long, have ﬁ elds for categories and have certain properties and 
characteristics for indexing. 
 The Dublin Core metadata schema refers to a 1995 Invitational Metadata Work-
shop hosted in Dublin, Ohio, by the Online Computer Library Center (OCLC), 
a library consortium, and the National Center for Supercomputing Applications 
(NCSA). The "Core" refers to metadata terms as “broad and generic, being usable for 
describing a wide range of resources.” The Dublin Core Metadata Initiative (DCMI) 
is an open forum for the development of interoperable online metadata standards. 
 During that fateful conference, the Dublin Core established an agreed standard 
of the essential ﬁ elds an asset must have for a librarian to manage it. Metadata needs 
a format to move between systems, and XML is the current preferred format. 
 No different than the media industry, in the library industry there are a number 
of organizations that create and manage the standards and protocols for metadata. 
 These organizations have established standards that clearly deﬁ ne where meta-
data lives within the asset, how it is transmitted, and what metadata the essence 
must have that is placed directly into it by a camera or encoder. 
 When it comes to standards and protocols, the metadata community is not to 
be left out and it most deﬁ nitely has its own set. There are a number of standards 
and protocols associated with metadata: 
 
 NISA 
 METS 
 MODS 
 
 OCLC/NCSA 
 
 Dublin Core DCMI 
 
 SMPTE 
 SMPTE 335M-2001 Metadata Dictionary 

Media Management
63
 
 ANSI /ISO 
 ISO/IEC 11179 
 The National Information Standards Association (NISA) has a set of guidelines 
for the use of metadata, this is mostly for library and information management, 
however it can include non-digital materials. 
 
 METS is the Metadata Encoding and Transmission Standard developed by a 
group of research libraries that is widely accepted. 
 
 MODS is the NISA Metadata Object Schema. 
 Most archivists will agree that the Dublin Core is the minimum requirement of 
metadata ﬁ elds and details for any digital object that is to be preserved. 
 As metadata migrated into the broadcast industry, SMPTE, ISO, and ANSI 
jumped in and created a set of metadata standards speciﬁ cally for the broadcast 
industry, as well as the transport protocols for how the data set is associated with 
the media ﬁ le (XML). 
 There are new standards for the layer of metadata embedded with a ﬁ le. MPEG7 
and MPEG21 speciﬁ cally address metadata and the second screen platform delivery. 
 The SMPTE Metadata Dictionary (SMPTE 335M-2001) is composed of a set of 
elements describing audio/video that can be grouped into categories: 
 
 Identiﬁ cation 
 
 Administration 
 
 Interpretation 
 
 Parametric 
 
 Process 
 
 Relational 
 
 Spatio-temporal 
 
 Organizationally Registered Metadata 
 
 Experimental Metadata 
 The ANSI-ISO/IEC standard consists of six parts: 
 Part 1 - Framework 
 Part 2 - Classiﬁ cation 
 Part 3 - Registry meta-model and basic attributes 
 Part 4 - Formulation of data deﬁ nitions 
 Part 5 - Naming and identiﬁ cation principles 
 Part 6 - Registration 
 An interesting note is that the ISO/IEC 11179 standard does not describe data as 
it is actually stored. The two main purposes for this standard are: 
 1) Deﬁ nition of the data and 
 2) Exchanging data 

64
Media Management
 While it appears that there are many slightly differing standards, they all refer to a 
controlled vocabulary and dictionary that carries the descriptive, structural, and 
administrative metadata. All metadata is organized in this structure. 
 The example below shows the transition from raw essence to monetized asset 
through a simple example using water as the essence and then applying metadata 
to turn it into a valued asset. 
 Data for Water 
 
  Essence : [type]Spring Water 
 
  Metadata :  [object] H2O: [type] spring water: [location] Vermont: [date 
of origin] 20 March 
 
  Content :  [category] A bottle of water 
 
  Asset :  [title] Vermont Water {branded for sale} 
 The water itself is the essence; by adding metadata descriptors, [object] H2O, [type] 
spring water, [location] Vermont, and [Date of Origin] 20 March, it becomes cat-
egorized content. Once it is branded, it has value and can be monetized. 
 The object and category are descriptive; the type, title, location, and date are 
administrative within the organization of metadata. 
 Now apply the same concept to media. Here is an example of how metadata 
turns media essence into an asset. 
 Data for Media 
  Essence: [type] Audio, Video, Image 
 
   Metadata: [object]  HDD: [format] ProRes: [date acquired] 20 March: [loca-
tion] Vermont: [air date] 20 June: [platform] mobile 
 
   Content: [ category]  News, Sports, Entertainment 
 
   Asset: [title] Emmy Winner Show 
 The essence “type” will be sound, video, or image. Adding metadata for the media 
format, date acquired, and location is what turns the essence into content. Next, 
a category is added. Finally, adding a title assigns value and completes the asset 
creation process. 
 Storytelling and the Value of Metadata 
 We will now discuss asset management, since metadata has been deﬁ ned and it’s 
clear what its value is in the handling and management of media. Up until now, the 
discussion has revolved around media management as a set of integrated systems 
and processes. It’s now time to look at the value of asset management. There are 
a number of names used to describe asset management, “MAM” for Media Asset 
Management or “DAM” for Digital Asset Management. 

Media Management
65
 This is not to be confused with DMS which is Document Management System 
which handles documents, PDF, PowerPoint, and spreadsheets or CMS which is 
a Content Management System that is a program that allows publishing, editing, 
and modifying content typically for web or mobile delivery. 
 Back to MAM/DAM. The value of asset management for media has increased 
substantially as the amount of media being consumed on different types of screens 
and platforms increases. 
 Metadata has not been the most exciting or interesting aspect of the transition 
to ﬁ le-based media production. It has been treated more like a necessary evil, but 
all that has changed or may be changing. Metadata has always been valued as an 
efﬁ cient means of automating and managing ﬁ le-based assets from production 
through scheduling and play-out. With broadband and mobile platforms, it has 
extended its usefulness to outside the broadcast center as an integral part of new 
ways that users can interact with media and is being used to open up new revenue 
opportunities. 
 This is the storytelling part. The most obvious example is live sports where 
statistical data is integrated with video. Metadata is not just the keywords embed-
ded for search engines. It is companion information or an interactive component 
that encourages engagement. This is an essential element in social communities 
and boosts the stickiness of websites. Metadata is the Electronic Program Guide 
(EPG) that is used to display schedules and also used by recommendation engines 
during a search. Descriptive metadata empowers the viewer to ﬁ nd content using 
a keyword search. 
 There is a fair amount of attention focused on the volume and sophistication 
of information supplied for each program (movie, music, book, etc.), which can 
range from the technical data that marks a program as part of a series and assigns 
digital rights for distribution. It will show program duration and have additional 
editorial detail like still images, biographies, reviews, or recommendations. 
 There is another side to storytelling. The return data from the distribution 
platforms and social networks provides a wealth of data to be analyzed. Business 
intelligence, once gathered and analyzed, will tell a story about the viewers to the 
marketing department. There is an enormous amount of data that ﬂ ows back to 
the broadcast center, challenging the IT network and overwhelming databases. 
 What is asset management? At its core, it is the indexing and cataloging of all 
content within the ﬁ le-based ecosystem and once it is considered valuable enough 
to retain and archive, making it accessible under controlled access. There are many 
other features and functions within the asset management application that are 
part of the entire IP- and ﬁ le-based workﬂ ow, but the primary function is to man-
age the media. 
 One of the main uses of an asset manager is to locate the asset. This is where 
controlled vocabularies and structured metadata are necessary in order for the 
search tools to perform properly and is key to having the correct vocabulary. 
 Librarians, archivists, and preservationists that manage large collections are all 
familiar with the terms “taxonomy” and “ontology.” While the genesis of these terms 

66
Media Management
originated in science and metaphysics, they are now very common when discussing 
asset management. 
 Taxonomy –  Greek  taxis  “arrangement” and  nomia  “method” is the science of 
identifying and naming species and arranging them into a classiﬁ cation. 
 Taxonomy is used to create classiﬁ cations according to a pre-determined system 
or controlled vocabulary. It is used with the resulting catalog as a framework for 
retrieval. 
 In asset management, taxonomies are used to organize assets and manage 
metadata. By employing a taxonomy to classify content and assets, it makes search-
ing or browsing using a digital asset management tool easier for users who do not 
know many details about what they are looking for. 
 Ontology  – Greek  onto - “being; that which is” and - logia  “science, study, the-
ory” is the philosophical study of the nature of being, existence, or reality as such. 
It is also the basic categories of being and their relations. 
 Ontology is a classiﬁ cation scheme. It is a way of deﬁ ning the relationships 
between objects in the world and organizing objects by subject categories. Ontol-
ogy also deﬁ nes how to divide up an object. This might not necessarily be by sub-
ject; an object may be divided instead by type, format, and location. 
 The following statement might be a touch contentious: “Asset Management” at 
its core is a library function. Asset Management is a process, workﬂ ow, and tech-
nologies. Looking at it from a technology perspective, it is the tool that catalogs, 
indexes, and makes media searchable and retrievable. There are different ways 
to search, such as by browsing the audio and video or by asking questions in a 
structured or unstructured way using keywords or pre-determined search criteria 
(ﬁ lters). 
 The asset management tool must maintain associations between the ﬁ nished 
program and the elements used to create it. There needs to be sufﬁ cient informa-
tion for an archive when it is retrieved in the future. 
 The following are the primary functions and operations of an asset manage-
ment system: 
 
 Catalog 
 
 Associate 
 
 Archive 
 
 Browse 
 
 Retrieve 
 
 Monetize 
 The asset management system is a complex software application based on a data-
base, media players, and with multiple interfaces to connect to other databases 
and systems for automation, usage rights, analytics, and others. The application 
organizes the media according to the metadata that’s been entered. 
 This assumes that media logging and tagging based on manual and automated 
metadata entries were actually made. Getting users to enter metadata consistently 

Media Management
67
and reliably is a considerable challenge. The media asset manager organizes the 
media and enables it to be searched, accessed, retrieved, and annotated. 
 The asset manager catalogs the media and creates the associations using agreed 
and accepted metadata schema and rules (e.g. parent–child), with search tools that 
make access browsing easier. The asset manager controls the movement of media 
within the storage architecture between applications from ingest to craft to delivery 
and archive, packaging the metadata that will travel with the media to the distribution 
networks. This metadata has the program guides, interactive services, and informa-
tion that will support additional revenue opportunities (i.e. promotions, specials). 
 So, now that Pandora’s Box is open, let’s take a look at Asset Management and 
the role it plays in media management systems. 
 The media/digital asset management system is essentially the digital library and 
provides a number of important operations in the ﬁ le-based environment. 
 
 Capture/Digitization 
 
 Standardize/Transform 
 
 Asset Management 
 
 Production/Distribution 
 
 Storage – Media & Metadata 
 
 Quality Assurance 
 First and foremost, the asset manager has to recognize all possible formats in a 
multitude of resolutions and bitrates. It must capture the metadata in a robust 
database that is well organized so that the asset is accessible. It has to provide access 
to the media and metadata for browsing, preparation, and transfer to the produc-
tion and distribution systems. 
 Many asset management products include a number other features and functions, 
such as automation, that are part of the entire media management system. 
 The asset manager and digital library is one of the core systems in the master 
IP- and ﬁ le-based architecture and ecosystem. Beginning at the point of ingest, 
the asset manager tracks the media and metadata, logs them into the system and 
places them in the appropriate storage location. During the capture or ingest 
process, it manages the encoding and any transcoding to conform the media to 
the house format. It does all this while tracking and cataloging any processes and 
changes that may be occurring at the same time. 
 As other elements are used in the production process, the asset manager 
maintains the relationship and tracks the associated metadata. As the metadata 
is updated and expanded, it manages where the asset is and controls access and 
usage. When the media is transferred to distribution, the asset manager deter-
mines which metadata moves with the media. 
 The digital library manages all the storage locations in which media resides 
and tracks the movement between systems. Similar to a “conventional” library, the 
asset manager “checks media in and out” of the systems as it moves from ingest to 
production with archive and distribution. 

68
Media Management
 The storage architecture is the foundation of the digital library. Storage can be 
segregated by location, process, resolution, or retention rules. This can be based 
on different types of media, high and low resolutions, and types of access (such 
as online, near-line, and ofﬂ ine), as well as access for other processes (e.g. ingest, 
production, distribution). 
 The digital library is the primary handler and controller of ﬁ le movement. It 
must also own quality control as well. As ﬁ les and streams pass through the system, 
the tools that analyze the ﬁ les and streams need to process the media and, if there 
are issues, alert the engineer and attempt to solve the problem. 
 The asset manager possesses the search and browse functions that enable users 
to access and retrieve media using metadata. As previously discussed, there are 
metadata standards to facilitate this. 
 It all started with the Dublin Core, which established that a small and fundamental 
group of text elements would provide enough metadata components so that most 
media resources could be described and cataloged by using only ﬁ fteen (15) base text 
ﬁ elds. When the European Broadcast Union (EBU) looked at establishing a metadata 
standard, they used the Dublin Core as their base and then created EBUCore which 
essentially added forty-ﬁ ve (45) more ﬁ elds for a total of sixty (60) base ﬁ elds now 
text, numeric, and alphanumeric. Then as the US Public Broadcasting System (PBS) 
moved to a ﬁ le-based system they looked at the EBUCore and embellished on it to 
create PBSCore. For the most part, as standards go, it stopped here and now these are 
mostly used as a guideline or a base by everyone else in creating their own metadata 
schema and calling it “My OwnCore.” While designing a metadata schema is always 
done with good intention and the best practices, most organizations create their own 
metadata schema with ﬁ eld sets that end up well into the hundreds. The US Library of 
Congress started with over 800 ﬁ elds for metadata and who knows where it is today. 
 The asset manager maintains the metadata in a structured and indexed data-
base based on taxonomy and ontology rules to organize the metadata. This struc-
ture becomes important as soon as a stakeholder wants to ﬁ nd something, then 
the search and browse tools come into play. 
 There are many different methods and types of search tools, it is interesting 
to note that most of them use one or both of the most popular techniques and 
algorithms that are known as faceted and contextual searching. 
 A faceted search is a structured technique for accessing a collection of informa-
tion about an asset created by using a faceted classiﬁ cation. A faceted classiﬁ ca-
tion allows an asset to be assigned multiple sets of attributes. By assigning a large 
number of attributes to an asset, the classiﬁ cation can be ordered in multiple ways, 
allowing users to explore by ﬁ ltering the available information. 
 Once an asset is categorized using multiple attributes, it can also be retrieved 
using multiple attributes. A user could use a single term or link together multiple 
terms which increases the chances of retrieving the asset they are looking for. For 
example, the search for an asset could be performed by having all the attributes ﬁ l-
tered by date, location, format, and type of programming or by speciﬁ c keywords. 
 A contextual search is more of an unstructured method, allowing the user to use 
random descriptors and letting the search engine look through all the metadata 

Media Management
69
in an unstructured way, ﬁ nding the most relevant matches. This type of search 
is more time intensive, requiring more CPU processing power. In the contextual 
search, it opens the search to more assumptions when returning results or enables 
targeted results based on the analysis of the search request. Contextual searching 
needs algorithms that can take random attributes or use metadata in an attempt 
to understand what is being searched for. 
 An example here is how advertisers can target users based on a search that has 
only a few parameters and then using recommendation engines, deliver a broad 
number of results promoting various products. 
 Searching through content and getting reliable results requires a good set of 
tools and integrity in the quality of metadata. However, in the absence of a good 
system, there are always more tried and true methods of searching like using a 
crystal ball, a magic wand, or pixie dust. 
 Storage Architecture 
 While the asset manager is the core application in the media management system, 
the assets need some place to live. The storage architecture is where the media and 
metadata reside. 
 There are multiple components to the storage architecture. Storage can be a 
local disk in a server, an externally attached storage device with one type of proto-
cols (i.e. USB, Thunderbolt, and IEEE1394) and Network Attached Storage (NAS) 
or a Storage Array Network (SAN) with a different set of protocols. This intro-
duces another layer as SAN and NAS systems will use Fiber Channel, SCSI, iSCSI, 
and Ultrawide SCSI as networks protocol to move ﬁ les between disk arrays and 
also have an Ethernet interface for the management layer. 
 Then there is the digital library which can be a robotic system for tape or 
optical disks, or it can just be multiple individual devices. Moving ﬁ les within 
the different types of storage can get complicated. Not all storage systems com-
municate with each other and not all applications recognize all types of storage 
technology. To resolve this and manage the ﬁ le movement within the storage 
architecture there are applications called data directors. These data direc-
tors are called Hierarchical Storage Management (HSM) systems. The HSM 
automatically moves the content and metadata between the different storage 
systems. The HSM is managed by the policy and retention rules in the asset 
management system. 
 This demonstrates that there are a number of considerations and factors that 
need to be accounted for when designing the storage architecture. One of the ﬁ rst 
questions is: how many tiers of storage are needed? 
 
 Online 
 
 Near-line 
 
 Ofﬂ ine 
 
 Proxy 

70
Media Management
 Online  is for immediate access and high availability; it is typically used in high 
throughput disk arrays. 
 Near-line  is still very accessible, but data will take more time to retrieve. This 
can be a digital library device like an LTO or optical disk robot. It can also be a disk 
array with less speed and throughput. 
 Ofﬂ ine  typically means the content is on some form of removable media and 
sitting on a shelf tracked by the asset manager. While high-resolution media may 
be moved to an ofﬂ ine location, the proxy and metadata are always online. 
 Proxy  is where the low-resolution version of the content that’s used for brows-
ing and creating ofﬂ ine EDLs is kept. In actuality, the fastest growing part of stor-
age is proxy storage. While the proxy is low resolution, proxies live forever even 
after the high resolution is moved to a shelf. This includes all variations, B-Roll, 
and any elements created or acquired. What this translates into is that the demand 
for proxy storage continues to grow actually more than high resolution. Proxy or 
low-resolution content is not just for browsing content. Proxy resolution is the 
content format that feeds to social communities and mobile services. These can 
be short clips of news, sports, and other live events. 
 This all has a signiﬁ cant impact when it’s time to consider scaling. Online or 
high availability and near-line storage will scale for one of two reasons: capacity 
or throughput. Proxy storage is all about capacity, throughput is rarely the issue. 
 Capacity  might seem pretty straightforward; as the volume of content 
increases, additional storage is needed. But is this a storage capacity issue or a 
media management problem? Is there unnecessary high-resolution media left on 
high availability storage that could be moved to near-line or ofﬂ ine? It is impor-
tant to remember that the proxy must always be available to review and access the 
media. Retention policies play a major role in storage usage. 
 Throughput  is more complicated. The number of simultaneous instances and 
the speed that a HDD disk, SSD, or optical disk can read/write from plays a major 
role in how to architect the storage. Disks and solid state storage have physical 
limitations on how many concurrent activities (i.e. read/writes) they can perform. 
The number of processes any one storage node is requested to perform determines 
how many storage nodes are needed. While the amount of storage capacity may 
seem to be enough the number of storage nodes to handle the amount of concur-
rent processes is not. There are a number of factors involved. As there are a number 
of processes, the ability of the storage node to handle multiple processes is also a 
function of bitrate. The higher the resolution, the higher the bitrate and as a result 
there can be fewer concurrent activities. This pressures the need for more disks to 
distribute the processing load to. 
 In one example of this, there’s a media management system that is designed for 
3 edit stations, 3 ingest encoders, and 2 play-out channels. Now, there’s an increase 
in the number of edit stations from 3 to 6 and the house format and bitrate are 
set to ProRes145. There may be enough storage  capacity  in the system to handle 
the increase in workstations, but on the performance the system slows down and 
cannot process that number of edit sessions at once. In this case, storage capacity 

Media Management
71
may not be the issue—the throughput or how much trafﬁ c the storage can handle 
at once is likely the culprit. The only way to resolve this is to add more disks. In this 
instance the actual scaling requirement is to increase the amount of throughput 
and getting more storage is just a side beneﬁ t. It shows how relieving the stress on 
throughput impacts the entire storage architecture. 
 Case Study 
  One of the major US cable networks has one of the largest installations of a well-
known manufacturer’s storage. Its size is not attributed to volume, but to the number 
of edit stations. As the number of user edit stations has increased, their systems and 
processes slow down; this is a throughput problem. So to resolve this more throughput 
is needed and to get the additional throughput they need, they have to keep adding 
more disks. And each time they add a disk to manage the throughput, they get a sub-
stantial increase in storage. 
 Whether the scaling is a result of capacity or throughput growth it still has direct 
capital and operating cost implications. Disk arrays need rack space, network 
ports, power and HVAC, and each storage type and manufacturer scales differ-
ently. Are there more frames required, are there management nodes, what about 
licenses? Does the data director need more nodes and licenses? 
 There are a quite a few questions to ask when designing the storage architec-
ture. Here are some of the key considerations: 
 
 How many ﬁ les are recorded at the same time? 
 
 How many ﬁ les total will be stored in each tier and for how long? 
 
 How many different bitrates are there? 
 
 Will the architecture be centralized or federated? 
 
 How many types of storage are needed? 
 
 How will the storage be segregated? (ingest, production, distribution) 
 
 How many systems and users need to simultaneously access each tier of storage? 
 
 What systems does the HSM need to support? 
 
 Which network protocol is right for the design? 
 
 What about switch ports and load balancers? 
 
 Does the asset manager need a speciﬁ c interface? 
 Where do I keep it?  The storage system can be a designed as a central repository 
or in a federated network. In a central repository, there is one large storage system 
that is partitioned to handle all the requirements. In the central repository model all 
systems and applications need simultaneous access to the storage. This becomes a 
design consideration to the network, as well as there will be a lot of high bandwidth 
trafﬁ c on the network. Also, in this scenario, if the system needs to scale whether 
for throughput or capacity the whole system has to scale. The federated design can 
be in the same physical location with different storage systems servicing different 

72
Media Management
processes within the entire ecosystem. In this model ingest, production, distribu-
tion, and archive are different storage systems, types, and protocols. Another type 
of federated design would be to have different storage systems in different locations; 
this allows the content to exist in diverse locations without needing to migrate large 
media ﬁ les across the network to a central storage location. In the federated storage 
design, when it comes to scaling, only the part of the storage being stressed needs 
to scale. When designing the storage architecture, network protocols, trafﬁ c, and 
bandwidth management are a major consideration. 
 How much do I keep?  Spinning disks or Solid State storage is expensive and 
removable media is less expensive. Retention policies are driven by business rules. 
The On-Demand access that users want of archival material has changed preserva-
tion policies. There are new considerations for a larger amount of content to be 
retained for potential use in the future, while the social communities and mobile 
services require an immediacy of content delivery. There is also a high demand for 
archival material for research, comparisons, or pure interest. For these platforms 
the format is typically at a lower resolution. This can mean having to retain mul-
tiple proxy sizes and formats. This also opens the question of keeping it internal or 
moving some of it to the cloud (this will be discussed in a later chapter). 
 Case Study 1 
 On one project the mandate of the organization is to record everything and save 
it forever. This translated into 22,000 hours of content per year. The question is 
how much of it has to be high availability vs. on demand. The archive version is 
the one kept forever, current content remained relevant for 30 days and could then 
move to near-line and once there could be moved to ofﬂ ine, essentially moving the 
removable media from the system to a shelf. In all cases the proxies and metadata 
still live forever in the asset management system. Any content can be searched on 
and restored for other uses. 
 Case Study 2 
 A different project also saves forever, however not everything. A rating system 
that’s part of a logging system sends metadata of the selected elements to the asset 
management system which creates the clips that are archived and saved forever. 
Here there is a selective process that is trying to be practical on how much is saved 
vs. the amount that is actually captured. In this situation, they are monitoring 
the amount of content that is requested through social communities. At the same 
time the media is never removed from the system so the robotic digital library has 
expanded more than the storage. 
 What do I keep it on?  In the preservation of media making choices and deciding 
on formats has been a difﬁ cult dilemma for as long as media has been archived. In 

Media Management
73
the ﬁ le-based world, the added dimension is not only what type of physical media 
will be durable and what kind of devices will be available for restoring, but also 
what ﬁ le types and resolution will be sustainable as new ﬁ le types, protocols, and 
containers formats are introduced. In addition to the retention policy and media 
type, there must be a migration policy. 
 
 How often should the library be migrated to the next generation? 
 
 How practical is it to keep one of each device for restoral? 
 
 Will the next application version be backwards compatible? 
 It’s important to keep in mind scaling and the associated costs when considering 
which type of technology to use for near-line and preservation storage, Near-line 
and preservation storage is typically on a removable media format and there are a 
large number of options. However if disk is a consideration, when looking at disk 
storage, while the cost of disks continues to decrease in proportion to their capac-
ity, the amount of storage needed has increased exponentially. And as discussed 
earlier, the need to increase storage by adding disks has attached costs such as 
physical space, heat and power, switch ports, and mean time to failure (MTTF) 
considerations. How long does it take to fail? How is it protected? Digital tape is 
less costly, but the formats tend to sunset quickly, so there is a migration require-
ment and device upgrade schedule requirement. Solid state disk and solid state 
storage have the same questions. The physical media is fragile and device compat-
ibility will need a migration and upgrade policy. Optical disk will have the same 
considerations as digital tape and solid state. 
 Among the key considerations in storage management is how long high resolu-
tion needs to stay online and whether a mezzanine (production resolution) version 
needs to be kept.   
 How to size storage?  This is a great question and not necessarily an easy answer. 
Does the high-resolution content move immediately to near-line? Do users work with 
the mezzanine and proxy resolutions for production? How long do the mezzanine ﬁ les 
stay online and does it have to be kept if the high resolution is being kept? 
 As mentioned earlier, capacity and throughput are two primary considerations 
in sizing storage. First, let’s discuss capacity. How much new media is going to 
be ingested and captured on a predictable production cycle? How many different 
resolutions are anticipated (since ﬁ le size does matter)? The table above shows the 
most common formats and the typical ﬁ le size for 1 hour of content. To record 
at 4K resolution requires 3.7 TB of storage for 1 hour and 62 GB for 1 minute 
of video. Now look at throughput: the 4K bitrate is 3.82 GB/s, and the network 
bitrate is 1.03GB/s. Now add in the bitrates for the disk to read/write plus allow 
for buffering time in the disk/ﬂ ash array. It’s easy to see where a log jam might 
occur if the throughput of the network and storage architecture are not sized cor-
rectly. The speed of a disk is one metric of throughput, measuring how fast it can 
read and write. Another metric is the storage architecture. When multiple disks 
or Solid State Drives are mounted together, the throughput is based on both the 

74
Media Management
transfer rate between storage devices and the network. This is where selecting the 
right transfer protocol (Fiber Channel, iSCSI) makes a big difference. 
 There is a lot of work being done with 4K, including applying both MPEG2 and 
MPEG4 compression techniques to bring down the bitrate and ﬁ le sizes. 
 Now it’s time to design the storage. Once the house ﬁ le format is selected, that 
will be used as the reference to calculate the typical ﬁ le size by multiplying the 
bitrate and time. 
 The following example calculates storage capacity ﬁ rst. Here are the speciﬁ cations: 
 
 MPEG2 is the main ﬁ le type. 
 One minute of DNX145 is a 1.02 GB ﬁ le. 
 
 At XDCAM 50Mb/s, the ﬁ le is 500MB; at MPEG4 AVC100, the ﬁ le is a 
270MB ﬁ le. 
 
 There is an average of 4 hours of original content created or ingested per day. 
 
 DNX145 is the house format of choice. 
 The mezzanine format is XDCAM50 plus a 2MB proxy. 
 
 One hour of PCM uncompressed 8-channel audio is 16.56 GB, making the ﬁ le 
larger and adding to storage requirements. 
 One interesting note is that the way codecs are identiﬁ ed is by the bitrate and 
ﬁ le size, it typically only accounts for video and politely leaves out audio and data 
 FIGURE 4-4 
Format
Name
Bitrates
File Size
Platform
MPEG2 4:2:2) @ 
MP/HL
XDCAM
25, 35, 
50Mbit/s
18 30 GB/Hr.
Production
MPEG2 4:2:2)@ 
MP/HL
DVCPRO
50, 100 Mbit/s
30- 60GB/Hr. Production/Library
MPEG2 4:2:2)@ 
MP/HL
ProRes 422/DN×HD 147, 220 MBit/s 100 GB/Hr.
iTunes/Production/
Library
MPEG2 4:2:0 @MP
DVCAM/Firewire
25 MBit/s
15GB/Hr.
Production
JPEG2000
J2K
100-250Mb/s
60-140GB/hr
4K RAW 
Red One
1.03 GB/s
3.72 TB/Hr.
Production
 
 
 
MPEG4 
Blu-Ray
40Mb/s
6GB/Hr.
DVD
MPEG4 Part 2
H.263
700K-3Mb/s
N/A
Video Conference/
Web
MPEG4 Part 2
H. 263
700K-3Mb/s
300MB
You Tube
MPEG4 Part  10 
H.264
700K-3Mb/s
300MB
Web, Mobile, 
Flash
MPEG4 
H.264/AVCHD
100Mb/s
16GB/Hr.
Production
 
 
 
HTML5 - Open Source H. 264
 700K-3Mb/s
Variable
Apple iPhone, iPad,
HTML5 - Open Source Web/VP8
 700K-3Mb/s
Web, Mobile

Media Management
75
(VANC, HANC, Aux, CC, SAP) when representing bitrates; however, when planning 
storage, it is important to account for the entire ﬁ le size including all overhead. 
 Here is the arithmetic to ﬁ gure out storage for only 4 hours: 
 
 1 hour DNX145 Video @ 61.02GB × 4 = 244.08GB 
 
 8 channels PCM 24bit 96k Audio @16.56 GB × 4 = 66.24GB 
  Total 4 hours DNx 145 = 310.32GB 
 
 1 hour XDCAMHD50 plus audio = 42.18GB 
 
 1 hour proxy 2Mb/s = .99GB 
 
  Subtotal = 353.49GB 
 
 Other Material @33% = 117.83GB 
 
  Total Storage 4 Hours/Day = 471.32GB 
 
{  5 Day week = 3.3TB 
 
{  Month = 13.2TB 
 Graphics, B-Roll, and effects can add up to approximately 33% of other materials, 
or 117.83GB in the above example. This brings the 4 hour total to 471.3GB per 
day; using a 5-day week program requirement increases total storage requirements 
to 3.3TB per week. 
 Using a media management retention policy that allows six months of content 
to be kept in online storage adds up to 79.2TB. It’s also safe to assume there will 
be incremental growth in production materials. Plus, all proxies stay on the online 
storage, which adds 40% (including an allowance for overhead)—all of which 
adds up to 110.9TB of storage for six months of online. And this is only based on 
raw storage capacity. Add in digitizing legacy materials, and storage easily gets to 
petabytes. 
 To complete the planning, add in throughput considerations based on the 
design point for disk access times. The higher the bitrate, the more physical disks 
are needed. Using a disk conﬁ guration of Raid 5 with 1TB disks works out to 
approximately 150–175 physical disks. In addition to the disk, there are frames, 
power supplies, and controllers. The number of physical disks will impact other 
systems that attach to the storage. Each disk protocol topology differs so the type 
of network will determine the number of additional network ports needed. 
 How does it scale?  What are the determining factors in scaling? More users 
accessing the content (trafﬁ c) will require more Input/Output(I/O) throughput, 
and budgeting for trafﬁ c can be accommodated by scheduling some of the ﬁ le 
transfer processes to off-times when other processes are not working which will 
reduce the bandwidth and throughput requirements. Moving high-resolution ﬁ les 
to near-line storage and managing the mezzanine ﬁ les does reduce online storage; 
however, the proxies just continue to grow and need to remain online. 
 Now that the storage environment is designed and sized with adequate growth 
capacity and scaling considerations, it is time to discuss storage management. Stor-
age management is based on policies and these policies control and manage the 
workﬂ ow within the storage architecture. There are policies that control access, 

76
Media Management
availability, usage, and retention. Some of these policies are part of the asset man-
agement systems and some are within the automation system. There may be other 
systems that govern the way ﬁ le movement within the storage architecture is man-
aged. There are different policies for active and archived content. A migration policy 
for long-term archived media should also be taken into consideration. 
 Retention, Preservation, and Migration 
 There is a subtle difference between the retention and preservation policies. The 
retention policies manage the active content and how it moves between the dif-
ferent storage tiers: the lifecycle of media within the infrastructure. The retention 
policies deﬁ ne when mezzanine content is purged from the system, leaving only 
high resolution, when the high resolution is moved to ofﬂ ine archive and when 
the active high resolution is purged from the online and near-line to make space 
for new content. 
 The high resolution in the archive can be restored to any resolution when there 
is a request for retrieval. In the ﬁ le-based ecosystem, the proxy is always online and 
can be used for search and browse, creating EDLs and requests for partial restores 
of archived content. 
 The archive or preservation policy determines the long-term life of the con-
tent. Is it really saved forever? Preservation includes the digitization of non-ﬁ le 
or digital legacy materials. The transition from tape-based media to ﬁ le includes 
the ability to use all the archived media in a ﬁ le-based program. There are two 
ways to achieve this, one is to digitize all the tape-based content and the other is 
to digitize elements as needed or on demand. In both scenarios, the content needs 
to be encoded and ingested, then placed in storage, and get registered to the asset 
management system with metadata. 
 The on-demand method is more ad hoc and less costly. Using this philoso-
phy when non-ﬁ le-based content is requested that has been tracked in the library 
database (which hopefully has been integrated to the asset manager so that when a 
search is triggered, the content is identiﬁ ed as off-system) and in a different media 
form. The media manager or librarian would retrieve the content from storage 
(in this case from a shelf), ingest it to the system and attach the content to the 
original metadata tags from the library—indicating to which part of the content 
something new has been added. The content would then become available across 
the entire system with associated metadata. 
 A structured digitization schedule looks at the volume and types of legacy 
materials, prioritizing the order or value and setting up an area to ingest and tag 
with metadata the entire or selected library. 
 One of the US sports organizations implemented a ﬁ le-based system and then 
elected to digitize all 150,000 hours of their tape-based recorded footage. 
 What is a migration policy?  One of the practices established with ﬁ lm and 
videotape is the migration of archived media from the original media format with 

Media Management
77
unknown or limited shelf life to a media format with a longer shelf life that would 
sustain playback capability. Another consideration in migration policy is the 
treatment of media located on potentially unavailable or unsupportable playback 
devices. An example of this is transferring video from 1”, 2”, and Beta tape masters 
to anything from D1 to D5 to DVC. 
 Let us now discuss ﬁ le-based media. What ﬁ le formats will be sustainable? 
What media form factors will be sustainable? In the realm of digital tape (DLT 
and LTO), the compression schemes and tape densities change almost yearly. The 
lifespan of these tapes is unknown; optical disks may have a longer life span, but 
the actual ﬁ le format may not. Solid state and ﬂ ash memory stability and longevity 
are still unknown. 
 Therefore, migration is a critical part of retention and archive policies. What is 
a practical cycle to adhere to when archiving to digital removable media to ensure 
that the content requested is retrievable, available, and restorable? 
 The migration policy may include a random restore sampling both before and 
after the migration, comparing the ﬁ les and conﬁ rming their integrity. Part of the 
migration strategy may be to keep one of each version of software and hardware 
devices, enabling restoration from a variety of versions. 
 The digital librarian or media manager is responsible for maintaining the 
accessibility of the content throughout its lifecycle. 
 Keeping the metadata updated is essential. Migration can be an automated 
operation, and the asset management system may be set up to handle this. It is 
important that the record of the content tracks its migration if it is re-mastered or 
re-archived to a different format or removable media type. 
 Governance 
 Having brought up the topic of policy, it’s time to discuss governance. Governance 
encompasses all the rules and policies that control the entire lifecycle and move-
ment of media. It is extremely important. 
 These policy areas include: 
 
 Creative 
 
 File movement 
 
 Integration between systems 
 
 Rights control 
 
 Permission and approval 
 
 Distribution 
 
 Media management 
 In all software applications and systems, there are rules and policies that control 
how data is managed, accessed, and distributed. This is no different in the ﬁ le-
based media ecosystem. 

78
Media Management
 Governance establishes a set of rules and policies created by the stakeholders, 
who own the media and metadata. It controls the way all user groups and end 
users use the data. Meanwhile, each organization has its own operational structure 
and infrastructure. Governance enables business and production units to integrate 
their data, processes, and workﬂ ows to place higher value on the assets and gener-
ate new revenue opportunities. 
 Each process in the ﬁ le-based workﬂ ow has a governing set of rules and policies. 
It is critical that these policies are coordinated and integrated. 
 These are the rules and policies that the automation processes use to move 
media through the media management from ingest to delivery. 
 Permission and approval processes manage content for distribution. Parent-child 
relationships are how elements within a ﬁ le are tracked both separately and together. 
 Roles and Responsibilities 
 There are a number of new job deﬁ nitions and operations: 
 
 Media management 
 
 Ingest control 
 
 Digital library 
 The media manager performs a number of functions with a considerable number 
of new responsibilities. As the ingest manager and possibly the media logger for 
metadata, is this role an extension of the duplication department, that “dub guy”? 
Or since it is controlling the management of the media, is the digital librarian an 
outgrowth of the tape library? Or is this function possibly a completely new area 
called media management, encompassing the digital librarian, ingest, media, and 
metadata management AND quality assurance? 
 In the ﬁ le and stream ecosystem of media management, there are new roles 
and responsibilities. From a technical perspective, this is a quality control position 
conﬁ rming the integrity of the ﬁ les and streams as they enter and move around the 
infrastructure. This is where the quality and integrity of metadata and conﬁ rma-
tion of the automated metadata entries are also conﬁ rmed. 
 
 The ingest manager is responsible for the encoding and transcoding processes. 
 
 The media manager monitors the movement of ﬁ les between systems. 
 
 The digital librarian manages the metadata, media storage, and access. 
 Depending on the size of the organization, these could be treated as a single position 
or, in a larger operation, media management could be its own department. 
 Media management is a critical piece to this new puzzle. In the IP- and ﬁ le-based 
ecosystem, media is handled differently—with new rules and policies. Managing 
media requires a different tool set (i.e. asset management). 
 

 The next piece of the puzzle explores the changes to the engineering group, as well 
as what the engineer’s considerations should be when planning and designing for 
IP- and ﬁ le-based media. The primary role of the broadcast engineer is to plan, 
design, build, and maintain. The engineer has to ensure that all systems perform as 
they’re supposed to, all problems are resolved, and everything is up and running. 
There are also new responsibilities for the engineer; business continuity, security, 
and disaster recovery. 
 The technology design of the IP broadcast facility infrastructure is very net-
work, server, and storage centric. The video production switcher and audio mixer 
are servers located in a centralized equipment room and the controllers are 
only work surfaces connected over Ethernet in the control rooms. And while 
these devices are still proprietary technology, they are more server-centric plat-
forms with hardware interfaces to handle the SDI/AES audio and video. All these 
devices have the ability to store settings and proﬁ les in onboard storage or to 
removable media (USB Flash drives). Editing, graphics, animation, and play-out 
for distribution systems are all applications that run on servers and are connected 
to common storage, where the ﬁ les move easily back and forth between all the 
different operations. Media management and metadata (for description, busi-
ness, and automation) can be entered at the same time that craft production is 
taking place. 
 The core infrastructure is now substantially based on applications, servers, 
storage, and network. Application servers host the different tools that process 
and handle the media and metadata. There are processes that require proprietary 
or purpose-speciﬁ c hardware for the application to perform. Some of these are 
 ﬁ ve 

80
Technology Infrastructure and Engineering
graphics cards with their own processes and memory supporting multiple screens, 
encoding cards for SDI input and output and based on the type of storage topol-
ogy, iSCSI, Fiber Channel, or other network interfaces. There are still broadcast 
products that while being server based have a proprietary hardware conﬁ guration 
with embedded operating systems (OS) and embedded hardware-based processes 
vs. software. These “closed systems” require external applications, software inter-
faces, and storage to integrate them into the broadcast architecture. Most of them 
rely on dedicated hardware- or web-based browser tools to access their monitor-
ing and management tools without allowing access to the core applications. 
 In the IP- and ﬁ le-based infrastructure all devices and systems are integrated 
and control is a layer within the Ethernet topology and integrated on the net-
work. Where in a tape-based environment each system or device was independent 
and controlled using serial protocols (RS-232, 422, 485) that are point to point 
connections. In the IP infrastructure, the media manager and automation system 
are software applications that handle the transport and tracking of the ﬁ le as it 
moves through the system. Content enters through the encoder that’s controlled 
by the media manager and then moves into craft production where edit or graph-
ics applications perform their roles then making it accessible to the distribution 
application while insuring a copy goes into archive. In the IP- and ﬁ le-based eco-
system automation plays a much larger role. 
 Application servers move the media over the network and throughout the stor-
age architecture. As discussed in earlier chapters there are a number of different 
storage conﬁ gurations. In a Storage Area Network (SAN) conﬁ guration, a distrib-
uted disk architecture handles all the media coming in and going out, however, 
this is transparent to all users. In a Network Attached Storage (NAS) conﬁ gura-
tion, each storage location is separate and treated separately. In either of these 
conﬁ gurations, using a separate storage philosophy instead of a central storage 
philosophy means there are different storage systems integrated on the network 
and they are organized to provide storage based on different workﬂ ows. This may 
include ingest storage to acquire the ﬁ les, production storage to handle all of the 
craft, studio and remote content, archive storage, and storage for play-out. 
 Following the lifecycle of a ﬁ le, as content is created, and a program or element 
is ﬁ nished, it moves to the media library (storage) and becomes available for dis-
tribution and delivery to all of the different platforms. 
 In the IP production infrastructure the integration between systems and 
devices is based on networks. There are different network layers, protocols, and 
topologies. In the IP infrastructure, a router is a gateway and devices are con-
nected with a switch, and the switch can be a managed or unmanaged device. 
This has caused great confusion when having discussion about media systems 
design. There is the SDI router, which technically speaking is a matrix switch 
with dedicated inputs and outputs. It may be managed and controlled from an 
application on a server or from a computer using a browser that’s connected over 
an Ethernet network. Audio is embedded, control is a combination of RS422, and 
IP and intercom is VoIP. 

Technology Infrastructure and Engineering
81
 In the IP infrastructure bandwidth management is critical. The infrastructure 
uses shared bandwidth in a managed network topology. Bandwidth availability 
and network congestion is a constant management challenge in broadcast. Media 
ﬁ les use a lot of bandwidth, so keeping this segregated from command and control 
systems is crucial. Broadcast networks need multiple segments called virtual LANs 
or VLANs to segregate the communication between different systems and device 
interaction. Media, communications, control, KVM, and metadata each need their 
own segments. As the designer, the broadcast engineer must plan the conﬁ gura-
tion of the network. 
 One major consideration and concern in designing the infrastructure is to avoid 
or minimize latency. Interestingly, the issue of latency has more to do with transport 
of media, automation, communications, and command and control than it does 
with the syncing of audio with video.  That is not to say that visual latency (Lip Sync) 
is NOT a huge issue and there should be no such thing as “acceptable latency.” How-
ever, if the media (audio and video) is fully embedded and travels together, this 
type of latency becomes less of an issue. Back to the real concern about latency, if 
a control command is issued and the SDI router doesn’t execute a switch or an IP 
automation command to trigger the playlist is late, there will be an interruption in 
program delivery. The same is true if the command to begin an ingest or encode 
is late which means the start of record is late—there will be missed content. For 
example, in all sports there is clock data associated with the competition and if the 
clock data and video are not in sync it impacts the ofﬁ ciating of the competition. 
 Latency is not the only concern, as devices communicate with each other over 
the network, packets from one system may corrupt the communication between 
other systems. 
 The design of the network and managing the segregation between systems is 
one of the key considerations. Here is an interesting description to better under-
stand the relationship of VLANs to the broadcast system. Traditionally each of the 
different signal types will have different cable types and typically have their own 
routers (matrices). For example in a typical broadcast facility there will be routers 
for SDI, AES, MADI, Intercom, and RS422/232 plus an Ethernet switch. 
 Looking at the infrastructure bandwidth, the next generation of SDI is 1080P 
and 3Gb/s, which will also enable it to handle new formats with higher bitrates, like 
2K, 4K, and 8K. Manufacturers of audio video routers and terminal equipment are 
pressed to provide products that will support 3G. In the IP world, the next generation 
is 10Gb/s, 40Gb/s, and 100Gb/s which in comparison is much bigger than 3Gb/s. 
 In the IP architecture, each of these signals either is encoded to IP or originates as 
IP. Therefore they all use the same cable type (CatxE or ﬁ ber) and there is a common 
network switch. In the IP architecture they are differentiated and segregated by using 
VLANs. Multiple VLANs can co-exist in a single network, on the same cable/ﬁ ber 
and on the same switch. The switch is managed and controlled so that each signal 
is optimized and maintains the integrity to perform its function correctly. There 
are different ways to manage network trafﬁ c and the segregation of VLANs, Access 
Control Lists (ACLs), and Trunking will be discussed a little later in this chapter.   

82
Technology Infrastructure and Engineering
Signal
Format
Video 
MPEGTS, JPEG2000, H.264
Audio
Madi, CobraNet
Intercom
VoIP
Command and Control
xml
Machine control
xml
Router control 
xml
 FIGURE 5-2 
Signal
Cable
Connector
SDI Video
Coax/fiber
BNC/ST
AES/Madi Audio
Coax, multi-pair/fiber
BNC/Phoenix/ST
Analog Audio
Single/multi-pair
XLR/Bare end
Machine Control RS422/232
Multi Conductor
DB9, Phoenix
Management
Cat5/6e
RJ45, Punch block
Intercom
Single/multi-pair
XLR, DIN
Router Control
Coax/multi-pair/CAT5/6e
BNC/phoenix/RJ45, 
DB9
 FIGURE 5-1  Non IP signals, cable and connector types 
 So in the transition to an IP-based core infrastructure, as media is encoded to IP, 
the output of these devices is now a network port that goes to a port on a network 
switch. Each port on the switch can be conﬁ gured for the signal it is transporting 
and isolated if necessary. There is now only one cable type, two actually since ﬁ ber 
is used for higher bandwidths and for sending the IP over extended distances. 
 So in essence, each VLAN can be compared to the unique cables previously used for 
each of the signal types. Now, each of the signals are in the same switch and groomed 
into a single IP stream that travels over the same cables/ﬁ bers. As it connects to each 
device, the device understands which VLAN and signal it is taking commands from, 
sending or receiving ﬁ les and what process to execute or ﬁ le to manage. 
 Planning and designing the IP infrastructure requires knowledge in network 
architecture, an understanding how to conﬁ gure applications, servers, and stor-
age. Most of these devices are managed and operated with keyboards and mice- or 
touch-controlled Graphical User Interfaces (GUI); they no longer have physical 

Technology Infrastructure and Engineering
83
controls on the front of the devices or use specialized remote controls based on 
serial (EIA RS-232, 485, and 422) and other proprietary protocols. 
 In the design of the IP broadcast center, the server hardware is centralized, 
and extenders are used for Keyboard, Video and Mouse (KVM), this could easily 
require multiple keyboards, mice, and monitors at every workstation. To avoid 
this congestion and keep cabling at a minimum in control spaces, there are KVM 
switches and matrices. This allows multiple users in multiple locations to access 
the applications and servers remotely and from a single workstation. This is similar 
to having multiple SDI router control panels or using a RS-422 machine control 
router to facilitate more than one user or control room accessing a pool of sources. 
The KVM extender uses an IP transport and travels over the network topology as a 
layer. Actually because the KVM provides the main user display, this layer requires 
high bandwidth for the high resolution video feeding all the displays. This puts a 
tremendous load on the network and the KVM does not require interaction with 
any of the other LAN segments, therefore keeping it isolated in its own VLAN is 
critical or it will cause issues (latency or collisions) with other network trafﬁ c. 
One Keyboard, Mouse and Video display screen can control hundreds of serv-
ers and network devices; in addition multiple KVM stations can access multiple 
devices from multiple locations; the user can toggle between servers. While many 
IP-based systems are managed using a web or browser interface, there needs to 
be a computer to open the browser, in the case where the devices have dedicated 
applications that run on servers or workstations with software interfaces called 
dashboards, the KVM is used to access them. 
 The following case study is a good example of how using KVM provides power 
and ﬂ exibility in a production environment. 
 Case Study 
 A large global organization has 25 meeting rooms plus 5 production studios, each 
with HD cameras and multiple channels of audio that are recorded to ﬁ le and broad-
cast live. These rooms and studios all operate at the same time. There are over 3000 
IP addressable devices with 300 of them using the KVM matrix for management 
and control. There are over 50 locations for operator control positions, each position 
having a single KVM station in addition to the other control surfaces to execute 
production. The technicians use the KVM to toggle between systems and applica-
tions in order to record and manage the content workﬂ ow from each meeting room 
and studio.  
 Changes in Engineering and Maintenance 
 The interaction between business units and the handling of media has changed 
many of the workﬂ ows and business processes. Business models have also 
signiﬁ cantly changed for creating and distributing media. As media moves 

84
Technology Infrastructure and Engineering
throughout the entire infrastructure, it needs to be accessible to all departments/
units including production, distribution, library, legal, ﬁ nance, marketing, and 
business intelligence. These groups need access to the media to input and export 
metadata and to ensure that the appropriate business rules, policies, and permis-
sions are in place so they are protected as the metadata is distributed into the 
marketplace. 
 In the ﬁ le-based and IP architecture, that premise remains the same; however, 
the engineering department now needs additional knowledge and skill sets. Previ-
ous puzzle pieces described changes in production workﬂ ows. This piece covers 
the challenges and opportunities the broadcast engineer faces when designing for 
ﬁ le-based and IP workﬂ ows particularly as an evolution from a tape-based SD/
HD-SDI facility. 
 Traditionally, IT engineers and managers have had different priorities for 
broadcast/production engineers. The requirements of an enterprise IT architec-
ture are substantially different from those of the broadcast IT architecture. Both at 
their core build on servers, applications, network, and storage. Both use commod-
ity hardware and software including operating systems and databases in addition 
to proprietary hardware and software. However, there are signiﬁ cant differences 
in the types of applications and system conﬁ gurations, and the two have very dif-
ferent priorities. 
 It’s time to highlight some of the differences between  Enterprise  IT and  Broad-
cast  IT engineering. 
 Broadcast  is all about getting on the air and staying on the air.  Enterprise is 
about security, communications, accessibility, web services, and storing docu-
ments. Enterprise has the luxury of taking systems ofﬂ ine for routine maintenance. 
Broadcast does not have that luxury. 
 The ﬁ rst and foremost difference is system availability. The broadcast IP infra-
structure has to be online and available 24/7 with no downtime. Bandwidth man-
agement in the broadcast IP network is critical. On the enterprise side, bandwidth 
management is less critical, except when users are watching movies online and 
interfering with business operations. 
 One of the most common differences between broadcast and enterprise is the 
need for urgency in solving problems. In the enterprise IT world, problems are 
solved by calling the help desk, opening a trouble ticket, and waiting for a response. 
Other than catastrophic failure, there is typically not the same sense of urgency 
in addressing enterprise problems. And when it comes to diagnosing and solving 
a problem or disruption in service, the Enterprise IT approach is to ﬁ rst forensi-
cally understand where the problem came from before restoring service. In the 
broadcast environment, if there is a disruption in service, the highest priority is to 
identify the problem, restore the service, and ONLY then, fully diagnose and solve 
the problem so it doesn’t interfere with services again. 
 In the broadcast world, the engineer knows that systems need to be working 
and staying on the “air” is the most important responsibility. In broadcast there is 
NO DOWNTIME! 

Technology Infrastructure and Engineering
85
 On the enterprise side, routine maintenance schedules take systems off-line. In 
broadcast, that is not really an option; services cannot have interruptions, so main-
tenance needs a process to work around production and distribution schedules. 
 The traditional role of the broadcast engineer is to design, build, and maintain 
all systems and equipment. Simply stated, their responsibilities are to keep every-
thing on the air and ensure there is no interruption of broadcast and production 
operations. 
 The broadcast engineer is now also responsible for multi-platform integration 
and systems administration in the IP ecosystem. 
 In each of these areas, the IP- and ﬁ le-based architecture is substantially dif-
ferent from the infrastructure of an SDI facility. In the migration from analog to 
digital to SD-SDI and then HD-SDI, the core principles in the architecture stayed 
the same. The core of SDI systems is the audio/video router with the produc-
tion devices being connected using the usual combination of distribution ampli-
ﬁ ers (DA), Analog to Digital and Digital to Analog converters (A/D and D/A) 
plus Frame Synchronizers, cross converters, and the rest of the “technical glue” 
technologies. It’s interesting that the management of these systems is through a 
proprietary software application or a browser interface accessed over the network. 
 In the IP ecosystem, the core of the broadcast infrastructure is  Ethernet rout-
ers and switches . IP is a duplex path with audio, video, command, and control; 
communications are on the same stream, cable and port on a device. The devices 
are encoders and transcoders, servers and hard drives. Incoming signals are IP or 
encoded to IP or as a ﬁ le transfer. 
 Quality control has changed; SDI signals are monitored using digital wave-
form monitors and VectorScope. Quality control now includes Quality of Service 
(QoS) and Quality of Experience (QoE). The engineer needs to understand the 
way trafﬁ c moves across the network, as well as the requirements for each delivery 
platform. 
 Planning for the Future 
 When planning and designing a facility, it is critical to try and anticipate growth. 
The core technology infrastructure is different and has several new considerations: 
 
 Extensibility 
 
 Scalability 
 
 Sustainability 
 Extensibility : A design principle where the implementation takes into consider-
ation planning for future growth. 
 
 Are the systems being designed anticipating growth? 
 
 Is there enough spare capacity in the base design? 

86
Technology Infrastructure and Engineering
 With technology changing as rapidly as it does, it is essential in the planning and 
design of a facility to assure that extensibility provides for and  anticipates future 
growth. 
 Scalability : This is the principle of selecting technologies and designing the sys-
tems that allow for incremental growth without needing to replace entire systems. 
 
 Is the system able to grow and how? 
 
 What is the growth based on? 
 
 Is there an increase in the number of people or systems that need access? 
 
 Is it servers, storage, network, or application licenses? 
 
 Are there enough ports on the switch? 
 
 What about bandwidth on the network and in storage? 
 
 Are there more content production, distribution channels, or delivery 
platforms? 
 Scalability is viewed slightly differently in the IP architecture. The core design of a 
traditional broadcast and production facility is centered on the capacity of audio 
and video equipment, terminal and distribution equipment, and size of the A/V 
router. AND most, if not all, engineers will agree that routers are always too small 
(not enough inputs) by too small (not enough outputs). In the SDI (baseband) 
environment, the task of adding an edit suite or an additional VTR for production 
or transmission was never a small task. There are many connections to infrastruc-
ture and terminal equipment with different cable and connector types that are all 
hard wired to speciﬁ c devices. 
 In the IP architecture, scaling, growing, or expanding is accomplished by adding 
servers, storage, and applications. Are there enough network ports? Is there enough 
bandwidth? What about throughput? 
 Sustainability : This has as much to do with Total Cost of Ownership as it does 
with maintenance. 
 
 Keeping technology productive and available over its usable life 
 
 What is the lifecycle? Does it have a sunset timetable? 
 
 How is it maintained? Hardware vs. Software? 
 
 Is there a service level agreement (SLA) with the manufacturer? 
 
 When is the decision to replace vs. repair/upgrade made, with so much hard-
ware and software changing and going out of date so quickly? 
 Maintenance is a different beast in the IP environment. Maintenance was always 
caring for a broad spectrum of equipment (e.g. cameras, switchers, monitors and 
mixers, routers, and terminal equipment) and keeping a cadre of spare parts and 
consumables on hand. The engineer now faces additional responsibilities with the 
maintenance of software, servers, networks, storage, and digital libraries. There 
are new little black boxes for adapting devices, converting signals, and interfacing 
systems. 

Technology Infrastructure and Engineering
87
 Even spares have a different concept, there are hot spares and cold spares. Cold 
spares sit on a shelf or can be integrated and wired in a rack and powered down. 
In the event of failure or maintenance the cold spare is brought online and the 
primary taken ofﬂ ine. Hot spares are devices that are online and conﬁ gured to 
failover in the event of a problem. This could be an extra card in a frame or a 
redundant server. There is also hot swappable, which is the ability to remove a 
failed component and replace it with a working component without power cycling 
the system. Thinking about what types of spares need to be kept on hand is an 
open and constantly changing question. How systems are maintained has changed 
substantially. This includes: 
 
 Software patches 
 
 Software upgrades 
 
 Bug ﬁ xes 
 
 Hardware 
 
 Cold spares vs. hot spares 
 Total Cost of Ownership 
 All businesses maintain budgets, and engineers are typically required to create 
capital and operating budgets to support a facility. Total cost of ownership (TCO) 
is all the costs associated with operating a device or system. 
 
 Annualized maintenance costs for software and hardware 
 
 Hardware replacement vs. upgrade 
 
 Software version upgrades 
 
 Sunsetting hardware and software 
 
 Infrastructure – Electric, Environment 
 When we look at total cost of ownership, some of the costs formerly associated 
with spare parts inventory are now allocated to annual maintenance called service 
level agreements (SLA) for hardware and software. This can be based on as much as 
15–18% of the original cost of the product. When looking at TCO there is a higher 
probability that the hardware will be replaced long before it actually expires. 
 When a vendor releases a new software version or a new application, they may 
require different hardware, faster processors, more storage, or more memory. This 
means that the software application and the hardware it runs on, which was per-
fectly ﬁ ne yesterday, now needs an upgrade! And that server that is only two years 
old and running ﬁ ne is not perfectly ﬁ ne for the new application. We are now 
seeing that, long before a device reaches the end of its lifecycle, there is the likely 
scenario of a potential upgrade or replacement. The product lifecycle of servers, 
disk, and ﬂ ash and digital tape storage is different from the lifespan of a video tape 
machine and magnetic tape. 

88
Technology Infrastructure and Engineering
 Another major difference in the transition to software-based systems is that 
vendors are upgrading versions very quickly, and they are releasing completely 
new versions that are more substantial than a new feature update or a bug ﬁ x. As 
vendors and service providers introduce new versions with new features and new 
functionality, they very strongly encourage users to buy them as they abandon 
support of the previous versions. This is gracefully described as “sunsetting.” 
 This creates a serious dilemma. There may be no requirement for new fea-
tures, and while there is nothing wrong with the current software version or the 
hardware that it’s running on, the vendor may no longer support it, despite the 
maintenance fees they are charging for service. Hardware and software are reach-
ing their end of product life on much shorter cycles than traditional broadcast 
equipment ever did. 
 For the engineer, building the IP infrastructure means installing and conﬁ gur-
ing software onto servers, using remote access to conﬁ gure switches, and being 
familiar with storage architecture to set up partitions based on user requirements 
and manufacturer speciﬁ cations. This is in addition to the specialty hardware and 
software for media services, like devices or adapters for encoding, decoding, trans-
coding, splicing, and grooming. 
 The broadcast infrastructure is still very much a hybrid of SDI and IP. The 
major studios and remote production units that produce programming continue 
to deliver it on HD digital tape, as do advertisers, so the broadcast engineer has to 
maintain a way to manage the tape and its content as it is being encoded to ﬁ les. 
The design needs to include interfacing all the software applications. 
 The IP architecture still needs maintenance; quality control now includes 
packet loss analysis and bit error detection. Software maintenance mostly includes 
updating service contracts, installing patches and upgrades, or ﬁ nding a bug that 
is causing service disruption. 
 Hardware is a bit different .  Hard drives in servers can be replaced; any devices 
that have cards or blades can be replaced. Power supplies still fail. Typically in 
server, storage, and switching products, a repair is considered swapping out a card 
or disk. Thus, the broadcast engineer is also a network and software engineer. 
 The design of the IP architecture includes delivery to multiple platforms. This 
encompasses formats, ﬁ rewalls, APIs, and software interfaces. 
 The broadcast network has a controlled interface to the enterprise network. 
The broadcast engineer is the systems administrator for the broadcast network 
and needs to interface with the enterprise system administrator to maintain seam-
less transactions across the two networks. 
 The IP infrastructure is a multi-layered topology of networks: 
 
 Media Transport 
 
 Automation 
 
 Command and Control 
 
 Management 
 
 Communication 

Technology Infrastructure and Engineering
89
 The core network is based on Ethernet. The storage uses a variety of different net-
work protocols and interfaces (e.g. Ethernet, Fiber Channel, iSCSI, and Ultra-
Wide SCSI). These are closed network layers that move large media ﬁ les between 
servers and have storage at higher bandwidth without adding congestion to the 
core network. All signals ultimately travel over the same IP topology. There are 
many layers within these layers; the Ethernet layer has the VLANs that manage 
audio, video, communications, command and control, automation, and manage-
ment. Each layer in the network has different performance requirements, and 
it is the engineer’s responsibility to design and build the network in a way that 
supports this. 
 The design should consider the relationship between the network segments 
and how the content that travels to and from the core business and production 
networks serving Over the Air and Internet delivery. Each of these network seg-
ments must be protected and isolated. This can be done by using ﬁ rewalls between 
each of the business units, managing the crossover between the enterprise IT and 
broadcast network. 
 There is overlap in many areas; at times, the two engineering groups need to 
work together to provide continuity in operations while still maintaining the nec-
essary separation. 
 The ﬁ rewalls control access and regulate bandwidth. It is interesting that, while 
it is important to keep the heavy media trafﬁ c on the broadcast network, there is 
a consistent growth in streaming media access as well as services like video con-
ferencing and IPTV that are on the enterprise side of the network. These growing 
services are creating new issues of network congestion and bandwidth usage. 
 The two IT groups have different knowledge and skill sets that need to come 
together to design the IP topology for a media-centric production environment. 
The broadcast engineer can identify the critical areas where latency and QoS can-
not be compromised, and the enterprise IT engineer can identify where trafﬁ c 
on the network for business continuity and security cannot be compromised. A 
robust network plan can be easily created with all the requirements and param-
eters identiﬁ ed in the planning stages. The enterprise network engineer has experi-
ence in VLANs, trunking, and creating the access control lists (ACLs) that will be 
critical when conﬁ guring the broadcast network. 
 The enterprise IT group typically has the primary Internet connection that 
controls all remote access and provides any wireless services. The IP broadcast 
center now more closely resembles a data center. This brings with it the need for 
engineers to develop new skills. The new tools of the trade are a keyboard, mouse, 
and monitoring systems that are browser-based dashboards. Meanwhile, the days 
of the greenie and tweaker are sunsetting like so many other things. 
 Broadcast and production technologies are based on applications, servers, stor-
age, and networks. The processes and technologies that are acquiring, transport-
ing, managing, transforming, and delivering media are all applications that run on 
servers. The engineer has to deal with operating system (OS) updates, and applica-
tion upgrades, and being concerned about compatibility. When an application gets 

90
Technology Infrastructure and Engineering
updated or upgraded, they must know how many other applications are affected 
and whether there is an impact on hardware and/or device interfaces. 
 Media and metadata are organized in digital libraries that live in the stor-
age architecture. These are hard disk arrays with multiple servers managing the 
disks. There is middleware (like a data director) that handles the communication 
between the media applications and the storage architecture. All these technolo-
gies are integrated over different kinds of networks. The digital library can be a 
robotic tape device. 
 A network analyzer monitors this infrastructure; there are management tools 
for each application and service. 
 Once the design and build are complete, the engineer is responsible for keeping 
the facility and systems functioning correctly. This includes testing and monitor-
ing to maintain signal quality. 
 There are a number of test and measurement tools that are used. When the 
transition to HD/SD-SDI ﬁ rst occurred, new monitoring tools were introduced 
and traditional waveforms changed to digital waveforms. And with all the embed-
ded information, the monitoring needed to be able to conﬁ rm the integrity of the 
entire signal. 
 In the production environment, there is still a video operator that needs test 
and measurement tools to shade cameras and assure quality. The audio engineer 
has broader responsibilities with multi-channel programming and uses software-
based tools for test and measurement. 
 In master control, systems are typically monitored by alerts and alarms that 
make a sound or light up. In the IP-based ecosystem, alerts from software-based 
systems have become colored icons on screens and can send an email or text mes-
sage. The engineer needs to make decisions on what type of alerts and alarms to 
design into the IP system, as well as how the response is managed. Many systems 
can be accessed remotely by a browser or KVM. The engineering shop looks very 
different these days: it is now computers, applications, and multi-view displays 
showing dashboards of servers. File and stream analyzers are applications on 
servers. 
 Transmission is just one of the many distribution chains and is much more 
than an RF signal. Transmission monitoring extends beyond spectrum analysis. 
Over the air transmitters are now fed by an ASI or IP stream that carry multiple 
channels embedded in a single stream. Files are delivered to On-Demand broad-
band, cable, satellite, IPTV, and mobile service providers. Streams are delivered 
to content distribution networks (CDN) for broadband and mobile distribution. 
 In broadcast, test signals and monitoring tools were created to ensure that all 
systems and devices could be calibrated to the same set of parameters and meet 
their performance speciﬁ cations. In the digital and IP environment,  nothing has 
changed philosophically ; test signals and measurement equipment have simply 
evolved to address SDI and IP. 
 Interestingly enough, the broadcast industry still uses a variant of color bars 
as the basic setup for video color reference and levels, as well as a “pilot” tone for 

Technology Infrastructure and Engineering
91
normalizing audio. There are new video test signals and patterns used for testing 
MPEG and SD/ HD-SDI with better audio test signals for normalizing systems. In 
IP and ﬁ le broadcast and production systems, there are NEW devices and systems 
to monitor with new ways to test and measure them. Media needs to be tested 
prior to and post encoding and compression. In the IP- and ﬁ le-based ecosystem, 
testing ﬁ les is different than testing streams. 
 SD/HD-SDI and surround sound brought a new layer to engineering skills 
and knowledge base. Test and measurement for SDI introduced Eye Waveforms, 
Gamut, Jitter, and Timing as waveforms for analyzing SD/HD-SDI signals. While 
SDI is considered uncompressed full bandwidth video, it is an MPEG2 stream with 
multiple channels, there are multiple AES audio channels embedded in the stream 
and the vertical interval is nonexistent. However there are still many of the same 
services that need transport within the main signal stream. These services and data 
sets that used to go into the vertical interval, on Line 21 and other places in the 
NTSC signal, now go into ancillary channels within the MPEG proﬁ le. These are 
called the Horizontal Ancillary (HANC) or Vertical Ancillary (VANC) and Auxil-
iary (AUX) channels within the MPEG stream. SDI is managed very similarly to 
the way analog was with discreet monitoring and measurement devices. There are 
multi-viewers that show many program streams in a multi-screen display. How-
ever the test and measurement tools still work on a single program stream, the 
output of the test equipment can be shown on the multi-viewer. 
 There are new waveforms and the engineers and operators use these waveforms 
to monitor and manage the audio and video levels. 
 There is component gamut, vector and eye waveform, plus the monitoring tools 
include analysis data showing cable loss details as well as other video analytics 
(e.g. Jitter, Gamut, and MPEG resolution). The engineer needs to read these wave-
forms, understand what kind of defects or anomalies would show in the MPEG 
SDI stream, and then determine what steps and devices could correct them. 
 Moving to IP- and ﬁ le-based systems introduces a new set of skills and knowl-
edge base for the broadcast engineer. IP streams over Ethernet and other transport 
and is monitored differently. The characteristics of IP- and ﬁ le-based video are dif-
ferent than SDI. For IP streams and ﬁ les, SDI tools do not represent what is actually 
going on. True, they show a decoded representation of the signal, but they do not 
show the IP stream or ﬁ le in its native form without introducing their own artifacts. 
 In the IP media architecture, the network has an equal if not greater role than 
the routing and distribution architecture in SDI. IP media moves as streams, or 
ﬁ les in packets, between devices over a network. The new tools in the engineer’s 
kit include packet analyzers, bit error detectors, network bandwidth analyzers, and 
packet loss detectors. 
 Meet the new broadcast engineer: his tool chest is a laptop, and his monitor is 
a KVM device with access to a variety of software applications that measure and 
validate the IP streams and ﬁ les, devices, and network. 
 Some of the characteristics the stream analyzer looks at are encoding errors, bit 
error, and packet loss. For ﬁ le analysis, there are other parameters that are analyzed 

92
Technology Infrastructure and Engineering
(e.g. checksum, syntax errors, frame rate, freeze frame detection, audio loss, and 
audio loudness). 
 In addition to monitoring stream and ﬁ le quality, the broadcast engineer needs 
to monitor network trafﬁ c; the Studio to Transmitter Link (STL) is now an Ether-
net circuit. The encoding and transcoding processes must be monitored to prevent 
artifacts or latency from being introduced to the stream or ﬁ le. These analysis 
tools for IP and ﬁ les are applications running on servers on the network. One 
consideration in conﬁ guring the analysis tools is the impact they have on network 
performance and access to ﬁ les. 
 File analysis tools can run faster than real-time. These tools may add over-
head to the ingest time, whether it’s an encode or ﬁ le transfer. Stream tools run 
in real-time. Both are a heavy burden on the processor, so it will need a dedicated 
server. 
 Broadcast engineers have different priorities from those of IT engineers. Both 
are responsible for ensuring that the core infrastructure supports business opera-
tions. The broadcast engineer’s priorities are quality assurance of the content, data, 
and delivery—getting on air and staying on air. There is NO acceptable downtime. 
The enterprise engineer focuses on network trafﬁ c, systems backups, and security. 
In contrast, taking systems ofﬂ ine or out of service for routine maintenance is 
typical for the enterprise IT engineer. 
 Latency is a major concern and should be an important consideration. While 
the broadcast architecture has become IT centric, it still requires a media-centric 
engineering philosophy and mind set. IT engineers and managers have had differ-
ent priorities from broadcast/production engineers. 
 Traditionally, network latency, synchronization, and timing were of no concern 
to IT, as they do not impact business operations or the performance of business 
applications. Broadcast engineers understand the needs of media, while IT engi-
neers understand servers, storage, and networks. Both are valid, and it is important 
to recognize the differences and how to support each other. 
 Both broadcast and IT engineers are concerned about stability, integrity, and 
uptime. 
 Broadcast operations are 24/7/365. This presents major challenges for routine 
maintenance and taking systems ofﬂ ine. Network congestion is not typical when 
moving documents or spreadsheets and accessing databases. Now, enterprise IT 
engineers are challenged with streaming media, online webinars, video confer-
ences, IPTV, and Skype. Latency comes in many colors and ﬂ avors, such as ﬁ les 
not arriving, command requests being delayed, as well as the most well-known 
latency between audio and video, introduced in the encoding or decoding process 
referred to as LIP SYNC. 
 More than in enterprise IT, in broadcast IT there is considerable automation to 
move ﬁ les around. On the enterprise side, a document may be shared or multiple 
users may have access to a shared database within an application. In media, the 
ﬁ le actually moves locations between ingest, archive, production, and distribu-
tion. There is automation in all aspects of the IP architecture and workﬂ ow. There 

Technology Infrastructure and Engineering
93
are different types of automation that perform multiple operations. Automation 
includes software applications that interface with other applications running on 
servers or dedicated devices. 
 On the enterprise side, a document doesn’t change format. A Word document 
stays a Word document or may become a PDF. On the broadcast side, not only 
could the format change, but also there are myriad versions based on distribu-
tion platform. For example, a .mov or .qt may become an MPEGTS, MXF, GXF, 
or LXF and then an .ﬂ v, Mp4 or .wmv—and at different bitrates from 250Mb/s 
to 500Kb/s. 
 Broadcast engineering in the IP and ﬁ le ecosystem opens new opportunity for 
skills and knowledge. In addition to the full complement of HD-SDI systems and 
devices, there are new devices and new architecture. The broadcast engineer is 
re-deﬁ ned. They are now responsible for: 
 
 Applications and OS installations 
 
 Software conﬁ gurations and management 
 
 Database management 
 
 Software and hardware maintenance and upgrades 
 
 Network engineering and management 
 Media is encoded to a stream or ﬁ le either directly out of the camera or in a pro-
duction control room. From there, it enters the IP and ﬁ le ecosystem, where engi-
neering and quality control change. In addition to audio and video production 
devices, engineering responsibilities now include servers, storage, network, appli-
cations, and database management. 
 It is fair to say that all broadcast and production equipment is essentially com-
puters and/or servers. Even the control surfaces and heads of SDI and AES pro-
duction devices are IP-based processors. Cameras have onboard processors and 
even lighting instruments have smart controllers built in. Microphones are one 
of the few holdouts. 
 As in all broadcast operations, these devices need to run 24/7 and be main-
tained without interfering with operations. The broadcast engineer is now also a 
network administrator, responsible for VLANs, SANs, NAS, and WANs. Quality 
control analysis tools are applications running on the network. In the IP and ﬁ le 
ecosystem QoS (Quality of Service) and QoE (Quality of Experience) are the new 
measures of performance acceptance. 
 Now let’s look at the integration of business systems into the broadcast and 
production systems.   Figure 5-3   shows, from the engineering perspective, which 
systems are under automation, where they interface to other systems, and how they 
integrate to form a whole architecture. 
 Other pieces to this puzzle captured the crossover between enterprise IT and 
broadcast, where the systems are intermingled and need to be able to communi-
cate seamlessly. As the picture of this new puzzle takes shape, these pieces look at 
the workﬂ ow integration and at the way the business (enterprise) network and 

94
Technology Infrastructure and Engineering
broadcast network integrate within the infrastructure. There is no clean separa-
tion between the broadcast and network segments; there are user groups and sub-
systems that need access to both broadcast and enterprise. 
 It is important to take a big picture perspective of the systems and subsystems 
when designing the IP infrastructure. It is imperative in building a new mainte-
nance philosophy to understand all the hardware and software interfaces in the 
various production and enterprise systems. 
 The changes in the engineer’s roles and responsibilities for designing, build-
ing, and maintaining the broadcast facility is another piece of this complicated 
puzzle.     
 FIGURE 5-3 

   The next set of puzzle pieces are transmission and delivery. Previously, whether 
sent to an over the air transmitter, cable, or satellite provider, the content was the 
same. There were differences in the programming but not in the format or media 
type. Not anymore. 
 Today, there are different delivery platforms and various distribution methods. 
Each has different considerations to meet the new delivery requirements. There 
are a number of different paths for distribution and media is formatted differ-
ently for each platform. Platforms today include Over the Air (OTA), Over-the-Top 
(OTT), web stream, web On-Demand, cable and satellite with On-Demand, smart 
TVs, tablets, smartphones, and gaming consoles. 
 Transmission includes IP as the new STL and the interconnection between dif-
ferent locations across the network (e.g. afﬁ liates, station groups, production cen-
ters, and remote venues like sports arenas and stadiums). 
 The entire media industry is focused on the “second screen” and how to capital-
ize on it. There are some that consider non-traditional TV devices the ﬁ rst screen—
an interesting concept that will be explored a little further into this chapter. 
 When media is delivered to smart devices, the consumer is expecting additional 
information and services. Metadata is not just for managing media within the 
production workﬂ ow. 
 So, is TV—or better said, the large screen—a thing of the past? What about 
cutting the cable? 
 Actually, it’s not likely. It is certainly true that the term “TV” or “television” 
(meaning a one-directional channelized display device) has changed and now we 
use it more to describe the type of program content we watch. 
 six 

96
Transmission and Delivery 
 Before we discuss multi-platform delivery though, let’s look at a few important 
terms: 
 
  Television (TV) is a telecommunication medium for transmitting and receiv-
ing moving images with or without accompanying sound. 
 
 A  television set  (also called TV) is a device that combines a tuner, display, and 
speakers for the purpose of viewing television. 
 
 A  television program  (also called a television show) is content that is intended 
for broadcast on television. 
 
 A  broadcast network  is an organization that provides live or recorded con-
tent, such as movies, newscasts, sports, public affairs programming and other 
television programs for broadcast over a group of stations. 
 
  Broadcasting  is the distribution of audio and video content to a dispersed 
audience via any audio-visual medium. 
 Looking at the above deﬁ nitions, it’s reasonable to say that streaming media and 
webcasting can now be accepted as a form of broadcasting despite the lack of 
physical broadcast stations—in which case, its service providers COULD poten-
tially be considered broadcasters or even broadcast networks. 
 This may be a bit of sacrilege! Cable and satellite networks have been accepted, so 
the leap of faith to acknowledge that broadband, online, On-Demand, mobile, and 
portable media delivery systems are also networks shouldn’t be that complicated. 
 The broadcaster or the program origination center delivers content to all these 
different types of networks, each having its own set of parameters and production 
 FIGURE 6-1 

Transmission and Delivery
97
requirements. In addition to the main program content, electronic program 
guides (EPGs), descriptive metadata, rights management, and protection are all 
companions to the programming. 
 The second screen is an addition to the viewing experience that is being used to 
augment and enhance user experience. Metadata is a valuable tool necessary to mon-
etize delivery to the second screen. Using sports as an example, fans are interested in 
getting statistics and player information, possibly playing a fantasy version of their 
favorite sport in a social community while watching the live sporting event. Being able 
to access all this is based on what metadata is included with the programming.  
 The basic concept of transmission and delivery is the same in the IP and ﬁ le 
architecture. It’s only the technologies that are different. Here is a good place to look 
at cutting the cable. This is the concept of using Over-the-Top and Internet services 
to view content, bypassing cable, satellite, and IPTV service providers. It doesn’t 
quite work that way. While online services are moving into the content origina-
tion business (i.e. YouTube, Amazon, Netﬂ ix, etc.), one of the ﬁ rst questions to ask 
is—how do users get their Internet? Hmm, cable, IPTV, or telephone company. So 
similar to when cable and satellite were introduced and it rang the death knell of the 
broadcaster, that didn’t quite happen. If users start watching programming from 
different program originators using Internet, the same service providers, i.e. cable 
and telephone companies, will change their rate structure and begin billing based on 
bandwidth usage. So it’s not really cutting the cable, more like changing it. 
 Back to transmission and delivery, once content is ready to leave the broadcast 
center, it is formatted for each of the different platforms. It will leave the broadcast 
center as a ﬁ le or stream as ASI, IP, or as ﬁ les in multiple formats, with protocols 
such as RTMP, RTSP, HTTP, and MPEGTS. 
 Many of these platforms use the same protocol: 
 
 ASI and IP transport streams feed digital transmitters, cable, satellite, broad-
band, and IPTV. 
 
 RTMP Adaptive and Smooth Streaming and HTTPLive Streaming all feed to 
the web, tablet, and smartphone. 
 FIGURE 6-2 

98
Transmission and Delivery 
 Files, however, are handled differently. Typically they are delivered to a content 
distribution network (CDN) that handles all the end user requests, authentication, 
replication, and load balancing. 
 Transmission is also about contribution and remote production. There are tech-
nologies that can multiplex cameras, that transport the ISO feed, provide return 
video and communications with the production control room back at the broadcast 
center, where it can be fully controlled, integrated with all the broadcast systems, and 
produce a ﬁ nished program for broadcast or live. In a ﬁ eld production, where the 
media is captured to a disk, a server, or even a laptop, if there is sufﬁ cient bandwidth 
or a dedicated return path (backhaul), all the media is uploaded back to the broad-
cast center. News crews will use the open Internet and with technologies that can 
bond broadband that enables them to get enough bandwidth to transmit over cel-
lular networks. 
 This raises other questions: 
 
 Where is the media going and how does it get there? As a ﬁ le or stream? 
 
 Which container or wrapper will be used—MXF, LXF, BXF, QT? 
 
 What about ﬁ le size vs. bitrate? How many types of formats? 
 
 Does it need protection? If so, what kind—DRM, watermark, or encryption? 
 It is also important to look at where the media is going. 
 
 What type of client device and what type of delivery bandwidth are 
available? 
 
 Is it contribution to a station or afﬁ liate? 
 
 Is it going Over-the-Air, Over-the-Top, or through broadband? 
 
 Is it a ﬁ le or stream? 
 
 Is it compressed or transcoded to a speciﬁ c ﬁ le size and/or bitrate 
requirement? 
 
 How many different bitrates are needed? (This is based on browser and player 
types PLUS client bandwidth limitations for distribution.) 
 
 Is it watermarked for tracking and clearances? 
 
 What about metadata elements and encryption for managing and controlling 
subscriber access? (This is next generation conditional access.) 
 These are all decisions that need to be made and established as a set of rules that 
govern distribution. 
 The format decision is based on the workﬂ ow and internal needs of the organi-
zation. There will be a consistent change in delivery formats. H.264 ~ H.265 and 
more on the horizon . . . MPEG7 and MPEG21 are in committee. 
 While delivery formats are striving for smaller ﬁ les and streams, production is 
looking towards 1080P, 4K, and 8K as production resolutions. 
 Resolution standards should be based on the entire workﬂ ow; one resolution 
does not address all needs.   

Transmission and Delivery
99
 The content delivery landscape is ﬂ uid. We are constantly expanding the num-
ber of platforms and devices that are getting content and making content useful 
and device appropriate so that it can be monetized. 
 
 Over-the-Air (broadcast) and mobile DTV 
 
 Cable, satellite 
 
 IPTV 
 
 TV everywhere/Over-the-top 
 
 Mobile 
 
 Broadband Live and On-Demand 
 
 Portable players—tablets, wearables 
 
 Media consoles—Roku, Apple TV, TiVO, VuDu, etc. 
 
 Gaming consoles—Wii, Xbox, PS3, etc. 
 It is important to mention that reaching the maximum number of end users is 
the Holy Grail of all program creators and that’s what leads to the second screen. 
Each of these platforms has its own speciﬁ cation for playing media in addition to 
a different production requirement to maximize the effectiveness of the platform. 
There is some commonality with variations for each platform when it comes to 
compression and delivery formats. When Over-the-Air became digital, it enabled 
multiple channels in the same spectrum. Now the mobile DTV standard which 
enables mobile devices to receive Over-the-Air has been approved and stations are 
upgrading their transmitters to enable the addition of mobile channels to mobile 
devices with ATSC tuners. Cable, satellite, and IPTV are using set top boxes 
(STBs) with DVR capability. This uses the STB with OTT as the player instead of 
streaming and using bandwidth. At the same time, service providers are offering 
bundled services with wireless services for second screen delivery within the same 
premises as the STB. Portable players can download media, while gaming consoles, 
phones, tablets, and web can both download and stream. 
 FIGURE 6-3 

100
Transmission and Delivery 
 Case Study 
 In addition to producing content in the broadcast center for multiple platforms, 
US sports producers are using live cameras at events and streaming directly to 
online distribution. In addition, technology providers are building integrated 
production in box systems that can pull from wireless and mobile devices into a 
production switcher/mixer and create ﬁ nished production switching live between 
different wireless devices and send stream directly to a CDN. 
 FIGURE 6-4 
Format
Name
Bitrates
File Size
Platform
MPEG2 4:2:2) @ MP/HL XDCAM
25, 35, 50Mbit/s
18 30 GB/Hr. Production
MPEG2 4:2:2)@ MP/HL
DVCPRO
50, 100 Mbit/s
30- 60GB/Hr. Production/Library
MPEG2 4:2:2)@ MP/HL
ProRes 422/
DNxHD
147, 220 MBit/s
100GB/Hr.
iTunes/Production/
Library
MPEG2 4:2:0 @MP
DVCAM/
Firewire
25 MBit/s
15GB/Hr.
Production
MPEG4 
Blu-Ray
40Mb/s
6GB/Hr.
DVD
MPEG4 Part 2
H.263
700K-3Mb/s
N/A
Video 
Conference/Web
MPEG4 Part 2
H. 263
700K-3Mb/s
300MB
You Tube
MPEG4 Part  10 
H.264
700K-3Mb/s
300MB
Web, Mobile, 
Flash
MPEG4 
H.264/AVCHD
100
16GB/Hr.
Production
HTML5
Open Source
Apple iPhone, 
iPad,
 The decisions on codec, bitrate, and container impact many systems and pro-
cesses in the IP architecture. Some of these decisions are driven by quality, tech-
nology choices, and budget, while others are driven by the ﬁ nal destination of the 
delivery channel. CDNs have speciﬁ c requirements for contribution before they 
will process for accessibility. 
 Files are handled differently than streams in distribution. Most delivery plat-
forms use embedded media players like Flash, HTML5, HTTP-live, and Silverlight. 
For web delivery, most browsers will handle all of these. Services like Netﬂ ix base 
their streaming service on Silverlight, while Amazon uses a proprietary format. 
There is considerable disruption with content providers. Flash had been the most 
common format, but that’s changing. For iTunes, Apple requires an 88-220Mb/s 
ProRes ﬁ le, while others use ON2, MXF, or QT (Mov). While this book is advocat ing 

Transmission and Delivery
101
for establishing a single format within the production center for managing media, 
distribution will still require transcoding to address the formats for different 
platforms. 
 There are multiple transport protocols used, such as ASI, IP (TCP and UDP) 
over Fast Ethernet or MPLS (typically over ﬁ ber that connects the broadcast cen-
ter to the transmitter and afﬁ liate stations). DVB-S ASI was ﬁ rst used for satellite 
delivery, and now with DVB S2-ASI, DVB S2-IP, and DVB-T, they are used for 
both contribution and distribution over terrestrial circuits as well. 
 There can be multiple formats that travel over these transport protocols, such 
as JPEG 2000 or J2K, MPEG2 TS, and MPEG4 H.264, and H.265, to name a few. 
The number of different platforms complicates program origination. Commercial 
integration is more complex as well. Third-party services handle the user-speciﬁ c 
ads and promotions that the digital platforms enable. 
 The broadcast center receives and originates content as high-resolution ﬁ les 
and streams. This is typically over dedicated ﬁ ber or satellite. The master control 
network origination systems and operations transcode from the high-resolution 
format to the myriad delivery formats and protocols. This is where an automation 
system is important for managing the transcoding and distribution.   
 FIGURE 6-5 
   Figure 6-5   shows each of the protocols and the platforms they service. There 
is no longer a simple term called “conventional broadcast”; DTV changed that. 
Over-the-Air is now a multi-channel delivery system with integrated mobile ser-
vices, with the studio to transmitter link (STL) as an IP stream. 

102
Transmission and Delivery 
 This changing landscape for distribution further re-enforces standardizing the 
media formats and management in the production and library processes, allowing 
transcoding to resolve the changes in delivery requirements. 
 SMPTE is working on new protocols like MPEG7 and 21, and H.265. These 
new protocols will impact compression, overall content quality, and the amount 
of embedded metadata that can be carried within the ﬁ le and/or stream. 
 The online, mobile, and On-Demand landscape is changing rapidly. There is 
a large effort to move away from royalty and license-based codecs (e.g. MPEG 
H.2xx) to an open source codec like VP8. 
 One thing that can easily be said is that there will be a constantly changing 
landscape in codecs, formats, and protocols. Staying on top will be an ongoing 
challenge. 
 In   Figure 6-7  , we see how from the origination center a single delivery for each 
format goes to a CDN, where it is replicated for ﬁ nal distribution to end clients. 
The client requests the content, the request is managed by the edge request router 
and metadata validates the user. The request moves into the distribution system, 
where it is conﬁ rmed, and the program is delivered. There are authentication keys 
Destination
Platform
Protocol
TV and Set Top Box
Over-the-Air/Cable
QAM
Satellite
DVB-S, DVB-S2, DVB-T
IPTV
MPEGTS/RTMP/HTTP Dynamic 
Streaming
iPhone, iPad, iPod
Apple iOS
HTTP Live Stream
Android, Blackberry, Other Mobile
3GPP
RTSP/RTP
Windows 7 and 8 Mobile
Silverlight
HTTP Smooth Streaming
 FIGURE 6-6 
 The broadcast center is now an origination center and delivers to cable, satellite, 
and IPTV operators with DVB S, S2-ASI, DVB T, and T1-IP. The cable, IPTV, or 
satellite operator aggregates the channels for delivery using QAM via a set top box 
that also provides Over-the-Top services. The broadcast center delivers to CDNs 
for broadband, web, and mobile services with an interactive return path for offering 
additional services and enabling transactions. There are a number of transport 
protocols and formats based on where the stream is going. 

Transmission and Delivery
103
in a subscriber-based network that are transmitted back to the end user for valida-
tion before the content is released. The CDN is similar to an Over-the-Air trans-
mitter in that its primary responsibility is to deliver the content to the end user 
device. In digital platform delivery, this takes on an interesting perspective. Back at 
the broadcast center, the content producer needs to format the programming for 
compatibility with all media players and browsers. The CDN needs to automati-
cally detect which player and how much bandwidth the end user device accessing 
the content has and then adjust dynamically for all browsers to ensure the delivery 
is seamless and immediate. 
 Cloud Services 
 The cloud has become a prominent player in technology for all aspects of business 
and is rapidly moving into production and broadcast operations. 
 First, let’s look at what the cloud really is. Until recently, the Internet was 
referred to as “the cloud.” While that hasn’t changed, the terms cloud and cloud 
service have expanded to mean remote datacenters that host many of the applica-
tions and services that the IT department and now the broadcast IT department 
were originally designed to support. 
 IT departments have been using virtual servers to obtain some efﬁ ciency in 
operations. This is done by using a single high-power multi-core piece of hard-
ware and running multiple operation systems with multiple applications on the 
same hardware. 
 By deﬁ nition and in practical application, there is a difference between virtual-
izing servers and moving services into the cloud. The most common use of the 
cloud is online storage through services such as Amazon EC2, Box.net, Dropbox, 
 FIGURE 6-7 

104
Transmission and Delivery 
and Google Drive. However, the CDN (Akamai, Limelight, Kaltura) is also a cloud 
service. There are a number of broadcast and production products that are now 
being offered either in the cloud or as cloud services. 
 Cloud services are deﬁ ned by three primary services: 
  Infrastructure as a Service  (IaaS)—This is when a provider sets up the servers, 
network, and storage as an outsource, but the user is responsible for all con-
ﬁ gurations and management. 
  Platform as a Service  (PaaS)—This is more common when the architecture 
is in place and the user loads their applications and can scale on demand. 
Amazon EC2 is an example of PaaS. 
  Software as a Service  (SaaS)—Salesforce.com put SaaS on the map. In the media 
production industry, Chyron’s Axis and Encoder.com are good examples. 
Adobe has taken this one step further with Adobe Anywhere and no longer 
offers their products as separate applications but only as a subscription 
service interconnecting their full bouquet of products. SaaS is good when an 
application can service a large user base from remote locations and offer col-
laboration and shared ﬁ les without the need to host and manage the appli-
cation on servers in the facility. The Subscription as a Service model opens 
both new opportunities and challenges. 
   Figure 6-8   helps clarify the differences between each of these offerings. In the 
packaged hardware and software, everything is on site in the broadcast facility and 
 FIGURE 6-8 

Transmission and Delivery
105
managed by engineering. In the Infrastructure as a Service (IaaS) model, as things 
are moved to the cloud, it provides the hardware for network storage and serv-
ers. The broadcast engineer is responsible for the operating system, middleware, 
application, and all data. 
 If it is run as Platform as a Service (PaaS), in addition to the servers, storage, 
and network, the cloud provider also manages operating systems and middleware. 
The broadcast engineer is responsible for any applications and managing data. 
 And last but not least, with Software as a Service (SaaS), the cloud provider 
manages everything—and provides it as a whole service. 
 There is also public cloud and private cloud. The public cloud is what most 
people are familiar with through Amazon, Google, Microsoft Azure, and iCloud. 
The public cloud is almost like a utility, where users can buy as much or as little 
processor or storage as they need and scale on demand. In the public cloud there 
can be multiple users on a single server. The private cloud services a single client 
on dedicated servers. As more capacity is requested resources are allocated but 
dedicated. The private cloud is offered through a different set of service providers, 
although Amazon offers both with its AWS EC2 service. A private cloud uses an 
offsite datacenter as a platform or host for applications. Most of the new products 
appearing as cloud services are running as Software as a Service (SaaS). 
 Cloud Services in Broadcast 
 In using cloud services for contribution, ﬁ eld producers can upload content to the 
cloud, and the broadcast center can download it without needing to create a direct 
connection. There are a number of initiatives to look at content distribution in the 
cloud where a programmer hosts their content with a cloud provider and their 
distribution channels can browse and access from there. There are other services 
where proxies are in the cloud for review and approval. As automation systems 
support the Channel in a Box and with CentralCasting, the automation systems 
can be cloud based. 
 And as mentioned earlier, the Adobe Anywhere product is a hybrid of an appli-
cation local on the end user’s machine that only stays activated by subscription 
to the cloud in real-time. It also keeps the core application in the cloud service, 
updating the local system each time the user logs on. 
 The most obvious use of cloud services is media delivery. The broadcast origi-
nation center delivers the content to a distribution network in the cloud. The dis-
tribution network has the necessary applications to host and replicate the content 
as needed, manage user authentication, and integrate interactive services. 
 Archiving and media management are other practical uses of the cloud. 
Archiving typically is a combination of a digital tape library or low-cost spinning 
disks. Both are a large capital investment plus have recurring maintenance costs. 
Placing the archive in the cloud reduces the footprint, the need for a protected 
environment, and overhead costs like electrical and mechanical services. It allows 
the archive to be accessed from anywhere and provides a level of disaster recovery. 

106
Transmission and Delivery 
Media management is another use of the cloud, as noted earlier, proxy storage 
increases faster than high availability high resolution storage. Placing proxy and 
metadata in the cloud makes the inventory accessible from anywhere, the tools 
enable a user to create an edit decision list (EDL) by browsing the proxies and 
instructions can be sent to a rendering agent in the facility to put together the ﬁ nal 
version for distribution. The orchestration or automation application manages 
the rendering and distribution. 
 The CDN also manages any transaction or subscription services that are avail-
able. Online media services such as Hulu, Vudu, Amazon, YouTube, Netﬂ ix, and 
iTunes are now considered cloud services. They use Akamai, Kaltura, Limelight, 
Brightcove, AppleTV, and others as their online video distribution network. 
 More and more broadcast and production products are simply software appli-
cations and many production and broadcast manufacturers are developing their 
products to operate in the cloud. What impact does this have on facility planning 
and design? 
 What cloud services work best for broadcast? And what needs to be considered 
when assessing the viability of these cloud services? 
 
 Contribution 
 
 Craft and Production 
 
 Management 
 
 Distribution 
 
 Automation 
 Case Study 1 
 In the US, the Public Broadcasting Service (PBS) has driven a number of initiatives 
to consolidate program and origination plus master control operations to central-
ized and cloud services. The Corporation for Public Broadcasting, the parent of 
the Public Broadcasting is the central repository of programming from the mem-
ber stations plus commissioning a lot of content. As an early adopter of ﬁ le-based 
workﬂ ow, PBS still sends tape to all the member stations for airing. PBS is looking 
to create a more efﬁ cient system and is looking at the cloud to host all the content 
and have all the member stations access it. In addition, a separate initiative is to 
have each of the stations’ trafﬁ c schedules integrated and let a cloud provider send 
the individual station program schedule to their transmitter, effectively running 
a multi-channel master control service in the cloud. This would let the member 
stations reduce operations for program origination and focus on production. 
 Case Study 2 
 There are vendors and service providers offering cloud solutions as central deliv-
ery of content as the next generation of centralcasting. In a different service 
model than PBS, these offerings are to enable commercial stations to reduce their 

Transmission and Delivery
107
overhead by using cloud services for all the master control functions, i.e. program 
origination and commercial integration, play-out to multiple platforms in mul-
tiple format and trafﬁ c integration 
 Cloud Decisions 
 What are the decision points to using the cloud? What’s the most practical and the 
most effective way to use cloud services? 
 One of the core advantages of the cloud is scaling. Scaling in the cloud is 
on demand and supports stable growth. One advantage to using a cloud service 
is that when there is a peak demand, rather than building out to support it for 
one time and then having too much capacity when the demand subsides, you use 
the cloud as needed and scale back when the demand is lower. Cloud works! In 
the cloud, you can add more capacity, pay for it, and turn it off when the demand 
subsides. 
 A few of the ﬁ rst decisions when moving to the cloud are: 
 
 Private or public cloud? 
 
 Rent or build? 
 Contribution—there are a number of new initiatives beginning to offer cloud-
based services using open Internet as the carrier for contribution. Obviously, 
bandwidth, security, and reliability are major factors here. 
 Craft and media management become more difﬁ cult or have limitations when 
considering cloud. What applications are most suited to the cloud? Is there a real 
beneﬁ t? Can cloud applications meet production requirements? 
 Distribution to web, portable, and mobile devices has always been a cloud 
service; it just didn’t have the name yet. Akamai, Brightcove, and Limelight have 
always worked in the cloud. YouTube is a cloud service. 
 The following list shows in detail the different aspects of public vs. private 
cloud services and some of the roles for which it is worth using cloud services. 
 Public Providers 
 
 Infrastructure as Service (IaaS) 
 
 Amazon EC2 
 
 Microsoft 
 
 Rackspace 
 
 IBM 
 
 HP 
 
 Platform as a Service (PaaS) 
 
 Google App Engine 
 
 Microsoft Azura 

108
Transmission and Delivery 
 
 Amazon AWS 
 
 Red Hat Openshift 
 
 Heroku 
 
 Software as a Service (SasS)
 
 Chyron (Axis) 
 
 Encoding.com 
 
 Kaltura 
 
 Adobe Anywhere 
 Public vs. Private Cloud 
 Public is just that, accessed via open Internet and offering infrastructure, platform, 
and software as a service. These are managed and hosted services in remote data-
centers that can be rented by processor, storage, and bandwidth. Multiple users 
share servers. 
 In the public arena for infrastructure and platform, Amazon, Google, and 
Microsoft have offers. While this is a business-to-business service, it still uses their 
core datacenters, servers, and storage. 
 Public cloud is a full outsource: renting what is needed for the services required. 
One of the logical choices for cloud services is setting up media-rich websites for 
On-Demand and streaming. Developing software for mobile apps is another good 
use, especially since the cloud providers offer load testing. From a security per-
spective, developing in the cloud avoids any danger of compromising live produc-
tion tools. Cloud storage and ﬁ le access from all devices is becoming more popular 
as an alternative to VPN and Citrix. 
 In broadcast and production, there are a number of vendors offering services 
in the cloud in the Software as a Service model. Chyron’s Axis, Encoding.com are 
changing the thought process in planning and design when looking at infrastruc-
ture planning. 
 Private Build vs. Outsource 
 Build 
 
 Provide access for remote user applications 
 
 Content and metadata accessible 
 
 Large capital investment 
 
 Operating overhead 
 
 Maintenance 
 
 Power and Cooling 
 
 Upgrades 
 
 Scalability 

Transmission and Delivery
109
 Outsource 
 
 Self-managed hosted servers and storage 
 
 Self-managed hosted applications 
 
 No capital costs 
 
 No overhead costs 
 The private cloud comes in two ﬂ avors. One is to build or leverage your own data-
center, creating a service model where users don’t have any applications on their 
desktop and must therefore access tools via browser or remote connection. 
 In this model, whether on- or off-site, you are still building and maintaining 
a datacenter. It changes the infrastructure build-out and concentrates the servers 
and applications to the datacenter. 
 The other option is to use one of the cloud providers to build-out on their 
premise a dedicated cloud environment where they host and manage all the serv-
ers, storage, and applications. This philosophy and using the IaaS and PaaS mod-
els will relieve the need to build and manage a datacenter. If more capacity is 
needed, the cloud provider can provide that, either on demand or for permanent 
growth. 
 Back at the broadcast center, having adopted the cloud, instead of designing a 
server farm and large storage environment, it’s all about network and bandwidth. 
 Not all production and broadcast functions are applicable to cloud, but this is 
changing rapidly. These are some of the areas where cloud can be applied: 
 
 Remote production contribution 
 
 Truck 
 
 Field 
 
 Studio 
 
 Outsourced production 
 
 Post production services 
 
 Graphics 
 
 Automation 
 
 Central casting 
 
 Independent production 
 
 Asset management and metadata log-in 
 
 Search and browse 
 
 Dailies review and approvals 
 One area that is seeing potential in the cloud is contribution. Where the open 
Internet has security and consistent bandwidth issues, the cloud services are lim-
ited by ﬁ rst- and last-mile bandwidth (last mile . . . nothing’s changed here). There 
are a number of companies offering transport over open Internet with clever algo-
rithms that optimize and accelerate transport. The broadcast engineer needs to 

110
Transmission and Delivery 
plan his infrastructure to support this. Security, ﬁ rewalls, and bandwidth are just 
some of the considerations. 
 Cloud-based craft services where graphic elements are created can be hosted in 
the cloud and not fully assembled until they are downloaded and rendered where 
the program is being produced. Cloud storage is also handy for large ﬁ le transfers. 
 Using the cloud to edit might look like this: a producer or editor opens the 
asset management system (via browser) and ﬁ nds the clips they need as proxies. 
Using the asset manager, they create the EDL. Once the EDL is complete, it’s sent 
to a rendering engine, which does the conform and renders to the ﬁ nal product. 
 The cloud offers producers a way to review production dailies in proxy form. 
By making some or all of the asset manager accessible in the cloud, media logging 
can be done from anywhere by watching a proxy version of the production and 
entering metadata. Once the asset manager is accessible, any user can search and 
browse the library from remote locations. 
 Distribution has been in the cloud—speciﬁ cally to online, broadband, and 
mobile—for a while. Content distribution networks like Akamai and Limelight 
have been cloud services since before it was in vogue. Popular video services like 
YouTube, Hulu, Vudu, and Netﬂ ix are all cloud services that distribute media. 
 The cloud is ideal for On-Demand media delivery and is used extensively to 
authenticate users for online delivered media that is subscriber controlled. The 
cloud service manages the decryption of protected content delivered to a PC, tab-
let, or mobile device once an authorization key has been conﬁ rmed. 
 The cloud connects the viewer to an engaging multi-screen experience, tying 
together social networks, private communities, and enriched program experiences 
as a companion to Over-the-Air, cable, satellite, and IPTV large-screen viewing. 
 The cloud brings another piece to our puzzle and that piece is helping to deﬁ ne 
what the whole picture of the IP- and ﬁ le-based architecture will look like. 

 Planning and design of the IP- and ﬁ le-based infrastructure is as much about 
bricks and mortar as it is about technology. Other pieces to this puzzle are the 
many facets in the technology architecture and infrastructure. But the puzzle 
would not be complete without the pieces for physical facility and facility design. 
 These pieces include the changes in physical design for studios, control rooms, 
and equipment rooms. They are changes in the mechanical and electrical designs 
driven by the installation requirements of IP technology. How does the integration 
of enterprise and broadcast networks affect the placement of technology? How 
does it impact cable design and management? 
 There are a few core elements critical to the physical side of planning and designing 
a facility. They include: 
 
 Space Planning 
 Space Adjacencies 
 
 Studios 
 
 Control Rooms 
 
 Media Operations (logging, ingest) 
 
 Program Origination Center (Master Control) 
 
 Craft Editing and Graphics 
 
 Core Equipment Room 
 
 Network Operation Center 
seven 

112
Facility Planning and Design
 
 IT Infrastructure 
 
 Network Switch Locations (MDF/IDF) 
 
 Demark Rooms for Carrier Services 
 
 Enterprise and Broadcast LAN Locations 
 
 Electrical 
 Critical Power 
 
 Protected Power 
 
 Power Distribution 
 
 Cable Pathways 
 
  Mechanical 
 
 Air Flow and Distribution 
 
 Temperature and Humidity 
 Operating Costs—Space/Power/HVAC 
 The amount of physical space also factors into the total cost of ownership, includ-
ing basic overhead costs such as space, power, and mechanical systems. It is a rea-
sonable assumption that if there is less space in an IP-based architecture, it’s also 
possible that the overhead is lower, in terms of potential power cost-savings. The 
amount of mechanical support systems needed in control and support spaces is 
also reduced signiﬁ cantly. IP-centric control surfaces for production devices use 
less power and are connected over Ethernet to a server in the equipment center. 
Video servers in comparison to video tape machines typically consume less power 
and the control rooms have less heat-producing equipment. This translates into 
needing less HVAC equipment and lowering operational costs. 
 In terms of mechanical systems,  s ervers, storage, and networks tend to be con-
solidated into the equipment center, with only control surfaces in the craft suites. 
There are different HVAC design considerations since the servers, network gear, 
and data storage have a lower tolerance for heat. It is more critical to maintain the 
right temperature, but the overall amount of cooling required is a little less. IP 
equipment does not handle power outages well and is very sensitive to high tem-
peratures. It is common to use UPS (battery backup) on all current production 
equipment, even without a generator to allow them to be shut down in an orderly 
fashion in the event of a power disruption. There are still heat- and power-related 
issues. 
 Modest control surfaces and new ﬂ at-panel display devices put out less heat. 
Workspaces for editing, graphics, studio and master control rooms are also dif-
ferent. These workstations don’t occupy as much space, require as much power, 
or generate as much heat as VTRs, audio and video support, switchers, mixing 
consoles, editor and support equipment. 
 The interdependency between spaces has changed as well. With the data center 
now serving as the media backbone, media is now more readily available over the 
network. 

Facility Planning and Design
113
 In the IP facility, there are considerable changes to all control room designs. 
There are fewer control surfaces because of the features and functionality built into 
the equipment changing the layout of consoles. There is a huge difference in what 
an edit room used to look like in a tape-based world compared to what one looks 
like now in a ﬁ le-based world. Multi-viewer monitor walls are dynamically conﬁ gu-
rable, with things like clocks, Tally, source ID, and audio monitoring integrated into 
the display. An operator can see and control all the devices, applications, and access 
to media in a smaller, easily managed space.  
 FIGURE 7-1 
 FIGURE 7-2 
 The origination center, or master control room, now looks a lot more like a network 
operations center, while the main equipment room looks a lot more like a data center.  

114
Facility Planning and Design
 Other puzzle pieces are servers, storage, and networks. Fiber optic cable is prob-
ably the real backbone of IP architecture, with categories 5e and 6e as the copper 
cables used to reach end points. Fiber and Cat6 are handled differently than coax, 
multi-pair audio, and multi-conductor control cable. 
 Most server-based media technology and dedicated devices have at least two (2) 
network ports: one for media and the other for control or management. Some 
have more for redundancy, they may go to the same switch, but they are conﬁ g-
ured on different VLANs. Even simple things like cable management are different. 
Category 5e and 6e (and coming soon, 8e) cable does not like to be squished or 
compressed! This means that instead of nylon cable ties, hook and loop (Velcro) 
is used. 
 With IP-based technologies there is very little equipment that has front 
panel controls anymore, user and admin access is all done through a software 
interface or a display with keyboard and mouse control. Servers are deeper than 
legacy broadcast equipment, so allowing for service and cabling in the racks is 
important. Network switches have their ports in the front, so cable manage-
ment changes. It is important to make sure there is enough space to install and 
remove the servers, so this changes the amount of space needed in front of the 
rack. This translates into each rack occupying more space, therefore reducing 
the number of racks in the equipment room. Equipment density in racks has 
changed where some servers can be stacked tightly however some have side 
vents for cooling which impacts rack width and spacing. At the same time they 
occupy fewer rack spaces than single purpose proprietary production devices 
(i.e. tape machines). 
 The equipment room now more closely resembles a data center. Master control 
resembles a network operation center (NOC). As a result of this, space, power, 
and environmental conditioning design needs to be modiﬁ ed to meet the require-
ments of servers, storage, and switches. 
 IP- and ﬁ le-based technology has a signiﬁ cant impact on the physical design—
that is to say, the bricks and mortar of the facility. 
 Space Planning  —Room layouts are different; the number of operating posi-
tions has changed, and spatial adjacencies, cable pathways, and access are all dif-
ferent design considerations. 
  Mechanical and Electrical — Servers, storage, and switches have different power 
and environmental needs than tape-based baseband. In some ways, the devices are 
more forgiving; in many ways, much less. 
 Connectivity and Telecommunications —These are almost the same thing in 
IP. While telephone (voice) and Internet come through a different service, they 
conceivably come from the same service providers—and telephone and intercom 
are now VoIP. There are still dedicated providers that specialize in moving media; 
however, the larger communications carriers have moved into this space as well. 
The IP STL is a ﬁ ber link over a common carrier network. And there are a number 
of new vendors using the open Internet with their own algorithms to make the 
transport more efﬁ cient. Now it’s all about bandwidth. 

Facility Planning and Design
115
 Network design has a core switch in the main equipment room and satellite 
switches closer to the end device in Intermediate Distribution Frame (IDF) rooms. 
There are more pathways for ﬁ ber and cable to service them. The entry point 
into the premises for high bandwidth ﬁ ber services needs power and environment 
conditioning. 
 Studios  —In designing studios, size is still ﬁ rst and foremost, and is decided by 
the kind of production. That being said, the lighting instruments use ﬂ uorescent 
bulbs (CFLs) and LEDs that are cooler, use less power, and emit less heat into the 
space. They are controlled over IP. 
 Virtual sets and large monitor walls that are used as backdrops need less space 
than physical sets. Studio cameras and lenses are smaller, and robotic camera ped-
estals can occupy a smaller footprint. Higher camera sensitivity allows the lighting 
designer to use fewer instruments while still creating a dramatic visual effect. One 
of the crucial issues no different than before is when using robotic camera pedes-
tals the ﬂ oor must be level and smooth. 
 Studio Control and Support Rooms —One major change in the studio con-
trol room design is that it can now be fully automated, manually operated, or a 
hybrid. The physical size of the control room now depends more on the number of 
production personnel positions than the number of operating positions. The pro-
duction switcher has digital effects, image store, and clip player fully integrated to 
one control surface instead of separate controllers. When using robotic cameras, 
a touch panel can be programmed with a shot list that includes camera moves. 
The touch panel can sit on an articulated arm in front of the technical director 
(TD) and over the production switcher. A single screen and keyboard handles all 
playback control on a play-out server located in the equipment room. The video 
operator has a master console for the camera control units (CCUs), and the cam-
era connection is a hybrid of copper and ﬁ ber. The operator has a multi-screen dis-
play that, in addition to the camera shot, shows the waveforms used to balance the 
cameras (with memories). The audio engineer has a control surface that resembles 
a mixing console, however, the server frame where all the audio sources connect 
resides in the equipment room. The operator conﬁ gures the mixing console with 
a keyboard, mouse, and monitor. 
 Master Control—Program Origination 
 The master control room is more of a multi-channel, multi-platform program 
origination center. All program providers deliver multiple channels over multiple 
delivery platforms. It’s the master control center that manages program distribution 
and quality control. Even single channel television stations are now multi-platform 
delivery networks. Over-the-Air can have up to six (6) full channels of SDI, or three (3) 
full HD channels and fourteen (14) mobile DTV channels. All program origination 
facilities support broadband, On-Demand, and mobile and these are all “channels” that 
can have their own play list, play-out, and need monitoring. The program origination 

116
Facility Planning and Design
center has multi-view displays with audio metering for each display. There are 
high-resolution monitors for quality control, and keyboards, mice, and screens 
to control all play-out and management. The play-out servers are located in the 
equipment room, so the physical master control room only needs enough space 
for operators, keyboards, and ﬂ at panels. 
 New Spaces  —Media management and ingest manager control rooms need 
to be designed into the facility. The operator positions are workstations with 
multi-channel displays that control the movement of the media. The actual space 
requirement is not large and there is very little heat and power necessary. It has 
similar physical requirements to an edit room. 
 Post Production or Craft —This is editing and graphics. Here, there have been 
signiﬁ cant changes in the physical requirements. These are all workstations con-
nected to servers or dedicated machines running high performance applications. 
The consoles have one to three monitors, a keyboard, and tracking device. There 
may be a separate networked computer for enterprise. There are no longer dedi-
cated controllers and tape machines. Even for the most complicated edit or graphic 
production, it’s still a single workstation. The workstations do not draw a lot of 
power and as result do not generate much heat. The physical size of the rooms has 
decreased, as have power and heat loads. 
 Core Equipment Room —This room hosts more systems than it did before 
IP. Now that most production is server-based, the servers are located in the core 
equipment room. In the core equipment room, there are still IP and non-IP 
systems. When designing the core equipment room, consider cable density and 
equipment density for power and heat distribution. What are the touch points 
between SDI and IP? How should the equipment be laid out in the most efﬁ cient 
way for signal ﬂ ow and cable management? 
 The QC position has changed; there are still test and measurement monitors 
with SDI router control to assure the integrity of SDI and AES audio and video. 
Now in addition to SDI test and measurement, there is a rack-mounted keyboard, 
tracking device, and monitor that can access all the servers and QC applications 
for monitoring ﬁ les and streams. 
 Applications, servers, switches, and storage don’t necessarily need the kind of 
clean power SDI equipment does; however, they do need protected power. Even 
devices not considered critical still need adequate time to shut down in the event 
of a power disruption. 
 Server, application, and storage crashes can be harmful. It is more complicated 
to shut down an application, disk array, and servers than it is to turn off a tape 
deck. Servers, switches, and storage can run at different voltages, so power distri-
bution changes. Servers, switches, and storage will shut down on high heat. 
 Traditionally, facilities were designed around the central equipment centers as 
the core, with studios and control and edit rooms close by. Tape machines for 
record and playback needed an operator position with monitoring and measure-
ment. There was a lot of cable trafﬁ c moving between rooms, and there were dedi-
cated control heads for each production device. 

Facility Planning and Design
117
 The move to an IP infrastructure changes that. 
 
 Facility Infrastructure—building 
 Core space planning 
{ Adjacencies 
{ Space allocation 
{ Equipment center 
 Ergonomics 
 
 MEP systems 
 
 Telecommunications 
 
 Wire management and cable planning 
 Similar to the construct of an enterprise network, operators and workstations can 
be located anywhere as long as the network topology is set up to support it. There 
are still obvious reasons to keep core operations together, some having more to do 
with physical plant design. 
 It’s easier to construct acoustically sensitive spaces near each other and away 
from non-technical operations. However, media management and even craft edit 
and graphics can be closer to the production team instead of closer to the equip-
ment center. 
 From an architectural (building) perspective, it’s still more cost effective to 
build an “acoustic envelope” of spaces rather than disbursing them around the 
plant. It also makes more sense mechanically and electrically. 
 But with studios, even those needing a lot of space (e.g. for an audience), they 
can still be more efﬁ cient. On-camera monitors and displays take up less space, 
require less power, and produce less heat. 
 In studio lighting, the alternatives to incandescent, halogen, and tungsten con-
tinue to improve, having a considerable impact on heat and power. In a comparison 
between an 860W Tungsten light and an LED alternative, the LED draws 36 watts 
of power for the same amount of light. LEDs can last for up to 100,000 hours and 
require approximately 75% less electricity and generate up to 80% less heat com-
pared to conventional lighting. This has an impact on both the cost to build and 
the cost to operate. 
 Take a look at the whole impact: the dimming system can be smaller, and so can 
the distribution wiring, resulting in less power service, lower electrical bills, less 
mechanical environment, and, if the lighting is on generator, less load when sizing. 
 Edit rooms need less space; even the beeﬁ est workstations require less power 
and cooling than a rack full of tape machines, DAs and audio/video processors. 
These are now all software plug-ins to the edit system. Monitoring is a single 
screen driven by a multi-viewer. 
 From the equipment center, distribution uses high bandwidth ﬁ ber to inter-
connect switches. Instead of individual cable, high capacity ﬁ ber connects from 
the core switch in the equipment room to satellite switches in the production con-
trol or edit rooms, carrying audio, video, control, and communications. The end 

118
Facility Planning and Design
devices connect over Cat5e or 6e cables to the local switch, and the switch manages 
whether the end device gets media or other control or management data. A single 
cable or ﬁ ber feeds the multi-view display and remote access over IP allows opera-
tors to reconﬁ gure the display wall. There is a reduction in cabling and all the other 
associated costs in running large bundles of different cable types. 
 Another beneﬁ t to an IP infrastructure is that when a new device is added, it’s a 
port on the network and there is no need to run additional cables from the equip-
ment center. It is a local connection within the control room, and the engineer can 
access the core and local switch to add the new device to the system. 
 In the equipment center, there are changes in rack layout. Servers can be 
36" deep, meaning racks are now 36"–42" deep. When racks face each other, 
there needs to be a minimum of 48" (4') to allow equipment to slide out on 
rails. Therefore, racks need no less than 4' of front clearance. And since network 
switches tend to front load and device connections are still rear, it is becoming 
common practice to rear face switches or recess mount them so that the cables 
and cable management does not protrude from the front. These changes also 
affect the rear clearances as well. Where before a minimum of 2 feet was allow-
able, now 3 feet is becoming more of the standard. This means that the overall 
footprint of a single rack is now approximately 10' × 2', or 20 ft 2 . This has a huge 
impact on space planning. 
 Ergonomics has always been a consideration in control room design. It’s all 
about how many things one person needs to look at and how many things they 
need to operate without straining their necks to see or constantly change their 
position to operate. The capabilities of production devices functionality is now 
concentrated into fewer devices. Images in the multi-viewer display are actually 
closer together because there a fewer bezels and no mullions from equipment 
racks. Also, the multi-viewer displays are easily programmable and can simultane-
ously show audio metering, source ID, and tally for each image. 
 Edit and graphics stations can have smaller consoles, with monitors mounted 
on articulated arms that keep them in the main ﬁ eld of vision. The operator uses 
a keyboard and tracking device instead of multiple control heads. In the ﬁ le-based 
and IP-production infrastructure, media is accessed from folders on a storage sys-
tem instead of routing a tape machine to an input on a device. This changes the 
type of repetitive physical movements in the production rooms. 
 One of the largest costs and complicated design considerations in building 
and operating a facility is the mechanical and electrical systems. In planning and 
design of the electrical plant, achieving clean power and good ground have always 
been the bane of all engineers. It has kept the HumBucker (Allen Avionics) people 
in business for a long time. While clean power and good ground practices are still 
important, there are a lot of changes that started with SDI and have continued to 
evolve ever since. 
 Theoretically, there is no ground noise in digital and switching power supplies 
that can introduce interference in the signal. This is mostly true. Look at what has 
changed in electrical and mechanical design for IP. 

Facility Planning and Design
119
 Servers, spinning disks, solid state storage, and network switches depend a lot 
on stable and consistent power and temperature. There is a slight difference in 
humidity speciﬁ cations for IP technologies vs. SDI, but not much. 
 This may sound like a primer, but consider the difference between turning on 
a tape machine vs. starting an encoder. With a tape machine, you would ﬂ ip the 
“ON” button while conﬁ rming audio and video is present, this includes entire 
signal path of routing, distribution, frame synchronizers, and other terminal 
equipment. 
 With an encoder, you power on the server and see that the BIOS is OK. Then, 
the OS starts up and needs to load, next up are the services for the hardware and 
databases, while establishing the network connections. Next up are the services 
that support the applications that handle the encoding process. After all that the 
applications that should auto-load need to start. Are there any manual applica-
tions that need initiation? Nothing corrupt? All good? 
 Now, when power fails with a VTR, ﬁ rst the power switch is turned off so that 
when power restores, any surge doesn’t impact the machine. If there is a tape in 
the machine, it stays there until power is restored, when it can be examined and is 
hopefully ﬁ ne, but if there’s a problem or it’s jammed, it’s only that one single tape. 
 On the server side, when the power goes out, all running applications crash, the 
network shuts down, and disks stop spinning while the heads are engaged. If this 
is in the middle of a process, that process could corrupt both the media, disks, and 
the application. There may be an impact on the operating system. Many servers do 
not have a physical power switch, so a restore with a surge could affect the power 
supply. Abrupt power failures can damage hard drives, as the disks are spinning at 
high speeds and a power outage could cause a head crash. Servers like to be turned 
off carefully so that processes and processors can stop in an orderly fashion. If 
there were ﬁ les open or in transit, there is also the potential for ﬁ le corruption. 
This is a good time to review the need for that archive and backup plan. 
 Now compare planning the electrical system. The IP architecture overall 
uses less power, yet at the same time, the design of the electrical systems is dif-
ferent. Servers, storage, and switches typically have dual power supplies wanting 
to be on different circuits and possibly different legs. They can also operate at 
different voltages; for example, instead of 110V single-phase, they can use 220V 
three-phase. This reduces the physical size of the power supply and allows them 
to operate at lower current loads. The design of the power service and distribu-
tion are also impacted. Servers, storage, and switches prefer not to be shutdown 
abruptly; they have shutdown sequences. Not all systems need to be on full UPS 
and generator back up; however, having enough UPS to give the servers, disks, and 
switches enough time to shut down gracefully is highly recommended. This only 
takes about 5–7 minutes, but it is an important 5–7 minutes. Many of these devices 
have a lights-out feature that ties into the UPS. In the event of a power outage, the 
lights-out feature tells these devices to go into an automated shutdown sequence. 
 If there is UPS and generator backup, this adds an additional layer to power 
distribution. There are three types of power; street power which is the service 

120
Facility Planning and Design
provided by the electric company, protected power which is UPS (Uninterruptible 
Power Supply) and critical power which is UPS with generator. There is actually 
one more layer which is generator only with UPS so there is a disruption in service 
until the generator starts, and generators are known to surge which can damage 
technology power supplies. It’s good practice to have critical systems on UPS and 
generator power, and non-critical systems on UPS only. 
 In control spaces, there are fewer devices, which draw less power. Another ben-
eﬁ t to the reduction in power loads is that the mechanical loads are less, reducing 
the size of mechanical systems—which also has an impact on the electrical design. 
 Speaking of mechanical systems, IP devices are being designed to run at higher 
temperatures, so even if they have an equal (if not greater) sensitivity to heat, the 
amount of cooling needed is reduced. And although humidity is still important, 
they are a little more tolerant (but condensation on disks is still frowned upon). 
 Servers and disks generate a fair amount of heat. Similar to broadcast, they 
typically have fans in the rear that draw cool air through the device, expelling heat 
to the rear. So in planning the equipment room, or controls rooms for that mat-
ter, it is important to have cold and hot aisles that manage airﬂ ow. The cold air 
(supply) is ducted from overhead in front of the rack and the return is also ducted 
and draws from the rear of the racks creating the correct air ﬂ ow and managing 
the heat. 
 Because servers tend to be stacked tightly into racks, there is a lot of heat gener-
ated in a relatively tight space. Keeping air moving and getting the heat away from 
the devices is critical. Server farms are being planned with hot and cold aisles to 
 FIGURE 7-3 

Facility Planning and Design
121
manage the air ﬂ ow. With IP technology, the actual loads tend to be consistent, so 
good planning with some allowance for growth will provide good results without 
surprises. 
 Figure 7.3 is a quick sketch to demonstrate the new distances and allowances 
in the equipment room. 
 The fronts are the cold aisle, with the cold supply air in the ceiling—cold air 
falls, and as mentioned earlier, with servers being deeper, there needs to be enough 
separation to pull a server out for servicing or replacing. 
 In the hot air aisle, the return air is also ceiling mounted, drawing the heat away 
from the racks and devices. 
 Case Study 
 On a major facility build out project, the organization engaged an architect and 
MEP team that used older data center design philosophies. Earlier design for data 
centers put positive pressure cold air in the raised ﬂ oor and hot air pulled from the 
top. In this instance the cold air was in the ﬂ oor and the return (hot) air was open 
in the room. This put undue strain on the equipment and created an overheating 
situation where equipment was shutting down. To remedy the situation as best 
as possible, air blocks were put in the bottom of each rack, perforated tiles were 
strategically placed in the ﬂ oor to maintain enough pressure to get the cool air out 
of the ﬂ oor enough for the equipment to draw it in before the ﬂ oor mounted units 
pulled the return air before it could reach the equipment. An expensive design that 
didn’t work. 
 Space 
 Building on the space conversation, control rooms can be more efﬁ cient in layout 
and need less space for control surfaces. Now that we have touched on automa-
tion, we can discuss a studio in a box and station in a box. These have fully auto-
mated studio control and origination control. All the broadcast and production 
devices are either software-based or dedicated hardware controlled by software. 
These are single operator systems managing programmed sequences. In the stu-
dio, automation includes camera positioning and the manipulation of virtual 
sets. Control rooms that are fully automated don’t require the same amount of 
support people. Editors now preview and assemble their EDLs from the asset 
manager and then use the full production editor to conform and render with all 
the effects and elements. 
 Workstations for graphics have been around for a while, possessing elements 
available in common storage or cloud. Searching is easier and improved in the ﬁ le-
based workﬂ ow. And we just saw the impact in the equipment room.   
 Telecommunications, Voice, and Data —Telecommunication is the backbone 
to the IP architecture. The bandwidth and transport has been examined now it’s 
time to add in the rest of the telecommunication services. Telephone services now 

122
Facility Planning and Design
 It is certainly true that using a single cable type that carries multiple signals 
reduces the number of cables. Also, since a lot of the signals are aggregated in 
a local network switch and then sent over ﬁ ber to the core switch, cable man-
agement may be more sensible. Fiber tends to come in multiples by number of 
strands. Fiber is very thin, so a lot of strands don’t take up much space. Conduits 
are fewer and smaller. What is important to ﬁ ber is bend radius, since it is glass and 
it can break. Cat 5e and 6e have standards to follow. However, when designing the 
cable plant, the troughs or conduit still need to consider the number of dedicated 
runs and allow for growth. Ethernet cable does have to stay away from electrical, 
typically crossing at 90 degrees. 
 Engineers know that all cabling is perfectly dressed and laced, at least for the 
ﬁ rst 15 minutes after the installation is complete. And then the changes and addi-
tions happen. Fiber and Ethernet cabling is dressed and harnessed. One small 
point of interest is that the telecom and data industry has standards for cabling, 
ANSI TIA/EIA 568B Structured Cable Standard that includes labeling, conduit 
sizing, bend radius, isolation, and management. The broadcast industry for all its 
 FIGURE 7-4 
include voice over IP (VoIP). On the production side, intercom systems are also VoIP. 
Telephone is typically handled in the enterprise design. But since production intercom, 
telephone hybrids, and other former discreet phone services are now IP, these need to 
be included for a complete design. Production and contribution use audio and video 
conferencing, Skype, and other services that integrate into the production. 
 It’s a good idea to include wire management and cable planning in the physical 
plant discussion. The broadcast industry actually does not have published stan-
dards for cabling and cable management, there are however industry accepted 
practices. In the IT world, there are published standards for structured cabling 
(EIA568B). Some of this has been adapted to broadcast installations; however, 
there are differences—speciﬁ cally in how horizontal and vertical cabling is handled. 

Facility Planning and Design
123
standards and protocols does NOT have a cabling standard, only recommended 
industry practice. Now we can begin the next topic of cabling standards for Eth-
ernet and ﬁ ber optics. 
 There are a number of organizations that produce and publish the various 
standards for the broadcast and production industries. SMPTE, IEEE, ISO, EIA, 
and ITU are the standard organizations for broadcast and production trans-
port, signals interfaces, and processes. For analog video, the US and Europe both 
accepted the EIA RS170M and 250C as standards; in addition the ITU uses the 
BT470 designation. 
 In moving to digital, SMPTE and ITU came together to standardize SD/HD-
SDI with SMPTE259, SMPTE 292, and ITU-R BT709. 
 ISO and IEC joined to form the MPEG working group, partnering with SMPTE 
and ITU and producing the myriad variations for the multitude of compression 
algorithms and protocols. 
 In the physical design of an IP facility, EIA is the standard bearer, with the well-
established EIA 644 standard for data rates over twisted pair and EIA/TIA 568B as 
the current standard for IP cabling topology. 
 When running network cables, you will hear terms like “MDF” and “IDF”; 
these stand for main distribution frame (MDF) and intermediate distribution 
frame (IDF). The MDF is located in proximity to the core switch, and the IDFs are 
aggregation points of end devices typically with a satellite switch. 
 Fiber optic cable handles the high bandwidth interconnection between the core 
switches and satellite switches and resolves distance limitation problems. In addi-
tion to bandwidth limitations, twisted pair has distance limitations similar to that 
of coax. 
   Figure 7-5   shows the different bandwidth topologies that represent the intercon-
nections between the core switch and sub switches. The core switch attaches to end 
devices over 1Gb/s and 10Gb/s connections, there is high bandwidth 40Gb/s con-
nections to the satellite switches and 100gb/s between the cores switches. The high 
bandwidth connections are all on ﬁ ber optics. Typically from any switch to an end 
device is 1Gb/s over twisted pair. When ﬁ ber is used there are different types of ﬁ ber 
as well, there is single mode and multi-mode and the decision regarding which to 
use is based on distance and bandwidth capacity. The main difference between single 
mode and multi-mode is the optical wavelength and the distance the wavelength 
will travel. Multi-mode is typically used for shorter distances, which can vary based 
on how much bandwidth is needed. Single mode ﬁ ber has fewer limitations and can 
handle greater distances. Another advantage of single mode is the ability to use mul-
tiple wavelengths on a single ﬁ ber. This allows multiple signals to be multiplexed on 
a single ﬁ ber pair. 

124
Facility Planning and Design
 When calculating cable management, if there is enough space available, keep-
ing cable types segregated will make them easier to manage. When choosing ﬁ ber, 
sleeves, or buffer tubes can come with a mixed set of single and multi-mode 
strands. These can be mixed if there is a combination of low bitrate and high 
bitrate trafﬁ c in the same path (e.g. video and automation control). Fiber optic 
cable is typically run through innerduct, a ﬂ exible conduit that complies with 
electrical codes and is easier to work with than rigid conduit. A ﬁ ber sleeve with 
twenty-four (24) strands is about the same diameter as a single coax cable, enabling 
more capacity while occupying considerably less physical space. 
 The IP- and ﬁ le-based broadcast environment is based on an IP network infra-
structure. There are different network requirements to support media. Some are 
the same as in the enterprise; however, most are different. 
 
 Network switches are the new core components: 
 
 There are new forms of routing and distribution 
 
 VLANs segregate media from command and control 
 FIGURE 7-5 

Facility Planning and Design
125
 Media is a bandwidth and network hog 
 QoS is critical to media performance. 
 The actual network routers, switches, and ﬁ rewalls are the new core in the media 
IP architecture. The topology of routing and distribution has changed. The router 
in IP design is very different from an audio/video router. The IP router is an access 
controller and it’s the managed switch that controls signal movement. Signals 
move without the need for separate matrix controllers; IP addressing enables ﬁ le 
and stream movement. The automation software triggers events to move ﬁ les and 
streams. The IP switch uses trunking and access lists to control which IP paths 
interconnect and where separation is needed. 
 Network switches handle a broad variety of packet trafﬁ c. Managing that trafﬁ c is 
critical, and shaping the ﬂ ow of data to ensure that there is no disruption in services 
means conﬁ guring the network with parameters and a priority structure—meaning 
determining which packets should have priority over others. This is known as Quality 
of Service (QoS), and is a critical conﬁ guration component of the network in media 
transport and management. 
 When planning the design and conﬁ guration, the engineer needs to consider 
the number of network segments, devices, distances, QoS, and latency require-
ments for each data type. 
 The most common devices in a large installation are layer 2 and layer 3 switches. 
The differences are in the features and functions between them. At the core, layer 2 
switches provide high speed and low latency, but do require a router to set up 
multiple VLANs. 
 Layer 3 switches can be placed anywhere in the network because they handle 
high-performance LAN trafﬁ c and can cost-effectively replace routers. 
 Above, there are layer 3 switches in each operational area, with a ﬁ rewall and 
router in between. Layer 3 network switches can handle the high bandwidth 
trafﬁ c and additionally have the control and access features needed in a VLAN 
infrastructure. The production and business groups both need Internet and VPN 
access. There is a ﬁ rewall and router between internal networks and outside net-
works. All Internet services are outside the ﬁ rewalls with controlled access. Even 
the streaming server sits outside the public ﬁ rewall. 
 The thinking process for designing an IP architecture and infrastructure is a 
departure from that of the more traditional infrastructure. 
 
 Infrastructure Technology 
 IP Network vs. SDI 
 IP Routers & Switches vs. AV Routers, DA, A/D, D/A 
 Copper vs. Fiber, Twisted Pair vs. Coax 
 
 Single Mode and Multi-mode 
 
 Standards EIA/TIA vs. SMPTE/ITU-R 
 Cable Pathways and Management 
 
 Bandwidth Management 
 
 Networks Segments 

126
Facility Planning and Design
 It’s all about the network. Servers and storage can be physically located anywhere 
on premises or off-site. With enough bandwidth and the proper conﬁ guration, 
users will be unaware of where the devices actually are. 
 Network switches cannot completely replace an SDI router quite yet. In the 
IP architecture, a layer 3 switch and ﬁ rewall are the new router, DA, A/D, and 
D/A. The designer needs to know when to transition from copper to ﬁ ber and 
when to use single mode vs. multi-mode. Good cable management means con-
forming to the EIA 568 wiring standard for telecommunications and computer 
networks. 
 When designing control and equipment rooms, today’s computer worksta-
tions, small control surfaces, articulated equipment arms, remote devices, and 
multi-view displays make it easier and more comfortable for the operators and 
occupy less physical space. 
 
 Operations Technology 
 
 Control Surfaces and Multi-viewers 
 
 KVM over IP Matrices 
 
 Robotics and Automation 
 
 Command and Control Embedded with the Stream 
 
 Database and Network Management 
 KVM over IP matrices are extenders that enable a single keyboard, tracking device 
(mouse), and screen to access and control a number of server applications from 
multiple user positions. 
 Robotics and automation, either fully automated or partially automated, create 
operational efﬁ ciencies. Network design is critical to ensure the automation com-
mands trigger the appropriate event at the exact time. 
 The IP network carries the media and all the command and control data. 
 Metadata, database management, and network management are the core tech-
nologies in the IP architecture and the engineer’s new responsibilities. 
 Let’s review the physical plant and the considerations in design: 
 
 Space Planning 
 
 Electrical Distribution and Protection 
 
 Mechanical Systems 
 
 Telecommunications 
 Technical and production spaces can be more efﬁ cient. The networked infrastruc-
ture enables edit, graphic, and media management to be located anywhere in the 
facility. New lighting technologies reduce the power and cooling requirements and 
change studio power distribution. 
 Computer workstations for craft production and LCD panels for displays 
reduce power to the control rooms. In the equipment center there are changes 
in power distribution and load calculations. Servers, switches, and storage can 

Facility Planning and Design
127
operate on higher voltages, adding different electrical distribution requirements. 
IP devices need sufﬁ cient UPS battery to allow them to intelligently shut down in 
the event of a power outage. Even in instances where there is no generator, critical 
systems should be protected. 
 The reduction in power and heat generation in control spaces has a beneﬁ cial 
impact on the cooling loads. While servers, storage, and switches generate heat in 
the equipment center, there is less overall load. Changing the mechanical loads 
also has a beneﬁ cial impact on the power loads. 
 And since the entire IP architecture and infrastructure is based on a telecom-
munications topology, this is a crucial component of the IP facility. Telephone 
services are IP, and Internet access is more than just providing web services and 
email—it is a critical service to some of the delivery platforms. Telecommunica-
tions includes all the dedicated IP paths for STL and point-to-point delivery of 
content to multiple delivery platforms. On the production side, intercom uses 
VoIP and remote access to many of the services within the media management 
systems. 
 Monitoring and management in the multi-platform, multi-channel distribu-
tion ecosystem brings a new dimension when planning and designing the physi-
cal IP facility. Multi-viewers are embedded into router frames; they have audio 
metering and clock and countdown timers with under-monitor displays. They can 
monitor servers directly without needing a video card in the server. 
 Planning and design for an IP production center changes the way people have 
been working and interacting. The engineering team needs additional skills and 
knowledge. It wouldn’t be a complete picture without looking at the physical plant 
and the changes there. 
 

 The transition to IP in the broadcast and production industry is a game changer 
and a new puzzle to solve. 
 This book identiﬁ es many of the new puzzle pieces and explains where they 
may ﬁ t in the complete picture of the IP- and ﬁ le-based infrastructure and archi-
tecture. The goal is to help the reader understand many of the requirements to 
meet these new challenges. 
 There are new considerations in the planning and design of an IP- and ﬁ le-
based broadcast and production facility. It’s important to understand the changes 
the new technology brings and the impact it has on operations, workﬂ ow, and 
processes, including the physical design of the facility. 
 This book takes a big-picture perspective in examining the entire lifecycle of 
media and the technology architecture, workﬂ ows, and business process that make 
up the broadcast and production ﬁ le-based and IP ecosystem. As the full picture 
of the puzzle takes shape it reveals the entire architecture inclusive of workﬂ ows, 
facility planning, and many of the changes in roles and responsibilities. 
 There is no single solution or “silver bullet” that addresses all the aspects of 
this change. From the beginning of broadcast television, there has been no single 
manufacturer or service provider that has had all the technology or solutions. 
Broadcast and production is an integrated environment and ecosystem of differ-
ent technologies, services, and operations to create, produce, manage, and deliver 
programs. That has not changed. 
 Broadcast and production has been evolving over time. There have been many 
transitions and new technologies introduced. These were all in support of a single 
NTSC or PAL format for distribution. The technologies brought more features 
eight 

Review
129
and capabilities with new standards. The industry went from analog to digital 
and then to high deﬁ nition. The transition to IP- and ﬁ le-based technologies con-
verged in a perfect storm of change as new delivery platforms were introduced 
demanding more content in so many new formats. 
 The planning and design process begins with some core decisions. Content for-
mats, ﬁ le formats, bitrates, archive and retention policies all play a role in deciding 
which technologies are best suited for the facility. How much automation will run 
the facility? In the past these were not the primary considerations when planning 
or upgrading a facility. 
 The ﬁ rst pieces of the new puzzle introduced the changes in the capture or 
recording process. The creation process originates the ﬁ rst elements of metadata 
and where the transition from essence to asset begins. Metadata is one of the key 
elements in the IP- and ﬁ le-based value chain and is the common thread through-
out the ecosystem. The capture process is based on the formats and standards for 
streams and ﬁ les. The acquisition process now includes in addition to cameras, 
computers, tablets, and phones as capturing devices. The recording is to a com-
puter, server, hard disk, ﬂ ash memory, solid state memory, optical, digital tape, 
and the cloud. 
 The introduction of new and multiple delivery platforms plus the addition 
of second and third screen interactive companion content has presented new 
demands on creation, production, and distribution. Creating program content 
for web, tablet, and phone is different than for a large-screen TV. All these digital 
platforms provide valuable information on what the consumer is interested in and 
their usage habits, using metadata. Metadata is stored and managed in databases 
that interact with other databases. Metadata is the command and control informa-
tion that moves content throughout the IP infrastructure. 
 IP- and ﬁ le-based broadcast and production is more than a DAM or a MAM. 
It is all the technologies that encompass the entire media lifecycle and value chain 
from acquisition and management to the transport of media. There is no single 
technology that addresses the entire lifecycle. 
 The IP- and ﬁ le-based technology infrastructure is network, applications, 
servers, and storage. The network is the core technology and backbone that 
the entire media management architecture sits on. It is a multi-layer topol-
ogy that needs to be carefully planned and designed. The network provides 
media transport, communications, command and control, management, and 
the integration between systems. There are different types of networks that 
integrate as a complete system, there are networks for transport (i.e. SONET, 
MPLS, and Fast Ethernet), infrastructure (TCP, UDP, UniCast, and Multicast) 
and storage (Fiber Channel, iSCSI, and UltraWide SCSI). The integration 
between the enterprise LAN and broadcast LAN depends on security policies 
and ﬁ rewalls. VLANs segregate the different services on the network replacing 
discreet systems. 
 There is a tighter integration across the production, broadcast, and business 
units in every organization. This tighter integration is between the applications 

130
Review
and databases each of the business units use, and that creates new workﬂ ows and 
processes. 
 Governance is the rules and policies that manage the media and metadata and 
control all the processes. These are the rules that tell the automation systems how 
to manage and move the media throughout the infrastructure. 
 The role of the broadcast engineer has expanded to network administrator, 
database manager, and applications admin. The diagnostics and tools to monitor, 
manage, and maintain an IP-centric environment are different than SDI. BitRate 
Error, Packet Loss, and Checksum are some of the new anomalies that can cor-
rupt media. The design criteria of the IP cable plant is different, there is more 
ﬁ ber optic cable plus Category5E, 6E, and 8E copper and these have different 
parameters than Coax and multi-pair. The broadcast engineer and IT engineer 
work closely together to maintain the systems. Maintenance is sustainability, and 
the broadcast engineer needs to be more aware of the lifecycle of hardware and 
software. 
 The equipment room resembles a datacenter and the control room is now a 
workstation with large ﬂ at panel displays using multi-viewers, operated and man-
aged by control surfaces, with keyboard, mouse, and display that access the servers 
and applications in the core equipment room using KVM extenders and matrices. 
There are different considerations for the architectural, electrical, and mechanical 
design. Some of these changes are more efﬁ cient and others are just different. The 
total cost of ownership of the technology includes annual service contracts and the 
overhead costs of the electrical and mechanical systems. 
 Transmission, contribution, and distribution technologies have changed. 
Cable, Satellite, and Over-the-Air are not the only distribution channels. There 
is Over-the-Top, broadband, and mobile. The STL to a transmitter is a ﬁ ber con-
nection and the transport stream is ASI and MPEGTS. There are ﬁ le accelerator 
service providers (i.e. Aspera and Signiant) that optimize the transport of ﬁ les over 
IP networks for contribution as well as delivering programs to cable and satel-
lite services providers. Files and streams are sent to content distribution networks 
(CDN) that also handle user authentication. 
 The cloud is changing workﬂ ows and posing questions on how much hardware 
infrastructure is practical to build in the facility that will have a limited lifecycle 
and have all the associated costs if the facility needs to scale to grow. The cloud 
offers scaling, access, and reduces overheads. Which processes will beneﬁ t by mov-
ing to the cloud? As the cost of bandwidth continues to drop, is it practical to 
move high value content to the cloud? Automation, content approval, and media 
management are all candidates for cloud solutions. 
 There are many more pieces to this complex puzzle. The broadcast industry 
continues to adjust and evolve, on the production side 1080P, 4K, and 8K will 
provide higher resolutions and quality and stay in SDI. However, if these are cap-
tured then it goes to ﬁ le at very high bitrates and if it is live, there needs to be high 
bandwidth transport available. Multiplexing cameras onto single ﬁ ber paths could 
change the way remote production is supported. On the distribution side MPEG4 

FIGURE 8-1

132
Review
and HEVC are optimizing compression and with higher quality, allowing service 
providers to use less bandwidth to deliver programs. 
 The planning and design of the IP- and ﬁ le-based broadcast center is a dif-
ferent knowledge base and skill set. The adoption of these new technologies and 
workﬂ ows is an entire change management program, and the topic of another 
book.  

 Deﬁ nitions 
 Digital Asset Management  consists of management tasks and decisions sur-
rounding the ingestion, annotation, cataloging, storage, retrieval, and distribution 
of digital assets. Digital photographs, animations, videos, and music are samples 
of media asset management 
 Digital Asset Management systems include computer software and/or hard-
ware systems that aid in the process of digital asset management. 
 The term “Digital Asset Management” (DAM) also refers to the protocol for 
downloading, renaming, backing up, rating, grouping, archiving, optimizing, 
maintaining, thinning, and exporting ﬁ les. 
 The term  “Media Asset Management ” (MAM) is a sub-category of “ Digital 
Asset Management ,” mainly for audio, video, and other media content. 
 Metadata  is the description of the asset and the description depth can vary 
depending on the needs of the system, designer, or user. Metadata can describe, 
but is not limited to, the description of: asset content (what is in the package?); 
the means of encoding/decoding (e.g. JPEG, tar, MPEG2); provenance (history to 
point of capture); ownership; rights of access; as well as many others. 
 Glossary 

134
Glossary
 Terminology 
 There are four core elements that are often identiﬁ ed as integral to digital manage-
ment systems. 
 1. Managing the Content  
  
With a database operating in the background, a collection of assets is orga-
nized through some sort of user interface. User interactions with the assets 
are managed through this engine, whether requests are made to temporarily 
take content off-line or to ask for content “ingests” to the system or content 
“downloads” or “streaming” for consumption. 
 2. Describing the Content 
  
The actual media item is referred to as “essence.” The item itself represents half 
of its value to an organization or consumers. The other half is captured in the 
well-formed descriptions associated with the item, whether the descriptions 
are about intellectual content, ownership, rights and use restrictions, or iden-
tiﬁ cations of the forms and formats in which an item is available for review 
and playback. This is metadata, and it is captured in another engine driven by 
a database. 
 3. Finding Content 
  
With metadata in place, a search engine is engaged in order to allow con-
sumers to ﬁ nd and display desired content. Searches can be conducted by 
keywords, database ﬁ eld values, virtual catalogs of items, or by hierarchically 
nested directories of content. 
 4. Controlling Access 
  
Often referred to as security logic, a layer of controls are present in order 
to generate types of user groups whose members have certain access and 
re-purposing privileges over content items. Access control is permissions 
control. 

Glossary
135
 Glossary 
 Archive 
 Manages policies for retention in digital, on-line, near-line, and off-line storage. A 
well-managed archive enables efﬁ cient searchable access to inventory irrespective 
of location or format. Maintains effective accessibility to collections through cata-
loging, quality control of metadata and format, preservation, and conservation of 
digital and analog carriers of content. 
 Asset 
 Anything that has value to an organization can be considered an asset. An asset 
is a conceptual work consisting of physical and/or digital entities and associated 
metadata. Video, audio, photos, graphic art, and documents (i.e. Docs, Presenta-
tions, Spreadsheets, PDF) are all examples of assets. An asset may exist as a concept 
before an instantiation is rendered. For example, a scheduled event that has not 
yet occurred is a conceptual asset and the eventual recording of the event is an 
instantiation of that asset. 
 Asset Management 
 The term “Asset Management” when taken in isolation can be confused with Digi-
tal Asset Management. In general, it relates to the management of either physi-
cal objects, locations, or items of value and might include: computers, furniture, 
property and buildings or ﬁ nancial instruments such as equities or bonds. Digital 
Asset Management and derivative terms like Brand Asset Management usually 
speciﬁ cally relate to digital ﬁ les which are usually (but not always) media related. 
 Audio Video Interleave (AVI) 
 Audio Video Interleave (AVI) is a popular multimedia format typically used for 
delivery of video content. AVI was invented by Microsoft in the early 1990s. Like 
QuickTime (a competing technology invented by Apple around the same time), 
AVI is known as a Container Format because it contains content that may be com-
pressed using a variety of other codecs such as MPEG. Despite being technically 
inferior to a number of other formats, it has achieved a high level of market pen-
etration and is widely supported by most video editing and playback software 
 Broadcast Asset Management 
 Broadcast Asset Management is another specialist area of Digital Asset Manage-
ment and enables organizations who own (or have rights to) time-based media 
assets (e.g. audio or video). In a Broadcast Asset Management system, greater 
emphasis is placed on the ability to manage dynamic media, for example, by being 
able to transcode footage or append metadata to speciﬁ c points in the content. 

136
Glossary
 Cataloging 
 Cataloging means the high-level process of adding metadata to assets in a Digital 
Asset Management system. These are the words and terms that are part of the 
metadata schema used to identify an asset that enables search and ﬁ ltering tools 
to locate it. These can be keywords, the relationship to other assets (parent–child), 
events, etc. 
 Cloud Computing 
 Cloud is often used as a euphemism for the Internet and Cloud Computing means 
services that may be offered by using multiple servers across the Internet. The 
main beneﬁ ts of Cloud Computing are scalability, robustness, and reduced capital 
expenditure (for the user of cloud-based services). Scalability is made possible by 
adding more nodes (servers) to increase available capacity and improve perfor-
mance. Robustness can be enhanced by distributing trafﬁ c across multiple nodes 
and providing redundancy or failover in the event one or more nodes fail. Capital 
expenditure can be saved when using cloud-based services since the service pro-
vider will provide the infrastructure and communications required to support 
their service (or build it on top of an existing cloud-based provider). Cloud Com-
puting makes considerable use of Virtualization technology to simplify the main-
tenance and deployment of multiple service nodes. There are a variety of services 
provided via the cloud and the deﬁ nition has become somewhat blurred in recent 
years as vendors or service providers endeavor to associate existing application 
services with a term which is perceived as fashionable. Some examples of Cloud 
Computing include the Amazon S3 storage platform, web-based application ser-
vices (e.g. Google Applications), Cloud Hosting, and Content Delivery Networks 
(CDNs). There are a range of other cloud-based services speciﬁ c to Digital Asset 
Management, video transcoding being a notable offering. 
 Codec 
 Codec stands for coder/decoder and refers to the encoding of analog media like 
audio or video into digital format and subsequent decoding upon playback. 
Codecs are methods of achieving this process (they are often called “algorithms”). 
The encoded media are sometimes referred to as essences. For practical purposes, 
the encoding usually means compressing the original media so it produces a ﬁ le 
that is usable and can be stored without occupying vast amounts of storage space. 
Media formats for audio and video employ different codecs—generally there is 
an inverse relationship between the level of compression and the quality of the 
corresponding output. 
 Container Format 
 Container format is usually applied to multimedia digital assets and means that 
the ﬁ le type is not a compression technology (or codec) but is used to hold media 

Glossary
137
that has been encoded by other technologies. Some popular container formats 
include MXF, LXF, GXF, AVI, DNG, and QuickTime. 
 Content Delivery Network (CDN) 
 Content Delivery Networks or CDNs are dedicated networks with high levels of 
capacity speciﬁ cally designed for the distribution of bandwidth heavy content. 
The most common use case scenario for a CDN is on-line advertising and in par-
ticular where rich media like video is utilized as part of the presentation. A CDN 
enables the content only to be distributed without the expense and complexity of 
building multiple servers. CDNs are speciﬁ cally optimized for media delivery and 
provide services such as media streaming. 
 Controlled Vocabulary 
 Controlled vocabularies are used in indexes, subject headings, thesauri, and taxon-
omies. Rather than presenting a free form natural language vocabulary where any 
term can be supplied, controlled vocabularies offer pre-selected terms for users to 
choose from. 
 Database Server 
 A database server is typically used in a DAM system to hold metadata about assets. 
The majority of modern databases are known as  Relational Databases  (the correct 
term is  RDBMS [Relational Database Management System] ). In a relational data-
base, tables of information are connected together by using identiﬁ ers (or indexes) 
to query them. Some examples of database servers in current use include: SQL 
Server, MySQL, Oracle, and Postgres. 
 Data Migration 
 Data migration is the transfer of data from one database to its replacement. After 
successful data migration, the original system usually ceases to be in use. Contrast 
with systems integration which involves the sharing of data between two live data-
bases systems that will both remain operational. 
 DBMS 
 An abbreviation for Database Management System. See Database Server for a 
more detailed description. 
 Derivative Files 
 Derivative ﬁ les describe assets that are created from the original. In Digital Asset 
Management systems, these can refer to previews that enable users to see what an 
asset looks like before they download it. They may include a variety of options 
such as thumbnail images, Flash Video, low resolution, or watermarked editions 

138
Glossary
of images. As well as previews, derivative ﬁ les sometimes refer to assets that will 
be used for production purposes but where some key aspect has been altered (e.g. 
the size, format, or color space). The term  derivative ﬁ les  can almost be used inter-
changeably with surrogate ﬁ les, although the former expression implies a wider 
range of uses. 
 Digital Asset Management (DAM) 
 Digital Asset Management (DAM) is a collective term applied to the process of 
storing, cataloguing, searching, and delivering computer ﬁ les (or digital assets). 
These may take the form of video, audio, images, print marketing collateral, ofﬁ ce 
documents, fonts, or 3D models. DAM systems centralize assets and establish a 
systematic approach to ingesting assets so they can be located more easily and 
used appropriately. 
 Digital Content Management (DCM) 
 Digital Content Management (DCM) is synonymous with Digital Asset Manage-
ment. Although technically it speciﬁ cally relates to media content as opposed to 
general data assets, in practical terms there is no difference between the two descrip-
tions. The phrase Digital Content Management is often used to avoid confusion 
with Asset Management, which has a variety of meanings across different industries. 
 Digital Rights Management (DRM) 
 Digital Rights Management (DRM) refers to technology and practices used to 
protect digital intellectual property from being used in a way that breaches the 
terms of its license. This generally means preventing assets from being illegally 
copied. The term can have multiple meanings depending on whether it is being 
used by asset consumers or asset suppliers. In the latter case it will often imply 
the use of some kind of technology to prevent media from being copied from one 
device to another (MP3 ﬁ les is particularly common), however, it can also mean 
controls established by media users to prevent intellectual property from being 
accidentally used without permission. 
 Digitization 
 Digitization is the conversion of analog or physical assets into digital equivalents. 
The methods for doing this are as varied as the media that a Digital Asset Manage-
ment system can support. The scanning of images and conversion of ﬁ lm or video 
tends to be the most common form of digitization activity. The need to digitize 
assets is gradually diminishing as more media is recorded directly in digital formats. 
 Dublin Core Metadata Initiative (DCMI) 
 The Dublin Core Metadata Initiative (DCMI) is a reference to a metadata standard 
and the organization that ﬁ rst established it. Dublin Core Metadata is common 

Glossary
139
in public sector Digital Asset Management systems as well as other archives and 
repositories of information. The aim is to provide a standardized core set of ﬁ elds 
or criteria for the description of content (in a broad sense) as well as a framework 
for adding content-speciﬁ c extensions. DCMI ﬁ elds can be theoretically applied to 
almost any type of asset. DCMI data is sometimes used either in-line in the Meta 
tags of web pages (or as a reference to an associated XML ﬁ le) as well as for other 
content such as photos, documents, videos, etc. 
 Enterprise Content Management (ECM) 
 Enterprise Content Management (ECM) is a wide-ranging term that is sometimes 
incorrectly used instead of Digital Asset Management (DAM). ECM systems tend 
to be large-scale repositories of many types of content held across the entirety of 
an organization. As well as digital media, nearly all material (including opera-
tional documents and ﬁ les) may be included in the scope of an ECM implementa-
tion. The objective of providing ECM is usually to offer a single interface where 
employees can gain access to all of an organization’s data. Many DAM systems are 
being integrated with ECM as an alternative method that enables organizations to 
leverage the beneﬁ ts of both. 
 Encapsulated PostScript (EPS) 
 Encapsulated PostScript or EPS is a derivative of the PostScript standard and is 
a digital image format. EPS ﬁ les are fully self-contained (or encapsulated) Post-
Script documents that come with an associated preview image so the user can 
view them. EPS ﬁ les are more prevalent with specialist structured drawing pro-
grams such as Adobe Illustrator but are still supported by most modern desktop 
applications. 
 Essences 
 Essences refer to raw audio or video streams used in media ﬁ les. Essences will usu-
ally be encoded with a Codec such as MPEG or MP3. 
 EXIF—Exchangeable Image File Format 
 EXIF is a metadata standard used to store information about digital images cre-
ated by the Japan Electronic Industries Development Association (JEIDA, later 
renamed JEITA—Japan Electronics and Information Technology Industries 
Association) in 1998. EXIF data is usually stored inside a JPEG or TIFF ﬁ le, i.e. it 
accompanies the image rather than being held in sidecar ﬁ les (in the same fashion 
as IPTC and XMP). In particular, EXIF is used by manufacturers to record techni-
cal information about the digital camera used to shoot an image. XMP data offers 
many of the beneﬁ ts of EXIF but in a more ﬂ exible and easier to manipulate fash-
ion, however, the support for it by the digital imaging industry has ensured that 
EXIF remains active and in widespread use. 

140
Glossary
 Flash 
 Flash is an application used to create ShockWave Flash (SWF) ﬁ les and associ-
ated media such as Flash Video (FLV). Although the ﬁ les are often referred to 
as “movies” they are frequently applications, interactive features, animations, or 
games. Flash was brought to prominence by Macromedia who acquired the origi-
nal application in 1996. In Video Digital Asset Management systems, Flash movies 
(and FLV in particular) are often used as preview formats to allow users to check 
video assets before downloading them. 
 Flash Video (FLV) 
 Flash Video or FLV is a compressed video format developed speciﬁ cally to allow 
video to be played back over the Internet via the Flash player. FLV ﬁ les tend to be 
considerably smaller than conventional video formats which make them especially 
useful for previewing media prior to download in Video Digital Asset Manage-
ment systems and websites that use video. 
 Guide File 
 Guide ﬁ les are a type of Controlled Vocabulary where an existing ﬁ le will be used 
to locate others. A common example is a report or brochure containing images. 
Users may know that a speciﬁ c image was used in a document, but be unable to 
locate it using other search strategies. Using the document as a guide ﬁ le, they can 
obtain a list of assets and search within this to isolate the one they require. 
 Hosting 
 Hosting refers to the process of storing and making accessible digital ﬁ les or services 
on a remote server. In most discussions about Digital Asset Management, hosting 
implies that the system will be managed externally by the vendor and/or an Internet 
Service Provider (ISP). Hosting can be either shared between several customers 
of the provider or dedicated where the whole server is set aside. For hosting to be 
effective, there are three key components necessary: an operational server, available 
storage capacity to hold ﬁ les, and bandwidth to send/receive requests. 
 ID3 
 ID3 is a metadata tagging standard typically used to embed metadata in MP3 
audio or MP4 video. Most end users come into contact with ID3 when using soft-
ware such as Apple’s iTunes to catalog their collections of music ﬁ les. Although 
popular, ID3 has a number of inherent limitations. As with the IPTC metadata 
standard for images, there are a ﬁ xed list of ﬁ elds: title, artist, album, year, genre, 
and comments. The key advantage of ID3 is the widespread availability of tools 
that can adjust the tag data and batch process audio or video ﬁ les. See the ofﬁ cial 
ID3 site for more information. 

Glossary
141
 Ingest 
 Ingest means to capture and acquire content through recording, transfer, or trans-
coding from mapped locations (recording equipment, third party storage, etc.). 
This process often includes pertinent cataloging of metadata. Ingest may be man-
aged from program schedule and ingest channel controller that controls all input 
devices, routing, and switching or may be performed via live, tape-to-ﬁ le or ﬁ le-
to-ﬁ le transfers, or other media acquisition gateways. 
 Interoperability 
 Interoperability means the ability of systems or processes to work together and is 
the conceptual basis of systems integration. Achieving interoperability involves 
two or more systems agreeing to a common protocol to exchange information. 
In more modern systems, this tends to be using technologies such as XML. 
The degree to which applications can easily integrate with each other depends 
on how detailed the protocol for communication is. There is a wide range of 
interoperability protocols used in Digital Asset Management, particularly in 
the area of exchanging metadata. A more common interoperability standard 
that has been widely adopted in the past is the Dublin Core Metadata Initiative 
(DCMI) schema. 
 JPEG 
 JPEG stands for Joint Photographic Experts Group, however, it more commonly 
refers to a compression standard that is used to reduce the disk space consumed 
by digital images. The compression method is referred to as “lossy” because some 
of the original data from the image is lost as part of the process. JPEG images are 
very common in Digital Asset Management solutions because they are natively 
supported by nearly all web browsers and their size is considerably smaller than 
other uncompressed formats such as TIFF (Tagged Image File Format). JPEG ﬁ les 
are usually recognizable by the extension .jpeg or .jpg. 
 Keywording 
 Keywording is a colloquial term applied to a speciﬁ c asset cataloguing activity 
where words, phrases, or terminology (or “keywords”) are attributed to assets 
as metadata. Keywording is particularly relevant for photographs and images as 
these types of assets lack any integral descriptive information to help users identify 
whether they are suitable for their needs. 
 Lossy 
 “Lossy” codecs are those that compress the source media by removing (or losing) 
some of the information to achieve the result; MPEG is an example of a lossy 
codec. 

142
Glossary
 Media Asset Management (MAM) 
 Media Asset Management (MAM) is generally considered as simply an alterna-
tive term for Digital Asset Management, although some would argue that a MAM 
system only supports media ﬁ les rather than any type of digital ﬁ le. To a greater 
extent, the terms are interchangeable; the expression tends to be favored when 
discussing Digital Asset Management for video or broadcast media contexts. In 
some cases, this term can refer to editorial or metadata activities associated with 
assets and DAM systems, for example, cataloging, keywording, or transcription 
of video footage or audio clips, although usually it will be called Media Asset Man-
agement Services. 
 Metadata 
 Metadata is often referred to as “data about data.” In a Digital Asset Management 
context it refers to descriptive information applied to assets to support a task or 
activity. The most common example is to help users to locate assets in searches. 
To help ﬁ nd suitable media, assets will generally have short descriptions or titles 
added to them as a basic minimum, although it is more common to add much 
more descriptive detail to help users to locate what they are looking for. As well as 
search metadata, workﬂ ow and business process information may also be added 
to determine what procedures are followed when users want to download assets. 
There are six primary types of metadata: administrative, technical, descriptive, 
preservation, rights management, and structural. 
 
 Administrative: Metadata related to the use and management of resources. 
 
 Preservation: A form of administrative metadata documenting the preserva-
tion processes performed on resources in both conventional and digitization 
workﬂ ow. 
 
 Rights Management: Metadata includes user-oriented rights information 
about copyright status, permissions, obligations, and restrictions pertaining 
to use of the asset. 
 
 Technical: Metadata that describes the creation and technical characteristics 
of digital and physical objects, such as ﬁ le format, resolution, size, duration, 
and track conﬁ guration. The automated capture of detailed technical meta-
data is central to obsolescence planning and preservation strategy. 
 
 Descriptive: Metadata that describes an asset for the purposes of discov-
ery and identification, such as titles, subjects, description, genres, and 
creators. 
 
 Structural Metadata: metadata may include tape labels, card catalog cards, 
meeting schedule information, logs, transcripts, etc. It is possible for metadata to 
exist before the asset exists; for instance when a scheduled event is given a title, 
date, and location, then record it later, the data about the event (the metadata) 
was created before the creation of a recording of the actual event. 

Glossary
143
 Metadata Standard 
 A metadata element set and/or schema that has been deﬁ ned and authorized by a 
national or international standards body, community, or professional association. 
A metadata standard serves as an authority on how to deﬁ ne and structure meta-
data (i.e. SMPTE, EBUCore). 
 MOV (QuickTime) 
 MOV is the ﬁ le format extension for QuickTime movies. 
 MPEG 
 MPEG stands for Moving Picture Experts Group and is a working group that 
develops standards for encoding digital video and audio. In the case of most Digi-
tal Asset Management systems, MPEG refers to a type of video format. There are 
three common variations of MPEG (named MPEG1, MPEG2, and MPEG4) along 
with MPEG7 and MPEG21. MPEG1 was the ﬁ rst standard for encoding video. 
MPEG2 enhanced the standard and improved support for digital storage on DVDs 
and other devices. MPEG3 was discontinued. MPEG4 increased the range of out-
put devices to cover mobile and Internet delivery. 
 MXF 
 MXF stands for Material eXchange Format and is a container format for time-
based media such as video and audio. MXF ﬁ les allow a number of essences 
encoded in a given codec to be (theoretically) stored in the same ﬁ le as the meta-
data which may be used to describe it. The implementation of MXF varies across 
different software systems, some will not actually use the same ﬁ le to store data 
but rely on a single MXF header ﬁ le with linked video, audio, and XML metadata 
stored as separate ﬁ les. Despite some problems with the handling of MXF it is 
gaining widespread acceptance among the A/V industry as a means of archiving 
time-based media. 
 NAS (Network Attached Storage) Server 
 Network Attached Storage (NAS) Servers are dedicated to the storage of digi-
tal ﬁ les. The purpose of having a computer whose sole purpose is ﬁ le storage is 
to reduce the load on a web, application, or database server. Unlike an external 
hard disk, a NAS is usually an actual computer with an operating system installed 
on it. Because NAS servers are specialized toward just providing storage, extra 
capacity can usually be added to them easily. NAS are commonly used for Digital 
Asset Management projects to provide sufﬁ cient storage capacity for reposito-
ries of larger ﬁ les such as video, print/artwork ﬁ les, or original high resolution 
images. SANs (Storage Area Network) are sometimes used as an alternative to a 

144
Glossary
NAS, although this is less common with a dedicated Digital Asset Management 
Software. 
 Normalization 
 The term “Normalization” is generally used when designing databases to hold asset 
metadata. This description is highly simpliﬁ ed but in essence it means to index or 
rationalize common groups of terms down to a series of numbers so that they 
can be searched more quickly. Normalized data is typically found in drop-down 
menus or sets of checkboxes. Fully normalized data is also considerably easier to 
manipulate, for example, if an index or ID is used to represent a tag, changing it 
once will cause all assets associated with that term to be updated also. Many of the 
biggest issues with normalization come when migrating data from a legacy system 
to a new Digital Asset Management system as the older application may not be as 
well normalized as the new one. 
 Ontology 
 Ontology has a philosophical deﬁ nition as well as an IT-oriented meaning which 
is more suitable in the context of Digital Asset Management. An ontology shows 
the relationships, properties, and functions between entities or concepts. Unlike a 
taxonomy, an ontology enables a wider range of relationships between attributes 
or terms than a simple hierarchy to be represented. This is of particular value when 
cataloging complex or multi-faceted asset repositories or if a DAM System is tightly 
integrated with Knowledge Management Systems (KMS) and Enterprise Search. 
 Proxy Files 
 This term refers to any ﬁ les that are created from the original for reference pur-
poses. They are used to represent assets—in general as a low resolution, trun-
cated, or otherwise constrained edition. The term is now the more popular way to 
describe non-original assets that have been rendered speciﬁ cally for use in Digital 
Asset Management systems. Also see Surrogate Files. 
 QuickTime 
 QuickTime is a widely adopted standard for delivery of multimedia content and 
was developed by Apple in the early 1990s, originally for Macintosh but Win-
dows support was added in a later release. Although capable of dealing with other 
types of media such as audio, text, and 3D panoramas (such as QuickTime VR), 
it is generally associated with video. The QuickTime ﬁ le format (see the MOV 
entry for more) is known as a Container Format because it holds various types of 
media—rather than being a native codec in its own right. The QuickTime player 
required to view media has a high penetration on Macintosh computers because 
it ships with this operating system. On Windows, it is reasonably widely deployed, 
however, it must be separately installed and this makes it less suitable than Flash 

Glossary
145
Video for pure web-based delivery using an online Video Digital Asset Manage-
ment system. 
 RAW 
 RAW ﬁ les are used by professional grade digital cameras to store images without pro-
cessing them into a more common image format such as JPEG or TIFF. The char-
acteristics of the RAW format that each camera writes tend to change depending on 
which component vendor a manufacturer has used for their device, this makes dealing 
with them using Digital Asset Management tools quite complex. The main beneﬁ t 
of retaining an image in RAW format is that the conversion to a more universally 
recognized standard tends to lose at least some information (i.e. the quality of the 
reproduction degrades). In this sense, RAW ﬁ les can be viewed as analogous to nega-
tives in traditional photography. Because original RAW ﬁ les are by their nature pre-
cious, sidecar ﬁ les are used by some applications such as Adobe Photoshop to store the 
changes made to them. Another format, DNG (Digital Negative) has been developed 
by Adobe as a Container Format for holding RAW ﬁ les along with other types of data. 
 RDBMS 
 An abbreviation for Relational Database Management System. See Database Server 
for more information. 
 Really Simple Syndication (RSS) 
 RSS is an XML-based Metadata standard that makes it simple for websites to syn-
dicate data from other web-based resources. RSS sources are typically referred to 
as “feeds” and include a short snippet about the article and a link back to it; they 
are common in blogs and with news sites as they allow readers to ﬁ nd out if there 
has been an update to a site without visiting it (using a “feed reader” or “aggrega-
tor”). RSS equipped Digital Asset Management systems allow new or revised assets 
to be published to other locations (e.g. an intranet or external website). 
 Real Time Streaming Protocol (RTSP) 
 Real Time Streaming Protocol is an Internet protocol for Media Streaming. It was 
developed by the Internet Engineering Task Force (IETF) to provide a basic set of 
commands for controlling dynamic media such as play, pause, record, etc. A vari-
ety of commercial and open source streaming products support RTSP, including 
Real Networks, Apple, and Microsoft via Windows Media Services. 
 SAN (Storage Area Network) 
 A SAN (Storage Area Network) is used to aggregate the storage capability available 
on different devices (e.g. servers) so they appear as a single disk. The key beneﬁ ts 
of this approach are efﬁ ciency and availability. By combining storage, SANs can 

146
Glossary
prevent uneven distribution of capacity and also offer greater reliability by repli-
cating data across the network. Most SANs require special ﬁ ber optic cabling to 
be effective as the performance across a conventional LAN is insufﬁ cient to be of 
practical use. DAM systems tend to be established as separate facilities so the use 
of a SAN is not as widespread, however, they can potentially offer some advantages 
and should be considered as an option when deciding a DAM hardware and host-
ing strategy. 
 Search 
 There are the various ways to ﬁ nd assets once they are cataloged: browsing, ﬁ ltered 
searches, associations (parent–child). There are faceted and contextual searches 
which use intuitive algorithms, and simple ﬁ ltered searches using known data 
points. 
 SWF (ShockWave Flash) 
 ShockWave Flash or SWF is the type of ﬁ le created by the Flash application. SWF 
movies are generally played back on the Flash player built into browsers, although 
the format can be used on mobile devices and is sometimes embedded into other 
programs also. 
 Sidecar Files 
 Sidecar ﬁ les are used to hold XMP data about a RAW image. This can include 
modiﬁ cations to the RAW ﬁ le, IPTC data, or other types of metadata. The beneﬁ t 
of using sidecar ﬁ les is that the metadata does not need to be contained with the 
image and can be manipulated separately. The disadvantage is that this does also 
mean that the metadata contained within them can become lost or divorced from 
the original. Sidecar ﬁ le data can also sometimes be stored in a database rather 
than ﬁ les to reduce the risk of loss at the expense of some ﬂ exibility. 
 Stemming 
 Stemming refers to a technique for increasing the quantity of search results by 
reducing a supplied keyword search term to the base element of the word (i.e. its 
 stem ) and then using that to try to identify other terms. For example, searching for 
 activation  in a Digital Asset Management system that supports stemming might 
yield results for  activate , actively , active ,  activeness , etc. 
 Streaming 
 Streaming means the ability of media to be viewed at the same time as it is being 
downloaded. The key beneﬁ t of streamed assets is that the user does not need to 
wait until the entire ﬁ le has been obtained before they can inspect it. There are 
two basic varieties of media streaming: live and archived. Live streaming involves 
capturing the output from a camera or other digital source and relaying it to 

Glossary
147
users in real-time as an event takes place. Archived streaming takes assets that 
have already been digitized. Streaming takes on particular signiﬁ cance when deal-
ing with dynamic time-based media such as audio or video and is (to a greater 
extent) essential for a Video Digital Asset Management system. There are a variety 
of media streaming protocols in widespread use, including FLV (Flash Video), 
Real Time Streaming Protocol (RTSP), and 3GP (for delivery to mobile devices). 
 Surrogate Files 
 This term is now losing favor to Proxy ﬁ les. Surrogate ﬁ les are those derived from 
an original digital asset and are typically used in combination with metadata to 
help users locate media prior to downloading them. They usually provide a pre-
view in the form of a thumbnail, smaller image, preview clip, or other ﬁ le that can 
be transferred quickly. In some cases, surrogate ﬁ les may be the actual ﬁ le supplied, 
for example, if an image is to be used in a PowerPoint presentation and the user 
does not have a graphics program installed. Surrogate ﬁ les are sometimes referred 
to as Derivative ﬁ les. Also see watermarking for information on how surrogate 
ﬁ les can be used to enforce copyright. 
 Systems Integration 
 Systems integration is the process of exchanging data between two or more IT sys-
tems to leverage further beneﬁ ts out of the original applications. In the context of 
Digital Asset Management it may mean either receiving digital assets from another 
system (e.g. artwork from a workﬂ ow system) or providing raw data to automate 
an on-going business process such as providing asset ordering and pricing infor-
mation to a ﬁ nance system. Frequently it now refers to the process of integrating 
Digital Asset Management systems into enterprise-wide search tools or portals 
using XML. Systems integration is distinct from data migration because both sys-
tems continue to be active and co-exist semi-independently. 
 Tagging 
 Tagging is a colloquial term given to the process of adding metadata generally and 
keywords in particular to digital assets. 
 Taxonomy 
 Taxonomy means a classiﬁ cation system that is usually hierarchical in nature (i.e. 
it has parent-child relationships between terms). Originally a scientiﬁ c term used 
to classify living organisms, taxonomies are now used to describe any abstract tree-
like metadata structure that is composed of categories, sub-categories, and nodes. 
The relationship between terms is more rigid than an ontology where terms can be 
inter-connected using a range of polyhierarchical or non-hierarchical systems, for 
example, venn diagrams or matrices (note that this does not mean that an ontol-
ogy is superior as a metadata structure—in many cases it is not). In Digital Asset 

148
Glossary
Management discussions, the design of a taxonomy to represent information 
about assets is important to enable the development of thesauri and controlled 
vocabularies for metadata entry and searching purposes. 
 Thesaurus 
 A thesaurus is a set of synonyms or related terms for a given word or description, 
unlike a taxonomy, it may be polyhierarchical and involve complex relationships 
such as broader or narrower terms. Thesauri describe the standard terms for con-
cepts in a controlled vocabulary. 
 Transcoding 
 Transcoding is the process of converting one video or audio format into another. 
In general it refers to the conversion of one codec to another (e.g. MPEG to FLV), 
although the description can also apply to conversions between container formats 
(e.g. QuickTime to AVI). 
 Usage Approval 
 A specialized type of Digital Asset Management workﬂ ow where a user must 
apply before they are given the rights to download or use an asset. Typically, it will 
involve the proposed usage being checked manually by a human being, although, 
if the asset has been tagged with suitable metadata it is possible to partially auto-
mate this process by directing it to the correct person. 
 Watermarking 
 Watermarking is often used to protect assets by applying a translucent logo or 
image over the top of a surrogate asset such as an image, video, or document to 
prevent it being copied and re-used without authorization. Watermarking is very 
common in stock photography libraries where Digital Asset Management systems 
have been used to create public catalogues. It is also common in corporate Brand 
Asset Management systems to help enforce copyright compliance. 
 Workﬂ ow 
 Workﬂ ow refers to the modeling of the steps required to achieve a task so it can 
be streamlined and managed more effectively. In the asset supply chains com-
monly used in Digital Asset Management systems, workﬂ ow is often used at the 
ingestion and usage approval stages. It may also be used to integrate with artwork 
tracking systems to automatically publish assets after they have been originated 
and approved. 
XBRL
 XBRL stands for eXtensible Business Reporting Language and is an XML-based 
metadata standard for representation of business, accounting, and ﬁ nancial data 

Glossary
149
as well as semantic relationships between these entities. As a standard it is speciﬁ -
cally focused on ﬁ nancial data that would historically have to be represented by 
either unstructured objects (e.g. a block of text) or an arbitrary method (e.g. a 
spreadsheet where there is an implicit structure but not necessarily a uniform 
structure). The type of ﬁ nancial data represented is not necessarily transactional 
but might include information such as net proﬁ t or other aggregate information. 
A typical use case scenario would be for investor relations or ﬁ nancial publishers 
who wish to represent corporate accounts in a method that can be analyzed via 
third party applications or other automated methods (e.g. stock screening). See 
the ofﬁ cial XRBL site for more details. 
 XML 
 XML is an abbreviation of eXtensible Markup Language. XML is a standard for 
creating markup languages which describe the structure of data so that it can be 
exchanged between two different systems. It is heavily used in systems integration. 
Most second generation Digital Asset Management (DAM) systems include fea-
tures that allow metadata and assets to be supplied to third party systems in XML 
format. More advanced Digital Asset Management systems also allow third party 
applications to integrate with them using XML web services. 
 XMP 
 XMP is an abbreviation of eXtensible Metadata Platform and is a form of XML and 
is a metadata standard for describing assets such as images and documents. XMP 
is widely regarded as the successor to IPTC as it allows the range of metadata ﬁ elds 
used to describe assets to be extended as required.   

This page intentionally left blank

 acoustics 117 
 acquisition 12, 14–15, 19, 40 
 administrative metadata 57 
 Adobe Anywhere 104, 105 
 analog, digital (SDI), and IP, evolution and 
comparison 28,  29 
 ANSI-ISO/IEC metadata standard 63 
 application servers 79–80 
 archiving 12;  see also storage architecture 
 asset creation process 64 
 asset management 12, 64–9 
 audio (AES) and video (SDI) 14, 18,  19 , 31, 
32; sychronization and latency 15, 81, 
92, 93; testing 91, 116 
 automation 9–11, 13; cloud-based systems 
105; and metadata 33, 56, 61; movement 
of media 48–50; “Studio in a Box” 32–3; 
studio production 31–2, 49; trafﬁ c 
workﬂ ow 51 
 bandwidth 81, 82; broadcast and 
enterprise, differences between 84; 
ﬁ rewalls 89; interconnections between 
switches 123,  124 ; private network  vs 
open Internet streaming 35–6 
 beginning-to-end planning 11–12 
 bitrates 25, 26, 36 
 broadcast and enterprise  131 ; differences 
between 84–5, 89, 92–3; integration 
between 38–42; interface 88 
 broadcast network, deﬁ nition 96 
 broadcasting, deﬁ nition 96 
 business integration 6–7, 42–4, 93–4 
 business intelligence (BI) 44–5, 61 
 business operations 40, 42 
 Business Process Managers 48 
 cables/ﬁ bers and connections 81, 82, 
114, 117–18; management 114, 122–4; 
non-IP signals  82 
 camera control units (CCUs) 115 
 camera robotics 31–2, 115 
 central repository and federated network 
storage 71–2 
 cloud services 35–6, 103–10; in broadcast 
105–7; decisions 107–8; editing and 
graphics systems 50, 110; potential 109–10; 
private build  vs outsource 108–9; public 
and private 105, 107–10 
 codecs 24–6 
 commercial networks 50–1; cloud services 
106–7 
 compression 19–20 
 concatenative loss 20 
 Index 
 Page numbers in  italics refer to ﬁ gures and tables. 

152
Index
 connectivity  see telecommunications 
 container formats 22, 24 
 content 3, 4, 5, 18 
 content distribution network (CDN) 4, 13, 
100, 102–3, 104, 106 
 contextual search 68–9 
 control rooms 32–3, 113;  see also “master 
control” room 
 costs: operating 9–11, 112–15; total cost of 
ownership (TCO) 8–11, 87–94 
 craft production 33–4, 40, 45–6; cloud 
services 50, 110; workstations 116, 126 
 creative process 40 
 data directors 69 
 database integration 58–60 
 decoding 20 
 delay  see latency 
 descriptive metadata 57 
 digital asset management (DAM) 64–5 
 digital ID 56 
 discovery, use of metadata 56 
 Discreet Cosine Transform (DCT) 
methodology 19–20 
 distribution 13, 41; cloud services 110 
 Dublin Core metadata schema 62, 63, 68 
 EBUCore 68 
 editing: EDL 45–6, 50, 106; NLE 
technology 49–50; workstations 46; 
 see also craft production 
 electrical system  see power 
 electronic program guide (EPG) 61, 65 
 encoding 18, 19; schemas 19–20;  see also 
codecs 
 engineering: broadcast and enterprise 
roles, differences between 92–3; 
hardware and software maintenance 52, 
88; and maintenance 83–5, 86–7, 89–90; 
monitoring and testing 90–3, 116; role 
and responsibilities 14, 15, 52 
 enterprise  see broadcast and enterprise 
 equipment room 114, 116, 117–18 
 ergonomics 118 
 essence 5, 12, 18; and asset creation 
process 64 
 Ethernet 49, 50, 80, 89; Fast 18, 35, 
36; router and switches 85; storage 
architecture 69 
 European Broadcast Union (EBU)/
EBUCore 68 
 executive management 61 
 extensibility, technology infrastructure 7, 
85–6 
 Extract-Transfer-Load (ELT) 59 
 faceted search 68 
 facitility infrastructure 8–9, 112–15; 
 see also  power; space 
 federated network and central repository 
storage 71–2 
 ﬁ ber optic cables  see cables/ﬁ bers and 
connections 
 ﬁ le(s): ingest process 19, 29; quality control 
20–1; and stream content 18 
 ﬁ nance group 43, 61 
 ﬁ rewalls 89 
 formats 21–3; and standards 23–7;  see also 
preservation 
 4K (UltraHD) 19, 21, 27, 29 
 governance 77–8 
 graphic production  see  craft production 
 hardware and software maintenance 52, 88 
 HD-SDI 21, 29, 91 
 heat: HVAC design 9, 112–15 
 Hierarchical Storage Management (HSM) 69 
 hot and cold spares 87 
 “Hot-Keys” 45 
 infrastructure as a service (IaaS) 104–5, 107 
 ingest 3, 4, 19; acquisition and capture 
27–31; ﬁ le-based architecture 19, 29 
 ingest manager 52, 78; control rooms 116 
 interoperability 56, 58–61 
 interpolation 20 
 IP: core processes 3–4; terminology 2–4; 
vocabulary 4–5 
 isolated (ISO) camera shots 29 
 keyboard, video and mouse (KVM) 83 
 latency 15, 81, 92, 93 
 legal department 43, 61 
 librarian, role of 52, 78 
 library/digital library 7, 40, 46; & 
preservation 56, 57; storage 69, 90; 
 see also asset management 
 lighting 117 
 lip sync 15, 81 
 live production 45–6;  see also  sports events 
 logging 3, 4, 12, 18, 45 
 long GOP (long group of pictures) 19–20 
 lossless and lossy compression 20 
 maintenance 83–5, 86–7, 89–90; software 
and hardware 52, 88 
 marketing: and business intelligence (BI) 
61; group 44–5 
 “master control” operators 52 

Index
153
 “master control” room 113, 115–27; alerts 
and alarms 90 
 media asset management (MAM) 42, 
64–5 
 media management 12–13,  39 , 40, 48, 
53–4,  55 ; asset management 12, 64–9; 
example 13; governance 77–8; new 
spaces 116; roles and responsibilities 78; 
storage architecture 13, 69–76;  see also 
metadata 
 meeting facilities 29–30 
 metadata  17 , 18; and automation 33; 
deﬁ nition 55–6; main types 56–7; 
standards 62–4, 68; storytelling and 
value of 64–9; teminology 5; trafﬁ c and 
scheduling process 51, 61; uses 13, 56; 
 see also asset management 
 mezzanine (production resolution) 
version 73 
 middleware 58–60, 61, 90 
 migration policy 73 
 mobile media 26–7 
 monitoring and testing 90–3, 116 
 MPEG formats 19–20, 21, 24–5, 26–7, 28, 91 
 multi-channel distribution 50 
 multiple-platform delivery 97–103 
 National Information Standards 
Association (NISA) 63 
 near-line storage 70, 73 
 Network Attached Storage (NAS) 80 
 network(s): integration and segregation 
80–3, 89; management 47; multi-layered 
topology 88–9; ports 114; switches 115, 
124–5 
 non linear editing (NLE) technology 49–50 
 ofﬂ ine storage 70, 73–4 
 online storage 70, 75 
 ontology in asset management 66 
 operating costs 9–11, 112–15 
 operations technology 126 
 operations workﬂ ows 45–8 
 orchestration 48–9 
 originator center  see  “master control” room 
 PBSCore 68 
 people/personnel: roles and responsibilities 
51–2; stakeholders and executives 41–2 
 platform as a service (PaaS) 104–5, 107–8 
 post-production 49–50 
 power 117–21; failures 118; loads 9; space 
and HVAC 8–9, 112–15 
 preservation 72–6; retention and migration 
76–7;  see also formats 
 private network  vs open Internet streaming 
35–6 
 private and public cloud services 105, 
107–10 
 process integration 42–3 
 production switchers 33, 35 
 production and technical workﬂ ows 45–8 
 proxy storage 70; cloud services 105, 106, 110 
 Public Broadcasting System (PBS): cloud 
services 106; PBSCore 68 
 public and private cloud services 105, 107–10 
 quality assurance and control 14–15, 20–1 
 remote and studio production  60 
 removable media 34–5 
 rights management 40–1; metadata 57 
 robotic cameras 31–2, 115 
 routers 80, 81, 85, 125 
 scalability/scaling 8, 86; cloud services 107; 
storage architecture 70–1, 73–4, 75 
 scheduling group 50–1 
 scheduling system (EPG) 61, 65 
 “scratch bins” 33–4 
 SDI 19; infrastructure 85; router 80, 81; 
types 21,  see also  audio (AES)/video 
(SDI);  speciﬁ c types 
 search tools 68–9 
 security 15 
 Serial Digital Interface  see SDI 
 servers 118, 119, 120–1 
 service level agreements (SLA) 87 
 sizing storage 73–5 
 SMPTE: Metadata Dictionary 63; new 
protocols 102 
 software and hardware: costs 87–8; 
maintenance 52, 88 
 software as a service (SaaS) 104–5, 108 
 solid state drives (SSD) 35 
 space 121, 126; acoustics 117; new 116; 
power and HVAC 8–9, 112–15 
 spares 87 
 sports events: American sports leagues 
centralized production 30–1; multiple-
platform delivery 100–3; Olympic 
Games 36 
 standards 123; and formats 23–7 
 storage architecture 13, 69–76 
 Storage Area Network (SAN) 80 
 stream/streaming 35–6; and ﬁ le content 18 
 structural metadata 57 
 studio: control and support rooms 115; 
design 115; lighting 117; production 
31–2, 49,  60 

154
Index
 “Studio in a Box” 32–3 
 “sunsetting” 88 
 sustainability of technology infrastructure 
8, 86 
 sychronization and latency 15, 81, 92, 93 
 taxonomy in asset management 66 
 technical operations 41, 47, 126 
 technology 42 
 technology infrastructure 7–8, 79–83; 
facility planning and design 125–6; 
planning for future 85–7 
 telecommunications 114, 121–2 
 television, deﬁ nitions of 96 
 terminology 2–4 
 testing, monitoring and 90–3, 116 
 total cost of ownership (TCO) 8–11, 
87–94 
 trafﬁ c: and scheduling process 51, 61; 
workﬂ ow 50–1 
 transcoding 19 
 transmission: and delivery 95–103 
( see also cloud services); monitoring 90 
 transmuxing 19 
 transport protocols 22 
 UltraHD (4K) 19, 21, 27, 29 
 video and audio  see audio (AES) and video 
(SDI) 
 virtual LANS (VLANS) 81–2, 89, 125 
 vocabulary 4–5 
 voice over IP (VoIP) 114, 121–2 
 waveform monitoring 91 
 workﬂ ow 11–12; and business process 
11, 38; deﬁ nition 38; processes and 
integration 5–6; production and 
technical 45–8; roles and responsibilities 
51–2; trafﬁ c 50–1 
 XML 58–60, 61, 62, 63 


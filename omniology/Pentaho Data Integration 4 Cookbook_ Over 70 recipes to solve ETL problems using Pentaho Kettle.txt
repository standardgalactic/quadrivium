
Pentaho Data Integration 4 Cookbook
Copyright © 2011 Packt Publishing
All rights reserved. No part of this book may be reproduced, stored in a retrieval system, 
or transmitted in any form or by any means, without the prior written permission of the 
publisher, except in the case of brief quotations embedded in critical articles or reviews.
Every effort has been made in the preparation of this book to ensure the accuracy of the 
information presented. However, the information contained in this book is sold without 
warranty, either express or implied. Neither the authors, nor Packt Publishing, and its 
dealers and distributors will be held liable for any damages caused or alleged to be 
caused directly or indirectly by this book.
Packt Publishing has endeavored to provide trademark information about all of the 
companies and products mentioned in this book by the appropriate use of capitals. 
However, Packt Publishing cannot guarantee the accuracy of this information.
First published: June 2011
Production Reference: 1170611
Published by Packt Publishing Ltd. 
32 Lincoln Road 
Olton 
Birmingham, B27 6PA, UK.
ISBN 978-1-849515-24-5  
www.packtpub.com 
Cover Image by Ed Maclean (edmaclean@gmail.com)

About the Authors
Adrián Sergio Pulvirenti was born in Buenos Aires, Argentina, in 1972. He earned his 
Bachelor's degree in Computer Sciences at UBA, one of the most prestigious universities 
in South America.
He has dedicated more than 15 years to developing desktop and web-based software 
solutions. Over the last few years he has been leading integration projects and 
development of BI solutions.
I'd like to thank my lovely kids Camila and Nicolas, who understood that I 
couldn't share with them the usual videogame sessions during the writing 
process. I'd also thank my wife who introduced me to the Pentaho world.
María Carina Roldán was born in Esquel, Argentina, in 1970. She earned her 
Bachelors degree in Computer Science at UNLP in La Plata; after that she did a 
postgraduate course in Statistics at the University of Buenos Aires (UBA) in Buenos Aires 
city where she lives since 1994.
She has worked as a BI consultant for more than 10 years. Over the last four years, she 
has been dedicated full time to developing BI solutions using Pentaho Suite. Currently 
she works for Webdetails, one of the main Pentaho contributors.
She is the author of Pentaho 3.2 Data Integration: Beginner's Guide published by Packt
Publishing in April 2010.
You can follow her on Twitter at @mariacroldan.

I'd like to thank those who have encouraged me to write this book: On one 
hand, the Pentaho community. They have given me a rewarding feedback 
after the Beginner's book. On the other side, my husband who without 
hesitation agreed to write the book with me. Without them I'm not sure I 
would have embarked on a new book project. 
I'd also like to thank the technical reviewers for the time and dedication that 
they have put in reviewing the book. In particular, thanks to my colleagues at 
Webdetails; it's a pleasure and a privilege to work with them every day.

About the Reviewers
Jan Aertsen has worked in IT and decision support for the past 10 years. Since the 
beginning of his career he has specialized in data warehouse design and business 
intelligence projects. He has worked on numerous global data warehouse projects 
within the fashion industry, retail, banking and insurance, telco and utilities, logistics, 
automotive, and public sector.
Jan holds the degree of Commercial Engineer in international business affairs from the 
Catholic University of Leuven (Belgium) and extended his further knowledge in the field of 
business intelligence through a Masters in Artificial Intelligence.
In 1999 Jan started up the business intelligence activities at IOcore together with some 
of his colleagues, rapidly making this the most important revenue area of the Belgian 
affiliate. They quickly gained access to a range of customers as KPN Belgium, Orange 
(now Base), Mobistar, and other Belgian Telcos.
After this experience Jan joined Cap Gemini Ernst & Young in Italy and rapidly became 
one of their top BI project managers. After having managed some large BI projects (up to 
1 million € projects) Jan decided to leave the company and pursue his own ambitions.
In 2002, he founded kJube as an independent platform to develop his ambitions in 
the world of business intelligence. Since then this has resulted in collaborations with 
numerous companies as Volvo, Fendi-LVMH, ING, MSC, Securex, SDWorx, Blinck, and 
Beate Uhse.
Over the years Jan has worked his way through every possible aspect of business 
intelligence from KPI and strategy definition over budgeting, tool selection, and software 
investments acquisition to project management and all implementation aspects with 
most of the available tools. He knows the business side as well as the IT side of the 
business intelligence, and therefore is one of the rare persons that are able to give you a 
sound, all-round, vendor-independent advice on business intelligence.
He continues to share his experiences in the field through his blog (blog.kjube.be) and 
can be contacted at jan.aertsen@kjube.be.

Pedro Alves, is the founder of Webdetails. A Physicist by formation, serious video 
gamer, volleyball player, open source passionate, and dad of two lovely children.
Since his early professional years he has been responsible for Business Software 
development and his career led him to work as a Consultant in several Portuguese 
companies.
In 2008 he decided it was time to get his accumulated experience and share his 
knowledge about the Pentaho Business Intelligence platform on his own. He founded 
Webdetails and joined the Mozilla metrics team. Now he leads an international team of 
BI Consultants and keeps nurturing Webdetails as a world reference Pentaho BI solutions 
provider and community contributor. He is the Ctools (CDF, CDA, CDE, CBF, CST, CCC) 
architect and, on a daily basis, keeps developing and improving new components and 
features to extend and maximize Pentaho's capabilities.
Slawomir Chodnicki specializes in data warehousing and ETL, with a background 
in web development using various programming languages and frameworks. He has 
established his blog at http://type-exit.org to help fellow BI developers embrace 
the possibilities of PDI and other open source BI tools.
I would like to thank all regular members of the ##pentaho IRC channel for 
their endless patience and support regarding PDI related questions. Very 
special thanks go to María Carina and Adrián Sergio for creating the Kettle 
Cookbook and inviting me to be part of the project. 
Paula Clemente was born in Sintra, Portugal, in 1983. Divided between the idea 
of spending her life caring about people and animals or spending quality time with 
computers, she started studying Computer Science at IST Engineering College—"the 
Portuguese MIT"—at a time where Internet Social Networking was a synonym of IRC. 
She graduated in 2008 after completing her Master thesis on Business Processes 
Management. Since then she is proudly working as a BI Consultant for Webdetails, a 
Portuguese company specialized in delivering Pentaho BI solutions that earned the 
Pentaho "Best Community Contributor 2011" award.

Samatar Hassan is an application developer focusing on data integration and business 
intelligence. He was involved in the Kettle project since the year it was open sourced. He 
tries to help the community by contributing in different ways; taking the translation effort 
for French language, participating in the forums, resolving bugs, and adding new features 
to the software.
He contributed to the "Pentaho Kettle Solutions" book edited by Wiley and written by Matt 
Casters, the founder of Kettle.
I would first like to thank Adrián Sergio and María Carina Roldán for taking 
the time to write this book. It is a great idea to show how to take advantage 
of Kettle through step-by-step recipes. Kettle users have their own ETL bible 
now. 
Finally, I'd like to thank all community members. They are the real power of 
open source software.
Nelson Sousa is a business intelligence consultant at Webdetails. He's part of the 
Metrics team at Mozilla where he helps develop and maintain Mozilla's Pentaho server 
and solution. He specializes in Pentaho dashboards using CDF, CDE, and CDA and also 
in PDI, processing vast amounts of information that are integrated daily in the various 
dashboards and reports that are part of the Metrics team day-to-day life.

Table of Contents
	
1
	
5
	
5
	
7
	
11
	
13
Getting data from a database by running a query built at runtime 
16 
Inserting or updating rows in a table 
18 
Inserting new rows where a simple primary key has to be generated 
23 
Inserting new rows where the primary key has to be generated based  
on stored values 
26
Deleting data from a table 
30
Creating or altering a database table from PDI (design time) 
34
Creating or altering a database table from PDI (runtime) 
37
Inserting, deleting, or updating a table depending on a field 
39
Changing the database connection at runtime 
43
Loading a parent-child table 
46
Chapter 2: Reading and Writing Files 
51
	
51
	
52
	
56
	
57
	
64
	
66
	
69
	
71
	
74
	
77

ii
	
79
	
82
	
82
	
86
	
89
	
93
	
93
	
94
	
97
	
103
	
106
	
108
	
112
	
114
	
121
	
125
	
125
	
126
	
129
	
131
	
135
	
137
	
139
	
141
	
144
	
149
	
149
	
150
	
154
	
157
	
160
	
165
	
169
	
173
	
175
Introduction 
175 
Splitting a stream into two or more streams based on a condition 
176 
Merging rows of two streams with the same or different structures 
184 
Comparing two streams and generating differences 
189

iii
Table of Contents
	
194
	
198
	
201
	
204
	
208
Chapter 7: Executing and Reusing Jobs and Transformations 
213
	
213
	
217 
Executing a job or a transformation from a job by setting arguments and 
parameters dynamically 
220 
Executing a job or a transformation whose name is determined at runtime 
223 
Executing part of a job once for every row in a dataset 
225
Executing part of a job several times until a condition is true 
231
Creating a process flow 
236
Moving part of a transformation to a subtransformation 
242
Chapter 8: Integrating Kettle and the Pentaho Suite 
249
	
249
Creating a Pentaho report with data coming from PDI 
252 
Configuring the Pentaho BI Server for running PDI jobs and transformations 257
Executing a PDI transformation as part of a Pentaho process 
259
Executing a PDI job from the Pentaho User Console 
266
Generating files from the PUC with PDI and the CDA plugin 
270
Populating a CDF dashboard with data coming from a PDI transformation 
276
Chapter 9: Getting the Most Out of Kettle 
283
	
283
	
283
	
288
	
293
	
302
Working with Json files 
305
Getting information about transformations and jobs (file-based) 
308
Getting information about transformations and jobs (repository-based) 
313
Appendix: Data Structures 
319
	
319
	
320
	
321
	
322
	
323

Preface
Pentaho Data Integration (PDI, also called Kettle), one of the data integration tools leaders, is 
broadly used for all kind of data manipulation, such as migrating data between applications 
or databases, exporting data from databases to flat files, data cleansing, and much more. Do 
you need quick solutions to the problems you face while using Kettle?
Pentaho Data Integration 4 Cookbook explains Kettle features in detail through clear and 
practical recipes that you can quickly apply to your solutions. The recipes cover a broad range 
of topics including processing files, working with databases, understanding XML structures, 
integrating with Pentaho BI Suite, and more.
Pentaho Data Integration 4 Cookbook shows you how to take advantage of all the aspects 
of Kettle through a set of practical recipes organized to find quick solutions to your needs. 
The initial chapters explain the details about working with databases, files, and XML 
structures. Then you will see different ways for searching data, executing and reusing jobs and 
transformations, and manipulating streams. Further, you will learn all the available options for 
integrating Kettle with other Pentaho tools.
Pentaho Data Integration 4 Cookbook has plenty of recipes with easy step-by-step instructions 
to accomplish specific tasks. There are examples and code that are ready for adaptation to 
individual needs.
Learn to solve data manipulation problems using the Pentaho Data Integration tool Kettle.
What this book covers
Chapter 1, Working with Databases helps you to deal with databases in Kettle. The recipes 
cover creating and sharing connections, loading tables under different scenarios, and creating 
dynamic SQL statements among others topics.
Chapter 2, Reading and Writing Files shows you not only the basics for reading and writing 
files, but also all the how-tos for dealing with files. The chapter includes parsing unstructured 
files, reading master/detail files, generating multi-sheet Excel files, and more.

Preface
2
Chapter 3, Manipulating XML Structures teaches you how to read, write, and validate XML 
data. It covers both simple and complex XML structures.
Chapter 4, File Management helps you to pick and configure the different options for copying, 
moving, and transferring lists of files or directories.
Chapter 5, Looking for Data explains the different methods for searching information in 
databases, text files, web services, and more.
Chapter 6, Understanding Data Flows focuses on the different ways for combining, splitting, or 
manipulating streams or flows of data in simple and complex situations.
Chapter 7, Executing and Reusing Jobs and Transformations explains in a simple fashion 
topics that are critical for building complex PDI projects. For example, building reusable 
jobs and transformations, iterating the execution of a transformation over a list of data and 
transferring data between transformations.
Chapter 8, Integrating Kettle and the Pentaho Suite. PDI aka Kettle is part of the Pentaho 
Business Intelligent Suite. As such, it can be used interacting with other components of the 
suite, for example as the datasource for reporting, or as part of a bigger process. This chapter 
shows you how to run Kettle jobs and transformations in that context.
Chapter 9, Getting the Most Out of Kettle covers a wide variety of topics, such as customizing 
a log file, sending e-mails with attachments, or creating a custom functionality.
Appendix, Data Structures describes some structures used in several recipes throughout the 
book.
What you need for this book
PDI is a multiplatform tool, meaning that you will be able to install the tool no matter what 
your operating system is. The only prerequisite to work with PDI is to have JVM 1.5 or a higher 
version installed. It is also useful to have Excel or Calc, a nice text editor, and access to a 
database engine of your preference.
Having an Internet connection while reading is extremely useful as well. Several links are 
provided throughout the book that complement what is explained. Besides, there is the PDI 
forum where you may search or post doubts if you are stuck with something.
Who this book is for
If you are a software developer or anyone involved or interested in developing ETL solutions, 
or in general, doing any kind of data manipulation, this book is for you. It does not cover PDI 
basics, SQL basics, or database concepts. You are expected to have a basic understanding of 
the PDI tool, SQL language, and databases.


Preface
4
If there is a topic that you have expertise in and you are interested in either writing or 
contributing to a book, see our author guide on www.packtpub.com/authors.
Customer support
Now that you are the proud owner of a Packt book, we have a number of things to help you to 
get the most from your purchase.
Downloading the example code
You can download the example code files for all Packt books you have purchased from your 
account at http://www.PacktPub.com. If you purchased this book elsewhere, you can visit 
http://www.PacktPub.com/support and register to have the files e-mailed directly to you.
Errata
Although we have taken every care to ensure the accuracy of our content, mistakes do happen. 
If you find a mistake in one of our books—maybe a mistake in the text or the code—we would be 
grateful if you would report this to us. By doing so, you can save other readers from frustration 
and help us improve subsequent versions of this book. If you find any errata, please report them 
by visiting http://www.packtpub.com/support, selecting your book, clicking on the errata
submission form link, and entering the details of your errata. Once your errata are verified, your 
submission will be accepted and the errata will be uploaded on our website, or added to any 
list of existing errata, under the Errata section of that title. Any existing errata can be viewed by 
selecting your title from http://www.packtpub.com/support.
Piracy
Piracy of copyright material on the Internet is an ongoing problem across all media. At Packt, 
we take the protection of our copyright and licenses very seriously. If you come across any 
illegal copies of our works, in any form, on the Internet, please provide us with the location 
address or website name immediately so that we can pursue a remedy.
Please contact us at copyright@packtpub.com with a link to the suspected pirated material.
We appreciate your help in protecting our authors, and our ability to bring you valuable content.
Questions
You can contact us at questions@packtpub.com if you are having a problem with any 
aspect of the book, and we will do our best to address it.

1
Working with 
Databases
In this chapter, we will cover:
f
Connecting to a database
f
Getting data from a database
f
Getting data from a database by providing parameters
f
Getting data from a database by running a query built at runtime
f
Inserting or updating rows in a table
f
Inserting new rows when a simple primary key has to be generated
f
Inserting new rows when the primary key has to be generated based on stored values
f
Deleting data from a table
f
Creating or altering a table from PDI (design time)
f
Creating or altering a table from PDI (runtime)
f
Inserting, deleting, or updating a table depending on a field
f
Changing the database connection at runtime
f
Loading a parent-child table
Introduction
Databases are broadly used by organizations to store and administer transactional data 
such as customer service history, bank transactions, purchases and sales, and so on. They 
also constitute the storage method for data warehouses, the repositories used in Business 
Intelligence solutions.







Working with Databases
12
4. Click on Preview. This will bring a sample list of rows so you can confirm that the data 
is as expected.
5. Press OK to close the Table Input configuration window, and you'll be ready to use the 
data for further manipulation.
How it works...
The Table Input step you used in the recipe is the main PDI step to get data from a database. 
When you run or preview the transformation, Kettle executes the SQL and pushes the rows 
of data coming from the database into the output stream of the step. Each column of the 
SQL statement leads to a PDI field and each row generated by the execution of the statement 
becomes a row in the PDI dataset.
Once you get the data from the database, it will be available for any kind of manipulation 
inside the transformation.
There's more...
In order to save time, or in case you are not sure of the name of the tables or columns in the 
database, instead of typing the SQL statement press the Get SQL select statement... button. 
This will bring the Database Explorer window. This window allows you to explore the selected 
database. By expanding the database tree and selecting the table that interests you, you will 
be able to explore that table through the different options available under the Actions menu 
as shown below:
Double-clicking the name of the table will generate a SELECT statement to query that table. 
You will have the chance to include all the field names in the statement, or simply generate a 
SELECT * statement. After bringing the SQL to the Table Input configuration window, you will 
be able to modify it according to your needs.








Working with Databases
20
4. Fill the grids as shown:
5. Save and run the transformation.
6. Explore the employees table. You will see that one employee was updated, two were 
inserted, and one remained untouched because the file had the same data as the 
database for that employee: 
+------+---------------+-------+-----+-------+--------------+ 
| ENUM | NAME          | EXT   | OFF | REPTO | JOBTITLE     | 
+------+---------------+-------+-----+-------+--------------+ 
| 1188 | Julie Firrelli| x2174 | 2   |  1143 |Sales Manager | 
| 1619 | Tom King      | x103  | 6   |  1088 |Sales Rep     | 
| 1810 | Anna Lundberg | x910  | 2   |  1143 |Sales Rep     | 
| 1811 | Chris Schulz  | x951  | 2   |  1143 |Sales Rep     | 
+------+---------------+-------+-----+-------+--------------+ 
4 rows in set (0.00 sec)
How it works...
The Insert/Update step, as its name implies, serves for both inserting or updating rows. For 
each row in your stream, Kettle looks for a row in the table that matches the condition you put 
in the upper grid, the grid labeled The key(s) to look up the value(s):. Take for example the 
last row in your input file:
1811, Schulz, Chris,x951,2,1143,Sales Rep
When this row comes to the Insert/Update step, Kettle looks for a row where 
EMPLOYEENUMBER equals 1811. It doesn't find one. Consequently, it inserts a row following 
the directions you put in the lower grid. For this sample row, the equivalent INSERT statement 
would be:
INSERT INTO employees (EMPLOYEENUMBER, LASTNAME, FIRSTNAME,
            EXTENSION, OFFICECODE, REPORTSTO, JOBTITLE)
       VALUES (1811, 'Schulz', 'Chris',
              'x951', 2, 1143, 'Sales Rep')




Working with Databases
24
| 4          | 43 Rue Jouffroy D'abbans | Paris         | 
| 5          | 4-1 Kioicho              | Tokyo         | 
| 6          | 5-11 Wentworth Avenue    | Sydney        | 
| 7          | 25 Old Broad Street      | London        | 
+------------+--------------------------+---------------+ 
7 rows in set (0.00 sec)
How to do it...
1. Create a transformation and create a connection to the sampledata database.
2. Use a Text file input step to read the offices.txt file with data about the new 
offices.
3. From the Data Warehouse category drag and drop a Combination lookup/update
step, and create a hop from the previous step towards this one.
4. Double-click the step, select the connection to the sampledata database, and type 
offices as the Target table.
5. Fill the Key fields grid as shown:
6. In the Technical key field type OFFICECODE. For the Creation of technical key fields 
leave the default values. Close the window.
7. 
From the Output category of steps, add an Update step.
8. Double-click the step, select the connection to the sampledata database, and type 
offices as the Target table.
9. In the first grid add rows with the text OFFICECODE both under Table field and under 
Stream field1. As Comparator choose =. This way, you will update the rows where 
OFFICECODE is equal to the office code in your stream.
10. In the lower grid add a row and type PHONE both under Table field and Stream field. 
Add a second row and type POSTALCODE in both columns.
11. Close the window.
12. It's time to save the transformation and run it to see what happens.
13. As you might guess, three new offices have been added, with primary keys 8, 9, and 
10. Look at the results:
SELECT OFFICECODE, ADDRESSLINE1, CITY
FROM   offices 
ORDER BY cast(officecode as unsigned);

Chapter 1
25
+------------+--------------------------+---------------+ 
| OFFICECODE | ADDRESSLINE1             | CITY          | 
+------------+--------------------------+---------------+ 
| 1          | 100 Market Street        | San Francisco | 
| 2          | 1550 Court Place         | Boston        | 
| 3          | 523 East 53rd Street     | NYC           | 
| 4          | 43 Rue Jouffroy D'abbans | Paris         | 
| 5          | 4-1 Kioicho              | Tokyo         | 
| 6          | 5-11 Wentworth Avenue    | Sydney        | 
| 7          | 25 Old Broad Street      | London        | 
| 8          | Avenida Paulista 1330    | Sao Paulo     | 
| 9          | Rua Boa Vista, 51        | Sao Paulo     | 
| 10         | Cabildo 2127             | Buenos Aires  | 
+------------+--------------------------+---------------+ 
10 rows in set (0.00 sec)
How it works...
In many situations, before inserting data into a table you have to generate the primary key. If 
the primary key is a simple sequence or the maximum primary key plus one, you can generate 
it by using a Combination lookup/update step.
In the recipe, for each row in your file, with the Combination lookup/update step, you look for 
a record in the offices table with the same values for address, city, and country.
Because the offices are new, (there aren't offices in the table with the same combination of 
address, city, and country values) the lookup fails. As a consequence, the step generates 
a key value as the maximum OFFICECODE in the table, plus 1. Then, it inserts a row with the 
generated primary key and the fields you typed in the grid.
Finally, the step adds to the stream the generated primary key value.
As a last task, we used that key to update the other fields coming into the file: POSTALCODE 
and PHONE.
There's more...
The Combination lookup/update step is within the Data Warehouse category, because is 
mainly used for loading junk dimension tables. But as you could see, it can also be used in 
the particular situation where you have to generate a primary key.
In the recipe you generated the PK as the maximum plus one, but as you can see in the 
setting window, a database sequence can also be used instead.



Working with Databases
28
5. Add an Add sequence step. Replace the default value valuename with delta_ 
value. For the rest of the fields in the setting window leave the default values.
6. Add a Calculator step to build the keys. You do it by filling the setting window as 
shown:
7. 
In order to insert the rows, add a Table output step, double-click it, and select the 
connection to the book's database.
8. As Target table type authors.
9. Check the option Specify database fields.
10. Select the Database fields tab and fill the grid as follows:
11. Save and run the transformation.
12. Explore the authors table. You should see the new authors:
SELECT * FROM authors ORDER BY id_author;
+----------+-----------+-------------+-----------+----------+ 
| lastname | firstname | nationality | birthyear | id_author| 
+----------+-----------+-------------+-----------+----------+ 
| Larsson  | Stieg     | Swedish     |      1954 | A00001   | 
| King     | Stephen   | American    |      1947 | A00002   | 
| Hiaasen  | Carl      | American    |      1953 | A00003   | 
| Handler  | Chelsea   | American    |      1975 | A00004   | 
| Ingraham | Laura     | American    |      1964 | A00005   | 
| Ramsey   | Dave      | American    |      1960 | A00006   | 
| Kiyosaki | Robert    | American    |      1947 | A00007   | 
| Rowling  | Joanne    | English     |      1965 | A00008   | 
| Riordan  | Rick      | American    |      1964 | A00009   | 
| Gilbert  | Elizabeth | unknown     |      1900 | A00010   | 
| Franzen  | Jonathan  | unknown     |      1900 | A00011   | 
| Collins  | Suzanne   | unknown     |      1900 | A00012   | 
| Blair    | Tony      | unknown     |      1900 | A00013   |


Working with Databases
30
Deleting data from a table
Sometimes you might have to delete data from a table. If the operation to do is simple, for 
example:
DELETE FROM LOG_TABLE WHERE VALID='N'
Or
DELETE FROM TMP_TABLE
You could simply execute it by using an SQL job entry or an Execute SQL script step. If you 
face the second of the above situations, you can even use a Truncate table job entry.
For more complex situations you should use the Delete step. Let's suppose the following 
situation: You have a database with outdoor products. Each product belongs to a category: 
tools, tents, sleeping bags, and so on. Now you want to delete all the products for a given list 
of categories, where the price is less than or equal to $50.
Getting ready
In order to follow the recipe, you should download the material for this chapter: a script for 
creating and loading the database, and an Excel file with the list of categories involved.
After creating the outdoor database and loading data by running the script provided, and 
before following the recipe you can explore the database. In particular execute the following 
statement:
SELECT   category, count(*) quantity
FROM     products p, categories c 
WHERE    p.id_category=c.id_category 
AND      price<=50 
GROUP BY p.id_category; 
+---------------+----------+ 
| category      | quantity | 
+---------------+----------+ 
| kitchen       |       19 | 
| lights        |       14 | 
| sleeping bags |        5 | 
| tents         |        4 | 
| tools         |        8 | 
+---------------+----------+ 
5 rows in set (0.00 sec)
SELECT   category, count(*) quantity
FROM     products p, categories c


Working with Databases
32
7. 
Finally, add a Delete step. You will find it under the Output category of steps.
8. Double-click the Delete step, select the outdoor connection, and fill in the key grid as 
follows:
9. Save and run the transformation.
10. Explore the database. If you run the same statements that you ran before starting the 
recipe, you'll note that all products belonging to the categories in the Excel file, with 
price less than or equal to $50 have been deleted. This is what you will see:
SELECT   category, count(*) quantity
FROM     products p, categories c 
WHERE    p.id_category=c.id_category 
AND      price<=50 
GROUP BY p.id_category;
+---------------+----------+ 
| category      | quantity | 
+---------------+----------+ 
| kitchen       |       19 | 
| lights        |       14 | 
| sleeping bags |        5 | 
+---------------+----------+ 
3 rows in set (0.00 sec)
SELECT   category, count(*) quantity
FROM     products p, categories c 
WHERE    p.id_category=c.id_category 
AND      price>50 
GROUP BY p.id_category;
+---------------+----------+ 
| category      | quantity | 
+---------------+----------+ 
| kitchen       |        5 | 
| lights        |        1 | 
| sleeping bags |        1 | 
| tents         |        8 | 
| tools         |        2 | 
+---------------+----------+ 
5 rows in set (0.00 sec)

Chapter 1
33
How it works...
The Delete step allows you to delete rows in a table in a database based on certain 
conditions. In this case, you intended to delete rows from the table products where the price 
was less than or equal to 50, and the category was in a list of categories, so the Delete step is 
the right choice. This is how it works.
PDI builds a prepared statement for the DELETE operation. Then, for each row in your stream, 
PDI binds the values of the row to the variables in the prepared statement.
Let's see it by example. In the transformation you built a stream where each row had a single 
category and the value for the price.
If you run the transformation with log level Detailed and look at the log, you will see the 
statement that is executed:
DELETE FROM products 
WHERE price < ?
AND id_category = ? 
The WHERE clause is built based on the conditions you entered in the Delete configuration 
window. For every row, the values of the fields you typed in the grid—max_price and id_ 
category—are bound to the question marks in the prepared statement.
Note that the conditions in the Delete step are based on fields in the same table. In this 
case, as you were provided with category descriptions and the products table does not have 
the descriptions but the ID for the categories, you had to use an extra step to get that ID: a 
Database lookup.
Suppose that the first row in the Excel file had the value tents. As the ID for the category 
tents is 4, the execution of the prepared statement with the values in this row has the same 
effect as the execution of the following SQL statement:
DELETE FROM products 
WHERE price < 50 
AND id_category = 4
See also
f
Looking for a value in a database table (Chapter 5, Looking for Data). Refer to this 
recipe if you need to understand how the Database lookup step works.







Working with Databases
40
+----------+----------------------------------------+-------+ 
| id_title | title                                  | price | 
+----------+----------------------------------------+-------+ 
| 123-400  | The Girl with the Dragon Tattoo        |    37 | 
| 123-401  | The Girl who Played with Fire          |  35.9 | 
| 123-402  | The Girl who Kicked the Hornett's Nest |    39 | 
+----------+----------------------------------------+-------+ 
3 rows in set (0.00 sec)
SELECT *
FROM   books 
WHERE  title="Mockingjay";
Empty set (0.00 sec)
How to do it...
1. Create a new transformation, and create a connection to the book's database.
2. Drop to the canvas a Text file input step and use the step to read the books_news. 
txt file. As separator, type |. Read all fields as String except the price that has to be 
read as a Number with 0.00 as the Format.
3. Do a preview to verify you have read the file properly. You should see this:
4. Use a Split Fields step to split the name field into two: firstname and lastname.
5. Use a Database lookup step to look up in the authors table for an author that 
matches the firstname and lastname fields. As Values to return from the lookup
table: add the id_author.
6. Check the option Do not pass the row if the lookup fails and close the window.
7. 
From the Output category of steps drag and drop to the canvas a Synchronize after
merge step, and create a hop from the last step toward this one. Your transformation 
looks like this:


Working with Databases
42
+----------+----------------------------------------+-------+ 
| id_title | title                                  | price | 
+----------+----------------------------------------+-------+ 
| 123-400  | The Girl with the Dragon Tattoo        | 34.98 | 
| 123-401  | The Girl who Played with Fire          | 35.99 | 
| 123-402  | The Girl who Kicked the Hornett's Nest | 37.99 | 
+----------+----------------------------------------+-------+ 
3 rows in set (0.00 sec)
SELECT *
FROM   books 
WHERE  title="Mockingjay"; 
+----------+------------+-----------+-------+-------+ 
| id_title | title      | id_author | price | genre | 
+----------+------------+-----------+-------+-------+ 
| 523-110  | Mockingjay | A00012    | 37.99 | Teens | 
+----------+------------+-----------+-------+-------+ 
1 row in set (0.00 sec)
How it works...
The Synchronize after merge step allows you to insert, update, or delete rows in a table 
based on the value of a field in the stream. In the recipe, you used the Synchronize after
merge step both for inserting the new books (for example, Mockingjay) and for updating the 
prices for the books you already had (for example, The Girl with the Dragon Tattoo).
In order to tell PDI whether to execute an insert or an update, you used the field comment. 
Under the Advanced tab, you told PDI that it should insert the records where the comment 
was equal to NEW, and update those where the comment was In Stock.
Note that, because you didn't intend to delete rows, you left the Delete when value equal
option blank. However, you could also have configured this option in the same way you 
configured the others. An example of that could be deleting the books that will stop being 
published. If you recognize those books after the expression out of market, you could type 
that expression in the Delete when value equal option and those books would be deleted.
The inserts and updates were made based on the fields you entered in the grids under the 
General tab, which work exactly as the grids in an Insert/Update or an Update step.
There's more...
Let's see a little more about the step you used in this recipe.


Working with Databases
44
f
A different database for each branch of your business.
f
A database for your sandbox, a second database for the staging area, and a third 
database fulfilling the production server purpose.
In any of those situations, it's likely that you need access to one or the other depending on 
certain conditions, or you may even have to access all of them one after the other. Not only 
that, the number of databases may not be fixed; it may change over time (for example, when a 
new branch is opened).
Suppose you face the second scenario: Your company has several branches, and the sales 
for each branch are stored in a different database. The database structure is the same for 
all branches; the only difference is that each of them holds different data. Now you want to 
generate a file with the total sales for the current year in every branch.
Getting ready
Download the material for this recipe. You will find a sample file with database connections to 
three branches. It looks like this:
branch,host,database 
0001 (headquarters),localhost,sales2010 
0002,183.43.2.33,sales 
0003,233.22.1.97,sales
If you intend to run the transformation, modify the file so it points to real databases.
How to do it...
1. Create a transformation that reads the file with connection data and copy the rows to 
results.
2. Create a second transformation, and define the following named parameters: 
BRANCH, HOST_NAME, and DATABASE_NAME.
3. Create a database connection. Choose the proper Connection Type:, and 
fill the Settings data. Type a value for the Port Number:, the User Name:, and the 
Password. As Host Name: type ${HOST_NAME}, and as Database Name: type 
${DATABASE_NAME}.
4. Use a Table Input step for getting the total sales from the database. Use the 
connection just defined.
5. Use a Text file output step for sending the sales summary to a text file. Don't forget to 
check the option Append under the Content tab of the setting window.
6. Create a job with two Transformation job entries, linked one after the other.
7. 
Use the first entry to call the first transformation you created, and the second 
entry to call the second transformation. The job looks like this:

Chapter 1
45
8.
Double-click the second transformation entry, select the Advanced tab, and check 
the Copy previous results to parameters? and the Execute for every input row? 
checkboxes.
9.
Select the Parameters tab and fill it as shown:
10. Save both transformations. Save the job, and run it.
11. Open the text file generated. It should have one line with sales information for each 
database in the file with the list of databases.
How it works...
If you have to connect to several databases, and you don't know in advance which or how 
many databases you will have to connect to, you can't rely on a connection with fixed values, 
or variables defined in a single place as for example in the kettle.properties file. In 
those situations, the best you could do is to define a connection with variables, and set the 
values for the variables at runtime.
In the recipe, you created a text file with a summary sales line for each database in a list.
The transformation that wrote the sales line used a connection with variables defined as 
named parameters. This means that whoever calls the transformation has to provide the 
proper values.
The main job loops on the list of database connections. For each row in that list, it calls the 
transformation copying the values from the file to the parameters in the transformation. In 
other words, each time the transformation runs, the named parameters are instantiated with 
the values coming from the file.



Working with Databases
48
11. Repeat step 10 for the third Transformation entry, but this time type 
.*Manager.* as the value for the LEVEL parameter.
12. Repeat step 10 for the fourth Transformation entry, but this time type Sales
Rep.* as the value for the LEVEL parameter.
13. Save and run the job. The table should have all employees loaded, as you 
can see below:
SELECT
   EMPLOYEENUMBER N
 , LASTNAME
 , REPORTSTO
 , JOBTITLE
FROM employees; 
+------+-----------+-----------+----------------------------+ 
| N    | LASTNAME  | REPORTSTO | JOBTITLE                   | 
+------+-----------+-----------+----------------------------+ 
| 1002 | Murphy    |      NULL | President                  | 
| 1056 | Patterson |      1002 | VP Sales                   | 
| 1076 | Firrelli  |      1002 | VP Marketing               | 
| 1088 | Patterson |      1056 | Sales Manager (JAPAN, APAC)| 
| 1102 | Bondur    |      1056 | Sale Manager (EMEA)        | 
| 1143 | Bow       |      1056 | Sales Manager (NA)         | 
| 1165 | Jennings  |      1143 | Sales Rep                  | 
| 1166 | Thompson  |      1143 | Sales Rep                  | 
| 1188 | Firrelli  |      1143 | Sales Rep                  |  
| ...  | ...       |      ...  | ...                        | 
+------+-----------+-----------+----------------------------+ 
23 rows in set (0.00 sec)
How it works...
If you have to load a table with parent-child relationships, loading all at once is not always 
feasible. Look at the sampledata database. There is no physical foreign key from the 
REPORTSTO column to the EMPLOYEENUMBER column, but if the foreign key had existed, 
loading all records at once would fail because of the foreign key constraint. Not only that; in 
this case loading all at once would be impossible because in the file you missed the ID of the 
parent employee needed for the REPORTSTO column.

Chapter 1
49
So, in this recipe there was one possible solution for loading the table. We loaded all 
employees, one role at a time, beginning by the president and followed by the roles below in 
the hierarchy. The transformation that loaded the other roles simply read the file, kept only 
the employees with the role being loaded, looked for the ID of the parent employee in the 
hierarchy, and inserted the records. For the roles you could have used fixed values but you 
used regular expressions instead. In doing so, you avoided calling the transformation once for 
each different role. For example, for loading the vice-presidents you called the transformation 
once with the regular expression VP.* which matched both VP Sales and VP Marketing.
See also
f
Inserting or updating rows in a table. If you are not confident with inserting data into 
a table see this recipe.


2
Reading and Writing 
Files
In this chapter, we will cover:
f
Reading a simple file
f
Reading several files at the same time
f
Reading unstructured files
f
Reading files having one field by row
f
Reading files having some fields occupying two or more rows
f
Writing a simple file
f
Writing an unstructured file
f
Providing the name of a file (for reading or writing) dynamically
f
Using the name of a file (or part of it) as a field
f
Reading an Excel file
f
Getting the value of specific cells in an Excel file
f
Writing an Excel file with several sheets
f
Writing an Excel file with a dynamic number of sheets
Introduction
Files are the most primitive, but also the most used format to store and interchange data. PDI 
has the ability to read data from all kind of files and different formats. It also allows you to 
write back to files in different formats as well.



Reading and Writing Files
54
There's more...
To work with these kinds of delimited text files, you could choose the CSV file input step. This 
step has a less powerful configuration, but it provides better performance.
If you explore the tabs of the Text file input setting window, you will see that there are more 
options to set, but the ones just explained are by far the most used. But there are a couple of 
additional features that may interest you:
Alternative notation for a separator
Instead of typing the separator for the fields, you can use the following notation:
$[H1, H2, ...]
Where the values H1, H2, ... are the hexadecimal codes for the separators. For example, 
for specifying a tilde (~) as the separator, instead of typing it, you could type $[7E]. However, 
this notation makes more sense when your separators are non printable characters.
For the enclosure string the hexadecimal notation is also allowed.
About file format and encoding
If you are trying to read a file without success, and you have already checked the most common 
settings, that is, the name of the file, the header, the separator and the fields, you should take a 
look at and try to fix the other available settings. Among those, you have Format and Encoding.
Format allows you to specify the format of your file(s): DOS (default value) or UNIX. If your file 
has a Unix format, you should change this setting. If you don't know the format, but you cannot 
guarantee that the format will be DOS, you can choose the mixed option.
Encoding allows you to specify the character encoding to use. If you leave it blank, Kettle will 
use the default encoding on your system. Alternatively, if you know the encoding and it is 
different from the default, you should select the proper option from the drop-down list.
About data types and formats
When you read a file and tell Kettle which fields to get from that file, you have to provide at 
least a name and a data type for those fields. In order to tell Kettle how to read and interpret 
the data, you have more options. Most of them are self-explanatory, but the format, length, 
and precision deserve an explanation:
If you are reading a number, and the numbers in your file have separators, dollar signs, and 
so on, you should specify a format to tell Kettle how to interpret that number. The format is a 
combination of patterns and symbols as explained in the Sun Java API documentation at the 
following URL:
http://java.sun.com/javase/6/docs/api/java/text/DecimalFormat.html





Chapter 2
59
f
What allows us to distinguish the first line for each roller coaster from the rest is that 
it is written in uppercase letters.
f
The first line below the name of the roller coaster is the name of the amusement park 
where it is located.
f
Most of the lines have a property of the roller coaster in the format of 
code:description, as for example Drop: 60 feet.
f
Above the properties, there is a line with the text Roller Coaster Stats, which 
doesn't add any information. It should be discarded.
f
There are lines with additional information about the roller coaster. There is nothing 
that distinguishes these lines. They simply do not fall into any of the other kinds of lines 
(lines with the name of the park, lines with properties of the roller coaster, and so on).
Once you understand the content of your file, you are ready to read it, and parse it.
How to do it...
Carry out the following steps:
1. Create a transformation and drag a Text file input step.
2. Use that step to read the file named rollercoasters_II.txt. Under the Content
tab, uncheck the Header option and under the Separator tab, type |. Under the 
Fields tab, enter a single field named text of type String. As the character | is not 
present in any part of the file, you are sure that the whole line will be read as a single 
field.
3. From the Scripting category of steps, add a Modified Java Script Value step, double-
click it, and under the main tab window type the following snippet of code: 
var attraction; 
trans_Status=CONTINUE_TRANSFORMATION;
if (getProcessCount('r') == 1) attraction = ''; 
if (text == upper(removeDigits(text))) {
   attraction = text;
   trans_Status=SKIP_TRANSFORMATION;
   } 
else if (text == 'Roller Coaster Stats')
   trans_Status=SKIP_TRANSFORMATION;
4. Click on the Get variables button to populate the grid with the variable attraction.
5. From the Transform category, add an Add value fields changing sequence step.
6. Double-click the step. As Result field type line_nr. In the first row of the grid type 
attraction.



Reading and Writing Files
62
How it works...
When you have an unstructured file, the first thing to do is understand its content, in order to 
be able to parse the file properly.
If the entities described in the file (roller coasters in this example) are spanned over several 
lines, the very first task is to identify the rows that make up a single entity. The usual method 
is to do it with a JavaScript step. In this example, with the JavaScript code, you used the fact 
that the first line of each roller coaster was written with uppercase letter, to create and add a 
field named attraction. In the same code, you removed unwanted lines.
In this example, as you needed to know which row was the first in each group, you added an 
Add value fields changing sequence step.
After doing this, which as noted is only necessary for a particular kind of file, you have to 
parse the lines. If the lines do not follow the same pattern, you have to split your stream in 
as many streams as kind of rows you have. In this example, you split the main stream into 
three, as follows:
1. One for parsing the lines with properties, for example Drop: 60 feet.
2. One for setting the name of the amusement park where the roller coaster was.
3. One for keeping the additional information.
In each stream, you proceeded differently according to the format of the line.
The most useful step for parsing individual unstructured fields is the Regexp Evaluation
step. It both validates if a field follows a given pattern (provided as a regular expression) 
and optionally, it captures groups. In this case, you used that step to capture a code and a 
description. In the preceding example (Drop: 60 feet), the Regexp Evaluation step allowed 
you to build two fields: code with value Drop, and desc with value 60 feet.
Once you parsed the line with the Regexp Evaluation or the step of your choice, you can 
continue transforming or modifying the fields according to your needs and the characteristics 
of your particular file.
In the same way, depending on the purpose of your transformation, you can leave the streams 
separated or join them back together as you did in the recipe.
There's more...
There are some common kinds of files that can be parsed in the way you parsed the roller 
coasters file:
Master/detail files
Suppose that you have a file of invoices such as the following:

Chapter 2
63
INV.0001-0045;02/28/2010;$323.99 
CSD-031;2;$34.00 
CSA-110;1;$100.99 
LSK-092;1;$189.00 
INV.0001-0046;02/28/2010;$53.99 
DSD-031;2;$13.00 
CXA-110;1;$40.99 
INV.0001-0047;02/28/2010;$1149.33 
...
The lines beginning with INV. are the invoice headers; the lines following the headers are the 
details of those invoices.
Files like these are not uncommon. If you have a file like this with records that represent 
headers followed by records that represent details about those headers, and the header and 
detail records have different structures, you could parse it as explained in the recipe.
Read the file, do whatever is necessary to find out if a row is a header or a detail, and split the 
stream in two. After that, parse header rows and detail rows accordingly.
Log files
Log files are among the most common kinds of unstructured files. Look at the following 
sample lines belonging to a Pentaho Server log:
... 
2010-09-30 13:01:30,437 DEBUG [org.pentaho.platform.engine.
   services.solution.SolutionEngine] fd386728-ccab-11df-9... 
2010-09-30 13:01:30,484 INFO  [org.pentaho.platform.reposit
   ory.solution.SolutionRepositoryBase] Solution Reposito... 
2010-09-30 13:01:30,484 INFO  [org.pentaho.platform.reposit
   ory.solution.SolutionRepositoryBase] Solution Reposit... 
2010-09-30 13:01:30,515 INFO  [org.pentaho.platform.reposit
   ory.solution.SolutionRepositoryBase] Could not find d... 
2010-09-30 13:01:30,531 ERROR [org.pentaho.platform.engine.
   services.solution.SolutionEngine] fd386728-ccab-11df-... 
2010-09-30 13:01:42,515 WARN  [org.hibernate.cache.EhCacheP
   rovider] Could not find configuration [file]; using d... 
...
In this case, all lines begin with a timestamp, followed by the level of log (DEBUG, INFO, and 
so on), and then the details of the log.
Despite being unstructured, the lines in a log file—the one shown above—have some text that 
let you know what kind of data is in those lines. Using that knowledge, you can parse different 
lines as explained in the recipe.


Chapter 2
65
3.
Double-click the step. As The field to flatten type or select text. Fill the grid with 
three rows with values title, publishing_date, and price.
4.
Do a preview on the last step. You'll see the following:
You already have the fields as columns! Now, you can go a little further and do some 
cleansing, as follows:
1.
From the Scripting category add a Regexp Evaluation step.
2.
Configure the step as follows: As Field to evaluate type or select publishing_date. 
Check the option Create fields for capture groups. As Regular expression: type 
(Published|Expected):(.+).
3.
In the Capture Group Fields grid, add two rows. In the first row create a new String 
field named status. In the second, create a Date field named pub_date with 
Format MMM yyy. In both rows, under the Trim column, select both.
4.
From the Transform category, add a Replace in string step. In the grid, add a row 
with the value price under the column In stream field, and Our price: under the 
column Search.
5.
Finally, use a Select values step to change the metadata of the price field: Change it 
to Number. As Format, type £#.00
6.
Do a preview and you'll see the following:






Chapter 2
71
Changing headers
If you want to change the name of a header, you could insert a Select values step from the 
Transform category just before the Text file output step. Under the Select & Alter tab, select 
the fields you want to rename and give them a better description. For example, you could 
select the desc_product fieldname and rename the field as Product.
In order to send all the other fields toward the Text file output step, you also have to check the 
Include unspecified fields, ordered by name option.
Giving the output fields a format
When you write a file and tell Kettle which fields to write to that file, you have the option 
of specifying the format to apply to those fields. That is particularly useful when you have 
numeric or date fields.
In both cases, you may specify a format using a mask of patterns and symbols.
In the case of numeric fields, you can find more information about formats at the following URL:
http://java.sun.com/javase/6/docs/api/java/text/DecimalFormat.html 
In the case of date fields, you will find a complete reference at the following URL:
http://java.sun.com/javase/6/docs/api/java/text/SimpleDateFormat.html
Writing an unstructured file
A standard file generated with Kettle is a file with several columns, which may vary according 
to how you configured the Fields tab of the Output step and one row for each row in your 
dataset, all with the same structure. If you want the file to have a header, the header is 
automatically created with the names of the fields. What if you want to generate a file 
somehow different from that?
Suppose that you have a file with a list of topics for a writing examination. When a student has 
to take the examination, you take that list of topics and generate a sheet like the following:
Student name: Mary Williams 
-------------------------------------------------------------
Choose one of the following topics and write a paragraph about it 
(write at least 300 words)
1. Should animals be used for medical research?
2. What do you think about the amount of violence on TV?
3. What does your country mean to you?
4. What would happen if there were no televisions?
5. What would you do if you found a magic wand?


Chapter 2
73
8. Double-click on the first Append streams step, as Head hop, select the name of the 
UDJE step, as Tail hop, select the name of the Data Grid step.
9. Double-click on the second Append streams step, as Head hop, select the name of 
the previous Append streams step, as Tail hop select the name of the Select values
step.
10. After this last Append streams step, add a Text file output step, enter a path and 
name for the output file, and as fields, type the name of the only field that exists: the 
field named text.
11. Run the transformation. Don't forget to provide a student name as the first command 
line argument. See the generated file; it should look exactly as the one shown in the 
introduction.
How it works...
When you generate a file with any of the Kettle output steps, the rows have to be 
homogeneous, that is, all of them have to have the same format, the same number of 
columns, the same type of data, and so on. This recipe showed you one of the ways for 
creating a file with rows that differ in structure. In this case, you had a main stream with two 
columns: a number and a writing topic. However, you also had several lines that made up 
a header for those topics. What you did was to build separate streams; in each stream you 
concatenated the different fields that you wanted to send to the file, creating a single field 
named text. Then you joined the streams by using Append streams steps, and sent the final 
dataset to a file with a single column.





Reading and Writing Files
78
Getting ready
In order to run this exercise, you need a directory (campingProducts) with text files named 
kitchen.txt, lights.txt, sleeping_bags.txt, tents.txt, and tools.txt. Each 
file contains descriptions of the products and their price separated with a |. For example:
Swedish Firesteel - Army Model|$19.97
Mountain House #10 Can Freeze-Dried Food|$53.50 
Coleman 70-Quart Xtreme Cooler (Blue)|$59.99 
Kelsyus Floating Cooler|$26.99 
Lodge LCC3 Logic Pre-Seasoned Combo Cooker|$41.99 
Guyot Designs SplashGuard-Universal|$7.96
How to do it...
Carry out the following steps:
1. Create a new transformation
2. Drop a Text file input step into the work area and use it to read the files: Under the 
File tab, type or browse to the campingProducts directory in the File or directory
textbox, and use .*\.txt as Regular Expression. Click on the Add button.
3. Under the Content tab, type | as the Separator and complete the Fields tab as 
follows:
4. Under the Additional output fields tab, type filename in the field Short filename
field.
5. Previewing this step, you can see that there is a new field named filename with the 
name of the file (for example: kitchen.txt).
6. Now, you must split the filename text to get the category. Add a Split Fields from the 
Transform category, double-click on it and fill the setting windows, as shown in the 
following screenshot:

Chapter 2
79
7. 
Previewing the last step of the transformation, you will see a dataset with the 
camping products, their price, and also a column named category with the proper 
product category.
How it works...
This recipe showed you the way to convert the names of the files into a new field named 
category. The source directory you entered in the Text file input step contains several files 
whose names are the categories of the products. Under the Additional output fields tab, you 
incorporated the Short filename as a field (for example tents.txt); you could also have 
included the extension, size, or full path among other fields.
The next step in the transformation, a Split Fields step uses a period (.) as the Delimiter
value to use from the field only the first part, which is the category (tents in the example). 
It eliminates the second part, which is the extension of the filename (txt). If you don't want 
to discard the extension, you must add another field in the grid (for example, a field named 
fileExtension). Note that for this field, you set the type, but you can also specify a format, 
length, and so on.
Reading an Excel file
Kettle provides the Excel input step, in order to read data from Excel files. In this recipe, you 
will use this step to read an Excel file regarding museums in Italy. The file has a sheet with one 
column for the name of the museum and other for the city where it is located. The data starts 
in the C3 cell (as shown in the screenshot in the next section).





Reading and Writing Files
84
So, in order to find what you were looking for, you read the first 10 columns by using generic 
names a, b, c, and so on. By normalizing the cells, you put the cells row by row. This way, each 
value remained in the row just beneath its label. For example, if the cell with the value YEAR
remained in the tenth row, the cell with value 2010 was in row 11.
You can confirm this by doing a preview on the Row Normalizer step.
For each row, the Analytic Query step went forward to get the value of the row below and 
brought it in as a new field in the current row. This way, the labels and the values were again 
next to each other, as shown in the following screenshot:
Note that the result of combining these two steps was to remove the leading columns both to 
the right and to the left of our table. Now, you could just remove the useless rows by keeping 
only those with labels equal to Year, ProductLine, or Origin, or do what was done in the 
recipe: Denormalize the data to get just one row. This row is ready to be used for setting the 
variables Year, ProductLine, and Origin just by adding a Set Variables step at the end of 
the stream.
There's more...
As you don't know which columns will hold which kind of data, the advice is to read all as 
string. This way, you avoid unexpected errors. However, after getting the data, you can change 
the metadata accordingly by using a Select values step.
Also note that you only read the first 10 columns. If you cannot be sure that the values are 
going to be in this range of cells, feel free to increase that value.
The following are the two use cases related to the main example:

Chapter 2
85
Labels and values horizontally arranged
What if, instead of having the labels and values as in the recipe, you have them horizontally 
arranged, as shown in the following screenshot:
The recipe still works if you make a simple modification. Edit the Analytic Query step and 
change the 1 to 10. Just that. This is how it works: When you denormalize the rows, the labels 
and their values remain 10 rows apart from each other. So, instead of looking for the next row, 
the Analytic Query has to look 10 rows forward and get the values on those rows. You can see 
it in the following screenshot, which is the result of a preview on this step:
Looking for a given cell
If you just have to look for a specific cell, for example D5, the solution is quite different, but 
fortunately pretty straightforward. Firstly, you have to know the number of column and row 
where your data is. As Excel starts counting at zero, you conclude that the sample cell D5 is 
in the third column, fourth row. Then you take an Excel input step and enter the name of the 
Excel file to read. In the grid located under the Sheets tab, add a row. Under Start row and 
Start column type the number of row and column of interest, in this case, 4 and 3. Under the 
Content tab, uncheck the Header and the No empty rows options, checked by default, and in 
the Limit textbox, type 1. Under the Fields tab, add a single field to hold your value. You are 
done. Do a preview of the Excel and you will see the following:





Reading and Writing Files
90
6. In this transformation, drop a Table input step, in order to obtain the different book's 
categories. The SQL statement, should be similar to the following:
SELECT DISTINCT genre FROM Books ORDER BY genre
7. 
Add a Copy rows to result from the Job folder and create a hop from the Excel
output step towards this one.
8. Create the second transformation called Trans_BookByCategory.
9. In the Transformation settings (CTRL-T), go to the Parameters tab, and add a new 
parameter named GENRE without default.
10. Drop a Table input step into the canvas. In the SQL frame, type the following 
statement, in order to select the books depending on the GENRE parameter:
SELECT * FROM Books WHERE genre='${GENRE}'
11. In this step, check the prompt Replace variables in script?
12. Add an Excel output step.
13. In the Filename textbox under the File tab, type the destination path and file. In the 
Extension textbox, leave the default value xls.
14. Under the Content tab, be sure to check Append.
15. Also here, in the Sheet name textbox, type ${GENRE}.
16. Under the Field tab click on the Get Fields button.
17. Come back to the job; edit the job entry details for the transformation Trans_ 
BookByCategory. Go to the Advanced tab and check Copy previous result to
parameters? and Execute for every input row? checkboxes.
18. Under the Parameters tab, add a new value: type GENRE in the Parameter column, 
and genre for the Stream column name.
19. When you run the job, the Excel file created should have a different sheet for each 
category, for example:



3
Manipulating XML 
Structures
In this chapter, we will cover:
f
Reading simple XML files
f
Specifying fields by using XPath notation
f
Validating well-formed XML files
f
Validating an XML file against DTD definitions
f
Validating an XML file against an XSD schema
f
Generating a simple XML document
f
Generating complex XML structures
f
Generating an HTML page using XML and XSL transformations
Introduction
XML is a markup language designed to describe data, the opposite of HTML which was 
designed only to display data. It is a self-descriptive language because its tags are not 
predefined. XML documents are not only used to store data, but also to exchange data 
between systems.
XML is recommended by W3C. You will find the details at the following URL:
http://www.w3.org/XML/
PDI has a rich set of steps and job entries for manipulating XML structures. The recipes in this 
chapter are meant to teach you how to read, write, and validate XML using those features.






Chapter 3
99
Getting ready
This recipe is theoretical and has the purpose of helping you when it's time to enter an XPath 
notation. You will not develop a transformation here. However, for a better understanding of 
what's being explained, you can do the following:
1. Download the sample XML file.
2. Read it by using the Get data from XML step.
3. Try introducing the different XPath notations in the Fields grid, as they are explained.
4. To check if you are entering the correct notations, do a preview and check it for yourself.
How to do it...
Carry out the following steps:
1. Pick the node that will be the base for specifying your fields. In the sample data, the 
node will be /w_cond/data.
For each desired element, repeat steps 2 and 3:
2. Look at the XML structure to see if it is a node or an attribute. In the sample data, 
the temperature scale and the units for the windspeed are attributes. The rest of the 
fields are nodes.
3. Identify the absolute location of the element, that is, the complete path from the root 
element to the desired element. For example, for city the absolute location would 
be /w_cond/data/request/query. If the element is an attribute, prepend @ to 
the name.
4. Identify the location relative to the base node identified in step 1. For example, for the 
city the relative location would be request/query. If the element is an attribute, 
prepend @ to the name.
The following table shows the absolute and relative locations for the sample data:
data
absolute location
relative location
city
/w_cond/data/request/query
request/query
observation 
time
/w_cond/data/current_cond/ 
observation_time
current_cond/ 
observation_time
Temperature 
(degrees)
/w_cond/data/current_cond/temp
current_cond/temp
Temperature 
(scale)
/w_cond/data/current_cond/ 
temp/@scale
current_cond/temp/@scale
Weather 
description
/w_cond/data/current_cond/ 
weatherDesc
current_cond/weatherDesc

Manipulating XML Structures
100
The preceding locations are the XPath notations for the selected data in the sample XML 
structure.
How it works...
XPath is a set of rules used for getting information from an XML document. XPath treats an 
XML structure as a tree of nodes. The tree can be compared to a directory tree in your system. 
The way you specify relative or absolute locations in that tree is much the same in both cases.
In Kettle you use XPath both for getting data from XML structures and for generating XML 
structures. 
The reason for specifying both absolute and relative locations in the recipe is that in Kettle 
you need one or the other depending of what you are doing. For example when you read an 
XML structure you have to select a node, and define the fields as locations relative to that 
node. When you join two XML structures, the XPath statement that you need to specify is an 
absolute location.
There's more...
The XPath notations in the recipe are the simplest XPath notations you will find, but XPath 
allows you to write really complex expressions. The next sections provide you with more 
detail about specifying nodes with XPath notation. For more information on XPath, you can 
follow this link: http://www.w3schools.com/XPath/ or see the W3C recommendation: 
http://www.w3.org/TR/xpath.
Getting data from a different path
When you read an XML structure, you don't specify absolute paths, but paths relative to a 
node selected as the current node. In the sample recipe, the current node was /w_cond/ 
data. If the fields are inside that node, you get the relative location just by cutting the root 
part from the absolute location. For example, the absolute path for the weather description is 
/w_cond/data/current_cond/weatherDesc.
Then, for getting the location relative to the current node, just cut /w_cond/data/ and you 
get current_cond/weatherDesc.
If the data you need is not in the tree below the selected node, you have to use the ..
notation, which is used to specify the parent of the current node. For example, suppose that 
the current node is /w_cond/data/current_cond and you want to know the name of the 
city to which this condition belongs. The city element is not inside the selected node. To 
reach it, you have to type ../request/city.

Chapter 3
101
Getting data selectively
If you are reading a structure where there might be more than one element with the same 
XPath notation, you have the option to select just the one that interests you. Look for example 
at the temperature elements in the sample structure:
      <temp scale="C">19</temp>
      <temp scale="F">66</temp>
These lines belong to the Celsius and the Fahrenheit scales respectively. Both lines share the 
same XPath notation. Suppose that you are interested in the Celsius line. To get that element, 
you have to use a predicate. A predicate is an expression used to find a specific node or a 
node that contains a specific value. In this case, you need a predicate to find a node that 
contains an attribute named scale with value C. The notation for getting that node is temp[@ 
scale='C']. In general, for getting a node that contains a specific value, the notation is 
XPath[condition], that is, the XPath expression followed by the condition within brackets.
Now, let's make it a bit more complicated. Suppose that you don't even know which scale to 
return, because the scale is part of the XML structure, as shown in the following example:
<request>
   <type>City</type>
   <query>Buenos Aires, Argentina</query>
   <preferredScale>C</preferredScale>
</request>
Each city will have its own preferred scale and you should return the temperature in Celsius or 
Fahrenheit depending on the city's preferred scale.
What you need is a dynamic predicate. The way to implement this is through the use of a non 
standard extension named Tokens. Let's explain it based on our example:
1. The first thing you have to do is to add the field in which the token is based: 
preferredScale. So, add a field named preferredScale and for XPath, type: 
../request/preferred_scale.
2. Then, add a new field for the temperature in the desired scale. For Name, type 
temperature and as XPath type: 
../temp[@scale =@_preferredScale-]/text()
3. Finally, under the Content tab, check Use tokens. If you don't, this will not work!

Manipulating XML Structures
102
Assuming that you defined the fields: city, preferredScale, temperature_C, and 
temperature_F for the temperature in Celsius and Fahrenheit degrees respectively, and 
temperature, if you do a preview you should see something like the following:
In general, the expression for a token is @_<tokenized_field>-, where <tokenized_ 
field> is the field in which the token is based and has to be previously defined.
PDI will build a dynamic predicate by replacing each <tokenized_field> by its current 
value and then returning the proper node value.
Getting more than one node when the nodes share their XPath 
notation
Look at the weather nodes in the sample XML structure:
    <weather>
      <date>2010-10-24</date>
      <tempMaxC>23</tempMaxC>
      ...
    </weather>
    <weather>
      <date>2010-10-25</date>
      <tempMaxC>23</tempMaxC>
      ...
    </weather>
    <weather>
      <date>2010-10-26</date>
      <tempMaxC>24</tempMaxC>
For each node /w_cond/data (the current node in the example), there are three different 
weather nodes.
Suppose that you want to read all of them. In this case, you have to use a predicate just as 
explained in the previous section. In this case the predicate is not used to find a node that 
contains a specific value, but to find the nodes by position: You need the first, second, and 
third weather nodes. The notation that you have to use is XPath[position], that is, the 
XPath expression followed by the position of the desired element within brackets. 

Chapter 3
103
In the example, the notation for the first, second, and third weather nodes would be 
weather[1], weather[2], and weather[3] respectively. For getting nodes inside those, 
the notation is as usual. For example, for getting the date of the second node, you should 
write weather[2]/date.
Note that if you are reading an XML structure, each element that you get by using this 
notation may be used as a new column in the dataset. If, instead of that you want to generate 
a different row for each weather node, then you should take another approach: Instead of 
using this notation, simply change the current node (Loop XPath element) from /w_cond/ 
data to /w_cond/data/weather.
Saving time when specifying XPath
In most of the Kettle steps where you have to provide an XPath, you must type it manually. That's 
why you need to understand this notation. However, when you read an XML structure by using 
the Get Data from XML step, you have the option to use the Get Fields button to get the nodes 
and attributes automatically. Note that Kettle will get only the trivial elements, that is:
f
It will get only the fields that are below the node you typed as Loop XPath.
f
It will bring all the elements. For getting elements selectively, as in the temperature 
example above, you'll have to modify the grid manually.
f
If there is more than one element with the same XPath notation, as in the weather 
example above, it will bring only the first element.
To summarize, if you fill the grid with the Get Fields button, you will save time but on most 
occasions, you will still have to adjust the data in the grid manually.
Validating well-formed XML files
PDI offers different options for validating XML documents, including the validation of a well-
formed document. The structure of an XML document is formed by tags that begin with 
the character < and end with the character >. In an XML document, you can find start-tags: 
<exampletag>, end-tags: </exampletag>, or empty-element tags: <exampletag/>, 
and these tags can be nested. An XML document is called well-formed when it follows the 
following set of rules:
f
They must contain at least one element
f
They must contain a unique root element – this means a single opening and closing 
tag for the whole document
f
The tags are case sensitive
f
All of the tags must be nested properly, without overlapping


Chapter 3
105
10. Under the Content tab, type /museums/museum in the Loop XPath textbox.
11. Finally, the grid under the Fields tab must be completed manually, as shown in the 
following screenshot:
12. When you run the job, you will obtain a museums dataset with data coming only from 
the well-formed XML files. You can take a look at the Logging window to verify this. 
You will see something like the following: 
2010/11/01 11:56:43 - Check if XML file is well formed - ERROR 
(version 4.1.0, build 13820 from 2010-08-25 07.22.27 by tomcat) :
Error while checking file [file:///C:/museums1.xml]. 
Exception : 
2010/11/01 11:56:43 - Check if XML file is well formed - ERROR 
(version 4.1.0, build 13820 from 2010-08-25 07.22.27 by tomcat) : 
org.xml.sax.SAXParseException:
Element type "museum" must be followed by either attribute 
specifications, ">" or "/>".
13. Further, you can see in the Logging window that only two files out of three were read:
2010/11/01 11:56:43 - Get XMLs well-formed.0 - Finished processing 
(I=0, O=0, R=2, W=2, U=0, E=0)
How it works...
You can use the Check if XML is well-formed job entry to check if one or more XML files are 
well-formed.
In the recipe, the job validates the XML files from the source directory and creates a list with 
only the valid XML files.
As you saw in the logging window, only two files were added to the list and used later in the 
transformation. The first file (C:/museums1.xml) had an error; it was not well-formed and 
because of that it was not added to the list of files.
The Get files from result step in the transformation get the list of well-formed XML documents 
created in the job. Then, a Get data from XML step read the files for further processing. 
Note that in this case, you didn't set the names of the files explicitly, but used the field path
coming from the previous step.








Chapter 3
113
6. Under the Fields tab, use the Get Fields button to get the fields. In the price field, 
set the Format to $0.00.
7. 
Run the transformation and look at the generated XML file. It should look like the 
following: 
<Books>
   <Book>
      <id_title>123-346</id_title> 
      <title>Carrie </title>
      <genre>Fiction</genre>
      <price>$41,00</price>
      <author>King, Stephen</author>
   </Book>
   <Book>
      <id_title>123-347</id_title>
      <title>Salem›s Lot </title> 
       …
   </Book>
   … 
</Books>
How it works...
The XML output step does the entire task. It creates the XML file with rows coming in the 
stream, using the Parent XML element and Row XML element values to complete the 
structure of the XML file. It encloses each row between tags with the name you provided for 
Row XML element (<Book> and </Book>) and the whole structure between tags with the 
name provided for Parent XML element (<Books> and </Books>).
The XML output step has some properties in common with other output steps. For example, 
the option to add the date and time as part of the name of the file or to split the output in 
several files using the Split every ... rows textbox from the Content tab.
There's more...
In the recipe, you wrote the XML information into a file, but you may want to have the 
information in XML format as a new column of your dataset. The following section explains 
how to do this.
Generating fields with XML structures
If, rather than generating the XML structure in a file, you want the structure as a new field, 
then you should use the Add XML step from the Transform category instead of using the XML
output step.




Chapter 3
117
3. Double-click on the step and fill the Target stream properties, Source stream
properties, and Join condition properties frames, as shown in the following 
screenshot:
4. In the Result XML field inside the Result Stream properties frame, type 
xmlauthors2.
5. Do a preview of this step. You will see that there is a new field named xmlauthors2
containing the XML structure for the root XML and the authors. Also note that there is 
an empty tag named books for each author's node: 
<result>
   <authors>
      <author id_author ="A00001">
         <lastname>Larsson</lastname>
         <firstname>Stieg</firstname>
         <nationality>Swedish</nationality>
         <birthyear>1954</birthyear>
         <books/>
      </author>
      <author id_author ="A00002">
         <lastname>King</lastname>
         <firstname>Stephen</firstname>
         <nationality>American</nationality>
     ...
      </author> 
...
   </authors> 
</result>

Manipulating XML Structures
118
Finally, it's time to create the XML structure for the books and merge them with the main 
structure:
1. Drop a Table input step into the canvas, in order to select all the books. Use the 
following SQL statement:
SELECT * 
FROM Books 
ORDER BY title
2. Add an Add XML step. Name this step as Create Books XML.
3. Under the Content tab, type book in the XML root element textbox and xmlBooks in 
the Output Value textbox.
4. Under the Fields tab, use the Get Field button to obtain all the fields. Select attribute
= Y for the id_title field. Also, for the price field, set Format to $0.00.
5. Do a preview on this step. You will see a new XML field named xmlBooks with the 
book's data. For example: 
<book id_title ="123-346">
   <title>Carrie </title>
   <price>$41,00</price>
   <genre>Fiction</genre> 
</book>
6. Finally, you must do the last merge, this time between the output of the Merge
Authors and root XML step and the output of the recently created Create Books
XML step. Add one more XML Join step and link these two steps. The transformation 
must look like the following:
7. 
In this last step, set the following properties:

Chapter 3
119
8. In the Result XML field inside the Result Stream properties frame, type 
xmlfinalresult. This field will contain the final result.
9. You can do a preview on this last step and you will obtain something like the 
following: 
<result>
   <authors> 
...
      <author id_author ="A00002">
         <lastname>King</lastname>
         <firstname>Stephen</firstname>
         <nationality>American</nationality>
         <birthyear>1947</birthyear>
         <books>
            <book id_title ="123-353">
               <title>Bag of Bones</title>
               <price>$40,90</price>
               <genre>Fiction</genre>
            </book>
            <book id_title=" 123-346">
               <title>Carrie</title>
               …
            </book>
            …
         </books>
      </author>
      <author id_author=" A00007">
         <lastname>Kiyosaki</lastname>
         …
      </author>
   </authors> 
</result>

Manipulating XML Structures
120
How it works...
The basic idea when you have to generate a complex XML structure is to create partial XML 
outputs in different steps and then use the XML Join step to create the merged structure.
The XML Join step allows you to incorporate one XML structure (with one or multiple rows) 
inside another leading XML structure that must have only one row.
In the first join step of the sample transformation, you combined the XML that contains the 
empty root structure with the authors XML structure. This is a simple join - the step replaces 
the tag <authors/> of the root XML structure (the target stream) with all of the authors 
coming from the author XML structure (the source stream). The XPath expression, /result/ 
authors tells Kettle which node in the root structure is to be filled with the authors' structure.
The second XML Join step is a little more complex. It combines the result from the first XML
Join step and the selection of books. In this case, you have a complex join because you need 
to join each group of books with their corresponding author. To do this, you must type the 
condition of the join with the following XPath expression:
/result/authors/author[@id_author='?']/books
The ? character is used as a placeholder. During execution, this character will be replaced 
with the Join Comparison Field value (in this case, the id_author field value). So, all books 
in XML format with a particular id_author will replace the tag <books/> inside the tag 
<author> who have the same id_author. For example, the following book by Stieg Larsson 
(already converted to the XML format):
<book id_title="123-401"> 
<title>The Girl who Played with Fire</title> 
<price>$35,90</price> 
<genre>Fiction</genre> 
</book>
is in a row where id_author is equal to "A00001". Therefore, this structure will be inserted 
in the main XML structure in the following path:
/result/authors/author[@id_author='A00001']/books
This is the path belonging to that author.
See also
f
The recipe named Generating a simple XML document in this chapter. For 
understanding how the Add XML step works, see the section named Generating fields
with XML structures inside this recipe.
f
The recipe named Specifying fields by using XPath notation in this chapter. See this 
recipe if you need to  understand the XPath specification.


Manipulating XML Structures
122
      <genre>Fiction</genre>
      <lastname>King</lastname>
      <firstname>Stephen</firstname>
   </Book>
   <Book>
      <title>Salem›s lot</title>
       …
   </Book>
   … 
</Books>
7. 
Now, you must create the XSL file (booksFormat.xsl), based on the books.xml
structure. Create a new file with your preferred text editor and type the following:
<?xml version="1.0" encoding="UTF-8"?>
<xsl:stylesheet   version="1.0"
  xmlns:xsl="http://www.w3.org/1999/XSL/Transform"
  xmlns="http://www.w3.org/1999/xhtml">
  <xsl:output method="xml" indent="yes" encoding="UTF-8"/>
  <xsl:template match="/Books">
    <html>
      <head> <title>Books</title> </head>
      <body>
        <h1>Books</h1>
        <table border="1">
        <!-- grid header -->
        <tr bgcolor="lightblue"><td>Title</td><td>Author</td>
          <td>Price</td><td>Genre</td></tr>
          <xsl:apply-templates select="Book">
            <xsl:sort select="title" />
          </xsl:apply-templates>
        </table>
      </body>
    </html>
  </xsl:template>
  <xsl:template match="Book">
    <!-- grid value fields -->
    <tr>
      <td><xsl:value-of select="title"/></td>
      <td><xsl:value-of select="lastname"/>, <xsl:value-of  
        select="firstname"/></td>
      <td><xsl:value-of select="price"/></td>
      <td><xsl:value-of select="genre"/></td>
    </tr>
  </xsl:template> 
</xsl:stylesheet>


Manipulating XML Structures
124
In order to apply the transformation defined in the XSL file effectively, you used the XSL
Transformation job entry. The configuration of the entry is straightforward: you simply provide 
names of the XML file, the XSL file and the resulting file, and you're done.
There's more...
As an option, right after creating the page, you may publish it automatically on the website. 
For doing that, simply extend the job with a file transfer entry.
You will find more information about XSL at the following URL:
http://en.wikipedia.org/wiki/XSLT
You can also follow the following tutorial:
http://www.w3schools.com/xsl/
See also
The recipe named Putting files on a remote server, in Chapter 4, File Management. See this 
recipe for instructions on extending the job for transferring the generated page.

4
File Management
In this chapter, we will cover:
f
Copying or moving one or more files
f
Deleting one or more files
f
Getting files from a remote server
f
Putting files on a remote server
f
Copying or moving a custom list of files
f
Deleting a custom list of files
f
Comparing files and folders
f
Working with ZIP files
Introduction
On many occasions, the development of Kettle jobs and transformations involves 
manipulation of files, such as reading or writing a file along with other manipulations. Look at 
this sample scenario, where you have to:
f
Get a file with orders from a remote server
f
Validate and load the orders into a database
f
Move the processed file to a designated folder
f
If a file with that name already exists, rename the older version
f
If the orders in the file are not valid, generate a log file with details of the errors and 
put that log file back on to the server for further review
In this situation besides reading and writing files, you also have to transfer, rename, and move 
them.









File Management
134
Your job would look like the one shown in the following screenshot:
Finally, in the Get a file with FTP entry, you should type that variable (for example, ${DAILY_ 
FILENAME}) as the wildcard.
Some considerations about connecting to an FTP server
In order to be able to connect to an FTP server, you must complete the connection settings for 
the FTP server under the General tab of the Get a file with FTP job entry. If you are working 
with an anonymous FTP server, you can use anonymous as the username and free text as the 
password. This means that you can access the machine without having to have an account on 
that machine.
If you need to provide authentication credentials for access via a proxy, you must also 
complete the following textboxes: Proxy host, Proxy port, Proxy username, and Proxy
password.
Access via SFTP
SFTP means SSH File Transfer Protocol. It's a network protocol used to secure the file 
transfer capability. With Kettle, you can get files from an SFTP server by using the Get a file
with SFTP job entry. To configure this entry, you have to enter the name or IP of the SFTP 
server in the SFTP server name / IP textbox. The rest of the configuration of the General and 
Files tabs is pretty similar to the Get a file with FTP entry.
For more information on SFTP, you can visit the following URL:
http://en.wikipedia.org/wiki/SSH_file_transfer_protocol
Access via FTPS
An FTPS server extends the standard FTP protocol, adding cryptographic protocols, such as 
the Transport Layer Security (TLS) and the Secure Sockets Layer (SSL). You can use the Get
a file with FTPS job entry to get files from an FTPS server. To configure this entry, you have 
to enter the name or IP address of the FTPS server in the FTPS server name / IP address:
textbox. The rest of the configuration of the General and Files tabs is pretty similar to the Get
a file with FTP entry.
More information about FTPS can be obtained from the following URL:
http://en.wikipedia.org/wiki/Ftps





Chapter 4
139
f
The recipe named Writing an Excel file with several sheets in Chapter 2, Reading and 
Writing Files for directions on how to write an Excel file.
Deleting a custom list of files 
Suppose a scenario where you have to delete some files but you don't have the names of the 
files to delete beforehand. If you can specify that list with regular expressions, that wouldn't 
be a problem, but sometimes that is not possible. In these cases you should use a helper 
transformation that builds the list of files to delete. This recipe shows you how to do it.
For this recipe, assume you want to delete from a source directory all the temporary files that 
meet two conditions: the files have a .tmp extension and a size of 0 bytes.
Getting ready
In order to create and test this recipe, you need a directory with a set of sample files; some 
of them should have the .tmp extension and zero size. Some example files are shown in the 
following screenshot:
In the preceding screenshot, the files that must be deleted are sample3.tmp, sample5.tmp, 
and sample7.tmp.
How to do it...
Carry out the following steps:
1. Create the transformation that will build the list of files to delete.
2. Drop a Get File Names step into the canvas.
3. Under the File tab, fill the Selected files: grid. Under File/Directory, type 
${Internal.Transformation.Filename.Directory}\sample_directory
and under Wildcard (RegExp), type .*\.tmp.
4. From the Flow category, add a Filter rows step.
5. Use this step to filter the files with size equal to zero. In order to do that, add the 
condition size = 0.

File Management
140
6. After the Filter rows step, add the Select values step. When asked for the kind of 
hop to create, select Main output of step. This will cause only those rows that meet 
the condition to pass the filter.
7. 
Use the Select values step to select the field's path and short_filename.
8. From the Job category of Steps, add a Copy rows to result step.
9. Save the transformation.
10. Create a new job and add a Start entry.
11. Add a Transformation entry and configure it to run the transformation previously 
created.
12. Add a Delete files entry from the File management category.
13. Double-click on it and check the Copy previous Results to args? prompt.
14. Save the job and run it. The files with a .tmp extension and size 0 bytes will be 
deleted.
How it works...
In this recipe, you deleted a list of files by using the Delete files job entry. In the selected files 
grid of that entry, you have to provide the complete name of the files to delete or the directory 
and a regular expression. Instead of typing that information directly, here you built the rows for 
the grid in a separate transformation.
The first step used in the transformation is the Get File Names. This step allows you to get 
information about a file or set of files or folders. In this example, the step gets the list of .tmp
files from the sample_directory folder.
The following screenshot shows all of the information that you obtain with this step:
You can see these field names by pressing the space bar while having the focus on the Get
File Names step.
After to that step, you used a Filter rows step to keep just the files with size 0 bytes.


File Management
142
How to do it...
Carry out the following steps:
1. Create a new job, and drop a Start entry into the work area.
2. Add a File Compare job entry from the File management category. Here you must type 
or browse to the two files that must be compared, as shown in the following screenshot:
3. Add a Transformation job entry and a DUMMY job entry, both from the General
category. Create a hop from the File Compare job entry to each of these entries.
4. Right-click on the hop between the File Compare job entry and the Transformation
job entry to show the options, choose the Evaluation item and then select the 
Follow when result is false item.
5. Right-click on the hop between the File Compare job entry and the DUMMY job entry, 
choose the Evaluation item, and this time select the Follow when result is
true item.
6. The job should look like the one shown in the following diagram:
7. 
Then, create a new transformation in order to read the XML file. Drop a Get data 
from XML step from the Input category into the canvas and type the complete path 
for the XML file in the File or directory textbox under the File tab. In this case, it 
is ${Internal.Transformation.Filename.Directory}\sampleFiles\ 
NewMuseumsFileReceived.xml. Use /museums/museum in the Loop XPath
textbox under the Content tab, and use the Get fields button under the Fields tab to 
populate the list of fields automatically.
8. Save the transformation.
9. Configure the Transformation job entry for the main job to run the transformation you 
just created.

Chapter 4
143
10. When you run the job, the two files are compared.
11. Assuming that your files are equal, in the Logging window you will see a line similar 
to the following: 
2010/11/05 10:08:46 - fileCompare - Finished job entry [DUMMY] 
(result=[true])
This line means that the flow went toward the DUMMY entry.
12. If your files are different, in the Job metrics window you will see that the fileCompare
entry fails, and under the Logging tab, you will see something similar to the following: 
...
... - Read XML file - Loading transformation from XML file 
[file:///C:/readXMLFile.ktr]
... - readXMLFile - Dispatching started for transformation 
[readXMLFile]
... - readXMLFile - This transformation can be replayed with 
replay date: 2010/11/05 10:14:10 
... - Read museum data.0 - Finished processing (I=4, O=0, R=0, 
W=4, U=0, E=0) 
... - fileCompare - Finished job entry [Read XML file] 
(result=[true]) 
...
13. This means that the transformation was executed.
How it works...
The File Compare job entry performs the comparison task. It verifies whether the two files 
have the same content. If they are different, the job entry fails. Then, the job proceeds with 
the execution of the transformation that reads the new file. However, if the files are the same, 
the job entry succeeds and the flows continue to the DUMMY entry.
In other words, the new file is processed if and only if the File Compare fails, that is, if the two 
files are different.
There's more...
Besides comparing files with Kettle, you can also compare directories; let's see how it works.
Comparing folders
If you want to compare the contents of two folders, you can use the Compare folder job entry 
from the File management category.


Chapter 4
145
How to do it...
You will create this recipe in two different steps. In the first step, you will compress the log 
files, and in the second step, you will uncompress them and organize them in monthly ZIP 
files.
So, let's compress the weblog files, by carrying out the following steps:
1. Create a new job and drop a Start job entry into the canvas.
2. Add a Zip file job entry from the File management category.
3. Under the General tab, select the source directory by clicking on the Folder... button. 
The example points to a web server log directory, such as C:\WINDOWS\system32\ 
Logfiles\W3SVC1\test_files.
4. Type .+\.log in the Include wildcard (RegExp): textbox, in order to read all the files 
with the .log extension.
5. In the Zip File name: textbox, type the path and name for the destination ZIP file. For 
example: C:\WINDOWS\system32\Logfiles\W3SVC1\test_files\weblogs.
zip.
6. You have several additional options for including date and time to the ZIP file name. 
You don't need to set those options for this recipe.
7. 
Under the Advanced tab, choose Delete files in the After Zipping drop-down list.
8. When running this job, a new file named weblogs.zip will be created containing 
the log information from the web server and the log files will be deleted.
Now assuming that you have copied the generated ZIP file to your computer, you want to unzip 
the weblogs and generate small ZIP files grouping them by month:
1. Create a new job and drop a Start job entry into the canvas.
2. Add an Unzip file job entry and open it.
3. In the Zip File name textbox, you must browse for the ZIP file you created previously 
(weblogs.zip).
4. Choose a Target Directory, for example: ${Internal.Job.Filename. 
Directory}\logs and check the Create folder checkbox to create that 
folder, if it doesn't exist.
5. Type .+\.log in the Include wildcard textbox.
6. In the After extraction drop-down box select Delete files.
7. 
Under the Advanced tab, set the Success on condition to At least we
successfully unzipped x files, and set the Limit files textbox to 1.
8. The next step for the job will be calling a transformation that creates the groups of 
logs for the smaller ZIP files. Add a Transformation job entry.


Chapter 4
147

Under Cut from type 2

Under Cut to type 6 and close the window.
14. Double-click on the UDJE step. With this step, you will create the fields for the .zip 
grid. Add three String fields named wildcard, wildcardexc, and destination. 
As Java expression, type "ex"+year_month+"[0-9][0-9]\\.log", "" and 
path+"\\zip_files\\"+year_month+".zip" respectively. Don't forget to set 
the Value type to String.
15. Double-click on the Select values step. Use it to select the field's path, wildcard, 
wildcardexc, and destination. Close the window.
16. Double-click on the Sort rows step. Use it to sort by destination. Check the Only
pass unique rows? (verifies keys only) option. With this, you generate a single row by 
month.
17. Do a preview. You should see something like the following:
18. Save the transformation.
19. Go back to the job, and add a Zip file job entry.
20. Double-click on the entry, check the Get arguments from previous option and close 
the window.
21. Save the job, and run it.
22. Browse the log folder. You will see all log files that were compressed in the weblogs. 
zip file, and a subfolder named zip.
23. Browse the ZIP folder. You will see one ZIP file for each month for which you had log 
files.
How it works...
In this recipe, you saw the functionality of the Zip File and Unzip File job entries.
First, you zipped all log files in a folder. You specified the files to zip and the name and location 
of the ZIP file to generate. This is quite simple and doesn't require further explanation.

File Management
148
In the second part of the recipe, you performed two different tasks:
The first was to unzip a file. You specified the ZIP file, the regular expression that indicates 
which files to extract from the ZIP file, and the destination of the files. This is also a simple and 
intuitive operation.
The last part, the most elaborate task of the recipe, was compressing the log files grouped by 
month. In order to do this, you couldn't fill the grid in the Zip file job entry manually because 
you didn't know in advance the names of the log files. Therefore, instead of filling the grid you 
checked the Get arguments from previous option. The transformation was responsible for 
generating the values for the grid in the Zip file entry setting window. The columns generated 
by the transformation were:
f
Folder of the files to zip
f
Regular expression for the files to zip
f
Regular expression for the files to exclude (in this case, you set a null value)
f
Destination ZIP file name
These are the four fields that the Zip file entry needs in order to zip the files. For each row 
generated in the transformation, a new ZIP file was created based on these values.
There's more...
Look at some notes about zipping and unzipping files:
Avoiding zipping files
If you need to zip some files for attaching to an e-mail, then you don't have to use the Zip file
entry. The Mail job entry does the task of zipping for you.
Avoiding unzipping files
In the recipe, you unzipped a file because you had to manipulate the files. If, instead of 
manipulating the files as you did, you need to read them you don't have to use the Unzip file
entry. Kettle is capable of reading those files as they are. For a complete reference on this 
subject, you can take a look at the following entry in Slawomir Chodnicki's blog:
http://type-exit.org/adventures-with-open-source-bi/2010/11/directly-
accessing-remote-andor-compressed-files-in-kettle/
See also
The recipe named Sending e-mails with attached files of Chapter 9, Getting the Most Out of 
Kettle. This recipe will teach you how to attach ZIP files in an e-mail.

5
Looking for Data
In this chapter, we will cover:
f
Looking for values in a database table
f
Looking for values in a database (with complex conditions or multiples tables involved)
f
Looking for values in a database with extreme flexibility
f
Looking for values in a variety of sources
f
Looking for values by proximity
f
Looking for values consuming a web service
f
Looking for values over an intranet or Internet
Introduction
With transformations, you manipulate data in many ways: doing mathematical or logical 
operations, applying string functions, grouping by one or more columns, sorting, and much 
more. Besides transforming the data you already have, you may need to search and bring data 
from other sources. Let's look at some examples:
f
You have some product codes and you want to look for their descriptions in an Excel 
file
f
You have a value and want to get all products whose price is below that value from a 
database
f
You have some addresses and want to get the coordinates (latitude, longitude) for 
those locations from a web service
Searching for information in databases, text files, web services, and so on is a very common 
task and Kettle has several steps for doing it. In this chapter, you will learn about the different 
options.




Chapter 5
153
2. If the rows are useless without the fields that you are looking for, then you can discard 
them. You do that by checking the Do not pass the row if the lookup fails option. 
This way, only the rows for which the lookup succeeds will pass to the next step.
Taking some action when there are too many results
The Database lookup step is meant to retrieve just one row of the table for each row in your 
dataset. If the search finds more than one row, the following two things may happen:
1. If you check the Fail on multiple results? option, the rows for which the lookup 
retrieves more than one row will cause the step to fail. In that case, in the Logging
tab window, you will see an error similar to the following: 
... - Database lookup (fail on multiple res.).0 – ERROR... Because 
of an error, this step can't continue: 
... - Database lookup (fail on multiple res.).0 – ERROR... : 
Only 1 row was expected as a result of a lookup, and at least 2 
were found!
Then you decide if you leave the transformation or capture the error.
2. If you don't check the Fail on multiple results? option, the step will return the first 
row it encounters. You can decide which one to return by specifying the order. You do 
that by typing an order clause in the Order by textbox. In the sampledata database, 
there are three products that meet the conditions for the Corvette row. If, for Order
by, you type PRODUCTSCALE DESC, PRODUCTNAME, then you will get 1958 Chevy
Corvette Limited Edition, which is the first product after ordering the three 
found products by the specified criterion.
If instead of taking some of those actions, you realize that you need all the resulting rows, you 
should take another approach: replace the Database lookup step with a Database join or a 
Dynamic SQL row step. For recipes explaining these steps, see the following See also section.
Looking for non-existent data
If instead of looking for a row you want to determine if the row doesn't exist, the procedure 
is much the same. You configure the Database lookup step to look for those rows. Then you 
capture the error, as depicted in the following diagram:
In this case, the stream that you use for capturing the error becomes your main stream. The 
rows that didn't fail will be discarded and the rows for which the lookup failed go directly to the 
main stream for further treatment.








Chapter 5
161
Getting ready
For doing this recipe, you will need the following:
f
A CSV file (authors.txt) with the author's data. The file should have the following 
columns: lastname, firstname, nationality, and id_author. The following 
are sample lines of this file:
"lastname","firstname","nationality","id_author" 
"Larsson","Stieg","Swedish","A00001" 
"King","Stephen","American","A00002" 
"Hiaasen","Carl ","American","A00003" 
"Handler","Chelsea ","American","A00004" 
"Ingraham","Laura ","American","A00005"
f
An Excel file with the book›s information (books.xls). The sheet should have the 
following columns: title, id_author, price, id_title, and id_genre as 
shown in the following screenshot:
You can also download sample files from the book's website.
How to do it...
Carry out the following steps:
1. Create a new transformation.
2. Drop an Excel input step and a Text file input step into the canvas.
3. In the Excel input step, browse for the books.xls file under the File tab and click 
on the Add button. Populate the grid under the Fields tab by clicking on the Get
fields from header row button.
4. In the Text file input step, browse for the authors.txt file and click on the Add
button. Type , as the Separator under the Content tab and finally, populate the 
Fields tab grid by clicking on the Get Fields button.
5. Add a Stream lookup step from the Lookup category.





Looking for Data
166
Getting ready
You must have a books database with the structure shown in the Appendix, Data Structures.
The recipe uses a file named booksOrder.txt with the following book titles, which 
deliberately include some typos:
Carry 
Slem's Lot 
TheShining 
The Ded sone 
Pet Cemetary 
The Tomyknockers 
Bag of Bones 
Star Island 
Harry Potter
How to do it...
Carry out the following steps:
1. Create a new transformation.
2. Drop a Text file input step into the work area, in order to retrieve the book order.
3. In the File or directory textbox under the File tab, browse to the booksOrder.txt
file and then click on the Add button.
4. Under the Content tab, uncheck the Header checkbox.
5. Under the Fields tab, add a new String field named TitleApprox.
6. Drop another step in order to read the books database. Use a Table input step and 
type the following SQL statement:
SELECT title
     , price 
FROM Books
7. 
Add a Fuzzy match step from Lookup category. Add a hop from the Table input step 
toward this step. Add another hop from the Text file input step created before also 
toward the Fuzzy match step. 
8. Double-click on the Fuzzy match step. Under the General tab, go to the Lookup
stream (source) frame and as Lookup step, select the Table input step created 
before. In the Lookup field, select title.
9. In the Main stream field, select the TitleApprox field.


Looking for Data
168
As minimum and maximum values, you specified 0 (meaning that the exact title was found) 
and 3 (meaning that you will accept as valid a title with a maximum of three edits).
For example, when you preview the result of this step, you can see a title named The Ded sone
which matches the real title The Dead Zone with a distance of 2. For the Star Island title the 
distance is 0 because the spelling was correct and a book with exactly the same title was found. 
Finally, for the Harry Potter row, there are no matching rows because you need too many 
editions to transform the provided title into one of the Harry Potter titles in the database.
There's more...
The Fuzzy match step allows you to choose among several matching algorithms, which are 
classified in the following two groups:
f
Algorithms based on a metric distance: The comparison is based on how the 
compared terms are spelled.
f
Phonetic algorithms: The comparison is based on how the compared terms sound, 
as read in English.
Let's see a brief comparative table for the implemented algorithms:
Algorithm
Classification
Explanation
Levenshtein
Metric distance
The distance is calculated as the minimum edit distance 
that transforms one string into the other. These edits can 
be character insertion or deletion or substitution of a single 
character.
Damerau-
Levenshtein
Metric distance
Similar to Levenshtein, but adds the transposition operation.
Needleman-
Wunsch
Metric distance
A variant of the Levenshtein algorithm. It adds a gap cost, 
which is a penalty for the insertions and deletions.
Jaro
Metric distance
Based on typical spelling deviations. The index goes from 0 to 
1, with 0 as no similarity and 1 as the identical value.
Jaro-Winkler
Metric distance
A variant of the Jaro algorithm, appropriate for short strings 
such as names.
Pair letters 
Similarity
Metric distance
The strings are divided into pairs, and then the algorithm 
calculates an index based on the comparison of the lists of 
pairs of both strings.
SoundEx
Phonetic
It consists of indexing terms by sound. It only encodes 
consonants. Each term is given a Soundex code, each 
soundex code consists of a letter and three numbers. Similar 
sounding consonants share the same digit (for example, b, f, 
p, v are equal to 1).
Refined 
SoundEx
Phonetic
A variant of the SoundEx algorithm optimized for spell 
checking.

Chapter 5
169
Algorithm
Classification
Explanation
Metaphone
Phonetic
The algorithm is similar to SoundEx algorithm, but produces 
variable length keys. Similar sounding words share the same 
keys.
Double 
Metaphone
Phonetic
An extension of the Metaphone algorithm where a primary 
and a secondary code are returned for a string.
The decision of which algorithm to choose depends on your problem and the kind of data you 
have or you expect to receive. You can even combine a fuzzy search with a regular search. For 
example, in the recipe, you didn't find a match for the Harry Potter row. Note that increasing 
the maximum value wouldn't have found the proper title. Try raising the maximum value to 10, 
and you will see that the algorithm brings Carrie as the result, which clearly has nothing to do 
with the wizard. However, if you look for this value with a Database join step by comparing with 
the LIKE operator, you could retrieve not just one, but all the Harry Potter titles.
Further details on the individual similarity metrics can be found at the following URL:
http://en.wikipedia.org/wiki/String_metrics
You can also read more about them at the following URL:
http://www.dcs.shef.ac.uk/~sam/simmetrics.html
This documentation belongs to SimMetrics, an open source Java library of similarity metrics.
Looking for values consuming a web service
Web services are interfaces that are accessed through HTTP and executed on a remote 
hosting system. They use XML messages that follow the SOAP standard.
With Kettle, you can look for values in available web services by using the Web service lookup
step. In this recipe, you will see an example that shows the use of this step.
Suppose that you have a dataset of museums and you want to know about their opening and 
closing hours. That information is available as an external web service.
The web service has a web method named GetMuseumHour that receives the id_museum
as a parameter, and returns the museum schedule as a String. The request and response 
elements for the GetMuseumHour web method used in this recipe look like the following:
f
Request: 
<soap:Envelope xmlns:xsi="http://www.w3.org/2001/XMLSchema-
instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema" 
xmlns:soap="http://schemas.xmlsoap.org/soap/envelope/">
  <soap:Body>
    <GetMuseumHour xmlns="http://tempuri.org/">






6
Understanding Data 
Flows
In this chapter, we will cover:
f
Splitting a stream into two or more streams based on a condition
f
Merging rows from two streams with the same or different structure
f
Comparing two streams and generating differences
f
Generating all possible pairs formed from two datasets
f
Joining two streams based on conditions
f
Interspersing new rows in between existent rows
f
Executing steps even when your stream is empty
f
Processing rows differently based on the row number
Introduction
The main purpose of Kettle transformations is to manipulate data in the form of a dataset; 
this task is done by the steps of the transformation.
When a transformation is launched, all its steps are started. During the execution, the 
steps work simultaneously reading rows from the incoming hops, processing them, and 
delivering them to the outgoing hops. When there are no more rows left, the execution of the 
transformation ends.
The dataset that flows from step to step is not more than a set of rows all having the same 
structure or metadata. This means that all rows have the same number of columns, and the 
columns in all rows have the same type and name.



Understanding Flows of Data
178
5. Create the hops, as shown in the following screenshot. When you create the hops, 
make sure that you choose the options according to the image: Result is TRUE for 
creating a hop with a green icon, and Result is FALSE for creating a hop with a red 
icon in it.
6. Double-click on the first Filter rows step and complete the condition, as shown in the 
following screenshot:
7. 
Double-click on the second Filter rows step and complete the condition with price
< 100.
8. You have just split the original dataset into three groups. You can verify it by 
previewing each Dummy step. The first one has products whose category is not tents; 
the second one, the tents under 100 US$; and the last group, the expensive tents; 
those whose price is over 100 US$.
9. The preview of the last Dummy step will show the following:

Chapter 6
179
How it works...
The main objective in the recipe is to split a dataset with products depending on their category 
and price. To do this, you used the Filter rows step.
In the Filter rows setting window, you tell Kettle where the data flows to depending on the result 
of evaluating a condition for each row. In order to do that, you have two list boxes: Send 'true'
data to step and Send 'false' data to step. The destination steps can be set by using the hop 
properties as you did in the recipe. Alternatively, you can set them in the Filter rows setting 
dialog by selecting the name of the destination steps from the available drop-down lists.
You also have to enter the condition. The condition has the following different parts:
f
The upper textbox on the left is meant to negate the condition.
f
The left textbox is meant to select the field that will be used for comparison.
f
Then, you have a list of possible comparators to choose from.
f
On the right, you have two textboxes: The upper textbox for comparing against a field 
and the bottom textbox for comparing against a constant value.
Also, you can include more conditions by clicking on the Add Condition button on the right. If 
you right-click on a condition, a contextual menu appears to let you delete, edit, or move it.
In the first Filter rows step of the recipe, you typed a simple condition: You compared a 
field (category) with a fixed value (tents) by using the equal (=) operator. You did this to 
separate the tents products from the others.
The second filter had the purpose of differentiating the expensive and the cheap tents.


Chapter 6
181
1. Add a Get Variables step from the Job category. Put it in the stream after the Text file 
input step and before the Filter Rows step; use it to create a new field named categ
of String type with the value ${categ} in the Variable column.
2. Now, the transformation looks like the one shown in the following screenshot:
3. After this, you can set the condition of the first Filter rows step to category =
categ, selecting categ from the listbox of fields to the right. This way, you will be 
filtering the kitchen products.
4. If you run the transformation and set the parameter to tents, you will obtain similar 
results to those that were obtained in the main recipe.
Avoiding the use of nested Filter Rows steps
Suppose that you want to compare a single field against a discrete and short list of possible 
values and do different things for each value in that list. In this case, you can use the Switch / 
Case step instead of nested Filter rows steps.
Let's assume that you have to send the rows to different steps depending on the category. The 
best way to do this is with the Switch / Case step. This way you avoid adding one Filter row
step for each category.
In this step, you have to select the field to be used for comparing. You do it in the Field name
to switch listbox. In the Case values grid, you set the Value—Target step pairs. The following 
screenshot shows how to fill in the grid for our particular problem:









Understanding Flows of Data
190
Both files have the same structure and look like the following:
Roller_Coaster|Speed|park|location|country|Year
Kingda Ka|128 mph|Six Flags Great Adventure|Jackson, New Jersey||2005 
Top Thrill Dragster|120 mph|Cedar Point|Sandusky, Ohio||2003 
Dodonpa|106.8 mph|Fuji-Q Highland|FujiYoshida-shi|Japan|2001 
Steel Dragon 2000|95 mph|Nagashima Spa Land|Mie|Japan|2000 
Millennium Force|93 mph|Cedar Point|Sandusky, Ohio||2000 
...
For the There's more section, you will also need a database with the first file already loaded in 
a table. You will find a script for creating and loading it also available for downloading.
How to do it...
Carry out the following steps:
1. Create a transformation.
2. Drag a Text file input step into the canvas and use it to read the file top_roller_ 
coasters.txt. As a separator, type |.
3. Do a preview to make sure that you are reading the file as expected.
4. Add a Sort rows step to sort the rows by roller_coaster and park.
5. Repeat the steps 2 to 4 to read the file named top_roller_coasters_updates. 
txt and sort the rows also by roller_coaster and park.
6. From the Join category, add a Merge Rows (diff) step and use it to join both streams 
as depicted in the following diagram:
7. 
Double-click on the step you just added. In the Reference rows origin: select the 
name of the step coming from the stream that reads the top_roller_coasters. 
txt file.
8. In the Compare rows origin: select the name of the step coming from the stream that 
reads the top_roller_coasters_updates.txt file.
9. As Flag fieldname, type flag.





Chapter 6
195
This particular recipe is in fact the implementation of the CAG or Community Acronym
Generator as proposed by Nicholas Goodman (@nagoodman) on twitter:
@webdetails @pmalves @josvandongen How about CAG? Community Acronym 
Generator? A project to generate new acronyms for community projects?!
There are already several community projects around Pentaho such as CDF, CDE, or CDA. Why 
don't we follow Nicholas's suggestion and develop the CAG as follows:
Given two lists of words, the Kettle transformation will generate all combinations of words that 
lead to potential community projects.
How to do it...
Carry out the following steps:
1. Create a new transformation and add two Data Grid steps from the Input category.
2. Use one of the Data Grid steps to create a dataset with a single String field named 
middle_word. Under the Data tab, enter a set of names for the middle word of the 
acronym. Here you have a sample list: Dashboard, Data, Chart, Acronym, Cube, 
Report.
3. Use the other Data Grid step to create a dataset with a single String field named 
last_word. Under the Data tab, enter a set of names for the last word of the 
acronym. Here you have a sample list: Framework, Editor, Translator, Access, 
Generator, Integrator, Component.
4. From the Join category, add a Join Rows (Cartesian product) step.
5. Create hops from the Data Grid steps toward this step. You will have something like 
the following:
6. From the Scripting category, add a User Defined Java Expression step (UDJE for 
short).
7. 
Use the UDJE to add two String fields. Name the first new_component, and as 
Java Expression type "Community "+middle_word+" "+last_word. Name 
the second field acronym and as Java Expression type "C"+middle_word. 
substring(0,1)+last_word.substring(0,1).


Chapter 6
197
As the stream coming out of the Get Variable step has a single row, the Cartesian product 
will have all the possible combinations of N rows of the main stream with a single row, that is, 
N rows. In this case, it is important that in the Main step to read from option, you select the 
main stream, the stream coming from the UDJE. Doing so, you tell Kettle that most of the data 
will come from this step and Kettle will cache or spool to disk the data coming from the Get
Variable step.
Limiting the number of output rows
With a Join Rows (Cartesian product) step, you can limit the number of output rows by 
entering a simple or complex condition in its setting window. The rows that don't meet the 
condition are discarded.
Back to the recipe. As you might have noticed, it is possible for the transformation to generate 
acronyms that already exist, for example, CDF. In the previous subsection, you added a 
second Join Rows (Cartesian product) step. In this step, you could add a condition to discard 
the rows with acronyms that already exist, excepting when the product is Enterprise Edition. 
The condition area in the setting window of the step would look like the one shown in the 
following screenshot (except for the exact list of acronyms which might have changed by the 
time you're reading this):
If you do a preview on this step, you will see something like the following:


Chapter 6
199
How to do it...
Carry out the following steps:
1. Create a new transformation.
2. Drop two Excel input steps into the canvas.
3. Use one step for reading the budget information (budget.xls file) and the other for 
reading the costs information (costs.xls file).
4. Under the Fields tab of these steps, click on the Get fields from header row… button 
in order to populate the grid automatically. Apply the format dd/MM/yyyy to the 
fields of type Date and $0.00 to the fields with costs.
5. Add a Merge Join step from the Join category, and create a hop from each Excel
input step toward this step. The following diagram depicts what you have so far:
6. Configure the Merge Join step, as shown in the following screenshot:
7. 
If you do a preview on this step, you will obtain the result of the two Excel files 
merged. In order to have the columns more organized, add a Select values step 
from the Transform category. In this new step, select the fields in this order: task, 
starting date (est.), starting date, end date (est.), end date, cost
(est.), cost.




Chapter 6
203
4. From the Scripting category, add a User Defined Java Expression step, and use 
it to add two fields: The first will be a String named desc_product, with value 
("Category: " + category).toUpperCase(). The second will be an Integer
field named order with value 1.
5. Use a Select values step to reorder the fields as category, desc_product, qty_ 
product, and order. Do a preview on this step; you should see the following result:
6. Those are the headers. The next step is mixing all the rows in the proper order. Drag 
an Add constants step into the canvas and a Sort rows step. Link them to the other 
steps as shown:
7. 
Use the Add constants to add two Integer fields: qty_prod and order. As Value, 
leave the first field empty, and type 2 for the second field.
8. Use the Sort rows step for sorting by category, order, and desc_product.
9. Select the last step and do a preview. You should see the rows exactly as shown in 
the introduction.



Understanding Flows of Data
206
5. From the Flow category, add a Detect empty stream step. Also, add a User Defined
Java Expression or UDJE step, and link all steps as follows:
6. Use the UDJE step and fill it in, as shown in the following screenshot:
That's all! Let's test the transformation:
1. Press F9 to run it; give the PROD_FILTER variable the value lamp (or any value that 
you know is part of the description of some of your products). You do this by typing 
the value into the grid named Variables. Click on Launch.
2. Open the generated file. It should look like the one shown in the following screenshot:
3. Run the transformation again, but this time, type a value that you know isn't part of 
the descriptions of your products, for example motorcycle.
4. Open the file. This time it should have the content as shown in the following 
screenshot:

Chapter 6
207
How it works...
When a step doesn't return data, the flow ends. None of the steps that follow that step are 
executed because they don't receive data for processing. The Detect empty stream step, as 
the name suggests, detects that situation. As a consequence, it generates a stream with the 
same metadata as the expected stream, and a single row with null values. This way, you avoid 
the stream to "die".
In order to understand what the step does in a better way, try the following:
1. In the transformation that you just created, select the Detect empty stream step.
2. Press F9 to do a preview, give to the variable PROD_FILTER the value lamp, and click 
on Launch.
3. You will see a message informing you that there are no rows to preview. That's 
because the main stream had rows and they went toward the Excel step.
4. Try the same procedure again, but this time, enter an invalid value, for example, 
motorcycle. You will see a single row with the columns category, id_product, 
desc_product, and price, all with null values.
In the recipe, in the step that follows the Detect empty stream step, you replaced the null 
value in the category column with the message you wanted to write in the file, and sent the 
data toward the Excel file.
The Excel output step doesn't care if the data came from the main stream or the alternative 
one that you created for the empty stream. It simply sends the columns to the Excel file.
Finally, it's worth mentioning why we used the UDJE step. The selection of this step is smart 
because it replaces the value of the category field. Most steps add new fields, but are not 
able to manipulate existing ones.
There's more...
You can use the Detect empty stream step in the same way you would implement error handling. 
The difference is that here there are no errors; you simply have an exceptional situation.
As you would do when handling errors, you can fix or manipulate the stream and send it back 
to the main stream, as you did in the recipe, or you could completely ignore the metadata 
generated by the Detect empty stream step and simply use that step as the beginning of a 
new independent stream. For example, instead of generating the Excel file when there are 
no rows, you could write a message to the log, such as criteria doesn't match any
product.


Chapter 6
209
5. Add an Add sequence step from the Transform category. Type rank in the Name of
value textbox.
6. By previewing this step, you will obtain a list of books ranked by their sales.
7. 
Add two Filter rows steps and three Dummy steps (all from the Flow category) and 
create the hops, as depicted in the following diagram:
8. In the first Filter rows, set the following condition: rank <= 5.
9. In the last Filter rows step add the condition rank <= 15.
10. The Dummy 1 step represents the 5 best-selling books. For example:
11. The Dummy 2 step represents the next 10 best-selling books.
12. The rest of the books can bee seen in the Dummy 3 step.
13. You can do a preview of each of these Dummy steps and verify the results.
How it works...
This recipe reads the sales_books.xls file to create a dataset of the book titles along with 
their sales information. The Sort rows step is necessary to order the books by sales starting 
with the best seller.
Then, you dropped an Add sequence step to enumerate the rows. In this case, the field you 
added represents the ranking value. The best selling book will have the number one.
At this moment, you have the list of books ranked by their sales. Now, you only have to filter 
the books based on their ranks. You do it by using the Filter rows step.

Understanding Flows of Data
210
The first Filter rows step uses the condition rank <= 5 to get the top five best-selling books. 
The rest of the books will be filtered again, now with the condition rank <= 15; this will bring 
the rows ranked from 6 to 15. The remaining books, those with a rank greater than 15, will go 
to the last Dummy step.
There's more...
In the recipe, you enumerated the rows and then you did different things based on the 
row number. There are also some specific use cases, which are explained in the following 
subsections.
Identifying specific rows 
Suppose that you only want to keep the books with rank 15 to 20 and discard the rest. In this 
case, you don't have to add the Add sequence step and the Filter rows step afterward. There 
is a simpler way of doing that. There is also a step named Sample rows in the Statistics
category that allows picking specific rows from a dataset. For example, filling the Lines range
textbox with 1..5,9,15..20, you will get:
f
The rows 1 to 5
f
The row 9
f
The rows 15 to 20
The rest of the lines will be discarded. For the preceding example, you should just type 15..20.
Identifying the last row in the stream
Suppose that you want to know which book sold the least. In this case, you cannot filter by 
row number because you don't know how many books there are. In this case, instead of 
enumerating the rows, you can use the Identify last row in a stream step from the Flow
category.
In this step, you only have to type a value for the Result fieldname textbox. When you execute 
the transformation, this new field will return Y for the last row and N otherwise. In the example, 
you can know which the least sold book was, by filtering the row where the field is equal to Y.
Avoiding using an Add sequence step to enumerate the rows
If you need to enumerate the rows just after reading the data, then you don't need to add an 
Add sequence step. In several of the input steps, such as Text file input or Get data from
XML, you have a checkbox named Rownum in output? under the Content tab. This allows you 
to create a new field with a sequence for the rows. The name of this new field must be typed in 
the Rownum fieldname textbox.
This also applies when you need to rank the rows as in the recipe, and your input data is 
already ordered.

Chapter 6
211
See also
The recipe named Splitting a stream into two or more streams based on a condition in this 
chapter. This recipe has all the details about the Filter row step.


7
Executing and 
Reusing Jobs and 
Transformations
In this chapter, we will cover:
f
Executing a job or a transformation by setting static arguments and parameters
f
Executing a job or a transformation from a job by setting arguments and parameters 
dynamically
f
Executing a job or a transformation whose name is determined at runtime
f
Executing part of a job once for every row in the dataset
f
Executing part of a job several times until a condition is true
f
Creating a process flow
f
Moving part of a transformation to a subtransformation
Introduction
A transformation by itself rarely meets all the requirements of a real-world problem. It's 
common to face some of the following situations:
f
You need to execute the same transformation over and over again
f
You need to execute a transformation more than once, but with different parameters 
each time


Chapter 7
215
A sample output file is as follows:
Hello, Eva! It's January 09, 09:37.
Sample transformation: Random list
This transformation generates a file with a list of random integers. The quantity generated is 
defined as a named parameter called QUANTITY, with a default value of 10.
The transformation looks like the one depicted in the following diagram:
A sample output file is as follows:
-982437245 
1169516784 
318652071 
-576481306 
1815968887
Sample transformation: Sequence
This transformation generates a file with a list of numbers. The transformation receives two 
command-line arguments representing FROM and TO values. It also has a named parameter
called INCREMENT with a default of 1. The transformation generates a list of numbers 
between FROM and TO, with increments of INCREMENT.
The transformation looks like the one shown in the following diagram:

Executing and Reusing Jobs and Transformations
216
A sample output file using from=0, to=6, increment=2 is as follows: 
0 
2 
4 
6
Sample transformation: File list
This transformation generates a file containing the names of the files in the current directory.
The transformation looks like the one depicted in the following diagram:
A sample output file is as follows:
gen_random.ktr 
gen_sequence.ktr 
get_file_names.ktr 
hello.ktr
Launching jobs and transformations
As said, the recipes in this chapter are focused on different ways of running Kettle 
transformations and jobs. Ultimately, you will end up with a main job. In order to test your job 
with different inputs or parameters, you can use Spoon as usual, but it might be useful or 
even simpler to use Kitchen, a command-line program meant to launch Kettle jobs. If you're 
not familiar with Kitchen, this section gives you a quick review.
In order to run a job with Kitchen:
1. Open the terminal window
2. Go to the Kettle installation directory
3. Run kitchen.bat /file:<kjb file name> (Windows system) or kitchen.sh / 
file:<kjb file name> (Unix-based system), where <kjb file name> is the name 
of your job, including the complete path. If the name contains spaces, you must 
surround it with double quotes.
If you want to provide command-line parameters, just type them in order as part of the 
command.
If you want to provide a named parameter, use the following syntax: 
/param:<parameter name>=<parameter value>


Executing and Reusing Jobs and Transformations
218
Make sure you have defined the variable ${OUTPUT_FOLDER} with the name of the 
destination folder. Also, make sure that the folder exists.
How to do it...
Carry out the following steps:
1. Create a job.
2. Drag a Start job entry and three Transformation job entries into the canvas. Link all 
the entries, one after the other.
3. Double-click on the first Transformation entry.
4. As Transformation filename, browse to and select the sample transformation gen_ 
sequence.ktr.
5. Select the Argument tab and fill the grid with a 1 in the first row and a 10 in the 
second row.
6. Double-click on the second Transformation entry.
7. 
As Transformation filename, select the sample transformation gen_sequence.ktr.
8. Select the Argument tab and fill the grid with a 0 in the first row and a 100 in the 
second.
9. Select the Parameters tab. In the first row of the grid, type INCREMENT under 
Parameter and 20 under Value.
10. Double-click on the last Transformation entry.
11. As Transformation filename, select the sample transformation gen_sequence.ktr.
12. Select the Argument tab and fill the grid with a 50 in the first row and a 500 in the 
second.
13. Select the Parameters tab. In the first row of the grid, type INCREMENT under 
Parameter and 50 under Value.
14. Save and run the job.
15. Check the output folder. You will find the following three files:

sequence_1_10_1.txt

sequence_0_100_20.txt

sequence_50_500_50.txt
16. Edit the files. You will see that they contain the sequences of numbers 0, 1,..., 10 in 
the first file, 0, 20, 40,..., 100 in the second, and 100, 200,.., 500 in the third, just as 
expected.



Chapter 7
221
5. After that step, add a Copy rows to result step. You will find it under the Job category.
6. Do a preview on the last step. You should see the following screen:
7. 
Save the transformation and create a job.
8. Drag a Start job entry and two Transformation job entries into the canvas. Link the 
entries, one after the other.
9. Double-click on the first Transformation entry and for Transformation filename, 
select the transformation you just created.
10. Close the window and double-click on the second Transformation entry.
11. For Transformation filename, select the sample transformation gen_sequence.ktr.
12. Select the Advanced tab and check the first three options: Copy previous results to
args?, Copy previous results to parameters?, and Execute for every input row?
13. Select the Parameters tab. For the first row in the grid, type INCREMENT under 
Parameter and increment_value under Stream column name.
14. Close the window.
15. Save and run the job.
16. As a result, you will have a new file named sequence_0_90_30.txt. The file will 
contain the sequence of numbers 0, 30, 60, 90, just as expected.
How it works...
The transformation you ran in the recipe expects two arguments: from and to. It also has 
a named parameter: INCREMENT. There are a couple of ways to pass those values to the 
transformation:
f
Typing them at the command line when running the transformation with Pan or 
Kitchen (if the transformation is called by a job).
f
Typing them in the transformation or job setting window when running it with Spoon.





Executing and Reusing Jobs and Transformations
226
Suppose that you have a file with a list of names, as for example:
name
Paul 
Santiago 
Lourdes 
Anna
For each person, you want to:
f
Generate a file saying hello to that person
f
Wait for 2 seconds
f
Write a message to the log
For a single person, these tasks can be done with a couple of entries. If you have a small 
known list of entities (persons in this example), then you could copy and paste that group of 
entries, once for each. On the other hand, if the list is long, or you do not know the values in 
advance, there is another way to achieve this. This recipe shows you how.
Getting ready
For this recipe, we will use the Hello transformation described in the introduction. 
The destination folder for the file that is generated is in a variable named  
${OUTPUT_FOLDER} that has to be predefined. Make sure that the folder exists.
You will also need a sample file such as the one described.
Both the sample file and the transformation are available for download.
How to do it...
This recipe is split into three parts. The first part is the development of a transformation that 
generates the list of people:
1. Create a transformation. This transformation will read the list of people and send the 
rows outside the transformation for further processing.
2. Read the sample file with a Text file input step.
3. After reading the file, add a Copy rows to result step. You will find it under the Job
category.
4. Do a preview on this step. You should see the following screen:

Chapter 7
227
5. Save the transformation.
Now, you will create a job that will generate a file for every person in the list; then deliberately 
wait for 2 seconds and write a message to the log.
1. Create a job. Add to the job a START, a Transformation, a Wait for (from the 
Conditions category), and a Write To Log (from the Utility category) entry. Link them 
one after the other in the same order.
2. Double-click on the Transformation entry and configure it to run the Hello
transformation explained earlier.
3. Double-click on the Wait for entry and set the Maximum timeout to 2 seconds.
4. Double-click on the Write To Log entry. For Log subject, type Information and for 
Log message, type A new file has been generated.
5. The final job should look similar to the following diagram:
6. Save the job.
Finally, you will create the main job by carrying out the following steps:
1. Create another job. Add to the job a START, a Transformation, and a Job entry. 
Create a hop from the START to the Transformation entry, and another hop from this 
entry toward the Job entry.
2. Use the first entry to run the transformation that reads the file and copies the rows to 
result.
3. Double-click on the second entry. As Job filename, select the job that generates a file 
for a single person, that is, the job created above.
4. Select the Advanced tab and check the Execute for every input row? and Copy
previous results to args? options.
5. Close the setting window and save the job.

Executing and Reusing Jobs and Transformations
228
6. Run the job. Under the Job metrics tab in the Execution results window, you will see 
something similar to the following:
7. 
If you explore the output directory, you will find one file for every person in the list. The 
dates of the files will differ by 2 seconds one from another.
How it works...
You needed to execute a couple of entries for every person in the list. The first thing you did 
was to create a job (let's call it the subjob from now on) encapsulating the functionality you 
wanted for each person: generate the file, wait for 2 seconds, and write a message to the log.
In order to iterate the execution of the subjob over the list of people, you did the following:
f
You created a transformation that built the list of people and copied the rows to 
result.
f
You created a main job that called that transformation and then executed the subjob 
once for every person in the list. You did this by clicking on the Execute for every
input row? option in the Job entry.
In the screenshot with the job metrics, you can see it: the execution of the transformation in 
the first place, followed by four executions of the subjob. Four is the number of people in our 
example list.
Finally, the Copy previous results to args? option that you checked in the Job entry caused 
the copied rows to become available (one at a time) to the subjob and in particular, to the 
Hello transformation inside the subjob, in the form of command-line arguments.


Executing and Reusing Jobs and Transformations
230
There are four ways or procedures for accessing the fields of the copied row in subsequent 
entries:
Procedure
How to do it
Entries that support this procedure
Copying them to the 
arguments of an entry
Checking the Copy previous 
results to args? option
Transformation, Job, Copy Files, 
Move Files, Delete files, Delete
folders, Zip file, Unzip file, Truncate
tables, Add filenames to result.
Copying them as 
parameters
Checking the Copy previous
results to parameters? option
Transformation, Job
Getting the rows from 
result
Using a Get rows from result
step at the beginning of a 
transformation
Transformation
With JavaScript
Accessing the variable row. 
For example the following 
expression gets the value of the 
field name of the first row:
rows[0]. 
getString("name", "")
JavaScript
In the recipe, you used the first of these options: you checked the Copy previous results to 
args? option and that caused the rows to become available as the command-line arguments 
to the Hello transformation.
In this particular example, you could also have used the last method. In the Hello
transformation, instead of reading the name as the command-line parameter 1, you could have 
used a Get rows from result step obtaining the same results. As implied from the preceding 
table, you don't have to check the Copy previous results to args? option in this case.
Executing a transformation once for every row in a dataset
If instead of a set of entries you just want to execute a single transformation once for every 
row in a dataset, then you don't have to move it to a subjob. Just leave the Transformation
entry in the main job. Double-click on the entry, select the Advanced tab, and check the 
Execute for every input row? option. This will cause the transformation to be executed once 
for every row coming from a previous result.
Executing a transformation or part of a job once for every file 
in a list of files
If you want to execute part of a job (as a sub-job) or a transformation once for every file in a 
list of files, then the procedure is much the same. In the transformation that builds the list, 
use a Get File Names step from the Input category, in order to create the list of files. After 
modifying the dataset with the list of files as needed, add a Copy rows to result step just as 
you did in the recipe. The list of files will be sent outside for further processing.


Executing and Reusing Jobs and Transformations
232
Getting ready
You will need the transformation that generates random numbers described in the 
introduction. If instead of downloading the transformation you created it yourself, then you will 
have to do a quick fix in order to make Kettle save the number of written lines to the log (this 
has already been done in the transformation available on the book's site):
1. Edit the transformation.
2. Press Ctrl-T to bring the transformation's setting window.
3. Select the Logging tab and click on Transformation.
4. In the Fields to log: grid, search for the entry named LINES_OUTPUT. Under Step
name, select the name of the step that generates the file of random numbers. The 
result is shown in the following screenshot:
5. Save the transformation.
How to do it...
Carry out the following steps:
1. Create a job.
2. From the General category, drag START, Set variables, and Transformation entries. 
Create a hop from the START entry toward the Set variables entry, and another from 
this entry toward the Transformation entry.
3. Double-click on the Set variables entry. Add a row in order to define the variable that 
will keep track of the number of lines written. Under Variable name, type total_ 
lines, for Value type 0, and for Variable scope type select Valid in the current job.
4. Configure the Transformation entry to run the transformation that generates the 
random numbers.
5. From the Scripting category, add a JavaScript entry.
6. From the Utility category, drag two Write To Log entries.
7. 
Link the entries as shown in the following diagram:

Chapter 7
233
8. Double-click on the JavaScript entry. In the JavaScript: area, type the following code: 
var total_lines = parseInt(parent_job.getVariable("total_lines")) 
var new_total_lines = total_lines + previous_result.
getNrLinesOutput(); 
parent_job.setVariable("total_lines", new_total_lines);
new_total_lines < 25;
9. Double-click on the Write To Log entry that is executed after the success of the 
JavaScript entry (the entry at the end of the green hop). For Log level, select Minimal
logging. For Log subject, type lines written=${total_lines}. For Log
message type Ready to run again.
10. Double-click on the other Write To Log entry, the one that is executed after the 
failure of the JavaScript entry (the entry at the end of the red hop). For Log level, 
select Minimal logging. For Log subject type ${total_lines} lines have been
written. For Log message, type The generation of random numbers has
succeeded.
11. Save the job.
12. Press F9 to run the job. For Log level, select Minimal logging and click on Launch.
13. In the Logging tab of the Execution results pane, you will see the following: 
2011/01/11 22:43:50 - Spoon - Starting job... 
2011/01/11 22:43:50 - main - Start of job execution 
2011/01/11 22:43:50 - lines written=10 - Ready to run again ... 
2011/01/11 22:43:50 - lines written=20 - Ready to run again ... 
2011/01/11 22:43:51 - 30 lines have been written. - The generation 
of random numbers has been successful.
2011/01/11 22:43:51 - main - Job execution finished 
2011/01/11 22:43:51 - Spoon - Job has ended.
14. In order to confirm that 30 lines have actually been written, open the generated files.
















8
Integrating Kettle and 
the Pentaho Suite
In this chapter, we will cover:
f
Creating a Pentaho report with data coming from PDI
f
Configuring the Pentaho BI Server for running PDI jobs and transformations
f
Executing a PDI transformation as part of a Pentaho process
f
Executing a PDI job from the PUC (Pentaho User Console)
f
Generating files from the PUC with PDI and the CDA (Community Data Access) plugin
f
Populating a CDF (Community Dashboard Framework) dashboard with data coming 
from a PDI transformation
Introduction
Kettle, also known as PDI, is mostly used as a stand-alone application. However, it is not 
an isolated tool, but part of the Pentaho Business Intelligence Suite. As such, it can also 
interact with other components of the suite; for example, as the datasource for a report, or as 
part of a bigger process. This chapter shows you how to run Kettle jobs and transformations in 
that context.
The chapter assumes a basic knowledge of the Pentaho BI platform and the tools that made 
up the Pentaho Suite. If you are not familiar with these tools, it is recommended that you visit 
the wiki page (wiki.pentaho.com) or the Pentaho BI Suite Community Edition (CE) site: 
http://community.pentaho.com/.
As another option, you can get the Pentaho Solutions book (Wiley) by Roland Bouman and Jos 
van Dongen that gives you a good introduction to the whole suite.

Integrating Kettle and the Pentaho Suite
250
A sample transformation
The different recipes in this chapter show you how to run Kettle transformations and 
jobs integrated with several components of the Pentaho BI suite. In order to focus on the 
integration itself rather than on Kettle development, we have created a sample transformation 
named weather.ktr that will be used through the different recipes.
The transformation receives the name of a city as the first parameter from the command line, 
for example Madrid, Spain. Then, it consumes a web service to get the current weather 
conditions and the forecast for the next five days for that city. The transformation has a couple 
of named parameters:
Name
Purpose
Default
TEMP
Scale for the temperature to be returned.
It can be C (Celsius) or F (Farenheit)
C
SPEED
Scale for the wind speed to be returned. It can be Kmph or Miles
Kmph
The following diagram shows what the transformation looks like:
It receives the command-line argument and the named parameters, calls the service, and 
retrieves the information in the desired scales for temperature and wind speed.
You can download the transformation from the book's site and test it. Do a preview on the 
next_days, current_conditions, and current_conditions_normalized steps to see what the 
results look like.
The following is a sample preview of the next_days step:




Integrating Kettle and the Pentaho Suite
254
5. Click on Preview; you will see an empty resultset. The important thing here is that 
the headers should be the same as the output fields of the current_conditions step: 
city, observation_time, weatherDesc, and so on.
6. Now, close that window and click on Edit Parameters.
7. 
You will see two grids: Transformation Parameter and Transformation Arguments. 
Fill in the grids as shown in the following screenshot. You can type the values or 
select them from the available drop-down lists:
8. Close the Pentaho Data Integration Data Source window. You should have the 
following:
The data coming from Kettle is ready to be used in your report.
9. Build the report layout: Drag and drop some fields into the canvas and arrange them 
as you please. Provide a title as well. The following screenshot is a sample report you 
can design:

Chapter 8
255
10. Now, you can do a Print Preview. The sample report above will look like the one 
shown in the following screenshot:
Note that the output of the current_condition step has just one row.



Integrating Kettle and the Pentaho Suite
258
If you intend to run jobs and transformations from a Kettle repository, then make sure you 
have the name of the repository and proper credentials (user and password).
How to do it...
Carry out the following steps:
1. If you intend to run a transformation or a job from a file, skip to the How it works
section.
2. Edit the settings.xml file located in the \biserver-ce\pentaho-solutions\ 
system\kettle folder inside the Pentaho BI Server installation folder.
3. In the repository.type tag, replace the default value files with rdbms. Provide 
the name of your Kettle repository and the user and password, as shown in the 
following example: 
<kettle-repository>
  <!-- The values within <properties> are passed directly to the 
Kettle Pentaho components. -->
       
   <!-- This is the location of the Kettle repositories.xml file, 
leave empty if the default is used: $HOME/.kettle/repositories.xml 
-->
   <repositories.xml.file></repositories.xml.file>
   <repository.type>rdbms</repository.type>
   <!--  The name of the repository to use -->
   <repository.name>pdirepo</repository.name>
   
   <!--  The name of the repository user -->
   <repository.userid>dev</repository.userid>
   
   <!--  The password -->
   <repository.password>1234</repository.password>
      
</kettle-repository>
4. Start the server. It will be ready to run jobs and transformations from your Kettle 
repository.
How it works...
If you want to run Kettle transformations and jobs, then the Pentaho BI server already 
includes the Kettle libraries. The server is ready to run both jobs and transformations from 
files. If you intend to use a repository, then you have to provide the repository settings. In order 
to do this, you just have to edit the settings.xml file, as you did in the recipe.


Integrating Kettle and the Pentaho Suite
260
Getting ready
In order to follow this recipe, you will need a basic understanding of action sequences and 
at least some experience with the Pentaho BI Server and Pentaho Design Studio, the action 
sequences editor.
Before proceeding, make sure that you have a Pentaho BI Server running. You will also need 
Pentaho Design Studio. You can download the latest version from the following URL:
http://sourceforge.net/projects/pentaho/files/Design%20Studio/
Finally, you will need the sample transformation weather.ktr.
How to do it...
This recipe is split into two parts: First, you will create the action sequence, and then you will 
test it from the PUC. So carry out the following steps:
1. Launch Design Studio. If this is your first use, then create the solution project where 
you will save your work.
2. Copy the sample transformation to the solution folder.
3. Create a new action sequence and save it in your solution project with the name 
weather.xaction.
4. Define two inputs that will be used as the parameters for your transformation: city_ 
name and temperature_scale.
5. Add two Prompt/Secure Filter actions and configure them to prompt for the name of 
the city and the temperature scale.
6. Add a new process action by selecting Get Data From | Pentaho Data Integration.
7. 
Now, you will fill in the Input Section of the process action configuration. Give the 
process action a name.
8. For Transformation File, type solution:weather.ktr. For Transformation Step, 
type current_conditions_normalized and for Kettle Logging Level, type or 
select basic.
9. In the Transformation Inputs, add the inputs city_name and temperature_ 
scale.
10. Select the XML source tab.
11. Search for the <action-definition> tag that contains the following line:
      <component-name>KettleComponent</component-name>


Integrating Kettle and the Pentaho Suite
262
19. Finally, in the Process Outputs section of the action sequence, add weather_result
as the only output.
20. Your final action sequence should look like the one shown in the following screenshot:
21. Save the file.
Now, it is time to test the action sequence that you just created.
1. Login to the PUC and refresh the repository, so that the weather.xaction that you 
just created shows up.
2. Browse the solution folders and look for the xaction and double-click on it.
3. Provide a name of a city and change the temperature scale, if you wish.
4. Click on Run; you will see something similar to the following:

Chapter 8
263
5. You can take a look at the Pentaho console to see the log of the transformation 
running behind the scenes.
6. Run the action sequence again. This time, type the name of a fictional city, for 
example, my_invented_city. This time, you will see the following message
Action Successful 
weather_result=No results for the city my_invented_city
How it works...
You can run Kettle transformations as part of an action sequence by using the Pentaho Data
Integration process action located within the Get Data From category of process actions.
The main task of a PDI process action is to run a Kettle transformation. In order to do 
that, it has a list of checks and textboxes where you specify everything you need to run the 
transformation and everything you want to receive back after having run it.
The most important setting in the PDI process action is the name and location of the 
transformation to be executed. In this example, you had a .ktr file in the same location as 
the action sequence, so you simply typed solution: followed by the name of the file.




Chapter 8
267
Getting ready
In order to follow this recipe, you will need a basic understanding of action sequences and 
at least some experience with the Pentaho BI Server and Pentaho Design Studio, the action 
sequences editor.
Before proceeding, make sure you have a Pentaho BI Server running. You will also need 
Pentaho Design Studio; you can download the latest version from the following URL:
http://sourceforge.net/projects/pentaho/files/Design%20Studio/
Besides, you will need a job like the one described in the introduction of the recipe. The job 
should have a named parameter called TMP_FOLDER and simply delete all files with extension 
.tmp found in that folder.
You can develop the job before proceeding (call it delete_files.kjb), or download it from 
the book's site.
Finally, pick a directory on your computer (or create one) with some tmp files for deleting.
How to do it...
This recipe is split into two parts: First, you will create the action sequence and then you will 
test the action sequence from the PUC.
1. Launch Design Studio. If it is the first time you do it, create the solution project where 
you will save your work.
2. Copy the sample job to the solution folder.
3. Create a new action sequence and save it in your solution project with the name 
delete_files.xaction.
4. Define an input that will be used as the parameter for your job: folder. As Default
Value, type the name of the folder with the .tmp files, for example, c:\myfolder.
5. Add a process action by selecting Execute | Pentaho Data Integration Job.
6. Now, you will fill in the Input Section of the process action configuration. Give the 
process action a name.
7. 
As Job File, type solution:delete_files.kjb.
8. In the Job Inputs, add the only input you have: folder.
9. Select the XML source tab.
10. Search for the <action-definition> tag that contains the following line:
      <component-name>KettleComponent</component-name>

Integrating Kettle and the Pentaho Suite
268
11. You will find something similar to the following:
 <action-definition> 
      <component-name>KettleComponent</component-name>
      <action-type>Pentaho Data Integration Job</action-type>
      <action-inputs> 
        <folder type="string"/> 
      </action-inputs>
      <action-resources> 
        <job-file type="resource"/> 
      </action-resources>
      <action-outputs/>
      <component-definition/>
 </action-definition>
12. Replace the <component-definition/> tag with the following:
      <component-definition> 
        <set-parameter> 
          <name>TMP_FOLDER</name>  
          <mapping>folder</mapping> 
        </set-parameter> 
      </component-definition> 
13. Save the file.
Now, it is time to test the action sequence that you just created.
1. Login to the Pentaho BI Server and refresh the repository.
2. Browse the solution folders and look for the delete_files action you just created. 
Double-click on it.
3. You should see a window with the legend Action Successful.
4. You can take a look at the Pentaho console to see the log of the job.
5. Take a look at the folder defined in the input of your action sequence. There should 
be no tmp files.
How it works...
You can run Kettle jobs as part of an action sequence by using the Pentaho Data Integration
Job process action located within the Execute category of process actions.
The main task of a PDI Job process action is to run a Kettle job. In order to do that, it has a 
series of checks and textboxes where you specify everything you need to run the job, and 
everything you want to receive back after having run it.



Chapter 8
271
5. Right-click on the file and select Edit. A new tab window should appear with the CDA 
Editor ready to edit your CDA file, as shown in the following screenshot:
6. The black area is where you will type the CDA file content. Type the skeleton of the file: 
<?xml version="1.0" encoding="UTF-8"?> 
<CDADescriptor> 
   <DataSources>
   </DataSources> 
</CDADescriptor>
7. 
Inside the <DataSources> tag, type the connection to the Kettle transformation: 
<Connection id="weather" type="kettle.TransFromFile"> 
  <KtrFile>weather_np.ktr</KtrFile> 
  <variables datarow-name="CITY"/> 
  <variables datarow-name="Scale" variable-name="TEMP"/> 
</Connection>
8. Now you will define a data access to that datasource. In CDA terminology, this is a 
query over the preceding connection. Below the closing tag </DataSources>, type 
the following: 
<DataAccess access="public"
            cache="true"
            cacheDuration="3600" 
            connection="weather" 
            id="current" 
            type="kettle">
  <Columns/>
  <Parameters> 
    <Parameter default="Lisbon, Portugal"
               name="CITY"
               type="String"/> 
    <Parameter default="C"
               name="Scale"

Integrating Kettle and the Pentaho Suite
272
               type="String"/> 
  </Parameters>
  <Query>current_conditions_normalized</Query>
  <Output indexes="0,1"/> 
</DataAccess>
9. Click on the Save button located at the top of the editor window.
10. Click on Preview and a new window is displayed with the CDA Previewer.
11. In the drop-down list, select the data access that you just defined.
12. Take a look at the Pentaho server console. You should see how the weather_np
transformation is being executed. The following is an excerpt of that log: 
... - weather_np - Dispatching started for transformation 
[weather_np]
... - Get ${TEMP}, ${SPEED} and ${CITY} - Finished processing 
(I=0, O=0, R=1, W=1, U=0, E=0)
... - key & days & format - Finished processing (I=0, O=0, R=1, 
W=1, U=0, E=0)
... - worldweatheronline - Finished processing (I=0, O=0, R=1, 
W=2, U=0, E=0)
... - ... 
... - current_conditions_normalized - Finished processing (I=0, 
O=0, R=11, W=11, U=0, E=0)
13. In the previewer, you will see the results of the CDA query, as shown in the following 
screenshot:





Chapter 8
277
Getting ready
In order to follow this recipe, you will need a minimal experience with the Pentaho BI Server. 
Some experience with the Community Dashboard Editor (CDE) is also desirable.
Before proceeding, make sure you have a Pentaho BI Server running. You will also need the 
CDE. You can download it from http://cde.webdetails.org. To install it, simply unzip 
the downloaded material and follow the instructions in the INSTALL.txt file.
Finally, you will need the sample transformation weather_np.ktr.
How to do it...
Carry out the following steps:
1. Log into the Pentaho User Console.
2. Create the solution folder where you will save your work.
3. Copy the sample transformation to the solution folder and refresh the repository.
4. From the File menu, select New | CDE Dashboard or click on the CDE icon in the 
toolbar.
5. Save the dashboard in the solution folder that you just created, close the dashboard 
window, and refresh the repository. A new file with extension wcdf will appear in the 
solution folder.
6. Go to the solution folder, right-click on the dashboard file and select Edit. The 
dashboard editor will appear. Maximize the window, so that you can work more 
comfortably.
7. 
Define the dashboard layout by adding rows and columns from the layout toolbar, 
until you get the following screen:
Now, let's add the visual elements of the dashboard.
1. Click on Components from the menu at the top-right area of the editor.
2. From the Generic category, add a Simple parameter. Name it city_param and type 
Lisbon, Portugal for Property value.

Integrating Kettle and the Pentaho Suite
278
3. From the Selects category, add a TextInput Component. Name it city_textbox. 
For Parameter, select city_param and for HtmlObject, select filter_panel.
4. Click on Save on the menu at the top of the editor.
5. Click on Preview; you will see the dashboard with a textbox prompting for the city_ 
name parameter, showing the default value Lisbon, Portugal.
6. Close the preview window.
7. 
Now, you will add the chart that will display the forecast. From the Charts category, 
add a CCC Bar Chart.
8. Fill in the properties as follows:

For Name, type forecast_bars

For Width, type 350

For Height, type 250

For Datasource, type forecast

For Crosstab mode, select True

For Title, type 5-days forecast

For HtmlObject, select chart_panel

For Listeners, select city_param
9. Click on the Parameters property and in the window that displays, add one parameter. 
For Arg0, type CITY and for Val0, type city_param, and then Click on Ok.
Finally, you have to create the datasource for that chart: forecast. The following steps will 
do it:
1. Click on Data Sources from the menu at the top-right area of the editor. In the 
list of available data sources, click on KETTLE Queries and select kettle over
kettleTransFromFile. A new datasource will be added.
2. Fill in the list of properties as explained in the following steps:
3. For Name, type forecast.
4. For Kettle Transformation File, type weather_np.ktr.
5. Click on Variables and in the window that displays, click on Add. For Arg0, type CITY
and click on Ok.
6. Click on Parameters and in the window that displays, click on Add.

For Arg0, type CITY

For Val0 type Lisbon, Portugal

For Type0 leave the default String and click on Ok.

Chapter 8
279
7. 
Click on Output Options and in the window that shows up, click on Add three times. 
For Arg0, Arg1, and Arg2, type 1, 2, and 3 respectively and click on Ok.
8. Click on Column Configurations and in the window that displays, click on Add twice. 
In the first row, type 2 for Arg0 and MIN for Val0. In the second row, type 3 for Arg1
and MAX for Val1.
9. Click on the little square to the next of the Query property. The Sql Wizard
shows up. In the editing area, type next_days and click on Ok.
10. Save the dashboard by clicking on Save and click on Preview to see the dashboard. 
You should see a result similar to that shown in the following screenshot:
11. If you take a look at the Pentaho console, then you will see the log of the Kettle 
transformation that is executed.
12. Try changing the value for the city and press Enter. The chart should be refreshed 
accordingly.
How it works...
In this recipe, you created a very simple dashboard. The dashboard allows you to enter the 
name of a city and then refreshes a bar chart displaying the 5-days forecast for that city. The 
special feature of this dashboard is that it gets data from a web service through the execution 
of a Kettle transformation.

Integrating Kettle and the Pentaho Suite
280
In order to use the output of your Kettle transformation as data source, you just have to add 
a new datasource from KETTLE Queries | kettle over kettleTransFromFile and configure it 
properly. This configuration involves providing the following properties:
Property 
Meaning / Purpose
Example
Name
Unique datasource name inside the 
dashboard
forecast
Kettle Transformation
File
Name of the transformation file
weather_np.ktr
Variables
Name of variables that are passed to the 
transformation. You have to provide it in 
pairs (<CDE parameter>,<Kettle named
parameter>) or (<CDE parameter>,"") 
if both coincide.
"CITY",""
Access Level
Public (available from outside) / Private
(available only from other data sources) 
Public
Parameters
Name, default value, and type for 
each parameter to be passed to the 
transformation
"CITY", "Lisbon,
Portugal",
"String"
Output Options (opt)
Indexes of the columns to pick among the 
fields coming out from the transformation
1,2,3
Column Configurations 
(opt)
Renaming the columns coming out from the 
transformation
2,"MIN"
Column Configurations
II (opt)
Calculating new columns based on other 
columns 
AVG, (MAX + MIN)/2
Query
Name of the step that delivers the data
next_days
Cache
Keep results in cache (True/False)
TRUE
Cache Duration
Time to keep the results in cache in seconds 
3600
Once you configured your Kettle transformation as a datasource, it was ready to be used in 
the components of your dashboard.
There's more...
CDF is a community project whose objective is mainly to integrate dashboards in the 
Pentaho's solution repository structure. In the recipe, you used the CDE, which is a graphical 
editor that complements the power of the CDF engine. With CDE, you can create dashboards 
without having to get involved in the low-level details of the CDF files, thus focusing on the 
business requirements.
Kettle is just one of several kinds of data sources accepted by CDF. Behind the scenes, most 
of the data sources definitions are saved in a CDA file.







Chapter 9
287
You can specify a more detailed level of log in the Loglevel listbox if you want.
Then, you need to include this file as an attached file. Double-click on the email job entry, 
select the Attached Files tab, and select the Log item in the Select file type listbox. As you 
need to select both General and Log items, you have to do it by pressing the Ctrl key.
After running the job, an e-mail will still be sent, but this time, the ZIP file attached will also 
contain a new file named logFile.log with the log information.
Sending e-mails in a transformation
There is a transformation step named Mail, in the Utility category, which is very similar to the 
Mail job entry. The main difference is that the Mail step sends one e-mail for each existent row.
Its configuration has listboxes instead of textboxes to refer to different settings (addresses, 
server configuration, subject, body, and so on). Here you should refer to existing fields instead 
of typing them.
In some cases, you could use either the step or the job entry with similar results, but there are 
particular scenarios where one approach is better than the other. For example:
f
If you need to send several e-mails to different destination addresses, and/or with 
different content, and/or with different attached files, then it is better to use the Mail
transformation step.

Getting the Most Out of Kettle
288
f
If you need to send an e-mail with some information about the executed process—for 
example, the time that it took to execute it—you must use the Mail job entry.
f
If you need to send only one e-mail with attached files coming from different jobs or 
transformations, then you should use the Mail job entry.
For an advanced example of using the Mail step for mailing, you can follow this link: http:// 
kjube.blogspot.com/2011/01/mailing-new-years-cards.html. The blog post by 
KJube explains a transformation that sends best wishes for the New Year to a list of people. 
The following two considerations arise about the example provided:
f
The transformation uses the Excel Writer plugin step. If you are working with Kettle 4.1, 
then you should install the plugin in order to open the transformation. That's not the 
case for Kettle 4.2, in which the plugin is already included as an Experimental step.
f
The values for the Server configuration and the Sender name and Sender address
are stored in Kettle variables; they don't change from row to row. However, as 
explained earlier, you cannot type the values directly in the step configuration window. 
You should refer to existing fields instead. Therefore, it is necessary to get those 
variables in a previous step.
Generating a custom log file
When you run a transformation or a job, all of what is happening in the process is shown in 
the Execution Results window, which has a tab named Logging where you can check the 
execution of your transformation step by step. By default, the level of the logging detail is 
Basic, but you can change it to show different levels of detail.
Under the Logging tab, you can see information about how the step is performing, for example, 
the number of rows coming from previous steps, the number of rows read, the number of rows 
written, errors in execution, and so on. All this data is provided by the steps automatically, but 
what if you want to write your custom messages to the Logging information? To do this, there is 
a step and an entry named Write to log, in the Utility folder.
To put them into practice, let's take a simple transformation that reads a text file with book 
novelties and splits them into two Excel files depending on their price. The objective here is to 
include, in the Logging window, custom messages about the incoming number of books and 
also how many of these books are cheap or expensive.
Getting ready
For checking this recipe, you will need a text file that includes information about book titles 
novelties. For example:
title;author_id;price;title_id;genre 
Bag of Bones;A00002;51,99;123-353;Fiction

Chapter 9
289
Basket Case;A00003;31,00;123-506;Fiction
Carrie;A00002;41,00;123-346;Fiction 
Cashflow Quadrant;A00007;55,00;323-604;Business 
Harry Potter and the Half-Blood Prince;A00008;61,00;423-005;Childrens 
Harry Potter and the Prisoner of Azkaban;A00008;29,00;423-
003;Childrens
Power to the People;A00005;33,00;223-302;Non-fiction
Who Took My Money;A00007;21,00;323-603;Business
You can download the sample file from the book's website.
How to do it...
Carry out the following steps:
1. Create a new transformation.
2. Drop a Text file input step into the canvas. Set the file to read under the File tab, and 
type ; as the character Separator under the Content tab. Finally, use the Get Fields 
button under the Fields tab to populate the grid automatically.
Previewing this step, you will obtain the data of the books from the text file. Now, let's add the 
steps for counting the books and writing the information.
1. Add a Group by step from the Statistics folder. Create a hop from the text file to 
this step. In the Aggregates grid at the bottom, add a new field named qty, choose 
a field (for example, title_id) in the Subject column, and select the Number of
Values (N) option in the Type column.
2. Add a User Defined Java Expression (UDJE for short) from the Scripting category 
and link it to the previous step. Create a new field named line of String type with 
the following Java expression:
"Book news = " + Java.lang.Long.toString(qty)
3. From the Utility folder, add a Write to log step and create a hop from the previous 
step towards this one; name it Write books counting. Add the line field to the 
Fields grid. Choose Basic logging in the Log level listbox.
Run the transformation using Basic logging and check the Logging tab for the results.
You can verify the basic logging information where you should see the following line:
2011/01/25 10:40:40 - Write books counting.0 - Book news = 8   

Getting the Most Out of Kettle
290
Now, you will generate two Excel files and write the information about cheap and expensive 
books to the log.
1. Drop one Filter rows, two Excel output, two Group by, two UDJE, two Block this step
until steps finish (from Flow category), and two Write to log steps into the canvas. 
Link the steps, as shown in the following diagram:
2. Create a hop from the Text file input step to the Filter rows step. Here, set the 
condition price ' 50. We will use an arbitrary price of $50 to determine if a book is 
cheap or expensive.
3. Point one Excel Output step filename to cheapBooks.xls, and use the Get Fields
button to populate the grid under the Fields tab.
4. In the Group by step, add a new field named qty in the Aggregates grid, choose the 
field title_id in the Subject column, and select the Number of Values (N) option in 
the Type column.
5. Add a field named line of String type in the UDJE step with the following Java
expression:
"Cheap books = " + Java.lang.Long.toString(qty)
6. In the Block this step until steps finish steps, select the step named Write books
counting in the step name column of the grid.
7. 
Finally, in the Write to log step, add the line field to the Fields grid and choose 
Basic logging in the Log level listbox.
8. Now, repeat the last five steps in order to configure the lower stream. This time use the 
Excel Output step (named Excel Output 2) to generate the expensiveBooks.xls
file and replace the text Cheap for Expensive in the other UDJE step.
Running the transformation using Basic logging, you can verify that your custom messages 
have been written to the log under the Logging tab. Here's an example:


Getting the Most Out of Kettle
292
Filtering the log file
Instead of adding text to the log, you may want to filter text in the existing log.
If you select the Logging tab in the Execution Results window, then you will see a toolbar. 
Under that toolbar, there is a button named Log settings that allows you to apply a filter. If you 
type some text into the Select filter textbox, then the log of the subsequent executions of the 
transformation will only show the lines that contain that text. For example, you could use the 
same text prefix in all of your custom messages, and then apply a filter using this fixed text to 
see only those messages.
This also works if you run the transformation from a job, and even if you restart Spoon 
because the filter is saved in a Kettle configuration file.
This is valid only in Spoon. If you intend to run a job or a transformation with Pan or Kitchen 
and you need to filter a log, for example, by keeping only the lines that contain certain words, 
then you have to take another approach. One way for performing that would be to save the job 
or transformation log in a file and then run another transformation that parses and filters that 
log file.
Creating a clean log file
The Logging window shows not only your messages, but also all of the information written 
by the steps. Sometimes you need a clean log file showing only your custom messages and 
discarding the rest.
There is another problem related to that - when you configure the Write to log step, you 
need to specify a level for your log. (In the recipe, you used Basic logging). If you run 
your transformation using a different log level, then you will not see any of your personalized 
messages.
One alternative would be using a Text file output instead of the Write to log step. With this, you 
will produce a new text file with only your desired messages. Be sure to point all of the Text
file output steps to the same Filename under the File tab, and use the Append checkbox 
under the Content tab, in order to avoid overwriting the file with each run.
Isolating log files for different jobs or transformations
It is possible that you want to see different log levels depending on the job or transformation, 
or that you simply want to isolate the log for a particular job or transformation. This is a simple 
task to accomplish. In the main job, right-click on the Job or Transformation entry of interest; 
under the Logging settings tab check the Specify logfile? option and you will be able to 
specify a name for the log file as well as the log level desired. In this way, you can create 
different log files with different log levels for each of the Jobs and Transformations that are 
part of your main job.


Getting the Most Out of Kettle
294
How to do it...
Carry out the following steps:
1. Create a new transformation.
2. Drop a Text file input step from the Input category. Browse to your file under the 
Files tab, and press the Add button to populate the selected files grid. For example: 
${Internal.Transformation.Filename.Directory}\samplefile.txt.
3. Under the Content tab, uncheck the Header checkbox, and type ##### in the 
Separator textbox.
4. Under the Fields tab, add a field named line of String type.
5. Add a UDJC step from the Scripting category. Also, drop two Dummy steps from the 
Flow category and name them: short sentences step and long sentences step.
6. Create the hops between the steps as per the ones shown in the following diagram:
7. Double-click on the UDJC step.
8. Under the Fields tab of the bottom grid, add a new field named qty. Select Integer in 
the Type listbox.
9. Under the Target steps tab, create two tags and name them: shortOnes and 
longOnes. Then, select the steps as shown in the following screenshot:
10. In the Classes and code fragments section on the left, open the Code Snippets
folder. Expand the Common use option and drop the Main item to the editing area on 
the right. A fragment of Java code will be written for the function processRow().
11. Replace or complete this fragment with the following code: 
private RowSet shortSentences = null; 
private RowSet longSentences = null;
public boolean processRow(StepMetaInterface smi,  
  StepDataInterface sdi) throws KettleException

Chapter 9
295
{
   Object[] r = getRow();
   if (r == null) {
      setOutputDone();
      return false;
   }
   if (first)  {
        first = false;
        shortSentences = findTargetRowSet("shortOnes");
        longSentences = findTargetRowSet("longOnes"); 
   }
   r = createOutputRow(r, data.outputRowMeta.size()); 
   String linein;      
   linein = get(Fields.In, "line").getString(r);
   long len = linein.length();
   long qty = 0;
   boolean currentSpecialChar = false; 
   for (int i=0;i'len;i++) {
      char ch = linein.charAt(i);
      switch(ch) {
         case ',':
         case '.':
         case ' ':
         case ';':
         case ':':
            if (!currentSpecialChar) qty++;
            currentSpecialChar = true;
            break;
         default:
            currentSpecialChar = false;
            break;
      }
   }
   if (!currentSpecialChar) qty++;   
   get(Fields.Out, "qty").setValue(r, qty);
   if (qty ' 7) {         
      putRowTo(data.outputRowMeta, r, shortSentences);     
      }    
   else {        
    putRowTo(data.outputRowMeta, r, longSentences);     
   }   
   return true; 
}


Chapter 9
297
Most of the window is occupied by the editing area. Here you write the Java code using the 
standard syntax of that language. On the left, there is a panel with many code fragments 
ready to use (Code Snippets), and a section with sets and gets for the input and output fields. 
To add a code fragment to your script either double-click on it, drag it to the location in your 
script where you wish to use it, or just type it in the editing area.
The Input and Outputs fields appear automatically in the tree when the Java code compiles 
correctly; you can use the Test class button to verify that the code is properly written. If an 
error occurs, then you will see an error window otherwise, you will be able to preview the 
results of the transformation step.
The Fields tab on the bottom is to declare the new fields added by the step. In this case, you 
declared the field qty of Integer type. This field will be the output variable that will return the 
word count.
Under the Target steps tab, you can declare the steps where the rows will be redirected. In 
the recipe, you pointed to the two target Dummy steps; one for the short sentences, and the 
other for the long ones.
Let's see the Java code in detail:
At the beginning, there is a section with the variable declarations:
private RowSet shortSentences = null; 
private RowSet longSentences = null;

Getting the Most Out of Kettle
298
These variables will represent the two possible target steps. They are declared at the 
beginning because this way they keep their values between the row processing.
Then, you have the main function: 
public boolean processRow(StepMetaInterface smi, StepDataInterface 
sdi) throws KettleException 
{
   Object[] r = getRow();
   if (r == null) {
      setOutputDone();
      return false;
   }
The processRow() function process a new row.  
The getRow() function gets the next row from the input steps. It returns an object array with 
the incoming row. A null value means that there are no more rows for processing.
The following code only executes for the first row: 
if (first)  {
        first = false;
        shortSentences = findTargetRowSet("shortOnes");
        longSentences = findTargetRowSet("longOnes"); 
   }
You can use the flag first to prepare a proper environment before processing the rows. In 
this case, you set the target steps into two variables for further use.
The next code use the get() method to set the internal variable linein with the value of the 
line field.
   r = createOutputRow(r, data.outputRowMeta.size()); 
   String linein;      
   linein = get(Fields.In, "line").getString(r);
Here is the main cycle:
   long len = linein.length();
   long qty = 0;
   boolean currentSpecialChar = false; 
   for (int i=0;i'len;i++) {
      char ch = linein.charAt(i);
      switch(ch) {
         case ',':
         case '.':
         case ' ':


Getting the Most Out of Kettle
300
As mentioned earlier, you can access the Kettle API from inside the UDJC code. To learn the 
details of the API, you should check the source. For instructions on getting the code, follow 
this link: http://community.pentaho.com/getthecode/.
Let's see some more information to take advantage of this very useful step:
Data type's equivalence
The code you type inside the UDJC step is pure Java. Therefore, the fields of your 
transformation will be seen as Java objects according to the following equivalence table:
Data type in Kettle
Java Class
String
Java.lang.String
Integer
Java.lang.Long
Number
Java.lang.Double
Date
Java.util.Date
BigNumer
BigDecimal
Binary
byte[]
The opposite occurs when you create an object inside the Java code and want to expose it as 
a new field to your transformation. For example, in the Java code, you defined the variable qty
as long but under the Fields tab, you defined the new output field as Integer.
Generalizing you code
You can generalize your code by using parameters. You can add parameters and their values 
using the grid located under the Parameters tab at the bottom of the UDJC window.
In our example, you could have defined a parameter named qtyParameter with the value 7. 
Then, in the Java code, you would have obtained this value with the following line of code:
long qty = getParameter("qtyParameter");
Looking up information with additional steps
You can also have additional steps that provide information to be read inside your Java code. 
They are called Info steps. You declare them in the grid located under the Info step tab at the 
bottom of the UDJC window.
In our recipe, suppose that you have the list of separators defined in a Data Grid step. In 
order to pick the separators from that list, you have to create a hop from the Data Grid
towards the UDJC step and fill the Info step grid. You must provide a Tag name (for example, 
charsList) and select the name of the incoming step. Then, in the Java code, you can use 
the findInfoRowSet() method to reference the info step, and the getRowFrom() method 
to read the rows in a cycle. Check the code:


Getting the Most Out of Kettle
302
Generating sample data for testing purposes
Having sample data to test your transformations is very useful and allows you to move faster 
through your development and testing process. There are several cases where you will want to 
generate sample data, for example:
f
To quickly populate datasets with random data
f
Manually generate specific information
f
Generate large volumes of custom data
f
Take a subset from a large volume of data
In this recipe, you will learn how to generate a dataset with 100 random rows in different 
formats (integer, string, and dates). Then, in the There's more section, you will find alternative 
solutions for generating data for testing.
How to do it...
Carry out the following steps:
1. Create a new transformation.
2. Drop a Generate rows step from the Input category. Here, set the Limit textbox to 100.
3. Add a Generate random value step from the Input category. Add two elements to the 
grid: randomInteger of random integer type, and randomString of random
string type.
4. Doing a preview on this last step, you will obtain a list of 100 random strings 
and integers. One example of this values would be: 1925608989 (integer) and 
1jhn0udelvmpe (string)
All the integer values have a large number of digits. What if, for some particular purpose, 
you want an integer with few digits or an integer in a specific range of values? Let's create a 
random integer in another way.
1. Drop a User Defined Java Expression from the Scripting category. Add a new field 
named shortInteger and type the following text in the Java expression column: 
(int)Math.floor(Math.random()*3650). Also, select Integer in the Value
type column.
2. Do a preview and check the new shorInteger field; it will have values between 0
and 3649.
Now, let's do a simple trick to create a random date.


Getting the Most Out of Kettle
304
There's more...
From the Generate random value step, you can also generate the following:
f
Random numbers: Float values between 0 and 1
f
Universally Unique Identifier (UUID): Identifier standard such as b8f395d5-1344-
11e0-af0b-6319f7359ecb
f
Random message authentication code (HMAC): Typically, used in cryptography. For 
example, d59ee7a0e026aa1edc5c3398531d4452
In the following subsections, you will find more ways to generate data for testing in other 
scenarios.
Using Data grid step to generate specific data
The Data grid step allows you to create a dataset including both metadata and values inside 
the transformation. The step has the following two tabs:
f
Meta: Under this tab, you add the fields and define their type and format. You can 
define fields using any of the Kettle data types.
f
Data: Under this tab, you can enter the values for the fields defined previously. 
This step is very useful when you need to populate a short list of rows with particular fields 
and/or values, thus avoiding creating a text file for that purpose.
Let's look at a practical example of the use of the Data grid. Suppose that you have created a 
regular expression and want to be sure that it is properly written. You can populate a data grid 
with a single String field. As values, you can type the list of values against which you want 
to test the regular expression, including values that should match the expression and values 
that shouldn't. After the Data grid, just add a Regexp Evaluation step, enter your regular 
expression, and do a preview on this step. That is a very easy and quick way of testing with the 
help of a Data grid.
The Data grid step was developed by KJube as part of its Kettle Franchising Factory (KFF) 
and contributed towards the standard PDI release. You can read more about KFF at the 
following URL:
http://kff.kjube.be/
Working with subsets of your data
On many occasions, you have to develop transformations that will process huge volumes of 
information. However, working with that volume during the development process is not a good 
idea: it slows down your development process, and makes testing what you are doing difficult. 
It's better to work with a small sample of that information.







Chapter 9
311
In the Get data from XML step, you also included the short filename of the transformation 
because you wanted to identify the source file of the transformations that had those steps.
There's more...
Here you can learn about the most important nodes in the transformation and job XML files.
Transformation XML nodes 
LoopXPath
Information 
/transformation
Root node for each transformation. 
/transformation/info
One node for each transformation file. Here, you have 
the name and description of the transformation, its 
parameters, and the creation dates among others.
/transformation/notepads
From here you can get the notepads. 
/transformation/info/log
You can get information about the transformation logging 
configuration.
/transformation/
connection
One node for each database connection used in the 
transformation (or one for each database connection 
defined, if you didn't check the Only save used
connections to XML? option)
/transformation/step
One node for each step of the transformation. You have 
the entire configuration of the step in this node; the most 
important elements here are the name of the step and its 
type. 
/transformation/step/ 
file
One node for each file or folder pointed to from a step. It 
includes data about the name of the file or folder, the mask 
typed, subfolders, and so on. 
/transformation/step/ 
field
One node for each field declared under the Fields tab of a 
step. You can get all the information about the fields, such 
as the name, its type, or the format configuration.
/transformation/order/ 
hop
One node for each hop. Here you have the links between 
the steps.

Getting the Most Out of Kettle
312
Job XML nodes 
LoopXPath
Information 
/job
One node for each job. From here, you can get the name of 
the job and the parameters among other things.
/job/notepads
From here you can get the notepads.
/job/connection
One node for each database connection used in the job 
(or one for each database connection defined if you didn't 
check the Only save used connections to XML? option)
/job/job-log-table
/job/jobentry-log-table
/job/channel-log-table
These nodes have the job logging configuration.
/job/entries/entry
One node for each job entry. You can look here for the 
name of the entries, their type, and the entire configuration 
fields for the entries.
/job/hops/hop
One entry for each link between job entries.
Steps and entries information
You can check the name and the type of all steps in the Step Information… option from the 
Help menu of Spoon, as shown in the following screenshot:
The ID column represents the steps identification, the Name column is how it appears in 
Spoon, and the Description column is the tool tip. You also can see the category of the step in 
the Category column.
In the recipe, you compared the type field in the transformation against the ID column 
in this list.
The Job Entry Information… option shows similar information, but for the job entries.



Chapter 9
315
Transformation tables
Table
Information
R_TRANSFORMATION
Here we have the basic information for the transformation, 
such as the transformation name or its creation date.
R_STEP
One record for each step. You can get the name of the step 
and the identification of its type.
R_STEP_TYPE
The list of step types. Here you can get the name, the 
description, and the tooltip for each kind of step.
R_TRANS_HOP
One record for each hop. It provides the information related 
to the link between steps.
R_STEP_ATTRIBUTE
Settings for the step. Each feature you define in a step is 
saved in the columns CODE and VALUE_NUM if the feature is 
a number, or CODE and VALUE_STR otherwise. For example, 
if the step represents the name of a file, then you will have 
CODE=filename and VALUE_STR=c:/files/sample. 
xls
R_TRANS_NOTE
Here are the notes from the transformation. There is a note 
identification field linked to the table R_NOTE, where you 
have the description of the note.

Getting the Most Out of Kettle
316
Job tables
Table
Information
R_JOB
Basic information on the job. For example, its name or creation 
date.
R_JOBENTRY
One record for each job entry. Here you get the name and the 
type of the job entries.
R_JOBENTRY_TYPE
The list of the job entries (identification, name, and description)
R_JOB_HOP
One record for each hop. With this information, you know how 
the job entries are linked.
R_JOBENTRY_ATTRIBUTE
Settings for the step. Each feature you define in a step is saved 
in the columns CODE and VALUE_NUM if the feature is a 
number, or CODE, and VALUE_STR otherwise. See R_STEP_ 
ATTRIBUTE in the previous table for an example.
R_JOB_NOTE
Here are the notes from the job. There is a note identification 
field linked to the table R_NOTE, where you have the 
description of the note.

Chapter 9
317
Database connections tables
Table
Information
R_DATABASE
Here we have the settings for the database connections used in 
the steps or entries
R_STEP_DATABASE
One record for each step that uses a database connection. It 
links the step with the database identification.
R_JOBENTRY_DATABASE
One record for each entry job that uses a database connection. 
It links the entry job with the database identification.
R_DATABASE_TYPE
List of database engines. For example, MYSQL, ORACLE, DB2, 
MSSQL, among others.
R_DATABASE_CONTYPE
Type of access to the database: Native, ODBC, OCI, Plugin, 
JNDI.
R_DATABASE_ATTRIBUTE
Here, you have the settings for each database connection. In 
the CODE column, you have the name of the variable whereas in 
the VALUE_STR column, you have the value.





Index
Symbols
${Internal.Transformation.Filename.Directory} 
variable  56
${KTR_NAME} variable  225 
${OUTPUT_FOLDER} variable  214, 218, 223 
<Parameter> tag  275
A
action sequence  259, 268
Add sequence step
avoiding  210 
Ad Hoc Reporting user interface  256 
alternative notation
for separator  54 
ALTER TABLE statement  36
arguments
values, supplying for  265 
Argument tab  219
attached files
e-mails, sending with  284-286 
authors.txt file  52
B
Books data structure  319, 320
Business Intelligence solutions  5
C
CAG  195
Calculator step  303 
Cartesian product
performing, between dataset  194, 196 
CDA  276 
CDA DataAccess  274
CDA plugin
about  270 
files, generating with  270-275
CDA previewer  272 
CDF  197, 276, 280
CDF dashboard
about  276 
populating, with data from PDI  
transformation  277-280 
visual elements, adding  277
cell
searching, in Excel file  85 
cell value
retrieving, from Excel file  82-84 
changed flag  192 
Check Db connection job entry  11 
clean log file
creating  292 
code
generalizing, parameters used  300 
Combination lookup/update step  26 
Comma Separated Values. See  CSV 
commercial databases  10 
Community Acronym Generator. See  CAG 
Community Chart Framework (CCF)  196
Community Dashboard Editor (CDE)  277 
Community Dashboard Framework. See  CDF 
compare stream  191 
complex conditions, issues
overcoming  182, 183 
complex XML structures 
generating  114-120 
compressed files  144
connection
creating, to database  7, 8 
connection settings, for database  7

324
copied row
accessing, from jobs  229, 230 
accessing, from transformations  229, 230
Copy Files job  126, 127 
copy/get rows mechanism  239, 240
Copy rows to result step  229, 231
countries.xml file  34 
CREATE INDEX statement  36 
CREATE TABLE statement  34, 36
cron  269 
CSV  55, 70 
CSV file  161
current_conditions_normalized step
preview  251 
current_conditions step
preview  251 
currentSpecialChar flag  299
custom list of files
copying  137, 138 
deleting  139-141 
moving  137, 138
custom log file
generating  288-291
D
Damerau-Levenshtein algorithm  168 
dashboard  276 
data
deleting, from table  30-33 
de-serializing  240, 241 
generating, Data grid step used  304 
retrieving, from database  11, 12 
retrieving, from different path  100 
retrieving, selectively  101 
retrieving, with parameters from  
database  13-16 
retrieving, with query  16, 17 
searching, by consuming web  
services  169-172 
searching, by proximity  165-168 
searching, in database  154-156 
searching, in database table  150, 151 
searching, in database with extreme  
flexibility  158-160 
searching, in resources  160-164 
searching, over internet  173, 174
searching, over intranet  173, 174 
serializing  240, 241
database
about  5 
advanced connection properties,  
specifying  10 
altering, PDI used  34, 35 
connecting to  7, 8 
connection, creating to  7, 8 
creating, from PDI  37, 38 
creating, PDI used  34, 35 
data, retrieving from  11, 12 
data, searching in  154-156 
same database connection, avoiding  9, 10
database connection
about  8 
modifying, at runtime  43, 45 
verifying, at runtime  11
Database connections option  7 
Database Explorer window  12 
Database join step  
about  155, 157 
advantages  157
Database lookup step  151, 157 
database storage methods  5 
database table
data, searching in  150, 151 
Data grid step
about  304 
data, generating with  304 
history  304
data lookup
issues  152, 153 
dataset
about  175 
Cartesian product, performing  
between  194, 196 
data source
defining, for report  253, 254 
data types  54 
data type’s equivalence  300 
data warehouses  5 
DBMS  6 
Default target step  182 
deleted files
figuring out  130, 131 
deleted flag  192

325
Delete File job  130 
Delete filenames from result job entry  286
delete operation  43 
DELETE operation  33 
delta_value sequence  29 
desc_product fieldname  71 
detail files  62 
Detect empty stream step  205, 207 
dialog
exploring, for UDJC step  296, 297 
dimension table  34 
disk-storage based databases  10 
Document Type Definition. See  DTD 
definitions 
doQuery feature  275 
Double Metaphone algorithm  169
DTD
limitations  108 
DTD definitions
about  106 
XML file, validating against  106-108
Dummy steps
avoiding  180 
Dynamic SQL row step  157-160
E
E4X
about  97 
URL  97
ECMAScript. See  E4X 
email job entry  284 
e-mails
benefits  283 
logs, sending through  286, 287 
sending, in transformation  287, 288 
sending, with attached files  284-286
Enable safe mode option  189
encoding  54 
Excel file
about  161 
cell, searching  85 
cells value, retrieving in  82-84 
labels, horizontally arranged  85 
reading  80, 81 
writing, with dynamic sheets  89-91 
writing, with multiple sheets  86-88
Excel Writer plugin step  288 
Execute a transformation window  214 
Execute for every input row? option  229
Extensible Stylesheet Language. See  XSL 
transformation
F
fields
comparing, against another field  180 
comparing, against constant value  180 
filename, using as  77-79 
generating, with XML structures  113, 114 
specifying, XPath notation used  97-100
file-based jobs
information, retrieving of  309, 310 
file-based transformations
information, retrieving of  309, 310 
File Compare job entry  143 
file existence
detecting  127 
file format  54 
file list transformation  216 
File management category  143 
File Management category  130 
filename
using, as field  77-79 
files
about  51 
accessing, with FTPS  134 
accessing, with SFTP  134 
comparing, with folders  141, 142 
copying  126, 127 
creating  217 
deleted file, figuring out  130, 131 
deleting  129, 130 
Excel file, reading  80, 81 
existence, detecting for  127 
generating, from PUC with CDA  
plugin  270-275 
generating, from PUC with PDI  270-275 
moving  126, 127 
multiple files, reading at once  56, 57 
name, providing to  74-76 
placing, on remote server  135, 136 
reading, with fixed width fields  55 
reading, with one field by row  64-66

326
retrieving, from remote server  132, 133 
simple file, reading  52, 53 
simple file, writing  69, 70 
simple XML files, reading  94-96 
specifying, to transfer  133, 134 
unstructured files, reading  57-62 
unstructured file, writing  71-73
files, reading
with fields, occupying two or more rows  66-68 
filesToAttach folder  284 
files, unzipping
avoiding  148 
files, zipping
avoiding  148 
file transfer
information, retrieving for  135 
File Transfer Protocol. See  FTP 
Filter rows step  176, 179, 180, 181
findInfoRowSet() method  300 
fixed width fields
files, reading with  55 
Flow category  176 
folders
comparing  143 
creating  128 
files, comapring with  141, 142
Foodmart  6 
format
providing, to output fields  71 
FTP  131 
FTPS
about  134 
files, accessing with  134
FTP server
connections considerations  134 
FULL OUTER join  200 
fuzzy match algorithm  167 
Fuzzy match step  
about  167 
algorithms  168, 169
G
Generate random value step  304 
Generate rows step  303 
get() method  298
getRowFrom() method  300
getRow() function  298 
Get rows from result step  239 
Get SQL select statement... button  12 
Get System Info step  77 
Get Variable step  197 
Get XML Data step  94, 95 
Group by step  291
H
H2  164 
headers
modifying  71 
hello transformation  214, 215 
hexadecimal notation  54 
hibernate database  6 
HMAC  304 
HOST_NAME variable  10 
HTML  93, 252 
HTML page
generating, XML transformation 
used  121-123
generating, XSL transformation 
used  121-123
HTTP Client step  174 
Hypersonic (HSQLDB)  6, 164
I
id_author field  27
identical flag  192
identifiers  15
Infobright  10 
information
retrieving, about file-based jobs  309, 310 
retrieving, about file-based  
tranformations  309, 310 
retrieving, about repository-based  
jobs  313, 314 
retrieving, about repository-based  
tranformations  313, 314 
retrieving, for file transfer  135
Informix  10 
Info steps  300 
in-memory databases  10 
INNER join  200 
insert operation  43 
INSERT statement  20, 35

327
internet
data, searching over  173, 174 
intranet
data, searching over  173, 174
J
Janino library  253 
jar file  10 
Jaro algorithm  168 
Jaro-Winkler algorithm  168 
Java
URL, for tutorials  299 
JavaScript step
executions, controlling  235 
JDBC  252 
JNDI data source  9 
job, executing
by setting parameters  217-219 
by setting parameters dynamically  220, 221 
by setting static arguments  217-219 
by setting static arguments  
dynamically  220, 221 
job name, determining at runtime  223-225
job part
executing, for every row in dataset  225-228 
executing, until true condition  231-234
jobs
copied row, accessing from  229, 230 
executing, by setting parameters  217-219 
executing, by setting static  
arguments  217-219 
job part, executing for every row  
in dataset  225-228 
launching  216 
log files, isolating for  292 
loops, implementing in  235
job XML nodes  312 
Join Rows (Cartesian product) step  197
joins options
about  200 
FULL OUTER join  200 
INNER join  200 
LEFT OUTER join  200 
RIGHT OUTER join  200
Json
about  305 
URL, for info  307
Json files
example  305 
reading, dynamically  307 
working with  306, 307 
writing  307, 308
Json input step   306 
junk dimension tables  25
K
Kettle
about  6, 8, 214, 249 
custom functionality, programming  293-299 
custom log file, generating  288-291 
e-mails, sending with attached files  284-286 
folders, comparing with files  141, 142 
jobs, launching  216 
process flow, creating  237-240 
rows, merging of two streams  184-187 
sample data, generating for testing  
purpose  302, 303 
transformations, launching  216 
unsupported database connection  10
KettleComponent inputs  265 
Kettle Franchising Factory (KFF)  304 
keywords  15 
Kitchen documentation
URL  217 
kjb file  10
KJube  288, 304 
ktr file  10, 225
L
last row
identifying, in stream  210 
LEFT OUTER join  200 
Levenshtein algorithm  168 
libext/JDBC directory  10
lib folder  257 
location
specifying, of transformation  265 
log files
about  63 
clean log file, creating  292 
filtering  292 
isolating, for different jobs  292 
isolating, for transformations  292

328
logs
customizing  301 
sending, through e-mail  286, 287
loops
implementing, in jobs  235 
Loop XPath textbox  95
M
Mail job entry  287 
Mail validator job entry  286 
master files  62 
Merge Join step  198 
Merge Rows (diff) step  191, 192
Metaphone algorithm  169 
MJSV  301 
modern column-oriented databases  10 
Modified Java Script Value. See  MJSV 
Modified Java Script Value steps  313
Mondrian cubes  276 
Mondrian distribution  6 
MS SQL Server  10 
multiple files
copying  126, 127 
deleting  129, 130 
generating, with different name  77 
generating, with similar structure  77 
moving  126, 127 
reading, at once  56, 57
multiple nodes
retrieving  102 
multiple sheets
Excel file, writing with  86-88 
Museums data structure  321 
MySQL  6
N
name
providing, to files  74-76 
named parameter  
about  17, 45, 196, 215 
values, supplying for  265
Native (JDBC)  9
Needleman-Wunsch algorithm  168 
nested Filter Rows steps
avoiding  181, 182 
new flag  192
new rows
interspersing, between existent rows  201-204 
next_days step
preview  250
O
offices.txt file  24 
OLAP  252 
OpenOffice calc files  81 
open source databases  10 
Oracle  10 
Oracle OCI connection  9
ORDER BY clause  18 
Outdoor data structure  321, 322 
outdoorProducts.txt file  177
output fields
format, providing to  71 
output row number
limiting  197, 198 
outputType parameter  276
P
Pair letters Similarity algorithm  168 
parameters
code, generalizing with  300
data, retrieving with  13-16
Parameters tab  219 
param + <name of param.> parameter  276 
parent-child table
about  46 
loading  47, 48
parent_job.getVariable() function  234 
parent_job.setVariable() function  234 
PDF  252 
PDI
about  39, 51, 93, 249 
complex XML structures, generating  114-120 
database, altering with  34, 35 
database, creating from  34-38 
fields, specifying with XPath notation  97-100 
files, generating with  270-275 
simple XML document, generating  112, 113 
simple XML files, reading  94-96 
URL, for wiki page  249 
well-formed XML files, validating  103-105

329
XML file, validating against DTD 
definitions  106-108
XML file, validating against XSD 
schema   108-111
PDI data
Pentaho report, creating from  253-256 
PDI job
executing, from PUC  267, 268 
PDI transformations
executing, as part of Pentaho  
process  259-264
PDI transformations data
CDF dashboard, populating with  277-280 
Pentaho BI platform  249 
Pentaho BI platform databases
about  6, 7 
hibernate  6 
quartz  6 
sampledata  6
Pentaho BI Platform Demo  6
Pentaho BI Server
about  257 
configuring, for running PDI jobs  257, 258 
configuring, for running PDI  
transformations  257, 258 
Pentaho BI Suite Community Edition (CE)  249
Pentaho Business Intelligence Suite  249
Pentaho Data Integration Job process  
action  268 
Pentaho Data Integration process action  263 
Pentaho Design Studio  260, 267 
Pentaho developer  7 
Pentaho report
creating, with data from PDI  253-256 
Pentaho Reporting Engine
about  252 
working  256, 257
Pentaho Server log  63 
Pentaho User Console. See  PUC 
phonetic algorithms  168 
PostgreSQL  10 
predicate  101 
previous_result element  236 
previous_result.getNrLinesOutput() function  
234 
primary key
generating  23-29
process flow
about  236 
creating  237-240
processRow() function  294, 298
PUC
about  259, 266, 270 
files, generating with CDA plugin  270-275 
files, generating with PDI  270-275 
PDI job, executing from  267, 268
Put a file with FTP job entry  136
Q
quartz database  6 
query
data, retrieving with  16, 17
R
random list transformation  215 
R_DATABASE_ATTRIBUTE table  317
R_DATABASE_CONTYPE table  317
R_DATABASE table  317
R_DATABASE_TYPE table  317
RDBMS  6 
records
inserting, in tables  21 
reference stream  191 
Refined SoundEx algorithm  168
remote server
files, placing on  135, 136 
files, retrieving from  132, 133
report
data source, defining for  253, 254 
reporting  266 
repositories  5 
repository-based jobs
information, retrieving of  313, 314 
repository-based transformations
information, retrieving of  313, 314 
Reservoir Sampling step  305 
resources
data, searching in  160-164 
result filelist feature  138, 231
RIGHT OUTER join  200 
R_JOBENTRY_ATTRIBUTE table  316
R_JOBENTRY_DATABASE table  317
R_JOBENTRY table  316

330
R_JOBENTRY_TYPE table  316
R_JOB_HOP table  316
R_JOB_NOTE table  316
R_JOB table  316 
Row flattener step  66 
rows
inserting, during simple primary key  
generation  23-29
inserting, in table  18-22 
merging, of streams  188, 189 
merging, of two streams with different  
structure  184-187 
merging, of two streams with similar  
structure  184-187 
processing, based on row number  208-210 
updating, in table  18-22
R_STEP_ATTRIBUTE table  315
R_STEP_DATABASE table  317
R_STEP table  315 
R_STEP_TYPE table  315 
R_TRANSFORMATION table  315 
R_TRANS_HOP table  315 
R_TRANS_NOTE table  315 
Ruby Scripting plugin  301 
runtime
database connection, modifying at  43, 45
S
sample data
generating, for testing purpose  302, 303 
sample databases  6 
sampledata database  6, 13 
sampleFiles directory  126 
sample transformations
about  214, 250 
creating  214 
file list transformation  216 
hello transformation  214, 215 
preview  250, 251 
random list transformation  215 
sequence transformation  215, 216
Secure Sockets Layer (SSL)  134 
SELECT * statement  12 
SELECT statement  
about  12, 13, 159 
executing, multiple times  16
separator
alternative notation for  54 
sequence transformation  215, 216 
serialize/de-serialize mechanism  240 
Set files in result step  231
settings.xml file  258 
SFTP
about  134 
files, accessing with  134
shared.xml file  10
SimMetrics  169
simple file
reading  52, 53 
writing  69, 70
simple XML document
generating  112, 113 
simple XML files
reading  94-96 
SMTP server  286 
SoundEx algorithm  168 
specific rows
identifying  210 
Spoon  7 
SSH File Transfer Protocol. See  SFTP 
stand-alone application  249 
Steel Wheels structure  150, 322 
steps
executing, empty stream condition  204-207 
Stream Lookup step  
about  163 
alternatives  164
streams
comparing  189-192 
joining, on given conditions  198-200 
last row, identifying in  210 
rows, merging of  188, 189 
splitting, based on condition  176-179
streams, merging
similar metadata condition  188 
subtransformation
about  242 
creating  242 
transformation part, moving to  242-248
Switch / Case step  181 
Synchronize after merge step  42, 43

331
T
table
data, deleting from  30-33 
deleting  39-43 
inserting  39-43 
records, inserting in  21 
rows, inserting in  18-21 
rows, updating in  18-21 
updating  39-43
Task Scheduler  269 
Text file input step  52 
tmp extension  139 
tokens  101 
total_lines variable  234 
traditional row-oriented databases  10 
transformation part
moving, to subtransformation  242-248 
transformations
about  175, 213 
copied row, accessing from  229, 230 
e-mails, sending in  287, 288 
executing, by setting parameters  217-219 
executing, by setting static  
arguments  217-219 
executing, for every row in dataset  230 
launching  216 
location, specifying of  265 
log files, isolating for  292 
values, supplying for arguments  265 
values, supplying for named parameters  265 
values, supplying for variables  265
transformations, executing
by setting parameters  217-219 
by setting parameters dynamically  220, 221 
by setting static arguments  217-219 
by setting static arguments  
dynamically  220, 221 
transformation name, determining  
at runtime  223-225 
transformation XML nodes  311 
Transport Layer Security (TLS)  134 
txt extension  126, 127
U
UDJC  293
UDJC step
benefits  293 
dialog, exploring for  296, 297 
scripting alternatives  301
UDJE  75, 301 
UDJE step  150, 182, 291 
Universally Unique Identifier. See  UUID 
unstructured file
writing  71-73 
unstructured files 
writing  57-62 
update opeartion  43 
UPDATE statement  21 
User Defined Java Class. See  UDJC 
User Defined Java Expression. See  UDJE 
UUID  304
V
Value Mapper step
using  165
values 
supplying, for arguments  265 
supplying, for named parameters  265 
supplying, for variables  265
variables
retrieving, in middle of stream  196, 197 
using, in database connection definition  10 
values, supplying for  265
W
W3C
URL, for XML  93 
Webdetails  276, 281 
web service consumption
avoiding  252 
Web service lookup step  169 
web services  169 
Web Services Description Language.  
See  WSDL 
well-formed XML files
validating  103-105 
WHERE clause  33 
Write to log step  291
WSDL  171

332
X
xls extension  126, 127 
XML  93, 252 
XML, as field  96 
XML file
validating, against DTD definitions  106-108 
validating, against XSD schema  108-111
XML file name, as field  96 
XML Schema Definition. See  XSD schema 
XML structures
fields, generating with  113, 114 
XML transformation
HTML page, generating with  121-123
XPath  100 
XPath notation
fields, specifying with  97-100 
multiple nodes, retrieving  102 
working  100
XSD schema
XML file, validating against  108-111 
XSL transformation
HTML page, generating with  121-123
Z
ZIP files
working with  145-147


NMF
Tensor
Vector
Matrix
Real
Poisson
2023
EDITION
 
License by
Bayesian
Matrix Decomposition
and Applications           
Jun Lu
 
ID
[I]
...
P
R


Bayesian Matrix Decomposition and Applications
Jun Lu
jun.lu.locky@gmail.com
© 2023
Jun Lu
Version 0.1

JUN LU
iii

Preface
In 1954, Alston S. Householder published Principles of Numerical Analysis, one of the ﬁrst
modern treatments on matrix decomposition that favored a (block) LU decomposition-the
factorization of a matrix into the product of lower and upper triangular matrices. And now,
matrix decomposition has become a core technology in machine learning, largely due to the
development of the back propagation algorithm in ﬁtting a neural network, and its way of
reducing the dimensionality of the data and representing it in a way that is easier for the
machine learning algorithms to process. Bayesian matrix decomposition is a relatively new
ﬁeld within the broader area of matrix decomposition and machine learning. The concept
of Bayesian matrix decomposition is rooted in Bayesian statistics in which case it combines
the principles of Bayesian statistics with matrix decomposition methods to perform matrix
factorization. The use of Bayesian methods in matrix decomposition was ﬁrst introduced in
the early 2000s, with the aim of addressing the limitations of traditional matrix factorization
techniques, e.g., limited explanatory and predictive performance.
The sole aim of this book is to give a self-contained introduction to concepts and math-
ematical tools in Bayesian matrix decomposition in order to seamlessly introduce matrix
decomposition techniques and their applications in subsequent sections. However, we clearly
realize our inability to cover all the useful and interesting results concerning Bayesian matrix
decomposition and given the paucity of scope to present this discussion, e.g., the separated
analysis of variational inference for conducting the optimization. We refer the reader to
literature in the ﬁeld of Bayesian analysis for a more detailed introduction to the related
ﬁelds.
This book is primarily a summary of purpose, signiﬁcance of important Bayesian matrix
decomposition methods, e.g., real-valued decomposition, nonnegative matrix factorization,
Bayesian interpolative decomposition, and the origin and complexity of the methods which
shed light on their applications. The mathematical prerequisite is a ﬁrst course in statistics
and linear algebra. Other than this modest background, the development is self-contained,
with rigorous proof provided throughout.
iv

JUN LU
v

Keywords
Bayesian inference, Gibbs sampling, Conjugate model, Alternating least squares (ALS),
Multiplicative update, Real-valued matrix decomposition, Nonnegative matrix decomposi-
tion, Interpolative decomposition, Ordinal matrix decomposition, Poisson matrix decompo-
sition.
Acknowledgment:
We would like to express gratitude towards Ulrich Paquet for providing
information on Bayesian ordinal matrix decomposition and towards Gilbert Strang for dis-
cussing the proof of the CUR decomposition. Additionally, we would like to thank Federico
Poloni for his comment on the proof of alternating least squares. The author also wishes
to acknowledge the cooperation of Joerg Osterrieder, Christine P. Chai, and Xuanyu Ye
towards the Bayesian approach for nonnegative matrix factorization and interpolative de-
composition. Their cooperation has helped enlighten the form of many discussions in the
book.
vi

JUN LU
vii

Contents
I
Backgrounds
2
Introduction and Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1
Monte Carlo Methods
14
1.1
The Bayesian Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
1.2
Approximate Inference
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
1.3
Monte Carlo (MC) Methods
. . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.3.1
Markov Chain Monte Carlo (MCMC) . . . . . . . . . . . . . . . . .
18
1.3.2
MC V.S. MCMC . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
1.3.3
Gibbs Sampler
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.3.4
Adaptive Rejection Sampling (ARS)
. . . . . . . . . . . . . . . . .
21
1.4
Bayesian Appetizers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.4.1
Beta-Bernoulli Model . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.4.2
Bayesian Linear Model with Zero-Mean Prior . . . . . . . . . . . .
25
1.4.3
Bayesian Linear Model with Semi-Conjugate Prior
. . . . . . . . .
27
1.4.4
Bayesian Linear Model with Full Conjugate Prior . . . . . . . . . .
29
2
Regular Probability Models and Conjugacy
32
2.1
Conjugate Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.2
Regular Univariate Models and Conjugacy
. . . . . . . . . . . . . . . . . .
33
2.3
Exponential and Conjugacy . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
2.4
Univariate Gaussian-Related Models . . . . . . . . . . . . . . . . . . . . . .
50
2.5
Multinomial Distribution and Conjugacy . . . . . . . . . . . . . . . . . . .
59
2.5.1
Dirichlet Distribution . . . . . . . . . . . . . . . . . . . . . . . . . .
60
2.5.2
Posterior Distribution for Multinomial Distribution . . . . . . . . .
62
2.6
Poisson and Multinomial . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
2.7
Multivariate Gaussian Distribution and Conjugacy . . . . . . . . . . . . . .
66
2.7.1
Multivariate Gaussian Distribution . . . . . . . . . . . . . . . . . .
67
2.7.2
Multivariate Student’s t Distribution . . . . . . . . . . . . . . . . .
70
2.7.3
Prior on Parameters of Multivariate Gaussian Distribution . . . . .
71
viii

JUN LU
2.7.4
Posterior Distribution of µ: Separated View . . . . . . . . . . . . .
75
2.7.5
Posterior Distribution of Σ: Separated View . . . . . . . . . . . . .
76
2.7.6
Gibbs Sampling of the Mean and Covariance: Separated View . . .
77
2.7.7
Posterior Distribution of µ and Σ Under NIW: Uniﬁed View
. . .
78
2.7.8
Posterior Marginal Likelihood of Parameters . . . . . . . . . . . . .
81
2.7.9
Posterior Marginal Likelihood of Data
. . . . . . . . . . . . . . . .
82
2.7.10
Posterior Predictive for Data without Observations . . . . . . . . .
82
2.7.11
Posterior Predictive for New Data with Observations . . . . . . . .
83
2.7.12
Further Optimization via the Cholesky Decomposition . . . . . . .
84
II
Non-Bayesian Matrix Decomposition
86
3
Alternating Least Squares
88
3.1
Preliminary: Least Squares Approximations
. . . . . . . . . . . . . . . . .
89
3.2
Netﬂix Recommender and Matrix Factorization
. . . . . . . . . . . . . . .
92
3.3
Regularization: Extension to General Matrices . . . . . . . . . . . . . . . .
96
3.4
Missing Entries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
3.5
Vector Inner Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
3.6
Gradient Descent (GD) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100
3.7
Regularization: A Geometrical Interpretation . . . . . . . . . . . . . . . . .
104
3.8
Stochastic Gradient Descent (SGD) . . . . . . . . . . . . . . . . . . . . . .
106
3.9
Bias Term
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
3.10
Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
3.10.1
Low-Rank Approximation . . . . . . . . . . . . . . . . . . . . . . .
111
3.10.2
Movie Recommender . . . . . . . . . . . . . . . . . . . . . . . . . .
113
4
Nonnegative Matrix Factorization (NMF)
118
4.1
Nonnegative Matrix Factorization . . . . . . . . . . . . . . . . . . . . . . .
119
4.2
NMF via Multiplicative Update (MU) . . . . . . . . . . . . . . . . . . . . .
119
4.3
Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
4.4
Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
4.5
Movie Recommender Context . . . . . . . . . . . . . . . . . . . . . . . . . .
123
III
Bayesian Matrix Decomposition
124
5
Bayesian Real Matrix Factorization
126
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
127
5.2
All Gaussian (GGG) Model and Markov Blanket . . . . . . . . . . . . . . .
129
5.3
All Gaussian Model with ARD Hierarchical Prior (GGGA) . . . . . . . . .
136
5.4
All Gaussian Model with Wishart Hierarchical Prior (GGGW) . . . . . . .
138
5.5
Gaussian Likelihood with Volume and Gaussian Priors (GVG) . . . . . . .
139
ix

6
Bayesian Nonnegative Matrix Factorization
142
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
6.2
Gaussian Likelihood with Exponential Priors (GEE) . . . . . . . . . . . . .
143
6.3
Gaussian Likelihood with Exponential Priors and ARD Hierarchical Prior
(GEEA) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
6.4
Gaussian Likelihood with Truncated-Normal Priors (GTT) . . . . . . . . .
150
6.5
Gaussian Likelihood with Truncated-Normal and Hierarchical Priors (GTTN)152
6.6
Gaussian Likelihood with Rectiﬁed-Normal Priors (GRR) and Hierarchical
Prior (GRRN) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
6.6.1
Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159
6.7
Priors as Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162
6.8
Gaussian L2
1 Norm (GL2
1) Model . . . . . . . . . . . . . . . . . . . . . . . .
163
6.9
Gaussian L2
2 Norm (GL2
2) and Gaussian L∞Norm (GL∞) Models . . . . .
166
6.9.1
Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
170
6.10
Semi-Nonnegative Matrix Factorization . . . . . . . . . . . . . . . . . . . .
175
6.10.1
Gaussian Likelihood with Exponential and Gaussian Priors (GEG)
176
6.10.2
Gaussian Likelihood with Nonnegative Volume and Gaussian Priors
(GnVG) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
176
6.11
Nonnegative Matrix Tri-Factorization (NMTF) . . . . . . . . . . . . . . . .
177
7
Bayesian Poisson Matrix Factorization
182
7.1
Poisson Likelihood with Gamma Priors (PAA) . . . . . . . . . . . . . . . .
183
7.2
Poisson Likelihood with Gamma Priors and Hierarchical Gamma Priors
(PAAA) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
7.3
Properties of PAA or PAAA . . . . . . . . . . . . . . . . . . . . . . . . . .
186
7.4
Recommendation Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
8
Bayesian Ordinal Matrix Factorization
190
8.1
Ordinal Likelihood with Gaussian Prior and Wishart Hierarchical Prior
(OGGW) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
8.1.1
Ordinal Regression Likelihood . . . . . . . . . . . . . . . . . . . . .
191
8.1.2
Matrix Factorization Modeling on Latent Variables . . . . . . . . .
193
8.1.3
Gibbs Sampler
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
8.2
Properties of OGGW
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
195
9
Bayesian Interpolative Decomposition
198
9.1
Interpolative Decomposition (ID)
. . . . . . . . . . . . . . . . . . . . . . .
199
9.2
Existence of the Column Interpolative Decomposition . . . . . . . . . . . .
201
9.3
Skeleton/CUR Decomposition
. . . . . . . . . . . . . . . . . . . . . . . . .
205
9.4
Row ID and Two-Sided ID . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
9.5
Bayesian Low-Rank Interpolative Decomposition . . . . . . . . . . . . . . .
208
9.6
Gibbs Sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
212
9.7
Aggressive Update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
214
9.8
Post-Processing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
9.9
Bayesian ID with Automatic Relevance Determination . . . . . . . . . . . .
215
x

JUN LU
9.10
Examples for Bayesian ID . . . . . . . . . . . . . . . . . . . . . . . . . . . .
217
9.10.1
Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . .
219
9.10.2
Convergence and Comparative Analysis
. . . . . . . . . . . . . . .
219
9.11
Bayesian Intervened Interpolative Decomposition (IID) . . . . . . . . . . .
220
9.11.1
Quantitative Problem Statement
. . . . . . . . . . . . . . . . . . .
220
9.11.2
Examples for Bayesian IID . . . . . . . . . . . . . . . . . . . . . . .
222
IV
Appendix
228
A Appendix
230
A.1
Taylor’s Expansion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231
A.2
Deriving the Dirichlet distribution . . . . . . . . . . . . . . . . . . . . . . .
232
A.2.1
Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
A.2.2
Properties of Dirichlet distribution . . . . . . . . . . . . . . . . . .
233
xi

List of Figures
1.1
Rejection sampling. Figure from Michael I. Jordan’s lecture notes. . . . . .
21
1.2
Adaptive rejection sampling. Figure from Michael I. Jordan’s lecture notes.
22
1.3
Beta probability density functions for diﬀerent values of the parameters a and
b. When a = b = 1, the beta distribution reduces to a uniform distribution in
the support of [0, 1]. The mean, variance, and mode of the Beta distribution
are E[x] =
a
a+b, Var[x] =
ab
(a+b+1)(a+b)2 , and mode[x] =
a−1
(a−1)+(b−1) if a >
1, b > 1, respectively.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
1.4
Prior distribution is Beta(x | 2, 2). The posterior distributions for the three
cases in Example 1.4.1 are Beta(x | 12, 2), Beta(x | 50, 4), and Beta(x |
188, 16), respectively. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2.1
Gaussian probability density functions for diﬀerent values of the mean and
variance parameters µ and σ2. . . . . . . . . . . . . . . . . . . . . . . . . . .
34
2.2
Student’s t distribution for diﬀerent values of the parameters ν and σ2.
. .
36
2.3
Gamma and inverse-Gamma probability density functions for diﬀerent values
of the parameters r and λ. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.4
Normal-inverse-Gamma probability density functions by varying diﬀerent pa-
rameters.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.5
Chi-squared and inverse-Chi-squared probability density functions for diﬀer-
ent values of parameters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
2.6
Exponential probability density functions for diﬀerent values of the rate pa-
rameter λ. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
2.7
Truncated-normal and general-truncated-normal probability density func-
tions for diﬀerent values of the parameters µ and τ.
. . . . . . . . . . . . .
52
2.8
Mean of truncated-normal and general-truncated-normal distribution by vary-
ing µ, τ, a, and b parameters.
. . . . . . . . . . . . . . . . . . . . . . . . . .
52
xii

JUN LU
LIST OF FIGURES
2.9
Half-normal and normal probability density functions for diﬀerent values of
the parameters µ and τ. The probability density of any value x ≥µ in the
half-normal distribution is twice as that in the normal distribution with the
same parameters µ, τ.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
2.10 Truncated-normal and rectiﬁed-normal probability density functions for dif-
ferent values of the parameters µ, τ, and λ. . . . . . . . . . . . . . . . . . .
56
2.11 Inverse-Gaussian probability density functions for diﬀerent values of the pa-
rameters µ, λ. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
2.12 Laplace and skew-Laplace probability density functions for diﬀerent values
of the parameters.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
2.13 Binomial probability mass functions for diﬀerent values of the parameters N, π. 61
2.14 Density plots (blue=low, red=high) for the Dirichlet distribution over the
probability simplex in R3 for various values of the concentration parameter
α. When α = [c, c, c], the distribution is called a symmetric Dirichlet dis-
tribution and the density is symmetric about the uniform probability mass
function (i.e., occurs in the middle of the simplex). When 0 < c < 1, there
are sharp peaks of density almost at the vertices of the simplex. When c > 1,
the density becomes monomodal and concentrated in the center of the sim-
plex. And when c = 1, it is uniform distributed over the simplex. Finally, if
α is not a constant vector, the density is not symmetric. . . . . . . . . . . .
63
2.15 Draw of 5, 000 points from Dirichlet distribution over the probability simplex
in R3 for various values of the concentration parameter α. . . . . . . . . . .
64
2.16 Poisson probability mass functions for diﬀerent values of the parameter λ. .
66
2.17 Density and contour plots (blue=low, yellow=high) for the multivariate Gaus-
sian distribution over the R2 space for various values of the covariance/scale
matrix with zero mean vector.
Fig 2.17(a) and 2.17(d): A spherical co-
variance matrix has a circular shape; Fig 2.17(b) and 2.17(e): A diagonal
covariance matrix is an axis aligned ellipse; Fig 2.17(c) and 2.17(f): A full
covariance matrix has an elliptical shape.
. . . . . . . . . . . . . . . . . . .
68
2.18 Density and contour plots (blue=low, yellow=high) for the multivariate Gaus-
sian distribution and multivariate Student’s t distribution over the R2 space
for various values of the covariance/scale matrix with zero mean vector.
Fig 2.18(a): A spherical covariance matrix has a circular shape; Fig 2.18(b):
A diagonal covariance matrix is an axis aligned ellipse; Fig 2.18(c): A full
covariance matrix has a elliptical shape; Fig 2.18(d) to Fig 2.18(f) for Stu-
dent’s t distribution with the same scale matrix and increasing ν such that
the diﬀerence between (a) and (f) in Fig 2.18(i) is approaching zero. . . . .
72
3.1
Three functions.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90
3.2
Figure 3.2(a) shows a function “density” and a contour plot (blue=low, yel-
low=high) where the upper graph is the “density”, and the lower one is the
projection of it (i.e., contour). Figure 3.2(b): −∇L(z) pushes the loss to
decrease for the convex function L(z). . . . . . . . . . . . . . . . . . . . . .
104
xiii

LIST OF FIGURES
3.3
Constrained gradient descent with z⊤z ≤C.
The green vector w is the
projection of v1 into z⊤z ≤C where v1 is the component of −∇l(z) perpen-
dicular to z1. The right picture is the next step after the update in the left
picture. z⋆denotes the optimal solution of {min l(z)}. . . . . . . . . . . . .
105
3.4
Unit ball of Lp norm in 2-dimensional space. The Lp norm over a vector x
is deﬁned as Lp(x) = (P
i |xi|p)1/p. When p < 1, the metric is not a norm
since it does not meet the triangle inequality property of a norm deﬁnition.
106
3.5
Unit ball of Lp norm in 3-dimensional space. When p < 1, the metric is
not a norm since it does not meet the triangle inequality property of a norm
deﬁnition. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
3.6
Constrained gradient descent with ∥z∥1 ≤C, where the red dot denotes the
breakpoint in L1 norm. The right picture is the next step after the update
in the left picture. z⋆denotes the optimal solution of {min l(z)}. . . . . . .
107
3.7
Bias terms in alternating least squares where the yellow entries denote ones
(which are ﬁxed) and cyan entries denote the added features to ﬁt the bias
terms. The dotted boxes give an example of how the bias terms work. . . .
109
3.8
A gray ﬂag image to be compressed. The size of the image is 600×1200 with
a rank of 402.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
3.9
Image compression for gray ﬂag image into a rank-5 matrix via the SVD, and
decompose into 5 parts where σ1 ≥σ2 ≥. . . ≥σ5, i.e., F1 ≤F2 ≤. . . ≤F5
with Fi =
σiuiv⊤−A

F for i ∈{1, 2, . . . , 5}. And reconstruct images by
single singular value and its corresponding left and right singular vectors,
cir⊤
i , wiz⊤
i
respectively. Upper: SVD; Middle: Pseudoskeleton; Lower:
ALS. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
3.10 Comparison of reconstruction errors measured by Frobenius norm among the
SVD, pseudoskeleton, and ALS where the approximated rank ranges from 3
to 100. ALS with well-selected parameters works similarly to SVD. . . . . .
113
3.11 Image compression for gray ﬂag image with diﬀerent ranks. . . . . . . . . .
114
3.12 Comparison of training and validation error for “MovieLens 100K” data set
with diﬀerent reduction dimensions and regularization parameters. . . . . .
115
3.13 Distribution of the insample and outsample under cosine and Pearson simi-
larity and the Precision-Recall curve of them. . . . . . . . . . . . . . . . . .
116
5.1
Graphical model representation of GGG model. Green circles denote prior
variables, orange circles represent observed and latent variables, and plates
represent repeated variables. The slash “/” in the variable represents “or”.
130
5.2
The Markov blanket of a directed acyclic graphical (DAG) model.
In a
Bayesian network, the Markov blanket of node A includes its parents, chil-
dren, and the other parents of all of its children. That is, the nodes in
the cycle are in the Markov blanket of node A. The ﬁgure is due to wikipedia
page of Markov blanket. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
5.3
Graphical model representation of GGGA and GGGW models. Green circles
denote prior variables, orange circles represent observed and latent variables,
and plates represent repeated variables. The slash “/” in the variable repre-
sents “or”, and the comma “,” in the variable represents “and”. . . . . . . .
136
xiv

JUN LU
LIST OF FIGURES
5.4
Graphical representation of GVG model. Green circles denote prior variables,
orange circles represent observed and latent variables, and plates represent
repeated variables. The slash “/” in the variable represents “or”. . . . . . .
140
6.1
Graphical model representation of GEE and GEEA models. Green circles
denote prior variables, orange circles represent observed and latent variables,
and plates represent repeated variables. The slash “/” in the variable repre-
sents “or”. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
144
6.2
Graphical model representation of GTT and GTTN models. Green circles
denote prior variables, orange circles represent observed and latent variables,
and plates represent repeated variables. The slash “/” in the variable repre-
sents “or”, and the comma “,” in the variable represents “and”. . . . . . . .
150
6.3
Graphical model representation of GTTN with same and diﬀerent hyper-
parameters.
Green circles denote prior variables, orange circles represent
observed and latent variables, and plates represent repeated variables. The
slash “/” in the variable represents “or”, and the comma “,” in the variable
represents “and”. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
6.4
Graphical representation of GRR and GRRN models. Green circles denote
prior variables, orange circles represent observed and latent variables, and
plates represent repeated variables. The slash “/” in the variable represents
“or”, and the comma “,” in the variable represents “and”. . . . . . . . . . .
155
6.5
Data distribution of MovieLens 100K and MovieLens 1M data sets.
The
MovieLens 1M data set has a larger fraction of users who give a rate of 5 and
a smaller fraction for rates of 3. . . . . . . . . . . . . . . . . . . . . . . . . .
159
6.6
Convergence of the models on the MovieLens 100K (upper) and the Movie-
Lens 1M (lower) data sets, measuring the training data ﬁt (mean squared er-
ror). When increasing latent dimension K, the GRRN continues to increase
the performance; while other models start to decrease on the MovieLens 100K
data set or stop increasing on the MovieLens 1M data set. . . . . . . . . . .
160
6.7
Ratio of the variance of data to the MSE of the predictions. The higher
the better. GRRN model performs similarly to other Bayesian NMF models.
Similar results can be found on the MovieLens 1M data set and other K
values and we shall not repeat the details. . . . . . . . . . . . . . . . . . . .
161
6.8
Predictive results on the MovieLens 100K (upper) and MovieLens 1M (lower)
data sets with the least fractions of unobserved data being 0.928 and 0.953
respectively (see Table 6.2 for the data description). We measure the predic-
tive performance (mean squared error) on a held-out data set for diﬀerent
fractions of unobserved data. The blue and red arrows compare the MSEs
of GTTN and GRRN models when the fractions of unobserved data are 0.96
and 0.98 respectively.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162
6.9
Graphical model representation of GL2
1, GL2
2, GL∞, and GL2
2,∞models.
Green circles denote prior variables, orange circles represent observed and
latent variables, and plates represent repeated variables. The slash “/” in
the variable represents “or”. . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
6.10 Data distribution of GDSC IC50 and Gene Body Methylation data sets. . .
171
xv

LIST OF FIGURES
6.11 Convergence of the models on the GDSC IC50 (upper) and the distribution
of factored W (lower), measuring the training data ﬁt (mean squared error).
When we increase the latent dimension K, the GEE and the introduced GL2
2
and GL2
2,∞algorithms continue to increase the performance; while GL2
1 starts
to decrease. The results of GL∞and GL2
2,∞models are similar so we only
present the results of the GL2
2,∞model for brevity. . . . . . . . . . . . . . .
172
6.12 Convergence of the models on the Gene Body Methylation data set (upper)
and the distribution of factored W (lower), measuring the training data ﬁt
(mean squared error). When we increase the latent dimension K, all the
models continue to increase the performance. . . . . . . . . . . . . . . . . .
173
6.13 Predictive results on the GDSC IC50 (upper) and Gene Body Methy-
lation (lower) data sets.
We measure the predictive performance (mean
squared error) on a held-out data set for diﬀerent fractions of unobserved
data.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
175
6.14 Ratio of the variance of data to the MSE of the predictions, the higher the
better. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
176
6.15 Graphical model representation of GEG and GnVG models. Green circles
denote prior variables, orange circles represent observed and latent variables,
and plates represent repeated variables. The slash “/” in the variable repre-
sents “or”. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
6.16 Graphical model representation of GEEE and GEEEA models. Green circles
denote prior variables, orange circles represent observed and latent variables,
and plates represent repeated variables. The slash “/” in the variable repre-
sents “or”, and the comma “,” in the variable represents “and”. . . . . . . .
178
7.1
Graphical model representations of PAA and PAAA models. Green circles
denote prior variables, orange circles represent observed and latent variables,
and plates represent repeated variables.
. . . . . . . . . . . . . . . . . . . .
184
7.2
Gamma probability density functions G(α, β) by reducing the shape param-
eter α.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
187
7.3
User activity and item popularity for the MovieLens 1M data set (see data
description in Table 6.2, p. 159).
. . . . . . . . . . . . . . . . . . . . . . . .
188
8.1
Graphical representation of the ordinal regression model.
. . . . . . . . . .
192
8.2
Ordinal probability of the ordinal regression model in Equation (8.3).
. . .
192
8.3
Graphical representation of OGGW model. Green circles denote prior vari-
ables, orange circles represent observed and latent variables, and plates rep-
resent repeated variables. The slash “/” in the variable represents “or”, and
the comma “,” in the variable represents “and”. . . . . . . . . . . . . . . . .
194
9.1
Demonstration of the column ID of a matrix where the yellow vector denotes
the linearly independent columns of A, white entries denote zero, and purple
entries denote one. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
200
9.2
Demonstration of the skeleton decomposition of a matrix where the yellow
vectors denote the linearly independent columns of A, and green vectors
denote the linearly independent rows of A.
. . . . . . . . . . . . . . . . . .
205
xvi

JUN LU
LIST OF FIGURES
9.3
Demonstration of the interpolative decomposition of a matrix where the yel-
low vector denotes the basis columns of matrix A, white entries denote zero,
purple entries denote one, blue and black entries denote elements that are not
necessarily zero. The Bayesian ID models ﬁnd the approximation A ≈XY
and the post-processing procedure calculates the approximation A ≈CW .
210
9.4
Graphical representation of GBT and GBTN models. Orange circles repre-
sent observed and latent variables, green circles denote prior variables, and
plates represent repeated variables. The slash “/” in the variable represents
“or”, and the comma “,” in the variable represents “and”. Parameters a and
b are ﬁxed with a = −1 and b = 1 in our case; while a weaker construction
can set them to a = −2, b = 2. . . . . . . . . . . . . . . . . . . . . . . . . . .
210
9.5
Data distribution of CCLE EC50, CCLE IC50, Gene Body Methylation,
and Promoter Methylation data sets. . . . . . . . . . . . . . . . . . . . . . .
216
9.6
Convergence results (upper), sampling mixing analysis (middle), and re-
constructive results (lower) on the CCLE EC50, CCLE IC50, Gene Body
Methylation, and Promoter Methylation data sets for various latent dimen-
sions.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
218
9.7
Convergence results (upper), and sampling mixing analysis (lower) on the
SH510050, SH510300, SH601939, SH601628, and SH601328 data sets for a
latent dimension of K = 10.
. . . . . . . . . . . . . . . . . . . . . . . . . .
224
9.8
Portfolio values of the ten assets (left), and the portfolio values (right) of
diﬀerent methods where we split by in-sample and out-of-sample periods,
and initialize with a unitary value for each period. The IID performs better
in the out-of-sample period (see also Table 9.5). . . . . . . . . . . . . . . . .
224
xvii

Notation
This section provides a concise reference describing notation used throughout this book. If
you are unfamiliar with any of the corresponding mathematical concepts, the book describes
most of these ideas in Chapter I (p. 4).
Numbers and Arrays
a
A scalar (integer or real)
a
A vector
A
A matrix
A
A tensor
In
Identity matrix with n rows and n columns
I
Identity matrix with dimensionality implied by context
ei
Standard basis vector [0, . . . , 0, 1, 0, . . . , 0] with a 1 at position i
diag(a)
A square, diagonal matrix with diagonal entries given by a
a
A scalar random variable
a
A vector-valued random variable
A
A matrix-valued random variable
xviii

JUN LU
LIST OF FIGURES
Sets
A
A set
∅
The null set
R
The set of real numbers
N
The set of natural numbers
C
The set of complex numbers
{0, 1}
The set containing 0 and 1
{0, 1, . . . , n}
The set of all integers between 0 and n
[a, b]
The real interval including a and b
(a, b]
The real interval excluding a but including b
A\B
Set subtraction, i.e., the set containing the elements of A that
are not in B
Indexing
ai
Element i of vector a, with indexing starting at 1
a−i
All elements of vector a except for element i
Aij, Aij, aij
Element i, j of matrix A
Ai,:
Row i of matrix A
A:,i
Column i of matrix A
Aijk, Aijk, aijk
Element (i, j, k) of a 3-D tensor A
A:,:,i
2-D slice of a 3-D tensor A
xix

LIST OF FIGURES
Linear Algebra Operations
A⊤
Transpose of matrix A
A+
Moore-Penrose pseudoinverse of A
A ⊛B
Element-wise (Hadamard) product of A and B
det(A)
Determinant of A
rref(A)
Reduced row echelon form of A
C(A)
Column space of A
N(A)
Null space of A
V
A general subspace
rank(A)
Rank of A
tr(A)
Trace of A
Calculus
dy
dx
Derivative of y with respect to x
∂y
∂x
Partial derivative of y with respect to x
∇xy
Gradient of y with respect to x
∇Xy
Matrix derivatives of y with respect to X
∇Xy
Tensor containing derivatives of y with respect to X
∂f
∂x
Jacobian matrix J ∈Rm×n of f : Rn →Rm
∇2
xf(x) or H(f)(x)
The Hessian matrix of f at input point x
Z
f(x)dx
Deﬁnite integral over the entire domain of x
Z
S
f(x)dx
Deﬁnite integral with respect to x over the set S
xx

JUN LU
LIST OF FIGURES
Probability and Information Theory
a⊥b
The random variables a and b are independent
a⊥b | c
They are conditionally independent given c
P(a)
A probability distribution over a discrete variable
p(a)
A probability distribution over a continuous variable, or over a
variable whose type has not been speciﬁed
a ∼P
Random variable a has distribution P
Ex∼P [f(x)] or E[f(x)]
Expectation of f(x) with respect to P(x)
Var[f(x)]
Variance of f(x) under P(x)
Cov[f(x), g(x)]
Covariance of f(x) and g(x) under P(x)
H(x)
Shannon entropy of the random variable x
DKL(P∥Q)
Kullback-Leibler divergence of P and Q
N(x|µ, Σ)
Gaussian distribution over x with mean µ and covariance Σ
Functions
f : A →B
The function f with domain A and range B
f ◦g
Composition of the functions f and g
f(x; θ)
A function of x parametrized by θ. (Sometimes we write f(x)
and omit the argument θ to lighten notation)
log(x)
Natural logarithm of x
σ(x)
Logistic sigmoid, i.e.,
1
1 + exp(−x)
ζ(x)
Softplus, log(1 + exp(x))
∥x∥p
Lp norm of x
∥x∥= ∥x∥2
L2 norm of x
x+
Positive part of x, i.e., max(0, x)
u(x)
Step function with value 1 when x ≥0 and value 0 otherwise
1{condition}
is 1 if the condition is true, 0 otherwise
Sometimes we use a function f whose argument is a scalar but apply it to a vector,
matrix, or tensor: f(x), f(X), or f(X). This denotes the application of f to the array
element-wise. For example, if C = σ(X), then Ci,j,k = σ(Xi,j,k) for all valid values of i, j,
and k.
xxi

LIST OF FIGURES
Abbreviations
MCMC
Markov chain Monte Carlo
i.i.d.
Independently and identically distributed
p.d.f.
Probability density function
p.m.f.
Probability mass function
OLS
Ordinary least squares
NG
Normal-Gamma distribution
NIG
Normal-inverse-Gamma distribution
NIX
Normal-inverse-Chi-squared distribution
TN
Truncated-normal distribution
GTN
General-truncated-normal distribution
RN
Rectiﬁed-normal distribution
IW
Inverse-Wishart distribution
NIW
Normal-inverse-Wishart distribution
ALS
Alternating least squares
GD
Gradient descent
SGD
Stochastic gradient descent
MU
Multiplicative update
MSE
Mean squared error
NMF
Nonnegative matrix factorization
ID
Interpolative decomposition
IID
Intervened interpolative decomposition
BID
Bayesian interpolative decomposition
xxii

JUN LU
LIST OF FIGURES
xxiii

JUN LU
LIST OF FIGURES
1

Part I
Backgrounds
2


Introduction and Background
Matrix decomposition has become a core technology in statistics (Banerjee and Roy, 2014;
Gentle, 1998), optimization (Gill et al., 2021), clustering and classiﬁcation (Li et al., 2009;
Wang et al., 2013; Lu, 2021c), computer vision (Goel et al., 2020), and recommender sys-
tem (Symeonidis and Zioupos, 2016), largely due to the development of its application in
machine learning (Goodfellow et al., 2016; Bishop, 2006). Machine learning algorithms are
designed to learn hidden patterns and relationships in data, but the data can often be
high-dimensional and complex. Matrix decomposition techniques provide a way of reducing
the dimensionality of the data and representing it in a way that is easier for the machine
learning algorithms to process.
Matrix decomposition algorithms such as QR decomposition, singular value decomposi-
tion (SVD), and nonnegative matrix factorization (NMF) provide a way of breaking down
a matrix into a smaller set of constituent matrices that represent the underlying structure
of the data. These decomposed matrices can be used as features for the machine learning
algorithms, which can then learn the patterns and relationships in the data more eﬀectively.
This process can also help reduce the noise and redundancy in the data, making it easier
for the machine learning algorithms to identify the important patterns and relationships.
On the other hand, matrix decomposition can be applied to a variety of matrix-based
problems, such as collaborative ﬁltering and link prediction (Marlin, 2003; Lim and Teh,
2007; Mnih and Salakhutdinov, 2007; Raiko et al., 2007; Chen et al., 2009):
• In the context of collaborative ﬁltering, matrix decomposition can be used to uncover
hidden patterns in user-item matrices. For example, given a matrix of user ratings for
diﬀerent items, matrix decomposition algorithms can be used to infer the latent user
preferences and item attributes. These latent variables can then be used to predict
the ratings for unseen items so as to suggest items to users (e.g., articles, movies, or
musics).
• In link prediction problems, matrix decomposition algorithms can be used to uncover
hidden patterns in networks. For example, given a network of users and their con-
nections, matrix decomposition methods can be used to infer latent user preferences,
which can then be used to predict new links in the network.
A (bilinear) matrix decomposition is a way of reducing a complex matrix into its con-
stituent parts which are in simpler forms and represent the original matrix as a product of
two (or more) factor matrices. The underlying principle of the decompositional approach
to matrix computation is that it is not the business of the matrix algorithmists to solve par-
ticular problems, but it is an approach that can simplify more complex matrix operations
which can be performed on the decomposed parts rather than on the original matrix itself.
For example, a matrix decomposition, which though is usually expensive to compute, can
be reused to solve new problems involving the original matrix in diﬀerent scenarios, e.g., as
long as the factorization of A is obtained, it can be reused to solve the set of linear systems
{b1 = Ax1, b2 = Ax2, . . . , bk = Axk}.
There are two main approaches that have been applied to inference in the (low-rank)
matrix factorization task.
The ﬁrst is to deﬁne a loss function and optimize over the
factored components using alternating updates (Comon et al., 2009; Lee and Seung, 1999).
The second is to build a probabilistic model representing the matrix factorization and then
4

JUN LU
to perform statistical inference to compute any desired components (Salakhutdinov and
Mnih, 2008; Arı et al., 2012).
Another core application of matrix decomposition in machine learning is that it provides
a way of incorporating prior knowledge into the machine learning algorithms. For example,
in Bayesian matrix decomposition (BMD, or Bayesian matrix factorization, BMF), prior
information about sparsity or nonnegativity can be encoded into the model, allowing the
algorithms to make more informed decisions and produce better results.
Bayesian matrix decomposition is a probabilistic model that decomposes a matrix into
its constituent parts. It was initially discussed in a factor analysis context (Canny, 2004;
Dunson and Herring, 2005) and in a matrix completion context (Zhou et al., 2010). BMD is a
generative graphical model that can be used to infer the low-dimensional latent factors from
a matrix, such as user preferences and item attributes. It has been successfully applied to a
variety of problems such as matrix completion, inpainting, denoising, and super-resolution.
The model is based on a Bayesian framework and inference is mainly optimized using
Markov chain Monte Carlo (MCMC) methods.
Given a data matrix A, the bilinear matrix decomposition considers factoring it as
A = W Z. By Bayesian approaches, the minimization problem of ﬁnding factored matrices
W , Z is described by probabilistic approaches as trying to infer the distributions over
latent variables W , Z after observing the data matrix A. Bayesian approaches extend this
by placing prior distributions over latent variables W , Z in which case we can either try
to infer a point estimate of a maximum likelihood estimator by maximizing the likelihood
maxW ,Z p(A | W , Z); or of a maximum a posteriori estimators by maxW ,Z p(W , Z | A);
or ﬁnd the full posterior distribution p(W , Z | A).
Bayesian matrix decomposition approaches use a likelihood distribution to capture noise
in the data, e.g., use a Poisson likelihood for count data, or a Gaussian likelihood for real-
valued or nonnegative data. Based on the constraints, e.g., nonnegativity, count, sparsity,
or ordinal values, diﬀerent priors are placed over the entries in W , Z. In non-probabilistic
matrix decomposition language, the likelihood can be regarded as a cost function (e.g., to
minimize over the mean squared error), and the priors can be treated as a penalization
term over the factored components (e.g., to favor a sparsity in W , Z). Situated within this
book, our goal is to expand the repertoire of Bayesian matrix factorization algorithms.
In numerical matrix decomposition methods (Lu, 2021b), a matrix decomposition task
on matrix A can be cast as,
• A = QU: where Q is an orthogonal matrix that contains the same column space as
A and U is a relatively simple and sparse matrix to reconstruct A.
• A = QT Q⊤: where Q is orthogonal such that A and T are similar matrices that
share the same properties such as same eigenvalues and sparsity. Moreover, working
on T is an easier task compared to that on A.
• A = UT V : where U, V are orthogonal matrices such that the columns of U and the
rows of V constitute an orthonormal basis of the column space and row space of A
respectively.
•
A
m×n = B
m×r
C
r×n: where B, C are full rank matrices that can reduce the memory
storage of A. In practice, a low-rank approximation
A
m×n ≈D
m×k
F
k×n can be em-
ployed where k < r is called the numerical rank of the matrix such that the matrix
5

can be stored much more inexpensively and can be multiplied rapidly with vectors
or other matrices. An approximation of the form A = DF is useful for storing the
matrix A more frugally (we can store D and F using k(m + n) ﬂoats, as opposed to
mn numbers for storing A), for eﬃciently computing a matrix-vector product b = Ax
(via c = F x and b = Dc), for data interpretation, and much more.
However, in Bayesian matrix decomposition, we consider simpler factorization forms, e.g.,
the (low-rank) real-valued factorization, nonnegative factorization, the factorization for
count and ordinal data sets, and the Bayesian interpolative decomposition (ID).
The sole aim of this book is to give a self-contained introduction to concepts and math-
ematical tools in Bayesian inference and matrix analysis in order to seamlessly introduce
matrix decomposition (or factorization) techniques and their applications in subsequent sec-
tions. However, we clearly realize our inability to cover all the useful and interesting results
concerning Bayesian matrix decomposition and given the paucity of scope to present this
discussion, e.g., the separated analysis of the high-order Bayesian decomposition, nonpara-
metric matrix factorization, and variational inference for Bayesian matrix decomposition.
We refer the reader to literature in the ﬁeld of Bayesian analysis for a more detailed intro-
duction to the related ﬁelds. Some excellent examples include Rai et al. (2015); Qian et al.
(2016); Lu (2021b); Takayama et al. (2022).
Notation and preliminaries.
In the rest of this section, we will introduce and recap
some basic knowledge about linear algebra related to matrix factorization. For the rest
of the important concepts, we deﬁne and discuss them as per need for clarity. Readers
with enough background in matrix analysis can skip this section. In the text, we simplify
matters by considering only matrices that are real.
Without special consideration, we
assume throughout that ∥·∥= ∥·∥2, i.e., the norm is an L2 norm or a Frobenius norm for
vectors or matrices.
In all cases, scalars will be denoted in a non-bold font possibly with subscripts (e.g., a, α,
αi). We will use boldface lower case letters possibly with subscripts to denote vectors (e.g.,
µ, x, xn, z) and boldface upper case letters possibly with subscripts to denote matrices
(e.g., A, Lj). The i-th element of a vector z will be denoted by zi in the non-bold font. In
the meantime, the normal fonts of scalars denote random variables (e.g., a and b1 are
random variables, while italics a and b1 are scalars); the normal fonts of boldface lower
case letters possibly with subscripts denote random vectors (e.g., a and b1 are random
vectors, while italics a and b1 are vectors); and the normal fonts of boldface upper case
letters possibly with subscripts denote random matrices (e.g., A and B1 are random
matrices, while italics A and B1 are matrices).
The n-th element in a sequence is denoted by a superscript (in parentheses), e.g., An or
A(n) denotes the n-th matrix in a sequence, ak or a(k) denotes the k-th vector in a sequence.
Subarrays are formed when a subset of the indices is ﬁxed. The i-th row and j-th column
value of matrix A (entry (i, j) of A) will be denoted by aij. Furthermore, it will be helpful
to utilize the Matlab-style notation, the i-th row to the j-th row and the k-th column
to the m-th column submatrix of the matrix A will be denoted by Ai:j,k:m. A colon is used
to indicate all elements of a dimension, e.g., A:,k:m denotes the k-th column to the m-th
column of the matrix A, and A:,k denotes the k-th column of A. Alternatively, the k-th
column of A may be denoted more compactly by ak.
6

JUN LU
When the index is not continuous, given ordered subindex sets I and J, A[I, J] denotes
the submatrix of A obtained by extracting the rows and columns of A indexed by I and J,
respectively; and A[:, J] denotes the submatrix of A obtained by extracting the columns of
A indexed by J where the [:, J] syntax in this expression selects all rows from A and only
the columns speciﬁed by the indices in J.
Deﬁnition 0.0.1 (Matlab Notation) Suppose A ∈Rm×n, and I = [i1, i2, . . . , ik] and
J = [j1, j2, . . . , jl] are two index vectors, then A[I, J] denotes the k × l submatrix
A[I, J] =


ai1,j1
ai1,j2
. . .
ai1,jl
ai2,j1
ai2,j2
. . .
ai2,jl
...
...
...
...
aik,j1
aik,j2
. . .
aik,jl

.
Whilst, A[I, :] denotes the k × n, and A[:, J] denotes the m × l analogously.
We note that it does not matter whether the index vectors I, J are row vectors or
column vectors. It matters which axis they index (rows of A or columns of A). We
should also notice the range of the index:
(
0 ≤min(I) ≤max(I) ≤m;
0 ≤min(J) ≤max(J) ≤n.
And in all cases, vectors are formulated in a column rather than in a row. A row vector
will be denoted by a transpose of a column vector such as a⊤. A speciﬁc column vector
with values is split by the semicolon symbol “; ”, e.g., x = [1; 2; 3] is a column vector in
R3. Similarly, a speciﬁc row vector with values is split by the comma symbol “, ”, e.g.,
y = [1, 2, 3] is a row vector with 3 values. Further, a column vector can be denoted by the
transpose of a row vector e.g., y = [1, 2, 3]⊤is a column vector.
The transpose of a matrix A will be denoted by A⊤and its inverse will be denoted by
A−1 . We will denote the p × p identity matrix by Ip (or simply I when the size can be
determined from context). A vector or matrix of all zeros will be denoted by a boldface
zero 0 whose size should be clear from context, or we denote 0p to be the vector of all zeros
with p entries.
In this context, we will highly use the idea about the linear independence of a set of
vectors. Two equivalent deﬁnitions are given as follows.
Deﬁnition 0.0.2 (Linearly Independent) A set of vectors {a1, a2, . . . , am} is called
linearly independent if there is no combination that can get x1a1+x2a2+. . .+xmam = 0
except all xi’s are zero. An equivalent deﬁnition is that a1 ̸= 0, and for every k > 1, the
vector ak does not belong to the span of {a1, a2, . . . , ak−1}.
In the study of linear algebra, every vector space has a basis and every vector is a linear
combination of members of the basis. We then deﬁne the span and dimension of a subspace
via the basis.
7

Deﬁnition 0.0.3 (Span) If every vector v in subspace V can be expressed as a linear
combination of {a1, a2, . . . , am}, then {a1, a2, . . . , am} is said to span V.
Deﬁnition 0.0.4 (Subspace) A nonempty subset V of Rn is called a subspace if xa +
ya ∈V for every a, b ∈V and every x, y ∈R.
Deﬁnition 0.0.5 (Basis and Dimension) A set of vectors {a1, a2, . . . , am} is called
a basis of V if they are linearly independent and span V. Every basis of a given subspace
has the same number of vectors, and the number of vectors in any basis is called the
dimension of the subspace V. By convention, the subspace {0} is said to have dimension
zero. Furthermore, every subspace of nonzero dimension has a basis that is orthogonal,
i.e., the basis of a subspace can be chosen orthogonal.
Deﬁnition 0.0.6 (Column Space (Range)) If A is an m × n real matrix, we deﬁne
the column space (or range) of A to be the set spanned by its columns:
C(A) = {y ∈Rm : ∃x ∈Rn, y = Ax}.
And the row space of A is the set spanned by its rows, which is equal to the column space
of A⊤:
C(A⊤) = {x ∈Rn : ∃y ∈Rm, x = A⊤y}.
Deﬁnition 0.0.7 (Null Space (Nullspace, Kernel)) If A is an m × n real matrix,
we deﬁne the null space (or kernel, or nullspace) of A to be the set:
N(A) = {y ∈Rn : Ay = 0}.
And the null space of A⊤is deﬁned as
N(A⊤) = {x ∈Rm : A⊤x = 0}.
Both the column space of A and the null space of A⊤are subspaces of Rn. In fact,
every vector in N(A⊤) is perpendicular to C(A) and vice versa.1
Deﬁnition 0.0.8 (Rank) The rank of a matrix A ∈Rm×n is the dimension of the
column space of A. That is, the rank of A is equal to the maximal number of linearly
independent columns of A, and is also the maximal number of linearly independent rows
of A. The matrix A and its transpose A⊤have the same rank. We say that A has
full rank if its rank is equal to min{m, n}. In another word, this is true if and only if
either all the columns of A are linearly independent, or all the rows of A are linearly
independent. Speciﬁcally, given a vector u ∈Rm and a vector v ∈Rn, then the m × n
1. Every vector in N(A) is also perpendicular to C(A⊤) and vice versa.
8

JUN LU
matrix uv⊤obtained by the outer product of vectors is of rank 1. In short, the rank of a
matrix is equal to:
• number of linearly independent columns;
• number of linearly independent rows;
• and remarkably, these are always the same (see Lu (2021b)).
Deﬁnition 0.0.9 (Orthogonal Complement in General) The orthogonal complement
V⊥of a subspace V contains every vector that is perpendicular to V. That is,
V⊥= {v : v⊤u = 0, ∀u ∈V}.
The two subspaces are disjoint that span the entire space. The dimensions of V and V⊥
add to the dimension of the whole space. Furthermore, (V⊥)⊥= V.
Deﬁnition 0.0.10 (Orthogonal Complement of Column Space) If A is an m×n
real matrix, the orthogonal complement of C(A), C⊥(A) is the subspace deﬁned as:
C⊥(A) = {y ∈Rm : y⊤Ax = 0, ∀x ∈Rn}
= {y ∈Rm : y⊤v = 0, ∀v ∈C(A)}.
Then we have the four fundamental spaces for any matrix A ∈Rm×n with rank r:
• C(A): Column space of A, i.e., linear combinations of columns with dimension r;
• N(A): Null space of A, i.e., all x with Ax = 0 with dimension n −r;
• C(A⊤): Row space of A, i.e., linear combinations of rows with dimension r;
• N(A⊤): Left null space of A, i.e., all y with A⊤y = 0 with dimension m −r,
where r is the rank of the matrix. Furthermore, N(A) is the orthogonal complement to
C(A⊤), and C(A) is the orthogonal complement to N(A⊤) (Lu, 2021b).
Deﬁnition 0.0.11 (Vector 2-Norm) For a vector x ∈Rn, the L2 vector norm is
deﬁned as ∥x∥2 =
p
x2
1 + x2
2 + . . . + x2n.
For a matrix A ∈Rm×n, we deﬁne the (matrix) Frobenius norm as follows.
Deﬁnition 0.0.12 (Frobenius Norm) The Frobenius norm of a matrix A ∈Rm×n is
deﬁned as
∥A∥F =
v
u
u
t
m,n
X
i=1,j=1
(aij)2 =
q
tr(AA⊤) =
q
tr(A⊤A) =
q
σ2
1 + σ2
2 + . . . + σ2r,
i.e., the square root of the sum of the squares of the elements of A. Where σi’s are the
singular values of A, and r is the rank of A.
9

The equivalence of
qPm,n
i=1,j=1(aij)2,
p
tr(AA⊤), and
p
tr(A⊤A) is trivial. While we can
use the singular value decomposition (SVD) to show the equivalence between
p
tr(AA⊤)
and
p
σ2
1 + σ2
2 + . . . + σ2r. Suppose A admits SVD A = UΣV ⊤, then it follows that
q
tr(AA⊤) =
q
tr(UΣV ⊤V ΣU ⊤) =
q
tr(Σ2) =
q
σ2
1 + σ2
2 + . . . + σ2r,
where σi’s are singular values of A. Apparently, the Frobenius norm can be also deﬁned by
a vector 2-norm such that ∥A∥F =
qPn
i=1 ∥ai∥2 where ai for all i ∈{1, 2, . . . , n} are the
columns of A.
Deﬁnition 0.0.13 (Permutation Matrix) A permutation matrix P is a square bi-
nary matrix that has exactly one entry of 1 in each row and each column, and 0’s else-
where.
Row Point.
That is, the permutation matrix P has the rows of the identity I in any
order and the order decides the sequence of the row permutation. Suppose we want to
permute the rows of matrix A, we just multiply on the left by P A.
Column Point.
Or, equivalently, the permutation matrix P has the columns of the
identity I in any order and the order decides the sequence of the column permutation.
And now, the column permutation of A is to multiply on the right by AP .
The permutation matrix P can be more eﬃciently represented via a vector J ∈Zn
+ of
indices such that P = I[:, J] where I is the n×n identity matrix and notably, the elements
in vector J sum to 1 + 2 + . . . + n = n2+n
2
.
Example 0.1 (Permutation) Suppose,
A =


1
2
3
4
5
6
7
8
9

,
and
P =


1
1
1

.
The row permutation is given by
P A =


4
5
6
7
8
9
1
2
3

,
where the order of the rows of A appearing in P A matches the order of the rows of I in
P . And the column permutation is given by
AP =


3
1
2
6
4
5
9
7
8

,
where the order of the columns of A appearing in AP matches the order of the columns of
I in P .
□
10

JUN LU
Deﬁnition 0.0.14 (Selection Matrix) A selection matrix S is a square diagonal ma-
trix with diagonals being 1 or 0. The 1 entries are the rows or columns that will be
selected.
Row Point.
That is, the selection matrix S has the rows of the identity I if we want
to select the corresponding rows, and otherwise, we mask the rows in the identity I by
zero. Suppose we want to select the rows of matrix A, we just multiply from left by SA.
Column Point.
Or, equivalently, the selection matrix S has the columns of the identity
I if we want to select the corresponding columns, or otherwise, we mask the columns in
the identity I by zero. And now, the column selection of A is to multiply from right by
AS.
Example 0.2 (Selection and Permutation) Suppose,
A =


1
2
3
4
5
6
7
8
9

,
and
S =


1
0
1

.
The row selection is given by
SA =


1
2
3
0
0
0
7
8
9

,
where the rows of A appearing in SA match the row entries of S. And the column selection
is given by
AS =


1
0
3
4
0
6
7
0
9

,
where the columns of A appearing in AS match the column entries of S. If now, we want
to reorder the selected rows or columns in the upper left of the ﬁnal matrix, we can construct
a permutation as follows
P =


1
1
1

,
such that
P SA =


1
2
3
7
8
9
0
0
0

,
and
ASP =


1
3
0
4
6
0
7
9
0

.
The trick is essential to some mathematical proofs.
□
In conclusion, regarding the equivalent claims of nonsingular matrices, we have the
following remark from an introductory course on linear algebra.
11

Remark 0.1: List of Equivalence of Nonsingularity for a Matrix
For a square matrix A ∈Rn×n, the following claims are equivalent:
• A is nonsingular;
• A is invertible, i.e., A−1 exists;
• Ax = b has a unique solution x = A−1b;
• Ax = 0 has a unique, trivial solution: x = 0;
• Columns of A are linearly independent;
• Rows of A are linearly independent;
• det(A) ̸= 0;
• dim(N(A)) = 0;
• N(A) = {0}, i.e., the null space is trivial;
• C(A) = C(A⊤) = Rn, i.e., the column space or row space span the whole Rn;
• A has full rank r = n;
• The reduced row echelon form is R = I;
• A⊤A is symmetric positive deﬁnite;
• A has n nonzero (positive) singular values;
• All eigenvalues are nonzero;
Keeping the above equivalence in mind is important to avoid confusion. On the other hand,
the following remark also shows the equivalent claims for singular matrices.
Remark 0.2: List of Equivalence of Singularity for a Matrix
For a square matrix A ∈Rn×n with eigenpair (λ, u), the following claims are equivalent:
• (A −λI) is singular;
• (A −λI) is not invertible;
• (A −λI)x = 0 has nonzero x ̸= 0 solutions, and x = u is one of such solutions;
• (A −λI) has linearly dependent columns;
• det(A −λI) = 0;
• dim(N(A −λI)) > 0;
• Null space of (A −λI) is nontrivial;
• Columns of (A −λI) are linearly dependent;
• Rows of (A −λI) are linearly dependent;
• (A −λI) has rank r < n;
• Dimension of column space = dimension of row space = r < n;
• (A −λI)⊤(A −λI) is symmetric semideﬁnite;
• (A −λI) has r < n nonzero (positive) singular values;
• Zero is an eigenvalue of (A −λI).
12

JUN LU
13

1
Monte Carlo Methods
Contents
1.1
The Bayesian Approach . . . . . . . . . . . . . . . . . . . . . . .
15
1.2
Approximate Inference . . . . . . . . . . . . . . . . . . . . . . . .
16
1.3
Monte Carlo (MC) Methods
. . . . . . . . . . . . . . . . . . . .
17
1.3.1
Markov Chain Monte Carlo (MCMC) . . . . . . . . . . . . . . . .
18
1.3.2
MC V.S. MCMC . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
1.3.3
Gibbs Sampler
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.3.4
Adaptive Rejection Sampling (ARS)
. . . . . . . . . . . . . . . .
21
Rejection Sampling . . . . . . . . . . . . . . . . . . . . . . . . . .
21
Adaptive Rejection Sampling . . . . . . . . . . . . . . . . . . . . .
21
1.4
Bayesian Appetizers
. . . . . . . . . . . . . . . . . . . . . . . . .
22
1.4.1
Beta-Bernoulli Model . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.4.2
Bayesian Linear Model with Zero-Mean Prior . . . . . . . . . . .
25
1.4.3
Bayesian Linear Model with Semi-Conjugate Prior
. . . . . . . .
27
1.4.4
Bayesian Linear Model with Full Conjugate Prior . . . . . . . . .
29
14

JUN LU
CHAPTER 1. MONTE CARLO METHODS
This book focuses on Markov chain Monte Carlo (MCMC) methods for probabilistic
inference, which draws conclusions from a probabilistic model. And this chapter surveys the
mathematical details of probabilistic inference, focusing on those aspects that will provide
the foundation for the rest of this book.
1.1. The Bayesian Approach
In the past decade, Bayesian approach has been used in a wide variety of problems in data
analysis, e.g., economic forecasting, medical imaging, and population studies (Besag, 1986;
Hill, 1994; Marseille et al., 1996). In modern statistics, Bayesian approaches have become
increasingly more important and widely used. Thomas Bayes came up with this idea but
died before publishing it. Fortunately, his friend Richard Price carried on his work and
published it in 1764. And it was later independently discovered by Laplace at the end of
the 18-th century. In this section, we describe the basic ideas about the Bayesian approach
and use the Beta-Bernoulli model and Bayesian linear model as an appetizer of the pros
and prior information of Bayesian models.
Bayesian modeling and statistics are fundamentally driven by Bayes’ theorem. Formally,
we have the following theorem.
Theorem 1.1: (Bayes’ Theorem)
Let S be a sample space and let B1, B2, . . . , BK be
a partition of A such that (1). ∪kBk = S and (2) Bi ∩Bj = ∅for all i ̸= j. Let further A
be any event. Then it follows that
P(Bk | A) = P(A | Bk)P(Bk)
P(A)
=
P(A | Bk)P(Bk)
PK
i=1 P(A | Bi)P(Bi)
.
In Bayesian modeling and statistics, Bayes’ Theorem oﬀers a straightforward method for
updating probabilities when new information arises such as observed data. This allows us
to adjust our prior beliefs about parameters of interest.
To be more speciﬁc, let X(x1:N) = {x1, x2, . . . , xN} be the observations of N data
points, and suppose they are independent and identically distributed (i.i.d.), with the prob-
ability parameterized by θ. Note that the parameters θ might include the hidden variables,
for example, the latent variables in a mixture model to indicate which cluster a data point
belongs to.
The idea of the Bayesian approach is to assume a prior probability distribution for θ
with hyperparameters α (i.e., p(θ | α), also known as the probability of the model) - that
is, a distribution representing the plausibility of each possible value of θ before the data is
observed and it captures our prior uncertainty regarding θ. The joint distribution of θ and
X is given by
p(θ, X) = p(θ | α)p(X | θ).
And we can integrate out θ to get the marginal distribution of X,
p(X) =
Z
θ
p(θ | α)p(X | θ)dθ.
Then, to make inferences about θ, one simply considers the conditional distribution of θ
given the observed data. This is referred to as the posterior distribution, since it represents
15

1.2. APPROXIMATE INFERENCE
the plausibility of each possible value of θ after seeing the data. The posterior distribution
is the solution space for given problems, since it measures the probability of the present
model in light of the data. Mathematically, this is expressed via Bayes’ theorem,
p(θ | X, α) = p(X | θ)p(θ | α)
p(X | α)
= p(X | θ)p(θ | α)
R
θ p(X, θ | α)
=
p(X | θ)p(θ | α)
R
θ p(X | θ)p(θ | α) ∝p(X | θ)p(θ | α),
(1.1)
where X is the observed data set and p(X | α) can be ignored in this case since it acts as a
scaling parameter (and we shall see the MCMC algorithm only needs relative probabilities).
In other words, we say the posterior is proportional to the likelihood times the prior. This
means that the relative probability at a point in the solution space is determined completely
by the likelihood, which is easily determined by comparing the model to the data, and the
prior, which is the probability of the model independent of the data. The prior encodes
any knowledge of the solution independent of the data. For example, a prior for a system
reducing over-clustering might give a higher probability to a larger cluster than to a small
cluster (Lu, 2021c).
More generally, the Bayesian approach - in a nutshell - is to assume a prior distribution
on any unknowns (θ in our case), and then just follow the rules of probability to answer
any questions of interest. For example, when we ﬁnd the parameter based on the maximum
posterior probability of θ, we turn to the maximum a posteriori (MAP) estimator.
Frequentists V.S. Bayesian
The frequentist approach to statistics, developed by Ney-
man, evaluates statistical procedures based on a probability distribution over all possible
data sets. To be more speciﬁc, frequentists consider the parameter vector θ to be ﬁxed
(albeit unknown), while introducing uncertainty over possible data sets X. In contrast,
the Bayesian approach treats the data set X as given, while introducing uncertainty over
θ. However, statisticians nowadays tend to move comfortably between these approaches
and popular statistical procedures often combine both of them. For instance, empirical
Bayesian methods have a Bayesian spirit but are not strictly Bayesian, and their analysis
is frequently frequentist (Haugh, 2021).
1.2. Approximate Inference
For this book, we focus on approximate probabilistic inference methods. In certain cases,
it is computationally feasible to compute the posterior exactly. For example, exponential
families with conjugate priors often enable analytical solutions. Although exact inference
methods exist and they are precise and useful for certain classes of problems, exact inference
methods in complicated models are usually intractable, because these methods typically
depend on integrals, summations, or intermediate representations that grow large as the
state space grows too large so as to make the computation ineﬃcient. For example, we may
use conjugate priors in a Gaussian mixture model. However, the model is hierarchical and
is too complicated to compute the exact posterior. In these cases, approximate probabilistic
inference methods are rather useful and necessary.
Generally, variational methods and Monte Carlo methods are two main classes of ap-
proximate inference. We here give a brief comparison of the two methods (Bonawitz, 2008).
16

JUN LU
CHAPTER 1. MONTE CARLO METHODS
In variational inference methods, we ﬁrst approximate the full model with a simpler model
in which the inference questions are tractable. Then, the parameters of this simpliﬁed model
are calculated by some methods (e.g. by optimization methods) to minimize a measure of
the dissimilarity between the original model and the simpliﬁed version; this calculation usu-
ally performs deterministically because of the optimization methods used. Finally, certain
queries can be calculated and executed in the simpliﬁed model. In other words, the main
idea behind variational methods is to pick a family of distributions over the parameters with
its own variational parameters - q(θ | ν) where ν is the variational parameters. Then, ﬁnd
the setting of the parameters that makes q close to the posterior of interest. As a detailed
example, we can refer to Ma et al. (2014). The main advantage of variational methods is
deterministic; however, the corresponding results are in the form of a lower bound of the
desired quantity, and the tightness of this bound depends on the degree to which the sim-
pliﬁed distribution can model the original posterior distribution. The variational inference
is an important tool for Bayesian deep learning (Jordan et al., 1999; Graves, 2011; Hoﬀman
et al., 2013; Ranganath et al., 2014; Mandt and Blei, 2014).
On the contrary, in Monte Carlo methods we ﬁrst draw a sequence of samples from the
true target posterior distribution. Then certain inference questions are then answered by
using this set of samples as an approximation of the target distribution itself. Monte Carlo
methods are guaranteed to converge – if you want a more accurate answer, you just need to
run the inference for longer; in the limit of running the Monte Carlo algorithm forever, the
approximation results from the samples converge to the target distribution (see Section 1.3).
1.3. Monte Carlo (MC) Methods
In Monte Carlo methods, we ﬁrst draw N samples θ1, θ2, . . . , θN from the posterior dis-
tribution p(θ | X, α) in Equation (1.1), and then approximate the distribution of interest
by
p(θ | −) ≈
∼p(θ | −) = 1
N
N
X
n=1
δθn(θ),
(1.2)
where δθi(θ) is the Dirac delta function1. As the number of samples increases, the approx-
imation (almost surely) converges to the true target distribution, i.e.,
∼p(θ)
a.s.
N→∞
−→p(θ).
These kinds of sampling-based methods are extensively used in modern statistics, due
to their ease of use and the generality with which they can be applied. The fundamental
problem solved by these methods is the approximation of expectations such as
Eh(Θ) =
Z
θ
h(θ)p(θ)dθ,
(1.3)
in the case of a continuous random variable Θ with probability density function (p.d.f.) p.
Or
Eh(Θ) =
X
θ
h(θ)p(θ),
(1.4)
1. The Dirac delta function δx0(x) has the properties that it is non-zero and equals 1 only at x = x0.
17

1.3. MONTE CARLO (MC) METHODS
in the case of a discrete random variable Θ with probability mass function (p.m.f.) p. The
general principle at work is that such expectations can be approximated by
Eh(Θ) ≈
N
X
n=1
h(θn),
(1.5)
If it were generally easy to draw samples directly from p(θ | X, α), the Monte Carlo story
would end here. Unfortunately, this is usually intractable. We can consider the posterior
form p(θ | X, α) = p(X|θ)p(θ|α)
p(X|α)
, where in many problems p(X | θ)p(θ | α) can be computed
easily, but p(X | α) cannot due to integrals, summations, etc. In this case Markov chain
Monte Carlo is especially useful.
1.3.1 Markov Chain Monte Carlo (MCMC)
Markov chain Monte Carlo (MCMC) algorithms, also called samplers, are numerical ap-
proximation algorithms. Since MCMC algorithms directly sample the solution space, un-
certainty estimates are determined simultaneously with a “best” solution. Further, provided
that the data support them, multiple solutions are possible. Intuitively, it is a stochastic
hill-climbing approach to inference, operating over the complete data set. This inference
method is designed to spend most of the computational eﬀorts to sample points from the
high probability regions of true target posterior distribution p(θ | X, α) (Andrieu et al.,
2003; Bonawitz, 2008; Hoﬀ, 2009; Geyer, 2011). In this sampler, a Markov chain stochastic
walk is taken through the state space Θ such that the probability of being in a particu-
lar state θt at any point in the walk is p(θt | X, α). Therefore, samples from the true
posterior distribution p(θ | X, α) can be approximated by recording the samples (states)
visited by the stochastic walk and some other post-processing methods such as thinning.
The stochastic walk is a Markov chain, i.e. the choice of state at time t + 1 depends only
on its previous state - the state at time t. Formally, if θt is the state of the chain at time
t, then p(θt+1 | θ1, θ2, . . . , θt) = p(θt+1 | θt). That is, Markov chains are history-free, and
we can get two main advantages from this history-free property:
• From this history-free property, the Markov chain Monte Carlo methods can be run
for an unlimited number of iterations without consuming additional memory space;
• The history-free property also indicates that the MCMC stochastic walk can be com-
pletely characterized by p(θt+1 | θt), known as the transition kernel.
We then focus on the discussion of the transition kernel. The transition kernel K can also
be formulated as a linear transform, thus if pt = pt(θ) is a row vector that encodes the
probability of the walk being in state θ at time t, then pt+1 = ptK. If the stochastic walk
starts from state θ0, then the distribution from this initial state is the delta distribution
p0 = δθ0(θ) and the state distribution for the chain after step t is pt = p0Kt. We can
easily ﬁnd that the key to Markov chain Monte Carlo is to choose kernel K such that
lim
t→∞pt = p(θ | X, α), independent of the choice of θ0. Kernels with this property are said
to converge to an equilibrium distribution peq = p(θ | X). Convergence is guaranteed if
both of the following criteria meet (see Bonawitz (2008)):
• peq is an invariant (or stationary) distribution for K. A distribution pinv is an invariant
distribution for K if pinv = pinvK;
18

JUN LU
CHAPTER 1. MONTE CARLO METHODS
• K is ergodic. A kernel is ergodic if it is irreducible (any state can be reached from
any other state) and aperiodic (the stochastic walk never gets stuck in cycles).
There are a large number of MCMC algorithms, too many to review here. Popular
families include Gibbs sampling, Metropolis-Hastings (MH), slice sampling, Hamiltonian
Monte Carlo, adaptive rejection sampling, and many others. Though the name is mislead-
ing, Metropolis-within-Gibbs (MWG) was developed ﬁrst by Metropolis et al. (1953), and
MH was a generalization of MWG (Hastings, 1970).
All MCMC algorithms are known
as special cases of the MH algorithm. Regardless of the algorithm, the goal of Bayesian
inference is to maximize the unnormalized joint posterior distribution and collect samples
of the target distributions, which are marginal posterior distributions, later to be used for
inference queries.
The most generalizable MCMC algorithm is the Metropolis-Hastings (MH) generaliza-
tion (Metropolis et al., 1953; Hastings, 1970) of the MWG algorithm. The MH algorithm
extended MWG to include asymmetric proposal distributions. In this method, it converts
an arbitrary proposal kernel q(θ⋆| θt) into a transition kernel with the desired invariant
distribution peq(θ). In order to generate a sample from a MH transition kernel, we ﬁrst
draw a proposal θ⋆∼q(θ⋆| θt), then evaluates the MH acceptance probability by
P[A(θ⋆| θt)] = min

1, p(θ⋆| α)q(θt | θ⋆)
p(θt | α)q(θ⋆| θt)

,
(1.6)
with probability P[A(θ⋆| θt)] being the proposal is accepted and we set θt+1 = θ⋆; other-
wise the proposal is rejected and we set θt+1 = θt. That is
θt+1 =
 θ⋆, with probability P[A(θ⋆| θt)];
θt, with probability 1 −P[A(θ⋆| θt)].
(1.7)
Intuitively, we may ﬁnd that the p(θ⋆|α)
p(θt|α) term tends to accept moves that lead to higher
probability parts of the state space, while also the q(θt|θ⋆)
q(θ⋆|θt) term tends to accept moves that
are easy to undo. Because in MH, we only evaluate p(θ) as part of the ratio p(θ⋆|α)
p(θt|α) , we do
not need compute p(X | α) as mentioned in Section 1.3.
The key in MH is the proposal kernel q(θ⋆| θt). However, the transition kernel is not
q(θ⋆| θt). Informally, the kernel K(θt+1 | θt) in MH is
p(θt+1 | accept)P[accept] + p(θt+1 | reject)P[reject].
While Tierney (1998) introduced that the precise transition kernel is
K(θt →θt+1) = p(θt+1 | θt)
= q(θt+1 | θt)A(θt+1 | θt) + δθt(θt+1)
Z
θ⋆
q(θ⋆| θt)(1 −A(θ⋆| θt)).
(1.8)
1.3.2 MC V.S. MCMC
As shown in previous sections, the purpose of Monte Carlo or Markov chain Monte Carlo
approximation is to obtain a sequence of parameter values {θ(1), . . . , θ(N)} such that
1
N
N
X
n=1
h(θ(n)) ≈
Z
θ
h(θ)p(θ)dθ,
(1.9)
19

1.3. MONTE CARLO (MC) METHODS
for any functions h of interest in the case of continuous random variables. In other words, we
want the empirical average of {h(θ(1)), . . . , h(θ(N))} to approximate the expected value of
h(θ) under a target probability distribution p(θ). In order for this to be a good approxima-
tion for a wide range of functions h, we require the empirical distribution of the simulated
sequence {θ(1), . . . , θ(N)} to look like the target distribution p(θ). MC and MCMC are
two ways of generating such a sequence. MC simulation, in which we generate independent
samples from the target distribution, is in some sense the “true situation”. Independent
MC samples automatically create a sequence that is representative of p(θ), which means
the probability that θ(n) ∈A for any set A is
Z
A
p(θ)dθ.
(1.10)
where n ∈{1, . . . , N}. However, this is not true for MCMC samples, in which case all we
are sure of is that
lim
n→∞Pr(θ(n) ∈A) =
Z
A
p(θ)dθ.
(1.11)
1.3.3 Gibbs Sampler
Gibbs sampling was introduced by Turchin (Turchin, 1971), and later by brothers Geman
and Geman (Geman and Geman, 1984) in the context of image restoration. The Geman
brothers named the algorithm after the physicist J. W. Gibbs, some eight decades after his
death, in reference to an analogy between the sampling algorithm and statistical physics.
Gibbs sampling is applicable when the joint distribution is not known explicitly or is
diﬃcult to sample from directly, but the conditional distribution of each variable is known
and easy to sample from.
A Gibbs sampler generates a draw from the distribution of
each parameter or variable in turn, conditional on the current values of the other pa-
rameters or variables. Therefore, a Gibbs sampler is a componentwise algorithm. In our
example, given some data X and a probability distribution p(θ | X, α) parameterized by
θ = {θ1, θ2, . . . , θp}. We can successively draw samples from the distribution by sampling
from
θ(t)
i
∼p(θi | θ(t−1)
−i
, X, α),
(1.12)
where θ(t−1)
−i
is all current values of θ in the (t −1)-th iteration except for θi. If we sample
long enough, these θi values will be random samples from the distribution p. If we sample
new values in turn for each parameter θi from Equation (1.12), we will eventually converge
to draws from the posterior p(θ | X, α). When doing this Gibbs sampler, we also have to
discard the ﬁrst k draws since it takes a while to converge (i.e., burn-in), and because the
consecutive draws are correlated we only use every j-th sample (i.e., thinning).
In deriving a Gibbs sampler, it is often helpful to observe that
p(θi | θ−i, X) = p(θ1, θ2, . . . , θp, X)
p(θ−i, X)
∝p(θ1, θ2, . . . , θp, X).
(1.13)
That is, the conditional distribution is proportional to the joint distribution. We will get
a lot of beneﬁts from this simple observation by dropping constant terms from the joint
distribution (relative to the parameters we are conditioned on).
20

JUN LU
CHAPTER 1. MONTE CARLO METHODS
Shortly, as a simpliﬁed example, given a joint probability distribution p(θ1, θ2 | X), a
Gibbs sampler would draw p(θ1 | θ2, X) , then p(θ2 | θ1, X) iteratively. The procedure
deﬁnes a sequence of realization of random variables θ1 and θ2
(θ0
1, θ0
2), (θ1
1, θ1
2), (θ2
1, θ2
2), · · ·
which converges to the joint distribution p(θ1, θ2). More details about Gibbs sampling can
be found in Turchin (1971); Geman and Geman (1984); Hoﬀ(2009); Gelman et al. (2013).
1.3.4 Adaptive Rejection Sampling (ARS)
The purpose of the adaptive rejection sampling algorithm is to provide a relatively eﬃcient
way to sample from a distribution from the large class of log-concave densities (Gilks and
Wild, 1992; Wild and Gilks, 1993). We only overview the algorithm here and we can ﬁnd
more details in Gilks and Wild (1992) and Wild and Gilks (1993).
Figure 1.1: Rejection sampling. Figure from Michael I. Jordan’s lecture notes.
Rejection Sampling
In rejection sampling, we want to sample from a target probability density function p(x),
given that we can sample from a probability density function q(x) easily. The target density
p(x) is not known. But the idea is that, if M × q(x) forms an envelope over p(x) for some
M > 1 as shown in Figure 1.1, i.e.
p(x)
q(x) < M, for all x.
(1.14)
Then if we sample some xi from q(x), and if yi = u × M × q(xi) lies below the region under
p(x) for some u ∼Uniform(0, 1), then we accept xi, otherwise, we reject xi.
Informally, what the method does is to sample xi from some distribution and then it
decides whether to accept it or reject it.
Adaptive Rejection Sampling
The adaptive rejection sampling method is based on rejection sampling and it works only
for log-concave densities. The basic idea is to form an upper envelope (the upper bound on
p(x)) adaptively and use this to replace M × q(x) in rejection sampling.
21

1.4. BAYESIAN APPETIZERS
Figure 1.2: Adaptive rejection sampling. Figure from Michael I. Jordan’s lecture notes.
As shown in Figure 1.2, the log density log p(x) is considered. We then sample xi from
the upper envelope, and either accepted or rejected as in rejection sampling. If it is rejected,
a tangent is drawn passing through x = xi and y = log(p); and the tangent is used to reduce
the upper envelope to decrease the number of rejected samples. The intersections of these
tangent planes enable the formation of an envelope adaptively. To sample from the upper
envelope, we need to transform from log space by exponentiating and using properties of
the exponential distribution.
1.4. Bayesian Appetizers
This section will cover semi-conjugate priors with Gibbs sampler and fully conjugate priors
without approximate inference to provide a deeper understanding of Bayesian approaches.
Readers who already have a basic understanding of Bayesian inference may choose to skip
this section.
1.4.1 Beta-Bernoulli Model
We formally introduce a Beta-Bernoulli model to show how the Bayesian approach works.
The Bernoulli distribution models binary outcomes, i.e., outputting two possible values. The
likelihood under this model is just the probability mass function of Bernoulli distribution
with parameter θ:
Bern(x | θ) = p(x | θ) = θx(1 −θ)1−x1(x ∈{0, 1}).
That is,
Bern(x | θ) = p(x | θ) =
(
1 −θ,
if x = 0;
θ,
if x = 1,
where θ is the probability of outputting 1 and 1 −θ is the probability of outputting 0. The
mean of the Bernoulli distribution is θ. Suppose X = {x1, x2, . . . , xn} are drawn i.i.d. from
the Bernoulli distribution Bern(θ). Then the likelihood under the Bernoulli distribution
22

JUN LU
CHAPTER 1. MONTE CARLO METHODS
0.0
0.2
0.4
0.6
0.8
1.0
x
0.0
0.5
1.0
1.5
2.0
2.5
3.0
f(x)
Beta Distribution PDF
a=1, b=1
a=2, b=2
a=3, b=3
a=4, b=4
Figure 1.3:
Beta probability density func-
tions for diﬀerent values of the parameters a
and b.
When a = b = 1, the beta distri-
bution reduces to a uniform distribution in
the support of [0, 1].
The mean, variance,
and mode of the Beta distribution are E[x] =
a
a+b, Var[x] =
ab
(a+b+1)(a+b)2 , and mode[x] =
a−1
(a−1)+(b−1) if a > 1, b > 1, respectively.
with parameter θ is given by
likelihood = p(X | θ) = θ
P xi(1 −θ)n−P xi,
which is a distribution on X and is called the likelihood function on X.
And we will see the prior under this model is the probability density function of a Beta
distribution:
prior = Beta(θ | a, b) = p(θ | a, b) =
1
B(a, b)θa−1(1 −θ)b−11(0 ≤θ ≤1),
where B(a, b) is the Euler’s beta function and it can be simply regarded as a constant
normalization term, and 1(a ≤x ≤b) is a step function that has a value of 1 when
a ≤x ≤b, and 0 when x < a or a > b. Figure 1.3 compares diﬀerent parameters for the
Beta distribution. When a = b = 1, the beta distribution reduces to a uniform distribution
in the support of [0, 1].
We put a Beta prior on the parameter θ of the Bernoulli distribution. The posterior is
proportional to the product of likelihood and prior densities, and it can be obtained by
posterior = p(θ | X) ∝p(X | θ)p(θ | a, b)
= θ
P xi(1 −θ)n−P xi ×
1
B(a, b)θa−1(1 −θ)b−1 · 1(0 ≤θ ≤1)
∝θa+P xi−1(1 −θ)b+n−P xi−1 · 1(0 ≤θ ≤1)
∝Beta
 
θ
 a +
n
X
i=1
xi, b + n −
n
X
i=1
xi
!
.
We ﬁnd that the posterior distribution shares the same form as the prior distribution. When
this happens, we call the prior a conjugate prior. The conjugate prior has a nice form such
that it is easy to work with for computing the posterior probability density function and
its derivatives, and sampling from the posterior.
Remark 1.2: Prior Information in Beta-Bernoulli Model
A comparison of the prior and posterior formulations would ﬁnd that the hyperparameter
a is the prior number of 1’s in the output and b is the prior number of 0’s in the output.
23

1.4. BAYESIAN APPETIZERS
And a+b is the prior information about the sample size. Uninformative prior parameters
are then a = b = 1, i.e., a uniform distribution.
Remark 1.3: Bayesian Estimator
From this example of the Beta-Bernoulli model, like the maximum likelihood estimator
and method of moment (MoM, i.e., using the moment information to calculate the model
parameter), Bayesian model is also a kind of point estimator.
But Bayesian models
output a probability of the parameter of interest, p(θ | X) in the example.
When we want to predict the results of new coming data, we do not give out the
prediction by a direct model p(xn+1 | θ). But rather an integration:
p(xn+1 | X) =
Z
p(xn+1 | θ)p(θ | X)dθ.
In another word, xn+1 is dependent of X. Observed data X provide information about
θ, which in turn provides information about xn+1 (i.e., X →θ →xn+1).
Example 1.4.1 (Amount of Data Matters) Suppose we have three observations for the
success in the Bernoulli distribution:
1. 10 out of 10 are observed to be success (1’s);
2. 48 out of 50 are observed to be success (1’s);
3. 186 out of 200 are observed to be success (1’s).
So, what is the probability of success in the Bernoulli model? The normal answer to case
1, 2, 3 are 100%, 96%, and 93%, respectively. But an observation of 10 inputs is rather a
small amount of data and noise can make it less convincing.
Suppose we put a Beta(1, 1) prior over the Bernoulli distribution. The posterior prob-
ability of success for each case would be
11
12 = 91.6%,
49
52 = 94.2%, and
187
202 = 92.6%,
respectively. Now we ﬁnd the case 1 has less probability of success compared to case 2.
A Bayesian view of the problem naturally incorporates the amount of data as well as its
average. This special case shown here is also called Laplace’s rate of succession (Ollivier,
2015). Laplace’s “add-one” rule (i.e., the Beta(1, 1) prior) of succession modiﬁes the ob-
served frequencies in a sequence of successes and failures by adding one to the observed
counts. This improves prediction by avoiding zero probabilities and corresponds to a uni-
form Bayesian prior on the parameter. Suppose further the prior parameters are a = b = 2,
Figure 1.4 compares the prior distribution and the posterior distributions for the three cases.
□
Why Bayes?
This example above also shows that Bayesian models consider prior information on the
parameters in the model making it particularly useful to regularize regression prob-
lems where data information is limited. And this is why the Bayesian approach gains
worldwide attention for decades.
24

JUN LU
CHAPTER 1. MONTE CARLO METHODS
0.0
0.2
0.4
0.6
0.8
1.0
x
0
5
10
15
20
f(x)
Prior and Posterior Beta Distribution
a=2, b=2 (prior)
a=12, b=2 (posterior)
a=50, b=4 (posterior)
a=188, b=16 (posterior)
Figure 1.4:
Prior
distribution
is
Beta(x | 2, 2).
The posterior distri-
butions for the three cases in Exam-
ple 1.4.1 are Beta(x | 12, 2), Beta(x |
50, 4), and Beta(x | 188, 16), respec-
tively.
The prior information p(θ) and likelihood function p(x | θ) represent a rational
person’s belief, and then the Bayes’ rule is an optimal method of updating this person’s
beliefs about θ given new information from the data (Fahrmeir et al., 2007; Hoﬀ, 2009).
The prior information given by p(θ) might be wrong if it does not accurately rep-
resent our prior beliefs. However, this does not mean that the posterior p(θ | x) is
not useful. A famous quote is “all models are wrong, but some are useful” (Box and
Draper, 1987). If the prior p(θ) approximates our beliefs, then the posterior p(θ | x) is
also a good approximation to posterior beliefs.
1.4.2 Bayesian Linear Model with Zero-Mean Prior
In the linear model, given the input data matrix X ∈Rn×p and the observation vector
y ∈Rn. The linear model considers the overdetermined system y = Xβ where the vector
β ∈Rp is a vector of weights for the linear model. It often happens that y = Xβ has no
solution since there are too many equations, i.e., the matrix X has more rows than columns
(n > p). Deﬁne the column space of X by {Xγ : ∀γ ∈Rp} and denoted by C(X). Thus
the linear system y = Xβ has no solution means that y is outside the column space of X.
This problem can be solved by ﬁnding the least mean squared error (MSE).
Instead of ﬁnding the least MSE directly, we can assume further a Gaussian noise vector
ϵ ∈Rn such that y = Xβ+ϵ where ϵ ∼N(0, σ2I) and σ2 is ﬁxed (where N(a, B) denotes a
multivariate Gaussian distribution 2 with mean a and covariance B. And a detailed analysis
of this model can be found in Rasmussen (2003); Hoﬀ(2009); Lu (2022a)), this additive
Gaussian noise assumption gives rise to the likelihood. Let X(x1:n) = {x1, x2, . . . , xn} be
the observations of n data points, the likelihood function under this Gaussian additive noise
model is
likelihood = y | X, β, σ2 ∼N(Xβ, σ2I).
(1.15)
Suppose we specify a multivariate Gaussian prior with zero-mean and covariance matrix Σ0
over the weight parameter β,
prior = β ∼N(0, Σ0).
2. We delay the deﬁnition in Chapter 2, p. 32 when we discuss regular conjugate models, see Deﬁnition 2.7.1,
p. 67.
25

1.4. BAYESIAN APPETIZERS
By the Bayes’ theorem, “posterior ∝likelihood × prior”, we obtain the posterior
posterior = p(β | y, X, σ2)
∝p(y | X, β, σ2) · p(β | Σ0)
=
1
(2πσ2)n/2 exp

−1
2σ2 (y −Xβ)⊤(y −Xβ)

×
1
(2π)n/2 |Σ0|1/2 exp

−1
2β⊤Σ−1
0 β

∝exp

−1
2(β −β1)⊤Σ−1
1 (β −β1)

∝N(β1, Σ1),
where
Σ1 =
 1
σ2 X⊤X + Σ−1
0
−1
,
β1 =
 1
σ2 X⊤X + Σ−1
0
−1  1
σ2 X⊤y

.
Therefore, the posterior distribution is also a multivariate Gaussian distribution (same form
as the prior distribution, i.e., a conjugate prior):
posterior = β | y, X, σ2 ∼N(β1, Σ1).
A word on the notation.
Note that we use {β1, Σ1} to denote the posterior mean
vector and posterior covariance matrix in the zero-mean prior model. Similarly, the posterior
mean vector and posterior covariance matrix in semi-conjugate prior and fully conjugate
prior models will be denoted as {β2, Σ2} and {β3, Σ3} respectively, for clarity (see sections
below).
Connection to ordinary least squares (OLS).
In the Bayesian linear model, we do
not need to assume X has full rank generally.
Note further that if we assume X has
full rank (i.e., X⊤X is invertible when n > p), in the limit, when Σ−1
0
→0, β1 →
ˆβ = (X⊤X)−1Xy, in which case, the maximum a posteriori (MAP) estimator from the
Bayesian model goes back to the ordinary least squares (OLS) estimator. And the posterior
is β | y, X, σ2 ∼N(ˆβ, σ2(X⊤X)−1), which shares a similar form with the OLS estimator
ˆβ ∼N(β, σ2(X⊤X)−1) under the Gaussian disturbance (see Lu (2022a)).
Remark 1.4: Ridge Regression
In the least squares approximation problem, we use Xβ to approximate y. Two issues
arise: the model can potentially overﬁt and X may not have full rank.
In a ridge
regression model, we regularize large values of β and thus favor simpler models. Instead
of minimizing ∥y −Xβ∥2, we minimize ∥y −Xβ∥2+λ ∥β∥2, where λ is a hyperparameter
that can be tuned accordingly, e.g., via cross-validation (CV):
arg min
β
(y −Xβ)⊤(y −Xβ) + λβ⊤β.
By diﬀerentiating and setting the derivative to zero we get
ˆβridge =

X⊤X + λI
−1
X⊤y,
26

JUN LU
CHAPTER 1. MONTE CARLO METHODS
in which case, (X⊤X + λI) is invertible even when X does not have full rank. Further
details on ridge regression will be left to the readers.
Connection to ridge regression.
We realize that when we set Σ0 = I, we obtain β1 =
 X⊤X + σ2I
−1 X⊤y and Σ1 =
  1
σ2 X⊤X + I
−1. Since the posterior is β | y, X, σ2 ∼
N(β1, Σ1). The MAP estimator of β = β1 = (X⊤X + σ2I)−1X⊤y, which shares the
same form as the ridge regression by letting σ2 = λ. Thus, we notice the ridge regression
estimator is a special case of the Bayesian linear model with zero-mean prior. And ridge
regression has a nice interpretation from the Bayesian approach - ﬁnding the mode of the
posterior.
An example of this Bayesian linear model is shown in Rasmussen (2003) where the “well
determined” (i.e., the distribution around the slope is more compact) slope of β is almost
unchanged after the posterior process while the intercept which is more dispersed is shrunk
towards zero. This is actually a regularization eﬀect on the parameter as the ridge regression
does.
1.4.3 Bayesian Linear Model with Semi-Conjugate Prior
We will use the Gamma distribution as the prior density of the inverse variance (precision)
parameter of a Gaussian distribution. The rigorous deﬁnition of the Gamma distribution
can be found in Chapter 2 (Deﬁnition 2.2.3, p. 36) when we discuss conjugate models. As
for the reason of using the Gamma distribution as the prior for precision, we quote the
description from Kruschke (2014):
Because of its role in conjugate priors for Gaussian likelihood functions, the Gamma
distribution is routinely used as a prior for precision (i.e., inverse variance). But there
is no logical necessity to do so, and modern MCMC methods permit more ﬂexible spec-
iﬁcation of priors. Indeed, because precision is less intuitive than standard deviation, it
can be more useful to give standard deviation a uniform prior that spans a wide range.
Same setting as Section 1.4.2, but we assume the variance σ2 of the Gaussian likelihood
is not ﬁxed now. Again, we have the likelihood function by
likelihood = y | X, β, σ2 ∼N(Xβ, σ2I).
We specify a non zero-mean Gaussian prior over the weight parameter β,
prior : β ∼N(β0, Σ0)
γ = 1/σ2 ∼G(a0, b0),
where we diﬀerentiate from previous descriptions by blue text and G(a, b) =
ba
Γ(a)xa−1 exp(−bx)
denotes a Gamma distribution with parameters a, b and Γ(a) =
R ∞
0 at−1 exp(−a)dt is the
Gamma function.
27

1.4. BAYESIAN APPETIZERS
Step 1, conditioned on σ2.
Then, given σ2, by the Bayes’ theorem “posterior ∝
likelihood × prior”, we get the conditional posterior density of β,
posterior = p(β | y, X, σ2) ∝p(y | X, β, σ2) · p(β | β0, Σ0)
=
1
(2πσ2)n/2 exp

−1
2σ2 (y −Xβ)⊤(y −Xβ)

×
1
(2π)n/2 |Σ0|1/2 exp

−1
2(β −β0)⊤Σ−1
0 (β −β0)

∝exp

−1
2(β −β2)⊤Σ−1
2 (β −β2)

∝N(β2, Σ2),
where the parameters are
Σ2 =
 1
σ2 X⊤X + Σ−1
0
−1
,
β2 = Σ2(Σ−1
0 β0 + 1
σ2 X⊤y) =
 1
σ2 X⊤X + Σ−1
0
−1 
Σ−1
0 β0 + 1
σ2 X⊤y

.
Therefore, the conditional posterior follows from a Gaussian distribution:
posterior = β | y, X, σ2 ∼N(β2, Σ2).
Connection to the zero-mean prior model.
We highlight the connection between the
zero-mean prior model and the semi-conjugate prior model as follows:
1. We note that β1 in Section 1.4.2 is a special case of β2 when β0 = 0.
2. And if we assume further X has full rank. When Σ−1
0
→0, β2 →ˆβ = (X⊤X)−1Xy
which reduces to the OLS estimator.
3. When σ2 →∞, β2 is approximately approaching β0, the prior expectation of pa-
rameter. However, in the zero-mean prior model, σ2 →∞will make β1 approach
0.
4. Weighted average: we reformulate β2 by
β2 =
 1
σ2 X⊤X + Σ−1
0
−1 
Σ−1
0 β0 + 1
σ2 X⊤y

=
 1
σ2 X⊤X + Σ−1
0
−1
Σ−1
0 β0 +
 1
σ2 X⊤X + Σ−1
0
−1 X⊤X
σ2
(X⊤X)−1X⊤y
= (I −A)β0 + Aˆβ,
where ˆβ = (X⊤X)−1X⊤y is the OLS estimator of β and A = ( 1
σ2 X⊤X+Σ−1
0 )−1 X⊤X
σ2
.
We see that the posterior mean of β is a weighted average of the prior mean and the
OLS estimator of β. Thus, if we set the prior parameter β0 = ˆβ, the posterior mean
of β will be exactly ˆβ.
28

JUN LU
CHAPTER 1. MONTE CARLO METHODS
Step 2, conditioned on β.
Given β, again, by Bayes’ theorem, we obtain the conditional
posterior density of γ =
1
σ2 ,
posterior = p(γ = 1
σ2 | y, X, β) ∝p(y | X, β, γ) · p(γ | a0, b0)
=
γn/2
(2π)n/2 exp
n
−γ
2(y −Xβ)⊤(y −Xβ)
o
× b0a0
Γ(a0)γa0−1 exp(−b0γ)
∝γ(a0+ n
2 −1) exp

−γ

b0 + 1
2(y −Xβ)⊤(y −Xβ)

,
and the conditional posterior follows from a Gamma distribution:
posterior of γ given β = γ | y, X, β ∼G

a0 + n
2 ,

b0 + 1
2(y −Xβ)⊤(y −Xβ)

.
Prior information on the noise/precision.
We can ﬁnd an intuitive prior interpreta-
tion as follows:
1. We notice that the prior mean and posterior mean of γ are E[γ] = a0
b0 and E[γ | β] =
a0+ n
2
b0+ 1
2 (y−Xβ)⊤(y−Xβ) respectively. So the latent meaning of 2a0 is the prior sample size
for the noise σ2 = 1
γ .
2. As we assume y = Xβ + ϵ where ϵ ∼N(0, σ2I), then (y−Xβ)⊤(y−Xβ)
σ2
∼χ2(n) and
E
1
2(y −Xβ)⊤(y −Xβ)

= n
2 σ2 3. So the latent meaning of b0
a0 is the prior variance
of the noise.
3. Some textbooks would write γ ∼G(n0/2, n0σ2
0/2) to make this explicit (in which case,
n0 is the prior sample size, and σ2
0 is the prior variance). But a prior in this form
seems coming from nowhere at ﬁrst glance.
Gibbs sampler.
By this Gibbs sampling method introduced in Section 1.3.3, we can
construct a Gibbs sampler for the Bayesian linear model with semi-conjugate prior:
0. Set initial values to β and γ =
1
σ2 ;
1. update β: posterior = β | y, X, γ ∼N(β2, Σ2);
2. update γ: posterior = γ | y, X, β ∼G
 a0 + n
2 , [b0 + 1
2(y −Xβ)⊤(y −Xβ)]

.
1.4.4 Bayesian Linear Model with Full Conjugate Prior
Putting a Gamma prior over the inverse variance is equivalent to putting an inverse-Gamma
prior 4 on the variance.
Same setting as the semi-conjugate prior distribution in Sec-
tion 1.4.3. We have the likelihood function:
likelihood = y | X, β, σ2 ∼N(Xβ, σ2I).
3. χ2(n) is a Chi-squared distribution with n degrees of freedom. See Deﬁnition 2.2.6 (p. 42).
4. We again delay the deﬁnition in Deﬁnition 2.2.4 (p. 40) when we discuss regular conjugate models.
29

1.4. BAYESIAN APPETIZERS
But now we specify a joint Gaussian and inverse-Gamma prior over the weight and variance
parameters by
prior : β | σ2 ∼N(β0, σ2Σ0)
σ2 ∼G−1(a0, b0),
where again we diﬀerentiate from previous descriptions by blue text.
Equivalently, we
can formulate the prior into a joint one which is called the normal-inverse-Gamma (NIG)
distribution:
prior : β, σ2 ∼NIG(β0, Σ0, a0, b0) = N(β0, σ2Σ0) · G−1(a0, b0).
Again by the Bayes’ theorem, “posterior ∝likelihood × prior”, we obtain the posterior
posterior = p(β, σ2 | y, X) ∝p(y | X, β, σ2) · p(β, σ2 | β0, Σ0, a0, b0)
=
1
(2πσ2)n/2 exp

−1
2σ2 (y −Xβ)⊤(y −Xβ)

×
1
(2πσ2)p/2 |Σ0|1/2 exp

−1
2σ2 (β −β0)⊤Σ−1
0 (β −β0)

× b0a0
Γ(a0)
1
(σ2)a0+1 exp(−b0
σ2 )
∝
1
(2πσ2)p/2 exp
 1
2σ2 (β −β3)⊤Σ−1
3 (β −β3)

×
1
(σ2)a0+ n
2 +1 exp

−1
σ2

b0 + 1
2(y⊤y + β⊤
0 Σ−1
0 β0 −β⊤
3 Σ−1
3 β3)

,
where the parameters are
Σ3 =

X⊤X + Σ−1
0
−1
,
β3 = Σ3(X⊤y + Σ−1
0 β0) =

X⊤X + Σ−1
0
−1
(Σ−1
0 β0 + X⊤y).
Let an = a0 + n
2 + 1 and bn = b0 + 1
2(y⊤y + β⊤
0 Σ−1
0 β0 −β⊤
3 Σ−1
3 β3). The posterior is thus
a NIG distribution:
posterior = β, σ2 | y, X ∼NIG(β3, Σ3, an, bn).
Connection to zero-mean prior and semi-conjugate prior models.
We highlight
the connection of the fully conjugate model to the zero-mean prior and semi-conjugate prior
models as follows:
1. If we assume further X has full rank, when Σ−1
0
→0, β3 →ˆβ = (X⊤X)−1Xy which
reduces to the OLS estimator.
2. When b0 →∞, then σ2 →∞and β3 is approximately approaching β0, the prior
expectation of parameter. Compared to β2 in Section 1.4.3, σ2 →∞will make β2
approach to β0 where σ2 is a ﬁxed hyperparameter.
30

JUN LU
CHAPTER 1. MONTE CARLO METHODS
3. Weighted average: we reformulate
β3 =

X⊤X + Σ−1
0
−1
(Σ−1
0 β0 + X⊤y)
=

X⊤X + Σ−1
0
−1
Σ−1
0 β0 +

X⊤X + Σ−1
0
−1
(X⊤X)(X⊤X)−1X⊤y
= (I −C)β0 + C ˆβ,
where ˆβ = (X⊤X)−1X⊤y is the OLS estimator of β and C = (X⊤X+Σ−1
0 )−1(X⊤X).
We see that the posterior mean of β is a weighted average of the prior mean and the
OLS estimator of β. Thus, if we set β0 = ˆβ, the posterior mean of β will be exactly
ˆβ.
4. From an = a0 + n
2 + 1, we know that 2a0 is the prior sample size for σ2.
5. Σ−1
3
= X⊤X + Σ−1
0 : the posterior precision matrix (inverse covariance matrix) is
equal to data precision X⊤X + prior precision.
31

2
Regular Probability Models and Conjugacy
Contents
2.1
Conjugate Priors
. . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.2
Regular Univariate Models and Conjugacy
. . . . . . . . . . .
33
2.3
Exponential and Conjugacy . . . . . . . . . . . . . . . . . . . . .
49
2.4
Univariate Gaussian-Related Models
. . . . . . . . . . . . . . .
50
2.5
Multinomial Distribution and Conjugacy . . . . . . . . . . . . .
59
2.5.1
Dirichlet Distribution . . . . . . . . . . . . . . . . . . . . . . . . .
60
2.5.2
Posterior Distribution for Multinomial Distribution . . . . . . . .
62
2.6
Poisson and Multinomial
. . . . . . . . . . . . . . . . . . . . . .
65
2.7
Multivariate Gaussian Distribution and Conjugacy . . . . . . .
66
2.7.1
Multivariate Gaussian Distribution . . . . . . . . . . . . . . . . .
67
2.7.2
Multivariate Student’s t Distribution . . . . . . . . . . . . . . . .
70
2.7.3
Prior on Parameters of Multivariate Gaussian Distribution . . . .
71
2.7.4
Posterior Distribution of µ: Separated View . . . . . . . . . . . .
75
2.7.5
Posterior Distribution of Σ: Separated View . . . . . . . . . . . .
76
2.7.6
Gibbs Sampling of the Mean and Covariance: Separated View . .
77
2.7.7
Posterior Distribution of µ and Σ Under NIW: Uniﬁed View
. .
78
Parameter Choice . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
Reducing Sampling Time by Maintaining Squared sum of customers 80
2.7.8
Posterior Marginal Likelihood of Parameters . . . . . . . . . . . .
81
2.7.9
Posterior Marginal Likelihood of Data
. . . . . . . . . . . . . . .
82
2.7.10
Posterior Predictive for Data without Observations . . . . . . . .
82
2.7.11
Posterior Predictive for New Data with Observations . . . . . . .
83
2.7.12
Further Optimization via the Cholesky Decomposition . . . . . .
84
Deﬁnition
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
Rank One Update . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
Speedup for Determinant . . . . . . . . . . . . . . . . . . . . . . .
84
Update in NIW . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
32

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
2.1. Conjugate Priors
In Section 1.4.1 (p. 22), we have discussed about conjugate priors shortly. We now present
the formal deﬁnition as follows.
Deﬁnition 2.1.1 (Conjugate Prior) Given a family {p(X | θ) : θ ∈Θ} of generating
distributions, a collection of priors pω(θ) indexed by ω ∈Ωis called a conjugate prior
family if for any ω and any data, the resulting posterior equals to pω′(θ | X) for some
ω′ ∈Ω.
A toy example of the Beta-Bernoulli model can give us a better sense of the meaning behind
the conjugate priors.
Example 2.1 (Beta-Bernoulli) Suppose X = {x1, x2, ..., xN} are drawn independently
and identically distributed (i.i.d.)
from a Bernoulli distribution with parameter θ, i.e.,
Bernoulli(x | θ). Beta(θ | a, b) distribution, with a, b > 0, is conjugate to Bernoulli(x | θ),
since the posterior density is p(θ | X) = Beta(θ | a + P xi, b + N −P xi).
□
Conjugate priors make it possible to do Bayesian reasoning in a computationally eﬃcient
manner, as well as having the philosophically satisfying interpretation of representing real or
imaginary prior data. Generally, there are basically two reasons why models with conjugate
priors are popular (Robert et al., 2007; Bernardo and Smith, 2009; Hoﬀ, 2009; Gelman
et al., 2013):
• they usually allow us to derive a closed-form expression for the posterior distribution;
• they are easy to interpret, as we can easily see how the parameters of the prior change
after the Bayesian update.
2.2. Regular Univariate Models and Conjugacy
Gaussian, p. 34
Gamma, p. 36
Student’s t, p. 35
Inverse-Gamma, p. 40
Truncated-Normal, p. 50
Inverse-Gaussian, p. 56
Chi-Square, p. 42
Normal-Inv-Gamma, p. 41
Inverse-Chi-Squared, p. 44
Nor-Inv-Chi-Squared, p. 45
General-Truncated-Nor, p. 53
Half-Normal, p. 54
Laplace, p. 57
Skew-Laplace, p. 59
Rectiﬁed-Normal, p. 54
Multinomial, p. 59
Dirichlet, p. 61
Poisson, p. 65
Exponential, p. 49
Multi Gaussian, p. 67
Multi Student’s t, p. 70
Wishart, p. 71
Inverse-Wishart, p. 73
Normal-Inv-Wishart, p. 74
Table 2.1: Links for common distributions.
In most of our Bayesian matrix decomposition developments, we express the models with
univariate distributions. While in the Gaussian model, we also apply multivariate distribu-
tions. In this section, we provide rigorous deﬁnitions for common univariate distributions
and their conjugate priors. Table 2.1 provides an overview of what we will cover in this
section. With special considerations, we also discuss the multivariate Gaussian distribution
and its conjugacy in the next section.
33

2.2. REGULAR UNIVARIATE MODELS AND CONJUGACY
Figure 2.1:
Gaussian probability density
functions for diﬀerent values of the mean and
variance parameters µ and σ2.
4
2
0
2
4
x
0.0
0.2
0.4
0.6
0.8
f(x)
Gaussian Distribution PDF
=0, 
2=1
=0, 
2=0.2
=0, 
2=3
=2, 
2=0.5
Deﬁnition 2.2.1 (Gaussian or Normal Distribution) A random variable x is said
to follow the Gaussian distribution (a.k.a., a normal distribution) with mean and variance
parameters µ and σ2 > 0, denoted by x ∼N(µ, σ2) a, if
f(x; µ, σ2) =
1
√
2πσ2 exp

−1
2σ2 (x −µ)2

=
r τ
2π exp
n
−τ
2(x −µ)2o
.
The mean and variance of x ∼N(µ, σ2) are given by
E[x] = µ,
Var[x] = σ2 = τ −1,
where τ is also known as the precision of the Gaussian distribution. Figure 2.1 compares
diﬀerent parameters µ, σ2 for the Gaussian distribution.
a. Note if two random variables a and b have the same distribution, then we write a ∼b.
Suppose X = {x1, x2, ..., xN} are drawn i.i.d. from a Gaussian distribution of N(x |
µ, σ2). For conjugate Bayesian analysis, we may rewrite the Gaussian probability density
function as follows,
p(X | µ, σ2) =
N
Y
i=1
N(xi | µ, σ2)
= (2π)−N/2(σ2)−N/2 exp
(
−1
2σ2
"
N(x −µ)2 + N
N
X
n=1
(xn −x)2
#)
= (2π)−N/2(σ2)−N/2 exp

−1
2σ2

N(x −µ)2 + NSx

,
(2.1)
where Sx = PN
n=1(xn −x)2 and x =
1
N
PN
i=1 xi. This form can help ﬁnd the conditional
posterior of Gaussian likelihood under normal-inverse-Gamma prior (Equation (2.11)).
Given ﬁxed mean µ and variance σ2 parameters, we have
p(x | µ, σ2) = N(x | µ, σ2) ∝exp

−1
2σ2 x2 + µ
σ2 x

,
(2.2)
34

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
where “∝” means “proportional to”. Therefore, if we ﬁnd the form conforming to the above
equation, we can say the random variable x follows the Gaussian distribution x ∼N(µ, σ2).
See example of a Bayesian GGG matrix decomposition model in Equation (5.10) (p. 133).
While the product of two Gaussian variables remains an open problem, the sum of
Gaussian variables follows from a new Gaussian distribution.
Remark 2.1: Sum of Gaussians
Let x and y be two Gaussian distributed variables with means µx, µy and variance σ2
x, σ2
y,
respectively.
• When there is no correlation between the two variables, then it follows that
x + y ∼N(µx + µy, σ2
x + σ2
y).
• When there exists a correlation of ρ between the two variables, then it follows that
x + y ∼N(µx + µy, σ2
x + σ2
y + 2ρσxσy).
Conjugate prior for mean of a Gaussian distribution and Normal-Normal model.
Gaussian distribution is a conjugate prior of the mean parameter of a Gaussian distribution
when the variance is ﬁxed. To see this, suppose X = {x1, x2, . . . , xN} are i.i.d. normal with
mean θ and precision λ, i.e., the likelihood is N(xi | θ, λ−1) where the variance σ2 = λ−1 is
ﬁxed, and θ is given a N(µ0, λ−1
0 ) prior: θ ∼N(µ0, λ−1). Using Bayes’ theorem, “posterior
∝likelihood × prior”, the posterior density is
p(θ | X) ∝
N
Y
i=1
N(xi | θ, λ−1) × N(θ | µ0, λ−1
0 ) ∝N(θ | eµ, eλ−1),
where, given x = 1
N
PN
i=1 xi,
eµ = λ0µ0 + λ PN
i=1 xi
λ0 + Nλ
=
λ0
λ0 + Nλµ0 +
Nλ
λ0 + Nλx,
eλ = λ0 + Nλ.
(2.3)
Thus, the posterior mean is a weighted mean of the prior mean µ0 and the sample average
x; the posterior precision is the sum of the prior precision and sample precision Nλ. We
show that the Gaussian distribution is, itself, a conjugate prior for the mean parameter of a
Gaussian distribution when ﬁxing variance. This is often referred to as the Normal-Normal
model and this model can be used to provide evidence for bimodal property of human
heights (Schilling et al., 2002).
Deﬁnition 2.2.2 (Student’s t Distribution) A random variable x is said to follow
the Student’s t distribution with parameters µ, σ2 > 0, and ν, denoted by x ∼τ(µ, σ2, ν),
if
f(x; µ, σ2, ν) = Γ(ν+1
2 )
Γ(ν
2)
1
σ√νπ ×

1 + (x −µ)2
νσ2
−( ν+1
2 )
,
(2.4)
35

2.2. REGULAR UNIVARIATE MODELS AND CONJUGACY
where σ2 is called the scale parameter, and ν is the degree of freedom. The dis-
tribution has fatter tails than a Gaussian distribution. The smaller ν is, the fatter the
tail. As ν →∞, the distribution converges towards a Gaussian. A particular case of
the Student’s t distribution is the Cauchy distribution, x ∼C(µ, σ2) if x ∼τ(µ, σ2, 1),
i.e., ν = 1. The mean and variance of x ∼τ(µ, σ2, ν) are given by
E[x] =
(
µ,
if ν > 1;
undeﬁned,
if ν ≤1.
Var[x] =



ν
ν −2σ2,
if ν > 2;
∞,
if 1 < ν ≤2.
Figure 2.2 compares diﬀerent parameters µ, σ2, ν for the Student’s t distribution.
In Figure 2.2(a), we vary ν parameter for the Student’s t distribution. As ν decreases,
the distribution becomes more spread out, leading to fatter tails compared to a Gaussian
distribution. This allows for more ﬂexibility in modeling data with greater uncertainty or
outliers since the Student’s t distribution has a greater probability of observing extreme
values. In Bayesian modeling, the Student’s t distribution is often used as a prior for the
mean parameter of a Gaussian likelihood, allowing for estimation of both the mean and
precision of the data. This results in a Student’s t-Normal model.
4
2
0
2
4
x
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
f(x)
Student′s t Distribution PDF
=0, 
2=1, =2
=0, 
2=1, =3
=0, 
2=1, =5
=0, 
2=1, =100
Gaussian with =0, 
2=1
(a) Student’s t distribution by varying parameter ν.
When ν = 100, the distribution is very close to a
Gaussian distribution.
4
2
0
2
4
x
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
f(x)
Student′s t Distribution PDF
=0, 
2=1, =2
=0, 
2=2, =2
=0, 
2=3, =2
=0, 
2=4, =2
Gaussian with =0, 
2=1
(b) Student’s t distribution by varying parameter σ2.
Figure 2.2: Student’s t distribution for diﬀerent values of the parameters ν and σ2.
Deﬁnition 2.2.3 (Gamma Distribution) A random variable x is said to follow the
Gamma distribution with shape parameter r > 0 and rate parameter λ > 0 a, denoted by
x ∼G(r, λ), if
f(x; r, λ) =



λr
Γ(r)xr−1 exp(−λx),
if x ≥0;
0,
if x < 0,
36

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
where Γ(x) =
R ∞
0 xt−1 exp(−x)dt is the Gamma function and we can just take it as a
function to normalize the distribution into sum to 1. In special cases when y is a positive
integer, Γ(y) = (y −1)!. The mean and variance of x ∼G(r, λ) are given by
E[x] = r
λ,
Var[x] = r
λ2 .
Specially, let x1, x2, . . . , xn be i.i.d., random variables drawn from G(ri, λ) for each i ∈
{1, 2, . . . , n}.
Then y = Pn
i=1 xi is a random variable following from G(Pn
i=1 ri, λ).
Figure 2.3(a) compares diﬀerent parameters r, λ for the Gamma distribution.
a. Note the inverse rate parameter 1/λ is called the scale parameter. In probability theory and statistics,
the location parameter shifts the entire distribution left or right, e.g., the mean parameter of a
Gaussian distribution; the shape parameter compresses or stretches the entire distribution; the
scale parameter changes the shape of the distribution in some way.
It’s crucial to keep in mind that the deﬁnition of the Gamma distribution does not
restrict r to be a natural number and it allows for r to be any positive number. However,
when r is a positive integer, the Gamma distribution can be interpreted as a sum of r
exponentials of rate λ (see Deﬁnition 2.3.1).
The summation property holds true more
generally for Gamma variables with the same rate parameter. If x1 and x2 are random
variables from G(r1, λ) and G(r2, λ) respectively, then their sum x1+x2 is a Gamma random
variable from G(r1 + r2, λ).
0
2
4
6
8
10
x
0.0
0.5
1.0
1.5
2.0
f(x)
Gamma Distribution PDF
r=1, =2.0
r=3, =2.0
r=3, =1.0
r=3, =0.5
(a) Gamma distribution.
0
2
4
6
8
10
x
0.0
0.1
0.2
0.3
0.4
f(x)
Inverse-Gamma Distribution PDF
r=0.5, =1
r=0.5, =3
r=1, =3
r=2, =3
(b) Inverse-Gamma distribution.
Figure 2.3: Gamma and inverse-Gamma probability density functions for diﬀerent values
of the parameters r and λ.
Conjugate prior for rate of a Gamma distribution and Gamma-Gamma model.
The Gamma distribution is a conjugate prior of the rate parameter of another Gamma
distribution. Suppose data X = {x1, x2, . . . , xN} follows i.i.d. from a Gamma distribution
xi ∼G(r, λ). Suppose further the rate parameter is given a Gamma prior λ ∼G(a, a
b).
37

2.2. REGULAR UNIVARIATE MODELS AND CONJUGACY
Using Bayes’ theorem, the posterior density is
p(λ | X) ∝
N
Y
i=1
G(xi | r, λ) × G(λ | a, a
b )
∝
N
Y
i=1
λr
Γ(r)xr−1
i
exp(−λxi) × (a
b)a
Γ(a)λa−1 exp(−a
b λ)
∝λNr+a−1 exp
(
−
 N
X
i=1
xi + a
b

λ
)
∝G(λ | eα, eβ),
where
eα = Nr + a,
eβ =
N
X
i=1
xi + a
b .
That is, the posterior density of rate λ follows from a Gamma distribution. We show that
the Gamma distribution is, itself, a conjugate prior for the rate parameter of a Gamma
distribution when ﬁxing the shape parameter. This is often referred to as the Gamma-
Gamma model.
Conjugate prior for precision of a Gaussian distribution.
The Gamma distribution
is a conjugate prior to the precision parameter of a Gaussian distribution. To see this,
suppose each entry amn of matrix A is i.i.d. normal model with mean bmn and precision τ,
i.e., the likelihood is p(A | B, τ −1) = N(A | B, τ −1), the prior of τ is p(τ) = G(τ | α, β)
where A, B ∈RM×N are two matrices containing amn and bmn respectively (the result can
be applied to vector or scalar cases). Using Bayes’ theorem, it can be shown that
p(τ | A, B, α, β) ∝N(A | B, τ −1) × G(τ | α, β)
=
M,N
Y
i,j=1
N(aij | bij, (τ)−1) × βα
Γ(α)τ α−1 exp(−βτ)
∝τ
MN
2 exp


−τ
2
M,N
X
i,j=1
(aij −bij)2


· τ α−1 exp(−βτ)
= τ
MN
2
+α−1 exp


−τ


M,N
X
i,j=1
1
2(aij −bij)2 + β





∝G(τ | eα, eβ),
(2.5)
where
eα = MN
2
+ α,
eβ =
M,N
X
i,j=1
1
2(aij −bij)2 + β.
(2.6)
That is, the posterior density of precision τ follows from a Gamma distribution.
38

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
Joint conjugate prior for Gaussian mean and precision.
Going further, when the
variance/precision parameter of the Gaussian distribution is not ﬁxed with x1, x2, . . . , xN
drawn i.i.d. from a normal distribution with mean θ and precision λ. The normal-Gamma
distribution NG(α, β, µ, c), with µ ∈R and α, β, c ∈R+ is a joint distribution on (θ, λ) by
letting
λ ∼G(α, β);
θ | λ ∼N(µ, (cλ)−1).
That is, the joint p.d.f. is
p(θ, λ) = N(θ | µ, (cλ)−1) · G(λ | α, β) = NG(θ, λ | α, β, µ, c).
It turns out the posterior density is again a normal-Gamma distribution with
p(θ, λ) ∝
N
Y
i=1
N(xi | θ, λ−1) · NG(θ, λ | α, β, µ, c) ∝NG(θ, λ | eα, eβ, eµ, ec),
where
eµ = cµ + PN
i=1 xi
c + N
,
ec = c + N,
eα = α + N
2 ,
eβ = β + 1
2

cµ2 −eceµ2 +
N
X
i=1
xi

.
In contrast to the Normal-Normal, this model is often referred to as the NormalGamma-
Normal model. The posterior mean for θ is a weighted average of the prior mean and the
sample mean,
eµ = cµ + PN
i=1 xi
c + N
=
c
c + N µ +
N
c + N x,
where x =
1
N
PN
i=1 xi. From the posterior form of ec, the prior interpretation of c can be
described as the prior sample size for estimating mean parameter θ. The posterior shape
parameter eα grows linearly with sample size. And the posterior rate parameter eβ can be
written as
eβ = β + 1
2

cµ2 −eceµ2 +
N
X
i=1
xi

= β + 1
2
N
X
i=1
(xi −x)2 + 1
2
cN
c + N (x −µ)2.
In other words, it is decomposed into a prior variation, observed variation (sample variance),
and variation between the prior mean and sample mean:
eβ = (prior variation) + 1
2N(observed variation) + 1
2
cN
c + N (variation between means).
Putting a Gamma prior over the inverse variance of a Gaussian distribution is equivalent
to putting an inverse-Gamma prior on the variance. We now give the formal deﬁnition of
the inverse-Gamma distribution.
39

2.2. REGULAR UNIVARIATE MODELS AND CONJUGACY
Deﬁnition 2.2.4 (Inverse-Gamma Distribution) A random variable x is said to
follow the inverse-Gamma distribution with shape parameter r > 0 and scale parame-
ter λ > 0, denoted by x ∼G−1(r, λ), if
f(x; r, λ) =



λr
Γ(r)x−r−1 exp(−λ
x),
if x > 0;
0,
if x ≤0.
And it is denoted by x ∼G−1(r, λ). The mean and variance of inverse-gamma distribution
are given by
E[x] =



λ
r −1,
if r ≥1;
∞,
if 0 < r < 1.
Var[x] =





λ2
(r −1)2(r −2),
if r > 2;
∞,
if 0 < r ≤2.
Figure 2.3(b) compares diﬀerent parameters r, λ for the inverse-Gamma distribution.
If x is Gamma distributed, then y = 1/x is inverse-Gamma distributed. Note that the
inverse-Gamma density is not simply the Gamma density with x replaced by 1
y. There is an
additional factor of y−2. 1 The inverse-Gamma distribution is useful as a prior for positive
parameters. It imparts a quite heavy tail and keeps probability further from zero than the
Gamma distribution (see examples in Figure 2.3(b)).
Conjugate prior for variance of a Gaussian distribution.
The inverse-Gamma dis-
tribution is a conjugate prior of the variance parameter of a Gaussian distribution with
ﬁxed mean parameter. To see this, let the likelihood be p(A | B, σ2) = N(A | B, σ2) where
A, B ∈RM×N are two matrices containing elements of amn, bmn respectively (again the
result can be applied to vector or scalar cases), the prior of σ2 be p(σ2) = G−1(σ2 | α, β).
Using Bayes’ theorem, it can be shown that
p(σ2 | A, B, α, β) ∝N(A | B, σ2) × G−1(σ2 | α, β)
=
M,N
Y
i,j=1
N(aij | bij, σ2) × βα
Γ(α)(σ2)−α−1 exp(−β
σ2 )
∝
1
σMN exp


−1
2σ2
M,N
X
i,j=1
(aij −bij)2


· (σ2)−α−1 exp(−β
σ2 )
= (σ2)−MN
2
−α−1 exp


−1
σ2


M,N
X
i,j=1
1
2(aij −bij)2 + β





∝G−1(σ2 | eα, eβ),
(2.7)
1. Which is from the Jacobian in the change-of-variables formula. A short proof is provided here. Let y = 1
x
where y ∼G−1(r, λ) and x ∼G(r, λ). Then, f(y)|dy| = f(x)|dx| which results in f(y) = f(x)
 dx
dy
 =
f(x)x2
y= 1
x
=====
λr
Γ(r)y−r−1 exp(−λ
y ) for y > 0.
40

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
where
eα = MN
2
+ α,
eβ =
M,N
X
i,j=1
1
2(aij −bij)2 + β.
(2.8)
That is, the posterior density of variance σ2 is also an inverse-Gamma distribution. And as
claimed, we ﬁnd the posterior parameters in Equation (2.8) are exactly the same as that in
Equation (2.6) from a Gamma prior.
As we have seen that the normal-Gamma density is a joint conjugate prior for the
mean and precision parameters of a Gaussian distribution. The normal-inverse-Gamma
(NIG) distribution deﬁned as follows is a joint conjugate prior for the mean and variance
parameters of a Gaussian distribution.
Deﬁnition 2.2.5 (Normal-Inverse-Gamma (NIG) Distribution) The joint den-
sity of normal-inverse-Gamma distribution is a density deﬁned as
NIG(µ, σ2 | m, κ, r, λ) = N(µ | m, σ2
κ ) · G−1(σ2 | r, λ)
=
1
ZNIG(κ, r, λ)(σ2)−2r+3
2
exp

−1
2σ2

κ(m −µ)2 + 2λ

,
(2.9)
where σ2, r, λ > 0, and ZNIG(κ, r, λ) is a normalizing constant:
ZNIG(κ, r, λ) = Γ(r)
λr
r
2π
κ .
(2.10)
Figure 2.4 shows some normal-inverse-Gamma probability density functions by varying
diﬀerent parameters.
Joint conjugate prior for the Gaussian mean and variance (under NIG).
The
normal-inverse-Gamma deﬁnes an equivalent prior over the mean and variance parameter of
a Gaussian distribution as the normal-Gamma prior, but is sometimes more convenient than
the latter one. Similar to the normal-Gamma prior, when the variance and mean parameters
of the Gaussian distribution are not ﬁxed with N data points X = {x1, x2, . . . , xN} drawn
i.i.d. from a normal distribution with mean µ and variance σ2. The normal-inverse-Gamma
NIG(m0, κ0, r0, λ0) with m0 ∈R and r0, λ0, κ0 ∈R+ is a joint distribution on µ, σ2 by
letting
σ2 ∼G−1(r0, λ0);
µ | σ2 ∼N(m0, σ2
κ0
).
With this prior, µ and σ2 decouple, and the posterior conditional densities of µ and σ2 are
Gaussian and inverse-Gamma respectively. The joint p.d.f of NIG prior can be written as
p(µ, σ2) = N(m0, σ2
κ0
) · G−1(r0, λ0) = NIG(µ, σ2 | m0, κ0, r0, λ0).
41

2.2. REGULAR UNIVARIATE MODELS AND CONJUGACY
Again, by Bayes’ theorem “posterior ∝likelihood × prior”, the posterior of the µ and
σ2 parameters under the NIG prior is
p(µ, σ2 | X, β)
∝N(X | µ, σ2) · NIG(µ, σ2 | β)
∝
N
Y
i=1
N(xi | µ, σ2) · NIG(µ, σ2 | m0, κ0, r0, λ0)
⋆=
C
(σ2)
2r0+3+N
2
exp

−1
2σ2

N(x −µ)2 + NSx

exp

−1
2σ2

2λ0 + κ0(m0 −µ)2
∝(σ2)−2rN +3
2
exp

−1
2σ2

λN + κN(mN −µ)2
∝NIG(µ, σ2 | mN, κN, rN, λN).
(2.11)
where β = {m0, κ0, r0, λ0}, C =
(2π)−N/2
ZN IG(κ0,r0,λ0), equality (⋆) is from Equation (2.1), and
mN = κ0m0 + Nx
κN
= κ0
κN
m0 + N
κN
x,
κN = κ0 + N,
rN = r0 + N
2 ,
λN = λ0 + 1
2(NSx + Nx2 + κ0m2
0 −κNm2
N)
= λ0 + 1
2

NSx +
κ0N
κ0 + N (x −m0)2

.
where Sx = PN
n=1(xn −x)2 and x = 1
N
PN
i=1 xi. Note in the above derivation, we use the
fact about the likelihood under Gaussian in Equation (2.1). We will discuss the posterior
marginal likelihood in the normal-inverse-Chi-squared (NIX) case. Further discussion on
the posterior marginal likelihood for the NIG prior can be found in Murphy (2007). We
will leave this to the readers as it is rather similar as that in the NIX prior.
Another distribution that is closely related to the Gamma distribution is called the Chi-
squared distribution which is extensively used in the distribution theory of linear models
(Lu, 2022a). The rigorous deﬁnition is given as follows.
Deﬁnition 2.2.6 (Chi-Squared Distribution) Let A ∼N(0, Ip) where Ip is a p × p
identity matrix. Then x = Pp
i aii follows the Chi-squared distribution with p degrees of
freedom. We write x ∼χ2(p), and we can see this is equivalent to x ∼G(p/2, 1/2):
f(x; p) =





1
2p/2Γ(p
2)x
p
2 −1 exp(−x
2),
if x ≥0;
0,
if x < 0.
42

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
0.5
0.0
0.5
0.4
0.6
0.8
1.0
1.2
1.4
2
m=0, =1, r=1, =1
6
0.09
0.12
0.15
0.18
0.21
0.24
0.27
0.5
0.0
0.5
2
m=0, =1, r=2, =1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.5
0.0
0.5
2
m=0, =1, r=3, =1
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
1.8
(a) Contour plot of normal-inverse-Gamma density by varying parameter r (purple=low, yellow=high).
0.5
0.0
0.5
0.4
0.6
0.8
1.0
1.2
1.4
1.6
2
m=0, =1, r=1, =1
0.03
0.06
0.09
0.12
0.15
0.18
0.21
0.24
0.27
0.5
0.0
0.5
2
m=0, =1, r=1, =2
0.030
0.045
0.060
0.075
0.090
0.105
0.5
0.0
0.5
2
m=0, =1, r=1, =3
0.018
0.036
0.042
0.048
0.054
0.060
(b) Contour plot of normal-inverse-Gamma density by varying parameter λ (purple=low, yellow=high).
0.5
0.0
0.5
0.4
0.6
0.8
1.0
1.2
1.4
1.6
2
m=0, =1, r=1, =1
0.03
0.06
0.09
0.12
0.15
0.18
0.21
0.24
0.27
0.5
0.0
0.5
2
m=0, =2, r=1, =1
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.5
0.0
0.5
2
m=0, =3, r=1, =1
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
(c) Contour plot of normal-inverse-Gamma density by varying parameter κ (purple=low, yellow=high).
0.5
0.0
0.5
0.4
0.6
0.8
1.0
1.2
1.4
1.6
2
m=0, =1, r=1, =1
0.03
0.06
0.09
0.12
0.15
0.18
0.21
0.24
0.27
0.5
1.0
1.5
2
m=1, =1, r=1, =1
0.06
0.09
0.12
0.15
0.18
0.21
0.24
0.27
1.5
2.0
2.5
2
m=2, =1, r=1, =1
0.06
0.09
0.12
0.15
0.18
0.21
0.24
0.27
(d) Contour plot of normal-inverse-Gamma density by varying parameter m (purple=low, yellow=high).
Figure 2.4:
Normal-inverse-Gamma probability density functions by varying diﬀerent
parameters.
43

2.2. REGULAR UNIVARIATE MODELS AND CONJUGACY
The mean, variance of x ∼χ2(p) are given by
E[x] = p,
Var[x] = 2p.
Figure 2.5(a) compares diﬀerent degrees of freedom p for the Chi-squared distribution.
0
2
4
6
8
10
x
0.0
0.2
0.4
0.6
0.8
1.0
1.2
f(x)
Chi-Squared Distribution PDF
p=1
p=2
p=3
p=4
(a) Chi-squared distribution.
0
2
4
6
8
10
x
0.0
0.1
0.2
0.3
0.4
f(x)
Inverse-Chi-Squared Distribution PDF
=0.25, s2=0.25
=0.25, s2=0.75
=0.5, s2=1.5
=1.0, s2=3.0
(b) Inverse-Chi-squared distribution.
Figure 2.5: Chi-squared and inverse-Chi-squared probability density functions for diﬀerent
values of parameters.
The analog of the inverse-Gamma distribution is known as the inverse-Chi-squared dis-
tribution. Following the deﬁnition of the inverse-Gamma distribution in Deﬁnition 2.2.4, we
provide the rigorous deﬁnition of the inverse-Chi-squared distribution as follows.
Deﬁnition 2.2.7 (Inverse-Chi-Squared Distribution) A random variable x is said
to follow the inverse-Chi-squared distribution with parameter ν > 0 and s2 > 0, denoted
by x ∼G−1(ν
2, νs2
2 ) if
f(x; ν, s2) =





(νs2
2 )
ν
2
Γ(ν
2) x−ν
2 −1 exp(−νs2
2x ),
if x > 0;
0,
if x ≤0.
And it is denoted by x ∼χ−2(ν, s2). The parameter ν > 0 is called the degrees of
freedom, and s2 > 0 is the scale parameter. And it is also known as the scaled
inverse-Chi-squared distribution. The mean and variance of the inverse-Chi-squared dis-
tribution are given by
E[x] =



νs2
ν −2,
if ν ≥2;
∞,
if 0 < ν < 2.
Var[x] =





2ν2s4
(ν −2)2(ν −4),
if ν ≥4;
∞,
if 0 < ν < 4.
To make a connection to the inverse-Gamma distribution, we can set S = νs2. Then the
inverse-Chi-squared distribution can also be denoted by x ∼G−1(ν
2, S
2 ) if x ∼χ−2(ν, s2)
the form of which conforms to the univariate case of the inverse-Wishart distribution
44

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
(Deﬁnition 2.7.4, p. 73). And we will see the similarity in the posterior parameters too.
Figure 2.5(b) compares diﬀerent parameters ν, s2 for the inverse-Chi-squared distribution.
Exercise 2.1 (Conjugate prior for variance of a Gaussian distribution) Show that
the inverse-Chi-squared distribution is a conjugate prior for the Gaussian variance param-
eter when the mean parameter is ﬁxed. Hint: the derivation is just the same as that in the
inverse-Gamma case.
As we have seen the normal-inverse-Gamma distribution is a joint conjugate prior for
Gaussian mean and variance parameters. The normal-inverse-Chi-squared (NIX) distribu-
tion deﬁned as follows is an alternative joint conjugate prior.
Deﬁnition 2.2.8 (Normal-Inverse-Chi-Squared (NIX) Distribution) Similar to
the normal-inverse-Gamma distribution, the normal-inverse-Chi-squared (NIX) distri-
bution is deﬁned as (where again we set S = νs2 as that in the inverse-Chi-square
distribution to make a connection to the normal-inverse-Gamma density)
NIX(µ, σ2 | m, κ, ν, S) = N(µ | m, σ2
κ ) · χ−2(σ2 | ν, s2)
=
1
ZNIX (κ, ν, s2)(σ2)−(ν/2+3/2) exp

−1
2σ2

νs2 + κ(m −µ)2
,
S=νs2
======
1
ZNIX (κ, ν, s2)(σ2)−(ν/2+3/2) exp

−1
2σ2

S + κ(m −µ)2
(2.12)
where σ2, ν, s2 > 0, and ZNIX (κ, ν, s2) is a normalizing constant:
ZNIX (κ, ν, s2) = Γ
 ν
2
  2
νs2
ν/2
r
2π
κ = Γ
 ν
2
  2
S
ν/2
r
2π
κ .
(2.13)
The normal-inverse-Chi-squared distribution can also be denoted by x ∼NIG(m, κ, ν
2, S
2 )
if x ∼NIX(m, κ, ν, s2) the form of which conforms to the univariate case of the normal-
inverse-Wishart distribution (Equation (2.40), p. 78). And we will see the similarity in
the posterior parameters as well.
Joint conjugate prior for the Gaussian mean and variance (under NIX).
Sim-
ilar to the normal-inverse-Gamma prior, when the variance and mean parameters of the
Gaussian distribution are not ﬁxed with N data points X = {x1, x2, . . . , xN} drawn i.i.d.
from a normal distribution with mean µ and variance σ2. The normal-inverse-Chi-squared
NIX(m0, κ0, ν0, S0 = ν0σ2
0) with m0 ∈R and κ0, µ0, S0 ∈R+ is a joint distribution on
µ, σ2 by letting
σ2 ∼χ−2(ν0, σ2
0);
µ | σ2 ∼N(m0, σ2
κ0
).
45

2.2. REGULAR UNIVARIATE MODELS AND CONJUGACY
Again, by Bayes’ theorem “posterior ∝likelihood × prior”, the conditional posterior of the
µ and σ2 parameters under the NIX prior is
p(µ, σ2 | X, β)
∝p(X | µ, σ2)p(µ, σ2 | β) = p(X, µ, σ2 | β)
=
C
(σ2)
ν0+3+N
2
exp

−1
2σ2

N(x −µ)2 + NSx

exp

−1
2σ2

S0 + κ0(m0 −µ)2
= C × (σ2)−νN +3
2
exp

−1
2σ2

SN + κN(mN −µ)2
∝NIX(µ, σ2 | mN, κN, νN, SN) = N(µ | mN, σ2
κN
) · χ−2(σ2 | νN, σ2
N),
(2.14)
where β = {m0, κ0, ν0, S0 = ν0σ2
0}, C =
(2π)−N/2
ZN IX (κ0,ν0,σ2
0), and
mN = κ0m0 + Nx
κN
= κ0
κN
m0 + N
κN
x,
κN = κ0 + N,
νN = ν0 + N,
SN = S0 + NSx + Nx2 + κ0m2
0 −κNm2
N
= S0 + NSx +
κ0N
κ0 + N (x −m0)2,
νNσ2
N = SN
leads to
−−−−−→
σ2
N = SN
νN
,
Then the posterior density is a normal-inverse-Chi-squared density. 2
Suppose ν0 ≥2, or N ≥2 such that νN ≥2, the posterior expectations are given by
E[µ | X, β] = mN,
E[σ2 | X, β] =
SN
νN −2.
Marginal posterior of σ2.
Integrate out µ in the posterior, we have
p(σ2 | X, β) =
Z
µ
p(µ, σ2 | X, β)dµ
=
Z
µ
N(µ | mN, σ2
κN
) · χ−2(σ2 | νN, σ2
N)dµ
= χ−2(σ2 | νN, σ2
N),
which is just an integral over a Gaussian distribution.
2. This posterior shares the same form as that in the multivariate case from Equation (2.45) except the N in
NSx which results from the diﬀerence between the multivariate Gaussian distribution and the univariate
Gaussian distribution. Similarly, in the inverse-Chi-squared language, we can show the νNσ2
N = SN.
46

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
Marginal posterior of µ.
Integrate out σ2 in the posterior, we have
p(µ | X, β) =
Z
σ2 p(µ, σ2 | X, β)dσ2
=
Z
σ2 N(µ | mN, σ2
κN
) · χ−2(σ2 | νN, σ2
N)dσ2
=
Z
σ2 C(σ2)−νN +3
2
exp

−1
2σ2

SN + κN(mN −µ)2
dσ2.
Let φ = σ2 and α = (νN + 1)/2, A = SN + κN(mN −µ)2, and x = A
2φ, we have
dφ
dx = −A
2 x−2.
where A can be easily veriﬁed to be positive and φ = σ2 > 0. It follows that
p(µ | X, β) =
Z ∞
0
C(φ)−α−1 exp

−A
2φ

dφ
=
Z 0
∞
C( A
2x)−α−1 exp (−x) (−A
2 x−2)dx
(since x = A
2φ)
=
Z ∞
0
C( A
2x)−α−1 exp (−x) (A
2 x−2)dx
= (A
2 )−α
Z
x
Cxα−1 exp (−x) dx
= (A
2 )−α(C · Γ(1))
Z
x
G(x | α, 1)dx
(see Deﬁnition 2.2.3)
= (C · Γ(1))

νNσ2
N + κN(mN −µ)2−νN +1
2
(a)
= (C · Γ(1))(νNσ2
N)−νN +1
2

1 +
κN
νNσ2
N
(mN −µ)2
−νN +1
2
We notice that C is deﬁned in Equation (2.14) (in terms of {κN, νN, σ2
N}) that
C
(b)
=
(2π)−N/2
ZNIX (κN, νN, σ2
N) =
(2π)−N/2
√
(2π)
√κN Γ(νN
2 )(
2
νNσ2
N )νN/2
∝(νNσ2
N)νN/2.
Combine equalities (a) and (b) above, we obtain
p(µ | X, β) ∝
1
σN/√κN

1 +
κN
νNσ2
N
(µ −mN)2
−νN +1
2
∝τ(µ | mN, σ2
N/κN, νN),
which is a univariate Student’s t distribution (Deﬁnition 2.2.2, p. 35).
47

2.2. REGULAR UNIVARIATE MODELS AND CONJUGACY
Marginal likelihood of data.
By Equation (2.14), we can get the marginal likelihood
of data under hyperparameter β = (m0, κ0, ν0, S0 = ν0σ2
0)
p(X | β) =
Z
µ
Z
σ2 p(X, µ, σ2 | β)dµdσ2
=
(2π)−N/2
ZNIX (κ0, ν0, σ2
0)
Z
µ
Z
σ2(σ2)−νN +3
2
exp

−1
2σ2

SN + κN(mN −µ)2
dµdσ2
= (2π)−N/2 ZNIX (κN, νN, σ2
N)
ZNIX (κ0, ν0, σ2
0)
= (π)−N/2 Γ(νN/2)
Γ(ν0/2)
r κ0
κN
(ν0σ2
0)ν0/2
(νNσ2
N)νN/2 .
Posterior predictive for new data with observations.
Let the number of samples
for data set {x⋆, X} be N⋆= N + 1, we have
p(x⋆| X, β) = p(x⋆, X | β)
p(X | β)
=

(2π)−N⋆/2 ZNIX (κN⋆, νN⋆, σ2
N⋆)
ZNIX (κ0, ν0, σ2
0)
  
(2π)−N/2 ZNIX (κN, νN, σ2
N)
ZNIX (κ0, ν0, σ2
0)

= (2π)−1/2 ZNIX (κN⋆, νN⋆, σ2
N⋆)
ZNIX (κN, νN, σ2
N)
= (π)−1/2
r κN
κN⋆
Γ(νN⋆
2 )
Γ(νN
2 )
(νNσ2
N)
νN
2
(νN⋆σ2
N⋆)
νN⋆
2
= Γ(νN+1
2
)
Γ(νN
2 )
s
κN
(κN + 1)
1
(πνNσ2
N)
(νN⋆σ2
N⋆)
(νNσ2
N)
−νN +1
2
.
(2.15)
We realize that
mN = κN⋆mN⋆−x⋆
κN
= (κ0 + N + 1)mN⋆−x⋆
κ0 + N
,
mN⋆= κNmN + x⋆
κN⋆
= (κ0 + N)mN + x⋆
κ0 + N + 1
,
SN⋆= SN + x⋆x⋆T −κN⋆m2
N⋆+ κNm2
N
= SN + κN + 1
κN
(mN⋆−x⋆)2
= SN +
κN
κN + 1(mN −x⋆)2,
Thus, we have
(νN⋆σ2
N⋆)
(νNσ2
N)
−νN +1
2
=
SN⋆
SN
−νN +1
2
= 1 + κN(mN −x⋆)2
(κN + 1)νNσ2
N
.
(2.16)
48

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
Substitute Equation (2.16) into Equation (2.15), it follows that
p(x⋆| X, β) = Γ(νN+1
2
)
Γ(νN
2 )
s
κN
(κN + 1)
1
(πνNσ2
N)

1 + κN(mN −x⋆)2
(κN + 1)νNσ2
N
−νN +1
2
= τ(x⋆| mN, κN + 1
κN
σ2
N, νN).
Posterior predictive for new data without observations.
Similarly, we have
p(x⋆| β) =
Z
µ
Z
σ2 p(x⋆, µ, σ2 | β)dµdσ2
= (2π)−1/2 ZNIX (κ1, ν1, σ2
1)
ZNIX (κ0, ν0, σ2
0)
= (π)−1/2
rκ0
κ1
Γ(ν1
2 )
Γ(ν0
2 )
(ν0σ2
0)
ν0
2
(ν1σ2
1)
ν1
2
= Γ(ν0+1
2 )
Γ(ν0
2 )
s
κ0
(κ0 + 1)
1
(πν0σ2
0)
(ν1σ2
1)
(ν0σ2
0)
−ν0+1
2
= τ
 x⋆| m0, κ0 + 1
κ0
σ2
0, ν0

.
2.3. Exponential and Conjugacy
The exponential distribution is a probability distribution commonly used in modeling events
that happen randomly over time, such as the time elapsed until the occurrence of a certain
event, or the time between two consecutive events. It is a special Gamma distribution with
support on nonnegative real values.
Deﬁnition 2.3.1 (Exponential Distribution) A random variable x is said to follow
the exponential distribution with rate parameter λ > 0, denoted by x ∼E(λ), if
f(x; λ) =
(
λ exp(−λx),
if x ≥0;
0,
if x < 0.
We can see this is equivalent to x ∼G(1, λ). The mean and variance of x ∼E(λ) are
given by
E[x] = λ−1,
Var[x] = λ−2.
The support of an exponential distribution is on (0, ∞). Figure 2.6 compares diﬀerent
parameters λ for the exponential distribution.
Note that the average λ−1 is the average time until the occurrence of the event of interest so
that λ is interpreted as a rate parameter. An important property of the exponential distri-
bution is that it is “memoryless”, meaning that the probability of waiting for an additional
amount of time x depends only on x, not on the past waiting time.
49

2.4. UNIVARIATE GAUSSIAN-RELATED MODELS
Figure 2.6: Exponential probability density
functions for diﬀerent values of the rate pa-
rameter λ.
0
2
4
6
8
10
x
0.0
0.2
0.4
0.6
0.8
1.0
f(x)
Exponential Distribution PDF
=0.25
=0.5
=1
=2
=10
Remark 2.2: Property of Exponential Distribution
Let x ∼E(λ). Then we have p(x ≥x + s | x ≥s) = p(x ≥x).
Conjugate prior for the exponential rate parameter.
The Gamma distribution is
a conjugate prior of the rate parameter of an exponential distribution. To see this, suppose
X = {x1, x2, . . . , xN} are drawn i.i.d. from an exponential distribution with rate λ, i.e.,
the likelihood is E(x | λ), and λ is given a G(α0, β0) prior: λ ∼G(α0, β0). Using Bayes’
theorem, the posterior is
p(λ | X) ∝
N
Y
i=1
E(xi | λ) × G(λ | α0, β0) ∝G(θ | eα, eβ),
where
eα = α0 + N,
eβ = β0 +
N
X
i=1
xi.
(2.17)
From this posterior form, the prior parameter α0 can be interpreted as the number of prior
observations, and β0 as the sum of the prior observations. The posterior mean is
eα
eβ
=
α0 + N
β0 + PN
i=1 xi
.
2.4. Univariate Gaussian-Related Models
The truncated-normal (TN) distribution is a variant of the normal distribution, where the
values smaller than zero are excluded. In other words, it is a normal distribution that is
“cut oﬀ” at zero. The support of the distribution is nonnegative real such that it can be
applied in a nonnegative matrix factorization context.
Deﬁnition 2.4.1 (Truncated-Normal (TN) Distribution) A random variable x is
said to follow the truncated-normal distribution with “parent” mean µ and “parent” pre-
50

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
cision τ > 0, denoted by x ∼T N(µ, τ −1), if
f(x; µ, τ −1) =





p τ
2π exp{−τ
2(x −µ)2}
1 −Φ(−µ√τ)
,
if x ≥0;
0,
if x < 0,
where Φ(y) =
R y
−∞N(u | 0, 1)du =
1
√
2π
R y
−∞exp(−u2
2 )du is the cumulative distribution
function (c.d.f.) of N(0, 1), the standard normal distribution. Generally, the cumulative
density function of y ∼N(µ, σ2) can be written as
F(y) = p(y ≤y) = Φ
y −µ
σ

= Φ
 (y −µ) · √τ

.a
The mean and variance of x ∼T N(µ, τ −1) are given by
E[x] = µ −1
√τ ·
−φ(α)
1 −Φ(α),
Var[x] = 1
τ
 
1 +
αφ(α)
1 −Φ(α) +
 αφ(α)
1 −Φ(α)
2!
,
where φ(y) =
1
√
2π exp(−y2
2 ) is the p.d.f. of the standard normal distribution, and α =
−µ · √τ (Burkardt, 2014). Figure 2.7(a) compares diﬀerent parameters µ, τ for the TN
distribution. Figure 2.8(a) shows the mean value of the TN distribution by varying µ
given ﬁxed τ; we can ﬁnd when µ →−∞, the mean is approaching zero.
a. Or equivalently,
for general Gaussian distribution y
∼
N(µ, σ2),
the c.d.f.
is F(y)
=
1
2
n
1 + erf

y−µ
σ
√
2
o
, where the error function is erf(t) =
2
√π
R t
0 exp(−y2)dy.
Conjugate prior for the nonnegative mean parameter of a Gaussian.
We have
shown that a Gaussian distribution is a conjugate prior of the mean parameter of another
Gaussian distribution when the variance is ﬁxed previously. The truncated-normal distribu-
tion is also a conjugate prior of the nonnegative mean parameter of a Gaussian distribution
when the variance is ﬁxed. To see this, suppose X = {x1, x2, . . . , 0xN} are drawn i.i.d. from
a normal distribution with mean θ and precision τ, i.e., the likelihood is N(x | θ, τ −1) where
the variance σ2 = τ −1 is ﬁxed, and θ is given a T N(µ0, τ −1
0 ) prior: θ ∼T N(µ0, τ −1
0 ). Using
Bayes’ theorem, the posterior is
p(θ | X) ∝
N
Y
i=1
N(xi | θ, τ −1) × T N(θ | µ0, τ −1
0 )
∝exp
(
−τ0 + Nτ
2
θ2 +
 τ
N
X
i=1
xi + τ0µ0

θ
)
· u(θ)
∝T N(θ | eµ, eτ −1),
(2.18)
51

2.4. UNIVARIATE GAUSSIAN-RELATED MODELS
where u(y) is the step function with value 1 if y ≥0 and value 0 if y < 0, and
eµ = τ0µ0 + τ PN
i=1 xi
τ0 + Nτ
,
eτ = τ0 + Nτ.
The posterior parameters are exactly the same as those in the Normal-Normal model (Equa-
tion (2.3)) and the posterior “parent” mean can also be written as a weighted mean of µ0
and x.
0
1
2
3
4
5
x
0.0
0.2
0.4
0.6
0.8
f(x)
Truncated-Normal Distribution PDF
=0.5, =1
=0.5, =3
=1, =3
=2, =3
(a) Truncated-normal.
1.0
1.5
2.0
2.5
3.0
x
0.0
0.5
1.0
1.5
2.0
2.5
f(x)
General-Truncated-Normal Distribution PDF
=0.5, =1, a=1, b=3
=0.5, =3, a=1, b=3
=1, =3, a=1, b=3
=2, =3, a=1, b=3
(b) General-truncated-normal.
Figure 2.7: Truncated-normal and general-truncated-normal probability density functions
for diﬀerent values of the parameters µ and τ.
1.0
0.5
0.0
0.5
1.0
1.5
2.0
0.0
0.5
1.0
1.5
2.0
E[x]
Mean of Truncated-Normal
=1
=10
=25
=50
(a) Truncated-normal.
2
0
2
4
6
8
0
1
2
3
4
5
6
7
E[x]
Mean of General-Truncated-Normal
=1, a=0, b=10
=1, a=1, b=10
=1, a=3, b=10
=1, a=4, b=10
(b) General-truncated-normal.
Figure 2.8: Mean of truncated-normal and general-truncated-normal distribution by vary-
ing µ, τ, a, and b parameters.
Going further from the truncated-normal distribution, the general-truncated-normal
(GTN) distribution is also a variant of the normal distribution, where the values outside a
certain range are excluded. In other words, it is a normal distribution that is “cut oﬀ” at
some speciﬁed lower and/or upper bound. The range between the lower and upper bound
is called the support of the distribution. 3
3. In some contexts, the general-truncated-normal distribution is named the truncated-normal directly. We
here diﬀerentiate the two as the deﬁnitions describe.
52

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
Deﬁnition 2.4.2 (General-Truncated-Normal (GTN) Distribution) A random vari-
able x is said to follow the general-truncated-normal distribution with “parent” mean µ
and “parent” precision τ > 0, denoted by x ∼GT N(µ, τ −1, a, b), if
f(x; µ, τ −1, a, b) =









0,
if x < a;
p τ
2π exp{−τ
2(x −µ)2}
Φ((b −µ) · √τ) −Φ((a −µ) · √τ),
if a ≤x ≤b;
1,
if 0 > b,
where Φ(·) is the c.d.f. of N(0, 1), the standard normal distribution. The mean and
variance of x ∼GT N(µ, τ −1, a, b) are given by
E[x] = µ −1
√τ · φ(β) −φ(α)
Φ(β) −Φ(α),
Var[x] = 1
τ
 
1 −βφ(β) −αφ(α)
Φ(β) −Φ(α)
−
βφ(β) −αφ(α)
Φ(β) −Φ(α)
2!
,
where φ(·) is the p.d.f. of the standard normal distribution, and
α = (a −µ) · √τ,
β = (b −µ) · √τ.
Note that, the truncated-normal distribution is a special general-truncated-normal with
a = 0 and b = ∞(Burkardt, 2014). Figure 2.7(b) compares diﬀerent parameters µ, τ for
the GTN distribution. Figure 2.8(b) shows the mean value of the GTN distribution by
varying µ given ﬁxed τ, a, b; we again ﬁnd when µ →−∞, the mean is approaching zero.
Conjugate prior for the constrained mean parameter of a Gaussian.
We have
shown that a Gaussian distribution is a conjugate prior of the mean parameter of another
Gaussian distribution when the variance is ﬁxed previously. The truncated-normal distribu-
tion is also a conjugate prior of the nonnegative mean parameter of a Gaussian distribution
when the variance is ﬁxed. To see this, suppose X = {x1, x2, . . . , xN} are drawn i.i.d. from
a normal distribution with mean θ and precision τ, i.e., the likelihood is N(x | θ, τ −1) where
the variance σ2 = τ −1 is ﬁxed, and θ is given a T N(µ0, τ −1
0 ) prior: θ ∼T N(µ0, τ −1
0 ). Using
Bayes’ theorem, the posterior is
p(θ | X) ∝
N
Y
i=1
N(xi | θ, τ −1) × GT N(θ | µ0, τ −1
0 , a, b)
∝exp
(
−τ0 + Nτ
2
θ2 +
 τ
N
X
i=1
xi + τ0µ0

θ
)
· 1(a ≤θ ≤b)
∝GT N(θ | eµ, eτ −1, a, b),
(2.19)
where 1(a ≤y ≤b) is the step function with value 1 if a ≤y ≤b and value 0 otherwise,
and
eµ = τ0µ0 + τ PN
i=1 xi
τ0 + Nτ
,
eτ = τ0 + Nτ.
53

2.4. UNIVARIATE GAUSSIAN-RELATED MODELS
The posterior parameters are again exactly the same as those in the Normal-Normal model
(Equation (2.3)).
0
1
2
3
4
5
x
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
f(x)
Half-Normal Distribution PDF
=0.5, =1
=0.5, =3
=1, =3
=2, =3
(a) Half-normal.
0
1
2
3
4
5
x
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
f(x)
Gaussian Distribution PDF
=0.5, =1
=0.5, =3
=1, =3
=2, =3
(b) Normal, τ is the precision parameter.
Figure 2.9: Half-normal and normal probability density functions for diﬀerent values of
the parameters µ and τ. The probability density of any value x ≥µ in the half-normal
distribution is twice as that in the normal distribution with the same parameters µ, τ.
The half-normal distribution is a special case of the normal distribution, where the
support is larger than the parameter µ of a normal distribution (known as the “parent”
mean parameter of the normal distribution) and the distribution is symmetrical around
µ. This distribution is often used in modeling the scale or standard deviation of a process
where the values cannot be smaller than µ.
Deﬁnition 2.4.3 (Half-Normal Distribution) A random variable x is said to follow
the half-normal distribution with “parent” mean µ and “parent” precision τ > 0, denoted
by x ∼HN(µ, τ −1), if
f(x; µ, τ −1) =





r
2τ
π exp
n
−τ
2(x −µ)2o
,
if x ≥µ;
0,
if x < µ.
The mean and variance of x ∼HN(µ, τ −1) are given by
E[x] = µ +
r
2
πτ ,
Var[x] = 1
τ
 1 −2
π

.
Figure 2.9(a) compares diﬀerent parameters µ, τ for the half-normal distribution.
In this text, the rectiﬁed-normal (RN) distribution is deﬁned as proportional to the
product of a Gaussian distribution and an exponential distribution. And it can also be
called the exponentially rectiﬁed-normal distribution.
Deﬁnition 2.4.4 (Rectiﬁed-Normal (RN) Distribution) A random variable x is
said to follow the rectiﬁed-normal distribution (or exponentially rectiﬁed-normal dis-
54

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
tribution) with “parent” mean µ, “parent” precision τ > 0, and “parent” rate λ > 0,
denoted by x ∼RN(µ, τ −1, λ), if
f(x; µ, τ −1, λ) = 1
C · N(x | µ, τ −1) · E(x | λ)
∝exp
(
−τ
2

x −τµ −λ
τ
2)
· u(x)
∝T N(x | τµ −λ
τ
, τ −1),
where T N(·) is the density function of a truncated-normal distribution, and C is a
constant value,
C = CRN(µ, τ, λ) = λ

1 −Φ

−τµ −λ
√τ

· exp

−µλ + λ2
2τ

.
(2.20)
That is, the rectiﬁed-normal distribution is a special truncated-normal distribution with
more ﬂexibility. The mean and variance of x ∼RN(µ, τ −1, λ) are given by
E[x] = τµ −λ
τ
−1
√τ ·
−φ(α)
1 −Φ(α),
Var[x] = 1
τ
(
1 +
αφ(α)
1 −Φ(α) +
 αφ(α)
1 −Φ(α)
2)
.
where α = −τµ−λ
τ
· √τ. Figure 2.10(b) compares diﬀerent parameters µ, τ for the RN
distribution.
The comparison between the truncated-normal and rectiﬁed-normal distributions is pre-
sented in Figure 2.10.
Conjugate prior for the nonnegative mean parameter of a Gaussian by RN.
Like
TN distribution, the RN distribution also serves to enforce nonnegative constraint, and is
conjugate to the Gaussian likelihood.
However, due to the extra parameter λ, the RN
distribution is more ﬂexible in this sense. And the derivation follows from Equation (2.18).
To see this, suppose X = {x1, x2, . . . , xN} are drawn i.i.d. from a normal distribution with
mean θ and precision τ, i.e., the likelihood is N(x | θ, τ −1) where the variance σ2 = τ −1 is
ﬁxed, and θ is given a RN(µ0, τ −1
0 , λ0) prior: θ ∼RN(µ0, τ −1
0 , λ0). Using Bayes’ theorem,
the posterior is
p(θ | X) ∝
N
Y
i=1
N(xi | θ, τ −1) × RN(θ | µ0, τ −1
0 , λ0)
=
N
Y
i=1
N(xi | θ, τ −1) × T N(θ | m0, τ −1
0 )
(m0 = τ0µ0 −λ0
τ0
)
∝T N(θ | eµ, eτ −1),
(2.21)
55

2.4. UNIVARIATE GAUSSIAN-RELATED MODELS
where
eµ = τ0m0 + τ PN
i=1 xi
τ0 + Nτ
,
eτ = τ0 + Nτ.
That is, the posterior density is a special RN or TN distribution.
0
1
2
3
4
5
x
0.0
0.2
0.4
0.6
0.8
f(x)
Truncated-Normal Distribution PDF
=0.5, =1
=0.5, =3
=1, =3
=2, =3
(a) Truncated-normal. Same as Figure 2.7(a).
0
1
2
3
4
5
x
0.0
0.2
0.4
0.6
0.8
1.0
f(x)
Rectified-Normal Distribution PDF
=0.5, =1, =1
=0.5, =3, =1
=1, =3, =1
=2, =3, =1
(b) Rectiﬁed-normal.
Figure 2.10: Truncated-normal and rectiﬁed-normal probability density functions for dif-
ferent values of the parameters µ, τ, and λ.
The inverse-Gaussian distribution, also known as the Wald distribution, is a continuous
probability distribution with two parameters, µ > 0 and b > 0. It is a versatile distribution
that is used in various applications including modeling waiting times, stock prices, and
lifetimes of mechanical systems. An important property of the distribution is that it is
well-suited for modeling nonnegative, continuous, and positively skewed data with ﬁnite
mean and variance.
Deﬁnition 2.4.5 (Inverse-Gaussian Distribution) A random variable x is said to
follow the inverse-Gaussian distribution with parameters µ > 0 and b > 0, denoted by
x ∼N −1(µ, λ) a, if
f(x; µ, λ) =





r
λ
2πx3 exp

−λ(x −µ)2
2µ2x

,
if x ≥0;
0,
if x < 0.
The mean and variance of x ∼N −1(µ, λ) are given by
E[x] = µ,
Var[x] = µ3
λ .
The support of an inverse-Gaussian distribution is on (0, ∞).
Figure 2.11 compares
diﬀerent parameters µ, λ for the inverse-Gaussian distribution.
a. Note we use N −1 to denote the inverse-Gaussian distribution and use G−1 to denote the inverse-
Gamma distribution.
The Laplace distribution, a.k.a., the double exponential distribution, is named after
Pierre-Simon Laplace (1749-1827), who obtained the distribution in 1774 (Kotz et al., 2001;
56

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
0
1
2
3
4
5
x
0.0
0.2
0.4
0.6
0.8
1.0
f(x)
Inverse-Gaussian Distribution PDF
=1, =1
=2, =1
=3, =1
=3, =2
Figure 2.11:
Inverse-Gaussian probability
density functions for diﬀerent values of the pa-
rameters µ, λ.
H¨ardle and Simar, 2007). The Laplace distribution is useful in modeling heavy-tailed data
since it has heavier tails than the normal distribution and it is used extensively in sparse-
favoring models since it expresses a high peak with heavy tails (same as the l1 regularization
term in non-probabilistic or non-Bayesian optimization methods). When we have a prior
belief that the parameter of interest is likely to be close to the mean with the potential
for large deviations, the Laplace distribution is then used in Bayesian modeling as a prior
distribution for this context.
Deﬁnition 2.4.6 (Laplace Distribution) A random variable x is said to follow the
Laplace distribution with location and scale parameters µ and b > 0, respectively, denoted
by x ∼L(µ, b), if
f(x; µ, b) = 1
2b exp

−|x −µ|
b

.
The mean and variance of x ∼L(µ, b) are given by
E[x] = µ,
Var[x] = 2b2.
Figure 2.12(a) compares diﬀerent parameters µ, b for the Laplace distribution.
2
1
0
1
2
3
4
5
x
0.0
0.1
0.2
0.3
0.4
0.5
f(x)
Laplace Distribution PDF
=1, b=1
=2, b=1
=3, b=1
=3, b=2
(a) Laplace distribution.
3
2
1
0
1
2
3
x
0.0
0.2
0.4
0.6
f(x)
Skew-Laplace Distribution PDF
=0, =1, =0.5
=0, =1, =1
=0, =1, =2
=0, =1, =3
(b) Skew-Laplace distribution.
Figure 2.12: Laplace and skew-Laplace probability density functions for diﬀerent values
of the parameters.
57

2.4. UNIVARIATE GAUSSIAN-RELATED MODELS
Laplace as a mixture of normal distributions.
Any Laplace random variable can
be thought of as a Gaussian random variable with the same mean value and a stochastic
variance that follows an exponential distribution. More formally, the Laplace distribution
can be rewritten as:
L(x | µ, b) =
Z ∞
0
N(x | µ, ϵ) · E(ϵ |
1
2b2 )dϵ.
(2.22)
To see this, we have
Z ∞
0
N(x | µ, ϵ) · E(ϵ |
1
2b2 )dϵ
=
Z ∞
0
1
√
2πϵ exp

−1
2ϵ(x −µ)2

· 1
2b2 exp(−1
2b2 ϵ)dϵ
=
1
2b2
Z ∞
0
1
√
2πϵ exp
(
−(x −µ)2 + ϵ2
b2
2ϵ
)
dϵ
=
1
2b2
Z ∞
0
ϵ
√
2πϵ3 exp

−(x −µ −ϵ
b)2 + 2 |x −µ| ϵ
b
2ϵ

dϵ
z:=|x−µ|b
========
1
2b2
Z ∞
0
ϵ
√
2πϵ3 exp

−(z −ϵ)2
2ϵb2

exp
|x −µ|
b

dϵ
λ:=|x−µ|
=======
1
√
λ
1
2b2 exp
|x −µ|
b
 Z ∞
0
ϵ
√
λ
√
2πϵ3 exp

−λ(z −ϵ)2
2ϵz2

dϵ
= 1
2b exp

−|x −µ|
b

,
where the last equality is from the mean value of an inverse-Gaussian distribution in Deﬁ-
nition 2.4.5.
Conjugate prior for the Laplace scale parameter.
Similar to the Gaussian case,
the inverse-Gamma distribution is a conjugate prior for the scale parameter of a Laplace
distribution. To see this, let the likelihood be p(A | B, b) = L(A | B, b) where A, B ∈
RM×N, the prior of b be p(b) = G−1(b | α, β). Using Bayes’ theorem, it can be shown that
p(b | A, B, α, β) ∝L(A | B, b) × G−1(b | α, β)
=
M,N
Y
i,j=1
L(aij | bij, b) × βα
Γ(α)(b)−α−1 exp(−β
b )
∝
1
bMN exp


−1
b
M,N
X
i,j=1
|aij −bij|


· b−α−1 exp(−β
b )
= (b)−MN−α−1 exp


−1
b


M,N
X
i,j=1
1
2 |aij −bij| + β





∝G−1(b | eα, eβ),
(2.23)
58

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
where
eα = MN + α,
eβ =
M,N
X
i,j=1
1
2 |aij −bij| + β.
(2.24)
That is, the posterior density of the scale parameter b is also an inverse-Gamma distribution.
The skew-Laplace distribution is a type of heavy-tailed probability distribution that is
similar to the Laplace distribution but allows for skewness (see Figure 2.12).
Deﬁnition 2.4.7 (Skew-Laplace Distribution) A random variable x is said to fol-
low the skew-Laplace (or the asymmetric Laplace) distribution with location and scale
parameters µ and α, β > 0, respectively, denoted by x ∼SL(µ, α, β), if
f(x; µ, α, β) =







αβ
α + β exp {−α(x −µ)} ,
if x ≥µ;
αβ
α + β exp {β(x −µ)} ,
if x < µ.
When α = β = 1
b, the skew-Laplace x ∼SL(µ, α, β) reduces to a Laplace density x ∼
L(µ, b). The mean and variance of x ∼SL(µ, α, β) are given by
E[x] = µ + β −α
αβ ,
Var[x] = α2 + β2
α2β2 .
Figure 2.12(b) compares diﬀerent parameters µ, α, β for the skew-Laplace distribution.
When α > β, the distribution is skewed to the right.
2.5. Multinomial Distribution and Conjugacy
The multinomial distribution is widely used in the Bayesian mixture model to introduce
latent variables. It is a discrete probability distribution that describes the probabilities
of obtaining diﬀerent outcomes from N independent trials, each with K diﬀerent possible
outcomes and with probabilities of the K outcomes that are speciﬁed. It models the distri-
bution of counts or frequencies of events among K categories. In speciﬁc, the multinomial
distribution is parameterized by an integer N and a p.m.f. π = {π1, π2, . . . , πK}, and can
be thought of as following: if we have N independent events, and for each event, the prob-
ability of outcome k is πk, then the multinomial distribution speciﬁes the probability that
outcome k occurs Nk times, for k = 1, 2, . . . , K. Formally, we have the following deﬁnition
of the multinomial distribution.
Deﬁnition 2.5.1 (Multinomial Distribution) A K-dimensional random vector N =
[N1, N2, . . . , NK] ∈{0, 1, 2, . . . , N}K where PK
k=1 Nk = N is said to follow the multino-
mial distribution with parameter N ∈N and π = [π1, π2, . . . , πK] ∈[0, 1]K such that
PK
k=1 πk = 1. Denoted by N ∼MultiK(N, π). Then its probability mass function is
59

2.5. MULTINOMIAL DISTRIBUTION AND CONJUGACY
given by
p
 N1, N2, . . . , NK|N, π = (π1, π2, . . . , πK)

=
N!
N1!N2! . . . NK!
K
Y
k=1
πNk
k
·1
( K
X
k=1
Nk = N
)
,
where {0, 1, 2, . . . , N} is a set of N + 1 elements and [0, 1] is a closed set with values
between 0 and 1. The mean, variance, and covariance of the multinomial distribution
are
E[Nk] = Nπk,
Var[Nk] = Nπk(1 −πk),
Cov[Nk, Nm] = −Nπkπm.
When K = 2, the multinomial distribution reduces to the binomial distribution.
Remark 2.3: Binomial Distribution
In multinomial distribution, when K = 2, it is also known as a binomial distribution. A
random variable x is said to follow the binomial distribution with parameter π ∈(0, 1)
and N ∈N, denoted by x ∼Binom(N, π), if
p(x | N, π) =
N
x

πx(1 −π)N−x.
The mean and variance of the binomial distribution are
E[x] = Nπ,
Var[x] = Nπ(1 −π).
Figure 2.13 compares diﬀerent parameters N, π for the binomial distribution.
A random variable is said to follow the Bernoulli distribution with parameter π ∈(0, 1),
denoted as x ∼Bern(π), if
f(x; π) = π1{x = 1} + (1 −π)1{x = 0},
with mean E[x] = π and variance Var[x] = π(1 −π) respectively.
Exercise 2.2 (Bernoulli and Binomial) Show that if x = PN
i=1 yi where yi
i.i.d.
∼Bern(π),
then we have x ∼Binom(N, π).
2.5.1 Dirichlet Distribution
The Dirichlet distribution is a multi-dimensional probability distribution over the simplex.
It takes a vector of positive real numbers as input and outputs a probability distribution
over a set of probabilities that sum to 1. The Dirichlet distribution commonly serves as a
prior distribution in Bayesian statistics, particularly in the context of discrete and categor-
ical data, and it is a conjugate prior for the probability parameter π of the multinomial
distribution.
60

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
0
10
20
30
40
x
0.00
0.05
0.10
0.15
0.20
0.25
f(x)
Binomial Distribution PMF
N=15, =0.5
N=30, =0.5
N=30, =0.7
N=38, =0.8
Figure 2.13:
Binomial probability mass
functions for diﬀerent values of the parame-
ters N, π.
Deﬁnition 2.5.2 (Dirichlet Distribution) A random vector x = [x1, x2, . . . , xK] ∈
[0, 1]K is said to follow the Dirichlet distribution with parameter α, denoted by x ∼
Dirichlet(α), if
f(x; α) =
1
D(α)
K
Y
k=1
xαk−1
k
,
(2.25)
such that PK
k=1 xk = 1, xk ∈[0, 1] and
D(α) =
QK
k=1 Γ(αk)
Γ(α+)
,
(2.26)
where α = [α1, α2, . . . , αK] is a vector of reals with αk > 0, ∀k, and α+ = PK
k=1 αk.
The α is also known as the concentration parameter in Dirichlet distribution. Γ(·)
is the Gamma function which is a generalization of the factorial function. The mean,
variance, and covariance are
E[xk] = αk
α+
,
Var[xk] = αk(α+ −αk)
α2
+(α+ + 1) ,
Cov[xk, xm] =
−αkαm
α2
+(α+ + 1).
When K = 2, the Dirichlet distribution reduces to the Beta distribution, The Beta dis-
tribution Beta(α, β) is deﬁned on [0, 1] with the probability density function given by
Beta(x | α, β) = Γ(α + β)
Γ(α)Γ(β)xα−1(1 −x)β−1.
That is, if x ∼Beta(α, β), then x = [x, 1 −x] ∼Dirichlet(α), where α = [α, β].
Interesting readers can refer to Appendix A.2 (p. 232) for a derivation of the Dirichlet
distribution. The sample space of the Dirichlet distribution lies on the (K −1)-dimensional
probability simplex, which is a surface in RK denoted by △K. That is a set of vectors in
RK whose components are nonnegative and sum to 1.
△K =
(
π : 0 ≤πk ≤1,
K
X
k=1
πk = 1
)
.
61

2.5. MULTINOMIAL DISTRIBUTION AND CONJUGACY
Marginal
Distribution
xi ∼Beta(αi, α+ −αi).
Conditional
Distribution
x−i | xi ∼(1 −xi)Dirichlet(α−i),
where x−i is a random vector excluding xi.
Aggregation
Property
If M = xi + xj, then [x1, . . . xi−1, xi+1, . . . , xj−1, xj+1, . . . , xK, M] ∼
Dirichlet([α1, . . . , αi−1, αi+1, . . . , αj−1, αj+1, . . . , αK, αi + αj]).
In general, If {A1, A2, . . . , Ar} is a partition of {1, 2, . . . , K}, then
P
i∈A1 xi, P
i∈A2 xi, . . . , P
i∈Ar xi

∼
Dirichlet
 P
i∈A1 αi, P
i∈A2 αi, . . . , P
i∈Ar αi

.
Table 2.2: Properties of the Dirichlet distribution.
Notice that △K lies on a (K −1)-dimensional space since each component is nonnegative,
and the components sum to 1.
Figure 2.14 shows various plots of the Dirichlet distribution’s density over the two-
dimensional simplex in R3 for a handful of values of the parameter vector α and Figure 2.15
shows the draw of 5, 000 points for each setting. In speciﬁc, the density plots of Dirichlet
in R3 is a surface plot in 4d-space. Figure 2.14(a) is a projection of a surface into 3d-space
where the z-axis is the probability density function and Figure 2.14(b) is a projection of
a surface into 3d-space where the z-axis is π3. Figure 2.14(c) to Figure 2.14(f) are the
projections into a 2d-space.
When the concentration parameter α = [1, 1, 1], the Dirichlet distribution reduces to
the uniform distribution over the simplex. This can be easily veriﬁed that Dirichlet(x | α =
[1, 1, 1]) =
Γ(3)
(Γ(1))3 = 2 which is a constant that does not depend on the speciﬁc value of x.
When α = [c, c, c] with c > 1, the density becomes monomodal and concentrated in the
center of the simplex. This can be seen from Dirichlet(x | α = [c, c, c]) =
Γ(3c)
(Γ(c))3
Q3
k=1 xc−1
k
such that a small value of xk will make the probability density approach zero. On the
contrary, when α = [c, c, c] with c < 1, the density has sharp peaks almost at the vertices
of the simplex.
More properties of the Dirichlet distribution are provided in Table 2.2, and the proof can
be found in Appendix A.2 (p. 232). And the derivation on the Dirichlet distribution in Ap-
pendix A.2 (p. 232) can also be utilized to generate samples from the Dirichlet distribution
by a set of samples from a set of Gamma distributions.
2.5.2 Posterior Distribution for Multinomial Distribution
For the conjugacy, that is, if (N | π) ∼MultiK(N, π) and π ∼Dirichlet(α), then (π | N) ∼
Dirichlet(α + N) = Dirichlet(α1 + N1, . . . , αK + NK).
62

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
(a) α =

10, 10, 10

, z-axis is pdf.
(b) α =

10, 10, 10

, z-axis is π3.
(c) α =

1, 1, 1

(d) α =

0.9, 0.9, 0.9

(e) α =

10, 10, 10

(f) α =

15, 5, 2

Figure 2.14: Density plots (blue=low, red=high) for the Dirichlet distribution over the
probability simplex in R3 for various values of the concentration parameter α.
When
α = [c, c, c], the distribution is called a symmetric Dirichlet distribution and the density is
symmetric about the uniform probability mass function (i.e., occurs in the middle of the
simplex). When 0 < c < 1, there are sharp peaks of density almost at the vertices of the
simplex. When c > 1, the density becomes monomodal and concentrated in the center of
the simplex. And when c = 1, it is uniform distributed over the simplex. Finally, if α is
not a constant vector, the density is not symmetric.
63

2.5. MULTINOMIAL DISTRIBUTION AND CONJUGACY
(a) α =

1, 1, 1

(b) α =

0.9, 0.9, 0.9

(c) α =

10, 10, 10

(d) α =

15, 5, 2

Figure 2.15: Draw of 5, 000 points from Dirichlet distribution over the probability simplex
in R3 for various values of the concentration parameter α.
64

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
Proof [of conjugate prior of multinomial distribution] By the Bayes’ theorem “posterior ∝
likelihood × prior”, we obtain the posterior density
posterior = p(π | α, N) ∝MultiK(N | N, π) · Dirichlet(π | α)
=
 
N!
N1!N2! . . . NK!
K
Y
k=1
πNk
k
!
·
 
1
D(α)
K
Y
k=1
παk−1
k
!
∝
K
Y
k=1
παk+Nk−1
k
∝Dirichlet(π | α + N).
Therefore, (π | N) ∼Dirichlet(α + N) = Dirichlet(α1 + N1, . . . , αK + NK).
A comparison between the prior and posterior distribution reveals that the relative sizes
of the Dirichlet parameters αk describe the mean of the prior distribution of π, and the
sum of αk’s is a measure of the strength of the prior distribution. The prior distribution is
mathematically equivalent to a likelihood resulting from PK
k=1(αk −1) observations with
αk −1 observations of the k-th group.
Since the Dirichlet distribution is a multivariate generalization of the Beta distribution,
the Beta distribution can be taken as a conjugate prior for binomial distribution (Hoﬀ,
2009; Frigyik et al., 2010).
2.6. Poisson and Multinomial
The Poisson distribution is a discrete probability distribution that expresses the number of
events in a ﬁxed interval of time or space, given the average number of events in that interval.
The Poisson distribution is commonly used to model the count data, such as the number of
calls received by a call center in an hour, or the number of emails received in a day provided
that the probability of a “success” for any given instant is “very small”.
Deﬁnition 2.6.1 (Poisson Distribution) A random variable x is said to follow the
Poisson distribution with rate parameter λ > 0, denoted by x ∼P(λ), if
f(x; λ) = λx
x! exp(−λ).
The mean and variance of x ∼P(λ) are given by
E[x] = λ,
Var[x] = λ.
The support of an exponential distribution is on {0, 1, 2, 3, . . .} = {0} ∪N. Figure 2.16
compares probability mass functions of diﬀerent parameter values λ for the Poisson dis-
tribution.
The mean and variance of the Poisson distribution are the same.
Roughly speaking, a
Poisson distribution is the limit of a binomial distribution when N →∞and π = λ/N,
i.e., the number of trials diverges to inﬁnity but the probability of success decreases to zero
linearly with respect to the number of trials. This is also known as the law of rare events.
65

2.7. MULTIVARIATE GAUSSIAN DISTRIBUTION AND CONJUGACY
Figure 2.16: Poisson probability mass func-
tions for diﬀerent values of the parameter λ.
0
5
10
15
20
x
0.00
0.05
0.10
0.15
0.20
0.25
f(x)
Poisson Distribution PMF
=2
=4
=8
=10
The sum of independently identical Poisson distributed random variables again follows
a Poisson distribution.
Theorem 2.4: (Sum of Independently Distributed Poisson)
Let xi ∼P(λi). Then
y = Pn
i=1 xi ∼P(Pn
i=1 λi).
For simplicity, we consider two independent Poisson random variables x ∼P(λ1) and y ∼
P(λ2). Deﬁne λ = λ1 + λ2 and z = x + y. Then z is a Poisson random variable with
parameter λ. To see this, we have
p(z) = P(z = z) =
z
X
k=1
P(x = k) · P(y = z −k)
=
z
X
k=1
λk
1
k! exp(−λ1) ·
λz−k
2
(z −k)! exp(−λ2)
= exp(−λ1 −λ2)
z!
z
X
k=1
z
k

λk
1λz−k
2
∗= exp(−λ)
z!
(λ1 + λ2)z = λz
z! exp(−λ),
where the equality (*) is from the binomial theorem. Working for general, once we know
the sum of two Poisson random variables, we can keep adding more and more of them to
obtain another Poisson variable.
Theorem 2.5: (Poisson and Multinomial)
Let xi ∼P(λi) be independent for i ∈
{1, 2, . . . , K}. Then the conditional distribution of x = [x1, x2, . . . , xk]⊤given PK
i=1 xi = N
is MultiK(N, {p1, p2, . . . , pK}) with
pi =
λi
λ1 + λ2 + . . . + λK
,
for all i ∈{1, 2, . . . , K}.
2.7. Multivariate Gaussian Distribution and Conjugacy
We have shown the conjugate prior for the mean parameter of a univariate Gaussian dis-
tribution when the variance (or precision) parameter is ﬁxed; and the joint conjugate prior
66

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
for the mean and variance (or precision) parameters of a univariate Gaussian distribution.
In this section, we further provide the conjugate analysis of the multivariate Gaussian dis-
tribution. See also discussion in Murphy (2007, 2012); Teh (2007); Kamper (2013); Das
(2014).
2.7.1 Multivariate Gaussian Distribution
A multivariate Gaussian distribution (also known as a multivariate normal distribution) is
a continuous probability distribution with jointly normal distribution over multiple vari-
ables. It is fully described by its mean vector (of size equal to the number of variables) and
covariance matrix (square matrix of size equal to the number of variables). The covariance
matrix encodes the pairwise relationships between variables in terms of the covariance be-
tween them. The multivariate Gaussian can be used to model complex data distributions in
various ﬁelds such as machine learning, statistics, and signal processing. We ﬁrst give the
rigorous deﬁnition of the multivariate Gaussian distribution as follows.
Deﬁnition 2.7.1 (Multivariate Gaussian Distribution) A random vector x ∈RD
is said to follow the multivariate Gaussian distribution with parameters µ ∈RD and
Σ ∈RD×D, denoted by x ∼N(µ, Σ), if
f(x; µ, Σ) = (2π)−D/2|Σ|−1/2 exp

−1
2(x −µ)⊤Σ−1(x −µ)

,
where µ ∈RD is called the mean vector, and Σ ∈RD×D is positive deﬁnite and is
called the covariance matrix. The mean, mode, and covariance of the multivariate
Gaussian distribution are given by
E[x] = µ,
Mode[x] = µ,
Cov[x] = Σ.
Figure 2.17 compares Gaussian density plots for diﬀerent kinds of covariance matrices.
Similar to the likelihood under univariate Gaussian distribution (Equation (2.1)) for
deriving the conjugate Bayesian result, the likelihood of N random observations X =
{x1, x2, . . . , xN} generated by a multivariate Gaussian with mean vector µ and covariance
67

2.7. MULTIVARIATE GAUSSIAN DISTRIBUTION AND CONJUGACY
3
2
1
0
1
2
3
3
2
1
0
1
2
3
4
0.00
0.05
0.10
0.15
0.20
(a) Gaussian, Σ =
1
0
0
1

.
3
2
1
0
1
2
3
3
2
1
0
1
2
3
4
0.00
0.05
0.10
0.15
0.20
(b) Gaussian, Σ =
1
0
0
3

.
3
2
1
0
1
2
3
3
2
1
0
1
2
3
4
0.00
0.05
0.10
0.15
0.20
(c) Gaussian, Σ =
 1
–0.5
–0.5
1.5

.
3
2
1
0
1
2
3
3
2
1
0
1
2
3
4
0.00
0.05
0.10
0.15
0.20
(d) Gaussian, Σ =
2
0
0
2

.
3
2
1
0
1
2
3
3
2
1
0
1
2
3
4
0.00
0.05
0.10
0.15
0.20
(e) Gaussian, Σ =
3
0
0
1

.
3
2
1
0
1
2
3
3
2
1
0
1
2
3
4
0.00
0.05
0.10
0.15
0.20
(f) Gaussian, Σ =
 3
–0.5
–0.5
1.5

.
Figure 2.17:
Density and contour plots (blue=low, yellow=high) for the multivariate
Gaussian distribution over the R2 space for various values of the covariance/scale matrix
with zero mean vector. Fig 2.17(a) and 2.17(d): A spherical covariance matrix has a circular
shape; Fig 2.17(b) and 2.17(e): A diagonal covariance matrix is an axis aligned ellipse;
Fig 2.17(c) and 2.17(f): A full covariance matrix has an elliptical shape.
matrix Σ is given by
p(X | µ, Σ) =
N
Y
n=1
N(xn | µ, Σ)
(a)
= (2π)−ND/2|Σ|−N/2 exp
(
−1
2
N
X
n=1
(xn −µ)⊤Σ−1(xn −µ)
)
(b)
= (2π)−ND/2|Σ|−N/2 exp

−1
2tr(Σ−1Sµ)

(c)
= (2π)−ND/2|Σ|−N/2 exp

−N
2 (µ −x)⊤Σ−1(µ −x)

exp

−1
2tr(Σ−1Sx)

,
(2.27)
68

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
where
Sµ =
N
X
n=1
(xn −µ)(xn −µ)⊤,
Sx =
N
X
n=1
(xn −x)(xn −x)⊤,
x = 1
N
N
X
n=1
xn.
(2.28)
The Sx is the matrix of sum of squares and is also known as the scatter matrix.
The
equivalence of equation (a) and equation (c) above follows from the identity (similar reason
for the equivalence of equation (a) and equation (b)):
N
X
n=1
(xn −µ)⊤Σ−1(xn −µ) = tr(Σ−1Sx) + N · (x −µ)⊤Σ−1(x −µ).
(2.29)
where the trace of a square matrix A is deﬁned to be the sum of the diagonal elements aii
of A:
tr(A) =
X
i
aii.
(2.30)
The formulation in equation (b) is useful for the separated view of the conjugate prior for
Σ, and equation (c) is useful for the uniﬁed view of the joint conjugate prior for µ, Σ in
the sequel.
Proof [Proof of Identity 2.29] There is a “trick” involving the trace that makes such cal-
culations easy (see also Chapter 3 of Gentle (2007))
x⊤Ax = tr(x⊤Ax) = tr(xx⊤A) = tr(Axx⊤)
(2.31)
where the ﬁrst equality follows from the fact that x⊤Ax is a scalar and the trace of a
product is invariant under cyclical permutations of the factors 4 .
We can then rewrite PN
n=1(xn −µ)⊤Σ−1(xn −µ) by
N
X
n=1
(xn −x)⊤Σ−1(xn −x) +
N
X
n=1
(x −µ)⊤Σ−1(x −µ)
= tr(Σ−1Sx) + N · (x −µ)⊤Σ−1(x −µ).
(2.32)
This concludes the proof.
By equivalence from the Identity (2.29), we cannot reduce the complexity, but it is useful
to show the conjugacy in Section 2.7.7 below.
4. Trace is invariant under cyclical permutations: tr(ABC) = tr(BCA) = tr(CAB) if all ABC, BCA,
and CAB exist.
69

2.7. MULTIVARIATE GAUSSIAN DISTRIBUTION AND CONJUGACY
Similar to the univariate Gaussian likelihood in Equation (2.2), given ﬁxed mean µ and
covariance Σ parameters, we have
p(x | µ, Σ) = N(x | µ, Σ) ∝exp

−1
2x⊤Σ−1x + x⊤Σ−1µ

.
(2.33)
Therefore, if we ﬁnd the form conforming to the above equation, we can say the random
variable x follows the Gaussian distribution x ∼N(µ, Σ). See the example of a Bayesian
GGGM matrix decomposition model in Equation (5.16) (p. 134).
2.7.2 Multivariate Student’s t Distribution
The multivariate Student’s t-distribution is a continuous probability distribution over mul-
tiple variables that generalizes the Gaussian distribution to allow for heavier tails, i.e., the
probability of extreme values is higher than that in a Gaussian distribution. The multi-
variate Student’s t distribution will be often used in the posterior predictive distribution of
multivariate Gaussian parameters. We rigorously deﬁne the distribution as follows.
Deﬁnition 2.7.2 (Multivariate Student’s t Distribution) A random vector x ∈RD
is said to follow the multivariate Student’s t distribution with parameters µ ∈RD,
Σ ∈RD×D, and ν, denoted by x ∼τ(µ, Σ, ν), if
f(x; µ, Σ, ν) = Γ(ν/2 + D/2)
Γ(ν/2)
|Σ|−1/2
νD/2πD/2 ×

1 + 1
ν (x −µ)⊤Σ−1(x −µ)
−( ν+D
2
)
= Γ(ν/2 + D/2)
Γ(ν/2)
|πV |−1/2 ×

1 + 1
ν (x −µ)⊤V −1(x −µ)
−( ν+D
2
)
,
where Σ is called the scale matrix and V = νΣ, and ν is the degree of freedom.
This distribution has fatter tails than a Gaussian one. The smaller the ν is, the fatter
the tails. As ν →∞, the distribution converges towards a Gaussian. The mean, mode,
and covariance of the multivariate Student’s t distribution are given by
E[x] = µ,
Mode[x] = µ,
Cov[x] =
ν
ν −2Σ.
Note that the Σ is called the scale matrix since it is not exactly the covariance matrix as
that in a multivariate Gaussian distribution.
Speciﬁcally, When D = 1, it follows that (see Deﬁnition 2.2.2)
τ(x | µ, σ2, ν) = Γ(ν+1
2 )
Γ(ν
2)
1
σ√νπ ×

1 + (x −µ)2
νσ2
−( ν+1
2 )
.
(2.34)
70

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
When D = 1, µ = 0, Σ = 1, then the p.d.f., deﬁnes the univariate t distribution.
τ(x | ν) = Γ(ν+1
2 )
Γ(ν
2)
1
√νπ ×

1 + x2
ν
−( ν+1
2 )
.
Figure 2.18 compares the Gaussian and the Student’s t distribution for various values such
that when ν →∞, the diﬀerence between the densities is approaching zero. Given the same
parameters in the densities, the Student’s t in general has longer “tails” than a Gaussian
which can be seen from the comparison between Figure 2.18(a) and Figure 2.18(d). This
provides the Student’s t distribution an important property known as robustness, which
means that it is much less sensitive than the Gaussian to the presence of a few data points
which are outliers (Bishop, 2006; Murphy, 2012).
A Student’s t distribution can be written as a Gaussian scale mixture
τ(x | µ, Σ, ν) =
Z ∞
0
N(x | µ, Σ/z) · G
 z | ν
2, ν
2

dz.
(2.35)
This can be thought of as an “inﬁnite” mixture of Gaussians, each with a slightly diﬀerent
covariance matrix.
In other words, a Student’s t distribution is obtained by adding up
an inﬁnite number of Gaussian distributions having the same mean vector but diﬀerent
covariance matrices. From this Gaussian scale mixture view, when ν →∞, the Gamma
distribution becomes a degenerate random variable with all the non-zero mass at the point
unity such that the multivariate Student’s t distribution converges to a multivariate Gaus-
sian distribution.
2.7.3 Prior on Parameters of Multivariate Gaussian Distribution
In Equation (2.7), we have shown that the inverse-Gamma distribution is a conjugate prior
to the variance parameter of a Gaussian distribution. A generalization to this is the inverse-
Wishart distribution which is a conjugate prior to the full covariance matrix of a multivariate
Gaussian distribution. That is, the inverse-Wishart distribution is a probability distribution
of random positive deﬁnite matrices that can be used to model random covariance matrices.
Before delving into the topic of the inverse-Wishart distribution, it’s important to note
that it originates from the Wishart distribution. As stated by Anderson (2003) in 1962,
“The Wishart distribution ranks next to the (multivariate) normal distribution in order of
importance and usefulness in multivariate statistics”.
Deﬁnition 2.7.3 (Wishart Distribution) A random symmetric positive deﬁnite ma-
trix Λ ∈RD×D is said to follow the Wishart distribution with parameter M ∈RD×D
and ν, denoted by Λ ∼Wi(M, ν), if
f(Λ; M, ν)
= |Λ|
ν−D−1
2
exp

−1
2tr(ΛM −1)
 "
2
νD
2 πD(D−1)/4|M|ν/2
D
Y
d=1
Γ
 ν + 1 −d
2

#−1
,
71

2.7. MULTIVARIATE GAUSSIAN DISTRIBUTION AND CONJUGACY
3
2
1
0
1
2
3
3
2
1
0
1
2
3
4
0.00
0.05
0.10
0.15
0.20
(a) Gaussian, Σ =
1
0
0
1

.
3
2
1
0
1
2
3
3
2
1
0
1
2
3
4
0.00
0.05
0.10
0.15
0.20
(b) Gaussian, Σ =
1
0
0
3

.
3
2
1
0
1
2
3
3
2
1
0
1
2
3
4
0.00
0.05
0.10
0.15
0.20
(c) Gaussian, Σ =
 1
–0.5
–0.5
1.5

.
3
2
1
0
1
2
3
3
2
1
0
1
2
3
4
0.00
0.05
0.10
0.15
0.20
(d) Student t, Σ =
1
0
0
1

, ν = 1.
3
2
1
0
1
2
3
3
2
1
0
1
2
3
4
0.00
0.05
0.10
0.15
0.20
(e) Student t, Σ =
1
0
0
1

, ν = 3.
3
2
1
0
1
2
3
3
2
1
0
1
2
3
4
0.00
0.05
0.10
0.15
0.20
(f) Stu t, Σ =
1
0
0
1

, ν = 200.
321
0123
3
2
1
0
1
2
3
4
0.00
0.05
0.10
0.15
0.20
(g) Diﬀbetween (a) and (d)
321
0123
3
2
1
0
1
2
3
4
0.00
0.05
0.10
0.15
0.20
(h) Diﬀbetween (a) and (e)
321
0123
3
2
1
0
1
2
3
4
0.00
0.05
0.10
0.15
0.20
(i) Diﬀbetween (a) and (f)
Figure 2.18:
Density and contour plots (blue=low, yellow=high) for the multivariate
Gaussian distribution and multivariate Student’s t distribution over the R2 space for various
values of the covariance/scale matrix with zero mean vector.
Fig 2.18(a): A spherical
covariance matrix has a circular shape; Fig 2.18(b): A diagonal covariance matrix is an
axis aligned ellipse; Fig 2.18(c): A full covariance matrix has a elliptical shape;
Fig 2.18(d) to Fig 2.18(f) for Student’s t distribution with the same scale matrix and
increasing ν such that the diﬀerence between (a) and (f) in Fig 2.18(i) is approaching zero.
where ν > D and M is a D × D symmetric positive deﬁnite matrix, and |Λ| = det(Λ)
is the determinant of matrix Λ. The ν is called the number of degrees of freedom,
and M is called the scale matrix. The mean and variance of the Wishart distribution
72

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
are given by
E[Λ] = νM,
Var[Λi,j] = ν(m2
ij + miimjj),
where mij is the (i, j)-th element of M.
When D = 1 and M = 1, the Wishart distribution reduces to the Chi-squared distri-
bution (Deﬁnition 2.2.6) such that:
Wi(x | 1, ν) = χ2(x | ν).
An interpretation of the Wishart distribution is as follows.
Suppose we sample i.i.d.
z1, z2, . . . , zν ∈RD from N(0, M). The sum of squares matrix of the collection of multi-
variate vectors is given by
ν
X
i=1
ziz⊤
i = Z⊤Z,
where Z is the ν × D matrix whose i-th row is z⊤
i .
It is trivial that Z⊤Z is positive
semideﬁnite (PSD) and symmetric. If ν > D and the zi’s are linearly independent, then
Z⊤Z will be positive deﬁnite (PD) and symmetric. That is Zx = 0 only happens when
x = 0. We can repeat over and over again, generating matrices Z⊤
1 Z1, Z⊤
2 Z2, . . . , Z⊤
l Zl.
The population distribution of these matrices follows a Wishart distribution with parameters
(M, ν). By deﬁnition,
Λ = Z⊤Z =
ν
X
i=1
ziz⊤
i ;
E[Λ] = E[Z⊤Z] = E
" ν
X
i=1
ziz⊤
i
#
= νE[ziz⊤
i ] = νM.
When D = 1, this reduces to the case that if z is drawn from a zero-mean univariate
normal random variable, then z2 is drawn from a Gamma random variable. To be speciﬁc,
suppose
z ∼N(0, a),
then
z2 ∼G(a/2, 1/2).
Just like the relationship between the inverse-Gamma distribution and the Gamma distri-
bution that if x ∼G(r, λ), then y = 1
x ∼G−1(r, λ). There is a similar connection between
the inverse-Wishart distribution and the Wishart distribution.
Since we often use the inverse-Wishart (IW) distribution as a prior distribution for a
covariance matrix, it is often useful to replace M in the Wishart distribution with S = M −1.
This results in that a random D × D symmetric positive deﬁnite matrix Σ follows an
inverse-Wishart IW(Σ | S, ν) distribution if Σ−1 = Λ follows a Wishart Wi(Λ | M, ν)
distribution.
Deﬁnition 2.7.4 (Inverse-Wishart Distribution) A random symmetric positive def-
inite matrix Σ ∈RD×D is said to follow the inverse-Wishart distribution with parameters
73

2.7. MULTIVARIATE GAUSSIAN DISTRIBUTION AND CONJUGACY
S ∈RD×D and ν, denoted by Σ ∼IW(S, ν), if
f(Σ; S, ν)
= |Σ|−ν+D+1
2
exp

−1
2tr(Σ−1S)

×
"
2
νD
2 πD(D−1)/4|S|−ν/2
D
Y
d=1
Γ
 ν + 1 −d
2

#−1
,
where ν > D and S is a D×D symmetric positive deﬁnite matrix, and |Σ| = det(Σ). The
ν is called the number of degrees of freedom, and S is called the scale matrix. And
it is denoted by Σ ∼IW(S, ν). The mean and mode of the inverse-Wishart distribution
are given by
E[Σ−1] = νS−1 = νM,
E[Σ] =
1
ν −D −1S,
Mode[Σ] =
1
ν + D + 1S.
(2.36)
Note that, sometimes, we replace S by M = S−1 such that E[Σ−1] = νM which does
not involve the inverse of the matrix.
When D = 1, the inverse-Wishart distribution reduces to the inverse-Gamma such
that ν
2 = r and S
2 = λ (see Deﬁnition 2.2.4):
IW(y | S, ν) = G−1(y | r, λ).
Note that the Wishart density is not simply the inverse-Wishart density with Σ replaced
by Λ = Σ−1. There is an additional factor of |Σ|−(D+1). See Theorem 7.7.1 in Anderson
(2003) that the Jacobian of the transformation Λ = Σ−1 is |Σ|−(D+1). Substitution of Σ−1
in the deﬁnition of the Wishart distribution and multiplying by |Σ|−(D+1) can yield the
inverse-Wishart distribution. 5
The multivariate analog of the normal-inverse-Chi-squared distribution (Deﬁnition 2.2.8)
is the normal-inverse-Wishart (NIW) distribution (Murphy, 2007). We will see that a sam-
ple drawn from a normal-inverse-Wishart distribution, a joint conjugate prior, gives a mean
vector and a covariance matrix that can deﬁne a multivariate Gaussian distribution. Sepa-
rately, we can ﬁrst sample a matrix Σ from an inverse-Wishart distribution parameterized
by {S0, ν0, µ} (this is called a semi-conjugate prior), and then sample a mean vector from
a Gaussian distribution parameterized by {m0, V0, Σ}. 6
Deﬁnition 2.7.5 (Normal-Inverse-Wishart (NIW) Distribution) Analog to the (uni-
variate) normal-inverse-Chi-squared distribution, the multivariate counterpart, the normal-
5. Which is from the Jacobian in the change-of-variables formula. A short proof is provided here. Let
Λ = g(Σ) = Σ−1 where Σ ∼IW(S, ν) and Λ ∼Wi(S, ν). Then, f(Σ) = f(Λ)|Jg| where Jg is the
Jacobian matrix results in f(Σ) = f(Λ)|Jg| = f(Λ)|Σ|−(D+1).
6. Here we use a subscript value of 0 to indicate the parameters are used for prior density. However, in the
Bayesian matrix decomposition analysis, things become more complex and the prior parameters could
have other subscript values.
74

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
inverse-Wishart (NIW) distribution is deﬁned as
NIW(µ, Σ | m, κ, ν, S) = N(µ | m, 1
κΣ) · IW(Σ | S, ν)
=
1
ZNIW(D, κ, ν, S)|Σ|−1/2 exp
nκ
2(µ −m)⊤Σ−1(µ −m)
o
× |Σ|−ν+D+1
2
exp

−1
2tr(Σ−1S)

=
1
ZNIW(D, κ, ν, S)|Σ|−ν+D+2
2
× exp

−κ
2(µ −m)⊤Σ−1(µ −m) −1
2tr(Σ−1S)

,
(2.37)
where the random vector µ ∈RD and the random positive deﬁnite matrix Σ ∈RD×D
are said to follow NIW, denoted by µ, Σ ∼NIW(m, κ, ν, S). And ZNIW(D, κ, ν, S) is
a normalizing constant:
ZNIW(D, κ, ν, S) = 2
(ν+1)D
2
πD(D+1)/4κ−D/2|S|−ν/2
D
Y
d=1
Γ
 ν + 1 −d
2

.
(2.38)
We then proceed to discuss the posterior density of the Gaussian model under the NIW
or IW prior from two perspectives: a separated view and a uniﬁed view.
2.7.4 Posterior Distribution of µ: Separated View
Consider again N random observations X = {x1, x2, . . . , xN} generated by a multivariate
Gaussian with mean vector µ and covariance matrix Σ. Suppose the covariance matrix Σ
of a multivariate Gaussian distribution is known in Equation (2.27), the likelihood function
is (equality (a) in Equation (2.27))
likelihood = p(X | µ) = N(X | µ, Σ) =
N
Y
n=1
N(xn | µ, Σ)
= (2π)−ND/2|Σ|−N/2 exp
(
−1
2
N
X
n=1
(xn −µ)⊤Σ−1(xn −µ)
)
∝exp

−1
2Nµ⊤Σ−1µ + Nx⊤Σ−1µ

,
75

2.7. MULTIVARIATE GAUSSIAN DISTRIBUTION AND CONJUGACY
where x = 1
N
PN
n=1 xn. The conjugate prior of the mean vector is also a Gaussian p(µ) =
N(µ | m0, V0),
prior = p(µ) = N(µ | m0, V0)
= (2π)−D/2|V0|−1/2 exp

−1
2(µ −m0)⊤V −1
0
(µ −m0)

= (2π)−D/2|V0|−1/2 exp

−1
2µ⊤V −1
0
µ + µ⊤V −1
0
m0 −1
2m⊤
0 V −1
0
m0

∝exp

−1
2µ⊤V −1
0
µ + µ⊤V −1
0
m0

.
By the Bayes’ theorem “posterior ∝likelihood×prior”, we can obtain a Gaussian posterior
for µ:
posterior = p(µ | X, Σ) ∝p(X | µ, Σ) × p(µ)
= exp

Nx⊤Σ−1µ −1
2Nµ⊤Σ−1µ

× exp

−1
2µ⊤V −1
0
µ + µ⊤V −1
0
m0

= exp

−1
2µ⊤(V −1
0
+ NΣ−1)µ + µ⊤(V −1
0
m0 + NΣ−1x)

∝N(µ | mN, VN)
where V −1
N
= V −1
0
+NΣ−1, and mN = VN(V −1
0
m0+NΣ−1x). In this case, the posterior
precision matrix is the sum of the prior precision matrix V −1
0
and data precision
matrix NΣ−1. By letting V0 →∞I, we can model an uninformative prior such that the
posterior distribution of the mean is p(µ | X, Σ) = N(µ | x, 1
N Σ).
2.7.5 Posterior Distribution of Σ: Separated View
Suppose now the mean vector µ of a multivariate Gaussian distribution is known in Equa-
tion (2.27), the likelihood is (equality (b) in Equation (2.27))
likelihood = p(X | µ, Σ) =
N
Y
n=1
N(xn | µ, Σ) = (2π)−ND/2|Σ|−N/2 exp

−1
2tr(Σ−1Sµ)

.
The corresponding conjugate prior is the inverse-Wishart distribution:
prior = IW(Σ | S0, ν0) = |Σ|−ν0+D+1
2
exp

−1
2tr(Σ−1S0)

×
"
2
ν0D
2 πD(D−1)/4|S0|−ν0/2
D
Y
d=1
Γ(ν0 + 1 −d
2
)
#−1
.
76

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
By Bayes’ theorem, “posterior ∝likelihood×prior”, we obtain an inverse-Wishart posterior
for Σ:
posterior = p(Σ | X, µ) ∝p(X | µ, Σ) × p(Σ)
∝|Σ|−N/2 exp

−1
2tr(Σ−1Sµ)

× |Σ|−ν0+D+1
2
exp

−1
2tr(Σ−1S0)

= |Σ|−ν0+N+D+1
2
exp

−1
2tr
 Σ−1[S0 + Sµ]

∝IW(Σ | S0 + Sµ, ν0 + N).
The posterior degree of freedom is the sum of the prior degree of freedom ν0 and the number
of observations N. And the posterior scale matrix is the sum of the prior scale matrix S0
and the data scale matrix Sµ. The mean of the posterior Σ is given by
E[Σ|X, µ] =
1
ν0 + N −D −1(S0 + Sµ)
=
ν0 −D −1
ν0 + N −D −1 · (
1
ν0 −D −1S0) +
N
ν0 + N −D −1 · ( 1
N Sµ)
= λ ·
 1
ν0 −D −1S0

+ (1 −λ) ·
  1
N Sµ

,
where λ =
ν0−D−1
ν0+N−D−1, (
1
ν0−D−1S0) is the prior mean of Σ, and ( 1
N Sµ) is an unbiased
estimator of the covariance such that ( 1
N Sµ) converges to the true population covariance
matrix when N →∞. Thus, the posterior mean of the covariance matrix can be
seen as the weighted average of the prior expectation and the unbiased esti-
mator. The unbiased estimator can be also shown to be equal to the maximum likelihood
estimator (MLE) of Σ. As N →∞, it can be shown that the posterior expectation of Σ is
a consistent 7 estimator of the population covariance. When we set ν0 = D + 1, λ = 0 and
we recover the MLE.
Similarly, the mode of the posterior Σ is given by
Mode[Σ] =
1
ν0 + N + D + 1(S0 + Sµ)
=
ν0 + D + 1
ν0 + N + D + 1(
1
ν0 + D + 1S0) +
N
ν0 + N + D + 1( 1
N Sµ)
= β(
1
ν0 + D + 1S0) + (1 −β)( 1
N Sµ),
(2.39)
where β =
ν0+D+1
ν0+N+D+1, and (
1
ν0+D+1S0) is the prior mode of Σ. The posterior mode is
a weighted average of the prior mode and the unbiased estimator. Again, the
maximum a posterior (MAP) estimator in Equation (2.39) is a consistent estimator.
2.7.6 Gibbs Sampling of the Mean and Covariance: Separated View
The separated view here is known as a semi-conjugate prior on the mean and covariance
of a multivariate Gaussian distribution since both conditionals, p(µ | X, Σ) and p(Σ | X, µ),
7. An estimator ˆθN of θ constructed on the basis of a sample of size N is said to be consistent if ˆθN
p
−→θ
as N →∞. See also Lu (2022a).
77

2.7. MULTIVARIATE GAUSSIAN DISTRIBUTION AND CONJUGACY
are individually conjugate. In the last two sections, we have shown
µ | X, Σ ∼N(mN, VN),
Σ | X, µ ∼IW(S0 + Sµ, ν0 + N).
The two full conditional distributions can be used to construct a Gibbs sampler. The Gibbs
sampler generates the mean and covariance {µt+1, Σt+1} for (t + 1)-th step from {µt, Σt}
in t-th step via the following two steps:
1. Sample µt+1 from its full conditional distribution: µt+1 ∼N(mN, VN), where {mN, VN}
depend on Σt.
2. Sample Σt+1 from its full conditional distribution: Σt+1 ∼IW(S0 + Sµ, ν0 + N),
where {S0 + Sµ, ν0 + N} depend on µt+1.
2.7.7 Posterior Distribution of µ and Σ Under NIW: Uniﬁed View
The NIW, on the other hand, serves as a fully conjugate prior for the mean vector and
covariance matrix of a multivariate Gaussian model.
Likelihood.
The likelihood of N random observations X = {x1, x2, . . . , xN} generated
by a multivariate Gaussian with mean vector µ and covariance matrix Σ is (equality (c) in
Equation (2.27))
p(X | µ, Σ) =
1
(2π)ND/2 |Σ|−N/2 exp

−N
2 (µ −x)⊤Σ−1(µ −x) −1
2tr(Σ−1Sx)

.
Prior.
A trivial prior is to combine the conjugate priors for µ and Σ respectively in the
above sections:
p(µ, Σ) = N(µ | m0, V0) · IW(Σ | S0, ν0).
However, this is not a conjugate prior of the likelihood with parameters {µ, Σ} since µ
and Σ appear together in a non-factorized way in the likelihood. For the full parameters
of a multivariate Gaussian distribution (i.e., mean vector µ and covariance matrix Σ), the
normal-inverse-Wishart (NIW) prior is fully conjugate:
NIW(µ, Σ | m0, κ0, ν0, S0) = N(µ | m0, 1
κ0
Σ) · IW(Σ | S0, ν0)
=
|Σ|−ν0+D+2
2
ZNIW(D, κ0, ν0, S0) · exp

−κ0
2 (µ −m0)⊤Σ−1(µ −m0) −1
2tr(Σ−1S0)

,
(2.40)
where
ZNIW(D, κ0, ν0, S0) = 2
(ν0+1)D
2
πD(D+1)/4κ−D/2
0
|S0|−ν0/2
D
Y
d=1
Γ
 ν0 + 1 −d
2

.
(2.41)
The speciﬁc form of the normalization term ZNIW(D, κ0, ν0, S0) will be useful to show the
posterior marginal likelihood of the data in Section 2.7.9.
78

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
A “prior” interpretation for the NIW prior.
The inverse-Wishart distribution will
ensure that the resulting covariance matrix is positive deﬁnite when ν0 > D. And if we
are conﬁdent that the true covariance matrix is near some covariance matrix Σ0, then we
might choose a large value of ν0 and set S0 = (ν0 −D −1)Σ0, making the distribution of
the covariance matrix Σ concentrated around Σ0. On the other hand, choosing ν0 = D + 2
and S0 = Σ0 will make Σ loosely concentrated around Σ0. More details can be referred to
Chipman et al. (2001); Fraley and Raftery (2007); Hoﬀ(2009); Murphy (2012).
An intuitive interpretation of the hyperparameters (Murphy, 2012; Hoﬀ, 2009): m0 is
our prior mean for µ, κ0 is how strongly we believe this prior for µ (the larger the stronger
we believe this prior mean), S0 is proportional to our prior mean for Σ, and ν0 controls
how strongly we believe this prior for Σ. Because the Gamma function is not deﬁned for
negative integers and zero, from Equation (2.41) we require ν0 > D −1 (which can also be
shown from the expectation of the covariance matrix Equation (2.36). And also S0 needs to
be a positive deﬁnite matrix, where an intuitive reason can be shown from Equation (2.36).
A more detailed reason can be found in Hoﬀ(2009).
Posterior.
By Bayes’ theorem, “posterior ∝likelihood × prior”, the posterior of the µ
and Σ parameters under the NIW prior is
p(µ, Σ | X, β) ∝p(X | µ, Σ)p(µ, Σ | β) = p(X, µ, Σ | β),
(2.42)
where β = {m0, κ0, ν0, S0} are the hyperparameters and the right hand side of Equa-
tion (2.42) is also known as the full joint distribution p(X, µ, Σ | β), and is given by
p(X, µ, Σ | β) = p(X | µ, Σ) · p(µ, Σ | β)
= C × |Σ|−ν0+N+D+2
2
×
exp
(
−N
2 (µ −x)⊤Σ−1(µ −x) −κ0
2 (µ −m0)⊤Σ−1(µ −m0)
−1
2tr(Σ−1Sx) −1
2tr(Σ−1S0)
)
,
(2.43)
where C =
(2π)−ND/2
ZN IW(D,κ0,ν0,S0) is a constant normalization term. This can be reduced to
p(X, µ, Σ | β)
= C|Σ|−ν0+N+D+2
2
×
exp
(
−κ0 + N
2

µ −κ0m0 + Nx
κN
⊤
Σ−1

µ −κ0m0 + Nx
κN

−1
2tr

Σ−1

S0 + Sx +
κ0N
κ0 + N (x −m0)(x −m0)⊤
 )
,
(2.44)
which is reformulated to compare with the NIW form in Equation (2.40), and we can see
the reason why we rewrite the multivariate Gaussian distribution into Equation (2.29) by
79

2.7. MULTIVARIATE GAUSSIAN DISTRIBUTION AND CONJUGACY
the trace trick. It follows that the posterior is also a NIW density with updated parameters
and gives the view of conjugacy for a multivariate Gaussian distribution:
p(µ, Σ | X, β) = NIW(µ, Σ | mN, κN, νN, SN),
(2.45)
where
mN = κ0m0 + Nx
κN
= κ0
κN
m0 + N
κN
x,
(2.46)
κN = κ0 + N,
(2.47)
νN = ν0 + N,
(2.48)
SN = S0 + Sx +
κ0N
κ0 + N (x −m0)(x −m0)⊤
(2.49)
= S0 +
N
X
n=1
xnx⊤
n + κ0m0m⊤
0 −κNmNm⊤
N.
(2.50)
A “posterior” interpretation for the NIW prior.
An intuitive interpretation of the
parameters in NIW can be obtained from the updated parameters above. The parameter
ν0 is the prior number of samples to observe the covariance matrix, and νN = ν0 + N is the
posterior number of samples. The posterior mean mN of the model mean µ is a weighted
average of the prior mean and the sample mean. The posterior scale matrix SN is the sum
of the prior scale matrix, empirical covariance matrix Sx, and an extra term due to the
uncertainty in the mean.
Parameter Choice
In practice, it is often better to use a weakly informative data-dependent prior. A common
choice is to set S0 = diag(Sx)/N, and ν0 = D + 2, to ensure E[Σ] = S0, and to set m0 = x
and κ0 to some small number, such as 0.01, where Sx is the sample covariance matrix and
x is the sample mean vector as shown in Equation (2.28) (Chipman et al., 2001; Fraley
and Raftery, 2007; Hoﬀ, 2009; Murphy, 2012). Equivalently, we can also standardize the
observation matrix X ﬁrst to have zero mean and unit variance for every feature, and then
let S0 = ID, and ν0 = D + 2, to ensure E[Σ] = ID, and to set m0 = 0 and κ0 to some
small number, such as 0.01.
Reducing Sampling Time by Maintaining Squared sum of customers
In this section, we introduce some tricks to implement NIW more eﬃciently. The trick is
largely used in the Gaussian mixture models (Das, 2014; Lu, 2021c). While in our Bayesian
matrix factorization context, this trick is useful when we use cross-validation (CV) to down-
sample matrix elements in each iteration (Section 5.4, p. 138). The readers will understand
the Chinese restaurant process terminology better in this section after reading Lu (2021c).
Feel free to skip this section.
We have seen the equivalence between Equation (2.49) and Equation (2.50). The reason
why we make a step further to Equation (2.50) from Equation (2.49) is to reduce sampling
time. Suppose now that the data is not ﬁxed and some data points can be removed from
80

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
or added to X. If we stick to the form in Equation (2.49), we need to calculate Sx and x
over and over again whenever the data points are updated.
In Chinese restaurant process/clustering terminology, if we use Equation (2.49) instead
of Equation (2.50), whenever a customer is removed from (or added to) a table, we have to
compute the matrix Sx, which requires to go over each point in this cluster (or each customer
in this table following the term from the Chinese restaurant process). Computing this term
every time when a customer is removed or added, could be computationally expensive.
We realize that the data terms in Equation (2.50) only involve a sum of the outer
product which does not contain any cross product (e.g., xix⊤
j for i ̸= j). By reformulating
into Equation (2.50), whenever a customer (e.g., a customer represented by xn) is removed
or added, we just have to subtract or add xnx⊤
n . Thus for each table, we only have to
maintain the squared sum of customer vectors PN
n=1 xnx⊤
n for SN.
Similarly, for mN, we need to maintain the sum of customer vectors PN
n=1 xn for the
same reason from Equation (2.46).
2.7.8 Posterior Marginal Likelihood of Parameters
The posterior marginal for Σ is given by
p(Σ | X, β) =
Z
µ
p(µ, Σ | X, β)dµ
= IW(Σ | SN, νN),
where the mean and mode can be obtained by Equation (2.36),
E[Σ | X, β] =
SN
νN −D −1,
Mode[Σ | X, β] =
SN
νN + D + 1.
The posterior marginal for µ follows from a multivariate Student’s t distribution (Deﬁni-
tion 2.7.2). We can show the posterior marginal for µ is given by
p(µ | X, β) =
Z
Σ
p(µ, Σ | X, β)dΣ
=
Z
Σ
NIW(µ, Σ | mN, κN, νN, SN)dΣ
= τ
 µ | mN,
1
κN(νN −D + 1)SN, νN −D + 1

,
which is from the Gaussian scale mixture property of the Student’s t distribution, see
Equation (2.35) and further discussion in Murphy (2012).
81

2.7. MULTIVARIATE GAUSSIAN DISTRIBUTION AND CONJUGACY
2.7.9 Posterior Marginal Likelihood of Data
By integrating the full joint distribution in Equation (2.44), we can get the marginal likeli-
hood of data under hyperparameter β = {m0, κ0, ν0, S0}:
p(X | β) =
Z
µ
Z
Σ
p(X, µ, Σ | β)dµdΣ
=
Z
µ
Z
Σ
N(X | µ, Σ) · NIW(µ, Σ | β)dµdΣ
=
(2π)−ND/2
ZNIW(D, κ0, ν0, S0)
Z
µ
Z
Σ
|Σ|−ν0+N+D+2
2
× exp

−κN
2 (µ −mN)Σ−1(µ −mN) −1
2tr(Σ−1SN)

dµdΣ
(∗)
= (2π)−ND/2 ZNIW(D, κN, νN, SN)
ZNIW(D, κ0, ν0, S0)
= π−ND
2 · κD/2
0
· |S0|ν0/2
κD/2
N
· |SN|νN/2
D
Y
d=1
Γ(νN+1−d
2
)
Γ(ν0+1−d
2
)
,
(2.51)
where the identity (*) above is from the fact that the integral reduces to the normalizing
constant of the NIW density given in Equation (2.45).
2.7.10 Posterior Predictive for Data without Observations
Similarly, suppose now we observe a data vector x⋆without observing any old data. Then
the predictive for the data vector can be obtained by
p(x⋆| β) =
Z
µ
Z
Σ
p(x⋆, µ, Σ | β)dµdΣ
=
Z
µ
Z
Σ
N(x⋆| µ, Σ) · NIW(µ, Σ | β)dµdΣ
= π−D/2
κD/2
0
|S0|ν0/2
(κ0 + 1)D/2|S1|ν1/2
D
Y
d=1
Γ(ν1+1−d
2
)
Γ(ν0+1−d
2
)
= π−D/2
κD/2
0
|S0|ν0/2
(κ0 + 1)D/2|S1|ν1/2
Γ(ν0+2−D
2
)
Γ(ν0
2 )
,
(2.52)
where ν1 = ν0 + 1, S1 = S0 +
κ0
κ0+1(x⋆−m0)(x⋆−m0)⊤. An alternative form of Equa-
tion (2.52) is to rewrite by a multivariate Student’s t distribution
p(x⋆|β) = τ
 x⋆| m0,
κ0 + 1
κ0(ν0 −D + 1)S0, ν0 −D + 1

.
(2.53)
82

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
2.7.11 Posterior Predictive for New Data with Observations
Similar to posterior predictive for data without observation, now suppose we observe a new
data vector x⋆given old observations X. Then the posterior predictive for this vector is
p(x⋆| X, β) = p(x⋆, X | β)
p(X | β)
.
(2.54)
The denominator of Equation (2.54) can be obtained directly from Equation (2.51). The
numerator of it can be obtained in a similar way from Equation (2.51) by considering
the marginal likelihood of the new set {X, x⋆}.
We just need to replace N by N⋆=
N + 1 in Equation (2.46), Equation (2.47), and Equation (2.48), and replace SN by SN⋆in
Equation (2.49). Therefore, we obtain
p(x⋆| X, β) = (2π)−D/2 ZNIW(D, κN⋆, νN⋆, SN⋆)
ZNIW(D, κN, νN, SN)
= π−D/2 (κN⋆)−D/2|SN|(νN)/2
(κN)−D/2|SN⋆|(νN⋆)/2
D
Y
d=1
Γ(νN⋆+1−d
2
)
Γ(νN+1−d
2
)
= π−D/2 (κN⋆)−D/2|SN|(νN)/2
(κN)−D/2|SN⋆|(νN⋆)/2
Γ(ν0+N+2−D
2
)
Γ(ν0+N
2
)
.
(2.55)
Again an alternative form of Equation (2.55) is to rewrite by a multivariate Student’s t
distribution:
p(x⋆| X, β) = τ
 x⋆| mN,
κN + 1
κN(νN −D + 1)SN, νN −D + 1

.
Thus, the mean and covariance of x⋆are given by
E[x⋆| X, β] = mN =
κ0
κ0 + N m0 +
N
κ0 + N x,
Cov[x⋆| X, β] =
κN + 1
κN(νN −D −1)SN =
κ0 + N + 1
(κ0 + N)(ν0 + N −D −1)SN,
where we can ﬁnd, on average, the new coming data has expectation mN.
We men-
tioned previously, κ0 controls how strongly we believe this prior for µ. When κ0 is large
enough, E[x⋆| X, β] converges to m0, the prior mean, and Cov[x⋆| X, β] converges to
SN
(κ0+N)(ν0+N−D−1). In the meantime, if we set ν0 large enough, the covariance matrix Σ is
concentrated around Σ0, and
SN →Sx
ν0
+
κ0N
ν0(κ0 + N)(x −m0)(x −m0)⊤,
which is largely controlled by data sample and data magnitude (rather than the prior
hyperparameters), so as the posterior variance Cov[x⋆| X, β].
83

2.7. MULTIVARIATE GAUSSIAN DISTRIBUTION AND CONJUGACY
2.7.12 Further Optimization via the Cholesky Decomposition
Definition
The Cholesky decomposition of a symmetric positive deﬁnite matrix S is its decomposition
into the product of a lower triangular matrix L and its transpose:
S = LL⊤,
(2.56)
where L is called the Cholesky factor of S. We realize that an alternative form of the
Cholesky decomposition is using its upper triangular U = L⊤, i.e., S = U ⊤U. A triangular
matrix is a special kind of square matrices. Speciﬁcally, a square matrix is called lower
triangular if all the entries above the main diagonal are zero. Similarly, a square matrix is
called upper triangular if all the entries below the main diagonal are zero.
If the matrix has dimensionality D, the complexity of Cholesky decomposition is O(D3).
In speciﬁc, it requires ∼
1
3D3 ﬂoating points operations (ﬂops) to compute a Cholesky
decomposition of a D × D positive deﬁnite matrix (Lu, 2021b), where the symbol “∼” has
the usual asymptotic meaning
lim
D→+∞
number of ﬂops
(1/3)D3
= 1.
Rank One Update
A rank 1 update of matrix S by vector x is of the form (Seeger, 2004; Lu, 2021b)
S′ = S + xx⊤.
If we have already calculated the Cholesky factor L of S, then the Cholesky factor L′ of S′
can be calculated eﬃciently. Note that S′ diﬀers from S only in the rank one matrix xx⊤.
Hence we can compute L′ from L using the rank one Cholesky update, which takes O(D2)
operations each saving from O(D3) if we do know L, the Cholesky decomposition of S.
Speedup for Determinant
The determinant of a positive deﬁnite matrix S can be computed from its Cholesky factor
L:
|S| =
D
Y
d=1
l2
dd,
log(|S|) = 2 log(|L|) = 2 ×
D
X
d=1
log(ldd),
where ldd is the (d, d)-th entry of matrix L. This is an O(D) operation, i.e., given the
Cholesky decomposition, the determinant is just the product of the diagonal terms.
Update in NIW
Now we consider computing the marginal likelihood of data in Equation (2.51) and the
posterior predictive for new coming data in Equation (2.54) of which the two cases are
similar.
Take the latter as an example, note that to compute posterior predictive for new coming
data p(x⋆| X, β) in Equation (2.54), we just need to evaluate p(x⋆,X|β)
p(X|β) , in which we must
84

JUN LU
CHAPTER 2. REGULAR PROBABILITY MODELS AND CONJUGACY
calculate |SN| and |SN⋆| eﬃciently where N⋆= N + 1.We deal with computing the deter-
minants |SN| and |SN⋆| by representing SN and SN⋆using their Cholesky decomposition
forms. In particular, updates to SN and SN⋆will be carried out by directly updating their
Cholesky decompositions; given the Cholesky decomposition, the determinant is just the
product of the diagonal terms. Write out S⋆by SN:
mN = κN⋆mN⋆−x⋆
κN
= (κ0 + N + 1)mN⋆−x⋆
κ0 + N
,
(2.57)
mN⋆= κNmN + x⋆
κN⋆
= (κ0 + N)mN + x⋆
κ0 + N + 1
,
(2.58)
SN⋆= SN + x⋆x⋆T −κN⋆mN⋆m⊤
N⋆+ κNmNm⊤
N
(2.59)
= SN + κ0 + N + 1
κ0 + N
(mN⋆−x⋆)(mN⋆−x⋆)⊤,
(2.60)
where Equation (2.60) implies that Cholesky decomposition of SN⋆can be obtained from
Cholesky decomposition of SN by a rank 1 update. Therefore if we know the Cholesky
decomposition of SN, the Cholesky decomposition of SN⋆can be obtained in O(D2) com-
plexity.
85

Part II
Non-Bayesian Matrix
Decomposition
86


3
Alternating Least Squares
Contents
3.1
Preliminary: Least Squares Approximations . . . . . . . . . . .
89
3.2
Netﬂix Recommender and Matrix Factorization
. . . . . . . .
92
3.3
Regularization: Extension to General Matrices . . . . . . . . .
96
3.4
Missing Entries
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
3.5
Vector Inner Product
. . . . . . . . . . . . . . . . . . . . . . . .
99
3.6
Gradient Descent (GD)
. . . . . . . . . . . . . . . . . . . . . . .
100
3.7
Regularization: A Geometrical Interpretation . . . . . . . . . .
104
3.8
Stochastic Gradient Descent (SGD) . . . . . . . . . . . . . . . .
106
3.9
Bias Term
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
3.10
Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
3.10.1
Low-Rank Approximation . . . . . . . . . . . . . . . . . . . . . . 111
3.10.2
Movie Recommender . . . . . . . . . . . . . . . . . . . . . . . . . 113
88

JUN LU
CHAPTER 3. ALTERNATING LEAST SQUARES
3.1. Preliminary: Least Squares Approximations
The linear model is the main technique in regression problems and the primary tool for
it is the least squares approximation which minimizes a sum of squared errors. This is a
natural choice when we’re interested in ﬁnding the regression function which minimizes the
corresponding expected squared error. Over the recent decades, linear models have been
used in a wide range of applications, e.g., decision-making (Dawes and Corrigan, 1974),
time series (Christensen, 1991; Lu, 2017), quantitative ﬁnance (Menchero et al., 2011), and
in many ﬁelds of study, e.g., production science, social science, and soil science (Fox, 1997;
Lane, 2002; Schaeﬀer, 2004; Mrode, 2014).
To be more concrete, we consider the overdetermined system b = Ax, where A ∈Rm×n
is the input data matrix, b ∈Rm is the observation vector (target vector), and the sample
number m is larger than the dimension number n. x is a vector of weights of the linear
model. Normally, A will have full column rank since the data from real world is often
uncorrelated (or it is uncorrelated after post-processing). In practice, a bias term is added
to the ﬁrst column of A such that the least square is to ﬁnd the solution of
e
Aex = [1, A]
x0
x

= b.
(3.1)
On the other hand, it often happens that b = Ax has no solution. The usual reason is:
too many equations, i.e., the matrix has more rows than columns. Deﬁne the column space
of A by {Aγ : ∀γ ∈Rn} and denoted by C(A). Thus the meaning of b = Ax has no
solution is that b is outside the column space of A. In another word, the error e = b −Ax
cannot get down to zero. When the error e is as small as possible in the sense of mean
squared error (MSE), xLS is a least squares solution, i.e., ∥b −AxLS∥2 is minimum. The
method of least squares is one of the most eﬀective tools for the mathematical sciences.
There are books devoted solely to it. Readers are also advised to consult Trefethen and
Bau III (1997); Strang (2019, 2021); Lu (2022a).
Least squares by calculus.
When ∥b −Ax∥2 is diﬀerentiable, and the parameter space
of x is an open set (the least achievable value is obtained inside the parameter space), the
least squares estimator must be the root of ∥b −Ax∥2. We thus come into the following
lemma.
Lemma 3.1: (Least Squares by Calculus)
Assume A ∈Rm×n is ﬁxed and has
full rank (i.e., the columns of A are linearly independent) with m ≥n. Consider the
overdetermined system b = Ax, the least squares solution by calculus via setting the
derivative in every direction of ∥b −Ax∥2 to be zero is xLS = (A⊤A)−1A⊤b. The value
xLS = (A⊤A)−1A⊤b is known as the ordinary least squares (OLS) estimator or simply
least squares (LS) estimator of x.
To prove the lemma above, we must show A⊤A is invertible. Since we assume A has
full rank and m ≥n. A⊤A ∈Rn×n is invertible if it has rank n which is the same as the
rank of A. This is proved in Lemma 3.2.
89

3.1. PRELIMINARY: LEAST SQUARES APPROXIMATIONS
Lemma 3.2: (Rank of A⊤A)
Given any matrix A, A⊤A and A have the same rank.
Proof [of Lemma 3.2] Let x ∈N(A), i.e., x is in the null space of A (Deﬁnition 0.0.7,
p. 8), we have
Ax = 0
leads to
−−−−−→
A⊤Ax = 0,
i.e., x ∈N(A) leads to
−−−−−→x ∈N(A⊤A), therefore N(A) ⊆N(A⊤A).
Further, let x ∈N(A⊤A), we have
A⊤Ax = 0 leads to
−−−−−→x⊤A⊤Ax = 0 leads to
−−−−−→∥Ax∥2 = 0 leads to
−−−−−→Ax = 0,
i.e., x ∈N(A⊤A) leads to
−−−−−→x ∈N(A), therefore N(A⊤A) ⊆N(A).
As a result, by “sandwiching”, it follows that
N(A) = N(A⊤A)
and
dim(N(A)) = dim(N(A⊤A)).
By the fundamental theorem of linear algebra, A⊤A and A have the same rank.
Apply the observation to A⊤, we can also prove that AA⊤and A have the same rank.
This result brings about the ordinary least squares estimator as follows.
Proof [of Lemma 3.1] Recall from calculus that a minimum of a function f(x) occurs
at a value xLS such that the derivative
∂
∂xf(x) = 0.
The diﬀerential of ∥b −Ax∥2 is
2A⊤Ax −2A⊤b. A⊤A is invertible since we assume A is ﬁxed and has full rank with
m ≥n. So the OLS solution of x is xLS = (A⊤A)−1A⊤b which completes the proof.
Deﬁnition 3.1.1 (Normal Equation) We can write the zero derivative of ∥b −Ax∥2
as A⊤AxLS = A⊤b. The equation is also known as the normal equation. In the as-
sumption, A has full rank with m ≥n. So A⊤A is invertible which implies xLS =
(A⊤A)−1A⊤b.
f(x) = x2
(a) A convex function.
f(x) =
x2
(b) A concave function.
f(x) = x2 + e
5(x
0.5)2
(c) A random function.
Figure 3.1: Three functions.
However, we do not actually know the least squares estimator obtained in Lemma 3.1
is the least or largest achievable estimator (or neither). An example is shown in Figure 3.1.
All we can get is that there only exists one root for the function ∥b −Ax∥2. The following
remark can address this concern.
90

JUN LU
CHAPTER 3. ALTERNATING LEAST SQUARES
Remark 3.3: Veriﬁcation of Least Squares Solution
Why does the zero derivative imply least mean squared error? The usual reason is from
the convex analysis as we shall see shortly. But here we verify directly that the OLS
solution ﬁnds the least squares. For any x ̸= xLS, we have
∥b −Ax∥2 = ∥b −AxLS + AxLS −Ax∥2 = ∥b −AxLS + A(xLS −x)∥2
= ∥b −AxLS∥2 + ∥A(xLS −x)∥2 + 2 (A(xLS −x))⊤(b −AxLS)
= ∥b −AxLS∥2 + ∥A(xLS −x)∥2 + 2(xLS −x)⊤(A⊤b −A⊤AxLS),
where the third term is zero from the normal equation and ∥A(xLS −x)∥2 ≥0. There-
fore,
∥b −Ax∥2 ≥∥b −AxLS∥2 .
Thus we show that the OLS estimator indeed gives the minimum, not the maximum or
a saddle point via the calculus approach.
A further question would be posed: why does this normal equation magically produce
solutions for x? A simple example would give the answer. x2 = −1 has no real solution.
But x · x2 = x · (−1) has a real solution ˆx = 0 in which case ˆx makes x2 and −1 as close as
possible.
Example 3.1 (Multiply From Left Can Change The Solution Set) Consider the ma-
trix and target vector
A =


−3
−4
4
6
1
1


and
b =


1
−1
0

.
It can be easily veriﬁed that Ax = b has no solution for x. However, if we multiply on the
left by
B =
0
−1
6
0
1
−4

.
Then we have xLS = [1/2, −1/2]⊤as the solution of BAx = Bb. This speciﬁc example
shows why the normal equation can give rise to the least square solution. Multiplying on
the left of a linear system will change the solution set.
□
Rank deﬁciency.
Note here, we assume A ∈Rm×n has full rank with m ≥n to make
A⊤A invertible. But when two or more columns of A are perfectly correlated, the matrix A
will be deﬁcient and A⊤A is singular. Choosing the x that minimizes x⊤
LSxLS which meets
the normal equation can help to solve the problem. I.e., choose the shortest magnitude least
squares solution. But this is not the main interest of the text. We will leave this topic to
the readers. In Lu (2021b), UTV decomposition and singular value decomposition (SVD)
are applied to tackle the rank-deﬁcient least squares problem.
91

3.2. NETFLIX RECOMMENDER AND MATRIX FACTORIZATION
3.2. Netﬂix Recommender and Matrix Factorization
In the Netﬂix prize (Bennett et al., 2007), the goal is to predict the ratings of users for
diﬀerent movies, given the existing ratings of those users for other movies. We index M
movies with m = 1, 2, . . . , M and N users by n = 1, 2, . . . , N. We denote the rating of the
n-th user for the m-th movie by amn. Deﬁne A to be an M ×N rating matrix with columns
an ∈RM containing ratings of the n-th user. Note that many ratings {amn} are missing
and our goal is to predict those missing ratings accurately.
We formally consider algorithms for solving the following problem: The matrix A is
approximately factorized into an M × K matrix W and a K × N matrix Z. Usually K
is chosen to be smaller than M or N, so that W and Z are smaller than the original
matrix A. This results in a compressed version of the original data matrix. An appropriate
decision on the value of K is critical in practice, but the choice of K is very often problem
dependent. The factorization is signiﬁcant in the sense, suppose A = [a1, a2, . . . , aN] and
Z = [z1, z2, . . . , zN] are the column partitions of A and Z respectively, then an = W zn,
i.e., each column an is approximated by a linear combination of the columns of W weighted
by the components in zn. Therefore, columns of W can be thought of as containing the
column basis of A.
To ﬁnd the approximation A ≈W Z, we need to deﬁne a loss function such that the
distance between A and W Z can be measured. The loss function is selected to be the
Frobenius norm (or mean squared error, MSE) between two matrices which vanishes to
zero if A = W Z where the advantage will be seen shortly.
To simplify the problem, let’s assume that there are no missing ratings ﬁrst. Project
data vectors an to a smaller dimension zn ∈RK with K < min{M, N}, such that the
reconstruction error measured by Frobenius norm is minimized (assume K is known):
min
W ,Z
N
X
n=1
M
X
m=1

amn −w⊤
mzn
2
,
(3.2)
where W = [w⊤
1 ; w⊤
2 ; . . . ; w⊤
M] ∈RM×K and Z = [z1, z2, . . . , zN] ∈RK×N containing wm’s
and zn’s as rows and columns respectively 1. The loss form in Equation (3.2) is known
as the per-example loss. It can be equivalently written as
L(W , Z) =
N
X
n=1
M
X
m=1

amn −w⊤
mzn
2
= ∥W Z −A∥2 .
Moreover, the loss L(W , Z) = PN
n=1
PM
m=1
 amn −w⊤
mzn

is convex with respect to Z
given W and vice versa. Therefore, we can ﬁrst minimize with respect to Z given W and
then minimize with respect to W given Z:





Z ←arg min
Z
L(W , Z);
(ALS1)
W ←arg min
W
L(W , Z).
(ALS2)
1. Note in some contexts, Z represents an N × K matrix such that A is decomposed into A ≈W Z⊤.
92

JUN LU
CHAPTER 3. ALTERNATING LEAST SQUARES
This is known as the coordinate descent algorithm in which case we employ the least squares
alternatively. Hence it is also called the alternating least squares (ALS) (Comon et al., 2009;
Tak´acs and Tikk, 2012; Giampouras et al., 2018). The convergence is guaranteed if the loss
function L(W , Z) decreases at each iteration and we shall discuss more on this in the sequel.
Remark 3.4: Convexity and Global Minimum
Although the loss function deﬁned by Frobenius norm ∥W Z −A∥2 is convex in W given
Z or vice versa, it is not convex in both variables together. Therefore we are not able to
ﬁnd the global minimum. However, the convergence is assured to ﬁnd local minima.
Given W , optimizing Z.
Now, let’s see what is in the problem of Z ←arg minZ L(W , Z).
When there exists a unique minimum of the loss function L(W , Z) with respect to Z, we
speak of the least squares minimizer of arg minZ L(W , Z). Given W ﬁxed, L(W , Z) can
be written as L(Z | W ) (or more compactly, as L(Z)) to emphasize on the variable of Z:
L(Z | W ) = ∥W Z −A∥2 = ∥W [z1, z2, . . . , zN] −[a1, a2, . . . , aN]∥2 =



W z1 −a1
W z2 −a2
...
W zN −aN



2
.
2
Now, if we deﬁne
f
W =


W
0
. . .
0
0
W
. . .
0
...
...
...
...
0
0
. . .
W

∈RMN×KN,
ez =


z1
z2
...
zN

∈RKN,
ea =


a1
a2
...
aN

∈RMN,
then the (ALS1) problem can be reduced to the normal least squares problem for minimizing
f
W ez −ea

2
with respect to ez. And the solution is given by
ez = (f
W ⊤f
W )−1 f
W ⊤ea.
However, it is not wise to obtain the result via this approach. It takes 2(KN)3 ﬂops to get
the inverse of f
W ⊤f
W (Lu, 2021b). Alternatively, a direct way to solve (ALS1) is to ﬁnd
the diﬀerential of L(Z | W ) with respect to Z:
∂L(Z | W )
∂Z
= ∂tr
 (W Z −A)(W Z −A)⊤
∂Z
= ∂tr
 (W Z −A)(W Z −A)⊤
∂(W Z −A)
∂(W Z −A)
∂Z
⋆= 2W ⊤(W Z −A) ∈RK×N,
(3.3)
2. The matrix norm used here is the Frobenius norm such that ∥A∥=
qPm,n
i=1,j=1(aij)2 if A ∈Rm×n. And
the vector norm used here is the L2 norm such that ∥x∥2 =
pPn
i=1 x2
i if x ∈Rn.
93

3.2. NETFLIX RECOMMENDER AND MATRIX FACTORIZATION
where the ﬁrst equality is from the deﬁnition of Frobenius norm (Deﬁnition 0.0.12, p. 9)
such that ∥A∥=
qPm,n
i=1,j=1(aij)2 =
p
tr(AA⊤), and equality (⋆) comes from the fact that
∂tr(AA⊤)
∂A
= 2A. When the loss function is a diﬀerentiable function of Z, we may determine
the least squares solution by diﬀerential calculus, and a minimum of the function L(Z | W )
must be a root of the equation:
∂L(Z | W )
∂Z
= 0.
By ﬁnding the root of the above equation, we have the “candidate” update on Z that ﬁnd
the minimizer of L(Z | W )
Z = (W ⊤W )−1W ⊤A ←arg min
Z
L(Z | W ).
(3.4)
This takes 2K3 ﬂops to compute the inverse of W ⊤W as compared to 2(KN)3 ﬂops to get
the inverse of f
W ⊤f
W . Before we declare a root of the above equation is actually a minimizer
rather than a maximizer (that’s why we call the update a “candidate” update above), we
need to verify the function is convex such that if the function is twice diﬀerentiable, this
can be equivalently done by verifying
∂2L(Z | W )
∂Z2
> 0,
i.e., the Hessian matrix is positive deﬁnite. To see this, we write out the twice diﬀerential
∂2L(Z | W )
∂Z2
= 2W ⊤W ∈RK×K,
(3.5)
which has full rank if W ∈RM×K has full rank (Lemma 3.2) and K < M.
Remark 3.5: Positive Deﬁnite Hessian if W Has Full Rank
We here claim that if W ∈RM×K has full rank K with K < M, then ∂2L(Z|W )
∂Z2
is
positive deﬁnite. This can be done by checking that when W has full rank, W x = 0
only when x = 0 since the null space of W is of dimension 0. Therefore,
x⊤(2W ⊤W )x > 0,
for any nonzero vector x ∈RK.
Now, the thing is that we need to check if W has full rank so that the Hessian of L(Z | W )
is positive deﬁnite, otherwise, we cannot claim the update of Z in Equation (3.4) decreases
the loss (due to convexity) so that the matrix decomposition is going into the right way to
better approximate the original matrix A by W Z in each iteration. We will shortly come
back to the positive deﬁniteness of the Hessian matrix in the sequel which relies on the
following lemma.
Lemma 3.6: (Rank of Z after Updating)
Suppose A ∈RM×N has full rank with
M ≤N and W ∈RM×K has full rank with K < M, then the least squares update of
Z = (W ⊤W )−1W ⊤A ∈RK×N in Equation (3.4) has full rank.
94

JUN LU
CHAPTER 3. ALTERNATING LEAST SQUARES
Proof [of Lemma 3.6] Since W ⊤W ∈RK×K has full rank if W has full rank (Lemma 3.2)
such that (W ⊤W )−1 has full rank.
Suppose W ⊤x = 0, this implies (W ⊤W )−1W ⊤x = 0. Thus, the following two null
spaces satisfy
N(W ⊤) ⊆N

(W ⊤W )−1W ⊤
.
Moreover, suppose (W ⊤W )−1W ⊤x = 0, and since (W ⊤W )−1 is invertible. This implies
W ⊤x = (W ⊤W )0 = 0, and
N

(W ⊤W )−1W ⊤
⊆N(W ⊤).
As a result, by “sandwiching”, it follows that
N(W ⊤) = N

(W ⊤W )−1W ⊤
.
(3.6)
Therefore, (W ⊤W )−1W ⊤has full rank K. Let T = (W ⊤W )−1W ⊤∈RK×M, and suppose
T ⊤x = 0. This implies A⊤T ⊤x = 0, and
N(T ⊤) ⊆N(A⊤T ⊤).
Similarly, suppose A⊤(T ⊤x) = 0. Since A has full rank with the dimension of the null space
being 0: dim
 N(A⊤)

= 0, (T ⊤x) must be zero. The claim follows since A has full rank
M with the row space of A⊤being equal to the column space of A where dim (C(A)) = M
and the dim
 N(A⊤)

= M −dim (C(A)) = 0. Therefore, x is in the null space of T ⊤if x
is in the null space of A⊤T ⊤:
N(A⊤T ⊤) ⊆N(T ⊤).
By “sandwiching” again,
N(T ⊤) = N(A⊤T ⊤).
(3.7)
Since T ⊤has full rank K < M ≤N, dim
 N(T ⊤)

= dim
 N(A⊤T ⊤)

= 0. Therefore,
Z⊤= A⊤T ⊤has full rank K. We complete the proof.
Given Z, optimizing W .
Given Z, L(W , Z) can be written as L(W | Z) to emphasize
the variable of W :
L(W | Z) = ∥W Z −A∥2 .
A direct way to solve (ALS2) is to ﬁnd the diﬀerential of L(W | Z) with respect to W :
∂L(W | Z)
∂W
= ∂tr
 (W Z −A)(W Z −A)⊤
∂W
= ∂tr
 (W Z −A)(W Z −A)⊤
∂(W Z −A)
∂(W Z −A)
∂W
= 2(W Z −A)Z⊤∈RM×K.
95

3.3. REGULARIZATION: EXTENSION TO GENERAL MATRICES
The “candidate” update on W is similar to ﬁnding the root of the diﬀerential ∂L(W |Z)
∂W
:
W ⊤= (ZZ⊤)−1ZA⊤←arg min
W
L(W | Z).
(3.8)
Again, we emphasize that the update is only a “candidate” update. We need to further
check whether the Hessian is positive deﬁnite or not. The Hessian matrix is given by
∂2L(W | Z)
∂W 2
= 2ZZ⊤∈RK×K.
(3.9)
Therefore, by analogous analysis, if Z has full rank with K < N, the Hessian matrix is
positive deﬁnite.
Lemma 3.7: (Rank of W after Updating)
Suppose A ∈RM×N has full rank
with M ≤N and Z ∈RK×N has full rank with K < N, then the update of W ⊤=
(ZZ⊤)−1ZA⊤in Equation (3.8) has full rank.
The proof of Lemma 3.7 is similar to that of Lemma 3.6, and we shall not repeat the details.
Key observation.
Combine the observations in Lemma 3.6 and Lemma 3.7, as long as
we initialize Z, W to have full rank, the updates in Equation (3.4) and Equation (3.8) are
reasonable since the Hessians in Equation (3.5) and (3.9) are positive deﬁnite. The
requirement on the M ≤N is reasonable in that there are always more users
than the number of movies. We conclude the process in Algorithm 1.
Algorithm 1 Alternating Least Squares
Require: Matrix A ∈RM×N with M ≤N;
1: Initialize W ∈RM×K, Z ∈RK×N with full rank and K < M ≤N;
2: Choose a stop criterion on the approximation error δ;
3: Choose cmaximal number of iterations C;
4: iter = 0;
▷Count for the number of iterations
5: while ∥A −W Z∥> δ and iter < C do
6:
iter = iter + 1;
7:
Z = (W ⊤W )−1W ⊤A ←arg minZ L(Z | W );
8:
W ⊤= (ZZ⊤)−1ZA⊤←arg minW L(W | Z);
9: end while
10: Output W , Z;
3.3. Regularization: Extension to General Matrices
Regularization is a machine learning technique used to prevent overﬁtting and improve
model generalization. Overﬁtting occurs when a model is overly complex and ﬁts the train-
ing data too closely, resulting in poor performance on new, unseen data. Regularization
adds a constraint or penalty term to the loss function used in model optimization, discour-
aging models that are overly complex. This results in a trade-oﬀbetween having a simple,
96

JUN LU
CHAPTER 3. ALTERNATING LEAST SQUARES
generalizable model and ﬁtting the training data well. L1 regularization, L2 regularization,
and elastic net regularization (the combination of L1 and L2 regularization) are all common
types of regularization; and they are commonly used in machine learning algorithms such
as linear regression, logistic regression, and neural networks.
In this context, we can add a L2 regularization term to minimize the following loss:
L(W , Z) = ∥W Z −A∥2 + λw ∥W ∥2 + λz ∥Z∥2 ,
λw > 0, λz > 0,
(3.10)
where the diﬀerential with respect to Z, W are given respectively by





∂L(W , Z)
∂Z
= 2W ⊤(W Z −A) + 2λzZ ∈RK×N;
∂L(W , Z)
∂W
= 2(W Z −A)Z⊤+ 2λwW ∈RM×K.
(3.11)
The Hessian matrices are given respectively by







∂2L(W , Z)
∂Z2
= 2W ⊤W + 2λzI ∈RK×K;
∂2L(W , Z)
∂W 2
= 2ZZ⊤+ 2λwI ∈RK×K,
which are positive deﬁnite due to the perturbation by the regularization. To see this, we
have









x⊤(2W ⊤W + 2λzI)x = 2x⊤W ⊤W x
|
{z
}
≥0
+2λz ∥x∥2 > 0,
for nonzero x;
x⊤(2ZZ⊤+ 2λwI)x = 2x⊤ZZ⊤x
|
{z
}
≥0
+2λw ∥x∥2 > 0,
for nonzero x.
The regularization makes the Hessian matrices positive deﬁnite even if W , Z are
rank-deﬁcient. And now the matrix decomposition can be extended to any matrix even
when M > N. In rare cases, K can be chosen as K > max{M, N} such that a high-rank
approximation of A is obtained. However, in most scenarios, we want to ﬁnd the low-rank
approximation of A such that K < min{M, N}. For example, the ALS can be utilized
to ﬁnd the low-rank neural networks to reduce the memory of the neural networks whilst
increasing the performance (Lu, 2021b). Therefore, the minimizers are given by ﬁnding the
roots of the diﬀerential:
(
Z = (W ⊤W + λzI)−1W ⊤A;
W ⊤= (ZZ⊤+ λwI)−1ZA⊤.
(3.12)
The regularization parameters λz, λw ∈R are used to balance the trade-oﬀbetween the
accuracy of the approximation and the smoothness of the computed solution. The selection
of the parameters is typically problem dependent and can be obtained by cross-validation.
Again, we conclude the process in Algorithm 2.
97

3.4. MISSING ENTRIES
Algorithm 2 Alternating Least Squares with Regularization
Require: Matrix A ∈RM×N;
1: Initialize W ∈RM×K, Z ∈RK×N randomly without condition on the rank and the
relationship between M, N, K;
2: Choose a stop criterion on the approximation error δ;
3: Choose regularization parameters λw, λz;
4: Choose the maximal number of iterations C;
5: iter = 0;
▷Count for the number of iterations
6: while ∥A −W Z∥> δ and iter < C do
7:
iter = iter + 1;
8:
Z = (W ⊤W + λzI)−1W ⊤A ←arg minZ L(Z | W );
9:
W ⊤= (ZZ⊤+ λwI)−1ZA⊤←arg minW L(W | Z);
10: end while
11: Output W , Z;
3.4. Missing Entries
Since the matrix decomposition via the ALS is extensively used in the Netﬂix recommender
data, where many entries are missing since many users have not watched some movies
or they would not rate the movies for some reasons. We can employ an additional mask
matrix M ∈RM×N where mmn ∈{0, 1} means if the user n has rated the movie m or not.
Therefore, the loss function can be deﬁned as
L(W , Z) = ∥M ⊛A −M ⊛(W Z)∥2 ,
where ⊛is the Hadamard product between matrices. For example, the Hadamard product
for a 3 × 3 matrix A with a 3 × 3 matrix B is
A ⊛B =


a11
a12
a13
a21
a22
a23
a31
a32
a33

⊛


b11
b12
b13
b21
b22
b23
b31
b32
b33

=


a11b11
a12b12
a13b13
a21b21
a22b22
a23b23
a31b31
a32b32
a33b33

.
To ﬁnd the solution of the problem, we decompose the updates in Equation (3.12) into:
(
zn = (W ⊤W + λzI)−1W ⊤an,
for n ∈{1, 2, . . . , N};
wm = (ZZ⊤+ λwI)−1Zbm,
for m ∈{1, 2, . . . , M},
(3.13)
where Z = [z1, z2, . . . , zN], A = [a1, a2, . . . , aN] are the column partitions of Z, A respec-
tively. And W ⊤= [w1, w2, . . . , wM], A⊤= [b1, b2, . . . , bM] are the column partitions of
W ⊤, A⊤respectively. The factorization of the updates indicates the update can be done
in a column-by-column fashion.
Given W .
Let on ∈RM denote the movies rated by user n where onm = 1 if user n
has rated movie m, and onm = 0 otherwise. Then the n-th column of A without missing
entries can be denoted as the Matlab style notation an[on]. And we want to approximate
98

JUN LU
CHAPTER 3. ALTERNATING LEAST SQUARES
the existing n-th column by an[on] ≈W [on, :]zn which is actually a rank-one least squares
problem:
zn =

W [on, :]⊤W [on, :] + λzI
−1
W [on, :]⊤an[on],
for n ∈{1, 2, . . . , N}.
(3.14)
Moreover, the loss function with respect to zn can be described by
L(zn | W ) =
X
m∈on

amn −w⊤
mzn
2
and if we are concerned about the loss for all users:
L(Z | W ) =
N
X
n=1
X
m∈on

amn −w⊤
mzn
2
.
Given Z.
Similarly, if pm ∈RN denotes the users that have rated the movie m with
pmn = 1 if the movie m has been rated by user n. Then the m-th row of A without missing
entries can be denoted as the Matlab style notation bm[pm]. And we want to approximate
the existing m-th row by bm[pm] ≈Z[:, pm]⊤wm, 3 which again is a rank-one least squares
problem:
wm = (Z[:, pm]Z[:, pm]⊤+ λwI)−1Z[:, pm]bm[pm],
for m ∈{1, 2, . . . , M}.
(3.15)
Moreover, the loss function with respect to wn can be described by
L(wn | Z) =
X
n∈pm

amn −w⊤
mzn
2
and if we are concerned about the loss for all users:
L(W | Z) =
M
X
m=1
X
n∈pm

amn −w⊤
mzn
2
.
The procedure is again formulated in Algorithm 3.
3.5. Vector Inner Product
We have seen that the ALS is to ﬁnd matrices W , Z such that W Z can approximate
A ≈W Z in terms of least squared loss:
min
W ,Z
N
X
n=1
M
X
m=1

amn −w⊤
mzn
2
,
that is, each entry amn in A can be approximated by the inner product between the two
vectors w⊤
mzn. The geometric deﬁnition of the vector inner product is given by
w⊤
mzn = ∥w∥· ∥z∥cos θ,
3. Note that Z[:, pm]⊤is the transpose of Z[:, pm], which is equal to Z⊤[pm, :], i.e., transposing ﬁrst and
then selecting.
99

3.6. GRADIENT DESCENT (GD)
Algorithm 3 Alternating Least Squares with Missing Entries and Regularization
Require: Matrix A ∈RM×N;
1: Initialize W ∈RM×K, Z ∈RK×N randomly without condition on the rank and the
relationship between M, N, K;
2: Choose a stop criterion on the approximation error δ;
3: Choose regularization parameters λw, λz;
4: Compute the mask matrix M from A;
5: Choose the maximal number of iterations C;
6: iter = 0;
▷Count for the number of iterations
7: while ∥M ⊛A −M ⊛(W Z)∥2 > δ and iter < C do
8:
iter = iter + 1;
9:
for n = 1, 2, . . . , N do
10:
zn =
 W [on, :]⊤W [on, :] + λzI
−1 W [on, :]⊤an[on];
▷n-th column of Z
11:
end for
12:
for m = 1, 2, . . . , M do
13:
wm = (Z[:, pm]Z[:, pm]⊤+ λwI)−1Z[:, pm]bm[pm];
▷m-th column of W ⊤
14:
end for
15: end while
16: Output W ⊤= [w1, w2, . . . , wM], Z = [z1, z2, . . . , zN];
where θ is the angle between w and z. So if the vector norms of w, z are determined, the
smaller the angle, the larger the inner product.
Come back to the Netﬂix data, where the rating are ranging from 0 to 5, and the larger
the “better” (the user more likes the movie). If wm and zn fall “close” enough, then w⊤z
will have a larger value. This reveals the meaning behind the ALS where wm represents
the features of movie m, whilst zn contains the features of user n. In other words, the ALS
associates each user with a latent vector of preference, and each movie with a latent vector
of attributes. And each element in wm and zn represents a same feature. For example, it
could be that the second feature wm2 4 represents if the movie is an action movie or not,
and zn2 denotes if the user n likes action movies or not. If it happens the case, then w⊤
mzn
will be large and approximates amn well.
Note that, in the decomposition A ≈W Z, we know the rows of W contain the hidden
features of the movies, and the columns of Z contain the hidden features of the users.
However, we cannot identify what are the meanings of the rows of W or the columns of Z
explicitly. We know they could be something like categories or genres of the movies, that
provide some underlying connections between the users and the movies, but we cannot be
sure what exactly they are. This is where the terminology “hidden” comes from.
3.6. Gradient Descent (GD)
In Algorithm 1, 2, and 3, we reduce the loss via the inverse of matrices. The reality, however,
is frequently far from straightforward, particularly in the big data era of today. As data
4. wm2 is the second element of vector wm.
100

JUN LU
CHAPTER 3. ALTERNATING LEAST SQUARES
volumes explode, the size of the inversion matrix will grow at a pace proportional to the
cube of the number of samples (e.g., the matrix inversion algorithm by LU decomposition
in Lu (2021b)), which poses a great challenge to the storage and computational resources.
This leads to the creation of an ongoing development of the gradient-based optimization
technique. The gradient descent (GD) method and its derivation, the stochastic gradient
descent (SGD) method, are among them the simplest, fastest, and most eﬃcient methods
(Lu, 2022d). Convex loss function optimization problems are frequently solved using this
type of approach. We now go into more details about its principle.
In Equation (3.13), we obtain the column-by-column update directly from the full matrix
way in Equation (3.12) (with regularization considered). Now let’s see what’s behind the
idea. Following Equation (3.10), the loss under the regularization is,
L(W , Z) = ∥W Z −A∥2 + λw ∥W ∥2 + λz ∥Z∥2 ,
λw > 0, λz > 0,
(3.16)
Since we are now considering the minimization of the above loss with respect to zn, we can
decompose the loss into
L(zn) = ∥W Z −A∥2 + λw ∥W ∥2 + λz ∥Z∥2
= ∥W zn −an∥2 + λz ∥zn∥2 +
X
i̸=n
∥W zi −ai∥2 + λz
X
i̸=n
∥zi∥2 + λw ∥W ∥2
|
{z
}
Czn
,
(3.17)
where Czn is a constant with respect to zn, and Z = [z1, z2, . . . , zN], A = [a1, a2, . . . , aN]
are the column partitions of Z, A respectively. Taking the diﬀerential
∂L(zn)
∂zn
= 2W ⊤W zn −2W ⊤an + 2λzzn,
under which the root is exactly the ﬁrst update of the column fashion in Equation (3.13):
zn = (W ⊤W + λzI)−1W ⊤an,
for n ∈{1, 2, . . . , N}.
Similarly, we can decompose the loss with respect to wm,
L(wm) = ∥W Z −A∥2 + λw ∥W ∥2 + λz ∥Z∥2
=
Z⊤W −A⊤
2
+ λw
W ⊤
2
+ λz ∥Z∥2
=
Z⊤wm −bn

2
+ λw ∥wm∥2 +
X
i̸=m
Z⊤wi −bi

2
+ λw
X
i̸=m
∥wi∥2 + λz ∥Z∥2
|
{z
}
Cwm
,
(3.18)
where Cwm is a constant with respect to wm, and W ⊤= [w1, w2, . . . , wM], A⊤= [b1, b2, . . . ,
bM] are the column partitions of W ⊤, A⊤respectively. Analogously, taking the diﬀerential
with respect to wm, it follows that
∂L(wm)
∂wm
= 2ZZ⊤wm −2Zbn + 2λwwm,
101

3.6. GRADIENT DESCENT (GD)
under which the root is exactly the second update of the column fashion in Equation (3.13):
wm = (ZZ⊤+ λwI)−1Zbm,
for m ∈{1, 2, . . . , M}.
Now suppose we write out the iteration number (k = 1, 2, . . .) as the superscript and we
want to ﬁnd the updates {z(k+1)
n
, w(k+1)
m
} in the (k + 1)-th iteration base on {Z(k), W (k)}
in the k-th iteration :







z(k+1)
n
←arg min
z(k)
n
L(z(k)
n );
w(k+1)
m
←arg min
w(k)
m
L(w(k)
m ).
For simplicity, we will be looking at z(k+1)
n
←arg minz(k)
n L(z(k)
n ), and the derivation for the
update on w(k+1)
m
will be the same.
Approximation by linear update.
Suppose we want to approximate z(k+1)
n
by a linear
update on z(k)
n :
Linear Update:
z(k+1)
n
= z(k)
n
+ ηv.
The problem now turns to the solution of v such that
v = arg min
v
L(z(k)
n
+ ηv).
By Taylor’s formula (Appendix A.1, p. 231), L(z(k)
n
+ ηv) can be approximated by
L(z(k)
n
+ ηv) ≈L(z(k)
n ) + ηv⊤∇L(z(k)
n ),
where η is small enough and ∇L(z(k)
n ) is the gradient of L(z) at z(k)
n . Then a search under
the condition ∥v∥= 1 given positive η is shown as follows:
v = arg min
∥v∥=1
L(z(k)
n
+ ηv) ≈arg min
∥v∥=1
n
L(z(k)
n ) + ηv⊤∇L(z(k)
n )
o
.
This is known as the greedy search. The optimal v can be obtained by
v = −∇L(z(k)
n )
∇L(z(k)
n )

,
i.e., v is in the opposite direction of ∇L(z(k)
n ). Therefore, the update of z(k+1)
n
is reasonable
to be taken as
z(k+1)
n
= z(k)
n
+ ηv = z(k)
n
−η ∇L(z(k)
n )
∇L(z(k)
n )

,
which is usually called the gradient descent (GD). Similarly, the gradient descent of w(k+1)
m
is given by
w(k+1)
m
= w(k)
m + ηv = w(k)
m −η ∇L(w(k)
m )
∇L(w(k)
m )

.
The updated procedure of Algorithm 2 via a gradient descent way is then formulated in
Algorithm 4.
102

JUN LU
CHAPTER 3. ALTERNATING LEAST SQUARES
Algorithm 4 Alternating Least Squares with Full Entries and Gradient Descent
Require: Matrix A ∈RM×N;
1: Initialize W ∈RM×K, Z ∈RK×N randomly without condition on the rank and the
relationship between M, N, K;
2: Choose a stop criterion on the approximation error δ;
3: Choose regularization parameters λw, λz, and step size ηw, ηz;
4: Choose the maximal number of iterations C;
5: iter = 0;
▷Count for the number of iterations
6: while ∥A −(W Z)∥2 > δ and iter < C do
7:
iter = iter + 1;
8:
for n = 1, 2, . . . , N do
9:
z(k+1)
n
= z(k)
n
−ηz
∇L(z(k)
n )
∇L(z(k)
n )
;
▷n-th column of Z
10:
end for
11:
for m = 1, 2, . . . , M do
12:
w(k+1)
m
= w(k)
m −ηw
∇L(w(k)
m )
∇L(w(k)
m )
;
▷m-th column of W ⊤
13:
end for
14: end while
15: Output W ⊤= [w1, w2, . . . , wM], Z = [z1, z2, . . . , zN];
Geometrical interpretation of gradient descent.
Lemma 3.8: (Direction of Gradients)
An important fact is that the gradients of
variables given a loss function are orthogonal to level curves (a.k.a., level surface).
Proof [of Lemma 3.8] This is equivalent to proving that the gradient is orthogonal to the
tangent of the level curve. For simplicity, let’s ﬁrst look at the 2-dimensional case. Suppose
the level curve has the form f(x, y) = c. This implicitly gives a relation between x and y
such that y = y(x) where y can be thought of as a function of x. Therefore, the level curve
can be written as
f(x, y(x)) = c.
The chain rule indicates
∂f
∂x
dx
dx
|{z}
=1
+∂f
∂y
dy
dx = 0.
Therefore, the gradient is perpendicular to the tangent:
∂f
∂x, ∂f
∂y

·
dx
dx, dy
dx

= 0.
In full generality, suppose the level curve of a vector x ∈Rn: f(x) = f(x1, x2, . . . , xn) = c.
Each variable xi can be regarded as a function of a variable t on the level curve f(x) = c:
f(x1(t), x2(t), . . . , xn(t)) = c. Diﬀerentiate the equation with respect to t by chain rule:
∂f
∂x1
dx1
dt + ∂f
∂x2
dx2
dt + . . . + ∂f
∂xn
dxn
dt = 0.
103

3.7. REGULARIZATION: A GEOMETRICAL INTERPRETATION
Therefore, the gradient is perpendicular to the tangent in n-dimensional case:
 ∂f
∂x1
, ∂f
∂x2
, . . . , ∂f
∂xn

·
dx1
dt , dx2
dt , . . . dxn
dt

= 0.
This completes the proof.
The lemma above reveals the geometrical interpretation of gradient descent. For ﬁnding a
solution to minimize a convex function L(z), gradient descent goes to the negative gradient
direction that can decrease the loss. Figure 3.2 depicts a 2-dimensional case, where −∇L(z)
pushes the loss to decrease for the convex function L(z).
z1
1.01.52.02.53.03.54.04.55.0
z2
0.00.51.01.52.02.53.03.54.0
L(z)
0
5
10
15
20
25
30
(a) A 2-dimensional convex function L(z)
z1
z2
L(z)
(b) L(z) = c is a constant
Figure 3.2: Figure 3.2(a) shows a function “density” and a contour plot (blue=low, yel-
low=high) where the upper graph is the “density”, and the lower one is the projection of it
(i.e., contour). Figure 3.2(b): −∇L(z) pushes the loss to decrease for the convex function
L(z).
3.7. Regularization: A Geometrical Interpretation
We have seen in Section 3.3 that regularization can extend the ALS to general matrices.
The gradient descent can reveal the geometrical meaning of the regularization. To avoid
confusion, we denote the loss function without regularization by l(z) and the loss with
regularization by L(z) = l(z) + λz ∥z∥2 where l(z) : Rn →R. When minimizing l(z),
descent method will search in Rn for a solution. However, in machine learning, searching
in the whole space Rn can cause overﬁtting. A partial solution is to search in a subset of
the vector space, e.g., searching in z⊤z < C for some constant C. That is
arg min
z
l(z),
s.t.,
z⊤z ≤C.
As shown above, a trivial gradient descent method will go further in the direction of −∇l(z),
i.e., update z by z ←z −η∇l(z) for a small step size η. When the level curve is l(z) = c1
and the current position of z = z1 where z1 is the intersection of z⊤z = C and l(z) = c1,
the descent direction −∇l(z1) will be perpendicular to the level curve of l(z1) = c1 as
104

JUN LU
CHAPTER 3. ALTERNATING LEAST SQUARES
𝑙(𝑧) = 𝑐1
𝑧𝑇𝑧=C
-𝛻𝑙(𝑧1)
𝑧1
𝑣1
0
𝑙(𝑧) = 𝑐2
𝑧𝑇𝑧=C
-𝛻𝑙(𝑧2)
𝑧2
𝑣2
0
𝑤
𝑧1
-𝛻𝑙(𝑧1)
𝑤
−2𝜆𝑧1
𝑧∗
𝑧∗
Figure 3.3: Constrained gradient descent with z⊤z ≤C.
The green vector w is the
projection of v1 into z⊤z ≤C where v1 is the component of −∇l(z) perpendicular to z1.
The right picture is the next step after the update in the left picture. z⋆denotes the optimal
solution of {min l(z)}.
shown in the left picture of Figure 3.3 (by Lemma 3.8). However, if we further restrict that
the optimal value can only be in z⊤z ≤C, the trivial descent direction −∇l(z1) will lead
z2 = z1 −η∇l(z1) outside of z⊤z ≤C. A solution is to decompose the step −∇l(z1) into
−∇l(z1) = az1 + v1,
where az1 is the component perpendicular to the curve of z⊤z = C, and v1 is the component
parallel to the curve of z⊤z = C. Keep only the step v1, then the update
z2 = project(z1 + ηv1) = project

z1 + η (−∇l(z1) −az1)
|
{z
}
v1

5
will lead to a smaller loss from l(z1) to l(z2) and it still matches the prerequisite of z⊤z ≤C.
This is known as the projection gradient descent. It is not hard to see that the update
z2 = project(z1 + ηv1) is equivalent to ﬁnding a vector w (shown by the green vector in
the left picture of Figure 3.3) such that z2 = z1 + w is inside the curve of z⊤z ≤C.
Mathematically, the w can be obtained by −∇l(z1) −2λz1 for some λ as shown in the
middle picture of Figure 3.3. This is exactly the negative gradient of L(z) = l(z) + λ ∥z∥2
such that
−∇L(z) = −∇l(z) −2λz,
and
w = −∇L(z)
leads to
−−−−−→
z2
= z1 + w = z1 −∇L(z).
And in practice, a small step size η can avoid going outside the curve of z⊤z ≤C:
z2 = z1 −η∇L(z),
5. where the project(x) will project the vector x to the closest point inside z⊤z ≤C. Notice here the
direct update z2 = z1 + ηv1 can still make z2 outside the curve of z⊤z ≤C.
105

3.8. STOCHASTIC GRADIENT DESCENT (SGD)
which is exactly what we have discussed in Section 3.3, the regularization term.
1.0
0.5
0.0
0.5
1.0
x1
1.0
0.5
0.0
0.5
1.0
x2
Unit ball of p-norm in 2D
p=0
p=0.5
p=1
p=2
p
Figure 3.4: Unit ball of Lp norm in 2-dimensional space. The Lp norm over a vector x is
deﬁned as Lp(x) = (P
i |xi|p)1/p. When p < 1, the metric is not a norm since it does not
meet the triangle inequality property of a norm deﬁnition.
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
p=
(a) p = ∞
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
p=2
(b) p = 2
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
p=1
(c) p = 1
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
p=0.5
(d) p = 0.5
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
1.00
0.75
0.50
0.25
0.00
0.25
0.50
0.75
1.00
p=0
(e) p = 0
Figure 3.5: Unit ball of Lp norm in 3-dimensional space. When p < 1, the metric is not
a norm since it does not meet the triangle inequality property of a norm deﬁnition.
Sparsity.
In rare cases, we want to ﬁnd a sparse solution z such that l(z) is minimized.
Regularization to be constrained in ∥z∥1 ≤C exists to this purpose where ∥·∥1 is the L1
norm of a vector or a matrix. The illustration of the L1 in 2-dimensional and 3-dimensional
space is shown in Figure 3.4 and 3.5.
Similar to the previous case, the L1 constrained
optimization pushes the gradient descent towards the border of the level of ∥z∥1 = C. The
situation in the 2-dimensional case is shown in Figure 3.6. In a high-dimensional case, many
elements in z will be pushed into the breakpoint of ∥z∥1 = C as shown in the right picture
of Figure 3.6.
3.8. Stochastic Gradient Descent (SGD)
The gradient descent method is a good optimization algorithm, but it has some defects in
real applications. In order to understand the problem of the gradient descent method, we
106

JUN LU
CHAPTER 3. ALTERNATING LEAST SQUARES
𝑙(𝑧) = 𝑐1
||𝑧||1=C
-𝛻𝑙(𝑧1)
𝑧1
𝑣1
0
𝑙(𝑧) = 𝑐2
-𝛻𝑙(𝑧2)
𝑧2
𝑣2
0
||𝑧||1=C
breakpoint
𝑧∗
𝑧∗
Figure 3.6: Constrained gradient descent with ∥z∥1 ≤C, where the red dot denotes the
breakpoint in L1 norm.
The right picture is the next step after the update in the left
picture. z⋆denotes the optimal solution of {min l(z)}.
take the mean squared error (MSE) from Equation (3.2):
1
MN min
W ,Z
N
X
n=1
M
X
m=1

amn −w⊤
mzn
2
,
(3.19)
The MSE needs to calculate the residual emn = (amn −w⊤
mzn)2 between the predicted value
and the real value of each observed entry amn, and ﬁnally get the total sum of residual
squares e = PMN
m,n=1 emn. When there is a large amount of training entries (i.e., MN is
large), the whole training process becomes very slow. In addition, the gradient between
diﬀerent input samples may cancel out, resulting in small changes in the entire parameter.
Based on the above problems, researchers have improved the gradient descent method to
the stochastic gradient descent (SGD) method. The core idea of the stochastic gradient
descent method is to randomly select a single sample from all training samples each time.
We consider again the per-example loss:
L(W , Z) =
N
X
n=1
M
X
m=1

amn −w⊤
mzn
2
+ λw
M
X
m=1
∥wm∥2 + λz
N
X
n=1
∥zn∥2 .
And when we iteratively decrease the per-example loss term l(wm, zn) =
 amn −w⊤
mzn
2
for all m ∈{1, 2, . . . , M}, n ∈{1, 2, . . . , N}, the full loss L(W , Z) can also be decreased.
This is also known as the stochastic coordinate descent. The diﬀerentials with respect to
wm, zn, and their roots are given by



















∇l(zn) = ∂l(wm, zn)
∂zn
= 2wmw⊤
mzn + 2λwwm −2amnwm
leads to
−−−−−→zn = amn(wmw⊤
m + λzI)−1wm;
∇l(wm) = ∂l(wm, zn)
∂wm
= 2znz⊤
n wm + 2λzzn −2amnzn
leads to
−−−−−→wm = amn(znz⊤
n + λwI)−1wn.
107

3.9. BIAS TERM
or analogously, the update can be done by gradient descent, and since we update by per-
example loss, it is also known as the stochastic gradient descent (SGD)







zn = zn −ηz
∇l(zn)
∥∇l(zn)∥;
wm = wm −ηw
∇l(wm)
∥∇l(wm)∥.
The stochastic gradient descent update for ALS is formulated in Algorithm 5.
And in
practice, the m, n in the algorithm can be randomly produced, that’s where the name
stochastic comes from.
Algorithm 5 Alternating Least Squares with Full Entries and Stochastic Gradient Descent
Require: Matrix A ∈RM×N;
1: Initialize W ∈RM×K, Z ∈RK×N randomly without condition on the rank and the
relationship between M, N, K;
2: Choose a stop criterion on the approximation error δ;
3: Choose regularization parameters λw, λz, and step size ηw, ηz;
4: Choose the maximal number of iterations C;
5: iter = 0;
▷Count for the number of iterations
6: while ∥A −(W Z)∥2 > δ and iter < C do
7:
iter = iter + 1;
8:
for n = 1, 2, . . . , N do
9:
for m = 1, 2, . . . , M do
▷in practice, m, n can be randomly produced
10:
zn = zn −ηz
∇l(zn)
∥∇l(zn)∥;
▷n-th column of Z
11:
wm = wm −ηw
∇l(wm)
∥∇l(wm)∥;
▷m-th column of W ⊤
12:
end for
13:
end for
14: end while
15: Output W ⊤= [w1, w2, . . . , wM], Z = [z1, z2, . . . , zN];
3.9. Bias Term
In ordinary least squares, a bias term is added to the raw matrix as shown in Equation (3.1).
A similar idea can be applied to the ALS problem. We can add a ﬁxed column with all 1’s
to the last column of W , thus an extra row should be added to the last row of Z to ﬁt
the features introduced by the bias term in W . Analogously, a ﬁxed row with all 1’s can
be added to the ﬁrst row of Z, and an extra column in the ﬁrst column of W to ﬁt the
features. The situation is shown in Figure 3.7.
108

JUN LU
CHAPTER 3. ALTERNATING LEAST SQUARES

N
M
A

K
M
W

N
K
Z


)
2
(
~

K
M
W
N
K
Z

)
2
(
~


Figure 3.7: Bias terms in alternating least squares where the yellow entries denote ones
(which are ﬁxed) and cyan entries denote the added features to ﬁt the bias terms. The
dotted boxes give an example of how the bias terms work.
Following the loss with respect to the columns of Z in Equation (3.17), suppose ezn =
 1
zn

is the n-th column of eZ, we have
L(zn) =
f
W eZ −A

2
+ λw
f
W

2
+ λz
 eZ

2
=
f
W
 1
zn

−an

2
+
λz ∥ezn∥2
|
{z
}
=λz∥zn∥2+λz
+
X
i̸=n
f
W ezi −ai

2
+ λz
X
i̸=n
∥ezi∥2 + λw
f
W

2
=


w0
W
  1
zn

−an

2
+ λz ∥zn∥2 + Czn =

W zn −(an −w0)
|
{z
}
an

2
+ λz ∥zn∥2 + Czn,
(3.20)
where w0 is the ﬁrst column of f
W , W is the last K columns of f
W , and Czn is a constant
with respect to zn.
Let an = an −w0, the update of zn is just similar to the one in
Equation (3.17) where the diﬀerential is given by:
∂L(zn)
∂zn
= 2W
⊤W zn −2W
⊤an + 2λzzn.
Therefore, the update on zn is given by the root of the above diﬀerential:
update on ezn is





zn = (W
⊤W + λzI)−1W
⊤an,
for n ∈{1, 2, . . . , N};
ezn =
 1
zn

.
109

3.10. APPLICATIONS
Figure 3.8: A gray ﬂag image to
be compressed. The size of the im-
age is 600 × 1200 with a rank of
402.
Similarly, following the loss with respect to each row of W in Equation (3.18), suppose
e
wm =
wm
1

is the m-th row of f
W (or m-th column of f
W ⊤), we have
L(wm) =
 eZ⊤f
W −A⊤
2
+ λw
f
W ⊤
2
+ λz
 eZ

2
=
 eZ⊤e
wm −bm

2
+
λw ∥e
wm∥2
|
{z
}
=λw∥wm∥2+λw
+
X
i̸=m
 eZ⊤e
wi −bi

2
+ λw
X
i̸=m
∥e
wi∥2 + λz
 eZ

2
=

h
Z
⊤
z0
i wm
1

−bm

2
+ λw ∥wm∥2 + Cwm
=
Z
⊤wm −(bm −z0)

2
+ λw ∥wm∥2 + Cwm,
(3.21)
where z0 is the last column of eZ⊤and Z
⊤contains the remaining columns of it, Cwm is a
constant with respect to wm, and W ⊤= [w1, w2, . . . , wM], A⊤= [b1, b2, . . . , bM] are the
column partitions of W ⊤, A⊤respectively. Let bm = bm −z0, the update of wm is again
just similar to the one in Equation (3.18) where the diﬀerential is given by:
∂L(wmd)
∂wm
= 2Z · Z
⊤wm −2Z · bm + 2λwwm.
Therefore the update on wm is given by the root of the above diﬀerential
update on e
wm is





wm = (Z · Z
⊤+ λwI)−1Z · bm,
for m ∈{1, 2, . . . , M};
e
wm =
wm
1

.
Similar updates by gradient descent under the bias terms or treatment on missing entries
can be deduced and we shall not repeat the details (see Section 3.6 and 3.4 for a reference).
3.10. Applications
110

JUN LU
CHAPTER 3. ALTERNATING LEAST SQUARES
(a) σ1u1v⊤
1
F1 = 60217
(b) σ2u2v⊤
2
F2 = 120150
(c) σ3u3v⊤
3
F3 = 124141
(d) σ4u4v⊤
4
F4 = 125937
(e) σ5u5v⊤
5
F5 = 126127
(f) All 5 singu-
lar
values:
P5
i=1 σiuiv⊤
i ,
F =44379
(g) c1r⊤
1
G1 = 60464
(h) c2r⊤
2
G2 = 122142
(i) c3r⊤
3
G3 = 123450
(j) c4r⊤
4
G4 = 125975
(k) c5r⊤
5
G5 = 124794
(l) Pseudoskeleton
Rank
5
P5
i=1 cir⊤
i ,
G = 45905.
(m) w1z⊤
1
S1 = 82727
(n) w2z⊤
2
S2 = 107355
(o) w3z⊤
3
S3 = 119138
(p) w4z⊤
4
S4 = 120022
(q) w5z⊤
5
S5 = 120280
(r) ALS Rank 5
P5
i=1 wiz⊤
i ,
S = 52157.
Figure 3.9: Image compression for gray ﬂag image into a rank-5 matrix via the SVD,
and decompose into 5 parts where σ1 ≥σ2 ≥. . . ≥σ5, i.e., F1 ≤F2 ≤. . . ≤F5 with
Fi =
σiuiv⊤−A

F for i ∈{1, 2, . . . , 5}. And reconstruct images by single singular value
and its corresponding left and right singular vectors, cir⊤
i , wiz⊤
i
respectively.
Upper:
SVD; Middle: Pseudoskeleton; Lower: ALS.
3.10.1 Low-Rank Approximation
Figure 3.8 shows an example of a gray image to be compressed. The size of the image
is 600 × 1200 with a rank of 402. Low-rank approximation via the ALS decomposition,
singular value decomposition (SVD), and Pseudoskeleton decomposition (Lu, 2021b) can
be applied to ﬁnd the compression.
In Figure 3.9(a) to 3.9(f), we approximate the image into a rank-5 matrix by truncated
SVD: A ≈P5
i=1 σiuiv⊤
i
(Lu, 2021b). It is known that the singular values contain the
spectrum information with higher singular values containing lower-frequency information.
And low-frequency contains more useful information (Leondes, 1995). We ﬁnd that the
image, σ1u1v⊤
1 , reconstructed by the ﬁrst singular value σ1, ﬁrst left singular vector u1,
and ﬁrst right singular vector v1 is very close to the original ﬂag image; and second to
the ﬁfth images reconstructed by the corresponding singular values and singular vectors
containing more details of the ﬂag to reconstruct the raw image.
Similar results can be observed for the low-rank approximation via the pseudoskeleton
decomposition (See Lu (2021b) for more details). At a high level, the pseudoskeleton de-
composition ﬁnds the low-rank approximation by A ≈CR where C ∈Rm×γ, R ∈Rγ×n if
111

3.10. APPLICATIONS
A ∈Rm×n such that C and R are rank-γ matrices. Suppose γ = 5, and
C = [c1, c2, . . . , c5],
and
R =


r⊤
1
r⊤
2...
r⊤
5

,
are the column and row partitions of C and R respectively. Then A can be approximated
by P5
i=1 cir⊤
i . The partitions are ordered such that
c1r⊤
1 −A

F
|
{z
}
G1
≤
c2r⊤
2 −A

F
|
{z
}
G2
≤. . . ≤
c5r⊤
5 −A

F
|
{z
}
G5
.
We observe (in Figure 3.9(g) to 3.9(l)) that c1r⊤
1 works similarly to that of σ1u1v⊤where
the reconstruction errors measured by the Frobenius norm are very close (60,464 in the
pseudoskeleton case compared to that of 60,217 in the SVD case). This is partly because
the pseudoskeleton decomposition relies on the SVD such that c1r⊤
1 internally has the
largest “singular value” meaning in this sense (Lu, 2021b).
Similarly, the ALS approximation is given by A ≈W Z where W ∈Rm×γ, Z ∈Rγ×n if
A ∈Rm×n such that W and Z are rank-γ matrices. Suppose
W = [w1, w2, . . . , w5],
and
Z =


z⊤
1
z⊤
2...
z⊤
5

,
are the column and row partitions of W , Z respectively 6. Then A can be approximated
by P5
i=1 wiz⊤
i . The partitions are ordered such that
w1z⊤
1 −A

F
|
{z
}
S1
≤
w2z⊤
2 −A

F
|
{z
}
S2
≤. . . ≤
w5z⊤
5 −A

F
|
{z
}
S5
.
We observe (in Figure 3.9(m) to 3.9(r)) that w1z⊤
1 works slightly diﬀerent to that of
σ1u1v⊤where the reconstruction errors measured by Frobenius norm are not close as well
(82,727 in the ALS case compared to that of 60,217 in the SVD case). As we mentioned
previously, c1r⊤
1 works similarly to that of σ1u1v⊤since the pseudoskeleton relies on the
SVD. However, in ALS, the reconstruction is all from least squares optimization. The key
diﬀerence between ALS and SVD is in the fact that, in SVD, the importance of each vector
in the basis is relative to the value of the singular value associated with that vector. This
usually means that the ﬁrst vector of the basis dominates and is the most used vector to
reconstruct data; then the second vector and so on. So the bases in SVD have an implicit
hierarchy and that doesn’t happen in ALS where we ﬁnd the second component w2z⊤
2 via
ALS in Figure 3.9(n) plays an important role in the reconstruction of the original ﬁgure;
whereas the second component σ2u2v⊤
2 via SVD in Figure 3.9(b) plays a small role in the
reconstruction.
6. For simplicity, note that this deﬁnition is diﬀerent from what we have deﬁned in Section 3.2 where we
deﬁne wi as the rows of W .
112

JUN LU
CHAPTER 3. ALTERNATING LEAST SQUARES
20
40
60
80
100
Rank
10000
20000
30000
40000
50000
60000
Frobenius Norm
SVD
Skeleton
ALS1: 
w =
z = 0
ALS2: 
w =
z = 0.03
ALS3: 
w =
z = 0.08
ALS4: 
w =
z = 0.15
ALS5: 
w =
z = 0.2
Figure 3.10: Comparison of reconstruction
errors measured by Frobenius norm among
the SVD, pseudoskeleton, and ALS where the
approximated rank ranges from 3 to 100. ALS
with well-selected parameters works similarly
to SVD.
We ﬁnally compare low-rank approximation among the SVD, pseudoskeleton, and ALS
with diﬀerent ranks (3 to 100). Figure 3.11 shows the diﬀerence of each compression with
rank 90, 60, 30, 10. We observe that the SVD reconstructs well with rank 90, 60, 30. The
pseudoskeleton approximation compresses well in the black horizontal and vertical lines in
the image. But it performs poorly in the details of the ﬂag. ALS works similarly to the
SVD in terms of visual expression and reconstruction errors measured by the Frobenius
norm. Figure 3.10 shows the comparison of the reconstruction errors among the SVD, the
pseudoskeleton, and the ALS approximations measured by the Frobenius norm ranging from
rank 3 to 100 where we ﬁnd in all cases, the truncated SVD does best in terms of Frobenius
norm. Similar results can be observed when applied to the spectral norm. The ALS works
better than the pseudoskeleton decomposition when λw = λz = 0.15. An interesting cutoﬀ
happens when λw = λz = {0.03, 0.08, 0.15}. That is, when the value of rank increases, the
ALS will be very close to the SVD in the sense of low-rank approximation.
3.10.2 Movie Recommender
The ALS is extensively developed for the movie recommender system.
To see this, we
obtain the “MovieLens 100K” data set from MovieLens (Harper and Konstan, 2015)7. It
consists of 100,000 ratings from 943 users on 1,682 movies. The rating values go from 0 to
5. The data was collected through the MovieLens website during the seven-month period
from September 19th, 1997 through April 22nd, 1998. This data has been cleaned up -
users who had less than 20 ratings or did not have complete demographic information were
removed from this data set such that simple demographic info for the users (age, gender,
occupation, zip) can be obtained. However, we will only work on the trivial rating matrix
8.
The data set is split into training and validation data, around 95,015 and 4,985 rat-
ings respectively. The error is measured by root mean squared error (RMSE). The RMSE
is frequently used as a measure of the diﬀerence between values.
For a set of values
7. http://grouplens.org
8. In the next chapters, the data set is further cleaned by removing movies with less than 3 users such that
1,473 movies are kept accordingly.
113

3.10. APPLICATIONS
(a) SVD with rank 90
Frobenius norm=6,498
(b) Pseudoskeleton with rank 90
Frobenius norm=13,751
(c) ALS with rank 90
Frobenius norm=6,622
(d) SVD with rank 60
Frobenius norm=8,956
(e) Pseudoskeleton with rank 60
Frobenius norm=14,217
(f) ALS with rank 60
Frobenius norm=9,028
(g) SVD with rank 30
Frobenius norm=14,586
(h) Pseudoskeleton with rank 30
Frobenius norm=17,853
(i) ALS with rank 30
Frobenius norm=18,624
(j) SVD with rank 10
Frobenius norm=31,402
(k) Pseudoskeleton with rank 10
Frobenius norm=33,797
(l) ALS with rank 10
Frobenius norm=33,449
Figure 3.11: Image compression for gray ﬂag image with diﬀerent ranks.
{x1, x2, . . . , xn} and its predictions {ˆx1, ˆx2, . . . , ˆxn}, the RMSE can be described as
RMSE(x, ˆx) =
v
u
u
t 1
n
n
X
i=1
(xi −ˆxi).
The minimal RMSE for validation is obtained when K = 185 and λw = λz = 0.15, and it
is equal to 0.806 as shown in Figure 3.12. Therefore, when the rating ranges from 0 to 5,
the ALS at least can predict whether the user likes to watch the movie (e.g., ranges 4 to 5)
or not (e.g., ranges 0 to 2).
Recommender 1.
A recommender system can work simply by suggesting the movie m
when amn ≥4 if user n has not rated the movie m.
Recommender 2.
Or in rare cases, it happens that the user n has rated all the movies
he likes (say rates ≥4). Then a partial solution is to ﬁnd out similar movies to the high-
rated movies to recommend. Suppose user n likes movie m very much and he has rated the
114

JUN LU
CHAPTER 3. ALTERNATING LEAST SQUARES
10
20
30
40
50
60
70
80
90
100
Rank
0.0
0.2
0.4
0.6
0.8
1.0
1.2
RMSE
Training: 
w =
z = 0.01
Training: 
w =
z = 0.08
Training: 
w =
z = 0.15
Training: 
w =
z = 0.2
(a) Training
10
20
30
40
50
60
70
80
90
100
Rank
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
RMSE
Training: 
w =
z = 0.01
Validation: 
w =
z = 0.08
Validation: 
w =
z = 0.15
Validation: 
w =
z = 0.2
(b) Validation
Figure 3.12: Comparison of training and validation error for “MovieLens 100K” data set
with diﬀerent reduction dimensions and regularization parameters.
movie m with 5: amn = 5. Under the ALS approximation A = W Z where each row of W
represents the hidden features of each movie (see Section 3.5 on the vector inner product).
The solution is given by ﬁnding the most similar movies to movie m that user n has not
rated (or watched). In mathematical language,
arg max
wi
similarity(wi, wm),
for all
i /∈on,
where wi’s are the rows of W representing the hidden feature of movie i and on is a mask
vector indicating the movies that user n has rated.
The method above relies on the similarity function between two vectors. The cosine
similarity is the most commonly used measure. It is deﬁned as the cosine of the angle
between the two vectors:
cos(x, y) =
x⊤y
∥x∥· ∥y∥,
where the value ranges from −1 to 1 with −1 is perfectly dissimilar and 1 is perfectly similar.
From the above deﬁnition, it follows that the cosine similarity depends only on the angle
between the two non-zero vectors, but not on their magnitudes since it can be regarded as
the inner product between the normalized version of these vectors. A second measure for
calculating similarity is known as the Pearson similarity:
Pearson(x, y) = Cov(x, y)
σx · σy
=
Pn
i=1(xi −¯x)(yi −¯y)
pPn
i=1(xi −¯x)2pPn
i=1(yi −¯y)2 ,
whose range varies between −1 and 1, where −1 is perfectly dissimilar, 1 is perfectly similar,
and 0 indicates no linear relationship. The Pearson similarity is usually used to measure
the linear correlation between two sets of data. It is the ratio between the covariance of
two variables and the product of their standard deviations.
Both Pearson correlation and cosine similarity are used in various ﬁelds, including ma-
chine learning and data analysis. Pearson correlation is commonly used in regression anal-
ysis, while cosine similarity is commonly used in recommendation systems and information
115

3.10. APPLICATIONS
retrieval as we will see the cosine similarity works better in the precision-recall curve anal-
ysis.
0.2
0.4
0.6
0.8
1.0
0
2
4
6
8
10
Probability
Cosine Bin Plot
InSample
OutSample
(a) Cosine Bin Plot
0.00
0.25
0.50
0.75
1.00
0
1
2
3
4
Probability
Pearson Bin Plot
InSample
OutSample
(b) Pearson Bin Plot
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Recall
0.5
0.6
0.7
0.8
0.9
1.0
Precision
0.3
0.4
0.5
0.6
0.7
0.75
0.8
0.81
0.82
0.85
0.9
0.95
0.96
0.97
0.98
0.99
0.3
0.4
0.5
0.6
0.7
0.75
0.8
0.81
0.82
0.85
0.9
0.95
0.96
0.97
0.98
0.99
Cosine 90%
Pearson 90%
Precision-Recall Curve
Pearson
Cosine
(c) PR Curve
Figure 3.13: Distribution of the insample and outsample under cosine and Pearson simi-
larity and the Precision-Recall curve of them.
Following from the example above on the MovieLens 100K data set, we choose λw = λz =
0.15 for the regularization and the rank 62 to minimize the RMSE. We want to look at the
similarity between diﬀerent movie hidden vectors and the goal is to see whether the matrix
factorization can help diﬀerentiate high-rated and low-rated movies, so that the system can
recommend the movies correlated to the existing high-rated movies for each user. Deﬁne
further the “insample” as the similarity between the movies having rates 5 for each user,
and “outsample” as the similarity between the movies having rates 5 and 1 for each user.
Figure 3.13(a) and 3.13(b) depict the bin plot of the distribution of insample and outsample
under cosine and Pearson similarity respectively. In both scenarios, a clear distinction is
observed between the distributions of the “insample” and “outsample” data showing that
the ALS decomposition can actually ﬁnd the hidden features of diﬀerent movies for each
user. Figure 3.13(c) shows the precision-recall (PR) curve of them where we ﬁnd the cosine
similarity works better such that it can ﬁnd out more than 73% of the potential high-rated
movies with a 90% precision. However, Pearson similarity can only separate out about 64%
of the high-rated movies to have a 90% precision. In practice, other measures can also be
explored, such as the negative Euclidean distance in which case the Euclidean distance can
measure the “dissimilarity” between two vectors, and a negative one thus represents the
similarity between them.
116

JUN LU
CHAPTER 3. ALTERNATING LEAST SQUARES
117

4
Nonnegative Matrix Factorization (NMF)
Contents
4.1
Nonnegative Matrix Factorization . . . . . . . . . . . . . . . . .
119
4.2
NMF via Multiplicative Update (MU) . . . . . . . . . . . . . .
119
4.3
Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
4.4
Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
4.5
Movie Recommender Context
. . . . . . . . . . . . . . . . . . .
123
118

JUN LU
CHAPTER 4. NONNEGATIVE MATRIX FACTORIZATION (NMF)
4.1. Nonnegative Matrix Factorization
Following the matrix factorization via the ALS, we now consider algorithms for solving the
nonnegative matrix factorization (NMF) problem:
• Given a nonnegative matrix A ∈RM×N
+
with rank r, ﬁnd nonnegative matrix factors
W ∈RM×K
+
and Z ∈RK×N
+
such that:
A ≈W Z.
That is, NMF is a machine learning technique used to factorize a nonnegative matrix into
two (or more) nonnegative matrices. Similar to the ALS approximation, the NMF can also
uncover hidden patterns and structures in the data represented by the nonnegative matrix.
NMF has a wide range of applications in areas such as text mining, image processing, and
recommender systems.
The accuracy of the approximation is determined by the loss, which is calculated as the
Frobenius norm of the diﬀerence between the original matrix and the approximation:
L(W , Z) = ∥W Z −A∥2 .
Early consideration of the NMF problem is due to Paatero et al. (1991); Paatero and Tapper
(1994); Anttila et al. (1995); Cohen and Rothblum (1993) where they called it positive
matrix factorization. While Lee and Seung (2000) later made the problem famous by the
multiplicative update in which case the factorization process is done through optimization
algorithms. See also the applications of the NMF in the survey paper Berry et al. (2007).
When we want to ﬁnd two nonnegative matrices W ∈RM×r
+
and Z ∈Rr×N
+
such that
A = W Z, the problem is known as the Exact NMF of A of size r. Exact NMF is NP-hard
(Gillis, 2020). Thus we only consider the approximation of NMF here.
4.2. NMF via Multiplicative Update (MU)
We consider the NMF via an alternating update. The hidden features in W , Z are modeled
as nonnegative vectors in low-dimensional space. These latent vectors are randomly initial-
ized and modiﬁed via an alternating multiplicative update rule to minimize the Kullback-
Leibler divergence between the observed and modeled matrices.
Following Section 3.2
(p. 92), given W ∈RM×K
+
, we want to update Z ∈RK×N
+
, the gradient with respect
to Z is given by Equation (3.3) (p. 93):
∂L(Z | W )
∂Z
= 2W ⊤(W Z −A) ∈RK×N.
Applying the gradient descent idea in Section 3.6 (p. 100), the trivial update on Z can be
done by
(GD on Z)
Z ←Z −η
∂L(Z | W )
∂Z

= Z −η

2W ⊤W Z −2W ⊤A

,
119

4.3. REGULARIZATION
where η is a small positive step size. Now if we impose a diﬀerent step size for each entry
of Z and incorporate the constant 2 into the step size, the update can be obtained by
(GD′ on Z)
zkn ←zkn −ηkn
2
∂L(Z | W )
∂Z

kn
= zkn −ηkn(W ⊤W Z −W ⊤A)kn,
k ∈{1, . . . , K}, n ∈{1, . . . , N},
where zkn is the (k, n)-th entry of Z. We further rescale the step size:
ηkn =
zkn
(W ⊤W Z)kn
,
then we obtain the update rule:
(MU on Z)
zkn ←zkn
(W ⊤A)kn
(W ⊤W Z)kn
,
k ∈{1, . . . , K}, n ∈{1, . . . , N},
which is known as the multiplicative update (MU) and is ﬁrst developed in Lee and Seung
(2000) and further discussed in Pauca et al. (2006). Analogously, the multiplicative update
on W can be obtained by
(MU on W ) wmk ←wmk
(AZ⊤)mk
(W ZZ⊤)mk
, m ∈{1, . . . , M}, k ∈{1, . . . , K}.
(4.1)
Theorem 4.1: (Convergence of Multiplicative Update)
The loss L(W , Z) =
∥W Z −A∥2 is non-increasing under the multiplicative update rules:









zkn ←zkn
(W ⊤A)kn
(W ⊤W Z)kn
,
k ∈{1, . . . , K}, n ∈{1, . . . , N};
wmk ←wmk
(AZ⊤)mk
(W ZZ⊤)mk
,
m ∈{1, . . . , M}, k ∈{1, . . . , K}.
We refer the proof of the above theorem to Lee and Seung (2000). Clearly the approxima-
tions W and Z remain nonnegative during the updates. It is generally best to update W
and Z “simultaneously”, instead of updating each matrix completely before the other. In
this case, after updating a row of Z, we update the corresponding column of W . In the
implementation, a small positive quantity, say the square root of the machine precision,
should be added to the denominators in the approximations of W and Z at each iteration
step. And a trivial ϵ = 10−9 will suﬃce. The full procedure is shown in Algorithm 6.
4.3. Regularization
Similar to the ALS with regularization discussed in Section 3.3 (p. 96), recall that the reg-
ularization can help extend the applicability of ALS to general matrices. Additionally, a
regularization term can be incorporated into the NMF framework to enhance its perfor-
mance:
L(W , Z) = ∥W Z −A∥2 + λw ∥W ∥2 + λz ∥Z∥2 ,
λw > 0, λz > 0,
120

JUN LU
CHAPTER 4. NONNEGATIVE MATRIX FACTORIZATION (NMF)
Algorithm 6 NMF via Multiplicative Updates
Require: Matrix A ∈RM×N;
1: Initialize W ∈RM×K, Z ∈RK×N randomly with nonnegative entries.
2: Choose a stop criterion on the approximation error δ;
3: Choose maximal number of iterations C;
4: iter = 0;
▷Count for the number of iterations
5: while ∥A −(W Z)∥2 > δ and iter < C do
6:
iter = iter + 1;
7:
for k = 1 to K do
8:
for n = 1 to N do
▷update k-th row of Z
9:
zkn ←zkn
(W ⊤A)kn
(W ⊤W Z)kn+ϵ;
10:
end for
11:
for m = 1 to M do
▷update k-th column of W
12:
wmk ←wmk
(AZ⊤)mk
(W ZZ⊤)mk+ϵ;
13:
end for
14:
end for
15: end while
16: Output W , Z;
Algorithm 7 NMF via Regularized Multiplicative Updates
Require: Matrix A ∈RM×N;
1: Initialize W ∈RM×K, Z ∈RK×N randomly with nonnegative entries.
2: Choose a stop criterion on the approximation error δ;
3: Choose maximal number of iterations C;
4: Choose regularization parameter λz, λw;
5: iter = 0;
▷Count for the number of iterations
6: while ∥A −(W Z)∥2 > δ and iter < C do
7:
iter = iter + 1;
8:
for k = 1 to K do
9:
for n = 1 to N do
▷udate k-th row of Z
10:
zkn ←zkn
(W ⊤A)kn−λzzkn
(W ⊤W Z)kn+ϵ ;
11:
end for
12:
for m = 1 to M do
▷udate k-th column of W
13:
wmk ←wmk
(AZ⊤)mk−λwwmk
(W ZZ⊤)mk+ϵ ;
14:
end for
15:
end for
16: end while
17: Output W , Z;
where the induced matrix norm is still the Frobenius norm. The gradient with respect to
Z given W is the same as that in Equation (3.11) (p. 97):
∂L(Z | W )
∂Z
= 2W ⊤(W Z −A) + 2λzZ ∈RK×N.
121

4.4. INITIALIZATION
The trivial gradient descent update can be obtained by
(GD on Z)
Z ←Z −η
∂L(Z | W )
∂Z

= Z −η

2W ⊤W Z −2W ⊤A + 2λzZ

,
Analogously, if we suppose a diﬀerent step size for each entry of Z and incorporate the
constant 2 into the step size, the update can be obtained by
(GD′ on Z)
zkn ←zkn −ηkn
2
∂L(Z | W )
∂Z

kn
= zkn −ηkn(W ⊤W Z −W ⊤A + λzZ)kn, k ∈{1, . . . , K}, n ∈{1, . . . , N}.
Now if we rescale the step size:
ηkn =
zkn
(W ⊤W Z)kn
,
then we obtain the update rule:
(MU on Z)
zkn ←zkn
(W ⊤A)kn −λzzkn
(W ⊤W Z)kn
,
k ∈{1, . . . , K}, n ∈{1, . . . , N}.
Similarly, the multiplicative update on W can be obtained by
(MU on W )
wmk ←wmk
(AZ⊤)mk −λwwmk
(W ZZ⊤)mk
,
m ∈{1, . . . , M}, k ∈{1, . . . , K}.
The procedure is then formulated in Algorithm 7.
A nonnegative matrix factorization A ≈W Z can also be used for clustering. The data
vector aj is assigned to cluster i if zij is the largest element in column j of Z (Brunet et al.,
2004; Gao and Church, 2005).
In the collaborative ﬁltering context, it is acknowledged that the NMF via multiplicative
update may lead to overﬁtting though the convergence results are good. The overﬁtting can
be partially mitigated through regularization, but its out-of-sample performance remains
low. Bayesian optimization through the use of generative models, on the other hand, can
eﬀectively prevent overﬁtting (see Brouwer et al. (2017); Lu and Ye (2022) or Chapter 6,
p. 142).
For other issues in the NMF, readers are advised to consult the survey of Berry et al.
(2007).
4.4. Initialization
A signiﬁcant challenge in NMF is that the convergence to a global minimum is not guaran-
teed. It often happens that convergence is slow and a suboptimal approximation is reached.
In the above discussion, we initialize W and Z randomly. To mitigate this issue, there are
also alternative strategies designed to obtain better initial estimates in the hope of converg-
ing more rapidly to a good solution (Boutsidis and Gallopoulos, 2008; Gillis, 2014). We
sketch the methods as follows for a reference:
122

JUN LU
CHAPTER 4. NONNEGATIVE MATRIX FACTORIZATION (NMF)
• Clustering techniques. Use some clustering methods on the columns of A, make the
cluster means of the top K clusters as the columns of W , and initialize Z as a proper
scaling of the cluster indicator matrix (that is, zkn ̸= 0 indicates an belongs to the
k-th cluster);
• Subset selection. Pick K columns of A and set those as the initial columns for W ,
and analogously, K rows of A are selected to form the rows of Z;
• SVD-based. Suppose the singular value decomposition (SVD) of A = Pr
i=1 σiuiv⊤
i
where each factor σiuiv⊤
i is a rank-one matrix with possible negative values in ui, vi,
and nonnegative σi. Denote [x]+ = max(x, 0), we notice
uiv⊤
i = [ui]+[vi]⊤
+ + [−ui]+[−vi]⊤
+ −[−ui]+[vi]⊤
+ −[ui]+[−vi]⊤
+.
Either [ui]+[vi]⊤
+ or [−ui]+[−vi]⊤
+ can be selected as a column and a row in W , Z. 1
However, these techniques are not guaranteed to perform better theoretically. We recom-
mend readers to the aforementioned papers for more information.
4.5. Movie Recommender Context
Both the NMF and the ALS methods approximate the matrix and reconstruct the entries
in the matrix with a set of basis vectors. The basis in the NMF is composed of vectors
with nonnegative elements while the basis in the ALS can have positive or negative values.
The diﬀerence then is that the NMF reconstructs each vector as a positive summation of
the basis vectors with a “relative” small component in the direction of each basis vector.
Whereas, in the ALS approximation, the data is modeled as a linear combination of the basis
such that we can add or subtract vectors as needed and the components in the direction
of each basis vector can be large positive values or negative values. Therefore, depending
on the application one or the other factorization can be utilized to describe the data with
diﬀerent meanings.
In the context of a movie recommender system, then the rows of W represent the hidden
features of the movies and columns of Z represent the hidden features of users. In the NMF
method we can say that a movie is 0.5 comedy, 0.002 action, and 0.09 romantic. However,
in the ALS approach, we can get combinations such as 4 comedy, −0.05 comedy, and −3
drama, i.e., a positive or negative component on that feature.
The ALS and NMF are similar in the sense that the importance of each basis vector is not
ranked in a hierarchical manner. Whereas, the key diﬀerence between the ALS (or NMF)
and the SVD is that, in the SVD, the importance of each vector in the basis is relative to the
value of the singular value associated with that vector. For the SVD of A = Pr
i=1 σiuiv⊤
i ,
this usually means that the reconstruction σ1u1v⊤
i via the ﬁrst set of basis vectors dominates
and is the most used set to reconstruct data, then the second set, and so on. So the basis
in the SVD has an implicit hierarchy and that doesn’t happen in the ALS or the NMF
approaches. Recall the low-rank approximation on the ﬂag image in Section 3.10.1 (p. 111)
where we ﬁnd the second component w2z⊤
2 via the ALS in Figure 3.9(n) (p. 111) plays an
important role in the reconstruction of the original ﬁgure, whereas the second component
σ2u2v⊤
2 via the SVD in Figure 3.9(b) (p. 111) plays a small role in the reconstruction.
1. Note here we consider general matrix A. If A is nonnegative, then ui and vi are nonnegative as well.
123

Part III
Bayesian Matrix Decomposition
124


5
Bayesian Real Matrix Factorization
Contents
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
127
5.2
All Gaussian (GGG) Model and Markov Blanket . . . . . . . .
129
5.3
All Gaussian Model with ARD Hierarchical Prior (GGGA)
.
136
5.4
All Gaussian Model with Wishart Hierarchical Prior (GGGW) 138
5.5
Gaussian Likelihood with Volume and Gaussian Priors (GVG) 139
126

JUN LU
CHAPTER 5. BAYESIAN REAL MATRIX FACTORIZATION
5.1. Introduction
The explosion of data from advancements in sensor technology and computer hardware
has presented new challenges for data analysis. The large volume of data often contains
noise and other distortions, requiring pre-processing for deductive science to be applied.
For example, signals received by antenna arrays often are contaminated by noise and other
degradations. To eﬀectively analyze the data, it is necessary to reconstruct or represent it
in a way that reduces inaccuracies while maintaining certain feasibility conditions.
Additionally, in many cases, the data collected from complex systems is the result of
multiple interrelated variables acting in unison. When these variables are not well deﬁned,
the information contained in the original data can be overlapping and unclear. By creating
a reduced system model, we can achieve a level of accuracy that is close to the original
system. The common approach in removing noise, reducing the model, and reconstructing
feasibility, is to replace the original data with a lower-dimensional representation obtained
through subspace approximation. Therefore, low-rank approximations or low-rank matrix
decompositions play a crucial role in a wide range of applications.
Low-rank matrix decomposition is a powerful technique used in machine learning and
data mining to represent a given matrix as the product of two or more matrices with lower
dimensions. It is used to capture the essential structure of a matrix while ignoring noise and
redundancies. The most common methods for low-rank matrix decomposition include sin-
gular value decomposition (SVD), principal component analysis (PCA), and multiplicative
update nonnegative matrix factorization (NMF).
Bayesian low-rank decomposition is a variant of low-rank matrix decomposition that
incorporates Bayesian modeling. It models the observed data as a low-rank matrix, where
the low-rank approximation is assumed to be generated from a prior distribution. This
allows for the inclusion of prior knowledge and uncertainty about the low-rank matrix into
the decomposition. The use of priors can also lead to more interpretable results by pro-
viding a probabilistic representation of the uncertainty associated with the factor matrices.
In addition, Bayesian methods allow for the quantiﬁcation of uncertainty in the results,
providing a measure of the conﬁdence in the estimated factor matrices. Thus it can help
to mitigate overﬁtting and produce more robust results making it a powerful method for
modeling both predictive and explanatory data.
Given an observed data set, represented as an M × N matrix of results, A, where rows
represent the number of observations and columns represent the variables of interest. Fol-
lowing Chapter 3 (p. 88), the real matrix factorization (RMF) problem, a common bilinear
decomposition problem, can be stated as A = W Z+E where A = [a1, a2, . . . , aN] ∈RM×N
is approximated factorized into an M × K matrix W ∈RM×K and a K × N matrix
Z ∈RK×N. The data set A needs not to be complete such that the indices of observed
entries can be represented by the mask matrix M ∈RM×N where a value of 1 indicates
the entry is observed and a value of 0 indicates the entry is missing. Matrices W and Z
represent the values of explanatory variables which, when multiplied, give a predictor of
the values in A. If entries in A are missing, then W and Z can be used to give predictions
of their values. When one of W and Z is observed, the factorization becomes a regression
problem.
127

5.1. INTRODUCTION
The factorization of the original data matrix A is achieved by ﬁnding two such real
matrices, one representing the basis or dictionary components and the other representing the
activations or coeﬃcients. Let zn denote the n-th column of Z. Then matrix multiplication
of W Z can be implemented as computing the column vectors of A as linear combinations
of the columns in W using coeﬃcients supplied by columns of Z:
an = W zn.
In the Netﬂix context, the value of amn, the (m, n)-th element of A, denotes the rating
of the n-th user for the m-th movie (the larger the user more likes the movie). Then wm
can represent the hidden features of the m-th movie and zn contains the features of user n
(Section 3.5, p. 99).
To simplify the problem, let us assume that there are no missing entries ﬁrstly. Project
data vectors an to a smaller dimension zn ∈RK with K < M, such that the reconstruction
error measured by Frobenius norm is minimized (assume K is known):
min
W ,Z
N
X
n=1
M
X
m=1

amn −w⊤
mzn
2
,
(5.1)
where W = [w⊤
1 ; w⊤
2 ; . . . ; w⊤
M] ∈RM×K and Z = [z1, z2, . . . , zN] ∈RK×N containing wm’s
and zn’s as rows and columns respectively 1. The loss form in Equation (5.1) is known
as the per-example loss. It can be equivalently written as
L(W , Z) =
N
X
n=1
M
X
m=1

amn −w⊤
mzn
2
= ∥W Z −A∥2 = tr
n
(W Z −A)⊤(W Z −A)
o
,
(5.2)
where tr(·) represents the trace of the quantity in the brackets. This matrix factorization
problem is similar to a standard “inverse” problem except that in the “inverse” problem
one of the factored components is known, and thus ordinary least squares or similar meth-
ods can be applied to ﬁnd the other component which minimizes the residuals between the
reconstruction and the data. When neither W nor Z is known, the factorization problem
is diﬃcult even if the latent dimension K is only 2 or 3. We sample the space of potential
solutions using the Markov chain Monte Carlo (MCMC) procedure to determine its prop-
erties because there are a vast number of possible solutions and no analytical approach to
identify them.
We have discussed the Bayesian approach in Section 1.1 (p. 15). In the Bayesian matrix
factorization context, the model leads to the speciﬁc form of Bayes’ equation,
p(W , Z | A) ∝p(A | W , Z) × p(W , Z),
(5.3)
where p(W , Z) captures the prior beliefs encoding the knowledge of the solution indepen-
dent of the data and p(A | W , Z) denotes the likelihood comparing the model to the data.
1. Note again in some contexts, Z represents an N × K matrix such that A is decomposed into A =
W Z⊤+ E.
128

JUN LU
CHAPTER 5. BAYESIAN REAL MATRIX FACTORIZATION
Name
Likelihood
Prior W
Prior Z
Hierarchical prior
GGG
N(amn|w⊤
mzn, σ2) N(wmk|0, (λW
mk)−1) N(zkn|0, (λZ
kn)−1)
/
GGGM N(amn|w⊤
mzn, σ2) N(wm|0, λ−1I)
N(zn|0, λ−1I)
/
GGGA
N(amn|w⊤
mzn, σ2) N(wmk|0, (λk)−1)
N(zkn|0, (λk)−1)
G(λk|αλ, βλ)
GGGW N(amn|w⊤
mzn, σ2) N(wm|µw, Σw)
N(zn|µz, Σz)
{µw, Σw}, {µz, Σz} ∼
NIW(m0, κ0, ν0, S0)
GVG
N(amn|w⊤
mzn, σ2)
W ∼
exp{−γW ⊤W }
N(zkn|0, (λZ
kn)−1)
/
Table 5.1: Overview of Bayesian real matrix factorization models.
Terminology.
There are three types of choices we make that determine the speciﬁc type
of matrix decomposition model we use, namely, the likelihood function, the priors we place
over the factored matrices W and Z, and whether we use any further hierarchical priors. We
will call the model by the density function in the order of the types of likelihood and priors.
For example, if the likelihood function for the model is chosen to be a Gaussian density,
and the two prior density functions are selected to be exponential density and Gaussian
density functions respectively, then the model will be denoted as the Gaussian Exponential-
Gaussian (GEG) model. Sometimes, we will put a hyperprior over the parameters of the
prior density functions, e.g., we put a Gamma prior over the exponential density, then it
will further be termed as a Gaussian Exponential-Gaussian Gamma (GEGA) model (“A” is
short for Gamma density to avoid confusion with Gaussian density). Table 5.1 summarizes
the Bayesian models for real matrix factorization in this chapter.
5.2. All Gaussian (GGG) Model and Markov Blanket
The all Gaussian (GGG) model is perhaps the most simple one for Bayesian RMF where
Gaussian priors are applied over factored matrices (Salakhutdinov and Mnih, 2008; G¨onen,
2012; Virtanen et al., 2011, 2012). The model involves using Gaussian likelihood and Gaus-
sian priors.
Likelihood.
We view the data A as being produced according to the probabilistic gen-
erative process shown in Figure 5.1. The observed (m, n)-th data entry amn of matrix A
is modeled using a Gaussian likelihood function with variance σ2 and mean given by the
latent decomposition w⊤
mzn (Equation (5.1)),
p(amn | w⊤
mzn, σ2) = N(amn | w⊤
mzn, σ2).
(5.4)
129

5.2. ALL GAUSSIAN (GGG) MODEL AND MARKOV BLANKET
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
ατ/ασ
βτ/βσ
τ/σ2
λW
mk
λZ
kn
Figure 5.1: Graphical model representation of GGG model. Green circles denote prior
variables, orange circles represent observed and latent variables, and plates represent re-
peated variables. The slash “/” in the variable represents “or”.
This is equivalent to assuming the residuals, emn, are i.i.d. drawn from a zero mean normal
with variance σ2, which gives rise to the following likelihood function,
p(A | θ) =
M,N
Y
m,n=1
N
 amn | (W Z)mn, σ2
=
M,N
Y
m,n=1
N
 amn | (W Z)mn, τ −1
(5.5)
where θ = {W , Z, σ2} denotes all parameters in the model, σ2 is the variance, τ −1 = σ2 is
the precision, and
N(x | µ, σ2) =
1
(2πσ2)1/2 exp

−1
2σ2 (x −µ)2

=
r τ
2π exp
n
−τ
2(x −µ)2o
is the normal density (Deﬁnition 2.2.1, p. 34).
Prior.
We assume W and Z are independently Gaussian distributed with precisions λW
mk
and λZ
kn respectively,
wmk ∼N(wmk | 0, (λW
mk)−1),
zkn ∼N(zkn | 0, (λZ
kn)−1);
p(W ) =
M,K
Y
m,k=1
N(wmk | 0, (λW
mk)−1),
p(Z) =
K,N
Y
k,n=1
N(zkn | 0, (λZ
kn)−1).
(5.6)
The prior for the noise variance σ2 is chosen as an inverse-Gamma density with shape ασ
and scale βσ (Deﬁnition 2.2.4, p. 40),
p(σ2) = G−1(σ2 | ασ, βσ) = βσασ
Γ(ασ)(σ2)−ασ−1 exp

−βσ
σ2

.
130

JUN LU
CHAPTER 5. BAYESIAN REAL MATRIX FACTORIZATION
By Bayes’ rule (Equation (1.1), p. 16), the posterior is proportional to the product of
likelihood and prior, it can be maximized to yield an estimate of W and Z.
Markov blanket.
The most widely used posterior inference methods in Bayesian infer-
ence models are Markov chain Monte Carlo (MCMC) methods as described in Section 1.3
(p. 17). The concept behind MCMC methods is to deﬁne a Markov chain on the hidden
variables that have the posterior as its equilibrium distribution (Andrieu et al., 2003). By
drawing samples from this Markov chain, one eventually obtains samples from the poste-
rior. A simple form of MCMC sampling is Gibbs sampling, where the Markov chain is
constructed by sampling the conditional distribution of each hidden variable given the val-
ues of other hidden variables and the observations. Gibbs sampling is widely used when
these conditional distributions can be sampled from easily.
Figure 5.2:
The Markov blanket of a directed acyclic graphical (DAG) model.
In a
Bayesian network, the Markov blanket of node A includes its parents, children, and the
other parents of all of its children. That is, the nodes in the cycle are in the Markov
blanket of node A. The ﬁgure is due to wikipedia page of Markov blanket.
To do Gibbs sampling, we need to derive the conditional posterior distributions for each
parameter conditioned on all the other parameters p(θi | θ−i, X), where X is again the set
of data points (here, the observed matrix A) and θi’s are the variables for which we want
to sample the distributions. But for a graphical model, this conditional distribution is a
function only of the nodes in the Markov blanket. For the GGG model shown in Figure 5.1,
which is a directed acyclic graphical (DAG) model, the Markov blanket of a node includes
the parents, the children, and the coparents (Jordan and Bishop, 2004), as shown in
Figure 5.2. The Markov blanket of node A is all nodes in the cycle.
An example on the Markov blanket.
The idea of the Markov blanket might be mys-
terious at ﬁrst glance. Suppose we want to sample the (m, k)-th element wmk of W for the
distribution of it. From Figure 5.1, we ﬁnd its parents, children, and coparents are {λW
mk},
{amn}, and {σ2, Z, W−mk}, respectively. Therefore, the conditional distribution of wmk
131

5.2. ALL GAUSSIAN (GGG) MODEL AND MARKOV BLANKET
only depends on the three pairs of parameters:
p(wmk | −) = p(wmk | A, W−mk, Z, σ2, λW
mk).
More speciﬁcally, from this graphical representation, we can ﬁnd the Markov blanket for each
parameter in the GGG model, and then ﬁgure out their conditional posterior distributions
to be derived:
p(wmk | −) = p(wmk | A, W−mk, Z, σ2, λW
mk),
(5.7)
p(zkn | −) = p(zkn | A, W , Z−kn, σ2, λZ
kn),
(5.8)
p(σ2 | −) = p(σ2 | A, W , Z, ασ, βσ, ).
(5.9)
We sequentially draw samples from the posterior of each parameter, conditioned on all other
parameters. It can be shown that the sequence of samples computed constitutes a Markov
chain for which the stationary distribution is the posterior in which we are interested. In
other words, Gibbs sampler moves the chain forward by one step as follows:
• Sample the ﬁrst component wmk for each observation from Equation (5.7) which is
known as the conditional distribution of wmk;
• Sample the second component zkn for each observation from Equation (5.8), which is
known as the conditional distribution of zkn;
• Sample the parameter σ2 from Equation (5.9) which is known as the conditional
distribution of data variance.
Posterior.
Given the observed matrix A, we want to estimate the conditional distribution
of the latent structure p(W , Z | A), termed the posterior, which is the key to matrix
decomposition-based applications. For example, in the Netﬂix context, we estimate the
posterior expectation of each user’s hidden feature/preferences and each movie’s hidden
attributes to perform predictions of which unconsumed movies each user will like. In this
book, we apply Gibbs sampling to perform optimization since it tends to be very accurate at
ﬁnding the true posterior. The advantage of the MCMC or Gibbs sampling methods is that
they produce exact results asymptotically. Other than this method, variational Bayesian
inference can be an alternative way but we shall not go into the details. For RMF, following
the Bayes’ rule and MCMC, this means we need to be able to draw from distributions (by
Markov blanket):
p(wmk | A, W−mk, Z, σ2, λW
mk),
p(zkn | A, W , Z−kn, σ2, λZ
kn),
p(σ2 | A, W , Z, ασ, βσ),
where W−mk denotes all elements of W except wmk and Z−kn denotes all elements of Z
except zkn. Using Bayes’ theorem, the conditional density of wmk depends on its parents
132

JUN LU
CHAPTER 5. BAYESIAN REAL MATRIX FACTORIZATION
(λW
mk), children (amn), and coparents (τ or σ2, W−mk, Z) 2. And it can be obtained by
p(wmk | A, W−mk, Z, σ2, λW
mk) ∝p(A | W , Z, σ2) × p(wmk | λW
mk)
=
M,N
Y
i,j=1
N

aij | w⊤
i zj, σ2
× N(wmk | 0, (λW
mk)−1)
∝exp


−1
2σ2
M,N
X
i,j=1
(aij −w⊤
i zj)2


× exp

−w2
mk
2 λW
mk

∝exp


−1
2σ2
N
X
j=1
(amj −w⊤
mzj)2


× exp

−w2
mk
2 λW
mk

∝exp


−1
2σ2
N
X
j=1

w2
mkz2
kj + 2wmkzkj
 K
X
i̸=k
wmizij −amj




· exp

−w2
mk
2 λW
mk

⋆∝exp

















−
 PN
j=1 z2
kj
2σ2
+ λW
mk
2
!
|
{z
}
1/(2 g
σ2
mk)
w2
mk + wmk

1
σ2
N
X
j=1
zkj

amj −
K
X
i̸=k
wmizij


|
{z
}
g
σ2
mk
−1
g
µmk

















∝N(wmk | g
µmk, g
σ2
mk),
(5.10)
where the equality (⋆) conforms to Equation (2.2) (p. 34) such that it follows from the
normal distribution with variance g
σ2
mk,
g
σ2
mk = 1
 
1
σ2
N
X
j=1
z2
kj + λW
mk


(5.11)
and mean g
µmk,
g
µmk =
g
σ2
mk
σ2 ·
N
X
j=1
zkj

amj −
K
X
i̸=k
wmizij

.
(5.12)
In this case, the posterior precision 1/g
σ2
mk is the sum of prior precision λW
mk and “data preci-
sion”
1
σ2
PN
j=1 z2
kj. This also shows, in the Netﬂix context, that the conditional distribution
over the movie feature vector wm, conditioned on the user features, observed rating matrix
A, and the values of the hyperparameters is a Gaussian density. And due to symmetry, a
completely analogous derivation for zkn is used for the Z factor.
The conditional density of σ2 depends on its parents (ασ, βσ), children (A), and copar-
ents (W , Z). And it is an inverse-Gamma distribution (by conjugacy in Equation (2.7),
2. See Figure 5.1 and Section 5.2.
133

5.2. ALL GAUSSIAN (GGG) MODEL AND MARKOV BLANKET
p. 40),
p(σ2 | A, W , Z, ασ, βσ) = G−1(σ2 | f
ασ, f
βσ),
f
ασ = MN
2
+ ασ,
f
βσ = 1
2
M,N
X
m,n=1
(A −W Z)2
mn + βσ.
(5.13)
Missing entries.
In many cases, e.g., the Netﬂix context, some entries of A are missing.
Let Ωdenote the set containing all observed entries in data A. Denote further Ωm = {n |
(m, n) ∈Ω}, i.e., the observed entries in the m-th row; Ωn = {m | (m, n) ∈Ω}, i.e., the
observed entries in the n-th column. The posterior density of wmk can be obtained by
wmk ∼N(wmk | g
µmk, g
σ2
mk),
(5.14)
where
g
σ2
mk = 1
 
1
σ2
X
j∈Ωm
z2
kj + λW
mk

,
g
µmk =
g
σ2
mk
σ2 ·
X
j∈Ωm
zkj

amj −
K
X
i̸=k
wmizij

.
(5.15)
In the following discussion, for simplicity, we only consider the data matrix A with full
observations. The results for missing entries can be derived similarly.
GGG with shared prior (GGGM).
When we set λ = λW
mk for all m ∈{1, 2, . . . , M}, k ∈
{1, 2, . . . , K}, the conditional posterior distributions we obtain in the Gibbs sampling algo-
rithm can be written as a multivariate Gaussian density (Deﬁnition 2.7.1, p. 67). In this
case, we place a multivariate Gaussian prior over each row wm of W and each column zn
of Z:
wm ∼N(wm | 0, λ−1I),
zn ∼N(zn | 0, λ−1I),
i.e., each of the M or N item factors follows a multivariate normal distribution. Again by
Bayes’ rule, let W−m denote all elements of W except the m-th row, conditional posterior
distribution we obtain in the Gibbs sampling algorithm can be obtained by
p(wm | σ2, W−m, Z, λ, A) ∝p(A | W , Z, σ2) × N(wm | 0, λ−1I)
∝N(A | W Z, σ2I) × N(wm | 0, λ−1I)
∝exp


−1
2σ2
N
X
j=1
(amj −w⊤
mzj)2


× exp

−λ
2 w⊤
mwm

⋆∝exp















−1
2w⊤
m

λI + 1
σ2
N
X
j=1
zjz⊤
j


|
{z
}
eΣ
−1
wm + w⊤
m
1
σ2
N
X
j=1
amjzj
|
{z
}
eΣ
−1eµ















∝N(wm | eµ, eΣ),
(5.16)
134

JUN LU
CHAPTER 5. BAYESIAN REAL MATRIX FACTORIZATION
where the equality (⋆) conforms to Equation (2.33) (p. 70) such that it follows from the
multivariate Gaussian distribution with covariance eΣ,
eΣ =

λI + 1
σ2
N
X
j=1
zjz⊤
j


−1
and mean vector eµ,
eµ = 1
σ2 eΣ ·
N
X
j=1
amjzj.
In this case, the posterior precision matrix eΣ
−1 is the sum of the prior precision
matrix λI and “data precision matrix”
1
σ2
PN
j=1 zjz⊤
j . And due to symmetry, a similar
expression for zn can be derived.
Algorithm 8 Gibbs sampler for GGG model in one iteration (prior on variance σ2 here,
similarly for the precision τ). The procedure presented here may not be eﬃcient but is
explanatory. A more eﬃcient one can be implemented in a vectorized manner. By default,
uninformative hyperparameters are ασ = βσ = 1, {λW
mk} = {λZ
kn} = 0.1.
Require: Choose initial ασ, βσ, λW
mk, λZ
kn;
1: for k = 1 to K do
2:
for m = 1 to M do
3:
Sample wmk from p(wmk | A, W−mk, Z, σ2, λW
mk);
▷Equation (5.10)
4:
end for
5:
for n = 1 to N do
6:
Sample zkn from p(zkn | A, W , Z−kn, σ2, λZ
kn);
▷Symmetry of Eq. (5.10)
7:
end for
8: end for
9: Sample σ2 from p(σ2 | A, W , Z, ασ, βσ);
▷Equation (5.13)
10: Report loss in Equation (5.2), stop if it converges.
Gibbs sampling.
Because conjugate priors for the parameters and hyperparameters are
used in the Bayesian matrix factorization model, it is easy to sample from the conditional
distributions derived from the posterior distribution. By this Gibbs sampling method in-
troduced in Section 1.3.3 (p. 20), we can construct a Gibbs sampler for the GGG model as
formulated in Algorithm 8. Due to our choice of priors, we can sample from all conditional
distributions directly using standard methods, which obviates slower sampling procedures
such as rejection sampling. And also in practice, all the parameters of Gaussian priors are
set to be the same value λ = {λW
mk}′s = {λZ
kn}′s for all m, k, n. By default, uninformative
hyperparameters are ασ = βσ = 1, {λW
mk} = {λZ
kn} = 0.1.
Prior by Gamma distribution.
We also notice that putting an inverse-Gamma prior
on the variance is equivalent to putting a Gamma prior on the precision parameter (the
inverse of variance). For the precision τ = σ−2, we use a Gamma distribution with shape
135

5.3. ALL GAUSSIAN MODEL WITH ARD HIERARCHICAL PRIOR (GGGA)
ατ > 0 and rate βτ > 0 (Deﬁnition 2.2.3, p. 36),
p(τ) ∼G(τ | ατ, βτ) =
βατ
τ
Γ(ατ)τ ατ−1 exp(−βτ · τ),
(5.17)
The posterior is obtained similarly (Equation (2.5), p. 38),
p(τ | W , Z, A) = G(τ; f
ατ, f
βτ),
f
ατ = MN
2
+ ατ,
f
βτ = 1
2
M,N
X
m,n=1
(A −W Z)2
mn + βτ.
In practice, the prior parameters ατ, βτ are chosen to be equal to ασ, βσ respectively.
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
ατ/ασ
βτ/βσ
τ/σ2
λk
αλ, βλ
(a) GGGA.
m = 1..M
n = 1..N
wm
amn
zn
ατ/ασ
βτ/βσ
τ/σ2
µw, Σw
m0, κ0,
ν0, S0
µz, Σz
(b) GGGW.
Figure 5.3: Graphical model representation of GGGA and GGGW models. Green circles
denote prior variables, orange circles represent observed and latent variables, and plates
represent repeated variables. The slash “/” in the variable represents “or”, and the comma
“,” in the variable represents “and”.
5.3. All Gaussian Model with ARD Hierarchical Prior (GGGA)
The all Gaussian with hierarchical Gamma prior (GGGA) model is proposed by Virtanen
et al. (2011, 2012) based on the GGG model where the diﬀerence lies in that the GGGA
model puts a hyperprior over the Gaussian prior. Moreover, the GGGA model favors an
automatic relevance determination (ARD) that helps perform automatic model selection
(Figure 5.3(a)).
Hyperprior.
Going further from the GGG model, we consider the ARD prior in which
case we place a further Gamma prior (i.e., a hierarchical prior)
wmk ∼N(0, (λk)−1),
zkn ∼N(0, (λk)−1),
λk ∼G(αλ, βλ),
(5.18)
136

JUN LU
CHAPTER 5. BAYESIAN REAL MATRIX FACTORIZATION
where k ∈{1, 2, . . . , K}, and λk is shared by all entries in the same column of W and the
same row of Z. The entire factor k is then either activated if λk has a low value or “turned
oﬀ” if λk has a high value (see Figure 2.1, p. 34).
Posterior.
For RMF, following the Bayes’ rule and MCMC, this means we need to be
able to draw from distributions (again by Markov blanket, Section 5.2):
p(σ2 | A, W , Z, ασ, βσ),
p(wmk | A, W−mk, Z, σ2, λ),
p(λk | W , Z, λ−k, σ2, αλ, βλ),
p(zkn | A, W , Z−kn, σ2, λ),
where λ ∈RK
+ is a vector including all λk values and λ−k denotes all elements of λ except
λk. The conditional posteriors for σ2, wmk’s, and zkn’s remain unchanged, except now we
replace λW
mk and λZ
kn by λk. The conditional posterior density of λk depends on its parents
(αλ, βλ), children (k-th row wk of W , k-th column zk of Z, see deﬁnition in Equation (5.1)),
and coparents (λ−k) 3. Then it follows that,
p(λk | W , Z, λ−k, σ2, αλ, βλ)
∝p(wk, zk | λk) · p(λk) =
M
Y
i=1
N(wik | 0, (λk)−1) ·
N
Y
j=1
N(zkj | 0, (λk)−1) · G(λk | αλ, βλ)
=
M
Y
i=1
λ1/2
k
exp

−λkw2
ik
2

·
N
Y
j=1
λ1/2
k
exp
(
−
λkz2
kj
2
)
· βαλ
λ
Γ(αλ)λαλ−1
k
exp(−λkβλ)
∝λ
M+N
2
+αλ−1
k
exp


−λk ·

1
2
M
X
i=1
w2
ik + 1
2
N
X
j=1
z2
kj + βλ





∝G(λk | f
αλ, f
βλ),
(5.19)
where
f
αλ = M + N
2
+ αλ,
f
βλ = 1
2
M
X
i=1
w2
ik + 1
2
N
X
j=1
z2
kj + βλ.
By the deﬁnition of the Gamma distribution (Deﬁnition 2.2.3, p. 36), we have the moments
of the posterior density for λk:
E[λk] = f
αλ
f
βλ
,
Var[λk] = f
αλ
f
βλ
2 .
Therefore, when the shape of the raw matrix A is larger (i.e., M + N is larger), we favor a
larger value of λk (during sampling). From Equation (5.18), we thus impose a larger and
sparser regularization over the model. Moreover, if the elements of the factored components
W and Z are larger, the f
βλ tends to be larger as well; hence a smaller value of λk will
be drawn. This is reasonable in the sense that we want to explore in a larger space if the
factored components have larger elements from previous Gibbs iterations.
3. See Figure 5.3(a) and Section 5.2.
137

5.4. ALL GAUSSIAN MODEL WITH WISHART HIERARCHICAL PRIOR (GGGW)
Gibbs sampling.
Again we can construct a Gibbs sampler for the GGGA model as
formulated in Algorithm 9. By default, uninformative hyperparameters are ασ = βσ = 1,
αλ = βλ = 1.
Algorithm 9 Gibbs sampler for GGGA model in one iteration (prior on variance σ2 here,
similarly for the precision τ). The procedure presented here may not be eﬃcient but is
explanatory. A more eﬃcient one can be implemented in a vectorized manner. By default,
uninformative hyperparameters are ασ = βσ = 1, αλ = βλ = 1.
Require: Choose initial ασ, βσ, αλ, βλ;
1: for k = 1 to K do
2:
for m = 1 to M do
3:
Sample wmk from p(wmk | A, W−mk, Z, σ2, λk);
▷Equation (5.10)
4:
end for
5:
for n = 1 to N do
6:
Sample zkn from p(zkn | A, W , Z−kn, σ2, λk);
▷Symmetry of Eq. (5.10)
7:
end for
8:
Sample λk from p(λk | W , Z, σ2, αλ, βλ);
▷Equation (5.19)
9: end for
10: Sample σ2 from p(σ2 | A, W , Z, ασ, βσ);
▷Equation (5.13)
11: Report loss in Equation (5.2), stop if it converges.
5.4. All Gaussian Model with Wishart Hierarchical Prior (GGGW)
The hierarchical prior with Wishart density is proposed in Salakhutdinov and Mnih (2008)
to increase ﬂexibility and calibration based on the GGG model. Instead of assuming inde-
pendence of each entry in the factored components W , Z, we now assume each row wm of
W and each column zn of Z comes from a multivariate Gaussian density (Deﬁnition 2.7.1,
p. 67) whose parameters are placed over a further normal-inverse-Wishart (NIW) prior
(Deﬁnition 2.7.5, p. 74). Figure 5.3(b) shows the graphical representation of the GGGW
model.
Prior and hyperprior.
Same as the GGG model, we consider the Gaussian likelihood
over the data matrix A, and the variance parameter σ2 is placed over an inverse-Gamma
prior with shape ασ and scale βσ. Given the m-th row wm of W and the n-th column zn
of Z, we consider the multivariate Gaussian density and the normal-inverse-Wishart prior
as follows:
wm ∼N(wm | µw, Σw),
µw, Σw ∼NIW(µw, Σw | m0, κ0, ν0, S0);
(5.20)
zn ∼N(zn | µz, Σz),
µz, Σz ∼NIW(µz, Σz | m0, κ0, ν0, S0),
(5.21)
where NIW(µ, Σ | m0, κ0, ν0, S0) = N(µ | m0, 1
κ0 Σ) · IW(Σ | S0, ν0) is the density of a
normal-inverse-Wishart distribution and IW(Σ | S0, ν0) is the inverse-Wishart distrbution
(Deﬁnition 2.7.4, p. 73). While we can also place normal and inverse-Wishart priors over
the mean and covariance parameters separately, i.e., a semi-conjugate prior. We do not
repeat the details here, see Section 2.7.4 and 2.7.5 (p. 75, p. 75).
138

JUN LU
CHAPTER 5. BAYESIAN REAL MATRIX FACTORIZATION
Following from the discussion in Section 2.7.7 (p. 78), the posterior density of {µw, Σw}
also follows a NIW distribution with updated parameters:
µw, Σw ∼NIW(µw, Σw | mM, κM, νM, SM)
(5.22)
where
mM = κ0m0 + Mw
κM
= κ0
κM
m0 + M
κM
w,
(5.23)
κM = κ0 + M,
(5.24)
νM = ν0 + M,
(5.25)
SM = S0 + Sw +
κ0M
κ0 + M (w −m0)(w −m0)⊤
(5.26)
= S0 +
M
X
m=1
wmw⊤
m + κ0m0m⊤
0 −κMmMm⊤
M,
(5.27)
w = 1
M
M
X
m=1
wm,
(5.28)
Sw =
M
X
m=1
(wm −w)(wm −w)⊤.
(5.29)
An intuitive interpretation for the parameters in NIW can be obtained from the updated
parameters above. The parameter ν0 is the prior number of samples to observe the covari-
ance matrix, and νM = ν0 + M is the posterior number of samples. The posterior mean
mM of the model mean µw is a weighted average of the prior mean m0 and the sample
mean w. The posterior scale matrix SM is the sum of the prior scale matrix S0, empirical
covariance matrix Sw, and an extra term due to the uncertainty in the mean. And due to
symmetry, a similar form for {µz, Σz} can be derived.
Gibbs sampling.
Again we can construct a Gibbs sampler for the GGGW model as
formulated in Algorithm 10. By default, uninformative hyperparameters are ασ = βσ = 1,
m0 = 0, κ0 = 1, ν0 = K + 1, S0 = I.
5.5. Gaussian Likelihood with Volume and Gaussian Priors (GVG)
The Gaussian likelihood with volume and Gaussian prior (GVG) is introduced by Arn-
gren et al. (2011). While the original paper applies the volume prior to unmix a set of
pixels into pure spectral signatures (endmembers) and corresponding fractional abundances
in hyperspectral image analysis in which case the factored components are nonnegative.
However, it can also be applied in the real-valued applications here. The prior over Z is
still a Gaussian density as that in the GGG model. Instead of assuming Gaussian prior
over W , Arngren et al. (2011) put a volume prior for the factored component W with
density W ∝exp{−γ det(W ⊤W )} (Figure 5.4). The prior has a single parameter γ that
is determined by hand.
139

5.5. GAUSSIAN LIKELIHOOD WITH VOLUME AND GAUSSIAN PRIORS (GVG)
Algorithm 10 Gibbs sampler for GGGW model in one iteration (prior on variance σ2 here,
similarly for the precision τ). By default, uninformative hyperparameters are ασ = βσ = 1,
m0 = 0, κ0 = 1, ν0 = K + 1, S0 = I.
Require: Choose initial ασ, βσ, m0, κ0, ν0, S0;
1: for m = 1 to M do
2:
Sample wm from p(wm | µw, Σw);
▷Equation (5.20)
3: end for
4: for n = 1 to N do
5:
Sample zn from p(zn | µz, Σz);
▷Symmetry of Eq. (5.21)
6: end for
7: Sample µw, Σw from p(µw, Σw | mM, κM, νM, SM)
▷Equation (5.22)
8: Sample µz, Σz from p(µz, Σz | mN, κN, νN, SN)
▷Symmetry of Eq. (5.22)
9: Sample σ2 from p(σ2 | A, W , Z, ασ, βσ);
▷Equation (5.13)
10: Report loss in Equation (5.2), stop if it converges.
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
ατ/ασ
βτ/βσ
τ/σ2
γ
λZ
kn
Figure 5.4: Graphical representation of GVG model.
Green circles denote prior vari-
ables, orange circles represent observed and latent variables, and plates represent repeated
variables. The slash “/” in the variable represents “or”.
Posterior.
Denote vector wm,−k ∈RK−1 as the m-th row of W excluding column k;
vector w−m,k ∈RM−1 as the k-th column of W excluding row m; matrix W−m,−k ∈
R(M−1)×(K−1) as W excluding row m and column k; matrix W:,−k ∈RM×(K−1) as W
excluding column k; scalar value D−k,−k = det
 W ⊤
:,−kW:,−k

; and the matrix adjugate of
 W ⊤
:,−kW:,−k

as A−k,−k = det
 W ⊤
:,−kW:,−k
 W ⊤
:,−kW:,−k
−1 ∈R(K−1)×(K−1). Then the
posterior density of wmk can be obtained by
wmk ∼N(wmk | g
µmk, g
σ2
mk),
(5.30)
140

JUN LU
CHAPTER 5. BAYESIAN REAL MATRIX FACTORIZATION
where
g
µmk = g
σ2
mk


γw⊤
m,−kA−k,−k(W ⊤
−m,−k)w−m,k + 1
σ2
N
X
j=1
(amj −
K
X
i̸=k
w⊤
m,−kzj,−k)zkj


,
and
g
σ2
mk = 1
 
1
σ2
N
X
j=1
z2
kj + γ

D−k,−k −w⊤
m,−kA−k,−kwm,−k


.
The above result can be obtained by extracting wmk from exp{−γ det(W ⊤W )} and from
the fact that det(M) = det(D) det(A−BD−1C) = det(A) det(D−CA−1B) if the matrix
M has the block formulation M =
A
B
C
D

.
141

6
Bayesian Nonnegative Matrix Factorization
Contents
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
6.2
Gaussian Likelihood with Exponential Priors (GEE) . . . . . .
143
6.3
Gaussian Likelihood with Exponential Priors and ARD Hier-
archical Prior (GEEA) . . . . . . . . . . . . . . . . . . . . . . . .
147
6.4
Gaussian Likelihood with Truncated-Normal Priors (GTT)
.
150
6.5
Gaussian Likelihood with Truncated-Normal and Hierarchical
Priors (GTTN)
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
152
6.6
Gaussian Likelihood with Rectiﬁed-Normal Priors (GRR) and
Hierarchical Prior (GRRN) . . . . . . . . . . . . . . . . . . . . .
154
6.6.1
Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159
6.7
Priors as Regularization . . . . . . . . . . . . . . . . . . . . . . .
162
6.8
Gaussian L2
1 Norm (GL2
1) Model . . . . . . . . . . . . . . . . . .
163
6.9
Gaussian L2
2 Norm (GL2
2) and Gaussian L∞Norm (GL∞) Models166
6.9.1
Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170
6.10
Semi-Nonnegative Matrix Factorization
. . . . . . . . . . . . .
175
6.10.1
Gaussian Likelihood with Exponential and Gaussian Priors (GEG) 176
6.10.2
Gaussian Likelihood with Nonnegative Volume and Gaussian Pri-
ors (GnVG) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
6.11
Nonnegative Matrix Tri-Factorization (NMTF) . . . . . . . . .
177
142

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
6.1. Introduction
The nonnegative matrix factorization (NMF) method analyzes data matrices with nonneg-
ative elements, which are common in data sets derived from texts and images (Berry et al.,
2007). In cases where the entries in A, W , and Z are nonnegative, NMF algorithms have
frequently improved performance. Thus, the scope of NMF research has grown rapidly in
recent years, particularly in the ﬁelds of machine learning (Lee and Seung, 1999, 2000).
Early work on nonnegative matrix factorizations was performed by a Finnish group
of researchers in the 1990s under the name positive matrix factorization (Paatero et al.,
1991; Paatero and Tapper, 1994; Anttila et al., 1995). This work is rarely cited by later
researchers partly due to the unfortunate phrasing of positive matrix factorization which is
misleading as Paatero and Tapper (1994) actually create a nonnegative matrix factorization.
Since its introduction by Lee and Seung (1999, 2000), the NMF problem has received a
signiﬁcant amount of attention, both in published and unpublished work, in various ﬁelds
such as science, engineering, and medicine. Diﬀerent authors have also proposed alternative
formulations for the NMF problem (Schmidt et al., 2009; Tan and F´evotte, 2013; Brouwer
and Lio, 2017; Lu and Ye, 2022).
The NMF problem can be stated as A = W Z + E, where A ∈RM×N is approximated
factorized into an M × K matrix W ∈RM×K
+
and a K × N matrix Z ∈RK×N
+
. The data
set A needs not be complete such that the indices of observed entries can be represented by
the mask matrix M ∈RM×N where an entry of one indicates the element is observed and
unobserved otherwise. This nonnegativity makes the resulting matrices easier to inspect
and more intuitive to interpret such as in an image analysis context.
To simplify the problem, let us assume that there are no missing entries ﬁrstly. Treating
missing entries in the Bayesian NMF context is just the same as that in the Bayesian RMF
cases (Section 5.2, p. 129). Project data vectors an to a smaller dimension zn ∈RK with
K < M, such that the reconstruction error measured by Frobenius norm is minimized
(assume K is known):
min
W ,Z L(W , Z) = min
W ,Z
N
X
n=1
M
X
m=1

amn −w⊤
mzn
2
,
(6.1)
where W = [w⊤
1 ; w⊤
2 ; . . . ; w⊤
M] ∈RM×K and Z = [z1, z2, . . . , zN] ∈RK×N containing wm’s
and zn’s as rows and columns respectively.
Terminology.
Again, we follow the same terminology in Bayesian RMF, we will call
the Bayesian NMF models by the density function in the order of likelihood, priors, and
hyperpriors (Section 5.1, p. 127). Table 6.1 summarizes the Bayesian models for nonnegative
matrix factorization in this chapter.
6.2. Gaussian Likelihood with Exponential Priors (GEE)
The Gaussian likelihood with exponential priors (GEE) model is perhaps the most simple
one for Bayesian NMF where Gaussian likelihood is applied and exponential priors are used
over factor matrices (Schmidt et al., 2009).
143

6.2. GAUSSIAN LIKELIHOOD WITH EXPONENTIAL PRIORS (GEE)
Name Likelihood
Prior W
Prior Z
Hierarchical prior
GEE
N(amn|w⊤
mzn, σ2) E(wmk|λW
mk)
E(zkn|λZ
kn)
/
GEEA N(amn|w⊤
mzn, σ2) E(wmk|λk)
E(zkn|λk)
G(λk|αλ, βλ)
GTT
N(amn|w⊤
mzn, σ2) T N(wmk|µW
mk,
1
τ W
mk )
T N(zkn|µZ
kn,
1
τ Z
kn )
/
GTTN N(amn|w⊤
mzn, σ2) T N(wmk|µW
mk,
1
τ W
mk )
T N(zkn|µZ
kn,
1
τ Z
kn )
{µW
mk, τ W
mk}, {µZ
kn, τ Z
kn} ∼
T NSNG(µµ, τµ, a, b)
GRR
N(amn|w⊤
mzn, σ2) RN(·|µW
mk,
1
τ W
mk , λW
mk)
RN(·|µZ
kn,
1
τ Z
kn , λZ
kn)
/
GRRN N(amn|w⊤
mzn, σ2) RN(·|µW
mk,
1
τ W
mk , λW
mk)
RN(·|µZ
kn,
1
τ Z
kn , λZ
kn)
{µW
mk, τ W
mk, λW
mk},
{µZ
kn, τ Z
kn, λZ
kn} ∼
RNSNG(µµ, τµ, a, b, αλ, βλ)
GL2
1
N(amn|w⊤
mzn, σ2)
W ∼exp
{ −λW
k
2
P
m(P
k wmk)2}
Z ∼exp
{ −λZ
k
2
P
n(P
k zkn)2}
/
GL2
2
N(amn|w⊤
mzn, σ2)
W ∼exp
{ −λW
k
2
P
m(P
k w2
mk)}
Z ∼exp
{ −λZ
k
2
P
n(P
k z2
kn)}
/
GL∞
N(amn|w⊤
mzn, σ2)
W ∼exp{−
λW
k
P
m(maxk |wmk|)}
Z ∼exp{−
λZ
k
P
n(maxk |zkn|)}
/
GEG
N(amn|w⊤
mzn, σ2) E(wmk|λW
mk)
N(zkn|0, (λZ
kn)−1)
/
GnVG N(amn|w⊤
mzn, σ2)
W ∼
exp{−γW ⊤W }u(W ) N(zkn|0, (λZ
kn)−1)
/
Table 6.1: Overview of Bayesian nonnegative and semi-nonnegative matrix factorization
models.
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
ατ/ασ
βτ/βσ
τ/σ2
λW
mk
λZ
kn
(a) GEE.
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
ατ/ασ
βτ/βσ
τ/σ2
λk
αλ, βλ
(b) GEEA.
Figure 6.1: Graphical model representation of GEE and GEEA models. Green circles
denote prior variables, orange circles represent observed and latent variables, and plates
represent repeated variables. The slash “/” in the variable represents “or”.
144

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
Likelihood.
Again, we view the data A as being produced according to the probabilistic
generative process shown in Figure 6.1(a). We assume the residuals, emn, are i.i.d. drawn
from a zero mean Gaussian distribution with variance σ2. This is equivalent to assuming
the observed (m, n)-th data entry amn of matrix A is modeled using a Gaussian likelihood
with variance σ2 and mean given by the latent decomposition w⊤
mzn (Equation (6.1)), which
gives rise to the following likelihood function,
p(A | θ) =
M,N
Y
m,n=1
N
 amn | (W Z)mn, σ2
=
M,N
Y
m,n=1
N
 amn | (W Z)mn, τ −1
(6.2)
where θ = {W , Z, σ2} denotes all parameters in the model, σ2 is the variance, τ −1 = σ2 is
the precision, and N(x | µ, σ2) is the normal density function.
Prior.
We treat the latent variables wmk’s (and zkn’s) as random variables. And we need
prior densities over these latent variables to express beliefs about their values, e.g., nonneg-
ativity in this context though there are many other possible constraints (semi-nonnegativity
in Ding et al. (2008), discreteness in Gopalan et al. (2014, 2015)). Here we assume wmk’s
and zkn’s are independently drawn from exponential priors with rate parameters λW
mk and
λZ
kn respectively (Deﬁnition 2.3.1, p. 49),
wmk ∼E(wmk | λW
mk),
zkn ∼E(zkn | λZ
kn);
p(W ) =
M,K
Y
m,k=1
E(wmk | λW
mk),
p(Z) =
K,N
Y
k,n=1
E(zkn | λZ
kn),
(6.3)
where E(x | λ) = λ exp(−λx)u(x) is the exponential density with u(x) being the unit step
function. This prior serves to enforce the nonnegativity constraint on the components W , Z.
Same as the GGG model, the prior for the noise variance σ2 is chosen as an inverse-Gamma
density with shape ασ and scale βσ (Deﬁnition 2.2.4, p. 40),
p(σ2) = G−1(σ2 | ασ, βσ) = βσασ
Γ(ασ)(σ2)−ασ−1 exp

−βσ
σ2

.
(6.4)
Again by Bayes’ rule (Equation (1.1), p. 16), the posterior is proportional to the product
of likelihood and prior, it can be maximized to yield an estimate of W and Z.
Posterior.
For NMF, following the Bayes’ rule and MCMC, this means we need to be
able to draw from distributions (by Markov blanket, Section 5.2, p. 129):
p(wmk | A, W−mk, Z, σ2, λW ),
p(zkn | A, W , Z−kn, σ2, λZ),
p(σ2 | A, W , Z, λW , λZ),
where λW is an M ×K matrix containing all {λW
mk} entries, λZ is a K ×N matrix including
all {λZ
kn} values, and W−mk denotes all elements of W except wmk. Using Bayes’ theorem,
145

6.2. GAUSSIAN LIKELIHOOD WITH EXPONENTIAL PRIORS (GEE)
the conditional density of wmk depends on its parents (λW
mk), children (amn), and coparents
(τ or σ2, W−mk, Z) 1. And it can be obtained by
p(wmk | A, W−mk, Z, σ2, λW
mk)
∝p(A | W , Z, σ2) × p(wmk | λW
mk) =
M,N
Y
i,j=1
N

aij | w⊤
i zj, σ2
× E(wmk | λW
mk)
∝exp


−1
2σ2
M,N
X
i,j=1
(aij −w⊤
i zj)2


×

λW
mk exp(−λW
mk · wmk)u(wmk)
∝exp


−1
2σ2
N
X
j=1
(amj −w⊤
mzj)2


· exp(−λW
mk · wmk)u(wmk)
∝exp


−1
2σ2
N
X
j=1

w2
mkz2
kj + 2wmkzkj
 K
X
i̸=k
wmizij −amj




· exp(−λW
mk · wmk)u(wmk)
∝exp

















−
 PN
j=1 z2
kj
2σ2
!
|
{z
}
1/(2 g
σ2
mk)
w2
mk + wmk

−λW
mk + 1
σ2
N
X
j=1
zkj

amj −
K
X
i̸=k
wmizij


|
{z
}
g
σ2
mk
−1
g
µmk

















· u(wmk)
∝N(wmk | g
µmk, g
σ2
mk) · u(wmk) = T N(wmk | g
µmk, g
σ2
mk),
(6.5)
where u(x) is the unit function with value 1 if x ≥0 and value 0 if x < 0,
g
σ2
mk =
σ2
PN
j=1 z2
kj
(6.6)
is the posterior “parent” variance of the normal distribution with “parent” mean g
µmk,
g
µmk =

−λW
mk + 1
σ2
N
X
j=1
zkj

amj −
K
X
i̸=k
wmizij

· g
σ2
mk
(6.7)
and T N(x | µ, σ2) is the truncated-normal (TN) density with “parent” mean µ and “parent”
variance σ2 (Deﬁnition 2.4.1, p. 50).
Interpretation of the Posterior: Sparsity Constraint
We can see that the exponential prior is equivalent to impose a L1 norm such that
the GEE model favors a sparsity constraint. The sparsity comes from the negative
term −λW
mk in Equation (6.7), when λW
mk becomes larger, the posterior “parent” mean
becomes smaller, and the TN distribution will have a larger probability for smaller
1. See Figure 6.1(a) and Section 5.2 (p. 129).
146

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
values since the draws of T N(wmk | g
µmk, g
σ2
mk) will be around zero thus imposing
sparsity (see Figure 2.8(a), p. 52).
Or after rearrangement, the posterior density of wmk can be equivalently described by
p(wmk | A, W−mk, Z, σ2, λW )
∝exp

















( −1
2σ2
N
X
j=1
z2
kj)w2
mk + wmk

1
σ2
N
X
j=1
zkj

amj −
K
X
i̸=k
wmizij


|
{z
}
d
σ2
mk
−1 d
µmk

















exp(−λW
mkwmk)u(wmk)
∝N(wmk | d
µmk, d
σ2
mk) · E(wmk | λW
mk) = RN(wmk | d
µmk, d
σ2
mk, λW
mk),
where d
σ2
mk = g
σ2
mk =
σ2
PN
j=1 z2
kj is the posterior “parent” variance of the normal distribution
with “parent” mean d
µmk,
d
µmk =
1
PN
j=1 z2
kj
·
N
X
j=1
zkj

amj −
K
X
i̸=k
wmizij

and RN(x | µ, σ2, λ) ∝N(x | µ, σ2)E(x | λ) is the rectiﬁed-normal (RN) density (Deﬁni-
tion 2.4.4, p. 54). And due to symmetry, a similar expression for zkn can be easily derived.
The conditional density of σ2 depends on its parents (ασ, βσ), children (A), and coparents
(W , Z). And it is an inverse-Gamma distribution (by conjugacy in Equation (2.7), p. 40),
p(σ2 | A, W , Z, ασ, βσ) = G−1(σ2 | f
ασ, f
βσ),
f
ασ = MN
2
+ ασ,
f
βσ = 1
2
M,N
X
m,n=1
(A −W Z)2
mn + βσ.
(6.8)
Gibbs sampling.
By this Gibbs sampling method introduced in Section 1.3.3 (p. 20),
we can construct a Gibbs sampler for the GEE model as formulated in Algorithm 11. And
also in practice, all the parameters of the exponential distribution are set to be the same
value λ = {λW
mk}′s = {λZ
nk}′s for all m, k, n. By default, uninformative hyperparameters
are ασ = βσ = 1, {λW
mk} = {λZ
kn} = 0.1.
6.3. Gaussian Likelihood with Exponential Priors and ARD Hierarchical
Prior (GEEA)
The Gaussian likelihood with exponential priors and hierarchical prior (GEEA) model is
proposed by Tan and F´evotte (2013) based on the GEE model where the diﬀerence lies
in that the GEEA model applies a hyperprior on the exponential prior. Moreover, GEEA
favors an automatic relevance determination (ARD) that helps perform automatic model
selection. This ARD works by replacing the individual scale parameter of exponential prior
for the factored components W , Z by one that is shared by all entries in the same column
of W and the same row of Z. In other words, the parameters are shared for each factor.
147

6.3. GAUSSIAN LIKELIHOOD WITH EXPONENTIAL PRIORS AND ARD HIERARCHICAL PRIOR
(GEEA)
Algorithm 11 Gibbs sampler for GEE model in one iteration (prior on variance σ2 here,
similarly for the precision τ). The procedure presented here may not be eﬃcient but is
explanatory. A more eﬃcient one can be implemented in a vectorized manner. By default,
uninformative hyperparameters are ασ = βσ = 1, {λW
mk} = {λZ
kn} = 0.1.
Require: Choose initial ασ, βσ, λW
mk, λZ
kn;
1: for k = 1 to K do
2:
for m = 1 to M do
3:
Sample wmk from p(wmk | A, W−mk, Z, σ2, λW
mk);
▷Equation (6.5)
4:
end for
5:
for n = 1 to N do
6:
Sample zkn from p(zkn | A, W , Z−kn, σ2λZ
kn);
▷Symmetry of Eq. (6.5)
7:
end for
8: end for
9: Sample σ2 from p(σ2 | A, W , Z, ασ, βσ);
▷Equation (6.8)
10: Report loss in Equation (6.1), stop if it converges.
Hyperprior.
For each prior density in Equation (6.3), we assume a Gamma distribution
on the parameters of exponential distributions in Equation (6.3),
wmk ∼E(wmk | λk),
zkn ∼E(zkn | λk),
λk ∼G(λk | αλ, βλ),
where λk is shared by all entries in the same column of W and the same row of Z. The
entire factor k is then either activated if λk has a low value or “turned oﬀ” if λk has a
high value (see Figure 2.6, p. 50). Therefore we can give an upper bound on the number of
hidden factors K instead of choosing the correct value of K. The graphical model is shown
in Figure 6.1(b).
Posterior.
For NMF, following the Bayes’ rule and MCMC, this means we need to be
able to draw from distributions (again by Markov blanket, Section 5.2, p. 129):
p(σ2 | A, W , Z, λ),
p(wmk | A, W−mk, Z, σ2, λ),
p(λk | W , Z, λ−k, αλ, βλ),
p(zkn | A, W , Z−kn, σ2, λ),
where λ ∈RK
+ is a vector including all λk values and λ−k denotes all elements of λ except
λk. The posteriors for wmk’s and zkn’s are the same as those in the GEE model, except now
we replace λW
mk and λZ
kn by λk. The posteriors for λk can be obtained using Bayes’ theorem.
The conditional density of λk depends on its parents (αλ, βλ), children (k-th column b
wk or
W , k-th row bzk of Z, note we deﬁne wm as the m-th row of W and zn as the n-th column
148

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
of Z in Equation (6.1)), and coparents (none) 2. Then it follows that,
p(λk | W , Z, αλ, βλ)
∝p( b
wk, bzk | λk) × p(λk) =
M
Y
i=1
E(wik | λk) ·
N
Y
j=1
E(zkj | λk) × G(λk | αλ, βλ)
=
M
Y
i=1
λk exp(−λkwik) ·
N
Y
j=1
λk exp(−λkzkj) × βαλ
λ
Γ(αλ)λαλ−1
k
exp(−λkβλ)
∝λM+N+αλ−1
k
exp
(
−λk ·
 K
X
k=1
(wmk + zkn) + βλ
!)
∝G(λk | f
αλ, f
βλ),
(6.9)
where
f
αλ = M + N + αλ,
f
βλ =
K
X
k=1
(wmk + zkn) + βλ.
From this posterior form, the prior parameter αλ can be interpreted as the number of prior
observations, and βλ as the sum of the prior observations. Therefore, week prior parameter
can be chosen as αλ = βλ = 1.
Gibbs sampling.
Again we can construct a Gibbs sampler for the GEEA model as for-
mulated in Algorithm 12. By default, uninformative hyperparameters are ασ = βσ = 1,
αλ = βλ = 1.
Algorithm 12 Gibbs sampler for GEEA model in one iteration (prior on variance σ2 here,
similarly for the precision τ). The procedure presented here may not be eﬃcient but is
explanatory. A more eﬃcient one can be implemented in a vectorized manner. By default,
uninformative hyperparameters are ασ = βσ = 1, αλ = βλ = 1.
Require: Choose initial ασ, βσ, αλ, βλ;
1: for k = 1 to K do
2:
for m = 1 to M do
3:
Sample wmk from p(wmk | A, W−mk, Z, σ2, λk);
▷Equation (6.5)
4:
end for
5:
for n = 1 to N do
6:
Sample zkn from p(zkn | A, W , Z−kn, σ2, λk);
▷Symmetry of Eq. (6.5)
7:
end for
8:
Sample λk from p(λk | W , Z, αλ, βλ);
▷Equation (6.9)
9: end for
10: Sample σ2 from p(σ2 | A, W , Z, ασ, βσ);
▷Equation (6.8)
11: Report loss in Equation (6.1), stop if it converges.
2. See Figure 6.1(b) and Section 5.2 (p. 129).
149

6.4. GAUSSIAN LIKELIHOOD WITH TRUNCATED-NORMAL PRIORS (GTT)
6.4. Gaussian Likelihood with Truncated-Normal Priors (GTT)
The Gaussian likelihood with truncated-normal priors (GTT) model is discussed in Brouwer
and Lio (2017) where truncated-normal (TN) priors are used over factored matrices (Fig-
ure 6.2(a)). The truncated-normal distribution is a variant of the normal distribution where
the values smaller than zero are excluded (Deﬁnition 2.4.1, p. 50) and thus it can impose
nonnegativity in Bayesian models. The likelihood is chosen to be the same as that in the
GEE model (Equation (6.2)).
Prior.
We assume W and Z are independently truncated-normal distributed with mean
and precision {µW , τ W }, {µZ, τ Z},
wmk ∼T N(wmk | µW
mk, (τ W
mk)−1),
zkn ∼T N(zkn | µZ
kn, (τ Z
kn)−1),
(6.10)
where µW is an M ×K matrix containing all {µW
mk} entries, µZ is a K ×N matrix including
all {µZ
kn} values, τ W is an M × K matrix containing all {τ W
mk} entries, and τ Z is a K × N
matrix including all {τ Z
kn} values.
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
ατ/ασ
βτ/βσ
τ/σ2
µW
mk, τ W
mk
µZ
kn, τ Z
kn
(a) GTT.
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
ατ/ασ
βτ/βσ
τ/σ2
µW
mk, τ W
mk
µZ
kn, τ Z
kn
µµ, τµ
a, b
(b) GTTN.
Figure 6.2: Graphical model representation of GTT and GTTN models. Green circles
denote prior variables, orange circles represent observed and latent variables, and plates
represent repeated variables. The slash “/” in the variable represents “or”, and the comma
“,” in the variable represents “and”.
Posterior.
Again, following the Bayes’ rule and MCMC, this means we need to be able
to draw from distributions (by Markov blanket, Section 5.2, p. 129):
p(wmk | A, W−mk, Z, σ2, µW
mk, τ W
mk),
p(zkn | A, W , Z−kn, σ2, µZ
kn, τ Z
kn),
p(σ2 | A, W , Z, ασ, βσ),
where W−mk denotes all elements of W except wmk and Z−kn denotes all elements of Z
except zkn. Using Bayes’ theorem, the conditional density of wmk depends on its parents
150

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
(µW
mk, τ W
mk), children (amn), and coparents (τ or σ2, W−mk, Z) 3. And it can be obtained
by (similar to computing the conditional density of wmk in the GEE model, Equation (6.5))
p(wmk | σ2, W−mk, Z, µW
mk, τ W
mk, A) ∝p(A | W , Z, σ2) · p(wmk | µW
mk, (τ W
mk)−1)
=
M,N
Y
i,j=1
N
 aij | w⊤
i zj, σ2
× T N(wmk | µW
mk, (τ W
mk)−1)
∝exp

−(
PN
j=1 z2
kj
2σ2
+ τ W
mk
2 )w2
mk + wmk
 1
σ2
N
X
j=1
zkj(amj −
K
X
i̸=k
wmizij) + τ W
mkµW
mk
	
u(wmk)
∝N(wmk | g
µmk, g
σ2
mk) · u(wmk) = T N(wmk | g
µmk, g
σ2
mk),
(6.11)
where g
σ2
mk =
σ2
PN
j=1 z2
kj+τ W
mk·σ2 is the posterior “parent” variance of the normal distribution
with “parent” mean g
µmk,
g
µmk =



1
σ2
N
X
j=1
zkj

amj −
K
X
i̸=k
wmizij

+ τ W
mkµW
mk


· g
σ2
mk.
Again, due to symmetry, a similar expression for zkn can be easily derived. Finally, the
conditional density of σ2 is the same as that in the GEE model (Equation (6.8)).
Algorithm 13 Gibbs sampler for GTT model in one iteration (prior on variance σ2 here,
similarly for the precision τ). The procedure presented here may not be eﬃcient but is
explanatory. A more eﬃcient one can be implemented in a vectorized manner. By default,
uninformative hyperparameters are ασ = βσ = 1, {µW
mk} = {µZ
kn} = 0, {τ W
mk} = {τ Z
kn} = 0.1.
Require: Choose initial ασ, βσ, µW
mk, τ W
mk, µZ
kn, τ Z
kn;
1: for k = 1 to K do
2:
for m = 1 to M do
3:
Sample wmk from p(wmk | A, W−mk, Z, σ2, µW
mk, τ W
mk);
▷Equation (6.11)
4:
end for
5:
for n = 1 to N do
6:
Sample zkn from p(zkn | A, W , Z−kn, σ2, µZ
kn, τ Z
kn);
▷Symmetry of Eq. (6.11)
7:
end for
8: end for
9: Sample σ2 from p(σ2 | A, W , Z, ασ, βσ);
▷Equation (6.8)
10: Report loss in Equation (6.1), stop if it converges.
Gibbs sampling.
We can again construct a Gibbs sampler for the GTT model as for-
mulated in Algorithm 13. And also in practice, all the parameters of the truncated-normal
priors are set to be the same value µW = {µW
mk}′s, µZ = {µZ
nk}′s, τ W = {τ W
mk}′s, τ Z =
{τ Z
nk}′s for all m, k, n.
By default, uninformative hyperparameters are ασ = βσ = 1,
{µW
mk} = {µZ
kn} = 0, {τ W
mk} = {τ Z
kn} = 0.1.
3. See Figure 6.1(a) and Section 5.2 (p. 129).
151

6.5. GAUSSIAN LIKELIHOOD WITH TRUNCATED-NORMAL AND HIERARCHICAL PRIORS
(GTTN)
6.5. Gaussian Likelihood with Truncated-Normal and Hierarchical Priors
(GTTN)
This hierarchical prior is proposed in Schmidt and Mohamed (2009) over a rectiﬁed-normal
distribution originally, and further discussed in Brouwer and Lio (2017) based on the GTT
model where the diﬀerence lies in that the GTTN model puts a hyperprior on the two
parameters of the truncated-normal distribution (Figure 6.2(b)).
Hyperprior.
We have shown in Equation (2.18) (p. 51) that the truncated-normal density
is a conjugate prior over the nonnegative mean parameter of a Gaussian distribution that
is favored in the GTT model. Moreover, if the prior over {wmk}’s and {zkn}’s had been
a Gaussian, appropriate conjugate priors for the mean and variance would be a normal-
inverse-Gamma or a normal-inverse-Chi-square distribution (Equation (2.11), p. 42; Equa-
tion (2.14), p. 46). However, these priors are not conjugate to the truncated-normal density.
And instead, a convenient prior called TN-scaled-normal-Gamma (TNSNG) distribution is
used (or a TN-scaled-normal-inverse-Gamma prior for “parent” mean and “parent” variance
parameters) 4:
µW
mk, τ W
mk | µµ, τµ, a, b ∼T NSNG(µW
mk, τ W
mk | µµ, τµ, a, b)
∝
1
q
τ W
mk

1 −Φ
 −µW
mk
q
τ W
mk

· N(µW
mk | µµ, (τµ)−1) · G(τ W
mk | a, b);
µZ
kn, τ Z
kn | µµ, τµ, a, b ∼T NSNG(µZ
kn, τ Z
kn | µµ, τµ, a, b).
Here we use the same hyperparameters {µµ, τµ, a, b} over diﬀerent entries {uW
mk, τ W
mk} and
{uZ
kn, τ Z
kn}. However, in rare cases, one may favor diﬀerent behaviors over W and Z, e.g.,
small values in W and large values in Z, the hyperparameters can be chosen as diﬀerent
values (see the comparison in Figure 6.3). Note that the scaled-normal-Gamma distribution
is not simply a product of a normal and a Gamma distribution. It is not easy to sample
from this distribution; however, we will see the posteriors have simple forms due to this
prior; that’s the reason why we add scaled terms in the prior density.
The prior can
decouple parameters µW
mk, τ W
mk, and the posterior conditional densities of them are normal
and Gamma respectively due to this convenient scale.
Posterior.
The posteriors for {wmk}’s, {zkn}’s, and σ2 are the same as those in the
GTT model. The posteriors for {uW
mk, τ W
mk} can be obtained using Bayes’ rule where the
conditional density of {µW
mk, τ W
mk} depend on their parents (µµ, τµ, a, b), children (wmk), and
coparents (none). Then it follows from the likelihood in Equation (6.10) that the conditional
4. The original hyperprior in Schmidt and Mohamed (2009) is a rectiﬁed-normal (RN) scaled one. Here we
scale it in the context of truncated-normal density.
152

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
ατ/ασ
βτ/βσ
τ/σ2
µW
mk, τ W
mk
µZ
kn, τ Z
kn
µµ, τµ
a, b
(a) GTTN with same hyperparameters. Same
as Figure 6.2(b).
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
ατ/ασ
βτ/βσ
τ/σ2
µW
mk, τ W
mk
µZ
kn, τ Z
kn
µµ, τµ
a, b
µ′
µ, τ ′
µ
a′, b′
(b) GTTN with diﬀerent hyperparameters.
Figure 6.3: Graphical model representation of GTTN with same and diﬀerent hyperpa-
rameters. Green circles denote prior variables, orange circles represent observed and latent
variables, and plates represent repeated variables. The slash “/” in the variable represents
“or”, and the comma “,” in the variable represents “and”.
densities of µW
mk is
p(µW
mk | τ W
mk, wmk, µµ, τµ, a, b)
∝T N(wmk | µW
mk, (τ W
mk)−1) ·
1
q
τ W
mk

1 −Φ
 −µW
mk
q
τ W
mk

N(µW
mk | µµ, τµ)G(τ W
mk | a, b)
∝exp









−τ W
mk + τµ
2
|
{z
}
et/2
(µW
mk)2 + µW
mk (τ W
mkwmk + τµµµ)
|
{z
}
em·et









∝N(µW
mk | em, et−1),
(6.12)
where et = τ W
mk + τµ, em = (τ W
mkwmk + τµµµ)/et. And the conditional density of τ W
mk is
p(τ W
mk | µW
mk, wmk, µµ, τµ, a, b)
∝T N(wmk | µW
mk, (τ W
mk)−1) ·
1
q
τ W
mk

1 −Φ
 −µW
mk
q
τ W
mk

N(µW
mk | µµ, τµ)G(τ W
mk | a, b)
∝(τ W
mk)a−1 exp

−

b + (wmk −µW
mk)2
2

τ W
mk

∝G(τ W
mk | ea,eb),
(6.13)
where ea = a,eb = b + (wmk−µW
mk)2
2
. And again due to symmetry, the expressions for µZ
kn
and τ Z
kn can be easily derived similarly. The Gibbs sampler for the GTTN model is then
153

6.6. GAUSSIAN LIKELIHOOD WITH RECTIFIED-NORMAL PRIORS (GRR) AND HIERARCHICAL
PRIOR (GRRN)
formulated in Algorithm 14. By default, uninformative hyperparameters are ασ = βσ = 1,
µµ = 0, τµ = 0.1, a = b = 1.
Algorithm 14 GibbssSampler for GTTN model in one iteration (prior on variance σ2
here, similarly for the precision τ). The procedure presented here may not be eﬃcient but
is explanatory. A more eﬃcient one can be implemented in a vectorized manner. By default,
uninformative hyperparameters are ασ = βσ = 1, µµ = 0, τµ = 0.1, a = b = 1.
Require: Choose initial ασ, βσ, µµ, τµ, a, b;
1: for k = 1 to K do
2:
for m = 1 to M do
3:
Sample wmk from p(wmk | A, W−mk, Z, σ2, µW
mk, τ W
mk);
▷Equation (6.11)
4:
Sample µW
mk from p(µW
mk | τ W
mk, wmk, µµ, τµ, a, b);
▷Equation (6.12)
5:
Sample τ W
mk from p(τ W
mk | µW
mk, wmk, µµ, τµ, a, b);
▷Equation (6.13)
6:
end for
7:
for n = 1 to N do
8:
Sample zkn from p(zkn | A, W , Z−kn, σ2, µZ
kn, τ Z
kn);
▷Symmetry of Eq. (6.11)
9:
Sample µZ
kn from p(µZ
kn | τ Z
kn, zkn, µµ, τµ, a, b);
▷Symmetry of Eq. (6.12)
10:
Sample τ Z
kn from p(τ Z
kn | µZ
kn, zkn, µµ, τµ, a, b);
▷Symmetry of Eq. (6.13)
11:
end for
12: end for
13: Sample σ2 from p(σ2 | A, W , Z, ασ, βσ);
▷Equation (6.8)
14: Report loss in Equation (6.1), stop if it converges.
6.6. Gaussian Likelihood with Rectiﬁed-Normal Priors (GRR) and
Hierarchical Prior (GRRN)
The Gaussian likelihood with rectiﬁed-normal and hierarchical priors (GRR and GRRN)
models are proposed in Lu and Ye (2022) to further favor ﬂexibility based on GTT and
GTTN models. Again, we view the data A as being produced according to the probabilistic
generative process shown in Figure 6.4(b). The observed (m, n)-th data entry amn of matrix
A is modeled using a Gaussian likelihood function with variance σ2 and mean given by the
latent decomposition w⊤
mzn (Equation (6.1)). The likelihood is again chosen to be the same
as that in the GEE model (Equation (6.2)).
Prior.
We treat the latent variables wmk’s (and zkn’s) as random variables.
And we
need prior densities over these latent variables to express beliefs for their values, e.g., non-
negativity in this context.
Here we assume further that the latent variables wmk’s and
zkn’s are independently drawn from a rectiﬁed-normal (RN) priors (a.k.a., an exponentially
rectiﬁed-normal distribution, Deﬁnition 2.4.4, p. 54),
p(wmk | ·) = RN(wmk | µW
mk, (τ W
mk)−1, λW
mk);
p(zkn | ·) = RN(zkn | µZ
kn, (τ Z
kn)−1, λZ
kn).
(6.14)
This prior serves to enforce the nonnegativity constraint on the components W , Z, and
is conjugate to the Gaussian likelihood (Equation (2.21), p. 55). In some scenarios, the
154

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
ατ/ασ
βτ/βσ
τ/σ2
µW
mk, τ W
mk,
λW
mk
µZ
kn, τ Z
kn,
λZ
kn
(a) GRR.
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
ατ/ασ
βτ/βσ
τ/σ2
µW
mk, τ W
mk,
λW
mk
µZ
kn, τ Z
kn,
λZ
kn
µµ, τµ
a, b
αλ, βλ
(b) GRRN.
Figure 6.4: Graphical representation of GRR and GRRN models. Green circles denote
prior variables, orange circles represent observed and latent variables, and plates represent
repeated variables. The slash “/” in the variable represents “or”, and the comma “,” in the
variable represents “and”.
two sets of latent variables can be drawn from two diﬀerent rectiﬁed-normal priors, e.g.,
enforcing sparsity in W while non-sparsity in Z. And we shall not consider this case for
our later examples as it is not the main interest of this book. The posterior density is a
truncated-normal distribution that is a special rectiﬁed-normal distribution. The model is
then called a Gaussian likelihood with rectiﬁed-normal priors (GRR) model. Since the RN
distribution is a special TN distribution, the GRR model is the same as the GTT model
with a careful choice of prior parameters. What makes the RN prior important is from the
hierarchical model that provides ﬂexibility and guidance on the prior parameter choices.
Hierarchical prior.
To further favor ﬂexibility, we choose a convenient joint hyperprior
density over the parameters {µW
mk, τ W
mk, λW
mk} of RN prior in Equation (6.14), namely, the
RN-scaled-normal-Gamma (RNSNG) prior,
p(µW
mk, τ W
mk, λW
mk | ·) = RNSNG(µW
mk, τ W
mk, λW
mk | µµ, τµ, a, b, αλ, βλ)
= C(µW
mk, τ W
mk, λW
mk) · N(µW
mk | µµ, (τµ)−1) · G(τ W
mk | a, b) · G(λW
mk | αλ, βλ),
(6.15)
where C(µW
mk, τ W
mk, λW
mk) is a constant in terms of {µW
mk, τ W
mk, λW
mk}. This prior can decouple
parameters µW
mk, τ W
mk, λW
mk, and the posterior conditional densities of them are Gaussian,
Gamma, and Gamma respectively due to this convenient scale. A similar RNSNG prior is
given over {µZ
kn, τ Z
kn, λZ
kn}.
155

6.6. GAUSSIAN LIKELIHOOD WITH RECTIFIED-NORMAL PRIORS (GRR) AND HIERARCHICAL
PRIOR (GRRN)
Posterior.
Again, following the Bayes’ rule and MCMC, this means we need to be able
to draw from distributions (by Markov blanket, Section 5.2, p. 129):
p(wmk | A, W−mk, Z, σ2, µW
mk, τ W
mk, λW
mk),
p(zkn | A, W , Z−kn, σ2, µZ
kn, τ Z
kn, λZ
kn),
p(σ2 | A, W , Z, ασ, βσ),
where W−mk denotes all elements of W except wmk and Z−kn denotes all elements of Z
except zkn. Using Bayes’ theorem, the conditional density of wmk depends on its parents
(µW
mk, τ W
mk, λW
mk), children (amn), and coparents (τ or σ2, W−mk, Z) 5. The conditional den-
sity of wmk is a truncated-normal density. And it can be obtained by (similar to computing
the conditional density of wmk in the GEE model, Equation (6.5)),
p(wmk | A, W−mk, Z, σ2, µW
mk, τ W
mk, λW
mk) ∝p(A | W , Z, σ2) × p(wmk | µmk, τmk, λmk)
∝
M,N
Y
i,j=1
N(aij | w⊤
i zj, σ2) × RN(µmk, (τmk)−1, λmk)
⋆∝
M,N
Y
i,j=1
N(aij | w⊤
i zj, σ2) × T N





τ W
mkµW
mk −λW
mk
τ W
mk
|
{z
}
:=µ′
, (τmk)−1





∝exp


−
 PN
j=1 z2
kj
2σ2
+ τ W
mk
2
!
w2
mk + wmk

1
σ2
N
X
j=1
zkj(amj −
K
X
i̸=k
wmkzij) + τ W
mkµ′




u(wmk)
∝N(wmk | g
µmk, g
σ2
mk)u(wmk) = T N(wmk | g
µmk, g
σ2
mk),
(6.16)
where the equality (⋆) is from the equivalence between the RN and TN distributions (Def-
inition 2.4.4, p. 54), g
σ2
mk =
σ2
PN
j=1 z2
kj+τ W
mk·σ2 is the posterior “parent” variance of the normal
distribution with posterior “parent” mean
g
µmk =

1
σ2
N
X
j=1
zkj(amj −
K
X
i̸=k
wmkzij) + τ W
mkµ′

· g
σ2
mk.
with µ′ = τ W
mkµW
mk−λW
mk
τ W
mk
being the “parent” mean of the truncated-normal density. Due to
symmetry, the conditional posterior for zkn can be easily derived similarly.
Extra update for GRRN
Following the graphical representation of the GRRN model
in Figure 6.4(b), we need to draw samples iteratively from
p(µW
mk | τ W
mk, λW
mk, µµ, τµ, a, b, αλ, βλ, wmk),
p(τ W
mk | µW
mk, λW
mk, µµ, τµ, a, b, αλ, βλ, wmk),
p(λW
mk | µW
mk, τ W
mk, µµ, τµ, a, b, αλ, βλ, wmk).
5. See Figure 6.1(a) and Section 5.2 (p. 129).
156

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
The conditional density for µW
mk is a truncated-normal (a special rectiﬁed-normal),
p(µW
mk | τ W
mk, λW
mk, µµ, τµ, a, b, αλ, βλ, wmk)
∝RN(wmk | µW
mk, (τ W
mk)−1, λW
mk) · RNSNG(µW
mk, τ W
mk, λW
mk | µµ, τµ, a, b, αλ, βλ)
∝RN(wmk | µW
mk, (τ W
mk)−1, λW
mk) · N(µW
mk | µµ, (τµ)−1) · G(τ W
mk | a, b) · G(λW
mk | αλ, βλ)
= N(wmk|µW
mk, (τ W
mk)−1) ·
E(wmk|λW
mk) · N(µW
mk|µµ, (τµ)−1) ·
G(τ W
mk|a, b) ·(((((((
G(λW
mk|αλ, βλ)
∝N(wmk | µW
mk, (τ W
mk)−1)N(µW
mk | µµ, (τµ)−1) ∝N(µW
mk | em, et−1),
(6.17)
where et = τ W
mk + τµ, em = (τ W
mkwmk + τµµµ)/et are the posterior mean and precision respec-
tively. The samples wmk’s are nonnegative due to the rectiﬁcation in the distribution (by
exponential distribution inside the density). However, this “parent” mean parameter µW
mk
is not limited to be nonnegative.
The conditional density for τ W
mk is a Gamma distribution,
p(τ W
mk | µW
mk, λW
mk, µµ, τµ, a, b, αλ, βλ, wmk)
∝RN(wmk | µW
mk, (τ W
mk)−1, λW
mk) · RNSNG(µW
mk, τ W
mk, λW
mk | µµ, τµ, a, b, αλ, βλ)
∝RN(wmk | µW
mk, (τ W
mk)−1, λW
mk) · N(µW
mk | µµ, (τµ)−1) · G(τ W
mk | a, b) · G(λW
mk | αλ, βλ)
= N(wmk|µW
mk, (τ W
mk)−1) ·
E(wmk|λW
mk) ·(((((((((
(
N(µW
mk|µµ, (τµ)−1) · G(τ W
mk|a, b) ·(((((((
G(λW
mk|αλ, βλ)
∝N(wmk | µW
mk, (τ W
mk)−1)G(τ W
mk | a, b)
∝(τ W
mk)a+ 1
2 −1 exp

−

b + (wmk −µW
mk)2
2

τ W
mk

∝G(τ W
mk | ea,eb),
(6.18)
where ea = a + 1
2,eb = b + (wmk−µW
mk)2
2
are the posterior shape and rate parameters.
Furthermore, the conditional density for λW
mk is also a Gamma distribution,
p(λW
mk | µW
mk, τ W
mk, µµ, τµ, a, b, αλ, βλ, wmk)
∝RN(wmk | µW
mk, (τ W
mk)−1, λW
mk) · RNSNG(µW
mk, τ W
mk, λW
mk | µµ, τµ, a, b, αλ, βλ)
∝RN(wmk | µW
mk, (τ W
mk)−1, λW
mk) · N(µW
mk | µµ, (τµ)−1) · G(τ W
mk | a, b) · G(λW
mk | αλ, βλ)
= (((((((((((
N(wmk|µW
mk, (τ W
mk)−1) · E(wmk|λW
mk) ·(((((((((
(
N(µW
mk|µµ, (τµ)−1) ·
G(τ W
mk|a, b) · G(λW
mk|αλ, βλ)
∝E(wmk | λW
mk)G(λW
mk | αλ, βλ) ∝G(λW
mk | f
αλ, f
βλ),
(6.19)
where f
αλ = αλ + 1, f
βλ = βλ + wmk. The importance for this hierarchical prior is revealed
that, from this conditional density, the prior parameter αλ can be interpreted as
the number of prior observations, and βλ as the prior knowledge of wmk. On the
one hand, an uninformative choice for αλ is αλ = 1. On the other hand, if one prefers a
sparse decomposition with larger regularization on the model, βλ can be chosen as a small
value, e.g., βλ = 0.01. Or a large value, e.g., βλ = 100, can be applied since we are in the
NMF context; a large value in W will enforce the counterparts in Z to have small values.
While an uninformative choice for βλ is as follows. Suppose the mean value of all entries of
matrix A is m0, then βλ can be set as βλ = pm0
K where the K is the latent dimension such
that each prior entry amn = w⊤
mzn is equal to m0. After developing this hierarchical prior,
157

6.6. GAUSSIAN LIKELIHOOD WITH RECTIFIED-NORMAL PRIORS (GRR) AND HIERARCHICAL
PRIOR (GRRN)
Algorithm 15 Gibbs sampler for GRRN in one iteration (prior on variance σ2 here, sim-
ilarly for the precision τ). The procedure presented here may not be eﬃcient but is ex-
planatory. A more eﬃcient one can be implemented in a vectorized manner. By default,
uninformative priors are ασ = βσ = 1, µµ = 0, τµ = 0.1, a = b = 1, αλ = 1, βλ = p m0
K .
One can even set βλ = 20 · pm0
K or 0.1 · pm0
K if one prefers a larger regularization.
1: Input: Choose parameters ασ, βσ, µµ, τµ, a, b, αλ, βλ;
2: for k = 1 to K do
3:
for m = 1 to M do
4:
Sample wmk from p(wmk | A, W−mk, Z, σ2, µW
mk, τ W
mk, λW
mk);
▷Equation (6.16)
5:
Sample µW
mk from p(µW
mk | τ W
mk, λW
mk, µµ, τµ, a, b, αλ, βλ, wmk);
▷Equation (6.17)
6:
Sample τ W
mk from p(τ W
mk | µW
mk, λW
mk, µµ, τµ, a, b, αλ, βλ, wmk);
▷Equation (6.18)
7:
Sample λW
mk from p(λW
mk | µW
mk, τ W
mk, µµ, τµ, a, b, αλ, βλ, wmk);
▷Equation (6.19)
8:
end for
9:
for n = 1 to N do
10:
Sample zkn from p(zkn | A, W , Z−kn, σ2, µZ
kn, τ Z
kn, λZ
kn);
▷Sytry. of Eq. (6.16)
11:
Sample µZ
kn from p(µZ
kn | τ Z
kn, λZ
kn, µµ, τµ, a, b, αλ, βλ, zkn); ▷Sytry. of Eq. (6.17)
12:
Sample τ Z
kn from p(τ Z
kn | µZ
kn, λZ
kn, µµ, τµ, a, b, αλ, βλ, zkn);
▷Sytry. of Eq. (6.18)
13:
Sample λZ
kn from p(λZ
kn | µZ
kn, τ Z
kn, µµ, τµ, a, b, αλ, βλ, zkn); ▷Sytry. of Eq. (6.19)
14:
end for
15: end for
16: Sample σ2 from p(σ2 | A, W , Z, ασ, βσ);
▷Equation (6.8)
17: Report loss in Equation (6.1), stop if it converges.
we realize its similarity with the GTTN model (ﬁrst introduced in a tensor decomposition
context (Schmidt and Mohamed, 2009), and further discussed in Brouwer and Lio (2017)).
However, the parameters in conditional densities of the GTTN model lack interpretation
and ﬂexibility so that there are no guidelines for parameter tuning when the performance
is poor. The GRRN model, on the other hand, can work well generally when we select the
uninformative prior βλ = p m0
K ; moreover, one can even set βλ = 20 · p m0
K or 0.1 · p m0
K if
one prefers a larger regularization as mentioned above.
Due to symmetry, the conditional expression for µZ
kn, τ Z
kn, and λZ
kn can be easily derived
similarly; and we shall not go into the details.
Gibbs sampling.
The full procedure is formulated in Algorithm 15. By default, unin-
formative priors are ασ = βσ = 1, µµ = 0, τµ = 0.1, a = b = 1, αλ = 1, βλ = pm0
K .
Computational complexity.
The adopted Gibbs sampling method for the GRRN model
has complexity O(MNK2) where the most costs come from the update on the conditional
density of wmk and zkn. In the meantime, all the methods we have introduced in the above
sections (GEE, GTT, GTTN) have complexity O(MNK2). Compared to the GTTN model,
the GRRN model only has an extra cost on the update of λW
mk which does not amount to
the bottleneck of the algorithm.
158

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
2
4
MovieLens 100K
2
4
MovieLens 1M
Figure 6.5:
Data distribution of
MovieLens 100K and MovieLens 1M
data sets.
The MovieLens 1M data
set has a larger fraction of users who
give a rate of 5 and a smaller fraction
for rates of 3.
Data set
Rows
Columns
Fraction obs.
MovieLens 100K
943
1473
0.072
MovieLens 1M
6040
3503
0.047
Table 6.2:
Data set description.
99,723 and
999,917 observed entries for MovieLens 100K and
MovieLens 1M data sets respectively (user vectors
or movie vectors with less than 3 observed entries
are cleaned). MovieLens 100K is relatively a small
data set and the MovieLens 1M tends to be large;
while both of them are sparse.
6.6.1 Examples
To demonstrate the main advantages of the introduced GRRN method, we conduct exper-
iments with diﬀerent analysis tasks; and diﬀerent data sets including the MovieLens 100K
and the MovieLens 1M from movie ratings for diﬀerent users (Harper and Konstan, 2015).
The data sets have a range from one to ﬁve stars with around 100,000 and 1,000,000 ratings
respectively and we want to predict the missing entries for users so that we can recommend
the movies they like (user vectors or movie vectors with less than 3 observed entries are
cleaned). A summary of the two data sets can be seen in Table 6.2 and their distributions
are shown in Figure 6.5. The MovieLens 1M data set has a larger fraction of users who
give a rate of 5 and a smaller fraction for rates of 3. We can see that the MovieLens 100K
is relatively a small data set and the MovieLens 1M tends to be large; while both of them
are sparse. On the other hand, the MovieLens 1M data set not only has a larger number of
users, but also has an increased dimension (the number of movies) making it a harder task
to evaluate.
In all scenarios, the same parameter initialization is adopted when conducting diﬀerent
tasks. We compare the results in terms of convergence speed and generalization. In a wide
range of scenarios across various models, GRRN improves convergence rates, and leads to
out-of-sample performances that are as good or better than other Bayesian NMF models.
Hyperparameters.
We follow the default hyperparameter setups in Brouwer and Lio
(2017). We use {λW
mk} = {λZ
kn} = 0.1 (GEE); {µZ
mk} = {µZ
kn} = 0, {τ Z
mk} = {τ Z
kn} = 0.1
(GTT); uninformative ασ = βσ = 1 (Gaussian likelihood in GEE, GTT, GTTN, GRRN);
µµ = 0, τµ = 0.1, a = b = 1 (hyperprior in GTTN, GRRN); αλ = 1, βλ = p m0
K (hyperprior
in GRRN). These are very weak prior choices and the models are not sensitive to them
(Brouwer and Lio, 2017). As long as the hyperparameters are set, the observed or unob-
served variables are initialized from random draws as this initialization procedure provides
a better initial guess of the right patterns in the matrices. In all experiments, we run the
Gibbs sampler 500 iterations with a burn-in of 400 iterations as the convergence analysis
shows the algorithm can converge in less than 200 iterations.
159

6.6. GAUSSIAN LIKELIHOOD WITH RECTIFIED-NORMAL PRIORS (GRR) AND HIERARCHICAL
PRIOR (GRRN)
50
100
150
200
Iterations
0.50
0.55
0.60
0.65
0.70
MSE
(1) MovieLens 100K, K=10
GEE
GTT
GTTN
GRRN
50
100
150
200
Iterations
(2) MovieLens 100K, K=20
GEE
GTT
GTTN
GRRN
50
100
150
200
Iterations
(3) MovieLens 100K, K=30
GEE
GTT
GTTN
GRRN
50
100
150
200
Iterations
(4) MovieLens 100K, K=40
GEE
GTT
GTTN
GRRN
50
100
150
200
Iterations
(5) MovieLens 100K, K=50
GEE
GTT
GTTN
GRRN
(a) Convergence on the MovieLens 100K data set with increasing latent dimension K.
50
100
150
200
Iterations
0.550
0.575
0.600
0.625
0.650
0.675
MSE
(1) MovieLens 1M, K=10
GEE
GTT
GTTN
GRRN
50
100
150
200
Iterations
(2) MovieLens 1M, K=20
GEE
GTT
GTTN
GRRN
50
100
150
200
Iterations
(3) MovieLens 1M, K=30
GEE
GTT
GTTN
GRRN
50
100
150
200
Iterations
(4) MovieLens 1M, K=40
GEE
GTT
GTTN
GRRN
50
100
150
200
Iterations
(5) MovieLens 1M, K=50
GEE
GTT
GTTN
GRRN
(b) Convergence on the MovieLens 1M data set with increasing latent dimension K.
Figure 6.6: Convergence of the models on the MovieLens 100K (upper) and the MovieLens
1M (lower) data sets, measuring the training data ﬁt (mean squared error). When increasing
latent dimension K, the GRRN continues to increase the performance; while other models
start to decrease on the MovieLens 100K data set or stop increasing on the MovieLens 1M
data set.
Convergence analysis.
Firstly we compare the convergence in terms of iterations on the
MovieLens 100K and MovieLens 1M data sets. We run each model with K = 10, 20, 30, 40, 50,
and the loss is measured by mean squared error (MSE). Figure 6.6 shows the average con-
vergence results of ten repeats. On the MovieLens 1M data set, all the methods converge
to better performance with smaller MSE when increasing latent dimension K; while the
performance of GRRN is better than the other models. Moreover, we observe that the con-
vergence results of GTT and GTTN models are rather close since they share similar hidden
structures though GTTN is a hierarchical model. On the other hand, when conducting on
the MovieLen 100K data set and increasing the feature dimension K, the GRRN model
continues to converge to better performance with MSE continuing to decrease. However,
GEE, GTT, and GTTN models ﬁrst converge to a better performance and then start to
diverge with a larger MSE observed or stop improving at all when increasing latent dimen-
sion K. From this perspective, GRRN is a better choice for data reduction compared to
other Bayesian NMF models.
Noise sensitivity.
We further measure the noise sensitivity of diﬀerent models with pre-
dictive performance when the data sets are noisy. To see this, we add diﬀerent levels of Gaus-
sian noise to the data. We add levels of {0%, 10%, 20%, 50%, 100%, 200%, 500%, 1000%}
noise-to-signal ratio noise (which is the ratio of the variance of the added Gaussian noise to
the variance of the data). The results for the MovieLens 100K with K = 50 are shown in
160

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
Figure 6.7. We observe that the GRRN model performs similarly to other Bayesian NMF
models. Similar results can be found on the MovieLens 1M data set and other K values
and we shall not repeat the details.
0%
10%
20%
50%
100% 200% 500% 1000%
Noise added (noise-to-signal ratio)
1
2
Ratio data variance to error
GEE
GTT
GTTN
GRRN
Figure 6.7: Ratio of the variance of
data to the MSE of the predictions.
The higher the better. GRRN model
performs similarly to other Bayesian
NMF models. Similar results can be
found on the MovieLens 1M data set
and other K values and we shall not
repeat the details.
K\Models
GEE
GTT
GTTN
GRRN
K=20
1.18
1.06
1.07
1.02
K=30
1.43
1.18
1.20
1.00
K=40
1.86
1.42
1.45
0.98
K=50
2.63
1.84
1.89
0.97
K=20
3.47
1.46
1.57
1.10
K=30
6.86
2.27
2.52
1.05
K=40
17056.27
4.07
4.79
1.04
K=50
236750.39
2650.21
5452.18
1.05
Table 6.3: Mean squared error measure when 97%
(upper table) and 98% (lower table) of data is un-
observed for MovieLens 100K data set.
The per-
formance of the GRRN model is only a little worse
when increasing the fraction of unobserved from 97%
to 98%. Similar situations can be observed in the
MovieLens 1M experiment.
Predictive analysis.
The training performance of the GRRN model steadily improves as
the model complexity grows. Inspired by this result, we measure the predictive performance
when the sparsity of the data increases to see whether the models overﬁt or not. For diﬀerent
fractions of unobserved data, we randomly split the data based on that fraction, train the
model on the observed data, and measure the performance on the held-out test data. Again,
we increase K from K = 20 to K = 30, 40, 50 for all models. The average MSE of ten repeats
is given in Figure 6.8. We observe that when K = 20 and the fraction of unobserved data
is relatively a small value (e.g., fraction unobserved = 0.93 in Figure 6.8(a) and fraction
unobserved = 0.96 in Figure 6.8(b)), all the models perform similarly (GRRN is only slightly
better). However, when the fraction of unobserved data increases or the latent dimension
K increases, GRRN performs much better than the other models.
Table 6.3 shows MSE predictions of diﬀerent models when the fraction of unobserved
data is 97% and 98%.
We observe that the performance of the GRRN model is only
a little worse when increasing the fraction of unobserved from 97% to 98% showing the
GRRN model is more robust with less overﬁtting. While for other competitive models,
the performances become extremely worse in this scenario. From Figure 6.6, we see that
GEE can converge to a better in-sample performance generally; this leads to a worse out-
of-sample performance as shown in Figure 6.8 (compared to GTT and GTTN). However,
the GRRN has both better in-sample and out-of-sample performances from this experiment
making it a more robust choice in predicting missing entries. Similar situations can be
observed in the MovieLens 1M case.
We also add a popular non-probabilistic NMF (NP-NMF) model to see the predictive
results (Lee and Seung, 2000). Empirical results (grey lines in Figure 6.8) show that the
NP-NMF can overﬁt easily compared to Bayesian NMF approaches even when the fraction
161

6.7. PRIORS AS REGULARIZATION
0.93
0.94
0.95
0.96
0.97
0.98
Fraction unobserved
0.8
1.0
1.2
1.4
1.6
MSE
(0.97625)
(0.96415)
(1) MovieLens 100K, K=20
NP-NMF
GEE
GTT
GTTN
GRRN
0.93
0.94
0.95
0.96
0.97
0.98
Fraction unobserved
(1.02211)
(0.96271)
(2) MovieLens 100K, K=30
NP-NMF
GEE
GTT
GTTN
GRRN
0.93
0.94
0.95
0.96
0.97
0.98
Fraction unobserved
(1.09982)
(0.94340)
(3) MovieLens 100K, K=40
NP-NMF
GEE
GTT
GTTN
GRRN
0.93
0.94
0.95
0.96
0.97
0.98
Fraction unobserved
(1.22910)
(0.92862)
(4) MovieLens 100K, K=50
NP-NMF
GEE
GTT
GTTN
GRRN
(a) Predictive results on the MovieLens 100K data set with increasing fraction of unobserved data and
increasing latent dimension K.
0.96
0.97
0.98
0.99
Fraction unobserved
0.5
1.0
1.5
2.0
2.5
MSE
(0.89747)
(0.82102)
(1) MovieLens 1M, K=20
NP-NMF
GEE
GTT
GTTN
GRRN
0.96
0.97
0.98
0.99
Fraction unobserved
(1.16371)
(0.83592)
(2) MovieLens 1M, K=30
NP-NMF
GEE
GTT
GTTN
GRRN
0.96
0.97
0.98
0.99
Fraction unobserved
(1.47590)
(0.84232)
(3) MovieLens 1M, K=40
NP-NMF
GEE
GTT
GTTN
GRRN
0.96
0.97
0.98
0.99
Fraction unobserved
(2.24823)
(0.84234)
(4) MovieLens 1M, K=50
NP-NMF
GEE
GTT
GTTN
GRRN
(b) Predictive results on the MovieLens 1M data set with increasing fraction of unobserved data and
increasing latent dimension K.
Figure 6.8: Predictive results on the MovieLens 100K (upper) and MovieLens 1M (lower)
data sets with the least fractions of unobserved data being 0.928 and 0.953 respectively (see
Table 6.2 for the data description). We measure the predictive performance (mean squared
error) on a held-out data set for diﬀerent fractions of unobserved data. The blue and red
arrows compare the MSEs of GTTN and GRRN models when the fractions of unobserved
data are 0.96 and 0.98 respectively.
of unobserved data is relatively small and latent dimension K is small though the issue is
less severe in the MovieLens 1M data set.
6.7. Priors as Regularization
Denote the prior parameters as θ and follow Bayes’ rule, the posterior is proportional to
the product of likelihood and prior density:
p(θ | A) ∝p(A | θ) · p(θ),
such that the log-likelihood follows
log p(θ | A) = log p(A | θ) + log p(θ) + C1
= log
M,N
Y
m,n=1
N

amn | w⊤
mzn, σ2
+ log p(W , Z) + C2
= −1
2σ2

amn −w⊤
mzn
2
+ log p(W , Z) + C3,
where C1, C2, C3 are some constants. The ultimate equation is the sum of the negative
squared loss of the training ﬁt and a regularization term over the factored components
162

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
Conditional wmk
g
µmk (mean)
g
σ2
mk (variance)
GEE
T N(wmk| g
µmk, g
σ2
mk)

−λW
mk
+
1
σ2
PN
j=1 zkj
 amj −PK
i̸=k wmizij
 g
σ2
mk
σ2
PN
j=1 z2
kj
GL2
1
T N(wmk| g
µmk, g
σ2
mk)

−λW
k
PK
j̸=k wmj +
1
σ2
PN
j=1 zkj
 amj −PK
i̸=k wmizij
 g
σ2
mk
σ2
PN
j=1 z2
kj+σ2λW
k
GL2
2
T N(wmk| g
µmk, g
σ2
mk)

1
σ2
PN
j=1 zkj
 amj −PK
i̸=k wmizij
 g
σ2
mk
σ2
PN
j=1 z2
kj+σ2λW
k
GL∞
T N(wmk| g
µmk, g
σ2
mk)

−λW
k · 1(wmk)
+
1
σ2
PN
j=1 zkj
 amj −PK
i̸=k wmizij
 g
σ2
mk
σ2
PN
j=1 z2
kj
GL2
2,∞T N(wmk| g
µmk, g
σ2
mk)

−λW
k · 1(wmk)
+
1
σ2
PN
j=1 zkj
 amj −PK
i̸=k wmizij
 g
σ2
mk
σ2
PN
j=1 z2
kj+σ2λW
k
Table 6.4: Posterior conditional densities of wmk’s for GEE, GL2
1, GL2
2, GL∞, and GL2
2,∞
models. The diﬀerence is highlighted in red. The conditional densities of zkn’s are similar
due to their symmetry to wmk’s. T N(x|µ, τ −1) =
√τ
2π exp{−τ
2 (x−µ)2}
1−Φ(−µ√τ)
u(x) is a truncated-
normal (TN) density with zero density below x = 0 and renormalized to integrate to one.
µ and τ are known as the “parent” mean and “parent” precision. Φ(·) is the cumulative
distribution function of standard normal density N(0, 1).
W , Z.
The prior distributions of W , Z then act as a regularization that can prevent
the model from overﬁtting the data and increase the predictive performance. To be more
concrete, the regularizers on W can be categorized as follows:
L1 =
M
X
m=1
K
X
k=1
wmk,
L1/2
2
=
M
X
m=1
v
u
u
t
K
X
k=1
wmk,
L2
1 =
M
X
m=1
 K
X
k=1
wmk
!2
,
L2
2 =
M
X
m=1
K
X
k=1
w2
mk.
(6.20)
We note that the L2
2 norm 6 is equivalent to an independent Gaussian prior (GGG model);
the L1 norm is equivalent to a Laplace prior in the real-valued decomposition and is equiv-
alently to an exponential prior (GEE model) in nonnegative matrix factorization. In the
next sections, we will discuss some Bayesian nonnegative matrix factorization models that
arise from diﬀerent norms, the diﬀerence of the conditional posterior for latent variable
wMK IS summarized in Table 6.4. The conditional densities of zkn’s are similar due to their
symmetry to wmk’s.
6.8. Gaussian L2
1 Norm (GL2
1) Model
The Gaussian L2
1 Norm Model (GL2
1) model is proposed by Brouwer and Lio (2017) based
on the L2
1 norm in Equation (6.20) for both W , Z. We again view the data A as being
produced according to the probabilistic generative process shown in Figure 6.9. The (m, n)-
th entry amn follows a Gaussian likelihood with variance σ2 and mean given by the latent
decomposition w⊤
mzn (Equation (6.1)).
6. To abuse the terminology, we call it a norm though it does not meet the criteria of a norm. A norm
should satisfy nonnegativity (∥A∥≥0), positive homogeneity (∥λA∥= |λ|·∥A∥), and triangle inequality
(∥A + B∥≤∥A∥+ ∥B∥) given matrices A, B and a scalar λ; see Lu (2021b).
163

6.8. GAUSSIAN L2
1 NORM (GL2
1) MODEL
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
ατ/ασ
βτ/βσ
τ/σ2
λW
k
λZ
k
Figure 6.9: Graphical model representation of GL2
1, GL2
2, GL∞, and GL2
2,∞models. Green
circles denote prior variables, orange circles represent observed and latent variables, and
plates represent repeated variables. The slash “/” in the variable represents “or”.
Prior.
The L2
1 prior follows immediately by replacing the L1 norm with the L2
1 norm in
the exponential prior. We assume W and Z are independently distributed with parameter
λW
k
and λZ
k proportional to an exponential function:
p(W | λW
k )
∝







exp

−λW
k
2
M
X
m=1
 K
X
k=1
wmk
!2
,
if wmk ≥0 for all m, k ;
0,
if otherwise;
p(Z | λZ
k )
∝







exp

−λZ
k
2
N
X
n=1
 K
X
k=1
zkn
!2
,
if zkn ≥0 for all n, k ;
0,
if otherwise.
(6.21)
Again, the prior for the noise variance σ2 = 1
τ is an inverse-Gamma density with shape ασ
and scale βσ.
Posterior.
Again, following the Bayes’ rule and MCMC, this means we need to be able
to draw from distributions (by Markov blanket, Section 5.2, p. 129):
p(wmk | A, W−mk, Z, σ2, λW
k , λZ
k ),
p(zkn | A, W , Z−kn, σ2, λW
k , λZ
k ),
p(σ2 | A, W , Z, ασ, βσ),
where W−mk denotes all elements of W except wmk and Z−kn denotes all elements of Z
except zkn. Using Bayes’ theorem, the conditional density of wmk depends on its parents
(λW
k ), children (amn), and coparents (τ or σ2, W−mk, Z) 7. Then, the conditional density
7. See Figure 6.9 and Section 5.2 (p. 129).
164

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
of wmk can be obtained by
p(wmk|A, W−mk, Z, σ2, λW
k ) ∝p(A|W , Z, σ2) · p(W |λW
k ) =
M,N
Y
i,j=1
N
 aij|w⊤
i zj, σ2
· p(W |λW
k )
∝exp


−1
2σ2
M,N
X
i,j=1
(aij −w⊤
i zj)2


× exp





−λW
k
2
M
X
i=1


K
X
j=1
wij


2




· u(wmk)
∝exp


−1
2σ2
N
X
j=1
(amj −w⊤
mzj)2


× exp





−λW
k
2

wmk +
K
X
j̸=k
wmj


2




· u(wmk)
∝exp
(
−
PN
j=1 z2
kj + σ2λW
k
2σ2

|
{z
}
1/(2 g
σ2
mk)
w2
mk + wmk

−λW
k
K
X
j̸=k
wmj +
N
X
j=1
zkj
σ2
 amj −
K
X
i̸=k
wmizij

|
{z
}
g
σ2
mk
−1 g
µmk
)
u(wmk)
∝N(wmk | g
µmk, g
σ2
mk) · u(wmk) = T N(wmk | g
µmk, g
σ2
mk),
(6.22)
where u(x) is the unit function with value 1 if x ≥0 and value 0 if x < 0, g
σ2
mk =
σ2
PN
j=1 z2
kj+σ2λW
k
is the “parent” posterior variance of the normal distribution,
g
µmk =


−λW
k ·
K
X
j̸=k
wmj + 1
σ2
N
X
j=1
zkj

amj −
K
X
i̸=k
wmizij


· g
σ2
mk
is the “parent” posterior mean of the normal distribution, and T N(x | µ, σ2) is the
truncated-normal density with “parent” mean µ and “parent” variance σ2 (Deﬁnition 2.4.1,
p. 50). Note the posterior density of wmk in Equation (6.22) is very similar to that of the
GEE model in Equation (6.5) where we highlight the diﬀerence in red text. See also the
comparison of conditional posteriors for wmk in Table 6.4.
Connection between GEE and GL2
1 models
We observe that there is an extra term in the denominator of the “parent” variance
value such that when all else are held equal, the GL2
1 has a smaller variance and the
distribution is more clustered in a smaller range. This is actually a stronger constrain-
t/regularizer than the GEE model.
Moreover, when {λW
mk} in the GEE model and {λW
k } in the GL2
1 model are equal
(see Table 6.4), the extra term PK
j̸=k wmj in the GL2
1 model plays an important role
in controlling the sparsity of factored components in the NMF context. To be more
concrete, when the distribution of elements in matrix A has a large portion of big
values, the extra term PK
j̸=k wmj will be larger than 1 and thus enforce the posterior
“parent” mean g
µmk of the truncated-normal density to be a small positive or even a
negative value. This in turn constraints the draws of T N(wmk|·) to be around zero thus
favoring sparsity (see the expectation of the variable of a truncated-normal distribution
for diﬀerent “parent” mean values in Figure 2.8(a), p. 52, the smaller the “parent” mean
165

6.9. GAUSSIAN L2
2 NORM (GL2
2) AND GAUSSIAN L∞NORM (GL∞) MODELS
value g
µmk, the smaller the expectation of the truncated-normal distributed variable
wmk; see also the example in Section 6.9 for the experiment on GDSC IC50 data set).
On the contrary, when the entries in matrix A are small, this extra term will be smaller
than 1, the parameter λW
k
has a little impact on the posterior “parent” mean g
µmk
which will possibly be a large value, and the factored component W or Z will be dense
instead (also see Section 6.9 for the experiment on Gene Body Methylation data set).
In this sense, the drawback of the GL2
1 model is revealed that it is not consistent
and not robust for diﬀerent types of the matrix A. In contrast, the GL2
2 and GL2
2,∞
models in the next section are consistent and robust for diﬀerent matrix types and
impose a larger regularization compared with the GEE model such that its predictive
performance is better (when the data matrix A has large values).
Or after rearrangement, the posterior density of wmk can be equivalently described by a
rectiﬁed-normal density (Deﬁnition 2.4.4, p. 54), and we shall not repeat the details.
And again, due to symmetry, a similar expression for zkn can be easily derived. The
conditional density of σ2 in GL2
1 is the same as that in Equation (6.8)
Algorithm 16 Gibbs sampler for GL2
1 model in one iteration (prior on variance σ2 here,
similarly for the precision τ). The procedure presented here may not be eﬃcient but is
explanatory. A more eﬃcient one can be implemented in a vectorized manner. By default,
uninformative priors are ασ = βσ = 1, {λW
k } = {λZ
k } = 0.1.
Require: Choose initial ασ, βσ, {λW
k }, {λZ
k };
1: for k = 1 to K do
2:
for m = 1 to M do
3:
Sample wmk from p(wmk | A, W−mk, Z, σ2, λW
k );
▷Equation (6.22)
4:
end for
5:
for n = 1 to N do
6:
Sample zkn from p(zkn | A, W , Z−kn, σ2, λZ
k );
▷Symmetry of Equation (6.22)
7:
end for
8: end for
9: Sample σ2 from p(σ2 | A, W , Z, ασ, βσ);
▷Equation (6.8)
10: Report loss in Equation (6.1), stop if it converges.
Gibbs sampling.
By this Gibbs sampling method introduced in Section 1.3.3 (p. 20),
we can construct a Gibbs sampler for the GL2
1 model as formulated in Algorithm 16. And
also in practice, all the parameters of the prior distribution are set to be the same value
λ = {λW
k } = {λZ
k }. By default, uninformative priors are ασ = βσ = 1, {λW
k } = {λZ
k } = 0.1.
6.9. Gaussian L2
2 Norm (GL2
2) and Gaussian L∞Norm (GL∞) Models
After the development of the GL2
1 model, more exploration of the behaviors for diﬀer-
ent “norms” are done in Lu and Chai (2022).
The Lp prior relies highly on the im-
plicit regularization in the GL2
1 model. For any vector x ∈Rn, the Lp norm is given by
Lp(x) = (Pn
i=1 |xi|p)1/p whose unit balls in 2-dimensional space and 3-dimensional space
166

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
are shown in Figure 3.4 (p. 106) and Figure 3.5 (p. 106) respectively.
The norms of a
vector are quite useful in machine learning. In Chapter 3 (p. 88), we mentioned the least
squares problem is to minimize the squared distance between observation b and expected
observation Ax: ∥Ax −b∥2
2, i.e., the L2 norm of Ax −b. On the other hand, minimizing
the L1 norm between the observation and the expected observation can result in a robust
estimator of x (Zoubir et al., 2012). While the Lp norm over the matrix W ∈RM×K can
be deﬁned as
Lp =
M
X
m=1
 K
X
k=1
|wmk|p
!1/p
.
(6.23)
In the context of NMF, the L1 norm (for the GEE model) in Equation (6.20) can be regarded
as an Lp norm with p = 1 since {wmk}’s are nonnegative. The L1 norm is known to have
a sparse constraint (see discussion in Section 6.2). We can further extend the Bayesian
models with L2
2 and L∞norms. Again, we view the data A as being produced according to
the probabilistic generative process shown in Figure 6.9, the same graphical model as the
GL2
1 model. The (m, n)-th entry amn follows a Gaussian likelihood with variance σ2 and
mean given by the latent decomposition w⊤
mzn (Equation (6.1)). Therefore, the posterior
density of the Gaussian variance parameter σ2, given an inverse-Gamma prior with shape
ασ and scale βσ parameters, is the same as the GEE model in Equation (6.8).
Prior for the GL2
2 model.
Based on the L2 norm, we assume W and Z are indepen-
dently distributed with parameters λW
k
and λZ
k proportional to an exponential function:
p(W | λW
k )
∝







exp
"
−λW
k
2
M
X
m=1
 K
X
k=1
w2
mk
!#
,
if wmk ≥0 for all m, k ;
0,
if otherwise;
p(Z | λZ
k )
∝







exp
"
−λZ
k
2
N
X
n=1
 K
X
k=1
z2
kn
!#
,
if zkn ≥0 for all n, k ;
0,
if otherwise.
(6.24)
Posterior for the GL2
2 model.
By Bayes rule (Equation (1.1), p. 16), the posterior is
proportional to the product of likelihood and prior, it can be maximized to yield an estimate
of W and Z. Using Bayes’ theorem, the conditional density of wmk depends on its parents
(λW ), children (amn), and coparents (τ or σ2, W−mk, Z) 8. And it can be obtained by
8. See Figure 6.9 and Section 5.2 (p. 129).
167

6.9. GAUSSIAN L2
2 NORM (GL2
2) AND GAUSSIAN L∞NORM (GL∞) MODELS
p(wmk | A, W−mk, Z, σ2, λW
k )
∝p(A | W , Z, σ2) × p(W | λW
k ) =
M,N
Y
i,j=1
N
 aij | w⊤
i zj, σ2
× p(W | λW
k ) · u(wmk)
∝exp


−1
2σ2
M,N
X
i,j=1
(aij −w⊤
i zj)2


× exp


−λW
k
2
M
X
i=1


K
X
j=1
w2
ij




· u(wmk)
∝exp


−1
2σ2
N
X
j=1
(amj −w⊤
mzj)2


× exp

−λW
k
2 w2
mk

· u(wmk)
∝exp
(
−
 PN
j=1 z2
kj + σ2λW
k
2σ2
!
|
{z
}
1/(2 g
σ2
mk)
w2
mk + wmk

1
σ2
N
X
j=1
zkj

amj −
K
X
i̸=k
wmizij


|
{z
}
g
σ2
mk
−1 g
µmk
)
· u(wmk)
∝N(wmk | g
µmk, g
σ2
mk) · u(wmk) = T N(wmk | g
µmk, g
σ2
mk),
(6.25)
where g
σ2
mk =
σ2
PN
j=1 z2
kj+σ2λW
k
is the posterior variance of the normal distribution, and
g
µmk =



1
σ2
N
X
j=1
zkj

amj −
K
X
i̸=k
wmizij


· g
σ2
mk
is the posterior mean of the normal distribution, and T N(x | µ, σ2) is the truncated-normal
density with “parent” mean µ and “parent” variance σ2 (Deﬁnition 2.4.1, p. 50). Note again
the posterior density of wmk in Equation (6.25) is very similar to that of the GEE model
in Equation (6.5) where we highlight the diﬀerence in red text. See also the comparison of
conditional posteriors for wmk in Table 6.4.
Connection between GEE, GL2
1, and GL2
2 models
We observe that the posterior “parent” mean g
µmk in the GL2
2 model is larger than that in
the GEE model since it does not contain the negative term −λW
mk (see Table 6.4). While
the posterior “parent” variance is smaller than that in the GEE model, such that the
conditional density of GL2
2 model is more clustered and it imposes a larger regularization
in the sense of data/entry distribution (see Figure 2.8(a), p. 52, the smaller the “parent”
variance of the truncated-normal distribution, the larger the “parent” precision, and
the smaller the expectation of the truncated-normal variable). This can induce sparsity
in the context of nonnegative matrix factorization. Moreover, the GL2
2 does not have
the extra term PK
j̸=k wmj in the GL2
1 model which causes the inconsistency for diﬀerent
types of matrix A such that the introduced GL2
2 model is more robust.
Prior for the GL∞model.
When p →∞, the Lp norm deﬁned over W is
L∞=
M
X
m=1
 K
X
k=1
|wmk|∞
!1/∞
=
M
X
m=1
max
k
|wmk| .
(6.26)
168

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
Based on the Lp norm. we assume W and Z are independently exponentially distributed
with scales λmk and λkn (Deﬁnition 2.3.1, p. 49),
p(W | λW
k )
∝







exp
"
−λW
k
M
X
m=1
max
k
|wmk|
#
,
if wmk ≥0 for all m, k ;
0,
if otherwise;
p(Z | λZ
k )
∝







exp
"
−λZ
k
N
X
n=1
max
k
|zkn|
#
,
if zkn ≥0 for all n, k ;
0,
if otherwise.
(6.27)
Note we remove the 2 in the denominator of λW
k
for consistency issues which we will see
shortly in the form of the conditional density in Equation (6.28).
Posterior for GL∞model.
By Bayes’ rule (Equation (1.1), p. 16), the posterior is
proportional to the product of likelihood and prior, it can be maximized to yield an estimate
of W and Z. Using Bayes’ theorem, the conditional density of wmk depends on its parents
(λW
k ), children (amn), and coparents (τ or σ2, W−mk, Z) 9. Denote 1(wmk) as the indicator
whether wmk is the largest one for k = 1, 2, . . . , K, the conditional density can be obtained
by
p(wmk | A, W−mk, Z, σ2, λW
k )
∝p(A | W , Z, σ2) × p(W | λW
k ) =
M,N
Y
i,j=1
N
 aij | w⊤
i zj, σ2
× p(W | λW
k ) · u(wmk)
∝exp


−1
2σ2
M,N
X
i,j=1
(aij −w⊤
i zj)2


× exp
(
−λW
k ·
M
X
i=1
max
k
|wij|
)
· u(wmk)
∝exp


−1
2σ2
N
X
j=1
(amj −w⊤
mzj)2


× exp

−λW
k · wmk
	
· u(wmk) · 1(wmk)
∝exp


−1
2σ2
N
X
j=1

w2
mkz2
kj + 2wmkzkj
 K
X
i̸=k
wmizij −amj


· exp

−wmkλW
k 1(wmk)
	
· u(wmk)
∝exp
(
−
 PN
j=1 z2
kj
2σ2
!
|
{z
}
1/(2 g
σ2
mk)
w2
mk + wmk

−λW
k 1(wmk) + 1
σ2
N
X
j=1
zkj

amj −
K
X
i̸=k
wmizij

|
{z
}
g
σ2
mk
−1 g
µmk
)
· u(wmk)
∝N(wmk | g
µmk, g
σ2
mk) · u(wmk) = T N(wmk | g
µmk, g
σ2
mk),
(6.28)
9. See Figure 6.9 and Section 5.2 (p. 129).
169

6.9. GAUSSIAN L2
2 NORM (GL2
2) AND GAUSSIAN L∞NORM (GL∞) MODELS
where u(x) is the unit function with value 1 if x ≥0 and value 0 if x < 0, g
σ2
mk =
σ2
PN
j=1 z2
kj
is the posterior “parent” variance of the normal distribution, and
g
µmk =


−λW
k · 1(wmk) + 1
σ2
N
X
j=1
zkj

amj −
K
X
i̸=k
wmizij


· g
σ2
mk
is the posterior “parent” mean of the normal distribution, and T N(x | µ, σ2) is the truncated
normal density with “parent” mean µ and “parent” variance σ2 (Deﬁnition 2.4.1, p. 50).
Connection between GEE and GL∞models
The posterior “parent” variance g
σ2
mk in the GL∞model is exactly the same as that in
the GEE model (see Table 6.4). Denote 1(wmk) as the indicator whether wmk is the
largest one among k = 1, 2, . . . , K. Suppose further the condition 1(wmk) is satisﬁed,
parameters {λW
mk} in the GEE model and {λW
k } in GL∞model are equal, the “parent”
mean g
µmk is the same as that in the GEE model as well. However, when wmk is not the
maximum value among {wm1, wm2, . . . , wmK}, the “parent” mean g
µmk is larger than
that in the GEE model since the GL∞model excludes this negative term. The GL∞
model then has the interpretation that it has a sparsity constraint when wmk is the
maximum value; and it has a relatively loose constraint when wmk is not the maximum
value. Overall, the GL∞favors a loose regularization compared with the GEE model.
Further extension: GL2
2,∞model.
The GL2
2,∞model takes the advantages of both
GL2
2 and GL∞, and the posterior parameters of GL2
2,∞are shown in Table 6.4. The implicit
prior of the GL2
2,∞model can be obtained by
p(W | λW
k ) ∝exp
(
−λW
k
2
M
X
m=1
 K
X
k=1
w2
mk + 2 max
k
|wmk|
)
u(W ).
(6.29)
Computational complexity and Gibbs sampler.
The adopted Gibbs sampling meth-
ods for GEE, GL2
1, GL2
2, GL∞, and GL2
2,∞models have complexity O(MNK2), where the
most expensive operation is the update on the conditional density of wmk’s and zkn’s. The
Gibbs sampler for the above models is formulated in Algorithm 17. By default, uninforma-
tive priors are {λW
k } = {λZ
k } = 0.1 (GL2
1, GL2
2, GL∞, GL2
2,∞); ασ = βσ = 1 (inverse-Gamma
prior in GL2
1, GL2
2, GL∞, GL2
2,∞).
6.9.1 Examples
We conduct experiments with various analysis tasks to demonstrate the main advantages
of the introduced GL2
2 and GL2
2,∞methods. We use two data sets from bioinformatics:
The ﬁrst one is the Genomics of Drug Sensitivity in Cancer data set10 (GDSC IC50) (Yang
et al., 2012), which contains a wide range of drugs and their treatment outcomes on diﬀerent
cancer and tissue types (cell lines). Following Brouwer and Lio (2017), we preprocess the
10. https://www.cancerrxgene.org/
170

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
Algorithm 17 Gibbs sampler for GL2
1, GL2
2, and GL∞models (prior on variance σ2 here,
similarly for the precision τ). The procedure presented here is for explanatory purposes,
and vectorization can expedite the procedure. By default, uninformative priors are {λW
k } =
{λZ
k } = 0.1 (GL2
1, GL2
2, GL∞, GL2
2,∞); ασ = βσ = 1 (inverse-Gamma prior in GL2
1, GL2
2,
GL∞, GL2
2,∞).
1: for k = 1 to K do
2:
for m = 1 to M do
3:
Sample wmk from p(wmk|·) = T N(wmk| g
µmk, g
σ2
mk) from Table 6.4;
4:
end for
5:
for n = 1 to N do
6:
Sample zkn from p(zkn|·) = T N(wmk|g
µkn, g
σ2
kn);
▷symmetry of wmk
7:
end for
8: end for
9: Sample σ2 from p(σ2 | A, W , Z, ασ, βσ);
▷Equation (6.8)
10: Report loss in Equation (6.1), stop if it converges.
GDSC IC50 data set by capping high values to 100, undoing the natural log transform, and
casting them as integers. The second one is the Gene Body Methylation data set (Koboldt
et al., 2012), which gives the amount of methylation measured in the body region of 160
breast cancer driver genes. We multiply the values in the Gene Body Methylation data set
by 20 and cast them as integers as well. A summary of the two data sets can be seen in
Table 6.5 and their distributions are shown in Figure 6.10. The GDSC IC50 data set has a
larger range whose values are unbalanced (either small as 0 or large as 100); while the Gene
Body Methylation data set has a smaller range whose values seem balanced. We can see
that the GDSC IC50 is relatively a large data set whose matrix rank is 139 and the Gene
Body Methylation data tends to be small whose matrix rank is 160.
0
25
50
75
10
GDSC IC50
5
10
15
20
Gene body methylation
Figure 6.10:
Data distribution of
GDSC IC50 and Gene Body Methy-
lation data sets.
Dataset
Rows
Columns
Fraction obs.
GDSC IC50
707
139
0.806
Gene Body Meth.
160
254
1.000
Table 6.5: Dataset description. Gene Body Methy-
lation is relatively a small data set and the GDSC
IC50 tends to be large. The description provides the
number of rows, columns, and the fraction of entries
that are observed.
The same parameter initialization is adopted in each scenario. We compare the results
in terms of convergence speed and generalization. In a wide range of scenarios across various
models, GL2
2 and GL2
2,∞improve convergence rates, and lead to out-of-sample performance
that is as good or better than other Bayesian NMF models with implicit regularization
meaning.
Hyperparameters.
We follow the default hyperparameter setups in Brouwer and Lio
(2017). We use {λW
mk} = {λZ
kn} = 0.1 (GEE); {λW
k } = {λZ
k } = 0.1 (GL2
1, GL2
2, GL2
2,∞);
171

6.9. GAUSSIAN L2
2 NORM (GL2
2) AND GAUSSIAN L∞NORM (GL∞) MODELS
uninformative ασ = βσ = 1 (inverse-Gamma prior in GEE, GL2
1, GL2
2, GL2
2,∞). These are
very weak prior choices and the models are insensitive to them (Brouwer and Lio, 2017).
As long as the hyperparameters are set, the observed or unobserved variables are initialized
from random draws as this initialization procedure provides a better initial guess of the right
patterns in the matrices. In all experiments, we run the Gibbs sampler 500 iterations with
a burn-in of 300 iterations as the convergence analysis shows the algorithm can converge in
fewer than 200 iterations.
50
100
150
200
Iterations
500
600
700
800
MSE
(1) GDSC IC50, K=10
50
100
150
200
Iterations
(2) GDSC IC50, K=20
50
100
150
200
Iterations
(3) GDSC IC50, K=30
50
100
150
200
Iterations
(4) GDSC IC50, K=40
0
100
200
300
Iterations
(5) GDSC IC50, K=50
GEE
GL2
2
GL2
2,
GL2
1
(a) Convergence on the GDSC IC50 data set with increasing latent dimension K.
10
20
30
Values
0.00
0.05
0.10
0.15
0.20
Probability
(1) GDSC IC50, K=10
10
20
30
Values
(2) GDSC IC50, K=20
10
20
30
Values
(3) GDSC IC50, K=30
10
20
30
Values
(4) GDSC IC50, K=40
10
20
30
Values
(5) GDSC IC50, K=50
GEE
GL2
2
GL2
2,
GL2
1
(b) Data distribution of factored component W in the last 20 iterations for GDSC IC50.
Figure 6.11: Convergence of the models on the GDSC IC50 (upper) and the distribution
of factored W (lower), measuring the training data ﬁt (mean squared error). When we
increase the latent dimension K, the GEE and the introduced GL2
2 and GL2
2,∞algorithms
continue to increase the performance; while GL2
1 starts to decrease. The results of GL∞
and GL2
2,∞models are similar so we only present the results of the GL2
2,∞model for brevity.
Convergence analysis for GDSC IC50 with relatively large entries.
Firstly we
compare the convergence in terms of iterations on the GDSC IC50 and Gene Body Methy-
lation data sets. We run each model with K = {10, 20, 30, 40, 50}, and the loss is measured
by mean squared error (MSE). Figure 6.11(a) shows the average convergence results of ten
repeats and Figure 6.11(b) shows the distribution of entries of the factored W for the last
20 iterations on the GDSC IC50 data set. The result is consistent with our analysis (see the
connection between diﬀerent models). Since the values of the data matrix for GDSC IC50
data set is large, the posterior “parent” mean g
µmk in the GL2
1 model is approaching zero
or even negative, thus it has a larger regularization than GEE model. This makes the GL2
1
model converge to a worse performance. GL2
2 and GL2
2,∞models, on the contrary, impose
a looser regularization than the GL2
1 model, and the convergence performances are close to
that of the GEE model.
Convergence analysis for Gene Body Methylation with relatively small en-
tries.
Figure 6.12(a) further shows the average convergence results of ten repeats, and
Figure 6.12(b) shows the distribution of the entries of the factored W for the last 20 it-
172

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
50
100
150
200
Iterations
0.25
0.50
0.75
1.00
1.25
MSE
(1) GB Meth., K=10
50
100
150
200
Iterations
(2) GB Meth., K=20
50
100
150
200
Iterations
(3) GB Meth., K=30
50
100
150
200
Iterations
(4) GB Meth., K=40
0
100
200
300
Iterations
(5) GB Meth., K=50
GEE
GL2
2
GL2
2,
GL2
1
(a) Convergence on the Gene Body Methylation data set with increasing latent dimension K.
0
1
2
3
4
Values
0.0
0.2
0.4
0.6
0.8
Probability
(1) GB Meth., K=10
0
1
2
3
4
Values
(2) GB Meth., K=20
0
1
2
3
4
Values
(3) GB Meth., K=30
0
1
2
3
4
Values
(4) GB Meth., K=40
0
1
2
3
4
Values
(5) GB Meth., K=50
GL2
1
GL2
2,
GL2
2
GEE
(b) Data distribution of factored component W in the last 20 iterations for Gene Body Methylation.
Figure 6.12: Convergence of the models on the Gene Body Methylation data set (upper)
and the distribution of factored W (lower), measuring the training data ﬁt (mean squared
error). When we increase the latent dimension K, all the models continue to increase the
performance.
erations on the Gene Body Methylation data set. The situation is diﬀerent for the GL2
1
model since the range of the entries of the Gene Body Methylation data set is smaller than
that of the GDSC IC50 data set (see Figure 6.10). This makes the −λW
k · PK
j̸=k wmj term
of posterior “parent” mean g
µmk in the GL2
1 model approach zero (see Table 6.4), and the
model then favors a looser regularization than the GEE model.
The situation can be further presented by the distribution of the factored component
W on the GDSC IC50 (Figure 6.11(b)) and the Gene Body Methylation (Figure 6.12(b)).
The GEE model has larger values of W on the former data set and smaller values on the
latter; while GL2
1 has smaller values of W on the former data set and larger values on
the latter. In other words, the regularization of the GEE and GL2
1 is inconsistent on the
two diﬀerent data matrices. In comparison, the introduced GL2
2 and GL2
2,∞are consistent
on diﬀerent data sets, making them more robust algorithms to compute the nonnegative
matrix factorization of the observed data.
Table 6.6 shows the mean values of the factored component W in the last 20 iterations
for GDSC IC50 (upper table) and Gene Body Methylation (lower table) where the value in
the parentheses is the sparsity evaluated by taking the percentage of values smaller than
0.1. The inconsistency of GEE for diﬀerent matrices can be observed (either large sparsity
or small sparsity), while the results for the introduced GL2
2 and GL2
2,∞models are more
consistent.
Predictive analysis.
The training performances of the GEE, GL2
2, and GL2
2,∞models
steadily improve as the model complexity grows. Inspired by this result, we measure the
predictive performance when the sparsity of the data increases to see whether the models
overﬁt or not. For diﬀerent fractions of unobserved data, we randomly split the data based
173

6.9. GAUSSIAN L2
2 NORM (GL2
2) AND GAUSSIAN L∞NORM (GL∞) MODELS
K
GEE
GL2
1
GL2
2
GL2
2,∞
10 8.1 (1.9)
1.3 (10.3) 2.4 (3.8)
2.4 (4.5)
20 8.6 (1.5)
0.8 (14.7) 2.3 (4.1)
2.2 (4.4)
30 8.7 (1.4)
0.7 (17.3) 2.2 (4.3)
2.2 (4.4)
40 8.3 (1.5)
0.6 (19.4) 2.2 (4.4)
2.2 (4.4)
50 8.0 (1.6)
0.5 (21.2) 2.2 (4.1)
2.2 (4.2)
10 0.1 (80.4) 0.7 (11.4) 0.7 (11.5) 0.7 (12.7)
20 0.1 (87.8) 0.6 (16.2) 0.5 (21.3) 0.5 (21.0)
30 0.0 (90.2) 0.6 (18.2) 0.3 (37.1) 0.3 (36.4)
40 0.0 (92.2) 0.6 (20.8) 0.3 (48.9) 0.3 (49.1)
50 0.0 (93.0) 0.5 (22.8) 0.2 (58.4) 0.2 (58.4)
Table 6.6: Mean values of the fac-
tored component W in the last 20
iterations, where the value in the
(parentheses) is the sparsity eval-
uated by taking the percentage of
values smaller than 0.1, for GDSC
IC50 (upper table) and Gene Body
Methylation (lower table). The in-
consistency of GEE and GL2
1 for dif-
ferent matrices can be observed.
Unobs.
K
GEE
GL2
1
GL2
2
GL2
2,∞
60%
20
787.60
880.36
769.24
768.27
30
810.39
888.47
774.53
773.27
40
802.39
892.01
783.26
784.30
50
795.72
895.05
806.14
807.44
70%
20
841.74
895.77
798.44
796.15
30
830.45
902.48
807.37
806.61
40
842.70
907.65
832.67
835.89
50
846.83
1018.97 ↑
864.58
869.15
80%
20
904.39
926.72
842.24
841.84
30
887.63
938.92
879.30
883.57
40
942.44
2634.69
935.09
939.77
50
952.45
2730.30 ↑
974.01
973.75
Table 6.7: Mean squared error measure when the
percentage of unobserved data is 60% (upper table),
70% (middle table), or 80% (lower table) for the
GDSC IC50 data set. The performance of the intro-
duced GL2
2 and GL2
2,∞models is only slightly worse
when we increase the fraction of unobserved from
60% to 80%; while the performance of GL2
1 becomes
extremely poor. Similar observations occur in the
Gene Body Methylation experiment. The symbol ↑
means the performance becomes extremely worse.
on that fraction, train the model on the observed data, and measure the performance on the
held-out test data. Again, we increase K from K = 20 to K = 30, 40, 50 for all models. The
average MSE of ten repeats is given in Figure 6.13. We still observe the inconsistency issue
in the GL2
1 model, the predictive performance of it is as good as that of the introduced GL2
2
and GL2
2,∞models on the Gene Body Methylation data set; while the predictive results of
the GL2
1 model are extremely poor on the GDSC IC50 data set.
For the GDSC IC50 data set, the introduced GL2
2 and GL2
2,∞models perform best when
the latent dimensions are K = 20, 30, 40; when K = 50 and the fraction of unobserved
data increases, the GEE model is slightly better. As aforementioned, the GL2
1 performs the
worst on this data set; and when the fraction of unobserved data increases or K increases,
the predictive results of GL2
1 deteriorate quickly.
For the Gene Body Methylation data set, the predictive performance of GL2
1, GL2
2 and
GL2
2,∞models are close (GL2
1 has a slightly larger error). The GEE model performs the
worst on this data set.
The comparison of the results on the two sets shows the introduced GL2
2 and GL2
2,∞
models have both better in-sample and out-of-sample performance, making them a more
robust choice in predicting missing entries.
Table 6.7 shows MSE predictions of diﬀerent models when the fractions of unobserved
data are 60%, 70%, and 80% respectively. We observe that the performances of the in-
174

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
0.3
0.4
0.5
0.6
0.7
0.8
Fraction unobserved
700
800
900
1000
1100
1200
MSE
(1) GDSC IC50, K=20
0.3
0.4
0.5
0.6
0.7
0.8
Fraction unobserved
(2) GDSC IC50, K=30
0.3
0.4
0.5
0.6
0.7
0.8
Fraction unobserved
(3) GDSC IC50, K=40
0.3
0.4
0.5
0.6
0.7
0.8
Fraction unobserved
(4) GDSC IC50, K=50
GEE
GL2
1
GL2
2
GL2
2,
(a) Predictive results on the GDSC IC50 data set with increasing fraction of unobserved data and increasing
latent dimension K.
0.3
0.4
0.5
0.6
0.7
0.8
Fraction unobserved
2.0
2.2
2.4
2.6
2.8
3.0
MSE
(1) GB Meth., K=20
0.3
0.4
0.5
0.6
0.7
0.8
Fraction unobserved
(2) GB Meth., K=30
0.3
0.4
0.5
0.6
0.7
0.8
Fraction unobserved
(3) GB Meth., K=40
0.3
0.4
0.5
0.6
0.7
0.8
Fraction unobserved
(4) GB Meth., K=50
GEE
GL2
1
GL2
2
GL2
2,
(b) Predictive results on Gene Body Methylation data set with increasing fraction of unobserved data
and increasing latent dimension K.
Figure 6.13: Predictive results on the GDSC IC50 (upper) and Gene Body Methy-
lation (lower) data sets. We measure the predictive performance (mean squared error) on
a held-out data set for diﬀerent fractions of unobserved data.
troduced GL2
2 and GL2
2,∞models are only slightly worse when we increase the fraction
of unobserved from 60% to 80%. This indicates the introduced GL2
2 and GL2
2,∞models
are more robust with less overﬁtting. While for the GL2
1 model, the performance becomes
extremely poor in this scenario.
Noise sensitivity.
Finally, we measure the noise sensitivity of diﬀerent models with pre-
dictive performance when the data sets are noisy. To see this, we add diﬀerent levels of
Gaussian noise to the data. We add levels of {0%, 10%, 20%, 50%, 100%} noise-to-signal
ratio noise (which is the ratio of the variance of the added Gaussian noise to the variance
of the data).
The results for the GDSC IC50 with K = 10 are shown in Figure 6.14.
The results are the average performance over 10 repeats. We observe that the introduced
GL2
2 and GL2
2,∞models perform slightly better than other Bayesian NMF models. The
introduced GL2
2 and GL2
2,∞models perform notably better when the noise-to-signal ratio
is smaller than 10% and slightly better when the ratio is larger than 20%. Similar results
can be found on the Gene Body Methylation data set and other K values and we shall not
repeat the details.
6.10. Semi-Nonnegative Matrix Factorization
Instead of forcing nonnegativity on both factored matrices, we can place this constraint
on only one of them (Ding et al., 2008; Fei et al., 2008).
By the Bayesian approach,
we can place a real-valued prior over one component, and a nonnegative prior over the
175

6.10. SEMI-NONNEGATIVE MATRIX FACTORIZATION
0%
10%
20%
50%
100%
Noise added (noise-to-signal ratio)
1.250
1.717
2.183
2.650
Ratio data variance to error
GEE
GL2
1
GL2
2
GL2
2,
Figure 6.14: Ratio of the variance of data to the MSE of the predictions, the higher the
better.
other. As discussed above, the NMF works for nonnegative data sets, e.g., data sets derived
from images and texts. Nevertheless, the main advantage of the semi-nonnegative matrix
factorization is that it allows us to handle real-valued data sets while still enforcing some
nonnegative constraints.
6.10.1 Gaussian Likelihood with Exponential and Gaussian Priors (GEG)
The Gaussian likelihood with exponential and Gaussian priors (GEG) model place an expo-
nential prior over the component W and a Gaussian prior over the component Z as those
were done in GEE and GGG models respectively (Section 6.2 and Section 5.2, p. 129).
The likelihood is chosen to be the same as that in the GEE model (Equation (6.2)). The
graphical representation of GEG model is shown in Figure 6.15(a).
Prior.
We assume W is independently exponentially distributed with scales λW
mk, and Z
is Gaussian distributed with precisions λZ
kn,
wmk ∼E(wmk | λW
mk),
zkn ∼N(zkn | 0, (λZ
kn)−1);
p(W ) =
M,K
Y
m,k=1
E(wmk | λW
mk),
p(Z) =
K,N
Y
k,n=1
N(zkn | 0, (λZ
kn)−1),
(6.30)
where E(x | λ) = λ exp(−λx)u(x) is the exponential density with u(x) being the unit step
function. The prior for the noise variance σ2 is again chosen as an inverse-Gamma density
with shape ασ and scale βσ (Equation (6.4)). The conditional posterior densities for wmk’s
and zkn’s are already provided in the GEE and GGG models respectively.
6.10.2 Gaussian Likelihood with Nonnegative Volume and Gaussian Priors
(GnVG)
The volume prior discussed in the GVG model (Section 5.5, p. 139) can be formulated to
be nonnegative so as to enforce nonnegativity (Figure 6.15(b)).
176

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
ατ/ασ
βτ/βσ
τ/σ2
λW
mk
λZ
kn
(a) GEG.
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
ατ/ασ
βτ/βσ
τ/σ2
λ
λZ
kn
(b) GnVG.
Figure 6.15: Graphical model representation of GEG and GnVG models. Green circles
denote prior variables, orange circles represent observed and latent variables, and plates
represent repeated variables. The slash “/” in the variable represents “or”.
Prior.
Same as the GGG and GEG models, we place a Gaussian density with precision
λZ
kn over Z:
zkn ∼N(zkn | 0, (λZ
kn)−1);
p(Z) =
K,N
Y
k,n=1
N(zkn | 0, (λZ
kn)−1).
(6.31)
The nonnegative volume prior is constructed as follows,
W ∼
(
exp{−γ det(W ⊤W )},
if wmk ≥0 for all m, k;
0,
if any wmk < 0.
(6.32)
The posterior density for zkn is the same as those in the GEG and GGG models. And the
posterior density is similar to that of the GVG model, in this case, we draw the samples
from a truncated-normal rather than a normal density.
6.11. Nonnegative Matrix Tri-Factorization (NMTF)
Similar to the bilinear nonnegative matrix factorization in which case we reduce the ob-
served matrix into a product of two factor matrices, the nonnegative matrix tri-factorization
(NMTF) extends the components into three, i.e., we factor the data matrix A into three
matrices A = W F Z + E where W ∈RM×K
+
, F ∈RK×L
+
, Z ∈RL×N
+
.
177

6.11. NONNEGATIVE MATRIX TRI-FACTORIZATION (NMTF)
k = 1..K
l = 1..L
m = 1..M
n = 1..N
wmk
amn
zln
fkl
ατ/ασ
βτ/βσ
τ/σ2
λW
mk
λZ
ln
λF
kl
(a) GEEE.
k = 1..K
l = 1..L
m = 1..M
n = 1..N
wmk
amn
zln
fkl
ατ/ασ
βτ/βσ
τ/σ2
λW
k
λZ
l
λF
kl
αλ, βλ
(b) GEEEA.
Figure 6.16: Graphical model representation of GEEE and GEEEA models. Green circles
denote prior variables, orange circles represent observed and latent variables, and plates
represent repeated variables. The slash “/” in the variable represents “or”, and the comma
“,” in the variable represents “and”.
Likelihood.
We again assume the residuals, emn, are i.i.d., zero mean normal with vari-
ance σ2, which gives rise to the likelihood,
p(A | θ) =
M,N
Y
m,n=1
N

amn | w⊤
mF zn, σ2
=
M,N
Y
m,n=1
N

amn | w⊤
mF zn, τ −1
(6.33)
where θ = {W , F , Z, σ2} denotes all parameters in the model, σ2 is the variance, and
τ −1 = σ2 is the precision. And now we want the reconstruction error measured by Frobenius
norm to be minimized:
min
W ,Z L(W , Z) = min
W ,Z
N
X
n=1
M
X
m=1

amn −w⊤
mF zn
2
.
(6.34)
Prior.
We assume W and Z are independently exponentially distributed with scales λW
mk
and λZ
kn (Deﬁnition 2.3.1, p. 49),
wmk ∼E(wmk | λW
mk),
fkl ∼E(fkl | λF
kl),
zln ∼E(zln | λZ
ln);
p(W ) =
M,K
Y
m,k=1
E(wmk | λW
mk),
p(F ) =
K,L
Y
k,l=1
E(fkl | λF
kl),
p(Z) =
L,N
Y
l,n=1
E(zln | λZ
ln),
(6.35)
178

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
where E(x | λ) = λ exp(−λx)u(x) is the exponential density with u(x) being the unit step
function. The prior for the noise variance σ2 is chosen as an inverse-Gamma density with
shape ασ and scale βσ (Deﬁnition 2.2.4, p. 40),
p(σ2) = G−1(σ2 | ασ, βσ) = βσασ
Γ(ασ)(σ2)−ασ−1 exp

−βσ
σ2

.
(6.36)
Therefore, the posterior density of σ2 is still from Equation (6.8).
Posterior.
For NMF, following the Bayes’ rule and MCMC, this means we need to be
able to draw from distributions (by Markov blanket, Section 5.2, p. 129):
p(wmk | A, W−mk, F , Z, σ2, λW , λF , λZ),
p(fkl | A, W , F−kl, Z, σ2, λW , λF , λZ),
p(zln | A, W , F , Z−ln, σ2, λW , λF , λZ),
p(σ2 | A, W , Z, ασ, βσ),
where λW is an M×K matrix containing all {λW
mk} entries, λZ is a L×N matrix including all
{λZ
ln} values, and W−mk denotes all elements of W except wmk. The conditional density
of wmk is just similar to that in the GEE model in Equation (6.5).
For simplicity, we
denote the k-th row of F as rk, and l-th column of F as cl. The conditional density of
wmk is the same as that in Equation (6.5), except now we replace zkj with r⊤
k zj in the
variance parameter of Equation (6.6), and replace zkj with r⊤
k zj and replace zij with r⊤
i zj
in Equation (6.7). The reason is obvious, when considering the conditional density of wmk,
we can treat F Z as a single matrix, and the problem becomes a bilinear decomposition.
Similarly, the conditional posterior density of zln can be derived due to symmetry to wmk.
Using Bayes’ theorem, the conditional density of fkl depends on its parents (λF
kl), chil-
dren (amn), and coparents (τ or σ2, W , Fkl, Z) 11. And it can be obtained by
p(fkl | A, W , F−kl, Z, σ2,
λW ,
λZ, λF , A) = p(fkl | A, W , F−kl, Z, σ2, λF
kl)
∝p(A | W , F , Z, σ2) × p(fkl | λF
kl) =
M,N
Y
i,j=1
N

aij | w⊤
i F zj, σ2
× E(wkl | λF
kl)
∝exp


−1
2σ2
M,N
X
i,j=1
(aij −w⊤
i F zj)2


×  λF
kl exp(−λF
kl · fkl)u(fkl)
∝exp


−1
2σ2
M,N
X
i,j=1

−2aij(w⊤
i F zj) + (w⊤
i F zj)2


· exp(−λF
kl · fkl)u(fkl).
(6.37)
To express the conditional density of {fkl | A, W , F−kl, Z, σ2, λF
kl} in terms of fkl, we write
out w⊤
i F zj in the above equation as
w⊤
i F zj =
K,L
X
s,t=1
wis fstztj = fkl (wik zlj) + C
11. See Figure 6.16(a) and Section 5.2 (p. 129).
179

6.11. NONNEGATIVE MATRIX TRI-FACTORIZATION (NMTF)
where
C =
K,L
X
(s,t)̸=(k,l)
wis fstztj
is a constant when considering fkl. Therefore, Equation (6.37) can be expressed as, by
excluding terms non-relevant to fkl,
p(fkl | A, W , F−kl, Z, σ2, λF
kl)
∝exp

















−
PM,N
i,j=1(wikzlj)2
2σ2
|
{z
}
1/(2 f
σ2
kl)
f2
kl + fkl

−λF
kl +
M,N
X
i,j=1
(wikzlj)
aij −C
σ2


|
{z
}
f
σ2
kl
−1
f
µkl

















· u(fkl)
∝N(fkl | f
µkl, f
σ2
kl) · u(wkl) = T N(fkl | f
µkl, f
σ2
kl),
(6.38)
where u(x) is the unit function with value 1 if x ≥0 and value 0 if x < 0,
f
σ2
kl =
σ2
PM,N
i,j=1(wikzlj)2
(6.39)
is the “parent” posterior variance of the normal distribution with posterior “parent” mean
f
µkl,
f
µkl =

−λF
kl +
M,N
X
i,j=1
(wikzlj)
aij −C
σ2

· f
σ2
kl
(6.40)
and T N(x | µ, σ2) is the truncated-normal density with “parent” mean µ and “parent”
variance σ2 (Deﬁnition 2.4.1, p. 50).
Sparsity.
Similar to the GEE model on the factored component wmk, the posterior pa-
rameters have a similar sparsity constraint on the component fkl. The sparsity comes from
the negative term −λF
kl in Equation (6.40). When λF
kl becomes larger, the posterior “parent”
mean becomes smaller, and the TN distribution will have a larger probability for smaller
values (or even approaching zero) since the draws of T N(fkl | f
µkl, f
σ2
kl) will be around zero
thus imposing sparsity (see Figure 2.8(a), p. 52).
Gibbs sampling.
By this Gibbs sampling method introduced in Section 1.3.3 (p. 20),
we can construct a Gibbs sampler for the GEEE model as formulated in Algorithm 18.
And also in practice, all the parameters of the exponential distribution are set to be the
same value λ = {λW
mk}′s = {λF
kl}′s = {λZ
ln}′s for all m, k, l, n. By default, uninformative
hyperparameters are ασ = βσ = 1, {λW
mk} = {λF
kl} = {λZ
ln} = 0.1.
Automatic relevance determination.
Similar to the GEEA model (Section 6.3), we
can use ARD to share the scale parameter of exponential priors for each row of W and each
180

JUN LU
CHAPTER 6. BAYESIAN NONNEGATIVE MATRIX FACTORIZATION
Algorithm 18 Gibbs sampler for GEEE Model in one iteration (prior on variance σ2 here,
similarly for the precision τ). The procedure presented here may not be eﬃcient but is
explanatory. A more eﬃcient one can be implemented in a vectorized manner. By default,
uninformative hyperparameters are ασ = βσ = 1, {λW
mk} = {λF
kl} = {λZ
ln} = 0.1.
Require: Choose initial ασ, βσ, λW
mk, λF
kl, λZ
ln;
1: for k = 1 to K do
2:
for m = 1 to M do
3:
Sample wmk from p(wmk | A, W−mk, F , Z, σ2, λW
mk);
▷Equation (6.5)
4:
end for
5:
for l = 1 to L do
6:
Sample fkl from p(fkl | A, W , F−kl, Z, σ2, λF
kl);
▷Equation (6.38)
7:
end for
8: end for
9: for l = 1 to L do
10:
for n = 1 to N do
11:
Sample zln from p(zln | A, W , F , Z−ln, σ2, λZ
ln);
▷Symmetry of Equation (6.5)
12:
end for
13: end for
14: Sample σ2 from p(σ2 | A, W , Z, ασ, βσ);
▷Equation (6.8)
15: Report loss in Equation (6.34), stop if it converges.
column of Z so as to perform automatic model selection. The graphical representation is
shown in Figure 6.16(b),
wmk ∼E(wmk | λW
k ),
zln ∼E(zln | λZ
l ),
λW
k ∼G(λW
k | αλ, βλ),
λZ
l ∼G(λZ
l | αλ, βλ).
Note in this case, we do not share any parameters for the entries of F . For brevity, we do
not go into the details of this model here.
181

7
Bayesian Poisson Matrix Factorization
Contents
7.1
Poisson Likelihood with Gamma Priors (PAA) . . . . . . . . .
183
7.2
Poisson Likelihood with Gamma Priors and Hierarchical Gamma
Priors (PAAA)
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
7.3
Properties of PAA or PAAA . . . . . . . . . . . . . . . . . . . .
186
7.4
Recommendation Systems . . . . . . . . . . . . . . . . . . . . . .
188
182

JUN LU
CHAPTER 7. BAYESIAN POISSON MATRIX FACTORIZATION
7.1. Poisson Likelihood with Gamma Priors (PAA)
The Poisson likelihood with Gamma priors (PAA) model is proposed by Gopalan et al.
(2013, 2015) in a recommendation system context due to the prevalence and popularity
of movie recommendation data sets like the Netﬂix Challenge. The PAA model extends
the Poisson factorization (Canny, 2004; Dunson and Herring, 2005; Cemgil, 2009) and it is
further discussed in Gopalan et al. (2014); Hu et al. (2015). The model is working on the
nonnegative count data, A ∈NM×N, e.g., a data matrix about users and items 1 where
each user has consumed and possibly rated a set of items. The observation amn is the rating
that user n gave to item m, or zero if no rating was given. We again assume the matrix is
factored as the product of W ∈RM×K
+
and Z ∈RK×N
+
.
To be more speciﬁc, the PAA model considers to minimize the following loss:
min
W ,Z L(W , Z) = min
W ,Z
N
X
n=1
M
X
m=1

amn −w⊤
mzn
2
,
(7.1)
where W = [w⊤
1 ; w⊤
2 ; . . . ; w⊤
M] ∈RM×K and Z = [z1, z2, . . . , zN] ∈RK×N containing wm’s
and zn’s as rows and columns respectively. Therefore, each item m is represented by a
vector of K latent attributes wm and each user n by a vector of K latent preferences zn. In
the Netﬂix context, the PAA model is speciﬁcally designed to accommodate the heteroge-
neous interests of users (some users tend to consume more than others), the diﬀerent types
of items (some items/movies are more popular than others), and the realistic distribution
of limited resources that users have to consume these items as the recommendation system
literature suggests that an eﬀective model should consider the heterogeneity among both
users and items (Koren et al., 2009).
Likelihood.
Given a data matrix A about users and items, where each user has consumed
and possibly rated a set of items. We assume each element amn is Poisson distributed with
mean given by the factored component w⊤
mzn (see Figure 7.1(a)),
amn ∼P(w⊤
mzn),
where P(·) is a Poisson distribution whose parameter is a linear combination of the cor-
responding user preferences and item attributes w⊤
mzn.
This is just like the Gaussian
likelihood where the expectation of amn is also given by w⊤
mzn (Deﬁnition 2.6.1, p. 65).
Suppose further we decompose amn by K components,
amn =
K
X
k=1
omnk.
The prior over amn then can be decomposed into
omnk ∼P(wmkzkn).
1. The items can be any of movies, songs, articles, or products. We also refer to movies in the Netﬂix
context.
183

7.1. POISSON LIKELIHOOD WITH GAMMA PRIORS (PAA)
Since, by Theorem 2.4 (p. 66), the sum of Poisson random variables is a Poisson random
variable, we obtain the assumed Poisson likelihood as follows,
amn =
K
X
k=1
omnk ∼P(w⊤
mzn).
(7.2)
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
α, β
α, β
(a) PAA.
k = 1..K
m = 1..M
n = 1..N
wmk
amn
zkn
a, b
α, λW
m
α, λZ
n
(b) PAAA.
Figure 7.1: Graphical model representations of PAA and PAAA models. Green circles
denote prior variables, orange circles represent observed and latent variables, and plates
represent repeated variables.
Prior.
We assume W and Z are independently Gamma distributed with shape and rate
parameters α and β respectively (Deﬁnition 2.2.3, p. 36),
wmk ∼G(wmk | α, β),
zkn ∼G(zkn | α, β).
(7.3)
This Gamma prior on the latent attributes and the latent preferences can drive the model
towards a sparse representation of users and items (see Figure 2.3(a), p. 37 for examples
of the Gamma distribution), which is more representative of real-world behavior.
Posterior.
Let omn = [omn1, omn2, . . . , omnK]⊤∈RK, by Theorem 2.5 (p. 66), the condi-
tional distribution of omn given amn = PK
k=1 omnk is
MultiK(omn | amn, p),
(7.4)
where p =
1
w⊤
mzn [wm1z1n, wm2z2n, . . . , w(mK)z(Kn)]⊤∈[0, 1]K such that 1⊤p = 1 and each
element pi in p is in the range of [0, 1].
The conditional posterior of wmk is again from Bayes’s rule,
p(wmk | A, W−mk, Z, α, β) ∝
N
Y
j=1
P(omjk | wmkzkn) · G(wmk | α, β)
∝w
(PN
j=1 omjk)
mk
exp


−wmk


N
X
j=1
zkj




· wα−1
mk exp (−βwmk)
∝G(wmk | eα, eβ),
(7.5)
184

JUN LU
CHAPTER 7. BAYESIAN POISSON MATRIX FACTORIZATION
where
eα = α +
N
X
j=1
omjk,
eβ = β +
N
X
j=1
zkj.
By the deﬁnition of the Gamma distribution (Deﬁnition 2.2.3, p. 36), the posterior mean
of wmk is given by
E[wmk | A, W−mk, Z, α, β] =
α + PN
j=1 omjk
β + PN
j=1 zkj
.
This is reasonable in the sense that, when PN
j=1 omjk is large, we are delving with a large
value of amn thus favoring a large value of wmk; while when the value of PN
j=1 zkj is large
from the last iteration of the Gibbs sampling procedure, we expect a small value of wmk to
compensate for this. By symmetry, a similar conditional density for zkn can be derived.
Algorithm 19 Gibbs sampler for PAA model in one iteration. By default, uninformative
hyperparameters are α = β = 1.
Require: Choose initial α, β;
1: for m = 1 to M do
2:
for n = 1 to N do
3:
Sample omn from p(omn | amn, p);
▷Equation (7.4)
4:
end for
5: end for
6: for k = 1 to K do
7:
for m = 1 to M do
8:
Sample wmk from p(wmk | A, W−mk, Z, α, β);
▷Equation (7.5)
9:
end for
10:
for n = 1 to N do
11:
Sample zkn from p(zkn | A, W , Z−kn, α, β);
▷Symmetry of Eq. (7.5)
12:
end for
13: end for
Gibbs sampling.
By this Gibbs sampling method introduced in Section 1.3.3 (p. 20),
we can construct a Gibbs sampler for the PAA model as formulated in Algorithm 19. In
practice, the initial hyperparameters can be set to a weak prior with α = β = 1.
7.2. Poisson Likelihood with Gamma Priors and Hierarchical Gamma
Priors (PAAA)
The Poisson likelihood with Gamma priors and hierarchical Gamma priors (PAAA) model
is introduced in Gopalan et al. (2015).
185

7.3. PROPERTIES OF PAA OR PAAA
Prior.
Going further from the PAA model, we put a hierarchical Gamma prior over the
Gamma parameter:
amn ∼P(amn | w⊤
mzn),
wmk ∼G(wmk | α, λW
m ),
zkn ∼G(zkn | α, λZ
n ),
λW
m ∼G(a, a
b ),
λZ
n ∼G(a, a
b ).
(7.6)
The hierarchical structure allows us to capture the diversity of users, i.e., some users tend
to consume more than others; and the diversity of items, i.e., some items are more popular
than others.
Posterior.
The conditional posteriors for wmk, zkn, and omn are identical to the PAA
model except we replace β with λW
m in the expression for eβ. For the conditional posterior
of λW
m , the hierarchical part, we again follow Bayes’ rule,
p(λW
m | W , α, a, b) ∝
K
Y
k=1
G(wmk | α, λW
m ) · G(λW
m | a, a
b )
∝
K
Y
k=1
(λW
m )α
Γ(α) wα−1
mk exp(−λW
m wmk) · (a
b)a
Γ(a)(λW
m )a−1 exp(−a
b λW
m )
∝(λW
m )Kα+a−1 exp
(
−λW
m
 
a
b +
K
X
k=1
wmk
!)
∝G(λW
m | eam,ebm),
(7.7)
where
eam = Kα + a,
ebm = a
b +
K
X
k=1
wmk.
Gibbs sampling.
By this Gibbs sampling method introduced in Section 1.3.3 (p. 20),
we can construct a Gibbs sampler for the PAA model as formulated in Algorithm 20 . In
practice, the initial parameters can be set to a weak prior with α = a = b = 1.
7.3. Properties of PAA or PAAA
After introducing the modeling details, we outline some statistical features of the PAA or
PAAA approaches. These characteristics oﬀer beneﬁts over the Gaussian likelihood matrix
factorization methods we have discussed in previous chapters when considering the Netﬂix
context.
PAA or PAAA captures sparse factors.
As mentioned previously, the Gamma priors
on the factored components, i.e., on the user preferences and item attributes, can encourage
sparse representations of users and items. A small shape parameter in the Gamma prior
results in most weights being close to zero, leaving only a few large ones (see Figure 7.2 for
examples of the Gamma distribution G(α, β) where we reduce the shape parameter α from
3 to 1, given β = 1. The density drives to zero.). This leads to a simpler and more easily
interpretable model.
186

JUN LU
CHAPTER 7. BAYESIAN POISSON MATRIX FACTORIZATION
Algorithm 20 Gibbs sampler for PAAA model in one iteration. By default, uninformative
hyperparameters are α = a = b = 1.
Require: Choose initial α, a, b;
1: for m = 1 to M do
2:
for n = 1 to N do
3:
Sample omn from p(omn | amn, p);
▷Equation (7.4)
4:
end for
5: end for
6: for k = 1 to K do
7:
for m = 1 to M do
8:
Sample wmk from p(wmk | A, W−mk, Z, α, λW
m );
▷Eq. (7.5), replace β by λW
m
9:
Sample λW
m from p(λW
m | W , α, a, b);
▷Equation (7.7)
10:
end for
11:
for n = 1 to N do
12:
Sample zkn from p(zkn | A, W , Z−kn, α, λZ
n );
▷Eq. (7.5), replace β by λZ
n
13:
Sample λZ
n from p(λZ
n | Z, α, a, b);
▷Symmetry of Eq. (7.7)
14:
end for
15: end for
0
2
4
6
8
10
x
0.0
0.5
1.0
1.5
2.0
2.5
f(x)
Gamma Distribution PDF
=0.5, =1
=1, =1
=2, =1
=3, =1
Figure 7.2: Gamma probability
density functions G(α, β) by re-
ducing the shape parameter α.
PAA or PAAA models the long-tail of users and items.
In the “implicit” consumer
data that we consider here amn equals one if user n consumed item m and zero otherwise 2,
the distribution of user activity (i.e., how many items a user consumed) and item popularity
(i.e., how many users consumed an item) in real-world user behavior data is characterized by
a long-tail distribution, where the majority of users consume only a few items while a small
number of “tail users” consume a large amount. To see this, we consider the MovieLens 1M
data set (see Table 6.2, p. 159) which contains movie ratings for 6,040 movies from 3,503
users. Figure 7.3(a) shows only a small portion of users have consumed more than 1,500
movies; and Figure 7.3(b) shows only a small portion of items have been consumed by more
than 500 users, indicating the long-tail behavior.
2. In contrast, the “explicit” consumer data contains a matrix of integer ratings.
187

7.4. RECOMMENDATION SYSTEMS
0
500
1000
1500
2000
2500
3000
3500
User activity
0
10
20
30
40
Number of users
User Activity
(a) User activity.
0
500
1000
1500
2000
Item popularity
0
20
40
60
80
Number of items
Item popularity
(b) Item popularity.
Figure 7.3: User activity and item popularity for the MovieLens 1M data set (see data
description in Table 6.2, p. 159).
The PAA or PAAA model can capture this property easily via a two-stage process. From
Equation (7.2), for each user n, again by Theorem 2.4 (p. 66) and Theorem 2.5 (p. 66), we
have
un =
M
X
i=1
ain ∼P
 M
X
i=1
w⊤
i zn
!
,
[a1n, a2n, . . . , aMn]⊤∼MultiM(un, q),
where q =
1
PM
i=1 w⊤
i zn [w⊤
1 zn, w⊤
2 zn, . . . , w⊤
Mzn]⊤∈[0, 1]M such that 1⊤q = 1. Therefore,
the PAA or PAAA model ﬁrst learns a budget un for each user n and then learns how to
distribute the budget across items. Learning this budget value is important for modeling
the long-tail behavior of user activity.
Similar for each item m, we have
vm =
N
X
i=1
ami ∼P
 N
X
i=1
w⊤
mzi
!
,
[am1, am2, . . . , amN]⊤∼MultiN(vm, s),
where s =
1
PN
i=1 w⊤
mzi [w⊤
mz1, w⊤
mz2, . . . , w⊤
mzN]⊤∈[0, 1]N such that 1⊤s = 1. The PAA or
PAAA model ﬁnds the popularity of item m by vm and then learns how the popularity is
distributed across users.
7.4. Recommendation Systems
In Section 3.10.2 (p. 113), we introduce two recommendation systems based on matrix
factorization. We shortly discuss in the following two paragraphs and we consider a new
recommender via this Bayesian matrix factorization model.
188

JUN LU
CHAPTER 7. BAYESIAN POISSON MATRIX FACTORIZATION
Recommender 1.
A recommender system can work simply by suggesting the uncon-
sumed movie m when amn for user n by the posterior expected Poisson parameters,
scoremn = E[w⊤
mzn | A].
(7.8)
The score E[w⊤
mzn | A] can be obtained by averaging the values during the Gibbs sampling
iterations.
Recommender 2.
After obtaining the item attributes {w1, w2, . . . , wM}, we compare
the similar matrix (with diﬀerent measures, e.g., Pearson similarity or cosine similarity) of
movies and suggest the movie with high similarity to the consumed item for each user n.
The precision-recall curve can help ﬁnd out a threshold to decide the ﬁnal recommendation.
Recommender 3.
In the movie recommendation case, the uncertainty about each entry
in A can be measured by its predictive standard deviation. A practical system can exploit
this to only suggest items with high conﬁdence. Since we can also model uncertainty via
the Bayesian approach. Absorbing the idea of the Sharpe ratio in quantitative ﬁnance. The
Sharpe ratio is a measure of the risk-adjusted return of an investment, calculated as the
ratio of its average return and its standard deviation. It measures the excess return per
unit of risk, and is widely used in ﬁnance to evaluate the performance of an investment
relative to its volatility. The higher the Sharpe ratio, the better the risk-adjusted return
of the investment is considered to be. Embracing the concept of the Sharpe ratio, we can
suggest the unconsumed item m when amn for user n by the uncertainty-adjusted score of
posterior expected Poisson parameters,
scoremn =
E[w⊤
mzn | A]
p
Var[w⊤
mzn | A]
.
189

8
Bayesian Ordinal Matrix Factorization
Contents
8.1
Ordinal Likelihood with Gaussian Prior and Wishart Hierar-
chical Prior (OGGW)
. . . . . . . . . . . . . . . . . . . . . . . .
191
8.1.1
Ordinal Regression Likelihood . . . . . . . . . . . . . . . . . . . . 191
8.1.2
Matrix Factorization Modeling on Latent Variables . . . . . . . . 193
8.1.3
Gibbs Sampler
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
8.2
Properties of OGGW . . . . . . . . . . . . . . . . . . . . . . . . .
195
190

JUN LU
CHAPTER 8. BAYESIAN ORDINAL MATRIX FACTORIZATION
8.1. Ordinal Likelihood with Gaussian Prior and Wishart Hierarchical
Prior (OGGW)
The properties of the Poisson factorization models (PF, e.g., PAA and PAAA models) show
the aim of them is to make recommendations by predicting future interactions between
users and items. Therefore, the PF is often applied to the implicit consumer data where the
data A ∈{0, 1}M×N, i.e., the data contains only the information that a user is interacting
with an item or not.
In many applications, the data matrix A will be further constrained.
The ordinal
matrix factorization (OMF) considers ordinal data (Stevens, 1946) where entries in A are
restricted to a ﬁnite ordered (ranked) set of values such that a judgment of preference is
made, e.g., in collaborate ﬁltering, we seek to predict a consumer’s rating of a novel item
on an ordinal scale such as good > average > bad; the temperature of a day is hot >
warm > cold; a teacher always rates his/her students by giving grades on their overall
performance having the ordering A > B > C > D > F (Paquet et al., 2005; Chu et al.,
2005; Gouvert et al., 2020). Although the real-valued or nonnegative matrix factorization
techniques introduced in previous chapters can be used to ﬁnd the decomposition, a speciﬁc
approach that explicitly models this ordinal data can be more eﬃcient.
Instead of factoring A = W Z + E directly as those were done in real-valued matrix
factorization and nonnegative matrix factorization, the Bayesian ordinary matrix factoriza-
tion introduces an additional hidden matrix H = W Z + E ∈RM×N which is then used as
an unobserved input to an ordinal regression model to obtain the data matrix A (see Fig-
ure 8.3). Therefore, the ordinary matrix factorization is also called a hierarchical Bayesian
model.
8.1.1 Ordinal Regression Likelihood
We now consider the data matrix A ∈AM×N where A is a ﬁnite set of A-ordered categories.
Without loss of generality, these categories can be denoted as consecutive integers A =
{1, 2, . . . , A} that conserve the known ordering information. The real number line is divided
into a set of contiguous intervals with boundaries {ba},
−∞= b1 < b2 < . . . < bA+1 = ∞,
such that the interval [ba, ba+1) corresponds to the discrete category a ∈A. To model the
hidden variable h, we introduce an additional hidden variable f (see Figure 8.1). The value
of the variable f implies the rank a if f falls in the rank a’s interval:
p(a | f) =
(
1,
if ba ≤f < ba+1
0,
otherwise = u(f −ba) −u(f −ba+1),
(8.1)
where u(y) is the step function with value 1 if y ≥0 and value 0 if y < 0.
Given the hidden value h, uncertainty about the exact location of f can be modeled by
a unit variance Gaussian,
p(f | h) = N(f | h, 1).
(8.2)
Averaging over f in p(a, f | h) = p(a | f)p(f | h), we have
p(a | h) =
Z
p(a, f | h)df = Φ(h −ba) −Φ(h −ba+1),
(8.3)
191

8.1. ORDINAL LIKELIHOOD WITH GAUSSIAN PRIOR AND WISHART HIERARCHICAL PRIOR
(OGGW)
where Φ(y) =
R y
−∞N(u | 0, 1)du =
1
√
2π
R y
−∞exp(−u2
2 )du is the cumulative distribution
function of N(0, 1). In Equation (8.3), we use the fact that
Φ(h −b) =
Z
N(f | h, 1) u(f −b)df
(see Albert and Chib (1993)). Figure 8.2 shows the probability functions for p(a | h) by
varying h. We observe when h falls outside of the interval [ba, ba+1], the probability is not
exactly zero. And when the interval ba+1 −ba is small, the probability tends to be small (in
a sense, a small interval of ba+1 −ba indicates there are a large number of categories such
that the probability tends to be small for falling into each interval).
h
f
a
Figure 8.1: Graphical representation of the ordinal regression model.
Full likelihood.
The ordinal regression model then maps continuous latent variables hmn
in H to probabilities p(amn | hmn),
p(amn | hmn) =
A
Y
a=1
[Φ(hmn −ba) −Φ(hmn −ba+1)]1(amn=a) .
(8.4)
Suppose the set of observed entries or the training set is denoted by X = {amn | (m, n) ∈
training set}, the likelihood of observed entries under the ordinal regression model is
p(X | H) =
Y
(m,n)
p(amn | hmn),
(8.5)
where the product is over all the observed entries or training set entries (m, n).
Figure 8.2: Ordinal probability
of the ordinal regression model in
Equation (8.3).
10.0
7.5
5.0
2.5
0.0
2.5
5.0
7.5
10.0
h
0.0
0.2
0.4
0.6
0.8
1.0
p(a | h)
Ordinal Probability
ba=1, ba + 1=2
ba=1, ba + 1=3
ba=1, ba + 1=4
ba=1, ba + 1=5
192

JUN LU
CHAPTER 8. BAYESIAN ORDINAL MATRIX FACTORIZATION
8.1.2 Matrix Factorization Modeling on Latent Variables
The Bayesian modeling is just the same as that of the GGGW model (Section 5.4, p. 138),
except that we now consider Gaussian likelihoods over the hidden variables hmn rather
than the observed ratings amn.
The full graphical representation of the model is then
shown in Figure 8.3 which we call the ordinal likelihood with Gaussian and hierarchical
normal-inverse-Wishart priors (OGGW) model.
Likelihood.
We assume the residuals, emn, are i.i.d., zero mean normal with precision
τ =
1
σ2 , which gives rise to the following likelihood function,
p(H | W , Z, τ) =
M,N
Y
m,n=1
N
 hmn | (W Z)mn, σ2
=
M,N
Y
m,n=1
N
 hmn | (W Z)mn, τ −1
(8.6)
where σ2 is the variance, and τ −1 = σ2 is the precision.
Prior.
Given the m-th row wm of W and the n-th column zn of Z, we consider the
multivariate Gaussian density and the normal-inverse-Wishart prior as follows:
wm ∼N(wm | µw, Σw),
µw, Σw ∼NIW(µw, Σw | m0, κ0, ν0, S0);
zn ∼N(zn | µz, Σz),
µz, Σz ∼NIW(µz, Σz | m0, κ0, ν0, S0),
(8.7)
where NIW(µ, Σ | m0, κ0, ν0, S0) = N(µ | m0, 1
κ0 Σ) · IW(Σ | S0, ν0) is the density of a
normal-inverse-Wishart distribution and IW(Σ | S0, ν0) is the inverse-Wishart distribution
(Equation (2.40), p. 78).
The prior for the noise variance σ2 is chosen as a conjugate inverse-Gamma density with
shape ασ and scale βσ (Deﬁnition 2.2.4, p. 40),
p(σ2) = G−1(σ2 | ασ, βσ) = βσασ
Γ(ασ)(σ2)−ασ−1 exp

−βσ
σ2

.
Or placing an inverse-Gamma prior on the variance is equivalent to applying a Gamma
prior on the precision parameter. For the precision τ = σ−2, we use a Gamma distribution
with shape ατ > 0 and rate βτ > 0 (Deﬁnition 2.2.3, p. 36),
p(τ) ∼G(τ | ατ, βτ) =
βατ
τ
Γ(ατ)τ ατ−1 exp(−βτ · τ),
8.1.3 Gibbs Sampler
To construct the Gibbs sampler, we need to obtain the conditional posterior for each vari-
able.
193

8.1. ORDINAL LIKELIHOOD WITH GAUSSIAN PRIOR AND WISHART HIERARCHICAL PRIOR
(OGGW)
m = 1..M
n = 1..N
wm
amn
hmn
zn
ατ/ασ
βτ/βσ
τ/σ2
µw, Σw
m0, κ0,
ν0, S0
µz, Σz
Figure 8.3: Graphical representation of OGGW model. Green circles denote prior vari-
ables, orange circles represent observed and latent variables, and plates represent repeated
variables. The slash “/” in the variable represents “or”, and the comma “,” in the variable
represents “and”.
Latent variables.
The conditional density for latent variables hmn is
p(hmn | amn, wm, zn, τ) ∝p(amn | hmn) p(hmn | wm, zn, τ).
(8.8)
To sample from this conditional density, we introduce back the hidden variable fmn. For
brevity, we omit the subscript m, n. The density f, h | a, w, z, τ then can be sampled from
in two steps, f | a, w, z, τ and h | f, w, z, τ. The joint marginal distribution of a, f, and h,
given m = w⊤z and τ, is
p(a | f) p(f | h) p(h | m, τ) = [u(f −ba) −u(f −ba+1)] N(f | h, 1) N(h | m, τ −1).
(8.9)
The conditional density of p(f | a, m, τ) follows from
p(f | a, m, τ) = GT N(f | m, 1 + τ −1, ba, ba+1),
a general-truncated-normal density (Deﬁnition 2.4.2, p. 53). Therefore, the sample h can
be obtained by
p(h | f, m, τ) ∝p(f | h) p(h | m, τ −1) = N(f | h, 1) N(h | m, τ −1)
∝N

h

f + mτ
1 + τ , (1 + τ)−1

.
(8.10)
Multivariate Gaussian parameters.
Same as the GGGW model, from the discussion
in Section 2.7.7 (p. 78), the posterior density of {µw, Σw} also follows a NIW distribution
with updated parameters:
µw, Σw ∼NIW(µw, Σw | mM, κM, νM, SM)
(8.11)
194

JUN LU
CHAPTER 8. BAYESIAN ORDINAL MATRIX FACTORIZATION
where
mM = κ0m0 + Mw
κM
= κ0
κM
m0 + M
κM
w
(8.12)
κM = κ0 + M
(8.13)
νM = ν0 + M
(8.14)
SM = S0 + Sw +
κ0M
κ0 + M (w −m0)(w −m0)⊤
(8.15)
= S0 +
M
X
m=1
wmw⊤
m + κ0m0m⊤
0 −κMmMm⊤
M
(8.16)
w = 1
M
M
X
m=1
wm.
(8.17)
Sw =
M
X
m=1
(wm −w)(wm −w)⊤
(8.18)
Gaussian variance parameter.
The conditional density of σ2 depends on its parents
(ασ, βσ), children (A), and coparents (W , Z). And it is an inverse-Gamma distribution
(by conjugacy in Equation (2.7), p. 40),
p(σ2 | W , Z, A) = p(σ2 | W , Z, A) = G−1(σ2 | f
ασ, f
βσ),
f
ασ = MN
2
+ ασ,
f
βσ = 1
2
M,N
X
m,n=1
(A −W Z)2
mn + βσ.
(8.19)
Gaussian precision parameter.
Alternatively, the conditional posterior density of τ =
1
σ2 is obtained similarly (Equation (2.5), p. 38),
p(τ | W , Z, A) = p(τ | W , Z, A) = G(τ | f
ατ, f
βτ),
f
ατ = MN
2
+ ατ,
f
βτ = 1
2
M,N
X
m,n=1
(A −W Z)2
mn + βτ.
(8.20)
In practice, the prior parameters ατ, βτ are chosen to be equal to ασ, βσ respectively.
Gibbs sampling.
By this Gibbs sampling method introduced in Section 1.3.3 (p. 20),
we can construct a Gibbs sampler for the OGGW model as formulated in Algorithm 21. A
speciﬁc choice of hyperparameters for the normal-inverse-Wishart prior on the mean and
covariance is not signiﬁcant in practice since there is a large amount of data for learning
that will override any reasonable weak prior. The weak prior can be chosen as m0 = 0, κ0 =
1, ν0 = K + 1, S0 = I. While the choice for ατ, βτ rather depends on the data sets. A week
prior choice is ατ = βτ = 1.
8.2. Properties of OGGW
The OGGW model’s signiﬁcant feature is that it is not limited to oﬀering only the expected
value of missing entries in A; it can also provide a probability distribution over feasible
195

8.2. PROPERTIES OF OGGW
Algorithm 21 Gibbs sampler for OGGW model in one iteration (prior on τ =
1
σ2 ). By
default, uninformative hyperparameters are m0 = 0, κ0 = 1, ν0 = K +1, S0 = I, ατ = βτ =
1.
Require: Choose initial ατ, βτ, m0, κ0, ν0, S0;
1: for m = 1 to M do
2:
Sample wm from p(wm | µm, Σm);
▷Equation (8.7)
3:
Sample hmn from p(hmn | amn, wm, zn, τ) for each n;
▷Equation (8.10)
4: end for
5: for n = 1 to N do
6:
Sample zn from p(zn | µz, Σz);
▷Equation (8.7)
7:
Sample hmn from p(hmn | amn, wm, zn, τ) for each m;
▷Equation (8.10)
8: end for
9: Sample τ from p(τ | W , Z, A);
▷Equation (8.20)
10: Sample µw, Σw from p(µw, Σw | W , M);
▷Equation (8.11)
11: Sample µz, Σz from p(µz, Σz | Z, N);
▷Symmetry of Eq. (8.11)
discrete values. Though this extra information cannot enhance root mean squared error
(RMSE) performance, other measurements, such as mean absolute error (MAE), can proﬁt
from it.
Given the hidden variables {hmn} and using the likelihood in Equation (8.3), the ex-
pected value of the category value for (m, n)-th entry is
A
X
a=1
a · p(a | hmn) =
A
X
a=1
a ·
 Φ(hmn −ba) −Φ(hmn −ba+1)

=
A
X
a=1
Φ(hmn −ba) −AΦ(hmn −bA+1).
Following likelihood in Equation (8.6) and integrating out hmn, we have
ymn :=
A
X
a=1
a · p(a | wm, zn, τ) =
A
X
a=1
Φ
w⊤
mzn −ba
√
1 + τ −1

.
(8.21)
Therefore, instead of using the score in Equation (7.8) (p. 189), the score E[ymn | A] can
be obtained by averaging the values of Equation (8.21) during the Gibbs sampling process.
Similar to the third recommendation system introduced in Section 7.4 (p. 188), the
OGGW model can also provide uncertainty about each entry in A. Adopting again the idea
of the Sharpe ratio, we can suggest the unconsumed movie m (in the Netﬂix context) when
amn for user n by the uncertainty-adjusted score of posterior expected Poisson parameters,
scoremn =
E[ymn | A]
p
Var[ymn | A]
.
196

JUN LU
CHAPTER 8. BAYESIAN ORDINAL MATRIX FACTORIZATION
197

9
Bayesian Interpolative Decomposition
Contents
9.1
Interpolative Decomposition (ID) . . . . . . . . . . . . . . . . .
199
9.2
Existence of the Column Interpolative Decomposition . . . . .
201
9.3
Skeleton/CUR Decomposition . . . . . . . . . . . . . . . . . . .
205
9.4
Row ID and Two-Sided ID
. . . . . . . . . . . . . . . . . . . . .
207
9.5
Bayesian Low-Rank Interpolative Decomposition . . . . . . . .
208
9.6
Gibbs Sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
212
9.7
Aggressive Update
. . . . . . . . . . . . . . . . . . . . . . . . . .
214
9.8
Post-Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
9.9
Bayesian ID with Automatic Relevance Determination . . . .
215
9.10
Examples for Bayesian ID . . . . . . . . . . . . . . . . . . . . . .
217
9.10.1
Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . 219
9.10.2
Convergence and Comparative Analysis
. . . . . . . . . . . . . . 219
9.11
Bayesian Intervened Interpolative Decomposition (IID) . . . .
220
9.11.1
Quantitative Problem Statement
. . . . . . . . . . . . . . . . . . 220
Formulaic Alphas . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
Evaluation Metrics
. . . . . . . . . . . . . . . . . . . . . . . . . . 222
9.11.2
Examples for Bayesian IID . . . . . . . . . . . . . . . . . . . . . . 222
Convergence and Comparative Analysis . . . . . . . . . . . . . . . 224
Quantitative Strategy . . . . . . . . . . . . . . . . . . . . . . . . . 225
198

JUN LU
CHAPTER 9. BAYESIAN INTERPOLATIVE DECOMPOSITION
9.1. Interpolative Decomposition (ID)
Low-rank real-valued or nonnegative matrix factorization is essential in modern data sci-
ence. Low-rank matrix approximation with respect to the Frobenius norm - minimizing
the sum squared diﬀerences to the target matrix - can be easily solved with singular value
decomposition (SVD) or the Bayesian real-valued/nonnegative matrix decomposition meth-
ods. For many applications, however, it is sometimes advantageous to work with a basis
that consists of a subset of the columns from the observed matrix itself (Halko et al., 2011;
Martinsson et al., 2011). The interpolative decomposition (ID) provides one such approxi-
mation. The distinguishing feature of the ID is that we can reuse columns from the original
matrix. This enables it to preserve matrix properties such as sparsity and nonnegativity
that also help reduce memory usage.
ID is widely used as a feature selection tool that extracts the essence and allows dealing
with big data that may originally be too large to ﬁt into RAM. In addition, we can remove
the non-relevant parts of the data which consist of errors and redundant information via
these methods (Liberty et al., 2007; Halko et al., 2011; Martinsson et al., 2011; Arı et al.,
2012; Lu, 2022b; Lu and Osterrieder, 2022). Locating the indices associated with the span-
ning columns is frequently valuable for the purpose of data interpretation and analysis. It
can be very useful to identify a subset of the columns that distills the information in the
matrix. When the columns of the observed matrix have some speciﬁc interpretations, e.g.,
they are transactions in a transaction data set, the columns of the factored matrix in ID
will retain the same meaning as well.
The column ID 1 factors a matrix into the product of two matrices, one of which con-
sists of selected columns from the original matrix, and the other of which contains a subset
of columns consisting of the identity matrix with all its values having absolute values no
greater than 1. We ﬁrst state and demonstrate the existence of the exact ID in the fol-
lowing theorem, and we will later describe the low-rank ID through Bayesian approaches.
Theorem 9.1: (Column Interpolative Decomposition)
Any rank-R matrix A ∈
Rm×n can be factored as
A
M×N =
C
M×R
W
R×N,
where C ∈RM×R is some R linearly independent columns of A, W ∈RR×N is the matrix
to reconstruct A which contains an R × R identity submatrix (under a mild column
permutation). Speciﬁcally, entries in W have values no larger than 1 in magnitude:
max |wij| ≤1, ∀i ∈[1, R], j ∈[1, N].
The storage required for the decomposition is then reduced or potentially increased from
MN ﬂoating-point numbers to MR, (N −R)R ﬂoats for storing C, W respectively and
extra R integers are required to remember the position of each column of C within A.
1. The column ID will simply be referred to as ID without further clariﬁcation.
199

9.1. INTERPOLATIVE DECOMPOSITION (ID)

N
M
A

R
M
C

N
R
W 

Figure 9.1: Demonstration of the column ID of a matrix where the yellow vector denotes
the linearly independent columns of A, white entries denote zero, and purple entries denote
one.
While we claim that entries in W have magnitudes no greater than 1, a weaker construc-
tion assumes that no entry of W has an absolute value exceeding 2. The illustration of the
column ID is shown in Figure 9.1 where the yellow vectors denote the linearly independent
columns of A and the purple vectors in W form an R × R identity submatrix. The posi-
tions of the purple vectors inside W are identical to the placements of the corresponding
yellow vectors within A. The column ID is very similar to the CR decomposition, both
select R linearly independent columns into the ﬁrst factor and the second factor comprises
an R × R identity submatrix (Strang, 2021; Strang and Moler, 2022; Lu, 2021b). The dif-
ference between the two is that the CR decomposition precisely selects the ﬁrst R linearly
independent columns into the ﬁrst factor and the identity submatrix appears in the pivot
positions. And more importantly, the second factor in the CR decomposition comes from
the reduced row echelon form (RREF). Therefore, the column ID can also be utilized in the
same applications as the CR decomposition, say proving the rank equals trace property in
idempotent matrices (Lu, 2021b), and demonstrating the elementary theorem in linear al-
gebra that the column rank equals row rank of a matrix (Lu, 2021a). Moreover, the column
ID is also a special case of the rank decomposition and is not unique (Lu, 2021b).
Notations that will be extensively used in the sequel.
Following again the Matlab-
style notation, if J is an index vector with size R that contains the indices of columns
selected from A into C, then C can be denoted as C = A[:, J] (Deﬁnition 0.0.1, p. 7). The
matrix C contains “skeleton” columns of A. From the “skeleton” index vector J, the R×R
identity matrix inside W can be recovered by
W [:, J] = IR ∈RR×R.
Suppose further we put the remaining indices of A into an index vector I where
J ∩I = ∅
and
J ∪I = {1, 2, . . . , N}.
The remaining N −R columns in W consist of an R × (N −R) expansion matrix since the
matrix contains expansion coeﬃcients to reconstruct the columns of A from C:
E = W [:, I] ∈RR×(N−R),
200

JUN LU
CHAPTER 9. BAYESIAN INTERPOLATIVE DECOMPOSITION
where the entries of E are known as the expansion coeﬃcients. Moreover, let P ∈RN×N
be a (column) permutation matrix (Deﬁnition 0.0.13, p. 10) deﬁned by P = IN[:, (J, I)]
such that
AP = A[:, (J, I)] = [C, A[:, I]] ,
and
W P = W [:, (J, I)] = [IR, E]
leads to
−−−−−→
W = [IR, E] P ⊤.
(9.1)
9.2. Existence of the Column Interpolative Decomposition
Cramer’s rule.
The proof of the existence of the column ID relies on Cramer’s rule which
will be brieﬂy covered in the following discussion. Cramer’s rule is an explicit formula for
the solution of a system of linear equations with as many equations as unknowns, and it is
valid whenever the system has a unique solution, i.e., the underlying matrix is nonsingular.
Consider a system of n linear equations for n unknowns, represented in the following matrix
multiplication form:
Mx = l,
where M ∈Rn×n is nonsingular and x, l ∈Rn. Then the theorem states that, in this case,
the system has a unique solution, whose individual values for the unknowns are given by:
xi = det(Mi)
det(M) ,
for all
i ∈{1, 2, . . . , n},
where Mi is the matrix formed by replacing the i-th column of M with the column vector
l. In full generality, Cramer’s rule considers the matrix equation
MX = L,
where M ∈Rn×n is nonsingular and X, L ∈Rn×m.
Let Ic = [i1, i2, . . . , ik] and Jc =
[j1, j2, . . . , jk] be two index vectors where 1 ≤i1 ≤i2 ≤. . . ≤ik ≤n and 1 ≤j1 ≤j2 ≤
. . . ≤jk ≤m. Then X[Ic, Jc] is a k × k submatrix of X. Let further ML(Ic, Jc) be the
n × n matrix formed by replacing the (is)-th column of M with (js)-th column of L for all
s ∈{1, 2, . . . , k}. Then
det(X[Ic, Jc]) = det (ML(Ic, Jc))
det(M)
.
When Ic and Jc are of size 1, it follows that
xij = det (ML(i, j))
det(M)
.
(9.2)
Now we are ready to prove the existence of the column ID.
Proof [of Theorem 9.1] We have mentioned above the proof relies on the Cramer’s rule.
If we can show the entries of W can be denoted by the Cramer’s rule equality in Equa-
tion (9.2) and the numerator is smaller than the denominator, then we can complete the
proof. However, we notice that the matrix in the denominator of Equation (9.2) is a square
matrix. Here comes the trick.
201

9.2. EXISTENCE OF THE COLUMN INTERPOLATIVE DECOMPOSITION
Step 1: column ID for full row rank matrix.
For a start, we ﬁrst consider the full
row rank matrix A (which implies R = M, M ≤N, and A ∈RR×N such that the matrix
C ∈RR×R is a square matrix in the column ID A = CW that we want). Determine the
“skeleton” index vector J by
J = arg max
Jt
{|det(A[:, Jt])| : Jt is a subset of {1, 2, . . . , N} with size R = M} ,
(9.3)
i.e., J is the index vector that is determined by maximizing the magnitude of the determi-
nant of A[:, Jt]. As we have discussed in the last section, there exists a (column) permutation
matrix such that
AP =

A[:, J]
A[:, I]

.
Since C = A[:, J] has full column rank R = M, it is then nonsingular. The above equation
can be rewritten as
A =

A[:, J]
A[:, I]

P ⊤
= A[:, J]

IR
A[:, J]−1A[:, I]

P ⊤
= C

IR
C−1A[:, I]

P ⊤
|
{z
}
W
,
where the matrix W is given by

IR
C−1A[:, I]

P ⊤=

IR
E

P ⊤by Equation (9.1).
To prove the claim that the magnitude of W is no larger than 1 is equivalent to proving
that entries in E = C−1A[:, I] ∈RR×(N−R) are no greater than 1 in absolute value.
Deﬁne the index vector [j1, j2, . . . , jN] as a permutation of [1, 2, . . . , N] such that
[j1, j2, . . . , jN] = [1, 2, . . . , N]P = [J, I].2
Thus, it follows from CE = A[:, I] that
[aj1, aj2, . . . , ajR]
|
{z
}
=C=A[:,J]
E = [ajR+1, ajR+2, . . . , ajN ]
|
{z
}
=A[:,I]:=B
,
where ai is the i-th column of A and we let B = A[:, I]. Therefore, by Cramer’s rule in
Equation (9.2), we have
ekl = det (CB(k, l))
det (C)
,
(9.4)
where ekl is the entry (k, l) of E and CB(k, l) is the R × R matrix formed by replacing the
k-th column of C with the l-th column of B. For example,
e11 = det
 [ajr+1, aj2, . . . , ajr]

det ([aj1, aj2, . . . , ajr]) ,
e12 = det
 [ajr+2, aj2, . . . , ajr]

det ([aj1, aj2, . . . , ajr]) ,
e21 = det
 [aj1, ajr+1, . . . , ajr]

det ([aj1, aj2, . . . , ajr]) ,
e22 = det
 [aj1, ajr+2, . . . , ajr]

det ([aj1, aj2, . . . , ajr]) .
Since J is chosen to maximize the magnitude of det(C) in Equation (9.3), it follows that
|ekl| ≤1,
for all
k ∈{1, 2, . . . , R}, l ∈{1, 2, . . . , N −R}.
2. Note here [j1, j2, . . . , jN], [1, 2, . . . , N], J, and I are row vectors.
202

JUN LU
CHAPTER 9. BAYESIAN INTERPOLATIVE DECOMPOSITION
Step 2: apply to general matrices.
To summarize what we have proved above and to
abuse the notation. For any matrix F ∈RR×N with full rank R ≤N, the column ID exists
that F = C0W where the values in W are no greater than 1 in absolute value.
Apply the ﬁnding to the full general matrix A ∈RM×N with rank R ≤{M, N}, it is
trivial that the matrix A admits a rank decomposition:
A
M×N =
D
M×R
F
R×N,
where D and F have full column rank R and full row rank R respectively (Lu, 2021b). For
the column ID of F = C0W where C0 = F [:, J] contains R linearly independent columns
of F . We notice by A = DF such that
A[:, J] = DF [:, J],
i.e., the columns indexed by J of (DF ) can be obtained by DF [:, J] which in turn are the
columns of A indexed by J. This makes
A[:, J]
| {z }
C
= DF [:, J]
|
{z
}
DC0
and
A = DF = DC0W = DF [:, J]
|
{z
}
C
W = CW .
This completes the proof.
The above proof reveals an intuitive way for computing the optimal column ID of matrix
A as shown in Algorithm 22. Nevertheless, any algorithm that is guaranteed to ﬁnd such
an optimally-conditioned factorization must have combinatorial complexity (Martinsson,
2019).
In the next sections, we will consider alternative ways to ﬁnd a relatively well-
conditioned factorization.
Example 9.1 (Compute the Column ID) Given a matrix
A =


56
41
30
32
23
18
80
59
42


with rank 2, the trivial process for computing the column ID of A is shown as follows. We
ﬁrst ﬁnd a rank decomposition
A = DF =


1
0
0
1
2
−1


56
41
30
32
23
18

.
Since rank R = 2, J is one of [1, 2], [0, 2], [0, 1] where the absolute determinant of F [:, J] are
48, 48, 24 respectively. We proceed by choosing J = [0, 2]:
e
C = F [:, J] =
56
30
32
18

,
M
= F [:, I] =
41
23

.
203

9.2. EXISTENCE OF THE COLUMN INTERPOLATIVE DECOMPOSITION
Algorithm 22 An Intuitive Method to Compute the Column ID
Require: Rank-Rr matrix A with size M × N;
1: Compute the rank decomposition
A
M×N =
D
M×R
F
R×N such as from a UTV decomposi-
tion (Lu, 2021b);
2: Compute column ID of F : F = F [:, J]W = e
CW :
2.1.



J = arg max
J
{|det(F [:, J])| : J is a subset of {1, 2, . . . , N} with size R} ;
I = {1, 2, . . . , N}\J;
2.2.
( e
C = F [:, J];
M = F [:, I];
2.3. F P = F [:, (J, I)] to obtain permutation matrix P ;
2.4. ekl =
det

e
CM(k, l)

det

e
C

,
for all
k ∈[1, R], l ∈[1, N −R] (Equation (9.4));
2.5. W = [IR, E]P ⊤(Equation (9.1)).
3: C = A[:, J];
4: Output the column ID A = CW ;
And
F P = F [: (J, I)] = F [:, (0, 2, 1)]
leads to
−−−−−→
P =


1
1
1

.
In this example, E ∈R2×1:
e11 = det
41
30
23
18
 
det
56
30
32
18

= 1;
e21 = det
56
41
32
23
 
det
56
30
32
18

= −1
2.
This makes
E =
 1
−1
2

leads to
−−−−−→
W = [I2, E]P ⊤=
1
1
0
0
−1
2
1

.
The ﬁnal selected columns are
C = A[:, J] =


56
30
32
18
80
42

.
The net result is given by
A = CW =


56
30
32
18
80
42


1
1
0
0
−1
2
1

,
204

JUN LU
CHAPTER 9. BAYESIAN INTERPOLATIVE DECOMPOSITION
where entries of W are no greater than 1 in absolute value as we want.
□
To end up this section, we discuss the source of the non-uniqueness in the column ID.
Remark 9.2: Non-uniqueness of the Column ID
In the above speciﬁc example 9.1, we notice the determinant for F [:, (1, 2)] and F [:, (0, 2)]
both get the maximal absolute determinant. Therefore, both of them can result in a
column ID of A. Whilst, we only select J from [1, 2], [0, 2], [0, 1]. When the J is ﬁxed
from the maximal absolute determinant search, any permutation of it can also be selected,
e.g., J = [0, 2] or J = [2, 0] are both good. The two choices on the selection of the column
index search yield the non-uniqueness of the column ID.
9.3. Skeleton/CUR Decomposition
To delve deeper into the topic of ID, we ﬁrst present the rigorous form of a related decom-
position known as the CUR or skeleton decomposition.
Theorem 9.3: (Skeleton Decomposition)
Any rank-R matrix A ∈RM×N can be
factored as
A
M×N =
C
M×R
U −1
R×R
R
R×N,
where C contains some R linearly independent columns of A, R contains some R linearly
independent rows of A, and U is the nonsingular submatrix on the intersection of C and
R.
• The storage for the decomposition is then reduced or potentially increased from MN
ﬂoats to R(M + N) + R2 ﬂoats.
• Or further, if we only record the position of the indices, it requires MR, NR ﬂoats
for storing C, R respectively and extra 2R integers to remember the position of each
column of C in that of A and each row of R in that of A (i.e., construct U from
C, R).

N
M
A

R
M
C

N
R
R 


1

1

R
R
U
Figure 9.2: Demonstration of the skeleton decomposition of a matrix where the yellow
vectors denote the linearly independent columns of A, and green vectors denote the linearly
independent rows of A.
205

9.3. SKELETON/CUR DECOMPOSITION
The skeleton decomposition is also referred to as the CUR decomposition following from
the notation in the decomposition. Compared to singular value decomposition (SVD), CUR
is better in terms of reiﬁcation issues since it uses the actual columns (rows) of the matrix
whereas SVD uses some artiﬁcial singular vectors that may not represent physical reality
(Mahoney and Drineas, 2009). Moreover, CUR maintains sparsity if the data is sparse. On
the other hand, similar to SVD, CUR decomposition can be used as a tool for data compres-
sion, feature extraction, or data analysis in many application areas (Mahoney and Drineas,
2009; An et al., 2012; Lee and Choi, 2008). The illustration of the skeleton decomposition
is shown in Figure 9.2 where the yellow vectors denote the linearly independent columns of
A and green vectors denote the linearly independent rows of A. Speciﬁcally, given index
vectors I, J both with size R that contain the indices of rows and columns selected from A
into R and C respectively, U can be denoted as U = A[I, J] (see Deﬁnition 0.0.1, p. 7).
Existence of the skeleton decomposition.
In linear algebra, the row rank and the
column rank of a matrix are equal. In another word, we can also claim that the dimension
of the column space and the dimension of the row space are equal (Lu, 2021a).
This
property is essential for proving the existence of the skeleton decomposition. The proof is
rather elementary.
Proof [of Theorem 9.3] The proof relies on the existence of such nonsingular matrix U
which is central to this decomposition method.
Existence of such nonsingular matrix U.
Since matrix A is of rank R, we can pick
R columns from A such that they are linearly independent. Suppose we put the speciﬁc
R linearly independent columns ai1, ai2, . . . , aiR into the columns of an M × R matrix
N = [ai1, ai2, . . . , aiR] ∈RM×R. The dimension of the column space of N is R so that
the dimension of the row space of N is also R. Again, we can pick R linearly independent
rows n⊤
j1, n⊤
j2, . . . , n⊤
jR from N and put the speciﬁc R rows into rows of an R × R matrix
U = [n⊤
j1; n⊤
j2; . . . ; n⊤
jR] ∈RR×R. Again, the dimension of the column space of U is also R
which means there are R linearly independent columns from U. So U is such a nonsingular
matrix with size R × R.
Main proof.
As long as we ﬁnd the nonsingular R × R matrix U inside A, we can ﬁnd
the existence of the skeleton decomposition as follows.
Suppose U = A[I, J] where I, J are index vectors of size R. Since U is a nonsingular
matrix, the columns of U are linearly independent. Thus the columns of matrix C based
on the columns of U are also linearly independent (i.e., select the R columns of A with
the same entries of the matrix U. Here C is equal to the N we construct above, and
C = A[:, J]).
As the rank of the matrix A is R, if we take any other column ai of A, ai can be
represented as a linear combination of the columns of C, i.e., there exists a vector x such
that ai = Cx, for all i ∈{1, 2, . . . , N}. Let R rows (entries) of ai ∈RN corresponding to
the row entries of U be ri ∈RR for all i ∈{1, 2, . . . , N} (i.e., ri contains R entries of ai).
That is, select the R entries of ai’s corresponding to the entries of U as follows:
A = [a1, a2, . . . , aN] ∈RM×N
−→
A[I, :] = [r1, r2, . . . , rN] ∈RR×N.
Since ai = Cx, U is a submatrix inside C, and ri is a subvector inside ai, we have ri = Ux
which states that x = U −1ri. Thus for every i ∈{1, 2, . . . , N}, we have ai = CU −1ri.
206

JUN LU
CHAPTER 9. BAYESIAN INTERPOLATIVE DECOMPOSITION
Combine the n columns of such ri into R = [r1, r2, . . . , rN], we obtain
A = [a1, a2, . . . , aN] = CU −1R,
from which the result follows.
In short, we ﬁrst ﬁnd R linearly independent columns of A into C ∈RM×R. From C,
we ﬁnd an R × R nonsingular submatrix U. The R rows of A corresponding to entries of
U can help to reconstruct the columns of A.
We note in the case where A is square and invertible, we have the skeleton decomposition
A = CU −1R where C = R = U = A such that the decomposition reduces to A =
AA−1A.
9.4. Row ID and Two-Sided ID
We term the interpolative decomposition above as column ID. This is no coincidence since
it has its siblings.
Theorem 9.4: (The Whole Interpolative Decomposition)
Any rank-R matrix A ∈
RM×N can be factored as
Column ID:
A
M×N
=
C
M×R
W
R×N;
Row ID:
= Z
M×R
R
R×N ;
Two-Sided ID:
= Z
M×R
U
R×R
W
R×N,
where
• C = A[:, J] ∈RM×R is some R linearly independent columns of A, W ∈RR×N is
the matrix to reconstruct A which contains an R × R identity submatrix (under a
mild column permutation): W [:, J] = IR;
• R = A[S, :] ∈RR×N is some R linearly independent rows of A, Z ∈RM×R is the
matrix to reconstruct A which contains an R × R identity submatrix (under a mild
row permutation): Z[S, :] = IR;
• Entries in W , Z have values no larger than 1 in magnitude: max |wij| ≤1 and
max |zij| ≤1;
• U = A[S, J] ∈RR×R is the nonsingular submatrix on the intersection of C and R;
• Skeleton decomposition: the three matrices C, R, U in the boxed texts share
same notation as the skeleton decomposition (Theorem 9.3, p. 205) where they even
have same meanings such that the three matrices make the skeleton decomposition
of A: A = CU −1R.
The proof of the row ID is just similar to that of the column ID. Suppose the column ID
of A⊤is given by A⊤= C0W0 where C0 contains R linearly independent columns of A⊤
(i.e., R linearly independent rows of A). Let R = C⊤
0 , Z = W ⊤
0 , the row ID is obtained by
A = ZR.
207

9.5. BAYESIAN LOW-RANK INTERPOLATIVE DECOMPOSITION
For the two-sided ID, recall from the skeleton decomposition (Theorem 9.3, p. 205).
When U is the intersection of C and R, it follows that A = CU −1R. Thus CU −1 = Z by
the row ID. And this implies C = ZU. By column ID, it follows that A = CW = ZUW
which proves the existence of the two-sided ID.
Data storage.
For the data storage of each ID, we summarize as follows:
• Column ID. It requires MR and (N −R)R ﬂoats to store C and W respectively, and
R integers to store the indices of the selected columns in A;
• Row ID. It requires NR and (M −R)R ﬂoats to store R and Z respectively, and R
integers to store the indices of the selected rows in A;
• Two-Sided ID. It requires (M −R)R, (N −R)R, and R2 ﬂoats to store Z, W , and U
respectively. And extra 2R integers are required to store the indices of the selected
rows and columns in A.
Further reduction on the storage of two-sided ID for sparse matrix A.
Suppose
the column ID of A = CW where C = A[:, J] and a good spanning rows index S set of C
could be found:
A[S, :] = C[S, :]W .
We observe that C[S, :] = A[S, J] ∈RR×R which is nonsingular (since full rank R in the
sense of both row rank and column rank). It follows that
W = (A[S, J])−1A[S, :].
Therefore, there is no need to store the matrix W explicitly. We only need to store A[S, :]
and (A[S, J])−1. Alternatively, when we are able to compute the inverse of A[S, J] on the
ﬂy, it only requires R integers to store J and recover A[S, J] from A[S, :]. The storage of
A[S, :] is inexpensive if A is sparse.
9.5. Bayesian Low-Rank Interpolative Decomposition
The low-rank ID problem of the observed matrix A can be stated as A = CW + E,
where A = [a1, a2, . . . , aN] ∈RM×N is approximately factorized into an M × K matrix
C ∈RM×K containing K basis columns of A and a K ×N matrix W ∈RK×N with entries
no larger than 1 in magnitude; the noise is captured by matrix E ∈RM×N. The value of
K is smaller than the rank R, hence the term low-rank ID. There are many methods to
compute the low-rank ID of a matrix. The most popular algorithm to compute the low-rank
ID approximation is the randomized ID (RID) algorithm (Liberty et al., 2007). At a high
level, the algorithm randomly samples S > K columns from A, uses column-pivoted QR
(CPQR) to select K of those S columns for basis matrix C, and then computes W via
least squares (Lu, 2021b). S is usually set to S = 1.2K to oversample the columns so as to
capture a large portion of the range of A.
The drawback of the randomized algorithm is that the maximal magnitude of the fac-
tored component W may exceed 1. Though this is not a big issue in many applications,
it has potential problems for other applications that have a high requirement for numerical
stability. Advani and O’Hagan (2021) even report that the randomized ID may have a
magnitude larger than 167 making it less numerically stable. While probabilistic models
208

JUN LU
CHAPTER 9. BAYESIAN INTERPOLATIVE DECOMPOSITION
can easily accommodate constraints on the speciﬁc range of the factored matrix. In this
light, we focus on the Bayesian ID (BID) of underlying matrices. The Bayesian ID algo-
rithms are proposed in Lu (2022b,c) and is further adapted in a feature selection context
by Lu and Osterrieder (2022). Training such models amounts to ﬁnding the best rank-K
approximation to the observed M × N target matrix A under the given loss function. Let
r ∈{0, 1}N be the state vector with each element indicating the type of the corresponding
column, i.e., basis column or interpolated (remaining) column: if rn = 1, then the n-th
column an is a basis column; if rn = 0, then an is interpolated using the basis columns plus
some error term. Suppose further J is the set of the indices of the selected basis columns
(with size K now), I is the set of the indices of the interpolated columns (with size N −K)
such that
J ∩I = ∅,
J ∪I = {1, 2, . . . , N};
J = J(r) = {n | rn = 1}N
n=1,
I = I(r) = {n | rn = 0}N
n=1.
Then C can be described as C = A[:, J] where the colon operator implies all indices. The
approximation A ≈CW can be equivalently stated that
A
M×N ≈
C
M×K
W
K×N =
X
M×N
Y
N×N
where X ∈RM×N and Y ∈RN×N with
X[:, J] = C ∈RM×K,
X[:, I] = 0 ∈RM×(N−K);
Y [J, :] = W ∈RK×N,
Y [I, :] = random matrix ∈R(N−K)×N.
We also notice that there exists an identity matrix IK ∈RK×K in W and Y :
IK = W [:, J] = Y [J, J].
(9.5)
To ﬁnd the low-rank ID of A ≈CW then can be transformed into the problem of ﬁnding the
A ≈XY with state vector r recovering the submatrix C (see Figure 9.3). To evaluate the
approximation, reconstruction error measured by mean squared error (MSE or Frobenius
norm) is minimized:
min
W ,Z
1
MN
N
X
n=1
M
X
m=1

amn −x⊤
myn
2
,
(9.6)
where xm, yn are the m-th row and n-th column of X, Y respectively. We approach
the magnitude constraint in W and Y by considering the Bayesian ID model as a latent
factor model and we describe a fully speciﬁed graphical model for the problem and employ
Bayesian learning methods to infer the latent factors. In this sense, explicit magnitude
constraints are not required on the latent factors, since this is naturally taken care of by
the appropriate choice of prior distribution; here we use a general-truncated-normal (GTN)
prior (Deﬁnition 2.4.2, p. 53).
Bayesian GBT and GBTN models for ID.
We then introduce the Bayesian ID model
called the GBT model. To further promote ﬂexibility and insensitivity in the hyperparame-
ter choices, we also introduce the hierarchical model known as the GBTN algorithm, which
has simple conditional density forms with little extra computation. We further provide an
example to show that the method can be successfully applied to the large, sparse, and very
imbalanced Movie-User data set, containing 100,000 user/movie ratings.
209

9.5. BAYESIAN LOW-RANK INTERPOLATIVE DECOMPOSITION

N
M
A

N
M
X

N
N
Y 


K
M
C

N
K
W 

Figure 9.3: Demonstration of the interpolative decomposition of a matrix where the yellow
vector denotes the basis columns of matrix A, white entries denote zero, purple entries
denote one, blue and black entries denote elements that are not necessarily zero.
The
Bayesian ID models ﬁnd the approximation A ≈XY and the post-processing procedure
calculates the approximation A ≈CW .
m = 1..M
n = 1..N
k = 1..N
l = 1..N
X/r
amn
ykl
ατ/ασ
βτ/βσ
τ/σ2
p(r)
µkl, τkl
a, b
(a) GBT.
m = 1..M
n = 1..N
k = 1..N
l = 1..N
X/r
amn
ykl
ατ/ασ
βτ/βσ
τ/σ2
p(r)
µkl, τkl
a, b
µµ, τµ
αt, βt
(b) GBTN.
Figure 9.4: Graphical representation of GBT and GBTN models. Orange circles represent
observed and latent variables, green circles denote prior variables, and plates represent
repeated variables. The slash “/” in the variable represents “or”, and the comma “,” in the
variable represents “and”. Parameters a and b are ﬁxed with a = −1 and b = 1 in our case;
while a weaker construction can set them to a = −2, b = 2.
Likelihood.
We view the data A as being produced according to the probabilistic gen-
erative process shown in Figure 9.4.
The observed (m, n)-th entry amn of matrix A is
modeled using a Gaussian likelihood function with variance σ2 and mean given by the
210

JUN LU
CHAPTER 9. BAYESIAN INTERPOLATIVE DECOMPOSITION
latent decomposition x⊤
myn (Equation (9.6)),
p(amn | x⊤
myn, σ2) = N(amn | x⊤
myn, σ2);
p(A | θ) =
M,N
Y
m,n=1
N
 amn | (XY )mn, σ2
=
M,N
Y
m,n=1
N
 amn | (XY )mn, τ −1
,
(9.7)
where θ = {X, Y , σ2} denotes all parameters in the model, N(· | ·) is a Gaussian distribu-
tion, σ2 is the variance, and τ −1 = σ2 is the precision.
Prior.
We choose a conjugate prior over the data variance, an inverse-Gamma distribution
with shape ασ and scale βσ,
p(σ2 | ασ, βσ) = G−1(σ2 | ασ, βσ).
(9.8)
While it can also be equivalently given a conjugate Gamma prior G(τ | ατ, βτ) over the
precision τ =
1
σ2 and we shall not repeat the details (see Equation (5.17), p. 136 in GGG
model).
We treat the latent variables ykl’s as random variables (with k, l ∈{1, 2, . . . , N}, see
Figure 9.4). And we need prior densities over these latent variables to express beliefs for
their values, e.g., constraint with magnitudes smaller than 1 in this context. Here we assume
further that the latent variable ykl’s are independently drawn from a general-truncated-
normal (GTN) prior (Deﬁnition 2.4.2, p. 53),
p(ykl | ·) = GT N(ykl | µkl, (τkl)−1, a = −1, b = 1)
=
p τkl
2π exp{−τkl
2 (ykl −µkl)2}
Φ
 (b −µkl) · √τkl

−Φ
 (a −µkl) · √τkl
 · 1(a ≤ykl ≤b),
(9.9)
where 1(a ≤x ≤b) is a step function that has a value of 1 when a ≤x ≤b, and 0 when
x < a or a > b. This prior serves to enforce the constraint on the components Y with
no entry of Y having an absolute value greater than 1, and is conjugate to the Gaussian
likelihood (Equation (2.19), p. 53). The posterior density is also a GTN distribution. While
in a weaker construction of interpolative decomposition, the constraint on the magnitude
can be relaxed to 2; the GTN prior is ﬂexible in that the parameters can adapt to this
change by adjusting to a = −2, b = 2 accordingly.
Hierarchical prior.
To further favor ﬂexibility, we choose a convenient joint hyperprior
density over the parameters {µkl, τkl} of GTN prior in Equation (9.9), namely, the GTN-
scaled-normal-Gamma (GTNSNG) prior,
p(µkl, τkl | ·) = GT NSNG(µkl, τkl | µµ, 1
τµ
, αt, βt)
=

Φ((b −µµ) · √τµ) −Φ((a −µµ) · √τµ)
	
· N(µkl | µµ, (τµ)−1) · G(τkl | αt, βt).
(9.10)
See Figure 9.4(b). This prior can decouple parameters µkl, τkl, and the posterior conditional
densities of them are normal and Gamma respectively due to this convenient scale.
211

9.6. GIBBS SAMPLER
Terminology.
Following again the Bayesian matrix factorization terminology in Sec-
tion 5.1 (p. 127), the Bayesian ID models are referred to as the GBT and GBTN models
where the letter “B” stands for Beta-Bernoulli density intrinsically.
9.6. Gibbs Sampler
In this section, we provide the derivation of the Gibbs sampler for the discussed Bayesian
ID models.
Update of latent variables.
The conditional posterior density of ykl is a GTN distribu-
tion. Denote all elements of Y except ykl as Y−kl, and following the graphical representation
of the GBT (or the GBTN) model shown in Figure 9.4, the conditional posterior density of
ykl can be obtained by
p(ykl | A, X, Y−kl, µkl, τkl, σ2) ∝p(A | X, Y , σ2) · p(ykl | µkl, τkl)
=
M,N
Y
i,j=1
N

aij | x⊤
i yj, σ2
× GT N(ykl | µkl, (τkl)−1, a = −1, b = 1)
∝exp


−1
2σ2
M,N
X
i,j=1
(aij −x⊤
i yj)2


exp
n
−τkl
2 (ykl −µkl)2o
u(ykl | a, b)
∝exp
(
−1
2σ2
M
X
i
(ail −x⊤
i yl)2
)
exp
n
−τkl
2 (ykl −µkl)2o
u(ykl | a, b)
∝exp


−1
2σ2
M
X
i

x2
iky2
kl + 2xikykl
  N
X
j̸=k
xijyjl −ail


exp{−τkl
2 (ykl −µkl)2}u(ykl | a, b)
∝exp













−y2
kl
 PM
i x2
ik
2σ2
+ τkl
2
!
|
{z
}
1
2 eτ
+ykl
 1
σ2
M
X
i
xik
 ail −
N
X
j̸=k
xijyjl

+ τklµkl

|
{z
}
eτ·eµ













u(ykl | a, b)
∝N(ykl | eµ, (eτ)−1)u(ykl | a, b) ∝GT N(ykl | eµ, (eτ)−1, a = −1, b = 1),
(9.11)
where again, for simplicity, we assume the rows of X are denoted by xi’s and columns
of Y are denoted by yj’s, eτ =
PM
i
x2
ik
σ2
+ τkl is the posterior “parent” precision of the GTN
density, and the posterior “parent” mean of the GTN density is
eµ =
 1
σ2
M
X
i
xik
 ail −
N
X
j̸=k
xijyjl

+ τklµkl

eτ.
Update of variance parameter.
The conditional density of σ2 is an inverse-Gamma
distribution by conjugacy,
p(σ2 | X, Y , A) = G−1(σ2 | f
ασ, f
βσ),
(9.12)
where f
ασ = MN
2
+ ασ, f
βσ = 1
2
PM,N
i,j=1(aij −x⊤
i yj)2 + βσ.
212

JUN LU
CHAPTER 9. BAYESIAN INTERPOLATIVE DECOMPOSITION
Update of state vector for GBT and GBTN without ARD.
Suppose further that
r ∈{0, 1}N is the state vector with each element indicating the type of the corresponding
column.
If rn = 1, then an is a basis column; otherwise, an is interpolated using the
basis columns plus some error term. Given the state vector r = [r1, r2, . . . , rN]⊤∈RN,
the relation between r and the index sets J is simple; J = J(r) = {n | rn = 1}N
n=1 and
I = I(r) = {n | rn = 0}N
n=1. A new value of state vector r is to select one index j from
index set J and another index i from index set I (we note that rj = 1 and ri = 0 for the
old state vector r) such that
j ∈J,
i ∈I;
oj = p(rj = 0, ri = 1 | A, σ2, Y , r−ji)
p(rj = 1, ri = 0 | A, σ2, Y , r−ji)
= p(rj = 0, ri = 1)
p(rj = 1, ri = 0) × p(A | σ2, Y , r−ji, rj = 0, ri = 1)
p(A | σ2, Y , r−ji, rj = 1, ri = 0),
(9.13)
where r−ji denotes all elements of r except j-th and i-th entries. Under a weak construction,
we can set p(rj = 0, ri = 1) = p(rj = 1, ri = 0). Then the full conditional probability of
p(rj = 0, ri = 1 | A, σ2, Y , r−ji) can be calculated by
p(rj = 0, ri = 1 | A, σ2, Y , r−ji) =
oj
1 + oj
.
(9.14)
Extra update for GBTN model.
Following the conceptual representation of the GBTN
model in Figure 9.4, the conditional density of µkl can be obtained by
p(µkl | τkl, µµ, τµ, αt, βt, ykl)
∝GT N(ykl | µkl, (τkl)−1, a = −1, b = 1) · GT NSNG(µkl, τkl | µµ, (τµ)−1, αt, βt)
∝GT N(ykl | µkl, (τkl)−1, a = −1, b = 1) ·

Φ((b −µµ) · √τµ) −Φ((a −µµ) · √τµ)
	
· N(µkl | µµ, (τµ)−1) ·(((((((
G(τkl | αt, βt)
∝√τkl · exp
n
−τkl
2 (ykl −µkl)2o
· exp
n
−τµ
2 (µµ −µkl)2o
∝exp









−µ2
kl
τkl + τµ
2
|
{z
}
1
2et
+µkl (τklykl + τµµµ)
|
{z
}
em·et









∝N(µkl | em, ( et )−1),
(9.15)
213

9.7. AGGRESSIVE UPDATE
where et = τkl + τµ, and em = (τklykl + τµµµ)/et are the posterior precision and mean of the
normal density. Similarly, the conditional density of τkl is,
p(τkl | µkl, µµ, τµ, αt, βt, ykl)
∝GT N(ykl | µkl, (τkl)−1, a = −1, b = 1) · GT NSNG(µkl, τkl | µµ, (τµ)−1, αt, βt)
∝GT N(ykl | µkl, (τkl)−1, a = −1, b = 1) ·

Φ((b −µµ) · √τµ) −Φ((a −µµ) · √τµ)
	
·(((((((((
(
N(µkl | µµ, (τµ)−1) · G(τkl | αt, βt)
∝exp

−τkl
(ykl −µkl)2
2

τ 1/2
kl τ αt−1
kl
exp {−βtτkl}
∝exp

−τkl

βt + (ykl −µkl)2
2

· τ (αt+1/2)−1
kl
∝G(τkl | ea,eb),
(9.16)
where ea = αt+1/2 and eb = βt+ (ykl−µkl)2
2
are the posterior parameters of the Gamma density.
The full procedure is then formulated in Algorithm 23 for GBT and GBTN models.
Algorithm 23 Gibbs sampler for GBT and GBTN ID models. The procedure presented
here may not be eﬃcient but is explanatory. A more eﬃcient one can be implemented in a
vectorized manner. By default, uninformative priors are a = −1, b = 1, ασ = 0.1, βσ = 1,
({µkl} = 0, {τkl} = 1) for GBT, (µµ = 0, τµ = 0.1, αt = βt = 1) for GBTN.
1: for t = 1 to T do
▷T iterations
2:
Sample state vector r from Equation (9.14);
3:
Update matrix X by A[:, J] where index vector J is the index of r with value 1 and
set X[:, I] = 0 where index vector I is the index of r with value 0;
4:
Sample σ2 from p(σ2 | X, Y , A) in Equation (9.12);
5:
for k = 1 to N do
6:
for l = 1 to N do
7:
Sample ykl from Equation (9.11);
8:
(GBTN only) Sample µkl from Equation (9.15);
9:
(GBTN only) Sample τkl from Equation (9.16);
10:
end for
11:
end for
12:
Report loss in Equation (9.6), stop if it converges.
13: end for
14: Report mean loss in Equation (9.6) after burn-in iterations.
9.7. Aggressive Update
In Algorithm 23, we notice that we set X[:, I] = 0 when the new state vector r is sampled.
However, in the next iteration step, the index set I may be updated in which case one entry
i of I may be altered to have a value of 1:
ri = 0 →ri = 1.
This may cause problems in the update of ykl in Equation (9.11), a zero i-th column in
X cannot update Y accordingly. One solution is to record a proposal state vector r2 and
214

JUN LU
CHAPTER 9. BAYESIAN INTERPOLATIVE DECOMPOSITION
a proposal factor matrix X2 from the r2 vector. When the update in the next iteration
selects the old state vector r, the factor matrix X is adopted to ﬁnish the updates; while
the algorithm chooses the proposal state vector r2, the proposal factor matrix X2 is applied
to do the updates. We call this procedure the aggressive update. The aggressive sampler
for the GBT model is formulated in Algorithm 24. For the sake of simplicity, we don’t
include the sampler for the GBTN model because it may be done in a similar way.
Algorithm 24 Aggressive Gibbs sampler for GBT ID model. The procedure presented
here may not be eﬃcient but is explanatory. A more eﬃcient one can be implemented in a
vectorized manner. By default, uninformative priors are a = −1, b = 1, ασ = 0.1, βσ = 1,
({µkl} = 0, {τkl} = 1) for GBT.
1: for t = 1 to T do
▷T iterations
2:
Sample state vector r from {r1, r2} by Equation (9.14);
3:
Decide Y : Y = Y1 if r is r1; Y = Y2 if r is r2;
4:
Update state vector r1 = r;
5:
Sample proposal state vector r2 based on r;
6:
Update matrix X by r = r1;
7:
Update proposal X2 by r2;
8:
Sample σ2 from p(σ2 | X, Y , A) in Equation (9.12);
9:
Sample Y1 = {ykl} using X;
10:
Sample Y2 = {ykl} using X2;
11:
Report loss in Equation (9.6), stop if it converges.
12: end for
13: Report mean loss in Equation (9.6) after burn-in iterations.
9.8. Post-Processing
The Gibbs sampling algorithm ﬁnds the approximation A ≈XY where X ∈RM×N and
Y ∈RN×N. As stated above, the redundant columns in X and redundant rows in Y can
be removed by the index vector J:
C = X[:, J] = A[:, J],
W = Y [J, :].
Since the submatrix Y [J, J] = W [:, J] (Equation (9.5)) from the Gibbs sampling procedure
is not enforced to be an identity matrix (as required in the interpolative decomposition). We
need to set it to be an identity matrix manually. This will basically reduce the reconstructive
error further. The post-processing procedure is shown in Figure 9.3.
9.9. Bayesian ID with Automatic Relevance Determination
We further extend the Bayesian models with automatic relevance determination (ARD) to
eliminate the need for model selection. Given the state vector r = [r1, r2, . . . , rN]⊤∈RN
whose index sets are J = J(r) = {n | rn = 1}N
n=1 and I = I(r) = {n | rn = 0}N
n=1. A new
215

9.9. BAYESIAN ID WITH AUTOMATIC RELEVANCE DETERMINATION
value of state vector r is to select one index j from either the index set J or the index set
I such that
j ∈J ∪I;
oj = p(rj = 0 | A, σ2, Y , r−j)
p(rj = 1 | A, σ2, Y , r−j) = p(rj = 0)
p(rj = 1) × p(A | σ2, Y , r−j, rj = 0)
p(A | σ2, Y , r−j, rj = 1),
(9.17)
where r−j denotes all elements of r except j-th element. Compare Equation (9.17) with
Equation (9.13), we may ﬁnd that in the former equation, the number of selected columns
is not ﬁxed now. Therefore, we let the inference decide the number of columns in basis
matrix C of interpolative decomposition. Again, we can set p(rj = 0) = p(rj = 1) = 0.5.
Then the full conditional probability of p(rj = 0, ri = 1 | A, σ2, Y , r−ji) can be calculated
by
p(rj = 0 | A, σ2, Y , r−j) =
oj
1 + oj
.
(9.18)
Then the full algorithm of GBT and GBTN with ARD is described in Algorithm 25 where
the diﬀerence lies in that we need to iterate over all elements of the state vector rather than
just one or two elements of it. We aware that many elements in the state vector can change
their signs making the update of matrix Y unstable. Therefore, we also deﬁne a number of
critical steps ν: after sampling the whole state vector r, we update several times (here we
repeat ν times) for the matrix Y and its related parameters (the diﬀerence is highlighted
in blue of Algorithm 25).
Data set
Num. Rows
Num. Columns
Fraction observed
Matrix rank
CCLE EC50
502
48
0.632
24
CCLE IC50
504
48
0.965
24
Gene Body Methylation
160
254
1.000
160
Promoter Methylation
160
254
1.000
160
Table 9.1:
Overview of the CCLE EC50, CCLE IC50, Gene Body Methylation, and
Promoter Methylation data sets, giving the number of rows, columns, the fraction of entries
that are observed, and the matrix rank.
0
1
2
CCLE EC50
1
0
CCLE IC50
0
2
Promoter Methylation
2
0
Gene Body Methylation
Figure 9.5: Data distribution of CCLE EC50, CCLE IC50, Gene Body Methylation, and
Promoter Methylation data sets.
216

JUN LU
CHAPTER 9. BAYESIAN INTERPOLATIVE DECOMPOSITION
Algorithm 25 Gibbs sampler for GBT and GBTN ID with ARD models. The procedure
presented here can be ineﬃcient but is explanatory. While a vectorized manner can be
implemented to ﬁnd a more eﬃcient algorithm. By default, weak priors are a = −1, b =
1, ασ = 0.1, βσ = 1, ({µkl} = 0, {τkl} = 1) for GBT, (µµ = 0, τµ = 0.1, αt = βt = 1) for
GBTN. Number of critical steps: ν.
1: for t = 1 to T do
▷T iterations
2:
for j = 1 to N do
3:
Sample state vector element rj from Equation (9.18);
4:
end for
5:
Update matrix X by A[:, J] where index vector J is the index of r with value 1 and
set X[:, I] = 0 where index vector I is the index of r with value 0;
6:
Sample σ2 from p(σ2 | X, Y , A) in Equation (9.12);
7:
for n = 1 to ν do
8:
for k = 1 to N do
9:
for l = 1 to N do
10:
Sample ykl from Equation (9.11);
11:
(GBTN only) Sample µkl from Equation (9.15);
12:
(GBTN only) Sample τkl from Equation (9.16);
13:
end for
14:
end for
15:
end for
16:
Output loss in Equation (9.6), stop iteration if it converges.
17: end for
18: Output averaged loss in Equation (9.6) for evaluation after burn-in iterations.
9.10. Examples for Bayesian ID
To evaluate the strategy and demonstrate the main advantages of the Bayesian ID method,
we conduct experiments with diﬀerent analysis tasks; and diﬀerent data sets including Can-
cer Cell Line Encyclopedia (CCLE EC50 and CCLE IC50 data sets (Barretina et al.,
2012)), cancer driver genes (Gene Body Methylation (Koboldt et al., 2012)), and the pro-
moter region (Promoter Methylation (Koboldt et al., 2012)) from bioinformatics. Following
Brouwer and Lio (2017), we preprocess these data sets by capping high values to 100 and
undoing the natural log transform for the former three data sets. Then we standardize to
have zero mean and unit variance for all data sets and ﬁll missing entries by 0. Finally,
we copy every column twice (for the CCLE EC50 and CCLE IC50 data sets) in order to
increase redundancy in the matrix; while for the latter two (Gene Body Methylation and
Promoter Methylation data sets), the number of columns is already larger than the matrix
rank such that we do not increase any redundancy. A summary of the four data sets is
reported in Table 9.1 and their distributions are presented in Figure 9.5.
In all scenarios, the same parameter initialization is adopted when conducting diﬀer-
ent tasks. Experimental evidence reveals that post-processing procedure can increase per-
formance to a minor extent, and that the outcomes of the GBT and GBTN models are
relatively similar (Lu, 2022b). For clariﬁcation, we only provide the ﬁndings of the GBT
217

9.10. EXAMPLES FOR BAYESIAN ID
model after post-processing. We compare the results of ARD versions of GBT and GBTN
with vanilla GBT and GBTN algorithms. In a wide range of experiments across various
data sets, the GBT or GBTN with ARD models improve reconstructive error and leads to
performance that is as good or better than the vanilla GBT or GBTN methods in low-rank
ID approximation.
In order to measure overall decomposition performance, we use mean squared error
(MSE, Equation (9.6)), which measures the similarity between the true and reconstructive;
the smaller the better performance.
0
200
400
600
800
1000
Iterations
0.5
1.0
1.5
MSE
(1) CCLE EC50
GBT with K=5
GBT with K=10
GBT with K=15
GBT with K=24
GBT with ARD
GBTN with ARD
0
200
400
600
800
1000
Iterations
(2) CCLE IC50
GBT with K=5
GBT with K=10
GBT with K=15
GBT with K=24
GBT with ARD
GBTN with ARD
0
200
400
600
800
1000
Iterations
(3) Gene Body Meth.
GBT with K=100
GBT with K=120
GBT with K=140
GBT with K=160
GBT with ARD
GBTN with ARD
0
200
400
600
800
1000
Iterations
(4) Promoter Meth.
GBT with K=100
GBT with K=120
GBT with K=140
GBT with K=160
GBT with ARD
GBTN with ARD
(a) Convergence of the models on the CCLE EC50, CCLE IC50, Gene Body Methylation, and Promoter
Methylation data sets, measured by the data ﬁt (MSE). The algorithm almost converges in less than 50
iterations.
0
10
20
30
40
Lags
0.0
0.2
0.4
0.6
Autocorrelation
(1) CCLE EC50
GBT with K=5
GBT with K=10
GBT with K=15
GBT with K=24
GBT with ARD
GBTN with ARD
0
10
20
30
40
Lags
(2) CCLE IC50
GBT with K=5
GBT with K=10
GBT with K=15
GBT with K=24
GBT with ARD
GBTN with ARD
0
10
20
30
40
Lags
(3) Gene Body Meth.
GBT with K=100
GBT with K=120
GBT with K=140
GBT with K=160
GBT with ARD
GBTN with ARD
0
10
20
30
40
Lags
(4) Promoter Meth.
GBT with K=100
GBT with K=120
GBT with K=140
GBT with K=160
GBT with ARD
GBTN with ARD
(b) Averaged autocorrelation coeﬃcients of samples of ykl computed using Gibbs sampling on the CCLE
EC50, CCLE IC50, Gene Body Methylation, and Promoter Methylation data sets.
0
250
500
750
1000
Iterations
20
25
30
35
40
45
Number of columns
(1) CCLE EC50
True Rank
GBT with ARD
GBTN with ARD
0
250
500
750
1000
Iterations
25
30
35
40
45
(2) CCLE IC50
True Rank
GBT with ARD
GBTN with ARD
0
250
500
750
1000
Iterations
100
150
200
250
(3) Gene Body Meth.
True Rank
GBT with ARD
GBTN with ARD
0
250
500
750
1000
Iterations
100
150
200
250
(4) Promoter Meth.
True Rank
GBT with ARD
GBTN with ARD
(c) Convergence of the number of selected columns on the CCLE EC50, CCLE IC50, Gene Body Methylation,
and Promoter Methylation data sets, measured by the data ﬁt (MSE). The algorithm almost converges
in less than 100 iterations.
Figure 9.6: Convergence results (upper), sampling mixing analysis (middle), and recon-
structive results (lower) on the CCLE EC50, CCLE IC50, Gene Body Methylation, and
Promoter Methylation data sets for various latent dimensions.
218

JUN LU
CHAPTER 9. BAYESIAN INTERPOLATIVE DECOMPOSITION
K1
K2
K3
K4
GBT (ARD)
GBTN (ARD)
CCLE EC50
0.354
0.218
0.131
0.046
0.034
0.031
CCLE IC50
0.301
0.231
0.161
0.103
0.035
0.031
Gene Body Methylation
0.433
0.443
0.466
0.492
0.363
0.372
Promoter Methylation
0.323
0.319
0.350
0.337
0.252
0.263
Table 9.2: Mean squared error measure with various latent dimension K parameters for
CCLE EC50, CCLE IC50, Gene Body Methylation, and Promoter Methylation data sets.
In all cases, K4 is the full rank of each matrix, K1 = 5, K2 = 10, K3 = 15 for the former
two data sets, and K1 = 100, K2 = 120, K3 = 140 for the latter two data sets. Results of
GBT and GBTN with ARD surpass those of the GBT and GBTN with full rank K4.
9.10.1 Hyperparameters
In this experiments, we use a = −1, b = 1, ασ = 0.1, βσ = 1, ({µkl} = 0, {τkl} = 1) for GBT,
(µµ = 0, τµ = 0.1, αt = βt = 1) for GBTN, and critical steps ν = 5 for GBT and GBTN
with ARD. The adopted parameters are very uninformative and weak prior choices and the
models are insensitive to them. The observed or unobserved variables are initialized from
random draws as long as these hyperparameters are ﬁxed since this initialization method
provides a better initial guess of the correct patterns in the matrices. In all scenarios, we
run the Gibbs sampling 1,000 iterations with a burn-in of 100 iterations and a thinning of
5 iterations as the convergence analysis shows the algorithm can converge in less than 100
iterations.
9.10.2 Convergence and Comparative Analysis
We ﬁrst show the rate of convergence over iterations on the CCLE EC50, CCLE IC50, Gene
Body Methylation, and Promoter Methylation data sets. We run the GBT model with K =
5, 10, 15, 24 for the CCLE EC50 and CCLE IC50 data sets where K = 24 is the full rank
of the matrices3, and K = 100, 120, 140, 160 for the Gene Body Methylation and Promoter
Methylation data sets where K = 160 is the full rank of the matrices; and the error is
measured by MSE. Figure 9.6(a) shows the rate of convergence over iterations. Figure 9.6(b)
shows the autocorrelation coeﬃcients of samples computed using Gibbs sampling.
We
observe that the mixings of the GBT and GBTN with ARD are close to those without
ARD. The coeﬃcients are less than 0.1 when the lags are more than 10 showing the mixing
of the Gibbs sampler is good.
In all experiments, the algorithm converges in less than
50 iterations. The results on the CCLE EC50, Gene Body Methylation, and Promoter
Methylation data sets show less noise in the sampling; while on the CCLE IC50 data set,
the sampling of GBT without ARD seems to be noisier than those of GBT and GBTN with
ARD.
Comparative results for the GBT and GBTN with ARD and those without ARD on the
four data sets are again shown in Figure 9.6(a) and Table 9.2. In all experiments, the GBT
and GBTN with ARD achieve the smallest MSE, even compared to the non-ARD versions
3. The results of GBT and GBTN without ARD are close so here we only provide the results of GBT for
clarity.
219

9.11. BAYESIAN INTERVENED INTERPOLATIVE DECOMPOSITION (IID)
with latent dimension K setting to full matrix rank (K = 24 for the CCLE EC50 and CCLE
IC50 data sets, and K = 160 for the Gene Body Methylation and Promoter Methylation
data sets). Figure 9.6(c) shows the convergence of the number of selected columns for GBT
and GBTN with ARD models on each data set. We observe that the samples are walking
around 27 for the CCLE EC50 and CCLE IC50 data sets; and around 130 for the Gene
Body Methylation and Promoter Methylation data sets. These samples are close to the true
rank of each matrix. The GBT and GBTN with ARD thus can determine the number of
columns inside the factored component X automatically.
9.11. Bayesian Intervened Interpolative Decomposition (IID)
Going further from the GBT model, we introduce the intervened interpolative decomposi-
tion (IID) algorithm (Lu and Osterrieder, 2022). The IID algorithm has exactly the same
generative process as shown in Equation (9.7), it applies an inverse-Gamma prior over the
variance parameter σ2 in Equation (9.8), and a GTN prior over the latent variables ykl’s in
Equation (9.9). However, we consider further that some columns of the observed matrix A
are more signiﬁcant and important, and they should be given a higher priority over other
columns.
Suppose the importance of each column of the observed matrix A is captured by a
raw importance vector bp ∈RN where bpi ∈[−∞, ∞] for all i in {1, 2, . . . , N}. The raw
importance vector can then be transformed into the range 0 to 1
p = Sigmoid(bp),
where Sigmoid(·) is the f(x) =
1
1+exp{−x} that can return a value in the range of 0 to 1.
The Sigmoid function acts as a squashing function because its domain is the set of all real
numbers, and its range is (0, 1). Then we take the p vector as the ﬁnal importance vector
to indicate the importance of each column in matrix A.
Going further from Equation (9.13), the intermediate variable oj is calculated instead
by
oj = p(rj = 0, ri = 1)
p(rj = 1, ri = 0) × p(A | σ2, Y , r−ji, rj = 0, ri = 1)
p(A | σ2, Y , r−ji, rj = 1, ri = 0)
= 1 −pj
pj
pi
1 −pi
× p(A | σ2, Y , r−ji, rj = 0, ri = 1)
p(A | σ2, Y , r−ji, rj = 1, ri = 0).
(9.19)
And again, the conditional probability of p(rj = 0, ri = 1 | A, σ2, Y , r−ji) can be obtained
by
p(rj = 0, ri = 1 | A, σ2, Y , r−ji) =
oj
1 + oj
.
(9.20)
Since we intervene in the procedure of the Gibbs sampling in Equation (9.19), hence the
name intervened interpolative decomposition (IID).
9.11.1 Quantitative Problem Statement
After the presentation of the algorithms for the intervened interpolative decomposition, one
may wonder about its signiﬁcance and its potential practical applications. It is well known
that large quantitative hedge funds and asset managers have been recruiting a signiﬁcant
220

JUN LU
CHAPTER 9. BAYESIAN INTERPOLATIVE DECOMPOSITION
number of data miners and ﬁnancial engineers in order to construct eﬀective alphas. The
number of alpha components might reach into millions or perhaps billions (Tulchinsky,
2019). As a result, creating a meta-alpha from all of the alphas or a large fraction of the
alpha pool might be troublesome for the following reasons:
1. If we use the same alphas as others, some illiquid alphas with low volume will be
traded heavily. This will make the strategy meaningless due to capacity constraints;
2. Using too many alphas may result in overﬁtting, resulting in poor out-of-sample (OS)
performance;
3. Many alphas might be mutually dependent, and certain machine learning algorithms,
such as neural networks and XGBoost models, might uncover their limits caused by
multi-linear diﬃculties while attempting to determine the meta-strategy from the
entire set of alphas;
4. Finding trading signals from the full alpha pool can be time-consuming because of
limited computing resources;
5. To minimize market risks, we constantly aim to discover a distinct subset of alphas
to test alternative methods with low correlation.
For the ﬁve reasons stated above, there is an urgent need to design algorithms that choose
a small subset of alphas from a large pool of them in order to prevent overﬁtting, make the
ﬁnal approach more scalable, and obtain the ﬁndings in a reasonable amount of time. It
is trivial to select an appropriate subset by the RankIC metric (see deﬁnition below), i.e.,
we select the alphas having the highest RankIC values. However, the problems still remain
that the selected subset will not represent the whole pool of alphas, and the selected alphas
may be mutually dependent.
Our objective is to identify as many representative alpha factors as possible with optimal
performance. The selected subset of alphas is representative in the sense that the small
subset of alphas can be used to reconstruct other alphas with a small replication error.
The traditional ID algorithm, either using a Randomized algorithm (Liberty et al., 2007)
or a Bayesian approach we have discussed above, can only help to ﬁnd the representative
ones. However, the end choices may seem to select alphas with low performance. Using
the discussed IID method, on the other hand, can help ﬁnd the representative (that can
reconstruct other alphas with small error) and the desirable (high RankIC scores) alphas
at the same time.
Formulaic Alphas
WorldQuant, a quantitative investment management ﬁrm, previously disclosed 101 formu-
laic short-term alpha determinants in 2016 (Kakushadze, 2016). Since then, the 191 alpha
factors from Guotai Junan Securities (GuotaiJunan, 2017) have also been welcomed by many
investors and institutions. These formulaic alpha components are derived from several stock
data elements, including, among others, volumes, prices, volatilities, and volume-weighted
average prices (vwap). As the name implies, a formulaic alpha is a type of alpha that can
be expressed as a formula or a mathematical expression. For example, a mean-reversion
alpha can be expressed in terms of a mathematical expression as follows:
Alpha = −(close(today) −close(5 days ago ) ) /close(5 days ago).
221

9.11. BAYESIAN INTERVENED INTERPOLATIVE DECOMPOSITION (IID)
In this sense, we take the opposite action as indicated by the closing price: we go short if
the price has risen during the previous ﬁve days, and we go long otherwise. At a high level,
the alpha value indicates the trend of the price in the days to come; the higher the alpha
value for each stock, the more likely it is that the stock’s price will rise in the next few days.
Evaluation Metrics
Let rt denote the yield rate of stock s on the t-th day. Suppose further pt is the closing
price of the asset at time t where t ∈{1, 2, . . . , T}, the return of the asset at time t can be
obtained by the following equation:
rt = pt −pt−1
pt
.
(9.21)
We use the Rank information coeﬃcient (RankIC) to evaluate the eﬀectiveness of an alpha:
RankIC(a, rh) = Spearman(a, rh),
(9.22)
where Spearman(·) indicates the Spearman correlation, a is the sequence of an alpha, rh
is the sequence of the return value with holding period h such that the i-th element of rh
represent the daily return of h days later. The RankIC then can be used as an indicator of
the importance of each alpha factor and plugged into Equation (9.19) directly.
9.11.2 Examples for Bayesian IID
For each stock s (i.e., s ∈{1, 2, . . . , S} where S is the total number of stocks), we have a
matrix As with shape As ∈RN×D where N is the number of alphas and D is the number
of dates so that each row of As is regarded as an alpha series. We want to select a subset of
the alphas (here we assume M out of the N alphas are selected). The RankIC between each
alpha series and the delayed return series with horizon h = 1 is then taken as the important
value directly, a higher RankIC indicates a higher priority.
Ticker
Type
Sector
Company
Avg. Amount
SH601988
Share
Bank
Bank of China Limited
427,647,786
SH601601
Share
Public Utility
China Paciﬁc Insurance (Group)
819,382,926
SH600028
Share
Public Utility
China Petroleum & Chemical Corporation
748,927,952
SH600016
Share
Bank
China Minsheng Banking Corporation
285,852,414
SH601186
Share
Public Utility
China Railway Construction Corporation
594,970,588
SH601328
Share
Bank
Bank of Communications Corporation
484,445,915
SH601628
Share
Public Utility
China Life Insurance Company Limited
368,179,861
SH601939
Share
Bank
China Construction Bank Corporation
527,876,669
SH510300
ETF
CSI 300
Huatai-PineBridge CSI 300 ETF
1,960,687,059
SH510050
ETF
CSI 50
ChinaAMC China CSI 50 ETF
2,020,385,879
Table 9.3: Summary of the underlying portfolios in the China market, ten assets in total.
The average amount (in the currency of RMB) is calculated in the period of the test set.
222

JUN LU
CHAPTER 9. BAYESIAN INTERPOLATIVE DECOMPOSITION
SH601988 SH601601 SH600028 SH600016 SH601186 SH601328 SH601628 SH601939 SH510300 SH510050
GBT Min. 5.235
5.814
5.235
6.381
5.819
5.700
5.734
5.785
5.462
6.297
IID Min.
4.567
5.700
4.843
6.490
5.104
5.658
5.445
5.435
4.876
5.767
GBT Mean 6.476
7.367
6.764
8.053
7.066
7.250
7.206
7.242
6.769
7.776
IID Mean
6.239
7.449
6.664
7.831
6.558
7.081
7.002
7.031
6.450
7.492
Table 9.4: Minimal and mean MSE measures after burn-in across diﬀerent iterations for
GBT and IID models on the 10 alpha matrices from 10 assets. In all cases, K = 10 is set
as the latent dimension. In most cases, the results of IID converge to a smaller value than
the GBT model.
Data set.
To assess the introduced algorithm and highlight the primary beneﬁts of the
IID technique, we perform experiments with several analytical tasks and use data for ten
assets from the China market and diverse industrial areas, including Bank, Public Utility,
and ETF. We obtain publicly available data from tushare 4. The data covers a three-year
period, i.e., 2018-07-18 to 2021-07-05 (720 trading days), where the data between 2018-07-
18 and 2020-07-09 is considered the training set (480 calendar days); while data between
2020-07-10 and 2021-07-05 is taken as the test set (240 trading days).
The underlying
portfolios are summarized in Table 9.3 and Figure 9.8(a) shows the series of diﬀerent assets
where we initialize each portfolio with a unitary value for clarity. The assets are chosen by
selecting the ones with high amount values (random ten assets among the ﬁfty assets with
the highest average amounts in China market during the selected period) so that there are
fewer trading restrictions.
We obtain 78 alphas from the 101 formulaic alphas (Kakushadze, 2016), 94 alphas from
the 191 formulaic alphas (GuotaiJunan, 2017), and 19 proprietary alphas. The alphas are
chosen to have a value that is neither too large nor too small. In this sense, the alpha
matrix As is of shape 214 × 720 for each asset.
In all scenarios, the same parameter initialization is adopted when conducting diﬀerent
tasks. Experimental evidence demonstrates that post-processing can marginally improve
performance. For clariﬁcation, we only provide the ﬁndings of the GBT (without ARD)
and IID models after post-processing. The IID model can select the important features
(alphas) with a higher priority while keeping the reconstructive error as small as possible,
resulting in performance that is as good as or better than the vanilla GBT method in
low-rank ID approximations across a wide range of experiments on diﬀerent data sets.
We again use the mean squared error (MSE, Equation (9.6)), which measures the similar-
ity between the observed and reconstructive matrices, to evaluate the overall decomposition
performance; the smaller the value, the better the performance.
Hyperparameters.
In those experiments, we use a = −1, b = 1, ασ = 0.1, βσ = 1,
({µkl} = 0, {τkl} = 1) for both GBT and IID models. The adopted parameters are unin-
formative and weak prior choices and the models are insensitive to them. The observed
or unobserved variables are initialized from random draws as long as those hyperparame-
ters are ﬁxed since this initialization method provides a better initial guess of the correct
patterns in the matrices. In all cases, we execute 1,000 iterations of Gibbs sampling with
a burn-in of 100 iterations and a thinning of 5 iterations, since the convergence analysis
indicates the algorithm can converge in fewer than 100 iterations.
4. https://tushare.pro/.
223

9.11. BAYESIAN INTERVENED INTERPOLATIVE DECOMPOSITION (IID)
0
250
500
750
1000
Iterations
6
8
10
12
MSE
(1) SH510050
GBT
IID
0
250
500
750
1000
Iterations
(2) SH510300
GBT
IID
0
250
500
750
1000
Iterations
(3) SH601939
GBT
IID
0
250
500
750
1000
Iterations
(4) SH601628
GBT
IID
0
250
500
750
100
Iterations
(5) SH601328
GBT
IID
(a) Convergence of the models on the SH510050, SH510300, SH601939, SH601628, and SH601328 data sets,
as measured by MSE. The algorithm almost converges in less than 100 iterations.
50
100
150
200
Iterations
0.00
0.05
0.10
0.15
0.20
Autocorrelation
(1) SH510050
GBT
IID
50
100
150
200
Iterations
(2) SH510300
GBT
IID
50
100
150
200
Iterations
(3) SH601939
GBT
IID
50
100
150
200
Iterations
(4) SH601628
GBT
IID
50
100
150
200
Iterations
(5) SH601328
GBT
IID
(b) Averaged autocorrelation coeﬃcients of samples of ykl computed using Gibbs sampling on the SH510050,
SH510300, SH601939, SH601628, and SH601328 data sets.
Figure 9.7: Convergence results (upper), and sampling mixing analysis (lower) on the
SH510050, SH510300, SH601939, SH601628, and SH601328 data sets for a latent dimension
of K = 10.
0
200
400
600
Date Index
0.5
1.0
1.5
2.0
2.5
Portfolio Value
SH601988
SH601601
SH600028
SH600016
SH601186
SH601328
SH601628
SH601939
SH510300
SH510050
(a) Ten diﬀerent portfolios where we initialize each
portfolio with a unitary value for clarity.
0
200
400
600
Date Index
1.0
1.5
2.0
2.5
Portfolio Value
Highest RankIC
Randomized ID
BID by GBT
BID by IID
SH510300 (ETF)
SH510050 (ETF)
IS and OS Split
(b) Portfolio values with the same strategy by using
diﬀerent alphas via comparative selection models.
Figure 9.8: Portfolio values of the ten assets (left), and the portfolio values (right) of
diﬀerent methods where we split by in-sample and out-of-sample periods, and initialize
with a unitary value for each period. The IID performs better in the out-of-sample period
(see also Table 9.5).
Convergence and Comparative Analysis
We ﬁrst show the rate of convergence over iterations on diﬀerent assets.
Due to space
constraints, we omit convergence results for the ﬁrst ﬁve assets and only present those for
224

JUN LU
CHAPTER 9. BAYESIAN INTERPOLATIVE DECOMPOSITION
portfolios SH510050, SH510300, SH601939, SH601628, and SH601328. Results for the other
assets are quantitatively similar.
We run GBT and IID models with K = 10 for the ﬁve data sets where 214 is the full
rank of the matrices, and the error is measured by MSE. Figure 9.7(a) shows the rate of
convergence over iterations. Figure 9.7(b) shows the autocorrelation coeﬃcients of samples
computed using Gibbs sampling. We observe that the mixings of the IID are close to those
of GBT. When the lags are greater than ten, the coeﬃcients are less than 0.1, indicating
that the Gibbs sampler mixes well. In all experiments, the algorithm converges in less than
100 iterations. We also observe that the IID model does not converge to a larger error than
the vanilla GBT model, though we put more emphasis on selecting the columns with high
RankIC. Table 9.4 presents the minimal MSE and mean MSE after burn-in across diﬀerent
iterations for GBT and IID models on the ten alpha matrices from ten assets. In most
cases, the IID can even converge to a smaller MSE value.
Algorithm 26 Alpha selection for portfolio allocation. Select holding period h, number of
alphas to select is M. Din is the in-sample number of days, D is the total number of days,
N is the total number of alphas. We then select M alphas out of the N alphas.
1: Split the alpha matrix for in-sample (IS) and out-of-sample (OS) evaluations:
Ain = As[:, 0 : Din] ∈RN×Din,
Aout = As[:, Din + 1 : D] ∈RN×(D−Din);
2: Using (column) ID to decide the alphas to be selected on matrix A⊤
in, with the selected
indices m:
b
Ain = As[m, 0 : Din] ∈RM×Din,
b
Aout = As[m, Din + 1 : D] ∈RM×(D−Din);
3: for m = 1 to M do
4:
Using the m-th IS alpha vector am = b
Ain[m, :] ∈RDin to decide the weight wm
and interception bm via ordinary least squares (OLS) so that the MSE between the
prediction a⊤
mwm+bm and the shifted return vector rh is minimized, i.e., minimizing
MSE(a⊤
mwm +bm, rh). The weight and interception are then used in OS evaluation.
5: end for
6: for d = 1 to D −Din do
7:
On each day in the OS period, we use the mean evaluation of each prediction from
the M alphas to decide to go long or not, i.e., to go long if PM
m=1 a⊤
mwm + bm > 0;
and do nothing otherwise since we restrict the analysis to long-only portfolios.
8:
Though we employ a long-only portfolio, we can favor a market-neutral strategy: we
open long positions only when we anticipate that at least half of the stocks will rise
on the following h day, and we weight each stock equally.
9: end for
Quantitative Strategy
After executing the GBT and IID algorithms for computing the interpolative decomposition
of each asset’s alpha matrix, the state vector r for each asset is saved and the ten alphas
225

9.11. BAYESIAN INTERVENED INTERPOLATIVE DECOMPOSITION (IID)
Methods
Highest RankIC
Randomized ID
BID with GBT
BID with IID
Mean RankIC
0.1035
0.0651
0.0553
0.0752
Mean Correlation
0.2276↓
0.5741↓
0.1132
0.1497
Sharpe Ratio (OS)
1.0276
1.0544
0.5045
1.5721
Sharpe Ratio (IS)
2.6511
1.3019
1.4965
2.3231
Annual Return (OS)
0.1043
0.0932
0.0484
0.1633
Annual Return (IS)
0.4390
0.2281
0.2425
0.3805
Max Drawdown (OS)
0.0632
0.0373
0.0484
0.0552
Max Drawdown (IS)
0.0892
0.1548
0.1232
0.0975
Table 9.5: Mean RankIC and correlation of the selected alphas across various assets for
diﬀerent methods. A higher mean RankIC and a lower mean correlation are better. The
IID method can ﬁnd the trade-oﬀbetween the mean RankIC and the mean correlation. In
all cases, IS means in-sample measurements, and OS means out-of-sample measurements.
The symbol “↓” means the performance is extremely poor.
with the largest mean selection during the 1,000 iterations are chosen (with a burn-in of
100 iterations, and thinning of 5 iterations).
Then we follow the quantitative strategy in Algorithm 26 (in which case h = 1, N = 214
alphas, M = 10 alphas, D = 720 trading days, and Din = 480 trading days). The procedure
shown in Algorithm 26 is a very simple quantitative strategy. However, the algorithm can
show precisely how the IID method can work in practice.
The strategy using the alphas selected by the IID method is only slightly worse than
the one selecting the highest RankIC alphas for the in-sample (IS) performance in terms of
Sharpe ratio, annual return, and maximum drawdown; however, the IID performs better
in the out-of-sample (OS) scenario and this is what we actually want (see Table 9.5 and
Figure 9.8(b)). To evaluate the strategy, we also adopt the Randomized algorithm to com-
pute the ID for comparison (Liberty et al., 2007), termed Randomized ID. The Randomized
ID performs even worse than BID with GBT (see Table 9.5). Though the IID does not
select alphas with the highest RankIC values, this does not mean that the alpha selection
procedure is meaningless for the following reasons:
1. Pool size: We only use a small alpha pool that only contains 214 alpha factors. When
the number of alphas is approaching millions or even billions, the alpha selection
procedure is expected to work better.
2. Correlation: The mean correlation of selected alphas across the ten assets of the IID
method is smaller than the highest RankIC method. In this sense, the alphas of the
latter method have high correlations and a low diversity.
If the correlated alphas
have low liquidity or perform poorly during a given period, the strategy’s risk might
increase.
3. Machine learning models: In our test, we only use OLS to ﬁnd the weight of each alpha
For more complex models, e.g., neural networks or XGBoost models, the correlated
alphas can cause multi-linear problems so that the performance and interpretability
are hampered.
226

JUN LU
CHAPTER 9. BAYESIAN INTERPOLATIVE DECOMPOSITION
4. Diversiﬁcation: Even if selecting the alphas with the highest RankIC can work well in
practice, we also want to diversify the strategies so that we are not exposed to speciﬁc
risks. The IID method can help ﬁnd diﬀerent strategies.
227

Part IV
Appendix
228


A
Appendix
Contents
A.1
Taylor’s Expansion . . . . . . . . . . . . . . . . . . . . . . . . . .
231
A.2
Deriving the Dirichlet distribution
. . . . . . . . . . . . . . . .
232
A.2.1
Derivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232
A.2.2
Properties of Dirichlet distribution . . . . . . . . . . . . . . . . . 233
230

JUN LU
APPENDIX A. APPENDIX
A.1. Taylor’s Expansion
Theorem A.1: (Taylor’s Expansion with Lagrange Remainder)
Let f(x) : R →R
be k-times continuously diﬀerentiable on the closed interval I with endpoints x and y, for
some k ≥0. If f(k+1) exists on the interval I, then there exists a x⋆∈(x, y) such that
f(x)
= f(y) + f′(y)(x −y) + f′′(y)
2!
(x −y)2 + . . . + f(k)(y)
k!
(x −y)k + f(k+1)(x⋆)
(k + 1)! (x −y)k+1
=
k
X
i=0
f(i)(y)
i!
(x −y)i + f(k+1)(x⋆)
(k + 1)! (x −y)k+1.
The Taylor’s expansion can be extended to a function of vector f(x) : Rn →R or a
function of matrix f(X) : Rm×n →R.
The Taylor’s expansion, or also known as the Taylor’s series, approximates the function
f(x) around the value of y by a polynomial in a single indeterminate x. To see where this
series comes from, we recall from the elementary calculus course that the approximated
function around θ = 0 for cos(θ) is given by
cos(θ) ≈1 −θ2
2 .
That is, the cos θ is approximated by a polynomial with a degree of 2. Suppose we want to
approximate cos θ by the more general polynomial with a degree of 2 by f(θ) = c1+c2θ+c3θ2.
An intuitive idea is to match the gradients around the 0 point. That is,





cos(0) = f(0);
cos′(0) = f′(0);
cos′′(0) = f′′(0);
leads to
−−−−−→





1 = c1;
−sin(0) = 0 = c2;
−cos(0) = −1 = 2c3.
This makes f(θ) = c1 + c2θ + c3θ2 = 1 −θ2
2 and agrees with our claim that cos(θ) ≈1 −θ2
2
around the 0 point. We shall not give the details of the proof.
231

A.2. DERIVING THE DIRICHLET DISTRIBUTION
A.2. Deriving the Dirichlet distribution
We derive the Dirichlet distribution and its properties in this appendix.
A.2.1 Derivation
Let x1, x2, . . . , xK be i.i.d. random variables drawn from the Gamma distribution such that
xk ∼G(αk, 1) for k ∈{1, 2, . . . , K}. The joint p.d.f. of x1, x2, . . . , xK is given by
fx1,x2,...,xK(x1, x2, . . . , xK) =







K
Y
k=1
1
Γ(αk)xαk−1
k
exp(−xk),
if all xk ≥0.
0,
if otherwise.
Deﬁne variables yk’s as follows
yk =
xk
PK
k=1 xk
,
∀k ∈{1, 2, . . . , K −1}
yK =
xK
PK
k=1 xk
= 1 −
K−1
X
k=1
yk,
(A.1)
and variable zK as
zK =
K
X
k=1
xk.
(A.2)
Let random variables X = [x1, x2, . . . , xK], Y = [y1, y2, . . . , yK−1, zK] and their draws
x = [x1, x2, . . . , xK], y = [y1, y2, . . . , yK−1, zK].
By multidimensional transformation of
variables, we have
fY(y) = fX(g−1(y))
det

Jg−1(y)
 ,
where


x1
x2
...
xK−1
xK


= g−1(y) = g−1









y1
y2
...
yK−1
zK









=


y1 · zK
y2 · zK
...
yK−1 · zK
yK · zK


,
and the Jacobian matrix is given by
Jg−1(y) =


∂
∂y1 g−1
1 (y)
· · ·
∂
∂yK−1 g−1
1 (y)
∂
∂zK g−1
1 (y)
...
...
· · ·
...
∂
∂y1 g−1
K (y)
· · ·
∂
∂yK−1 g−1
K (y)
∂
∂zK g−1
K (y)


=


zK
0
· · ·
0
y1
0
zK
· · ·
0
y2
...
...
...
...
...
0
0
· · ·
zK
yK−1
−zK
−zk
· · ·
−zK
(1 −PK−1
k=1 yk)


= zK−1
K
.
232

JUN LU
APPENDIX A. APPENDIX
This implies the joint p.d.f of Y is
fY(y) = fX(g−1(y))zK−1
K
= yα1−1
1
yα2−1
2
. . . yαK−1−1
K−1
yαK−1
K
QK
k=1 Γ(αk)
exp(−zK)zα1+α2+...+αK−1
K
.
We realize that the righthand side of the above equation is proportional to a p.d.f. of a
Gamma distribution and
Z
exp(−zK)zα1+α2+...+αK−1
K
dzK = Γ(α1 + α2 + . . . + αK).
Let α+ = α1 + α2 + . . . + αK, this implies
f(y1, y2, . . . , yK−1) =
Γ(α+)
QK
k=1 Γ(αk)
K
Y
k=1
yαk−1
k
.
We notice that yk’s are deﬁned that 0 < yk < 1 for all k ∈{1, 2, . . . , K}, and PK
k=1 yk = 1.
This implies the above equation is the p.d.f. of the Dirichlet distribution. The construction
shown above can be utilized to generate random variables from the Dirichlet distribution.
A.2.2 Properties of Dirichlet distribution
Suppose Y = [y1, y2, . . . , yK] ∼Dirichlet(α) with α = [α1, α2, . . . , αK], we here show the
moments and properties of the Dirichlet distribution.
Mean of Dirichlet distribution.
Write out the expectation,
E[y1] =
Z
· · ·
Z
y1 · Dirichlet(y | α)dy1dy2 · · · dyK
=
Z
· · ·
Z
y1
Γ(α+)
QK
k=1 Γ(αk)
K
Y
k=1
yαk−1
k
dy1dy2 · · · dyK
=
Γ(α+)
QK
k=1 Γ(αk)
Z
· · ·
Z
yα1+1−1
1
K
Y
k=2
yαk−1
k
dy1dy2 · · · dyK
=
Γ(α+)
QK
k=1 Γ(αk)
Γ(α1 + 1) QK
k=2 Γ(αk)
Γ(α+ + 1)
= Γ(α+)
Γ(α1)
Γ(α1 + 1)
Γ(α+ + 1) = α1
α+
,
where the last equality comes from the fact that Γ(x + 1) = xΓ(x).
Variance of Dirichlet distribution.
Write out the variance Var[yi] = E[y2
i ] −E[yi]2.
Similarly from the proof of the mean, we have
E[y2
i ] =
Γ(α+)
Γ(α+ + 2)
Γ(αi + 2)
Γ(αi)
= (αi + 1)αi
(α+ + 1)α+
.
This implies
Var[yi] = E[y2
i ] −E[yi]2 = (αi + 1)αi
(α+ + 1)α+
−( αi
α+
)2 = αi(α+ −αi)
α2
+(α+ + 1) .
233

A.2. DERIVING THE DIRICHLET DISTRIBUTION
Covariance of Dirichlet distribution.
Write out the covariance Cov[yiyj] = E[yiyj] −
E[yi]E[yj]. Again, similar to the proof of the mean, for i ̸= j, we have
E[yiyj] =
Γ(α+)
Γ(α+ + 2)
Γ(αi + 1)
Γ(αi)
Γ(αj + 1)
Γ(αj)
=
αiαj
α+(α+ + 1).
This implies
Cov[yiyj] = E[yiyj] −E[yi]E[yj] =
αiαj
α+(α+ + 1) −αiαj
α2
+
=
−αiαj
α2
+(α+ + 1).
Marginal distribution of yi.
By deﬁnitions in Equation (A.1) and Equation (A.2), we
have zK −xi ∼G(α+ −αi, 1). This implies
yi = xi
zK
=
xi
xi + (zK −xi) ∼Beta(αi, α+ −αi).
which is from the fact about the p.d.f. of two independent Gamma random variables. 1
Aggregation property.
Suppose [y1, y2, . . . , yK] ∼Dirichlet([α1, α2, . . . , αK]), Then, let
M = yi + yj, it follows that
[y1, . . . yi−1, yi+1, . . . , yj−1, yj+1, . . . , yK, M]
∼Dirichlet([α1, . . . , αi−1, αi+1, . . . , αj−1, αj+1, . . . , αK, αi + αj]).
Proof We realize that M ∼G(αi + αj, 1). Again by the multidimensional transformation
of variables as shown at the beginning of this section, we conclude the result.
The results can be extended to a more general case. If {A1, A2, . . . , Ar} is a partition of
{1, 2, . . . , K}, then

X
i∈A1
yi,
X
i∈A2
yi, . . . ,
X
i∈Ar
yi

∼Dirichlet



X
i∈A1
αi,
X
i∈A2
αi, . . . ,
X
i∈Ar
αi



.
Conditional distribution.
Let y0 = PK
k=3 yi and α0 = α+ −α1 −α2, then [y1, y2, y0] ∼
Dirichlet([α1, α2, α0]). Therefore
fy1,y2(y1, y2) = Γ(α1 + α2 + α0)
Γ(α1)Γ(α2)Γ(α0)yα1−1
1
yα2−1
2
(1 −y1 −y2)α0−1.
Similarly, we have
fy2(y2) = Γ(α1 + α2 + α0)
Γ(α2)Γ(α1 + α0)yα2−1
2
(1 −y2)α1+α0−1 = Beta(y1 | α2, α1 + α0),
which is a p.d.f. of a Beta distribution. Therefore, the conditional p.d.f., of y1 | y2 = y2 is
given by
fy1|y2=y2(y1 | y2) = fy1,y2(y1, y2)
fy2(y2)
= Γ(α1 + α0)
Γ(α1)Γ(α0)

y1
1 −y2
α1−1 
1 −
y1
1 −y2
α0−1
1
1 −y2
,
1. Suppose x ∼G(a, λ) and y ∼G(b, λ), then
x
x+y ∼Beta(a, b).
234

JUN LU
APPENDIX A. APPENDIX
which implies
1
1 −y2
y1 | y2 = y2 ∼Beta(α1, α0).
Apply this procedure, we will have
Y−i | yi ∼(1 −yi)Dirichlet(α−i),
where Y−i contains all the K −1 variables except yi, and similarly for α−i.
235

Bibliography
Rishi Advani and Sean O’Hagan.
Eﬃcient algorithms for constructing an interpolative
decomposition. arXiv preprint arXiv:2105.07076, 2021. 208
James H Albert and Siddhartha Chib.
Bayesian analysis of binary and polychotomous
response data. Journal of the American statistical Association, 88(422):669–679, 1993.
192
˙Ismail An, Umut S¸im¸sekli, Ali Taylan Cemgil, and Laie Akarun. Large scale polyphonic
music transcription using randomized matrix decompositions. In 2012 Proceedings of the
20th European Signal Processing Conference (EUSIPCO), pages 2020–2024. IEEE, 2012.
206
Theodore Wilbur Anderson. An introduction to multivariate statistical analysis. Technical
report, Wiley New York, 2003. 71, 74
Christophe Andrieu, Nando De Freitas, Arnaud Doucet, and Michael I Jordan. An intro-
duction to MCMC for machine learning. Machine learning, 50(1):5–43, 2003. 18, 131
Pia Anttila, Pentti Paatero, Unto Tapper, and Olli J¨arvinen. Source identiﬁcation of bulk
wet deposition in Finland by positive matrix factorization. Atmospheric Environment,
29(14):1705–1718, 1995. 119, 143
Ismail Arı, A Taylan Cemgil, and Lale Akarun. Probabilistic interpolative decomposition.
In 2012 IEEE International Workshop on Machine Learning for Signal Processing, pages
1–6. IEEE, 2012. 5, 199
Morten Arngren, Mikkel N Schmidt, and Jan Larsen. Unmixing of hyperspectral images
using Bayesian non-negative matrix factorization with volume prior. Journal of Signal
Processing Systems, 65(3):479–496, 2011. 139
Sudipto Banerjee and Anindya Roy. Linear algebra and matrix analysis for statistics, volume
181. CRC Press Boca Raton, FL, USA:, 2014. 4
236

JUN LU
APPENDIX A. APPENDIX
Jordi Barretina, Giordano Caponigro, Nicolas Stransky, Kavitha Venkatesan, Adam A Mar-
golin, Sungjoon Kim, Christopher J Wilson, Joseph Leh´ar, Gregory V Kryukov, Dmitriy
Sonkin, et al. The Cancer Cell Line Encyclopedia enables predictive modelling of anti-
cancer drug sensitivity. Nature, 483(7391):603–607, 2012. 217
James Bennett, Stan Lanning, et al. The Netﬂix prize. In Proceedings of KDD cup and
workshop, volume 2007, page 35. New York, NY, USA., 2007. 92
Jos´e M Bernardo and Adrian FM Smith. Bayesian theory, volume 405. John Wiley & Sons,
2009. 33
Michael W Berry, Murray Browne, Amy N Langville, V Paul Pauca, and Robert J Plem-
mons. Algorithms and applications for approximate nonnegative matrix factorization.
Computational statistics & data analysis, 52(1):155–173, 2007. 119, 122, 143
Julian Besag. On the statistical analysis of dirty pictures. Journal of the Royal Statistical
Society: Series B (Methodological), 48(3):259–279, 1986. 15
Christopher M Bishop. Pattern recognition. Machine learning, 128(9), 2006. 4, 71
Keith Allen Bonawitz. Composable Probabilistic Inference with Blaise. PhD thesis, Mas-
sachusetts Institute of Technology, 2008. 16, 18
Christos Boutsidis and Efstratios Gallopoulos. SVD based initialization: A head start for
nonnegative matrix factorization. Pattern recognition, 41(4):1350–1362, 2008. 122
George EP Box and Norman R Draper. Empirical model-building and response surfaces.
John Wiley & Sons, 1987. 25
Thomas Brouwer and Pietro Lio. Prior and likelihood choices for Bayesian matrix factori-
sation on small datasets. arXiv preprint arXiv:1712.00288, 2017. 143, 150, 152, 158, 159,
163, 170, 171, 172, 217
Thomas Brouwer, Jes Frellsen, and Pietro Li´o. Comparative study of inference methods for
Bayesian nonnegative matrix factorisation. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pages 513–529. Springer, 2017. 122
Jean-Philippe Brunet, Pablo Tamayo, Todd R Golub, and Jill P Mesirov.
Metagenes
and molecular pattern discovery using matrix factorization. Proceedings of the national
academy of sciences, 101(12):4164–4169, 2004. 122
John Burkardt. The truncated normal distribution. Department of Scientiﬁc Computing
Website, Florida State University, 1:35, 2014. 51, 53
John Canny. GaP: a factor model for discrete data. In Proceedings of the 27th annual inter-
national ACM SIGIR conference on Research and development in information retrieval,
pages 122–129, 2004. 5, 183
Ali Taylan Cemgil. Bayesian inference for nonnegative matrix factorisation models. Com-
putational intelligence and neuroscience, 2009, 2009. 183
237

A.2. DERIVING THE DIRICHLET DISTRIBUTION
Gang Chen, Fei Wang, and Changshui Zhang.
Collaborative ﬁltering using orthogonal
nonnegative matrix tri-factorization. Information Processing & Management, 45(3):368–
379, 2009. 4
Hugh Chipman, Edward I George, Robert E McCulloch, Merlise Clyde, Dean P Foster,
and Robert A Stine. The practical implementation of Bayesian model selection. Lecture
Notes-Monograph Series, pages 65–134, 2001. 79, 80
Ronald Christensen. Linear models for multivariate, time series, and spatial data, volume 1.
Springer, 1991. 89
Wei Chu, Zoubin Ghahramani, and Christopher KI Williams. Gaussian processes for ordinal
regression. Journal of machine learning research, 6(7), 2005. 191
Joel E Cohen and Uriel G Rothblum. Nonnegative ranks, decompositions, and factorizations
of nonnegative matrices. Linear Algebra and its Applications, 190:149–168, 1993. 119
Pierre Comon, Xavier Luciani, and Andr´e LF De Almeida.
Tensor decompositions, al-
ternating least squares and other tales.
Journal of Chemometrics: A Journal of the
Chemometrics Society, 23(7-8):393–405, 2009. 4, 93
Rajarshi Das.
Collapsed Gibbs sampler for Dirichlet process Gaussian mixture models
(DPGMM). Talk, 2014. 67, 80
Robyn M Dawes and Bernard Corrigan. Linear models in decision making. Psychological
bulletin, 81(2):95, 1974. 89
Chris HQ Ding, Tao Li, and Michael I Jordan. Convex and semi-nonnegative matrix factor-
izations. IEEE transactions on pattern analysis and machine intelligence, 32(1):45–55,
2008. 145, 175
David B Dunson and Amy H Herring. Bayesian latent variable models for mixed discrete
outcomes. Biostatistics, 6(1):11–25, 2005. 5, 183
Ludwig Fahrmeir, Thomas Kneib, Stefan Lang, and Brian Marx. Regression. Springer,
2007. 25
Wang Fei, Li Tao, and Zhang Changshui. Semi-supervised clustering via matrix factoriza-
tion. In Proc. SIAM Int. Conf. on Data Mining, 2008. 175
John Fox. Applied regression analysis, linear models, and related methods. Sage Publica-
tions, Inc, 1997. 89
Chris Fraley and Adrian E Raftery. Bayesian regularization for normal mixture estimation
and model-based clustering. Journal of classiﬁcation, 24(2):155–181, 2007. 79, 80
Bela A Frigyik, Amol Kapila, and Maya R Gupta. Introduction to the Dirichlet distribution
and related processes. Department of electrical engineering, university of washignton.
Technical report, UWEETR-2010-0006, 2010. 65
238

JUN LU
APPENDIX A. APPENDIX
Yuan Gao and George Church. Improving molecular cancer class discovery through sparse
non-negative matrix factorization. Bioinformatics, 21(21):3970–3975, 2005. 122
Andrew Gelman, John B Carlin, Hal S Stern, David B Dunson, Aki Vehtari, and Donald B
Rubin. Bayesian data analysis. CRC press, 2013. 21, 33
Stuart Geman and Donald Geman.
Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images.
IEEE Transactions on pattern analysis and machine
intelligence, (6):721–741, 1984. 20, 21
James E Gentle. Numerical linear algebra for applications in statistics. Springer Science &
Business Media, 1998. 4
James E Gentle.
Matrix algebra:
theory, computations, and applications in statistics.
Springer Science & Business Media, 2007. 69
Charles Geyer. Introduction to Markov chain Monte Carlo. Handbook of markov chain
monte carlo, pages 3–48, 2011. 18
Paris V Giampouras, Athanasios A Rontogiannis, and Konstantinos D Koutroumbas. Alter-
nating iteratively reweighted least squares minimization for low-rank matrix factorization.
IEEE Transactions on Signal Processing, 67(2):490–503, 2018. 93
Walter R Gilks and Pascal Wild. Adaptive rejection sampling for Gibbs sampling. Applied
Statistics, pages 337–348, 1992. 21
Philip E Gill, Walter Murray, and Margaret H Wright. Numerical linear algebra and opti-
mization. SIAM, 2021. 4
Nicolas Gillis. The why and how of nonnegative matrix factorization. Connections, 12:2–2,
2014. 122
Nicolas Gillis. Nonnegative matrix factorization. SIAM, 2020. 119
Abhinav Goel, Caleb Tung, Yung-Hsiang Lu, and George K Thiruvathukal. A survey of
methods for low-power deep learning and computer vision.
In 2020 IEEE 6th World
Forum on Internet of Things (WF-IoT), pages 1–6. IEEE, 2020. 4
Mehmet G¨onen. Predicting drug–target interactions from chemical and genomic kernels
using Bayesian matrix factorization. Bioinformatics, 28(18):2304–2310, 2012. 129
Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016. 4
Prem Gopalan, Jake M Hofman, and David M Blei. Scalable recommendation with Poisson
factorization. arXiv preprint arXiv:1311.1704, 2013. 183
Prem Gopalan, Francisco J Ruiz, Rajesh Ranganath, and David Blei. Bayesian nonpara-
metric Poisson factorization for recommendation systems. In Artiﬁcial Intelligence and
Statistics, pages 275–283. PMLR, 2014. 145, 183
239

A.2. DERIVING THE DIRICHLET DISTRIBUTION
Prem Gopalan, Jake M Hofman, and David M Blei. Scalable recommendation with hierar-
chical Poisson factorization. In UAI, pages 326–335, 2015. 145, 183, 185
Olivier Gouvert, Thomas Oberlin, and C´edric F´evotte. Ordinal non-negative matrix fac-
torization for recommendation. In International Conference on Machine Learning, pages
3680–3689. PMLR, 2020. 191
Alex Graves. Practical variational inference for neural networks. Advances in neural infor-
mation processing systems, 24, 2011. 17
Securities GuotaiJunan. Multi factor stock selection system based on the characteristics of
short cycle price. 2017. 221, 223
Nathan Halko, Per-Gunnar Martinsson, and Joel A Tropp. Finding structure with ran-
domness: Probabilistic algorithms for constructing approximate matrix decompositions.
SIAM review, 53(2):217–288, 2011. 199
Wolfgang Karl H¨ardle and L´eopold Simar. Applied multivariate statistical analysis. Springer
Nature, 2007. 57
F Maxwell Harper and Joseph A Konstan. The movielens datasets: History and context.
Acm transactions on interactive intelligent systems (tiis), 5(4):1–19, 2015. 113, 159
W Keith Hastings. Monte Carlo sampling methods using markov chains and their applica-
tions. Biometrika, 57(1):97–109, 1970. 19
Martin B Haugh. A tutorial on Markov chain Monte Carlo and Bayesian modeling. Available
at SSRN 3759243, 2021. 16
Bruce M Hill. Bayesian forecasting of economic time series. Econometric theory, 10(3-4):
483–513, 1994. 15
Peter D Hoﬀ. A ﬁrst course in Bayesian statistical methods. Springer Science & Business
Media, 2009. 18, 21, 25, 33, 65, 79, 80
Matthew D Hoﬀman, David M Blei, Chong Wang, and John Paisley. Stochastic variational
inference. Journal of Machine Learning Research, 14(5), 2013. 17
Changwei Hu, Piyush Rai, and Lawrence Carin. Zero-truncated Poisson tensor factorization
for massive binary tensors. arXiv preprint arXiv:1508.04210, 2015. 183
Michael I Jordan and Chris Bishop. An introduction to graphical models, 2004. 131
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul.
An
introduction to variational methods for graphical models. Machine learning, 37(2):183–
233, 1999. 17
Zura Kakushadze. 101 formulaic alphas. Wilmott, 2016(84):72–81, 2016. 221, 223
Herman Kamper. Gibbs sampling for ﬁtting ﬁnite and inﬁnite Gaussian mixture models,
2013. 67
240

JUN LU
APPENDIX A. APPENDIX
Daniel C Koboldt, Robert S Fulton, Michael D McLellan, Heather Schmidt, Joelle Kalicki-
Veizer, Joshua F McMichael, Lucinda L Fulton, David J Dooling, Li Ding, Elaine R
Mardis, et al. Comprehensive molecular portraits of human breast tumours. Nature, 490
(7418):61–70, 2012. 171, 217
Yehuda Koren, Robert Bell, and Chris Volinsky. Matrix factorization techniques for recom-
mender systems. Computer, 42(8):30–37, 2009. 183
Samuel Kotz, Tomasz Kozubowski, and Krzysztof Podg´orski. The Laplace distribution and
generalizations: A revisit with applications to communications, economics, engineering,
and ﬁnance. Number 183. Springer Science & Business Media, 2001. 56
John Kruschke. Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan. 2014.
27
PW Lane. Generalized linear models in soil science. European Journal of Soil Science, 53
(2):241–251, 2002. 89
Daniel Lee and H Sebastian Seung. Algorithms for non-negative matrix factorization. Ad-
vances in neural information processing systems, 13, 2000. 119, 120, 143, 161
Daniel D Lee and H Sebastian Seung. Learning the parts of objects by non-negative matrix
factorization. Nature, 401(6755):788–791, 1999. 4, 143
Hyekyoung Lee and Seungjin Choi. CUR+NMF for learning spectral features from large
data matrix. In 2008 IEEE International Joint Conference on Neural Networks (IEEE
World Congress on Computational Intelligence), pages 1592–1597. IEEE, 2008. 206
Cornelius T Leondes. Multidimensional Systems: Signal Processing and Modeling Tech-
niques: Advances in Theory and Applications. Elsevier, 1995. 111
Tao Li, Yi Zhang, and Vikas Sindhwani. A non-negative matrix tri-factorization approach to
sentiment classiﬁcation with lexical prior knowledge. In Proceedings of the Joint Confer-
ence of the 47th Annual Meeting of the ACL and the 4th International Joint Conference
on Natural Language Processing of the AFNLP, pages 244–252, 2009. 4
Edo Liberty, Franco Woolfe, Per-Gunnar Martinsson, Vladimir Rokhlin, and Mark Tygert.
Randomized algorithms for the low-rank approximation of matrices. Proceedings of the
National Academy of Sciences, 104(51):20167–20172, 2007. 199, 208, 221, 226
Yew Jin Lim and Yee Whye Teh. Variational Bayesian approach to movie rating prediction.
In Proceedings of KDD cup and workshop, volume 7, pages 15–21. Citeseer, 2007. 4
Jun Lu. Machine learning modeling for time series problem: Predicting ﬂight ticket prices.
arXiv preprint arXiv:1705.07205, 2017. 89
Jun Lu. On the column and row ranks of a matrix. arXiv preprint arXiv:2112.06638, 2021a.
200, 206
241

A.2. DERIVING THE DIRICHLET DISTRIBUTION
Jun Lu. Numerical matrix decomposition and its modern applications: A rigorous ﬁrst
course. arXiv preprint arXiv:2107.02579, 2021b. 5, 6, 9, 84, 91, 93, 97, 101, 111, 112,
163, 200, 203, 204, 208
Jun Lu.
A survey on Bayesian inference for Gaussian mixture model.
arXiv preprint
arXiv:2108.11753, 2021c. 4, 16, 80
Jun Lu. A rigorous introduction to linear models. Eliva Press, 2022a. 25, 26, 42, 77, 89
Jun Lu. Bayesian low-rank interpolative decomposition for complex datasets. arXiv preprint
arXiv:2205.14825, 2022b. 199, 209, 217
Jun Lu. Comparative study of inference methods for interpolative decomposition. arXiv
preprint arXiv:2206.14542, 2022c. 209
Jun Lu.
Gradient descent, stochastic optimization, and other tales.
arXiv preprint
arXiv:2205.00832, Eliva Press, 2022d. 101
Jun Lu and Christine P Chai.
Robust Bayesian nonnegative matrix factorization with
implicit regularizers. arXiv preprint arXiv:2208.10053, 2022. 166
Jun Lu and Joerg Osterrieder.
Feature selection via the intervened interpolative de-
composition and its application in diversifying quantitative strategies. arXiv preprint
arXiv:2209.14532, 2022. 199, 209, 220
Jun Lu and Xuanyu Ye. Flexible and hierarchical prior for Bayesian nonnegative matrix
factorization. arXiv preprint arXiv:2205.11025, 2022. 122, 143, 154
Zhanyu Ma, Pravin Kumar Rana, Jalil Taghia, Markus Flierl, and Arne Leijon. Bayesian
estimation of Dirichlet mixture model with variational inference. Pattern Recognition, 47
(9):3143–3157, 2014. 17
Michael W Mahoney and Petros Drineas. CUR matrix decompositions for improved data
analysis. Proceedings of the National Academy of Sciences, 106(3):697–702, 2009. 206
Stephan Mandt and David Blei. Smoothed gradients for stochastic variational inference.
arXiv preprint arXiv:1406.3650, 2014. 17
Benjamin M Marlin. Modeling user rating proﬁles for collaborative ﬁltering. Advances in
neural information processing systems, 16, 2003. 4
GJ Marseille, R de Beer, M Fuderer, AF Mehlkopf, and D van Ormondt. Bayesian esti-
mation of MR images from incomplete raw data. In Maximum Entropy and Bayesian
Methods, pages 13–22. Springer, 1996. 15
Per-Gunnar Martinsson. Randomized methods for matrix computations. The Mathematics
of Data, 25(4):187–231, 2019. 203
Per-Gunnar Martinsson, Vladimir Rokhlin, and Mark Tygert. A randomized algorithm for
the decomposition of matrices. Applied and Computational Harmonic Analysis, 30(1):
47–68, 2011. 199
242

JUN LU
APPENDIX A. APPENDIX
Jose Menchero, D Orr, and Jun Wang. The Barra US equity model (USE4), methodology
notes. English, MSCI (May, 2011. 89
Nicholas Metropolis, Arianna W Rosenbluth, Marshall N Rosenbluth, Augusta H Teller,
and Edward Teller. Equation of state calculations by fast computing machines. The
journal of chemical physics, 21(6):1087–1092, 1953. 19
Andriy Mnih and Russ R Salakhutdinov. Probabilistic matrix factorization. Advances in
neural information processing systems, 20, 2007. 4
Raphael A Mrode. Linear models for the prediction of animal breeding values. Cabi, 2014.
89
Kevin P Murphy. Conjugate Bayesian analysis of the Gaussian distribution. def, 1(2σ2):
16, 2007. 42, 67, 74
Kevin P Murphy. Machine learning: A probabilistic perspective. MIT press, 2012. 67, 71,
79, 80, 81
Yann Ollivier. Laplace’s rule of succession in information geometry. In International Con-
ference on Geometric Science of Information, pages 311–319. Springer, 2015. 24
Pentti Paatero and Unto Tapper.
Positive matrix factorization: A non-negative factor
model with optimal utilization of error estimates of data values. Environmetrics, 5(2):
111–126, 1994. 119, 143
Pentti Paatero, Unto Tapper, Pasi Aalto, and Markku Kulmala. Matrix factorization meth-
ods for analysing diﬀusion battery data. Journal of Aerosol Science, 22:S273–S276, 1991.
119, 143
Ulrich Paquet, Sean Holden, and Andrew Naish-Guzman. Bayesian hierarchical ordinal
regression. In International Conference on Artiﬁcial Neural Networks, pages 267–272.
Springer, 2005. 191
V Paul Pauca, Jon Piper, and Robert J Plemmons. Nonnegative matrix factorization for
spectral data analysis. Linear algebra and its applications, 416(1):29–47, 2006. 120
Dong Qian, Bei Wang, Xiangyun Qing, Tao Zhang, Yu Zhang, Xingyu Wang, and Masatoshi
Nakamura. Bayesian nonnegative CP decomposition-based feature extraction algorithm
for drowsiness detection. IEEE Transactions on neural systems and rehabilitation engi-
neering, 25(8):1297–1308, 2016. 6
Piyush Rai, Yingjian Wang, and Lawrence Carin. Leveraging features and networks for
probabilistic tensor decomposition. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 29, 2015. 6
Tapani Raiko, Alexander Ilin, and Juha Karhunen. Principal component analysis for large
scale problems with lots of missing values. In European Conference on Machine Learning,
pages 691–698. Springer, 2007. 4
243

A.2. DERIVING THE DIRICHLET DISTRIBUTION
Rajesh Ranganath, Sean Gerrish, and David Blei.
Black box variational inference.
In
Artiﬁcial intelligence and statistics, pages 814–822. PMLR, 2014. 17
Carl Edward Rasmussen. Gaussian processes in machine learning. In Summer school on
machine learning, pages 63–71. Springer, 2003. 25, 27
Christian P Robert et al.
The Bayesian choice: from decision-theoretic foundations to
computational implementation, volume 2. Springer, 2007. 33
Ruslan Salakhutdinov and Andriy Mnih. Bayesian probabilistic matrix factorization us-
ing Markov chain Monte Carlo. In Proceedings of the 25th international conference on
Machine learning, pages 880–887, 2008. 5, 129, 138
Lawrence R Schaeﬀer. Application of random regression models in animal breeding. Live-
stock Production Science, 86(1-3):35–45, 2004. 89
Mark F Schilling, Ann E Watkins, and William Watkins. Is human height bimodal? The
American Statistician, 56(3):223–229, 2002. 35
Mikkel N Schmidt and Shakir Mohamed. Probabilistic non-negative tensor factorization
using Markov chain Monte Carlo. In 2009 17th European Signal Processing Conference,
pages 1918–1922. IEEE, 2009. 152, 158
Mikkel N Schmidt, Ole Winther, and Lars Kai Hansen. Bayesian non-negative matrix fac-
torization. In International Conference on Independent Component Analysis and Signal
Separation, pages 540–547. Springer, 2009. 143
Matthias Seeger. Low rank updates for the Cholesky decomposition. Technical report, 2004.
84
Stanley Smith Stevens. On the theory of scales of measurement. Science, 103(2684):677–680,
1946. 191
Gilbert Strang. Linear algebra and learning from data. Wellesley-Cambridge Press Cam-
bridge, 2019. 89
Gilbert Strang. Linear algebra for everyone. Wellesley-Cambridge Press Wellesley, 2021.
89, 200
Gilbert Strang and Cleve Moler. LU and CR elimination. SIAM Review, 64(1):181–190,
2022. 200
Panagiotis Symeonidis and Andreas Zioupos. Matrix and Tensor Factorization Techniques
for Recommender Systems, volume 1. Springer, 2016. 4
G´abor Tak´acs and Domonkos Tikk. Alternating least squares for personalized ranking. In
Proceedings of the sixth ACM conference on Recommender systems, pages 83–90, 2012.
93
244

JUN LU
APPENDIX A. APPENDIX
Hiromu Takayama, Qibin Zhao, Hidekata Hontani, and Tatsuya Yokota.
Bayesian ten-
sor completion and decomposition with automatic CP rank determination using MGP
shrinkage prior. SN Computer Science, 3(3):1–17, 2022. 6
VY Tan and C F´evotte. Automatic relevance determination in nonnegative matrix fac-
torization with the β-divergence. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 35(7):1592–1605, 2013. 143, 147
Yee Whye Teh.
Exponential families: Gaussian, Gaussian-Gamma, Gaussian-Wishart,
multinomial, 2007. 67
Luke Tierney. A note on Metropolis-Hastings kernels for general state spaces. Annals of
applied probability, pages 1–9, 1998. 19
Lloyd N Trefethen and David Bau III. Numerical linear algebra, volume 50. SIAM, 1997.
89
Igor Tulchinsky. Finding Alphas: A quantitative approach to building trading strategies.
John Wiley & Sons, 2019. 221
Valentin F Turchin. On the computation of multidimensional integrals by the Monte-Carlo
method. Theory of Probability & Its Applications, 16(4):720–724, 1971. 20, 21
Seppo Virtanen, Arto Klami, and Samuel Kaski. Bayesian CCA via group sparsity. In
Proceedings of the 28th International Conference on International Conference on Machine
Learning, pages 457–464, 2011. 129, 136
Seppo Virtanen, Arto Klami, Suleiman Khan, and Samuel Kaski. Bayesian group factor
analysis. In Artiﬁcial Intelligence and Statistics, pages 1269–1277. PMLR, 2012. 129, 136
Jim Jing-Yan Wang, Xiaolei Wang, and Xin Gao. Non-negative matrix factorization by
maximizing correntropy for cancer clustering. BMC bioinformatics, 14(1):1–11, 2013. 4
Pascal Wild and WR Gilks. Algorithm AS 287: Adaptive rejection sampling from log-
concave density functions. Journal of the Royal Statistical Society. Series C (Applied
Statistics), 42(4):701–709, 1993. 21
Wanjuan Yang, Jorge Soares, Patricia Greninger, Elena J Edelman, Howard Lightfoot,
Simon Forbes, Nidhi Bindal, Dave Beare, James A Smith, I Richard Thompson, et al.
Genomics of drug sensitivity in cancer (GDSC): A resource for therapeutic biomarker
discovery in cancer cells. Nucleic Acids Research, 41(D1):D955–D961, 2012. 170
Mingyuan Zhou, Chunping Wang, Minhua Chen, John Paisley, David Dunson, and
Lawrence Carin.
Nonparametric Bayesian matrix completion.
In 2010 IEEE Sensor
Array and Multichannel Signal Processing Workshop, pages 213–216. IEEE, 2010. 5
Abdelhak M Zoubir, Visa Koivunen, Yacine Chakhchoukh, and Michael Muma. Robust
estimation in signal processing: A tutorial-style treatment of fundamental concepts. IEEE
Signal Processing Magazine, 29(4):61–80, 2012. 167
245

Alphabetical Index
ALS, 93
Bayesian inference, 122
Bayesian matrix decomposition, 122
Bayesian optimization, 122
Binomial distribution, 60
Chi-squared distribution, 42
Conditional independence, xxi
Conjugate prior, 33
Covariance, xxi
Cramer’s rule, 201
Decomposition: ALS, 91
Decomposition: CUR, 205
Decomposition: GBT, 209
Decomposition: GBT with ARD, 215
Decomposition: GBTN, 209
Decomposition: GEE, 143
Decomposition: GEEA, 147
Decomposition: GEG, 176
Decomposition: GGG, 129
Decomposition: GGGA, 136
Decomposition: GGGM, 134
Decomposition: GGGW, 138
Decomposition: GL2
1, 163
Decomposition: GL2
2, 166
Decomposition: GL∞, 166
Decomposition: GnVG, 176
Decomposition: GRR, 154
Decomposition: GRRN, 154
Decomposition: GTT, 149
Decomposition: GTTN, 151
Decomposition: GVG, 139
Decomposition: IID, 220
Decomposition: NMF, 119
Decomposition: NMTF, 177
Decomposition: OGGW, 191
Decomposition: PAA, 183
Decomposition: PAAA, 185
Decomposition: Skeleton, 205
Derivative, xx
Determinant, xx
Element-wise product, see Hadamard
product
Exponential distribution, 49
Gamma distribution, 36
Gaussian distribution, 33
General-truncated-normal distribution,
52
Gradient descent, 100
Graph, xix
Hadamard product, xx
Half-normal distribution, 54
Hessian matrix, xx
Independence, xxi
246

JUN LU
ALPHABETICAL INDEX
Integral, xx
Inverse-Gamma distrbution, 39
Inverse-Gaussian distribution, 56
Jacobian matrix, xx
Kullback-Leibler divergence, xxi
Laplace distribution, 57
Least squares, 89
Machine precision, 120
Matrix, xviii, xix
Matrix indexing, xix
Matrix norm, 9
Multinomial distribution, 59
Multivariate Student’s t distribution, 70
Netﬂix, 91
NMF, 118
Norm, xxi
Normal-inverse-Chi-squared distribution,
45
Normal-inverse-Gamma distribution, 41
Normal-inverse-Wishart distribution, 74
Notation, xviii
Poisson distribution, 65
Rectiﬁed-normal distribution, 54
Scalar, xviii, xix
Set, xix
Sets, xviii
Shannon entropy, xxi
Sigmoid, xxi
Skew-Laplace distribution, 59
Softplus, xxi
Student’s t distribution, 35
Taylor’s expansion, 231
Taylor’s formula, 102, 231
Tensor, xviii, xix
Transpose, xx
Truncated, 113
Truncated SVD, 113
Truncated-normal distribution, 50
Variance, xxi
Vector, xviii, xix
Vector norm, 9
Wishart distribution, 71
247


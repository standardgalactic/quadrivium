Lecture Notes in Computer Science
4513
Commenced Publication in 1973
Founding and Former Series Editors:
Gerhard Goos, Juris Hartmanis, and Jan van Leeuwen
Editorial Board
David Hutchison
Lancaster University, UK
Takeo Kanade
Carnegie Mellon University, Pittsburgh, PA, USA
Josef Kittler
University of Surrey, Guildford, UK
Jon M. Kleinberg
Cornell University, Ithaca, NY, USA
Friedemann Mattern
ETH Zurich, Switzerland
John C. Mitchell
Stanford University, CA, USA
Moni Naor
Weizmann Institute of Science, Rehovot, Israel
Oscar Nierstrasz
University of Bern, Switzerland
C. Pandu Rangan
Indian Institute of Technology, Madras, India
Bernhard Steffen
University of Dortmund, Germany
Madhu Sudan
Massachusetts Institute of Technology, MA, USA
Demetri Terzopoulos
University of California, Los Angeles, CA, USA
Doug Tygar
University of California, Berkeley, CA, USA
Moshe Y. Vardi
Rice University, Houston, TX, USA
Gerhard Weikum
Max-Planck Institute of Computer Science, Saarbruecken, Germany

Matteo Fischetti David P. Williamson (Eds.)
Integer Programming
and Combinatorial
Optimization
12th International IPCO Conference
Ithaca, NY, USA, June 25-27, 2007
Proceedings
1 3

Volume Editors
Matteo Fischetti
University of Padova
DEI, Dipartimento di Ingegneria dell’Informazione
via Gradenigo, 6/A, 35131 Padova, Italy
E-mail: matteo.ﬁschetti@unipd.it
David P. Williamson
Cornell University
School of Operations Research and Information Engineering
Rhodes 236, Ithaca, NY 14853, USA
E-mail: dpw@orie.cornell.edu
Library of Congress Control Number: 2007928347
CR Subject Classiﬁcation (1998): G.1.6, G.2.1, F.2.2, I.3.5
LNCS Sublibrary: SL 1 – Theoretical Computer Science and General Issues
ISSN
0302-9743
ISBN-10
3-540-72791-4 Springer Berlin Heidelberg New York
ISBN-13
978-3-540-72791-0 Springer Berlin Heidelberg New York
This work is subject to copyright. All rights are reserved, whether the whole or part of the material is
concerned, speciﬁcally the rights of translation, reprinting, re-use of illustrations, recitation, broadcasting,
reproduction on microﬁlms or in any other way, and storage in data banks. Duplication of this publication
or parts thereof is permitted only under the provisions of the German Copyright Law of September 9, 1965,
in its current version, and permission for use must always be obtained from Springer. Violations are liable
to prosecution under the German Copyright Law.
Springer is a part of Springer Science+Business Media
springer.com
© Springer-Verlag Berlin Heidelberg 2007
Printed in Germany
Typesetting: Camera-ready by author, data conversion by Scientiﬁc Publishing Services, Chennai, India
Printed on acid-free paper
SPIN: 12069569
06/3180
5 4 3 2 1 0

Preface
This volume contains the papers selected for presentation at IPCO XII, the 12th
Conference on Integer Programming and Combinatorial Optimization, held June
25–27, 2007, in Ithaca, New York, USA. Since its inception in 1990, the IPCO
conference series has become an important forum for researchers and practi-
tioners working on various aspects of integer programming and combinatorial
optimization. The aim of the conference is to present recent developments in
theory, computation, and applications in these areas.
IPCO is sponsored by the Mathematical Programming Society, and is held in
those years in which no International Symposium on Mathematical Programming
takes place. The previous Symposium was held in 2006 in Rio de Janeiro, Brazil,
and the previous two IPCOs were held in 2004 and 2005 in New York, USA and
Berlin, Germany, respectively.
There were over 120 submissions to the conference. During its meeting in
early January of 2007, the Program Committee carefully selected 36 papers for
presentation in non-parallel sessions at the conference. Because of the limited
number of time slots for presentations, many excellent submissions could not be
accepted.
During the selection process, the extended abstracts were refereed according
to the standards of refereed conferences. As a result, this volume contains papers
describing high-quality research eﬀorts. The page limit for contributions to these
proceedings was set to 15. We expect full versions of these papers to appear in
scientiﬁc journals in the near future.
We gratefully acknowledge IBM Research, ILOG, and the Oﬃce of Naval
Research for their sponsorship of IPCO 2007. We are grateful for the use of
EasyChair (www.easychair.org), which greatly simpliﬁed the process of col-
lecting submissions, reviewing papers, and assembling this proceedings volume.
We thank Phoebe Sengers and the Culturally Embedded Computing Group at
Cornell, whose server was used to host the IPCO 2007 Web site. We thank the
members of the Program Committee and the many subreferees who spent un-
told hours examining all of the submissions. And ﬁnally, we especially thank the
many authors for submitting their work to the conference.
March 2007
Matteo Fischetti
David P. Williamson

Conference Organization
Program Committee
Dimitris Bertsimas (MIT)
Dan Bienstock (Columbia)
Alberto Caprara (Bologna)
Bill Cook (Georgia Tech)
G´erard Cornu´ejols (CMU)
Matteo Fischetti, Chair (Padova)
Bertrand Guenin (Waterloo)
Christoph Helmberg (TU Chemnitz)
Tibor Jord´an (ELTE Budapest)
Tom McCormick (UBC)
David P. Williamson (Cornell)
Gerhard Woeginger (Eindhoven)
Local Organization
David P. Williamson (Cornell)
Conference Sponsors

Table of Contents
Session 1
Inequalities from Two Rows of a Simplex Tableau . . . . . . . . . . . . . . . . . . . .
1
Kent Andersen, Quentin Louveaux, Robert Weismantel, and
Laurence A. Wolsey
Cuts for Conic Mixed-Integer Programming . . . . . . . . . . . . . . . . . . . . . . . . .
16
Alper Atamt¨urk and Vishnu Narayanan
Sequential-Merge Facets for Two-Dimensional Group Problems . . . . . . . .
30
Santanu S. Dey and Jean-Philippe P. Richard
Session 2
Triangle-Free Simple 2-Matchings in Subcubic Graphs . . . . . . . . . . . . . . . .
43
David Hartvigsen and Yanjun Li
The Smoothed Number of Pareto Optimal Solutions in Bicriteria
Integer Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
Rene Beier, Heiko R¨oglin, and Berthold V¨ocking
Finding a Polytope from Its Graph in Polynomial Time . . . . . . . . . . . . . . .
68
Eric J. Friedman
Session 3
Orbitopal Fixing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
Volker Kaibel, Matthias Peinhardt, and Marc E. Pfetsch
New Variants of Lift-and-Project Cut Generation from the LP Tableau:
Open Source Implementation and Testing . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
Egon Balas and Pierre Bonami
Orbital Branching. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
James Ostrowski, JeﬀLinderoth, Fabrizio Rossi, and
Stefano Smriglio
Session 4
Distinct Triangle Areas in a Planar Point Set . . . . . . . . . . . . . . . . . . . . . . . .
119
Adrian Dumitrescu and Csaba D. T´oth
Scheduling with Precedence Constraints of Low Fractional Dimension. . .
130
Christoph Amb¨uhl, Monaldo Mastrolilli, Nikolaus Mutsanas, and
Ola Svensson

VIII
Table of Contents
Approximation Algorithms for 2-Stage Stochastic Scheduling
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
David B. Shmoys and Mauro Sozio
Session 5
On Integer Programming and the Branch-Width of the Constraint
Matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
158
William H. Cunningham and Jim Geelen
Matching Problems in Polymatroids Without Double Circuits . . . . . . . . .
167
M´arton Makai, Gyula Pap, and J´acint Szab´o
Maximizing a Submodular Set Function Subject to a Matroid
Constraint . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182
Gruia Calinescu, Chandra Chekuri, Martin P´al, and Jan Vondr´ak
Session 6
On a Generalization of the Master Cyclic Group Polyhedron . . . . . . . . . .
197
Sanjeeb Dash, Ricardo Fukasawa, and Oktay G¨unl¨uk
A Framework to Derive Multidimensional Superadditive Lifting
Functions and Its Applications. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
210
Bo Zeng and Jean-Philippe P. Richard
On the Exact Separation of Mixed Integer Knapsack Cuts. . . . . . . . . . . . .
225
Ricardo Fukasawa and Marcos Goycoolea
Session 7
A Faster Strongly Polynomial Time Algorithm for Submodular
Function Minimization. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
240
James B. Orlin
On Convex Minimization over Base Polytopes . . . . . . . . . . . . . . . . . . . . . . .
252
Kiyohito Nagano
Computational Geometric Approach to Submodular Function
Minimization for Multiclass Queueing Systems . . . . . . . . . . . . . . . . . . . . . .
267
Toshinari Itoko and Satoru Iwata
Session 8
Generating Multiple Solutions for Mixed Integer Programming
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
280
Emilie Danna, Mary Fenelon, Zonghao Gu, and Roland Wunderling
A Branch and Bound Algorithm for Max-Cut Based on Combining
Semideﬁnite and Polyhedral Relaxations . . . . . . . . . . . . . . . . . . . . . . . . . . . .
295
Franz Rendl, Giovanni Rinaldi, and Angelika Wiegele

Table of Contents
IX
DINS, a MIP Improvement Heuristic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
310
Shubhashis Ghosh
Session 9
Mixed-Integer Vertex Covers on Bipartite Graphs . . . . . . . . . . . . . . . . . . . .
324
Michele Conforti, Bert Gerards, and Giacomo Zambelli
On the MIR Closure of Polyhedra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
337
Sanjeeb Dash, Oktay G¨unl¨uk, and Andrea Lodi
The Intersection of Continuous Mixing Polyhedra and the Continuous
Mixing Polyhedron with Flows. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
352
Michele Conforti, Marco Di Summa, and Laurence A. Wolsey
Session 10
Simple Explicit Formula for Counting Lattice Points of Polyhedra . . . . . .
367
Jean B. Lasserre and Eduardo S. Zeron
Characterizations of Total Dual Integrality . . . . . . . . . . . . . . . . . . . . . . . . . .
382
Edwin O’Shea and Andr´as Seb˝o
Sign-Solvable Linear Complementarity Problems . . . . . . . . . . . . . . . . . . . . .
397
Naonori Kakimura
Session 11
An Integer Programming Approach for Linear Programs with
Probabilistic Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
410
James Luedtke, Shabbir Ahmed, and George Nemhauser
Infrastructure Leasing Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
424
Barbara M. Anthony and Anupam Gupta
Robust Combinatorial Optimization with Exponential Scenarios . . . . . . .
439
Uriel Feige, Kamal Jain, Mohammad Mahdian, and Vahab Mirrokni
Session 12
Approximation Algorithms for the Multi-item Capacitated Lot-Sizing
Problem Via Flow-Cover Inequalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
454
Retsef Levi, Andrea Lodi, and Maxim Sviridenko
Optimal Eﬃciency Guarantees for Network Design Mechanisms . . . . . . . .
469
Tim Roughgarden and Mukund Sundararajan
The Set Connector Problem in Graphs. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
484
Takuro Fukunaga and Hiroshi Nagamochi
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
499

Inequalities from Two Rows of a Simplex
Tableau⋆
Kent Andersen1, Quentin Louveaux2, Robert Weismantel3,
and Laurence A. Wolsey4
1 Institute for Mathematical Sciences, University of Copenhagen, Denmark
kha@math.ku.dk
2 CORE and INMA, Universit´e catholique de Louvain, Belgium
louveaux@core.ucl.ac.be
3 Department of Mathematics, Otto-von-Guericke Universit¨at, Magdeburg, Germany
weismant@mail.math.uni-magdeburg.de
4 CORE and INMA, Universit´e catholique de Louvain, Belgium
wolsey@core.ucl.ac.be
Abstract. In this paper we explore the geometry of the integer points in
a cone rooted at a rational point. This basic geometric object allows us to
establish some links between lattice point free bodies and the derivation
of inequalities for mixed integer linear programs by considering two rows
of a simplex tableau simultaneously.
1
Introduction
Throughout this paper we investigate a mixed integer linear program (MIP) with
rational data deﬁned for a set I of integer variables and a set C of continuous
variables
(MIP)
max cT x subject to Ax = b, x ≥0, xi ∈Z for i ∈I.
Let LP denote the linear programming relaxation of MIP. From the theory of
linear programming it follows that a vertex x∗of the LP corresponds to a basic
feasible solution of a simplex tableau associated with subsets B and N of basic
and nonbasic variables
xi +

j∈N
¯ai,jxj = ¯bi for i ∈B.
Any row associated with an index i ∈B ∩I such that ¯bi ̸∈Z gives rise to a set
X(i) :=

x ∈R|N| | ¯bi −

j∈N
¯ai,jxj ∈Z, xj ≥0 for all j ∈N

⋆This work was partly carried out within the framework of ADONET, a European
network in Algorithmic Discrete Optimization, contract no. MRTN-CT-2003-504438.
The second author is supported by FRS-FNRS. This text presents research results of
the Belgian Program on Interuniversity Poles of Attraction initiated by the Belgian
State, Prime Minister’s Oﬃce, Science Policy Programming. The scientiﬁc responsi-
bility is assumed by the authors.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 1–15, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

2
K. Andersen et al.
whose analysis provides inequalities that are violated by x∗. Indeed, Gomory’s
mixed integer cuts [4] and mixed integer rounding cuts [6] are derived from such
a basic set X(i) using additional information about integrality of some of the
variables. Interestingly, unlike in the pure integer case, no ﬁnite convergence
proof of a cutting plane algorithm is known when Gomory’s mixed integer cuts
or mixed integer rounding cuts are applied only. More drastically, in [3], an
interesting mixed integer program in three variables is presented, and it is shown
that applying split cuts iteratively does not suﬃce to generate the cut that is
needed to solve this problem.
Example 1: [3] Consider the mixed integer set
t ≤x1,
t ≤x2,
x1 + x2 + t ≤2,
x ∈Z2 and t ∈R1
+.
The projection of this set onto the space of x1 and x2 variables is given by
{(x1, x2) ∈R2
+ : x1 + x2 ≤2} and is illustrated in Fig. 1. A simple analysis
shows that the inequality x1 + x2 ≤2, or equivalently t ≤0, is valid. In [3] it
is, however, shown that with the objective function z = max t, a cutting plane
algorithm based on split cuts does not converge ﬁnitely.
⊓⊔
2
1
2
1
r2
r1
r3
0
x1 + x2 ≤2
f = ( 2
3 . 2
3 )
x2
x1
Fig. 1. The Instance in [3]
The analysis given in this paper will allow us to explain the cut t ≤0 of Example
1. To this end we consider two indices i1, i2 ∈B ∩I simultaneously. It turns out
that the underlying basic geometric object is signiﬁcantly more complex than
its one-variable counterpart. The set that we denote by X(i1, i2) is described as
X(i1, i2) :=

x ∈R|N| | ¯bi −

j∈N
¯ai,jxj ∈Z for i = i1, i2, xj ≥0 for all j ∈N

.

Inequalities from Two Rows of a Simplex Tableau
3
Setting
f :=
¯bi1,¯bi2
T ∈R2, and
rj :=

¯ai1,j, ¯ai2,j
T ∈R2,
the set obtained from two rows of a simplex tableau can be represented as
PI := {(x, s) ∈Z2 × Rn
+ : x = f +

j∈N
sjrj},
where f is fractional and rj ∈R2 for all j ∈N. Valid inequalities for the set PI
was studied in [5] by using superadditive functions related to the group problem
associated with two rows. In this paper, we give a characterization of the facets
of conv(PI) based on its geometry.
Example 1 (revisited): For the instance of Example 1, introduce slack vari-
ables, s1, s2 and y1 in the three constraints. Then, solving as a linear program,
the constraints of the optimal simplex tableau are
t
+ 1
3s1 + 1
3s2 + 1
3y1 = 2
3
x1
−2
3s1 + 1
3s2 + 1
3y1 = 2
3
x2 + 1
3s1 −2
3s2 + 1
3y1 = 2
3
Taking the last two rows, and rescaling using s′
i = si/3 for i = 1, 2, we obtain
the set PI
x1
−2s′
1 +1s′
2 + 1
3y1 = + 2
3
x2
+1s′
1 −2s′
2 + 1
3y1 = + 2
3
x ∈Z2, s ∈R2
+, y1 ∈R1
+.
Letting f = ( 2
3, 2
3)T , r1 = (2, −1)T, r2 = (−1, 2)T and r3 = (−1
3, −1
3)T (see
Fig. 1), one can derive a cut for conv(PI) of the form
x1 + x2 + y1 ≥2 or equivalently t ≤0,
which, when used in a cutting plane algorithm, yields immediate termination.
⊓⊔
Our main contribution is to characterize geometrically all facets of conv(PI).
All facets are intersection cuts [2], i.e., they can be obtained from a (two-
dimensional) convex body that does not contain any integer points in its interior.
Our geometric approach is based on two important facts that we prove in this
paper
– every facet is derivable from at most four nonbasic variables.
– with every facet F one can associate three or four particular vertices of
conv(PI). The classiﬁcation of F depends on how the corresponding k = 3, 4
integer points in Z2 can be partitioned into k sets of cardinality at most two.
More precisely, the facets of conv(PI) can be distinguished with respect to the
number of sets that contain two integer points. Since k = 3 or k = 4, the
following interesting situations occur

4
K. Andersen et al.
– no sets with cardinality two: all the k ∈{3, 4} sets contain exactly one tight
integer point. We call cuts of this type disection cuts.
– exactly one set has cardinality two: in this case we show that the inequality
can be derived from lifting a cut associated with a two-variable subproblem
to k variables. We call these cuts lifted two-variable cuts.
– two sets have cardinality two. In this case we show that the corresponding
cuts are split cuts.
Furthermore, we show that inequalities of the ﬁrst two families are not split
cuts. Our geometric approach allows us to generalize the cut introduced in Ex-
ample 1. More speciﬁcally, the cut of Example 1 is a degenerate case in the sense
that it is “almost” a disection cut and “almost” a lifted two-variable cut: by
perturbing the vectors r1, r2 and r3 slightly, the cut in Example 1 can become
both a disection cut and a lifted two-variable cut.
We review some basic facts about the structure of conv(PI) in Section 2. In
Section 3 we explore the geometry of all the feasible points that are tight for a
given facet of conv(PI), explain our main result and presents the classiﬁcation
of all the facets of conv(PI).
2
Basic Structure of conv(PI)
The basic mixed-integer set considered in this paper is
PI := {(x, s) ∈Z2 × Rn
+ : x = f +

j∈N
sjrj},
(1)
where N := {1, 2, . . ., n}, f ∈Q2 \ Z2 and rj ∈Q2 for all j ∈N. The set
PLP := {(x, s) ∈R2 × Rn
+ : x = f + 
j∈N sjrj} denotes the LP relaxation of
PI. The jth unit vector in Rn is denoted ej. In this section, we describe some
basic properties of conv(PI). The vectors {rj}j∈N are called rays, and we assume
rj ̸= 0 for all j ∈N.
In the remainder of the paper we assume PI ̸= ∅. The next lemma gives a
characterization of conv(PI) in terms of vertices and extreme rays.
Lemma 1.
(i) The dimension of conv(PI) is n.
(ii) The extreme rays of conv(PI) are (rj, ej) for j ∈N.
(iii) The vertices (xI, sI) of conv(PI) take the following two forms:
(a) (xI, sI) = (xI, sI
jej), where xI = f + sI
jrj ∈Z2 and j ∈N
(an integer point on the ray {f + sjrj : sj ≥0}).
(b) (xI, sI) = (xI, sI
jej +sI
kek), where xI = f +sI
jrj +sI
krk ∈Z2 and j, k ∈N
(an integer point in the set f + cone({rj, rk})).
Using Lemma 1, we now give a simple form for the valid inequalities for conv(PI)
considered in the remainder of the paper.

Inequalities from Two Rows of a Simplex Tableau
5
Corollary 1. Every non-trivial valid inequality for PI that is tight at a point
(¯x, ¯s) ∈PI can be written in the form

j∈N
αjsj ≥1,
(2)
where αj ≥0 for all j ∈N.
For an inequality 
j∈N αjsj ≥1 of the form (2), let N 0
α := {j ∈N : αj = 0}
denote the variables with coeﬃcient zero, and let N ̸=0
α
:= N \ N 0
α denote the re-
mainder of the variables. We now introduce an object that is associated with the
inequality 
j∈N αjsj ≥1. We will use this object to obtain a two dimensional
representation of the facets of conv(PI).
Lemma 2. Let 
j∈N αjsj ≥1 be a valid inequality for conv(PI) of the form
(2). Deﬁne vj := f +
1
αj rj for j ∈N ̸=0
α . Consider the convex polyhedron in R2
Lα := {x ∈R2 : there exists s ∈Rn
+ s.t. (x, s) ∈PLP and

j∈N
αjsj ≤1}.
(i) Lα = conv({f} ∪{vj}j∈N ̸=0
α )+ cone({rj}j∈N 0
α).
(ii) interior(Lα) does not contain any integer points.
(iii) If cone({rj}j∈N) = R2, then f ∈interior(Lα).
Example 2: Consider the set
PI = {(x, s) : x = f +

2
1
	
s1 +

1
1
	
s2 +

−3
2
	
s3 +

0
−1
	
s4 +

1
−2
	
s5},
where f =
 1
41
2
	
, and consider the inequality
2s1 + 2s2 + 4s3 + s4 + 12
7 s5 ≥1.
(3)
The corresponding set Lα is shown in Fig. 2. As can be seen from the ﬁgure,
Lα does not contain any integer points in its interior. It follows that (3) is valid
for conv(PI). Note that, conversely, the coeﬃcients αj for j = 1, 2, . . . , 5 can be
obtained from the polygon Lα as follows: αj is the ratio between the length of
rj and the distance between f and vj. In particular, if the length of rj is 1, then
αj is the inverse of the distance from f to vj.
⊓⊔
The interior of Lα gives a two-dimensional representation of the points x ∈
R2 that are aﬀected by the addition of the inequality 
j∈N αjsj ≥1 to the
LP relaxation PLP of PI. In other words, for any (x, s) ∈PLP that satisﬁes

j∈N αjsj < 1, we have x ∈interior(Lα). Furthermore, for a facet deﬁning
inequality 
j∈N αjsj ≥1 of conv(PI), there exist n aﬃnely independent points

6
K. Andersen et al.
r1
r2
r3
r4
r5
v1
v2
v3
v4
v5
Fig. 2. The set Lα for a valid inequality for conv(PI)
(xi, si) ∈PI, i = 1, 2, . . . , n, such that 
j∈N αjsi
j = 1. The integer points
{xi}i∈N are on the boundary of Lα, i.e., they belong to the integer set:
Xα := {x ∈Z2 : ∃s ∈Rn
+ s.t. (x, s) ∈PLP and

j∈N
αjsj = 1}.
We have Xα = Lα ∩Z2, and Xα ̸= ∅whenever 
j∈N αjsj ≥1 deﬁnes a
facet of conv(PI). We ﬁrst characterize the facets of conv(PI) that have zero
coeﬃcients.
Lemma 3. Any facet deﬁning inequality 
j∈N αjsj ≥1 for conv(PI) of the
form (2) that has zero coeﬃcients is a split cut. In other words, if N 0
α ̸= ∅, there
exists (π, π0) ∈Z2 × Z such that Lα ⊆{(x1, x2) : π0 ≤π1x1 + π2x2 ≤π0 + 1}.
Proof: Let k ∈N 0
α be arbitrary. Then the line {f +μrk : μ ∈R} does not contain
any integer points. Furthermore, if j ∈N 0
α, j ̸= k, is such that rk and rj are not
parallel, then f + cone({rk, rj}) contains integer points. It follows that all rays
{rj}j∈N 0
α are parallel. By letting π′ := (rk)⊥= (−rk
2, rk
1)T and π′
0 := (π′)T f, we
have that {f + μrk : μ ∈R} = {x ∈R2 : π′
1x1 + π′
2x2 = π′
0}. Now deﬁne:
π1
0 := max{π′
1x1 + π′
2x2 : π′
1x1 + π′
2x2 ≤π′
0 and x ∈Z2}, and
π2
0 := min{π′
1x1 + π′
2x2 : π′
1x1 + π′
2x2 ≥π′
0 and x ∈Z2}.
We have π1
0 < π′
0 < π2
0, and the set Sπ := {x ∈R2 : π1
0 ≤π′
1x1 + π′
2x2 ≤π2
0}
does not contain any integer points in its interior. We now show Lα ⊆Sπ by
showing that every vertex vm = f +
1
αm rm of Lα, where m ∈N ̸=0
α , satisﬁes
vm ∈Sπ. Suppose vm satisﬁes π′
1vm
1 +π′
2vm
2 < π1
0 (the case π′
1vm
1 +π′
2vm
2 > π2
0 is
symmetric). By deﬁnition of π1
0, there exists xI ∈Z2 such that π′
1xI
1+π′
2xI
2 = π1
0,
and xI = λvm + (1 −λ)(f + δrk), where λ ∈]0, 1[, for some δ > 0. We then have

Inequalities from Two Rows of a Simplex Tableau
7
xI = f+ λ
αm rm+ δ(1−λ)rk. Inserting this representation of xI into the inequality

j∈N αjsj ≥1 then gives αm λ
αm + αkδ(1 −λ) = λ < 1, which contradicts the
validity of 
j∈N αjsj ≥1 for PI. Hence Lα ⊆Sπ.
To ﬁnish the proof, we show that we may write Sπ = {x ∈R2 : π0 ≤π1x1 +
π2x2 ≤π0 + 1} for some (π, π0) ∈Z2 × Z. First observe that we can assume (by
scaling) that π′, π1
0 and π2
0 are integers. Next observe that any common divisor
of π′
1 and π′
2 also divides both π1
0 and π2
0 (this follows from the fact that there
exists x1, x2 ∈Z2 such that π′
1x1
1 + π′
2x1
2 = π1
0 and π′
1x2
1 + π′
2x2
2 = π2
0). Hence we
can assume that π′
1 and π′
2 are relative prime. Now the Integral Farkas Lemma
(see [8]) implies that the set {x ∈Z2 : π′
1x1 + π′
2x2 = 1} is non-empty. It follows
that we must have π2
0 = π1
0 + 1, since otherwise the point ¯y := x′ + x1 ∈Z2,
where x′ ∈{x ∈Z2 : π′
1x1 + π′
2x2 = 1} and x1 ∈{x ∈Z2 : π′
1x1 + π′
2x2 = π1
0},
satisﬁes π1
0 < π′
1¯y1 + π′
2¯y2 < π2
0, which contradicts that Sπ does not contain any
integer points in its interior.
⊓⊔
3
A Characterization of conv(Xα) and conv(PI)
As a preliminary step of our analysis, we ﬁrst characterize the set conv(Xα). We
assume αj > 0 for all j ∈N. Clearly conv(Xα) is a convex polygon with only
integer vertices, and since Xα ⊆Lα, conv(Xα) does not have any integer points
in its interior. We ﬁrst limit the number of vertices of conv(Xα) to four (see [1]
and [7] for this and related results).
Lemma 4. Let P ⊂R2 be a convex polygon with integer vertices that has no
integer points in its interior.
(i) P has at most four vertices
(ii) If P has four vertices, then at least two of its four facets are parallel.
(iii) If P is not a triangle with integer points in the interior of all three facets
(see Fig. 3.(c)), then there exists parallel lines πx = π0 and πx = π0 + 1,
(π, π0) ∈Z3, such that P is contained in the corresponding split set, i.e.,
P ⊆{x ∈R2 : π0 ≤πx ≤π0 + 1}.
Lemma 4 shows that the polygons in Fig. 3 include all possible polygons that
can be included in the set Lα in the case when Lα is bounded and of dimension
2. The dashed lines in Fig. 3 indicate the possible split sets that include P. We
excluded from Fig. 3 the cases when Xα is of dimension 1. We note that Lemma
4.(iii) (existence of split sets) proves that there cannot be any triangles where
two facets have interior integer points, and also that no quadrilateral can have
more than two facets that have integer points in the interior.
Next, we focus on the set Lα. As before we assume αj > 0 for all j ∈N.
Due to the direct correspondence between the set Lα and a facet deﬁning in-
equality 
j∈N αjsj ≥1 for conv(PI), this gives a characterization of the facets

8
K. Andersen et al.
(a) A triangle: no facet
has interior integer points
(b) A triangle: one facet
has interior integer points
(c) A triangle: all facets
have
interior
integer
points
(d) A quadrilateral: no
facet has interior integer
points
(e) A quadrilateral: one
facet has interior integer
points
(f) A quadrilateral: two
facets have interior inte-
ger points
Fig. 3. All integer polygons that do not have interior integer points
of conv(PI). The main result in this section is that Lα can have at most four
vertices. In other words, we prove
Theorem 1. Let 
j∈N αjsj ≥1 be a facet deﬁning inequality for conv(PI) that
satisﬁes αj > 0 for all j ∈N. Then Lα is a polygon with at most four vertices.
Theorem 1 shows that there exists a set S ⊆N such that |S| ≤4 and 
j∈S αjsj ≥
1 is facet deﬁning for conv(PI(S)), where
PI(S) := {(x, s) ∈Z2 × R|S|
+ : x = f +

j∈S
sjrj}.
Throughout this section we assume that no two rays point in the same di-
rection. If two variables j1, j2 ∈N are such that j1 ̸= j2 and rj1 = δrj2
for some δ > 0, then the halﬂines {x ∈R2 : x = f + sj1rj1, sj1 ≥0} and
{x ∈R2 : x = f + sj2rj2, sj2 ≥0} intersect the boundary of Lα at the same
point, and therefore Lα = conv({f} ∪{vj}j∈N) = conv({f} ∪{vj}j∈N\{j2}),
where vj := f +
1
αj rj for j ∈N. This assumption does therefore not aﬀect the
validity of Theorem 1.
The proof of Theorem 1 is based on characterizing the vertices conv(PI) that
are tight for 
j∈N αjsj ≥1. We show that there exists a subset S ⊆N of
variables and a set of |S| aﬃnely independent vertices of conv(PI) such that

Inequalities from Two Rows of a Simplex Tableau
9
|S| ≤4 and {αj}j∈S is the unique solution to the equality system of the polar
deﬁned by these vertices. The following notation will be used intensively in the
remainder of this section.
Notation 1
(i) The number k ≤4 denotes the number of vertices of conv(Xα).
(ii) The set {xv}v∈K denotes the vertices of conv(Xα), where K := {1, 2, . . ., k}.
Recall that Lemma 1.(iii) demonstrates that for a vertex (¯x, ¯s) of conv(PI), ¯s
is positive on at most two coordinates j1, j2 ∈N, and in that case ¯x ∈f +
cone({rj1, rj2}). If ¯s is positive on only one coordinate j ∈N, then ¯x = f + ¯sjrj,
and the inequality of the polar corresponding to (¯x, ¯s) is αj¯sj ≥1, which simply
states αj ≥
1
¯sj . A point ¯x ∈Z2 that satisﬁes ¯x ∈{x ∈R2 : x = f + sjrj, sj ≥0}
for some j ∈N is called a ray point in the remainder of the paper. In order to
characterize the tight inequalities of the polar that correspond to vertices xv of
conv(Xα) that are not ray points, we introduce the following concepts.
Deﬁnition 1. Let 
j∈N αjsj ≥1 be valid for conv(PI). Suppose ¯x ∈Z2 is not
a ray point, and that ¯x ∈f + cone({rj1, rj2}), where j1, j2 ∈N. This implies
¯x = f + sj1rj1 + sj2rj2, where sj1, sj1 > 0 are unique.
(a) The pair (j1, j2) is said to give a representation of ¯x.
(b) If αj1sj1 + αj2sj2 = 1, (j1, j2) is said to give a tight representation of ¯x wrt.

j∈N αjsj ≥1.
(c) If (i1, i2) ∈N×N satisﬁes cone({ri1, ri2}) ⊆cone({rj1, rj2}), the pair (i1, i2)
is said to deﬁne a subcone of (j1, j2).
Example 2 (continued): Consider again the set
PI = {(x, s) : x = f +
2
1
	
s1 +
1
1
	
s2 +
−3
2
	
s3 +
 0
−1
	
s4 +
 1
−2
	
s5},
where f =
 1
41
2
	
, and the valid inequality 2s1 + 2s2 + 4s3 + s4 + 12
7 s5 ≥1 for
conv(PI). The point ¯x = (1, 1) is on the boundary of Lα (see Fig. 2). We have
that ¯x can be written in any of the following forms
¯x =f+1
4r1+1
4r2,
¯x =f+3
7r1
+ 1
28r3,
¯x =f
+ 3
4r2
+1
4r4.
It follows that (1, 2), (1, 3) and (2, 4) all give representations of ¯x. Note that
(1, 2) and (1, 3) give tight representations of ¯x wrt. the inequality 2s1 + 2s2 +

10
K. Andersen et al.
4s3 + s4 + 12
7 s5 ≥1, whereas (2, 4) does not. Finally note that (1, 5) deﬁnes a
subcone of (2, 4).
⊓⊔
Observe that, for a vertex xv of conv(Xα) which is not a ray point, and a tight
representation (j1, j2) of xv, the corresponding inequality of the polar satisﬁes
αj1tj1 + αj2tj2 = 1, where tj1, tj2 > 0. We now characterize the set of tight
representations of an integer point ¯x ∈Z2, which is not a ray point
Tα(¯x) := {(j1, j2) : (j1, j2) gives a tight representation of ¯x wrt.

j∈N
αjsj ≥1}.
We show that Tα(¯x) contains a unique maximal representation (j ¯x
1 , j ¯x
2 ) ∈
Tα(¯x) with the following properties.
Lemma 5. There exists a unique maximal representation (j ¯x
1 , j ¯x
2 ) ∈Tα(¯x) of ¯x
that satisﬁes:
(i) Every subcone (j1, j2) of (j ¯x
1 , j ¯x
2 ) that gives a representation of ¯x satisﬁes
(j1, j2) ∈Tα(¯x).
(ii) Conversely, every (j1, j2) ∈Tα(¯x) deﬁnes a subcone of (j ¯x
1 , j ¯x
2 ).
To prove Lemma 5, there are two cases to consider. For two representations
(i1, i2) and (j1, j2) of ¯x, either one of the two cones (i1, i2) and (j1, j2) is contained
in the other (Lemma 6), or their intersection deﬁnes a subcone of both (i1, i2)
and (j1, j2) (Lemma 7). Note that we cannot have that their intersection is
empty, since they both give a representation of ¯x.
Lemma 6. Let 
j∈N αjsj ≥1 be a facet deﬁning inequality for conv(PI) that
satisﬁes αj > 0 for all j ∈N, and let ¯x ∈Z2. Then (j1, j2) ∈Tα(¯x) implies
(i1, i2) ∈Tα(¯x) for every subcone (i1, i2) of (j1, j2) that gives a representation
of ¯x.
Proof: Suppose (j1, j2) ∈Tα(¯x). Observe that it suﬃces to prove the following:
for any j3 ∈N such that rj3 ∈cone({rj1, rj2}) and (j1, j3) gives a representation
of ¯x, the representation (j1, j3) is tight wrt. 
j∈N αjsj ≥1. The result for all
remaining subcones of (j1, j2) follows from repeated application of this result.
For simplicity we assume j1 = 1, j2 = 2 and j3 = 3.
Since ¯x ∈f +cone({r1, r2}), ¯x ∈f +cone({r1, r3}) and r3 ∈cone({r1, r2}), we
may write ¯x = f +u1r1 +u2r2, ¯x = f +v1r1 +v3r3 and r3 = w1r1 +w2r2, where
u1, u2, v1, v3, w1, w2 ≥0. Furthermore, since (1, 2) gives a tight representation of
¯x wrt. 
j∈N αjsj ≥1, we have α1u1+α2u2 = 1. Finally we have α1v1+α3v3 ≥1,
since 
j∈N αjsj ≥1 is valid for PI. If also α1v1 + α3v3 = 1, we are done, so
suppose α1v1 + α3v3 > 1.
We ﬁrst argue that this implies α3 > α1w1+α2w2. Since ¯x = f +u1r1+u2r2 =
f + v1r1 + v3r3, it follows that (u1 −v1)r1 = v3r3 −u2r2. Now, using the
representation r3 = w1r1 + w2r2, we get (u1 −v1 −v3w1)r1 + (u2 −v3w2)r2 = 0.
Since r1 and r2 are linearly independent, we obtain:
(u1 −v1) = v3w1 and u2 = v3w2.

Inequalities from Two Rows of a Simplex Tableau
11
Now we have α1v1+α3v3 > 1 = α1u1+α2u2, which implies (v1−u1)α1−α2u2+
α3v3 > 0. Using the identities derived above, we get −v3w1α1−α2v3w2 +α3v3 >
0, or equivalently v3(−w1α1 −α2w2 +α3) > 0. It follows that α3 > α1w1 +α2w2.
We now derive a contradiction to the identity α3 > α1w1 + α2w2. Since

j∈N αjsj ≥1 deﬁnes a facet of conv(PI), there must exist x′ ∈Z2 and k ∈N
such that (3, k) gives a tight representation of x′ wrt. 
j∈N αjsj ≥1. In other
words, there exists x′ ∈Z2, k ∈N and δ3, δk ≥0 such that x′ = f + δ3r3 + δkrk
and α3δ3 + αkδk = 1. Furthermore, we can choose x′, δ3 and δk such that r3 is
used in the representation of x′, i.e., we can assume δ3 > 0.
Now, using the representation r3 = w1r1 + w2r2 then gives x′ = f + δ3r3 +
δkrk = f +δ3w1r1 +δ3w2r2 +δkrk. Since 
j∈N αjsj ≥1 is valid for PI, we have
α1δ3w1+α2δ3w2+ αkδk ≥1 = α3δ3+αkδk. This implies δ3(α3−α1w1−α2w2) ≤
0, and therefore α3 ≤α1w1 −α2w2, which is a contradiction.
⊓⊔
Lemma 7. Let 
j∈N αjsj ≥1 be a facet deﬁning inequality for conv(PI) satis-
fying αj > 0 for j ∈N, and suppose ¯x ∈Z2 is not a ray point. Also suppose the
intersection between the cones (j1, j2), (j3, j4) ∈Tα(¯x) is given by the subcone
(j2, j3) of both (j1, j2) and (j3, j4). Then (j1, j4) ∈Tα(¯x), i.e., (j1, j4) also gives
a tight representation of ¯x.
Proof: For simplicity assume j1 = 1, j2 = 2, j3 = 3 and j4 = 4. Since the cones
(1, 2) and (3, 4) intersect in the subcone (2, 3), we have r3 ∈cone({r1, r2}), r2 ∈
cone({r3, r4}), r4 /∈cone({r1, r2}) and r1 /∈cone({r3, r4}). We ﬁrst represent ¯x
in the translated cones in which we have a tight representation of ¯x. In other
words, we can write
¯x = f + u1r1 + u2r2,
(4)
¯x = f + v3r3 + v4r4 and
(5)
¯x = f + z2r2 + z3r3,
(6)
where u1, u2, v3, v4, z2, z3 > 0. Note that Lemma 6 proves that (6) gives a tight
represention of ¯x. Using (4)-(6), we obtain the relation
T1,1I2 T1,2I2
T2,1I2 T2,2I2
r2
r3

=
u1r1
v4r4

,
(7)
where T is the 2 × 2 matrix T :=
T1,1 T1,2
T2,1 T2,2

=
(z2 −u2)
z3
z2
(z3 −v3)

and I2 is
the 2×2 identity matrix. On the other hand, the tightness of the representations
(4)-(6) leads to the following identities
α1u1+α2u2 = 1,
(8)
α3v3+α4v4 = 1 and
(9)
α2z2 +α4z3 = 1,
(10)

12
K. Andersen et al.
where, again, the last identity follows from Lemma 6. Using (8)-(10), we obtain
the relation
T1,1 T1,2
T2,1 T2,2
α2
α3

=
u1α1
v4α4

.
(11)
We now argue that T is non-singular. Suppose, for a contradiction, that
T1,1T2,2 = T1,2T2,1. From (5) and (6) we obtain v4r4 = (z3 −v3)r3 + z2r2, which
implies z3 < v3, since r4 /∈cone({r1, r2}) ⊇cone({r2, r3}). Multiplying the ﬁrst
equation of (11) with T2,2 gives T2,2T1,1α2 + T2,2T1,2α3 = u1T2,2α1, which im-
plies T1,2(T2,1α2 + T2,2α3) = u1T2,2α1. By using the deﬁnition of T , this can be
rewritten as z3(α2z2 + (z3 −v3)α3) = u1α1(z3 −v3). Since z2α2 + z3α3 = 1, this
implies z3(1 −v3α3) = u1α1(z3 −v3). However, from (9) we have v3α3 ∈]0, 1[,
so z3(1 −v3α3) > 0 and u1α1(z3 −v3) < 0, which is a contradiction. Hence T is
non-singular.
We now solve (7) for an expression of r2 and r3 in terms of r1 and r4.
The inverse of the coeﬃcient matrix on the left hand side of (7) is given by
T −1
1,1 I2 T −1
1,2 I2
T −1
2,1 I2 T −1
2,2 I2

, where T −1 :=
T −1
1,1 T −1
1,2
T −1
2,1 T −1
2,2

denotes the inverse of T . We there-
fore obtain
r2 = λ1r1 + λ4r4 and
(12)
r3 = μ1r1 + μ4r4,
(13)
where λ1 := u1T −1
1,1 , λ4 := v4T −1
1,2 , μ1 := u1T −1
2,1 and μ4 := v4T −1
2,2 . Similarly,
solving (11) to express α2 and α3 in terms of α1 and α4 gives
α2 = λ1α1 + λ4α4 and
(14)
α3 = μ1α1 + μ4α4.
(15)
Now, using for instance (4) and (12), we obtain
¯x = f + (u1 + u2λ1)r1 + (u2λ4)r4, and:
(u1 + u2λ1)α1 + (u2λ4)α4
=
(using (8))
(1 −u2α2) + u2λ1α1 + (u2λ4)α4
=
1 + u2(λ1α1 + λ4α4 −α2)
= 1. (using (14))
To ﬁnish the proof, we only need to argue that we indeed have ¯x ∈f +
cone({r1, r4}), i.e., that ¯x = f + δ1r1 + δ4r4 with δ1 = u1 + u2λ1 and δ4 = u2λ4
satisfying δ1, δ4 ≥0. If δ1 ≤0 and δ4 > 0, we have ¯x = f + δ1r1 + δ4r4 =
f + u1r1 + u2r2, which means δ4r4 = (u1 −δ1)r1 + u2r2 ∈cone({r1, r2}), which
is a contradiction. Similarly, if δ1 > 0 and δ4 ≤0, we have ¯x = f + δ1r1 +
δ4r4 = f + v3r3 + v4r4, which implies δ1r1 = v3r3+ (v4 −δ4)r4 ∈cone({r3, r4}),
which is also a contradiction. Hence we can assume δ1, δ4 ≤0. However, since
δ1 = u1 +u2λ1 and δ4 = u2λ4, this implies λ1, λ4 ≤0, and this contradicts what

Inequalities from Two Rows of a Simplex Tableau
13
was shown above, namely that the representation ¯x = f + δ1r1 + δ4r4 satisﬁes
α1δ1 + α4δ4 = 1.
⊓⊔
It follows that only one tight representation of every point x of conv(Xα) is
needed. We now use Lemma 5 to limit the number of vertices of Lα to four. The
following notation is introduced. The set Jx := ∪(j1,j2)∈Tα(x){j1, j2} denotes
the set of variables that are involved in tight representations of x. As above,
(jx
1 , jx
2 ) ∈Tα(x) denotes the unique maximal representation of x. Furthermore,
given any (j1, j2) ∈Tα(x), let (tj2
j1(x), tj1
j2(x)) satisfy x = f +tj2
j1(x)rj1 +tj1
j2(x)rj2.
Lemma 5 implies that rj ∈cone(rjx
1 , rjx
2 ) for every j ∈Jx. Let (wj
1(x), wj
2(x))
satisfy rj = wj
1(x)rjx
1 + wj
2(x)rjx
2 , where wj
1(x), wj
2(x) ≥0 are unique.
Let 
j∈N αjsj ≥1 be a valid inequality for conv(PI) that satisﬁes αj > 0
for j ∈N. The inequality 
j∈N αjsj ≥1 is facet deﬁning for conv(PI), if and
only if the coeﬃcients {αj}j∈N deﬁne a vertex of the polar of conv(PI). Hence

j∈N αjsj ≥1 is facet deﬁning for conv(PI), if and only if the solution to the
system
αj1tj2
j1(x) + αj2tj1
j2(x) = 1,
for every x ∈Xα and (j1, j2) ∈Tα(x).
(16)
is unique. We now rewrite the subsystem of (16) that corresponds to a ﬁxed
point x ∈Xα.
Lemma 8. Let 
j∈N αjsj ≥1 be a facet deﬁning inequality for conv(PI) that
satisﬁes αj > 0 for j ∈N. Suppose x ∈Xα is not a ray point. The system
αj1tj2
j1(x) + αj2tj1
j2(x) = 1,
for every (j1, j2) ∈Tα(x).
(17)
has the same set of solutions {αj}j∈Jx as the system
1 = tj2
j1(x)αj1 + tj1
j2(x)αj2,
for (j1, j2) = (jx
1 , jx
2 ),
(18)
αj = wj
1(x)αjx
1 + wj
2(x)αjx
2 ,
for j ∈Jx \ {jx
1 , jx
2 }.
(19)
We next show that it suﬃces to consider vertices of conv(Xα) in (16).
Lemma 9. Let 
j∈N αjsj ≥1 be a facet deﬁning inequality for conv(PI) that
satisﬁes αj > 0 for j ∈N. Suppose x ∈Xα is not a vertex of conv(Xα). Then
there exists vertices y and z of conv(Xα) such that the equalities
αj1tj2
j1(y) + αj2tj1
j2(y) = 1,
for every (j1, j2) ∈Tα(y) and
(20)
αj1tj2
j1(z) + αj2tj1
j2(z) = 1,
for every (j1, j2) ∈Tα(z)
(21)
imply the equalities:
αj1tj2
j1(x) + αj2tj1
j2(x) = 1,
for every (j1, j2) ∈Tα(x).
(22)
By combining Lemma 8 and Lemma 9 we have that, if the solution to (16) is
unique, then the solution to the system
tjx
2
jx
1 (x)αjx
1 + tjx
1
jx
2 (x)αjx
2 = 1,
for every vertex x of conv(Xα).
(23)

14
K. Andersen et al.
is unique. Since (23) involves exactly k ≤4 equalities and has a unique solution,
exactly k ≤4 variables are involved in (23) as well. This ﬁnishes the proof of
Theorem 1.
We note that from an inequality 
j∈S αjsj ≥1 that deﬁnes a facet of
conv(PI(S)), where |S| = k, the coeﬃcients on the variables j ∈N \ S can be
simultaneously lifted by computing the intersection point between the halﬂine
{f + sjrj : sj ≥0} and the boundary of Lα.
We now use Theorem 2 to categorize the inequalities 
j∈N αjsj ≥1 that
deﬁne facets of conv(PI). For simplicity, we only consider the most general case,
namely when none of the vertices of conv(Xα) are ray points. Furthermore, we
only consider k = 3 and k = 4. When k = 2, 
j∈N αjsj ≥1 is a facet deﬁning
inequality for a cone deﬁned by two rays. We divide the remaining facets of
conv(PI) into the following three main categories.
(i) Disection cuts (Fig. 4.(a) and Fig. 4.(b)):
Every vertex of conv(Xα) belongs to a diﬀerent facet of Lα.
(ii) Lifted two-variable cuts (Fig. 4.(c) and Fig. 4.(d)):
Exactly one facet of Lα contains two vertices of conv(Xα). Observe that this
implies that there is a set S ⊂N, |S| = 2, such that 
j∈S αjsj ≥1 is facet
deﬁning for conv(PI(S)).
(iii) Split cuts:
Two facets of Lα each contain two vertices of conv(Xα).
(a) Disection cut from a triangle
(b) Disection cut from a quadrilateral
(c) Lifted
two-variable
cut
from
quadrilateral
(d) Lifted two-variable cut from tri-
angle
Fig. 4. Disection cuts and lifted two-variable cuts
An example of a cut that is not a split cut was given in [3] (see Fig. 1). This
cut is the only cut when conv(Xα) is the triangle of Fig. 4.(c), and, necessarily,
Lα = conv(Xα) in this case. Hence, all three rays that deﬁne this triangle are

Inequalities from Two Rows of a Simplex Tableau
15
ray points. As mentioned in the introduction, the cut in [3] can be viewed as
being on the boundary between disection cuts and lifted two-variable cuts.
Since the cut presented in [3] is not a split cut, and this cut can be viewed
as being on the boundary between disection cuts and lifted two-variable cuts, a
natural question is whether or not disection cuts and lifted two-variable cuts are
split cuts. We ﬁnish this section by answering this question.
Lemma 10. Let 
j∈N αjsj ≥1 be a facet deﬁning inequality for conv(PI)
satisfying αj > 0 for j ∈N. Also suppose 
j∈N αjsj ≥1 is either a disection
cut or a lifted two-variable cut. Then 
j∈N αjsj ≥1 is not a split cut.
Proof: Observe that, if 
j∈N αjsj ≥1 is a split cut, then there exists (π, π0) ∈
Z2 × Z such that Lα is contained in the split set Sπ := {x ∈R2 : π0 ≤π1x1 +
π2x2 ≤π0 + 1}. Furthermore, all points x ∈Xα and all vertices of Lα must be
either on the line πT x = π0, or on the line πT x = π0 + 1. However, this implies
that there must be two facets of Lα that do not contain any integer points.
⊓⊔
References
1. J. R. Arkinstall.
Minimal requirements for Minkowski’s theorem in the plane I.
Bulletin of the Australian Mathematical Society, 22:259–274, 1980.
2. E. Balas. Intersection cuts - a new type of cutting planes for integer programming.
Operations Research, 19:19–39, 1971.
3. W.J. Cook, R. Kannan, and A. Schrijver. Chv´atal closures for mixed integer pro-
gramming problems. Mathematical Programming, 47:155–174, 1990.
4. R.E. Gomory. An algorithm for the mixed integer problem. Technical Report RM-
2597, The Rand Corporation, 1960a.
5. E. Johnson. On the group problem for mixed integer programming. Mathematical
Programming, 2:137–179, 1974.
6. G.L. Nemhauser and L.A. Wolsey. Integer and combinatorial optimization. Wiley,
1988.
7. S. Rabinowitz. A census of convex lattice polygons with at most one interior lattice
point. Ars Combinatoria, 28:83–96, 1989.
8. A. Schrijver. Theory of linear and integer programming. Wiley, 1986.

Cuts for Conic Mixed-Integer Programming
Alper Atamt¨urk and Vishnu Narayanan
Department of Industrial Engineering and Operations Research, University of
California, Berkeley, CA 94720-1777 USA
atamturk@berkeley.edu, vishnu@ieor.berkeley.edu
Abstract. A conic integer program is an integer programming problem
with conic constraints. Conic integer programming has important ap-
plications in ﬁnance, engineering, statistical learning, and probabilistic
integer programming.
Here we study mixed-integer sets deﬁned by second-order conic con-
straints. We describe general-purpose conic mixed-integer rounding cuts
based on polyhedral conic substructures of second-order conic sets. These
cuts can be readily incorporated in branch-and-bound algorithms that
solve continuous conic programming relaxations at the nodes of the
search tree. Our preliminary computational experiments with the new
cuts show that they are quite eﬀective in reducing the integrality gap of
continuous relaxations of conic mixed-integer programs.
Keywords: Integer programming, conic programming, branch-and-cut.
1
Introduction
In the last two decades there have been major advances in our capability of
solving linear integer programming problems. Strong cutting planes obtained
through polyhedral analysis of problem structure contributed to this success
substantially by strengthening linear programming relaxations of integer pro-
gramming problems. Powerful cutting planes based on simpler substructures of
problems have become standard features of leading optimization software pack-
ages. The use of such structural cuts has improved the performance of the linear
integer programming solvers dramatically.
On another front, since late 1980’s we have experienced signiﬁcant advances
in convex optimization, particularly in conic optimization. Starting with Nes-
terov and Nemirovski [22, 23, 24] polynomial interior point algorithms that have
earlier been developed for linear programming have been extended to conic opti-
mization problems such as convex quadratically constrained quadratic programs
(QCQP’s) and semideﬁnite programs (SDP’s).
Availability of eﬃcient algorithms and publicly available software (CDSP[9],
DSDP[7], SDPA[33], SDPT3[32], SeDuMi[30]) for conic optimization spurred
many optimization and control applications in diverse areas ranging from med-
ical imaging to signal processing, from robust portfolio optimization to truss
design. Commercial software vendors (e.g. ILOG, MOSEK, XPRESS-MP) have
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 16–29, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

Cuts for Conic Mixed-Integer Programming
17
responded to the demand for solving (continuous) conic optimization problems
by including stable solvers for second-order cone programming (SOCP) in their
recent versions.
Unfortunately, the phenomenal advances in continuous conic programming
and linear integer programming have so far not translated to improvements in
conic integer programming, i.e., integer programs with conic constraints. Solu-
tion methods for conic integer programming are limited to branch-and-bound
algorithms that solve continuous conic relaxations at the nodes of the search
tree. In terms of development, conic integer programming today is where linear
integer programming was before 1980’s when solvers relied on pure branch-and-
bound algorithms without the use of any cuts for improving the continuous
relaxations at the nodes of the search tree.
Hereweattemptto improvethesolvabilityofconicinteger programs.Wedevelop
general purpose cuts that can be incorporated into branch-and-bound solvers for
conic integer programs. Toward this end, we describe validcuts for the second-order
conic mixed-integer constraints (deﬁned in Section 2). The choice of second-order
conic mixed-integer constraint is based on (i) the existence of many important ap-
plications modeled with such constraints, (ii) the availability of eﬃcient and stable
solvers for their continuous SOCP relaxations, and (iii) the fact that one can form
SOCP relaxations for the more general conic programs, which make the cuts pre-
sented here widely applicable to conic integer programming.
1.1
Outline
In Section 2 we introduce conic integer mixed-programming, brieﬂy review the rel-
evant literature and explain our approach for generating valid cuts. In Section 3 we
describe conic mixed-integer rounding cuts for second-order conic mixed-integer
programming and in Section 4 we summarize our preliminary computational re-
sults with the cuts.
2
Conic Integer Programming
A conic integer program (CIP) is an integer program with conic constraints. We
limit the presentation here to second-order conic integer programming. However,
as one can relax more general conic programs to second-order conic programs
[14] our results are indeed applicable more generally.
A second-order conic mixed-integer program is an optimization problem of the
form
min cx + ry
(SOCMIP)
s.t. ∥Aix + Giy −bi ∥≤dix + eiy −hi,
i = 1, 2, . . . , k
x ∈Zn, y ∈Rp .
Here ∥· ∥is the Euclidean norm, Ai, Gi, b are rational matrices with mi rows,
and c, r, di, ei are rational row vectors of appropriate dimension, and hi is a

18
A. Atamt¨urk and V. Narayanan
rational scalar. Each constraint of SOCMIP can be equivalently stated as (Aix+
Giy −bi, dix + eiy −h) ∈Qmi+1, where
Qmi+1 := {(t, to) ∈Rmi × R : ∥t ∥≤to} .
For n = 0, SOCMIP reduces to SOCP, which is a generalization of linear pro-
gramming as well as convex quadratically constrained quadratic programming.
If Gi = 0 for all i, then SOCP reduces to linear programming. If ei = 0 for all
i, then it reduces to QCQP after squaring the constraints. In addition, convex
optimization problems with more general norms, fractional quadratic functions,
hyperbolic functions and others can be formulated as an SOCP. We refer the
reader to [2, 6, 10, 18, 25] for a detailed exposure to conic optimization and
many applications of SOCP.
2.1
Relevant Literature
There has been signiﬁcant work on deriving conic (in particular SDP) relax-
ations for (linear) combinatorial optimization problems [1, 13, 19] for obtaining
stronger bounds for such problems than the ones given by their natural linear
programming relaxations. We refer the reader to Goemans [12] for a survey on
this topic. However, our interest here is not to ﬁnd conic relaxations for linear
integer problems, but for conic integer problems.
Clearly any method for general nonlinear integer programming applies to conic
integer programming as well. Reformulation-Linearization Technique (RLT) of
Sherali and Adams [27] initially developed for linear 0-1 programming has been
extended to nonconvex optimization problems [28]. Stubbs and Mehrotra [29]
generalize the lift-and-project method [5] of Balas et. al for 0-1 mixed convex
programming. See also Balas [4] and Sherali and Shetti [26] on disjunctive pro-
gramming methods. Kojima and Tun¸cel [15] give successive semideﬁnite relax-
ations converging to the convex hull of a nonconvex set deﬁned by quadratic
functions. Lasserre [16] describes a hierarchy of semideﬁnite relaxations nonlin-
ear 0-1 programs. Common to all of these general approaches is a hierarchy of
convex relaxations in higher dimensional spaces whose size grows exponentially
with the size of the original formulation. Therefore using such convex relaxations
in higher dimensions is impractical except for very small instances. On the other
hand, projecting these formulations to the original space of variables is also very
diﬃcult except for certain special cases.
Another stream of more practically applicable research is the development
of branch-and-bound algorithms for nonlinear integer programming based on
linear outer approximations [8, 17, 31]. The advantage of linear approximations
is that they can be solved fast; however, the bounds from linear approximations
may not be strong. However, in the case of conic programming, and in particular
second-order cone programming, existence of eﬃcient algorithms permits the use
of continuous conic relaxations at the nodes of the branch-and-bound tree.

Cuts for Conic Mixed-Integer Programming
19
The only study that we are aware of on developing valid inequalities for conic
integer sets directly is due to C¸ezik and Iyengar [11]. For a pointed, closed,
convex cone K ⊆Rm with nonempty interior, given S = {x ∈Zn : b −Ax ∈K},
their approach is to write a linear aggregation
λ′Ax ≤λ′b for some ﬁxed λ ∈K∗,
(1)
where K∗is the dual cone of K and then apply the Chv´atal-Gomory (CG) integer
rounding cuts [20] to this linear inequality. Hence, the resulting cuts are linear
in x as well.
For the mixed-integer case as the convex hull feasible points is
not polyhedral and has curved boundary (see Figure 2 in Section 3). Therefore,
nonlinear inequalities may be more eﬀective for describing or approximating the
convex hull of solutions.
2.2
A New Approach
Our approach for deriving valid inequalities for SOCMIP is to decompose the
second-order conic constraint into simpler polyhedral sets and analyze each of
these sets. Speciﬁcally, given a second-order conic constraint
∥Ax + Gy −b ∥≤dx + ey −h
(2)
and the corresponding second-order conic mixed-integer set
C :=

x ∈Zn
+, y ∈Rp
+ : (x, y) satisﬁes (2)

,
by introducing auxiliary variables (t, to) ∈Rm+1, we reformulate (2) as
to ≤dx + ey −h
(3)
ti ≥|aix + giy −bi| , i = 1, . . . , m
(4)
to ≥∥t ∥,
(5)
where ai and gi denote the ith rows of matrices A and G, respectively. Observe
that each constraint (4) is indeed a second-order conic constraint as (aix+giy −
bi, ti) ∈Q1+1, yet polyhedral. Consequently, we refer to a constraint of the form
(4) as a polyhedral second-order conic constraint.
Breaking (2) into polyhedral conic constraints allows us to exploit the implicit
polyhedral set for each term in a second-order cone constraint. Cuts obtained for
C in this way are linear in (x, y, t); however, they are nonlinear in the original
space of (x, y).
Our approach extends the successful polyhedral method for linear integer
programming where one studies the facial structure of simpler building blocks
to second-order conic integer programming. To the best of our knowledge such
an analysis for second-order conic mixed-integer sets has not been done before.

20
A. Atamt¨urk and V. Narayanan
3
Conic Mixed-Integer Rounding
For a mixed integer set X ⊆Zn × Rp, we use relax(X) to denote its continu-
ous relaxation in Rn × Rp obtained by dropping the integrality restrictions and
conv(X) to denote the convex hull of X. In this section we will describe the cuts
for conic mixed-integer programming, ﬁrst on a simple case with a single integer
variable. Subsequently we will present the general inequalities.
3.1
The Simple Case
Let us ﬁrst consider the mixed-integer set
S0 := {(x, y, w, t) ∈Z × R3
+ : | x + y −w −b | ≤t}
(6)
deﬁned by a simple, yet non-trivial polyhedral second-order conic constraint
with one integer variable. The continuous relaxation relax(S0) has four extreme
rays: (1, 0, 0, 1), (−1, 0, 0, 1), (1, 0, 1, 0), and (−1, 1, 0, 0), and one extreme point:
(b, 0, 0, 0), which is infeasible for S0 if f := b−⌊b⌋> 0. It is easy to see that if f >
0, conv(S0) has four extreme points: (⌊b⌋, f, 0, 0), (⌊b⌋, 0, 0, f), (⌈b⌉, 0, 1 −f, 0)
and (⌈b⌉, 0, 0, 1 −f). Figure 1 illustrates S0 for the restriction y = w = 0.
Proposition 1. The simple conic mixed-integer rounding inequality
(1 −2f)(x −⌊b⌋) + f ≤t + y + w
(7)
cuts oﬀall points in relax(S0) \ conv(S0).
Observe that inequality (7) is satisﬁed at equality at all extreme points of
conv(S0). Proposition 1 can be proved by simply checking that every intersec-
tion of the hyperplanes deﬁning S0 and (7) is one of the four extreme points of
conv(S0) listed above.
x
⌈b⌉
⌊b⌋
b
⌊b⌋−
f
1−2f
t ≥| x −b |
t ≥| (1 −2f)(x −⌊b⌋) + f |
t
1 −f
f
Fig. 1. Simple conic mixed-integer rounding cut

Cuts for Conic Mixed-Integer Programming
21
The simple conic mixed-integer rounding inequality (7) can be used to derive
nonlinear conic mixed-integer inequalities for nonlinear conic mixed-integer sets.
The ﬁrst observation useful in this direction is that the piecewise-linear conic
inequality
|(1 −2f)(x −⌊b⌋) + f| ≤t + y + w
(8)
is valid for S0. See Figure 1 for the restriction y = w = 0.
In order to illustrate the nonlinear conic cuts, based on cuts for the polyhedral
second-order conic constraints (4), let us now consider the simplest nonlinear
second-order conic mixed-integer set
T0 :=

(x, y, t) ∈Z × R × R :

(x −b)2 + y2 ≤t

.
(9)
The continuous relaxation relax(T0) has exactly one extreme point (x, y, t) =
(b, 0, 0), which is infeasible for T0 if b ̸∈Z. Formulating T0 as
t1 ≥|x −b|
(10)
t ≥

t2
1 + y2,
(11)
we write the piecewise-linear conic inequality (8) for (10). Substituting out the
auxiliary variable t1, we obtain the simple nonlinear conic mixed-integer round-
ing inequality

((1 −2f)(x −⌊b⌋) + f)2 + y2 ≤t,
(12)
which is valid for T0.
Proposition 2. The simple nonlinear conic mixed-integer rounding inequality
(12) cuts oﬀall points in relax(T0) \ conv(T0).
Proof. First, observe that for x = ⌊b⌋−δ, the constraint in (9) becomes t ≥

(δ + f)2 + y2, and (12) becomes t ≥

(f −(1 −2f)δ)2 + y2. Since (δ + f)2 −
(f −(1 −2f)δ)2 = 4fδ(1 + δ)(1 −f) ≥0 for δ ≥0 and for δ ≤−1, we see
that (12) is dominated by relax(T0) unless ⌊b⌋< x < ⌈b⌉. When −1 < δ < 0
(i.e., x ∈(⌊b⌋, ⌈b⌉)), 4fδ(1 + δ)(1 −f) < 0, implying that (12) dominates the
constraint in (9).
We now show that if (x1, y1, t1)
∈
relax(T0) and satisﬁes (12), then
(x1, y1, t1) ∈conv(T0). If x1 ̸∈(⌊b⌋, ⌈b⌉), it is suﬃcient to consider (x1, y1, t1) ∈
relax(T0) as (12) is dominated by relax(T0) in this case. Now, the ray R1 :=
{(b, 0, 0) + α(x1 −b, y1, t1)
:
α ∈R+} ⊆relax(T0). Let the intersections of
R1 with the hyperplanes x = ⌊x1⌋and x = ⌈x1⌉be (⌊x1⌋, ¯y1, ¯t1), (⌈x1⌉, ˆy1, ˆt1),
which belong to T0. Then (x1, y1, t1) can be written as a convex combination of
points (⌊x1⌋, ¯y1, ¯t1), (⌈x1⌉, ˆy1, ˆt1); hence (x1, y1, t1) ∈conv(T0).
On the other hand, if x1 ∈(⌊b⌋, ⌈b⌉), it is suﬃcient to consider (x1, y1, t1)
that satisﬁes (12), since (12) dominates the constraint in (9) for x ∈[⌊b⌋, ⌈b⌉].
If f = 1/2, (x1, y1, t1) is a convex combination of (⌊b⌋, y1, t1) and (⌈b⌉, y1, t1).
Otherwise, all points on the ray R2 := {(x0, 0, 0)+α(x1 −x0, y1, t1) : α ∈R+},
where x0 = ⌊b⌋−
f
1−2f , satisfy (12). Let the intersections of R2 with the

22
A. Atamt¨urk and V. Narayanan
hyperplanes x = ⌊b⌋and x = ⌈b⌉be (⌊b⌋, ¯y1, ¯t1), (⌈b⌉, ˆy1, ˆt1), which belong to
T0. Note that the intersections are nonempty because x0 ̸∈[⌊b⌋, ⌈b⌉]. Then we
see that (x1, y1, t1) can be written as a convex combination of (⌊b⌋, ¯y, ¯t) and
(⌈b⌉, ˆy, ˆt). Hence, (x1, y1, t1) ∈conv(T0) in this case as well.
⊓⊔
Proposition 2 shows that the curved convex hull of T0 can be described us-
ing only two second-order conic constraints. The following example illustrates
Proposition 2.
Example 1. Consider the second-order conic set given as
T0 =
⎧
⎨
⎩(x, y, t) ∈Z × R × R :

x −4
3
2
+ (y −1)2 ≤t
⎫
⎬
⎭.
The unique extreme point of relax(T0) ( 4
3, 1, 0) is fractional. Here ⌊b⌋= 1 and
f = 1
3; therefore,
conv(T0) =
⎧
⎨
⎩(x, y, t) ∈R3 :

x −4
3
2
+ (y −1)2 ≤t,

1
9x2 + (y −1)2 ≤t
⎫
⎬
⎭.
We show the inequality

1
9x2 + (y −1)2 ≤t and the region it cuts oﬀin
Figure 2. Observe that the function values are equal at x = 1 and x = 2 and the
cut eliminates the points between them.
Fig. 2. Nonlinear conic integer rounding cut

Cuts for Conic Mixed-Integer Programming
23
3.2
The General Case
In this section we present valid inequalities for the mixed-integer sets deﬁned by
general polyhedral second-order conic constraints (4). Toward this end, let
S := {x ∈Zn
+, y ∈Rp
+, t ∈R : t ≥|ax + gy −b|} .
We refer to the inequalities used in describing S as the trivial inequalities. The
following result simpliﬁes the presentation.
Proposition 3. Any non-trivial facet-deﬁning inequality for conv(S) is of the
form γx + πy ≤π0 + t. Moreover, the following statements hold:
1. πi < 0 for all i = 1, . . . , p;
2.
πi
πj =
 gi
gj
 for all i, j = 1, . . . , p.
Hence it is suﬃcient to consider the polyhedral second-order conic constraint
ax + y+ −y−−b
 ≤t,
(13)
where all continuous variables with positive coeﬃcients are aggregated into y+ ∈
R+ and those with negative coeﬃcients are aggregated into y−∈R+ to represent
a general polyhedral conic constraint of the form (4).
Deﬁnition 1. For 0 ≤f < 1 let the conic mixed-integer rounding function
ϕf : R →R be
ϕf(v) =

(1 −2f)n −(v −n),
if n ≤v < n + f,
(1 −2f)n + (v −n) −2f, if n + f ≤v < n + 1 . n ∈Z
(14)
The conic mixed-integer rounding function ϕf is piecewise linear and continuous.
Figure 3 illustrates ϕf.
Lemma 1. The conic mixed-integer rounding function ϕf is superadditive on R.
Theorem 1. For any α ̸= 0 the conic mixed-integer rounding inequality
n

j=1
ϕfα(aj/α)xj −ϕfα(b/α) ≤(t + y+ + y−)/|α|,
(15)
where fα = b/α −⌊b/α⌋, is valid for S. Moreover, if α = aj and b/aj > 0 for
some j ∈{1, . . . , n}, then (15) is facet-deﬁning for conv(S).
Proof. (Sketch) It can be shown that ϕfaj is the lifting function of inequality
(1 −2f)(x −⌊b⌋) + f ≤(t + y+ + y−)/|aj|
(16)
for the restriction
ajxj + y+ −y−−b
 ≤t

24
A. Atamt¨urk and V. Narayanan
of (13) with xi = 0 for i ̸= j. Then the validity as well as the facet claim follows
from superadditive lifting [3] of (16) with xi for i ̸= j. For α ̸= 0 validity follows
by introducing an auxiliary integer variable xo with coeﬃcient α and lifting
inequality
αxo + y+ −y−−b
 ≤t
with all xi, i = 1, . . . , n and then setting xo = 0.
⊓⊔
Remark 1. The continuous relaxation relax(S) has at most n fractional extreme
points (xj, 0, 0, 0) of the form xj
j = b/aj > 0, and xj
i = 0 for all i ̸= j, which
are infeasible if b/aj ̸∈Z. It is easy to check that conic mixed-integer rounding
inequalities with α = aj are suﬃcient to cut oﬀall fractional extreme points
(xj, 0, 0, 0) of relax(S) as for xj
i = 0 inequality (15) reduces to (7).
v
ϕf(v)
1
1 −2f
f
−f
Fig. 3. Conic mixed-integer rounding function.
Next we show that mixed-integer rounding (MIR) inequalities [21, 20] for
linear mixed-integer programming can be obtained as conic MIR inequalities.
Consider a linear mixed-integer set
ax −y ≤b, x ≥0, y ≥0, x ∈Zn, y ∈R
(17)
and the corresponding valid MIR inequality

j

⌊aj⌋+ (fj −f)+
1 −f

xj −
1
1 −f y ≤⌊b⌋,
(18)
where fj := aj −⌊aj⌋for j = 1, . . . , n and f := b −⌊b⌋.

Cuts for Conic Mixed-Integer Programming
25
Proposition 4. Every MIR inequality is a conic MIR inequality.
Proof. We ﬁrst rewrite inequalities ax −y ≤b and y ≥0, in the conic form
−ax + 2y + b ≥|ax −b|
and then split the terms involving integer variables x on the right hand side into
their integral and fractional parts as
−ax+2y+b ≥

⎛
⎝
fj≤f
⌊aj⌋xj +

fj>f
⌈aj⌉xj
⎞
⎠+

fj≤f
fjxj −

fj>f
(1 −fj)xj −b

.
Then, since z = 
fj≤f⌊aj⌋xj +
fj>f⌈aj⌉xj is integer and y+ = 
fj≤f fjxj ∈
R+ and y−= 
fj>f(1 −fj)xj ∈R+, we write the simple conic MIR inequal-
ity (8)
−ax + 2y + b +

fj≤f
fjxj +

fj>f
(1 −fj)xj
≥(1 −2f)
⎛
⎝
fj≤f
⌊aj⌋xj +

fj>f
⌈aj⌉xj −⌊b⌋
⎞
⎠+ f .
After rearranging this inequality as
2y+2(1−f)⌊b⌋≥

fj≤f
((1−2f)⌊aj⌋−fj+aj)xj+

fj>f
((1−2f)⌈aj⌉−(1−fj)+aj)xj
and dividing it by 2(1 −f) we obtain the MIR inequality (18).
⊓⊔
Example 2. In this example we illustrate that conic mixed-integer rounding cuts
can be used to generate valid inequalities that are diﬃcult to obtain by Chv´atal-
Gomory (CG) integer rounding in the case of pure integer programming. It is
well-known that CG rank of the polytope given by inequalities
−kx1 + x2 ≤1, kx1 + x2 ≤k + 1, x1 ≤1, x1, x2 ≥0
for a positive integer k equals exactly k [20]. Below we show that the non-trivial
facet x2 ≤1 of the convex hull of integer points can be obtained by a single
application of the conic MIR cut.
Writing constraints −kx1 + x2 ≤1 and kx1 + x2 ≤k + 1 in conic form, we
obtain
kx1 −k
2
 ≤k
2 + 1 −x2 .
(19)
Dividing the conic constraint (19) by k and treating 1/2 + 1/k −x2/k as a
continuous variable, we obtain the conic MIR cut
1
2 ≤1
2 + 1
k −x2
k
which is equivalent to x2 ≤1.

26
A. Atamt¨urk and V. Narayanan
Conic Aggregation
We can generate other cuts for the second order conic mixed integer set C
by aggregating constraints (4) in conic form: for λ, μ ∈Rm
+, we have λ′t ≥
λ′(Ax + Gy −b), and μ′t ≥μ′(−Ax −Gy + b). Writing these two inequalities in
conic form, we obtain
λ + μ
2
′
t +
μ −λ
2
′
(Ax + Gy) +
λ −μ
2
′
b
≥

μ −λ
2
′
t +
λ + μ
2
′
(Ax + Gy) −
λ + μ
2
′
b
 .
(20)
Then we can write the corresponding conic MIR inequalities for (20) by treating
the left-hand-side of inequality (20) as a single continuous variable. Constraint
(20) allows us to utilize multiple polyhedral conic constraints (4) simultaneously.
4
Preliminary Computational Results
In this section we report our preliminary computational results with the conic
mixed-integer rounding inequalities. We tested the eﬀectiveness of the cuts on
SOCMIP instances with cones Q2, Q25, and Q50. The coeﬃcients of A, G, and
b were uniformly generated from the interval [0,3]. All experiments were per-
formed on a 3.2 GHz Pentium 4 Linux workstation with 1GB main memory using
CPLEX1 (Version 10.1) second-order conic MIP solver. CPLEX uses a barrier
algorithm to solve SOCPs at the nodes of a branch-and-bound algorithm.
Conic MIR cuts (15) were added only at the root node using a simple sep-
aration heuristic. We performed a simple version of conic aggregation (20) on
pairs of constraints using only 0 −1 valued multipliers λ and μ, and checked for
violation of conic MIR cut (15) for each integer variable xj with fractional value
for the continuous relaxation.
In Table 1 we report the size of the cone (m), number (n) of integer vari-
ables in the formulation, the number of cuts, the integrality gap (the percentage
gap between the optimal solution and the continuous relaxation), the number
of nodes explored in the search tree, and CPU time (in seconds) with and with-
out adding the conic mixed-integer rounding cuts (15). Each row of the table
represents the averages for ﬁve instances. We have used the default settings of
CPLEX except that the primal heuristics were turned oﬀ. CPLEX added a small
number of MIR cuts (18) to the formulations in a few instances.
We see in Table 1 the conic MIR cuts have been very eﬀective in closing the inte-
grality gap. Most of the instances had 0% gap at the root node after adding the cuts
and were solved without branching. The remaining ones were solved within only a
few nodes. These preliminary computational results are quite encouraging on the
positive impact of conic MIR cuts on solving conic mixed-integer programs.
1 CPLEX is a registered trademark of ILOG, Inc.

Cuts for Conic Mixed-Integer Programming
27
Table 1. Eﬀectiveness of conic MIR cuts (15)
without cuts
with cuts
m
n
% gap nodes time
cuts % gap nodes time
100
95.8
19
0
87
0.4
1
0
200
90.8
29
0
192
0.6
1
0
2
300
90.3
38
0
248
0.6
1
0
400
85.2
62
0
322
0.0
0
0
500
86.4
71
0
349
0.7
1
0
100
8.6
10
0
35
2.6
2
0
200
41.2
80
2
101
4.5
12
1
25 300
46.1
112
4
20
0.0
0
2
400
68.3
5951
295
99
17.8
63
12
500
74.6
505
24
116
3.4
6
3
100
24.5
7
1
42
0.0
0
1
200
51.3
67
6
44
0.0
0
1
50 300
52.6
105
13
51
3.2
3
2
400
55.6
158
20
49
5.4
7
5
500
66.9
233
43
62
1.3
2
3
References
[1] F. Alizadeh. Interior point methods in semideﬁnite programming and applications
to combinatorial optimization. SIAM Journal on Optimization, 5:13–51, 1995.
[2] F. Alizadeh and D. Goldfarb. Second-order cone programming.
Mathematical
Programming, 95:3–51, 2003.
[3] A. Atamt¨urk. Sequence independent lifting for mixed–integer programming. Op-
erations Research, 52:487–490, 2004.
[4] E. Balas.
Disjunctive programming.
Annals of Discrete Mathematics, 5:3–51,
1979.
[5] E. Balas, S. Ceria, and G. Cornu´ejols. A lift-and-project cutting plane algorithm
for mixed 0-1 programs. Mathematical Programming, 58:295–324, 1993.
[6] A. Ben-Tal and A. Nemirovski. Lectures on Modern Convex Optimization: Anal-
ysis, Algorithms, and Engineering Applications. SIAM, 2001.
[7] S. J. Benson and Y. Ye. DSDP5: Software for semideﬁnite programming. Techni-
cal Report ANL/MCS-P1289-0905, Mathematics and Computer Science Division,
Argonne National Laboratory, September 2005.
[8] P. Bonami, L. T. Biegler, A. R. Conn, G. Cornu´ejols, Grossmann I. E, C. D. Laird,
J. Lee, A. Lodi, F. Margot, N. Sawaya, and A. W¨achter. An algorithmic framework
for convex mixed integer nonlinear programs. Technical Report RC23771, IBM,
November 2005.
[9] B. Borchers. CDSP, a C library for semideﬁnite programing. Optimization Meth-
ods and Software, 11:613–623, 1999.
[10] S. Boyd and L. Vandenberghe. Convex Optimization. Cambridge University Press,
Cambridge, 2004.

28
A. Atamt¨urk and V. Narayanan
[11] M. T. C¸ezik and G. Iyengar. Cuts for mixed 0-1 conic programming. Mathematical
Programming, 104:179–202, 2005.
[12] M. X. Goemans. Semideﬁnite programming in combinatorial optimization. Math-
ematical Programming, 79:143–161, 1997.
[13] M. X. Goemans and D. P. Williamson. Improved approximation algorithms for
maximum cut and satisfyibility problems using semideﬁnite programming. Jour-
nal of the ACM, 42:1115–1145, 1995.
[14] S. Kim, M. Kojima, and M. Yamashita. Second order cone programming relax-
ation of a positive semideﬁnite constraint. Optimization Methods and Software,
18:535–451, 2003.
[15] M. Kojima and L. Tuncel. Cones of matrices and successive convex relaxations
of nonconvex sets. SIAM Journal on Optimization, 10:750–778, 2000.
[16] J. B. Lasserre. An explicit exact SDP relaxation for nonlinear 0-1 programs. In
K. Aardal and A. M. H. Gerards, editors, Lecture Notes in Computer Science,
volume 2081, pages 293–303. 2001.
[17] J. Linderoth. A simplical branch-and-bound algorithm for solving quadratically
constrained quadratic programs. Mathematical Programming, 103:251–282, 2005.
[18] M. Lobo, L. Vandenberghe, S. Boyd, and H. Lebret. Applications of second-order
cone programming. Linear Algebra and its Applications, 284:193–228, 1998.
[19] L. Lov´asz and A. Schrijver. Cones of matrices and set-functions and 0-1 optimiza-
tion. SIAM Journal on Optimization, 1:166–190, 1991.
[20] G. L. Nemhauser and L. A. Wolsey.
Integer and Combinatorial Optimization.
John Wiley and Sons, New York, 1988.
[21] G. L. Nemhauser and L. A. Wolsey. A recursive procedure for generating all cuts
for 0-1 mixed integer programs. Mathematical Programming, 46:379–390, 1990.
[22] Y. Nesterov and A. Nemirovski. A general approach to polynomial-time algorithm
design for convex programming. Technical report, Center. Econ. & Math. Inst,
USSR Acad. Sci., Moskow, USSR, 1988.
[23] Y. Nesterov and A. Nemirovski. Self-concordant functions and polynomial time
methods in convex programming. Technical report, Center. Econ. & Math. Inst,
USSR Acad. Sci., Moskow, USSR, 1990.
[24] Y. Nesterov and A. Nemirovski.
Conic formulation of a convex programming
problem and duality. Technical report, Center. Econ. & Math. Inst, USSR Acad.
Sci., Moskow, USSR, 1991.
[25] Y. Nesterov and A. Nemirovski. Interior-point polynomial algorithms for convex
programming. SIAM, Philedelphia, 1993.
[26] H. D. Sherali and C. Shetti. Optimization with disjunctive constraints, volume
181 of Lectures on Econ. Math. Systems.
Springer Verlag, Berlin, Heidelberg,
New York, 1980.
[27] H. D. Sherali and C. H. Tun¸cbilek. A hierarchy of relaxations between continu-
ous and convex hull representations for zero-one programming problems. SIAM
Journal on Discrete Mathematics, 3:411–430, 1990.
[28] H. D. Sherali and C. H. Tun¸cbilek. A reformulation-convexiﬁcation approach for
solving nonconvex quadratic programming problems. Journal of Global Optimiza-
tion, 7:1–31, 1995.
[29] R. Stubbs and S. Mehrotra.
A branch-and-cut methods for 0-1 mixed convex
programming. Mathematical Programming, 86:515–532, 1999.
[30] J. Sturm. Using SeDuMi 1.02, a MATLAB toolbox for ptimization over symmetric
cones. Optimization Methods and Software, 11:625–653, 1999.

Cuts for Conic Mixed-Integer Programming
29
[31] M. Tawarmalani and N. V. Sahinidis. Global optimization of mixed-integer non-
linear programs: A theoretical and computational study. Mathematical Program-
ming, 99:563–591, 2004.
[32] K.C. Toh, M. J. Todd, and R. H. T¨ut¨unc¨u. SDPT3 – a Matlab software package
for semideﬁnite programming. Optimization Methods and Software, 11/12:545–
581, 1999.
[33] M. Yamashita, K. Fujisawa, and M. Kojima. Implementation and evaluation of
SDPA 6.0 (SemiDeﬁnite Programming Algorithm 6.0). Optimization Methods and
Software, 18:491–505, 2003.

Sequential-Merge Facets for Two-Dimensional
Group Problems⋆
Santanu S. Dey and Jean-Philippe P. Richard
School of Industrial Engineering, Purdue University,
315 N. Grant Street, West Lafayette, IN 47906-2023
Abstract. In this paper, we show how to generate strong cuts for unstruc-
tured mixed integer programs through the study of high-dimensional group
problems. We present a new operation that generates facet-deﬁning inequ-
alitiesfortwo-dimensionalgroup problemsby combiningtwofacet-deﬁning
inequalities of one-dimensional group problems. Because the procedure al-
lows the use of a large variety of one-dimensional constituent inequalities, it
yields large families of new cutting planes for MIPs that we call sequential-
merge inequalities. We show that sequential-merge inequalities can be used
to generate inequalities whose continuous variable coeﬃcients are stronger
than those of one-dimensional cuts and can be used to derive the three-
gradient facet-deﬁning inequality introduced by Dey and Richard [4].
1
Introduction
Over the last decade, a vast amount of research has been directed towards gener-
ating strong general purpose cutting planes for unstructured integer programs;
see Marchand et al. [13] and Johnson et al. [12]. One approach to generate
strong cutting planes is to use constraints of the problems one at a time. This
approach has proven to be successful in many cases and cuts generated from
single constraint relaxations of MIPs are currently used in all commercial MIP
solvers. It seems however that an option to generate stronger cutting planes is to
use information from multiple constraints concurrently. In this paper, we show
how to generate such strong cuts through the study of two-dimensional group
relaxations.
In a series of papers Gomory [6], Gomory and Johnson [7, 8, 9], Gomory et
al. [10], and Johnson [11] showed how to use group relaxations to generate cutting
planes for general Integer Programs. Although their theory applies to problems
with multiple constraints, most research has considered only one-dimensional
group relaxations; see Gomory and Johnson [7, 8, 9], Gomory et al. [10], Ar´aoz
et al. [2], Miller et al. [14], Richard et al. [15], and Dash and G¨unl¨uk [3]. There
are only a few papers that focus on the systematic study of group problems
with multiple constraints. In [11], Johnson presents general theoretical results
for group relaxations of Mixed Integer Programs with multiple constraints. Re-
cently, Dey and Richard [4] introduced tools to study two-dimensional inﬁnite
⋆This research was supported by NSF Grant DMI-03-48611.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 30–42, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

Sequential-Merge Facets for Two-Dimensional Group Problems
31
group problems and introduced two families of facet-deﬁning inequalities for
two-dimensional group relaxations. We note however that very few families of
facet-deﬁning inequalities are known for two-dimensional group problems. Fur-
ther, Gomory and Johnson [9] recently write about strong inequalities of two-
dimensional group problems that
“There are reasons to think that such inequalities would be stronger
since they deal with the properties of two rows, not one. They can also
much more accurately reﬂect the structure of the continuous variables.”
Similarly, in a recent review of non-traditional approaches to Mixed Integer
Programming, Aardal, Weismantel and Wolsey [1] mention that:
“Given the recent computational interest in using Gomorys fractional
cuts, mixed integer rounding inequalities and Gomorys mixed integer
cuts, this reopens questions about the possible use of alternative subad-
ditive functions to generate practically eﬀective cutting planes. It is also
natural to ask whether interesting higher dimensional functions can be
found and put to use...”
In this paper, we present a general procedure for generating large fami-
lies of facet-deﬁning inequalities for two-dimensional inﬁnite group problems.
This procedure in turn yields a large number of new cutting planes for general
MIPs. Although Dey and Richard [4] already showed that a speciﬁc aggrega-
tion scheme yields facet-deﬁning inequalities for two-dimensional group prob-
lems from facet-deﬁning inequalities of the one-dimensional group problem, the
procedure presented in this paper shows diﬀerent, richer relations between facets
of one-dimensional and two-dimensional group problems.
In Sect. 2 we introduce and present fundamental results and concepts about
the group problem. We also describe its relation to lifting. In Sect. 3, we present
a sequential-merge procedure that generates inequalities for the two-dimensional
group problem by combining inequalities for the one-dimensional group problem
in a speciﬁc fashion. We also show that the procedure shares some relationship
with the two-step MIR procedure of Dash and G¨unl¨uk [3] and can be used to
derive the family of three-gradient facet of Dey and Richard [4]. In Sect. 4 we
show that, under mild conditions, the procedure presented in Sect. 3 generates
facets for the two-dimensional inﬁnite group problem. We conclude in Sect. 5
with directions of future research.
2
Group Problem and Lifting-Space
In this section, we present important results about group problems that were
introduced and proven by Gomory and Johnson [9]. We then introduce the notion
of valid and facet-deﬁning inequalities for group problems and discuss how such
inequalities can be derived from certain lifting functions. We denote by Im the
group of real m-dimensional vectors where the group operation is performed as
addition modulo 1 componentwise, i.e., Im = {(x1, x2...xm) | 0 ≤xi < 1 ∀1 ≤

32
S.S. Dey and J.-P.P. Richard
i ≤m}. In particular, the symbol + is used to denote both the addition in Rm
and in Im. We refer to the vector (0, 0, ..., 0) ∈Im as o. Next we give a formal
deﬁnition of the group problem.
Deﬁnition 1 ([11]). For r ∈Im with r ̸= o, the group problem PI(r, m) is the
set of functions t : Im →R such that
1. t has a ﬁnite support, i.e., t(u) > 0 for a ﬁnite subset of Im.
2. t(u) is a non-negative integer for all u ∈Im,
3. 
u∈Im ut(u) = r.
Next we deﬁne the concept of a valid inequality for the group problem.
Deﬁnition 2 ([11]). A function φ : Im →R+ is said to deﬁne a valid inequality
for PI(r,m) if φ(o) = 0, φ(r) = 1 and 
u∈Im φ(u)t(u) ≥1, ∀t ∈PI(r, m).
In the remainder of this paper, we will use the terms valid function and valid
inequality interchangeably. For a vector a ∈Rm, deﬁne P(a) = (a1(mod1),
...am(mod1)).
It can be veriﬁed that given m rows of the simplex tableau n
i=1 aixi = b of
an integer program P, the inequality n
i=1 φ(P(ai))xi ≥1 is valid for P, if φ is
valid for PI(r, m), and P(b) = r; see Gomory and Johnson [9]. We next describe
necessary conditions for valid inequalities φ to be strong.
Deﬁnition 3 ([7]). A valid inequality φ for PI(r, m) is said to be subadditive
if φ(u) + φ(v) ≥φ(u + v), ∀u, v ∈Im.
Gomory and Johnson [7] prove that all valid functions of PI(r, m) that are not
subadditive are dominated by valid subadditive functions of PI(r, m). Therefore
it is suﬃcient to study the valid subadditive functions of PI(r, m). Next we
introduce a deﬁnition to characterize strong inequalities.
Deﬁnition 4 ([11]). A valid inequality φ is minimal for PI(r, m) if there does
not exist a valid function φ∗for PI(r, m) diﬀerent from φ such that φ∗(u) ≤φ(u)
∀u ∈Im.
We next present a result characterizing minimal functions. This result is proven
in Gomory and Johnson [7] and Johnson [11].
Theorem 1 ([7]). If φ is a valid function for PI(r, m) and φ(u)+φ(r −u) = 1
∀u ∈Im then φ is minimal.
□
Minimal inequalities for PI(r, m) are strong because they are not dominated by
any single valid inequality. However, there is a stronger class of valid inequalities
that Gomory and Johnson refer to as facet-deﬁning inequalities. Next, we present
the deﬁnition of facet-deﬁning inequality in the context of PI(r, 2).
Deﬁnition 5 (Facet). Let P(φ) = {t ∈PI(r, 2)| 
u∈I2,t(u)>0 φ(u)t(u) = 1}.
We say that an inequality φ is facet-deﬁning for PI(r, 2) if there does not exist
a valid function φ∗such that P(φ∗) ⊋P(φ).

Sequential-Merge Facets for Two-Dimensional Group Problems
33
Gomory and Johnson [8] proved that all facet-deﬁning inequalities are minimal
inequalities. To prove that a function is facet-deﬁning, Gomory and Johnson [9]
introduced a tool that they refer to as Facet Theorem. We describe the Facet
Theorem in Theorem 2 and introduce the necessary deﬁnitions next.
Deﬁnition 6 (Equality Set, [9]). For each point u ∈I2, we deﬁne g(u) to
be the variable corresponding to the point u. We deﬁne the set of equalities of φ
to be the system of equations g(u) + g(v) = g(u + v) for all u, v ∈I2 such that
φ(u) + φ(v) = φ(u + v). We denote this set as E(φ).
Theorem 2 (Facet Theorem, [9]). If φ is minimal and subadditive, and if φ
is the unique solution of E(φ) then φ is facet-deﬁning.
□
Currently all known facets for inﬁnite group problems are piecewise linear func-
tions. A function φ is deﬁned to be piecewise linear, if I2 can be divided into
polytopes such that the function φ is linear over each polytope; see Gomory and
Johnson [9] and Dey and Richard [4]. Further, Gomory and Johnson [9] conjec-
tured that all facets of inﬁnite group problems are piecewise linear. Therefore,
when introducing tools to prove that inequalities are facet-deﬁning, it is usual to
assume that the inequality under study is piecewise linear. Next we present in
Theorem 4 a result regarding the continuity of functions of PI(r, 2) that is used
in the proof of the Sequential-Merge Theorem of Sect. 4. Theorem 4 is proven
using the following preliminary result.
Theorem 3 ([5]). If a valid function φ for PI(r, m) satisﬁes the following con-
ditions
1. φ(x) + φ(y) ≥φ(x + y) ∀x, y ∈Im,
2. limh↓0
φ(hd)
h
exists for any d ∈Rm,
then φ is continuous.
□
Theorem 4. Let φ be a minimal piecewise linear and continuous function for
PI(r, 2). If ψ is a valid function for PI(r, 2) such that E(φ) ⊆E(ψ) then ψ is
continuous.
□
Generating strong inequalities for group problems is often diﬃcult. Richard et
al. [15] showed that lifting can be used to derive valid and facet-deﬁning inequali-
ties for one-dimensional group problems. The family of facet-deﬁning inequalities
we present here is also easier to derive using lifting functions. In the remainder of
this section, given any x ∈Im, we denote ˜x as the element of Rm with the same
numerical value as x. Similarly, for x ∈Rm such that 0 ≤xi < 1 ∀1 ≤i ≤m,
we denote ˙x to be the element of Im such that ˜˙x = x.
Deﬁnition 7 (Lifting-Space Representation). Given a valid inequality φ
for PI(r, m), we deﬁne the lifting-space representation of φ as [φ]r : Rm →R
where
[φ]r(x) =
m

i=1
xi −
m

i=1
˜riφ(P(x)).

34
S.S. Dey and J.-P.P. Richard
To illustrate the idea that motivates this deﬁnition, we discuss the case where
m = 1. Consider a row of the simplex tableau n
i=1 aixi = a0 of an integer pro-
gram, where ai ∈R, the fractional part of a0 is r ̸= 0, and xi are nonnegative in-
teger variables. If φ is a valid function for PI(r, 1) we have that n
i=1 φ(ai)xi ≥1
is a valid cut for the original IP. Multiplying this cut with r and then subtracting
it from the original row we obtain n
i=1[φ]r(ai)xi ≤[φ]r(a0). One well-known
example of the relation between the group-space and the lifting-space represen-
tation of an inequality is that of Gomory Mixed Integer Cut (GMIC) and the
Mixed Integer Rounding (MIR) inequality. It can be easily veriﬁed that the form
in which MIR is presented is [GMIC]r. Thus, intuitively, the construction of the
lifting-space representation given in Deﬁnition 7 is a generalization of the relation
that GMIC shares with MIR to other group cuts of one- and higher-dimensions.
Propositions 1 and 2 are generalizations of results from Richard et al. [15].
Proposition 1. If φ is valid function for PI(r, m),
1. [φ]r(x + ei) = [φ]r(x) + 1, where ei is the ith unit vector of Rm. We say that
[φ]r is pseudo-symmetric.
2. [φ]r is superadditive iﬀφ is subadditive.
□
Motivated by Deﬁnition 7, we deﬁne next the inverse operation to [φ]r(x).
Deﬁnition 8 (Group-Space Representation). Given a superadditive func-
tion ψ : Rm →R which is pseudo-symmetric, we deﬁne the group-space repre-
sentation of ψ as [ψ]−1
r
: Im →R where [ψ]−1
r ( ˙x) =
 m
i=1 xi−ψ(x)
 m
i=1 ri
.
In Fig. 1, a three-gradient facet [4] of the two-dimensional group problem is
shown in its group-space and lifting-space representation.
Proposition 2. A valid group-space function g : Im →R is minimal iﬀ[g]r is
superadditive and [g]r(x) + [g]r(r −x) = 0.
□
3
Sequential-Merge Inequalities for Two-Dimensional
Group Problems
In this section, we introduce an operation that produces valid inequalities for
PI(r, 2) from valid inequalities for PI(r′, 1). To simplify the notation, we denote
˜x and ˙x by x since the symbol is clear from the context.
Deﬁnition 9 (Sequential-merge inequality). Assume that g and h are valid
functions for PI(r1, 1) and PI(r2, 1) respectively. We deﬁne the sequential-merge
of g and h as the function g♦h : I2 →R+ where
g♦h(x1, x2) = [[g]r1(x1 + [h]r2(x2))]−1
r (x1, x2)
(1)
and r = (r1, r2). In this construction, we refer to g as the outer function and to
h as the inner function.
Figure 2 gives an example of a sequential-merge inequality that is facet-deﬁning
of PI((r1, r2), 2).

Sequential-Merge Facets for Two-Dimensional Group Problems
35
Fig. 1. Group-space and lifting-space representations of a three-gradient facet of PI(r,2)
Fig. 2. Examples of sequential-merge operation
We ﬁrst observe that there is an intuitive interpretation to the construction
presented in Deﬁnition 9. Given two rows of a simplex tableau, we ﬁrst gener-
ate a cutting plane in the lifting-space of the ﬁrst row. This cutting plane is
added to the second row of the tableau to generate a combined inequality. Fi-
nally, a one-dimensional cutting plane is generated from the combined inequality.
Proposition 4 states that the group-space representation of inequalities gener-
ated using this procedure are valid inequalities for PI(r, 2) under the condition
that the outer function is nondecreasing in the lifting-space.
Before we present this result, we give a formula for the sequential-merge in-
equality in terms of the inner and outer functions in their group-space represen-
tations.
Proposition 3. g♦h(x1, x2) = r2h(x2)+r1g(P(x1+x2−r2h(x2)))
r1+r2
.
□

36
S.S. Dey and J.-P.P. Richard
Using Proposition 3 it is easy to verify that the sequential-merge ♦operator
is non-commutative. In the next proposition, we record that sequential-merge
inequalities are valid for the two-dimensional group problem.
Proposition 4. If g, h are valid functions for PI(r1, 1) and PI(r2, 1) respec-
tively, and [g]r1 is nondecreasing then g♦h is a valid function for PI(r, 2) where
r ≡(r1, r2).
□
In Fig. 3 we illustrate all the diﬀerent types of valid inequalities that can be
obtained using GMIC, a two-step MIR and a three-slope facet of the one-
dimensional group problem as inner and outer functions in the sequential-merge
construction. These inequalities are valid for PI(r, 2) since all the three build-
ing functions used have non-decreasing lifting-space representations. It can be
proven that the inequalities obtained in this way are strong.
Proposition 5. If g and h are minimal and [g]r1 is nondecreasing, then g♦h is
minimal.
□
We next give two examples of well-known valid inequalities for group problems
that can be obtained using the sequential-merge procedure.
Fig. 3. Examples of sequential-merge inequalities for PI(r, 2)

Sequential-Merge Facets for Two-Dimensional Group Problems
37
Proposition 6. Consider κ(x) = [[ξ]r(x + [ξ]r(x))]−1
(r,r)(x, x), where ξ is the
GMIC, i.e., κ(x) is the sequential-merge inequality obtained using the same con-
straint twice and using GMIC as both the inner and outer function. Then κ(x)
is a two-step MIR function from Dash and G¨unl¨uk [3].
□
We observe that sequential-merge procedure shares some relations with the two-
step MIR procedure of Dash and G¨unl¨uk [3]. An important diﬀerence however
is that the sequential-merge procedure uses in general two diﬀerent rows of a
simplex tableau. Also the two-step MIR procedure only uses MIR inequalities
as constituent functions.
We describe in the next proposition another family of facets for the two-
dimensional group problem that can be obtained using the sequential-merge
procedure.
Proposition 7. Consider ρ(x, y) = [[ξ]r(x + [ξ]r(y))]−1
(r1,r2)(x, y), where ξ is the
GMIC, i.e. ρ(x, y) is the sequential-merge inequality obtained using GMIC as
both the inner and outer function. This inequality is the three-gradient facet-
deﬁning inequality for P((r1, r2), 2) presented in Dey and Richard [4].
□
4
Facet-Deﬁning Sequential-Merge Inequalities
In this section, we derive conditions under which sequential-merge inequalities
are facet-deﬁning for the two-dimensional group problem PI(r, 2). We begin by
studying some geometric properties of g♦h.
Deﬁnition 10. We deﬁne the set of points {(x, y) | x = (−y + r2h(y))(mod1)}
as the support of the function g♦h. We denote the support of g♦h as S(g♦h).
It is easy to verify that given a value of y, there is an unique value of x such
that (x, y) ∈S(g♦h).
In Fig. 4 we illustrate the support of a function g♦h for the case where the
inner function is the three-slope facet deﬁning inequality of Gomory and John-
son [9] with right-hand-side of 0.2. The support of g♦h is important because
Fig. 4. Example of S(g♦h) where h is the three-slope facet of Gomory and Johnson [9]

38
S.S. Dey and J.-P.P. Richard
it contains all the equalities that h satisﬁes. In particular, the next proposition
states that for every equality that h satisﬁes, there exists a related equality that
g♦h satisﬁes, which only involves points of its support.
Proposition 8. Let g and h be valid subadditive inequalities and let [g]r1 be
nondecreasing. If v1, v2 ∈I1 are such that h(v1) + h(v2) = h(v1 + v2) and
(u1, v1), (u2, v2) ∈S(g♦h) then
1. (u1 + u2, v1 + v2) ∈S(g♦h).
2. g♦h(u1, v1) + g♦h(u2, v2) = g♦h(u1 + u2, v1 + v2)
□
Deﬁnition 11. Let φ be a valid continuous function for PI(r, 1). We say E(φ)
is unique up to scaling if for any other continuous function φ′ : I1 →R+,
E(φ′) ⊇E(φ) implies that φ′ = cφ for c ∈R.
Intuitively, because the function g♦h has the equalities of h on its support,
E(g♦h) will have an unique solution on its support up to scaling whenever E(h)
has a unique solution up to to scaling. This key result is used in the proof of the
Sequential-Merge Theorem 5 to show that E(g♦h) is unique and therefore show
that g♦h is facet-deﬁning.
Proposition 9. Let g, h be piecewise linear and continuous valid inequalities for
PI(r1, 1) and PI(r2, 1) respectively and assume that E(h) has an unique solution
up to scaling. Let ψ be a valid function for PI(r, 2) such that E(ψ) ⊇E(g♦h),
then the value of ψ(u1, u2) = cg♦h(u1, u2) =
cr2
r1+r2 h(u2) ∀(u1, u2) ∈S(g♦h)
where c is a nonnegative real number.
□
Although Proposition 9 establishes that E(g♦h) has an unique solution up to
scaling on its support, it falls short of proving that E(g♦h) has an unique solution
over I2. Therefore, we identify in Propositions 10 and 11 some equalities that
g♦h satisﬁes that help in extending the result of Proposition 9 to I2.
Proposition 10. Let g and h be valid functions for PI(r1, 1) and PI(r2, 1)
respectively such that [g]r1 is nondecreasing, then g♦h(x1, y1) + g♦h(δ, 0) =
g♦h(x1 + δ, y1) ∀δ ∈I1 and ∀(x1, y1) ∈S(g♦h).
□
Proposition 11. Let g and h be valid functions for PI(r1, 1) and PI(r2, 1) re-
spectively and assume that [g]r1 and [h]r2 are nondecreasing functions. Then
g♦h(x1, x2) =
x1+x2
r1+r2
for 0 ≤x1 ≤r1, and 0 ≤x2 ≤r2. Furthermore
g♦h(u1, v1) + g♦h(u2, v2) = g♦h(u1 + u2, v1 + v2) whenever u1, u2, u1 + u2 ≤r1
and v1, v2, v1 + v2 ≤r2.
□
Theorem 5 (Sequential-Merge Theorem). Assume that g and h are con-
tinuous, piecewise linear, facet-deﬁning inequalities of PI(r1, 1) and PI(r2, 1)
respectively. Assume also that E(g) and E(h) are unique up to scaling and
[g]r1 and [h]r2 are nondecreasing. Then g♦h is a facet-deﬁning inequality for
PI((r1, r2), 2).
□

Sequential-Merge Facets for Two-Dimensional Group Problems
39
We brieﬂy present the outline of the proof of the above theorem. We ﬁrst as-
sume by contradiction that g♦h is not facet-deﬁning. Then using Theorem 2 we
conclude that there exists a function φ′ that is diﬀerent from g♦h and satisﬁes
all the equalities of g♦h. Using Theorem 4 we can prove that φ′ is continuous.
Using Proposition 9 we show that the function φ′ is a scalar multiple times g♦h
over S(g♦h). Finally, we use Proposition 10 and Proposition 11 and the fact that
E(g) is unique up to scaling to show that the value of this scalar is 1 and that
φ′(u) = g♦h(u) ∀u ∈I2, which is the required contradiction.
In Theorem 5, we assumed the technical condition that E(h) and E(g) are
unique up to scaling. This assumption is not very restrictive as it is satisﬁed
by all known facet-deﬁning inequalities for PI(r, 1). The condition that [g]r1
and [h]r2 are nondecreasing on the other hand is more restrictive since there
exists facet-deﬁning inequalities of PI(r, 1) that do not satisfy this condition.
Finally note that, Theorem 5 implies that all the functions illustrated in Fig. 3
are facet-deﬁning for the two-dimensional group problem.
We now extend the family of inequalities obtained in Theorem 5 to the mixed
integer case. To this end we use a result from Johnson [11] which states that the
coeﬃcient of a continuous variable in a minimal group cut φ can be found as
μφ(u) = limh→0+ φ(P(hu))
h
where u ∈R2 is the column vector of coeﬃcients of
this continuous variable in the simplex tableau.
The following proposition describes how the sequential-merge facets obtained
for PI(r, 2) can be extended into two-dimensional mixed integer group cuts.
Proposition 12. Let c+
g
= limϵ→0+ g(ϵ)
ϵ
=
1
r1 , c−
g
= limϵ→0+ g(1−ϵ)
ϵ
, c+
h =
limϵ→0+ h(ϵ)
ϵ
=
1
r2 and c−
h = limϵ→0+ h(1−ϵ)
ϵ
. The coeﬃcients of the continuous
variables for g♦h are given by
μg♦h(u1, u2) =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
u1+u2
r1+r2
u1 ≥0, u2 ≥0
u2−r1c−
g (u1)
r1+r2
u1 < 0, u2 ≥0
−r2c−
h u2+r1c−
g (−u1−u2−r2c−
h u2)
r1+r2
u1 ≤0, u2 ≤0
u1+u2
r1+r2
u1 > 0, u2 < 0
u1 + u2 + r2c−
h u2 > 0
−r2c−
h u2+r1c−
g (−u1−u2−r2c−
h u2)
r1+r2
u1 > 0, u2 < 0
u1 + u2 + r2c−
h u2 ≤0
(2)
□
Next we illustrate on an example how the sequential-merge procedure can be
applied to mixed integer programs.
Example 1. Consider the following mixed integer set
7
17x −7
17y ≤154
85
(3)

40
S.S. Dey and J.-P.P. Richard
7
17x + 10
17y ≤359
170
(4)
x, y ∈Z+.
(5)
We introduce non-negative slack variables s1 and s2 and perform a few simplex
iterations to obtain
x + 1.4286s1 + s2 = 4.7
y −s1 + s2 = 0.3.
Using Proposition 12 and using GMIC as both the inner and outer functions
we obtain the sequential-merge cut 0.4286s1 + 2s2 ≥1 which is equivalent to
x + y ≤4. It can easily be veriﬁed that this inequality is facet-deﬁning for the
convex hull of solutions to (3), (4) and (5).
Moreover the two GMICs generated from the individual rows are 2.048s1 +
1.4286s2 ≥1 and 1.4286s1 + 3.3333s2 ≥1 which are equivalent to x ≤4 and
10x + 7y ≤44. It can be veriﬁed that these inequalities are not facet-deﬁning
for the convex hull of solutions to (3), (4) and (5).
It can be seen from Proposition 12 that the sequential-merge inequalities yield
very diverse coeﬃcients for continuous variables. To understand the strength
of the continuous coeﬃcients in sequential-merge inequalities we consider the
following general example.
Example 2. Consider a continuous variable with u1 > 0, u2 < 0, u1 + u2 +
r2c−
h u2 = 0. The coeﬃcient of this continuous variable in g♦h is
r2
r1+r2 (−u2c−
h ).
If the group cut h was used to generate a cut from the second constraint alone, the
coeﬃcient of the continuous variable would have been −u2c−
h >
r2
r1+r2 (−u2c−
h ).
Similarly, if the group cut g was derived using the ﬁrst constraint alone, the coeﬃ-
cient of the continuous variable would have been 1
r1 u1. Since u1+u2+r2c−
h u2 = 0
the coeﬃcient of the continuous variable using g♦h, is
r2
r1+r2 (−u2c−
h ) = u1+u2
r1+r2 <
1
r1 u1 as u2 < 0. Therefore in this case the continuous coeﬃcients generated us-
ing the two diﬀerent cuts g and h individually will be strictly weaker than those
generated using g♦h.
We conclude from Example 2 that if both the inner and outer functions used in
the sequential-merge procedure are GMICs then the coeﬃcient generated for the
continuous variable is stronger than the coeﬃcient generated using each of the
individual group cuts when the column corresponding to the continuous vari-
able is (u1, u2) with u1 > 0, u2 < 0, u1 + u2 + r2c−
h u2 = 0 (i.e., the coeﬃcients
of the sequential-merge inequalities are not dominated by the GMIC). This re-
sult is signiﬁcant because it can be proven that GMIC generates the strongest
possible coeﬃcients for continuous variables among all facets of one-dimensional
group problems. We note that this result was numerically observed for the three-
gradient facet in Dey and Richard [4].
Note also that although the above discussion was based on the speciﬁc case
where u1 > 0, u2 < 0 and u1 + u2 + r2c−
h u2 = 0, there exists a large range of

Sequential-Merge Facets for Two-Dimensional Group Problems
41
continuous variables coeﬃcient for which the sequential-merge procedure yields
inequalities whose coeﬃcients are not dominated by the continuous coeﬃcient
of the one-dimensional group cuts derived from individual rows.
5
Conclusion
In this paper we presented a general procedure that produces a wide array of
facet-deﬁning inequalities for two-dimensional group problems. We showed that,
under very general conditions, these inequalities are facet-deﬁning. These cuts
illustrate that strong coeﬃcients for continuous variables can be found by con-
sidering group relaxations with multiple constraints. In particular, it is possible
to obtain inequalities that are not dominated by group cuts generated from in-
dividual constraints. Sequential-merge inequalities are also interesting because
they show strong relations between facet-deﬁning inequalities of one-dimensional
and two-dimensional group problems.
A few important theoretical and practical questions arise from this paper.
First we observe that all the known facet-deﬁning inequalities for the two-
dimensional group problem obtained to date are derived either using aggrega-
tion [4] or using the sequential-merge procedure. This is an interesting character-
ization of a subset of facets of the two-dimensional group problem. However this
implies that all known facet-deﬁning inequalities of the two-dimensional prob-
lem are tightly related to facet-deﬁning inequalities of the one-dimensional group
problem. An interesting open question is that of ﬁnding a family of group cuts
for the two-dimensional inﬁnite group problem that cannot be easily obtained
using one-dimensional group cuts.
Second because the sequential merge approach can be applied using the same
constraint twice instead of using two diﬀerent constraints, one interesting ques-
tion is that of determining when the sequential-merge procedure generates strong
inequalities for one-dimensional group problems. The question is particularly in-
teresting since we have shown in Sect. 3 that some two-step MIRs can be derived
in this way.
Finally, a large numerical experimentation is needed to determine how and
when to use multi-dimensional group cuts to solve MIPs. In particular, numer-
ical determination of how much two-dimensional group cuts improve on one-
dimensional group cuts is an important direction of research.
References
[1] K. Aardal, R. Weismantel, and L. A. Wolsey. Non-standard approaches to integer
programming. Discrete Applied Mathematics, 123:5–74, 2002.
[2] J. Ar´aoz, L. Evans, R. E. Gomory, and E. L. Johnson. Cyclic groups and knapsack
facets. Mathematical Programming, 96:377–408, 2003.
[3] S. Dash and O. G¨unl¨uk. Valid inequalities based on simple mixed integer set.
Mathematical Programming, 106:29–53, 2006.

42
S.S. Dey and J.-P.P. Richard
[4] S. S. Dey and J.-P. P. Richard. Facets of two-dimensional inﬁnite group problems.
Under Review, 2006.
http://www.optimization-online.org/DB HTML/2006/01/1280.html.
[5] S. S. Dey, J.-P. P. Richard, L. A. Miller, and Y. Li.
Extreme inequalities for
inﬁnite group problems. 2006.
http://www.optimization-online.org/DB HTML/2006/04/1356.html.
[6] R. E. Gomory. Some polyhedra related to combinatorial problems. Journal of
Linear Algebra and Applications, 2:341–375, 1969.
[7] R. E. Gomory and E. L. Johnson. Some continuous functions related to corner
polyhedra, part I. Mathematical Programming, 3:23–85, 1972.
[8] R. E. Gomory and E. L. Johnson. Some continuous functions related to corner
polyhedra, part II. Mathematical Programming, 3:359–389, 1972.
[9] R. E. Gomory and E. L. Johnson. T-space and cutting planes.
Mathematical
Programming, 96:341–375, 2003.
[10] R. E. Gomory, E. L. Johnson, and L. Evans. Corner polyhedra and their connec-
tion with cutting planes. Mathematical Programming, 96:321–339, 2003.
[11] E. L. Johnson. On the group problem for mixed integer programming. Mathe-
matical Programming Study, 2:137–179, 1974.
[12] E. L. Johnson, G. L. Nemhauser, and M. W. P. Savelsbergh.
Progress in lin-
ear programming-based algorithms for integer programming: an exposition. IN-
FORMS Journal of Computing, 12:2–23, 2000.
[13] H. Marchand, A. Martin, R. Weismantel, and L. A. Wolsey. Cutting planes in
integer and mixed integer programming. Discrete Applied Mathematics, 123:397–
446, 2002.
[14] L. Miller, Y. Li, and J.-P. P. Richard. New facets for ﬁnite and inﬁnite group
problems from approximate lifting. Technical Report MN-ISYE-TR-06-004, Uni-
versity of Minnesota Graduate Program in Industrial and Systems Engineering,
2006.
[15] J.-P. P. Richard, Y. Li, and L. A. Miller. Strong valid inequalities for mips and
group polyhedra from approximate lifting. Under Review, 2006.

Triangle-Free Simple 2-Matchings in Subcubic
Graphs (Extended Abstract)
David Hartvigsen1 and Yanjun Li2
1 Mendoza College of Business, University of Notre Dame
Notre Dame, IN 46556, USA
dhartvig@nd.edu
2 Krannert School of Management, Purdue University
West Lafayette, IN 47907, USA
li14@purdue.edu
Abstract. A simple 2-matching in an edge-weighted graph is a subgraph
all of whose vertices have degree 1 or 2. We consider the problem of ﬁnding
a maximum weight simple 2-matching that contains no triangles, which is
closely related to a class of relaxations of the TSP. Our main results are,
for graphs with maximum degree 3, a complete description of the convex
hull of incidence vectors of triangle-free simple 2-matchings and a strongly
polynomial time algorithm for the above problem. Our system requires the
use of a type of comb inequality (introduced by Gr¨otschel and Padberg for
the TSP polytope) that has {0,1,2}-coeﬃcients and hence is more general
than the well-known blossom inequality used in Edmonds’ characteriza-
tion of the simple 2-matching polytope.
1
Introduction
We consider undirected graphs G = (V, E) with no parallel edges or loops. With
every edge e ∈E we associate a real weight we. A simple 2-matching in a
graph G = (V, E) is a subgraph of G all of whose vertices have degree 1 or
2. Hence the connected components of a simple 2-matching are simple paths
or cycles, each with at least one edge. (For the sake of brevity, we henceforth
drop the adjective “simple.”) A 2-factor in G is a special type of 2-matching
that contains all the vertices of G and all of whose components are cycles. The
problems of ﬁnding a maximum weight 2-matching and a maximum weight 2-
factor in a graph are well studied. Polynomial time algorithms (see Johnson [19])
and polyhedral characterizations (see Edmonds [10]) are known for both, as well
as many other results (see Schrijver [25] for a thorough survey). The key type of
inequalities used for the polyhedron is typically called blossom inequalities (they
are diﬀerent from the blossoms used by Edmonds [9] for the classical matching
problem).
In this paper we focus on a variation of these problems, which we next deﬁne.
A 2-matching or 2-factor is called Ck-free if it contains no cycles of length ≤k,
for k a positive integer. The Ck-free 2-matching (2-factor) problem is to ﬁnd a
Ck-free 2-matching (2-factor) with maximum weight.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 43–52, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

44
D. Hartvigsen and Y. Li
These problems were originally studied in the context of the travelling sales-
man problem by Fisher, Nemhauser, and Wolsey [11] and by Cornu´ejols and
Pulleyblank [7]. It is shown in [11] how solutions to the Ck-free 2-factor prob-
lem in a complete graph, for increasing values of k, yield increasingly accurate
appoximations of the optimal value of a travelling salesman tour. Observe that
for n/2 ≤k ≤n −1 (where n is the number of vertices in the graph), solving
the Ck-free 2-factor problem is equivalent to the TSP.
Other results are known for these problems, which lead to some open ques-
tions. For k ≥5, the Ck-free 2-factor problem with weights all 0, 1 was shown to
be NP-hard by Papadimitriou (the proof appears in [7]). This implies that, for
k ≥5, the Ck-free 2-matching problem with general weights is NP-hard. (See
also Hell et al [16], where similar complexity results are presented.) Vornberger
[29] showed that the C4-free 2-factor problem (with general weights) is NP-hard.
For the case that the edge weights are all 0, 1, an algorithm solving the C3-free
2-matching problem (hence the C3-free 2-factor problem) appears in [14]. (It is
quite complex.) So two obvious open problems (discussed in [29] and [7]) are to
ﬁnd the complexity of the C3-free 2-factor (2-matching) problem, with general
weights, and the C4-free 2-factor (2-matching) problem, with weights all 0, 1.
Another obvious open problem is to describe the polytope associated with the
C3-free 2-factor (2-matching) problem (assuming it is polynomial).
One way of approaching the open problems mentioned above has been to con-
sider them on special classes of graphs. For example, Hartvigsen [15] presented,
for bipartite graphs, a polynomial time algorithm for the C4-free 2-matching
problem with weights all 0, 1 (hence for the corresponding 2-factor problem as
well). Nam [22] presented a polynomial time algorithm for the C4-free 2-factor
problem for general graphs with the property that no two squares share a vertex.
(The algorithm is quite complex.) In this paper we consider such problems on
the cubic (subcubic) graphs; that is, those graphs for which every vertex has
degree 3 (at most 3). Some work in this regard has been done. For cubic graphs,
Vornberger [29] presented a polynomial time algorithm for the C3-free 2-factor
problem. Furthermore, his proof that the C4-free 2-factor problem (with general
weights) is NP-hard was done for cubic graphs. He also showed that the C5-free
2-factor problem with weights all 0, 1 is NP-hard for cubic graphs. Some partial
polyhedral results have also been obtained by Cunningham and Wang [8], who
presented a class of valid inequalities for the polytope associated with the Ck-
free 2-factor problem and studied the special structure of these inequalities for
the C3-free 2-factor problem.
Our main results are, for subcubic graphs, a complete description of the convex
hull of C3-free 2-matchings and a strongly polynomial time algorithm for ﬁnding
a maximum weight C3-free 2-matching. (This polytope immediately yields the
polytope for C3-free 2-factors in subcubic graphs.) An interesting property of
this polytope is that it requires the use of two types of inequalities not needed
for the 2-matching polytope. One type is straightforward and simply requires
that, for each triangle in the graph, the sum of the associated edge variables is
at most 2. The second type is a class, we call them tri-combs, that properly falls

Triangle-Free Simple 2-Matchings Subcubic Graphs
45
between the blossom inequalities (as introduced by Edmonds for the 2-matching
polytope [10]) and the more general comb inequalities (introduced by Gr¨otschel
and Padberg [13] for the travelling salesman polytope; another class, the Chv´atal
combs [4], also properly falls between the blossom and comb inequalities, but is
diﬀerent from our tri-combs). Results of Cunningham and Wang [8] show that
the tri-combs are not suﬃcient to describe the C3-free 2-factor polytope on
general graphs. They show that additional needed inequalities include, but are
not limited to, the so-called bipartition inequalities.
A tri-comb is a comb such that every tooth has at most 3 vertices and every
tooth has exactly one node not in the handle. A blossom is a comb such that every
tooth has exactly two vertices. Another property of our tri-comb inequalities that
distinquishes them from blossoms is that they are not {0,1}-inequalities; i.e., the
variables have coeﬃcients in {0,1,2}. We also show, somewhat surprisingly, that
only the {0,1}-inequalities in this class (i.e., the blossoms) are needed to describe
the polytope for C3-free 2-factors in subcubic graphs (which is a face of the
polytope for C3-free 2-matchings). Thus we see that C3-free 2-matchings and C3-
free 2-factors, in this domain, have signiﬁcantly diﬀerent polyhedral descriptions.
To the best of our knowledge, this is the only matching example known to have
this property.
Our main polyhedral result is proved using the algorithm, which is primal-dual
in the style used by Edmonds [9] for the classical matching problem. Polyhedral
and algorithmic results for the 2-matching problem (and more general problems)
are often proved in the literature by using a reduction to classical matchings due
to Tutte [28] (see Schrijver [25]). Algorithms that avoid such a reduction are
typically more eﬃcient (but more complex) and have also been studied (e.g.,
see Johnson [19] and Gabow [12]). We have been unable to ﬁnd such a reduc-
tion for the C3-free 2-matching problem in subcubic graphs, hence our algorithm
works directly on the original graph. However, for the restricted case of C3-free
2-factors, a reduction was discovered by Vornberger [29]. It yields a polynomial
algorithm for ﬁnding C3-free 2-factors in cubic graphs (which can be extended,
in a straightforward manner, to ﬁnding C3-free 2-factors in subcubic graphs). We
show that this same reduction idea, combined with the 3-cut polyhedral reduc-
tion idea for the TSP problem developed by Cornu´ejols, Naddef, and Pulleyblank
[6], yields the C3-free 2-factor polytope for subcubic graphs.
Let us remark on a topic one might expect to accompany work of this type.
In Edmonds’ primal-dual algorithm for classical matchings, the primal stage is
essentially an algorithm for ﬁnding a maximum (0, 1)-weight matching, which is
applied to a special subgraph of the original graph. Hence, as a by-product, one
obtains a simpler algorithm for ﬁnding a maximum (0, 1)-weight matching and
one can prove, directly from the algorithm, theorems such as Tutte’s characteri-
zation of the graphs with a perfect matching [27] and the min-max Tutte-Berge
theorem [1]. The algorithm we have d eveloped does not appear to have these
nice by-products, at least in a direct way. Hence we plan to address the special
case of (0, 1)-weights elsewhere.

46
D. Hartvigsen and Y. Li
This extended abstract is organized as follows. In Section 2 we state our two
main polyhedral results. Section 3 contains an outline of our non-algorithmic
proof of the second, simpler result. The ﬁnal section contains an overview of
the primal-dual algorithm for C3-free 2-matchings. The details of the proofs and
algorithm will appear elsewhere.
We close this section by referencing some related research. A {0,1,2}-matching
is an assignment of 0,1, or 2 to each edge in a graph so that the sum of the
values on the edges incident with each node is at most 2. (In this terminol-
ogy, we consider {0,1}-matchings in this paper.) Cornu´ejols and Pulleyblank
in [7] completely characterize the convex hull of C3-free {0,1,2}-matchings and
present a polynomial time algorithm for ﬁnding maximum weight C3-free {0,1,2}-
matchings. Their algorithm is similar in style to the one presented in this paper:
it is primal-dual (as in [9]) and provides a proof of the polyhedral result.
Finally, we note that there is a fairly extensive literature involving the study
of matching problems in regular graphs, particularly in cubic graphs. Here are
some of the highlights:
• Kaiser and Skrekovski [20] showed that every bridgeless cubic graph has a
2-factor that intersects all edge cuts of size 3 or 4. This result implies that
every bridgeless cubic graph has a C3-free 2-factor.
• Petersen [23] showed that every bridgeless cubic graph can be decomposed
into a 2-factor and a perfect matching.
• Tait [26] showed that every planar bridgeless cubic graph can be decomposed
into three perfect matchings iﬀthe 4-color conjecture holds (which, of course,
is now a theorem). And Petersen [24] showed that this is not true for non-
planar bridgeless cubic graphs by exhibiting what we now call the Petersen
graph. Holyer [18] showed that it’s NP-complete to decide if a cubic graph
can be decomposed into three matchings.
• Bertram and Horak [2] showed that there is a polynomial time algorithm to
decompose any 4-regular graph into two C3-free 2-factors, if such a decom-
position exists.
• The study of Hamilton cycles in cubic graphs is fairly extensive. A key result
is that the problem of deciding if a planar, bipartite, subcubic graph has a
Hamiltonian cycle is NP-complete (see [21]). Additional results can be found
in [17].
2
Main Results
In this section we present our main polyhedral results: complete descriptions of
the convex hulls of C3-free 2-matchings and 2-factors for subcubic graphs. We
begin with some deﬁnitions.
Let G = (V, E) be a graph. For V ′ ⊆V , let δ(V ′) denote the set of edges
of G with exactly one vertex in V ′; and let γ(V ′) denote the set of edges of G
with both vertices in V ′. For S ⊆E and x ∈IRE, let x(S) = 
e∈S xe. If G′ is
a subgraph of G, then a vector x ∈{0, 1}E is called the 0-1 incidence vector for

Triangle-Free Simple 2-Matchings Subcubic Graphs
47
G′ if xe = 1 if and only if e is in G′. A triangle of G is a set of three edges of G
that form a cycle. A C3-free 2-matching is also called a triangle-free 2-matching.
Let P M(G) denote the convex hull of incidence vectors of triangle-free 2-
matchings in G. Let P F (G) denote the convex hull of incidence vectors of
triangle-free 2-factors in G.
A tri-comb of G is a set {H, T1, . . . T2k+1} of subsets of V , where k ≥1, that
satisfy the following conditions:
1. T1, . . . T2k+1 are pairwise disjoint;
2. For each i, γ(Ti) is either a single edge or a triangle;
3. Each Ti has exactly one vertex not in H.
We call H the handle and T1, . . . T2k+1 the teeth of the tri-comb. A tooth
with two vertices is called an edge-tooth and a tooth with three edges is called a
triangle-tooth. Observe that every triangle-tooth has exactly one edge in common
with γ(H), which we call a common edge. See Fig. 1, which contains a tri-comb
with two triangle-teeth, one edge-tooth, and two common edges.
Fig. 1. A tri-comb
Consider the following variations on conditions 2 and 3:
2′. For each i, γ(Ti) is a single edge;
3′. Each Ti has, at least, one vertex in H and one vertex not in H.
The sets {H, T1, . . . , T2k+1} that satisfy conditions 1, 2′, and 3 are the well-
known class of blossoms and the sets that satisfy conditions 1 and 3′ are the
well-known class of combs. Hence tri-combs are more general than blossoms and
a special case of combs.
For x a variable vector indexed on E, we associate with each tri-comb C =
{H, T1, . . . , T2k+1} the following tri-comb inequality:
x(γ(H)) +
2k+1

i=1
x (γ(Ti)) ≤|H| +
2k+1

i=1
(|Ti| −1) −(k + 1),
which we abbreviate, a(C)x ≤b(C). In general, these inequalities have non-zero
variable coeﬃcients in {1, 2}, with the 2s precisely on the common edges. The tri-
comb inequalities can be shown to be feasible for P M(G) (hence for P F (G)) using

48
D. Hartvigsen and Y. Li
standard arguments (e.g., see [5]). For the cases of blossoms and combs, this same
inequality is the well-known blossom inequality and comb inequality, respectively.
Observethat for blossoms, the right hand side of the inequality simpliﬁes to |H|+k.
(We have borrowed the notation for this inequality from [5].)
We are now ready to state our characterization of P M(G).
Theorem 1. For a subcubic graph G = (V, E), P M(G) is determined by
x(δ(v)) ≤2
∀v ∈V
(1)
xe ≤1
∀e ∈E
(2)
x(T ) ≤2
∀triangles T
(3)
a(C)x ≤b(C)
∀tri-combs C
(4)
xe ≥0
∀e ∈E
(5)
Let SM(G) denote the system in Theorem 1. If we remove inequalities (3) and
replace “tri-combs” with “blossoms” in inequality (4), then we obtain the convex
hull of incidence vectors of 2-matchings given by Edmonds [10] for general graphs.
If, instead, we remove inequalities (4) and replace inequalities (2) with xe ≤
2 ∀e ∈E, we obtain the convex hull of incidence vectors of C3-free {0,1,2}-
matchings given by Cornu´ejols and Pulleyblank [7] for general graphs.
Fig. 2 contains a subcubic graph that illustrates the need for the tri-comb
inequalities in SM(G). The numbers indicate a fractional solution x that can
be seen to be extreme for the system of inequalities: (1), (2), (3), and (5). The
tri-comb inequality associated with the entire graph (which is a tri-comb C
with indicated handle H) has b(C) = 6; however, the fractional solution has
a(C)x = 6.5. In contrast, one can easily check that no blossom inequality is able
to cut oﬀthis fractional extreme solution.
Fig. 2. A fractional extreme point cut oﬀby a tri-comb inequality
Our characterization of P F (G) follows.
Theorem 2. For a subcubic graph G = (V, E), P F (G) is determined by
x(δ(v)) = 2
∀v ∈V
(6)
xe ≤1
∀e ∈E
(7)
x(T ) = 2
∀triangles T
(8)
a(C)x ≤b(C)
∀blossoms C
(9)
xe ≥0
∀e ∈E
(10)

Triangle-Free Simple 2-Matchings Subcubic Graphs
49
Let SF (G) denote the system in Theorem 2. If we remove inequalities (8), then
we obtain the convex hull of incidence vectors of 2-factors given by Edmonds
[10] for general graphs.
3
The 2-Factor Polytope for Subcubic Graphs
In this section we outline the proof of Theorem 2. The proof uses a triangle
shrinking operation and a variation on the Basic Polyhedral Theorem in [6].
Fig. 3 shows the four triangle patterns that can exist in a subcubic graph (i.e.,
a triangle can have 0, 1, 2, or 3 vertices of degree 2). If two or three vertices of a
triangle have degree 2 in the graph, then there is obviously no feasible solution
to the triangle-free 2-factor problem. Therefore, we only consider the subcubic
graphs with the triangle patterns shown in Fig. 3(a) and Fig. 3(b).
Fig. 3. Four triangle patterns
Let G = (V, E) be a graph and let S ⊆V , such that |S| ≥2. We let G × S
denote the graph obtained by shrinking (or contracting) S. That is, the vertices
of G×S are the vertices of V \S, plus a new vertex, say v, obtained by identifying
all the vertices in S. The edges of G × S are the edges in γ (V \S) and the edges
of δ(S), each of which now has one vertex v and its original vertex in V \S. All
edges in G × S retain their identities from G.
If T is a triangle (which is a set of three edges), we let V (T ) denote the vertices
of T .
The following lemmas will help us obtain P F (G).
Lemma 1. Let G be a subcubic graph and T be a triangle of G that has the
pattern of Figure 3(a) or Figure 3(b). A linear system suﬃcient to deﬁne P F (G)
is obtained by taking the union of linear systems suﬃcient to deﬁne P F (G ×
V (T )) and P F (G × (V \V (T ))).
Lemma 2. P F (G × (V \V (T ))) is determined by
x(δ(v)) = 2
∀v ∈V (T )
xe ≤1
∀e ∈E(G × (V \V (T )))

50
D. Hartvigsen and Y. Li
x(T ) = 2
xe ≥0
∀e ∈E(G × (V \V (T )))
The proof of Lemma 1 is quite similar to the proof of the Basic Polyhedral
Theorem in [6]. That theorem says that one can obtain a linear system suﬃcient
to deﬁne the TSP polytope T SP(G) by the union of two linear systems suﬃcient
to deﬁne T SP(G×S) and T SP(G× ¯S), where G has a 3-edge cutset with shores
S and ¯S. The proof of Lemma 2 is straightforward.
We can now obtain P F (G) as follows: First, we iteratively shrink triangles
that have three original nodes of G and apply the above two lemmas until every
triangle in the graph has at least one shrunk node. Then we apply the polyhedral
description of 2-factors [10] to this ﬁnal graph. Finally, the union of all the linear
systems obtained through applying Lemma 1 and Lemma 2 and the linear system
suﬃcient to deﬁne the 2-factors of the ﬁnal graph deﬁnes P F (G).
4
The Algorithm for Finding Max Weight 2-Matchings
in Subcubic Graphs
In this section we give an overview of the algorithm, followed by a few details
describing how the algorithm is set up. The details of the algorithm are approx-
imately ten pages long and will appear elsewhere.
The algorithm has two main phases: primal and dual. While maintaining
primal and dual feasible solutions, the algorithm alternates between these two
phases until it produces primal and dual feasible solutions that satisfy comple-
mentary slackness, and hence are optimal. The primal phase has two main stages.
We call the ﬁrst stage “triangle alteration.” In this stage we identify special tri-
angles in the original graph and alter each by either shrinking the triangle to a
vertex, shrinking an edge of the triangle to a vertex, or deleting one of its edges.
In some sense, this is an elaborated version of the shrinking operation described
in the preceeding section; however, this type of shrinking cannot be done just
once at the beginning of the algorithm – it occurs repeatedly throughout. In the
second stage of the primal phase we grow an alternating tree looking for ways to
satisfy violated complementary slackness conditions. If no such improvement is
found, the algorithm moves into its second phase, the dual change. In this phase
the dual solution is changed, again in an eﬀort to satisfy violated complementary
slackness conditions. The growth and dual change steps are, in a general sense,
typical of Edmonds-style matching algorithms; the triangle alteration stage is
unique to this problem. The primal growth stage is quite straightforward due
to the simple structure of the graphs and our triangle alteration stage; however,
the dual change is signiﬁcantly more complex than is typical of such algorithms.
We next present some details we need to set up the algorithm. For an arbitrary
subcubic graph G = (V, E), let w ∈IRE be an arbitrary weight vector, let T
denote the set of all triangles in G, and let T C denote the set of all tri-combs in
G. We let T (e) denote the triangles of G that contain edge e, let T C1(e) denote
the tri-combs of G that contain e as a non-common edge, and let T C2(e) denote
the tri-combs of G that contain e as a common edge.

Triangle-Free Simple 2-Matchings Subcubic Graphs
51
The primal LP is the following:
max wx s.t. x ∈SM(G).
If we associate variable vectors y, z, τ, π with constraints (1), (2), (3), (4),
respectively, then we obtain the corresponding dual LP:
min 2

v∈V
yv +

e∈E
ze + 2

T ∈T
τT +

C∈T C
b(C)πC
s.t.
yu + yv + zuv + 2

T ∈T (e)
τT +

C∈T C1(e)
πC + 2

C∈T C2(e)
πC ≥we ∀e = uv ∈E
y, z, τ, π ≥0.
From linear programming theory, a primal feasible solution x and a dual
feasible solution y, z, τ, π are both optimal if and only if they satisfy the following
complementary slackness conditions:
xe > 0 ⇒yu + yv + zuv +

C∈T C1(e)
πC + 2(

T ∈T (e)
τT +

C∈T C2(e)
πC) = we; (11)
yv > 0 ⇒x(δ(v)) = 2;
(12)
ze > 0 ⇒xe = 1;
(13)
τT > 0 ⇒x(T ) = 2;
(14)
πC > 0 ⇒a(C)x = b(C).
(15)
At each stage of the primal-dual algorithm we maintain an integral primal
feasible solution x, which is the incidence vector of a triangle-free 2-matching,
and a dual feasible solution y, z, τ, π, which satisﬁes (11), (13), (14) and (15).
Condition (12) is not, in general, satisﬁed. The algorithm modiﬁes the variables
x, y, z, τ, π (maintaining primal and dual feasibility as well as conditions (11),
(13), (14) and (15)) until condition (12) is satisﬁed at which point x is the
incidence vector of a maximum weight triangle-free 2-matching.
References
1. Berge, C.: Sur le couplate maximum d’un graphe. Comptes Rendus Hebdomadaires
des S´eances de l’Acad´emie Sciences [Paris] 247 (1958) 258–259
2. Bertram, E., Horak, P.: Decomposing 4-regular graphs into triangle-free 2-factors.
SIAM J. Disc. Math. 10 (1997) 309–317
3. Boyd, S.C., Cunningham, W.: Small travelling salesman polytopes. Math. Oper.
Res. 16 (1991) 259–271
4. Chv´atal, V.,: Edmonds polytopes and weakly hamiltonian graphs. Math. Prog. 5
(1973) 29–40
5. Cook, W.J., Cunningham, W.H., Pulleyblank, W.R., Schrijver, A.: Combinatorial
Optimization. John Wiley & Sons, New York (1998)

52
D. Hartvigsen and Y. Li
6. Cornu´ejols, G., Naddef, D., and Pulleyblank, W.R.: The traveling salesman prob-
lem in graphs with 3-edge cutsets. J.A.C.M. 32 (1985) 383–410
7. Cornu´ejols, G., Pulleyblank, W.R.: A matching problem with side constraints. Disc.
Math. 29 (1980) 135–159
8. Cunningham, W.H., Wang, Y.: Restricted 2-factor polytopes. Math. Prog. 87
(2000) 87–111
9. Edmonds, J.: Paths, trees, and ﬂowers. Canad. J. Math. 17 (1965) 449–467
10. Edmonds, J.: Maximum matching and a polyhedron with 0,1 vertices. J. of Res.
National Bur. of Standards 69 (1965) 125–130
11. Fisher, M.L., Nemhauser, G.L., Wolsey, L.A.: An analysis of approximations for
ﬁnding a maximum weight Hamiltonian circuit. Oper. Res. 27 (1979) 799–809
12. Gabow, H.N.: An eﬃcient reduction technique for degree-constrained subgraph and
bideredted network ﬂow problems. In: Proceedings of the Fifteenth Annual ACM
Symposium on Theory of Computing. The Association for Computing Machinery,
New York (1983) 448–456
13. Gr¨otschel, M., Padberg, M.W.: On the symmetric travelling salesman problem II:
Lifting theorems and facets. Math. Prog. 16 (1979) 282–302
14. Hartvigsen, D.: Extensions of Matching Theory. Ph.D. Thesis, Carnegie-Mellon
University (1984); under the supervision of G´erard Cornu´ejols.
15. Hartvigsen, D.: Finding maximum square-free 2-matchings in bipartite graphs. J.
of Comb. Th. B 96 (2006) 693–705
16. Hell, P., Kirkpatrick, D., Kratochv´ıl, J., K´r´ı´z, I.: On restricted 2-factors. SIAM J.
Disc. Math 1 (1988) 472–484
17. Holton, D., Aldred, R.E.L.: Planar graphs, regular graphs, bipartite graphs and
Hamiltonicity. Australas. J. Combin. 20 (1999) 111–131
18. Holyer, I.: The NP-completeness of edge coloring. SIAM Journal on Computing 10
(1981) 718–720
19. Johnson, E.: Network Flows, Graphs and Integer Programming. Ph.D. Thesis,
University of California, Berkeley (1965)
20. Kaiser, T., Skrekovski, R.: Cycles intersecting cuts of prescribed sizes. Manuscript
(2005)
21. Lawler, E.L., Lenstra, J.K., Rinnooy Kan, A.H.G., Shmoys, D.B.: The Traveling
Salesman Problem – A Guided Tour of Combinatorial Optimization. Wiley, Chich-
ester (1985)
22. Nam, Y.: Matching Theory: Subgraphs with Degree Constraints and other Prop-
erties. Ph.D. Thesis, University of British Columbia (1994); Under the supervision
of R. Anstee.
23. Petersen, J.: Die Theorie der regul¨aren graphs. Acta Mathematica 15 (1891)
193–220
24. Petersen, J.: Sur le theoreme de Tait. L’Intermediaire des Mathematiciens 5 (1898)
225–227
25. Schrijver, A.: Combinatorial Optimization, Polyhedra and Eﬃciency. Springer,
Berlin (2003)
26. Tait, P.G.: Remarks on the previous communication. Proceedings of the Royal
Society of Edinburgh 10 (1878-80) 729
27. Tutte, W.T.: The factorization of linear graphs. J. London Math. Soc. 22 (1947)
107–111
28. Tutte, W.T.: A short proof of the factor theorem for ﬁnite graphs. Canadian J. of
Math. 6 (1954) 347–352
29. Vornberger, O.: Easy and hard cycle covers. Manuscript, Universit¨at Paderborn
(1980)

The Smoothed Number of Pareto Optimal
Solutions in Bicriteria Integer Optimization⋆
Rene Beier1, Heiko R¨oglin2, and Berthold V¨ocking2
1 Max-Planck-Institut f¨ur Informatik
Saarbr¨ucken, Germany
rbeier@mpi-inf.mpg.de
2 Department of Computer Science
RWTH Aachen, D-52056 Aachen, Germany
{roeglin,voecking}@cs.rwth-aachen.de
Abstract. A well established heuristic approach for solving various bi-
criteria optimization problems is to enumerate the set of Pareto optimal
solutions, typically using some kind of dynamic programming approach.
The heuristics following this principle are often successful in practice.
Their running time, however, depends on the number of enumerated so-
lutions, which can be exponential in the worst case.
In this paper, we prove an almost tight bound on the expected number
of Pareto optimal solutions for general bicriteria integer optimization
problems in the framework of smoothed analysis. Our analysis is based
on a semi-random input model in which an adversary can specify an
input which is subsequently slightly perturbed at random, e. g., using a
Gaussian or uniform distribution.
Our results directly imply tight polynomial bounds on the expected
running time of the Nemhauser/Ullmann heuristic for the 0/1 knapsack
problem. Furthermore, we can signiﬁcantly improve the known results on
the running time of heuristics for the bounded knapsack problem and for
the bicriteria shortest path problem. Finally, our results also enable us
to improve and simplify the previously known analysis of the smoothed
complexity of integer programming.
1
Introduction
We study integer optimization problems having two criteria, say proﬁt and
weight, which are to be optimized simultaneously. A common approach for solv-
ing such problems is generating the set of Pareto optimal solutions, also known
as the Pareto set. Pareto optimal solutions are optimal compromises of the two
criteria in the sense that any improvement of one criterion implies an impair-
ment to the other. In other words, a solution S∗is Pareto optimal if there exists
no other solution S that dominates S∗, i. e., has at least the proﬁt and at most
the weight of S∗and at least one inequality is strict. Generating the Pareto set
⋆This work was supported by DFG grant VO 889/2 and by the EU within the 6th
Framework Programme under contract 001907 (DELIS).
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 53–67, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

54
R. Beier, H. R¨oglin, and B. V¨ocking
is of great interest in many scenarios and widely used in practice. Unfortunately,
this approach fails to yield reasonable results in the worst case because even
integer optimization problems with a simple combinatorial structure can have
exponentially many Pareto optimal solutions. In practice, however, generating
the Pareto set is often feasible since typically the number of Pareto optimal
solutions does not attain its worst-case bound.
The discrepancy between practical experience and worst-case results moti-
vates the study of the number of Pareto optimal solutions in a more realistic
scenario. One possible approach is to study the average number of Pareto opti-
mal solutions rather than the worst case number. In order to analyze the average,
one has to deﬁne a probability distribution on the set of instances, with respect
to which the average is taken. In most situations, however, it is not clear how to
choose a probability distribution that reﬂects typical inputs. In order to bypass
the limitations of worst-case and average-case analysis, Spielman and Teng de-
ﬁned the notion of smoothed analysis [15]. They consider a semi-random input
model in which an adversary speciﬁes an input which is then randomly per-
turbed. One can hope that semi-random input models are more realistic than
worst-case and average-case input models since the adversary can specify an arbi-
trary input with a certain structure, and the subsequent perturbation generates
an instance which is still close to the adversarial one.
We consider integer optimization problems in a semi-random setting, in which
an adversary can specify an arbitrary set S ⊆Dn of feasible solutions and two
objective functions: proﬁt p: S →R and weight w: S →R, where D ⊂Z
denotes a ﬁnite set of integers. We assume that the proﬁt is to be maximized
and the weight is to be minimized. This assumption is without loss of generality
as our results are not aﬀected by changing the optimization direction of any
of the objective functions. In our model, the weight function w can be chosen
arbitrarily by the adversary, whereas the proﬁt p has to be linear of the form
p(x) = p1x1 + · · · + pnxn. The adversary can choose an arbitrary vector of
proﬁts from [−1, 1]n, but in the second step of the semi-random input model, the
proﬁts pi are randomly perturbed by adding an independent Gaussian random
variable with mean 0 and standard deviation σ to each proﬁt pi. The standard
deviation σ can be seen as a parameter measuring how close the analysis is to
a worst-case analysis: The smaller σ is chosen, the smaller is the inﬂuence of
the perturbation and, hence, the closer is the analysis to a worst-case analysis.
Our probabilistic analysis is not restricted to Gaussian perturbations but is
much more general. In fact, it covers arbitrary probability distributions with a
bounded density function and a ﬁnite absolute mean value. In particular, if one
is interested in obtaining a positive domain for the proﬁts, one can restrict the
adversary to proﬁts pi ∈[0, 1] and perturb these proﬁts by adding independent
random variables that are distributed uniformly over some interval [0, c].
We present a new method for bounding the expected number of Pareto op-
timal solutions in the aforementioned scenario which yields an upper bound
that depends polynomially on the number of variables n, the integer with the
largest absolute value in D, and the reciprocal of the standard deviation σ. This

The Smoothed Number of Pareto Optimal Solutions
55
immediately implies polynomial upper bounds on the expected running time
of several heuristics for generating the Pareto set of problems like, e. g., the
Bounded Knapsack problem. Previous results of this kind were restricted to the
case of binary optimization problems. For this special case, our method yields an
improved upper bound, which matches the known lower bound. Furthermore, we
show that our results on the expected number of Pareto optimal solutions yield
a signiﬁcantly simpliﬁed and improved analysis of the smoothed complexity of
integer programming.
1.1
Previous Results
Multi-objective optimization is a well studied research area. Various algorithms
for generating the Pareto set of various optimization problems like, e. g., the
(bounded) knapsack problem [11,8], the bicriteria shortest path problem [4,14]
and the bicriteria network ﬂow problem [5,10], have been proposed. The running
time of these algorithms depends crucially on the number of Pareto optimal
solutions and, hence, none of them runs in polynomial time in the worst case.
In practice, however, generating the Pareto set is tractable in many situations.
For instance, M¨uller-Hannemann and Weihe [9] study the number of Pareto
optimal solutions in multi-criteria shortest path problems experimentally. They
consider examples that arise from computing the set of best train connections
(in view of travel time, fare, and number of train changes) and conclude that
in this application scenario generating the complete Pareto set is tractable even
for large instances. For more examples, we refer the reader to [6].
One way of coping with the bad worst-case behavior is to relax the requirement
of ﬁnding the complete Pareto set. Papadimitriou and Yannakakis present a gen-
eral framework for ﬁnding approximate Pareto sets. A solution S is ε-dominated
by another solution S′ if p(S)/p(S′) ≤1 + ε and w(S′)/w(S) ≤1 + ε. We say
that Pε is an ε-approximation of a Pareto set P if for any solution S ∈P there
is a solution S′ ∈Pε that ε-dominates it. Papadimitriou and Yannakakis show
that for any Pareto set P, there is an ε-approximation of P with polynomially
many points (w. r. t. the input size and 1/ε) [12]. Furthermore they give neces-
sary and suﬃcient conditions under which there is an FPTAS to generate Pε.
Vassilvitskii and Yannakakis [16] show how to compute ε-approximate Pareto
curves of almost minimal size.
Beier and V¨ocking analyze the expected number of Pareto optimal solu-
tions for binary optimization problems [2]. They consider the aforementioned
scenario with D = {0, 1} and show that the expected number of Pareto opti-
mal solutions is bounded from above by O(n4/σ). This result implies that the
Nemhauser/Ullmann algorithm [11] has polynomial expected running time. Fur-
thermore, they also present a lower bound of Ω(n2) on the expected number of
Pareto optimal solutions for proﬁts that are chosen uniformly from the interval
[0, 1].
In [3] Beier and V¨ocking analyze the smoothed complexity of binary optimiza-
tion problems. They consider optimization problems with one objective function
in which the set of feasible solutions is given as S∩B1∩. . .∩Bm, where S ⊆{0, 1}n

56
R. Beier, H. R¨oglin, and B. V¨ocking
denotes a ﬁxed ground set and Bi denotes a halfspace induced by a linear con-
straint of the form wi,1x1 + · · · + wi,nxn ≤ti. Similar to the aforementioned
model it is assumed that the coeﬃcients wi,j are perturbed by adding indepen-
dent random variables to them. Based on the probabilistic analysis of certain
structural properties, Beier and V¨ocking show that a binary optimization prob-
lem in this form has polynomial smoothed complexity if and only if there exists
a pseudo-polynomial (w. r. t. the wi,j) time algorithm for solving the problem.
The term polynomial smoothed complexity is deﬁned analogously to the way
polynomial complexity is deﬁned in average-case complexity theory, adding the
requirement that the running time should be polynomially bounded not only in
the input size but also in 1/σ. This characterization is extended to the case of
integer optimization problems where D ⊂Z is a ﬁnite set of integers by R¨oglin
and V¨ocking [13].
1.2
Our Results
In this paper, we present a new approach for bounding the expected number
of Pareto optimal solutions for bicriteria integer optimization problems. This
approach yields the ﬁrst bounds for integer optimization problems and improves
the known bound for the binary case signiﬁcantly. We show that the expected
number of Pareto optimal solutions is bounded from above by O(n2k2 log(k)/σ)
if D = {0, . . ., k −1}. We also present a lower bound of Ω(n2k2), assuming that
the proﬁts are chosen uniformly at random from the interval [−1, 1]. For the case
in which the adversary is restricted to linear weight functions, we present a lower
bound of Ω(n2k log k). Furthermore, for the binary case D = {0, 1}, the upper
bound simpliﬁes to O(n2/σ), which improves the previously known bound by a
factor of Θ(n2) and matches the lower bound in [2] in terms of n. Hence, our
method yields tight bounds in terms of n and almost tight bounds in terms of k
for the expected number of Pareto optimal solutions and, thereby, even simpliﬁes
the proof in [2]. In the following, we list some applications of these results.
Knapsack Problem. The Nemhauser/Ullmann algorithm solves the knapsack
problem by enumerating all Pareto optimal solutions [11]. Its running time on
an instance with n items is Θ(n
i=1 qi), where qi denotes the number of Pareto
optimal solutions of the knapsack instance that consists only of the ﬁrst i items.
Beier and V¨ocking analyze the expected number of Pareto optimal solutions and
show that the expected running time of the Nemhauser/Ullmann algorithm is
bounded by O(n5/σ) if all proﬁts are perturbed by adding Gaussian or uniformly
distributed random variables with standard deviation σ [2]. Based on our im-
proved bounds on the expected number of Pareto optimal solutions, we conclude
the following corollary.
Corollary 1. For semi-random knapsack instances in which the proﬁts are per-
turbed by adding independent Gaussian or uniformly distributed random variables
with standard deviation σ, the expected running time of the Nemhauser/Ullmann
algorithm is O(n3/σ).

The Smoothed Number of Pareto Optimal Solutions
57
For uniformly distributed proﬁts Beier and V¨ocking present a lower bound on the
expected running time of Ω(n3). Hence, we obtain tight bounds on the running
time of the Nemhauser/Ullmann algorithm in terms of the number of items n.
This lower bound can easily be extended to the case of Gaussian perturbations.
Bounded Knapsack Problem. In the bounded knapsack problem, a number k ∈N
and a set of n items with weights and proﬁts are given. It is assumed that k
instances of each of the n items are given. In [7] it is described how an in-
stance with n items of the bounded knapsack problem can be transformed into
an instance of the (binary) knapsack problem with Θ(n log (k + 1)) items. Us-
ing this transformation, the bounded knapsack problem can be solved by the
Nemhauser/Ullmann algorithm with running time Θ(n log (k+1)
i=1
qi), where qi
denotes the number of Pareto optimal solutions of the binary knapsack instance
that consists only of the ﬁrst i items. Based on our results on the expected
number of Pareto optimal solutions, we obtain the following corollary.
Corollary 2. The expected running time of the Nemhauser/Ullmann algorithm
on semi-random bounded knapsack instances in which the proﬁts are perturbed
by adding independent Gaussian or uniformly distributed random variables with
standard deviation σ is bounded from above by O(n3k2(log2 (k + 1))/σ) and
bounded from below by Ω(n3k log2 (k + 1)).
Hence, our results yield tight bounds in terms of n for the expected running time
of the Nemhauser/Ullmann algorithm.
Bicriteria Shortest Path Problem. Diﬀerent algorithms have been proposed for
enumerating the Pareto set in bicriteria shortest path problems [4,14]. In [4] a
modiﬁed version of the Bellman/Ford algorithm is suggested. Beier shows that
the expected running time of this algorithm is O(nm5/σ) for graphs with n
nodes and m edges [1]. We obtain the following improved bound.
Corollary 3. For semi-random bicriteria shortest path problems in which one
objective function is linear and its coeﬃcients are perturbed by adding indepen-
dent Gaussian or uniformly distributed random variables with standard devia-
tion σ, the expected running time of the modiﬁed Bellman/Ford algorithm is
O(nm3/σ).
Smoothed Complexity of Integer Programming. We were not able to bound the
expected number of Pareto optimal solutions for optimization problems with
more than two objective functions. One approach for tackling multicriteria prob-
lems is to solve a constrained problem in which all objective functions except
for one are made constraints. Our results for the bicriteria case can be used to
improve the smoothed analysis of integer optimization problems with multiple
constraints. In [13] we show that an integer optimization problem has polyno-
mial smoothed complexity if and only if there exists a pseudo-polynomial time
algorithm for solving the problem. To be more precise, we consider integer op-
timization problems in which an objective function is to be maximized over a

58
R. Beier, H. R¨oglin, and B. V¨ocking
feasible region that is deﬁned as the intersection of a ﬁxed ground set S ⊆Dn
with halfspaces B1, . . . , Bm that are induced by m linear constraints of the form
wi,1x1 +· · ·+wi,nxn ≤ti, where the wi,j are independently perturbed by adding
Gaussian or uniformly distributed random variables with standard deviation σ
to them.
The term polynomial smoothed complexity is deﬁned such that it is robust
under diﬀerent machine models analogously to the way polynomial average-case
complexity is deﬁned. One disadvantage of this deﬁnition is that polynomial
smoothed/average-case complexity does not imply expected polynomial run-
ning time. For the binary case it is shown in [3] that problems that admit a
pseudo-linear algorithm, i. e., an algorithm whose running time is bounded by
O(poly(N)W), where N denotes the input size and W the largest coeﬃcient
|wi,j| in the input, can be solved in expected polynomial time in the smoothed
model. Based on our analysis of the expected number of Pareto optimal solutions,
we generalize this result to the integer case.
Theorem 4. Every integer optimization problem that can be solved in time
O(poly(N)W), where N denotes the input size and W = maxi,j |wi,j|, allows an
algorithm with expected polynomial (in N and 1/σ) running time for perturbed
instances, in which an independent Gaussian or uniformly distributed random
variables with standard deviation σ is added to each coeﬃcient.
In the following section, we introduce the probabilistic model we analyze, which
is more general than the Gaussian and uniform perturbations described above.
After that, in Sections 3 and 4, we present the upper and lower bounds on the
expected number of Pareto optimal solutions. Finally, in Section 5, we present
the applications of our results to the smoothed analysis of integer programming.
2
Model and Notations
For the sake of a simple presentation, using the framework of smoothed analy-
sis, we described our results in the introduction not in their full generality. Our
probabilistic analysis assumes that the adversary can choose, for each pi, a prob-
ability distribution according to which pi is chosen independently of the other
proﬁts. We prove an upper bound that depends linearly on the maximal density
of the distributions and on the expected distance to zero. The maximal density
of a continuous probability distribution, i. e., the supremum of the density func-
tion, is a parameter of the distribution, which we denote by φ. Similar to the
standard deviation σ for Gaussian random variables, φ can be seen as a mea-
sure specifying how close the analysis is to a worst-case analysis. The larger φ,
the more concentrated the probability mass can be. For Gaussian and uniformly
distributed random variables, we have φ ∼1/σ.
In the following, we assume that pi is a random variable with density fi
and that fi(x) ≤φi for all x ∈R. Furthermore, we denote by μi the expected
absolute value of pi, i. e., μi = E [|pi|] =

x∈R |x|fi(x) dx. Let φ = maxi∈[n] φi and
μ = maxi∈[n] μi. We denote by [n] the set {1, . . ., n}, and we use the notations
d = |D| and D = max{a −b | a, b ∈D}.

The Smoothed Number of Pareto Optimal Solutions
59
3
Upper Bound on the Expected Number of Pareto
Optimal Solutions
While the proﬁt function is assumed to be linear with stochastic coeﬃcients,
the weight function w : S →R can be chosen arbitrarily. We model this by
assuming an explicit ranking of the solutions in S, which can be chosen by the
adversary. This way, we obtain a bicriteria optimization problem that aims at
maximizing the rank as well as the proﬁt. Observe that the weight function can
map several solutions to the same value whereas the rank of a solution is always
unique. This strict ordering, however, can only increase the number of Pareto
optimal solutions.
Theorem 5. Let S ⊆Dn be a set of arbitrarily ranked solutions with a ﬁnite
domain D ⊂Z. Deﬁne d = |D| and D = max{a −b | a, b ∈D}. Assume that
each proﬁt pi is a random variable with density function fi : R →R≥0. Suppose
μi = E [|pi|] and φi = supx∈R fi(x). Let q denote the number of Pareto optimal
solutions. Then
E [q] ≤2DdHd
 n

i=1
φi
  n

i=1
μi

+ O(dn) ,
where Hd is the d-th harmonic number. For D = {0, . . . , k −1} and μ =
maxi∈[n] μi and φ = maxi∈[n] φi the bound simpliﬁes to
E [q] = O(μφn2k2 log k) .
Note that the number of Pareto optimal solutions is not aﬀected when all proﬁts
are scaled by some constant c ̸= 0. This property is also reﬂected by the above
bound. The random variable cpi has maximal density φi/c and the expected
absolute value is cμi. Hence, the product φμ is invariant under scaling too.
Proof (Theorem 5). We use the following classiﬁcation of Pareto optimal solu-
tions. We say that a Pareto optimal solution x is of class c ∈D if there exists an
index i ∈[n] with xi ̸= c such that the succeeding Pareto optimal solution y satis-
ﬁes yi = c, where succeeding Pareto optimal solution refers to the highest ranked
Pareto optimal solution that is lower ranked than x. The lowest ranked Pareto
optimal solution, which does not have a succeeding Pareto optimal solution, is
not contained in any of the classes. A Pareto optimal solution can be in several
classes but it is at least in one class. Let qc denote the number of Pareto optimal
solutions of class c. Since q ≤1 + 
c∈D qc it holds E [q] ≤1 + 
c∈D E [qc].
Lemma 6 enables us to bound the expected number of class-0 Pareto opti-
mal solutions. In order to bound E [qc] for values c ̸= 0 we analyze a modiﬁed
sequence of solutions. Starting from the original sequence S = x1, x2, . . . , xl
(xj ∈Dn), we obtain a modiﬁed sequence Sc by subtracting (c, . . . , c) from each
solution vector xj. This way, the proﬁt of each solution is reduced by c  pi.

60
R. Beier, H. R¨oglin, and B. V¨ocking
x∗
ˆx
pTx
t
Rank
(in decreasing order)
Fig. 1. If ˆx is an ordinary class-0 Pareto optimal solution, then there must be an index
i with x∗
i = 0 and ˆxi ̸= 0
Observe that this operation does not aﬀect the set of Pareto optimal solutions. A
solution x is class-c Pareto optimal in S if and only if the corresponding solution
x−(c, . . . , c) is class-0 Pareto optimal in Sc. Hence, the number of class-c Pareto
optimal solutions in S corresponds to the number of class-0 Pareto optimal
solutions in Sc. We apply Lemma 6 for the solution set Sc with a corresponding
domain Dc = {z −c : z ∈D}. Since the diﬀerence between the largest and the
smallest element of the domain does not change, applying Lemma 6 yields that
E [q] is bounded from above by
1 +

c∈D
E [q0(Sc)] ≤1 +

c∈D
⎛
⎝D
⎛
⎝

v∈Dc\{0}
|v|−1
⎞
⎠
 n

i=1
φi
  n

i=1
μi

+ n
⎞
⎠,
and the theorem follows.
⊓⊔
Lemma 6. Let S ⊆Dn be a set of arbitrarily ranked solutions with a ﬁnite
domain D ⊂Z with 0 ∈D. Let D denote the diﬀerence between the largest and
the smallest element in D. Let q0 denote the number of class-0 Pareto optimal
solutions. Then
E [q0] ≤D
⎛
⎝

v∈D\{0}
|v|−1
⎞
⎠
 n

i=1
φi
  n

i=1
μi

+ n .
Proof. The key idea is to prove an upper bound on the probability that there
exists a class-0 Pareto optimal solution whose proﬁt falls into a small interval
(t −ε, t), for arbitrary t and ε. We will classify class-0 Pareto optimal solutions
to be ordinary or extraordinary. Considering only ordinary solutions allows us to
prove a bound that depends not only on the length ε of the interval but also on
|t|, the distance to zero. This captures the intuition that it becomes increasingly

The Smoothed Number of Pareto Optimal Solutions
61
y
x
pTx
yi = 0
xi ̸= 0
z
zi ̸= 0
Rank
(in decreasing order)
Fig. 2. In this case x is an extraordinary class-0 Pareto optimal solution
unlikely to observe solutions whose proﬁts are much larger than the expected
proﬁt of the most proﬁtable solution. The ﬁnal bound is obtained by observing
that there can be at most n extraordinary class-0 Pareto optimal solutions.
We want to bound the probability that there exists an ordinary class-0 Pareto
optimal solution whose proﬁt lies in the interval (t −ε, t). Deﬁne x∗to be the
highest ranked solution from S satisfying pTx ≥t. If x∗exists then it is Pareto
optimal. Let ˆx denote the Pareto optimal solution that precedes x∗, i. e., ˆx has
the largest proﬁt among all solutions that are higher ranked than x∗(see Fig. 1).
We aim at bounding the probability that ˆx is an ordinary class-0 Pareto optimal
solution and falls into the interval (t −ε, t).
We classify solutions to be ordinary or extraordinary as follows. Let x be
a class-0 Pareto optimal solution and let y be the succeeding Pareto optimal
solution, which must exist as the lowest ranked Pareto optimal solution is not
class-0 Pareto optimal. We say that x is extraordinary if for all indices i ∈[n]
with xi ̸= 0 and yi = 0, zi ̸= 0 holds for all Pareto optimal solutions z that
preceed x. In other words, for those indices i that make x class-0 Pareto optimal,
y is the highest ranked Pareto optimal solution that is independent of pi (see
Fig. 2). For every index i ∈[n] there can be at most one extraordinary class-0
Pareto optimal solution. In the following we will restrict ourselves to solutions
ˆx that are ordinary. Deﬁne
Λ(t) =

t −pTˆx if x∗and ˆx exist and ˆx is ordinary class-0 Pareto optimal
⊥
otherwise.
Let P0 denote the set of ordinary class-0 Pareto optimal solutions. Whenever
Λ(t) < ε, then there exists a solution x ∈P0 with pTx ∈(t −ε, t), namely ˆx.
The reverse is not true because it might be the case that ˆx ̸∈P0 but that there
exists another solution x ∈P0 with pTx ∈(t−ε, t). If, however, ε is smaller than
the minimum distance between two Pareto optimal solutions, then the existence
of a solution x ∈P0 with pTx ∈(t −ε, t) implies ˆx = x and hence Λ(t) < ε. Let

62
R. Beier, H. R¨oglin, and B. V¨ocking
A(t, ε) denote the event that there is at most one Pareto optimal solution with
a proﬁt in the interval (t −ε, t). Then
Pr [Λ(t) < ε] ≥Pr [(Λ(t) < ε) ∧A(t, ε)]
= Pr

(∃x ∈P0 : pTx ∈(t −ε, t)) ∧A(t, ε)

≥Pr

∃x ∈P0 : pTx ∈(t −ε, t)

−Pr [¬A(t, ε)] ,
and therefore
lim
ε→0
Pr [Λ(t) < ε]
ε
≥lim
ε→0
Pr

∃x ∈P0 : pTx ∈(t −ε, t)

ε
−lim
ε→0
Pr [¬A(t, ε)]
ε
.
In the full version we show that for every t ̸= 0 the probability of that two
solutions lie in the interval (t −ε, t) decreases like ε2 for ε →0. Hence, for every
t ̸= 0, limε→0 Pr[¬A(t,ε)]
ε
= 0. Since the expected number of ordinary class-0
Pareto optimal solutions can be written as
 ∞
−∞
lim
ε→0
Pr

∃x ∈P0 : pTx ∈(t −ε, t)

ε
dt ≤
 ∞
−∞
lim
ε→0
Pr [Λ(t) < ε]
ε
dt ,
it remains to analyze the term Pr [Λ(t) < ε]. In order to analyze this probability
we deﬁne a set of auxiliary random variables such that Λ(t) is guaranteed to
always take a value also taken by one of the auxiliary random variables. Then
we analyze the auxiliary random variables and use a union bound to conclude
the desired bound for Λ(t).
Deﬁne D′ = D\{0} and Sxi=v = {x ∈S | xi = v} for all i ∈[n] and v ∈D.
Let x∗(i) denote the highest ranked solution from Sxi=0 with proﬁt at least t.
For each i ∈[n] and v ∈D′ we deﬁne the set L(i,v) as follows. If x∗(i) does not
exist or x∗(i) is the highest ranked solution in Sxi=0 then we deﬁne L(i,v) = ∅.
Otherwise L(i,v) consists of all solutions from Sxi=v that have a higher rank than
x∗(i). Let ˆx(i,v) denote the lowest ranked Pareto optimal solution from the set
L(i,v), i. e., ˆx(i,v) has the largest proﬁt among all solutions in L(i,v). Finally we
deﬁne for each i ∈[n] and v ∈D′ the auxiliary random variable
Λv
i (t) =

t −pTˆx(i,v) if ˆx(i,v) exists,
⊥
otherwise.
If Λv
i (t) ∈(0, ε) (which excludes Λv
i (t) =⊥) then the following three events must
co-occur:
1. E1 : There exists an x ∈Sxi=0 with pTx ≥t.
2. E2 : There exists an x ∈Sxi=0 with pTx < t.
3. E3 : ˆx(i,v) exists and its proﬁt falls into the interval (t −ε, t).
The events E1 and E2 only depend on the proﬁts pj, j ̸= i. The existence
and identity of ˆx(i,v) is completely determined by those proﬁts as well.
Hence, if we ﬁx all proﬁts except for pi, then ˆx(i,v) is ﬁxed and its proﬁt is

The Smoothed Number of Pareto Optimal Solutions
63
c + vpi for some constant c that depends on the proﬁts already ﬁxed. Ob-
serve that the random variable c + vpi has density at most φi/|v|. Hence we
obtainPr

pTˆx(i,v) ∈(t −ε, t)
 ˆx(i,v) exists

≤ε φi
|v|. Deﬁne
P + =

j:pj>0
pj
and
P −=

j:pj<0
pj .
Moreover let d+ and d−denote the largest and the smallest element in D.
For t ≥0, the event E1 implies t ≤d+P + + d−P −, and hence Pr [E1] ≤
Pr [d+P + + d−P −≥t]. For t ≤0, the event E2 implies t > d+P −+ d−P +
and hence Pr [E2] ≤Pr [d+P −+ d−P + ≤t]. By combining these results we get
Pr [Λv
i (t) ∈(0, ε)] ≤

Pr [d+P + + d−P −≥t] ε φi
|v|, for t ≥0, and
Pr [d+P −+ d−P + ≤t] ε φi
|v|, for t ≤0.
Next we argue that Λ(t) < ε implies Λv
i (t) ∈(0, ε) for at least one pair
(i, v) ∈[n] × D′. So assume that Λ(t) < ε. By deﬁnition, x∗and ˆx exist and ˆx
is an ordinary class-0 Pareto optimal solution. Since ˆx is class-0 Pareto optimal
and x∗is the succeeding Pareto optimal solution, there exists an index i ∈[n]
such that
(a) x∗
i = 0 and ˆxi = v ̸= 0 for some v ∈D′, and
(b) x∗is not the highest ranked solution in Sxi=0.
The second condition is a consequence of the assumption, that ˆx is not extraor-
dinary, i. e., there exists a Pareto optimal solution z with zi = 0 that has higher
rank than ˆx. Recall that x∗(i) is deﬁned to be the highest ranked solution in
Sxi=0 with pTx ≥t. As x∗∈Sxi=0, x∗= x∗(i). Moreover, L(i,v) consists of
all solutions from Sxi=v that have a higher rank than x∗. Thus, ˆx ∈L(i,v). By
construction, ˆx has the largest proﬁt among the solutions in L(i,v) and, therefore
ˆx(i,v) = ˆx and Λv
i (t) = Λ(t). Applying a union bound yields, for all t ≥0,
Pr [Λ(t) < ε] ≤
n

i=1

v∈D′
Pr [Λv
i (t) < ε]
≤
n

i=1

v∈D′
Pr

d+P + + d−P −≥t

ε φi
|v|
≤Pr

d+P + + d−P −≥t

ε
n

i=1

v∈D′
φi
|v| .
For t ≤0 we get analogously
Pr [Λ(t) < ε] ≤Pr

d+P −+ d−P + ≤t

ε
n

i=1

v∈D′
φi
|v| .

64
R. Beier, H. R¨oglin, and B. V¨ocking
Now we can bound the expected number of class-0 Pareto optimal solutions,
taking into account that at most n of them can be extraordinary.
E [q0] ≤n +
 ∞
−∞
lim
ε→0
Pr [Λ(t) ≤ε]
ε
dt
≤n +
 ∞
0
lim
ε→0
Pr [d+P + + d−P −≥t] ε n
i=1

v
φi
|v|
ε
dt
+
 0
−∞
lim
ε→0
Pr [d+P −+ d−P + ≤t] ε n
i=1

v
φi
|v|
ε
dt
≤n +

v
1
|v|
  n

i=1
φi
  ∞
0
Pr

d+P + + d−P −≥t

dt
+
 ∞
0
Pr

−d+P −−d−P + ≥t

dt

As 0 ∈D, it holds d+ ≥0 and d−≤0. Hence we have d+P + + d−P −≥0,
−d+P −−d−P + ≥0, and
 ∞
0
Pr

d+P + + d−P −≥t

dt +
 ∞
0
Pr

−d+P −−d−P + ≥t

dt
= E

d+P + + d−P −
+ E

−d+P −−d−P +
= (d+ −d−) E

P + −P −
= (d+ −d−) E
 n

i=1
|pi|

= D
n

i=1
μi .
⊓⊔
4
Lower Bounds on the Expected Number of Pareto
Optimal Solutions
In this section we present a lower bound of Ω(n2k log(1 + k)) on the number of
Pareto optimal solutions for D = {0, . . . , k}, generalizing a bound for the binary
domain presented in [2]. In Theorem 8 we prove the stronger bound Ω(n2k2)
under slightly stronger assumptions. The weaker bound provides a vector of
weights w1, . . . , wn, such that the bound holds for a linear weight function wTx.
For the stronger bound we can only prove that there is some weight function
w: S →R for which the bound holds but this function might not be linear.
In combinatorial optimization, however, many problems have linear objective
functions. The proofs of the theorems in this section will be contained in the full
version of this paper.
Theorem 7. Let D = {0, . . . , k}. Suppose proﬁts are drawn independently at
random according to a continuous probability distribution with non-increasing
density function f : R≥0 →R≥0. Let q denote the number of Pareto optimal
solutions over S = Dn. Then there is a vector of weights w1, . . . , wn ∈R>0 for
which
E [q]
≥Hk
4 k(n2 −n) + kn + 1 ,

The Smoothed Number of Pareto Optimal Solutions
65
where Hk is the k-th harmonic number. If the proﬁts are drawn according to the
uniform distribution over some interval [0, c] with c > 0 then the above inequality
holds with equality.
Similarly, a lower bound of Ω(n2k log k) can be obtained for the case that f is
the density of a Gaussian random variable with mean 0. Since all weights wi are
larger than 0, a solution with a negative proﬁt cannot be contained in a Pareto
optimal solution. Hence, we can ignore those items. Restricted to the interval
[0, ∞) the density of a Gaussian random variable with mean 0 is non-increasing
and, hence, we can apply Theorem 7.
Now we consider general weight functions and show a lower bound of Ω(n2k2)
on the expected number of Pareto optimal solutions for D = {0, . . ., k} and
S = Dn. We assume that k is a function of n with (5(c + 1) + 1) log n ≤k ≤nc
for some constant c. We use the probabilistic method to show that, for each
suﬃciently large n ∈N, a ranking exists for which the expected number of Pareto
optimal solutions is lower bounded by n2k2/κ for some constant κ depending
only on c, that is, we create a ranking at random (but independently of the
proﬁts) and show that the expected number of Pareto optimal solutions (where
the expectation is taken over both the random ranking and the random proﬁts)
satisﬁes the desired lower bound. This implies that, for each suﬃciently large
n ∈N, there must exist a deterministic ranking on {0, . . ., k}n for which the
expected number of Pareto optimal solutions (where the expectation is now
taken only over the random proﬁts) is at least n2k2/κ.
Theorem 8. Let (5(c + 1) + 1) log n ≤k ≤nc for some c ≥2 and assume that
n is a multiple of c + 2. There exists a constant κ depending only on c and a
ranking on {0, . . ., k}n such that the expected number of Pareto optimal solutions
is lower bounded by n2k2/κ if each proﬁt pi is chosen independently, uniformly
at random from the interval [−1, 1].
5
Smoothed Complexity of Integer Programming
In [13], we analyze the smoothed complexity of integer programming. We con-
sider integer programs in which an objective function is to be maximized over a
feasible region that is deﬁned as the intersection of a ﬁxed ground set S ⊆Dn
with a halfspace B that is induced by a linear constraint w1x1 + · · · + wnxn ≤t,
where the wi are independent random variables which can be represented by
densities that are bounded by φ. We show that an integer optimization problem
in this form has polynomial smoothed complexity if and only if there exists a
pseudo-polynomial algorithm (w. r. t. the wi) for solving it.
The main technical contribution in [13] is the analysis of the random variables
loser gap and feasibility gap. The feasibility gap Γ is deﬁned as the slack of the
optimal solution from the threshold t. To be more precise, let x∗denote the
optimal solution, that is, x∗denotes the solution from S ∩B that maximizes the
objective function. Then the feasibility gap can be deﬁned as Γ = t −wTx∗. A
solution x ∈S is called a loser if it has a higher objective value than x∗but is

66
R. Beier, H. R¨oglin, and B. V¨ocking
infeasible due to the linear constraint, that is, wTx > t. We denote the set of all
losers by L. Furthermore, we deﬁne the minimal loser x ∈L to be the solution
from L with minimal weight, that is, x = argmin{wTx | x ∈L}. The loser gap
Λ denotes the slack of the minimal loser from the threshold t, i. e., Λ = wTx −t.
If both the loser and the feasibility gap are not too small, then rounding
all weights wi with suﬃcient accuracy does not change the optimal solution.
Rounding the weights can only aﬀect the optimal solution if either x∗becomes
infeasible or a loser x becomes feasible. The former event can only occur if the
feasibility gap is small; the latter event can only occur if the loser gap is small.
In a rather technical and lengthy analysis we show the following lemma on the
probability that the loser or the feasibility gap is small.
Lemma 9. (Separating Lemma [13]) Let S ⊆Dn with 0n /∈S be chosen arbi-
trarily, let μ = maxi∈[n] E [|wi|], d = |D|, and dmax = max{|a| | a ∈D}. Then,
for all ε ∈[0, (32μn5d7dmaxφ2)−1],
Pr [Γ ≤ε] ≤2(ε·32μn5d7dmaxφ2)1/3 and Pr [Λ ≤ε] ≤2(ε·32μn5d7dmaxφ2)1/3.
In the full version of this paper we present a much simpler proof for the following
improved version of the previous lemma.
Theorem 10. Let S ⊆Dn with 0n /∈S be chosen arbitrarily, and let D =
max{a −b | a, b ∈D} ≤2dmax. There exists a constant κ such that, for all
ε ≥0,
Pr [Γ ≤ε] ≤εκφ2μn3Dd log2 d and Pr [Λ ≤ε] ≤εκφ2μn3Dd log2 d .
In [13] we show that Lemma 9 can also be used to analyze integer optimization
problems with more than one linear constraint. We consider integer optimization
problems in which an objective function is to be maximized over a feasible region
that is deﬁned as the intersection of a ﬁxed ground set S ⊆Dn with halfspaces
B1, . . . , Bm that are induced by m linear constraints of the form wi,1x1 + · · · +
wi,nxn ≤ti, where the wi,j are independent random variables which can be
represented by densities that are bounded by φ.
The feasibility gap Γ for multiple constraints is deﬁned to be the minimal slack
of the optimal solution x∗from one of the thresholds, i. e., Γ = mini∈[m](ti −
(wi,1x1+· · ·+wi,nxn)). The loser gap Λ for multiple constraints is deﬁned as Λ =
minx∈L maxi∈[m](wi,1x1 +· · ·+wi,nxn −ti). In [13] we show how Lemma 9 gives
rise to bounds on the sizes of loser and feasibility gap for multiple constraints.
Based on this observation we show that an integer optimization problem with
multiple constraints has polynomial smoothed complexity if and only if there
exists a pseudo-polynomial algorithm (w. r. t. the wi,j) for solving it. By applying
the same arguments, our bounds in Theorem 10 yield the following corollary.
Corollary 11. Let S ⊆Dn with 0n /∈S be chosen arbitrarily, let D = max{a −
b | a, b ∈D} ≤2dmax, and let the set of feasible solutions be given as S ∩B1 ∩
. . . ∩Bm. There exists a constant κ such that, for all ε ≥0,
Pr [Γ ≤ε] ≤εκφ2μmn3Dd log2 d and Pr [Λ ≤ε] ≤εκφ2μmn3Dd log2 d .

The Smoothed Number of Pareto Optimal Solutions
67
The main improvement upon our previous analysis is that the bounds in Corol-
lary 11 depend only linearly on ε instead of ε1/3. Due to this improvement we
can prove Theorem 4 in the same way as its binary version in [3], which is not
possible with the bounds derived in [13].
References
1. Ren´e Beier. Probabilistic Analysis of Discrete Optimization Problems. PhD thesis,
Universit¨at des Saarlandes, 2004.
2. Ren´e Beier and Berthold V¨ocking. Random knapsack in expected polynomial time.
Journal of Computer and System Sciences, 69(3):306–329, 2004.
3. Ren´e Beier and Berthold V¨ocking.
Typical properties of winners and losers in
discrete optimization. SIAM Journal on Computing, 35(4):855–881, 2006.
4. H.W. Corley and I.D Moon.
Shortest paths in networks with vector weights.
Journal of Optimization Theory and Application, 46(1):79–86, 1985.
5. Matthias Ehrgott. Integer solutions of multicriteria network ﬂow problems. Inves-
tigacao Operacional, 19:61–73, 1999.
6. Matthias Ehrgott and Xavier Gandibleux. Multiple Criteria Optimization, volume
491 of Lecture Notes in Economics and Mathematical Systems, chapter Multiob-
jective Combinatorial Optimization. Springer-Verlag, 2000.
7. H. Kellerer, U. Pferschy, and D. Pisinger. Knapsack Problems. Springer, Berlin,
Germany, 2004.
8. Kathrin Klamroth and Margaret M. Wiecek. Dynamic programming approaches
to the multiple criteria knapsack problem. Naval Research Logistics, 47(1):57–76,
2000.
9. Matthias M¨uller-Hannemann and Karsten Weihe. Pareto shortest paths is often
feasible in practice. In Proceedings of the 5th International Workshop on Algorithm
Engineering (WAE), pages 185–198, 2001.
10. Adli Mustafa and Mark Goh. Finding integer eﬃcient solutions for bicriteria and
tricriteria network ﬂow problems using dinas. Computers & OR, 25(2):139–157,
1998.
11. George L. Nemhauser and Zev Ullmann. Discrete dynamic programming and cap-
ital allocation. Management Science, 15:494–505, 1969.
12. Christos H. Papadimitriou and Mihalis Yannakakis. On the approximability of
trade-oﬀs and optimal access of web sources.
In Proceedings of the 41st An-
nual Symposium on Foundations of Computer Science (FOCS), pages 86–92. IEEE
Computer Society, 2000.
13. Heiko R¨oglin and Berthold V¨ocking. Smoothed analysis of integer programming.
In Proceedings of the 11th International Conference on Integer Programming and
Combinatorial Optimization (IPCO), volume 3509 of Lecture Notes in Computer
Science, pages 276–290. Springer, 2005.
14. Anders J. V. Skriver and Kim Allan Andersen. A label correcting approach for
solving bicriterion shortest-path problems. Computers & OR, 27(6):507–524, 2000.
15. Daniel A. Spielman and Shang-Hua Teng.
Smoothed analysis of algorithms:
Why the simplex algorithm usually takes polynomial time. Journal of the ACM,
51(3):385–463, 2004.
16. Sergei Vassilvitskii and Mihalis Yannakakis. Eﬃciently computing succinct trade-
oﬀcurves. Theoretical Computer Science, 348(2-3):334–356, 2005.

Finding a Polytope from Its Graph in
Polynomial Time
Eric J. Friedman
School of Operations Research and Information Engineering, Cornell University
ejf27@cornell.edu
http://www.people.cornell.edu/pages/ejf27/
Abstract. We show that one can compute a (simple) polytope from its
graph in Polynomial time. This computation of a polytope from its graph
was shown to be solvable by Blind and Mani and more recently Kalai
provided a simple proof that leads to an exponential time algorithm.
Our proof relies on a Primal-Dual characterization by Joswig, Kaibel
and Korner. We describe an exponential Linear Programming which can
be used to construct the solution and show that it can be solved in
polynomial time.
1
Introduction
In [1] Blind and Mani showed, using tools from homology theory, that one can
construct the entire face lattice of a (simple1) polytope from its graph. Then
in [7], Kalai presented an elementary proof of this result. Whereas Blind and
Mani’s result was essentially nonconstructive, Kalai’s result was constructive
but required exponential time (in the size of the graph).
More recently, Joswig, Kaibel and Korner [4] extended Kalai’s analysis to
provide polynomial certiﬁcates for this problem, based on a pair of combinato-
rial optimization problems that form a primal dual pair. However, they do not
provide polynomial algorithms for either of these problems and thus left open
the question of whether this problem can be solved in polynomial time.
In this paper, we present a polynomial time algorithm for computing the face
lattice of a polytope from its graph, resolving this question. We present a linear
program for computing the 2-faces of the polytope from its graph which can be
solved in polynomial time. As discussed in [5,6] this resolves the issue, as one
can compute the full face lattice from the set of 2 faces.
Our discussion in the remainder of the paper will be self contained, but terse.
For more details see the related papers [4,5] and the book [9].
2
2-Systems and Pseudo-polytopes
Let G = (V, E) be the graph of a simple (full dimensional) polytope, P, in ℜd,
where V is the set of vertices of the polytope and E are its edges.
1 Note that if the polytope is not simple then it is not uniquely deﬁned by its graph.
Thus, we will only consider simple polytopes.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 68–73, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

Finding a Polytope from its Graph in Polynomial Time
69
A 2-frame, centered at v, is a set of three distinct nodes, v, v′, v′′ such that
(v, v′) and (v, v′′) are both elements of E. A 2-system is a set of cycles in G such
that every 2-frame is contained in a unique cycle.
Let O be an acyclic orientation on G. Deﬁne H(O) to be the number of 2-
frames that are sinks under O, where the 2-frame (v, v′, v′′) is a sink if both
edges (v, v′) and (v, v′′) are oriented towards the center of the frame, v.
Our analysis will be based on the following (minor) extension of the main
result from [4]. Our modiﬁcation is that we require a speciﬁed vertex not be a
source.
Theorem 1 (Joswig, Korner and Kaibel). Let P be a simple d-polytope.
For every 2-system S of G, vertex v, and every acyclic orientation O of G, such
that no 2-frame centered at v is a 2-sink, the inequalities
|S| ≤|V2(P)| ≤H(O)
hold, where the ﬁrst inequality holds with equality if and only if S = V2(P) (the
set of 2 faces of P), and the second holds with equality if and only if O induces
precisely one sink on every 2-face of P.
Proof: Our proof is a slight modiﬁcation of that in [4], since we require that a
chosen vertex not be a source. First note that for an acyclic orientation that
every cycle must contain a 2-sink. Thus we must have that |S| ≤H(O). In
addition, since V2(P) is a 2-system this implies that |V2(P)| ≤H(O) which in
turn implies that |S| ≤|V2(P)|. The second inequality holds with equality when
O is an abstract objective function with v as a source. Such an AOF exists since
there exists a linear objective function on the polytope where v is the worst
vertex. That S = V2(P) when the ﬁrst holds with equality can be shown using
the same proof as in [4].
⊓⊔
Thus, if we can ﬁnd a 2-system S that maximizes |S| in polynomial time, then
we have found V2(p) and from that one can compute the full face lattice of P in
polynomial time. See [5,6] for details.
We use the above theorem to deﬁne a “pseudo-polytopal multi-graph” to be a
multi-graph G such that there exists a vertex v and “pseudo 2-face set”, V2(G)
such that Theorem 1 holds. Clearly the graph of a polytope is pseudo-polytopal;
however, as we now show, other multi-graphs (which do not arise from simple
polytopes) may also be pseudo-polytopal.
Given a graph G of a polytope P deﬁne the contraction of G by a 2-face f to
be a new multi-graph Cf(G), where all the nodes in f are contracted to a single
node, denoted v. Note that this is a multi-graph as there may be multiple edges
connecting v to an adjacent node. We consider each of these to be distinct and
may even have a 2-face on only 2 nodes.
Theorem 2. Let G be the graph of a simple polytope P and F be a 2-face of P.
Then G′ = Cf(G) is a pseudo-polytopal multi-graph.
Proof: The proof is identical to the proof of Theorem 1 where we choose V2(G′)
to be the V2(P) \ f and O to be the contraction of an AOF for G where all

70
E.J. Friedman
vertices on the face f are worse than all other vertices. To construct such an
AOF simply take the linear objective function with f as a level set and perturb
it slightly.
⊓⊔
In the following section, we will present a binary integer program with an expo-
nential number of variables for computing this 2-system. Somewhat surprisingly,
this can be solved in polynomial time.
3
Solving Via Linear Programming
Let T be the set of all 2-frames in G and t ∈T be the 2-frame (t0, t1, t2) centered
at t0. Let W be the set of all loops in G. Then to compute V2(P) we need to
solve:
max

w∈W
xw
(IP −S)
s.t.
∀t ∈T :

w∋t
xw = 1
xw ∈{0, 1}
where we write w ∋t as a shorthand for the 2-frames t contained in w. First we
consider the following relaxation of this integer program.
max

w∈W
xw
(LP-S)
s.t.
∀t ∈T :

w∋t
xw ≤0
xw ≥0
Next, we consider the dual of this LP:
min

t∈T
vt
(LP-SD)
s.t.
∀w ∈W :

t∈w
vt ≥1
vt ≥0
Let IP-SD be the related binary integer program for LP-SD, i.e., replace 0 ≤vt
with vt ∈{0, 1}. Now, consider an acyclic orientation, O of G and let vt = 1
represent the case when the 2-frame t is a 2-sink. Then the integer program
for minimizing H(O) over all acyclic orientations can be written by adding the
constraint that v must arise from an acyclic orientation on G, to IP-SD.

Finding a Polytope from its Graph in Polynomial Time
71
min

t∈T
vt
(IP-H)
s.t.
∀w ∈W :

t∈w
vt ≥1
vt ≥0
vt arises from an acyclic orientation of G
This sequence of optimization problems allows us to present our ﬁrst result:
Theorem 3. Let P be a simple d-polytope with graph G. Then the following
optimization problems for G all have the same optimal value: IP-S, LP-S, LP-
SD, IP-SD and IP-H.
Proof: Let Opt(problem) be the optimal objective value for the optimization prob-
lem, “problem”. Then it is easy to see that Opt(IP −S) ≤Opt(LP −S) and
Opt(LP −SD) ≤Opt(IP −SD) ≤Opt(IP −H) as these are sequences of relax-
ations. By strong duality, we have Opt(LP −S) = Opt(LP −SD). Now, Theorem 1
completes the proof since it implies that Opt(IP −S) = Opt(IP −H).
⊓⊔
4
Solution and Integrality of the Linear Program
To complete our analysis we show that LP-SD can be solved in polynomial time
and that its solution is actually a solution to IP-S, yielding V2(P).
Note that even though LP-SD has an exponential number of constraints it can
be solved in polynomial time by the ellipsoid method if there exists a polynomial
separation algorithm [3]. That is, an algorithm which given a vector v can check
whether v is feasible and if not, ﬁnd a constraint violated by v. In our case such
a constraint is a cycle w ∈W such that 
t∈w wt < 1. This can be solved easily
in polynomial time via a graphical algorithm.
For example, one can search node by node for a loop starting at that node
that violates the constraint. This can be done by ﬁnding a shortest path from
that node to a copy of itself on a modiﬁed version of G where the speciﬁed node
is doubled and the graph is directed to force any such path to be a cycle.
To complete the analysis one must guarantee that the the solution of the LP
is binary. Note that the optimal solution of IP-S is unique, since there is only
one true set of 2-faces for a polytope. So it suﬃces to show that the extreme
point solution of LP-S is unique.
Theorem 4. LP-S has a unique optimal solution.
Proof: Suppose that LP-S has the binary optimal solution x∗and second extreme
point x′. Then there must exist some w ∈W such that x∗
w = 1 and x′
w = 0,
otherwise (1 + ϵ)x′ −ϵx∗would also be an optimal solution, for small enough
ϵ > 0, implying that x′
w is not an extreme point.

72
E.J. Friedman
Let f ∈F denote the face implied by x∗
w and contract the graph G by f,
denoting this node by f and the contracted graph by G′ = Cf(G).
Now consider IP-S on this graph where we drop the constraints for 2-frames
centered at f but require all the remaining ones. Since G′ is psuedo-polytopal
our previous argument holds for the string of optimization problems induced
by G′. In particular, the solution of IP-S must have objective value equal to
|V2(P)| −1; however the projection of x′ is feasible for LP-S but has a greater
objective value (|V2(P)|), providing a contradiction and proving the theorem. ⊓⊔
Thus, we can ﬁnd an integral solution of LP-S and hence a solution of IP-S, in
polynomial time, and can ﬁnd the full face lattice in polynomial time.
5
Discussion
Our analysis shows that one can compute the face lattice of a polytope from its
graph in polynomial time and suggests that similar techniques might be useful
for ﬁnding abstract objective functions in polynomial time, an interesting open
problem.
One interesting question is whether one can tell whether a graph is polytopal,
i.e., arising from a polytope. One way to show that a graph is not polytopal
would be to show that the linear program (LP-S) does not have an integral
solution or the related existence of a duality gap. However, the existence of
psuedo-polytopal graphs shows that this would not be suﬃcient.
Lastly, we note that our analysis appears to have connections to recent work
on unique sink orientations [8,2], as our intermediate integer program (IP-SD) is
essentially solving for a unique sink orientation. Thus, minimizing over unique
sink orientations is equivalent to minimizing over abstract objective functions.
We conjecture that using this equivalence, one could provide an optimization
based characterization of unique sink orientations for general polytopes analo-
gous to our deﬁnition of pseudo-polytopal graphs.
Acknowledgements
I’d like to thank Adrian Lewis, David Shmoys and Mike Todd for helpful con-
versations. This research has been supported in part by the NSF under grant
ITR-0325453.
References
1. R. Blind and P. Mani-Levitska. On puzzles and polytope isomorphisms. Aequationes
Math., 34:287297, 1987.
2. B. G¨artner, W. D. Morris, Jr., and L. R¨ust.
Unique sink orientations of grids.
In Proc. 11th Conference on Integer Programming and Combinatorial Optimization
(IPCO), volume 3509 of Lecture Notes in Computer Science, pages 210–224, 2005.

Finding a Polytope from its Graph in Polynomial Time
73
3. M. Gr¨otschel, L. Lov´asz, and A. Schrijver. Geometric Algorithms and Combinatorial
Optimization. Springer-Verlag, Berlin, 1988.
4. M. Joswig, F. Korner, and V. Kaibel. On the k-systems of a simple polytope. Isr.
J. Math., 129:109–118, 2002.
5. V. Kaibel.
Reconstructing a simple polytope from its graph.
In M. Junger,
G. Reinelt, and G. Rinaldi, editors, Combinatorial Optimization – Eureka, You
Shrink!, pages 105–118. Springer, New York, 2002.
6. V. Kaibel and M. Pfetsch. Computing the face lattice of a polytope from its vertex-
facet incidences. Comput. Geom., 23:281–290, 2002.
7. G. Kalai. A simple way to tell a simple polytope from its graph. J. Comb. Theory,
Ser. A, 49:381–383, 1988.
8. Tibor Szab´o and E. Welzl. Unique sink orientations of cubes. In Proc. 42nd Ann.
IEEE Symp. on Foundations of Computer Science (FOCS), pages 547–555, New
York, 2001.
9. G. M. Ziegler. Lectures on Polytopes. Springer Verlag, New York, 1998.

Orbitopal Fixing⋆
Volker Kaibel1,⋆⋆, Matthias Peinhardt1, and Marc E. Pfetsch2
1 Otto-von-Guericke Universität Magdeburg, Fakultät für Mathematik,
Universitätsplatz 2, 39106 Magdeburg, Germany
{kaibel,peinhard}@ovgu.de
2 Zuse Institute Berlin, Takustr. 7, 14195 Berlin, Germany
pfetsch@zib.de
Abstract. The topic of this paper are integer programming models in
which a subset of 0/1-variables encode a partitioning of a set of objects
into disjoint subsets. Such models can be surprisingly hard to solve by
branch-and-cut algorithms if the order of the subsets of the partition is
irrelevant. This kind of symmetry unnecessarily blows up the branch-
and-cut tree.
We present a general tool, called orbitopal ﬁxing, for enhancing the
capabilities of branch-and-cut algorithms in solving such symmetric inte-
ger programming models. We devise a linear time algorithm that, applied
at each node of the branch-and-cut tree, removes redundant parts of the
tree produced by the above mentioned symmetry. The method relies on
certain polyhedra, called orbitopes, which have been investigated in [11].
It does, however, not add inequalities to the model, and thus, it does not
increase the diﬃculty of solving the linear programming relaxations. We
demonstrate the computational power of orbitopal ﬁxing at the example
of a graph partitioning problem motivated from frequency planning in
mobile telecommunication networks.
1
Introduction
Being welcome in most other contexts, symmetry causes severe trouble in the so-
lution of many integer programming (IP) models. This paper describes a method
to enhance the capabilities of branch-and-cut algorithms with respect to hand-
ling symmetric models of a certain kind that frequently occurs in practice.
We illustrate this kind of symmetry by the example of a graph partitioning
problem (another notorious example is the vertex coloring problem). Here, one
is given a graph G = (V, E) with nonnegative edge weights w ∈
 E
≥0 and an
integer q ≥2. The task is to partition V into q disjoint subsets such that the
sum of all weights of edges connecting nodes in the same subset is minimized.
A straight-forward IP model arises by introducing 0/1-variables xij for all
i ∈[p] := {1, . . . , p} and j ∈[q] that indicate whether node i is contained in
⋆Supported by the DFG Research Center Matheon Mathematics for key technologies
in Berlin.
⋆⋆During the research of this work the ﬁrst author was a visiting professor at Technische
Universität Berlin.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 74–88, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

Orbitopal Fixing
75
subset j (where we assume V = [p]). In order to model the objective function,
we furthermore need 0/1-variables yik for all edges {i, k} ∈E indicating whether
nodes i and k are contained in the same subset. This yields the following model
(see, e.g., [5]):
min

{i,k}∈E
wik yik
s.t.
q

j=1
xij = 1
for all i ∈[p]
xij + xkj −yik ≤1
for all {i, k} ∈E, j ∈[q]
xij ∈{0, 1}
for all i ∈[p], j ∈[q]
yik ∈{0, 1}
for all {i, k} ∈E.
(1)
The x-variables describe a 0/1-matrix of size p × q with exactly one 1-entry
per row. They encode the assignment of the nodes to the subsets of the partition.
The methods that we discuss in this paper do only rely on this structure and thus
can be applied to many other models as well. We use the example of the graph
partitioning problem as a prototype application and report on computational
experiments in Sect. 5. Graph partitioning problems are discussed in [3, 4, 5],
for instance as a relaxation of frequency assignment problems in mobile telecom-
munication networks. The maximization version is relevant as well [6, 12]. Also
capacity bounds on the subsets of the partition (which can easily be incorporated
into the model) are of interest, in particular the graph equipartitioning problem
[7, 8, 18, 19]. For the closely related clique partitioning problem, see [9, 10].
As it is given above, the model is unnecessarily diﬃcult for state-of-the-art IP
solvers. Even solving small instances requires enormous eﬀorts (see Sect. 5). One
reason is that every feasible solution (x, y) to this model can be turned into q!
diﬀerent ones by permuting the columns of x (viewed as a 0/1-matrix) in an
arbitrary way, thereby not changing the structure of the solution (in particular:
its objective function value). Phrased diﬀerently, the symmetric group of all
permutations of the set [q] operates on the solutions by permuting the columns of
the x-variables in such a way that the objective function remains constant along
each orbit. Therefore, when solving the model by a branch-and-cut algorithm,
basically the same work will be done in the tree at many places. Thus, there
should be potential for reducing the running times signiﬁcantly by exploiting
the symmetry. A more subtle second point is that interior points of the convex
hulls of the individual orbits are responsible for quite weak linear programming
(LP) bounds. We will, however, not address this second point in this paper.
In order to remove symmetry, the above model for the graph partitioning
problem is often replaced by models containing only edge variables, see, e.g. [7].
However, for this to work the underlying graph has to be complete, which might
introduce many unnecessary variables. Moreover, formulation (1) is sometimes
favorable, e.g., if node-weighted capacity constraints should be incorporated.
One way to deal with symmetry is to restrict the feasible region in each
of the orbits to a single representative, e.g., to the lexicographically maximal

76
V. Kaibel, M. Peinhardt, and M.E. Pfetsch
(with respect to the row-by-row ordering of the x-components) element in the
orbit. In fact, this can be done by adding inequalities to the model that enforce
the columns of x to be sorted in a lexicographically decreasing way. This can
be achieved by O(pq) many column inequalities. In [11] even a complete (and
irredundant) linear description of the convex hull of all 0/1-matrices of size p×q
with exactly one 1-entry per row and lexicographically decreasing columns is
derived; these polytopes are called orbitope. The description basically consists of
an exponentially large superclass of the column inequalities, called shifted column
inequalities, for which there is a linear time separation algorithm available. We
recall some of these results in Sect. 2.
Incorporating the inequalities from the orbitope description into the IP model
removes symmetry. At each node of the branch-and-cut tree this ensures that the
corresponding IP is infeasible as soon as there is no representative in the subtree
rooted at that node. In fact, already the column inequalities are suﬃcient for
this purpose.
In this paper, we investigate a way to utilize these inequalities (or the or-
bitope that they describe) without adding any of the inequalities to the models
explicitly. The reason for doing this is the unpleasant eﬀect that adding (shifted)
column inequalities to the models results in more diﬃcult LP relaxations. One
way of avoiding the addition of these inequalities to the LPs is to derive logical
implications instead: If we are working in a branch-and-cut node at which the
x-variables corresponding to index subsets I0 and I1 are ﬁxed to zero and one,
respectively, then there might be a (shifted) column inequality yielding impli-
cations for all representatives in the subtree rooted at the current node. For
instance, it might be that for some (i, j) ̸∈I0 ∪I1 we have xij = 0 for all feasi-
ble solutions in the subtree. In this case, xij can be ﬁxed to zero for the whole
subtree rooted at the current node, enlarging I0. We call the iterated process
of searching for such additional ﬁxings sequential ﬁxing with (shifted) column
inequalities.
Let us mention at this point that deviating from parts of the literature, we
do not distinguish between “ﬁxing“ and “setting“ of variables in this paper.
Sequential ﬁxing with (shifted) column inequalities is a special case of con-
straint propagation, which is well known from constraint logic programming.
Modern IP solvers like SCIP [1] use such strategies also in branch-and-cut algo-
rithms. With orbitopes, however, we can aim at something better: Consider a
branch-and-cut node identiﬁed by ﬁxing the variables corresponding to sets I0
and I1 to zero and one, respectively. Denote by W(I0, I1) the set of all vertices x
of the orbitope with xij = 0 for all (i, j) ∈I0 and xij = 1 for all (i, j) ∈I1.
Deﬁne the sets I⋆
0 and I⋆
1 of indices of all variables, for which no x in W(I0, I1)
satisﬁes xij = 1 for some (i, j) ∈I⋆
0 or xij = 0 for some (i, j) ∈I⋆
1 . Fixing of
the corresponding variables is called simultaneous ﬁxing at the branch-and-cut
node. Simultaneous ﬁxing is always at least as strong as sequential ﬁxing.
Investigations of sequential and simultaneous ﬁxing for orbitopes are the cen-
tral topic of the paper. The main contributions and results are the following:

Orbitopal Fixing
77
◦We present a linear time algorithm for orbitopal ﬁxing, i.e., for solving the
problem to compute simultaneous ﬁxings for orbitopes (Theorem 4).
◦We show that, for general 0/1-polytopes, sequential ﬁxing, even with com-
plete and irredundant linear descriptions, is weaker than simultaneous ﬁx-
ing (Theorem 2), We clarify the relationships between diﬀerent versions of
sequential ﬁxing with (shifted) column inequalities, where (despite the situ-
ation for general 0/1-polytopes) the strongest one is as strong as orbitopal
ﬁxing (Theorem 3).
◦We report on computer experiments (Sect. 5) with the graph partitioning
problem described above, showing that orbitopal ﬁxing leads to signiﬁcant
performance improvements for branch-and-cut algorithms.
Margot [14, 15, 17] considers a related method for symmetry handling. His ap-
proach works for more general types of symmetries than ours. Similarly to our
approach, the basic idea is to assure that only (partial) solutions which are lexi-
cographical maximal in their orbit are explored in the branch-and-cut tree. This
is guaranteed by an appropriate ﬁxing rule. The ﬁxing and pruning decisions
are done by means of a Schreier-Sims table for representing the group action.
While Margot’s approach is much more generally applicable than orbitopal ﬁx-
ing, the latter seems to be more powerful in the special situation of partitioning
type symmetries. One reason is that Margot’s method requires to choose the
branching variables according to an ordering that is chosen globally for the en-
tire branch-and-cut tree.
Another approach has recently been proposed by Linderoth et al. [13] (in this
volume). They exploit the symmetry arising in each node of a branch-and-bound
tree when all ﬁxed variables are removed from the model. Thus one may ﬁnd
additional local symmetries. Nevertheless, for partitioning type symmetries one
still may miss some part of the (ﬁxed) global symmetry we are dealing with.
We will elaborate on the relations between orbitopal ﬁxing, isomorphism prun-
ing, and orbital branching in more detail in a journal version of the paper.
2
Orbitopes
Throughout the paper, let p and q be integers with p ≥q ≥2. The orbitope O=
p,q
is the convex hull of all 0/1-matrices x ∈{0, 1}[p]×[q] with exactly one 1-entry
per row, whose columns are in decreasing lexicographical order (i.e., they satisfy
p
i=1 2p−ixij > p
i=1 2p−ixi,j+1 for all j ∈[q −1]). Let the symmetric group of
size q act on {0, 1}[p]×[q] via permuting the columns. Then the vertices of O=
p,q
are exactly the lexicographically maximal matrices (with respect to the row-by-
row ordering of the components) in those orbits whose elements are matrices
with exactly one 1-entry per row. As these vertices have xij = 0 for all (i, j)
with i < j, we drop these components and consider O=
p,q as a subset of the space
Ip,q with Ip,q := {(i, j) ∈{0, 1}[p]×[q] : i ≥j}. Thus, we consider matrices, in
which the i-th row has q(i) := min{i, q} components.

78
V. Kaibel, M. Peinhardt, and M.E. Pfetsch
i
j
η
(a)
i
j
(b)
i
j
(c)
i
j
(d)
Fig. 1. (a) Example for coordinates (9, 5) = ⟨5, 5⟩. (b), (c), (d) Three shifted column
inequalities, the left one of which is a column inequality.
In [11], in the context of more general orbitopes, O=
p,q is referred to as the
partitioning orbitope with respect to the symmetric group. As we will conﬁne
ourselves with this one type of orbitopes in this paper, we will simply call it
orbitope.
The main result in [11] is a complete linear description of O=
p,q. In order to
describe the result, it will be convenient to address the elements in Ip,q via a
diﬀerent “system of coordinates”: For j ∈[q] and 1 ≤η ≤p −j + 1, deﬁne
⟨η, j⟩:= (j + η −1, j). Thus (as before) i and j denote the row and the column,
respectively, while η is the index of the diagonal (counted from above) containing
the respective element; see Figure 1 (a) for an example.
A set S = {⟨1, c1⟩, ⟨2, c2⟩, . . . , ⟨η, cη⟩} ⊂Ip,q with c1 ≤c2 ≤· · · ≤cη and
η ≥1 is called a shifted column. For (i, j) = ⟨η, j⟩∈Ip,q, a shifted column S as
above with cη < j, and B = {(i, j), (i, j+1), . . ., (i, q(i))}, we call x(B)−x(S) ≤0
a shifted column inequality. The set B is called its bar. In case of c1 = · · · = cη =
j −1 the shifted column inequality is called a column inequality. See Figure 1
for examples.
Finally, a bit more notation is needed. For each i ∈[p], we deﬁne rowi :=
{(i, j) : j ∈[q(i)]}. For A ⊂Ip,q and x ∈
Ip,q, we denote by x(A) the sum

(i,j)∈A xij.
Theorem 1 (see [11]). The orbitope O=
p,q is completely described by the non-
negativity constraints xij ≥0, the row-sum equations x(rowi) = 1, and the
shifted column inequalities.
In fact, in [11] it is also shown that, up to a few exceptions, the inequalities
in this description deﬁne facets of O=
p,q. Furthermore, a linear time separation
algorithm for the exponentially large class of shifted column inequalities is given.
3
The Geometry of Fixing Variables
In this section, we deal with general 0/1-integer programs and, in particular,
their associated polytopes. We will deﬁne some basic terminology used later in
the special treatment of orbitopes, and we are going to shed some light on the
geometric situation of ﬁxing variables.

Orbitopal Fixing
79
We denote by [d] the set of indices of variables, and by Cd = {x ∈
d :
0 ≤xi ≤1 for all i ∈[d]} the corresponding 0/1-cube. For two disjoint subsets
I0, I1 ⊆[d] (with I0 ∩I1 = ∅) we call
{x ∈Cd : xi = 0 for all i ∈I0, xi = 1 for all i ∈I1}
the face of Cd deﬁned by (I0, I1). All nonempty faces of Cd are of this type.
For a polytope P ⊆Cd and for a face F of Cd deﬁned by (I0, I1), we denote
by FixF (P) the smallest face of Cd that contains P ∩F ∩{0, 1}d (i.e., FixF (P)
is the intersection of all faces of Cd that contain P ∩F ∩{0, 1}d). If FixF (P) is
the nonempty cube face deﬁned by (I⋆
0 , I⋆
1), then I⋆
0 and I⋆
1 consist of all i ∈[d]
for which xi = 0 and xi = 1, respectively, holds for all x ∈P ∩F ∩{0, 1}d. In
particular, we have I0 ⊆I⋆
0 and I1 ⊆I⋆
1, or FixF (P) = ∅. Thus, if I0 and I1
are the indices of the variables ﬁxed to zero and one, respectively, in the current
branch-and-cut node (with respect to an IP with feasible points P ∩{0, 1}d),
the node can either be pruned, or the sets I⋆
0 and I⋆
1 yield the maximal sets of
variables that can be ﬁxed to zero and one, respectively, for the whole subtree
rooted at this node. Unless FixF (P) = ∅, we call (I⋆
0 , I⋆
1) the ﬁxing of P at
(I0, I1). Similarly, we call FixF (P) the ﬁxing of P at F.
Remark 1. If P, P ′ ⊆Cd are two polytopes with P ⊆P ′ and F and F ′ are two
faces of Cd with F ⊆F ′, then FixF (P) ⊆FixF ′(P ′) holds.
In general, it is not clear how to compute ﬁxings eﬃciently. Indeed, computing
the ﬁxing of P at (∅, ∅) includes deciding whether P ∩{0, 1}d = ∅, which, of
course, is NP-hard in general. Instead, one can try to derive as large as possible
subsets of I⋆
0 and I⋆
1 by looking at relaxations of P. In case of an IP that is
based on an intersection with an orbitope, one might use the orbitope as such a
relaxation. We will deal with the ﬁxing problem for orbitopes in Sect. 4.
If P is given via an inequality description, one possibility is to use the knapsack
relaxations obtained from single inequalities out of the description. For each of
these relaxations, the ﬁxing can easily be computed. If the inequality system
describing P is exponentially large, and the inequalities are only accessible via
a separation routine, it might still be possible to decide eﬃciently whether any
of the exponentially many knapsack relaxations allows to ﬁx some variable (see
Sect. 4.2).
Suppose, P = {x ∈Cd : Ax ≤b} and Pr = {x ∈Cd : aT
r x ≤br} is the
knapsack relaxation of P for the rth-row aT
r x ≤br of Ax ≤b, where r = 1, . . . , m.
Let F be some face of Cd. The face G of Cd obtained by setting G := F and
then iteratively replacing G by FixG(Pr) as long as there is some r ∈[m] with
FixG(Pr) ⊊G, is denoted by FixF (Ax ≤b). Note that the outcome of this
procedure is independent of the choices made for r, due to Remark 1. We call
the pair (˜I0, ˜I1) deﬁning the cube face FixF (Ax ≤b) (unless this face is empty)
the sequential ﬁxing of Ax ≤b at (I0, I1). In the context of sequential ﬁxing we
often refer to (the computation of) FixF (P) as simultaneous ﬁxing.
Due to Remark 1 it is clear that FixF (P) ⊆FixF (Ax ≤b) holds.

80
V. Kaibel, M. Peinhardt, and M.E. Pfetsch
Theorem 2. In general, even for a system of facet-deﬁning inequalities describ-
ing a full-dimensional 0/1-polytope, sequential ﬁxing is weaker than simultaneous
ﬁxing.
Proof. The following example shows this. Let P ⊂C4 be the four-dimensional
polytope deﬁned by the trivial inequalities xi ≥0 for i ∈{1, 2, 3}, xi ≤1 for
i ∈{1, 2, 4}, the inequality −x1+x2+x3−x4 ≤0 and x1−x2+x3−x4 ≤0. Let F
be the cube face deﬁned by ({4}, ∅). Then, sequential ﬁxing does not ﬁx any
further variable, although simultaneous ﬁxing yields I⋆
0 = {3, 4} (and I⋆
1 = ∅).
Note that P has only 0/1-vertices, and all inequalities are facet deﬁning (x4 ≥0
and x3 ≤1 are implied).
⊓⊔
4
Fixing Variables for Orbitopes
For this section, suppose that I0, I1 ⊆Ip,q are subsets of indices of orbitope
variables with the following properties:
(P1) |I0 ∩rowi| ≤q(i) −1 for all i ∈[p]
(P2) For all (i, j) ∈I1, we have (i, ℓ) ∈I0 for all ℓ∈[q(i)] \ {j}.
In particular, P1 and P2 imply that I0 ∩I1 = ∅. Let F be the face of the 0/1-
cube CIp,q deﬁned by (I0, I1). Note that if P1 is not fulﬁlled, then O=
p,q ∩F = ∅.
The following statement follows immediately from Property P2.
Remark 2. If a vertex x of O=
p,q satisﬁes xij = 0 for all (i, j) ∈I0, then x ∈F.
We assume that the face FixF (O=
p,q) is deﬁned by (I⋆
0, I⋆
1 ), if FixF (O=
p,q) is
not empty. Orbitopal ﬁxing is the problem to compute the simultaneous ﬁxing
(I⋆
0, I⋆
1) from (I0, I1), or determine that FixF (O=
p,q) = ∅.
Remark 3. If FixF (O=
p,q) ̸= ∅, it is enough to determine I⋆
0, as we have (i, j) ∈I⋆
1
if and only if (i, ℓ) ∈I⋆
0 holds for for all ℓ∈[q(i)] \ {j}.
4.1
Intersection of Orbitopes with Cube Faces
We start by deriving some structural results on orbitopes that are crucial in
our context. Since O=
p,q ⊂CIp,q is a 0/1-polytope (i.e., it is integral), we have
conv(O=
p,q ∩F ∩{0, 1}Ip,q) = O=
p,q ∩F. Thus, FixF (O=
p,q) is the smallest cube face
that contains the face O=
p,q ∩F of the orbitope O=
p,q.
Let us, for i ∈[p], deﬁne values αi := αi(I0) ∈[q(i)] recursively by setting
α1 := 1 and, for all i ∈[p] with i ≥2,
αi :=

αi−1
if αi−1 = q(i) or (i, αi−1 + 1) ∈I0
αi−1 + 1
otherwise.
The set of all indices of rows, in which the α-value increases, is denoted by
Γ(I0) := {i ∈[p] : i ≥2, αi = αi−1 + 1} ∪{1}
(where, for technical reasons 1 is included).
The following observation follows readily from the deﬁnitions.

Orbitopal Fixing
81
Remark 4. For each i ∈[p] with i ≥2 and αi(I0) < q(i), the set Si(I0)
:=
{(k, αk(I0) + 1) : k ∈[i] \ Γ(I0)} is a shifted column with Si(I0) ⊆I0.
Lemma 1. For each i ∈[p], no vertex of O=
p,q ∩F has its 1-entry in row i in a
column j ∈[q(i)] with j > αi(I0).
Proof. Let i ∈[p]. We may assume αi(I0) < q(i), because otherwise the state-
ment trivially is true. Thus, B := {(i, j) ∈rowi : j > αi(I0)} ̸= ∅.
Let us ﬁrst consider the case i ∈Γ(I0). As we have αi(I0) < q(i) ≤i and
α1(I0) = 1, there must be some k < i such that k ̸∈Γ(I0). Let k be maximal
with this property. Thus we have k′ ∈Γ(I0) for all 1 < k < k′ ≤i. According to
Remark 4, x(B)−x(Sk(I0)) ≤0 is a shifted column inequality with x(Sk(I0)) =
0, showing x(B) = 0 as claimed in the lemma.
Thus, let us suppose i ∈[p] \ Γ(I0). If αi(I0) ≥q(i) −1, the claim holds
trivially. Otherwise, B′ := B \ {(i, αi(I0) + 1)} ̸= ∅. Similarly to the ﬁrst case,
now the shifted column inequality x(B′) −x(Si−1(I0)) ≤0 proves the claim.
⊓⊔
For each i ∈[p] we deﬁne μi(I0) := min{j ∈[q(i)] : (i, j) ̸∈I0}. Because of
Property P1, the sets over which we take minima here are non-empty.
Lemma 2. If we have μi(I0) ≤αi(I0) for all i ∈[p], then the point x⋆=
x⋆(I0) ∈{0, 1}Ip,q with x⋆
i,αi(I0) = 1 for all i ∈Γ(I0) and x⋆
i,μi(I0) = 1 for all i ∈
[p] \ Γ(I0) and all other components being zero, is contained in O=
p,q ∩F.
Proof. Due to αi(I0) ≤αi−1(I0) + 1 for all i ∈[p] with i ≥2, the point x⋆is
contained in O=
p,q. It follows from the deﬁnitions that x⋆does not have a 1-entry
at a position in I0. Thus, by Remark 2, we have x⋆∈F.
⊓⊔
We now characterize the case O=
p,q ∩F = ∅(leading to pruning the corresponding
node in the branch-and-cut tree) and describe the set I⋆
0.
Proposition 1.
1. We have O=
p,q ∩F = ∅if and only if there exists i ∈[p] with μi(I0) > αi(I0).
2. If μi(I0) ≤αi(I0) holds for all i ∈[p], then the following is true.
(a) For all i ∈[p] \ Γ(I0), we have
I⋆
0 ∩rowi = {(i, j) ∈rowi : (i, j) ∈I0 or j > αi(I0)}.
(b) For all i ∈[p] with μi(I0) = αi(I0), we have
I⋆
0 ∩rowi = rowi \{(i, αi(I0))}.
(c) For all s ∈Γ(I0) with μs(I0) < αs(I0) the following holds: If there is
some i ≥s with μi(I0) > αi(I0 ∪{(s, αs(I0))}), then we have
I⋆
0 ∩rows = rows \{(s, αs(I0))}.
Otherwise, we have
I⋆
0 ∩rows = {(s, j) ∈rows : (s, j) ∈I0 or j > αs(I0)}.

82
V. Kaibel, M. Peinhardt, and M.E. Pfetsch
0
0
0
0
0
0
1
1
(a)
0
0
0
0
0
1
1
1
(b)
0
0
0
0
1
1
(c)
0
0
0
1
(d)
Fig. 2. (a): Example for Prop. 1 (1). Light-gray entries indicate the entries (i, μi(I0))
and dark-gray entries indicate entries (i, αi(I0)). (b): Example of ﬁxing an entry to 1 for
Prop. 1 (2c). As before light-gray entries indicate entries (i, μi(I0)). Dark-gray entries
indicate entries (i, αi(I0 ∪{(s, αs(I0))})) with s = 3. (c) and (d): Gray entries show
the SCIs used in the proofs of Parts 1(a) and 1(b) of Thm. 3, respectively.
Proof. Part 1 follows from Lemmas 1 and 2.
In order to prove Part 2, let us assume that μi(I0) ≤αi(I0) holds for all i ∈[p].
For Part 2a, let i ∈[p] \ Γ(I0) and (i, j) ∈rowi. Due to I0 ⊂I⋆
0 , we only have to
consider the case (i, j) ̸∈I0. If j > αi(I0), then, by Lemma 1, we ﬁnd (i, j) ∈I⋆
0.
Otherwise, the point that is obtained from x⋆(I0) (see Lemma 2) by moving the
1-entry in position (i, μi(I0)) to position (i, j) is contained in O=
p,q ∩F, proving
(i, j) ̸∈I⋆
0 .
In the situation of Part 2b, the claim follows from Lemma 1 and O=
p,q ∩F ̸= ∅
(due to Part 1).
For Part 2c, let s ∈Γ(I0) with μs(I0) < αs(I0) and deﬁne I′
0 := I0 ∪
{(s, αs(I0))}. It follows that we have μi(I′
0) = μi(I0) for all i ∈[p].
Let us ﬁrst consider the case that there is some i ≥s with μi(I0) > αi(I′
0).
Part 1 (applied to I′
0 instead of I0) implies that O=
p,q ∩F does not contain a
vertex x with xs,αs(I0) = 0. Therefore, we have (s, αs(I0)) ∈I⋆
1 , and thus I⋆
0 ∩
rows = rows \{(s, αs(I0))} holds (where for “⊆“ we exploit O=
p,q ∩F ̸= ∅by
Part 1, this time applied to I0).
The other case of Part 2c follows from s ̸∈Γ(I′
0) and αs(I′
0) = αs(I0) −1.
Thus, Part 2a applied to I′
0 and s instead of I0 and i, respectively, yields the
claim (because of (s, αs(I0)) ̸∈I⋆
0 due to s ∈Γ(I0) and O=
p,a ∩F ̸= ∅).
⊓⊔
4.2
Sequential Fixing for Orbitopes
Let us, for some ﬁxed p ≥q ≥2, denote by SSCI the system of the nonnegativity
inequalities, the row-sum equations (each one written as two inequalities, in
order to be formally correct) and all shifted column inequalities. Thus, according
to Theorem 1, O=
p,q is the set of all x ∈
Ip,q that satisfy SSCI. Let SCI be the
subsystem of SSCI containing only the column inequalities (and all nonnegativity
inequalities and row-sum equations).
At ﬁrst sight, it is not clear whether sequential ﬁxing with the exponentially
large system SSCI can be done eﬃciently. A closer look at the problem reveals,
however, that one can utilize the linear time separation algorithm for shifted

Orbitopal Fixing
83
column inequalities (mentioned in Sect. 2) in order to devise an algorithm for
this sequential ﬁxing, whose running time is bounded by O(ϱpq), where ϱ is the
number of variables that are ﬁxed by the procedure.
In fact, one can achieve more: One can compute sequential ﬁxings with respect
to the aﬃne hull of the orbitope. In order to explain this, consider a polytope
P = {x ∈Cd : Ax ≤b}, and let S ⊆
d be some aﬃne subspace containing P.
As before, we denote the knapsack relaxations of P obtained from Ax ≤b by P1,
. . . , Pm. Let us deﬁne FixS
F (Pr) as the smallest cube-face that contains Pr ∩S ∩
{0, 1}d∩F. Similarly to the deﬁnition of FixF (Ax ≤b), denote by FixS
F (Ax ≤b)
the face of Cd that is obtained by setting G := F and then iteratively replacing
G by FixS
G(Pr) as long as there is some r ∈[m] with FixS
G(Pr) ⊊G. We call
FixS
F (Ax ≤b) the sequential ﬁxing of Ax ≤b at F relative to S. Obviously, we
have FixF (P) ⊆FixS
F (Ax ≤b) ⊆FixF (Ax ≤b). In contrast to sequential ﬁxing,
sequential ﬁxing relative to aﬃne subspaces in general is NP-hard (as it can be
used to decide whether a linear equation has a 0/1-solution).
Theorem 3.
1. There are cube-faces F 1, F 2, F 3 with the following properties:
(a) FixF 1(SSCI) ⊊FixF 1(SCI)
(b) Fix
aﬀ(O=
p,q)
F 2
(SCI) ⊊FixF 2(SSCI)
(c) Fix
aﬀ(O=
p,q)
F 3
(SSCI) ⊊Fix
aﬀ(O=
p,q)
F 3
(SCI)
2. For all cube-faces F, we have Fix
aﬀ(O=
p,q)
F
(SSCI) = FixF (O=
p,q).
Proof. For Part 1(a), we chose p = 5, q = 4, and deﬁne the cube-face F1 via
I1
0 = {(3, 2), (5, 1), (5, 2), (5, 3)} and I1
1 = {(1, 1), (5, 4)}. The shifted column
inequality with shifted column {(2, 2), (3, 2)} and bar {(5, 4)} allows to ﬁx x22 to
one (see Fig. 2 (c)), while no column inequality (and no nonnegativity constraint
and no row-sum equation) allows to ﬁx any variable.
For Part 1(b), let p = 4, q = 4, and deﬁne F 2 via I2
0 = {(3, 2), (4, 1), (4, 2)}
and I2
1 = {(1, 1)}. Exploiting that x43 + x44 = 1 for all x ∈aﬀ(O=
p,q) ∩F 2, we
can use the column inequality with column {(2, 2), (3, 2)} and bar {(4, 3), (4, 4)}
to ﬁx x22 to one (see Fig. 2 (d)), while no ﬁxing is possible with SSCI only.
For Part 1(c), we can use F 3 = F 1. The proof of Part 2 is omitted here.
⊓⊔
The diﬀerent versions of sequential ﬁxing for partitioning orbitopes are dom-
inated by each other in the following sequence: SCI →{SSCI, aﬃne SCI} →
aﬃne SSCI, which ﬁnally is as strong as orbitopal ﬁxing. For each of the ar-
rows there exists an instance for which dominance is strict. The examples in the
proof of Theorem 3 also show that there is no general relation between SSCI and
aﬃne SCI.
In particular, we could compute orbitopal ﬁxings by the polynomial time
algorithm for sequential ﬁxing relative to aﬀ(O=
p,q). It turns out, however, that
this is not the preferable choice. In fact, we will describe below a linear time
algorithm for solving the orbitopal ﬁxing problem directly.

84
V. Kaibel, M. Peinhardt, and M.E. Pfetsch
Algorithm 1. Orbitopal Fixing
1: Set I⋆
0 ←I0, I⋆
1 ←I1, μ1 ←1, α1 ←1, and Γ = ∅.
2: for i = 2, . . . , p do
3:
compute μi ←min{j : (i, j) ̸∈I0}.
4:
if αi−1 = q(i) or (i, αi−1 + 1) ∈I0 then
5:
αi ←αi−1
6:
else
7:
αi ←αi−1 + 1, Γ ←Γ ∪{i}
8:
if μi > αi then
9:
return “Orbitopal ﬁxing is empty”
10:
Set I⋆
0 ←I⋆
0 ∪{(i, j) : j > αi}.
11:
if |I⋆
0 ∩rowi | = q(i) −1 then
12:
set I⋆
1 ←I⋆
1 ∪(rowi \I⋆
0 ).
13: for all s ∈Γ with (s, αs) /∈I⋆
1 do
14:
Set βs ←αs −1.
15:
for i = s + 1, . . . , p do
16:
if βi−1 = q(i) or (i, βi−1 + 1) ∈I0 then
17:
βi ←βi−1
18:
else
19:
βi ←βi−1 + 1
20:
if μi > βi then
21:
I⋆
1 ←I⋆
1 ∪{(s, αs)} and I⋆
0 ←rows \{(s, αs)}.
22:
Proceed with the next s in Step 13.
4.3
An Algorithm for Orbitopal Fixing
Algorithm 1 describes a method to compute the simultaneous ﬁxing (I⋆
0, I⋆
1) from
(I0, I1) (which are assumed to satisfy Properties P1 and P2). Note that we use βi
for αi(I0 ∪{(s, αs(I0))}).
Theorem 4. A slight modiﬁcation of Algorithm 1 solves the orbitopal ﬁxing
problem in time O(pq).
Proof. The correctness of the algorithm follows from the structural results given
in Proposition 1.
In order to prove the statement on the running time, let us assume that the data
structures for the sets I0, I1, I⋆
0 , and I⋆
1 allow both membership testing and addition
of single elements in constant time (e.g., the sets can be stored as bit vectors).
As none of the Steps 3 to 12 needs more time than O(q), we only have to
take care of the second part of the algorithm starting in Step 13. (In fact, used
verbatim as described above, the algorithm might need time Ω(p2).)
For s, s′ ∈Γ with s < s′ denote the corresponding β-values by βi (i ≥s) and
by β′
i (i ≥s′), respectively. We have βi ≤β′
i for all i ≥s′, and furthermore, if
equality holds for one of these i, we can deduce βk = β′
k for all k ≥i. Thus, as
soon as a pair (i, βi) is used a second time in Step 20, we can break the for-loop
in Step 15 and reuse the information that we have obtained earlier.
This can, for instance, be organized by introducing, for each (i, j) ∈Ip,q, a
ﬂag f(i, j) ∈{red, green, white} (initialized by white), where f(i, j) = red / green

Orbitopal Fixing
85
means that we have already detected that βi = j eventually leads to a posi-
tive/negative test in Step 20. The modiﬁcations that have to be applied to the
second part of the algorithm are the following: The selection of the elements
in Γ in Step 13 must be done in increasing order. Before performing the test
in Step 20, we have to check whether f(i, βi) is green. If this is true, then we
can proceed with the next s in Step 13, after setting all ﬂags f(k, βk) to green
for s ≤k < i. Similarly, we set all ﬂags f(k, βk) to red for s ≤k ≤i, before
switching to the next s in Step 22. And ﬁnally, we set all ﬂags f(k, βk) to green
for s ≤k ≤p at the end of the body of the s-loop starting in Step 13.
As the running time of this part of the algorithm is proportional to the number
of ﬂags changed from white to red or green, the total running time indeed is
bounded by O(pq) (since a ﬂag is never reset).
⊓⊔
5
Computational Experiments
We performed computational experiments for the graph partitioning problem
mentioned in the introduction. The code is based on the SCIP 0.90 framework
by Achterberg [1], and we use CPLEX 10.01 as the basic LP solver. The com-
putations were performed on a 3.2 GHz Pentium 4 machine with 2 GB of main
memory and 2 MB cache running Linux. All computation times are CPU sec-
onds and are subject to a time limit of four hours. Since in this paper we are
not interested in the performance of heuristics, we initialized all computations
with the optimal primal solution. We compare diﬀerent variants of the code by
counting winning instances. An instance is a winner for variant A compared to
variant B, if A ﬁnished within the time limit and B did not ﬁnish or needed a
larger CPU time; if A did not ﬁnish, then the instance is a winner for A in case
that B did also not ﬁnish, leaving, however, a larger gap than A. If the diﬀerence
between the times or gaps are below 1 sec. and 0.1, respectively, the instance is
not counted.
In all variants, we ﬁx the variables xij with j > i to zero. Furthermore,
we heuristically separate general clique inequalities 
i,j∈C yij ≥b, where b =
1
2t(t −1)(q −r) + 1
2t(t + 1)r and C ⊆V is a clique of size tq + r > q with
integers t ≥1, 0 ≤r < q (see [3]). The separation heuristic for a fractional
point y⋆follows ideas of Eisenblätter [5]. We generate the graph G′ = (V, E′)
with {i, k} ∈E′ if and only if {i, k} ∈E and y⋆
ik < b(b + 1)/2, where y⋆is the
y-part of an LP-solution. We search for maximum cliques in G′ with the branch-
and-bound method implemented in SCIP (with a branch-and-bound node limit
of 10 000) and check whether the corresponding inequality is violated.
Our default branching rule combines ﬁrst index and reliability branching. We
branch on the ﬁrst fractional x-variable in the row-wise variable order used for
deﬁning orbitopes, but we skip columns in which a 1 has appeared before. If
no such fractional variable could be found, we perform reliability branching as
described by Achterberg, Koch, and Martin [2].
We generated random instances with n vertices and m edges of the following
types. For n = 30 we used m = 200 (sparse), 300 (medium), and 400 (dense).

86
V. Kaibel, M. Peinhardt, and M.E. Pfetsch
Table 1. Results of the branch-and-cut algorithm. All entries are rounded averages
over three instances. CPU times are given in seconds.
basic
Iso Pruning
OF
n
m
q
nsub
cpu
nsub
cpu
nsub
cpu
#OF
30 200
3
1 082
6
821
4
697
5
6
30 200
6
358
1
122
0
57
0
25
30 200
9
1
0
1
0
1
0
0
30 200 12
1
0
1
0
1
0
0
30 300
3
3 470
87
2 729
64
2 796
69
7
30 300
6
89 919
445
63 739
168
8 934
45
353
30 300
9
8 278
19
5 463
5
131
0
73
30 300 12
1
0
1
0
1
0
0
30 400
3
11 317
755
17 433
800
9 864
660
8
30 400
6
458 996
14 400
1 072 649
11 220
159 298
3 142
1 207
30 400
9
2 470 503
14 400
1 048 256
2 549
70 844
450
7 305
30 400 12
3 668 716
12 895
37 642
53
2 098
12
1 269
50 560
3
309 435
10 631
290 603
14 400
288 558
10 471
10
50 560
6
1 787 989
14 400
3 647 369
14 400
1 066 249
9 116
4 127
50 560
9
92
0
2 978
5
10
0
10
50 560 12
1
0
1
0
1
0
0
Additionally, for n = 50 we choose m = 560 in search for the limits of our
approach. For each type we generated three instances by picking edges uniformly
at random (without recourse) until the speciﬁed number of edges is reached. The
edge weights are drawn independently uniformly at random from the integers
{1, . . ., 1000}. For each instance we computed results for q = 3, 6, 9, and 12.
In a ﬁrst experiment we tested the speedup that can be obtained by perform-
ing orbitopal ﬁxing. For this we compare the variant (basic) without symmetry
breaking (except for the zero-ﬁxing of the upper right x-variables) and the ver-
sion in which we use orbitopal ﬁxing (OF); see Table 1 for the results. Columns
nsub give the number of nodes in the branch-and-bound tree. The results show
that orbitopal ﬁxing is clearly superior (OF winners: 26, basic winners: 3), see
also Figure 3.
Table 1 shows that the sparse instances are extremely easy, the instances with
m = 300 are quite easy, while the dense instances are hard. One eﬀect is that
250 s
250 s
250 s
500 s
500 s
500 s
750 s
750 s
750 s
1000 s
1000 s
1000 s
3
3
3
3
3
3
3
3
3
6
6
6
6
6
6
6
6
6
9
9
9
9
9
9
9
9
9
2 h
2 h
2 h
2 h
2 h
4 h
4 h
4 h
4 h
4 h
50 %
50 %
50 %
50 %
50 %
100 %
100 %
100 %
100 %
100 %
150 %
150 %
150 %
150 %
150 %
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
9
9
9
9
9
9
9
9
9
9
9
9
9
9
9
12 12 12
12 12 12
12 12 12
12 12 12
12 12 12
3
3
3
3
3
3
3
3
3
3
3
3
3
3
3
6
6
6
6
6
6
6
6
6
6
6
6
6
6
6
2 h
2 h
2 h
2 h
2 h
4 h
4 h
4 h
4 h
4 h
50 %
50 %
50 %
50 %
50 %
100 %
100 %
100 %
100 %
100 %
Fig. 3. Computation times/gaps for the basic version (dark gray) and the version with
orbitopal ﬁxing (light gray). From left to right: instances with n = 30, m = 300,
instances for n = 30, m = 400, instances for n = 50, m = 560. The number of
partitions q is indicated on the x-axis. Values above 4 hours indicate the gap in percent.

Orbitopal Fixing
87
often for small m and large q the optimal solution is 0 and hence no work has to
be done. For m = 300 and 400, the hardest instances arise when q = 6. It seems
that for q = 3 the small number of variables helps, while for q = 12 the small
objective function values help. Of course, symmetry breaking methods become
more important when q gets larger.
In a second experiment we investigated the symmetry breaking capabilities
built into CPLEX. We suspect that it breaks symmetry within the tree, but no
detailed information was available. We ran CPLEX 10.01 on the IP formulation
stated in Sect. 1. In one variant, we ﬁxed variables xij with j > i to zero, but
turned symmetry breaking oﬀ. In a second variant, we turned symmetry breaking
on and did not ﬁx variables to zero (otherwise CPLEX seems not to recognize
the symmetry). These two variants performed about equally good (turned-on
winners: 13, turned-oﬀwinners: 12). The variant with no symmetry breaking
and no ﬁxing of variables performed extremely badly. The results obtained by
the OF-variant above are clearly superior to the best CPLEX results (CPLEX
could not solve 10 instances within the time limit, while OF could not solve 2).
Probably this is at least partially due to the separation of clique inequalities and
the special branching rule in our code.
In another experiment, we turned oﬀorbitopal ﬁxing and separated shifted
column inequalities in every node of the tree. The results are that the OF-version
is slightly better than this variant (OF winners: 13, SCI winners: 10), but the
results are quite close (OF average time: 1563.3, SCI average time: 1596.7).
Although by Part 2 of Theorem 3, orbitopal ﬁxing is not stronger than ﬁxing
with SCIs (with the same branching decisions), the LPs get harder and the
process slows down a bit.
Finally, we compared orbitopal ﬁxing to the isomorphism pruning approach
of Margot. We implemented the ranked branching rule (see [16]) adapted to the
special symmetry we exploit, which simpliﬁes Margot’s algorithm signiﬁcantly. It
can be seen from Table 1 that isomorphism pruning is inferior to both orbitopal
ﬁxing (OF winners: 25, isomorphism pruning winners: 3) and shifted column
inequalities (26:2), but is still a big improvement over the basic variant (23:7).
6
Concluding Remarks
The main contribution of this paper is a linear time algorithm for the orbitopal
ﬁxing problem, which provides an eﬃcient way to deal with partitioning type
symmetries in integer programming models. The result can easily be extended to
“packing orbitopes” (where, instead of x(rowi) = 1, we require x(rowi) ≤1). Our
proof of correctness of the procedure uses the linear description of O=
p,q given
in [11]. However, we only need the validity of the shifted column inequalities in
our arguments. In fact, one can devise a similar procedure for the case where
the partitioning constraints x(rowi) = 1 are replaced by covering constraints
x(rowi) ≥1, though, for the corresponding “covering orbitopes” no complete lin-
ear descriptions are known at this time. A more detailed treatment of this will be

88
V. Kaibel, M. Peinhardt, and M.E. Pfetsch
contained in a journal version of the paper, which will also include comparisons
to the isomorphism pruning method [14, 15, 17] and to orbital branching [13].
References
[1] T.
Achterberg,
SCIP
–
A
framework
to
integrate
constraint
and
mixed
integer
programming,
Report
04-19,
Zuse
Institute
Berlin,
2004.
http://www.zib.de/Publications/abstracts/ZR-04-19/.
[2] T. Achterberg, T. Koch, and A. Martin, Branching rules revisited, Oper.
Res. Lett., 33 (2005), pp. 42–54.
[3] S. Chopra and M. Rao, The partition problem, Math. Program., 59 (1993),
pp. 87–115.
[4]
, Facets of the k-partition polytope, Discrete Appl. Math., 61 (1995), pp. 27–
48.
[5] A. Eisenblätter, Frequency Assignment in GSM Networks: Models, Heuristics,
and Lower Bounds, PhD thesis, TU Berlin, 2001.
[6] J. Falkner, F. Rendl, and H. Wolkowicz, A computational study of graph
partitioning, Math. Program., 66 (1994), pp. 211–239.
[7] C. Ferreira, A. Martin, C. de Souza, R. Weismantel, and L. Wolsey,
Formulations and valid inequalities of the node capacitated graph partitioning prob-
lem, Math. Program., 74 (1996), pp. 247–266.
[8]
, The node capacitated graph partitioning problem: A computational study,
Math. Program., 81 (1998), pp. 229–256.
[9] M. Grötschel and Y. Wakabayashi, A cutting plane algorithm for a clustering
problem, Math. Prog., 45 (1989), pp. 59–96.
[10]
, Facets of the clique partitioning polytope, Math. Prog., 47 (1990), pp. 367–
387.
[11] V. Kaibel and M. E. Pfetsch, Packing and partitioning orbitopes, Math. Pro-
gram., (2007). In press.
[12] G. Kochenberger, F. Glover, B. Alidaee, and H. Wang, Clustering of
microarray data via clique partitioning, J. Comb. Optim., 10 (2005), pp. 77–92.
[13] J. Linderoth, J. Ostrowski, F. Rossi, and S. Smriglio, Orbital branching,
in Proceedings of IPCO XII, M. Fischetti and D. Williamson, eds., vol. 4513 of
LNCS, Springer-Verlag, 2007, pp. 106–120.
[14] F. Margot, Pruning by isomorphism in branch-and-cut, Math. Program., 94
(2002), pp. 71–90.
[15]
, Exploiting orbits in symmetric ILP, Math. Program., 98 (2003), pp. 3–21.
[16]
, Small covering designs by branch-and-cut, Math. Program., 94 (2003),
pp. 207–220.
[17]
, Symmetric ILP: Coloring and small integers, Discrete Opt., 4 (2007),
pp. 40–62.
[18] A. Mehrotra and M. A. Trick, Cliques and clustering: A combinatorial ap-
proach, Oper. Res. Lett., 22 (1998), pp. 1–12.
[19] M. M. Sørensen, Polyhedral computations for the simple graph partitioning prob-
lem, working paper L-2005-02, Århus School of Business, 2005.

New Variants of Lift-and-Project Cut
Generation from the LP Tableau: Open Source
Implementation and Testing
Egon Balas1,⋆and Pierre Bonami2
1 Tepper School of Business, Carnegie Mellon University, Pittsburgh PA
eb17@andrew.cmu.edu.
2 T.J. Watson Research Center, IBM, Yorktown Heights, NY
pbonami@us.ibm.com
Abstract. We discuss an open source implementation and preliminary
computational testing of three variants of the Balas-Perregaard proce-
dure for generating lift-and-project cuts from the original simplex
tableau, two of which are new. Variant 1 is the original procedure of
[6] with minor modiﬁcations. Variant 2 uses a new procedure for choos-
ing the pivot element: After identifying the set of row candidates for
an improving pivot, the pivot element (and column) is chosen by opti-
mizing over the entries of all candidate rows. Finally, Variant 3 replaces
the source row with its disjunctive modularization, and after each pivot
it again modularizes the resulting source row. We report on computa-
tional results with the above three variants and their combinations on
65 MIPLIB.3 instances.
Keywords: integer programming, branch and cut algorithms.
1
Introduction
The revolution of the last 15 years in the state of the art of integer programming
was brought about, besides faster computers and more eﬃcient linear program-
ming codes, also by improved cutting plane techniques. Lift-and-project (L&P)
cuts were the ﬁrst to be generated in rounds and to be embedded into a branch-
and-cut framework. They were also the ﬁrst locally valid cuts lifted into globally
valid ones. Soon after the success of L&P cuts [3,4], it was shown [5] that mixed
integer Gomory (MIG) cuts used in the same manner could also enhance the
performance of MIP solvers. Thus, during the nineties a number of diﬀerent cut
families (cover and ﬂow cover inequalities, MIG cuts, simple disjunctive cuts,
MIR cuts etc.) became part of the toolkit of commercial MIP solvers and have
led to a radical improvement of their performance. The L&P cuts themselves,
however, were found to be computationally too expensive to be incorporated into
commercial codes, as each such cut came at the price of solving a Cut Generating
Linear Program (CGLP) in a higher dimensional space. It was not until a few
⋆Research supported by the National Science Foundation through grant #DMI-
0352885 and by the Oﬃce of Naval Research through contract N00014-03-1-0133.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 89–103, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

90
E. Balas and P. Bonami
years later, when a way was found [6] to generate L&P cuts by pivoting in the
original simplex tableau, without constructing the higher dimensional CGLP,
that these cuts became suﬃciently cost-eﬀective to be incorporated into a state-
of-the-art MIP solver, where they soon proved their value [10] and became the
default cut generator.
Although the algorithm for generating L&P cuts from the original simplex
tableau is now in practical use and has contributed to solving countless hard in-
teger programs, its implementation was until now commercial property not pub-
licly available, which made it harder for researchers to experiment with diﬀerent
versions of it. In this paper we discuss an implementation of this algorithm in the
COIN-OR framework, publicly available [9] since September 2006, and compare
three diﬀerent variants of it. Variant 1 is a slightly modiﬁed version of the origi-
nal algorithm [6] for generating L&P cuts by pivoting in the original LP tableau
which incorporates the various improvements proposed in [10,11], whereas the
other two variants contain substantial changes in the algorithm, which give rise
to diﬀerent pivot sequences and therefore diﬀerent cuts. Variant 2 uses a new
rule for choosing the entering variable in the pivoting procedure. Instead of ﬁrst
choosing a most promising pivot row and then identifying the best column in
that row, this version of the algorithm ﬁrst identiﬁes all candidate rows for an im-
proving pivot, then chooses the pivot element as the best one among the entries
of all the candidate rows. Variant 3 uses recursive disjunctive modularization of
the source row. In other words, rather than ﬁrst generating an unstrengthened
“deepest” L&P cut through a sequence of pivots in the original LP tableau and
then strengthening the end product by modular arithmetic, this version replaces
the source row with its disjunctive modularization, and after each pivot it again
applies the disjunctive modularization to the resulting transformed source row.
Each of the three Variants give rise to sequences of pivots diﬀerent from each
other. In the case of both Variants 2 and 3, each pivot is guaranteed to produce
an improvement in cut strength at least equal to that produced by the corre-
sponding pivot of Variant 1, but this additional improvement comes at some
computational cost.
After describing each of the three Variants, we compare them on a battery
of MIPLIB test problems and assess the results by trying to identify the merits
and demerits of each Variant.
Consider a problem of the form min{cx : x ∈P, xj ∈Z, j = 1, . . . , p} (MIP)
and its linear programming relaxation min{cx : x ∈P} (LP), where P is the
polyhedron deﬁned by the system
Ax ≥b
−xj ≥−1
j = 1, . . . , p
x ≥0
(1)
Here A is m × n, 1 ≤p ≤n, and (1) will also be denoted as ˜Ax ≥˜b. Note
that the vector s ∈Rm+p+n of surplus variables has n components of the form
sm+p+j = xj, which represent just a set of diﬀerent names for the structural
variables xj.

New Variants of Lift-and-Project Cut Generation from the LP Tableau
91
Let x∗be an optimal solution to (LP) and let
xk = ¯ak0 −

j∈J
¯akjsj
(2)
be the row of the optimal simplex tableau corresponding to basic variable xk,
with 0 < ¯ak0 < 1 and J the index set of nonbasic variables. The intersection
cut [1] from the convex set {x ∈Rn : 0 ≤xk ≤1}, also known as the simple
disjunctive cut from the condition xk ≤0 ∨xk ≥1 applied to (2), is πs ≥π0,
where π0 = ¯ak0(1 −¯ak0) and πj := max {¯akj(1 −¯ak0), −¯akj¯ak0} ,
j ∈J.
This cut can be strengthened [1] using the integrality of some variables in
J, by replacing π with ¯π, where ¯πj = πj for j ∈J \ {1, . . . , p}, and ¯πj :=
min{fkj(1 −¯ak0), (1 −fkj)¯ak0}, j ∈J ∩{1, . . . , p}, where fkj = ¯akj −⌊¯akj⌋. This
strengthened intersection cut or strengthened simple disjunctive cut is the same
as the mixed integer Gomory (MIG) cut.
On the other hand, given the same optimal solution x∗to (LP), a deepest lift-
and-project (L&P) cut αx ≥β is obtained by solving a Cut Generating Linear
Program [3] in a higher dimensional space:
min αx∗−β
s.t.
α
−u ˜A
+ u0ek
= 0
α
−v ˜A
−v0ek = 0
−β + u˜b
= 0
−β
+ v˜b
+ v0
= 0
ue + ve + u0
+ v0
= 1
u, v, u0, v0 ≥0
(CGLP)k
where e = (1, . . . , 1) and ek is the k-th unit vector.
While an optimal solution to (CGLP)k yields a “deepest” cut αx ≥β, i.e.
one that cuts oﬀx∗by a maximum amount, any solution to the constraint set
of (CGLP)k yields a member of the family of L&P cuts. If (α, β, u, v, u0, v0)
is a basic solution to (CGLP)k, the coeﬃcients of the corresponding L&P cut
are β = u˜b = v˜b + v0, αk = max{u ˜Ak −uk −u0, v ˜Ak −vk + v0}, and αj =
max{u ˜Aj −uj, v ˜Aj −vj}, j ̸= k where ˜Aj is the j-th column of ˜A.
Again, this cut can be strengthened using the integrality of some of the struc-
tural variables by replacing α with ¯α, where ¯αj = αj for j = k and j /∈{1, . . . , p},
and ¯αj = min{u ˜Aj −uj + u0⌈mj⌉, v ˜Aj −vj −v0⌊mj⌋}, j ∈{1, . . . , p}\{k}, with
mj = (v ˜Aj −vj −u ˜Aj + uj)/(u0 + v0).
In [6] it was shown that the intersection cut obtained from a given component
xk of a basic feasible solution of (LP) is equivalent to the L&P cut obtained
from a basic solution to (CGLP)k, where the bases in question are related to
each other in a well deﬁned manner. The same relationship holds between the
strengthened version of the intersection cut, i.e. the mixed integer Gomory cut,
on the one hand, and the strengthened L&P cut on the other. Furthermore, a
strengthened L&P cut is equivalent to a MIG cut from some LP tableau that

92
E. Balas and P. Bonami
in general is neither optimal nor feasible, and the search for a deepest L&P cut
can be viewed as the search for the appropriate simplex tableau from which to
derive the corresponding MIG cut. The next section discusses this connection.
2
The Correspondence Between L&P Cuts and MIG
Cuts
Let αx ≥β be a L&P cut corresponding to a basic feasible solution (α, β, u, v, u0,
v0) of (CGLP)k, and let ¯ax ≥β be its strengthened version. Further, let u0 > 0,
v0 > 0 (these are known to be the only solutions yielding cuts that diﬀer from the
rows of ˜Ax ≥˜b), and let M1 and M2 be the index sets of the basic components
of u and v respectively. Then M1 ∩M2 = 0, |M1 ∪M2| = n, and the square
submatrix ˆA of ˜A whose rows are indexed by M1 ∪M2 is nonsingular (see [6]).
Now deﬁne J := M1∪M2. Then letting ˆb denote the subvector of ˜b corresponding
to ˆA and writing sJ for the surplus variables indexed by J, we have
ˆAx −sJ = ˆb
or
x = ˆA−1ˆb −ˆA−1sJ
(3)
and the row of (3) corresponding to xk (a basic variable, since k /∈J) can be
written as
xk = ¯ak0 −

j∈J
¯akjsj,
(4)
where ¯ak0 = ek ˆA−1ˆb and ¯akj = −ˆA−1
kj . Notice that (4) is the same as (2).
Furthermore, it can be shown that 0 < ¯ak0 < 1, and we have (from [6])
Theorem 1. The MIG cut ¯πs ≥π0 from (4) is equivalent to the strengthened
L&P cut αx ≥β.
Conversely, suppose (4) is the row associated with xk in a basic solution to (LP),
not necessarily optimal or even feasible, such that 0 < ¯ak0 < 1. Then we have
Theorem 2. Let (M1, M2) be any partition of J such that j ∈M1 if ¯akj < 0
and j ∈M2 if ¯akj > 0. Then the solution to (CGLP)k corresponding to the basis
with components (α, β, u0, v0, {ui : i ∈M1}, {vi : i ∈M2}) deﬁnes a L&P cut
αx ≥β whose strengthened version ¯αx ≥β is equivalent to the MIG cut ¯πs ≥π0
derived from (4).
Note that the partition (M1, M2) of J, and therefore the basis of (CGLP)k
deﬁned by it, is not unique, since the variables j ∈J such that ¯akj = 0 can be
assigned either to M1 or to M2. This means that the correspondence between
bases described above maps each basis B of (LP) into a set of bases ϕ(B) of
(CGLP)k, where typically |ϕ(B)| > 1. However, all bases in ϕ(B) correspond to
the same solution of (CGLP)k, i.e. they are degenerate, and the correspondence
between basic solutions (as opposed to bases) of (LP) and (CGLP)k is one to
one (see [6] for details).

New Variants of Lift-and-Project Cut Generation from the LP Tableau
93
3
The Lift-and-Project Procedure in the Original LP
Tableau
The lift-and-project procedure in the (LP) tableau uses the above correspon-
dence to mimic the optimization of (CGLP)k by the simplex algorithm. Consider
the row corresponding to xk of the form (2) which we call the source row. At
each iteration of the procedure, we perform a pivot in a row i ̸= k, which brings
about a linear combination of the source row with row i
xk + γxi = ak0 + γai0 −

j∈J
(akj + γaij)sj
(5)
such that the intersection cut obtained from this new row is more violated by
x∗than the one obtained from the source row. This combination (the choice of
the row i and of γ), is guided by the correspondence with (CGLP)k. Each row i
of the (LP) simplex tableau corresponds to a pair of columns of (CGLP)k with
associated nonbasic variables ui, vi.
The ﬁrst main step in the procedure is to compute the reduced costs rui and
rvi in (CGLP)k for all i ̸∈J ∪{k}. As shown in [6], these reduced costs can be
expressed in terms of the entries ¯aij of the (LP) tableau and the solution x∗.
We use these expressions in our computations. If there is no negative reduced
cost, the current basis is optimal for (CGLP)k and the optimal strengthened
lift-and-project cut is obtained as the MIG cut from the source row of (LP)
(using the correspondence of Theorem 2). On the other hand, if at least one
negative reduced cost exists, then the cut can be improved by performing a
pivot in (CGLP)k where the corresponding variable ui or vi enters the basis. In
the (LP) tableau, this negative reduced cost (rui or rvi) corresponds to a basic
variable xi which has to leave the basis.
Choosing the variable xi to enter the basis is the second main step of the
procedure. In [6], two evaluation functions f +(γ) (resp. f −(γ)) were deﬁned,
which represent the objective function value of (CGLP)k, i.e. the violation of the
cut resulting from the combination of row k and row i for positive, respectively
negative values of γ. These two functions are minimized to select the variable to
enter the basis which leads to the largest improvement in cut violation among
all variables that can replace the exiting variable.
Once the exiting and entering variables have been selected, the pivot in (LP)
is performed and the procedure is iterated from the new basis until (CGLP)k is
optimized. The pseudo-code of Figure 1 describes this procedure.
As shown in [4], the lift-and-project cuts are more eﬃciently generated in
a subspace where all the non-basic structural variables of (LP) are ﬁxed to
their values in the optimal solution. Performing the separation in the subspace
while working in the (LP) tableau is done simply by removing all the structural
nonbasic variables from it before starting the pivoting procedure. At the end of
the procedure a lifting step is performed to obtain a valid cut for the original
problem by recomputing the source row in the full space and generating the
corresponding MIG cut.

94
E. Balas and P. Bonami
Let x∗be the optimal solution to (LP).
Let k ∈{1, . . . , p}with x∗
k fractional.
Let I and J be the index sets of basic and non-basic variables in an optimal
basis of (LP).
Let A be the optimal tableau.
Let num pivots:= 0.
while num pivots < pivot limit do
Compute the reduced costs rui, rvi for each i ̸∈J ∪{k}
if There exists i such that rui < 0 ∨rvi < 0
then
Let ˆi := arg
min
i̸∈J∪{k}{rui, rvi},
Let J′ = {j ∈J : |aˆıj| ≥ϵ, } be the set of admissible pivots.
Let J+ = J′ ∩{j ∈J : −akj/aˆıj < 0}.
Let ˆj := arg min{arg min
j∈J+ f +(γj), arg
min
j∈J′\J+ f −(γj)}.
Perform a pivot in (LP) by pivoting out ˆı and pivoting in ˆj.
Let I := I ∪{ˆj} \ {ˆı}.
Let A be the updated tableau in the new basis.
Let num pivots+= 1.
else /* cut is optimal. */
Generate the MIG cut from row k of the current tableau.
exit
ﬁ
od
Fig. 1. Lift-and-Project Procedure
4
Computation of the Reduced Cost and of the
Evaluation Functions
A key point for eﬃciently implementing the lift-and-project procedure is the
computation of the reduced costs and the evaluation functions.
As shown in [11], for a given partition (M1, M2) (as deﬁned in section 2) the
expressions for the reduced costs depend only linearly on the coeﬃcients of the
tableau, and therefore the reduced costs of all non-basic variables in (CGLP)k
can be computed by doing only one multiplication with the basis inverse. The
expressions for the reduced costs are
rui = −σ + ai0(1 −x∗
k) −τi and rvi = −σ −ai0(1 −x∗
k) + s∗
i + τi
where s∗= ˜Ax∗−b, σ = ( 
j∈M2
akjs∗
j −ak0(1 −x∗
k))/(1 + 
j∈J
|akj|) is the current
objective value of (CGLP)k, and τi = 
j∈M1
σaij + 
j∈M2
(s∗
j −σ)aij.

New Variants of Lift-and-Project Cut Generation from the LP Tableau
95
A critical element in computing the reduced costs is the choice of the partition
(M1, M2). If for all j ∈J, akj is non-zero, this partition is uniquely deﬁned; but
if this is not the case, several partitions can be chosen. The rule given in [6] is
to take M1 = {j ∈J : akj < 0 ∧(akj = 0 ∧aij > 0)} (and M2 = J \ M1) for
computing rui, and M1 = {j ∈J : akj < 0 ∧(akj = 0∧aij < 0)} for computing
rvi. This rule has the advantage that if a negative reduced cost is found, then
the corresponding pivot leads to a strictly better cut. On the other hand, to
determine this partition, one has to compute the coeﬃcients aij for all j such
that akj = 0 and all i. Therefore we use another rule. Namely, following [11], we
randomly assign all the zero elements of the source row to either M1 or M2. This
rule has the disadvantage that although the reduced cost for the perturbed row
is negative, it may happen that all the pivots with the corresponding variable
ui or vi entering the basis are degenerate in (CGLP)k. Nevertheless, in our
experiments, this rule had a clear computational advantage.
The second main step of the procedure is the computation of the evaluation
functions f + and f −, given by
f +(γ) =

j∈J
max{¯akj, −γ¯aij}s∗
j −¯ak0 + (¯ak0 + γ¯ai0)x∗
k
1 + γ + 
j∈J
|¯akj + γ¯aij|
and
f −(γ) =

j∈J
max{0, ¯akj + γ¯aij}s∗
j −(¯ak0 + γ¯ai0)(1 −x∗
k)
1 −γ + 
j∈J
|¯akj + γ¯aij|
As shown in [11], these functions are unimodal piecewise continuously diﬀer-
entiable and their minimum can be found eﬃciently, once rows k and i of the
tableau are speciﬁed, by computing the values of f + (resp. f −) by increasing
(resp. decreasing) the value of γl = −akl
ail for valid pivots of the correct sign.
5
Most Violated Cut Selection Rule
Here we present a variant of the lift-and-project procedure which uses a new
rule for choosing the leaving and entering variables in the pivot sequence. The
lift-and-project procedure in the (LP) tableau usually requires a remarkably
small number of pivots to obtain the optimal L&P cut, nevertheless it may be
computationally interesting to reduce this number further by studying alternate
rules for this choice. The rule discussed here performs, at each iteration, the pivot
to the adjacent basis in (LP) for which the objective of (CGLP)k is decreased
by the largest amount or, equivalently, the one for which the intersection cut
obtained from the row k of (LP) is the most violated by x∗.
Let us denote by f +
i (γ) (resp. f −
i (γ)) the function f +(γ) (resp. f −(γ)) deﬁned
for source row k and a row i of the tableau. Recall that these functions give the
violation of the intersection cut derived from the row obtained by adding γ times
row i to row k, depending on the sign of γ. Thus, the violation of the cut in the

96
E. Balas and P. Bonami
adjacent basis of (LP) where variable i leaves the basis and variable j enters the
basis is given by f +
i (γj) if γj = −akj/aij > 0 and f −
i (γj) if γj = −akl/aij < 0,
and the most violated intersection cut which can be derived from an adjacent
basis has violation
ˆσ =
min
i∈I\{k} min{ min
j∈J+ f +
i (γj), min
j∈J−f −
i (γj)}
where I is the basic index set and J+, J−are the index sets for γj > 0 and
γj < 0, respectively.
Here the variables ˆı and ˆj for which this minimum is attained are selected as
the leaving and entering variables respectively. By computing the reduced costs
rui and rvi, we ﬁrst identify all the candidate rows for an improving pivot. Then
for each such row i we minimize the functions f +
i
and f −
i .
This clearly amounts to more computation at each iteration than the selection
rule used in Variant 1, where only one minimization of the evaluation function
is performed at each pivot. But on the other hand, the cut violation is increased
at each iteration by an amount at least as large, and therefore one may expect
to obtain in less iterations a cut with a given violation. In particular, in the
presence of zero elements in the source row, it presents the advantage that fewer
degenerate pivots in (CGLP)k are performed.
6
Disjunctive Modularization
L&P cuts are obtained from disjunctions of the type
(u ˜Ax −u0xk ≥u˜b) ∨(v ˜Ax + v0xk ≥v˜b + v0)
where solving the (CGLP)k optimizes the multipliers u, u0, v and v0. Once the
optimal values for these multipliers are obtained, the cut can be further strength-
ened, as mentioned in section 1, by using modular arithmetic on the coeﬃcients
of the integer-constrained components of x. This latter operation can be inter-
preted (see [4]) as subtracting from xk on each side of the disjunction a product
of the form mx, where m is an integer vector, and then optimizing the com-
ponents of m over all integer values. In other words, the strengthened deepest
intersection cut is the result of a sequence of two optimization procedures, ﬁrst
in the mutipliers u, v, u0 and v0, then in the components of m. But this raises the
quest for a procedure that would simultaneously optimize both the continuous
multipliers and the integer vector m. While this is an intricate task, equivalent
to ﬁnding an optimal split cut, which has been treated elsewhere [7], the dis-
junctive modularization procedure described below is meant to approximate this
goal.
Consider again the equation of the source row (2) for an intersection cut or
a MIG cut. By applying disjunctive modularization to this equation we mean
deriving from it the modularized equation
yk = ϕk0 −

j∈J
ϕkjsj
(6)

New Variants of Lift-and-Project Cut Generation from the LP Tableau
97
where yk is a new, integer-constrained variable of unrestricted sign, ϕk0 = ¯ak0,
ϕkj :=
⎧
⎨
⎩
¯akj −⌊¯akj⌋, j ∈J+
1 := {j ∈J1 : ¯akj −⌊¯akj⌋≤¯ak0}
¯akj −⌈¯akj⌉, j ∈J−
1 := {j ∈J1 : ¯akj −⌊¯akj⌋> ¯ak0}
¯akj
j ∈J2 := J \ J1
and J1 := J ∩{1, . . ., p}.
Clearly, every set of sj, j ∈J, that satisﬁes (2) with xk integer, also satisﬁes
(6) with yk integer; hence the equation (6) is valid. Also, it is easy to see that the
intersection cut derived from (6) is the strengthened intersection cut, or MIG
cut derived from (2). However, at this point we do not intend to generate a cut.
Instead, we append (6) to the optimal (LP) tableau and declare it the source
row in place of (2) for the entire pivoting sequence. Further, after each pivot in
row ˆı and column ˆj the transformed row of yk, say yk = ϕ′
k0 −
j∈J′ ϕ′
kjsj where
J′ := (J \ {ˆj}) ∪{ˆı}, is treated again with disjunctive modularization. Namely,
this time the row of yk is replaced with yk = ¯ϕk0 −
j∈J′ ϕkjsj where ϕk0 = ϕ′
k0,
and
ϕkj :=
⎧
⎨
⎩
ϕ′
kj −⌊ϕ′
kj⌋, j ∈(J′
1)+
ϕ′
kj −⌈ϕ′
kj⌉, j ∈(J′
1)−
ϕ′
kj
j ∈J′
2
(7)
with (J′
1)+, (J′
1)−and J′
2 deﬁned analogously to J+
1 , J−
1 and J2.
The expressions used for calculating the reduced costs rui, rvi and the evalua-
tion functions f +(γ), f −(γ) used for selecting the pivot element at each iteration
remain valid, except for the fact that the entries ¯akj of the current row (2) of xk
are replaced (since this is no longer the source row) with the entries ϕkj of the
current row of yk (see [2] for details).
It is clear that the modularized source row, if used for cut generation, would
yield a cut that dominates the one from the unmodularized source row. It can
also be shown that every iteration of the cut generating algorithm that uses
disjunctive modularization improves the cut obtainable from the source row.
7
Computational Results
The algorithm for generating L&P cuts from the (LP) tableau was implemented,
in all three of its Variants discussed above, as a cut generator called CglLandP [9]
in the COIN-OR framework. This generator is open-source and is available since
September 2006 as part of the Cut Generation Library [8]. All the computations
have been carried out using the publicly available version of the cut generator
and were performed on a computer equipped with a 2 GHz AMD Optetron CPU
and 3 GB of RAM.
Before presenting our results, it will be useful to recall a comparison between
the computational eﬀorts required by the original procedure that generates L&P
cuts by solving the higher dimensional (CGLP), and the new one that pivots in
the (LP) tableau. Based on running XPRESS on about 100 test problems with

98
E. Balas and P. Bonami
Table 1. Comparing 10 rounds of diﬀerent cuts at the root node
MIG Cuts
Lift-and-Project Cuts
Variant 1
Variant 2
Variant 3
time (sec)
%
gap closed
average
cut
violation
time (sec)
%
gap closed
average
cut
violation
time (sec)
%
gap closed
average
cut
violation
time (sec)
%
gap closed
average
cut
violation
10teams
1.7
100.00
2.9728e-04
10.3
100.00
5.4248e-03
12.0
100.00
2.9581e-03
10.1
100.00
6.0645e-03
air03
0.3
100.00
1.0055e-04
0.7
100.00
2.5349e-02
0.3
100.00
1.0055e-04
0.8
100.00
2.5638e-02
air04
15.1
13.13
2.6198e-05
176.4
18.17
4.1924e-04
284.8
22.32
1.9377e-04
182.4
19.83
3.9787e-04
air05
11.8
6.89
4.0196e-05
98.9
12.99
1.3294e-03
163.1
14.12
4.6990e-04
114.4
12.65
1.3018e-03
arki001
2.6
52.07
4.2588e-03
6.9
52.89
2.2820e-02
6.4
52.07
3.0722e-02
5.8
43.80
2.6925e-02
bell3a
0.0
72.11
1.0944e-02
0.0
72.04
1.3493e-02
0.0
70.74
1.3322e-02
0.0
71.07
1.19344e-02
bell5
0.0
90.71
2.1735e-02
0.1
92.32
2.1099e-02
0.1
92.62
2.2064e-02
0.1
92.39
2.1817e-02
blend2
0.1
34.14
1.8580e-03
0.3
37.21
2.1410e-02
0.3
34.34
1.8501e-02
0.2
36.14
3.0460e-02
cap6000
0.2
62.50
3.9211e-05
1.7
62.50
7.1012e-05
2.2
62.50
6.3832e-05
2.5
62.50
8.1029e-05
dano3mip
75.2
0.03
2.9685e-03
498.4
0.03
1.2500e-02
223.7
0.03
1.0568e-02
147.6
0.03
1.5132e-02
danoint
0.7
1.74
7.4911e-04
7.5
1.59
1.2143e-02
10.5
1.88
8.8077e-03
9.2
1.38
9.2614e-03
dcmulti
0.4
69.54
2.5191e-02
2.5
78.17
4.5511e-02
2.3
83.10
4.5834e-02
1.5
76.60
4.6794e-02
dsbmip
0.3
no gap
4.3132e-02
0.6
no gap
5.8988e-02
0.5
no gap
6.6541e-02
0.5
no gap
6.1998e-02
egout
0.0
99.83
3.9095e-02
0.0
100.00
7.6798e-02
0.0
100.00
7.6902e-02
0.0
100.00
7.6798e-02
enigma
0.0
no gap
6.6426e-03
0.0
no gap
1.1151e-02
0.0
no gap
8.8013e-03
0.0
no gap
1.7631e-02
fast0507
80.9
3.45
6.8759e-06
325.2
3.67
7.3836e-04
297.6
4.05
1.6800e-04
357.3
3.40
1.2375e-03
ﬁber
0.8
79.79
8.8217e-04
1.6
87.07
4.3319e-03
1.9
92.65
5.8489e-03
2.8
88.35
4.2286e-03
ﬁxnet6
0.3
85.77
7.4356e-03
1.3
87.90
3.6747e-02
2.3
89.15
3.7913e-02
1.3
89.09
3.6157e-02
ﬂugpl
0.0
14.05
1.2964e-02
0.0
15.94
1.7391e-02
0.0
16.09
1.5143e-02
0.0
15.94
1.7391e-02
gen
0.1
81.97
3.2112e-03
0.2
81.42
1.5461e-02
0.3
81.97
1.5146e-02
0.2
80.33
1.4654e-02
gesa2
1.0
75.12
5.0931e-03
1.8
76.33
1.3842e-02
2.5
90.64
1.6601e-02
1.3
77.62
1.3775e-02
gesa2 o
1.1
63.28
5.0887e-03
1.5
63.74
1.2591e-02
3.6
63.91
1.4217e-02
2.1
64.40
1.2421e-02
gesa3
1.0
56.16
2.4234e-03
2.0
80.37
7.4798e-03
2.6
84.18
9.6241e-03
2.2
83.16
7.5700e-03
gesa3 o
0.9
58.13
3.1590e-03
2.5
80.62
9.3592e-03
5.2
83.74
8.9960e-03
2.1
77.89
8.4678e-03
gt2
0.0
100.00
5.4967e-03
0.1
100.00
1.0840e-02
0.0
100.00
2.0432e-02
0.0
100.00
2.1752e-02
harp2
0.9
37.29
3.1194e-04
2.0
40.62
3.8258e-03
4.2
45.51
4.8609e-03
3.3
40.02
4.4110e-03
khb05250
0.2
94.34
3.2644e-02
0.3
96.68
5.3187e-02
0.4
97.18
6.1590e-02
0.3
96.44
5.6201e-02
l152lav
1.7
20.78
1.4208e-04
8.4
39.87
3.3926e-03
9.5
40.30
2.0602e-03
13.8
33.10
3.7684e-03
lseu
0.0
85.81
2.8617e-03
0.0
88.83
1.3431e-02
0.1
89.19
1.0132e-02
0.1
85.27
1.5530e-02
markshare1
0.0
0.00
2.4035e-03
0.0
0.00
1.5459e-02
0.0
0.00
7.8762e-03
0.0
0.00
1.0355e-02
markshare2
0.0
0.00
1.7025e-03
0.0
0.00
4.5266e-03
0.0
0.00
5.3009e-03
0.0
0.00
1.0701e-02
mas74
0.1
7.62
3.6506e-04
0.5
8.89
5.3231e-03
0.5
8.75
3.0362e-03
0.4
8.29
7.5558e-03
mas76
0.0
7.40
2.5738e-04
0.4
9.09
2.1974e-03
0.4
8.63
1.6655e-03
0.4
8.84
5.8109e-03
misc03
0.1
20.00
3.7024e-03
0.2
19.44
2.6853e-02
0.7
23.75
1.9687e-02
0.1
17.24
3.0294e-02
misc06
0.1
78.26
1.3575e-03
0.3
90.22
5.6188e-03
0.2
95.65
5.8362e-03
0.3
90.22
6.7744e-03
misc07
0.0
0.72
3.7471e-03
0.1
0.72
2.8068e-02
0.3
2.51
2.9601e-02
0.1
0.72
3.0492e-02
mitre
0.3
100.00
1.5473e-03
0.4
100.00
3.8563e-03
0.6
100.00
5.6427e-03
0.4
100.00
3.8195e-03
mkc
3.3
30.66
2.7229e-03
4.7
49.98
1.7965e-02
4.4
46.18
1.8324e-02
4.7
43.84
1.8470e-02
mod008
0.0
30.44
3.1792e-04
0.1
33.73
5.8576e-03
0.1
41.62
5.4240e-03
0.1
39.35
2.4730e-02
mod010
0.1
100.00
2.2217e-04
0.7
100.00
7.6252e-03
0.1
100.00
2.5233e-04
1.4
94.79
3.9760e-03
mod011
6.4
38.50
3.1465e-02
19.3
39.61
5.7859e-02
60.0
41.42
6.1338e-02
17.6
39.37
5.7518e-02
modglob
0.3
61.05
2.5368e-02
0.5
62.31
4.5024e-02
1.0
58.17
4.4318e-02
0.6
63.89
4.3180e-02
noswot
0.0
no gap
1.3127e-02
0.1
no gap
3.4442e-02
0.1
no gap
4.1982e-02
0.2
no gap
4.2526e-02
nw04
7.2
100.00
5.7806e-06
6.0
100.00
8.4070e-03
10.2
100.00
5.7806e-06
34.0
100.00
6.1504e-03
p0033
0.0
76.98
9.4281e-03
0.0
75.57
2.3048e-02
0.0
78.38
2.2338e-02
0.0
75.75
1.9114e-02
p0201
0.2
54.97
2.1807e-03
1.1
83.28
1.1511e-02
1.6
79.78
1.1691e-02
1.2
84.92
9.7795e-03
p0282
0.1
24.10
1.0334e-02
0.2
55.66
6.8690e-02
0.3
48.78
5.6026e-02
0.3
59.46
7.2102e-02
p0548
0.2
95.57
7.7912e-03
0.3
97.60
1.7712e-02
0.5
94.83
1.7799e-02
0.3
97.16
1.7990e-02
p2756
0.6
97.90
2.2700e-02
0.8
97.00
4.4864e-02
1.0
97.42
3.6267e-02
1.0
97.16
4.4811e-02
pk1
0.0
0.00
2.9938e-03
0.0
0.00
9.5531e-03
0.1
0.00
1.4450e-02
0.0
0.00
3.7375e-02
pp08a
0.3
90.39
3.5520e-02
0.6
89.49
4.5770e-02
0.5
92.49
5.1440e-02
0.6
92.59
4.6828e-02
pp08aCUTS
0.5
65.48
2.2789e-02
1.0
71.99
3.7008e-02
1.4
75.06
3.4990e-02
1.1
71.49
3.5609e-02
qiu
2.0
8.37
5.6655e-03
23.4
29.18
1.1153e-02
45.5
30.60
1.0984e-02
23.4
29.18
1.1153e-02
qnet1
1.5
36.18
3.5439e-04
4.5
39.39
6.6478e-03
4.6
46.06
5.3104e-03
7.1
42.29
5.8200e-03
qnet1 o
1.2
56.80
9.8367e-04
4.1
67.05
1.4260e-02
3.4
69.15
1.4546e-02
5.0
68.93
1.4840e-02
rentacar
0.3
30.56
2.7062e-02
0.4
37.20
5.3647e-02
5.5
43.26
5.0001e-02
0.4
37.20
5.3647e-02
rgn
0.0
12.30
5.9142e-03
0.0
16.37
4.3256e-02
0.0
19.76
3.9477e-02
0.1
33.62
-0.00558971
rout
0.2
5.19
2.1877e-03
3.4
24.54
1.2801e-02
4.3
35.35
1.0830e-02
3.7
29.17
1.4507e-02
set1ch
0.5
68.44
5.5305e-02
0.9
75.66
7.3559e-02
1.1
75.43
7.6604e-02
1.6
73.00
7.3731e-02
seymour
4.9
14.27
1.0106e-02
21.7
14.60
1.1268e-02
30.7
19.92
1.5199e-02
20.5
15.75
1.2845e-02
stein27
0.0
0.00
3.2688e-02
0.0
0.00
6.0127e-02
0.0
0.00
6.1522e-02
0.0
0.00
6.0228e-02
stein45
0.0
0.00
1.9886e-02
0.3
0.00
5.5584e-02
0.3
0.00
5.3331e-02
0.3
0.00
5.5584e-02
swath
4.1
26.87
1.1443e-04
5.4
27.60
1.0963e-02
6.5
27.20
6.4202e-03
4.6
28.36
1.0804e-02
vpm1
0.0
52.70
8.7372e-03
0.0
75.59
2.0991e-02
0.1
76.82
2.1853e-02
0.0
75.59
2.1079e-02
vpm2
0.0
53.51
8.6445e-03
0.1
61.66
1.7934e-02
0.2
64.13
1.6697e-02
0.1
59.29
2.0777e-02
Average
3.566
48.45
9.515e-3
19.27
53.62
2.232e-2
18.81
55.05
2.164e-2
15.28
53.69
2.297e-2
each of the two methods, Perregaard [10] reported that the new method required
5% of the number of pivots and 1.3% of the time required by the original one
for generating a L&P cut.

New Variants of Lift-and-Project Cut Generation from the LP Tableau
99
Table 1 presents a comparison for 10 rounds of cuts generated at the root node,
where a round means a cut for every integer-constrained fractional variable. In
this experiment, the problems are preprocessed with COIN CglPreproces pro-
cedure and then 10 rounds of cuts are generated. The test set consists of 65
problems from the MIPLIB.3 library. The four methods compared are mixed
integer Gomory (MIG) cuts, and the three variants of lift-and-project cuts pre-
sented in this paper: Variant 1 (Balas and Perregaard’s algorithm cf. Sections 3
and 4), Variant 2 (the algorithm using the most violated cut selection rule, cf.
Section 5) and Variant 3 (the algorithm using disjunctive modularization cf. Sec-
tion 6). For each of the methods, we report the running time, the percentage of
the initial integrality gap closed, and the average violation for each cut generated
in 10 rounds (where the violation is the change in the objective of (CGLP)k after
each cut added).
As can be seen from the table, generating lift-and-project cuts with the three
diﬀerent variants proposed here is not much more expensive than generating
MIG cuts. For our test set, it took on the average 3.566 seconds per instance to
perform 10 rounds of MIG cuts, while it took 19, 19 and 15 seconds respectively
per instance for the three variants of lift-and-project cuts. Considering that cut
generation takes less than 5% of the total time needed to solve a mixed integer
program (see [10]), this diﬀerence is not signiﬁcant. This extra computational
cost made it possible to close a signiﬁcantly larger fraction of the integrality gap,
namely 54%, 55% and 54% with Variants 1, 2 and 3, respectively, versus 48% for
the MIG cuts. Of the 65 instances, there are only two (bell3a and p2756) on which
the MIG cuts close a slightly larger fraction of the gap than the three ﬂavors of
lift-and-project cuts. Even more striking is the diﬀerence in the strength of the
individual cuts, as measured by the amount of their violation by the current LP
solution: it is on the average 2.4 times as large for the lift-and-project cuts as it
is for the MIG cuts.
To more thoroughly assess the eﬀectiveness of lift-and-project cuts, it is of course
necessary to solve the instances to completion by running a branch-and-cut code
and using these cuts to strengthen the LP relaxation. To this end, we present two
comparative experiments of complete resolution for the MIPLIB.3 problems. The
ﬁrst experiment, presented in Table 2, consists in performing 10 rounds of cut gen-
eration at the root node and then solving the problem by branch-and-bound with-
out further cut generation. In the second experiment, summarized in Table 3, 10
rounds of cuts are generated at the root node and 1 round is performed every 10
nodes of the branch-and-cut tree. Again, the four cut generation methods tested
are MIG cuts and the three variants of lift-and-project cuts. For all three variants,
the limit on the number of pivots is set to 10.
The branch-and-cut runs are performed by using Cbc (COIN-OR Branch and
Cut) with some speciﬁc settings: a two hours time limit for solving each problem
is imposed; all the default cut generation procedures of Cbc are deactivated; the
variable selection strategy used is strong branching with the default parameters
of Cbc (i.e. performing strong branching on the 5 most fractional variables); the
node selection strategy is best bound.

100
E. Balas and P. Bonami
Table 2. Comparing complete resolutions with cut-and-branch with 10 rounds of cuts.
S means solved within the time limit, T means aborted because of the time limit.
MIG Cuts
Lift-and-Project Cuts
Variant 1
Variant 2
Variant 3
status time (sec) # nodes status time (sec) # nodes status time (sec) # nodes status time (sec) # nodes
Group A:Instances solved with MIG cuts in less than 10 seconds
air03
S
0.52
1
S
1.14
1
S
0.82
1
S
1.18
1
dcmulti
S
4.27
57
S
6.46
51
S
6.49
41
S
6.61
51
egout
S
0.04
5
S
0.03
1
S
0.04
3
S
0.02
1
enigma
S
2.91
1168
S
2.62
931
S
3.00
1257
S
0.25
40
ﬁxnet6
S
4.78
49
S
10.26
43
S
27.16
147
S
11.21
67
ﬂugpl
S
0.38
243
S
0.44
309
S
0.49
347
S
0.44
309
gen
S
1.28
35
S
1.92
31
S
0.87
17
S
1.46
35
gt2
S
0.01
1
S
1.16
229
S
2.15
259
S
4.60
1041
khb05250
S
1.28
31
S
1.61
31
S
1.68
17
S
1.64
31
lseu
S
1.18
425
S
1.40
537
S
2.56
611
S
2.11
709
misc03
S
2.92
157
S
5.85
111
S
5.97
73
S
3.59
81
misc06
S
1.08
26
S
0.85
10
S
0.51
7
S
1.06
13
mitre
S
0.59
1
S
1.18
1
S
1.50
1
S
1.34
1
mod008
S
6.65
919
S
7.22
729
S
6.29
691
S
3.06
313
mod010
S
0.59
1
S
1.08
1
S
0.71
1
S
1.32
1
p0033
S
0.24
157
S
0.17
53
S
0.18
71
S
0.05
9
p0201
S
4.07
153
S
10.53
215
S
7.75
65
S
2.63
17
p0282
S
0.50
47
S
1.11
43
S
1.01
55
S
0.92
51
p0548
S
5.24
409
S
11.59
883
S
9.29
657
S
2.60
187
p2756
S
8.39
168
S
20.41
306
S
10.68
158
S
13.88
200
rentacar
S
4.42
13
S
4.81
13
S
5.16
13
S
4.80
13
rgn
S
2.07
527
S
4.93
533
S
2.71
367
S
3.47
363
stein27
S
3.34
873
S
3.33
877
S
3.58
891
S
3.52
893
vpm1
S
4.81
415
S
0.21
5
S
0.27
5
S
0.23
5
Group B: Instances solved with MIG cuts in a time between 10 seconds and 10 minutes
10teams
S
301.57
1091
S
561.25
1600
S
129.68
329
S
321.45
1030
bell3a
S
19.69
12871
S
27.31
19765
S
28.30
19205
S
21.42
12927
bell5
S
46.12
22015
S
72.72
29655
S
20.82
9215
S
43.22
17755
blend2
S
27.73
2117
S
25.43
1723
S
7.66
271
S
40.36
2725
cap6000
S
311.07
1557
S
502.07
1923
S
465.71
1853
S
445.67
1825
dsbmip
S
15.26
168
S
16.00
159
S
12.15
145
S
43.14
528
ﬁber
S
428.58
8339
S
115.94
2607
S
32.48
257
S
109.52
923
gesa3
S
34.59
483
S
20.82
129
S
12.98
87
S
29.17
219
gesa3 o
S
36.45
591
S
48.02
353
S
57.45
319
S
32.43
225
l152lav
S
189.21
657
S
157.63
465
S
214.21
293
S
280.03
439
misc07
S
148.21
3745
S
229.06
4913
S
182.43
4161
S
235.73
4593
nw04
S
10.47
1
S
14.05
1
S
33.34
1
S
71.95
1
qnet1
S
170.77
567
S
121.46
263
S
94.20
287
S
214.00
489
qnet1 o
S
35.52
171
S
80.97
261
S
39.46
131
S
68.18
189
stein45
S
125.43
9819
S
119.04
11767
S
113.12
10093
S
118.13
11381
vpm2
S
401.08
40191
S
165.28
13717
S
267.30
19531
S
309.16
23561
Group C: Instances solved with MIG cuts in more than 10 minutes or unsolved
air04
S
4244.27
903
S
2945.54
689
S
6559.62
841
S
2057.31
467
air05
S
1872.60
1199
S
3048.48
1055
S
6301.18
1783
S
6061.42
1795
gesa2
S
1742.22
34263
S
3525.33
92709
S
3574.77
74509
S
3843.77
83425
gesa2 o
T
7200.61
90683
T
7199.81
97291
T
7201.03
79284
T
7200.91
88376
mas76
S
3643.00
765927
S
2729.41
730081
S
1733.07
783863
S
2104.06
731935
mod011
T
7199.75
19457
T
7200.16
17990
T
7199.88
16488
T
7200.20
13504
modglob
S
2140.48
257313
S
714.17
38231
S
1122.85
68141
S
563.46
29151
pk1
S
656.52
318694
S
665.89
321034
S
651.76
328540
S
681.34
357178
pp08a
S
1164.49
55853
S
745.10
30769
S
261.32
12081
S
537.56
26211
pp08aCUTS
S
962.24
45755
S
646.92
20095
S
782.85
29135
S
869.54
23443
qiu
S
3077.94
8505
S
4278.59
4665
T
7200.33
3647
S
3864.11
4665
Average
—
711.1
3.351e+04
—
707.4
2.843e+04
—
870.6
2.883e+04
—
734.1
2.83e+04
Geo. Mean
—
23.226
599.19
—
27.901
508.96
—
25.4
420.4
—
25.897
428.82
In Table 2, for each problem and each method, we indicate the status, the
computing time and the number of nodes to solve the problem. Averages and

New Variants of Lift-and-Project Cut Generation from the LP Tableau
101
geometric means are reported in the last two lines of the table. Among the
65 problems of the MIPLIB.3, 14 were not solved in the two hours time limit
with any of the methods tested (namely arki01, dano3mip, danoint, fast0507,
harp2, mas74, markshare1, markshare2, mkc, noswot, rout, set1ch, seymour
and swath). We do not include statistics for these 14 problems.
As Table 2 shows, the average size of the branch-and-bound trees generated
by each of Variants 1, 2 and 3 is about 15% smaller than the one obtained with
the MIG cuts. The average time needed to solve an instance remains roughly
the same for Variants 1 and 3 as for the MIG cuts, and increases by a ﬁfth for
Variant 2.
The experiment reported in Table 2 was in cut-and-branch mode, in that cuts
were only generated at the root node. Our next experiment explores the use
of cuts in the branch-and-cut mode: it generates 10 rounds of cuts at the root
node, and one round of cuts at every 10-th node of the branch-and-bound tree.
A summary of its results are reported in Table 3 (we only report averages by
groups of instances the same as the ones constituted in Table 2). The complete
results are available at [9]
It is highly edifying to examine the eﬀect of cut generation in the branch-and-
bound tree. One would expect these extra cuts to reduce the size of the search
tree by making the linear programming relaxation tighter, evidently at some com-
putational cost not so much from generating the cuts as from the increased time
needed to solve the linear programs with more constraints. So our expectation was
for a decrease in the size of the tree, but an increase of the computing time per
node. Surprisingly, a very diﬀerent picture emerges from comparing Tables 2 and
3. The average number of search tree nodes is indeed smaller for Table 3, but only
by 1.4% in the case of MIG cuts and by 7%, 9% and 18% respectively for the three
variants of lift-and-project cuts. On the other hand, the total computing time is
reduced by 32% in the case of the MIG cuts, and by 31%, 26% and 35% respectively
for Variants 1, 2 and 3 of the lift-and-project cuts. In other words, adding cuts at
some nodes of the branch-and-bound tree has reduced, rather than increased, the
computing time per node. Another aspect of this ﬁnding is the fact that in many
instances an increase in the number of search tree nodes is accompanied by a de-
crease in the total number of pivots performed during the procedure (excluding
those used for cut generation, see [9]).
In trying to explain this strange phenomenon, we looked in detail at several
runs and found that the most likely explanation lies in the fact that the cuts
added at some nodes tend to substantially enhance the power of reduced cost
ﬁxing. In other words, they help ﬁx more 0-1 variables whose reduced cost ex-
ceeds the diﬀerence between the value of the incumbent solution and the current
upper bound (diﬀerence which is reduced as the result of adding the cuts), and
thereby they facilitate the solution of the subproblems rather than making it
harder. This explanation is partially corroborated by the detailed data that we
were able to retrieve for a few instances, in the sense that in all cases the number
of variables ﬁxed by reduced cost throughout the run is signiﬁcantly larger (by
anywhere from 15% to 50% to even 400% in one case) for the runs of Table 3

102
E. Balas and P. Bonami
Table 3. Summary of the results comparing complete resolutions with branch-and-cut,
generating 10 rounds of cuts at the root node and then one round every 10 nodes
MIG Cuts
Lift-and-Project Cuts
Variant 1
Variant 2
Variant 3
# instances time (sec) # nodes time (sec) # nodes time (sec) # nodes time (sec) # nodes
Group A
24
2.126
214
2.789
195.9
3.065
177.2
3.042
168.9
Group B
16
77.15
3649
75.56
3276
72.84
3150
123.3
3827
Group C
11
2135
1.475e+05
2154
1.183e+05
2898
1.163e+05
2020
1.013e+05
Average
–
485.8
3.305e+04
489.5
2.663e+04
649.3
2.615e+04
475.9
2.312e+04
Geo. Mean
–
17.307
510.8
19.734
432.86
19.324
383.63
20.243
409.7
than for those of Table 2, but this does not solve the mystery, which requires
further study.
As it is well known that cutting planes tend to play a more signiﬁcant role
in solving hard instances than easy ones (easy instances are often solved faster
without cutting planes), we turned our attention to the behavior of our proce-
dures on the hardest of the instances that we solved. There were 11 instances
whose solution required over 10 minutes, but that were nevertheless solved within
our time limit of 2 hours, and their data are collected in Table 4for the case of
branch-and-cut with 10 rounds of cuts at the root node and another round of
cuts after every 10-th node (the same runs described in Table 3). Out of these 11
instances, the procedure using MIG cuts was fastest in 3 cases, whereas Variants
1, 2 and 3 of the lift-and project based procedure were fastest in 1, 3 and 4 cases,
respectively. Similarly, in terms of the number of branch and bound tree nodes
generated, the MIG cuts did best in 2 instances, whereas Variants 1, 2 and 3
were best in 2, 2 and 4 instances, respectively. Table 4 shows in boldface the best
Table 4. Comparing branch-and-cut with 10 rounds of cuts at the root node and one
round at every 10 nodes, on the 11 instances requiring more than 10 minutes. The best
performers for each instance (in terms of time and nodes) are boldfaced.
MIG Cuts
Lift-and-Project Cuts
Variant 1
Variant 2
Variant 3
status time (sec) # nodes status time (sec) # nodes status time (sec) # nodes status time (sec)
# nodes
air04
S
2310.95
1027
S
1766.50
685
S
1220.82
533
S
1289.52
481
air05
S
890.55
865
S
1364.21
1085
S
2688.23
1965
S
1500.95
1115
gesa2
S
1089.67
23899
S
791.22
17767
S
1834.55
39627
S
1337.58
31899
gesa2 o
S
3266.18
50109
S
3630.03
58059
T
>7208.71
>76979
S
5962.31
100257
mas76
S
3965.26
609723
S
6293.23
734951
T
>7201.54
>740897
S
2414.62
557405
mod011
S
4907.91
23463
S
4950.25
22453
S
6118.13
25097
S
6034.56
23825
modglob
S
2517.83
392369
S
752.74
49065
S
653.60
43677
S
388.67
22811
pk1
S
764.46
354890
S
805.46
321690
S
777.89
297800
S
752.74
291104
pp08a
S
1575.11
120203
S
1038.75
67785
S
323.80
18977
S
677.60
43437
pp08aCUTS
S
858.07
40175
S
479.43
20751
S
472.34
23113
S
833.71
37959
qiu
S
1344.05
5429
S
1819.18
6649
S
3375.35
10315
S
1030.98
3561
Average
—
2135
1.475e+05
—
2154
1.183e+05
—
2898
1.163e+05
—
2020
1.013e+05
Geo. Mean
—
1758.9
33285
—
1543.5
24451
—
1760
25885
—
1393
22853

New Variants of Lift-and-Project Cut Generation from the LP Tableau
103
performers for each instance. The geometric means of the computing time and
of the number of search tree nodes for Variant 3 of the L&P procedure are less
than the corresponding means for the MIG cut-based procedure by 48% and 31
%, respectively.
Generating lift-and-project cuts from the LP simplex tableau rather than
the higher dimensional Cut Generating Linear Program is a new approach (the
correspondence making this possible was discovered around 2002 [6] and its ﬁrst
implementation [10], corresponding more or less to our Variant 1, was done in
2003). Therefore the parameters used in our runs reported in this paper are
ﬁrst choices, to be improved upon by further research and experimentation. It
is therefore legitimate to also look at the performance of the best of the three
Variants in comparison with the classical MIG cuts on this set of hard problems.
The result of that comparison is that the ”best of three” is the fastest on 8 of the
11 instances, and generates the fewest search tree nodes on 9 of the 11 instances.
References
1. E. Balas, “Disjunctive Programming.” Annals of Discrete Mathematics, 5, 1979,
3-51.
2. E. Balas, “Generating deepest mixed integer cuts by disjunctive modularization.”
Tepper School of Business, Carnegie Mellon University, May 2002.
3. E. Balas, S. Ceria, and G. Cornu´ejols, “A lift-and-project cutting plane algorithm
for mixed 0-1 programs.” Mathematical Programming, 58 1993, 295-324.
4. E. Balas, S. Ceria, G. Cornu´ejols, “Mixed 0-1 programming by lift-and-project in
a branch-and-cut framework.” Man. Science, 112, 1996, 1229-1246.
5. E. Balas, S. Ceria, G. Cornu´ejols, and Natraj, “Gomory cuts revisited.” OR Letters,
19, 1996, 1-10.
6. E. Balas and M. Perregaard, “A precise correspondence between lift-and-project
cuts, simple disjunctive cuts, and mixed integer Gomory cuts for 0-1 programming.”
Math. Program. B, 94 2003, 221-245.
7. E. Balas and A. Saxena, “Optimizing over the split closure.” MSRR# 674, Tepper
School of Business, Carnegie Mellon University, 2005. To appear in Math Program-
ming A.
8. COIN-OR website: http://www.coin-or.org/
9. CglLandP: https://projects.coin-or.org/Cgl/wiki/CglLandP.
10. M. Perregaard, “A practical implementation of lift-and-project cuts. International
Symposium on Mathematical Programming, Copenhagen (2003).
11. M. Perregaard, “Generating Disjunctive Cuts for Mixed Integer Programs.” Ph.D.
Thesis, Carnegie Mellon University, 2003.

Orbital Branching
James Ostrowski1, JeﬀLinderoth1, Fabrizio Rossi2, and Stefano Smriglio2
1 Department of Industrial and Systems Engineering, Lehigh University,
200 W. Packer Ave., Bethlehem, PA 18015, USA
{jao204,jtl3}@lehigh.edu
2 Dipartimento di Informatica, Universit`a di L’Aquila, Via Vetoio I-67010 Coppito
(AQ), Italy
{rossi,smriglio}@di.univaq.it
Abstract. We introduce orbital branching, an eﬀective branching method
for integer programs containing a great deal of symmetry. The method is
based on computing groups of variables that are equivalent with respect to
the symmetry remaining in the problem after branching, including sym-
metry which is not present at the root node. These groups of equivalent
variables, called orbits, are used to create a valid partitioning of the fea-
sible region which signiﬁcantly reduces the eﬀects of symmetry while still
allowing a ﬂexible branching rule. We also show how to exploit the sym-
metries present in the problem to ﬁx variables throughout the branch-and-
bound tree. Orbital branching can easily be incorporated into standard IP
software. Through an empirical study on a test suite of symmetric inte-
ger programs, the question as to the most eﬀective orbit on which to base
the branching decision is investigated. The resulting method is shown to
be quite competitive with a similar method known as isomorphism prun-
ing and signiﬁcantly better than a state-of-the-art commercial solver on
symmetric integer programs.
1
Introduction
In this work, we focus on packing and covering integer programs (IP)s of the
form
max
x∈{0,1}n{eT x | Ax ≤e} and
(PIP)
min
x∈{0,1}n{eTx | Ax ≥e} ,
(CIP)
where A ∈{0, 1}m×n, and e is a vector of ones of conformal size. Our particular
focus is on cases when (CIP) or (PIP) is highly-symmetric, a concept we formalize
as follows. Let Πn be the set of all permutations of In = {1, . . . , n}. Given a
permutation π ∈Πn and a permutation σ ∈Πm, let A(π, σ) be the matrix
obtained by permuting the columns of A by π and the rows of A by σ, i.e.
A(π, σ) = PσAPπ, where Pσ and Pπ are permutation matrices. The symmetry
group G of the matrix A is the set of permutations
G(A)
def
= {π ∈Πn | ∃σ ∈Πm such that A(π, σ) = A} .
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 104–118, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

Orbital Branching
105
So, for any π ∈G(A), if ˆx is feasible for (CIP) or (PIP) (or the LP relaxations
of (CIP) or (PIP)), then if the permutation π is applied to the coordinates of ˆx,
the resulting solution, which we denote as π(ˆx), is also feasible. Moreover, the
solutions ˆx and π(ˆx) have equal objective value.
This equivalence of solutions induced by symmetry is a major factor that might
confoundthebranch-and-boundprocess.For example,supposeˆxisa (non-integral)
solution to an LP relaxation of PIP or CIP, with 0 < ˆxj < 1, and the decision
is made to branch down on variable xj by ﬁxing xj = 0. If ∃π ∈G(A) such that
[π(ˆx)]j = 0, then π(ˆx) is a feasible solution for this child node, and eT ˆx = eT (π(ˆx)),
so the relaxation value for the child node will not change. If the cardinality of G(A)
is large, then there are many permutations through which the parent solution of
the relaxation can be preserved in this manner, resulting in many branches that do
not change the bound on the parent node. Symmetry has long been recognized as
a curse for solving integer programs, and auxiliary (often extended) formulations
are often sought that reduce the amount of symmetry in an IP formulation [1,2,3].
In addition, there is a body of research on valid inequalities that can help exclude
symmetric feasible solutions [4,5,6]. Kaibel and Pfetsch [7] formalize many of these
arguments by deﬁning and studying the properties of a polyhedron known as an
orbitope, the convex hull of lexicographically maximal solutions with respect to a
symmetry group. Kaibel et al. [8] then use the properties of orbitopes to remove
symmetry in partitioning problems.
A diﬀerent idea, isomorphism pruning, introduced by Margot [9,10] in the con-
text of IP and dating back to Bazaraa and Kirca [11], examines the symmetry
group of the problem in order to prune isomorphic subproblems of the enumera-
tion tree. The branching method introduced in this work, orbital branching, also
uses the symmetry group of the problem. However, instead of examining this group
to ensure that an isomorphic node will never be evaluated, the group is used to
guide the branching decision. At the cost of potentially evaluating isomorphic sub-
problems, orbital branching allows for considerably more ﬂexibility in the choice of
branching entity than isomorphism pruning. Furthermore, orbital branching can
be easily incorporated within a standard MIP solver and even exploit problem
symmetry that may only be locally present at a nodal subproblem.
The remainder of the paper is divided into ﬁve sections. In Sect. 2 we give some
mathematical preliminaries. Orbital branching is introduced and formalized in
Sect. 3, and a mechanism to ﬁx additional variables based on symmetry con-
siderations called orbital ﬁxing is described there. A more complete comparison
to isomorphism pruning is also presented in Sect. 3. Implementation details are
provided in Sect. 4, and computational results are presented in Sect. 5. Conclu-
sions about the impact of orbital branching and future research directions are
given in Sect. 6.
2
Preliminaries
Orbital branching is based on elementary concepts from algebra that we recall in
this section to make the presentation self-contained. Some deﬁnitions are made

106
J. Ostrowski et al.
in terms of an arbitrary permutation group Γ, but for concreteness, the reader
may consider the group Γ to be the symmetry group of the matrix G(A).
For a set S ⊆In, the orbit of S under the action of Γ is the set of all subsets
of In to which S can be sent by permutations in Γ, i.e.,
orb(S, Γ)
def
= {S′ ⊆In | ∃π ∈Γ such that S′ = π(S)} .
In the orbital branching we are concerned with the orbits of sets of cardinality
one, corresponding to decision variables xj in PIP or CIP. By deﬁnition, if j ∈
orb({k}, Γ), then k ∈orb({j}, Γ), i.e. the variable xj and xk share the same
orbit. Therefore, the union of the orbits
O(Γ)
def
=
n

j=1
orb({j}, Γ)
forms a partition of In = {1, 2, . . ., n}, which we refer to as the orbital partition
of Γ, or simply the orbits of Γ. The orbits encode which variables are “equivalent”
with respect to the symmetry Γ.
The stabilizer of a set S ⊆In in Γ is the set of permutations in Γ that send
S to itself.
stab(S, Γ) = {π ∈Γ | π(S) = S} .
The stabilizer of S is a subgroup of Γ.
We characterize a node a = (F a
1 , F a
0 ) of the branch-and-bound enumeration
tree by the indices of variables ﬁxed to one F a
1 and ﬁxed to zero F a
0 at node a.
The set of free variables at node a is denoted by N a = In \ F a
0 \ F a
1 . At node a,
the set of feasible solutions to (CIP) or (PIP) is denoted by F(a), and the value
of an optimal solution for the subtree rooted at node a is denoted as z∗(a).
3
Orbital Branching
In this section we introduce orbital branching, an intuitive way to exploit the
orbits of the symmetry group G(A) when making branching decisions. The clas-
sical 0-1 branching variable dichotomy does not take advantage of the problem
information encoded in the symmetry group. To take advantage of this infor-
mation in orbital branching, instead of branching on individual variables, orbits
of variables are used to create the branching dichotomy. Informally, suppose
that at the current subproblem there is an orbit of cardinality k in the orbital
partitioning. In orbital branching, the current subproblem is divided into k + 1
subproblems: the ﬁrst k subproblems are obtained by ﬁxing to one in turn each
variable in the orbit while the (k + 1)st subproblem is obtained by ﬁxing all
variables in the orbit to zero. For any pair of variables xi and xj in the same
orbit, the subproblem created when xi is ﬁxed to one is essentially equivalent
to the subproblem created when xj is ﬁxed to one. Therefore, we can keep in

Orbital Branching
107
the subproblem list only one representative subproblem, pruning the (k −1)
equivalent subproblems. This is formalized below.
Let A(F a
1 , F a
0 ) be the matrix obtained by removing from the constraint matrix
A all columns in F a
0 ∪F a
1 and either all rows intersecting columns in F a
1 (CIP
case) or all columns nonorthogonal to columns in F a
1 (PIP case). Note that if
x ∈F(a) and x is feasible with respect to the matrix A, then x is feasible with
respect to the matrix A(F a
1 , F a
0 ).
Let O = {i1, i2, . . . , i|O|} ⊆N a be an orbit of the symmetry group G(A(F a
1 ,
F a
0 )). Given a subproblem a, the disjunction
xi1 = 1 ∨xi2 = 1 ∨. . . xiO = 1 ∨

i∈O
xi = 0
(1)
induces a feasible division of the search space. In what follows, we show that for
any two variables xj, xk ∈O, the two children a(j) and a(k) of a, obtained by
ﬁxing respectively xj and xk to 1 have the same optimal solution value. As a
consequence, disjunction (1) can be replaced by the binary disjunction
xh = 1 ∨

i∈O
xi = 0 ,
(2)
where h is a variable in O. Formally, we have Theorem 1.
Theorem 1. Let O be an orbit in the orbital partitioning O(G(A(F a
1 , F a
0 ))),
and let j, k be two variable indices in O. If a(j) = (F a
1 ∪{j}, F a
0 ) and a(k) =
(F a
1 ∪{k}, F a
0 ) are the child nodes created when branching on variables xj and
xk, then z∗(a(j)) = z∗(a(k)).
Proof. Let x∗be an optimal solution of a(j) with value z∗(a(j)). Obviously
x∗is also feasible for a. Since j and k are in the same orbit O, there exists a
permutation π ∈G(A(F a
1 , F a
0 )) such that π(j) = k. By deﬁnition, π(x∗) is a
feasible solution of a with value z∗(a(j)) such that xk = 1. Therefore, π(x∗) is
feasible for a(k), and z∗(a(k)) = z∗(a(j)).
⊓⊔
The basic orbital branching method is formalized in Algorithm 1.
Algorithm 1. Orbital Branching
Input:
Subproblem a = (F a
1 , F a
0 ), non-integral solution ˆx.
Output: Two child subproblems b and c.
Step 1. Compute orbital partition O(G(A(F a
1 , F a
0 ))) = {O1, O2, . . . , Op}.
Step 2. Select orbit Oj∗, j∗∈{1, 2, . . . , p}.
Step 3. Choose arbitrary k ∈Oj∗. Return subproblems b = (F a
1 ∪{k}, F a
0 ) and
c = (F a
1 , F a
0 ∪Oj∗).
The consequence of Theorem 1 is that the search space is limited, but orbital
branching has also the relevant eﬀect of reducing the likelihood of encountering

108
J. Ostrowski et al.
symmetric solutions. Namely, no solutions in the left and right child nodes of
the current node will be symmetric with respect to the local symmetry. This is
formalized in Theorem 2.
Theorem 2. Let b and c be any two subproblems in the enumeration tree. Let a
be the ﬁrst common ancestor of b and c. For any x ∈F(b) and π ∈G(A(F a
0 , F a
1 )),
π(x) does not belong F(c).
Proof. Suppose not, i.e., that there ∃x ∈F(b) and a permutation π ∈G(A(F a
0 ,
F a
1 )) such that π(x) ∈F(c). Let Oi ∈O(G(A(F a
1 , F a
0 ))) be the orbit chosen to
branch on at subproblem a. W.l.o.g. we can assume xk = 1 for some k ∈Oi.
We have that xk = [π(x)]π(k) = 1, but π(k) ∈Oi. Therefore, by the orbital
branching dichotomy, π(k) ∈F c
0 , so π(x) ̸∈F(c).
⊓⊔
Note that by using the matrix A(F a
1 , F a
0 ), orbital branching attempts to use sym-
metry found at all nodes in the enumeration tree, not just the symmetry found
at the root node. This makes it possible to prune nodes whose corresponding
solutions are not symmetric in the original IP.
3.1
Orbital Fixing
In orbital branching, all variables ﬁxed to zero and one are removed from the
constraint matrix at every node in the enumeration tree. As Theorem 2 demon-
strates, using orbital branching in this way ensures that any two nodes are not
equivalent with respect to the symmetry found at their ﬁrst common ancestor.
It is possible however, for two child subproblems to be equivalent with respect
to a symmetry group found elsewhere in the tree. In order to combat this type
of symmetry we perform orbital ﬁxing, which works as follows.
Consider the symmetry group G(A(F a
1 , ∅)) at node a. If there exists an orbit
O in the orbital partition O(G(A(F a
1 , ∅))) that contains variables such that O ∩
F a
0 ̸= ∅and O ∩N a ̸= ∅, then all variables in O can be ﬁxed to zero. In the
following theorem, we show that such variable setting (orbital ﬁxing) excludes
feasible solutions only if there exists a feasible solution of the same objective
value to the left of the current node in the branch and bound tree. (We assume
that the enumeration tree is oriented so that the branch with an additional
variable ﬁxed at one is the left branch).
To aid in our development,we introduce the concept of a focus node. For
x ∈F(a), we call node b(a, x) a focus node of a with respect to x if ∃y ∈F(b)
such that eT x = eT y and b is found to the left of a in the tree.
Theorem 3. Let {O1, O2, . . . Oq} be an orbital partitioning of G(A(F a
1 , ∅)) at
node a, and let the set
S
def
= {j ∈N a | ∃k ∈F a
0 and j, k ∈Oℓfor some ℓ∈{1, 2, . . .q}}
be the set of free variables that share an orbit with a variable ﬁxed to zero at a.
If x ∈F(a) with xi = 1 for some i ∈S, then there exists a focus node for a with
respect to x.

Orbital Branching
109
Proof. Suppose that a is the ﬁrst node in any enumeration tree where S is non-
empty. Then, there exist j ∈F a
0 and i ∈S such that i ∈orb({j}, G(A(F a
1 , ∅))),
i.e., there exists a π ∈G(A(F a
1 , ∅)) with π(i) = j. W.l.o.g., suppose that j is
any of the ﬁrst such variables ﬁxed to zero on the path from the root node to
a and let c be the subproblem in which such a ﬁxing occurs. Let ρ(c) be the
parent node of c. By our choice of j as the ﬁrst ﬁxed variable, for all i ∈F a
0 , we
have xπ(i) = 0. Then, there exists x ∈F(a) with xi = 1 such that π(x) is not
feasible in a (since it does not satisfy the bounds) but it is feasible in ρ(c) and
has the same objective value of x. Since j was ﬁxed by orbital branching then
the left child of ρ(c) has xh = 1 for some h ∈orb({j}, G(A(F ρ(c)
1
, F ρ(c)
0
))). Let
π′ ∈G(A(F ρ(c)
1
, F ρ(c)
0
)) have π′(j) = h. Then π′(π(x)) is feasible in the left node
with the same objective value of x. The left child node of ρ(c) is then the focus
node of a with respect to x.
If a is not a ﬁrst node in the enumeration tree one can apply the same argu-
ment to the ﬁrst ancestor b of a such that S ̸= ∅. The focus node of c = (b, x) is
then a focus node of (a, x).
⊓⊔
An immediate consequence of Theorem 3 is that for all i ∈F a
0 and for all
j ∈orb({i}, G(A(F a
1 , ∅))) one can set xj = 0. We update orbital branching to
include orbital ﬁxing in Algorithm 2.
Algorithm 2. Orbital Branching with Orbital Fixing
Input:
Subproblem a = (F a
1 , F a
0 ) (with free variables N a = In \ F a
1 \ F a
0 ), frac-
tional solution ˆx.
Output: Two child nodes b and c.
Step 1. Compute orbital partition O(G(A(F a
1 , ∅))) = { ˆO1, ˆO2, . . . , ˆOq}. Let S
def
=
{j ∈N a | ∃k ∈F a
0 and (j ∩k) ∈ˆOℓfor some ℓ∈{1, 2, . . . q}}.
Step 2. Compute orbital partition O(G(A(F a
1 , F a
0 ))) = {O1, O2, . . . , Op}.
Step 3. Select orbit Oj∗, j∗∈{1, 2, . . . , p}.
Step 4. Choose arbitrary k ∈Oj∗. Return child subproblems b = (F a
1 ∪{k}, F a
0 ∪S)
and c = (F a
1 , F a
0 ∪Oj∗∪S).
In orbital ﬁxing, the set S of additional variables set to zero is a function of
F a
0 . Variables may appear in F a
0 due to a branching decision or due to traditional
methods for variable ﬁxing in integer programming, e.g. reduced cost ﬁxing or
implication-based ﬁxing. Orbital ﬁxing, then, gives a way to enhance traditional
variable-ﬁxing methods by including the symmetry present at a node of the
branch and bound tree.
3.2
Comparison to Isomorphism Pruning
The fundamental idea behind isomorphism pruning is that for each node a =
(F a
1 , F a
0 ), the orbits orb(F a
1 , G(A)) of the “equivalent” sets of variables to F a
1 are

110
J. Ostrowski et al.
computed. If there is a node b = (F b
1, F b
0 ) elsewhere in the enumeration tree such
that F b
1 ∈orb(F a
1 , G(A)), then the node a need not be evaluated—the node a is
pruned by isomorphism. A very distinct and powerful advantage of this method
is that no nodes whose sets of ﬁxed variables are isomorphic will be evaluated.
One disadvantage of this method is that computing orb(F a
1 , G(A)) can require
computational eﬀort on the order of O(n|F a
1 |!). A more signiﬁcant disadvantage
of isomorphism pruning is that orb(F a
1 , G(A)) may contain many equivalent sub-
sets to F a
1 , and the entire enumeration tree must be compared against this list to
ensure that a is not isomorphic to any other node b. In a series of papers, Margot
oﬀers a way around this second disadvantage [9,10]. The key idea introduced is
to declare one unique representative among the members of orb(F a
1 , G(A)), and
if F a
1 is not the unique representative, then the node a may safely be pruned.
The advantage of this extension is that it is trivial to check whether or not node
a may be pruned once the orbits orb(F a
1 , G(A)) are computed. The disadvantage
of the method is ensuring that the unique representative occurs somewhere in the
branch and bound tree requires a relatively inﬂexible branching rule. Namely, all
child nodes at a ﬁxed depth must be created by branching on the same variable.
Orbital branching does not suﬀer from this inﬂexibility. By not focusing on
pruning all isomorphic nodes, but rather eliminating the symmetry through
branching, orbital branching oﬀers a great deal more ﬂexibility in the choice
of branching entity. Another advantage of orbital branching is that by using the
symmetry group G(A(F a
1 , F a
0 )), symmetry introduced as a result of the branching
process is also exploited.
Both methods allow for the use of traditional integer programming methodolo-
gies such as cutting planes and ﬁxing variables based on considerations such as
reduced costs and implications derived from preprocessing. In isomorphism prun-
ing, for a variable ﬁxing to be valid, it must be that all non-isomorphic optimal
solutions are in agreement with the ﬁxing. Orbital branching does not suﬀer from
this limitation. A powerful idea in both methods is to combine the variable ﬁxing
with symmetry considerations in order to ﬁx many additional variables. This idea
is called orbit setting in [10] and orbital ﬁxing in this work (see Sect. 3.1).
4
Implementation
The orbital branching method has been implemented using the user application
functions of MINTO v3.1 [12]. The branching dichotomy of Algorithm 1 or 2
is implemented in the appl divide() method, and reduced cost ﬁxing is im-
plemented in appl bounds(). The entire implementation, including code for all
the branching rules subsequently introduced in Sect. 4.2 consists of slightly over
1000 lines of code. All advanced IP features of MINTO were used, including
clique inequalities, which can be useful for instances of (PIP).
4.1
Computing G(·)
Computation of the symmetry groups required for orbital branching and orbital
ﬁxing is done by computing the automorphism group of a related graph. Recall

Orbital Branching
111
that the automorphism group Aut(G(V, E)) of a graph G = (V, E), is the set of
permutations of V that leave the incidence matrix of G unchanged, i.e.
Aut(G(V, E)) = {π ∈Π|V | | (i, j) ∈E ⇔(π(i), π(j)) ∈E} .
The matrix A whose symmetry group is to be computed is transformed into a
bipartite graph G(A) = (N, M, E) where vertex set N = {1, 2, . . ., n} represents
the variables, and vertex set M = {1, 2, . . ., m} represents the constraints. The
edge (i, j) ∈E if and only if aij = 1. Under this construction, feasible solutions
to (PIP) are subsets of the vertices S ⊆N such that each vertex i ∈M is
adjacent to at most one vertex j ∈S. In this case, we say that S packs M.
Feasible solutions to (CIP) correspond to subsets of vertices S ⊆N such that
each vertex i ∈M is adjacent to at least one vertex j ∈S, or S covers M. Since
applying members of the automorphism group preserves the incidence structure
of a graph, if S packs (covers) M, and π ∈stab(M, Aut(G(A))), then there exists
a σ ∈Πm such that σ(M) = M and π(S) packs (covers) σ(M). This implies that
if π ∈stab(M, Aut(G(A))), then the restriction of π to N must be an element of
G(A), i.e. using the graph G(A), one can ﬁnd elements of symmetry group G(A).
In particular, we compute the orbital partition of the stabilizer of the constraint
vertices M in the automorphism group of G(A), i.e.
O(stab(M, Aut(G(A)))) = {O1, O2, . . . , Op} .
The orbits O1, O2, . . . , Op in the orbital partition are such that if i ∈M and
j ∈N, then i and j are not in the same orbit. We can then refer to these orbits
as variable orbits and constraint orbits. In orbital branching, we are concerned
only with the variable orbits.
There are several softwarepackages that can compute the automorphism groups
required to perform orbital branching. The program nauty [13], by McKay, has
been shown to be quite eﬀective [14], and we use nauty in our orbital branching
implementation.
The complexity of computing the automorphism group of a graph is not
known to be polynomial time. However, nauty was able to compute the symme-
try groups of our problems very quickly, generally faster than solving an LP at
a given node. One explanation for this phenomenon is that the running time of
nauty’s backtracking algorithm is correlated to the size of the symmetry group
being computed. For example, computing the automorphism group of the clique
on 2000 nodes takes 85 seconds, while graphs of comparable size with little or no
symmetry require fractions of a second. The orbital branching procedure quickly
reduces the symmetry group of the child subproblems, so explicitly recomputing
the group by calling nauty is computational very feasible. In the table of results
presented in the Appendix, we state explicitly the time required in computing
automorphism groups by nauty.
4.2
Branching Rules
The orbital branching rule introduced in Sect. 3 leaves signiﬁcant freedom in
choosing the orbit on which to base the partitioning. In this section, we discuss

112
J. Ostrowski et al.
mechanisms for deciding on which orbit to branch. As input to the branching de-
cision, we are given a fractional solution ˆx and orbits O1, O2, . . . Op (consisting of
all currently free variables) of the orbital partitioning O(G(A(F a
0 , F a
1 ))) for the
subproblem at node a. Output of the branching decision is an index j∗of an orbit
on which to base the orbital branching. We tested six diﬀerent branching rules.
Rule 1: Branch Largest: The ﬁrst rule chooses to branch on the largest orbit
Oj∗:
j∗∈arg
max
j∈{1,...p} |Oj| .
Rule 2: Branch Largest LP Solution: The second rule branches on the orbit
Oj∗whose variables have the largest total solution value in the fractional solution
ˆx:
j∗∈arg
max
j∈{1,...p} ˆx(Oj) .
Rule 3: Strong Branching: The third rule is a strong branching rule. For each
orbit j, two tentative child nodes are created and their bounds z+
j and z−
j are
computed by solving the resulting linear programs. The orbit j∗for which the
product of the change in linear program bounds is largest is used for branching:
j∗∈arg
max
j∈{1,...p}(|eT ˆx −z+
j |)(|eT ˆx −z−
j |) .
Note that if one of the potential child nodes in the strong branching procedure
would be pruned, either by bound or by infeasibility, then the bounds on the
variables may be ﬁxed to their values on the alternate child node. We refer to
this as strong branching ﬁxing, and in the computational results in the Appendix,
we report the number of variables ﬁxed in this manner. As discussed at the end
of Sect. 3.1, variables ﬁxed by strong branching ﬁxing may result in additional
variables being ﬁxed by orbital ﬁxing.
Rule 4: Break Symmetry Left: This rule is similar to strong branching, but
instead of ﬁxing a variable and computing the change in objective value bounds,
we ﬁx a variable and compute the change in the size of the symmetry group.
Speciﬁcally, for each orbit j, we compute the size of the symmetry group in
the resulting left branch if orbit j (including variable index ij) was chosen for
branching, and we branch on the orbit that reduces the symmetry by as much
as possible:
j∗∈arg
min
j∈{1,...p} (|G(A(F a
1 ∪{ij}, F a
0 ))|) .
Rule 5: Keep Symmetry Left: This branching rule is the same as Rule 4,
except that we branch on the orbit for which the size of the child’s symmetry
group would remain the largest:
j∗∈arg
max
j∈{1,...p} (|G(A(F a
1 ∪{ij}, F a
0 ))|) .
Rule 6: Branch Max Product Left: This rule attempts to combine the fact
that we would like to branch on a large orbit at the current level and also keep

Orbital Branching
113
a large orbit at the second level on which to base the branching dichotomy.
For each orbit O1, O2, . . . , Op, the orbits P j
1 , P j
2 , . . . , P j
q of the symmetry group
G(A(F a
1 ∪{ij}, F a
0 )) of the left child node are computed for some variable index
ij ∈Oj. We then choose to branch on the orbit j∗for which the product of the
orbit size and the largest orbit of the child subproblem is largest:
j∗∈arg
max
j∈{1,...p}

|Oj|(
max
k∈{1,...q} |P j
k|)

.
5
Computational Experiments
In this section, we give empirical evidence of the eﬀectiveness of orbital branch-
ing, we investigate the impact of choosing the orbit on which branching is based,
and we demonstrate the positive eﬀect of orbital ﬁxing. The computations are
based on the instances whose characteristics are given in Table 1. The instances
beginning with cod are used to compute maximum cardinality binary error cor-
recting codes [15], the instances whose names begin with cov are covering designs
[16], the instance f5 is the “football pool problem” on ﬁve matches [17], and the
instances sts are the well-known Steiner-triple systems [18]. The cov formu-
lations have been strengthened with a number of Sch¨oenheim inequalities, as
derived by Margot [19]. All instances, save for f5, are available from Margot’s
web site: http://wpweb2.tepper.cmu.edu/fmargot/lpsym.html.
Table 1. Symmetric Integer Programs
Name Variables
cod83
256
cod93
512
cod105
1024
cov1053
252
cov1054
2252
cov1075
120
cov1076
120
cov954
126
f5
243
sts27
27
sts45
45
The computations were run on ma-
chines with AMD Opteron proces-
sors clocked at 1.8GHz and having
2GB of RAM. The COIN-OR soft-
ware Clp was used to solve the lin-
ear programs at nodes of the branch
and bound tree. All code was com-
piled with the GNU family of compil-
ers using the ﬂags -O3 -m32. For each
instance, the (known) optimal solu-
tion value was set to aid pruning and
reduce the “random” impact of ﬁnd-
ing a feasible solution in the search.
Nodes were searched in a best-ﬁrst
fashion. When the size of the maxi-
mum orbit in the orbital partitioning
is less than or equal to two, nearly all
of the symmetry in the problem has
been eliminated by the branching procedure, and there is little use to perform
orbital branching. In this case, we use MINTO’s default branching strategy. The
CPU time was limited in all cases to four hours.
In order to succinctly present the results, we use performance proﬁles of Dolan
and Mor´e [20]. A performance proﬁle is a relative measure of the eﬀectiveness of
one solution method in relation to a group of solution methods on a ﬁxed set of

114
J. Ostrowski et al.
problem instances. A performance proﬁle for a solution method m is essentially
a plot of the probability that the performance of m (measured in this case with
CPU time) on a given instance in the test suite is within a factor of β of the best
method for that instance.
Figure 1 shows the results of an experiment designed to compare the perfor-
mance of the six diﬀerent orbital branching rules introduced in Sect. 4.2. In this
experiment, both reduced cost ﬁxing and orbital ﬁxing were used. A complete
table showing the number of nodes, CPU time, CPU time computing automor-
phism groups, the number of variables ﬁxed by reduced cost ﬁxing, orbital ﬁxing,
and strong branching ﬁxing, and the deepest tree level at which orbital branching
was performed is shown in the Appendix.
Prob(within factor ß of fastest)
ß
 0
 0.2
 0.4
 0.6
 0.8
 1
 1
 2
 4
 8
 16
 32
break−symmetry−left
keep−symmetry−left
branch−max−product−left
strong−branch
branch−largest−lp
branch−largest
Fig. 1. Performance Proﬁle of Branching Rules
A somewhat surprising result from the results depicted in Fig. 1 is that the
most eﬀective branching method was Rule 5, the method that keeps the sym-
metry group size large on the left branch. (This method gives the “highest”
line in Fig. 1). The second most eﬀective branching rule appears to be the rule
that tries to reduce the group size by as much as possible. While these methods
may not prove to be the most robust on a richer suite of diﬃcult instances, one
conclusion that we feel safe in making from this experiment is that considering
the impact on the symmetry of the child node of the current branching decision
is important. Another important observation is that for speciﬁc instances, the
choice of orbit on which to branch can have a huge impact on performance.
For example, for the instance cov1054, branching rules 4 and 5 both reduce the
number of child nodes to 11, while other mechanisms that do not consider the
impact of the branching decision on the symmetry of the child nodes cannot
solve the problem in four hours of computing time.
The second experiment was aimed at measuring the impact of performing
orbital ﬁxing, as introduced in Sect. 3.1. Using branching rule 5, each instance
in Table 1 was run both with and without orbital ﬁxing. Figure 2 shows a
performance proﬁle comparing the results in the two cases. The results shows
that orbital ﬁxing has a signiﬁcant positive impact.

Orbital Branching
115
Prob(within factor ß of fastest)
ß
 0
 0.2
 0.4
 0.6
 0.8
 1
 1
 2
 4
 8
 16
 32
 64
no−orbital−fixing
orbital−fixing
Fig. 2. Performance Proﬁle of Impact of Orbital Fixing
The ﬁnal comparison we make here is between orbital branching (with keep-
symmetry-left branching), the isomorphism pruning algorithm of Margot, and
the commercial solver CPLEX version 10.1, which has features for symmetry
detection and handling. Table 2 summarizes the results of the comparison. The
results for isomorphism pruning are taken directly from the paper of Margot
using the most sophisticated of his branching rules “BC4” [10]. The paper [10]
does not report results on sts27 or f5. The CPLEX results were obtained on
an Intel Pentium 4 CPU clocked at 2.40GHz. Since the results were obtained on
three diﬀerent computer architectures and each used a diﬀerent LP solver for
the child subproblems, the CPU times should be interpreted appropriately.
The results show that the number of subproblems evaluated by orbital
branching is smaller than isomorphism pruning in three cases, and in nearly
all cases, the number of nodes is comparable. For the instance cov1076, which
is not solved by orbital branching, a large majority of the CPU time is spent
computing symmetry groups at each node. In a variant of orbital branching that
Table 2. Comparison of Orbital Branching, Isomorphism Pruning, and CPLEX v10.1
Orbital Branching Isomorphism Pruning CPLEX v10.1
Instance Time
Nodes
Time
Nodes
Time
Nodes
cod83
2
25
19
33
391
32077
cod93
176
539
651
103
fail
488136
cod105
306
11
2000
15
1245
1584
cov1053
50
745
35
111
937
99145
cov1054
2
11
130
108
fail
239266
cov1075
292
377
118
169
141
10278
cov1076 fail
13707
3634
5121
fail
1179890
cov954
22
401
24
126
9
1514
f5
66
935
-
-
1150
54018
sts27
1
71
-
-
0
1647
sts45
3302
24317
31
513
24
51078

116
J. Ostrowski et al.
Table 3. Performance of Orbital Branching Rules on Symmetric IPs
Nauty
# Fixed # Fixed # Fixed
Deepest
Instance
Branching Rule
Time
Nodes
Time
by RCF
by OF
by SBF Orbital Level
cod105
Break Symmetry
305.68
11
22.88
0
1020
0
4
cod105
Keep Symmetry
306.47
11
22.92
0
1020
0
4
cod105
Branch Largest LP Solution
283.54
7
11.87
0
0
0
2
cod105
Branch Largest
283.96
9
18.01
0
0
0
3
cod105
Max Product Orbit Size
302.97
9
17.41
0
920
0
3
cod105
Strong Branch
407.14
7
11.85
0
1024
1532
2
cod83
Break Symmetry
2.35
25
1.09
44
910
0
7
cod83
Keep Symmetry
2.38
25
1.10
44
910
0
7
cod83
Branch Largest LP Solution
8.81
93
2.76
209
534
0
6
cod83
Branch Largest
10.03
113
3.41
183
806
0
14
cod83
Max Product Orbit Size
9.39
115
4.59
109
634
0
11
cod83
Strong Branch
9.44
23
0.97
27
878
394
6
cod93
Break Symmetry
175.47
529
75.15
3382
3616
0
17
cod93
Keep Symmetry
175.58
529
75.31
3382
3616
0
17
cod93
Branch Largest LP Solution 3268.89 12089
1326.26
181790
3756
0
29
cod93
Branch Largest
2385.80
8989
920.90
142351
4986
0
49
cod93
Max Product Orbit Size
587.06
2213
215.68
28035
1160
0
29
cod93
Strong Branch
2333.22
161
19.76
380
2406
13746
14
cov1053
Break Symmetry
50.28
745
27.51
0
836
0
33
cov1053
Keep Symmetry
50.31
745
27.54
0
836
0
33
cov1053 Branch Largest LP Solution 1841.41 23593
990.12
0
5170
0
71
cov1053
Branch Largest
148.37
2051
70.73
0
1504
0
36
cov1053
Max Product Orbit Size
192.18
2659
91.72
0
1646
0
68
cov1053
Strong Branch
1998.55
1455
53.96
0
5484
34208
54
cov1054
Break Symmetry
1.77
11
0.85
0
186
0
4
cov1054
Keep Symmetry
1.76
11
0.85
0
186
0
4
cov1054 Branch Largest LP Solution
14400
54448
7600.80
0
814
0
35
cov1054
Branch Largest
14400
54403
7533.80
0
1452
0
49
cov1054
Max Product Orbit Size
14400
52782
7532.77
0
1410
0
38
cov1054
Strong Branch
14400
621
87.76
0
204
4928
32
cov1075
Break Symmetry
14400
9387
13752.11
37121
0
0
2
cov1075
Keep Symmetry
291.85
377
268.45
379
926
0
15
cov1075 Branch Largest LP Solution
906.48
739
861.57
1632
716
0
23
cov1075
Branch Largest
268.49
267
248.45
793
1008
0
13
cov1075
Max Product Orbit Size
395.11
431
366.24
1060
1066
0
21
cov1075
Strong Branch
223.53
67
60.71
106
128
1838
10
cov1076
Break Symmetry
14400
8381
13853.35
2
0
0
3
cov1076
Keep Symmetry
14400
13707 13818.47
11271
1564
0
26
cov1076 Branch Largest LP Solution
14400
6481
13992.74
10
116
0
14
cov1076
Branch Largest
14400
6622
13988.71
0
176
0
13
cov1076
Max Product Orbit Size
14400
6893
13967.86
71
580
0
14
cov1076
Strong Branch
14400
1581
3255.74
5
164
58
23
cov954
Break Symmetry
21.72
401
14.81
570
1308
0
14
cov954
Keep Symmetry
21.70
401
14.83
570
1308
0
14
cov954
Branch Largest LP Solution
11.30
175
7.03
498
48
0
5
cov954
Branch Largest
15.69
265
10.51
671
212
0
12
cov954
Max Product Orbit Size
14.20
229
9.25
602
212
0
11
cov954
Strong Branch
17.55
45
1.74
50
100
1084
8
f5
Break Symmetry
65.86
935
23.25
2930
2938
0
17
f5
Keep Symmetry
65.84
935
23.26
2930
2938
0
17
f5
Branch Largest LP Solution
91.32
1431
28.95
7395
272
0
8
f5
Branch Largest
100.66
1685
30.75
7078
434
0
11
f5
Max Product Orbit Size
102.54
1691
30.96
7230
430
0
13
f5
Strong Branch
671.51
123
2.59
187
760
8586
15
sts27
Break Symmetry
0.84
71
0.71
0
8
0
10
sts27
Keep Symmetry
0.83
71
0.71
0
8
0
10
sts27
Branch Largest LP Solution
2.33
115
2.12
3
86
0
14
sts27
Branch Largest
0.97
73
0.83
1
28
0
13
sts27
Max Product Orbit Size
2.88
399
2.42
1
888
0
11
sts27
Strong Branch
1.63
75
1.15
2
76
0
14
sts45
Break Symmetry
3302.70 24317
3230.12
12
0
0
4
sts45
Keep Symmetry
3301.81 24317
3229.88
12
0
0
4
sts45
Branch Largest LP Solution 4727.29 36583
4618.66
25
0
0
2
sts45
Branch Largest
4389.80 33675
4289.45
36
0
0
2
sts45
Max Product Orbit Size
4390.39 33675
4289.79
36
0
0
2
sts45
Strong Branch
1214.04
7517
884.79
2
144
45128
21
uses a symmetry group that is smaller but much more eﬃcient to compute (and
which space prohibits us from describing in detail here), cov1076 can be solved
in 679 seconds and 14465 nodes. Since in any optimal solution to the Steiner
triple systems, more than 2/3 of the variables will be set to 1, orbital branching
would be much more eﬃcient if all variables were complemented, or equivalently
if the orbital branching dichotomy (2) was replaced by its complement. Margot
[10] also makes a similar observation, and his results are based on using the
complemented instances, which may account for the large gap in performance

Orbital Branching
117
of the two methods on sts45. We are currently instrumenting our code to deal
with instances for which the number of ones in an optimal solution is larger than
1/2. Orbital branching proves to be faster than CPLEX in six cases, while in all
cases the number of evaluated nodes is remarkably smaller.
6
Conclusion
In this work, we presented a simple way to capture and exploit the symmetry of an
integer program when branching. We showed through a suite of experiments that
the new method, orbital branching, outperforms state-of-the-art solvers when a
high degree of symmetry is present. In terms of reducing the size of the search tree,
orbital branching seems to be of comparable quality to the isomorphism pruning
method of Margot [10]. Further, we feel that the simplicity and ﬂexibility of orbital
branching make it an attractive candidate for further study. Continuing research
includes techniques for further reducing the number of isomorphic nodes that are
evaluated and on developing branching mechanisms that combine the child bound
improvement and change in symmetry in a meaningful way.
Acknowledgments
The authors would like to thank Kurt Anstreicher and Fran¸cois Margot for
inspiring and insightful comments on this work. In particular, the name orbital
branching was suggested by Kurt. Author Linderoth would like to acknowledge
support from the US National Science Foundation (NSF) under grant DMI-
0522796, by the US Department of Energy under grant DE-FG02-05ER25694,
and by IBM, through the faculty partnership program. Author Ostrowski is
supported by the NSF through the IGERT Grant DGE-9972780.
References
1. Barnhart, C., Johnson, E.L., Nemhauser, G.L., Savelsbergh, M.W.P., Vance, P.H.:
Branch and Price: Column generation for solving huge integer programs. Opera-
tions Research 46 (1998) 316–329
2. Holm, S., Sørensen, M.: The optimal graph partitioning problem: Solution method
based on reducing symmetric nature and combinatorial cuts. OR Spectrum 15
(1993) 1–8
3. M´endez-D´ıaz, I., Zabala, P.: A branch-and-cut algorithm for graph coloring. Dis-
crete Applied Mathematics 154(5) (2006) 826–847
4. Macambira, E.M., Maculan, N., de Souza, C.C.: Reducing symmetry of the SONET
ring assignment problem using hierarchical inequalities.
Technical Report ES-
636/04, Programa de Engenharia de Sistemas e Computa¸c˜ao, Universidade Federal
do Rio de Janeiro (2004)
5. Rothberg, E.: Using cuts to remove symmetry. Presented at the 17th International
Symposium on Mathematical Programming
6. Sherali, H.D., Smith, J.C.: Improving zero-one model representations via symmetry
considerations. Management Science 47(10) (2001) 1396–1407

118
J. Ostrowski et al.
7. Kaibel, V., Pfetsch, M.: Packing and partitioning orbitopes. Mathemathical Pro-
gramming (2007) To appear.
8. Kaibel, V., Peinhardt, M., Pfetsch, M.:
Orbitopal ﬁxing.
In: IPCO 2007: The
Twelfth Conference on Integer Programming and Combinatorial Optimization,
Springer (2007) To appear.
9. Margot, F.: Pruning by isomorphism in branch-and-cut. Mathematical Program-
ming 94 (2002) 71–90
10. Margot, F.:
Exploiting orbits in symmetric ILP.
Mathematical Programming,
Series B 98 (2003) 3–21
11. Bazaraa, M.S., Kirca, O.: A branch-and-bound heuristic for solving the quadratic
assignment problem. Naval Research Logistics Quarterly 30 (1983) 287–304
12. Nemhauser, G.L., Savelsbergh, M.W.P., Sigismondi, G.C.: MINTO, a Mixed IN-
Teger Optimizer. Operations Research Letters 15 (1994) 47–58
13. McKay, B.D.: Nauty User’s Guide (Version 1.5). Australian National University,
Canberra. (2002)
14. Foggia, P., Sansone, C., Vento, M.: A preformance comparison of ﬁve algorithms
for graph isomorphism. Proc. 3rd IAPR-TC15 Workshop Graph-Based Represen-
tations in Pattern Recognition (2001) 188–199
15. Litsyn, S.:
An updated table of the best binary codes known.
In Pless, V.S.,
Huﬀman, W.C., eds.: Handbook of Coding Theory. Volume 1. Elsevier, Amsterdam
(1998) 463–498
16. Mills, W.H., Mullin, R.C.:
Coverings and packings.
In: Contemporary Design
Theory: A Collection of Surveys. Wiley (1992) 371–399
17. Hamalainen, H., Honkala, I., Litsyn, S., ¨Osterg˚ard, P.: Football pools—A game for
mathematicians. American Mathematical Monthly 102 (1995) 579–588
18. Fulkerson, D.R., Nemhauser, G.L., Trotter, L.E.: Two computationally diﬃcult
set covering problems that arise in computing the 1-width of incidence matrices of
Steiner triples. Mathematical Programming Study 2 (1973) 72–81
19. Margot, F.: Small covering designs by branch-and-cut. Mathematical Programming
94 (2003) 207–220
20. Dolan, E., Mor´e, J.: Benchmarking optimization software with performance pro-
ﬁles. Mathematical Programming 91 (2002) 201–213

Distinct Triangle Areas in a Planar Point Set
Adrian Dumitrescu1,⋆and Csaba D. T´oth2
1 Deptartment of Computer Science, University of Wisconsin-Milwaukee, WI
53201-0784, USA
ad@cs.uwm.edu
2 Department of Mathematics, MIT, Cambridge, MA 02139, USA
toth@math.mit.edu
Abstract. Erd˝os, Purdy, and Straus conjectured that the number of
distinct (nonzero) areas of the triangles determined by n noncollinear
points in the plane is at least ⌊n−1
2 ⌋, which is attained for ⌈n/2⌉and
respectively ⌊n/2⌋equally spaced points lying on two parallel lines. We
show that this number is at least 17
38n −O(1) ≈0.4473n. The best pre-
vious bound, (
√
2 −1)n −O(1) ≈0.4142n, which dates back to 1982,
follows from the combination of a result of Burton and Purdy [5] and
Ungar’s theorem [23] on the number of distinct directions determined by
n noncollinear points in the plane.
1
Introduction
Let S be a ﬁnite set of points in the plane. Consider the (nondegenerate) triangles
determined by triples of points of S. There are at most
n
3

triangles, some of
which may have the same area. Denote by g(S) the number of distinct (nonzero)
areas of the triangles determined by S. For every n ∈N, let g(n) be the minimum
of g(S) over all sets S of n noncollinear points in the plane. The problem of
ﬁnding g(n) has a long history; the attention it has received is perhaps due to
its simplicity and elegance, as well as to its connections to another fundamental
problem in combinatorial geometry—that of ﬁnding the minimum number of
directions spanned by n points in the plane. The problem of distinct areas is
also similar in nature to a notoriously hard problem of distinct distances. It is
listed for instance in the problem collection by Croft, Falconer, and Guy [6], and
more recently by Braß, Moser, and Pach [3]; see also [12].
The ﬁrst estimates on g(n) were given in 1976 by Erd˝os and Purdy [10], who
proved that
c1n3/4 ≤g(n) ≤c2n,
for some absolute constants c1, c2 > 0. The upper bound follows easily if we
consider the points (i, j) ∈N2 for 1 ≤i, j ≤√n and observe that every triangle
area is a multiple of 1
2 and bounded by n/2. A simple construction that consists
of two sets of ⌈n/2⌉and respectively ⌊n/2⌋equally spaced points lying on two
⋆Supported in part by NSF CAREER grant CCF-0444188.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 119–129, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

120
A. Dumitrescu and C.D. T´oth
parallel lines was found by Burton and Purdy [5], and also by Straus [21]: It
gives ⌊n−1
2 ⌋triangles of distinct areas.
In 1979, Burton and Purdy [5] obtained a linear lower bound, which follows
from a linear bound on the number of directions determined by n noncollinear
points in the plane. More precisely, denoting by f(n) the minimum number of
directions determined by n noncollinear points in the plane, they showed that
n
2

≤f(n) ≤2
n
2

.
Using this result, an averaging argument of Burton and Purdy gave
0.32n ≤g(n) ≤
n −1
2

.
In 1982, Ungar proved a sharp bound
f(n) = 2
n
2

(1)
on the minimum number of directions determined by n noncollinear points, using
a purely combinatorial approach of allowable sequences devised by Goodman and
Pollack [14,15]. A combination of Burton and Purdy’s argument [5] with Ungar’s
theorem [23] immediately gives
(
√
2 −1)n −O(1) ≤g(n) ≤
n −1
2

.
In this paper, we reﬁne Burton and Purdy’s averaging argument by applying
yet one more time (and perhaps not for the last time) Ungar’s technique on
allowable sequences, and further improve the lower bound on distinct triangle
areas.
Theorem 1. The number of triangles of distinct areas determined by n non-
collinear points in the plane is at least
g(n) ≥17
38n −O(1) ≈0.4473n.
In fact, we prove Theorem 1 in a stronger form: There are at least 17n/38−O(1)
triangles of distinct areas having a common side, in other words there are at least
this many points of our set at distinct distances from the line determined by a
pair of points in the set. One can draw here a parallel with the problem of
distinct distances raised by Erd˝os in 1946: What is the minimum number of
distinct distances t(n) determined by n points in the plane? Erd˝os conjectured
that t(n) = Ω(n/√log n), and moreover, that there is a point in the set which
determines this many distinct distances to other points. In a sequence of recent
breakthrough developments since 1997, all new lower bounds on t(n) due to
Sz´ekely [22], Solymosi and C. T´oth [20], and including the current best one due
to Katz and Tardos [16], in fact give lower bounds on the maximum number

Distinct Triangle Areas in a Planar Point Set
121
of inter-point distances measured from a single point. For triangles areas in the
plane, we have a similar phenomenon: By the argument of Burton and Purdy [5],
every set S of n noncollinear points in the plane contains two distinct points
p, q ∈S such that the points of S determine Ω(n) distinct distances to the
line pq, therefore at least this many triangles with distinct areas. As mentioned
above, our bound holds also in this stronger sense. A similar example is that
of tetrahedra of distinct volumes determined by a set of n points in R3 (not all
in the same plane): we have recently shown [8] that n points determine Ω(n)
tetrahedra of distinct volumes, which share a common side. One exception to
this phenomenon is the problem of distinct distances among vertices of a convex
polygon, as the results of [1,2,7] show (see also [3]).
2
Proof of Theorem 1
Burton and Purdy’s idea. We ﬁrst review Burton and Purdy’s argument [5]. Let
S be a set of n noncollinear points in the plane, and let L denote the set of
connecting lines (i.e., lines incident to at least 2 points of S). We may assume
w.l.o.g. that there is no horizontal line in L. For a line ℓ∈L, let ℓ1, ℓ2, . . . , ℓr ∈L
be all connecting lines parallel to ℓ(including ℓ) such that ℓi lies to the left of
ℓi+1 for 1 ≤i < r. Let ki ≥2 denote the number of points along ℓi ∈L for
i = 1, . . . , r. Let s be the number of singleton points of S not covered by any of
ℓ1, . . . , ℓr. We clearly have r
i=1 ki + s = n. Taking any two points p, q ∈S on
ℓ1 or on ℓr, the triangles Δpqzi have diﬀerent areas for at least r + ⌈s/2⌉−1
indices i, where zi are either singleton points or points on diﬀerent connecting
lines lying all on the same side of pq. Therefore the number m of distinct areas
satisﬁes
m ≥r + ⌈s/2⌉−1.
The next step is selecting a suitable direction of connecting lines, more pre-
cisely, one with a small number of pairs of points, i.e., with a small value of
r
i=1
ki
2

. By Ungar’s theorem, there is a direction corresponding to the lines
ℓ1, . . . , ℓr, such that
r
	
i=1

ki
2

≤

n
2

(n −1) = n
2 .
After observing that r
i=1
ki
2

is minimal if the points on these r connecting
lines are distributed as evenly as possible, Burton and Purdy derive a quadratic
equation whose solution gives (using Ungar’s theorem instead of their weaker
bound of ⌊n/2⌋on the number of directions) a lower bound of m ≥(
√
2 −1)n −
O(1) ≈0.4142n on the number of distinct triangle areas. Detailed calculations
show that a conﬁguration attaining the Burton-Purdy bound should have 2+
√
2
points on each connecting line parallel to the certain direction (determined by
at most n/2 pairs of points), a value which is certainly infeasible.

122
A. Dumitrescu and C.D. T´oth
A tiny improvement. We ﬁrst formulate a system of linear inequalities (the
linear program (LP1) below). Unlike Burton and Purdy’s quadratic equation,
our linear program imposes an integrality condition on the number of points
on each connecting line parallel to a speciﬁed direction; which leads to a tiny
improvement (5/12 versus
√
2−1). More important, our linear system paves the
way for a more substantial improvement obtained by two linear programs with
additional constraints (to be described later).
Assume that the connecting lines ℓ1, ℓ2 . . . , ℓr ∈L are vertical and contain
at most n/2 point pairs (by Ungar’s theorem). Every vertical line of L (passing
through at least two points) is called a regular line. A regular line passing through
exactly k points (k ≥2) is called a k-line. We call a vertical line passing through
exactly one point of S a singleton line.
Partition the n points of S as follows. Let s be a real number 0 ≤s < 1
such that there are sn singleton points to the left of the leftmost regular line ℓ1.
Similarly, let tn be the number of singleton points to the right of ℓr, and let a1n
be the number of remaining singleton points. (See Figure 1.) For k = 2, 3, . . ., 8,
let akn be the number of points on k-lines. Finally denote by a9 the total number
of points on regular lines with at least 9 points each. We have accounted for all
points of S, hence we have
s + t +
9
	
k=1
ak = 1.
tn
sn
a1n, a2n, a3n, . . . , a9n
ℓ1
ℓ2
ℓ3
ℓ4
Fig. 1. The orthogonal projection of a point set S in a direction determined by S
Let xn = r
i=1
ki
2

be the total number of point pairs on vertical lines. Let en
denote the number of distinct horizontal distances measured from the leftmost
regular line ℓ1 to its right: Consequently, there are en triangles with distinct areas
having a common side along the leftmost regular line. Similarly, let fn denote
the number of distinct horizontal distances measured from the rightmost regular
line ℓr to its left. We can deduce lower bounds on e and f: Since en ≥tn+a1n+
a2n/2+a3n/3+. . .+a8n/8−1, we have e ≥t+a1+a2/2+a3/3+. . .+a8/8−1/n,

Distinct Triangle Areas in a Planar Point Set
123
and similarly, f ≥s + a1 + a2/2 + a3/3 + . . . + a8/8 −1/n. We can also give a
lower bound for x in terms of the previous parameters. We have
x ≥1
2a2 + 2
2a3 + 3
2a4 + . . . + 8
2a9,
since if there are akn points on k-lines, then the number of k-lines is akn/k,
and each k-line contains
k
2

vertical point pairs. Hence, there are akn
k
2

/k =
akn(k−1)/2 pairs of points on k-lines, k = 2, 3, . . . , 8. Similarly there are at least
8
2a9n pairs of points on lines incident to at least 9 points. Putting all of these
equations and inequalities together, we formulate the following linear program.
minimize r
(LP1)
subject to x ≤0.5;
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
s + t + a1 + a2 + a3 + a4 + a5 + a6 + a7 + a8 + a9 = 1;
1
2a2 + a3 + 3
2a4 + 2a5 + 5
2a6 + 3a7 + 7
2a8 + 4a9 ≤x;
t + a1 + 1
2a2 + 1
3a3 + 1
4a4 + 1
5a5 + 1
6a6 + 1
7a7 + 1
8a8 −1
n ≤e;
s + a1 + 1
2a2 + 1
3a3 + 1
4a4 + 1
5a5 + 1
6a6 + 1
7a7 + 1
8a8 −1
n ≤f;
e ≤r;
f ≤r;
s, t, a1, a2, a3, a4, a5, a6, a7, a8, a9, e, f, r, x ≥0;
The linear system (LP1) does not describe completely a point conﬁguration
(e.g., we do not make any distinction among k-lines for k ≥9), but all these
inequalities must hold if the variables correspond to a point set S. Let (LP1’) be
the linear program obtained from (LP1) by removing the two terms 1
n, and let r
be its solution. Since the constraints are linear, the term 1
n can only contribute
a constant additive blow-up in the LP solution. That is, if r is the solution of
(LP1’), the solution of (LP1) is r −O(1/n). We can deduce that there are at
least rn −O(1) distinct triangle areas with a common side on either ℓ1 or ℓr.
A solution to (LP1’) is r = 5/12 ≈0.4166, attained for s = t = 1/4, a3 = 1/2,
a1 = a2 = a4 = a5 = a6 = a7 = a8 = a9 = 0, e = f = 5/12, and x = 1/2. That
is, there are n/6 3-lines in the middle, and n/4 singleton lines on each side, and
5n/12−O(1) distinct areas measured from left or right. Another optimal solution
that looks similar consists of n/12 4-lines in the middle, and n/3 singleton lines
on each side, for which the number of distinct areas is also 5n/12 −O(1).
Allowable sequences. We now give a very brief account on Ungar’s technique (fol-
lowing [23]) and allowable sequences [12], as they are relevant to our proof. Allow-
able sequences occur in the context of transforming the permutation 1, 2, . . ., n
into the reverse permutation n, n −1, . . . , 1 by going through a sequence of per-
mutations. The operation between two consecutive permutations, called move,
consists of inverting pairwise disjoint increasing strings. In a geometric context,
each symbol corresponds to a point in the plane; each permutation is the left-
to-right order in an orthogonal projections of the points on a directed line. The
directed line is rotated around the origin, and a move occurs when the normal of

124
A. Dumitrescu and C.D. T´oth
this line coincides with a direction of a connecting line (a line in L). An exam-
ple of a sequence arising in this way is 1(23)4(56), 13(246)5, (136)425, 63(14)25,
6(34)(125), 64(35)21, 6(45)321, and 654321. We have put parentheses around the
increasing string (called blocks) reversed at the next move. So each permutation
with the blocks enclosed in parentheses describes also the next move.
Ungar’s theorem states that for even n, going from 1, 2, . . ., n to n, n−1, . . . , 1
but not in one move, requires at least n moves (in other words, if every block
reversed has fewer than n elements, at least n moves are needed). The general
idea in the proof is that building up a long increasing block involves many moves
required by dismantling other (possibly long) decreasing blocks formed at earlier
moves, and vice versa. More precisely, the moves have the following properties.
(I) In one move, a decreasing string can get shorter by at most one element at
each end.
(II) in one move, an increasing string can get longer by at most one element at
each end.
For instance, the reason for (I) is that a move reverses increasing strings, and
so only the ﬁrst and the last elements of a decreasing string can be part of a
block in a move. We refer the reader to [23] for more details. Properties (I) and
(II) further imply that if a block B of size at least 3 is reversed in one move,
then all but the two extreme elements of B must be singletons in the next move.
Analogously, if a block B of size at least 3 is reversed in a move, then at least
one of its elements is a singleton in the previous move.
tan
sbn
b1n, b2n, b3n, . . . , b10n
tbn
san
a1n, a2n, a3n, . . . , a10n
Fig. 2. The orthogonal projection of a point set S in two consecutive directions, a and
b, determined by S

Distinct Triangle Areas in a Planar Point Set
125
New bound. The idea for our new bound is the following. Recall that two optimal
solutions of (LP1’) we have seen have a similar structure: (A) n/6 3-lines in the
middle, and n/4 singleton lines on each side, or (B) n/12 4-lines in the middle,
and n/3 singleton lines on each side. Assume that there are two consecutive
moves, π1 and π2, in an allowable sequence such that both look like (A) or
(B). Notice that our observations regarding the blocks of size at least 3 imply
that there cannot be two consecutive such moves, since the ﬁrst move would
force many singletons in the middle segment of π2 (at least one for each block
of π1). This suggests that one of two consecutive directions of L must give a
conﬁguration where the solution of (LP1’) is above 5/12. We follow with the
precise technical details in the proof of Theorem 1.
By Ungar’s theorem, the average number of pairs determining the same di-
rection is at most n/2, so there are two consecutive moves (corresponding to two
consecutive directions of lines in L) parallel to at most n pairs of points. We
introduce a similar notation as above for a single direction, but we distinguish
the notation by indices a and b, respectively (e.g., san and sbn are the number
of points which give singletons at the left side of the ﬁrst and the second per-
mutation, respectively). This time we count up to 9-lines (rather than 8-lines)
and group together the k-lines for k ≥10. We denote by a10n and b10n the total
number of points on lines with at least 10 points each. By symmetry, we need to
consider only two cases (instead of the four combinations of sa ⋚sb and ta ⋚tb).
Case (i): sb ≤sa and tb ≤ta.
Case (ii): sa ≤sb and tb ≥ta.
We are lead to minimizing the following two linear programs (LP2i) and (LP2ii),
where (LP2i) corresponds to Case (i) and (LP2ii) corresponds to Case (ii).
Case (i): sb ≤sa and tb ≤ta. We formulate the linear program (LP2i) as follows.
We repeat the constraints of (LP1) for both moves, and impose the constraint
xa + xb ≤1 since the total number of pairs for the two consecutive directions
is at most n. We introduce two linear constraints to express r = max(ra, rb).
Constraints (α) and (β) are crucial: Constraint (α) indicates that if in the ﬁrst
move, a block B of size at least 3 is reversed, then all but the two extreme
elements of B must be singletons in the next move; constraint (β) speciﬁes that
each block B of size at least 3 which is reversed in the second move must contain
an element which is a singleton in the ﬁrst move (with the possible exception of
two blocks that lie on the boundary of the singletons sa and ta).
Here is an example regarding constraint (β). Let π1 and π2 denote the two
consecutive moves (each represented by pairwise disjoint blocks). The preﬁxes
(resp., suﬃxes) of length sb (resp., tb) coincide, and are made of singletons. So
each block of size at least 3 in the second move in between these common preﬁx
and suﬃx strings (to be reversed in the second move) must pick up at least a
singleton in a1 from π1 or must be made entirely up of singletons in the (sa −sb)
and (ta −tb) segments of π1 (except for at most two blocks crossing segment
borders). For instance, if a move transforms permutation π1 = . . . (47)(359) . . .
to π′
1 = . . . 74953 . . ., then no triple (or other longer block) may be formed in the

126
A. Dumitrescu and C.D. T´oth
next move. But if there was a singleton in between, like in π1 = . . . (47)6(359) . . .,
then a triple may be formed in the next move: For instance, π2 = . . . 7(469)53 . . ..
minimize r
(LP2i)
subject to sb ≤sa;
tb ≤ta;
(LP1)a
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
sa + ta + a1 + a2 + a3 + a4 + a5 + a6 + a7 + a8 + a9 + a10 = 1;
1
2a2 + a3 + 3
2a4 + 2a5 + 5
2a6 + 3a7 + 7
2a8 + 4a9 + 9
2a10 ≤xa;
ta + a1 + a2
2 + a3
3 + a4
4 + a5
5 + a6
6 + a7
7 + a8
8 + a9
9 −1
n ≤ea;
sa + a1 + a2
2 + a3
3 + a4
4 + a5
5 + a6
6 + a7
7 + a8
8 + a9
9 −1
n ≤fa;
ea ≤ra;
fa ≤ra;
(LP1)b
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
sb + tb + b1 + b2 + b3 + b4 + b5 + b6 + b7 + b8 + b9 + b10 = 1;
1
2b2 + b3 + 3
2b4 + 2b5 + 5
2b6 + 3b7 + 7
2b8 + 4b9 + 9
2b10 ≤xb;
tb + b1 + b2
2 + b3
3 + b4
4 + b5
5 + b6
6 + b7
7 + b8
8 + b9
9 −1
n ≤eb;
sb + b1 + b2
2 + b3
3 + b4
4 + b5
5 + b6
6 + b7
7 + b8
8 + b9
9 −1
n ≤fb;
eb ≤rb;
fb ≤rb;
xa + xb ≤1;
ra ≤r;
rb ≤r;
(α) 1
3a3 + 2
4a4 + 3
5a5 + 4
6a6 + 5
7a7 + 6
8a8 + 7
9a9 + 8
10a10 ≤b1;
(β) b3 + b4 + b5 + b6 + b7 + b8 + b9 + b10 −2
n ≤3a1 + sa −sb + ta −tb;
sa, ta, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, ea, fa, ra, xa ≥0;
sb, tb, b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, eb, fb, rb, xb ≥0;
r ≥0;
When we ignore the terms O( 1
n), we get a new system (LP2i’) with the fol-
lowing solution: r = 17/38 ≈0.4473, attained for sa = ta = 15/38, a1 = a2 =
a3 = 0, a4 = 4/19, a5 = a6 = a7 = a8 = a9 = a10 = 0 for the ﬁrst permuta-
tion, and sb = tb = 3/38, b1 = b2 = 2/19, b3 = 12/19, b4 = b5 = b6 = b7 =
b8 = b9 = b10 = 0 for the second permutation; also xa = 6/19, xb = 13/19,
ea = fa = ra = eb = fb = rb = 17/38.
Case (ii): sb ≤sa and tb ≥ta. The linear program (LP2ii) is very similar to
(LP2i). Besides the ﬁrst two constraints, which are speciﬁc to this case, only
constraints (γ) and (δ) are diﬀerent: Constraint (γ) speciﬁes that each block B
of size at least 3 which is reversed in the second move must contain at least one

Distinct Triangle Areas in a Planar Point Set
127
singleton in the ﬁrst move; constraint (δ) speciﬁes the same thing when going
back from the second permutation to the ﬁrst one (by time reversibility).
minimize r
(LP2ii)
subject to sb ≤sa;
ta ≤tb;
(LP1)a
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
sa + ta + a1 + a2 + a3 + a4 + a5 + a6 + a7 + a8 + a9 + a10 = 1;
1
2a2 + a3 + 3
2a4 + 2a5 + 5
2a6 + 3a7 + 7
2a8 + 4a9 + 9
2a10 ≤xa;
ta + a1 + a2
2 + a3
3 + a4
4 + a5
5 + a6
6 + a7
7 + a8
8 + a9
9 −1
n ≤ea;
sa + a1 + a2
2 + a3
3 + a4
4 + a5
5 + a6
6 + a7
7 + a8
8 + a9
9 −1
n ≤fa;
ea ≤ra;
fa ≤ra;
(LP1)b
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
sb + tb + b1 + b2 + b3 + b4 + b5 + b6 + b7 + b8 + b9 + b10 = 1;
1
2b2 + b3 + 3
2b4 + 2b5 + 5
2b6 + 3b7 + 7
2b8 + 4b9 + 9
2b10 ≤xb;
tb + b1 + b2
2 + b3
3 + b4
4 + b5
5 + b6
6 + b7
7 + b8
8 + b9
9 −1
n ≤eb;
sb + b1 + b2
2 + b3
3 + b4
4 + b5
5 + b6
6 + b7
7 + b8
8 + b9
9 −1
n ≤fb;
eb ≤rb;
fb ≤rb;
xa + xb ≤1;
ra ≤r;
rb ≤r;
(γ) a3 + a4 + a5 + a6 + a7 + a8 + a9 + a10 −1
n ≤3b1 + tb −ta;
(δ) b3 + b4 + b5 + b6 + b7 + b8 + b9 + b10 −1
n ≤3a1 + sa −sb;
sa, ta, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10, ea, fa, ra, xa ≥0;
sb, tb, b1, b2, b3, b4, b5, b6, b7, b8, b9, b10, eb, fb, rb, xb ≥0;
r ≥0;
When we ignore the terms O( 1
n), we get a new system (LP2ii’) with the
following solution: r = 25/54 ≈0.4629, attained for sa = ta = 23/54, a1 = 1/27,
a2 = a3 = a4 = a5 = a6 = a7 = a8 = a9 = 0, a10 = 1/9, for the ﬁrst
permutation, and sb = tb = 23/54, b1 = 1/27, b2 = b3 = b4 = b5 = b6 = b7 =
b8 = b9 = 0, b10 = 1/9, for the second permutation; also xa = 1/2, xb = 1/2,
ea = fa = ra = eb = fb = rb = 25/54.
Since the solution of (LP2i’) is smaller than that of (LP2ii’), i.e., 17/38 <
25/54, we conclude that there are always 17
38n −O(1) ≈0.4473n triangles of
distinct areas.
One may ask if the same result can be obtained using fewer variables in the
LPs, or whether a better result can be obtained by increasing the number of
variables in the LPs. The answer to both questions is negative.

128
A. Dumitrescu and C.D. T´oth
3
Remarks
In 1982, Erd˝os, Purdy, and Straus [13] considered the generalization of the prob-
lem of distinct triangle areas to higher dimensions and posed the following:
Problem (Erd˝os, Purdy, and Straus). Let S be a set of n points in Rd not all
in one hyperplane. What is the minimal number gd(n) of distinct volumes of
nondegenerate simplices with vertices in S?
By taking d sets of about n/d equally spaced points on parallel lines through the
vertices of a (d −1)-simplex, one gets gd(n) ≤⌊n−1
d ⌋. Erd˝os, Purdy, and Straus
conjectured that equality holds at least for suﬃciently large n (see also [6]). The
ﬁrst development in this old problem for higher dimensions is only very recent:
for d = 3 we have shown that the tetrahedra determined by n points in R3, not
all in a plane, have at least Ω(n) distinct volumes, which thereby conﬁrms the
conjecture in 3-space apart from the multiplicative constant [8].
We conclude with two problems on distinct triangle areas. The former is di-
rectly related to the original problem of distinct areas studied here, and appears
to have been ﬁrst raised by Erd˝os and Pach in the 1980s [17], while the latter
appears to be new.
Given a planar point set S, consider the set L of connecting lines. A connecting
line is called an ordinary line if it passes through exactly two points of S. By the
well known Sylvester-Gallai theorem [18,3], any ﬁnite set of noncollinear points
in the plane determines an ordinary line. Consider now the set Θ of directions
of lines in L. A direction θ ∈Θ is called an ordinary direction if all connecting
lines of direction θ are ordinary lines.
Problem 1. Let S be a set of n noncollinear points in the plane. Is it true that
apart from a ﬁnite set of values of n, Θ always contains an ordinary direction?
It should be clear that such a direction would be enough to prove the Erd˝os-Purdy-
Strauss conjecture that S determines at least ⌊(n −1)/2⌋distinct (nonzero) tri-
angle areas — apart from a ﬁnite set of exceptions for n. Observe that n = 7 is
such an exception, since the conﬁguration of 7 points given by the three vertices
of a triangle, the midpoints of its three sides, and the triangle center admits no
ordinary direction.
Problem 2. Let S be a set of n noncollinear points in the plane. Is it true that
each point p ∈S is the vertex of Ω(n) triangles of distinct areas determined by
S? In other words, is there a constant c > 0 such that for every p ∈S, the point
set S determines at least cn triangles of distinct areas, all incident to p?
References
1. E. Altman, On a problem of Erd˝os, American Mathematical Monthly, 70 (1963),
148–157.
2. E. Altman, Some theorems on convex polygons, Canadian Mathematical Bulletin,
15 (1972), 329–340.

Distinct Triangle Areas in a Planar Point Set
129
3. P. Braß, W. Moser, and J. Pach, Research Problems in Discrete Geometry, Springer,
New York, 2005.
4. P. Braß, G. Rote, and K. J. Swanepoel, Triangles of extremal area or perimeter in
a ﬁnite planar point set, Discrete & Computational Geometry 26 (2001), 51–58.
5. G. R. Burton and G. Purdy, The directions determined by n points in the plane,
Journal of London Mathematical Society 20 (1979), 109–114.
6. H. T. Croft, K. J. Falconer, and R. K. Guy, Unsolved Problems in Geometry,
Springer, New York, 1991.
7. A. Dumitrescu, On distinct distances from a vertex of a convex polygon, Discrete
& Computational Geometry 36 (2006), 503–509.
8. A. Dumitrescu and Cs. D. T´oth, On the number of tetrahedra with minimum,
uniform, and distinct volumes in three-space, in Proceedings of the 18th ACM-
SIAM Symposium on Discrete Algorithms, ACM Press, 2007, 1114–1123.
9. P. Erd˝os, On sets of distances of n points, American Mathematical Monthly 53
(1946), 248–250.
10. P. Erd˝os and G. Purdy, Some extremal problems in geometry IV, Congressus Nu-
merantium 17 (Proceedings of the 7th South-Eastern Conference on Combina-
torics, Graph Theory, and Computing), 1976, 307–322.
11. P. Erd˝os and G. Purdy, Some extremal problems in geometry V, Proceedings of the
8th South-Eastern Conference on Combinatorics, Graph Theory, and Computing,
1977, 569–578.
12. P. Erd˝os and G. Purdy, Extremal problems in combinatorial geometry. in Handbook
of Combinatorics, Vol. 1, 809–874, Elsevier, Amsterdam, 1995.
13. P. Erd˝os, G. Purdy, and E. G. Straus, On a problem in combinatorial geometry,
Discrete Mathematics 40 (1982), 45–52.
14. J. E. Goodman and R. Pollack, On the combinatorial classiﬁcation of nondegener-
ate conﬁgurations in the plane, Journal of Combinatorial Theory Ser. A 29 (1980),
220–235.
15. J. E. Goodman and R. Pollack, A combinatorial perspective on some problems in
geometry, Congressus Numerantium 32 (1981), 383–394.
16. N. H. Katz and G. Tardos, A new entropy inequality for the Erd˝os distance prob-
lem, in Towards a Theory of Geometric Graphs (J. Pach, ed.), vol. 342 of Contem-
porary Mathematics, AMS, Providence, RI, 2004, 119–126.
17. J. Pach, personal communication, January 2007.
18. J. Pach and P. K. Agarwal, Combinatorial Geometry, John Wiley, New York, 1995.
19. J. Pach and G. Tardos, Isosceles triangles determined by a planar point set, Graphs
and Combinatorics, 18 (2002), 769–779.
20. J. Solymosi and Cs. D. T´oth, Distinct distances in the plane, Discrete & Compu-
tational Geometry, 25 (2001), 629–634.
21. E. G. Straus, Some extremal problems in combinatorial geometry, in Proceedings of
the Conference on Combinatorial Theory, vol. 686 of Lecture Notes in Mathematics,
Springer, 1978, pp. 308–312.
22. L. Sz´ekely, Crossing numbers and hard Erd˝os problems in discrete geometry, Com-
binatorics, Probability and Computing 6 (1997), 353–358.
23. P. Ungar, 2N noncollinear points determine at least 2N directions, Journal of
Combinatorial Theory Ser. A 33 (1982), 343–347.

Scheduling with Precedence Constraints of Low
Fractional Dimension
Christoph Amb¨uhl1, Monaldo Mastrolilli2, Nikolaus Mutsanas2,
and Ola Svensson2
1 University of Liverpool - Great Britain
christoph@csc.liv.ac.uk
2 IDSIA- Switzerland
{monaldo,nikolaus,ola}@idsia.ch
Abstract. We consider the single machine scheduling problem to mini-
mize the average weighted completion time under precedence constrains.
Improving on the various 2-approximation algorithms is considered one
of the ten most prominent open problems in scheduling theory. Recently,
research has focused on special cases of the problem, mostly by restrict-
ing the set of precedence constraints to special classes such as convex
bipartite, two-dimensional, and interval orders.
In this paper we extend our previous results by presenting a framework
for obtaining (2 −2/d)-approximation algorithms provided that the set
of precedence constraints has fractional dimension d. Our generalized
approach yields the best known approximation ratios for all previously
considered classes of precedence constraints, and it provides the ﬁrst
results for bounded degree and interval dimension 2 orders.
As a negative result we show that the addressed problem remains
NP-hard even when restricted to the special case of interval orders.
1
Introduction
The problem we consider in this paper is a classical problem in scheduling theory,
known as 1|prec| 
j wjCj in standard scheduling notation (see e.g. Graham et
al. [12]). It is deﬁned as the problem of scheduling a set N = {1, . . . , n} of n jobs
on a single machine, which can process at most one job at a time. Each job j has
a processing time pj and a weight wj, where pj and wj are nonnegative integers.
Jobs also have precedence constraints between them that are speciﬁed in the form
of a partially ordered set (poset) P = (N, P), consisting of the set of jobs N and
a partial order i.e. a reﬂexive, antisymmetric, and transitive binary relation P on
N, where (i, j) ∈P (i ̸= j) implies that job i must be completed before job j
can be started. The goal is to ﬁnd a non-preemptive schedule which minimizes
n
j=1 wjCj, where Cj is the time at which job j completes in the given schedule.
The described problem was shown to be strongly NP-hard already in 1978
by Lawler [17] and Lenstra & Rinnooy Kan [18]. While currently no inapprox-
imability result is known (other than that the problem does not admit a fully
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 130–144, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

Scheduling with Precedence Constraints of Low Fractional Dimension
131
polynomial time approximation scheme), there are several 2-approximation al-
gorithms [26,29,13,6,5,20,2]. Closing this approximability gap is a longstanding
open problem in scheduling theory (see e.g. [30]).
Due to the diﬃculty to obtain better than 2-approximation algorithms, much
attention has recently been given to special cases which manifests itself in recent
approximation and exact algorithms [16,33,7,2,3].
On the negative side, Woeginger [33] proved that many quite severe restric-
tions on the weights and processing times do not inﬂuence approximability. For
example, the special case in which all jobs either have pj = 1 and wj = 0, or
pj = 0 and wj = 1, is as hard to approximate as the general case. This sug-
gests that in order to identify classes of instances which allow a better than
2-approximation one has to focus on the precedence constraints rather than the
weights and processing times.
Indeed, Lawler [17] gave an exact algorithm for series-parallel orders already
in 1978. For interval orders and convex bipartite precedence constraints, Woeg-
inger [33] gave approximation algorithms with approximation ratio arbitrarily
close to the golden ratio 1
2(1 +
√
5) ≈1.61803.
Recently, Amb¨uhl & Mastrolilli [2] settled an open problem ﬁrst raised by
Chudak & Hochbaum [6] and whose answer was subsequently conjectured by
Correa & Schulz [7]. The results in [2,7] imply that 1|prec | wjCj is a special
case of the weighted vertex cover problem. More precisely, they proved that
every instance S of 1|prec |  wjCj can be translated in polynomial time into
a weighted graph GP, such that ﬁnding the optimum of S can be reduced to
ﬁnding an optimum vertex cover in GP. This result even holds for approximate
solutions: Finding an α-approximate solution for S can be reduced to ﬁnding an
α-approximate vertex cover in GP.
Based on these results, three of the authors [3] discovered an interesting con-
nection between 1|prec |  wjCj and the dimension theory of posets [32], by
observing that the graph GP is well known in dimension theory as the graph of
incomparable pairs of a poset P. Applying results from dimension theory allowed
to describe a framework for obtaining simple and eﬃcient approximation algo-
rithms for 1|prec |  wjCj with precedence constraints of low dimension, such
as convex bipartite and semi-orders. In both cases, the new 4/3-approximation
algorithms outperform the previously known results. The approach even yields
a polynomial algorithm for 2-dimensional precedence constraints, based on the
fact that the minimum weighted vertex cover on GP can be solved in polynomial
time since GP is bipartite for a 2-dimensional poset P [32,7]. This considerably
extends Lawler’s result [17] for series-parallel orders. Unfortunately, the frame-
work in [3] fails in the case of interval orders (in this case the dimension can be
of the order of log log n [32]).
The work in this paper originated from the study of 1|prec |  wjCj under
interval orders (abbreviated 1|interval-order| 
j wjCj). Interval orders appear
in many natural contexts [10]. We provide both positive and negative results.
In the ﬁrst part of the paper, we further generalize our previous frame-
work [3] such that it can be applied to precedence constraints of low fractional

132
C. Amb¨uhl et al.
dimension [4] (Section 3). The extended framework yields (2−2/d)-approximation
algorithms whenever precedence constraints have fractional dimension bounded
by a constant d and satisfy a mild condition (see Section 3). Since the fractional
dimension of interval orders is bounded by 4 (see Section 4.1), this gives a 1.5-
approximation algorithm and improves the previous result in [33]. The extended
framework can also be applied to interval dimension two posets (Section 4.2),
bounded degree posets (Section 4.3), and posets obtained by the lexicographic
sums (Section 4.4).
In the second part of the paper, we show that 1|interval-order| 
j wjCjre-
mains NP-hard (Section 5). This result is rather unexpected as many prob-
lems can be solved in polynomial time when restricted to interval orders (see
e.g. [25]). The reduction heavily relies on the connection between 1|prec |  wjCj
and weighted vertex cover described in [2].
In summary, our results indicate a strong relationship between the approxima-
bility of 1|prec| 
j wjCj and the fractional dimension d of the precedence con-
straints. In particular, it is polynomial for d = 2, but NP-hard already for d ≥3.
The latter stems from the facts that problem 1|prec| 
j wjCj is strongly NP-hard
even for posets with in-degree 2 [17], and the fractional dimension of these posets
is bounded by 3 [8]. This leaves the complexity for 2 < d < 3 as an open question.
2
Deﬁnitions and Preliminaries
2.1
Posets and Fractional Dimension
Let P = (N, P) be a poset. For x, y ∈N, we write x ≤y when (x, y) ∈P, and
x < y when (x, y) ∈P and x ̸= y. When neither (x, y) ∈P nor (y, x) ∈P, we
say that x and y are incomparable, denoted by x||y. We call inc(P) = {(x, y) ∈
N × N : x||y in P} the set of incomparable pairs of P. A poset P is a linear
order (or a total order) if for any x, y ∈N either (x, y) ∈P or (y, x) ∈P,
i.e. inc(P) = ∅. A partial order P ′ on N is an extension of a partial order P
on the same set N, if P ⊆P ′. An extension that is a linear order is called a
linear extension. Mirroring the deﬁnition of the fractional chromatic number of a
graph, Brightwell & Scheinerman [4] introduce the notion of fractional dimension
of a poset. Let F = {L1, L2, . . . , Lt} be a nonempty multiset of linear extensions
of P. The authors in [4] call F a k-fold realizer of P if for each incomparable
pair (x, y), there are at least k linear extensions in F which reverse the pair
(x, y), i.e., |{i = 1, . . . , t : y < x in Li}| ≥k. We call a k-fold realizer of size t
a k:t-realizer. The fractional dimension of P is then the least rational number
fdim(P) ≥1 for which there exists a k:t-realizer of P so that k/t ≥1/fdim(P).
Using this terminology, the dimension of P, denoted by dim(P), is the least t for
which there exists a 1-fold realizer of P. It is immediate that fdim(P) ≤dim(P)
for any poset P. Furthermore [4], fdim(P) = 1, or fdim(P) ≥2.
2.2
Scheduling, Vertex Cover, and Dimension Theory
In [7,2,3] a relationship between 1|prec| 
j wjCj, weighted vertex cover, and
the dimension theory of posets is shown. This relationship will turn out to be

Scheduling with Precedence Constraints of Low Fractional Dimension
133
useful for both improving the approximation ratio for several classes of prece-
dence constraints and establishing the NP-hardness of 1|interval-order| 
j wjCj.
Let P = (N, P) be any poset, that is not a linear order. Felsner and Trotter [9]
associate with P a hypergraph HP, called the hypergraph of incomparable pairs,
deﬁned as follows. The vertices of HP are the incomparable pairs in P. The edge
set consists of those sets U of incomparable pairs such that no linear extension of
P reverses all incomparable pairs in U. Let GP denote the ordinary graph, called
the graph of incomparable pairs, determined by all edges of size 2 in HP. In [9,32]
it is shown that the dimension of P is equal to the chromatic number of HP,
i.e., dim(P) = χ(HP) ≥χ(GP). In [4], it was noted that the same relationship
holds for the fractional versions, i.e., fdim(P) = χf(HP) ≥χf(GP). We refer
the reader to [28] for an introduction to fractional graph coloring.
Given an instance S of 1|prec| 
j wjCj, we associate with S a weighted vertex
cover instance V CS on GP, where GP is the graph of incomparable pairs of the
poset P representing the precedence constraints and each vertex (i, j) ∈inc(P)
has weight pi · wj. We denote the value of a solution s by val(s).
Theorem 1 ([2,3,7]). Let S be an instance of 1|prec| 
j wjCj where prece-
dence constraints are given by the poset P = (N, P). Then the following trans-
formations can be performed in polynomial time.
1. Any feasible solution s′ of S can be turned into a feasible solution c′ of V CS,
such that
val(c′) ≤val(s′) −

(i,j)∈P
pi · wj.
2. Any feasible solution c′ to V CS can be turned into a feasible solution s′ of
S, such that
val(s′) ≤val(c′) +

(i,j)∈P
pi · wj.
In particular, if c∗and s∗are optimal solutions to V CS and S, respectively, we
have val(c∗) = val(s∗) −
(i,j)∈P pi · wj.
We remark that the term 
(i,j)∈P pi · wj is a ﬁxed cost and it is present in all
feasible schedules of S. This follows from the facts that a job’s processing time
is always included in its completion time, and any feasible schedule of S must
schedule job i before job j if i < j in P.
3
Scheduling and Fractional Dimension
In this section, we present an algorithmic framework that can be used to obtain
better than 2-approximation algorithms provided that the set of precedence con-
straints has low fractional dimension. Applications that follow this pattern are
given in Section 4.
We say that a poset P admits an eﬃciently samplable k:t-realizer if there exists
a randomized algorithm that, in polynomial time, returns any linear extension
from a k-fold realizer F = {L1, L2, . . . , Lt} with probability 1/t.

134
C. Amb¨uhl et al.
Let S be an instance of 1|prec| 
j wjCj where precedence constraints are
given by a poset P = (N, P). Assuming that P admits an eﬃciently samplable
k:t-realizer F = {L1, . . . , Lt}, we proceed as follows.
Let VP and EP be the vertex set and edge set, respectively, of the graph of
incomparable pairs GP. Consider the following integer program formulation of
the weighted vertex cover V CS:
min

i∈VP
wixi
s.t.
xi + xj ≥1
{i, j} ∈EP
xi ∈{0, 1}
i ∈VP
where wi denotes the weight of vertex vi ∈Vp, as speciﬁed in the deﬁnition of
V CS (see Section 2.2). Let [VC-LP] denote the linear relaxation of the integer
program above.
Nemhauser & Trotter [23,24] proved that any basic feasible solution to [VC-
LP] is half-integral, that is xi ∈{0, 1
2, 1} for all i ∈V . Let Vi be the set of nodes
whose corresponding variables took value i ∈{0, 1
2, 1} in the optimal solution of
[VC-LP].
Observe that for any linear extension L, the set of all incomparable pairs that
are reversed in L is an independent set in the graph of incomparable pairs GP.
Now, pick uniformly at random a linear extension L of F in polynomial time.
Note that V0 ∪(V1/2 \ L) deﬁnes an independent set of GP. Generalizing a result
by Hochbaum in [14], we prove that the complement of V0 ∪(V1/2 \L) is a vertex
cover whose expected value is within (2 −2 k
t ) times the weight of an optimum
cover. By Theorem 1, we can transform (in polynomial time) the solution of V CS
into a feasible solution of S of expected value at most (2 −2 k
t ) times the value
of an optimum schedule. We summarize the above arguments in the following
theorem.
Theorem 2. The problem 1|prec| 
j wjCj, whenever precedence constraints ad-
mit an eﬃciently samplable k:t-realizer, has a randomized (2−2 k
t )-approximation
algorithm.
For a proof of this theorem, see Appendix A.1. Following a similar argumen-
tation, Hochbaum’s approach [14] for approximating the vertex cover prob-
lem can be extended to fractional coloring, yielding the same approximation
result.
A natural question is for which posets one can have an eﬃciently samplable
k:t-realizer. In the general case, Jain & Hedge [15] recently proved that it is
hard to approximate the dimension of a poset with n elements within a fac-
tor n0.5−ϵ, and the same hardness of approximation holds for the fractional
dimension. However, for several special cases, including interval orders (Sec-
tion 4.1) and bounded degree posets (Section 4.3), eﬃciently samplable
k:t-realizers exist.

Scheduling with Precedence Constraints of Low Fractional Dimension
135
4
Precedence Constraints with Low Fractional Dimension
4.1
Interval Orders
A poset P = (N, P) is an interval order if there is a function F, which assigns to
each x ∈N a closed interval F(x) = [ax, bx] of the real line R, so that x < y in P
if and only if bx < ay in R. Interval orders can be recognized in O(n2) time [21,25].
The dimension of interval orders can be of the order of log log n [32], whereas the
fractional dimension is known to be less than 4 [4], and this bound is asymptoti-
cally tight [8]. In the following we show how to obtain a 1.5-approximation algo-
rithm for 1|interval-order| 
j wjCj. By Theorem 2, it is suﬃcient to prove that
interval orders admit an eﬃciently samplable k:t-realizer with t/k = 4.
Given a poset P = (N, P), disjoint subsets A and B of the ground set N, and
a linear extension L of P, we say that B is over A in L if, for every incomparable
pair of elements (a, b) with a ∈A and b ∈B, one has b > a in L. The following
property of interval orders is fundamental.
Theorem 3 (Rabinovitch [27,10]). A poset P = (N, P) is an interval order
if and only if for every pair (A, B) of disjoint subsets of N there is a linear
extension L of P with B over A.
By using this property we can easily obtain a k-fold realizer F = {L1, . . . , Lt}
with k = 2n−2 and t = 2n, where n = |N|. Indeed, consider every subset A of
N and let LA be a linear extension of P in which B = N \ A is over A. Now
let F be the multiset of all the LA’s. Note that |F| = 2n. Moreover, for any
incomparable pair (x, y) there are at least k = 2n−2 linear extensions in F for
which x ∈B and y ∈A. Finally, observe that we can eﬃciently pick uniformly
at random one linear extension from F: for every job j ∈N put j either in A or
in B with the same probability 1/2.
By the previous observations and Theorem 2, we have a randomized polyno-
mial time 1.5-approximation for 1|interval-order| 
j wjCj. The described algo-
rithm can easily be derandomized by using the classical method of conditional
probabilities.
Theorem 4. Problem 1|interval-order| 
j wjCj has a deterministic polynomial
time 1.5-approximation algorithm.
4.2
Interval Dimension Two
The interval dimension of a poset P = (N, P), denoted by dimI(P), is deﬁned [32]
as the least t for which there exist t extensions Q1, Q2, . . . , Qt, so that:
– P = Q1 ∩Q2 ∩· · · ∩Qt and
– (N, Qi) is an interval order for i = 1, 2, . . . , t.
Generally dimI(P) ≤dim(P). Obviously, if P is an interval order, dimI(P) = 1.
The class of posets of interval dimension 2 forms a proper superclass of the
class of interval orders. Posets of interval dimension two can be recognized in

136
C. Amb¨uhl et al.
O(n2) time due to Ma & Spinrad [19]. Given a poset P with dimI(P) = 2, their
algorithm also yields an interval realizer {Q1, Q2}. As described in Section 4.1,
we obtain k-fold realizers F1 = {L1, L2, . . . , Lt} and F2 = {L′
1, L′
2, . . . , L′
t} of Q1
and Q2, respectively, with k = 2n−2 and t = 2n. It is immediate that F = F1∪F2
is a k-fold realizer of P of size 2t = 2n+1. Furthermore, we can eﬃciently pick
uniformly at random one linear extension from F: pick uniformly at random a
linear extension from either F1 or F2 with the same probability 1/2. Again by
using conditional probabilities we have the following.
Theorem 5. Problem 1|prec| 
j wjCj, whenever precedence constraints have in-
terval dimension at most 2, has a polynomial time 1.75-approximation algorithm.
4.3
Posets of Bounded Degree
In the following we will see how to obtain, using Theorem 2, an approximation
algorithm for 1|prec |  wjCj when the precedence constraints form a poset of
bounded degree. Before we proceed, we need to introduce some deﬁnitions.
Let P = (N, P) be a poset. For any job j ∈N, deﬁne the degree of j,
denoted deg(j), as the number of jobs comparable (but not equal) to j in P.
Let Δ(P) = max{deg(j) : j ∈N}. Given a job j, let D(j) denote the set of all
jobs which are less than j, and U(j) those which are greater than j in P. Deﬁne
degD(j) = |D(j)| and ΔD(P) = max{degD(j) : j ∈N}. The quantities degU(j)
and ΔU(P) are deﬁned dually.
We observe that the NP-completeness proof for 1|prec |  wjCj given by
Lawler [17] was actually provided for posets P with ΔD(P) = 2. By using
fractional dimension we show that these posets (with bounded min{ΔD, ΔU})
allow for better than 2-approximation.
Theorem 6. Problem 1|prec |  wjCj has a polynomial time (2 −2/f)-approx-
imation algorithm, where f = 1 + min{ΔD, ΔU, 1}.
Proof. Let P = (N, P) be the poset representing the precedence constraints
with bounded min{ΔD, ΔU}. Assume, without loss of generality, that P is not
decomposable with respect to lexicographic sums (see Section 4.4). Otherwise,
a decomposition with respect to lexicographic sums can be done in O(n2) time
(see e.g. [22]), and each component can be considered separately. We call an
incomparable pair (x, y) ∈inc(P) a critical pair if for all z, w ∈N \ {x, y}
1. z < x in P implies z < y in P, and
2. y < w in P implies x < w in P.
Critical pairs play an important role in dimension theory: if for each critical pair
(x, y), there are at least k linear extensions in F which reverse the pair (x, y)
then F is a k-fold realizer of P and vice versa [4].
For any permutation M of N, consider the set C(M) of critical pairs (x, y)
that satisfy the following two conditions:
1. x > (D(y) ∪{y}) in M if |D(y)| < ΔD
2. x > D(y) in M if |D(y)| = ΔD

Scheduling with Precedence Constraints of Low Fractional Dimension
137
In [8], Felsner & Trotter present an algorithm that converts in polynomial time a
permutation M of N to a linear extension L of P so that L reverses all critical pairs
in the set C(M). Now set t = |N|! and consider the set M = {M1, M2, . . . , Mt}
of all permutations of the ground set N. Observe that for any critical pair (x, y)
there are at least n!/(ΔD + 1) diﬀerent permutations Mi ∈M, where the critical
pair is reversed, i.e., (y, x) ∈C(Mi). Applying the algorithm in [8] we obtain a
k-fold realizer F = {L1, . . . , Lt} of P with t = n! and k = n!/(ΔD +1). Moreover,
we can eﬃciently pick uniformly at random one linear extension from F: generate
uniformly at random one permutation of jobs (e.g. by using Knuth’s shuﬄe algo-
rithm) and transform it into a linear extension with the described properties by
using the algorithm in [8]. The described algorithm can be derandomized by us-
ing the classical method of conditional probabilities. Finally observe that we can
repeat a similar analysis by using ΔU instead of ΔD.
⊓⊔
In fact, this result is stronger than the same statement with d = Δ(P). To
see this, consider the graph poset P(G) = (N, P) deﬁned as follows: given an
undirected graph G(V, E), let N = V ∪E and for every v ∈V and e = {v1, v2} ∈
E, put (v, e) ∈P if and only if v ∈{v1, v2}. If Δ(G) is unbounded, this also
holds for Δ(P). However, since every edge is adjacent to only two vertices, ΔD
is bounded by 2, thus the value 1 + min{ΔU, ΔD} is also bounded. On the
other hand, for the complete graph on n nodes, Kn, Spencer [31] showed that
dim(P(Kn)) = Θ(log log n). Therefore, the poset P(Kn) is an example where
the dimension of the poset is unbounded, while min{ΔD, ΔU} (and thus also
the fractional dimension) is bounded. This means that the fractional dimension
approach can yield a substantially better result than the dimension approach
used in [3].
4.4
Lexicographic Sums
In this section we show how to use previous results to obtain approximation al-
gorithms for new ordered sets. The construction we use here, lexicographic sums,
comes from a very simple pictorial idea (see [32] for a more comprehensive dis-
cussion). Take a poset P = (N, P) and replace each of its points x ∈N with a
partially ordered set Qx, the module, such that the points in the module have the
same relation to points outside it. A more formal deﬁnition follows. For a poset
P = (N, P) and a family of posets S = {(Yx, Qx) | x ∈N} indexed by the ele-
ments in N, the lexicographic sum of S over (N, P), denoted 
x∈(N,P )(Yx, Qx)
is the poset (Z, R) where Z = {(x, y) | x ∈N, y ∈Yx} and (x1, y1) ≤(x2, y2) in
R if and only if one of the following two statements holds:
1. x1 < x2 in P.
2. x1 = x2 and y1 ≤y2 in Qx1.
We call P = P ∪F the components of the lexicographic sum. A lexicographic
sum is trivial if |N| = 1 or if |Yx| = 1 for all x ∈N. A poset is decomposable with
respect to lexicographic sums if it is isomorphic to a non-trivial lexicographic
sum.

138
C. Amb¨uhl et al.
In case the precedence constraints of every component admit an eﬃciently sam-
plable realizer, we observe that this translates into a randomized approximation
algorithm:
Theorem 7. Problem 1|prec| 
j wjCj, whenever precedence constraints form
a lexicographic sum whose components i ∈P admit eﬃciently samplable realiz-
ers, has a polynomial time randomized (2−2t
k )−approximation algorithm, where
t/k = maxi∈P(ti/ki).
Finally, we point out that, if the approximation algorithm for each component
can be derandomized, this yields a derandomized approximation algorithm for
the lexicographic sum.
5
NP-Completeness for Interval Orders
In this section we show that 1|prec| 
j wjCj remains NP-complete even in the
special case of interval order precedence constraints. To prove this we exploit
the vertex cover nature of problem 1|prec |  wjCj.
Theorem 8. Problem 1|interval-order| 
j wjCj is NP-complete.
Proof. A graph G is said to have bounded degree d if every vertex v in G is
adjacent to at most d other vertices. The problem of deciding if a graph G
with bounded degree 3 has a (unweighted) vertex cover of size at most m is
NP-complete [11]. We provide a reduction from the minimum vertex cover on
graphs with bounded degree 3 to 1|interval-order| 
j wjCj.
Given a connected graph G = (V, E) with bounded degree 3, we construct an
instance S of 1|interval-order| 
j wjCj so that S has a schedule with value less
than m + c + 1 if and only if G has a vertex cover of size at most m, where c is
a ﬁxed value deﬁned later (see Equation (1)). We present the construction of S
in two stages.
Stage 1 (Tree-layout of the graph). Starting from any vertex s ∈V , consider
the tree T = (V, ET ), with ET ⊆E, rooted at s on the set of nodes reachable
from s by using, for example, breadth-ﬁrst search. Furthermore, we number the
vertices of T top-down and left-right. Figure 1 shows the breadth-ﬁrst search
tree T for K4.
Deﬁne G′ = (V ′, E′) to be the graph obtained from T in the following way. For
each vertex vi in T we add two new vertices ui
2, ui
1 and edges {ui
2, ui
1}, {ui
1, vi}.
Furthermore, for each edge {vi, vj} ∈E \ ET with i < j we add vertices eij
1 , eij
2
and edges {vi, eij
1 }, {eij
1 , eij
2 }, {eij
2 , uj
2}.
The following claim relates the optimum unweighted vertex covers of G and G′.
Claim 1. Let C∗⊆V and C′
∗⊆V ′ be optimum vertex cover solutions to G and
G′, respectively, then |C∗| = |C′
∗|−|V |−|E\ET |. (For a proof, see Appendix A.2).

Scheduling with Precedence Constraints of Low Fractional Dimension
139
      


      


         


         


        



         


            



      


         


        



      


         


      


         


G = K4
G′
T
v1
v1
v2
v2
v3
v3
v4
v4
u1
1
u2
1
u3
1
u4
1
u1
2
u2
2
u3
2
u4
2
e23
1
e23
2
e24
1
e24
2
e34
1
e34
2
Fig. 1. The breadth ﬁrst search tree T = (V, ET ) for the graph G = K4, and the graph
G′. The solid edges belong to ET .
Stage 2 (Construction of scheduling instance). Given the vertex cover
graph G = (V, E) and its corresponding tree T = (V, ET ), we construct the
scheduling instance S with processing times, weights, and precedence constraints
to form an interval order I as deﬁned below (see Figure 2 for an example), where
k is a value to be determined later.
Job
Interval Repr.
Proc. Time Weight
s0
[-1,0]
1
0
s1
[0, 1]
1/k
1
sj, j = 2, . . . , |V |
[i, j], where
1/kj
ki
{vi, vj} ∈ET , i < j
mi, i = 1, . . . , |V |
[i −1
2, |V | + i]
1/k(|V |+i)
ki
ei, i = 1, . . . , |V |
[|V | + i, |V | + i + 1] 0
k(|V |+i)
bij, where
{vi, vj} ∈E \ ET , i < j [i, j −1
2]
1/kj
ki
Remark 1. Let i and j be two jobs in S with interval representations [a, b] and
[c, d] respectively, where a ≤d. By the construction of the scheduling instance
S we have pi ≤1/k⌈b⌉and wj ≤k⌈c⌉. It follows that pi · wj = 1 or pi · wj ≤
1/k if i and j are incomparable, since pi · wj ≥k implies that b < c, i.e., i’s
interval representation is completely to the left of j’s interval representation.
Furthermore, if pi · wj = 1 then ⌈b⌉= ⌈c⌉.
Let D =
{(s0, s1)}
∪{(si, sj) : vi is the parent of vj in T }
∪{(si, mi), (mi, ei) : i = 1, 2, . . ., |V |}
∪{(si, bij), (bij, mj) : {vi, vj} ∈E \ ET , i < j}
By the interval representation of the jobs and the remark above, we have the
following:
Claim 2. A pair of incomparable jobs (i,j) has pi ·wj = 1 if (i, j) ∈D; otherwise
if (i, j) ̸∈D then pi · wj ≤1/k.

140
C. Amb¨uhl et al.
        



            



            



        



            



      


         


            



            



      


        



            



            



         


G′ ∼= G′
I
I
(s0, s1)
(s1, s2)
(s1, s3)
(s1, s4)
(s1, m1)
(s2, m2)
(s3, m3)
(s4, m4)
(m1, e1)
(m2, e2)
(m3, e3)
(m4, e4)
(s2, b23)
(b23, m3)
(s2, b24)
(b24, m4)
(s3, b34)
(b34, m4)
0
1
2
3
4
5
6
7
8
9
s0
s1
s2
s3
s4
m1
m2
m3
m4
e1
e2
e3
e4
b23
b24
b34
Fig. 2. The interval order I obtained from K4; G′
I is the subgraph induced on the
graph of incomparable pairs GI by the vertex subset D (the vertices with weight 1)
Claim 3. Let G′
I = (D, EI) be the subgraph induced on the graph of incompa-
rable pairs GI by the vertex subset D. Then G′ and G′
I are isomorphic. (For a
proof, see Appendix A.3).
By Claim 2, each incomparable pair of jobs (i, j) ̸∈D satisﬁes p(i)·w(j) ≤1/k.
Let n be the number of jobs in the scheduling instance S and select k to be n2+1.
Let C,CI, and C′
I be optimal vertex cover solutions to G, GI and G′
I (deﬁned
as in Claim 3), respectively. Then, by the selection of k and Claim 2, we have
|C′
I| ≤|CI| ≤|C′
I| +

(i,j)∈inc(I)\D
piwj < |C′
I| + 1. Furthermore, Claims 3 and 1
give us that |C|+|V |+|E \ ET | ≤|CI| < |C|+|V |+|E \ ET |+1. This, together
with Theorem 1, implies that |C| ≤m if and only if there is a schedule of S with
value less than m + c + 1, where
c = |V | + |E \ ET | +

(i,j)∈I
pi · wj.
(1)
⊓⊔
Acknowledgements
We are grateful to Andreas Schulz for many helpful discussions we had with
him during his visit at IDSIA. This research is supported by Swiss National Sci-
ence Foundation project 200021-104017/1, “Power Aware Computing”, and by
the Swiss National Science Foundation project 200020-109854, “Approximation
Algorithms for Machine scheduling Through Theory and Experiments II”. The
ﬁrst author is supported by Nuﬃeld Foundation Grant NAL32608.

Scheduling with Precedence Constraints of Low Fractional Dimension
141
References
1. P. Alimonti and V. Kann. Some APX-completeness results for cubic graphs. Theor.
Comput. Sci., 237(1-2):123–134, 2000.
2. C. Amb¨uhl and M. Mastrolilli. Single machine precedence constrained scheduling
is a vertex cover problem. In Proceedings of the 14th Annual European Symposium
on Algorithms (ESA), volume 4168 of Lecture Notes in Computer Science, pages
28–39. Springer, 2006.
3. C. Amb¨uhl, M. Mastrolilli, and O. Svensson.
Approximating precedence-
constrained single machine scheduling by coloring.
In Proceedings of APPROX
+ RANDOM, volume 4110 of Lecture Notes in Computer Science, pages 15–26.
Springer, 2006.
4. G. R. Brightwell and E. R. Scheinerman. Fractional dimension of partial orders.
Order, 9:139–158, 1992.
5. C. Chekuri and R. Motwani. Precedence constrained scheduling to minimize sum
of weighted completion times on a single machine. Discrete Applied Mathematics,
98(1-2):29–38, 1999.
6. F. A. Chudak and D. S. Hochbaum. A half-integral linear programming relax-
ation for scheduling precedence-constrained jobs on a single machine. Operations
Research Letters, 25:199–204, 1999.
7. J. R. Correa and A. S. Schulz. Single machine scheduling with precedence con-
straints. Mathematics of Operations Research, 30(4):1005–1021, 2005. Extended
abstract in Proceedings of the 10th Conference on Integer Programming and Com-
binatorial Optimization (IPCO 2004), pages 283–297.
8. S. Felsner and W. T. Trotter. On the fractional dimension of partially ordered
sets. DMATH: Discrete Mathematics, 136:101–117, 1994.
9. S. Felsner and W. T. Trotter. Dimension, graph and hypergraph coloring. Order,
17(2):167–177, 2000.
10. P. C. Fishburn. Interval Orders and Interval Graphs. John Wiley and Sons, 1985.
11. M. R. Garey, D. S. Johnson, and L. J. Stockmeyer. Some simpliﬁed NP-complete
graph problems. Theor. Comput. Sci., 1(3):237–267, 1976.
12. R. Graham, E. Lawler, J. K. Lenstra, and A. H. G. Rinnooy Kan. Optimization
and approximation in deterministic sequencing and scheduling: A survey. In Annals
of Discrete Mathematics, volume 5, pages 287–326. North–Holland, 1979.
13. L. A. Hall, A. S. Schulz, D. B. Shmoys, and J. Wein. Scheduling to minimize av-
erage completion time: oﬀ-line and on-line algorithms. Mathematics of Operations
Research, 22:513–544, 1997.
14. D. S. Hochbaum. Eﬃcient bounds for the stable set, vertex cover and set packing
problems. Discrete Applied Mathematics, 6:243–254, 1983.
15. K. Jain, R. Hedge. Some inapproximability results for the (fractional) poset di-
mension Personal communication, 2006
16. S. G. Kolliopoulos and G. Steiner. Partially-ordered knapsack and applications to
scheduling. In Proceedings of the 10th Annual European Symposium on Algorithms
(ESA), pages 612–624, 2002.
17. E. L. Lawler. Sequencing jobs to minimize total weighted completion time subject
to precedence constraints. Annals of Discrete Mathematics, 2:75–90, 1978.
18. J. K. Lenstra and A. H. G. Rinnooy Kan. The complexity of scheduling under
precedence constraints. Operations Research, 26:22–35, 1978.
19. T.-H. Ma and J. P. Spinrad. On the 2-chain subgraph cover and related problems.
J. Algorithms, 17(2):251–268, 1994.

142
C. Amb¨uhl et al.
20. F. Margot, M. Queyranne, and Y. Wang. Decompositions, network ﬂows and a
precedence constrained single machine scheduling problem. Operations Research,
51(6):981–992, 2003.
21. R. H. M¨ohring.
Computationally tractable classes of ordered sets. In I. Rival,
editor, Algorithms and Order, pages 105–193. Kluwer Academic, 1989.
22. R. H. M¨ohring. Computationally tractable classes of ordered sets. Algorithms and
Order, pages 105–194, 1989.
23. G. L. Nemhauser and L. E. Trotter. Properties of vertex packing and independence
system polyhedra. Mathematical Programming, 6:48–61, 1973.
24. G. L. Nemhauser and L. E. Trotter. Vertex packings: Structural properties and
algorithms. Mathematical Programming, 8:232–248, 1975.
25. C. H. Papadimitriou and M. Yannakakis. Scheduling interval-ordered tasks. SIAM
Journal of Computing, 8:405–409, 1979.
26. N. N. Pisaruk. A fully combinatorial 2-approximation algorithm for precedence-
constrained scheduling a single machine to minimize average weighted completion
time. Discrete Applied Mathematics, 131(3):655–663, 2003.
27. I. Rabinovitch. The dimension of semiorders. J. Comb. Theory, Ser. A, 25:50–61,
1978.
28. E. R. Scheinerman and D. H. Ullman. Fractional Graph Theory. John Wiley and
Sons Inc., 1997.
29. A. S. Schulz. Scheduling to minimize total weighted completion time: Performance
guarantees of LP-based heuristics and lower bounds. In Proceedings of the 5th Con-
ference on Integer Programming and Combinatorial Optimization (IPCO), pages
301–315, 1996.
30. P. Schuurman and G. J. Woeginger. Polynomial time approximation algorithms for
machine scheduling: ten open problems. Journal of Scheduling, 2(5):203–213, 1999.
31. J. Spencer. On minimum scrambling sets of simple orders. Acta Mathematica,
22:349–353, 1971.
32. W. T. Trotter.
Combinatorics and Partially Ordered Sets: Dimension Theory.
Johns Hopkins Series in the Mathematical Sciences. The Johns Hopkins University
Press, 1992.
33. G. J. Woeginger. On the approximability of average completion time scheduling
under precedence constraints. Discrete Applied Mathematics, 131(1):237–252, 2003.
A
Omitted Proofs
A.1
Proof of Theorem 2
Proof. Let S be an instance of 1|prec| 
j wjCj where precedence constraints
are given by a poset P = (N, P) that admits an eﬃciently samplable k:t-realizer
F = {L1, L2, . . . , Lt}. Furthermore, we assume that fdim(P) ≥2. The case when
fdim(P) = 1, i.e., P is a linear order, is trivial.
Let VP and EP be the vertex set and edge set, respectively, of the graph of in-
comparable pairs GP. Consider the weighted vertex cover V CS on GP where each
vertex (incomparable pair) (i, j) ∈VP has weight w(i,j) = pi · wj, as speciﬁed in
the deﬁnition of V CS (see Section 2.2). Solve the [VC-LP] formulation of V CS (see
Section 3) and let Vi be the set of vertices with value i (i = 0, 1
2, 1) in the optimum
solution. Denote by GP[V1/2] the subgraph of GP induced by the vertex set V1/2.

Scheduling with Precedence Constraints of Low Fractional Dimension
143
We consider the linear extensions of F as outcomes in a uniform sample space. For
an incomparable pair (x, y), the probability that y is over x in F is given by
ProbF[y > x] = 1
t |{i = 1, . . . , t : y > x ∈Li}| ≥k
t
(2)
The last inequality holds because every incomparable pair is reversed in at least
k linear extensions of F.
Let us pick one linear extension L uniformly at random from F = {L1, . . . , Lt}.
Then, by linearity of expectation, the expected value of the independent set I1/2,
obtained by taking the incomparable pairs in V1/2 that are reversed in L, is
E[w(I1/2)] =

(i,j)∈V1/2
ProbF[j > i] · w(i,j) ≥k
t · w(V1/2)
(3)
A vertex cover solution C for the graph GP[V1/2] can be obtained by picking the
nodes that are not in I1/2, namely C = V1/2 \ I1/2. The expected value of this
solution is
E[w(C)] = w(V1/2) −E[w(I1/2)] ≤

1 −k
t

w(V1/2)
As observed in [14], V1 ∪C gives a valid vertex cover for graph GP. Moreover,
the expected value of the cover is bounded as follows
E[w(V1 ∪C)] ≤w(V1) +

1 −k
t

w(V1/2)
(4)
≤2

1 −k
t
 
w(V1) + 1
2w(V1/2)

(5)
≤

2 −2k
t

OPT
(6)
where the last inequality holds since w(V1) + 1
2w(V1/2) is the optimal value
of [VC-LP]. Note that t/k ≥fdim(P) ≥2 was used for the second inequal-
ity. Theorem 1 implies that any α-approximation algorithm for V CS also gives
an α-approximation algorithm for S. Thus we obtain a randomized (2 −2 k
t )-
approximation algorithm for S.
⊓⊔
A.2
Proof of Claim 1
This proof is similar to the proof in [1] for proving APX-completeness of vertex
cover on cubic graphs.
Proof of Claim. It is easy to see that from every vertex cover C ⊆V of G we
can construct a vertex cover C′ ⊆V ′ of G′ of size exactly |C| + |V | + |E \ ET |.
In C′ we include ui
1 for all i ∈{i : vi ∈V \ C}; ui
2 for all i ∈{i : vi ∈C}; eij
1
for each (vi, vj) ∈E \ ET with vi ∈V \ C; eij
2 for each (vi, vj) ∈E \ ET with
vi ∈C; and every vertex in C.
Given a vertex cover C′ ⊆V ′ of G′ we transform it into a vertex cover C ⊆V
of G in the following manner. Suppose there exists vi, vj ∈V with i < j such

144
C. Amb¨uhl et al.
that {vi, vj} ∈E and vi ̸∈C′, vj ̸∈C′. Since C′ is a feasible vertex cover of G′
we have that {vi, vj} ∈E\ET and either {eij
1 , eij
2 , uj
1} ⊆C′ or {eij
1 , uj
2, uj
1} ⊆C′.
Thus we can obtain a vertex cover C′′ ⊆V ′ of G′ with |C′′| ≤|C′| by letting
C′′ = (C′ \ {uj
1, eij
2 }) ∪{vj, uj
2}. Repeating this procedure will result in a vertex
cover C′′′ ⊆V ′ of G′ with |C′′′| ≤|C′| such that C = C′′′ ∩V is a feasible vertex
cover of G. Furthermore it is easy to see that |C| ≤|C′′′| −|V | −|E \ ET |.
⊓⊔
A.3
Proof of Claim 3
Proof of Claim. We relate the two graphs G′
I and G′ by the bijection f : D →V ′,
deﬁned as follows.
f((a, b)) =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
vj, if (a, b) = (si, sj),
ui
1, if (a, b) = (si, mi),
ui
2, if (a, b) = (mi, ei),
eij
1 , if (a, b) = (si, bij),
eij
2 , if (a, b) = (bij, mj).
Suppose {(a, b), (c, d)} ∈EI. Since I is an interval order (does not contain
any 2 + 2 structures as induced posets [21,32]) and by the deﬁnition of D we
have that b = c. Now consider the possible cases of {(a, b), (b, d)}.
(a = si, b = sj, d = sk, i < j < k) By construction of I, vj is the parent of vk,
i.e., (f((si, sj)), f((sj, sk)) = (vj, vk) ∈ET ⊆E′.
(a = si, b = sj, d = bjk, i < j < k) Then f((si, sj)) = vj and f((sj, bjk)) = eij
1
and by deﬁnition of G′ we have (vj, ejk
1 ) ∈E′.
The remaining cases (a = si, b = sj, d = mj, i < j), (a = si, b = bij, d =
mj, i < j), (a = si, b = mi, d = ei), and (a = bij, b = mj, d = ej, i < j)
are similar to the two above and it is straightforward to check the implication
{(a, b), (b, d)} ∈EI ⇒{f((a, b)), f((b, c))} ∈E′.
On the other hand, suppose (a, b) ∈E′ and again consider the diﬀerent pos-
sible cases.
(a = vi, b = vj, i < j) Then vi is the parent of vj in T and f −1(vi) = (sk, si)
and f −1(vj) = (si, sj) for some k < i < j. Since sk’s interval representation
is completely to the left of sj’s interval representation in I the incomparable
pairs (sk, si) and (si, sj) cannot be reversed in the same linear extension,
i.e., {(sk, si), (si, sj)} ∈EI.
(a = vi, b = eij
1 , i < j) Then f −1(vi) = (sk, si) and f −1(eij
1 ) = (si, bij) for some
k < i < j. Since sk’s interval representation is completely to the left of
bij’s interval representation in I the incomparable pairs (sk, si) and (si, bij)
cannot be reversed in the same linear extension, i.e., {(sk, si), (si, bij)} ∈EI.
The remaining cases (a = eij
1 , b = eij
2 , i < j), (a = eij
2 , b = uj
2, i < j), (a = uj
1, b =
uj
2, i < j), and (a = vj, b = uj
1, i < j) are similar to the two above and omitted.
We have thus proved that {(a, b), (b, d)} ∈EI ⇔{f((a, b)), f((b, c))} ∈E′,
i.e., the function f deﬁnes an isomorphism between G′
I and G′.
⊓⊔

Approximation Algorithms for 2-Stage Stochastic
Scheduling Problems
David B. Shmoys1,⋆and Mauro Sozio2,⋆⋆
1 School of ORIE and Dept. of Computer Science, Cornell University, Ithaca, NY 14853
shmoys@cs.cornell.edu
2 Dept. of Computer Science, University of Rome “La Sapienza”, Italy
sozio@di.uniroma1.it
Abstract. There has been a series of results deriving approximation algorithms
for 2-stage discrete stochastic optimization problems, in which the probabilistic
component of the input is given by means of “black box”, from which the algo-
rithm “learns” the distribution by drawing (a polynomial number of ) indepen-
dent samples. The performance guarantees proved for such problems, of course,
is generally worse than for their deterministic analogue. We focus on a 2-stage
stochastic generalization of the problem of ﬁnding the maximum-weight subset
of jobs that can be scheduled on one machine where each job is constrained to
be processed within a speciﬁed time window. Surprisingly, we show that for this
generalization, the same performance guarantee that is obtained for the determin-
istic case can be obtained for its stochastic extension.
Our algorithm builds on an approach of Charikar, Chekuri, and P´al: one ﬁrst
designs an approximation algorithm for the so-called polynomial scenario model
(in which the probability distribution is restricted to have the property that there
are only a polynomial number of possible realizations of the input that occur with
positive probability); then one shows that by sampling from the distribution via
the “black box” to obtain an approximate distribution that falls in this class and
approximately solves this approximation to the problem, one nonetheless obtains
a near-optimal solution to the original problem. Of course, to follow this broad
outline, one must design an approximation algorithm for the stochastic optimiza-
tion problem in the polynomial scenario model, and we do this by extending a
result of Bar-Noy, Bar-Yehuda, Freund, Naor, and Schieber.
Furthermore, the results of Bar-Noy et al. extend to a wide variety of resource-
constrained selection problems including, for example, the unrelated parallel-
machine generalization R|rj|  wjUj and point-to-point admission control
routing in networks (but with a different performance guarantee). Our techniques
can also be extended to yield analogous results for the 2-stage stochastic gener-
alizations for this class of problems.
1
Introduction
Consider the following 2-stage stochastic optimization problem: there are n users, each
of whom might request a particular communication channel, which can serve at most
⋆Research supported partially by NSF grants CCR-0635121 & DMI-0500263.
⋆⋆This work was done while this author was a visiting student at Cornell University. The work
was partially supported by NSF grant CCR-0430682 and by EC project DELIS.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 145–157, 2007.
c⃝Springer-Verlag Berlin Heidelberg 2007

146
D.B. Shmoys and M. Sozio
one user at a time, for a speciﬁed length of time within a speciﬁed time interval; for
a given planning period, it is not known which of the n users will actually make their
request – all that is known is a probability distribution over the subsets of users indicat-
ing which subset might be active; each user has an associated proﬁt for actually being
scheduled on the channel; alternatively, the manager of the channel can redirect the
user to other providers, thereby obtaining a speciﬁed (but signiﬁcantly smaller) proﬁt;
the aim is to decide which users to defer so as to maximize the expected proﬁt over
the two stages (where the expectation is with respect to the probability distribution over
subsets of active users). Thus, this is a stochastic generalization of the (maximization
version) of the single machine scheduling problem that is denoted in the notation of
[4] as 1|rj|  wjUj and we shall refer to this generalization as the 2-stage stochas-
tic 1|rj|  wjUj. For the deterministic version of this problem, Bar-Noy, Bar-Yehuda,
Freund, Naor, & Schieber give a ρ-approximation algorithm for any constant ρ > 2;
rather surprisingly, we show that the exact same result holds for the stochastic general-
ization. (A ρ-approximation algorithm for an optimization problem is a (randomized)
polynomial-time algorithm that ﬁnds a feasible solution with (expected) cost within a
factor of ρ of optimal.)
Recently, there has been a series of results for 2-stage discrete stochastic optimiza-
tion problems with recourse, starting with the work of Dye, Stougie, and Tomasgard[3]
that addressed a knapsack-like single-node network provisioning problem. That paper
made the simplifying assumption of the polynomial scenario model in which there are
(only) a polynomial number of scenarios that can be realized in the second stage, and
thereby derived the ﬁrst worst-case performance guarantees for polynomial-time algo-
rithms for models of this type. Kong & Schaefer [8] gave an 2-approximation algo-
rithm for a 2-stage variant of the the maximum-weight matching problem, again in a
polynomial scenario model. Later, Immorlica, Karger, Minkoff, and Mirrokni [7], and
also Ravi and Sinha [9] addressed analogous questions based on deterministic prob-
lems such as the vertex cover problem, the set covering problem, the uncapacitated
facility location problem, and network ﬂow problems. The former paper also con-
sidered the situation when the probability distribution conformed to an independent
activation model which, in our setting for example, would mean that there is a prob-
ability associated with each user and the active set is drawn by assuming that these
are independent Bernoulli random events. However, for these latter results they in-
troduced the proportionality assumption in which the corresponding costs for an el-
ement in the two stages had constant ratio λ for all elements. Gupta, P´al, Ravi, and
Sinha [5] proposed a much more general mechanism for specifying the probability
distribution, in which one has access to a black box from which to generate inde-
pendent samples according to the distribution, and thereby make use of a polynomial
number of samples in the process of computing the ﬁrst-stage decisions. They gave con-
stant approximation algorithms for a number of 2-stage stochastic optimization prob-
lems in this model, most notably the minimum-cost rooted Steiner tree problem and
the uncapacitated facility location problem, but they also require the proportionality
assumption.

Approximation Algorithms for 2-Stage Stochastic Scheduling Problems
147
Shmoys & Swamy [10] gave an LP-rounding technique, and showed that one could
derive a polynomial-time approximation scheme for the exponentially-large linear pro-
gramming relaxations in order to derive the ﬁrst approximation algorithms in the black
box model without the proportionality assumption, in particular for a variety of set
covering-related problems, the uncapacitated facility location problem, and multi-com-
modity ﬂow problems. Swamy & Shmoys [11] extend this to constant-stage models,
and also show that the so-called sample average approximation yields a polynomial
approximation scheme for the LP relaxations. Charikar, Chekuri, and P´al [2] gave a
general technique based on the sample average approximation that, for a broad class of
2-stage stochastic minimization problem with recourse, in effect reduced the problem
of obtaining a good approximation algorithm for the black box model, to the problem
of obtaining the analogous result in the polynomial scenario setting.
We build on these results, by ﬁrst constructing an approximation algorithm for our
maximization problem in the polynomial scenario model, and then derive a maximiza-
tion variant of the result of [2] (but still specialized to our class of problems) to obtain
approximation algorithms in the black box probability model.
We focus on the central model in the class proposed by Bar-Noy, Bar-Yehuda, Fre-
und, Naor, and Schieber [1], who gave primal-dual algorithms for a rich class of de-
terministic resource allocation and scheduling problems. In their terminology, there is
a set of activities, {A1, . . . , An}; let N = {1, . . ., n} index this set. For each activ-
ity Aj, j ∈N, there is a set of possible instances Aj that specify the various ways
in which the activity might be handled (so, in the description above, assuming integer
data for the input times, for each user we have one instance for each possible integer
starting time that would have it complete by the deadline). This approach appears to
convert the original input to a new input in which there are a pseudopolynomial number
of instances for each activity. However, Bar-Noy et al. also show how to convert their
pseudopolynomial-time algorithm into a polynomial-time one, while losing only a 1+ϵ
factor in the performance guarantee.
Our algorithm is a rather natural extension of the approach of Bar-Noy et al. We ﬁrst
run their algorithm on each of the polynomially many scenarios, where the proﬁt of
selecting an instance is its contribution to the overall expected second stage proﬁt. For
each scenario (which is, after all just an ordinary deterministic input), this generates a
feasible dual solution. The deterministic dual variables are of two types: those that are
dual to the constraint that says that each activity is scheduled in at most one way (that
is, at most one instance of each activity is selected); and those that correspond to the
constraint that at each time at most one instance (over all activities) is active. The usual
interpretation of dual variables leads us to view the former as providing the marginal ex-
pected proﬁt attainable by having this activity on hand in a particular scenario. Thus, we
decide to defer an activity Aj, if the total of the corresponding dual variables, summed
over all scenarios, is less than the proﬁt collected by actually deferring that activity.
This gives the stage I actions. The stage II actions for each scenario are computed by
adapting the algorithm of Bar-Noy et al.; we ﬁrst compute a dual solution that includes
even the deferred activities, but then does not select any instance of a deferred activity
in constructing the primal solution.

148
D.B. Shmoys and M. Sozio
The analysis of our algorithm is also surprisingly simple, and is based on a primal-
dual approach using an integer programming formulation of the 2-stage problem. We
show that the dual solutions constructed in each scenario can be pieced together to
yield a feasible solution for the dual to the linear programming relaxation, and can
then show that the expected proﬁt of the primal solution constructed is at least half the
value of the feasible dual solution found. This yields that the resulting algorithm is a
2-approximation algorithm. Like the algorithm of Bar-Noy et al., this is a pseudopoly-
nomial-time algorithm, but an approach identical to the one they employed yields a
polynomial-time algorithm, while losing a factor of 1 + ϵ in the performance guaran-
tee. Although we focus on this single-machine scheduling model, our approach can be
generalized to yield analogously strong results for 2-stage stochastic generalization of
the class of problems for which the framework of Bar-Noy et al. applies. This will be
discussed in detail in the full version of this paper.
There are other potential 2-stage stochastic extensions of the problem of computing
a maximum-weight subset of jobs that can be feasible scheduled. One other natural ap-
proach is to use the ﬁrst stage to make initial decisions about which users to service
(but to commit to serve them if they are active), and then to allow the possibility of
serving additional users in the second stage, once the probabilistic choice of scenario
has been made (with correspondingly lesser proﬁt). We show that the maximum in-
dependent set problem can be reduced to an extremely restricted special case of this
model in an approximation-preserving way, and hence we cannot hope to obtain a good
approximation algorithm for this setting (unless P = NP). There are few (if any) such
strong inapproximability results known for stochastic optimization problems for which
their deterministic analogue is relatively easily approximable.
2
IP and LP Formulations: 2-Stage Stochastic Models
We start by giving a natural integer (linear) programming formulation (and its dual) for
the 2-stage stochastic version of 1|rj| 
j wjUj, in its pseudopolynomial-sized variant.
Let S be a collection of explicitly given scenarios {S1, . . . , Sm} that occur with
positive probability; in each scenario S, for each activity Aj, there is an associated set
of available instances Aj(S) ⊆Aj. For each instance I, there is an associated starting
time s(I), and an associated ending time e(I). For each scenario S ∈S, there is an
associated probability q(S), where q(S) ≥0 and 
S∈S q(S) = 1. In stage I, we must
decide which activities to defer, and thereby obtain a (small) proﬁt of pI
j, or else retain
for stage II, in which for each scenario S we can obtain a proﬁt pII
j (I, S) for assigning
this activity using instance I ∈Aj(S). We give an integer programming formulation
of this problem. For each activity Aj, we have a 0-1 variable xj that indicates whether
activity Aj is deferred in the ﬁrst phase or not (where xj = 1 means that it is deferred).
For each instance I of activity Aj(S), we have a variable yj(I, S) whose value is 1 if
and only if instance I of this activity is scheduled. Let T be the set of all start-times and
end-times of all instances belonging to all activities and let TI = {t ∈T |s(I) ≤t <
e(I)} for each instance I. Moreover, let f(I) ∈T be maximal such that f(I) < e(I).

Approximation Algorithms for 2-Stage Stochastic Scheduling Problems
149
We can formulate the 2-stage problem of maximizing the total expected proﬁt as
follows:
max

j∈N
pI
jxj +

j∈N

S∈S

I∈Aj(S)
q(S)pII
j (I, S)yj(I, S)
(SIP)
s.t. xj +

I∈Aj(S)
yj(I, S) ≤1
∀j ∈N, S ∈S, (1)

j∈N

I∈Aj(S):t∈TI
yj(I, S) ≤1
∀S ∈S, t ∈T , (2)
xj, yj(I, S) ∈{0, 1},
∀j ∈N, S ∈S, I ∈Aj(S). (3)
Let (SLP) be the LP obtained by replacing (3) by non-negativity constraints for these
variables. If we let uj(S) be the dual variables corresponding to the constraints (1), and
let vt(S) denote the dual variables corresponding to the constraints (2), then we can
write the LP dual of (SLP) as:
min

j∈N

S∈S
uj(S) +

S∈S

t∈T
vt(S)
(SD)
s.t.

S∈S
uj(S) ≥pI
j,
∀j ∈N,
(4)
uj(S) +

t∈TI
vt(S) ≥q(S)pII
j (I, S),
∀j ∈N, S ∈S, I ∈Aj(S),
(5)
uj(S), vt(S) ≥0.
(6)
It is important to note that our algorithm will not need to solve any of these linear
programs! We will simply apply an algorithm for the deterministic variant (for which a
performance guarantee relative the optimal value of the deterministic LP is known) to
an input based on each scenario S ∈S, and then use the linear programs to analyze the
performance of the resulting algorithm.
3
An Algorithm for the Polynomial Scenario Model
We shall show how to adapt the primal-dual algorithmic framework of Bar-Noy, Bar-
Yehua, Freund, Naor, & Schieber [1] to yield an approximation algorithm with the
identical performance guarantee for the 2-stage stochastic variant of 1|rj|  wjUj, in
the polynomial scenario model. For this model, it is straightforward to derive a constant
approximation algorithm. The simplest approach is to randomize, and with probability
1/2 to defer all jobs, and otherwise, to run the 2-approximation algorithm of Bar-Noy
et al. on the active jobs in the second stage; this is a randomized 4-approximation algo-
rithm. In the polynomial scenario model, one can improve upon this by comparing the
beneﬁt of deferring all users with the expected proﬁt obtained by the Bar-Noy algorithm
based on not deferring anyone, and then selecting the better of the two. This is easily
shown to be a 3-approximation algorithm (and can be extended to the black box model
while losing only a factor of 1 + ϵ). Thus, the surprising aspect of our result is that it is

150
D.B. Shmoys and M. Sozio
in fact possible to obtain an algorithm for the 2-stage generalization without degrading
the performance guarantee at all.
The framework of Bar-Noy et al. works in two phases: a pushing phase in which
a dual solution is constructed along with a stack of instances that might be selected
to be scheduled; and a popping phase in which elements of the stack are popped off,
and accepted for scheduling provided that they do not conﬂict with activities already
scheduled by this procedure.
The algorithm for the 2-stage problem proceeds as follows. For each scenario S ∈S,
the deterministic proﬁt pj(I) is q(S)pII
j (I, S) for each j ∈N, and each I ∈Aj(S).
We execute the pushing procedure of the algorithm proposed in Bar-Noy et al. for each
scenario S ∈S. Algorithm 1 shows the pseudocode for this procedure. We let uj(S)
denote the dual variable corresponding to the deterministic analogue of (1) computed
by this procedure. Then, for each activity Aj, j ∈N, we check if
pI
j ≥

S∈S
uj(S),
(7)
and defer each activity Aj that satisﬁes this condition. This completes the ﬁrst stage
action. We shall also denote this solution by setting ¯xj = 1 for each deferred activity
Aj, and setting ¯xj = 0 otherwise.
In what follows, we shall say that an instance I is uncovered if constraint (5) for
instance I is not satisﬁed and we say that I is tight if this constraint is satisﬁed with
equality.
For the second stage, for a given scenario S ∈S, we recompute the execution of the
pushing procedure. Then we compute a feasible schedule by executing the popping pro-
cedure of the algorithm of Bar-Noy et al., but we delete each activity that was deferred
in the ﬁrst phase. We denote this solution by setting ¯yj(I, S) = 1 for each scheduled
instance I, and setting ¯yj(I, S) = 0 otherwise. Algorithm 2 shows the pseudocode for
the second phase for a given scenario.
The main intuition behind the deferring rule is the following. Suppose at the end of
the pushing phase the total value of variables u of an activity Aj is “small”. There are
two possible reasons for this. The total proﬁt of all instances of Aj is smaller than pI
j.
In this case, it is clear that deferring the activity is the best we can do. If the total proﬁt
P of instances of Aj is greater than pI
j, then since u is “small”, there are many other
instances of other activities which are in conﬂict with instances of Aj. Hence, P can
be “replaced” by the proﬁt of these instances, and we can gain other proﬁt by deferring
Aj. More generally, the value of the sum reﬂects the total expected marginal value of
the activity Aj; if this is less than the (sure) proﬁt gained by deferring it, then certainly
deferring it is a good thing to do.
We shall prove that the performance guarantee of the two-phase algorithm is 2. The
main idea behind this proof is the following. Each instance increases the total value
of the dual variables by some amount 2δ. For instances that belong to a non-deferred
activity, we are able to charge δ to a scheduled instance. For instances that belong to a
deferred activity, we charge this amount to the proﬁt gained by deferring that activity.
Given a scenario S we say that I ∈Aj(S) and ˆI ∈Al(S) are incompatible if j = l
or their time intervals overlap. For each instance I ∈Aj(S), we refer to the variables
which occur in the constraint (5) for I, as “the variables of I”.

Approximation Algorithms for 2-Stage Stochastic Scheduling Problems
151
Algorithm 1. Pushing procedure for the ﬁrst phase in scenario S
1: Stack(S)=∅;
2: uj(S) ←0
∀j ∈N;
3: vt(S) ←0
∀t ∈T ;
4: while no uncovered instance is left do
5:
select an uncovered instance I ∈Aj(S), j ∈N with minimum end-time;
6:
push(I,Stack(S));
7:
let δ(I, S) = (q(S)pII
j (I, S) −uj(S) −
t∈TI vt(S))/2;
8:
uj(S) ←uj(S) + δ(I, S);
9:
vf(I)(S) ←vf(I)(S) + δ(I, S);
10: end while
Algorithm 2. The algorithm for the second phase in scenario S
1: /* pushing procedure */
2: Stack(S)=∅;
3: uj(S) ←0
∀j ∈N;
4: vt(S) ←0
∀t ∈T ;
5: while no uncovered instance is left do
6:
select an uncovered instance I ∈Aj(S), j ∈N with minimum end-time;
7:
push(I,Stack(S));
8:
let δ(I, S) = (q(S)pII
j (I, S) −uj(S) −
t∈TI vt(S))/2;
9:
uj(S) ←uj(S) + δ(I, S);
10:
vf(I)(S) ←vf(I)(S) + δ(I, S);
11: end while
12: /* scheduling procedure */
13: while Stack(S) is not empty do
14:
I=pop(Stack(S));
15:
Let j ∈N : I ∈Aj(S);
16:
if Aj is not deferred and I is not in conﬂict with other scheduled instances then
17:
schedule I and set ¯yj(I, S) = 1;
18:
end if
19: end while
Theorem 1. For the 2-stage stochastic maximization version of 1|rj|  wjUj, there is
a (2 + ϵ)-approximation algorithm in the polynomial scenario model.
Proof. We shall consider only the version of the problem in which we have a pseu-
dopolynomial representation of the input: that is, for each activity, we have an explic-
itly given set of allowed starting times. However, for each scenario, this is exactly the
algorithm of Bar-Noy et al. (on a carefully constructed input), who show that it can be
converted to run in polynomial time for 1|rj|  wjUj, while losing a factor of 1 + ϵ in
the performance guarantee. This will thereby yield the theorem in the form stated above.
Let ¯uj(S) and ¯vt(S) be the value of the dual variables u and v at the end of the algo-
rithm. First consider the constraints (5); the algorithm ensures that these are satisﬁed by
the dual solution computed. This is a consequence of the fact that as long as there exists

152
D.B. Shmoys and M. Sozio
an uncovered instance, the algorithm pushes an instance in the stack and increases its
dual variables making a constraint (5) tight. Hence, at the end of the algorithm, there
does not exist an uncovered instance, and each constraint (5) is satisﬁed. On the other
hand, constraint (4) can be violated by any deferred activity. In order to satisfy this
constraint, we increase the value of dual variables in the following way. Let
δj = pI
j −

S∈S
¯uj(S)
j = 1, . . . , n
and let ¯S ∈S, be an arbitrarily chosen scenario. For each activity Aj, we increase the
value of ¯uj(S) by δj. Clearly, this maintains that the other constraints are satisﬁed, and
ensures that constraint (4) is satisﬁed now as well.
We now prove the performance guarantee of the algorithm is 2. The essence of the
proof is as follows. In each scenario S, for each instance I of a non-deferred activity,
we charge δ(I, S) to some scheduled instance. For each instance I of a deferred activity
Aj, we charge δj and δ(I, S) to the proﬁt pI
j. Hence, at the end of the algorithm, all
amounts δ are “charged” to some proﬁt. Moreover, the sum of all these δ, multiplied by
2, gives a bound on the total value of the dual variables. The theorem then follows from
weak duality.
Consider a scenario S. Let ˆI ∈Aj(S) be an instance scheduled in S such that Aj is
not deferred, j ∈N. Let BˆI(S) be a set which contains ˆI and as well as instances that
are:
– incompatible with ˆI and
– pushed onto Stack(S) before ˆI.
Consider each instance I in BˆI(S). When I is placed on the stack, there are two dual
variables that are increased by δ(I, S). For each such I, one of these two variables are
variables of ˆI. If I ∈Aj(S), then the variable uj(S) occurs in constraint (5) for ˆI. Oth-
erwise, since e(ˆI) ≥e(I), then the variable vf(I)(S) occurs in this constraint. Let ˆu and
ˆv be the value of dual variables u and v at the time ˆI is pushed in the stack. We have that:

I∈Bˆ
I(S)
δ(I, S) ≤ˆuj(S) +

t∈T ˆ
I
ˆvt(S) ≤qSpII
j (ˆI, S)
(8)
where last inequality follows from the fact that ˆI is uncovered before being pushed on
the stack and after that, its variables are increased in order to make constraint (5) tight.
Note that each instance I of a non-deferred activity belongs to the set BˆI(S) for
some instance ˆI. This follows from the fact that either I is scheduled or there is another
instance ˆI pushed after I in the stack, which has been scheduled instead of I. This
implies that for each scenario S ∈S

j∈N:
¯xj=0

I∈Aj(S)
δ(I, S) =

j∈N:
¯xj=0

ˆI∈Aj(S):
yj(ˆI,S)=1

I∈Bˆ
I(S)
δ(I, S)
≤

j∈N:
¯xj=0

ˆI∈Aj(S)
qSpII
j (ˆI, S)¯yj(ˆI, S)
(9)

Approximation Algorithms for 2-Stage Stochastic Scheduling Problems
153
For each deferred activity Aj, we have that:
δj +

S∈S

I∈Aj(S)
δ(I, S) =

S∈S
¯uj(S) = pI
j
(10)
By combining Equation (9) and Equation (10), we obtain

j∈N
⎛
⎜
⎜
⎝δj +

S∈S
I∈Aj(S)
δ(I, S)
⎞
⎟
⎟
⎠=

S∈S

j∈N :
¯xj=0

I∈Aj(S)
δ(I, S) +

j∈N :
¯xj=1
⎛
⎜
⎜
⎝δj +

S∈S
I∈Aj(S)
δ(I, S)
⎞
⎟
⎟
⎠
≤

S∈S

j∈N :
¯xj=0

I∈Aj(S)
qSpII
j (I, S)¯yj(I, S) +

j∈N :
¯xj=1
pI
j
≤

j∈N
pI
j¯xj +

j∈N

S∈S
I∈Aj(S)
q(S)pII
j (I, S)¯yj(I, S)
(11)
Since the initial value of each dual variable is zero, and each instance I ∈Aj(S)
increases the total value of the dual variables by at most 2δ(I, S), we can sum over all
such δ to bound the total value of the dual variables:

j∈N

S∈S
¯uj(S) +

S∈S

t∈T
¯vt(S) ≤2
⎛
⎝
j∈N
⎛
⎝δj +

S∈S

I∈Aj(S)
δ(I, S)
⎞
⎠
⎞
⎠
(12)
Equations (11) and (12), together with the weak duality theorem, immediately imply
the claimed result.
4
An Algorithm for the Black Box Model
We show next that we can adapt the algorithm derived in the previous section for the
polynomial scenario setting to the black box model, where the probability distribution is
speciﬁed only by allowing access to an oracle from which independent samples accord-
ing the distribution can be drawn. We show that applying the previous algorithm to an
approximate version of the distribution based on sampling can be shown to still yield
the same performance guarantee. Our analysis uses the structure of the analysis used
for the previous algorithm, and builds on the general result for minimization 2-stage
stochastic problems derived by Charikar, Chekuri, and P´al [2].
We shall make use of the following version of the Chernoff bound.
Lemma 1. Let X1, . . . XN be independent random variables with Xi ∈[0, 1] and let
X = N
i=1 Xi. Then, for any ϵ ≥0, we have Pr [|X −E[X]| > ϵN] ≤2 exp(−ϵ2N).
We assume that there is an inﬂation factor λ ≥1 such that pII
j (I, S) ≤λpI
j, ∀j ∈
N, ∀S ∈S, ∀I ∈Aj(S).
The algorithm ﬁrst takes a polynomial-sized sample from the set of scenarios and
then proceeds just as the Algorithm 1 in Section 3 while using a slightly different
deferring rule.

154
D.B. Shmoys and M. Sozio
More precisely, it takes N = Θ( λ2
ϵ2 log n
γ ) independent random samples S1, . . . , SN
from the black box, where n is the number of activities, ϵ will be the allowed additional
relative error, and γ is the conﬁdence parameter (that is, we shall obtain that the desired
approximation is found with probability at least 1 −γ). Then the algorithm executes
the pushing procedure (see Algorithm 1) for each scenario that occurs in the polynomial
sample. Observe that the data used by this algorithm for scenario S is described to be
q(S)pII
j (I, S). At ﬁrst glance, this might be worrying, but of course the value q(S) is just
a uniformscalar multiple for all proﬁts, and so it makes sense to deﬁne ˜u and ˜v as the dual
variables computed after executing this algorithm with inputs pII
j (I, S). Observe that the
values ¯u and ¯v for a scenario S from our exact distribution are equal to q(S)˜u and q(S)˜v,
respectively. Given ϵ > 0, we shall defers an activity Aj, j ∈N, if and only if:
(1 + ϵ)pI
j ≥1
N
N

i=1
˜uj(Si)
(13)
This is the deferring rule for the black box model.
This concludes the description of the ﬁrst stage action. For the second stage, for
a given scenario S ∈S, we execute Algorithm 2 for scenario S. (Again, note that
the linearity effect of q(S) implies that we can run the algorithm with inputs pII
j (I, S)
instead.)
Let us analyze the performance guarantee of this algorithm. The proof proceeds by
showing that, under the assumption that there is an inﬂation factor λ, equation (13) is a
good approximation for equation (7). This approach is inspired by the proof in [2] for
“low scenarios”.
Theorem 2. For any ϵ > 0 and γ > 0, with probability at least 1 −γ, the proposed
deferring rule is a (2 + ϵ)-approximation algorithm for the 2-stage stochastic variant
of the problem 1|rj|  wjUj in the black box model.
Proof. Suppose we run Algorithm 1 in each of the exponentially-manyscenarios and let
¯u and ¯v be the value of dual variables computed in this way. Consider activity Aj. Let
r =

S∈S
¯uj(S) =

S∈S
q(S)˜uj(S)
ˆr = 1
N
N

i=1
˜uj(Si).
We will prove that, with “high” probability, ˆr is “close” to r. We can view ˆr as the arith-
metic mean of N independent copies Q1, . . . , QN of the random variable Q deﬁned as
Q = ˜uj(S).
Note that E[Q] = r. Let Yi be the variable Qi/M where M = λpI
j and let Y = 
i Yi.
Note that for each activity Aj and for each scenario S ∈S, there exists some I ∈Aj(S)
such that ˜uj(S) ≤pII
j . This implies that Yi ∈[0, 1]. Moreover, Y = 
i Qi/M = N
M ˆr
and E[Y ] = 
i E[Qi]/M =
N
M r. By applying the Chernoff bound, we obtain the
following:
Pr

|Y −E[Y ]| > ϵ
λN

≤2 exp

−ϵ2
λ2 N

⇔Pr

|r −ˆr| > ϵpI
j

≤γ
n,
(14)

Approximation Algorithms for 2-Stage Stochastic Scheduling Problems
155
where the last inequality follows from the choice of the value of N. By taking the
union bound over all activities, we obtain that r is “close” to ˆr for all activities, with
probability at least 1 −γ.
We use the same argument as we used in the polynomial scenario model to show that
constraint (5) is satisﬁed. Consider constraint (4) for some scenario; it may be violated
by any activity. We show that it is satisﬁed, with high probability, by a non-deferred
activity. For a deferred activity, we shall increasing the value of its dual variables, as
we did in the polynomial scenario model so that the corresponding constraint is also
satisﬁed with high probability. (It is important to note that this increase in the dual
variables is not performed by the algorithm; it is only used for the analysis.)
For each deferred activity Aj, let
δj = pI
j −

S∈S
¯uj(S)
j = 1, . . . , N
and let S ∈S be an arbitrarily selected scenario. We increase the value of ¯uj(S) by
δj for each deferred activity Aj. From the fact that r is a good approximation of ˆr, it
follows that, for each activity Aj, if
1
N
N

i=1
˜uj(Si) ≤(1 + ϵ)pI
j,
then with probability at least 1 −γ,

S∈S
¯uj(S) ≤(1 + 2ϵ)pI
j.
(15)
This implies that with high probability, for each deferred activity Aj
δj +

S∈S

I∈Aj(S)
δ(I, S) =

S∈S
¯uj(S) ≤(1 + 2ϵ)pI
j
(16)
In a similar way, if for an activity Aj
1
N
N

i=1
¯uj(Si) > (1 + ϵ)pI
j
then with probability at least 1 −γ, it follows that

S∈S
¯uj(S) > pI
j.
Hence, the new solution is dual feasible with high probability. Note that Equation (16)
is an approximation to Equation (10). This implies that by replacing this new equation
in the previous proof we obtain

j∈N

S∈S
¯uj(S) +

S∈S

t∈T
¯vt(S) ≤2(1 + 2ϵ)

j∈N
pI
j ¯xj +
+ 2(1 + 2ϵ)

j∈N

S∈S

I∈Aj(S)
q(S)pII
j (I, S)¯yj(I, S),
(17)
which completes the proof.

156
D.B. Shmoys and M. Sozio
5
An NP-Hardness of Approximation Result
We show that, in contrast to the results of the previous sections, another natural 2-stage
stochastic generalization of the problem 1|rj|  wjUj (even in a very simple case) can
not be approximated. Suppose that in the ﬁrst phase, we select a set of activities that
we are committed to serve. In the second phase, for a given scenario, we must schedule
exactly one instance of each activity selected in the ﬁrst phase, and we may augment
this solution by scheduling other instances of additional activities. We wish to maximize
is the total expected proﬁt (where it is now natural to assume that the proﬁt obtained
for an instance in the second phase is less than the corresponding proﬁt in the ﬁrst). We
will refer to this problem as the augmentation 2-stage stochastic 1|rj|  wjUj.
An integer programming formulation for this problem is obtained by changing (SIP)
in the following way: a 0-1 variable xj indicates (with value 1) that activity Aj is
selected in the ﬁrst phase; constraint (1) is replaced by the following two constraints:

I∈Aj(S)
yj(I, S) ≥xj
∀S ∈S, j ∈N : Aj(S) ̸= ∅
(18)

I∈Aj(S)
yj(I, S) ≤1
∀j ∈N, S ∈S
(19)
Unfortunately, it is straightforward to show that selecting a feasible set of activities
in the ﬁrst phase can be used to model the maximum independent set problem. This is
formalized in the following lemma.
Lemma 2. If there is a ρ-approximation algorithm for the augmentation 2-stage sto-
chastic 1|rj|  wjUj, then there is a ρ-approximation algorithm for maximum inde-
pendent set problem.
Proof Sketch. We give an approximation-preserving reduction from the maximum in-
dependent set problem. Given a graph G, we build the following input for the aug-
mentation 2-stage stochastic 1|rj|  wjUj. For each vertex vj, there is an activity Aj,
j = 1, . . . , n, each activity is always released at time 0, has deadline time 1, and takes
one time unit to complete; each activity has ﬁrst-stage proﬁt 1, and second-stage proﬁt
0. For each edge ei = (vj, vk), there is a scenario Si in which only the activities Aj
and Ak are active. Each scenario Si occurs with positive probability, and hence our
ﬁrst stage selection must contain at most one of the endpoints of ei. Thus, there is a
one-to-one correspondence between independent sets in G and feasible ﬁrst-stage deci-
sions. Furthermore, the objective function value of any ﬁrst-stage decision is exactly the
number of activities selected (since the second stage does not contribute any expected
proﬁt). Hence, we see that the two optimization problems are identical.
From Lemma (2) and the result in [6] we obtain the following theorem.
Theorem 3. For any ϵ > 0, there does not exist a polynomial-time algorithm that ap-
proximates the augmentation 2-stage stochastic 1|rj|  wjUj within a factor n1/2−ϵ,
unless P = NP.

Approximation Algorithms for 2-Stage Stochastic Scheduling Problems
157
References
1. A. Bar-Noy, R. Bar-Yehuda, A. Freund, J. Naor, and B. Schieber. A uniﬁed approach to ap-
proximating resource allocation and scheduling. Journal of the ACM, 48:1069–1090, 2001.
2. M. Charikar, C. Chekuri, and M. P´al.
Sampling bounds for stochastic optimization.
In
Proceedings of APPROX-RANDOM 2005, pages 257–269, 2005.
3. S. Dye, L. Stougie, and A. Tomasgard. The stochastic single resource service-provision
problem. Naval Research Logistics, 50:869–887, 2003.
4. R. L. Graham, E. L. Lawler, J. K. Lenstra, and A. H. G. Rinnooy Kan. Optimization and
approximation in deterministic sequencing and scheduling: A survey. Ann. Discrete Math.,
5:287–326, 1979.
5. A. Gupta, M. P´al, R. Ravi, and A. Sinha. Boosted sampling: approximation algorithms for
stochastic optimization. In Proceedings of the 36th Annual ACM Symposium on Theory of
Computing, pages 265–274, 2004.
6. J. H˚astad. Clique is hard to approximate within n1−ϵ. Acta Mathematica, 182:105–142,
1999.
7. N. Immorlica, D. Karger, M. Minkoff, and V. S. Mirrokni. On the costs and beneﬁts of pro-
crastination: approximation algorithms for stochastic combinatorial optimization problems.
In Proceedings of the 16th ACM-SIAM Symposium on Discrete Algorithms, pages 691–700,
2004.
8. N. Kong and A. J. Schaefer. A factor 1/2 approximation algorithm for two-stage stochastic
matching problems. European Journal of Operational Research, 172:740–746, 2006.
9. R. Ravi and A. Sinha. Hedging uncertainty: Approximation algorithms for stochastic opti-
mization problems. In D. Bienstock and G. Nemhauser, editors, Integer Programming and
Combinatorial Optimization: 10th International IPCO Conference, number 3064 in Lecture
Notes in Computer Science, pages 101–115. Springer-Verlag, 2004.
10. D. B. Shmoys and C. Swamy. Stochastic optimization is (almost) as easy as deterministic
optimization. In Proceedings of the 45th Annual Symposium on Foundations of Computer
Science, pages 228–237, 2004.
11. C. Swamy and D. B. Shmoys. The sampling-based approximation algorithms for multi-stage
stochastic optimization. In Proceedings of the 46th Annual Symposium on Foundations of
Computer Science, pages 357–366, 2005.

On Integer Programming and the Branch-Width
of the Constraint Matrix
William H. Cunningham and Jim Geelen
Department of Combinatorics and Optimization
University of Waterloo
Waterloo, Canada N2L 3G1
{whcunnin,jfgeelen}@uwaterloo.ca
http://www.math.uwaterloo.ca/C andO Dept/index.shtml
Abstract. Consider an integer program max(ctx : Ax = b, x ≥0, x ∈
Zn) where A ∈Zm×n, b ∈Zm, and c ∈Zn. We show that the integer
program can be solved in pseudo-polynomial time when A is non-negative
and the column-matroid of A has constant branch-width.
1
Introduction
For positive integers m and n, let A ∈Zm×n, b ∈Zm, and c ∈Zn. Consider the
following integer programming problems:
(IPF) Find x ∈Zn satisfying (Ax = b, x ≥0).
(IP) Find x ∈Zn maximizing ctx subject to (Ax = b, x ≥0).
Let M(A) denote the column-matroid of A. We are interested in properties of
M(A) which lead to polynomial-time solvability for (IPF) and (IP). Note that,
even when A (or, equivalently, M(A)) has rank one, the problems (IPF) and (IP)
are NP-hard. Papadimitriou [9] considered these problems for instances where
A has constant rank.
Theorem 1 (Papadimitriou). There is a pseudopolynomial-time algorithm
for solving (IP) on instances where the rank of A is constant.
Robertson and Seymour [10] introduced the parameter “branch-width” for
graphs and also, implicitly, for matroids. We postpone the deﬁnition until Sec-
tion 2. Our main theorem is the following; a more precise result is given in
Theorem 6.
Theorem 2. There is a pseudopolynomial-time algorithm for solving (IP) on
instances where A is non-negative and the branch-width of M(A) is constant.
The branch-width of a matroid M is at most r(M)+1. Theorem 2 does not imply
Papadimitriou’s theorem, since we require that A is non-negative. In Section 6
we show that the non-negativity can be dropped when we have bounds on the
variables. However, the following result shows that we cannot just relax the
non-negativity.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 158–166, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

On Integer Programming and the Branch-Width of the Constraint Matrix
159
Theorem 3. (IPF) is NP-hard even for instances where M(A) has branch-
width ≤3 and the entries of A are in {0, ±1}.
We also prove the following negative result.
Theorem 4. (IPF) is NP-hard even for instances where the entries of A and b
are in {0, ±1} and M(A) is the cycle matroid of a graph.
We ﬁnd Theorem 4 somewhat surprising considering the fact that graphic ma-
troids are regular. Note that, if A is a (0, ±1)-matrix and M([I, A]) is regular,
then A is a totally unimodular matix and, hence, we can solve (IP) eﬃciently.
It seems artiﬁcial to append the identity to the constraint matrix here, but for
inequality systems it is more natural.
Recall that M(A) is regular if and only if it has no U2,4-minor (see Tutte [13] or
Oxley [8], Section 6.6). Moreover, Seymour [12] found a structural characteriza-
tion of the class of regular matroids. We suspect that the class of R-representable
matroids with no U2,l- or U ∗
2,l-minor is also “highly structured” for all l ≥0 (by
which we mean that there is likely to be a reasonable analogue to the graph mi-
nors structure theorem; see [11]). Should such results ever be proved, one could
imagine using the structure to solve the following problem.
Problem 1. Given a non-negative integer l ≥0, is there a polynomial-time algo-
rithm for solving max(ctx : Ax ≤b, x ≥0, x ∈Zn) on instances where A is a
(0, ±1)-matrix and M([I, A]) has no U2,l- or U ∗
2,l-minor?
2
Branch-Width
For a matroid M and X ⊆E(M), we let λM(X) = rM(X) + rM(E(M) −X) −
r(M)+1; we call λM the connectivity function of M. Note that the connectivity
function is symmetric (that is, λM(X) = λM(E(M) −X) for all X ⊆E(M))
and submodular (that is, λM(X) + λM(Y ) ≥λM(X ∩Y ) + λM(X ∪Y ) for all
X, Y ⊆E(M)).
Let A ∈Rm×n and let E = {1, . . . , n}. For X ⊆E, we let
S(A, X) := span(A|X) ∩span(A|(E −X)),
where span(A) denotes the subspace of Rm spanned by the columns of A and
A|X denotes the restriction of A to the columns indexed by X. By the modularity
of subspaces,
dim S(A, X) = λM(A)(X) −1.
A tree is cubic if its internal vertices all have degree 3. A branch-decomposition
of M is a cubic tree T whose leaves are labelled by elements of E(M) such that
each element in E(M) labels some leaf of T and each leaf of T receives at most
one label from E(M). The width of an edge e of T is deﬁned to be λM(X) where
X ⊆E(M) is the set of labels of one of the components of T −{e}. (Since λM
is symmetric, it does not matter which component we choose.) The width of T

160
W.H. Cunningham and J. Geelen
is the maximum among the widths of its edges. The branch-width of M is the
minimum among the widths of all branch-decompositions of M.
Branch-width can be deﬁned more generally for any real-valued symmetric set-
function. For graphs, the branch-width is deﬁned using the function λG(X); here,
for each X ⊆E(G), λG(X) denotes the number of vertices incident with both
an edge in X and an edge in E(G)−X. The branch-width of a graph is within a
constant factor of its tree-width. Tree-width is widely studied in theoretical com-
puter science, since many NP-hard problems on graphs can be eﬃciently solved
on graphs of constant tree-width (or, equivalently, branch-width). The most
striking results in this direction were obtained by Courcelle [1]. These results
have been extended to matroids representable over a ﬁnite ﬁeld by Hlinˇen´y [4].
They do not extend to all matroids or even to matroids represented over the
reals.
Finding Near-Optimal Branch-Decompositions
For any integer constant k, Oum and Seymour [7] can test, in polynomial time,
whether or not a matroid M has branch-width k (assuming that the matroid
is given by its rank-oracle). Moreover their algorithm ﬁnds an optimal branch-
decomposition in the case that the branch-width is at most k. The algorithm is
not practical; the complexity is O(n8k+13). Fortunately, there is a more practical
algorithm for ﬁnding a near-optimal branch-decomposition. For an integer con-
stant k, Oum and Seymour [6] provide an O(n3.5) algorithm that, for a matroid
M with branch-width at most k, ﬁnds a branch-decomposition of width at most
3k −1. The branch decomposition is obtained by solving O(n) matroid inter-
section problems. When M is represented by a matrix A ∈Zm×n, each of these
matroid intersection problems can be solved in O(m2n log m) time; see [2]. Hence
we can ﬁnd a near-optimal branch-decomposition for M(A) in O(m2n2 log m)
time.
3
Linear Algebra and Branch-Width
In this section we discuss how to use branch decompositions to perform certain
matrix operations more eﬃciently. This is of relatively minor signiﬁcance, but it
does improve the eﬃciency of our algorithms.
Let A ∈Zm×n and let E = {1, . . . , n}. Recall that, for X ⊆E, S(A, X) =
span(A|X)∩span(A|(E −X)) and that dim S(A, X) = λM(A)(X)−1. Now let T
be a branch-decomposition of M(A) of width k, let e be an edge of T , and let X be
the label-set of one of the two components of T −e. We let Se(A) := S(A, X). The
aim of this section is to ﬁnd bases for each of the subspaces (Se(A) : e ∈E(T ))
in O(km2n) time.
Converting to Standard Form
Let B ⊆E be a basis of M(A). Now let AB = A|B and A′ = (AB)−1A. Therefore
M(A) = M(A′) and Se(A) = {ABv : v ∈Se(A′)}. Note that we can ﬁnd B

On Integer Programming and the Branch-Width of the Constraint Matrix
161
and A′ in O(m2n) time. Given a basis for Se(A′), we can determine a basis for
Se(A) in O(km2) time. Since T has O(n) edges, if we are given bases for each
of (Se(A′) : e ∈E(T )) we can ﬁnd bases for each of (Se(A) : e ∈E(T )) in
O(km2n) time.
Matrices in Standard Form
Henceforth we suppose that A is already in standard form; that is A|B = I for
some basis B of M(A). We will now show the stronger result that we can ﬁnd a
basis for each of the subspaces (Se(A) : e ∈E(T )) in O(k2mn) time (note that
k ≤m + 1).
We label the columns of A by the elements of B so that the identity A|B
is labelled symmetrically. For X ⊆B and Y ⊆E, we let A[X, Y ] denote the
submatrix of A with rows indexed by X and columns indexed by Y .
Claim. For any partition (X, Y ) of E,
λM(A)(X) = rank A[X ∩B, X −B] + rank A[Y ∩B, Y −B] + 1.
Moreover S(A, X) is the column-span of the matrix

X −B
Y −B
X ∩B
A[X ∩B, X −B]
0
Y ∩B
0
A[Y ∩B, Y −B]

.
Proof. The formula for λM(A)(X) is straightforward and well known. It follows
that S(A, X) has the same dimension as the column-space of the given matrix.
Finally, it is straightforward to check that each column of the given matrix is
spanned by both A|X and A|(E −X).
Let (X, Y ) be a partition of E. Note that B ∩X can be extended to a maximal
independent subset BX of X and B ∩Y can be extended to a maximal indepen-
dent subset BY of Y . Now S(A, X) = S(A|(BX ∪By), BX). Then, by the claim
above, given BX and BY we can trivially ﬁnd a basis for S(A, X).
Finding Bases
A set X ⊆E is called T -branched if there exists an edge e of T such that
X is the label-set for one of the components of T −e. For each T -branched
set X we want to ﬁnd a maximal independent subset B(X) of X containing
X ∩B. The number of T -branched sets is O(n), and we will consider them in
order of non-decreasing size. If |X| = 1, then we can ﬁnd B(X) in O(m) time.
Suppose then that |X| ≥2. Then there is a partition (X1, X2) of X into two
smaller T -branched sets. We have already found B(X1) and B(X2). Note that
X is spanned by B(X1) ∪B(X2). Moreover, for any T -branched set Y , we have
rM(A)(Y ) −|Y ∩B| ≤rM(A)(Y ) + rM(A)(E −Y ) −r(M(A)) = λM(A)(Y ) −1.
Therefore |(B(X1)∪B(X2))−(B ∩X)| ≤2(k−1). Recall that A|B = I. Then in
O(k2m) time (O(k) pivots on an m × k-matrix) we can extend B ∩X to a basis
B(X) ⊆B(X1) ∪B(X2). Thus we can ﬁnd all of the required bases in O(k2mn)
time.

162
W.H. Cunningham and J. Geelen
4
The Main Result
In this section we prove Theorem 2. We begin by considering the feasibility
version.
IPF(k).
Instance: Positive integers m and n, a non-negative matrix A ∈Zm×n, a non-
negative vector b ∈Zm, and a branch-decomposition T of M(A) of width k.
Problem: Does there exist x ∈Zn satisfying (Ax = b, x ≥0)?
Theorem 5. IPF(k) can be solved in O((d + 1)2kmn + m2n) time, where d =
max(b1, . . . , bm).
Note that for many combinatorial problems (like the set partition problem), we
have d = 1. For such problems the algorithm requires only O(m2n) time (consid-
ering k as a constant). Recall that S(A, X) denotes the subspace span(A|X) ∩
span(A|(E −X)), where E is the set of column-indices of A.
The following lemma is the key.
Lemma 1. Let A ∈{0, . . ., d}m×n and let X ⊆{1, . . . , n} such that λM(A)(X)=
k. Then there are at most (d + 1)k−1 vectors in S(A, X) ∩{0, . . ., d}m.
Proof. Since λM(A)(X) ≤k, S(A, X) has dimension k−1; let a1, . . . , ak−1 ∈Rm
span S(A, X). There is a (k−1)-element set Z ⊆{1, . . . , n} such that the matrix
(a1|Z, . . . , ak−1|Z) is non-singular. Now any vector x ∈R that is spanned by
(a1, . . . , ak−1) is uniquely determined by x|Z. So there are at most (d + 1)k−1
vectors in {0, . . ., d}m that are spanned by (a1, . . . , ak−1).
Proof (Proof of Theorem 5.). Let A′ = [A, b], E = {1, . . . , n}, and E′ = {1, . . . ,
n + 1}. Now, let T be a branch-decomposition of M(A) of width k and let T ′ be
a branch-decomposition of M(A′) obtained from T by subdividing an edge and
adding a new leaf-vertex, labelled by n + 1, adjacent to the degree 2 node. Note
that T ′ has width ≤k + 1. Recall that a set X ⊆E is T -branched if there is an
edge e of T such that X is the label-set of one of the components of T −e. By
the results in the previous section, in O(m2n) time we can ﬁnd bases for each
subspace S(A′, X) where X ⊆E is T ′-branched.
For X ⊆E, we let B(X) denote the set of all vectors b′ ∈Zm such that
(1) 0 ≤b′ ≤b,
(2) there exists z ∈ZX with z ≥0 such that (A|X)z = b′, and
(3) b′ ∈span(A′|(E′ −X)).
Note that, if b′ ∈B(X), then, by (2) and (3), b′ ∈S(A′, X). If λM(A′)(X) ≤
k + 1, then, by Lemma 1, |B(X)| ≤(d + 1)k. Moreover, we have a solution to
the problem (IPF) if and only b ∈B(E).
We will compute B(X) for all T ′-branched sets X ⊆E using dynamic pro-
gramming. The number of T ′-branched subsets of E is O(n), and we will consider
them in order of non-decreasing size. If |X| = 1, then we can easily ﬁnd B(X) in
O(dm) time. Suppose then that |X| ≥2. Then there is a partition (X1, X2) of
X into two smaller T ′-branched sets. We have already found B(X1) and B(X2).
Note that b′ ∈B(X) if and only if

On Integer Programming and the Branch-Width of the Constraint Matrix
163
(a) there exist b′
1 ∈B(X1) and b′
2 ∈B(X2) such that b′ = b′
1 + b′
2,
(b) b′ ≤b, and
(c) b′ ∈S(A′, X).
The number of choices for b′ generated by (a) is O((d + 1)2k). For each such
b′ we need to check that b′ ≤b and b′ ∈S(A′, X). Since we have a basis for
S(A′, X) and since S(A′, X) has dimension ≤k, we can check whether or not
b′ ∈S(A′, X) in O(m) time (considering k as a constant). Therefore we can ﬁnd
B(E) in O((d + 1)2kmn + m2n) time.
We now return to the optimization version.
IP(k).
Instance: Positive integers m and n, a non-negative matrix A ∈Zm×n, a non-
negative vector b ∈Zm, a vector c ∈Zn, and a branch-decomposition T of M(A)
of width k.
Problem: Find x ∈Zn maximizing ctx subject to (Ax = b, x ≥0).
Theorem 6. IP(k) can be solved in O((d + 1)2kmn + m2n) time, where d =
max(b1, . . . , bm).
Proof. The proof is essentially the same as the proof of Theorem 5, except that
for each b′ ∈B(X) we keep a vector x ∈ZX maximizing (cixi : i ∈X)
subject to ((A|Xe)x = b′, x ≥0). The details are easy and left to the reader.
Theorem 6 implies Theorem 2.
5
Hardness Results
In this section we prove Theorems 3 and 4. We begin with Theorem 3. The
reduction is from the following problem, which is known to be NP-hard; see
Lueker [5].
Single Constraint Integer Programming Feasibility (SCIPF).
Instance: A non-negative vector a ∈Zn and an integer b.
Problem: Does there exist x ∈Zn satisfying (atx = b, x ≥0)?
Proof (Proof of Theorem 3.). Consider an instance (a, b) of (SCIPF). Choose
an integer k as small as possible subject to 2k+1 > max(a1, . . . , an). For each
i ∈{1, . . . , n}, let (αi,k, αi,k−1, . . . , αi,0) be the binary expansion of ai. Now
consider the following system of equations and inequalities:
(1)
n

i=1
k

j=0
αijyij = b.
(2)
yij −xi −i−1
l=0 yi,l = 0, for i ∈{1, . . ., n} and j ∈{0, . . . , k}.
(3)
xi ≥0 for each i ∈{1, . . . , n}.

164
W.H. Cunningham and J. Geelen
If (yij : ∈{1, . . . , n}, j ∈{0, . . . , k}) and (x1, . . . , xn) satisfy (2), then yij =
2jxi, and (1) simpliﬁes to (aixi : i ∈{1, . . ., n}) = b. Therefore there is an
integer solution to (1), (2), and (3) if and only if there is an integer solution to
(atx = b, x ≥0).
The constraint matrix B for system (2) is block diagonal, where each block is
a copy of the matrix:
C =
⎛
⎜
⎜
⎝
1
2
3
. . .
k + 1
k + 2
1
1
−1
−1
· · ·
−1
−1
2
0
1
−1
−1
−1
...
...
...
k + 1
0
0
0
· · ·
1
−1
⎞
⎟
⎟
⎠.
It is straightforward to verify that M(C) is a circuit and, hence, M(C) has
branch-width 2. Now M(B) is the direct sum of copies of M(C) and, hence,
M(B) has branch-width 2. Appending a single row to B can increase the branch-
width by at most one.
Now we turn to Theorem 4. Our proof is by a reduction from 3D Matching
which is known to be NP-complete; see Garey and Johnson [3], pp. 46.
3D Matching.
Instance: Three disjoint sets X, Y , and Z with |X| = |Y | = |Z| and a collection
F of triples {x, y, z} where x ∈X, y ∈Y , and z ∈Z.
Problem: Does there exist a partition of X ∪Y ∪Z into triples, each of which
is contained in F?
Proof (Proof of Theorem 4.). Consider an instance (X, Y, Z, F) of 3D Matching.
For each triple t ∈F we deﬁne elements ut and vt. Now construct a graph
G = (V, E) with
V = X ∪Y ∪Z ∪{ut : t ∈F} ∪{vt : t ∈F}, and
E =

t={x,y,z}∈F
{(ut, x), (ut, y), (ut, vt), (vt, z)}.
Note that G is bipartite with bipartition (X∪Y ∪{vt : t ∈F}, Z∪{ut : t ∈F}).
Now we deﬁne b ∈ZV such that but = 2 for each t ∈F and bw = 1 for
all other vertices w of G. Finally, we deﬁne a matrix A = (ave) ∈ZV ×E such
that ave = 0 whenever v is not incident with e, ave = 2 whenever v = ut and
e = (ut, vt) for some t ∈F, and ave = 1 otherwise; see Figure 1.
It is straightforward to verify that (X, Y, Z, F) is a yes-instance of the 3D
Matching problem if and only if there exists x ∈ZE satisfying (Ax = b, x ≥0).
Now A and b are not (0, ±1)-valued, but if, for each t ∈F, we subtract the
vt-row from the ut-row, then the entries in the resulting system A′x = b′ are in
{0, ±1}.
It remains to verify that M(A) is graphic. It is straightforward to verify that
A is equivalent, up to row and column scaling, to a {0, 1}-matrix A′′. Since G

On Integer Programming and the Branch-Width of the Constraint Matrix
165
                                                                        







                                                                







                                                                







y
x
vt
Z
Y
t
X
u
z
1
1
1
1
1
1
2
1
1
1
1
1
2
Fig. 1. The reduction
is bipartite, we can scale some of the rows of A′′ by −1 to obtain a matrix B
with a 1 and a −1 in each column. Now M(B) = M(A) is the cycle-matroid of
G and, hence, M(A) is graphic.
6
Bounded Variables
In this section we consider integer programs with bounds on the variables.
Integer Programming with Variable Bounds (BIP)
Instance: Positive integers m and n, a matrix A ∈Zm×n, a vector b ∈Zm, and
vectors c, d ∈Zn.
Problem: Find x ∈Zn maximizing ctx subject to (Ax = b, 0 ≤x ≤d).
We can rewrite the problem as: Find y ∈Z2n maximizing ˆcty subject to
( ˆAy = ˆb, y ≥0), where
ˆA =

A 0
I I

, ˆb =

b
d

, and ˆc =

c
0

.
Note that, for i ∈{1, . . . , n}, the elements i and i+n are in series in M( ˆA), and,
hence, M( ˆA) is obtained from M(A) by a sequence of series-coextensions. Then
it is easy to see that, if the branch-width of M(A) is k, then the branch-width
of M( ˆA) is at most max(k, 2).
Now note that the all-ones vector is in the row-space of ˆA. Therefore, by taking
appropriate combinations of the equations ˆAy = ˆb, we can make an equivalent
system ˜Ay = ˜b where ˜A is non-negative. Therefore, we obtain the following
corollary to Theorem 2.

166
W.H. Cunningham and J. Geelen
Corollary 1. There is a pseudopolynomial-time algorithm for solving (BIP) on
instances where the branch-width of M(A) is constant.
Acknowledgements
We thank Bert Gerards and GeoﬀWhittle for helpful discussions regarding the
formulation of Problem 1 and the proof of Theorem 4. This research was partially
sponsored by grants from the Natural Science and Engineering Research Council
of Canada.
References
1. B. Courcelle, “Graph rewriting: An algebraic and logical approach”, in: Handbook
of Theoretical Computer Science, vol. B, J. van Leeuwnen, ed., North Holland
(1990), Chapter 5.
2. W.H. Cunningham, Improved bounds for matroid partition and intersection algo-
rithms, SIAM J. Comput. 15 (1986), 948-957.
3. M.R. Garey and D.S. Johnson, Computers and Intractability. A guide to the theory
of NP-completeness, A series of books in the mathematical sciences, W.H. Freeman
and Co., San Francisco, California, 1979.
4. P. Hlinˇen´y, Branch-width, parse trees and monadic second-order logic for matroids,
manuscript, 2002.
5. G.S. Lueker, Two NP-complete problems in non-negative integer programming, Re-
port No. 178, Department of Computer Science, Princeton University, Princeton,
N.J., (1975).
6. S. Oum and P. D. Seymour, Approximating clique-width and branch-width, J. Com-
bin. Theory, Ser. B 96 (2006), 514-528.
7. S. Oum and P. D. Seymour, Testing branch-width, to appear in J. Combin. Theory,
Ser. B.
8. J. G. Oxley, Matroid Theory, Oxford University Press, New York, 1992.
9. C.H. Papadimitriou, On the complexity of integer programming, J. Assoc. Comput.
Mach. 28 (1981), 765-768.
10. N. Robertson and P. D. Seymour, Graph Minors. X. Obstructions to tree-
decomposition, J. Combin. Theory, Ser. B 52 (1991), 153–190.
11. N. Robertson and P. D. Seymour, Graph Minors. XVI. Excluding a non-planar
graph, J. Combin. Theory, Ser. B 89 (2003), 43-76.
12. P. D. Seymour, Decomposition of regular matroids, J. Combin. Theory, Ser. B 28
(1980), 305–359.
13. W. T. Tutte, A homotopy theorem for matroids, I, II, Trans. Amer. Math. Soc. 88
(1958), 144–174.

Matching Problems in Polymatroids
Without Double Circuits⋆
Márton Makai, Gyula Pap, and Jácint Szabó
MTA-ELTE Egerváry Research Group on Combinatorial Optimization
{marci,gyuszko,jacint}@cs.elte.hu
http://www.cs.elte.hu/egres
Abstract. According to the present state of the theory of the matroid
matching problem, the existence of a good characterization to the size of
a maximum matching depends on the behavior of certain substructures,
called double circuits. In this paper we prove that if a polymatroid has
no double circuits at all, then a partition-type min-max formula charac-
terizes the size of a maximum matching. We provide applications of this
result to parity constrained orientations and to a rigidity problem.
A polynomial time algorithm is constructed by generalizing the prin-
ciple of shrinking blossoms used in Edmonds’ matching algorithm [2].
Keywords: matroids and submodular functions.
1
Introduction
Polymatroid matching is a combinatorial optimization problem which is con-
cerned with parity and submodularity. Early well-solved special cases are the
matching problem of graphs and the matroid intersection problem, which have
in fact motivated Lawler to introduce the matroid and polymatroid matching
problems. Jensen, Korte [6], and Lovász [9] have shown that, in general, the
matroid matching problem is of exponential complexity under the independence
oracle framework. The major breakthrough came when Lovász gave a good char-
acterization to the size of a maximum matching and also a polynomial algorithm
for linearly represented matroids [12,9]. Lovász [10], and Dress and Lovász [1]
observed that the solvability of the linear case is due to the fact that these ma-
troids can be embedded into a matroid satisfying the so-called double circuit
property, or DCP for short. It was also shown that full linear, full algebraic, full
graphic, and full transversal matroids are DCP matroids [1]. The disadvantage of
this approach is that, due to the embedding into a bigger matroid, the min-max
formula is rather diﬃcult to interpret in a combinatorial way, and often does not
even imply a good characterization. However, the diversity and the importance
of solvable special cases of the matroid matching problem is a motivation to
explore those techniques implying a combinatorial characterization.
⋆Research is supported by OTKA grants K60802, T037547 and TS049788, by
European MCRTN Adonet, Contract Grant No. 504438.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 167–181, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

168
M. Makai, G. Pap, and J. Szabó
In this paper we investigate the class of those polymatroids having no non-
trivial compatible double circuits, called ntcdc-free for short, deﬁned later. We
prove that in these polymatroids a partition-type combinatorial formula charac-
terizes the maximum size of a matching. We remark that in the min-max formula
for DCP matroids, for example representable matroids, we have to take a parti-
tion and a projection into consideration. Contrarily, in ntcdc-free polymatroids,
it suﬃces to consider partitions in the min-max formula. As an application, we
show that two earlier results are special cases of this approach. The ﬁrst appli-
cation is that the parity constrained orientation problem of Király and Szabó
[7] can be formulated as a matching problem in a ntcdc-free polymatroid, which
implies the partition-type formula given in [7]. Second, we deduce a result of
Fekete [3] on the problem of adding a clique of minimum size to a graph to
obtain a graph that is generically rigid in the plane.
1.1
The Partition Formula
To formulate our main result, some deﬁnitions are in order. We denote by R+
and N the set of non-negative reals and non-negative integers, respectively. Let
S be a ﬁnite ground set. A set-function f : 2S →Z is called submodular if
f(X) + f(Y ) ≥f(X ∩Y ) + f(X ∪Y )
(1)
holds whenever X, Y ⊆S. b is called supermodular if −b is submodular. The set-
function f is said to be non-decreasing if f(X) ≤f(Y ) for every ∅̸= X ⊆Y ⊆S,
and we say that f is non-increasing if −f is non-decreasing. A non-decreasing
submodular set-function f : 2S →N with f(∅) = 0 is called a polymatroid
function. A polymatroid function f : 2S →Z+ induces a polymatroid P(f) and
a base polyhedron B(f) deﬁned by
P(f) := {x ∈RS : x ≥0, x(Z) ≤f(Z) for all Z ⊆S},
(2)
B(f) := {x ∈RS : x(S) = f(S), and x ≥0, x(Z) ≤f(Z) for all Z ⊆S},
(3)
where x(Z) := 
i∈Z xi for some Z ⊆S. A vector m ∈ZS is called even if mi is
even for every i ∈S. The even vectors m ∈P(f) are called the matchings of f.
The size of a matching is m(S)/2. The polymatroid matching problem is to ﬁnd
a maximum matching, i.e. a matching of maximum size
ν(f) = max{m(S)/2 : m is a matching of f}.
We will investigate the polymatroid matching problem in ntcdc-free polyma-
troids, deﬁned below. Our main result goes as follows.
Theorem 1. Let f : 2S →N be a ntcdc-free polymatroid function. Then
ν(f) = min
t

j=1
f(Uj)
2

,
where the minimum is taken over all partitions U1, U2, . . . , Ut of S.

Matching Problems in Polymatroids Without Double Circuits
169
We propose two diﬀerent proofs. In the ﬁrst proof we exploit a theorem of Lovász,
and a couple of polymatroid operations. The second proof relies on a (semi-
strongly) polynomial time algorithm, which is based on a generalization of the
contraction of blossoms in Edmonds’ matching algorithm [2].
1.2
Circuits and Compatible Double Circuits in Polymatroids
Consider a polymatroid function f : 2S →N, and a vector x ∈NS. For a set
Z ⊆S, we call deff,x(Z) := x(Z) −f(Z) the deﬁciency of set Z with respect
to f, x. A set is called k-deﬁcient with respect to f, x if deff,x(Z) = k. The
deﬁciency of a vector x is deﬁned by deff(x) := maxZ⊆S deff,x(Z), which is
non-negative. Notice that deff,x(·) is a supermodular set-function, hence the
family of sets Z such that deff,x(Z) = deff(x) is closed under taking unions and
intersections.
Consider a 1-deﬁcient vector x. x is called a circuit if supp(x) is equal to the
unique inclusionwise minimal 1-deﬁcient set.
Consider a 2-deﬁcient vector x ∈NS, and let W := supp(x). x is called a com-
patible double circuit (or cdc, for short), if W is the unique inclusionwise minimal
2-deﬁcient set, and there is a partition π = {W1, · · · , Wk} of W such that k ≥2
and {W −Wi : i = 1, · · · , k} is equal to the family of all inclusionwise minimal
1-deﬁcient sets. We remark that if x is a cdc, then π is uniquely determined –
let it be called the principal partition of x. If k = 2, then x is called a trivial
cdc. If k ≥3, then x is called a non-trivial compatible double circuit, or ntcdc,
for short.
A polymatroid is called ntcdc-free if there is no ntcdc.
2
First Proof of the Partition Formula
For some well-known notions and results on the theory of matroids, polymatroids
and matroid matching, see [14]. We need some more preparation.
2.1
Preliminaries
There is a close relation between polymatroid functions and matroids. First, if
M = (T, r) is a matroid and ϕ : T →S is a function then f : 2S →N, X →
r(ϕ−1(X)) is a polymatroid function, the homomorphic image of M under ϕ.
Second, for any polymatroid function f it is possible to deﬁne a matroid M, the
homomorphic image of which is f, in such a way that M is “most independent” in
some sense. The ground set T of M is the disjoint union of sets Ti for i ∈S of size
|Ti| ≥f({i}). If X ⊆T then we deﬁne the vector χX ∈NS with χX
i = |X ∩Ti|
for i ∈S. With this notation, a set X ⊆T is deﬁned to be independent in M if
χX ∈P(f). It is routine to prove that M is indeed a matroid with rank function
r(X) = minY ⊆X(|Y |+f(ϕ(X −Y ))), where ϕ : T →S maps t to i if t ∈Ti. This
M is called a prematroid of f. Note that a prematroid M is uniquely determined
by f and by the sizes |Ti|, i ∈S. If M is a matroid with rank function r then the

170
M. Makai, G. Pap, and J. Szabó
prematroids of r are the parallel extensions of M. If we consider a prematroid
M then we tacitly assume that M = (T, r) and that the function ϕ : T →S is
given with t →i if t ∈Ti.
If f is a polymatroid function and x ∈ZS then we deﬁne the rank of x as
rf(x) = minU⊆S(x(S −U) + f(U)). If x ∈NS then rf(x) = x(S) if and only if
x ∈P(f). Besides, if M = (T, r) is a prematroid of f and X ⊆T then rf(χX) =
r(X). The span of x ∈NS is deﬁned by spf(x) = {i ∈S : rf(x + χi) = rf(x)}.
If M is a prematroid of f and X ⊆T then spf(χX) = {i ∈S : Ti ⊆spM(X)}.
2.2
Circuits and Double Circuits in Matroids
Let M = (T, r) be a matroid. A set C ⊆T is said to be a circuit if r(C −
x) = r(C) = |C| −1 for every x ∈C. A set D ⊆T is a double circuit if
r(D −x) = r(D) = |D| −2 for every x ∈D. If D is a double circuit then the
dual of M|D is a matroid of rank 2 without loops, that is a line, showing that
there exists a principal partition D = D1 ˙∪D2 ˙∪. . . ˙∪Dd, d ≥2, such that the
circuits of D are exactly the sets of the form D −Di, 1 ≤i ≤d. We say that D
is non-trivial if d ≥3, and trivial otherwise. A trivial double circuit is simply
the direct sum of two circuits.
Analogously, we deﬁne circuits and double circuits of the polymatroid function
f : 2S →N. For a vector x ∈RS
+ let supp(x) = {i ∈S : xi > 0}. A vector c ∈NS
is a circuit of f if rf(c −χi) = rf(c) = c(S) −1 for every i ∈supp(c). A vector
w ∈NS is a double circuit of f if rf(w −χi) = rf(w) = w(S) −2 for every
i ∈supp(w). It is also easy to see the exact relation between matroidal and
polymatroidal double circuits, which is given as follows.
Lemma 1. Let M be a prematroid of f, D ⊆T and χD = w. Then D is a
double circuit of M if and only if w is a double circuit of f.
Recall that we have already deﬁned cdc’s and ntcdc’s. Next we add another
deﬁnition, which is easily seen to be equivalent with those above. For x ∈RS and
U ⊆S we introduce the notation x|U for the vector by (x|U)i := xi for i ∈U and
(x|U)i := 0 for i ∈S −U. Let M be a prematroid of f and w be a double circuit
of f such that there is a set D ⊆T with χD = w. By Lemma 1, D is a double
circuit of M, thus it has a principal partition D = D1 ˙∪D2 ˙∪. . . ˙∪Dd′. We deﬁne
the principal partition of w as follows. Due to the structure of prematroids it is
easy to check that supp(w) has a partition W0 ˙∪W1 ˙∪. . . ˙∪Wd with the property
that each set Dj is either a singleton belonging to some Ti with wi ≥2 and
i ∈W0, or is equal to D ∩
i∈Wh Ti for some 1 ≤h ≤d. Note that a partition
W0 ˙∪W1 ˙∪. . . ˙∪Wd of supp(w) is the principal partition of w if and only if w −χi
is a circuit of f and wi ≥2 whenever i ∈W0, moreover, w|W−Wi is a circuit
of f for each 1 ≤i ≤d. A double circuit w is said to be compatible if W0 = ∅,
and it is trivial if D is trivial. We remark that these deﬁnitions are easily see
equivalent with the above ones.
We shortly mention what is the double circuit property, or DCP, for short.
If M = (T, r) is a prematroid of the polymatroid function f and Z ⊆T then
ϕ(M/Z) is called a contraction of f. A polymatroid function f is said to have the

Matching Problems in Polymatroids Without Double Circuits
171
DCP if whenever w is a non-trivial compatible double circuit in a contraction f ′
of f with principal partition W1 ˙∪. . . ˙∪Wd then f ′(
1≤i≤d sp(w|W−Wi)) > 0, [1].
A polymatroid function without non-trivial compatible double circuits has not
necessarily the DCP, as its contractions may have many non-trivial compatible
double circuits.
Note that every polymatroid function has double circuits, say (f({i}) + 2)χi
for some i ∈S. However, these are not compatible, as W0 = {i}.
Lemma 2. If w ∈NS is a double circuit of the polymatroid function f : 2S →N
with principal partition W = W0 ˙∪W1 ˙∪. . . ˙∪Wd then f(W) = w(W) −2 and
f(W −Wi) = w(W −Wi) −1 for 1 ≤i ≤d.
Proof. We prove that if x ∈NS is a vector with the property that rf(x) =
rf(x −χi) for all i ∈supp(x) then f(supp(x)) = rf(x). By deﬁnition, rf(x) =
x(S−Y )+f(Y ) for some Y ⊆S. Note that rf(x−χi) ≤(x−χi)(S−Y )+f(Y ) =
rf(x) −1 for all i ∈supp(x) −Y . Thus supp(x) ⊆Y . Finally, f(Y ) = rf(x) ≤
f(supp(x)) ≤f(Y ), since f is non-decreasing. If x is a circuit or a double circuit
then rf(x) = rf(x −χi) for all i ∈supp(x), we are done.
2.3
Polymatroid Operations
Next we investigate how two polymatroid operations (translation, deletion) eﬀect
double circuits. If f : 2S →N is a function and n ∈ZS then deﬁne f +n : 2S →N
by X →f(X) + n(X). If f is a polymatroid function and n ∈NS then f + n is
clearly a polymatroid function, too.
Lemma 3. If n ∈ZS and f and f + n are polymatroid functions then a vector
w is a double circuit of f with W = supp(w) if and only if w + n|W is a double
circuit of f + n. In this case their principal partition coincide.
Proof. Clearly, rf+n(x + n) −(x + n)(S) = rf(x) −x(S) for all x ∈ZS. Thus by
symmetry, it is enough to prove that if w is a double circuit of f with support
W then wi + ni > 0 for every i ∈W. Otherwise by Lemma 2 we would have
w(W −i) −ni ≥w(W) = f(W) + 2 ≥f(W −i) −ni + 2, which is impossible.
Let u ∈NS be a bound vector and deﬁne f\u = ϕ(rM|Z) where M is a prema-
troid of f and Z ⊆T with χZ = u. The matroid union theorem asserts that
(f\u)(X) = minY ⊆X(u(Y ) + f(X −Y )). If M is a matroid with rank function
r then r\u is the rank function of M|supp(u).
Lemma 4. Let u ∈NS. If w ∈NS is a double circuit of f ′ := f\u then w
is either a double circuit of f with the same principal partition, or trivial, or
non-compatible.
Proof. Let M = (T, r) be a prematroid of f and Z ⊆T with χZ = u. If w ≤χZ
then w is a double circuit of f with the same principal partition by Lemma 1.
Observe that wi ≤f ′({i}) + 2 and f ′({i}) ≤ui for every i ∈S. Thus if w ̸≤χZ
then there exists an i ∈S such that wi −f ′({i}) ∈{1, 2}. If wi = f ′({i}) + 2

172
M. Makai, G. Pap, and J. Szabó
then rf ′(wiχi) = wi −2, thus W0 = supp(w) = {i}, implying that w is non-
compatible. If wi = f ′({i}) + 1 then wiχi is a circuit of f ′ thus if W0 ̸= ∅then
w is non-compatible and if W0 = ∅then w is trivial.
Finally we cite Lovász’s deep and important theorem on 2-polymatroids, which
can be translated to arbitrary polymatroids as follows. This theorem will be a
key to our ﬁrst proof below.
Theorem 2 (Lovász [10]). If f : 2S →N is a polymatroid function then at
least one of the following cases holds.
1. f(S) = 2ν(f) + 1.
2. There exists a partition S = S1 ˙∪S2, Si ̸= ∅, s.t. ν(f) = ν(f|2S1 ) + ν(f|2S2 ).
3. There exists an i ∈S, f(i) ≥2 such that for each maximum matching m we
have i ∈spf(m).
4. There exists a certain substructure, called ν-double ﬂower in f, which we do
not deﬁne here, but which always contains a non-trivial compatible double
circuit.
Proof (First proof of Theorem 1). It is easy to see that ν(f) ≤t
j=1

f(Uj)
2
	
holds for every partition U1, U2, . . . , Ut of S. For the other direction we argue by
induction on the pair (|S|, |K(f)|), where K(f) = {s ∈S : s ∈spf(m) for each
maximum matching m of f}. If S = ∅then the statement is trivial. If K(f) = ∅
then either 1. or 2. holds in Theorem 2. If 1. holds then the trivial partition will
do, while if 2. holds then we can use our induction hypothesis applied to f|2S1
and f|2S2.
Next, let K(f) ̸= ∅. We prove that if m is a maximum matching of f + 2χs
then m(s) ≥2. Indeed, assume that m(s) = 0. As m is a maximum matching,
there exists a set s ∈U ⊆S with m(U) ≥(f + 2χs)(U) −1. Thus m(U −s) =
m(U) ≥(f +2χs)(U)−1 ≥f(U −s)+1, which is a contradiction. It is also clear
that m + 2χs is a matching of f + 2χs for each matching m of f. Therefore, m
is a maximum matching of f if and only if m + 2χs is a maximum matching of
f + 2χs.
Let s ∈K(f). Clearly, ν(f) ≤ν(f + χs) ≤ν(f + 2χs) = ν(f) + 1 and we
claim that in fact, ν(f + χs) = ν(f) holds. Indeed, if ν(f + χs) = ν(f) + 1 and
m is a maximum matching of f + χs then m is also a maximum matching of
f + 2χs, thus m(s) ≥2. Then m −2χs is a maximum matching of f and, as
s ∈spf(m −2χs), there exists a set s ∈U ⊆S with (m −2χs)(U) = f(U). This
implies m(U) = f(U) + 2, contradicting to that m is a matching of f + χs.
So if m is a maximum matching of f then m is a maximum matching of
f + χs, too, and clearly, spf(m) = spf+χs(m) −s. Thus we have K(f + χs) ⊆
K(f) −s. By Lemma 3, f + χs has no non-trivial compatible double circuits,
so we can apply induction to f + χs. This gives a partition U1, U2, . . . , Ut of S
such that ν(f + χs) = t
j=1

 1
2(f + χs)(Uj)

. But then, ν(f) = ν(f + χs) =
t
j=1

 1
2(f + χs)(Uj)

≥t
j=1

f(Uj)
2
	
.

Matching Problems in Polymatroids Without Double Circuits
173
3
Second, Constructive Proof of the Partition Formula
The second proof is based on projections of blossoms, which is the generalization
of the principle in Edmonds’ matching algorithm [2]. For this, of course, we need
some more deﬁnitions and direct observations concerning projections.
3.1
Projections
Consider a polymatroid function f on groundset S, as above. For a subset B ⊆S
we deﬁne the projection f B : 2S−B →N by f B(X) := min{ f(X), f(X ∪B) −
f(B) + 1 } for X ⊆S −B. It is easy to see that f B is a polymatroid function,
and its induced polymatroid is equal to
P(f B) = {y ∈RS−B : there is [z, y] ∈P(f) s.t. z(B) = f(B) −1}.
(4)
For x ∈RS, Z ⊆S we introduce the notation x||Z ∈RZ for the vector such
that (x||Z)i = xi for all i ∈Z.
Consider a family H = {H1, · · · , Hm} of disjoint subsets of S. Assume that
there is a vector x ∈P(f) such that for all i = 1, · · · , m, we have x(Hi) =
f(Hi) −1, and there is an element hi ∈Hi such that x + χhi ∈P(f). By (4)
we get that x||S−Hi ∈P(f Hi), thus f Hi(Hj) = f(Hj) for all i ̸= j. This implies
that we obtain the same polymatroid function on groundset S −∪H no matter
which order the sets Hi are projected. Let f H denote the unique polymatroid
function obtained by projecting all the members of H. Then
P(f H) = {y ∈RS−∪H : there is [z, y] ∈P(f) s.t. z(Hi) = f(Hi) −1},
(5)
and we get that for any X ⊆S − H,
f H(X) = min {f(X ∪H′) −x(H′) : H′ ⊆H} .
(6)
We remark without proof that f H may be evaluated in strongly polynomial time.
3.2
Blossoms
The notion of blossoms comes from an algorithmic point of view, which is the ana-
logue of Edmonds’ blossoms in the matching algorithm. An ear-decomposition of
a matching is constructed by ﬁnding a circuit induced in the matching, and it-
erating this procedure after the projection. More precisely, the deﬁnition is the
following. If y ∈P(f), y + χu ∈P(f), y + 2χu /∈P(f), u ∈C ⊆S, and C is
the unique inclusionwise minimal 1-deﬁcient set for y + 2χu, then we say that “u
induces a circuit on C in y”.
Consider a matching x with respect to a polymatroid function f : 2S →N.
Consider a laminar family F = {B1, · · · , Bk} of subsets of S, that is, any two
members of F are either disjoint or one contains the other. For indices i =
1, · · · , k, let Fi denote the family of inclusionwise maximal proper subsets of Bi
in F, and let Gi := Bi −∪Fi. Consider a set U = {u1, · · · , uk} ⊆S such that
ui ∈Gi. Hence F, U is called an x-ear-decomposition if

174
M. Makai, G. Pap, and J. Szabó
(a) x(Bi) = f(Bi) −1, and
(b) ui induces a circuit on Gi in x||S−∪Fi with respect to f Fi.
Notice that the above deﬁnition implies that x+χui ∈P(f) holds whenever Bi is
an inclusionwise minimal member of F. This implies that the projection of F, or
Fi satisﬁes the assumption in the previous section, and thus the projection may
be performed in arbitrary order. Notice, if we drop an inclusionwise maximal
member Bi ∈F together with ui, we retain another ear-decomposition. A set B
appearing in the family F of some ear-decomposition is called an x-blossom. An
ear-decomposition of a blossom B is an ear-decomposition F, U such that B is
the unique inclusionwise maximal member of F.
The following Lemma 5 will be our crucial inductive tool to deal with ear-
decompositions by extending a matching with respect to f F to a matching with
respect to f.
Lemma 5. Suppose we are given a matching x, an x-blossom B together with
an x-ear-decomposition, and a vector y ∈P(f B). There is a polynomial time
algorithm to ﬁnd either
(A) a ntcdc, or
(B) an even vector z ∈(2N)B such that z(B) = f(B) −1 and [z, y] ∈P(f).
Proof. Let us use notation from above. The algorithm is recursive on the number
k of ears. Firstly, notice that deff([x||B, y]) ≤1. If deff([x||B, y]) = 0, then (B)
holds for z = x||B, and we are done. Henceforth we suppose that deff([x||B, y]) =
1, and let D denote the inclusionwise minimal 1-deﬁcient set for [x||B, y]. Say
B = Bk and G = Gk.
We claim that either [x||G, y] ∈P(f Fk), or D ⊆(S −B) ∪G. Suppose
[x||G, y] /∈P(f Fk). By (4), there is a set Q such that deff,[x||B,y](Q) ≥1, and
for all Bi ∈Fk we have Q ∩Bi = ∅or Q ⊇Bi. Clearly, deff,[x||B,y](B) = −1.
Since y ∈P(f B), we get that deff,[x||B,y](B ∪Q) ≤0. Thus, by supermodularity
of deﬁciency, 0 ≤deff,[x||B,y](B ∩Q) = deff,x(B ∩Q). Recall that for every
inclusionwise minimal set Bi ∈F we have x + χui ∈P(f) for ui ∈Bi. Thus,
ui /∈B ∩Q, which implies that D ⊆Q ⊆(S −B) ∪G.
Now suppose that [x||G, y] ∈P(f Fk). Thus, by (4), there is a (not necessarily
even) vector z′ ∈N∪Fk such that [z′, x||G, y] ∈P(f), and z′(Bi) = b(Bi) −1
for all Bi ∈Fk. Then we apply the algorithm recursively for Bi ∈Fk and
[z′, x||G, y], that is, we replace z′||Bi step-by-step by an even vector retaining
the above properties – or we ﬁnd a ntcdc.
Finally suppose that D ⊆(S −B) ∪G. Notice that y ∈P(f B) implies D ∩
B ̸= ∅. Also, x ∈P(f) implies D −B ̸= ∅. Moreover, y ∈P(f B) implies
deff,[x|B,y](B∪D) ≤0. Recall that deff,[x||B,y](D) = 1 and deff,[x||B,y](B) = −1.
By supermodularity of deﬁciency, deff,[x||B,y](B ∩D) ≥0. Thus, by (b) we get
that uk /∈D. Consider an arbitrary element d ∈D ∩B. By (b), [x||G + 2χuk −
χd, 0] ∈P(f Fk). By applying the algorithm recursively for [x||G + 2χuk −χd, 0]
one can ﬁnd either a ntcdc, or an even vector q ∈(2N)∪Fk such that [q, x||G +
2χuk −χd, 0] ∈P(f). Next, we will ﬁnd out whether there is an element e such

Matching Problems in Polymatroids Without Double Circuits
175
that z = [q, x||G +2χuk −2χe] satisﬁes (B). Clearly, all these vectors are even. It
is easy to see that deff([q, x||G+2χuk, y]) is 1 or 2. If deff([q, x||G+2χuk, y]) = 1,
then for some element e we get that [q, x||G +2χuk −2χe, y], and we are done. If
deff([q, x||G + 2χuk, y]) = 2, then let W denote the unique minimal 2-deﬁcient
set. If there is an element e ∈W such that all the 1-deﬁcient sets contain e,
then [q, x||G + 2χuk −2χe, y] ∈P(f), and we are done. Otherwise, if for every
element e there is a 1-deﬁcient set e /∈We, then [q, x||G + 2χuk, y]|W ∈NS is a
cdc. Notice that B and D are circuits in [q, x||G + 2χuk, y], thus W −B ∈π and
W −D ∈π. Since d ∈B ∩D ̸= ∅, this implies |π| ≥3.
3.3
A Semi-strongly Polynomial Time Algorithm
We construct a semi-strongly polynomial time algorithm which either returns a
ntcdc, or returns a maximum matching x and a partition certifying its maximal-
ity. The algorithm maintains a matching, and iteratively augments its size by
one, until it either ﬁnds a certifying partition, or a ntcdc. We may initiate x as a
basis of P(f), rounded down to the closest even vector. This initialization may
be performed in semi-strongly polynomial time, where “semi-” comes only from
the fact that we have to take lower integer part to detect parity. The remaining
part of the algorithm may be performed in strongly polynomial time.
The idea behind the algorithm is the following. If our matching x is a basis
in the polymatroid, then we are done. Thus we ﬁnd an element u ∈S such that
x + χu ∈P(f). If x + 2χu ∈P(f), then that gives a larger matching, and we are
done. Otherwise, we may assume that x + χu ∈P(f) and x + 2χu /∈P(f), i.e. u
induces a circuit in x, which can be used building blossoms and projections. If
we ﬁnd a larger matching in the projection, then we use Lemma 5 to expand
blossoms and retain a larger matching over the original groundset. This idea is
developed in detail below.
Consider a matching x. Deﬁne C := ∅. In a general step of the algorithm,
C = {B1, · · · , Bk} is a family of disjoint x-blossoms. This implies that x||S−∪C ∈
P(f C). We distinguish three cases on how close x||S−∪C is to a basis of P(f C).
Case 1. Suppose that x(S − C) = f C(S − C). Then, by claim (6), there is a
set C′ ⊆C such that f(S − C +  C′) = x(S − C′ +  C′). Then C′ = ∅, since
for all blossoms Bi ∈C there is an element t ∈Bi such that x + χt ∈P(f). We
conclude that x is a maximum matching, certiﬁed by the partition C ∪{S − C}.
Case 2. Suppose that x||S−∪C + χu ∈P(f C), but x||S−∪C + 2χu /∈P(f C). Then
there is a set u ∈Z ⊆S −∪C such that u induces a circuit on Z in x||S−∪C
with respect to f C. By claim (6) there is a set C′ ⊆C such that f(Z ∪ C′) =
x(Z ∪ C′) + 1. Thus, C −C′ + {Z ∪ C′} is a blossom family.
Case 3. Suppose that x||S−∪C +2χu ∈P(f C). In this case, by applying Lemma 5
for members of C, we construct either a matching larger than x, or a ntcdc. This is
done as follows. By assertion (5), there is a (not necessarily even) vector z ∈N∪C
such that x′ := [z, x||S−∪C +2χu] ∈P(f), and z(Bi) = f(Bi)−1 for i = 1, · · · , k.
Thus, for an arbitrary index i ∈{1, · · · , k} we get that x′||S−Bi ∈P(f Bi). By

176
M. Makai, G. Pap, and J. Szabó
applying Lemma 5 for Bi, we either construct a ntcdc, or we may replace entries
of x′ in Bi with even numbers, and retain the above properties. By repeating
this procedure for i = 1, · · · , k we retain a matching x′ that is larger than x.
4
Applications
4.1
A Parity Constrained Orientation Theorem
Frank, Jordán and Szigeti [4] proved that the existence of a k-rooted-connected
orientation with prescribed parity of in-degrees can be characterized by a parti-
tion type condition. Recently, Király and Szabó [7] proved that the connectivity
requirement in this parity constrained orientation problem can be given by a
more general non-negative intersecting supermodular function. It is well-known
that all these problems can be formalized as polymatroid parity problems. In
this section we show that it is possible to formalize the problem of Király and
Szabó in such a way that the arising polymatroid function has no non-trivial
double circuits. So Theorem 1 can be applied to yield the result in [7].
H = (V, E) is called a hypergraph if V is a ﬁnite set and ∅/∈E is a collection
of multisets of V , the set of hyperedges of H. If in every hyperedge h ∈E we
designate a vertex v ∈h as the head vertex then we get a directed hypergraph
D = (V, A), called an orientation of H. For a set X ⊆V , let δD(X) denote the
set of directed hyperedges entering X, that is the set of hyperedges with head
in X and at least one vertex in V −X.
Let p : 2V →N be a function with p(∅) = p(V ) = 0. An orientation D of
a hypergraph H = (V, E) covers p if |δD(X)| ≥p(X) for every X ⊆V . In a
connectivity orientation problem the question is the existence of an orientation
covering p. When we are talking about parity constrained orientations, we are
looking for connectivity orientations such that the out-degree at each vertex is
of prescribed parity. Now deﬁne b : 2V →Z by
b(X) =

h∈E
h(X) −|E[X]| −p(X) for X ⊆V,
(7)
where E[X] denotes the set of hyperedges h ∈E with h ∩(V −X) = ∅, and
h equivalently stands for the hyperedge and its multiplicity function. It is clear
that if x : V →N is the out-degree vector of an orientation covering p then
x ∈B(b). The contrary is also easy to prove, see e.g. in [14]:
Lemma 6. Let H = (V, E) be a hypergraph, p : 2V →N be a function with
p(∅) = p(V ) = 0, and x : V →N. Then H has an orientation covering p such
that the out-degree of each vertex v ∈V is x(v) if and only if x ∈B(b).
The function b : 2V →Z is said to be intersecting submodular if (1) holds
whenever X ∩Y ̸= ∅. Similarly, p : 2V →Z is intersecting supermodular if
−p is intersecting submodular. If b : 2V →N is a non-negative, non-decreasing
intersecting submodular function then we can deﬁne a polymatroid function
b : 2V →N by b(X) = min
t
i=1 b(Xi) : X1 ˙∪X2 ˙∪. . . ˙∪Xt = X

for X ⊆V ,

Matching Problems in Polymatroids Without Double Circuits
177
which is called the Dilworth truncation of b. It is also well-known that, if p :
2V →N is intersecting supermodular with p(V ) = 0, then p is non-increasing.
Thus if p : 2V →N is an intersecting supermodular function with p(∅) =
p(V ) = 0 then b : 2V →Z, as deﬁned in (7), is a non-decreasing intersect-
ing submodular function, but it is not necessarily non-negative. The following
theorem can be proved using basic properties of polymatroid functions.
Theorem 3. Let H = (V, E) be a hypergraph and p : 2V →N be an intersecting
supermodular function with p(∅) = p(V ) = 0. Deﬁne b as in (7). Then H has an
orientation covering p if and only if b(V ) ≤t
j=1 b(Uj) holds for every partition
U1, U2, . . . , Ut of V .
Let H = (V, E) be a hypergraph and T ⊆V . Our goal is to ﬁnd an orientation
of H covering p, where the set of odd out-degree vertices is as close as possible
to T .
Theorem 4 (Király and Szabó [7]). Let H = (V, E) be a hypergraph, T ⊆V ,
p : 2V →N be an intersecting supermodular function with p(∅) = p(V ) = 0,
and assume that H has an orientation covering p. Deﬁne b as in (7). For an
orientation D of H let YD ⊆V denote the set of odd out-degree vertices in D.
Then
min {|T △YD| : D is an orientation of H covering p} =
max

b(V ) −t
j=1 b(Uj) + |{j : b(Uj) ̸≡|T ∩Uj| mod 2}|

,
(8)
where the maximum is taken on partitions U1, U2, . . . , Ut of V .
An interesting corrollary is the following non-defect form, which is again a gen-
eralization of Theorem 3.
Theorem 5. Let H = (V, E) be a hypergraph, T ⊆V , and let p : 2V →N be
an intersecting supermodular function with p(∅) = p(V ) = 0. Then, H has an
orientation covering p with odd out-degrees exactly in the vertices of T, if and
only if
b(V ) ≤t
j=1 b(Uj) −|{j : b(Uj) ̸≡|T ∩Uj| mod 2}|
(9)
holds for every partition U1, U2, . . . , Ut of V .
Proof. For every v ∈T add a loop 2χv to E, resulting in the hypergraph H′ =
(V, E′). Deﬁne b′ as in (7), w.r.t. H′. As there is a straightforward bijection
between the orientations of H and H′, we have min{|T △YD| : D is an orientation
of H covering p} = min{|YD′| : D′ is an orientation of H′ covering p}, and
b(V )−t
j=1 b(Uj)+|{j : b(Uj) ̸≡|T ∩Uj| mod 2}| = b′(V )−t
j=1 b′(Uj)+|{j :
b′(Uj) is odd }|. Thus we can assume that T = ∅.
By Lemma 6, the integer vectors of B(b) are exactly the out-degree vectors of
the orientations of H covering p. Thus the ≥direction is easy to check. Now we
prove the other direction. As H has an orientation covering p, if ∅⊆U ⊆V then
b(U)+b(V −U) ≥b(V ) by Theorem 3, implying that b(U) ≥b(V )−b(V −U) ≥0.

178
M. Makai, G. Pap, and J. Szabó
Thus, b is non-decreasing, and we can deﬁne the polymatroid function f = b.
We claim that it is enough to prove that ν(f) = min s
i=1

 1
2f(Vi)

, where
the minimum is taken over all partitions V1, V2, . . . , Vs of V . Indeed, using the
deﬁnition of the Dilworth-truncation and that b(V ) = f(V ) by Theorem 3, we
get
min{|YD| : D is an ori. of H covering p} = f(V ) −2ν(f) =
= b(V ) −min {s
i=1 f(Vi) −|{i : f(Vi) is odd}| : V1, . . . , Vs partitions V } ≤
≤b(V ) −min
t
j=1 b(Uj) −|{j : b(Uj) is odd}| : U1, . . . , Ut partitions V

.
Thus by Theorem 1 it is enough to prove that b has no non-trivial compatible
double circuits. The next lemma does the job.
Lemma 7. Let H = (V, E) be a hypergraph and let p : 2V →N an intersecting
supermodular function with p(∅) = 0. Suppose moreover that b : 2V →Z deﬁned
by (7) is non-negative and non-decreasing. Then the polymatroid function f := b
has no non-trivial compatible double circuits.
Proof. Assume that w : V →N is a non-trivial compatible double circuit of
f with principal partition W = W1 ˙∪W2 ˙∪. . . ˙∪Wd. Clearly, b(W) ≥w(W) −2.
Let 1 ≤i < j ≤d and Z = W −Wi. As w|Z is a circuit, Lemma 2 yields
that w(Z) −1 = f(Z) = min {b(Xi) : X1, . . . , Xk partitions Z}. However,
if a non-trivial partition with k ≥2 gave equality here, then we would have
f(Z) =  b(Xi) ≥ f(Xi) ≥ w(Xi) = w(Z) > f(Z), because w|Xi ∈P(f).
Thus w(W −Wi) −1 = b(W −Wi), and similarly, x(W −Wj) −1 = b(W −Wj).
By applying intersecting submodularity to W −Wi and W −Wj, and using that
w|W−Wi−Wj ∈P(f), we get 0 ≥b(W) −b(W −Wi) −b(W −Wj) + b(W −Wi −
Wj) ≥(w(W)−2)−(w(W −Wi)−1)−(w(W −Wj)−1)+w(W −Wi−Wj) = 0, so
equality holds throughout. As a corollary, each hyperedge e ∈E[W] is spanned
by one of the Wi’s, and
d −1
2

(b(W) + 2) =
d −1
2

w(W) =
=

1≤i<j≤d
w(W −Wi −Wj) =

1≤i<j≤d
b(W −Wi −Wj).
(10)
On the other hand,
d −1
2
 
h∈E
h(W) =

1≤i<j≤d

h∈E
h(W −Wi −Wj),
since 
h∈E h is modular, and
d −1
2

p(W) ≤

1≤i<j≤d
p(W −Wi −Wj),

Matching Problems in Polymatroids Without Double Circuits
179
since p is non-negative and non-increasing. Finally,
d −1
2

|E[W]| =
d −1
2

d

i=1
|E[Wi]| =

1≤i<j≤d
|E[W −Wi −Wj]|.
By the deﬁnition of b, the last 3 equalities together contradict (10).
Let us give an example showing that polymatroids without non-trivial compat-
ible double circuits are not closed under contractions. Let V = {v1, v2, v3, v4},
E = {v1vi, vivi : i ∈{2, 3, 4}}, p({v1}) = 1 and p(U) = 0 for the other sets.
Then, by Lemma 7, b has no non-trivial compatible double circuits, while the
polymatroid obtained from b by contracting an element in the prematroid from
the preimage of v1 has the non-trivial compatible double circuit (1, 2, 2, 2).
4.2
A Planar Rigidity Problem
If G = (V, E) is a graph and p : V →R2 is an embedding into the Euclidean
plane then (G, p) is said to be a framework. We think of the edges of G as
rigid bars with ﬂexible joins at the vertices. An inﬁnitesimal motion means an
assignment of velocities x(v) ∈R2 to each vertex v ∈V such that the bar lengths
are preserved, that is (p(u)−p(v)) ⊥(x(u)−x(v)). The framework (G, p) is called
rigid if all inﬁnitesimal motions of (G, p) correspond to isometries of R2. The
question of pinning down a minimum vertex set resulting a rigid framework was
solved by Lovász in his seminal paper [10] about matroid parity. We say that
G = (V, E) is generic rigid if all frameworks (G, p) with algebraically independent
coordinates p are rigid. The problem of ﬁnding a vertex set Z ⊆V of minimum
size such that G + KZ is generic rigid is left open by [10], and it was solved
recently by Fekete [3]. For more on the 2-dimensional rigidity see Laman [8] and
Lovász and Yemini [11].
The setup of [3] puts the problem into a bit more general setting. Let G =
(V, E) be a graph, and for l ∈{2, 3} let M2,l be the matroid on ground set E
such that F ⊆E is independent in M2,l if and only if |F[X]| ≤2|X| −l for all
X ⊆V , |X| ≥2. It can be proved that M is really a matroid. For clarity, M2,2
is two times the cycle matroid of G, and so G has two edge-disjoint spanning
trees if and only if r2,2(E) = 2|V | −2. As M2,3 is the rigidity matroid of G,
the graph G is generic rigid if and only if r2,3(E) = 2|V | −3. For Z ⊆V let
KZ = (Z, EZ) be the graph with vertex set Z having 4−l parallel edges between
any two vertices of Z. Our goal is to ﬁnd a set Z ⊆V of minimum size such that
E + EZ has rank 2|V | −l. For l = 2, this is equivalent to shrinking a minimum
vertex set Z such that G/Z has two edge-disjoint spanning trees.
We assume that E is independent in M2,l, since if E is replaced by one of
its bases then the solution set does not change. Fekete [3] proved the following
lemma. For X ⊆V let e(X) denote the number of edges having at least one end
vertex in X.
Lemma 8 ([3]). Let l ∈{2, 3}. Assume that E is independent in M2,l and
that r2,l(E) < 2|V | −l. Let Z ⊆V . Then r(E + EZ) = 2|V | −l if and only if
e(Y ) ≥2|Y | for every Y ⊆V −Z.

180
M. Makai, G. Pap, and J. Szabó
Therefore, the goal is to ﬁnd a set Z ⊆V of minimum size such that e(Y ) ≥2|Y |
for every Y ⊆V −Z. Let f : 2V →N be the polymatroid function with
f(X) = minY ⊆X 2|Y |+e(X−Y ), i.e. f is obtained from the polymatroid function
X →e(X) by deleting with the vector (2, 2, . . . , 2). Hence for l = 2 the value
|V |−ν(f) means the minimum size of a set Z whose contraction results in a graph
with two edge-disjoint spanning trees, and for l = 3 it is the minimum size of a
set Z such that G+KZ is generic rigid. In [10] the computation of ν(f) is reduced
to the matching problem of graphs, yielding a partition type characterization.
This characterization follows from the previous results of this paper, too. First,
by Lemma 7 with the choice p = 0, the polymatroid function X →e(X) has
no non-trivial compatible double circuits. As f is obtained from X →e(X) by
deletion, Claim 4 yields that nor f has. Thus, ν(f) = min t
j=1

 1
2f(Uj)

, where
the minimum is taken over all partitions U1, U2, . . . , Ut of V . By the deﬁnition
of f, we get the following.
Theorem 6 (Fekete, [3]). Let l ∈{2, 3}. Assume that E is independent in
M2,l and that r2,l(E) < 2|V | −l. Then the minimum size of a set Z ⊆V such
that r(E + EZ) = 2|V | −l is |V | −ν(f), where
ν(f) = min

V −
t
j=1
Uj

+
t

j=1
e(Uj)
2

,
where the minimum is taken over all subpartitions U1, U2, . . . , Ut of V .
Acknowledgments. The authors are grateful for the support of András Frank,
and discussions with Zsolt Fekete.
References
1. Dress, A. and Lovász, L., On some combinatorial properties of algebraic matroids,
Combinatorica, (1987), 7/1, 39–48
2. J. Edmonds, Paths, trees, and ﬂowers, Canadian Journal of Mathematics, 17 (1965)
449–467
3. Fekete,
Z.,
Source
location
with
rigidity
and
tree
packing
requirements,
www.cs.elte.hu/egres, TR-2005-04
4. Frank, A. and Jordán, T. and Szigeti, Z., An orientation theorem with parity
conditions, Discrete Appl. Math., (2001), 115/1-3, 37–47
5. Frank, A. and Király, T. and Király, Z., On the orientation of graphs and hyper-
graphs, Discrete Appl. Math., (2003), 131/2, 385–400
6. Jensen, P.M. and Korte, B., Complexity of matroid property algorithms, SIAM
Journal on Computing, (1982), 11/1, 184–190
7. Király,
T.
and
Szabó,
J.,
A
note
on
parity
constrained
orientations,
www.cs.elte.hu/egres, TR-2003-11
8. Laman, G., On graphs and rigidity of plane skeletal structures, J. Engrg. Math.,
(1970), 4, 331–340
9. Lovász, L., The matroid matching problem, Algebraic methods in graph theory,
Vol. I, II (Szeged, 1978), Colloq. Math. Soc. János Bolyai, 25, 495–517

Matching Problems in Polymatroids Without Double Circuits
181
10. Lovász, L., Matroid matching and some applications, J. Combin. Theory Ser. B,
(1980), 28/2, 208–236
11. Lovász, L. and Yemini, Y., On generic rigidity in the plane, SIAM J. Algebraic
Discrete Methods, (1982) 3/1, 91–98
12. Lovász, L., Selecting independent lines from a family of lines in a space, Acta Sci.
Math. (Szeged), (1980), 42/1-2, 121–131
13. Nebeský, L., A new characterization of the maximum genus of a graph, Czechoslo-
vak Math. J., (1981), 31(106), 604–613
14. Schrijver, A., Combinatorial optimization. Polyhedra and eﬃciency, Springer-
Verlag, Berlin, 2003

Maximizing a Submodular Set Function
Subject to a Matroid Constraint
(Extended Abstract)
Gruia Calinescu1,⋆⋆, Chandra Chekuri2, Martin P´al3, and Jan Vondr´ak4
1 Computer Science Dept., Illinois Institute of Technology, Chicago, IL
calinescu@iit.edu.
2 Dept. of Computer Science, University of Illinois, Urbana, IL 61801
chekuri@cs.uiuc.edu.
3 Google Inc., 1440 Broadway, New York, NY 10018
mpal@google.com.
4 Dept. of Mathematics, Princeton University, Princeton, NJ 08544
jvondrak@math.princeton.edu.
Abstract. Let f : 2N →R+ be a non-decreasing submodular set func-
tion, and let (N, I) be a matroid. We consider the problem maxS∈I f(S).
It is known that the greedy algorithm yields a 1/2-approximation [9] for
this problem. It is also known, via a reduction from the max-k-cover prob-
lem, that there is no (1 −1/e + ϵ)-approximation for any constant ϵ > 0,
unless P = NP [6]. In this paper, we improve the 1/2-approximation to
a (1−1/e)-approximation, when f is a sum of weighted rank functions of
matroids. This class of functions captures a number of interesting prob-
lems including set coverage type problems. Our main tools are the pi-
page rounding technique of Ageev and Sviridenko [1] and a probabilistic
lemma on monotone submodular functions that might be of independent
interest.
We show that the generalized assignment problem (GAP) is a special
case of our problem; although the reduction requires |N| to be expo-
nential in the original problem size, we are able to interpret the recent
(1 −1/e)-approximation for GAP by Fleischer et al. [10] in our frame-
work. This enables us to obtain a (1 −1/e)-approximation for variants
of GAP with more complex constraints.
1
Introduction
This paper is motivated by the following optimization problem. We are given
a ground set N of n elements and a non-decreasing submodular set function
f : 2N →R+. The function f is submodular iﬀf(A)+f(B) ≥f(A∪B)+f(A∩B)
for all A, B ⊆N. We restrict attention to non-decreasing (or monotone) sub-
modular set functions, that is f(A) ≥f(B) for all B ⊆A and f(∅) = 0. An
independence family I ⊆2N is a family of subsets that is downward closed, that
⋆⋆Research partially supported by NSF grant CCF-0515088.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 182–196, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

Maximizing a Submodular Set Function Subject to a Matroid Constraint
183
is, A ∈I and B ⊆A implies that B ∈I. A set A is independent iﬀA ∈I.
A family I is a p-independence family for an integer p ≥1 if for all A ∈I
and e ∈N there exists a set B ⊆A such that |B| ≤p and A \ B + e is inde-
pendent. For computational purposes we will assume that f and I are speciﬁed
as oracles although in many speciﬁc settings of interest, an explicit description
is often available. The problem (or rather class of problems) of interest in this
paper is the following: maxS∈I f(S). We will be mostly interested in the special
case when I consists of the independent sets of a matroid on N. The problem
of maximizing a submodular set function subject to independence constraints
has been studied extensively. A number of interesting and useful combinatorial
optimization problems, including NP-hard problems, are special cases. Some no-
table examples are maximum independent set in a matroid, weighted matroid
intersection, and maximum coverage. Below we describe some candidates for f
and I that arise frequently in applications.
Modular functions: A function f : 2N →R+ is modular iﬀf(A) + f(B) =
f(A∪B)+f(A∩B). If f is modular then there is a weight function w : N →R+
such that f(A) = w(A) = 
e∈A w(e).
Set Systems and Coverage: Given a universe U and n subsets S1, S2, . . . , Sn of
U we obtain several natural submodular functions on the set N = {1, 2, . . ., n}.
First, the coverage function f given by f(A) = |∪i∈ASi| is submodular. This nat-
urally extends to the weighted coverage function; given a non-negative weight
function w : U →R+, f(A) = w(∪i∈ASi). We obtain a multi-cover version
as follows. For x ∈U let k(x) be an integer. For each x ∈U and Si let
c(Si, x) = 1 if x ∈Si and 0 if x /∈Si. Given A ⊆N, let c′(A, x), the cov-
erage of x under A, be deﬁned as c′(A, x) = min{k(x), 
i∈A c(Si, x)}. The
function f with f(A) = 
x∈U c′(A, x) is submodular. A related function de-
ﬁned by f(A) = 
x∈U maxi∈A w(Si, x) is also submodular where w(Si, x) is a
non-negative weight for Si covering x.
Weighted rank functions of matroids and their sums: The rank function of a
matroid M = (N, I), rM(A) = max{|S| : S ⊆A, S ∈I}, is submodular. Given
w : N →R+, the weighted rank function deﬁned by rM,w(A) = max{w(S) :
S ⊆A, S ∈I} is a submodular function. A sum of weighted rank functions is
also submodular. Functions arising in this way form a rich class of submodular
functions. In particular, all the functions on set systems and coverage mentioned
above are captured by this class. However, the class does not include all monotone
submodular functions; one notable exception is multi-cover by multisets.
Matroid Constraint: An independence family of particular interest is one induced
by a matroid M = (N, I). A very simple matroid constraint that is of much
importance in applications [5,14,2,3,10] is the partition matroid; N is partitioned
into ℓsets N1, N2, . . . , Nℓwith associated integers k1, k2, . . . , kℓ, and a set A ⊆N
is independent iﬀ|A ∩Ni| ≤ki. In fact even the case of ℓ= 1 (the uniform
matroid) is of interest. Laminar matroids generalize partition matroids. We have
a laminar family of sets on N and each set S in the family has an integer value
kS. A set A ⊆N is independent iﬀ|A ∩S| ≤kS for each S in the family.

184
G. Calinescu et al.
Intersection of Matroids: A natural generalization of the single matroid case is
obtained when we consider intersections of diﬀerent matroids M1, M2, . . . , Mp
on the same ground set N. That is, I = ∩iIi where Ii is the independence family
of Mi. A simple example is the family of hypergraph matchings in a p-partite
graph (p = 2 is simply the family of matchings in a bipartite graph).
Matchings: Given a general graph G = (V, N) the set of matchings forms a 2-
independent family. Given a hypergraph G = (V, N) such that each edge e ∈N
is of cardinality at most p, the set of matchings in G induce a p-independent
family. Note that matchings in general graphs are not captured as intersections
of matroids.
The Greedy Algorithm: A simple greedy algorithm is quite natural for this
problem. The algorithm incrementally builds a solution (without backtracking)
starting with the empty set. In each iteration it adds an element that most
improves the current solution (according to f) while maintaining independence of
the solution. The greedy algorithm yields a 1/p-approximation for maximizing a
modular function subject to a p-independence constraint [12,13]. For submodular
functions, the greedy algorithm yields a ratio of 1/(p + 1) [9]. 1 These ratios for
greedy are tight for all p even when the p-independent system is obtained as
an intersection of p matroids. For large but ﬁxed p, the p-dimensional matching
problem is NP-hard to approximate to within an Ω(log p/p) factor [11].
For the problem of maximizing a submodular function subject to a matroid
constraint (special case of p = 1), the greedy algorithm achieves a ratio of 1/2.
When the matroid is the simple uniform matroid (S ⊆N is independent iﬀ
|S| ≤k) the greedy algorithm yields a (1−1/e)-approximation [14]. This special
case already captures the maximum coverage problem for which it is shown
in [6] that, unless P = NP, no 1 −1/e + ϵ approximation is possible for any
constant ϵ > 0. This paper is motivated by the following question. Is there a
(1−1/e)-approximation algorithm for maximizing a submodular function subject
to (any given) matroid constraint? We resolve this question for a subclass of
monotone submodular functions, which can be expressed as a sum of weighted
rank functions of matroids. The following is our main result.
Theorem 1. Given a ground set N, let f(S) = m
i=1 gi(S) where g1, . . . , gm :
2N →R+ are weighted rank functions, gi deﬁned by a matroid Mi = (N, Xi)
and weight function wi : N →R+. Given another matroid M = (N, I) and
membership oracles for M1, M2, . . . , Mm and M, there is a polynomial time
(1 −1/e)-approximation for the problem maxS∈I f(S).
As immediate corollaries we obtain a (1 −1/e)-approximation for a number
of coverage problems under a matroid constraint. It is known that there exist
submodular monotone functions that cannot be expressed as a sum of weighted
rank functions of matroids (see [16], 44.6e). For such functions, our framework
1 We give a somewhat new proof of this result in the full version of the paper. If only
an α-approximate oracle (α ≤1) is available for the function evaluation, the ratio
obtained is α/(p+α). Several old and recent applications of greedy can be explained
using this observation.

Maximizing a Submodular Set Function Subject to a Matroid Constraint
185
does not seem to apply at this moment. We leave it as an open question whether
a (1 −1/e)-approximation is possible for all monotone submodular functions.
Our main tools are the the pipage rounding technique of Ageev and Sviri-
denko [1], and the following useful lemma.
Lemma 1. Let f : 2N →R+ be a monotone submodular function and let f ∗:
[0, 1]N →R+ be deﬁned as f ∗(y) = minS(f(S) + 
i yi(f(S + i) −f(S))). For
y ∈[0, 1]N, let ˆy denote a random vector in {0, 1}N obtained by independently
setting ˆyi = 1 with probability yi and 0 otherwise. Then, E[f(ˆy)] ≥(1−1/e)f ∗(y).
We give a non-trivial application of Theorem 1 to variants of the generalized
assignment problem (GAP). In GAP we are given n bins and m items. Each
item i speciﬁes a size sji and a value (or proﬁt) vji for each bin j. Each bin
has capacity 1 and the goal is to assign a subset of items to bins such that the
bin capacities are not violated and the proﬁt of the assignment is maximized.
Recently Fleischer et al. [10] gave a (1 −1/e)-approximation for this problem,
improving upon a 1/2-approximation [4]. We rederive the same ratio casting the
problem as a special case of submodular function maximization. Moreover our
techniques allow us to obtain a (1−1/e)-approximation for GAP even under any
given laminar matroid constraint on the bins. A simple and easy to understand
example is GAP with the added constraint that at most k of the n bins be used.
Theorem 2. Let A be an instance of GAP with n bins and m items and let
B be the set of bins. Let M = (B, I) be a laminar matroid on B. There is a
polynomial time (1 −1/e)-approximation to ﬁnd a maximum proﬁt assignment
to bins such that the subset S ⊆B of bins that are used in the assignment satisfy
the constraint S ∈I.
We note that the approximation ratio for GAP has been improved to 1−1/e+δ1
for a small δ1 > 0 in [8] using the same LP as in [10]. However, the algorithm in
[10] extends to even more general assignment problems in which the sets of items
allowed in a bin are further constrained; for such allocation problems it is shown
in [10] that it is NP-hard to obtain an approximation ratio of 1 −1/e + ϵ for any
constant ϵ > 0. Our framework also extends to this wider class of assignment
problems and hence 1 −1/e is the best approximation factor one can achieve
with this approach.
1.1
Preliminaries
Given a submodular function f : N →R+ and A ⊂N, the function fA deﬁned
by fA(S) = f(S ∪A)−f(A) is also submodular. Further, if f is monotone, fA is
also monotone. For i ∈N, we abbreviate S ∪{i} by S + i. By fA(i), we denote
the “marginal value” f(A+i)−f(A). Submodularity is equivalent to fA(i) being
non-increasing as a function of A for every ﬁxed i.
Given a matroid M = (N, I), we denote by rM the rank function of M
where rM(A) = max{|S| : S ⊆A, S ∈I}. The rank function is monotone and
submodular. We denote by P(M) the polytope associated with M; this is the set

186
G. Calinescu et al.
of all real vectors y ∈[0, 1]N that satisfy the constraints: y(S) ≤rM(S) ∀S ⊆
N, where y(S) = 
i∈S yi. Edmonds showed that the vertices of P(M) are
precisely the characteristic vectors of the independent sets of M. Further, given
a membership oracle for M (that is given S ⊆N, the oracle answers if S ∈I or
not), one can optimize linear functions over P(M).
A base of M is a set S ∈I such that rM(S) = rM(N). The base polytope B(M)
of M is given by {y ∈P(M) | y(N) = rM(N)}. The extreme points of B(M)
are the characteristic vectors of the bases of M. Given the problem maxS∈I f(S),
where M = (N, I) is a matroid, there always exists an optimum solution S∗where
S∗is a base of M. Note that this is false if f is not monotone. Thus, for monotone
f, it is equivalent to consider the problem maxS∈B f(S) where B is the set of bases
of M. See [16] for more details on matroids and polyhedral aspects.
2
Pipage Rounding Framework
Ageev and Sviridenko [1] developed an elegant technique for rounding solutions
of linear and non-linear programs that they called “pipage rounding”. Subse-
quently, Srinivasan [17] and Gandhi et al. [15] interpreted some applications of
pipage rounding as a deterministic variant of dependent randomized rounding.
In a typical scenario, randomly rounding a fractional solution of a linear program
does not preserve the feasibility of constraints, in particular equality constraints.
Nevertheless, the techniques of [1,17,15] show that randomized rounding can be
applied in a certain controlled way to guide a solution that respects certain class
of constraints. In particular these techniques were used to round fractional so-
lutions to the generalized assignment problem. In this paper we show that the
rounding framework applies quite naturally to our problem. Further, our analysis
also reveals the important role of submodularity in this context.
We now describe the pipage rounding framework as adapted to our problem. We
follow [1] in spirit although our notation and description is somewhat diﬀerent and
tailored to our application: given a monotone submodular function f : 2N →R+
and a matroid M = (N, I), we wish to solve maxS∈I f(S). Let yi ∈{0, 1} be
a variable that indicates whether i is picked in a solution to the problem. Then
maxS∈I f(S) can be written as the following problem: max{f(y) : y ∈P(M), y ∈
{0, 1}N}. As we observed in Section 1.1, this is equivalent to max{f(y) : y ∈
B(M), y ∈{0, 1}N} where B(M) is the base polytope of M.
The framework relies on the ability to solve a relaxation of the problem in
polynomial time. To obtain a relaxation we let y ∈[0, 1]N. This also requires
us to ﬁnd an extension of f to a function ˜f : [0, 1]N →R+ such that the
problem max{ ˜f(y) : y ∈P(M)} can be solved in polynomial time. We require
two properties of the extension: (i) ˜f(y) = f(y) for all y ∈{0, 1}N, and (ii)
monotonicity, that is ˜f(y) ≥˜f(z), for all y ≥z; y, z ∈[0, 1]N. Note that the
optimum value of the relaxation is at least the integral optimum solution denoted
by OPT. Given an optimum fractional solution y∗to the relaxation, our goal is
to round y∗to an integer solution z such that f(z) ≥α ˜f(y∗) ≥αOPT. Clearly
the quality of the relaxation depends on the extension function ˜f. The rounding

Maximizing a Submodular Set Function Subject to a Matroid Constraint
187
framework relies on a potential function F : [0, 1]N →R+, derived from f, that
guides the rounding and at the same time allows one to derive bounds on the
quality of the approximation. The reason to consider ˜f and F separately will
become clear later. Assuming the existence of ˜f and F, we describe the pipage
rounding algorithm for our problem.
Given y ∈[0, 1]n we say that i is fractional in y if 0 < yi < 1. For y ∈P(M),
a set A ⊆N is tight if y(A) = rM(A). The following useful proposition follows
easily from the submodularity of the rank function rM.
Proposition 1. If A and B are two tight sets with respect to y then A ∩B and
A ∪B are also tight with respect to y.
The monotonicity of ˜f also implies the following.
Proposition 2. There exists an optimum solution y∗to max{ ˜f(y) : y ∈P(M)}
such that y∗(N) = 
i∈N y∗
i = rM(N).
Alternatively we can solve the problem max{ ˜f(y) : y ∈B(M)} which would
automatically ensure that y∗(N) = rM(N). We are interested in tight sets that
contain a fractional variable. Observe that a tight set with a fractional variable
has at least two fractional variables. Given a tight set A with fractional variables
i, j, we let yij(ϵ) be the vector obtained by adding ϵ to yi and subtracting ϵ
from yj and leaving the other values unchanged. Let ϵ+
ij(y) = max{ϵ ≥0 |
yij(ϵ) ∈P(M)}. Similarly we let ϵ−
ij(y) = min{ϵ ≤0 | yij(ϵ) ∈P(M)}. We
let y+
ij = yij(ϵ+
ij) and y−
ij = yij(ϵ−
ij). For a given y and i, j ∈N, we deﬁne a
real-valued function F y
ij : [ϵ−
ij(y), ϵ+
ij(y)] →R+ where F y
ij(δ) = F(yij(δ)).
Algorithm PipageRound(y):
While (y is not integral) do
Let A be a minimal tight set containing fractional i, j ∈A
If (F(y+
ij) ≥F(y−
ij))
y ←y+
ij
Else
y ←y−
ij
EndWhile
Output y, f(y).
Lemma 2. The pipage rounding algorithm outputs an integral feasible y in
O(n2) iterations. Given an oracle access to F and a membership oracle for M,
the algorithm can be implemented in polynomial time.
Proof (sketch). Using Proposition 2, we assume that N is tight with respect to
y. Since y+
ij and y−
ij both belong to P(M), the algorithm maintains the invariant
that y ∈P(M) and that N is tight. Thus there is always a tight set with two
fractional variables as long as y is not integral. We observe that the algorithm
does not alter a variable yi once yi ∈{0, 1}. To simplify the algorithm’s analysis
we can alter it slightly so that the set A that is picked in each iteration is not
only minimal but also of minimum cardinality among such minimal sets. Let
y(h) be the vector y at the beginning of iteration h. We claim that y(h + n −1)

188
G. Calinescu et al.
has at least one more integral variable than y(h). This will give us the desired
bound of O(n2) on the total number of iterations.
To prove the claim, let Ah be the tight set picked by the algorithm, and
ih, jh ∈Ah the two fractional variables modiﬁed in iteration h. If one of them
becomes integral in y(h+1), we are done. Otherwise we claim that |Ah+1| < |Ah|,
hence after n−1 iterations we are guaranteed to have one more integral variable.
To see that |Ah+1| < |Ah|, assume wlog that y(h + 1) = y(h)+
ihjh; since ih, jh
are still fractional, there is a new tight set B with respect to y(h + 1), which
prevented us from going further. B contains exactly one of ih, jh, otherwise y(B)
does not change in iteration h. From Proposition 1, it follows that B ∩Ah is also
tight, it contains a fractional variable, and |B∩Ah| < |Ah|. In the next iteration,
we can use Ah+1 = B ∩Ah. To implement an iteration, we need to compute y+
ij,
y−
ij and the new tight set in polynomial time. These can be done by appealing
to known methods [16]. We defer the details to a full version of the paper.
To obtain a guarantee on the quality of the solution, F needs to satisfy some
properties, as suggested in [1].
– F is an extension of f and F(y) ≥α ˜f(y) for all y ∈[0, 1]N.
– F y
ij is convex for all y and i, j.
Given the above two conditions, it is shown in [1] that the pipage rounding
algorithm yields the following: given an optimum fractional solution y∗, the
rounding yields an integral solution z such that F(z) ≥F(y∗). This follows from
the convexity requirement on F y
ij; either F(y+
ij) ≥F(y) or F(y−
ij) ≥F(y) and
the choice of the algorithm ensures that in each iteration the value of F does
not decrease. Therefore we can conclude that f(z) = F(z) ≥F(y∗) ≥α ˜f(y∗).
Since ˜f(y∗) ≥OPT, we have f(z) ≥αOPT.
3
Extensions of Submodular Functions
In this section, we address the issue of extending a monotone submodular func-
tion f : 2N →R+ to continuous functions ˜f, F : [0, 1]N →R+, as required by
the framework.
F as the expected value of f: We consider a simple and natural candidate
for F that is implicitly generated from f. Deﬁne F(y) = E[f(ˆy)] where ˆy is
a random integer vector obtained from y by independently rounding each i to
1 with probability yi and to 0 with probability 1 −yi. In shorthand, we write
F = Ef. We can evaluate F = Ef to any desired accuracy by taking several
independent samples. We defer details that show that a polynomial number of
samples suﬃce to obtain a (1−1/poly(n))-approximation to F(y). Alternatively
we could use a randomized version of the pipage rounding that does not require
us to evalute F explicitly.
In [1], F was given as an explicit function for some simple functions and the
convexity of F y
ij was explicitly shown. A nice feature of F = Ef is that the
convexity requirement is satisﬁed for all submodular f.

Maximizing a Submodular Set Function Subject to a Matroid Constraint
189
Lemma 3. For any submodular f, if F = Ef, then F y
ij is convex for all y ∈
[0, 1]N and i, j ∈N.
Proof. Let F = Ef. For S ⊆N \ {i, j} and y ∈[0, 1]N, let py(S) = 
l∈S yl

l∈N\{i,j}\S(1 −yl) be the probability that S is precisely the set obtained by
randomized rounding on N \ {i, j}. Then
F(y) =

S⊆N\{i,j}
py(S) ((1 −yi)(1 −yj)f(S) + (1 −yi)yjf(S + j))
+yi(1 −yj)f(S + i) + yiyjf(S + i + j)).
We have F y
ij(δ) = F(yij(δ)). Let x = yij(δ), i.e. xi = yi + δ, xj = yj −δ and
xl = yl for all l ∈N\{i, j}. Hence it follows that px(S) = py(S) for S ⊆N\{i, j}.
It can be seen that F(yij(δ)) = F(x) = c2δ2 + c1δ + c0 where c2, c1, c0 do not
depend on δ (they depend only on y and f). Thus to show that F y
ij(δ) is convex
in δ, it is suﬃcient to prove that c2 ≥0. It is easy to check that
c2 =

S⊆N\{i,j}
py(S)(−f(S) + f(S + j) + f(S + i) −f(S + i + j)).
By submodularity, f(S+i)+f(S+j) ≥f((S+i)∩(S+j))+f((S+i)∪(S+j)) =
f(S) + f(S + i + j) which proves that c2 ≥0.
Next, we need an extension ˜f such that max{ ˜f(y) : y ∈P(M)} can be solved
in polynomial time. The approximation guarantee is the largest α such that
F(y) ≥α ˜f(y).
Extension f +: Our ﬁrst candidate for ˜f is an extension similar to the objective
function of the “Conﬁguration LP” [10,7,8].
– f +(y)=max

S⊆N αSf(S) : 
S αS ≤1, αS ≥0 & ∀j; 
S:j∈S αS ≤yj

.
Extension f ∗: Another candidate is a function appearing in [14] and subse-
quently [9,18,19], where it is used indirectly in the analysis of the greedy algo-
rithm for submodular function maximization:
– f ∗(y) = min

f(S) + 
j∈N fS(j)yj : S ⊆N

.
Unfortunately, as the theorem below shows, it is NP-hard to evaluate f +(y)
and f ∗(y) and also to optimize them over matroid polytopes.
Theorem 3. It is NP-hard to compute f +(y) or f ∗(y) for a given y ∈[0, 1]n
and a given monotone submodular function f. Also, there is δ > 0 such that
for a given matroid M it is NP-hard to ﬁnd any point z ∈P(M) such that
f +(z) ≥(1 −δ) max{f +(y) : y ∈P(M)}. Similarly, it is NP-hard to ﬁnd any
point z ∈P(M) such that f ∗(z) ≥(1−δ) max{f ∗(y) : y ∈P(M)}. These results
hold even for coverage-type submodular functions and partition matroids.

190
G. Calinescu et al.
We defer the proof to a full version of the paper; the authors are unaware of prior
work that might have addressed this question. Still, both f +(y) and f ∗(y) will
be useful in our analysis. We remark that for any class of submodular functions
where either f +(y) or f ∗(y) is computable in polynomial time, we obtain a
(1 −1/e)-approximation for our problem.
It is known and easy to see that for y ∈{0, 1}N, both f + and f ∗functions
coincide with f and thus they are indeed extensions of f. For any y ∈[0, 1]N,
we ﬁrst show the following.
Lemma 4. For any monotone submodular f, F(y) ≤f +(y) ≤f ∗(y).
Proof. To see the ﬁrst inequality, let αS = 
i∈S yi

i/∈S(1 −yi) be the proba-
bility that we obtain ˆy = χS by independent rounding of y. Since 
S:j∈S αS =
Pr[ˆyj = 1] = yj, this is a feasible solution for f +(y) and therefore f +(y) ≥

S αSf(S) = E[f(ˆy)] = F(y).
For the second inequality, consider any feasible vector αS and any set T ⊆N:

S
αSf(S) ≤

S
αS
⎛
⎝f(T ) +

j∈S
fT (j)
⎞
⎠≤f(T ) +

j∈N
yjfT (j)
using submodularity and the properties of αS. By taking the maximum on the
left and the minimum on the right, we obtain f +(y) ≤f ∗(y).
It is tempting to conjecture that f +(y) and f ∗(y) are in fact equal, due to some
duality relationship. However, this is not the case: both inequalities in Lemma 4
can be sharp and both gaps can be close to 1 −1/e. For the ﬁrst inequality,
consider the submodular function f(S) = min{|S|, 1} and yj = 1/n for all j;
then F(y) = 1 −(1 −1/n)n and f +(y) = 1. For the second inequality, choose
a large but ﬁxed k, f(S) = 1 −(1 −|S|/n)k and yj = 1/k for all j. The reader
can verify that f +(y) = 1 −(1 −1/k)k, while f ∗(y) ≥1 −k/n →1 as n →∞.
We prove that 1 −1/e is the worst possible gap for both inequalities. Moreover,
even the gap between F(y) and f ∗(y) is bounded by 1 −1/e.
Lemma 5. For any monotone submodular f, F(y) ≥

1 −1
e

f ∗(y).
Proof. For each element j ∈N, set up an independent Poisson clock Cj of rate
yj, i.e. a device which sends signals at random times, in any inﬁnitesimal time
interval of size dt independently with probability yjdt. We deﬁne a random
process which starts with an empty set S(0) = ∅at time t = 0. At any time
when the clock Cj sends a signal, we include element j in S, which increases
its value by fS(j). (If j is already in S, nothing happens; the marginal value
fS(j) is zero in this case.) Denote by S(t) the random set we have at time t.
By the deﬁnition of a Poisson clock, S(1) contains element j independently with
probability 1 −e−yj ≤yj. Since such a set can be obtained as a subset of the
random set deﬁned by ˆy, we have E[f(S(1))] ≤F(y) by monotonicity. We show
that E[f(S(1))] ≥(1 −1/e)f ∗(y) which will prove the claim.
Let t ∈[0, 1]. Condition on S(t) = S and consider how f(S(t)) changes in an
inﬁnitesimal interval [t, t+dt]. The probability that we include element j is yjdt.

Maximizing a Submodular Set Function Subject to a Matroid Constraint
191
Since dt is very small, the events for diﬀerent elements j are eﬀectively disjoint.
Thus the expected increase of f(S(t)) is (up to O(dt2) terms)
E[f(S(t + dt)) −f(S(t)) | S(t) = S] =

j∈N
fS(j)yjdt ≥(f ∗(y) −f(S))dt
using the deﬁnition of f ∗(y). We divide by dt and take the expectation over S:
1
dtE[f(S(t + dt)) −f(S(t))] ≥f ∗(y) −E[f(S(t))].
We deﬁne φ(t) = E[f(S(t))], i.e. dφ
dt ≥f ∗(y) −φ(t). We solve this diﬀerential
inequality by considering ψ(t) = etφ(t) and dψ
dt = et( dφ
dt + φ(t)) ≥etf ∗(y). Since
ψ(0) = φ(0) = 0, this implies
ψ(x) =
 x
0
dψ
dt dt ≥
 x
0
etf ∗(y)dt = (ex −1)f ∗(y)
for any x ≥0. We conclude that E[f(S(t))] = φ(t) = e−tψ(t) ≥(1 −e−t)f ∗(y)
and F(y) ≥E[f(S(1))] ≥(1 −1/e)f ∗(y).
We remark that we did not actually use submodularity in the proof of Lemma 5!
Formally, it can be stated for all monotone functions f. However, f ∗(y) is not a
proper extension of f when f is not submodular (e.g., f ∗(y) is identically zero
if f(S) = 0 for |S| ≤1). So the statement of Lemma 5 is not very meaningful in
this generality.
To summarize what we have proved so far, we have two relaxations of our
problem:
– max{f +(y) : y ∈P(M)}
– max{f ∗(y) : y ∈P(M)}
Our framework together with Lemma 4 and Lemma 5 implies that both of these
relaxations have integrality gap at most 1 −1/e. Theorem 3 shows NP-hardness
of solving the relaxations. We show how to use the framework eﬃciently in a
restricted case of interest which is described in the following section.
4
Sums of Weighted Rank Functions
We achieve a (1 −1/e)-approximation, under a matroid constraint M, for any
submodular function f that can be expressed as a sum of “weighted rank func-
tions” of matroids. This is the most general subclass of submodular functions
for which we are able to use the framework outlined in Section 2 in an eﬃcient
way. Here we describe this in detail.
Weighted rank functions of matroids: Given a matroid (N, X) and a weight
function w : N →R+, we deﬁne a weighted rank function g : 2N →R+,
g(S) = max{

j∈I
wj : I ⊆S & I ∈X}.

192
G. Calinescu et al.
It is well known that such a function is monotone and submodular. A simple
special case is when X = {I | |I| = 1}. Then g(S) returns simply the maximum-
weight element of S; this will be useful in our application to GAP.
Sums of weighted rank functions: We consider functions f : 2N →R+
of the form f(S) = m
i=1 gi(S) where each gi is a weighted rank function for
matroid (N, Xi) with weights wij. Again, f(S) is monotone and submodular.
The functions that can be generated in this way form a fairly rich subclass
of monotone submodular functions. In particular, they generalize submodu-
lar functions arising from coverage systems. Coverage-type submodular func-
tions can be obtained by considering a simple uniform matroid (N, X) with
X = {I ⊆N | |I| ≤1}. For a collection of sets {Aj}j∈N on a ground set
[m], we can deﬁne m collections of weights on N, where wij = 1 if Aj con-
tains element i, and 0 otherwise. Then the weighted rank function gi(S) =
max{wij : j ∈S} is simply an indicator of whether 
j∈S Aj covers element
i. The sum of the rank functions gi(S) gives exactly the size of this union
f(S) = m
i=1 gi(S) =

j∈S Aj
. Generalization to the weighted case is straight-
forward.
LP formulation for sums of weighted rank functions: For a submodular
function given as f(S) = m
i=1 gi(S) where gi(S) = max{wi(I) : I ⊆S, I ∈Xi},
consider an extension g+
i (y) for each gi, as deﬁned in Section 3:
g+
i (y) = max{

S⊆N
αSgi(S) :

S
αS ≤1, αS ≥0 & ∀j;

S:j∈S
αS ≤yj}.
Here, we can assume without loss of generality that αS is nonzero only for S ∈Xi
(otherwise replace each S by a subset I ⊆S, I ∈Xi, such that gi(S) = wi(I)).
Therefore, g+
i can be written as
g+
i (y) = max{

I∈Xi
αI

j∈I
wij :

I∈Xi
αI ≤1, αI ≥0 & ∀j;

I∈Xi:j∈I
αI ≤yj}.
We can set xij = 
I∈Xi:j∈I αI and observe that a vector xi = (xij)j∈N can
be obtained in this way if and only if it is a convex linear combination of
independent sets; i.e., if it is in the matroid polytope P(Xi). The objective
function becomes 
j∈N wij

I∈Xi:j∈I αI = 
j∈N wijxij and so we can write
equivalently
g+
i (y) = max{

j∈N
wijxij : xi ∈P(Xi) & ∀j; xij ≤yj}.
We sum up these functions to obtain an extension ˜f(y) = m
i=1 g+
i (y). This
leads to the following LP formulation for the problem max{ ˜f(y) : y ∈P(M)}:

Maximizing a Submodular Set Function Subject to a Matroid Constraint
193
max
m

i=1

j∈N
wijxij;
∀i, j; xij ≤yj,
∀i; xi ∈P(Xi),
y ∈P(M).
We can solve the LP using the ellipsoid
method, since a separation oracle can be ef-
ﬁciently implemented for each matroid poly-
tope, and therefore also for this LP. To obtain
a (1−1/e)-approximation (Theorem 1) via the
above LP using the pipage rounding frame-
work from Section 2, it is suﬃcient to prove
the following lemma.
Lemma 6. For any sum of weighted rank functions f, F(y) ≥(1 −1/e) ˜f(y).
Proof. By Lemma 5, F(y) ≥(1 −1/e)f ∗(y) and hence it suﬃces to prove that
f ∗(y) ≥˜f(y). By Lemma 4, g+
i (y) ≤g∗
i (y) where g∗
i (y) = minSi(gi(Si) +

j yjgi,Si(j)). (Here, gi,Si(j) = gi(Si + j) −gi(Si).) Consequently,
˜f(y) =
m

i=1
g+
i (y) ≤
m

i=1
min
Si (gi(Si) +

j∈N
yjgi,Si(j))
≤min
S
m

i=1
(gi(S) +

j∈N
yjgi,S(j)) = min
S (f(S) +

j∈N
yjfS(j)) = f ∗(y).
5
The Generalized Assignment Problem
Here we consider an application of our techniques to the Generalized Assignment
Problem (“GAP”). An instance of GAP consists of n bins and m items. Each
item i has two non-negative numbers for each bin j; a value vji and a size sji.
We seek an assignment of items to bins such that the total size of items in each
bin is at most 1, and the total value of all items is maximized.
In [10], a (1−1/e)-approximation algorithm for GAP has been presented. The
algorithm uses LP1.
LP1 :
max

j,S∈Fj
yj,Svj(S);
∀j;

S∈Fj
yj,S ≤1,
∀i;

j,S∈Fj:i∈S
yj,S ≤1,
∀j, S; yj,S ≥0.
In LP1, Fj denotes the collection of all
feasible assignments for bin j, i.e. sets
satisfying 
i∈S sji ≤1. The variable yj,S
represents bin j receiving a set of items S.
Although this is an LP of exponential size,
it is shown in [10] that it can be solved to
an arbitrary precision in polynomial time.
Then the fractional solution can be rounded
to an integral one to obtain a (1 −1/e)
approximation.
We show in this section that this (1 −1/e)-approximation algorithm can be
interpreted as a special case of submodular maximization subject to a matroid

194
G. Calinescu et al.
constraint2, and this framework also allows some generalizations of GAP3. For
this purpose, we reformulate the problem as follows.
We deﬁne N = {(j, S) | 1 ≤j ≤n, S ∈Fj} and a submodular function
f : 2N →R+,
f(S) =
m

i=1
max{vji : ∃(j, S) ∈S, i ∈S}.
We maximize this function subject to a matroid constraint M, where S ∈M
iﬀS contains at most one pair (j, S) for each j. Such a set S corresponds to
an assignment of set S to bin j for each (j, S) ∈S. This is equivalent to GAP:
although the bins can be assigned overlapping sets in this formulation, we only
count the value of the most valuable assignment for each item. We can write
f(S) = m
i=1 gi(S) where gi(S) = max{vji : ∃(j, S) ∈S, i ∈S} is a weighted
rank function of a matroid Xi on N. In the matroid Xi an element (j, S) ∈N has
weight vji if i ∈S and 0 otherwise. A set is independent in Xi iﬀits cardinality
is at most 1. Therefore the problem falls under the umbrella of our framework.
We now write explicitly the LP arising from interpreting GAP as a submodular
function problem. We have variables yj,S for each j and S ∈Fj. In addition, for
each matroid Xi, we deﬁne copies of these variables xi,j,S. The resulting linear
program is given as LP2.
LP2 :
max

j,S∈Fj,i∈S
vjixi,j,S;
∀i, j, S; xi,j,S ≤yj,S,
∀i; xi ∈P(Xi),
y ∈P(M).
LP2 has exponentially many variables
and exponentially many constraints. How-
ever, observe that a feasible solution yj,S
for LP1 is also feasible for LP2, when we set
xi,j,S = yj,S for i ∈S and 0 otherwise. This
is because the constraint 
j,S:i∈S yj,S ≤1
in LP1 implies xi ∈P(Xi), and the con-
straint 
S yj,S ≤1 implies y ∈P(M).
Therefore, we can solve LP1 using the techniques of [10] and then convert the
result into a feasible solution of LP2. Finally, we can apply the pipage rounding
technique to obtain a (1 −1/e)-approximation.
This is simply a reformulation of the algorithm from [10]. However, the ﬂex-
ibility of our framework allows a more complicated matroid constraint M than
each bin choosing at most one set. We brieﬂy discuss this below.
Laminar matroid constraints on the bins: Let B be the set of bins in a
GAP instance. Consider a laminar matroid M on B. We consider the problem
of assigning items to a subset of bins B′ ⊆B such that B′ is independent in M.
An example is when M is the simple uniform matroid; that is B′ is independent
iﬀ|B′| ≤k. This gives rise to a variant of GAP in which at most k of the n bins
2 This formulation of GAP is also described in [10] as a personal communication from
an author of this paper.
3 In [10] more general allocation problems are considered that allow constraints on the
sets of items packable within a bin. Our approach also works for such problems but
in this extended abstract we limit our discussion to GAP.

Maximizing a Submodular Set Function Subject to a Matroid Constraint
195
can be used. One can modify LP1 by adding a new constraint: 
j,S∈Fj yj,S ≤k,
to obtain a relaxation LP3 for this new problem.
LP3 : max

j,S∈Fj
yj,Svj(S);
∀j;

S∈Fj
yj,S ≤1,
∀i;

j,S∈Fj:i∈S
yj,S ≤1,

j,S∈Fj
yj,S ≤k,
∀j, S; yj,S ≥0.
Using the same ideas as those in [10],
one
can
solve
LP3
to
an
arbitrary
precision in polynomial time. The simple
rounding scheme of [10] for LP1 does not
apply to LP3. However, as before, we can
see that a solution to LP3 is feasible for LP2
where the matroid M now also enforces
the additional constraint that at most k
elements from N are chosen. Thus pipage
rounding can be used to obtain a (1−1/e)-
approximation. A similar reasoning allows
us to obtain a (1 −1/e)-approximation for
any laminar matroid constraint on the bins
B. We defer the details to a full version of the paper.
6
Conclusions
We obtained a (1 −1/e)-approximation for an interesting and useful class of
submodular functions. We note that the methods in the paper apply to some
interesting submodular functions that are not in the class. An example is the
maximum multiset multicover problem which generalizes the multicover problem
deﬁned in Section 1. The diﬀerence between multicover and multiset multicover
is that a set can cover an element multiple times (at most the requirement of the
element). We can obtain a (1−1/e) approximation for this problem even though
this function cannot be expressed as a weighted sum of matroid rank functions.
We defer the details. It would be of much interest to prove or disprove the
existence of a (1 −1/e)-approximation for all monotone submodular functions.
Note that our hardness results (Theorem 3) hold even when f can be expressed
as a sum of weighted rank functions of matroids, yet we can obtain a (1 −1/e)-
approximation in this case.
The unconstrained problem maxS⊆N f(S) is NP-hard and hard to approxi-
mate if f is a non-monotone submodular set function; the Max-Cut problem is a
special case. However, the pipage rounding framework is still applicable to non-
monotone functions (as already shown in [1]). For non-monotone functions, the
problem we need to consider is maxS∈B f(S) where B is the set of bases of M. It
is easy to see that Lemma 2 and Lemma 3 still apply. Thus, the approximation
ratio that can be guaranteed depends on the extension ˜f.
Pipage rounding [1] and dependent randomized rounding [17,15] are based on
rounding fractional solutions to the assignment problem into integer solutions
while maintaining the quality of a solution that is a function of the variables on
the edges of the underlying bipartite graph. A number of applications are given
in [1,17,15]. This paper shows that submodularity and uncrossing properties of
solutions to matroids and other related structures are the basic ingredients in

196
G. Calinescu et al.
the applicability of the pipage rounding technique. We hope this insight will lead
to more applications in the future.
References
1. A. Ageev and M. Sviridenko.
Pipage rounding: a new method of constructing
algorithms with proven performance guarantee. J. of Combinatorial Optimization,
8:307–328, 2004.
2. C. Chekuri and A. Kumar. Maximum coverage problem with group budget con-
straints and applications. Proc. of APPROX, Springer LNCS, 72–83, 2004.
3. C. Chekuri and M. P´al. A recursive greedy algorithm for walks in directed graphs.
Proc. of IEEE FOCS, 2005.
4. C. Chekuri and S. Khanna. A PTAS for the multiple knapsack problem. SIAM J.
on Computing, 35(3):713–728, 2004.
5. G. Cornuejols, M. Fisher and G. Nemhauser. Location of bank accounts to optimize
ﬂoat: an analytic study of exact and approximate algorithms. Management Science,
23: 789–810, 1977.
6. U. Feige. A threshold of ln n for approximating set cover. JACM, 45(4):634–652,
1998
7. U. Feige. On maximizing welfare when utility functions are subadditive. Proc. of
ACM STOC, 41–50, 2006.
8. U. Feige and J. Vondr´ak. Approximation algorithms for allocation problems: Im-
proving the Factor of 1 −1/e. Proc. of IEEE FOCS, 667–676, 2006.
9. M. L. Fisher, G. L. Nemhauser and L. A. Wolsey. An analysis of approximations
for maximizing submodular set functions - II. Math. Prog. Study, 8:73–87, 1978.
10. L. Fleischer, M.X. Goemans, V.S. Mirrokni and M. Sviridenko. Tight approxima-
tion algorithms for maximum general assignment problems. Proc. of ACM-SIAM
SODA, 611–620, 2006.
11. E. Hazan, S. Safra and O. Schwartz. On the complexity of approximating k-set
packing. Proc. of APPROX, 2003.
12. T. A. Jenkyns. The eﬃciency of the “greedy” algorithm. Proc. of 7th South Eastern
Conference on Combinatorics, Graph Theory and Computing, 341–350, 1976.
13. B. Korte and D. Hausmann. An analysis of the greedy heuristic for independence
systems. Annals of Discrete Math., 2:65–74, 1978.
14. G. L. Nemhauser, L. A. Wolsey and M. L. Fisher. An analysis of approximations
for maximizing submodular set functions - I. Math. Prog., 14:265–294, 1978.
15. R. Gandhi, S. Khuller, S. Parthasarathy and A. Srinivasan. Dependent rounding
and its applications to approximation algorithms. JACM, 53(3):324–360, 2006.
16. A. Schrijver.
Combinatorial optimization - polyhedra and eﬃciency.
Springer,
2003.
17. A. Srinivasan. Distributions on level-sets with applications to approximation algo-
rithms. Proc. of IEEE FOCS, 588–597, 2001.
18. L. Wolsey. An analysis of the greedy algorithm for the submodular set covering
problem. Combinatorica, 2:385–393, 1982.
19. L. Wolsey. Maximizing real-valued submodular functions: Primal and dual heuris-
tics for location Problems. Math. of Operations Research, 7:410–425, 1982.

On a Generalization of the Master Cyclic Group
Polyhedron
Sanjeeb Dash1, Ricardo Fukasawa2,⋆, and Oktay G¨unl¨uk1
1 Mathematical Sciences Department
IBM T. J. Watson Research Center, Yorktown Heights, NY, 10598
sanjeebd@us.ibm.com, oktay@watson.ibm.com
2 H. Milton Stewart School of Industrial and Systems Engineering
Georgia Institute of Technology, Atlanta, GA, 30332
rfukasaw@isye.gatech.edu
Abstract. We study the Master Equality Polyhedron (MEP) which gen-
eralizes the Master Cyclic Group Polyhedron and the Master Knapsack
Polyhedron.
We present an explicit characterization of the nontrivial facet-deﬁning
inequalities for MEP. This result generalizes similar results for the Master
Cyclic Group Polyhedron by Gomory [9] and for the Master Knapsack
Polyhedron by Araoz [1]. Furthermore, this characterization also gives
a polynomial time algorithm for separating an arbitrary point from the
MEP.
We describe how facet deﬁning inequalities for the Master Cyclic
Group Polyhedron can be lifted to obtain facet deﬁning inequalities for
the MEP, and also present facet deﬁning inequalities for the MEP that
cannot be obtained in such a way. Finally, we study the mixed-integer ex-
tension of the MEP and present an interpolation theorem that produces
valid inequalities for general Mixed Integer Programming Problems us-
ing facets of the MEP.
Keywords: integer programming, polyhedral combinatorics.
1
Introduction
We study the Master Equality Polyhedron (MEP), which we deﬁne as:
K(n, r) = conv

(x, y) ∈Zn
+ × Zn
+ :
n

i=1
ixi −
n

i=1
iyi = r

(1)
where n, r ∈Z and n > 0. Without loss of generality we assume that r ≥0. To
the best of our knowledge, K(n, r) was ﬁrst deﬁned by Uchoa [14] in a slightly
diﬀerent form and described as an important object for study.
⋆Work developed while at IBM Research.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 197–209, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

198
S. Dash, R. Fukasawa, and O. G¨unl¨uk
As lower dimensional faces, MEP contains two well known polyhedra from the
literature: The Master Cyclic Group Polyhedron (MCGP), which is deﬁned as
P(n, r) = conv

(x, y) ∈Zn−1
+
× Z+ :
n−1

i=1
ixi −nyn = r

,
(2)
where r, n ∈Z, and 0 ≤r < n; and the Master Knapsack Polyhedron (MKP),
which is deﬁned as
K(r) = conv

x ∈Zr
+ :
r

i=1
ixi = r

,
(3)
where r ∈Z and r > 0.
Facets of P(n, r) are a useful source of cutting planes for general MIPs. The
Gomory mixed-integer cut (also known as the mixed-integer rounding (MIR) in-
equality) can be derived from a facet of P(n, r) [10]. Other facets and
studies related to the Master Cyclic Group Polyhedron can be found in
[2,4,5,6,7,8,11,12,13]. In particular, several relationships between facet-deﬁning
inequalities of the MCGP and facet-deﬁning inequalities of the MKP were es-
tablished in [2]. We note that the Master Cyclic Group Polyhedron is usually
presented as
P ′(n, r) = conv

x ∈Zn−1
+
:
n−1

i=1
ixi ≡r
mod n

which is the projection of P(n, r) in the space of x variables. We use (2) as it
makes the comparison to K(n, r) easier and clearer.
Gomory [9] and Araoz [1] give an explicit characterization of the polar of the
nontrivial facets of P(n, r) and K(r). In this paper, we give a similar description
of the nontrivial facets of K(n, r), yielding as a consequence a polynomial time
algorithm to separate over it. We also analyze some structural properties of the
MEP and relate it to the MCGP.
In addition, we describe how to obtain valid inequalities for general MIPs
using facet deﬁning inequalities for the MEP.
Finally, we remark that another motivation to study the MEP is that it also
arises as a natural structure in a reformulation of the Fixed-Charge Network
Flow problem, which has recently been used in [15] to derive strong cuts for the
Capacitated Minimum Spanning Tree Problem and can also be used in other
problems such as the Capacitated Vehicle Routing Problem.
2
Polyhedral Analysis of K(n, r)
From this point until the end of section 2.1 we assume 0 < r ≤n. In
subsections 2.2 and 2.3, we consider the cases r = 0 and r > n. We start with
some basic polyhedral properties of K(n, r).

On a Generalization of the Master Cyclic Group Polyhedron
199
Lemma 1.
dim(K(n, r)) = 2n −1.
Lemma 2. The nonnegativity constraints of K(n, r) are facet-deﬁning if n ≥2.
Let ei denote the unit vector with a one in the component corresponding to vari-
able xi and fi denote the unit vector with a one in the component corresponding
to variable yi.
Clearly, K(n, r) is an unbounded polyhedron. We next characterize all the
extreme rays (unbounded one-dimensional faces) of K(n, r). We represent an
extreme ray {u + λv : u, v ∈R2n
+ , λ ≥0} of K(n, r) simply by the vector v. Let
rij = jei + ifj for any i, j ∈{1, . . . , n}.
Lemma 3. The set of extreme rays of K(n, r) is given by R={rij : 1 ≤i, j ≤n}.
As K(n, r) is not a full-dimensional polyhedron, any valid inequality πx+ρy ≥πo
for K(n, r) has an equivalent representation with ρn = 0. If a valid inequality
does not satisfy this condition, one can add an appropriate multiple of the equa-
tion n
i=1 ixi −n
i=1 iyi = r to it. Therefore, without loss of generality, we may
assume that all valid inequalities for K(n, r) satisfy ρn = 0.
We classify the facets of K(n, r) as trivial and non-trivial facets.
Deﬁnition 1. The following facet-deﬁning inequalities of K(n, r) are called
trivial:
xi ≥0, ∀i = 1, . . . , n
yi ≥0, ∀i = 1, . . . , n −1
All other facet-deﬁning inequalities of K(n, r) are called nontrivial.
Notice that we left inequality yn ≥0 out of the trivial set. That happens just
because of technical details to simplify the statement of our theorems and lem-
mas. In fact there is nothing particularly special about the yn ≥0 inequality
other than it is the only nonnegativity constraint that does not comply directly
with the ρn = 0 assumption.
Let N = {1, . . ., n}. We next state our main result:
Theorem 1. The inequality πx + ρy ≥πo deﬁnes a nontrivial facet of K(n, r)
if and only if it can be represented as an extreme point of T ⊆R2n+1 where T
is deﬁned by the following linear equations and inequalities:
πi + ρj ≥πi−j,
∀i, j ∈N,
i > j,
(F1)
πi + πj ≥πi+j,
∀i, j ∈N,
i + j ≤n,
(F2)
ρk + πi + πj ≥πi+j−k,
∀i, j, k ∈N,
1 ≤i + j −k ≤n,
(F3)
πi + πr−i = πo,
∀i ∈N,
i < r,
(EP1)
πr = πo,
(EP2)
πi + ρi−r = πo,
∀i ∈N
i > r,
(EP3)
ρn = 0,
(N1)
πo = 1.
(N2)

200
S. Dash, R. Fukasawa, and O. G¨unl¨uk
This theorem implies that for 0 < r ≤n, the separation problem over K(n, r)
can be solved in polynomial time. Although the restriction that 0 < r ≤n might
seem undesirable, later in Sect. 2.3 we show that the separation can be done for
every value of r.
Note that the deﬁnition of T in Theorem 1 is similar to that of a polar of
K(n, r). However, T is not a polar, as it does not contain extreme points of the
polar that correspond to the trivial facet-deﬁning inequalities. In addition, some
of the extreme rays of the polar are not present in T . It is possible to interpret
T as an important subset of the polar that contains all extreme points of the
polar besides the ones that lead to the trivial inequalities.
2.1
Facet Characterization
In this section we develop the required analysis to prove Theorem 1. We start by
noting some necessary conditions for validity, which arise by looking at points
and rays of K(n, r):
Observation 2. Let πx + ρy ≥πo be a valid inequality for K(n, r), then the
following holds:
jπi + iρj ≥0, ∀i, j ∈N
(R1)
πi + πr−i ≥πo, ∀1 ≤i < r
(P1)
πr ≥πo
(P2)
πi + ρi−r ≥πo, ∀r < i ≤n
(P3)
Note that (R1) is obtained by considering the extreme rays of K(n, r) and
(P1)-(P3) are obtained by considering the following feasible points of K(n, r):
{ei + er−i, ∀1 ≤i < r} ∪er ∪{ei + fi−r, ∀r < i ≤n}
We call these points the Elementary points of K(n, r). Note that there are
n −
 r−1
2

Elementary points.
We next present some conditions satisﬁed by all nontrivial facet deﬁning in-
equalities.
Lemma 4. Let πx+ρy ≥πo be a nontrivial facet-deﬁning inequality of K(n, r),
then it satisﬁes (F1)-(F3) as well as (EP1)-(EP3).
Proof. (F1): Pick a point (x∗, y∗) tight at πx + ρy ≥πo such that x∗
i−j > 0.
Note that (x∗, y∗) + (ei + fj −ei−j) is a point of K(n, r). Thus, (F1) holds.
The proofs of (F2) and (F3) are analogous.
(EP1): Pick points (x′, y′) and (x′′, y′′) tight at (π, ρ, πo) such that x′
i > 0
and x′′
r−i > 0. Then (x′′′, y′′′) = (x′, y′) + (x′′, y′′) −ei −er−i ∈K(n, r), thus
(π, ρ)T (x′′′, y′′′) = (π, ρ)T (x′, y′)+(π, ρ)T (x′′, y′′)−πi −πr−i = 2πo−πi −πr−i ≥
πo ⇒πi + πr−i ≤πo. So (P1) ⇒(EP1).

On a Generalization of the Master Cyclic Group Polyhedron
201
Proofs of (EP2) and (EP3) are analogous, using (P2) and (P3) instead of
(P1).
It is worth mentioning that conditions (EP1)-(EP3) imply that all nontrivial
facets intersect at a nonempty lower dimensional face of K(n, r). Note that
all Elementary Points of K(n, r) are in this lower-dimensional face, which has
therefore dimension at least n −
 r−1
2

−1.
In the following Lemma we show that a subset of the conditions presented in
Theorem 1 suﬃces to ensure the validity.
Lemma 5. Let (π, ρ, πo) satisfy (EP2), (F1), (F2) and (F3). Then πx+ρy ≥πo
deﬁnes a valid inequality for K(n, r).
Proof. We will prove this by contradiction. Assume that πx + ρy ≥πo satisﬁes
(EP2), (F1), (F2) and (F3) but πx + ρy ≥πo does not deﬁne a valid inequality
for K(n, r), r > 0. Let (x∗, y∗) be an integer point in K(n, r) that has minimum
L1 norm amongst all points violated by πx + ρy ≥πo. Note that since r > 0,
then x∗̸= 0.
If ||(x∗, y∗)||1 = 0 then (x∗, y∗) = 0 ̸∈K(n, r). If ||(x∗, y∗)||1 = 1 then clearly
x∗= er and y∗= 0 but as πr = πo, (x∗, y∗) does not violate the inequality.
Therefore ||(x∗, y∗)||1 ≥2. We next consider three cases.
Case 1: Assume that y∗= 0.
In this case, n
i=1 ix∗
i = r. By successively
applying (F2), we obtain
πo >
n

i=1
πix∗
i ≥
n

i=1
πix∗
i ≥π n
i=1 ix∗
i = πr
which contradicts (EP2). Therefore y∗̸= 0.
Case 2: Assume that x∗
i > 0 and y∗
j > 0 for some i > j.
Let (x′, y′) =
(x∗, y∗)+(ei−j−ei−fj). Note that (x′, y′) ∈K(n, r), and ||(x′, y′)||1 =||(x∗, y∗)||1
−1. Moreover, since πx + ρy ≥πo satisﬁes (F1), πx′ + ρy′ = πx∗+ ρy∗+ πi−j −
πi −ρj ≤πx∗+ ρy∗< πo, which contradicts the choice of (x∗, y∗). Therefore
i ≤j whenever x∗
i > 0 and y∗
j > 0.
Case 3:
Assume that for any i, j ∈N, if x∗
i > 0 and y∗
j > 0, then
i ≤j.
Suppose there exists i, j ∈N such that x∗
i > 0, x∗
j > 0 or x∗
i ≥2
(in which case, we let j = i). If i + j ≤n, let (x′, y′) = (x∗, y∗) + (ei+j −ei −ej).
If i + j > n, since y∗̸= 0 there exists k such that y∗
k > 0 and k ≥i, thus
i + j −k ≤n. So let (x′, y′) = (x∗, y∗) + (ei+j−k −ei −ej −fk).
Note that in either case (x′, y′) ∈K(n, r) and ||(x′, y′)||1 < ||(x∗, y∗)||1. More-
over, since (π, ρ, πo) satisfy (F2) and (F3), in either case πx′ +ρy′ ≤πx∗+ρy∗<
πo, which contradicts the choice of (x∗, y∗).
One condition that so far has not been mentioned is (N2), which is a nor-
malization condition like (N1). The following Lemma states that we are not
eliminating any nontrivial facets by making such an assumption.
Lemma 6. Let πx+ρy ≥πo be a nontrivial facet-deﬁning inequality of K(n, r),
that satisﬁes ρn = 0. Then πo > 0.

202
S. Dash, R. Fukasawa, and O. G¨unl¨uk
Combining Lemmas 4-6 with some more technical observations it is possible to
prove Theorem 1. As a corollary of the theorem, we also make the following
observation:
Observation 3. Let (π, ρ, πo) be an extreme point of T , then for all k ∈N:
⌈k/r⌉≥πk ≥0
⌈n/r⌉≥ρk ≥−⌈k/r⌉
As a ﬁnal remark, it is interesting to note that conditions (R1) do not appear
in the description of T even though they are necessary for any valid inequality.
This happens because conditions (R1) are implied by (F1), (F2) and (F3). We
formally state this fact in the next observation:
Observation 4. Let (π, ρ, πo) ∈T . Then:
jπi + iρj ≥0,
∀1 ≤i, j ≤n
2.2
Facets of K(n, 0)
Observe that LK(n, 0), the linear relaxation of K(n, 0), is a cone and is
pointed (as it is contained in the nonnegative orthant) and has a single ex-
treme point (x, y) = (0, 0). Therefore LK(n, 0) equals its integer hull, i.e.,
LK(n, 0)=K(n, 0). In Lemma 3, we characterized the extreme rays of K(n, r)
and thereby showed that the characteristic cone of K(n, r) is generated by
the vectors {rij}. But the characteristic cone of K(n, r) for some r > 0 is
just K(n, 0). Therefore, LK(n, 0) is generated by the vectors {rij}, and the
next result follows.
Theorem 5. The inequality πx + ρy ≥πo is facet deﬁning for K(n, 0) if and
only if (π, ρ, πo) is a minimal face of
To =
jπi + iρj ≥0 , ∀i, j ∈N,
πo = 0.
In his work on the MCGP, Gomory also studied the convex hull of non-zero
integral solutions in P(n, 0) and gave a dual characterization of its facets. We
now consider a similar modiﬁcation of K(n, 0) and study the set:
¯K(n, 0) = conv

(x, y) ∈Zn
+ × Zn
+ :
n

i=1
ixi −
n

i=1
iyi = 0, (x, y) ̸= 0

By an analysis similar to the case where r > 0, it is possible to prove the following
theorem:

On a Generalization of the Master Cyclic Group Polyhedron
203
Theorem 6. The inequality πx + ρy ≥πo deﬁnes a nontrivial facet of ¯K(n, 0)
if and only if it can be represented as an extreme point of ¯To, where ¯To is deﬁned
by the following linear equations and inequalities:
πi + ρj ≥πi−j,
∀i, j ∈N,
i > j,
(F1)
πi + ρj ≥ρj−i,
∀i, j ∈N,
i < j,
(F1’)
πi + ρi = πo,
∀i ∈N,
(EP1-R0)
πo = 1,
(N1-R0)
ρn = 0.
(N2-R0)
2.3
Separating over K(n, r)
We ﬁnish this section by presenting the following theorem stating that sepa-
ration over K(n, r) can be done in polynomial time when r is bounded by a
polynomial function of n and pseudo-polynomial time otherwise. This theorem
is an immediate consequence of Theorems 1 and 5.
Theorem 7. Given (x∗, y∗) ∈Rn × Rn, the problem of separating (x∗, y∗) from
K(n, r) can be solved in time polynomial in max{n, r}.
Proof. If 0 < r ≤n, the separation problem can be solved in time polynomial
in n by ﬁrst checking if (x∗, y∗) violates any nonnegativity constraint or the
constraint n
i=1 ixi −n
i=1 iyi = r and if not, solve:
min{(π, ρ, πo)T (x∗, y∗, 0) : (π, ρ, πo) ∈T }
If there exists (π, ρ, πo) ∈T such that (π, ρ)T (x∗, y∗) < 1, then πx + ρy ≥πo
deﬁnes a hyperplane that separates (x∗, y∗) from K(n, r). Otherwise, (x∗, y∗)
is in the same aﬃne subspace as K(n, r) and satisﬁes all nontrivial and trivial
facets of K(n, r), thus (x∗, y∗) ∈K(n, r).
If r > n, then deﬁne (x′, y′) ∈Rr × Rr such that x′
i = x∗
i ; y′
i = y∗
i , ∀1 ≤i ≤n
and x′
i = y′
i = 0, ∀n < i ≤r. and note that (x′, y′) ∈K(r, r) ⇐⇒(x∗, y∗) ∈
K(n, r), so the separation can be done in time polynomial in r.
In the case where r = 0, we can solve min{(π, ρ)T (x∗, y∗) : (π, ρ) ∈To} and
we’ll know (x∗, y∗) ∈K(n, 0) if and only if the optimum is 0. Otherwise, the
problem is unbounded, in which case the ray which proves unboundedness gives
us a valid inequality separating (x∗, y∗) from K(n, 0).
3
Lifting Facets of P (n, r)
Lifting is a general principle for constructing valid (facet deﬁning) inequalities
for higher dimensional sets using valid (facet deﬁning) inequalities for lower
dimensional sets. Starting with the early work of Gomory [9], this approach was
generalized by Wolsey [16], Balas and Zemel [3] and Gu et. al [17], among others.
In this section we discuss how facets of P(n, r) can be lifted to obtain facets
of K(n, r). P(n, r) can also be considered as an n−1 dimensional face of K(n, r)

204
S. Dash, R. Fukasawa, and O. G¨unl¨uk
obtained by setting n variables to their lower bounds. Throughout this section
we assume that n > r > 0.
We start with a result of Gomory [9] that gives a complete characterization of
the nontrivial facets (i.e., excluding the non-negativity inequalities) of P(n, r).
Theorem 8 (Gomory [9]). Inequality ¯πx ≥1 deﬁnes a non-trivial facet of
P(n, r) if and only if ¯π ∈Rn−1 is an extreme point of
Q =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
πi + πj ≥π(i+j) mod n
∀i, j ∈{1, . . . , n −1},
πi + πj = πr
∀i, j such that r = (i + j) mod n,
πj
≥0
∀j ∈{1, . . . , n −1},
πr
= 1.
Given a non-trivial facet deﬁning inequality for P(n, r)
n−1

i=1
¯πixi ≥1
(4)
it is possible to lift this inequality to obtain a facet-deﬁning inequality
n−1

i=1
¯πixi + π′
nxn +
n−1

i=1
ρ′
iyi ≥1
(5)
for K(n, r). We call inequality (5) a lifted inequality and note that in general
for a given starting inequality there might be an exponential number of lifted
inequalities, see [16].
3.1
The Restricted Coeﬃcient Polyhedron T ¯π
First note that a non-trivial facet of P(n, r) can only yield a non-trivial facet
of K(n, r). This, in turn, implies that (¯π, π′
n, ρ′, 0) has to be an extreme point
of the coeﬃcient polyhedron T . Therefore, the lifting procedure can also be
seen as a way of extending an extreme point of Q to obtain an extreme point
of T .
Let p = (¯π, π′
n, ρ′, 0) be an an extreme point of T . Then, p also has to be an
extreme point of the lower dimensional polyhedron
T ¯π = T ∩

πi = ¯πi, ∀i ∈{1, . . . , n −1}

obtained by ﬁxing some of the coordinates.
Let L = {n −r + 1, . . . , n −1}.
Lemma 7. If inequality (4) deﬁnes a non-trivial facet of P(n, r), then T ¯π ̸= ∅
and it has the form

On a Generalization of the Master Cyclic Group Polyhedron
205
T ¯π =
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
τ
≥
πn ≥0
ρk
≥lk
∀k ∈L
ρk + πn
≥tk
∀k ∈L
ρk −πn
≥fk
∀k ∈L
πn + ρn−r = 1
ρn
= 0
ρk
= ¯πn−k ∀k ∈{1, . . ., n −r −1}
πi
= ¯πi
∀i ∈{1, . . . , n −1}
where numbers lk, tk, fk and τ can be computed easily using ¯π.
We next make a simple observation that will help us show that T ¯π has a small
(polynomial) number of extreme points.
Lemma 8. If p = (¯π, π′
n, ρ′, 0) is an extreme point of T ¯π, then
ρ′
k = max

lk, tk −π′
n, fk + π′
n

for all k ∈L.
We next characterize the set possible values π′
n can take at an extreme point
of T ¯π.
Lemma 9. Let p = (¯π, π′
n, ρ′, 0) be an extreme point of T ¯π, if π′
n ̸∈{0, τ}, then
π′
n ∈Λ =
 
k∈L1

tk −lk, lk −fk
   
k∈L2

(tk −fk)/2

where L1 = {k ∈L : tk + fk < 2lk} and L2 = L \ L1.
Combining the previous Lemmas, we have the following result:
Theorem 9. Given a non-trivial facet deﬁning inequality (4) for P(n, r), there
are at most 2r lifted inequalities that deﬁne facets of K(n, r).
Proof. The set L in the proof of Lemma 9 has r −1 members and therefore
together with 0 and τ, there are at most 2r possible values for π′
n in a facet
deﬁning lifted inequality (5). As the value of π′
n uniquely determines the re-
maining coeﬃcients in the lifted inequality, by Lemma 8, the claim follows.
Note that, in general determining all possible lifted inequalities is a hard task.
However, the above results show that obtaining all possible facet-deﬁning in-
equalities lifted from facets of P(n, r) is straightforward and can be performed
in polynomial time. We conclude this section with a result on sequential lifting.
Lemma 10. If variable xn is lifted before all yk for k ∈{n −r, . . ., n −1}, then
independent of the rest of the lifting sequence the lifted inequality is
n−1

i=1
¯πixi +
n−1

i=1
¯πn−iyi ≥1.

206
S. Dash, R. Fukasawa, and O. G¨unl¨uk
4
Mixed Integer Rounding Inequalities
In this section we study MIR inequalities in the context of K(n, r). Our analysis
also provides an example that shows that lifting facets of P(n, r) cannot give
all facets of K(n, r). Throughout, we will use the notation ˆx := x −⌊x⌋. Recall
that, for a general single row system of the form:

w ∈Zp
+ : p
i=1 aiwi = b

where ˆb > 0, the MIR inequality is:
p

i=1

⌊ai⌋+ min

ˆai/ˆb, 1

wi ≥⌈b⌉.
We deﬁne the 1
t -MIR (for t ∈Z+) to be the MIR inequality obtained from the
following equivalent representation of K(n, r):
K(n, r) =

(x, y) ∈Zn
+ × Zn
+ :
n

i=1
(i/t)xi −
n

i=1
(i/t)yi = r/t

.
Lemma 11. Given t ∈Z such that 2 ≤t ≤n, the 1
t -MIR inequality
n

i=1
i
t

+ min
 i
mod t
r
mod t, 1

xi+
n

i=1

−
i
t

+ min
(t −i)
mod t
r
mod t
, 1

yi ≥
r
t

is facet deﬁning for K(n, r) provided that r/t ̸∈Z.
It is easy to check that if t > n, then the 1
t -MIR is not facet deﬁning for K(n, r).
Moreover, note that if r/t ∈Z, then the condition that ˆb > 0 is not satisﬁed, thus
the 1
t -MIR inequalities are not facet deﬁning unless they satisfy the conditions
of Lemma 11.
By using the 1
t -MIR as an example, one can then show the following corollary:
Corollary 1. Not all facet-deﬁning inequalities of K(n, r) can be obtained from
lifting facet-deﬁning inequalities of P(n, r), for 0 < r ≤n −2, n ≥9
For r = n −1, it is harder to say, since in this case all points in T automati-
cally satisfy all equations in Q. So every facet-deﬁning inequality of K(n, r) can
be obtained by lifting a valid inequality for P(n, r) corresponding to a point in
Q. However, this point is not necessarily an extreme point of Q, and thus the
corresponding valid inequality is not necessarily a facet of P(n, r).
5
Mixed-Integer Extension
Consider the mixed-integer extension of K(n, r):
K′(n, r) =

(v+, v−, x, y) ∈R2 × Z2n : v+ −v−+
n

i=1
ixi −
n

i=1
iyi = r


On a Generalization of the Master Cyclic Group Polyhedron
207
where n, r ∈Z and n > r > 0. As with the mixed-integer extension of the
master cyclic group of Gomory studied by Gomory and Johnson [10], the facets
of K′(n, r) can easily be derived from the facets of K(n, r) when r is an integer.
Proposition 1. All non-trivial facet deﬁning inequalities for K′(n, r) have the
form
π1v+ + ρ1v−+
n

i=1
πixi +
n

i=1
ρiyi ≥π0.
(6)
Furthermore, inequality (6) is facet deﬁning if and only if πx + ρy ≥πo deﬁnes
a non-trivial facet of K(n, r).
5.1
General Mixed-Integer Sets
Gomory and Johnson used facets of P(n, r) to derive valid inequalities for knap-
sack problems. In particular, they derived subadditive functions from facet co-
eﬃcients via interpolation. We show here how to derive valid inequalities for
knapsack problems from facets of K(n, r).
Deﬁnition 2. Given a facet deﬁning inequality πx + ρy ≥πo for K(n, r), let
f z : Z ∩[−n, n] →R be deﬁned as:
f z(s) =
⎧
⎨
⎩
πs
if s > 0
0
if s = 0
ρ−s if s < 0
We say f : [−n, n] →R where
f(v) = (1 −ˆv)f z(⌊v⌋) + ˆvf z(⌈v⌉)
is a facet-interpolated function derived from (π, ρ, π0).
Proposition 2. Let f be a facet-interpolated function derived from a facet of
K(n, r). Consider the set
Q =

(s, w) ∈Rq
+ × Zp
+ :
q

i=1
cisi +
p

i=1
aiwi = b

,
where the coeﬃcients of the knapsack constraint deﬁning Q are rational numbers.
Let t be such that tai, tb ∈[−n, n] and tb > 0. Then
f(1)
q

i=1
(tci)+si + f(−1)
q

i=1
(−tci)+si +
p

i=1
f(tai)wi ≥f(tb)
where (α)+ = max(α, 0), is a valid inequality for Q.
6
Conclusion
We studied a generalization of the Master Cyclic Group Polyhedron and pre-
sented an explicit characterization of the polar of its nontrivial facet-deﬁning

208
S. Dash, R. Fukasawa, and O. G¨unl¨uk
inequalities. We also showed that one can obtain valid inequalities for a general
MIP that cannot be obtained from facets of the Master Cyclic Group Polyhe-
dron.
In addition, for mixed-integer knapsack sets with rational data and nonnega-
tive variables without upper bounds, our results yield a pseudo-polynomial time
algorithm to separate and therefore optimize over their convex hull. This can
be done by scaling their data and aggregating variables to ﬁt into the Master
Equality Polyhedron framework.
Our characterization of the MEP can also be used to ﬁnd violated Homoge-
neous Extended Capacity Cuts eﬃciently. These cuts were proposed in [15] for
solving Capacitated Minimum Spanning Tree problems and Capacitated Vehicle
Routing problems.
An interesting topic for further study is the derivation of “interesting” classes
of facets for the MEP, i.e., facets which cannot be derived trivially from facets
of the MCGP or as rank one mixed-integer rounding inequalities.
References
1. J. Araoz. Polyhedral Neopolarities. Phd thesis, University of Waterloo, Department
of Computer Sciences, 1974.
2. J. Araoz, L. Evans, R. E. Gomory, and E. Johnson. Cyclic group and knapsack
facets. Mathematical Programming Ser. B, 96(2):377–408, 2003.
3. E. Balas and E. Zemel.
Facets of the knapsack polytope from minimal covers.
SIAM Journal of Applied Mathematics, 34:119–148, 1978.
4. S. Dash and O. G¨unl¨uk. On the strength of gomory mixed-integer cuts as group
cuts. Technical Report RC23967, IBM Research Division, Yorktown Heights, NY
10598, 2006.
5. S. Dash and O. G¨unl¨uk. Valid inequalities based on simple mixed-integer sets.
Mathematical Programming, 105:29–53, 2006.
6. S. Dash and O. G¨unl¨uk. Valid inequalities based on the interpolation procedure.
Mathematical Programming, 106:111–136, 2006.
7. M. Fischetti and M. Monaci. How tight is the corner relaxation? Discrete Opti-
mization, 2007. To appear.
8. M. Fischetti and C. Saturni. Mixed-integer cuts from cyclic groups. Mathematical
Programming A, 109(1):27–53, 2007.
9. R. Gomory. Some polyhedra related to combinatorial problems. Journal of Linear
Algebra and its Applications, 2:451–558, 1969.
10. R. Gomory and E. Johnson. Some continuous functions related to corner polyhedra
I. Mathematical Programming, 3:23–85, 1972.
11. R. Gomory and E. Johnson. Some continuous functions related to corner polyhedra
II. Mathematical Programming, 3:359–389, 1972.
12. R. Gomory and E. Johnson. T-space and cutting planes. Mathematical Program-
ming, 96:341–375, 2003.
13. R. Gomory, E. Johnson, and L. Evans. Cyclic group and knapsack facets. Mathe-
matical Programming, 96:321–339, 2003.
14. E. Uchoa.
Robust branch-and-cut-and-price for the CMST problem and ex-
tended capacity cuts.
Presentation in the MIP 2005 Workshop, Minneapolis
(2005). Available at http://www.ima.umn.edu/matter/W7.25-29.05/activities/
Uchoa-Eduardo/cmst-ecc-IMA.pdf

On a Generalization of the Master Cyclic Group Polyhedron
209
15. E. Uchoa, R. Fukasawa, J. Lysgaard, A. Pessoa, M. Poggi de Arag˜ao, and D. An-
drade. Robust branch-cut-and-price for the capacitated minimum spanning tree
problem over a large extended formulation. Mathematical Programming, To appear.
16. L. Wolsey. Facets and strong valid inequalities for integer programs. Oper. Res.,
24:367–372, 1976.
17. G. Nemhauser Z. Gu and M. Savelsbergh. Sequence independent lifting in mixed
integer programming. J. Comb. Optim., 4:109–129, 2000.

A Framework to Derive Multidimensional
Superadditive Lifting Functions and Its
Applications⋆
Bo Zeng and Jean-Philippe P. Richard
School of Industrial Engineering, Purdue University, 315 N. Grant Street,
West Lafayette, IN 47907-2023.
Abstract. In this paper, we present a systematic method to derive
strong superadditive approximations of multidimensional lifting func-
tions using single-dimensional superadditive functions. This constructive
approach is based on the observation that, in many cases, the lifting func-
tion of a multidimensional problem can be expressed or approximated
through the single-dimensional lifting function of some of its components.
We then apply our approach to two variants of classical models and show
that it yields an eﬃcient procedure to derive strong valid inequalities.
1
Introduction
Lifting is the process of deriving valid inequalities for a complex mixed integer
program (MIP) from valid inequalities of a simple restriction. Lifting, in its com-
mon acception, was introduced by Padberg [15] and generalized by Wolsey [23].
It was used to study the polyhedral structure of many mixed integer programs
such as 0−1 knapsack sets (see Balas [4], Hammer et al. [11] and Wolsey [22])
and single node ﬂow sets (see Gu et al. [9] and Atamt¨urk [1]). More importantly,
cutting planes obtained through lifting have been proven to be very eﬀective at
reducing solution times for 0−1 MIPs; see Crowder et al. [7], Gu et al. [8] and
Van Roy and Wolsey [20]. As a consequence, lifted cuts generated from simple
substructures of MIPs have been implemented in various commercial software,
including CPLEX and X-Press.
Given a valid inequality (seed inequality) that is strong for the restriction of
a set of interest, lifting is typically implemented sequentially, i.e. ﬁxed variables
are reintroduced into the inequality one at a time (or one group at a time).
Furthermore, to determine the lifting coeﬃcient of a variable, it is necessary to
obtain an optimal solution of the lifting problem which is itself an MIP. Because
the lifting problems to be solved are diﬀerent for each lifted variable, lifting
can rapidly become prohibitive. Nevertheless, when the lifting function of a seed
inequality is well-structured, lifting can be performed eﬃciently. In particular,
Wolsey [24], Gu et al. [10] and Atamt¨urk [3] showed that if the lifting function
of the seed inequality is superadditive, then all the lifting coeﬃcients can be
obtained from the ﬁrst lifting problem.
⋆This research is supported by NSF Grant DMI-03-48611.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 210–224, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

A Framework to Derive Multidimensional Superadditive Lifting Functions
211
There are various inequalities whose lifting functions are naturally superad-
ditive. Examples include some ﬂow covers inequalities (see Gu et al. [10]), and
mixed integer cover inequalities for knapsack problems with a single continuous
variable (see Marchand and Wolsey [13]). However, most often, lifting functions
are not superadditive. In these cases, a superadditive lower approximation of the
exact lifting function can be used to generate strong cuts [3, 10]. This idea was
successfully used by Gu et al. [10] for 0−1 knapsack problems, by Gu et al. [9] and
Louveaux and Wolsey [12] for single node ﬂow models, by Shebalov and Klabjan
[18] for mixed-integer programs with variable upper bounds and by Atamt¨urk
[2] for general mixed integer knapsack sets. We note however that the lifting
functions used in all of these cases are single-dimensional.
In one dimension, constructing a high-quality superadditive approximation of
a lifting function is typically diﬃcult and verifying that it is superadditive is
often cumbersome. Although Atamt¨urk [3] proved that multidimensional super-
additive lifting functions yield sequence independent lifting for general MIPs, a
practical implementation of the idea seems to be diﬃcult at ﬁrst because in addi-
tion to the diﬃculties mentioned for single-dimensional problems, the derivation
of exact multidimensional lifting functions is diﬃcult and the proof that approx-
imations are of good quality is hard. To the best of our knowledge, all but one
of the superadditive lifting functions that were investigated to date are single-
dimensional. The only exception is our study of the 0−1 knapsack problem with
disjoint cardinality constraints [26] where we derived provably strong superaddi-
tive approximations of the multidimensional lifting function of cover inequalities.
In [26], we observed that the high-dimensional exact lifting function of a minimal
cover inequality could be represented using a composition of lower-dimensional
exact lifting functions. We used this observation to build multidimensional su-
peradditive lifting functions from the known superadditive approximations of
the lower-dimensional lifting functions.
In this paper, we generalize these results to typical 0−1 MIP sets and propose
a framework to construct high-dimensional superadditive lifting functions us-
ing known lower-dimensional superadditive lifting functions. We also show how
this approach can be applied to variants of the knapsack and single node ﬂow
models with additional constraints. In particular, we obtain with our approach
various families of strong inequalities for MIPs that are diﬃcult to study using
traditional tools.
The paper is organized as follows. In Section 2, after brieﬂy reviewing sequence
independent and superadditive lifting, we describe a way to represent /approx-
imate high-dimensional exact lifting functions of valid inequalities for 0−1 MIP
sets using the exact lifting functions of simpler 0−1 MIP sets. Then, we propose a
framework to construct high-dimensional superadditive approximations of lifting
functions using this representation. In Section 3, we apply our framework to the
precedence-constrained knapsack model (PCKP). In particular, we build strong
multidimensional superadditive lifting functions and derive strong lifted inequal-
ities. Similarly, in Section 4, we obtain a family of facet-deﬁning inequalities for

212
B. Zeng and J.-P.P. Richard
the single node ﬂow model with disjoint cardinality constraints (SNFCC). In
Section 5, we give a conclusion and discuss future directions of research.
2
Constructing Multidimensional Superadditive Lifting
Functions
In this section, we ﬁrst review basic results about lifting. Then, we give a method
to represent or approximate high-dimensional exact lifting functions using lower-
dimensional ones. Finally, we describe an approach to build high-dimensional
superadditive lifting functions that is based on the previous representation.
2.1
Lifting and Superadditive Lifting Functions
In this section, we brieﬂy review lifting concepts and techniques. We focus on
0−1 MIP models in which the continuous variables have variable upper bounds.
The description for pure 0−1 integer program is simpler and can be obtained
similarly.
Let N = {1, . . . , n}. Consider S = {(x, y) ∈{0, 1}n × Rn
+ : Ax + By ≤
d, yj ≤ujxj, ∀j ∈N}. We deﬁne PS to be the convex hull of S and deﬁne
PS(N0, N1) = conv{(x, y) ∈S : xj = 0 ∀j ∈N0, xj = 1 ∀j ∈N1, yj = 0 ∀j ∈
N0, yj = uj ∀j ∈N1}. We use a similar notation for pure 0−1 sets.
Assume that

j∈ˆ
N
αjxj +

j∈ˆ
N
βjyj ≤α0
(1)
is a strong valid inequality for PS(N0, N1) with ˆN = N\(N0 ∪N1). We wish
to reintroduce (lift) the ﬁxed variables (xj, yj) for j ∈N0 ∪N1 into the seed
inequality (1). Without loss of generality, we denote N0 ∪N1 = {1, . . . , ˆn} and
assume that (x1, y1) is the ﬁrst pair of variables to be lifted. Deﬁne l0 = l1 = 0
if (x1, y1) is lifted from (0, 0) and deﬁne l0 = 1 and l1 = u1 if (x1, y1) is lifted
from (1, u1). The inequality obtained through lifting is

j∈ˆ
N
αjxj +

j∈ˆ
N
βjyj + α1(x1 −l0) + β1(y1 −l1) ≤α0
(2)
where α1 and β1 are chosen in such a way that
α1(x1 −l0) + β1(y1 −l1) ≤f(A1(x1 −l0) + B1(y1 −l1))
(3)
for (x1, y1) ∈{(s, t) ∈{0, 1} × [0, ul] : t ≤sul} and where
f(z) = min α0 −

j∈ˆ
N
αjxj +

j∈ˆ
N
βjyj
s.t.

j∈ˆ
N
(Ajxj + Bjyj) ≤d −z, yj ≤ujxj, ∀j ∈ˆN.
(4)

A Framework to Derive Multidimensional Superadditive Lifting Functions
213
By sequentially applying the above lifting operation with respect to the variables
(x1, y1), . . . , (xˆn, yˆn), the seed inequality (1) is progressively converted into a
strong valid inequality for PS. It is proven in Nemhauser and Wolsey [14] that
if PS(N0, N1) is full-dimensional, (1) is facet-deﬁning for PS(N0, N1) and (3)
is satisﬁed at equality by two new aﬃnely independent solutions, then (2) is
facet-deﬁning for PS(N0\{1}, N1) if lifted from (0, 0) or for PS(N0, N1\{1}) if
lifted from (1, u1).
Usually f(z) in (4) is referred to as the exact lifting function (or lifting func-
tion) of the seed inequality (1). As we mentioned in Section 1, generating strong
cuts through sequential lifting is typically computationally intensive. Wolsey
[24], Gu et al. [10], and Atamt¨urk [3] showed that if the lifting function f is su-
peradditive, i.e. f(z1)+f(z2) ≤f(z1 +z2) for z1, z2, z1 +z2 in the domain of the
lifting function, then lifting coeﬃcients are independent of the lifting sequence
and can be directly obtained from f. Since most lifting functions are not super-
additive, superadditive lower approximations are often used to generate strong
cuts [10, 3]. We use the criteria of non-dominance and maximality proposed by
Gu et al. [10] to measure the strength of superadditive approximations.
2.2
Representation of High-Dimensional Lifting Function
In this section, we give a representation of high-dimensional lifting functions
of given seed inequalities using low-dimensional lifting functions. Although the
method does not always describe the high-dimensional lifting functions exactly,
it has two advantages. First, it signiﬁcantly reduces the diﬃculties associated
with describing high-dimensional exact lifting functions. Second, it can be used
to derive a superadditive approximation of the initial high-dimensional function
using superadditive approximation of the lower-dimensional lifting functions.
An intuitive explanation of our scheme is as follows. When a new constraint is
introduced into the initial constraint matrix, it forces some variables to become
0 or 1. If these variables are known, we can use the lifting function associated
with the initial set of constraints to represent or approximate the exact lifting
function of the new system.
Consider PS = conv{x ∈{0, 1}n : Ax ≤b} with A = {A1, . . . , An} ∈Rm×n
and b ∈Rm. Let 
j∈ˆ
N πjxj ≤π0 be a valid inequality for PS(N\ ˆN, ∅) and
denote its lifting function by f. Assume now that the constraint 
j∈N pjxj ≤
bm+1 is added to A and denote the augmented constraint matrix by A′. Then,
deﬁne f ′ to be the lifting function based on A′. Clearly, f : Rm →R and
f ′ : Rm+1 →R. Note that, the objective functions of the lifting problems deﬁning
f and f ′ are identical. It is also clear that f(z) = f ′ z
−∞

for z ∈Rm.
Proposition 1. Let j∗∈ˆN and assume that ˆx is an optimal solution to f ′z
p

.
(i) If ˆxj∗= 0, then
f ′
z
p

≥max{f(z −Aj∗) + πj∗, f(z)};
(5)

214
B. Zeng and J.-P.P. Richard
(ii) If ˆxj∗= 1, then
f ′
z
p

≥max{f(z + Aj∗) −πj∗, f(z)}.
(6)
□
The conclusion of Proposition 1 is very general since it is independent of the
structure of the 0−1 set and of the seed inequality. It is helpful because in many
lifting functions of 0−1 sets with multiple constraints, it is easy to determine the
variables that are forced to 0 or 1 by the addition of a constraint. In such cases,
we can derive a representation of f ′ for all
z
p

∈Rm+1. Furthermore, we observe
that the expressions of Proposition 1 can be recursively used to approximate
complicated situations. For example, if {x1, x2} are forced to 0 and 1 respectively
at
z
p

in an optimal solution to f ′, then we can write f ′z
p

≥max{f(z −A1 +
A2)+π1−π2, f(z−A1)+π1, f(z+A2)−π2, f(z)}. We also observe that in various
types of multidimensional lifting functions, the inequalities in (5) and (6) can
be proven to be satisﬁed at equality, i.e. we can use the low-dimensional lifting
functions through (5) and (6) to represent the high-dimensional lifting functions
exactly; see Zeng and Richard [26] for a proof in the case of cover inequalities
for knapsack problems with disjoint cardinality constraints.
The situation for general mixed integer program is more diﬃcult than that
presented in Proposition 1. Next, we generalize these results to describe the
eﬀect on the lifting function of adding constraints to mixed integer sets of the
ﬂow type, which form an important class of MIPs.
Consider PS = conv{(x, y) ∈Rn
+ ×{0, 1}n : Ax+By ≤d, yj ≤ujxj, ∀j} with
A = {A1, . . . , An}, B = {B1, . . . , Bn} ∈Rm×n. Let 
j∈ˆ
N αjxj + 
j∈ˆ
N βjyj ≤
α0 be a valid inequality for PS(N0, N1) and denote its lifting function by g.
Assume that the constraint 
j∈N pjxj ≤dm+1 is added to [AB] and denote the
augmented constraint matrix by [A′B]. Then, deﬁne g′ to be the lifting function
based on [A′B].
Proposition 2. Let j∗∈N\(N0 ∪N1) and assume that (ˆx, ˆy) is an optimal
solution to g′z
p

.
(i) If ˆxj∗= 0, then
g′
z
p

≥max{g(z −Aj∗−Bj∗uj∗) + αj∗+ βj∗uj∗, g(z)};
(7)
(ii) If ˆxj∗= 1, then
g′
z
p

≥max{g(z + Aj∗+ Bj∗ˆyj∗) −αj∗−βj∗ˆyj∗, g(z)}.
(8)
where ˆyj∗is the value of the ith element of ˆy.
□
Note that Proposition 2 can also be applied recursively to approximate more
complicated situations.

A Framework to Derive Multidimensional Superadditive Lifting Functions
215
2.3
A Framework to Build Multidimensional Superadditive
Functions
In
this
subsection,
we
propose
a
constructive
framework
to
build
high-dimensional superadditive lifting functions from lower-dimensional ones.
This framework is composed of a general scheme that can be enhanced by sev-
eral simple rules. These rules can be used alone or in combination to create new
superadditive lifting functions.
First observe that f ′ and g′ in Proposition 1 and Proposition 2 contain forms
such as f(z −Aj∗) + πj∗and g(z −Aj∗−Bj∗uj∗) + αj∗+ βj∗uj∗. In Theo-
rem 1, we show how to ﬁnd superadditive approximations for a generalization
of these forms and show how to combine them to obtain a multidimensional
superadditive approximation of the lifting function. We ﬁrst generalize the con-
cept of superadditivity. Let ϕ : Rm1 	−→Rm2. We say that ϕ is non-decreasing
if ϕ(x) ≥ϕ(y) when x ≥y for x, y ∈Rm1. We say that ϕ is superadditive if
ϕ(x) + ϕ(y) ≤ϕ(x + y) for all x, y ∈Rm1.
Theorem 1. Let π1 : Rm1 	−→R, π2 : Rm1 × Rm2 	−→R, and π3 : Rm1 ×
Rm2 	−→Rm1 be superadditive functions over their domains, and assume that
π1(0) = π2(0, 0) = 0 and π3(0, 0) = 0. Assume that π1 is non-decreasing. The
function κ : Rm1 × Rm2 	−→R deﬁned as
κ(x, y) = π1(x + π3(x, y)) + π2(x, y)
(9)
is superadditive over Rm1 ×Rm2 with κ(0, 0) = 0. Furthermore, if π2 and π3 are
non-decreasing, then κ is non-decreasing.
□
Next, we present several simple rules to compose superadditive functions. Rule 1
presents a way to extend an existing superadditive function to a larger domain.
Rule 2 and Rule 3 are adapted from Nemhauser and Wolsey [14].
Rule 1. Let π1(x) : D1 ⊆Rm1 →R be a superadditive function. Let y ∈
Rm1\D1. Then, the function
κ(x) =
π1(x),
if x ∈D1
sup{π1(x1) + π1(x2) : x = x1 + x2, x1, x2 ∈D1, }, if x = y
is superadditive over D1 ∪{y}.
□
Assume now that γi : Rm 	−→R are superadditive functions for i = 1, 2.
Rule 2. The function κ deﬁned as κ(x) = min{γ1(x), γ2(x)} is superadditive
over Rm.
□
Rule 3. The function κ deﬁned as κ(x) = γ1(x) + γ2(x) is superadditive over
Rm.
□
In Section 3 and in Section 4, we show how to apply Theorem 1 and Rules 1−3 to
build strong multidimensional superadditive lifting functions for speciﬁc MIPs.

216
B. Zeng and J.-P.P. Richard
3
Superadditive Lifting Functions for 0−1 PCKP
Let N = {1, . . . , n} and 0 ≤aj ≤b for j ∈N. The precedence-constrained
knapsack model is deﬁned as
XPCKP = {x ∈{0, 1}n :

j∈N
ajxj ≤b, xt(i) ≤xh(i), i = 1, . . . , r}
(10)
where t(i) ̸= h(i) for i = 1, . . . , r. We denote the convex hull of XPCKP as PPCKP
and the convex hull of the classical knapsack set as PKP .
Precedence-constrained knapsack problems arise frequently in planning,
scheduling and project management. The polyhedral structure of PCKP has
been studied by various authors; see Boyd [6], Park and Park [17], van de Leensel
et al. [19] and Boland et al. [5]. In particular, van de Leensel et al. [19] proved
that lifting minimal induced cover inequality is a NP-hard problem in general.
In this section, we focus on deriving strong inequalities from minimal cover in-
equalities of the knapsack constraint using superadditive lifting functions. Note
that precedence constraints are one of the many additional features that we can
incorporate into the lifting function using the framework we proposed in Sec-
tion 2. Another variant of knapsack problem, the 0−1 knapsack problem with
disjoint cardinality constraints was studied in Zeng and Richard [25, 26]. These
papers are the roots of the results of Section 2.
For the traditional 0−1 knapsack polytope PKP , we say that a set C ⊆N is
a cover if 
j∈C aj > b. Furthermore, we say that a cover C is minimal if, for all
j ∈C, C\{j} is not a cover. Given a minimal cover C, the cover inequality

j∈C
xj ≤|C| −1
(11)
is facet-deﬁning for PKP (N\C, ∅).
We now use cover inequality (11) to derive strong valid inequality for PPCKP .
To simplify the exposition, we assume in this paper that the precedence con-
straints are organized into s disjoint paths such that xji,1 ≥· · · ≥xji,|Ni|
where Ni = {ji,1, . . . , ji,|Ni|} is the ith path. It is not restrictive to assume
that N = N1 ∪· · · ∪Ns since path can have length one. Furthermore, because
the precedence constraint structure of any PCKP problem can be relaxed into a
set of disjoint paths, our results are applicable to the general case.
Deﬁne i[j] to be the index of the path xj belongs to, i.e. j ∈Ni[j]. Also
denote C ∩Ni = Ci for i = 1, . . . , s and deﬁne F(j) for j ∈N\C to be the set all
ancestors of xj in its path. We next present necessary and suﬃcient conditions
for (11) to be strong for PPCKP .
Proposition 3. Let C be a minimal cover. The cover inequality (11) is facet-
deﬁning for PPCKP(N\C, ∅) if and only if Ci = {ji,1} or Ci = ∅for i =
1, . . . , s.
□
Next, we describe how to lift minimal cover inequality. First observe that the
lifting of variables in a given path is ﬁxed because of the precedence constraints.

A Framework to Derive Multidimensional Superadditive Lifting Functions
217
In fact, when lifting xj from 0 to 1, all the ancestors of j are forced to 1 because
of the precedence constraints. Based on this observation and using Theorem 1
and Rule 1, we can easily derive a multidimensional superadditive lifting func-
tion using the single-dimensional superadditive approximation θ of the lifting
function for a minimal cover inequality proposed in Gu et al. [10]. We denote
this multidimensional function by φ
z
v

with (z, v) ∈[0, b]×D where D = {0, 1}s.
Proposition 4. The function
φ
z
v

=
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
θ(z)
if v = 0
θ(z)
if v = ei and Ci = ∅
max{θ(z + aji,1) −1, θ(z)}
if v = ei and Ci = {ji,1}
sup
v=
k∈V ek,z=
k∈V zk, zk≥0 ∀k
{

k∈V
φ
zk
ek

} if v /∈{0, e1, . . . , es}
for (z, v) ∈[0, b]×D is a valid superadditive approximation of the lifting function
of (11) that is non-dominated and maximal.
□
In the next theorem, we present the lifted cover inequality for PPCKP that can
be obtained using the multidimensional superadditive lifting function φ.
Theorem 2. Assume that C is a minimal cover from the knapsack constraint
of PPCKP that satisﬁes the condition of Proposition 3, then

j∈C
xj +

j∈N\C

φ
p+
j + aj
ei[j]

−φ
 p+
j
ei[j]

xj ≤|C| −1.
(12)
with p+
j = 
k∈F (j)\Ci[j] ak is a valid inequality for PPCKP.
□
Note that the inequality (12) can be derived very eﬃciently since the function
θ (and therefore the function φ) is known in closed form. Next, we show in
Example 1 that (12) is strong by comparing it to the lifted cover inequality
obtained using the single-dimensional superadditive lifting function θ from [10].
Example 1. Let
S = {x ∈{0, 1}8 : 8x1 + 7x2 + 6x3 + 4x4 + 6x5 + 6x6 + 5x7 + 8x8 ≤22,
x6 ≤x5 ≤x2, x8 ≤x7 ≤x2}.
The cover inequality x1 +x2 +x3 +x4 ≤3 is facet-deﬁning for PS({5, 6, 7, 8}, ∅).
Using the traditional single-dimensional superadditive lifting function of the
cover inequality, we obtain the following lifted inequality
x1 + x2 + x3 + x4 + 0.5x5 + 0.5x6 + x8 ≤3.
(13)
We now show that we can obtain a stronger inequality using the precedence
structure. First, as illustrated in Figure 1, we relax the precedence constraints
into two disjoint paths. We then apply the results of Proposition 4 to obtain the

218
B. Zeng and J.-P.P. Richard
multidimensional superadditive approximation φ
z
v

for (z, v) ∈[0, b]×{0, e1}×
{0, e2} that is illustrated in Figure 2. The solid line in Figure 2 shows the exact
lifting function and the doted line describes the superadditive approximation.
Lifting the variables x5, x6, x7 and x8, we obtain α5 = φ
 6
e1

= θ(13) −1 =
1, α6 = φ
12
e1

−φ
 6
e1

= θ(19) −θ(13) = 1, α7 = φ
 5
e2

= θ(5) = 0, α8 =
φ
13
e2

−φ
 5
e2

= θ(13) −θ(5) = 2. Therefore, the lifted cover inequality is
x1 + x2 + x3 + x4 + x5 + x6 + 2x8 ≤3.
(14)
which clearly dominates (13).
□
X7
X8
X6
X5
X2
X7
X8
X6
X5
X2
Fig. 1. Relaxing the precedence structure into disjoint paths
(a) φ
z
0

= φ
 z
e2

= θ(z)
(b) φ
 z
e1

= φ

z
e1+e2

Fig. 2. Exact lifting function and superadditive approximation
4
Superadditive Lifting Functions for SNFCC
The single node ﬂow model is a relaxation of numerous logistics, transportation
and telecommunication network design problems. Research on single node ﬂow
model is very extensive; see Padberg et al. [16], Van Roy and Wolsey [21] and
Gu et al. [9, 10] among many others. In this paper, we consider a variant of this
model with disjoint cardinality constraints. It is deﬁned as
XSNFCC = {(x, y) ∈{0, 1}n × Rn
+ :

j∈N
yj ≤b, yj ≤ajxj, ∀j ∈N,

j∈Ni
xj ≤Ki, i = 1, . . . , r}
where a1 ≥· · · ≥an > 0, Ki ≥1 for i = 1, . . . , r, Ni ∩Nj = ∅if i ̸= j and
N = N0 ∪· · · ∪Nr.

A Framework to Derive Multidimensional Superadditive Lifting Functions
219
We denote the convex hull of SNFCC as PSNFCC. Again, cardinality con-
straints are one of the side constraints that can easily be incorporated into the
lifting function in our scheme and help us illustrate our multidimensional lifting
approach. A ﬂow cover is a set C ⊆N such that 
j∈C aj −b = λ > 0. Let
C+ = {j ∈C : aj > λ}. The corresponding ﬂow cover inequality is

j∈C
yj +

j∈C+
(aj −λ)(1 −xj) ≤b.
(15)
It is proven in Nemhauser and Wolsey [14] that this valid inequality is facet-
deﬁning for PSNF(N\C, ∅). Gu et al. [10] studied the problem of lifting (15) and
proved that the lifting function ψ(z) of (15) is superadditive over [−λ, +∞).
Therefore, the lifting of variables (xj, yj) for j ∈N\C is sequence independent
because aj > 0 for j ∈N. We generalize these results for the cardinality con-
strained cases. For the sake of brevity, we consider here the most general case
where C is not a subset of Ni for i = 0, . . . , r. Next, we present a set of suﬃcient
conditions under which (15) is strong for PSNFCC.
Proposition 5. The
ﬂow
cover
inequality
(15)
is
facet-deﬁning
for
PSNFCC(N\C, ∅) if |C ∩Ni| ≤Ki for i = 1, . . . , r.
□
Deﬁne now C ∩Ni = Ci, iM ∈arg max{aj : j ∈Ci, j ∈C+} and im ∈
arg min{aj : j ∈Ci, j /∈C+}. Also, assume that Ci = {ji,1, . . . , ji,|Ci|} with
aji,1 ≥· · · ≥aji,|Ci|. To distinguish it from the traditional lifting function ψ of
the ﬂow cover, we refer to the lifting function of the ﬂow cover inequality with
the consideration of cardinality constraints as Ψ
z
v

for v ∈D′ = {0, e1, . . . , er}.
There are three nontrivial cases that we need to consider to approximate Ψ as a
function of ψ using Proposition 2.
Theorem 3. The lifting function Ψ
z
v

for (z, v) ∈R+ ×D′ is Ψ
z
0

= ψ(z) and
(i) if Ci ⊆C+, then
Ψ
 z
ei

≥
ψ(z)
if |Ci| ≤Ki −1,
max{ψ(z −aiM ) + λ, ψ(z)} if |Ci| = Ki.
(16)
(i) if Ci ∩C+ = ∅and Ci ̸= ∅, then
Ψ
 z
ei

≥

ψ(z)
if |Ci| ≤Ki −1,
max{ψ(z −aim) + aim, ψ(z)} if |Ci| = Ki.
(17)
(iii) if Ci ⊈C+ and Ci ∩C+ ̸= ∅, then
Ψ
 z
ei

≥

ψ(z)
if |Ci| ≤Ki −1,
min{ψ1(z), ψ2(z)} if |Ci| = Ki
(18)
where ψ1(z) = max{ψ(z −aiM ) + λ, ψ(z)} and ψ2(z) = max{ψ(z −aim) +
aim, ψ(z)}.
□

220
B. Zeng and J.-P.P. Richard
In fact, we can further prove that (16)-(18) are satisﬁed at equality. Then, we
use Theorem 1, Rule 1 and Rule 2 to verify that Ψ is naturally superadditive.
Theorem 4. The function Ψ
z
v

deﬁned as
Ψ
z
v

=
⎧
⎪
⎨
⎪
⎩
Ψ
z
v

if v ∈D′
sup
{z= s
i=1 zi, zi≥0 ∀i}
{
s

i=1
Ψ
zi
ei

} if v /∈D′
(19)
is superadditive over [0, b]×D where D = [0, b]×{0, . . ., K1}×· · ·×{0, . . ., Kr}. □
It follows from Theorem 4 that we can apply sequence independent lifting to
obtain the lifted ﬂow cover inequality

j∈C
yj +

j∈C+
(aj −λ)(1 −xj) +

j∈N\C
(αjxj + βjyj) ≤b.
(20)
To derive the coeﬃcients αj and βj for j ∈N\C, we deﬁne Aj = j
h=1 ah
for j = 1, . . . , n and A0 = 0. Also, we let s+ = |C+|. In Theorem 5, we present
the lifting coeﬃcients for the lifted ﬂow cover inequality using Ψ
z
v

. We do not
list here the results for the case where Ci ⊈C+ and Ci ∩C+ ̸= ∅because the
expressions for lifting coeﬃcients are similar but more complicated.
Theorem 5. Inequality (20) is facet-deﬁning for PSSNFCC if (αj, βj) ∈Hj for
j ∈N\C where Hj is deﬁned as follows when j ∈Ni:
(i) When (1) i = 0, or (2) |Ci| ≤Ki −1, or (3) |Ci| = Ki and aiM = a1.
If aj ≤a1 −λ, we Hj = {(0, 0)}. Otherwise, let l = arg max0≤h≤s+{aj ≥
Ah −λ} and deﬁne Hj = {(0, 0)} ∪H1
j ∪H2
j with
H1
j = {(λ(k −1) −λ(Ak −λ)
ak
, λ
ak
) : k = 2, . . . , l}
and
H2
j =
⎧
⎨
⎩
∅
if aj = Al −λ
{(lλ −Al, 1)} if Al −λ < aj ≤Al or aj > As+
{(lλ −ajρ, ρ)} if aj < As+ and Al < aj < Al+1 −λ
where ρ =
λ
aj+λ−Al .
(ii) When |Ci| = Ki and Ci ⊆C+.
If aj ≤aiM −λ, deﬁne Hj = {(0, 0)}. Otherwise, let l = arg max1≤h≤s+{aj ≥
min{Ah −λ, Ah−1 + aiM −λ}} and deﬁne Hj = {(0, 0)} ∪H1
j ∪H2
j with
H1
j = {(λ(k −1) −
λ(min{Ak, Ak−1 + aiM } −λ)
min{Ak, Ak−1 + aiM } −min{Ak−1, Ak−2 + aiM },
λ
min{Ak, Ak−1 + aiM } −min{Ak−1, Ak−2 + aiM }) : k = 2, . . . , l}
and H2
j is equal to (5) with Al replaced by min{Al, Al−1 + aiM }.

A Framework to Derive Multidimensional Superadditive Lifting Functions
221
(iii) When |Ci| = Ki and Ci ∩C+ = ∅.
If 0 < aj ≤aim + a1 −λ, deﬁne Hj = {0,
aim
max{aj,aim}}. Otherwise, let
l = arg max1≤h≤s+{aj ≥Ah + aim −λ} and deﬁne A+
j
= Aj + aim for
j = 1, . . . , s+ and Hj = {(0,
aim
aim +a1−λ)} ∪H1
j ∪H2
j with
H1
j = {((k −1)λ + aim −λ(Ak −λ + aim)
ak
, λ
ak
) : k = 2, . . . , l}
and
H2
j =
⎧
⎨
⎩
∅
if aj = A+
l −λ
{(lλ −Al, 1)}
if A+
l −λ < aj ≤A+
l or aj > A+
s+
{(lλ + aim −ajρ+, ρ+)} if aj < A+
s+ and A+
l < aj < A+
l+1 −λ
where ρ+ =
λ
aj+λ−A+
l .
□
We note that part (i) of Theorem 5 is identical to Theorem 9 in [10]. However,
Theorem 5 in general yields stronger coeﬃcients as illustrated in Example 2.
Example 2. Let
S = {(x, y) ∈{0, 1}8 × R8
+ : y1 + y2 + y3 + y4 + y5 + y6 + y7 + y8 ≤24,
y1 ≤9x1, y2 ≤7x2, y3 ≤6x3, y4 ≤3x4,
y5 ≤2x5, y6 ≤2x6, y7 ≤3x7, y8 ≤12x8,
x6 + x7 + x8 ≤1}.
The ﬂow cover inequality
6

j=1
yj + (9 −5)(1 −x1) + (7 −5)(1 −x2) + (6 −5)(1 −x3) ≤24
based on C = {1, 2, 3, 4, 5, 6} is facet-deﬁning for PS({7, 8}, ∅) since λ = 5.
In Figure 3, we show the multidimensional lifting function Ψ and the single-
dimensional function ψ. If we use the single-dimensional lifting function ψ, we
obtain the inequalities
6

j=1
yj + 4(1 −x1) + 2(1 −x2) + (1 −x3) +
⎧
⎨
⎩
0
−20
7
−6
⎫
⎬
⎭x8 +
⎧
⎨
⎩
0
5
7
1
⎫
⎬
⎭y8 ≤24.
Using the results of Theorem 5(iii), we obtain
6

j=1
yj +4(1−x1)+2(1−x2)+1(1−x3)+ 2
3y7 +
 0
−3

x8 +
 1
35
6

y8 ≤24. (21)
All the inequalities represented by (21) are facet-deﬁning for the convex hull
of S.
□

222
B. Zeng and J.-P.P. Richard
(a) Ψ
z
0

= ψ(z)
(b) Ψ
z
1

Fig. 3. Superadditive lifting function Ψ
5
Conclusion
In this paper we propose a novel approach to construct multidimensional su-
peradditive lifting functions and apply it to study two variants of classical MIP
models. This approach is based on the observation that it is usually possible
to represent high-dimensional lifting functions using lower-dimensional lifting
functions. The approach we propose is systematic, constructive and the multi-
dimensional superadditive lifting functions obtained yield strong inequalities for
models in which a direct lifting approach would have been diﬃcult. In particular,
we obtained multidimensional superadditive lifting functions for the precedence-
constrained knapsack model and for the single node ﬂow model with disjoint
cardinality constraints. For these models, we presented a set of cutting planes
that are stronger than those obtained from the knapsack or ﬂow constraint only.
To the best of our knowledge, our framework is the ﬁrst attempt to construct
multidimensional superadditive lifting functions.
We are currently generalizing the procedure to generate more complicated
multidimensional superadditive lifting functions for unstructured MIPs with
multiple constraints. We are also considering several more general MIP models
that have practical signiﬁcance in transportation and network design. Finally,
we are currently carrying an empirical evaluation of the cutting planes produced
by the multidimensional lifting techniques presented in this paper.
References
[1] A. Atamt¨urk. Flow pack facets of the single node ﬁxed-charge ﬂow polytope.
Operations Research Letters, 29:107–114, 2001.
[2] A. Atamt¨urk.
On the facets of the mixed-integer knapsack polyhedron.
Mathematical Programming, 98:145–175, 2003.
[3] A. Atamt¨urk. Sequence independent lifting for mixed-integer programming.
Operations Research, 52:487–490, 2004.
[4] E. Balas. Facets of the knapsack polytope. Mathematical Programming, 8:
146–164, 1975.

A Framework to Derive Multidimensional Superadditive Lifting Functions
223
[5] N. Boland, C. Fricke, G. Froylandz, and R. Sotirov. Clique-based facets
for the precedence constrained knapsack polyhedron. Technical report, The
University of Melbourne , Australia, 2005.
[6] E. Boyd. Polyhedral results for the precedence-constrained knapsack prob-
lem. Discrete Applied Mathematics, 41:185–201, 1993.
[7] H. Crowder, E. Johnson, and M. Padberg. Solving large scale zero-one linear
programming problem. Operations Research, 31:803–834, 1983.
[8] Z. Gu, G. Nemhauser, and M. Savelsbergh. Lifted cover inequalities for
0-1 integer programs: computation. INFORMS Journal on Computing, 10:
427–437, 1998.
[9] Z. Gu, G. Nemhauser, and M. Savelsbergh. Lifted ﬂow cover inequalities
for mixed 0-1 integer programs. Mathematical Programming, 85:439–468,
1999.
[10] Z. Gu, G. Nemhauser, and M. Savelsbergh. Sequence independent lifting
in mixed integer programming. Journal of Combinatorial Optimization, 4:
109–129, 2000.
[11] P. Hammer, E. Johnson, and U.Peled.
Facets of regular 0-1 polytopes.
Mathematical Programming, 8:179–206, 1975.
[12] Q. Louveaux and L. Wolsey. Lifting, superadditivity, mixed integer rounding
and single node ﬂow sets revisited. 4OR, 1:173–207, 2003.
[13] H. Marchand and L. Wolsey. The 0-1 knapsack problem with a single con-
tinuous variable. Mathematical Programming, 85:15–33, 1999.
[14] G. Nemhauser and L. Wolsey.
Integer and Combinatorial Optimization.
Wiley, 1988.
[15] M. Padberg. On the facial structure of set packing polyhedra. Mathematical
Programming, 5:199–215, 1973.
[16] M. Padberg, T. Van Roy, and L. Wolsey. Valid inequalities for ﬁxed charge
problems. Mathematical Programming, 33:842–861, 1985.
[17] K. Park and S. Park.
Lifting cover inequalities for the precedence-
constrained knapsack problem. Discrete Applied Mathematics, 72:219–241,
1997.
[18] S. Shebalov and D. Klabjan. Sequence independent lifting for mixed integer
programs with variable upper bounds.
Mathematical Programming, 105:
523–561, 2006.
[19] R.L.M.J. van de Leensel, C.P.M. van Hoesel, and J.J. van de Klundert.
Lifting valid inequalities for the precedence constrained knapsack problem.
Mathematical Programming, 86:161–185, 1999.
[20] T. Van Roy and L. Wolsey. Solving mixed integer programming problems
using automatic reformulation. Operations Research, 35:45–57, 1987.
[21] T. Van Roy and L. Wolsey.
Valid inequalities for mixed 0-1 programs.
Discrete Applied Mathematics, 14:199–213, 1986.
[22] L. Wolsey.
Faces for a linear inequality in 0-1 variables.
Mathematical
Programming, 8:165–178, 1975.
[23] L. Wolsey. Facets and strong valid inequalities for integer programs. Oper-
ations Research, 24:367–372, 1976.

224
B. Zeng and J.-P.P. Richard
[24] L. Wolsey. Valid inequalities and superadditivity for 0/1 integer programms.
Mathematics of Operations Research, 2:66–77, 1977.
[25] B. Zeng and J.-P.P Richard. Sequentially lifted valid inequalities for 0−1
knapsack problem with disjoint cardinality constraints. Technical report,
Purdue University, 2006.
[26] B. Zeng and J.-P.P Richard. Sequence independent lifting for 0−1 knapsack
problem with disjoint cardinality constraints.
Technical report, Purdue
University, 2006.

On the Exact Separation of Mixed Integer
Knapsack Cuts
Ricardo Fukasawa1 and Marcos Goycoolea2
1 H. Milton Stewart School of Industrial and Systems Engineering
Georgia Institute of Technology
rfukasaw@isye.gatech.edu
2 School of Business
Universidad Adolfo Iba˜nez
marcos.goycoolea@uai.cl
Abstract. During the last decades, much research has been conducted
deriving classes of valid inequalities for single-row mixed integer pro-
gramming polyhedrons. However, no such class has had as much practical
success as the MIR inequality when used in cutting plane algorithms for
general mixed integer programming problems. In this work we analyze
this empirical observation by developing an algorithm which takes as in-
put a point and a single-row mixed integer polyhedron, and either proves
the point is in the convex hull of said polyhedron, or ﬁnds a separating
hyperplane. The main feature of this algorithm is a specialized subroutine
for solving the Mixed Integer Knapsack Problem which exploits cost and
lexicographic dominance. Separating over the entire closure of single-row
systems allows us to establish natural benchmarks by which to evaluate
speciﬁc classes of knapsack cuts. Using these benchmarks on Miplib 3.0
instances we analyze the performance of MIR inequalities. Computations
are performed in exact arithmetic.
Keywords: cutting plane algorithms, integer programming.
1
Introduction
Consider positive integers n, m and let d ∈Qm, D ∈Qm×n, l ∈{Q ∪{−∞}}n
and u ∈{Q ∪{+∞}}n. Let I ⊆N := {1, . . ., n} and consider the mixed integer
set:
P = {x ∈Rn : Dx ≤d, l ≤x ≤u, xi ∈Z, ∀i ∈I}.
We say that a mixed integer knapsack set of the form,
K = {x ∈Rn : ax ≤b, l ≤x ≤u, xi ∈Z, ∀i ∈I}
with b ∈Q, a ∈Qn is implied by P if (a, b) is a non-negative linear combination
of rows obtained from (D, d). Observe that if K is implied by P, then P ⊆K.
Hence, any inequality which is valid for K is also valid for P. We henceforth call
such inequalities knapsack cuts derived from K.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 225–239, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

226
R. Fukasawa and M. Goycoolea
Deriving strong knapsack cuts is of great practical importance to Mixed Inte-
ger Programming (MIP). In fact, most cutting planes known for general mixed
integer programming are knapsack cuts. For example, Gomory Mixed Integer
cuts [19,28] are knapsack cuts derived from the tableaus of linear programming
relaxations, and Lifted Cover Inequalities [12,23] are knapsack cuts derived from
the original rows of P. Other classes of knapsack cuts include mixed-integer-
rounding (MIR) cuts and their variations [11,26,28], split cuts [10], lift-and-
project cuts [4], and group cuts [15,20] – to name but a few.
In this paper we discuss an empirical methodology for evaluating sub-classes
of knapsack cuts. Formally, consider P as deﬁned above, c ∈Qn, and C a set of
valid inequalities for P. Deﬁne,
z∗(C) = min{cx : Dx ≤d, l ≤x ≤u, πx ≤πo ∀(π, πo) ∈C}.
Observe that the value z∗(C) deﬁnes a benchmark by which to evaluate classes
of cuts that are subsets of C. For example, consider a family of implied knapsack
sets K and let CK represent the set of all knapsack cuts which can be derived from
some set K ∈K. Likewise, let MK represent the set of all MIR inequalities which
can be derived from some set K ∈K. Given that MK ⊆CK it is easy to see that
z∗(CK) ≥z∗(MK) and that the proximity of these two values gives an indication
of the strength of MIR inequalities derived from that particular family K.
In our computational experiments we will consider two speciﬁc families of
implied knapsack sets: The set F of all formulation rows of P ; and, given a
basic solution of the simplex algorithm, the set T of all tableau rows.
Boyd [8] and Yan and Boyd [30] compute z∗(CF) for a subset of pure and
mixed 0-1 instances in MIPLIB 3.0 [7]. Fischetti and Lodi [18] extend this result
by computing z∗(CA), where A is the set of all implied knapsack polyhedra, for
a similar test set of pure 0-1 problems.
In this paper we compute the values z∗(CF) and z∗(CT ) for a larger subset
of MIPLIB 3.0 instances, including general mixed integer problems. We compare
these values to estimates of z∗(MF) and z∗(MT ) (i.e., the bounds obtained by us-
ing MIR inequalities) and attempt to address the well acknowledged observation
that it is diﬃcult to identify classes of knapsack inequalities which systematically
outperform the MIR inequality in broad test sets. Recently, Dash and G¨unl¨uk [15]
also try to analyze this issue in terms of cuts from the cyclic group problem.
The organization of this paper is as follows. In the next section, we discuss how
to solve the problem of separating over a single mixed integer knapsack set. This
methodology described requires the use of a subroutine for solving the mixed
integer knapsack problem. An algorithm for solving this problem is discussed in
Sect. 3. Computational results are presented in Sect. 4, while ﬁnal remarks and
a discussion ensues in Sect. 5.
2
Identifying Violated Knapsack Cuts
Consider x∗∈Rn and a mixed integer knapsack set K. In this section we address
the following questions: Is x∗∈conv(K)? If not, can we ﬁnd an inequality
πx ≤πo which is valid for K, and such that πx∗> πo?

On the Exact Separation of Mixed Integer Knapsack Cuts
227
We assume that K has no free variables, since it is easy to substitute a free
variables by two non-negative variables. Let {x1, x2, . . . , xq} and {r1, r2, . . . , rt}
represent the extreme points and extreme rays of conv(K). The following propo-
sition, which follows from the work of Applegate et. al [1], allows us to address
this question.
Proposition 1. Consider the following linear programming (LP) problem with
variables u, v, π ∈Rn, and πo ∈R:
LP1 : min
n
i=1
(ui + vi)
s.t.
πxk −πo
≤0
∀k = 1 . . . q
(C1)
πrk
≤0
∀k = 1 . . . t
(C2)
πx∗−πo
= 1
(C3)
π + u −v
= 0
(C4)
u ≥0, v ≥0.
If this problem is infeasible, then x∗∈conv(K), and thus there exists no knap-
sack cut violated by x∗. Otherwise, this problem admits an optimal solution
(u, v, π, πo) such that inequality πx ≤πo is a valid knapsack cut maximizing:
πx∗−πo
||π||1
That is, the hyperplane deﬁned by (π, πo) maximizes the L1 distance to x∗.
Because LP1 has an exponential number of constraints, we use a dynamic cut
generation algorithm to solve the problem. We begin with constraints (C3)−(C4)
and a subset of constraints (C1) −(C2). The cut generation algorithm requires
solving the problem max{πx : x ∈K} at each iteration. If this problem is
unbounded at any given iteration, then there exits an extreme ray rj of conv(K)
such that πrj > 0. That is, we have identiﬁed a violated constraint. If this
problem is not unbounded, then there exists an optimal solution corresponding
to an extreme point xk of conv(K). If πxk > πo then we have found a violated
constraint. Otherwise, it means that all constraints of the problem are satisﬁed.
Solving the oracle problem is discussed in Sect. 3.
Notice that in general, it is not possible to assure that the solution of max{πx :
x ∈K} given by the oracle will correspond to an extreme point or ray of
conv(K). However, constraints (C1) −(C2) can be re-deﬁned in terms of all
points/rays of K without aﬀecting the correctness of Proposition 1. Even though
this would result in an inﬁnite number of constraints, under very mild assump-
tions [17], the dynamic cut generation algorithm will still converge in a ﬁnite
number of iterations.
In order to speed up the solution of LP1 we make use of certain characteriza-
tions of violated knapsack cuts.
Let K = {x ∈Rn : ax ≤b, l ≤x ≤u, xi ∈Z, ∀i ∈I}. We may assume
without loss of generality [21] that the bound constraints are tight. Say that a

228
R. Fukasawa and M. Goycoolea
knapsack cut for K is trivial if it is implied by the linear programming relaxation
of K. A proof of the following result concerning non-trivial knapsack cuts can
be found in Atamt¨urk [3].
Proposition 2. Every non-trivial facet-deﬁning knapsack cut πx ≤πo of
conv(K) satisﬁes the following properties:
(i) If ai > 0, πi ≥0
(ii) If ai < 0, πi ≤0
(iii) πi = 0 for all i /∈I such that ai > 0 and ui = +∞.
(iv) πi = 0 for all i /∈I such that ai < 0 and li = −∞.
(v) There exists a constant α > 0 such that πi = αai for all i /∈I such that
ai > 0 and li = −∞, and for all i /∈I such that ai < 0 and ui = +∞.
The following result concerning violated and non-trivial knapsack cuts is a simple
generalization of a technique employed in Boyd [8].
Proposition 3. Consider x∗/∈conv(K). Let H+ = {i ∈N : ai > 0, x∗
i = li}
and H−= {i ∈N : ai < 0, x∗
i = ui}. If there does not exist a trivial inequality
separating x∗from conv(K), then there exists a knapsack cut πx ≤πo such that
πi = 0, ∀i ∈H+ ∪H−.
We make use of Propositions 2 – 3 in the following way: We restrict the signs
of coeﬃcients according to Proposition 2 items (i) and (ii). Coeﬃcients πi with
i = 1, . . . , n which can be assumed to be zero are eliminated from LP1. Further,
a single variable is used for all coeﬃcients πi with i = 1, . . . , n for which we
know that πi = αai. Note that this last reduction is equivalent to aggregating
the unbounded continuous variables into a single variable.
Two other techniques are used to speed up the separation process. The ﬁrst
one uses the fact that MIR inequalities are knapsack cuts. With that in mind,
we ﬁrst apply an MIR separation heuristic to try to ﬁnd violated knapsack cuts
and only use the above separation procedure if the MIR heuristic fails.
The other technique relies on the following simple observation. Let U = {i ∈
N : x∗
i = ui} and L = {i ∈N : x∗
i = li}. If we deﬁne,
K∗= K ∩{x : xi = ui ∀i ∈U} ∩{x : xi = li ∀i ∈L},
we know that x∗∈conv(K) iﬀx∗∈conv(K∗). Thus, answering the question:
“Is x∗∈conv(K)?” can be done in a space of usually much smaller dimension
by testing instead if x∗∈conv(K∗).
If our test shows that x∗∈conv(K∗), we are done with the separation since
we know that in this case x∗∈conv(K). However, if x∗/∈conv(K∗) we still need
to get a cut separating x∗from conv(K) and thus we have to run our separation
algorithm in the original space. Notice, however, that if x∗/∈conv(K∗), our
separation algorithm will return a cut separating x∗from conv(K∗), so one
could potentially lift this cut to obtain a cut separating x∗from conv(K). We
have not implemented this feature yet, but we expect that it will signiﬁcantly
speed up our algorithm.
To summarize, we outline the complete algorithm below:

On the Exact Separation of Mixed Integer Knapsack Cuts
229
Algorithm 1. Outline of knapsack separation process
Input: x∗and K
Output: x∗∈conv(K) or a cut separating x∗from conv(K)
begin
Run the MIR separation heuristic
if cut found then
return the MIR cut separating x∗from conv(K)
else
Apply Propositions 1 and 2 to simplify LP1
Solve LP1 in a reduced space to separate x∗from conv(K∗)
if x∗∈conv(K∗) then
return x∗∈conv(K)
else
Solve LP1 in the original variable space to separate x∗from conv(K)
end
3
Solving the Mixed Integer Knapsack Problem
In this section we are concerned with the problem of solving the Mixed Integer
Knapsack Problem (MIKP),
max{cx : x ∈K}
(1)
We will assume that the problem is feasible, and are interested in either (a)
proving that the problem is unbounded by ﬁnding an extreme ray r∗of conv(K),
or (b) computing the optimal value of the problem by ﬁnding the optimal solution
x∗∈K.
Variants of MIKP have long been studied in the research literature. In these
it is typically assumed that all coeﬃcients deﬁning the problem are integer, that
all variables must take integer values (i.e. no continuous variables are allowed),
and that li = 0 for all i = 1, . . . , n. In addition: In the Knapsack Problem (KP)
ui = 1 for all i = 1, . . . , n, in the Bounded Knapsack Problem (BKP) ui < ∞
for all i = 1, . . . , n, and in the Unbounded Knapsack Problem (UKP) ui = ∞
for all i = 1, . . . , n. Most modern algorithms for solving KP, BKP, and UKP are
based either on branch and bound (following the work of Horowitz and Sahni
[24]) and on dynamic programming (following the work of Bellman [6]). However,
the most eﬃcient codes seldom make explicit use of Linear Programming and in
addition, they never consider the use of both integer and continuous variables.
For excellent surveys describing the rich literature on this topic, the reader is
advised to consult Kellerer et al [25] and Martello and Toth [27].
While it is reasonable to expect that many of these algorithms could be
adapted for solving our general case with a mix of continuous, integer, bounded
and unbounded variables, the fact that they are designed to work with integer co-
eﬃcients raises certain concerns with regards to the application discussed in this

230
R. Fukasawa and M. Goycoolea
paper. In fact, part of our motivation is to study the eﬃcacy of cuts derived from
tableau rows. However, these rows are rarely are made up of integer coeﬃcients,
and whats more, they are typically very ill conditioned. Thus, scaling them so
as to obtain integers may result in extremely large numbers. Considering this
important shortcoming, and the need to further study these algorithms in order
to account for the mixed use of bounded, unbounded, continuous and integer
variables, our approach has been to pursue an LP-based branch and bound ap-
proach, which seems naturally suited to mixed integer programming problems.
This issue, however, is one which merits further research. In what follows we
describe our algorithm for solving MIKP.
Detecting Unbounded Solutions
For each i ∈1, . . . , n deﬁne the eﬃciency of variable xi as ei = ci/ai if ai ̸= 0,
as ei = +∞if ai = 0 and ci > 0, and as ei = −∞if ai = 0 and ci < 0. In
addition, we say that xi is a potentiator if,
(ai ≤0, ci > 0, ui = +∞) or (ai ≥0, ci < 0, li = −∞).
We say that xi is an incrementor if,
(ai > 0, ci > 0, ui = +∞) or (ai < 0, ci < 0, li = −∞).
We say that xi is a decrementor if,
(ai > 0, ci ≥0, li = −∞) or (ai < 0, ci ≤0, ui = +∞).
By identifying a potentiator, or instead, by identifying the most eﬃcient in-
crementor and the least eﬃcient decrementor, it is possible to easily establish if
a problem is unbounded, as shown by the following Proposition:
Proposition 4. MIKP is unbounded if and only if one of the following condi-
tions hold,
• MIKP admits a potentiator xj.
• MIKP admits an incrementor xi and a decrementor xj such that ei > ej.
Note that Proposition 4 implies that it can be determined if MIKP is unbounded
in linear time. Note also that once the potentiator, or instead, the incrementor
and decrementor have been identiﬁed, it is easy to construct an extreme ray of
conv(K).
Preprocessing
We consider the following four-step preprocessing algorithm (see [21],[29]) which
assumes the problem is not unbounded.
1. Fix to ui all variables xi such that ci ≥0 and ai ≤0. Fix to li to all variables
xi such that ci ≤0 and ai ≥0.

On the Exact Separation of Mixed Integer Knapsack Cuts
231
2. Make all bounds as tight as possible.
3. Aggregate variables. If two variables xi, xj of the same type (integer or con-
tinuous) are such that ai = aj and ci = cj aggregate them into a new variable
xk of the same type such that ak = ai = aj, ck = ci = cj, lk = li + lj and
uk = ui + uj.
4. Sort variables in order of decreasing eﬃciency. Break ties checking for vari-
able types (integer or continuous).
Branch and Bound
We use a depth-ﬁrst-search branch and bound algorithm which always branches
on the unique fractional variable. We use a simple linear programming algorithm,
a variation of Dantzig’s algorithm [13] , which runs in linear time by taking ad-
vantage of the fact that variables are sorted by decreasing eﬃciency. We do not
use any cutting planes in the algorithm, nor any heuristics to generate feasi-
ble solutions. The algorithm uses variable reduced-cost information to improve
variable bounds at each node of the tree.
Domination
Consider x1 and x2, two feasible solutions of MIKP. We say that x1 cost-
dominates x2 if cx1 > cx2 and ax1 ≤ax2. On the other hand, we say that
x1 lexicographically-dominates x2 if cx1 = cx2 and ax1 ≤ax2, and if in addi-
tion, there exists i ∈1, . . . , n such that x1
i < x2
i and x1
k = x2
k, ∀k ∈1, . . . , (i−1).
We say that a solution is dominated if it is cost-dominated or lexicographically-
dominated. Observe that there exists a unique non-dominated optimal solution
(or none at all).
Traditional branch and bound algorithms work by pruning nodes when (a)
they are proven infeasible, or (b) when it can be shown that the optimal solution
in those nodes has value worse than a bound previously obtained. In our imple-
mentation, we additionally prune nodes when (c) it can be shown that every
optimal solution in those nodes is dominated.
Using dominance to improve the branch and bound search can have an im-
portant impact on the eﬀectiveness of the search. In fact, lexicographic and cost
dominance allow us to disregard feasible solutions that are not the unique lexi-
cographically smallest optimum solution, hence signiﬁcantly reducing the search
space.
In general, the problem of detecting if a solution is dominated can be ex-
tremely diﬃcult. In what follows we describe a simple methodology for identify-
ing speciﬁc cases of domination.
Consider indices i, j ∈I, and non-zero integers ki, kj. If aiki + ajkj ≥0 and
ciki+cjkj < 0 we say that (i, j, ki, kj) deﬁnes an integer cost-domination tuple. If
ki ≥0, aiki +ajkj ≥0 and ciki +cjkj = 0 we say that (i, j, ki, kj) deﬁnes an in-
teger lexicographic-domination tuple. Observe that whenever (ci, ai) and (cj, aj)
are linearly independent there exist an inﬁnite amount of cost-domination pairs.
Likewise, there exist an inﬁnite amount of lexicographic-domination tuples in

232
R. Fukasawa and M. Goycoolea
the linear dependence case. However, in each case, there always exists a minimal
domination tuple. That is, a domination tuple (i, j, ki, kj) such that all other
domination tuples (i, j, k′
i, k′
j) deﬁned for the same variables, satisfy |ki| ≤|k′
i|
and |kj| ≤|k′
j|. The propositions below show how domination tuples allow for
the easy identiﬁcation of dominated solutions.
Proposition 5. Consider an integer cost-domination tuple (i, j, ki, kj) and let
x be a feasible MIKP solution. If any of the following three conditions hold:
• ki > 0, kj > 0, xi ≥li + ki and xj ≥lj + kj,
• ki < 0, kj > 0, xi ≤ui + ki and xj ≥lj + kj,
• ki < 0, kj < 0, xi ≤ui + ki and xj ≤uj + kj.
Then x is cost-dominated.
Proposition 6. Consider an integer lexicographic-domination tuple (i, j, ki, kj)
and let x be a feasible MIKP solution. If either of the following conditions hold:
• kj > 0, xi ≥li + ki, and xj ≥lj + kj,
• kj < 0, xi ≥li + ki, and xj ≤uj + kj,
then x is lexicographically-dominated.
To see that these propositions are true, it is simply a matter of observing that if
the conditions hold for a feasible x, then deﬁning x′ so that x′
i = xi −ki, x′
j =
xj −kj and x′
k = xk for k ̸= i, j, we have x′ is feasible and dominates x.
The following propositions illustrate how domination tuples can be used to
strengthen branch and bound algorithm. This is achieved by preventing nodes
with dominated solutions from being created through additional enforced bound
changes.
Proposition 7. Consider two integer type variables xi and xj and a domination
tuple (i, j, ki, kj) such that ki > 0. If in some node of the branch and bound tree
we impose xi ≥li + αi, where αi ≥ki, then:
• If kj > 0 we can impose the constraint xj ≤lj + kj −1 in that node.
• If kj < 0 we can impose the constraint xj ≥uj + kj + 1 in that node.
The case ki < 0 is analogous.
In order to use the above propositions in the branch and bound algorithm
we compute what we call a domination table before initiating the solve. This
table is deﬁned as a list of all possible (minimal) domination tuples. Observe
that we only need store domination tuples (i, j, ki, kj) such that |ki| ≤(ui −li)
and |kj| ≤(uj −lj). In order to compute domination tuples we perform a simple
enumeration algorithm which uses bounds to identify where to start and stop
the enumerations.

On the Exact Separation of Mixed Integer Knapsack Cuts
233
 0
 200
 400
 600
 800
 1000
 1200
 1400
 1600
 0.01
 0.1
 1
 10
 100
 1000
 10000
Number of instances solved
Time (s)
’kbb’
’cpx’
Fig. 1. Histogram comparing KBB algorithm with CPLEX
4
Computational Experiments
In this section, our computational experiments are described. All implementa-
tions were compiled using the “C” and “C++” programming languages, using
the Linux operating system (v2.4.27) and Intel Xeon dual-processor computers
(2GB of RAM, at 2.66GHz). Since generating cuts which are invalid is a real
point of concern, we found it appropriate to use the exact arithmetic, both for
solving LP1, and for the MIKP oracle. Thus, we used Applegate et al. [2] exact
LP solver for LP1, and the GNU Multiple Precision (GMP) Arithmetic library
[22] to implement the MIKP algorithm.
4.1
The Optimization Oracle
We ﬁrst compare the performance of our MIKP algorithm (“kbb”) with the
performance of CPLEX 9.0 (“cpx”), the only alternative for MIKP we know of
to date. Note that CPLEX was ran with all its default settings, except for the
tolerance, which was set to 10−6. Note also that our MIKP algorithm was ran
using double ﬂoating arithmetic, with a tolerance of 10−6.
In our ﬁrst implementation of the separation algorithm we had incorporated a
version of kbb which did not use domination branching. We quickly realized that
this algorithm was not eﬃcient enough. When running this version of the code,
we saved all problems which took our algorithm more than 2.0 seconds to solve.
These are the 1,556 problems that we now use to compare cpx with the full version
of kbb. It is important to note that by the nature of the way these instances were
generated, there might be some of instances that are very similar to each other.
In Fig. 1 we present a histogram summarizing the running times of kbb and
cpx. Each point in the curves represents the number of instances which were
solved within a given maximum time. For instance, note that the number of
instances solved to optimality by kbb within a second is roughly 1150, whereas

234
R. Fukasawa and M. Goycoolea
the number of instances solved to optimality by cpx is roughly 700. Note that
the time is represented in logarithmic scale. Further, observe that the hardest
instance for kbb takes several hundred seconds – roughly ten times less than the
hardest instance for cpx.
It is clear from this histogram that the kbb algorithm outperforms cpx in
the instance set. Note that this does not necessarily mean that kbb solves every
instance faster than cpx, but rather, that cumulatively, kbb performs better.
In fact, on average, kbb takes 81% less time than cpx, and explores 37.5% less
branch-and-bound nodes. Moreover, in 49 instances, CPLEX fails to ﬁnd the
optimum solution since it runs out of memory after creating too large a branch
and bound tree.
4.2
Knapsack Cuts
We next use an implementation of the algorithms presented in Sect. 2 and Sect.
3 to compare the practical performance of MIR cuts against the performance of
separating all possible knapsack cuts. As detailed in Sect. 1, given a family K of
knapsack sets implied by P, such a comparison can be made by comparing the
values z∗(CK) and z∗(MK). In this article we only consider the set K = F, i.e.,
the family of knapsack sets induced by the original formulation rows, and the
set K = T , i.e., the family of knapsack sets induced by the simplex tableau rows
of the optimal LP solution for the original LP relaxation.
Computing z∗(MK) is NP-hard [9], so instead we approximate this value
using an MIR separation heuristic. Given a point x∗, for every K ∈K we try to
ﬁnd MIR inequalities that are violated by x∗. We add these inequalities to the
LP relaxation of P and repeat the process until no more MIR inequalities are
found. The MIR inequalities for each K are derived by a separation heuristic
which combines scaling and variable complementation techniques (for details see
[21], [26], and [14]). Denote by zK
M the objective function value at the end of the
procedure. Since this is just a heuristic, after completing a run, there may be
violated MIR inequalities which have not been identiﬁed. Therefore zK
M should
be considered an estimate of z∗(MK).
Note that though the MIR separation problem is NP-hard, one could use the
approaches of Balas and Saxena [5] or Dash, G¨unl¨uk and Lodi [16] to better
approximate z∗(MK).
To compute z∗(CK), we proceed as follows. Given a fractional solution, we
loop through all of the mixed integer knapsack sets K ∈K. For each of these
we invoke the procedure outlined in Sect. 2 and identify a violated cut if such
exists. After completing this loop we add the cuts to the problem and repeat.
The procedure ends when for every K we can prove that there is no violated
knapsack cut.
Computational tests are performed on all MIPLIB 3.0 instances using the
mixed integer knapsack sets K = F and K = T . For each problem instance let
z∗
UB represent the value of the optimal (or best known) solution and z∗
LP the
LP relaxation value. For each set K and each instance we compute the following
performance measures:

On the Exact Separation of Mixed Integer Knapsack Cuts
235
LP-PERF: Performance of the original LP formulation. That is, the value of
the LP relaxation gap:
z∗
UB −z∗
LP
|z∗
UB|
.
KNAP-PERF: Performance of the knapsack cuts. That is, how much of the
LP gap was closed by the knapsack cuts:
z∗(CK) −z∗
LP
z∗
UB −z∗
LP
.
MIR-PERF: Performance of MIR separation heuristic. That is, how much of
the LP gap closed by the knapsack cuts was closed by the MIR cuts:
zK
M −z∗
LP
z∗(CK) −z∗
LP
Knapsack Cuts Derived from Formulation Rows
In this section we analyze the performance of knapsack and MIR inequali-
ties on formulation rows of MIPLIB 3.0 instances. Results are summarized in
Table 1. Of the 59 instances in the library, we eliminated eight instances which
were unﬁnished at the time of writing the article (arki001, cap6000, dano3mip,
harp2, mitre, mod008, pk1 and rout), three for which LP-PERF was equal to
0.0 (dsbmip, enigma, and noswot), and thirty two for which KNAP-PERF and
MIR-PERF were both equal to 0.0.
Table 1. Benchmarks for Formulation Closure
Instance LP-PERF KNAP-PERF MIR-PERF
ﬁber
61.55%
93.82%
97.06 %
gen
0.16%
99.78%
100.00 %
gesa2
1.18%
71.03%
98.48 %
gesa3
0.56%
49.33%
96.90 %
gt2
36.41%
94.52%
97.93 %
l152lav
1.39%
1.36%
0.41 %
lseu
25.48%
76.09%
88.25 %
mod010
0.24%
18.34%
100.00 %
p0033
18.40%
87.42%
87.31 %
p0201
9.72%
33.78%
100.00 %
p0282
31.56%
98.59%
95.42 %
p0548
96.37%
84.34%
62.76 %
p2756
13.93%
86.35%
51.49 %
qnet1
10.95%
89.06%
56.68 %
qnet1 o
24.54%
95.12%
88.65 %
rgn
40.63%
57.49%
100.00 %

236
R. Fukasawa and M. Goycoolea
First, note that knapsack cuts alone can considerably close the remaining LP
gap in some problems (column KNAP-PERF). In fact, in 9 problems out of the
16 problems in which knapsack cuts improved the gap, over 84% of the gap was
closed, and in 14 out of 16 problems, over 50 % of the gap was closed. On average,
the GAP closed by the knapsack cuts among these 16 instances is around 71%.
It is interesting, however, that in thirty two instances knapsack cuts should do
nothing to improve the gap. If in addition we consider in our average the thirty
two instances for which KNAP-PERF is 0.0%, this drops to 23.66%.
Second, consider the column MIR in which we can get an idea of how well
the mixed integer rounding cut closure compares to the knapsack cut closure.
Observe that of the 16 problems, in 12 of them, by using the MIR cuts alone, we
close over 87% of the GAP closed by the knapsack cuts. This indicates that MIR
inequalities are a very important subset of knapsack inequalities; at least for the
instances considered. A natural question is the following: How much could we
improve the value of MIR-PERF if we used an exact MIR separation algorithm
as opposed to a heuristic? In an attempt to answer this question we ﬁne-tuned
the settings of the MIR heuristic for the problems p0033 and qnet1. In these,
we managed to improve the value of MIR-PERF from 87.31% to 100% and from
56.68% to 77.27% respectively.
Knapsack Cuts Derived from Tableau Rows
In this section we analyze the performance of knapsack and MIR inequalities
on tableau rows of MIPLIB 3.0 instances. For this we compute z∗
LP and store
the tableau rows in the set of knapsack polyhedra K = T , which we use for all
subsequent computations. Results are summarized in Table 2. Of the 59 instances
in the library, we eliminated thirty two instances which were unﬁnished at the
time of writing the article, three for which LP-PERF was equal to 0.0 (dsbmip,
enigma, and noswot), and two for which KNAP-PERF and MIR-PERF were
both equal to 0.0 (stein27 and stein45).
First, it is important to remark that separating knapsack cuts from tableau
rows is considerable more diﬃcult than separating knapsack cuts from original
formulation rows. This is due to several reasons: Tableau rows are typically much
more dense, coeﬃcients tend to be numerically very bad, and rows tend to have
a lot of continuous variables. This added diﬃculty is reﬂected in the fact that out
of 59 instances, in two days of runs we just managed to solve 24 instances to com-
pletion, as opposed to the 48 which we solved when considering formulation rows.
Second, it is interesting to note that the value KNAP-PERF is very erratic,
uniformly ranging in values from 100% to 0.0%. In contrast to the case of for-
mulation rows, only two instances are such that KNAP-PERF is 0.0%.
The last, and perhaps most startling observation, is that the MIR-PERF is
always at 100%, if not very close. If this result were true in general, it would
be very surprising. However, because there are still thirty two instances which
have not been solved one must be very careful. Because of the way in which we
computed these numbers, it could be the case that those instances with MIR-
PERF close to 100% are easier for our methodology to solve. It is very reasonable

On the Exact Separation of Mixed Integer Knapsack Cuts
237
Table 2. Benchmarks for Tableau Closure
Instance
LP-PERF KNAP-PERF MIR-PERF
air03
0.38 %
100.00 %
100.00%
bell3a
1.80 %
60.15 %
100.00%
bell5
3.99 %
14.68 %
98.94%
dcmulti
2.24 %
50.49 %
99.94%
egout
73.67 %
55.33 %
100.00%
ﬁxnet6
69.85 %
11.08 %
100.00%
ﬂugpl
2.86 %
11.74 %
100.00%
gesa2
1.18 %
28.13 %
99.98%
gesa2 o
1.18 %
29.65 %
99.67%
khb05250
10.31 %
75.14 %
100.00%
misc03
43.15 %
7.24 %
100.00%
misc06
0.07 %
26.98 %
100.00%
misc07
49.64 %
0.72 %
100.00%
modglob
1.49 %
18.05 %
100.00%
p0033
18.40 %
74.71 %
100.00%
p0201
9.72 %
34.36 %
100.00%
pp08a
62.61 %
50.97 %
100.00%
qiu
601.15 %
3.47 %
100.00%
rgn
40.63 %
9.78 %
100.00%
set1ch
41.31 %
39.18 %
100.00%
vpm1
22.92 %
49.09 %
96.30%
vpm2
28.08 %
19.39 %
98.85%
to expect that instances with MIR-PERF well below 100% are more diﬃcult to
solve as they require more iterations of the knapsack separation algorithm as
opposed to iterations of the MIR separation heuristic.
5
Final Remarks
It is important to note that these results are very preliminary. We put great
care into ensuring that the generated cuts are valid and that the procedure runs
correctly, but this makes the methodology very slow. For example, some of the
KNAP-PERF values computed took as much as 5 days to obtain. Some of the
unsolved instances have been ran for over a week without a ﬁnal answer being
reported. We are currently developing further techniques by which these compu-
tations can be accelerated. Part of the diﬃculty arises from the fact that exact
arithmetic is being employed. In average, we have observed that performing ex-
act arithmetic computations take 100 times longer than ﬂoating point arithmetic
computations.
One of the main goals of this study has been to assess the overall eﬀectiveness
of MIR inequalities relative to knapsack cuts. The motivation being the empirical
observation that though much research has been conducted studying inequalities
derived from single row systems, no such class of inequalities has been able

238
R. Fukasawa and M. Goycoolea
to systematically improve upon the performance of MIRs. In this regard, the
results we present are surprising. We observe that in most test problems, the
bound obtained by optimizing over the MIR closure is very similar in value (if
not equal) to the bound obtained optimizing over the knapsack closure. Though
it is important to note that this observation is limited in the number of test
problems considered, it does help explain the lack of success in generating other
cuts from tableau and formulation rows, and, suggests that for further bound
improvements we might have to consider new row aggregation schemes, or cuts
derived from multiple row systems.
References
1. D. Applegate, R. E. Bixby, V. Chv´atal, and W. Cook. TSP cuts which do not
conform to the template paradigm. In Computational Combinatorial Optimization,
Optimal or Provably Near-Optimal Solutions [based on a Spring School], pages 261–
304, London, UK, 2001. Springer-Verlag GmbH.
2. D. Applegate, W. Cook, S. Dash, and D. Espinoza.
Exact solutions to linear
programming problems. Submitted to Operations Research Letters, 2006.
3. A. Atamt¨urk. On the facets of the mixed–integer knapsack polyhedron. Mathe-
matical Programming, 98:145–175, 2003.
4. E. Balas and M. Perregaard. A precise correspondence between lift-and-project
cuts, simple disjuntive cuts, and mixed integer Gomory cuts for 0-1 programming.
Mathematical Programming, 94:221–245, 2003.
5. E. Balas and A. Saxena. Optimizing over the split closure. Mathematical Program-
ming, To appear.
6. R. E. Bellman. Dynamic Programming. Princeton University Press, 1957.
7. R. E. Bixby, S. Ceria, C. M. McZeal, and M. W. P Savelsbergh. An updated mixed
integer programming library: MIPLIB 3.0. Optima, (58):12–15, June 1998.
8. A. E. Boyd. Fenchel cutting planes for integer programs. Operations Research,
42:53–64, 1992.
9. A. Caprara and A. Letchford. On the separation of split cuts and related inequal-
ities. Mathematical Programming, 94(2-3):279–294, 2003.
10. W. Cook, R. Kannan, and A. Schrijver. Chv´atal closures for mixed integer pro-
gramming problems. Mathematical Programming, 47:155–174, 1990.
11. G. Cornu´ejols, Y. Li, and D. Vanderbussche. K-cuts: A variation of gomory mixed
integer cuts from the LP tableau. Informs Journal On Computing, 15:385–396,
2003.
12. H. Crowder, E.L. Johnson, and M. Padberg. Solving large-scale zero-one linear-
programming problems. Operations Research, 31:803–834, 1983.
13. G. B. Dantzig.
Discrete variable extremum problems.
Operations Research,
5(2):266–277, 1957.
14. S. Dash, M. Goycoolea, and O. G¨unl¨uk. Two-step mir inequalities for mixed-integer
programs. Optimization Online, Jul 2006.
15. S. Dash and O. G¨unl¨uk. On the strength of gomory mixed-integer cuts as group
cuts. IBM research report RC23967, 2006.
16. S. Dash, O. G¨unl¨uk, and A. Lodi. MIR closures of polyhedral sets. Available online
at http://www.optimization-online.org/DB HTML/2007/03/1604.html.

On the Exact Separation of Mixed Integer Knapsack Cuts
239
17. D. G. Espinoza.
On Linear Programming, Integer Programming and Cutting
Planes. PhD thesis, School of Industrial and Systems Engineering, Georgia In-
stitute of Technology, March 2006.
18. M. Fischetti and A. Lodi. On the knapsack closure of 0-1 integer linear problems.
Presentation at 10th International Workshop on Combinatorial Optimization, Aus-
sois (2006). Available at http://www-id.imag.fr/IWCO2006/slides/Fischetti.pdf.
19. R. E. Gomory. Early integer programming (reprinted). Operations Research, 50:
78–81, Jan 2002.
20. R. E. Gomory and E.L. Johnson.
Some continuous functions related to corner
polyhedra I. Mathematical Programming, 3:23–85, 1972.
21. M. Goycoolea. Cutting Planes for Large Mixed Integer Programming Models. PhD
thesis, Georgia Institute of Technology, 2006.
22. T. Granlund. The GNU multiple precision arithmetic library. Available on-line at
http://www.swox.com/gmp/.
23. Z. Gu, G. L. Nemhauser, and M. W. P. Savelsbergh. Lifted cover inequalities for 0-
1 integer programs: Computation. INFORMS Journal on Computing, 10:427–437,
1998.
24. E. Horowitz and S. Sahni. Computing partitions with applications to the knapsack
problem. Journal of the ACM, 21:277–292, 1974.
25. H. Kellerer, U. Pferschy, and D. Pisinger. Knapsack Problems. Springer, Berlin,
Germany, 2004.
26. H. Marchand and L.A. Wolsey. Aggregation and mixed integer rounding to solve
MIPs. Operations Research, 49:363–371, 2001.
27. S. Martello and P. Toth. Knapsack Problems: Algorithms and Computer Imple-
mentations. J. Wiley, New York, 1990.
28. G. L. Nemhauser and L. A. Wolsey. A recursive procedure for generating all cuts
for 0-1 mixed integer programs. Mathematical Programming, 46:379–390, 1990.
29. M.W.P. Savelsbergh. Preprocessing and probing for mixed integer programming
problems. ORSA Journal on Computing, 6:445–454, 1994.
30. X. Q. Yan and E. A. Boyd. Cutting planes for mixed-integer knapsack polyhedra.
Mathematical Programming, 81:257–262, 1998.

M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 240–251, 2007. 
© Springer-Verlag Berlin Heidelberg 2007 
A Faster Strongly Polynomial Time Algorithm for  
Submodular Function Minimization 
James B. Orlin 
Sloan School of Management, MIT 
Cambridge, MA  02139 
jorlin@mit.edu 
Abstract.  We consider the problem of minimizing a submodular function f 
defined on a set V with n elements.  We give a combinatorial algorithm that 
runs in O(n5 EO + n6) time, where EO is the time to evaluate f(S) for some S ⊆ 
V.  This improves the previous best strongly polynomial running time by more 
than a factor of n. 
1   Introduction 
Let V = {1, 2, …, n}.  A set function f on V is said to be submodular if the following 
is true:  
f (X)+ f (Y ) ≥f (X ∪Y )+ f (X ∩Y )   for all subsets X,Y ⊆V. 
(1) 
Here we consider the problem of Submodular Function Minimization (SFM), that is, 
determining a subset S ⊆ V that minimizes f( ).  Our contribution is to develop a 
strongly polynomial time algorithm for SFM that improves upon the best previous 
time bounds by a factor greater than n. 
For a given subset X ⊆V, let fX (Y ) = f (X ∪Y )−f (X). It is elementary and well 
known that for fixed X, the function fX( ) is submodular whenever f( ) is submodular. 
An equivalent way of defining submodularity is as follows. 
For all subsets X, Y of V, and for each element v ∉ (X ∪ Y), if X ⊆ Y then 
fY(v) ≤ fX(v). 
In this way, submodular functions model decreasing marginal returns, and are 
economic counterparts of concave functions.  Nevertheless, Lovasz [11] showed that 
they behave algorithmically more similarly to convex functions, and provided 
analysis on why this is true. 
Examples of submodular functions include cut capacity functions, matroid rank 
functions, and entropy functions.   For additional examples of submodular functions 
and for applications of SFM see McCormick [12], Fleischer [4], Fushishige [6], and 
Schrijver [14]. 
We assume without loss of generality that f(∅) = 0.  Otherwise, if f(∅) ≠ 0, we can 
subtract f(∅) from f(S) for all S ⊆ V.    
Grotschel, Lovasz, and Schrijver [7] and [8] gave the first polynomial time and 
strongly polynomial time algorithms for minimizing a submodular function.  Their 

 
A Faster Strongly Polynomial Time Algorithm for SFM 
241 
algorithms rely on the ellipsoid algorithm.  Schrijver [13] and Iwata, Fleischer, and 
Fujishige [10] independently developed strongly polynomial time combinatorial 
algorithms for minimizing a submodular function.  Both algorithms build on the work 
of Cunningham [1], who developed a pseudo-polynomial time algorithm for 
minimizing a submodular function.   
Let EO be the maximum amount of time it takes to evaluate f(S) for a subset S ⊆ V.  
EO stands for evaluation of the oracle function, as per McCormick [12].  In general, 
one expects EO to be at least n since the input size is Ω(n); however, this running time 
can sometimes be improved in an amortized sense if one is evaluating EO multiple 
times consecutively, as is done by many of the SFM algorithms including the one 
presented here. Let M be an upper bound on |f(S)| for all S ⊆ V. 
The running times of the algorithms of Schrijver [13] and Iwata, Fleischer, and 
Fujishige [10] were shown to be O(n8 EO + n9).  Fleischer and Iwata [5] improved the 
running time of the combinatorial algorithms to O(n7 EO + n8).  Vygen [15] showed 
that the running time of Schrijver’s original algorithm was also O(n7 EO + n8). 
Subsequently Iwata [9] developed a scaling based algorithm whose running time is 
O(n4 EO log M + n5 log M).  To date, the best strongly polynomial time combinatorial 
algorithm for SFM was the strongly polynomial version of Iwata’s algorithm, which 
runs in O((n6 EO + n7) log n) time. 
We present a new approach for solving submodular minimization.  As have 
previous approaches, our algorithm relies on expressing feasible points in the base 
polyhedron as a convex combination of extreme points.  However, our algorithm 
works directly with vectors of the base polyhedron rather than relying on an auxiliary 
network, or on augmenting paths, or on flows.    
We present a strongly polynomial time algorithm that runs in O(n5 EO + n6) steps, 
thus improving upon Iwata’s time bound by a factor of n log n.  This also improves 
upon the best strongly polynomial time implementation of the ellipsoid algorithm for 
SFM, which runs in  O(n5 EO + n7) as reported by McCormick [12], where 
 O indicates that factors of log n may have been omitted from the time bound.   Most 
of the proofs in this manuscript are omitted.  A complete draft including the proofs is 
available on the author’s website. 
2   The Base Polyhedron 
For a vector 
| |
V
x ∈\
, let x(v) denote the v-th component.  We let 
( )
x
v
−
 =  
min {0, x(v)}.  For a subset S ⊆ V, , we let x(S) =
x(v).
v∈S
∑
 
The base polyhedron is 
( )
{ |
, ( )
( ),
: ( )
( )}.
n
B f
x x
x V
f V
S
V
x S
f S
=
∈
=
∀
⊆
≤
\
 
A vector in B( f ) is called a base.  An extreme point of B( f )is called an extreme 
base.  Edmonds 3 established the following duality theorem, which Cunningham 1 
used to develop a pseudo-polynomial time algorithm for SFM.  Subsequently all other 
efficient algorithms for SFM use the following duality theorem or a closely related 
result.  

242 
J.B. Orlin 
Theorem 1 (Edmonds).  For a submodular function 
: 2
.
V
f
→\   
max{x−(V): x ∈B( f )} = min{ f (S):S ⊆V}. 
(2) 
The function x−( )  is not linear, and the optimizer of max{x−(V) : x ∈B( f )} is 
not, in general, an extreme point of the base polyhedron.  The polynomial time 
algorithms in [9] and [10] proceed by representing vectors in the base polyhedron as a 
convex combination of extreme bases of the base polyhedron. 
An extreme base can be computed by the greedy algorithm of Edmonds and 
Shapley [3] as follows:  Let L = {v1, …, vn} be any linear ordering (permutation) of 
the elements of V.  In our notation, for each j, vj  is in the j-th position of the 
permutation.  The extreme base yL induced by L is obtained by letting  
yL(vj) = f({v1, …, vj}) – f({v1, …, vj-1}) for j = 1 to n. 
If P(j) = {v1, …, vj}, then we can also write yL(vj) = fP( j−1)(vj ) , which is the 
marginal contribution for f of adding vj to {v1, …, vj-1}. 
3   Distance Functions and Optimality Conditions  
A distance function is a mapping d :V →{0,1,...,n} . Each distance function d induces 
a linear order L(d) (denoted as  ≺d ) of V as follows:  u p d v  if d(u) < d(v) or if d(u) = 
d(v) and u < v. The extreme base induced by the order L(d) will be denoted as yd.  
In the algorithm presented in Section 5, at each iteration of the algorithm, we will 
maintain a collection D of O(n) different distance functions of V, a vector x in the 
base polyhedron, and a vector λ. The vectors x and λ satisfy the following: 
x =
λd
d∈D
∑
yd ,
λd
d∈D
∑
= 1, and λ ≥0.  
(3) 
We also write this as x = λDyD, where yD = {yd : d ∈ D}.  We let Dmin(v) be 
shorthand for min{d(v) : d ∈ D}.  We say that the triple  (x, λ, D) is valid if the 
following is true: 
1. If x(v) < 0, then d(v) = 0 for all d ∈ D; 
2. d(v) ≤ Dmin(v) + 1 for all d ∈ D and v ∈ V;  
The algorithm will maintain a valid triple (x, λ, D) throughout all iterations.  
Sometimes, we will just say that the collection D of distance functions is valid. 
Definition. We say that the quadruple (D, λ, x, S) satisfies the optimality conditions if 
it satisfies (3) and if it satisfies (4–6). 
x(v) ≤0  for v ∈S  
(4) 
x(v) ≥0  for v ∈V \ S  
(5) 
and 
\ .
 for all 
,
 
d
w
V
S
v
w
d
D v
S
∈
∈
∈
≺
 
(6) 

 
A Faster Strongly Polynomial Time Algorithm for SFM 
243 
We will also say that the triple (D, λ, x) satisfies the optimality conditions if there 
is a subset S ⊆ V such that (D, λ, x, S) satisfies the optimality conditions.  By (6), 
given any element d ∈ V, one can narrow the choice of S to n possibilities.   
Lemma 1 (Sufficiency of Optimality Conditions). If the quadruple (D, λ, x, S) for 
SFM satisfies the optimality conditions, then S is a minimum cost set, and x is an 
optimal base in the base polyhedron.  
Proof. By assumption, x is in the base polyhedron. Moreover, suppose without loss of 
generality that the elements are reordered so that S = {1, 2, …, |S| }. Then  
x−(V) = x−(S) = x(S) =
λd
d∈D
∑
yd(S) =
λd
d∈D
∑
f (S) = f (S). 
(7) 
Thus x−(V) = f (S) , and by Theorem 1, S is optimal.                                            ♦ 
Lemma 2 (Existence of Optimality Conditions). If S is a minimum cost set for 
SFM, then there is a quadruple (D, λ, x, S) for SFM that satisfies the optimality 
conditions.  
Proof. Let x be an optimal base in the base polyhedron. Moreover, suppose without 
loss of generality that the elements are reordered so that S = {1, 2, …, |S| }. Then  
x−(V) ≤x−(S) ≤x(S) =
λd
d∈D
∑
yd(S) =
λd
d∈D
∑
f (S) = f (S). 
(8) 
Since x−(V) = f (S),  it follows that  (D, λ, x, S) satisfies the optimality conditions.  
(We have not established that D is valid, but our algorithm will produce a valid D as 
well).                                                                                                                                   ♦ 
Definition. We say that the quadruple (D, λ, x, S) satisfies the partial optimality 
conditions if it satisfies (3) and if it satisfies (5) and (6). 
Lemma 3 (Partial Optimality Conditions).  If the quadruple (D, λ, x, S) for SFM 
satisfies the partial optimality conditions, then there is a minimum cost set S* ⊆ S.  
We first claim that if the partial optimality conditions are satisfied, then ∅ is an 
optimal set for fS among subsets of V\S.  If the claim is true then for any subset T of V,  
f(T) ≥ f(S∩ T) + f(S ∪ T) – f(S) = f(S ∩ T) + fS(T\S) ≥ f(S ∩ T). 
So, if the claim is true, then the Lemma is true.  We next prove the claim. 
For each d ∈ D, let ′yd  be the extreme base induced by d for the base polyhedron 
B(fS) defined over the set of elements u ∈ V\s, and let 
′x =
λd ′yd
d∈D
∑
.  We will 
show that for each d ∈ D and for each u ∈ V\S, ′yd(u) = yd(u) .  If this statement is 
true, it follows that x’(u) = x(u) for u ∈ V\S, and thus (D, λ, x’, ∅) satisfies the 
optimality conditions for fS over the set V\S and thus the claim is true. 
So, suppose that d ∈ D and u ∈ V\S.  Let P(d,u) = {v ∈V :v p d u}.  By (6), S ⊆ 
P(d, u). Thus 

244 
J.B. Orlin 
 
′yd(u) = fS(u + P(d,u) \ S)−fS(P(d,u) \ S)  
 
         = [ f (u + P(d,u))−f (S)]−[ f (P(d,u))−f (S)] = yd(u). 
This establishes that the claim is true, and thus the lemma is true.                          ♦ 
We will also say that the triple (D, λ, x) satisfies the partial optimality conditions if 
there is a subset S ⊆ V such that (D, λ, x, S) satisfies the partial optimality conditions. 
A sufficient condition for the partial optimality conditions to hold for valid 
distance functions D is the presence of a distance gap at level k, which is value k with 
0 < k < n such that  
1. there is some v with Dmin(v) = k, and  
2. there is no u with Dmin(u) = k -1.   
By letting S = {u ∈ V with Dmin(u) < k}, it is easy to verify that (D, λ, x, S) will 
satisfy the partial optimality conditions.  In such a case, we will eliminate all elements 
in V\S from the problem.  It would be possible to maintain these elements if we 
wanted to determine an optimal base, but they are not needed if we just want to 
determine a minimum cost set. 
4   Distance Functions and Extreme Vectors 
Suppose that d is a distance function.  We let INC(d, v) be the distance function 
obtained by incrementing the distance label of v by 1 and keeping all other distance 
labels the same.  That is, if d’ = INC(d, v), then 
′
d (u) =
d(v)+1    if u = v
d(u)
    if u ≠v
⎧
⎨
⎩
 .
 
Lemma 4. Suppose that d’ = INC(d, v).  Then  
1. y ′
d (v) ≤yd(v), 
2. y ′
d (u) ≥yd(u)   if  u ≠v. 
Proof. For each u ∈ V, let 
( )
{
:
}.
d
P u
w
V w
u
=
∈
≺
 Let 
( )
{
:
}.
d
P u
w
V w
u
′
′
=
∈
≺
 
Note that u ∉ P(u), and u ∉ P’(u).  Then for all u ∈ V, 
( )
( )
d
d
y
u
y
u
′
−
 
( )
( )
( )
( ).
P v
P v
f
u
f
u
′
=
−
 
Since 
P(v) ⊆
′
P (v) , 
it 
follows 
from 
the 
submodularity 
of 
f 
that 
f ′
P (v)(v) ≤fP(v)(v) , and so y ′
d (v)−yd (v) ≤0. Similarly, for u ≠ v, 
′
P (u) ⊆P(u) , 
and so fP(v)(u) ≤f ′
P (v)(u) .                                                                                          ♦ 
For any subset S ⊆ V, We let d(S) =
d(v)
v∈S
∑
.  We will maintain the distance 
functions in D in non-decreasing order of d(V).   
For each v ∈ V, we will maintain a primary distance function p(v) ∈ D, which is 
the first element d of D such that d(v) = Dmin(v).   By the way that we ordered the 

 
A Faster Strongly Polynomial Time Algorithm for SFM 
245 
elements of D, the primary distance function for v will minimize d(V) among all 
d ∈ D with d(v) = Dmin(v).  In addition, for every v ∈ V, we will maintain a secondary 
distance function s(v) = INC(p(v), v).  Our algorithm modifies x by increasing λs(v) and 
simultaneously decreasing λp(v) for v ∈ V.  
We maintain the order of D, and the functions p(v) and s(v) for all v by running the 
Procedure Update as follows: 
 
Procedure Update(D, p, s) 
begin 
    D := {d : λd > 0}; 
    order the vectors in D in non-decreasing order of d(V); 
    for each v ∈ V, let p(v) be the first element of D with d(v) = Dmin(v); 
    for each v ∈ V, let s(v) = INC(p(v), v); 
end 
5   A Strongly Polynomial Algorithm for SFM 
In this section, we present the strongly polynomial time algorithm for SFM.  But first, 
we point out that occasionally the size of D grows too large and we want to decrease 
its size.  Accordingly, we run a procedure called Reduce(x, λ, D) to reduce the size of 
D without affecting the base vector x. 
Procedure Reduce(x,λ, D)   
INPUT:  a collection D of distance functions, a non-negative vector λ such that 
λd
d∈D
∑
= 1.   Let x =
λd
d∈D
∑
yd . 
OUTPUT:  a subset
′
D ⊆D and a vector ′
λ  such that  
1. 
′
λd
d∈′
D
∑
= 1 and  ′
λ ≥0,  and x =
′
λd
d∈′
D
∑
yd  , and  
2. the set {yd :d ∈
′
D }  is linear independent. 
We will call the procedure when 3n ≤ |D| < 4n, and so the running time will be 
O(n3) using standard techniques from linear programming.  For details on how to 
carry out Reduce, see Schrijver [13] or McCormick [12].  
In the following procedure, let V0 = {v ∈ V : x(v) = 0}.  Let V+ = {v ∈ V : x(v) > 0}. 
Algorithm SFM 
begin 
d := 0;  
D = {d};  λd := 1;  x := yd;  
while the optimality conditions are not satisfied 
begin 
 
choose an element v* ∈ V+; 
choose a vector γ  ≥ 0 with γ ≠ 0 so that 
γ (v)
v∈V 0+v*
∑
[ys(v)(u)−yp(v)(u)] = 0  for all u ∈ V0 ; 

246 
J.B. Orlin 
let ′x :=
γ (v)
v∈V 0+v*
∑
[ys(v) −yp(v)] ; 
choose α maximum so that  x(u) + α x’(u) ≥ 0 for all u ∈ V+, and  
α
γ (u)
u:p(u)=d
∑
≤λd for all d ∈ D; 
x := x +α ′x ; 
λd := λd +α
γ (u)
u:s(u)=d
∑
−α
γ (u)
u:p(u)=d
∑
 for all d ∈ D ∪ {s(u): u ∈ V}; 
 
Update(D, p, s); 
if |D| ≥ 3n, then Reduce(x, λ, D); 
if there is a distance gap at level k, then V := {v ∈ V : Dmin(v) ≤ k}; 
end while 
end 
The algorithm initializes by letting x = yd, where d(v) = 0 for all v ∈ V.  
Subsequently, the algorithm continues until the optimality conditions are satisfied. 
At each iteration, the algorithm selects a non-zero vector γ ≥ 0 with the property 
that one can modify x by increasing ys(v) by γ(v) and decreasing yp(v) by γ(v) for all v so 
that the following is true:  if x(v) = 0 prior to the modification, then x(v) = 0 after the 
modification. It is not obvious that such a vector γ exists.  We prove its existence in 
the next section, and show that it can be determined easily by solving a system of 
linear equations. 
Once we determine the vector γ, we modify λ and x.  After the modification, at 
least one of the following changes takes place: either V0 increases in size or there is 
some primary vector p(v) that leaves D because dp(v) = 0 after the modification.  In 
fact, α is chosen sufficiently large so that one of these two events occur and so that no 
element ever leaves V0, and so that any element leaving V+ must enter V0. 
We reduce the size of D whenever |D| ≥ 3n, and we eliminate elements from V 
whenever a distance gap is found. 
In Section 7, we will show that the algorithm terminates in O(n6) steps with an 
optimal set.  The proof of the time bound relies on a potential function argument. 
6   The Auxiliary Matrix and How to Choose γ 
In this section, we show how to choose γ by solving a system of at most n equations. 
One of the key steps of the algorithm is as follows:  choose a vector γ ≥ 0 with  
γ ≠ 0 so that  
γ (v)
v∈V 0+v*
∑
[ys(v)(u)−yp(v)(u)] = 0  for all u ∈ V0; 
We consider two separate cases.   
Case 1.  γ(v*) = 0. 
In this case, we need to solve 
γ (v)
v∈V 0
∑
[ys(v)(u)−yp(v)(u)] = 0  for all u ∈ V0 ; 
 
 

 
A Faster Strongly Polynomial Time Algorithm for SFM 
247 
Case 2. γ(v*) = 1.  (We can always scale γ  so that this is true whenever γ(v*) ≠ 0).   
In this case, we need to solve 
γ (v)
v∈V 0
∑
[ys(v)(u)−yp(v)(u)] = yp(v*)(u)−ys(v*)(u)  
for all u ∈ V0.   
Suppose that the rows and columns of the constraint matrix are both indexed by the 
elements of V0.  Then the constraint matrices for Cases 1 and 2 are identical.  The 
right hand side b in Case 2 may be non-zero; however, by Lemma 4, b ≤ 0.   
We refer to the constraint matrix A* for Cases 1 and 2 as the auxiliary matrix.  By 
Lemma 4, the auxiliary matrix satisfies the following properties: 
6.1. A* is an | V0| × | V0| matrix. 
6.2. The diagonal elements of A* are non-positive. 
6.3. All non-diagonal elements of A* are non-negative. 
6.4. Each column sum of A* is non-positive. 
In the case that A* is invertible, it is the negative of what is known in the literature 
as an M-matrix, and thus the inverse of A* is non-positive.  See, for example, [1] for 
results on M-matrices. 
Theorem 2. Let A* be an auxiliary matrix. If A* is singular, then there is a vector 
w’ ≠ 0, such that w’ ≥ 0, and A*w’ = 0.  If A* is non-singular then (A*)-1 ≤ 0, and thus 
the solution to A*w’ = b is non-positive whenever b is non-negative.  
Proof. The second half of the theorem is well known.  The first half can easily be 
derived from [1], but we include a proof for completeness.  Suppose that A* is 
singular.  Choose w ≠ 0 so that Aw = 0.  If w ≥ 0, there is nothing to prove.  Similarly 
if w ≤ 0, then we can replace w by –w and there is nothing to prove.  So, suppose that 
there are k < n positive coefficients of w.  Without loss of generality assume that w(v) 
> 0 for v = 1 to k.  (Otherwise, one can simultaneously reorder the rows and columns 
so that this is true.)  
Let us write A* = A11
A12
A21
A22
⎡
⎣
⎢
⎤
⎦
⎥, where A11 denotes the first k rows and columns 
of A*. Let us rewrite w as w = w1
w2
⎡
⎣⎢
⎤
⎦⎥, where w1 denotes the first k components of w. By 
assumption, A11w1 + A12w2 = 0  and A11w1 + A21w2 = 0 . By 6.3, A12 ≥ 0. By 
assumption, w2 ≤ 0.  Therefore, A11w1 ≥ 0.  We will next show that A11w1 = 0. 
Let 1 denote a row vector of k ones.  Then 1A11 ≤ 0 by 6.3 and 6.4.  If 1A11 ≠  0, 
then 1A11w1 < 0, contradicting that A11w1 ≥ 0.  We conclude that 1A11 =  0. It follows 
that 1A11w1 = 0, which combined with A11w1 ≥ 0 shows that A11w1 = 0. In addition, by 
6.1c and 6.1d, A21 = 0. 
Finally, we extend w to a vector w’ of |V0| components by letting  
′
w = w1
0
⎡
⎣⎢
⎤
⎦⎥
.
 
Then Aw’ = 0, which is what we wanted to prove.                                                   ♦ 

248 
J.B. Orlin 
By Theorem 2, the solution for γ in cases 1 and 2 can both be found by solving a 
system of equations on the auxiliary matrix, which takes O(|V0|3) = O(n3) time.  
Moreover, the running time is faster when the auxiliary matrix only changes by q 
columns in an iteration.  In this case, the time to solve the system of equations at a 
given iteration is O(qn2). 
We note that occasionally a column of the auxiliary matrix is 0, in which case it is 
trivial to find a non-zero vector w’ with Aw’ = 0.  However, this speedup does not 
affect the worst case analysis. 
7   Proof of Correctness and Time Bound 
In this section we establish the correctness of the SFM algorithm and show that it runs 
in O(n5 EO + n6) time. 
We first establish that the following remain true throughout the execution of the 
algorithm: 
7.1. At each iteration, there is a set D of valid distance functions, an element x ∈ 
B(f) and a vector λ such that (3) is satisfied.   
7.2. If x(v) = 0 at some iteration, then x(v) = 0 at all subsequent iterations; 
7.3. Dmin(v) is non decreasing over all iterations for all v ∈ V. 
7.4. If x(v) < 0, then Dmin(v) = 0; 
Theorem 3. Conditions 7.1 to 7.4 are satisfied at each stage of the algorithm SFM. 
Proof. Conditions 7.1 to 7.4 are all satisfied immediately subsequent to the 
initialization.  Suppose inductively that they are satisfied at some iteration of the 
algorithm, and we consider what happens after some procedure is called. 
We first consider the procedure Reduce.  This procedure maintains (3) and 
eliminates a number of elements of D.  It is easy to verify that 7.1-7.4 remain true 
subsequent to the call of Reduce. 
Next, we consider eliminating elements when a distance gap is found.  This results 
in eliminating components from yd for all d and from x, and also changes the base 
polyhedron.  However, it is easy to see that 7.1 to 7.4 remain satisfied with respect to 
the new base polyhedron.   
Finally, we consider changes that occur in Procedure SFM.  When we modify λ, note 
that every increase in λs(v) is matched by a decrease in  λp(v).  For this reason, if 
λd = 1
d∈D
∑
holds prior to modifying λ, it also holds afterwards.  Also, by our choice of 
α we modify λ in such a way that it is always non-negative, and so (3.1) is still satisfied.  
The solution to the system of linear equations yields a vector x’ with the property 
that x’(v) = 0 for all v ∈ V0.  So, 7.2 is true after we replace x by x + αx’.   
We next consider 7.3.  The only distance functions added to D are of the form s(v).  
If u ≠ v, then Dmin(u) is unchanged if s(v)  is added to D.  As for Dmin(v), the vector d = 
p(v) is chosen so that Dmin(v) = d(v).  Accordingly, if d’ = s(v), then d’(v) = Dmin(v) 
+1, and so 7.3 remains satisfied. 
7.4 also remains satisfied.  If x(v) < 0, then we do not create any distance functions 
with d(v) ≥ 1.  This completes the proof.                                                                      ♦ 

 
A Faster Strongly Polynomial Time Algorithm for SFM 
249 
Theorem 4. The SFM algorithm terminates with a set S that minimizes the 
submodular function and finds an optimum solution x in the base polyhedron.  The 
algorithm runs in O(n5 EO + n6) time. 
Prior to proving the main theorem, we state our potential function, and prove three 
lemmas.   
For v ∈ V, let h(v) = d(V), where d = p(v). Thus h(v) is the sum of the distances in p(v). 
Let  

h(v) =
(d(u) −Dmin(u)
u∈V
∑
).  Since D is valid, it follows that 
ˆ
0
( )
h v
n
≤
≤
 for 
all v ∈ V.  Moreover, 
min
ˆ
( )
( )
( ).
v V
h v
h v
D
v
∈
−
= ∑
 
Let H(v) = {d ∈ D:  d(V) = h(v) and Dmin(v) = d(v)}.  Note that any distance 
functions in H(v) could have been chosen as a primary distance function for v if we 
had broken ties differently in ordering the elements of D.  
We define the potential function Φ   as follows: 
Φ(v) = H(v)  and Φ =
Φ(v).
v∈V in
∑
 
The next two lemmas concern h(v) and H(v). 
Lemma 5. For each v ∈ V, the number of times that h(v) changes over all iterations of 
the algorithm is O(n2). 
Proof. We will actually bound the number of changes in ˆ( )
h v .  Note that it is 
possible for h(v) to change while ˆ( )
h v stays constant if Dmin(u) increases.  But the 
number of changes in Dmin( ) over all iterations is O(n2).  If the number of changes of 
ˆ( )
h v is O(n2), then so is the number of changes of h(v). 
Recall that 0 ≤  

h(v)  ≤ n.  We first consider changes in 
h(v) in between successive 
changes in Dmin(v), and we refer to this set of iterations as a phase. The value  

h(v)  
cannot decrease during a phase unless Dmin(u) increases for some u ∈ V, in which case 
 

h(v)  can decrease by at most 1.  All other changes in h(v) during the phase are 
increases.  So the total number of changes in 
h(v)  is at most n plus the two times the 
number of increases in Dmin(u) for some u.  Suppose that we “charge” the latter 
changes in  

h(v)  to changes in Dmin.  In this case, the number of charged changes in 
 

h(v)  over all iterations is O(n2), and the number of other changes in ˆ( )
h v  is at most n 
per phase.  So the number of changes in ˆ( )
h v  is O(n2) over all phases.                      ♦ 
Lemma 6. The distance function s(v) ∉ H(u) for any u ∈ V. 
Proof. Let d = p(v), and let d’ = s(v).  We note that d’ ∉ H(v) because d’(V) = h(v) 
+ 1. So, we consider u ≠ v.  If Dmin(u)= d’(u), then Dmin(u) = d(u).  In this case h(u) ≤ 
d(V) < d’(V), and so d’ ∉ H(u).                                                                                     ♦ 
We next prove a lemma concerning the potential function Φ .  We note that Φ  
decreases at some iterations and increases at others.  By the total decrease in Φ  over 
all iterations, we mean the sum of the decreases in Φ  as summed over all iterations at 
which Φ  decreases.  We define total increase analogously. 

250 
J.B. Orlin 
Lemma 7. The total increase in Φ  over all iterations is O(n4), and the total decrease 
in Φ  over all iterations is also O(n4). 
Proof. Given that Φ  = O(n2), it suffices to show that the total increase over all 
iterations is O(n4) after which the O(n4) bound on the total decrease will follow.  
We first note that the only vectors that are added to D are vectors d = s(v) for some 
v ∈ V0.  By Lemma 6, these additions to D do not change the potential function (until 
p(v) is deleted from D).  The potential function changes only when one of the 
following two steps takes place: 
1. changes in H(v) while h(v) remains constant; 
2. changes in H(v) when h(v) also changes. 
By Lemma 6, each change in H(v) while h(v) remains constant can only result in a 
decrease in Φ(v).   So, we only need to bound increases in changes in Φ  when h(v) 
changes for some v.  
Each change in h(v) can lead to an increase of at most |D| = O(n) in Φ(v) .  By 
Lemma 5, there are O(n2) changes in h(v) for each v and thus the total increase in Φ  
over all iterations due changes in h( ) is O(n4).                                                             ♦ 
We are now ready to prove Theorem 4.  
Proof of Theorem 4. We first note that if the algorithm terminates, then it must 
terminate with an optimal solution since satisfying the optimality conditions is the 
only termination criterion. 
The bottlenecks of the algorithm are the following: 
1. Adding columns A(v) = s(v) – p(v) to the auxiliary matrix. 
2. Solving a system of equations A*w = b or A*w = 0; 
3. Reducing the number of columns in D via Procedure Reduce. 
We add a column to A(v) only when p(v) was deleted from D.  A deletion of p(v) 
for some v either leads to a change in h(v) or else it leads to a decrease in |H(v)|.  The 
former can happen O(n3) times by Lemma 5.  We now consider the latter case. 
Deleting a single element d = p(v) can result in several columns needing to be 
added to A*.  In particular, it is possible that d = p(u) for a subset U ⊆ V.  If d is 
deleted from D, then we need to replace |U| different columns of A*.  But in this case, 
deleting d from D reduces |H(u)| for all u ∈ U, and thus reduces Φ  by |U|.  We 
conclude that the number of columns added to A* is at most the total decrease in Φ  
over all iterations, which is O(n4) by Lemma  7. 
Thus the running time for adding columns to the auxiliary matrix is O(n5 EO) since 
determining the values for a column takes O(n EO) steps. For each column added to 
the auxiliary matrix, it takes O(n2) time to carry out elementary row operations to get 
A* into canonical form for solving the system of equations.  This takes O(n6) time 
over all iterations.  Thus the running time for adding columns to A* and carrying out 
elementary row operations is O(n5 EO + n6). 
We call the procedure Reduce when |D| ≥ 3n, and we eliminate at least 2n 
elements of D.  The running time is thus O(n3) for each call of Reduce.  Each distance 
function d that is deleted from D must have been added as a vector of the form s(v) at 

 
A Faster Strongly Polynomial Time Algorithm for SFM 
251 
some iteration, and this happens only O(n4) times.  Thus the total time to carry out 
Reduce is O(n6).   
We conclude that the total running time is O(n5 EO + n6) time.                              ♦ 
We have developed a strongly polynomial time algorithm for SFM that dominates 
previous strongly polynomial time algorithms by a factor greater than n.  Moreover, 
whereas other algorithms rely on the combinatorics of paths and flows, our algorithm 
relies on an iterative local search plus a combinatorial potential function argument. 
Acknowledgments. This research was supported by the Office of Naval Research 
under Grant N00014-98-1-0317.  I also thank Professors Satoru Iwata and Satoru 
Fujishige for their constructive comments on an earlier draft of this manuscript. 
References 
1. Berman, A., and Plemmons R. J.: Nonnegative Matrices in the Mathematical Sciences, 
SIAM, 1994. 
2. Cunningham, W. H..  On Submodular Function Minimization: Combinatorica 3  (1985) 
185-192. 
3. Edmonds, J.: Submodular Functions, Matroids, and Certain Polyhedra.  In Combinatorial 
Structures and their Applications, R. Guy, H. Hanani, N. Sauer, and J. Schönheim, eds., 
Gordon and Breach (1970) 69-87. 
4. Fleischer, L. K.:  Recent Progress in Submodular Function Minimization.  Optima, 2000, 
1-11. 
5. Fleischer, L.K. and Iwata, S.: Improved Algorithms for Submodular Function 
Minimization and Submodular Flow. Proceedings of the 32th Annual ACM Symposium 
on Theory of Computing (2000) 107–116.  
6. Fujishige, S.: Submodular Functions and Optimization. Second Edition. North-Holland 
(2005). 
7. The Ellipsoid Algorithm and its Consequences in Combinatorial Optimization. 
Combinatorica, 1 (1981), 499–513. 
8. Grötschel, M. Lovász, L.and Schrijver, A.: Geometric Algorithms and Combinatorial 
Optimization. Springer-Verlag. (1988). 
9. Iwata, S.. A Faster Scaling Algorithm for Minimizing Submodular Functions. SIAM J. on 
Computing 32 (2002) 833–840. 
10. Iwata, S., Fleischer, L., and. Fujishige, S: A Combinatorial, Strongly Polynomial-Time 
Algorithm for Minimizing Submodular Functions. J. ACM 48 (2001) 761–777. 
11. Lovász, L.: Submodular Functions and Convexity. In Mathematical Programming — The 
State of the Art, A. Bachem, M. Gr¨otschel, B. Korte eds., Springer, Berlin (1983) 235–257. 
12. McCormick, S.T.: Submodular Function Minimization.  In Discrete Optimization,  K. 
Aardal, G. Nemhauser, and R. Weismantel, eds. Handbooks in Operations Research and 
Management Science, Volume 12.  Elsevier. (2005). 
13. Schrijver, A.: A Combinatorial Algorithm Minimizing Submodular Functions in Strongly 
Polynomial Time. J. Combin. Theory Ser. B 80 (2000) 346–355. 
14. Schrijver, A.: Combinatorial Optimization: Polyhedra and Efficiency. Springer, Berlin 
(2003). 
15. Vygen, J..  A Note on Schrijver's Submodular Function Minimization Algorithm. Journal 
of Combinatorial Theory B 88 (2003) 399-402. 

On Convex Minimization over Base Polytopes
Kiyohito Nagano⋆
University of Tokyo, Tokyo 113-8656, Japan, and
Kyoto University, Kyoto 606-8502, Japan
kiyohito nagano@mist.i.u-tokyo.ac.jp
Abstract. This note considers convex optimization problems over base
polytopes of polymatroids. We show that the decomposition algorithm
for the separable convex function minimization problems helps us give
simple suﬃcient conditions for the rationality of optimal solutions and
that it leads us to some interesting properties, including the equivalence
of the lexicographically optimal base problem, introduced by Fujishige,
and the submodular utility allocation market problem, introduced by
Jain and Vazirani. In addition, we develop an eﬃcient implementation
of the decomposition algorithm via parametric submodular function min-
imization algorithms. Moreover, we show that, in some remarkable cases,
non-separable convex optimization problems over base polytopes can be
solved in strongly polynomial time.
Keywords: submodular functions, convex optimization.
1
Introduction
This note considers convex optimization problems over base polytopes of poly-
matroids, which is associated with monotone submodular functions. Submodular
functions appear in the systems of graphs and networks. Besides, they natu-
rally model economies of scale. In fact, convex optimization problems over base
polytopes have numerous applications. As a generalization of lexicographically
optimal ﬂow introduced by Megiddo [18], Fujishige [7] deﬁned the concept of
lexicographically optimal base of a polymatroid and showed that ﬁnding that
point is equivalent to minimizing a separable convex quadratic function over the
base polytope. Although they came from diﬀerent backgrounds, it is known that
the egalitarian solution of Dutta and Ray [4] in a convex game is essentially
the same concept as the lexicographically optimal base. Jain and Vazirani [15]
introduced the submodular utility allocation (SUA) market, in which a submod-
ular function speciﬁes the maximum utility, and captured an equilibrium for the
SUA market as an optimal solution to the maximization of the sum of logarithm
functions over a base polytope.
Let us see some indirect applications in which such problems will appear
as subproblems. Fujishige [8] showed the minimum norm point in the base
polytope can be utilized for submodular function minimization (SFM). For
⋆Supported by Grant-in-Aid for JSPS Fellows.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 252–266, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

On Convex Minimization over Base Polytopes
253
uncapacitated facility location problems with submodular penalties and other
problems, Chudak and Nagano [2] designed approximation algorithms which
require solving convex optimization problems over submodular constraints iter-
atively. With the aid of the lexicographically optimal base, the minimum ratio
problem minX{ f(X)/ 
v∈X wv : ∅̸= X ⊆V } can be solved immediately, where
f is a submodular set function deﬁned on subsets of a ﬁnite set V = {1, . . . , n}
and w = (wv : v ∈V ) is a positive vector in Rn (though it can be solved in a
more direct manner by the discrete Newton method). In the greedy algorithm for
the set covering problem with submodular costs due to Hayrapetyan, Swamy and
Tardos [12], several minimum ratio problems have to be solved to ﬁnd a subset
which has the smallest cost-eﬀectiveness. Besides, the primal-dual algorithm for
the prize collecting forest problems with submodular penalties given by Sharma,
Swamy and Williamson [23] repeatedly solves minimum ratio problems in order
to determine the next dual constraint that will go tight at each step. For other
applications of minimum ratio problems, see Fujishige’s book [9, §7.2(b.3)].
Fujishige [7] presented a decomposition algorithm to ﬁnd the lexicographically
optimal base by O(n) calls of SFM, where n is the number of elements of the ground
set. In [11], Groenevelt extended Fujishige’s algorithm to solve a general separable
convex minimization, though explicit running time was not given and the ratio-
nality of values in the algorithm was not considered. On the other hand, Fleischer
and Iwata [6] extended their push-relabel algorithm for SFM to solve the para-
metric minimization problem for a strong map sequence of submodular functions
and they noted that, in a way similar to the parametric maximum ﬂow algorithm
of Gallo, Grigoriadis and Tarjan [10], their algorithm can be applied to solve the
lexicographically optimal base problem eﬃciently. Taking a diﬀerent approach,
Hochbaum [13] proposed scaling-based algorithms for separable convex minimiza-
tion over submodular constraints. Her algorithm calls a membership oracle for the
base polytope, that is to say, an SFM oracle as a basic operation.
In this note, we mainly consider the minimization of separable and strictly con-
vex functions. By describing the decomposition algorithm in a simpliﬁed form,
we reveal the running time and give simple suﬃcient conditions for the rational-
ity of the optimal solution to the minimization of the separable convex function
over the base polytope. Furthermore, we illustrate some interesting properties,
including a new remark that the lexicographically optimal base problem and the
SUA market problem are equivalent. A part of these nice properties can also be
derived from the result on the universal bases given by Murota [19]. At the same
time, his approach is diﬀerent from ours. Besides, by reﬁning and generalizing
the discussion in [6], we develop an eﬃcient implementation of the decomposition
algorithm via the Fleischer-Iwata push/relabel algorithm for SFM. We believe
that a parametric minimization version of Orlin’s new algorithm [21] could be
developed and our framework would also work in that case. Finally, we deal
with non-separable convex functions and show that in some remarkable cases
the minimization can be carried out in strongly polynomial time.
This note is organized as follows. In Section 2, we deﬁne the main problem and
see optimality conditions. In Section 3, we will see about related problems, give

254
K. Nagano
examples of objective functions and check the rationality of optimal solutions
to subproblems. Section 4 describes the decomposition algorithm in a simpliﬁed
form and discusses the rationality and the equivalence of some problems. In Sec-
tion 5, we review the basic framework of combinatorial algorithms for SFM and
develop an eﬃcient implementation of the decomposition algorithm via para-
metric SFM algorithms. Lastly we consider the minimization of non-separable
convex functions in Section 6.
2
Preliminaries
Let V be a ﬁnite nonempty set with |V | = n. Suppose that V = {1, . . . , n}. A set
function f deﬁned on 2V is submodular if f(X)+ f(Y ) ≥f(X∪Y )+ f(X∩Y ) for
each X, Y ⊆V and monotone if f(X) ≤f(Y ) for each X, Y ⊆V with X ⊆Y .
It is easy to see that the minimizers of submodular function f are closed under
union and intersection, and thus there exist the (unique) minimal minimizer and
the (unique) maximal minimizer. For a vector x ∈RV and an element v ∈V ,
we denote by x(v) the component of x on v.
Let f : 2V →R be a submodular function with f(∅) = 0. We assume that
f is given by a value-giving oracle. With such a function f, the base polytope
B( f) is deﬁned by
B( f) = { x ∈RV : x(X) ≤f(X) (∀X ⊆V ), x(V ) = f(V )} ⊆RV
where x(X) = 
v∈X x(v). It is known that B( f) is nonempty and bounded.
A vector in B( f) is called a base and an extreme point of B( f) is called an
extreme base. Consider any total order ≺in V . The greedy algorithm [5] gives
an extreme base b≺∈RV by setting b≺(v) = f(L≺(v) ∪{v}) −f(L≺(v)) for
each v ∈V , where L≺(v) = {u ∈V : u ≺v}. Conversely, it is known that each
extreme base can be obtained in this way. For submodular functions f1, f2, if
Y ⊇X implies f1(Y ) −f1(X) ≥f2(Y ) −f2(X), we write f1 →f2 or f2 ←f1.
We call the relation f1 →f2 a strong map.
Throughout this note, we suppose the function f : 2V →R is rational, sub-
modular, monotone and satisﬁes f(∅) = 0, f({v}) > 0 for each v ∈V . In
other words, f is a rank function of a polymatroid. So we have B(f) ⊆RV
≥0
and B(f) ∩RV
>0 is nonempty. Let gv : R →R ∪{+∞} be a convex function on
dom gv for each v ∈V . In this note, we mainly consider the separable convex
function minimization problem over the base polytope :
min
x {g(x) : x ∈B(f)} where g(x) = 
v∈V
gv(x(v)).
(1)
For each v ∈V , let ev ∈RV be the characteristic vector that has value 1 on v and
0 elsewhere. The following theorem states that the local optimality with respect
to directions of the form eu −ev for u, v ∈V implies the global optimality.
Theorem 1 ([11], [9, Theorem 8.1]). For x ∈B(f), x is an optimal solution
of (1) if and only if for each u, v ∈V such that x + ε(eu −ev) ∈B(f) for some

On Convex Minimization over Base Polytopes
255
ε > 0, we have D+
u (x(u)) ≥D−
v (x(v)) where D+
u is the right derivative of gu and
D−
u is the left derivative of gv.
For any base x and each subset X with ∅̸= X ⊂V , by standard arguments about
tight subsets, one can show that x(X) = f(X) if and only if x + ε(eu −ev) /∈B(f)
for any ε > 0 and each pair (u, v) with u ∈X, v ∈V \ X. In particular, we have :
Corollary 2 ([9, Theorem 8.2]). Suppose that gv is diﬀerentiable and strictly
convex for each v ∈V . Let x ∈B(f) and ξ1 < · · · < ξℓdenote the distinct values
of g′
v(x(v)). Let Hs = {v ∈V : g′
v(x(v)) ≤ξs} for s = 1, . . . , ℓ. Then, x
is the optimal solution of problem (1) if and only if x(Hs) = f(Hs) for each
s = 1, . . . , ℓ.
To simplify the discussion, we mainly assume that gv is diﬀerentiable and strictly
convex and deﬁne the interval J ⊆R by J := 
v∈V {g′
v(x(v)) : x(v) ∈dom gv}.
For any B ≥0 (or > 0) and each nonempty subset U ⊆V , we suppose the problem
min
(x(v) : v∈U){ 
v∈U
gv(x(v)) : x(U) = B}
(2)
has the optimal solution xU ∈RU such that there exists α ∈J such that
xU(v) = (g′
v)−1(α) for each v ∈U. In Section 4, we will see problem (1) has the
rational optimal solution if the optimal solution to (2) is always rational for any
rational B and each nonempty subset U.
3
Examples of Problems
We review some related problems. After that, we give examples of function g
and check the rationality of optimal solutions to (2) for rational B and U ⊆V .
3.1
Related Problems
Let w ∈RV
>0 be a positive vector. For x ∈RV , let Tw(x) = ( x(v1)
w(v1), . . . , x(vn)
w(vn)) be
the real n-sequence such that x(v1)
w(v1) ≤· · · ≤x(vn)
w(vn) where {v1, . . . , vn} = V . For
two real n-sequences ρ = (ρ1, . . . , ρn) and σ = (σ1, . . . , σn), we write ρ ≥LEX σ
if ρ = σ or ρ ̸= σ and ρi > σi for the minimum index i such that ρi ̸= σi.
Lexicographically Optimal Bases. A base x is called a lexicographically
optimal (lex-optimal) base with respect to w if Tw(x) ≥LEX Tw(y) for all y in
the base polytope. Fujishige [7] showed that such a base xLEX of B(f) is unique
and coincides with the optimal solution of (1) with gv(x(v)) = x(v)2/w(v). Now
the monotonicity of f is not crucial because, for any M ∈R, xLEX + M · w is
lexicographically optimal in B(f + M · w) (= B(f) + M · w) w.r.t. w.
In Section 6, we will use the following characterization of the lex-optimal
bases, which is a special case of Theorem 1.
Corollary 3. For x ∈B(f), x minimizes 
v∈V
x(v)2
w(v) over the base polytope
B(f) if and only if for each elements u, v ∈V such that x + ε(eu −ev) ∈B(f)
for some ε > 0, we have
x(u)
w(u) ≥x(v)
w(v).

256
K. Nagano
Minimum Ratio Problems. Consider the minimum ratio problem which asks
for a subset X ∈2V \ {∅} minimizing f(X)/w(X). Now the monotonicity of
f is not essential again. Let xLEX ∈B(f) be the lex-optimal base w.r.t. w,
ξ1 = minv
xLEX(v)
w(v)
and X1 = {v : xLEX(v)
w(v)
= ξ1}. For any X ⊆V with X ̸= ∅, we
have ξ1w(X) ≤xLEX(X) ≤f(X) and so ξ1 ≤f(X)/w(X). On the other hand,
by Corollary 2, xLEX(X1) = f(X1) and thus ξ1 = f(X1)/w(X1). Therefore, us-
ing the lex-optimal base, the minimum ratio problem can be easily solved. This
problem, however, can be solved more directly via the discrete Newton method.
See, e.g., [6, §4.1].
Egalitarian Allocations. Imagine that V is a set of players. We assume that
set function val : 2V →R is a convex game, that is, −val is submodular and it
satisﬁes val(∅) = 0. It is pointed out that the egalitarian allocation in a convex
game [3,4], which is often called the Dutta-Ray solution, is essentially the same
concept as the lex-optimal base w.r.t. 1 = (1, . . . , 1). To be precise, it is the
lex-optimal base of B( f) w.r.t. 1 where f(X) = val(V ) −val(V \ X) (X ⊆V ).
Submodular Utility Allocation Markets. Let m(v) > 0 be the money pos-
sessed by buyer v ∈V . The maximization of 
v m(v) ln x(v) over the polytope
P = {x ∈RV : x(X) ≤f(X) (X ⊆V ), x ≥0} is called the submodular utility
allocation (SUA) market problem [15]. As the base polytope B(f) is the set of
all the maximal points of P with respect to the partial order ≤among vectors
in RV , this problem is a special case of problem (1).
The Minimum Norm Point. Let f : 2V →R be a submodular function with
f(∅) = 0 and let xMN be the point that minimize ∥x∥over B( f) where ∥.∥is
the Euclidean norm, that is, ∥x∥=

v x(v)2 for x ∈RV . By Corollary 2, it is
easy to see X<0 := {v ∈V : xMN(v) < 0} is the unique minimal minimizer and
X≤0 := {v ∈V : xMN(v) ≤0} is the unique maximal minimizer of f.
3.2
Examples of Objective Functions
Let w and m be vectors in QV
>0, a be a vector in QV , q be a vector in RV
>0, p ∈R
be a number such that p ̸= 0, −1 and let g0 : R →R ∪{+∞} be a diﬀerentiable
and strictly convex function with dom g0 ⊇R≥0. Deﬁne function sgn : R →R
by sgn(τ) = 0 if τ = 0 and sgn(τ) = τ/|τ| otherwise. For example, we consider
the convex functions
gMN(x) = 1
2∥x∥2, gMN′(x) = 1
2∥x + a∥2
and
gLex
v
(x(v)) =
1
2w(v)x(v)2,
gPow
v
(x(v)) = sgn(p) x(v)p+1
(p+1) w(v)p ,
gSUA
v
(x(v)) = −m(v) ln x(v),
gexp
v
(x(v)) = exp(x(v) + a(v)),
gS
v(x(v)) = g0(x(v)),
gW
v (x(v)) = x(v)g0( w(v)
x(v) ),
gBad
v
(x(v)) = x(v)
q(v) (ln x(v) −1),

On Convex Minimization over Base Polytopes
257
for each v ∈V . Functions gPow and gS are deﬁned on RV
≥0, and gSUA, gW and
gBad are deﬁned on RV
>0.
Among these functions, let us see that it suﬃces to examine gS, gW and gBad as
objective functions of (1) and (2). Trivially, gLex = gPow if p = 1, and gMN = gS
if g0(τ) =
τ 2
2 . By resetting f := f −a + (maxv a(v)) · 1, f is still monotone
and the minimization of gexp and gMN′ can be reduced to the problem with
g(x) = gS(x). The function gPow is a special case of gW where g0(τ) = sgn(p)
p+1 τ −p.
If g0(τ) = τ ln τ and w = m, we have gW
v (x(v)) = gSUA
v
(x(v)) + m(v) ln m(v)
for each v. Thus gSUA is a special case of gW ignoring the constant term. In
some sense, we may view gSUA as gPow with p = −1 because gSUA
v
(x(v)) =
limp→−1(gPow
v
(x(v)) +
1
(p+1)w(v)p ).
Let B ≥0 (or > 0) and U ∈2V \ {∅}. We denote the optimal solutions of
problem (2) with g = gS, gW and gBad by xS
U, xW
U and xBad
U
∈RU, respectively.
Easily we have
xS
U(v) =
1
|U| · B ,
for each v ∈U.
(3)
So, if B is rational, xS
U is a rational vector and the size of vector xS
U is polyno-
mially bounded by n and the size of B. Now we let g = gW. Then g′
v(x(v)) =
g0( w(v)
x(v) ) −w(v)
x(v) g′
0( w(v)
x(v) ). Thus, for vector (x(v) : v ∈U) if there exists a number
α ∈J such that g′
v(x(v)) = α for each v ∈U, we can write x(v) = C · w(v) for
each v ∈U, where C is some constant. Therefore we have
xW
U (v) = w(v)
w(U) · B ,
for each v ∈U.
(4)
Thus the size of xW
U is polynomially bounded by the sizes of w and B.
Finally, let us see a bad example in which the rationality of the optimal
solution of (2) does not hold. Now we let g = gBad. Then J = R and (g′
v)−1(α) =
(exp(α))q(v) for any α ∈R. So we can write xBad
U
(v) = βq(v) for each v ∈U
where β > 0 is the unique positive solution of the equation 
v∈U βq(v) = B.
In general, we cannot give β or xBad
U
in a closed form. Moreover xBad
U
is not
necessarily rational.
4
The Decomposition Algorithm
We describe the decomposition algorithm [7,11] for separable convex minimiza-
tion problems over base polytopes in a quite simpliﬁed form. The point of this
section is that the correctness of the algorithm gives us suﬃcient conditions for
the rationality of optimal solutions and some good properties. In Sections 4 and
5, we assume that gv is diﬀerentiable and strictly convex for each v ∈V . Though
each gv is not necessarily diﬀerentiable, a similar algorithm also works and thus
things essentially do not change.

258
K. Nagano
4.1
The Decomposition Algorithm and the Rationality
Let x∗be the optimal solution of (1). We denote the distinct values of g′
v(x∗(v))
by ξ∗
1 < · · · < ξ∗
ℓand let H∗
s = {v ∈V : g′
v(x∗(v)) ≤ξ∗
s} for s = 1, . . . , ℓ. For
convenience, we let ξ0 = −∞, ξℓ+1 = +∞, H∗
0 = ∅and H∗:= {H∗
0, . . . , H∗
ℓ}.
For α ∈J, deﬁne xα ∈RV as
xα(v) = (g′
v)−1(α)
(5)
for each v ∈V . If α < α′, we have xα < xα′. For α ∈J and U ⊆V , the vector
(xα(v) : v ∈U) is the unique optimal solution to (2) with B = xα(U). Remark
that set function fα := f −xα is submodular for α ∈J and that we have the
relation fα →fα′ if α < α′.
Lemma 4. Let α ∈J. If ξ∗
s < α < ξ∗
s+1, H∗
s is the unique minimizer of fα. If
α = ξ∗
s, H∗
s−1 is the unique minimal minimizer and H∗
s is the unique maximal
minimizer of fα.
Proof. Suppose ξ∗
s < α < ξ∗
s+1. As g′
v is strictly increasing, x∗(v) −xα(v) < 0
if v ∈H∗
s and x∗(v) −xα(v) > 0 otherwise. Thus using Corollary 2, for each
X ⊆V with X ̸= Hs, we have
f(X) −xα(X) ≥x∗(X) −xα(X) > x∗(H∗
s ) −xα(H∗
s ) = f(H∗
s ) −xα(H∗
j ).
Suppose α = ξs holds. Then v ∈H∗
s−1 iﬀx∗(v) −xα(v) < 0 and v ∈H∗
s iﬀ
x∗(v) −xα(v) ≤0. So H∗
s−1 and H∗
s minimize fα and any minimizer X satisﬁes
H∗
s−1 ⊆X ⊆H∗
s .
⊓⊔
This lemma implies that problem (1) can be reduced to the parametric problem :
minX{fα(X) : X ⊆V } for all α ∈J.
(6)
This fact leads us to the framework of the decomposition algorithm [7,11]. By
successively computing xα for some appropriately chosen α ∈J and minimizing
fα, we ﬁnd H∗
s ∈H∗one by one and ﬁnally we obtain the chain H∗
0 ⊂· · · ⊂H∗
ℓ
and the point x∗.
The algorithm is recursive and now the description is simpliﬁed to reveal the
running time explicitly. First we know that H∗
0 = ∅and H∗
ℓ= V , although
we do not know how much ℓis. Let S = H∗
s and T = H∗
t for some s and t
such that 0 ≤s < t ≤ℓ. Now we give the procedure DA(S, T ) which returns a
vector x∗
T \S ∈RT \S. We denote by αS, T the number α ∈J satisfying xα(T \S) =
f(T )−f(S). We let α = αS, T . Note that α = ξt if s+1 = t and that ξs+1 < α < ξt
if s + 1 < t. Moreover xS,T := (xα(v) : v ∈T \ S) is the optimal solution of
problem (2) with U = T \ S and B = f(T ) −f(S). Deﬁne the submodular
function f S, T : 2T \S →R by
f S, T (W) = fαS, T (W ∪S) −fαS, T (S)
= f(W ∪S) −f(S) −xS, T (W)
(7)

On Convex Minimization over Base Polytopes
259
for each W ⊆T \ S. The procedure computes the maximal minimizer R of
fα. Since any minimizer R′ of fα satisﬁes S ⊆R′ ⊆T , it suﬃces to compute
xS,T and minimize f S, T . If R = T , that is, t = s + 1, then x∗(v) = xα(v)
for each v ∈T \ S and DA(S, T ) returns x∗
T \S := xS, T . Next, consider the
case where R ⊂T and so S ⊂R. By Lemma 4, R = H∗
r for some r with
s + 1 ≤r < t. Let x1 = x∗
R\S and x2 = x∗
T \R be vectors returned by DA(S, R)
and DA(R, T ), respectively. The procedure DA(S, T ) returns the vector x∗
T \S
obtained by setting x∗
T \S(v) := x∗
1(v) for each v ∈R \ S and x∗
T \S(v) := x∗
2(v)
for each v ∈T \ R. By induction, we can see that x∗
T \S = (x∗(v) : v ∈T \ S).
Thus DA(∅, V ) returnes the optimal solution of problem (1).
This algorithm implies suﬃcient conditions for the rationality of the optimal
solution of problem (1).
Theorem 5. Suppose problem (2) has a rational optimal solution for any ra-
tional number B ≥0 and each subset U ∈2V \ {∅}. Then the optimal solution
of problem (1) is rational.
Proof. Let v ∈H∗
s \ H∗
s−1. The optimal solution to (2) with U = H∗
s \ H∗
s−1 and
B = f(H∗
s ) −f(H∗
s−1) is rational. So x∗(v) is also rational.
⊓⊔
Note that this theorem can also be shown directly from Corollary 2. Moreover,
we can immediately get the following useful observation.
Corollary 6. Suppose the problems of the form (2) with objective function g
and g always have the same optimal solution for any B ≥0 and each U. Then
the optimal solutions to the problems of the form (1) are also the same.
Additionally, we assume that the optimal solution xU to (2) is rational for any
B ∈Q≥0 and each U ⊆V and that arithmetic operations involving values xU(v)
(v ∈U) can be carried out as basic steps. In the algorithm, we compute xS,T
and minimize f S,T for some S, T ∈H∗at most n times respectively. Since f and
xS,T are rational, f S,T is also rational. Therefore we can utilize a submodular
function minimization algorithm to minimize f S,T in polynomial time. Notice
that we need the maximal minimizer of fS,T . This point will be discussed in the
next section.
Theorem 7. The optimal solution of problem (1) can be obtained by solving
problem (2) and performing submodular function minimization at most n times
respectively.
4.2
Equivalence of Problems
The decomposition algorithm directly leads us to some interesting facts about
the convex minimization over B(f). Recall the functions introduced in Section
3. By Corollary 6, in view of (3) and (4), we have :
Corollary 8. The minimum norm point xMN of B(f) is the (unique) optimal
solution to problem minx{
v g0(x(v)) : x ∈B(f)}.

260
K. Nagano
Corollary 9. The following are equivalent:
(9.a) x ∈RV minimizes 
v
1
2w(v)x(v)2 over B(f);
(9.b) x ∈RV minimizes 
v
sgn(p)
(p+1) w(v)p x(v)p+1 over B(f) where p ̸= 0, −1;
(9.c) x ∈RV maximizes 
v w(v) ln x(v) over B(f);
(9.d) x ∈RV minimizes 
v x(v)g0( w(v)
x(v) ) over B(f).
The equivalence of (9.a) and (9.d), that is, Corollary 9 itself can also be derived
from a general result on the universal bases of Murota [19], which is obtained in
a diﬀerent way from our approach. In view of (9.a) and (9.c), however, it is still
a somewhat surprising fact and a new remark that the lexicographically optimal
base problem [7] and the SUA market problem [15] are equivalent.
Corollary 8 can be slightly generalized using Theorem 1. Let gN : R →R ∪
{∞} be a convex function which is not necessarily diﬀerentiable. For example,
gN(τ) = |τ| or gN(τ) = max{−τ, 0}. Note that many submodular function
minimization algorithms are based on the maximization of 
v min{0, x(v)} over
base polytopes (see §5.1).
Corollary 10. Let xMN ∈RV be the minimum norm point of B(f). Then xMN
also minimizes 
v gN(x(v)) over B(f). Besides, if gN(x(v)) is strictly convex,
xMN is the unique minimizer.
5
An Eﬃcient Implementation
For submodular function minimization (SFM), Fleischer and Iwata [6] developed
a push/relabel algorithm using Schrijver’s subroutine [22]. They also extended
their algorithm to parametric minimization for a strong map sequence of sub-
modular functions. In addition, they noted that their algorithm can be used to
solve the lexicographically optimal base problem eﬃciently (though their discus-
sion includes some minor errors). By reﬁning and generalizing the discussion of
[6], we propose an eﬃcient implementation of the decomposition algorithm via
the Fleischer-Iwata algorithm. We also believe that our framework could be ex-
tended via (possible) parametric variants of Orlin’s new algorithm [21] for SFM.
In this section, if α = αS, T for some subsets S, T ⊆V with S ⊂T , we
assume that the time of function evaluation of fα is bounded by EO and that
arithmetic operations involving values fα(X) (X ⊆V ) can be regarded as basic
steps. Let ≺◦be a total order in V = {1, . . . , n} such that 1 ≺◦· · · ≺◦n. Let
f : 2V →R be any submodular function with f(∅) = 0 and minX f(X) =: f ∗.
We also denote by EO the upper bound on the time to evaluate f.
5.1
Submodular Function Minimization
We brieﬂy review the basic framework of combinatorial algorithms for SFM.
See McCormick [17] for a nice survey on SFM. For x ∈RV , deﬁne the vector
x−∈RV by x−(v) = min{0, x(v)} for each v ∈V . For any base x ∈B( f)

On Convex Minimization over Base Polytopes
261
and each X ⊆V , we have x−(V ) ≤x(X) ≤f(X). Furthermore, the result of
Edmonds [5] immediately implies that maxx{x−(V ) : x ∈B( f)} = f ∗.
In Schrijver’s algorithm [22] and the Fleischer-Iwata algorithm [6], at any step,
we keep a point x ∈B( f) as a convex combination x = 
i∈I λib≺i of extreme
bases where I is a ﬁnite set of indices with |I| = O(n) and each ≺i is a total
order in V . With such a point x ∈B( f), we consider a digraph D = (V, AI)
where AI = {(u, v) : u ≺i v for some i ∈I}. Let P = {v ∈V : x(v) > 0} and
N = {v ∈V : x(v) < 0}. We iteratively update x until D has no path from P
to N. Then let R1 be the vertices that can reach N and R2 be the vertices that
are not reachable from P in D. Clearly, we have N ⊆Rh ⊆V \ P for h = 1, 2.
Both of the algorithms [6,22], which can be implemented in O(n8 +n7 EO) time,
return R1 or R2 and terminate. It is easy to see that x is an optimal solution to
maxx{x−(V ) : x ∈B( f)} and f(R1) = f(R2) = f ∗.
Remark that the maximal minimizer of a submodular function is needed in
the decomposition algorithm and little attention was paid to this point in the
algorithm of [6]. The following example shows that R1 and R2 are not necessarily
maximal. Suppose that n = 5 and a = (0, −1, 0, 1, 0) ∈RV and the function
f : 2V →R is deﬁned by f(X) = 
v∈X a(v). Then the minimal minimizer
is {2} and the maximal minimizer is {1, 2, 3, 5}. By initially setting I = {1},
≺1=≺◦and λ1 = 1, we get the point x = b≺◦= (0, −1, 0, 1, 0) and the digraph
D has no directed path from P = {4} to N = {2}. So we obtain R1 = {1, 2}
and R2 = {1, 2, 3}, neither of which is maximal. Fortunately, it is known that
we can compute the maximal and the minimal minimizer in O(n3EO) additional
time using the result of [1]. See e.g. Note 10.11 of [20] for details.
Lemma 11. Given a maximizer x = 
i∈I λib≺i of maxx{x−(V ) : x ∈B( f)}
with |I| = O(n), the unique maximal minimizer and the unique minimal mini-
mizer of f can be found in O(n3EO) time.
5.2
The Fleischer-Iwata Algorithm for Parametric SFM
We review the computation of a minimizer of every function in a strong map
sequence of submodular functions, f1 →· · · →fk, via the ordinary and the
reverse push/relabel algorithms for SFM of Fleischer and Iwata [6].
Consider a base x = 
i∈I λib≺i ∈B( f) and the digraph D for x. We need a
concept of a valid labeling on V . A labeling d : V →Z is valid for x if d(v) = 0
for v ∈N, d(u) ≤d(v) + 1 for each (u, v) ∈AI and d(v) ≤n for v ∈V . A
labeling dR : V →Z is r-valid for x if dR(v) = 0 for v ∈P, dR(v) ≤dR(u) + 1
for each (u, v) ∈AI and dR(v) ≤n for v ∈V . Obviously, the labeling d◦:= 0
on V is always valid and r-valid for any x.
Let f (1), f (2) and f (3) be submodular functions deﬁned on 2V such that
f (h)(∅) = 0 for h = 1, 2, 3 and f (1) →f (2) →f (3). For each total order ≺in
V , we denote the outputs of the greedy algorithm w.r.t. f (h) by b≺
h ∈B(f (h))
for each h. Notice that b≺
1
≥b≺
2
≥b≺
3 for each total order ≺. To simplify
the notation, we use subindex 1, 2 and 3 for digraphs associated with B(f (1)),
B(f (2)), and B(f (3)) respectively. Suppose that we have bases x1 ∈B(f (1)) and

262
K. Nagano
x3 ∈B(f (3)) with x1 = 
i∈I1 λib≺i
1
and x3 = 
j∈I3 μjb≺j
3
where |I1| and |I3|
are O(n).
First we let d be a valid labeling for x3. By setting x2 := 
j∈I3 μjb≺j
2 , a base of
B(f (2)) can be obtained in O(n2EO) time. From the construction, the digraphs
D2 and D3 have the same edges, x2 ≥x3 and N2 ⊆N3. Therefore d is still
valid for x2 ∈B(f (2)). The ordinary push/relabel (OPR) algorithm in [6] ﬁnds
a maximizer x2 = 
j∈I μjb≺j
2
of maxx{x−(V ) : x ∈B(f (2))} with |I| = O(n)
and a valid labeling d for x2 such that d ≥d in O((d(V ) −d(V ))(n6 + n5EO))
time. Thus, using the OPR algorithm, each function in fk ←· · · ←f1 can be
minimized in this order in O(n8 + (n7 + kn2)EO) time in total. Note that the
minimization of fk can be started with the exreme base b≺◦∈B(fk) and the
labeling d = 0.
Next we let dR be an r-valid labeling for x1. As above, one can show that dR is
also r-valid for x2 := 
i∈I1 λib≺i
2
∈B(f (2)). Starting with x2 and dR, the reverse
push/relabel (RPR) algorithm [6] ﬁnds a maximizer x2 and an r-valid labeling dR
for x2 with dR ≥dR in O((dR(V )−dR(V ))(n6 +n5EO)) time. So, every function
in f1 →· · · →fk can be minimized in this order in O(n8 + (n7 + kn2)EO) time.
5.3
An Eﬃcient Implementation of the Decomposition Algorithm
If we know vectors {xα1, . . . , xαℓ−1} such that ξ∗
j < αj < ξ∗
j+1 for each j =
1, . . . , ℓ−1, it follows from the relation fα1 →· · · →fαℓ−1 that algorithms for
parametric SFM solve problem (1) in the same asymptotic running time as a
single SFM. In the decomposition algorithm, however, each vector xα is obtained
in an on-line manner. As in [10] and [6], we introduce the procedure Slice, which
plays the same role as the procedure DA in Section 4. We can assume that f−∞
has the unique minimizer ∅and f+∞has the unique minimizer V . For S, T ⊆V
with S ⊂T , deﬁne the function fS, T : 2T \S →R by fS, T (W) = f(W ∪S)−f(S)
for each W ⊆T \ S.
Let S, T ∈H∗be subsets with S ⊂T such that S is the maximal minimizer
of f (1) := fα1 and T is the maximal minimizer of f (3) := fα3 for α1, α3 ∈
J ∪{±∞} with α1 < α3. In addition, we are given subsets S′, T ′ ∈H∗such
that S′ ⊆S and T ⊆T ′. Now we regard V ′ := T ′ \ S′ as the universe. Deﬁne
the function f ′ : 2V ′ →R by f ′ := fS′, T ′ and deﬁne f ′
α in the same way
for any α ∈J, that is, f ′
α := (fα)S′, T ′. Suppose that a labeling dR
1 on V ′ is
r-valid for x1 = 
i∈I1 λib≺i
1
∈B(f ′
α1) and a labeling d3 on V ′ is valid for
x3 = 
j∈I3 μjb≺j
3
∈B(f ′
α3).
In the procedure Slice(f ′, α1, α3, x1, x3, dR
1 , d3) for ﬁnding vector x∗
T \S in
RT \S, we set α2 := αS, T , x′
2 := 
i∈I1 λib≺i
2 , x′′
2 := 
j∈I3 μjb≺j
2
∈B(f ′
α2), and
try to minimize f (2) := fα2 (or, equivalently, f ′
α2) by concurrently running the
RPR algorithm, starting with x′
2, dR
1 , and the OPR algorithm, starting with x′′
2,
d3. Suppose the RPR algorithm stops ﬁrst with the base x2 = 
i∈I2 λib≺i
2
of
B(f ′
α2) and the r-valid labeling dR
2 on V ′ for x2. (The other case is symmet-
ric.) By lemma 11, the maximal minimizer R of f (2) can be found in O(n3EO)

On Convex Minimization over Base Polytopes
263
additional time and it holds that S ⊂R ⊆T . If R = T , the procedure Slice
returns x∗
T \S := xS, T . So we consider the case where S ⊂R ⊂T . If 2|R| < |S|+
|T |, we perform Slice(f ′, α2, α3, x2, x3, dR
2 , d3) and Slice(f ′′, α1, α2, b≺◦, b≺◦,
d◦, d◦) where f ′′ := fS, R, d◦= 0 ∈RR\S and ≺◦is some total order of R \ S.
If 2|R| ≥|S| + |T |, we continue the OPR algorithm until it stops. Then, re-
placing x2 by the resulting base and letting d2 be the valid labeling for x2, we
perform Slice(f ′, α1, α2, x1, x2, dR
1 , d2) and Slice(f ′′, α2, α3, b≺◦, b≺◦, d◦, d◦)
where f ′′ := fR, T . Remark that in any case the problem is divided into two
problems, the larger one of size n′ and the quite smaller one, whose size is less
than or equal to n′/2.
By running Slice(f, −∞, +∞, b≺◦, b≺◦, d◦, d◦), problem (1) can be solved.
Now we show this algorithm can be implemented to run in O(n8 + n7EO) time.
The running time analysis is quite similar to that of the lexicographically opti-
mal ﬂow algorithm proposed by Gallo et al. [10]. Let RPR(n) (OPR(n)) denote
the time to minimize every function fk′ : 2V →R in a strong map sequence
f1 →· · · →fk with k ≤n via the RPR (OPR) algorithm. That is, RPR(n)
and OPR(n) are O(n8 + n7EO). Once we incur 2(RPR(n) + OPR(n)) time (the
factor 2 comes from the fact that we concurrently run the two push/relabel al-
gorithms), then, in the larger subproblems of the procedure Slice, subsequent
implementations of the two push/relabel algorithms can be regarded as being
free. We denote by T (n′) the upper bound on the time to run the procedure
Slice(f ′, α1, α2, x1, x2, dR
1 , d2). We obtain the equalities T (1) = O(1) and
T (n) = O(n8 + n7EO) + max

T (n1) + T (n2) + · · · + T (nh) :
n1 + · · · + nh < n;
n1, . . . , nh ≥1;
nh′ ≤1
2(n −n1 −· · · −nh′−1) for h′ = 1, . . . , h

.
From these formulas, it is not diﬃcult to verify that T (n) = O(n8 + n7EO).
Theorem 12. The optimal solution of problem (1) can be obtained in the same
asymptotic running time as a single implementation of the push/relabel algorithm
for SFM by Fleischer and Iwata, that is, O(n8 + n7EO) time.
This time complexity is better than the bound O((n8 + n7EO) log n) which is
obtained by performing Iwata’s algorithm [14] iteratively in the decomposition
algorithm. Quite recently, however, Orlin [21] developed a faster algorithm for
SFM which runs in O(n6 + n5EO) time. Therefore, using Orlin’s algorithm,
problem (1) can be solved in O(n7 +n6EO) time by Theorem 7. It is not certain
but we believe that parametric minimization versions of Orlin’s algorithm could
be developed and our framework would also work in that case.
6
Non-separable Convex Functions
This section deals with non-separable convex objective functions. We will see
that, using the characterization of the lex-optimal base, some non-separable
convex optimization problems can be solved in strongly polynomial time.

264
K. Nagano
For x ∈RV and u, v ∈V , let xu, v ∈RV be the vector deﬁned by
xu, v(v′) =
⎧
⎨
⎩
x(v) if v′ = u,
x(u) if v′ = v,
x(v′) if v′ ∈V \ {u, v}.
We let gP : RV →R ∪{+∞} be a strictly convex function with dom gP ⊇
RV
≥0 which has invariant value under permutation of indices. That is, we have
gP(xu, v) = gP(x) for any x and each u, v. If V = {1, 2}, x(1)2+x(2)2+x(1)x(2)
is such an example. Consider the minimization of gP over the base polytope B(f).
The following observation, which was originally shown by Maruyama [16] about
three decades ago, is a generalization of Corollary 8.
Corollary 13. The minimum norm point xMN ∈RV of B(f) is the (unique)
minimizer of minx{gP(x) : B(f)} and so this problem can be solved in strongly
polynomial time.
Proof. Let x be the (unique) optimal solution of min{gP(x) : B(f)}. Let δ be
any positive number and (u, v) be any ordered pair of elements in V such that
x + δ(eu −ev) ∈B(f). By Corollary 3 and the uniqueness of the minimum
norm point, in order to verify the statement, it suﬃces to show the inequality
x(u) ≥x(v). Assume, to the contrary, x(u) < x(v). Put β = x(v) −x(u) > 0
and y = x + β(eu −ev). By the deﬁnition of gP, we have gP(x) = gP(y). Let
ε = 1
2 min{δ, β}. Then we have 0 < ε < β and x + ε(eu −ev) ∈B(f). The strict
convexity of gP implies gP(x) = (1 −ε
β)gP(x) + ε
βgP(y) > gP(x + ε(eu −ev)), a
contradiction to the deﬁnition of x.
⊓⊔
Let w be a positive vector. Next we consider a class of problems whose opti-
mal solutions are the lex-optimal bases. For distinct elements u, v ∈V , deﬁne
a cone E(u, v) ⊂RV by E(u, v) = {x ∈RV
≥0 : x(u)/w(u) = x(v)/w(v)}. Let
gL(x) : RV →R ∪{+∞} be a diﬀerentiable and strictly convex function with
dom gL ⊇RV
≥0. For any x ∈RV
≥0 and each v ∈V , let Dv(x) = ∂gL(x)
∂x(v) . Now we
consider the minimization of gL over B(f) in the case where gL satisﬁes
L1. Du(x) = Dv(x), for each u, v ∈V and any x ∈E(u, v);
L2. For any x ∈RV
≥0 and u, v ∈V , function cu, v(λ) = Du(x + λ(eu −ev))
is strictly increasing on the interval {λ ∈R : x + λ(eu −ev) ∈RV
≥0}.
Clearly, the separable function gW(x) deﬁned in Section 3 satisﬁes these prop-
erties. For example, if V = {1, 2, 3} and w = (2, 2, 3), then x(1)2 + x(2)2 +
3
4x(3)2 + 1
2x(1)x(2)+ 1
2x(1)x(3)+ 1
2x(2)x(3) satisﬁes L1 and L2. Using Corollary
3, we can generalize Corollary 9 as follows.
Corollary 14.
Suppose that gL satisﬁes properties L1 and L2 and xL ∈RV is
the lex-optimal base of B(f) w.r.t. w. Then xL is the (unique) optimal solution
to problem minx{gL(x) : x ∈B(f)}.

On Convex Minimization over Base Polytopes
265
Proof. Let x be the optimal solution to min{gL(x) : B(f)}, and let δ > 0 be any
number and (u, v) be any pair of elements such that x + δ(eu −ev) ∈B(f). It
suﬃces to show x(u)
w(u) ≥x(v)
w(v). Assume that x(u)
w(u) < x(v)
w(v). We let β > 0 be a number
such that x(u)+β
w(u)
= x(v)−β
w(v) . Put y = x + β(eu −ev). Property L1 implies that
Du(y) = Dv(y), and property L2 implies that Du(x) < Du(y) and Dv(y) < Dv(x).
So we have Du(x) < Dv(x). Thus, ∃ε > 0 such that x + ε(eu −ev) =: x ∈B(f)
and gL(x) > gL(x), which contradicts the deﬁnition of x.
⊓⊔
Acknowledgements
I thank Satoru Iwata for a number of useful suggestions and I am grateful to Tom
McCormick for helpful comments. I also thank Satoru Fujishige, who informed
me of [16].
References
1. R. E. Bixby, W. H. Cunningham and D. M. Topkis: The partial order of a polyma-
troid extreme point. Mathematics of Operations Research, 10 (1985), pp. 367–378.
2. F. A. Chudak and K. Nagano: Eﬃcient solutions to relaxations of combinatorial
problems with submodular penalties via the Lov´asz extension and non-smooth
convex optimization. Proceedings of the 18th Annual ACM-SIAM Symposium on
Discrete Algorithms (2007), pp. 79–88.
3. B. Dutta: The egalitarian solution and reduced game properties in convex games.
International Journal of Game Theory, 19 (1990), pp. 153–169.
4. B. Dutta and D. Ray: A Concept of egalitarianism under participation constraints.
Econometrica, 57 (1989), pp. 615–635.
5. J. Edmonds: Submodular functions, matroids, and certain polyhedra. In R. Guy,
H. Hanai, N. Sauer, and J. Sch¨onheim, editors, Combinatorial Structures and Their
Applications, Gordon and Breach, New York, 1970, pp. 69–87.
6. L. Fleischer and S. Iwata: A push-relabel framework for submodular function mini-
mization and applications to parametric optimization. Discrete Applied Mathemat-
ics, 131 (2003), pp. 311–322.
7. S. Fujishige: Lexicographically optimal base of a polymatroid with respect to a
weight vector. Mathematics of Operations Research, 5 (1980), pp. 186–196.
8. S. Fujishige: Submodular systems and related topics. Mathematical Programming
Study, 22 (1984), pp. 113–131.
9. S. Fujishige: Submodular Functions and Optimization (Second Edition). Elsevier,
Amsterdam, 2005.
10. G. Gallo, M. D. Grigoriadis and R. E. Tarjan: A fast parametric maximum ﬂow
algorithm and applications. SIAM Journal on Computing, 18 (1989), pp. 30–55.
11. H. Groenevelt: Two algorithms for maximizing a separable concave function over a
polymatroid feasible region. European Journal of Operational Research, 54 (1991),
pp. 227–236.
12. A. Hayrapetyan, C. Swamy and ´E. Tardos: Network design for information net-
works. Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete Algo-
rithms (2005), pp. 933–942.

266
K. Nagano
13. D. S. Hochbaum: Lower and upper bounds for the allocation problem and other
nonlinear optimization problems. Mathematics of Operations Research, 19 (1994),
pp. 390–409.
14. S. Iwata: A faster scaling algorithm for minimizing submodular functions. SIAM
Journal on Computing, 32 (2003), pp. 833–840.
15. K. Jain and V. V. Vazirani: Eisenberg-Gale markets: algorithms and structural
properties. Proceedings of the 39th ACM Symposium on Theory of Computing
(2007), to appear.
16. F. Maruyama: A uniﬁed study on problems in information theory via polymatroids.
Graduation Thesis, University of Tokyo, Japan, 1978. (In Japanese.)
17. S.
T.
McCormick:
Submodular
function
minimization.
In
K.
Aardal,
G. L. Nemhauser, and R. Weismantel, editors, Discrete Optimization (Handbooks
in Operations Research and Management Science 12), Elsevier, Amsterdam, 2005,
Chapter 7, pp. 321–391.
18. N. Megiddo: Optimal ﬂows in networks with multiple sources and sinks. Mathe-
matical Programming, 7 (1974), pp. 97–107.
19. K. Murota: Note on the universal bases of a pair of polymatroids. Journal of
Operations Research Society of Japan, 31 (1988), pp. 565–573.
20. K. Murota: Discrete Convex Analysis. SIAM, Philadelphia, 2003.
21. J. B. Orlin: A faster strongly polynomial time algorithm for submodular function
minimization. Proceedings of the 12th IPCO Conference (2007). This proceedings.
22. A. Schrijver: A combinatorial algorithm minimizing submodular functions in
strongly polynomial time. Journal of Combinatorial Theory (B), 80 (2000), pp.
346–355.
23. Y. Sharma, C. Swamy and D. P. Williamson: Approximation algorithms for prize-
collecting forest problems with submodular penalty functions. Proceedings of the
18th Annual ACM-SIAM Symposium on Discrete Algorithms (2007), pp. 1275–
1284.

Computational Geometric Approach to
Submodular Function Minimization for
Multiclass Queueing Systems
Toshinari Itoko1 and Satoru Iwata2
1 Tokyo Research Laboratory, IBM Japan
itoko@jp.ibm.com
2 Research Institute for Mathematical Sciences, Kyoto University
iwata@kurims.kyoto-u.ac.jp
Abstract. This paper presents an eﬃcient algorithm for minimizing a
certain class of submodular functions that arise in analysis of multiclass
queueing systems. In particular, the algorithm can be used for testing
whether a given multiclass M/M/1 achieves an expected performance by
an appropriate control policy. With the aid of the topological sweeping
method for line arrangement, our algorithm runs in O(n2) time, where
n is the cardinality of the ground set. This is much faster than direct
applications of general submodular function minimization algorithms.
1
Introduction
Let V be a ﬁnite set of cardinality n. For a vector x := [xi]i∈V indexed by
V and a subset X ⊆V , we denote 
i∈X xi by x(X). Let h be a nonnegative
nondecreasing convex function. This paper deals with the problem of ﬁnding a
subset X ⊆V that minimizes
f(X) := z(X) −y(X) h(x(X))
(X ⊆V )
(1)
for given nonnegative vectors x, y, z indexed by V . Such a minimization prob-
lem arises in performance analysis of the most fundamental multiclass queueing
system — multiclass M/M/1 (see Section 2).
This problem is a special case of submodular function minimization. A set
function f is called submodular if it satisﬁes
f(X) + f(Y ) ≥f(X ∩Y ) + f(X ∪Y ),
∀X, Y ⊆V.
It can be shown that the function f in (1) is submodular (see Appendix). Re-
cent results on submodular functions are expounded in Fujishige [10] and in
McCormick [16].
A number of strongly polynomial algorithms have been devised for general
submodular function minimization. The ﬁrst one due to Gr¨otschel, Lov´asz, and
Schrijver [12,13] is based on the ellipsoid method, which is not eﬃcient in prac-
tice. Combinatorial strongly polynomial algorithms are devised independently
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 267–279, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

268
T. Itoko and S. Iwata
by Schrijver [19] and by Iwata, Fleischer, and Fujishige [14]. However, these
combinatorial algorithms are not yet very fast. Even a very recent algorithm
of Orlin [17], which is currently the fastest combinatorial strongly polynomial
algorithm, runs in O(n5γ + n6) time, where γ designates the time required for
computing the function value of f. Thus, it is still desirable to have a faster
algorithm for minimizing a speciﬁc class of submodular functions that naturally
arise in applications.
Instead of applying an algorithm for general submodular function minimiza-
tion, we take a completely diﬀerent approach based on computational geometry.
The ﬁrst step is to interpret our problem in a three-dimensional space as fol-
lows. Each subset X ⊆V corresponds to a point (x(X), y(X), z(X)) in the
three-dimensional space. The original problem is then equivalent to ﬁnding the
minimum value of f(x, y, z) := z −y h(x) among all such points (x, y, z) corre-
sponding to the subsets of V .
The convex hull of these 2n points forms a special polytope called zonotope.
It will be shown that the minimizer of f is among the lower extreme points of
the zonotope, i.e., extreme points that are visible from below (see Section 3).
The number of such lower extreme points are bounded by O(n2). Furthermore,
exploiting the duality relation between a zonotope in a three-dimensional space
and a line arrangement in a plane, we are able to enumerate all the lower ex-
treme points in O(n2) time with the aid of the topological sweeping method of
Edelsbrunner and Guibas [6,7]. Thus our algorithm ﬁnds a minimizer of f in
O(n2) time and O(n) space, if the function value of h is computable in a con-
stant time. This is substantially more eﬃcient than direct applications of general
submodular function minimization algorithms.
In terms of the application to multiclass M/M/1, the above algorithm provides
an eﬃcient way of testing if a given performance speciﬁcation is achievable by
some control policy. Designing an appropriate control policy in the achievable
case is another issue. Extending our approach, we also devise an algorithm for
doing this in O(n4) time, which is yet faster than general submodular function
minimization algorithms (see Section 4).
2
Multiclass Queueing Systems
This section is devoted to a brief exposition on a connection between our min-
imization problem and the performance analysis of the fundamental multiclass
queueing system called multiclass M/M/1. For comparison, we also give a brief
description of the same type of problems for nonpreemptive case.
2.1
Preemptive M/M/1
Multiclass M/M/1 is a system which deals with various types of jobs whose
arrival interval and service time follow exponential distributions. Each job of
diﬀerent classes wait in diﬀerent queues and the server chooses the job to serve
the next by a control policy. A queueing system allowing preemptive control

Computational Geometric Approach to Submodular Function
269
policies is called preemptive. In the following, the set of classes is denoted by
V = {1, 2, . . ., n}.
In a multiclass M/M/1, when the expected arrival rates and the expected
service rates of the job classes are given, the performance of the system depends
only on the control policy. A region of performance-measuring vectors achieved
by all control policies is called achievable region (see e.g. [4]). The performance
of a multiclass M/M/1 is often measured by the expected staying time vector T
:= [ Ti ]i∈V , where Ti is the expected staying time in the system for class i jobs.
For preemptive multiclass M/M/1, achievable region of T is known as follows.
Theorem 1 ([3]). Consider a preemptive multiclass M/M/1 whose mean ar-
rival rates are λ:= [ λi ]i∈V and mean service rates are μ:= [ μi ]i∈V . Let ρi be
the utilization λi/μi of the server for class i jobs and assume 
i∈V ρi < 1 to
ensure the existence of equilibrium. The achievable region of the expected staying
time vector T := [ Ti ]i∈V is a polyhedron represented by 2n inequalities:

i∈X
ρiTi ≥

i∈X
ρi
μi
1 −

i∈X
ρi
,
∀X ⊆V.
(2)
Given a target expected staying time vector ˇT , it is important for system de-
signers to check performance achievability: whether ˇT is in the achievable region
( ˇT is achieved by some control policy) or not. This problem was posed by Fed-
ergruen and Groenevelt [8]. They provided an eﬃcient algorithm for the special
case of identical service time distribution. This assumption is too restrictive in
practice, as we usually classify the jobs by their properties including expected
service time.
If we deﬁne xi := ρi, yi := ρi
μi
and h(x) :=
1
1 −x, then y(X) h

x(X)

coincides
with the right-hand side function of (2). Furthermore, if we deﬁne zi := ρi ˇTi,
then the problem of checking performance achievability of preemptive multiclass
M/M/1 is reduced to our minimization problem. The target expected staying
time vector ˇT is achievable if and only if the minimum value of f is equal to
zero.
For preemptive multiclass M/M/1, there is an another representation of ach-
ievable region. Bertsimas, Paschalidis, and Tsitsiklis [2] and Kumar and Ku-
mar [15] independently observed that the achievable region is the projection of
a polyhedron in a higher dimensional space. This makes it possible to check the
achievability by solving linear programming problem, which however involves
O(n2) variables and O(n2) inequalities.
2.2
Nonpreemptive M/G/1
For nonpreemptive M/M/1, which does not allow preemption, the performance
achievability can be tested by a simpler method. The achievable region of the

270
T. Itoko and S. Iwata
expected waiting time in queue W := [ Wi ]i∈V is a polyhedron represented by
2n inequalities:

i∈X
ρi Wi ≥

i∈V
ρi
μi


i∈X
ρi
1 −

i∈X
ρi
,
∀X ⊆V.
This is obtained as a special case of the fact shown in Gelenbe and Mitrani [11]
that the achievable region of the nonpreemptive M/G/1, which admits general
service time distributions, is characterized by

i∈X
ρi Wi ≥

1
2

i∈V
λiM 2
i


i∈X
ρi
1 −

i∈X
ρi
,
∀X ⊆V,
where M 2
i denotes the second order moment of the service time distribution for
class i.
Let ˇ
W be a target expected waiting time vector. The problem of checking
performance achievability is reduced to minimizing a submodular function b in
the form of
b(X) := z(X) −h(x(X))
(X ⊆V ),
where xi := ρi, zi := ρi ˇ
Wi, and h(x) :=
c x
1 −x with c = 1
2

i∈V λiM 2
i . This is
much simpler than our problem. In fact, it can be solved by sorting job classes
in the order of zi/xi. For k = 0, 1, . . ., n, let Yk denote the set of k jobs with
smallest values of zi/xi. Then the minimizer of b is among the candidates Yk for
k = 0, 1, . . . , n. See Federgruen and Groenevelt [9] for validity.
3
Geometric Approach
In this section, we present an algorithm for ﬁnding a minimum value of f de-
ﬁned in (1). The problem can be seen in a three-dimensional space as follows.
Each subset X ⊆V corresponds to a point (x(X), y(X), z(X)) in the three-
dimensional space. Let R+ denote the set of nonnegative reals. We also write
u(X) := (x(X), y(X), z(X)) ∈R3
+ and U := {u(X) | X ⊆V }. Then our prob-
lem is equivalent to ﬁnding a point (x, y, z) ∈U that attains the minimum value
of f(x, y, z) = z−y h(x). An example of a contour surface of f is shown in Fig. 1.
The convex hull of U forms a special polytope called zonotope, which is deﬁned
by the bounded linear combination of vectors (for example, see Fig. 2), and we
denote the zonotope by Z, namely
Z := conv(U) =
	
i∈V
ηi ui





 0 ≤ηi ≤1 (∀i ∈V )

,

Computational Geometric Approach to Submodular Function
271
0
1
2
3
4
5
6
7
8
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
Fig. 1. A contour surface of f
in the case of h(x) = 2/(10 −x)
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
Fig. 2. A zonotope generated by
(4, 1, 1), (2, 1, 3), (1, 2, 3), (1, 4, 1)
where ui := u({i}) (∀i ∈V ).
A point (x, y, z) in Z is called a lower point if (x, y, z′) /∈Z for any z′ < z. If
in addition (x, y, z) is an extreme point of Z, it is called a lower extreme point.
The number of lower extreme points of Z is known to be O(n2), which is clariﬁed
in Section 3.3.
Our algorithm enumerates all the lower extreme points of Z, and then it
identiﬁes a lower extreme point that attains the minimum value of f. It will be
shown in Section 3.2 that the minimum value among these lower extreme points
is in fact the minimum value of f among all the points in U. How to enumerate
the lower extreme points will be described in Section 3.3. The total running time
of this algorithm is O(n2).
3.1
Lower Extreme Points
Every lower point of Z is described as a maximizer of a certain linear objective
function whose coeﬃcient of z is negative. For any α, β ∈R, we denote by
F(α, β) the set of maximizers for (α, β, −1) direction, namely
F(α, β) := Argmax{α x + β y −z | (x, y, z) ∈Z}.
For a ﬁxed (α, β), elements in V are classiﬁed by the sign of α xi + β yi −zi,
namely
S+(α, β) := { i ∈V | α xi + β yi −zi > 0 },
S◦(α, β) := { i ∈V | α xi + β yi −zi = 0 },
S−(α, β) := { i ∈V | α xi + β yi −zi < 0 }.
(3)
Then F(α, β) is given by
F(α, β) =

u(S+(α, β)) +

i∈S◦(α,β)
ηi ui




 ∀i ∈S◦(α, β), 0 ≤ηi ≤1

.
(4)

272
T. Itoko and S. Iwata
This implies the following lemma that characterizes the lower extreme points
of Z.
Lemma 1. A vector v is a lower extreme point of Z if and only if v = u(S+(α, β))
for some (α, β).
Proof. Since ui ≥0, it follows from (4) that u(S+(α, β)) is an extreme point of
F(α, β). Hence u(S+(α, β)) is an lower extreme point of Z. Conversely, suppose
v is an lower extreme point of Z. There exists a pair (α, β) such that v is
the unique maximizer of αx + βy −z in Z. Note that v = u(X) for some
X ⊆V . Then we have X = S+(α, β) ∪Y for some Y ⊆S◦(α, β). Furthermore,
since v is the unique maximizer, ui = 0 holds for any i ∈Y , which implies
u(X) = u(S+(α, β)).
We denote {S+(α, β) | α, β ∈R} by L. Then Lemma 1 asserts that the set of
lower extreme points are given by {u(X) | X ∈L}. The following two lemmas
concerning lower points of Z will be used in the proof of the validity of our
algorithm in Section 3.2.
Lemma 2. Any lower point v that is on an edge of Z is a convex combination
of two lower extreme points u(X1) and u(X2) with X1 ⊆X2.
Proof. There exists a pair (α, β) such that F(α, β) is the edge that contains
v. Then it follows from (4) that F(α, β) is a line segment between u(X1) and
u(X2), where X1 = S+(α, β) and X2 = S+(α, β) ∪S◦(α, β).
Lemma 3. Any lower point v of Z is a convex combination of some at most
three lower extreme points u(X0), u(X1), and u(X2) with X0 ⊆X1 ⊆X2.
Proof. There exists a pair (α, β) such that F(α, β) is the minimal face that
contains v. Then u(X0) with X0 = S+(α, β) is an extreme point of F(α, β). Let
t be the intersection of the half line from u(X0) through v and the boundary
of F(α, β). Note that v is a convex combination of u(X0) and t. Since t is on
an edge of Z, Lemma 2 implies that t is a convex combination of lower extreme
points u(X1) and u(X2) with X1 ⊆X2. Furthermore, since u(X1) and u(X2)
are extreme points of F(α, β), we have X0 ⊆X1, X2. Therefore, v is a convex
combination of u(X0), u(X1), and u(X2) with X0 ⊆X1 ⊆X2.
3.2
Finding the Minimum Value
The following theorem shows that it suﬃces to examine the lower extreme points
of Z on behalf of the points in U. This leads us to an eﬃcient algorithm for
ﬁnding the minimum value of f, provided that an enumeration algorithm for the
lower extreme points is available.
Theorem 2. The minimum value of f is attained by a member of L, i.e.,
min{f(X) | X ⊆V } = min{f(X) | X ∈L}.

Computational Geometric Approach to Submodular Function
273
Proof. Let ¯v = (¯x, ¯y, ¯z) be a lower point of Z such that ¯x = x(Y ) and ¯y = y(Y )
for Y ⊆V . By Lemma 3, there exist three lower extreme points u(X0), u(X1),
and u(X2) of Z with X0 ⊆X1 ⊆X2 such that
¯v = σ0u(X0) + σ1u(X1) + σ2u(X2)
for some σ0, σ1, σ2 ≥0 with σ0 + σ1 + σ2 = 1. We denote u(Xj) by (xj, yj, zj)
for j = 0, 1, 2. Then we have
¯y h(¯x) = (σ0y0 + σ1y1 + σ2y2) h(σ0x0 + σ1x1 + σ2x2)
≤(σ0y0 + σ1y1 + σ2y2)(σ0h(x0) + σ1h(x1) + σ2h(x2))
= σ0y0h(x0) + σ1y1h(x1) + σ2y2h(x2) −σ0σ1(y1 −y0)(h(x1) −h(x0))
−σ1σ2(y2 −y1)(h(x2) −h(x1)) −σ0σ2(y2 −y0)(h(x2) −h(x0))
≤σ0y0h(x0) + σ1y1h(x1) + σ2y2h(x2),
where the ﬁrst inequality follows from the convexity of h and the second one
from the monotonicity. Since z(Y ) ≥¯z = σ0z0 + σ1z1 + σ2z2, we obtain
f(Y ) = z(Y ) −y(Y ) h(x(Y ))
≥¯z −¯y h(¯x)
≥σ0(z0 −y0h(x0)) + σ1(z1 −y1h(x1)) + σ2(z2 −y2h(x2))
= σ0f(X0) + σ1f(X1) + σ2f(X2).
Therefore, if f(Y ) attains the minimum value, then X0, X1, and X2 must attain
the minimum value as well. Thus the minimum value of f is attained by a
member of L.
3.3
Duality Between Zonotope and Hyperplane Arrangement
In this section, we discuss how to enumerate all the lower extreme points of Z.
A one-to-one correspondence has been established between zonotopes in the d-
dimensional space and hyperplane arrangements in the d −1-dimensional space
(see e.g. [5,20]). We exploit this duality principle with d = 3.
To visualize the transition of S+(α, β) in L with respect to α and β, we
consider the arrangement of n lines li : α xi + β yi −zi = 0, for i ∈V in the
(α, β)-plane. Then it follows from Lemma 1 that the lower extreme points of Z
corresponds to the cells in this line arrangement. Note that the number of cells
in the line arrangement is O(n2), and so is the number of lower extreme points
of Z. Further correspondence between the lower faces of Z and the components
of the line arrangement are summarized in Table 1 (see also Fig. 3).
Based on this duality, it suﬃces to enumerate all the cells of the line arrange-
ment. Since x and y are nonnegative, algorithms for sweeping the arrangement
along α or β axis keep a maximal chain on V that corresponds to n cells and
enumerate all the cells one by one with updating the chain. Our algorithm main-
tain not only the chain but also the vectors u(S+) for all S+ in the chain to

274
T. Itoko and S. Iwata
Table 1. Correspondence between lower faces of Z and components of line arrangement
Lower faces of Z Components of line arrangement
extreme point
cell
edge
line segment
facet
intersection point
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
0
1
2
3
4
5
6
7
8
 
  
 

 



 







 


 




 


Fig. 3. Correspondence between a lower extreme point of Z and a cell of line arrange-
ment
compute the value of f(S+) in a constant time on expected. This is achieved by
minor modiﬁcations of existing algorithms at no additional expense of running
time bound and space complexity.
For sweeping line arrangement, the topological sweeping method [6,7] is the
most eﬃcient algorithm, which runs in O(n2) time and O(n) space. Thus we
obtain an algorithm to solve the minimization problem in O(n2) time and O(n)
space. An implementation of the topological sweeping method is available from
the Web [18].
4
Extension
The algorithm presented in Section 3 enables us to determine whether a preemp-
tive multiclass M/M/1 achieves a performance by some control policy or not.
However, even if a given performance speciﬁcation turns out to be achievable,
the algorithm does not yield a concrete control policy. In the real application,
an eﬃcient algorithm for ﬁnding such an achieving control policy is essential. In
order to ﬁnd an achieving control policy in the achievable case, we discuss the
following problem.
Let g be a set function deﬁned by
g(X) := y(X) h(x(X))
(X ⊆V )
(5)

Computational Geometric Approach to Submodular Function
275
with nonnegative vectors x, y indexed by V and a nonnegative nondecreasing
convex function h. Consider a polytope (i.e. bounded polyhedron)
B(g) := {z | z(X) ≥g(X), ∀X ⊂V and z(V ) = g(V )}.
(6)
We now deal with the problem of ﬁnding a convex combination of some extreme
points of B(g) for a given point on B(g). Generally, a polytope in the form of
B(g) is called a base polytope if g is supermodular i.e.,
g(X) + g(Y ) ≤g(X ∩Y ) + g(X ∪Y ),
∀X, Y ⊆V.
Since the function g in (5) is supermodular (see Appendix), this problem is
a membership problem on base polytope for a speciﬁc class of supermodular
function.
Recall Theorem 1 and deﬁne g with xi := ρi, yi := ρi
μi
and h(x) :=
1
1 −x
as in Section 2.1. For simplicity, we introduce a new performance vector z by
zi := ρi Ti. Without loss of generality, assume z(V ) = g(V ) for any performance
z. This assumption means that we consider only work-conserving system, which
never idle if there exists a job in the queue. Then the achievable region of z forms
a base polytope in the form of (6), and each of its extreme points is achieved
by some absolute priority policy; An absolute priority policy is a control policy
deﬁned by a total order ≺on V which gives preference to jobs of class i over
jobs of class j if i ≺j.
We provide an achieving control policy as a random mixture of absolute pri-
ority policies. For any achievable performance ˇz, our algorithm ﬁnds a convex
combination ˇz = n
k=1 ak π(k) where π(k) is an extreme point of B(g). An
achieving control policy is derived as a control policy which follows the kth
corresponding absolute priority policy with probability ak.
Note that such a policy is not the only achieving control policy, i.e. there can
be other policies that achieve a given vector of performance. For example, in the
case of a nonpreemptive multiclass M/G/c with identical servers, Federgruen-
Groenevelt [9] provided an algorithm for ﬁnding another type of achieving control
policy, which is a slightly generalized dynamic or delay dependent control policy,
where a job’s priority is proportional to its time spent in the queue with the
coeﬃcients being class dependent.
Generally, minimizing a submodular function f deﬁned by f(X) := z(X) −
g(X) (X ⊆V ) enables us to determine whether a point z is in B(g) or not. The
point z is in B(g) if and only if the minimum value of f, as well as f(V ), is
equal to zero. Most combinatorial algorithms for general submodular function
minimization yield a convex combination for z when z is in B(g). However the
algorithm shown in Section 3 does not.
By extending geometric approach, we present an eﬃcient algorithm ﬁnding a
convex combination for a given point on B(g). With the aid of the topological
sweeping method for plane arrangement, the algorithm runs in O(n4) time and
O(n2) space, that is still faster than general submodular function minimization
algorithms.

276
T. Itoko and S. Iwata
Algorithm for Finding a Convex Combination
As shown in Fig. 4, our algorithm ﬁnds extreme points contributing to a convex
combination by successively projecting a considering point into lower dimen-
sional faces of B(g). First, the algorithm restricts the considering face F into
the minimal face that includes the current point (Step 1). Note that the current
point is initialized by a given point on B(g). Second, it selects an appropriate
extreme point of F as an observer’s position for the next step (Step 2). If the
dimension of F is zero i.e. F is a point, it terminates. Third, with the observer,
it projects the current point onto a proper face of F (Step 3). The algorithm
terminates after at most n iterations (of Steps 1–3) since the dimension of the
considering face decreases at least one by one. The selected observers (extreme
points of B(g)) are the solution.
 


	





 

	

   


	









   


Fig. 4. Frame advance of the algorithm in the case of n = 3
The bottle neck of the algorithm is ﬁnding a projected point in Step 3. As
shown below, it can be reduced to sweeping cells of a plane arrangement in the
three-dimensional space. According to [1], this is possible in O(n3) time and
O(n2) space, which indicates that the entire algorithm runs in O(n4) time and
O(n2) space.
In Section 3, we proved that, for checking whether a point z is in B(g) or not,
it is suﬃcient to enumerate all the cells of an arrangement deﬁned by n lines in
the (α, β)-plane:
α xi + β yi −zi = 0
(i ∈V ).
(7)
This is because each of the cells corresponds to X ⊆V for which the validity
of an inequality z(X) ≥g(X) should be checked. The set of those subsets was
denoted by L. Note that all the deﬁnitions of z, u, Z and L in Section 3 depends
on the point z to be checked. If z varies, so do the three-dimensional points
u(X) (X ⊆V ). As the three-dimensional points moving, their convex hull Z
and its lower extreme points changes. We denote by L(z) the family of sets that
correspond to the lower extreme points of Z depending on z.
In Step 3, a point to be checked moves along the half line from an observer
vertex π through the current point ˇz. Any point to be checked is deﬁned by
z(t) := π + t (ˇz −π) with t ∈[1, +∞). Consider zi in (7) as a variable with t,

Computational Geometric Approach to Submodular Function
277
and replace zi with zi(t) = πi + t (ˇzi −πi) for all i ∈V . We obtain n planes in
the (α, β, t)-space:
α xi + β yi + t (πi −ˇzi) −πi = 0
(i ∈V ).
(8)
All the cells of the arrangement deﬁned by (8) corresponds to the set family
L′ := {S+(α, β, t) | α, β ∈R, t ∈[1, +∞)}
(9)
where S+(α, β, t) := {i ∈V | α xi + β yi −zi(t) > 0}. Since L′ is the union of
L(z(t)) for all t ∈[1, +∞), checking validity of the inequalities z(X) ≥g(X)
(X ∈L′) is suﬃcient to determine whether z(t) ∈B(g) for any t ∈[1, +∞). This
readily follows from the proofs in Section 3 for the case of a ﬁxed t. By selecting
the maximal t such that z(X) ≥g(X) is valid for all X ∈L′ and denoting it by
¯t, we can ﬁnd the projected point as z(¯t).
5
Conclusion
We have presented an eﬃcient algorithm for minimizing a class of submodular
functions that arises in queueing analysis: checking performance achievability of
preemptive multiclass M/M/1. With the aid of the topological sweeping method
for line arrangement, our algorithm runs in O(n2) time, which is much faster
than previously known methods. We have also presented a fast algorithm for
ﬁnding a concrete control policy in the achievable case.
Acknowledgments
We thank Takeshi Tokuyama of Tohoku University for helpful information on
topological sweeping method. We also thank Takayuki Osogami of Tokyo Re-
search Laboratory, IBM Japan, for helpful information on performance analysis
of priority queues.
References
1. E. Anagnostou, V. G. Polimenis, and L. J. Guibas. Topological sweeping in three
dimensions. In Proceedings of International Symposium on Algorithms, pages 310–
317. Springer-Verlag, 1990.
2. D. Bertsimas, I. Ch. Paschalidis, and J. N. Tsitsiklis. Optimization of multiclass
queueing networks: Polyhedral and nonlinear characterizations of achievable per-
formance. The Annals of Applied Probability, 4:43–75, 1994.
3. E. G. Coﬀman, Jr. and I. Mitrani. A characterization of waiting time performance
realizable by single-server queues. Operations Research, 28:810–821, 1980.
4. M. Dacre, K. Glazebrook, and J. Ni˜no-Mora. The achievable region approach to
the optimal control of stochastic systems. Journal of the Royal Statistical Society,
B, 61:747–791, 1999.
5. H. Edelsbrunner. Algorithms in Combinatorial Geometry. Springer-Verlag, 1987.

278
T. Itoko and S. Iwata
6. H. Edelsbrunner and L. J. Guibas. Topologically sweeping an arrangement. Journal
of Computer and Systems Sciences, 38:165–194, 1989.
7. H. Edelsbrunner and L. J. Guibas. Topologically sweeping an arrangement — a
correction. Journal of Computer and Systems Sciences, 42:249–251, 1991.
8. A. Federgruen and H. Groenevelt. Characterization and optimization of achievable
performance in general queueing systems. Operations Research, 36:733–741, 1988.
9. A. Federgruen and H. Groenevelt. M/G/c queueing systems with multiple customer
classes: Characterization and control of achievable performance under nonpreemp-
tive priority rules. Management Science, 34:1121–1138, 1988.
10. S. Fujishige. Submodular Function and Optimization. North-Holland, 2005.
11. E. Gelenbe and I. Mitrani. Analysis and Synthesis of Computer Systems. Academic
Press, 1980.
12. M. Gr¨otschel, L. Lov´asz, and A. Schrijver. The ellipsoid method and its conse-
quences in combinatorial optimization. Combinatorica, 1:169–197, 1981.
13. M. Gr¨otschel, L. Lov´asz, and A. Schrijver. Geometric Algorithms and Combinato-
rial Optimization. Springer-Verlag, 1988.
14. S. Iwata, L. Fleischer, and S. Fujishige. A combinatorial strongly polynomial al-
gorithm for minimizing submodular functions. Journal of the ACM, 48:761–777,
2001.
15. S. Kumar and P. R. Kumar.
Performance bounds for queueing networks and
scheduling policies. IEEE Transactions on Automatic Control, 39:1600–1611, 1994.
16. S.
T.
McCormick.
Submodular
function
minimization.
In
K.
Aardal,
G. Nemhauser, and R. Weismantel, editors, Handbook on Discrete Optimization.
Elsevier, 2005.
17. J. B. Orlin. A faster strongly polynomial time algorithm for submodular function
minimization. In Proceedings of the Twelfth International Conference on Integer
Programming and Combinatorial Optimization. Springer-Verlag, to appear.
18. E. Rafalin, D. Souvaine, and I. Streinu. Topological sweep in degenerate cases.
In Proceedings of the 4th International Workshop on Algorithm Engineering and
Experiments (ALENEX), pages 155–165, 2002. (Their implementation is avialable
from http://www.cs.tufts.edu/research/geometry/sweep/).
19. A. Schrijver.
A combinatorial algorithm minimizing submodular functions in
strongly polynomial time. Journal of Combinatorial Theory, B, 80:346–355, 2000.
20. G. M. Ziegler. Lectures on Polytopes. Springer-Verlag, 1995.
Appendix
This Appendix is devoted to showing that the function f is a submodular
function. For this purpose, it suﬃces to show that the function g deﬁned by
g(X) := y(X) h(x(X)) is supermodular, i.e.,
g(X) + g(Y ) ≤g(X ∩Y ) + g(X ∪Y ),
∀X, Y ⊆V.
Since h is convex, for any X ⊆V and i, j ∈V , we have
h

x(X ∪{i})

≤
xj
xi + xj
h

x(X)

+
xi
xi + xj
h

x(X ∪{i, j})

,
h

x(X ∪{j})

≤
xi
xi + xj
h

x(X)

+
xj
xi + xj
h

x(X ∪{i, j})

.

Computational Geometric Approach to Submodular Function
279
By adding these two inequalities, we obtain
h

x(X ∪{i})

+ h

x(X ∪{j})

≤h

x(X)

+ h

x(X ∪{i, j})

,
which implies that h(x(·)) is a supermodular function. Because of this super-
modularity, the nonnegativity of x and y, and the monotonicity of h, we have
g(X ∪Y ) + g(X ∩Y ) −g(X) −g(Y )
= h(x(X ∪Y )) y(X ∪Y ) + h(x(X ∩Y )) y(X ∩Y ) −h(X) y(X) −h(Y ) y(Y )
=

h(x(X ∪Y )) + h(x(X ∩Y )) −h(x(X)) −h(x(Y ))

y(X ∩Y )
+

h(x(X ∪Y )) −h(X)

y(X \ Y ) +

h(x(X ∪Y )) −h(Y )

y(Y \ X)
≥0
for any X, Y ⊆V . Thus g is shown to be supermodular. In addition, it is easy
to see that g(X) ≥0 for any X ⊆V and g(∅) = 0 hold.

Generating Multiple Solutions for
Mixed Integer Programming Problems
Emilie Danna, Mary Fenelon, Zonghao Gu, and Roland Wunderling
ILOG, Inc. 889 Alder Avenue, Suite 200, Incline Village, NV 89451
{edanna,mfenelon,gu,rwunderling}@ilog.com
Abstract. As mixed integer programming (MIP) problems become eas-
ier to solve in pratice, they are used in a growing number of applications
where producing a unique optimal solution is often not enough to answer
the underlying business problem. Examples include problems where some
optimization criteria or some constraints are diﬃcult to model, or where
multiple solutions are wanted for quick solution repair in case of data
changes. In this paper, we address the problem of eﬀectively generat-
ing multiple solutions for the same model, concentrating on optimal and
near-optimal solutions. We ﬁrst deﬁne the problem formally, study its
complexity, and present three diﬀerent algorithms to solve it. The main
algorithm we introduce, the one-tree algorithm, is a modiﬁcation of the
standard branch-and-bound algorithm. Our second algorithm is based on
MIP heuristics. The third algorithm generalizes a previous approach that
generates solutions sequentially. We then show with extensive computa-
tional experiments that the one-tree algorithm signiﬁcantly outperforms
previously known algorithms in terms of the speed to generate multiple
solutions, while providing an acceptable level of diversity in the solutions
produced.
1
Introduction
Solving a standard mixed-integer programming (MIP) model minx∈X cT x where
X = {x ∈
 d : Ax ≤b, xi ∈
, ∀i ∈I ⊆{1, . . . , d}} usually means ﬁnding
a solution x∗that is feasible: x∗∈X, and optimal: cT x∗≤cT x, ∀x ∈X.
However, there might exist not only one but several diﬀerent solutions that ﬁt
those two criteria. In this paper, we address the problem of generating multiple
feasible solutions eﬀectively for the same model, concentrating on optimal and
near-optimal solutions.
1.1
Motivation
The three main reasons that motivate generating multiple solutions instead of
only one come from the applications of mixed-integer linear programming. MIP
is used extensively in industry to make short-term and long-term decisions, such
as scheduling operations on various machines in a factory, deciding how much of
each product to manufacture, choosing new locations for additional factories, etc.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 280–294, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

Generating Multiple Solutions for Mixed Integer Programming Problems
281
However, the mathematical model given to a MIP solver is often a simpliﬁcation
of the real business problem. Such a model may leave out details that are diﬃcult
to express as linear expressions or that make the model hard to solve. The data
used in the mathematical model are also often an estimate of the ﬂuctuating
real data. More importantly, some optimization criteria are inherently subjective
and diﬃcult to quantify. For example, Schittekat and Sorensen [19] studied the
problem of choosing subcontractors in the automotive industry. The obvious
choice criterion is the price each subcontractor demands for the same job, but
there are other factors to consider, such as the quality of work and service, and
these factors are diﬃcult to quantify. Because of these diﬀerences between the
mathematical model and the real business problem, it is interesting to generate
multiple optimal or near-optimal solutions for the mathematical model so that
the decision maker can examine them, and, in the end, choose the best solution
overall, i.e., the one that also performs best for the criteria that could not be
expressed in the MIP model.
The second reason for generating multiple solutions is that MIP is increas-
ingly used beyond the simple framework of formulating a model, solving it, and
implementing the solution. If the data have changed between the moment the
model was written and the moment the solution is to be implemented (for ex-
ample in scheduling, if a machine has broken down), then it is valuable to have
immediately at hand an alternative solution that does not use this machine, or
a variety of solutions that can be used to repair the current solution. Another
application mentioned by Schittekat and Sorensen [19] is that being able to show
a subcontractor alternative solutions that have approximately the same cost and
use the subcontractor’s competitors was a very eﬀective tool in negotiating.
Finally, a more technical application is that MIP is increasingly used to solve
subproblems inside the branch-and-cut framework itself. For example, several
approaches [9,10,2,8] formulate cut separation as a MIP where the objective is to
maximize the cut violation, and each solution corresponds to a cut. Although the
eﬃciency of a cut can be predicted approximately by its violation, more complex
measures [14,5] could be useful to evaluate eﬃciency more accurately, but they
are too complex to express in the aforementioned MIP models. Moreover, adding
several cuts at a time is a well known technique to obtain quick progress in the
best bound. MIP models for cut separation would therefore be an interesting
application for generating multiple solutions.
1.2
Related Work
Generating multiple solutions to optimization problems has been the subject of
few papers: Lee et al. [15] generate multiple solutions for LP; Bacchus solves
#SAT in [1]; Schittekat and Sorensen [19] use metaheuristics to generate so-
lutions for a logistics problem. As for generating multiple solutions for MIP,
Glover et al. [11] present an interesting approach based on MIP heuristics. How-
ever, because of its heuristic nature, this approach lacks the capacity of proving
how many diﬀerent solutions exist for a given problem, and does not guaran-
tee generating all possible solutions. The work closest to our approach is by

282
E. Danna et al.
Greisdorfer et al. [12]. The authors compute two solutions to a MIP model, ei-
ther by solving two successive MIP models (sequential approach), or by solving
a MIP twice as big as the original model (simultaneous approach). Their paper
compares both algorithms in terms of performance and diversity of solutions
obtained, showing that the sequential approach outperforms the simultaneous
approach. The problem with both algorithms is that, although they can be gen-
eralized to p solutions instead of two, they do not scale well when p becomes
large, as we will show in Sec. 4.1.
1.3
Outline of the Paper
The remainder of the paper is organized as follows. Sec. 2 formally deﬁnes the
problems we are going to solve, examines their complexity and presents our mea-
sure for solution diversity. Sec. 3 describes our three algorithms for generating
multiple solutions. Sec. 4 presents computational results. Sec. 5 concludes with
a summary and directions for future work.
2
Deﬁnitions
2.1
Problem Deﬁnition and Complexity
Given a mixed integer programming model P = minx∈X cT x where X = {x ∈
 d : Ax ≤b, xi ∈
, ∀i ∈I ⊆{1, . . ., d}}, for which an optimal solution is x∗,
we deﬁne the following problems:
– MIP(p): Generate p diﬀerent feasible solutions for P
– #MIP: How many diﬀerent feasible solutions does P have?
– MIP(p, q): Generate p diﬀerent feasible solutions x(1), . . . , x(n) for P within
q% of the optimum, i.e., such that cT x(i) ≤cT x∗+q|cT x∗|/100, ∀i = 1, . . . , n
– #MIP(q): How many diﬀerent feasible solutions within q% of the optimum
does P have?
We consider two solutions to be diﬀerent if and only if they diﬀer by at least
one integer variable. The ﬁrst reason for not taking into account continuous
variables is that the main decision variables are integer, whereas continuous
variables usually are less important. Secondly, there might exist an inﬁnite num-
ber of solutions that diﬀer only by continuous variables, especially if there is no
constraint on the objective value of the solution. Thirdly, the computer repre-
sentation of real values is not exact, therefore it is diﬃcult to say in pratice that
two continuous variables are diﬀerent without resorting to numerical tolerances.
The four problems are at least as hard as MIP(1), therefore are NP-hard.
In addition, #MIP and #MIP(q) belong to #P, the class of counting prob-
lems [20,21]. It follows from the polynomial reduction from SAT to MIP that
#MIP and #MIP(q) are at least as hard as #SAT, therefore are #P-complete.
In the rest of this paper, we will focus on solving the objective-controlled
version of the problems: MIP(p, q) and #MIP(q), because, however imprecise
the objective modeling can be, it is still very important.

Generating Multiple Solutions for Mixed Integer Programming Problems
283
2.2
Solution Diversity
Next to objective value, diversity is the most important characteristic to take into
account when comparing sets of solutions produced by diﬀerent algorithms [12].
Indeed, multiple solutions are mostly useful if they are structurally diﬀerent from
each other [19].
We deﬁne the diversity of a set S of solutions as the average pairwise distance:
D(S) =
1
|S|2

s,s′∈S d(s, s′), where the distance between two solutions is the
Hamming distance on the set B of binary variables: d(s, s′) =
1
|B|

i∈B |si −s′
i|.
Our measure generalizes to |S| > 2 the diversity used in Greisdorfer et al. [12].
D(S) ≤1
2 for all sets S of solutions. Indeed, D(S) =
1
|S|2|B|

i∈B

s,s′∈S |si−
s′
i|. Looking at each variable i ∈B individually, it is clear that 
s,s′∈S |si−s′
i| is
maximal if ⌊|S|/2⌋of the solutions have si = 0 and the remaining solutions have
si = 1. In that case, at most half of the addends |si −s′
i| are equal to one, while
at least half of them are zero. It follows that D(S) ≤
1
|S|2|B|

i∈B
1
2|S|2 = 1
2.
3
Algorithms
We now describe the three algorithms considered in this paper in detail.
3.1
The One-Tree Algorithm
The standard branch-and-bound algorithm for solving integer programming
models aims at progressively reducing the search space as quickly and as much
as possible so that it is easier both to ﬁnd the optimal solution and to prove
that it is optimal. However, when the aim is to generate multiple solutions, the
perspective needs to be diﬀerent: if the search space is reduced too much, it
will not contain enough solutions. The one-tree algorithm we propose is adapted
from the standard branch-and-bound algorithm (outlined in Algorithm 1) for
this purpose1. It proceeds in two phases. During the ﬁrst phase (outlined in
Algorithm 2), the branch-and-bound tree is constructed and explored to ﬁnd
the optimal solution, and its nodes are kept for the second phase. During the
second phase (outlined in Algorithm 3), the tree built in the ﬁrst phase is reused
and explored in a diﬀerent way to yield multiple solutions. The diﬀerences with
the standard branch-and-bound algorithm relate to storing integer solutions,
fathoming nodes, branching, and dual tightening.
In standard branch-and-bound, an integer solution is stored only if it improves
on the incumbent. When generating solutions in the second phase, we store in
the set S all integer solutions that are within q% of the optimum value.
In standard branch-and-bound, a node is fathomed when the sub-model it
deﬁnes cannot yield any improving integer solution, i.e., when its LP solution is
integer-valued or has an objective value worse than the incumbent. In the ﬁrst
1 Algorithm 1 is of course a very rudimentary outline of branch-and-bound. We left
out many techniques, such as cuts and heuristics, and many implementation details
to concentrate on the features that diﬀer in the one-tree algorithm.

284
E. Danna et al.
Algorithm 1. Outline of standard branch-and-bound algorithm
Preprocessing
Set of open nodes: Nopen ←{rootnode}
Objective value of the incumbent: z∗←+∞
while
Nopen ̸= ∅
do
Choose a node n from Nopen
Solve LP at node n. Solution is x(n) with objective z(n).
if
z(n) ≥z∗
then
Fathom the node: Nopen ←Nopen \ {n}
else
if
x(n) is integer-valued
then
x(n) becomes new incumbent: x∗←x(n); z∗←z(n)
Do reduced cost fixing
Fathom the node: Nopen ←Nopen \ {n}
else
Choose branching variable i such that xi(n) is fractional
Build children nodes n1 = n ∩{xi ≤⌊xi(n)⌋} and n2 = n ∩{xi ≥⌊xi(n)⌋+ 1}
Nopen ←Nopen ∪{n1, n2} \ {n}
end if
end if
end while
Algorithm 2. Outline of one-tree algorithm: phase I
Preprocessing with only primal reductions
Set of open nodes: Nopen ←{rootnode}
Set of stored nodes: Nstored ←∅
Objective value of the incumbent: z∗←+∞
while
Nopen ̸= ∅
do
Choose a node n from Nopen
Solve LP at node n. Solution is x(n) with objective z(n).
if
z(n) ≥z∗
then
Fathom the node and keep it for phase II: Nopen ←Nopen \ {n}; Nstored ←Nstored ∪{n}
else
if
x(n) is integer-valued
then
x(n) becomes new incumbent: x∗←x(n); z∗←z(n)
Fathom the node and keep it for phase II: Nopen ←Nopen \ {n}; Nstored ←Nstored ∪{n}
else
Choose branching variable i such that xi(n) is fractional
Build children nodes n1 = n ∩{xi ≤⌊xi(n)⌋} and n2 = n ∩{xi ≥⌊xi(n)⌋+ 1}
Nopen ←Nopen ∪{n1, n2} \ {n}
end if
end if
end while
Algorithm 3. Outline of one-tree algorithm: phase II
Reuse tree from phase I: Nopen ←Nstored
Reuse incumbent from phase I: Set of solutions: S ←{x∗}
while
Nopen ̸= ∅
do
Choose a node n from Nopen
Solve LP at node n. Solution is x(n) with objective z(n)
if
z(n) > z∗+ q|z∗|/100
then
Fathom the node: Nopen ←Nopen \ {n}
else
if
x(n) is integer-valued then
x(n) is added to the pool of solutions if it is not a duplicate: if x(n) /
∈S, then S ←S ∪{x(n)}
end if
Choose branching variable i such that it is not fixed by the local bounds of node n: lbi(n) < ubi(n)
Build children nodes n1 = n ∩{xi ≤⌊xi(n)⌋} and n2 = n ∩{xi ≥⌊xi(n)⌋+ 1}
Nopen ←Nopen ∪{n1, n2} \ {n}
end if
end while
phase of the one-tree algorithm, nodes are fathomed by the same criterion but
instead of being discarded, they are stored for further examination during the
second phase. During the second phase, a node is fathomed if it cannot yield
any additional integer solution within q% of the optimum value, i.e., if its LP
solution is integer-valued and all integer variables have been ﬁxed by the local
bounds of the node, or if the objective value of its LP value is strictly more than
q% worse than the optimum value.

Generating Multiple Solutions for Mixed Integer Programming Problems
285
In standard branch-and-bound, only variables that are fractional in the node
LP solution are branched on. When generating solutions in the second phase,
we also branch on variables that are integral in the LP node solution if they are
not ﬁxed by local bounds. Suppose the node LP solution is integral and binary
variable xi = 0 at the node. There might exist some solutions with xi = 1; for
this reason, we need to create the right child node with xi ﬁxed to 1. But there
might also exist solutions with xi = 0 and diﬀerent values for other variables;
for this reason, we need to create the left child node with xi = 0. This branch,
however, contains the same integer solution as the parent. So, in order not to
count the same solution twice, either we check that it is not a duplicate of a
previous solution before adding it to the set S of solutions; or, we keep the LP
value of the branching variable in the parent node and discard the solution if
this value satisﬁes the local bounds of the variable at the node. Either way, it is
best to explore ﬁrst the branch that does not contain the solution of the parent
node in order to get more diverse solutions.
In standard branch-and-bound, the search space is pruned because of consid-
erations on the objective function. This pruning takes place during preprocess-
ing [13,6], and during the exploration of the tree through reduced cost ﬁxing [22].
In both phases of the one-tree algorithm, we do not want to eliminate poten-
tial solutions because they are suboptimal, so in theory we should turn oﬀdual
tightening completely, as outlined in Algorithm 2 and 3. However, this choice
has a signiﬁcant impact on performance, and turning oﬀdual tightening com-
pletely is not required in practice to obtain a large enough number of solutions.
In practice, we need to choose a reasonable tradeoﬀbetween speed and number
of solutions we wish to obtain. Not fathoming a node even if its objective value
exceeds the incumbent, as we explained above, can be seen as a ﬁrst level of
turning dual tightening oﬀ. It does not cost anything in terms of speed; it is just
expensive in terms of memory. The second level is to turn oﬀdual reductions
during reduced cost ﬁxing. This setting is the level we used in our ﬁrst set of ex-
periments. We will show in Sec. 4.1 how it impacts performance. The third level,
which is needed for exhaustive enumeration (see the experiments in Sec. 4.3), is
to also turn oﬀdual presolve reductions.
Let us note that one could enumerate solutions in a single phase, simply by
fathoming nodes and branching as in the second phase, and turning oﬀsome
dual tightening. We chose to separate the algorithm into two phases for the fol-
lowing reasons. The ﬁrst reason is computational: generating multiple solutions
carries a performance penalty. So, if we are solving MIP(p, q) or #MIP(q), we
want to avoid spending time generating solutions within q% of a suboptimal
incumbent which will be discarded later when the optimal solution is found, and
it turns out those solutions are more than q% above the optimum. The second
reason is that the one-tree algorithm is to be used as an interactive discovery
tool, where the tree is built once during phase I and explored many times dur-
ing successive invocations of the phase II algorithm — possibly with diﬀerent
additional constraints, diﬀerent q values, or diﬀerent stopping criteria — until
the user ﬁnds suitable solutions for his business problem. This is also why we

286
E. Danna et al.
chose to turn oﬀdual tightening completely instead of carrying out dual reduc-
tions based on the incumbent value augmented by q%, which is also a possible
implementation.
Finally, let us mention the stopping criterion for both phases. As with the
standard branch-and-bound algorithm, the algorithm can be stopped in both
phases before the set of open nodes is empty for reasons such as time limit,
node limit, number of solutions generated, etc. An interesting case is the gap
criterion. During standard branch-and-bound, the best bound value (minimum
of objective value over open nodes) is always less than the incumbent. During
the second phase, it will happen, however, that the best bound value becomes
greater than the incumbent as nodes get fathomed. If the gap becomes less than
−q%, then no additional solutions within q% of the optimum can be generated,
and the algorithm can be stopped.
3.2
Heuristics
Heuristics are a natural way to generate multiple solutions [11,19]. The algo-
rithm we propose, outlined in Algorithm 4, is similar to the one-tree algorithm.
The diﬀerence is ﬁrst that solutions are generated mainly by MIP heuristics such
as ﬁx-and-dive heuristics [3], RINS [7], and solution polishing [17,18], instead of
relying only on the integral node LP solutions. In addition, heuristics can gener-
ate solutions that violate local bounds; therefore it is not necessary to store the
fathomed nodes as in the ﬁrst phase of the one-tree algorithm. Finally, it should
be noted that this algorithm is not eﬃcient for exhaustive enumeration (#MIP
and #MIP(q)), as heuristics do not explore the search space systematically like
the one-tree algorithm and risk generating the same solutions many times over.
Algorithm 4. Outline of the algorithm using heuristics for MIP(p, q)
Phase I:
Solve the model with standard branch-and-bound
Optimal solution is x∗with objective value z∗
Set of solutions: S ←{x∗}
Phase II:
Start a new tree: Nopen ←{rootnode}
while
|S| < p
do
Choose a node n from Nopen
Solve LP at node n.
Run fix-and-dive heuristics, RINS, and solution polishing with an objective cutoff of z∗+ q|z∗|/100
If the solutions found are not duplicate of already stored solutions, they are added to S.
Rules for fathoming nodes, branching, and dual tigthening are the same as for the second phase of the one-tree
algorithm.
end while
3.3
The Sequential Algorithm
The last algorithm we present in this paper is a simple generalization of the
sequential generation of Greisdorfer et al. [12]. We present it mainly for com-
parison with previous work and for its variant that maximizes diversity. Instead

Generating Multiple Solutions for Mixed Integer Programming Problems
287
of generating multiple solutions using a unique branch-and-bound tree as in the
one-tree algorithm and in heuristics, we solve here a sequence of integer program-
ming models, each providing one solution. The details are given in Algorithm 5.
Algorithm 5. Sequential algorithm for #MIP(q)
Solve P with standard branch-and-bound. Optimal solution is x∗of cost z∗.
Set of solutions: S ←{x∗}
B = set of binary variables
Add constraint on objective value: X ←X ∩{cT x ≤z∗+ q|z∗|/100}
while
P is feasible
do
Change objective function of P to maximizing distance to already discovered solutions:
max 
s∈S

i∈B:si=0 xi + 
i∈B:si=1 (1 −xi)
Add diversity constraint to exclude the previously found solution:
X ←X ∩{
i∈B:x∗
i =0 xi + 
i∈B:x∗
i =1 (1 −xi) ≥1}
Solve P with standard branch-and-bound. Optimal solution is x∗.
Store the new solution: S ←S ∪{x∗}
end while
It is easy to see that this sequential algorithm will be slow to generate a large
number of solutions, as no information is reused from one iteration to the next.
However, the advantage of this algorithm is that any objective function can be
used once the optimal solution of the original problem has been obtained. Our
computational experience is that the most eﬀective way is to use the original
objective. In the rest of the paper, we will refer to this algorithm as the plain
sequential algorithm. But, as outlined in Algorithm 5, we can also try to max-
imize the distance to already discovered solutions. This algorithm is a greedy
procedure that aims at maximizing the diversity of the set of solutions obtained
in the end, as deﬁned in Sec. 2.2.
4
Computational Results
When we evaluate the performance of algorithms that generate multiple solu-
tions, several dimensions, possibly mutually conﬂicting, need to be considered:
the number of solutions generated, the solving time needed to generate these
solutions, the objective value and the diversity of the solutions generated. For a
comparison of algorithms to be valid, it is best to control as many dimensions as
possible and to let only one or two vary at a time. This consideration is an addi-
tional reason why all our experiments are about the objective-controlled version
of the problems: we solve MIP(10, 1), #MIP(1), and #MIP(0).
All experiments were carried out with CPLEX 10.1 on a 3.4 GHz GNU/Linux
machine with 2 GB memory. We experimented with models from MIPLIB3.0 [4]
and MIPLIB2003 [16] that can be solved to optimality within half an hour.
4.1
Comparison of Performance
The ﬁrst set of experiments answers the question of how fast each algorithm
generates solutions. We compare the time needed for each algorithm to solve
MIP(10, 1), i.e., to generate 10 solutions within 1% of the optimum. For this

288
E. Danna et al.
experiment, we also enforce a time limit of 1 hour, except for the sequential
algorithm maximizing diversity. Additionally, we answer the question in a slightly
diﬀerent way by comparing how many solutions within 1% of the optimum each
algorithm generates in 1 hour. For this experiment, we also enforce a solution
limit of 100000. The results, detailed model by model, are given in Table 2,
page 290, and Table 3, page 291. Since the limits of 10 solutions or one hour are
rather arbitrary, we also present graphically in Fig. 1 the evolution of the number
of solutions in function of the time needed to generate them for model 10teams.
In summary, when generating 10 solutions, the ﬁrst phase of the one-tree
algorithm is on average 2.2 times slower than the ﬁrst phase of heuristics2. This
diﬀerence is due to the fact that dual tightening is turned oﬀduring the tree
exploration for the one-tree algorithm. But, during the second phase, the one-tree
algorithm is on average 10.9 times faster than heuristics to generate solutions.
When comparing the total time, the one-tree algorithm has clearly the best
performance: it is on average 2.1 times faster than heuristics, 5.5 times faster
than the plain sequential algorithm and 20.2 times faster than the sequential
algorithm maximizing diversity. These results are corroborated by the number
of solutions that each algorithm can generate in one hour: the one-tree algorithm
generates on average 2.5 times more solutions than heuristics, 18.2 times more
solutions than the plain sequential algorithm, and 52.1 times more solutions than
the sequential algorithm maximizing diversity.
4.2
Comparison of Diversity
Only one of the algorithms we have presented explicitly tries to maximize the
diversity of the set of solutions to be obtained. However, the experiments of
the previous section showed that the sequential algorithm maximizing diversity
is much too slow to be practical. We were, therefore, curious to know whether
the better performance of the one-tree algorithm, heuristics, and, to a lesser
extent, the plain sequential algorithm, were obtained at the expense of a smaller
diversity in the solutions these two algorithms generate. Table 4 at page 292
compares the diversity obtained by each algorithm when solving MIP(10, 1). On
average, the diversity of the one-tree algorithm, of heuristics, and of the plain
sequential algorithm are respectively 3.5, 5.1, and 3.3 smaller than the diversity
obtained by the sequential algorithm when maximizing diversity. Given that the
ﬁrst two algorithms are signiﬁcantly faster and have much room for improvement
in solution diversity, we believe that these results are encouraging and show an
interesting trade-oﬀbetween performance and diversity. Our future work will be
directed at improving the diversity of the solutions they produce.
4.3
Exhaustive Enumeration of Optimal Solutions
Our last set of experiments concerns solving #MIP(0), i.e., enumerating all
possible optimal solutions. We have restricted ourselves to models that contain
2 When presenting average numbers, we compute the geometric mean of the ratio of
the performance of the two algorithms compared.

Generating Multiple Solutions for Mixed Integer Programming Problems
289
 0
 100
 200
 300
 400
 500
 0
 600
 1200
 1800
 2400
 3000
 3600
Number of solutions
Time (seconds)
One-tree
Heuristics
Sequential (plain)
Sequential (max. diversity)
Fig. 1. Number of solutions generated over time for model 10teams
Table 1. Enumeration of all optimal solutions for pure binary models
Model
Number of
One-tree time
Sequential time
optimal solutions (in seconds)
(in seconds)
10teams ≥14764
> 1 day (found 14764 sol.)
> 1 day (found 470 sol.)
air03
1
49.96
1.54
air04
8
37.70
166.41
air05
2
115.32
51.95
cap6000 1
7178.83
6.81
disctom ≥547
> 1 day (found 547 sol.)
> 1 day (found 130 sol.)
enigma
3
0.90
1.24
l152lav
1
3.92
1.89
lseu
2
0.27
0.37
mitre
80
234.82
993.45
mod008 6
1.83
1.46
mod010 128
255.81
353.89
nw04
1
499.62
74.74
p0033
9
0.01
0.11
p0201
4
0.79
1.68
p0282
1
0.73
0.31
p0548
≥100000
> 551.66 (found 100000 sol.)
> 1 day (found 2940 sol.)
p2756
≥100000
> 13519.22 (found 100000 sol.) > 1 day (found 1401 sol.)
stein27
2106
5.60
19819.2
stein45
70
50.71
1679.87
only binary variables because of the ambiguity of what all solutions mean when
continuous variables are involved (see Sec. 2.2), and because the sequential algo-
rithm cannot handle general integer variables (although the one-tree algorithm
can). Table 1 shows that we successfully enumerate all optimal solutions for 16

290
E. Danna et al.
Table 2. Time (in seconds) to enumerate 10 solutions within 1% of the optimum. The ‡
symbol means that the sequential algorithm maximizing diversity had to be limited to
two hours for each iteration in order to produce 10 solutions in a reasonable time.
Model
One-tree One-tree One-tree Heuristics Heuristics Heuristics Sequential Sequential
phase I
phase II total
phase I
phase II
total
plain
max diversity
10teams
68.79
54
122.79
31.32
215.6
246.92
260.89
72103.9 ‡
aﬂow30a
104.85
17.97
122.82
23.57
72.44
96.01
375.26
1221.3
air03
0.58
1.04
1.62
0.55
2.78
3.33
24.41
35.98
air04
41.28
2.74
44.02
17.1
19.84
36.94
248.41
38772.7 ‡
air05
41.31
6.52
47.83
19.42
125.67
145.09
281
7307.85 ‡
arki001
3305.11
0.92
3306.03
27.43
3.62
31.05
2634.18
64954.7 ‡
bell3a
9.17
0.07
9.24
2.76
0.53
3.29
5.64
2.85
bell5
0.14
0.01
0.15
0.13
0.07
0.2
2.17
2.79
blend2
8.17
1.18
9.35
2.56
14.15
16.71
28.47
70.57
cap6000
12.38
0.51
12.89
0.56
0.82
1.38
14.8
49.18
dcmulti
0.69
0.07
0.76
0.54
0.59
1.13
6.42
46.18
disctom
362.47
387.55
750.02
363.48
>3600
>3600
2255.54
54780.6 ‡
dsbmip
0.44
0.38
0.82
0.35
1.35
1.7
3
17.86
egout
0.01
0.01
0.02
0.01
1.69
1.7
0.05
0.04
enigma
0.16
0.79
0.95
0.2
6.74
6.94
2.24
1.47
ﬁber
1.04
0.24
1.28
0.22
1052.65
1052.87
11.99
18.65
ﬁxnet6
1.44
0.08
1.52
1.78
15.87
17.65
20.11
16.14
ﬂugpl
0.01
>3600
>3600
0.01
>3600
>3600
>3600
>3600
gen
0.02
0.07
0.09
0.02
1.04
1.06
0.49
1.78
gesa2
0.99
0.15
1.14
0.4
0.33
0.73
5.4
105.1
gesa2 o
4.04
0.09
4.13
2.34
0.28
2.62
48.82
3266.6
gesa3
1.13
0.23
1.36
0.68
1.52
2.2
72.03
864.76
gesa3 o
0.89
0.13
1.02
0.8
1.01
1.81
17.97
48543.2 ‡
gt2
0.01
0.04
0.05
0.01
0.07
0.08
0.17
0.44
khb05250
0.14
0.2
0.34
0.11
66.15
66.26
2.44
4.97
l152lav
1.4
0.19
1.59
0.94
4.03
4.97
16.21
53.93
lseu
0.23
0.18
0.41
0.1
2407.72
2407.82
0.97
1.11
mas76
213.48
26.61
240.09
81.94
16.75
98.69
2188.01
1146.94
misc03
0.53
0.02
0.55
0.2
1.16
1.36
9.96
36.36
misc06
0.1
0.04
0.14
0.1
0.6
0.7
1.94
5.23
misc07
86.35
0.73
87.08
8.45
3.75
12.2
891.92
3119.26
mitre
0.64
20.57
21.21
0.59
5.64
6.23
14.78
27.7
mod008
1.17
0.7
1.87
0.17
10.83
11
2.41
28.63
mod010
0.42
0.15
0.57
0.27
0.73
1
4.48
13.69
mod011
125.32
82.58
207.9
56.47
>3600
>3600
1417.06
10956.5 ‡
modglob
0.24
0.04
0.28
0.16
0.19
0.35
3.15
45.94
mzzv11
253.15
45.38
298.53
179.42
42.88
222.3
3249.93
21817.4 ‡
mzzv42z
122.4
3.61
126.01
68.99
43.01
112
921.46
51016.4 ‡
nw04
744.85
23.7
768.55
29.11
106.88
135.99
1337.25
1479.38
p0033
0.01
0.01
0.02
0.01
0.15
0.16
0.12
0.16
p0201
1.02
0.16
1.18
0.24
7.61
7.85
6.1
4.95
p0282
0.74
0.12
0.86
0.16
0.97
1.13
2.41
5
p0548
0.12
0.06
0.18
0.06
0.08
0.14
0.99
3.42
p2756
0.66
0.11
0.77
0.36
0.29
0.65
4.82
10.78
pk1
187.93
7.37
195.3
89.98
>3600
>3600
269.59
206.46
pp08aCUTS 4.26
0.47
4.73
2.62
7.77
10.39
37.52
111.83
pp08a
1.6
0.46
2.06
0.92
8.28
9.2
22.2
62.06
qiu
250.26
0.24
250.5
53.06
73.46
126.52
2450.64
1311.36
qnet1
2.72
0.74
3.46
2.26
1.24
3.5
35.46
68.27
qnet1 o
1.72
0.29
2.01
1.69
0.69
2.38
26.46
65.8
rgn
1.26
0.02
1.28
0.76
0.33
1.09
8.54
1
rout
975.64
4.41
980.05
34.49
16.95
51.44
2940.5
2097.27
set1ch
0.54
0.08
0.62
0.58
0.57
1.15
9.75
51.31
stein27
0.79
0.01
0.8
0.4
0.2
0.6
7.16
2.51
stein45
20.56
3.26
23.82
17.19
9.83
27.02
177.87
65.5
vpm1
0.01
0.05
0.06
0.01
0.06
0.07
0.35
1.91
vpm2
1.66
0.54
2.2
0.9
1.6
2.5
14.8
101.48

Generating Multiple Solutions for Mixed Integer Programming Problems
291
Table 3. Number of solutions within 1% of the optimum enumerated in one hour. The
(p) symbol means the algorithm has proved that no other solutions exist within 1% of
the optimum.
Model
One-tree Heuristics Sequential Sequential
plain
max. diversity
10teams
363
93
68
5
aﬂow30a
84
61
68
20
air03
938
425
165
126
air04
8015
268
80
1
air05
6728
207
56
7
arki001
12493
20964
13
1
bell3a
≥100000 221
1
1
bell5
≥100000 7115
16
134
blend2
10 (p)
10 (p)
10 (p)
10 (p)
cap6000
79112
36648
225
23
dcmulti
≥100000 1541
640
145
disctom
37
3
25
1
dsbmip
≥100000 98385
1366
378
egout
2 (p)
2 (p)
2 (p)
2 (p)
enigma
3 (p)
2
3 (p)
3 (p)
ﬁber
136
10
279
160
ﬁxnet6
28426
97
555
185
ﬂugpl
5
4
1
1
gen
≥100000 ≥100000 717
371
gesa2
≥100000 13164
505
59
gesa2 o
≥100000 16268
308
3
gesa3
28989
1361
118
17
gesa3 o
≥100000 2051
430
5
gt2
≥100000 ≥100000 13
13
khb05250
28 (p)
28 (p)
28 (p)
28 (p)
l152lav
15958
8101
233
56
lseu
5 (p)
5 (p)
5 (p)
5 (p)
mas76
49
11
14
1
misc03
24 (p)
24 (p)
24 (p)
24 (p)
misc06
≥100000 778
781
382
misc07
72
72
42
11
mitre
10308
10091
114
11
mod008
68 (p)
25
68 (p)
68 (p)
mod010
21263
10612
350
180
mod011
49
8
18
5
modglob
≥100000 4151
661
43
mzzv11
562
48703
11
1
mzzv42z
1076
43908
30
1
nw04
86
70
28
16
p0033
15 (p)
15 (p)
15 (p)
15 (p)
p0201
44 (p)
44 (p)
44 (p)
44 (p)
p0282
≥100000 ≥100000 1042
623
p0548
≥100000 18670
921
515
p2756
10164
92586
412
227
pk1
1 (p)
1 (p)
1 (p)
1 (p)
pp08aCUTS 64 (p)
57
64 (p)
64 (p)
pp08a
64 (p)
56
64 (p)
64 (p)
qiu
144
108
14
17
qnet1
30067
14834
147
97
qnet1 o
28196
13266
138
94
rgn
720
720
720
720
rout
3393
574
11
10
set1ch
≥100000 6096
313
112
stein27
2106 (p)
2106 (p)
904
793
stein45
70 (p)
70
70 (p)
70 (p)
vpm1
≥100000 18729
1393
1344
vpm2
33 (p)
33
33 (p)
33 (p)

292
E. Danna et al.
Table 4. Diversity of solutions obtained when solving MIP(10, 1). The ‡ symbol means
that the sequential algorithm maximizing diversity had to be limited to two hours for
each iteration in order to produce 10 solutions in a reasonable time.
Model
One-tree Heuristics Sequential Sequential
plain
max. diversity
10teams
0.021
0.029
0.037
0.040 ‡
aﬂow30a
0.032
0.030
0.038
0.056
air03
0.001
0.001
0.001
0.003
air04
0.004
0.001
0.003
0.011 ‡
air05
0.003
0.002
0.001
0.011 ‡
arki001
0.038
0.022
0.059
0.434 ‡
bell3a
0.000
0.000
0.000
0.000
bell5
0.042
0.102
0.063
0.331
blend2
0.010
0.010
0.010
0.010
cap6000
0.002
0.001
0.002
0.028
dcmulti
0.039
0.044
0.025
0.253
disctom
0.109
0.137
0.423
0.476 ‡
dsbmip
0.028
0.044
0.173
0.237
egout
0.018
0.018
0.018
0.018
enigma
0.062
0.060
0.062
0.062
ﬁber
0.003
0.015
0.004
0.019
ﬁxnet6
0.006
0.017
0.027
0.046
ﬂugpl
0.000
0.000
0.000
0.000
gen
0.030
0.039
0.036
0.318
gesa2
0.034
0.012
0.008
0.151
gesa2 o
0.006
0.011
0.007
0.159
gesa3
0.006
0.000
0.009
0.086
gesa3 o
0.002
0.000
0.005
0.132 ‡
gt2
0.027
0.028
0.074
0.074
khb05250
0.138
0.119
0.138
0.200
l152lav
0.010
0.009
0.003
0.020
lseu
0.110
0.110
0.110
0.110
mas76
0.067
0.041
0.066
0.000
misc03
0.071
0.075
0.075
0.078
misc06
0.030
0.023
0.023
0.334
misc07
0.056
0.052
0.057
0.063
mitre
0.002
0.002
0.003
0.043
mod008
0.012
0.015
0.012
0.019
mod010
0.011
0.003
0.005
0.022
mod011
0.049
0.067
0.035
0.097 ‡
modglob
0.063
0.063
0.023
0.421
mzzv11
0.006
0.004
0.009
0.026 ‡
mzzv42z
0.006
0.004
0.013
0.033 ‡
nw04
0.000
0.000
0.000
0.000
p0033
0.158
0.228
0.108
0.242
p0201
0.121
0.077
0.144
0.153
p0282
0.014
0.020
0.006
0.164
p0548
0.007
0.018
0.015
0.080
p2756
0.001
0.017
0.009
0.083
pk1
0.000
0.000
0.000
0.000
pp08aCUTS 0.130
0.053
0.100
0.207
pp08a
0.069
0.103
0.100
0.207
qiu
0.333
0.239
0.346
0.370
qnet1
0.007
0.005
0.004
0.026
qnet1 o
0.005
0.006
0.003
0.026
rgn
0.062
0.055
0.066
0.071
rout
0.084
0.058
0.077
0.095
set1ch
0.034
0.034
0.020
0.145
stein27
0.363
0.353
0.393
0.444
stein45
0.383
0.346
0.415
0.429
vpm1
0.048
0.038
0.046
0.120
vpm2
0.052
0.048
0.058
0.067

Generating Multiple Solutions for Mixed Integer Programming Problems
293
models. As expected, the one-tree algorithm is generally faster than the sequen-
tial algorithm, but it is slower when there is a very small number of solutions
to be enumerated because it requires all dual tightening (in presolve and during
the tree) to be turned oﬀ. Note that for models with a very large number of so-
lutions, there are probably smarter ways to enumerate all of them, for example,
by taking into account symmetries.
5
Conclusion and Future Work
In this paper, we have formally introduced four problems representative of the
issues of generating multiple solutions for mixed integer programming problems.
We have presented three new algorithms to solve them, and we have shown
with extensive computational experiments on the MIPLIB model library that it
is within our reach to generate multiple solutions eﬀectively. In particular, the
main algorithm we introduced in this paper, the one-tree algorithm, improves
signiﬁcantly over previously known algorithms. Unlike previous approaches such
as heuristics, this algorithm is able to compute all solutions for a model and prove
that no other solutions exist. It also performs on average signiﬁcantly faster than
previously known algorithms, such as heuristics and sequential enumeration,
especially when a large number of solutions is requested.
We have also studied the diversity of the solutions produced, as this charac-
teristic is very important for applications. We have presented a variation of the
sequential algorithm that explicitly maximizes diversity. This algorithm is very
slow but useful to compare the diversity of the solutions produced by our other
algorithms. Our preliminary results are encouraging, as the one-tree algorithm
and, to a lesser extent, our MIP heuristics are signiﬁcantly faster but still pro-
vide an acceptable level of diversity. We will work in the future on improving
the diversity of the solutions produced by the one-tree algorithm. We will also
work on taking into account general integer variables and continuous variables
in diversity measures.
Acknowledgments
We would like to thank David Woodruﬀfor awakening our interest in generating
multiple and diverse solutions, and Tobias Achterberg, Kathleen Callaway, John
Gregory, and Andrea Lodi for their proofreading.
References
1. Tian Sang, Fahiem Bacchus, Paul Beame, Henry Kautz, and Toniann Pitassi. Com-
bining Component Caching and Clause Learning for Eﬀective Model Counting,
SAT 2004.
2. Egon Balas, Anuret Saxena. Optimizing over the split closure, Technical Report
2006-E5, Tepper School of Business, CMU, 2005.

294
E. Danna et al.
3. Robert E. Bixby, Mary Fenelon, Zonghao Gu, Edward Rothberg, and Roland Wun-
derling. MIP: Theory and practice — closing the gap. In M. J. D. Powell and S.
Scholtes (eds.), System Modelling and Optimization: Methods, Theory, and Appli-
cations, pages 19–49. Kluwer Academic Publishers, 2000.
4. Robert E. Bixby, S. Ceria, C. M. McZeal, M. W. P Savelsbergh. An updated mixed
integer programming library: MIPLIB 3.0. Journal Optima: 58, 12–15, 1998.
5. William Cook, Ricardo Fukasawa, and Marcos Goycoolea. Choosing the best cuts.
Workshop on mixed integer programming, MIP 2006.
6. CPLEX 10.0 Manual, Ilog Inc., 2006.
7. Emilie Danna, Edward Rothberg, Claude Le Pape. Exploring relaxation induced
neighborhoods to improve MIP solutions. Mathematical Programming: 102(1), 71–
90, 2005.
8. Sanjeeb Dash, Oktay G¨unl¨uk, and Andrea Lodi. Separating from the MIR closure
of polyhedra. Workshop on mixed integer programming, MIP 2006.
9. Matteo Fischetti, Andrea Lodi. Optimizing over the ﬁrst Chv´atal closure, in M.
J¨unger and V. Kaibel (eds.), Integer Programming and Combinatorial Optimiza-
tion - IPCO 2005, LNCS 3509, Springer-Verlag, 12–22, 2005.
10. Matteo Fischetti, Andrea Lodi. MIP models for MIP separation. Workshop on
mixed integer programming, MIP 2006.
11. Fred Glover, Arne Løkketangen, and David L. Woodruﬀ. Scatter search to gener-
ate diverse MIP solutions. In OR computing tools for modeling, optimization and
simulation: interfaces in computer science and operations research, M. Laguna and
J.L. Gonz´alez-Velarde (eds.), Kluwer Academic Publishers, 299–317, 2000.
12. Peter Greistorfer, Arne Løkketangen, Stephan Voß, and David L. Woodruﬀ. Ex-
periments concerning sequential versus simultaneous maximization of objective
function and distance. Submitted to Journal of Heuristics, 2006.
13. Karla Hoﬀman and Manfred Padberg. Improving Representations of Zero-one Lin-
ear Programs for Branch-and-Cut, ORSA Journal of Computing:3, 121–134, 1991.
14. Miroslav Karamanov and Gerard Cornuejols. Cutting Planes Selection. Workshop
on mixed integer programming, MIP 2006.
15. Sangbum Lee, Chan Phalakornkule, Michael M. Domach, and Ignacio E. Gross-
mann. Recursive MILP model for ﬁnding all the alternate optima in LP models
for metabolic networks. Computers and Chemical Engineering, 24:711–716, 2000.
16. MIPLIB 2003, http://miplib.zib.de/
17. Edward Rothberg. It’s a beautiful day in the neighborhood — Local search in
mixed integer programming. Workshop on mixed integer programming, MIP 2005.
18. Edward Rothberg. An evolutionary algorithm for polishing mixed integer program-
ming solutions. To appear in INFORMS Journal on Computing.
19. Patrick Schittekat and Kenneth Sorensen. Coping with unquantiﬁable criteria
by generating structurally diﬀerent solutions — Applications to a large real-life
location-routing problem in the automative industry. ISMP 2006.
20. L. G. Valiant. The complexity of computing the permanent. Theoretical Computer
Science, 8:189–201, 1979.
21. L. G. Valiant. The complexity of enumeration and reliability problems. SIAM Jour-
nal of Computing, 9:410–421, 1979.
22. Laurence A. Wolsey, Integer Programming, Wiley, New York, 1998.

A Branch and Bound Algorithm for Max-Cut
Based on Combining
Semideﬁnite and Polyhedral Relaxations⋆
Franz Rendl1, Giovanni Rinaldi2, and Angelika Wiegele1
1 Alpen-Adria-Universit¨at Klagenfurt, Institut f¨ur Mathematik,
Universit¨atsstr. 65-67, 9020 Klagenfurt, Austria
franz.rendl@uni-klu.ac.at, angelika.wiegele@uni-klu.ac.at
2 Istituto di Analisi dei Sistemi ed Informatica “Antonio Ruberti” – CNR,
Viale Manzoni, 30, 00185 Roma, Italy
rinaldi@iasi.cnr.it
Abstract. In this paper we present a method for ﬁnding exact solu-
tions of the Max-Cut problem max xT Lx such that x ∈{−1, 1}n. We
use a semideﬁnite relaxation combined with triangle inequalities, which
we solve with the bundle method. This approach is due to Fischer, Gru-
ber, Rendl, and Sotirov [12] and uses Lagrangian duality to get upper
bounds with reasonable computational eﬀort. The expensive part of our
bounding procedure is solving the basic semideﬁnite programming relax-
ation of the Max-Cut problem.
We review other solution approaches and compare the numerical re-
sults with our method. We also extend our experiments to unconstrained
quadratic 0-1 problems and to instances of the graph bisection problem.
The experiments show, that our method nearly always outperforms
all other approaches. Our algorithm, which is publicly accessible through
the Internet, can solve virtually any instance with about 100 variables
in a routine way.
1
Introduction
The Max-Cut problem is one of the fundamental NP-hard combinatorial opti-
mization problems. It corresponds to unconstrained quadratic optimization in
binary variables. We will present an exact method for this problem, which allows
us to solve instances of modest size (about 100 binary variables) in a routine
manner.
Since the late 1980’s a systematic investigation based on polyhedral combina-
torics was carried out to get exact solutions of the Max-Cut problem (see, e.g.,
[2, 3, 9, 11, 23, 1]). This approach is quite successful on sparse instances (e.g., in
[9] the solution of toroidal grid graphs of sizes up to 22 500 nodes is reported),
but it becomes no more usable for dense instances with more than, say, 50 nodes.
⋆Supported in part by the EU project Algorithmic Discrete Optimization (ADONET),
MRTN-CT-2003-504438.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 295–309, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

296
F. Rendl, G. Rinaldi, and A. Wiegele
A major theoretical break-through occured in the early 1990’s, when Goemans
and Williamson [16] showed that a semideﬁnite programming (SDP) relaxation
of Max-Cut has an error of no more than about 14%, independent of the density
of the underlying problem, provided the edge weights in the problem are all
nonnegative. This raised the hope that the use of this relaxation might open the
way to deal also with dense instances. Unfortunately, this SDP bound is still too
weak, see [28]. Closing an initial gap of more than 10% by Branch and Bound is
very likely to produce a huge number of subproblems to be investigated, leading
to excessive computation times.
In this paper we take up the approach from Helmberg and Rendl [18] of using
this SDP bound tightened by the inclusion of triangle inequalities in a Branch
and Bound framework. The major improvement as compared to [18] consists in
the way we compute the resulting relaxation. We use the approach of Fischer
et al. [12], which combines an interior-point method to compute the basic SDP
relaxation with the bundle method to handle the triangle inequalities, and which
we tuned for the Branch and Bound setting. A similar approach, but based on
a pure polyhedral relaxation, was used quite successfully by Frangioni, Lodi,
and Rinaldi [13] to compute the bound based on the triangle inequalities very
eﬀectively. We report computational results with this approach on a wide variety
of instances and compare with virtually all existing methods. With the exception
of very sparse graphs, our approach is a substantial improvement over all existing
methods to solve the Max-Cut problem to optimality.
The paper is organized as follows. After a quick introduction to the problem
(Sect. 2), we describe the SDP bound enhanced with triangle inequalities in
Sect. 3. In Sect. 4 we brieﬂy touch the other features of our Branch and Bound
approach. We test our approach on a variety of data sets. Some characteristics
of these data along with their origin are given in Sect. 5. In Sect. 6 we compare
our approach with existing exact methods. Finally we discuss some extensions
of our approach to the graph equipartition problem.
Notation. We use standard notation from graph theory. The vector of all ones
(of appropriate dimension) is denoted by e, A is a linear operator mapping
symmetric matrices to vectors in Rm, and AT is its adjoint operator. For a
vector v of size n we denote by Diag(v) the matrix D of order n with Dii = vi
and with all the oﬀ-diagonal elements equal to zero. For a matrix D of order n,
diag(D) denotes the n-dimensional vector v with vi = Dii. Finally, tr D denotes
the trace of the square matrix D, i.e., the sum of its diagonal elements.
2
The Max-Cut Problem
The Max-Cut problem is one of the basic NP-hard problems and has attracted
scientiﬁc interest from the combinatorial optimization community, and also from
people interested in nonlinear optimization. There are two essentially equivalent
formulations of the problem.

A Branch and Bound Algorithm for Max-Cut
297
Max-Cut in a Graph. Given an undirected graph G = (V, E) on |V | = n
vertices with edge weights we for e ∈E, every bipartition (S, T ) of V (where S
or T can be empty) deﬁnes a cut (S : T ) = {ij ∈E : i ∈S, j ∈T }. The problem
is to ﬁnd a bipartition (S, T ) such that the weight of the corresponding cut
w(S, T ) :=

e∈(S:T )
we
is maximized. It will be convenient to use matrix notation and introduce the
weighted adjacency matrix A = (aij) with aij = aji = we for edge e = [ij] ∈E
and aij = 0 if [ij] /∈E. Given A we also introduce the matrix L deﬁned by
L = Diag(Ae) −A, often called the Laplacian, associated to A.
If we represent bipartitions (S, T ) by vectors x ∈{−1, 1}n with xi = 1 exactly
if i ∈S, then it is easy to show that w(S, T ) = 1
4xT Lx. Hence ﬁnding a cut in
a graph with maximum weight is equivalent to solving the following quadratic
optimization problem.
(MC)
zMC = max{xT Lx : x ∈{−1, 1}n}.
Quadratic 0-1 Minimization.
Given a matrix Q of order n and a vector c,
let q(y) := yT Qy + cT y. We consider the following problem.
(QP)
min{q(y) : y ∈{0, 1}n}.
It is not diﬃcult to show that solving (QP) is equivalent to solving (MC)
(see for instance [3]). We consider both models, as both are dealt with in the
literature.
3
Semideﬁnite Relaxations of (MC)
The following semideﬁnite relaxation of (MC) uses xT Lx = trL(xxT ) and intro-
duces a new matrix variable X taking the role of xxT .
zSDP = max{tr LX : diag(X) = e, X ⪰0}.
(1)
Its dual form
min{eT u : Diag(u) −L ⪰0}
(2)
was introduced by Delorme and Poljak [10] as the (equivalent) eigenvalue opti-
mization problem
min{nλmax(L −Diag(u)) : u ∈Rn, uT e = 0}.
(3)
The primal version (1) can be found in [28]. In [16] it is shown that this relaxation
has an error of no more than 13.82%, i.e.,
zSDP
zMC
≤1.1382,

298
F. Rendl, G. Rinaldi, and A. Wiegele
provided there are non-negative weights on the edges (we ≥0). This relaxation
can be further tightened by including the following triangle inequalities (that
deﬁne the semimetric polytope, the basic polyhedral relaxation of Max-Cut).
⎛
⎜
⎜
⎝
−1 −1 −1
−1
1
1
1 −1
1
1
1 −1
⎞
⎟
⎟
⎠
⎛
⎝
xij
xik
xjk
⎞
⎠≤
⎛
⎜
⎜
⎝
1
1
1
1
⎞
⎟
⎟
⎠
1 ≤i < j < k ≤n.
We abbreviate all 4
	n
3

of these constraints as A(X) ≤e. Hence we get
zSDP MET = max{tr LX : diag(X) = e, A(X) ≤e, X ⪰0}.
(4)
Helmberg and Rendl [18] apply this semideﬁnite relaxation (solved by an
interior point code) in a Branch and Bound scheme. Later on, Helmberg [17] im-
proved this algorithm by ﬁxing variables. The experiments in [18] clearly indicate
that an eﬃcient computation of this relaxation is crucial for further computa-
tional improvements.
Instead of solving this relaxation with a limited number of inequality con-
straints by interior point methods, as done in [18], we use the bundle approach,
suggested in [12], which we modify to gain computational eﬃciency in the Branch
and Bound process.
The set E := {X : diag(X) = e, X ⪰0} deﬁnes the feasible region of (1).
Therefore (4) can compactly be written as
zSDP MET = max{⟨L, X⟩: X ∈E, A(X) ≤e}.
(5)
We now brieﬂy recall the approach from [12] to approximate zSDP MET (from
above). Let us introduce the Lagrangian with respect to A(X) ≤e
L(X, γ) := ⟨L, X⟩+ γT (e −A(X))
(6)
and the associated dual function
f(γ) := max
X∈E L(X, γ) = eT γ + max
X∈E ⟨L −AT (γ), X⟩.
(7)
We get for any ˆγ ≥0 that
zSDP MET = max
X∈E min
γ≥0 L(X, γ) = min
γ≥0 f(γ) ≤f(ˆγ).
The problem now consists in ﬁnding a ‘good’ approximation ˆγ to the correct
minimizer of f.
The function f is well-known to be convex but non-smooth. Evaluating f for
some γ ≥0 amounts to solving a problem of type (1), which can be done easily
for problem sizes of our interest. We use a primal-dual interior-point method to
solve it, which also provides an optimality certiﬁcate Xγ, uγ (optimal solutions
to (1) and (2)). The primal matrix Xγ will turn out to be useful in our algorithmic
setup. We have, in particular that
f(γ) = L(Xγ, γ).

A Branch and Bound Algorithm for Max-Cut
299
Moreover, a subgradient of f at γ is given by e −A(Xγ).
Dualizing all triangle constraints would result in a dual problem of dimen-
sion roughly 2
3n3. We prefer a more economical approach where inequalities are
included only if they are likely to be active at the optimum.
Let I be a subset of the triangle inequalities, hence AI(X) ≤eI. We also write
γI for the variables dual to the inequalities in I. Setting the dual variables not
in I to zero, it is clear that for any I and any γI ≥0, we get an upper bound on
zSDP MET . Approximating the value zSDP MET therefore breaks down into the
following two independent tasks:
1. Identify a subset I of triangle inequalities.
2. For a given set I of inequalities, determine an approximate minimizer γI ≥0
of f.
The second step can be carried out with any of the subgradient methods for
convex nonsmooth functions. For computational eﬃciency we use the bundle
method with a limit on the number of function evaluations.
Carrying out the ﬁrst step is less obvious. We are interested in constraints
which are active at the optimum, but this information is in general not avail-
able. Therefore we use the optimizer XγI, corresponding to an approximate
minimizer γI of f, and add to the current set I of constraints the t triangle
inequalities most violated by XγI. (Here t is a parameter which is dynamically
chosen.) Thus we can identify promising new inequalities to be added to I.
On the other hand, we remove any constraint from I where the dual multi-
plier is close to zero, as this is an indication that the constraint is unlikely to
be binding. We iterate this process of selecting and updating a set of triangle
inequalities, and then solving the respective relaxation, as long as the decrease
of the upper bound is suﬃciently large.
4
Branching Rules and Heuristics
4.1
Branching Strategies
We subdivide the set of feasible solutions by simply separating, or merging two
vertices i, j. This results again in an instance of (MC), see [27]. There are several
natural choices for such a pair i, j for branching.
Easy First. A ﬁrst idea is to branch on pairs i, j where the decision seems to
be obvious. We choose i and j such that their rows are ‘closest’ to a {−1, 1}
vector, i.e., they minimize n
k=1(1 −|xik|)2. We may assume, that for these
two very well articulated nodes the value |xij| is also very large. Setting xij
opposite to its current sign should lead to a sharp drop of the optimal solution
in the corresponding subtree. Hoping that the bound also drops as fast, we will,
presumably, be able to cut oﬀthis subtree quickly. This rule has been used also
in [18] and called R2.

300
F. Rendl, G. Rinaldi, and A. Wiegele
Diﬃcult First. Another possibility for branching is to ﬁx the hard decisions
ﬁrst. We branch on the pair i, j which minimizes |xij|. This means, we ﬁx the
most diﬃcult decisions and hope that the quality of the bound gets better fast
and that the subproblems become easier. Following [18] we call this rule R3.
Depending on the class of problems, either rule R2 or R3 was more eﬃcient
than the other. We also experimented with the so-called strong branching, as
this strategy is quite successful for linear programming based relaxations. Un-
fortunately, sensitivity information, necessary for selecting the branching pair,
is much harder to get in the case of semideﬁnite relaxations, hence there is
no computational trade oﬀ. Consequently, we did not pursue this strategy any
further.
4.2
Generating Feasible Solutions
Generating feasible solutions is done iteratively in basically three steps:
1. Apply the Goemans-Williamson hyperplane rounding technique [16] to the
primal matrix X obtained from solving the SDP during the bundle iterations.
This gives a cut vector ¯x.
2. Cut ¯x is locally improved by checking all possible moves of a single vertex
to the opposite partition block. This gives a cut ˜x.
3. Bring the matrix X towards a good cut by using a convex-combination of
X and ˜x˜xT . With this new matrix go to 1. and repeat as long as one ﬁnds
better cuts.
It turned out, that with this heuristic for most of the instances the optimal cut
was found at the root node of the Branch and Bound tree.
5
Random Data for (MC) and (QP)
In this section some random data for presenting numerical results of our algo-
rithm are speciﬁed. All the data sets can be downloaded from http://www.math.
uni-klu.ac.at/or/Software. These instances are taken from various sources.
Here we provide some of the characteristics of the data sets.
5.1
Max-Cut Instances
Instances by the Graph Generator ‘rudy’. The ﬁrst group of instances
follows [18] and consists of random graphs (of speciﬁed edge density) with various
types of random edge weights. All graphs were produced by the graph generator
‘rudy’ [30]. For a detailed description and a list of the rudy-calls the reader is
referred to the dissertation of Wiegele [31]. We generated ten instances of size
n = 100 and given density d of the following types of graphs:
– G0.5: unweighted graphs with density d = 0.5.
– G−1/0/1: complete graphs with edge weights chosen uniformly from {−1, 0, 1}.

A Branch and Bound Algorithm for Max-Cut
301
– G[−10,10]: Graphs with integer edge weights chosen from [−10, 10] and d ∈
{0.5, 0.9}.
– G[0,10]:
Graphs with integer edge weights chosen from [0, 10] and d ∈
{0.5, 0.9}.
Applications in Statistical Physics: Ising Instances. We also consider a
set of test-problems of Frauke Liers [personal communication, 2005] coming from
physical applications. The ﬁrst group consists of two- and three-dimensional
grid graphs with Gaussian distributed weights (zero mean and variance one).
The second group consists of dense Ising instances which are obtained in the
following way: all nodes lie evenly distributed on a cycle. The weights of the
edges depend on the Euclidean distance between two nodes and a parameter σ,
such that the proportion cij ∼ϵij
rσ
ij holds (ϵij is chosen according to a Gaussian
distribution with zero mean and variance one and rij is the Euclidean distance
between nodes i and j).
5.2
(QP) Instances
Pardalos and Rodgers [25] have proposed a test problem generator for uncon-
strained quadratic binary programming. Their routine generates a symmetric
integer matrix Q to deﬁne the objective function for (QP), with the linear term c
represented by the main diagonal of Q, and has several parameters to control the
characteristics of the problem. These parameters are the number n of variables,
the density d, i.e., the probability that a nonzero will occur in the oﬀ-diagonal
part of Q, the lower and upper bounds of the main diagonal of Q are given
by c−, c+. The lower and upper bounds for the oﬀ-diagonal part of Q are given
by q−, q+. Furthermore we have qii ∼discrete uniform in (c−, c+) and qij = qji ∼
discrete uniform in (q−, q+).
Several test problems generated this way are provided in the OR-library [4],
[5]. We have chosen all the problems of sizes of our interest, which are the data
sets bqpgka, due to [14] and bqp100 and bqp250, see [6]. Furthermore, in [7] the
sets c and e of bqpgka are extended. We call these instances bqpbe.
The characteristics are as follows:
– bqpgka:
n
d
c−
c+
q−
q+
bqpgka, set a 30, . . . , 100 0.0625, . . ., 0.5 −100 100 −100 100
bqpgka, set b 20, . . . , 120
1.0
0
63 −100 0
bqpgka, set c 40, . . . , 100
0.1, . . . , 0.8
−100 100 −50 50
bqpgka, set d
100
0.1, . . . , 1.0
−75 75 −50 50
bqpgka, set e
200
0.1, . . . , 0.5
−100 100 −50 50
– bqpbe
Size ranging from n = 100 to n = 250 nodes; density ranging from d = 0.1
to d = 1.0; c−= −100; c+ = 100; q−= −50 and q+ = 50.
– beasley
Two sizes of n = 100 and n = 250 nodes; d = 0.1; c−= −100; c+ = 100;
q−= −100 and q+ = 100.

302
F. Rendl, G. Rinaldi, and A. Wiegele
6
Numerical Results
The algorithm was implemented in C and made publicly available for experimen-
tal runs as “Biq Mac” – a solver for binary quadratic and Max-Cut problems,
see [29]. If not stated otherwise, test runs were performed on a Pentium IV, 3.6
GHz and 2 GB RAM, operating system Linux. For a more detailed study of the
numerical results the reader is referred to the dissertation [31].
6.1
Summarizing Existing Methods and Their Limits
Before we present our computational results, we summarize existing exact meth-
ods for (MC) together with their limits, as reported in the publications under-
lying these approaches.
LP:
Linear programming based Branch and Bound approaches go back to
Barahona et al. [3]. Liers et al. [23] enhance the algorithm and focus on solving
toroidal grid graphs arising from physical applications, the so-called Ising model.
V:
Linear programming combined with volume algorithm has been investi-
gated by Barahona and Lad´anyi [1]. Also in this work, there is an emphasis on
toroidal grid graphs.
EO:
An exact approach using eigenvalue optimization based on (3) has been
ﬁrst investigated by Poljak and Rendl [27].
QP:
The recent work of Billionnet and Elloumi [7] presents an approach
based on convex quadratic optimization. This algorithm convexiﬁes the objective
function and uses a mixed-integer quadratic programming solver to obtain an
exact solution of the problem.
SDPMET:
An approach based on SDP and the triangle inequalities was
ﬁrst investigated by Helmberg and Rendl [18]. They solve (4) by an interior point
algorithm.
PP:
Pardalos and Rodgers [25], [26] solve the quadratic program by Branch
and Bound using a preprocessing phase where they try to ﬁx some of the vari-
ables. The test on ﬁxing the variables exploits information of the partial deriva-
tives of the cost function.
SOCP:
Kim and Kojima [21] and, later on, Muramatsu and Suzuki [24] use
a second-order cone programming (SOCP) relaxation as bounding routine in a
Branch and Bound framework to solve Max-Cut problems. However, the basic
SDP relaxation performs better than their SOCP relaxation and the algorithm
is capable of solving very sparse instances only. Therefore we omit comparing
with this algorithm in the subsequent sections.
In Table 1 we give a very na¨ıve overview of the capability of these approaches.
We consider diﬀerent types of instances and use the following symbols. A ✔
means, that the approach can solve instances of this type in a routine way. A K
indicates that one can have (at least) one cup of coﬀee while waiting for the
solution and maybe there are instances that cannot be solved at all. The ®
suggests to have some holidays and come back in a couple of days to see whether
the job is ﬁnished and the  indicates that the chances for solving the problem

A Branch and Bound Algorithm for Max-Cut
303
Table 1. Who can do what?
LP V EO QP SDPMET PP Biq Mac
quadr 0-1, n = 100, d = .1
✔✔
✔
K
✔
✔
quadr 0-1, n = 250, d = .1
?
?




K
2-dim. torus, n = 20 × 20
✔✔


?
®
3-dim. torus, n = 7 × 7 × 7 ✔✔


?
K
G0.5, n = 100
 ?

®
®
?
✔
G−1/0/1, n = 100
 ? ®
®
®
?
✔
Table 2. Average Biq Mac results for Max-Cut problems. Run times on a Pentium
IV, 3.6 GHz, 2GB RAM.
min avg max min avg max
graph
n
d
solved time (h:min)
nodes
G0.5
100 0.5
10
5
50 3:44
65 610 2925
G−1/0/1 100 0.99
10
7
56 2:31
79 651 1811
G[−10,10] 100 0.5
10
9
38 1:13
97 435
815
G[−10,10] 100 0.9
10
5
57 3:12
51 679 2427
G[1,10]
100 0.5
10
7
48 2:02 111 576 1465
G[1,10]
100 0.9
10
12
40 1:26 155 464 1007
with this method are very low. If we do not know, whether an algorithm can
solve certain classes of instances or not, we indicate this with a question mark.
Most likely, we could place  instead of a question mark.
6.2
Numerical Results of Max-Cut Instances
Instances by the Graph Generator ‘rudy’. Table 2 lists the computation
times (minimum, average and maximum) and the number of nodes (minimum,
average, maximum) of the resulting Branch and Bound (B&B) tree. The branch-
ing rule used for this kind of instances is R2.
The average computation time for all instances is approximately one hour.
Nevertheless, instances may also be solved within some minutes, and it could
also take more than three hours for some graphs to obtain a solution.
The results show that on these classes of instances we outperform all other so-
lution approaches known so far. The currently strongest results on these graphs
are due to Billionnet and Elloumi [7]. They are not able to solve instances G−1/0/1
of size n = 100 at all. Also, they could solve only two out of ten instances of G0.5,
n = 100.
Applications in Statistical Physics: Ising Instances. As explained in
Sect. 5.1, we consider two kinds of Ising instances: toroidal grid graphs and
complete graphs.

304
F. Rendl, G. Rinaldi, and A. Wiegele
Table 3. Test runs on torus graphs with Gaussian distribution. Branch and Cut al-
gorithm run on 1.8 GHz machine, Biq Mac done on a Pentium IV, 3.6 GHz. Time in
seconds.
Problem
[23] Biq Mac Problem
[23]
Biq Mac
number
n time
time
number
n
time
time
2 dimensional
3 dimensional
g10 5555 100 0.15
10.12 g5 5555 125
2.68
18.01
g10 6666 100 0.14
15.94 g5 6666 125
3.29
24.52
g10 7777 100 0.18
14.89 g5 7777 125
3.07
26.00
g15 5555 225 0.44
304.03 g6 5555 216
20.56
280.85
g15 6666 225 0.78
359.87 g6 6666 216
37.74
2025.74
g15 7777 225 0.67
346.89 g6 7777 216
27.30
277.95
g20 5555 400 1.70
6690.99 g7 5555 343
95.25
432.71
g20 6666 400 3.50 35205.95 g7 6666 343 131.34
550.12
g20 7777 400 2.61
8092.80 g7 7777 343 460.01 117782.75
Instances of the ﬁrst kind can be solved eﬃciently by an LP-based Branch
and Cut algorithm (see [23]). The computation times of [23] and our algorithm
are reported in Table 3. As can be seen, on these sparse instances the LP-based
method clearly outperforms our algorithm. However, we ﬁnd a solution within
a gap of 1% in reasonable time for all these samples.
The run time of the Branch-Cut & Price algorithm [22] developed for the sec-
ond kind of problems depends strongly on the parameter σ. For σ close to zero,
we have a complete graph with Gaussian distributed weights. But for σ chosen
suitably large, some of the edges become ‘unimportant’ and the pricing works
very well for these graphs. In Table 4 the computation times of [22] and our
algorithm are given. For σ = 3.0, we have roughly speaking the same computa-
tion times on the smallest instances. For the biggest ones, our approach clearly
dominates. For σ = 2.5, the Branch-Cut & Price algorithm already takes more
than 20 hours for instances of size n = 150, whereas our algorithm needs almost
similar computation times as in the σ = 3.0 case.
For both kinds of instances we used branching rule R3.
6.3
Numerical Results of (QP) Instances
In this section we report the results for the instances derived from (QP). Best
known lower and upper bounds for bqpgka and beasley data are reported at
the pseudo-Boolean website [8]. Our results are as follows:
– bqpgka.
• Set a. All problems are solved in the root node of the B&B tree within
seconds.
• Set b. These instances could all be solved, but were extremely chal-
lenging for our algorithm. The reason is, that the objective value in the
Max-Cut formulation is of magnitude 106, and therefore even a relative

A Branch and Bound Algorithm for Max-Cut
305
Table 4. Test runs on Ising instances (complete graphs). Branch-Cut & Price on a
1.8 GHz machine, Biq Mac on a 3.6 GHz PC. Times in hours:minutes:seconds.
Problem
[22]
Biq Mac Problem
[22]
Biq Mac
number
n
time
time
number
n
time
time
σ = 3.0
σ = 2.5
100 5555 100
4:52
1:36 100 5555 100
18:22
1:32
100 6666 100
0:24
0:34 100 6666 100
6:27
1:06
100 7777 100
7:31
0:48 100 7777 100
10:08
0:47
150 5555 150
2:36:46
4:38 150 5555 150 21:28:39
4:25
150 6666 150
4:49:05
3:55 150 6666 150 23:35:11
5:39
150 7777 150
3:48:41
6:06 150 7777 150 31:40:07
9:19
200 5555 200
9:22:03
10:07 200 5555 200
–
10:05
200 6666 200 32:48:03
18:53 200 6666 200
–
17:55
200 7777 200
8:53:26
22:42 200 7777 200
–
21:38
250 5555 250 21:17:07
1:46:29 250 5555 250
–
3:00:28
250 6666 250
7:42:25
15:49 250 6666 250
–
1:17:04
250 7777 250 17:30:13
57:24 250 7777 250
–
1:10:50
300 5555 300 17:20:54
2:20:14 300 5555 300
–
6:43:47
300 6666 300 10:21:40
1:32:22 300 6666 300
–
9:04:38
300 7777 300 18:33:49
3:12:13 300 7777 300
– 13:00:10
gap of 0.1% does not allow to fathom the node. However, by allowing
a relative error of at most 0.1%, we can solve all problems in the root
node of the B&B tree.
• Set c. Similar to set a, also these instances were solved within a few
seconds in the root node of the B&B tree.
• Set d. Here n = 100. The problems of set d could be solved within at
most 7 minutes.
• Set e. We recall n = 200. The instances with densities 0.1, 0.2, 0.3
and 0.4 could all be solved within 2 hours of computation time. The
instance with d = 0.5 has been solved after 35 hours. According to [8],
none of these problems were solved before.
– bqpbe.
We report the results of Billionnet and Elloumi [7] and our results in Table 5.
As is shown in this table, [7] could not solve all out of the ten problems from
the n = 120 variables and density 0.8 instances on, whereas our method
still succeeded to solve them all. From the instances n = 150, d = 0.8 on,
the convex-quadratic approach failed to solve any instance within their time
limit of 3 hours. We still managed to obtain solutions to all of these instances
(although for one graph it took about 54 hours to prove the optimality of
the solution).
– beasley.
Solving the 10 problems of size n = 100 can be done in the root node
within one minute. Regarding the n = 250 instances, only two out of the ten
problems have been solved before (see [8]), for the other eight problems we

306
F. Rendl, G. Rinaldi, and A. Wiegele
Table 5. Comparison between [7] and Biq Mac. Computation times of the convex-
quadratic algorithm were obtained on a laptop Pentium IV, 1.6 GHz (time limit 3
hours), our results were computed on a Pentium IV of 3.6 GHz.
[7]
Biq Mac
CPU time (sec)
CPU time (sec)
n
d
solved min avg.
max solved
min
avg.
max
100 1.0
10
27
372
1671
10
86
178
436
120 0.3
10
168 1263
4667
10
29
162
424
120 0.8
6
322 3909
9898
10
239
1320
3642
150 0.3
1
6789
10
1425
2263
2761
150 0.8
0
–
10
1654
1848
2133
200 0.3
0
–
10
7627 37265 193530
200 0.8
0
–
10
5541 47740 148515
250 0.1
0
–
10
12211 13295
16663
could prove optimality for the ﬁrst time. Six out of these eight were solved
within 5 hours, the other two needed 15 and 80 hours, respectively.
Deciding which branching rule is advisable for these instances is not so ob-
vious anymore. Tentatively, for sparse problems R3 is superior, but the denser
the instances are, the better is the performance of R2. A general recipe or an
intelligent way of deciding at the top levels of the B&B tree which rule to follow
would be very useful.
7
Equipartition
Finding a bisection of a graph such that each of the sets S and T have equal
cardinality is often called equipartition. It is also customary to minimize the
weight of edges in the cut. Hence the problem is a minor extension of (MC).
zEP = min{xT Lx : eT x = 0, x ∈{−1, 1}n}
(8)
This leads to the following semideﬁnite relaxation.
zEP −SDP = min{trLX : trJX = 0, diag(X) = e, X ⪰0},
(9)
where J = eeT . Let A be the adjacency matrix of the given graph. We consider
the Max-Cut instance with cost matrix B = −A + J. The “−” in B = −A + J
arises, because we minimize instead of maximizing, and the J comes from the
constraint trJX = 0, that comes with a Lagrange multiplier (set equal to 1 for
unweighted instances) into the objective function.
We consider the instances introduced in [19] of size n = 124 and n = 250
and summarize in Table 6 the best results for these instances known so far
(see [20]). With our algorithm we prove optimality of the known lower bounds
of all instances of size n = 124, and one of the instances of size n = 250. To the

A Branch and Bound Algorithm for Max-Cut
307
Table 6. Best known results of the bisection problem for the Johnson graphs and the
new gap obtained by Biq Mac
best known
best known
d
bound |Ecut| gap new gap
d
bound |Ecut| gap new gap
n = 124
n = 250
0.02
12.01
13
0
0 0.01
26.06
29
2
0
0.04
61.22
63
1
0 0.02 103.61
114
10
8
0.08 170.93
178
7
0 0.04 327.88
357
29
22
0.16 440.08
449
8
0 0.08 779.55
828
48
35
best of our knowledge, these exact solutions were obtained for the ﬁrst time. The
improved gap for the instances of size n = 250 and densities 0.02, 0.04 and 0.08
were obtained after a time limit of 32 hours cpu-time.
8
Summary
In this paper we have presented an algorithm, that uses a Branch and Bound
framework to solve the Max-Cut and related problems. At each node of the tree
we calculate the bound by using a dynamic version of the bundle method that
solves the basic semideﬁnite relaxation for Max-Cut strengthened by triangle
inequalities. We conclude, that
– our approach solves any instance of all the test-bed considered with n ≈100
nodes in a routine way. To the best of our knowledge, no other algorithm
can manage these instances in a similar way.
– we solve problems of special structure and sparse problems up to n = 300
nodes.
– for the ﬁrst time optimality could be proved for several problems of the OR-
library. All problems that are reported at the Pseudo-Boolean website [8]
with dimensions up to n = 250 are now solved.
– for the ﬁrst time optimality of the bisection problem for some of the Johnson
graphs has been proved, for those where we could not close the gap we
reduced the best known gap signiﬁcantly.
– for sparse problems it is not advisable to use our approach. Since linear
programming based methods are capable of exploiting sparsity, solutions
might be obtained much faster when applying these methods to sparse data.
Using our algorithm to solve this problem has been made publicly available
[29].
References
[1] F. Barahona and L. Lad´anyi.
Branch and cut based on the volume al-
gorithm: Steiner trees in graphs and max-cut. RAIRO Oper. Res., 40(1):
53–73, 2006.

308
F. Rendl, G. Rinaldi, and A. Wiegele
[2] F. Barahona, M. Gr¨otschel, M. J¨unger, and G. Reinelt. An application of
combinatorial optimization to statistical physics and circuit layout design.
Operations Research, 36:493–513, 1988.
[3] F. Barahona, M. J¨unger, and G. Reinelt.
Experiments in quadratic 0-1
programming. Math. Programming, 44(2, (Ser. A)):127–137, 1989.
[4] J. E. Beasley. Or-library: distributing test problems by electronic mail. J.
Oper. Res. Soc., 41(11):1069–1072, 1990.
[5] J. E. Beasley. Or-library, 1990. http://people.brunel.ac.uk/∼mastjjb/
jeb/info.html.
[6] J. E. Beasley. Heuristic algorithms for the unconstrained binary quadratic
programming problem. Technical report, The Management School, Imperial
College, London SW7 2AZ, England, 1998.
[7] A. Billionnet and S. Elloumi. Using a mixed integer quadratic programming
solver for the unconstrained quadratic 0-1 problem. Math. Programming,
109(1, Ser. A):55–68, 2007.
[8] E. Boros, P. L. Hammer, and G. Tavares. The pseudo-boolean optimization
website, 2005. http://rutcor.rutgers.edu/∼pbo/.
[9] C. De Simone, M. Diehl, M. J¨unger, P. Mutzel, G. Reinelt, and G. Rinaldi.
Exact ground states of Ising spin glasses: New experimental results with a
branch-and-cut algorithm. J. Statist. Phys., 80(1-2):487–496, 1995.
[10] C. Delorme and S. Poljak. Laplacian eigenvalues and the maximum cut
problem. Math. Programming, 62(3, Ser. A):557–574, 1993.
[11] M. Elf, M. J¨unger, and G. Rinaldi. Minimizing breaks by maximizing cuts.
Operations Research Letters, 31:343–349, 2003.
[12] I. Fischer, G. Gruber, F. Rendl, and R. Sotirov. Computational experience
with a bundle approach for semideﬁnite cutting plane relaxations of Max-
Cut and equipartition. Math. Programming, 105(2-3, Ser. B):451–469, 2006.
[13] A. Frangioni, A. Lodi, and G. Rinaldi. New approaches for optimizing over
the semimetric polytope. Math. Program., 104(2-3, Ser. B):375–388, 2005.
[14] F. Glover, G. Kochenberger, and B. Alidaee.
Adaptative memory tabu
search for binary quadratic programs.
Management Sci., 44(3):336–345,
1998.
[15] M. X. Goemans and D. P. Williamson. .878-approximation algorithms for
max cut and max 2sat. In Proceedings of the Twenty-Sixth Annual ACM
Symposium on the Theory of Computing, pages 422–431, Montreal, Quebec,
Canada, 1994.
[16] M. X. Goemans and D. P. Williamson. Improved approximation algorithms
for maximum cut and satisﬁability problems using semideﬁnite program-
ming. J. Assoc. Comput. Mach., 42(6):1115–1145, 1995. preliminary version
see [15].
[17] C. Helmberg. Fixing variables in semideﬁnite relaxations. SIAM J. Matrix
Anal. Appl., 21(3):952–969 (electronic), 2000.
[18] C. Helmberg and F. Rendl. Solving quadratic (0, 1)-problems by semideﬁ-
nite programs and cutting planes. Math. Programming, 82(3, Ser. A):291–
315, 1998.

A Branch and Bound Algorithm for Max-Cut
309
[19] D. S. Johnson, C. R. Aragon, L. A. McGeoch, and C. Schevon. Optimization
by simulated annealing: an experimental evaluation. part i, graph partition-
ing. Oper. Res., 37(6):865–892, 1989.
[20] S. E. Karisch and F. Rendl. Semideﬁnite programming and graph equipar-
tition. In Topics in semideﬁnite and interior-point methods (Toronto, ON,
1996), volume 18 of Fields Inst. Commun., pages 77–95. Amer. Math. Soc.,
Providence, RI, 1998.
[21] S. Kim and M. Kojima.
Second order cone programming relaxation of
nonconvex quadratic optimization problems.
Optim. Methods Softw., 15
(3-4):201–224, 2001.
[22] F. Liers. Contributions to Determining Exact Ground-States of Ising Spin-
Glasses and to their Physics. PhD thesis, Universit¨at zu K¨oln, 2004.
[23] F. Liers, M. J¨unger, G. Reinelt, and G. Rinaldi. Computing exact ground
states of hard ising spin glass problems by branch-and-cut. In A. Hartmann
and H. Rieger, editors, New Optimization Algorithms in Physics, pages
47–68. Wiley, 2004.
[24] M. Muramatsu and T. Suzuki.
A new second-order cone programming
relaxation for MAX-CUT problems.
J. Oper. Res. Soc. Japan, 46(2):
164–177, 2003.
[25] P. M. Pardalos and G. P. Rodgers. Computational aspects of a branch and
bound algorithm for quadratic zero-one programming. Computing, 45(2):
131–144, 1990.
[26] P. M. Pardalos and G. P. Rodgers. Parallel branch and bound algorithms
for quadratic zero-one programs on the hypercube architecture. Ann. Oper.
Res., 22(1-4):271–292, 1990.
[27] S. Poljak and F. Rendl. Solving the max-cut problem using eigenvalues.
Discrete Appl. Math., 62(1-3):249–278, 1995.
[28] S. Poljak and F. Rendl. Nonpolyhedral relaxations of graph-bisection prob-
lems. SIAM J. Optim., 5(3):467–487, 1995.
[29] F. Rendl, G. Rinaldi, and A. Wiegele.
Biq Mac – a solver for binary
quadratic and max-cut problems, 2006. http://BiqMac.uni-klu.ac.at/.
[30] G. Rinaldi. Rudy, 1998. http://www-user.tu-chemnitz.de/∼helmberg/
rudy.tar.gz.
[31] A. Wiegele. Nonlinear optimization techniques applied to combinatorial op-
timization problems. PhD thesis, Alpen-Adria-Universit¨at Klagenfurt, 2006.

DINS, a MIP Improvement Heuristic
Shubhashis Ghosh⋆
Department of Computing Science, University of Alberta, Canada
sghosh@ualberta.ca
Abstract. We introduce Distance Induced Neighbourhood Search
(DINS), a MIP improvement heuristic that tries to ﬁnd improved MIP fea-
sible solutions from a given MIP feasible solution. DINS is based on a vari-
ation of local search that is embedded in an exact MIP solver, namely a
branch-and-bound or a branch-and-cut MIP solver. The key idea is to use
a distance metric between the linear programming relaxation optimal solu-
tion and the current MIP feasible solution to deﬁne search neighbourhoods
at diﬀerent nodes of the search tree generated by the exact solver. DINS
considers each deﬁned search neighbourhood as a new MIP problem and
explores it by an exact MIP solver with a certain node limit. On a set of
standard benchmark problems, DINS outperforms the MIP improvement
heuristics Local Branching due to Fischetti and Lodi and Relaxation In-
duced Neighbourhood Search due to Danna, Rothberg, and Pape, as well
as the generic commercial MIP solver Cplex.
1
Introduction
Mixed integer programs (MIPs) arise in many contexts; they are often intractable
and NP-hard, even for feasibility [14]. Therefore, there is interest in designing
eﬀective heuristic methods for MIPs. Recently MIP heuristic development has
specialized into ﬁnding better feasibility heuristic (that tries to ﬁnd an initial
MIP feasible solution), and improvement heuristic (that tries to ﬁnd improved
MIP feasible solutions from a given MIP feasible solution). In this paper, we
present a new improvement heuristic.
Recent improvement heuristics such as Local Branching (LB), introduced
by Fischetti et al. [9] and re-engineered by Danna et al. [5], and Relaxation
Induced Neighbourhood Search (RINS), introduced by Danna et al. [5],
work in tandem with a state-of-the-art exact solver such as Cplex MIP solver as
follows. The exact solver generates a search tree using either branch-and-bound
or branch-and-cut approach; the new heuristics periodically select nodes of the
search tree at which to perform a localized search. Our heuristic also follows this
approach. The heuristics diﬀer primarily in the deﬁnition of the search neigh-
bourhood; in LB the search neighbourhood is deﬁned by restricting the number
of 0-1 variables to switch their bounds from the known MIP feasible solution
(referred as soft ﬁxing), and in RINS it is deﬁned by ﬁxing some variables at
their current values in the known MIP feasible solution (referred as hard ﬁxing).
⋆The research support of NSERC is gratefully acknowledged.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 310–323, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

DINS, a MIP Improvement Heuristic
311
Our search neighbourhood is deﬁned in terms of a distance metric between a
relaxation solution and the current MIP feasible solution, where the distance
metric comes from the intuition that improved solutions are more likely to be
close to the relaxation solution at the nodes of the search tree.
On a set of standard benchmark MIP instances, DINS outperforms Cplex,
RINS, and LB with respect to the quality of solutions obtained within a time
limit.
2
Related Previous Work
In order to show the strength of our heuristic, we compare it against Cplex,
the exact solver in which it is embedded, and LB and RINS, the two recent
improvement heuristics that are most similar in design.
Much research has been done in other kinds of MIP heuristics. There are
several heuristics, introduced by Balas et al. [1], Faaland et al. [7], Hillier [12],
and Ibaraki et al. [13], that incorporate some form of neighbourhood search, and
most of them do so from the relaxation solution of MIP in order to ﬁnd a MIP
feasible solution.
There are also several pivot based heuristics, introduced by Balas et al. [2,3],
Løkketangen et al. [15], Nediak et al. [18], and Løkketangen et al. [16], for MIP
that try to obtain a MIP solution by performing pivots on the simplex tableau of
the relaxation of MIP. Another heuristic introduced by Balas et al. [4], starting
from the relaxation solution of MIP, tries to ﬁnd a MIP solution by ﬁrst using
some pivoting on the simplex tableau and then doing some form of neighbour-
hood search. Recently Fischetti et al. [8] introduce another heuristic to ﬁnd a
MIP solution from the relaxation solution of MIP, where they solve a sequence
of linear programs in the process of ﬁnding a MIP feasible solution.
3
Methods
We assume that the input program P is a generic MIP of the form shown below,
where c, x, b, A have dimensions n, n, m, m×n respectively, N = {1, . . . , n} is the
set of variable indices of P which is partitioned into (B, G, C) with B, G, and C
denoting the indices of 0-1, general integer, and continuous variables respectively.
An integer variable is any variable in B ∪G.
P : min { cT x | Ax ≥b, xi ∈{0, 1} ∀i ∈B,
xj ≥0 and integer ∀j ∈G, xj ≥0 ∀j ∈C}
Since we compare DINS with LB and RINS, we describe LB and RINS in
some details.
3.1
Local Branching
LB deﬁnes the neighbourhood of a feasible solution x∗by limiting at some integer
p the number of 0-1 variables currently at 0 or 1 that can switch their bounds.
This is achieved by adding to the instance the LB inequality D(x, x∗) ≤p, where

312
S. Ghosh
D(x, x∗) :=

j∈V0
xj +

j∈V1
(1 −xj),
and where V0 and V1 are the index sets of the 0-1 variables that are at 0 and 1
respectively in x∗.
LB has been implemented in two diﬀerent ways. Originally, Fischetti and
Lodi [9] treated it as an external branching framework (i.e., creates branches
in the search tree by D(x, x∗) ≤p and D(x, x∗) ≥p + 1 as opposed to the
standard branching which are done on the variables in the branch-and-bound
framework) in addition to an heuristic and obtained the diversiﬁcation (i.e.,
switching the search in a diﬀerent region of the MIP feasible space) by deﬁning
the neighbourhoods with a change in the value of the parameter p. Later, Danna
et al. [5] implemented LB solely as a heuristic and obtained the diversiﬁcation
by deﬁning the neighbourhoods on the new solutions found during the MIP
search tree exploration. Danna et al. showed that their implementation of LB
outperformed the original. For this reason, we choose the Danna et al. version
of LB to compare against our DINS.
3.2
Relaxation Induced Neighbourhood Search
During the exploration of the MIP search tree, the relaxation solution at suc-
cessive nodes (that are not pruned by infeasibility or bound) provides a better
objective value than the objective value of the current MIP solution. Using this,
Danna et al. introduce RINS making the intuition that, in improved MIP solu-
tions, it is more likely for the variables to stay at the same values those agree
in the current MIP solution and current node relaxation solution. Thus RINS
deﬁnes the promising neighbourhood ﬁxing all variables whose values at the
current MIP solution are equal to their respective values at the current node
relaxation solution.
In the implementation of RINS1, the procedure for exploring the RINS de-
ﬁned neighbourhood is invoked at a particular node of the MIP search tree. At
the termination of the procedure, the MIP search tree is resumed, and if the
procedure ﬁnds a new MIP solution, the MIP solution at the MIP search tree is
updated.
As noted by Danna et al. in [5], consecutive nodes of the MIP search tree
provide almost identical relaxation solution. Therefore, the RINS procedure is
called only every f nodes for some reasonably large f.
3.3
Distance Induced Neighbourhood Search
In contrast to RINS, which performs only hard ﬁxing of variables, and LB,
which performs only soft ﬁxing of variables, our DINS incorporates some hard
ﬁxing, some soft ﬁxing, and some rebounding (changing lower and upper bounds
1 ILOG Cplex 9.13 comes with an implementation of RINS and can be invoked by
setting the Cplex parameter IloCplex::MIPEmphasis to 4 [5].

DINS, a MIP Improvement Heuristic
313
of the variables), all based on a distance metric. In the next sections we show that
DINS outperforms both RINS and LB2 on an instance test bed that includes
all the instances studied in [5,9] as well as some other hard instances from other
sources.
Like RINS, DINS also rely on the fact that, during exploring the MIP search
tree, the relaxation solution at successive nodes (those are not pruned by infea-
sibility or bound) provides a better objective value compared to the objective
value provided by the current MIP solution.
But unlike RINS, the intuition in DINS is that the improved MIP solutions
are more likely to be the close ones to the current relaxation solution. An exact
modeling of this intuition would require inclusion of the following quadratic
inequality which unfortunately cannot be expressed as a linear constraint.

j∈N
(xj −xj(node))2 ≤

j∈N
(xj(mip) −xj(node))2,
where xmip and xnode denote the current MIP solution and the current relaxation
solution, and for a variable xj, xj(mip) and xj(node) denote the values of xj in
xmip and xnode respectively.
DINS relaxes the intuition by considering that the improved MIP solutions
are close to xnode only with respect to the integer variables and choosing the
following inequality based on absolute diﬀerences as the measure of close ones.

j∈B∪G
|xj −xj(node)| ≤

j∈B∪G
|xj(mip) −xj(node)|.
DINS then partially captures this inequality (the chosen distance metric) by
deﬁning a neighbourhood with some rebounding, some hard ﬁxing, and some
soft ﬁxing of the integer variables.
We notice that if an integer variable xj, for which the absolute diﬀerence,
|xj(mip) −xj(node)|, is less than 0.5, takes a diﬀerent value than xj(mip) in an
improved solution, the absolute diﬀerence increases. On the contrary, if an integer
variable, for which the absolute diﬀerence is greater or equal to 0.5, takes a
diﬀerent value than xj(mip) in an improved solution, the absolute diﬀerence may
not increase.
DINS computes new lower and upper bounds of an integer variable xj, for
which the absolute diﬀerence is greater or equal to 0.5, so that at an improved
solution the absolute diﬀerence does not increase. Considering lold
j
and uold
j
as
the existing lower and upper bounds of xj, DINS computes the new lower and
upper bound lnew
j
and unew
j
respectively as follows:
if (xj(mip) ≥xj(node)) then
lnew
j
←max(lold
j
, ⌈xj(node) −(xj(mip) −xj(node))⌉),
unew
j
←xj(mip)
elsif (xj(mip) < xj(node)) then
lnew
j
←xj(mip),
unew
j
←min(uold
j , ⌊xj(node) + (xj(node) −xj(mip))⌋).
2 In [5], Danna et al. have tried two hybrid strategies of RINS and LB and concluded
that their performance were not better than RINS alone.

314
S. Ghosh
We refer it as rebounding; the rebounding does not change existing bounds for
all the variables that fall in this category (for example, no 0-1 variable in this
category change its bounds). If all the integer variables, for which |xj(mip) −
xj(node)| < 0.5, are ﬁxed to their respective current values, then any solution
found from this neighbourhood exploration will obviously be a closer one to
xnode in terms of the chosen distance metric. But the sum of absolute diﬀerences
can also decrease if the total decrease d in the sum of absolute diﬀerences caused
by the integer variables for which |xj(mip) −xj(node)| ≥0.5 is greater than the
total increase d′ in the sum of absolute diﬀerences caused by the integer variables
for which |xj(mip) −xj(node)| < 0.5.
DINS partially captures this observation by allowing the integer variables xj,
for which |xj(mip) −xj(node)| < 0.5, to change their values in xmip so that d′
is not larger than a chosen small number p. It does this by performing some
soft ﬁxing and some hard ﬁxing of these variables. DINS performs soft ﬁxing
through the LB inequality which requires introduction of new variables when
general integer variables are considered. As in [9] and [5], DINS constructs LB
inequality using only 0-1 variables. Therefore, all the general integer variables
xj with |xj(mip) −xj(node)| < 0.5 are ﬁxed (hard ﬁxing) at xj(mip).
Among the 0-1 variables with |xj(mip) −xj(node)| < 0.5, DINS performs some
hard ﬁxing like RINS, but incorporates some more intuition in this process. Like
RINS, DINS chooses the same set of variables, that agree in both the current
MIP solution and the current node relaxation solution, as the primary candidates
for hard ﬁxing. Then it applies a ﬁltering step to this primary candidate set
using two information. First information comes from the intuition that if an
integer variable, in the primary candidate set, takes the same value in the root
relaxation solution of MIP search tree and current node relaxation solution, is
more likely to take the same value in improved MIP feasible solutions. The second
information comes from the intuition that if an integer variable, in the primary
candidate set, takes the same value in the previously encountered MIP solutions,
is more likely to take the same value in improved MIP feasible solutions. This
two information actually gather knowledge from both the relaxation solutions
and previously encountered MIP solutions. DINS uses an array of ﬂag for the
integer variables to keep track which variables have taken diﬀerent values in the
previously encountered MIP solutions. Thus the hard ﬁxing in DINS can be
stated more explicitly in the following way: let xmip, xnode, and xroot denote
the current MIP solution, the current node relaxation solution, and the root
relaxation solution respectively. Also let Δ is an array where Δ[j] is set if xj
has taken diﬀerent values in previously encountered MIP solutions. Therefore, a
variable xj is ﬁxed (hard ﬁxing) at value xj(mip) if xj(mip) = xj(node) = xj(root)
and Δ[j] is clear.
Consider F and H denote the set of variables for which rebounding and hard
ﬁxing has been performed respectively. Now assume R be the set of variables
where R = (B ∪G) −F −H. According to our construction R contains only 0-1
variables.

DINS, a MIP Improvement Heuristic
315
DINS now performs soft ﬁxing on the variables in R, when |R| ̸= φ, by adding
the following LB inequality:

j∈R ∧xj(mip)=0
xj +

j∈R ∧xj(mip)=1
(1 −xj) ≤p
As noted earlier, our intuition is that improved feasible solutions are more
likely to be obtained by getting close to the current relaxation solution from the
current MIP solution. Therefore, DINS generates the promising neighbourhood
taking small value for p which means that a solution, in this deﬁned neighbour-
hood, can have a sum of absolute diﬀerences increased by at most p.
Whenever DINS procedure is invoked at a particular node of MIP search
tree, it creates the described neighborhood with the initial chosen value of p
and explores it using a branch-and-bound or a branch-and-cut solver with a
speciﬁed node limit nl. If the exploration reaches the node limit without ﬁnding
a new solution, DINS reduces p by 5 and explores a new neighbourhood. This
continues until p < 0, or the neighbourhood exploration ﬁnds a new solution
or the neighbourhood is explored completely without ﬁnding a new solution.
Whenever the neighbourhood exploration ﬁnds a new solution, p is reset to its
initial chosen value and continues in the same fashion. The procedure in Figure 1
describes the operation sequence of DINS at a particular node of the MIP search
tree. At the termination of the procedure, the MIP search tree is resumed and,
if the procedure ﬁnds a new MIP solution, the MIP solution at the MIP search
tree is updated.
Like RINS, the DINS procedure is called ﬁrst when the MIP search tree ﬁnds
its ﬁrst MIP solution and, thereafter, at every f nodes of the MIP search tree.
4
Computational Results
4.1
Experimental Setup and Instance Test Bed
We implement LB, RINS, and DINS in the C programming language with the
MIP search tree generated by Cplex 9.13 MIP solver. All experiments are run on
an 2403 MHz AMD Athlon processor with 128 MByte of memory under Redhat
Linux 9.0. An implementation of DINS is available at [11].
We compose a benchmark test bed of MIP instances with the property that the
test bed excludes the instances which default Cplex either solves to optimality
or fails to ﬁnd a MIP solution in one CPU-hour. With this criteria we have 64
MIP instances (all have some 0-1 variables), described in [10], from the following
sources commonly used as benchmark instances for MIP solvers.
– Twenty six instances used in the local branching paper [9]. These instances
have been collected from the instance set maintained by DEIS operations
research group [6].
– Twelve more instances from the instance set maintained by DEIS operations
research group [6].

316
S. Ghosh
– Eleven instances from MIPLIB 2003 [17].
– Five job-shop scheduling instances with earliness and tardiness costs used in
[8].
– Eleven network design and multi-commodity routing instances used in [5].
Procedure DINS at tree node
Input: a 0-1 mixed integer problem P, the current MIP solution xmip,
the current node relaxation solution xnode, the root relaxation solution xroot,
parameter p, node limit nl, and the ﬂag array Δ.
Output: A new MIP solution x∗(xmip in case of failure in ﬁnding a new solution).
1.
if (xmip is a new MIP solution compared to the MIP solution
at the termination of last call of this procedure)
update the array Δ accordingly
2.
x∗←xmip, pcurrent←p, exploreAndNoSolution ←false
3.
repeat
4.
construct P+ from P as follows:
(i) perform rebounding on the variables xj for which |x∗
j −xj(node)| ≥0.5,
(ii) perform hard ﬁxing of the general integer variables xj for which
|x∗
j −xj(node)| < 0.5,
(iii) perform hard ﬁxing of the 0-1 integer variables xj for which
|x∗
j −xj(node)| < 0.5 and x∗
j = xj(node) = xj(root) and Δ[j] is clear,
(iv) let R be the set of remaining 0-1 integer variables.
if (R ̸= φ) perform soft ﬁxing by adding the inequality

j∈R ∧x∗
j =0 xj + 
j∈R ∧x∗
j =1(1 −xj) ≤p
5.
Apply black-box MIP solver to P+ with node limit nl and
an objective cutoﬀequal to the objective value provided by x∗
6.
if (a new solution xnew is obtained) then
7.
x∗←xnew, pcurrent ←p, update the array Δ
8.
elsif (node limit reached without having a new solution) then
9.
if(|R| = φ) pcurrent = −1
10
else pcurrent ←pcurrent −5
11.
else exploreAndNoSolution ←true
12. until (pcurrent < 0 or exploreAndNoSolution)
13. return x∗
Fig. 1. Procedure DINS at tree node
4.2
Comparison Among Methods
We compare DINS against RINS, LB, and Cplex in its default setup (default
Cplex). One CPU-hour is set to be the execution time for each method and it
seems to be suﬃcient to distinguish the eﬀectiveness of all the methods.
Default Cplex is used for exploring the neighbourhoods generated in LB,
RINS, and DINS. The three methods namely LB, RINS, and DINS have a
set of parameters which need to be set. As used in [5], for LB, we set p = 10
and nl = 1000, and for RINS, we use Cplex 9.13 with the parameter IloC-
plex::MIPEmphasis set to 4 where, according to [5], f = 100 and nl = 1000. For

DINS, a MIP Improvement Heuristic
317
DINS, we set p = 5 (diﬀerent from LB to relax our intuition a little as well as
to make the neighbourhood small), f = 100 and nl = 1000.
Following Danna et al. [5], we carry out two set of experiments; in one set of
experiments we invoke all four methods with a presumably poor solution at the
root node of the MIP search tree, and in the other we invoke all four methods with
a presumably good solution at the root node of the MIP search tree. Although
there is no exact way to distinguish a good and a bad MIP solution, following
Danna et al. [5], we presume that the ﬁrst MIP solution found by the default
Cplex MIP solver represents a poor solution, and the solution obtained by default
Cplex in one CPU-hour represents a good solution.
In order to capture the quality of obtained solution by each method, we use
the measure percentage of gap deﬁned by 100*|(obj. value of obtained solution -
obj. value of the best known solution) /obj. value of the best known solution|.
Table 1 and Table 2 show the percentage of gap obtained at the end of one
CPU-hour by all the four methods considered in this paper, where the bold face
identiﬁes the best method for the corresponding instance (multiple bold faces
appear if there are multiple methods obtaining the same solution).
Following Danna et al. [5], we group the instances into three diﬀerent sets so
that the eﬀectiveness of diﬀerent methods in diﬀerent groups becomes visible.
According to [5], the groups are deﬁned as ‘small spread’, ‘medium spread’, and
‘large spread’ instances where the gap between the worst solution found by any
of the four methods considered in this paper and the best known solution is
less than 10%, between 10% and 100%, and larger than 100% respectively. The
percentage of gap shown in Table 1 and Table 2 are used to group the instances.
We use three measures to evaluate the performance of diﬀerent methods.
Our ﬁrst measure is best in number of instances, which represents the number
of instances at which a method ﬁnds the best solution among the solutions
obtained by all the four methods. If multiple methods ﬁnd the same best solution
for an instance, then the instance contributes one in the measures for all the
corresponding methods.
Our second measure is the average percentage of gap, which represents the
arithmetic mean of the percentage of gaps obtained by a method on a group of
instances at a certain point of execution.
Our third measure is the average percentage of improvement, which represents
the arithmetic mean of percentage of improvements obtained by a method on a
group of instances at a certain point of execution. In order to visualize how much
improvement has been obtained by diﬀerent methods starting from a presumably
poor and good solution, we deﬁne the percentage of improvement for an instance
as 100*|(obj. value of the initial solution - obj. value of the obtained solution)
/obj. value of the initial solution|.
Table 3 represents the comparative results of four diﬀerent methods for both
set of experiments.
As expected, DINS, comparing against all other three methods in both set
of experiments, has higher percentage of improvement and lower percentage of
gap for each of the categorized group of instances, and obtains best solution in

318
S. Ghosh
Table 1. Percentage of Gap = 100 ∗|(obj. value of obtained solution - obj. value of
the best known solution)/obj. value of the best known solution| in one CPU-hour
problem
Percentage of Gap
Default Cplex
LB RINS DINS
Small spread instances
a1c1s1
2.347 0.250 0.000 0.079
a2c1s1
2.978 1.889 0.000 0.024
b1c1s1
5.977 1.786 0.933 4.444
b2c1s1
4.240 2.701 0.559 1.010
biella1
0.309 0.806 0.426 0.739
danoint
0.000 0.000 0.000 0.000
mkc
0.180 0.049 0.043 0.021
net12
0.000 0.000 0.000 0.000
nsrand-ipx
0.625 0.625 0.313 0.000
rail507
0.000 0.000 0.000 0.000
rail2586c
2.518 2.204 1.994 1.574
rail4284c
1.774 1.867 1.027 1.027
rail4872c
1.742 1.290 1.097 1.032
seymour
0.473 0.473 0.000 0.236
sp97ar
0.428 0.513 0.335 0.000
sp97ic
0.793 0.642 0.551 0.000
sp98ar
0.184 0.106 0.177 0.228
sp98ic
0.270 0.146 0.204 0.072
tr12-30
0.000 0.024 0.000 0.000
arki001
0.003 0.003 0.004 0.002
roll3000
0.543 0.303 0.070 0.070
umts
0.013 0.049 0.022 0.002
berlin-5-8-0
0.000 0.000 0.000 0.000
bg512142
7.257 5.192 0.161 0.000
blp-ic97
0.779 0.653 0.358 0.000
blp-ic98
0.961 1.056 0.746 0.515
blp-ar98
0.655 0.060 0.461 0.000
cms750-4
2.372 0.791 1.186 0.791
dc1l
2.018 8.166 6.994 1.572
railway-8-1-0
0.250 0.000 0.250 0.250
usabbrv-8-25-70
3.306 2.479 0.000 1.653
aﬂow40b
0.257 1.455 0.000 0.000
dano3mip
2.602 3.595 4.724 2.230
fast0507
0.000 0.575 0.575 0.000
harp2
0.001 0.001 0.023 0.000
t1717
7.948 1.939 5.979 7.948
noswot
0.000 0.000 0.000 0.000
timtab1
7.469 7.779 0.000 0.000
ljb2
0.256 3.329 1.576 3.329
rococoB10-011000
0.802 2.848 0.437 0.437
rococoB11-010000
5.039 5.839 1.768 2.196
rococoB12-111111
5.204 4.489 3.738 2.541

DINS, a MIP Improvement Heuristic
319
Table 2. Percentage of Gap = 100 ∗|(obj. value of obtained solution - obj. value of
the best known solution)/obj. value of the best known solution| in one CPU-hour
Continued from Table 1
problem
Percentage of Gap
Default Cplex
LB
RINS
DINS
Small spread instances
rococoC10-001000
0.044
0.113
0.044
0.000
rococoC11-011100
6.018
9.991
9.244
5.879
rococoC12-111100
5.188
5.188
1.298
4.016
Medium spread instances
glass4
13.014
7.534
2.740
4.794
swath
18.067
5.679
8.089
4.622
dg012142
17.457
25.984
4.963
3.943
liu
2.475
10.066
3.465
5.281
timtab2
16.373
18.484
3.188
0.912
ljb7
7.424
21.834
4.367
8.908
ljb9
50.717
70.866
55.074
50.690
ljb10
0.807
13.929
13.693
8.578
rococoB10-011001
7.660
5.309
5.220
10.082
rococoB11-110001
9.994
19.558
4.267
6.894
rococoC10-100001
16.041
7.387
13.316
10.070
rococoC11-010100
27.431
13.615
10.546
9.029
rococoC12-100000
12.928
10.090
5.623
2.799
Large spread instances
markshare1
500.000
400.00
400.00
500.00
markshare2
1300.000 1100.000 2000.000 1800.000
dc1c
695.213
2.353
0.296
0.773
trento1
0.000
193.118
1.912
0.402
ds
11.226
945.745
11.226
6.119
ljb12
39.273
323.183
49.599
64.987
higher number of instances. It is to be noted that, starting from a presumably
good solution, DINS has become best in more number of instances than the
number of instances in which it has been best in the experimentation with bad
solution.
Furthermore, for diﬀerent group of instances in Figure 2−4, we sketch how
diﬀerent methods improve the solution quality (average percentage of gap) over
time starting from presumably poor solutions. We can draw some basic conclu-
sions analyzing these ﬁgures. For all three group of instances, DINS performance
is worse comparing to that of RINS at the initial level of computation, but DINS
performance becomes better as the computation progresses and once it becomes
better, it maintains its lead over RINS for the remaining part of the computa-
tion. For small and large spread instances, DINS obtains the lead over RINS
earlier than in medium spread instances. Similarly in medium and large spread
instances, DINS performance is worse comparing to that of default Cplex at
the initial level of computation, but DINS outperforms default Cplex as the

320
S. Ghosh
Table 3. A comparative performance summary for diﬀerent methods
Average % of improvement
Group of Instances Default
LB
RINS DINS
(# of instances)
Cplex
experiments from the presumably poor solutions
all instances (64)
36.19
35.49
38.01
38.05
small spread (45)
23.41
23.61
23.90
23.92
medium spread (13)
60.43
60.25
62.05
62.29
large spread (6)
80.78
70.90
91.64
91.66
experiments from the presumably good solutions
all instances (64)
2.35
3.04
3.45
3.96
small spread (45)
0.45
0.78
1.26
1.29
medium spread (13)
2.50
4.91
5.10
6.57
large spread (6)
16.31
15.96
16.27
18.47
Average % of gap
Group of Instances Default
LB
RINS DINS
(# of instances)
Cplex
experiments from the presumably poor solutions
all instances (64)
44.22
51.19
41.33
39.73
small spread (45)
1.86
1.81
1.05
0.97
medium spread (13)
15.41
17.72
10.35
9.74
large spread (6)
424.28 494.07 410.51 395.38
experiments from the presumably good solutions
all instances (64)
32.43
31.67
31.21
29.14
small spread (45)
1.41
1.07
0.56
0.54
medium spread (13)
13.57
10.63
10.46
8.59
large spread (6)
305.92 306.77 306.06 288.17
Best in # of instances
Group of Instances Default
LB
RINS DINS
(# of instances)
Cplex
experiments from the presumably poor solutions
all instances (64)
13
12
25
39
small spread (45)
9
9
19
32
medium spread (13)
2
1
4
6
large spread (6)
2
2
2
1
experiments from the presumably good solutions
all instances (64)
16
23
29
48
small spread (45)
13
17
26
35
medium spread (13)
1
5
2
8
large spread (6)
2
1
1
5
computation progresses. LB is always worse than RINS and DINS where, at
the end of time limit, LB has an edge over default Cplex only in small spread
instances.
In an attempt to see how good intuition DINS has made, we provide some
statistical measures from our experimental results. It has been seen that, the

DINS, a MIP Improvement Heuristic
321
 0
 0.5
 1
 1.5
 2
 2.5
 3
 3.5
 0
 10
 20
 30
 40
 50
 60
Average percentage of gap
Time (in CPU-minutes)
Cplex-D
LB
RINS
DINS
Fig. 2. progress of diﬀerent methods in reducing percentage of gap on the 45 small
spread instances
 0
 5
 10
 15
 20
 25
 30
 35
 40
 45
 50
 0
 10
 20
 30
 40
 50
 60
Average percentage of gap
Time (in CPU-minutes)
Cplex-D
LB
RINS
DINS
Fig. 3. progress of diﬀerent methods in reducing percentage of gap on the 13 medium
spread instances

322
S. Ghosh
 0
 200
 400
 600
 800
 1000
 0
 10
 20
 30
 40
 50
 60
Average percentage of gap
Time (in CPU-minutes)
Cplex-D
LB
RINS
DINS
Fig. 4. progress of diﬀerent methods in reducing percentage of gap on the 6 large
spread instances
number of times neighbourhood exploration ﬁnds a new solution in all the in-
stances, the chosen distance metric was satisﬁed in 80.89% occurrences, and the
quadratic distance metric was satisﬁed in 80.5% occurrences. These experimental
results support our intuition that improved solutions are more likely to be close
to the node relaxation solutions, and also support our choice of distance metric.
Moreover, relaxing the chosen distance metric a little bit gives DINS the extra
power of ﬁnding those improved solutions that do not satisfy the chosen distance
metric at the node at which the solution has been obtained, but probably would
satisfy the chosen distance metric at some deeper nodes of the MIP search tree.
5
Conclusions
We have introduced DINS, a heuristic to ﬁnd improved MIP feasible solutions
from a known MIP feasible solution, based on a distance metric between the
current MIP solution and the current node relaxation solution.
A comparison of DINS against existing neighbourhood search based heuristics
shows that it outperforms both RINS and LB in obtaining good MIP solutions
within a certain time limit and in the power of improving both poor and good
MIP solutions.
Unlike RINS, DINS uses the change of relaxation solution between the root
and the node and the change in the encountered MIP solutions in guiding the
hard ﬁxing of 0-1 variables; this has an eﬀect in ﬁnding the good MIP solutions as

DINS, a MIP Improvement Heuristic
323
the computation progresses. This has been experimentally visualized by having
a comparatively worse performance on the benchmark instances by running a
modiﬁed DINS where the hard ﬁxing of 0-1 variables are carried out according
to the hard ﬁxing of RINS.
Acknowledgements. We thank Emilie Danna for the useful email discussions
during the implementation and analysis of the methods.
References
1. E. Balas, S. Ceria, M. Dawande, F. Margot, and G. Pataki. Octane: a new heuristic
for pure 0-1 programs. Operations Research, 49(2):207–225, 2001.
2. E. Balas and C.H. Martin. Pivot and complement – a heuristic for 0-1 program-
ming. Management Science, 26(1):86–96, 1980.
3. E. Balas and C.H. Martin. Pivot and shift – a heuristic for mixed integer program-
ming. Technical report, GSIA, Carnegie Mellon University, 1986.
4. E. Balas, S. Schmieta, and C. Wallace. Pivot and shift – a mixed integer program-
ming heuristic. Discrete Optimization, 1:3–12, 2004.
5. E. Danna, E. Rothberg, and C.L. Pape. Exploring relaxation induced neighborhh-
ods to improve mip solutions. Mathematical Programming, 102:71–90, 2005.
6. DEIS. Library of instances. www.or.deis.unibo.it/research pages/ORinstances/.
7. B.H. Faaland and F.S. Hillier. Interior path methods for heuristic integer program-
ming procedures. Operations Research, 27(6):1069–1087, 1979.
8. M. Fischetti, F. Glover, and A. Lodi. The feasibilty pump. to be appeared on
Mathematical Programming.
9. M. Fischetti and A. Lodi. Local branching. Mathematical Programming B, 98:
23–49, 2003.
10. S. Ghosh.
Description of all the used benchmark instances in this paper.
www.cs.ualberta.ca/∼shubhash/dins/benchmarks.ps.
11. S. Ghosh. Implementation of DINS. www.cs.ualberta.ca/∼shubhash/codes.html.
12. F.S. Hillier. Eﬃcient heuristic procedures for integer linear programming with an
interior. Operations Research, 17(4):600–637, 1969.
13. T. Ibaraki, T. Ohashi, and H. Mine. A heuristic algorithm for mixed-integer pro-
gramming problems. Math. Program. Study., 2:115–136, 1974.
14. R. M. Karp. Reducibility among combinatorial problems. In R. E. Miller and J. W.
Thatcher, editors, Complexity of Computer Computations, pages 85–103. Plenum
Press, New York, 1972.
15. A. Løkketangen, K. J¨ornsten, and S. Storøy.
Tabu search within a pivot and
complement framework. International Transactions in Operational Research, 1(3):
305–317, 1994.
16. A. Løkketangen and D.L. Woodruﬀ. Integrating pivot based search with branch
and bound for binary mip’s. Control and Cybernetics, Special issue on Tabu Search,
29(3):741–760, 2001.
17. A. Martin, T. Achterberg, and T. Koch. Miplib 2003. http://miplib.zib.de.
18. M. Nediak and J. Eckstein. Pivot, cut, and dive: A heuristic for mixed 0-1 integer
programming. RUTCOR Research Report, RRR 53-2001, 2001.

Mixed-Integer Vertex Covers on Bipartite
Graphs
Michele Conforti1, Bert Gerards2,3, and Giacomo Zambelli1
1 Dipartimento di Matematica Pura e Applicata, Universit´a di Padova, Via Trieste
63, 35121 Padova, Italy
conforti@math.unipd.it, giacomo@math.unipd.it
2 Centrum voor Wiskunde en Informatica, Kruislaan 413, 1098 SJ Amsterdam,
The Netherlands
Bert.Gerards@cwi.nl
3 Technische Universiteit Eindhoven, Den Dolech 2, Eindhoven, The Netherlands
Abstract. Let A be the edge-node incidence matrix of a bipartite graph
G = (U, V ; E), I be a subset of the nodes of G, and b be a vector such
that 2b is integral. We consider the following mixed-integer set:
X(G, b, I) = {x : Ax ≥b, x ≥0, xi integer for all i ∈I}.
We characterize conv(X(G, b, I)) in its original space. That is, we de-
scribe a matrix (C, d) such that conv(X(G, b, I)) = {x : Cx ≥d}. This
is accomplished by computing the projection onto the space of the x-
variables of an extended formulation, given in [1], for conv(X(G, b, I)).
We then give a polynomial-time algorithm for the separation problem for
conv(X(G, b, I)), thus showing that the problem of optimizing a linear
function over the set X(G, b, I) is solvable in polynomial time.
1
Introduction
Given a bipartite graph G = (U, V ; E), a vector b = (be)e∈E, with the property
that b is half-integral, i.e. 2be ∈Z, e ∈E, and a set I ⊆(U ∪V ), we consider
the problem of characterizing the convex hull of all nonnegative x ∈RU∪V such
that
xi + xj ≥bij for every ij ∈E,
xi ∈Z
for every i ∈I.
That is, given the edge-node incidence matrix A of a bipartite graph G, a par-
tition (I, L) of its column-set, and an half-integral vector b, we consider the
following mixed-integer set:
X(G, b, I) = {x : Ax ≥b, x ≥0, xi integer for all i ∈I}.
(1)
In this paper we provide a formulation for the polyhedron conv(X(G, b, I)),
where a formulation for a polyhedron P is a description of P as the intersection
of a ﬁnite number of half-spaces. So it consists of a ﬁnite set of inequalities
Cx ≥d such that P = {x : Cx ≥d}.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 324–336, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

Mixed-Integer Vertex Covers on Bipartite Graphs
325
An extended formulation of P is a formulation for a polyhedron P ′ in a higher
dimensional space that includes the original space, so that P is the projection
of P ′ onto the original space.
A general technique to describe an extended formulation for the set of solu-
tions of a system Ax ≥b, when A⊤is a network matrix and some of the variables
are restricted to be integer, was introduced in [1]. In Section 2 we derive such
an extended formulation for conv(X(G, b, I)), while in Section 3 we describe a
formulation in the original space by explicitly computing the projection of the
polyhedron deﬁned by the extended formulation. Finally, in Section 4, we give a
polynomial-time algorithm to solve the separation problem for conv(X(G, b, I)).
1.1
The Main Result
Given a bipartite graph G = (U, V ; E), a partition (I, L) of U ∪V , and an half-
integral vector b, we say that a path P of G is an I-path if at least one endnode
of P is in I, and no intermediate node of P is in I. We say that P is odd if P
has an odd number of edges e such that be = 1
2 mod 1. Whenever we have a
vector v with entries indexed by some set S, given a subset T of S we denote
v(T ) = 
i∈T vi. In this paper we show the following:
Theorem 1. The polyhedron conv(X(G, b, I)) is deﬁned by the following in-
equalities:
xi + xj ≥
bij
ij ∈E,
(2)
2x(V (P) ∩L) + x(V (P) ∩I) ≥b(P) + 1
2 P odd I-path,
(3)
xi ≥
0
i ∈U ∪V .
(4)
Eisenbrand [4] conjectured that the inequalities in (2)-(4) are suﬃcient to char-
acterize conv(X(G, b, I)) when G is a path. Theorem 1 shows that this conjecture
holds in a quite more general setting (and it certainly cannot be extended be-
yond that). Preliminary results for the path case were obtained by Skutella [11]
and Eisenbrand [4].
1.2
First Chv´atal Closure
The following observation allows us to describe X(G, b, I) in terms of a pure
integer set.
Observation 2. Let ¯x be a vertex of conv(X(G, b, I)). Then 2¯x is integral.
Proof: If not, let U ′ and V ′ be the sets of nodes i in U and V , respectively, such
that 2¯xi is not integer. Then, for ϵ small enough, the vectors ¯x + ϵχU′ −ϵχV ′
and ¯x −ϵχU′ + ϵχV ′ are both in conv(X(G, b, I)), where we denote by χS the
incidence vector of S for any S ⊆U ∪V .
□
Let b′ = 2b, A′ be obtained form A by multiplying by 2 the columns correspond-
ing to nodes in I. By Observation 2, the linear transformation x′
i = xi, i ∈I,

326
M. Conforti, B. Gerards, and G. Zambelli
x′
i = 2xi, i ∈L, maps X(G, b, I) into {x′ : A′x′ ≥b′, x′ ≥0, x′ integral}, which
is a pure integer set.
Let P = v1, . . . vn be an I-path. Notice that b(P) = 1
2 mod 1 is equivalent to
b′(P) odd. Then the inequality

i∈V (P )
x′
i ≥
b′(P)
2

(5)
is a Gomory-Chv´atal inequality of {x′ : A′x′ ≥b′, x′ ≥0}. Indeed, assume
v1 ∈I. If vn ∈I, then (5) is obtained from
1
2(2x′
v1 +x′
v2 ≥b′
v1v2)+
n−2

i=2
1
2(x′
vi +x′
vi+1 ≥b′
vivi+1)+ 1
2(x′
vn−1 +2x′
vn ≥b′
vn−1vn)
by rounding up the right-hand-side. If xn /∈I, then (5) is obtained from
1
2(2x′
v1 + x′
v2 ≥b′
v1v2) +
n−1

i=2
1
2(x′
vi + x′
vi+1 ≥b′
vivi+1) + 1
2(x′
vn ≥0)
by rounding up the right-hand-side.
Furthermore the inequalities in (5) correspond to the inequalities in (3).
Therefore Theorem 1 implies that the polyhedron deﬁned by A′x′ ≥b′, x′ ≥0
has Chv´atal rank 1. In the case where G is a path with no intermediate node
in I, this last fact follows immediately from a theorem of Edmonds and Jonhn-
son [2,3], since in this case A′ satisﬁes the condition that the sum of the absolute
values of the entries of each column is at most 2.
1.3
The Motivation
A (general) mixed-integer set is a set of the form
{x | Ax ≥b, xi integer i ∈I}
(6)
where I is a subset of the columns of A and b is a vector that may contain
fractional components.
In [1], it is shown that the problem of deciding if the above set is nonempty
is NP-complete, even if b is an half-integral vector and A is a network matrix.
(We refer the reader to [7] or [10] for deﬁnitions and results related to network
matrices and, more generally, totally unimodular matrices.)
However, it may be possible that, when A is the transpose of a network matrix,
the associated mixed-integer programming problem is polynomially solvable. In-
deed, let MIX2T U be a mixed-integer set of the form (6) when A⊤is a network
matrix.
An extended formulation of the polyhedron conv(MIX2T U) was described
in [1]. The extended formulation involves an additional variable for each possible
fractional part taken by the variables at any vertex of conv(MIX2T U). If this

Mixed-Integer Vertex Covers on Bipartite Graphs
327
number is polynomial in the size of (A, b), then such a formulation is compact,
i.e. of polynomial size in the size of (A, b). Therefore the problem of optimizing
a linear function over MIX2T U can be eﬃciently solved in this case. However,
it seems to be rather diﬃcult to compute the projection in the original x-space.
It follows from Observation 2 that if ¯x is a vertex of conv(X(G, b, I)), then ¯xi −
⌊¯xi⌋∈{0, 1
2}. Therefore the extended formulation for conv(X(G, b, I)) (which
will be introduced in Section 2) is compact. The main contribution of this paper
is the explicit description of the projection of the polyhedron deﬁned by this
extended formulation in the original x-space.
The mixed-integer set X(G, b, I) is related to certain mixed-integer sets that
arise in the context of production planning (see [9]). The case when G is a star
with center node in L and leaves in I has been studied by Pochet and Wolsey
in [8], where they gave a compact extended formulation for the convex hull of
feasible solutions. G¨unl¨uk and Pochet [5] projected this formulation onto the
original space, thus showing that the family of mixing inequalities gives the
formulation in the x-space.
Miller and Wolsey [6] extended the results in [8] to general bipartite graphs,
with the restriction that the partition (I, L) coincides with the bipartition (U, V )
of the graph. Their result shows that the mixing inequalities associated with
every single star of G having center a node in L and leaf nodes all nodes in I
give a formulation for this case.
2
The Extended Formulation
We use here a modeling technique introduced by Pochet and Wolsey [8] and
extensively investigated in [1].
Observation 2 allows to express each variable xi , i ∈L, as
xi = μi + 1
2δi, μi ≥0, 0 ≤δi ≤1, μi, δi integer.
(7)
For now, we assume I = ∅, that is, L = (U ∪V ).
Lemma 3. Let ij ∈E, and suppose xi, xj, μi, μj, δi, δj satisfy (7).
If bij = 1
2 mod 1, xi, xj satisfy xi + xj ≥bij if and only if
μi + μj ≥⌊bij⌋
μi + δi + μj + δj ≥⌈bij⌉.
(8)
If bij = 0 mod 1, xi, xj satisfy xi + xj ≥bij if and only if
μi + δi + μj ≥bij
μi + μj + δj ≥bij .
(9)
Proof: Assume xi, xj, μi, μj, δi, δj satisfy (7). Then, if bij = 1
2 mod 1, constraint
xi + xj ≥bij is satisﬁed if and only if μi + μj ≥⌊bij⌋and δi + δj ≥1 whenever
μi+μj = ⌊bij⌋. If bij = 0 mod 1, the constraint is satisﬁed if and only if μi+μj ≥
bij −1 and δi = δj = 1 whenever μi + μj = bij −1.
It is easy to see that these conditions are enforced by the above constraints. □

328
M. Conforti, B. Gerards, and G. Zambelli
Observation 4. Given ij ∈E, the constraints (8) and (9) belong to the ﬁrst
Chv´atal closure of the polyhedron deﬁned by
μi + 1
2δi + μj + 1
2δj ≥bij
μi, μj ≥0
δi, δj ≤1
δi, δj ≥0
whenever bij = 1
2 mod 1 and bij = 0 mod 1, respectively.
By applying the unimodular transformation μ0
i = μi, μ1
i = μi+δi, the constraints
xi = μi + 1
2δi, μi ≥0, 0 ≤δi ≤1 become
xi −1
2(μ0
i + μ1
i ) = 0
(10)
μ0
i ≥0
0 ≤μ1
i −μ0
i ≤1
(11)
and constraints (8) and (9) become:
μ0
i + μ0
j ≥⌊bij⌋
μ1
i + μ1
j ≥⌈bij⌉
(12)
μ1
i + μ0
j ≥bij
μ0
i + μ1
j ≥bij
(13)
Theorem 5. The projection onto the space of the x variables of the polyhedron
Q deﬁned on the space of the variables (x, μ0, μ1) by the inequalities
(10), (11) for every i ∈U ∪V,
(12) for every ij ∈E s.t. bij = 1
2 mod 1
(13) for every ij ∈E s.t. bij = 0 mod 1
is the polyhedron conv(X(G, b, ∅)).
Proof: Since the variable xi is determined by (10) for all i ∈U ∪V , we only need
to show that the polyhedron deﬁned by inequalities (11) for every i ∈U ∪V , (12)
for every ij ∈E s.t. bij = 1
2 mod 1, and (13) for every ij ∈E s.t. bij = 0 mod 1,
is integral. Let Aμ be the constraint matrix of the above system. Since G is a
bipartite graph, then the matrix ¯A, obtained by multiplying by −1 the columns
of Aμ relative to the variables μ0
i , μ1
i , i ∈V , has at most a 1 and at most a −1
in each row. Therefore ¯A is the transpose of a network matrix, so Aμ is totally
unimodular (see [10]). Since the right-hand-sides of (11)-(13) are all integer, the
statement follows from the theorem of Hoﬀman and Kruskal.
□
Observation 6. For any i ∈U ∪V , xi is integer valued if and only if δi = 0.
Therefore, for a given I ⊆(U ∪V ), the polyhedron conv(X(G, b, I)) is the projec-
tion onto the space of the x variables of the face QI of Q deﬁned by the equations
μ1
1 −μ0
i = 0, i ∈I (which correspond to δi = 0, i ∈I).

Mixed-Integer Vertex Covers on Bipartite Graphs
329
3
The Formulation in the Original Space
In this section we prove Theorem 1 by projecting the polyhedron QI onto the
space of the x variables.
Let pi = μ0
i −μ1
i
2
. The μ0
i = xi +pi and μ1
i = xi −pi. The inequalities (10)-(13),
deﬁning Q, become:
pi + pj ≥⌊bij⌋−xi −xj, ij ∈E s.t. bij = 1
2 mod 1,
−pi −pj ≥⌈bij⌉−xi −xj, ij ∈E s.t. bij = 1
2 mod 1,
pi −pj ≥bij −xi −xj,
ij ∈E s.t. bij = 0 mod 1,
−pi + pj ≥bij −xi −xj,
ij ∈E s.t. bij = 0 mod 1,
pi ≥−1
2,
i ∈U ∪V,
−pi ≥0,
i ∈U ∪V,
pi ≥−xi,
i ∈U ∪V.
By Observation 6, conv(X(G, B, I)) is the projection onto the x-space of the
polyhedron deﬁned by the above inequalities and by pi = 0 for every i ∈I.
Associate multipliers to the above constraints as follows:
(u++
ij )
pi + pj ≥⌊bij⌋−xi −xj
(u−−
ij ) −pi −pj ≥⌈bij⌉−xi −xj
(u+−
ij )
pi −pj ≥bij −xi −xj
(u−+
ij ) −pi + pj ≥bij −xi −xj
(u
1
2
i )
pi ≥−1
2
(u0
i )
−pi ≥0
(ux
i )
pi ≥−xi
(14)
Any valid inequality for conv(X(G, b, I)) has the form αux ≥βu, where
αux =

bij= 1
2 mod 1
(u++
ij
+ u−−
ij )(xi + xj) +

bij=0 mod 1
(u+−
ij
+ u−+
ij )(xi + xj) +

i∈U∪V
ux
i xi
(15)
βu =

bij= 1
2 mod 1
(u−−
ij ⌈bij⌉+ u++
ij ⌊bij⌋) +

bij=0 mod 1
(u+−
ij
+ u−+
ij )bij −

i∈L
1
2u
1
2
i
(16)
for some nonnegative vector u = (u++
ij , u−−
ij , u+−
ij , u−+
ij , u
1
2
i , u0
i , ux
i ) such that
uP = 0, where P is the column-submatrix of the above system (14) involv-
ing columns corresponding to variables pi, i ∈L (see e.g. Theorem 4.10 in [7]).
For instance the inequality xi + xj ≥bij, for ij ∈E with bij =
1
2 mod 1, is
obtained by setting u++
ij
= u−−
ij
= 1
2, and all other entries of u to be 0.

330
M. Conforti, B. Gerards, and G. Zambelli
We are interested in characterizing the nonnegative vectors u such that uP = 0
and αux ≥βu is facet-deﬁning for conv(X(G, b, I)), and such that the inequality
αux ≥βu is not of the form xi + xj ≥bij, for some ij ∈E, or xi ≥0, for some
i ∈U ∪V . From now on we will assume, w.l.o.g., that the entires of u are integer
and relatively prime.
We deﬁne an auxiliary graph Γu = (L ∪{d}, F), where d is a dummy node
not in U ∪V , and F is deﬁned as follows.
– For every edge ij ∈E such that i, j ∈L, there are u++
ij
+ u−−
ij
+ u+−
ij
+ u−+
ij
parallel edges between i and j in F, each edge corresponding to a multiplier
among u++
ij , u−−
ij , u+−
ij , u−+
ij .
– For each node i ∈L, there are u
1
2
i + u0
i + ux
i + 
j∈I : ij∈E(u++
ij
+ u−−
ij
+
u+−
ij
+ u−+
ij ) parallel edges between d and i in F, each edge corresponding
to a multiplier among u
1
2
i , u0
i , ux
i , or u++
ij , u−−
ij , u+−
ij , u−+
ij , for some j ∈I.
We impose a bi-orientation ω on Γu, that is, to each edge e ∈F, and each
endnode i of e that belongs to L, we associate the value ω(e, i) = tail if e cor-
responds to an inequality of (14) where pi has coeﬃcient −1, while we associate
the value ω(e, i) = head if e corresponds to an inequality of (14) where pi has
coeﬃcient +1. The dummy node d is neither a tail nor a head of any edge. Thus,
each edge of Γu can have one head and one tail, two heads, two tails, or, if d is
one of the two endnodes, only one head and no tail or only one tail and no head.
For each i ∈L, we denote with δin
ω (i) the number of edges in F of which i is
a head, and with δout
w (i) the number of edges in F of which i is a tail.
We say that Γu is ω-eulerian if δin
ω (i) = δout
ω (i) for every i ∈L.
Observation 7. Γu is ω-eulerian if and only if uP = 0.
We deﬁne a closed ω-eulerian walk in Γu as a closed-walk in Γu,
v0, e0, v1, e1, . . . , vk, ek, vk+1,
where v0 = vk+1, with the property that ω(eh−1, vh) ̸= ω(eh, vh) for every h
such that vh is in L, h = 0, . . . , k, k + 1, where the indices are taken modulo k.
That is, if vh ∈L, then vh is a head of eh−1 if and only if vh is a tail of eh.
Observation 8. Γu is ω-eulerian if and only if Γu is the disjoint union of closed
ω-eulerian walks. In particular, every node in L ∪{d} has even degree in Γu.
Observe that, if v0, e0, . . . , ek, vk+1 is a closed ω-eulerian walk in Γu, then both
graphs Γ ′, Γ ′′ on L ∪{d} with edge-sets F ′ = {e1, . . . , ek} and F ′′ = F \ F ′,
respectively, are ω-eulerian. Suppose F ′′ ̸= ∅. Then there are nonnegative integer
vectors u′ and u′′, both diﬀerent from zero, such that u′P = 0, u′′P = 0, Γ ′ = Γu′
and Γ ′′ = Γu′′, and u = u′+u′′. By the fact that Γ ′ and Γ ′′ are ω-eulerian, and by
the structure of the inequalities in (14), the vectors (αu′, βu′) and (αu′′, βu′′) are
both non-zero. Furthermore αu = αu′ + αu′′ and βu = βu′ + βu′′, contradicting
the fact that αux ≥βu is facet-deﬁning and the entries of u are relatively prime.
Hence we have shown the following.

Mixed-Integer Vertex Covers on Bipartite Graphs
331
Observation 9. Every closed ω-eulerian walk of Γu traverses all the edges in
F. In particular, there exists a closed ω-eulerian walk v0, e0, . . . , ek, vk+1 of Γu
such that F = {eh | h = 1, . . . , k}.
Suppose d has positive degree in Γ. Then we may assume, w.l.o.g., that v0 =
vk+1 = d. Suppose d = vh for some h = 1, . . . , k. Then v0, e0, v1, . . . , eh−1vh is
a closed ω-eulerian walk, contradicting the previous observation. Hence we have
the following.
Observation 10. Node d has degree 0 or 2 in Γu.
Next we show the following.
Lemma 11. Every node in L ∪{d} has degree 0 or 2 in Γu.
Proof: We have already shown d has degree 0 or 2 in Γu. If d has degree 2, we
assume d = v0 = vk+1, else v0 is arbitrarily chosen. If there is a node in L with
degree at least 4, then there exists distinct indices s, t ∈{1, . . . , k} such that
vs = vt. We choose s and t such that t −s is positive and as small as possible.
Therefore C = vs, es, . . . , et−1, vt is a cycle of Γu containing only nodes in L.
Since G is a bipartite graph, C has even length, hence the edges in C can be
partitioned into two matchings M0, M1 of cardinality |C|/2. We will denote with
HH, T T , HT the sets of edges of F with, respectively, two heads, two tails, one
head and one tail.
If vs is the head of exactly one among es and et−1, then C is a closed ω-
eulerian walk, contradicting Observation 9. Hence vs is either a head of both es
and et−1 or a tail of both es and et−1. This shows that |C ∩T T | = |C ∩HH|±1.
Therefore there is an odd number of edges e in C such that be = 1
2 mod 1. By
symmetry, we may assume 
e∈M0 be ≥
e∈M1 be + 1
2. Then the inequality
2

i∈V (C)
xi ≥

e∈C
be + 1
2
(17)
is valid for conv(X(G, b, I)), since it is implied by the valid inequalities xi +xj ≥
bij, ij ∈M0, because
2

i∈V (C)
xi = 2

ij∈M0
(xi + xj) ≥2

ij∈M0
bij ≥

e∈M0
be +

e∈M1
be + 1
2 =

e∈C
be + 1
2.
Case 1: Node vs is a tail of both es and et−1.
Then |C ∩T T | = |C ∩HH| + 1, hence

e∈C∩T T
⌊be⌋+

e∈C∩HH
⌈be⌉+

e∈C∩HT
be =

e∈C
be + 1
2.
(18)
Let u′ be the vector obtained from u as follows
u′∗∗
ij = u∗∗
ij −1 for every ij ∈C
u′0
vs = u0
vs + 2

332
M. Conforti, B. Gerards, and G. Zambelli
all other components of u′ and u being identical, where u∗∗
ij is the variable among
u++
ij , u−−
ij , u+−
ij , u−+
ij
corresponding to edge ij of C.
Then one can easily see that Γu′ is the graph obtained from Γu by removing
the edges es, . . . , et, and adding two parallel edges vsd both with tail in vs, hence
Γu′ is ω-eulerian and u′P = 0. By (18)
βu′ = βu −

e∈C
be −1
2,
while by construction
αux = αu′x + 2

i∈V (C)
xi.
Thus αux ≥βu can be obtained by taking the sum of αu′x ≥βu′ and (17),
contradicting the assumption that αux ≥βu is facet-deﬁning.
Case 2: Node vs is a head of both es and et−1.
Then |C ∩T T | = |C ∩HH| −1, hence

e∈C∩T T
⌊be⌋+

e∈C∩HH
⌈be⌉+

e∈C∩HT
be =

e∈C
be −1
2.
(19)
Let u′ be the vector obtained from u as follows

u′∗∗
ij = u∗∗
ij −1 for every ij ∈C
u
′ 1
2
vs = u
1
2vs + 2
all other components of u′ and u being identical.
Then one can easily see that Γu′ is the graph obtained from Γu by removing the
edges es, . . . , et, and adding two parallel edges vsd both with head in vs, hence
u′P = 0. By (19)
βu′ = βu −

e∈C
be + 1
2 −21
2,
while by construction
αux = αu′x + 2

i∈V (C)
xi.
Thus αux ≥βu can be obtained by taking the sum of αu′x ≥βu′ and (17),
contradicting the assumption that αux ≥βu is facet-deﬁning.
□
We are now ready to give the proof of the main theorem.
Proof of Theorem 1. We show that all facet-deﬁning inequalities αux ≥βu, where
u is nonnegative, integral, and with entries that are relatively prime, that are
not inequalities in (2) or (4), are of the form (3).

Mixed-Integer Vertex Covers on Bipartite Graphs
333
First we show the following.

ij∈E
u−−
ij
>

ij∈E
u++
ij
+

i∈U∪V
u
1
2
i
(20)
In fact, we can write the inequality
αux ≥

bij= 1
2 mod 1
(u−−
ij
+ u++
ij )bij +

bij=0 mod 1
(u+−
ij
+ u−+
ij )bij
as nonnegative combination of inequalities of the form (2) or (4), therefore we
must have
βu >

bij= 1
2 mod 1
(u−−
ij
+ u++
ij )bij +

bij=0 mod 1
(u+−
ij
+ u−+
ij )bij.
Thus
0 < βu −

bij= 1
2 mod 1
(u−−
ij
+ u++
ij )bij −

bij=0 mod 1
(u+−
ij
+ u−+
ij )bij
= 1
2(

ij∈E
u−−
ij
−

ij∈E
u++
ij
−

i∈U∪V
u
1
2
i )
which proves (20).
By Lemma (11) and Observation (9), Γu consists of an induced cycle C and
isolated nodes, where every node in V (C) ∩L is a head of exactly one edge and
a tail of exactly one edge.
If d is an isolated node, then each edge ij of C corresponds to a variable of
the form u∗∗
ij , and since the total number of heads in C equals the number of
tails, then 
ij∈E u−−
ij
= 
ij∈E u++
ij
and 
i∈U∪V u
1
2
i = 0, contradicting (20).
Thus we may assume that C = v0, e0, . . . , ek, vk+1 where d = v0 = vk+1.
Claim: The following are the only possible cases, up to symmetry.
1. Edges dv1, dvk of Γu correspond to variables ux
v1 and ux
vk, respectively;
2. dv1 corresponds to variable u−−
wv1 or u−+
wv1 for some w ∈I, and dvk corresponds
to ux
vk;
3. dv1 corresponds to variables u−−
wv1 or u−+
wv1 for some w ∈I, and dvk corresponds
to variable u−−
w′vk or u−+
w′vk for some w′ ∈I.
Proof of claim. If v1 is a head of e0 and vk is a head of ek, then the number of
edges among e1, . . . , ek−1 with two tails is one plus the number of edges with two
heads. Since the former correspond to variables of type u−−
ij
for some ij ∈E,
and the latter correspond to to variables of type u++
ij
for some ij ∈E, then
by (20) dv1 does not correspond to variable u
1
2v1 or to a variable u++
wv1 for any
w ∈I, and dvk does not correspond to variable u
1
2vk or to a variable u++
wvk for any
w ∈I, thus one of the above three cases holds.

334
M. Conforti, B. Gerards, and G. Zambelli
If v1 is a tail of e0 and vk is a head of ek, then the number of edges among
e1, . . . , ek−1 with two tails is equal the number of edges with two heads. By (20),
dv1 corresponds to variable u−−
wv1 for some w ∈I, and dvk corresponds to either
ux
vk or to a variable u−+
w′vk for some w′ ∈I, thus case 2 or 3 holds.
If v1 is a tail of e0 and vk is a tail of ek, then the number of edges among
e1, . . . , ek−1 with two tails is equal one minus the number of edges with two heads.
By (20), dv1 corresponds to variable u−−
wv1 for some w ∈I, and dvk corresponds
to a variable u−−
w′vk for some w′ ∈I, thus case 3 holds. This completes the proof
of the claim.
Case 1: Edges dv1, dvk of Γu correspond to variables ux
v1 and ux
vk, respectively.
In this case the path P = v1, e1, . . . , ek−1, vk of Γu is also a path of G containing
only nodes in L, and P contains an odd number of edges e such that be =
1
2 mod 1. The inequality αux ≥βu is then 2x(V (P)) ≥b(P) + 1
2. The edges
of P can be partitioned into two matchings M0 and M1, thus we may assume,
w.l.o.g., 
e∈M0 be ≥
e∈M1 be + 1
2. Thus 2x(V (P)) ≥2 
ij∈M0(xi + xj) ≥
2 
ij∈M0 bij ≥
e∈M0 be + 
e∈M1 be + 1
2 = b(P) + 1
2, hence αux ≥βu is not
facet-deﬁning.
Case 2: dv1 corresponds to variable u−−
wv1 or u−+
wv1 for some w ∈I, and dvk
corresponds to ux
vk.
In this case, P = w, v1, e1, . . . , ek−1, vk is an odd I-path of G between w ∈I and
vk ∈L. The inequality αux ≥βu is 2x(V (P) ∩L) + xw ≥b(P) + 1
2, which is one
of the inequalities in (3).
Case 3: dv1 corresponds to variables u−−
wv1 or u−+
wv1 for some w ∈I, and dvk
corresponds to variable u−−
w′vk or u−+
w′vk for some w′ ∈I.
If w ̸= w′, then the path P = w, v1, e1, . . . , ek−1, vk, w′ is an odd I-path of G
between w ∈I and w′ ∈I. The inequality αux ≥βu is 2x(V (P)∩L)+xw+xw′ ≥
b(P) + 1
2, which is one of the inequalities in (3).
If w = w′, then we must have v1 ̸= vk, since otherwise v1 would be either the
head or the tail of both edges of Γu incident to v1. Thus C′ = w, v1, . . . , vk, w
is a cycle of G. Since G is a bipartite graph, C′ has even length, hence the
edges in C′ can be partitioned into two matchings M0, M1 of cardinality |C′|/2.
Since C′ contains an odd number of edges e such that bw = 1
2 mod 1, then we
may assume, w.l.o.g., 
e∈M0 be ≥
e∈M1 be + 1
2. The inequality αux ≥βu is
2x(V (C′)) ≥b(C′) + 1
2. But 2x(V (C′)) = 2 
ij∈M0(xi + xj) ≥2 
ij∈M0 bij ≥

e∈M0 be + 
e∈M1 be + 1
2 = b(C′) + 1
2, hence αux ≥βu is not facet-deﬁning.
□
4
Separation
Theorem 5 and Observation 6 imply that the problem of minimizing a linear func-
tion over the set X(G, b, I) is solvable in polynomial time, since it reduces to solving
a linear programming problem over the set of feasible points for (10)-(13).

Mixed-Integer Vertex Covers on Bipartite Graphs
335
In this section we give a combinatorial polynomial-time algorithm for the sepa-
ration problem for the set conv(X(G, b, I)), thus giving an alternative proof that
the problem of optimizing a linear function over such polyhedron, and thus over
X(G, b, I), is polynomial.
Clearly, given a nonnegative vector x∗, we can check in polynomial-time whether
x∗satisﬁes (2) for every edge. Thus, by Theorem 1, we only need to describe a
polynomial-time algorithm that, given a nonnegative vector x∗satisfying (2), ei-
ther returns an inequality of type (3) violated by x∗, or proves that none exists.
For every ij ∈E, let s∗
ij = x∗
i + x∗
j −bij. Since x∗satisﬁes (2), then s∗
e is
nonnegative for every e ∈E. Let P = v1, . . . vn be an odd I-path.
Claim. The vector x∗satisﬁes 2x∗(V (P) ∩L) + x∗(V (P) ∩I) ≥b(P) + 1
2 if and
only if s∗(P) + x∗({v1, vn} ∩L) ≥1
2.
Indeed, assume v1 ∈I. If vn ∈I then
n−1

i=1
s∗
vivi+1 =
n−1

i=1
(x∗
vi + x∗
vi+1 −bvivi+1)
gives the equality s∗(P) = 2x∗(V (P)∩L)+x∗(V (P)∩I)−b(P), hence 2x∗(V (P)∩
L) + x∗(V (P) ∩I) ≥b(P) + 1
2 if and only if s∗(P) ≥1
2.
If vn /∈I, then
n−1

i=1
s∗
vivi+1 + x∗
vn =
n−1

i=1
(x∗
vi + x∗
vi+1 −bvivi+1) + x∗
vn
gives the equality s∗(P) + x∗
vn = 2x∗(V (P) ∩L) + x∗(V (P) ∩I) −b(P), hence
2x∗(V (P) ∩L) + x∗(V (P) ∩I) ≥b(P) + 1
2 if and only if s∗(P) + x∗
vn ≥1
2.
This completes the proof of the Claim.
Therefore, if we assign length s∗
e to every e ∈E, we need to give an algorithm
that, for any two nodes r, t such that r ∈I, either determines that the shortest
odd I-path between r and t (if any) has length at least 1
2 −x∗({t} ∩L), or returns
an odd I-path P for which 2x∗(V (P) ∩L) + x∗(V (P) ∩I) < b(P) + 1
2.
Observe that any walk W between r and t that contains an odd number of edges
e such that be = 1
2 mod 1 either contains a sub-path P that is an odd I-path or it
contains a cycle C that contains an odd number of edges e such that be = 1
2 mod 1.
In the former case, either both endnodes of P are in I, or t is the only endnode of
P in L. Hence, if s∗(W) < 1
2 −x∗({t} ∩L), then also s∗(P) < 1
2 −x∗({t} ∩L),
hence 2x∗(V (P) ∩L) + x∗(V (P) ∩I) < b(P) + 1
2. In the second case, since G is
bipartite, the edges of C can be partitioned into two matchings M0 and M1 such
that b(M0) ≥b(M1) + 1
2. Thus s∗(C) = 
ij∈C(x∗
i + x∗
j −bij) = 2x∗(V (C)) −
b(C) ≥2(x∗(V (C)) −b(M0)) + 1
2 = 2 
ij∈M0(x∗
i + x∗
j −bij) + 1
2 ≥1
2, hence
s∗(W) ≥1
2.
Thus we only need to ﬁnd, for every pair r, t ∈U ∪V with r ∈I, the shortest
walk W between r and t, w.r.t. the distance s∗, among all such walks containing
an odd number of edges e such that be =
1
2 mod 1. If, for a given choice of r, t,

336
M. Conforti, B. Gerards, and G. Zambelli
s(W) < 1
2−x∗({t}∩L), then by the above argument we can ﬁnd in polynomial time
a sub-path P of W such that P is an odd I-path and 2x∗(V (P)∩L)+x∗(V (P)∩I) <
b(P) + 1
2, otherwise we can conclude that x∗∈conv(X(G, b, I)).
To conclude, we only need to show a polynomial-time algorithm that, given an
undirected graph Γ with nonnegative lengths on the edges ℓe, e ∈E(Γ), a subset
F ⊆E(Γ), and a pair of nodes r, t ∈V (Γ), determines the walk W of minimum
length between r and t such that |E(W) ∩F| is odd, or determines that no such
walk exists. The latter problem can be solved in polynomial time. Since, as far as
we know, this fact is folklore, we brieﬂy describe an algorithm.
We construct a new graph Γ ′ as follows. For every node v ∈V (Γ), there is a
pair of nodes v, v′ in V (Γ ′). For every edge uv ∈E(Γ), E(Γ ′) contains the edges
uv′ and u′v if uv ∈F, and the edges uv and u′v′ if uv /∈F, each with length ℓuv.
One can verify that a walk W between r and t with an odd number of edges in F
exists in Γ if and only if there exists a walk of the same length between r and t′ in
Γ ′. Hence we only need to ﬁnd a shortest path between r and t′ in Γ ′, if any exists,
and output the corresponding walk in Γ.
References
1. M. Conforti, M. Di Summa, F. Eisenbrand, L.A. Wolsey, Network formulations of
mixed-integer programs, In preparation, 2006.
2. J. Edmonds and E.L. Johnson, Matching: a well-solved class of integer linear pro-
grams, Combinatorial Structures and Their Applications (R.K. Guy, et al., eds.),
Gordon and Breach, New York, 1970, 89-92.
3. J. Edmonds and E.L. Johnson, Matching, Euler tours and the Chinese postman,
Mathematical Programming 5 (1973), 88-124.
4. F. Eisenbrand, Mixed Integer Programming over TU systems, Manuscript, 2006.
5. O. G¨unl¨uk and Y. Pochet, Mixing mixed integer inequalities, Mathematical Pro-
gramming 90 (2001), 429-457 .
6. A. Miller and L.A. Wolsey, Tight formulations for some simple MIPs and convex
objective IPs, Mathematical Programming B 98 (2003), 73–88.
7. G.L. Nemhauser, L.A. Wolsey, Integer and Combinatorial Optimization, Wiley In-
terscience, New York, 1988.
8. Y. Pochet and L.A. Wolsey, Polyhedra for lot-sizing with Wagner-Whitin costs,
Mathematical Programming 67 (1994), 297–324.
9. Y. Pochet and L.A. Wolsey, Production Planning by Mixed Integer Programming,
Springer Series in Operations Research and Financial Engineering, New York, 2006.
10. A. Schrijver, Theory of Linear and Integer Programming, Wiley, New York, 1986.
11. M. Skutella, Mixed Integer vertex cover on paths, Manuscript, 2005.

On the MIR Closure of Polyhedra
Sanjeeb Dash1, Oktay G¨unl¨uk2, and Andrea Lodi3
1 IBM T.J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598
sanjeebd@us.ibm.com
2 IBM T.J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598
gunluk@us.ibm.com
3 DEIS, University of Bologna, viale Risorgimento 2 - 40136 Bologna - Italy
alodi@deis.unibo.it
Abstract. We study the mixed-integer rounding (MIR) closure of poly-
hedra. The MIR closure of a polyhedron is equal to its split closure and
the associated separation problem is NP-hard. We describe a mixed-
integer programming (MIP) model with linear constraints and a non-
linear objective for separating an arbitrary point from the MIR closure
of a given mixed-integer set. We linearize the objective using additional
variables to produce a linear MIP model that solves the separation prob-
lem approximately, with an accuracy that depends on the number of
additional variables used. Our analysis yields a short proof of the result
of Cook, Kannan and Schrijver (1990) that the split closure of a polyhe-
dron is again a polyhedron. We also present some computational results
with our approximate separation model.
1
Introduction
We study the mixed-integer rounding (MIR) closure of a given mixed-integer set
P = {v ∈R|J|, x ∈Z|I| : Cv + Ax ≥b, v, x ≥0}
where all numerical data is rational. In other words, we are interested in the set
of points that satisfy all MIR inequalities
(λC)+v + (−λ)+(Cv + Ax −b) + min{λA −⌊λA⌋, r}x + r ⌊λA⌋x ≥r ⌈λb⌉
that can be generated by some λ of appropriate dimension. Here r = λb −⌊λb⌋,
(·)+ denotes max{0, ·} and all operators are applied to vectors component-wise.
In Section 2, we discuss in detail how these inequalities are derived and why
they are called MIR inequalities.
The term mixed-integer rounding was ﬁrst used by Nemhauser and Wolsey
[18, pp.244] to denote valid inequalities that can be produced by what they
call the MIR procedure. These authors in [17] strengthen and redeﬁne the MIR
procedure and the resulting inequality. The same term was later used to denote
seemingly simpler inequalities in Marchand and Wolsey [16], and Wolsey [20].
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 337–351, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

338
S. Dash, O. G¨unl¨uk, and A. Lodi
The deﬁnition of the MIR inequality we use in this paper is equivalent to the
one in [17], though our presentation is based on [20].
Split cuts were deﬁned by Cook, Kannan and Schrijver in [9], and are a special
case of the disjunctive cuts introduced by Balas [2]. In [17], Nemhauser and
Wolsey show that MIR cuts are equivalent to split cuts in the sense that, for a
given polyhedron, the MIR closure is identical to the split closure. In [9], Cook,
Kannan and Schrijver show that the split closure of a polyhedron is again a
polyhedron. In this paper, we present a short proof of the same fact by analyzing
MIR closure of polyhedra. This is not a new result but our proof is signiﬁcantly
easier to follow and present.
The problem of separating an arbitrary point from the MIR closure of a poly-
hedron is NP-hard as it was shown (using split cuts) by Caprara and Letchford
[7]. The same also holds for the (Gomory-)Chv´atal closure of a polyhedron as
shown by Eisenbrand [13]. Gomory-Chv´atal cuts are dominated by MIR cuts
and therefore Chv´atal closure is contained in the MIR closure, usually strictly.
In [15], Fischetti and Lodi show that, even though it is theoretically hard, in
practice it is possible to separate points from the Chv´atal closure in a reasonable
time. Their approach involves formulating the separation problem as an MIP, and
solving it with a black-box MIP solver. By repeatedly applying their separation
algorithm to MIPLIB instances, they are able to approximately optimize over the
Chv´atal closure and obtain very tight bounds on the value of optimal solutions.
Motivated by their work, we describe an MIP model for separating from the MIR
closure of a polyhedron and present computational results on approximately
optimizing over the MIR closure for problems in the MIPLIB 3.0.
Our work is also closely related with two recent papers written independently.
The ﬁrst one is a paper [4] by Balas and Saxena who experiment with a paramet-
ric MIP model to ﬁnd violated split cuts. The second one is the paper by Vielma
[19] which presents a proof of the fact that the split closure of a polyhedron is
again a polyhedron.
The paper is organized as follows: In Section 2, we deﬁne MIR inequalities
and their basic properties. In Section 3 we present a mixed-integer programming
model that approximately separates an arbitrary point from the MIR closure of
a given polyhedron. In Section 4, we present a simple proof that the MIR (or,
split) closure of a polyhedron is again a polyhedron. In Section 5 we present a
summary of the experiments with the approximate separation model.
2
Mixed-Integer Rounding Inequalities
In [20], Wolsey develops the MIR inequality as the only non-trivial facet of the
following simple mixed-integer set:
Q0 =

v ∈R, x ∈Z : v + x ≥b,
v ≥0

.
It is easy to see that
v ≥ˆb(⌈b⌉−x)
(1)

On the MIR Closure of Polyhedra
339
where ˆb = b −⌊b⌋is valid and facet deﬁning for Q0. In [20] this inequality is
called the basic mixed-integer inequality.
To apply this idea to more general sets deﬁned by a single inequality, one
needs to group variables in a way that resembles Q0. More precisely, given a
set
Q1 =

v ∈R|J|, x ∈Z|I| :

j∈J
cjvj +

i∈I
aixi ≥b,
v, x ≥0

the deﬁning inequality is relaxed to obtain
 
j∈J
max{0, cj}vj +

i∈I′
ˆaixi

+
 
i∈I\I′
xi +

i∈I
⌊ai⌋xi

≥b
where ˆai = ai −⌊ai⌋and I′ ⊆I. As the ﬁrst part of the left hand side of this
inequality is non-negative, and the second part is integral, the MIR inequality

j∈J
max{0, cj}vj +

i∈I′
ˆaixi ≥ˆb

⌈b⌉−

i∈I\I′
xi −

i∈I
⌊ai⌋xi

is valid for Q1. Notice that I′ = {i ∈I : ˆai < ˆb} gives the strongest inequality
of this form and therefore the MIR inequality can also be written as

j∈J
(cj)+vj +

i∈I
min{ˆai,ˆb}xi + ˆb

i∈I
⌊ai⌋xi ≥ˆb⌈b⌉.
(2)
To apply this idea to sets deﬁned by m > 1 inequalities, the ﬁrst step is to
combine them to obtain a single base inequality and then apply inequality (2).
Let
P =

v ∈Rl, x ∈Zn : Cv + Ax ≥b, v, x ≥0

be a mixed-integer set where C, A and b are vectors of appropriate dimension.
To obtain the base inequality, one possibility is to use a vector λ ∈Rm, λ ≥0 to
combine the inequalities deﬁning P. This approach leads to the base inequality
λCv + λAx ≥λb and the corresponding MIR inequality
(λC)+v + min{λA −⌊λA⌋, r}x + r ⌊λA⌋x ≥r ⌈λb⌉,
(3)
where operators (·)+, ⌊·⌋and min{·, ·} are applied to vectors component-wise,
and r = λb −⌊λb⌋.
Alternatively, it is also possible to ﬁrst introduce slack variables to the set of
inequalities deﬁning P and combine them using a vector λ which is not neces-
sarily non-negative. This gives the base inequality λCv + λAx −λs = λb and
the corresponding MIR inequality
(λC)+v + (−λ)+s + min{λA −⌊λA⌋, r}x + r ⌊λA⌋x ≥r ⌈λb⌉,
(4)
where s denotes the (non-negative) slack variables. Finally, substituting out the
slack variables gives the following MIR inequality in the original space of P:
(λC)+v + (−λ)+(Cv + Ax −b) + min{λA −⌊λA⌋, r}x + r ⌊λA⌋x ≥r ⌈λb⌉. (5)

340
S. Dash, O. G¨unl¨uk, and A. Lodi
These inequalities are what we call MIR inequalities in this paper.
Notice that when λ ≥0, inequality (5) reduces to inequality (3). When λ ̸≥0,
however, there are inequalities (5) which cannot be written in the form (3). We
present an example to emphasize this point (a similar one was independently
developed in [5]).
Example 1. Consider the simple mixed-integer set T = {v ∈R, x ∈Z
:
−v−4x ≥−4, −v+4x ≥0, v, x ≥0} and the base inequality generated by λ =
[−1/8, 1/8] x + s1/8 −s2/8 ≥1/2 where s1 and s2 denote the slack variables for
the ﬁrst and second constraint, respectively. The corresponding MIR inequality is
1/2x + s1/8 ≥1/2, which after substituting out s1, becomes −v/8 ≥0 or simply
v ≤0. This inequality deﬁnes the only non-trivial facet of T .
Notice that it is not possible to generate this inequality using non-negative
multipliers. Any base inequality generated by λ1, λ2 ≥0 has the form (−λ1 −
λ2)v +(−4λ1 +4λ2)x ≥−4λ1 where variable v has a negative coeﬃcient. There-
fore, the MIR inequality generated by this base inequality would have a coeﬃcient
of zero for the v variable, establishing that v ≤0 cannot be generated as an MIR
inequality (3).
2.1
Basic Properties of MIR Inequalities
Let P LP denote the continuous relaxation of P. A linear inequality hv + gx ≥d
is called a split cut for P if it is valid for both P LP ∩{¯αx ≤¯β} and P LP ∩{¯αx ≥
¯β + 1}, where ¯α and ¯β are integral. Inequality hv + gx ≥d is said to be derived
from the disjunction ¯αx ≤¯β and ¯αx ≥¯β + 1. Obviously all points in P satisfy
any split cut for P. Note that multiple split cuts can be derived from the same
disjunction.
The basic MIR inequality (1) is a split cut for Q0 with respect to x derived
from the disjunction x ≤⌊b⌋and x ≥⌊b⌋+ 1. Therefore, the MIR inequality (5)
is also a split cut for P derived from the disjunction ¯αx ≤¯β and ¯αx ≥¯β + 1
where ¯β = ⌊λb⌋and
¯αi =

⌈(λA)i⌉if (λA)i −⌊(λA)i⌋≥λb −⌊λb⌋
⌊(λA)i⌋otherwise.
We note that this observation also implies that if a point (v∗, x∗) ∈P LP violates
the MIR inequality (5) then ¯β + 1 > ¯αx∗> ¯β.
Furthermore, Nemhauser and Wolsey [17] showed that every split cut for P can
be derived as an MIR cut for P. As what we call MIR inequalities in this paper
are equivalent to the MIR inequalities deﬁned in [17], the same observation holds
for the MIR inequalities written in the form of inequality (5). We next formally
deﬁne the MIR closure of polyhedra.
Deﬁnition 2. The MIR closure of a polyhedron is the set of points satisfying
all MIR inequality (5) that can be generated by some multiplier vector λ ∈Rm.
Thus, the split closure of a polyhedron is the same as its MIR closure. We
next show that in certain cases, the closure of a polyhedron is invariant to
reformulation.

On the MIR Closure of Polyhedra
341
3
The Separation Problem
In this section, we study the problem of separating an arbitrary point from the
MIR closure of the polyhedron P = {v ∈Rl, x ∈Zn : Cv + Ax ≥b, v, x ≥0}.
In other words, for a given point, we are interested in either ﬁnding violated
inequalities or concluding that none exists. For convenience of notation, we ﬁrst
argue that without loss of generality we can assume P is given in equality form.
Consider the MIR inequality (4) for P,
(λC)+v + (−λ)+s + min{λA −⌊λA⌋, r}x + r ⌊λA⌋x ≥r ⌈λb⌉,
where s denotes the slack expression (Cv + Ax −b). If we explicitly deﬁne the
slack variables, by letting ˜C = (C, −I) and ˜v = (v, s), then the constraints
deﬁning P become ˜C˜v + Ax = b, ˜v ≥0, x ≥0, and the MIR inequality can be
written as
(λ ˜C)+˜v + min{λA −⌊λA⌋, r}x + r ⌊λA⌋x ≥r ⌈λb⌉.
(6)
In other words, all continuous variables, whether slack or structural, can be
treated uniformly. In the remainder we assume that P is given in the equality
form P = {v ∈Rl, x ∈Zn : Cv+Ax = b, v, x ≥0}, and we denote its continuous
relaxation by P LP .
3.1
Relaxed MIR Inequalities
Let
Π =

(λ, c+, ˆα, ¯α, ˆβ, ¯β) ∈Rm × Rl × Rn × Zn × R × Z :
c+ ≥λC, ˆα + ¯α ≥λA, ˆβ + ¯β ≤λb, c+ ≥0, 1 ≥ˆα ≥0, 1 ≥ˆβ ≥0

.
Note that for any (λ, c+, ˆα, ¯α, ˆβ, ¯β) ∈Π,
c+v + (ˆα + ¯α)x ≥ˆβ + ¯β
(7)
is valid for P LP as it is a relaxation of (λC)v + (λA)x = λb. Furthermore, using
the basic mixed-integer inequality (1), we infer that
c+v + ˆαx + ˆβ ¯αx ≥ˆβ(¯β + 1)
(8)
is a valid inequality for P. We call inequality (8) where (λ, c+, ˆα, ¯α, ˆβ, ¯β) ∈Π a
relaxed MIR inequality derived using the base inequality (7). We next show some
basic properties of relaxed MIR inequalities.
Lemma 3. A relaxed MIR inequality (8) violated by (v∗, x∗) ∈P LP satisﬁes
(i)1 > ˆβ > 0, (ii)1 > Δ > 0, (iii) the violation of the inequality is at most
ˆβ(1 −ˆβ) ≤1/4, where Δ = ¯β + 1 −¯αx∗and violation is deﬁned to be the right
hand side of inequality (8) minus its left hand side.

342
S. Dash, O. G¨unl¨uk, and A. Lodi
Proof: If ˆβ = 0, then the relaxed MIR is trivially satisﬁed by all points in P LP .
Furthermore, if ˆβ = 1, then inequality (8) is identical to its base inequality (7)
which again is satisﬁed by all points in P LP. Therefore, a non-trivial relaxed
MIR cut satisﬁes 1 > ˆβ > 0.
For part (ii) of the Lemma, note that if ¯αx∗≥¯β + 1 then inequality (8)
is satisﬁed, as c+, ˆα, ˆβ ≥0 and (v∗, x∗) ≥0. Furthermore, if (v∗, x∗) satisﬁes
inequality (7) and ¯αx∗≤¯β, then so is inequality (8) as ˆβ ≤1. Therefore, as the
cut is violated, 1 > Δ > 0. It is also possible to show this by observing that
inequality (8) is a split cut for P derived from the disjunction Δ ≥1 and Δ ≤0.
For the last part, let w = c+v∗+ ˆαx∗so that the base inequality (7) becomes
w ≥ˆβ + Δ −1 and the relaxed MIR inequality (8) becomes w ≥ˆβΔ. Clearly
ˆβΔ −w ≤ˆβ(w + 1 −ˆβ) −w = ˆβ(1 −ˆβ) −(1 −ˆβ)w ≤ˆβ(1 −ˆβ), and the last
inequality follows from the fact that w ≥0 and ˆβ ≤1.
⊓⊔
Next, we relate MIR inequalities to relaxed MIR inequalities.
Lemma 4. For any λ ∈Rm, the MIR inequality (6) is a relaxed MIR inequality.
Proof: For a given multiplier vector λ, deﬁne α to denote λA. Further, set
c+ = (λC)+, ¯β = ⌈λb⌉and ˆβ = λb −⌊λb⌋. Also, deﬁne ˆα and ¯α as follows:
ˆαi =

αi −⌊αi⌋if αi −⌊αi⌋< ˆβ
0
otherwise
,
¯αi =

⌊αi⌋if αi −⌊αi⌋< ˆβ
⌈αi⌉otherwise
,
Clearly, (λ, c+, ˆα, ¯α, ˆβ, ¯β) ∈Π and the corresponding relaxed MIR inequality (8)
is the same as the MIR inequality (6).
⊓⊔
Lemma 5. MIR inequalities dominate relaxed MIR inequalities.
Proof: Let (v∗, x∗) ∈P LP violate a relaxed MIR inequality I which is obtained
with (λ, c+, ˆα, ¯α, ˆβ, ¯β) ∈Π. We will show that (v∗, x∗) also violates the MIR
inequality (6).
Due to Lemma 3, we have ¯β + 1 −¯αx∗> 0 and therefore increasing ˆβ only
increases the violation of the relaxed MIR inequality. Assuming I is the most
violated relaxed MIR inequality, ˆβ = min{λb −¯β, 1}. By Lemma 3, we know
that ˆβ < 1, and therefore ˆβ = λb −¯β and ¯β = ⌊λb⌋.
In addition, due to the deﬁnition of Π we have c+ ≥(λC)+ and ˆα + ˆβ ¯α ≥
min{λA−⌊λA⌋, ˆβ}+ ˆβ ⌊λA⌋. As (v∗, x∗) ≥0, the violation of the MIR inequality
is at least as much as the violation of I.
⊓⊔
Combining Lemmas 4 and 5, we observe that a point in P LP satisﬁes all MIR
inequalities, if and only if it satisﬁes all relaxed MIR inequalities. Therefore, we
can deﬁne the MIR closure of polyhedra using relaxed MIR inequalities and thus
without using operators that take minimums, maximums or extract fractional
parts of numbers. Let ¯Π be the projection of Π in the space of c+, ˆα, ¯α, ˆβ and
¯β variables. In other words, ¯Π is obtained by projecting out the λ variables. We
now describe the MIR closure of P as:

On the MIR Closure of Polyhedra
343
P MIR =

(v, x) ∈P LP : c+v+ˆαx+ ˆβ ¯αx ≥ˆβ(¯β+1) for all (c+, ˆα, ¯α, ˆβ, ¯β) ∈¯Π

.
We would like to emphasize that ¯Π is not the polar of P MIR and therefore
even though ¯Π is a polyhedral set (with a ﬁnite number of extreme points and
extreme directions), we have not yet shown that the polar of P MIR is polyhedral.
The polar of a polyhedral set is deﬁned to be the set of points that yield valid
inequalities for the original set. If the original set is deﬁned in Rn, its polar
is deﬁned in Rn+1 and the ﬁrst n coordinates of any point in the polar give
the coeﬃcients of a valid inequality for the original set, and the last coordinate
gives the right hand side of the valid inequality. Therefore, polar of P MIR is the
collection of points (c+, ˆα + ˆβ¯α, ˆβ(¯β + 1)) ∈Rl+n+1 where (c+, ˆα, ¯α, ˆβ, ¯β) ∈¯Π.
A set is polyhedral if and only if its polar is polyhedral.
For a given point (v∗, x∗) ∈P LP, testing if (v∗, x∗) ∈P MIR can be achieved
by solving the following non-linear integer program (MIR-SEP):
max

ˆβ(¯β + 1) −(c+v∗+ ˆαx∗+ ˆβ¯αx∗) : (c+, ˆα, ¯α, ˆβ, ¯β) ∈
¯Π

.
If the optimal value of this program is non-positive, then (v∗, x∗) ∈P MIR.
Otherwise, if the optimal value is positive, the optimal solution gives a most
violated MIR inequality.
3.2
An Approximate Separation Model
We next (approximately) linearize the nonlinear terms that appear in the ob-
jective function of MIR-SEP. To this end, we ﬁrst deﬁne a new variable Δ that
stands for the term (¯β + 1 −¯αx). We then approximate ˆβ by a number ˜β ≤ˆβ
representable over some E = {ϵk : k ∈K}. We say that a number δ is repre-
sentable over E if δ = 
k∈¯
K ϵk for some ¯K ⊆K. We can therefore write ˜β as

k∈K ϵkπk using binary variables πk and approximate ˆβΔ by ˜βΔ which can
now be written as 
k∈K ϵkπkΔ. Finally, we linearize terms πkΔ using standard
techniques as πk is binary and Δ ∈(0, 1) for any violated inequality.
An approximate MIP model APPX-MIR-SEP reads as follows:
max

k∈K
ϵkΔk −(c+v∗+ ˆαx∗)
(9)
s.t.
(c+, ˆα, ¯α, ˆβ, ¯β) ∈¯Π
(10)
ˆβ ≥

k∈K
ϵkπk
(11)
Δ = (¯β + 1) −¯αx∗
(12)
Δk ≤Δ
∀k ∈K
(13)
Δk ≤πk
∀k ∈K
(14)
λ ∈Rm,
π ∈{0, 1}|K|
(15)

344
S. Dash, O. G¨unl¨uk, and A. Lodi
In our experiments, we use E = {2−k : k = 1, . . . , ¯k} for some small number
¯k. With this choice of E, notice that for any ˆβ there exists a ˜β representable over
E such that 2−¯k ≥ˆβ −˜β ≥0. This observation is used to bound the error of the
approximate model.
Theorem 6. Let E = {2−k : k = 1, . . . , ¯k} for some positive integer ¯k and de-
note the optimal values of MIR-SEP and APPX-MIR-SEP by zsep and zapx−sep,
respectively. Then,
zsep ≥zapx−sep > zsep −2−¯k.
(16)
Proof: By (10), any (integral) feasible solution of APPX-MIR-SEP yields a
feasible solution of MIR-SEP. Further, deﬁne ˜β to be 
k∈K ϵkπk. As Δk ≤
Δπk for all k ∈K, 
k∈K ϵkΔk
≤

k∈K ϵkπkΔ = ˜βΔ
≤
ˆβΔ. Therefore,
zsep ≥zapx−sep.
Note that zapx−sep ≥0 as we can get a feasible solution of APPX-MIR-SEP
with objective 0 by setting Δ to 1, and the remaining variables to 0. Therefore
the second inequality in (16) holds if zsep ≤0. Assume that zsep > 0. Let
(c+, ˆα, ¯α, ˆβ, ¯β) ∈¯Π be an optimal solution of MIR-SEP. For the variables in
APPX-MIR-SEP common with MIR-SEP, set their values to the above optimal
solution of MIR-SEP. Let ˜β be the largest number representable over E less
than or equal to ˆβ. Clearly, 2−¯k ≥ˆβ −˜β ≥0. Choose π ∈{0, 1}¯k such that
˜β = 
k∈K ϵkπk. Set Δ = ¯β + 1 −¯αx∗. Set Δk = 0 if πk = 0, and Δk = Δ
if πk = 1. Then Δk = πkΔ for all k ∈K, and ˜βΔ = 
k∈K ϵkΔk. Therefore,
2−¯k > 2−¯kΔ ≥ˆβΔ −˜βΔ = ˆβΔ −
k∈K ϵkΔk. The second inequality in (16)
follows.
⊓⊔
The previous result says that a solution of APPX-MIR-SEP with positive ob-
jective value yields a violated MIR cut, and if there is an MIR cut with a “large
enough” violation, we will ﬁnd some violated MIR cut by solving APPX-MIR-
SEP.
In the next section (Theorem 12) we show that APPX-MIR-SEP becomes an
exact model for ﬁnding violated MIR cuts when E is chosen as {ϵk = 2k/Φ, ∀k =
{1, . . ., ⌈logΦ⌉}} where Φ is the least common multiple of all subdeterminants of
A|C|b.
4
A Simple Proof That the MIR Closure Is a Polyhedron
In this section we give a short proof that the MIR closure of a polyhedron
is a polyhedron. As MIR cuts are equivalent to split cuts, this result obviously
follows from the work of Cook, Kannan and Schrijver [9] on split cuts. Andersen,
Cornu´ejols and Li [1], and Vielma [19] give alternative proofs that the split
closure of a polyhedron is a polyhedron.
The main tool in the proof is a ﬁnite bound on the multipliers λ needed for
non-redundant MIR cuts given in Lemma 9 . The bounds on λ can be tightened
if the MIP is a pure integer program, and we give these tighter bounds in the
next lemma whose proof is omitted due to the lack of space.

On the MIR Closure of Polyhedra
345
In this section we assume that the coeﬃcients in Cv + Ax = b are integers.
Denote the ith equation of Ax+Cv = b by civ+aix = bi. An equation civ+aix =
bi is a pure integer equation if ci = 0.
Lemma 7. If some MIR inequality is violated by the point (v∗, x∗), then there
is another MIR inequality violated by (v∗, x∗) derived using λi ∈[0, 1) for every
pure integer equation.
Deﬁnition 8. We deﬁne Ψ to be the largest absolute value of subdeterminants
of C, and 1 if C = 0, where m is the number of rows in Ax + Cv = b.
Lemma 9. If there is an MIR inequality violated by the point (v∗, x∗), then there
is another MIR inequality violated by (v∗, x∗) with λi ∈(−mΨ, mΨ), where m is
the number of rows in Ax + Cv = b.
Proof: Let the MIR cut (λC)+v + ˆαx + ˆβ¯αx ≥ˆβ(¯β + 1) be violated by (v∗, x∗).
Then (λ, (λC)+, ˆα, ¯α, ˆβ, ¯β) ∈Π with 0 < ˆβ < 1. Let Cj stand for the jth column
of C. Let S1 = {j : λCj > 0} and S2 = {j : λCj ≤0}.
Consider the cone C = {v ∈Rm : vCi ≤0
∀i ∈S1,
vCi ≥0
∀i ∈S2}.
Obviously λ belongs to C. We will ﬁnd a vector λ′ in C, such that ¯λ = λ −λ′ is
integral and belongs to C. C is a polyhedral cone, and is generated by a ﬁnite set
of vectors μ1, . . . , μt, for some t > 0. (Observe that if C = 0, then C = Rm, and
μ1, . . . , μt can be chosen to be the unit vectors times ±1.) We can assume these
vectors are integral (by scaling); we can also assume the coeﬃcients of μ1, . . . , μt
have absolute value at most Ψ. Further, we can assume that μ1, . . . , μk (here
k ≤m) are linearly independent vectors such that λ = k
j=1 vjμj, with vj ∈
R, vj > 0. If vj < 1 for j = 1, . . . , k, then each coeﬃcient of λ has absolute value
less than mΨ, and there is nothing to prove. If vj ≥1 for any j ∈{1, . . . , k}, then
let λ′ = k
j=1 ˆvjμj ⇒λ −λ′ = k
j=1⌊vj⌋μj, where ˆvj = vj −⌊vj⌋. Clearly λ′
belongs to C, and has coeﬃcients with absolute value at most mΨ. Also, λ′ ̸= 0
as λ′ = 0 ⇒λ is integral ⇒ˆβ = 0. Let ¯λ = λ −λ′; obviously ¯λ belongs to C
and is integral. Further, (λC)+ −(λ′C)+ = (¯λC)+. Therefore (λ′, (λ′C)+, ˆα, ¯α−
¯λA, ˆβ, ¯β −¯λb) ∈Π. It follows that the multipliers λ′ lead to the MIR
(λ′C)+v + ˆαx + ˆβ(¯α −¯λA)x ≥ˆβ(¯β −¯λb + 1).
(17)
The rhs of the old MIR minus the rhs of the new MIR equals
ˆβ¯λb = ˆβ¯λ(Ax∗+ Cv∗) = ˆβ¯λAx∗+ ˆβ¯λCv∗
≤ˆβ¯λAx∗+ ˆβ(¯λC)+v∗.
(18)
The lhs of the old MIR (with v∗, x∗substituted) minus the lhs of the new MIR
equals the last term in (18). Therefore the new MIR is violated by at least as
much as the old MIR.
⊓⊔
Theorem 10. If there is an MIR inequality violated by the point (v∗, x∗), then
there is another MIR inequality violated by (v∗, x∗) for which ˆβ and the compo-
nents of λ, ˆα are rational numbers with denominator equal to a subdeterminant
of A|C|b, and each component of λ is contained in the interval [−mΨ, mΨ].

346
S. Dash, O. G¨unl¨uk, and A. Lodi
Proof: Let (v∗, x∗) be a point in P LP which violates an MIR cut. Let this MIR
cut be deﬁned by (λo, c+
o , ˆαo, ¯αo, ˆβo, ¯βo) ∈Π. By Lemma 9 , we can assume each
component of λo lies in the range (−mΨ, mΨ). Deﬁne Δo = ¯βo +1−¯αT
o x∗. Then
ˆβoΔo −c+
o v∗−ˆαox∗> 0. Consider the following LP:
max

ˆβΔo −c+v∗−ˆαx∗: (λ, c+, ˆα, ¯αo, ˆβ, ¯βo) ∈Π, −mΨ ≤λi ≤mΨ

.
Note that the objective is a linear function as Δo is ﬁxed. Further, we have
ﬁxed the variables ¯α and ¯β in the constraints deﬁning Π. The bounds on λ
come from Lemma 9, except that we weaken them to non-strict inequalities.
This LP has at least one solution for (λ, c+, ˆα, ˆβ) with positive objective value,
namely (λo, c+
o , ˆαo, ˆβo). Therefore a basic optimal solution of this LP has positive
objective value. Consider the MIR cut deﬁned by an optimal solution along with
¯αo and ¯βo. It is obviously an MIR cut with violation at least the violation of the
original MIR cut. Therefore, 0 < ˆβ < 1. Further, it is easy to see that the LP
constraints (other than the bounds on the variables) can be written as
⎡
⎣
AT −I
CT
−I
bT
−1
⎤
⎦
⎛
⎜
⎜
⎝
λ
ˆα
c+
ˆβ
⎞
⎟
⎟
⎠
≤
≤
≥
⎛
⎝
¯αo
0
¯βo
⎞
⎠.
The theorem follows.
⊓⊔
Corollary 11 The MIR closure of a polyhedron P is a polyhedron.
Proof: By Theorem 10, each non-redundant MIR inequality is deﬁned by λ =
(λi) where λi is a rational number in [−mΨ, mΨ] with a denominator equal to a
subdeterminant of A|C|b. Thus, the number of non-redundant MIR inequalities
is ﬁnite.
⊓⊔
As the MIR closure equals the split closure, it follows that the split closure of a
polyhedron is again a polyhedron. Let the split closure of P be denoted by PS.
For integral c and d, deﬁne P(c,d) by
P(c,d) = conv{(P ∩{cx ≤d}) ∪(P ∩{cx ≥d + 1})} ⇒PS = ∩c∈Zn,d∈ZP(c,d),
where x has n components. Lemma 9
gives a characterization of the useful
disjunctions in the deﬁnition of the split closure. Deﬁne the vector μ ∈Rm by
μi =

mΨ if ci ̸= 0
1
if ci = 0
Deﬁne D = {(c, d) ∈Zn × Z : −μ|A| ≤c ≤μ|A|, ⌊−μ|b|⌋≤d ≤⌊μ|b|⌋}, where
D is clearly a ﬁnite set, and PS = ∩c∈Zn,d∈ZP(c,d) = ∩(c,d)∈DP(c,d). To see this,
let x∗be a point in P but not in PS. Then some split cut, which is also an MIR
cut, is violated by x∗. By Lemma 9 , there is an MIR cut with −μ < λ < μ
which is violated by x∗. This MIR cut has the form (λC)+v + ˆαx + ˆβ¯αx ≥

On the MIR Closure of Polyhedra
347
ˆβ(¯β +1), where (¯α, ¯β) ∈D. Thus x∗does not belong to P(¯α, ¯β). This implies that
∩(c,d)∈DP(c,d) ⊆∩c∈Zn,d∈ZP(c,d), and the two sets in the expression above are
equal as the reverse inclusion is true by deﬁnition.
Theorem 12. Let Φ be the least common multiple of all subdeterminants of
A|C|b, K = {1, . . ., logΦ}, and E = {ϵk = 2k/Φ, ∀k ∈K}. Then APPX-MIR-
SEP is an exact model for ﬁnding violated MIR cuts.
Proof: By Theorem 10, ˆβ in a violated MIR cut can be assumed to be a rational
number with a denominator equal to a subdeterminant of A|C|b and therefore
of Φ. But such a ˆβ is representable over E.
⊓⊔
5
Computational Experiments
In this section we brieﬂy discuss our computational experience with the approx-
imate separation model MIR-SEP . The goal is to approximately optimize over
the MIR closure of a given MIP instance by repeatedly solving APPX-MIR-SEP
to get violated MIR cuts. The general idea is to start oﬀwith the continuous
relaxation of the given MIP. Then the following separation step is repeated.
APPX-MIR-SEP is solved to ﬁnd one or more MIR inequalities violated by the
optimal solution of the current relaxation of the MIP, and the current relaxation
is strengthened by adding these cuts. Even though this procedure is guaranteed
to terminate after a ﬁnite number of iterations (for any ﬁxed precision), in prac-
tice, there is no guarantee that we can actually optimize over the (approximate)
MIR closure in a reasonable amount of time. Our approach, therefore, should be
considered as a heuristic that tries to ﬁnd good bounds in a reasonable amount
of time.
We next sketch some practical issues and heuristic ideas to obtain good bounds
faster.
1. Numerical Issues. A major issue is that the point (v∗, x∗) to be separated
from the MIR closure of P is only approximately contained in P LP if it is
obtained using a practical LP solver. We deal with these numerical issues by
modifying (v∗, x∗) and b to get a truly feasible solution of a diﬀerent set of
constraints. We let v′ = max{v∗, 0}, and x′ = max{x∗, 0}, and then deﬁne
b′ as Cv′ + Ax′. We then use APPX-MIR-SEP to separate (v′, x′) from the
MIR closure of Cv + Ax = b′, v, x ≥0, x ∈Z. We use the multipliers λ in
the solution of APPX-MIR-SEP to compute an MIR cut for P. Of course,
this cut may not be violated by (v∗, x∗), but mostly is, as the point (v′, x′)
is usually very close to (v∗, x∗).
2. Reducing the size of the separation problem. It is clear that in APPX-
MIR-SEP, the variables c+
i , ˆaj, ¯aj corresponding to v∗
i = 0 and x∗
j = 0 do
not contribute to the objective. For some of the problems in MIPLIB 3.0,
this approach is quite crucial in allowing us to use APPX-MIR-SEP at all.
For example, the MIP nw04 has 36 constraints, and over 87000 0-1 variables.
If we let ¯k = 5, the ﬁrst separation MIP has at most 36+5 integer variables.

348
S. Dash, O. G¨unl¨uk, and A. Lodi
3. Separation Heuristics. To speed up this process we implemented several
ideas which can essentially be seen as ﬁnding heuristic solutions to MIR-SEP.
(a) Preprocessing. We take a subset S of integer variables, and for every
xi with i ∈S, we solve LPs to maximize and minimize xi for x ∈P LP .
(b) Gomory mixed-integer cuts. Gomory mixed-integer cuts for the ini-
tial LP-relaxation of the MIP are known to be MIR inequalities [16]
where the multipliers used to aggregate the rows of the formulation are
obtained from the inverse of the optimal basis. Of course, we use these
cuts only in the ﬁrst iteration of the cutting plane algorithm to be sure
that they have rank 1.
(c) Cuts based on the rows of the formulation. Another heuristic con-
siders rows of the formulation, one at a time, and obtains base inequal-
ities by scaling them. Variables that have upper bounds are sometimes
complemented using the bound constraints. The procedure is in the spirit
of [12].
(d) Cuts based on pure integer base inequalities. One way to generate
eﬀective MIR cuts is to concentrate on base inequalities that only contain
integer variables. To obtain such base inequalities, the multiplier vector
λ, used to aggregate the rows of the formulation, is required to satisfy
λC ≤0 so that (λC)+ = 0. This can simply be achieved by ﬁxing
variables c+ to zero in MIR-SEP thus obtaining a model called INT-
SEP. This heuristic in a way mimics the procedure to generate the so-
called projected Chv´atal-Gomory (pro-CG) cuts [6] for mixed integer
programs.
(e) Cuts generated by MIR-SEP. The only parameter which must be
speciﬁed for the deﬁnition and solution of MIR-SEP is the value of ¯k,
i.e., the parameter responsible of the degree of approximation we use for
ˆβ. In all computational experiments, we do use ¯k = 6 which is a good
compromise between computational eﬃciency and precision. In such a
way, as proved by Theorem 6, our approximate model is guaranteed to
ﬁnd a cut violated by at least 1/64 = .015625 which can be considered
a reasonable threshold value to distinguish eﬀective violated cuts.
4. Piloting the black-box MIP solver. A few tricks in the line of what
already done in [6,15] can be used to force the black-box MIP solver, in our
experiments ILOG-Cplex 10.0.1, to return good heuristic solutions of both
INT-SEP and MIR-SEP. Indeed, it has to be stressed that we do not need
to solve any of the separation problems to optimality in our cutting plane
algorithm but, eventually, a ﬁnal MIR-SEP so as to prove that no MIR
inequality exists, i.e., the MIR closure has been computed.
The detailed computational results are reported in Tables 1 and 2 where the
bounds we obtain with a time limit of 1 hour of CPU time on a standard PC
are compared with those obtained in [4,6,15]. Our results conﬁrm what other
authors have already noticed, i.e., that those closures indeed provide a very tight
approximation of the optimal solution of the problems in the MIPLIB. Most
of the times we are able to compute bounds comparable with the ones already

On the MIR Closure of Polyhedra
349
Table 1. IPs of the MIPLIB 3.0
% gap
time
% CG gap
time
% gap
time
instance
|I|
# iter
# cuts
closed
MIR
closed
CG
split
split
air03
10,757
1
36
100.00
1
100.0
1
100.00
3
air04
8,904
5
294
9.18
3,600
30.4
43,200
91.23
864,360
air05
7,195
8
238
12.08
3,600
35.3
43,200
61.98
24,156
cap6000
6,000
120
334
50.55
3,600
22.5
43,200
65.17
1,260
fast0507
63,009
14
330
1.66
3,600
5.3
43,200
19.08
304,331
gt2
188
83
254
98.21
664
91.0
10,800
98.37
599
harp2
2,993
122
796
59.99
260
49.5
43,200
46.98
7,671
l152lav
1,989
57
214
12.66
3,600
59.6
10,800
95.20
496,652
lseu
89
103
306
92.28
3,600
93.3
175
93.75
32,281
mitre
10,724
12
158
100.00
380
16.2
10,800
100.00
5,330
mod008
319
41
173
100.00
11
100.0
12
99.98
85
mod010
2,655
1
39
100.00
0
100.0
1
100.00
264
nw04
87,482
100
301
95.16
3,600
100.0
509
100.00
996
p0033
33
27
115
87.42
2,179
85.3
16
87.42
429
p0201
201
394
1357
74.43
3,600
60.6
10,800
74.93
31,595
p0282
282
223
1474
99.60
3,600
99.9
10,800
99.99
58,052
p0548
548
255
1309
96.35
3,600
62.4
10,800
99.42
9,968
p2756
2,756
83
717
35.32
3,600
42.6
43,200
99.90
12,673
seymour
1,372
1
559
8.35
3,600
33.0
43,200
61.52
775,116
stein27
27
70
325
0.00
3,600
0.0
521
0.00
8,163
stein45
45
420
1930
0.00
3,600
0.0
10,800
0.00
27,624
Table 2. MILPs of the MIPLIB 3.0. For instance arki001 we used an upper bound of
value 7,580,813.0459.
% gap
time
% CG gap
time
% gap
time
instance
|I|
|J|
# iter
# cuts
closed
MIR
closed
CG
split
split
10teams
1,800
225
338
3341
100.00
3,600
57.14
1,200
100.00
90
arki001
538
850
14
124
33.93
3,600
28.04
1,200
83.05
193,536
bell3a
71
62
21
166
98.69
3,600
48.10
65
65.35
102
bell5
58
46
105
608
93.13
3,600
91.73
4
91.03
2,233
blend2
264
89
723
3991
32.18
3,600
36.40
1,200
46.52
552
dano3mip
552
13,321
1
124
0.10
3,600
0.00
1,200
0.22
73,835
danoint
56
465
501
2480
1.74
3,600
0.01
1,200
8.20
147,427
dcmulti
75
473
480
4527
98.53
3,600
47.25
1,200
100.00
2,154
egout
55
86
37
324
100.00
31
81.77
7
100.00
18,179
fiber
1,254
44
98
408
96.00
3,600
4.83
1,200
99.68
163,802
fixnet6
378
500
761
4927
94.47
3,600
67.51
43
99.75
19,577
flugpl
11
7
11
26
93.68
3,600
19.19
1,200
100.00
26
gen
150
720
11
127
100.00
16
86.60
1,200
100.00
46
gesa2
408
816
433
1594
99.81
3,600
94.84
1,200
99.02
22,808
gesa2 o
720
504
131
916
97.74
3,600
94.93
1,200
99.97
8,861
gesa3
384
768
464
1680
81.84
3,600
58.96
1,200
95.81
30,591
gesa3 o
672
480
344
1278
69.74
3,600
64.53
1,200
95.20
6,530
khb05250
24
1,326
65
521
100.00
113
4.70
3
100.00
33
markshare1
50
12
4781
90628
0.00
3,600
0.00
1,200
0.00
1,330
markshare2
60
14
4612
87613
0.00
3,600
0.00
1,200
0.00
3,277
mas74
150
1
1
12
6.68
0
0.00
0
14.02
1,661
mas76
150
1
1
11
6.45
0
0.00
0
26.52
4,172
misc03
159
1
143
727
33.65
450
34.92
1,200
51.70
18,359
misc06
112
1,696
112
1125
99.84
376
0.00
0
100.00
229
misc07
259
1
432
2135
11.03
3,600
3.86
1,200
20.11
41,453
mod011
96
10,862
253
1781
17.30
3,600
0.00
0
72.44
86,385
modglob
98
324
357
2645
60.77
254
0.00
0
92.18
1,594
mkc
5,323
2
112
2745
12.18
3,600
1.27
1,200
36.16
51,519
pk1
55
31
4229
22088
0.00
3,600
0.00
0
0.00
55
pp08a
64
176
246
1400
95.97
3,600
4.32
1,200
97.03
12,482
pp08aCUTS
64
176
143
687
62.99
3,600
0.68
1,200
95.81
5,666
qiu
48
792
847
2243
28.41
3,600
10.71
1,200
77.51
200,354
qnet1
1,417
124
182
805
64.60
3,600
7.32
1,200
100.00
21,498
qnet1 o
1,417
124
90
409
83.78
3,600
8.61
1,200
100.00
5,312
rentacar
55
9,502
92
281
23.41
3,600
0.00
5
0.00
—
rgn
100
80
114
666
99.81
1,200
0.00
0
100.00
222
rout
315
241
2225
17230
16.07
3,600
0.03
1,200
70.70
464,634
set1ch
240
472
156
694
63.39
3,600
51.41
34
89.74
10,768
swath
6,724
81
167
1421
33.96
3,600
7.68
1,200
28.51
2,420
vpm1
168
210
53
241
99.93
158
100.00
15
100.00
5,010
vpm2
168
210
74
314
71.48
224
62.86
1,022
81.05
6,012

350
S. Dash, O. G¨unl¨uk, and A. Lodi
reported in [4,6,15] in a much shorter computing time although sometimes a very
large computational eﬀort seems customary to obtain tight approximations. In
a few cases, we have been able to improve over the best bound known so far. Of
course, 1 hour of CPU time to strengthen the initial formulation can be by far
too much, but as shown in [4,15] it might be the case that such a preprocessing
allows the solution of hard unsolved problems and making it quicker has an
intrinsic value.
Acknowledgments
Part of this research was carried out when the third author was Herman Golds-
tine Fellow of the IBM T.J. Watson Research Center, whose support is strongly
acknowledged. The third author was also supported in part by the EU projects
ADONET (contract n. MRTN-CT-2003-504438) and ARRIVAL (contract n.
FP6-021235-2).
References
1. K. Andersen, G. Cornuejols, Y. Li, Split Closure and Intersection Cuts, Mathemat-
ical Programming 102 (2005), 457-493.
2. E. Balas, Disjunctive programming, Annals of Discrete Mathematics 5 (1979),
3–51.
3. E. Balas, S. Ceria, G. Cornu´ejols, G. Natraj, Gomory cuts revisited, Operations
Research Letters 19 (1996) 1–9.
4. E. Balas, A. Saxena, Optimizing over the Split Closure, 2005. Mathematical Pro-
gramming, to appear.
5. P. Bonami, G. Cornu´ejols, A note on the MIR Closure. Manuscript, 2006.
6. P. Bonami, G. Cornu´ejols, S. Dash, M. Fischetti, A. Lodi, Projected Chv´atal-
Gomory cuts for mixed integer linear programs, 2005. Mathematical Programming,
to appear.
7. A. Caprara, A. Letchford, On the separation of split cuts and related inequalities,
Mathematical Programming 94 (2003), 279–294.
8. V. Chv´atal, Edmonds polytopes and a hierarchy of combinatorial problems, Dis-
crete Mathematics 4 (1973), 305–337.
9. W. J. Cook, R. Kannan, A. Schrijver, Chv´atal closures for mixed integer program-
ming problems, Mathematical Programming 47 (1990) 155–174.
10. G. Cornu´ejols, Y. Li, Elementary closures for integer programs, Operations Re-
search Letters 28 (2001), 1–8.
11. G. Cornu´ejols, Y. Li, On the Rank of Mixed 0,1 Polyhedra, Mathematical Program-
ming 91 (2002), 391–397.
12. S. Dash, O. G¨unl¨uk, M. Goycoolea. Two step MIR inequalities for mixed-integer
programs. Manuscript, 2005.
13. F. Eisenbrand, On the membership problem for the elementary closure of a poly-
hedron, Combinatorica 19 (1999), 297–300.
14. R. E. Gomory, An algorithm for the mixed integer problem, RM-2597, The Rand
Corporation, 1960.

On the MIR Closure of Polyhedra
351
15. M. Fischetti, A. Lodi, Optimizing over the ﬁrst Chv´atal closure, Integer Program-
ming and Combinatorial Optimization (M. Juenger and V. Kaibel eds.), Lecture
Notes in Computer Science 3509, Springer-Verlag, Berlin, 12-22, 2005. Mathemat-
ical Programming, to appear.
16. H. Marchand, L. A. Wolsey, Aggregation and Mixed Integer Rounding to solve
MIPs, Operations Research 49 (2001), 363–371.
17. G. Nemhauser, L. A. Wolsey, A recursive procedure to generate all cuts for 0-1
mixed integer programs, Mathematical Programming 46 (1990), 379–390.
18. G. Nemhauser, L. A. Wolsey, Integer and Combinatorial Optimization, Wiley, New
York (1988).
19. J. P. Vielma, A Constructive Characterization of the Split Closure of a Mixed
Integer Linear Program, 2005. Operations Research Letters, to appear.
20. L. A. Wolsey, Integer Programming, Wiley, New York (1998).

The Intersection of Continuous Mixing
Polyhedra and the Continuous Mixing
Polyhedron with Flows⋆
Michele Conforti1, Marco Di Summa1, and Laurence A. Wolsey2
1 Dipartimento di Matematica Pura ed Applicata, Universit`a degli Studi di Padova,
Via Trieste 63, 35121 Padova, Italy
{conforti,mdsumma}@math.unipd.it
2 Center for Operations Research and Econometrics (CORE), Universit´e catholique
de Louvain, 34, Voie du Roman Pays, 1348 Louvain-la-Neuve, Belgium
wolsey@core.ucl.ac.be
Abstract. In this paper we investigate two generalizations of the con-
tinuous mixing set studied by Miller and Wolsey [5] and Van Vyve [7]:
the intersection set
XI = {(σ, r, y) ∈IRn
+ × IRn
+ × ZZn
+ : σk + rt + yt ≥bkt, 1 ≤k, t ≤n}
and the continuous mixing set with ﬂows
XCMF = {(s, r, x, y) ∈IR+ × IRn
+ × IRn
+ × ZZn
+ :
s + rt + xt ≥bt, xt ≤yt, 1 ≤t ≤n} ,
which appears as a strong relaxation of some single-item lot-sizing prob-
lems. We give two extended formulations for the convex hull of each of
these sets. In particular, for XCMF the sizes of the extended formula-
tions are polynomial in the size of the original description of the set,
thus proving that the corresponding linear optimization problem can be
solved in polynomial time.
Keywords: integer programming.
1
Introduction
In the last 5-10 years several mixed-integer sets have been studied that are
interesting in their own right as well as providing strong relaxations of single-
item lot-sizing sets. One in particular is the continuous mixing set XCM:
s + rt + yt ≥bt, 1 ≤t ≤n
s ∈IR+, r ∈IRn
+, y ∈ZZn
+ .
⋆This work was partly carried out within the framework of ADONET, a European
network in Algorithmic Discrete Optimization, contract no. MRTN-CT-2003-504438.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 352–366, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

The Intersection of CM Polyhedra and the CM Polyhedron with Flows
353
The continuous mixing polyhedron conv(XCM), which is the convex hull of
the above set, was introduced and studied by Miller and Wolsey in [5], where
an extended formulation of conv(XCM) with O(n2) variables and O(n2) con-
straints was given. Van Vyve [7] gave a more compact extended formulation
of conv(XCM) with O(n) variables and O(n2) constraints and a formulation of
conv(XCM) in its original space.
We study here two generalizations of the continuous mixing set. First we
consider the intersection set XI, the intersection of several continuous mixing
sets with distinct σk variables and common r and y variables:
σk + rt + yt ≥bkt, 1 ≤k, t ≤n
(1)
σ ∈IRn
+, r ∈IRn
+, y ∈ZZn
+ .
(2)
Then we consider XCMF, the “ﬂow version” of the continuous mixing set:
s + rt + xt ≥bt, 1 ≤t ≤n
(3)
xt ≤yt, 1 ≤t ≤n
(4)
s ∈IR+, r ∈IRn
+, x ∈IRn
+, y ∈ZZn
+ .
(5)
We now show two links between the continuous mixing set with ﬂows XCMF
and lot-sizing. The ﬁrst is to the single-item constant capacity lot-sizing problems
with backlogging over n periods, which can be formulated (including redundant
equations) as:
sk−1 + t
u=k wu + rt = t
u=k du + st + rk−1, 1 ≤k ≤t ≤n
wu ≤Czu, 1 ≤u ≤n; s ∈IRn+1
+
, r ∈IRn+1
+
, w ∈IRn
+, z ∈{0, 1}n .
Here du is the demand in period u, su and ru are the stock and backlog at the end
of period u, zu takes value 1 if there is a set-up in period u allowing production
to take place, wu is the production in period u and C is the capacity (i.e. the
maximum production). To see that this set has a relaxation as the intersection
of n continuous mixing sets with ﬂows, take C = 1 wlog, ﬁx k, set s = sk−1,
xt = t
u=k wu, yt = t
u=k zu and bt = t
u=k du, giving a ﬁrst relaxation:
s + xt + rt ≥bt, k ≤t ≤n
(6)
0 ≤xu −xu−1 ≤yu −yu−1 ≤1, k ≤u ≤n
(7)
s ∈IR+, r ∈IRn−k+1
+
, x ∈IRn−k+1
+
, y ∈ZZn−k+1 .
(8)
Now summing (7) over k ≤u ≤t (for each ﬁxed t = k, . . . , n) and dropping the
upper bound on yt, one obtains precisely the continuous mixing set with ﬂows
XCMF.
The set XCMF also provides an exact model for the two stage stochastic lot-
sizing problem with constant capacities and backlogging. Speciﬁcally, at time 0
one must choose to produce a quantity s at a per unit cost of h. Then in period 1,
n diﬀerent outcomes are possible. For 1 ≤t ≤n, the probability of event t is φt,
the demand is bt and the unit production cost is pt, with production in batches

354
M. Conforti, M. Di Summa, and L.A. Wolsey
of size up to C; there are also a ﬁxed cost of qt per batch and a possible bound
kt on the number of batches. As an alternative to production there is a linear
backlog (recovery) cost et. Finally the goal is to satisfy all demands and minimize
the total expected cost. The resulting problem is
min
hs + n
t=1 φt(ptxt + qtyt + etrt)
s.t.
s + rt + xt ≥bt, 1 ≤t ≤n
(9)
xt ≤Cyt, yt ≤kt, 1 ≤t ≤n
(10)
s ∈IR+, r ∈IRn
+, x ∈IRn
+, y ∈ZZn
+ .
(11)
When kt = 1 for all t, this is a standard lot-sizing problem, and in general
(assuming C = 1 wlog) this is the set XCMF ∩{(s, r, x, y) : yt ≤kt, 1 ≤t ≤n}.
Now we describe the contents of this paper. Note that throughout, a formu-
lation of a polyhedron P ⊆IRn is an external description of P in its original
space. It consists of a ﬁnite set of inequalities Ax ≤d such that P = {x ∈
IRn : Ax ≤d}. A formulation of P is extended whenever it gives an external
description of P in a space IRn+m that includes the original space, so that, given
Q = {(x, w) ∈IRn+m : A′x + B′w ≤d′}, P is the projection of Q onto the
x-space. Given a mixed-integer set X, an extended formulation of conv(X) is
compact if the size of the matrix (A′ | B′ | d′) is polynomial in the size of the
original description of X.
In Sect. 2 we give two extended formulations for the polyhedron conv(XI).
In the ﬁrst one, we split XI into smaller sets, where the fractional parts of the
σ variables are ﬁxed. We then ﬁnd an extended formulation for each of these
sets and we use Balas’ extended formulation for the convex hull of the union of
polyhedra [1] to obtain an extended formulation of conv(XI).
To construct the second extended formulation, we introduce extra variables
to represent all possible fractional parts taken by the continuous variables at a
vertex of conv(XI). We then strengthen the original inequalities and show that
the system thus obtained yields an extended formulation of conv(XI).
When bkt = bt −bk, 1 ≤t, k ≤n, the intersection set is called a diﬀerence set,
denoted XDIF. For conv(XDIF), we prove in Sect. 3 that our two extended for-
mulations are compact. On the other hand, we show in Sect. 4 that the extended
formulations of conv(XI) are not compact when the values bkt are arbitrary.
We then study the polyhedron conv(XCMF). We show in Sect. 5 that there is
an aﬃne transformation which maps the polyhedron conv(XCMF) into the inter-
section of a polyhedron conv(XDIF) with a polyhedron that admits an easy exter-
nal description. This yields two compact extended formulations for conv(XCMF),
showing in particular that one can optimize over XCMF in polynomial time.
2
Two Extended Formulations for the Intersection Set
The intersection set XI is the mixed-integer set deﬁned by (1)–(2). Note that XI
is the intersection of n continuous mixing sets XCM
k
, each one associated with a
distinct variable σk and having common variables r, y.

The Intersection of CM Polyhedra and the CM Polyhedron with Flows
355
In order to obtain extended formulations for conv(XI), we introduce two ver-
sions of the intersection set in which the fractional parts of the continuous vari-
ables σk, rt are restricted in value.
In the following we call fractional part any number in [0, 1). Also, for a number
a ∈IR, f(a) = a −⌊a⌋denotes the fractional part of a, and for a vector v =
(v1, . . . , vq), f(v) is the vector (f(v1), . . . , f(vq)).
In the ﬁrst case, we consider a list Lσ = {f 1, . . . , f ℓ} of n-vectors whose
components are fractional parts and a list Lr = {g1, . . . , gm} of fractional parts
and deﬁne the set
XI
1 = {(σ, r, y) ∈XI : f(σ) ∈Lσ, f(rt) ∈Lr, 1 ≤t ≤n} .
We say that the lists Lσ, Lr are complete for XI if for every vertex (¯σ, ¯r, ¯y) of
conv(XI), f(¯σ) ∈Lσ and f(¯rt) ∈Lr, 1 ≤t ≤n.
Remark 1. If Lσ, Lr are complete lists for XI then conv(XI
1) = conv(XI).
In the second case, we consider a single list L = {f1, . . . , fℓ} of fractional parts
and deﬁne the set
XI
2 = {(σ, r, y) ∈XI : f(σk) ∈L, f(rt) ∈L, 1 ≤k, t ≤n} .
We say that the list L is complete for XI if for every vertex (¯σ, ¯r, ¯y) of conv(XI)
and for every 1 ≤k, t ≤n, f(¯σk) ∈L and f(¯rt) ∈L.
Remark 2. If L is a complete list for XI then conv(XI
2) = conv(XI).
2.1
An Extended Formulation for conv(XI
1)
We give here an extended formulation of conv(XI
1) with O(ℓmn) variables and
O(ℓn(m + n)) constraints.
For each ﬁxed vector f i ∈Lσ, let XI
1,i = {(σ, r, y) ∈XI
1 : f(σ) = f i}. Notice
that XI
1 = ℓ
i=1 XI
1,i. First we ﬁnd an extended formulation for each of the sets
conv(XI
1,i), 1 ≤i ≤ℓ, and then, since conv(XI
1) = conv
 ℓ
i=1 conv(XI
1,i)

, we
use Balas’ extended formulation for the convex hull of the union of polyhedra [1],
in the fashion introduced in [3].
In the following we assume wlog g1 > g2 > · · · > gm. The set XI
1,i can be
modeled as the following mixed-integer set:
σk = μk + f i
k, 1 ≤k ≤n
rt = νt + m
j=1 gjδtj, 1 ≤t ≤n
μk + νt + m
j=1 gjδtj + yt ≥bkt −f i
k, 1 ≤k, t ≤n
m
j=1 δtj = 1, 1 ≤t ≤n
μk, νt, yt, δtj ≥0, 1 ≤t, k ≤n, 1 ≤j ≤m
μk, νt, yt, δtj integer, 1 ≤t, k ≤n, 1 ≤j ≤m .

356
M. Conforti, M. Di Summa, and L.A. Wolsey
Using Chv´atal-Gomory rounding, the above system can be tightened to
σk = μk + f i
k, 1 ≤k ≤n
(12)
rt = νt + m
j=1 gjδtj, 1 ≤t ≤n
(13)
μk + νt + 
j:gj≥f(bkt−f i
k) δtj + yt ≥⌊bkt −f i
k⌋+ 1, 1 ≤k, t ≤n
(14)
m
j=1 δtj = 1, 1 ≤t ≤n
(15)
μk, νt, yt, δtj ≥0, 1 ≤t, k ≤n, 1 ≤j ≤m
(16)
μk, νt, yt, δtj integer, 1 ≤t, k ≤n, 1 ≤j ≤m .
(17)
Let A be the constraint matrix of (14)–(15). We show that A is a totally
unimodular (TU) matrix.
Order the columns of A according to the following ordering of the variables:
μ1, . . . , μn; y1, ν1, δ11, . . . , δ1m; y2, ν2, δ21, . . . , δ2m; . . . ; yn, νn, δn1, . . . , δnm .
For each row of A, the 1’s that appear in a block [yt, νt, δt1, . . . , δtm] are consec-
utive and start from the ﬁrst position. Furthermore, for each row of A only one
of these blocks contains nonzero elements.
Consider an arbitrary column submatrix of A. We give color red to all the μi
(if any) and then, for each of the blocks [yt, νt, δt1, . . . , δtm], we give alternating
colors, always starting with blue, to the columns of this block which appear in
the submatrix. Since this is an equitable bicoloring, the theorem of Ghouila-
Houri [4] shows that A is TU. Since the right-hand side of the constraints is
integer, the theorem of Hoﬀman and Kruskal implies that (14)–(15) (along with
the nonnegativity conditions) deﬁne an integral polyhedron.
Since (12)–(13) just deﬁne variables σk, rt, we can remove the integrality con-
straints from (12)–(17), thus obtaining an extended formulation for conv(XI
1,i):
conv(XI
1,i) = {(σ, r, y) such that there exist δ, μ satisfying (12)–(16)} .
This formulation involves O(mn) variables and O(n(m + n)) constraints.
Using Balas’ description for the union of polyhedra [1], we obtain:
Theorem 3. The following linear system is an extended formulation of the poly-
hedron conv(XI
1) with O(ℓmn) variables and O(ℓn(m + n)) constraints:
σk = ℓ
i=1 σi
k, 1 ≤k ≤n
rt = ℓ
i=1 ri
t, 1 ≤t ≤n
yt = ℓ
i=1 yi
t, 1 ≤t ≤n
ℓ
i=1 λi = 1
σi
k = μi
k + f i
kλi, 1 ≤k ≤n, 1 ≤i ≤ℓ
ri
t = νi
t + m
j=1 gjδi
tj, 1 ≤t ≤n, 1 ≤i ≤ℓ
μi
k + νi
t + 
j:gj≥f(bkt−f i
k) δi
tj + yi
t ≥(⌊bkt −f i
k⌋+ 1)λi, 1 ≤k, t ≤n, 1 ≤i ≤ℓ
m
j=1 δi
tj = λi, 1 ≤t ≤n, 1 ≤i ≤ℓ
μi
k, νi
t, yi
t, δi
tj, λi ≥0, 1 ≤k, t ≤n, 1 ≤j ≤m, 1 ≤i ≤ℓ.

The Intersection of CM Polyhedra and the CM Polyhedron with Flows
357
By Remark 1 we then obtain:
Corollary 4. If the lists Lσ, Lr are complete for XI then the linear system given
in Theorem 3 is an extended formulation of conv(XI).
2.2
An Extended Formulation for conv(XI
2)
We give an extended formulation for conv(XI
2) with O(ℓn) variables and O(ℓn2)
constraints. We include zero in the list L. Also, for technical reasons we deﬁne
f0 = 1. Wlog we assume 1 = f0 > f1 > · · · > fℓ= 0.
The set XI
2 can be modeled as the following mixed-integer set:
σk = μk + ℓ
j=1 fjδk
j , 1 ≤k ≤n
(18)
rt = νt + ℓ
j=1 fjβt
j, 1 ≤t ≤n
(19)
σk + rt + yt ≥bkt, 1 ≤k, t ≤n
(20)
ℓ
j=1 δk
j = 1, 1 ≤k ≤n
(21)
ℓ
j=1 βt
j = 1, 1 ≤t ≤n
(22)
σk ≥0, rt ≥0, yt ≥0, 1 ≤k, t ≤n
δk
j , βt
j ≥0, 1 ≤k, t ≤n, 1 ≤j ≤ℓ
μk, νt, yt, δk
j , βt
j integer, 1 ≤k, t ≤n, 1 ≤j ≤ℓ.
Now deﬁne the unimodular transformation
μk
0 = μk, μk
j = μk + j
h=1 δk
h, 1 ≤k ≤n, 1 ≤j ≤ℓ
νt
0 = νt + yt, νt
j = νt + yt + j
h=1 βt
h, 1 ≤t ≤n, 1 ≤j ≤ℓ.
Then (18) and (19) become
σk = ℓ−1
j=0(fj −fj+1)μk
j , 1 ≤k ≤n
rt = −yt + ℓ−1
j=0(fj −fj+1)νt
j, 1 ≤t ≤n ,
while (21)–(22) become μk
ℓ−μk
0 = 1, 1 ≤k ≤n, and νt
ℓ−νt
0 = 1, 1 ≤t ≤n.
Constraints δk
j ≥0, 1 ≤k ≤n, 1 ≤j ≤ℓ, can be modeled as μk
j −μk
j−1 ≥0.
Similarly βt
j ≥0, 1 ≤t ≤n, 1 ≤j ≤ℓ, can be modeled as νt
j −νt
j−1 ≥0.
Inequalities σk ≥0, 1 ≤k ≤n, become μk
0 ≥0, while rt ≥0, 1 ≤t ≤n,
become νt
0 −yt ≥0.
We now model (20). Deﬁne ℓkt = max{τ : fτ ≥f(bkt)}. Also, for an index
0 ≤j ≤ℓkt −1, deﬁne hj
kt = max{τ : fτ ≥1 + f(bkt) −fj+1} and for an index
ℓkt ≤j ≤ℓ−1, deﬁne hj
kt = max{τ : fτ ≥f(bkt) −fj+1}.
Lemma 5. Assume that a point (σ, r, y) satisﬁes (18), (19), (21) and (22). Then
(σ, r, y) satisﬁes (20) if and only if the following inequalities are valid for (σ, r, y):
μk
hj
kt + νt
j ≥⌊bkt⌋, 0 ≤j ≤ℓkt −1
(23)
μk
hj
kt + νt
j ≥⌊bkt⌋+ 1, ℓkt ≤j ≤ℓ−1 .
(24)

358
M. Conforti, M. Di Summa, and L.A. Wolsey
Proof. We ﬁrst assume that (σ, r, y) satisﬁes (18)–(22). Suppose 0 ≤j ≤ℓkt −
1. Constraint (20) can be written as μk + νt + yt + ℓ
i=1 fiδk
i + ℓ
i=1 fiβt
i ≥
(⌊bkt⌋−1) + 1 + f(bkt). Since the δk
i ’s (resp. βt
i’s) are binary variables such that
ℓ
i=1 δk
i = 1 (resp. ℓ
i=1 βt
i = 1), this implies μk +νt +yt+hj
kt
i=1 fiδk
i +fhj
kt+1 +
j
i=1 fiβt
i + fj+1 ≥(⌊bkt⌋−1) + 1 + f(bkt), thus μk
hj
kt + νt
j ≥(⌊bkt⌋−1) + 1 +
f(bkt) −fhj
kt+1 −fj+1. As 1 + f(bkt) −fhj
kt+1 −fj+1 > 0 for 0 ≤j ≤ℓkt −1 and
as μk
hj
kt + νt
j is an integer, (23) is valid.
Suppose now ℓkt ≤j ≤ℓ−1. Constraint (20) can be written as μk + νt +
yt + ℓ
i=1 fiδk
i + ℓ
i=1 fiβt
i ≥⌊bkt⌋+ f(bkt). Similarly as before, this implies
μk
hj
kt + νt
j ≥⌊bkt⌋+ f(bkt) −fhj
kt+1 −fj+1. As f(bkt) −fhj
kt+1 −fj+1 > 0 for
ℓkt ≤j ≤ℓ−1 and as μk
hj
kt + νt
j is an integer, (24) is valid.
Now assume that (σ, r, y) satisﬁes (18), (19), (21) and (22), along with (23)–
(24). Speciﬁcally, assume σk = μk + fi and rt = νt + fl.
Suppose l ≤ℓkt. Inequality (23) for j = l −1 is μk
hl−1
kt
+ νt + yt ≥⌊bkt⌋.
If i ≤hl−1
kt , the inequality is μk + νt + yt ≥⌊bkt⌋−1, thus σk + rt + yt ≥
⌊bkt⌋−1 + fi + fl ≥⌊bkt⌋+ f(bkt) = bkt. And if i > hl−1
kt , the inequality is
μk + νt + yt ≥⌊bkt⌋, thus σk + rt + yt ≥⌊bkt⌋+ fl ≥⌊bkt⌋+ f(bkt) = bkt. Thus
(20) is satisﬁed when l ≤ℓkt. The case l > ℓkt is similar.
⊓⊔
Thus we obtain the following result.
Theorem 6. The following linear system is an extended formulation of the poly-
hedron conv(XI
2) with O(ℓn) variables and O(ℓn2) constraints:
σk = ℓ−1
j=0(fj −fj+1)μk
j , 1 ≤k ≤n
(25)
rt = −yt + ℓ−1
j=0(fj −fj+1)νt
j, 1 ≤t ≤n
(26)
μk
hj
kt + νt
j ≥⌊bkt⌋, 1 ≤k, t ≤n, 0 ≤j ≤ℓkt −1
(27)
μk
hj
kt + νt
j ≥⌊bkt⌋+ 1, 1 ≤k, t ≤n, ℓkt ≤j ≤ℓ−1
(28)
μk
ℓ−μk
0 = 1, νt
ℓ−νt
0 = 1, 1 ≤k, t ≤n
(29)
μk
j −μk
j−1 ≥0, νt
j −νt
j−1 ≥0, 1 ≤k, t ≤n, 1 ≤j ≤ℓ
(30)
μk
0 ≥0, νt
0 −yt ≥0, yt ≥0, 1 ≤k, t ≤n .
(31)
Proof. XI
2 is the set of points (σ, r, y) such that there exist integral vectors δ, μ
satisfying (25)–(31). Changing the sign of the νt
j and yt variables, the constraint
matrix of (27)–(31) is a dual network matrix (that is, the transpose of a network
ﬂow matrix), in particular it is TU. Since the right-hand side is an integer vector
and since (25)–(26) just deﬁne variables σk, rt,
conv(XI
2) = {(σ, r, y) such that there exist δ, μ satisfying (25)–(31)} .
⊓⊔

The Intersection of CM Polyhedra and the CM Polyhedron with Flows
359
By Remark 2 we then obtain:
Corollary 7. If the list L is complete for XI then the linear system given in
Theorem 6 is an extended formulation of conv(XI).
3
The Diﬀerence Set
The following set is the diﬀerence set XDIF:
σk + rt + yt ≥bt −bk, 0 ≤k < t ≤n
σ ∈IRn+1
+
, r ∈IRn
+, y ∈ZZn
+ ,
where 0 = b0 ≤b1 ≤. . . ≤bn. Note that XDIF is an intersection set where
bkt = bt −bk, as for k ≥t the constraint σk + rt + yt ≥bt −bk is redundant.
Here we prove that the extended formulations given in Sect. 2 are compact
for a set of the type XDIF. This will be useful in Sect. 5, where we study XCMF.
Theorem 8. Let (σ∗, r∗, y∗) be a vertex of conv(XDIF). Then there exists an
index h ∈{0, . . . , n} such that σ∗
k > 0 for k < h and σ∗
k = 0 for k ≥h.
Furthermore there is an index ℓ≥h such that f(σ∗
k) = f(bℓ−bk) for 0 ≤k < h.
Proof. Let (σ∗, r∗, y∗) be a vertex of conv(XDIF), let α = max1≤t≤n{bt−r∗
t −y∗
t }
and let Tα ⊆{1, . . . , n} be the subset of indices for which this maximum is
achieved.
Claim 1: For each 1 ≤k ≤n, σ∗
k = max{0, α −bk}.
Proof. The inequalities that deﬁne XDIF show that σ∗
k ≥max{0, α −bk}. If
σ∗
k > max{0, α −bk}, then there is an ε > 0 such that (σ∗, r∗, y∗) ± ε(ek, 0, 0)
are both in conv(XDIF), a contradiction to the fact that (σ∗, r∗, y∗) is a vertex.
This concludes the proof of the claim.
Let h = min{k : α −bk ≤0}. (This minimum is well deﬁned: since the
only inequality involving σn is σn ≥0, certainly σ∗
n = 0; then, by Claim 1,
α −bn ≤0.) Since 0 = b0 ≤b1 ≤· · · ≤bn, Claim 1 shows that σ∗
k > 0 for k < h
and σ∗
k = 0 for k ≥h and this proves the ﬁrst part of the theorem. Furthermore
σ∗
k + r∗
t + y∗
t = bt −bk for all k < h and t ∈Tα.
Claim 2: Either r∗
t = 0 for some t ∈Tα or f(rt) = f(bt −bh) for every t ∈Tα.
Proof. We use the fact that (σ∗, r∗) is a vertex of the polyhedron:
Q = {(σ, r) ∈IRn+1
+
× IRn
+ : σk + rt ≥bt −bk −y∗
t , 0 ≤k < t ≤n} .
We now consider the following two cases:
Case 1: α −bh < 0.
For k ≥h, the only inequality that is tight for (σ∗, r∗) and contains σk in its
support is σk ≥0. For k < h, the only inequalities that are tight for (σ∗, r∗) and
contain σk in their support are σk + rt ≥bt −bk −y∗
t , t ∈Tα.
Let eH be the (n+1)-vector having the ﬁrst h components equal to 1 and the
others to 0, let eTα be the incidence vector of Tα and assume that r∗
t > 0 for all

360
M. Conforti, M. Di Summa, and L.A. Wolsey
t ∈Tα. Then the vectors (σ∗, r∗) ± ε(eH, −eTα) for some ε > 0 are both in Q,
contradicting the fact that (σ∗, r∗) is a vertex of Q. So r∗
t = 0 for some t ∈Tα.
Case 2: α −bh = 0.
Then (σ∗, r∗, y∗) satisﬁes σ∗
h + r∗
t + y∗
t = bt −bh for all t ∈Tα. Since σ∗
h = 0 and
y∗
t is integer, then f(r∗
t ) = f(bt −bh) for all t ∈Tα and this completes the proof
of Claim 2.
Assume r∗
t = 0 for some t ∈Tα. Since σ∗
k + r∗
t + y∗
t = bt −bk for all k < h and
y∗
t is an integer, then f(σ∗
k) = f(bt −bk) for all k < h.
If f(r∗
t ) = f(bt −bh) for all t ∈Tα, since σ∗
k + r∗
t + y∗
t = bt −bk for all t ∈Tα
and for all k < h and since y∗is an integer vector, then f(σ∗
k) = f(bh −bk) for
all k < h.
⊓⊔
Corollary 9. If (σ∗, r∗, y∗) is a vertex of conv(XDIF), then f(r∗
t ) ∈{f(bt −
bk), 1 ≤k ≤n} for 1 ≤t ≤n.
Proof. The result follows from Theorem 8 and the observation that at a vertex
of conv(XDIF) either r∗
t = 0 or σ∗
k + r∗
t + y∗
t = bt −bk for some k.
⊓⊔
We then obtain the following result.
Theorem 10. The polyhedron conv(XDIF) admits an extended formulation of
the type given in Theorem 3 with O(n5) variables and constraints and an ex-
tended formulation of the type given in Theorem 6 with O(n3) variables and
O(n4) constraints.
Proof. Recall that XDIF is an intersection set. Deﬁne Lσ as the set of all possible
(n + 1)-vectors of fractional parts taken by σ at a vertex of conv(XDIF) and Lr
as the set of all possible fractional parts taken by the variables rt at a vertex of
conv(XDIF). Since these lists are complete for XDIF, Corollary 4 implies that the
linear system given in Theorem 3 is an extended formulation of conv(XDIF). By
Theorem 8, ℓ= |Lσ| = O(n2) and by Corollary 9, m = |Lr| = O(n2), therefore
this formulation has O(n5) variables and O(n5) constraints.
Now deﬁne L as the set of all possible fractional parts taken by the variables
σk, rt at a vertex of conv(XDIF). Since this list is complete for XDIF, by Corol-
lary 7 the system given in Theorem 6 is an extended formulation of conv(XDIF).
Since ℓ= |L| = O(n2) (see Theorem 8 and Corollary 9), this formulation has
O(n3) variables and O(n4) constraints.
⊓⊔
We point out that the result of the above theorem can be improved as follows.
Consider the ﬁrst formulation. If for each set XI
1,i we deﬁne a diﬀerent list of
fractional parts for the variables rt, say Li
r, then we can easily choose such lists
so that |Li
r| = O(n). In this case the ﬁrst extended formulation for conv(XDIF)
involves O(n4) variables and constraints.
Consider now the second formulation. Instead of deﬁning a unique list for all
variables, we can deﬁne a list for each variable, say Lσk and Lrt, 1 ≤k, t ≤n.
It is not diﬃcult to verify that the construction of the extended formulation
can be carried out with straightforward modiﬁcations. Since in this case |Lσk| =
O(n) (by Theorem 8) and |Lrt| = O(n) (by Corollary 9), the second extended
formulation involves O(n2) variables and O(n3) constraints.

The Intersection of CM Polyhedra and the CM Polyhedron with Flows
361
Theorem 11. The polyhedron conv(XCMF) admits an extended formulation
with O(n2) variables and O(n3) constraints.
4
Intersection Sets with an Exponential Number of
Fractional Parts
In this section we show that the extended formulations derived in Sect. 2 are
not compact in general. Speciﬁcally, we prove here the following result:
Theorem 12. In the set of vertices of the polyhedron deﬁned by
σk + rt ≥3(t−1)n+k
3n2+1
, 1 ≤k, t ≤n
(32)
σ ∈IRn
+, r ∈IRn
+
(33)
the number of distinct fractional parts taken by variable σn is exponential in n.
Remark 13. Since the vertices of the above polyhedron are the vertices on the
face deﬁned by y = 0 of the polyhedron conv(XI) with the same right-hand
side, Theorem 12 shows that any extended formulation that explicitly takes into
account a list of all possible fractional parts taken at a vertex by the continuous
variables (such as those introduced to model conv(XI
1) and conv(XI
2)) will not
be compact in general.
Now let bkt be as in the theorem, i.e. bkt = 3(t−1)n+k
3n2+1
, 1 ≤k, t ≤n.
Remark 14. bkt < bk′t′ if and only if (t, k) ≺(t′, k′), where ≺denotes the lexi-
cographic order. Thus b11 < b21 < · · · < bn1 < b12 < · · · < bnn.
Lemma 15. The following properties hold.
1. Suppose that α ∈ZZq
+ with αj < αj+1 for 1 ≤j ≤q −1, and deﬁne
Φ(α) = q
j=1(−1)q−j3αj. Then 1
23αq < Φ(α) < 3
23αq.
2. Suppose that α is as above and β ∈ZZq′
+ is deﬁned similarly. Then Φ(α) =
Φ(β) if and only if α = β.
Proof. 1. αq−1
j=0 3j =
3αq −1
3−1
<
1
23αq. Now Φ(α) ≥3αq −αq−1
j=1 3j > 3αq −
1
23αq = 1
23αq, and Φ(α) ≤3αq + αq−1
j=1 3j < 3αq + 1
23αq = 3
23αq.
2. Suppose α ̸= β. Wlog we assume q ≥q′. Assume ﬁrst (αq−q′+1, . . . , αq) = β.
Then q > q′ (otherwise α = β) and, after deﬁning ¯α = (α1, . . . , αq−q′), we have
Φ(α) −Φ(β) = Φ(¯α) > 0 by 1. Now assume (αq−q′+1, . . . , αq) ̸= β. Deﬁne
h = min{τ : αq−τ ̸= βq′−τ} and suppose αq−h > βq′−h (the other case is
similar). If we deﬁne the vectors ¯α = (α1, . . . , αq−h) and ¯β = (β1, . . . , βq′−h), 1.
gives Φ(α) −Φ(β) = Φ(¯α) −Φ(¯β) > 1
23αq−h −3
23βq′−h ≥0, as αq−h > βq′−h.
⊓⊔

362
M. Conforti, M. Di Summa, and L.A. Wolsey
We now give a construction of an exponential family of vertices of (32)–(33)
such that at each vertex variable σn takes a distinct fractional part. Therefore
this construction proves Theorem 12.
Let (k1, . . . , km) and (t1, . . . , tm−1) be two increasing subsets of {1, . . . , n}
with k1 = 1 and km = n. For 1 ≤k, t ≤n, let p(k) = max{j : kj ≤k} and
q(t) = max{j : tj ≤t}, with q(t) = 0 if t < t1.
Consider the following system of equations:
σk1 = 0
σkj + rtj = bkjtj,
1 ≤j ≤m −1
σkj+1 + rtj = bkj+1tj,
1 ≤j ≤m −1
σkq(t)+1 + rt = bkq(t)+1t, t /∈{t1, . . . , tm−1}
σk + rtp(k) = bktp(k),
k /∈{k1, . . . , km} .
The unique solution of this system is:
σk1 = 0
σkj = j−1
ℓ=1 bkℓ+1tℓ−j−1
ℓ=1 bkℓtℓ, 2 ≤j ≤m
rtj = j
ℓ=1 bkℓtℓ−j−1
ℓ=1 bkℓ+1tℓ, 1 ≤j ≤m −1
σk = bktp(k) −rtp(k),
k /∈{k1, . . . , km}
rt = bkq(t)+1t −σkq(t)+1,
t /∈{t1, . . . , tm−1} .
As each of these variables σk, rt takes a value of the form Φ(α)/3n2+1, by
Lemma 15 (i) we have that σkj > 1
2bkjtj−1 > 0 for 2 ≤j ≤m, rtj > 1
2bkjtj > 0
for 1 ≤j ≤m−1, σk > 1
2bktp(k) > 0 for k /∈{k1, . . . , km} and rt > 1
2bkq(t)+1t > 0
for t /∈{t1, . . . , tm−1}. Therefore the nonnegativity constraints are satisﬁed.
Now we show that the other constraints are satisﬁed. Consider the k, t con-
straint with t /∈{t1, . . . , tm−1}. We distinguish some cases.
1. p(k) ≤q(t). Then σk + rt ≥rt > 1
2bkq(t)+1t ≥1
2bkp(k)+1t ≥3
2bkt > bkt.
2. p(k) > q(t) and k /∈{k1, . . . , km}. Then σk+rt ≥σk > 1
2bktp(k) ≥1
2bktq(t)+1 ≥
3n
2 bkt > bkt.
3. p(k) = q(t) + 1 and k = kj for some 1 ≤j ≤m (thus p(k) = j = q(t) + 1).
In this case the k, t constraints is satisﬁed at equality by construction.
4. p(k) > q(t) + 1 and k = kj for some 1 ≤j ≤m (thus p(k) = j > q(t) + 1).
Then σk + rt ≥σk > 1
2bktj−1 ≥1
2bktq(t)+1 ≥3n
2 bkt > bkt.
The argument with k /∈{k1, . . . , km} is similar.
Finally suppose that k = kj and t = th with h /∈{j −1, j}. If h > j, σk +rt ≥
rt > 1
2bkhth ≥3
2bkjth > bkt. If h < j−1, σk +rt ≥σk > 1
2bkjtj−1 ≥3n
2 bkjth > bkt.
This shows that the solution is feasible and as it is unique, it deﬁnes a vertex
of (32)–(33).
Now let akt = (t −1)n + k, so that bkt = 3akt/3n2+1 and take
α = (ak1t1, ak2t1, ak2t2, ak3t2, . . . , akmtm−1) .

The Intersection of CM Polyhedra and the CM Polyhedron with Flows
363
As σn = Φ(α)/3n2+1, Lemma 15 (ii) implies that in any two vertices constructed
as above by diﬀerent sequences (k1, . . . , km), (t1, . . . , tm−1) and (k′
1, . . . , k′
m′),
(t′
1, . . . , t′
m′−1), the values of σn are distinct numbers in the interval (0, 1). As
the number of such sequences is exponential in n, this proves Theorem 12.
5
An Extended Formulation for conv(XCMF)
Now we address the question of showing that the linear optimization problem
over the continuous mixing set with ﬂows (3)–(5) is solvable in polynomial time.
Speciﬁcally we derive compact extended formulations for conv(XCMF).
We assume that 0 < b1 ≤· · · ≤bn. Consider the set Z:
s + rt + yt ≥bt, 1 ≤t ≤n
(34)
s + rk + xk + rt + yt ≥bt, 1 ≤k < t ≤n
(35)
s + rt + xt ≥bt, 1 ≤t ≤n
(36)
s ∈IR+, r ∈IRn
+, x ∈IRn, y ∈ZZn
+ .
(37)
Note that x is unrestricted in Z.
Proposition 16. Let XCMF and Z be deﬁned on the same vector b. Then
XCMF ⊆Z and XCMF = Z ∩{(s, r, x, y) : 0 ≤x ≤y}.
Proof. Clearly (34)–(37) are valid for the points in XCMF. The only inequalities
that deﬁne XCMF but do not appear in the deﬁnition of Z are 0 ≤x ≤y.
⊓⊔
Lemma 17. The 3n+1 extreme rays of conv(XCMF) are the vectors (1, 0, 0, 0),
(0, ei, 0, 0), (0, 0, 0, ei), (0, 0, ei, ei). The 3n + 1 extreme rays of conv(Z) are the
vectors (1, 0, −1, 0), (0, ei, −ei, 0), (0, 0, ei, 0), (0, 0, 0, ei). Therefore both reces-
sion cones of conv(XCMF) and conv(Z) are full-dimensional simplicial cones,
thus showing that conv(XCMF) and conv(Z) are full-dimensional polyhedra.
Proof. The ﬁrst part is obvious. We characterize the extreme rays of conv(Z).
The recession cone C of conv(Z) is deﬁned by
s + rk + xk + rt + yt ≥0, 1 ≤k < t ≤n
s + rt + xt ≥0, 1 ≤t ≤n
s ∈IR+, r ∈IRn
+, x ∈IRn, y ∈IRn
+ .
One can verify that the vectors ρ = (1, 0, −1, 0), ui = (0, ei, −ei, 0), vi =
(0, 0, ei, 0), zi = (0, 0, 0, ei) are extreme rays of conv(Z) by checking that each
of them satisﬁes at equality 3n linearly independent inequalities deﬁning C (in-
cluding nonnegativity constraints).
Thus we only have to show that every vector in C can be expressed as conic
combination of the above rays. Let (¯s, ¯r, ¯x, ¯y) be in C. Notice that (¯s, ¯r, ¯x, ¯y) =
¯sρ + n
i=1 ¯riui + n
i=1(¯s + ¯ri + ¯xi)vi + n
i=1 ¯yiwi. Since (¯s, ¯r, ¯x, ¯y) ∈C, all the
coeﬃcients appearing in the above combination are nonnegative.
It can also be checked that the above rays are linearly independent.
⊓⊔

364
M. Conforti, M. Di Summa, and L.A. Wolsey
Lemma 18. Let (s∗, r∗, x∗, y∗) be a vertex of conv(Z). Then
s∗= max{0; bt −r∗
t −y∗
t , 1 ≤t ≤n} ,
x∗
k = max{bk −s∗−r∗
k; bt −s∗−r∗
k −r∗
t −y∗
t , 1 ≤k < t ≤n} .
Proof. Assume s∗> 0 and s∗+ r∗
t + y∗
t > bt, 1 ≤t ≤n. Then, there is an ε ̸= 0
such that (s∗, r∗, x∗, y∗)±ε(1, 0, −1, 0) belong to conv(Z), a contradiction. This
proves the ﬁrst statement. The second one is obvious.
⊓⊔
Proposition 19. Let (s∗, r∗, x∗, y∗) be a vertex of conv(Z). Then 0 ≤x∗≤y∗.
Proof. Assume that {t : x∗
t < 0} ̸= ∅and let h = min{t : x∗
t < 0}. Then
s∗+ r∗
h > bh > 0 and together with y∗
h ≥0, this implies s∗+ r∗
h + y∗
h > bh.
Claim: r∗
h > 0.
Proof. Assume r∗
h = 0. Then s∗> bh > 0. By Lemma 18, s∗+ r∗
t + y∗
t = bt
for some index t. It follows that s∗≤bt, thus t > h (as bh < s∗≤bt). Equation
s∗+ r∗
t + y∗
t = bt, together with s∗+ r∗
h + x∗
h + r∗
t + y∗
t ≥bt, gives r∗
h + x∗
h ≥0,
thus r∗
h > 0, as x∗
h < 0, and this concludes the proof of the claim.
The inequalities s∗+ r∗
h + y∗
h > bh and r∗
k + x∗
k ≥0, 1 ≤k < h, imply
s∗+ r∗
k + x∗
k + r∗
h + y∗
h > bh, 1 ≤k < h.
All these observations show the existence of an ε ̸= 0 such that both points
(s∗, r∗, x∗, y∗) ± ε(0, eh, −eh, 0) belong to conv(Z), a contradiction to the fact
that the point (s∗, r∗, x∗, y∗) is a vertex of conv(Z). Thus x∗≥0.
Suppose now that there exists h such that x∗
h > y∗
h. Then constraint s + rh +
yh ≥bh gives s∗+r∗
h+x∗
h > bh. Lemma 18 then implies that s∗+r∗
h+x∗
h+r∗
t +y∗
t =
bt for some t > h. This is not possible, as inequalities x∗
h > y∗
h ≥0, r∗
h ≥0 and
s∗+ r∗
t + y∗
t ≥bt imply s∗+ r∗
h + x∗
h + r∗
t + y∗
t > bt. Thus x∗≤y∗.
⊓⊔
For the main theorem of this section we present a lemma whose proof is given
in [2].
For a polyhedron P in IRn and a vector a ∈IRn, let μP (a) be the value
min{ax, x ∈P} and MP (a) be the face {x ∈P : ax = μP (a)}, where MP (a) = ∅
whenever μP (a) = −∞.
Lemma 20. Let P ⊆Q be two pointed polyhedra in IRn, with the property that
every vertex of Q belongs to P. Let Cx ≥d be a system of inequalities that are
valid for P such that for every inequality cx ≥δ of the system, P ̸⊂{x ∈IRn :
cx = δ}. If for every a ∈IRn such that μP (a) is ﬁnite but μQ(a) = −∞, Cx ≥d
contains an inequality cx ≥δ such that MP (a) ⊆{x ∈IRn : cx = δ}, then
P = Q ∩{x ∈IRn : Cx ≥d}.
Proof. See [2].
Theorem 21. Let XCMF and Z be deﬁned on the the same vector b. Then
conv(XCMF) = conv(Z) ∩{(s, r, x, y) : 0 ≤x ≤y}.
Proof. By Proposition 16, conv(XCMF) ⊆conv(Z). By Propositions 19 and 16,
every vertex of conv(Z) belongs to conv(XCMF).

The Intersection of CM Polyhedra and the CM Polyhedron with Flows
365
Let a = (h, d, p, q), h ∈IR1, d ∈IRn, p ∈IRn, q ∈IRn, be such that
μconv(XCMF)(a) is ﬁnite and μconv(Z)(a) = −∞. Since by Lemma 17, the extreme
rays of conv(Z) that are not rays of conv(XCMF) are the vectors (0, 0, ei, 0),
(0, ei, −ei, 0) and (1, 0, −1, 0), then either pi < 0 for some index i or di < pi for
some index i or h < n
t=1 pt.
If pi < 0, then Mconv(XCMF)(a) ⊆{(s, r, x, y) : xi = yi}.
If di < pi, then Mconv(XCMF)(a) ⊆{(s, r, x, y) : xi = 0}, otherwise, given an
optimal solution with xi > 0, we could increase ri by a small ε > 0 and decrease
xi by ε, thus obtaining a feasible point with lower objective value.
If h < n
t=1 pt, let N + = {j : pj > 0} and k = min{j : j ∈N +}: we show that
Mconv(XCMF)(a) ⊆{(s, r, x, y) : xk = 0}. Suppose that xk > 0 in some optimal
solution. As the solution is optimal and pk > 0, we cannot just decrease xk and
remain feasible. Thus s + rk + xk = bk, which implies that s < bk. Then for all
j ∈N + we have rj + xj ≥bj −s > bj −bk ≥0, as j ≥k. Since we can assume
dt ≥pt for every t (otherwise we are in the previous case), rt = 0 for every t: if
not, chosen an index t such that rt > 0, one can decrease rt by a small ε > 0
and increase xt by ε, thus obtaining a feasible point with lower objective value,
a contradiction. So rt = 0 for every t and thus, since rj + xj > 0 for all j ∈N +,
we have xj > 0 for all j ∈N +. Then we can increase s by a small ε > 0 and
decrease xj by ε for all j ∈N +. The new point is feasible in XCMF and has
lower objective value, a contradiction.
We have shown that for every vector a such that μconv(XCMF)(a) is ﬁnite and
μconv(Z)(a) = −∞, the system 0 ≤x ≤y contains an inequality which is tight for
the points in Mconv(XCMF)(a). To complete the proof, since conv(XCMF) is full-
dimensional (Lemma 17), the system 0 ≤x ≤y does not contain an improper
face of conv(XCMF). So we can now apply Lemma 20 to conv(XCMF), conv(Z)
and the system 0 ≤x ≤y.
⊓⊔
Therefore, if we have a compact extended formulation of conv(Z), then this
will immediately yield a compact extended formulation of conv(XCMF). Such a
formulation exists, as Z is equivalent to a diﬀerence set:
Theorem 22. Let XDIF be a diﬀerence set and XCMF be deﬁned on the same
vector b. The aﬃne transformation σ0 = s, σt = s+rt +xt −bt, 1 ≤t ≤n, maps
conv(XCMF) into conv(XDIF)∩{(σ, r, y) : 0 ≤σk−σ0−rk+bk ≤yk, 1 ≤k ≤n}.
Proof. Let Z be deﬁned on the same vector b. It is straightforward to check that
the aﬃne transformation σ0 = s, σt = s + rt + xt −bt, 1 ≤t ≤n, maps conv(Z)
into conv(XDIF). By Theorem 21, conv(XCMF) = conv(Z) ∩{(s, r, x, y) : 0 ≤
x ≤y} and the result follows.
⊓⊔
Then the extended formulations of conv(XDIF) described in Sects. 2–3 give ex-
tended formulations of conv(XCMF) which are compact. By Theorem 11 we
have:
Theorem 23. The polyhedron conv(XCMF) admits an extended formulation
with O(n2) variables and O(n3) constraints. It follows that the linear optimiza-
tion problem over XCMF can be solved in polynomial time.

366
M. Conforti, M. Di Summa, and L.A. Wolsey
5.1
An Extended Formulation for the Two Stage Stochastic
Lot-Sizing Problem with Constant Capacities and Backlogging
We brieﬂy consider the set XCMF ∩W, where
W = {(s, r, x, y) : lj ≤yj ≤uj, ljk ≤yj −yk ≤ujk, 1 ≤j, k ≤n} ,
with lj, uj, ljk, ujk ∈ZZ ∪{+∞, −∞}, 1 ≤j, k ≤n. We assume that for each
1 ≤i ≤n, W contains a point satisfying yi ≥1.
In the following we show that an extended formulation of conv(XCMF ∩W)
is obtained by adding the inequalities deﬁning W to one of the extended formu-
lations of conv(XCMF) derived above. The proof uses the same technique as in
Sect. 5, where Z (resp. XCMF) has to be replaced with Z ∩W (resp. XCMF∩W).
We only point out the main diﬀerences.
To see that the proof of Theorem 21 is still valid, note that the extreme rays
of conv(Z ∩W) are of the following types:
1. (1, 0, −1, 0), (0, ei, −ei, 0), (0, 0, ei, 0);
2. (0, 0, 0, y) for suitable vectors y ∈ZZn.
However, the rays of type 2 are also rays of conv(XCMF∩W). Also, the condition
that for every index i, W contains a vector with yi > 0, shows that none of the
inequalities 0 ≤xi ≤yi deﬁnes an improper face of conv(XCMF ∩W) and
Lemma 20 can still be applied. Thus the proof of Theorem 21 is still valid.
The rest of the proof is a straightforward adaptation of Theorem 22.
Since (9)–(11) deﬁne a set of the type XCMF ∩W (assuming C = 1 wlog),
the above result yields an extended formulation for the feasible region of the two
stage stochastic lot-sizing problem with constant capacities and backlogging.
References
1. Balas, E.: Disjunctive programming: properties of the convex hull of feasible points.
Invited paper, Discrete Appl. Math. 89 (1988) 1–44
2. Conforti, M., Di Summa, M., Wolsey, L. A.: The mixing set with ﬂows. CORE DP
2005/92, Universit´e catholique de Louvain. SIAM J. Discret. Math. (to appear)
3. Conforti, M., Wolsey, L. A.: Compact formulations as a union of polyhedra. CORE
DP 2005/62, Universit´e catholique de Louvain. Math. Program. (to appear)
4. Ghouila-Houri, A.: Caract´erisations des matrices totalement unimodulaires. C. R.
Acad. Sci. Paris 254 (1968) 155–163
5. Miller, A. J., Wolsey, L. A.: Tight formulations for some simple mixed integer pro-
grams and convex objective integer programs. Math. Program. B 98 (2003) 73–88
6. Van Vyve, M.: A solution approach of production planning problems based on
compact formulations for single-item lot-sizing models. Ph. D. thesis, Facult´e des
Sciences Appliqu´ees, Universit´e catholique de Louvain (2003)
7. Van Vyve, M.: The continuous mixing polyhedron. Math. Oper. Res. 30 (2005)
441–452

Simple Explicit Formula for Counting Lattice
Points of Polyhedra
Jean B. Lasserre1 and Eduardo S. Zeron2
1 LAAS-CNRS and Institute of Mathematics,
LAAS, 7 Av. du Colonel Roche, 31077 Toulouse, France
lasserre@laas.fr
http://www.laas.fr/~lasserre
2 Depto. Matem´aticas, Apdo. Postal 14-740
Cinvestav-IPN, Mexico D.F. 07000, Mexico
eszeron@math.cinvestav.mx
http://www.cinvestav.mx/SantillanSp.htm
Abstract. Given z ∈Cn and A ∈Zm×n, we provide an explicit expres-
sion and an algorithm for evaluating the counting function h(y; z) :=
{ zx | x∈Zn; Ax=y, x≥0}. The algorithm only involves simple (but pos-
sibly numerous) calculations. In addition, we exhibit ﬁnitely many ﬁxed
convex cones of Rn explicitly and exclusively deﬁned by A, such that
for any y ∈Zm, h(y; z) is obtained by a simple formula that evaluates
 zx over the integral points of those cones only. At last, we also pro-
vide an alternative (and diﬀerent) formula from a decomposition of the
generating function into simpler rational fractions, easy to invert.
1
Introduction
Consider the (not necessarily compact) polyhedron
Ω(y) := {x ∈Rn | Ax = y; x ≥0},
(1)
with y ∈Zm and A ∈Zm×n of maximal rank for n ≥m; besides, given z ∈Cn,
let h : Zm →C be the counting function
y →h(y; z) :=

x∈Ω(y)∩Zn
zx
(2)
(where zx stands for 
k zkxk). The complex vector z ∈Cn may be chosen close
enough to zero in order to ensure that h(y; z) is well deﬁned even when Ω(y) is
not compact. If Ω(y) is compact, then y →h(y; z) provides us with the exact
number of points in the set Ω(y) ∩Zn by either evaluating h(y, 1), or even
rounding h(y; z) up to the nearest integer when all the entries of z are close
enough to one.
Computation of h has attracted a lot of attention in recent years, from
both theoretical and practical computation viewpoints. Barvinok and Pommer-
sheim [4], Brion and Vergne [8], have provided nice exact (theoretical) formulas
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 367–381, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

368
J.B. Lasserre and E.S. Zeron
for h(y; z); see also Szenes and Vergne [15]. For instance, Barvinok considers
z →h(y; z) as the generating function (evaluated at z := ec ∈Cn) of the
indicator function x →IΩ(y)∩Zn(x) of the set Ω(y) ∩Zn and provides a decom-
position into a sum of simpler generating functions associated with supporting
cones (themselves having a signed decomposition into unimodular cones). We
call this a primal approach because y is ﬁxed, and one works in the primal space
Rn in which Ω(y) is deﬁned. Remarkably, Barvinok’s counting algorithm which
is implemented in the software LattE (see De Loera et al. [10]) runs in time
polynomial in the problem size when the dimension n is ﬁxed. The software
developed by Verdoolaege [17] extends the LattE software to handle paramet-
ric polytopes. On the other hand, Brion and Vergne [8] consider the generating
function H : Cm →C of y →h(y; z), that is,
w →H(w) :=

y∈Zm
h(y; z)wy =
n

k=1
1
1 −zkwAk .
(3)
They provide a generalized residue formula, and so obtain h(y; z) in closed form
by inversion. We call this latter approach dual because z is ﬁxed, and one works
in the space Cm of variables w associated with the m constraints Ax = y.
As a result of both primal and dual approaches, h(y; z) is ﬁnally expressed as
a weighted sum over the vertices of Ω(y). Similarly, Beck [5], and Beck, Diaz and
Robins [6] provided a complete analysis based on residue techniques for the case
of a tetrahedron (m = 1). Despite its theoretical interest, Brion and Vergne’s
formula is not directly tractable because it contains many products with com-
plex coeﬃcients (roots of unity) which makes the formula diﬃcult to evaluate
numerically. However, in some cases, this formula can be exploited to yield an
eﬃcient algorithm as e.g. in [2] for ﬂow polytopes, in [7] for transportation poly-
topes, and more generally when the matrix A is totally unimodular as in [9].
Finally, in [12,13], we have provided two algorithms based on Cauchy residue
techniques to invert H in (3), and an alternative algebraic technique based on
partial fraction expansion of H. A nice feature of the latter technique of [13] is
to avoid computing residues.
Contribution: Our contribution is twofold as it is concerned with both pri-
mal and dual approaches. On the primal side, we provide an explicit expression
of h(y; z) and an algorithm which involves only elementary operations. It uses
Brion’s identity along with an explicit description of the supporting cones at the
vertices of Ω(y). It also has a simple equivalent formulation as a (ﬁnite) group
problem. Finally, we exhibit ﬁnitely many ﬁxed convex cones of Rn, explicitly
and exclusively deﬁned from A, such that for any y ∈Zm, the sum h(y; z) is
obtained by a simple formula which evaluates  zx over the integral points of
those cones only.
On the dual side, we analyze the counting function h, via its generating func-
tion H in (3). Inverting H is diﬃcult in general, except if an appropriate ex-
pansion of H into simple fractions is available, as in e.g. [13]. In their landmark

Simple Explicit Formula for Counting Lattice Points of Polyhedra
369
paper [8], Brion and Vergne provided a generalized residue formula which yields
the generic expansion
H(w) =

σ∈JA

g∈Gσ
Qg,σ

k∈σ
[zkwAk]δk,σ
1 −ρgk
q [zkwAk]1/q .
(4)
Here, σ ∈JA whenever Aσ is invertible, q is the smallest common multiple
of all | det Aσ| ̸= 0, ρq = e2πi/q is the q-root of unity, δk,σ ∈{0, 1/q}, and
Qg,σ ∈C. The ﬁnite group Gσ has qm elements. The coeﬃcients Qg,σ are diﬃcult
to evaluate. Our contribution is to expand H in (3) in the form
H(w) =

σ∈JA
 
j∈σ
1
1 −zjwAj

×
1
R2(σ; z)

u̸σ∈Zn−m
μσ
zη[σ,u̸σ] wAη[σ,u̸σ],
(5)
where: Zμσ = {0, 1, . . ., μσ −1}, μσ = | det Aσ|, each η[σ, u̸σ] ∈Zn and:
z →R2(σ; z) :=

k/∈σ
	
1 −

zkz−A−1
σ Ak
σ
μσ
.
(6)
Identity (5) is a nontrivial simpliﬁcation of the residue formula (4) because the
η[σ, u̸σ]’s are given explicitly. And so the coeﬃcients of the rational fraction (5)
in w are very simple to evaluate with no root of unity involved (it can also be
done symbolically); however this task can be tedious as for each σ ∈JA one has
| det Aσ|n−m terms η[σ, u̸σ] to determine. But once determined, (5) is easy to
invert and provides h(y; z) for any y ∈Zm.
2
Brion’s Decomposition
2.1
Notation and Deﬁnitions
The notation C, R and Z stand for the usual sets of complex, real and integer
numbers, respectively. Moreover, the set of natural numbers {0, 1, 2, . . .} is de-
noted by N, and for every natural number μ ∈N, the ﬁnite set {0, 1, . . ., μ −1}
of cardinality μ is denoted by Zμ. The notation B′ stands for the transpose of
a matrix (or vector) B ∈Rs×t; and the kth column of the matrix B is denoted
by Bk := (B1,k, . . . , Bs,k)′. When y = 0, the cone Ω(0) in (1) is convex, and its
dual cone is given by,
Ω(0)∗:= {b ∈Rn | b′x ≥0 for every x ∈Ω(0)}.
(7)
Notice that Ω(0)∗≡Rn if Ω(0) = {0}, which is the case if Ω(y) is compact.
Deﬁnition 1. Let A ∈Zm×n be of maximal rank. An ordered set σ = {σ1, . . . ,
σm} of natural numbers is said to be a basis if it has cardinality |σ| = m, the
sequence of inequalities 1 ≤σ1 < σ2 < · · · < σm ≤n holds, and the square
[m × m] submatrix :
Aσ := [Aσ1|Aσ2| · · · |Aσm]
is invertible.
(8)
We denote the set of all bases σ by JA.

370
J.B. Lasserre and E.S. Zeron
Deﬁnition 2. Given a maximal rank matrix A ∈Zm×n, and any basis σ ∈JA,
the complementary matrices Aσ ∈Zm×n and A̸σ ∈Zm×(n−m) stand for [Ak]k∈σ
and [Ak]k/∈σ, respectively. Similarly, given z ∈Cn, the complementary vectors
zσ ∈Cm and z̸σ ∈Cn−m stand for (zk)k∈σ and (zk)k/∈σ, respectively.
For each basis σ ∈JA with associated matrix Aσ ∈Zm×m, introduce the indi-
cator function δσ : Zm →N deﬁned by :
y →δσ(y) :=

1 if A−1
σ y ∈Zm,
0 otherwise.
(9)
Notice that δσ is a multi-periodic function with periods Aσ and μσ := | det Aσ|,
meaning that δσ(y +Aσq) = δσ(y +μσq) = δσ(y) for all y, q ∈Zm. Finally, given
a triplet (z, x, u) ∈Cn × Zn × Rn, introduce the notation :
zx := zx1
1 zx2
2 · · · zsxn,
∥z∥:= max {|z1|, |z2|, . . . , |zn|},
ln⟨z⟩:= (ln(z1), ln(z2), . . . , ln(zn)).
(10)
Notice that zx = zxσ
σ zx̸σ
̸σ , for all bases σ ∈JA and all z ∈Cn, x ∈Zn.
2.2
Brion’s Decomposition
Let Ω(y) be the convex polyhedron in (1) with y ∈Zm, A ∈Zm×n being of
maximal rank, and let h : Zm →C be the counting function in (2), with ∥z∥< 1.
Obviously h(y; z) = 0 whenever the equation Ax = y has no solution x ∈Nn.
The main idea is to decompose the function h following Brion’s ideas. Given any
convex rational polyhedron P ⊂Rn, let [P] : Rn →{0, 1} be its characteristic
function, and f[P] : C →C its associated rational function, such that
z →f[P, z] :=

x∈P ∩Zn
zx,
(11)
holds whenever the sum converges absolutely. For every vertex V of P, deﬁne
Co(P, V ) ⊂Rn to be the supporting cone of P at V . Then, Brion’s formula
yields the decomposition :
[P] =

vertices V
[Co(P, V )],
(12)
modulo the group generated by the characteristic functions of convex polyhedra
which contain aﬃne lines. And so,
f[P, z] =

vertices V
f[Co(P, V ), z].
(13)
The above summation is formal because in general there is no z ∈Cn for
which the series

{zx | x ∈P ∩Zn}
and

{zx | x ∈Co(P, V ) ∩Zn}

Simple Explicit Formula for Counting Lattice Points of Polyhedra
371
converge absolutely for all vertices V . The notation  E stands for the sum of
all elements of a countable set E ⊂C. It is a complex number whenever the
resulting series converges absolutely; otherwise it stands for a formal series.
Example: Let P := [0, 1] ⊂R so that Co(P, {0}) = [0, +∞) and Co(P, {1}) =
(−∞, 1]. Simple enumeration yields f[P, z] = z0 + z = 1 + z, but one also has:
f[P, z] = f[Co(P, {0}), z] + f[(P, {1}), z] = 1/(1 −z) + z2/(z −1) = 1 + z.
3
Computing h(y; z): A Primal Approach
Let C(JA) := {Ax | x ∈Nn} ⊂Rm be the cone generated by the columns of A,
and for any basis σ ∈JA, let C(σ) ⊂Rm be the cone generated by the columns
Ak with k ∈σ. As A has maximal rank, C(JA) is the union of all C(σ), σ ∈JA.
With any y ∈C(JA) associate the intersection of all cones C(σ) that contain y.
This deﬁnes a subdivision of C(JA) into polyhedral cones. The interiors of the
maximal subdivisions are called chambers. In each chamber γ, the polyhedron
Ω(y) is simple, i.e. A−1
σ y > 0 for all σ ∈JA such that A−1
σ y ≥0.
For any chamber γ, deﬁne,
B(JA, γ) := {σ ∈JA | γ ⊂C(σ)}.
(14)
The intersection of all C(σ) with σ ∈B(JA, γ) is the closure γ of γ.
Back to our original problem, and setting P := Ω(y), the rational function
f[P, z] is equal to h(y; z) in (2) whenever ∥z∥< 1. We next provide an explicit
description of the rational function f[Co(P, V ), z] for every vertex V of P.
Let δσ be the function deﬁned in (9), and let Zμσ := {0, 1, . . ., μσ −1} with
μσ := | det Aσ|. A vector V ∈Rn is a vertex of P = Ω(y) if and only if there
exists a basis σ ∈JA such that :
Vσ = A−1
σ y ≥0
and
V̸σ = 0,
(15)
where Vσ and V̸σ are given in Deﬁnition 2. Moreover, the supporting cone of P
at the vertex V is described by :
Co(Ω(y), V ) := {x ∈Rn | Ax = y; xk ≥0 if Vk = 0} .
(16)
Let us now deﬁne the larger set
C(Ω(y), σ) := {x ∈Rn | Aσxσ + A̸σx̸σ = y; x̸σ ≥0},
(17)
so that Co(Ω(y), V ) is a subcone of C(Ω(y), σ) for all bases σ ∈JA and vertex
V of Ω(y) which satisfy V̸σ = 0 (recall (15)). Besides, when V̸σ = 0 and y ∈γ for
some chamber γ, then C(Ω(y), σ) and Co(Ω(y), V ) are identical because Ω(y)
is a simple polytope, and so A−1
σ y > 0 for all σ ∈JA.
Recall that Aσ ∈Zm×n and A̸σ ∈Zm×(n−m) stand for [Ak]k∈σ and [Ak]k/∈σ,
respectively. Similarly, given a vector x ∈Zn, the vectors xσ and x̸σ stand for
(xk)k∈σ and (xk)k/∈σ respectively. The following result is from [8, p. 818].

372
J.B. Lasserre and E.S. Zeron
Proposition 1. Let y ∈Rm and let Ω(y) be as in (1), and let y ∈γ for some
chamber γ. Then,
[Ω(y)] =

σ∈B(JA,γ)
[C(Ω(y), σ)],
(18)
modulo the group generated by the characteristic functions of convex polyhedra
which contain aﬃne lines.
Proof. Using notation of [8, p. 817], deﬁne the linear mapping p : Rn →Rm
with p(x) = Ax, so that the polyhedra PΔ(y) and Ω(y) are identical. Moreover,
for every basis σ ∈B(JA, γ), vσ : Rm →Rn is the linear mapping:
y →
[vσ(y)]σ = A−1
σ y
and
[vσ(y)]̸σ = 0,
y ∈Rm.
Finally, for every ˆx ∈Rn with ˆx ≥0, ρσ(ˆx) := ˆx −vσ(Aˆx) satisﬁes,
[ρσ(ˆx)]σ = −A−1
σ A̸σˆx̸σ
and
[ρσ(ˆx)]̸σ = ˆx̸σ.
Therefore, the cone [vσ(y) + ρσ(C)] in [8] is the set of points x ∈Rm such that
x̸σ ≥0 and xσ = A−1
σ (y −A̸σx̸σ); and so this cone is just [C(Ω(y), σ)] in (17).
Therefore a direct application of (3.2.1) in [8, p. 818] yields (18).
Theorem 1. Let y ∈Zm, z ∈Cn with ∥z∥< 1, and let y ∈γ for some chamber
γ. Recall the set of bases B(JA, γ) deﬁned in (14). With P := Ω(y), the rational
function h deﬁned in (2) can be written:
h(y; z) =

σ∈B(JA,γ)
f[C(Ω(y), σ), z] =

σ∈B(JA,γ)
R1(y, σ; z)
R2(σ; z) ,
(19)
with
z →R1(y, σ; z) := zA−1
σ y
σ

u∈Zn−m
μσ
δσ(y −A̸σu) zu
̸σ
zA−1
σ A̸σu
σ
,
(20)
and
z →R2(σ; z) :=

k/∈σ
	
1 −

zkz−A−1
σ Ak
σ
μσ
.
(21)
The pair {R1, R2} is well deﬁned whenever z ∈Cn satisﬁes zk ̸= 0 and zk ̸=
zσA−1
σ Ak for every basis σ ∈JA which does not contain the index k ̸∈σ.
Proof. By a direct application of Brion’s theorem to the sum (18), the associated
rational functions f[Ω(y), z] and f[C(Ω(y), σ), z] satisfy:
h(y, z) = f[Ω(y), z] =

σ∈B(JA,γ)
f[C(Ω(y), σ), z].
(22)
Therefore, in order to show (19), one only needs to prove that the rational
function R1(y,σ;z)
R2(σ;z) is equal to f[C(Ω(y), σ), z], i.e.,
R1(y, σ; z)
R2(σ; z)
=

{zx | x ∈C(Ω(y), σ) ∩Zn},
(23)

Simple Explicit Formula for Counting Lattice Points of Polyhedra
373
on the domain Dσ = {z ∈Cn | 1 > |zkz−A−1
σ Ak
σ
| for each k ̸∈σ}. Notice that
1
R2(σ; z) =

k/∈σ
1
1 −

zkz−A−1
σ Ak
σ
μσ =
=

k/∈σ

vk∈N

zk
zA−1
σ Ak
σ
μσvk
=

v∈Nn−m
zμσv
̸σ
zμσA−1
σ A̸σv
σ
,
on Dσ. On the other hand, according to (17), the integer vector x ∈Zn lies
inside the cone C(P, Vσ) if and only if :
xσ = A−1
σ (y −A̸σx̸σ),
δσ(y −A̸σx̸σ) = 1
and
x̸σ = u + μσv,
with
u ∈Zn−m
μσ
and
v ∈Nn−m.
From the deﬁnition (20) of R1(y, σ; z) and zx = zx̸σ
̸σ zxσ
σ
= zx̸σ
̸σ zA−1
σ (y−A̸σx̸σ)
σ
,
R1(y, σ; z)
R2(σ; z)
= zA−1
σ y
σ

u∈Zn−m
μσ

v∈Nn−m
δσ(y −A̸σu) zx̸σ
̸σ
zA−1
σ A̸σx̸σ
σ
,
(24)
=

{zx | x ∈C(Ω(y), σ) ∩Zn} = f[C(Ω(y)σ), z],
which is exactly (23). Notice that x̸σ = u + μσv, and so δσ(y −A̸σu) = δσ(y −
A̸σx̸σ) because of the deﬁnition (9) of δσ. Finally, using (24) in (22) yields that
(19) holds whenever ∥z∥< 1 and R1(y, σ; z) and R2(σ; z) are all well deﬁned.
Notice that R2 is constant with respect to y, and from the deﬁnition (9) of
δσ, R1 is quasiperiodic with periods Aσ and μσ, meaning that
R1(y + Aσq, σ; z) = R1(y, σ; z) zq
σ
and
R1(y + μσq, σ; z) = R1(y, σ; z)

zA−1
σ q
σ
μσ
(25)
hold for all y, q ∈Zm. Obviously, the more expensive part in calculating R2(·) in
(21) is to compute the determinant μσ = | det Aσ|. On the other hand, computing
R1(·) in (20) may become quite expensive when μσ is large, as one must evaluate
μn−m
σ
terms, the cardinality of Zn−m
μσ
. However, as detailed below, a more careful
analysis of (20) yields some simpliﬁcations.
3.1
Simpliﬁcations Via Group Theory
From the proof of Theorem 1, the closed forms (20)–(21) for R1(·) and R2(·) are
deduced from (24), i.e.,
R1(y, σ; z)
R2(σ; z)
= zA−1
σ y
σ

x̸σ∈Zn−m
δσ(y −A̸σx̸σ) zx̸σ
̸σ
zA−1
σ A̸σx̸σ
σ
,

374
J.B. Lasserre and E.S. Zeron
after setting x̸σ = u + μσv and recalling that δσ(y) is a periodic function, i.e.,
δσ(y + μσq) = δσ(y) for all y, q ∈Zm. However, we have not used yet that
δσ(y + Aσq) = δσ(y) as well. For every σ ∈JA, consider the lattice :
Λσ :=

j∈σ
AjZ ⊂Zm,
(26)
generated by the columns Aj, j ∈σ. The following quotient group
Gσ := Zm/Λσ = Zm 
j∈σ
AjZ
(27)
= {Ec[0, σ], Ec[2, σ], . . . , Ec[μσ −1, σ]}
is commutative, with μσ = | det Aσ| elements (or, equivalence classes) Ec[j, σ],
and so, Gσ is isomorphic to a ﬁnite Cartesian product of cyclic groups Zηk, i.e.,
Gσ ∼= Zη1 × Zη2 × · · · × Zηs.
Obviously, μσ = η1η2 · · · ηs, and so, Gσ is isomorphic to the cyclic group Zμσ
whenever μσ is a prime number. Actually, Gσ = {0} whenever μσ = 1. Notice
that the Cartesian product Zη1 × · · · × Zηs can be seen as the integer space Zs
modulo the vector η := (η1, η2, · · · , ηs)′ ∈Ns.
Hence, for every ﬁnite commutative group Gσ, there exist a positive integer
sσ ≥1, a vector ησ ∈Nsσ with positive entries, and a group isomorphism,
gσ : Gσ →Zsσ mod ησ,
(28)
where gσ(ξ) mod ησ means evaluating [gσ(ξ)]k mod [ησ]k, for all indices 1 ≤k ≤
sσ. For every y ∈Zm, there exists a unique equivalence class Ec[jy, σ] which
contains y, and so we can deﬁne the following group epimorphism,
ˆhσ : Zm →Zsσ
mod ησ,
(29)
y →ˆhσ(y) := gσ(Ec[jy, σ]).
On the other hand, the unit element of Gσ is the equivalence class Ec[0, σ]
which contains the origin, that is, Ec[0, σ] = {Aσq | q ∈Zm}.
Hence, ˆhσ(y) = 0 if and only if there exists q ∈Zm such that y = Aσq. We
can then redeﬁne the function δσ as follows,
y →δσ(y) :=

1 if ˆhσ(y) = 0,
0 otherwise,
(30)
One also needs the following additional notation; given any matrix B ∈Zm×t,
ˆhσ(B) := [ˆhσ(B1)|ˆhσ(B2)| · · · |ˆhσ(Bt)] ∈Zsσ×t.
(31)
And so, from (20), ˆhσ(y −A̸σu) ≡ˆhσ(y) −ˆhσ(A̸σ)u mod ησ. Finally, using
(30) in (20), one obtains a simpliﬁed version of R1(·) in the form:
R1(y, σ; z) =

zA−1
σ y
σ
zu
̸σ
zA−1
σ A̸σu
σ

u ∈Zn−m
μσ
;
ˆhσ(y) ≡ˆhσ(A̸σ)u mod ησ

.
(32)

Simple Explicit Formula for Counting Lattice Points of Polyhedra
375
Next, with q ∈Zm ﬁxed, νqA−1
σ q ∈Zm for some integer νq, if and only if
νqˆhσ(q) = 0 mod ησ. If we set νq = μσ, then μσA−1
σ q ∈Zm, and μσˆhσ(q) =
0 mod ησ, because Gσ has μσ = | det Aσ| elements. Nevertheless, μσ may not be
the smallest positive integer with that property. So, given σ ∈JA and k /∈σ,
deﬁne νk,σ ≥1 to be order of ˆhσ(Ak). That is, νk,σ is the smallest positive
integer such that νk,σˆhσ(Ak) = 0 mod ησ, or equivalently :
νk,σA−1
σ Ak ∈Zm.
(33)
Obviously νk,σ ≤μσ. Moreover, μσ is a multiple of νk,σ for it is the order of
an element in Gσ. For example, the group Z2 modulo η =

2
7

has 14 elements;
and the elements b1 =

1
0

, b2 =

0
1

and b3 =

1
1

have respective orders : 2, 7
and 14. Notice that, 2b1 ≡7b2 ≡14b3 ≡0 mod η. But, 2b3 ≡2b2 ̸≡0 and
7b3 ≡b1 ̸≡0 mod η.
The important observation is that δσ(y −νk,σAkq) = δσ(y) for all q ∈Zm and
k /∈σ, which follows from (33) and (9). Thus, following step by step the proof
of Theorem 1, we obtain:
Corollary 1. Let y ∈Zm, z ∈Cn with ∥z∥< 1, and let y ∈γ for some chamber
γ. Recall the set of bases B(JA, γ) deﬁned in (14). With σ ∈B(JA, γ), let R1 and
R2 be as in Theorem 1. Then
R1(y, σ; z)
R2(y; z)
= R∗
1(y, σ; z)
R∗
2(y; z) ,
(34)
where :
R∗
2(σ; z) :=

k/∈σ
	
1 −

zkz−A−1
σ Ak
σ
νk,σ
,
(35)
R∗
1(y, σ; z) := zA−1
σ y
σ

u̸σ∈U̸σ
δσ(y −A̸σu̸σ) zu̸σ
̸σ
zA−1
σ A̸σu̸σ
σ
=
(36)
=

zA−1
σ y
σ
zu̸σ
̸σ
zA−1
σ A̸σu̸σ
σ

u̸σ ∈U̸σ;
ˆhσ(y) ≡ˆhσ(A̸σ)u̸σ
mod ησ

,
with U̸σ := {u̸σ ∈Nn−m | 0 ≤uk ≤νk,σ −1; k /∈σ}.
One can also obtain (34) by noticing that:
R1(y, σ; z)
R∗
1(y, σ; z) = R2(σ; z)
R∗
2(σ; z) =

k/∈σ

1 + βνk,σ + · · · + βμσ−νk,σ
,
where βk,σ = zkz−A−1
σ Ak
σ
, and μσ is a multiple of νk,σ.

376
J.B. Lasserre and E.S. Zeron
3.2
Simpliﬁcations Via Finite Number of Generators
Decompose Zm into μσ := | det Aσ| disjoint equivalent classes, where y, ξ ∈Zm
are equivalent if and only if δσ(y −ξ) = 1. For every basis σ ∈JA, let Gσ be the
quotient group deﬁned in (27), that is,
Gσ = Zm 
j∈σ
AjZ = {Ec[0, σ], . . . , Ec[μσ −1, σ]}.
Notice that y, ξ ∈Zn belong to Ec[j, σ] if and only if A−1
σ (y −ξ) ∈Zn, and that
Zm is equal to the disjoint union of all classes Ec[j, σ].
Next, pick up a minimal representative element of every class, i.e., ﬁx
ξ[j, σ] ∈Ec[j, σ]
such that
A−1
σ y ≥A−1
σ ξ[j, σ] ≥0,
(37)
for every y ∈Ec[j, σ] with A−1
σ y ≥0. The minimal representative elements ξ[j, σ]
in (37) can be computed as follows: Let d ∈Ec[j, σ], arbitrary, and let d∗∈Zm
be such that his k-entry d∗
k is the smallest integer greater than or equal to the
k-entry of −A−1
σ d. The vector ξ[j, σ] deﬁned by d + Aσd∗satisﬁes (37).
Notice that d∗+ A−1
σ d ≥0. Besides, let d, y ∈Ec[j, σ] with A−1
σ y ≥0. There
exists q ∈Zm such that y = d + Aσq. Hence q ≥−A−1
σ d; in addition, q ≥d∗
follows from the above deﬁnition of d∗, and so A−1
σ y ≥d∗+ A−1
σ d ≥0.
Therefore, the vector ξ[j, σ] := d+Aσd∗satisﬁes (37). In particular, if Ec[0, σ]
is the class which contains the origin of Zm, then ξ[0, σ] = 0. Notice that for
every integer vector y ∈Zm, there exists a unique ξ[j, σ] such that :
y = ξ[j, σ] + Aσ q,
for
q ∈Zm.
Moreover, the extra condition A−1
σ y ≥0 holds if and only if:
y = ξ[j, σ] + Aσ q
with
q ∈Nm.
(38)
We obtain a compact form of h(y; z) when y ∈Zm ∩γ, for some chamber γ.
Theorem 2. Let h and ξ[j, σ] be as in (2) and (37), respectively. Let y ∈Zm∩γ,
for some chamber γ. Recall the set of bases B(JA, γ) deﬁned in (14). For every
basis σ ∈B(Δ, γ) there is a unique index 0 ≤j[y, σ] < μσ such that y is contained
in the equivalence class Ec[j[y, σ], σ] deﬁned in (27), and so:
h(y; z) =

σ∈B(Δ,γ)
R1(ξ[j[y, σ], σ], σ; z)
R2(σ; z)
z⌊A−1
σ y⌋
σ
,
(39)
where ⌊Aσ
−1y⌋∈Zm is such that his k-entry is the largest integer less than or
equal to the k-entry of A−1
σ y.
Proof. Recall that if y ∈Zm ∩γ
h(y; z) =

σ∈B(Δ,γ)
R1(y, σ; z)
R2(y; z)

Simple Explicit Formula for Counting Lattice Points of Polyhedra
377
Next, recalling the deﬁnition (14) of B(JA, γ), A−1
σ y ≥0 for every basis σ ∈
B(JA, γ) with y ∈γ. Recall that there is a unique index j[y, σ] < μσ such that
y = ξ[j[y, σ], σ] + Aσq with q ∈Nm; see (38) and the comment just before.
To obtain the vector q ∈Nm, recall that the minimal representative element
ξ[j[y, σ], σ] in (37) is the sum y + Aσy∗where y∗∈Zm is such that his k-entry
y∗
k is the smallest integer greater than or equal to −A−1
σ y, for we only need to
ﬁx d = y in the paragraph that follows (37). In particular, ⌊Aσ
−1y⌋= −y∗, and
ξ[j[y, σ], σ] = y −Aσ⌊Aσ
−1y⌋, which when used in (20) and (25), yields,
R1(y, σ; z) = R1(ξ[j[y, σ], σ], σ; z) z⌊A−1
σ y⌋
σ
.
And so (19) implies (39).
Theorem 2 explicitly shows that it suﬃces to compute R1(v, σ; z) for ﬁnitely
many values v = ξ[j, σ], with σ ∈B(Δ, γ) and 0 ≤j < μσ, in order to calculate
h(y; z) for arbitrary values y ∈Zm ∩γ, via (39).
In other words, in the closure γ of a chamber γ, one only needs to consider
ﬁnitely many ﬁxed convex cones C(Ω(ξ[j, σ]), σ) ⊂Rn, where σ ∈B(Δ, γ) and
0 ≤j < μσ, and compute their associated rational function (39). The counting
function h(y; z) is then obtained as follows.
Input: y ∈Zm ∩γ, z ∈Cn.
Output ρ = h(y; z).
Set ρ := 0. For every σ ∈B(Δ, γ) :
• Compute ξ[j[y, σ], σ] := y −Aσ⌊A−1
σ y⌋∈Zm.
• Read the value R1(ξ[j[y, σ], σ], σ; z)/R2(σ; z), and update ρ by:
ρ := ρ + R1(ξ[j[y, σ], σ], σ; z)
R2(σ; z)
z⌊A−1
σ y⌋
σ
.
For the whole space Zm it suﬃces to consider all chambers γ and all cones
C(Ω(ξ[j, σ]), σ) ⊂Rn, where σ ∈B(Δ, γ) and 0 ≤j < μσ.
Finally, in view of (20)-(21), the above algorithm can be symbolic, i.e., z ∈Cm
can be treated symbolically, and ρ becomes a rational fraction of z.
4
Generating Function
An appropriate tool for computing the exact value of h(y; z) in (2) is the formal
generating function H : Cm →C,
s →H(s) :=

y∈Zm
h(y; z) sy =
n

k=1
1
1 −zksAk ,
(40)
where sy is deﬁned in (10) and the sum is understood as a formal power series, so
that we need not consider conditions for convergence. This generating function
was already considered in Brion and Vergne [8] with λ = ln⟨s⟩.

378
J.B. Lasserre and E.S. Zeron
Following notation of [8, p. 805], let 0 ≤ˆx ∈Rn be a regular vector, i.e., no
entry [A−1
σ Aˆx]j vanishes for any basis σ ∈JA or index 1 ≤j ≤m. Deﬁne :
εj,σ :=

1 if
[A−1
σ Aˆx]j > 0,
−1 if
[A−1
σ Aˆx]j < 0.
(41)
Next, for every basis σ ∈JA, index j ∈σ and vector u̸σ ∈Zn−m, ﬁx :
θ[j, σ, u̸σ] ∈Z :
the smallest integer greater
than or equal to
−εj,σ[A−1
σ A̸σu̸σ]j.
(42)
Deﬁne also the vector η[σ, u̸σ] ∈Zn by :
η[σ, u̸σ]j =
⎧
⎨
⎩
uj
if j ̸∈σ;
θ[j, σ, u̸σ]
if j ∈σ, εj,σ = 1;
1 −θ[j, σ, u̸σ] if j ∈σ, εj,σ = −1.
(43)
The following expansion can be deduced from [8].
Theorem 3. Let 0 ≤ˆx ∈Rn be regular and consider the vectors η[σ, u̸σ] ∈Zn
deﬁned in (43) for σ ∈JA and u̸σ ∈Zn−m. The following expansion holds:
n

k=1
1
1 −zksAk =

σ∈JA
 
j∈σ
1
1 −zjsAj

×
(44)
×
1
R2(σ; z)

u̸σ∈Zn−m
μσ
zη[σ,u̸σ] sAη[σ,u̸σ],
where Zμσ = {0, 1, . . ., μσ −1}, μσ = | det Aσ| and:
z →R2(σ; z) :=

k/∈σ
	
1 −

zkz−A−1
σ Ak
σ
μσ
.
(45)
Proof. From Brion and Vergne’s identity [8, p. 813],
n

j=1
1
1 −ewk =

σ∈JA
 
j∈σ
εj,σ

F(Cσ
ˆx + ρσ(C), L),
(46)
where F(Cσ
ˆx + ρσ(C), L) is the formal power series 
l el added over all el-
ements l in the intersection of the cone Cσ
ˆx + ρσ(C) with the integer lattice
L = Z[w1, . . . , wn]. Moreover, the coeﬃcients εj,σ are deﬁned in (41) and the
cone Cσ
ˆx is deﬁned by the following formula [8, p. 805],
Cσ
ˆx =
 
j∈σ
εj,σ xj wj
 xσ ∈Rm, xσ ≥0

.
(47)
Finally, given the real vector space W = R[w1, . . . , wn], every ρσ : W →W is
a linear mapping deﬁned by its action on each basis element wk of W,
ρσ(wk) := wk −

j∈σ
[A−1
σ Ak]jwj.
(48)

Simple Explicit Formula for Counting Lattice Points of Polyhedra
379
Hence, ρσ(wj) = 0 for every j ∈σ, and the cone ρσ(C) is given by
ρσ(C) =
 
k̸∈σ
xkwk −

j∈σ
[A−1
σ A̸σx̸σ]jwj

x̸σ ∈Rn−m,
x̸σ ≥0

;
(49)
see [8, p.805]. Thus, every element in the intersection of the cone Cσ
ˆx + ρσ(C)
with the lattice Z[w1, . . . , wn] must be of the form :

k̸∈σ
xkwk +

j∈σ
εj,σ ξj wj,
with
x̸σ ∈Nn−m,
(50)
ξσ ∈Zm
and
ξj ≥−εj,σ[A−1
σ A̸σx̸σ]j.
On the other hand, for every basis σ, deﬁne μσ = | det Aσ| and :
x̸σ = u̸σ + μσv̸σ,
with
u̸σ ∈Zn−m
μσ
and
v̸σ ∈Nn−m.
(51)
Moreover, as in (42), ﬁx θ[j, σ, u̸σ] ∈Z to be the smallest integer greater than
or equal to −εj,σ[A−1
σ A̸σu̸σ]j. Thus, we can rewrite (50) so that the intersection
of the cone Cσ
ˆx + ρσ(C) with the lattice Z[w1, . . . , wn] must be of the form :

k̸∈σ

ukwk + vkμσρ(wk)

+

j∈σ
εj,σwj

θ[j, σ, u̸σ]j + qj

,
(52)
with
u̸σ ∈Zn−m
μσ
,
v̸σ ∈Nn−m
and
qσ ∈Nm.
We can deduce (52) from (50) by recalling the deﬁnition (48) of ρσ(wk) and
letting :
ξj := θ[j, σ, u̸σ] + qj −εj,σμσ[A−1
σ A̸σv̸σ]j.
Since F(Cσ
ˆx +ρσ(C), L) is the formal power series 
l el with summation over
all elements l in (52), one obtains
F(Cσ
ˆx + ρσ(C), L) =
(53)

u̸σ∈Zn−m
μσ
 
j∈σ
eεj,σθ[j,σ,u̸σ]wj
1 −eεj,σwj
 
k̸∈σ
eukwk
1 −eμσρσ(wk)

.
With η[σ, u̸σ] ∈Zn being as in (43), using (53) into (46) yields the expansion
n

j=1
1
1 −ewk =

σ∈JA

u̸σ∈Zn−m
μσ
 
j∈σ
1
1 −ewj

×
(54)
×

n

j=1
eη[σ,u̸σ]jwj
 
k̸∈σ
1
1 −eμσρσ(wk)

.
Finally, we deﬁned wk := ln(zk) + ln⟨s⟩Ak for every index 1 ≤k ≤n, where
the vectors s, z ∈Cn have all their entries diﬀerent from zero and ln⟨s⟩is the

380
J.B. Lasserre and E.S. Zeron
[1×n] matrix deﬁned in (10). So ewk = zksAk. Moreover, recalling the deﬁnition
(48) of ρσ(wk), the following identities hold for all 1 ≤k ≤n,
ρσ(wk) = ln(zk) −

j∈σ
ln(zj)[A−1
σ Ak]j.
(55)
Notice 
j∈σ Aj[A−1
σ Ak]j = Ak. A direct application of (55) and the identities
ewk = zksAk yields (44), i.e.:
n

k=1
1
1 −zksAk =

σ∈JA

u̸σ∈Zn−m
μσ
zη[σ,u̸σ]sAη[σ,u̸σ]
R2(σ; z)

j∈σ
1
1 −zjsAj ,
with
R2(σ; z) =

k/∈σ
	
1 −

zkz−A−1
σ Ak
σ
μσ
.
A direct expansion of (44) yields the following:
Theorem 4. Let 0 ≤ˆx ∈Rn be regular, and let h and η be as in (2) and (43),
respectively. Let JA be the set of bases associated with A. Then for every pair of
(y, z) ∈Zm × Cn with ∥z∥< 1:
h(y; z) =

σ∈JA, A−1
σ y≥0
zA−1
σ y
σ
R2(σ; z)

u∈Zn−m
μσ
zu
̸σ
zA−1
σ A̸σu
σ
×
(56)
×

1 if A−1
σ

y −Aη[σ, u]

∈Nm,
0 otherwise,
where:
Zμσ = {0, 1, . . ., μσ −1}, μσ = | det Aσ|,
0 ≤

A−1
σ Aη[σ, u]

j ≤1
for each
j ∈σ
(57)
and
R2(σ; z) :=

k/∈σ
	
1 −

zkz−A−1
σ Ak
σ
μσ
.
(58)
The proof is based on arguments similar to those developed in [13].
Observe that (56) is diﬀerent from (19) or (34) because in (19) and (34) the
summation is over bases σ in the subset B(Δ, γ) ⊂{JA; A−1
σ y ≥0}.
References
1. Baldoni-Silva, W., Vergne, M.: Residues formulae for volumes and Ehrhart poly-
nomials of convex polytopes. arXiv:math.CO/0103097 v1, 2001.
2. Baldoni-Silva, W., De Loera, J.A., Vergne, M.: Counting integer ﬂows in networks.
Found. Comput. Math. 4 (2004), 277–314.
3. Barvinok, A.I.: Computing the volume, counting integral points and exponentials
sums. Discr. Comp. Geom. 10 (1993), 123–141.

Simple Explicit Formula for Counting Lattice Points of Polyhedra
381
4. Barvinok, A.I., Pommersheim, J.E.: An algorithmic theory of lattice points in
polyhedral. in: New Perspectives in Algebraic Combinatorics, MSRI Publications
38 (1999), 91–147.
5. Beck, M.: Counting Lattice Points by means of the Residue Theorem. Ramanujan
Journal 4 (2000), 399–310.
6. Beck, M., Diaz, R., Robins, S.: The Frobenius problem, rational polytopes, and
Fourier-Dedekind sums. J. Numb. Theor. 96 (2002), 1–21.
7. Beck, M., Pixton, D.: The Ehrhart polynomial of the Birkhoﬀpolytope. Discr.
Comp. Math. 30 (2003), 623–637.
8. Brion, M., Vergne, M.: Residue formulae, vector partition functions and lattice
points in rational polytopes. J. Amer. Math. Soc. 10 (1997), 797–833.
9. Cochet, C.: R´eduction des graphes de Goretsky-Kottwitz-MacPherson; nom-
bres de Kostka et coeﬃcients de Littlewodd-Richardson. Th`ese de Doctorat:
Math´ematiques, Universit´e Paris 7, Paris, D´ecembre 2003.
10. De Loera, J.A., R. Hemmecke, R., Tauzer, J., Yoshida, R.: Eﬀective lattice point
counting in rational convex polytopes. J. of Symb. Comp., to appear.
11. Pukhlikov, A.V., Khovanskii, A.G.: A Riemann-Roch theorem for integrals and
sums of quasipolynomials over virtual polytopes. St. Petersburg Math. J. 4 (1993),
789–812.
12. Lasserre, J.B., E.S. Zeron, E.S.: On counting integral points in a convex rational
polytope. Math. Oper. Res. 28 (2003), 853–870.
13. Lasserre, J.B., Zeron, E.S.: An alternative algorithm for counting lattice points in
a convex polytope. Math. Oper. Res. 30 (2005), 597–614.
14. Schrijver, A.: Theory of Linear and Integer Programming. John Wiley & Sons,
Chichester, 1986.
15. Szenes, A., Vergne, M.: Residue formulae for vector partitions and Euler-MacLaurin
sums. Adv. in Appl. Math.30 (2003), 295–342.
16. Szenes, A.: Residue theorem for rational trigonometric sums and Verlinde’s for-
mula. Duke Math. J. 118 (2003), 189–227.
17. Verdoolaege, S., Beyls, K., Bruynooghe, M., Seghir, R., Loechner, V.: Analytical
Computation of Ehrhart Polynomials and its Applications for Embedded Systems.
Technical report # 376, Computer Science Department, KUL University, Leuwen,
Belgium.

Characterizations of Total Dual Integrality⋆
Edwin O’Shea1 and Andr´as Seb˝o2
1 Department of Mathematics, University of Kentucky, Lexington,
KY 40506-0027, USA
oshea@ms.uky.edu
2 CNRS, Laboratoire G-SCOP, 46, Avenue F´elix Viallet, 38000 Grenoble 38031
Grenoble, Cedex 1, France
Andras.Sebo@g-scop.inpg.fr
Abstract. In this paper we provide new characterizing properties of
TDI systems. A corollary is Sturmfels’ theorem relating toric initial ide-
als generated by square-free monomials to unimodular triangulations. A
reformulation of these test-sets to polynomial ideals actually generalizes
the existence of square-free monomials to arbitrary TDI systems, pro-
viding new relations between integer programming and Gr¨obner bases of
toric ideals. We ﬁnally show that stable set polytopes of perfect graphs
are characterized by a reﬁned fan that is a triangulation consisting only
of unimodular cones, a fact that endows the Weak Perfect Graph Theo-
rem with a computationally advantageous geometric feature. Three ways
of implementing the results are described and some experience about one
of these is reported.
1
Introduction
Let A = [a1 a2 · · · an] ∈Zd×n and assume that A has rank d. With an abuse
of notation the ordered vector conﬁguration consisting of the columns of A will
also be denoted by A. For every σ ⊆[n] := {1, . . . , n} we have the d×|σ| matrix
Aσ given by the columns of A indexed by σ. Let cone(A), ZA and NA denote
the non-negative real, integer and non-negative integer span of A respectively
and assume that ZA = Zd.
Fixing c ∈Rn, for each b ∈Rd the linear program (or primal program)
LPA,c(b) and its dual program DPA,c(b) are deﬁned by
LPA,c(b) := minimize { c · x : Ax = b, x ≥0 }
and DPA,c(b) := maximize { y · b : yA ≤c }. Let Pb and Qc denote the
feasible regions of LPA,c(b) and DPA,c(b) respectively. Note that the linear
program LPA,c(b) is feasible if and only if b ∈cone(A). We refer to Schrijver
[21] for basic terminology and facts about linear programming.
⋆The ﬁrst author was supported by a Fulbright grant and by NSF grants DMS-
9983797 and DMS-0401047. The research of the second author was supported by the
“Marie Curie Training Network” ADONET of the European Community.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 382–396, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

Characterizations of Total Dual Integrality
383
The integer program is deﬁned as
IPA,c(b) := minimize { c · x : Ax = b, x ∈Nn }.
We say that c ∈Rn is generic for A if the integer program IPA,c(b) has a unique
optimal solution for all b ∈NA. In this case, each linear program LPA,c(b) also
has a unique optimal solution for all b ∈cone(A) but the converse is not true
in general. (However, for TDI systems the two are equivalent.)
The system yA ≤c is totally dual integral (TDI) if LPA,c(b) has an integer
optimal solution x ∈Nn for each b ∈cone(A) ∩Zd. In other words, the system
yA ≤c is TDI exactly if the optima of LPA,c(b) and of IPA,c(b) coincide for all
b ∈cone(A) ∩Zd. This is a slight twist of notation when compared to habits in
combinatorial optimization: we deﬁned the TDI property for the dual problem.
We do this in order to be in accordance with notations in computational algebra.
Totally dual integral (TDI) systems of linear inequalities play a central role
in combinatorial optimization. The recognition of TDI systems and the task of
eﬃciently solving integer linear programs constrained by TDI systems of inequal-
ities and their duals are among the main challenges of the ﬁeld. This problem
is open even for generic systems (Problem 1). Recent graph theory results of
Chudnovsky, Cornu´ejols, Xinming and Vuˇskovi´c [7] allows one to recognize TDI
systems with 0−1 coeﬃcient matrices A and right hand sides b. However, solving
the corresponding dual pair of integer linear programs (including the coloration
of perfect graphs) in polynomial time with combinatorial algorithms remains
open even in this special case.
In Section 2, new characterizing properties of TDI systems are provided. These
properties involve tools from both combinatorial optimization and computational
algebra. Section 3 specializes these results to integral set packing polytopes.
Finally, Section 4 will exhibit the utility of the computational algebraic tools in
recognizing TDI systems.
If A is a matrix whose ﬁrst d × (n −d) submatrix is a 0 −1 matrix and whose
last d × d submatrix is −Id, and c is all 1 except for the last d coordinates
which are 0, then DPA,c(b) is called a set packing problem, and Qc a set packing
polytope. We will show that if the set packing polytope is integral then the
lexicographic perturbation technique of linear programming can be used to make
the set packing polytope non-degenerate while keeping TDI-ness. This means
that the normal fan of the set packing polytope has a reﬁnement which is a
unimodular triangulation, and this does not hold for TDI systems in general.
The remainder of this introduction is devoted to providing some background.
A collection of subsets {σ1, . . . , σt} of [n] will be called a regular subdivision of
A if there exists c ∈Rn, and z1, . . . , zt ∈Rd, such that zi · aj = cj for all j ∈σi
and zi·aj < cj for all j /∈σi. The sets σ1, . . . , σt are called the cells of the regular
subdivision and the regular subdivision is denoted by Δc(A) = {σ1, . . . , σt} or
simply Δc when A is unambiguous.
Equivalently, regular subdivisions are simply capturing complementary slack-
ness from linear programming. Namely, a feasible solution to LPA,c(b) is optimal

384
E. O’Shea and A. Seb˝o
if and only if the support of the feasible solution is a subset of some cell of Δc.
Geometrically, Δc can be thought of as a partition of cone(A) by the inclusion-
wise maximal ones among the cones cone(Aσ1), . . . , cone(Aσt); each such cone
is generated by the normal vectors of deﬁning inequalities of faces of Qc, each
maximal cell indexes the set of normal vectors of deﬁning inequalities of a vertex
(or minimal face) of Qc. So the regular subdivision Δc is geometrically realized
as the normal fan of Qc.
A regular subdivision of A is called a triangulation if the columns of each Aσi
are linearly independent for all i = 1, . . . , t. Note that a regular subdivision Δc
is a triangulation if and only if every vertex is contained in exactly d facets;
that is, the polyhedron Qc is simple, or, non-degenerate. A triangulation Δc is
called unimodular if det(σi) = ±1 for each maximal cell of Δc. The reﬁnement
of a subdivision Δc of A is another subdivision Δc′ of A so that each cell of
Δc′ is contained in some cell of Δc. A vector conﬁguration B ⊂Zd is a Hilbert
basis if NB = cone(B) ∩Zd. Note that if for some c ∈Rn Δc is a unimodular
triangulation of A then Cramer’s rule implies that A itself is a Hilbert basis.
A simple but helpful characterization of the TDI property in terms of the
Hilbert basis property of regular subdivisions has been provided by Schrijver [21].
We prove another elementary characterization in Section 2 in terms of test-sets:
Let IPA,c := {IPA,c(b) : b ∈NA} denote the family of integer programs
IPA,c(b) having a feasible solution. Informally, a test set for the family of integer
programs IPA,c is a ﬁnite collection of integer vectors, called test vectors, with
the property that any non-optimal feasible solution can be improved (in objective
value) by subtracting a test vector from it. Test sets for the family of integer
programs IPA,c were ﬁrst introduced by Graver [13].
Theorem 1 (one of the equivalences). A system of linear inequalities is TDI if
and only if its coeﬃcient vectors form a Hilbert basis, and there exists a test set
for IPA,c where all test vectors have positive entries equal to 1, and a linearly
independent positive support.1
This simple result has the virtue of presenting a not too big test-set: there is
at most one test-vector for each at most d element subset of {1, . . . , n}, so the
number of test-vectors is O(nd). This will allow to deduce shortly Cook, Lov´asz
and Schrijver’s result on testing for TDI in ﬁx dimension, providing a short proof
for this result.
It also has the other virtue that it has a nice and useful reformulation to
polynomial ideals. This reformulation generalizes a well-known algebraic result
proved by Sturmfels [26, Corollary 8.9] relating toric initial ideals to unimod-
ular triangulations. The basic connections between integer programming and
1 In oral and electronic communication the condition on test-sets was replaced by the
following still equivalent condition: “A system of linear inequalities is TDI if and only
if the coeﬃcient vectors form a Hilbert basis, and there exists an integer dual solution
for objective functions that are sums of linearly independent coeﬃcient vectors”,
implying TDI test in ﬁx dimension [5], in practically all interesting cases. This is
just another wording of Applegate, Cook and McCormick’s Theorem 2 (Operations
Research Letters 10 (1991) 37–41), as we learnt from several colleagues.

Characterizations of Total Dual Integrality
385
computational algebra was initiated by Conti and Traverso [3] and studied by
Sturmfels and Thomas, Weismantel and Ziegler and further explained from var-
ious viewpoints in [26], [25], [27] and [28]. Knowledge of this algebraic viewpoint
will not be assumed and a useful part will be described in Section 2.
In Section 3 we show that the converse of the following fact (explained at
the end of Section 2) holds for normal fans of integral set packing polytopes: if
c, c′ ∈Rn are such that Δc′ is a reﬁnement of Δc, where Δc′ is a unimodular
triangulation, then yA ≤c is TDI. In general, the converse does not hold.
Thus Schrijver’s above mentioned result cannot necessarily be strengthened by
asserting a unimodular reﬁnement of A. In general, the most that is known in
this direction is the existence of just one full dimensional subset of the columns
of A which is unimodular [11]. Not even a “unimodular covering” of a Hilbert
basis may be possible [1]. However, the converse does hold for normal fans of
integral set packing polytopes. More precisely, the main result of Section 3 is the
following:
Theorem 2. Given a set-packing problem deﬁned by A and c, Qc has integer
vertices if and only if there exists c′ such that Δc′ is a reﬁnement of the normal
fan Δc of Qc, where Δc′ is a unimodular triangulation.
The proof relies on the basic idea of Fulkerson’s famous “pluperfect graph theo-
rem” [12] stating that the integrality of such polyhedra implies their total dual
integrality in a very simple “greedy” way. Chandrasekaran and Tamir [2] and
Cook, Fonlupt and Schrijver [4] exploited Fulkerson’s method by pointing out
its lexicographic or advantageous Caratheodory feature. In [23, §4] it is noticed
with the same method that the active rows of the dual of integral set packing
polyhedra (the cells of their normal fan) have a unimodular subdivision, which
can be rephrased as follows: the normal fan of integral set packing polyhedra has a
unimodular reﬁnement. However, the proof of the regularity of such a reﬁnement
appears for the ﬁrst time in the present work.
These results oﬀer three methods for recognizing TDI systems, explained and
illustrated in Section 4.
2
TDI Systems
In this section we provide some new characterizations of TDI systems. We show
the equivalence of ﬁve properties, three polyhedral (one of them is the TDI
property) and two concern polynomial ideals. A third property is also equivalent
to these in the generic case.
While the proofs of the equivalences of the three polyhedral properties use
merely polyhedral arguments, the last among them – (iii) – has an appealing
reformulation into the language of polynomial ideals. Therefore, we start this
section by introducing the necessary background on polynomial ideals; namely,

386
E. O’Shea and A. Seb˝o
toric ideals, their initial ideals and Gr¨obner bases. The characterizations of TDI
systems involving polynomial ideals are useful generalizations of known results
in computational algebra. See [8] and [26] for further background.
An ideal I in a polynomial ring R := k[x1, . . . , xn] is an R-vector subspace
with the property that I · R = I. It was proven by Hilbert that every ideal is
ﬁnitely generated. That is, given an ideal I there exists a ﬁnite set of polynomials
f1, . . . , ft ∈I such that for every f ∈I there exists h1, . . . , ht ∈R with f =
h1f1 + · · ·+ htft. We call such a collection f1, . . . , ft ∈I a generating set for the
ideal I and denote this by I = ⟨f1, . . . , ft⟩. For the monomials in R we write
xu = xu1
1 · · · xun
n
for the sake of brevity. We call u the exponent vector of xu.
A monomial xu is said to be square-free if u ∈{0, 1}n. An ideal is called a
monomial ideal if it has a generating set consisting only of monomials. For any
ideal J of R, mono(J) denotes the largest monomial ideal in R contained in J.
Alternatively, mono(J) is the ideal generated by all monomials in J. There is an
algorithm [20, Algorithm 4.4.2] for computing the generators of the monomial
ideal mono(J).
Every weight vector c ∈Rn induces a partial order ⪰on the monomials in R
via xu ⪰xv if c·u ≥c·v. If c ∈Rn where 1 is the monomial of minimum c-cost
(that is, c · u ≥0 for every monomial xu), then we can deﬁne initial terms and
initial ideals. Given a polynomial f = 
u∈Nn ruxu ∈I the initial term of f with
respect to c, is denoted by inc(f), and equals the sum of all ruxu of f, where c·u
is maximum. The initial ideal of I with respect to c is deﬁned as the ideal in R
generated by the initial terms of the polynomials in I: inc(I) := ⟨inc(f) : f ∈I ⟩.
A Gr¨obner basis of an ideal I with respect to c, is a ﬁnite collection of elements
g1, . . . , gs in I such that inc(I) = ⟨inc(g1), inc(g2), . . . , inc(gs) ⟩. Every Gr¨obner
basis is a generating set for the ideal I.
If inc(I) is a monomial ideal then a Gr¨obner basis is reduced if for every i ̸= j,
no term of gi is divisible by inc(gj). The reduced Gr¨obner basis is unique. In this
case, the set of monomials in inc(I) equal {xu : u ∈U} with U := D +Nn where
D is the set of exponent vectors of the monomials inc(g1), inc(g2), . . . , inc(gs).
Dickson’s lemma states that sets of the form D + Nn, where D is arbitrary have
only a ﬁnite number of minimal elements (with respect to coordinate wise in-
equalities). This is an alternative proof to Hilbert’s result that every polynomial
ideal is ﬁnitely generated. In this case, the Gr¨obner basis also provides a gen-
eralization of the Euclidean algorithm for polynomial rings with two or more
variables called Buchberger’s algorithm (see [8]). This algorithm solves the ideal
membership problem: decide if a given polynomial is in an ideal or not. However,
a Gr¨obner basis for an ideal can have many elements (relative to a minimal
generating set for the ideal).
The toric ideal of A is the ideal IA = ⟨xu −xv : Au = Av, u, v ∈Nn ⟩
and is called a binomial ideal since it is generated by polynomials having at
most terms. Every reduced Gr¨obner basis of a toric ideal consists of binomials.
A toric initial ideal is any initial ideal of a toric ideal. The following lemma is a
natural connection between integer programming and toric initial ideals.

Characterizations of Total Dual Integrality
387
Lemma 1. [20, Lemma 4.4.7] Let A ∈Zd×n and c ∈Rn. Then the monomial
ideal mono(inc(IA)) is equal to
⟨xω : ω ∈Nn is non-optimal solution for IPA,c(Aω) ⟩.
One direction of the proof of Lemma 1 is straightforward: let ω be a non-optimal
solution, and ω′ an optimal solution to IPA,c(Aω). Then xω −xω′ ∈IA is a
binomial with xω as its initial term with respect to c and xω is a monomial in
mono(inc(IA)). Our proof of the converse made essential use of Gr¨obner bases,
and was longer, it is intuitive enough to be used without proof with the reference
[20, Lemma 4.4.7] in the background.
A test set for the family of integer programs IPA,c is a collection of integer
vectors {v+
i −v−
i
: Av+
i = Av−
i , v+
i , v−
i ∈Nn, i = 1, . . . , s} with the property
that u is a feasible, non-optimal solution to IPA,c(b) if and only if there exists an
i, 1 ≤i ≤s, such that u−(v+
i −v−
i ) ≥0. We can now state our characterizations:
Theorem 1. Fix A ∈Zd×n and c ∈Rn, where A is a Hilbert basis. The follow-
ing statements are equivalent:
(i) The system yA ≤c is TDI.
(ii) The subconﬁguration Aσ of A is a Hilbert basis for every cell σ in Δc.
(iii) There exists a test-set for IPA,c where all the positive coordinates are equal
to 1, the positive support consists of linearly independent columns, (and the
negative support is a subset of a cell of Δc).
(iv) The monomial ideal ⟨xω : ω ∈Nn is not an optimal solution for IPA,c(Aω) ⟩
has a square-free generating set.
(v) The monomial ideal generated by the set of monomials in inc(IA) has a
square-free generating set, that is, mono(inc(IA)) has a square-free generat-
ing set.
Proof. (i) is equivalent to (ii) : This is well-known from Schrijver’s work, (see
for instance [21]), but we provide the (very simple) proof here for the sake of
completeness: Suppose the system yA ≤c is TDI, and let σ ∈Δc. We show that
Aσ is a Hilbert basis. Let b ∈cone(Aσ). Since the optimal solutions for LPA,c(b)
are exactly the non-negative combinations of the columns of Aσ with result b,
the TDI property means exactly that b can also be written as a non-negative
integer combination of columns in Aσ, as claimed.
(ii) implies (iii) : Suppose (ii) holds true for Δc of A. For every τ ⊆[n] with τ
not contained in any cell of Δc, let bτ := 
i∈τ ai = A(
i∈τ ei). Since τ is not
contained in any cell of Δc, there exists an optimal solution βτ to LPA,c(bτ)
with c · βτ < c · 
i∈τ ei. By the optimality of βτ we must have supp(βτ) ⊆σ
for some σ a cell of Δc. Since (ii) holds Aσ is a Hilbert basis for every cell of Δc
and therefore βτ can be chosen to be an integral vector. Let
TA,c := {

i∈τ
ei −βτ : τ not contained in any cell ofΔc }.

388
E. O’Shea and A. Seb˝o
We claim that TA,c is a test set for IPA,c. Suppose b ∈Zd and ω ∈Nn satisﬁes
Aω = b. That is, ω is a feasible solution to LPA,c(b).
If ω is an optimal solution then supp(ω) is contained in a cell in Δc. Thus no
vector in TA,c can be subtracted from it and remain in Nn. Conversely, if ω is not
an optimal solution to LPA,c(b) then supp(ω) ⊆[n] is not contained in any cell σ
of Δc and so by basic linear programming there exists τ ⊆supp(ω), Aτ is linearly
independent which is also not contained in any cell. ω −(
i∈τ ei −βτ) ≥0.
Note that this integer vector is cheaper than ω with respect to c.
(iii) implies (i): Suppose (iii) is true but for some b ∈cone(A) the linear
program LPA,c(b) does not have an integer optimal solution. Let ω ∈Nn be the
optimal solution to the integer program IPA,c(b) and let α/D be the optimal
solution to LPA,c(b) where α ∈Nn, and D is a positive integer. Since LPA,c(b)
does not have an integer optimal solution, we have c · α/D < c · ω. This also
implies that Dω is not an optimal solution to IPA,c(Db).
By (iii) there exists a test set for solving the integer program IPA,c(Db) and so
there exists a γ+−γ−with γ+ ∈{0, 1}n and γ−∈Nn such that c·(γ+−γ−) > 0
and with Dω −(γ+ −γ−) ∈Nn. Hence, supp(γ+) ⊆supp(Dω) = supp(ω).
Since the value of all elements in γ+ is 0 or 1 then we also have ω ≥γ+, so
ω−(γ+−γ−) ∈Nn is also a feasible solution to IPA,c(b) with c·(ω−(γ+−γ−)) <
c · ω, in contradiction to the optimality of ω.
(iii) is equivalent to (iv): Both (iii) and (iv) can be reformulated as follows:
If ω ∈Nn is not an optimal solution to LPA,c(Aω) then the vector ω′ deﬁned
as ω′
i := 1 if i ∈supp(ω) and 0 otherwise is also a non-optimal solution to
LPA,c(Aω′).
(iv) is equivalent to (v): This is a special case of Lemma 1.
⊓⊔
Recall that we deﬁned c ∈Rn to be generic with the ﬁrst of the following
conditions, but the others are also equivalent to the deﬁnition [28]:
– The integer program IPA,c(b) has a unique optimal solution for all b ∈NA.
– The toric initial ideal inc(IA) is a monomial ideal.
– There exists a Gr¨obner basis {xu+
1 −xu−
1 , . . . , xu+
s −xu−
s } of IA with c·u+
i >
c · u−
i for each i = 1, . . . , s.
In the generic case, by Cramer’s rule, (ii) is equivalent to Δc being a unimod-
ular triangulation which gives the following corollary.
Corollary 1. (Sturmfels) [26, Corollary 8.9] Let A ∈Zd×n and let c ∈Rn be
generic with respect to A. Then Δc is a unimodular triangulation if and only if
the toric initial ideal inc(IA) is generated by square-free monomials.
Still concerning generic c it is worth to note the following result of Conti and
Traverso which provides another connection between integer linear programming
and Gr¨obner bases. Here we think of an element xv+ −xv−in the reduced
Gr¨obner basis as a vector v+ −v−.

Characterizations of Total Dual Integrality
389
Proposition 1. (Conti-Traverso) [2] – see [29, Lemma 3]
If IPA,c(b) has a
unique optimal solution for every b ∈NA then the reduced Gr¨obner basis is a
minimal test set for the family of integer programs IPA,c.
This proposition means for us that in the generic case the following (vi) can be
added to Theorem 1:
(vi) The initial terms in the reduced Gr¨obner basis are square-free.
In particular, in the generic case of condition (iii) of Theorem 1 the unique
inclusion wise minimal test set is deﬁned by the reduced Gr¨obner basis, which,
by (vi) has only square-free terms initial terms.
As is typically the case in combinatorial optimization, the cost vector c is not
generic for A. Theorem 1 was found by a desire to generalize Sturmfels’ theorem.
In the rest of this section we study the limits of proﬁting from the advantages
of the generic case by reﬁnement. ¿From the implication “(ii) implies (i)” we
immediately get the following:
Proposition 2. If c, c′ ∈Rn are such that Δc′ of A is a reﬁnement of Δc of
A, where Δc′ is a unimodular triangulation of A, then yA ≤c is TDI.
Clearly, the unimodular triangulation does not even need to be regular – a uni-
modular cover of the cells is actually enough as well for verifying – by Cramer’s
rule – that Aσ is a Hilbert basis, and therefore (ii) holds. We are interested
in the converse of Proposition 2, that is, the existence of such a c′ for every
TDI system. In general such a converse does not hold. It is not even true that
a Hilbert basis has a unimodular partition or a unimodular covering [1]. This
counterexample [1] inspires two important remarks. First, it cannot be expected
that the equivalence of (i) and (v) can be reduced to Sturmfels’ generic case,
even though square-free generating sets exist for general TDI systems as well.
Secondly, it should be appreciated that the converse of this remark does hold in
the important set packing special case, as we will see in the next Section 3.
3
Set Packing
Let a set packing problem be deﬁned with a matrix A and vector c, and recall
c := (1, 0) ∈Rn, where the last d entries of c are 0. If the set packing polytope
Qc has integer vertices then the matrix A and the polytope Qc are said to be
perfect. (We will not use the well-known equivalence of this deﬁnition with the
integer values of optima: this will follow.) Lov´asz’ (weak) perfect graph theorem
[16] is equivalent to: the matrix A deﬁning a set packing polytope is perfect if
and only if its ﬁrst (n −d) columns form the incidence vectors (indexed by the
vertices) of the inclusion wise maximal complete subgraphs of a perfect graph.
A polyhedral proof of the perfect graph theorem can be split into two parts:
Lov´asz’ replication lemma [16] and Fulkerson’s pluperfect graph theorem [12]. The
latter states roughly that a set packing polytope with integer vertices is described
by a TDI system of linear inequalities. In this section we restate Fulkerson’s result

390
E. O’Shea and A. Seb˝o
in a sharper form: there is a unimodular regular triangulation that reﬁnes the
normal fan of any integral set packing polytope. We essentially repeat Fulkerson’s
proof, completing it with a part that shows unimodularity along the lines of the
proof of [23, Theorem 3.1]. The following theorem contains the weak perfect
graph theorem and endows it with an additional geometric feature. Denote the
common optimal value of LPA,c(b) and DPA,c(b) by γc(b). Note that γc is a
monotone increasing function in all of the coordinates.
Theorem 2. Let Qc be a set packing polytope deﬁned by A and c. Then there
exists a vector ε ∈Rn such that c′ := (1, 0) + ε deﬁnes a regular triangulation
Δc′ reﬁning Δc, and this triangulation is unimodular, if and only if Qc is perfect.
We do not claim that the following proof of this theorem is novel. All essential
ingredients except unimodularity are already included in the proof of Fulker-
son’s pluperfect graph theorem [12]. Cook, Fonlupt and Schrijver [4] and Chan-
drasekaran, Tamir [2] both exploited the fact that the greedy way of taking
active rows leads to integer basic solutions in this case. The latter paper exten-
sively used lexicographically best solutions, which is an important tool in linear
programming theory, and this was used in observing the existence of a unimod-
ular reﬁnement of the normal fan in [23]. This same lexicographic perturbation
is accounted for by the vector ε of Theorem 2, showing that the unimodular
reﬁnement is regular. This motivated the following problem, thus containing
perfectness test:
Problem 1. [24] Given a d × n integer matrix A and an n dimensional integer
vector c, decide in polynomial time whether the normal fan of Qc consists only
of unimodular cones. Equivalently, can it be decided in polynomial time that Qc
is non-degenerate, and the determinant of Aσ is ±1 for all σ ∈Δc.
Theorem 2 is a last step in a sharpening series of observations all having essen-
tially the same proof. We begin similarly, with the proof of Fulkerson’s pluperfect
graph theorem which will indicate what the c′ of Theorem 2 should be, and then
ﬁnish by showing that Δc′ is a unimodular triangulation.
Assume that A is a perfect matrix for the remainder of this section and that
c = (1, 0) as before. For all b ∈Zd and column index i ∈{1, . . . , n} let
λc,i(b) := max{xi : x is an optimal solution of LPA,c(b)}.
That is, λc,i(b) is the largest value of xi such that c·x is minimum under x ∈Pb.
An additional remark: if σ is the minimal cell of Δc such b ∈cone(Aσ), then
b−λc,i(b)ai ∈cone(Aσ′) where σ′ ∈Δc, σ′ ⊆σ and the dimension of cone(Aσ′)
is strictly smaller than that of cone(Aσ). Furthermore, b −λai /∈cone(Aσ) if
λ > λc,i(b).
For all b ∈Zd we show that λc,i(b) is an integer for every i = 1, . . . , n. This
is the heart of Fulkerson’s pluperfect graph theorem [12, Theorem 4.1]. We state
it here in a way that is most useful for our needs:

Characterizations of Total Dual Integrality
391
Lemma 2. Suppose γc(b) ∈Z for all b ∈Zd. If x is an optimal solution to
LPA,c(b) with xl ̸= 0 for some 1 ≤l ≤n, then there exists x∗also optimal for
the same b, such that x∗
l ≥1.
Note that this lemma implies the integrality of λ := λc,l(b) for all l = 1, . . . , n:
if λ were not an integer then setting b′ := b −⌊λ⌋al we have λc,l(b′) = {λ}
where 0 ≤{λ} := λ −⌊λ⌋< 1, contradicting Lemma 2.
Proof. Suppose x ∈Pb with c · x = γ(b) and xl > 0 for some 1 ≤l ≤n. We
have two cases: either 1 ≤l ≤n −d or n −d + 1 ≤l ≤n.
If n −d + 1 ≤l ≤n then al = −el−(n−d) ∈Rd and cl = 0. In this case, we
have γc(b) = γc(b+xlel−(n−d)) because replacing xl by 0 in x we get a solution
of the same objective value for the right hand side b + xlel−(n−d) which gives
γc(b) ≥γc(b+xlel−(n−d)). The reverse inequality follows from the (coordinate-
wise) monotonicity of γc. But then
γc(b + el−(n−d)) ≤γc(b + xlel−(n−d)) + 1 −xl = γc(b) + 1 −xl,
and since γc(b + el−(n−d)) is integer and 1 −xl < 1, we conclude that γc(b +
el−(n−d)) = γc(b).
So for any optimal x′ ∈Pb+el−(n−d) where c · x′ = γc(b), letting x∗:=
x′ + el−(n−d) ∈Pb we have c · x∗≤γc(b) and so x∗is optimal and x∗
l ≥1.
On the other hand, suppose 1 ≤l ≤n −d. By the monotonicity of γc, and
noting that replacing xl in x by 0 we get a point in Pb−xlal. This point has
objective value c · x −xl < c · x = γc(b), and so we have
γ(b −al) ≤γ(b −xlal) < γ(b).
Since the left and right hand sides are both integer values then γ(b −al) ≤
γ(b) −1. In other words, for any optimal x′ ∈Pb−al we have c · x′ ≤γc(b) −1.
Letting x∗:= x′ +el ∈Pb we get c·x∗≤γc(b)−1 + 1 = γc(b) with x∗
l ≥1.
⊓⊔
Let us know deﬁne the appropriate c′ for the theorem, depending only on c.
Deﬁne c′ := c + ε ∈Rn where εi := −(1/nn+2)i for each i = 1, . . . , n. Note that
the absolute value of the determinant of a {−1, 0, 1}-matrix cannot exceed nn.
It follows, by Cramer’s rule, that the coeﬃcients of linear dependencies between
the columns of A are at most nn in absolute value, and then the sum of absolute
values of the coeﬃcients between two solutions of an equation Ax = b for any
b ∈Rn can diﬀer by at most a factor of nn+2. After this observation two facts can
be immediately checked (this is well-known from courses of linear programming):
(i) Any optimal solution to LPA,c′(b) is also optimal for LPA,c(b).
(ii) If x′ and x′′ are both optimal solutions to LPA,c(b) then x′ is lexicograph-
ically bigger than x′′ (that is, the ﬁrst non-zero coordinate of x′ −x′′ is
positive) if and only if c′ · x′ < c′ · x′′.
Fact (i) means that Δc′ reﬁnes Δc, and (ii) means that an optimal solution
to LPA,c′(b) is constructed by deﬁning b0 := b and recursively
xi := λc,i(bi−1), bi := bi−1 −xiai.

392
E. O’Shea and A. Seb˝o
Furthermore, this optimum is unique and it follows that Δc′ is a triangulation.
We are now ready to prove Theorem 2.
Proof of Theorem 2. The necessity of the condition is straightforward: each vertex
y ∈Qc satisﬁes the linear equation of the form yAσ′ = 1, where σ′ is a cell of
Δc′, b ∈cone(Aσ′) ⊆cone(Aσ), σ ∈Δc. Since the determinant of Aσ is ±1, by
Cramer’s rule, y is integer.
Conversely, we will prove the assertion supposing only that γc(b) is integer
for all b ∈Zd. (Note that then by the already proven easy direction we will
have proved from this weaker statement that Qc is perfect, as promised at the
deﬁnition of perfectness.)
Without loss of generality, suppose that b cannot be generated by less than
d columns of A, that is, the minimal cell σ of Δc such that b ∈cone(Aσ) is a
maximal cell of Δc. That is, cone(Aσ) is d-dimensional. Because of fact (i), an
optimal solution to LPA,c′(b) will have support in σ and fact (ii) implies that
such an optimal solution is constructed as follows:
Let s1 := min{i : i ∈σ} and xs1 := λc,s1(b). Recursively, for j = 2, . . . , d let
sj be the smallest element in σ indexing a column of A on the minimal face of
cone(Aσ) containing
b −
j−1

i=1
xsiasi.
Since b is in the interior of cone(Aσ) then xsi > 0 for each i = 1, ..., d,
and by Lemma 2, these d xsi’s are integer. Moreover, since the dimension of
cone(Aσ\{s1,...,si}) is strictly decreasing as i = 2, . . . , d progresses then
b −
d

i=1
xsiasi = 0
and, setting U := {s1, . . . , sd} ⊆σ, we have the columns of AU are linearly
independent. Note that U is a cell of Δc′ and every maximal cell of Δc′ arises
in this fashion. We show that the matrix AU has determinant ±1.
Suppose not. Then the inverse of the matrix AU is non-integer, and from the
matrix equation (AU)−1AU =id we see that there exists a unit vector ej ∈Rd
which is a noninteger combination of columns in AU:
d

i=1
xsiasi = ej.
For α ∈R let {α} := α −⌊α⌋, and deﬁne:
d

i=1
{xsi}asi =: z
Clearly, z ∈cone(AU) and furthermore z ∈Zd since it diﬀers from ej by an
integer combination of the columns of AU. So Lemma 2 can be applied to b := z:

Characterizations of Total Dual Integrality
393
letting l := min{i : {xsi} ̸= 0} we see that λc,sl(z) < 1 contradicting Lemma 2.
Hence both AU and (AU)−1 are integer, their determinant is ±1; since AU was
an arbitrary maximal cell of Δc′, we conclude that Δc′ is unimodular.
⊓⊔
The argument concerning the inverse matrix replaces the use of parallelepipeds
(compare with [23, proof of Theorem 3.1]) that we wanted to avoid here to stay
in elementary terms.
Note that all the numbers in the deﬁnition of c′ are at most nn2, so they have
a polynomial number of digits: the perturbed problem has polynomial size in
terms of the original one, reducing perfectness test to Problem 1.
4
Computation
In this section we wish to give an idea of how the results presented in this work
lead to practical algorithms. There are three essentially diﬀerent approaches.
A ﬁrst, general, elementary algorithm can be based on Theorem 1, or more
precisely on the proof of its Corollary ??. Indeed, the procedure described in
this corollary is a general algorithm for testing the TDI property in O(nd) time.
If d is ﬁxed, it is a polynomial algorithm. This is very recent and has not yet
been implemented.
The equivalences of (i) and (v) in Theorem 1 along with an algorithm [20, Al-
gorithm 4.4.2] for computing the generators of the monomial ideal mono(inc(IA))
permit us to detect TDI using algebraic methods: the generators are square-free
if and only if the system yA ≤c is TDI.
This algorithm works for all cost vectors, be they generic or non-generic, but it
is not yet implemented and our suspition is that mono(inc(IA)) could be rather
diﬃcult to compute in the non-generic case. However, in the generic case, inc(IA)
is already a monomial ideal and can be attained in practice. In addition, even
if c is non-generic, it may have a generic perturbation yielding a unimodular
triangulation and then the toric initial ideals can be studied with respect to
the perturbed vector. Computing the toric initial ideal may be far easier than
investigating the unimodularity of the corresponding triangulation.
Let us have a look at one example of an A and c coming from a set packing
problem. A more eﬃcient way of treating the data is at hand in the generic
case. Then we can use the computationally well studied reduced Gr¨obner bases
according to Proposition 1.
The perfect graph in Figure 1 with 21 maximal cliques on 20 vertices was
constructed by Padberg in [18]. The matrix A is a (20 × 41)-matrix and the
toric ideal IA lives in the polynomial ring k[a, . . . , u, v1, . . . , v20] where a, . . . , u
correspond to the maximal cliques of G (the ﬁrst 21 columns of A) and where
v1, . . . , v20 correspond to the vertices of G (the ordered columns of −I20, the last
20 columns of A) as before.
The toric initial ideal with an appropriate perturbation has 61 elements, all
of which are square-free. The computation was carried out in Macaulay 2 [14]
(in less than 1 second) and its implementation can be seen in [17, Appendix A].

394
E. O’Shea and A. Seb˝o
1
2
3
4
19
20
18
17
16
15
14
13
9
10
12
11
5
6
7
8
b
c
d
e
f
g
i
a
k
l
n
m
o
j
h
p
r
s
q
t
u
Fig. 1. Padberg’s graph G with 21 maximal cliques on 20 vertices
However, we could (equivalently) have asked if a well-deﬁned triangulation re-
ﬁning Δc, was a unimodular triangulation. This is a far more exhausting task than
computing the monomial toric initial ideal. Because of the bijection between the
cells of Δc and the vertices of Qc, using PORTA [9] we computed that Qc had pre-
cisely 5901 vertices. Next, using TOPCOM [19] a number of these 5901 cells are each
reﬁned into many pieces by the reﬁnement. To conﬁrm TDI, the determinant in-
dexed by each of the many reﬁned cells would have to be computed.
Acknowledgments
The authors wish to thank Rekha Thomas for her valuable input and suggestions.
Some work related to the results of this article, including the computational
experimentation, can be found in the ﬁrst author’s Ph.D. dissertation at the
University of Washington. Thanks are also due to our colleagues who developed
the computational packages Macaulay 2 and TOPCOM.
References
1. W. Bruns, J. Gubeladze, Normality and Covering Properties of Aﬃne Semi-
groups, manuscript.
2. R. Chandrasekaran, A. Tamir , On the integrality of an extreme solution to
pluperfect graph and balanced systems, Oper. Res. Let., 3, (1984), 215–218.
3. P. Conti, C. Traverso, Buchberger algorithm and integer programming, Applied
algebra, algebraic algorithms and error-correcting codes, Lecture Notes in Comput.
Sci., 539, Springer, Berlin, 1991

Characterizations of Total Dual Integrality
395
4. W. Cook, J. Fonlupt, A. Schrijver, An integer analogue of Carath´eodory’s
theorem. J. Combin. Theory(B), 40, (1986), 63–70.
5. W. Cook, L. Lov´asz, A. Schrijver, A polynomial-time test for total dual inte-
grality in ﬁxed dimension, Mathematical Programming Study 22, (1984), 64–69.
6. G. Cornu´ejols, Combinatorial optimization: packing and covering, CBMS-NSF
regional conference series in applied mathematics, 74 SIAM, 2001.
7. M. Chudnovsky, G. Cornu´ejols, L. Xinming, K. Vuˇskovi´c, Recognizing
Berge graphs. Combinatorica, 25, (2005), no. 2, 143–186.
8. D. Cox, J. Little, D. O’Shea, Ideals, varieties and algorithms, 2nd edition,
Springer-Verlag, NY, 1996.
9. T. Christof, A. L¨obel, PORTA (POlyhedron Representation Transformation
Algorithm), available from http://www.zib.de/Optimization/Software/Porta/.
10. V. Chv´atal, On certain polytopes associated with graphs Journal of Comb. The-
ory,(B), 18, (1975), 138–154.
11. A. Gerards, A. Seb˝o, Total dual integrality implies local strong unimodularity,
Mathematical Programming, 38 (1987), 69-73.
12. D. R. Fulkerson, Anti-blocking polyhedra, Journal of Comb. Theory,(B), 12,
(1972), 50–71.
13. J. Graver, On the foundations of linear and integer programming I, Math. Pro-
gramming 8, (1975), 207–226.
14. D. Grayson, M. Stillman, Macaulay 2, a software system for research in alge-
braic geometry, available from http://www.math.uiuc.edu/Macaulay2/.
15. S. Hos¸ten, R.R. Thomas, Gomory integer programs, Mathematical Program-
ming(B), 96, (2003), 271–292.
16. L. Lov´asz, Normal hypergraphs and the perfect graph conjecture, Discrete Math-
ematics, 2, (1972), 253–267.
17. E. O’Shea, Toric algebra and the weak perfect graph theorem, Ph.D. dissertation,
University of Washington, 2006.
18. M. Padberg, Perfect zero-one matrices, Math. Programming 6 (1974), 180–196.
19. J.
Rambau, TOPCOM (Triangulations
Of Point Conﬁgurations
and
Ori-
ented
Matroids),
available
from
http://www.uni-bayreuth.de/departments/
wirtschaftsmathematik/rambau/TOPCOM/.
20. M. Saito, B. Sturmfels, N. Takayama, Gr¨obner deformations of hyperge-
ometric diﬀerential equations, Algorithms and Computation in Mathematics, 6
Springer-Verlag, Berlin, 2000.
21. A. Schrijver, Theory of linear and integer programming, Wiley, 1986.
22. A. Schrijver, Combinatorial optimization: polyhedra and eﬃciency, Algorithms
and Combinatorics 24, Springer, 2003.
23. A. Seb˝o, Hilbert bases, Caratheodory’s theorem and combinatorial optimization,
Integer Programming and Combinatorial Optimization (eds: R. Kannan, W. Pul-
leyblank) Mathematical Programming Society, University of Waterloo Press, Wa-
terloo, 1990.
24. A. Seb˝o, Problem A.6, “TDI Matrices”, in Open Problems (Workshop on “The
Perfect Graph Conjecture”, (2002),
http://www.aimath.org/pastworkshops/perfectgraph.html.
25. B. Sturmfels, R. Weismantel, G. M. Ziegler, Gr¨obner bases of lattices, corner
polyhedra, and integer programming, Beitr¨age Algebra Geom., 36, (1995), 281–298.
26. B. Sturmfels, Gr¨obner bases and convex polytopes, University Lecture Series 8,
American Mathematical Society, Providence, RI, 1996.

396
E. O’Shea and A. Seb˝o
27. B. Sturmfels, Algebraic recipes for integer programming, AMS Shortcourse:
Trends in Optimization (eds: S. Ho¸sten, J. Lee, R.R. Thomas) Proceedings of Sym-
posia in Applied Mathematics, 61, American Mathematical Society, Providence,
RI, 2004.
28. B. Sturmfels and R. R. Thomas, Variations of cost functions in integer pro-
gramming, Math. Programming 77, (1997), 357–387.
29. R. R. Thomas, Algebraic methods in integer programming, Encyclopedia of Op-
timization (eds: C. Floudas and P. Pardalos), Kluwer Academic Publishers, Dor-
drecht, 2001

Sign-Solvable Linear Complementarity Problems
Naonori Kakimura
Department of Mathematical Informatics,
Graduate School of Information Science and Technology,
The University of Tokyo, Tokyo 113-8656, Japan
naonori kakimura@mist.i.u-tokyo.ac.jp
Abstract. This paper presents a connection between qualitative matrix
theory and linear complementarity problems (LCPs). An LCP is said to
be sign-solvable if the set of the sign patterns of the solutions is uniquely
determined by the sign patterns of the given coeﬃcients. We provide a
characterization for sign-solvable LCPs such that the coeﬃcient matrix
has nonzero diagonals, which can be tested in polynomial time. This
characterization leads to an eﬃcient combinatorial algorithm to ﬁnd the
sign pattern of a solution for these LCPs. The algorithm runs in O(γ)
time, where γ is the number of the nonzero coeﬃcients.
Keywords: Linear Complementarity Problems, Combinatorial Matrix
Theory.
1
Introduction
This paper deals with linear complementarity problems (LCPs) in the following
form:
LCP(A, b): ﬁnd (w, z)
s.t. w = Az + b,
wTz = 0,
w ≥0, z ≥0,
where A is a real square matrix, and b is a real vector. The LCP, introduced
by Cottle [4], Cottle and Dantzig [5], and Lemke [16], is one of the most widely
studied mathematical programming problems, which contains linear program-
ming and convex quadratic programming. Solving LCP(A, b) for an arbitrary
matrix A is NP-complete [3], while there are several classes of matrices A for
which the associated LCPs can be solved eﬃciently. For details of the theory of
LCPs, see the books of Cottle, Pang, and Stone [6] and Murty [20].
The sign pattern of a real matrix A is the {+, 0, −}-matrix obtained from A
by replacing each entry by its sign. When we develop an LCP model in practice,
the entries of A and b are subject to many sources of uncertainty including
errors of measurement and absence of information. On the other hand, the sign
patterns of A and b are structural properties independent of such uncertainty.
This motivates us to provide a combinatorial method that exploits the sign
patterns before using numerical information.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 397–409, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

398
N. Kakimura
Sign pattern analysis for matrices and linear systems, called qualitative ma-
trix theory, was originated in economics by Samuelson[24]. Various results about
qualitative matrix theory are compiled in the book of Brualdi and Shader [1].
For a matrix A, we denote by Q(A) the set of all matrices having the same sign
pattern as A, called the qualitative class of A. The qualitative class of a vector
is deﬁned similarly. A square matrix A is said to be sign-nonsingular if ˜A is
nonsingular for any ˜A ∈Q(A). The problem of recognizing sign-nonsingular ma-
trices has many equivalent problems in combinatorics[17,21,25,27], while its time
complexity had been open for a long time. In 1999, Robertson, Seymour, and
Thomas [22] presented a polynomial-time algorithm for solving this problem (cf.
McCuaig [18,19]).
For linear programming, Iwata and Kakimura [11] proposed sign-solvability in
terms of qualitative matrix theory. A linear program max{cx | Ax = b, x ≥0},
denoted by LP(A, b, c), is sign-solvable if the set of the sign patterns of the
optimal solutions of LP( ˜A,˜b, ˜c) is the same as that of LP(A, b, c) for any ˜A ∈
Q(A), ˜b ∈Q(b), and ˜c ∈Q(c). They showed that recognizing sign-solvability of
a given LP is NP-hard, and gave a suﬃcient condition for sign-solvable linear
programs, which can be tested in polynomial time. Moreover, they devised a
polynomial-time algorithm to obtain the sign pattern of an optimal solution for
linear programs satisfying this suﬃcient condition.
In this paper, we introduce sign-solvability for linear complementarity prob-
lems. We say that LCP(A, b) is sign-solvable if the set of the sign patterns of the
solutions of LCP( ˜A,˜b) coincides with that of LCP(A, b) for any ˜A ∈Q(A) and
˜b ∈Q(b). An LCP(A, b) such that all diagonal entries of A are nonzero is said
to have nonzero diagonals. The class of LCPs with nonzero diagonals includes
LCPs associated with positive deﬁnite matrices, P-matrices, and nondegenerate
matrices, which are all of theoretical importance in the context of LCPs (e.g.
[6, Chapter 3]). LCPs with P-matrices are related to a variety of applications
such as circuit equations with piecewise linear resistances[8] and linear systems
of interval linear equations [23]. We present a characterization for a sign-solvable
LCP(A, b) with nonzero diagonals, and describe a polynomial-time algorithm to
solve them from the sign patterns of A and b.
We ﬁrst provide a suﬃcient condition for sign-solvable LCPs with nonzero
diagonals. A square matrix A is term-nonsingular if the determinant of A con-
tains at least one nonvanishing expansion term. A square matrix A is term-
singular if it is not term-nonsingular. A matrix A is term-singular if and only
if ˜A is singular for any ˜A ∈Q(A). An m × n matrix with m ≤n is said
to be totally sign-nonsingular if all submatrices of order m are either
sign-nonsingular or term-singular, namely, if the nonsingularity of each subma-
trix of order m is determined uniquely by the sign pattern of the matrix. Totally
sign-nonsingular matrices were investigated in the context of sign-solvability of
linear systems [1,12,13,26] (the terms “matrices with signed mth compound”
and “matrices with signed null space” are used instead). Recognizing totally

Sign-Solvable Linear Complementarity Problems
399
sign-nonsingular matrices can be done in polynomial time by testing sign-non
singularity of related square matrices [11]. We show that, if the matrix M = (A b)
is totally sign-nonsingular and A has nonzero diagonals, then LCP(A, b) is sign-
solvable.
We then present a characterization of sign-solvable LCPs with nonzero di-
agonals. A row of a matrix is called mixed if it has both positive and negative
entries. A matrix is row-mixed if every row is mixed. For an LCP(A, b) with
nonzero diagonals, we introduce the residual row-mixed matrix, which is the spe-
cial submatrix of M = (A b) deﬁned in Sect. 3. Then LCP(A, b) with nonzero
diagonals is sign-solvable if and only if its residual row-mixed matrix M ′ satisﬁes
one of followings: M ′ does not contain the subvector of b, M ′ has no rows, or
M ′ is totally sign-nonsingular. The residual row-mixed matrix can be obtained
in polynomial time. Thus the sign-solvability of a given LCP(A, b) with nonzero
diagonals can be recognized in polynomial time.
This characterization leads to an eﬃcient combinatorial algorithm to solve a
given LCP(A, b) with nonzero diagonals from the sign patterns of A and b. The
algorithm tests the sign-solvability, and ﬁnds the sign pattern of a solution if it
is a sign-solvable LCP with solutions. In this algorithm, we obtain a solution of
LCP( ˜A,˜b) for some ˜A ∈Q(A) and ˜b ∈Q(b). If LCP(A, b) is sign-solvable, then
LCP(A, b) has a solution with the same sign pattern as the obtained one. The
time complexity is O(γ), where γ is the number of nonzero entries in A and b.
We note that the obtained sign pattern easily derives a solution of the given
LCP by Gaussian elimination. Thus a sign-solvable LCP with nonzero diagonals
is a class of LCPs which can be solved in polynomial time.
Before closing this section, we give some notations and deﬁnitions used in the
following sections.
For a matrix A, the row and column sets are denoted by U and V . If A is a
square matrix, suppose that U and V are both identical with N. We denote by
aij the (i, j)-entry in A. Let A[I, J] be the submatrix in A with row subset I and
column subset J, where the orderings of the elements of I and J are compatible
with those of U and V . The submatrix A[J, J] is abbreviated as A[J]. The
support of a row subset I, denoted by Γ(I), is the set of columns having nonzero
entries in the submatrix A[I, V ], that is, Γ(I) = {j ∈V | ∃i ∈I, aij ̸= 0}.
For a vector b, the jth entry of b is denoted by bj. The vector b[J] means the
subvector with index subset J. The support of a vector b is the column index
subset {j | bj ̸= 0}.
For a square matrix A, let π be a bijection from the row set N to the column
set N. We denote by p(A|π) = sgnπ 
i∈N aiπ(i) the expansion term of det A cor-
responding to π. Then a matrix A is term-nonsingular if and only if there exists
a bijection π : N →N with p(A|π) ̸= 0. A square matrix A is sign-nonsingular if
and only if A is term-nonsingular and every nonvanishing expansion term of det A
has the same sign [1, Theorem 1.2.5]. Thus, if A is sign-nonsingular, the determi-
nant of every matrix in Q(A) has the same sign. It is also shown in [1, Theorem
2.1.1] that, if a square matrix A is sign-nonsingular, then A is not row-mixed.

400
N. Kakimura
This paper is organized as follows. In Sect. 2, we provide a suﬃcient condition
using totally sign-nonsingular matrices. Section 3 gives a characterization for sign-
solvable LCPs with nonzero diagonals. In Sect. 4, we describe a polynomial-time
algorithm to solve sign-solvable LCPs with nonzero diagonals from the sign pat-
terns of the given coeﬃcients.
2
Totally Sign-Nonsingular Matrices
In this section, we give a suﬃcient condition for sign-solvable LCPs using totally
sign-nonsingular matrices. For that purpose, we deﬁne sign-nondegenerate ma-
trices. A square matrix A is nondegenerate if every principal minor is nonzero.
A matrix A is nondegenerate if and only if LCP(A, b) has a ﬁnite number of
solutions for any vector b [6]. Recognizing nondegenerate matrices is co-NP-
complete [2,20]. A square matrix A is said to be sign-nondegenerate if ˜A is non-
degenerate for any ˜A ∈Q(A). Then the following lemma holds, which implies
that sign-nondegeneracy can be tested in polynomial time.
Lemma 2.1. A square matrix A is sign-nondegenerate if and only if A is a
sign-nonsingular matrix with nonzero diagonals.
Proof. To see the necessity, suppose that A is sign-nondegenerate. Let ˜A be a
matrix in Q(A). Since all principal minors in ˜A are nonzero, all diagonal entries
are nonzero. Moreover, det ˜A is nonzero, which implies that A is sign-nonsingular.
Thus A is a sign-nonsingular matrix with nonzero diagonals.
To see the suﬃciency, suppose that A is a sign-nonsingular matrix with
nonzero diagonals. Let J ⊆N be an index subset. Since the principal sub-
matrix A[J] has nonzero diagonals, A[J] is term-nonsingular. Let σ1 and σ2 be
bijections from J to J such that p(A[J]|σ1) ̸= 0 and p(A[J]|σ2) ̸= 0. Deﬁne
bijections πk : N →N to be πk(j) = j if j ∈N \ J and πk(j) = σk(j) if
j ∈J for k = 1, 2. Since A has nonzero diagonals, p(A|π1) and p(A|π2) are
both nonzero. By p(A|πk) = p(A[J]|σk) 
i∈N\J aii for k = 1, 2, it follows from
sign-nonsingularity of A that the two nonzero terms p(A[J]|σ1) and p(A[J]|σ2)
have the same sign. Thus A[J] is sign-nonsingular, which implies that A is sign-
nondegenerate.
⊓⊔
We now obtain the following theorem. For LCP(A, b), let M be the matrix in
the form of M = (A b), where the column set is indexed by N ∪{g}.
Theorem 2.2. For a linear complementarity problem LCP(A, b) with nonzero
diagonals, if the matrix M = (A b) is totally sign-nonsingular, then LCP(A, b)
is sign-solvable.
Proof. First assume that LCP(A, b) has a solution (w, z). Let J be the support
of z. Then we have AJ
w[N \ J]
z[J]

+ b = 0, where AJ is the matrix in the form
of
AJ =

O
A[J]
−I A[N \ J, J]

.

Sign-Solvable Linear Complementarity Problems
401
Since A is sign-nondegenerate by Lemma 2.1, each principal submatrix is sign-
nonsingular, and hence AJ is also sign-nonsingular by det AJ = ± det A[J]. Then
it holds by Cramer’s rule that
zj =

−det Aj
J/det AJ, if j ∈J,
0,
if j ∈N \ J,
(1)
wj =

0,
if j ∈J,
−det Aj
J/det AJ, if j ∈N \ J,
(2)
where Aj
J is the matrix obtained from AJ by replacing the jth column vector of
AJ with b. The determinant of Aj
J is represented by
det Aj
J =
 ± det M[J, J −j + g], if j ∈J,
± det M[J + j, J + g], if j ∈N \ J,
(3)
where J −j + g means J \ {j} ∪{g} with g being put at the position of j in J,
the set J + j coincides with J ∪{j}, and J + g means J ∪{g} in which g is put
at the same position as that of j in J + j.
We show that Aj
J is either term-singular or sign-nonsingular for any J ⊆N
and j ∈N. Assume that there exists j ∈N such that Aj
J is term-nonsingular, but
not sign-nonsingular. First suppose that j ∈J. By (3), the submatrix M[J, J−j+
g] is term-nonsingular, but not sign-nonsingular. Then there exist two bijections
σ1 and σ2 from J to J −j + g such that p(M[J, J −j + g]|σ1) and p(M[J, J −
j + g]|σ2) are both nonzero, and have the opposite signs. Deﬁne two bijections
πk : N →N −j + g to be πk(i) = i if i ∈N \ J and πk(i) = σk(i) if i ∈J
for k = 1, 2. By p(M[N, N −j + g]|πk) = p(M[J, J −j + g]|σk) 
i∈N\J aii for
k = 1, 2, the two nonzero terms p(M[N, N −j +g]|π1) and p(M[N, N −j +g]|π2)
are both nonzero, and have the opposite signs. This contradicts the total sign-
nonsingularity of M. Next suppose that j ∈N \ J. Then, by (3), M[J + j, J + g]
is term-nonsingular, but not sign-nonsingular. Let σ1 and σ2 be bijections from
J + j to J + g such that p(M[J + j, J + g]|σ1) and p(M[J + j, J + g]|σ2) are both
nonzero, and have the opposite signs. Deﬁne two bijections πk : N →N −j + g
for k = 1, 2 to be πk(i) = i if i ∈N \ (J ∪{j}) and πk(i) = σk(i) if i ∈J ∪{j}.
Then the two nonzero terms p(M[N, N −j + g]|π1) and p(M[N, N −j + g]|π2)
have the opposite signs, which contradicts the total sign-nonsingularity of M.
Thus Aj
J is either term-singular or sign-nonsingular for any index j. The
matrix AJ is sign-nonsingular. Therefore, it follows from (1) that the sign pattern
of (w, z) is independent of the magnitudes of A and b. Hence LCP( ˜A,˜b) has a
solution with the same sign pattern as that of (w, z) for any ˜A ∈Q(A) and
˜b ∈Q(b). Thus LCP(A, b) is sign-solvable.
Next assume that LCP(A, b) has no solutions. Note that LCP(A, b) has no
solutions if and only if AJx+b = 0 has no nonnegative solutions for any J ⊆N,
that is, there exists j ∈N such that (A−1
J b)j < 0 for any J ⊆N. It follows from
Cramer’s rule that we have (A−1
J b)j = −det Aj
J/det AJ < 0. Since det Aj
J ̸= 0,
the matrix Aj
J is sign-nonsingular. Hence it holds that −det ˜Aj
J/det ˜AJ < 0 for

402
N. Kakimura
any ˜A ∈Q(A) and ˜b ∈Q(b). Thus LCP( ˜A,˜b) has no solutions for any ˜A ∈Q(A)
and ˜b ∈Q(b), which means that LCP(A, b) is sign-solvable.
⊓⊔
Sign-solvable LCPs do not necessarily satisfy this suﬃcient condition. Indeed,
consider LCP(A, b), where A and b are deﬁned to be
A =
−p1 −p2
+p3 +p4

and b =
 0
+p5

for positive constants p1, . . . , p5 > 0. Then LCP(A, b) has a unique solution
w = (0 p5)T and z = 0, and hence LCP(A, b) is sign-solvable. However, this does
not satisfy the condition of Theorem 2.2, as A is not sign-nonsingular.
We conclude this section with sign-solvability of LCPs associated with another
class of matrices. A square matrix A is a P-matrix if every principal minor is
positive. A P-matrix is clearly nondegenerate. It is known that A is a P-matrix
if and only if LCP(A, b) has a unique solution for any vector b. Recognizing P-
matrices is co-NP-complete [7]. A matrix A is a sign-P-matrix if all matrices in
Q(A) are P-matrices. Then similar statements to Lemma 2.1 and Theorem 2.2
hold for sign-P-matrices.
Corollary 2.3. A square matrix A is a sign-P-matrix if and only if A is a
sign-nonsingular matrix with positive diagonals.
Corollary 2.4. For a linear complementarity problem LCP(A, b) with positive
diagonals, if the matrix M = (A b) is totally sign-nonsingular, then LCP( ˜A,˜b)
has a unique solution with the same sign pattern as that of LCP(A, b).
3
Sign-Solvable LCPs with Nonzero Diagonals
In this section, we describe a characterization for a sign-solvable LCP(A, b) with
nonzero diagonals. Recall that M is the matrix in the form of M = (A b), where
the column set is indexed by N ∪{g}.
3.1
The Residual Row-Mixed Matrix
We ﬁrst introduce the residual row-mixed matrix of LCP(A, b) with nonzero
diagonals.
For each row index i, the ith equation of LCP(A, b) is represented by
wi =

j∈Γ({i})
aijzj + bi.
(4)
First assume that M has a nonpositive row i, that is, bi ≤0 and aij ≤0 for
all j ∈N. Suppose that bi < 0. Since any solution of LCP(A, b) is nonnegative,
the ith row implies that LCP(A, b) has no solutions. Next suppose that bi = 0.
Then, if LCP(A, b) has a solution (w, z), the solution (w, z) must satisfy that
zj = 0 for any j ∈Γ({i}).

Sign-Solvable Linear Complementarity Problems
403
Next assume that M has a nonnegative row i, that is, bi ≥0 and aij ≥0 for all
j ∈N. Let (w, z) be a solution of LCP(A, b). If wi > 0, then the complementarity
implies zi = 0. Suppose that wi = 0. Since any solution is nonnegative, (w, z)
must satisfy zj = 0 for any j ∈Γ({i}), and hence zi = 0 by aii ̸= 0. Thus,
if LCP(A, b) has a solution and M has a nonnegative row i, any solution of
LCP(A, b) must satisfy that zi = 0. Note that there exists j ∈Γ({i}) with
zj > 0 if and only if the left-hand side of (4) is positive, i.e., wi > 0.
Therefore, if M has a nonnegative or nonpositive row, then we know that
some entries of any solution must be zero. We can repeat this process as follows.
Set M (1) = M. For a positive integer ν and a matrix M (ν), let I(ν)
−
be the
set of nonpositive rows in M (ν), and I(ν)
+
be the set of nonnegative rows that
have a nonzero entry in M (ν). If Γ(I(ν)
−) contains the index g, then the LCP
has no solutions. Deﬁne I(ν) = I(ν)
+
∪I(ν)
−
and J(ν) = I(ν)
+
∪Γ(I(ν)
−). Then any
solution (w, z) of LCP(A, b) satisﬁes zj = 0 for any j ∈J(ν). Let M (ν+1) be
the matrix obtained from M (ν) by deleting the rows indexed by I(ν) and the
columns indexed by J(ν). Repeat this for ν = 1, 2, . . . until I(ν) = J(ν) = ∅, that
is, until either M (ν) is row-mixed or M (ν) has no rows.
We call the remaining row-mixed submatrix M ′ the residual row-mixed matrix
of LCP(A, b). Note that, if LCP(A, b) has solutions, the column index g is not
deleted in each iteration.
Assume that the column set of M ′ contains the index g. Let M ′ be in the forms
of M ′ = (A′ b′), where b′ is the subvector of b and A′ is the submatrix of A with
row set U ′ and column set V ′. We denote ¯U ′ = N \ U ′ and ¯V ′ = N \ V ′. Since A
has nonzero diagonals, ¯U ′ ⊆¯V ′ holds, and hence we have V ′ ⊆U ′. Suppose that
M ′ has no rows. Then ¯V ′ = N holds, which means that any solution (w, z) of
LCP(A, b) must satisfy z = 0. Since g is not deleted in each iteration, the vector
b is nonnegative. Thus (b, 0) is a unique solution of LCP(A, b). Next suppose
that M ′ is row-mixed. Consider the following system:
w = A′z + b′,
wT
i zi = 0, for any i ∈V ′,
w ≥0, z ≥0.
(5)
We claim that there exists a one-to-one correspondence between solutions of
LCP(A, b) and (5). For a solution (w, z) of LCP(A, b), the pair (w[U ′], z[V ′]) is
a solution of (5). Conversely, let (w′, z′) be a solution of (5). Deﬁne (w, z) to
be z[V ′] = z′, z[ ¯V ′] = 0, and w = Az + b. Then w[U ′] = A′z′ + b′ = w′ ≥0
holds. Moreover, since each row in A[ ¯U ′, V ′] is nonnegative, we have w[ ¯U ′] =
A[ ¯U ′, V ′]z′ +b[ ¯U ′] ≥0. By V ′ ⊆U ′, the pair (w, z) satisﬁes the complementarity
wTz = 0. Thus (w, z) is a solution of LCP(A, b).
3.2
Characterization
Using the residual row-mixed matrix M ′ of LCP(A, b), we have the following
theorem.

404
N. Kakimura
Theorem 3.1. Let LCP(A, b) be a linear complementarity problem with nonzero
diagonals, and M ′ be the residual row-mixed matrix. Then LCP(A, b) is sign-
solvable if and only if one of the followings holds:
– The column set of M ′ does not contain the index g.
– The residual row-mixed matrix M ′ has no rows.
– The residual row-mixed matrix M ′ is totally sign-nonsingular.
In order to prove this theorem, we give some deﬁnitions. A linear system Ax =
b has signed nonnegative solutions if the set of the sign patterns of nonnegative
solutions of ˜Ax = ˜b is the same as that of nonnegative solutions of Ax = b for
any ˜A ∈Q(A) and ˜b ∈Q(b). A matrix A is said to have signed nonnegative
null space if Ax = 0 has signed nonnegative solutions. Matrices with signed
nonnegative null space were examined by Fisher, Morris, and Shapiro [9]. They
showed that a row-mixed matrix has signed nonnegative null space if and only
if it is the matrix called mixed dominating, which is deﬁned to be a row-mixed
matrix which does not contain a square row-mixed submatrix. By the result of
mixed dominating matrices, the following two lemmas hold.
Lemma 3.2 (Fischer and Shapiro [10]). If a row-mixed matrix A has signed
nonnegative null space, then the rows of A are linearly independent.
A matrix A is said to have row-full term-rank if A has a term-nonsingular sub-
matrix with row size. A matrix A has column-full term-rank if AT has row-full
term-rank.
Lemma 3.3 (Fischer, Morris, and Shapiro [9]). An n × (n + 1) row-mixed
matrix has signed nonnegative null space if and only if it is a totally sign-
nonsingular matrix with row-full term-rank.
We have the following lemmas.
Lemma 3.4. Suppose that the matrix (A b) is row-mixed. If the linear system
Ax + b = 0 has signed nonnegative solutions, then it has a solution all of whose
entries are positive.
Proof. Since (A b) is row-mixed, there exist ˜A ∈Q(A) and ˜b ∈Q(b) such that
the sum of the columns of ˜A and ˜b is zero, that is, ˜A1 + ˜b = 0, where 1 is the
column vector whose entries are all one. This implies that ˜Ax = ˜b has a solution
all of whose entries are positive for any ˜A ∈Q(A) and ˜b ∈Q(b).
⊓⊔
Lemma 3.5. Suppose that M = (A b) is row-mixed. The linear system Ax+b =
0 has signed nonnegative solutions if and only if M has signed nonnegative null
space.
Proof. Suppose that the matrix M has signed nonnegative null space. Since
{x | Ax + b = 0, x ≥0} = {x | (A b)
x
1

= 0, x ≥0} is contained in the set of
nonnegative vectors in the null space of M, the linear system Ax + b = 0 has
signed nonnegative solutions.

Sign-Solvable Linear Complementarity Problems
405
Next suppose that Ax + b = 0 has signed nonnegative solutions, and that M
does not have signed nonnegative null space. Then M is not mixed dominating,
which means that there exists a row-mixed square submatrix in M. Note that
a row-mixed square submatrix which does not contain any row-mixed square
proper submatrix is term-nonsingular. Choose a row-mixed term-nonsingular
submatrix M[I, J] such that |J| is maximum. Since M is row-mixed, the maxi-
mality implies that each row of M[N \ I, J] is mixed or zero.
We deﬁne B to be B = M[N, J \ {g}] if g ∈J, and B = M[N, J] otherwise.
Then (B b) does not have signed nonnegative null space. The set of the nonnega-
tive vectors in the null space of (B b) consists of the union of {x | Bx = 0, x ≥0}
and {x | (B b)
 x
xg

= 0, x ≥0, xg > 0}. Since the set of sign patterns in the sec-
ond one coincides with that of {x | Bx+b = 0, x ≥0} and Bx+b = 0 has signed
nonnegative solutions, we may assume that B does not have signed nonnegative
null space. Let ˜B ∈Q(B) be a matrix such that ˜B has column-full rank. Then
the null space of ˜B is empty, and ˜Bx + b = 0 has a unique solution all of whose
entries are positive by Lemma 3.4. By the assumption, there exists ˆB ∈Q(B)
such that ˆBx = 0 has a nonnegative, nonzero solution x∗. Lemma 3.4 implies
that ˆBx+b = 0 has a solution x0 all of whose entries are positive. Then x0−μx∗,
where μ = mini∈N x0
i /x∗
i , is also a nonnegative solution of ˆBx + b = 0. Thus
the linear system Bx + b = 0 does not have signed nonnegative solutions, which
contradicts that Ax + b = 0 has signed nonnegative solutions.
⊓⊔
We are now ready to prove Theorem 3.1.
Proof of Theorem 3.1. To show the necessity, suppose that LCP(A, b) is sign-
solvable. Assume that M ′ has a row and that M ′ is in the form of M ′ = (A′ b′),
where b′ is the subvector of b indexed by g. Let x be a nonnegative vector with
A′x + b′ = 0. Since there exists a one-to-one correspondence between solutions
of LCP(A, b) and (5), (0, x) is a solution of (5). Hence the sign-solvability of
LCP(A, b) implies that the linear system A′x + b′ = 0 has signed nonnegative
solutions. It follows from Lemma 3.5 that M ′ = (A′ b′) has signed nonnegative
null space. By Lemma 3.2 and V ′ ⊆U ′, it holds that U ′ = V ′, i.e., A′ is square.
Therefore, Lemma 3.3 implies that M ′ is totally sign-nonsingular.
We next show the suﬃciency. If the column set of M ′ does not contain the
index g, then clearly LCP(A, b) is a sign-solvable LCP with no solutions. Suppose
that M ′ is in the forms of M ′ = (A′ b′). If M ′ has no rows, then (b, 0) is
a unique solution of LCP(A, b), which means that LCP(A, b) is sign-solvable.
Next suppose that M ′ = (A′ b′) is totally sign-nonsingular. By V ′ ⊆U ′, it holds
that |U ′| = |V ′| or |U ′| = |V ′| + 1. If |U ′| = |V ′|, then M ′ is sign-nonsingular,
which contradicts that M ′ is row-mixed. Hence we have |U ′| = |V ′| + 1. Since
A′ has nonzero diagonals, (5) forms the linear complementarity problem with
nonzero diagonals. By Theorem 2.2, LCP(A′, b′) is sign-solvable, and hence so is
LCP(A, b).
⊓⊔
Note that LCP(A, b) is a sign-solvable LCP with no solutions if and only if the
column set of M ′ does not contain g.

406
N. Kakimura
If M is row-mixed, then the residual row-mixed matrix is M itself. Hence
Theorem 3.1 implies the following corollary.
Corollary 3.6. Let A have nonzero diagonals, and M = (A b) be a row-mixed
matrix. Then LCP(A, b) is sign-solvable if and only if the matrix M is totally
sign-nonsingular.
We close this section with an example of sign-solvable LCPs with nonzero di-
agonals. Consider LCP(A, b), where A and b have the sign patterns, respectively,
⎛
⎜
⎜
⎜
⎜
⎝
+ + 0 0 0
−+ + 0 +
+ −+ −0
−0 −−+
0 −+ 0 +
⎞
⎟
⎟
⎟
⎟
⎠
and
⎛
⎜
⎜
⎜
⎜
⎝
0
+
0
0
−
⎞
⎟
⎟
⎟
⎟
⎠
.
The residual row-mixed matrix is⎛
⎝
+ −0 0
−−+ 0
+ 0 + −
⎞
⎠,
which is obtained from the matrix (A b) by deleting the ﬁrst two rows and the
ﬁrst two columns. This residual row-mixed matrix is totally sign-nonsingular,
and hence LCP(A, b) is sign-solvable.
4
Algorithm for Sign-Solvable LCPs with Nonzero
Diagonals
In this section, we describe an algorithm for a given LCP(A, b) with nonzero
diagonals. The algorithm tests sign-solvability of LCP(A, b), and ﬁnds the sign
pattern of a solution of LCP(A, b) if it is sign-solvable.
The algorithm starts with ﬁnding the residual row-mixed matrix M ′ as de-
scribed in the previous section. If the column set of M ′ does not contain the
index g, then LCP(A, b) is sign-solvable and has no solutions. Let M ′ be in the
forms of M ′ = (A′ b′), where b′ is the subvector of b and A′ is the submatrix of
A with row set U ′ and column set V ′. We denote ¯U ′ = N \ U ′ and ¯V ′ = N \ V ′.
Note that V ′ ⊆U ′ holds. If M ′ has a row and M ′ is not totally sign-nonsingular,
then return that LCP(A, b) is not sign-solvable by Theorem 3.1.
Assume that M ′ has no rows. Then LCP(A, b) is sign-solvable, and (b, 0) is a
unique solution of LCP(A, b). Next assume that M ′ has a row and M ′ = (A′ b′) is
totally sign-nonsingular. Then LCP(A, b) is sign-solvable by Theorem 3.1. Since
M ′ is row-mixed, there exists ˜
M = ( ˜A ˜b) ∈Q(M) such that the sum of the
columns of ˜
M ′ ∈Q(M ′) is zero. Hence it follows from (5) that the pair (w, z),
deﬁned to be z[ ¯V ′] = 0, z[V ′] = +1, and w = ˜Az +˜b, is a solution of LCP( ˜A,˜b).
This means that the vector w satisﬁes that wj > 0 if j ∈¯U ′ and A[{j}, V ′] has
nonzero entries, and wj = 0 otherwise. Since LCP(A, b) is sign-solvable, (w, z)
is the sign pattern of a solution of LCP(A, b).
We now summarize the algorithm description.

Sign-Solvable Linear Complementarity Problems
407
Algorithm: An algorithm for LCPs with nonzero diagonals.
Input: A linear complementarity problem LCP(A, b) with nonzero diagonals.
Output: The sign pattern of a solution if LCP(A, b) is sign-solvable.
Step 1: Set M (1) = M and ν = 1. Repeat the following until I(ν) = J(ν) = ∅.
1-1: Find I(ν)
−
and I(ν)
+ , where I(ν)
−
is the set of nonpositive rows in M (ν),
and I(ν)
+
is the set of nonnegative rows that have a nonzero entry in
M (ν).
1-2: If g ∈Γ(I(ν)
−), then return that LCP(A, b) is sign-solvable and has no
solutions.
1-3: Let I(ν) = I(ν)
+ ∪I(ν)
−
and J(ν) = I(ν)
+ ∪Γ(I(ν)
−). Deﬁne M (ν+1) to be the
matrix obtained by deleting the rows indexed by I(ν) and the columns
indexed by J(ν) from M (ν).
1-4: Set ν = ν + 1 and go back to Step 1.
Step 2: Let M ′ = (A′ b′) be the remaining submatrix, and U ′, V ′ be the row
and column sets of A′, respectively. If M ′ has a row and M ′ is not totally
sign-nonsingular, then return that LCP(A, b) is not sign-solvable. Otherwise
go to Step 3.
Step 3: Return that LCP(A, b) is sign-solvable and do the following.
3-1: If U ′ is empty, then return the sign pattern of a solution (w, z) = (b, 0).
3-2: Otherwise, return the sign pattern of (w, z) deﬁned to be
sgn zj =

+, if j ∈V ′
0, otherwise
and
sgn wj =

+, if j ∈K
0, otherwise
(6)
where K is the set of rows which have nonzero entries in A[ ¯U ′, V ′], that
is, K = {j ∈¯U ′ | Γ({j}) ∩V ′ ̸= ∅}.
Applying this algorithm to the example at the end of Sect. 3, we obtain the
sign pattern of a solution, w = ( 0 +
0 0 0)T and z = ( 0 0 + + +)T.
Based on this algorithm, we can compute a solution of a sign-solvable LCP as
well as the sign pattern of a solution. Suppose that M ′ has a row. The solution
(w, z) with the obtained sign pattern satisﬁes that A′z[V ′] + b′ = 0, z[ ¯V ′] = 0.
Since A′ is nonsingular by total sign-nonsingularity of M ′, we can compute a
solution of LCP(A, b) by performing Gaussian elimination.
The running time bound of the algorithm is now given as follows. Note that
an n × (n + 1) row-mixed matrix A is a totally sign-nonsingular matrix with
row-full term-rank if and only if all square submatrices of order n are sign-
nonsingular [1, Theorem 5.3.3]. Such matrix is called an S-matrix in [1,15], which
can be recognized in O(n2) time [14].
Theorem 4.1. For a linear complementarity problem LCP(A, b) with nonzero
diagonals, let n be the matrix size of A, and γ the number of nonzero entries in A
and b. Then the algorithm tests sign-solvability in O(n2) time, and, if LCP(A, b)
is sign-solvable, the algorithm ﬁnds the sign pattern of a solution in O(γ) time.

408
N. Kakimura
Proof. In the νth iteration in Step 1, it requires O(γν) time to ﬁnd I(ν) and
J(ν), where γν is the number of nonzero entries in the columns deleted in the νth
iteration. Since each column is deleted at most once, Step 1 takes O(γ) time in
total. In Step 2, if the residual row-mixed matrix M ′ is totally sign-nonsingular,
M ′ has row-full term-rank and the column size is one larger than the row size.
Hence testing total sign-nonsingularity of M ′ is equivalent to recognizing S-
matrices. Thus it requires O(n2) time to test sign-solvability in Step 2. Step 3
requires O(γ) time. Thus this statement holds.
⊓⊔
Acknowledgements
The author is very obliged to Satoru Iwata for his suggestions and reading the
draft of the paper. This work is supported by the 21st Century COE Program on
Information Science and Technology Strategic Core at the University of Tokyo
from the Ministry of Education, Culture, Sports, Science and Technology of
Japan.
References
1. Brualdi, R.A., Shader, B.L.: Matrices of Sign-solvable Linear Systems. Cambridge
University Press, Cambridge (1995)
2. Chandrasekaran, R., Kabadi, S.N., Murty, K.G.: Some NP-complete problems in
linear programming. Operations Research Letters 1 (1982) 101–104
3. Chung, S.J.: NP-completeness of the linear complementarity problem. Journal of
Optimization Theory and Applications 60 (1989) 393–399
4. Cottle, R.W.:
The principal pivoting method of quadratic programming.
In
Dantzig, G.B., Veinott, A.F., eds.: Mathematics of Decision Sciences, Part 1. Amer-
ican Mathematical Society, Providence R. I. (1968) 142–162
5. Cottle, R.W., Dantzig, G.B.: Complementary pivot theory of mathematical pro-
gramming. Linear Algebra and Its Applications 1 (1968) 103–125
6. Cottle, R.W., Pang, J.S., Stone, R.E.:
The Linear Complementarity Problem.
Academic Press (1992)
7. Coxson, G.E.: The P-matrix problem is co-NP-complete. Mathematical Program-
ming 64 (1994) 173–178
8. Eijndhoven, J.T.J.V.: Solving the linear complementarity problem in circuit sim-
ulation. SIAM Journal on Control and Optimization 24 (1986) 1050–1062
9. Fischer, K.G., Morris, W., Shapiro, J.: Mixed dominating matrices. Linear Algebra
and Its Applications 270 (1998) 191–214
10. Fischer, K.G., Shapiro, J.: Mixed matrices and binomial ideals. Journal of Pure
and Applied Algebra 113 (1996) 39–54
11. Iwata, S., Kakimura, N.: Solving linear programs from sign patterns. Mathematical
Programming, to appear.
12. Kim, S.J., Shader, B.L.: Linear systems with signed solutions. Linear Algebra and
Its Applications 313 (2000) 21–40
13. Kim, S.J., Shader, B.L.: On matrices which have signed null-spaces. Linear Algebra
and Its Applications 353 (2002) 245–255

Sign-Solvable Linear Complementarity Problems
409
14. Klee, V.: Recursive structure of S-matrices and O(m2) algorithm for recognizing
strong sign-solvability. Linear Algebra and Its Applications 96 (1987) 233–247
15. Klee, V., Ladner, R., Manber, R.: Sign-solvability revisited. Linear Algebra and
Its Applications 59 (1984) 131–158
16. Lemke, C.E.: Bimatrix equilibrium points and mathematical programming. Man-
agement Science 11 (1965) 681–689
17. Lov´asz, L., Plummer, M.D.: Matching Theory. Volume 29 of Annals of Discrete
Mathematics. North-Holland, Amsterdam (1986)
18. McCuaig, W.: Brace decomposition. Journal of Graph Theory 38 (2001) 124–169
19. McCuaig, W.: P´olya’s permanent problem. The Electronic Journal of Combina-
torics 11, R79 (2004)
20. Murty, K.G.: Linear Complementarity, Linear and Nonlinear Programming. Inter-
net Edition (1997)
21. P´olya, G.: Aufgabe 424. Archiv der Mathematik und Physik 20 (1913) 271
22. Robertson, N., Seymour, P.D., Thomas, R.: Permanents, Pfaﬃan orientations, and
even directed circuits. Annals of Mathematics 150 (1999) 929–975
23. Rohn, J.: Systems of linear interval equations. Linear Algebra and Its Applications
126 (1989) 39–78
24. Samuelson, P.A.: Foundations of Economics Analysis. Harvard University Press,
1947; Atheneum, New York, 1971.
25. Seymour, P., Thomassen, C.: Characterization of even directed graphs. Journal of
Combinatorial Theory, Series B 42 (1987) 36–45
26. Shao, J.Y., Ren, L.Z.: Some properties of matrices with signed null spaces. Discrete
Mathematics 279 (2004) 423–435
27. Vazirani, V.V., Yannakakis, M.: Pfaﬃan orientations, 0-1 permanents, and even
cycles in directed graphs. Discrete Applied Mathematics 25 (1989) 179–190

An Integer Programming Approach for
Linear Programs with Probabilistic Constraints
James Luedtke, Shabbir Ahmed, and George Nemhauser
H. Milton Stewart School of Industrial and Systems Engineering
Georgia Institute of Technology
Atlanta, GA, USA
{jluedtke,sahmed,gnemhaus}@isye.gatech.edu
Abstract. Linear programs with joint probabilistic constraints (PCLP)
are known to be highly intractable due to the non-convexity of the feasi-
ble region. We consider a special case of PCLP in which only the right-
hand side is random and this random vector has a ﬁnite distribution.
We present a mixed integer programming formulation and study the re-
laxation corresponding to a single row of the probabilistic constraint,
yielding two strengthened formulations. As a byproduct of this analy-
sis, we obtain new results for the previously studied mixing set, subject
to an additional knapsack inequality. We present computational results
that indicate that by using our strengthened formulations, large scale
instances can be solved to optimality.
Keywords: Integer programming, probabilistic constraints, stochastic
programming.
1
Introduction
Consider a linear program with a probabilistic or chance constraint
(PCLP)
min

cx : x ∈X, P{ ˜Tx ≥ξ} ≥1 −ϵ

(1)
where X =

x ∈Rd
+ : Ax = b

is a polyhedron, c ∈Rd, ˜T is an m × d random
matrix, ξ is a random vector taking values in Rm, and ϵ is a conﬁdence parameter
chosen by the decision maker, typically near zero, e.g., ϵ = 0.01 or ϵ = 0.05. Note
that in (1) we enforce a single probabilistic constraint over all rows, rather than
requiring that each row independently be satisﬁed with high probability. Such
a constraint is known as a joint probabilistic constraint, and is appropriate in a
context in which it is important to have all constraints satisﬁed simultaneously
and there may be dependence between random variables in diﬀerent rows.
Problems with joint probabilistic constraints have been extensively studied;
see [1] for background and an extensive list of references. Probabilistic constraints
have been used in various applications including supply chain management [2],
production planning [3], optimization of chemical processes [4,5] and surface
water quality management [6]. Unfortunately, linear programs with probabilistic
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 410–423, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

Integer Programming Approach for Probabilistic Constraints
411
constraints are still largely intractable except for a few very special cases. There
are two primary reasons for this intractability. First, in general, for a given
x ∈X, the quantity φ(x) := P{ ˜Tx ≥ξ} is hard to compute, as it requires multi-
dimensional integration. Second, the feasible region deﬁned by a probabilistic
constraint is generally not convex.
Recently, several approaches have been proposed which can ﬁnd highly reli-
able feasible solutions to probabilistic programs. Examples of these conservative
approximations include scenario approximation [7,8], Bernstein approximation
[9] and robust optimization, e.g., [10,11,12]. These methods are attractive when
high reliability is most important and solution cost is a secondary objective.
However, when very high reliability is not crucial, for example if the proba-
bilistic constraint represents a service level constraint, a decision maker may be
interested in exploring the trade-oﬀbetween solution cost and system reliability,
and would be interested in obtaining solutions which are on or near the eﬃcient
frontier of these competing objectives. The aforementioned conservative approx-
imations generally do not yield bounds on the optimal solution cost at a given
reliability level ϵ, and hence cannot distinguish whether the produced solutions
are close to the eﬃcient frontier. This latter context is the motivation for using
integer programming to solve PCLP so that we can obtain solutions that are
provably optimal or near optimal.
In this work, we demonstrate that by using integer programming techniques,
PCLP can be solved eﬃciently under the following two simplifying assumptions:
(A1) Only the right-hand side vector ξ is random; the matrix ˜T = T is deter-
ministic.
(A2) The random vector ξ has a ﬁnite distribution.
Despite its restrictiveness, the special case given by assumption A1 has received a
lot of attention in the literature, see, e.g., [1,13,14]. A notable result for this case
is that if the distribution of the right-hand side is log-concave, then the feasible
region deﬁned by the joint probabilistic constraint is convex [15]. This allows
problems with small dimension of the random vector to be solved to optimal-
ity, but higher dimensional problems are still intractable due to the previously
mentioned diﬃculty in checking feasibility of the probabilistic constraint. Spe-
cialized methods have been developed in [14] for the case in which assumption
A1 holds and the random vector has discrete but not necessarily ﬁnite distri-
bution. However, these methods also do not scale well with the dimension of
the random vector. Assumption A2 may also seem very restrictive. However, if
the possible values for ξ are generated by taking Monte Carlo samples from a
general distribution, we can think of the resulting problem as an approxima-
tion of the problem with this distribution. Under some reasonable assumptions
we can show that the optimal solution of the sampled problem converges expo-
nentially fast to the optimal solution of the original problem as the number of
scenarios increases. Also, the optimal objective of the sampled problem can be
used to develop statistical lower bounds on the optimal objective of the original
problem. See [16,17,18] for some related results. It seems that the reason such
a sampling approach has not been seriously considered for PCLP in the past is

412
J. Luedtke, S. Ahmed, and G. Nemhauser
that the resulting sampled problem has a non-convex feasible region, and thus
is still generally intractable. Our contribution is to demonstrate that, at least
under assumption A1, it is nonetheless possible to solve the sampled problem in
practice.
Under assumption A2 it is possible to write a mixed integer programming
formulation for PCLP, as has been done, for example, in [19]. In the general
case, such a formulation requires the introduction of “big-M” type constraints,
and hence is diﬃcult to solve. However, the particular case of assumption A1 has
not been studied from an integer programming perspective; by doing so, we are
able to develop strong mixed integer programming formulations. Our approach
in developing these formulations is to consider the relaxation obtained from a
single row in the probabilistic constraint. It turns out that this yields a system
similar to the mixing set introduced by G¨unl¨uk and Pochet [20], subject to an
additional knapsack inequality. We are able to derive strong valid inequalities for
this system by ﬁrst using the knapsack inequality to “pre-process” the mixing
set, then applying the mixing inequalities of [20], see also [21,22]. We also derive
an extended formulation, equivalent to one given by Miller and Wolsey in [23].
Making further use of the knapsack inequality, we are able to derive more general
classes of valid inequalities, for both the original and extended formulations. If
all scenarios are equally likely, the knapsack inequality reduces to a cardinality
restriction. In this case, we are able to characterize the convex hull of feasible
solutions to the extended formulation for the single row case. Although these
results are motivated by the application to PCLP, they can be used in any
problem in which a mixing set appears along with a knapsack constraint.
2
The MIP Formulation
We now consider a probabilistically constrained linear programming problem,
with random right-hand side given by
(PCLPR)
min cx
s.t. Ax = b
P{T x ≥ξ} ≥1 −ϵ
x ≥0 .
(2)
Here A is an r × d matrix, b ∈Rr, T is an m × d matrix, ξ is a random vector
in Rm, ϵ ∈(0, 1) (typically small) and c ∈Rd. We assume that ξ has ﬁnite
support, that is there exist vectors, ξi, i = 1, . . . , n such that P{ξ = ξi} = πi for
each i where πi ≥0 and n
i=1 πi = 1. We will refer to the possible outcomes as
scenarios. We assume without loss of generality that ξi ≥0 and πi ≤ϵ for each
i. We also deﬁne the set N = {1, . . . , n}.
Before proceeding, we note that PCLPR is NP-hard even under assumptions
A1 and A2.
Theorem 1. PCLPR is NP-hard, even in the special case in which πi = 1/n
for all i ∈N, the constraints Ax = b are not present, T is the m × m identity
matrix, and c = (1, . . . , 1) ∈Rm.

Integer Programming Approach for Probabilistic Constraints
413
We now formulate PCLPR as a mixed integer program [19]. To do so, we in-
troduce for each i ∈N, a binary variable zi, where zi = 0 will guarantee that
T x ≥ξi. Observe that because ϵ < 1 we must have T x ≥ξi for at least one
i ∈N, and because ξi ≥0 for all i, this implies T x ≥0 in any feasible solution
of PCLPR. Then, introducing variables v ∈Rm to summarize T x, we obtain the
MIP formulation of PCLPR given by
(PMIP)
min cx
s.t. Ax = b, T x −v = 0
(3)
v + ξizi ≥ξi
i = 1, . . . , n
(4)
n

i=1
πizi ≤ϵ
(5)
x ≥0,
z ∈{0, 1}n .
3
Strengthening the Formulation
Our approach is to strengthen PMIP by ignoring (3) and ﬁnding strong formu-
lations for the set
F :=

(v, z) ∈Rm
+ × {0, 1}n : (4), (5)

.
(6)
Note that
F =
m

j=1
{(v, z) : (vj, z) ∈Gj} ,
where for j = 1, . . . , m
Gj = {(vj, z) ∈R+ × {0, 1}n : (5),
vj + ξijzi ≥ξij
i = 1, . . . , n} .
Thus, a natural ﬁrst step in developing a strong formulation for F is to develop a
strong formulation for each Gj. In particular, note that if an inequality is facet-
deﬁning for conv(Gj), then it is also facet-deﬁning for conv(F). This follows
because if an inequality valid for Gj is supported by n + 1 aﬃnely independent
points in Rn+1, then because this inequality will not have coeﬃcients on vi for
any i ̸= j, the set of supporting points can trivially be extended to a set of n+m
aﬃnely independent supporting points in Rn+m by appropriately setting the vi
values for each i ̸= j.
The above discussion leads us to consider the generic set
G = {(y, z) ∈R+ × {0, 1}n : (5),
y + hizi ≥hi
i = 1, . . . , n}
(7)
obtained by dropping the index j in the set Gj and setting y = vj and hi = ξij
for each i. For convenience, we assume that the hi are ordered so that h1 ≥h2 ≥
· · · ≥hn. The mixing set
P = {(y, z) ∈R+ × {0, 1}n : y + hizi ≥hi
i = 1, . . . , n}

414
J. Luedtke, S. Ahmed, and G. Nemhauser
has been extensively studied, in varying degrees of generality, by Atamt¨urk et. al
[21], G¨unl¨uk and Pochet [20], Guan et. al [22] and Miller and Wolsey [23]. If we
ignore the knapsack constraint in G, we can apply these results to obtain the
set of valid inequalities
y +
l

j=1
(htj −htj+1)ztj ≥ht1 ∀T = {t1, . . . , tl} ⊆N ,
(8)
where t1 < t2 < · · · < tl and htl+1 := 0. Following [21], we call (8) the star
inequalities. In addition, these inequalities can be separated in polynomial time
[20,21,22]. It has been shown in these same works that these inequalities deﬁne
the convex hull of P and are facet deﬁning if and only if t1 = 1. We can do
considerably better, however, by using the knapsack constraint in G to ﬁrst
strengthen the inequalities, and then derive the star inequalities. In particular,
let p := max

k : k
i=1 πi ≤ϵ

. Then, due to the knapsack constraint, we cannot
have zi = 1 for all i = 1, . . . , p + 1 and thus we have y ≥hp+1. This also implies
that the mixed integer constraints in G are redundant for i = p+1, . . ., n. Thus,
we can write a tighter formulation of G as
G = {(y, z) ∈R+ × {0, 1}n : (5),
y + (hi −hp+1)zi ≥hi
i = 1, . . . , p} . (9)
Remark 1. In addition to yielding a tighter relaxation, this description of G is
also more compact. In typical applications, ϵ will be near 0, suggesting p << n.
When applied for each j in the set F, this will yield a formulation with mp <<
mn rows.
If we now apply the star inequalities to the improved formulation of G, we obtain
the following result, which can be obtained by applying results in [20],[21] or [22].
Theorem 2. The inequalities
y +
l

j=1
(htj −htj+1)ztj ≥ht1
∀T = {t1, . . . , tl} ⊆{1, . . . , p}
(10)
with t1 < . . . < tl and htl+1 := hp+1, are valid for G. Moreover, (10) is facet-
deﬁning for conv(G) if and only if ht1 = h1.
We refer to the inequalities (10) as the strengthened star inequalities.
Remark 2. The diﬀerence between the star inequalities (8) and strengthened
star inequalities (10) is that in (10) we have htl+1 := hp+1 whereas in (8) we
have htl+1 := 0, corresponding to the fact that our lower bound on y was shifted
from 0 to hp+1 by using the knapsack inequality.
Remark 3. The strengthened star inequalities are not suﬃcient to characterize
the convex hull of G, even in the special case in which all probabilities are equal,
that is πi = 1/n for all i.

Integer Programming Approach for Probabilistic Constraints
415
We now consider the special case in which πi = 1/n for all i ∈N. Note that
in this case we have p := max

k : k
i=1 1/n ≤ϵ

= ⌊nϵ⌋and the knapsack
constraint (5) becomes
n

i=1
zi ≤nϵ
which, by integrality on zi can be strengthened to the simple cardinality
restriction
n

i=1
zi ≤p .
(11)
Thus, the feasible region for our single row formulation becomes
G′ = {(y, z) ∈R+ × {0, 1}n : (11),
y + (hi −hp+1)zi ≥hi
i = 1, . . . , p} .
Now, observe that for any (γ, α) ∈Rn+1, the problem
min {γy + αz : (y, z) ∈G′}
is easy. Indeed, if γ < 0, then the problem is unbounded, so we can assume
γ ≥0. Then, one need only consider setting y to hk for k = 1, . . . , p + 1, and
setting the zi accordingly. That is, if y = hk for k ∈{1, . . . , p + 1}, then we must
set zi = 1 for i = 1, . . . , k −1. The remaining zi can be set to 0 or 1 as long as
n
i=k zi ≤p−k+1. Hence, we set zi = 1 if and only if i ∈S∗
k where
S∗
k ∈arg min
S⊆{k,...,n}
	
i∈S
αi : |S| ≤p−k+1

.
Since we can optimize over G′ eﬃciently, we know that we can separate over
conv(G′) eﬃciently. Indeed, given (y∗, z∗) we can write an explicit polynomial
size linear program for separation over conv(G′). Although this would yield a
theoretically eﬃcient way to separate over conv(G′), it still may be too expensive
to solve a linear program to generate cuts. We would therefore prefer to have an
explicit characterization of a class or classes of valid inequalities for G′ with an
associated combinatorial algorithm for separation. The following theorem gives
an example of one such class.
Theorem 3. Let m ∈{1, . . . , p −1}, T = {t1, . . . , tl} ⊆{1, . . . , m} and Q =
{q1, . . . , qp−m} ⊆{p+1, . . ., n} . Deﬁne Δm
1 = hm+1 −hm+2 and
Δm
i = max
⎧
⎨
⎩Δm
i−1, hm+1 −hm+i+1 −
i−1

j=1
Δm
j
⎫
⎬
⎭for i = 2, . . . , p−m .
Then, with htl+1 := hm+1,
y +
l

j=1
(htj −htj+1)ztj +
p−m

j=1
Δm
j (1 −zqj) ≥ht1
(12)
is valid for G′ and facet-deﬁning for conv(G′) if and only if ht1 = h1.

416
J. Luedtke, S. Ahmed, and G. Nemhauser
Example 1. Let n = 10 and ϵ = 0.4 so that p = 4 and suppose h1−5 =
{20, 18, 14, 11, 6}. Let m = 2, T = {1, 2} and Q = {5, 6}. Then, Δ2
1 = 3 and
Δ2
2 = max {3, 8 −3} = 5 so that (12) yields
y + 2z1 + 4z3 + 3(1 −z5) + 5(1 −z6) ≥20 .
Separation of inequalities (12) can be accomplished by a simple modiﬁcation to
the routine for separating the strengthened star inequalities. We have identiﬁed
other classes of valid inequalities, but have so far failed to ﬁnd a general class
that characterizes the convex hull of G′.
4
A Strong Extended Formulation
Let
FS = {(y, z) ∈R+ × [0, 1]n : (5), (10)} .
FS represents the polyhedral relaxation of G, augmented with the strengthened
star inequalities. Note that the inequalities y + (hi −hp+1)zi ≥hi are included
in FS by taking T = {i}, so that enforcing integrality in FS would yield a valid
single row formulation for the set G. Our aim is to develop a reasonably compact
extended formulation which is equivalent to FS. To do so, we introduce variables
w1, . . . , wp and let
EG =

(y, z, w) ∈R+ × {0, 1}n+p : (13) −(16)

where
wi −wi+1 ≥0 i = 1, . . . , p
(13)
zi −wi ≥0 i = 1, . . . , p
(14)
y +
p

i=1
(hi −hi+1)wi ≥h1
(15)
n

i=1
πizi ≤ϵ .
(16)
and wp+1 := 0. The variables wi can be interpreted as deciding whether or not
scenario i is satisﬁed for the single row under consideration, and because they
are speciﬁc to this single row, the inequalities (13) can be safely added. The
inequalities (14) then ensure that if a scenario is infeasible for this row, then it is
infeasible overall, and the lower bound on y is now given by the single inequality
(15). We let EF be the polyhedron obtained by relaxing integrality in EG.
Theorem 4. Proj(y,z)(EG) = G, that is, EG is a valid formulation for G.
An interesting result is that the linear relaxation of this extended formulation is
as strong as having all strengthened star inequalities in the original formulation.
A similar result has been proved in [23].

Integer Programming Approach for Probabilistic Constraints
417
Theorem 5. Proj(y,z)(EF) = FS.
Because of the equivalence between EF and FS, Remark 3 holds for this formu-
lation as well, that is, even in the special case in which all probabilities are equal,
this formulation does not characterize the convex hull of feasible solutions of G.
We therefore investigate what other valid inequalities exist for this formulation.
We ﬁrst introduce the notation
fk :=
k

i=1
πi,
k = 0, 1, . . ., p .
Theorem 6. Let k ∈{1, . . . , p} and let S ⊆{k, . . . , n} be such that 
i∈S πi ≤
ϵ −fk−1. Then,

i∈S
πizi +

i∈{k...,p}\S
πiwi ≤ϵ −fk−1
(17)
is valid for EG.
Now, consider the special case in which πi = 1/n for i = 1, . . . , n. Then the
extended formulation becomes
EG′ =

(y, z, w) ∈R+ × {0, 1}n+p : (11) and (13) −(15)

.
Letting Sk = {S ⊆{k, . . . , n} : |S| ≤p−k+1} for k = 1, . . . , p, the inequali-
ties (17) become

i∈S
zi +

i∈{k,...,p}\S
wi ≤p−k+1
∀S ∈Sk, k = 1, . . . , p .
(18)
Example 2. Let n = 10 and ϵ = 0.4 so that p = 4. Let k = 2 and S = {4, 5, 6}.
Then (18) becomes
z4 + z5 + z6 + w2 + w3 ≤3 .
Now, let
EH′ =

(y, z, w) ∈R+ × [0, 1]n+p : (11), (13) −(15) and (18)

be the linear relaxation of the extended formulation, augmented with this set of
valid inequalities.
Theorem 7. The convex hull of the extended formulation EG′ is given by the
inequalities deﬁning EG′ and the inequalities (18); that is, EH′ = conv(EG′).
We close this section by noting that inequalities (18) can be separated in poly-
nomial time. Indeed, suppose we wish to separate the point (z∗, w∗). Then sep-
aration can be accomplished by solving
max
S∈Sk
⎧
⎨
⎩

i∈S
z∗
i +

i∈{k,...,p}\S
w∗
i
⎫
⎬
⎭= max
S∈Sk
	
i∈S
θ∗
i

+
p

i=k
w∗
i

418
J. Luedtke, S. Ahmed, and G. Nemhauser
for k = 1, . . . , p where
θ∗
i =
z∗
i −w∗
i i = 1, . . . , p
z∗
i
i = p + 1, . . . , n .
Hence, a trivial separation algorithm is to ﬁrst sort the values θ∗
i in non-
increasing order, then for each k, ﬁnd the maximizing set S ∈Sk by search-
ing this list. This yields an algorithm with complexity O(n log n + p2) = O(n2).
However, by considering the values of k in the order p, . . . , 1 and updating an or-
dered list of eligible indices Sk for each k, it is possible to improve the complexity
to O(n log n). For the more general inequalities (17), (heuristic) separation can
be accomplished by (heuristically) solving p knapsack problems.
5
Computational Experience
We performed computational tests on a probabilistic version of the classical
transportation problem. We have a set of suppliers S and a set of customers D
with |D| = m. The suppliers have limited capacity Mi for i ∈S. There is a
per-unit transportation cost cij for (producing and) shipping a unit of product
from supplier i ∈S to customer j ∈D. The customer demands are random and
are represented by a random vector ˜d ∈Rm
+. We assume we must choose the
shipment quantities before the customer demands are known. We enforce the
following probabilistic constraint:
P{

i∈S
xij ≥˜dj, j = 1, . . . , m} ≥1 −ϵ .
(19)
The objective is to minimize distribution costs subject to (19), non-negativity
on the ﬂow variables xij, and the supply capacity constraints

j∈D
xij ≤Mi,
∀i ∈S .
We randomly generated instances with the number of suppliers ﬁxed at 40 and
varying numbers of customers and scenarios. The supply capacities and cost
coeﬃcients were generated using normal and uniform distributions respectively.
For the random demands, we experimented with independent normal, dependent
normal and independent Poisson distributions. We found qualitatively similar
results in all cases, but the independent normal case yielded the most challenging
instances, so for our experiments we focus on this case. For each instance, we ﬁrst
randomly generated the mean and variance of each customer demand. We then
generated the number n of scenarios required, independently across scenarios and
across customer locations, as Monte Carlo samples with these ﬁxed parameters.
In most instances we assumed all scenarios occur with probability 1/n, but
we also did some tests in which the scenarios have general probabilities, which
were also randomly generated. CPLEX 9.0 was used as the MIP solver and all
experiments were done on a computer with two 2.4 Ghz processors (although

Integer Programming Approach for Probabilistic Constraints
419
no parallelism is used) and 2.0 Gigabytes of memory. We set a time limit of
one hour. For each problem size we generated 5 random instances and, unless
otherwise speciﬁed, the computational results reported are averages over the 5
instances.
5.1
Comparison of Formulations
In Table 1 we compare the results obtained by solving our instances using
1. formulation PMIP given by (3) - (5),
2. formulation PMIP with strengthened star inequalities (10), and
3. the extended formulation of Sect. 4, but without (17) or (18).
When the strengthened star inequalities are not used, we still used the improved
formulation of G corresponding to (9). Recall that the strengthened star inequal-
ities subsume the rows deﬁning the formulation PMIP; therefore, when we us-
ing these inequalities we initially add only a small subset of the mp inequalities
in the formulation. Subsequently separating the strengthened star inequalities as
needed guarantees the formulation remains valid. For formulation PMIP without
strengthened star inequalities, we report the average optimality gap that remained
after the hour time limit was reached. For the other two formulations, which we
refer to as the strong formulations, we report the geometric average of the time
to solve the instances to optimality. We used ϵ = 0.05 and ϵ = 0.1, reﬂecting the
natural assumption that we want to meet demand with high probability.
The ﬁrst observation from Table 1 is that formulation PMIP without the
strengthened star inequalities fails to solve these instances within an hour, often
leaving large optimality gaps, whereas the instances are solved eﬃciently using
the strong formulations. The number of nodes required to solve the instances for
the strong formulations is very small. The instances with equi-probable scenarios
were usually solved at the root, and even when branching was required, the root
relaxation usually gave an exact lower bound. Branching in this case was only
required to ﬁnd an integer solution which achieved this bound. The instances
with general probabilities required slightly more branching, but generally not
more than 100 nodes. Observe that the number of strengthened star inequalities
that were added is small relative to the number of rows in the formulation PMIP
itself. For example, for ϵ = 0.1, m = 200 and n = 3, 000, the number of rows in
PMIP would be mp = 60, 000, but on average, only 5, 541 strengthened star in-
equalities were added. Next we observe that in most cases the computation time
using the extended formulation is signiﬁcantly less than the formulation with
strengthened star inequalities. Finally, we observe that the instances with gen-
eral probabilities take somewhat longer to solve than those with equi-probable
scenarios but can still be solved eﬃciently.
5.2
Testing Inequalities (18)
With small ϵ the root relaxation given by the extended formulation is extremely
tight, so that adding the inequalities (18) is unlikely to have a positive impact on

420
J. Luedtke, S. Ahmed, and G. Nemhauser
Table 1. Average solution times for diﬀerent formulations
PMIP
PMIP+Star
Extended
Probabilities
ϵ
m
n
Gap
Cuts Time(s) Time(s)
Equal
0.05 100 1000 0.18%
734.8
7.7
1.1
100 2000 1.29% 1414.2
31.8
4.6
200 2000 1.02% 1848.4
61.4
12.1
200 3000 2.56% 2644.0
108.6
12.4
0.10 100 1000 2.19% 1553.2
34.6
12.7
100 2000 4.87% 2970.2
211.3
41.1
200 2000 4.48% 3854.0
268.5
662.2
200 3000 5.84% 5540.8
812.7
490.4
General
0.05 100 1000 0.20%
931.8
9.0
3.9
100 2000 1.04% 1806.6
55.2
13.2
0.10 100 1000 1.76% 1866.0
28.7
52.5
100 2000 4.02% 3686.2
348.5
99.2
computation time. However, for larger ϵ, we have seen that the extended formu-
lation may have a substantial optimality gap. We therefore investigated whether
using inequalities (18) can improve solution time in this case. In Table 3 we present
results comparing solution times and node counts with and without inequalities
(18) for instances with larger ϵ. We performed these tests on smaller instances
since these instances are already hard for these values of ϵ. We observe that adding
inequalities (18) at the root can decrease the root optimality gap signiﬁcantly. For
the instances that could be solved in one hour, this leads to a signiﬁcant reduction
in the number of nodes explored, and a moderate reduction in solution time. For
the instances which were not solved in one hour, the remaining optimality gap
was usually, but not always, lower when the inequalities (18) were used. These
results indicate that when ϵ is somewhat larger, inequalities (18) may be helpful
on smaller instances. However, they also reinforce the diﬃculty of the instances
with larger ϵ, since even with these inequalities, only the smallest of these smaller
instances could be solved to optimality within an hour.
5.3
The Eﬀect of Increasing ϵ
The results of Table 1 indicate that the strong formulations can solve large in-
stances to optimality when ϵ is small, which is the typical case. However, it
is still an interesting question to investigate how well this approach works for
larger ϵ. Note ﬁrst that we should expect solution times to grow with ϵ if only
because the formulation sizes grow with ϵ. However, we observe from Table 2
that the situation is much worse than this. This table shows the root LP solve
times and optimality gaps achieved after an hour of computation time for an
example instance with equi-probable scenarios, m = 50 rows and n = 1, 000 sce-
narios at increasing levels of ϵ, using the formulation PMIP with strengthened
star inequalities. Root LP solve time here refers to the time until no further
strengthened star inequalities could be separated. We see that the time to solve

Integer Programming Approach for Probabilistic Constraints
421
Table 2. Eﬀects of increasing ϵ on an instance with m = 50 and n = 1000
ϵ
0.10 0.20 0.30
0.40
0.50
0.60
0.70
0.80
0.90
Root LP Time (s) 21.7 37.1 82.7 144.3 227.8 327.6 505.6 792.6 1142.6
Optimality Gap
0.0% 0.0% 2.2% 5.8% 10.5% 16.2% 28.7% 35.1% 44.4%
the root linear programs does indeed grow with ϵ as expected, but the optimality
gaps achieved after an hour of computation time deteriorate even more drasti-
cally with growing ϵ. This is explained by the increased time to solve the linear
programming relaxations combined with a weakening of the relaxation bound as
ϵ increases.
Table 3. Results with and without inequalities (18)
Root Gap
Nodes
Time(s) or Gap
m
ϵ
n
Ext
+(18)
Ext
+(18)
Ext
+(18)
25
0.3
250 1.18% 0.67%
276.9
69.0
121.2
93.9
0.3
500 1.51% 0.58%
455.0 165.8
750.6
641.3
0.35
250 2.19% 1.50% 1259.4 409.0
563.2
408.4
0.35
500 2.55% 1.61% 2297.6 968.8 0.22%
0.06%
50
0.3
500 2.32% 2.00%
991.8 238.6 1.37%
1.41%
0.3 1000 2.32% 1.75%
28.3
8.5 1.98%
1.66%
0.35
500 4.10% 3.31%
650.4
92.9 3.03%
2.66%
0.35 1000 4.01% 3.23%
22.7
6.2 3.58%
3.17%
6
Concluding Remarks
We have presented strong integer programming formulations for linear programs
with probabilistic constraints in which the right-hand side is random with ﬁnite
distribution. In the process we made use of existing results on mixing sets, and
have introduced new results for the case in which the mixing set additionally
has a knapsack restriction. Computational results indicate that these formula-
tions are extremely eﬀective on instances in which reasonably high reliability
is enforced, which is the typical case. However, instances in which the desired
reliability level is lower remain diﬃcult to solve, partly due to increased size of
the formulations, but more signiﬁcantly due to the weakening of the formulation
bounds. Moreover, these instances remain diﬃcult even when using the inequal-
ities which characterize the single row relaxation convex hull. This suggests that
relaxations which consider multiple rows simultaneously need to be studied to
yield valid inequalities which signiﬁcantly improve the relaxation bounds for
these instances.
Future work in this area should focus on addressing the two assumptions we
made at the beginning of this paper. The ﬁnite distribution assumption can be
addressed by using the results about the statistical relationship between a prob-
lem with probabilistic constraints and its Monte Carlo sample approximation

422
J. Luedtke, S. Ahmed, and G. Nemhauser
to establish methods for generating bounds on the optimal value of the original
problem. Computational studies will need to be performed to establish the prac-
ticality of this approach. We expect that relaxing the assumption that only the
right-hand side is random will be more challenging. A natural ﬁrst step in this
direction will be to extend results from the generalized mixing set [23,24] to the
case in which an additional knapsack constraint is present.
Acknowledgments. This research has been supported in part by the National
Science Foundation under grants DMI-0121495 and DMI-0522485.
References
1. Pr´ekopa, A.: Probabilistic programming. In Ruszczy´nski, A., Shapiro, A., eds.:
Stochastic Programming. Volume 10 of Handbooks in Operations Research and
Management Science. Elsevier (2003)
2. Lejeune, M.A., Ruszczy´nski, A.: An eﬃcient trajectory method for probabilistic
inventory-production-distribution problems. Operations Research (Forthcoming,
2007)
3. Murr, M.R., Pr´ekopa, A.: Solution of a product substitution problem using stochas-
tic programming. In Uryasev, S.P., ed.: Probabilistic Constrained Optimization:
Methodology and Applications. Kluwer Academic Publishers (2000) 252–271
4. Henrion, R., Li, P., M¨oller, A., Steinbach, M.C., Wendt, M., Wozny, G.: Stochastic
optimization for operating chemical processes under uncertainty. In Gr¨otschel, M.,
Krunke, S., Rambau, J., eds.: Online Optimization of Large Scale Systems. (2001)
457–478
5. Henrion, R., M¨oller, A.: Optimization of a continuous distillation process under
random inﬂow rate.
Computers and Mathematics with Applications 45 (2003)
247–262
6. Takyi, A.K., Lence, B.J.:
Surface water quality management using a multiple-
realization chance constraint method. Water Resources Research 35 (1999) 1657–
1670
7. Calaﬁore, G., Campi, M.: Uncertain convex programs: randomized solutions and
conﬁdence levels. Mathematical Programming, Ser. A 102 (2005) 25–46
8. Nemirovki, A., Shapiro, A.: Scenario approximation of chance constraints. Preprint
available at www.optimization-online.org (2004)
9. Nemirovski, A., Shapiro, A.: Convex approximations of chance constrained pro-
grams. Preprint available at www.optimization-online.org (2004)
10. Ben-Tal, A., Nemirovski, A.: Robust convex optimization. Mathematics of Oper-
ations Research 23 (1998) 769–805
11. Bertsimas, D., Sim, M.: The price of robustness. Operations Research 52 (2004)
35–53
12. Ghaoui, L.E., Lebret, H.: Robust solutions to least-squares problems with uncertain
data. SIAM Journal on Matrix Analysis and Applications 18 (1997) 1035–1064
13. Cheon, M.S., Ahmed, S., Al-Khayyal, F.: A branch-reduce-cut algorithm for the
global optimization of probabilistically constrained linear programs. Mathematical
Programming, Ser B 108 (2006) 617–634
14. Dentcheva, D., Pr´ekopa, A., Ruszczy´nski, A.: Concavity and eﬃcient points of
discrete distributions in probabilistic programming. Mathematical Programming,
Ser A 89 (2000) 55–77

Integer Programming Approach for Probabilistic Constraints
423
15. Pr´ekopa, A.: On probabilistic constrained programmming. In Kuhn, H.W., ed.:
Proceedings of the Princeton Symposium on Mathematical Programming, Prince-
ton, N.J., Princeton University Press (1970) 113–138
16. Ahmed, S., Shapiro, A.: The sample average approximation method for stochastic
programs with integer recourse. Preprint available at www.optimization-online.org
(2002)
17. Atlason, J., Epelman, M.A., Henderson, S.G.: Call center staﬃng with simulation
and cutting plane methods. Annals of Operations Research 127 (2004) 333–358
18. Shapiro, A., Homem-de-Mello, T.: On the rate of convergence of optimal solutions
of monte carlo approximations of stochastic programs. SIAM Journal of Optimiza-
tion 11 (2000) 70–86
19. Ruszczy´nski, A.: Probabilistic programming with discrete distributions and prece-
dence constrained knapsack polyhedra.
Mathematical Programming, Ser A 93
(2002) 195–215
20. G¨unl¨uk, O., Pochet, Y.: Mixing mixed-integer inequalities. Mathematical Pro-
gramming 90 (2001) 429–457
21. Atamt¨urk, A., Nemhauser, G.L., Savelsbergh, M.W.P.: The mixed vertex packing
problem. Mathematical Programming 89 (2000) 35–53
22. Guan, Y., Ahmed, S., Nemhauser, G.L.: Sequential pairing of mixed integer in-
equalities. Discrete Optimization (Forthcoming, 2007)
23. Miller, A.J., Wolsey, L.A.: Tight formulations for some simple mixed integer pro-
grams and convex objective integer programs.
Mathematical Programming 98
(2003) 73–88
24. Van Vyve, M.: The continuous mixing polyhedron. Mathematics of Operations
Research 30 (2005) 441–452

Infrastructure Leasing Problems⋆
Barbara M. Anthony1 and Anupam Gupta2
1 Dept. of Mathematical Sciences, Carnegie Mellon University, Pittsburgh PA 15213
banthony@andrew.cmu.edu
2 Computer Science Department, Carnegie Mellon University, Pittsburgh PA 15213
anupamg@cs.cmu.edu
Abstract. Consider the following Steiner Tree leasing problem. Given a
graph G = (V, E) with root r, and a sequence of terminal sets Dt ⊆V for
each day t ∈[T]. A feasible solution to the problem is a set of edges Et for
each t connecting Dt to r. Instead of obtaining edges for a single day at
a time, or for inﬁnitely long (both of which give Steiner tree problems),
we lease edges for say, { a day, a week, a month, a year }. Naturally,
leasing an edge for a longer period costs less per unit of time. What is a
good leasing strategy? In this paper, we give a general approach to solv-
ing a wide class of such problems by showing a close connection between
deterministic leasing problems and problems in multistage stochastic op-
timization. All our results are in the oﬀline setting.
Keywords: Approximation algorithms, graph and network algorithms,
stochastic combinatorial optimization, randomized algorithms.
1
Introduction
Traditional network design problems require us to make decisions about how to
send data, and how to provision bandwidth on various links of the network. A
standard feature in most models for network design that have been considered,
and in the algorithms that have been developed, has been the permanence of
the bandwidth allocation—and this has been true even in cases where demands
arrive online: once some amount of bandwidth is allocated on an edge, this
bandwidth can be used at any time in the future (perhaps by paying some
additional incremental “routing cost” per unit of ﬂow). Some works have also
considered the question of buying versus renting, but the simplifying assumption
again has been that buying gives permanent access to the commodity. But what
if we are allowed only to lease bandwidth on the links of the network for ﬁxed
lengths of time: which leases on which network links should we obtain over time
to satisfy our demands?
Given a situation with multiple lease lengths, it is natural to assume that
a longer lease is a cheaper one (per day), and that we pay more dearly for the
⋆This research is partly supported by an NSF CAREER award CCF-0448095, and by
an Alfred P. Sloan Fellowship.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 424–438, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

Infrastructure Leasing Problems
425
ﬂexibility aﬀorded by the short-term leases.1 Hence, if our traﬃc consists of some
stable parts and other bursty parts, we can use long-term leases to satisfy the
stable traﬃc, and the short-term leases to handle the more volatile demands: a
clever leasing strategy can reduce costs substantially over a na¨ıve one. Note that
solving this problem requires us to simultaneously perform clustering over space
(in order to ﬁgure out which edges to allocate bandwidth on) and over time (to
ﬁgure out which traﬃc is stable and requires longer leases, and which is bursty
and is best served by shorter leases).
The question of ﬁnding good leasing strategies is relevant in the context of
other problems as well: in planning for demands arriving over multiple periods in
classical facility location problems, one might want to lease warehouses/plants
for varying lengths of time. Moreover, the idea that leases of varying lengths
are available is fairly natural: even in situations where there is a standard lease
length (say plants are usually leased for a year), the presence of a secondary
market for reselling or sub-letting might naturally give rise to the situation with
multiple lease lengths we consider in this paper.
In this paper, we initiate a systematic study of Leasing problems, and give
algorithms for several classic infrastructure design problems in the presence of
ﬁnite-duration leases. To illustrate our general model, we will use the Steiner
Tree Leasing problem as our running example.
We are given a graph G = (V, E) with a root r. For each day t, we are
given a set of terminals Dt and a set K of permissible lease lengths, where
the cost of leasing any edge e for length ℓ∈K is c(ℓ): we ensure that for
any lengths ℓ1 < ℓ2 in K, c(ℓ2) ≤c(ℓ1)× ℓ2
ℓ1 . Note that an edge leased on
day t for duration ℓcan be used on any of the days t, t + 1, . . . , t + ℓ−1,
and is said to be active on all these days. Deﬁne Xt(ℓ) ⊆E to be the set
of edges leased for duration ℓon day t, and Ft = ∪ℓ∈K ∪j∈[t−ℓ+1,t] Xj(ℓ)
to be the set of active edges on day t. A solution (given by edge sets
Xt(ℓ) for all t and ℓ) is feasible if on each day t, the induced active edge
sets Ft connect the demand set Dt to the root r. The goal is to ﬁnd a
feasible solution of minimum cost 
t,ℓ[c(ℓ) × |Xt(ℓ)|].
One can follow this general idea and deﬁne other infrastructure design problems:
in Facility Location Leasing, we are given demand sets Dt for each day, and
may want to lease diﬀerent facilities for diﬀerent periods of time, with the goal
of minimizing the resulting cumulative facility opening costs plus the connection
costs for the clients on their respective days. (In this case, one may even imagine
a “non-uniform” scenario where the diﬀerent facilities have diﬀerent lease cost
functions.) And an even more general problem is that of Set Cover Leasing,
where we are given sets Dt ⊆U of elements to cover on the tth day, and want to
lease sets such that the active sets at time t form a feasible cover of the set Dt.
While such problems of ﬁnite-period leases are related to the substantial body
of work on perishable commodities [29,13] in inventory theory, we are not aware
1 More formally, we assume leasing for length ℓcosts no more than two leases of length
ℓ/2. This sub-additive cost structure also allows amortization of one-time costs.

426
B.M. Anthony and A. Gupta
of work that directly addresses the questions under consideration in this paper.
Loosely speaking, given supply of a perishable good—e.g., cartons of milk with a
lifetime of ℓdays—and demands over time, research on perishable commodities
has considered questions pertaining to inventory positions (in deterministic vs
stochastic settings, with several classes of customers, etc.), and to pricing such
perishable goods. At a high level, our leasing problems can be viewed as solving
multiple perishable goods problems to solve a global network design problem.
1.1
Our Results and Techniques
The main result of this paper is the following, showing a close connection between
leasing problems as described above, and stochastic optimization problems.
Theorem 1 (General Leasing Theorem). The oﬄine leasing version of a
subadditive combinatorial optimization problem Π with |K| = k lease lengths can
be reduced to the stochastic optimization version of Π in the model of k-stage
stochastic optimization with recourse.
We feel this theorem is somewhat surprising: even though the leasing version
of the problem Π can be completely deterministic with a given input and no
stochastic component, this theorem shows that an algorithm to solve the (multi-
stage) stochastic version of the problem suﬃces to solve the (non-stochastic)
leasing problem. The proof of this theorem turns out to be fairly clean, and
appears in Section 4.1. Given this main theorem, we can use recently-developed
algorithms for multistage stochastic combinatorial optimization [34,37] to infer:
Corollary 1 (Optimal Algorithms for Leasing). There exist O(1)-approx-
imation algorithms for the Facility Location Leasing and Vertex Cover Leasing
problems, and an O(log n)-approximation for the Set Cover Leasing problem.
All these results are asymptotically optimal (up to constants). For the Steiner
Tree Leasing problem we were using as our running example, we get the following
result by combining Theorem 1 with known results [17,19].
Theorem 2 (Steiner Tree Leasing). There is an O(min{k, log n})-approxi-
mation algorithm for oﬄine Steiner Tree Leasing with |K| = k lease lengths.
It seems improving the approximation to o(k) requires techniques that also im-
prove results for the Stochastic Steiner Problem, which remains an open question.
New Algorithms for Network Problems: We go on to study other network
leasing problems that generalize the Steiner Tree Leasing problem. In these prob-
lems, instead of just connecting up the terminals, we are now required to allocate
“suﬃcient” bandwidth on the connecting edges as well. However, the cost of al-
locating bandwidth is itself a concave function g(b) of the amount of bandwidth
b allocated on the edge: these are commonly known as buy-at-bulk problems. In
the leasing framework, this translates into problems where the cost of leasing b
units of bandwidth for a period of length ℓis c(ℓ) × g(b).

Infrastructure Leasing Problems
427
Theorem 3 (Buy-at-Bulk Theorems). There is an O(k) approximation for
the k-stage Stochastic versions of the single-sink Rent-or-Buy, and the single-
sink Buy-at-Bulk problems. Moreover, the Stochastic Buy-at-Bulk problem with
multiple sinks has an O(k log n) approximation algorithm.
By Theorem 1, we get the same approximation ratios for the corresponding
network leasing versions of these problems as well.
Related Work. There has been a tremendous amount of work on network
design where the the cost of bandwidth obeys natural economies of scale (often
called “buy-at-bulk” network design). It is beyond the scope of this paper to
survey this body of work, so we point the reader to [25,26,4,32,2,14,12,38,1,10]
and the many references therein. This line of work is related to our work both
in spirit, as well as in some of the technical methodology. In this paper, we
also show how we can extend some of the current algorithms for these “buy-
at-bulk” problems to the case when the bandwidth is leased and not bought
permanently.
As mentioned above, leasing for ﬁnite periods is related to a large body of
work on perishable commodities [29,13] in inventory theory; however, to the
best of our knowledge, such problems have not been directly considered in the
literature.
The Steiner Tree Leasing problem was ﬁrst explored in a paper on the “parking
permit problem” [27]. The paper noted that dynamic programming could be used
to solve the Steiner Tree Leasing problem when the graph was a single edge (or to
obtain an approximation scheme if the numbers are large), and gave an O(log k)
competitive algorithm in the online case where the terminal set Dt is revealed
only on day t. These results can be extended naturally to general graphs using
standard tree-approximation techniques [5,11] by losing an extra O(log n) factor.
However, it does not seem clear how to improve their techniques directly in the
oﬄine case to avoid this loss of O(log n) and obtain an approximation dependent
only on k, or to extend them to the other problems we consider here.
In this paper, we show a concrete connection of network leasing to multistage
stochastic optimization problems. While the history of stochastic optimization
begins in the 1950s, this work is directly related to recent work on approxima-
tion algorithms for stochastic combinatorial problems [9,20,31,16,19,33,8,7]. We
draw most directly from the results of [19,17,34] on the multistage stochastic op-
timization problems, and on the results in [16,17] to convert algorithms for the
non-stochastic versions of problems to their multistage stochastic counterparts.
A standard tool in algorithms design today is the tree approximation tech-
nique of [5,11], as well as the general techniques for solving covering problems
from, e.g., [30,35,36,23]. These techniques will allow us to get some simple ap-
proximation bounds; one of the goals of this paper is to develop algorithms that
beat these na¨ıve bounds by making use of the combinatorics of the problems,
and to explore connections to problems in multistage stochastic optimization.
As an aside, let us note that a problem called the “Network Leasing” problem
has been previously studied in the literature [3]; since that problem has come

428
B.M. Anthony and A. Gupta
to be better known as the “Rent-or-Buy” problem, we have taken the liberty of
claiming the term “leasing” to refer to an orthogonal concept in this paper.
2
Models and Notation
Consider a general subadditive optimization problem Π with k lease lengths.
Formally, we are given a set U of potential clients or demands, such that on
each day t ∈{1, 2, . . . , T}, some subset Dt ⊆U of these clients actually appear
and demand service. (We will soon discuss how these sets Dt are given to us.)
We also have a set of elements X that we can use to build solutions: for each
subset of clients D ⊆U, we are given some set of solutions Sols(D) ⊆2X to the
client set D. On day t, we would like to own a set of elements Ft ∈Sols(Dt).
If each element could only be leased for a single day at a time, then this
would just require us to solve T instances of the problem Π; on the other hand,
if elements could only be leased indeﬁnitely (i.e., “bought”), we would just solve
the problem on ∪tDt. The “leasing” aspect of the problem is reﬂected in the
fact that each of these elements e ∈X can be leased for several periods: i.e.,
on any day t, given any duration ℓ∈K, one can obtain a lease of length ℓon
element e ∈X for cost ce(ℓ) and use it on days t, t + 1, . . . , t + ℓ−1. Formally,
let Xt(ℓ) be the elements for which leases of length ℓwere obtained on day t,
and Ft = ∪ℓ∈K ∪t
t′=t−ℓ+1 Xt(ℓ), then a feasible leasing strategy is a sequence of
sets Xt(ℓ) which results in Ft ∈Sols(Dt) for each day t.
Deﬁnition 1 (Uniform vs. Non-Uniform). A leasing problem is called uni-
form if the cost functions ce(·) for all elements e ∈X are identical (here we will
drop the subscript and refer to it as c(·)), and is called non-uniform otherwise.
As may be expected, we will be able to obtain better results for uniform problems
in some cases. One immediate advantage of uniform network design problems
will be the applicability of tree-approximation techniques (see Lemma 3); see
also Section 4.1 for other advantages of uniformity.
Stochastic Optimization. The relevant stochastic model is k-stage stochastic
optimization with recourse: the demand set D is revealed on day-k drawn from
some known distribution π, but on each of days 1, 2, . . . , k −1, we are given
additional information about the set D. (One can view this process as having a
joint distribution over “signals” s1, s2, . . . , sk−1, sk received on the various days,
with the actual demand set some known function of this signals.) One can see,
e.g., [34,17] for more details about the model. The costs of elements change over
time (usually getting more expensive over time): the uniform inﬂation model
assumes the cost costt(e) of element e ∈X on day-t (or stage-t) to be σi ×
costt−1(e) (and hence cost1(e) 
1<j≤i σj). Note that the σi’s are uniform, and
independent of the element e. In the more powerful non-uniform model, the
costs of diﬀerent elements can change diﬀerently as time progresses.
We use the Boosted Sampling framework to develop new algorithms for some
network design problems: these will require us to use terminology about cost
shares, which can be found in Appendix A.

Infrastructure Leasing Problems
429
ℓ= 1
ℓ= 2
ℓ= 4
ℓ= 1
ℓ= 2
ℓ= 4
Fig. 1. A solution, and the corresponding nested version (right), as in Lemma 2
3
Observations and Reductions
Before we give the main results of this paper, we give some observations which
will be helpful in the rest of the paper. We investigate how solutions can be
assumed to have a simple structure, what results tree-approximations can give
for Steiner Tree Leasing (giving us a baseline to compare to), and what tree-
approximation techniques can give for more complex network leasing problems.
Structure of Solutions. The following two lemmas allow us to impose a simple
structure on the instances we solve and solutions we seek. They are fairly stan-
dard (e.g., [27, Thms 2.1 & 2.2]) and are given for completeness. Recall that the
set of permissible lease lengths is K = {ℓ1, ℓ2, . . . , ℓk} with ℓ1 < ℓ2 < . . . < ℓk.
Lemma 1. Given any instance I of a leasing problem, we can convert it into
an instance I′ in which the lengths of leases exactly divide each other (i.e., ℓi|ℓj
for i < j), and where the costs satisfy c(ℓj) < c(ℓi) × (ℓj/ℓi). Moreover, there is
an optimal solution to I′ which has cost at most 2 times the optimal cost for I.
The above lemma can be proved, e.g., by rounding all the lease lengths down to
the closest powers of 2, and by discarding leases that do not satisfy the subaddi-
tivity property. The following lemma shows that we can focus our attention only
on “nested” solutions; i.e., solutions where we never have a short-term lease still
active when a longer-term lease begins or ends.
Lemma 2 (Nested Solutions). Given an instance I of a leasing problem,
there is a solution which has cost at most 2 times the optimum, where a lease of
length ℓis obtained only for intervals of the form [t, t + ℓ) with t a multiple of ℓ.
See Fig. 1 for an example of a non-nested solution on the left, and a nested
solution whose cost is at most twice the cost of the former.
Reduction to Trees/Single-Edges. Given a graph G = (V, E), a theorem of
Fakcharoenphol et al. [11] (see also [5]) says that there is a distribution D over
dominating trees T (i.e., dG(u, v) ≤dT (u, v) for any T in the support of D) such
that the expected stretch ET ←D[dT (u,v)]
dG(u,v)
≤O(log n). The following use of this
result is fairly standard by now (see [2]).
Lemma 3 (Reduction to Trees/Edges). Given an instance of Steiner Tree
Leasing which is uniform (where the cost functions ce(·) are the same for all
edges), an α-approximation for the single-edge case gives an α approximation
for trees, and an O(α log n) approximation for the general graph case.

430
B.M. Anthony and A. Gupta
The proof uses the fact that the reduction to a tree instance loses an O(log n);
once on a tree, the paths to be chosen are unique, and hence it suﬃces to run the
single-edge algorithm on each edge to determine when to lease it. (The simple
details are deferred to the ﬁnal version of the paper.) Since we can solve the
leasing problem on a single edge exactly, we get an O(log n)-approximation for
the Steiner Tree Leasing problem.
General (Uniform) Leasing Strategies and CIPs. Consider a much more
general network design problem where at each time step t we are given a traﬃc
matrix Dt, and want to allocate enough bandwidth to route Dt. We are now
given a set L = {Lj = (Ij, bj, pj)}j of possible leases, where each lease Lj in L
is speciﬁed by a time interval Ij during which this lease is active, an amount bj
of bandwidth and a price pj for it. Moreover, for any lease Lj, we may have an
upper bound uj on the number of copies of this lease we can buy per edge. This
is a much more general model than the one we have been looking at, since we
allow “one-time-only” oﬀers (a special deal valid only for some days at a special
price, limit one only), etc: this captures Buy-at-Bulk Leasing, and much more.
However, as long as the problem is uniform (i.e., each edge e has the same
set L of potential leases), we can use a reduction akin to Lemma 3 to ran-
domly reduce the problem to a tree and hence to a single edge, where it can
be solved using general theorems on CIPs, covering integer problems techniques
(e.g., see [30,35,6,36,23]). Applying these techniques to our problems give us ap-
proximation ratios that typically depend on log ℓmax, and log bmax, where bmax is
the maximum bandwidth requirement. (See the full version for precise details.)
In this paper, we attempt to give algorithms that are better—i.e., independent
of ℓmax; it is easy to see that log ℓmax ≥k, and we think of log ℓmax ≫k.
4
Algorithms for Leasing Problems
In this section, we will prove the main result: that Leasing Problems can be
cast as Stochastic Optimization problems. This will allow us to get approxi-
mation algorithms for a variety of leasing problems from the corresponding al-
gorithms for stochastic optimization. While we use many stochastic algorithms
already in the literature, we will give new algorithms for some problems like
Stochastic Rent-or-Buy and Stochastic Buy-at-Bulk, and hence for their leasing
versions.
4.1
Reduction to Multistage Stochastic Optimization
Let us assume, without loss of generality, that ℓ1 = 1, and denote the maximum
lease length by ℓmax. By Lemma 2 we can assume that our solutions are nested.
Theorem 4. [Reduction to k-stage Stochastic Optimization] Any (non-uniform)
oﬄine problem Π in the above framework with |K| = k lease lengths can be re-
duced to the standard k-stage stochastic optimization version of Π.

Infrastructure Leasing Problems
431
π1 π2
πt
π16
ℓk
ℓk−1
Nk−1 =
ℓk
ℓk−1
Nk−2 = ℓk−1
ℓk−2
π1 π2
πt
π16
Fig. 2. A nested leasing instance (k = 4), and the resulting stochastic tree T
Proof. As in the proof of Theorem 2, let us consider tiling time by intervals of
length ℓk, each of which are divided into Nk−1 =
ℓk
ℓk−1 consecutive intervals of
length ℓk−1, each of which are further subdivided into Nk−2 = ℓk−1
ℓk−2 intervals of
length ℓk−2, and so on. Note that this gives a diﬀerent representation of time: we
can describe time t = k
p=1 jp ℓp as a k-tuple of the form (jk, jk−1, . . . , j1)—and
we will denote this tuple by ¯τ(t). (Note that jp is simply ⌊t/ℓp⌋(mod ℓp+1),
where we assume ℓk+1 = ∞). Corresponding to this notation, we will refer to
the set Xt(ℓi) also as X(jk,jk−1,...,j1)(ℓi), where t, ℓi and the jk’s are as above.
Recall that we are looking for nested solutions, and hence each lease of length
ℓi will be obtained at the beginning of some interval of length ℓi; hence Xt(ℓi) = ∅
for t ̸≡0 (mod ℓi). Moreover, since the longest interval is of length ℓk, all permits
will have to be purchased afresh at the end of each length ℓk interval, and hence
we can focus on the time interval from 0 to T = ℓk−1. Using these facts, consider
a leasing solution that for each t ∈[T ] and p ∈[k], buys leases of length ℓp on
the elements in Xt(ℓp) at time t. The (expected) cost of this solution is2
E
⎡
⎣

e∈X0(ℓk)
ce(ℓk) +

t:ℓk−1|t

e∈Xt(ℓk−1)
ce(ℓk−1) +

t:ℓk−2|t

e∈Xt(ℓk−2)
ce(ℓk−2) + . . .
⎤
⎦.
(1)
We now deﬁne an instance of the k-stage stochastic optimization problem
Stock(Π) with the same optimal value as (1), and hence an α-approximation to
the stochastic problem gives an α-approximation to our network leasing problem.
The Stochastic Instance. Consider the tree T in Fig. 2 where the root has
Nk−1 =
ℓk
ℓk−1 children, each node at depth 1 has Nk−2 = ℓk−1
ℓk−2 children, and so on.
This gives rise to ℓk leaves associated with the distributions π1, π2, . . . , πℓt from
left to right. The k-stage stochastic problem now involves k stages of decision-
making. In the ﬁrst stage, a particle is placed at the root, and we buy a set
Y1 ⊆X, where element e ∈X costs ce(ℓk). After this, the particle moves to
one of the children of the root at random; after we learn the identity of this
vertex of T, we can buy a “stage-2” set Y2 ⊆X, but the cost of e now becomes
2 We allow randomized leasing policies, and so expectation is over the coin tosses of
our algorithm, as well as over randomness in the choice of the sets St in case we are
working in the stochastic oﬄine model where St is drawn from the distribution πt.

432
B.M. Anthony and A. Gupta
ce(ℓk−1) × Nk−1. In this way, after t steps, the particle reaches some node at
depth t, whence we buy some “stage-t+1” set Yt+1 ⊆X with the costs ce(ℓk−t)×

1≤p≤t Nk−p = ce(ℓk−t) ×
ℓk
ℓk−t . Finally, when the particle reaches some leaf vk
(at depth k −1, say it is the tth leaf), the algorithm ﬁnally gets a random set of
clients St ∈R πt, and must output a set Yk such that Y1 ∪Y2 ∪. . .∪Yk ∈Sols(St);
as above, the costs are now ce(ℓ1) × 
1≤p≤k Nk−p = ce(ℓ1) × ℓk
ℓ1 .
The Correspondence. Note that a solution to this process associates a (po-
tentially) random set Y (v) with each vertex of tree T; the expected cost is
E
⎡
⎣

e∈Y (root)
ce(ℓk) +
k−1

p=1

v at depth p
Pr[reach v]

e∈Y (v)
ce(ℓk−p) ×
ℓk
ℓk−p
⎤
⎦
(2)
Finally, we place the nodes at level p of T in correspondence with integers t
such that ℓk−p|t, associate Y (v) with Xt(ℓk−p), and observe the probability of
reaching any ﬁxed node at level p is ℓk−p
ℓk
to get that (2) and (1) are identical.
Costs and Inﬂations. The instances of Stock(Π) created by the reduction
above have the property that when we go from stage p −1 to stage p of the
stochastic problem, the cost of each element e increases by an inﬂation factor of
σe,p .= ce(ℓk−p+1) × Nk−p+1
ce(ℓk−p+2)
,
(3)
which by our assumptions is at least 1. If the leasing problem was uniform (the
functions ce(·) were the same for all e ∈X), this inﬂation parameter depends
only on the stage p but not on the element e (the uniform inﬂation case). But, if
the leasing problem was non-uniform, we get a non-uniform inﬂation stochastic
problem. This distinction will be useful, since depending on the problem Π,
diﬀerent approximation guarantees exist for uniform and non-uniform versions.
4.2
Leasing Algorithms from Existing Stochastic Algorithms
There has been much recent work on designing algorithms for multistage stochas-
tic optimization with provable guarantees; see [34,17,19]; some are in the uniform
inﬂation model, whereas others are more general. Using Theorem 4, we get:
Problem
Inﬂation
Approximation Ratio Stochastic
type
for Leasing problem
Citation
Steiner Tree
uniform
8k
2k [17,19]
Facility Location non-uniform
9.4
2.36 [37]
Vertex Cover
non-uniform
8
2 [28,37]
Set Cover
non-uniform
4 ln n
ln n [37]
We note that as presented, the algorithms for the k-stage stochastic problems
specify which elements to buy in an “online-like” fashion; given the observations

Infrastructure Leasing Problems
433
of what has happened in the past, the stochastic algorithms prescribe the el-
ements to buy at the current time instant. In particular, they do not give an
explicit representation of the sets Y (v) of elements to buy for each node v of the
distribution tree T. However, the above algorithms can easily be altered to give
all these sets; the details are deferred to the ﬁnal version of the paper.
5
New Stochastic/Leasing Approximations
In this paper, we give new results for k-stage stochastic optimization (and hence
for Network Leasing) on a group of network design problems, all of which lie
under the umbrella of “buy-at-bulk”-type problems. In these problems, the de-
mand Dt for day t is not just a set of clients that have to be connected (as in
Steiner Tree), but instead is a traﬃc matrix specifying how much traﬃc ﬂows
between various pairs of nodes in the network. In addition to the lease-cost func-
tion c : K →R+ given earlier, we are also given a “bandwidth-cost” function
g : R+ →R+. The cost of leasing b bandwidth on an edge for ℓlength of time
is now Cost(b, ℓ) = g(b) × c(ℓ). (We consider these problems only in the uniform
model, and hence both the functions c(·) and g(·) are the same for each edge.)
We will give the following results for some buy-at-bulk type problems, using
the Boosted Sampling approach and deﬁning “strict” cost-shares to prove these
results; a quick overview is provided in Appendix A.
Problem
Inﬂation Approximation Ratio Citation
Buy-at-Bulk
uniform
O(k log n)
Theorem 5
Single-Sink Rent-or-Buy uniform
O(k)
Theorem 6
Single-Sink Buy at Bulk uniform
O(k)
Theorem 7
5.1
Multiple-Sink Buy-at-Bulk
There are many ways to specify the Buy-at-Bulk problem which are all equivalent
to within a factor of 2 (see, e.g., [38]), so let us ﬁx one. We are given a demand
matrix D ∈Rn×n where Dij gives the traﬃc from vi to vj. We have a monotone
subadditive cost function g(·), where the cost of bandwidth b is g(b). By well-
known properties of subadditive functions, we can ﬁnd a concave cost function
h(·) such that g(b) ≤h(b) ≤2g(b) for all b ̸= 0. We assume that the cost of
bandwidth allocation is h(b) for all non-zero values of b; this only changes the
problem by a factor of 2.
The best-known algorithm for the Buy-at-Bulk problem is by Awerbuch and
Azar [2]. We approximate the graph by a random tree (as in Lemma 3), and
given the Buy-at-Bulk problem on the tree, we can solve it on an edge-by-edge
basis. We now show how to get an algorithm for the stochastic version.
Theorem 5. The k-stage stochastic version of the Buy-at-Bulk problem on the
tree has an O(k) approximation, and hence Buy-at-Bulk on general graphs has
an O(k log n) approximation.

434
B.M. Anthony and A. Gupta
Proof. Let us give an algorithm for a single edge in the tree that separates V into
A and V \A: we can calculate the traﬃc crossing this edge e as De = 
ij∈∂A Dij.
For this edge, we allocate capacity De and divide the cost h(De) equally among
each of the De units of demand. Clearly the cost shares are cross-monotone:
if more demand passes through the edge, the cost only decreases because h is
concave. Moreover, the algorithm is a 1-approximation with respect to these
cost-shares, since we share the exact cost of the algorithm amongst the players.
Moreover, we can check that these cost shares are 1-c-strict (as deﬁned in (5)):
indeed, if we divided the traﬃc De into two parts S and T , and allocated S units
of bandwidth ﬁrst, then the cost shares ξ(X/A(S), T, T ) = h(S+T )−h(S) would
be at most the cost-shares ξ(X, S ∪T, T ) = h(S + T ) ×
T
S+T ascribed to T when
both S and T were in the fray; this follows from the concavity of h.
Given that we have 1-c-strict and cross-monotone cost shares ξ and a 1-
approximation algorithm A with respect to ξ, we can apply Theorem 8 to in-
fer a k-approximation (with respect to the cost function h), and hence a 2k-
approximation with respect to the original cost function g. Finally, since we
moved to a random tree, we lose another O(log n) in translating the solution
back to the original graph G. This concludes the proof.
5.2
Single-Sink Buy-at-Bulk Problems
In the Single-Sink Rent-or-Buy problem (a special case of the Buy-at-Bulk prob-
lem), we are given a graph G = (V, E) with a distinguished root vertex r. Each
vertex j wants to send dj amount of traﬃc to r. The bandwidth cost function
is g(b) = min{b, M} for some parameter M. We show the following result:
Theorem 6. The Single-Sink Rent-or-Buy problem has an O(1)-approximation
algorithm with respect to 1-c-strict cost sharing functions; moreover, these cost-
shares are cross-monotone.
Combined with Theorem 8, this gives an O(k)-approximation for stochastic
Single-Sink Rent-or-Buy, and hence an O(k)-approximation for the Single-Sink
Rent-or-Buy Leasing problem, where buying b bandwidth for ℓcosts g(b) · c(ℓ).
Proof. The algorithm A is the SimpleCFL algorithm from [15]. This algorithm
starts oﬀwith F = {r}, and add each vertex j to F independently with prob-
ability dj/M. It then builds an approximate Steiner tree on F using the MST
heuristic, and allocates unlimited capacity on its edges (hence paying M on each
such edge). It then sends dj units of ﬂow from j to its closest vertex in F (which
may be j itself, in case j ∈F); for this it pays cost 1 per unit of ﬂow.
Deﬁne the cost-share for node j as ξRoB(v) = E[M ξMST (v)] + E[dj l(v, F)].
(Here ξMST is a cross-monotonic cost-sharing function ξMST for the minimum
spanning tree problem—e.g., given in [22,21], and l(v, F) is the distance from
v to the nearest vertex in F.) It is known that ξRoB is cross-monotone, and
moreover that A is a 4-approximation for Single-Sink Rent-or-Buy with respect
to these cost-shares ξRoB [24,18].

Infrastructure Leasing Problems
435
We claim ξRoB is 1-c-strict with respect to A. By the deﬁnition of 1-c-strict-
ness, we want to show that given S, T ⊆V , ξ(G, S ∪T, T ) ≥E[ξ(G/A(S), T, T )];
here the expectation on the right hand side is over the coins ﬂipped by A(S).3 Of
course, to compute both the cost shares ξ’s, we also have to take expectations.
Since the expressions on the left and the right both involve ﬂipping an indepen-
dent coin for each of the nodes in S ∪T , let us couple the two random processes
in the natural way by making the same set of coin tosses in both expressions.
Consider a particular choice of coin ﬂips for S ∪T , which chooses FS ⊆S and
FT ⊆T ; set F = {r} ∪FS ∪FT . The cost-shares on the right involve paying for
the MST on FT (in the graph G/A(S)), and paying for connections from each
j ∈T \ FT to F. Charging for the latter is easy, since we pay for the distance
from j to F in the left expression too. To pay for the former, we look at the
primal-dual process that generates ξMST . In the run on G/A(S) with terminals
FT , a node j in FT obtains cost-shares as long as its moat does not contain the
root of the graph G/A(S). Since all nodes in FS are contracted to the root in
G/A(S), in the process for the left hand side the moat of j must not have hit any
moat of FS ∪{r}, and hence must get at least as much cost-share. This implies
that for any particular set of coin ﬂips, the cost-share on the right is bounded
above by the cost-share on the left, and hence this holds in expectation as well.
This can be extended to give the following theorem:
Theorem 7. The Single-Sink Buy-at-Bulk problem has an O(1)-approximation
algorithm with respect to 1-c-strict cross-monotone cost sharing functions.
The proof of Theorem 7 extends the proof of Theorem 6. While we defer it until
the ﬁnal version of the paper, we sketch it here: the algorithm is essentially the
SimpleSSBB algorithm from [15], which uses the above SimpleCFL algorithm re-
peatedly to collect the traﬃc, which is then aggregated at some randomly chosen
locations. Each time the aggregation is done using cables of larger capacity, and
results in fewer and fewer locations, until ﬁnally all the traﬃc is at one location,
whence it is sent to the root. Since we repeatedly use the algorithm SimpleCFL,
the cost-share of a node u is just the expected cost-share of u accumulated over
the various runs of SimpleCFL (where its cost-share is zero when there is no more
traﬃc at u). The proof of strictness again proceeds by coupling the run on S ∪T
to the run where we build a solution on S, and then augment it to T .
6
Conclusions
In this paper, we deﬁned several natural “Leasing” problems, in which an op-
timization problem is solved repeatedly over time (each time with a diﬀerent
set of clients), and the elements chosen to serve the clients can be leased for
extended periods of time to take advantage of temporal trends in the sets of
clients. The costs of these leases satisfy standard economies of scale, and hence
longer leases cost less per unit of time. We study leasing problems in an oﬄine
3 The added expectation sign over the deﬁnition (5) is required since A is randomized.

436
B.M. Anthony and A. Gupta
setting, and give approximation algorithms for them via a connection with mul-
tistage stochastic optimization. We also give new algorithms for some network
design problems in the multistage stochastic framework.
Many future directions of research suggest themselves: an important one is
to extend the results to online or stochastic versions of leasing problems. In
this paper, the demands Dt were given up front, but one can also consider cases
where the demands Dt appear only on day t, chosen adversarially (i.e., the online
model) or from some probability distribution (i.e., the stochastic model). While
some of these problems can be solved by solving associated LPs and rounding
them online (as in [27]), obtaining general results for these online problems
is a direction we are exploring in ongoing work. There seem to be interesting
questions involved in pricing these leases as well. It would be good to extend the
“buy-at-bulk” results to cases where the cost function is not separable g(b)f(ℓ).
Finally, getting o(k)-approximations for the Steiner Tree Leasing problem is an
intriguing question—it seems that the ideas for such an improvement would be
useful for the multistage stochastic versions as well.
References
1. Andrews, M., Zhang, L.: Wavelength assignment in optical networks with ﬁxed
ﬁber capacity. In: 31st ICALP. Volume 3142 of LNCS. (2004) 134–145
2. Awerbuch, B., Azar, Y.:
Buy-at-bulk network design.
In: 38th FOCS. (1997)
542–547
3. Awerbuch, B., Azar, Y., Bartal, Y.: On-line generalized Steiner problem. Theoret.
Comput. Sci. 324(2-3) (2004) 313–324
4. Balakrishnan, A., Magnanti, T.L., Mirchandani, P.: Network design. In Dell’Amico,
M., Maﬃoli, F., Martello, S., eds.: Annotated bibliographies in combinatorial op-
timization. John Wiley & Sons Ltd., Chichester (1997) 311–334
5. Bartal, Y.: Probabilistic approximations of metric spaces and its algorithmic ap-
plications. In: 37th FOCS. (1996) 184–193
6. Carr, R., Fleischer, L., Leung, V., Phillips, C.: Strengthening integrality gaps for
capacitated network design and covering problems. In: 11th SODA. (2000) 106–115
7. Charikar, M., Chekuri, C., P´al, M.: Sampling bounds for stochastic optimization.
In: 9th APPROX. Volume 3624 of LNCS. Springer, Berlin (2005) 257–269
8. Dhamdhere, K., Ravi, R., Singh, M.: On two-stage stochastic minimum spanning
trees. In: IPCO. Volume 3509 of LNCS. Springer, Berlin (2005) 321–334
9. Dye, S., Stougie, L., Tomasgard, A.: The stochastic single resource service-provision
problem. Naval Research Logistics 50(8) (2003) 869–887
10. Eisenbrand, F., Grandoni, F., Oriolo, G., Skutella, M.: New approaches for virtual
private network design. In: 32nd ICALP. Volume 3580 of LNCS. (2005) 1151–1162
11. Fakcharoenphol, J., Rao, S., Talwar, K.: A tight bound on approximating arbitrary
metrics by tree metrics. J. Comput. System Sci. 69(3) (2004) 485–497
12. Garg, N., Khandekar, R., Konjevod, G., Ravi, R., Salman, F.S., Sinha, A.: On
the integrality gap of a natural formulation of the single-sink buy-at-bulk network
design formulation. In: 8th IPCO. Volume 2081 of LNCS. (2001) 170–184
13. Goyal, S., Giri, B.C.: Recent trends in modeling of deteriorating inventory. Euro-
pean Journal of Operational Research 134(1) (2001) 1–16

Infrastructure Leasing Problems
437
14. Guha, S., Meyerson, A., Munagala, K.: Hierarchical placement and network design
problems. In: 41th FOCS. (2000) 603–612
15. Gupta, A., Kumar, A., Roughgarden, T.: Simpler and better approximation algo-
rithms for network design. In: 35th STOC. (2003) 365–372
16. Gupta, A., P´al, M., Ravi, R., Sinha, A.: Boosted sampling: Approximation algo-
rithms for stochastic optimization problems. In: 36th STOC. (2004) 417–426
17. Gupta, A., P´al, M., Ravi, R., Sinha, A.: What about Wednesday? approximation
algorithms for multistage stochastic optimization. In: 8th APPROX. (2005) 86–98
18. Gupta, A., Srinivasan, A., Tardos, ´E.: Cost-sharing mechanisms for network design.
In: 7th APPROX. Volume 3122 of LNCS. (2004) 139–150
19. Hayrapetyan, A., Swamy, C., Tardos, E.: Network design for information networks.
In: 16th SODA. (2005) 933–942
20. Immorlica, N., Karger, D., Minkoﬀ, M., Mirrokni, V.: On the costs and beneﬁts of
procrastination: Approximation algorithms for stochastic combinatorial optimiza-
tion problems. In: 15th SODA. (2004) 684–693
21. Jain, K., Vazirani, V.V.:
Equitable cost allocations via primal-dual-type algo-
rithms. In: 34th STOC, ACM Press (2002) 313–321
22. Kent, K.J., Skorin-Kapov, D.: Population monotonic cost allocations on MSTs. In:
Proceedings of the 6th International Conference on Operational Research (Rovinj,
1996), Croatian Oper. Res. Soc., Zagreb (1996) 43–48
23. Kolliopoulos, S.G., Young, N.E.: Tight approximation results for general covering
integer programs. In: 42nd FOCS. (2001) 522–528
24. Leonardi, S., Sch¨afer, G.: Cross-monotonic cost sharing methods for connected
facility location games. Theoret. Comput. Sci. 326(1-3) (2004) 431–442
25. Magnanti, T.L., Wong, R.T.: Network design and transportation planning: Models
and algorithms. Transportation Science 18 (1984) 1–55
26. Magnanti, T.L., Mirchandani, P., Vachani, R.:
Modeling and solving the two-
facility capacitated network loading problem. Oper. Res. 43(1) (1995) 142–157
27. Meyerson, A.: The parking permit problem. In: 46th FOCS. (2005) 274–284
28. Munagala, K. personal communication
29. Nahmias, S.: Perishable inventory theory: A review. Operations Research 30(4)
(1982) 680–708
30. Raghavan, P., Thompson, C.D.: Randomized rounding: a technique for provably
good algorithms and algorithmic proofs. Combinatorica 7(4) (1987) 365–374
31. Ravi, R., Sinha, A.: Hedging uncertainty: Approximation algorithms for stochastic
optimization problems. In: 10th IPCO. (2004) 101–115
32. Salman, F.S., Cheriyan, J., Ravi, R., Subramanian, S.: Approximating the single-
sink link-installation problem in network design.
SIAM J. Optimization 11(3)
(2000) 595–610
33. Shmoys, D., Swamy, C.: Stochastic optimization is (almost) as easy as deterministic
optimization. In: 45th FOCS. (2004) 228–237
34. Shmoys, D., Swamy, C.: Sampling-based approximation algorithms for multi-stage
stochastic optimization. In: 46th FOCS. (2005) 357–366
35. Srinivasan, A.: Improved approximation guarantees for packing and covering inte-
ger programs. SIAM J. Comput. 29(2) (1999) 648–670
36. Srinivasan, A.: New approaches to covering and packing problems. In: 12th SODA.
(2001) 567–576
37. Srinivasan, A.: Approximation algorithms for stochastic and risk-averse optimiza-
tion. In: 18th SODA. (2007) 1305–1313
38. Talwar, K.: Single-sink buy-at-bulk LP has constant integrality gap. In: 9th IPCO.
Volume 2337 of LNCS. (2002) 475–486

438
B.M. Anthony and A. Gupta
A
Cost Shares and Stochastic Algorithms
We will draw on some techniques developed in recent work on converting ap-
proximation algorithms for standard (non-stochastic) versions of optimization
problems into those for the stochastic versions of the problems [16,17]. In par-
ticular, we use the following theorem.
Theorem 8 ([17]). Given a problem Π, if A is an α-approximation algorithm
w.r.t. a 1-c-strict cost-sharing function ξ, and if ξ is cross-monotone, then there
is an αk-approximation algorithm for the k-stage stochastic version of Π.
Let us brieﬂy discuss the basics of the cost-sharing concepts we will use in
this paper; we refer the reader to [17] for a detailed discussion of the concepts.
Loosely, a cost-sharing function ξ divides the cost of a solution among the client
set S. We use the notation ξ(G, S, j) to denote the share of the client j when
the input is the graph G and the set of clients is S. (By convention, we will
assume that ξ(G, S, j) > 0
=⇒j ∈S.) The function ξ is cross-monotone if
for every pair of client sets S ⊆T and a client j such that j ∈S, we have
ξ(G, T, j) ≤ξ(G, S, j). (I.e., if more clients join the system, the share of any
individual client does not increase.)
Competitiveness. We will try to relate algorithms A to cost-sharing functions
ξ, and hence ξ will conceptually behave like a “dual”. Hence a crucial property
is that ξ give a lower bound on the cost of the optimal solution: A cost-sharing
function ξ is competitive if for every client set S, it holds that

j∈S ξ(G, S, j) ≤OPT(X, S).
(4)
We will focus only on competitive cost-sharing functions. (Henceforth, we will
use the notation ξ(G, S, S′) to denote 
j∈S′ ξ(G, S, j).)
Strictness. Let S, T ⊆V be sets of users. Suppose G is the original graph, and
G/A(G, S) is the graph after the client set S has already been served by running
the algorithm A on it. Then the cost-sharing function ξ is β-c-strict if
ξ(G/A(G, S), T, T ) ≤β × ξ(G, S ∪T, T ).
(5)
In other words, the total cost shares for the set T of users in the reduced instance
G/A(G, S) is at most β times the cost-shares for T if the users in S were present
as well. In this paper, we will deal only with the case when β = 1; i.e., cases
where the cost shares for T when it appears with S are at least as much as when
S is served earlier, and then T has to be served by itself.
Finally, we call A an α-approximation algorithm with respect to the cost-
sharing function ξ
c(A(G, S)) ≤α ξ(G, S, S).
(6)
Notethatchaining theinequalities(6)and(4)impliesthatAisanα-approximation
algorithm in the conventional sense as well.

Robust Combinatorial Optimization with
Exponential Scenarios
Uriel Feige1, Kamal Jain1, Mohammad Mahdian2, and Vahab Mirrokni1
1 Microsoft Research
{urifeige,kjain,mirrokni}@microsoft.com
2 Yahoo! Research
mahdian@yahoo-inc.com
Abstract. Following the well-studied two-stage optimization framework
for stochastic optimization [15,18], we study approximation algorithms
for robust two-stage optimization problems with an exponential num-
ber of scenarios. Prior to this work, Dhamdhere et al. [8] introduced
approximation algorithms for two-stage robust optimization problems
with explicitly given scenarios. In this paper, we assume the set of pos-
sible scenarios is given implicitly, for example by an upper bound on
the number of active clients. In two-stage robust optimization, we need
to pre-purchase some resources in the ﬁrst stage before the adversary’s
action. In the second stage, after the adversary chooses the clients that
need to be covered, we need to complement our solution by purchasing
additional resources at an inﬂated price. The goal is to minimize the cost
in the worst-case scenario. We give a general approach for solving such
problems using LP rounding. Our approach uncovers an interesting con-
nection between robust optimization and online competitive algorithms.
We use this approach, together with known online algorithms, to develop
approximation algorithms for several robust covering problems, such as
set cover, vertex cover, and edge cover. We also study a simple buy-
at-once algorithm that either covers all items in the ﬁrst stage or does
nothing in the ﬁrst stage and waits to build the complete solution in
the second stage. We show that this algorithm gives tight approximation
factors for unweighted variants of these covering problems, but performs
poorly for general weighted problems.
1
Introduction
In many combinatorial optimization problems, the objective is to minimize the
cost of building an installation to serve a number of clients. In classical opti-
mization problems, it is often assumed that the parameters of the system are
known in advance. However, in reality, it is almost always impossible or costly
to obtain accurate data about various parameters of the optimization problem
at the time of planning. For example, the cost of acquiring a resource or the
set of clients that need to be serviced might be unknown. The goal of the ﬁelds
of stochastic optimization and robust optimization is to provide algorithms for
minimizing the cost in presence of uncertainty.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 439–453, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

440
U. Feige et al.
In stochastic optimization [6,7], it is assumed that we have information about
the probability distribution governing the data. Given this information, the
goal is to plan ahead to minimize the expected cost. In particular, in two-stage
stochastic optimization, a solution is built in two stages: in the ﬁrst stage, we
need to decide which resources to purchase given only distributional information
about the instance. In the second stage, the exact information about the data is
revealed and we are allowed to complement the solution built in the ﬁrst stage
by purchasing extra resources at an inﬂated cost.
Robust optimization [4,5,17,3] can be considered the worst-case analogue of the
stochastic optimization. In a robust optimization problem, we are given bounds
on various parameters of the system, and the goal is to ﬁnd a solution that min-
imizes the cost in a worst-case scenario (or be feasible in a worst-case scenario).
A two-stage robust optimization problem (introduced in [3,8]) is similar to a two-
stage stochastic problem except instead of a distribution, we have a set of possible
scenarios (given either explicitly, or implicitly by giving bounds on various param-
eters), and instead of expectation, we would like to minimize the maximum cost
of the solution, where maximum is taken over the set of all possible scenarios.
During the past few years, stochastic optimization (and in particular, two-
stage stochastic optimization) has received considerable attention from the per-
spective of approximation algorithms. Eﬃcient approximation algorithms are
given for a wide class of optimization problems, both for cases where the dis-
tribution is given explicitly by listing the set of all possible scenarios and the
corresponding probabilities [18,13], and in the more general case where the dis-
tribution is given implicitly, as the product of a number of independent trials,
or by an oracle [15,20,12,8].
For robust optimization, Ben-tal et al [3] initiated the study of two-stage ro-
bust optimization problems. Dhamdhere et al. [8] introduced the ﬁrst approxima-
tion algorithms for two-stage robust covering problems when the set of scenarios
is given explicitly. In this paper, we take on the task of studying approximation
algorithms for two-stage robust optimization problems, where the set of possible
scenarios is given implicitly. In particular, we focus on the case where the set of
possible scenarios is given by an upper bound on the number of active clients,
and give approximation algorithms for the robust version of several classical
covering problems such as set cover, vertex cover, and edge cover.
1.1
The Model
In this section we give a formal deﬁnition of the robust optimization model
that will be studied in this paper. This model is a generalization of the model
introduced by Dhamdhere et al. [8] (in the case of explicitly listed scenarios),
and is motivated by similar models for two-stage stochastic optimization [15,13].
In a covering problem, we have a set C of potential clients, and a set R of
resources. Each resource r ∈R can be purchased at a cost cr. In order to serve
a set of clients, a set of resources must be purchased. The collection of all sets
of resources which can serve the set S ⊂C of clients is denoted by sol(S). In
covering problems the collection sol(S) is an upper ideal, i.e., if a set of resources

Robust Combinatorial Optimization with Exponential Scenarios
441
can serve S, so can any superset of this set. An unweighted covering problem is
a covering problem in which all cr’s are equal to 1.
Generally, the collection sol(S) is given implicitly by specifying a set of con-
straints. Three examples that we will focus on in this paper are set cover, vertex
cover, and edge cover. In the set cover problem, each resource r ∈R corresponds
to a set of clients, and the collection sol(S) consists of all sets of resources whose
union covers S. In the vertex cover problem, the set of clients and the set of
resources are the edge set and the vertex set of a given graph, respectively, and
a set S ⊂C can be served by any set of vertices that contain at least one of the
endpoints of each edge in S. In the edge cover problem, each resource is an edge
and each client is a vertex of a given graph, and a set S of clients can be covered
by any set of edges that has at least one edge adjacent to any vertex in S.
In a two-stage robust covering problem, we have a collection S of scenarios,
each given by a set of active clients (i.e., clients that need to be covered). The
objective is to purchase a set of resources in the ﬁrst stage to minimize the cost of
these resources plus a given inﬂation factor λ times the maximum over scenarios
S in S, of the cost of completing the solution for scenario S. In other words, after
we purchase a set of resources in the ﬁrst stage, an adversary decides in which
scenario we are. After that, we need to complete the solution by purchasing
more resources at costs inﬂated by a factor λ (or more generally, by an inﬂation
factors λS
r which depends on the resource r ∈R and the scenario S ∈S).
The robust optimization problem can be studied in several diﬀerent models,
depending on how the list of scenarios S is given to the algorithm. One model,
studied by Dhamdhere et al. [8] and Golovin et al [11], is to assume that the
list of all possible scenarios is given explicitly. This model is suitable for situ-
ations where the number of possible scenarios is not very large. An alternative
model, motivated by the independent trials model of stochastic optimization, is
to assume that the list of scenarios is given implicitly by an upper bound on
the maximum number of active clients. More formally, in this model an integer
k is given and S is deﬁned as {S ⊆C : |S| ≤k}.1 Finally, motivated by the
oracle model in stochastic optimization, we deﬁne an oracle model for robust
optimization where the list of possible scenarios is given by an oracle which,
given the set of resources purchased in the ﬁrst stage, outputs the worst-case (or
approximately the worst-case) scenario for the second stage.
An important distinction between our oracle model and the oracle model for
stochastic optimization is that in our model, the problem the oracle needs to
solve is often computationally intractable. For example, if the set of scenarios
in a robust set cover problem is given by an upper bound on the number of
active clients, the oracle needs to ﬁnd a subset of k clients whose minimum cost
of covering is maximized. We call this problem the max-min set cover problem,
1 More generally, we can consider a model where the set of clients is partitioned into
subsets C1, . . . , Ct, and the set of scenarios is the collection of all sets that have at
most ki clients from the set Ci. Although the results in Sections 2 and 3 work for
this more general model, for clarity of exposition we restrict ourselves to the simpler
model.

442
U. Feige et al.
and will observe that it is computationally hard. Considering the hardness of the
oracle problem, the algorithms designed for the oracle model need to be able to
work with an approximate oracle as well. Furthermore, in order to solve a robust
optimization problem in the model where the scenarios are given by an upper
bound on the number of active clients, in addition to designing an algorithm for
the oracle model, we need to give an approximation algorithm for the max-min
version of the problem.
1.2
Our Contribution
In this paper, we mostly focus on the model where the scenarios are given implic-
itly by an upper bound on the number of active scenarios. This is motivated by
real-world situations where a good estimate of the total number of clients who
will show up is available, but we do not exactly know where they will appear. We
will also give a general LP-based algorithm for the oracle model, assuming that
the oracle gives a good approximation of the worst-case scenario with respect to
the fractional solution.
A naive idea to solve the robust optimization problems is a buy-at-once al-
gorithm: either cover all items in the ﬁrst stage in which case nothing needs to
be done in the second stage. Or do nothing in the ﬁrst stage and construct a
solution in the second stage, after the adversary makes its choices. The choice
of which of the two options to use is based on a polynomial-time test that is
problem speciﬁc. We study this algorithm in Section 4 and prove that when the
inﬂation factor is the same for all scenarios, the approximation ratio of this algo-
rithm for robust unweighted set cover, vertex cover, and edge cover problems are
max(log m, log n), 2, and 2, respectively. However, the following example shows
that for the weighted version of robust vertex cover, any buy-at-once algorithm
(even with unbounded computing power) performs poorly. Consider a clique on
n vertices, with k = 1 and λ = √n. All vertices have weight 1, except for two
vertices that have weight w = √n. The buy-at-once algorithm will either pay at
least n in the ﬁrst stage, or at least λw = n in the second stage. However, an op-
timal algorithm can choose only one of the heavy vertices in the ﬁrst stage, and
then pay at most w + λk = 2√n. Hence the approximation ratio of the buy-at-
once algorithm for weighted robust vertex cover is no better than Ω(√n). This
example indicates the need for more sophisticated approximation algorithms for
robust two-stage optimization problems.
In Section 2, we give a general LP-based framework for solving robust covering
problems given access to an oracle that solves the fractional max-min problem
(or the adversary’s problem) and another oracle that rounds the LP solution for
the classical (i.e., non-robust) optimization problem. For example, for the robust
set cover problem, we need an oracle that given an integer k and a collection of
subsets S1, S2, . . . , Sm of a universe F each with a cost c(Si), ﬁnds a subset T ⊆F
of size at most k for which the cost of fractional set cover is maximized, and
another oracle that rounds a fractional set cover to an integral one. In Section 3,
we show how an online algorithm can be used to solve the max-min problem when
the set of feasible scenarios are given by an upper bound on the number of active

Robust Combinatorial Optimization with Exponential Scenarios
443
clients. We use this to give an O(log m)-approximation algorithm for max-min
fractional set cover. We also show that the max-min fractional set cover problem
is not approximable within a factor better than Ω(
log m
log log m) under reasonable
complexity assumptions. As a result of this framework, we get an O(log n log m)-
approximation for the robust set cover problem. Following similar ideas, we
design constant-factor approximation algorithms for robust vertex cover and
edge cover problems. This framework can be extended easily to more general
settings in which the scenarios are given implicitly in more general ways. The
main step for these extensions is to design good approximation algorithms for
the max-min problems.
Finally, in Section 5 we show that our algorithms for max-min fractional set
cover and edge cover achieve essentially the best possible approximation factors,
assuming reasonable complexity assumptions.
2
An LP-Rounding Approach for Robust Set Cover
In this section, we give an LP-based approach for robust set cover. Our tech-
niques work for a more general covering problem where each resource r ∈R
can be picked an integer number of times xr, and a client is covered if a corre-
sponding inequality of the form 
r airxr ≥1 (where air are given non-negative
coeﬃcients) is satisﬁed. The details of this generalization are omitted here.
We start by giving an LP formulation of two-stage robust set cover.2
minimize
Z +

r∈R
cry0
r
(1)
subject to ∀S ∈S, ∀i ∈S :

r: i∈r
(y0
r + yS
r ) ≥1
(2)
∀S ∈S :

r∈R
λcryS
r ≤Z.
(3)
The variable y0
r in the above LP indicates whether the resource r is purchased
in the ﬁrst stage. Similarly, the variable yS
r indicates whether this resource is
purchased in the second stage, if the adversary selects the set S as the set of
active clients. The variable Z indicates the maximum cost of the second stage,
where the maximum is taken over all possible scenarios. Clearly, if the variables
y0
r and yS
r are restricted to be integers, the above integer program captures the
robust set cover problem precisely. Therefore, relaxing the integrality condition
gives us a linear program whose solution is a lower bound on the cost of the
optimal solution to the robust set cover problem.
The main diﬃculty with this LP formulation is that it contains an exponential
number of constraints and an exponential number of variables, and therefore can-
not be solved directly using the ellipsoid method. We can deal with this problem
2 We present this LP in the case that the inﬂation factor λ does not depend on the
resource r or the scenario S. However, it is easy to see that all proofs in this section
apply to the more general case.

444
U. Feige et al.
using a technique developed by Shmoys and Swamy [20] for stochastic optimiza-
tion: we consider the projection of the above LP onto the space corresponding
to the variables y0
r’s and Z, and then give a separation oracle for the reduced
LP. The projection of the above LP corresponds to the following program.
minimize
Z +

r∈R
cry0
r
(P)
subject to ∀S ∈S : Z ≥cost2(S, y0)
Here cost2(S, y0) denotes the cost of the optimal fractional solution for the second
stage when the set of active clients is S, given that resource r is already purchased
to the extent of y0
r in the ﬁrst stage.
The separation oracle for this LP corresponds to an algorithm that computes
the optimal strategy for the adversary of the robust fractional set cover problem.
We call this the max-min fractional set cover problem. More precisely, the max-
min fractional set cover problem is the following: given a fractional ﬁrst-stage
solution (i.e., y0
r’s), select a scenario (in the example we will focus on in this
paper, a set of at most k clients) so that the cost of a fractional solution for
the second stage is maximized. The following lemma, proved using a simple
application of the ellipsoid method, shows that given an approximation algorithm
for the max-min fractional problem, we can compute an approximate solution
of the above LP in polynomial time.
Lemma 1. Assume we have a polynomial time γ-approximation algorithm for
the max-min fractional problem. Then, we can compute a γ-approximation to
the solution of the linear program (P) in polynomial time.
The proof, which is omitted here, is based on the ellipsoid algorithm and the tech-
niques used by Shmoys and Swamy [20] in the context of stochastic optimization.
The above lemma requires us to be able to solve the max-min fractional set
cover problem given a fractional ﬁrst-stage solution. In other words, for each
client i we are given a fractional value θi, so that if the adversary chooses i
in the set of active clients, we will have to cover i to the extent of θi. In the
following lemma, we show that it is enough to be able to solve the max-min
problem given that θi’s are zero or one. In other words, given a subset C′ of the
clients (corresponding to those with θi = 1), we need to be able to ﬁnd a set of
at most k clients in C′ whose minimum fractional covering cost is maximized.
We call this problem the max-min fractional set cover problem with integer
requirements.
Lemma 2. Assume we have a polynomial time γ-approximation algorithm A
for the max-min fractional set cover problem with integer requirements. Then,
we can compute a (γ+1)-approximation to the solution of the linear program (P)
in polynomial time.
Proof. We iteratively run the ellipsoid algorithm to check whether (P) has a
solution with an objective function value of at most R, and use binary search to
ﬁnd the smallest value of R for which the ellipsoid algorithm declares that there

Robust Combinatorial Optimization with Exponential Scenarios
445
is such a solution. For the separation oracle, we do the following: let C′ = {i ∈
C : 
r:i∈r y0
r < 1/(γ + 1)} denote the set of clients covered by the fractional
ﬁrst-stage solution to an extent less than 1/(γ + 1). We run algorithm A to ﬁnd
the max-min fractional set cover among clients in C′. Let T denote the cost of
the solution returned by A. This means that there is at least one scenario S∗∈S
such that the cost of minimum fractional cover for S∗∩C′ is at least T , and for
every scenario S ∈S, the cost of the minimum fractional cover for S ∩C′ is at
most γT . Our separation oracle accepts (y0) if the cost of the ﬁrst stage solution
y0 plus
γ
γ+1T is at most R; otherwise it rejects (i.e., declares that there is no
solution with a ﬁrst stage solution of y0 of total cost at most R).
First, we show that if the above separation oracle rejects y0, then an exact
separation oracle would do the same. This is because with a ﬁrst stage solution
of y0, clients in C′ need to be covered to the extent of at least
γ
γ+1 in the second
stage, and therefore the cost of the second stage in scenario S∗cannot be less
than
γ
γ+1 times the cost of the minimum fractional cover for S∗∩C′, which, by
deﬁnition, is at least T .
Next, we prove that if our separation oracle accepts y0, then we can build a
feasible solution for (P) of cost at most (γ + 1)R. This is done by multiplying y0
by (γ + 1). The set of clients not covered by this inﬂated ﬁrst stage solution is a
subset of C′. Therefore, the cost of the second stage is at most γT . The overall
cost of this solution is at most (γ + 1) 
r∈R cry0
r + γT ≤(γ + 1)R, where the
latter inequality follows from the fact that our separation oracle accepts y0.
Now, let R∗be the smallest value of R for which our algorithm decides that the
linear program has a solution. This means that for R < R∗, our separation oracle
never accepts any ﬁrst stage solution y0. By our ﬁrst observation, the ellipsoid
algorithm with an exact separation oracle would return the same answer. Hence,
R∗is a lower bound on the solution of (P). Since the ellipsoid algorithm for
R = R∗ﬁnds a y0 which our separation oracle accepts, our second observation
implies that there is a solution of value (γ + 1)R∗for (P). This is a (γ + 1)-
approximate solution for the program (P).
□
The solution obtained by solving the linear program (P) can be rounded into
an integral solution using an LP-based algorithm that solves the (non-robust)
optimization problem. Combining this with Lemma 2, we obtain the following.
Theorem 1.
Assume there is an α-approximation algorithm A1 for the max-
min fractional set cover problem with integer requirements, and an algorithm
A2 that given a subset S of clients, ﬁnds an integral solution that covers the
clients in S and whose cost is at most β times the minimum cost of fractionally
covering S. Then there is a (α + 1)β-approximation algorithm for the robust set
cover problem.
Proof. We run the algorithm described in the proof of Lemma 2 to compute an
(α + 1)-approximate solution to the LP (P). The solution that this algorithm
ﬁnds corresponds to (α + 1)y0, where y0 is a ﬁrst stage solution accepted by
our separation oracle. As in the separation oracle, we deﬁne C′ = {i ∈C :

r:i∈r y0
r < 1/(α + 1)}. Now, we run the algorithm A2 to ﬁnd an integral set

446
U. Feige et al.
cover solution that covers clients in C′. The cost of this ﬁrst stage solution is at
most (α + 1)β times the cost of y0. Also, for every scenario in the second stage,
we use A2 to ﬁnd a β-approximation to the optimal fractional second stage cost.
This deﬁnes a (α + 1)β approximation algorithm for robust set cover.
□
By the above theorem, the main ingredient in solving a robust optimization
problem with implicitly given scenarios is the algorithm for the max-min prob-
lem. In the next section, we show how an online algorithm for the underlying
optimization problem can be used to approximately solve the max-min problem.
3
The Max-Min Problems
The results of the previous section show that in order to solve the LP relaxation
of the robust set cover problem, we need to consider the max-min problem.
In this section, we design an O(log m)-approximation algorithm for max-min
fractional set cover problem. In fact, we present a general framework to design
an approximation algorithms for a max-min problem using online competitive
algorithms for the underlying optimization problem. Note that the max-min
problems that we need to solve for approximating the robust covering problems
are the fractional variants of the problems.
Given a universe F of clients and a subset T ⊆F, let opt(T ) be the cost of an
optimal (fractional) solution to cover all clients in T . Let A be an α-competitive
online algorithm for a covering problem. Namely, upon the arrival of any client
ak to an existing set of clients a1, a2, . . . , ak−1, A augments the current solution
to a feasible solution for a1, . . . , ak−1, ak. The algorithm is α-competitive if for
every sequence of clients a1, . . . , ak the cost of the online solution produced by
A is at most α times the cost of the optimal (oﬄine) solution for a1, a2, . . . , ak.
Let A(b|a1, a2, . . . , ak) denote the marginal increase in the cost of the solution
constructed by algorithm A when we add a new element b to an existing sequence
of clients (a1, . . . , ak).
Consider two solutions w and w′ for a fractional covering problem. Solution w′
dominates solution w if for each set S the fractional value given to its respective
variable in w′ is at least as large as that given in w. We say that the covering
problem satisﬁes the monotonicity property, if for any two given solutions w and
w′ such that w′ dominates w and any element a, the optimal marginal increase
in expanding w′ to cover a is not more than the optimal marginal increase in
expanding w to cover a. It is not hard to prove that the set cover problem and
its special cases satisfy this property.
The following theorem presents a relation between competitive online algo-
rithms and approximation algorithms for the max-min problem.
Theorem 2. Let A be an α-competitive online algorithm for a covering problem.
If the covering problem satisﬁes the monotonicity property then the corresponding
max-min problem admits an (
e
e−1)α-approximation algorithm.
Proof. Given the online algorithm A for the covering problem, we prove that the
following algorithm B is a (1 −1
e)α-approximation algorithm for the max-min
problem:

Robust Combinatorial Optimization with Exponential Scenarios
447
1. T = ∅.
2. for i = 1, . . . , k do
(a) Find a client ai that maximizes A(ai|a1, a2, . . . , ai−1) and add it to T .
Let the optimal solution to the max-min problem be the set {b1, . . . , bk} of
clients. Let OPT∗be the optimal cost of covering {b1, . . . , bk}. Let Wi be the cost
of the solution of the online algorithm after i elements a1, . . . , ai have arrived and
Li = max[0, OPT∗−Wi]. We prove that Li ≤(1 −1
k)Li−1. Consider expanding
the solution of the online algorithm for {a1, . . . , ai−1} in the optimal way so that
it covers {b1, b2, . . . , bk}. The cost of this new solution is at least OPT∗. Hence
there is some item bj (with 1 ≤j ≤k) such that there is diﬀerence of at least
OPT
∗−Wi−1
k
in cost between the case in which the clients b1, . . . , bj−1 are added
(or no clients at all, if j = 1) and the case in which the clients b1, . . . , bj are
added. Since the covering problem satisﬁes the monotonicity property, adding
bj alone to {a1, . . . , ai−1} requires an increase in cost of at least OPT
∗−Wi−1
k
compared to the cost of A covering {a1, . . . , ai−1}. Hence Wi−1 + (OPT
∗−Wi−1
k
)
is a lower bound on the cost of A for covering {a1, . . . , ai−1, bj}. Since algorithm
B chooses in the ith step the ai that maximizes the marginal increase in the
cost of A, we will indeed have that Wi ≥Wi−1 + OPT
∗−Wi−1
k
, and thus, Li ≤
Li−1(1 −1
k). Thus Li ≤(1 −1
k)iL0. Therefore, Lk ≤(1 −1
k)kOPT∗≤1
eOPT∗.
This shows that Wk ≥(1 −1
e)OPT∗. Since A is an α-competitive algorithm,
the true cost of covering {a1, . . . , ak} is at least Wk/α, and algorithm B is a
(
e
e−1)α-approximation algorithm for the max-min problem.
□
Using Theorem 2 and known online algorithms, we can design approximation
algorithms for the max-min problems. For example, an O(log m)-competitive
algorithm for the online fractional set cover problem by Alon et al. [1] and The-
orem 2 implies an O(log m)-approximation algorithm for the max-min fractional
set cover problem. In Section 5, we show that this result is nearly best possible
(assuming certain complexity theoretic assumptions). This algorithm, together
with the O(log n) randomized rounding algorithm for set cover and Theorem 1,
imply the following.
Theorem 3. There exists an O(log m log n)-approximation algorithm for the ro-
bust two-stage set cover problem.
Using the ideas of the 2-approximation algorithm for vertex cover by Bar-Yehuda
and Even [2], we can design a 2-competitive online algorithm for vertex cover
problem as follows. In the online algorithm, we keep track of a value r(u) for
each vertex u of the graph. We initialize these values to r(u) = w(u). Upon the
arrival of a new edge e = uv, the online algorithm sets ru = ru −min(ru, rv)
and rv = rv −min (ru, rv). Observe that after this update either r(u) = 0 or
r(v) = 0. At any moment, the fractional vertex cover solution is to pick 1−r(u)
w(u)
fraction of each vertex u. This means that we fully pick u or v for edge e = uv
and this solution is a feasible fractional vertex cover. Similar to the proof of
Bar-Yehuda and Even [2], we can prove that this algorithm is a 2-competitive

448
U. Feige et al.
online algorithm. Using Theorem 2, this 2-competitive online algorithm implies a
( 2e
e−1)-approximation algorithm for the max-min fractional vertex cover problem.
Applying the above results and the 2-approximate rounding procedure for the
vertex cover problem, we get a 2( 2e
e−1 +1)-approximation algorithm for the robust
(weighted) vertex cover problem. The details are omitted.
For the edge cover problem, a simple 2-competitive online algorithm is to cover
every arriving vertex with the cheapest edge incident to it. This, together with
Theorems 2 and 1, give a constant-factor approximation algorithm for the robust
edge cover problem. We do not optimize the constants of the approximation ratio
for the weighted problems. However, in Section 4 we show a tight buy-at-once
2-approximation for unweighted vertex cover and edge cover.
4
Improved Algorithms for Unweighted Problems
In this section, we give buy-at-once approximation algorithms for unweighted
variants of robust set cover, vertex cover, and edge cover.
4.1
Robust Unweighted Set Cover
In the robust unweighted set cover problem, all sets have unit cost. The input
of the problem consists of n items, a collection of m sets, a parameter k (for
number of items to be chosen by adversary), and an inﬂation factor λ > 1.
To simplify notation, we assume here that parameters such as k, m and n are
suﬃciently large, and hence we shall ignore eﬀects such as rounding ln m to the
nearest integer. They aﬀect the approximation ratio only by low order terms.
The buy-at-once approximation algorithm for robust set cover is as follows:
1. Compute a minimum fractional set cover that covers all potential clients and
let t be its size.
2. If t <
λk
ln n, use the greedy algorithm to ﬁnd a set cover. It will be of size at
most t ln n. Nothing needs to be done in the second stage.
3. If t ≥
λk
ln n, do nothing in ﬁrst stage. In the second stage, use a greedy
algorithm to cover the items chosen by the adversary.
Theorem 4. The above buy-at-once algorithm achieves an approximation ratio
no worse than max(ln n, ln m) (up to low order terms) for unweighted robust set
cover.
Proof. Observe that by duality, t is the size of the maximum fractional packing.
Let αt be the number of sets chosen by the optimal solution (to the robust
set cover problem) in the ﬁrst stage. Removing all items covered by these sets,
the remaining set cover instance still has a fractional packing of value at least
(1 −α)t. (We may assume that α ≤1, as otherwise the analysis becomes even
simpler.) Pick a set T of items, where each item is selected into T independently,
with probability equal to its fractional value in the maximum fractional packing.
The expected size of T is at least (1 −α)t. In fact, known bounds by Siegel [19]

Robust Combinatorial Optimization with Exponential Scenarios
449
imply that |T | ≥⌊(1 −α)t⌋with probability at least 1/2. Moreover, every set is
expected to contain at most one item from T , and Chernoﬀbound implies that
with probability at least 1/2, no set will contain more than ln 2m items. For
simplicity of notation, we shall assume that T contains exactly (1 −α)t items,
and no set contains more than ln m items from T . (This assumption aﬀects only
low order terms in the approximation ratio.) Hence in the second stage opt will
pay at least min((1 −α)t, k)
λ
ln m, and in the two stages combined opt pays at
least αt + min((1 −α)t, k)
λ
ln m. This is a piecewise linear function in α, and its
minimum is achieved when α is either 0 or 1, or when (1 −α)t = k. It follows
that opt pays at least min(t,
kλ
ln m).
Now we can analyze the approximation ratio of our algorithm. When t <
λk
ln n,
the algorithm pays at most t ln n ≤λk, which is a factor of ln n larger than t,
and at most a factor of ln m larger than
kλ
ln m. Hence the approximation ratio in
this case is at most max(ln m, ln n).
When t ≥
λk
ln n, the algorithm pays nothing in the ﬁrst stage, and at most
λk ≤t ln n in the second stage. Again, the approximation ratio can be seen to
be at most max(ln m, ln n).
□
The following example shows that the above analysis for the buy-at-once algo-
rithm is tight up to a log log m factor: consider an instance of the two-stage robust
set cover where the ground set consists of n+n1/4 elements and k = n1/4 and λ =
n1/4. The family of subsets in the set cover instance is the family of all subsets of
size n1/4 of set {1, 2, . . ., n} and all singleton sets {n+1}, {n+2}, ..., {n+n1/4}.
The optimal solution is to buy all singleton sets in advance and wait for the
scenario. Since the adversary should choose a set of size n1/4 of {1..n} and this
set is in the family of sets in the set cover instance, we can cover any scenario
by buying one set at cost λ = n1/4 later. Thus, the cost of the optimal solution
is at most 2n1/4. If we do not buy any set in advance, the adversary selects
{n + 1, n + 2, ..., n + n1/4} and we should pay λk = n1/2 in the second stage. On
the other hand, in order to cover all elements in the ﬁrst stage, we need to buy at
least n3/4 sets. Both of these cases are more than a factor of n1/4 = Ω(
log m
log log m)
larger than the optimal solution.
Also, observe that the term ln n in the approximation ratio cannot be im-
proved by any polynomial-time algorithm (e.g., when λ = ∞), due to the hard-
ness of approximating minimum set cover [10]. It is not clear whether the term
ln m can be improved.
4.2
Robust Unweighted Vertex Cover
Consider the following buy-at-once algorithm for the robust unweighted vertex
cover problem: compute a maximum matching M in G, and let |M| denote its
size. If |M| < λk, then we pick a vertex cover of size no larger than 2|M| in
the ﬁrst stage (for example, by picking both endpoints of every edge in M) and
nothing needs to be done in the second stage. If |M| ≥λk, we do nothing in the
ﬁrst stage and in the second stage, for each edge that is present in the realized
scenario, we pick one of its endpoints arbitrarily.

450
U. Feige et al.
Theorem 5. The above algorithm achieves an approximation ratio of 2 for the
unweighted robust vertex cover problem.
Proof. Let OPT denote an optimal algorithm, and let x denote the number of
vertices purchased by this algorithm in the ﬁrst stage. Thus, at least |M| −x of
the edges in M are not covered by OPT in the ﬁrst stage. Consider the scenario
where the adversary picks min(k, |M| −x) of these edges in the second stage.
The overall cost of OPT in this scenario is T := x + λ min(k, |M| −x). Since
λ ≥1, we have T ≥min(λk, |M|).
Now, we show that the cost of our algorithm is always at most 2T . We consider
two cases: if |M| < λk, our algorithm buys a vertex cover of cost at most 2|M| in the
ﬁrst stage. Therefore, the cost of our algorithm is at most 2|M| = 2 min(λk, |M|) ≤
2T . If |M| ≥λk, our algorithm incurs a cost of λk = min(λk, |M|) ≤T . Therefore,
in this case our algorithm is actually optimal.
□
Note that any algorithm that approximates unweighted robust vertex cover
within a ratio better than 2 can be used to approximate the minimum ver-
tex cover problem within a ratio better than 2 (e.g., by setting λ = ∞), and
achieving this would resolve a long standing open problem.
4.3
Robust Unweighted Edge Cover
The input of the unweighted edge cover problem is a graph with n vertices, m
edges, a parameter k (for number of vertices to be chosen by adversary), and
an inﬂation factor λ > 1. All edges have unit cost. Observe that the number of
edges needed to cover ℓvertices is always between ℓ/2 and ℓ. This fact serves as a
basis for a tight 2-approximation for the robust unweighted edge cover problem.
The algorithm and the proof are left to the full version of the paper. Moreover,
we can prove that if P̸=NP, then the max-min variant and the robust two-stage
variant of the edge cover problem cannot be approximated within a factor better
than 2.
5
Hardness of Max-Min Problems
In this section, we give a strong inapproximability result for the max-min (frac-
tional) set cover problem. First, we show a hardness result for the max-min
(fractional) edge cover problem.
Theorem 6. For every ϵ > 0, it is NP-hard to approximate the max-min un-
weighted edge cover problem within a ratio better than 2 −ϵ.
Proof. The proof is by reduction from the maximum independent set problem.
As shown in [14], for every suﬃciently small ϵ > 0, it is NP-hard to distinguish
between the following two classes of graphs:
Yes instances: graphs on n vertices that contain an independent set of size ϵn.
No instances: graphs on n vertices with no independent set of size ϵ5n.

Robust Combinatorial Optimization with Exponential Scenarios
451
In order to distinguish whether a given graph G is a yes instance or a no
instance, we construct an instance of the max-min edge cover problem that
consists of G and k = ϵn. On yes instances, one can select k vertices that
form an independent set in G, and then k edges are needed in order to cover
them. On no instances, whenever there are more than ϵ5k vertices, two of them
share an edge in G. It follows that any selection of k vertices can be covered by
ϵ5k +(1 −ϵ5)k/2 < k/(2 −ϵ) edges. Therefore, any algorithm that approximates
the max-min unweighted edge cover problem within a factor better than 2 −ϵ
can be used to distinguish between these classes.
□
The proof of Theorem 6 can be adopted easily for the max-min fractional edge
cover problem. This implies that for any ϵ > 0, it is NP-hard to approximate
the max-min fractional edge cover problem within a factor better than 2 −ϵ.
This hardness ratio can be strengthened to nearly logarithmic factors for the
fractional set cover problem, but proving this using current techniques seems to
require assumptions stronger than P ̸= NP. Picking p(n) = √n in Theorem 7
shows that the max-min (fractional) set cover problem cannot be approximated
within a ratio better than Ω(
log N
log log N ) (on instances of size N) unless 3SAT can
be solved in time 2O(√n) (on instances of size n).
Theorem 7. For every 0 < δ < 1 and p(n) = nδ, the max-min fractional set
cover problem cannot be approximated within a ratio better than Ω(
p(n)
log p(n)) on
instances of size N = 2O(p(n)) (in time polynomial in N), unless NP problems
(say 3SAT) can be solved in time 2O(p(n)).
Proof. The proof is presented for the integral set cover problem, but the ap-
proximation hardness applies also to the max-min fractional set cover problem,
because in the yes instance the cover is disjoint. The proof is based on the
structure of instances of set cover that are generated by the reduction described
in [10], and speciﬁcally, on the parameters given in Section 6 in [10]. Here we
only sketch the proof.
Recall that in [10], the hardness of approximation result is based on a certain
multiple-prover proof system. We shall need the number of provers (denoted
in [10] by k) to be p(n). (Hence one cannot use here the earlier [16] instead
of [10].) In [10] it suﬃces that the number of parallel repetitions ℓis logarithmic
in the number of provers, hence we can have ℓ= O(log(p(n))). (Remark: later
work [9] used a version of a multilayered PCP which is somewhat simpler than
the multiple prover system of [10]. This simpler version requires ℓto grow at a
faster rate than p(n), and would result in weaker conclusions if used in the proof
of Theorem 7.) This results in a set cover instance with 2O(p(n)) clients and sets.
Each subset in [10] would be an item in the max-min set cover problem. Each
item in [10] would be a set in the max-min set cover problem. Note that in [10]
all sets are of the same size, and there is a disjoint set cover for yes instances, say,
by t sets. We shall set k for the max-min set cover problem to be equal to this
t. Hence yes instances of [10] correspond to yes instances of max-min set cover
for which k clients can be selected that require k sets in order to be covered.

452
U. Feige et al.
The property of no instances of [10] that we shall use is the following: for every
q < p(n), for every collection of tq/p(n) sets, there is some item that belongs to
O(p(n)/q) of the sets. Extensions of the analysis in [10] can be used in order to
prove this property, but this is omitted from the current paper.
The property above implies that for no instances in [10], for every collection
of t sets there are O(t log(p(n))
p(n)
) clients that hit all the sets. This implies that in
no instances of the max-min set cover problem, the optimum solution has value
O(t log(p(n))
p(n)
).
□
Acknowledgements
This work beneﬁted greatly from insightful comments that anonymous reviewers
provided on previous versions of this manuscript.
References
1. N. Alon, B. Awerbuch, Y. Azar, N. Buchbinder, and J. Naor. A general approach
to online network optimization problems. In SODA, pages 577–586, 2004.
2. R. Bar-Yehuda and S. Even.
A linear time approximation algorithm for the
weighted vertex cover problem. Journal of Algorithms, 2:198–203, 1981.
3. A. Ben-Tal, A. Goryashko, E. Guslitzer, A. Nemirovski. Adjustable robust solutions
of uncertain linear programs. 351-376 Mathematical Programming, 99:2:351-376,
2004.
4. D. Bertsimas and M. Sim. The price of robustness. Operation Research, 52:35–53.
5. D. Bertsimas and M. Sim. Robust discrete optimization and network ﬂows. Math-
ematical Programming Series B, 98:49–71.
6. J. Birge and F. Louveaux.
Introduction to stochastic programming.
Springer,
Berlin, 1997.
7. G. B. Dantzig.
Linear programming under uncertainty.
Management Science,
1:197–206, 1955.
8. K. Dhamdhere, V. Goyal, R. Ravi, and M. Singh. How to pay, come what may:
Approximation algorithms for demand-robust covering problems. FOCS, 2005.
9. I. Dinur, V. Guruswami, S. Khot, and O. Regev. New multilayered pcp and the
hardness of hypergraph vertex cover. SIAM Journal of Computing, 34(5):1129–
1146, 2005.
10. U. Feige. A threshold of ln n for approximating set cover. JACM, 45(4):634–652,
1998.
11. D. Golovin, V. Goyal, and R. Ravi.
Pay today for a rainy day: Improved ap-
proximation algorithms for demand-robust min-cut and shortest path problems.
STACS, 2006.
12. A. Gupta, M. Pal, R. Ravi, and A. Sinha.
Boosted sampling: Approximation
algorithms for stochastic optimization. In STOC, pages 170–178, 2004.
13. A. Gupta, R. Ravi, and A. Sinha. An edge in time saves nine: Lp rounding ap-
proximation algorithms for stochastic network design. FOCS, 45, 2004.
14. J. Hastad. Clique is hard to approximate. In FOCS, pages 627–636, 1996.
15. N. Immorlica, D. Karger, M. Minkoﬀ, and V. S. Mirrokni.
On the costs and
beneﬁts of procrastination: Approximation algorithms for stochastic combinatorial
optimization problems. In SODA, 2004.

Robust Combinatorial Optimization with Exponential Scenarios
453
16. C. Lund and M. Yannakakis.
On the hardness of approximating minimization
problems. JACM, 41(5):960–981, 1994.
17. Y. Nikulin. Robustness in combinatorial optimization and scheduling theory: An
annotated bibliography. Technical Report SOR-91-13, Statistics and Operation Re-
search, http://www.optimization-online.org/DB FILE/2004/11/995.pdf, 2004.
18. R. Ravi and A. Sinha. Hedging uncertainty: Approximation algorithms for stochas-
tic optimization problems. IPCO, pages 101–115, 2004.
19. A. Siegel, Median Bounds and Their Application. J. Algorithms 38(1): 184-236
(2001).
20. D. Shmoys and S. Swamy.
Stochastic optimization is (almost) as easy as
deterministic optimization. In FOCS, 2004.

Approximation Algorithms for the Multi-item
Capacitated Lot-Sizing Problem Via Flow-Cover
Inequalities
Retsef Levi1, Andrea Lodi2, and Maxim Sviridenko3
1 Sloan School of Management, MIT, Cambridge, MA, 02139
retsef@mit.edu
2 DEIS, University of Bologna, viale Risorgimento 2 - 40136 Bologna - Italy
alodi@deis.unibo.it
3 IBM T.J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY 10598
sviri@us.ibm.com
Abstract. We study the classical multi-item capacitated lot-sizing problem with
hard capacities. There are N items, each of which has speciﬁed sequence of
demands over a ﬁnite planning horizon of discrete T periods; the demands are
known in advance but can vary from period to period. All demands must be sat-
isﬁed on time. Each order incurs a time-dependent ﬁxed ordering cost regardless
of the combination of items or the number of units ordered, but the total number
of units ordered cannot exceed a given capacity C. On the other hand, carrying
inventory from period to period incurs holding costs. The goal is to ﬁnd a feasible
solution with minimum overall ordering and holding costs.
We show that the problem is strongly NP-Hard, and then propose a novel fa-
cility location type LP relaxation that is based on an exponentially large subset of
the well-known ﬂow-cover inequalities; the proposed LP can be solved to opti-
mality in polynomial time via an efﬁcient separation procedure for this subset of
inequalities. Moreover, the optimal solution of the LP can be rounded to a feasi-
ble integer solution with cost that is at most twice the optimal cost; this provides a
2-Approximation algorithm, being the ﬁrst constant approximation algorithm for
the problem. We also describe an interesting on-the-ﬂy variant of the algorithm
that does not require to solve the LP a-priori with all the ﬂow-cover inequalities.
As a by-product we obtain the ﬁrst theoretical proof regarding the strength of
ﬂow-cover inequalities in capacitated inventory models. We believe that some of
the novel algorithmic ideas proposed in this paper have a promising potential in
constructing strong LP relaxations and LP-based approximation algorithms for
other inventory models, and for the capacitated facility location problem.
Keywords:
approximation
algorithms,
integer
programming,
polyhedral
combinatorics, randomized algorithms, scheduling theory and algorithms.
1
Introduction
The issue of capacity constrains arises in many practical and theoretical inventory man-
agement problems as well as in problems in other application domains, such as facility
location problems. In most practical inventory systems there exist capacity constrains
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 454–468, 2007.
c⃝Springer-Verlag Berlin Heidelberg 2007

Approximation Algorithms for the Multi-item Capacitated Lot-Sizing Problem
455
that limit the quantities that one can order, ship or produce. Unfortunately, it is often
the case that models with capacity constrains are computationally far more challenging
than their counterpart models with no capacity constrains. In particular, in many prob-
lems with capacity constrains computing optimal policies and sometimes even feasible
policies is a very challenging task.
In recent years there has been an immense work to develop integer programming
methods for solving hard, large-scale deterministic inventory management problems.
(We refer the reader to the recent book of Pochet and Wolsey [18].) A major part of
this work has been focused on constructing strong formulations for the corresponding
inventory models. In fact, it is essential to have an integer programming formulation
with a strong linear programming relaxation. Stronger formulations are achieved by
identifying valid inequalities that are satisﬁed by all feasible integral solutions and cut
off fractional solutions. Another key aspect within an integer programming framework
is the ability to construct good feasible integer solutions to the corresponding model.
This has been known to have a huge impact on decreasing the computational effort
involved. In models with capacity constrains, ﬁnding good feasible solutions can be
very challenging.
In this paper, we study the classical multi-item capacitated lot-sizing problem, which
is an extension of the single-item economic lot-sizing problem [18]. We propose a novel
facility location type linear programming (LP), and show how to round its optimal so-
lution to a feasible integral solution with cost that is guaranteed to be at most twice the
optimal cost. This is called a 2-Approximation algorithm, that is, the cost of the solution
constructed by the algorithm is guaranteed to be at most twice the optimal cost. (This
is the ﬁrst constant approximation algorithm for this problem.) The LP relaxation is
based on a variant of a well-known class of valid inequalities called ﬂow-cover inequal-
ities. These inequalities have been introduced over two decades ago [16] and have been
shown empirically to be very effective in solving several inventory and facility location
problems with capacity constrains [1,18]. (In Section 2 below, we discuss the relevant
literature on ﬂow-cover inequalities in more details.) Our results have several signiﬁcant
contributions: (i) To the best of our knowledge, this is the ﬁrst theoretical evidence for
the strength of ﬂow-cover inequalities applied to capacitated inventory models. (All the
previous theoretical results have been obtained for ﬁxed-charge single-node problems.
See Section 2 below for details.) (ii) Our approach provides a conceptually simple way
to generate provably good feasible solutions, and can be easily implemented within an
integer programming framework. (iii) Several of the newly proposed algorithmic ideas
in this paper have a promising potential of applicability in other inventory models with
capacity constrains. Moreover, we believe that they can be used to develop strong LP
relaxations and LP-based approximation algorithms for the capacitated facility location
problem.
The model. The details of the inventory model discussed in this paper are as follows.
There are N items indexed by i = 1, . . . , N, each of which has a speciﬁed sequence of
demands over a ﬁnite planning horizon of T discrete periods indexed by t = 1, . . . , T.
The demand of item i in period t is denoted by dit. The demands are known in advance
but can vary from period to period. Moreover, all of the demands must be fully satisﬁed
on time, that is, dit must be fully ordered by time period t. At the beginning of each

456
R. Levi, A. Lodi, and M. Sviridenko
period s = 1, . . . , T, it is possible to place an order for any combination of items,
and this incurs a ﬁxed ordering cost Ks regardless of the combination of items or the
number of units ordered. However, the overall quantity of units ordered in period s
cannot exceed a certain capacity limit Cs ≥0. These are usually called hard capacity
constrains in contrast to soft capacity constrains, where in each period s, the order is
placed in batches, each of which has capacity Cs and incurs an additional ﬁxed ordering
cost Ks. We consider the special case with uniform capacities, i.e., Cs = C, for each
s = 1, . . . , T.
The units ordered in period s are assumed to arrive instantaneously, and can be used
to satisfy demands in that period and subsequent periods. The ﬁxed ordering cost is
balanced with a cost to maintain physical inventory that is called holding cost. In most
of the existing literature the holding costs are linear and additive. Speciﬁcally, for each
item i and period t, there is a holding cost parameter hit ≥0 that denotes the per-unit
cost to carry one unit of item i in inventory from period t to period t + 1. Following
Levi, Roundy and Shmoys [12], we model the holding cost in a more general way. For
each demand point (i, t) and a potential order s ≤t, let hi
st ≥0 be the per-unit cost of
holding one unit of item i in inventory from period s to period t. The only assumption is
that, for a ﬁxed (i, t), the parameters hi
st are non-increasing in s. (This implies that if dit
is ordered from a closer period to t the resulting holding cost is not bigger.) The way we
model the holding cost is more general, and can capture several important phenomena
such as perishable goods. We also note that we can incorporate a per-unit ordering cost
into the holding cost parameters. The goal is to ﬁnd a feasible policy that satisﬁes all of
the demands on time and has minimum overall ordering and holding cost.
Literature review. As we already mentioned, this is a classical model in inventory the-
ory that has been studied by several researchers throughout the years. The special case
with a single-item (N = 1) and uniform capacities is polynomially solvable both with
hard capacities [11] and soft capacities [17]. (This is usually called single-item capac-
itated economic lot-sizing problem.) Moreover, there are known extended LPs, that is,
LPs with integrality property that provide an exact description of the set of feasible so-
lutions. The single item problem with non-uniform capacities is known to be weakly
NP-Hard [11], but there is a fully polynomial time approximation scheme (FPTAS)
[19]. For results on other variants of single-item models, we refer the reader to [6,18].
Federgruen, Meisner and Tzur [10] have studied the model discussed in this paper
with traditional holding cost, but with additional ﬁxed item ordering costs that are in-
curred in each period, in which item i is ordered. Under the assumption that all of the
demands and the cost parameters are uniformly bounded by constants, they have pro-
posed a dynamic-programming-based algorithm, and shown that it is asymptotically
optimal as the number of periods increases to inﬁnity. In a subsequent paper [9], they
provide a probabilistic analysis of the algorithm. Another dynamic-programming-based
algorithm for a special case of the model discussed in this paper has been proposed
by Anily and Tzur [3]. (They have studied a model with traditional holding costs and
stationary cost parameters, i.e., hit = h and Kt = K, for each i and t.) However,
the running time of their algorithm grows exponentially fast in the number of items,
and thus, it is not practical unless there are few items. In a recent paper Anily, Tzur

Approximation Algorithms for the Multi-item Capacitated Lot-Sizing Problem
457
and Wolsey [4] have considered the same model with time-dependent cost parameters,
but with the additional monotonicity assumption on the holding parameters. In par-
ticular, the assumption is that the items are ordered, such that each item has higher
holding costs than all previously ordered items, uniformly for all periods. Speciﬁcally,
h1t ≤h2t ≤· · · ≤hNt, for all periods t = 1, . . . , T. For this problem, they have
proposed an extended linear programming formulation with O(NT 2) constrains and
variables that solves the problem to optimality. (This implies that this special case is
polynomially solvable.)
Our results and techniques. Our ﬁrst result shows that the multi-item capacitated lot-
sizing problem with hard or soft capacities is strongly NP-Hard. (This is done by re-
duction from 3-PARTITION Problem, the details are omitted due lack of space.) This
implies that the monotonicity assumption of Anily, Tzur and Wolsey [4] is somewhat
essential to get a polynomial time optimization algorithm. We propose a novel facility
location type LP relaxation for the problem that is very different than the one used by
Anily, Tzur and Wolsey [4]. Our LP is based on the family of ﬂow-cover inequalities
in the same spirit as the LP proposed by Aardal, Pochet and Wolsey for the capacitated
facility location problem [2]. However, it incorporates only a subset of the class of ﬂow-
cover inequalities: there are exponentially many inequalities in this subset, but we show
that they can be separated in polynomial time. Thus, the LP can be solved optimally in
polynomial time, using the Ellipsoid method. We then use an extremely simple round-
ing algorithm. The optimal solution of the LP relaxation is scaled by a suitably chosen
factor, and the scaled solution is used to execute a randomized rounding procedure that
outputs the sequence of periods, in which orders are placed. Given the output of the ﬁrst
phase, demands are assigned to orders by solving the induced transportation problem,
and this minimizes the resulting holding costs. The main challenge in the worst-case
analysis is to show that the ﬁrst phase of the algorithm opens capacity that is sufﬁ-
cient to serve all of the demands, and that the resulting solution is of low cost. This is
done by exploiting the structure of the ﬂow-cover inequalities. In particular, we show
that together with the scaling at the ﬁrst phase of the algorithm, they guarantee that the
resulting transportation problem has a low cost feasible solution. This provides a ran-
domized 2-Approximation algorithm. The randomized procedure can be derandomized
to provide a deterministic 2-Approximation algorithm. As a by-product, we obtain the
ﬁrst theoretical proof of the strength of ﬂow-cover inequalities in capacitated inven-
tory models. (As already mentioned, all previous results are restricted to ﬁxed-charge
single-node problems, see Section 2 below.)
Finally, the insights from the worst-case analysis are used to construct an on-the-ﬂy
variant of the algorithm. Instead of solving the LP a-priori with all the correspond-
ing ﬂow-cover inequalities, we propose an iterative procedure. In each iteration, a well
designed rounding procedure is applied to the optimal fractional solution of the LP
relaxation. If this procedure comes to an end successfully, it can be shown that the
resulting integral solution is feasible and has cost that is at most twice the optimal
cost. On the other hand, if the procedure is terminated in the middle, it is guaran-
teed to identify a violated ﬂow-cover inequality. The corresponding inequality is added
to the LP, which is then solved again. The on-the-ﬂy algorithm can be viewed as

458
R. Levi, A. Lodi, and M. Sviridenko
running the Ellipsoid method until termination or until the ﬁrst time the rounding proce-
dure is ‘stuck’, whereas then we are guaranteed to have a good feasible integral solution.
We believe that the on-the-ﬂy algorithm might be computationally more efﬁcient, since
it does not require solving the LP a-priori with all the ﬂow-cover inequalities. (This
algorithmic approach is similar in spirit to what is discussed in Carr at al. [8] in the
context of a single-node ﬁxed charge problem.)
The rest of the paper is organized as follows. In Section 2, we describe the LP re-
laxation and discuss the ﬂow-cover inequalities. In Section 3 we describe the rounding
algorithms and the worst-case analysis.
2
A Flow-Cover-Inequality-Based LP Relaxation
A natural Mixed Integer linear Programming (MIP) formulation of the multi-item ca-
pacitated lot-sizing problem can be obtained by using two sets of variables:
– For each s = 1, . . . , T, let ys be a binary variable that is equal to 1 if an order is
placed in period s and 0 otherwise.
– For each i = 1, . . . , N, t = 1, . . . , T and s = 1, . . . , t, let xi
st be the fraction of the
demand dit satisﬁed by an order placed in period s.
– For each i = 1, . . . , N, t = 1, . . . , T and s = 1, . . . , t, let Hi
st = hi
stdit be the cost
of holding the all demand dit if ordered in period s.
The corresponding MIP formulation is as follows:
min
T

s=1
Ksys
+
N

i=1
T

s=1
T

t=s
Hi
stxi
st
(1)

s≤t
xi
st =
1
i = 1, . . . , N, t = 1, . . . , T, dit > 0,
(2)
xi
st ≤
ys
i = 1, . . . , N, s = 1, . . . , T, t ≥s,
(3)
N

i=1

t≥s
ditxi
st ≤Cys
s = 1, . . . , T,
(4)
xi
st ≥
0
i = 1, . . . , N,
(5)
ys ∈{0, 1}
s = 1, . . . , T.
(6)
If we relax the integrality constrains to 0 ≤ys ≤1, we get an LP relaxation that
provides a lower bound on the cost of the optimal solution. However, this LP relaxation
is weak in that the gap between its optimal value and the value of the optimal integral
solution can be arbitrarily high. Thus, there is no hope to use the LP to construct con-
stant approximation algorithms. For example, consider an instance with a single-item
and 2 periods, no holding costs, ﬁxed ordering costs K1 = 0 and K2 = 1, and demands
d1 = 0 and d2 = C + 1. The optimal policy must open two orders incurring a cost of 1.
The optimal fractional solution can achieve a cost of 1/C by setting y1 = 1, ys = 1/C,
x12 = C/(C + 1) and x22 = 1/(C + 1).

Approximation Algorithms for the Multi-item Capacitated Lot-Sizing Problem
459
2.1
Flow-Cover Inequalities
In this section, we introduce the class of ﬂow-cover inequalities that we use to
strengthen the LP induced by (1)-(6). Flow-cover inequalities have been introduced
by Padberg, Van Roy and Wolsey [16] over two decades ago in the context of the ﬁxed
charge single-node problem. In this problem there is a single-node of demand D and
a collection of T capacitated arcs. The goal is to open arcs and send a ﬂow of D units
to the demand node. Opening arc s incurs a ﬁxed cost Ks, and sending ﬂow over arc s
incurs a per-unit cost hs, for each unit of ﬂow. Padberg, Van Roy and Wolsey [16] have
used ﬂow-cover inequalities to construct an extended LP for this problem with uniform
arc capacities. They have also shown that these ﬂow-cover inequalities can be sepa-
rated in polynomial time. Carr et al. [8] have shown that another variant of ﬂow-cover
inequalities can be used to construct an LP relaxation for the ﬁxed-charge single-node
problem with nonuniform capacities, whose optimal solution can be rounded to a feasi-
ble solution with cost that is at most twice the optimal cost. Carnes and Shmoys [7] have
used the same LP to construct a prima-dual algorithm with the same worst-case perfor-
mance guarantee. Aardal, Pochet and Wolsey [2] have used aggregation of constrains
to leverage the ﬂow-cover inequalities to multi-location problems, speciﬁcally, hard
capacitated facility location problems. They have reported that ﬂow-cover inequalities
seem to be effective in narrowing the integrality gap and enhance integer programming
solution procedures. However, to the best of our knowledge there has been no theo-
retical analysis regarding the strength of ﬂow-cover inequalities in facility location or
inventory models.
In the spirit of [2], we next introduce ﬂow-cover inequalities for the multi-item ca-
pacitated lot-sizing problem. Given a subset A of demand points, i.e., a collections of
pairs (i, t), i = 1, . . . , N, t = 1, . . . , T, let D(A) = 
(i,t)∈A dit denote the cumu-
lative demand of the set A; ℓA = ⌈D(A)
C
⌉be the cover number of A, i.e., the min-
imum number of orders required to satisfy the demands in A; λA = ℓAC −D(A);
RA = C −λA be the residual capacity of A, i.e., the capacity required to satisfy the
demands in A after ℓA −1 orders are fully used; and rA = RA/C(= D(A)
C
−⌊D(A)
C
⌋)
be the fraction of the residual capacity. Observe that by deﬁnition 0 < RA ≤C and
0 < rA ≤1. Moreover, a subset F of orders (i.e., F ⊆{1, . . . , T}) is called a cover of
A if |F| ≥ℓA.
Then, we claim that the following inequalities are valid:

(i,t)∈A

s∈F
ditxi
st −RA

s∈F
ys ≤D(A) −ℓARA.
(7)
The validity of inequalities (7) in the multi-item capacitated lot-sizing problem can
be obtained as a special case of the general mixed integer rounding inequalities, or in
short MIR inequalities (see, e.g., Nemhauser and Wolsey [15]). An MIR inequality is
deﬁned with respect to the simple mixed-integer set Q = {x ∈R, y ∈Z : x + y ≥
b, x ≥0}, for which it is easy to prove the validity of the inequality x + ˆby ≥
ˆb⌈b⌉, where ⌈b⌉is equal to b rounded up to the next integer, and ˆb = ⌈b⌉−b. This
can be generalized to more complicated sets that involve more variables, as long as

460
R. Levi, A. Lodi, and M. Sviridenko
the variables can be split into an integral part and a continuous nonnegative part. In
particular, we apply an MIR derivation to:
1
C

(i,t)∈A

s̸∈F
ditxi
st +

s∈F
ys ≥D(A)
C
.
(8)
It is easy to see that Inequality (8) is valid for the system (1)–(6). Speciﬁcally, Constraint
(2) implies that D(A) = 
(i,t)∈A

s∈F ditxi
st + 
(i,t)∈A

s̸∈F ditxi
st; then replace
the ﬁrst term in the right hand side of the equality by an upper bound C 
s∈F ys (see
Constraint (4)) and divide by C to get the desired Inequality (8).
Thus, by applying an MIR derivation to (8) one obtains:
1
C

(i,t)∈A

s̸∈F
ditxi
st + rA

s∈F
ys ≥rAℓA,
(9)
which coincides with (7) after splitting D(A) as done before and dividing by C.
Separation. To the best of our knowledge the complexity of separating ﬂow-cover
inequalities is unknown. Aardal [1] has shown that ﬂow-cover inequalities can be sep-
arated in polynomial time for a ﬁxed set of demand points. (There is a simple greedy
procedure.)
Next we consider a ﬁxed subset of orders ¯F ⊆{1, . . ., T}, and describe a polyno-
mial time algorithm to separate ﬂow-cover inequalities that correspond to the subset of
orders ¯F. For the description of the algorithms, it will be useful to rewrite ﬂow-cover
inequalities that correspond to the set ¯F as

(i,t)∈A
dit
C (1 −

s∈¯
F
xi
st) ≥rA(ℓA −

s∈¯
F
ys).
(10)
Observe that (10) above may still contain exponentially many constrains, one for each
subset A of demand points that can be covered by ¯F. However, this is similar to the
residual capacity inequalities introduced by Magnanti, Marchandani and Vachani [14]
for the mixed-integer set called splittable ﬂow arc set X = {(x, y) :
n
i=1 aixi ≤
a0 + y, x ∈[0, 1]n, y ∈{0, 1}}. It has been shown that residual capacity inequalities
are sufﬁcient to characterize conv(X) [14,5]. Moreover, Atamt¨urk and Rajan [5] have
described an O(n) time separation algorithm .
Building on the results of Atamt¨urk and Rajan [5], we can obtain the following the-
orem. (The proof is omitted due to lack of space.)
Theorem 1. Consider a subset of orders ¯F. Then there exists a polynomial time sepa-
ration algorithm for the inequalities in (10). The algorithm runs in O(NT 2) time.
An LP. Next we describe an LP based on (1)-(5), relaxation of the integrality constraint
of (6) and a subset of the ﬂow-cover inequalities deﬁned in (7) above.
Let F := {[s, t] :
1 ≤s ≤t ≤T } be the collection of all subsets of orders
deﬁned by intervals [s, t]. Consider the LP deﬁned by (1)-(5), the relaxation of (6) and

Approximation Algorithms for the Multi-item Capacitated Lot-Sizing Problem
461
only the inequalities in (7) that correspond to subsets of orders F ∈F. Recall that in
Theorem 1 we have shown that ﬂow-cover inequalities that correspond to a ﬁxed subset
of orders can be separated in polynomial time. Since the cardinality of the set F is
O(T 2), it follows that the above LP can be solved to optimality in polynomial time by
using the Ellipsoid method. Let (ˆx, ˆy) be the optimal solution of that LP and VLP be
the respective optimal value. Since the LP is a relaxation of the problem, it follows that
VLP is a lower bound on the optimal cost denoted by VOP T .
3
The Random-Shift Algorithm with Median Demand Assignment
In this section, we describe an approximation algorithm for the multi-item capacitated
lot-sizing problem with hard capacities that is based on the linear programming relax-
ation deﬁned above by (1)-(5), relaxation of the integrality constraint of (6) and the
ﬂow-cover inequalities in (7) that correspond to the collection of subsets F deﬁned
above.
We shall ﬁrst show how to round the optimal fractional solution (ˆx, ˆy) of this LP to a
feasible integer solution with cost that is at most twice VLP . Since VLP is a lower bound
on the optimal cost, this implies that the algorithm is a 2-Approximation. In addition,
we shall describe an on-the-ﬂy variant of the algorithm that does not require to add
all the respective ﬂow-cover inequalities a-priori, but instead adds violated constrains
on-the-ﬂy until a (good) integer solution is obtained.
First, we present a randomized rounding procedure that we call Random-Shift with
Median Assignment. This procedure rounds the fractional optimal solution (ˆx, ˆy) to
a feasible integer solution (˜x, ˜y) with expected cost that is at most twice the optimal
cost VOP T . We then discuss how to derandomize the algorithm, and get a deterministic
2−Approximation algorithm.
The rounding algorithm runs in two phases. In the ﬁrst phase of the algorithms we
determine in which periods to place orders. Based on the outcome of the ﬁrst phase of
the algorithm, we decide how to assign demand points to orders.
3.1
Phase I: The Random-Shift Procedure
We ﬁrst describe Phase I of the algorithm which we call the Random-Shift procedure.
(This is similar in spirit to the work of Levi, Roundy, Shmoys and Sviridenko [13] on the
single-warehouse and multi-retailer problem.) In this phase we decide, in which periods
to place orders. This simple randomized procedure is based on the values ˆy1, . . . , ˆyT .
For each s = 1, . . . , T, let ¯ys = min{2ˆys, 1}, i.e., we double the original value of each
variable ˆys ≤0.5 and make it equal to 1 if ˆys > 0.5. We call ˆys and ¯ys the fractional
order and scaled fractional order in period s = 1, . . . , T, respectively. Next we shall
use the values ¯y1, . . . , ¯yT to determine the periods in which orders are placed.
For the description of the Random-Shift procedure,consider the interval (0,T
s=1¯ys],
which corresponds to the total weight of scaled fractional orders. Each period r =
1, . . . , T is then associated with the corresponding interval (r−1
s=1 ¯ys, r
s=1 ¯ys], which
is of length ¯yr. In particular, some periods can correspond to empty intervals of length
0 (if ˆyt = ¯yt = 0). The input for this procedure is a shift-parameter α that is chosen

462
R. Levi, A. Lodi, and M. Sviridenko
uniformly at random in (0, 1]. Let W be the smallest integer that is greater than or equal
to T
s=1 ¯ys. Speciﬁcally, W is the upper ceiling of the total cumulative weight of the
scaled fractional orders; that is, W = ⌈T
s=1 ¯ys⌉. Note that the interval (0, T
s=1 ¯ys]
is contained in the interval [0, W]. Within the interval [0, W] focus on the sequence of
points 0, 1, . . . , W −1. The shift-parameter α induces a sequence of what we call shift-
points. Speciﬁcally, the set of shift-points is deﬁned as {α + w : w = 0, . . . , W −1}.
This set is constructed through a shift of random length α to the right of the points
0, 1, . . ., W −1. Thus, there are W shift-points that are all located within the interval
[0, W]. Observe that the sequence of shift-points is a-priori random and is realized with
the shift-parameter α.
The shift-points determine the periods, in which orders are placed. For each period
r = 1, . . . , T, we place an order in that period if there is a shift-point within the interval
(r−1
s=1 ¯ys, r
s=1 ¯ys] that is associated with period r. That is, we place an order in period
r, if for some integer 0 ≤w ≤W −1 there exists a shift- point α + w that falls within
the interval (r−1
s=1 ¯ys, r
s=1 ¯ys]. Let T := {r1 < r2 < ... < rQ} be the set of periods
of the orders as determined in the ﬁrst phase of the algorithm using the random-shift
procedure. We set ˜yrm = 1, for each m = 1, . . . , Q, and call r1, . . . , rQ the opened
orders.
Next we bound the expected ordering cost incurred by the random shift procedure.
(The proof is omitted due lack of space.)
Lemma 2. Consider the Random-Shift procedure described above. Then, for each pe-
riod r = 1, . . . , T, the probability to place an order in period r is at most ¯yr ≤2ˆyr.
Thus, the total expected ordering cost of the Random-Shift procedure, denoted by K
is at most twice the total ordering costs in the optimal LP solution. That is, K ≤
T
s=1 ¯ysKs ≤2 T
s=1 ˆysKs.
Given the opened orders r1, . . . , rQ, we can compute the cheapest assignments of de-
mand points to opened orders by solving the corresponding transportation problem. The
solution of the transportation problem will determine the values of ˜xi
st, for each (i, t)
and s ≤t. However, it is not clear a-priori that the induced transportation problem has
a feasible solution, and even if it has one, there is a question regarding the cost of this
solution. Next we shall show that the induced transportation problem indeed has a fea-
sible solution with cost denoted by H that is at most twice the holding cost incurred by
the optimal fractional solution (ˆx, ˆy). That is, the holding cost incurred by the algorithm
is H ≤2 N
i=1
T
t=1
t
s=1 Hi
stxi
st.
3.2
The Median Assignment
Next we describe a constructive procedure, called the Median Assignment, that assigns
all the demand points to the opened orders r1, . . . , rQ, and incurs an holding cost that
is at most twice the holding cost incurred by the optimal fractional solution (ˆx, ˆy). Ob-
serve that the optimal solution to the transportation problems induced by the opened
orders r1, . . . , rQ incurs even lower holding cost. To describe the procedure we intro-
duce the notion of ﬂow-requirements of demand point (i, t). Focus on a speciﬁc demand
point (i, t), and let s1 < s2 < · · · < sG be the fractional orders that fractionally serve

Approximation Algorithms for the Multi-item Capacitated Lot-Sizing Problem
463
this demand point in the optimal LP solution (ˆx, ˆy). In particular, ˆxi
sg,t > 0, for each
g = 1 . . . , G, and G
g=1 ˆxi
sg,t = 1. Let sM be the median order of (i, t), i.e., the latest
point in time such that at least half of the demand dit is satisﬁed from orders within
[sM, t]. That is, M = max{m : G
g=m ˆxi
sg,t ≥0.5}. For each g = 1, . . . , G, let zi
sg,t
be the ﬂow-requirement of (i, t) that is due sg. Speciﬁcally, for each g = 1, .., M −1,
we deﬁne zi
sg,t = 2ˆxi
sg,tdit; for g = M we deﬁne zi
sM,t = 2(M
q=1 ˆxi
sq,t −0.5)dit;
and for each g = M + 1, . . . , G, we deﬁne zi
sg,t = 0.
Note that the ﬂow-requirements deﬁned above do not necessarily provide a feasi-
ble assignment of demands to orders. Intuitively, we consider the median order that
splits the assignment of demand point (i, t) in the optimal fractional solution (ˆx, ˆy) into
two equal halves. We then ignore the upper (later) half and scale the lower (earlier)
half by 2. However, we shall use the ﬂow-requirements zi
s1,t, . . . , zi
sG,t to construct a
feasible assignment of demands with relatively low holding costs. First, observe that
G
g=1 zi
sg,t = dit. We wish to construct a feasible assignment that, for each demand
point (i, t) and an order sg, satisﬁes at least zi
sg,t units of dit from orders within the in-
terval [sg, t]. That is, the ﬂow-requirement zi
sg,t is satisﬁed either from sg or from orders
later in time. We will say that such an assignment satisﬁes all the ﬂow-requirements.
(Recall that sg and zi
sg,t are speciﬁc to demand point (i, t) based on the optimal frac-
tional solution (ˆx, ˆy).)
Consider any assignment of demands that satisﬁes all the ﬂow-requirements of all
demands. Since the assignment satisﬁes zi
sg,t units of dit either from sg or even from
orders later in time, we conclude that the holding cost incurred by each demand point
(i, t) is at most G
g=1 zi
sg,thi
sg,t. However, by the deﬁnition of the ﬂow-requirements,
we have
G

g=1
zi
sg,thi
sg,t ≤2
G

g=1
ˆxi
sg,tdithi
sg,t = 2
G

g=1
ˆxi
sg,tHi
sg,t.
That is, the holding cost incurred is at most twice the holding costs incurred by (i, t) in
(ˆx, ˆy). In light of Lemma 2 above, if such an assignment exists, the resulting feasible
solution (˜x, ˜y) has cost that is at most twice the optimal values of the LP VLP . Since
VLP is a lower bound on the optimal cost, it follows that the cost of the solution is at
most twice the optimal cost. It is then left to show that such an assignment does exist.
Next we shall describe the details of the Median Assignment procedure.
We construct the Median Assignment in stages indexed by τ = T, . . . , 1. In each
stage τ, we consider the set of positive ﬂow-requirements due within τ, i.e., the set
Bτ = {zi
τt > 0 :
i = 1, . . . , N, t = τ, . . . , T}. These are the ﬂow-requirements
that need to be satisﬁed from orders within [τ, T ]. Partition the set Bτ into sets Bτt, for
t = τ, . . . , T, where Bτt = {zi
τt > 0 : i = 1, . . . , N}. We then consider the sets Bτt in
decreasing order t = T, . . . , τ. For each ﬂow-requirement zi
τt ∈Bτt, we consider the
opened orders (decided upon in Phase I) within [τ, t] in decreasing order from latest to
earliest. The ﬂow-requirement zi
τt is then assigned to these orders greedily according to
the current available capacity. More rigorously, consider a speciﬁc ﬂow-requirement zi
τt
and let T[τ,t] = T ∩[τ, t] = {e1 < e2 < · · · < eV } be the set of opened orders within
the interval [τ, t]. Let δV = min{zi
τt, (C −N
j=1
T
u=eV ˜xj
eV ,udju)+}, and for each

464
R. Levi, A. Lodi, and M. Sviridenko
v = V −1, . . . , 1, let δv = min{(zi
τt−V
q=v+1 δq)+, (C−N
j=1
T
u=ev ˜xj
ev,udju)+}.
(Recall that (x)+ = max{x, 0}.) We then update ˜xi
ev,t = ˜xi
ev,t + δv/dit, for each
v = 1, . . . , V . By construction it follows that if completed successfully, the Median
Assignment described above satisﬁes the ﬂow-requirements of all demand points. Thus,
to establish a bound on the holding cost incurred by the algorithm, it is sufﬁcient to show
that the Median Assignment can be completed successfully.
Before we prove that, we would like to state a technical lemma that draws a connec-
tion between the cumulative fractional orders opened by the fractional solution (ˆx, ˆy),
and the corresponding number of integral orders opened in Phase I of the algorithm.
(The proof is omitted due lack of space.)
Lemma 3. Consider the interval [s, t] for some s ≤t, and suppose that the cumulative
fractional orders opened by the LP optimal solution (ˆx, ˆy) is equal L + β, where L is
a non-negative integer and β is between 0 and 1. That is , t
u=s ˆyu = L + β. Then if
β ≥0.5, the number of orders placed in Phase I of the algorithm over the interval [s, t]
is at least L + 1. That is, t
u=s ˜yu ≥L + 1.
Lemma 4. The Median Assignment can be completed successfully.
Proof : Assume by contradiction that the Median Assignment cannot be completed at
some stage τ due lack of capacity to satisfy the ﬂow-requirement zi
τ,¯t of some demand
point (i, ¯t). It follows that all the opened orders within the interval [τ, ¯t] are currently
fully used by the integer partial solution (˜x, ˜y). That is, for each r ∈T ∩[τ, ¯t], we have
N
i=1
T
u=r ˜xi
rudiu = C. Now let ¯r be the earliest opened order within (¯t, T ] ∩T that
still has free capacity or T + 1 if no such order exists. That is,
¯r = min{argmin{r ∈T ∩(¯t, T ] :
N

i=1
T

u=r
˜xi
rudiu < C}, T + 1}.
Let F = [τ, ¯r) be the corresponding interval of orders.
Next we focus on the set of demand points (i, u) with positive ﬂow-requirements that
are due within [τ, ¯r), i.e., the set A = {(i, t) : t ∈[τ, ¯r) and t
s=τ zi
st > 0}. Using
the notation in Section 2 we write D(A) = 
(i,t)∈A dit = (ℓA −1)C + RA, where
ℓA ≥1 is an integer and 0 < RA ≤C. Consider again the integer partial solution (˜x, ˜y)
at the moment the Median Assignment terminated due to lack of capacity. Recall that
¯r = T + 1 or ¯r ∈T is an opened order with free capacity. By the construction of the
Median Assignment it follows that no demand point outside the interval [τ, ¯r) is being
served by the partial solution (˜x, ˜y) from orders within the interval. That is, ˜xi
st = 0 for
each (i, t) with t ≥¯r and s ∈[τ, ¯r). This implies that all the available capacity of the
opened orders within the interval [τ, ¯r) is fully used by the integer partial solution (˜x, ˜y)
to serve demand points in A. Moreover, since the Median Assignment could have not
been completed, it follows that the ﬂow-requirements of demand points in A that are
due within the interval F exceed the total opened capacity over F. That is,

(i,t)∈A

u∈F
zi
ut >

u∈F
˜yuC.
(11)

Approximation Algorithms for the Multi-item Capacitated Lot-Sizing Problem
465
Now consider the set of fractional orders in the optimal LP solution (ˆx, ˆy) over F.
Let 
u∈F ˆyuC = (L−1)C+R, where L ≥1 is a nonnegative integer and 0 < R ≤C.
The rest of the proof is based on comparing ℓA and RA to L and R, respectively, and
deriving a contradiction.
We ﬁrst claim that L ≤ℓA. We have already seen in the proof of Lemma 3 that if

u∈F ˆyu ≥L −1, then the Random-Shift procedure will open at least L −1 orders
over the interval F, i.e., 
u∈F ˜yuC ≥(L −1)C. However, Inequality (11) implies
that 
(i,t)∈A

u∈F zi
ut > ˜yuC ≥(L −1)C. Finally, observe that the overall ﬂow-
requirements of demand points in A cannot exceed D(A), which is at most ℓAC. The
claim then follows.
Next we claim that R/C < 0.5. Assume otherwise. It follows that 
u∈F ˆyu ≥
(L−1)+0.5, and by Lemma 3 we conclude that there are at least L opened orders over
F. That is, 
u∈F ˜yuC ≥LC. However, the ﬂow-requirements are always bounded by
the original ﬂow in the fractional optimal solution (ˆx, ˆy). That is,

u∈F

(i,t)∈A
zi
ut ≤

u∈F

(i,t)∈A
ˆxi
utdit ≤

u∈F
ˆyuC = (L −1)C + R,
where the last inequality follows from (4). It follows that capacity of LC units is suf-
ﬁcient to satisfy all the ﬂow-requirements that are due within F of all demand points
in A, which leads to contradiction. (There are at least L opened orders over F, all of
which are used to satisfy ﬂow-requirement of demand points in A.)
Next we claim that L = ℓA. Assume otherwise, i.e., ℓA > L. Since each demand
point (i, t) ∈A has positive ﬂow-requirements over F, it follows that 
u∈F ˆxi
ut > 0.5.
However, by the construction of the ﬂow-requirements this implies that its total ﬂow-
requirements over F can be expressed as

u∈F
zi
ut = 2dit(

u∈F
ˆxi
ut −0.5).
Thus, the total ﬂow-requirements of demand points in A over F can be expressed as

(i,t)∈A

u∈F
zi
ut = 2(

(i,t)∈A

u∈F
ˆxi
utdit −0.5D(A))
≤2(

u∈F
ˆyuC −0.5D(A)) = 2(L −1)C + 2R −(ℓA −1)C −RA
≤(L −1)C.
The last inequality follows from the assumptions that ℓA −1 ≥L and that 2R < C.
Moreover, this implies that capacity of (L−1)C units is sufﬁcient to satisfy all the ﬂow-
requirements that are due within F of the demand points (i, t) ∈A. However, we have
already seen that there are at least L −1 opened orders over F, i.e., 
u∈F ˜yu ≥L −1.
Since all of them are fully used to satisfy ﬂow-requirements of demand points in A, this
again leads to contradiction.
Suppose now that R/C < 0.5 and ℓA = L. This implies that the set of orders F is a
cover of the set of demands A. Moreover, F ∈F, which implies that the solution (ˆx, ˆy)

466
R. Levi, A. Lodi, and M. Sviridenko
satisﬁes the ﬂow-cover inequality that corresponds to F and A. It follows that
D(A) −

(i,t)∈A

u∈F
ˆxi
utdit ≥RA(ℓA −

u∈F
ˆyu) = RA (ℓA −(L −1) −R/C)
= RA(1 −R/C) ≥0.5RA.
The ﬁrst inequality follows from the ﬂow-cover inequality with respect to F and A. The
ﬁrst equality follows from the fact that 
u∈F ˆyu = L−1+R/C.The last inequality fol-
lows from the fact that R/C < 0.5. We conclude that 2(D(A)−
(i,t)∈A

u∈F ˆxi
utdit)
≥RA. However, D(A) −
(i,t)∈A

u∈F ˆxi
utdit is exactly the portion of D(A) that
is being served in the optimal fractional solution (ˆx, ˆy) from outside F. Moreover, we
have already seen that, for each demand point (i, t) ∈A, more than half of the demand
dit is served by (ˆx, ˆy) from within F, i.e., 
u∈F ˆxi
ut > 0.5. By the construction of
the ﬂow-requirements this implies that the total ﬂow-requirements of demand points
(i, t) ∈A that are due outside F is exactly 2(D(A) −
(i,t)∈A

u∈F ˆxi
utdit) ≥RA.
In turn, this implies that the total ﬂow-requirements that are due within F is at most
(ℓA −1)C = (L −1)C. However, as we have already seen, this leads to contradiction
since there are at least (L −1) opened orders over F. We conclude that the Median
Assignment can be completed successfully.
Corollary 5. The overall holding cost incurred by the algorithm is at most
2 N
i=1
T
t=1
t
s=1 ˆxi
stHi
st.
Lemma 2 and Corollary 5 imply the following theorem.
Theorem 6. The Random-Shift algorithm is a randomized 2-Approximation algorithm
for the multi-item capacitated lot-sizing problem with hard capacities.
We note that the same analysis holds in the presence of soft capacities. Finally, we
describe how to derandomize the algorithms and get a deterministic 2-Approximation
algorithm. We have already mentioned that once the periods, in which orders are placed
are determined, the problem is reduced to solving a transportation problem that mini-
mizes the holding costs. The worst-case analysis implies that for any outcome of Phase
I, the induced transportation problem has a low-cost feasible solution with cost that is
at most twice the holding costs incurred by the optimal fractional solution. Thus, it is
sufﬁcient to derandomize Phase I, and this can be done by enumerating over all the
values of α that yield a different set of orders in Phase I. (There are only O (T ) such
values.)
Theorem 7. There exists a deterministic 2-Approximation algorithm for the multi-item
capacitated lot-sizing problem with hard capacity constrains.
3.3
On-The-Fly Algorithm
In this section, we shall describe an on-the-ﬂy variant of the algorithm described above.
The underlying idea is similar to what discussed by Carr et al. [8] in the context of the
ﬁxed-charge single-node problem.

Approximation Algorithms for the Multi-item Capacitated Lot-Sizing Problem
467
This variant does not require to solve the LP a-priori with all the ﬂow-cover inequali-
ties deﬁned by the collection of subsets F. Instead, we shall have an iterative procedure
that is based on an oracle that, in each iteration, either ﬁnds a violated ﬂow-cover in-
equality or generates a feasible solution with cost that is at most twice the optimal cost.
Having an efﬁcient oracle that can separate the respective ﬂow-cover inequalities
enables us to run the Ellipsoid method and solve the corresponding LP, and then use
the rounding algorithm described in Sections 3.1 and 3.2 above. The resulting integer
solution has cost that is at most twice the optimal cost.
However, instead of using an oracle that can separate all the corresponding ﬂow-
cover inequalities, the on-the-ﬂy algorithm will use the Median Assignment procedure.
If the Median Assignment procedure is stuck, then we can easily identify a violated
ﬂow-cover inequality that corresponds to the set of demand points A and the set of
orders F as deﬁned in Section 3.2 above. As long as this is the case we execute the
Ellipsoid method. On the other hand, if the Median Assignment procedure is completed
successfully and the Ellipsoid method is stuck, then we have constructed a solution with
cost that is at most twice the optimal cost. The main observation is that the our rounding
algorithm can be applied to any feasible fractional solution, where it either ends up with
a violated ﬂow-cover inequality or in turn, provides a feasible integer solution with cost
that is at most twice the cost of the fractional solution.
We note that in practice the on-the-ﬂy algorithm can be implemented using the Sim-
plex method. Since in each iteration we add a constraint to the primal LP, the Dual-
Simplex method might be very attractive to ﬁnd the new optimal solution of the LP.
Acknowledgements
Part of this research has been carried out when the ﬁrst and the second authors were
Herman Goldstine Postdoctoral Fellows in the Department of Mathematical Sciences
of the IBM T.J. Watson Research Center, whose support is strongly acknowledged.
The second author was also supported in part by the EU project ADONET (contract n.
MRTN-CT-2003-504438). We warmly thank Oktay G¨unl¨uk who brought to our atten-
tion reference [5], and David Shmoys for numerous fruitful discussions.
References
1. K. Aardal. Capacitated facility location: separation algorithms and computational experi-
ence. Mathematical Programming, 81:149–175, 1998.
2. K. Aardal, Y. Pochet, and L. A. Wolsey. Capacitated facility location: valid inequalities and
facets. Mathematics of Operations Research, 20:562–582, 1995.
3. S. Anily and M. Tzur. Shipping multiple-items by capacitated vehicles - an optimal dynamic
programming approach. Transportation Science, 39:233–248, 2005.
4. S. Anily, M. Tzur, and L. A. Wolsey. Multi-item lot-sizing with joint set-up cost. Technical
Report 2005/70, CORE, 2005. Working paper.
5. A. Atamt¨urk and D. Rajan. On splittable and unsplittable ﬂow capacity network design
arc-set polyhedra. Mathematical Programming, 92:315–333, 2002.
6. G. R. Bitran and H. H. Yanasee.
Computational complexity of the capacitated lot-size
problem. Management Science, 28:1174–1186, 1982.

468
R. Levi, A. Lodi, and M. Sviridenko
7. T. Carnes and D. B. Shmoys. A primal-dual 2-approximation algorithm the single-demand
ﬁxed-charge minimum-cost ﬂow problem. Working paper, 2006.
8. R. D. Carr, L. K. Fleischer, V. J. Leung, and C. A. Phillips. Strengthening integrality gaps for
capacitated network design and covering problems. In Proceedings of the 11th ACM/SIAM
Symposium on Discrete Algorithms (SODA), 2000.
9. A. Federgruen and J. Meissner. Probabilistic analysis of multi-item capacitated lot sizing
problems. Working paper, 2004.
10. A. Federgruen, J. Meissner, and M. Tzur. Progressive interval heuristics for the multi-item
capacitated lot sizing problem. To appear in Operations Research, 2003.
11. M. Florian, J. K. Lenstra, and A. H. G. Rinooy Kan. Deterministic production planning:
Algorithms and complexity. Management Science, 26:669–679, 1980.
12. R. Levi, R. O. Roundy, and D. B. Shmoys. Primal-dual algorithms for deterministic inventory
problems. Mathematics of Operations Research, 31:267–284, 2006.
13. R. Levi, R. O. Roundy, D. B. Shmoys, and M. Sviridenko. First constant approximation
algorithm for the single-warehouse multi-retailer problem. Under revision in Management
Science, 2004.
14. T. L. Magnanti, P. Mirchandani, and R. Vachani. The convex hull of two core capacitated
network design problems. Mathematical Programming, 60:233–250, 1993.
15. G. Nemhauser and L. A. Wolsey. Integer Programming and Combinatorial Optimization.
Wiley, 1990.
16. M. W. Padberg, T. J. V. Roy, and L. A. Wolsey. Valid inequalities for ﬁxed charge problems.
Operations Research, 33:842–861, 1985.
17. Y. Pochet and L. A. Wolsey. Lot-sizing with constant batches: Formulation and valid in-
equalities. Mathematics of Operations Research, 18:767–785, 1993.
18. Y. Pochet and L. A. Wolsey. Production Planning by Mixed Integer Programming. Springer
Verlag, 2006.
19. C. P. M. van Hoesel and A. P. M. Wagelmans. Fully polynomial approximation schemes for
single-item capacitated economic lot-sizing problems. Mathematics of Operations Research,
26:339–357, 2001.

Optimal Eﬃciency Guarantees for Network
Design Mechanisms⋆
Tim Roughgarden⋆⋆and Mukund Sundararajan⋆⋆⋆
Department of Computer Science, Stanford University,
353 Serra Mall, Stanford, CA 94305
Abstract. A cost-sharing problem is deﬁned by a set of players vying
to receive some good or service, and a cost function describing the cost
incurred by the auctioneer as a function of the set of winners. A cost-
sharing mechanism is a protocol that decides which players win the auc-
tion and at what prices. Three desirable but provably mutually incompat-
ible properties of a cost-sharing mechanism are: incentive-compatibility,
meaning that players are motivated to bid their true private value for re-
ceiving the good; budget-balance, meaning that the mechanism recovers
its incurred cost with the prices charged; and eﬃciency, meaning that
the cost incurred and the value to the players served are traded oﬀin an
optimal way.
Our work is motivated by the following fundamental question: for
which cost-sharing problems are incentive-compatible mechanisms with
good approximate budget-balance and eﬃciency possible? We focus on
cost functions deﬁned implicitly by NP-hard combinatorial optimization
problems, including the metric uncapacitated facility location problem,
the Steiner tree problem, and rent-or-buy network design problems. For
facility location and rent-or-buy network design, we establish for the
ﬁrst time that approximate budget-balance and eﬃciency are simulta-
neously possible. For the Steiner tree problem, where such a guarantee
was previously known, we prove a new, optimal lower bound on the ap-
proximate eﬃciency achievable by the wide and natural class of “Moulin
mechanisms”. This lower bound exposes a latent approximation hierar-
chy among diﬀerent cost-sharing problems.
1
Introduction
Mechanism Design. In the past decade, there has been a proliferation of large sys-
tems used and operated by independent agents with competing objectives (most
⋆Preliminary versions of most of these results appear in a technical report [32].
⋆⋆Supported in part by ONR grant N00014-04-1-0725, an NSF CAREER Award,
and an Alfred P. Sloan Fellowship.
⋆⋆⋆Supported in part by OSD/ONR CIP/SW URI ”Software Quality and Infrastruc-
ture Protection for Diﬀuse Computing” through ONR Grant N00014-01-1-0795
and by OSD/ONR CIP/SW URI ”Trustworthy Infrastructure, Mechanisms, and
Experimentation for Diﬀuse Computing” through ONR Grant N00014-04-1-0725.
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 469–483, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

470
T. Roughgarden and M. Sundararajan
notably the Internet). Motivated by such applications, an increasing amount of al-
gorithm design research studies optimization problems that involve self-interested
entities. Naturally, game theory and economics are important for modeling and
solving such problems. Mechanism design is a classical area of microeconomics
that has been particularly inﬂuential. The ﬁeld of mechanism design studies how
to solve optimization problems in which part of the problem data is known only
to self-interested players. It has numerous applications to, for example, auction
design, pricing problems, and network protocol design [8,15,24,27].
Selling a single good to one of n potential buyers is a paradigmatic problem
in mechanism design. Each bidder i has a valuation vi, expressing its maximum
willingness to pay for the good. We assume that this value is known only to the
bidder, and not to the auctioneer. A mechanism (or auction) for selling a single
good is a protocol that determines the winner and the selling price. Each bidder i
is “selﬁsh” in the sense that it wants to maximize its “net gain” (vi −p)xi from
the auction, where p is the price, and xi is 1 (0) if the bidder wins (loses).
What optimization problem underlies a single-good auction? One natural goal
is economic eﬃciency, which in this context demands that the good is sold
to the bidder with the highest valuation. This goal is trivial to accomplish if
the valuations are known a priori. Can it be achieved when the valuations are
private?
Vickrey [34] provided an elegant solution. First, each player submits a sealed
bid bi to the seller, which is a proxy for its true valuation vi. Second, the seller
awards the good to the highest bidder. This achieves the eﬃcient allocation if
we can be sure that players bid their true valuations—if bi = vi for every i.
To encourage players to bid truthfully, we must charge the winner a non-zero
price. (Otherwise, all players will bid gargantuan amounts in an eﬀort to be the
highest.) On the other hand, if we charge the winning player its bid, it encour-
ages players to underbid. (Bidding your maximum willingness to pay ensures a
net gain of zero, win or lose.) Vickrey [34] suggested charging the winner the
value of the second-highest bid, and proved that this price transforms truthful
bidding into an optimal strategy for each bidder, independent of the bids of the
other players. In turn, the Vickrey auction is guaranteed to produce an eﬃcient
allocation of the good, provided all players bid in the obvious, optimal way.
Cost-Sharing Mechanisms. Economic eﬃciency is not the only important ob-
jective in mechanism design. Revenue is a second obvious concern, especially in
settings where the mechanism designer incurs a non-trivial cost. This cost can
represent production costs, or more generally some revenue target.
A cost-sharing problem is deﬁned by a set U of players vying to receive some
good or service, and a cost function C : 2U →R+ describing the cost incurred
by the mechanism as a function of the auction outcome—the set S of winners.
We assume that C(S) is nonnegative for every set S ⊆U, that C(∅) = 0, and
that C is nondecreasing (S ⊆T implies C(S) ≤C(T )). Note that there is no
explicit limit on the number of auction winners, although a large number of
winners might result in extremely large costs. With outcome-dependent costs,
the eﬃcient allocation is the one that maximizes the social welfare W(S) =

Optimal Eﬃciency Guarantees for Network Design Mechanisms
471

i∈S vi −C(S)—the outcome that trades oﬀs the valuations of the winners and
the cost incurred in an optimal way. The problem of selling a single good can
be viewed as the special case in which C(S) = 0 if |S| ≤1 and C(S) = +∞
otherwise.
In this paper, we focus on cost functions that are deﬁned implicitly by an in-
stance of a combinatorial optimization problem. For example, U could represent
a set of potential clients, located in an undirected graph with ﬁxed edge costs,
that want connectivity to a server r [7,17]. In this application, C(S) denotes the
cost of connecting the terminals in S to r—the cost of the minimum-cost Steiner
tree that spans S ∪{r}.
A cost-sharing mechanism, given a set U and a function C, is a protocol that
decides which players win the auction and at what prices. Typically, such a
mechanism is also (perhaps approximately) budget-balanced, meaning that the
cost incurred is passed on to the auction’s winners. Budget-balanced cost-sharing
mechanisms provide control over the revenue generated, relative to the cost in-
curred by the mechanism designer.
Summarizing, we have identiﬁed three natural goals in auction and mechanism
design: (1) incentive-compatibility, meaning that every player’s optimal strategy
is to bid its true private value vi for receiving the service; (2) budget-balance,
meaning that the mechanism recovers its incurred cost with the prices charged;
and (3) eﬃciency, meaning that the cost and valuations are traded oﬀin an
optimal way.
Unfortunately, properties (1)–(3) cannot be simultaneously achieved, even in
very simple settings [10,30]. This impossibility result motivates relaxing at least
one of the these properties. Until recently, nearly all work in cost-sharing mecha-
nism design completely ignored either budget-balance or eﬃciency. If the budget
balance constraint is discarded, then there is an extremely powerful and ﬂexi-
ble mechanism that is incentive-compatible and eﬃcient: the VCG mechanism
(see e.g. [26]). This mechanism specializes to the Vickrey auction in the case
of selling a single good, but is far more general. Since the VCG mechanism is
typically not approximately budget-balanced for any reasonable approximation
factor (see e.g. [6]), it is not suitable for many applications.
The second approach is to insist on incentive-compatibility and budget-
balance, while regarding eﬃciency as a secondary objective. The only general
technique for designing mechanisms of this type is due to Moulin [25]. Over
the past ﬁve years, researchers have developed approximately budget-balanced
Moulin mechanisms for cost-sharing problems arising from numerous diﬀerent
combinatorial optimization problems, including ﬁxed-tree multicast [1,6,7]; the
more general submodular cost-sharing problem [25,26]; Steiner tree [17,18,20];
Steiner forest [21,22]; facility location [23,29]; rent-or-buy network design [14,29],
and various covering problems [5,16]. Most of these mechanisms are based on
novel primal-dual approximation algorithms for the corresponding optimization
problem. With one exception discussed below, none of these works provided any
guarantees on the eﬃciency achieved by the proposed mechanisms.

472
T. Roughgarden and M. Sundararajan
Approximately Eﬃcient Cost-Sharing Mechanisms. Impossibility results are,
of course, common in optimization. From conditional impossibility results like
Cook’s Theorem to information-theoretic lower bounds in restricted models of
computation, as with online and streaming algorithms, algorithm designers are
accustomed to devising heuristics and proving worst-case guarantees about them
using approximation measures. This approach can be applied equally well to
cost-sharing mechanism design, and allows us to quantify the inevitable eﬃ-
ciency loss in incentive-compatible, budget-balanced cost-sharing mechanisms.
As worst-case approximation measures are rarely used in economics, this research
direction has only recently been pursued.
Moulin and Shenker [26] were the ﬁrst to propose quantifying the eﬃciency
loss in budget-balanced Moulin mechanisms. They studied an additive notion of
eﬃciency loss for submodular cost functions. This notion is useful for ranking
diﬀerent mechanisms according to their worst-case eﬃciency loss, but does not
imply bounds on the quality of a mechanism’s outcome relative to that of an
optimal outcome. A more recent paper [31] provides an analytical framework for
proving approximation guarantees on the eﬃciency attained by Moulin mech-
anisms. The present paper builds on this framework. (See [4,11] for other very
recent applications.)
Several deﬁnitions of approximate eﬃciency are possible, and the choice of
deﬁnition is important for quantifying the ineﬃciency of Moulin mechanisms.
Feigenbaum et al. [6] showed that, even for extremely simple cost functions,
budget-balance and social welfare cannot be simultaneously approximated to
within any non-trivial factor. This negative approximation result is characteristic
of mixed-sign objective functions such as welfare.
An alternative formulation of exact eﬃciency is to choose a subset mini-
mizing the social cost, where the social cost π(S) of a set S is the sum of
the incurred service cost and the excluded valuations: C(S) + 
i/∈S vi. Since
π(S) = −W(S)+
i∈U vi for every set S, where U denotes the set of all players,
a subset maximizes the social welfare if and only if it minimizes the social cost.
The two functions are not, of course, equivalent from the viewpoint of approxi-
mation. Similar transformations have been used for “prize-collecting” problems
in combinatorial optimization (see e.g. [3]). We call a cost-sharing mechanism
α-approximate if it always produces an outcome with social cost at most an α
factor times that of an optimal outcome. Also, a mechanism is β-budget-balanced
if the sum of prices charged is always at most the cost incurred and at least a
1/β fraction of this cost.
Previouswork[31]demonstratedthatO(polylog(k))-approximate,O(1)-budget
-balanced Moulin mechanisms exist for two important types of cost-sharing prob-
lems: submodular cost functions, and Steiner tree cost functions. (Here k denotes
thenumber ofplayers.)Thiswastheﬁrstevidencethatproperties(1)–(3)abovecan
be approximately simultaneously satisﬁed, and motivates the following fundamen-
tal question: which cost-sharing problems admit incentive-compatible mechanisms
that are approximately budget-balanced and approximately eﬃcient?

Optimal Eﬃciency Guarantees for Network Design Mechanisms
473
Our Results. This paper presents three contributions. We ﬁrst consider metric
uncapacitated facility location (UFL) cost-sharing problems, where the input
is a UFL instance, the players U are the demands of this instance, and the
cost C(S) is deﬁned as the cost of an optimal solution to the UFL sub-instance
induced by S. The only known O(1)-budget-balanced Moulin mechanism for this
problem is due to P´al and Tardos [29] (the PT mechanism). The PT mechanism
is 3-budget-balanced [29], and no Moulin mechanism for the problem has bet-
ter budget balance [16]. We provide the ﬁrst eﬃciency guarantee for the PT
mechanism by proving that it is O(log k)-approximate, where k is the number of
players. Simple examples show that every O(1)-budget-balanced Moulin mecha-
nism for UFL is Ω(log k)-approximate. Thus the PT mechanism simultaneously
optimizes both budget balance and eﬃciency over the class of Moulin mecha-
nisms for UFL.
Second, we design and analyze Moulin mechanisms for rent-or-buy network
design cost-sharing problems. For example, the single-sink rent-or-buy (SSRoB)
problem is a generalization of the Steiner tree problem in which several source
vertices of a network (corresponding to the players U) want to simultaneously
send one unit of ﬂow each to a common root vertex. For a subset S ⊆U of
players, the cost C(S) is deﬁned as the minimum-cost way of installing suﬃcient
capacity for the players of S to simultaneously send ﬂow to the root. Capacity on
an edge can be rented on a per-unit basis, or an inﬁnite amount of capacity can
be bought for M times the per-unit renting cost, where M ≥1 is a parameter.
(Steiner tree is the special case where M = 1.) Thus the SSRoB problem is a
simple model of capacity installation in which costs obey economies of scale. The
multicommodity rent-or-buy (MRoB) problem is the generalization of SSRoB in
which each player corresponds to a source-sink vertex pair, and diﬀerent players
can have diﬀerent sink vertices.
Gupta, Srinivasan, and Tardos [14] and Leonardi and Sch¨afer [23] indepen-
dently showed how to combine the SSRoB algorithm of [13] with the Jain-
Vazirani Steiner tree mechanism [17] to obtain an O(1)-budget-balanced SSRoB
mechanism. (Earlier, P´al and Tardos [29] designed an O(1)-budget-balanced SS-
RoB mechanism, but it was more complicated and its budget balance factor
was larger.) We note that the mechanism design ideas in [14,23], in conjunction
with the recent 2-budget-balanced Steiner forest mechanism due to K¨onemann,
Leonardi, and Sch¨afer [21], lead to an O(1)-budget-balanced MRoB mechanism.
Much more importantly, we prove that this SSRoB mechanism and a variant
of this MRoB mechanism are O(log2 k)-approximate, the ﬁrst eﬃciency guaran-
tees for any approximately budget-balanced mechanisms for these problems. Our
third result below implies that these are the best-achievable eﬃciency guarantees
for O(1)-budget-balanced Moulin mechanisms for these problems.
Third, we prove a new lower bound that exposes a non-trivial, latent hierarchy
among diﬀerent cost-sharing problems. Speciﬁcally, we prove that every O(1)-
budget-balanced Moulin mechanism for Steiner tree cost functions is Ω(log2 k)-
approximate. This lower bound trivially also applies to Steiner forest, SSRoB,
and MRoB cost functions.

474
T. Roughgarden and M. Sundararajan
This lower bound establishes a previously unobservable separation between
submodular and facility location cost-sharing problems on the one hand, and
the above network design cost-sharing problems on the other. All admit O(1)-
budget-balanced Moulin mechanisms, but the worst-case eﬃciency loss of Moulin
mechanisms is provably larger in the second class of problems than in the ﬁrst
one.
All previous lower bounds on the eﬃciency of Moulin mechanisms were derived
from either budget-balance lower bounds or, as for the problems considered in
this paper, from a trivial example equivalent to a cost-sharing problem in a
single-link network [31]. This type of example cannot prove a lower bound larger
than the kth Harmonic number Hk = Θ(log k) on the approximate eﬃciency of a
Moulin mechanism. We obtain the stronger bound of Ω(log2 k) by a signiﬁcantly
more intricate construction that exploits the complexity of Steiner tree cost
functions.
2
Preliminaries
Cost-Sharing Mechanisms. We consider a cost function C that assigns a cost
C(S) to every subset S of a universe U of players. We assume that C is nonneg-
ative and nondecreasing (i.e., S ⊆T implies C(S) ≤C(T )). We sometimes refer
to C(S) as the service cost, to distinguish it from the social cost (deﬁned below).
We also assume that every player i ∈U has a private, nonnegative valuation vi.
A mechanism collects a nonnegative bid bi from each player i ∈U, selects a set
S ⊆U of players, and charges every player i a price pi. In this paper, we focus
on cost functions that are deﬁned implicitly as the optimal solution of an instance
of a (NP-hard) combinatorial optimization problem. The mechanisms we consider
also produce a feasible solution to the optimization problem induced by the served
set S, which has cost C′(S) that in general is larger than the optimal cost C(S).
We also impose the following standard restrictions and assumptions. We only
allow mechanisms that are “individually rational” in the sense that pi = 0 for
players i /∈S and pi ≤bi for players i ∈S. We require that all prices are nonneg-
ative (“no positive transfers”). Finally, we assume that players have quasilinear
utilities, meaning that each player i aims to maximize ui(S, pi) = vixi−pi, where
xi = 1 if i ∈S and xi = 0 if i /∈S.
Our incentive-compatibility constraint is the well-known strategyproofness
condition, which intuitively requires that a player cannot gain from misreporting
its bid. Formally, a mechanism is strategyproof (SP) if for every player i, every bid
vector b with bi = vi, and every bid vector b′ with bj = b′
j for all j ̸= i, ui(S, pi) ≥
ui(S′, p′
i), where (S, p) and (S′, p′) denote the outputs of the mechanism for the
bid vectors b and b′, respectively.
For a parameter β ≥1, a mechanism is β-budget balanced if C′(S)/β ≤

i∈S pi ≤C(S) for every outcome (set S, prices p, feasible solution with service
cost C′(S)) of the mechanism. In particular, this requirement implies that the
feasible solution produced by the mechanism has cost at most β times that of
optimal.

Optimal Eﬃciency Guarantees for Network Design Mechanisms
475
As discussed in the Introduction, a cost-sharing mechanism is α-approximate
if, assuming truthful bids, it always produces a solution with social cost at most
an α factor times that of an optimal solution. Here, the social cost incurred by
the mechanism is deﬁned as the service cost C′(S) of the feasible solution it pro-
duces for the instance corresponding to S, plus the sum 
i/∈S vi of the excluded
valuations. The optimal social cost is minS⊆U[C(S) + 
i/∈S vi]. A mechanism
thus has two sources of ineﬃciency: ﬁrst, it might choose a suboptimal set S of
players to serve; second, it might produce a suboptimal solution to the optimiza-
tion problem induced by S.
Moulin Mechanisms and Cross-Monotonic Cost-Sharing Methods. Next we re-
view Moulin mechanisms, the preeminent class of SP, approximately budget-
balanced mechanisms. Such mechanisms are based on cost sharing methods, de-
ﬁned next.
A cost-sharing method χ is a function that assigns a non-negative cost share
χ(i, S) for every subset S ⊆U of players and every player i ∈S. We consider
cost-sharing methods that, given a set S, produce both the cost shares χ(i, S) for
all i ∈S and also a feasible solution for the optimization problem induced by S.
A cost-sharing method is β-budget balanced for a cost function C and a parameter
β ≥1 if it always recovers a 1/β fraction of the cost: C′(S)/β ≤
i∈S χ(i, S) ≤
C(S), where C′(S) is the cost of the produced feasible solution. A cost-sharing
method is cross-monotonic if the cost share of a player only increases as other
players are removed: for all S ⊆T ⊆U and i ∈S, χ(i, S) ≥χ(i, T).
A cost-sharing method χ for C deﬁnes the following Moulin mechanism Mχ
for C. First, collect a bid bi for each player i. Initialize the set S to all of U and
invoke the cost-sharing method χ to deﬁne a feasible solution to the optimization
problem induced by S and a price pi = χ(i, S) for each player i ∈S. If pi ≤bi
for all i ∈S, then halt, output the set S, the corresponding feasible solution, and
charge prices p. If pi > bi for some player i ∈S, then remove an arbitrary such
player from the set S and iterate. A Moulin mechanism based on a cost-sharing
method thus simulates an iterative auction, with the method χ suggesting prices
for the remaining players at each iteration. The cross-monotonicity constraint
ensures that the simulated auction is ascending, in the sense that the prices
that are compared to a player’s bid are only increasing with time. Note that if
χ produces a feasible solution in polynomial time, then so does Mχ. Also, Mχ
clearly inherits the budget-balance factor of χ. Finally, Moulin [25] proved the
following.
Theorem 1 ([25]). If χ is a cross-monotonic cost-sharing method, then the
corresponding Moulin mechanism Mχ is strategyproof.1
Theorem 1 reduces the problem of designing an SP, β-budget-balanced cost-
sharing mechanism to that of designing a cross-monotonic, β-budget-balanced
cost-sharing method.
1 Moulin mechanisms also satisfy a stronger notion of incentive compatibility called
groupstrategyproofness (GSP), which is a form of collusion resistance [26].

476
T. Roughgarden and M. Sundararajan
Summability and Approximate Eﬃciency. Roughgarden and Sundararajan [31]
showed that the approximate eﬃciency of a Moulin mechanism is completely
controlled by its budget-balance and one additional parameter of its underlying
cost-sharing method. We deﬁne this parameter and the precise guarantee next.
Deﬁnition 1 (Summability [31]). Let C and χ be a cost function and a cost-
sharing method, respectively, deﬁned on a common universe U of players. The
method χ is α-summable for C if
|S|

ℓ=1
χ(iℓ, Sℓ) ≤α · C(S)
for every ordering σ of U and every set S ⊆U, where Sℓand iℓdenote the set of
the ﬁrst ℓplayers of S and the ℓth player of S (with respect to σ), respectively.
Theorem 2 ([31]). Let U be a universe of players and C a nondecreasing cost
function on U with C(∅) = 0. Let M be a Moulin mechanism for C with un-
derlying cost-sharing method χ. Let α ≥0 and β ≥1 be the smallest numbers
such that χ is α-summable and β-budget-balanced. Then the mechanism M is
(α + β)-approximate and no better than max{α, β}-approximate.
In particular, an O(1)-budget-balanced Moulin mechanism is Θ(α)-approximate
if and only if the underlying cost-sharing method is Θ(α)-summable. Analyzing
the summability of a cost-sharing method, while non-trivial, is a tractable prob-
lem in many important cases. Because summability is deﬁned as the accrued
cost over a worst-case “insertion order” of the players, summability bounds are
often reminiscent of performance analyses of online algorithms.
3
An Optimal Facility Location Cost-Sharing Mechanism
In this section we consider the metric uncapacitated facility location (UFL)
problem.2 The input is given by a set U of demands (the players), a set F of
facilities, an opening cost fq for each facility q ∈F, and a metric c deﬁned
on U ∪F. The cost C(S) of a subset S ⊆U of players is then deﬁned as
the cost of an optimal solution to the UFL problem induced by S. In other
words, C(S) = min∅̸=F ∗⊆F [
q∈F ∗fq + 
i∈S minq∈F ∗c(q, i)]. We seek an O(1)-
budget-balanced Moulin mechanism for UFL with the best-possible approximate
eﬃciency. Theorems 1 and 2 reduce this goal to the problem of designing an O(1)-
budget-balanced cross-monotonic cost-sharing method with the smallest-possible
summability.
We begin with a simple lower bound, similar to that given in [31] for submod-
ular cost-sharing problems.
Proposition 1 (Lower Bound on UFL Approximate Eﬃciency). For
every k ≥1, there is a k-player UFL cost function C with the following property:
for every β ≥1 and every β-budget-balanced Moulin mechanism M for C, M is
no better than Hk/β-approximate.
2 Due to space constraints, we omit all proofs. Details are in [32].

Optimal Eﬃciency Guarantees for Network Design Mechanisms
477
P´al and Tardos [29] showed that every UFL cost function admits a 3-budget-
balanced cross-monotonic cost-sharing method χP T . We call this the PT method,
and the induced Moulin mechanism the PT mechanism. (See [29] or [32] for
details.) Our main result in this section shows that the PT mechanism matches
the lower bound in Proposition 1, up to a constant factor.
Theorem 3 (Upper Bound on PT Summabilitity). Let C be a k-player
UFL cost function and χP T the corresponding PT method. Then χP T is Hk-
summable for C.
Applying Theorem 2 yields an eﬃciency guarantee for the PT mechanism.
Corollary 1 (Upper Bound on PT Approximate Eﬃciency). Let C be a
k-player UFL cost function and MP T the corresponding PT mechanism. Then
MP T is (Hk + 3)-approximate.
Theorem 3 follows from two lemmas. The ﬁrst states that single-facility instances
supply worst-case examples for the summability of the PT method.
Lemma 1. For every k ≥1, the summability of PT methods for k-player UFL
cost functions is maximized by the cost functions that correspond to single-facility
instances.
Lemma 1 is based on a monotonicity property that we prove for the PT method:
increasing the distance between a demand and a facility can only increase cost
shares. This monotonicity property allows us to argue that in worst-case UFL
instances, players are partitioned into non-interacting groups, each clustered
around one facility. We complete the proof of Lemma 1 by arguing that the
summability of the PT method for one of these single-facility clusters in at least
that in the original facility location instance.
Our second lemma bounds the summability of PT methods in single-facility
instances.
Lemma 2. Let C be a k-player UFL cost function corresponding to a single-
facility instance. If χP T is the corresponding PT method, then χP T is Hk-
summable for C.
4
Optimal Rent-or-Buy Cost-Sharing Mechanisms
Single-Sink Rent-or-Buy: Next we consider single-sink rent-or-buy (SSRoB) cost-
sharing problems. The input is given by a graph G = (V, E) with edge costs that
satisfy the Triangle Inequality, a root vertex t, a set U of demands (the players),
each of which is located at a vertex of G, and a parameter M ≥1. A feasible
solution to the SSRoB problem induced by S is a way of installing suﬃcient ca-
pacity on the edges of G so that every player in S can simultaneously route one

478
T. Roughgarden and M. Sundararajan
unit of ﬂow to t. Installing x units of capacity on an edge e costs ce·min{x, M}; the
parameter M can be interpreted as the ratio between the cost of “buying” inﬁnite
capacity for a ﬂat fee and the cost of “renting” a single unit of capacity. The cost
C(S) of a subset S ⊆U of players is then deﬁned as the cost of an optimal solution
to the SSRoB problem induced by S. We sometimes abuse notation and use i ∈U
to denote both a player and the vertex of G that hosts the player.
Gupta, Srinivasan, and Tardos [14] and Leonardi and Sch¨afer [23] indepen-
dently designed the following O(1)-budget-balanced cross-monotonic cost-
sharing method for SSRoB, which we call the GST method. Given an SSRoB
cost function and a set S ⊆U of players, we use the randomized algorithm
of [13] to produce a feasible solution. This algorithm ﬁrst chooses a random sub-
set D ⊆S by adding each player i ∈S to D independently with probability
1/M. Second, it computes an approximate Steiner tree spanning D ∪{t} using,
for example, the 2-approximate MST heuristic [33], and buys inﬁnite capacity
on all of the edges of this tree. Third, for each player i /∈D, it rents one unit of
capacity for exclusive use by i on a shortest path from its vertex to the closest
vertex in D ∪{t}. This deﬁnes a feasible solution with probability 1, and the
expected cost of this solution at most 4 times that of an optimal solution to the
SSRoB instance induced by S [13].
The GST cost share χGST (i, S) is deﬁned as the expectation of the following
random variable Xi, over the random choice of the set D in the above algo-
rithm: if i /∈D, then Xi equals one quarter of the length of the shortest path
used to connect i to a vertex in D ∪{t}; if i ∈D, then Xi equals M/2 times the
Jain-Vazirani cost share χJV (i, D) of i with respect to the Steiner tree instance
deﬁned by G, c, t, and the players D (see [17] for the details of χJV ). These
cost shares are 4-budget-balanced with respect to the optimal cost of the SSRoB
instance induced by S, as well as the expected cost of the above randomized al-
gorithm that produces a feasible solution to this instance. We prove the following
result.
Theorem 4. For every k-player SSRoB cost function, the corresponding GST
mechanism is O(log2 k)-approximate.
Theorem 6 below implies that this is the best eﬃciency guarantee possible for
an O(1)-budget-balanced SSRoB Moulin mechanism.
With an eye toward extending Theorem 4 to the MRoB problem, we sum-
marize very brieﬂy the main steps in the proof (details are in [32]). First, we
decompose each GST cost share χGST (i, S) into two terms, a term χbuy(i, S)
for the contributions of samples D ⊆S in which i ∈D, and a term χrent(i, S)
for the contributions of the remaining samples. Proving Theorem 4 reduces to
proving that both χbuy and χrent are O(log2 k)-summable. Second, we use the
O(log2 k)-summability of χJV [31] together with a counting argument inspired
by [13,19] to prove that χbuy is O(log2 k)-summable. Third, we prove that the
cost-sharing method χJV is O(1)-strict in the sense of [12]. This roughly means
that whenever a player i is included in the random sample D, then the cost
share χJV (i, D) is at least a constant factor times the cost share it would have

Optimal Eﬃciency Guarantees for Network Design Mechanisms
479
received had it not been included.3 We leverage the strictness of χJV to prove
that the summability of χrent is at most a constant times that of χbuy.
Multicommodity Rent-or-Buy. We next extend Theorem 4 to the MRoB prob-
lem, where each player i corresponds to a vertex pair (si, ti). (All other aspects
of the problem are the same.) The high-level approach is similar, but the tech-
nical challenges are much more formidable. In the proof of Theorem 4, the Jain-
Vazirani cost-sharing method χJV played a heroic role: it is cross-monotonic,
which is necessary for the GST cost-sharing method to be cross-monotonic; it is
O(log2 k)-summable, which is necessary for χbuy to be O(log2 k)-summable; and
it is O(1)-strict in the sense of [12] with respect to the MST heuristic for Steiner
tree, which is necessary for χrent to be O(log2 k)-summable. Is there a compara-
bly all-purpose cost-sharing method for the Steiner Forest problem—the problem
of ﬁnding the min-cost subgraph of a given graph that includes a path between
every given vertex pair (si, ti)? The only known cross-monotonic cost-sharing
method χKLS for Steiner Forest cost-sharing problems was recently given by
K¨onemann, Leonardi, and Sch¨afer [21]. This method is deﬁned by a primal-dual
algorithm; the cost shares are a natural byproduct of a dual growth process,
and the primal is a 2-approximate feasible solution to the given Steiner Forest
instance. Using the ideas in [9,12,14,23], these facts suﬃce to deﬁne an O(1)-
budget-balanced Moulin mechanism for MRoB cost-sharing problems. Moreover,
the KLS method was very recently shown to be O(log2 k)-summable [4]; thus, the
corresponding cost-sharing method χbuy is O(log2 k)-summable. Unfortunately,
the KLS cost-sharing method is Ω(k)-strict with respect to the corresponding
primal solution [12], which precludes bounding the summability of χrent in terms
of χbuy. While several strict cost-sharing methods are known for diﬀerent Steiner
Forest approximation algorithms [2,9,12,28], none of these are cross-monotonic
methods.
Our high-level approach is to modify the above composition of the KLS
method with the mechanism design techniques of [14,23] in a way that achieves
O(1)-strictness while sacriﬁcing only a small constant factor in the budget bal-
ance. Similar ideas have been used previously to obtain strictness guarantees for
other Steiner forest algorithms [2,12,28].
Theorem 5. Every k-player MRoB cost function admits an O(1)-budget-
balanced, O(log2 k)-approximate Moulin mechanism.
5
An Ω(log2 k) Lower Bound for Steiner Tree Problems
An instance of the Steiner tree cost-sharing problem [17] is given by an undi-
rected graph G = (V, E) with a root vertex t and nonnegative edge costs, with
each player of U located at some vertex of G. For a subset S ⊆U, the cost
C(S) is deﬁned as that of a minimum-cost subgraph of G that spans all of the
3 Formally, strictness of a cost-sharing method is deﬁned with respect to some primal
algorithm; see [12] for a precise deﬁnition.

480
T. Roughgarden and M. Sundararajan
players of S as well as the root t. There are O(1)-budget-balanced, O(log2 k)-
approximate Moulin mechanisms for such problems [4,17,21,31]. The main result
of this section is a matching lower bound on the approximate eﬃciency of every
O(1)-budget-balanced Moulin mechanism.
Theorem 6. There is a constant c > 0 such that for every constant β ≥1, every
β-budget-balanced Moulin mechanism for Steiner tree cost-sharing problems is at
least (β−1c log2 k)-approximate, where k is the number of players served in an
optimal outcome.
Theorem 6 implies that Steiner tree cost-sharing problems and their general-
izations are fundamentally more diﬃcult for Moulin mechanisms than facility
location (Theorem 3) and submodular cost-sharing problems (see [31]).
We now outline the proof of Theorem 6. Fix values for the parameters k ≥2
and β ≥1. We construct a sequence of networks, culminating in G. The network
G0 consists of a set V0 of two nodes connected by an edge of cost 1. One of these
is the root t. The player set U0 is
√
k players that are co-located at the non-root
node. (Assume for simplicity that k is a power of 4.) For j > 0, we obtain the
network Gj from Gj−1 by replacing each edge (v, w) of Gj−1 with m internally
disjoint two-hop paths between v and w, where m is a suﬃciently large function
of k of β. (We will choose m ≥8β
√
k · (2β)
√
k.) See Figure 1. The cost of each
of these 2m edges is half of the cost of the edge (v, w). Thus every edge in Gj
has cost 2−j.
root t
Fig. 1. Network G2 in the proof of Theorem 6, with m = 3. All edges have length 1/4
Let Vj denote the vertices of Gj that are not also present in Gj−1. We augment
the universe by placing
√
k new co-located players at each vertex of Vj; denote
these new players by Uj. The ﬁnal network G is then Gp, where p = (log k)/2.
Let V = V0 ∪· · · ∪Vp and U = U0 ∪· · · ∪Up denote the corresponding vertex
and player sets. Let C denote the corresponding Steiner tree cost function.
Now ﬁx β ≥1 and an arbitrary cross-monotonic, β-budget balanced Steiner
tree cost-sharing method χ. By Theorem 2, we can prove Theorem 6 by exhibit-
ing a subset S ⊆U of size k and an ordering σ of the players of S such that
k
ℓ=1 χ(iℓ, Sℓ) ≥(c log2 k/β) · C(S), where iℓand Sℓdenote the ℓth player and
the ﬁrst ℓplayers with respect to σ.

Optimal Eﬃciency Guarantees for Network Design Mechanisms
481
We construct the set S iteratively. For j = 0, 1, . . ., p, we will identify a subset
Sj ⊆Uj of players; the set S will then be S0 ∪· · · ∪Sp. Recall that Uj consists
of groups of
√
k players, each co-located at a vertex of Vj, with m such groups
for each edge of Gj−1. The set Sj will consist of zero or one such group of
√
k
players for each edge of Gj−1.
The set S0 is deﬁned to be U0. For j > 0, suppose that we have already deﬁned
S0, . . . , Sj−1. Call a vertex v ∈V0 ∪· · · ∪Vj−1 active if v is the root t or if the
√
k players co-located at v were included in the set S0 ∪· · · ∪Sj−1. Call an edge
(v, w) of Gj−1 active if both of its endpoints are active and inactive otherwise.
To deﬁne Sj, we consider each edge (v, w) of Gj−1 in an arbitrary order. Each
such edge gives rise to m groups of
√
k co-located players in Gj. If (v, w) is
inactive in Gj−1, then none of these m
√
k players are included in Sj. If (v, w) is
active in Gj−1, then we will choose precisely one of the m groups of players, and
will include these
√
k co-located players in Sj. We ﬁrst state two lemmas that
hold independently of how this choice is made; we then elaborate on our criteria
for choosing groups of players.
Lemma 3. For every j ∈1, 2, . . . , p, |Sj| = 2j−1√
k. Also, |S0| =
√
k.
Lemma 3 implies that |S| =
√
k(1 + p−1
j=0 2j) = k. The next lemma states that
our construction maintains the invariant that the players selected in the ﬁrst j
iterations lie “on a straight line” in G.
Lemma 4. For every j ∈0, 1, . . . , p, C(S0 ∪· · · ∪Sj) = 1.
Lemmas 3 and 4 both follow from straightforward inductions on j.
We now explain how to choose one out of the m groups of co-located players
that arise from an active edge. Fix an iteration j > 0 and let ˆS denote the set
of players selected in previous iterations (S0, . . . , Sj−1) and previously in the
current iteration. Let (v, w) be the active edge of Gj−1 under consideration and
A1, . . . , Am ⊆Uj the corresponding groups of co-located players. We call the
group Ar good if the
√
k players of Ar can be ordered i1, i2, . . . , i√
k so that
χ(iℓ, ˆS ∪{i1, . . . , iℓ}) ≥1
4β · 2−j
ℓ
(1)
for every ℓ∈{1, 2, . . .,
√
k}. We then include an arbitrary good group Ar in the
set Sj. See [32] for a proof of the following lemma.
Lemma 5. Provided m is a suﬃciently large function of k and β, for every
j ∈{1, . . . , p}, every ordering of the active edges of Gj−1, and every edge (v, w)
in this ordering, at least one of the m groups of players of Uj that corresponds
to (v, w) is good. Also, the group S0 is good.
We conclude by using the lemma to ﬁnish the proof of Theorem 6.
We have already deﬁned the subset S ⊆U of players. We deﬁne the ordering
σ of the players in S as follows. First, for all j ∈{1, . . . , p}, all players of Sj−1

482
T. Roughgarden and M. Sundararajan
precede all players of Sj in σ. Second, for each j ∈{1, . . ., p}, the players of
Sj are ordered according to groups, with the
√
k players of a group appearing
consecutively in σ. The ordering of the diﬀerent groups of players of Sj is the
same as the corresponding ordering of the active edges of Gj−1 that was used to
deﬁne these groups. Third, each (good) group of
√
k co-located players is ordered
so that (1) holds.
Now consider the sum k
ℓ=1 χ(iℓ, Sℓ), where iℓand Sℓdenote the ℓth player
and the ﬁrst ℓplayers of S with respect to σ, respectively. Since (1) holds for
every group of players, for every j ∈{0, 1, . . ., p}, every group of players in Sj
contributes at least
√
k

ℓ=1
1
4β · 2−j
ℓ
= 2−jH√
k
4β
to this sum. By Lemma 3, for each j ∈{1, . . ., p}, there are 2j−1 such groups.
There is also the group S0. Thus the sum k
ℓ=1 χ(iℓ, Sℓ) is at least
H√
k
4β
⎛
⎝1 +
(log k)/2

j=1
2j−1 · 2−j
⎞
⎠≥c
β log2 k =
 c
β log2 k
	
· C(S)
for some constant c > 0 that is independent of k and β. This completes the proof
of Theorem 6.
References
1. A. Archer, J. Feigenbaum, A. Krishnamurthy, R. Sami, and S. Shenker. Approx-
imation and collusion in multicast cost sharing. Games and Economic Behavior,
47(1):36–71, 2004.
2. L. Becchetti, J. K¨onemann, S. Leonardi, and M. P´al. Sharing the cost more eﬃ-
ciently: Improved approximation for multicommodity rent-or-buy. In SODA ’05,
pages 375–384.
3. D. Bienstock, M. X. Goemans, D. Simchi-Levi, and D. P. Williamson.
A note
on the prize-collecting traveling salesman problem. Mathematical Programming,
59(3):413–420, 1993.
4. S. Chawla, T. Roughgarden, and M. Sundararajan. Optimal cost-sharing mecha-
nisms for network design. In WINE ’06.
5. N. R. Devanur, M. Mihail, and V. V. Vazirani. Strategyproof cost-sharing mecha-
nisms for set cover and facility location games. In EC ’03, pages 108–114.
6. J. Feigenbaum, A. Krishnamurthy, R. Sami, and S. Shenker. Hardness results for
multicast cost sharing. Theoretical Computer Science, 304:215–236, 2003.
7. J. Feigenbaum, C. Papadimitriou, and S. Shenker. Sharing the cost of multicast
transmissions. Journal of Computer and System Sciences, 63(1):21–41, 2001.
8. J. Feigenbaum and S. J. Shenker. Distributed algorithmic mechanism design: Re-
cent results and future directions. In DIAL M ’02, pages 1–13.
9. L. Fleischer, J. K¨onemann, S. Leonardi, and G. Sch¨afer.
Simple cost-sharing
schemes for multicommodity rent-or-buy and stochastic Steiner tree.
In STOC
’06, pages 663–670.

Optimal Eﬃciency Guarantees for Network Design Mechanisms
483
10. J. Green, E. Kohlberg, and J. J. Laﬀont. Partial equilibrium approach to the free
rider problem. Journal of Public Economics, 6:375–394, 1976.
11. A. Gupta, J. K¨onemann, S. Leonardi, R. Ravi, and G. Sch¨afer. An eﬃcient cost-
sharing mechanism for the prize-collecting Steiner forest problem. In SODA ’07.
12. A. Gupta, A. Kumar, M. P´al, and T. Roughgarden.
Approximation via cost-
sharing: A simple approximation algorithm for the multicommodity rent-or-buy
problem. In FOCS ’03, pages 606–615.
13. A. Gupta, A. Kumar, and T. Roughgarden. Simpler and better approximation
algorithms for network design. In STOC ’03, pages 365–372.
14. A. Gupta, A. Srinivasan, and ´E. Tardos. Cost-sharing mechanisms for network
design. In APPROX ’04, pages 139–150.
15. J. D. Hartline.
Optimization in the Private Value Model: Competitive Analysis
Applied to Auction Design. PhD thesis, University of Washington, 2003.
16. N. Immorlica, M. Mahdian, and V. S. Mirrokni. Limitations of cross-monotonic
cost-sharing schemes. In SODA ’05, pages 602–611.
17. K. Jain and V. Vazirani. Applications of approximation algorithms to cooperative
games. In STOC ’01, pages 364–372.
18. K. Jain and V. Vazirani.
Equitable cost allocations via primal-dual-type algo-
rithms. In STOC ’02, pages 313–321.
19. D. R. Karger and M. Minkoﬀ. Building Steiner trees with incomplete global knowl-
edge. In FOCS ’00, pages 613–623.
20. K. Kent and D. Skorin-Kapov. Population monotonic cost allocation on MST’s.
In Operational Research Proceedings KOI, pages 43–48, 1996.
21. J. K¨onemann, S. Leonardi, and G. Sch¨afer. A group-strategyproof mechanism for
Steiner forests. In SODA ’05, pages 612–619.
22. J. K¨onemann, S. Leonardi, G. Sch¨afer, and S. van Zwam. From primal-dual to
cost shares and back: A stronger LP relaxation for the steiner forest problem. In
ICALP ’05, pages 1051–1063.
23. S. Leonardi and G. Sch¨afer. Cross-monotonic cost-sharing methods for connected
facility location. In EC ’04, pages 242–243.
24. A. Mas-Colell, M. D. Whinston, and J. R. Green. Microeconomic Theory. Oxford
University Press, 1995.
25. H. Moulin.
Incremental cost sharing: Characterization by coalition strategy-
proofness. Social Choice and Welfare, 16:279–320, 1999.
26. H. Moulin and S. Shenker. Strategyproof sharing of submodular costs: Budget
balance versus eﬃciency. Economic Theory, 18:511–533, 2001.
27. M. J. Osborne and A. Rubinstein. A Course in Game Theory. MIT Press, 1994.
28. M. P´al. Cost Sharing and Approximation. PhD thesis, Cornell University, 2005.
29. M. P´al and ´E. Tardos.
Group strategyproof mechanisms via primal-dual algo-
rithms. In FOCS ’03, pages 584–593.
30. K. Roberts. The characterization of implementable choice rules. In J. J. Laﬀont,
editor, Aggregation and Revelation of Preferences. North-Holland, 1979.
31. T. Roughgarden and M. Sundararajan. New trade-oﬀs in cost-sharing mechanisms.
In STOC ’06, pages 79–88.
32. T. Roughgarden and M. Sundararajan. Approximately eﬃcient cost-sharing mech-
anisms. Technical Report cs.GT/0606127, arXiv, 2006.
33. V. V. Vazirani. Approximation Algorithms. Springer, 2001.
34. W. Vickrey. Counterspeculation, auctions, and competitive sealed tenders. Journal
of Finance, 16(1):8–37, 1961.

The Set Connector Problem in Graphs
Takuro Fukunaga and Hiroshi Nagamochi
Department of Applied Mathematics and Physics,
Graduate School of Informatics, Kyoto University, Japan
{takuro,nag}@amp.i.kyoto-u.ac.jp
Abstract. Given a graph G = (V, E) with an edge cost and families
Vi ⊆2V , i = 1, 2, . . . , m of disjoint subsets, an edge subset F ⊆E
is called a set connector if, for each Vi, the graph (V, F)/Vi obtained
from (V, F) by contracting each X ∈Vi into a single vertex x has
a property that every two contracted vertices x and x′ are connected
in (V, F)/Vi. In this paper, we introduce a problem of ﬁnding a mini-
mum cost set connector, which contains several important network de-
sign problems such as the Steiner forest problem, the group Steiner tree
problem, and the NA-connectivity augmentation problem as its special
cases. We derive an approximate integer decomposition property from
a fractional packing theorem of set connectors, and present a strongly
polynomial 2α-approximation algorithm for the set connector problem,
where α = max1≤i≤m(
X∈Vi |X|) −1.
1
Introduction
Let G = (V, E) be an undirected graph with vertex set V and edge set E. For
a family V ⊆2V of disjoint vertex subsets, we let G/V stand for the graph
obtained from G by contracting each X ∈V into a single vertex x, which is
called a V-terminal. As a general concept of the edge connectivity between two
vertices, we deﬁne the edge-connectivity λ(V; G) for V ⊆2V as the minimum
edge-connectivity of two V-terminals in G/V. If V consists of two singletons {u}
and {v}, then λ(V; G) is equivalent to the edge-connectivity between two vertices
u and v.
Let X ⊆V be a vertex subset. We deﬁne δ(X) as the set of edges in E that
join a vertex in X and another in V −X, where we let δ(V ) = ∅for convenience.
We say that X separates V if either Y ⊆X or Y ⊆V −X holds for each Y ∈V,
and Y ⊆X ⊆V −Y ′ for some Y, Y ′ ∈V. We note that λ(V; G) is also deﬁned
as min{|δ(X)| | X ⊂V separates V}.
In this paper, we consider the set connector problem, which is deﬁned as
follows.
Set connector problem
Given a simple undirected graph G = (V, E), an edge cost c : E →Q+, and
families V1, . . . , Vm ⊆2V of disjoint vertex subsets, ﬁnd a minimum cost edge
subset F ⊆E such that λ(Vi; GF ) ≥1 for 1 ≤i ≤m, where GF denotes the
graph (V, F).
M. Fischetti and D.P. Williamson (Eds.): IPCO 2007, LNCS 4513, pp. 484–498, 2007.
c
⃝Springer-Verlag Berlin Heidelberg 2007

The Set Connector Problem in Graphs
485
U
W
X
Y
Z
Fig. 1. An instance of the set connector problem with V1 = {U, W, Z} and V2 =
{U, X, Y }, where a set connector F consists of the edges depicted by dashed lines
We call a feasible solution for the set connector problem a set connector.
Notice that a minimal set connector is a forest. Figure 1 shows an instance
(G, V1 = {U, W, Z}, V2 = {U, X, Y }) of the set connector problem, where the
subsets U, W, X, Y, Z ⊆V are respectively depicted by gray areas, and a set
connector F is given by the edges represented by dashed lines.
The set connector problem contains many fundamental problems. For ex-
ample, it is equivalent to the Steiner forest problem when each Vi consists of
singletons. Besides this, it contains the group Steiner tree problem, which is an-
other generalization of the Steiner tree problem. As will be stated in Section 5,
the group Steiner tree problem contains the set cover problem, the tree cover
problem, and the terminal Steiner tree problem as its special cases.
Our main contribution of this paper is to present a 2α-approximation algo-
rithm for the set connector problem, where α = max1≤i≤m(
X∈Vi |X|) −1. To
the best of our knowledge, this is the ﬁrst approximation algorithm that approx-
imates the Steiner forest problem and the group Steiner tree problem simulta-
neously. The approximation ratio of our algorithm to the set connector problem
matches with the best approximation ratios of several special cases such as the
Steiner forest problem, as will be discussed in Section 5. Our algorithm is based
on the approximate integer decomposition property [5]. A polyhedron P has an f-
approximate integer decomposition property for a real f > 0 if, for every rational
vector x ∈P and every integer k such that kx is an integer vector, there exist k
integer vectors x1, . . . , xk ∈P such that fkx ≥x1 +· · ·+xk holds. This property
implies that the integrality gap of polyhedron P is at most f for any non-negative
cost vector c, since an integer vector xj attaining min{cT xi | i = 1, . . . , k} satis-
ﬁes fcT x ≥cT xj. C. Chekuri and F. B. Shepherd [5] showed the 2-approximate
integer decomposition property of an LP relaxation for the Steiner forest prob-
lem via the following Steiner packing theorem, which generalizes a well-known
spanning tree packing theorem due to Gusﬁeld [12] in Eulerian graphs.
Theorem 1. [5] Let G be an Eulerian multigraph. Then G contains k edge-
disjoint forests F1, . . . , Fk such that λ(u, v; GFi) ≥1, 1 ≤i ≤k holds for ev-
ery two vertices u and v that belong to the same 2k-edge-connected component
in G.
⊓⊔

486
T. Fukunaga and H. Nagamochi
For an |E|-dimensional real vector x and an edge subset F ⊆E, we let x(F)
denote the sum of elements of x corresponding to edges in F. The set connector
problem can be formulated as the following integer programming.
minimize cT x
subject to x(δ(X)) ≥1 for every X ⊂V separating some Vi ∈{V1, . . . , Vm}
x ∈{0, 1}E.
Let LPsc be the linear programming obtained by relaxing the integrality con-
straint x ∈{0, 1}E of this problem into x ∈RE
+, and let Psc denote its feasible
region. For obtaining the 2α-approximate integer decomposition property of Psc,
it suﬃces to show the following set connector packing theorem, which is a gen-
eralization of Theorem 1.
Theorem 2. Let G be an Eulerian multigraph, V1, . . . , Vm be families of disjoint
vertex subsets, and α = max1≤i≤m(
X∈Vi |X|) −1. If λ(Vi; G) ≥2αk for 1 ≤
i ≤m, then G contains k edge-disjoint set connectors.
⊓⊔
The approximate integer decomposition property depends on the fact that x ∈P
is a rational vector. Hence we actually prove the following fractional packing
theorem instead of Theorem 2. The proof of the theorem can be easily modiﬁed
to imply Theorem 2.
Theorem 3. Let x ∈Psc and α = max1≤i≤m(
X∈Vi |X|) −1 for a simple
undirected graph G = (V, E) and families V1, . . . , Vm of disjoint vertex subsets.
Then there exist set connectors C1, . . . , Ck with positive weights w1, . . . , wk such
that 2αx ≥k
i=1 wiXCi and k
i=1 wi = 1, where XCi ∈{0, 1}E denotes the
incidence vector of Ci.
⊓⊔
This paper is organized as follows. Section 2 introduces notations and induction
techniques. Section 3 provides a proof of Theorem 3, and Section 4 describes
a 2α-approximation algorithm for the set connector problem. Section 5 shows
applications of the set connector problem. Section 6 concludes this paper with
some remarks.
2
Preliminaries
2.1
Notations
Let R+, Q+ and Z+ stand for the sets of non-negative reals, rationals, and
integers, respectively. Let G = (V, E) be an undirected graph and x ∈RE
+.
For an edge e ∈E, x(e) denotes the element of x corresponding to e. Let Ex
represent the support for x, i.e., Ex = {e ∈E | x(e) > 0}. For an edge subset
F ⊆E, let x(F) = 
e∈F x(e), where we deﬁne x(∅) = 0 for convenience. Let
GF = (V, F), xF ∈RF
+ denote the projection of x onto F, and XF ∈{0, 1}E
denote the incidence vector of F.

The Set Connector Problem in Graphs
487
For a vertex subset U ⊆V , E[U] denotes the set of edges whose both end
vertices are in U, and G[U] denotes the subgraph (U, E[U]) of G induced by
U. Moreover δ(U) represents the set of edges in G that join a vertex in U and
another in V −U. A singleton set U = {u} may be written as u. For a partition
P = {V1, . . . , Vp} of V into non-empty subsets, δ(P) denotes ∪p
i=1δ(Vi). For a
family V ⊆2V of disjoint vertex subsets, G/V denotes a graph obtained by
contracting each X ∈V into a single vertex.
In this paper, we often discuss the edge-connectivity of a simple graph G =
(V, E) whose edges are weighted by a vector x ∈RE
+. In this case, we assume
without loss of generality that G is the complete graph on V by augmenting E with
edges e ∈
V
2

−E, where we let x(e) = 0, e ∈
V
2

−E. We denote such an edge-
weighted graph by (V, x). We deﬁne the edge-connectivity λ(V; V, x) of a family
V ⊆2V of disjoint vertex subsets in (V, x) as min{x(δ(X)) | X ⊂V separates V}.
If V consists of two elements X and Y , we may denote λ(V; V, x) by λ(X, Y ; V, x).
A k-edge-connected component of (V, x) is an inclusion-wise maximal subset U ⊆
V that satisﬁes λ(u, v; V, x) ≥k for all u, v ∈U.
For F ⊆
V
2

and a positive real w, we let (F, w) stand for a subgraph (V, F)
weighted by w. A set of weighted subgraphs (F1, w1), (F2, w2), . . . , (Fk, wk) is
called a fractional forest packing of an edge-weighted graph (V, x) if Fi is a forest,
1 ≤i ≤k, x ≥
1≤i≤k wiXFi, and 
1≤i≤k wi = 1. Notice that Fi ⊆Ex holds
for 1 ≤i ≤k here. If each of F1, . . . , Fk is a spanning tree on V (resp., set con-
nector), we especially call it fractional spanning tree packing (resp., fractional set
connector packing). We may simply say that a set of edge subsets F1, F2, . . . , Fk
is a fractional forest packing of (V, x) if there are weights w1, w2, . . . , wk such
that (Fi, wi), i = 1, 2, . . . , k is a fractional forest packing of (V, x).
2.2
Induction Techniques
In this subsection, we review graph operations called contraction and splitting.
In this paper, we use these operations in order to prove some claims inductively.
First, let us see the contraction. Contracting a vertex set S ⊆V into a single
vertex s means that S is replaced by s, resultant loops are deleted, and one end
vertex of every edge in δ(S) is changed from a vertex in S to s. Let V ′ denote
the vertex set obtained by the contraction, i.e., V ′ = (V −S) ∪s. If we execute
the contraction in (V, x), then x is modiﬁed into x′ ∈R(V ′
2 )
+
so that x′(e) = x(e)
for each e ∈
V ′
2

−δ(s), and x′(e) = 
u∈S x(uv) for each e = sv ∈δ(s).
Lemma 1. Let (V ′, x′) be an edge-weighted undirected graph obtained from
(V, x) by contracting S ⊆V into a single vertex s. If there exists a fractional
forest packing C′ of (V ′, x′), then we can obtain a fractional forest packing C of
(V, x), every forest in which consists of edges in Ex −
S
2

. Every two vertices
in V −S connected by all forests in C′ are also connected by the union of every
forest in C and every spanning tree on S.
⊓⊔

488
T. Fukunaga and H. Nagamochi
Next, let us see the splitting. Splitting a pair {sa, sb} of edges by ϵ > 0 is an oper-
ation that decreases x(sa) and x(sb) by ϵ and increases x(ab) by ϵ, where possibly
a = b and ϵ is supposed to be at most min{x(su), x(sv)}. Historically, this oper-
ation was introduced by L. Lov´asz to study the edge-connectivity of multigraphs
G (i.e., x ∈ZE
+ and ϵ ∈Z+). W. Mader [21] showed that if |δ(s; G)| ̸= 3, there
always exists a pair of edges incident to s such that splitting them by 1 pre-
serves the edge-connectivity between every two vertices in V −s. Furthermore,
A. Frank [7] showed that for any edge incident to s, there always exists such
a pair that contains the edge if |δ(s; G)| is even. A proof of his theorem uses
the fact that splitting {sa, sb} by a real ϵ ∈R+ preserves the edge-connectivity
between every two vertices in V −s if and only if
ϵ ≤1
2 min{x(δ(X)) −λ(u, v; V, x) | a, b, u ∈X ⊆V −(s ∪v), s ̸= v}.
(1)
This fact can be derived from the observation that splitting {sa, sb} by ϵ de-
creases x(δ(X)) by 2ϵ if a, b ∈X ⊂V −s, and does not change x(δ(X))
otherwise. We let qx(a, b) denote the right hand side of inequality (1), and
ϵa,b = min{x(sa), x(sb), qx(a, b)}. Notice that ϵa,b can be regarded as the max-
imum value such that splitting {sa, sb} by ϵa,b preserves the edge-connectivity
between every two vertices in V −s.
In this paper, we use the splitting in order to isolate a vertex s ∈V in (V, x),
i.e., x(sv) = 0 for every v ∈V −s. A complete splitting at s denotes an operation
that isolates s by repeating splitting edges incident to s. The following theorem
tells that it always can be executed in strongly polynomial time while preserving
the edge-connectivity between every two vertices in V −s.
Theorem 4. Let s be an arbitrary vertex in (V, x). There exists a complete
splitting at s such that λ(u, v; V, x) = λ(u, v; V −s, x′) holds for every u, v ∈V −s,
where x′ ∈R(V −s
2 )
+
is the resulting edge weight from the complete splitting. Such
a complete splitting can be found in strongly polynomial time.
⊓⊔
We note that splitting every pair {sa, sb} of edges incident to s by ϵa,b gives
the complete splitting in the theorem. The strong polynomiality of the complete
splitting comes from the fact that ϵa,b can be computed in strongly polynomial
time. We use the splitting for the induction as described below.
Lemma 2. Let x′ ∈R(
V −s
2 )
+
be the edge-weight obtained from x ∈R(
V
2)
+
by a
complete splitting at s. If there exists a fractional forest packing C′ of (V, x′),
then we can construct a fractional forest packing C of (V, x). Every two vertices
in V −s connected by all forests in C′ are also connected by all in C.
⊓⊔
3
Proof of the Fractional Set Connector Packing
Theorem
In this section, we give a proof of Theorem 3. First of all, let us review a fractional
version of Tutte’s tree packing theorem [25].

The Set Connector Problem in Graphs
489
Theorem 5. [25] Let G = (V, x) be an edge-weighted undirected graph. Then
there exits a fractional spanning tree packing of G if and only if
x(δ(P)) ≥|P| −1 for every partition P of V into nonempty classes.
(2)
⊓⊔
We can derive the following lemma from the above theorem.
Lemma 3. Let G = (V, x) be an edge-weighted undirected graph, and K ⊂V
be an inclusion-wise minimal subset such that x(δ(K)) < 2. Then there exists a
fractional spanning tree packing of (K, x(
K
2)).
Proof. We show that (2) holds for graph (K, x(
K
2)). Let P be a partition of
K into nonempty classes. Then for any X ∈P (i.e., X ⊂K), it holds that
x(δ(X)) ≥2 by the minimality of K. Therefore x(K
2)(δ(P)) = (
X∈P x(δ(X))−
x(δ(K)))/2 > |P| −1 holds. Then by applying Theorem 5 to (K, x(K
2)), we can
obtain a fractional spanning tree packing of (K, x(K
2)).
⊓⊔
To prove Theorem 3, we use a result on the Steiner forest packing due to
C. Chekuri and F. B. Shepherd [5]. Here we state a fractional packing version of
Theorem 1. The proof is based on that of C. Chekuri and F. B. Shepherd [5].
Theorem 6. Let G = (V, x) be an edge-weighted undirected graph. Then there
exists a fractional forest packing C of G such that λ(u, v; GF ) ≥1 for every
F ∈C and u, v ∈V with λ(u, v; V, x) ≥2.
Proof. We prove this theorem by an induction on the number N of 2-edge-
connected components in (V, x). First, let us consider the case of N = 1. Then
for any nonempty X ⊂V , it holds that x(δ(X)) ≥2, which implies that (2) holds
for x because x(δ(P)) = 
X∈P x(δ(X))/2 ≥|P|. Therefore, we can obtain a
required fractional forest packing by Theorem 5.
Next, consider the case of N ≥2. Let K ⊂V be an inclusion-wise minimal
subset such that x(δ(K)) < 2 (such K exists since the edge-connectivity between
two vertices in diﬀerent components is less than 2). Then K is the union of some
2-edge-connected components. By Lemma 3, there exists a fractional spanning
tree packing {(Ti, βi) | 1 ≤i ≤p} of (K, x(
K
2)). Let G′ = (V ′ = (V −K), x′ ∈
V ′
2

) be the graph obtained by contracting K into a single vertex vK, executing
the complete splitting at vK, and removing isolated vK. Note that any two
vertices u, v ∈V ′ that belong to the same 2-edge-connected component in (V, x)
remains 2-edge-connected in (V ′, x′).
By the inductive hypothesis, (V ′, x′) has a fractional forest packing {(Hi, θi) |
1 ≤i ≤q} such that each of H1, . . . , Hq connects every two vertices u, v ∈V ′
with λ(u, v; V ′, x′) ≥2 (and hence λ(u, v; V, x) ≥2). Let {(H′
i, θ′
i) | i = 1, . . . , q′}
be the fractional forest decomposition of (V, x) obtained from {(Hi, θi) | i =
1, . . . , q} by applying Lemmas 1 and 2. Then clearly {(Ti ∪H′
j, βiθ′
j) | 1 ≤i ≤
p, 1 ≤j ≤q′} is a required fractional forest packing.
⊓⊔

490
T. Fukunaga and H. Nagamochi
Lemma 4. Let G=(V, x) be an edge-weighted undirected graph, and V1,. . . ,Vm ⊆
2V be families of disjoint vertex subsets such that λ(Vi; V, x) ≥2(
X∈Vi |X|)−2
for every i = 1, . . . , m. If F ⊆
V
2

satisﬁes λ(u, v; GF ) ≥1 for all u, v ∈V with
λ(u, v; V, x) ≥2, then F is a set connector for V1, . . . , Vm.
Proof. Consider a family Vi ∈{V1, . . . , Vm}, and let {{X1, . . . , Xq}, {Y1, . . . , Yr}}
be a partition of Vi into two classes. We denote ∪q
j=1Xj by X and ∪r
j=1Yj by Y .
In the following, we show that there exists two vertices u ∈X and v ∈Y with
λ(u, v; G, x) ≥2. This implies the lemma since an edge set that connects such
vertices is a set connector in this case.
Now we suppose conversely that λ(u, v; V, x) < 2 holds for every u ∈X and
v ∈Y . We construct a partition P of V and a family Q ⊆2V of vertex subsets as
follows. First we set P = {V } and Q = ∅. Let us consider the moment at which
some two vertices u ∈X and v ∈Y belong to the same class of P. Then choose
W ⊂V such that u ∈W, v ∈V −W and x(δ(W)) < 2 (such W exists since
λ(u, v; G, x) < 2) and update P := ∪Z∈P{Z ∩W, Z −W} and Q := Q ∪{W}.
Repeat this procedure until every two vertices in X and in Y belong to diﬀerent
classes of P.
We can see that the number of the repetitions is at most |X| + |Y | −1 by the
induction on |X|+|Y | as follows. Let W be chosen as a member of Q after running
the procedure once. For separating vertices in W ∩X from those in W ∩Y , at most
|W ∩(X ∪Y )| −1 repetitions are enough by the inductive hypothesis. Similarly,
at most |(X ∪Y )−W|−1 repetitions separates vertices in X −W from those in
Y −W. Since W separates vertices in W from those in (X∪Y )−W, the number of
the repetitions is at most 1+(|W∩(X∪Y )|−1)+(|(X∪Y )−W|−1) = |X|+|Y |−1.
From this fact, |Q| ≤|X| + |Y | −1 = (
Z∈Vi |Z|) −1 holds. Moreover,
we can see that δ(P) ⊆∪W∈Qδ(W). Now let U = ∪p
j=1Vj, where V1, . . . , Vp
be the classes of P that contain vertices in X. Notice that U separates Vi.
Since x(∪W∈Qδ(W)) < 2|Q| ≤2(
Z∈Vi |Z|) −2, it holds that x(δ(U)) ≤
x(∪p
j=1δ(Vj)) ≤x(δ(P)) < 2(
Z∈Vi |Z|)−2. These facts imply that λ(Vi; V, x) <
2(
Z∈Vi |Z|) −2, a contradiction.
⊓⊔
Now we are ready to prove Theorem 3. In the proof, we show the following
observation together with Theorem 3.
Observation 1. Set connectors in Theorem 3 can be given as forests connecting
all vertices in each 2-edge-connected component of (V, 2αx).
⊓⊔
Proof (Proof of Theorem 3 and Observation 1). Since x ∈Psc, we see that
λ(Vi; V, x) = min{x(δ(X)) | X separates Vi} ≥1 holds for every 1 ≤i ≤m.
Therefore, λ(Vi; G, 2αx) = 2αλ(Vi; G, x) ≥2α ≥2(
X∈Vi |X| −1) holds for
1 ≤i ≤m. By Lemma 4, at least one pair {u, v} of vertices u ∈∪q
i=1Xi and
v ∈∪r
i=1Yi is contained in the same 2-edge-connected component of (V, 2αx) for
any partition {{X1, . . . , Xq}, {Y1, . . . , Yr}} of Vi into two classes. Hence every
forest that connects all vertices in each 2-edge-connected component of (V, 2αx)
is a set connector.

The Set Connector Problem in Graphs
491
By Theorem 6, there exist a fractional forest packing {F1, . . . , Fk} of (V, 2αx)
such that every two vertices u, v ∈V with λ(u, v; V, 2αx) are connected by each
of F1, . . . , Fk. By the above observation, this is a desired factional set connector
packing.
⊓⊔
As a corollary of Theorem 3, we can see that the integrality gap of LPsc is at
most 2α.
Corollary 1. For any vectors x ∈Psc and c ∈QE
+, there always exists a set
connector F ⊆E such that 2αcT x ≥c(F). Such F can be given as a forest
connecting all vertices in each 2-edge-connected component of (V, 2αx).
⊓⊔
This gap is tight in the following instance. Given an integer d ≥1, let G = (V, E)
be the complete graph on a vertex set V of cardinality n > 2d, and c(e) = 1 for
all e ∈E. Moreover specify a vertex s ∈V and deﬁne V1, . . . , Vm as the families
{{s}, U} for all subsets U ⊆V −s with |U| = α, where m =
|V |−1
α

. In this
instance, α = max1≤i≤m(
X∈Vi |X|) −1 holds.
Deﬁne a rational vector x ∈QE
+ as x(e) = 1/(n −1) if e is incident to s, and
x(e) = 1/(a(n−1)) otherwise. Then we can verify that x ∈Psc holds. Hence the
optimal cost of rational solutions is at most cT x = (n−1)/(n−1)+
n−1
2

/(α(n−
1)) = (n + 2α −2)/(2α). On the other hand, let us consider an optimal integral
solution F ⊆E. Consider the connected component S that contains s in GF . If
|S| < n −α + 1, i.e., |V −S| ≥α, then 0 = δ(S; GF ) ≥λ(Vi; GF ) would hold
for some Vi = {{s}, U} with a set U ⊆V −S. Hence |S| ≥n −α + 1. By this,
|F| ≥|S| −1 ≥n −α + 1 −1 = n −α. Therefore the integrality gap of this
instance is
The optimal cost of integer solutions
The optimal cost of rational solutions ≥c(F)
cT x ≥
n −α
(n + 2α −2)/(2α).
We can see that the most right term approaches 2α as n gets larger.
4
Approximation Algorithm
In Corollary 1, we saw that any vector x ∈Psc can be rounded to a set connec-
tor F with c(F) ≤2αcT x, and that such F can be given as a forest connect-
ing all vertices in each 2-edge-connected component of (V, 2αx) (i.e., 1/α-edge-
connected component of (V, x)). Hence by applying a ρ-approximation algorithm
of the Steiner forest problem to constructing such a forest in G, we have a 2αρ-
approximation algorithm for the set connector problem, where currently ρ ≤2
is known [11]. However, the arguments in Section 3 indicate a 2α-approximation
algorithm for the set connector problem. In this section, we describe this.
In the ﬁrst step, our algorithm computes an optimal solution x of LPsc for the
given instance consisting of G = (V, E), c ∈QE
+, and V1, . . . , Vm ⊆2V . We then
augment x into R(
V
2)
+
by adding 0’s, and c into Q(
V
2)
+
by adding +∞’s if G is not
complete. Then our algorithm constructs a forest F ⊆Ex ⊆E that connects all
vertices in each 1/α-edge-connected component of (V, x) as follows.

492
T. Fukunaga and H. Nagamochi
Let K ⊆V be an inclusion-wise minimal vertex set such that x(δ(K)) <
1/α. Recall that the proof of Theorem 6 computes a fractional tree packing of
(K, x(K
2)) by applying Lemma 3. Instead of this, our algorithm computes a min-
imum cost tree TK ⊆Ex ∩E[K] spanning K. Then we contract K into a single
vertex vK, and execute a complete splitting at vK. When our algorithm executes
contraction or splitting, it modiﬁes the edge cost simultaneously. After this, it
recursively computes a sequence of trees in the resulting edge-weighted graph
and edge cost until the vertex set becomes a singleton. As reverse operations
of contraction and splitting, our algorithm modiﬁes the forest and output the
sum of TK and the modiﬁed forest as a solution. Below, we describe how to
modify the edge cost and how to modify the forest in the reverse operations of
contraction and splitting.
First, let us consider the contraction. Let x′ ∈R(V ′
2 )
+
be the vector obtained
from x by the contracting K into vK, where V ′ = (V −K) ∪vK. Together
with this contraction, our algorithm modiﬁes edge cost c into c′ ∈Q(
V ′
2 )
+
so that
c′(uvK) = min{c(us) | s ∈K, x(us) > 0} for each u ∈V −K and c′(uv) = c(uv)
for each u, v ∈V −K. Suppose our algorithm has computed a forest F ′ ⊆Ex′
for (V ′, x′) and c′. Then it constructs a forest F ⊆Ex −
K
2

for (V, x) and c
from F ′ in the reverse operation of the contraction as follows. If F ′ contains
no edge in δ(vK), we set F to F ′. Otherwise, prepare an edge uv such that
c(uv) = c′(uvK) for each uvK ∈F ′ ∩δ(vK), and let F ′′ be the set of those
edges. Then F is deﬁned as (F ′ −δ(vK)) ∪F ′′. Notice that c(F) = c′(F ′) holds.
Moreover, F ′ ∪TK connects every two vertices connected by F.
Next, let us consider the splitting. Let x′ ∈R(V ′
2 )
+
be the vector from x ∈R(V ′
2 )
+
by splitting a pair {vKa, vKb} of edges by ϵvKa,vKb > 0 in the complete splitting
at vK. Together with this splitting, our algorithm modiﬁes the edge cost c into
a new cost c′ ∈Q(
V ′
2 )
+
so that c′(ab) = min{c(ab), c(vKa) + c(vKb)} if x(ab) > 0
and c′(ab) = c(vKa) + c(vKb) otherwise while c′(e) = c(e) for e ∈
V ′
2

−ab.
Suppose our algorithm has computed a forest F ′ ⊆Ex′ for (V ′, x′) and c′.
Then it constructs a forest F ⊆Ex for (V, x) and c from F ′ in the reverse
operation of the splitting as follows. If c′(ab) = c(vKa) + c(vKb), then F is set
to (F ′ −ab) ∪{vKa, vKb} Otherwise, F is set to F ′. Notice that c(F) = c′(F ′)
holds in both cases.
We note that the reverse operation of contraction and splitting described
above can be easily executed by maintaining p(e) for each e ∈
V
2

. At the
beginning of our algorithm, p(e) is set to {e}. Our algorithm then updates
p(uvK) := p(uv) when a set K containing v is contracted into vK and c′(uvK) is
deﬁned as c(uv), and p(ab) := p(vKa) ∪p(vKb) when a pair {vKa, vKb} is split
and c′(ab) is updated to c(vKa)+c(vKb). Observe that ∪e∈F ′p(e) represents the
edge set constructed from a forest F ′ in both reverse operations.
Now we are ready to see the entire algorithm. The following describes how to
compute a solution after an optimal solution x of LPsc is given.

The Set Connector Problem in Graphs
493
Algorithm SETCONNECT
Input: A vertex set V , a vector x ∈R(V
2)
+ , and an edge cost c ∈Q(V
2)
+
Output: A forest F ⊆
V
2

1: K := an inclusion-wise minimal X ⊆V with x(δ(X)) < 1/α;
2: Compute a minimum cost tree TK ⊆Ex ∩
K
2

spanning K; # possibly
|K| = 1 or K = V
3: if |V | −1 ≤|K| ≤|V | then
4:
Return F := TK as a solution and halt
5: end if;
# contract K into vK
6:
c′ := c;
7:
For each e ∈
V
2

, deﬁne p(e) := {e};
8:
V ′ := (V −K) ∪vK; x′
(V −K
2 ) := x(
V −K
2 );
9: for u ∈V −K do
10:
x′(uvK) := 
v∈K x(uv);
11:
if x′(uvK) > 0 then
12:
e := an edge attaining min{c(uv) | v ∈K, x(uv) > 0};
13:
c′(uvK) := c(e); p(uvK) := p(e)
14:
end if
15: end for;
# complete splitting at vK
16: for distinct a, b ∈V ′ −vK do
17:
Compute ϵa,b in (V ′, x′);
18:
if ϵa,b > 0 and x′(ab) = 0 or c′(ab) > c′(vKa) + c′(vKb) then
19:
c′(ab) := c′(vKa) + c′(vKb); p(ab) := p(vKa) ∪p(vKb)
20:
end if;
21:
x′(vKa) := x′(vKa)−ϵa,b; x′(vKb) := x′(vKb)−ϵa,b; x′(ab) := x′(ab)+ϵa,b
22: end for;
23: V ′ := V ′ −vK;
24: F ′ := A solution output by SETCONNECT applied to V ′, x′
(V ′
2 ) and c′;
25: Return F := TK ∪e∈F ′ p(e) as a solution;
Theorem 7. The set connector problem can be approximated within factor of
2α by applying algorithm SETCONNECT to an optimal solution x of LPsc.
Proof. connecting all vertices in a 1/α-edge-connected component of (V, x) by
the induction on |V |, the combination of which and Lemma 4 implies that F is
a set connector for G and V1, . . . , Vm.
By the choice of TK, it holds that TK ⊆Ex . By the induction hypothesis,
F ′ ⊆Ex′, and then ∪e∈F ′p(e) ⊆Ex. Since F = TK ∪e∈F ′ p(e), F ⊆Ex holds. On
the other hand, let u and v be two vertices in V such that λ(u, v; V, x) ≥1/α.
Then these are contained either in K or in V −K during the algorithm. If

494
T. Fukunaga and H. Nagamochi
u, v ∈K, these are connected by F since F contains a tree TK spanning K.
In what follows, we suppose that u, v ∈V −K. Let x′ represent the vector
maintained in the end of the algorithm. Since contracting K into vK and the
complete splitting at vK does not decrease the edge-connectivity between u
and v, it follows that λ(u, v; V ′, x′
(
V ′
2 )) ≥1/α. By the inductive hypothesis, F ′
connects u and v, and thereby F = TK ∪e∈F ′ p(e) connects such u and v.
Next, let {(Ci, wi) | i = 1, . . . , k} be a fractional set connector packing of
(V, 2αx) and V1, . . . , Vm appeared in Theorem 3. In the following, we show that
c(F) ≤c(Ci) for every i = 1, . . . , k by the induction on |V | again. This implies
that F is a 2α-approximate solution for the set connector problem.
Recall that the proof of Theorem 3 constructs Ci as the union of T and
∪e∈Hp(e), where T ⊆Ex ∩
K
2

is a spanning tree on K and H ⊆Ex′ ∩
V −K
2

is a forest in a fractional forest packing of (V ′, x′). By the choice of TK, obvi-
ously c(TK) ≤c(T ) holds. On the other hand, c′(F ′) ≤c′(H) by the inductive
hypothesis. As observed in the above, it holds that c′(F ′) = c(∪e∈F ′p(e)) and
c′(H) = c(∪e∈Hp(e)). Since F = TK ∪e∈F ′ p(e) and Ci = T ∪e∈H p(e), we have
obtained c(F) ≤c(Ci).
⊓⊔
We note that running time of algorithm SETCONNECT is strongly polyno-
mial, where we use Tardos’ algorithm [27] to solve LPsc. All steps of algorithm
SETCONNECT except solving LPsc are combinatorial.
5
Applications
In this section, we review some problems related to the set connector problem.
5.1
NA-Connectivity
Herewementiontheprior worksonthenodetoareaconnectivity(NA-connectivity).
H. Ito [16] considered the edge-connectivity λ(v, X) between a vertex v ∈V and
a vertex subset X ⊆V , and called it NA-connectivity. Then augmentation-type
problem of NA-connectivity was considered by some researchers [15,17,22]. For ex-
ample, the following problem was shown to be NP-hard by H. Miwa and H. Ito [22].
1-NA-connectivity augmentation problem
Given an undirected graph G = (V, E) and a family V ⊆2V , ﬁnd an edge set
F ⊆
V
2

−E of minimum cardinality such that λ(v; X; GE∪F ) ≥1 holds for all
X ∈V and v ∈V −X.
By using an algorithm due to Z. Nutov [23], this problem can be approximated
within 7/4.
The edge-connectivity for a family of vertex subsets we deﬁned in this paper
generalizes the NA-connectivity since λ(v, X; G) = λ(VX; G) holds if we set
VX = {{v}, X} for X ∈V. Hence the above augmentation problem is contained
in the set connector problem even if it is generalized so that an edge cost c :
V
2

−E →Q+ is also given and c(F) is minimized.

The Set Connector Problem in Graphs
495
Theorem 8. The 1-NA-connectivity augmentation problem with an edge cost
can be approximated within a factor of 2 maxX∈V |X|.
⊓⊔
5.2
Steiner Forest Problem
The Steiner forest problem is formulated as follows.
Steiner forest problem
Given an undirected graph G = (V, E) and disjoint vertex subsets X1, . . . , Xℓ⊆
V , ﬁnd a minimum cost edge set F ⊆E that connects every two vertices in Xi
for every i = 1, . . . , ℓ.
The Steiner forest problem can be formulated as the set connector problem
by setting each family Vi of vertex subsets as {{u}, {v}}, where u, v ∈Xj, j =
1, . . . , ℓ. Our algorithm to the set connector problem attains the approximation
factor of 2α = 2, which coincides with the prior best result on the Steiner forest
problem [11].
5.3
Group Steiner Tree Problem
The group Steiner tree problem is a generalization of the Steiner tree problem.
It is formulated as follows.
Group Steiner tree problem
Given an undirected graph G = (V, E), an edge cost c : E →Q+, and a family
U ⊆2V of vertex subsets, ﬁnd a minimum cost tree T ⊆E which spans at least
one vertex in every X ∈U.
The group Steiner tree problem was introduced by G. Reich and P. Wid-
mayer [24]. Their motivation came from the wire routing with multi-port ter-
minals in VLSI design. After their work, it turned out that this problem has a
close relationship with the Steiner tree problem both in undirected graphs and
in directed graphs [13,28]. In addition to the Steiner tree problem, the problem
is known to generalize several important other problems such as the tree cover
problem [1,8,9,18], the terminal (full) Steiner tree problem [6,19,20], and the
set cover problem. Especially a reduction from the set cover problem implies
that the group Steiner tree problem does not admit any approximation factor of
(1−o(1)) ln m unless NP ⊆DTIME(nlog log n), where m = |U| and n = |∪X∈U X|.
Besides this, E. Halperin and R. Krauthgamer [14] proved that the group Steiner
tree problem is hard to approximate within a factor better than Ω(log2−ϵ m) for
every ϵ > 0 unless NP problems have quasi-polynomial time Las-Vegas algo-
rithms. On the other hand, a (1 + ln m/2)√m-approximation algorithm was
proposed by C. D. Bateman et. al. [2]. Currently the best approximation factors
are O(log m log |V | log N) due to [3,4,10], and 2N(1 −1/n) due to P. Slav´ık [26],
where N = maxX∈U |X|.
Although the set connector problem resembles the group Steiner tree problem,
they are diﬀerent in the fact that the set connectors may be forests. However,

496
T. Fukunaga and H. Nagamochi
the group Steiner tree problem can be reduced to the set connector problem as
follows. Pick up a designated subset S ∈U. For each s ∈S, run the algorithm of
the set connector problem for the instance with G, c, and VU = {s, U}, U ∈U−S.
Then this provides the approximation factor of 2α = 2N. This approximation
factor almost coincides with Slav´ık’s result [26].
Theorem 9. The group Steiner tree problem can be approximated within a fac-
tor of 2 maxX∈V |X|.
⊓⊔
6
Concluding Remarks
In this paper, we have introduced the set connector problem as an important
generalization of previously known fundamental problems such as the Steiner
forest problem, and have presented a 2α-approximation algorithm to the prob-
lem, where α = max1≤i≤m(
X∈Vi |X|) −1. Our algorithm is based on the
2α-approximate integer decomposition property, which is proven via the set con-
nector decomposition theorem.
Some problems remain open yet. One is whether the set connector problem
admits the approximation factor better than 2α. In the example presented in
Section 4 for the tightness of the integrality gap, Vi consists of two vertex sub-
sets one of which is always singleton. Hence this does not deny the possibility
of a better approximation factor than 2 max1≤i≤m maxX∈Vi |X|. Constructing
combinatorial approximation algorithms for the set connector problem is also an
interesting and important issue.
Acknowledgement
This research was partially supported by the Scientiﬁc Grant-in-Aid from Min-
istry of Education, Culture, Sports, Science and Technology of Japan.
References
1. E. M. Arkin, M. M. Halld´orsson, and R. Hassin, Approximating the tree and tour
covers of a graph, Information Processing Letters, 47 (1993), pp. 275–282.
2. C. D. Bateman, C. S. Helvig, G. Robins, and A. Zelikovsky, Provably good routing
tree construction with multi-port terminals, in Proceedings of the 1997 International
Symposium on Physical Design, 1997, pp. 96–102.
3. M. Charikar, C. Chekuri, A. Goel, and S. Guha, Rounding via trees: deterministic
approximation algorithms for group Steiner trees and k-median, in Proceedings
of the thirtieth Annual ACM Symposium on Theory of Computing, 1998, pp.
114–123.
4. C. Chekuri, G. Even, and G. Kortsarzc, A greedy approximation algorithm for the
group Steiner problem, Discrete Applied Mathematics, 154 (2006), pp. 15–34.
5. C. Chekuri and F. B. Shepherd, Approximate integer decompositions for undirected
network design problems. Manuscript, 2004.

The Set Connector Problem in Graphs
497
6. D. E. Drake and S. Hougardy, On approximation algorithms for the terminal
Steiner tree problem, Information Processing Letters, 89 (2004), pp. 15–18.
7. A. Frank, On a theorem of Mader, Discrete Mathematics, 191 (1992), pp. 49–57.
8. T. Fujito, On approximability of the independent/connected edge dominating set
problems, Information Processing Letters, 79 (2001), pp. 261–266.
9. T. Fujito, How to trim an MST: A 2-approximation algorithm for minimum cost
tree cover, in Proceedings of Automata, Languages and Programming, 33rd In-
ternational vol. 4051 of Lecture Notes in Computer Science, Venice, Italy, 2006,
pp. 431–442.
10. N. Garg, G. Konjevod, and R. Ravi, A polylogarithmic approximation algorithm
for the group Steiner tree problem, Journal of Algorithms, 37 (2000), pp. 66–84.
11. M. X. Goemans and D. P. Williamson, A general approximation technique for
constrained forest problems, SIAM Journal on Computing, 24 (1995), pp. 296–317.
12. D. Gusﬁeld, Connectivity and edge-disjoint spanning trees, Information Processing
Letters, 16 (1983), pp. 87–89.
13. E. Halperin, G. Kortsarz, R. Krauthgamer, A. Srinivasan, and N. Wang, Integrality
ratio for group Steiner trees and directed steiner trees, in Proceedings of the Four-
teenth Annual ACM-SIAM Symposium on Discrete Algorithms, 2003, pp. 275–284.
14. E. Halperin and R. Krauthgamer, Polylogarithmic inapproximability, in Proceed-
ings of the Thirty-Fifth Annual ACM Symposium on Theory of Computing, 2003,
pp. 585–594.
15. T. Ishii and M. Hagiwara, Augmenting local edge-connectivity between vertices and
vertex subsets in undirected graphs, in Proceedings of 28th International Sympo-
sium on Mathematical Foundations of Computer Science, vol. 2747 of Lecture
Notes in Computer Science, 2003, pp. 490–499.
16. H. Ito, Node-to-area connectivity of graphs, Transactions of the Institute of Elec-
trical Engineers of Japan, 11C (1994), pp. 463–469.
17. H. Ito and M. Yokoyama, Edge connectivity between nodes and node-subsets, Net-
works, 31 (1998), pp. 157–164.
18. J. K¨onemann, G. Konjevod, O. Parekh, and A. Sinha, Improved approximations
for tour and tree covers, Algorithmica, 38 (2004), pp. 441–449.
19. G. Lin and G. Xue, On the terminal Steiner tree problem, Information Processing
Letters, 84 (2002), pp. 103–107.
20. C. L. Lu, C. Y. Tang, and R. C.-T. Lee, The full Steiner tree problem, Theoretical
Computer Science, 306 (2003), pp. 55–67.
21. W. Mader, A reduction method for edge-connectivity in graphs, Annals of Discrete
Mathematics, 3 (1978), pp. 145–164.
22. H. Miwa and H. Ito, Edge augmenting problems for increasing connectivity between
vertices and vertex subsets, in 1999 Technical Report of IPSJ, vol. 99-AL-66, 1999,
pp. 17–24.
23. Z. Nutov, Approximating connectivity augmentation problems, in Proceedings of
the 16th Annual ACM-SIAM Symposium on Discrete Algorithms, 2005, pp.
176–185.
24. G. Reich and P. Widmayer, Beyond Steiner’s problem: a VLSI oriented generaliza-
tion, in Proceedings of Graph-Theoretic Concepts in Computer Science, vol. 411
of Lecture Notes in Computer Science, 1990, pp. 196–210.
25. A. Schrijver, Combinatorial Optimization: Polyhedra and Eﬃciency, Springer,
2003.
26. P. Slav´ık, Approximation algorithms for set cover and related problems, PhD thesis,
University of New York, 1998.

498
T. Fukunaga and H. Nagamochi
27. E. Tardos, A strongly polynomial algorithm to solve combinatorial linear programs,
Operations Research, 34 (1986), pp. 250–256.
28. L. Zosin and S. Khuller, On directed Steiner trees, in Proceedings of the thirteenth
annual ACM-SIAM symposium on Discrete algorithms, 2002, pp. 59–63.

Author Index
Ahmed, Shabbir
410
Amb¨uhl, Christoph
130
Andersen, Kent
1
Anthony, Barbara M.
424
Atamt¨urk, Alper
16
Balas, Egon
89
Beier, Rene
53
Bonami, Pierre
89
Calinescu, Gruia
182
Chekuri, Chandra
182
Conforti, Michele
324, 352
Cunningham, William H.
158
Danna, Emilie
280
Dash, Sanjeeb
197, 337
Dey, Santanu S.
30
Di Summa, Marco
352
Dumitrescu, Adrian
119
Feige, Uriel
439
Fenelon, Mary
280
Friedman, Eric J.
68
Fukasawa, Ricardo
197, 225
Fukunaga, Takuro
484
Geelen, Jim
158
Gerards, Bert
324
Ghosh, Shubhashis
310
Goycoolea, Marcos
225
Gu, Zonghao
280
G¨unl¨uk, Oktay
197, 337
Gupta, Anupam
424
Hartvigsen, David
43
Itoko, Toshinari
267
Iwata, Satoru
267
Jain, Kamal
439
Kaibel, Volker
74
Kakimura, Naonori
397
Lasserre, Jean B.
367
Levi, Retsef
454
Li, Yanjun
43
Linderoth, Jeﬀ
104
Lodi, Andrea
337, 454
Louveaux, Quentin
1
Luedtke, James
410
Mahdian, Mohammad
439
Makai, M´arton
167
Mastrolilli, Monaldo
130
Mirrokni, Vahab
439
Mutsanas, Nikolaus
130
Nagamochi, Hiroshi
484
Nagano, Kiyohito
252
Narayanan, Vishnu
16
Nemhauser, George
410
O’Shea, Edwin
382
Orlin, James B.
240
Ostrowski, James
104
P´al, Martin
182
Pap, Gyula
167
Peinhardt, Matthias
74
Pfetsch, Marc E.
74
Rendl, Franz
295
Richard, Jean-Philippe P.
30, 210
Rinaldi, Giovanni
295
R¨oglin, Heiko
53
Rossi, Fabrizio
104
Roughgarden, Tim
469
Seb˝o, Andr´as
382
Shmoys, David B.
145
Smriglio, Stefano
104
Sozio, Mauro
145
Sundararajan, Mukund
469
Svensson, Ola
130
Sviridenko, Maxim
454
Szab´o, J´acint
167
T´oth, Csaba D.
119

500
Author Index
V¨ocking, Berthold
53
Vondr´ak, Jan
182
Weismantel, Robert
1
Wiegele, Angelika
295
Wolsey, Laurence A.
1, 352
Wunderling, Roland
280
Zambelli, Giacomo
324
Zeng, Bo
210
Zeron, Eduardo S.
367


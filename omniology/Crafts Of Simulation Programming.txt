Crafts of
Simulation
Programming

This page intentionally left blank
This page intentionally left blank

NEW JERSEY  ‚Ä¢  LONDON  ‚Ä¢  SINGAPORE  ‚Ä¢  BEIJING  ‚Ä¢  SHANGHAI  ‚Ä¢  HONG KONG  ‚Ä¢  TAIPEI  ‚Ä¢  CHENNAI ‚Ä¢  TOKYO
World ScientiÔ¨Åc
Crafts of
Simulation
Programming
E Jack Chen

Published by
:RUOG6FLHQWL¬øF3XEOLVKLQJ&R3WH/WG
7RK7XFN/LQN6LQJDSRUH
86$RI¬øFH:DUUHQ6WUHHW6XLWH+DFNHQVDFN1-
8.RI¬øFH6KHOWRQ6WUHHW&RYHQW*DUGHQ/RQGRQ:&++(
Library of Congress Cataloging-in-Publication Data
1DPHV&KHQ(-DFN
7LWOH&UDIWVRIVLPXODWLRQSURJUDPPLQJE\(-DFN&KHQ%$6)&RUSRUDWLRQ86$
'HVFULSWLRQ>+DFNHQVDFN@1HZ-HUVH\:RUOG6FLHQWL¬øF_

,QFOXGHVELEOLRJUDSKLFDOUHIHUHQFHVDQGLQGH[
,GHQWL¬øHUV/&&1_,6%1KDUGFRYHUDONSDSHU
6XEMHFWV/&6+&RPSXWHUVLPXODWLRQ
&ODVVL¬øFDWLRQ/&&4$&&_''&GF
/&UHFRUGDYDLODEOHDWKWWSOFFQORFJRY
British Library Cataloguing-in-Publication Data
$FDWDORJXHUHFRUGIRUWKLVERRNLVDYDLODEOHIURPWKH%ULWLVK/LEUDU\
&RS\ULJKW¬ãE\:RUOG6FLHQWL¬øF3XEOLVKLQJ&R3WH/WG
$OOULJKWVUHVHUYHG7KLVERRNRUSDUWVWKHUHRIPD\QRWEHUHSURGXFHGLQDQ\IRUPRUE\DQ\PHDQV
HOHFWURQLFRUPHFKDQLFDOLQFOXGLQJSKRWRFRS\LQJUHFRUGLQJRUDQ\LQIRUPDWLRQVWRUDJHDQGUHWULHYDO
system now known or to be invented, without written permission from the publisher.
)RUSKRWRFRS\LQJRIPDWHULDOLQWKLVYROXPHSOHDVHSD\DFRS\LQJIHHWKURXJKWKH&RS\ULJKW&OHDUDQFH
&HQWHU,QF5RVHZRRG'ULYH'DQYHUV0$86$,QWKLVFDVHSHUPLVVLRQWRSKRWRFRS\
LVQRWUHTXLUHGIURPWKHSXEOLVKHU
3ULQWHGLQ6LQJDSRUH

In memory of my grandmother

This page intentionally left blank
This page intentionally left blank

Preface
Computer simulation is the discipline of studying a wide range of models
of real-world systems by numerical evaluation using software designed to
imitate the system‚Äôs operations or characteristics. That is, computer sim-
ulation is the process of designing and creating a computerized model of a
real or proposed system for the purpose of conducting experiments to give
us a better understanding of the behavior of the system under study for a
given set of conditions. Simulation output usually consists of one or more
random variables because of the stochastic nature of the output data; and
output analysis refers to the examination of the data generated by a simu-
lation. Simulation studies have been used to investigate the characteristics
of systems, for example, the probability of a machine breakdown.
There are many books on simulation programming with simulation mod-
eling languages, e.g., Arena, ProModel. Those books provide nice introduc-
tions of computer simulation and how to build simulation models with high-
level languages. On the other hand, we discuss simulation programming in
general-purpose languages, namely, C. This approach is highly customiz-
able and Ô¨Çexible, but also painfully tedious and error-prone since models
had to be coded pretty much from scratch every time. Nevertheless, by
learning to simulate in a general-purpose language, one will have a greater
understanding of how simulations actually work. Furthermore, we provide
the derivation and basis of various algorithms. That is, we investigate the
engine under the hood of the simulation modeling languages. The chapters
are organized as follows.
‚Ä¢ Chapter 1 discusses the algorithm of generating uniform random
numbers, random variates, and some utilities.
‚Ä¢ Chapter 2 discusses the eÔ¨Äect of sample sizes and stopping rules
vii

viii
Crafts of Simulation Programming
for steady-state simulations.
‚Ä¢ Chapter 3 presents a procedure to manufacture independent and
identically distributed batch means so that classical statistical tech-
nique of constructing conÔ¨Ådence intervals can be used.
‚Ä¢ Chapter 4 reviews the fundamentals of order statistics and how
they are applied in simulation.
‚Ä¢ Chapter 5 discusses order statistics from correlated normal random
variables and its relationship with ranking and selection.
‚Ä¢ Chapter 6 presents procedures to estimate multiple quantiles via
empirical histograms of the underlying distributions.
‚Ä¢ Chapter 7 presents procedures to construct metamodels, which can
be used as a surrogate to study the underlying system.
‚Ä¢ Chapter 8 presents procedures to estimate density of underlying
distributions.
‚Ä¢ Chapter 9 presents procedures to compare two normal populations.
‚Ä¢ Chapter 10 reviews ranking and selection procedures as well as
multiple comparisons.
‚Ä¢ Chapter 11 compares the indiÔ¨Äerence-zone selection procedures
with optimal computation allocation strategy.
‚Ä¢ Chapter 12 presents some insight of using common random num-
bers with selection procedures to increase the probability of correct
selection.
‚Ä¢ Chapter 13 discuss using parallel and distributed simulation to in-
crease the capacity of simulation studies.
‚Ä¢ Chapter 14 reviews multiple-objective selection procedures and a
Pareto set.
‚Ä¢ Chapter 15 reviews a generic selection-with-constraints procedure.
The constraints can be based on a standard or a control.
E. J. Chen

Contents
Preface
vii
1.
Basic Simulation Programming
1
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Random Numbers Generators . . . . . . . . . . . . . . . .
1
1.2.1
Basic Generators . . . . . . . . . . . . . . . . . . .
4
1.2.2
The Need for Multiple Substreams . . . . . . . . .
5
1.2.3
Computing (a √ó s) mod m
. . . . . . . . . . . . .
6
1.2.4
Computing the Jumping Matrices . . . . . . . . .
6
1.2.5
A Random Number Package
. . . . . . . . . . . .
7
1.2.6
Jumping Backward
. . . . . . . . . . . . . . . . .
9
1.3
Examples of Using Random Number Generator . . . . . .
10
1.4
Nonuniform Random Variates . . . . . . . . . . . . . . . .
13
1.4.1
Random Variates of Various Distributions . . . . .
14
1.4.2
Correlated Random Variates . . . . . . . . . . . .
18
1.5
Utilities
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.5.1
Numerical Approximation of Normal Distribution
20
1.5.2
Quantile of Normal Distribution . . . . . . . . . .
21
1.5.3
Quantile of t Distribution . . . . . . . . . . . . . .
22
1.5.4
Quantile of Chi-square Distribution . . . . . . . .
23
1.5.5
Standard Deviation . . . . . . . . . . . . . . . . .
25
1.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
26
2.
Sample Sizes and Stopping Rules
27
2.1
DeÔ¨Ånitions . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.2
Batch-Means Method . . . . . . . . . . . . . . . . . . . . .
32
ix

x
Crafts of Simulation Programming
2.3
Determining the Simulation Run Length . . . . . . . . . .
33
2.3.1
The von Neumann Test of Independence
. . . . .
33
2.3.2
A Source Code of the von Neumann Test . . . . .
33
2.3.3
The Runs Test of Independence
. . . . . . . . . .
34
2.3.4
A Source Code of Runs Up Test . . . . . . . . . .
36
2.3.5
An Implementation of Determining the Simulation
Run Length
. . . . . . . . . . . . . . . . . . . . .
38
2.4
Constructing the ConÔ¨Ådence Interval . . . . . . . . . . . .
38
2.5
A Correlation Adjustment . . . . . . . . . . . . . . . . . .
39
2.6
An Implementation of Batch-Means Method . . . . . . . .
39
2.7
An Illustration of Allocated Sample Sizes
. . . . . . . . .
43
2.8
Empirical Experiments . . . . . . . . . . . . . . . . . . . .
43
2.8.1
Experiment 1 . . . . . . . . . . . . . . . . . . . . .
46
2.8.2
Experiment 2 . . . . . . . . . . . . . . . . . . . . .
47
2.8.3
Experiment 3 . . . . . . . . . . . . . . . . . . . . .
48
2.9
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.
Generating Independent and Identically Distributed
Batch Means
51
3.1
Discussion of Batch-Means Method . . . . . . . . . . . . .
51
3.2
Generating Independent and Normally Distributed Batch
Means . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.2.1
Validation of Normality . . . . . . . . . . . . . . .
52
3.2.2
A Source Code of Normality test . . . . . . . . . .
53
3.2.3
Batch Means Variance Estimator . . . . . . . . . .
54
3.2.4
The Implementation . . . . . . . . . . . . . . . . .
56
3.2.5
Discussions of Batch-Means Procedures . . . . . .
58
3.3
Empirical Experiments . . . . . . . . . . . . . . . . . . . .
59
3.3.1
Experiment 1: Independence and Normality Tests
60
3.3.2
Experiment 2: Batch Sizes Determination . . . . .
62
3.3.3
Experiment 3: Coverages of ConÔ¨Ådence Interval
.
64
3.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
66
4.
Distributions of Order Statistics
69
4.1
Joint and Conditional Distributions of Order Statistics . .
72
4.2
Using Range Statistics to Perform Equivalence Tests . . .
74
4.2.1
IndiÔ¨Äerence-Zone Selection . . . . . . . . . . . . .
74
4.2.2
Variance of Weighted Sample Means . . . . . . . .
75

Contents
xi
4.2.3
EÔ¨Äects of the IndiÔ¨Äerence Amount and Sample Size
76
4.2.4
Equivalence Tests
. . . . . . . . . . . . . . . . . .
77
4.2.5
ConÔ¨Ådence Interval Half Width of Interest
. . . .
79
4.3
Statistical Analysis of the Range
. . . . . . . . . . . . . .
80
4.3.1
Simulating the Sample Range . . . . . . . . . . . .
81
4.3.2
Estimating Quantiles of the Range . . . . . . . . .
82
4.4
Empirical Experiments . . . . . . . . . . . . . . . . . . . .
84
4.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
85
5.
Order Statistics from Correlated Normal Random Variables
87
5.1
Order Statistics of Correlated Random Variables
. . . . .
88
5.1.1
Method of Evaluation of the Percentage Points . .
89
5.2
Applications of Correlated Order Statistics . . . . . . . . .
90
5.2.1
Multiple Comparisons with a Control . . . . . . .
90
5.2.2
Multiple Decision (Ranking and Selection)
. . . .
92
5.2.3
Multiple Comparisons with a Control: Unknown
Equal Variances . . . . . . . . . . . . . . . . . . .
92
5.2.4
Multiple Comparisons with a Control: Unknown
Unequal Variances . . . . . . . . . . . . . . . . . .
93
5.3
Empirical Experiments . . . . . . . . . . . . . . . . . . . .
94
5.3.1
Experiment 1: Known Equal Variances . . . . . .
94
5.3.2
Experiment 2: Unknown Equal Variances . . . . .
95
5.3.3
Experiment 3: Unknown Unequal Variances
. . .
95
5.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
97
6.
Histogram and Quasi-Independent Procedure
99
6.1
Introduction and DeÔ¨Ånitions . . . . . . . . . . . . . . . . .
101
6.1.1
The Natural Estimators . . . . . . . . . . . . . . .
103
6.1.2
Proportion Estimation
. . . . . . . . . . . . . . .
104
6.1.3
Quantile Estimation . . . . . . . . . . . . . . . . .
105
6.2
Methodologies . . . . . . . . . . . . . . . . . . . . . . . . .
106
6.2.1
Determining the Simulation Run Length
. . . . .
106
6.2.2
Histogram Approximation
. . . . . . . . . . . . .
106
6.2.3
Two-Phase Quantile Estimation . . . . . . . . . .
110
6.2.4
A Source Code of Quantile Estimation
. . . . . .
113
6.3
Empirical Experiments . . . . . . . . . . . . . . . . . . . .
120
6.3.1
Independent Sequences . . . . . . . . . . . . . . .
120
6.3.2
Correlated Sequences
. . . . . . . . . . . . . . . .
122

xii
Crafts of Simulation Programming
6.3.3
A Practical Application . . . . . . . . . . . . . . .
125
6.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
127
7.
Metamodels
129
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . .
129
7.2
Constructing Metamodels of Quantiles . . . . . . . . . . .
131
7.3
Constructing Quantile ConÔ¨Ådence Interval . . . . . . . . .
133
7.4
Empirical experiments . . . . . . . . . . . . . . . . . . . .
133
7.4.1
Choosing the Design Points . . . . . . . . . . . . .
133
7.4.2
Estimating Quantiles of Moving-Average and Au-
toregressive
Processes
via
Non-functional-form
Metamodels
. . . . . . . . . . . . . . . . . . . . .
134
7.4.3
Estimating Quantiles of Queuing Systems via Non-
functional-form Metamodels
. . . . . . . . . . . .
136
7.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
138
8.
Density Estimation
141
8.1
Theoretical Basis . . . . . . . . . . . . . . . . . . . . . . .
142
8.1.1
Empirical Distribution Functions . . . . . . . . . .
142
8.1.2
The Density Estimator
. . . . . . . . . . . . . . .
144
8.1.3
The Complication of Lack of Independence . . . .
146
8.2
An Implementation . . . . . . . . . . . . . . . . . . . . . .
146
8.2.1
Determine the Bandwidth . . . . . . . . . . . . . .
147
8.2.2
Determine the Sample Size . . . . . . . . . . . . .
148
8.2.3
Density ConÔ¨Ådence Interval . . . . . . . . . . . . .
149
8.2.4
The Density-Estimation Procedure . . . . . . . . .
150
8.3
Empirical Experiments . . . . . . . . . . . . . . . . . . . .
151
8.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
157
9.
Comparing Two Alternatives
161
9.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . .
161
9.1.1
Inference Procedures of Two Means . . . . . . . .
161
9.1.2
Null Hypothesis Tests of Equivalence
. . . . . . .
163
9.2
Methodology
. . . . . . . . . . . . . . . . . . . . . . . . .
164
9.2.1
A Weighted-Sample-Means Approach . . . . . . .
164
9.2.2
Fix the Value of Œ≤ = Œ±/2 of Null Hypothesis Tests
165
9.3
Empirical Experiments . . . . . . . . . . . . . . . . . . . .
167
9.3.1
Experiment 1: DiÔ¨Äerence of Means . . . . . . . . .
167

Contents
xiii
9.3.2
Experiment 2: Null Hypothesis of Equal Means
.
169
9.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
170
10. Ranking and Selection
171
10.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . .
172
10.1.1
Generalized Subset Selection . . . . . . . . . . . .
172
10.1.2
Order Statistics of Continuous Distributions
. . .
173
10.1.3
A Review of ConÔ¨Ådence Interval Half Width . . .
175
10.1.4
ConÔ¨Ådence Interval Half Width of Interest
. . . .
176
10.1.5
Adjustment of the DiÔ¨Äerence of Sample Means . .
177
10.1.6
The Source Code of Computing Additional Sample
Size . . . . . . . . . . . . . . . . . . . . . . . . . .
179
10.1.7
A Sequential Ranking and Selection Procedure
(SRS)
. . . . . . . . . . . . . . . . . . . . . . . .
180
10.2
Some Extensions of Selection of Continuous Distributions
182
10.2.1
Restricted Subset Selection . . . . . . . . . . . . .
182
10.2.2
An IndiÔ¨Äerence-Zone Procedure to Select Only
and/or All The Best Systems . . . . . . . . . . . .
183
10.2.3
Ratio Statistics of Variance of Normally Dis-
tributed Variables . . . . . . . . . . . . . . . . . .
186
10.2.4
Multiple Comparisons with the Best . . . . . . . .
190
10.3
Lognormally Distributed Samples . . . . . . . . . . . . . .
191
10.3.1
The Property of the Constant hL
. . . . . . . . .
192
10.4
Other Approach of Selection Procedures . . . . . . . . . .
195
10.5
Empirical Experiments . . . . . . . . . . . . . . . . . . . .
196
10.5.1
Experiment 1: Normal Populations
. . . . . . . .
196
10.5.2
Experiment 2: Exponential Populations . . . . . .
197
10.5.3
Experiment 3: Lognormal Populations . . . . . . .
197
10.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
199
11. Computing Budget Allocation of Selection Procedures
201
11.1
Problem Statement . . . . . . . . . . . . . . . . . . . . . .
202
11.2
A Heuristic Computing Budget Allocation Rule . . . . . .
203
11.2.1
ConÔ¨Ådence Interval Half-Width and Computing
Budget . . . . . . . . . . . . . . . . . . . . . . . .
207
11.2.2
Maximizing Probability of Correction Selection
with a Given Computing Budget . . . . . . . . . .
209
11.2.3
Optimal Computing Budget Allocation (OCBA) .
211

xiv
Crafts of Simulation Programming
11.3
Empirical Experiments . . . . . . . . . . . . . . . . . . . .
212
11.3.1
Experiment 1 Equal Variances . . . . . . . . . . .
213
11.3.2
Experiment 2 Increasing Variances . . . . . . . . .
214
11.3.3
Experiment 3 Decreasing Variances
. . . . . . . .
216
11.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
216
12. Using Common Random Numbers with Selection Procedures
219
12.1
Common Random Numbers . . . . . . . . . . . . . . . . .
219
12.2
The Basis of Correlated Order Statistics . . . . . . . . . .
220
12.2.1
Using CRNs with Dudewicz and Dalal‚Äôs Procedure 220
12.2.2
Subset Selection with CRN . . . . . . . . . . . . .
222
12.3
Empirical Experiments . . . . . . . . . . . . . . . . . . . .
225
12.3.1
Experiment 1: All Systems are Correlated
. . . .
225
12.3.2
Experiment 2: Best System is Independent with
Others
. . . . . . . . . . . . . . . . . . . . . . . .
226
12.3.3
Experiment 3: Best System is Negatively Corre-
lated with Others
. . . . . . . . . . . . . . . . . .
229
12.3.4
Experiment 4: Unequal Variances
. . . . . . . . .
229
12.3.5
Experiment 5: Subset Selection - All Systems are
Correlated
. . . . . . . . . . . . . . . . . . . . . .
231
12.3.6
Experiment 6: Subset Selection - Independence Be-
tween Groups 1
. . . . . . . . . . . . . . . . . . .
233
12.3.7
Experiment 7: Subset Selection - Independence Be-
tween Groups 2
. . . . . . . . . . . . . . . . . . .
233
12.3.8
Experiment 8: Subset Selection - Unequal Vari-
ances
. . . . . . . . . . . . . . . . . . . . . . . . .
233
12.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
236
13. Parallel and Distributed Simulation
237
13.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . .
237
13.2
Parallel and Distributed Selection . . . . . . . . . . . . . .
238
13.3
The Framework . . . . . . . . . . . . . . . . . . . . . . . .
240
13.4
Selection with All Pairwise Comparisons . . . . . . . . . .
241
13.5
Empirical Experiments . . . . . . . . . . . . . . . . . . . .
243
13.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
245
14. Multi-Objective Selection
247
14.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . .
248

Contents
xv
14.1.1
A Multi-Objective Selection Procedure
. . . . . .
249
14.2
Methodologies . . . . . . . . . . . . . . . . . . . . . . . . .
250
14.2.1
Prolog . . . . . . . . . . . . . . . . . . . . . . . . .
250
14.2.2
The Strategy . . . . . . . . . . . . . . . . . . . . .
251
14.2.3
The Incomplete Pareto Set Selection Procedure
.
252
14.2.4
The Two-Stage Pareto Set Selection Procedure . .
253
14.2.5
Incorporating IndiÔ¨Äerence-Zone
. . . . . . . . . .
254
14.3
Empirical Experiments . . . . . . . . . . . . . . . . . . . .
255
14.3.1
Experiment 1: The Parameter mp = 2 . . . . . . .
255
14.3.2
Experiment 2: The Parameter mp = 3 . . . . . . .
256
14.3.3
Experiment 3: The Parameter mp = 3 . . . . . . .
257
14.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
257
15. Generic Selection with Constraints
261
15.1
Methodologies . . . . . . . . . . . . . . . . . . . . . . . . .
262
15.1.1
Multi-Objective Selection . . . . . . . . . . . . . .
262
15.1.2
A Generic Selection-With-Constraints Procedure .
263
15.1.3
Variance as the Constraint . . . . . . . . . . . . .
266
15.1.4
Variance as the Primary Performance Measure . .
267
15.2
Empirical Experiments . . . . . . . . . . . . . . . . . . . .
268
15.2.1
Selection With Constraints . . . . . . . . . . . . .
268
15.2.2
Variance as the Constraint . . . . . . . . . . . . .
270
15.2.3
Variance as the Primary Performance Measure . .
271
15.3
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . .
274
Appendix A
Tables of Critical Constants
275
Bibliography
277
Index
285

Chapter 1
Basic Simulation Programming
1.1
Introduction
Computer simulation is the process of designing and creating a comput-
erized model of a real or proposed system for the purpose of conducting
experiments to give us a better understanding of the behavior of the sys-
tem under study for a given set of conditions. Simulation studies have been
used to investigate the characteristics of systems, to assess and analyze
risks, for example, the probability of a machine breakdown.
Simulation studies are typically proceed by transforming in a more or
less complicated way of a sequence of numbers between 0 and 1 produced by
a pseudorandom generator into an observation of the measure of interest. A
facility for generating sequences of pseudorandom numbers is a fundamental
part of computer simulation systems.
1.2
Random Numbers Generators
A portable set of software utilities is described for uniform random-number
generation. It provides for multiple generators (streams) running simul-
taneously, and each generator (stream) has its sequence of numbers par-
titioned into many long disjoint contiguous substreams. Simple procedure
calls allow users to make any generator ‚Äújump‚Äù ahead/back v steps (random
numbers). Implementation issues are discussed. An eÔ¨Écient and portable
code is also provided to implement the package.
A collection of random variables x1, x2, . . . , xn is a random sample if
they are independent and identically distributed (i.i.d.). True random num-
bers cannot be produced by a deterministic algorithm, and hence, random
numbers generated by using a recursive equation are referred to as pseudo-
1
by U
only.

2
Crafts of Simulation Programming
random numbers. Usually, in practice, such deterministic algorithms pro-
duce a deterministic sequence of values, but externally these values should
appear to be drawn independently from a uniform distribution between 0
and 1 (i.e., U(0, 1)). Furthermore, multiple independent streams of random
numbers are often required in simulation studies, for instance, to facilitate
synchronization for variance-reduction purposes, and for making indepen-
dent replications. A random number generator (RNG) is an algorithm that
starting from an initial seed (or seeds), produces a stream of numbers that
behaves as if it was a random sample when analyzed using statistical tests.
The RNG is closely related to the Deterministic Random Bit Generators
(DRBGs). See [L‚ÄôEcuyer (1990)] and references therein for more informa-
tion on RNGs. We describe a portable set of software utilities for uniform
random-number generation. It provides for multiple generators (streams)
running simultaneously, and each generator (stream) has its sequence of
numbers partitioned into many long disjoint contiguous substreams. The
basic underlying generator CMRG (Combined Multiple Recursive Gener-
ator) combines two multiple recursive random number generators with a
period length of approximately 2191 (‚âà3.1 √ó 1057), good speed, and excel-
lent theoretical properties. See [L‚ÄôEcuyer et al. (2002)].
There are a number of methods for generating the random numbers, of
which the most popular are the congruential methods (mixed, multiplica-
tive, and additive). The (mixed) linear congruential generators (LCGs) are
deÔ¨Åned by
xi = (axi‚àí1 + c) mod m, ui = xi/m, x0 ‚àà{1, ¬∑ ¬∑ ¬∑ , m ‚àí1}, i > 0.
Here m (the modulus) is a positive integer (usually a very large primary
number), a (the multiplier) ‚àà{0, 1, ¬∑ ¬∑ ¬∑ , m ‚àí1} and c (the increment) is
a nonnegative integer. This mathematical notation signiÔ¨Åes that xi is the
remainder of (axi‚àí1 + c) divided by m.
Hence, xi ‚àà{0, 1, ¬∑ ¬∑ ¬∑ , m ‚àí1}.
Thus, random variable ui is a uniform 0, 1 variable. Note that xi+v =
(avxi +c(av ‚àí1)/(a‚àí1)) mod m. Hence, every xi is completely determined
by m, a, c, and x0. The sequence xi repeats once it returns to a previously
visited value. The period of a generator is the length of a generated steam
before it begins to repeat. If u0 = up (where p > 0), then the length p
is called the period . The longest possible period for a LCG is m, i.e., m
represents the desired number of diÔ¨Äerent values that could be generated for
the random numbers. Hence, the modulus m is often taken as a large prime
number close to the largest integer directly representable on the computer
(i.e., equal or near 231 ‚àí1 for 32-bit computers). If p = m, we say that the
by U
only.

Basic Simulation Programming
3
generator has full period. The required conditions on how to choose m, a,
and c so that the corresponding LCGs will have full period are known, see
[Knuth (1998)] or [Law (2014)].
When c > 0, the LCGs are called mixed LCGs. When c = 0, xi =
axi‚àí1 mod m, ui = xi/m, x0 ‚àà{1, ¬∑ ¬∑ ¬∑ , m ‚àí1}, i > 0. These LCGs are
called multiplicative LCGs. Note that if xi = 0, then all subsequent xi are
identically 0. Thus, the longest possible period for a multiplicative LCG
is m ‚àí1. Furthermore, xi+v = avxi mod m. Most experts now recognize
that small LCGs with moduli around 231 or so should no longer be used as
general-purpose random-number generators. Not only can one exhaust the
period in a few minutes on a PC (personal computer), but more importantly
the poor structure of the points can dramatically bias simulation results for
sample sizes much smaller than the period length.
One way of extending the basic LCG is to combine two or more LCGs
through summation. Another way of extending the basic LCG is to use
a higher-order recursion. A multiple recursive random number generator
(MRG) , which goes from integer to integer according to the recursion
xi = (a1xi‚àí1 + ¬∑ ¬∑ ¬∑ + akxi‚àík) mod m, ui = xi/m.
A seed x0, . . . , xk‚àí2, xk‚àí1 ‚àà{1, . . . , m ‚àí1}, where i, k, and m are positive
integers, and a1, . . . , ak ‚àà{0, 1, . . . , m ‚àí1}. To increase the eÔ¨Éciency and
ease the implementation, the MRG algorithm usually set all but two ai‚Äôs to
0. Furthermore, the nonzero ai should be small. However, these conditions
are generally in conÔ¨Çict with those required for having a good lattice struc-
ture and statistical robustness. See [Law (2014)] on the lattice structure of
pseudorandom numbers. The longest possible period for a MRG is mk ‚àí1
[L‚ÄôEcuyer (1996)]. Other way of extending the basic LCG is the additive
congruential RNG (ACRON). The ACRON sets a = 1 and replace c by
some random number preceding xi in the sequence, for example, xi‚àí1 (so
that more than one seed is required to start calculating sequence). [Wilkr-
maratna (2008)] indicated that the ACRON is a special case of a multiple
recursive generator.
Other classes of RNGs are available, e.g., twisted generalized feedback
shift register generators [Matsumoto and Nishimura (1998)]. The RNGs
discussed so far are by computational methods. Another approach is by
physical methods, e.g., [Kanter et al.
(2010)].
The output based on a
physical method is usually unpredictable. Thus, this class of random bit
generators is commonly known as non-deterministic random bit genera-
tors (NRBGs). Those physical devices are unnecessary and unpractical for
by U
only.

4
Crafts of Simulation Programming
Monte Carlo methods, because deterministic algorithmic methods are much
more convenient. See [Law (2014)] for more information on other methods
of generating random numbers.
We discuss the algorithm and implementation of [L‚ÄôEcuyer
(1999)]
combined multiple recursive random number generator.
A combination
of generators can have a much longer period than any of its components.
Furthermore, with well-chosen parameters, the CMRG has good structural
properties and passes many statistical tests.
1.2.1
Basic Generators
A newly employed generator is the combined multiple recursive random
number generator, which goes from integer to integer according to the re-
cursion
xj,n = (aj,1xj,n‚àí1 + ¬∑ ¬∑ ¬∑ + aj,kxj,n‚àík) mod mj;
for
j = 1, . . . , J (1.1)
zn =
J

j=1
(Œ¥jxj,n) mod m1
(1.2)
un =
 zn/(m1 + 1)
if zn > 0
m1/(m1 + 1) if zn = 0.
(1.3)
Here J is the number of MRGs used in the CMRG, j, n, k, and mj are
positive integers, and each aj,k belongs to Zm = {0, 1, . . . , m ‚àí1}, Œ¥j are
integers, and the greatest common divisor of Œ¥j and mj is one for each j.
The transformation of un makes sure that un is never equal to 0 or 1 (oth-
erwise, trouble may arise, e.g., when taking the logarithm to generate an
exponential random variate). Assume that the mj are distinct primes and
that each recurrence j has a primitive characteristic polynomial, and thus
period length œÅj = mk
j ‚àí1 and the longest possible period for a CMRG is
œÅ1 ¬∑ ¬∑ ¬∑ œÅJ/2J‚àí1. The combined generator is a close approximation to a single
MRG with a modulus equal to the product of the moduli of the components
MRGs. Thus, the CMRG has the advantages associated with a larger mod-
ulus while permitting an implementation using smaller values. The CMRG
combining multiple recursive sequences provides an eÔ¨Écient way of imple-
menting random-number generators with long periods and good structural
properties. If the parameters are well chosen, such generators are statis-
tically far more robust than simple linear congruential generators that Ô¨Åt
into a single computer word [L‚ÄôEcuyer (1999)].
by U
only.

Basic Simulation Programming
5
1.2.2
The Need for Multiple Substreams
Many disjoint random number subsequences are often required in simula-
tion studies, for instance, 1) to make independent replications and/or; 2)
to associate distinct ‚Äústreams‚Äù of random numbers with diÔ¨Äerent sources of
randomness in the system to facilitate synchronization for variance reduc-
tion. To produce such ‚Äústreams,‚Äù diÔ¨Äerent seeds (values of vector Si) must
be obtained far enough apart in the sequence to insure that the streams
do not overlap. Selection of seeds should also consider statistical properties
between streams, such as apparent independence. In other words, given any
Xi (with seed Si) and positive integer v, there should be a quick way to
compute Xi+v (with seed Si+v ) without generating all intermediate values.
The availability of eÔ¨Écient jump-ahead methods is very useful because it
permits one to partition the RNG sequence into long disjoint stream and
substreams of random numbers. Most packages oÔ¨Äer no facility for jump-
ing ahead directly from Xi to Xi+v or to compute distant seeds eÔ¨Éciently.
Many simulation languages oÔ¨Äer a limited number of streams, all based on
the same generator, but using Ô¨Åxed starting seeds set say 100,000 values
apart. This provides relatively low Ô¨Çexibility. Suppose, for instance, that
you want to perform independent pairs of replications with common ran-
dom numbers across the conÔ¨Ågurations (i.e., between any two runs of the
same pair) in order to compare two diÔ¨Äerent conÔ¨Ågurations of a system. To
insure proper synchronization, you want every generator to start from the
same seed in both runs of the same pair. However, in general, these two
runs will make a diÔ¨Äerent number of calls to a generator, and programming
‚Äútricks‚Äù should be used to skip a proper amount of random numbers to
resynchronize the generators for the next pair without overlap in the ran-
dom number streams. This requires extra programming eÔ¨Äort and could
be error prone. Good software tools should ease the programmer‚Äôs task in
that respect. A simple procedure call should permit resetting a generator
to previous seed or jumping ahead to a new seed for the next run.
Of
course, the sequence of ‚Äúnew seeds‚Äù (one per run) should be the same of
both conÔ¨Ågurations for the system. Implementing such tools requires eÔ¨É-
cient ‚Äújumping ahead‚Äù facilities, which in turn ask for eÔ¨Écient procedures
to compute (a √ó s) mod m, where a and s are positive integers.
by U
only.

6
Crafts of Simulation Programming
1.2.3
Computing (a √ó s) mod m
Consider a 32-bit computer on which all integers between ‚àí232 ‚àí1 and
232‚àí1 (exclusive) are well represented. We want to compute (a√ós) mod m,
where a, s, and m are positive integers smaller than 232 ‚àí1.
Without
loss of generality, we assume that a < m and s < m (if not, replace a
and s by a mod m and s mod m, respectively).
In order to keep seeds
with 32-bit precision, all operations can not produce any number greater
than 253 (the IEEE-754 standard, i.e., the Ô¨Çoating-point numbers have at
least 53 bits of precision for the mantissa). Therefore, special algorithm is
needed to compute (a √ó s) mod m, where a and s are less than 232 (around
4.3 √ó 109).
The following algorithm is used.
Let a = a1 √ó 217 + a2 so
a √ó s = a1 √ó s √ó 217 + a2 √ó s. Therefore,
(a √ó s) mod m = (((a1 √ó s) mod m) √ó 217 + a2 √ó s) mod m,
where a1 < 215, so a1 √ó s < 247(= 215 √ó 232), and a2 < 217, so a2 √ó s < 249
(= 217 √ó 232). Because z = (a1 √ó s) mod m < 232, we have z √ó 217 < 249
so that all the intermediate terms in the above computations are less than
253. Therefore, all seed values will have exact accuracy.
1.2.4
Computing the Jumping Matrices
The initial state of substreams can be computed easily if jumping-ahead
facilities are available for the individual MRG components; that is, if an
eÔ¨Écient algorithm is available for computing the state of the MRG v steps
ahead of the current one, for large values of v. [L‚ÄôEcuyer (1990)] explained
one way of doing that, based on the fact that the MRG can be viewed as
a LCG in matrix form, whose state is a k-dimensional vector and whose
multiplier is a k √ó k matrix A. To jump ahead by v values, just multiply
the current state by (Av mod m). The matrix by (Av mod m) can be pre-
computed in time O(log v), using the divide-and-conquer algorithm [Knuth
(1998)]. The divide-and-conquer algorithm uses the following recursion:
Av mod m =
‚éß
‚é®
‚é©
A
if v = 1;
A(Av‚àí1 mod m) mod m
if v > 1, v odd;
(Av/2 mod m)(Av/2 mod m) mod m if v > 1, v even.
That is, for the MRG random number generator, Xi+v can be computed
directly from Xi using
Xi+v = (AvXi) mod m = (Av mod m)Xi mod m
(1.4)
by U
only.

Basic Simulation Programming
7
where
A =
‚éõ
‚éú
‚éú
‚éú
‚éù
0
1
¬∑ ¬∑ ¬∑ 0
...
...
... ...
0
0
¬∑ ¬∑ ¬∑ 1
ak ak‚àí1 ¬∑ ¬∑ ¬∑ a1
‚éû
‚éü
‚éü
‚éü
‚é†
(1.5)
is an invertible k √ó k matrix and
Xi =
‚éõ
‚éú
‚éú
‚éú
‚éù
xi
xi+1
...
xi+k‚àí1
‚éû
‚éü
‚éü
‚éü
‚é†
(1.6)
is a k √ó 1 vector.
When A has this special structure, the Ô¨Årst k ‚àí1 components of Xi
are obtained by shifting the last k ‚àí1 components of Xi‚àí1, and the last
component of Xi is a linear combination of the components of Xi‚àí1 ac-
cording to the MRG recursion [L‚ÄôEcuyer (1990)]. [Kroese et al. (2011)]
point out that an MRG can be interpreted and implemented as a matrix
multiplicative congruential generator.
1.2.5
A Random Number Package
We now propose a portable set of utilities for random number generation.
We consider a basic underlying generator of period p. Let s0 be the basic
seed (initial state) for this generator and s1, s2, . . . be its sequence of suc-
cessive states. Let T denote the transition function of the generator, that
is, the operator T : S ‚ÜíS such that T(si) = si+1, and T q its q-fold compo-
sition (T q(si) = si+q). Starting from states I1 = s0, I2 = sq = T q(I1), I3 =
s2q = T q(I2), . . . , IG = s(G‚àí1)q = T q(IG‚àí1), respectively. Each of these G
sequences corresponds to a ‚Äústream‚Äù, with length q. Note that q needs to
be a very large number, say no smaller than 109. Hence, there will be G
virtual RNGs. Ig is called the initial seed of stream g. At any moment
during program execution, stream g is in some state Cg, say, in its subse-
quence number r, that is, such that Cg = T r‚àí1(Ig). We call Cg the current
state of stream g.
If the product of aj,i(mj ‚àí1) is less than 253, then the integer aj,i
xj,i is always represented exactly in Ô¨Çoating point on a 32-bit computer
that supports the IEEE Ô¨Çoating-point arithmetic standard, with at least 53
bits of precision for the mantissa. The generator can then be implemented
by U
only.

8
Crafts of Simulation Programming
directly in Ô¨Çoating-point arithmetic, which is typically faster than an integer
arithmetic implementation. On the other hand, with this implementation,
the state of the generator is represented over 64k J bits, as opposed to 32k
J bits when the xj,i are represented as 32-bit integers.
The mrg32k3a of [L‚ÄôEcuyer (1999)] implemented a CMRG with 2 com-
ponents ordered 3, whose coeÔ¨Écients satisfy above condition. The moduli
and coeÔ¨Écients are m1 = 232 ‚àí209, a1,1 = 0, a1,2 = 1403580, a1,3 =
‚àí810728, m2 = 232 ‚àí22853, a2,1 = 527612, a2,2 = 0, a2,3 = ‚àí1370589. Its
period length is œÅ = (m3
1 ‚àí1)(m3
2 ‚àí1)/2 ‚âà2191 ‚âà3.1 √ó 1057. This imple-
mentation set Œ¥1 = ‚àíŒ¥2 = 1. The parameters have been chosen so that the
period is long, a fast implementation is available, and the generator per-
forms well with respect to the spectral test. Before this procedure is called
for the Ô¨Årst time, one must initialize the global variables s10, s11, s12 to
(exact) non-negative integer values less than m1 and not all zero, and s20,
s21, s22 to non-negative integers less than m2 and not all zero. The vectors
(s10, s11, s12) and (s20, s21, s22) are the initial values of (x1,0, x1,1, x1,2)
and (x2,0, x2,1, x2,2), respectively. They constitute the seed. Therefore, in
this implementation
x1,n = (1403580.0x1,n‚àí2 ‚àí810728.0x1,n‚àí3) mod 4294967087
x2,n = (527612.0x2,n‚àí1 ‚àí1370589.0x2,n‚àí3) mod 4294944443
Zn = (x1,n ‚àíx2,n) mod 4294967087
un =
 zn/4294967088
if zn > 0
4294967087/4294967088 if zn = 0.
(1.7)
The matrix A1 for (x1,0, x1,1, x1,2) to jump ahead one step is
A1 =
‚éõ
‚éù
0
1
0
0
0
1
‚àí810728 1403580 0
‚éû
‚é†
and the matrix A2 for (x2,0, x2,1, x2,2) to jump ahead one step is
A2 =
‚éõ
‚éù
0
1
0
0
0
1
‚àí1370589 0 527612
‚éû
‚é†.
The
following
is
an
implementation
of
the
CMRG
(L‚ÄôEcuyer‚Äôs
mrg32k3a) in C.
by U
only.

Basic Simulation Programming
9
#define norm 2.328306549295728e-10
// 1.0/(m1+1)
#define norm2 2.328318825240738e-10
// 1.0/(m2+1)
#define m1 4294967087.0
#define m2 4294944443.0
// the initial seed
double s[2][3] = {
{0.0,0.0,1.0},
{0.0,0.0,1.0}
};
double MRG32k3a()
{
long k;
double p;
p = 1403580.0 * s[0][1] - 810728.0 * s[0][0];
k = long ( p / m1); p -= k*m1; if (p < 0.0) p += m1;
s[0][0] = s[0][1]; s[0][1] = s[0][2]; s[0][2] = p;
p = 527612.0 * s[1][2] - 1370589.0 * s[1][0];
k = long ( p / m2); p -= k*m2; if (p < 0.0) p += m2;
s[1][0] = s[1][1]; s[1][1] = s[1][2]; s[1][2] = p;
if (s[0][2] <= s[1][2])
return ((s[0][2] - s[1][2] + m1) * norm);
else
return ((s[0][2] - s[1][2]) * norm);
}
1.2.6
Jumping Backward
The jump-back matrix also exists. This is true because each component
has a primitive characteristic polynomial (a necessary condition). Given a
vector Sn, one may jump back v steps to Sn‚àív. For example, the jump-
back-one-step matrix Bj is AœÅj‚àí1
j
mod mj , where œÅj is the period of the
jth MRG of the CMRG. As one might expect, Av
j √ó Bv
j mod mj turns out
by U
only.

10
Crafts of Simulation Programming
to be the identity matrix I, for v = 1, 2, . . . , œÅj. This equation is consistent
with the intuition that AœÅj‚àí1
j
mod mj = I. The matrix Bj generates the
same stream but in reverse order. As we can see from the new recursion
below, the values of the parameters bi,j are much larger than the original
ones, where bi,j are the parameters of the new CMRG. Furthermore, we no
longer have bi,j(mj ‚àí1) < 253 for all bij. Therefore, the original recursion
provides a more eÔ¨Écient generator. The matrix B1 for S1n to jump back
one step (i.e., jump ahead œÅ1 ‚àí1 step) is
B1 =
‚éõ
‚éù
184888585 0 1945170933
1
0
0
0
1
0
‚éû
‚é†
and the matrix B2 for S2n to jump back one step (i.e., jump ahead œÅ2 ‚àí1
step) is
B2 =
‚éõ
‚éù
0 360363334 4225571728
1
0
0
0
1
0
‚éû
‚é†.
The reverse stream follows the recursion
x1,n = (184888585 √ó x1,n+1 + 1945170933 √ó x1,n+3) mod 4294967087,
x2,n = (360366334 √ó x2,n+2 + 4225571728 √ó x2,n+3) mod 4294944443
= (360366334 √ó x2,n+2 ‚àí69372715 √ó x2,n+3) mod 4294944443.
1.3
Examples of Using Random Number Generator
In the following example, we illustrate how to use the functions in the
mrg32k32 RNG software package.
The initial seed of the main gener-
ator s0 is the starting point of the Ô¨Årst stream (i.e., the Ô¨Årst state)
I1. In the proposed package, the initial seed is set to the default value
(12345, 12345, 12345, 12345, 12345, 12345), but this value can be changed
by the user (via SetPackageSeed). Each time a new RngStream object is
created, its starting point (initial seed) Ig is set v = 2127 steps ahead of the
starting point of the last created object. A vector named nextSeed is used
to keep the seed values of the next created RngStream object (stream). For
example, the declaration ‚ÄúRngStream g;‚Äù creates a stream with Ig equal
to nextSeed and advances nextSeed by v steps. Because the initial seed
for each RngStream object is computed dynamically, no pre-computed list
of seeds is needed.
by U
only.

Basic Simulation Programming
11
The methods Reset* reset a given stream either to its initial state, or
to the beginning of its current substream, or to the beginning of its next
substream.
The method GetState returns the state of a stream.
One
can change the seed of a given stream, without modifying that of other
streams, by invoking SetSeed or AdvanceState.
However, after calling
SetSeed for a given stream, the initial states of the diÔ¨Äerent streams are no
longer spaced v values apart. Therefore, this method should be used only in
exceptional cases. The methods Reset* suÔ¨Éces for almost all applications.
The method RandU01 generates the uniform (pseudo)random numbers with
the seed as an input parameter and advance the state of the seed by one
step.
Example 1 shows how we generate a list of ten seed vectors. We set the
package seed to {327612383, 317095578, 14704821, 884064067, 1017894425,
16401881} by calling SetPackageSeed before instantiating any RngStream
object. Note that if SetPackageSeed is not executed, {12345, 12345, 12345,
12345, 12345, 12345} will be used as the default package seed. The dec-
laration ‚ÄúRngStream RngObj‚Äù is inside the for loop on i. Therefore, the
instance of the object dies before the next iteration, i.e., the object RngObj
is not available outside the for loop on i.
Each declaration will create
the RngObj instance with Ig = Cg = nextSeed and advance nextSeed
by 2127 steps. Thus, the output seeds will be 2127 apart. The procedure
‚ÄúWriteState()‚Äù prints the values of the seed.
by U
only.

12
Crafts of Simulation Programming
#include "RngStream.h"
int main ()
{
unsigned long seed[6] =
{ 327612383, 317095578, 14704821,
884064067, 1017894425, 16401881 };
RngStream::SetPackageSeed (seed);
for (int i = 1; i <= 10; ++i) {
RngStream RngObj;
RngObj.WriteState();
};
}
Example 2 shows how to apply some of the utilities supplied in the pack-
age. The declarations ‚ÄúRngStream RngObj1‚Äù and ‚ÄúRngStream RngObj2‚Äù
will create the RNG objects with
Ig = Cg = {12345, 12345, 12345, 12345, 12345, 12345}
and
Ig = Cg =
{3692455944, 1366884236, 2968912127, 335948734, 4161675175, 475798818},
respectively, i.e., 2127 steps apart.
The Ô¨Årst RNG object RngObj1 may
be dedicated to generate inter-arrival times while the second RNG object
RngObj2 may be dedicated to generate service times, for some queueing
system to be simulated. We generate Ô¨Åve inter-arrival times and Ô¨Åve ser-
vice times, then move each RNG to its next substream. This is repeated
ten times, thus yielding ten vectors, each containing Ô¨Åve inter-arrival times
and Ô¨Åve service times. Moreover, these ten vectors will be exactly the same
at iterations r = 0 and r = 1 of the outer for loop, because the state-
ments ‚ÄúRngObj1.ResetStartStream‚Äù and ‚ÄúRngObj2.ResetStartStream‚Äù
will reset the current seeds Cg of both RNG objects to the initial seed Ig.
The function ‚Äúdouble Rand()‚Äù returns a (pseudo)random number from
the uniform distribution over the interval (0, 1), after advancing the state
of the internal seed by one step.
by U
only.

Basic Simulation Programming
13
#include <math.h>
#include "RngStream.h"
int main ()
{
RngStream RngObj1;
RngStream RngObj2;
double interArrival;
double serviceTime;
for (int k = 0; k <= 1; ++k) {
for (int j = 1; j <= 10; ++j) {
for (int i = 1; i <= 5; ++i) {
interArrival = -log(1.0 - RngObj1.Rand());
serviceTime = -0.9 * log(1.0 - RngObj2.Rand());
};
RngObj1.ResetNextSubstream ();
RngObj2.ResetNextSubstream ();
};
RngObj1.ResetStartStream ();
RngObj2.ResetStartStream ();
};
}
1.4
Nonuniform Random Variates
To simulate the underlying system under study, various nonuniform ran-
dom variates are required. These random variates are used to simulate the
observations from the desired distribution. Random variates from distribu-
tions other than the uniform over [0,1] are generated by applying further
transformations to the output values ui of the uniform RNG. There is a
vast collection of literature on how to generate random variates from a given
distribution in order to run the simulation model. One of the popular ref-
erences is [Law (2014)], where many further references on this subject is
listed. The simplest way of generating a random variate X with distribu-
tion function F is using the inverse transformation. Based on the fact that
y = F(x) ‚àºU(0, 1), where ‚Äò‚àº‚Äô denotes ‚Äòis distributed as‚Äô, we generate a
by U
only.

14
Crafts of Simulation Programming
random variate y ‚àºU(0, 1) and return the value x = F ‚àí1(y) as a vari-
ate with distribution F. Here F ‚àí1 is the inverse distribution function (or
quantile function) and is deÔ¨Åned by
F ‚àí1(y) = inf{x : F(x) ‚â•y}.
The cumulative distribution function (cdf) of the inverse transform F ‚àí1(y)
is given by
Pr(F ‚àí1(y) ‚â§x) = Pr(y ‚â§F(x)) = F(x).
Figure 1.1 illustrates the algorithm. For the discrete case the inverse-
Fig. 1.1
Inverse-transform method for continuous variables
transform method can be done by Ô¨Ånding the smallest positive integer k
such that F(xk) ‚â•y and return X = xk. Here the discrete random variable
X taking values x1 < x2 < ¬∑ ¬∑ ¬∑ with probabilities p1, p2, ¬∑ ¬∑ ¬∑ and  pi = 1.
For more information of using inverse transform to generate random vari-
ates, see [Law (2014)]. Other methods are available when F ‚àí1 is diÔ¨Écult or
expensive to compute, e.g., composition, convolution, acceptance-rejection,
and special properties, see [Kroese et al. (2011)].
1.4.1
Random Variates of Various Distributions
In this section, we list the source code of generating various nonuniform
random variates.
by U
only.

Basic Simulation Programming
15
/*----------------------------------------------------------*/
/* Remark: Implementation of Marsaglia and Bray (1964)‚Äôs
*/
/*
polar method of generating N(0,1) random
*/
/*
variates.
*/
/*
*/
double RandStdNormal(double seed[6])
{
double v1, v2, temp;
double w = 2.0;
static double even = 0;
if ( even ) {
temp = even;
even = 0;
return (temp);
}
while ( w > 1 ) {
v1 = (2 * RandU01(seed)) - 1;
v2 = (2 * RandU01(seed)) - 1;
w = pow(v1, 2) + pow(v2, 2);
}
temp = sqrt (-2 * log (w) / w);
even = v2 * temp;
return (v1 * temp);
}
double RandNormal(double seed[6], double mean, double variance)
{
return (RandStdNormal(seed)*sqrt(variance) + mean);
}
/*----------------------------------------------------------*/
/* Remark: Implementation of generating exponential
*/
/*
distribution with parameter beta, i.e.,
*/
by U
only.

16
Crafts of Simulation Programming
/*
expon(beta).
*/
/*
*/
double RandExpon(double seed[6], double beta)
{
return (-beta * log(RandU01(seed)));
}
/*----------------------------------------------------------*/
/* Remark: Implementation of generating erlang distribution */
/*
with parameters beta, alpha.
*/
/*
*/
double RandErlang(double seed[6], double alpha, double beta)
{
double u=0.0;
int j;
for (j = 1; j <= floor(beta); ++j)
u += expon(seed, alpha);
return (u);
}
/*----------------------------------------------------------*/
/* Remark:
Implementation of Cheng (1977)‚Äôa algorithm of
*/
/*
generating gamma distribution
*/
/*
with parameters alpha, beta.
*/
/*
*/
#define M_E
2.718281828
double RandGamma(double seed[6], double alpha, double beta)
{
double a = 1 / sqrt(2*alpha-1);
double b = alpha - log(4.0);
double q = alpha + 1/a;
double theta = 4.5;
double d = 1+log(4.5);
by U
only.

Basic Simulation Programming
17
double u1 = RandU01(seed);
double u2 = RandU01(seed);
double v = a * log(u1/(1-u1));
double y = alpha * pow(M_E,v);
double z = pow(u1,2)*u2;
double w = b + q*v - y;
if (w + d >= theta * z)
return (y*beta);
if (w >= log(z))
return (y*beta);
return (RandGamma(seed, alpha, beta));
}
/*----------------------------------------------------------*/
/* Remark:
Implementation of generating beta distribution
*/
/*
with parameters alpha, beta.
*/
/*
*/
double RandBeta(double seed[6], double alpha, double beta)
{
double y1 = RandGamma(seed, alpha, 1);
AdvanceState(seed,127,0)
double y2 = RandGamma(seed, beta, 1);
return (y1/(y1+y2));
}
/*----------------------------------------------------------*/
/* Remark: Implementation of generating chi square
*/
/*
distribution with parameter df.
*/
/*
*/
double RandChi(double seed[6], int df)
{
by U
only.

18
Crafts of Simulation Programming
return (RandGamma(seed, (1.0*df)/2, 2));
}
/*----------------------------------------------------------*/
/* Remark: Implementation of generating weibull
*/
/*
distribution with parameters alpha, beta.
*/
/*
*/
double RandWeibull(double seed[6], double alpha, double beta)
{
return (alpha * pow(-1*log(RandU01(seed)), 1.0/beta));
}
/*----------------------------------------------------------*/
/* Remark:
Implementation of generating t distribution
*/
/*
with parameter df.
*/
/*
*/
double RandT(double seed[6], int df)
{
double y = RandNormal(seed, 0, 1);
AdvanceState(seed,127,0)
double z = RandChi(seed, df);
return (y/sqrt(z/df));
}
1.4.2
Correlated Random Variates
In this section, we list the source code of generating various correlated
random variates.
/*----------------------------------------------------------*/
/* Remark: Implementation of generating steady-state
*/
/*
autoregressive AR1 distribution with mean mu and */
/*
correlation coefficient phi.
The error term has */
/*
a N(0,1) distribution.
*/
/*
parameters: seed is the seed for the uniform number
*/
by U
only.

Basic Simulation Programming
19
/*
generator
*/
/*
phi is the correlation coefficient
*/
/*
mu is the expected value
*/
/*
x0 is the current value
*/
/*
*/
double ar1(double seed[6], double phi, double mu, double* x0)
{
*x0 = mu + phi*(*x0 - mu) + RandNormal(seed, 0.0, 1.0);
return (*x0);
}
/*----------------------------------------------------------*/
/* Remark: Implementation of generating steady-state Moving */
/*
Average MA1 distribution with correlation
*/
/*
coefficient phi, mean mu previous noise e, where */
/*
e has a N(0,1) distribution.
*/
/*
parameters: seed is the seed for the uniform number
*/
/*
generator
*/
/*
phi is the correlation coefficient
*/
/*
mu is the expected value
*/
/*
e is a normal(0,1) white noise
*/
/*
*/
double ma1(double seed[6], double phi, double mu, double *e)
{
double temp = RandNormal(seed, 0.0, 1.0);
double x0 = temp + mu + phi * (*e);
*e = temp;
return (x0);
}
/*----------------------------------------------------------*/
/* Remark: Implementation of generating the waiting time of */
/*
M/M/1 delay in queue with the arrival rate
*/
by U
only.

20
Crafts of Simulation Programming
/*
lambda and server rate mu.
*/
/*
parameters: seed is the seed for the uniform number
*/
/*
generator
*/
/*
mu is service rate
*/
/*
lambda is arrival rate
*/
/*
w is the waiting time of the previous customer
*/
/*
*/
double mm1que(double seed[6], double mu,
double lambda,double* w)
{
double a = RandExpon(seed,1.0/lambda);
AdvanceState(seed,127,0);
double s = RandExpon(seed,1.0/mu);
*w += s - a;
if (*w
< 0) *x0
= 0.0;
return (*w);
}
1.5
Utilities
In this section, we list several utilities that are often needed for simulation
studies. [Hastings (1955)] lists approximations of commonly used distri-
butions. We list the implementations of computing cdf of standard normal
distribution, quantile of normal, t, and chi-square distributions as well as
procedures to compute variance.
1.5.1
Numerical Approximation of Normal Distribution
There is no explicit closed form solution of normal cumulative distribu-
tion function.
The following is a numerical approximation proposed by
[Abramowiz and Stegun (1964)].
/*----------------------------------------------------------*/
/* Remark: Compute the value of standard normal probability */
/*
density function
*/
/*
parameters: x is the location
*/
by U
only.

Basic Simulation Programming
21
/*
*/
#define M_E
2.718281828
#define M_PI 3.14159265358979
double pdf_normal(double x)
{
return ( pow(M_E, -0.5*pow(x,2)) / sqrt(2 * M_PI) );
}
/*----------------------------------------------------------*/
/* Remark: Compute the value of standard normal cdf
*/
/*
parameters: z is the quantile
*/
/*
*/
double cdf_normal(double z)
{
if (z > 6.0) return (1.0);
// to guard against overflow
if (z < -6.0) return (0.0);
double b1 = 0.31938153;
double b2 = -0.356563782;
double b3 = 1.781477937;
double b4 = -1.821255978;
double b5 = 1.330274429;
double p = 0.2316419;
double c2 = 0.3989423;
double a=fabs(z);
double t = 1.0/(1.0+a*p);
double b = c2*exp((-z)*(z/2.0));
double n = ((((b5*t+b4)*t+b3)*t+b2)*t+b1)*t;
n = 1.0-b*n;
if (z < 0.0) n = 1.0-n;
return (n);
}
1.5.2
Quantile of Normal Distribution
/*----------------------------------------------------------*/
/* Remark: Compute the quantile of the standard normal dist */
by U
only.

22
Crafts of Simulation Programming
/*
parameters: beta between 0 and 1
*/
/*
*/
double quantile_normal(double beta)
{
double zz, n, x1, t;
x1 = beta;
if (beta < 0.5) beta = 1 - beta;
t = sqrt(-2 * log(1-beta));
zz = 2.515517 + t * (0.802853 + 0.010328 * t);
n = 1 + t * (1.432788 + t * (0.189269 + 0.001308*t));
zz = t - zz/n;
if (x1 < 0.5)
return (-zz);
else
return (zz);
}
Other
procedures
to
compute
quantile function of the standard normal distribution are available, e.g.,
http://home.online.no/‚àºpjacklam/notes/invnorm/impl/sprouse/ltqnorm.c.
1.5.3
Quantile of t Distribution
/*----------------------------------------------------------*/
/* Remark: Compute the quantile of the t-distribution
*/
/*
parameters: beta between 0 and 1
*/
/*
n is degrees of freedom
*/
/*
*/
double quantile_t(double beta, int n)
{
double t, a, c, u;
by U
only.

Basic Simulation Programming
23
if (n == 1) return (tan(M_PI*(beta-0.5)));
if (n == 2) {
t = 2*beta - 1;
return (sqrt(2)*t/sqrt(1-t*t));
}; /* n == 2 */
a = n - (2.0/3.0) + 1.0/(10*n);
c = (n - (5.0/6.0)) / (a*a);
u = quantile_norm(beta);
t = sqrt(n*exp(c*u*u) - n);
if (beta > 0.5)
return (t);
else
return (-t);
}
1.5.4
Quantile of Chi-square Distribution
/*----------------------------------------------------------*/
/* Remark: Compute the quantile of the chi-square dist
*/
/*
parameters: beta between 0 and 1
*/
/*
n is degrees of freedom
*/
/*
*/
double quantile_chi(double beta, int n)
{
double chi, h60, q, zz;
int c;
double h[15];
h[0] =
0.0118;
h[1] = -0.0067;
h[2] = -0.0033;
h[3] = -0.0010;
h[4] =
0.0001;
h[5] =
0.0006;
h[6] =
0.0006;
h[7] =
0.0002;
h[8] = -0.0003;
h[9] = -0.0006;
h[10] = -0.0005;
h[11] =
0.0002;
h[12] =
0.0017;
h[13] =
0.0043;
h[14] =
0.0082;
c = -1;
q = quantile_norm(beta);
by U
only.

24
Crafts of Simulation Programming
zz = -3.5;
while (zz < q) {
c += 1;
zz += 0.5;
}
if (q < -3.5)
h60 = 0;
else
h60 = h[c]+ 2*(h[c+1]-h[c])*(q-zz+0.5);
chi = 1 - 2.0/(9*n)+(q-60/n*h60)*sqrt(2.0/(9*n));
return
(n*exp(3*log(chi)));
}
/* chi square test */
int chisquare_test(int* r, double* p, int df)
{
int i;
int total = 0;
double score = 0.0;
for (i = 0; i <= df; ++i)
total += r[i];
for (i = 0; i <= df; ++i)
score += pow(r[i] - total*p[i],2) / (total*p[i]);
if (score < quantile_chi(0.9, df))
return (1);
else
return (0);
}
by U
only.

Basic Simulation Programming
25
1.5.5
Standard Deviation
Standard deviation can be computed by S2(N) = (N
1 Xi ‚àí¬ØX)2/(N ‚àí1),
where
¬ØX
= N
1 Xi/N.
However, if we use the equation S2(N) =
(N
1 X2
i /N ‚àí¬ØX2)N/(N ‚àí1) to compute the variance estimator, in-
stead of (X1, X2, . . . , XN), we are only required to store the triplet
(N, N
i Xi, N
i X2
i ).
It seems straightforward to compute ¬ØX = N
1 Xi/N. However, if the
sample size N is a very huge number, we may encounter problems. For
example, the value of A = n
i Xi for some n < N may be much greater
than Xn+1, consequently, the value of n+1
i
Xi will equal to A as well. The
severity of the error becomes greater when the diÔ¨Äerence of N ‚àín becomes
larger.
/*----------------------------------------------------------*/
/* Remark: Compute the variance: algorithm 1
*/
/*
parameters: n is the number of values in the Array
*/
/*
Array contains n values
*/
/*
mean is the output and is the average of n values
*/
/*
*/
double variance1(int n, double* Array, double* mean)
{
int i;
double variance=0.0;
*mean = 0.0;
for (i = 0; i < n; ++i)
*mean += Array[i];
*mean /= n;
for (i = 0; i < n; ++i)
variance+=pow(Array[i]-*mean,2);
return(variance / (n-1));
}
by U
only.

26
Crafts of Simulation Programming
/*----------------------------------------------------------*/
/* Remark: Compute the variance: algorithm 2
*/
/*
parameters: n is the number of values used to compute
*/
/*
Sumx and SumX2
*/
/*
SumX is the sum of n values
*/
/*
SumX2 is the sum of the square of n values
*/
/*
*/
double variance2(int n, double SumX, double SumX2)
{
double xAvg = SumX/n;
double x2Avg = SumX2/n;
double variance = (x2Avg - pow(xAvg,2));
variance = variance * n / (n-1);
return (variance);
}
1.6
Summary
We have discussed the backbone CMRG random-number generator, several
utilities to enhance its practical use, and implementation issues. This imple-
mentation eliminates the need to use a Ô¨Åxed number of pre-computed seeds,
which provides little Ô¨Çexibility. The presented random-number-generation
package provides jumping facilities, has good speed, a long period length,
and excellent theoretical/statistical properties. It provides for multiple gen-
erators (streams) running simultaneously, and each generator (stream) has
its sequence of numbers partitioned into many long disjoint contiguous sub-
streams. The RNG mrg32k3a has been implemented in several software
packages. We also provided some subroutines to generate non-uniform ran-
dom variables and some useful utilities.
by U
only.

Chapter 2
Sample Sizes and Stopping Rules
Simulation can be generally classiÔ¨Åed into two types: 1) Finite-Horizon
(terminating) simulation: a simulation where there is a speciÔ¨Åc starting
and stopping condition that is part of the model; and 2) Steady-State (non-
terminating) simulation: a simulation where there is no speciÔ¨Åc starting
and ending conditions.
The purpose of a steady-state simulation is the
study of the long-run behavior of a system.
A performance measure is
called a steady-state parameter if it is a characteristic of the equilibrium
distribution of an output stochastic process. Steady-state distribution does
not depend on initial conditions (when the initialization bias have been
properly taken care of), but the nature and rate of convergence of the
transient distributions can depend heavily on the initial conditions. The
length of the simulation must be determined dynamically to ensure that a
representative steady-state model behavior is reached. We seek algorithms
that provide good results and do not aÔ¨Äect the performance of a given
system.
Stopping rules are one of the crucial components of sequential simula-
tion procedures aÔ¨Äecting the performance of such procedures. These rules
are often used to determine the simulation run length (sample size). Many
stopping rules aim to provide an accurate estimate of the sampling error
of a certain unknown parameter. The rule would give the experimenter an
idea of the precision with which the estimate reÔ¨Çects the true but unknown
parameter. For example, when estimating a steady-state performance pa-
rameter such as the mean Œº of some discrete-time stochastic output process
{Xi : i ‚â•1} via simulation, it is desirable to devise an algorithm to deter-
mine the simulation run length n so that 1) the mean estimator (i.e., the
sample mean ¬ØX(n) = n
i=1 Xi/n) is unbiased; 2) the conÔ¨Ådence interval
(c.i.) for Œº is of a prescribed width; and 3) the actual coverage probability
27

28
Crafts of Simulation Programming
of the c.i. is close to the nominal coverage probability 1 ‚àíŒ±. Simulation
can also be classiÔ¨Åed into two categories: 1) discrete (time) event; and 2)
continuous time. We focus on discrete-event simulation in our discussion.
Before a simulation can be run, one must provide initial values for all of
the simulation‚Äôs state variables. Since the experimenter may not know what
initial values are appropriate for the state variables, these values might be
chosen somewhat arbitrarily. For instance, we might decide that it is ‚Äúmost
convenient‚Äù to initialize a queue as empty and idle. Such a choice of initial
conditions can have a signiÔ¨Åcant but unrecognized impact on the simulation
run‚Äôs outcome. It is well known that just after initialization (presumably
the initialized state is not steady-state) any stochastic system with random
inputs is in a transient state, during which its stochastic characteristics
vary with time and diÔ¨Äer from the steady-state distribution. This is caused
by the fact that the stochastic system initially moves along nonstationary
paths. After a period of time, the system approaches its statistical equilib-
rium on a stationary path if the system is stable, or remains permanently
on a nonstationary path if the system is not stable [Pawlikowski (1990)].
Output data collected during transient periods do not characterize
steady-state behavior of simulated systems, so they can cause quite sig-
niÔ¨Åcant bias of the Ô¨Ånal steady-state results. The inÔ¨Çuence that the initial
transient data can have on the Ô¨Ånal results is a function of the strength of
the autocorrelation of collected observations, the length of a simulation run,
and the rate of convergence to steady state. Thus, the initialization bias
problem can lead to errors, particularly in steady-state output analysis.
The mean estimator would be unbiased provided that the underlying
process is stationary; i.e., the joint distribution of the Xi‚Äôs is insensitive to
time shifts. That is, the simulation could theoretically go on indeÔ¨Ånitely
with no statistical change in behavior. In cases where the process is not
strictly stationary due to initialization eÔ¨Äects, this inÔ¨Çuence can be arbi-
trarily weakened by running the simulating program suÔ¨Éciently long. But
in practical situations simulation experiments are restricted in time and
it is not known in advance what is the required simulation run length to
eliminate the eÔ¨Äect of initial transient. One can usually resort to approx-
imate stationarity by removing some number of initial observations from
sequential simulation. Determining the number of initial observations to
be removed is itself a research topic and we do not address this issue here.
Moreover, many procedures have been developed to determine the initial
transient period, see [Hoad et al. (2010)]. Those procedures can be applied
to determine the warm-up period before using procedures that require the

Sample Sizes and Stopping Rules
29
underlying process to be stationary.
Simulation results do not provide exact answers, those results are only
estimates. Hence, statistical methods are required to analyze the results of
simulation experiments. The usual method of c.i. construction from classi-
cal statistics, which requires i.i.d. normal observations, is not directly appli-
cable since simulation output data are generally correlated and non-normal.
Statistical analysis needs to be performed to estimate the standard error
and conÔ¨Ådence interval as well as to Ô¨Ågure out the number of observations
required to achieve a desired error or conÔ¨Ådence interval. Given that the
sample size and consequently the quality of c.i. are largely determined by
the stopping rules, there is signiÔ¨Åcant interest in further expanding knowl-
edge base of stopping rules for sequential simulation procedures.
There
exists many stopping rules, see [Law (2014); Singham (2014)] for details.
Many of these existing techniques can be either overly restrictive to be
applied to general cases or excessively complicated to be implemented for
practical use.
We discuss a generic stopping rule to determine simulation run length
by test of independence, which can be easily incorporated into simulation
procedures. The experimental results indicate this stopping rule works well.
These procedures perform statistical analysis on systematic samples that
are collected on strictly stationary stochastic processes. The systematic
samples are obtained by systematic sampling; i.e., Ô¨Årst select a number l
between 1 and L (a positive integer), choose that observation and then
every lth observation thereafter.
Here, the chosen l will be suÔ¨Éciently
large so that observations l apart appear to be independent. We need to
specify v, the required number of near-independent observations, and l for
the systematic sampling. The minimum required total run length is then
N = vl.
The procedures sequentially increase the lag l, and consequently the
simulation run length, until a sequence of (v) systematic samples passes
the test of independence. The simulation run length thus derived is based
entirely on the data proper and requires no user intervention, provided
that the autocorrelation of the stochastic-process output sequence dies oÔ¨Ä
as the lag between observations increases, in the sense of œÜ-mixing, see
Section 2.1. These mild assumptions are satisÔ¨Åed in virtually all practical
settings. Roughly speaking, a stochastic process is œÜ-mixing if its distant
future behavior is essentially independent of its present or past behavior
[Billingsley (1999)].
We investigate two procedures of test of independence: 1) the von Nu-

30
Crafts of Simulation Programming
mann [von Neumann
(1941)] test; 2) runs test. One drawback of using
the runs test to determine whether a sequence appears to be independent
is that it can require a large sample size, for example, 4000 or more sam-
ples. This trait may render the proposed stopping rule cost-prohibitive for
certain instances. As an alternative, one may employ the von Neumann
test, which requires a much smaller sample size; for example, 180 samples.
Note that von Neumann test is computationally more expensive and less
powerful than the runs test. Hence the alternative proposed herein repre-
sents a tradeoÔ¨Äoption that the experimenter can make when constructing
the simulation procedures. The proposed stopping rule can be used as a
generic method to determine the simulation run length for many simula-
tion procedures. Based on the nature of the underlying simulation study,
diÔ¨Äerent tests of independence can be used to determine the simulation run
length. As an illustration, we apply this stopping rule with the von Neu-
mann test of independence to determine the batch size of batch means, see
[Chen (2012)].
2.1
DeÔ¨Ånitions
This section reviews 1) œÜ-mixing; 2) the batch-means method, which is
used to estimate the variance of the sample mean; and 3) the von Neumann
test and the runs test, which determine whether a sequence is suÔ¨Éciently
independent for the purpose of simulation tasks.
Let {Xi; ‚àí‚àû< i < ‚àû} be a stationary sequence of random variables
deÔ¨Åned on a probability space (Œ©, F, P).
Thus, if M k
‚àí‚àûand M ‚àû
k+j are,
respectively, the œÉ-Ô¨Åelds generated by {Xi; i ‚â§k} and {Xi; i ‚â•k + j}, and
if E1 ‚ààM k
‚àí‚àûand E2 ‚ààM ‚àû
k+j, then for all k(‚àí‚àû< k < ‚àû) and j(j ‚â•1),
if
|Pr(E2|E1) ‚àíPr(E2)| ‚â§œÜ(j),
where 1 ‚â•œÜ(1) ‚â•œÜ(2) ‚â•¬∑ ¬∑ ¬∑ , and limj‚Üí‚àûœÜ(j) = 0, then {Xi; ‚àí‚àû< i <
‚àû} is called œÜ-mixing. For more information on œÜ-mixing, see [Billingsley
(1999)]. Intuitively, X1, X2, ¬∑ ¬∑ ¬∑ , Xn is œÜ-mixing if Xi and Xi+j become
virtually independent as j becomes large.
These weakly dependent, stationary processes typically obey a Central
Limit Theorem (CLT) of the form
‚àön[ ¬ØX(n) ‚àíŒº]
Œ©
D
‚àí‚ÜíN(0, 1) as n ‚Üí‚àû,
(2.1)

Sample Sizes and Stopping Rules
31
where Œ©2 is the steady-state variance constant (SSVC), N(Œº, œÉ2) denotes
the normal distribution with mean Œº and the variance œÉ2, and
D
‚àí‚Üídenotes
convergence in distribution [Billingsley (1999)].
For correlated sequences, the SSVC
Œ©2 ‚â°lim
n‚Üí‚àûnVar[ ¬ØX(n)] =
‚àû

i=‚àí‚àû
Œ≥i,
where Œ≥i = Cov(Xk, Xk+i) for any k is the lag-i covariance. A suÔ¨Écient
condition for the SSVC to exist is that the output process is stationary and
‚àû
‚àí‚àû|Œ≥i| < ‚àû. If the sequence is independent, then the SSVC is equal to
the process variance œÉ2
x = Var(Xi). For a Ô¨Ånite sample n, let
œÉ2(n) = Œ≥0 + 2
n‚àí1

i=1
(1 ‚àíi/n)Œ≥i.
It follows that limn‚Üí‚àûœÉ2(n) = œÉ2 so
Var[ ¬ØX(n)] = œÉ2(n)/n ‚âàœÉ2/n,
provided that n is suÔ¨Éciently large. The procedures may fail when the
underlying stochastic process does not satisfy the limiting result of Eq.
(2.1).
Furthermore, for œÜ-mixing sequences, the natural estimators (see Sec-
tion 6.1.1) of other distribution characteristics also perform well.
For
0 < p < 1, the pth quantile (percentile)
of a distribution is the value
at or below which 100p percent of the distribution mass lies. Quantile esti-
mation can be computed using standard non-parametric estimation based
on order statistics, which can be used not only when the data are i.i.d.
but also when the data are drawn from a stationary, œÜ-mixing process of
continuous random variables. It is shown in [Sen (1972)] that quantile es-
timates, based on order statistics, have a normal limiting distribution and
are asymptotically unbiased, if certain conditions are satisÔ¨Åed.
Although asymptotic results are often applicable when the amount of
data is ‚Äúlarge enough,‚Äù the point at which the asymptotic results become
valid generally depends on unknown factors. An important practical de-
cision must be made regarding the sample size n required to achieve the
desired precision. Therefore, both asymptotic theory and workable Ô¨Ånite-
sample approaches are needed by the practitioner.

32
Crafts of Simulation Programming
2.2
Batch-Means Method
The Batch-Means (BM) method is a well-known technique for estimating
the variance of point estimators computed from simulation experiments. It
attempts to reduce autocorrelation by batching observations. In the non-
overlapping BM (NOBM) method, the simulation output sequence {Xi :
i = 1, 2, . . . , n} is divided into b adjacent non-overlapping batches, each
batch of a size m. The sample size n is therefore n = bm. The sample
mean, ¬ØXj, for the jth batch is
¬ØXj = 1
m
mj

i=m(j‚àí1)+1
Xi for j = 1, 2, . . . , b.
For the given batch size m, we have
Var[ ¬ØXj] = Œ©2(m)/m, where Œ©2(m) = Œ≥0 + 2
m‚àí1

i=1
(1 ‚àíi/m)Œ≥i.
The grand mean ÀÜŒº of the individual batch means, given by
ÀÜŒº = 1
b
b

j=1
¬ØXj,
(2.2)
is used as a point estimator for Œº. Here ÀÜŒº = ¬ØX(n) = n
i Xi/n, the sample
mean of all n individual Xi‚Äôs, and we seek to construct a c.i. for Œº based
on the point estimator of Eq. (2.2).
The BM variance estimator (for estimating Var[ ¬ØXj]) is simply the sam-
ple variance of the mean estimator ¬ØXj computed from the batch means
S2 =
1
b ‚àí1
b

j=1
( ¬ØXj ‚àíÀÜŒº)2.
(2.3)
Consequently, the batch-means estimator for Œ©2 is ÀÜŒ©2
B ‚â°mS2.
Asymptotic validity of the c.i. constructed by the batch-means method
- i.e., the coverage probability of the c.i. is close to the nominal coverage
probability - tend to depend on the validity of the assumption that the
batch means are approximately i.i.d. normal. That is, for a large batch size
m, the batch means are approximately i.i.d. normal with unknown mean
Œº and unknown variance Œ©2(m)/m. Other BM methods, e.g., [Tafazzoli et
al. (2011)], may choose to construct a c.i. for the mean by adjusting the
c.i. half-width based on the strength of the autocorrelations between BM as
well as the skewness of BM. There are many BM methods, see [Hoad et al.
(2011)] for the ‚Äòfamily trees‚Äô of BM methods in the simulation literature.

Sample Sizes and Stopping Rules
33
2.3
Determining the Simulation Run Length
In this section, we discuss the von Neumann test of independence, the
runs test of independence, the strategy of using test of independence to
implement a stopping rule as well as a procedure to construct a conÔ¨Ådence
interval for the mean.
2.3.1
The von Neumann Test of Independence
The von Neumann test of independence is relatively simple and most ad-
equate for cases calling for small sample sizes. The test constructs a met-
ric commonly known as the von Neumann ratio. Several BM procedures
have used the von Neumann ratio to test for independence, e.g., [Fishman
(1978)].
The von Neumann test [von Neumann
(1941); Fishman
(2001)] can
be put forth as the following. For the null hypothesis H0: the samples
X1, X2, . . . , Xv are uncorrelated, the von Neumann ratio is
Cv = 1 ‚àí
v
i=2(Xi ‚àíXi‚àí1)2
2 v
i=1(Xi ‚àí¬ØX(v))2 .
Note that Cv
is an estimator of the lag-1 autocorrelation œâ1
‚â°
Corr(Xi, Xi+1), adjusted for end eÔ¨Äects that diminish in importance as
the number of samples v increases. Note that v is the sample size relevant
to the von Neumann test and is generally diÔ¨Äerent than the simulation run
length n.
The von Neumann test statistic for H0 is
Z =

v2 ‚àí1
v ‚àí2 Cv.
Under H0, Z ‚àºN(0, 1), so one rejects H0 at level 1 ‚àíŒ±ind if Z > z1‚àíŒ±ind,
where z1‚àíŒ±ind is the 1 ‚àíŒ±ind quantile of the standard normal distribution.
[Fishman (2001)] points out that the von Neumann test of independence
is likely to accept H0 when {Xi} has an autocorrelation function that is neg-
atively correlated and exhibits damped harmonic behavior around zero. In
this case, the variance estimator is biased high instead of biased low. While
one does not lose any performance in terms of coverage, the half-width of
the conÔ¨Ådence interval for Œº will be wider than otherwise achievable.
2.3.2
A Source Code of the von Neumann Test
This subroutine implements the von Neumann test of independence.

34
Crafts of Simulation Programming
/* von Neumann test of independence */
int vonNeumann(double* BatchN, int bnum, double muhat)
{
double t;
int i;
double temp1 = 0.0;
double temp2 = 0.0;
temp2 = pow(BatchN[0] - muhat,2);
for (i = 1; i < bnum; ++i) {
temp1 += pow(BatchN[i] - BatchN[i-1],2);
temp2 += pow(BatchN[i] - muhat,2);
};
t = sqrt((bnum*bnum-1)/(bnum-2))*(1-temp1/(2*temp2));
if ( t <= quantile_norm(0.9) ) {
return (1);
} else {
return (0);
};
}
2.3.3
The Runs Test of Independence
Another test of independence is the runs test, it examines the sequence
of output data for unbroken subsequences of maximal length within which
the sequence increases (decreases) monotonically; such a subsequence is
called a run up (run down), and the length of the subsequence is called
the run length. Note that this run length (for the test of independent) is
diÔ¨Äerent from the simulation run length. The run length used by the runs
test is to determine whether a sequence appears to be independent. On
the other hand, simulation run length is the total length of a particular
simulation execution, measured either in number of observations collected
(for a discrete-time output process), or amount of simulated time (for a
continuous-time output process); we focus on discrete-time processes. The
runs test looks solely for lack of independence and has been shown to be very

Sample Sizes and Stopping Rules
35
powerful. When we examine the run length of consecutive subsequences, a
long run will tend to be followed by a short run, and conversely. This is
because adjacent runs are not independent. For example, if the run length
of a runs-up subsequence is r, then for some constant i, Xi+j‚àí1 < Xi+j for
j = 1, 2, ¬∑ ¬∑ ¬∑ , r‚àí1, and Xi+r‚àí1 > Xi+r. If r is large, then Xi+r‚àí1 is likely to
be large. Therefore, even though Xi+r‚àí1 > Xi+r, Xi+r could be a relatively
large number and will result in a short run. This lack of independence is
enough to invalidate a straightforward œá2 test. [Knuth (1998)] suggests a
vastly simpler and practical runs test. The procedure will throw away the
element that immediately follows a run, so that when Xj is greater than
Xj+1 we start the next run with Xj+2. Hence, if the output data are i.i.d.,
then the run lengths are independent because the last element of the Ô¨Årst
subsequence Xj and the Ô¨Årst element of the adjacent subsequence Xj+2 are
independent. Consequently, a simple œá2 test may be used. For this simple
runs-up test, the probability of a run of length r is r = r/(r+1)!, under the
null hypothesis that the output data are i.i.d. random variables. If we chose
to record six run lengths, 1 through 5 and 6+ (run length of six or longer),
then for a large sample size n, the test statistic will have an approximate
œá2 distribution with Ô¨Åve degrees of freedom (d.f.). [Knuth (1998)] suggests
that n be at least 4000. That is, deÔ¨Åne the statistic
V =

1‚â§r‚â§6
(Rr ‚àíSPr)2
SPr
,
where Rr, r = 1, 2, . . . , 6 is the number of subsequences with run length r
(or ‚â•r in the case of r = 6), S = 
1‚â§r‚â§6 Rr, and Pr = r/(r + 1)!. Then
under the null hypothesis that the output data are i.i.d. random variables,
the statistic V has a œá2 distribution with Ô¨Åve d.f., when n is large (‚â•4000).
Theorem 2.1. The probability of a run of length r is r/(r + 1)!, under the
null hypothesis that the data are i.i.d. (continuous) random variables.
Proof. For the run length to be r, we need to have exactly r+1 observations
in a subsequence (with probability zero that any two values are equal).
Because these are i.i.d. random variables, by the permutation rule there
are exactly (r + 1)! diÔ¨Äerent subsequences with r + 1 observations.
To
have run length equal to r, the observations from x1 to xr must be in
ascending order but xr must be greater than xr+1. There are only r such
subsequences; with r + 1 observations sorted into ascending order, each run
length r subsequence is created by taking the ith observation (for i = 1 to r)
and concatenated to the end of the sorted subsequence. It can be shown

36
Crafts of Simulation Programming
by induction that
n

r=1
r
(r + 1)! = 1 ‚àí
1
(r + 1)!.
Furthermore,
‚àû

r=1
r
(r + 1)! = lim
n‚Üí‚àû
n

r=1
r
(r + 1)! = 1.
This completes the proof.
The runs test requires data to be absolutely continuous and may not
work properly if this condition is not satisÔ¨Åed. The probability of a run of
length of 1 will be unusually high at a discontinuous point where there is a
jump in the cumulative distribution function. This over mixing will cause
the subsequence to fail the runs test. To correct this problem, [Chen and
Kelton (2010)] propose to increase the run length with probability 1/(r+1)
when two elements are equal, where r is the current run length. Because
if there are r + 1 observations, the probability of the (r + 1)th observation
being the largest is 1/(r + 1). For example, if Xi = Xi+1 and the runs-up
run length r up to Xi is 2, we will generate a uniform (0,1) random value
u; if u < 1/3, we will let Xi ‚â§Xi+1 and increase the run length by one,
otherwise we will let Xi ‚â•Xi+1 and start a new runs-up sequence.
2.3.4
A Source Code of Runs Up Test
This subroutine implements the quasi-independent algorithm. It can be
used to determine the simulation run length.

Sample Sizes and Stopping Rules
37
/* check whether the sequence is independent */
#RUNSUPSZ
4000
int checkind(double* Observ, double seed[6])
{
/* proportion of each run length */
static double p[6] = {1/2,1/3,1/8,1/30,1/144,1/840};
int i;
int r[6] = {0, 0, 0, 0, 0, 0};;
int runup=1;
int df = 5;
for (i = 1;
i < RUNSUPSZ; ++i ) {
/* save run up statistics */
if (runup == 0) {
runup = 1;
} else if (Observ[i] > Observ[i - 1]) {
++runup;
} else {
if ((Observ[i] == Observ[i - 1]) &&
(RnadU01(seed) < 1.0/(runup+1))) {
++runup;
} else {
if (runup >= df+1)
++r[df];
else
++r[runup-1];
runup = 0;
};
};
}; /* for i */
if ( chisquare_test(r, p, df) )
return (1);
else
return (0);
}

38
Crafts of Simulation Programming
2.3.5
An Implementation of Determining the Simulation
Run Length
Sequential procedures usually start a simulation run with an initial sam-
ple size, which may be determined in advance or dynamically at run time.
Based on information obtained with the initial samples, sequential proce-
dures then progressively increase the sample size until the prescribed stop-
ping rule is satisÔ¨Åed. If the estimator satisÔ¨Åes the speciÔ¨Åed precision, then
the procedure returns the estimated value and terminates. Otherwise, the
procedure continues the simulation, either by adding more samples in the
current replication or more replications.
Let li be the lag at which systematic samples pass the test of indepen-
dence. We call the sequence of the (lag-li) systematic samples that appear
to be independent the quasi-independent (QI) sequence. Note that inde-
pendent sequences are likely to pass the test of independence with lag 1
observations. Hence, the procedure will allocate small sample sizes when
the underlying output sequences are independent. This allows users to use
the QI procedures without allocating excessively large number of samples.
Furthermore, the QI stopping rule can be used with various sequential pro-
cedures to estimate mean, variance, quantile, density, proportion, etc.
In the following section, we apply a BM method to estimate the SSVC
and construct a c.i. of the steady-state mean employing the proposed stop-
ping rules as an illustration.
2.4
Constructing the ConÔ¨Ådence Interval
In the approach, we Ô¨Årst compute the lag l for the systematic samples. Once
the lag l is large enough for the lag-ls systematic samples in the buÔ¨Äer to
pass the test of independence, we check whether the intermediate batches
(i.e., the averages of ls consecutive primitive batches that will be discussed
later) appear to be independent. Note that the lag at which systematic
samples appear to be independent li = lls. When the intermediate batches
appear to be independent, we then compute the batch means (by taking
the average of several consecutive intermediate batches) and the variance
estimator S2 based on the batch means.
Otherwise, the procedure will
continue to increase the lag li and consequently the sample size.
The Ô¨Ånal step of the procedure is to determine whether the c.i. meets the
user‚Äôs half-width requirement. This requirement usually takes one of the
following forms: 1) a maximum absolute half-width œµ; or 2) a maximum

Sample Sizes and Stopping Rules
39
relative fraction Œ≥ of the magnitude of the Ô¨Ånal mean estimator ÀÜŒº. Let
w = tb‚àí1,1‚àíŒ±/2S/
‚àö
b denote the c.i. half-width. Here tf,1‚àíŒ± is the 1 ‚àíŒ±
quantile of the t distribution with f degrees of freedom. If the precision of
the c.i. is satisÔ¨Åed (i.e., w ‚â§œµ or w ‚â§Œ≥|ÀÜŒº|), then the procedure terminates,
and we return the mean estimate ÀÜŒº and the c.i. with half-width w. If the
precision requirement is not satisÔ¨Åed, the procedure will increase the sample
size to shorten the half-width. The procedure will increase the number of
batches b to
(w/œµ)2b or (w/(Œ≥ÀÜŒº))2b.
(2.4)
This step will be executed repeatedly until the half-width is within the
speciÔ¨Åed precision.
2.5
A Correlation Adjustment
There may be residual correlations between the Ô¨Ånal batch means, which
will cause the c.i. half-width to be narrower than necessary and result in
coverages less than the speciÔ¨Åed nominal value. The strategy of SBatch
[Lada et al. (2008)] can be used to adjust the c.i half-width for any residual
correlation between the batch means.
The correlation-adjusted c.i. half
width
wA =
‚àö
Aw.
(2.5)
The correlation adjustment A is computed as
A = 1 + ÀÜC ¬Ø
X
1 ‚àíÀÜC ¬Ø
X
,
where the standard estimator of the lag-1 correlation of the batch means is
ÀÜC ¬Ø
X = 
Corr( ¬ØXj, ¬ØXj+1) = 1
b
b‚àí1

j=1
(Xj ‚àíÀÜŒº)(Xj+1 ‚àíÀÜŒº)/S2.
Note that in the implementation we set A = 1 when A < 1 so that w ‚â§wA.
2.6
An Implementation of Batch-Means Method
We use the QI stopping rule to implement a batch-means procedure, QI-
Batch2. Figure 2.1 displays a high-level Ô¨Çow chart of QIBatch2. We al-
locate two buÔ¨Äers with size 3v to keep systematic samples and primitive

40
Crafts of Simulation Programming
batches (PB), i.e., the average of l observations.
Batch means are then
computed from PB in the buÔ¨Äer. Initially l = 1 and each observation is
treated as a PB; as the procedure proceeds, l will be doubled every two sub-
iterations. The steps between generating more observations are considered
a sub-iteration.
Fig. 2.1
High-level Ô¨Çow chart of QIBatch2

Sample Sizes and Stopping Rules
41
To facilitate the description of the algorithm, we index the sub-iterations
with subscript A and B so that most relevant quantities can be described
in simpler forms. The procedure performs the test of independence in both
A and B sub-iterations and terminates when both the lag-ls systematic
samples in the buÔ¨Äer and the intermediate batches (IBs, the average of ls
adjacent PBs) appear to be independent.
Finally, we aggregate the available PBs into 30 BMs by averaging the
adjacent PBs. Using 30 as the Ô¨Ånal number of batches is somewhat arbi-
trary. The rationale is to insure that the number of batches is large enough
to obtain a good autocorrelation estimate.
The QIBatch2 algorithm:
The size of the buÔ¨Äer used to store the PB is 3v, l is the lag used to
create the systematic samples and is also the number of observations used
to compute the PB, Œ¥ is the incremental sample size, r is the index of
iterations. Each iteration r contains two sub-iterations rA and rB.
(1) Initialization: Set v = 180, b = 30, q = v/b, l = 1, ls = 1, Œ¥ = v, and
r = 0.
(2) Generate Œ¥ systematic samples and PBs, where each PB is the average
of l observations.
(3) If this is a rA (r > 0) iteration, set ls = 2. If this is a rB (r > 0)
iteration, set ls = 3.
(4) Compute the intermediate batches, i,e., the average of ls adjacent PBs.
(5) Check whether the lag-ls systematic samples in the buÔ¨Äer and the
intermediate batches pass the von Neumann test.
(6) If these systematic samples and intermediate batches pass the von
Neumann test, go to step 11.
(7) If this is the initial or a rB iteration, set r = r + 1 and start a rA
iteration. If this is a rA iteration, start a rB iteration.
(8) If this is a rA (r > 1) iteration, then re-calculate the PBs in the buÔ¨Äer
by taking the average of two consecutive PBs, and reindex the rest of
the 3v/2 PBs in the Ô¨Årst half of the buÔ¨Äer. Re-index the systematic
samples in the buÔ¨Äer by discarding every other samples, and reindex
the rest of the 3v/2 systematic samples in the Ô¨Årst half of the buÔ¨Äer.
Set l = 2r‚àí1, Œ¥ = v/2.
(9) If this is a rB (r > 1) iteration, set Œ¥ = v.
(10) Go to step 2.

42
Crafts of Simulation Programming
Table 2.1
Properties of QIBatch2 at each iteration
Iteration
0
1A
1B
2A
2B
. . .
rA
rB
n
v
2v
3v
4v
6v
. . .
2rv
2r‚àí13v
Vi
v
2v
3v
2v
3v
. . .
2v
3v
l
1
1
1
2
2
. . .
2r‚àí1
2r‚àí1
li
1
2
3
4
6
. . .
2r‚àí12
2r‚àí13
(11) Set the value of the batch means to the average of q‚Ä≤(= qls) PBs in the
buÔ¨Äer (i.e., ¬ØXj = q‚Ä≤(j‚àí1)+q‚Ä≤
q‚Ä≤(j‚àí1)+1 Vj/q‚Ä≤ for j = 1, 2, . . . , b, where Vj is the
value of the jth PB). Compute the batch-means variance estimator S2
according to Eq. (2.3) and conÔ¨Ådence interval half-width according to
Eq. (2.5).
(12) Let œµ be the desired absolute half-width, and let Œ≥|ÀÜŒº| be the desired
relative half-width. If the half-width of the c.i. is greater than œµ or
Œ≥|ÀÜŒº|, compute b‚Ä≤, the required number of batches according to Eq.
(2.4), generate Œ¥‚Ä≤ = ‚åà(b‚Ä≤ ‚àíb)/2‚åâadditional batches, set b = b + Œ¥‚Ä≤, and
go to step 11; otherwise the procedure returns the c.i. estimator and
terminates.
The QIBatch2 procedure starts with an initial sample size of 180 and
doubles the sample sizes every two sub-iterations.
We choose the value
v = 180 because this is the sample size we used for the von Neumann test.
Note that we set Œ¥ = v/2 at A iterations and Œ¥ = v at B iterations so that
the number of PBs will always be a multiple of v at the end of an iteration.
The procedure progressively increases the lag l until the lag-ls systematic
samples in the buÔ¨Äer and the intermediate batches pass the von Neumann
test. The procedure needs only to process each observation once and does
not require storing the entire output sequence. See Section 2.7 for some
properties of the procedure as it proceeds from iteration to iteration.
We calculate the required number of batch to meet the speciÔ¨Åed preci-
sion requirement by Eq. (2.4) when the default precision does not meet the
requirement. However, instead of allocating the entire additional number
of batches, we allocate only half of those, i.e., Œ¥‚Ä≤ = ‚åà(b‚Ä≤‚àíb)/2‚åâat each itera-
tion. This is to reduce the frequency of over allocating the required number
of batches. The mean Œº and the c.i. half-width are estimated, respectively,
by ÀÜŒº and Eq. (2.5).

Sample Sizes and Stopping Rules
43
2.7
An Illustration of Allocated Sample Sizes
Table 2.1 illustrates how sampling progresses from iteration to iteration.
The Iteration row lists the running index of the iterations. The n row lists
the total number of observations to be taken during a certain iteration. The
Vi row lists the total number of PBs in the buÔ¨Äer at a certain iteration.
The l row lists the lag l that is used to obtain the systematic samples in the
buÔ¨Äer. Note that l is also the number of observations used to obtain the
PB in the buÔ¨Äer. The li row lists the lag li at which the systematic samples
appear to be independent when the lag ls of the systematic samples in the
buÔ¨Äer pass the von Neumman test of independence.
For example, at the end of sub-iteration 1B, the total number of ob-
servations is 3v, there are 3v PBs in the buÔ¨Äer, and each PB is just the
value of each observation. At the beginning of sub-iteration 2A, we reduce
the number of PBs in the buÔ¨Äer from 3v to 3v/2 by taking the average of
every two consecutive PBs. By reducing the number of PBs in the buÔ¨Äer
at rA(r > 1) iterations, the number of PBs will always be no more than
3v. We will generate v/2 PBs (with the new increased batch size 2) at
sub-iteration 2A; so we will have 2v PBs at the end of the iteration.
Table 2.2 lists the value of the PBs times 2r‚àí1 at each iteration. Note
that the size of number of observations of each PB at the r iterations is 2r‚àí1.
That is, the value of each PB (i.e., Vi for i = 1, 2, . . . , 3v) is the average of
2r‚àí1 observations. Here xi is the realization of Xi. At the end of the initial
iteration, there will be v PBs. At the end of the rA and rB iterations, there
will be 2v and 3v PBs, respectively. Let œ±i for i = 1, 2, . . . , v denote the ith
intermediate batch. Table 2.3 lists the value of the v intermediate batches
that are used for the von Neumann test of independent.
Once these v
intermediate batch means appear to be independent, the b = 30 Ô¨Ånal batch
means are obtained by averaging the consecutive q = v/b intermediate
batch means. Table 2.4 lists the value of the Ô¨Ånal b batch means.
2.8
Empirical Experiments
In this section, we present some empirical results from simulation exper-
iments using the proposed procedure.
We test the procedure with six
stochastic processes:
‚Ä¢ Observations are i.i.d. uniform between 0 and 1, denoted U(0, 1).
‚Ä¢ Observations are i.i.d. N(0, 1).

44
Crafts of Simulation Programming
Table 2.2
The value of the PBs times 2r‚àí1 at each iteration
Iter
V1
. . .
Vv
. . .
V2v
. . .
V3v
0
x1
. . .
xv
1A
x1
. . .
xv
. . .
x2v
1B
x1
. . .
xv
. . .
x2v
. . .
x3v
2A
2
i=1 xi
. . .
2v
i=2v‚àí1 xi
. . .
4v
i=4v‚àí1 xi
2B
2
i=1 xi
. . .
2v
i=2v‚àí1 xi
. . .
4v
i=4v‚àí1 xi
. . .
6v
i=6v‚àí1 xi
...
...
rA
2r‚àí1
i=1
xi
. . .
2r‚àí1v
i=2r‚àí1(v‚àí1)+1 xi
. . .
2r‚àí12v
i=2r(v‚àí1)+1 xi
rB
2r‚àí1
i=1
xi
. . .
2r‚àí1v
i=2r‚àí1(v‚àí1)+1 xi
. . .
2r‚àí12v
i=2r‚àí1(2v‚àí1)+1 xi
. . .
2r‚àí13v
i=2r‚àí1(3v‚àí1)+1 xi

Sample Sizes and Stopping Rules
45
Table 2.3
The value of the v intermediate batches that used for the von Neumann test
Iteration
œ±1
œ±2
. . .
œ±v
0
V1
V2
. . .
Vv
rA
(V1 + V2)/2
(V3 + V4)/2
. . .
(V2v‚àí1 + V2v)/2
rB
(V1 + V2 + V3)/3
(V4 + V5 + V6)/3
. . .
(V3v‚àí2 + V3v‚àí1 + V3v)/3
Table 2.4
The value of the Ô¨Ånal b batches
¬Ø
X1
¬Ø
X2
. . .
¬Ø
Xb
q
i=1 œ±i/q
2q
i=q+1 œ±i/q
. . .
v
i=(b‚àí1)q+1 œ±i/q
‚Ä¢ Observations are i.i.d. exponential with mean 1, denoted expon(1).
‚Ä¢ Steady-state Ô¨Årst-order moving average process, generated by the re-
currence
Xi = Œº + œµi + Œ∏œµi‚àí1 for i = 1, 2, . . . ,
where the œµi are i.i.d. N(0, 1), and ‚àí1 < Œ∏ < 1. We set Œº to 2 in our
experiments. This process is denoted MA1(Œ∏).
‚Ä¢ Steady-state Ô¨Årst-order autoregressive process, generated by the re-
currence
Xi = Œº + œï(Xi‚àí1 ‚àíŒº) + œµi for i = 1, 2, . . . ,
where the œµi are i.i.d. N(0, 1) , and ‚àí1 < œï < 1. We set Œº is set to 2
in our experiments. This process is denoted AR1(œï). We set X0 to a
random variate drawn from a N(0,
1
1‚àíœï2 ) distribution.
‚Ä¢ Steady-state M/M/1 delay-in-queue process with arrival rate Œª and
service rate ŒΩ = 1. This process is denoted MM1(œÅ), where œÅ = Œª/ŒΩ
is the traÔ¨Éc intensity.
First, we evaluate the performance of the von Neumann test with dif-
ferent levels of Œ±. Based on the experimental results, the conÔ¨Ådence level
of the von Neumann test is set to 90% in latter experiments. Note that a
lower conÔ¨Ådence level of these tests will increase the chance of committing a
Type I error (rejecting the null hypothesis when it is true) and will increase
the simulation run length.
We check the interdependence between the lag l at which the systematic
samples pass the independence test and the strength of the autocorrelation
of the output sequence.
We evaluate the performance of the QIBatch2
procedure to estimate the variance of sample means.

46
Crafts of Simulation Programming
Table 2.5
The percentage of the output se-
quences pass the von Neumann test of indepen-
dence with 180 samples at diÔ¨Äerent levels of Œ±
Process
P(90%)
P(95%)
P(99%)
U(0, 1)
90%
95%
99%
N(0, 1)
90%
95%
99%
expon(1)
90%
95%
99%
MA1(0.15)
25%
39%
65%
MA1(0.25)
2.5%
6.0%
20%
MA1(0.35)
0.05%
0.23%
1.9%
MA1(0.50)
0%
0%
0.02%
MA1(0.75)
0%
0%
0%
AR1(0.15)
24%
37%
63%
AR1(0.25)
2.1%
4.9%
16%
AR1(0.35)
0.03%
0.1%
0.85%
AR1(0.50)
0%
0%
0.01%
AR1(0.75)
0%
0%
0%
MM1(0.15)
23%
28%
38%
MM1(0.25)
3.7%
5.2%
9.4%
MM1(0.35)
0.28%
0.46%
1.1%
MM1(0.50)
0%
0.01%
0.02%
MM1(0.75)
0%
0%
0%
2.8.1
Experiment 1
In this experiment, we used the von Neumann test to check whether a
sequence of observations passes the tests of independence.
The number
of observations used to perform the test is v = 180. Table 2.5 lists the
experimental results. Each design point is based on 10,000 independent
simulation runs. The P(100(1‚àíŒ±ind)%) columns list the observed percent-
age of these 10,000 runs passing the von Neumann test when the nominal
probability of the test of independence is set to 1 ‚àíŒ±ind. We set Œ±ind and
Œ±nor to 0.1, 0.05, and 0.01.
The von Neumann test performs very well in terms of Type I error
(i.e., independent sequences fail the test of independence) ‚Äì it is close to
the speciÔ¨Åed Œ±ind level. A Type II error is the event that we accept the
null hypothesis when it is false, i.e., correlated sequences pass the test of
independence. For slightly correlated sequences, the frequency of commit-
ting a Type II error is high, for example, the MA1(0.15), AR1(0.15), and
MM1(0.15) processes. Note that we can use a larger number of samples
to increase the power of a test and reduce the Type II error. If we mis-
takenly treat slightly correlated sequences as being i.i.d., the performance
measurements should still be fairly accurate; thus, the low probability of
correct decision for those slightly correlated sequences should not pose a

Sample Sizes and Stopping Rules
47
Table 2.6
Average lag l at which the output sequences pass
the
test
of
independence
Process
MA1(Œ∏)
AR1(œï)
MM1(œÅ)
Œ∏,œï,œÅ
¬Øl
stdv(l)
¬Øl
stdv(l)
¬Øl
stdv(l)
0.15
1.84
0.58
1.88
0.62
2.13
0.86
0.25
2.08
0.37
2.36
0.63
3.09
1.09
0.35
2.11
0.34
2.82
0.74
4.44
1.42
0.50
2.10
0.33
3.85
0.87
7.72
2.14
0.75
2.12
0.36
7.85
1.39
28.24
6.31
0.90
2.11
0.34
18.28
2.65
138.16
27.84
problem. On the other hand, for mildly or highly correlated sequences, the
von Neumann test detects the dependence almost all the time.
The experimental results of the runs test of independence are available
in [Chen and Kelton (2003)].
2.8.2
Experiment 2
In this experiment, we check the interdependence between the average lag
at which the systematic samples appear to be independent and the strength
of the autocorrelation of the output sequence. We set Œ±1 = 0.1. Table 2.6
lists the experimental results. Each design point is based on 1,000 inde-
pendent simulation runs. The Œ∏, œï, œÅ column lists the coeÔ¨Écient values of
the corresponding stochastic process. The MA1(Œ∏) column lists the results
of the underlying moving average output sequences.
The ¬Øl column lists
the average lag at which the systematic samples appear to be independent.
Hence, the average computation run length for that particular stochastic
process is v¬Øl. The stdv(l) column lists the standard deviation of the lag l.
In general, the average lag at which the systematic samples appear to be
independent increases as the autocorrelation increases. The MA1(Œ∏) pro-
cesses are only slightly correlated even with Œ∏ as large as 0.9. Therefore,
lag 2 observations of the MA1(0.90) output sequences generally appear to
be independent. On the other hand, the MM1(0.90) output sequences are
highly correlated, the average lag at which the systematic samples appear
to be independent is as large as 138.
The results from this experiment
indicate a strong correlation between the strength of the autocorrelation
and the average lag l at which the systematic samples appear to be inde-
pendent. We also performed the experiment using diÔ¨Äerent v. The average
lag at which the systematic samples appear to be independent generally
increases with a larger v.
The experimental results of the runs test of independence are available

48
Crafts of Simulation Programming
Table 2.7
Coverage of 90% conÔ¨Ådence inter-
vals of independent samples
Process
U(0, 1)
N(0, 1)
expon(1)
Œº
0.50
0.00
1.00
avg samp
202
203
204
avg hw
0.042
0.142
0.144
stdv hw
0.015
0.049
0.054
coverage
90.7%
90.4%
90.6%
Table
2.8
Coverage
of
90%
conÔ¨Ådence
intervals
of auto-correlated samples
Process
MA1(0.9)
AR1(0.9)
MM1(0.9)
Œº
2.00
2.00
9.00
avg samp
416
13928
243671
avg hw
0.191
0.182
0.814
stdv hw
0.068
0.074
0.397
coverage
90.3%
92.4%
87.4%
in [Chen and Kelton (2003)].
2.8.3
Experiment 3
In this experiment, we use the QIBatch2 procedure to construct c.i. of
means. Since batch means are often used to estimate the variance of sample
means, we evaluate the accuracy of the variance estimated by the procedure.
In these experiments, no relative or absolute precisions were speciÔ¨Åed, so
the half-width of the c.i. is the result of the default precision.
In this
experiment we list the results of c.i. coverage when the entire sequences are
divided into b = 6 batches.
Table 2.7 lists the experimental results obtained from sampling obser-
vations from three diÔ¨Äerent i.i.d. processes. The Œº row lists the true mean.
The avg samp row lists the average sample size. The avg and stdv hw rows
list, respectively, the average half-width and standard deviation of the half-
width obtained by the procedure. The coverage row lists the percentage of
the c.i.‚Äôs that cover the true mean value. The c.i. coverages are around the
speciÔ¨Åed 90% conÔ¨Ådence level. The procedure correctly detects that the
underlying sequences are independent and the size of allocated samples is
around the theoretical value, which is approximated 200 (=1.¬Ø1 √ó 180).
Table 2.8 lists the experimental results from more-complicated stochas-
tic processes. The steady-state distributions of the MA1 and AR1 processes

Sample Sizes and Stopping Rules
49
are normal and the c.i. coverages are around the speciÔ¨Åed 90% conÔ¨Ådence
level. For the M/M/1 queuing process, samples are not only highly corre-
lated but also far from normal; the steady-state distribution of the M/M/1
queuing process is not only asymmetric but also discontinuous at x = 0.
Hence, the lag l of the systematic samples of the M/M/1 queuing process
to appear independent distributed is signiÔ¨Åcantly larger than that of the
MA1 and AR1 processes. The coverage of the M/M/1 queuing process is
slightly lower than that of the MA1 and AR1 processes.
2.9
Summary
We have presented an algorithm for determining the simulation run length,
and a strategy for building a c.i. for the mean Œº of a steady-state simu-
lation response. The procedure estimates the required sample size based
entirely on data and does not require user intervention and can easily be
incorporated into any simulation procedures. The experimental evaluation
reveals that the procedure determines batch sizes that are suÔ¨Éciently large
for achieving adequate c.i. coverage. For independent sequences, the QI
procedures will allocate a small sample size (a little more than 200) and
deliver a valid c.i. with the default precision. As the autocorrelation of
the sequences increases, the procedures will increase the sample size to
compensate for the lack of independence. Using a straightforward test of
independence to determine the simulation run length making the procedure
easy to understand and simple to implement. Furthermore, the procedure
needs only to process each observation once and does not require storing the
entire output sequence. As such, the QI procedure is an online algorithm,
it can process its input piece-by-piece in a serial fashion without having the
entire input sequence available from the start. Online algorithm is needed
in situations where decisions must be made and resources allocated without
knowledge of the future.

Chapter 3
Generating Independent and
Identically Distributed Batch Means
Many simulation procedures are derived based on the assumption that
data are i.i.d. normal; for example, ranking and selection procedures and
multiple-comparison procedures. Batch means are the method of choice
to manufacture data that are approximately i.i.d. normal. In this chap-
ter, we discuss a procedure to manufacture batch means that appear to be
i.i.d. normal, as determined by the von Neumman test of independence and
chi-square test of normality. The procedure performs statistical analysis
on sample sequences collected on strictly stationary stochastic processes.
It determines the batch sizes based entirely on data and does not require
any user intervention. The only required condition is that the autocorrela-
tions of the stochastic process output sequence die oÔ¨Äas the lag between
observations increases, in the sense of œÜ-mixing.
While some methods attempt to estimate the SSVC for construction of
a c.i., non-overlapping batch means (NOBM) in its classical setting (i.e.,
when the number of batches is Ô¨Åxed) does not. The presented QIN method
of [Chen and Kelton (2007)] is a classical NOBM method and it does not
estimate the SSVC.
3.1
Discussion of Batch-Means Method
There are a number of batch-size-determination procedures that aim to
manufacture independent batch means. However, these methods have fo-
cused on selecting a batch size large enough to achieve near independence
of the batch means and ignored the question of normality based on the
assumption that if the batch size is large enough for the batch means to
be approximately independent, then the batch size is large enough for the
batch means to be approximately normally distributed. The procedure of
51

52
Crafts of Simulation Programming
[Law and Carson (1979)] starts with 400 batches of size 2 and doubles sam-
ple sizes every two iterations until an estimate for lag-1 correlation among
400 batch means becomes smaller than 0.4 and larger than the estimated
lag-1 correlation among 200 batch means with twice the batch size. A draw-
back of the method is that it does not address the issue of the normality of
the batch means. Another widely studied batch-means procedures are the
set of methods LBatch and ABatch, see [Fishman (2001)]. However, these
procedures require users enter the simulation run length at the beginning of
the execution. Hence, even though these procedures dynamically determine
the optimal batch sizes (so that batch means converge faster to normality),
they are Ô¨Åxed-sample-size procedures. [Schmeiser (1982)] reviews several
batch-means procedures and concludes that selecting between 10 and 30
batches should suÔ¨Éce for most simulation experiments.
3.2
Generating
Independent
and
Normally
Distributed
Batch Means
In this section, we discuss the strategy of estimating the required batch size
so that batch means appear to be independent and normally distributed as
well as a procedure to construct conÔ¨Ådence interval of the mean. [Chen and
Kelton
(2007)] denote the method quasi-independent-and-normal (QIN)
procedure.
3.2.1
Validation of Normality
Batch means that appear to be independent are not necessarily normally
distributed and vice versa. However, our experimental results indicate that
c.i.‚Äôs constructed with the assumption that samples are i.i.d. normal gen-
erally have coverages close to the nominal value when samples are inde-
pendent but not normal. Therefore, it is not as critical to ensure batch
means are normally distributed as to ensure batch means are independent
in terms of c.i. coverage. Nevertheless, ensure batch means are normal can
improve the c.i. coverage and shorten the half-width. Furthermore, some
procedures require data are i.i.d. normal.
Let z1‚àíŒ± be the 1 ‚àíŒ± quantile of the standard normal distribution.
To determine whether batch means appear to be normally distributed, we
check the proportions of the value of batch means in each interval bounded
by (-‚àû, ÀÜŒº ‚àíz0.8333S, ÀÜŒº ‚àíz0.6667S, ÀÜŒº, ÀÜŒº + z0.6667S, ÀÜŒº + z0.8333S, ‚àû), where

Generating Independent and Identically Distributed Batch Means
53
ÀÜŒº and S are, respectively, the grand sample mean of these n observations
and standard error of these b batch means.
These intervals are strategically chosen so that the proportions of batch
means in each interval are approximately equal. Under the null hypoth-
esis that the batch means are normally distributed, the proportions of
batch means in each interval are approximately (0.1667, 0.1666, 0.1667,
0.1667, 0.1666, 0.1667).
We apply the chi-square test of normality, see
[Law (2014)], to these batch means. We use a 0.9 conÔ¨Ådence level for the
chi-square test.
The powers of the independence and the normality tests increase as the
number of batches used to perform the test increases. Based on previous
experimental results we recommend the sample size used for this normality
test and the von Neumann test of independence be at least 180 for the
intend that we are using these tests. See Section 3.3 for some experimen-
tal results of the chi-square test of normality and the von Neumann test
of independence. There are other more sophisticated normality tests, for
example, the Shapiro-Wilk test of normality [Bratley et al.
(1987)]. We
chose chi-square test because it is easy to apply and serves the purpose
well, however, other normality tests can be used in place of the chi-square
test of normality in the method if users desire.
3.2.2
A Source Code of Normality test
This subroutine implements the normality test.
/* normality test */
int normality(double* BatchN, int bnum,
double muhat, double stdev)
{
/* proportion of each proportion */
static double p[6] = {0.1667,0.1666,0.1667,
0.1667,0.1666,0.1667};
int i;
int n[6] = {0, 0, 0, 0, 0, 0};
int df = 5;
for (i = 0; i < bnum; ++i) {
if (BatchN[i] < muhat - quantile_normal(0.8333)*stdev)
n[0] = n[0] + 1;

54
Crafts of Simulation Programming
else
if (BatchN[i] < muhat - quantile_normal(0.6667)*stdev)
n[1] = n[1] + 1;
else
if (BatchN[i] < muhat)
n[2] = n[2] + 1;
else
if (BatchN[i] < muhat + quantile_normal(0.8333)stdev)
n[3] = n[3] + 1;
else
if (BatchN[i] < muhat + quantile_normal(0.6667)*stdev)
n[4] = n[4] + 1;
else
n[5] = n[5] + 1;
}; /* for */
if ( chisquare_test(n, p, df) ) {
return (1);
} else {
return (0);
};
}
3.2.3
Batch Means Variance Estimator
[Fishman (2001)] classiÔ¨Åes the diÔ¨Äerence of S2 ‚àíœÉ2 into three categories:
1) error due to Ô¨Ånite sample size n; 2) error due to ignoring correlation
between batches; 3) error due to random sampling. He collectively refers
to the errors in the Ô¨Årst two categories as systematic variance error. Recall
that in NOBM n = bm, where b is the number of batches and m is the
batches size. He then points out that under relatively weak conditions, the
systematic variance error behaves as O(1/m), and the standard error of
error due to random sampling behaves as O(1/
‚àö
b). Here O(an) denotes a
quantity that converges to zero at least as fast as does the sequence {an},
as n ‚Üí‚àû. Hence, using a Ô¨Åxed number of batches the batch size m ‚àùn
would diminish the systematic variance error most rapidly. On the other
hand, if we want to reduce the error due to random sampling, we should
increase the number of batches.
The basic idea of QIN is that Eq. (2.3) has become asymptotically valid

Generating Independent and Identically Distributed Batch Means
55
when the batch means ¬ØXj, for j = 1, 2, . . . , b, appear to be independent
and normally distributed, as determined by the von Neumann test and chi-
square test. That is, the degree of systematic variance error has diminished
to a level that can be neglected. Hence, we don‚Äôt need to increase the batch
size any further since the goal is to manufacture as many as i.i.d. normal
batch means as possible with a given sample size.
If {Xi} is an i.i.d. N(Œº, œÉ2) sequence, then the 1‚àíŒ± half-width of sample
mean constructed with n observations is
w = z1‚àíŒ±/2
œÉ
‚àön.
If the sequence is divided into b non-overlapping batch means with batch
size m, i.e., n = bm, then the variance of batch means is œÉ2/m and the
1 ‚àíŒ± half-width constructed with b batch means is
w = z1‚àíŒ±/2
œÉ/‚àöm
‚àö
b
= z1‚àíŒ±/2
œÉ
‚àön.
That is, for i.i.d. normal sequences the batch size has no impact on the c.i.
half-width w when the sample size is Ô¨Åxed. However, this property generally
does not hold when the variance is unknown, and the c.i. half-width needs
to be estimated by
w = t1‚àíŒ±/2,b‚àí1
S
‚àö
b
.
(3.1)
[Chien et al. (1997)] show that under certain assumptions:
Var[S2] ‚Üí2œÉ4(b + 1)
(b ‚àí1)2
+ O(1/b2) as b ‚Üí‚àû.
(3.2)
[Fishman (2001)] shows that if S2 converges to œÉ2 in mean square, then for
Ô¨Åxed Œ± the ratio of expected widths as a function of the number of batches
b would asymptotically be
tb‚àí1,1‚àíŒ±/2
z1‚àíŒ±/2
[1 ‚àí
b + 1
2(b ‚àí1)2 ] ‚â•1, ‚àÄb ‚â•2.
Once the QIN algorithm has determined that the sample size is large
enough for the asymptotic approximation to become valid, we then compute
the mean and variance based on these batch means. The mean and the
c.i. half-width are estimated, respectively, by ¬ØX and Eq. (3.1). The Ô¨Ånal
step in the procedure is to determine whether the c.i. meets the user‚Äôs
half-width requirement, a maximum absolute half-width œµ or a maximum
relative fraction Œ≥ of the magnitude of the Ô¨Ånal point mean estimator ¬ØX.
If the relevant requirement w ‚â§œµ, or w ‚â§Œ≥| ¬ØX(n)| for the precision of

56
Crafts of Simulation Programming
the c.i. is satisÔ¨Åed, then the procedure terminates, and we return the point
estimator ¬ØX(n) and the c.i. with half-width w. If the precision requirement
is not satisÔ¨Åed, then the procedure will increase the number of batches
according to Eq. (2.4).
3.2.4
The Implementation
The procedure progressively increases the batch size until the batch means
appear to be independent, as determined by the von Neumann test of in-
dependence, and appear to be normally distributed, as determined by the
chi-square test. We divide the entire output sequence into b = 180 batches.
To reduce the storage requirement, we allocate a buÔ¨Äer with size 3b to keep
sample means, i.e., the average of l observations. Batch means are then
computed from these sample means in the buÔ¨Äer. Initially each observa-
tion is treated as a sample mean; as the procedure proceeds the number
of observations used to compute these sample means will be doubled every
two iterations.
The following shows certain properties at each iteration:
Iteration
0 1A 1B 2A 2B . . .
rA
rB
Total Observations
b 2b 3b 4b 6b . . .
2kb 2k‚àí13b
No. of Sample Means b 2b 3b 2b 3b . . .
2b
3b
No. of Observations
1
1
1
2
2 . . . 2k‚àí1
2k‚àí1
Batch Size
1
2
3
4
6 . . .
2k 2k‚àí13
The Iteration row shows the index of the iteration. The Total Obser-
vations row shows the total number of observations at a certain iteration.
The No. of Sample Means row shows the total number of sample means in
the buÔ¨Äer at a certain iteration. The No. of Observations row shows the
number of observations used to obtain the sample means in the buÔ¨Äer, i.e.,
the value of l. The Batch Size row shows the number of observations used
to compute the batch means, i.e., the batch size. Note that the sample
mean here could be the value of one observation.
There maybe b, 2b, or 3b sample means in the buÔ¨Äer. We aggregate
the available sample means into b batch means by averaging the adjacent
sample means. For example, at the end of the 2th
A iteration, the batch size is
4. We compute batch means by averaging 2 consecutive samples means that
are the average of 2 consecutive samples. The procedure will progressively
increase l (the number of observations used to obtain sample means in the

Generating Independent and Identically Distributed Batch Means
57
buÔ¨Äer) and consequently the batch size until these b batch means appear
to be independent and normally distributed.
The quasi-independent-and-normal algorithm:
(1) Remark: The size of the buÔ¨Äer used to store the sample means is
Bs = 3b, l is the number of observations used to compute sample
means in the buÔ¨Äer, r is the index of iterations.
Each iteration r
contains two sub-iterations rA and rB.
(2) Set b = 180, l = 1, and r = 0. Generate b observations as the initial
samples.
(3) If this is the initial iteration, set the value of batch means to the initial
b sample means in the buÔ¨Äer. If this is a rA iteration, set the value of
batch means to the average of two consecutive sample means in the
buÔ¨Äer. If this is a rB iteration, set the value of batch means to the
average of three consecutive sample means in the buÔ¨Äer.
(4) Carry out tests to determine whether these b batch means appear to
be independent and normally distributed. If the batch means appear
to be independent and normal, go to step 10.
(5) If the current iteration is the initial or rB iterations, set r = r +1 and
start a rA iteration. If the current iteration is rA iterations, start a
rB iteration.
(6) If this is the 1th
A or 1th
B iteration, generate b observations, store those
values in the buÔ¨Äer after the ones already there and go to step 3.
(7) If this is a rA iteration (r > 1), then re-calculate the sample means
in the buÔ¨Äer by taking the average of two consecutive sample means,
reindex the rest of 3b/2 sample means in the Ô¨Årst half of the buÔ¨Äer.
Generate another b/2 sample means; each sample mean is the average
of consecutive l = 2r‚àí1 observations and store those values in the later
portion of the buÔ¨Äer.
(8) If this is a rB iteration (r > 1), generate another b sample means;
each sample mean is the average of consecutive l = 2r‚àí1 observations
and store those values in the later portion of the buÔ¨Äer.
(9) Go to step 3.
(10) Compute the variance estimator according to Eq. (2.3) and conÔ¨Ådence
interval half-width according to Eq. (3.1).
(11) Let œµ be the desired absolute half-width and Œ≥| ¬ØX| be the desired rela-
tive half-width. If the half-width of the c.i. is greater than œµ or Œ≥| ¬ØX|,
compute b‚Ä≤, the required number of batches by Eq. (2.4), generate

58
Crafts of Simulation Programming
b‚Ä≤ ‚àíb additional batches, set b = b‚Ä≤, and go to step 10; otherwise the
procedure returns the c.i. estimator and terminates.
The QIN procedure starts with an initial sample size of 180 and doubles
the sample sizes every two iterations. The procedure progressively increase
the batch size so that these batch means appear to be independent and
normally distributed. Hence, we can use these batch means to construct a
classical conÔ¨Ådence interval without any adjustment. The procedure needs
only process each observation once and does not require storing the entire
output sequence. That is, the QIN procedure is an online algorithm.
The main purpose of the procedure is to manufacture batch means that
appear to be independent and normally distributed. If users are more inter-
ested in using these batch means to construct c.i.‚Äôs, then smaller number of
batches with larger batch size can be used. In general, with a Ô¨Åxed-length
sequence the c.i. half-width becomes smaller as the number of batches in-
creases. For example, it is likely that the constructed c.i. will have great
coverage with large c.i. half-width if we divide the entire sequence into
two batches. For details on batch-size eÔ¨Äects in the analysis of simulation
output once the sample size is Ô¨Åxed, see [Schmeiser (1982)].
3.2.5
Discussions of Batch-Means Procedures
In this section, we discuss the rationale of the QIN procedure and point out
the diÔ¨Äerence and similarity between QIN and the procedure of [Law and
Carson (1979)], denoted LC and the set of methods LBatch and ABatch
[Fishman (2001)].
The sample size incremental strategy of QIN is very similar to that of
LC; LC doubles the sample sizes every two iterations, by setting n0 = 600,
n1 = 800, and ni = 2ni‚àí2. However, the stopping criteria are diÔ¨Äerent.
Once the stopping criteria are satisÔ¨Åed, LC constructs c.i. by dividing the
entire sequence into 40 batches. LC aggregates every 10 batch means that
appear to be independent as the Ô¨Ånal batch means and does not explicitly
check whether these batch means appear to be normally distributed. Fur-
thermore, if the obtained c.i. half-width is wider than desired, LC will keep
the number of batches at 40 and increases only the batch size. Note that
in the implementation we only need to process each observation once.
The set of methods LBatch and ABatch [Fishman (2001)] incorporates
two diÔ¨Äerent sample size incremental strategies: FNB (Fixed Number of
Batches) and SQRT (the number of batches and batch size are increased

Generating Independent and Identically Distributed Batch Means
59
by
‚àö
2 at each iteration). Both FNB and SQRT double the sample size
at each iteration. However, LBatch and ABatch require users enter the
sample size n for a simulation run. Let b be the initial number of batches.
If n equals n‚Ä≤, the minimal sample size for the batch means with batch
size n‚Ä≤/b to appear to be independent, then the batch sizes determined
by LBatch and ABatch are the same since these two procedures use FNB
rule exclusively and invoke SQRT rule only after batch means appear to be
independent. [Fishman (2001)] points out that batch-means methods that
based entirely on the FNB rule, for example LC, do obtain asymptotically
valid variance estimator S2, however, they do not converge in mean square
to œÉ2 and are not statistically eÔ¨Écient.
QIN also uses FNB rule, however, it doubles the sample size every two
iterations instead of every iteration. Moreover, QIN continues to use FNB
rule to increase batch size until batch means appear to be normally dis-
tributed. However, after QIN estimates a suÔ¨Écient large batch size such
that batch means appear to be independent and normally distributed, it
does not increase the batch size any further. For example, if the obtained
c.i. half-width is wider than desired, QIN will only increase the number of
batches with the estimated batch size, which will reduce the variation of
the variance estimator more rapidly, see Eq. (3.2). On the other hand, if
n > n‚Ä≤ LBatch and ABatch will increase both the number of batches and
batch sizes. Note that LBatch and ABatch do not explicitly check whether
batch means appear to be normally distributed and SQRT rule can ensure
a faster convergent to normality with a given n.
3.3
Empirical Experiments
In this section, we present some empirical results from simulation experi-
ments using the proposed procedure. We use 180 batch means for the von
Neumann test of independence and chi-square test of normality. We test
the procedure with six stochastic processes:
‚Ä¢ Observations are i.i.d. uniform between 0 and 1, denoted U(0,1).
‚Ä¢ Observations are i.i.d. normal with mean 0 and variance 1, denoted
N(0, 1).
‚Ä¢ Observations are i.i.d. exponential with mean 1, denoted expon(1).
‚Ä¢ Steady-state of the Ô¨Årst-order moving average process, generated by

60
Crafts of Simulation Programming
the recurrence
Xi = Œº + œµi + Œ∏œµi‚àí1 for i = 1, 2, . . . ,
where œµi is i.i.d. N(0, 1) and 0 < Œ∏ < 1. Œº is set to 0 in the experiments.
This process is denoted MA1(Œ∏).
‚Ä¢ Steady-state of the Ô¨Årst-order autoregressive process, generated by the
recurrence
Xi = Œº + œï(Xi‚àí1 ‚àíŒº) + œµi for i = 1, 2, . . . ,
where œµi is i.i.d. N(0, 1), and 0 < œï < 1. Œº is set to 0 and X0 is set
to a random variate drawn from the steady-state distribution in the
experiments. This process is denoted AR1(œï).
‚Ä¢ Steady-state of the M/M/1 delay-in-queue process with the arrival
rate (Œª) and the service rate (ŒΩ = 1). This process is denoted MM1(œÅ),
where œÅ = Œª/ŒΩ is the traÔ¨Éc intensity.
We carry out experiments to evaluate the performance of using the von
Neumann test to determine whether these batch means appear to be inde-
pendent, and using the chi-square test to determine whether these batch
means appear to be normally distributed. We check the batch size at which
these batch means appear to be independent and normally distributed, and
check the interdependence between the batch size at which these batch
means appear to be independent and the strength of the autocorrelation
of the output sequence. The conÔ¨Ådence level of the von Neumann test and
the chi-square test is set to 90% in the experiments. Note that a lower
conÔ¨Ådence level of these tests will increase the batch size and the simula-
tion run length. We evaluate the performance of using these approximately
i.i.d. normal batch means to estimate the variance of sample means.
3.3.1
Experiment 1: Independence and Normality Tests
In this experiment, we used the von Neumann test to determine whether a
sequence appears to be independent and the chi-square test to determine
whether a sequence appears to be normal. It can be viewed as batch size
m = 1 and the number of batches b = 180. Tables 3.1 and 3.2 lists the
experimental results. Each design point is based on 10,000 independent
simulation runs. The P(100(1 ‚àíŒ±ind)%) columns under Independence list
the observed percentage of correct decision when the nominal probability of
the von Neumann test is set to 1‚àíŒ±ind, i.e., the percentage of these 10,000
runs passing the von Neumann test of independence for the U(0,1), N(0, 1),

Generating Independent and Identically Distributed Batch Means
61
Table 3.1
Test of independence with 180 batch means
Test
Independence
Process
P(90.00%)
P(95.00%)
P(99.00%)
U(0,1)
89.91%
94.96%
99.00%
N(0, 1)
90.29%
95.19%
98.94%
expon(1)
90.23%
94.75%
98.54%
MA1(0.15)
74.97%
61.45%
34.77%
MA1(0.25)
97.51%
93.97%
80.28%
MA1(0.35)
99.95%
99.77%
98.08%
MA1(0.50)
100.00%
100.00%
99.98%
MA1(0.75)
100.00%
100.00%
100.00%
MA1(0.90)
100.00%
100.00%
100.00%
AR1(0.15)
75.72%
62.58%
36.60%
AR1(0.25)
97.93%
95.13%
84.13%
AR1(0.35)
99.97%
99.90%
99.15%
AR1(0.50)
100.00%
100.00%
99.99%
AR1(0.75)
100.00%
100.00%
100.00%
AR1(0.90)
100.00%
100.00%
100.00%
MM1(0.15)
77.29%
72.47%
62.13%
MM1(0.25)
96.27%
94.83%
90.65%
MM1(0.35)
99.72%
99.54%
98.87%
MM1(0.50)
100.00%
99.99%
99.98%
MM1(0.75)
100.00%
100.00%
100.00%
MM1(0.90)
100.00%
100.00%
100.00%
and expon(1) design points and failing the von Neumann test for all other
design points. The P(100(1 ‚àíŒ±nor)%) columns under Normality list the
observed percentage of correct decision when the nominal probability of
the chi-square test of normality is set to 1 ‚àíŒ±nor, i.e., the percentage of
these 10,000 runs passing the chi-square test for the N(0, 1), MA1(Œ∏), and
AR1(œï) design points and failing the chi-square test for all other design
points. We set Œ±ind and Œ±nor to 0.1, 0.05, and 0.01.
A Type I error of a hypothesis test is the event that we reject the null
hypothesis when it is true, i.e., independent sequences fail the von Neu-
mann test. The von Neumann test performs very well in terms of Type
I error, it is around the speciÔ¨Åed Œ±ind level. A Type II error is the event
that we accept the null hypothesis when it is false, i.e., correlated sequences
pass the test of independence. When the sequence is only slightly corre-
lated, Type II error is high, for example, the MA1(0.15), AR1(0.15), and
MM1(0.15) processes. Note that we can use a larger number of batches to
increase the power of test and reduce Type II error. If we mistakenly treat
slightly correlated sequences as being i.i.d., the performance measurements
should still be fairly accurate, thus, the low probability of correct decision
of those slightly correlated sequences should not pose a problem. On the

62
Crafts of Simulation Programming
Table 3.2
Test of normality with 180 batch means
Test
Normality
Process
P(90.00%)
P(95.00%)
P(99.00%)
U(0,1)
80.41%
65.46%
31.86%
N(0, 1)
96.64%
98.30%
99.72%
expon(1)
99.93%
99.82%
99.42%
MA1(0.15)
96.28%
98.37%
99.72%
MA1(0.25)
96.11%
98.25%
99.69%
MA1(0.35)
96.36%
98.40%
99.68%
MA1(0.50)
96.56%
98.52%
99.81%
MA1(0.75)
96.56%
98.58%
99.78%
MA1(0.90)
96.34%
98.43%
99.73%
AR1(0.15)
96.65%
98.40%
99.70%
AR1(0.25)
96.26%
98.23%
99.74%
AR1(0.35)
96.44%
98.46%
99.83%
AR1(0.50)
96.35%
98.32%
99.70%
AR1(0.75)
93.54%
96.70%
99.10%
AR1(0.90)
78.31%
86.01%
94.20%
MM1(0.15)
100.00%
100.00%
100.00%
MM1(0.25)
100.00%
100.00%
100.00%
MM1(0.35)
100.00%
100.00%
100.00%
MM1(0.50)
100.00%
100.00%
100.00%
MM1(0.75)
95.90%
93.73%
87.21%
MM1(0.90)
83.08%
77.27%
65.02%
other hand, when the output sequences are mild to highly correlated, the
von Neumann test detects the dependence almost all the time. The results
of the normality test indicate that the autocorrelations among the samples
has very little impact on the chi-square normality test when the autocor-
relations are small. However, as the autocorrelations become stronger the
chi-square normality test starts to break down, for example, the AR1(0.9)
and MM1(0.9) processes. Even though the von Neumann test of indepen-
dence does not guarantee independence, the sequence passes the test will
only be slightly correlated if it is not independent. Hence, the residual cor-
relation (if there is any) left in the batch means will have little impact on
the chi-square normality test in the QIN procedure. Since Œ±ind = 0.1 and
Œ±nor = 0.1 have a better performance in balancing Type I and II errors, we
chose Œ±ind = 0.1 and Œ±nor = 0.1 for these tests in the procedure.
3.3.2
Experiment 2: Batch Sizes Determination
In this section, we check the batch size at which the batch means appear to
be normally distributed and the batch size at which the batch means appear
to be independent with respect to the strength of the autocorrelation of

Generating Independent and Identically Distributed Batch Means
63
Table 3.3
Average batch size m at which these 180 batch means ap-
pear to be normally distributed (pass the chi-square normality test)
Process
U(0, 1)
N(0, 1)
expon(1)
¬Øm
stdv(m)
¬Øm
stdv(m)
¬Øm
stdv(m)
1.85
0.46
1.04
0.19
3.96
1.30
the output sequence. Each design point is based on 10,000 independent
simulation runs.
Table 3.3 lists the experimental results. The ¬Øm column lists the average
batch size at which the batch means appear to be normally distributed.
The stdv(m) column lists the standard deviation of the batch size m. For
example, in average with batch size of 1.85 U(0, 1) observations appear to
be normally distributed. The procedure correctly detects the normality of
the observations sampled from the N(0, 1) distribution; hence, the average
batch size m is 1.04 close to the theoretical value of 1. This is consistent
with the result in experiment 1 that the test of normality encounters Type
I error approximately 4% of the time with Œ±nor = 0.1, consequently, the
average batch size is approximately 1 √ó 0.96 + 2 √ó 0.04.
Table 3.4 lists the experimental results of the batch size at which the
batch means appear to be independent. The Œ∏, œï, œÅ column lists the coeÔ¨É-
cient values of the corresponding stochastic process. The MA1(Œ∏) column
lists the results of the underlying moving average output sequences.
In
general, the average batch size at which the batch means appear to be
independent increases as the autocorrelation increases. The MA1(Œ∏) pro-
cesses are only slightly correlated even with Œ∏ as large as 0.9. Therefore,
in average batch means with batch size of 4 of the MA1(0.90) output se-
quences appear to be independent.
On the other hand, the MM1(0.90)
output sequences are highly correlated, the average batch size at which the
batch means appear to be independent is as large as 321. The results from
this experiment indicate a strong correlation between the average batch size
m at which the batch means appear to be independent and the strength of
the autocorrelation.
We also performed the experiment using diÔ¨Äerent number of batches for
the underlying independence and normality tests. The average batch size
at which the batch means appear to be independent and normal generally
increases as the number of batches used for those tests increases since power
of test increases as sample size increases. We choose to use sample size
of 180 for these tests because based on previous experimental tests using
this sample size meets the precision requirement and does not cause the

64
Crafts of Simulation Programming
Table 3.4
Average batch size m at which these 180 batch means
appear to be independent (pass von Neumann test of independence)
Process
MA1(Œ∏)
AR1(œï)
MM1(œÅ)
Œ∏,œï,œÅ
¬Øm
stdv(m)
¬Øm
stdv(m)
¬Øm
stdv(m)
0.15
2.07
0.86
2.22
0.99
2.78
1.48
0.25
2.64
0.89
3.43
1.21
5.16
2.09
0.35
2.93
0.96
4.65
1.39
8.20
2.57
0.50
3.22
1.04
6.91
1.73
15.54
4.03
0.75
3.44
1.09
14.77
2.87
60.74
12.46
0.90
3.48
1.10
34.68
5.46
320.68
55.58
simulation to run longer than necessary.
3.3.3
Experiment 3: Coverages of ConÔ¨Ådence Interval
In this experiment, we use the QIN procedure to determine batch sizes so
that batch means appear to be independent and normally distributed. Since
batch means are often used to estimate the variance of sample means, we
evaluate the accuracy of the variance estimated by the procedure. In these
experiments, no relative precision or absolute precision were speciÔ¨Åed, so
the half-width of the c.i. is the result of the default precision. Furthermore,
based on the common rule of thumb that the optimal number of batch
means is around 30 to 40 when using batch means to estimate variance
and the sample size is Ô¨Åxed, in this experiment we list the results of c.i.
coverage when the entire sequences are divided into 180 batches as well as
30 batches.
Table 3.5 lists the experimental results of sampling independently from
three diÔ¨Äerent distributions. The Œº row lists the true mean. The ¬ØX row
lists the grand sample mean. The avg and stdv rp rows list, respectively,
the average relative precision and standard deviation of the relative pre-
cision of the point estimator.
Here, the relative precision is deÔ¨Åned as
Œ≥ = | ¬ØX ‚àíŒº|/ ¬ØX, where |x| is the absolute value of x. The avg and stdv
samp rows list, respectively, the average sample size and standard devia-
tion of the sample size. The avg and stdv bsize rows list, respectively, the
average batch size and standard deviation of the batch size obtained by the
procedure. The avg and stdv hw rows list, respectively, the average half-
width and standard deviation of the half-width obtained by the procedure.
The coverage row lists the percentage of the c.i.‚Äôs that cover the true mean
value. The average batch size is 1.19 when sampling from i.i.d. normal dis-
tribution, which is close to the theoretical value of 1 and reÔ¨Çects the fact
that the underlying observations are independent and normally distributed.

Generating Independent and Identically Distributed Batch Means
65
Table
3.5
Coverage
of
90%
conÔ¨Ådence
inter-
vals of independent samples
Process
U(0, 1)
N(0, 1)
expon(1)
Œº
0.50
0.00
1.00
¬Ø
X
0.499828
-0.000299
1.000176
avg rp
0.025552
1.0
0.028747
stdv rp
0.020226
0.0
0.023740
avg samp
365
214
973
stdv samp
166
111
800
batches
180
avg bsize
2.03
1.19
5.41
stdv bsize
0.92
0.62
4.45
avg hw
0.026186
0.117864
0.058698
stdv hw
0.004453
0.014815
0.013726
coverage
90.09%
90.57%
89.85%
batches
30
avg bsize
12.17
7.12
32.44
stdv bsize
5.54
3.70
26.68
avg hw
0.026469
0.119077
0.059236
stdv hw
0.005379
0.020267
0.015295
coverage
89.97%
90.25%
89.75%
On the other hand, the batch means do not appear to be normal until the
average batch size is 2.03 when sampling from U(0,1) distribution. Because
exponential distribution is asymmetric, the batch means do not passed the
normality test until the average batch size is 5.41. The c.i. coverages are
around the speciÔ¨Åed 90% conÔ¨Ådence level. For independent observations,
it seems that the batch sizes determined by the procedure are large enough
for these batch means to be normally distributed so that there is no signif-
icant diÔ¨Äerence in coverages when batch sizes are increased 6 times. This
is consistent with the discussion in Section 3.2.3.
Table 3.6 lists the experimental results of the chosen stochastic pro-
cesses. For these three tested processes, the c.i. coverages are around the
speciÔ¨Åed 90% conÔ¨Ådence level. Since the steady-state distribution of the
MA1 and AR1 processes are normal, we believe that some of the batch
means that passed the test of independence may be slightly correlated. For
the M/M/1 queuing process, samples are not only highly correlated but
also far from normal; the steady-state distribution of the M/M/1 queuing
process is not only asymmetric but also discontinuous at x = 0. Hence,
the batch size for the M/M/1 queuing process to appear independent and
normally distributed is signiÔ¨Åcantly larger than that of the MA1 and AR1
processes. Consequently, the QIN procedure will allocate a large sample

66
Crafts of Simulation Programming
Table
3.6
Coverage
of
90%
conÔ¨Ådence
intervals
of correlated samples
Process
MA1(0.9)
AR1(0.9)
MM1(0.9)
Œº
2.00
2.00
9.00
¬Ø
X
1.998995
1.998049
8.989608
avg rp
0.028616
0.036842
0.012098
stdv rp
0.023504
0.031344
0.010345
avg samp
830
14560
3296931
stdv samp
980
12481
2992981
batches
180
avg bsize
4.61
80.89
18316.29
stdv bsize
5.45
69.34
16627.67
avg hw
0.113788
0.140637
0.201692
stdv hw
0.021196
0.026812
0.064079
coverage
88.4%
88.1%
87.0%
batches
30
avg bsize
27.66
485.33
109897.73
stdv bsize
32.68
416.05
99766.04
avg hw
0.119555
0.147523
0.203708
stdv hw
0.026072
0.033256
0.067350
coverage
89.3%
89.2%
86.7%
size and deliver a tight c.i. when underlying processes are asymmetric and
discontinuous. In addition to some correlated batch means pass the test
of independence, some non-normal batch means pass the test of normality,
thus, the coverage of the M/M/1 queuing process is slightly lower than that
of the MA1 and AR1 processes.
3.4
Summary
We have presented an algorithm for estimating the required batch size so
that batch means appear to be i.i.d. normal and a strategy for building a
c.i. on a steady-state simulation response. The QIN algorithm works well
in determining the required batch size for the asymptotic approximation of
batch means to become valid. The procedure estimates the required sample
size based entirely on data and does not require any user intervention.
Moreover, the QIN procedure needs to process each observation only once
and does not require storing the entire output sequence so the storage
requirement is minimal. The QIN procedure can be used as a pre-processor
of simulation procedures that require i.i.d. normal data. The experimental
evaluation reveals that QIN determines batch sizes that are suÔ¨Éciently large
for achieving approximately i.i.d. normal batch means and for achieving

Generating Independent and Identically Distributed Batch Means
67
adequate c.i. coverage. The main advantage of the approach is that by using
a straightforward test of independence and test of normality to determine
the valid batch size so that batch means appear to be independent and
normally distributed, we can apply classical statistical techniques directly
and do not require more advanced statistical theory, thus making it easy
to understand and simple to implement.

Chapter 4
Distributions of Order Statistics
Order statistics are among the most fundamental tools in non-parametric
statistics and inference and have been widely studied and applied to many
real-world issues. It is often of interest to estimate the reliability of the
component/system from the observed lifetime data. In many applications
we want to estimate the probability that a future observation will exceed a
given high level during some speciÔ¨Åed epoch. For example, a machine may
break down if a certain temperature is reached.
Let Xi, i = 1, 2, . . . , n, denote a sequence of mutually independent
random samples from a common distribution of the continuous type hav-
ing a probability density function (pdf) f and a cumulative distribution
function (cdf) F.
Let X[u] be the uth smallest of these Xi such that
X[1] ‚â§X[2] ‚â§¬∑ ¬∑ ¬∑ ‚â§X[n]. Then X[u], u = 1, 2, . . . , n, is called the uth order
statistic of the random sample Xi, i = 1, 2, . . . , n. Note that though the
samples X1, X2, ¬∑ ¬∑ ¬∑ , Xn are i.i.d., the order statistics X[1], X[2], ¬∑ ¬∑ ¬∑ , X[n]
are not independent because of the order restriction.
The diÔ¨Äerence
R = X[n] ‚àíX[1] is called the sample range. It is a measure of the dis-
persion in the sample and should reÔ¨Çect the dispersion in the population.
For more information on order statistics see [David and Nagaraja (2003)].
Suppose U1, U2 ‚àºU(0, 1), where ‚Äú‚àº‚Äù denotes ‚Äúis distributed as‚Äù and
‚ÄúU(a, b)‚Äù denotes a uniform distribution with range [a, b]. We are interested
in the distribution of U[2] = max(U1, U2), which can be viewed as the second
order statistic. The cdf of U[2] is
Pr(max(U1, U2) ‚â§x) = Pr(U1 ‚â§x, U2 ‚â§x)
= Pr(U1 ‚â§x)Pr(U2 ‚â§x)
= x2.
69
by 
nly.

70
Crafts of Simulation Programming
Furthermore, the cdf of U[1] = min(U1, U2), which can be viewed as the
Ô¨Årst order statistic, is
Pr(min(U1, U2) ‚â§x) = 1 ‚àíPr(min(U1, U2) > x)
= 1 ‚àíPr(U1 > x)Pr(U2 > x)
= 1 ‚àí(1 ‚àíx)2.
Consider a device contains a series of two identical components and the
lifetime of the component Lc ‚àºU(0, 1). If any of these two components
fail, the device fails, such as chains. Hence, the lifetime of the device Ld ‚àº
min(Lc1, Lc2). On the other hand, if these two components are conÔ¨Ågured
in parallel and the device fails only when both components fail, such as
a strand contains a bundle of threads.
Then the lifetime of the device
Ld ‚àºmax(Lc1, Lc2). These techniques can be used in reliability analysis to
estimate the reliability of the component/system from the observed lifetime
data. When there are more than two samples, the distributions of order
statistics can be obtained via the same technique.
[Hogg et al. (2012)] show that the distribution of the uth order statistic
of n samples of X (i.e., X[u]) is
gu:n(x) = Œ≤(F(x); u, n ‚àíu + 1)f(x),
where
Œ≤(x; a, b) =
1
Œ≤(a, b)xa‚àí1(1 ‚àíx)b‚àí1 and Œ≤(a, b) = Œì(a)Œì(b)
Œì(a + b)
is the beta function with shape parameters a and b. Note that Œì(a) =
(a ‚àí1)! for any positive integer a. Furthermore, the cdf
Gu:n(x) =
 x
‚àí‚àû
gu:n(y)dy.
The cdf Gu:n(x) can also be expressed as
Pr(X[u] ‚â§x) = Pr(at least u of the Xi are not greater than x)
=
n

i=u
n
i

F i(x)(1 ‚àíF(x))n‚àíi.
The last equality holds because each term in the summation corresponds
to the probability that exactly i of the X1, X2, ¬∑ ¬∑ ¬∑ , Xn are not greater than
x.
by 
nly.

Distributions of Order Statistics
71
In particular, the Ô¨Årst order statistic, or the sample minimum, X[1] has
the pdf
g1:n(x) = n[1 ‚àíF(x)]n‚àí1f(x)
and the cdf
G1:n(x) = 1 ‚àí[1 ‚àíF(x)]n.
The nth order statistic, or the sample maximum, X[n] has the pdf
gn:n(x) = n[F(x)]n‚àí1f(x)
and the cdf
Gn:n(x) = [F(x)]n.
In the case that f is the uniform [0,1] distribution,
gu:n(x) = Œ≤(x; u, n ‚àíu + 1).
That is, let x = U ‚àºU(0, 1), then the random variate y = Œ≤(U; u, n‚àíu+1),
is the uth order statistic of n random variables with uniform [0, 1] as the
parent distribution. From this we can deduce that
E[X[u]] =
u
n + 1,
(4.1)
and
Var[X[u]] =
u(n ‚àíu + 1)
(n + 1)2(n + 2).
This result can be used to generate the uth order statistic of any variates
with a strictly increasing cdf. A straightforward way of simulating order
statistics is to generate a pseudorandom sample from the distribution F(X)
and then sort the sample in ascending order. This general method requires
sorting and can be avoided by using the probability integral transform. The
probability integral transform or transformation states that data values that
are modelled as random variables from any given continuous distribution
can be converted to random variables having a uniform distribution. Based
on the fact that y = F(x) ‚àºU(0, 1), we generate a random variate y =
Œ≤(U; u, n ‚àíu + 1), where U ‚àºU(0, 1), and return the quantile x = F ‚àí1(y)
as the uth order statistic with parent distribution F. Recall that F ‚àí1 is
the inverse distribution function (or quantile function) and is deÔ¨Åned by
F ‚àí1(y) = inf{x : F(x) ‚â•y}.
In the case that F is uniform [0, 1], y = F ‚àí1(y).
Note that if
U1, U2, . . . , Un are i.i.d. U(0, 1) random variables and X1, X2, . . . , Xn are
i.i.d. random variables with common distribution function F such that
Xi = F ‚àí1(Ui), then X[i] = F ‚àí1(U[i]).
by 
nly.

72
Crafts of Simulation Programming
4.1
Joint and Conditional Distributions of Order Statistics
The joint density function of (X[u], X[v]), 1 ‚â§u < v ‚â§n is denoted by
gu,v:n(x, y). It can be shown that
gu,v:n(x, y) = n![F(x)]u‚àí1
(u ‚àí1)!
[1 ‚àíF(y)]n‚àív
(n ‚àív)!
[F(y) ‚àíF(x)]v‚àíu‚àí1
(v ‚àíu ‚àí1)!
f(x)f(y).
In particular, the minimum and the maximum, (X[1], X[n]), have the joint
density
g1,n:n(x, y) = n(n ‚àí1)[F(y) ‚àíF(x)]n‚àí2f(x)f(y).
From the joint distribution of two order statistics we can Ô¨Ånd the distribu-
tion of various other statistics, e.g., the sample range R = X[n] ‚àíX[1]. The
pdf and cdf of the sample range R, respectively, are
gR(R) = n(n ‚àí1)
 ‚àû
‚àí‚àû
[F(x + R) ‚àíF(x)]n‚àí2f(x)f(x + R)dx,
and
GR(R) = n
 ‚àû
‚àí‚àû
[F(x + R) ‚àíF(x)]n‚àí1f(x)dx.
Because F(x + R) may not be expressed as a function of F(x), it often
requires numerical integration to solve the integration.
In the case that
F = Œ¶ is the standard normal distribution, [Tippett (1925)] showed that
the mean value of R is given by
E(R) =
 ‚àû
‚àí‚àû
1 ‚àí[Œ¶(x)]n ‚àí[1 ‚àíŒ¶(x)]ndx.
Furthermore, the joint pdf of any number of the order statistics can be
constructed, in particular, the joint pdf of all of the order statistics is
g1,2,...,n:n(X[1], X[2], . . . , X[n]) = n!f(X[1])f(X[2]) . . . f(X[n]).
In the case that f is the uniform [0,1] distribution,
gu,v:n(x, y) = n! xu‚àí1
(u ‚àí1)!
[1 ‚àíy]n‚àív
(n ‚àív)!
[y ‚àíx]v‚àíu‚àí1
(v ‚àíu ‚àí1)! ,
and
Cov[X[u], X[v]] =
u(n ‚àív + 1)
(n + 1)2(n + 2).
As expected, Cov[X[u], X[v]] decreases as the diÔ¨Äerence between v ‚àíu in-
creases, given u or v. Moreover, Cov[X[1], X[n]] ‚Üí0, as n ‚Üí‚àû. That is,
by 
nly.

Distributions of Order Statistics
73
in large samples the minimum and maximum are in general approximately
independent. Moreover, the pdf and cdf of the sample range, respectively,
are
gR(R) = n(n ‚àí1)Rn‚àí2
 1‚àíR
0
dx = Œ≤(R; n ‚àí1, 2),
and
GR(R) = n(1 ‚àíR)Rn‚àí1 + Rn.
From this we can deduce that
E[R] = n ‚àí1
n + 1,
and
Var[R] =
2(n ‚àí1)
(n + 1)2(n + 2).
That is, the sample range R approaches the true range and variance of the
sample range R becomes smaller as the sample size n becomes larger.
For 1 ‚â§u < v ‚â§n, the conditional distribution of X[u] given X[v] = xb
is the same as the unconditional distribution of the uth order statistic in a
sample of size v‚àí1 from a new distribution, namely the original F truncated
at the right at xb. In notation,
gX[u]|X[v]=xb(x) =
(v ‚àí1)!
(u ‚àí1)!(v ‚àí1 ‚àíu)!( F(x)
F(xa))u‚àí1(1 ‚àíF(x)
F(xa))v‚àí1‚àíu f(x)
F(xa),
x < xb.
Similarly, for 1 ‚â§u < v ‚â§n, the conditional distribution of X[v] given
X[u] = xa is the same as the unconditional distribution of the (v ‚àíu)th
order statistic in a sample of size n ‚àíu from a new distribution, namely
the original F truncated at the left at xa. In notation,
gX[v]|X[u]=xa(x) =
(n ‚àíu)!
(v ‚àíu ‚àí1)!(n ‚àív)!(F(x) ‚àíF(xa)
1 ‚àíF(xa)
)v‚àíu‚àí1( 1 ‚àíF(x)
1 ‚àíF(xa))n‚àív
f(x)
1 ‚àíF(xa),
x > xa.
For more detail, see [DasGupta (2011)].
by 
nly.

74
Crafts of Simulation Programming
4.2
Using Range Statistics to Perform Equivalence Tests
In this section, we preview the indiÔ¨Äerence-zone approach and show how
range statistics are applied in simulation to perform equivalence tests.
4.2.1
IndiÔ¨Äerence-Zone Selection
Let Œºil be the lth smallest of the Œºi‚Äôs, so that Œºi1 ‚â§Œºi2 ‚â§. . . ‚â§Œºik. In
selection, the goal is to select the best system with the smallest (or largest)
expected responses, i.e., system i1 (or ik). Let CS denote the event of ‚Äúcor-
rect selection.‚Äù In a stochastic simulation, a CS can never be guaranteed
with certainty. The probability of CS, denoted by P(CS), is a random vari-
able depending on sample sizes and other uncontrollable factors. Moreover,
in practice, if the diÔ¨Äerence between Œºi1 and Œºi2 is very small, we might
not care if we mistakenly choose system i2, whose expected response is Œºi2.
The ‚Äúpractically signiÔ¨Åcant‚Äù diÔ¨Äerence d‚àó(a positive real number) between
a desired and a satisfactory system is called the indiÔ¨Äerence zone in sta-
tistical literature, and it represents the smallest diÔ¨Äerence which we care
about. Therefore, we want a procedure that avoids making a large number
of replications or batches to resolve diÔ¨Äerences less than d‚àó. That means
we want P(CS) ‚â•P ‚àóprovided that Œºi2 ‚àíŒºi1 ‚â•d‚àó, where the minimal CS
probability P ‚àóand the ‚ÄúindiÔ¨Äerence‚Äù amount d‚àóare both speciÔ¨Åed by the
users.
The indiÔ¨Äerence-zone selection procedure of [Dudewicz and Dalal
(1975)] to select the smallest performance measure of k system proceeds
as follows.
(1) Simulate the initial n0 samples for all systems. Compute the Ô¨Årst-
stage sample means with the equation
¬ØX(1)
i
(n0) = 1
n0
n0

j=1
Xij,
and the sample variances with the equation
S2
i (n0) =
n0
j=1(Xij ‚àí¬ØX(1)
i
(n0))2
n0 ‚àí1
,
for i = 1, 2, . . . , k.
(2) Compute the required sample sizes with the equation
Ni = max(n0 + 1, ‚åà(h1Si(n0)/d‚àó)2‚åâ), for i = 1, 2, . . . , k,
(4.2)
by 
nly.

Distributions of Order Statistics
75
where ‚åàz‚åâis the smallest integer that is greater than or equal to the
real number z, and h1 is a critical constant will be described later.
(3) Simulate additional Ni ‚àín0 samples, for i = 1, 2, . . . , k.
(4) Compute the second-stage sample means with the equation
¬ØX(2)
i
(Ni ‚àín0) =
1
Ni ‚àín0
Ni

j=n0+1
Xij, for i = 1, 2, . . . , k.
(5) Compute the weighted sample means with the equation
ÀúXi(Ni) = Wi1 ¬ØX(1)
i
(n0) + Wi2 ¬ØX(2)
i
(Ni ‚àín0), for i = 1, 2, . . . , k
and select the system with the smallest ÀúXi(Ni).
Note that h1 (which depends on k, P ‚àó, and n0) is a constant that
can be found from the tables in [Law
(2014)] and can be estimated by
the procedure of [Chen and Li (2010)]. Furthermore, the weights can be
derived from the equations
Wi1 = n0
Ni

1 +

1 ‚àíNi
n0

1 ‚àí(Ni ‚àín0)(d‚àó)2
h2S2
i (n0)

(4.3)
and Wi2 = 1 ‚àíWi1, for i = 1, 2, . . . , k. The expression for Wi1 was chosen
to guarantee ( ÀúXi(Ni)‚àíŒºi)/(d‚àó/h1) would have a t distribution with n0 ‚àí1
d.f.; see [Dudewicz and Dalal (1975)].
4.2.2
Variance of Weighted Sample Means
Let ÀúXi (i.e., ÀúXi(Ni)) be the weighted sample means as deÔ¨Åned. The proce-
dure of [Dudewicz and Dalal (1975)] is derived based on the fact that
Ti =
ÀúXi ‚àíŒºi
d‚àó/h1
,
for i = 1, 2, ¬∑ ¬∑ ¬∑ , k have a t distribution with n0 ‚àí1 d.f. They point out that
ÀúXi ‚àíŒºi
Si(n0)/‚àöai
‚àºN(0, œÉ2
i /S2
i (n0)),
where ai = (h1Si(n0)/d‚àó)2. That means
ÀúXi ‚àíŒºi
œÉi/‚àöai
‚àºN(0, 1)
and ÀúXi ‚àºN(Œºi, œÉ2
i /ai). Recall that ¬ØXi ‚àºN(Œºi, œÉ2
i /Ni) and œÉ2
i /Ni ‚â§œÉ2
i /ai
because ai ‚â§Ni = max(n0 + 1, ‚åà(h1Si(n0)/d‚àó)2‚åâ). The diÔ¨Äerence between
by 
nly.

76
Crafts of Simulation Programming
œÉ2
i /Ni and œÉ2
i /ai is more apparent when ai < n0 and the diÔ¨Äerence increases
as ai(< n0) becomes smaller. For example, if ai = 1, then Ni = n0 + 1 and
the variances of ¬ØXi and ÀúXi are œÉ2
i /(n0 + 1) and œÉ2 respectively. Note that
Ti =
¬ØXi ‚àíŒºi
S/‚àöNi
,
for i = 1, 2, ¬∑ ¬∑ ¬∑ , k are t distributed with Ni ‚àí1 d.f., hence, they likely have
diÔ¨Äerent distributions, i.e., diÔ¨Äerent d.f.
Consequently, if Œºi < Œºj, then Pr[ ÀúXi <
ÀúXj] ‚â§Pr[ ¬ØXi <
¬ØXj]. Even
though Pr[ ÀúXi1 < ÀúXil for l = 2, 3, . . . , k] ‚â§Pr[ ¬ØXi1 < ¬ØXil for l = 2, 3, . . . , k]
has not been proven to be true, we believe it is fairly safe to use ¬ØXi instead
of ÀúXi to perform selection. For a discussion of the performance of using
the weighted sample means and the overall sample means, please see [Chen
(2011)].
4.2.3
EÔ¨Äects of the IndiÔ¨Äerence Amount and Sample Size
Fig. 4.1
The probability density of performance measures of systems i and j
Traditionally, optimization procedures will resolve to a single selection
from all alternatives and hope the selected system is the true best one.
With indiÔ¨Äerence-zone and ordinal optimization, we are selecting a set of
good enough alternatives with high probability. One of the tenets of ordinal
optimization indicates that it is much easier to determine whether or not
alternative i is better than j than to determine the distance dij = Œºj ‚àí
Œºi.
Let ¬ØXi(Ni) and ¬ØXj(Nj) be the average of Ni and Nj, respectively,
normally distributed performance measures of two simulation experiments.
by 
nly.

Distributions of Order Statistics
77
The probability of mis-aligning systems i and j is roughly proportional to
the area (of mis-alignment) under the overlapping tail of the two density
functions center at Œºi and Œºi + dij as illustrated in Figure 4.1, see [Ho
(1996)]. Note that d‚àó< dij in this case. Moreover, the size of the area is
dependent on the variances of ¬ØXi(Ni) and ¬ØXi(Ni) as well as the distance
dij. The smaller the variances and the larger the distance, the smaller the
area of mis-alignment. For example, the area under the overlapping tail of
the two density functions centered at Œºi and Œºi + dij is smaller than the
center at Œºi and Œºi + d‚àó.
When we increase the size of the indiÔ¨Äerence amount d‚àó, we increase
the alignment probability because the area of mis-alignment will be smaller
and the number of designs i such that Œºi < Œºi1 + d‚àómay increase, i.e.,
the number of members in the subset G may increase. Consequently, the
required sample sizes to achieve the speciÔ¨Åed P(CS) will be smaller. Hence,
increasing the indiÔ¨Äerence amount can ease the computational burden.
In testing the null hypothesis H0 : Œºi ‚â§Œºi1, for us to reject the null
hypothesis and conclude with conÔ¨Ådence level 1 ‚àíŒ± that Œºi > Œºi1 is the
same as the lower endpoint of the one-tailed 1 ‚àíŒ± c.i. is positive, i.e.,
¬ØXi ‚àí¬ØXi1 ‚àíwii1 > 0, where wii1 denotes the half-width of the one-tailed
1 ‚àíŒ± c.i. of Œºi ‚àíŒºi1. The half-width wii1 depends on the sample sizes and
becomes smaller as the sample sizes become large. This implies the sample
sizes (Ni and Ni1) should be large enough so that wii1 < ¬ØXi ‚àí¬ØXi1. By
symmetry of the normal distribution Pr[ ¬ØXi‚àí¬ØXi1 ‚â•(Œºi‚àíŒºi1)‚àíwii1] ‚â•1‚àíŒ±.
To obtain Pr[ ¬ØXi ‚àí¬ØXi1 > 0] ‚â•1‚àíŒ±, the sample size should be large enough
so that wii1 < Œºi ‚àíŒºi1.
Let ÀÜdi1il = ¬ØXil ‚àí¬ØXi1. Procedures developed based on the LFC achieve
wili1 < d‚àóand consequently the one-tailed 1 ‚àíŒ± c.i. of di1il CI1 = ( ÀÜdi1il ‚àí
d‚àó, ‚àû]. Whereas procedures that take into account sample means attempt
to achieve wili1 < di1il = Œºil ‚àíŒºi1 and CI2 = ( ÀÜdi1il ‚àídi1il, ‚àû] ‚âà(0, ‚àû].
Hence, the allocated sample sizes are just large enough for us to conclude
Œºi1 < Œºi (provided Œºi1 + d‚àó‚â§Œºi) with a desired conÔ¨Ådence but no more
than necessary.
4.2.4
Equivalence Tests
This section investigates the hypothesis testing of equivalence of mean of
multiple systems using range statistics.
The most useful comparison of
the system means is done by comparing each system with the unknown
best system or best systems within the group.
Consider sample means
by 
nly.

78
Crafts of Simulation Programming
of certain observations drawn at random from k systems. The diÔ¨Äerence,
R = ¬ØX[k]‚àí¬ØX[1], between the largest and the smallest sample means is called
the sample range. The ratio, q = R/S, of the range to an independent root-
mean-square estimate, S, of the population standard deviation, œÉ, is called
a studentized range. The studentized range is the test statistic of Tukeys
range test to Ô¨Ånd which means are signiÔ¨Åcantly diÔ¨Äerent from one another
based on the distribution of q. Note that Tukeys range test assumes equal
variances and uses equal sample sizes for each system.
In these procedures, we consider the equivalence tests with critical re-
gion CR = { ¬ØX[k] ‚àí¬ØX[1] ‚â§Œ¥}, where Œ¥ > 0 is a user-speciÔ¨Åed upper bound of
the sample range for the hypothesis test. Note that in these cases variances
may be unequal among systems and the studentized range can no longer
be computed in a straightforward manner.
Let
Ti =
ÀúXi ‚àíŒºi
Œ¥/h3
for i = 1, 2, . . . , k.
Note that the weight needs to be computed with d‚àóreplaced by Œ¥ and the
required sample sizes ni need to be computed by Eq. (4.4), which and the
value of the critical constant h3 will be discussed later.
Moreover, Ti‚Äôs
are independent t-distributed random variables with n0 ‚àí1 d.f.
Let F
denote the cdf of the t distribution with n0 ‚àí1 d.f. Let P(CD) denote
the probability of correct decision and let ÀúXci denote the ith smallest ÀúXi.
Under the null hypothesis that Œºi1 = Œºi2 = ¬∑ ¬∑ ¬∑ = Œºik, we can write
P(CD) = Pr[ ÀúXck ‚àíÀúXc1 ‚â§Œ¥]
= Pr [Tck ‚àíTc1 ‚â§h3]
= GR(h3)
= k
 ‚àû
‚àí‚àû
[F(t + h3) ‚àíF(t)]k‚àí1f(t)dt.
The last equality follows because of the cdf of the sample range. We equate
the right-hand side to P ‚àóand solve for h3. Numerical approximation has
been used to solve the integration. Nevertheless, other approach can be
used to solve h3, which will be discussed later. Furthermore,
P(CD) = Pr[ ÀúXck ‚àíÀúXc1 ‚â§Œ¥]
‚â•
 ‚àû
‚àí‚àû
[Gk:k(t + h3)]kg1:k(t)dt
=
 ‚àû
‚àí‚àû
[F(t + h3)]kg1:k(t)dt.
by 
nly.

Distributions of Order Statistics
79
The inequality follows because Tck and Tc1 are positively correlated.
[Chen (2014a)] proposed using the following procedure to test the null
hypothesis that all means are equal.
(1) Simulate the initial n0 samples for all systems.
Compute the Ô¨Årst-
stage sample means ¬ØX(1)
i
(n0) and the sample variances S2
i (n0) for i =
1, 2, . . . , k.
(2) Compute the required sample sizes with the formula
Ni = max(n0 + 1, ‚åà(h3Si(n0)/Œ¥)2‚åâ) for i = 1, 2, . . . , k.
(4.4)
(3) Simulate additional Ni ‚àín0 samples.
(4) Compute the second-stage sample means with the formula
¬ØX(2)
i
=
1
Ni ‚àín0
Ni

j=n0+1
Xij for i = 1, 2, . . . , k.
(5) Compute the weighted sample means ÀúXi(Ni). Reject the null hypoth-
esis that all means are equal when ÀúXck ‚àíÀúXc1 > Œ¥; otherwise do not
reject the null hypothesis.
4.2.5
ConÔ¨Ådence Interval Half Width of Interest
Let œë = Tck ‚àíTc1. Then the cdf of œë
GR(h3) = Pr[œë ‚â§h3].
Note that the cdf GR(h3) is determined only by the d.f. of the t-distribution
given k. By deÔ¨Ånition h3 is the P ‚àóquantile of the distribution of œë when
GR(h3) = P ‚àó.
Furthermore,
œë = Tck ‚àíTc1
= ( ÀúXck ‚àíÀúXc1) ‚àí(Œºck ‚àíŒºc1)
Œ¥/h3
=
( ÀúXck ‚àíÀúXc1) ‚àí(Œºck ‚àíŒºc1)

S2ck(n0)/(2Nck) + S2c1(n0)/(2Nc1).
Without loss of generality, we temporarily assume that Nck and Nc1 are
real numbers. The last equality holds because
S2
ck(n0)
2Nck
= S2
c1(n0)
2Nc1
= Œ¥2
2h2
3
and

S2ck(n0)
2Nck
+ S2c1(n0)
2Nc1
= Œ¥
h3
.
by 
nly.

80
Crafts of Simulation Programming
Consequently,
GR(h3) = Pr

( ÀúXck ‚àíÀúXc1) ‚àí(Œºck ‚àíŒºc1)

S2ck(n0)/(2Nck) + S2c1(n0)/(2Nc1) ‚â§h3

= P ‚àó.
Let
wckc1 = h3
‚àö
2

S2ck(n0)
Nck
+ S2c1(n0)
Nc1
.
Then
Pr[ ÀúXck ‚àíÀúXc1 ‚àíwckc1 ‚â§Œºck ‚àíŒºc1] = P ‚àó.
By
deÔ¨Ånition,
wckc1
is
the
one-tailed
P ‚àó
conÔ¨Ådence
interval
half
width.Under the null hypothesis that all means are equal, Œºck ‚àíŒºc1 = 0.
Consequently,
Pr[ ÀúXck ‚àíÀúXc1 ‚â§wckc1 ‚â§Œ¥] = P ‚àó.
To obtain wckc1
‚â§
Œ¥, sample sizes Ni
‚â•
(h3Si(n0)/Œ¥)2, provided
(h3Si(n0)/Œ¥)2 > n0.
The expected value of ÀúXcj ‚àíÀúXci (the diÔ¨Äerence of the weighted sample
means of systems cj and ci) decreases as j ‚àíi > 0 decreases. Even though
the variance of ÀúXcj ‚àíÀúXci increases as j ‚àíi > 0 decreases, the P ‚àóquantile
of the random variate ÀúXcj ‚àíÀúXci is smaller than h3 for i, j = 1, 2, . . . , k ‚àí1,
i Ã∏= j. Consequently, let
wij = h3
‚àö
2

S2
i (n0)
Ni
+
S2
j (n0)
Nj
,
then
Pr[ ÀúXi ‚àíÀúXj ‚àíwij ‚â§Œºi ‚àíŒºj] ‚â•P ‚àófor i, j = 1, 2, ¬∑ ¬∑ ¬∑ , k, i Ã∏= j
and
Pr[ ÀúXi ‚àíÀúXj ‚àíwij ‚â§Œºi ‚àíŒºj ‚â§ÀúXi ‚àíÀúXj + wij] ‚â•2P ‚àó‚àí1
for i, j = 1, 2, ¬∑ ¬∑ ¬∑ , k, i Ã∏= j.
4.3
Statistical Analysis of the Range
In this section, we investigate how to simulate sample range and estimate
the values of the cdf of the range.
by 
nly.

Distributions of Order Statistics
81
Fig. 4.2
Empirical probability density of a range
4.3.1
Simulating the Sample Range
Recall that a straightforward way of simulating sample range is to gener-
ate n pseudorandom samples from the distribution F(X) and record the
sample minimum and maximum. The sample range can then be calculated
with the sample minimum and maximum. This general method requires
generating n samples. On the other hand, the sample minimum and maxi-
mum can also be generated directly with the properties of order statistics.
Let U1 = Œ≤(u1, 1, n) and U2 = Œ≤(u2, n, n), where u1, u2 ‚àºU(0, 1) and are
independent.
Then F ‚àí1(U1) and F ‚àí1(U1) are, respectively, the sample
minimum and maximum. However, the distribution of the random variate
F ‚àí1(U2)‚àíF ‚àí1(U1) will have greater variance than the true distribution of
the sample range because the minimum and maximum of samples are not
independent.
Figure 4.2 lists the empirical distributions of the range of 10 t distributed
(with 19 d.f.) random variables. The correlated and independent distribu-
tions are, respectively, the distribution of t[10] ‚àít[1] when t[1] and t[10] are
generated correlated and independently. The correlated one has a smaller
variance, the value of the mode is smaller and the pdf value of the mode is
greater, hence, the higher quaniles (say p > 0.5) will be smaller.
by 
nly.

82
Crafts of Simulation Programming
From the discussion earlier, the nth order statistic x[n] given the Ô¨Årst
order statistic x[1] = xa = Œ≤(u1, 1, n) is the same as the (n ‚àí1)th order
statistics of sample size n ‚àí1 with the same distribution F truncated at
xa. Hence, we can use the conditional probability of x[n] given x[1] = xa to
estimate the range.
The conditional distribution
gX[n]|X[1]=xa(x) = (n ‚àí1)(F(x) ‚àíF(xa)
1 ‚àíF(xa)
)n‚àí2
f(x)
1 ‚àíF(xa), x > xa.
In the case that F is the cdf of uniform [0,1] distribution
gX[n]|X[1]=xa(x) = (n ‚àí1)(x ‚àíxa
1 ‚àíxa
)n‚àí2
1
1 ‚àíxa
, x > xa.
Moreover,
GX[n]|X[1]=xa(x) = (x ‚àíxa
1 ‚àíxa
)n‚àí1, x > xa.
The random variate X[n] can then be generated using the inverse trans-
formation, i.e., we set
(X[n] ‚àíxa
1 ‚àíxa
)n‚àí1 = U ‚àºU(0, 1),
to obtain
X[n] = xa + (1 ‚àíxa) ‚àóU 1/(n‚àí1).
The sample range R = X[n] ‚àíX[1]. In cases that F is not uniform [0,1], the
sample range R = F ‚àí1(X[n]) ‚àíF ‚àí1(X[1]). Recall that if Xi = F ‚àí1(Ui),
then X[i] = F ‚àí1(U[i]).
4.3.2
Estimating Quantiles of the Range
The equivalence tests are derived from the equation
P(CD) = Pr[Tck ‚àíTc1 ‚â§h3].
We equal the right-hand side to P ‚àóto obtain h3. Note that by deÔ¨Ånition
h3 is the P ‚àóquantile of the variate Tck ‚àíTc1. In this section, we present
an approach to estimate the quantiles of a range.
[Chen and Kelton (2008)] control the precision of quantile estimates by
ensuring that the p quantile estimator ÀÜxp satisÔ¨Åes the following:
Pr[xp ‚ààÀÜxp¬±œµ] ‚â•1‚àíŒ±1, or equivalently Pr[|F(ÀÜxp)‚àíp| ‚â§œµ] ‚â•1‚àíŒ±1. (4.5)
Figure 4.3 demonstrates this requirement with an exponential distribution,
see [EickoÔ¨Äet al. (2006)] Using this precision requirement (i.e., Eq. (4.5)),
by 
nly.

Distributions of Order Statistics
83
Fig. 4.3
ConÔ¨Ådence intervals for quantiles
the required sample size np for a Ô¨Åxed-sample-size procedure of estimating
the p quantile of an i.i.d. sequence is the minimum np that satisÔ¨Åes
np ‚â•
z2
1‚àíŒ±1/2p(1 ‚àíp)
œµ2
,
(4.6)
where z1‚àíŒ±1/2 is the 1 ‚àíŒ±1/2 quantile of the standard normal distribution,
œµ is the maximum proportional half-width of the conÔ¨Ådence interval, and
1 ‚àíŒ±1 is the conÔ¨Ådence level. For example, if the data are independent
and one wants to have 95% conÔ¨Ådence that the coverage of the 0.9 quantile
estimator has no more than œµ = 0.0001 (coverage) deviation from the true
but unknown quantile, the required sample size is np ‚â•34574400 (i.e.,
1.96020.9(1 ‚àí0.9)/0.00012). Consequently, we will have 97.5% conÔ¨Ådence
that the quantile estimate will cover at least p ‚àí0.0001 (for p ‚â•0.9), with
sample size 34574400.
To avoid storing and sorting the entire sequence, a modiÔ¨Åed version of
the histogram-approximation procedure of [Chen and Kelton (2008)] was
used to estimate quantiles.
See [Chen and Li
(2010)] for more details.
Table 4.1 lists the critical constant h3.
by 
nly.

84
Crafts of Simulation Programming
Table 4.1
Values of critical constant h3 (Quantile of sample range)
P ‚àó
n0/k
3
4
5
6
7
8
9
10
0.90
10
3.310
3.739
4.053
4.301
4.507
4.683
4.837
4.974
15
3.150
3.542
3.825
4.046
4.228
4.382
4.516
4.634
20
3.081
3.457
3.727
3.937
4.108
4.253
4.379
4.489
25
3.042
3.409
3.672
3.876
4.041
4.182
4.303
4.409
30
3.017
3.379
3.637
3.837
3.999
4.136
4.254
4.358
35
2.999
3.358
3.613
3.810
3.970
4.105
4.221
4.323
40
2.987
3.342
3.595
3.790
3.948
4.082
4.196
4.297
0.95
10
3.874
4.304
4.618
4.869
5.076
4.693
5.411
5.549
15
3.650
4.033
4.311
4.528
4.707
4.859
4.991
5.108
20
3.554
3.919
4.180
4.385
4.551
4.744
4.816
4.924
25
3.501
3.855
4.108
4.305
4.466
4.601
4.719
4.822
30
3.467
3.815
4.062
4.255
4.411
4.544
4.658
4.758
35
3.444
3.787
4.031
4.221
4.374
4.504
4.616
4.714
40
3.427
3.766
4.008
4.195
4.347
4.475
4.585
4.682
Table 4.2
Mean conÔ¨Åguration
Model
Œºi
EMC
Œºi = 0, for i = 1, . . . , k
LFC
Œº1 = 0 and Œºi = d‚àó, for i = 2, 3, . . . , k
4.4
Empirical Experiments
In this section, we present some empirical results. Instead of using stochas-
tic system simulation examples, which oÔ¨Äer less control over the factors that
aÔ¨Äect the performance of a procedure, we use various normally distributed
random variables to represent the systems. We chose the Ô¨Årst-stage sample
size to be n0 = 20. The number of systems under consideration is k = 10.
The critical range Œ¥ is set to 0.5/‚àön0 and d‚àó= 2Œ¥. The required minimal
P(CS), P ‚àó, is set to 0.90.
The equal means conÔ¨Åguration (EMC) and the least favorable conÔ¨Ågu-
ration (LFC) are used. Under the LFC, Œº1 + d‚àó= Œºi, for i Ã∏= 1. Tables
4.2 and 4.3 list the conÔ¨Ågurations of mean and variance, respectively. With
the EMC, a correct decision means we do not reject the null hypothesis
H0 : Œº1 = Œº2 = ¬∑ ¬∑ ¬∑ = Œºk. With the LFC, a correct decision means we
reject the null hypothesis.
We performed 10,000 independent runs to estimate the observed P(CD),
which is computed by dividing the number of times the procedure made a
CS by 10,000. We list the results (the observed P(CD), the average sample
size of each simulation run T, i.e., T = 10000
r=1
k
i=1 Nr,i/10000, Nr,i is the
total number of replications or batches for system i in the rth independent
by 
nly.

Distributions of Order Statistics
85
Table 4.3
Variance conÔ¨Ågura-
tion
Model
Œºi
Equal
1
Increasing
1 + (i ‚àí1)d‚àó
Decreasing
1
1+(i‚àí1)d‚àó
Table 4.4
The observed P(CD) with P ‚àó= 0.90, n0 = 20 and equal variances among
systems
Setting/k
3
4
5
6
7
8
9
10
EMC
0.9270
0.9329
0.9333
0.9427
0.9453
0.9466
0.9523
0.9545
LFC
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
T
2274
3820
5550
7435
9442
11570
13799
16113
Table 4.5
The observed P(CD) with P ‚àó= 0.90, n0 = 20 and increasing variances
among systems
Setting/k
3
4
5
6
7
8
9
10
EMC
0.9237
0.9305
0.9342
0.9429
0.9427
0.9480
0.9506
0.9516
LFC
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
T
2784
5103
8037
11596
15785
20632
26150
32337
Table 4.6
The observed P(CD) with P ‚àó= 0.90, n0 = 20 and decreasing variances
among systems
Setting/k
3
4
5
6
7
8
9
10
EMC
0.9273
0.9321
0.9353
0.9410
0.9414
0.9458
0.9493
0.9475
LFC
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
T
1901
2965
4032
5084
6112
7116
8093
9039
run) in Tables 4.4 through 4.6. All the observed P(CD)s are greater than
the speciÔ¨Åed nominal level of 0.90. We believe it is because the critical
constant h3 is a round up of the true value.
Hence, as the number of
systems k increases the P(CD)s becomes larger. The procedure correctly
increases (decreases) the sample sizes as the variances increase (decrease).
Consequently, the observed P(CD)s do not degrade as the variances increase
(decrease).
4.5
Summary
Order statistics have proved useful for both theoretical and computational
purposes.
Important special cases of order statistics are the minimum
by 
nly.

86
Crafts of Simulation Programming
and maximum value of samples, and the sample median and other sam-
ple quantiles. We have investigated several properties of order statistics
and discussed how order statistics are applied in simulation, e.g., generate
the distributions of order statistics without sorting and presented simula-
tion procedures that are derived based on the inference of order statistics,
e.g., hypothesis tests of equivalence of means. Properties of order statistics
allow us to derive many simulation algorithms in a concise manner. These
procedures are easy to understand and simple to implement.
by 
nly.

Chapter 5
Order Statistics from Correlated
Normal Random Variables
In this chapter, we investigate an important special case of order statistics,
namely, the underlying variates are correlated normal random variables and
show how it is related to ranking and selection. [Gupta et al. (1973)] point
out that the study of these order statistics is important because that the
joint distributions of standardized correlated variables often tend asymptot-
ically to the multivariate normal distribution. [Arellano and Genton (2007,
2008)] have derived the density of correlated normal random variables from
a multivariate normal distribution. [Liu et al. (2012)] develop a method to
construct exact simultaneous conÔ¨Ådence interval for a Ô¨Ånite set of contrasts
of three, four or Ô¨Åve generally correlated normal means.
We are interested in the case that X1, X2, ¬∑ ¬∑ ¬∑ , Xn are n standardized
normal random variables with correlation matrix {œÅij}. In the special case
that œÅij = œÅ ‚â•0 (i Ã∏= j), it can be shown that
Fn(h; œÅ) = Pr[X[n] ‚â§h] =
 ‚àû
‚àí‚àû
Œ¶n{(xœÅ1/2 + h)/(1 ‚àíœÅ)1/2}œÜ(x)dx, (5.1)
where Œ¶(x) and œÜ(x) are, respectively, the cumulative distribution func-
tion and the probability density function of a standardized normal random
variable, see [Dunnett and Sobel (1955)]. By the symmetry of the normal
distribution Fn(h; œÅ) is also equal to Pr[X[1] ‚â•‚àíh].
[Gupta et al.
(1973)] point out that in several cases of applications,
the percentage points (quantile) of the distribution of X[n] are needed and
provide tables of percentage points for several selected values of n and
positive œÅ.
87

88
Crafts of Simulation Programming
Fig. 5.1
Empirical density of the Ô¨Årst-order statistic of 9 N(0, 1) random variables
5.1
Order Statistics of Correlated Random Variables
[Chen
(2014b)] investigated the distribution of the order statistic X[u]
when X are correlated. Figure 5.1 shows the empirical density functions
of three Ô¨Årst-order statistic of 9 standard normal random variables: 1)
samples are independent; 2) samples with covariance of 0.5; 3) samples with
covariance of 1. See [Arellano and Genton (2007, 2008)] for more graphical
illustrations of distributions of order statistics of correlated normal random
variables. The standard normal can be reviewed as the Ô¨Årst-order statistic
of 9 perfectly correlated standard normal random variables. [Gupta et al.
(1973)] show that the cdf of X[u] when X are correlated normal random
variables with œÅij = œÅ (i Ã∏= j) is:
Fu(ha; œÅ) = Pr[X[u] ‚â§ha] =
 ‚àû
‚àí‚àû
Iq(z)(u, n ‚àíu + 1)dœÜ(z),
where q(z) = Œ¶{(zœÅ1/2 + ha)/(1 ‚àíœÅ)1/2}, and Iq(a, b)(a, b > 0) is the usual
incomplete beta function. When the correlation matrix has the structure
œÅij = cicj(i Ã∏= j), where ‚àí1 ‚â§ci ‚â§1 for i = 1, 2, . . . , n, then Fu(ha; œÅ) with
u = n is reduced to Eq. (5.1). Furthermore, Xi can be represented as
Xi = ciZ0 + (1 ‚àíc2
i )1/2Zi, for i = 1, 2, . . . , n,
(5.2)
where Z0, Z1, . . . , Zn are independent standard normal random variables.
If œÅij = œÅ ‚â•0, then the structural assumption that œÅij = cicj is satisÔ¨Åed

Order Statistics from Correlated Normal Random Variables
89
Table 5.1
Values of critical constant ha with œÅ = 0.5
(known equal variances)
n/Œ±
0.010
0.025
0.050
0.100
0.250
2
2.5581
2.2124
1.9165
1.5770
1.0139
3
2.6852
2.3492
2.0622
1.7336
1.1895
4
2.7718
2.4420
2.1605
1.8385
1.3056
5
2.8372
2.5117
2.2341
1.9164
1.3912
6
2.8894
2.5671
2.2924
1.9781
1.4586
7
2.9327
2.6132
2.3407
2.0289
1.5139
8
2.9701
2.6523
2.3816
2.0721
1.5605
9
3.0022
2.6865
2.4173
2.1095
1.6008
10
3.0307
2.7165
2.4486
2.1424
1.6362
12
3.0795
2.7677
2.5019
2.1981
1.6959
14
3.1198
2.8101
2.5461
2.2441
1.7452
16
3.1545
2.8464
2.5837
2.2833
1.7868
18
3.1846
2.8778
2.6163
2.3172
1.8230
20
3.2115
2.9058
2.6452
2.3473
1.8547
22
3.2352
2.9306
2.6709
2.3740
1.8830
24
3.2571
2.9532
2.6943
2.3981
1.9085
26
3.2767
2.9737
2.7155
2.4202
1.9318
28
3.2949
2.9926
2.7349
2.4403
1.9530
30
3.3117
3.0100
2.7529
2.4589
1.9726
32
3.3270
3.0262
2.7697
2.4763
1.9908
34
3.3417
3.0414
2.7853
2.4923
2.0077
36
3.3554
3.0555
2.7998
2.5074
2.0235
38
3.3684
3.0690
2.8137
2.5216
2.0384
40
3.3803
3.0815
2.8266
2.5350
2.0524
42
3.3921
3.0935
2.8389
2.5476
2.0657
44
3.4029
3.1046
2.8504
2.5595
2.0782
46
3.4133
3.1155
2.8615
2.5710
2.0901
48
3.4232
3.1258
2.8721
2.5818
2.1015
50
3.4328
3.1357
2.8823
2.5923
2.1123
by taking ci = ‚àöœÅ for all i. An observation of X[n] from correlated normal
random variables with correlation œÅ can be simulated as follows. Let y =
Œ≤(U; n, 1) and set X[n] = ‚àöœÅZ0 + ‚àö1 ‚àíœÅF ‚àí1(y).
5.1.1
Method of Evaluation of the Percentage Points
Let Œ± be the probability of committing the Type I error, i.e., do not accept
the null hypothesis when it is correct. The value of ha is the 1‚àíŒ± quantile of
the correlated normal random variable X[n]. We used the method described
in Chapter 6 to estimate the percentage points (i.e., quantiles). This ap-
proach is Ô¨Çexible and can be used to estimate the critical constant for the
problem at hand, even when the correlations are unknown or unequal.
Table 5.1 lists the critical constant ha with œÅ = 0.5 for selected Œ± and

90
Crafts of Simulation Programming
Table 5.2
Values of critical constant hb with œÅ = 0.5
(unknown equal variances with the number of obser-
vations n0
=
20)
n/Œ±
0.010
0.025
0.050
0.100
0.250
2
2.8416
2.4018
2.0458
1.6562
1.0436
3
3.0143
2.5760
2.2222
1.8364
1.2326
4
3.1353
2.6973
2.3444
1.9600
1.3601
5
3.2290
2.7903
2.4373
2.0534
1.4553
6
3.3052
2.8657
2.5125
2.1284
1.5312
7
3.3691
2.9289
2.5753
2.1910
1.5940
8
3.4239
2.9832
2.6292
2.2447
1.6476
9
3.4727
3.0312
2.6766
2.2916
1.6941
10
3.5161
3.0737
2.7186
2.3332
1.7355
sample sizes n. The values are within ¬±0.0004 of those listed in [Gupta
et al. (1973)], where a wide range of critical constant is listed. When the
variance is unknown, it needs to be estimated by sample variance. Each
sample mean will be the average of n0 observations, the standard error
of these n0 observations are used as an estimator of standard deviation.
Table 5.2 in Section 5.2.3 lists the critical constant hb with œÅ = 0.5 and the
number of observations n0 = 20 for selected sample sizes n.
5.2
Applications of Correlated Order Statistics
[Gupta et al.
(1973)] discuss several applications illustrating the use of
the tables. These applications relate to multiple decision procedures, mul-
tiple comparison problems and some tests of hypotheses. In this section,
we review multiple comparison and multiple decision procedures. Multiple
comparisons provide simultaneous conÔ¨Ådence intervals on selected diÔ¨Äer-
ences among the systems. Multiple-decision procedure is used to select the
system with the smallest or the largest mean.
5.2.1
Multiple Comparisons with a Control
In multiple comparisons with a control, we are interested in the lower conÔ¨Å-
dence limits for Œºi ‚àíŒº0 (i = 1, 2, . . . , k) with joint conÔ¨Ådence P ‚àóin the case
of k+1 normal populations œÄi (i = 0, 1, . . . , k) with mean Œºi (i = 0, 1, . . . , k)
and common known variance œÉ2.
Let Xij denote the jth observation
of the ith system.
Let
¬ØX0 = m0
j=1 X0j/m0 and
¬ØXi = n0
j=1 Xij/n0
(i = 1, 2, . . . , k) be the sample means based on a sample size m0 from
the control œÄ0 and samples of size n0 from each of the k populations œÄi

Order Statistics from Correlated Normal Random Variables
91
(i = 1, 2, . . . , k). Then ¬ØXi ‚àí¬ØX0 ‚àºN(Œºi ‚àíŒº0, œÉ2(1/m0 + 1/n0)), where
N(Œº, œÉ2) denotes the normal distribution with mean Œº and variance œÉ2.
Let h denote the P conÔ¨Ådence critical constant. By the deÔ¨Ånition of the
conÔ¨Ådence limit
Pr[Œºi ‚àíŒº0 ‚â•¬ØXi ‚àí¬ØX0 ‚àíhœÉ

1/m0 + 1/n0] = P for i = 1, 2, . . . , k.
We are interested in constant ha such that
Pr[Œºi ‚àíŒº0 ‚â•¬ØXi ‚àí¬ØX0 ‚àíhaœÉ

1/m0 + 1/n0 for i = 1, 2, . . . , k] = P ‚àó.
Let
Zi = ( ¬ØXi ‚àí¬ØX0) ‚àí(Œºi ‚àíŒº0)
œÉ

1/m0 + 1/n0
for i = 1, 2, . . . , k.
Then
Pr[Zi ‚â§ha for i = 1, 2, . . . , k] = P ‚àó.
Furthermore, let Z[k] = maxk
i=1 Zi for i = 1, 2, . . . , k. Then
Pr[Z[k] ‚â§ha] = P ‚àó.
Note that Zi are standardized normal variables with equal correlation
œÅ = n0/(m0+n0) and Z[k] is the kth order statistic of Zi. Consequently, the
constant ha = Ha(P ‚àó, k, n0/(m0+n0)), where Ha(1‚àíŒ±, k, œÅ) is the percent-
age point with Type I error probability Œ±, sample size k, and correlation
œÅ.
Note that Z = (Z1, . . . , Zk)T has an exchangeable multivariate normal
distribution (i.e., its covariance matrix is equicorrelated) with a common
mean Œº = 0, a common variance œÉ2 and a common correlation coeÔ¨Écient
œÅ. That is, Z is multivariate normal Nk((Œº‚àíŒº0)1k, œÉ2{(1‚àíœÅ)Ik +œÅ1k1T
k })
with œÅ ‚àà[0, 1), 1k ‚ààRk a vector of ones, and Ik ‚ààRk√ók the identity
matrix. It can be shown that the probability density function fz[k] of z[k] is
fz[k](z) = kœÜ1(z; Œº, œÉ2)Œ¶k‚àí1(

1 ‚àíœÅ(z ‚àíŒº)/œÉ1k‚àí1; 0, Ik‚àí1 + œÅ1k‚àí11T
k‚àí1),
where z ‚ààR, œÜ1(z; Œº, œÉ2) is the marginal probability density function of Zk
and Œ¶k‚àí1(z; Œº, Œ£) is the k‚àí1 multivariate cumulative distribution function.
See [Arellano and Genton (2008)].

92
Crafts of Simulation Programming
5.2.2
Multiple Decision (Ranking and Selection)
In multiple decision, we are interested in selecting the system having the
largest mean, with known equal variance œÉ2 under the indiÔ¨Äerence zone
setting.
Without loss of generality, assume that Œºi = Œºk ‚àíd‚àófor i =
1, 2, ¬∑ ¬∑ ¬∑ , k ‚àí1. Here d‚àóis an indiÔ¨Äerence amount speciÔ¨Åed by the users.
The probability of correct selection, P(CS), is then
P(CS) = Pr[ ¬ØXi < ¬ØXk for i = 1, 2, ¬∑ ¬∑ ¬∑ , k ‚àí1]
= Pr[
¬ØXi ‚àíŒºi
œÉ/‚àön0
<
¬ØXk ‚àíŒºk
œÉ/‚àön0
+
d‚àó
œÉ/‚àön0
for i = 1, 2, ¬∑ ¬∑ ¬∑ , k ‚àí1]
= Pr[Z[k‚àí1] < Zk +
d‚àó
œÉ/‚àön0
]
=
 ‚àû
‚àí‚àû
Œ¶k‚àí1(z +
d‚àó
œÉ/‚àön0
)dŒ¶(z).
We set the right-hand side to P ‚àóto solve for n0. Note that
(Z1 ‚àíZk, . . . , Zk‚àí1 ‚àíZk) ‚àºNk‚àí1(‚àíd‚àó1k‚àí1, œÉ2{0.5Ik‚àí1 + 0.51k‚àí11T
k‚àí1}).
Because Pr[(Z[k‚àí1] ‚àíZk)/
‚àö
2 < ha] ‚â•P ‚àó, (here ha = Ha(P ‚àó, k ‚àí1, 0.5)),
the sample size n0 should large enough such that
‚àö
2ha ‚â§d‚àó‚àön0/œÉ, i.e.,
n0 ‚â•2(haœÉ/d‚àó)2. See [Bechhofer (1954)] for details.
5.2.3
Multiple Comparisons with a Control:
Unknown
Equal Variances
Let Œº0 denote the performance measure of the control (i.e., a benchmark
system). We are interested in the lower conÔ¨Ådence limits for Œºi ‚àíŒº0 (i =
1, 2, . . . , k) with joint conÔ¨Ådence P ‚àóin the case of k +1 normal populations
œÄi (i = 0, 1, . . . , k) with mean Œºi (i = 0, 1, . . . , k) and common unknown
variance œÉ2. Let ¬ØXi (i = 0, 1, . . . , k) be the sample means based on samples
of size n0 from each of the k + 1 populations œÄi (i = 0, 1, . . . , k). Then
¬ØXi ‚àí¬ØX0 ‚àºN(Œºi ‚àíŒº0, œÉ22/n0). Let hb denote the critical constant and
S2 be the unbiased variance estimate of the unknown variance œÉ2. We are
interested in constant hb such that
Pr[Œºi ‚àíŒº0 ‚â•¬ØXi ‚àí¬ØX0 ‚àíhbS

2/n0 for i = 1, 2, . . . , k] = P ‚àó.
Let
Ti = ( ¬ØXi ‚àí¬ØX0) ‚àí(Œºi ‚àíŒº0)
S

2/n0
for i = 1, 2, . . . , k.

Order Statistics from Correlated Normal Random Variables
93
Then
Pr[Ti ‚â§hb for i = 1, 2, . . . , k] = P ‚àó.
Furthermore, let T[k] = maxk
i=1 Ti. Then
Pr[T[k] ‚â§hb] = P ‚àó.
Note that Ti for i = 1, 2, . . . , k are t-distributed (with n0 ‚àí1 d.f.) with
equal correlation œÅ = 1/2 and T[k] is the kth order statistic of Ti. That is,
T = (T1, . . . , Tk)T is an exchangeable random vector with a multivariate
Student t distribution with v = n0 ‚àí1 d.f. and
T ‚àºStudentk(Œº1k, œÉ2{(1 ‚àíœÅ)Ik + œÅ1k1T
k }, v).
It can be shown that the pdf fT[k] of T[k] is
fT[k](t) = kt1(t; Œº, œÉ2, v)√ó
Tk‚àí1(

1 ‚àíœÅ(t ‚àíŒº)/œÉ1k‚àí1; 0, v + z2
v + 1 {Ik‚àí1 + œÅ1k‚àí11T
k‚àí1}, v + 1), t ‚ààR,
where t1(t; Œº, œÉ2) is the marginal probability density function of Tk, z2 =
(t ‚àíŒº)2/œÉ2 and Tk‚àí1(z; Œº, Œ£, v + 1) is the k ‚àí1 multivariate t cumulative
distribution function. See [Arellano and Genton (2008)]. Consequently, the
constant hb = Hb(P ‚àó, k, 0.5, n0), where Hb(1 ‚àíŒ±, k, œÅ, n0) is the percentage
point with Type I error probability Œ±, sample size k, correlation œÅ, and the
number of observations n0.
We compute the sample variances with the equation
S‚Ä≤2
i =
n
j=1((Xij ‚àíX0j) ‚àí( ¬ØXi ‚àí¬ØX0))2
n0 ‚àí1
,
for i = 1, 2, . . . , k. The quantity S‚Ä≤
i/‚àön0 will be used in the place of S

2/n0
for each Ti for i = 1, 2, . . . , k. Note that S‚Ä≤
i already incorporates the stan-
dard error of X0, hence, the multiplier
‚àö
2 is no longer needed.
5.2.4
Multiple Comparisons with a Control: Unknown Un-
equal Variances
Let ÀúXi be the weighted sample means (with the Ô¨Årst-stage sample size n0)
as deÔ¨Åned in [Dudewicz and Dalal (1975)]. Then
œëi = Ti ‚àíT0 = ( ÀúXi ‚àíÀúX0) ‚àí(Œºi ‚àíŒº0)
d‚àó/hc
=
( ÀúXi ‚àíÀúX0) ‚àí(Œºi ‚àíŒº0)

S2
i (n0)/(2Ni) + S2
0(n0)/(2N0)
.

94
Crafts of Simulation Programming
Here S2
i (n0) is the unbiased estimator of the variance œÉ2
i with n0 samples.
The correlations of ÀúXi ‚àíÀúX0 and ÀúXj ‚àíÀúX0 for i Ã∏= j are unknown, but the
correlations of œëi (i.e., Ti ‚àíT0) and œëj (i.e., Tj ‚àíT0) for i Ã∏= j are œÅij = 1/2.
Without loss of generality, we temporarily assume that Ni and N0 are real
numbers. The last equality holds because
S2
i (n0)
2Ni
= S2
0(n0)
2N0
= (d‚àó)2
2h2c
and

S2
i (n0)
2Ni
+ S2
0(n0)
2N0
= d‚àó
hc
.
The distribution of œëi is symmetric and its percentile (or quantile) can be
evaluated numerically. Let
wi0 = hc
‚àö
2

S2
i (n0)
Ni
+ S2
0(n0)
N0
.
Then wi0 = d‚àó. In practice, Ni for i = 0, 1, . . . , k are integers and wi0 ‚â§d‚àó.
For some discussion of the properties of ¬ØX and ÀúX, see [Chen (2011)].
We are interested in the value hc such that Pr[ ÀúXi ‚àíÀúX0 ‚àíwi0 ‚â§Œºi ‚àí
Œº0 for i = 1, 2, . . . , k] = P ‚àó. That is, Pr[œëi ‚â§hc for i = 1, 2, . . . , k] = P ‚àó.
Similarly, Pr[T[k] ‚àíT0 ‚â§hc] =
 ‚àû
‚àí‚àûGk:k(t + hc)f(t)dt = P ‚àó. Note that
Ti for i = 0, 1, . . . , k are independent and the parent distribution of Gk:k
is the t-distribution with n0 ‚àí1 d.f. With the indiÔ¨Äerence-zone approach,
the allocated sample sizes obtain wi0 ‚â§d‚àó. To increase the eÔ¨Éciency, the
strategy of [Chen (2004)] is to allocate sample sizes such that wi0 ‚â§Œºi‚àíŒº0.
Because Œºi for i = 0, 1, 2, . . . , k are unknown, in practice, the procedure
obtains wi0 ‚â§¬ØXi ‚àí¬ØX0.
5.3
Empirical Experiments
In this section, we present some empirical results of comparison with a
control. The number of systems under consideration is k = 5.
5.3.1
Experiment 1: Known Equal Variances
In this experiment, the unknown true means for all systems are Œºi = 0
and the known variances of all systems are œÉ2
i = 1.0 for i = 0, 1, 2, . . . , k.
The sample size for each system is n0 = 20.
Consequently, the corre-
lation of
¬ØXi ‚àí¬ØX0 for i = 1, 2, . . . , k is œÅ = 0.5.
The nominal con-
Ô¨Ådence level is P ‚àó= 0.90, 0.95, and 0.99, hence, the critical constant
ha(P ‚àó, 5, 0.5) = 1.9164, 2.2341, and 2.8372. We list ÀÜP(C), the percentage

Order Statistics from Correlated Normal Random Variables
95
Table 5.3
Percentage of the cover-
age with k = 5 and known variances
P ‚àó
0.90
0.95
0.99
ÀÜP(C)
0.8982
0.9505
0.9889
Table 5.4
Percentage of the coverage
with k = 5 and unknown variances
P ‚àó
0.90
0.95
0.99
ÀÜP(C)
0.8999
0.9508
0.9891
that the simultaneous lower conÔ¨Ådence limits contain the true diÔ¨Äerences
Œºi‚àíŒº0 (i.e., 0 ‚â•¬ØXi‚àí¬ØX0‚àíha
‚àö
2/
‚àö
20) for i = 1, 2, . . . , k. Table 5.3 lists the
results of experiment 1. The observed coverages are close to the nominal
values.
5.3.2
Experiment 2: Unknown Equal Variances
In this experiment, the unknown means for all systems are Œºi = 0 and the
unknown variances of all systems are œÉ2
i = 1.0 for i = 0, 1, 2, . . . , k. Note
that the variances of ¬ØXi ‚àí¬ØX0 need to be estimated. The sample size for
each system is n0 = 20. The nominal conÔ¨Ådence level is P ‚àó= 0.90, 0.95,
and 0.99, hence, the critical constant hb(P ‚àó, 5, 0.5, 20) = 2.0534, 2.4373,
and 3.2290.
We list ÀÜP(C), the percentage that the simultaneous lower
conÔ¨Ådence limits contain the true diÔ¨Äerences Œºi ‚àíŒº0 (i.e., 0 ‚â•¬ØXi ‚àí¬ØX0 ‚àí
hbS‚Ä≤
i/
‚àö
20) for i = 1, 2, . . . , k. Table 5.4 lists the results of experiment 2.
The observed coverages are close to the nominal values. The coverages in
experiments 1 and 2 are basically the same. However, the half width of
the c.i‚Äôs in experiment 2 are wider than those in experiment 1 because the
variance needs to be estimated using sample variance.
5.3.3
Experiment 3: Unknown Unequal Variances
In this experiment, the unknown means for all systems are Œºi = 0 and the
unknown variances of system i œÉ2
i for i = 0, 1, 2, . . . , k may be diÔ¨Äerent.
Table 5.5 lists the conÔ¨Åguration of variances. The required parameter d‚àó
is set to 0.5 or 1.0, which implies that the targeted one-tailed conÔ¨Ådence
interval half-width is 0.5 or 1.0, respectively.
The sample size for each
system is computed according to
Ni = max(n0 + 1, ‚åà(hcSi(n0)/d‚àó)2‚åâ), for i = 0, 1, . . . , k.
(5.3)

96
Crafts of Simulation Programming
Table 5.5
Variance conÔ¨Åguration
Model
œÉ2
0, œÉ2
1, œÉ2
2, œÉ2
3, œÉ2
4, œÉ2
5
Equal
1, 1, 1, 1, 1, 1
Increasing
1, 1 + d‚àó, 1 + 2d‚àó, 1 + 3d‚àó, 1 + 4d‚àó, 1 + 5d‚àó
Decreasing
1,
1
1+d‚àó,
1
1+2d‚àó,
1
1+3d‚àó,
1
1+4d‚àó,
1
1+5d‚àó
Table 5.6
Percentage of the coverage with equal means and d‚àó= 0.5
ConÔ¨Åguration
P ‚àó
half width
0.90
0.95
0.99
Weighted ÀÜP(C)
d‚àó
0.8936
0.9474
0.9905
Overall ÀÜP(C)
d‚àó
0.9011
0.9499
0.9904
Equal
Weighted ÀÜP(C)
wi0
0.8842
0.9442
0.9903
Overall ÀÜP(C)
wi0
0.8932
0.9470
0.9903
¬ØT
200
276
462
Weighted ÀÜP(C)
d‚àó
0.9027
0.9475
0.9902
Overall ÀÜP(C)
d‚àó
0.9072
0.9497
0.9904
Increasing
Weighted ÀÜP(C)
wi0
0.8985
0.9457
0.9899
Overall ÀÜP(C)
wi0
0.9031
0.9481
0.9903
¬ØT
446
618
1041
Weighted ÀÜP(C)
d‚àó
0.8952
0.9504
0.9900
Overall ÀÜP(C)
d‚àó
0.9471
0.9690
0.9917
Decreasing
Weighted ÀÜP(C)
wi0
0.8422
0.9247
0.9865
Overall ÀÜP(C)
wi0
0.9151
0.9558
0.9903
¬ØT
142
167
250
The nominal conÔ¨Ådence level is P ‚àó= 0.90, 0.95, and 0.99, hence, the
critical constant hc(P ‚àó, 5, 20) = 2.8703, 3.3767, and 4.3872.
Note that
hc(P ‚àó, k, n0) = h1(P ‚àó, k + 1, n0). We list the percentage that the simul-
taneous lower conÔ¨Ådence limits (constructed four diÔ¨Äerent ways) contain
the true diÔ¨Äerences Œºi ‚àíŒº0 (i.e., 0 ‚â•ÀúXi ‚àíÀúX0 ‚àíd‚àó, 0 ‚â•¬ØXi ‚àí¬ØX0 ‚àíd‚àó,
0 ‚â•ÀúXi ‚àíÀúX0 ‚àíwi0, 0 ‚â•¬ØXi ‚àí¬ØX0 ‚àíwi0) for i = 1, 2, . . . , k.
Tables 5.6 and 5.7 list the results of experiment 3 with d‚àó= 0.5 and 1.0,
respectively. In addition to the observed coverages, we also list the average
sample size of each simulation run T, i.e., T = 10000
r=1
k
i=0 Nr,i/10000,
Nr,i is the total number of replications or batches for system i in the rth
independent run. The procedure correctly increases and decreases the al-
located sample sizes as the variance increases and decreases, respectively.
The observed coverages of the c.i. constructed by 0 ‚â•ÀúXi ‚àíÀúX0 ‚àíd‚àóand
0 ‚â•¬ØXi ‚àí¬ØX0 ‚àíwi0 are close to the nominal values. Because the overall
sample mean ¬ØXi has smaller variance than the weighted sample mean ÀúXi,
the coverages of the conÔ¨Ådence intervals built by ¬ØXi is slightly greater than
those built by ÀúXi, with the same half width.

Order Statistics from Correlated Normal Random Variables
97
Table 5.7
Percentage of the coverage with equal means and d‚àó= 1.0
ConÔ¨Åguration
P ‚àó
half width
0.90
0.95
0.99
Weighted ÀÜP(C)
d‚àó
0.9026
0.9478
0.9905
Overall ÀÜP(C)
d‚àó
0.9973
0.9973
0.9983
Equal
Weighted ÀÜP(C)
wi0
0.6780
0.8195
0.9704
Overall ÀÜP(C)
wi0
0.9145
0.9546
0.9927
¬ØT
126
126
132
Weighted ÀÜP(C)
d‚àó
0.9018
0.9455
0.9907
Overall ÀÜP(C)
d‚àó
0.9618
0.9778
0.9948
Increasing
Weighted ÀÜP(C)
wi0
0.8008
0.8962
0.9835
Overall ÀÜP(C)
wi0
0.8876
0.9445
0.9915
¬ØT
193
253
408
Weighted ÀÜP(C)
d‚àó
0.8997
0.9484
0.9896
Overall ÀÜP(C)
d‚àó
0.9997
1.0000
0.9999
Decreasing
Weighted ÀÜP(C)
wi0
0.5797
0.7257
0.9215
Overall ÀÜP(C)
wi0
0.9310
0.9672
0.9922
¬ØT
126
126
127
In the setting that d‚àó= 1.0, the initial sample size (i.e., 126= (20+1)6)
is large enough to achieve greater precision than the speciÔ¨Åed precision in
many cases.
The weighted sample means purposely lose information so
that the observed coverages of the c.i. constructed by ÀúXi ‚àíÀúX0 ‚àíd‚àóare
close to the nominal values. On the other hand, in those cases the observed
coverages of the c.i. constructed by ÀúXi ‚àíÀúX0 ‚àíwi0 are less than the nominal
values because Ni = n0 + 1 for i = 0, 1, . . . , k are greater than the required
sample sizes (say ai for i = 0, 1, . . . , k). Hence,
wi0 = hc
‚àö
2

S2
i (n0)
Ni
+ S2
0(n0)
N0
‚â§hc
‚àö
2

S2
i (n0)
ai
+ S2
0(n0)
a0
‚âàd‚àó.
That is, in the cases that ai ‚â§n0, the variance of ÀúXi is greater than ¬ØXi while
the c.i. half width wi0 is computed based on the variance of ¬ØXi and results
in low coverages. On the other hand, the coverages of c.i. constructed by
¬ØXi ‚àí¬ØX0 ‚àíd‚àóare greater than the nominal values. i.e., the half width is
wider than necessary.
5.4
Summary
We have investigated properties of order statistics of correlated normal ran-
dom variables and discussed how order statistics are applied in simulation,
e.g., to perform comparison with a control and multiple decision. Further-
more, we proposed a new approach to estimate the values of the cdf of

98
Crafts of Simulation Programming
correlated normal random variables. The new approach is Ô¨Çexible and can
be used to estimate the critical constants for the problem at hand, even
when the correlations are unknown or unequal.

Chapter 6
Histogram and Quasi-Independent
Procedure
Simulation is often used to investigate system characteristics, i.e., the dis-
tributional properties of an output statistic calculated from the simulation‚Äôs
results, such as the mean or variance of system performance measures. In
many applications we want to estimate the probability that a future obser-
vation will exceed a given level during some speciÔ¨Åed epoch. For example,
a machine may break down if a certain temperature is reached. Among
many other aspects, reliability analysis (risk analysis) studies the expected
life and the failure rate of a component or a system of components linked
together in some structure. It is often of interest to estimate the reliability
of the component/system from the observed lifetime data. With probabil-
ity modeling, normal and lognormal densities are commonly used to model
certain lifetimes in reliability and survival analysis as well as risk manage-
ment.
Risk management is the identiÔ¨Åcation, assessment, and prioritization of
risks to minimize the probability of unfortunate events, e.g., machine break
down, loss of Ô¨Ånancial assets, cyberattacks. Inadequate risk management
can result in severe consequences for individuals as well as the entire society.
For instance, the U.S. subprime mortgage crisis and the associated reces-
sion were largely caused by the loose credit risk management of Ô¨Ånancial
Ô¨Årms. Risk management often starts with a probabilistic risk assessment, a
systematic and comprehensive methodology to identify and evaluate risks.
Based on the assessment, the planner then comes up a strategy and/or plan
to minimize the loss (cost), or to maximize the return (opportunity). The
simplest models (of analyzing risks) often consist of a probability multiplied
by an impact (severity of the possible adverse consequences). Understand-
ing risks may be diÔ¨Écult as multiple factors (impacts) can contribute to
the total probability of risk. In Ô¨Ånancial mathematics and Ô¨Ånancial risk
99

100
Crafts of Simulation Programming
management, value at risk (a quantile-based risk measurement) is a widely
used risk measure of the risk of loss on speciÔ¨Åc portfolio of Ô¨Ånancial assets.
We propose a method to construct an empirical distribution of the out-
put statistic of interest. The associated distributional characteristics are
then estimated based on the empirical distribution. For 0 < p < 1, the p
quantile (or 100p percentile) of a distribution is the value at or below which
100p percent of the distribution lies. Related to quantiles, a histogram is
a graphical estimate of the underlying probability density (or mass) func-
tion. The range of the output data is divided into intervals, or bins, and the
number or proportion of the observations falling into each bin is tabulated
or plotted. A histogram can be constructed with a properly selected set of
quantiles.
We extend the quasi-independent (QI) algorithm of [Chen and Kelton
(2003)] to determine the simulation run length and use grid points to con-
struct a histogram (with a selected set of quantiles) and use the histogram
as an empirical distribution for estimating quantiles. [Iglehart (1976); Seila
(1982); Hurley (1995)] have developed quantile-estimation algorithms based
on grid points. However, their procedures require that users enter the val-
ues of the grid points. Moreover, the procedures of [Iglehart (1976)] and
[Seila (1982)] require that the underlying processes have the regenerative
property, which can be diÔ¨Écult to know in large, complex simulation mod-
els.
The procedures of [Heidelberger
(1984)] and [Raatikainen
(1990)]
requires that the output sequences satisfy œÜ-mixing conditions. Similarly,
the procedure described in this chapter requires that as well. There are
many other quantile-estimation procedures, see, e.g., [Alexopoulos et al.
(2014)] and references therein.
The asymptotic validity of the QI procedure occurs as the QI sequence
appears to be independent, as determined by the runs test.
The main
advantage of the approach is that by using grids to approximate the un-
derlying distribution, it avoids the burden of storing and sorting all the
observations, which becomes prohibitive in very long runs that might be
needed in steady-state simulation. However, the savings come at a cost.
Using interpolation to obtain quantile estimates introduces bias. Fortu-
nately, the bias can be reduced by specifying Ô¨Åner grid points, which would
then require longer execution time, so there is a natural trade-oÔ¨Ähere.

Histogram and Quasi-Independent Procedure
101
6.1
Introduction and DeÔ¨Ånitions
Sample quantiles based on the order statistics are the natural estimator of
quantiles and have strong theoretical basis. Furthermore, order-statistics
quantile estimates are non-parametric and are valid regardless the shape
of the underlying distribution.
On the contrary, indirect approaches or
parametric quantile estimation procedures, which assume that the data are
drawn from a known parametric family of distributions, need to be used
with caution.
For example, if one approximates the steady-state waiting time in sys-
tem distribution of the M/M/1 queuing process with the service rate ŒΩ = 1
and the arrival rate Œª = 0.8 with a normal distribution and uses the sample
mean ÀÜŒº plus z0.9Sx to estimate the 0.9 quantile will get good results by
coincidence. Here z1‚àíŒ± is the 1 ‚àíŒ± quantile of the standard normal distri-
bution and Sx is the standard deviation of the samples. The high accuracy
of estimating the 0.9 quantiles does not hold when estimating some other
quantiles.
Let Wi denote the waiting time in system of the ith customer and let
œÅ = Œª/ŒΩ be the traÔ¨Éc intensity. Then, if œÅ < 1, the theoretical steady-state
waiting time in system distribution of this M/M/1 queuing process is
F(x) = Pr[Wi ‚â§x] ‚Üí1 ‚àíe‚àí(ŒΩ‚àíŒª)x as i ‚Üí‚àû
for all x ‚â•0. Hence, the true steady-state distribution and the true 0.9
quantile of this M/M/1 queuing process are, respectively, F(x) ‚Üí1 ‚àí
e‚àí(1‚àí0.8)x and approximately 11.512925. Furthermore, both the mean and
standard deviation of this distribution are 5 (i.e., 1/(ŒΩ ‚àíŒª) = 1/(1 ‚àí0.8)).
Consequently, the naive quantile estimator is approximately 11.41 (i.e.,
5 + 1.282 √ó 5), which is close to the true 0.9 quantile. However, this naive
quantile estimator may be away from the true quantile when p Ã∏= 0.9.
Let MM1(0.8) denote the steady-state waiting time in system distri-
bution of the M/M/1 queuing process with traÔ¨Éc intensity 0.8 and let
N(Œº, œÉ2) denote the normal distribution with mean Œº and variance œÉ2. Fig-
ure 6.1 compares the true distributions of N(5, 52) and MM1(0.8) for x ‚â•0.
From the diagram, the naive estimator will be close to the true quantile
around the 0.23 and 0.89 quantiles, where these two distributions intersect.
The naive estimator is likely to be biased high for 0.23 < p < 0.89. For
instance, the true 0.5 quantile of N(5, 52) and MM1(0.8) are, respectively, 5
and 3.465736. Again, one may get good results by coincide when the mean
or variance estimators are biased low, which are likely when the underlying
sequences are correlated.

102
Crafts of Simulation Programming
Fig. 6.1
True distributions of N(5, 52) and MM1(0.8)
It turns out that the diagram will be similar if the value of œÅ is changed,
i.e., the two distributions intersect around the 0.23 and 0.89 quantiles re-
gardless the value of œÅ. Let p quantile be an intersect of the two distribu-
tions. Then ÀÜxp = 1/(ŒΩ ‚àíŒª) + zp/(ŒΩ ‚àíŒª) and
F(ÀÜxp) = 1 ‚àíe‚àí(ŒΩ‚àíŒª)ÀÜx = 1 ‚àíe‚àí(1+zp).
Note that F(ÀÜxp) is independent of ŒΩ and Œª. Consequently, we will have good
results using normal approximation to estimate p quantiles (for p close to
0.23 or 0.89) of the steady-state waiting time distribution of the M/M/1
queuing process regardless the value of œÅ; assuming the mean and variance
estimates are accurate. Unfortunately, in most cases the distribution from
which quantiles are to be estimated is unknown and we don‚Äôt know whether
or where it intersects with the approximated (normal) distribution or how
close these two distributions are. Furthermore, it is not known whether the
intersects stay at the same quantiles when the parameters of the system
change.
Moreover, non-order-statistics (indirect) quantile estimates have not
shown to be normally distributed and can not be used directly as input
of selection procedures. Batch means can be used to manufacture approx-
imately i.i.d. normal data so that they can be used as input of selection
procedures. Nevertheless, incorporating batch means into the process in-
troduces additional complexities. Note that if samples are i.i.d. normal, the

Histogram and Quasi-Independent Procedure
103
c.i. coverage should stay the same when the sample sizes are Ô¨Åxed regardless
whether batch means are used.
On the other hand, order-statistics (direct) quantile estimates are
asymptotically normally distributed and can be used as input of selection
procedures directly. Consequently, any ranking and selection procedures
that are developed to select the design with the best mean can be used
to select design with the best quantile.
The criticism of the direct ap-
proach are that they require large sample sizes, large data storage, and
are computationally intensive; especially when estimating multiple quan-
tiles simultaneously. With the advance of modern computers, except in
extreme cases most of these issues are no longer major concerns. More-
over, the histogram-approximation-quantile-estimation procedure does not
require storing and sorting all the observations and can estimate multiple
quantiles simultaneously without specifying the quantiles to be estimated
in advance.
6.1.1
The Natural Estimators
Let F(¬∑) and FN(¬∑) respectively denote the true and the (sampled) empir-
ical steady-state cumulative distribution function of the simulation-output
process under study, where N is the simulation run length (we assume
a discrete-time output process X1, X2, . . .). For purpose of analysis, it is
convenient to express FN(¬∑) as
FN(x) = 1
N
N

i=1
I(‚àí‚àû,x](Xi), where I(‚àí‚àû,x](Xi) =
1 if Xi ‚â§x,
0 if Xi > x.
In addition to the autocorrelations of the stochastic process output se-
quence approaching zero as the lag between observations increases, we re-
quire two things for the method to work. First, FN(¬∑) must converge to
F(¬∑) as N ‚Üí‚àû. In random (i.e., independent) sampling, X1, X2, . . . , XN
is a random sample of size N, and if N ‚Üí‚àû, then FN(¬∑) will tend with
probability one to F(¬∑). Second, if a statistic T is estimating some charac-
teristic property Œ® of the distribution, then this characteristic must satisfy
certain smoothness properties (e.g., continuity and diÔ¨Äerentiability). Most
characteristics such as moments and percentiles are smooth and so their es-
timators have distributions that can be estimated. The procedure may fail
when T is a statistic estimating a function that is not smooth. The property
Œ® could be, for example, the mean, variance, or a quantile. The natural
point estimator for Œ®, denoted by ÀÜŒ®, is typically the sample mean, the

104
Crafts of Simulation Programming
sample variance, the sample quantile, or a simple function of the relevant
order statistics, chosen to imitate the performance measure Œ®. Further-
more, the natural estimators are appropriate for estimating any Œ®, even
in the presence of autocorrelation, which follows since FN(¬∑) converges to
F(¬∑).
6.1.2
Proportion Estimation
Here, we are interested in estimating the probability p that the output
random variable Xi belongs to a pre-speciÔ¨Åed set œâ: p = Pr[Xi ‚ààœâ]. Let
Ii be the indicator functions
Ii =
 1 if Xi ‚ààœâ,
0 otherwise.
An estimate of p is based on a transformation of the output sequence {Xi}
to the sequence {Ii}, i = 1, 2, . . . , N:
ÀÜp = 1
N
N

i=1
Ii.
For data that are i.i.d., the following properties of Ii are well known
[Hogg et al. (2012)]: E(Ii) = p and Var(Ii) = p(1‚àíp). Moreover, E(ÀÜp) = p
and Var(ÀÜp) = p(1 ‚àíp)/N. Thus, an exact c.i. for the estimated proportion
ÀÜp can be obtained using the binomial distribution. That is,
p ‚ààÀÜp ¬± tn‚àí1,1‚àíŒ±/2

ÀÜp(1 ‚àíÀÜp)
n ‚àí1 .
However, we cannot assume that the Ii‚Äôs are independent.
Instead, we
assume merely that the sequence {Ii} is covariance-stationary. A discrete-
time stochastic process X1, X2, . . . is said to be covariance-stationary if
Œºi = Œº and œÉ2
i = œÉ2 for i = 1, 2, . . . .
Here ‚àí‚àû< Œº < ‚àûand œÉ2 < ‚àû. Furthermore, Œ≥i,i+k = Cov(Xi, Xi+k) is
independent of i for k = 1, 2, . . .. That is, the covariance between Xi and
Xi+k depends only on k. Hence, the covariance-stationary lag-k covariance
of the process Œ≥k = Œ≥i,i+k for all i.
The steady-state variance constant of the process is
SSVC(Ii) = Œ≥0 + 2 lim
N‚Üí‚àû
N‚àí1

k=1
(1 ‚àík/N)Œ≥k.
(6.1)

Histogram and Quasi-Independent Procedure
105
Note that the lag-k correlation of the process
Ck =
Œ≥i,i+k

œÉ2
i œÉ2
i+k
= Œ≥k
œÉ2 = Œ≥k
Œ≥0
for k = 0, 1, 2, . . . .
It follows from the deÔ¨Ånition of the steady-state variance constant that for
large N
Var(ÀÜp) ‚âàSSVC(Ij)/N.
(6.2)
Since ÀÜp is based on the mean of the random variable Ij, we can use any
method developed for estimating the variance of the mean to estimate
Var(ÀÜp).
6.1.3
Quantile Estimation
If {Xi : i = 1, 2, . . . , N} is a sequence of i.i.d. random variables from a
continuous distribution F(x) with pdf f(x), let xp (0 < p < 1) denote the
100pth percentile (or the p quantile), which has the property that F(xp) =
Pr[X ‚â§xp] = p. Thus, xp = inf{x : F(x) ‚â•p}. Note that estimating
quantiles is the inverse of of the problem of estimating a proportion or
probability. If {Yi : i = 1, 2, . . . , N}, are the order statistics corresponding
to the Xi‚Äôs from N independent observations, (i.e. Yi is the ith smallest of
X1, X2, . . . , XN) then a point estimator for xp based on the order statistics
is the sample p quantile ÀÜxp,
ÀÜxp = Y‚åàNp‚åâ,
(6.3)
where ‚åàz‚åâdenotes the integer ceiling (round-up function) of the real number
z.
If the Xi‚Äôs are i.i.d., we have the following properties [David and Na-
garaja (2003)]:
E(ÀÜxp) = xp ‚àíp(1 ‚àíp)f ‚Ä≤(xp)
2(n + 2)f 3(xp) + O(1/N 2),
Var(ÀÜxp) =
p(1 ‚àíp)
(n + 2)f 2(xp) + O(1/N 2).
(6.4)
It follows from the central limit theorem for the sample mean that
ÀÜxp ‚àíxp

Var(ÀÜxp)
D
‚àí‚ÜíN(0, 1) as N ‚Üí‚àû.
(6.5)
If the Xi‚Äôs are correlated, however (as is usually the case if they are
a simulation-run output sequence), quantile estimation is much more dif-
Ô¨Åcult than in the i.i.d. case. The usual order-statistic point estimate ÀÜxp

106
Crafts of Simulation Programming
is still asymptotically unbiased; however, its variance needs to be adjusted
for the autocorrelation of the sequence. [Sen
(1972)] shows that for œÜ-
mixing sequences, the variance of sample quantiles is inÔ¨Çated by a factor of
SSVC(Ij)/(p(1 ‚àíp)).
6.2
Methodologies
This section presents the methodologies we will use for the quantile and
histogram estimation.
6.2.1
Determining the Simulation Run Length
The procedure will progressively increase the simulation run length N until
a subsequence of n observations (taken from the original output sequence)
appears to be independent, as determined by the runs test.
Let
[P]1
0 =
‚éß
‚é®
‚é©
P if 0 ‚â§P ‚â§1,
0 if P < 0,
1 if P > 1,
and let œµ be the desired half-width of the level 1 ‚àíŒ±1 conÔ¨Ådence interval
for the proportion.
Note that parameters œµ and Œ±1 are associated with
the proportion and are diÔ¨Äerent from œµ‚Ä≤, œµ‚Ä≤‚Ä≤, and Œ± that will be introduced
later. The half-width œµ is dimensionless; it is a proportion value with no
measurement unit and must be between 0 and max(p, 1 ‚àíp), 0 < p < 1.
Recall that if the data are i.i.d. and we would like to have 95% conÔ¨Ådence
that the 0.5 quantile estimator has no more than œµ = 0.005 (coverage)
deviation from the true but unknown quantile, the required simulation run
length (sample size) is N = n0.5 ‚â•1.9620.5(1‚àí0.5)
0.0052
= 38, 416, see Chapter
4. For correlated sequence, the simulation run length will be N = n0.5l,
where l is the lag that systematic samples pass the test of independence,
see Chapter 2. The rational is that the sample size N determined this way
is suÔ¨Éciently large such that quantile estimates are i.i.d. normal.
6.2.2
Histogram Approximation
To avoid storing and sorting the whole output sequence, the procedure
computes sample quantiles only at certain grid points and use (four-point)
Lagrange interpolation [Knuth (1998)] to estimate the general p quantile.

Histogram and Quasi-Independent Procedure
107
The procedure requires that users enter the values of œµ and Œ±1 for the
desired precision and conÔ¨Ådence. There are two categories of grids: main
grids and auxiliary grids. Main grids are constructed based on the initial
observations that ‚Äúanchor‚Äù the grid of the simulation-generated histogram,
while auxiliary grids are extensions of main grids to ensure the grids cover
all future observations. The number of main grid points is Gm = ‚åà1/œµ‚åâ, and
the number of auxiliary grid points is Ga = 2‚åàŒ¥Gm‚åâ+ 3, where 0 < Œ¥ < 1.
Based on previous experiments, we recommend Œ¥ ‚â•0.1. The total number
of grid points is thus G = Gm + Ga. Let the beginning and ending indices
of the main grid points be respectively b = ‚åàŒ¥Gm‚åâ+ 1 and e = b + Gm;
denote the main grid points by gi for i = b + 1, . . . , e. The value of the grid
points g0, g1, . . . , gG‚àí1 will be constructed as follows: g0 and gG‚àí1 are set
to ‚àí‚àûand ‚àû(in practice the minimum and maximum values on the host
computer) respectively. If the analyst knows what can be the minimum or
maximum values of the distribution, those values should be used instead.
For example, the waiting time in any queuing system cannot be negative,
so the analyst should enter 0 as the minimum g0.
The selection of grid size is based on initial observations. Grid point gb
is set to the minimum of the initial n, 2n, or 3n observations, depending on
the degree of the autocorrelation of the sequence, as determined by the runs
test. Grid points gb+i, i = 1, 2, . . . , Gm, are set to the i/Gm quantile of the
initial n, 2n, or 3n observations, depending on the degree of autocorrelation
of the sequence. We will set grid points g1 through gb‚àí1 and ge+1 through
gG‚àí2 to appropriate values so that g1 through gb+1 will have the same grid
size (the length between two adjacent grid points) and ge‚àí1 through gG‚àí2
will have the same grid size. A corresponding array Œ∑1, Œ∑2, . . . , Œ∑G‚àí1 is used
to store the number of observations between two consecutive grid points.
For example, the number of observations between gi‚àí1 and gi is Œ∑i.
Once the QI algorithm has determined that the simulation run length
is long enough for the required precision, we can then compute the quantile
estimator by Lagrange interpolation of the quantile at four grid points. The
array Œ∑i, i = 1, 2, . . . , G‚àí1, stores the number of observations between grid
points gi‚àí1 and gi, so the quantile of gi at grid point i can be computed
as ÀÜF(gi) = pi = i
j=1 nj/N, for i = 1, 2, . . . , G ‚àí1, where N = G‚àí1
j=1 nj
is the simulation run length (i.e., number of observations). Thus, for some
k such that pk‚àí1 < p ‚â§pk, the p quantile estimator can be computed as

108
Crafts of Simulation Programming
follows. Let
œñj =
4

i=1,iÃ∏=j
p ‚àípk+i‚àí3
pk+j‚àí3 ‚àípk+i‚àí3
, for j = 1, 2, 3, 4,
and
ÀÜxp =
4

j=1
œñjgk+j‚àí3.
(6.6)
In two extreme cases, p0 < p ‚â§p1 or pG‚àí2 < p ‚â§pG‚àí1, linear interpolation
will be used. Users can also rerun the simulation with Ô¨Åner grid points
around the estimated quantile value to get a more accurate estimate.
Because we are estimating quantiles of stochastic systems, inferences
based on only one output sequence are unreliable. Therefore, we will run R
(we use R = 3 in the algorithm) replications to get R quantile estimators.
Let ÀÜxp,r denote the estimator of xp in the rth replication. We use
¬ØÀÜxp = 1
R
R

r=1
ÀÜxp,r
(6.7)
as a point estimator of xp. Assuming that the asymptotic approximation
is valid with the simulation run length determined by the procedure, each
ÀÜxp,r has a limiting normal distribution. By the central limit theorem, a c.i.
for xp using the i.i.d. ÀÜxp,r‚Äôs can be approximated using standard statistical
procedures. That is, the ratio
T =
¬ØÀÜxp ‚àíxp
S/
‚àö
R
would have an approximate t distribution with R ‚àí1 d.f., where
S2 =
1
R ‚àí1
R

r=1
(ÀÜxp,r ‚àí¬ØÀÜxp)
2
is the usual estimator of œÉ2
p(n), the variance of ÀÜxp. This would then lead to
the 100(1 ‚àíŒ±)% c.i., for xp,
¬ØÀÜxp ¬± tR‚àí1,1‚àíŒ±/2
S
‚àö
R
.
(6.8)

Histogram and Quasi-Independent Procedure
109
The quasi-independent quantile estimation algorithm:
(1) The quantity œµ is the desired half-width of the 100(1 ‚àíŒ±1)% c.i. for
the proportion speciÔ¨Åed by the user, Bs is the size of the buÔ¨Äer that
is used to store QI samples, and r is the number of iterations. Each
iteration r contains two sub-iterations rA and rB. Note that the loop
of steps 3 through 10 is considered an iteration.
(2) Compute the required QI sequence size n from Eq. (4.6) with the
user-speciÔ¨Åed Œ±1, and p set to 0.5. If n < 4000, set n = 4000. If n is
odd, then increase n by 1. Set Bs = 3n and r = 0. Generate N = n
observations as the initial samples.
(3) Carry out runs tests to determine whether the sequence appears to
be independent. The runs test uses 4000 samples. If this is the initial
iteration, use lag-1 samples in the QI sequence. If this is a rA iteration,
use lag-2 samples in the QI sequence. If this is a rB iteration, use lag-3
samples in the QI sequence.
(4) If the QI samples appear to be independent or the run length has
reached Bs, compute the value of grid points as discussed, i.e., the
grids between two consecutive main grid points will contain a propor-
tion approximately equal to œµ of the distribution.
(5) If the QI samples appear to be independent, go to step 11.
(6) If the current iteration is a rA iteration, start a rB iteration. If the
current iteration is the initial one or a rB iteration, set r = r + 1 and
start a rA iteration.
(7) If this is the 1th
A or 1th
B iteration, generate n observations, store those
values in the buÔ¨Äer after the ones already there, and go to step 3.
(8) If this is a rA iteration (r ‚â•2), then discard every other sample in
the buÔ¨Äer, and reindex the rest of the 3n samples in the Ô¨Årst half of
the buÔ¨Äer. Generate 2r‚àí2n observations and store those n/2 samples
that are lag-2r‚àí1 observations apart in the later portion of the buÔ¨Äer.
(9) If this is a rB iteration (r ‚â•2), generate 2r‚àí1n observations and
store those n samples that are lag-2r‚àí1 observations apart in the later
portion of the buÔ¨Äer.
(10) Go to step 3.
(11) Compute the p quantile estimator according to Eq. (6.6).
(12) Run R replications and compute the conÔ¨Ådence interval of the quantile
estimator according to Eq. (6.8).
(13) Let œµ‚Ä≤ be the desired absolute half-width (or let Œ≥|ÀÜxp| be the desired
relative half-width). If the absolute (or relative) half-width of the c.i.

110
Crafts of Simulation Programming
is less than œµ‚Ä≤ (or Œ≥|ÀÜxp|), terminate the algorithm. Note that œµ‚Ä≤ has
the same units of measurement as do the observations and does not
need to be a fraction.
(14) Run one more replication, and set R = R + 1. Go to step 13.
The QI procedure addresses the problem of determining the simula-
tion run length that is required to satisfy the assumptions of independence
and normality of the quantile estimator. Theoretically, if these assump-
tions are satisÔ¨Åed, then the actual coverage of the c.i.‚Äôs should be close to
the pre-speciÔ¨Åed level. However, we are not sure whether the asymptotic
approximation is valid, so the c.i. constructed by Eq. (6.8) may have cover-
age less than speciÔ¨Åed. On the other hand, the quantile estimators should
satisfy the precision requirement of Eq. (4.5).
Let the half-width be w = tR‚àí1,1‚àíŒ±/2S/
‚àö
R. The Ô¨Ånal step of the QI pro-
cedure is to determine whether the c.i. meets the user‚Äôs half-width require-
ment, a maximum absolute half-width œµ‚Ä≤ or a maximum relative fraction Œ≥
of the magnitude of the Ô¨Ånal point quantile estimator ¬ØÀÜxp. If the relevant re-
quirement w ‚â§œµ‚Ä≤ or w ‚â§Œ≥|¬ØÀÜxp| for the precision of the c.i. is satisÔ¨Åed, then
the QI procedure terminates, and we return the point quantile estimator ¬ØÀÜxp
and the c.i. with half-width w. If the precision requirement is not satisÔ¨Åed
with R replications, then the QI procedure will increase the number of repli-
cations by one. This step can be repeated iteratively until the pre-speciÔ¨Åed
half-width criterion is achieved. This histogram-approximation method can
estimate multiple quantiles simultaneously without much extra eÔ¨Äort.
6.2.3
Two-Phase Quantile Estimation
This section presents the methodologies we will use to construct a c.i. of the
quantiles such that they satisfy the absolute or relative half-width require-
ments, i.e., the quantile estimator ÀÜxp satisÔ¨Åes the precision requirement
Pr[xp ‚ààÀÜxp ¬± œµ‚Ä≤] ‚â•1 ‚àíŒ± or
Pr[xp ‚ààÀÜxp ¬± Œ≥|ÀÜxp|] ‚â•1 ‚àíŒ±,
(6.9)
where œµ‚Ä≤ > 0 and 0 < Œ≥ < 1 are, respectively, the absolute and relative
precision sought.
For i.i.d. sequences (l = 1) if N ‚â•np (of Eq. (4.6)), then the quantile
estimator should satisfy the precision requirement of Eq. (4.5). From Eq.
(6.4), asymptotically
Var(ÀÜxp) ‚âàp(1 ‚àíp)
Nf 2(xp).

Histogram and Quasi-Independent Procedure
111
Therefore, when data are i.i.d.
Var(ÀÜxp)
Var(ÀÜp) ‚âà1/f 2(xp).
(6.10)
Thus, if the run length is N ‚Ä≤ ‚â•N/f 2(xp), then Pr[xp ‚ààÀÜxp ¬±œµ‚Ä≤‚Ä≤] ‚â•1‚àíŒ±,
where œµ‚Ä≤‚Ä≤ has the same numerical value as œµ (which is unitless) and has the
same units as xp. Moreover, if œµ‚Ä≤ is the desired absolute precision and
N ‚Ä≤ ‚â•
N
f 2(xp)(œµ‚Ä≤‚Ä≤
œµ‚Ä≤ )2 (or equivalently
N
f 2(xp)( œµ
œµ‚Ä≤ )2),
then (approximately) Pr[xp ‚ààÀÜxp ¬± œµ‚Ä≤] ‚â•1 ‚àíŒ±. Note that 0 < œµ < 1, and
œµ‚Ä≤ > 0. Furthermore, if Œ≥ is the desired relative precision and
N ‚Ä≤ ‚â•
N
f 2(xp)( œµ‚Ä≤‚Ä≤
Œ≥ÀÜxp
)2 (or equivalently
N
f 2(xp)( œµ
Œ≥ÀÜxp
)2),
then (approximately) Pr[xp ‚ààÀÜxp ¬± Œ≥|ÀÜxp|] ‚â•1 ‚àíŒ±.
Theoretically, when N is large, the value of f(xp) can be approximated
by Ô¨Ånite forward diÔ¨Äerences:
f(xp) =
1
N(ÀÜxp+1/N ‚àíÀÜxp)
(6.11)
because
F ‚Ä≤(xp) = lim
x‚Üíxp
F(x) ‚àíF(xp)
x ‚àíxp
‚âàF(xp+1/N) ‚àíF(xp)
xp+1/N ‚àíxp
.
Alternatively, the value of f(xp) can be approximated by Ô¨Ånite central
diÔ¨Äerences:
f(xp) =
2
N(ÀÜxp+1/N ‚àíÀÜxp‚àí1/N) because F ‚Ä≤(xp) ‚âàF(xp+1/N) ‚àíF(xp‚àí1/N)
xp+1/N ‚àíxp‚àí1/N
.
However, in practice the required value for N is not known for this
approximation to be good, so f(xp) may be very diÔ¨Äerent from f(xp).
The performance of derivative estimation with Ô¨Ånite diÔ¨Äerences using the
empirical distribution constructed with the method described in Section
6.2.2 is generally excellent in terms of c.i. coverage and relative precision;
see [Chen
(2003)]. The value f(xp) has great inÔ¨Çuence on the required
simulation run length. Because we don‚Äôt know the value xp and the pdf
f(¬∑) of the underlying process, we use the estimated value ÀÜf(ÀÜxp). To be
conservative, we use the value ÀÜf(ÀÜxp) such that asymptotically Pr[f(xp) ‚â•
ÀÜf(ÀÜxp)] ‚â•0.6.

112
Crafts of Simulation Programming
The two-phase quantile estimation algorithm:
(1) The quantity œµ is the desired proportional 100(1‚àíŒ±1)% c.i. half-width,
Œ≥ is the relative precision speciÔ¨Åed by the user, 100(1 ‚àíŒ±)% is the
desired conÔ¨Ådence of coverage with œµ‚Ä≤, and N is the run length.
(2) Use the quantile and histogram estimation algorithm, described in
Section 6.2.2, to obtain proportional precision quantile estimates.
(3) Use the Ô¨Ånite forward diÔ¨Äerences Eq. (6.11) to obtain the derivative
estimate ÀÜdp = ÀÜf(ÀÜxp).
(4) Let N ‚Ä≤ = ‚åàN
ÀÜd2
p (
œµ
Œ≥ÀÜxp )2‚åâ.
(5) If N ‚Ä≤ > N, increase the run length to N ‚Ä≤, and go to step 2.
(6) Otherwise, the quantile estimate should already satisfy the relative-
precision requirement.
(7) Run R replications and compute the conÔ¨Ådence interval of the quantile
estimator according to Eq. (6.8).
To improve the precision of the second-phase quantile estimation, we can
put more grid points in the grid that contains the quantile estimator ÀÜxp in
the Ô¨Årst phase and the surrounding grids before we start the second phase.
Of course, the newly set-up grid points need to be based on interpolation.
For example, if gi‚àí1 < ÀÜxp ‚â§gi and the grid between gi‚àí1 and gi contains
Œ∑i observations and approximately 100pi% of the distribution, then (say)
Œ∫ + 1 new grid points can be set up between ÀÜxp‚àípi/2 and ÀÜxp+pi/2. Let g‚Ä≤
j
for j = 0, 1, . . . , Œ∫ be the new grid points; then g‚Ä≤
j = ÀÜxp‚àípi/2+j(pi/Œ∫). The
array contains the number of observations between newly set-up grid points
Œ∑‚Ä≤
j ‚âàŒ∑i/Œ∫, for j = 1, 2, . . . , Œ∫ ‚àí1 and Œ∑‚Ä≤
Œ∫ = Œ∑i ‚àí(Œ∫ ‚àí1)Œ∑‚Ä≤
1.
This method can also be used to determine the required simulation run
length to obtain a discrete distributed quantile estimate that satisÔ¨Åes a pre-
speciÔ¨Åed precision. However, the quantile estimate is computed through
interpolation so it may not be a valid value of the underlying discrete dis-
tribution. If the output data can be read through again, then a valid value
can be estimated. In the Ô¨Årst phase we obtain lower and upper bounds
(Ya and Yb) of the quantile. When we read the data again in the second
phase, we will count the number of observations that are less than the lower
bound, record the values that are between lower and upper bounds, and
count the number of observations in each of those values. For example, if
there are N observations in total, Œ∑0 observations are less than the lower
bound Ya, Œ• values (Ya+i, i = 1, 2, . . . , Œ•) are between the lower and upper
bound and their corresponding number of observations are Œ∑i (i.e., Œ∑i is the

Histogram and Quasi-Independent Procedure
113
number of observations having the value Ya+i), then the p quantile will be
the value Yj such that Np ‚â§Œ∑0 + j
i=1 Œ∑i. Note that the Yi‚Äôs correspond
to the order statistics of the values of Xi‚Äôs that are between Ya and Yb.
6.2.4
A Source Code of Quantile Estimation
#define RUNSUPSZ 4000
/* compute the quantile estimators */
int estimatelag(int GRID, int GRID2, nSampSize,
int dist, double beta,double* alpha,double gamma,double* x0,
double* Xp,double* Np,int* gG,double* xSum)
{
int rank;
int i, j, k, l;
int cont = 0;
int runup=1;
double Observ[RUNSUPSZ];
double temp;
long
bufferSize=0;
int
G=*gG;
int
*Np2;
double *Xp2;
double *ObservA;
double *ObservB;
Np2
= (int
*) calloc(G,
sizeof(int)
);
Xp2
= (double *) calloc(G,
sizeof(double));
ObservA
= (double *) calloc(3*nSampSize, sizeof(double));
ObservB
= (double *) calloc(3*nSampSize, sizeof(double));
*xSum
= 0.0;
for (k = 0; k < 3; ++k) {
l = k + 1;
for (i = k*nSampSize; i < l*nSampSize; ++i) {
ObservA[i] = nextrand(dist,1,beta,alpha,gamma,x0);
*xSum += ObservA[i];
};

114
Crafts of Simulation Programming
for (i = 0; i < RUNSUPSZ; ++i)
Observ[i] = ObservA[l*i];
if (checkind(Observ)) {
cont = -1;
/* break out the while loop */
k = 3;
/* break out the for loop */
break;
}
};
bufferSize = 3*nSampSize;
for (i = 0; i < bufferSize; ++i)
ObservB[i] = ObservA[i];
qsort(ObservA, (unsigned int) bufferSize,
sizeof(ObservA[0]), &comparf);
/* set up the grid points */
Xp[0] = ObservA[0];
Xp[G] = ObservA[bufferSize - 1];
Xp2[GRID2+1]=Xp[0];
Np2[GRID2+1]=0;
for (i = 1; i <= GRID; ++i) {
rank = (int) ceil((1.0/GRID)*i*bufferSize) - 1;
Xp2[GRID2+1+i] = ObservA[rank];
Np2[GRID2+1+i] = rank+1;
};
for (i = GRID; i >= 2; --i)
Np2[GRID2+1+i] = Np2[GRID2+1+i] - Np2[GRID2+i];
/* Set up right tail */
temp = Xp2[GRID+GRID2+1] - Xp2[GRID+GRID2];
for (i = 1; i <= GRID2; ++i) {
Xp2[GRID+GRID2+1+i] = Xp2[GRID+GRID2+i] + temp;
Np2[GRID+GRID2+1+i] = 0;
};

Histogram and Quasi-Independent Procedure
115
temp = Xp2[GRID2+2] - Xp2[GRID2+1];
for (i = 1; i <= GRID2; ++i) {
Xp2[GRID2+1-i] = Xp2[GRID2+2-i] - temp;
Np2[GRID2+1-i] = 0;
};
/* clean up the buffers */
i = 1;
j = 1;
while (j < G) {
Xp[i] = Xp2[j];
Np[i] = Np2[j];
while (++j < G) {
if (Xp[i] == Xp2[j])
Np[i] += Np2[j];
else
break;
};
++i;
}; /* while j < G */
free(Xp2);
free(Np2);
G = i;
*gG = i;
int p[3];
p[0] = (int) (1.5*nSampSize);
p[1] = 2*nSampSize;
p[2] = 3*nSampSize;
while (++cont) {
for (i = 1; i < p[0]; ++i)
ObservB[i] = ObservB[2*i];

116
Crafts of Simulation Programming
k = (int) (pow(2,cont) - 1);
for (l = 0; l < 2; ++l) {
for (i = p[l]; i < p[l+1]; ++i) {
ObservB[i] = nextrand(dist,1,beta,alpha,gamma,x0);
*xSum += ObservB[i];
bsettable(G, Np, Xp, ObservB[i]);
for (j = 0; j < k; ++j) {
temp = nextrand(dist,1,beta,alpha,gamma,x0);
*xSum += temp;
bsettable(G, Np, Xp, temp);
}; /* for j */
}; /* for i */
for (i = 0; i < RUNSUPSZ; ++i)
Observ[i] = ObservB[(l+2)*i];
if (checkind(Observ)) {
cont = -1;
/* break out the while loop */
l = 3;
/* break out the for loop */
continue;
};
}; /* for l */
}; /* while cont */
free (ObservA);
free (ObservB);
return(1);
}
/* function to compare double for qsort */
int comparf(const void *A, const void *B)

Histogram and Quasi-Independent Procedure
117
{
double *TA;
double *TB;
TA = (double *)A;
TB = (double *)B;
if (*TA > *TB)
return (1);
if (*TA < *TB)
return (-1);
else
return (0);
}
/* procedure to set the Np table in binary order */
void bsettable(int G, double* Np, double* Xp, double target)
/* G is the size of the tables Np and Xp
Np stores the number of observations between grid points
Xp stores the value of grid points 0,1,..,G-1
target is the value to be recorded */
{
int l=1;
int u=G-2;
int b=(1+G-2)/2;
int i=1;
if (target > Xp[G-2]) {
++Np[G-1];
return;
} else if (target <= Xp[1]) {
++Np[1];
return;
}
while (i) { /* binary search */

118
Crafts of Simulation Programming
if (target > Xp[b]) {
l = b;
b = (b+u)/2;
if (l == b)
++b;
} else {
if (target > Xp[b-1]) {
++Np[b];
i = 0;
return;
};
u = b;
b = (l+b)/2;
}; /* if target */
}; /* while */
}
/* Lagrange interpolation */
double Lagrange(double p, double* Np, double* Xp,
int G, double bufferSize)
/* p is the fraction between 0 and 1
Np stores the number of observations between grid points
Xp stores the value of grid points 0,1,..,G-1
G is the size of tables Np and Xp
bufferSize is the total number of observations */
{
int
i, j, k;
double A[4];
double Y[4];
double P[4];
double temp;
double tempq = p;
double xp;
double size=0;
for (i = 1;
i < G; ++i) {
temp = 1.0*Np[i]/bufferSize;

Histogram and Quasi-Independent Procedure
119
if (tempq > temp) {
size += Np[i];
tempq -= temp;
continue;
} else
break;
};
k = i - 2; /* starting index for Lagrange */
if (i < 2) {
xp = Xp[0] + tempq*Xp[1];
return (xp);
} else
if (i > G-2) {
xp = Xp[E-2] + tempq*Xp[E-1];
return (xp);
};
size -= Np[k+1];
P[0] = 1.0*size / bufferSize; /* up to k=i-2 */
for (i = 1; i < 4; ++i)
P[i] = P[i-1] + 1.0*Np[k+i]/bufferSize;
tempq += P[1];
xp = 0.0;
for (i = 0; i < 4; ++i) {
A[i] = 1.0;
Y[i] = Xp[k+i];
for (j = 0; j < 4; ++j) {
if (i == j) continue;
A[i] *= (tempq - P[j])/(P[i] - P[j]);
};
xp += A[i]*Y[i];
};
return (xp);
}

120
Crafts of Simulation Programming
6.3
Empirical Experiments
In this section, we present some empirical results obtained from simulations
using the two-phase quantile estimation procedure. The purpose of the ex-
periments was not only to test the methods thoroughly, but also to demon-
strate the interdependence between the correlation of simulation output
sequences and simulation run lengths, and the validity of our methods. We
tested the proposed procedure with several i.i.d. and correlated sequences.
In these experiments, we use R = 3 (see step 7 in the algorithm) inde-
pendent replications to construct c.i.‚Äôs. We estimated four quantile points:
0.25, 0.50, 0.75, and 0.90 for each distribution and used a relative precision
of 0.05 for our experiments. We conservatively set the required parame-
ters of determining the simulation run length (i.e., Eq. (4.6)) with p = 0.5,
œµ = 0.005, and Œ±1 = 0.05. The conÔ¨Ådence level Œ±2 of the quantile c.i. (i.e.,
Eq. (6.8)) is set to 0.1. Moreover, the conÔ¨Ådence level of the runs-up test
of independent is set to 90%.
6.3.1
Independent Sequences
We tested two independent sequences:
‚Ä¢ Observations are i.i.d. normal with mean 0 and variance 1, denoted as
N(0, 1).
‚Ä¢ Observations are i.i.d. negative exponential with mean 1, denoted as
expon(1).
The summary of our experimental results of the i.i.d. sequences are
listed in Tables 6.1 and 6.2. Each design point is based on 100 independent
simulation runs. The p row lists the quantile we want to estimate. The
quantile row lists the true p quantile value.
The cover p row lists the
percentage of the quantile estimates that satisfy Eq. (4.5), i.e., the coverage
deviation of the quantile estimator is within the speciÔ¨Åed value œµ.
The
coverage row lists the percentage of the c.i.‚Äôs that cover the true quantile
value. The avg. rp row lists the average of the relative precision of the xp
estimators. Here, the relative precision is deÔ¨Åned as rp = |ÀÜxp ‚àíxp|/|ÀÜxp|.
The stdev rp row lists the standard deviation of the relative precision of
the quantile estimators. The avg. hw row lists the average of the absolute
half-width. The stdev hw row lists the standard deviation of the absolute
half-width. The avg. sp row lists the average of the sample size in each
independent replication. The stdev sp row lists the standard deviation of

Histogram and Quasi-Independent Procedure
121
Table 6.1
Coverage of 90% conÔ¨Ådence quantile estimators
of the N(0, 1) distribution
Item
Quantile
p
0.25
0.45
0.75
0.90
quantile
-0.674189
-0.125381
0.674189
1.28173
cover p
100%
100%
100%
100%
coverage
94%
94%
85%
90%
avg. rp
0.004832
0.010206
0.004693
0.002997
stdev rp
0.003624
0.007715
0.003680
0.002195
avg. hw
0.010011
0.004996
0.008920
0.012771
stdev hw
0.004653
0.002447
0.005363
0.006741
avg. sp
41875
175039
41875
41875
stdev sp
6267
42596
6267
6267
the sample size in each independent replication.
Table 6.1 lists the experimental results of the N(0, 1) distribution. For
the 0.5 quantile estimates, the parameter under investigation x0.5 is 0.
Since ÀÜx0.5 ‚âà0, n‚Ä≤ = ‚åàn
z2
p ( œµ‚Ä≤
Œ≥ÀÜxp )2‚åâwill be very large. For example, the sample
size in the Ô¨Årst phase is 38416 (1.962 √ó 0.5 √ó 0.5/0.0052), the estimator
ÀÜx0.5 = ‚àí0.000146, and zp = ÀÜf(ÀÜx0.5) = 0.363357. The required sample size
is then n‚Ä≤ = ‚åà
38416
0.3633572 (
0.005
0.05√ó0.000146)2‚åâ> 1.36 √ó 1011. It will require several
days for common desktop computers to obtain one estimator. If users know
that the true quantile value xp ‚âà0, then absolute precision can be used
instead of relative precision. To avoid the required long execution time, we
estimated the 0.45 quantile instead. The c.i.‚Äôs coverage of 0.75 quantile is
85%, which is less than the speciÔ¨Åed nominal value of 90%. We believe this
is caused by the randomness of the experiment and the half-width being
too small. For example, the absolute value of the 0.25 and 0.75 quantile are
the same, however, the average half-width of the 0.75 quantile estimates
is only 0.008920 compares to 0.010011 of the 0.25 quantile. Furthermore,
the average relative precision of the 0.75 quantile estimators is smaller than
that of the 0.25 quantile estimators, those 0.75 quantile c.i.‚Äôs that do not
cover the true quantile value must miss only by a very small amount. All
half-widths of the c.i.‚Äôs, are less than Œ≥|xp|, where Œ≥ = 0.05 and are in
general within 30% of Œ≥|xp|.
Table 6.2 lists the experimental results of the expon(1) distribution.
The sample sizes are the same for all four design points because all quan-
tile estimations have n‚Ä≤ = ‚åàn
z2
p ( œµ‚Ä≤
Œ≥ÀÜxp )2‚åâ< n. Since the value Zp = f(Xp)
decreases as Xp increases, sample size n does not increase as quantile value
increases when œµ‚Ä≤ is suÔ¨Éciently small and relative precision is used. On
the other hand, if absolute precision is used, then sample sizes will increase

122
Crafts of Simulation Programming
Table 6.2
Coverage of 90% conÔ¨Ådence quantile estimators
of the expon(1) distribution
Item
Quantile
p
0.25
0.50
0.75
0.90
quantile
0.287682
0.693147
1.38629
2.30258
cover p
100%
100%
100%
100%
coverage
91%
93%
91%
92%
avg. rp
0.004594
0.003241
0.002641
0.002982
stdev rp
0.003691
0.002630
0.002318
0.002186
avg. hw
0.004180
0.007144
0.013503
0.021339
stdev hw
0.001809
0.003672
0.007390
0.011029
avg. sp
41747
41747
41747
41747
stdev sp
6204
6204
6204
6204
as the quantile value increase. In this experiment, with relative precision
Œ≥ = 0.05, the quantile estimator obtained with the Ô¨Årst-phase sample size
should satisfy both Eq. (4.5) and Eq. (6.9). Again, all quantile estimators
satisfy the precision requirement of Eq. (4.5), and the c.i. coverage of these
design points are above the speciÔ¨Åed 90% conÔ¨Ådence level. Furthermore,
all half-widths are less than Œ≥|xp|. Since the true p quantile value increases
as p increases, the average half-width increases as p increases.
Because
we set the conÔ¨Ådence level of the runs-up test of independence to be 90%,
independent sequences will not pass the runs-up about 10% of the times.
Consequently, the Ô¨Årst-phase sample size for independent sequences will be
around 38416 √ó 1.1 = 42258.
6.3.2
Correlated Sequences
We tested four correlated sequences:
‚Ä¢ Steady-state of the Ô¨Årst-order moving average process, generated by
the recurrence relation
Xi = Œº + œµi + Œ∏œµi‚àí1 for i = 1, 2, . . . ,
where œµi is i.i.d. N(0, 1) and 0 < Œ∏ < 1. This process is denoted as
MA1(Œ∏). Œº is set to 0 in our experiments. It can be shown that X has
an asymptotic N(0, 1 + Œ∏2) distribution.
‚Ä¢ Steady-state of the Ô¨Årst-order autoregressive process, generated by the
recurrence relation
Xi = Œº + œï(Xi‚àí1 ‚àíŒº) + œµi for i = 1, 2, . . . ,

Histogram and Quasi-Independent Procedure
123
where œµi is i.i.d. N(0, 1), and
E(œµi) = 0,
E(œµiœµj) =
 œÉ2 if i = j ,
0
otherwise
0 < œï < 1.
This process is denoted as AR1(œï). Œº is set to 0 in our experiments.
It can be shown that X has an asymptotic N(0,
1
1‚àíœï2 ) distribution.
‚Ä¢ Steady-state of the M/M/1 delay-in-queue process with the arrival
rate (Œª) and the service rate (ŒΩ = 1).
This process is denoted as
MM1(Œª). Let Wi denote the waiting time of the ith customer and
œÅ = Œª/ŒΩ be the traÔ¨Éc intensity.
Then, if œÅ < 1, the theoretical
steady-state distribution of this M/M/1 queuing process is F(x) =
P(Wi ‚â§x) ‚Üí1 ‚àíœÅe‚àí(ŒΩ‚àíŒª)x as i ‚Üí‚àûfor all x ‚â•0. Let {An} denote
the interarrival-time i.i.d. sequence and {Sn} denote the service-time
i.i.d. sequence. Then the waiting-time sequence {Wn} is deÔ¨Åned by
Wn+1 = (Wn + Sn ‚àíAn+1)+ for
n ‚â•1
where w+ = max(w, 0).
‚Ä¢ Steady-state of the M/M/s delay-in-queue process with the arrival
rate (Œª) and the service rate (ŒΩ). This process is denoted as MMS(Œª).
We set s = 2, Œª = 3, and ŒΩ = 2. The traÔ¨Éc intensity of this process
is œÅ =
Œª
sŒΩ = 0.75.
We tested the MA1 model with Œ∏ = 0.75, the AR1 model with œï = 0.75,
and the M/M/1 and M/M/2 models with the traÔ¨Éc intensity œÅ = 0.75. In
order to eliminate the initial bias, œµ0 and w0 are set to a random variate
drawn from the steady-state distribution. Because the true 0.5 quantile
value for the tested MA1 and AR1 processes is 0, we estimated the 0.45
quantile to avoid an extremely large sample size.
The summary of our experimental results for MA1(0.75) and AR1(0.75)
are listed in Tables 6.3 and 6.4. All quantile estimators satisfy the precision
requirement of Eq. (4.5). Most of the c.i. coverage of these design points, ex-
cept the coverage of the 0.75 quantile of the AR1(0.75) process, are around
the speciÔ¨Åed 90% conÔ¨Ådence level. We believe this is caused by the half-
width being too small. For these two processes, n‚Ä≤ = ‚åàn
z2
p ( œµ‚Ä≤
Œ≥ÀÜxp )2‚åâ> n only
when estimating the 0.45 quantile. The sample sizes are larger than the
independent cases because the lag l for the QI sequence that appears to be
independent is larger.

124
Crafts of Simulation Programming
Table 6.3
Coverage of 90% conÔ¨Ådence quantile estimators
of
the
MA1(0.75)
process
Item
Quantile
p
0.25
0.45
0.75
0.90
quantile
-0.842737
-0.156726
0.842737
1.60216
cover p
100%
100%
100%
100%
coverage
91%
93%
90%
88%
avg. rp
0.004897
0.010006
0.004437
0.002558
stdev rp
0.003227
0.007751
0.003252
0.001905
avg. hw
0.011336
0.005633
0.010974
0.013519
stdev hw
0.005510
0.002892
0.005519
0.006688
avg. sp
80805
326563
80805
80805
stdev sp
6977
42356
6977
6977
Table 6.4
Coverage of 90% conÔ¨Ådence quantile estimators
of
the
AR1(0.75)
process
Item
Quantile
p
0.25
0.45
0.75
0.90
quantile
-1.01928
-0.189558
1.01928
1.93779
cover p
100%
100%
100%
100%
coverage
91%
92%
82%
91%
avg. rp
0.003340
0.009194
0.003583
0.001861
stdev rp
0.002279
0.007469
0.002411
0.001473
avg. hw
0.012107
0.005560
0.010085
0.012185
stdev hw
0.005923
0.002606
0.005668
0.006173
avg. sp
348067
1424348
348067
348067
stdev sp
54251
344237
54251
54251
If œÅ < 1, the waiting-time distribution function of a stationary M/M/1
delay in queue is discontinuous at F(x) ‚Üí1 ‚àíœÅ, (i.e. x = 0); thus, the
quantiles for M/M/1 delay-in-queue are applicable only when the estimated
quantiles are larger than or equal to 1 ‚àíœÅ. Therefore, it is useful to know
whether a desired quantile is attainable before conducting an informative
experiment.
The summary of our experimental results of the M/M/1 delay-in-queue
process is summarized in Table 6.5. We experienced some problems when
estimating the 0.25 quantile of the M/M/1 queuing process with œÅ = 0.75,
because the distribution is not continuous at the true quantile value 0.
Thus, the derivative does not exist at 0.25 quantile. Because the distri-
bution function has a jump at this quantile point, the procedure often
obtains ‚àûas an estimate of the derivative since ÀÜxp = ÀÜxp+1/N in this case.
Therefore, we estimate the 0.30 quantile instead of the 0.25 quantile. How-
ever, the procedure can return the quantile estimate obtained in the Ô¨Årst

Histogram and Quasi-Independent Procedure
125
Table 6.5
Coverage of 90% conÔ¨Ådence quantile estima-
tors of the M/M/1 delay-in-queue process with œÅ = 0.75
Item
Quantile
p
0.30
0.50
0.75
0.90
quantile
0.275972
1.62186
4.39445
8.05961
cover p
100%
100%
100%
100%
coverage
90%
91%
92%
90%
avg. rp
0.005469
0.004432
0.003583
0.003919
stdev rp
0.004258
0.003228
0.002532
0.002947
avg. hw
0.005576
0.021964
0.051527
0.105334
stdev hw
0.003185
0.012968
0.029221
0.058994
avg. sp
5491879
1363070
1363070
1363070
stdev sp
1541480
297519
297519
297519
phase. Users should then investigate if the distribution is continuous at
this particular quantile. Again, all quantile estimators satisfy the precision
requirement of Eq. (4.5), and c.i. coverages are above or close to the spec-
iÔ¨Åed 90%. The average c.i. half-width of the 0.90 quantiles of the M/M/1
delay in queue is much larger than the other quantiles since the quantile
under estimation has a larger value.
The summary of our experimental results of the M/M/2 delay-in-queue
process is listed in Table 6.6. If œÅ < 1, the theoretical steady-state dis-
tribution of this M/M/2 queuing process is F(x) ‚Üí1 ‚àí9e‚àíx/14, where
x ‚â•0. Therefore, for this M/M/2 process quantiles less than 5/14 are not
attainable, we estimated 0.40 quantile instead. All estimators satisfy the
probability coverage requirements. Moreover, the percentages of the c.i.‚Äôs
that cover the true quantiles are close to the speciÔ¨Åed nominal value of 90%.
The sample size determined by the QI procedure is roughly the same for
the waiting-time of M/M/2 and the M/M/1 delay in queue with the same
traÔ¨Éc intensity of œÅ = 0.75. However, the c.i. coverage of M/M/2 delay in
queue is not as good as M/M/1. Again, we believe this is caused by the
half-width being too small.
6.3.3
A Practical Application
In this section, we propose a new approach to estimate the critical constant
for [Rinott (1978)] procedure.
For l = 1, 2, . . . , k, let zil and œáil denote the variables having the stan-
dard normal distribution and the œá2 distribution with n0 ‚àí1 d.f., respec-

126
Crafts of Simulation Programming
Table 6.6
Coverage of 90% conÔ¨Ådence quantile estima-
tors of the M/M/2 delay-in-queue process with œÅ = 0.75
Precision
TraÔ¨Éc Intensity œÅ
0.75
p
0.40
0.50
0.75
0.90
quantile
0.068993
0.251314
0.944462
1.86075
cover p
100%
100%
100%
100%
coverage
89%
90%
88%
84%
avg. rp
0.006573
0.002424
0.004137
0.004463
stdev rp
0.005017
0.001859
0.002950
0.003152
avg. hw
0.001627
0.002265
0.012300
0.024968
stdev hw
0.000828
0.001122
0.006728
0.014283
avg. sp
7925708
1326701
1326701
1326701
stdev sp
2921767
285481
285481
285481
tively. Let
Til =
zil

œáil/(n0 ‚àí1)
.
The variables Til‚Äôs are i.i.d. t-distributed with n0 ‚àí1 d.f. The selection
procedure of [Dudewicz and Dalal (1975)] is derived from the equation
P(CS) ‚â•P[Ti1 < Til + h1, l = 2, 3, . . . , k] =
 ‚àû
‚àí‚àû
F k‚àí1(t + h1)f(t)dt,
where f and F are the pdf and cdf of the t distribution with n0 ‚àí1 d.f.,
respectively. This equality holds exactly under the LFC.
We follow the steps of Proposition 3 of [Rinott
(1978)] to show that
under the LFC
P(CS)
= P[Ti1 < Til + hr, l = 2, 3, . . . , k]
= P[
Ti1 ‚àíTil

(n0 ‚àí1)(1/œái1 + 1/œáil)
<
hr

(n0 ‚àí1)(1/œái1 + 1/œáil)
, l = 2, 3, . . . , k]
‚â•Œ¶(
hr

(n0 ‚àí1)(1/œái1 + 1/œáil)
, l = 2, 3, . . . , k)
= P[zil

(n0 ‚àí1)(1/œái1 + 1/œáil) < hr, l = 2, 3, . . . , k].
The inequality follows Slepian‚Äôs inequality because variables
Z‚Ä≤
il =
Ti1 ‚àíTil

(n0 ‚àí1)(1/œái1 + 1/œáil)
‚àºN(0, 1)
and Z‚Ä≤
il‚Äôs are correlated. The last equality follows since zil for l = 2, 3, . . . , k
are i.i.d. N(0, 1) variables.
Consequently, the value of h1 such that

Histogram and Quasi-Independent Procedure
127
 ‚àû
‚àí‚àûF k‚àí1(t + h1)f(t)dt = P ‚àóis no larger than the value of hr such
that P[zil

(n0 ‚àí1)(1/œái1 + 1/œáil) < hr, l = 2, 3, . . . , k] = P ‚àó. That is,
h1 ‚â§hr.
The hr values can be reduced for [Rinott (1978)] procedure by taking
into account the correlation between Z‚Ä≤
il‚Äôs.
Let Œæil ‚àºN(0, 0.5) for l =
1, 2, . . . , k and let Z‚Ä≤‚Ä≤
il = Œæil ‚àíŒæi1 for l = 2, 3, . . . , k. Note that Z‚Ä≤‚Ä≤
il ‚àºN(0, 1)
and correlates with the correlation coeÔ¨Écient 1/2, i.e.,
Cov(Z‚Ä≤‚Ä≤
ia, Z‚Ä≤‚Ä≤
ib) = (0.5
0.5 + 1)‚àí1/2(0.5
0.5 + 1)‚àí1/2, a Ã∏= b.
Let Œ© = maxk
l=2 Z‚Ä≤‚Ä≤
il

(n0 ‚àí1)(1/œái1 + 1/œáil). The value of hr is then com-
puted such that
P[Œ© < hr] = P ‚àó.
(6.12)
The estimated hr values are available in [Chen (2011)]. These hr values
are smaller than the corresponding values listed in [Wilcox
(1984)] and
are slightly larger than the corresponding h1 values for the procedure of
[Dudewicz and Dalal (1975)].
Without the knowledge of œÉ2
il‚Äôs, letting Œæil ‚àºN(0, 0.5) for l = 1, 2, . . . , k
is a suitable alternative. If Œæi1 ‚àºN(0, 0) and Œæil ‚àºN(0, 1) for l = 2, 3, . . . , k,
then Z‚Ä≤‚Ä≤
il‚Äôs are independent and the corresponding hr values are the same
as those listed in [Wilcox (1984)]. On the other hand, if Œæi1 ‚àºN(0, 1) and
Œæil ‚àºN(0, 0) for l = 2, 3, . . . , k, then Z‚Ä≤‚Ä≤
il‚Äôs degenerate to a single variable
and the corresponding hr values will be smaller than the true values.
6.4
Summary
We have presented an algorithm for estimating the histogram of a stationary
process. Some histogram estimates require more observations than others
before the asymptotic approximation becomes valid. The proposed quasi-
independent algorithm works well in determining the required simulation
run length for the asymptotic approximation to become valid. The QI pro-
cedure estimates the required sample size based entirely on data and does
not require any user intervention. Moreover, the QI procedure processes
each observation only once and does not require storing the entire output
sequence. Since the procedure stops when the QI subsequence appears to
be independent, the procedure obtains high precision and small half-width
with long simulation run length by default.

128
Crafts of Simulation Programming
The histogram-approximation algorithm computes quantiles only at cer-
tain grid points and generates an empirical distribution (histogram) of the
output sequence, which can provide valuable insights of the underlying
stochastic process. Because the QI procedure does not need to read the
output sequence repeatedly, it is an online algorithm and the storage re-
quirement is minimal.
The main advantage of the approach is that by
using a straightforward runs test to determine the simulation run length
and using natural estimators to construct the c.i., we can apply classical
statistical techniques directly and do not require more advanced statistical
theory, thus making it easy to understand and simple to implement.
This algorithm has been implemented in simulation software packages,
e.g., the BigHouse Simulator [Meisner et al. (2012)]. Because a histogram
is constructed as an empirical distribution of the underlying process, it is
possible to estimate multiple quantiles simultaneously,and estimate other
characteristics of the distribution, such as a proportion, or derivative [Chen
(2003)], under the same framework. Preliminary experimental results indi-
cate that the natural estimators obtained based on the empirical distribu-
tion are fairly accurate.

Chapter 7
Metamodels
This chapter reviews metamodels, investigates the accuracy of the Ô¨Åtted
quantile curves (response surface) and how to construct non-functional-
form metamodels with a set of histograms at certain design points.
7.1
Introduction
In many cases, users are interested in the system responses under diÔ¨Äerent
design points (input combinations, scenarios).
Thus, a series of design
points need to be evaluated, i.e., we need to construct a metamodel (or a
response surface). For example, we are interested in the mean waiting time
(or system time) of queuing systems with diÔ¨Äerent traÔ¨Éc intensities. The
metamodel can be ‚Äúused as a proxy for the full-blown simulation itself in
order to get at least a rough idea of what would happen for a large number
of design points‚Äù.
The purpose of constructing metamodels is to estimate or approximate
the response surface. We could then use the metamodel to learn how the
response surface would behave over various input-parameter combinations,
such as output sensitivity.
This approach is ‚Äúhelpful when the simula-
tion is very large and costly, precluding exploration of all but a few input-
parameter combinations‚Äù. Note that the metamodel is designed to provide
the overall tendency of performance measures rather than accurate esti-
mates at all input-parameter combinations. These metamodels can also be
used for visualization. Graphical representations of metamodels are use-
ful for providing a simple and easy form to communicate the input-output
relationship.
Traditionally, a metamodel is speciÔ¨Åed to be a standard regression model
and is described by a formula obtained through regression on several cho-
129

130
Crafts of Simulation Programming
sen design points, see, e.g., [Turner et al. (2013)]. However, we usually do
not know what functional form to specify for the regression terms. Fur-
thermore, the value at design points obtained via the metamodel (formula)
may be diÔ¨Äerent from the original observed value at the design point. On
the other hand, Kriging [van Beers and Kleijnen
(2008)] is a metamod-
eling methodology (originally developed for deterministic simulation) that
estimates the value at non-design points by interpolation. Since no regres-
sion is performed to obtain a formula, for selected design points (input
combinations), the Kriging metamodel simply returns the observed values.
In non-deterministic simulation, the observed value at design points may
be the average of several replications. For a non-design point, the inter-
polated value is a weighted average of the values observed at all design
points. [Jones et al. (1998)] detail the diÔ¨Äerence between regression-based
metamodels and interpolation-based metamodels.
Kriging requires a correlation function (i.e., the spatial dependence) to
compute the weights, which depend on the distances between the input
combination to be estimated and the existing input combinations already
observed. Kriging assumes that the closer the input combinations are, the
stronger positively correlated the responses are. The choice of correlation
function should be motivated by the underlying system we want to model.
However, the (true) correlation function is unknown and both its type and
parameter values must be estimated. Furthermore, some of the weights
may be negative. This correlation function is similar to the kernel function
used to smooth estimates when estimating density.
See [van Beers and
Kleijnen (2008)] for an overview of Kriging.
Design of experiments can be used during the metamodel construction
to improve the construction process and quality of the metamodel.
An
important issue in metamodeling is how to select the design points and
estimate their responses to which the metamodel is Ô¨Åtted. We investigate
the issue of Ô¨Ånding both the number and the placement of design points
to achieve the required precision. To account for the stochastic nature of
the output, two additional metamodels are constructed: lower and upper
bounds. For non-design points, the interpolated value (at each dimension)
is a linear interpolation of two bounding design points, not a weighted
average of all design points. Hence, the value at non-design points can be
estimated inexpensively. This approach is a special case of the correlation
function LIN in the Matlab Kriging toolbox developed by [Lophaven et al.
(2002)].

Metamodels
131
7.2
Constructing Metamodels of Quantiles
A simulation model can be thought of as a function that turns input param-
eters into output performance measures. For example, if we use simulation
to estimate xp (the p quantile) of the M/M/1 queuing process with certain
traÔ¨Éc intensity 0 < œÅ < 1, we could in principle write
xp = F(p, œÅ)
for some function F that is stochastic and unknown and we use simulation
to evaluate F for numerical input values of p and œÅ. We are interested in
the p quantile xp for any point in the two-dimensional region deÔ¨Åned by
the proportion p and the traÔ¨Éc intensity œÅ. Both p and œÅ are continuous, it
is impossible to evaluate all combinations of p and œÅ numerically. In some
other cases, it maybe possible to evaluate all input combination numerically;
but most likely is not practical to do so. Instead, we evaluate the function at
certain input combinations and Ô¨Åt the results to some curves. The response
of other input combinations is then approximated via the Ô¨Åtted curves
(surfaces).
It is nice to have a single formula to represent the responses of a sys-
tem, but the formula is generally unknown and is likely complex. Fitting
the grid points to a formula also introduces error into the estimates. In
the approach, instead of obtaining a formula or a regression model, we
treat the collection of grid points themselves and the set of Ô¨Åtted curves
as metamodels. Consequently, the metamodel is not described by a single
(simple) formula. We call the metamodel of this kind the non-functional-
form metamodel. For example, there is no closed form of the cdf of the
standard normal distribution and the quantiles are customarily listed in
tables instead of by a regression model. Without the help of a calculator,
the quantile value can not be easily computed from the formula.
A metamodel will have greater accuracy and precision when constructed
with a larger set of input-parameter combinations. On the other hand, a
larger set of input-parameter combinations requires more simulation eÔ¨Äorts.
Furthermore, the required number of input-parameter combinations of con-
structing metamodels to achieve the pre-speciÔ¨Åed accuracy and precision
depends on the underlying systems.
We can sequentially determine the
required number of input-parameter combinations by observing the Ô¨Åtted
curves. Even though both p and œÅ are input parameters, only œÅ is desig-
nated as a design point during the metamodel construction; since œÅ = Œª/ŒΩ
can be controlled by setting the values of arrival rate Œª or server rate ŒΩ. The

132
Crafts of Simulation Programming
value at non-design points will be estimated via interpolation. Furthermore,
the randomness of the response at design points is described by conÔ¨Ådence
intervals. Consequently, three response surfaces will be constructed: the
lower bound, the expected value, and the upper bound. It is likely that
the form and complexity of the metamodel vary substantially for diÔ¨Äerent
systems. Thus, the metamodeling procedure needs to include a scheme for
model selection: given the simulation data, obtain a metamodel that is of
the least complexity but adequate to characterize the underlying response
surface.
Assuming the range of interest of the (controllable) parameter œÅ is
[œÅL, œÅU], we initially simulate the system with Ô¨Åve evenly spaced design
points, e.g., œÅ (say œÅ1 = œÅL < œÅ2 < ¬∑ ¬∑ ¬∑ < œÅ5 = œÅU). Let Fi be the simulated
(estimated) response of input combination (p, œÅi) for some selected p. We
recommend the selected p should include at least Ô¨Åve evenly spaced points
between the range of interest. We Ô¨Åt curves C1 and C2, respectively, to
the three ((p, œÅ1), (p, œÅ3), (p, œÅ5)) and Ô¨Åve evenly spaced input-parameter
combinations and their corresponding responses Fi. We then use curve C1
to estimate the response of (p, œÅ2) and (p, œÅ4) and let ÀÜF2 and ÀÜF4 be the
estimated responses from C1. If the relative precision
| ÀÜFi ‚àíFi|/Fi < 4Œ≥
(7.1)
is satisÔ¨Åed for all i, then the procedure returns the collection of all the
histograms as the non-functional-form metamodel; otherwise design points
œÅj1 = (œÅi‚àí1 +œÅi)/2 and œÅj2 = (œÅi +œÅi+1)/2 will be added for each i that the
precision requirement is not satisÔ¨Åed. Note that the value Œ≥ is the intended
relative precision of quantile estimates, i.e.,
|¬ØÀÜxp ‚àíxp|
xp
‚â§Œ≥,
where ¬ØÀÜxp is the Ô¨Ånal point estimator of xp, see Eq. (6.7). Using the value
4Œ≥ in Eq. (7.1) is somewhat arbitrary. The rationale are that the distance
between the design points œÅi‚àí1 and œÅi is half of the distance (between œÅi‚àí1
and œÅi+1) used to estimate ÀÜFi and the angle of the line between design points
is also reduced by no less than half. The step of adding more design points
will be performed repeatedly until the speciÔ¨Åed precision on the selected
quantile estimators is achieved. Thus, the Ô¨Ånal number of design points is
not known until the simulation is complete. The preliminary experimental
results indicate that the required design points to achieve the speciÔ¨Åed
precision is not excessive.

Metamodels
133
Table 7.1
Chosen design points
Procedure
TraÔ¨Éc Intensity
LI
0.1
0.3
0.4
0.5
0.6
0.7
0.75
0.8
0.85
0.9
V&K
0.1
0.3
0.5
0.7
0.8
0.85
0.875
0.8875
0.89375
0.9
7.3
Constructing Quantile ConÔ¨Ådence Interval
With a given design point (e.g., traÔ¨Éc intensity œÅ), we use the Histogram
Approximation (HA) procedure discussed in Chapter 6 to construct his-
tograms and estimate quantiles. We construct the curve of quantiles, the
curves of the lower and upper conÔ¨Ådence limits of quantiles at the design
points. The quantile point estimator and c.i. at non-design points can then
be obtained via linear interpolation. Let xp and yp, respectively, denote the
p quantile estimators of the time in system of the M/M/1 queuing process
with traÔ¨Éc intensity œÅ1 and œÅ2. Then the p quantile estimator with traÔ¨Éc
intensity œÅ1 < œÅ < œÅ2 is estimated by linear interpolation, i.e.,
zp = xp + œÅ ‚àíœÅ1
œÅ2 ‚àíœÅ1
(yp ‚àíxp).
The conÔ¨Ådence limits of zp are computed in the same manner. [Chen and
Li (2014)] discuss how to apply this procedure in higher dimensions.
7.4
Empirical experiments
In this section, we present some empirical results obtained from simulations
using non-functional-form metamodels to estimate quantiles.
7.4.1
Choosing the Design Points
In order to compare our customized design with other approaches, we per-
form a similar experiment as in [van Beers and Kleijnen (2008)]. We want
to construct a metamodel of the waiting-time of the M/M/1 queuing sys-
tem for traÔ¨Éc intensity 0.1 ‚â§œÅ ‚â§0.9. We begin with Ô¨Åve evenly spaced
traÔ¨Éc intensity (i.e., pilot runs) and sequentially increase the number of
design points until the number of design points reach 10, the same number
as in [van Beers and Kleijnen (2008)] for comparison. In the experiment,
each response is the true response instead of the simulated response.
Let Œ≥i = | ÀÜFi ‚àíFi|/Fi. To adapt to this stopping rule, we add the design
point œÅj1 = (œÅj‚àí1 + œÅj)/2 or œÅj2 = (œÅj + œÅj+1)/2 when Œ≥j = max Œ≥i. Table

134
Crafts of Simulation Programming
Fig. 7.1
Plots of the linear interpolated versus true M/M/1 system time
7.1 lists the Ô¨Ånal 10 design points. The LI and V&K rows, respectively,
are for the linear interpolated and the procedure of [van Beers and Klei-
jnen
(2008)]. The LI approach places design point more evenly because
the strategy basically aims to minimize the maximum of Œ≥i and selects rel-
atively few design points in the area that the responses change linearly as
the input changes. On the other hand, the approach of V&K aims to min-
imize the integrated mean square error and is computationally expensive.
Consequently, they place all the additional design points between traÔ¨Éc
intensities 0.7 and 0.9, where the responses have larger values and larger
variances. Figure 7.1 displays the true responses and the linear-interpolated
responses.
7.4.2
Estimating Quantiles of Moving-Average and Autore-
gressive Processes via Non-functional-form Meta-
models
In this experiment, we present some empirical results of estimating quantiles
of the Ô¨Årst-order moving-average MA1(Œ∏) and the Ô¨Årst-order autoregressive
AR1(œï) processes obtained from non-functional-form metamodels.

Metamodels
135
The MA1(Œ∏) process is generated by the sequence
Xi = Œº + œµi + Œ∏œµi‚àí1 for i = 1, 2, . . . .
The AR1(œï) process is generated by the recurrence relation
Xi = Œº + œï(Xi‚àí1 ‚àíŒº) + œµi for i = 1, 2, . . . ,
where X0 is speciÔ¨Åed as a random variate drawn from the steady-state
distribution. In both process we set Œº to 2 and œµi to be i.i.d. N(0, 1).
The values of the required parameters of determining the simulation run
length (i.e., Eq. (4.6)) are p = 0.5, œµ = 0.01, and Œ±1 = 0.05. The conÔ¨Ådence
level of the test of independence is set to 0.952. In these experiments, the
point quantile estimate (i.e., ¬ØÀÜxp) at design points is the average of R = 3
independent replications.
We use relative precision (i.e., Eq. (7.1) with
Œ≥ = 5%) to determine the number of design points.
We are interested in quantiles of these processes with correlation coeÔ¨É-
cient between 0.7 and 0.95. Since at least Ô¨Åve design points should be used,
the initial diÔ¨Äerence between design points is 0.0625 (i.e., (0.95-0.7)/4).
We construct histograms at the following design points (i.e., the correla-
tion coeÔ¨Écients œï and Œ∏): 0.7, 0.7625, 0.825, 0.8875, and 0.95 and estimate
0.85, 0.9, and 0.95 quantiles at the following correlation coeÔ¨Écients: 0.7,
0.75, 0.8, 0.85, 0.9, and 0.95.
No additional design points are included
because the relative precision of quantile estimates from the histogram at
correlation coeÔ¨Écient 0.7625 (0.8875) and from linear interpolation with
histograms at correlation coeÔ¨Écients 0.7 and 0.825 (0.825 and 0.95) are
within the speciÔ¨Åed relative precision 4Œ≥ = 20%, see Section 7.2.
The
quantile estimates at 0.7 and 0.95 correlation coeÔ¨Écients are obtained di-
rectly from the histograms.
The quantile estimates at other correlation
coeÔ¨Écients are obtained through linear interpolation.
The left-hand-side graphs in Figure 7.2 show the quantile-estimation
results for MA1. They are relative deviation plots with the p quantile being
0.85, 0.90, and 0.95. For these graphs, the x-axis represents the correlation
coeÔ¨Écient Œ∏, the y-axis represents the relative deviation, i.e.,
¬ØÀÜxp ‚àíxp
xp
√ó 100%,
and every point (p, xp) in the graph represents the observed relative devi-
ation at certain Œ∏ from one of the 100 independent simulation runs. The
right-hand-side graphs in Figure 7.2 show the quantile-estimation results
for AR1.

136
Crafts of Simulation Programming
For the MA1 process, the p quantile changes approximately linearly as
Œ∏ changes, hence, the observed relative deviations are roughly the same
at all Œ∏. The relative precisions are all within 1%. For the AR1 process,
the p quantile changes non-linearly as œï changes, likely concave upward.
The accuracy of quantile estimates obtained from the histograms at the
design points (i.e., at œï = 0.7 and 0.95) is high indicating the performance
of the HA procedure is good.
The accuracy of the estimates degraded
when estimating quantile of input-parameter combinations that are not de-
sign points. This is not unexpected because using interpolation introduces
errors. Nevertheless, the relative precisions are well within the speciÔ¨Åed
Œ≥ = 5%.
7.4.3
Estimating Quantiles of Queuing Systems via Non-
functional-form Metamodels
We estimate the steady-state waiting time in system quantile of the M/M/1
queuing systems. The values of the required parameters of determining the
simulation run length are as set in the previous experiments.
We construct histograms at 0.7, 0.7625, 0.825, 0.8563 (‚âà(0.825 +
0.8875)/2), 0.8875, 0.9188 (‚âà(0.8875 + 0.95)/2), and 0.95 traÔ¨Éc inten-
sities (i.e., design points) and estimate 0.85, 0.9, and 0.95 quantiles at 0.7,
0.75, 0.8, 0.85, 0.9, and 0.95 traÔ¨Éc intensities. TraÔ¨Éc intensities 0.8563
and 0.9188 are included in the design points because the relative precision
of quantile estimates from the histogram at traÔ¨Éc intensity of 0.8875 and
from linear interpolation with histograms at traÔ¨Éc intensities 0.825 and
0.925 is larger then the speciÔ¨Åed relative precision 4Œ≥ = 20%, see Section
7.2.
Two types of plots were made to display graphically the 100 realizations
of each quantile estimator: 1) relative deviation plots; and 2) absolute de-
viation plots, in which quantile estimates are plotted around their true
values. The left-hand-side and right-hand-side graphs on Figure 7.3 show
the relative plots and the absolute plots, respectively. In the absolute devi-
ation plots, the solid curve represents a piecewise linear version of the true
quantile curve across various traÔ¨Éc intensities and the quantile estimates
are plotted as a point. Note that a piecewise linear version of quantile esti-
mates can provide information regarding the general tendency of quantiles
and a rough idea of quantile sensitivity as the traÔ¨Éc intensity changes. For
the M/M/1 queuing process, the quantile estimates obtained through linear
interpolation are biased high, which can be explained by the curves in the

Metamodels
137
Table 7.2
Estimated 90% conÔ¨Ådence interval half width and coverage for M/M/1
0.85 Quantile
0.90 Quantile
0.95 Quantile
œÅ
Avg HW
Coverage
Avg Hw
Coverage
Avg HW
Coverage
0.70
0.115
0.86
0.153
0.89
0.235
0.88
0.75
0.149
0.89
0.188
0.88
0.291
0.95
0.80
0.185
0.29
0.244
0.48
0.371
0.58
0.85
0.241
0.90
0.319
0.89
0.481
0.88
0.90
0.347
0.21
0.463
0.30
0.747
0.42
0.95
0.716
0.90
0.943
0.90
1.538
0.92
right-hand-side graphs on Figure 7.5. Given a Ô¨Åxed p, the p quantile vs.
œÅ curve is concave upward, i.e., given a Ô¨Åxed p F(p, œÅ) is a convex func-
tion. As the value of œÅ deviates further away from the design points, e.g.,
œÅ = 0.80 and 0.90, the accuracy of the estimates gets worse. Note that the
(absolute or relative) precision can be improved by increasing the number
of design points. The relative precision of all estimates from the histogram
approximated non-functional-form metamodel are well within the speciÔ¨Åed
Œ≥ = 5%.
A c.i. is constructed for each quantile with R = 3 independent quantile
estimators. Table 7.2 shows the results. The column labeled ‚ÄúAvg HW‚Äù is
the average of the 90% c.i. half width calculated from the 100 realizations
of the quantile estimator. The ‚ÄúCoverage‚Äù column lists the proportion of
these 100 c.i.‚Äôs that cover the true quantile. The c.i. coverage at the design
points (i.e., œÅ = 0.70 and 0.95) are around the nominal value of 0.90, which
indicates the estimated c.i. half widths are also accurate. However, the c.i.
coverages of œÅ that are further away from the design points (e.g., œÅ = 0.80
and 0.90) are less than the nominal value. This is expected since the point
estimators are bias high; while the half widths are about the right length.
We don‚Äôt think this is a major drawback of the procedure.
The main
purpose of the metamodel is to gain the overall tendency of quantiles of the
system under study and is not to obtain accurate quantile c.i.‚Äôs throughout
the entire traÔ¨Éc-intensity range of interest. Note that accurate quantile
c.i.‚Äôs can be obtained by designating the traÔ¨Éc intensity of interest œÅ as
a design point. Furthermore, with the help of the quantile plots, we are
able to predict approximately the accuracy of the interpolated quantile
estimates.
If the c.i. coverage at non-design points is a concern, a conservative
adjustment can be used to increase the coverage of the c.i. estimated via
interpolation. Let Œ≥i = | ÀÜFi ‚àíFi|/Fi. When using interpolation with curves

138
Crafts of Simulation Programming
Ci‚àí1 and Ci or Ci and Ci+1 to estimate c.i., we set
c.i. =

(¬ØÀÜxp ‚àíw ‚àíŒ≥i¬ØÀÜxp, ¬ØÀÜxp + w) ÀÜFi > Fi
(¬ØÀÜxp ‚àíw, ¬ØÀÜxp + w + Œ≥i¬ØÀÜxp) ÀÜFi < Fi.
While this adjusted c.i. will increase the coverage to be greater than the
nominal value, the range of the c.i. will be much larger.
7.5
Summary
Non-functional-form metamodels of providing overall tendency of quantiles
can be constructed with a set of carefully selected histograms. The meta-
model can be used as a proxy for the full-blown simulation itself in order
to get at least a rough idea of what would happen for a large number of
input-parameter combinations. Estimates obtained via linear interpolation
of grid points of metamodels are as good as those obtained via other sophis-
ticated methods when the structure of the underlying systems is unknown.

Metamodels
139
Fig. 7.2
Plots of the quantile estimates for MA1 and AR1 via metamodels

140
Crafts of Simulation Programming
Fig. 7.3
Relative and absolute plots of the quantile estimates for M/M/1 via metamod-
els

Chapter 8
Density Estimation
Simulation studies have been used to investigate the characteristics of sys-
tems, for example the mean and the variance of certain system performance
measures like waiting times in queue. However, without knowledge of the
underlying distribution, the mean and the variance provide only limited in-
formation. On the other hand, the pdf f gives a natural description of the
distribution of a stationary continuous output random variable X produced
by a simulation and reveals many characteristics of the underlying distri-
butions beyond just the mean and variance (for instance, tail probabilities
and quantiles).
Density estimation from observed data is a useful tool for data explo-
ration.
For example, [Silverman
(1986)] points out that ‚Äúdensity esti-
mates are ideal for presentation of data to provide explanation and illus-
tration of conclusions, since they are fairly easily comprehensible to non-
mathematicians.‚Äù One approach to density estimation is parametric, as-
suming that the data are drawn from a known parametric family of distri-
butions, for example the normal distribution with mean Œº and variance œÉ2.
The density f of the underlying data is then estimated simply by estimat-
ing the values of Œº and œÉ2 from the data and substituting these estimates
into the formula for the normal density. Another approach is nonparamet-
ric, where less rigid assumptions are made about the distribution of the
observed data. We consider the nonparametric approach since it is more
robust for the wide variety of data behavior possible in simulation output.
Furthermore, the procedure is data-based, i.e., it can be embodied in a soft-
ware package whose input is the simulation output data (X1, . . . , Xn), and
whose output is the density estimate. Several diÔ¨Äerent approaches have re-
ceived extensive treatment; see [Silverman (1986); Scott and Sain (2004)]
and the references therein.
141

142
Crafts of Simulation Programming
The most widely used density estimator is the histogram, basically a
graphical estimate of the underlying probability density function that re-
veals all the essential distributional features of a simulation output random
variable, such as skewness and multi-modality. Hence, a histogram is often
used in the informal investigation of the properties of a given set of data.
A steady-state distribution can be constructed with a properly selected
set of quantiles. For both i.i.d. and œÜ-mixing sequences, sample quantiles
will be asymptotically unbiased if certain conditions are satisÔ¨Åed; see [Sen
(1972)]. With a given set of valid quantile estimates, estimating the density
is more complicated than estimating the cumulative distribution because
direct density estimates by central Ô¨Ånite diÔ¨Äerences are more sensitive to
the bin (band) width of the underlying histogram. This chapter investigates
the performance of estimating the density of a simulation output random
variable with a dynamically determined bandwidth.
8.1
Theoretical Basis
In this section, we review the deÔ¨Ånition of probability density functions and
the basis of density estimation. From the deÔ¨Ånition of a probability density,
if the random variable X has density f, then
f(x) = lim
b‚Üí0
1
2bP(x ‚àíb < X < x + b) = lim
b‚Üí0
F(x + b) ‚àíF(x ‚àíb)
2b
.
Here b > 0 is a real number.
8.1.1
Empirical Distribution Functions
We often want to use the observed data themselves to specify a distribution,
called an empirical distribution, from which random values are generated
during the simulation. Consider an i.i.d. sequence Xi for i = 1, 2, ¬∑ ¬∑ ¬∑ , n with
distribution function F. One of the simplest and most important functions
of the order statistics is the sample cumulative distribution function Fn,
which can be constructed by placing a mass 1/n at each observation Xi.
Hence, Fn may be represented as
Fn(x) = 1
n
n

i=1
I(Xi ‚â§x), ‚àí‚àû< x < ‚àû.
Here I(¬∑) is the indicator function. That is, Fn(x) is the fraction of values
in a sample of n values not exceeding x.

Density Estimation
143
Note that
Fn(x) =
‚éß
‚é®
‚é©
0
x < x[1]
i
n x[i] ‚â§x < x[i+1]
1
x[n] ‚â§x.
The empirical cdf Fn(x) for all real x, represents the proportion of sample
values that do not exceed x. For each Ô¨Åxed x, the strong law of large number
implies that Fn(x) ‚ÜíF(x) asymptotically (as the sample size n goes to ‚àû).
The empirical distribution can then be used as a surrogate of the unknown
true distribution to estimate system performance. Furthermore, the density
function f = F ‚Ä≤. Hence, we can use the derivative of Fn to estimate f. Since
Fn is discontinuous and thus not diÔ¨Äerentiable, we consider the central Ô¨Ånite
diÔ¨Äerence
fn(x) = Fn(x + b) ‚àíFn(x ‚àíb)
2b
=
n
i=1 I(x ‚àíb < Xi ‚â§x + b)
2nb
as an estimate of the density function. The quantity 2b is referred to as the
bandwidth. A graphical representation of fn(x) is called a histogram. Figure
8.1 shows an empirical histogram vs the density curve. The smoothness of
the histogram is controlled by the parameter b. Furthermore, the starting
point of each bin edge can produce diÔ¨Äerent impressions of the shape, and
hence a diÔ¨Äerent histogram.
Fig. 8.1
Histogram vs density curve, bandwidth=0.5

144
Crafts of Simulation Programming
8.1.2
The Density Estimator
A natural estimator by a histogram ÀÜfbn(x) of the density is given by choos-
ing a small real number bn (which is a realization of b and depends on the
sample size n) and setting
ÀÜfbn(x) =
1
2nbn
[no. of X1, . . . , Xn falling in (x ‚àíbn, x + bn)]
=
1
nbn
n

i=1
W
x ‚àíXi
bn

.
Here the weight function W(¬∑) = I(¬∑)/2 and I(¬∑) is the indicator function
for the interval (‚àí1, 1).
Let
Ii(bn, x) =
 1 if |x ‚àíXi| ‚â§bn
0 otherwise.
The estimator ÀÜfbn(x) is based on a transformation of the output sequence
{Xi} to the sequence {Ii(bn, x)}, i = 1, 2, . . . , n:
ÀÜfbn(x) =
1
2nbn
n

i=1
Ii(bn, x).
For data that are i.i.d., the following properties of Ii(bn, x) are well known
[Hogg et al. (2012)]:
E(Ii(bn, x)) = p and Var(Ii(bn, x)) = p(1 ‚àíp),
where p = P(‚àíbn < x ‚àíX < bn). Since ÀÜfbn(x) is based on the mean
of the random variable Ii(bn, x), we can use any method developed for
estimating the variance of the mean to estimate Var( ÀÜfbn(x)). By elementary
manipulations, for each x,
E( ÀÜfbn(x)) =
1
nbn
n

i=1
E

W
x ‚àíXi
bn

=
1
2bn

I
x ‚àíy
bn

f(y)dy
=
p
2bn

Density Estimation
145
and
Var( ÀÜfbn(x)) =
n
(nbn)2 Var

W
x ‚àíXi
bn

=
1
4nb2n
Var

I
x ‚àíXi
bn

= p(1 ‚àíp)
4nb2n
.
Note that ÀÜfbn(x) is a variation of a binomial distribution. It follows from
the deÔ¨Ånition that ÀÜfbn is not a continuous function, but has jumps at the
points Xi ¬± bn and has zero derivative everywhere else.
This gives the
estimate a somewhat ragged character.
To overcome the diÔ¨Éculties stemming from the ragged character of
ÀÜfbn(x), one can replace the weight function by a kernel function K,
which satisÔ¨Åes the condition
 ‚àû
‚àí‚àûK(x)dx = 1. For simplicity, the ker-
nel K usually is a symmetric function satisfying

xK(x)dx = 0, and

x2K(x)dx = k2 Ã∏= 0; an example is the normal density.
The kernel
density estimator with kernel K is deÔ¨Åned by
ÀÜfK,bn(x) =
1
nbn
n

i=1
K
x ‚àíXi
bn

.
The weight function W of the histogram density estimate satisÔ¨Åes the con-
ditions for a Kernel function. Consequently, the histogram density estimate
is just a special case of the kernel density estimate with the kernel func-
tion K(X) = W(X) = I(X)/2. For the remainder of this chapter, we will
refer the histogram density estimate as a kernel density estimate with the
indicator function as the kernel. For a detailed discussion of kernel density
estimate, see [Silverman (1986)].
It can be shown that
E( ÀÜfK,bn(x)) = 1
bn

K
x ‚àíy
bn

f(y)dy;
Var( ÀÜfK,bn(x)) = 1
n

1
b2n

K
x ‚àíy
bn
2
f(y)dy ‚àí

E( ÀÜfK,bn(x))
2

‚âà
1
nbn
f(x)

K(y)2dy,
and
biasbn(x) = E( ÀÜfK,bn(x) ‚àíf(x))
= 1
2b2
nf ‚Ä≤‚Ä≤(x)k2 + higher-order terms in bn.

146
Crafts of Simulation Programming
From this we can deduce that ÀÜfK,bn(x) is an asymptotically unbiased esti-
mator of the density f(x), because biasbn(x) ‚Üí0 when bn ‚Üí0 as n ‚Üí0.
See [Silverman (1986)] for details.
The approximation of bias and variance indicates one of the fundamental
diÔ¨Éculties of density estimation. To eliminate the bias, a small value of bn
should be used, but then the variance will become large.
On the other
hand, a large value of bn will reduce the variance, but will increase the
bias. The mean square error (MSE) is widely used to evaluate the quality
of estimates and addresses the trade-oÔ¨Äbetween variance and bias. Note
that MSE = Variance + Bias2.
Furthermore, to achieve MSE ‚Üí0 as
n ‚Üí‚àû, the following two conditions must hold: bn ‚Üí0 and nbn ‚Üí‚àû.
Since the shape of the true density is of most interest, a relevant criterion
is the integrated mean squared error (IMSE) [Roseblatt (1971)].
There are other approaches of estimating density, e.g., [Golyandina et
al. (2012)] propose using Singular Spectrum Analysis to estimate both the
distribution function and the density function.
8.1.3
The Complication of Lack of Independence
The density estimator (obtained by the methods described above) would
be asymptotically unbiased provided that the observations are independent.
However, simulation output data are generally correlated and consequently
the estimator maybe biased when the sample size is not suÔ¨Éciently large.
Hence, to estimate the density of such a stochastic processes, the proce-
dures need to determine the sample sizes dynamically to ensure that the
density estimator is unbiased. That is, we assume that the simulation out-
put sequence satisÔ¨Åes the œÜ-mixing conditions. Furthermore, we assume
that the underlying process is stationary; i.e., the joint distribution of the
Xi‚Äôs is insensitive to time shifts (in a simulation context, this would mean
that the model has been adequately ‚Äúwarmed up‚Äù).
8.2
An Implementation
This section presents a procedure to compute the point density estimator
via the histogram by (four-point) Lagrange interpolation [Knuth (1998)].
Let gi for i = 1, 2, . . . , G denote the grid points where the density esti-
mate ÀÜf(gi) is available. For some k such that gk‚àí1 < x ‚â§gk, the density

Density Estimation
147
estimator at point x can be computed as follows. Let
œñj =
4

j‚Ä≤=1,j‚Ä≤Ã∏=j
x ‚àígk+j‚Ä≤‚àí3
gk+j‚àí3 ‚àígk+j‚Ä≤‚àí3
, for j = 1, 2, 3, 4,
then ÀÜf(x) = 4
j=1 œñj ÀÜf(gk+j‚àí3). In two extreme cases, g1 < x ‚â§g2 or
gG‚àí1 < x ‚â§gG, linear interpolation will be used.
Since the procedure uses interpolation to obtain point estimates, it elim-
inates the ragged character of the histogram. Hence, the density estimates
for diÔ¨Äerent points within the same bin can have diÔ¨Äerent values. Unfor-
tunately, with interpolation the integral over the real line of the resulting
function likely will not be equal to 1.
8.2.1
Determine the Bandwidth
[Scott and Factor (1981)] point out that ‚Äúthe great potential of nonpara-
metric density estimators in data analysis is not being fully realized, primar-
ily because of the practical diÔ¨Éculty associated with choosing the smooth-
ing parameter given only data X1, X2, . . . , Xn.‚Äù There are various data-
based algorithms for determining the bandwidth bn for the kernel density
estimate. [Duin
(1976)] uses a modiÔ¨Åed maximum-likelihood approach,
[Scott et al. (1977)] use an iterative algorithm based on an asymptotically
optimal smoothing parameter. [Hearne and Wegman (1994)] use random
bandwidths. [Sheather (2004)] discussed various cross-validation and plug-
in methods. [Chan et al. (2010)] propose an approach for local bandwidth
selection. However, these algorithms are computationally intensive.
The computationally simplest method for choosing a global band-
width bn is based on rules of thumb.
[Silverman
(1986)] suggests that
the bandwidth of a general kernel estimator be bn = 0.9An‚àí1/5, where
A = min(standard deviation, inter-quartile range/1.34). For many pur-
poses this will be an adequate choice of bandwidth in terms of obtaining
a small IMSE. For others, it will be a good starting point for subsequent
Ô¨Åne tuning. Let xp be the pth sample quantile. In the procedure, we set
A = min(standard error, (x0.75‚àíx0.25)/1.34). Let x[1] and x[n], respectively,
denote the minimum and maximum of the initial n0, 2n0, or 3n0 observa-
tions, depending on the correlation of the output sequences. Note that the
sample sizes of the Ô¨Årst three iterations are, respectively, n0, 2n0, and 3n0,
where n0 is the size of the QI (Quasi-Independent Sequence) buÔ¨Äer. The
Ô¨Ånal sample size is unknown at this point and the maximum initial sample
size n = n0, 2n0, or 3n0 is used to compute bn. Hence, the bandwidth is

148
Crafts of Simulation Programming
likely to be larger than optimal for strongly correlated sequences because a
much larger sample size will be eventually allocated.
We use the following strategy to determine the bin points. The are two
categories of bins: main bins and auxiliary bins. Main bins are constructed
based on the initial observations that ‚Äúanchor‚Äù the bin of the simulation-
generated histogram, while auxiliary bins are extensions of main bins to
ensure that the bins cover future observations. The number of main bin
points is Gm = ‚åà(x[n] ‚àíx[1])/(2bn)‚åâ, and the number of auxiliary bin points
is Ga = 2‚åàŒ∂Gm‚åâ, where 0 < Œ∂ < 1. We set Œ∂ = 0.1 in the implementation.
The total number of bin points is thus G = Gm+2Ga+1. Let the beginning
indices of the main bin point (i.e., the origin) be b = Ga +1. The procedure
sets gb+i = x[1] +2ibn, for i = 0, 1, . . . , Gm +Ga ‚àí1, and gb‚àíi = x[1] ‚àí2ibn,
for i = 1, 2, . . . , Ga ‚àí1. Bin point g1 is set to ‚àí‚àûand gG is set to ‚àû.
It is straightforward to compute the histogram density estimator. The
array ni, i = 2, 3, . . . , G stores the number of observations between bin
points gi‚àí1 and gi, so the density of xi = (gi‚àí1 + gi)/2 can be estimated
by ÀÜf(xi) = (ni/n)/(gi ‚àígi‚àí1), where n = G
i=2 ni is the total number of
observations. To obtain the normal kernel estimator, the procedure needs
to read through the output sequence again. Because the Ô¨Ånal sample size
is known, the bandwidth will be re-calculated.
8.2.2
Determine the Sample Size
The asymptotic validity of the density estimate is reached as the sample
size or simulation run length gets large. However, in practical situations
simulation experiments are restricted in time and it is not known in advance
what the required simulation run length might be for the estimator to be-
come essentially unbiased. Moreover, estimating the variance of the density
estimator is needed to evaluate its precision. Therefore, a workable Ô¨Ånite
sample size must be determined dynamically for the precision required.
We use an initial sample size of n0 = 4000, which is somewhat arbitrary
but is tested in the empirical results below. For correlated sequences, the
sample size n will be replaced with N = nl. Here l will be chosen suÔ¨Éciently
large so that systematic samples that are lag-l observations apart are essen-
tially uncorrelated. This is possible because we assume that the underlying
process satisÔ¨Åes the property that the autocorrelation approaches zero as
the lag approaches inÔ¨Ånity. Consequently, the Ô¨Ånal sample size N increases
as the autocorrelation increases.
Since we need to process the sequence again to obtain the kernel estima-

Density Estimation
149
tor, we re-compute the bandwidth bn with the Ô¨Ånal sample size N and the
number of bin points with the new sample range. We need to allocate only
the main bins because the minimum and maximum are known. Further-
more, the sample error and the quantiles x0.25 and x0.75 will be estimated
through the histogram constructed while calculating the natural estimator.
That is, the variance is conservatively estimated by
S2
H =
G

i=2
max((gi‚àí1 ‚àí¬ØX(N))
2, (gi ‚àí¬ØX(N))
2)Pi.
Note that N = nl = G
i=2 ni, ¬ØX(N) = 1
N
N
j=1 Xj, and Pi = ni/N.
To estimate the error, the IMSE is approximated by
IMSE = (2bn/R)
R

r=1
G‚àí1

i=2
[ ÀÜfr(gi) ‚àíf(gi)]2,
where R is the number of replications and ÀÜfr(¬∑) is the estimate in the rth
replication. The density of g1 and gG is not included in the calculation
because they could be ‚àí‚àûand ‚àû, respectively. Furthermore, if the true
minimum m the true maximum M are known, the values gi < m or M < gi
will not be included in the calculation.
8.2.3
Density ConÔ¨Ådence Interval
Let ÀÜfr(x) denote the estimator of f(x) in the rth replication. We use
¬Øf(x) = 1
R
R

r=1
ÀÜfr(x)
as a point estimator of f(x). Assuming ¬Øf(x) has a limiting normal distri-
bution, by the central limit theorem a c.i. for f(x) using the i.i.d. ÀÜfr(x)‚Äôs
can be approximated using standard statistical procedures. That is, the
ratio
T =
¬Øf(x) ‚àíf(x)
S/
‚àö
R
would have an approximate t distribution with R ‚àí1 d.f., where
S2 =
1
R ‚àí1
R

r=1
( ÀÜfr(x) ‚àí¬Øf(x))
2
is the usual unbiased estimator of the variance of ÀÜf(x). This would then
lead to the 100(1 ‚àíŒ±)% c.i., for f(x),
¬Øf(x) ¬± tR‚àí1,1‚àíŒ±/2
S
‚àö
R
,
(8.1)

150
Crafts of Simulation Programming
where tR‚àí1,1‚àíŒ±/2 is the 1 ‚àíŒ±/2 quantile for the t distribution with R ‚àí1
d.f. (R ‚â•2).
Let the half-width w = tR‚àí1,1‚àíŒ±/2S/
‚àö
R. The Ô¨Ånal step in the proce-
dure is to determine whether the c.i. meets the user‚Äôs half-width require-
ment, a maximum absolute half-width œµ‚Ä≤ or a maximum relative fraction Œ≥
of the magnitude of the Ô¨Ånal point density estimator ¬Øf(x). If the relevant
requirement w ‚â§œµ‚Ä≤ or w ‚â§Œ≥| ¬Øf(x)| for the precision of the conÔ¨Ådence in-
terval is satisÔ¨Åed, then the procedure terminates, returns the point density
estimator ¬Øf(x), and the c.i. with half-width w. If the precision requirement
is not satisÔ¨Åed with R replications, then the procedure will increase the
number of replications to
(w/œµ‚Ä≤)2R or (w/(Œ≥ ¬Øf(x)))2R.
(8.2)
This step will be executed repeatedly until the half-width is within the
speciÔ¨Åed precision.
8.2.4
The Density-Estimation Procedure
The procedure progressively increases the simulation run length N until a
pre-determined number of systematic samples (e.g. n0) appear to be uncor-
related, as assessed by the runs test of independence. We allocate a buÔ¨Äer,
QI (Quasi-Independent), with size Bs = 3n0 to store systematic samples
yi, 1 ‚â§i ‚â§Bs. Note that lag l‚Ä≤ (=1,2,3) of the systematic samples is used
to refer to systematic samples ykl‚Ä≤+1, for k = 0, 1, 2, . . . , n0 ‚àí1 and will be
used by the runs test at various iterations.
An embedded pilot run is executed to set up the bin points. On each
iteration, the algorithm operates as follows. The simulation outputs are
funneled into bins.
The number of observations in each bin is updated
dynamically as the observation is produced during the simulation run. The
systematic samples are obtained through lag-l observations and are stored
in a buÔ¨Äer. The initial value of l is 1. Let l‚Ä≤ = 1, 2, 3 denote the lag of
the systematic samples stored in the buÔ¨Äer. If lag-l‚Ä≤ systematic samples
appear to be dependent, then the lag l is doubled every other iteration
and the process is repeated until the lag-l‚Ä≤ systematic samples appear to be
independent. The initial value of l‚Ä≤ is 0 and will be updated each iteration
by the following rule: ‚Äúif l‚Ä≤ < 3, then l‚Ä≤ = l‚Ä≤ + 1; else l‚Ä≤ = 2.‚Äù
Note that l0 is the lag used to obtain systematic samples, Œ¥ is the in-
cremental sample size, and r is the index of iterations. Each iteration r
contains two sub-iterations rA and rB. We limit the number of systematic

Density Estimation
151
samples used in the runs test to n0 = 4000.
The quasi-independent-density-estimation algorithm:
(1) Initialization: Set n0 = 4000, l0 = 1, Œ¥ = n0, and r = 0.
(2) Generate Œ¥ systematic samples, which are lag-l0 observations apart. If
r > 1, record the number of observations in each bin.
(3) If this is the initial iteration, set l‚Ä≤ = 1. If this is a rA iteration, set
l‚Ä≤ = 2. If this is a rB iteration, set l‚Ä≤ = 3.
(4) Carry out the runs test to assess whether lag-l‚Ä≤ systematic samples
appear to be uncorrleated.
(5) If the lag-l‚Ä≤ systematic samples appear to be uncorrleated, go to step
12.
(6) If r = 0, set r = r + 1 and start the 1Ath iteration by going to step 2.
(7) If this is the 2Ath iteration, then compute the bin points and the num-
ber of observations in each bin.
(8) If this is a rB iteration, set r = r + 1 and start a rA iteration. If this
is a rA iteration, start a rB iteration.
(9) If this is a rA iteration (r > 1), then discard the even systematic samples
in the buÔ¨Äer, and re-index the rest of the 3n0/2 systematic samples in
the Ô¨Årst half of the buÔ¨Äer. Set l0 = 2r‚àí1, Œ¥ = n0/2.
(10) If this is a rB iteration (r > 1), set Œ¥ = n0.
(11) Go to step 2.
(12) If the number of replications is less than speciÔ¨Åed, go to step 1.
(13) Construct the conÔ¨Ådence interval for f(x) according to Eq. (8.1).
(14) Let œµ‚Ä≤ be the desired absolute half-width criterion, or let Œ≥| ¬Øf(x)| be
the desired relative half-width criterion. If the half-width of the c.i. is
greater than œµ‚Ä≤ or Œ≥| ¬Øf(x)|, compute R‚Ä≤, the required number of inde-
pendent replications according to Eq. (8.2), set R = R‚Ä≤, and go to step
1; otherwise the procedure returns the c.i. estimator and terminates.
8.3
Empirical Experiments
In this section, we present some empirical results of estimating density func-
tions with the descried method. We tested the procedure with several i.i.d.
and correlated sequences. In these experiments, we used R = 3 indepen-
dent replications to construct c.i.‚Äôs. We constructed density c.i.‚Äôs at four
pre-speciÔ¨Åed points for each distribution. The conÔ¨Ådence level 1 ‚àíŒ± of the
density c.i. (i.e., Eq. (8.1)) is set to 0.90. Moreover, the conÔ¨Ådence level of

152
Crafts of Simulation Programming
Table
8.1
Coverage
of
90%
conÔ¨Ådence
den-
sity
estimators
for
the
N(0, 1.81)
distribution
avg N
4449
stdev N
820
x
-0.5
0
1.0
2.0
f(x)
0.2767
0.2965
0.2250
0.0982
Indicator Function (0.000475, 0.000228)
coverage
89.4%
88.8%
90.3%
90.0%
avg Œ≥
0.0168
0.0161
0.0185
0.0284
stdev Œ≥
0.0124
0.0119
0.0140
0.0214
avg hw
0.0144
0.0145
0.0133
0.0092
stdev hw
0.0074
0.0080
0.0069
0.0051
Standard Normal (0.000278, 0.000145)
coverage
84.1%
80.5%
87.8%
88.2%
avg Œ≥
0.0164
0.0174
0.0149
0.0265
stdev Œ≥
0.0119
0.0119
0.0110
0.0196
avg hw
0.0111
0.0112
0.0100
0.0073
stdev hw
0.0059
0.0059
0.0053
0.0039
the runs test for no correlation is set to (approximately) 0.90 as well. In
the implementation, the runs test contains both runs up and runs down,
with both tests set to Œ± = 0.05.
We tested the following independent sequences:
‚Ä¢ Observations are i.i.d. from the normal distribution N(0, 1.81).
‚Ä¢ Observations are i.i.d. from the Weibull distribution Weibull(1/2, 1),
where Weibull(Œ±, Œ≤) denotes the Weibull distribution with shape pa-
rameter Œ± and scale parameter Œ≤.
Tables 8.1 and 8.2 list the experimental results using the normal and
Weibull distributions, respectively. Each design point was based on 1000
replications. The avg N row lists the average of the sample size of each
independent run.
The stdev N row lists the standard deviation of the
sample size.
The x row lists the point where we want to estimate the
density. The f(x) row lists the true density. The values after each of the
estimation methods are the IMSE and the standard error of the integrated
mean squared error.
The coverage row lists the percentage of the c.i.‚Äôs
that cover the true f(x). The avg Œ≥ row lists the average of the relative
precisions of the density estimators. Here, the relative precision is deÔ¨Åned
as Œ≥ = | ÀÜf(x) ‚àíf(x)|/f(x). The stdev Œ≥ row lists the standard deviation
of the relative precision of the density estimators. The avg hw row lists
the average of the c.i. half-widths. The stdev hw row lists the standard
deviation of the c.i. half-width.

Density Estimation
153
Table
8.2
Coverage
of
90%
conÔ¨Ådence
density
estimators
for
the
Weibull(1/2, 1)
distribution
avg N
4427
stdev N
798
x
0.5
1.0
1.5
2.0
f(x)
0.7358
0.2707
0.0996
0.0366
Indicator Function (0.001473, 0.000820)
coverage
90.4%
91.0%
92.1%
89.9%
avg Œ≥
0.0196
0.0300
0.0537
0.0915
stdev Œ≥
0.0149
0.0223
0.0398
0.0686
avg hw
0.0466
0.0257
0.0177
0.0103
stdev hw
0.0243
0.0135
0.0088
0.0054
Standard Normal (0.000590, 0.000342)
coverage
86.3%
88.5%
91.8%
89.6%
avg Œ≥
0.0153
0.0256
0.0420
0.0729
stdev Œ≥
0.0116
0.0188
0.0310
0.0552
avg hw
0.0311
0.0207
0.0133
0.0082
stdev hw
0.0168
0.0107
0.0069
0.0042
As expected, the IMSE with the standard normal as the kernel is bet-
ter than with the indicator function as the kernel. Even though using the
standard normal as the kernel requires more computation, the additional
computational cost is minimal with today‚Äôs computers. In these experi-
ments, no relative or absolute precisions were speciÔ¨Åed, so the half-width of
the c.i. is the result of the default precision. The coverages are around or
slightly less than the nominal value of 90%. In general, the variance of the
estimates is larger with the indicator function as the kernel. The results
indicate that the variance estimates are smaller than necessary with the
normal function as the kernel and result in coverage less than the nominal
value. Furthermore, the estimates are likely biased low at the mode x = 0.
With Œ± = 0.10, the independent sequences will fail the runs test for no
correlation 10% of the time. The average sample sizes, 4449 and 4427, are
close to the theoretical value, i.e., ‚àû
i=0 n0Œ±i, where n0 = 4000.
Figures 8.2 and 8.3, respectively, show the empirical and true densities
of the normal and Weibull distributions, generated from the Ô¨Årst run of the
estimate of the experiments. These Ô¨Ågures reveal the essential character-
istics of the underlying density functions. However, with the sample size
just over 4000, the estimated density curves are rather ragged, especially
with the indicator kernel function. As expected, the normal kernel function
smoothes the ragged empirical density curve.
The density of the Weibull distribution is 0 when x ‚â§0 and as x > 0
approaches 0 from the right, the density increases. The standard normal

154
Crafts of Simulation Programming
Fig. 8.2
Empirical density of the N(0, 1.81) distribution
kernel density estimates over-smooths the density curve at the bounded tail
and has an inÔ¨Çecting point. To deal with this diÔ¨Éculty, various adaptive
methods have been proposed; see [Silverman (1986)] for more details.
We also tested the following correlated sequences:
‚Ä¢ Observations are from the Ô¨Årst-order moving average process Xi =
Œº + œµi + Œ∏œµi‚àí1 for i = 1, 2, . . ., where œµi is i.i.d. N(0, 1) and 0 < Œ∏ < 1.

Density Estimation
155
Fig. 8.3
Empirical density of the Weibull(1/2, 1) distribution
This process is denoted as MA1(Œ∏). Œº is set to 0 in the experiments. It
can be shown that X has an asymptotic N(0, 1 + Œ∏2) distribution.
‚Ä¢ Observations are the delays in queue (exclusive of service times) from
the M/M/2 queuing model, with the arrival rate Œª = 9 and service rate
ŒΩ = 5.
For the MA1 process, we set Œ∏ to 0.90. In order to eliminate the initial

156
Crafts of Simulation Programming
Table 8.3
Coverage of 90% conÔ¨Ådence density es-
timators for the MA1(0.9) process
avg N
8457
stdev N
864
x
-0.5
0.0
1.0
2.0
f(x)
0.2767
0.2965
0.2250
0.0982
Indicator Function (0.000302, 0.000130)
coverage
88.9%
90.2%
89.9%
90.8%
avg Œ≥
0.0134
0.0131
0.0151
0.0234
stdev Œ≥
0.0099
0.0097
0.0115
0.0175
avg hw
0.0118
0.0121
0.0105
0.0076
stdev hw
0.0062
0.0064
0.0054
0.0040
Normal Distribution (0.000183, 0.000090)
coverage
83.5%
80.8%
88.7%
89.5%
avg Œ≥
0.0135
0.0137
0.0125
0.0212
stdev Œ≥
0.0092
0.0095
0.0094
0.0159
avg hw
0.0092
0.0093
0.0083
0.0062
stdev hw
0.0048
0.0050
0.0042
0.0032
bias, X0 is set to a random variate drawn from the steady-state distribu-
tion N(0, 1.81). Table 8.3 lists the experimental results of the MA1 process.
The c.i. coverage of these four design points are around the speciÔ¨Åed 90%
conÔ¨Ådence level for both estimators. The simulation run length generally
increases as the correlation coeÔ¨Écient Œ∏ of the MA1 process increases. The
simulation run length of the MA1 process with Œ∏ = 0.9 is larger than for
independent sequences (with N(0, 1.81) distribution) and consequently pro-
duces smaller IMSE, smaller values of the relative precisions and tighter
half-width.
A summary of the experimental results of the M/M/2 delay-in-queue
process is in Table 8.4. The queuing processes are strongly correlated and
the procedure correctly allocates a large sample size.
Figures 8.4 and 8.5, respectively, show the empirical distributions of the
MA1 process with Œ∏ = 0.9 and the M/M/2 delay-in-queue process with
Œª = 9 and ŒΩ = 5, generated from the Ô¨Årst run of the experiments. The
theoretical steady-state distributions of this MA1 process and this M/M/2
queuing process are, respectively, N(0, 1.81) and 1 ‚àí(81/95)e‚àíx, where
x ‚â•0. Again, the experimental results show that these density estimates
provide excellent approximations to the underlying steady-state probabil-
ity density. The waiting-time density of the stationary M/M/2 delay in
queue (with Œª = 9 and ŒΩ = 5) is f(x) = (81/95)e‚àíx for x ‚â•0 and is dis-
continuous at x = 0. Both estimators over estimate the density around the
discontinuity point. With a larger sample size allocated for strongly cor-

Density Estimation
157
Table
8.4
Coverage
of
90%
conÔ¨Ådence
density
estimators for the M/M/2 process
avg N
1120128
stdev N
224082
x
0.5
1.0
2.0
3.0
f(x)
0.5171
0.3137
0.1154
0.0425
Indicator Function (0.010743, 0.009440)
coverage
89.6%
88.3%
90.9%
89.3%
avg Œ≥
0.0049
0.0058
0.0129
0.0250
stdev Œ≥
0.0038
0.0044
0.0098
0.0198
avg hw
0.0079
0.0056
0.0046
0.0034
stdev hw
0.0041
0.0031
0.0024
0.0018
Standard Normal (0.000074, 0.000053)
coverage
91.0%
90.2%
90.4%
91.9%
avg Œ≥
0.0047
0.0057
0.0124
0.0250
stdev Œ≥
0.0036
0.0044
0.0097
0.0198
avg hw
0.0079
0.0059
0.0047
0.0035
stdev hw
0.0043
0.0031
0.0026
0.0018
related processes, the accuracy and precision of the empirical distributions
are greater.
Furthermore, the bandwidth with standard normal kernel,
which is calculated with the Ô¨Ånal sample size, is much smaller than with
the indicator kernel.
8.4
Summary
We have evaluated a sequential procedure for estimating the density f(x) of
a stationary stochastic process, with or without intra-process independence.
The c.i. constructed with both the indicator kernel and the normal kernel
estimates obtained coverages above or around the nominal value. Because
to obtain the normal kernel estimate, the procedure needs to compute the
indicator kernel estimate, a prudent course is to take into account both
estimates. If there are signiÔ¨Åcant diÔ¨Äerences between the two estimates,
further studies with large sample sizes should then be performed to inves-
tigate the cause. For example, the diÔ¨Äerence in estimates may be caused
by an over-smoothed normal kernel estimate.
The indicator kernel is more suitable as a generic density estimator
because it requires less computation, determines the bandwidth without
the Ô¨Ånal sample size, delivers a valid c.i., and has no diÔ¨Éculty estimating
the density around a bounded tail, though its IMSE is generally larger.
Some density estimates require more observations than others before the
asymptotics necessary for density estimates to become approximately valid.

158
Crafts of Simulation Programming
Fig. 8.4
Empirical density of the MA1 process
Our algorithm works well in determining the required simulation run length
for the asymptotic approximation to become valid. The results from our
empirical experiments show that the procedure is excellent in achieving the
pre-speciÔ¨Åed accuracy. The procedure computes quantiles only at bin points
and uses Lagrange interpolation to estimate the density at certain points.
Consequently, the density at diÔ¨Äerent points within the same bin can have
diÔ¨Äerent values.
The procedure also generates an empirical distribution

Density Estimation
159
Fig. 8.5
Empirical density of the M/M/2 process
(histogram) of the output sequence, which can provide insights into the
underlying stochastic process.
Our approach has the desirable properties that it is a sequential proce-
dure and it does not require users to have a priori knowledge of values that
the data might take on. This allows users to apply this method without
having to execute a separate pilot run to determine the range of values
to be expected, or guess and risk having to re-run the simulation. The

160
Crafts of Simulation Programming
main advantage of our approach is that, by using a straightforward test
for lack of correlation to determine the simulation run length and obtain
quantiles at bin points, we can apply classical statistical techniques directly
and do not require more advanced statistical theory, thus making it easy
to understand, and simple to implement.

Chapter 9
Comparing Two Alternatives
This chapter investigates the (Behrens-Fisher) problem of interval estima-
tion and hypothesis testing concerning the diÔ¨Äerence between the means of
two normally distributed populations when the variances of the two popu-
lations maybe unequal, based on two independent samples.
Let ni denote the sample size of system i. Let Xij, i = 1, 2 and j =
1, 2, . . . , ni denote a sequence of mutually independent random samples
from system i, and let Œºi = E(Xij) be the expected response of interest.
We are interested in a conÔ¨Ådence interval for Œ¥ = Œº1‚àíŒº2. There are various
inference procedures to construct conÔ¨Ådence interval for Œ¥. We propose a
new approach to construct the c.i. for Œ¥. The procedure allows the unknown
variances to be unequal and allows unequal sample sizes. Furthermore, the
variance reduction technique of common random numbers can be used with
the procedure to reduce the conÔ¨Ådence interval half width. We then develop
a new approach to test the null hypothesis that Œº1 = Œº2.
9.1
Background
In this section, we review inference procedures of two means and the fun-
damentals of indiÔ¨Äerence-zone selection.
9.1.1
Inference Procedures of Two Means
Let Xij ‚àºN(Œºi, œÉ2
i ). Then X1j ‚àíX2j ‚àºN(Œº1 ‚àíŒº2, œÉ2
1 +œÉ2
2). Furthermore,
let ¬ØXi = ni
j=1 Xij/ni for i = 1, 2, then ¬ØX1 ‚àí¬ØX2 ‚àºN(Œº1 ‚àíŒº2, œÉ2
1/n1 +
œÉ2
2/n2). We review three inference procedures of constructing conÔ¨Ådence
interval of diÔ¨Äerence of Œº1 ‚àíŒº2: 1) paired-t c.i.; 2) pooled two-sample-t c.i.;
3) un-pooled two-sample-t c.i.
161

162
Crafts of Simulation Programming
When constructing the paired-t c.i., the same sample sizes for each
system must be equal, i.e., n1 = n2 = n. We then pair X1j with X2j to
deÔ¨Åne Zj = X1j ‚àíX2j, for j = 1, 2, . . . , n. Then Zj‚Äôs are i.i.d. normal
random variables with E(Zj) = Œ¥. Compute sample mean
¬ØZ =
n
j=1 Zj
n
and sample variance
S2(n) =
n
j=1[Zj ‚àí¬ØZ]2
n ‚àí1
.
Then the 100(1 ‚àíŒ±) percent c.i. of Œº1 ‚àíŒº2 is
¬ØZ ¬± tn‚àí1,1‚àíŒ±/2S(n)/‚àön.
In the pooled two-sample-t approach, the sample sizes n1 and n2 can be
diÔ¨Äerent, but the variance must be equal, i.e., Var(X1j) = Var(X2j). Let
S2
i (ni) =
ni
j=1[Xij ‚àí¬ØX2
i ]
ni ‚àí1
for i = 1, 2. The pooled variance of X1j and X2j is
S2
p = (n1 ‚àí1)S2
1(n1) + (n2 ‚àí1)S2
2(n2)
n1 + n2 ‚àí2
.
Similarly, the pooled variance of ¬ØX1 ‚àí¬ØX2 is S2
p/n1 + S2
p/n2. Note that
this approach requires X1j‚Äôs be independent of X2j‚Äôs. Then the 100(1 ‚àíŒ±)
percent c.i. of Œº1 ‚àíŒº2 is
¬ØX1 ‚àí¬ØX2 ¬± tn1+n2‚àí2,1‚àíŒ±/2Sp

1/n1 + 1/n2.
For more detail, see [Devroye (1995)]. [ScheÔ¨Ä¬¥e (1970)] points out that if
n1 = n2 the pooled two-sample-t approach is fairly safe even if the variances
are unequal.
In the un-pooled two-sample-t approach, the variances can be unequal,
an approximate c.i. can be constructed, see [Welch (1938)]. Compute the
estimated degrees of freedom
ÀÜf =
(S2
1(n1)/n1 + S2
2(n2)/n2)2
(S2
1(n1)/n1)2/(n1 ‚àí1) + (S2
2(n2)/n2)2/(n2 ‚àí1).
Then the 100(1 ‚àíŒ±) percent c.i. of Œº1 ‚àíŒº2 is
¬ØX1 ‚àí¬ØX2 ¬± t ÀÜ
f,1‚àíŒ±/2

S2
1(n1)/n1 + S2
2(n1)/n2.
Because ÀÜf likely will not be an integer, interpolation of t tables can be used.

Comparing Two Alternatives
163
Fig. 9.1
Power of a statistical test
9.1.2
Null Hypothesis Tests of Equivalence
The second aspect of the Behrens-Fisher problem is testing the equality
of two normal means when variances are unknown and maybe unequal.
That is, the problem is to test the null hypothesis H0 : Œº1 = Œº2 against
the alternative hypothesis Ha : Œº1 Ã∏= Œº2. While this hypothesis test can be
performed by checking whether the constructed c.i. of Œº1‚àíŒº2 contains 0, the
power of a statistical test is not addressed. Recall that in null hypothesis
tests, there are Type I error with probability Œ± that we reject the null
hypothesis when it is true and Type II error with probability Œ≤ that we
accept the null hypothesis when it is false. The power of a statistical test is
then 1 ‚àíŒ≤. See Figure 9.1 for an illustration. Let the 1 ‚àíŒ± c.i. half width
of Œ¥ = Œº1 ‚àíŒº2 be w. The shaded area under the curve is Œ≤ and the rest of
the area under the curve is the power of a statistical test.
To address this issue, users need to specify two more parameters in addi-
tion to the conÔ¨Ådence level 1‚àíŒ±: 1) the value of Œî, see Section 9.2.2; 2) the
power of a statistical test 1‚àíŒ≤. [Dudewicz et al. (2007)] investigated three
exact solutions of the Behrens-Fisher problem: 1) Dudewicz and Almed‚Äôs
[Dudewicz and Ahmed (1998)] procedure; 2) Chapman‚Äôs [Chapman (1950)]
procedure; 3) Prokof‚Äôyev and Shishkin‚Äôs [Prokof‚Äôyev and Shishkin (1974)]

164
Crafts of Simulation Programming
procedure. One drawback of these procedures is that they are not user
friendly.
9.2
Methodology
In this section, we present a new approach to construct c.i. of the diÔ¨Äerence
of two means.
9.2.1
A Weighted-Sample-Means Approach
Let H denote the desired c.i. half width of Œº1 ‚àíŒº2. If the c.i. half width
w12, obtained by the initial samples, is greater than H, then lager samples
are required. For example, if the paired-t c.i. half width with sample size
n w12 > H. Then the sample size will be increased to (w12/H)2n.
Let ÀúXi be the weighted sample means (with the Ô¨Årst-stage sample size
n0) as deÔ¨Åned in [Dudewicz and Dalal
(1975)], see Section 4.2.1. They
show that
Ti =
ÀúXi ‚àíŒºi
d‚àó/h
for i = 1, 2
would have a t distribution with n0 ‚àí1 degrees of freedom. Here h is a
critical constant that will be discussed later. Furthermore,
œë = T1 ‚àíT2 = ( ÀúX1 ‚àíÀúX2) ‚àí(Œº1 ‚àíŒº2)
d‚àó/h
=
( ÀúX1 ‚àíÀúX2) ‚àí(Œº1 ‚àíŒº2)

S2
1(n0)/(2N1) + S2
2(n0)/(2N1)
.
Here S2
i (n0) is the unbiased estimator of the variance œÉ2
i with n0 samples.
Without loss of generality, we temporarily assume that N1 and N2 are real
numbers. The last equality holds because
S2
1(n0)
2N1
= S2
2(n0)
2N2
= (d‚àó)2
2h2
and

S2
1(n0)
2N1
+ S2
2(n0)
2N2
= d‚àó
h .
The distribution of œë is symmetric and its percentile (or quantile), h, can
be evaluated numerically. Let
w12 = h
‚àö
2

S2
1(n0)
N1
+ S2
2(n0)
N2
.
Then w12 = d‚àó. In practice,
Ni = max(n0 + 1, ‚åà(hSi(n0)/d‚àó)2‚åâ), for i = 1, 2
(9.1)
are integers and w12 ‚â§d‚àó.

Comparing Two Alternatives
165
The value h is obtained such that Pr[T1 ‚àíT2 ‚â§h] =
 ‚àû
‚àí‚àûF(t +
h)f(t)dt = P ‚àó. Consequently, Pr[œë ‚â§h] = P ‚àóand Pr[ ÀúX1 ‚àíÀúX2 ‚àíd‚àó‚â§Œº1 ‚àí
Œº2] ‚â•P ‚àó. Note that Ti for i = 1, 2 are independent and the t-distribution
with n0‚àí1 d.f. Because the distribution of ( ÀúX1‚àíÀúX2)‚àí(Œº1‚àíŒº2) has a sym-
metrical distribution, Pr[ ÀúX1‚àíÀúX2‚àíd‚àó‚â§Œº1‚àíŒº2 ‚â§ÀúX1‚àíÀúX2+d‚àó] ‚â•2P ‚àó‚àí1.
The variance of ¬ØXi is no more than the variance of ÀúXi, consequently,
Pr[ ¬ØX1 ‚àí¬ØX2 ‚àíd‚àó‚â§Œº1 ‚àíŒº2 ‚â§
¬ØX1 ‚àí¬ØX2 + d‚àó] ‚â•Pr[ ÀúX1 ‚àíÀúX2 ‚àíd‚àó‚â§
Œº1 ‚àíŒº2 ‚â§ÀúX1 ‚àíÀúX2 + d‚àó] ‚â•2P ‚àó‚àí1. For some discussion of the properties
of ¬ØX and ÀúX, see [Chen (2011)]. Furthermore, when ‚åà(hSi(n0)/d‚àó)2‚åâ> n0,
Pr[ ¬ØX1 ‚àí¬ØX2 ‚àíw12 ‚â§Œº1 ‚àíŒº2 < ¬ØX1 ‚àí¬ØX2 + w12] ‚â•Pr[ ÀúX1 ‚àíÀúX2 ‚àíw12 ‚â§
Œº1 ‚àíŒº2 < ÀúX1 ‚àíÀúX2 + w12] ‚â•2P ‚àó‚àí1. When ‚åà(hSi(n0)/d‚àó)2‚åâ‚â§n0, the
weighted sample ÀúXi purposely loses some information so that the required
c.i. half width is d‚àó.
The interval ¬ØX1‚àí¬ØX2¬±w12 is a valid c.i. for Œº1‚àíŒº2 with tight half-width
w12 ‚â§d‚àó. However, when the goal is to test the null hypothesis Œº1 = Œº2,
the weighted sample means should be used because they have the same
distribution.
9.2.2
Fix the Value of Œ≤ = Œ±/2 of Null Hypothesis Tests
Our solution of the Behrens-Fisher problem is similar to that of [Chapman
(1950)]. To make the approach user friendly, we Ô¨Åx the value of Œ≤ = Œ±/2.
The test at conÔ¨Ådence level 1 ‚àíŒ± of H0 : Œº1 = Œº2 against the alternative
Ha : Œº1 Ã∏= Œº2 is based on the test statistic
œë = T1 ‚àíT2.
The acceptance region for this test is |œë| ‚â§h1‚àíŒ±/2, where h1‚àíŒ± is the 1‚àíŒ±
quantile of the distribution of œë. That is,
| ÀúX1 ‚àíÀúX2| ‚â§h1‚àíŒ±/2
‚àö
2

S2
1(n0)
N1
+ S2
2(n0)
N2
.
Let
Y =
ÀúX1 ‚àíÀúX2 ‚àíŒî

S2
1(n0)
2N1
+ S2
2(n0)
2N2
.
When the the diÔ¨Äerence of the two means Œî = |Œ¥|, the probability of
committing a Type II error, i.e., concluding that the null hypothesis is true

166
Crafts of Simulation Programming
when in fact it is false, is
Œ≤ = Pr[‚àíh1‚àíŒ±/2 ‚àí
Œî

S2
1(n0)
2N1
+ S2
2(n0)
2N2
‚â§Y ‚â§h1‚àíŒ±/2 ‚àí
Œî

S2
1(n0)
2N1
+ S2
2(n0)
2N2
]
= G(h1‚àíŒ±/2 ‚àí
Œî

S2
1(n0)
2N1
+ S2
2(n0)
2N2
) ‚àíG(‚àíh1‚àíŒ±/2 ‚àí
Œî

S2
1(n0)
2N1
+ S2
2(n0)
2N2
)
‚â§G(h1‚àíŒ±/2 ‚àí
Œî

S2
1(n0)
2N1
+ S2
2(n0)
2N2
).
Here G is the distribution function of œë. Furthermore, the value of
G(‚àíh1‚àíŒ±/2 ‚àí
Œî

S2
1(n0)
2N1
+ S2
2(n0)
2N2
)
will be very small, i.e., much smaller than Œ≤.
For Ô¨Åxed Œî and Œ±, Œ≤ can be evaluated as a function of sample sizes N1
and N2. Suppose we want to limit the probability of Œ≤, the sample sizes
N1 and N2 should be large enough such that
h1‚àíŒ±/2 ‚àí
Œî

S2
1(n0)
2N1
+ S2
2(n0)
2N2
= hŒ≤.
Note that the true probability of committing Type II error will be less than
Œ≤. The equation can be achieved when
N1
S2
1(n0) =
N2
S2
2(n0) = (h1‚àíŒ±/2 ‚àíhŒ≤
Œî
)2.
That is, the sample size is Ni = ‚åà(h1‚àíŒ±/2 ‚àíhŒ≤)2S2
i (n0)/Œî2‚åâfor i = 1, 2.
Let Œ≤ = Œ±/2 = 1 ‚àíP ‚àó. Recall that h1‚àíŒ±/2 = ‚àíhŒ±/2. The sample size
for system i is then computed by
Ni = max(n0 + 1, ‚åà(2h1‚àíŒ±/2Si(n0)/Œî)2‚åâ).
(9.2)
This is the same as Eq. (9.1) with d‚àó= Œî/2. Similarly, the weight Wi1
of Eq. (4.3) needs to computed with d‚àó= Œî/2. When Ni > n0, w12 =
(h1‚àíŒ±/2/
‚àö
2)

S2
1(n0)/N1 + S2
2(n0))/N2 ‚â§Œî/2. If Œº1 = Œº2, then Pr[| ÀúX1 ‚àí
ÀúX2| < w12] ‚â•1 ‚àíŒ±. If |Œº1 ‚àíŒº2| ‚â•Œî, then Pr[| ÀúX1 ‚àíÀúX2| < w12] ‚â§Œ≤.
With this approach, the critical constant h1‚àíŒ±/2 does not depend on Œî
and a simpler critical constant table can be used. We believe this can in-
crease the usability of the procedure. Note that the value of |Œ¥| is unknown.
Nevertheless, if the speciÔ¨Åed Œî ‚â§|Œ¥|, then the probability of committing
Type II error will be than less Œ≤.
Table 9.1 lists the critical constant h, which depends on Œ± and n0.

Comparing Two Alternatives
167
Table 9.1
Values of critical constant h
n0/Œ±
0.010
0.025
0.050
0.100
0.250
2
9.9998
9.9996
9.9992
6.1568
2.0000
3
9.9991
6.5399
4.5661
3.0405
1.3663
4
6.4562
4.6400
3.5122
2.5045
1.2039
5
5.1889
3.9624
3.1152
2.2919
1.1358
6
4.6282
3.6389
2.9144
2.1780
1.0965
7
4.3182
3.4518
2.7942
2.1074
1.0711
8
4.1232
3.3304
2.7145
2.0596
1.0533
9
3.9897
3.2454
2.6578
2.0250
1.0401
10
3.8925
3.1827
2.6155
1.9989
1.0301
12
3.7613
3.0966
2.5566
1.9620
1.0156
14
3.6769
3.0402
2.5176
1.9373
1.0058
16
3.6181
3.0005
2.4898
1.9196
0.9986
18
3.5747
2.9710
2.4691
1.9063
0.9932
20
3.5415
2.9483
2.4531
1.8959
0.9890
22
3.5152
2.9302
2.4403
1.8876
0.9855
24
3.4940
2.9155
2.4298
1.8808
0.9827
26
3.4764
2.9033
2.4211
1.8751
0.9804
28
3.4615
2.8930
2.4138
1.8703
0.9784
30
3.4489
2.8842
2.4075
1.8662
0.9766
32
3.4380
2.8766
2.4020
1.8626
0.9751
34
3.4285
2.8700
2.3973
1.8595
0.9738
36
3.4202
2.8641
2.3931
1.8567
0.9727
38
3.4128
2.8589
2.3893
1.8543
0.9717
40
3.4062
2.8543
2.3860
1.8521
0.9707
42
3.4003
2.8502
2.3830
1.8501
0.9699
44
3.3950
2.8464
2.3803
1.8483
0.9691
46
3.3901
2.8430
2.3778
1.8467
0.9684
48
3.3857
2.8399
2.3756
1.8452
0.9678
50
3.3817
2.8370
2.3735
1.8438
0.9672
9.3
Empirical Experiments
In this section, we present some empirical results.
9.3.1
Experiment 1: DiÔ¨Äerence of Means
In this experiment, the unknown means for all systems are Œºi = 0 and
the unknown variances of system i œÉ2
i for i = 1, 2.
Table 9.2 lists the
conÔ¨Åguration of variances. The required parameter d‚àóis set to 0.2 or 0.6,
which implies that the targeted one-tailed conÔ¨Ådence interval half-width
is 0.2 or 0.6, respectively. The sample size for each system is computed
according to Eq. (9.1). The initial sample size n0 = 20. We experimented
with the following nominal conÔ¨Ådence levels 0.95, 0.975, and 0.99, hence,
the critical constant h = 2.4531, 2.9483, and 3.5415. The resulting two-

168
Crafts of Simulation Programming
Table 9.2
Variance conÔ¨Åg-
uration
Model
œÉ2
1, œÉ2
2
Equal
1, 1
Increasing
1, 1 + d‚àó
Decreasing
1,
1
1+d‚àó
Table 9.3
Percentage of the coverage with equal means and d‚àó= 0.2
ConÔ¨Åguration
1 ‚àíŒ±
half width
0.90
0.95
0.98
Weighted
d‚àó
0.9033
0.9472
0.9802
Overall
d‚àó
0.9053
0.9474
0.9801
Equal
Weighted
w12
0.9028
0.9468
0.9801
Overall
w12
0.9050
0.9471
0.9799
¬ØT
302
434
626
Weighted
d‚àó
0.9027
0.9533
0.9789
Overall
d‚àó
0.9028
0.9534
0.9790
Increasing
Weighted
w12
0.9021
0.9527
0.9787
Overall
w12
0.9024
0.9527
0.9790
¬ØT
331
479
690
Weighted
d‚àó
0.8984
0.9504
0.9795
Overall
d‚àó
0.8977
0.9507
0.9790
Decreasing
Weighted
w12
0.8980
0.9500
0.9793
Overall
w12
0.8972
0.9507
0.9789
¬ØT
276
399
577
tailed c.i. will have 0.90, 0.95, and 0.98 conÔ¨Ådence. We list the percentage
that the c.i. (constructed four diÔ¨Äerent ways) contain the true diÔ¨Äerences
Œº1 ‚àíŒº2 (i.e., 0 ‚ààÀúX1 ‚àíÀúX2 ¬± d‚àó, 0 ‚àà¬ØX1 ‚àí¬ØX2 ¬± d‚àó, 0 ‚ààÀúX1 ‚àíÀúX2 ¬± w12,
0 ‚àà¬ØX1 ‚àí¬ØX2 ¬± w12).
Tables 9.3 and 9.4 list the results with d‚àó= 0.2 and 0.6, respectively.
In addition to the observed coverages, we also list the average sample size
of each simulation run T, i.e., T = 10000
r=1
k
i=0 Nr,i/10000, Nr,i is the
total number of replications or batches for system i in the rth independent
run. The procedure correctly increases and decreases the allocated sample
sizes as the variance increases and decreases, respectively. The observed
coverages of the c.i. constructed by 0 ‚ààÀúX1 ‚àíÀúX2 ¬±d‚àóand 0 ‚àà¬ØX1 ‚àí¬ØX2 ¬±w12
are close to the nominal values. Because the overall sample mean ¬ØXi has
smaller variance than the weighted sample mean ÀúXi, the coverages of the
conÔ¨Ådence intervals built by ¬ØXi is generally greater than those built by ÀúXi,
with the same half width.
In the setting that d‚àó= 0.6, the initial sample size (i.e., 42= (20+1)2) is
large enough to achieve greater precision than the speciÔ¨Åed precision in two

Comparing Two Alternatives
169
Table 9.4
Percentage of the coverage with equal means and d‚àó= 0.6
ConÔ¨Åguration
1 ‚àíŒ±
half width
0.90
0.95
0.98
Weighted
d‚àó
0.8973
0.9484
0.9776
Overall
d‚àó
0.9492
0.9619
0.9808
Equal
Weighted
w12
0.8413
0.9317
0.9754
Overall
w12
0.9046
0.9503
0.9792
¬ØT
42
52
70
Weighted
d‚àó
0.9047
0.9461
0.9791
Overall
d‚àó
0.9346
0.9574
0.9812
Increasing
Weighted
w12
0.8704
0.9377
0.9768
Overall
w12
0.9090
0.9497
0.9801
¬ØT
49
64
91
Weighted
d‚àó
0.8995
0.9532
0.9788
Overall
d‚àó
0.9685
0.9756
0.9850
Decreasing
Weighted
w12
0.7995
0.9203
0.9706
Overall
w12
0.9053
0.9555
0.9803
¬ØT
42
47
59
cases. The weighted sample means purposely lose information so that the
observed coverages of the c.i. constructed by ÀúX1 ‚àíÀúX2 ¬± d‚àóare close to the
nominal values. On the other hand, in those cases the observed coverages
of the c.i. constructed by ÀúX1 ‚àíÀúX2 ¬± w12 are less than the nominal values
because Ni = n0 + 1 for i = 1, 2 are greater than the required sample sizes
(say ai for i = 1, 2) in two cases. Hence,
w12 = h
‚àö
2

S2
1(n0)
N1
+ S2
2(n0)
N2
‚â§h
‚àö
2

S2
1(n0)
a1
+ S2
2(n0)
a2
‚âàd‚àó.
That is, in the cases that ai ‚â§n0, the variance of ÀúXi is greater than ¬ØXi while
the c.i. half width w12 is computed based on the variance of ¬ØXi and results
in low coverages. On the other hand, the coverages of c.i. constructed by
¬ØX1 ‚àí¬ØX2 ¬± d‚àóare greater than the nominal values. i.e., the half width is
wider than necessary.
9.3.2
Experiment 2: Null Hypothesis of Equal Means
In this experiment, the unknown means have two settings: 1) Œº1 = Œº2 =
0; 2) Œº1 = 0 and Œº2 = d‚àó. The conÔ¨Åguration of variances is as before,
i.e., Table 9.2.
The required parameter Œî is set to 0.2, which implies
that the targeted one-tailed conÔ¨Ådence interval half-width is d‚àó= Œî/2 =
0.1. The sample size for each system is computed according to Eq. (9.2).
The nominal conÔ¨Ådence level is 1 ‚àíŒ± = 0.90, 0.95, and 0.98. We list the
percentage of P(CD) (i.e., the probability of correct decision). That is, in

170
Crafts of Simulation Programming
Table 9.5
Percentage | Àú
X1 ‚àíÀú
X2| < w12 with equal means
ConÔ¨Åguration
1 ‚àíŒ±
0.90
0.95
0.98
Equal
P(CD)
0.9011
0.9516
0.9808
¬ØT
1203
1736
2519
Increasing
P(CD)
0.9000
0.9526
0.9813
¬ØT
1323
1916
2770
Decreasing
P(CD)
0.9008
0.9488
0.9795
¬ØT
1105
1587
2303
Table 9.6
Percentage | Àú
X1 ‚àíÀú
X2| ‚â•w12 with unequal means
ConÔ¨Åguration
1 ‚àíŒ≤
0.95
0.975
0.99
Equal
P(CD)
0.9519
0.9733
0.9895
¬ØT
1203
1736
2519
Increasing
P(CD)
0.9507
0.9770
0.9915
¬ØT
1323
1916
2770
Decreasing
P(CD)
0.9500
0.9733
0.9900
¬ØT
1105
1587
2303
the Ô¨Årst setting, we list the percentage that | ÀúX1 ‚àíÀúX2| < w12, i.e., we don‚Äôt
reject the null hypothesis that Œº1 = Œº2. In the second setting, we list the
percentage that | ÀúX1 ‚àíÀúX2| ‚â•w12, i.e., we reject the null hypothesis that
Œº1 = Œº2.
Tables 9.5 and 9.6 list the results of settings 1 and 2, respectively. The
observed P(CD)‚Äôs are close to the nominal values. The procedure correctly
allocates the sample sizes according to the variances and is eÔ¨Äective in terms
of Type I and Type II errors.
9.4
Summary
We have presented a new approach to construct c.i. of the diÔ¨Äerence of
two means as well as performing the null hypothesis test of equivalence
that controls the probability of Type II error. The approach allows the
unknown variances to be unequal and allow unequal sample sizes. Because
the required critical constant does not involve the parameter Œî, the table
of critical constants is simpliÔ¨Åed, which can increase the useability of the
procedure. Furthermore, common random numbers can be used with the
procedure to reduce the half width and the range of the c.i.

Chapter 10
Ranking and Selection
Among many other aspects, reliability analysis studies the expected life
and the failure rate of a component or a system of components linked
together in some structure.
We propose applying ranking-and-selection
procedures to this analysis. That is, we are more interested in whether
a given component is better than the others rather than the accuracy of
the performance measures. The underling philosophy is to rank estimators
through ordinal comparison while the precision of the estimates are still
poor [Ho et al. (1992)], hence, increase the eÔ¨Éciency of reliability analysis.
A detailed analysis to obtain more accurate performance measures can then
be carried out on the few chosen systems.
When evaluating k alternative system designs, one or more systems are
selected as the best; and the probability that the selected systems really are
the best is controlled. Let Œºi denote the expected response of system i and
let Œºil denote the lth smallest of the Œºi‚Äôs such that Œºi1 ‚â§Œºi2 ‚â§. . . ‚â§Œºik.
The goal is to select a subset of size m containing the v best of k systems.
We derive the probability lower bound of correctly selecting a subset based
on the distribution of order statistics in a clear and concise manner. If m =
v = 1, then the problem is to choose the best system. When m > v = 1,
we are interested in choosing a subset of size m containing the best. If
m = v > 1, we are interested in choosing the m best systems.
Many selection procedures are derived based on the least favorable con-
Ô¨Åguration (LFC), i.e., assuming Œºi1 = Œºi2 = ¬∑ ¬∑ ¬∑ = Œºiv and Œºiv + d‚àó=
Œºiv+1 = ¬∑ ¬∑ ¬∑ = Œºik. This is because the minimal P(CS), or the probability of
Correct Selection, occurs under the LFC. If the diÔ¨Äerence Œºiv+1 ‚àíŒºiv < d‚àó,
then these systems are considered to be in the indiÔ¨Äerence zone for cor-
rect selection. On the other hand, if the diÔ¨Äerence Œºiv+1 ‚àíŒºiv ‚â•d‚àó, then
these systems are considered to be in the preference zone for correct se-
171

172
Crafts of Simulation Programming
lection. IndiÔ¨Äerence-zone selection procedures attempt to avoid making a
large number of replications or batches to resolve diÔ¨Äerences less than d‚àó.
The goal is to make a correct selection with a probability of at least P ‚àó
provided that Œºiv+1 ‚àíŒºiv ‚â•d‚àó.
We present a framework for indiÔ¨Äerence-zone selection that is applicable
for the normal and lognormal populations. This framework is derived based
on the distribution of order statistics.
10.1
Introduction
First, some notations:
Xij: the observations from the jth replication or batch of the ith system;
n0: the number of initial replications for all systems of multi-stage proce-
dures;
ni: the number of replications for system i, where i ‚â•1;
Œºi: the expected performance measure for system i, i.e., Œºi = E(Xij);
¬ØXi(n0): the sample mean performance measure for system i, i.e., ¬ØXi(n0) =
n0
j=1 Xij/n0;
¬ØXi: shorthand for ¬ØXi(ni), i.e., the sample means with all samples of system
i;
œÉ2
i : the variance of the observed performance measure of system i from one
replication or batch, i.e., œÉ2
i = Var(Xij);
S2
i (ni): the sample variance of system i with ni replications, i.e., S2
i (ni) =
ni
j=1(Xij ‚àí¬ØXi)2/(ni ‚àí1).
We derive the subset selection procedure and extend the procedure to
select the best system when the parameter of interest is variance or the
underlying populations are lognormally distributed.
10.1.1
Generalized Subset Selection
[Mahamunulu (1967)] considers a generalized version of the selection prob-
lem. The objective is to select a subset of size m containing at least c of
the v best of k systems, where max(1, m + v + 1 ‚àík) ‚â§c ‚â§min(m, v) and
max(m, v) ‚â§k ‚àí1. Furthermore, the minimum required P(CS) satisÔ¨Åes

Ranking and Selection
173
the following:
P ‚àó‚â•P(c, v, m, k) =
 k
m
‚àí1 min(m,v)

i=c
v
i
k ‚àív
m ‚àíi

.
If P ‚àó< P(c, v, m, k), then the precision requirement can be achieved by
choosing the subset at random. In the case that c = v = m = 1, the goal
is to select the best of k systems.
Selection procedures generally simulate ni samples for system i and rank
the sample means such that ¬ØXb1 ‚â§¬ØXb2 ‚â§¬∑ ¬∑ ¬∑ ‚â§¬ØXbk and select systems bl
for l = 1, 2, . . . , m as the best m systems. P(CS) of this selection problem
is now assessed. Let ¬ØX[c] be the cth smallest sample mean from ¬ØXil for
l = 1, 2, . . . , v and let Œº[c] be its unknown true mean. Let ¬ØX[u] (u = m‚àíc+1)
be the smallest sample mean from ¬ØXil for l = v + 1, v + 2, . . . , k and let
Œº[u] be its unknown true mean. To simplify the notation, we will use ¬ØXc,
¬ØXu, Œºc, and Œºu instead of ¬ØX[c], ¬ØX[u], Œº[c], and Œº[u] in the remainder of
this chapter. Then correct selection occurs if and only if ¬ØXc < ¬ØXu. An
important point is that systems c and u are unknown.
Let wcu be the one-tailed P ‚àóc.i. half-width of ¬ØXc ‚àí¬ØXu. Then
P(CS) = Pr[ ¬ØXc < ¬ØXu]
= Pr[Œºu ‚àíŒºc ‚àíd‚àó< ¬ØXu ‚àí¬ØXc]
‚â•Pr[Œºu ‚àíŒºc ‚àíwcu < ¬ØXu ‚àí¬ØXc]
= Pr[Œºu ‚àíŒºc < ¬ØXu ‚àí¬ØXc + wcu]
= P ‚àó.
The second equality holds because Œºu ‚àíŒºc = d‚àóunder the LFC. The
inequality holds because the procedure will allocate large enough sample
sizes such that wcu ‚â§d‚àó. The last equality holds from the deÔ¨Ånition of the
c.i.
10.1.2
Order Statistics of Continuous Distributions
Let f(¬∑|Œ∏) and F(¬∑|Œ∏), respectively, denote pdf and cdf of the random vari-
able Y given a parameter Œ∏, e.g., the mean of a normal distribution. Let
y be a realization of Y . From Chapter 4, the distribution of the uth order
statistics of k observations of Y is
gu:k(y|Œ∏) = Œ≤(F(y|Œ∏); u, k ‚àíu + 1)f(y|Œ∏).
Let
I(p; a, b) =
 p
0
Œ≤(x; a, b)dx, where a, b > 0.

174
Crafts of Simulation Programming
Assuming E[f(¬∑|Œ∏c)] = Œºic and E[f(¬∑|Œ∏u)] = Œºiu. Let Tc ‚àºgc:v(¬∑|Œ∏c) and
Tu ‚àºgm‚àíc+1:k‚àív(¬∑|Œ∏u). Then,
Pr[Tc ‚àíTu ‚â§h]
=
 ‚àû
‚àí‚àû
 y+h
‚àí‚àû
gc:v(x|Œ∏c)gm‚àíc+1:k‚àív(y|Œ∏u)dxdy
=
 ‚àû
‚àí‚àû
Gc:v(y + h|Œ∏c)dGm‚àíc+1:k‚àív(y|Œ∏u)
=
 ‚àû
‚àí‚àû
I(F(y + h|Œ∏c), c, v ‚àíc + 1)dI(F(y|Œ∏u), m ‚àíc + 1, k ‚àív).
The right-hand side is then equated to P ‚àóto solve for h. Values of the
critical constant h can be found in Table A.1. When h = 0 the equation is
the same as Eq. (4.14) in [Mahamunulu (1967)].
Fig. 10.1
Empirical probability densities of T
For example, if we are interested in the probability of correctly selecting
a subset of size 5 containing 3 of the Ô¨Årst 3 best from 10 alternatives, then
Tc ‚àºg3:3(tc) and Tu ‚àºg3:7(tu). Furthermore, if the initial sample size is
n0 = 20, then f and F are, respectively, the pdf and cdf of the t-distribution
with 19 d.f. Figure 10.1 displays the pdf of the random variables T having
a t-distribution with 19 d.f., Tc, Tu, and Tc ‚àíTu.
Note that the value of h is determined such that Pr [Tc ‚àíTu ‚â§h] = P ‚àó.
Let œë = Tc ‚àíTu, then Pr [œë < h] = P ‚àó. That is, under the LFC the value

Ranking and Selection
175
of h is the P ‚àóquantile of the distribution of œë. This property can be used
to estimate the value of h for the problem at hand.
The absolute indiÔ¨Äerence zone (i.e., 0 < Œºiv+1 ‚àíŒºiv < d‚àó
a = d‚àó) is
generally applied when the performance measure of interest is a location
parameter. When the performance measure of interest is a scale parameter,
the relative indiÔ¨Äerence zone (i.e., 1 ‚â§Œºiv+1/Œºiv < d‚àó
r ) is generally applied.
In the case of the relative indiÔ¨Äerence zone
Pr[Tc/Tu ‚â§h]
=
 ‚àû
‚àí‚àû
I(F(yh|Œ∏c), c, v ‚àíc + 1)dI(F(y|Œ∏u), m ‚àíc + 1, k ‚àív).
The right-hand side is then equated to P ‚àóto solve for h. Note that the
value of h is determined such that Pr [œï < h] = P ‚àó, where œï = Tc/Tu.
In cases where the parameter Œ∏ is either a location parameter or a scale
parameter, the procedure needs only the value of d‚àó
a or d‚àó
r to determine the
required sample size. On the other hand, the procedure needs the values
of Œ∏c and Œ∏u to determine the required sample sizes, when Œ∏ is neither a
location parameter nor a scale parameter and Œ∏c Ã∏= Œ∏u.
10.1.3
A Review of ConÔ¨Ådence Interval Half Width
Let Xi be a normally distributed random variable with mean Œºi and vari-
ance œÉ2
i . Let zp denote the p quantile of the standard normal distribution.
It is known that
Pr
 ¬ØXi ‚àíwi ‚â§Œºi

= p.
Here wi = zpœÉi/‚àöni is the one-tailed p c.i. half width of Œºi. Furthermore,
by the symmetry of the normal distribution
Pr

Œºi ‚â§¬ØXi + wi

= p.
Under the LFC, Œºi1 = Œºi2 = ¬∑ ¬∑ ¬∑ = Œºiv and Œºiv+1 = Œºiv+2 = ¬∑ ¬∑ ¬∑ = Œºik.
We consider the case that œÉ2
i = œÉ2 and ni = n0 for i = 1, 2, . . . , k. Let
Zi =
¬ØXi ‚àíŒºi
œÉ/‚àön0
for i = 1, 2, . . . , k.
Let Zc be the cth order statistics of the Zi for i = 1, 2, . . . , v. Note that Zc
is no longer normally distributed. Let hx be a critical constant such that
Pr [Zc ‚â§hx] = p.
Then Pr
 ¬ØXc ‚àíwc ‚â§Œºc

= p. Note that hx is the p quantile of the distribu-
tion of ( ¬ØXc ‚àíŒºc)/(œÉ/‚àön0) and wc = hxœÉ/‚àön0 is the one-tailed p c.i. half

176
Crafts of Simulation Programming
width of Œºc. Note that the value of œÉ is a scale parameter of the distribu-
tion of ¬ØXc. Moreover, based on the primitive study, ¬ØXc has a symmetric
distribution so that Pr

Œºc ‚â§¬ØXc + wc

= p.
Let Zu be the uth order statistics of the Zi for i = v + 1, v + 2, . . . , k.
It is known if Pr
 ¬ØXc ‚àíwc ‚â§Œºc

= p and Pr

Œºu ‚â§¬ØXu + wu

= p, then
Pr
 ¬ØXc ‚àí¬ØXu ‚â§Œºc ‚àíŒºu + wc + wu

= 2p‚àí1. Hence, if wc+wu ‚â§Œºu‚àíŒºc =
d‚àó, then Pr
 ¬ØXc ‚â§¬ØXu

‚â•2p ‚àí1. Let p = (1 + P ‚àó)/2.
To achieve the
speciÔ¨Åed probability guarantee, the sample sizes should be large enough
such that wc and wu are less than d‚àó/2. That is, nc = ‚åà(2hxœÉc/d‚àó)2‚åâand
nu = ‚åà(2hyœÉu/d‚àó)2‚åâ. Here hy is a critical constant similar to hx.
We would like to point out that, when the underlying distributions are
non-normal, there may not be a scale parameter and the c.i. half width
may be dependent upon other distribution parameters, such as location or
shape parameters.
10.1.4
ConÔ¨Ådence Interval Half Width of Interest
In the previous section, we show how to estimate the required sample size
based on the c.i. half widths wc and wu. In this section, we show that it
may be possible to estimate the required sample sizes based on wcu, i.e.,
the c.i. half-width of Œºc ‚àíŒºu.
Assume that wcu is determined by a vector of unknown parameters
œÄ and an unknown function Œ®, e.g., wcu = Œ®(h, nc, nu, œÄ).
Then it is
necessary to allocate the sample sizes nc and nu so that Œ®(h, nc, nu, œÄ) ‚â§d‚àó;
where d‚àóis the absolute indiÔ¨Äerence amount. Unfortunately, the function
Œ® is generally unknown.
In the special case that f(x) is the pdf of the t-distribution with n0 ‚àí1
d.f., Œ®(h, nc, nu, œÄ) is known. Recall that œë = Tc ‚àíTu. Then the cdf of œë
G(h) = Pr[œë ‚â§h]
=
v!
(c ‚àí1)!(v ‚àíc)!
(k ‚àív)!
(m ‚àíc)!(k ‚àív ‚àím + c ‚àí1)! √ó
 ‚àû
‚àí‚àû
 y+h
‚àí‚àû
[F(x)]c‚àí1[1 ‚àíF(x)]v‚àícf(x) √ó
[F(y)]m‚àíc[1 ‚àíF(y)]k‚àív‚àím+c‚àí1f(y)dxdy.
Note that the cdf G(h) is determined only by the d.f. of the t-distribution
given c, v, m, k. By deÔ¨Ånition h is the P ‚àóquantile of the distribution of œë
when G(h) = P ‚àó.

Ranking and Selection
177
Furthermore,
œë = Tc ‚àíTu = ( ÀúXc ‚àíÀúXu) ‚àí(Œºc ‚àíŒºu)
d‚àó/h
=
( ÀúXc ‚àíÀúXu) ‚àí(Œºc ‚àíŒºu)

S2c(n0)/(2nc) + S2u(n0)/(2nu)
.
Without loss of generality, we temporarily assume nc and nu are real num-
bers. The last equality holds since
S2
c(n0)
2nc
= S2
u(n0)
2nu
= (d‚àó)2
2h2
and

S2c(n0)
2nc
+ S2u(n0)
2nu
= d‚àó
h .
Consequently,
Pr

( ÀúXc ‚àíÀúXu) ‚àí(Œºc ‚àíŒºu)

S2c(n0)/(2nc) + S2u(n0)/(2nu)
‚â§h

= P ‚àó.
Let
wcu = h
‚àö
2

S2c(n0)
nc
+ S2u(n0)
nu
.
Then,
Pr[ ÀúXc ‚àíÀúXu ‚àíwcu ‚â§Œºc ‚àíŒºu] = P ‚àó.
Hence, to obtain wcu ‚â§d‚àó, ni ‚â•(hSi(n0)/d‚àó)2, provided (hSi(n0)/d‚àó)2 >
n0. This is the same result as that of [Dudewicz and Dalal (1975)], where
h1 is obtained with c = v = m = 1.
10.1.5
Adjustment of the DiÔ¨Äerence of Sample Means
It is known that (absolute) indiÔ¨Äerence-zone procedures that are derived
from the LFC are conservative and allocate samples based entirely on the
variances. The eÔ¨Éciency of the procedures can be improved by taking into
account true/sample means. Let di = Œºi ‚àíŒºi1 for i = 1, 2, . . . , k. Then
max(d‚àó, di) instead of d‚àówill be used to compute the required sample sizes.
In reality we use ÀÜdi, an estimator of di, to compute sample sizes.
We
involuntarily introduce error into the process, hence, the procedure does not
guarantee P(CS) ‚â•P ‚àó. Nevertheless, we can use a conservative adjustment
to increase P(CS). The adjustment takes into consideration the randomness
of ¬ØXb(n0) and allocates more replications or batches to more promising
designs.
Let the adjustment a = tn0‚àí1,P ‚àóSb( ¬ØXb(n0)), where S2
b ( ¬ØXb(n0)) is the
variance of ¬ØXb(n0). We use the one-tailed upper P ‚àóconÔ¨Ådence limit of Œºb,
U( ¬ØXb(n0)) = ¬ØXb(n0) + a, as an estimator of the reference point. If the

178
Crafts of Simulation Programming
procedure is to Ô¨Ånd the maximum, then the one-tailed lower P ‚àóconÔ¨Ådence
limit should be used. Even though this is somewhat arbitrary, we know
that Pr[Œºb ‚â§U( ¬ØXb(n0))] ‚âàP ‚àó. In this setting,
d‚Ä≤
i = max(d‚àó, ¬ØXi(n0) ‚àíU( ¬ØXb(n0))).
(10.1)
Conservative users can use higher conÔ¨Ådence of U( ¬ØXb(n0)) to increase
P(CS). We then compute the sample sizes for each design based on the
following formula.
Ni = max(n0 + 1, ‚åà(hSi(n0)/d‚Ä≤
i)2‚åâ), for i = 1, 2, . . . , k.
(10.2)
Let L( ¬ØXi(n0) ‚àí¬ØXb(n0)) be the lower conÔ¨Ådence limit of ¬ØXi(n0) ‚àí¬ØXb(n0).
One can also use d‚Ä≤
i = max(d‚àó, L( ¬ØXi(n0) ‚àí¬ØXb(n0))), which requires more
computations.
With this adjustment, we will allocate more simulation replications or
batches to more promising designs. Let
ÀÜdi = max(d‚àó, ¬ØXi(n0) ‚àí¬ØXb(n0))
(10.3)
and
N ‚Ä≤
i = max(n0 + 1, ‚åà(hSi(n0)/ ÀÜdi)2‚åâ), for i = 1, 2, . . . , k.
If ¬ØXi(n0) = ¬ØXb(n0) + Œ¥ and Ni and N ‚Ä≤
i > n0 + 1, then
Ni
N ‚Ä≤
i
=
‚éß
‚é®
‚é©
1
0 < Œ¥ ‚â§d‚àó
(Œ¥/d‚àó)2
d‚àó< Œ¥ ‚â§d‚àó+ a
(Œ¥/(Œ¥ ‚àía))2 d‚àó+ a < Œ¥.
Therefore, design i with the Ô¨Årst stage sample mean ¬ØXi(n0), such that
d‚àó< ¬ØXi(n0)‚àí¬ØXb(n0) ‚â§d‚àó+a, will have signiÔ¨Åcantly increased sample sizes
with this adjustment. The sample size for that particular design is increased
by (Œ¥/d‚àó)2 ‚àí1 times. On the other hand, if d‚àó+ a < ¬ØXi(n0) ‚àí¬ØXb(n0), the
additional sample sizes allocated with this adjustment is very minimal. If
¬ØXi(n0) ‚àí¬ØXb(n0) ‚â§d‚àó, then there are no changes in sample sizes.
We would like to point out that the purpose of R&S procedures is to
select a good design i and is not to estimate Œºi1. Because the procedure uses
¬ØXb(n0) as an estimator of the reference point, we would like to have some
conÔ¨Ådence of ¬ØXb(n0). If the variance of the samples of the best alternative
at the Ô¨Årst stage is small, we are more conÔ¨Ådent with this mean estimator,
therefore, the adjustment will be small.
In contrast, if the variance is
large, we are less conÔ¨Ådent with this mean estimator, consequently, the
adjustment will be large.

Ranking and Selection
179
10.1.6
The Source Code of Computing Additional Sample
Size
This subroutine implements the algorithm of computing the additional sam-
ple size of each design for next iteration of simulation as discussed in pre-
vious section.
void etss(double* s_mean,double* s_var,int nd, int* n,float h,
float d,int adj,int *add_budget,int *an)
/* s_mean[i]: sample mean of design i, i=0,1,..,nd-1
s_var[i]: sample variance of design i, i=0,1,..,nd-1
nd: the number of designs
n[i]: number of simulation replication of design i,
i=0,1,..,nd-1
h: the critical value,
d: the indifference amount,
adj: switch for the adjustment,
add_budget: the simulation budget, set to -1 if no limit
an[i]: additional number of simulation replication
assigned to design i, i=0,1,..,nd-1
*/
{
int i;
int b;
double viz;
double adjAmt = 0.0;
b=best(s_mean, nd);
if (adj) adjAmt = sqrt(s_var[b]/n[b]);
for(i=0;i<nd;i++) {
if (*add_budget == 0 ) {
an[i] = 0;
continue;
};
viz = s_mean[i] - s_mean[b] - adjAmt;
if (viz < d) viz = d;

180
Crafts of Simulation Programming
an[i] = (ceil(s_var[i]*pow(h/viz,2)) - n[i]+1) / 2;
if (an[i] < 0) an[i] = 0;
if (*add_budget > 0) {
if (*add_budget < an[i])
an[i] = *add_budget;
*add_budget -= an[i];
}; /* if add_budget */
}; /* for i */
}
int best(float* t_s_mean,int nd)
/* Determines the best design based on current simulation
results
t_s_mean[i]: tempary array for sample mean of design i,
i=0,1,..,ND-1
nd: the number of designs
*/
{
int i, min_index;
min_index=0;
for (i=1;i<nd;i++)
if(t_s_mean[i]<t_s_mean[min_index])
min_index=i;
return min_index;
}
10.1.7
A
Sequential
Ranking
and
Selection
Procedure
(SRS)
From the discussion in Section 10.1.1, if wcu ‚â§Œºu ‚àíŒºc, then P(CS) ‚â•P ‚àó.
Of course, the values of Œºu and Œºc are unknown and can not be used
to compute the c.i. half-width. In practice, sample means are necessary
in order to estimate the required wcu. Sort the sample means such that
¬ØXb1 ‚â§¬ØXb2 ‚â§¬∑ ¬∑ ¬∑ ‚â§¬ØXbk. Let U( ¬ØXbv) and L( ¬ØXbm+1), respectively, be the
upper and lower P ‚àóconÔ¨Ådence limits of Œºbv and Œºbm+1. [Chen and Kelton

Ranking and Selection
181
(2005)] replace d‚àóby
dbl =
max(d‚àó, L( ¬ØXbm+1) ‚àí¬ØXbl) 1 ‚â§l ‚â§v
max(d‚àó, ¬ØXbl ‚àíU( ¬ØXbv))
v + 1 ‚â§l ‚â§k
when computing the required sample sizes.
We now present cost-eÔ¨Äective sequential approach to select a subset of
size m that contains c of the v best system from k alternatives. We denote
this procedure SRS.
(1) Initialize the set I to include all k designs. Simulate n0 replications
or batches for each design i ‚ààI. Set the iteration number r = 0, and
N1,r = N2,r = . . . = Nk,r = n0, where Ni,r is the sample size allocated
for design i at the rth iteration.
(2) Set r = r+1 and compute Œ¥i,r, the incremental number of replications
or batches for design i at the rth iteration according to equation Eq.
(10.4).
(3) If Œ¥i,r = 0 and i Ã∏= b (where ¬ØXb,r = mini‚ààI ¬ØXi,r), delete design i from
the subset I.
(4) If there is only one element in the subset I, go to step 6.
(5) Simulate Œ¥i,r additional replications or batches for each design i ‚ààI
at the rth iteration. Go to step 2.
(6) Calculate and rank the sample means such that ¬ØXb1 ‚â§¬ØXb2 ‚â§. . . ‚â§
¬ØXbk. Select system bl iÔ¨Ä¬ØXbl ‚â§¬ØXbm.
Let di,r denote di at the rth iteration. The additional sample size for
alternative i at iteration r + 1 is
Œ¥i,r+1 = ‚åà((hSi(Ni,r)/di,r)2 ‚àíNi,r)+/2‚åâ, for i = 1, 2, . . . , k.
(10.4)
Here
(x)+
=
max(0, x)
and
the
critical
constant
h
depends
on
(c, v, m, k, P ‚àó, n0). Note that h(1, 1, 1, k, P ‚àó, n0) = h1(k, P ‚àó, n0). Further-
more,
Ni,r+1 = Ni,r + Œ¥i,r+1.
Let us consider the steps between taking additional samples, that is steps
2 through 5, be one iteration. We can reduce the number of iterations with
a larger incremental sample size for system i at the rth iteration, but we
run the risk of allocating more samples than necessary to non-promising
systems. We believe use Eq. (10.4) to compute the additional sample size
for alternative i at iteration r + 1 is a good compromise.
Note that sample means ¬ØXi, instead of weighted sample means ÀúXi,
are used to determine the subset.
This is because there are more than

182
Crafts of Simulation Programming
two stages of sample means and we can no longer use the approach of
[Dudewicz and Dalal (1975)] to compute weighted sample means. While
¬ØXi for i = 1, 2, . . . , k are still t-distributed, they have diÔ¨Äerent degrees of
freedom. This sequential procedure is asymptotically valid and performs
well in terms of P(CS) and sample sizes. For a discussion of the performance
of using the weighted sample means and the overall sample means in two-
stage procedures, please see [Chen (2011)].
10.2
Some Extensions of Selection of Continuous Distribu-
tions
This section discusses several variations of selection procedures: restricted
subset selection, optimal subset selection, relative indiÔ¨Äerence-zone selec-
tion, and multiple comparison with the best.
10.2.1
Restricted Subset Selection
This section investigates the problem of restricted subset selection, i.e., the
selected subset attempts to exclude systems that are deviated more than d‚àó
from the best. It can be shown that the sample sizes should be large enough
such that the c.i. half width wcu ‚â§d‚àó/2.
In other words, the required
sample size for system i is ni ‚â•(2hSi(n0)/d‚àó)2, provided (2hSi(n0)/d‚àó)2 >
n0. The procedure selects system bl if and only if ÀúXbl ‚â§ÀúXb1 + d‚àó/2. The
size of the selected subset is random but at most m populations (which is
speciÔ¨Åed by users) will Ô¨Ånally be chosen.
In the special case that Œºi1 = Œºi2 = ¬∑ ¬∑ ¬∑ = Œºik, we want to select all
good systems. We can write
P(CS) = Pr[ ÀúXbk < ÀúXb1 + d‚àó/2]
= Pr
 ÀúXbk ‚àíŒºbk
d‚àó/(2h3) <
ÀúXb1 ‚àíŒºb1
d‚àó/(2h3) ‚àíŒºbk ‚àíŒºb1 ‚àíd‚àó/2
d‚àó/(2h3)

= Pr [Tbk < Tb1 + h3]
=
 ‚àû
‚àí‚àû
Gk:k(tb1 + h3)dG1:k(tb1).
The right-hand side is then equated to P ‚àóto solve for h3.
The re-
quired sample size for system i is then ni ‚â•(2h3Si(n0)/d‚àó)2, provided
(2h3Si(n0)/d‚àó)2 > n0.

Ranking and Selection
183
In the special case that Œºi1 + d‚àó= Œºi2 = ¬∑ ¬∑ ¬∑ = Œºik, we want to select
only the best system. Let ÀúXc2 = mink
l=2 Œºil. We can write
P(CS) = Pr[ ÀúXc2 > ÀúXi1 + d‚àó/2]
= Pr
 ÀúXc2 ‚àíŒºi2
d‚àó/(2h1) >
ÀúXi1 ‚àíŒºi1
d‚àó/(2h1) ‚àíŒºc2 ‚àíŒºi1 ‚àíd‚àó/2
d‚àó/(2h1)

= Pr [Ti1 < Tc2 + h1]
=
 ‚àû
‚àí‚àû
F(tc2 + h1)dG1:k‚àí1(tc2).
Since h3 > h1, the sample sizes that guarantee all the sample means of
the best systems are within d‚àó/2 from the best sample mean (with the spec-
iÔ¨Åed probability P ‚àó) also guarantee the sample means of non-d‚àó-near-best
systems (i.e., systems with mean greater than Œºi1 + d‚àó) will not be within
d‚àó/2 from the best sample means (with no less the speciÔ¨Åed probability
P ‚àó).
A Sequential Restricted Subset Selection (SRSS) Procedure
(1) Let Ni,r be the sample size allocated for system i and ¬ØXi,r be the
sample mean of system i at the rth iteration.
Simulate n0 samples
for all systems. Set the iteration number r = 0, and N1,r = N2,r =
¬∑ ¬∑ ¬∑ = Nk,r = n0. Note that for r ‚â•1, Ni,r can have diÔ¨Äerent values
for diÔ¨Äerent i. Specify the value of the indiÔ¨Äerence amount d‚àóand the
required precision P ‚àó.
(2) Calculate the sample means and sample variances. Rank the sample
means such that ¬ØXb1 ‚â§¬ØXb2 ‚â§. . . ‚â§¬ØXbk.
(3) Calculate
the
required
sample
size
Nbl,r+1 = max(n0, ‚åà(2h3Sbl(Nbl,r)/dbl)2‚åâ), for l = 1, 2, . . . , k. Here,
dbl =
 max(d‚àó, L( ¬ØXbm+1) ‚àí¬ØXbl) 1 ‚â§l ‚â§v
max(d‚àó, ¬ØXbl ‚àíU( ¬ØXbv))
v + 1 ‚â§l ‚â§k.
(4) If Ni,r+1 ‚â§Ni,r, for i = 1, 2, . . . , k, go to step 6.
(5) Simulate additional ‚åà(Ni,r+1 ‚àíNi,r)+/2‚åâsamples for system i.
Set
r = r + 1. Go to step 2.
(6) Select system bl iÔ¨Ä¬ØXbl ‚â§min( ¬ØXbm, ¬ØXb1 + d‚àó/2).
10.2.2
An
IndiÔ¨Äerence-Zone
Procedure
to
Select
Only
and/or All The Best Systems
In this section, we develop an indiÔ¨Äerence-zone procedure to select the op-
timal subset, i.e., the subset contains only and/or all the best systems. We

184
Crafts of Simulation Programming
use the parameter Œ¥ to guide our decision. Based on the restricted-subset-
selection procedure, we consider the case that the indiÔ¨Äerence amount
d‚àó= 2Œ¥.
That is, we will set Œ¥ = d‚àó/2 when the indiÔ¨Äerence amount
d‚àóis speciÔ¨Åed. For a speciÔ¨Åed conÔ¨Ådence level P ‚àó, the subset is guaranteed
to contain all the best systems and none of the systems that deviate more
than d‚àófrom the best system(s). The size of the subset is determined by
the procedure and users do not need to specify the upper bound.
The procedure satisÔ¨Åes the following:
(1) If Œºi1 = Œºi2 = ¬∑ ¬∑ ¬∑ = Œºik, then
Pr[ ÀúXbk ‚â§ÀúXb1 + Œ¥] ‚â•P ‚àó.
(2) If Œºi1 + 2Œ¥ = Œºi2 = ¬∑ ¬∑ ¬∑ = Œºik, then
Pr[ ÀúXc2 > ÀúXi1 + Œ¥] ‚â•P ‚àó.
Here ÀúXc2 = mink
l=2 ÀúXil.
(3) In the cases that Œºi1 = Œºi2 = ¬∑ ¬∑ ¬∑ = Œºiv and Œºiv + 2Œ¥ = Œºiv+1 =
¬∑ ¬∑ ¬∑ = Œºik, where 2 ‚â§v ‚â§k ‚àí1. Let ÀúXc1 and ÀúXcv be the smallest
and largest weighted sample mean of systems il for l = 1, 2, . . . , v,
respectively. Let ÀúXcu be the smallest weighted sample mean of systems
il for l = v + 1, v + 2, . . . , k. We have
Pr[ ÀúXcv ‚â§ÀúXc1 + Œ¥] ‚â•P ‚àó
and
Pr[ ÀúXcu > ÀúXc1 + Œ¥] ‚â•P ‚àó.
Recall that ÀúXb1 = mink
l=1 ÀúXil, ÀúXbk = maxk
l=1 ÀúXil, ÀúXc1 = minv
l=1 ÀúXil,
and ÀúXcv = maxv
l=1 ÀúXil. Because ÀúXcv ‚àºGv:v(t) and ÀúXc1 ‚àºG1:v(t) while
ÀúXbk ‚àºGk:k(t) and ÀúXb1 ‚àºG1:k(t) and v < k, Pr[ ÀúXcv ‚â§
ÀúXc1 + Œ¥] ‚â•
Pr[ ÀúXbk ‚â§
ÀúXb1 + Œ¥].
Note that E( ÀúXcv) ‚â§E( ÀúXbk), E( ÀúXb1) ‚â§E( ÀúXc1),
Var( ÀúXcv) ‚â•Var( ÀúXbk), and Var( ÀúXc1) ‚â•Var( ÀúXb1).
Figure 10.2 shows
the empirical distributions of the Ô¨Årst order statistic of k = 1, 5, and 9
N(0, 1) random variables. Hence, the sample sizes that guarantee case 1
(i.e. Pr[ ÀúXbk ‚â§ÀúXb1 + Œ¥] ‚â•P ‚àó) also guarantee Pr[ ÀúXcv ‚â§ÀúXc1 + Œ¥] ‚â•P ‚àó.
Recall that ÀúXc2 = mink
l=2 ÀúXil and ÀúXcu = mink
l=v+1 ÀúXil. Because ÀúXcu ‚àº
G1:k‚àív(t) and ÀúXc1 ‚àºG1:v(t) while ÀúXc2 ‚àºG1:k‚àí1(t) and k ‚àív ‚â§k ‚àí1,
Pr[ ÀúXcu ‚â•ÀúXc1 + Œ¥] ‚â•Pr[ ÀúXc2 ‚â•ÀúXi1 + Œ¥]. Furthermore, E( ÀúXcu) ‚â•E( ÀúXc2),
E( ÀúXi1) ‚â•E( ÀúXc1), Var( ÀúXcu) ‚â•Var( ÀúXc2), and Var( ÀúXi1) ‚â•Var( ÀúXc1). Hence,
the sample sizes that guarantee case 2 (i.e. Pr[ ÀúXc2 > ÀúXi1 + Œ¥] ‚â•P ‚àó) also
guarantee Pr[ ÀúXcu > ÀúXc1 + Œ¥] ‚â•P ‚àó.
Consequently, as long as the Ô¨Årst

Ranking and Selection
185
Fig. 10.2
First order statistic of k N(0, 1) random variables
two cases are satisÔ¨Åed, case 3 will be satisÔ¨Åed as well.
Systems l with
Œºi1 < Œºil ‚â§Œºi1 + 2Œ¥ may be included in the Ô¨Ånal subset, but there is no
probability guarantees.
Furthermore, the probability of systems l being
included in the Ô¨Ånal subset decreases as Œºil deviates farther away from Œºi1.
For case 1, the probability guarantee is achieved when the sample size
for system i nci is computed by Eq. (4.4). We now derive the required
sample size to achieve the probability guarantee for case 2. We can write
P(CD) = Pr[ ÀúXc2 > ÀúXi1 + Œ¥]
= Pr[Ti1 < Tc2 + h1]
=
 ‚àû
‚àí‚àû
F(t + h1)g1:k‚àí1(t)dt.
We equate the right-hand side to P ‚àóand solve for h1. The required sample
size for system i ndi for system i is then computed by Eq. (4.2) with d‚àó
replaced by Œ¥. Note that the h1 values are available from the tables in
[Law
(2014)].
It is known that in this setting ndi ‚â§nci, hence, ni =
max(nci, ndi) = nci.
We now present a cost-eÔ¨Äective sequential approach to select the optimal
subset.
We denote this approach the SOSS (Sequential Optimal Subset
Selection) procedure.
(1) Let Ni,r be the sample size allocated for system i and ¬ØXi,r be the

186
Crafts of Simulation Programming
sample mean of system i at the rth iteration. Simulate n0 samples for
all systems. Set the iteration number r = 0, and N1,r = N2,r = ¬∑ ¬∑ ¬∑ =
Nk,r = n0. Specify the value of the indiÔ¨Äerence amount d‚àóand the
required precision P ‚àó.
(2) Calculate the sample means and sample variances. Rank the sample
means such that ¬ØXb1 ‚â§¬ØXb2 ‚â§. . . ‚â§¬ØXbk.
(3) Calculate the required sample size
Nbl,r+1 = max(n0, ‚åà(2h3Sbl(Nbl,r)/dbl)2‚åâ), for l = 1, 2, . . . , k.
Here, dbl = max(d‚àó, ¬ØXbl ‚àíU( ¬ØXb1)), for l = 1, 2, . . . , k.
(4) If Ni,r+1 ‚â§Ni,r, for i = 1, 2, . . . , k, go to step 6.
(5) Simulate additional ‚åà(Ni,r+1 ‚àíNi,r)+/2‚åâsamples for system i.
Set
r = r + 1. Go to step 2.
(6) Select system bl iÔ¨Ä¬ØXbl ‚â§¬ØXb1 + d‚àó/2.
The critical value h3 depends on k, n0, and P ‚àó. Even though the sample
sizes for each system change at each iteration, we use the initial value of
h3 through all iterations. This simpliÔ¨Åes programming eÔ¨Äorts and provides
conservative estimates of the sample sizes.
10.2.3
Ratio Statistics of Variance of Normally Distributed
Variables
The range statistics and/or the absolute indiÔ¨Äerence zone is applied when
the parameter of interest is a location parameter. On the other hand, the
ratio statistics and/or the relative indiÔ¨Äerence zone needs to be used when
the parameter of interest is a scale parameter, e.g., variance. The variance
of Xij quantiÔ¨Åes the dispersion of Xij and is denoted by Var(Xij) = œÉ2
i
and, if it exists, œÉ2
i = E[(Xij ‚àíŒºi)2], where Œºi = E(Xij) is the mean
of Xij.
Let ni denote the number of samples of the ith system.
The
sample variance with ni observations S2
i (ni) = ni
j=1(Xij‚àí¬ØXi(ni))2/(ni‚àí1)
is the unbiased variance estimator of œÉ2
i and ¬ØXi(ni) = ni
j=1 Xij/ni is
the unbiased mean estimator of Œºi. When Xij‚Äôs are normally distributed,
the variables œái = (ni ‚àí1)S2
i (ni)/œÉ2
i for i = 1, 2, . . . , k are independent
œá2 random variables with ni ‚àí1 degrees of freedom.
It is known that
E(œái) = ni ‚àí1 and Var(œái) = 2(ni ‚àí1). Hence, E(S2
i (ni)) = œÉ2
i and
Var(S2
i (ni)) = 2œÉ4
i /(ni ‚àí1).
Let ne be the sample size of all k systems, i.e., ni = ne for i = 1, 2, . . . , k.
We use the notation S2
i instead of S2
i (ni) for the rest of this section. Let

Ranking and Selection
187
œábl be the lth smallest of œái such that œáb1 ‚â§œáb2 ‚â§¬∑ ¬∑ ¬∑ ‚â§œábk. Because the
distribution of œái for i = 1, 2, . . . , k, is known, given the sample size (i.e.,
the d.f.), the quantile of the distribution of œábk/œáb1 can be determined. For
example, let Œ¥r be the 0.90 quantile of the distribution of œáb2/œáb1 when
k = 2, then Pr[œáb2/œáb1 ‚â§Œ¥r] = 0.90. Hence, we do not reject the null
hypothesis that œÉ2
1 = œÉ2
2 with 0.90 conÔ¨Ådence, when œáb2/œáb1 ‚â§Œ¥r, or
similarly, S2
b2/S2
b1 ‚â§Œ¥r. Furthermore, given a user speciÔ¨Åed Œ¥r, the required
sample size ne to achieve Pr[œáb2/œáb1 ‚â§Œ¥r] = 0.90 can be determined. In
the cases that k > 2, the quantile of the distribution of œábk/œáb1 can be used
to test the null hypothesis that œÉ2
1 = œÉ2
2 = ¬∑ ¬∑ ¬∑ = œÉ2
k.
We now assess P(CS) of this selection problem, i.e., to select the optimal
subset of systems having the smallest variances from k alternatives. Let S2
b1
and S2
bk, respectively, be the smallest and the largest sample variances from
œÉ2
i for i = 1, 2, . . . , k and let œÉ2
b1 and œÉ2
bk, respectively, be their unknown
true variances. That is, S2
b1 and S2
bk are, respectively, the Ô¨Årst and the
kth order statistics. Note that systems b1 and bk are likely to be diÔ¨Äerent
in diÔ¨Äerent replications. Furthermore, let Œ¥r > 1 be a user speciÔ¨Åed upper
bound of the ratio for the hypothesis test. System i having S2
i /S2
b1 < Œ¥r will
be considered feasible, i.e., there is no signiÔ¨Åcant diÔ¨Äerence between œÉ2
i and
œÉ2
b1. Then, under the (equivalence) null hypothesis that œÉ2
1 = œÉ2
2 = ¬∑ ¬∑ ¬∑ = œÉ2
k
(and consequently, œÉ2
b1 = œÉ2
bk),
P(CS) = Pr[S2
bk/S2
b1 < Œ¥r]
= Pr

(ne ‚àí1)S2
bk/œÉ2
bk
(ne ‚àí1)S2
b1/œÉ2
b1
œÉ2
bk
œÉ2
b1
< Œ¥r

= Pr [œábk/œáb1 < Œ¥r] .
The third equality follows because œábk = (ne ‚àí1)S2
bk/œÉ2
bk, and œáb1 =
(ne ‚àí1)S2
b1/œÉ2
b1.
Note that œáb1 and œábk are, respectively, the Ô¨Årst and
the kth order statistics of (ne ‚àí1)S2
i /œÉ2
i for i = 1, 2, . . . , k.
Let œâ and
Œ©, respectively, denote the pdf and cdf of the œá2 distribution with ne ‚àí1
d.f. Then, œáb1 ‚àºG1:k(œá) and œábk ‚àºGk:k(œá) with f and F, respectively,
replaced by œâ and Œ©. Figure 10.3 shows the empirical distributions of the
Ô¨Årst order statistic of k = 1, 5, and 9 œá2 (with 19 d.f.) random variables.
Because œáb1 and œábk are no longer œá2 distributed when k ‚â•2, the
random variable œábk/œáb1 is no longer F-distributed. Nevertheless,
P(CS) ‚â•
 ‚àû
0
Gk:k(œáb1Œ¥r)g1:k(œáb1)dœáb1.
Let jl be the system having the lth smallest variance such that œÉ2
j1 ‚â§œÉ2
j2 ‚â§
¬∑ ¬∑ ¬∑ ‚â§œÉ2
jk. Let d‚àó
r > 1 be the relative indiÔ¨Äerence amount, i.e., we will not

188
Crafts of Simulation Programming
Fig. 10.3
First order statistic of k œá2 (19 d.f.) random variables
distinguish system j1 and systems i having œÉ2
i < œÉ2
j1d‚àó
r. The LFC (least
favorable conÔ¨Åguration) of relative indiÔ¨Äerence zone is œÉ2
j1d‚àó
r = œÉ2
j2 = ¬∑ ¬∑ ¬∑ =
œÉ2
jk.
Given the user speciÔ¨Åed Œ¥r, we equate the right-hand side to P ‚àóto
determine the required d.f. of the underlying œá2 distribution Œ©, i.e., the
required sample size ne. To perform the null hypothesis test that all k vari-
ances are equal, we obtain the required sample size ne given the variances
ratio Œ¥r. Simulate ne samples for each system and compute the sample
variances S2
i for i = 1, 2, . . . , k. We do not reject the null hypothesis that
œÉ2
1 = œÉ2
2 = ¬∑ ¬∑ ¬∑ = œÉ2
k when S2
bk/S2
b1 < Œ¥r.
For the un-equivalence cases, under the LFC, we would like to ensure
that system jl for l = 2, 3, . . . , k will not be accepted. Let S2
c2 = mink
l=2 S2
jl
and let nf denote the required sample size of each system. We can write
P(CS) = Pr[S2
c2/S2
j1 ‚â•Œ¥r]
= Pr

(nf ‚àí1)S2
c2/œÉ2
c2
(nf ‚àí1)S2
j1/œÉ2
j1
œÉ2
c2
œÉ2
j1
‚â•Œ¥r

= Pr [œác2 ‚â•œáj1Œ¥r/d‚àó
r]
= Pr [œáj1/œác2 ‚â§d‚àó
r/Œ¥r] .
The third equality follows because œác2 = (nf ‚àí1)S2
c2/œÉ2
c2 and œáj1 = (nf ‚àí
1)S2
j1/œÉ2
j1. Note that œác2 is the Ô¨Årst order statistic of (nf ‚àí1)S2
i /œÉ2
i for

Ranking and Selection
189
i = 2, 3, . . . , k.
We equate the right-hand side to P ‚àóto determine the
required d.f. of the underlying œá2 distribution, i.e., the required sample size
nf. Note that the sample sizes ne and nf can be estimated by the procedure
of [Chen and Li (2010)]. When the absolute indiÔ¨Äerence amount is d‚àó, we
set the amount for comparison with a control Œ¥ = d‚àó/2. When the relative
indiÔ¨Äerence amount is d‚àó
r, we set the amount for comparison with a control
Œ¥r =

d‚àór. In this setting,
P(CS) ‚â•
 ‚àû
0
Œ©(œác2Œ¥r)g1:k‚àí1(œác2)dœác2.
The underlying pdf (œâ) and cdf (Œ©) of the order-statistic distribution
g1:k‚àí1(x) are the œá2 distribution with nf ‚àí1 d.f. Consequently, if we com-
pute the sample variances with n = max(ne, nf) samples and select systems
i having S2
i /S2
b1 < Œ¥r, then the selected subset contains only and/or all the
best systems with probability no less than P ‚àó. Again, good systems i hav-
ing œÉ2
i < œÉ2
j1d‚àó
r may be included in the selected subset, however, there is
no probability guarantees. See [Chen (2008)] for an example of selecting a
subset up to size m contains at least c of the v systems having the smallest
variances from k alternatives.
In cases of the relative indiÔ¨Äerence zone, the single-stage procedure al-
locates sample sizes equally among systems with the Ô¨Åxed total sample size
N = kn0. Under the non-LFC, allocating sample sizes equally among sys-
tems is not optimal. Let Yi be the estimate of the performance measure
of system i and Yb1 ‚â§Yb2 ‚â§¬∑ ¬∑ ¬∑ ‚â§Ybk. [Chen (2008)] proposes allocating
sample sizes according to the following rules to increase P(CS):
nbi
nbj
=
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
Ybi
Ybj
1 ‚â§i, j ‚â§v,
1
v ‚â§i, j ‚â§v + 1,
Ybj
Ybi
v + 1 ‚â§i, j ‚â§k.
(10.5)
The two-stage selection procedure proceeds as follows:
(1) Specify the maximum total sample size N and the initial sample size
n0 < N/k.
(2) Simulate n0 samples for each system i = 1, 2, . . . , k.
(3) Compute the estimate Yi with n0 samples for each system i =
1, 2, . . . , k.
(4) Rank the Yi and allocate ni according to the ratios of Eq. (10.5). Note
that k
i=1 ni = N.
(5) Simulate additional max(0, ni ‚àín0) samples for each system i =
1, 2, . . . , k.

190
Crafts of Simulation Programming
(6) Compute and rank the estimate Yi with ni samples for each system
i = 1, 2, . . . , k.
(7) Rank the sample variances such that S2
c1(nc1) ‚â§S2
c2(nc2) ‚â§¬∑ ¬∑ ¬∑ ‚â§
S2
ck(nck). Return the set {c1, c2, . . . , cm}.
Note that the ratios of Eq. (10.5) may not be precisely achieved when the
speciÔ¨Åed N is not large enough. This is because the procedure can not take
back the computation budget that has already been spent. Furthermore,
the two-stage procedure can easily be extended to a sequential procedure.
For example, instead of allocating all the additional samples in the second
stage, only a portion of additional samples can be allocated and used to
recompute the ratios as the procedure proceeds.
10.2.4
Multiple Comparisons with the Best
Multiple comparisons provide simultaneous conÔ¨Ådence intervals on selected
diÔ¨Äerences among the systems. It is known that indiÔ¨Äerence-zone selection
procedures also guarantee that the c.i. coverage of multiple comparisons
with the best (MCB) to have the same conÔ¨Ådence level of the selection
procedures. These c.i.‚Äôs bound the diÔ¨Äerences between the performance of
each system and the best of the others with a pre-speciÔ¨Åed conÔ¨Ådence level.
Let wij be the one-tailed P = 1 ‚àí(1 ‚àíP ‚àó)/(k ‚àí1) c.i. half-width. These
MCB c.i.‚Äôs are
Pr[Œºi ‚àímin
jÃ∏=i Œºj ‚àà[max
jÃ∏=i ( ¬ØXi ‚àí¬ØXj ‚àíwij)‚àí, max
jÃ∏=i ( ¬ØXi ‚àí¬ØXj + wij)+], ‚àÄi] ‚â•P ‚àó.
Here (x)‚àídenotes min(0, x) and (x)+ denotes max(0, x).
DeÔ¨Åne the events
E = {Œºi ‚àíŒºi1 ‚â§¬ØXi ‚àí¬ØXi1 + wii1, ‚àÄi Ã∏= i1},
EL = {Œºi ‚àímin
jÃ∏=i Œºj ‚â•max
jÃ∏=i ( ¬ØXi ‚àí¬ØXj ‚àíwij)‚àí, ‚àÄi},
EU = {Œºi ‚àímin
jÃ∏=i Œºj ‚â§max
jÃ∏=i ( ¬ØXi ‚àí¬ØXj + wij)+, ‚àÄi},
ET = {Œºi ‚àímin
jÃ∏=i Œºj ‚àà[ max
jÃ∏=i ( ¬ØXi ‚àí¬ØXj ‚àíwij)‚àí, max
jÃ∏=i ( ¬ØXi ‚àí¬ØXj + wij)+], ‚àÄi}.
Note that E is the event that the upper one-tailed conÔ¨Ådence intervals
for MCC (Multiple Comparison with a Control), with the control being
design i1, contain all of the true diÔ¨Äerences Œºi ‚àíŒºi1. Since Pr[Œºi ‚àíŒºi1 ‚â§
¬ØXi‚àí¬ØXi1 +wii1] ‚â•P ‚àÄi, Pr[E] ‚â•P ‚àó. Now following an argument developed

Ranking and Selection
191
by [Edwards and Hsu
(1983)], we have that E ‚äÇEL ‚à©EU, which will
establish the result Pr[ET ] ‚â•P ‚àó.
First we prove that E ‚äÇEL:
E ‚äÇ{Œºi1 ‚àíŒºj ‚â•¬ØXi1 ‚àí¬ØXj ‚àíwi1j, ‚àÄj Ã∏= i1}
‚äÇ{Œºi1 ‚àíŒºi2 ‚â•¬ØXi1 ‚àí¬ØXj ‚àíwi1j, ‚àÄj Ã∏= i1}
‚äÇ{Œºi ‚àíŒºi2 ‚â•max
jÃ∏=i ( ¬ØXi ‚àí¬ØXj ‚àíwij)‚àí, ‚àÄi}
‚äÇ{Œºi ‚àímin
jÃ∏=i Œºj ‚â•max
jÃ∏=i ( ¬ØXi ‚àí¬ØXj ‚àíwij)‚àí, ‚àÄi},
where the second step follows since Œºi1 ‚àíŒºi2 ‚â•Œºi1 ‚àíŒºj for all j Ã∏= i1 and
the third step follows since Œºi ‚àíŒºi2 ‚â•0 for all i Ã∏= i1 and (x)‚àí‚â§0.
Now we show E ‚äÇEU.
E ‚äÇ{Œºi ‚àíŒºi1 ‚â§max
jÃ∏=i ( ¬ØXi ‚àí¬ØXj + wij), ‚àÄi Ã∏= i1}
‚äÇ{Œºi ‚àímin
jÃ∏=i Œºj ‚â§max
jÃ∏=i ( ¬ØXi ‚àí¬ØXj + wij)+, ‚àÄi},
where the Ô¨Årst step follows since maxjÃ∏=i( ¬ØXi‚àí¬ØXj+wij) ‚â•¬ØXi‚àí¬ØXi1 +wii1 for
all i Ã∏= i1 and the last step follows since Œºi1 ‚àíminjÃ∏=i1 Œºj ‚â§0 and (x)+ ‚â•0.
Hence, E ‚äÇEL ‚à©EU, and the proof is complete. See [Edwards and Hsu
(1983)] for more detail on multiple comparisons.
IndiÔ¨Äerence-zone procedures derived based on the LFC achieve wij ‚â§d‚àó
when c = v = m = 1 and the MCB c.i.‚Äôs are simpliÔ¨Åed to
Pr[Œºi ‚àímin
jÃ∏=i Œºj ‚àà[(ÀÜŒºi ‚àímin
jÃ∏=i ÀÜŒºj ‚àíd‚àó)‚àí, (ÀÜŒºi ‚àímin
jÃ∏=i ÀÜŒºj + d‚àó)+], ‚àÄi] ‚â•P ‚àó.
However, these tight c.i‚Äôs come at a cost. Our procedure takes into account
the diÔ¨Äerences of sample means, hence, the c.i. half-width wii1 is around
max(d‚àó, Œºi ‚àíŒºi1) instead of d‚àó.
Recall that the objective is to select a
good design and not to estimate the diÔ¨Äerence of sample means. This is
consistent with the philosophy of ordinal comparison [Ho et al. (1992)] that
it is advantageous to rank estimators while the precision of the estimates
are still poor when the objective is to select a good design.
10.3
Lognormally Distributed Samples
Let N(Œº, œÉ2) denote the normal distribution with mean Œº and variance œÉ2.
Let LN(Œº, œÉ2) denote the lognormal distribution with parameter Œº and œÉ2.
The variable X is LN(Œº, œÉ2) only if the variable Y = ln X is N(Œº, œÉ2).
Note that the mean of LN(Œº, œÉ2) is exp(Œº + œÉ2/2). It is clear that unless

192
Crafts of Simulation Programming
œÉ2 is known, inferences based on Œº alone from the transformed variable Y
are not suÔ¨Écient for the inference on a parameter that is a function of both
Œº and œÉ2.
Consider k independent, lognormally-distributed random variables with
parameters Œºi and œÉ2
i , i = 1, 2, . . . , k. It is assumed that Œºi and œÉ2
i are
unknown and unequal. [John and Chen (2006)] procedure can select the
system having the largest linear combination of Œºi and œÉ2
i , i.e., the largest
Œ∏i(a, b) = exp(aŒºi+bœÉ2
i ). We are interested in selecting the system with the
largest mean, i.e., we limit the discussion to the case that Œ∏i = Œ∏i(1, 1/2).
However, the techniques proposed here can be used to select the largest
Œ∏i(a, b) for any combinations of a and b as well.
Since Œ∏i is a scale parameter of the exponential function exp, we apply
the relative indiÔ¨Äerence amount. Let Œ∏il be the lth largest mean such that
0 < Œ∏i1 ‚â§Œ∏i2 ‚â§. . . ‚â§Œ∏ik. If the ratio 1 ‚â§Œ∏ik/Œ∏ik‚àí1 < d‚àó
r, we say that
the systems are in the (relative) indiÔ¨Äerence zone for correct selection. On
the other hand, if the ratio Œ∏ik/Œ∏ik‚àí1 ‚â•d‚àó
r, we say that the systems are in
the preference zone for correct selection. Furthermore, the minimal P(CS)
(probability of CS) occurs under the LFC, i.e., Œ∏i1 = Œ∏i2 = . . . = Œ∏ik‚àí1 =
Œ∏ik/d‚àó
r.
Let Yij = ln Xij. We then compute ¬ØYi (sample mean of Yij) and S2
i , the
unbiased estimator of Œºi and œÉ2
i , from all samples. Let ÀÜŒ≤i = ln ÀÜŒ∏i = ¬ØYi +
S2
i /2, and select the systems with the largest ÀÜŒ≤i as the one having mean Œ∏ik.
Note that ÀÜŒ≤i is strictly increasing of ÀÜŒ∏i. Furthermore, Œ≤i = ln Œ∏i = Œºi +œÉ2
i /2
and under the LFC Œ≤ik ‚àíd‚àó
a = Œ≤il for l = 1, 2, . . . , k ‚àí1. That is, the
indiÔ¨Äerence amount d‚àó
a has been transformed from the relative form (i.e.,
Œ∏ik/Œ∏ik‚àí1 = d‚àó
r) to the absolute form (i.e., Œ≤ik ‚àíŒ≤ik‚àí1 = d‚àó
a). Note that in
this case d‚àó
a = ln d‚àó
r.
10.3.1
The Property of the Constant hL
In this section, we deÔ¨Åne the constant hL and describe an approach to Ô¨Ånd
its value. Without loss of generality, assume that the systems are under the
LFC and system ik is the best system. Let S2
i,0 be the unbiased estimator
of œÉ2
i with n0 observations. For i = 1, 2, . . . , k, let
Wi = œÉ2
i + œÉ4
i /2
ni
, and wi = œÉ2
i
S2
i,0
+ œÉ4
i
S4
i,0
.

Ranking and Selection
193
For l = 1, 2, . . . , k ‚àí1, let
Zil = (ÀÜŒ≤il ‚àíÀÜŒ≤ik) + d‚àó
a

Wil + Wik
,
Dil =
d‚àó
a

Wil + Wik
,
and
Qil =
hL
‚àöwil + wik
.
We can write the probability of correct selection as:
P(CS) = Pr[ÀÜŒ≤il < ÀÜŒ≤ik, l = 1, 2, . . . , k ‚àí1]
= Pr[Zil < Dil, l = 1, 2, . . . , k ‚àí1]
‚â•Pr[Zil < Qil, l = 1, 2, . . . , k ‚àí1].
The inequality follows because nil (the sample size of system il) is deter-
mined such that nil ‚â•max((hLSil,0/d‚àó
a)2, (hLS2
il,0/d‚àó
a)2/2) and Dil ‚â•Qil;
see [John and Chen (2006)]. Note that Dil and Qil can be considered as
the one-tailed (P ‚àó)1/(k‚àí1) conÔ¨Ådence interval half width of Zil.
Let Œ¶ denote the cdf of the standard normal distribution. Then
Pr[Zil < Qil, l = 1, 2, . . . , k ‚àí1]
= E[Pr[Zil < Qil, l = 1, 2, . . . , k ‚àí1|S2
i1,0, S2
i2,0, . . . , S2
ik,0]]
‚â•E[Œ†k‚àí1
l=1 Œ¶(Qil)].
The inequality follows from Slepian‚Äôs inequality [Tong (1980)] because Zil
for l = 1, 2, . . . , k ‚àí1 are correlated.
Furthermore, under the LFC and
conditioning on S2
i1,0, S2
i2,0, . . . , S2
ik,0, Zil follows N(0, 1) asymptotically.
Let œâ denote the pdf of the œá2 distribution with n0 ‚àí1 d.f. Let
Y =
hL

(n0 ‚àí1)(1/x + 1/y) + (n0 ‚àí1)2(1/x2 + 1/y2)
.
Based on the property that the variables œáil = (n0 ‚àí1)S2
il,0/œÉ2
il, l =
1, 2, . . . , k are independent œá2 variables with n0 ‚àí1 d.f., it can be shown
that
E[Œ†k‚àí1
l=1 Œ¶(Qil)] =
E
‚é°
‚é£Œ†k‚àí1
l=1 Œ¶
‚éõ
‚éù
hL

(n0 ‚àí1)(1/œáil + 1/œáik) + (n0 ‚àí1)2(1/œá2
il + 1/œá2
ik)
‚éû
‚é†
‚é§
‚é¶

194
Crafts of Simulation Programming
and
P(CS) ‚â•
 ‚àû
0
$ ‚àû
0
Œ¶ (Y ) œâ(x)dx
%k‚àí1
œâ(y)dy.
We set the right-hand side to P ‚àóand solve for hL. Unfortunately, analytical
solutions to compute hL from the above equation are diÔ¨Écult to obtain. We
use another property of hL to estimate its values. It can be shown that
P(CS)
‚â•Pr[Zil < Qil, l = 1, 2, . . . , k ‚àí1]
= Pr[Zil

œÉ2
il/S2
il,0 + œÉ4
il/S4
il,0 + œÉ2
ik/S2
ik,0 + œÉ4
ik/S4
ik,0 < hL,
l = 1, 2, . . . , k ‚àí1]
= Pr[Zil

(n0 ‚àí1)(1/œáil + 1/œáik) + (n0 ‚àí1)2(1/œá2
il + 1/œá2
ik) < hL,
l = 1, 2, . . . , k ‚àí1].
Let
Œ• =
k‚àí1
max
l=1 Zil

(n0 ‚àí1)(1/œáil + 1/œáik) + (n0 ‚àí1)2(1/œá2
il + 1/œá2
ik),
then
P(CS) ‚â•Pr[Œ• < hL] = P ‚àó.
That is, hL is the P ‚àóquantile of the distribution of the variable Œ• under the
LFC. Consequently, we can use simulation procedures of estimating quan-
tile to estimate the value of hL. This approach is easier than the numerical
integration because quantile estimates of independent and identically dis-
tributed (i.i.d.)
samples can be easily obtained through order statistics
without any complicated operations.
Note that variables Zil for l = 1, 2, . . . , k ‚àí1 are correlated, and the
correlation coeÔ¨Écients are unknown. Let Œ∂il for l = 1, 2, . . . , k ‚àí1 be i.i.d.
N(0,1) variables and
œÑ =
k‚àí1
max
l=1 Œ∂il

(n0 ‚àí1)(1/œáil + 1/œáik) + (n0 ‚àí1)2(1/œá2
il + 1/œá2
ik).
Follow from Slepian‚Äôs [Tong (1980)] inequality
P(CS) ‚â•Pr[Œ• < hL] ‚â•Pr[œÑ < hL].
A cost-eÔ¨Äective sequential approach to select the best system from k
lognormal populations.

Ranking and Selection
195
The Sequential Lognormal Selection algorithm:
(1) Initialize the set I to include all k lognormal populations. Simulate n0
samples for each system i ‚ààI. Set the iteration number r = 0, and
N1,r = N2,r = . . . = Nk,r = n0, where Ni,r is the sample size allocated
for system i in the rth iteration.
(2) Compute the incremental number of samples for system i in the (r+1)th
iteration Œ¥i,r+1 = ‚åà(max((hLSi,r/ ÀÜdi,r)2, (hLS2
i,r/ ÀÜdi,r)2/2) ‚àíNi,r)+/2‚åâ,
where ÀÜdi,r = max(d‚àó
a, L(ÀÜŒ≤b,r) ‚àíÀÜŒ≤i,r).
(3) If Œ¥i,r+1 = 0 and i Ã∏= b (where ÀÜŒ≤b,r = maxi‚ààI ÀÜŒ≤i,r), remove system i
from the subset I.
(4) If there is only one element in the subset I, go to step 6.
(5) Simulate Œ¥i,r+1 additional samples for each system i ‚ààI in the (r+1)th
iteration. Set r = r + 1 and go to step 2.
(6) Return the values b and ÀÜŒ≤b, where ÀÜŒ≤b = maxk
i=1 ÀÜŒ≤i.
10.4
Other Approach of Selection Procedures
The indiÔ¨Äerence-zone selection procedures described so far attempt to min-
imize the sample sizes given the required minimal P(CS). Another goal
is to maximize the P(CS) given a Ô¨Åxed sample size, e.g., OCBA (Optimal
Computing Budget Allocation, see Section 11.2.3 and [Chen et al. (2000)]).
[Chen (2004); Chen and Kelton (2005)] compare the approach of OCBA
and the enhanced indiÔ¨Äerence-zone selection procedures (when the under-
lying variables are normally distributed). The resulting ratios of the al-
located sample sizes of OCBA and the enhanced selection procedures are
close (when the underlying variables are normally distributed). In fact, the
problem of optimizing P(CS) is the dual of optimizing sample sizes (without
the indiÔ¨Äerence-zone approach) and has similar solutions.
Another approach to solve ranking and selection is to use the prop-
erty of Brownian motion process (see, e.g., [Andrad¬¥ottir and Kim (2010)]).
They call this kind of procedures ‚Äúfully sequential procedure.‚Äù The proce-
dures increase one sample of each system that is still under consideration at
each iteration and eliminate systems from further simulation as procedures
proceed. These procedures are eÔ¨Écient in terms of sample sizes, but not
necessarily in terms of runtime.
Furthermore, several approaches have been proposed to select systems
when there are multiple objectives, which will be discussed in Chapter 14.

196
Crafts of Simulation Programming
Table 10.1
ÀÜP(CS) and sample sizes for experiment 1
P ‚àó= 0.90
P ‚àó= 0.95
Procedure
ÀÜP(CS)
T
std(T)
ÀÜP(CS)
T
std(T)
SRS(20)
0.9925
1243
268
0.9956
1567
309
SRS(30)
0.9946
1221
253
0.9969
1517
294
10.5
Empirical Experiments
In this section, we present some empirical results of using the proposed
framework to select the best system(s) from normal, exponential, and log-
normal populations.
10.5.1
Experiment 1: Normal Populations
There are ten alternative designs in the selection subset. Suppose Xij ‚àº
N(i, 62), i = 1, 2, . . . , 10, where N(Œº, œÉ2) denotes the normal distribution
with mean Œº and variance œÉ2. We want to select a design with the minimum
mean: design 1. The indiÔ¨Äerence amount d‚àóis set to 0.90 for all cases. We
list the actual P(CS) of the SRS procedure. We use two diÔ¨Äerent initial
number of replications, n0 = 20 and 30. Furthermore, 10,000 independent
experiments are performed to estimate the actual P(CS) by ÀÜP(CS): the
proportion of the 10,000 experiments in which we obtained the correct
selection.
The results of experiment 1 are summarized in Table 10.1.
The
ÀÜP(CS) column lists the proportion of correct selection.
The T column
lists the average of the total number of simulation replications (T
=
10000
r=1
10
i=1 Tr,i/10000, and Tr,i is the number of total replications or
batches for design i at the rth simulation run) used in each procedure.
The std(T) column lists the standard deviation of the number of total sam-
ple sizes T at each independent simulation run. The SRS(20) row lists the
results of the procedure executed with initial replications n0 = 20 (and
similarly for n0 = 30).
The ÀÜP(CS)‚Äôs are all larger than the speciÔ¨Åed P ‚àó. Because the variance of
the sample is larger with a smaller initial sample size n0, the SRS procedure
allocates more samples with smaller n0.

Ranking and Selection
197
10.5.2
Experiment 2: Exponential Populations
Let expo(Œ≤) denote the exponential distribution with parameter Œ≤. There
are k = 10 alternatives with distribution expo(Œ≤i) for i = 1, 2, . . . , k and
the indiÔ¨Äerence amount is d‚àó
r = 1.4. Note that the parameter Œ≤ of the ex-
ponential distribution is a scale parameter. Hence, the relative indiÔ¨Äerence
zone should be used. The objective is to select a subset of size 3 containing
the best 3 alternatives.
To estimate the h value, set Œ≤il = 1.0 for l = 1, 2, 3 and Œ≤il = 1.4
for l = 4, 5 . . . , k. Let ¬ØXil(n) for l = 1, 2, . . . , k be the sample means ob-
tained with n samples from each system.
Let ¬ØXc = max3
l=1 ¬ØXil(n) and
¬ØXu = min10
l=4 ¬ØXil(n). When n = 1, the 0.01786 quantile of the distribution
of œï = ¬ØXc/ ¬ØXu is 1.0. In other words, with only one observation from each
system the P(CS) is only 0.01786. From these experiments, it is observed
that the 0.90 quantile of the random variable œï (i.e., the h value) is ap-
proximately 0.999378 when n = 108. That is, Pr[ ¬ØXc < 0.999378 ¬ØXu] ‚âà0.90.
Consequently, Pr[ ¬ØXc < ¬ØXu] will be slightly greater than 0.90.
The subset selection procedure is then tested under the LFC, i.e., as-
suming Œ≤il = 1.0 for l = 1, 2, 3 and Œ≤il = 1.4 for l = 4, 5, . . . , k. In each
replication, 108 samples are simulated to compute the sample means for
each system.
The three systems having the smallest sample means are
then selected as the best three systems. 10,000 independent replications
are performed and the observed P(CS) is the proportion of independent
replications that the procedure correctly selected systems il for l = 1, 2, 3.
The observed P(CS) is 0.8990, slightly below the nominal value of 0.90.
The selection was also performed with the two-stage procedure described
in Section 10.2.3, i.e., the ratios of the allocated sample sizes are similar to
those of Eq. (10.5). The total sample size was Ô¨Åxed to 1080 with the initial
sample size n0 = 20. The observed P(CS) is 0.9145, slighter higher than
the single-stage procedure.
10.5.3
Experiment 3: Lognormal Populations
In this experiment, we test the procedure with the systems under the LFC,
i.e., Œ≤1 = Œ≤2 = Œ≤3 ‚àíd‚àó
a = 1.0. We chose the Ô¨Årst-stage sample size to be
n0 = 5, 15, 25, 35 and 45. The number of systems under consideration is
k = 3. The indiÔ¨Äerence amount, d‚àó
a, is set to 0.5. The required minimal
P(CS), P ‚àó, is set to 0.90. Table 10.2 lists the conÔ¨Åguration of the mean
and variance. We test the LFC because the minimal P(CS) occurs under

198
Crafts of Simulation Programming
Table 10.2
Parameter conÔ¨Åguration of experiment 3
Parameter
Œº1
Œº2
Œº3
œÉ1
œÉ2
œÉ3
Value
-0.125
-0.28
0.055
1.5
1.6
1.7
Table 10.3
The observed P(CS) with P ‚àó= 0.90 and k = 3
JC
SLS
n0
T 1
T 2
T 3
ÀÜP(CS)
T 1
T 2
T 3
ÀÜP(CS)
Iter
5
620
735
907
0.949
394
529
752
0.985
10
15
173
213
276
0.930
117
152
250
0.961
9
25
146
180
229
0.929
104
134
210
0.933
8
35
134
167
213
0.935
101
123
196
0.933
8
45
126
162
205
0.945
99
120
190
0.920
8
this conÔ¨Åguration. Note that the greatest beneÔ¨Åt of the SLS (Sequential
Lognormal Selection) procedure will be fully realized under the non-LFC.
Table 10.3 lists the results from [John and Chen (2006)] (denoted JC)
procedure and the SLS procedure. The n0 column is the initial sample size.
The T i columns list the average of the sample sizes allocated for system i.
The ÀÜP(CS) column lists the observed P(CS), i.e., the proportion that system
3 is selected. The iter column lists the average number of iterations of the
SLS procedure. These results are based on 1,000 independent simulation
runs. All observed P(CS)‚Äôs are greater than the speciÔ¨Åed nominal value
of 0.90, which indicates that these procedures are conservative even under
the LFC. The SLS procedure generally achieves the speciÔ¨Åed P(CS) with
smaller sample sizes than the JC procedure.
Furthermore, the invoked
number of iterations by the SLS procedure is small.
It seems that the SLS procedure is likely to over-estimate the variance
with a smaller initial sample size, thereby allocating larger than necessary
sample sizes and achieving the P(CS) that is much greater than the nominal
value. As the initial sample size increases, the variance estimates become
more accurate and the observed P(CS)‚Äôs become closer to the nominal value.
Moreover, as variance estimates become more accurate, the diÔ¨Äerence of the
allocated sample sizes between SLS and JC becomes smaller.
Under the LFC, the reduction of sample sizes is mainly the result of se-
quentializing the procedure because ÀÜdlk ‚âàd‚àó
a. The sequentialized procedure
reduces the frequency of over-allocating samples.

Ranking and Selection
199
10.6
Summary
This chapter has presented a framework for selecting the best system or a
subset of the best systems. The framework can be applied to the normal
and lognormal populations and potentially provide helpful insights and ad-
ditional capabilities to existing ranking and selection processes. In cases
that the underlying distributions are unknown, users can use batch means
to ‚Äúmanufacture‚Äù approximately i.i.d. normal samples and apply selection
procedures that are developed for normal populations. The eÔ¨Éciency of se-
lection procedures can be increased by using variance reduction techniques,
e.g., common random numbers.

Chapter 11
Computing Budget Allocation of
Selection Procedures
One crucial element in R&S procedures is the event of ‚Äúcorrect selection‚Äù
(CS) of the true best system. In a stochastic simulation, the possibility of
CS, denoted by P(CS), increases as the sample sizes become larger. Most
indiÔ¨Äerence-zone-selection procedures are directly or indirectly based on the
procedures of [Dudewicz and Dalal (1975)] and [Rinott (1978)]. However,
these procedures determine the number of additional replications based on
a conservative LFC assumption and do not take into account the value of
sample means. If the accuracy requirement is high and the total number
of designs in a decision problem is large, then the total simulation cost can
easily become prohibitively high.
Some new approaches, such as Optimal Computing Budget Allocation
(OCBA) [Chen et al.
(2000)] and the Enhanced Two-Stage Selection
(ETSS) procedure [Chen and Kelton (2005)], incorporate Ô¨Årst-stage sample
mean information with sample variance in determining the number of ad-
ditional replications. In numerical testing, both OCBA and ETSS demon-
strate a signiÔ¨Åcant reduction in computing eÔ¨Äort compared to Rinott‚Äôs pro-
cedure. The basic idea of those procedures is that to ensure a high proba-
bility of correctly selecting a good design, a larger portion of the computing
budget should be allocated to those designs that are critical in the process
of identifying good designs. Overall simulation eÔ¨Éciency is improved as less
computational eÔ¨Äort is spent on simulating non-critical designs and more
is spent on critical designs.
In this chapter, we investigate the sample size allocation strategy of
several selection procedures. We focus on those procedures which intend
to allocate simulation trials to designs in a way that maximizes P(CS)
within a given computing budget. Other researches have previously exam-
ined various approaches for eÔ¨Éciently allocating a Ô¨Åxed computing budget
201

202
Crafts of Simulation Programming
across design alternatives, in particular [Chen et al. (2000)]. Traditional
indiÔ¨Äerence-zone-selection procedures and optimal computing budget allo-
cations have been treated as two completely separate approaches. [Chen
and Kelton (2005)] demonstrate that these two classes of approaches ob-
tain very similar results. Based on those Ô¨Åndings, we examine the rela-
tionship between P(CS) and sample size allocation strategy and develop
an approach for solving the budget allocation problem. The new proce-
dure not only oÔ¨Äers analyst more options without losing anything in terms
of computational/statistical eÔ¨Éciency but also provides some intuition and
insight of existing procedures. For sequential procedures to work eÔ¨Éciently,
a good incremental sample size must be used. We develop a new strategy to
calculate the incremental size dynamically at each iteration to further im-
prove the eÔ¨Éciency of these procedures. The proposed approach is simple,
general, practical and complementary to other techniques.
11.1
Problem Statement
There exists a large literature on assessing P(CS) based on classical statis-
tical models. To facilitate the derivation of our approximation of P(CS), we
assume the means and variances are known. Let œÜ(x) and Œ¶(x) denote the
probability density and distribution function, respectively, of the standard
normal distribution. Let Œ¥il = Œºil ‚àíŒºi1 for l = 2, 3, . . . , k, and Œ¥i1 = Œ¥i2.
Then
P(CS) = P[ ¬ØXi1 < ¬ØXil, for l = 2, 3, . . . , k]
= P[ ¬ØXi1 ‚àí¬ØXil + Œ¥il < Œ¥il, for l = 2, 3, . . . , k]
‚â•Œ†k
l=2P[ ¬ØXi1 ‚àí¬ØXil + Œ¥il < Œ¥il]
= Œ†k
l=2Œ¶(Œ¥il/

œÉ2
il/Nil + œÉ2
i1/Ni1)
= Œ†k
l=2Œ¶(Yil).
The inequality follows from Slepian‚Äôs inequality [Tong
(1980)] since the
values ¬ØXi1 ‚àí¬ØXil are positively correlated.
The second to last equality
follows from the fact that the variate
Zil =
¬ØXi1 ‚àí¬ØXil + Œ¥il

œÉ2
il/Nil + œÉ2
i1/Ni1
has a N(0, 1) distribution. The last equality follows since
Yil = Œ¥il/

œÉ2
il/Nil + œÉ2
i1/Ni1.

Computing Budget Allocation of Selection Procedures
203
[Chen (2004)] develops a normal approximated two-stage selection pro-
cedure (NTSS). The result is summarized as follows:
Theorem 11.1. For k competing designs whose performance measure Xij
are normally distributed with means Œº1, Œº2, . . . , Œºk and unknown variances
that need to be estimated by sample variances S2
1(r), S2
2(r), . . . , S2
k(r), where
r is the current sample size, P(CS) will be at least P ‚àówhen the sample size
for design i is
Ni = max(r, ‚åà(htSi(r)/di)2‚åâ), for i = 1, 2, . . . , k,
where the critical value ht =
‚àö
2tP,r‚àí1, P
= (P ‚àó)1/(k‚àí1), and di =
max(d‚àó, Œºi ‚àíŒºi1).
When Œºi2 ‚â•Œºi1 + d‚àó, the allocated sample sizes guarantee Œ¶(Yil) ‚â•
(P ‚àó)1/(k‚àí1) for l = 2, 3, ¬∑ ¬∑ ¬∑ , k.
Hence P(CS) ‚â•Œ†k
l=2Œ¶(Yil) ‚â•P ‚àó.
In
practice, however, the true means are unknown, sample means are used to
estimate di, e.g., Eq. (10.1).
11.2
A Heuristic Computing Budget Allocation Rule
While the allocated sample sizes in the previous section guarantee P(CS)
‚â•P ‚àówhen true means are known, we don‚Äôt know whether they are optimal.
We aim to derive sample size allocation rules by considering the following
optimization problem:
min
k

i=1
Ni
subject to
k

l=2
Œ¶(Yil) ‚â•P ‚àó
Ni ‚ààN, i = 1, 2, . . . , k.
Here N is the set of non-negative integers and k
i=1 Ni denotes the total
computational cost assuming the simulation times for diÔ¨Äerent designs are
roughly the same.
That is, we want to minimize the sample sizes that
achieve the speciÔ¨Åed minimal P(CS), P ‚àó. Because of the complexity there is
no known analytical solutions of this optimization problem. It will be better
if we can obtain analytical solution of the above optimization problem.
However, &k
l=2 Œ¶(Yil) is a lower bound of P(CS). It is not clear whether

204
Crafts of Simulation Programming
the optimal sample sizes subject to &k
l=2 Œ¶(Yil) ‚â•P ‚àóconstraint is the
optimal sample sizes subject to P(CS) ‚â•P ‚àóconstraint. Moreover, even
though we are interested in the solution of this deterministic problem, in
practice this optimization is a stochastic programming problem since the
means and the variances are unknown, therefore, are not deterministic, and
need to be estimated by sample means and sample variances.
To obtain a near optimal solution we propose to heuristically decompose
the above optimization problem into the following two formulations.
I: min
k

l=2
Yil
subject to
k

l=2
Œ¶(Yil) ‚â•P ‚àó.
and
II: min Ni1 + Ni2
subject to
Yi2 ‚â•zP
Ni1, Ni2 ‚ààN.
Recall that zP is the P quantile of the standard normal distribution.
Throughout the rest of the chapter, we assume the right hand side of the
equations used to compute Ni results in integer; if it is not integer then the
smallest integer that is greater than that value should be used. This de-
composition is based on the formulation of assessing P(CS) in Section 11.1.
We cannot even conjecture whether the optimizer of these two problems is
the optimizer of the original problem, however, it is a reasonable approach
to search for a near optimal solution. Furthermore, the purpose of budget
allocation is to improve simulation eÔ¨Éciency, we need a relatively fast and
inexpensive way of achieving P ‚àówithin the budget allocation procedure.
EÔ¨Éciency is more crucial than estimation accuracy in this setting.
We now show that I is optimized when Yil = zP , i.e., Œ¶(Yil) = P, for
l = 2, 3, . . . , k; where P = (P ‚àó)1/(k‚àí1). The Lagrangian relaxation function
L =
k

l=2
Yil ‚àíŒª(
k

l=2
Œ¶(Yil) ‚àíP ‚àó).
The Karush-Kuhn-Tucker (KKT) conditions:
‚àÇL
‚àÇYil
= 1 ‚àíŒª
k

j=2,jÃ∏=l
Œ¶(Yij)œÜ(Yil) = 0, for l = 2, 3 . . . , k.

Computing Budget Allocation of Selection Procedures
205
‚àÇL
‚àÇŒª =
k

l=2
Œ¶(Yil) ‚àíP ‚àó= 0.
The KKT conditions are satisÔ¨Åed when Yil = zP , for l = 2, 3, . . . , k, hence,
it must be the optimal solution.
The following shows that II is optimized when Ni1 = (zP /Œ¥i2)2(œÉi1 +
œÉi2)œÉi1 and Ni2 = (zP /Œ¥i2)2(œÉi1 +œÉi2)œÉi2. The Lagrangian relaxation func-
tion
L = Ni1 + Ni2 ‚àíŒª(Œ¥i2/

œÉ2
i1/Ni1 + œÉ2
i2/Ni2 ‚àízP ).
The KKT conditions:
‚àÇL
‚àÇNi1
= 1 ‚àíŒª(Œ¥i2œÉ2
i1/(2N 2
i1(

œÉ2
i1/Ni1 + œÉ2
i2/Ni2)3)) = 0.
‚àÇL
‚àÇNi2
= 1 ‚àíŒª(Œ¥i2œÉ2
i2/(2N 2
i2(

œÉ2
i1/Ni1 + œÉ2
i2/Ni2)3)) = 0.
‚àÇL
‚àÇŒª = Œ¥i2/

œÉ2
i1/Ni1 + œÉ2
i2/Ni2 ‚àízP = 0.
From the Ô¨Årst two equations, we obtain
Ni1
Ni2
= œÉi1
œÉi2
.
Solve the third equation to obtain
Ni1 = (zP /Œ¥i2)2(œÉi1 + œÉi2)œÉi1,
(11.1)
and
Ni2 = (zP /Œ¥i2)2(œÉi1 + œÉi2)œÉi2.
(11.2)
Consequently, for k > 2 P(CS) ‚â•&k
l=2 Œ¶(Yil) ‚â•P ‚àócan be achieved
when
Ni1 =
k
max
l=2 (zP /Œ¥il)2(œÉi1 + œÉil)œÉi1
and
Nil = (zP /Œ¥il)2(œÉi1 + œÉil)œÉil, for l = 2, 3, . . . , k.
(11.3)
To simplify the computation eÔ¨Äort, we set
Ni1 = (zP /Œ¥i2)2(œÉi1 + œÉm)œÉi1,
(11.4)
where œÉ2
m = maxk
i=1,iÃ∏=i1 œÉ2
i . Recall that Œ¥i1 = Œ¥i2 ‚â§Œ¥il = Œºil ‚àíŒºi1, for
l = 2, 3, ¬∑ ¬∑ ¬∑ , k.

206
Crafts of Simulation Programming
There are k‚àí1 pairwise comparisons between the best design i1 and the
rest, therefore, the sample size Ni1 can be computed k‚àí1 diÔ¨Äerent ways. We
compute Ni1 based on Eq. (11.4) to ensure it is no less than the maximum
of those k ‚àí1 values. The allocated sample sizes achieve &k
l=2 Œ¶(Yil) = P ‚àó
when œÉ2
i = œÉ2 for i = 1, 2, ¬∑ ¬∑ ¬∑ , k and Œ¥il = Œ¥ for l = 2, 3, ¬∑ ¬∑ ¬∑ , k. For all
other cases, the allocated sample sizes will result in &k
l=2 Œ¶(Yil) > P ‚àó.
Theorem 11.2. For k competing designs whose performance measure Xij
are normally distributed with means Œº1, Œº2, . . . , Œºk and unknown variances
that need to be estimated by sample variances S2
1(r), S2
2(r), . . . , S2
k(r), where
r is the current sample size, P(CS) will be at least P ‚àówhen the sample size
for design i1 is
Ni1 = max(r, (tP,r‚àí1/di2)2(Si1(r) + Sm(r))Si1(r)),
(11.5)
where S2
m(r) = maxk
i=1,iÃ∏=i1 S2
i (r), and for design il
Nil = max(r, (tP,r‚àí1/dil)2(Si1(r) + Sil(r))Sil(r)), for l = 2, 3, . . . , k.
where P = (P ‚àó)1/(k‚àí1), and di = max(d‚àó, Œºi ‚àíŒºi1).
We denote the procedure developed based on Theorem 11.2 Near Opti-
mal Selection (NOS) procedure in the remainder of this chapter. Note that
the sample sizes allocated by NOS are optimal when all k systems have the
same variance and di = d‚àó, for i = 1, 2, ¬∑ ¬∑ ¬∑ , k. So far, di has been estimated
by Eqs. (10.1) or (10.3). Since to achieve P[ ¬ØXi1 < ¬ØXi2] ‚â•P, we only need
to allocate sample sizes large enough so that the one-tailed P conÔ¨Ådence
interval half width wi1i2 ‚â§Œºi2 ‚àíŒºi1; see Section 10.1.1 and [Chen (2004)].
The allocated sample sizes are larger than necessary when Œ¥i2 = Œºi2‚àíŒºi1
is far in excess of d‚àó. We can eliminate this drawback by setting di1 = di2 =
max(d‚àó, Œºi2 ‚àíŒºi1). Consequently, the allocated sample sizes are optimal
when all k systems have the same variance and di, for i = 1, 2, ¬∑ ¬∑ ¬∑ , k,
have the same value.
When the true means are unknown, we set ÀÜdb =
mink
i=1,iÃ∏=b ÀÜdi for design b (recall that ¬ØXb(r) = min1‚â§i‚â§k ¬ØXi(r)), i.e., ÀÜdi =
max(d‚àó, | ¬ØXi(r) ‚àímink
j=1,jÃ∏=i ¬ØXj(r)|).
Generally speaking, we can improve the eÔ¨Éciency of R&S procedures
with a pre-selection. Subset pre-selection is a screening device that attempts
to select a (random-size) subset of the k alternative designs that contains
the best one. Inferior designs will be excluded from further consideration,
reducing the overall simulation eÔ¨Äort.
In the NOS procedure, design i
having Ni = n0 is excluded from further consideration. Hence, like ETSS
and NTSS, NOS also has an intrinsic subset pre-selection built-in and does
not require performing a pre-selection separately; see [Chen (2004)].

Computing Budget Allocation of Selection Procedures
207
11.2.1
ConÔ¨Ådence
Interval
Half-Width
and
Computing
Budget
[Chen and Kelton (2003)] discuss the relationship between c.i. half-width
and selection procedures. The one-tailed P c.i. half width between designs
i1 and i2
wi1i2 = zP

œÉ2
i1
Ni1
+ œÉ2
i2
Ni2
.
Moreover, optimization problem II stays the same when the constraint
Yi2 ‚â•zp is replaced by wi1i2 ‚â§Œºi2 ‚àíŒºi1. We investigate the following
optimization problem.
III: min wi1i2
subject to
Ni1 + Ni2 = T
Ni1, Ni2 ‚ààN.
Similarly, optimization problem III stays the same when the objective
min wi1i2 is replaced by max Yi2, or max Œ¶(Yi2). The following shows that
wi1i2 is optimized when Ni1 = œÉi1T/(œÉi1 +œÉi2) and Ni2 = œÉi2T/(œÉi1 +œÉi2).
The Lagrangian relaxation function
L = zP

œÉ2
i1/Ni1 + œÉ2
i2/Ni2 ‚àíŒª(Ni1 + Ni2 ‚àíT).
The KKT conditions:
‚àÇL
‚àÇNi1
= ‚àízpœÉ2
i1/(2N 2
i1

œÉ2
i1/Ni1 + œÉ2
i2/Ni2) ‚àíŒª = 0.
‚àÇL
‚àÇNi2
= ‚àízpœÉ2
i2/(2N 2
i2

œÉ2
i1/Ni1 + œÉ2
i2/Ni2) ‚àíŒª = 0.
‚àÇL
‚àÇŒª = Ni1 + Ni2 ‚àíT = 0.
From the Ô¨Årst two equations, we obtain
Ni1
Ni2
= œÉi1
œÉi2
.
Solve the third equation to obtain Ni1 = œÉi1T/(œÉi1 + œÉi2) and Ni2 =
œÉi2T/(œÉi1 + œÉi2).
The solutions of the optimization problems II and III indicate that the
ratio of the optimal sample sizes to obtain a speciÔ¨Åed probability of correct

208
Crafts of Simulation Programming
comparison is the same as that to obtain a minimized c.i. half-width with
a given computing budget. Without loss of generality, assume œÉi2/œÉi1 = Œ≥
and Ni1 + Ni2 = T. The one-tailed P c.i. half width between designs i1
and i2 computed at the end of NOS is
wO = zp
'
(
(
) œÉ2
i1
T
1+Œ≥
+ œÉ2
i2
Œ≥T
1+Œ≥
= zp

w2
O
(1 + Œ≥)z2
P
+
Œ≥w2
O
(1 + Œ≥)z2
P
,
computed at the end of NTSS is
wN = zp
'
(
(
) œÉ2
i1
T
1+Œ≥2
+ œÉ2
i2
Œ≥2T
1+Œ≥2
= zp

w2
N
2z2
P
+ w2
N
2z2
P
,
and results in wO ‚â§wN. Similarly, when k = 2 the sample size allocation
strategy of NOS will result in obtaining greater value of Yi2 and Œ¶(Yi2), i.e.,
greater accuracy of pairwise comparison. This implies that the empirical
distribution of ¬ØXi2 ‚àí¬ØXi1 has smaller variance when the sample sizes are
allocated by NOS than when the sample sizes are allocated by NTSS.
NTSS allocates Ni so that œÉ2
i /Ni = Œ¥i/(2z2
P ), for i = 1, 2, ¬∑ ¬∑ ¬∑ , k. Hence,
for l = 2, 3, ¬∑ ¬∑ ¬∑ , k
wi1il = zP

Œ¥2
i2
2z2
P
+ Œ¥2
il
2z2
P
‚â§zP

Œ¥2
il
2z2
P
+ Œ¥2
il
2z2
P
= Œ¥il.
Let Œ∑l be real numbers, it is possible to allocate Nil for l = 3, 4, ¬∑ ¬∑ ¬∑ , k so
that
wi1il = zP

Œ¥2
i2
2z2
P
+ (Œ¥il + Œ∑l)2
2z2
P
= Œ¥il.
It can be shown that Œ¥il + Œ∑l
=

2Œ¥2
il ‚àíŒ¥2
i2.
Hence,
if we set
di1
= di2
= max(d‚àó, Œºi2 ‚àíŒºi1) and dil
= max(d‚àó,

2Œ¥2
il ‚àíŒ¥2
i2) =
max(d‚àó,

2(Œºil ‚àíŒºi1)2 ‚àí(Œºi2 ‚àíŒºi1)2), for l
=
3, 4, ¬∑ ¬∑ ¬∑ , k, then even
though the allocated sample sizes may not be optimal, they obtains
Œ¶(Yil) = P and &k
l=2 Œ¶(Yil) = P ‚àó.
Similarly, if we set Ni1 and Ni2 according to Eqs. (11.1) and (11.2), and
for l = 3, 4, ¬∑ ¬∑ ¬∑ , k
Nil = (zP œÉil)2
œÉi1 + œÉi2
(œÉi1 + œÉi2)Œ¥2
il ‚àíœÉi1Œ¥2
i2
,
in the NOS procedure, we obtain Œ¶(Yil) = P and &k
l=2 Œ¶(Yil) = P ‚àó.

Computing Budget Allocation of Selection Procedures
209
11.2.2
Maximizing Probability of Correction Selection with
a Given Computing Budget
Based on the Ô¨Ånding in previous section, we treat the optimization problem
of maximizing P(CS) as the dual of minimizing the sample sizes. That is, we
assume the ratio of sample sizes that maximizing P(CS) with a given com-
puting budget and that minimizing the sample sizes with a speciÔ¨Åed min-
imal P(CS) are the same. We derive the sample-sizes-allocation rules that
maximize &k
l=2 Œ¶(Yil) by the computing the ratios of sample sizes allocated
by Eqs. (11.3) and (11.4). Thus, given a total number of simulation samples
T to be allocated to k competing designs and their known means and vari-
ances are Œº1, Œº2, . . . , Œºk, and œÉ2
1, œÉ2
2, . . . , œÉ2
k respectively, &k
l=2 Œ¶(Yil) will be
near optimal (in term of the allocated sample sizes) when
Ni1
Ni2
= (œÉi1 + œÉm)œÉi1
(œÉi1 + œÉi2)œÉi2
,
(11.6)
where œÉ2
m = maxk
i=1,iÃ∏=i1 œÉ2
i , and
Nil
Ni2
=
Œ¥i2
Œ¥il
2 (œÉi1 + œÉil)œÉil
(œÉi1 + œÉi2)œÉi2
, l ‚àà{2, 3, . . . , k}.
(11.7)
On the other hand, the ratio derived from NTSS is
Nil
Ni2
=
Œ¥i2
Œ¥il
2 œÉ2
il
œÉ2
i2
, l ‚àà{1, 2, . . . , k}.
(11.8)
If the variances are equal, i.e., œÉ2
i = œÉ2 for i = 1, 2, ¬∑ ¬∑ ¬∑ , k, then Eqs. (11.6)
and (11.7) can be simpliÔ¨Åed to Eq. (11.8), i.e., the allocated sample sizes
are the same for NOS and NTSS.
When the objective is to maximize P(CS) with a given computing bud-
get, we should not use the indiÔ¨Äerence amount d‚àówhen allocating the
sample sizes. If the indiÔ¨Äerence amount d‚àó> Œºi2 ‚àíŒºi1 and the procedure
evokes d‚àóin computing ÀÜdi (i.e. ÀÜdi = max(d‚àó, | ¬ØXi(r) ‚àímink
j=1,jÃ∏=i ¬ØXj(r)|),
then a larger portion of the computing budget will be allocated to inferior
designs whose mean is far in excess of the best design and will result in
suboptimal allocation.
We now present a cost-eÔ¨Äective sequential approach based on the con-
cept described earlier to select the best design from k alternatives with a
given computing budget. In our procedure, we use mean and variances esti-
mators ¬ØXi(r) and S2
i (r) to compute the ratios of Eqs. (11.6) and (11.7), and
the estimator of Œ¥i is ÀÜdi = | ¬ØXi(r) ‚àímink
j=1,jÃ∏=i ¬ØXj(r)|. We use the equation
S2
i (r) = (r
j X2
ij/r ‚àí¬ØXi(r)
2)r/(r ‚àí1) to compute the variance estimator,

210
Crafts of Simulation Programming
therefore, we are only required to store the triple (r, r
j=1 Xij, r
j X2
ij),
instead of the entire sequences (Xi1, Xi2, . . . , Xir).
Initially, n0 simulation replications for each of the k designs are con-
ducted to get some information about the performance of each design dur-
ing the Ô¨Årst stage. As simulation proceeds, the sample means and sample
variances of each design are computed from the data already collected up to
that stage. According to this collected simulation output, an incremental
computing budget for each iteration, Œîl is distributed to each design based
on the ratios of Eqs. (11.6) and (11.7), where l is the iteration number.
Ideally, each new replication should bring us closer to the optimal solu-
tion. The procedure will be iterated repeatedly until we have exhausted
the pre-determined computing budget T. The algorithm is summarized as
follows.
A Sequential Algorithm for Computing Budget Allocation:
(1) Simulate n0 replications or batches for each design. Set l = 0, N l
1 =
N l
2 = . . . = N l
k = n0, and T = T ‚àíkn0.
(2) Let S2
m(N l
m)
=
maxk
i=1,iÃ∏=b S2
i (N l
i),
¬ØXb(N l
b)
=
mink
i=1 ¬ØXi(N l
i),
¬ØXs(N l
s) = mink
i=1,iÃ∏=b ¬ØXi(N l
i), ÀÜdi = | ¬ØXi(N l
i) ‚àímink
j=1,jÃ∏=i ¬ØXj(N l
j)|. Set
l = l + 1.
Increase the computing budget (i.e., the number of ad-
ditional simulations) by Œîl and compute the new budget allocation,
N l
1, N l
2, . . . , N l
k, such that
N l
b
N ls
= (Sb(N l‚àí1
b
) + Sm(N l‚àí1
m ))Sb(N l‚àí1
b
)
(Sb(N l‚àí1
b
) + Ss(N l‚àí1
s
))Ss(N l‚àí1
s
)
,
(11.9)
and
N l
i
N ls
=
* ÀÜds
ÀÜdi
+2
(Sb(N l‚àí1
b
) + Si(N l‚àí1
i
))Si(N l‚àí1
i
)
(Sb(N l‚àí1
b
) + Ss(N l‚àí1
s
))Ss(N l‚àí1
s
)
,
i Ã∏= b and i ‚àà{1, 2, . . . , k}.
(3) Simulate additional max(0, N l
i ‚àíN l‚àí1
i
) replications or batches for each
design i, i = 1, 2, . . . , k.
(4) T = T ‚àíŒîl. If T > 0, go to step 2.
(5) Return the values b and ¬ØXb(N l
b), where ¬ØXb(N l
b) = min1‚â§i‚â§k ¬ØXi(N l
i).
As simulation evolves, design b, which is the design with the smallest
sample mean, may change from iteration to iteration, although it will con-
verge to the optimal design as l goes to inÔ¨Ånity. In addition, we need to

Computing Budget Allocation of Selection Procedures
211
select the initial number of simulations, n0, and the increment, Œîl, at each
iteration. A suitable choice for n0 is between 5 and 20 [Law (2014)]. Also,
with a small Œîl, we need to iterate the computation procedure in step 2
many times. On the other hand, with a large Œîl, we are putting too much
conÔ¨Ådence on the mean and variance estimators of early iterations and can
result in waste of computation time to obtain an unnecessarily high con-
Ô¨Ådence level of non-critical designs. Instead of using a Ô¨Åxed Œîl for every
iteration, we suggest computing Œîl dynamically at each iteration
Œîl = min(T, max(k, ‚åàT/2‚åâ)).
(11.10)
Thus, the sequential procedure allocates incremental sample sizes aggres-
sively at earlier iterations and become less aggressive as the procedure pro-
ceeds and the computing budget becomes scarce. This way we will be able
to reduce the number of iterations of step 2 without the risk of putting too
much resources to simulate non-critical designs.
In general, a smaller Œîl will result in higher P(CS) with longer execution
time. Hence, a conservative alternative to the above strategy is to increase
sample size by one for each design having been allocated extra samples.
For example, if the above sample-size-allocation rules have allocated extra
N l
i ‚àíN l‚àí1
i
(> 0) samples for design i, we set N l
i = N l‚àí1
i
+ 1.
11.2.3
Optimal Computing Budget Allocation (OCBA)
[Chen et al. (2000)] propose OCBA that is based on a Ô¨Åxed total computing
budget T = k
i=1 Ni and attempts to maximize Approximate Probability
of Correct Selection (APCS). By the Bonferroni inequality [Law (2014)],
P(CS) = P[ ¬ØXi1 ‚àí¬ØXil + Œ¥il < Œ¥il, for l = 2, 3, . . . , k]
‚â•1 ‚àí
k

l=2
(1 ‚àíP[ ¬ØXi1 ‚àí¬ØXil + Œ¥il < Œ¥il])
= 1 ‚àí
k

l=2
(1 ‚àíŒ¶(Yil))
= 2 ‚àík +
k

l=2
Œ¶(Yil).

212
Crafts of Simulation Programming
They consider the following optimization problem:
max APCS :
k

l=2
Œ¶ (Yil)
subject to
k

i=1
Ni = T.
Ni ‚ààN, i = 1, 2, . . . , k.
They show that for a Ô¨Åxed number of replications or batches, the APCS
can be asymptotically maximized when
Ni
Nj
=
 œÉi/Œ¥b,i
œÉj/Œ¥b,j
2
, i, j ‚àà{1, 2, . . . , k}, and i Ã∏= j Ã∏= b,
Nb = œÉb
'
(
(
)
k

i=1,iÃ∏=b
N 2
i
œÉ2
i
,
where Œ¥b,i = ¬ØXi ‚àí¬ØXb, ¬ØXb = min1‚â§i‚â§k ¬ØXi, and œÉi is the standard deviation
of the response of design i. Since œÉi‚Äôs are unknown, standard error Si(r)‚Äôs
are used to compute the sample sizes in the implementation. Note that
when k = 2 the sample sizes allocated by OCBA satisfy the requirement
Ni/Nj = œÉi/œÉj, i.e., the optimal solution of problem I.
While the NOS uses the maximum variance of the k ‚àí1 system to
compute the value of Nb, i.e., Eq. (11.4), the derivation of the results above
assumes Nb >> NiÃ∏=b. However, it is not clear the results from OCBA
imply the optimal sample sizes are reached when Œ¶ (Yil), for l = 2, 3, ¬∑ ¬∑ ¬∑ , k,
are the same. Based on our experimental results, the allocated sample sizes
from all three approaches are consistent with each other, i.e., the diÔ¨Äerences
in performance are minor.
11.3
Empirical Experiments
[Chen et al.
(2000)] present the numerical results of OCBA and other
commonly used R&S procedures. They demonstrate that OCBA is more
eÔ¨Écient in terms of sample size allocation. In this section we present some
empirical results obtained from simulations using NTSS, NOS, and OCBA.
Firstly, we compare the allocated sample sizes from NTSS and NOS to

Computing Budget Allocation of Selection Procedures
213
Table 11.1
ÀÜP(CS) and sample sizes for experiment 1
P ‚àó= 0.90
P ‚àó= 0.95
Procedure
ÀÜP(CS)
T
std(T)
ÀÜP(CS)
T
std(T)
NTSS(20)
0.9520
1263
483
0.9641
1584
616
NOS(20)
0.9573
1313
475
0.9646
1638
606
NTSS(30)
0.9689
1272
413
0.9780
1550
524
NOS(30)
0.9718
1312
406
0.9786
1605
520
obtain the required minimal P(CS). Secondly, we compare the level of em-
pirical P(CS) with given computing budgets.
Furthermore, we use Eq.
(11.10) to compute the incremental sample size at each iteration when we
execute these procedures sequentially.
11.3.1
Experiment 1 Equal Variances
There are ten alternative designs in the selection subset. Suppose Xij ‚àº
N(i, 62), i = 1, 2, . . . , 10. We want to select a design with the minimum
mean: design 1. We perform 10,000 independent experiments to estimate
the actual P(CS) by ÀÜP(CS): the proportion of the 10,000 experiments in
which we obtained the correct selection.
Fig. 11.1
ÀÜP(CS) and Sample Sizes for Experiment 1

214
Crafts of Simulation Programming
The results of experiment 1 to minimize sample sizes are in Table
11.1.
The ÀÜP(CS) column lists the proportion of correct selection.
The
T column lists the average of the number of total simulation replications
(T = 10000
R=1
10
i=1 TR,i/10000, and TR,i is the number of total replications
or batches for design i at the Rth simulation run) used in each procedure.
The std(T) column lists the standard deviation of the number of total sim-
ulation replications at each independent simulation run. The NTSS and
NOS rows list the results of the respective procedures with initial replica-
tions n0 = 20 (and similarly for n0 = 30). All the ÀÜP(CS)‚Äôs are greater than
the speciÔ¨Åed P ‚àó= 0.90 and P ‚àó= 0.95. Theoretically when the variances
are equal among designs, NTSS and NOS are the same (see Section 11.2.2),
however, in practice NOS uses the largest sample variance when computing
Nb, i.e., Eq. (11.5). Hence, NOS generally obtains slightly higher P(CS)
with slightly larger sample size than NTSS.
Figure 11.1 lists the results of experiment 1 when the objective is to
maximize P(CS). We set the number of initial replications n0 = 20. The
computing budgets range from 400 to 1200 with increment size 100. The
diÔ¨Äerence in ÀÜP(CS) is minor among these three procedures. The ÀÜP(CS)
are greater than 0.98 when the given computing budget is 1200, which in-
dicates that sequentializing selection procedures can signiÔ¨Åcantly improve
their performance, i.e., obtaining higher ÀÜP(CS) with smaller sample sizes
when compared to two-stage procedures; see Table 11.1. However, sequen-
tialized selection procedures may require longer run time since the sample
means and sample variances need to be computed at each iteration.
The number of additional simulation replications for each design de-
creases as the diÔ¨Äerences ÀÜdiÃ∏=b = ¬ØXi ‚àí¬ØXb(> 0) increase. This makes sense
because as ÀÜdiÃ∏=b increases, it is more likely that we will conclude Œºi > Œºb.
In other words, as the observed diÔ¨Äerence of sample means across alterna-
tives ÀÜdiÃ∏=b increases, it is less likely that we will conclude Œºi < Œºb. In all
procedures, inferior designs, for instance designs 8 through 10, are almost
always excluded from further simulation, i.e., Ni ‚âàn0.
11.3.2
Experiment 2 Increasing Variances
This is a variation of experiment 1. All settings are preserved except that
the variance of each design increases as the mean increases. Namely, Xij ‚àº
N(i, (6 + (i ‚àí1)/2)2), i = 1, 2, . . . , 10.
The results are in Table 11.2 and Figure 11.2. Since most designs have
larger variances than in experiment 1, ÀÜP(CS) are not as good when the

Computing Budget Allocation of Selection Procedures
215
Table 11.2
ÀÜP(CS) and sample sizes for experiment 2
P ‚àó= 0.90
P ‚àó= 0.95
Procedure
ÀÜP(CS)
T
std(T)
ÀÜP(CS)
T
std(T)
NTSS(20)
0.9477
1614
739
0.9526
2028
932
NOS(20)
0.9463
1793
713
0.9601
2279
916
NTSS(30)
0.9664
1534
607
0.9737
1920
774
NOS(30)
0.9672
1707
587
0.9761
2117
740
Fig. 11.2
ÀÜP(CS) and Sample Sizes for Experiment 2
computing budget is Ô¨Åxed.
NOS has the best performance in this set-
ting. All three procedures allocate relatively more additional simulation
replications for designs with larger variances. These procedures take into
consideration the diÔ¨Äerence between sample means, so Ni < Nj even when
S2
i (n0) > S2
j (n0). [Chen and Kelton (2005)] indicate that procedures that
take into account the diÔ¨Äerence between sample means have the most sig-
niÔ¨Åcant reduction in the number of replications or batches (compared to
Rinott‚Äôs procedure) when the inferior alternatives have larger variances. In
such a case, Rinott‚Äôs procedure tends to allocate most computing resource
to inferior designs and so is ineÔ¨Écient.

216
Crafts of Simulation Programming
Table 11.3
ÀÜP(CS) and sample sizes for experiment 3
P ‚àó= 0.90
P ‚àó= 0.95
Procedure
ÀÜP(CS)
T
std(T)
ÀÜP(CS)
T
std(T)
NTSS(20)
0.9583
1085
358
0.9685
1340
456
NOS(20)
0.9598
1089
363
0.9692
1346
459
NTSS(30)
0.9730
1112
303
0.9823
1350
384
NOS(30)
0.9694
1117
311
0.9788
1358
394
11.3.3
Experiment 3 Decreasing Variances
This is another variation of experiment 1. All settings are preserved except
that the variance of each design decreases as the mean increases. Namely,
Xij ‚àºN(i, (6 ‚àí(i ‚àí1)/2)2), i = 1, 2, . . . , 10.
The results are in Table 11.3 and Figure 11.3. Since designs have smaller
variance, ÀÜP(CS) are better than in setting 1 when the computing budget is
Ô¨Åxed. All three procedures allocate fewer additional simulation replications
for designs that are clearly inferior in this setting, i.e., large sample means
with small variances. Since inferior designs have smaller variances, we are
conÔ¨Ådent to exclude those designs from further simulations. For instance,
designs 7 through 10, are always excluded from further simulation, i.e.,
Ni = n0. This suggests that we should use a smaller initial sample size.
These experiments indicate that the computing budget also experiences
the eÔ¨Äect of diminishing returns. For example, ÀÜP(CS) increases by more
than 0.03 in these experiments when the computing budget is increased
from 400 to 500, while the increase in ÀÜP(CS) is no more than 0.006 when
the computing budget is increased from 1100 to 1200. Thus, if the objective
is to minimize sample sizes and the given P ‚àóis a small value, then analysts
should consider a higher P ‚àósince the marginal cost is small.
11.4
Summary
Traditional indiÔ¨Äerence-zone selection procedures are derived based on the
LFC and are conservative. New approaches that take into account both the
sample variances and the sample means can signiÔ¨Åcantly improve the eÔ¨É-
ciency of selection procedures. We investigated the sample size allocation
strategy of these procedures and developed a highly eÔ¨Écient procedure to
identify a good design out of k alternatives. The purpose of this technique
is to further enhance the eÔ¨Éciency of ranking and selection in simulation ex-
periments. The objective is to maximize the simulation eÔ¨Éciency, expressed
as P(CS) within a given computing budget. The incremental sample sizes

Computing Budget Allocation of Selection Procedures
217
Fig. 11.3
ÀÜP(CS) and Sample Sizes for Experiment 3
at each iteration for each design are computed dynamically according to the
sample means, the sample variances, and the available computing budget
at each iteration. Our procedure allocates replications in such a way that
optimally improves P(CS).
The performance diÔ¨Äerences among these three procedures are minor.
Even though the incremental sample sizes for each design at each itera-
tion are easier to compute in NTSS and NOS than OCBA, these formulas
would be buried somewhere in the software so a little more simplicity of
expression is not too important.
However, the derivation of NTSS and
NOS provides some insight of OCBA and computing budget allocation for
selection. While it will result in sub optimal if NTSS and NOS evoke indif-
ference amount in allocating sample sizes when the objective is to maximize
P(CS), NTSS and NOS are able to explore the information of the indiÔ¨Äer-
ence amount and estimate the required sample size for each design when the
objective is to minimize computing budget given a required minimal P(CS),
which can improve the computation eÔ¨Éciency. While ordinal optimization
can converge exponentially fast, our simulation budget allocation proce-
dure provides a way to further improve overall simulation eÔ¨Éciency. The

218
Crafts of Simulation Programming
techniques presented in this chapter can be considered as a pre-processing
step that precedes any other optimization or search techniques.

Chapter 12
Using Common Random Numbers
with Selection Procedures
The common random numbers (CRN) simulation technique is a variance-
reduction method in which policy alternatives are tested against the same
random input streams. The CRN literature suggests that positively corre-
lated input streams will generate positively correlated policy responses and,
therefore, that the variance of CRN estimators of response diÔ¨Äerences will
be smaller than the variance of independent sample estimators. However,
because the assumption of independent samples across systems is used to
develop the two-stage selection procedures (see, e.g., [Dudewicz and Dalal
(1975)]), CRN are generally not used with those procedures for variance
reduction. There are other variance-reduction techniques, e.g., antithetic
variates, control variates, see [Law (2014)] for details.
12.1
Common Random Numbers
In this section, we review the rational of using CRN. Consider the case
of two alternative systems, where X1j and X2j are the observations from
the Ô¨Årst and second systems on the jth independent replication, and we
want to estimate Œº1 ‚àíŒº2. If we make n replications of each system and let
Zj = X1j ‚àíX2j, for j = 1, 2, . . . , n, then E(Zj) = Œº1 ‚àíŒº2. It is known
that ¬ØZ(n) = n
j=1 Zj/n is an unbiased estimator of Œº1 ‚àíŒº2. Furthermore,
nVar( ¬ØZ(n)) = Var(Zj) = Var(X1j) + Var(X2j) ‚àí2Cov(X1j, X2j).
If the simulations of the two diÔ¨Äerent systems are carried out indepen-
dently, i.e., with diÔ¨Äerent random numbers, X1j and X2j will be indepen-
dent, so that Cov(X1j, X2j) = 0. On the other hand, if we carried out the
simulations of systems 1 and 2 such that X1j and X2j are positively corre-
lated, then Cov(X1j, X2j) > 0, so that the variance of the estimator ¬ØZ(n)
219

220
Crafts of Simulation Programming
is reduced. CRN is a technique used to induce this positive corvariance
by using the same random numbers to simulation all systems. From an
application perspective, using CRN allows us to compare diÔ¨Äerent systems
under similar circumstances.
For an overview of CRN, please see [Law
(2014)]).
In this chapter, we investigate the impact of using CRN in [Dudewicz
and Dalal (1975)] procedure as well as its extension for subset selection. We
show that it is generally safe to use CRN in these selection procedures even
though they are derived based on the assumption of independent sampling.
See [Nazzal et al. (2012)] for a practical example of applying CRN with
indiÔ¨Äerence-zone selection procedures for simulation optimization.
12.2
The Basis of Correlated Order Statistics
Let E(X) and E(Y ), respectively, denote the expected value of the random
variables X ‚àºf and Y ‚àºgu:k.
Here ‚Äú‚àº‚Äù denotes ‚Äúis distributed as‚Äù.
The following are characteristics of the order statistics (of normal or t-
distributed populations) that are veriÔ¨Åed with numerical analysis.
(1) If u < (k + 1)/2, then E(Y ) < E(X).
(2) If u = (k + 1)/2, then E(Y ) = E(X).
(3) If u > (k + 1)/2, then E(Y ) > E(X).
(4) Var(Y ) < Var(X).
(5) The density of the mode of Y is larger than the density of the mode of
X.
(6) If X‚Äôs are correlated, gu:k converges to f as the corvariance become
stronger.
(7) If X‚Äôs are perfectly correlated (i.e., correlation coeÔ¨Écient is 1), gu:k = f.
Because E(Y ) changes as the covariance (of X) changes, we need to take
into account the change in E(Y ) in addition to the change in Var(Y ) when
considering the eÔ¨Äect of using CRN in selection procedures.
12.2.1
Using CRNs with Dudewicz and Dalal‚Äôs Procedure
Let PrC[E] and PrI[E], respectively, denote the probability of event E with
and without CRN. Intuitively, PrI[Ti1 < Til + h1 for l = 2, 3, . . . , k] <
PrC[Ti1 < Til + h1 for l = 2, 3, . . . , k] because Var(Ti1 ‚àíTil) for l =
2, 3, . . . , k is smaller with CRN.
We investigate the eÔ¨Äect of using CRN in selection procedures from the

Using Common Random Numbers with Selection Procedures
221
order-statistics perspective. Let Tu = mink
l=2 Til. Then Tu ‚àºg1:k‚àí1, with
f and F, respectively, are the pdf and cdf of the t distribution with n0 ‚àí1
d.f. Furthermore,
P(CS) = Pr[Ti1 ‚àíTil ‚â§h1 for l = 2, 3, . . . , k] = Pr[Ti1 ‚àíTu ‚â§h1].
Recall that Var(Ti1 ‚àíTu) = Var(Ti1) + Var(Tu) ‚àí2Cov(Ti1, Tu) and
Cov(Ti1, Tu) > 0 when the CRN are properly synchronized.
The stan-
dard deviation within each alternative from the Ô¨Årst stage is not inÔ¨Çuenced
by using CRN across systems. Thus, the required number of simulation
replications or batches for each alternative should be consistent regardless
of whether CRN are used. Let VarI(Tu) and VarC(Tu), respectively, denote
the variance of Tu with independent sampling and with CRN. Depending
on the strength of the correlation (0 ‚â§œÅ ‚â§1) among Til for l = 2, 3, . . . , k,
VarI(Tu) ‚â§VarC(Tu) ‚â§Var(Ti1). Note that Var(Tu) is greater with CRN
than without CRN and VarC(Tu) = Var(Ti1) when samples across systems
are perfectly correlated, i.e., œÅ = 1.
Because Var(Tu) is greater with CRN, Var(Ti1‚àíTu) may be greater with
CRN. Furthermore, if Cov(Ti1, Tu) = 0, then Var(Ti1 ‚àíTu) will be greater
with CRN. On the other hand, as discussed in Section 12.2, E(Tu) ‚â§E(Ti1).
Moreover, E(Tu) ‚ÜíE(Ti1) as the (positive) covariances become stronger,
i.e., E(Tu) is increasing as the (positive) covariances become stronger. In
the extreme, all k ‚àí1 observations are perfectly correlated, E(Tu) = E(Ti1)
because g1:k‚àí1 = f.
With this insight, it is clear that (as P ‚àódeviates
more from 1) even when Cov(Ti1, Tu) = 0 and Var(Ti1 ‚àíTu) is greater with
CRN, PrI[Ti1 < Tu] < PrC[Ti1 < Tu]. Recall that Pr[Ti1 > Tu] is roughly
proportional to the area under the overlapping tail of the density functions
Ti1 and Tu, see [Chen (2004)].
To show that CRN is valid in selection, we need to show that the P ‚àó
quantile of Ti1 ‚àíTu (i.e., the critical constant h1) is smaller with CRN. That
is, if FI(h1) = FC(hc), then hc ‚â§h1, where FI(¬∑) and FC(¬∑), respectively,
denote the cdf of the distribution of Ti1 ‚àíTu without and with CRN. Hence,
FI(h1) ‚â§FC(h1).
Figures 12.1 and 12.2 show the empirical density functions of Xi1 ‚àíXu,
where Xi1 ‚àºN(0, 1) and Xu is the Ô¨Årst-order statistics of 9 N(0, 1) random
variables. The graph on Figure 12.1 lists the pdf when Xi1 and Xu are
correlated, while the graph on Figure 12.2 lists the pdf when Xi1 and Xu
are independent. The covariances are 0, 0.5, and 0.95. Even though these
graphs do not provide a rigorous proof, it does show that it is generally safe
to use CRN in selection procedures to increase P(CS), especially when Xi1

222
Crafts of Simulation Programming
Fig. 12.1
Empirical density of Xi1 ‚àíXu of 10 N(0, 1) random variables
and Xu are correlated. In the cases that Xi1 and Xu are uncorrelated, it is
not clear whether the pdf curves of correlated Xi1 ‚àíXu intersects the pdf
curves of independent Xi1 ‚àíXu as X ‚Üí‚àû. Nevertheless, it is clear that
those pdf curves do not intersect when X is not in the extreme. Hence, as
P ‚àódecreases, the probability of using CRN ‚ÄúbackÔ¨Åre‚Äù decreases.
12.2.2
Subset Selection with CRN
Let PrI[E] and PrC[E], respectively, denote the probability of event E
without and with CRN. Recall that Tc ‚àºgc:v, Tu ‚àºgm‚àíc+1:k‚àív, and
Œ• = Tc ‚àíTu. We intend to show that PrI [Œ• < h] ‚â§PrC [Œ• < h]. We begin
our discussion with an example. If we are interested in the probability of
correctly selecting a subset of size 5 containing 3 of the Ô¨Årst 3 best from
10 alternatives, then Tc ‚àºg3:3(tc) and Tu ‚àºg3:7(tu). Furthermore, if the
initial sample size is n0 = 20, then f and F are, respectively, the pdf and
cdf of the t-distribution with 19 d.f.
The distributions of Tc and Tu converge to T as the covariance becomes
stronger. Consequently, the distribution of Œ• converges to a single value
0 (i.e., variance is 0) as the covariance becomes stronger. Note that E(Œ•)
may be non-positive. To investigate the eÔ¨Äect of CRN to the distribution
of Tc ‚àíTu. We investigate the distribution of Xc ‚àíXu, where Xc ‚àºgc:v

Using Common Random Numbers with Selection Procedures
223
Fig. 12.2
Empirical density of Xi1 ‚àíXu of 10 N(0, 1) random variables
and Xu ‚àºgm‚àíc+1:k‚àív with f and F, respectively, being the pdf and cdf of
the standard normal distribution. It is easier to manipulate the standard
normal distribution than the t-distribution. Moreover, the pdf curves of
Tc ‚àíTu and Xc ‚àíXu are similar.
Fig. 12.3
Empirical probability densities of Xc ‚àíXu

224
Crafts of Simulation Programming
Fig. 12.4
Empirical probability densities of Xc ‚àíXu
We investigate two cases: 1) Xc ‚àºg1:3 and Xu ‚àºg5:7 (i.e., c = 1, v =
3, m = 5, k = 10); 2) Xc ‚àºg3:3 and Xu ‚àºg3:7 (i.e., c = 3, v = 3, m = 5, k =
10). The empirical curves are generated with covariance equals to 0.0, 0.5,
and 0.95. In the Ô¨Årst case, E(Xc ‚àíXu) < 0 and is listed in Figure 12.3. In
the second case, E(Xc ‚àíXu) > 0 and is listed in the Figure 12.4.
Recall that the h value is obtained such that P(Œ• ‚â§h) ‚â•P ‚àówith inde-
pendent sampling. Hence, with a given h > 0 (assuming 0.5 < P ‚àó< 1.0),
P(Œ• ‚â§h) increases as the covariance increases. That is, the probability of
correct selection increases as the covariance increases. Note that if h ‚â§0,
then the speciÔ¨Åed P ‚àócan be achieved by selecting the subset based on the
Ô¨Årst-stage samples.
Figures 12.5 and 12.6 show the pdf of Xc ‚àíXu when 1) Xi for i = 1, 2, 3
are correlated while Xi for i = 4, 5, ¬∑ ¬∑ ¬∑ , 10 are independent (Figure 12.5);
2) Xi for i = 1, 2, 3 are independent while Xi for i = 4, 5, ¬∑ ¬∑ ¬∑ , 10 are
correlated (Figures 12.6). In these cases, i.e., Xc and Xu are independent,
the P(CS) may be smaller with CRN. Note that the pdf curve of correlated
Xc ‚àíXu intersects the pdf curve of independent Xc ‚àíXu on the right tails.
Consequently, depending on the speciÔ¨Åed nominal value P ‚àó, FI(h) ‚â•FC(h).
Unfortunately, systems c and u are unknown. Consequently, to ensure that
using CRN does not ‚ÄúbackÔ¨Åre‚Äù, all systems must be positively correlated.

Using Common Random Numbers with Selection Procedures
225
Fig. 12.5
Empirical probability densities of Xc ‚àíXu
12.3
Empirical Experiments
In this section, we present some empirical results obtained from simula-
tions using the PE [Dudewicz and Dalal (1975)] and PR [Rinott (1978)]
procedures with CRN.
12.3.1
Experiment 1: All Systems are Correlated
We use the following techniques to create positively correlated random vari-
ates.
Let Œæ0 ‚àºN(0, œÉ2
0), Œæi ‚àºN(Œºi, 1 ‚àíœÉ2
0) for i = 1, 2, . . . , k and let
Zi = Œæi ‚àíŒæ0 for i = 1, 2, . . . , k. Let œÇi ‚àºN(0, œÉ2
0). It can be viewed as that
Zi = Œæi ‚àíœÇi for i = 1, 2, . . . , k and œÇi is simulated with CRN. We set Œº1 = 0
and Œºi = d‚àó, for i = 2, 3, . . . , k. Note that Z1 ‚àºN(0, 1) and Zi ‚àºN(d‚àó, 1)
for i = 2, 3, . . . , k and correlates with
Cov(Zi, Zj) = (1 ‚àíœÉ2
0
œÉ2
0
+ 1)‚àí1/2(1 ‚àíœÉ2
0
œÉ2
0
+ 1)‚àí1/2 = œÉ2
0, i Ã∏= j.
We set œÉ2
0 = 0.00, 0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, P ‚àó= 0.95, n0 = 20,
k = 10, and the indiÔ¨Äerent amount d‚àó= 0.01. Note that Œæ0 = 0 when
œÉ2
0 = 0.00, hence, Zi = Œæi for i = 1, 2, . . . , k are independent.
We consider the LFC, Œº1 + d‚àó= Œº2 = . . . = Œº10. The minimum P(CS)
should occur at this conÔ¨Åguration. Furthermore, 10,000 independent exper-

226
Crafts of Simulation Programming
Fig. 12.6
Empirical probability densities of Xc ‚àíXu
iments are performed to estimate the actual P(CS) by ÀÜP(CS), the propor-
tion of the 10,000 experiments in which we obtained the correct selection.
Table 12.1 lists the results. We list the actual P(CS) of the PE and PR
procedures with diÔ¨Äerent values of œÉ2
0. The T column lists the average of
the total simulation replications (T = 10000
r=1
k
i=1 Nr,i/10000, Nr,i is the
number of total replications or batches for system i in the rth independent
run) used in each procedure.
As expected, the ÀÜP(CS)s increase as the
covariance becomes stronger while the average sample sizes are about the
same. The ÀÜP(CS)s of both PE and PR are slightly less than the nominal
value when œÉ2
0 = 0.00 and 0.01. We believe it is because of the stochastic
nature of the experiments. Furthermore, the diÔ¨Äerences of the ÀÜP(CS)s of
PE and PR are very small.
12.3.2
Experiment 2:
Best System is Independent with
Others
In this experiment, Z1 ‚àºN(0, 1) and Zi ‚àºN(d‚àó, 1) for i = 2, 3, . . . , k and
correlates with Cov(Zi, Zj) = œÉ2
0, i Ã∏= j, i Ã∏= 1, j Ã∏= 1, while Cov(Z1, Zj) = 0
for j = 2, 3, . . . , k. All other settings are the same as previous experiment.
Table 12.2 lists the results. As expected, the ÀÜP(CS)s are less than the
corresponding ÀÜP(CS)s of experiment 1 because system 1 is independent of

Using Common Random Numbers with Selection Procedures
227
Table 12.1
ÀÜP(CS) and sample sizes for experiment 1
Procedure/œÉ2
0
0.00
0.01
0.05
0.10
0.25
0.50
0.75
0.90
PE
0.9469
0.9492
0.9604
0.9618
0.9749
0.9918
0.9975
0.9997
PR
0.9472
0.9495
0.9601
0.9620
0.9751
0.9918
0.9975
0.9997
T
13534
13525
13539
13558
13549
13583
13593
13616

228
Crafts of Simulation Programming
Table 12.2
ÀÜP(CS) and sample sizes for experiment 2
Procedure/œÉ2
0
0.00
0.01
0.05
0.10
0.25
0.50
0.75
0.90
PE
0.9469
0.9481
0.9510
0.9495
0.9541
0.9618
0.9707
0.9772
PR
0.9472
0.9479
0.9501
0.9495
0.9542
0.9616
0.9704
0.9770
T
13534
13498
13530
13542
13566
13572
13597
13584

Using Common Random Numbers with Selection Procedures
229
all other alternatives. That is, in this setting Var(Ti1 ‚àíTu) = Var(Ti1) +
Var(Tu)‚àí2Cov(Ti1, Tu) and Cov(Ti1, Tu) = 0 while Var(Tu) is greater with
CRN. Hence, Var(Ti1‚àíTu) is greater with CRN. But we still achieve greater
ÀÜP(CS) because E(Tu) is larger with CRN. The ÀÜP(CS) generally increases
as the covariance becomes stronger while the sample sizes are about the
same.
12.3.3
Experiment 3: Best System is Negatively Correlated
with Others
In this experiment, Z1 ‚àºN(0, 1) and Zi ‚àºN(d‚àó, 1) for i = 2, 3, . . . , k and
correlates with Cov(Zi, Zj) = œÉ2
0, i Ã∏= j, i Ã∏= 1, j Ã∏= 1, while Cov(Z1, Zj) < 0
for j = 2, 3, . . . , k. Namely, Z1 = Œæ1 + Œæ0. All other settings are the same
as previous experiment.
Table 12.3 lists the results.
As expected, the ÀÜP(CS)s are less than
the corresponding ÀÜP(CS)s of experiment 2 because system 1 is negatively
correlated with all other alternatives, i.e., Cov(Ti1, Tu) < 0. Furthermore,
the ÀÜP(CS) generally decrease as the (negative) covariance becomes stronger
while the sample sizes are about the same. The negative eÔ¨Äect in ÀÜP(CS)
of Cov(Ti1, Tu) < 0 is greater than the positive eÔ¨Äect of E(Tu) being larger
with CRN. Hence, using CRN will backÔ¨Åre if the induced covariances are
negative between the best system and all other systems.
12.3.4
Experiment 4: Unequal Variances
In this experiment, there are three settings.
‚Ä¢ Independent samples: Zi ‚àºN(Œºi, (2 + i)/2) for i = 1, 2, . . . , k and
Cov(Zi, Zj) = 0, for i Ã∏= j.
‚Ä¢ Increasing covariance: Zi ‚àºN(Œºi, (13 ‚àíi)/2) for i = 1, 2, . . . , k and
correlate with
Cov(Zi, Zj) =
2

(13 ‚àíi)(13 ‚àíj)
, i Ã∏= j.
That is, Zi = Œæi ‚àíŒæ0 for i = 1, 2, . . . , k, where Œæ0 ‚àºN(0, 1), Œæi ‚àº
N(Œºi, (11 ‚àíi)/2) for i = 1, 2, . . . , k.
‚Ä¢ Decreasing covariance: Zi ‚àºN(Œºi, (2 + i)/2) for i = 1, 2, . . . , k and
correlate with
Cov(Zi, Zj) =
2

(2 + i)(2 + j)
, i Ã∏= j.

230
Crafts of Simulation Programming
Table 12.3
ÀÜP(CS) and sample sizes for experiment 3
Procedure/œÉ2
0
0.00
0.01
0.05
0.10
0.25
0.50
0.75
0.90
PE
0.9469
0.9457
0.9468
0.9435
0.9354
0.9272
0.9252
0.9335
PR
0.9472
0.9457
0.9462
0.9431
0.9352
0.9272
0.9254
0.9337
T
13534
13525
13532
13526
13567
13566
13595
13603

Using Common Random Numbers with Selection Procedures
231
Table 12.4
ÀÜP(CS) and sample sizes for experiment 4
Procedure/Setting
Independent
Increase
Decrease
PE
0.9494
0.9749
0.9810
PR
0.9498
0.9748
0.9811
T
47337
47470
47366
That is, Zi = Œæi ‚àíŒæ0 for i = 1, 2, . . . , k, where Œæ0 ‚àºN(0, 1), Œæi ‚àº
N(Œºi, i/2) for i = 1, 2, . . . , k.
We set Œº1 = 0, Œºi = d‚àófor i = 2, 3, . . . , k. All other settings are the same
as previous experiment. Hence, the inferior systems have various degrees of
correlation with the best system. Table 12.4 lists the results. The ÀÜP(CS)s
are greater with CRN in both increasing and decreasing covariances cases.
We also performed subset selection using CRN with all the previous
conÔ¨Ågurations, the results are similar.
12.3.5
Experiment 5:
Subset Selection - All Systems are
Correlated
If this experiment, Œæ0 ‚àºN(0, œÉ2
0), Œæi ‚àºN(Œºi, 1 ‚àíœÉ2
0) and Zi = Œæi ‚àíŒæ0 for
i = 1, 2, . . . , k. We set Œºi = 0, for i = 1, 2, 3 and Œºi = d‚àó, for i = 4, 5, . . . , k.
Note that Zi ‚àºN(0, 1), for i = 1, 2, 3 and Zi ‚àºN(d‚àó, 1) for i = 3, 4, . . . , k
and correlates with
Cov(Zi, Zj) = (1 ‚àíœÉ2
0
œÉ2
0
+ 1)‚àí1/2(1 ‚àíœÉ2
0
œÉ2
0
+ 1)‚àí1/2 = œÉ2
0, i Ã∏= j.
We set œÉ2
0 = 0.00, 0.01, 0.05, 0.10, 0.25, 0.50, 0.75, 0.90, P ‚àó= 0.95, n0 = 20,
k = 10, and the indiÔ¨Äerent amount d‚àó= 0.01. Note that Œæ0 = 0 when
œÉ2
0 = 0.00, hence, Zi = Œæi for i = 1, 2, . . . , k are independent.
We consider the LFC, Œº1 = Œº2 = Œº3, and Œº3 + d‚àó= Œº4 = Œº5 = . . . =
Œº10. The minimum P(CS) should occur at this conÔ¨Åguration. Furthermore,
10,000 independent experiments are performed to estimate the actual P(CS)
by ÀÜP(CS), the proportion of the 10,000 experiments in which we obtained
the correct selection.
Table 12.5 lists the results. We list the actual P(CS) of the PE and PR
procedures with diÔ¨Äerent values of œÉ2
0. The T column lists the average of
the total simulation replications (T = 10000
r=1
k
i=1 Nr,i/10000, Nr,i is the
number of total replications or batches for system i in the rth independent
run) used in each procedure.
As expected, the ÀÜP(CS)s increase as the
covariance becomes stronger while the average sample sizes are about the

232
Crafts of Simulation Programming
Table 12.5
ÀÜP(CS) and sample sizes for experiment 5
Procedure/œÉ2
0
0.00
0.01
0.05
0.10
0.25
0.50
0.75
0.90
PE
0.9529
0.9496
0.9578
0.9587
0.9739
0.9901
0.9966
0.9995
PR
0.9529
0.9504
0.9579
0.9584
0.9740
0.9900
0.9964
0.9995
T
8145
8143
8149
8150
8165
8165
8187
8195

Using Common Random Numbers with Selection Procedures
233
same. The ÀÜP(CS) of the PE procedure is slightly less than the nominal
value when œÉ2
0 = 0.01. We believe it is because of the stochastic nature of
the experiments. Furthermore, the diÔ¨Äerences of the ÀÜP(CS)s of PE and PR
are very small.
12.3.6
Experiment 6: Subset Selection - Independence Be-
tween Groups 1
In this experiment, Zi ‚àºN(d‚àó, 1), for i = 4, 5, . . . , k , and Zi ‚àºN(0, 1) for
i = 1, 2, 3 and correlates with Cov(Zi, Zj) = œÉ2
0, i Ã∏= j, i < 4, j < 4, while
Cov(Zi, Zj) = 0 for i = 1, 2, 3, and j = 4, 5, . . . , k. All other settings are
the same as previous experiment.
Table 12.6 lists the results.
As expected, the ÀÜP(CS)s are less than
the corresponding ÀÜP(CS)s of experiment 1 because the best c systems
are independent of the other k ‚àíc systems.
That is, in this setting
Var(Tc ‚àíTu) = Var(Tc) + Var(Tu) ‚àí2Cov(Tc, Tu) and Cov(Tc, Tu) = 0
while Var(Tc) is greater with CRN. Hence, Var(Tc ‚àíTu) is greater with
CRN. The increased in ÀÜP(CS) as the correlation becomes stronger is very
small in this setting.
12.3.7
Experiment 7: Subset Selection - Independence Be-
tween Groups 2
In this experiment, Zi ‚àºN(0, 1), for i = 1, 2, 3, and Zi ‚àºN(d‚àó, 1) for
i = 4, 5, . . . , k and correlates with Cov(Zi, Zj) = œÉ2
0, i Ã∏= j, i > 3, j > 3,
while Cov(Zi, Zj) = 0 for i = 1, 2, 3, and j = 4, 5, . . . , k. All other settings
are the same as previous experiment.
Table 12.7 lists the results. Using CRN backÔ¨Åres in this setting. The
decrease in ÀÜP(CS) becomes larger as the correlation becomes stronger. This
result is visualized in Figures 12.5 and 12.6.
12.3.8
Experiment 8: Subset Selection - Unequal Variances
In this experiment, there are three settings.
‚Ä¢ Independent samples: Zi ‚àºN(Œºi, (2 + i)/2) for i = 1, 2, . . . , k and
Cov(Zi, Zj) = 0, for i Ã∏= j.
‚Ä¢ Increasing covariance: Zi ‚àºN(Œºi, (13 ‚àíi)/2) for i = 1, 2, . . . , k and

234
Crafts of Simulation Programming
Table 12.6
ÀÜP(CS) and sample sizes for Experiment 6
Procedure/œÉ2
0
0.00
0.01
0.05
0.10
0.25
0.50
0.75
0.90
PE
0.9529
0.9534
0.9485
0.9492
0.9499
0.9535
0.9587
0.9644
PR
0.9529
0.9529
0.9484
0.9495
0.9503
0.9542
0.9587
0.9648
T
8145
8157
8161
8166
8152
8152
8152
8154

Using Common Random Numbers with Selection Procedures
235
Table 12.7
ÀÜP(CS) and sample sizes for experiment 7
Procedure/œÉ2
0
0.00
0.01
0.05
0.10
0.25
0.50
0.75
0.90
PE
0.9529
0.9502
0.9474
0.9472
0.9408
0.9289
0.9219
0.9161
PR
0.9529
0.9497
0.9471
0.9466
0.9409
0.9298
0.9230
0.9168
T
8145
8158
8152
8148
8145
8170
8184
8188

236
Crafts of Simulation Programming
Table 12.8
ÀÜP(CS) and sample sizes for experiment 8
Procedure/Setting
Independent
Increase
Decrease
PE
0.9498
0.9793
0.9665
PR
0.9500
0.9790
0.9663
T
28547
28574
28589
correlate with
Cov(Zi, Zj) =
2

(13 ‚àíi)(13 ‚àíj)
, i Ã∏= j.
That is, Zi = Œæi ‚àíŒæ0 for i = 1, 2, . . . , k, where Œæ0 ‚àºN(0, 1), Œæi ‚àº
N(Œºi, (11 ‚àíi)/2) for i = 1, 2, . . . , k.
‚Ä¢ Decreasing covariance: Zi ‚àºN(Œºi, (2 + i)/2) for i = 1, 2, . . . , k and
correlate with
Cov(Zi, Zj) =
2

(2 + i)(2 + j)
, i Ã∏= j.
That is, Zi = Œæi ‚àíŒæ0 for i = 1, 2, . . . , k, where Œæ0 ‚àºN(0, 1), Œæi ‚àº
N(Œºi, i/2) for i = 1, 2, . . . , k.
We set Œº1 = 0, Œºi = d‚àófor i = 2, 3, . . . , k. All other settings are the same
as previous experiment. Hence, the inferior systems have various degrees
of correlation with the best systems.
Table 12.8 lists the results. The ÀÜP(CS)s are greater with CRN in both
increasing and decreasing covariances cases.
12.4
Summary
We show that it is generally safe to use CRN to increase the probability
of correct selection of selection procedures, especially when all systems are
correlated with similar strength. The characteristics of the correlated order
statistics provides the insight that the increase in ÀÜP(CS) is mostly because
the expected value of the Ô¨Årst-order statistics becomes larger as the (posi-
tive) covariances become stronger. The probability of ‚ÄúbackÔ¨Åre‚Äù decreases
when the best systems are correlated with non-best systems. Furthermore,
as P ‚àó< 1 decreases, it is less likely that the P(CS) will be became smaller
as the (positive) covariances become stronger.

Chapter 13
Parallel and Distributed Simulation
Parallel and distributed simulation (PADS) studies how a network of sev-
eral interconnected models work together to support decision making by
distributing the execution of a discrete event simulation (DES) program
over multiple computers. Parallel DES programs are executed on multi-
processor computing platforms containing multiple central processing units
that interact frequently. Distributed DES programs are executed on loosely
coupled systems that may be geographically distributed and require addi-
tional interaction times. However, with new computing paradigms such as
clusters of workstations and grid computing, the distinction has become
un-noticeable. In both cases the execution of a single simulation model,
likely composed of several simulation programs, is distributed over multi-
ple processors (computers) and can be executed concurrently. Hence, one
can reduce the execution time by up to a factor equal to the number of pro-
cessors that are used. Distributing the execution across multiple computers
and utilizing the resources of many computer systems is also beneÔ¨Åcial in
allowing the execution of larger simulations where the capacity of one com-
puter may not be enough to carry out the simulation. A more detailed
discussion of distributed and parallel simulation can be found in [Fujimoto
(2000)].
13.1
Introduction
A logical process (LP) is a distinct Ô¨Çow of control, containing a combination
of computation and operation. The simulation of each design can be treated
as a LP and selection involves simulating a collection of LPs. Since system
designs are independent of each other, the simulation of each system, i.e,
LP, can be performed independently in a parallel and distributed fashion.
237

238
Crafts of Simulation Programming
The main goal is to compute the results of the simulation as quickly as
possible to improve the eÔ¨Äectiveness of the simulation tool. Thus, our im-
mediate concern is capability rather than run-time performance. If a better
alternative is found early in the process, it can be used to eliminate inferior
designs at an early stage during the simulation process. When all alter-
natives are divided into several non-overlapping groups, if the best sample
mean of all alternatives at any given moment is used to eliminate only
those inferior designs within the same group, the overall eÔ¨Éciency may suf-
fer. However, we will be able to process R&S of several groups in parallel,
i.e., the entire R&S can be performed by a set of concurrently executing
processes. Thus, the duration of run time will be decreased. Furthermore,
it is possible to compute the best sample mean of all alternative designs
from all groups at any iteration and use the best sample mean to eliminate
inferior designs in all groups. Even though the application of parallel simu-
lation technology has been limited, there has been some work done in this
area, for example, [Luo et al. (2000)] deploy OCBA (Optimal Computation
Budget Allocation) to distribute simulation replications over the web for
R&S problems.
13.2
Parallel and Distributed Selection
In this section, we present the basis of selection in a parallel and distributed
fashion. PADS attempts to decrease simulation analysis time by distribut-
ing the simulation workload among multiple computers (processors).
A
simulation program operates on a models state variables by executing a
time-ordered sequence of simulation events. Each event may change the
state of the simulated system and schedule one or more future events. In
most discrete-event simulation, the order in which events are executed is
stored in the event list and is determined by a next-event time advance
mechanism for the simulation clock, see [Law (2014)]. Events are executed
in nondecreasing time-stamp order so that the simulation clock always ad-
vances. A conventional PADS decomposes a simulation model into commu-
nicating LPs to perform diÔ¨Äerent events. The PADS procedure maps each
LP to a processor and uses interprocessor communication to allow LP on
diÔ¨Äerent processors to communicate with each other.
The application of parallel and distributed simulation has been limited.
It is generally diÔ¨Écult to implement simulation in a parallel and distributed
environment because of the sequential nature of most simulations. How-

Parallel and Distributed Simulation
239
ever, simulation based ranking and selection is well suited for parallel and
distributed simulation because the behavior of each system can be simu-
lated independently. For example, a network of computers can be used to
perform R&S problems, several computers can be assigned to simulate the
performance of one or several systems, while a computer dedicated to per-
form the comparisons between systems orchestrates the overall simulation
strategy.
To set up the procedure, let G1, G2, . . . , Gm be groups of designs such
that G1 ‚à™G2 ‚à™. . . ‚à™Gm = {1, 2, ..., k}, Gi ‚à©Gj = ‚àÖfor i Ã∏= j, ki = |Gi| ‚â•2
for all i, where |I| denotes the cardinality of the set I. When we perform
selection in group e, the designs in Ge will be compared to others in the same
group. Without loss of generality, assume i1 ‚ààGg for some 1 ‚â§g ‚â§m.
For each group e, let ¬ØXe
b = mini‚ààGe ¬ØXi denote the best sample mean in
group e. In each group, designs q ‚ààGe such that
¬ØXq ‚â§¬ØXe
b + d‚àówill
be considered as surviving designs.
If we perform the selection in each
group e, then P[i1 surviving Gg selection] ‚â•P. Since the procedure has
allocated substantial sample sizes to those surviving designs, the additional
samples in subsequent selections will most likely be small. The surviving
designs are then grouped into a single group or multiple groups depending
on the number of surviving designs. Note that the probability that design
i1 is among the set of surviving designs is at least P.
To simplify our
discussion, we group the surviving designs into a single group. Let k‚Ä≤ denote
the number of surviving designs and let the set I contain the k‚Ä≤ surviving
designs. We then select from all surviving designs in I. Note that for each
design i ‚ààI, we have the triple (ni, ni
j=1 Xij, ni
j=1 X2
ij). We perform all
pairwise comparisons among these k‚Ä≤ designs to eliminate inferior designs,
i.e., design j having ¬ØXj > ¬ØXi + wij for some i ‚ààI will be excluded from
further simulation. Furthermore, if wij < d‚àóand ¬ØXi > ¬ØXj for i, j ‚ààI,
remove i from I. Let r be the iteration index and ÀÜdi = max(d‚àó, ¬ØXi‚àíU( ¬ØXb))
for i ‚ààI, where ¬ØXb = mini‚ààI ¬ØXi. Compute Œ¥i,r = ‚åà(htSi(ni)/ ÀÜdi)2‚åâ‚àíni.
Let ÀÜŒ¥r = mini‚ààI Œ¥i,r. Simulate additional Œ¥r replications or batches for each
design i ‚ààI at iteration r. If Œºi2 ‚àíŒºi1 > d‚àó, the probability of i1 being
eliminated by some design i Ã∏= i1 is less than (1‚àíP ‚àó)/(k‚àí1). Hence, by the
Bonferroni inequality, the probability that design i1 survives the selection
is larger than P ‚àó= 1 ‚àík‚àí1
i=1 (1 ‚àíP ‚àó)/(k ‚àí1).
We can increase the eÔ¨Éciency of deploying the selection procedure
in a parallel and distributed environment by passing the best sample
mean at each iteration to all groups.
Let
¬ØXB be the best sample
mean from all groups, i.e.,
¬ØXB = min1‚â§e‚â§m ¬ØXe
b , and pass the triple

240
Crafts of Simulation Programming
(nB, nB
j=1 XBj, nB
j=1 X2
Bj) to all groups. Since ¬ØXB ‚â§¬ØXe
b for 1 ‚â§e ‚â§m,
the overall eÔ¨Éciency of selection may be improved. Note that ¬ØXe
b from dif-
ferent groups may be obtained at diÔ¨Äerent iteration with diÔ¨Äerent sample
sizes.
Another beneÔ¨Åt of distributed simulation is the ability to integrate sev-
eral diÔ¨Äerent simulators, i.e., diÔ¨Äerent simulation software packages, into
a single simulation environment. Since the underlying simulator used to
generate the simulation results has no impact on the Ô¨Ånal selection, we can
simulate alternative system designs with diÔ¨Äerent simulation packages. The
ability to integrate diÔ¨Äerent simulators allows the simulation models to be
executed and developed concurrently.
Sequential selection procedures oÔ¨Äer a natural way of performing selec-
tion in a parallel and distributed fashion, providing further gains in sim-
ulation eÔ¨Éciency. Once the number of alternative designs and number of
groups have been determined, several control computers, denoted Œ©, can
be used to orchestrate the execution of simulation. These Œ© computers will
initiate the execution of simulation and invoke a set of remote computers,
for example through remote method invocation, to generate r samples of
one or several system designs. That is, each system is evaluated through
distributed runs of discrete simulation models. The simulation results can
be written to a shared Ô¨Åle server or ftp (Ô¨Åle transfer protocol) to the com-
puters Œ© that are performing the R&S. Based on the analysis, computers Œ©
send the required additional sample size for each system design to the cor-
responding computer. The communication between these computers will
repeat iteratively until simulation is complete. Deploying selection proce-
dures in parallel and distributed environment requires little communication
between processors and overhead, making it practical and attractive.
13.3
The Framework
The use of networks of workstations interconnected through LAN/WANs
(Local Area Network/Wide Area Network) has evolved into an eÔ¨Äective and
popular platform for parallel and distributed computing. The advantages
of these network computing environments include 1) ready availability, 2)
low cost, and 3) incremental scalability. Furthermore, network computing
environments retain their ability to serve as a general-purpose computing
platform and to run commercially available software products. One diÔ¨É-
culty associated with PADS is time management when ensuring that the

Parallel and Distributed Simulation
241
execution of the distributed simulation, i.e., LP, is properly synchronized.
Communication must be sent between processors when corresponding parts
of the model interact logically. As a result, issues concerning the sub mod-
els ability to proceed at its own pace arise because generally PADS LP can
schedule further events not only for itself but also for other LPs. There-
fore, events cannot be executed in a straightforward time-stamp manner.
As pointed out earlier, for the R&S procedure to perform correctly, it only
needs the triple (ni, ni
j=1 Xij, ni
j=1 X2
ij) from each design.
Thus, time
management does not complicate processing R&S in parallel. In this case,
each LP is viewed as an independent and autonomous discrete event sim-
ulator.
This means that each LP maintains its local state information
corresponding to the entities it is simulating and a list of events that have
been scheduled for this LP. Furthermore, each LP only schedules additional
events for itself but not for other LPs. Hence, it is straightforward to im-
plement R&S in a parallel and distributed fashion.
The World Wide Web or the Internet is a loose coupling of thousands
of networks and millions of computers around the globe. The Internet has
become one of the most important information sources and communica-
tion platforms in industry.
An inherent characteristic of the Internet is
its distributed nature, hence, it provides an excellent basis for distributed
simulation.
13.4
Selection with All Pairwise Comparisons
Most selection procedure requires the input data are i.i.d. (independent and
identically distributed) normal. Many performance measures of interest are
taken over some average of a sample path or a batch of samples. Thus, many
applications tend to have a normally distributed simulation output. If the
non-normality of the samples is a concern, users can use batch means (see
[Law (2014)]) to obtain samples that are essentially i.i.d. normal.
EÔ¨Äective reduction of computation eÔ¨Äorts while obtaining a good deci-
sion is crucial. It has been proposed to sequentialize selection procedures
to eliminate the drawback of two-stage procedures and to improve its ef-
Ô¨Åciency. [Rinott (1978)] procedure and its variants are derived based on
P(CS)=Pr[ ¬ØXi1 < ¬ØXil, for l = 2, 3, . . . , k] ‚â•P ‚àó. To further improve the
eÔ¨Éciency of sequentialized selection procedure we incorporate all pairwise
comparisons at each iteration. Let P = 1‚àí(1‚àíP ‚àó)/(k‚àí1). Inferior design
i such that Pr[ ¬ØXi > ¬ØXj] ‚â•P of some design j will be excluded from further
simulation at each iteration.

242
Crafts of Simulation Programming
For completeness, the related theorem and proposition are listed here.
Theorem 13.1. For k competing designs whose performance measure Xij
are normally distributed with means Œº1, Œº2, . . . , Œºk and unknown variances
that need to be estimated by sample variances S2
1(nc), S2
2(nc), . . . , S2
k(nc),
where nc is the current sample size, P(CS) will be at least P ‚àówhen the
sample size for design i is
Ni = max(nc, ‚åà(htSi(nc)/di)2‚åâ), for i = 1, 2, . . . , k,
where nc is the current sample size, the critical value ht =
‚àö
2tnc‚àí1,P ,
P = 1 ‚àí(1 ‚àíp‚àó)/(k ‚àí1), and di = max(d‚àó, Œºi ‚àíŒºi1).
Let the set I contain competing designs and let ¬ØXb = mini‚ààI ¬ØXi at
each iteration. Since the true means Œºi are unknown, di is conservatively
estimated by ÀÜdi = max(d‚àó, ¬ØXi ‚àíU( ¬ØXb)), where U( ¬ØXb) is the upper one-
tailed P ‚àóconÔ¨Ådence limit of Œºb, i.e., Pr[Œºb ‚â§U( ¬ØXb)] ‚â•P ‚àó. The value of
tnc‚àí1,P can be approximated easily, see Section 1.5.3 and [Hastings (1955)].
Theorem 13.2. Let the set I contain k competing designs whose perfor-
mance measure are normally distributed with unknown means and unknown
variances that need to be estimated by sample means ¬ØX1, ¬ØX2, . . . , ¬ØXk, and
sample variances S2
1(n1), S2
2(n2), . . . , S2
k(nk). If k ‚àí1 designs are removed
(eliminated) sequentially from I with each eliminated design j satisÔ¨Åes the
equation that ¬ØXj(nc) > ¬ØXi(nc) + wij,
i, j ‚ààI, where nc is the current
sample size for each design, wij = tnc‚àí1,P

S2
i (nc)/nc + S2
j (nc)/nc and
P = 1 ‚àí(1 ‚àíp‚àó)/(k ‚àí1), then Pr[i1 ‚ààI] ‚â•P ‚àó.
The Sequential Selection Procedure with All Pairwise Com-
parisons (SAPC):
(1) Initialize the set I to include all k designs. Let Ni,r and ¬ØXi,r, respec-
tively, be the allocated sample size and the sample mean of design i at
the rth iteration. Simulate n0 replications or batches for each design
i ‚ààI. Set the iteration number r = 0, and nc = N1,r = N2,r = ¬∑ ¬∑ ¬∑ =
Nk,r = n0, Set P = 1 ‚àí(1 ‚àíP ‚àó)/(k ‚àí1).
(2) Perform all pairwise comparisons and delete inferior design j from I;
i.e., ¬ØXj > ¬ØXi+wij, i, j ‚ààI. Note wij is the one-tailed P c.i. half-width.
(3) If wij < d‚àóand ¬ØXj > ¬ØXi, remove design j from I.

Parallel and Distributed Simulation
243
(4) If there is no more than one element (or the pre-determined number of
best designs) in I, go to step 8.
(5) Compute the critical value ht =
‚àö
2tnc‚àí1,P .
(6) Let ¬ØXb,r = mini‚ààI ¬ØXi,r, For all i ‚ààI, compute ÀÜdi,r = max(d‚àó, ¬ØXi,r ‚àí
U( ¬ØXb,r)), where U( ¬ØXb,r) be the upper one-tailed P ‚àóconÔ¨Ådence limit
of Œºb at the rth iteration, and compute
Œ¥i,r+1 = ‚åà((htSi(nc)/ ÀÜdi,r)2 ‚àínc)+‚åâ.
(7) Set r = r + 1. If Œ¥i,r = 0, set Œ¥i,r = 1. Set the incremental sample size
at the rth iteration Œ¥r = mini‚ààI Œ¥i,r. For ‚àÄi ‚ààI, simulate additional Œ¥r
samples, set nc = nc + Œ¥r. Go to step 2.
(8) Return the values b and
¬ØXb(Nb), where
¬ØXb(Nb)
=
min ¬ØXi(Ni),
1 ‚â§i ‚â§k and i was not eliminated by all pairwise comparisons.
In the sequential selection procedure, all the alternatives 1 ‚â§i ‚â§k are
initially included in the set I for R&S. If all k ‚àí1 designs were eliminated
from I through the two-sample-t test, then Pr[i1 ‚ààI] ‚â•P ‚àó.
On the
other hand, if some designs were eliminated from I because wij < d‚àó,
then the procedure can only guarantee P(CS) ‚â•P ‚àó.
The basic idea is
that the procedure sequentially removes k ‚àí1 designs from I.
If Œºi2 ‚àí
Œºi1 ‚â•d‚àó, then the probability of wrongly removing design i1 is 1 ‚àíP
each time a design is removed. By the Bonferroni inequality Pr[i1 ‚ààI] ‚â•
1 ‚àík‚àí1
i=1 (1 ‚àíP) = P ‚àó. We use the equation S2
i (nc) = (nc
j X2
ij/nc ‚àí
¬ØX2
i )nc/(nc ‚àí1) to compute the variance estimator so that we are only
required to store the triple (nc, nc
j=1 Xij, nc
j X2
ij), instead of the entire
sequences (Xi1, Xi2, . . . , Xinc).
13.5
Empirical Experiments
In this section we list the empirical results of SAPC and running SAPC un-
der a parallel and distributed environment, denoted NPDS. Furthermore,
NPDSs denotes performing selection separately within each group, i.e., the
best sample mean from other groups is not used to eliminate inferior in cur-
rent group. Instead of using stochastic systems simulation examples, which
oÔ¨Äer less control over the factors that aÔ¨Äect the performance of a proce-
dure, we use various normally distributed random variables to represent
the system performance measures.
In the experiment, there are 10 alternative designs under consideration,
and each Xij ‚àºN(Œºi, (
‚àö
10)2), where N(Œº, œÉ2) denotes the normal distri-

244
Crafts of Simulation Programming
Table 13.1
ÀÜP(CS) and sample sizes
P ‚àó= 0.90
P ‚àó= 0.95
Procedure
ÀÜP(CS)
T
ÀÜP(CS)
T
SAPC
0.9567
535
0.9801
746
NPDS
0.9458
531
0.9760
777
NPDSs
0.9561
659
0.9744
842
Table 13.2
Detailed sample sizes of NPDS
Design
Œº
Sample
Within
Sample
Select
1
0
65
0.9737
96
0.9458
2
1
62
0.4156
73
0.0113
3
1
62
0.4174
73
0.0116
4
1
62
0.4215
73
0.0119
5
2
52
0.0093
52
0.0
6
1
34
0.4342
50
0.0181
7
2
29
0.0265
29
0.0004
8
2
28
0.0294
29
0.0003
9
2
28
0.0311
29
0.0006
10
3
23
0.0005
23
0.0
bution with mean Œºi and variance œÉ2. See Table 13.2 for the values of Œºi.
We divide these designs into two non-overlapping groups: group 1 contains
designs 1 through 5, group 2 contains designs 6 though 10. The indiÔ¨Äerence
amount d‚àóis set to 1.0 in all cases. We set the initial replication n0 = 10
and the minimal probability of CS P ‚àó= 0.90 and P ‚àó= 0.95. Furthermore,
10,000 independent experiments are performed to estimate the actual P(CS)
by ÀÜP(CS): the proportion of the 10,000 experiments in which we obtained
the correct selection: design 1.
Table 13.1 lists the experimental results. The SAPC, NPDS, and NPDSs
rows list the results of each procedure. The ÀÜP(CS) column lists the pro-
portion of correct selection. The T column lists the average of the total
simulation replications (T = 10000
r=1
k
i=1 Nr,i/10000, Nr,i is the total num-
ber of replications or batches for design i in the rth independent run) used
in each procedure. The observed P(CS)‚Äôs are greater than the speciÔ¨Åed
nominal levels of 0.90 and 0.95. The sample sizes allocated by NPDS are
about the same as those allocated by SAPC since the best design is used to
eliminate inferior designs early in the selection process in all groups. How-
ever, NPDS can perform selection in parallel and may result in a shorter
run time, especially when the best sample means in each group is about
the same.
Table 13.2 lists the results when the best sample mean is available to all

Parallel and Distributed Simulation
245
Table 13.3
Detailed sample sizes of NPDSs
Design
Œº
Sample
Within
Sample
Select
1
0
90
0.9934
101
0.9561
2
1
66
0.3613
67
0.0095
3
1
67
0.3588
67
0.0074
4
1
66
0.3552
67
0.0084
5
2
26
0.0120
26
0.0
6
1
91
0.9930
100
0.0184
7
2
67
0.3624
67
0.0
8
2
67
0.3633
67
0.0001
9
2
67
0.3645
67
0.0001
10
3
26
0.0112
26
0.0
groups. The Œº column lists the true mean of design i. The Within column
lists the proportion that ¬ØXi ‚â§¬ØXb +d‚àó, where ¬ØXb = min1‚â§i‚â§k ¬ØXi is the best
sample mean of all groups. The Select column lists the proportion that the
particular design has the best sample mean. The Ô¨Årst Sample column lists
the resulting sample sizes from the subdivided groups. The second Sample
column lists the Ô¨Ånal sample sizes.
Table 13.3 lists the results when the best sample mean is available only
to the group contain this particular design.
The Within column under
NPDS without sharing list the proportion that particular design has sample
mean within the best sample mean in the same group (instead of the best
sample mean of all groups) plus the indiÔ¨Äerence amount. All other Ô¨Åelds
are as deÔ¨Åned in Table 13.2. The detailed results for P ‚àó= 0.95 are similar.
The allocated sample sizes for designs in the second group, i.e., designs
6 through 10, are signiÔ¨Åcantly reduced when the best sample means from
all groups are used to eliminate inferior designs. Even though the means
within each group have similar conÔ¨Åguration, the resulting sample sizes
and within from each group are not similar. This is because design 6 is no
longer the best design in group 2 when the best sample from all groups is
included for selection.
13.6
Summary
Parallel and distributed simulation can reduce execution time for time-
consuming applications, such as ranking and selection of stochastic sys-
tems. We have presented a framework for deploying selection procedures
in a parallel and distributed environment. All the alternative designs are
subdivided into several groups and the entire selection is performed by a set

246
Crafts of Simulation Programming
of concurrently executing processes. Thus, we may be able to shorten the
run time. The procedure incorporates all pairwise comparisons to eliminate
inferior designs at each iteration, which may reduce the overall computa-
tional eÔ¨Äort. The proposed procedure takes into account the diÔ¨Äerence of
sample means when determining the sample sizes and is suitable even when
the number of designs under consideration is large.

Chapter 14
Multi-Objective Selection
The ranking and selection studies in previous chapters have focused on
selection based on a single measure of system performance. In many prac-
tical situations, however, we need to select systems based on multiple cri-
teria (attributes, objectives, or performance measures). For example, in
product-design optimization, the cost and the quality of products are two
conÔ¨Çicting objectives. In evaluating airline Ô¨Çight schedules, we may want
to select Ô¨Çight schedules in terms of minimal (Ô¨Çight and ground) staÔ¨Äs and
minimal percentage of late arrivals. In this setting, the problem of select-
ing the best systems from a set of alternatives through simulation becomes
a multi-objective ranking and selection problem. Researchers have devel-
oped procedures to address this problem. [Butler et al. (2010)] combine
the multiple attribute utility theory with the [Rinott (1978)] procedure to
handle multiple performance measures. Before performing selection, users
transform multiple performance measures into one utility score. The pro-
cedure then Ô¨Ånds the system that gives the highest utility. [Dudewicz and
Taneja (1978)] propose a multivariate procedure by deÔ¨Åning a multivariate
normal vector composed of œâ > 1 component variates with an unknown
and unequal variance-covariance matrix.
They redeÔ¨Åne the indiÔ¨Äerence-
zone parameter as the Euclidean distance from a mean vector to the best
mean vector.
[Lee et al. (2004)] point out that when there are multiple performance
measures the selected ‚Äúbest‚Äù system would be strongly dependent on the
decision maker‚Äôs preference. Instead, they develop a multi-objective com-
puting budget allocation (MOCBA) procedure which incorporates the con-
cept of Pareto optimality into the ranking and selection scheme to Ô¨Ånd all
non-dominated solutions, i.e., the non-dominated Pareto set . In the case
that the problems are multi-objective in nature, there may not exist a sin-
247

248
Crafts of Simulation Programming
gle best solution, but rather a set of non-dominated solutions is referred to
as the pareto set of solutions. They represent the ‚Äúbest‚Äù systems and are
characterized by the deÔ¨Ånition that no other solution exists that is superior
in all the objectives. A solution is called Pareto-optimal if there exists no
other solution that is better in all criteria. In general, eventually a single
system must be selected for a given situation. In the planning phase, how-
ever, it is desirable to have a set of good options available. For example,
a company could manage several diÔ¨Äerent warehouses, each with its own
requirements. Thus, each warehouse can select a Ô¨Ånal system from the set
of good options according to its own requirements. Moreover, many mail-
order companies oÔ¨Äer several diÔ¨Äerent delivery options of their products:
1) the shortest delivery time; 2) the least expensive; 3) some other deliv-
ery options with modest delivery time and modest cost. Each options are
non-dominated. Customers can choose the delivery option based on their
own need. The MOCBA procedure aims to determine the Pareto set by
allocating the sample sizes intelligently and minimize the required sample
sizes.
[Lee et al. (2006)] integrate selection procedures with search mechanism
to solve multi-objective simulation-optimization problems. In the applica-
tion of using evolutionary algorithms to solve multi-objective problems, the
concept of Pareto optimality is often employed to Ô¨Ånd the non-dominated
Pareto set. In this chapter, we extend a R&S procedure to select a Pareto
set containing non-dominated systems. We try to provide a non-dominated
Pareto set of systems to the decision maker, rather than reducing the prob-
lem to a single-objective model and providing a single ‚Äúbest‚Äù system. We
attempt to minimize the required sample sizes that provide a probability
guarantee that the selected systems are non-dominated. Alternatively, the
procedure can be used to maximize the probability of correctly selecting
non-dominated systems given a Ô¨Åxed sample size.
14.1
Introduction
In this section, we introduce the necessary notation and background:
œâ:
the number of performance measures of interest,
q:
the index of performance measures, i.e., q = 1, 2, ¬∑ ¬∑ ¬∑ , œâ,
Xiqj:
the independent and normally distributed observations from
the jth replication or batch of the qth performance measure of
the ith system,

Multi-Objective Selection
249
Ni:
the total number of replications or batches for system i,
ni:
the intermediate number of replications or batches for system
i,
Œºiq:
the qth expected performance measures of system i,
‚ÉóŒºi:
the vector of œâ expected performance measures of system i, i.e.,
‚ÉóŒºi = (Œºi1, Œºi2, . . . , Œºiœâ) = (E(Xi1j), E(Xi2j), . . . , E(Xiœâj)),
¬ØXiq:
the sample mean of the qth performance measure of system i
with ni samples, i.e., ni
j=1 Xiqj/ni,
œÉ2
iq:
the variance of the qth observed performance measure of system
i from one replication or batch, i.e., œÉ2
iq = Var(Xiqj),
S2
iq(ni):
the sample variance of the qth performance measure of system
i with ni replications or batches, i.e., S2
iq(ni) = ni
j=1(Xiqj ‚àí
¬ØXiq)2/(ni ‚àí1).
14.1.1
A Multi-Objective Selection Procedure
Based on the Bayesian methodologies, [Lee et al. (2004)] develop a proce-
dure to select the Pareto set based on a performance index œài that measures
the sum of the probabilities that other systems are better than a given sys-
tem i.
Let P(‚ÉóŒºl ‚â∫‚ÉóŒºi) denote the probability that system l dominates
system i. Then
œài =
k

l=1,lÃ∏=i
P(‚ÉóŒºl ‚â∫‚ÉóŒºi) =
k

l=1,lÃ∏=i
œâ

q=1
P(Œºlq ‚â§Œºiq),
and at least one of those inequalities is strict. Note that œài is not a probabil-
ity, it is a function of probability and can be greater than 1. The procedure
requires a user-speciÔ¨Åed parameter œà‚àó, a performance index for systems in
the Pareto set to be retained at the end of the simulation. Furthermore,
the procedure assumes that the number of non-dominated systems is known
in advance. The procedure does not incorporate the indiÔ¨Äerence-zone ap-
proach, it performs an optimization to minimize sample sizes subject to the
constraints that œài ‚â§œà‚àófor all system i in the Pareto set. Alternatively,
the procedure can optimize  œài subject to the constraints that the total
sample size is less than a user speciÔ¨Åed computing budget T. The parame-
ter œà‚àóis analogous to the parameter P ‚àóin the procedure, but œà‚àóis not as
easy to specify or interpret.

250
Crafts of Simulation Programming
14.2
Methodologies
In this section we present a strategy of applying R&S technique to select
a Pareto set of non-dominated systems. We assume that the performance
measures are independent from one another. Like other selection proce-
dures, the proposed procedures assume input data are i.i.d. normal and
allow unknown and unequal variances across systems. If non-normality of
the input data is a concern, users can use batch means to obtain sample
means that are essentially i.i.d. normal.
14.2.1
Prolog
The problem considered in this study is as follows. Suppose that we have a
set of k systems, where each is evaluated in terms of œâ independent objec-
tives. We want to Ô¨Ånd the non-dominated (Pareto) set of systems by run-
ning simulations. The goal is to determine an allocation of the simulation
replications to the systems, so that there is certain probability guarantee
that the selected systems are true non-dominated.
We apply the SRS selection procedure in each objective (performance
measure) to select mp < k non-dominated systems. The selected systems
are non-dominated because they are the best in at least one objective.
The collection of these non-dominated systems is an incomplete Pareto
set, because the collection may not include all the non-dominated systems.
To simplify the discussion, we assume mp ‚â§œâ.
That is, we select no
more than one system in each objective initially. The procedure obtains
approximately P ‚àóprobability that the selected systems are the best in at
least one performance measure and, thus, are non-dominated. Here the
value of mp and P ‚àóare both speciÔ¨Åed by the users.
To adapt the procedure for not using the indiÔ¨Äerence amount d‚àó, the
controlled distances will be computed as follows.
dbl =
 ¬ØXb2 ‚àí¬ØXbl l = 1
¬ØXbl ‚àí¬ØXb1 l = 2, 3, . . . , k.
(14.1)
Then the required sample size is computed by
Nbl = max(n0 + 1, ‚åà(hSbl(n0))/dbl)2‚åâ) for l = 1, 2, . . . , k.
(14.2)
Furthermore, the incremental sample size
Œ¥i,r+1 =
(Ni,r+1 ‚àíNi,r)+ if Ni,r+1 ‚àíNi,r < 5
5
otherwise.
(14.3)

Multi-Objective Selection
251
Table 14.1
Means and standard deviations of systems
Systems
Mean1
Std1
Mean2
Std2
. . .
Meanœâ
Stdœâ
1
Œº11
œÉ11
Œº12
œÉ12
. . .
Œº1œâ
œÉ1œâ
2
Œº21
œÉ21
Œº22
œÉ22
. . .
Œº2œâ
œÉ2œâ
...
...
...
...
...
...
...
...
k
Œºk1
œÉk1
Œºk2
œÉk2
. . .
Œºkœâ
œÉkœâ
Here r is the iteration index.
This is because the diÔ¨Äerence of the best sample mean and the second
best sample mean could be very small and without a user speciÔ¨Åed d‚àóas a
lower bound, the estimated sample size could be much larger than necessary.
The choice to limit the incremental sample size to 5 is somewhat arbitrary,
however, we believe it provides a good compromise between the number of
iterations and the total sample size.
14.2.2
The Strategy
To facilitate the discussion, we assume that the mean and standard devia-
tion of the œâ performance measure of k systems are as listed in Table 14.1.
Let Tq = k
i œÖiq be the minimal required sample size to correctly select the
system having the best performance measure q, where œÖiq is the optimal
sample size for system i based on performance measure q. Without loss of
generality, assume T1 ‚â§T2 ‚â§. . . ‚â§Tœâ. Hence, the minimal required sample
size to obtain mp non-dominated systems should be k
i=1 maxmp
q=1(œÖiq) ‚â•
Tmp.
We denote this the theoretical optimal sample sizes allocation of
selecting a Pareto set TOAP. Note that k
i=1 maxmp
q=1(œÖiq) = Tmp when
œÖimp = maxmp
q=1(œÖiq) for i = 1, 2, . . . , k. The procedure simulates n0 samples
initially and computes the sample mean ¬ØXiq and sample variance S2
iq(ni)
for i = 1, 2, . . . , k and q = 1, 2, . . . , œâ. For q = 1, 2, . . . , œâ, it ranks the
sample means such that ¬ØXb1q ‚â§¬ØXb2q ‚â§. . . ‚â§¬ØXbkq and computes the t
score
Tq =
¬ØXb2q ‚àí¬ØXb1q

S2
b2q(nb2)/nb2 + S2
b1q(nb1)/nb1
.
It then ranks the t scores Tq for q = 1, 2, . . . , œâ such that Tq1 ‚â§Tq2 ‚â§. . . ‚â§
Tqœâ. Let P = (P ‚àó)1/(k‚àí1), nbs = min(nb1, nb2). If Tq > tnbs‚àí1,P , then
we declare that system b1 is non-dominated with respect to performance
measure q.
The procedure terminates when it Ô¨Ånds the non-dominated
systems under mp performance measures or the incremental sample size of

252
Crafts of Simulation Programming
system i Œ¥i = 0 for i = 1, 2, . . . , k. Note that a system can be declared
non-dominated under more than one performance measure.
In each iteration, the procedure Ô¨Ånds the largest Tq that is less than
tnbs‚àí1,P , i.e., a non-dominated system has not been declared and a system
is the most likely to be declared non-dominated with respect to perfor-
mance measure q than other performance measures. Hence, the procedure
allocates more samples according to the estimated incremental sample size
of performance measure q. This approach is similar to the greedy algorithm
of linear programming.
14.2.3
The Incomplete Pareto Set Selection Procedure
We denote the extended procedure: the SRSIP (Sequential Ranking and
Selection of an Incomplete Pareto Set) procedure.
Remark: Specify mp ‚â§œâ, the number of performance measures that
non-dominated systems will be selected and n0 the number of initial sample
size. Initialize the counter of the number of systems that has been declared
non-dominated l = 0.
The SRSIP Procedure:
(1) For each performance measure q = 1, 2, . . . , œâ perform steps 2 and 3 of
the SRS procedure.
(2) Rank the t scores such that Tq1 ‚â§Tq2 ‚â§. . . ‚â§Tqœâ.
(3) For each q such that Tq > tnbs‚àí1,P , we declare that system b1 is non-
dominated with respect to performance measure q and set l = l + 1.
(4) Find the performance measure ql such that Tql
= max(Tq|Tq
<
tnbs‚àí1,P , q = 1, 2, . . . , œâ).
Note that the degrees of freedom nbs ‚àí1
may be diÔ¨Äerent between diÔ¨Äerent performance measures.
(5) For each performance measure q such that Ni,r+1 ‚â§Ni,r, for i =
1, 2, . . . , k, then set l = l + 1.
(6) If l ‚â•mp, then go to step 8.
(7) If Ni,r+1 ‚àíNi,r < 5, set Œ¥i,r+1 = (Ni,r+1 ‚àíNi,r)+.
Otherwise, set
Œ¥i,r+1 = 5.
Simulate Œ¥i,r+1 additional samples for system i.
Set
Ni,r+1 = Ni,r + Œ¥i,r+1 and r = r + 1. Go to step 1.
(8) Return the best system of each performance measures ql, for l =
1, 2, . . . , mp.
The procedure can easily be modiÔ¨Åed when the total sample size is Ô¨Åxed.
Furthermore, the stopping conditions can be revised so that the procedure

Multi-Objective Selection
253
terminates when non-dominated systems are declared from mp performance
measures and/or non-dominated systems are declared under certain perfor-
mance measures. Let Œæi denote the probability that the selected system i
is non-dominated. The SRSIP procedure guarantees that the selected sys-
tems i are non-dominated with Œæi ‚â•P ‚àó; but there is no guarantee that all
non-dominated systems are selected. Conversely, there is no more 1 ‚àíP ‚àó
probability that a selected system is dominated. Let Mp denote the current
(incomplete) Pareto set. If the size of Mp is I = |Mp|, then the probabil-
ity that the selected Pareto set contains a dominated system is less than
(1 ‚àíP ‚àó)I.
14.2.4
The Two-Stage Pareto Set Selection Procedure
Once those non-dominated systems (that are the best in at least one ob-
jective) are selected, a subsequent process can be performed to select all
non-dominated systems. Let M C
p denote the complement of Mp, i.e., the
set of systems under consideration j such that j /‚ààMp. For each system
j‚Ä≤ ‚ààM C
p , we will compare it against every system i ‚ààMp to Ô¨Ånd other non-
dominated systems. This process is carried out by setting each j‚Ä≤ ‚ààM C
p as
non-dominated and is (temporarily) designated as non-dominated when no
design i ‚ààMp is found to dominate it. Note that Mp is updated sequen-
tially, i.e., each time a non-dominated design is found. The procedure then
performs all-pairwise comparisons among systems that are (temporarily)
designated as non-dominated to remove systems that are dominated.
In the best scenarios that no more than one system is (temporarily)
designated as non-dominated, no pairwise comparisons is required. In the
worst scenarios that all systems that are not in the Pareto set are (temporar-
ily) designated as non-dominated, with the comparisons already performed
the second-phase has performed all-pairwise comparisons among the k sys-
tems. However, with the sample sizes allocation strategy described in Sec-
tion 14.2.3 there is no statistical guarantee that the non-dominated systems
that are not the best in any objectives will be included in the Pareto set.
The reason is that under some performance measure q the best system may
be much better than all other systems, i.e., Œºi2q ‚àíŒºi1q >> Œºiaq ‚àíŒºibq where
a > b and a, b Ã∏= 1. Note that Œºi1q ‚â§Œºi2q ‚â§. . . ‚â§Œºikq. Consequently, rel-
atively smaller sample sizes are enough to select the best system with the
required precision; but the allocated sample sizes are not large enough to
rank any other two systems with the required precision. Note that that the
smaller the diÔ¨Äerence of Œºiaq ‚àíŒºibq the larger the required sample sizes to

254
Crafts of Simulation Programming
compare these two systems correctly.
14.2.5
Incorporating IndiÔ¨Äerence-Zone
A promising solution is to incorporate the indiÔ¨Äerence-zone approach into
selecting the Pareto set. Let dq be the user speciÔ¨Åed indiÔ¨Äerence amount
of performance measure q.
For i = 1, . . . , k and q = 1, 2, . . . , œâ, let the
required sample size of design i based on performance measure q
Niq = max(n0 + 1, ‚åà(hSiq(n0))/dq)2‚åâ).
(14.4)
Then the required sample size for design i will be
Ni =
œâ
max
q=1 Niq.
(14.5)
The procedure then allocates all Ni ‚àín0 samples for design i in the second
stage. The selection process is developed as a two-stage-Pareto-set-selection
procedure (TSP), i.e., the procedure Ô¨Årst selects non-dominated systems
that are the best in some performance measure, then selects other non-
dominated systems that are not the best in any performance measures.
Figure 14.1 lists a high-level Ô¨Çowchart of TSP.
The rationale is that we are performing several pairwise comparisons
among systems in each performance measure. Note that if 0 < dabq = Œºaq ‚àí
Œºbq, then Pabq = Pr[ ¬ØXbq < ¬ØXaq] ‚â•0.5. The value of Pabq increases as the
value of dabq increases. With the sample sizes Ni, if the true performance
measure q of designs are deviated more than dq, then the procedure can
rank any two systems correctly with high conÔ¨Ådence, e.g., Pr[ ¬ØXbq < ¬ØXaq] ‚â•
P = (P ‚àó)1/(k‚àí1) when Œºbq + dq ‚â§Œºaq.
Consequently, we will have P ‚àó
conÔ¨Ådence that the selected best systems are non-dominated. Similarly,
when selecting other designs that belong to the Pareto set but are not the
best in any performance measures, there is P probability that the selected
design belongs to the Pareto set when for each incumbent non-dominated
design there exists some q such that the qth performance measure of the
selected design is at least dq better.
On the other hand, if the true performance measures are deviated no
more than dq for some q, then the procedure cannot rank any two systems
correctly with high conÔ¨Ådence and consequently the selected systems may
be dominated (even though for practical purposes they are considered to
be indiÔ¨Äerent with respect to a particular performance measure). For TSP,
under the assumption that the performance measures deviated more than
dq for all q, the P(CS) is the probability of identifying the correct Pareto
set.

Multi-Objective Selection
255
14.3
Empirical Experiments
In this experiment, we test the multi-objective selection procedures SR-
SIP and TSP. In order to compare with other known Pareto-set-selection
procedures we use a similar setting in [Lee et al. (2004)].
14.3.1
Experiment 1: The Parameter mp = 2
The number of systems under consideration k = 5 and the number of
performance measures œâ = 3. The means to generate the systems are listed
in Table 14.2. The variance of all systems is 92. The indiÔ¨Äerence amount of
performance measures 1, 2, 3, are d1 = 1, d2 = 4, and d3 = 8. From Table
14.2, system 3 is dominated by systems 1 and 2; system 4 is dominated by
system 2; system 5 is dominated by systems 2 and 4; and systems 1 and 2
are non-dominated systems.
With mp = 2, the procedure will select non-dominated systems from
two performance measures. We perform 10,000 independent experiments
to obtain the actual P(CS). The number of times we successfully selected
the true best system in each performance measure is counted among the
10,000 independent experiments. P(CS), the correct selection proportion,
is then obtained by dividing this number by 10,000.
Table 14.3 lists the experimental results of SRSIP. Note that the correct
selection under performance measures 1, 2, and 3 are systems 1, 2, and 1,
respectively. The observed P(CS) under performance measures 1, 2, and 3
are, respectively, 0.6752, 0.9203, and 0.9999; which are, as expected, less
than, around, and greater than the nominal value of 0.90. With mp = 2
the allocated sample sizes are neither large enough nor distributed correctly
to provide the required probability guarantee to rank systems correctly
under performance measure 1, which is likely to have the smallest t score.
Consequently, the Pareto set contains only systems 1 and 2 0.8080 fraction
of the time. System 3 is incorrectly included in the Pareto set at least 0.0962
fraction of the time. In this case the allocated sample sizes are only large
enough to have 90% conÔ¨Ådence that the selected system is non-dominated
when the system is deviated more than 2 (i.e., 42-40 under performance
measure 2). Furthermore, most of the samples are allocated to rank systems
2 and 4. On the other hand, while system 3 deviated from system 1 exactly
2 (i.e., 18-16) under performance measure 1, the samples are not allocated
properly to rank systems 1 and 3 with the desired precision.
Table 14.4 shows the sample sizes among the three algorithms: SRSIP,

256
Crafts of Simulation Programming
TOAP, and MOCBA. Under each algorithm, the T column lists the allo-
cated sample size for each system and the Œ∏ column lists the proportion of
the sample size allocated for each system. The critical constant h = 2.747
and with variance œÉ2
iq = 92, the theoretical required sample sizes based on
performance measure 3 are 10 (i.e., ‚åà(hœÉi1/d3)2‚åâ= ‚åà(2.747 ‚àó9/8)2‚åâ), 10,
8, 7, and 6, respectively.
The theoretical required sample sizes base on
performance measure 2 are 39 (i.e., ‚åà(hœÉi2/d2)2‚åâ= ‚åà(2.747 ‚àó9/4)2‚åâ), 153,
25, 153, and 68, respectively. Hence, the TOAP for systems 1 through 5
are, respectively, 39 (i.e., max(10, 39)), 153, 25, 153, and 68. The average
total sample size allocated by the SRSIP procedure is 421, which is slightly
less than 436 (i.e., TOAP). This is consistent with the results of [Lee et al.
(2004)]. They suggest the reason may be due to the fact that sequential
procedures can make use of the sampling information from the previous
steps to make decisions regarding the allocation of additional samples. The
total sample size of the MOCBA procedure is less than that of SRSIP
and TOAP. Note that the goal of the MOCBA procedure is diÔ¨Äerent than
SRSIP and TOAP.
14.3.2
Experiment 2: The Parameter mp = 3
With mp = 3, the procedure will select non-dominated systems from three
performance measures. Tables 14.5 and 14.6 list the results of SRSIP. Ta-
bles 14.7 and 14.8 list the results of TSP. For SRSIP, the observed P(CS)
under all performance measures are greater than the nominal value of 0.9
with the allocated sample size 1414 less than the theoretical value of 1598.
The results indicate that system 1 are non-dominated under two diÔ¨Äer-
ent performance measures. This information may help decision maker to
select system 1 since it is non-dominated in more performance measures
than other systems. As expected, the observed P(CS) of TSP under all
performance measures are greater than those of SRSIP with larger sample
sizes. The sample sizes allocated by the TSP procedure are large enough
to rank systems with the desired precision when the diÔ¨Äerences between
systems (under all performance measures) are greater than the indiÔ¨Äerence
amounts speciÔ¨Åed for each performance measure. The Pareto set contains
only systems 1 and 2 by SRSIP and TSP, respectively, 0.9622 and 0.9971
fraction of the time. For TSP, systems 3, 4, and 5 are incorrectly included
in the Pareto set approximately 0.0021, 0.0002, and 0.0064 fraction of the
time, respectively. For both procedures, the allocated sample sizes are close
to their theoretical values.

Multi-Objective Selection
257
Table 14.2
Means to generate the systems
Systems
Mean1
Mean2
Mean3
1
16
44
56
2
17
40
64
3
18
45
65
4
19
42
66
5
20
43
67
Table
14.3
Proportion
of
a
system
having the smallest performance mea-
sure of experiment 1
System
PM1
PM2
PM3
1
0.6752
0.0045
0.9999
2
0.2065
0.9203
0.0001
3
0.0962
0.0008
0
4
0.0149
0.0545
0
5
0.0072
0.0199
0
14.3.3
Experiment 3: The Parameter mp = 3
In this experiment, not all non-dominated systems have the best perfor-
mance measure in at least one objective. The means to generate the sys-
tems are listed in Table 14.9. The variance of all systems is 92. From Table
14.9, systems 4 and 5 are dominated by systems 2 and 3; and systems 1, 2,
and 3 are non-dominated systems. Tables 14.10 and 14.11 list the results of
TSP. The procedure correctly selected systems 1, 2, and 3 as non-dominated
systems 0.9669 fraction of the time; and systems 4 and 5 are incorrectly
included in the Pareto set approximately 0.027 and 0.064 fraction of the
time, respectively. The TSP is derived based on the least favorable conÔ¨Ågu-
ration and is conservative. Consequently, it achieves high P(CS) with large
sample sizes. For example, the second performance of systems 2 and 3 are
39 and 41, respectively; both values deviated more than the indiÔ¨Äerence
amount (i.e., d1 = 1) from 44 (the second performance measure of system
1). Hence, the allocated sample sizes are suÔ¨Écient to rank systems 1 and
2, and systems 1 and 3 with the required precision.
14.4
Summary
In this chapter, we present a framework for the ranking and selection prob-
lem when the systems are evaluated with more than one performance mea-
sure. The procedure incorporates the concept of Pareto optimality into the

258
Crafts of Simulation Programming
Table 14.4
Comparison of SRSIP with TOAP and MOCBA
System
SRSIP
TOAP
MOCBA
No.
T
Œ∏
T
Œ∏
T
Œ∏
1
82
19.5
39
8.9
88
29.5
2
132
31.3
153
34.9
110
37.1
3
53
12.6
25
5.7
22
7.5
4
96
22.8
153
34.9
48
16.0
5
58
13.8
68
15.6
29
9.9
Total
421
100
436
100
297
100
Table 14.5
Proportion of a system hav-
ing the smallest performance measure
of experiment 2 by SRSIP
System
PM1
PM2
PM3
1
0.9370
0.0013
0.9999
2
0.0426
0.9793
0
3
0.0142
0.0007
0.0001
4
0.0050
0.0135
0
5
0.0012
0.0052
0
Table
14.6
Comparison
of
SRSIP
with
TOAP
System
SRSIP
TOAP
No.
T
Œ∏
T
Œ∏
1
454
32.1
612
38.3
2
532
37.6
612
38.3
3
164
11.6
153
9.6
4
175
12.4
153
9.6
5
89
6.3
68
4.2
Total
1414
100
1598
100
ranking and selection scheme, and attempts to Ô¨Ånd non-dominated systems
in the Pareto set rather than a single ‚Äúbest‚Äù system.
We present two
procedures to solve the problem. The Ô¨Årst procedure attempts to select an
incomplete Pareto set, i.e., all non-dominated systems that are the best in
at least one performance measure; whereas the second procedure attempts
to select the Pareto set, i.e., all non-dominated systems. These procedures
are versatile and easy to state. However, the TSP procedure is derived
based on the least favorable conÔ¨Åguration and is conservative.

Multi-Objective Selection
259
Table
14.7
Proportion
of
a
sys-
tem having the smallest performance
measure
of
experiment
2
by
TSP
System
PM1
PM2
PM3
1
0.9654
0
1
2
0.0345
0.9994
0
3
0.0001
0
0
4
0
0.0006
0
5
0
0
0
Table 14.8
Comparison of TSP with TOAP
System
TSP
TOAP
No.
T
Œ∏
T
Œ∏
1
613
20.0
612
20.0
2
610
20.0
612
20.0
3
611
20.0
612
20.0
4
607
20.0
612
20.0
5
612
20.0
612
20.0
Total
3053
100
3060
100
Table 14.9
Means to generate the systems
Systems
Mean1
Mean2
Mean3
1
16
44
56
2
18
39
64
3
17
41
65
4
19
42
66
5
20
43
67
Table 14.10
Proportion of a system
having the smallest performance mea-
sure of experiment 3
System
PM1
PM2
PM3
1
0.9675
0
1
2
0
0.9997
0
3
0.0325
0.0003
0
4
0
0
0
5
0
0
0
Table 14.11
Comparison of TSP with TOAP
System
TSP
TOAP
No.
T
Œ∏
T
Œ∏
1
613
20.0
612
20.0
2
610
20.0
612
20.0
3
611
20.0
612
20.0
4
607
20.0
612
20.0
5
612
20.0
612
20.0
Total
3053
100
3060
100

260
Crafts of Simulation Programming
Fig. 14.1
High-level Ô¨Çowchart of TSP

Chapter 15
Generic Selection with Constraints
[Andrad¬¥ottir and Kim (2010)] develop procedures to Ô¨Ånd the system with
the best primary performance measure in the presence of a stochastic con-
straint on a secondary performance measure, i.e., selection with constraints.
Their fully sequential procedures increase one sample for each system that
is still under consideration at each iteration and eliminate systems from
further simulation when the partial sum is greater than a threshold. These
procedures are eÔ¨Écient in terms of sample sizes but not necessarily in terms
of runtime.
[Morrice and Butler
(2006)] extend the multiple-attribute-
utility theorem [Butler et al. (2010)] to perform selection with constraints.
However, in some cases, it is not straightforward to assign utility scores.
There are no hard constraints in selecting the Pareto set; however in
selection with constraints, systems that do not satisfy the constraints will
be removed from further consideration. The bounds of current selection
with constraints are based on user speciÔ¨Åed values of the underlying per-
formance measures. Hence, it is possible that there is no feasible solutions
with a given performance bound. We propose using the relative perfor-
mance measures as the constraints. That is, systems with the performance
measure within a user-speciÔ¨Åed amount of the unknown best are consid-
ered as feasible systems. In this setting, there always will be feasible solu-
tions with a given indiÔ¨Äerence amount of a particular performance measure.
We call this a generic selection-with-constraint procedure because the con-
straints can be a combination of user speciÔ¨Åed values and/or user speciÔ¨Åed
indiÔ¨Äerence amounts, which can be either absolute or relative.
261

262
Crafts of Simulation Programming
15.1
Methodologies
In this section, we formulate a generic selection-with-constraints procedure.
Please refer to Section 14.1 for notations. As with most selection proce-
dures, the proposed selection procedures require the input data to be i.i.d.
normal. However, the variance can be diÔ¨Äerent across systems. Many per-
formance measures of interest are taken over some average of a sample path
or a batch of samples. Thus, many applications tend to have a normally
distributed simulation output.
If the non-normality of the samples is a
concern, users can use batch means to ‚Äúmanufacture‚Äù samples that appear
to be i.i.d. normal, as determined by the tests of independence and nor-
mality (see, e.g., [Chen and Kelton (2007)]). In the selection procedures
described below, the sampling operations can be carried out independently
across systems. Hence, one can deploy the selection procedures in a parallel
and distributed environment.
15.1.1
Multi-Objective Selection
When there are multiple selection criteria, we use the Bonferroni inequality
to compute the required precision in each objective (performance measure).
For example, if there are œâ objectives and the desired overall P(CS) is P ‚àó,
then the required P(CS) in each objective is P ‚àó
œâ = 1 ‚àí(1 ‚àíP ‚àó)/œâ. Let
q = 1 be the primary performance measure. In selection with constraints,
the goal is to Ô¨Ånd the best feasible system
arg min
i=1,...,k Œºi1
s.t. Œºiq < Œº0q + d‚àó
q for q = 2, 3, . . . , œâ.
Here Œº0q and d‚àó
q are, respectively, the user speciÔ¨Åed standard and indiÔ¨Äer-
ence amount of performance measure g and each constraint will hold with
probability at least P ‚àó
œâ.
When there are hard constraints, say Cq for some q ‚àà{1, 2, . . . , œâ},
systems i having Œºiq ‚â•Cq will be removed from consideration.
To in-
corporate the indiÔ¨Äerence-zone approach, Cq can be written as Œº0q + d‚àó
q,
where Œº0q is the soft constraint of performance measure q. It is ideal to
select desirable systems i having Œºiq < Œº0q; however, it is acceptable if
Œº0q ‚â§Œºiq < Œº0q + d‚àó
q. Hence, systems i having Œºiq < Œº0q + d‚àó
q are feasible
systems. With the indiÔ¨Äerence-zone approach, desirable systems will be
included in the subset with high probability; the acceptable systems may
be included in the subset, but there is no probability guarantee. Let ¬ØXiq

Generic Selection with Constraints
263
denote the sample mean of the qth performance measure of system i. The
proposed procedure will treat systems i having ¬ØXiq < Œº0q +d‚àó
q/2 as feasible
systems.
With Cq = Œº0q +d‚àó
q as the constraints, users need to specify the indiÔ¨Äer-
ence amount d‚àó
q and the value Œº0q. However, users may not have the priori
information to specify a meaningful Œº0q. We propose new constraints us-
ing the technique of comparison with the best. With the indiÔ¨Äerence-zone
approach, we intend to have Cq = Œºi1q+d‚àó
q. It is ideal to select desirable sys-
tems i having Œºiq = Œºi1q; however, it is acceptable if Œºi1q < Œºiq < Œºi1q +d‚àó
q.
Hence, systems i having Œºiq < Œºi1q + d‚àó
q are feasible systems. However,
the best system (i1) and its mean (Œºi1q) are unknown. Let system bl be
the system having the lth smallest sample mean of performance measure q,
i.e., ¬ØXb1q ‚â§¬ØXb2q ‚â§¬∑ ¬∑ ¬∑ ‚â§¬ØXbkq. The proposed procedure will treat systems
bl having ¬ØXblq < ¬ØXb1q + d‚àó
q/2 as feasible systems.
That is, the bounds
Œº0q + d‚àó
q/2 is replaced by ¬ØXb1q + d‚àó
q/2. This is analogous to Comparison
With the Standard vs. Comparison With a Control. The true means of
the control Œºi1q are unknown and need to be estimated. Using comparison
with a control as constraint also allows us to exploit using common random
number to increase eÔ¨Éciency.
15.1.2
A Generic Selection-With-Constraints Procedure
In this section, we deÔ¨Åne additional notations and formulate a selection-
with-constraints problem. We call this the SWCG (Selection With Con-
straints - Generic) Procedure.
Let 0 ‚â§œâ‚Ä≤ ‚â§œâ, performance mea-
sures q = 2, 3, . . . , œâ‚Ä≤ will be subjected to the constraints of comparison
with a control (the best being the control) and performance measures
q = œâ‚Ä≤ + 1, œâ‚Ä≤ + 2, . . . , œâ will be subjected to the constraints of compar-
ison with the standard. That is, the goal is to Ô¨Ånd the best feasible system
arg min
i=1,...,k Œºi1
s.t. Œºiq < Œºi1q + d‚àó
q for q = 2, 3, . . . , œâ‚Ä≤
Œºiq < Œº0q + d‚àó
q for q = œâ‚Ä≤ + 1, œâ‚Ä≤ + 2, . . . , œâ.
Let Œ∏ be the set of systems that are still under consideration and is
initialized to include all k systems. Note that Œ∏ will be changed from iter-
ation to iteration. Let ¬ØXiq denote the sample mean of the qth performance
measure of system i. Let ¬ØXb1q = mini‚ààŒ∏ ¬ØXiq and let U( ¬ØXb1q) be the upper

264
Crafts of Simulation Programming
P ‚àóconÔ¨Ådence limits of Œºb1q. Then
diq = max(d‚àó
q, ¬ØXiq ‚àíU( ¬ØXb1q)) for q = 1, 2, . . . , œâ‚Ä≤.
(15.1)
For the primary performance measure, the sample size will be computed
according to Eq. (4.2) with d‚àóreplaced by di1.
For the secondary per-
formance measures, we are performing a selecting only and/or all the
best systems (for q = 2, 3, . . . , œâ‚Ä≤) or Comparison With the Standard (for
q = œâ‚Ä≤ + 1, œâ‚Ä≤ + 2, . . . , œâ).
Furthermore, to take into account the dif-
ference of sample means when computing the required sample sizes for
q = œâ‚Ä≤ + 1, œâ‚Ä≤ + 2, . . . , œâ, we use the control distance
diq =
 max(d‚àó
q, L( ¬ØXiq) ‚àíŒº0q) when ¬ØXiq > Œº0q
max(d‚àó
q, Œº0q ‚àíU( ¬ØXiq)) when ¬ØXiq ‚â§Œº0q.
(15.2)
Here L( ¬ØXiq) and U( ¬ØXiq) are, respectively, the lower and upper P ‚àóconÔ¨Å-
dence limits of Œºiq.
There is no more than Œ≤ = (1‚àíP ‚àó)/œâ probability that a system that is
infeasible with respect to performance measure q be declared feasible. Con-
sequently, with these sample sizes, we have (approximately) P ‚àóconÔ¨Ådence
that the selected system is a feasible d‚àó
q-near-best system.
To improve eÔ¨Éciency of the procedure, we also compare systems that
are still under consideration with systems that are found to be feasible.
Note that Œ∏ contains systems that are declared to be feasible and systems
that need more sampling to verify whether they are feasible. Systems that
are found to be infeasible are excluded from Œ∏.
Let S2
iq(Ni) denote the
sample variance of the qth performance measure of system i with sample
size Ni. When Xiqj is i.i.d. normal, it is known that the random variable
Yi1j = ¬ØXi1 ‚àí¬ØXj1 (i Ã∏= j) has approximately a t distribution with fij d.f.,
where
fij =
(S2
i1(Ni)/Ni + S2
j1(Nj)/Nj)2
(S2
i1(Ni)/Ni)2/(Ni ‚àí1) + (S2
j1(Nj)/Nj)2/(Nj ‚àí1);
(15.3)
see [Law
(2014)] for details.
Since fij will not, in general, be an inte-
ger, interpolation will probably be necessary. The procedure will eliminate
system j such that ¬ØXj1 >
¬ØXi1 + wij for some feasible system i.
Here
wij = tfij,P

S2
i1(Ni)/Ni + S2
j1(Nj)/Nj is the one-tailed P(= (P ‚àó
œâ)1/(k‚àí1))
conÔ¨Ådence interval half width. The SWCG procedure is as follows.

Generic Selection with Constraints
265
A Generic Selection-With-Constraints Procedure
(1) Let Ni,r be the sample size allocated for system i and ¬ØXiq,r be the
sample mean of the qth performance measure of system i at the rth
iteration. Simulate n0 samples for all systems. Set the iteration number
r = 0, and N1,r = N2,r = ¬∑ ¬∑ ¬∑ = Nk,r = n0. Specify the value of the
indiÔ¨Äerence amount dq for q = 1, 2, . . . , œâ, the soft constraints Œº0q for
q = œâ‚Ä≤ +1, œâ‚Ä≤ +2, . . . , œâ, and the required precision P ‚àó. Let Œ∏ be the set
of systems that are still under consideration and is initialized to include
all k systems. Compute P ‚àó
œâ = 1 ‚àí(1 ‚àíP ‚àó)/œâ and P = (P ‚àó
œâ)1/(k‚àí1).
The critical constants h1 and h3 are obtained with k, n0, and P ‚àó
œâ.
(2) Calculate the sample means and sample variances for each performance
measure of each system. Obtain the index b1(= arg mini‚ààŒ∏ ¬ØXi1), i.e.,
the system having the smallest sample mean of the primary perfor-
mance measure.
(3) Calculate the new sample sizes
Ni1,r+1 = max(n0, ‚åà(h1Si1(Ni,r)/di1)2‚åâ), for i ‚ààŒ∏,
Niq,r+1 = max(n0, ‚åà(2h3Siq(Ni,r)/diq)2‚åâ),
for i ‚ààŒ∏ and q = 2, 3, . . . , œâ‚Ä≤,
and
Niq,r+1 = max(n0, ‚åà(2tNi,r‚àí1,P Siq(Ni,r)/diq)2‚åâ),
for i ‚ààŒ∏ and q = œâ‚Ä≤ + 1, œâ‚Ä≤ + 2, . . . , œâ.
Here diq, for i ‚ààŒ∏ and q = 1, 2, . . . , œâ‚Ä≤, are computed according to
Eq. (15.1), diq, for i ‚ààŒ∏ and q = œâ‚Ä≤ + 1, œâ‚Ä≤ + 2, . . . , œâ, are computed
according to Eq. (15.2), and S2
iq(Ni,r) is the sample variance of the qth
performance measure of system i with sample size Ni,r.
(4) Let wib1q = tfib1,P

S2
iq(Ni,r)/Ni,r + S2
b1q(Nb1,r)/Nb1,r. For each sys-
tem i ‚ààŒ∏, if ¬ØXiq > ¬ØXb1q + wib1q for some q = 2, 3, . . . , œâ, then remove
system i from Œ∏.
(5) Let wi0q = tNi,r‚àí1,P Siq(Ni,r)/

Ni,r. For each system i ‚ààŒ∏, if ¬ØXiq >
Œº0q + wi0q for some q = œâ‚Ä≤ + 1, œâ‚Ä≤ + 2, . . . , M, then remove system
i from Œ∏. For each system i ‚ààŒ∏, if ¬ØXiq < Œº0q ‚àíwi0q for some q =
œâ‚Ä≤ + 1, œâ‚Ä≤ + 2 . . . , œâ, then set Niq,r+1 = Niq,r.

266
Crafts of Simulation Programming
(6) For each i ‚ààŒ∏ having Niq,r+1 = Niq,r, for q = 1, 2, . . . , œâ, then for each
j ‚ààŒ∏ compute wijq = tfij,P

S2
iq(1, Ni,r)/Ni,r + S2
jq(1, Nj,r)/Nj,r. If
¬ØXj1 > ¬ØXi1 + wij1, then remove system j from Œ∏.
(7) If Œ∏ = ‚àÖ, then terminate the program. There is no feasible solution.
(8) Let
Ni,r+1 =
max
q=1,...,œâ Niq,r+1
and let
N ‚Ä≤
i,r+1 =
min
q=1,...,œâ{Niq,r+1 ‚àíNiq,r|Niq,r+1 ‚àíNiq,r > 0}.
If Ni,r+1 ‚â§Ni,r, for i = 1, 2, . . . , k, go to step 10.
(9) Simulate additional min(N ‚Ä≤
i,r+1, ‚åà(Ni,r+1 ‚àíNi,r)+/2‚åâ) samples for sys-
tems i ‚ààŒ∏. Set r = r + 1. Go to step 2.
(10) Select system b1 = mini‚ààŒ∏ ¬ØXi1.
In the case that there is only one secondary performance measure, a
tighter lower bound can be achieved. Let Œ≤1 be the probability that an
unacceptable system is included in the subset and let Œ≤2 be the probability
that the best feasible system is not selected when compared to other feasible
systems in isolation. When Œ≤1 = Œ≤2 = Œ≤, [Andrad¬¥ottir and Kim (2010)]
show that
P(CS) ‚â•2(1 ‚àíŒ≤)(k‚àí1)/2 ‚àí1 ‚àíŒ≤ = P ‚àó.
(15.4)
15.1.3
Variance as the Constraint
[Batur and Choobineh (2010)] discuss a selection procedure based on both
the mean and variance. In this section, we implement a procedure with
variance as the constraint. The goal is to Ô¨Ånd the best feasible system
arg min
i=1,...,k Œºi
s.t. œÉ2
i /œÉ2
j1 < d‚àó
r for i = 1, 2, 3, . . . , k.
Again, for the primary performance measure, the sample size will be
computed according to Eq. (4.2) with d‚àóreplaced by di = max(d‚àó, ¬ØXi ‚àí
U( ¬ØXb1)) for i = 1, 2, 3, . . . , k. For the secondary performance measure, we
are performing a selecting only and/or all the best systems with respect to
variance. From Section 10.2.3, once the indiÔ¨Äerence ratio d‚àó
r is speciÔ¨Åed,
the required sample size Nd‚àó
r (i.e. d.f.) to select feasible systems can be

Generic Selection with Constraints
267
determined. Hence, we can set the initial sample sizes n0 = Nd‚àó
r and then
the procedure is simpliÔ¨Åed to select the best system with respect to the
primary performance measure. The procedure is as follows.
SWCG with variance as the constraint
(1) Let Œ∏ be the set of systems that are still under consideration and is ini-
tialized to include all k systems. Specify the value of the indiÔ¨Äerence
amounts d‚àóand d‚àó
r for the primary and secondary performance mea-
sures and the required precision P ‚àó. Compute P ‚àó
2 = 1 ‚àí(1 ‚àíP ‚àó)/2.
Determine the required sample size Nd‚àó
r. Set n0 = Nd‚àó
r. The critical
constant h1 is obtained with k, n0, and P ‚àó
2 .
(2) Let Ni,r be the sample size allocated for system i at the rth iteration.
Simulate n0 samples for all systems. Set the iteration number r = 0,
and N1,r = N2,r = ¬∑ ¬∑ ¬∑ = Nk,r = n0.
(3) Calculate the sample variance of each system. Obtain the index j1(=
arg mini‚ààŒ∏ S2
i ), i.e., the system having the smallest variance. Remove
systems i such that (Si/Sj1)2 >

d‚àór from Œ∏.
(4) Calculate the sample mean and sample variance of each system. Ob-
tain the index b1(= arg mini‚ààŒ∏ ¬ØXi), i.e., the system having the smallest
sample mean.
(5) Calculate the new sample sizes
Ni1,r+1 = max(n0, ‚åà(h1Si(Ni,r)/di1)2‚åâ), for i ‚ààŒ∏.
(6) If Ni,r+1 ‚â§Ni,r, for i = 1, 2, . . . , k, go to step 8.
(7) Simulate additional ‚åà(Ni,r+1 ‚àíNi,r)+‚åâsamples for systems i ‚ààŒ∏. Set
r = r + 1. Go to step 4.
(8) Select system b1 = mini‚ààŒ∏ ¬ØXi.
15.1.4
Variance as the Primary Performance Measure
In this section, we implement a procedure with variance as the primary
performance measurement and mean as the constraint. The goal is to Ô¨Ånd
the best feasible system
arg min
i=1,...,k œÉ2
i
s.t. Œºi < Œºi1 + d‚àófor i = 1, 2, 3, . . . , k.
We Ô¨Årst obtain the required sample size Nd‚àó
r with the given d‚àó
r, k, and
P ‚àó
2 . We then set the initial sample sizes n0 = Nd‚àó
r and consequently the

268
Crafts of Simulation Programming
procedure is simpliÔ¨Åed to determine the feasible systems. The procedure is
as follows.
SWCG with Variance as Primary Performance Measure
(1) Let Œ∏ be the set of systems that are still under consideration and is
initialized to include all k systems. Specify the value of the indiÔ¨Äerence
amounts d‚àó
r and d‚àófor the primary and secondary performance mea-
sures and the required precision P ‚àó. Compute P ‚àó
2 = 1‚àí(1‚àíP ‚àó)/2 and
P = (P ‚àó
2 )1/k. Determine the required sample size Nd‚àó
r. Set n0 = Nd‚àó
r.
The critical constant h3 is obtained with k, n0, and P ‚àó
2 .
(2) Let Ni,r be the sample size allocated for system i at the rth iteration.
Simulate n0 samples for all systems. Set the iteration number r = 0,
and N1,r = N2,r = ¬∑ ¬∑ ¬∑ = Nk,r = n0.
(3) Calculate the sample means and sample variances. Obtain the index
b1(= arg mini‚ààŒ∏ ¬ØXi), i.e.,the system having the smallest sample means.
(4) Calculate the new sample sizes Ni,r+1 = max(n0, ‚åà(2h3Si(Ni,r)/di1)2‚åâ),
for i ‚ààŒ∏.
(5) If Ni,r+1 ‚â§Ni,r, for i = 1, 2, . . . , k, go to step 7.
(6) Simulate additional ‚åà(Ni,r+1 ‚àíNi,r)+‚åâsamples for systems i ‚ààŒ∏. Set
r = r + 1. Go to step 3.
(7) For each i ‚ààŒ∏, remove i from Œ∏ when ¬ØXi > ¬ØXb1 + d‚àó/2. Select system
b1 = mini‚ààŒ∏ S2
i (Ni,r).
15.2
Empirical Experiments
In this section, we present some empirical results of performing selection
with constraints.
15.2.1
Selection With Constraints
In this experiment, we test the Selection-With-Constraints Procedure. We
use a similar setting in [Andrad¬¥ottir and Kim (2010)].
The number of
systems under consideration k = 25. There are two performance measures,
i.e., œâ = 2. The primary performance measure
Œºi1 =
‚éß
‚é™
‚é™
‚é®
‚é™
‚é™
‚é©
0,
i = 1, 2, . . . , b ‚àí1,
‚àíd‚àó,
i = b,
0,
i = b + 1, . . . , b + a,
(1 ‚àíi)d‚àó, i = b + a + 1, . . . , k;

Generic Selection with Constraints
269
and the secondary performance measure
Œºi2 =
‚éß
‚é®
‚é©
‚àíœµ, i = 1, 2, . . . , b,
0,
i = b + 1, . . . , b + a,
œµ,
i = b + a + 1, . . . , k.
The primary and secondary performance measures of each system are inde-
pendent. The variances of both performance measures of each system are
1. The initial sample size n0 is set to n0 = 20. Moreover, the indiÔ¨Äerence
amount of the primary performance measure d‚àó
1 and the tolerance œµ are
set to 1/‚àön0. Furthermore, the soft constraint Œº02 = ‚àíœµ and the indif-
ference amount of the secondary performance measure d‚àó
2 = 2œµ, thus, the
hard constraint C2 = œµ. The desired P(CS) is set to P ‚àó= 0.95. We make
10,000 independent replications to obtain the observed P(CS). Note that
with œâ = 2 performance measures P ‚àó
œâ = 1‚àí(1‚àíP ‚àó)/2 = 0.975. The critical
constants h1 = 4.60 of Eq. (4.2) and h3 = 6.23 of Eq. (4.4) are obtained with
n0 = 20, k = 25, and P ‚àó
œâ = 0.975. Furthermore, P = P ‚àó
œâ
1/(k‚àí1) ‚âà0.999.
In cases that there is only one secondary performance measure, a tighter
lower bound can be achieved. Let Œ≤1 be the probability that an unaccept-
able system is included in the subset and let Œ≤2 be the probability that
the best feasible system is not selected when compared to other feasible
systems in isolation.
If Œ≤1 = Œ≤2 = Œ≤, Eq.
(15.4) can be used.
When
P ‚àó= 0.95, Œ≤ ‚âà0.002. Hence, for the SWCGO (SWCG with the parame-
ters optimized) procedure, we set P = (1 ‚àí0.002) = 0.998. Furthermore,
the critical constants h1 = 4.22 of Eq. (4.2) and h3 = 5.88 of Eq. (4.4) are
obtained with k = 25 and P ‚àó
œâ = 0.954 (i.e. 0.99824).
Tables 15.1 and 15.2, respectively, show the experimental results when
the constraint is the standard and the unknown best. We list the observed
P(CS) and the average of the allocated sample sizes T, the standard error
of T and the average number of iterations of SWCG, SWCGO, and AK+
[Andrad¬¥ottir and Kim (2010)]) for various numbers of acceptable systems a
when k = 25 and a+b = 13. The observed P(CS)‚Äôs are all greater than the
speciÔ¨Åed nominal value. However, the allocated sample sizes with unknown
Œºi12 as the constraint are much larger than those with known constrain
Œº02. Both SWCG and SWCGO are conservative, achieve high precision
with large sample sizes. AK+ is eÔ¨Écient in terms of sample sizes, however,
it requires many more iterations than SWCG and SWCGO.

270
Crafts of Simulation Programming
Table 15.1
The performance of selection with constraints: given the standard
and indiÔ¨Äerence amount
b = 13
b = 12
b = 10
b = 7
b = 3
b = 1
a = 0
a = 1
a = 3
a = 6
a = 10
a = 12
SWCG
PCS
0.982
0.979
0.978
0.986
0.982
0.987
T
6494
6335
6065
5646
5096
4789
std(T)
602
623
593
511
454
440
Iter
13
13
13
12
11
9
SWCGO
PCS
0.976
0.972
0.971
0.990
0.984
0.991
T
5816
5711
5483
5197
4774
4585
std(T)
489
468
458
408
387
377
Iter
13
13
13
12
11
10
AK+
PCS
0.960
0.962
0.963
0.963
0.966
0.968
T
3976
3749
3726
3686
3615
3581
ÀÜ
Iter
140
130
130
128
125
124
Table 15.2
The performance of selection with constraints:
given the in-
diÔ¨Äerence amount only
b = 13
b = 12
b = 10
b = 7
b = 3
b = 1
a = 0
a = 1
a = 3
a = 6
a = 10
a = 12
SWCG
PCS
0.967
0.966
0.980
0.979
0.996
0.996
T
11877
11512
10810
9929
9214
9847
std(T)
902
954
993
1108
1377
1806
Iter
11
11
11
11
10
6
SWCGO
PCS
0.963
0.959
0.969
0.987
0.990
1.000
T
10586
10290
9712
8998
8413
8999
std(T)
869
907
930
1044
1200
1630
Iter
11
11
11
11
10
7
15.2.2
Variance as the Constraint
In this experiment, we test the Selection-With-Constraints procedure using
variance as the constraint. We use a similar setting in [Batur and Choobineh
(2010)]. However, the goal of the procedure is diÔ¨Äerent than that of [Batur
and Choobineh (2010)], which is aim to obtain a Pareto set, i.e., the set of
non-dominated systems. The number of systems under consideration k = 5.
The experiment conÔ¨Åguration are listed in Table 15.3. The CSM and CSV
columns list, respectively, the best system when the primary performance
measure is the mean and variance.
For example, system 5 is the best
system when mean is the primary performance measure under conÔ¨Åguration
4 because all other systems do not satisfy the variance constraint. The
required P(CS) P ‚àó= 0.95, the absolute indiÔ¨Äerence amount d‚àó= 0.1 and

Generic Selection with Constraints
271
the relative indiÔ¨Äerence amount d‚àó
r = 1.21.
With Œ¥r =

d‚àór = 1.1, P ‚àó
œâ = 1 ‚àí(1 ‚àíP ‚àó)/2 = 0.975, and k = 5,
the required sample sizes are ne = 3881 and nf = 2636. Hence, we set
n0 = max(ne, nf) = 3881.
Furthermore, with n0 = 3881, P ‚àó
œâ = 0.975,
and k = 5, the critical constant h1 = 3.456.
Note that the theoretical
sample size to achieve P(CS) of P ‚àó
œâ of the best mean is ‚åà(h1S/d‚àó)2‚åâ=
‚åà(3.456 ‚àó1.1/0.1)2‚åâ= 1446 < n0 = 3881. Consequently, the initial sample
size n0 (to achieve P(CS) of P ‚àó
œâ of the variance constraint) is already large
enough to achieve P ‚àó.
We then solve Eq. (15.4) with k = 5 and P ‚àó= 0.95 to obtain Œ≤ ‚âà0.01.
Hence, for the SWCGO procedure, we set P = (1 ‚àí0.01) = 0.99. With
Œ¥r =

d‚àór = 1.1, P ‚àó
œâ = 0.961 (i.e. 0.994), and k = 5, the required sample
sizes are ne = 3495 and nf = 2270. Hence, we set n0 = max(ne, nf) = 3495.
Furthermore, with n0 = 3495, P ‚àó
2 = 0.961, and k = 5, the critical constant
h1 = 3.206. The theoretical sample size to achieve P(CS) of P ‚àó
œâ of the best
mean is ‚åà(h1S/d‚àó)2‚åâ= ‚åà(3.206 ‚àó1.1/0.1)2‚åâ= 1244 < n0 = 3495. Again,
the initial sample size n0 is already large enough to achieve P ‚àó.
Table 15.4 shows the results. The observed P(CS)‚Äôs are all greater than
the speciÔ¨Åed nominal value. The required sample size to achieve P(CS) of
the variance constraint is based on the LFC and is conservative.
15.2.3
Variance as the Primary Performance Measure
In this experiment, we test the Selection-With-Constraints procedure us-
ing mean as the constraint and variance as the primary performance mea-
sure. The conÔ¨Ågurations of the systems are the same as the previous ex-
periment. In this conÔ¨Åguration, we set n0 = nf = 2636. Furthermore,
with n0 = 2636, P = 0.975, and k = 5, the critical constant h3 = 4.281.
The theoretical sample size to achieve P(CS) of P ‚àó
œâ of the best mean is
‚åà(h3S/(d‚àó/2))2‚åâ= ‚åà(4.281 ‚àó1.1/(0.1/2))2‚åâ= 8871, which is greater than
n0 = 2636. Consequently, the initial sample size n0 is not large enough to
achieve P ‚àó.
For the SWCGO procedure, we set n0 = nf = 2270.
With n0 =
2270, P ‚àó
œâ = 0.961, and k = 5, the critical constant h3 = 4.067.
The
theoretical sample size to achieve P(CS) of P ‚àó
œâ of the best mean is
‚åà(h3S/(d‚àó/2))2‚åâ= ‚åà(4.067 ‚àó1.1/(0.1/2))2‚åâ= 8006, which is greater than
n0 = 2270. Hence, the initial sample size n0 is not large enough to achieve
P ‚àó.
Table 15.5 shows the results.
The observed P(CS)‚Äôs are all greater

272
Crafts of Simulation Programming
Table 15.3
ConÔ¨Ågurations of the systems tested
Sys. 1
Sys. 2
¬∑ ¬∑ ¬∑
Sys. i
¬∑ ¬∑ ¬∑
Sys. k
CSM
CSV
ConÔ¨Åg 1
Mean
1
1 + d‚àó
¬∑ ¬∑ ¬∑
1 + d‚àó
¬∑ ¬∑ ¬∑
1 + d‚àó
1
1
Variance
1
d‚àó
r
¬∑ ¬∑ ¬∑
d‚àó
r
¬∑ ¬∑ ¬∑
d‚àó
r
ConÔ¨Åg 2
Mean
1
1 + d‚àó
¬∑ ¬∑ ¬∑
1 + d‚àó
¬∑ ¬∑ ¬∑
1 + d‚àó
1
1
Variance
1
1
¬∑ ¬∑ ¬∑
1
¬∑ ¬∑ ¬∑
1
ConÔ¨Åg 3
Mean
1
1
¬∑ ¬∑ ¬∑
1
¬∑ ¬∑ ¬∑
1
1
1
Variance
1
d‚àó
r
¬∑ ¬∑ ¬∑
d‚àó
r
¬∑ ¬∑ ¬∑
d‚àó
r
ConÔ¨Åg 4
Mean
1
1 + d‚àó
¬∑ ¬∑ ¬∑
1 + (i ‚àí1)d‚àó
¬∑ ¬∑ ¬∑
1 + (k ‚àí1)d‚àó
5
1
Variance
d‚àó
r
(k‚àí1)
d‚àó
r
(k‚àí2)
¬∑ ¬∑ ¬∑
d‚àó
r
(k‚àíi)
¬∑ ¬∑ ¬∑
1
ConÔ¨Åg 5
Mean
1
1 + d‚àó
¬∑ ¬∑ ¬∑
1 + (i ‚àí1)d‚àó
¬∑ ¬∑ ¬∑
1 + (k ‚àí1)d‚àó
1
1
Variance
1
d‚àó
r
¬∑ ¬∑ ¬∑
d‚àó
r
(i‚àí1)
¬∑ ¬∑ ¬∑
d‚àó
r
(k‚àí1)

Generic Selection with Constraints
273
Table 15.4
The performance of selection with constraints (with
variance as the constraint)
ConÔ¨Åg
1
2
3
4
5
SWCG
PCS
1.000
0.9952
0.9978
0.9979
1.000
T
19405
19405
19405
19405
19405
std(T)
0
0
0
0
0
Iter
1
1
1
1
1
SWCGO
PCS
1.000
0.9926
0.9963
0.9984
1.000
T
17475
17475
17475
17475
17475
std(T)
0
0
0
0
0
Iter
1
1
1
1
1
Table 15.5
The performance of selection with constraints (with
mean as the constraint)
ConÔ¨Åg
1
2
3
4
5
SWCG
PCS
1.000
0.9982
0.9959
0.9994
1.000
T
31878
27954
43217
34272
21403
std(T)
7222
5802
390
3915
2573
Iter
3
3
4
3
2
SWCGO
PCS
1.000
0.9962
0.9927
0.9983
1.000
T
29665
25818
39040
30946
19196
std(T)
6558
5350
376
2843
2406
Iter
3
3
4
2
2
than the speciÔ¨Åed nominal value. Even though the required sample size to
achieve P(CS) of P ‚àówith mean as the constraint is not based on the LFC,
the procedure is conservative. First, the SWSG procedure is derived based
on the Bonferroni inequality. Second, for each system the same sample size
is used to calculate all performance measures and the largest sample size
to obtain the required precision P ‚àó
œâ of a particular performance measure is
used, hence, the obtained precision for all other performance measures are
likely to be greater than the nominal value. One approach is to distribute
the probability of 1 ‚àíP ‚àóunequally among œâ performance measures to
minimize the required sample size such that (1‚àíP ‚àó
œâq) = 1‚àíP ‚àó, where P ‚àó
œâq
is the speciÔ¨Åed P(CS) for performance measure q. For example, instead of
setting P ‚àó
œâ = 0.975 when P ‚àó= 0.95, we can set P ‚àó
œâ1 = 0.97 and P ‚àó
œâ2 = 0.98
when the Ô¨Årst performance measure requires more samples to achieve the
same level of precision.

274
Crafts of Simulation Programming
15.3
Summary
We have presented a generic selection-with-constraints procedure. The con-
straints can be a known bound or an unknown bound based on the unknown
best. The procedure is developed based on the Bonferroni inequality and is
conservative, achieves high P(CS) with large sample sizes. Nevertheless, the
procedure is versatile, easy to understand, and simple to implement. Fur-
thermore, inequality Eq. (15.4) developed by [Andrad¬¥ottir and Kim (2010)]
can be used to increase eÔ¨Éciency of the procedure when there is only one
secondary performance measure. A generalized inequality when there are
more than one secondary performance measures is being developed.

Appendix A
Tables of Critical Constants
Table A.1
Values of critical constant h for the subset selection procedure
P ‚àó= 0.90
P ‚àó= 0.95
k
m
v
c
15
20
25
30
15
20
25
30
8
3
1
1
1.853
1.830
1.817
1.808
2.305
2.268
2.247
2.234
8
3
2
1
0.889
0.882
0.878
0.875
1.243
1.232
1.225
1.221
2
2.630
2.586
2.561
2.545
3.070
3.007
2.972
2.949
8
3
3
1
0.329
0.326
0.324
0.323
0.657
0.652
0.648
0.646
2
1.686
1.667
1.656
1.649
2.037
2.011
1.996
1.987
3
3.621
3.533
3.484
3.453
4.122
4.004
3.939
3.897
9
3
1
1
1.968
1.943
1.929
1.920
2.417
2.380
2.358
2.344
9
3
2
1
1.027
1.019
1.013
1.010
1.375
1.362
1.355
1.350
2
2.740
2.694
2.668
2.651
3.180
3.115
3.078
3.055
9
3
3
1
0.508
0.504
0.501
0.500
0.828
0.821
0.816
0.814
2
1.818
1.796
1.784
1.776
2.163
2.133
2.116
2.106
3
3.723
3.630
3.579
3.546
4.221
4.096
4.028
3.984
10
4
1
1
1.740
1.719
1.707
1.699
2.181
2.146
2.126
2.114
10
4
2
1
0.796
0.790
0.787
0.784
1.135
1.126
1.120
1.117
2
2.388
2.350
2.329
2.315
2.810
2.754
2.723
2.703
10
4
3
1
0.270
0.268
0.267
0.266
0.578
0.574
0.571
0.570
2
1.453
1.439
1.431
1.426
1.770
1.751
1.741
1.733
3
2.977
2.920
2.889
2.869
3.3403
3.325
3.282
3.255
10
4
4
1
0
0
0
0
0.150
0.149
0.148
0.148
2
0.929
0.922
0.917
0.914
1.223
1.212
1.206
1.202
3
2.064
2.039
2.024
2.015
2.396
2.362
2.343
2.331
4
3.888
3.788
3.732
3.696
4.381
4.249
4.175
4.129
10
5
1
1
1.453
1.434
1.423
1.417
1.894
1.862
1.844
1.832
10
5
2
1
0.479
0.475
0.474
0.472
0.815
0.808
0.804
0.802
2
2.041
2.008
1.989
1.977
2.459
2.408
2.379
2.361
10
5
3
1
0
0
0
0
0.221
0.219
0.218
0.218
2
1.076
1.067
1.062
1.058
1.392
1.378
1.370
1.365
3
2.506
2.460
2.434
2.418
2.921
2.855
2.818
2.794
10
5
4
1
0
0
0
0
0
0
0
0
2
0.510
0.506
0.504
0.503
0.797
0.791
0.788
0.785
3
1.560
1.544
1.535
1.529
1.871
1.849
1.837
1.829
4
3.027
2.966
2.932
2.910
3.448
3.364
3.318
3.289
275

276
Crafts of Simulation Programming
Table A.2
Values of P(CS) of ratio subset selection with n0 = 20
d‚àó
k
m
v
c
1.2
1.4
1.6
1.8
2.0
2.2
8
3
1
1
0.5698
0.7289
0.8399
0.9102
0.9510
0.9740
8
3
2
1
0.9134
0.9626
0.9849
0.9943
0.9978
0.9991
2
0.2515
0.4283
0.5955
0.7303
0.8275
0.8929
8
3
3
1
0.9376
0.9804
0.9941
0.9983
0.9995
0.9998
2
0.5221
0.7201
0.8501
0.9243
0.9630
0.9822
3
0.0656
0.1558
0.2773
0.4096
0.5346
0.6430
9
3
1
1
0.5250
0.6897
0.8107
0.8901
0.9389
0.9667
9
3
2
1
0.7844
0.9016
0.9587
0.9837
0.9938
0.9976
2
0.2105
0.3772
0.5451
0.6877
0.7950
0.8705
9
3
3
1
0.9088
0.9693
0.9903
0.9970
0.9991
0.9997
2
0.4518
0.6610
0.8102
0.9007
0.9498
0.9752
3
0.0491
0.1245
0.2347
0.3622
0.4876
0.6012
10
4
1
1
0.6014
0.7593
0.8651
0.9286
0.9636
0.9822
10
4
2
1
0.8497
0.9416
0.9792
0.9931
0.9977
0.9992
2
0.3052
0.5018
0.6748
0.8026
0.8862
0.9368
10
4
3
1
0.9481
0.9860
0.9965
0.9991
0.9997
0.9999
2
0.5944
0.7920
0.9051
0.9600
0.9839
0.9937
3
0.1180
0.2612
0.4318
0.5937
0.7253
0.8221
10
4
4
1
0.9841
0.9968
0.9994
0.9998
0.9999
0.9999
2
0.7943
0.9220
0.9733
0.9914
0.9972
0.9991
3
0.3131
0.5372
0.7225
0.8466
0.9195
0.9591
4
0.0265
0.0815
0.1744
0.2931
0.4197
0.5394
10
5
1
1
0.7003
0.8378
0.9187
0.9616
0.9825
0.9924
10
5
2
1
0.9182
0.9739
0.9925
0.9979
0.9994
0.9998
2
0.4401
0.6469
0.7989
0.8934
0.9464
0.9740
10
5
3
1
0.9806
0.9960
0.9992
0.9998
0.9999
0.9999
2
0.7553
0.9006
0.9642
0.9881
0.9962
0.9988
3
0.2370
0.4389
0.6299
0.7755
0.8711
0.9294
10
5
4
1
0.9963
0.9994
0.9999
0.9999
1.0000
1.0000
2
0.9136
0.9763
0.9941
0.9986
0.9996
0.9999
3
0.5328
0.7553
0.8874
0.9526
0.9812
0.9926
4
0.0986
0.2371
0.4108
0.5786
0.7157
0.8171

Bibliography
Abramowiz, M. and Stegun, I. A. (1964). Handbook of Mathematical Functions.
National Bureau of Standards.
Alexopoulos, C., Goldsman, D., Mokashi, A., Nie, R., Sun, Q., Tien, K. W.
and Wilson, J. R. (2014). Sequest: A Sequential Procedure for Estimating
Steady-State Quantiles. Proceedings of the 2014 Winter Simulation Con-
ference, pp. 662‚Äì673.
Andrad¬¥ottir, S. and Kim, S. H. (2010). Fully Sequential Procedures for Comparing
Constrained Systems via Simulation. Naval Research Logistics 57, 5, pp.
403‚Äì421.
Ankenman, B., Nelson, B. L. and Staum, J. (2008). Stochastic Kriging for Sim-
ulation Metamodeling, in Proceedings of the 2008 Winter Simulation Con-
ference, pp. 362‚Äì370.
Arellano-Valle, R. B. and Genton, M. G. (2007). On the Exact Distribution of Lin-
ear Combinations of Order Statistics from Dependent Random Variables.
Journal of Multivariate Analysis 98, pp. 1876‚Äì1894.
Arellano-Valle, R. B. and Genton, M. G. (2008). On the Exact Distribution of the
Maximum of Absolutely Continuous Dependent Random Variables. Statis-
tics & Probability Letters 78, pp. 27‚Äì35.
Batur, D. and Choobineh, F. F. (2010). Mean-Variance Based Randking and
Selection, in Proceedings of the 2010 Winter Simulation Conference, pp.
1160‚Äì1166.
Bechhofer, R. E. (1954). A Single-Sample Multiple Decision Procedure for Rank-
ing Means of Normal Populations with Known Variances. Ann. Math.
Statist 25, pp. 16‚Äì39.
Billingsley, P. (1999). Convergence of Probability Measures, 2nd edn. (John Wiley
& Sons, Inc, New York).
Brassard, G. and Bratley, P. (1988). Algorithmics, Theory and Practice (Prentice-
Hall).
Bratley, P., Fox, B. L. and Schrage, L. E. (1987). A Guide to Simulation, 2nd
edn. (Springer-Verlag, New York).
Butler, J., Morrice, D. J. and Mullarkey, P. W. (2001). A Multiple attribute
utility theory approach to ranking and Selection. Management Science 47,
277

278
Crafts of Simulation Programming
pp. 800‚Äì816.
Chan, N. H., Lee, T. C. M. and Peng, L. A. (2010). On nonparametric Local In-
ference for Density Estimation. Computational Statistics and Data Analysis
54, 2, pp. 509‚Äì515.
Chapman, D. G. (1950). Some Two Sample Tests. Annals of Mathematical Statis-
tics 21, pp. 601‚Äì606.
Chen, C. H., Lin J., Y¬®ucesan, E. and Chick, S. E. (2000). Simulation Budget
Allocation for Further Enhancing the EÔ¨Éciency of Ordinal Optimization.
Journal of Discrete Event Dynamic Systems 10, 3, pp. 251‚Äì270.
Chen, E. J. (2003). Derivative Estimation with Finite DiÔ¨Äerences. Simulation:
Transactions of The Society for Modeling and Simulation International 79,
10, pp. 598‚Äì609.
Chen, E. J. (2004). Using Ordinal Optimization Approach to Improve EÔ¨Éciency
of Selection Procedures. Discrete Event Dynamic Systems 14, 2, pp. 153‚Äì
170.
Chen, E. J. (2008). Selecting Designs With the Smallest Variance of Normal
Populations. Journal of Simulation 2, 3, pp. 186‚Äì194.
Chen, E. J. (2011). A Revisit of Two-Stage Selection Procedures. European Jour-
nal of Operational Research 210, 2, pp. 281‚Äì286.
Chen, E. J. (2012). A Stopping Rule Using the Quasi-Independent Stopping Se-
quence. Journal of Simulation 6, 2, pp. 71‚Äì80.
Chen, E. J. (2013). Some Insights of Using Common Random Numbers in Selec-
tion Procedures. Discrete Event Dynamic Systems 23, 3, pp. 241‚Äì259.
Chen, E. J. (2014a). Range Statistics and Equivalence Tests. Journal of Simula-
tion 8, 2, pp. 143‚Äì150.
Chen, E. J. (2014b). Selection and Order Statistics from Correlated Normal Ran-
dom Variables. Discrete Event Dynamic Systems 24, 4, pp. 659‚Äì668.
Chen, E. J. and Kelton, W. D. (2003). Determining Simulation Run Length with
the Runs Test. Simulation Modelling Practice and Theory 11, (3-4), pp.
237‚Äì250.
Chen, E. J. and Kelton, W. D. (2005). Sequential Selection Procedures: Using
Sample Means to Improve EÔ¨Éciency. European Journal of Operational Re-
search 166, 2, pp. 133‚Äì153.
Chen, E. J. and Kelton, W. D. (2007). A Procedure for Generating Batch-Means
ConÔ¨Ådence Intervals for Simulation: Checking Independence and Normal-
ity. Simulation: Transactions of The Society for Modeling and Simulation
International 83, 10, pp. 683‚Äì694.
Chen, E. J. and Kelton, W. D. (2008). Estimating Steady-State Distributions via
Simulation-Generated Histograms. Computers and Operations Research 35,
4, pp. 1003‚Äì1016.
Chen, E. J. and Kelton, W. D. (2010). ConÔ¨Ådence-Interval Estimation Using
Quasi-Independent Sequences. IIE Transactions 42, 1, pp. 83‚Äì93.
Chen, E. J. and Kelton, W. D. (2014). Density Estimation from Correlated Data.
Journal of Simulation 8, 4, pp. 281‚Äì292.
Chen, E. J. and Lee, L. H. (2014). A Multi-Objective Selection Procedure of
Determining a Pareto Set. Computers & Operations Research 36, pp. 1872‚Äì

Bibliography
279
1879
Chen, E. J. and Li, M. (2010) A New Approach to Estimate the Critical Con-
stant of Selection Procedures. Advances in Decision Sciences, Volume 2010,
Article ID 948359, 12 pages.
Chen, E. J. and Li, M. (2014) Design of Experiments for Interpolation-Based
Metamodels. Simulation Modelling Practice and Theory 44, pp. 14‚Äì25.
Cheng, R. C. H. (1997). The Generation of Gamma Variables with Non-integral
Shape Parameter. Appl. Statist. 26, pp. 71-75.
Chien, C., Goldsman D. and Melamed, B. (1997). Large-Sample Results for Batch
Means. Management Science 43, 9, pp. 288‚Äì1295.
DasGupta, A. (2011). Probability for Statistics and Machine Learning (Springer,
New York).
David, H. A. and Nagaraja, H. N. (2003). Order statistics, 3d edn. (Wiley, New
York).
Devroye, L. J. (1995). Probability and Statistics for Engineering and the Sciences,
4th edn. (Brooks/Cole, Monterey, California).
Dudewicz, E. J. and Ahmed, S. U. (1998). New Exact and Asymptotically Opti-
mal Solution to the Behrens-Fisher Problem, With Tables. American Jour-
nal of Mathematical and Management Sciences 18, pp. 359‚Äì426.
Dudewicz, E. J. and Dalal, S. R. (1975). Allocation of Observations in Ranking
and Selection with Unequal Variances. Sankhya B37, pp. 28‚Äì78.
Dudewicz, E. J., Ma, Y., Mai, E. and Su, H. (2007). Exact Solutions to the
Behrens-Fisher Problem: Asymptotically Optimal and Finite Sample Ef-
Ô¨Åcient Choice Among. Journal of Statistical Planning and Inference 137,
pp. 1584‚Äì1605.
Dudewicz, E. J., Taneja, V. S. (1978). Multivariate Ranking and Selection With-
out Reduction to a Univariate Problem, in Proceedings of the 1978 Winter
Simulation Conference, pp. 207‚Äì210.
Duin, R. P. W. (1976). On the Choice of Smoothing Parameters for Parzen Esti-
mators of Probability Density Functions. IEEE Transactions on Computers
C-25, 11, pp. 1175‚Äì1179.
Dunnett, C. W and Sobel, M. (1955). Approximations to the Probability Integral
and Certain Percentage Points of a Multivariate Analogue of Student‚Äôs t-
distribution. Biometrika 42, pp. 258‚Äì260.
Edwards, D. G. and Hsu, J. C. (1983). Multiple comparisons with the best treat-
ment. Journal of the American Statistical Association 78, pp. 965-971.
EickhoÔ¨Ä, M., McNickle, D. and Pawlikowski, K. (2006). Analysis of the Time
Evolution of Quantiles in Simulation. International Journal on Simulation:
Systems, Science & Technology 7, 6, pp. 44‚Äì55.
Fishman, G. S. (1978). Grouping observations in digital simulation. Management
Science 24, 5, pp. 510-521.
Fishman, G. S. (2001). Discrete-Event Simulation: Modeling Programming and
Analysis. (Springer-Verlag, New York).
Fujimoto, R. M. (2000). Parallel and Distributed Simulation Systems. (Wiley In-
terscience).
Golyandina, N., Pepelyshev, A. and Steland, A. (2012). New Approaches to Non-

280
Crafts of Simulation Programming
parametric Density Estimation and Selection of Smoothing Parameters.
Computational Statistics and Data Analysis 56, pp. 2206‚Äì2218.
Gupta, S. S., Nagel, K. and Panchapakesan, S. (1973). On the Order Statistics
from Equally Correlated Normal Random Variables. Biometrika 60, 2, pp.
403‚Äì413.
Hastings, C., Jr. (1955). Approximations for Digital Computers (Princeton Univ.
Press, Princeton, New Jersey).
Hearne, L. B. and Wegman, E. J. (1994). Fast Multidimensional Density Estima-
tion Based on Random-Width Bins. Computing Science and Statistics 26,
pp. 150‚Äì155.
Heidelberger, P. and Lewis, P. A. W. (1984). Quantile estimation in dependent
sequences. Operations Research 32, pp. 185-209.
Ho, Y. C. (1996). Soft Optimization for Hard Problems. (Computerized Lecture
Via Private Communication/Distribution).
Ho, Y. C., Sreenivas R. S. and Vakili, P. (1992). Ordinal Optimization of DEDS.
Journal of Discrete Event Dynamic Systems 2, pp. 61‚Äì68.
Hoad, K., Robinson, S. and Davies, R. (2010). Automating warm-up length esti-
mation. Journal of the Operational Research Society 61, 6, pp. 1389-1403.
Hoad, K., Robinson, S. and Davies, R. (2011). AutoSimOA: a framework for
automated analysis of simulation output. Journal of Simultion 5, 1, pp.
9-24.
Hogg, R. V., McKean, J. and Craig, A. T. (2012). Introduction to Mathematical
Statics, 7th edn. (Pearson Education).
Hurley, C. and Modarres, R. (1995). Low-storage quantile estimation. Computa-
tional Statistics 10, pp. 311-325.
Iglehart, D. L. (1976). Simulating stable stochastic systems; VI. quantile estima-
tion. Journal of the Association of Computing Machinery 23, pp. 347-60.
John, T. T. and Chen, P. (2006). Lognormal Selection With Applications to
Lifetime Data. IEEE Transactions on Reliability 55, 1, pp. 135‚Äì148.
Jones, D. R., Schonlau, M. and Welch, W. J. (1998). EÔ¨Écient Global Optimization
of Expensive Black-Box Functions. Journal of Glogal Optimization 13, pp.
455‚Äì492.
Kanter, I., Aviad., Y., Reidler, I., Cohen, E. and Rosenbluh, M. (2010). An optical
ultrafast random bit generator. Nature Photonics 4, 1, pp. 58‚Äì61.
Knuth, D. E. (1998). The Art of Computer Programming, Volume 2: Seminu-
merical Algorithms, 3rd edn. (Addison-Wesley, Reading, Mass).
Kroese, D. P., Taimre, T. and Botev, Z. I. (2011). Handbook of Monte Carlo
Methods, (John Wiley & Sons, New York).
L‚ÄôEcuyer, P. (1990). Random numbers for simulation. Communications of the
ACM 33, 10, pp. 85‚Äì97.
L‚ÄôEcuyer, P. (1996). Combined multiple recursive random number generators.
Operations Research 44, 5, pp. 816‚Äì822.
L‚ÄôEcuyer, P. (1999). Good parameters and implementations for combined multiple
recursive random number generators. Operations Research 47, 1, pp. 159‚Äì
164.
L‚ÄôEcuyer, P., Simard, R., Chen, E. J., Kelton, W. D. (2002). An object-oriented

Bibliography
281
random-number package with many long streams and substreams. Opera-
tions Research 50, 6, pp. 1073‚Äì1074.
Lada, E. K., Steiger, N. M. and Wilson, J. R. (2008). SBatch: A spaced batch
means procedure for steady-state simulation analysis. Journal of Simulation
2, 3, pp. 170-185.
Law, A. M. (2014). Simulation Modeling and Analysis, 5th edn. (McGraw-Hill,
New York).
Law, A. M. and Carson, J. S. (1979). A Sequential Procedure For Determining
the Length of a Steady-State Simulation. Operations Research 27, pp. 1011-
1025.
Lee, L. H., Chew, E. P., Teng, S. and Goldsman, D. (2004). Optimal Computing
Budget Allocation for Multi-Objective Simulation Models, in Proceedings
of the 2004 Winter Simulation Conference, pp. 586‚Äì594.
Lee, L. H., Chew, E. P. and Teng, S. (2006). Integration of Statistical Se-
lection with Search Mechanism for Solving Multi-Objective Simulation-
Optimization Problems, in Proceedings of the 2006 Winter Simulation Con-
ference, pp. 294‚Äì303.
Liu, W., Ah-Kine, P., Bretz, F. and Hayter, A. J. (2012). Exact Simultaneous
ConÔ¨Ådence Intervals for a Finite Set of Contrasts of Three, Four or Five
Generally Correlated Normal Means. Computational Statistics and Data
Analysis 57, pp. 141‚Äì148.
Lophaven, S. N., Nielsen, H. B. and Sondergaard, J. (2002) A Matlab Kriging
Toolbox, Version 2.5, IMM Technical University of Denmark, Lyngby.
Luo, Y-C, Chen, C. H., Y¬®ucesan, E. and Lee, I. (2000). Distributed Web-
Based Simulation Optimization, in Proceedings of the 2000Winter Simu-
lation Conference, pp. 1785-1793.
Mahamunulu, D. M. (1967). Some Fixed-Sample Ranking and Selection Problems.
Ann Math Stat 38, pp. 1079-1091.
Marsaglia, G. and Bray, T. A. (1964). A Convenient Method for Generating
Normal Varialbes. SIAM Review 6, 3, pp. 260‚Äì264.
Matsumoto, M. and Nishimura, T. (1998). Mersenne twister: A 623-dimensionally
equidistributed uniform pseudo-random number generator. ACM Transac-
tions on Modeling and Computer Simulation 8, 1, pp. 3‚Äì30.
Meisner, D., Wu, J. and Wenisch, T. F. (2012). BigHouse: A simulation infras-
tructure for data center systems. International Symposium on Performance
Analysis of Systems and Software (ISPASS).
Morrice, D. J. and Butler, J. C. (2006). Ranking and selection with multiple
‚Äútargets‚Äù, in Proceedings of the 2006 Winter Simulation Conference, pp.
222‚Äì230.
Nakayama, M. K. (1997). Multiple-comparison procedures for steady-state simu-
lations. Annals of Statistics 25, pp. 2433-2450.
Nazzal, D., Mollaghasemi, M., Hedlund, H. and Bozorgi, A. (2012). Using ge-
netic algorithms and an indiÔ¨Äerence-zone ranking and selection procedure
under common random numbers for simulation optimisation. Journal of
Simulation 6, 1, pp. 56-66.
Pawlikowski, K. (1990). Steady-State Simulation of Queuing Processes: A Survey

282
Crafts of Simulation Programming
of Problems and Solutions. ACM Computing Surveys 22, 2, pp. 123-170.
Prokof‚Äôyev, V. N. and Shishkin, A. D. (1974). Successive ClassiÔ¨Åcation of Normal
Sets with Unknown Variances. Radio Engng. Electron. Phys. 19, 2, pp.
141‚Äì143.
Raatikainen, K. E. E. (1990). Sequential procedure for simultaneous estimation
of several percentiles. Transactions of the Society for Computer Simulation
7, 1, pp. 21-44.
Rinott, Y. (1978). On two-stage selection procedures and related probability in-
equalities. Communications in Statistics A7, 8, pp. 799-811.
Rosenblatt, M. (1971). Curve Estimates. Annals of Mathematical Statistics 42,
6, pp. 1815‚Äì1842.
ScheÔ¨Ä¬¥e, H. (1970). Practical solutions of the Behrens-Fisher Problem. Journal of
American Statistician Association 65, pp. 1501‚Äì1508.
Schmeiser, B. W. (1982). Batch-size eÔ¨Äects in the analysis of simulation output.
Operations Research 30, pp. 556-568.
Scott, D. W. and Factor, L. E. (1981). Monte Carlo Study of Three Data-Based
Nonparametric Probability Density Estimators. Journal of the American
Statistical Association 76, 373, pp. 9‚Äì15.
Scott, D. W. and Sain, S. R. (2004). Multi-dimensional Density Estimation in
Handbook of Statistics. Vol 23: Data Mining and Computational Statistics.
eds: CR Rao and EJ Wegman. (Elsevier, Amsterdam).
Scott, D. W., Tapia, R. A. and Thompson, J. R. (1977). Kernel Density Es-
timation Revisited. Journal of Nonlinear Analysis, Theory, Methods and
Applications 1, 4, pp. 339‚Äì372.
Seila, A. F. (1982). A Batching approach to quantile estimation in regenerative
simulations. Management Science 28, 5, pp. 573-81.
Sen, P. K. (1972). On the Bahadur Representation of Sample Quantiles for Se-
quences of œÜ-mixing Random Variables. Journal of Multivariate Analysis
2, 1, pp. 77‚Äì95.
Sheather, S. J. (2004). Density Estimation. Statistical Science 19, 4, 588‚Äì597.
Silverman, B. W. (1986). Density Estimation for Statistics and Data Analysis.
(Chapman and Hall, New York).
Singham D. I. (2014). Selecting Stopping Rules for ConÔ¨Ådence Interval Procedure.
ACM Transactions on Modeling and Computer Simulation 24, 3, Article
No.: 18.
Tafazzoli, A., Steiger, N. M. and Wilson, J. R. (2011). N-Skart: A Nonsequential
Skewness-and Autoregression-Adjusted Batch-Means Procedure for Simu-
lation Analysis. Automatic Control, IEEE Transactions 45, 2, pp. 254‚Äì264.
Tippett, L. H. C. (1925). On the Extreme Individuals and the Range of Samples
Taken from a Normal Population. Biometrika 17, pp. 264‚Äì387.
Tong, Y. L. (1980). Probability Inequalities in Multivarate Distributions. (Aca-
demic Press, New York).
Turner, A. J., Balestrini-Robinson, S. and Mavris, D. (2013). Heuristics for the
Regression of Stochastic Simulations Journal of Simulation 7, pp. 229‚Äì239.
van Beers, W. C. M. and Kleijnen, J. P. C. (2008). Customized sequential designs
for random simulation experiments: Kriging metamodeling and bootstrap-

Bibliography
283
ping. European Journal of Operational Research 186, pp. 1099‚Äì1113.
von Neumann, J. (1941). Distribution of the Ratio of the Mean Square Successive
DiÔ¨Äerence to the Variance. Annals of Mathematical Statistics 12, 4, pp.
367‚Äì395.
Welch, B. L. (1938). The SigniÔ¨Åcance of the DiÔ¨Äerence Between Two Means When
the Population Variances are Unequal. Biometrika 25, pp. 350‚Äì362.
Wilcox, R. R. (1984). A Table for Rinott‚Äôs Selection Procedure. Journal of Quality
Technology 16, pp. 97‚Äì100.
Wilkrmaratna, R. S. (2008). The additive congruential random number generator
- A special case of a multiple recursive generator. Journal of Computational
and Applied Mathematics 216, 2, pp. 371‚Äì387.

This page intentionally left blank
This page intentionally left blank

Index
œÜ-mixing, 30, 100
Appendix, 275
Bandwidth, 147
Rule of thumb, 147
Batch means, 32, 51
Behrens-Fisher problem, 163
Central diÔ¨Äerences, 111
Combined multiple recursive
generator, 2
Common random numbers, 219
Critical constant
h, 166, 174, 275
h1, 75, 185
h3, 78, 83, 182
hL, 192
ha, 89
hb, 90
hc, 94
Density estimation, 141
Divide and conquer, 6
Equivalencc test
Two means, 163
Equivalence test, 77
Forward diÔ¨Äerences, 111
Histogram, 100, 143
IndiÔ¨Äerence amount, 77
IndiÔ¨Äerence zone, 74
Relative, 175
initialization bias, 28
Inverse distribution function, 14
Inverse transform, 14, 82
Kernel function, 145
Kriging, 130
Lagrange interpolation, 106, 146
Least favorable conÔ¨Åguration, 171
Linear congruential generator, 2
Mean square error, 146
Metamodel, 129
Multi-objective, 247
Multiple comparisons
Unknown equal variances, 92
With a control
Known equal variances, 90
Unknown unequal variances,
93
With the best, 190
With the standard, 264
Multiple recursive random number
generator, 3
Natural estimator, 103
285

286
Crafts of Simulation Programming
Nonparameteric, 141
Online algorithm, 49, 58, 128
Optimal Computing Budget
Allocation, 211
Order statistics, 69
Conditional distribution, 73, 82
Correlated, 87, 220
Pdf, 87
First, 81
Joint distribution, 72
Last, 81
Ordinal comparison, 191
Parallel and distributed simulation,
237
Parametric, 141
Pareto optimal, 248
Pareto set, 247
Percentile, 31
Period of RNG, 2
Power of a test, 163
Proportion estimation, 104
Pseudorandom, 2
Quantile, 31, 100
Range, 82
Quantile function, 14
Quasi independent, 38
Random number generator, 2
Range
Cdf, 72
Pdf, 72
Quantile, 82
Sample, 69
Ranking and selection, 171
Ratio statistics, 186
Reliability analysis, 99
Risk analysis, 99
Risk management, 99
Run length, 34
Runs test, 30, 34
Sample range, 78
Seed, 3
Selection with constraints, 261
Shapiro-Wilk test, 53
Simulation run length, 34
Standard deviation, 25
Steady-state simulation, 27
Steady-state variance constant, 31
Stopping rules, 27
Subset selection, 171
Restricted, 182
Systematic sampling, 29
Terminating simulation, 27
Transient state, 28
Type I error, 45, 61, 89, 163
Type II error, 46, 61, 163, 165
Value at risk, 100
Variance
Weighted sample mean, 76
Variance-reduction technqiues, 219
von Neumann test, 30, 33
Warm-up period, 28
Weighted sample mean, 75, 164
Variance, 76
Weight, 75


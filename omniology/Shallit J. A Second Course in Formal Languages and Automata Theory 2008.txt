
This page intentionally left blank

A Second Course in Formal Languages and Automata Theory
Intended for graduate students and advanced undergraduates in computer science, A
Second Course in Formal Languages and Automata Theory treats topics in the theory
of computation not usually covered in a ﬁrst course.
After a review of basic concepts, the book covers combinatorics on words, regular
languages, context-free languages, parsing and recognition, Turing machines, and other
language classes. Many topics often absent from other textbooks, such as repetitions
in words, state complexity, the interchange lemma, 2DPDAs, and the incompressibility
method, are covered here. The author places particular emphasis on the resources needed
to represent certain languages. The book also includes a diverse collection of more than
200 exercises, suggestions for term projects, and research problems that remain open.
jeffrey shallit is professor in the David R. Cheriton School of Computer Science
at the University of Waterloo. He is the author of Algorithmic Number Theory (co-
authored with Eric Bach) and Automatic Sequences: Theory, Applications, Generaliza-
tions (coauthored with Jean-Paul Allouche). He has published approximately 90 articles
on number theory, algebra, automata theory, complexity theory, and the history of math-
ematics and computing.


A Second Course in Formal Languages and
Automata Theory
JEFFREY SHALLIT
University of Waterloo

CAMBRIDGE UNIVERSITY PRESS
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, São Paulo
Cambridge University Press
The Edinburgh Building, Cambridge CB2 8RU, UK
First published in print format
ISBN-13    978-0-521-86572-2
ISBN-13
978-0-511-43700-7
© Jeffrey Shallit 2009
2008
Information on this title: www.cambridge.org/9780521865722
This publication is in copyright. Subject to statutory exception and to the 
provision of relevant collective licensing agreements, no reproduction of any part
may take place without the written permission of Cambridge University Press.
Cambridge University Press has no responsibility for the persistence or accuracy 
of urls for external or third-party internet websites referred to in this publication, 
and does not guarantee that any content on such websites is, or will remain, 
accurate or appropriate.
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
eBook (EBL)
hardback

Contents
Preface
page ix
1
Review of formal languages and automata theory
1
1.1
Sets
1
1.2
Symbols, strings, and languages
1
1.3
Regular expressions and regular languages
3
1.4
Finite automata
4
1.5
Context-free grammars and languages
8
1.6
Turing machines
13
1.7
Unsolvability
17
1.8
Complexity theory
19
1.9
Exercises
21
1.10 Projects
26
1.11 Research problems
26
1.12 Notes on Chapter 1
26
2
Combinatorics on words
28
2.1
Basics
28
2.2
Morphisms
29
2.3
The theorems of Lyndon–Sch¨utzenberger
30
2.4
Conjugates and borders
34
2.5
Repetitions in strings
37
2.6
Applications of the Thue–Morsesequence
and squarefree strings
40
2.7
Exercises
43
2.8
Projects
47
2.9
Research problems
47
2.10 Notes on Chapter 2
47
v

vi
Contents
3
Finite automata and regular languages
49
3.1
Moore and Mealy machines
49
3.2
Quotients
52
3.3
Morphisms and substitutions
54
3.4
Advanced closure properties of regular languages
58
3.5
Transducers
61
3.6
Two-way ﬁnite automata
65
3.7
The transformation automaton
71
3.8
Automata, graphs, and Boolean matrices
73
3.9
The Myhill–Nerodetheorem
77
3.10 Minimization of ﬁnite automata
81
3.11 State complexity
89
3.12 Partial orders and regular languages
92
3.13 Exercises
95
3.14 Projects
105
3.15 Research problems
105
3.16 Notes on Chapter 3
106
4
Context-free grammars and languages
108
4.1
Closure properties
108
4.2
Unary context-free languages
111
4.3
Ogden’s lemma
112
4.4
Applications of Ogden’s lemma
114
4.5
The interchange lemma
118
4.6
Parikh’s theorem
121
4.7
Deterministic context-free languages
126
4.8
Linear languages
130
4.9
Exercises
132
4.10 Projects
138
4.11 Research problems
138
4.12 Notes on Chapter 4
138
5
Parsing and recognition
140
5.1
Recognition and parsing in general grammars
141
5.2
Earley’s method
144
5.3
Top-down parsing
152
5.4
Removing LL(1) conﬂicts
161
5.5
Bottom-up parsing
163
5.6
Exercises
172
5.7
Projects
173
5.8
Notes on Chapter 5
173

Contents
vii
6
Turing machines
174
6.1
Unrestricted grammars
174
6.2
Kolmogorov complexity
177
6.3
The incompressibility method
180
6.4
The busy beaver problem
183
6.5
The Post correspondence problem
186
6.6
Unsolvability and context-free languages
190
6.7
Complexity and regular languages
193
6.8
Exercises
198
6.9
Projects
200
6.10 Research problems
200
6.11 Notes on Chapter 6
200
7
Other language classes
202
7.1
Context-sensitive languages
202
7.2
The Chomsky hierarchy
212
7.3
2DPDAs and Cook’s theorem
213
7.4
Exercises
221
7.5
Projects
222
7.6
Research problems
222
7.7
Notes on Chapter 7
223
Bibliography
225
Index
233


Preface
Goals of this book
This is a textbook for a second course on formal languages and automata theory.
Many undergraduates in computer science take a course entitled “Introduc-
tion to Theory of Computing,”in which they learn the basics of ﬁnite automata,
pushdown automata, context-free grammars, and Turing machines. However,
few students pursue advanced topics in these areas, in part because there is no
really satisfactory textbook.
For almost 20 years I have been teaching such a second course for fourth-
year undergraduate majors and graduate students in computer science at the
University of Waterloo: CS 462/662, entitled “Formal Languages and Parsing.”
For many years we used Hopcroft and Ullman’s Introduction to Automata
Theory, Languages, and Computation as the course text, a book that has proved
very inﬂuential. (The reader will not have to look far to see its inﬂuence on the
present book.)
In 2001, however, Hopcroft and Ullman released a second edition of their
text that, in the words of one professor, “removed all the good parts.”In other
words, their second edition is geared toward second- and third-year students,
and omits nearly all the advanced topics suitable for fourth-year students and
beginning graduate students.
Because the ﬁrst edition of Hopcroft and Ullman’s book is no longer easily
available, and because I have been regularly supplementing their book with my
own handwritten course notes, it occurred to me that it was a good time to write
a textbook on advanced topics in formal languages. The result is this book.
The book contains many topics that are not available in other textbooks. To
name just a few, it addresses the Lyndon–Sch¨utzenberger theorem, Thue’s re-
sults on avoiding squares, state complexity of ﬁnite automata, Parikh’s theorem,
the interchange lemma, Earley’s parsing method, Kolmogorov complexity, and
Cook’s theorem on the simulation of 2DPDAs. Furthermore, some well-known
ix

x
Preface
theorems have new (and hopefully simpler) proofs. Finally, there are almost
200 exercises to test students’knowledge. I hope this book will prove useful to
advanced undergraduates and beginning graduate students who want to dig a
little deeper in the theory of formal languages.
Prerequisites
Iassume the reader is familiarwiththematerialcontainedinatypicalﬁrstcourse
in the theory of computing and algorithm design. Because not all textbooks
use the same terminology and notation, some basic concepts are reviewed in
Chapter 1.
Algorithm descriptions
Algorithms in this book are described in a “pseudocode”notation similar to
Pascal or C, which should be familiar to most readers. I do not provide a formal
deﬁnition of this notation. The readers should note that the scope of loops is
denoted by indentation.
Proof ideas
Although much of this book follows the traditional theorem/proof style, it does
have one nonstandard feature. Many proofs are accompanied by “proofideas,”
which attempt to capture the intuition behind the proofs. In some cases, proof
ideas are all that is provided.
Common errors
I have tried to point out some common errors that students typically make when
encountering this material for the ﬁrst time.
Exercises
There are a wide variety of exercises, from easy to hard, in no particular order.
Most readers will ﬁnd the exercises with one star challenging and exercises
with two stars very challenging.
Projects
Each chapter has a small number of suggested projects that are suitable for term
papers. Students should regard the provided citations to the literature only as
starting points; by tracing forward and backward in the citation history, many
more papers can usually be found.
Research problems
Each chapter has a small number of research problems. Currently, no one knows
how to solve these problems, so if you make any progress, please contact me.

Preface
xi
Acknowledgments
Over the last 18 years, many students in CS 462/662 at the University of Wa-
terloo have contributed to this book by ﬁnding errors and suggesting better
arguments. I am particularly grateful to the following students (in alphabet-
ical order): Margareta Ackerman, Craig Barkhouse, Matthew Borges, Tracy
Damon, Michael DiRamio, Keith Ellul, Sarah Fernandes, Johannes Franz,
Cristian Gaspar, Ryan Golbeck, Mike Glover, Daniil Golod, Ryan Hadley,
Joe Istead, Richard Kalbﬂeisch, Jui-Yi Kao, Adam Keanie, Anita Koo, Dalia
Krieger, David Landry, Jonathan Lee, Abninder Litt, Brendan Lucier, Bob Lutz,
Ian MacDonald, Angela Martin, Glen Martin, Andrew Martinez, Gyanendra
Mehta, Olga Miltchman, Siamak Nazari, Sam Ng, Lam Nguyen, Matthew
Nichols, Alex Palacios, Joel Reardon, Joe Rideout, Kenneth Rose, Jessica
Socha, Douglas Stebila, Wenquan Sun, Aaron Tikuisis, Jim Wallace, Xiang
Wang, Zhi Xu, Colin Young, Ning Zhang, and Bryce Zimny.
Four people have been particularly inﬂuential in the development of this
book, and I single them out for particular thanks: Jonathan Buss, who taught
from this book while I was on sabbatical in 2001 and allowed me to use his
notes on the closure of CSLs under complement; Narad Rampersad, who taught
from this book in 2005, suggested many exercises, and proofread the ﬁnal draft;
Robert Robinson, who taught from a manuscript version of this book at the
University of Georgia and identiﬁed dozens of errors; and Nic Santean, who
carefully read the draft manuscript and made dozens of useful suggestions.
All errors that remain, of course, are my responsibility. If you ﬁnd an error,
please send it to me. The current errata list can be found on my homepage;
http://www.cs.uwaterloo.ca/~shallit/ is the URL.
This book was written at the University of Waterloo, Ontario, and the Uni-
versity of Arizona. This work was supported in part by grants from the Natural
Sciences and Engineering Research Council (Canada).
Waterloo, Ontario; January 2008
Jeffrey Shallit


1
Review of formal languages
and automata theory
In this chapter we review material from a ﬁrst course in the theory of computing.
Much of this material should be familiar to you, but if not, you may want to
read a more leisurely treatment contained in one of the texts suggested in the
notes (Section 1.12).
1.1 Sets
A set is a collection of elements chosen from some domain. If S is a ﬁnite
set, we use the notation |S| to denote the number of elements or cardinality of
the set. The empty set is denoted by ∅. By A ∪B (respectively A ∩B, A −B)
we mean the union of the two sets A and B (respectively intersection and set
difference). The notation A means the complement of the set A with respect
to some assumed universal set U; that is, A = {x ∈U : x ̸∈A}. Finally, 2A
denotes the power set, or set of all subsets, of A.
Some special sets that we talk about include N = {0, 1, 2, 3, . . .}, the natural
numbers, and Z = {. . . , −3, −2, −1, 0, 1, 2, 3, . . .}, the integers.
1.2 Symbols, strings, and languages
One of the fundamental mathematical objects we study in this book is the string.
In the literature, a string is sometimes called a word or sentence. A string is
made up of symbols (or letters). (We treat the notion of symbol as primitive
and do not deﬁne it further.) A nonempty set of symbols is called an alphabet
and is often denoted by ; in this book,  will almost always be ﬁnite. An
alphabet is called unary if it consists of a single symbol. We typically denote
elements of  by using the lowercase italic letters a, b, c, d.
1

2
1
Review of formal languages and automata theory
A string is a ﬁnite or inﬁnite list of symbols chosen from . The symbols
themselves are usually written using the typewriter font. If unspeciﬁed, a
string is assumed to be ﬁnite. We typically use the lowercase italic letters
s, t, u, v, w, x, y, z to represent ﬁnite strings. We denote the empty string by
ϵ. The set of all ﬁnite strings made up of letters chosen from  is denoted by
∗. For example, if  = {a, b}, then ∗= {ϵ, a, b, aa, ab, ba, bb, aaa, . . .}.
Note that ∗does not contain inﬁnite strings. By + for an alphabet , we
understand ∗−{ϵ}, the set of all nonempty strings over .
If w is a ﬁnite string, then its length (the number of symbols it contains)
is denoted by |w|. (There should be no confusion with the same notation used
for set cardinality.) For example, if w = five, then |w| = 4. Note that |ϵ| = 0.
We can also count the number of occurrences of a particular letter in a string.
If a ∈ and w ∈∗, then |w|a denotes the number of occurrences of a in w.
Thus, for example, if w = abbab, then |w|a = 2 and |w|b = 3.
We say a string y is a subword of a string w if there exist strings x, z such
that w = xyz. We say x is a preﬁxof w if there exists y such that w = xy. The
preﬁx is proper if y ̸= ϵ and nontrivial if x ̸= ϵ. For example, if w = antsy,
then the set of preﬁxes of w is {ϵ, a, an, ant, ants, antsy} (see Exercise 4).
The set of proper preﬁxes of w is {ϵ, a, an, ant, ants}, and the set of nontrivial
preﬁxes of w is {a, an, ant, ants, antsy}.
Similarly, we say that z is a sufﬁx of w if there exists y such that w = yz.
The sufﬁx is proper if y ̸= ϵ and nontrivial if z ̸= ϵ.
We say that x is a subsequence of y if we can obtain x by striking out 0 or
more letters from y. For example, gem is a subsequence of enlightenment.
If w = a1a2 · · · an, then for 1 ≤i ≤n, we deﬁne w[i] = ai. If 1 ≤i ≤n
and i −1 ≤j ≤n, we deﬁne w[i..j] = aiai+1 · · · aj. Note that w[i..i] = ai
and w[i..i −1] = ϵ.
If w = ux, we sometimes write x = u−1w and u = wx−1.
Now we turn to sets of strings. A language over  is a (ﬁnite or inﬁnite) set
of strings—in other words, a subset of ∗.
Example 1.2.1. The following are examples of languages:
PRIMES2 = {10, 11, 101, 111, 1011, 1101, 10001, . . .} (the primes represen-
ted in base 2)
EQ = {x ∈{0, 1}∗: |x|0 = |x|1} (strings containing an equal number
of each symbol)
= {ϵ, 01, 10, 0011, 0101, 0110, 1001, 1010, 1100, . . .}
EVEN = {x ∈{0, 1}∗: |x|0 ≡0 (mod 2)} (strings with an even number of 0s)
SQ = {xx : x ∈{0, 1}∗} (the language of squares)

1.3 Regular expressions and regular languages
3
Given a language L ⊆∗, we may consider its preﬁx and sufﬁx languages.
We deﬁne
Pref(L) = {x ∈∗: there exists y ∈L such that x is a preﬁx of y};
Suff(L) = {x ∈∗: there exists y ∈L such that x is a sufﬁx of y}.
One of the fundamental operations on strings is concatenation. We concate-
nate two ﬁnite strings w and x by juxtaposing their symbols, and we denote
this by wx. For example, if w = book and x = case, then wx = bookcase.
Concatenation of strings is, in general, not commutative; for example, we have
xw = casebook. However, concatenation is associative: we have w(xy) =
(wx)y for all strings w, x, y.
In general, concatenation is treated notationally like multiplication, so that,
for example, wn denotes the string www · · · w (n times).
If w = a1a2 · · · an and x = b1b2 · · · bn are ﬁnite words of the same length,
then by wXx we mean the word a1b1a2b2 · · · anbn, the perfect shufﬂe of w and
x. For example, shoe X cold = schooled, and clip X aloe = calliope,
and (appropriately for this book) term X hoes = theorems.
If w = a1a2 · · · an is a ﬁnite word, then by wR we mean the reversal of the
word w; that is, wR = anan−1 · · · a2a1. For example, (drawer)R = reward.
Note that (wx)R = xRwR. A word w is a palindrome if w = wR. Examples
of palindromes in English include radar, deified, rotator, repaper, and
redivider.
We now turn to orders on strings. Given a ﬁnite alphabet , we can impose an
order on the elements. For example, if  = k = {0, 1, 2, . . . , k −1}, for some
integer k ≥2, then 0 < 1 < 2 < · · · < k −1. Suppose w, x are equal-length
strings over . We say that w is lexicographically smaller than x, and write w <
x, if there exist strings z, w′, x′ and letters a, b such that w = zaw′, x = zbx′,
and a < b. Thus, for example, trust < truth. We can extend this order to the
radix order deﬁned as follows: w < x if |w| < |x|, or |w| = |x| and w precedes
x in lexicographic order. Thus, for example, rat < moose in radix order.
1.3 Regular expressions and regular languages
As we have seen earlier, a language over  is a subset of ∗. Languages may be
of ﬁnite or inﬁnite cardinality. We start by deﬁning some common operations
on languages.
Let L, L1, L2 ⊆∗be languages. We deﬁne the product or concatenation
of languages by
L1L2 = {wx : w ∈L1, x ∈L2}.

4
1
Review of formal languages and automata theory
Common Error 1.3.1. Note that the deﬁnition of language concatenation
implies that L∅= ∅L = ∅. Many students mistakenly believe that L∅= L.
We deﬁne L0 = {ϵ} and deﬁne Li as LLi−1 for i ≥1. We deﬁne
L≤i = L0 ∪L1 ∪· · · ∪Li.
We deﬁne L∗as 
i≥0 Li; the operation L∗is sometimes called Kleene
closure. We deﬁne L+ = L L∗; the operation + in the superscript is sometimes
called positive closure. If L is a language, then the reversed language is deﬁned
as follows: LR = {xR : x ∈L}.
We now turn to a common notation for representing some kinds of languages.
A regular expression over the base alphabet  is a well-formed string over
the larger alphabet  ∪A, where A = {ϵ, ∅, (,
),
+,
*}; we assume
 ∩A = ∅. In evaluating such an expression, * represents Kleene closure and
has highest precedence. Concatenation is represented by juxtaposition, and has
next highest precedence. Finally, + represents union and has lowest precedence.
Parentheses are used for grouping. A formal deﬁnition of regular expressions
is given in Exercise 33.
If the word u is a regular expression, then L(u) represents the language
that u is shorthand for. For example, consider the regular expression u =
(0 + 10)*(1 + ϵ). Then L(u) represents all ﬁnite words of 0s and 1s that do not
contain two consecutive 1s. Frequently we will abuse the notation by referring
to the language as the naked regular expression without the surrounding L(
).
A language L is said to be regular if L = L(u) for some regular expres-
sion u.
1.4 Finite automata
A deterministic ﬁnite automaton, or DFA for short, is the simplest model of a
computer. We imagine a ﬁnite control equipped with a read head and a tape,
divided into cells, which holds a ﬁnite input. At each step, depending on the
machine’s internal state and the current symbol being scanned, the machine
can change its internal state and move right to the next square on the tape.
If, after scanning all the cells of the input the machine is in any one of a
number of ﬁnalstates, we say the input is accepted; otherwise it is rejected (see
Figure 1.1).
Formally, a DFA is a 5-tuple (Q, , δ, q0, F), where
• Q is a ﬁnite nonempty set of states;
•  is a ﬁnite nonempty input alphabet;
• δ : Q ×  →Q is a transition function;

1.4 Finite automata
5
Finite
control
i
n
p
u
t
Figure 1.1: A deterministic ﬁnite automaton
• q0 ∈Q is the start or initial state;
• F ⊆Q is the set of ﬁnal states.
The transition function δ can be extended to a transition function δ∗: Q ×
∗→Q as follows:
• δ∗(q, ϵ) = q for all q ∈Q;
• δ∗(q, xa) = δ(δ∗(q, x), a) for all q ∈Q, x ∈∗, and a ∈.
Since δ∗agrees with δ on the domain of δ, we often just write δ for δ∗.
Now we give the formal deﬁnition of L(M), the language accepted by a
DFA M. We have
L(M) = {x ∈∗: δ(q0, x) ∈F}.
We often describe deterministic ﬁnite automata by providing a transition
diagram. This is a directed graph where states are represented by circles, ﬁnal
states represented by double circles, the initial state is labeled by a headless
arrow entering a state, and transitions represented by directed arrows, labeled
with a letter. For example, the transition diagram in Figure 1.2 represents the
DFA that accepts the language EVEN = {x ∈{0, 1}∗: |x|0 ≡0 (mod 2)}.
Representation as a transition diagram suggests the following natural gener-
alization of a DFA: we allow the automaton to have multiple choices (or none
at all) on what state to enter on reading a given symbol. We accept an input if
and only if some sequence of choices leads to a ﬁnal state. For example, the
0
0
1
1
q
1
0
q
Figure 1.2: Transition diagram for a DFA

6
1
Review of formal languages and automata theory
0, 1
1
0, 1
0, 1
0, 1
q
q
q
q
q
1
2
3
4
0
Figure 1.3: Transition diagram for an NFA
transition diagram in Figure 1.3 represents a nondeterministic ﬁnite automaton
(NFA) that accepts the language L4, where
Ln := {x ∈{0, 1}∗: the nth symbol from the right is 1}.
It is possible to show that the smallest DFA accepting Ln has at least 2n states
(see Exercise 3.14), so NFAs, while accepting the same class of languages as
DFAs, can be exponentially more concise.
Formally, an NFA is a 5-tuple M = (Q, , δ, q0, F), where δ : Q ×  →
2Q. We deﬁne the extended transition function δ∗by
• δ∗(q, ϵ) = {q};
• δ∗(q, xa) = 
r∈δ∗(q,x) δ(r, a).
The language accepted by an NFA, L(M), is then given by
L(M) = {x ∈∗: δ∗(q0, x) ∩F ̸= ∅}.
The following theorem shows that NFAs accept exactly the regular lan-
guages.
Theorem 1.4.1. If M is an NFA, then there exists a DFA M′ such that L(M) =
L(M′).
Proof Idea.
We let the states of M′ be all subsets of the state set of M. The
ﬁnal states of M′ are those subsets containing at least one ﬁnal state of M.
Exercise 31 asks you to show that the subset construction for NFA-to-DFA
conversion can be carried out in O(kn2n) time, where k = || and n = |Q|.
Yet another generalization of DFA is to allow the DFA to change state spon-
taneously without consuming a symbol of the input. This can be represented in a
transition diagram by allowing arrows labeled ϵ, which are called ϵ-transitions.
An NFA-ϵ is a 5-tuple M = (Q, , δ, q0, F), where δ : Q × ( ∪{ϵ}) →2Q.
The most important theorem on regular languages is Kleene’s theorem:
Theorem 1.4.2. The following language classes are identical:
(a) the class of languages speciﬁedby regular expressions;
(b) the class of languages accepted by DFAs;

1.4 Finite automata
7
(c) the class of languages accepted by NFAs;
(d) the class of languages accepted by NFA-ϵ’s.
As a corollary, we can deduce some important closure properties of regular
languages. We say a class of languages is closed under an operation if whenever
the arguments to the operation are in the class, the result is also. If there are
any counterexamples at all, we say the class is not closed.
Corollary 1.4.3. The class of regular languages is closed under the operations
of union, concatenation, Kleene ∗, and complement.
The pumping lemma is an important tool for proving that certain languages
are not regular.
Lemma 1.4.4. Suppose L is a regular language. Then there exists a constant
n ≥1, depending on L, such that for all z ∈L with |z| ≥n, there exists a
decomposition z = uvw with |uv| ≤n and |v| ≥1 such that uviw ∈L for all
i ≥0. In fact, we may take n to be the number of states in any DFA accept-
ing L.
Proof Idea. The basic idea of the proof is that the path through the transition
diagram for any sufﬁciently long accepted word must contain a loop. We may
then go around this loop any number of times to obtain an inﬁnite number of
accepted words of the form uviw.
Example 1.4.5. Let us show that the language
PRIMES1 = {a2, a3, a5, . . .},
the prime numbers represented in unary, is not regular. Let n be the pumping
lemma constant, and choose a prime p > n; we know such a prime exists
by Euclid’s theorem that there are inﬁnitely many primes. Let z = ap. Then
there exists a decomposition z = uvw with |uv| ≤n and |v| ≥1 such that
uviw ∈PRIMES1 for all i ≥0. Suppose |v| = r. Then choose i = p + 1. We
have |uviw| = p + (i −1)r = p(r + 1). Since r ≥1, this number is not a
prime, a contradiction.
Example 1.4.6. Here is a deeper application of the pumping lemma. Let us
show that the language
PRIMES2 = {10, 11, 101, 111, 1011, 1101, 10001, . . .},
the prime numbers represented in binary, is not regular. Let n be the pumping
lemma constant and p be a prime p > 2n. Let z be the base-2 representation

8
1
Review of formal languages and automata theory
of p. If t is a string of 0s and 1s, let [t]2 denote the integer whose base-2
representation is given by t. Write z = uvw. Now
[z]2 = [u]22|vw| + [v]22|w| + [w]2,
while
[uviw]2 = [u]22i|v|+|w| + [v]2(2|w| + 2|vw| + · · · + 2|vi−1w|) + [w]2.
Now 2|w| + 2|vw| + · · · + 2|vi−1w| is, by the sum of a geometric series, equal
to 2|w| 2i|v|−1
2|v|−1 . Now by Fermat’s theorem, 2p ≡2 (mod p) if p is a prime.
Hence, setting i = p, we get [uvpw]2 −[uvw]2 ≡0 (mod p). But since z has
no leading zeroes, [uvpw]2 > [uvw]2 = p. (Note that 2|v| −1 ̸≡0 (mod p)
since |v| ≥1 and |uv| ≤n ⇒2|v| ≤2n < p.) It follows that [uvpw]2 is an
integer larger than p that is divisible by p, and so cannot represent a prime
number. Hence, uvpw ̸∈PRIMES2. This contradiction proves that PRIMES2 is
not regular.
1.5 Context-free grammars and languages
In the previous section, we saw two of the three important ways to specify
languages: namely, as the language accepted by a machine or the language
speciﬁed by a regular expression. In this section, we explore a third important
way, the grammar. A machine receives a string as input and processes it, but a
grammar actually constructs a string iteratively through a number of rewriting
rules. We focus here on a particular kind of grammar, the context-free grammar
(CFG).
Example 1.5.1. Consider the CFG given by the following production rules:
S →a
S →b
S →aSa
S →bSb.
The intention is to interpret each of these four rules as rewriting rules. We
start with the symbol S and can choose to replace it by any of a, b, aSa, bSb.
Suppose we replace S by aSa. Now the resulting string still has an S in it, and
so we can choose any one of four strings to replace it. If we choose the rule
S →bSb, we get abSba. Now if we choose the rule S →b, we get the string
abbba, and no more rules can be performed.

1.5 Context-free grammars and languages
9
It is not hard to see that the language generated by this process is the set of
palindromes over {a, b} of odd length, which we call ODDPAL.
Example 1.5.2. Here is a somewhat harder example. Let us create a CFG to
generate the nonpalindromes over {a, b}.
S →aSa | bSb | aT b | bT a
T →aT a | aT b | bT a | bT b | ϵ | a | b.
The basic idea is that if a string is a nonpalindrome, then there must be at
least one position such that the character in that position does not match the
character in the corresponding position from the end. The productions S →aSa
and S →bSb are used to generate a preﬁx and sufﬁx that match properly, but
eventually one of the two productions involving T on the right-hand side must
be used, at which point a mismatch is introduced. Now the remaining symbols
can either match or not match, which accounts for the remaining productions
involving T .
Example 1.5.3. Finally, we conclude with a genuinely challenging example.
Consider the language
L = {x ∈{0, 1}∗: x is not of the form ww} = SQ
= {0, 1, 01, 10, 000, 001, 010, 011, 100, 101, 110, 111, 0001, 0010,
0011, 1000, . . .}.
Exercise 25 asks you to prove that this language can be generated by the
following grammar:
S →AB | BA | A | B
A →0A0 | 0A1 | 1A0 | 1A1 | 0
B →0B0 | 0B1 | 1B0 | 1B1 | 1.
Formally, we deﬁne a CFG G to be a 4-tuple G = (V, , P, S), where V
is a nonempty ﬁnite set of variables,  is a nonempty ﬁnite set of terminal
symbols, P is a ﬁnite set of productions of the form A →α, where A ∈V and
α ∈(V ∪)∗(i.e., a ﬁnite subset of V × (V ∪)∗), and S is a distinguished
element of V called the start symbol. We require that V ∩ = ∅. The term
context-free comes from the fact that A may be replaced by α, independent of
the context in which A appears.
A sentential form is any string of variables and terminals. We can go from
one sentential form to another by applying a rule of the grammar. Formally,
we write αBγ =⇒αβγ if B →β is a production of P. We write
∗
=⇒for the

10
1
Review of formal languages and automata theory
reﬂexive, transitive closure of =⇒. In other words, we write α
∗
=⇒β if there
exist sentential forms α = α0, α1, . . . , αn = β such that
α0 =⇒α1 =⇒α2 =⇒· · · =⇒αn.
A derivation consists of 0 or more applications of =⇒to some sentential form.
If G is a CFG, then we deﬁne
L(G) = {x ∈∗: S
∗
=⇒x}.
A leftmost derivation is a derivation in which the variable replaced at each
step is the leftmost one. A rightmost derivation is deﬁned analogously. A
grammar G is said to be unambiguous if every word w ∈L(G) has exactly one
leftmost derivation and ambiguous otherwise.
A parse tree or derivation tree for w ∈L(G) is an ordered tree T where
each vertex is labeled with an element of V ∪ ∪{ϵ}. The root is labeled
with a variable A and the leaves are labeled with elements of  or ϵ. If a node
is labeled with A ∈V and its children are (from left to right) X1, X2, . . . , Xr,
then A →X1X2 · · · Xr is a production of G. The yield of the tree is w and
consists of the concatenation of the leaf labels from left to right.
Theorem 1.5.4. A grammar is unambiguous if and only if every word generated
has exactly one parse tree.
The class of languages generated by CFGs is called the context-free lan-
guages (CFLs).
We now recall some basic facts about CFGs. First, productions of the form
A →ϵ are called ϵ-productions and productions of the form A →B unit
productions. There is an algorithm to transform a CFG G into a new grammar
G′ without ϵ-productions or unit productions, such that L(G′) = L(G) −{ϵ}
(see Exercise 27). Furthermore, it is possible to carry out this transformation
in such a way that if G is unambiguous, G′ is also.
We say a grammar is in Chomsky normal form if every production is of
the form A →BC or A →a, where A, B, C are variables and a is a single
terminal. There is an algorithm to transform a grammar G into a new grammar
G′ in Chomsky normal form, such that L(G′) = L(G) −{ϵ}; (see Exercise 28).
We now recall a basic result about CFLs, known as the pumping lemma.
Theorem 1.5.5. If L is context-free, then there exists a constant n such that for
all z ∈L with |z| ≥n, there exists a decomposition z = uvwxy with |vwx| ≤n
and |vx| ≥1 such that for all i ≥0, we have uviwxiy ∈L.
Proof Idea.
If L is context-free, then we can ﬁnd a Chomsky normal form
grammar G generating L −{ϵ}. Let n = 2k, where k is the number of variables

1.5 Context-free grammars and languages
11
Finite
control
i
n
p
u
t
C
K
S
T
A
Figure 1.4: A pushdown automaton
in G. If z ∈L and |z| ≥n, then the parse tree for z must contain a relatively
long path—long enough, in fact, that some variable A gets repeated. We then
have derivations of the form
S
∗
=⇒uAy
A
∗
=⇒vAx
A
∗
=⇒w
for some strings u, v, w, x, y. Thus, uviwxiy ∈L for all i ≥0. (The length
conditions on |vwx| and |vx| come from a more precise analysis of the
path.)
We now turn to a machine model for the CFLs. Consider augmenting an
NFA-ϵ with the ability to store symbols on a stack or pushdown store (see
Figure 1.4). (Recall that in a stack we are only able to push elements on top of
the stack and pop an element from the top of the stack.)
Formally,
a
pushdown
automaton
(or
PDA)
is
a
7-tuple
M =
(Q, , , δ, q0, Z0, F). Here Q, , q0, and F are deﬁned as earlier, and 
is a ﬁnite alphabet of symbols that may appear in the stack, Z0 is the symbol
representing the initial stack contents, and δ is the transition function. The
function δ maps Q × ( ∪{ϵ}) ×  to ﬁnite subsets of Q × ∗. The meaning
of
δ(q, a, X) = {(p1, γ1), . . . , (pr, γr)}
is that the machine M in state q with X on top of the stack may on input a
consume that symbol from the input and nondeterministically choose an i such
that it changes state to pi, pops X from the stack, and pushes the symbols of γi
on top of the stack in its place.

12
1
Review of formal languages and automata theory
We now deﬁne the notion of conﬁguration, which is intended to be a com-
plete description of the current state of the machine. A conﬁguration is an
element of Q × ∗× ∗. A triple of the form (q, w, α) means that the ma-
chine is currently in state q, with w the input not yet read (and the tape head
currently scanning the ﬁrst symbol of w) and α the stack contents. We write
the contents of the stack with the top at the left.
Moves of the machine take us from one conﬁguration to the next. We write
(q, aw, Xα) ⊢(p, w, βα)
for q ∈Q, a ∈ ∪{ϵ}, w ∈∗, X ∈, α, β ∈∗if there exists a transition
of the form (p, β) ∈δ(q, a, X). We write
∗⊢for the reﬂexive, transitive closure
of ⊢.
We are now ready to deﬁne acceptance by ﬁnal state in a PDA. We have
L(M) = {x ∈∗: (q0, x, Z0)
∗⊢(q, ϵ, α) for some q ∈F and α ∈∗}.
Note that this deﬁnition requires that in order for a string to be accepted, all
of its symbols must actually be processed by the PDA.
There is another possible deﬁnition of acceptance in a PDA, namely, accep-
tance by empty stack. We have Le(M) = {x ∈∗: (q0, x, Z0)
∗⊢(q, ϵ, ϵ) for
some q ∈Q}.
Theorem 1.5.6. The two conventions of acceptance (ﬁnal-state, empty stack)
are equivalent in the sense that for all PDAs M, there exists a PDA M′ such
that L(M) = Le(M′) and vice versa.
We now come to the most important theorem of this section.
Theorem 1.5.7. For all CFGs G, there exists a PDA M such that L(G) =
Le(M). For all PDAs M, there exists a CFG G such that Le(M) = L(G).
Proof Idea.
Let G = (V, , P, S) be a CFG. We create a one-state PDA
M = ({q}, , V
∪, δ, q, S, ∅), which accepts L(G) by empty stack, as
follows: for each A ∈V , we deﬁne
δ(q, ϵ, A) = {(q, α) : A →α is a production}.,
and for each a ∈, we deﬁne
δ(q, a, a) = {(q, ϵ)}.
An easy induction now proves that M accepts exactly L(G).

1.6 Turing machines
13
For the other direction, assume M = (Q, , , δ, q0, Z0, ∅). Create a gram-
mar G = (V, , P, S), where
V = {S} ∪{[q, A, p] : p, q ∈Q, A ∈},
and the productions P are given by
S →[q0, Z0, q] for each q ∈Q
[q, A, qm+1] →a[q1, B1, q2][q2, B2, q3] · · · [qm, Bm, qm+1] for each q,
q1, . . . , qm, qm+1 ∈Q, each a ∈ ∪{ϵ}, each A, B1, . . . ,
Bm ∈ such that (q1, B1B2 · · · Bm) ∈δ(q, a, A).
A nontrivial argument now proves that Le(M) = L(G).
Theorem 1.5.7 is useful for proving some theorems where CFGs are not a
useful characterization of the CFLs. For example:
Theorem 1.5.8. If L is a CFL and R is regular, then L ∩R is a CFL.
Proof Idea. If L is a CFL, then L = L(M1) for some PDA M1 = (Q1, , , δ1,
q1, Z1, F1). If R is regular then R = L(M2) for some DFA M2 = (Q2, ,
δ2, q2, F2). We create a PDA M = (Q1 × Q2, , , δ, q0, Z1, F1 × F2) ac-
cepting L ∩R. The idea is that M simulates M1 in the ﬁrst component of its
state and M2 in the second component. We deﬁne q0 = [q1, q2] and
δ([p, q], a, A) = {([p′, q′], γ ) : (p′, γ ) ∈δ1(p, a, A) and δ2(q, a) = q′}.
To complete the proof, prove by induction on |x| that ([q1, q2], x, Z1)
∗⊢([p, q], ϵ, α) in M if and only if (q1, x, Z1)
∗⊢(p, ϵ, α) in M1 and δ(q2, x) =
q in M2.
Corollary 1.5.9. If R ⊆∗is a regular language, then R is a CFL.
Proof. In Theorem 1.5.8 take L = ∗. Then R = L ∩R is a CFL.
1.6 Turing machines
In previous sections we have reviewed machine models such as the ﬁnite
automaton and pushdown automaton. We now turn to a more powerful model
of computation, the Turing machine.
A Turing machine (or TM for short) is a computing device equipped with
an unbounded tape divided into individual cells. For purposes of reference,

14
1
Review of formal languages and automata theory
Finite
control
B
u
t
n
p
i
B
B
Figure 1.5: A Turing machine
we number the cells 0, 1, 2, . . ., but the TM itself has no access to the cell
numbers. The TM has a ﬁnite control, and based on its current state and
the current symbol being scanned, the TM can change state, rewrite the
symbol, and move either left or right. The input initially appears in cells
1, 2, . . . , n. Cell 0 and cells n + 1, n + 2, . . . initially hold a distinguished
character called the blank symbol, which we will write as B. This character, as
well as all others, can be rewritten during the course of the computation (see Fig-
ure 1.5).
Formally, a TM is a 6-tuple (Q, , , δ, q0, h), where Q is a ﬁnite set of
states,  ⊆ is the input alphabet,  is the tape alphabet, q0 is the initial
state, and h ∈Q is a special distinguished state called the halting state. By
convention we have B ̸∈, but B ∈. The transition function δ is a partial
function from Q ×  to Q ×  × {L, R, S}. By partial function, we mean that
it may not be deﬁned for some pairs (q, α) in its domain. By convention, a TM
has no transitions leaving its halting state.
In a single move, the TM examines the current cell, and based on the contents
and its current state, it rewrites the current cell, changes state, and moves either
left (L), right (R), or stays stationary (S).
Informally, a TM M accepts its input if, when M starts with x as its input,
scanning cell 0, it eventually enters the halting state h. Note we do not require
that M actually read all its input.
In order to deﬁne acceptance formally, we need to deﬁne the notion of
conﬁguration. A conﬁguration of a TM is a string of the form wqx, where
w, x ∈∗and q ∈Q. The meaning of wqx is that the M is in state q, the
current contents of the tape is wx, and q is scanning the ﬁrst symbol of x.
Since the tape is unbounded, some clariﬁcation is needed about the string wx
representing the tape contents. Our convention is that all characters to the right
of the rightmost character of x must be B. This means that our deﬁnition of
conﬁguration is not unique, but is unique up to trailing blank symbols.
Transitions of the TM correspond to moving from one conﬁguration to
another:

1.6 Turing machines
15
(a) If δ(p, X) = (q, Y, L), then αZpXβ ⊢αqZYβ.
(b) If δ(p, X) = (q, Y, R), then αpXβ ⊢αYqβ.
(c) If δ(p, X) = (q, Y, S), then αpXβ ⊢αqYβ.
We use
∗⊢for the reﬂexive, transitive closure of ⊢, and we deﬁne L(M) =
{x ∈∗: q0Bx
∗⊢αhβ for some α, β ∈∗}.
Starting with a given input, a TM may eventually either
(a) enter a conﬁguration that has no further move (i.e., for which δ is unde-
ﬁned);
(b) attempt to move left off the edge of its tape;
(c) enter an inﬁnite loop; or
(d) enter the halting state h.
In cases (a)–(c)we say that M does not accept its input. In the last case we
say that M accepts its input. In cases (a)–(b)we say that M crashes.
If L = L(M) for some TM M, then we say L is recursively enumerable
(often abbreviated r.e.). (The origin of this somewhat obscure term appears in
Exercise 32.) If L = L(M) for some TM M that has the property that M never
enters an inﬁnite loop on any input, then we say L is recursive.
There are many variations on TMs, such as allowing extra tracks on a
single tape, or allowing multiple tapes, or allowing tapes to be unbounded
to both the right and the left, or allowing two-dimensional tapes, or allowing
nondeterminism. All of these variations can be shown to be equivalent in
computing power to the vanilla TM model (see Exercise 29).
There is a special TM, MU, called the universal TM. This TM has the
property that it takes an input consisting of an encoded version of some TM T
and an encoded version of an input x and simulates T on x, accepting if and
only if T accepts x.
Which encoding should be used? To some extent it is not important, as long
as all machines use the same convention. One possible encoding is as follows:
we ﬁx an inﬁnite universal alphabet U = {a1, a2, . . .} and assume that all
inputs and tape symbols are drawn from U. Similarly, we ﬁx an inﬁnite
universal set of states QU = {q0, q1, . . .} and assume that all TMs use state
names chosen from QU. We then encode an element ai ∈U by the string
e(ai) = 0i+1 and encode the blank symbol B by the string 0. We encode an
alphabet  = {b1, b2, . . . , br} by the string
e() = 111e(b1)1e(b2)1 · · · 1e(br)111.
We encode an element qi ∈QU by the string e(qi) = 0i+1. We encode the
directions of moves of a TM by e(L) = 0, e(R) = 00, and e(S) = 000. To

16
1
Review of formal languages and automata theory
encode a move m of a TM, say δ(q, a) = (p, b, D), we write
e(m) = 11e(q)1e(a)1e(p)1e(b)1e(D)11.
Finally, to encode an entire TM M with moves m1, m2, . . . , mt, we deﬁne
e(M) = 11111e(q0)1e(h)1e()1e()1e(m1)1 · · · 1e(mt)11111.
Theorem 1.6.1. There exists a universal TM MU that has the following behav-
ior: on input e(T )e(x), MU simulates T on input x and halts if and only if T
halts on x.
Proof Idea.
The TM MU uses three tapes. The ﬁrst tape is used to hold the
input e(T )e(x). The second tape holds an encoded version of T ’s tape and
the third tape holds the encoded state that T is in. A step of MU consists of
determining if any moves on tape 1 match the current conﬁguration and then
performing the move on tape 2. If the simulated machine enters the halt state,
so does MU. A move may require replacing an encoded version of one symbol
with another. Since these encodings could be of different lengths, some shifting
of tape 2 may be required. Finally, the new state is written on tape 3.
We now turn to two classes of languages that are based on TMs. The
following theorem gives an alternative characterization of the class of recursive
languages.
Theorem 1.6.2. A language L is recursive if and only if there exists a TM M
that on input x halts with either 1 or 0 written in cell 1 on its tape (and blank
symbols on the rest of the tape) such that 1 is written if x ∈L and 0 is written
if x ̸∈L.
Such a TM is sometimes said to decide L.
The following theorems are easy exercises.
Theorem 1.6.3. The class of recursive languages is closed under the operations
of union, intersection, complement, Kleene ∗, and concatenation.
Theorem 1.6.4. The class of r.e. languages is closed under the operations of
union, intersection, Kleene ∗, and concatenation.
We can view the TM in Theorem 1.6.2 as computing a function—in that
case, a function from ∗to {0, 1}. We can generalize this to allow TMs to
compute a function from ∗to ∗.

1.7 Unsolvability
17
1.7 Unsolvability
A decision problem is a problem with at least one parameter that takes inﬁnitely
many values, and for which the answer is always “yes”or “no.”
We can associate a language with a decision problem as follows: we take the
set of all encodings of instances of the decision problem for which the answer
is “yes.”Of course, this raises the question of what encoding to use, but often
there is a “natural”encoding that suggests itself.
Example 1.7.1. Consider the following decision problem: given an integer n,
decide whether or not n is a prime number. The input n can take inﬁnitely many
values (all n ≥2, for example), and the answer is “yes”(the number is prime)
or “no”(the number is not).
A natural encoding of an integer n is representation in base 2. The language
associated with the previous decision problem is therefore
PRIMES2 = {10, 11, 101, 111, 1011, 1101, 10001, . . .}.
We say a decision problem is solvable if its associated language L is
recursive—in other words, if there is a TM M that decides L. Note that a
solvable decision problem corresponds to what we ordinarily think of as solv-
able by mechanical means: there exists a ﬁnite deterministic procedure that,
given an instance of the problem, will halt in a ﬁnite amount of time and answer
either “yes”or “no.”
Turing’s fundamental paper of 1936 proved that there exist unsolvable (or
“uncomputable”)decision problems. The next theorem concerns what is prob-
ably the most famous one.
Theorem 1.7.2. The decision problem “Given a Turing machine T and an
input w, does T halt on w?”is unsolvable.
Proof. Let us assume that this problem, called the halting problem, is solvable.
This means that there exists a TM, call it MH, that takes an input of the form
e(T )e(w) and eventually halts, writing 1 on its output tape if T halts on w and
0 otherwise. This is illustrated by Figure 1.6.
e(T)e(w)
MH
1
0
if T halts on input w
if T does not halt on input w
Figure 1.6: Hypothetical TM MH

18
1
Review of formal languages and automata theory
e(T)e(w)
M
if T halts on input w
if T does not halt on input w
L
loops forever
halts
1
0
MH
Figure 1.7: Constructing TM ML from MH
Now let us create a new TM ML as follows: it simulates MH and then
examines the tape after MH halts. If the tape contains a 1, then ML enters an
inﬁnite loop (e.g., by moving right on every tape symbol). If the tape contains
a 0, then ML halts. This is illustrated by Figure 1.7.
Finally, we make a new TM MD as follows: on input e(T ), MD computes
the encoding of e(T ), that is, e(e(T )), and writes it on the tape after e(T ). Then
it calls ML. This is illustrated in Figure 1.8.
We are now ready to obtain a contradiction. Feed MD with e(MD) as input.
The result is that MD halts on input e(MD) if and only if it does not halt. This
contradiction proves that our original assumption, the existence of MH, must
not hold.
Another way to state Turing’s theorem is the following.
Corollary 1.7.3. The halting language LH = {e(T )e(w) : T halts on w} is
recursively enumerable but not recursive.
We can obtain additional unsolvable problems by using reductions. We say
a problem P1 Turing-reduces to a problem P2, and we write P1 ≤P2, if, given
a TM T2 that solves P2, we could use T2 as a subroutine in a TM T1 that solves
P1. Similarly, we say a language L1 Turing-reduces to a language L2, and we
write L1 ≤T L2, if, given a TM T2 deciding L2, we could use T2 as a subroutine
in another TM deciding L1.
e(T)
e(T)e(e(T))
ML
loops forever
if T halts on input e(T)
if T does not halt on input e(T)
MD
loops
forever
halts
halts
Figure 1.8: Constructing TM MD from ML

1.8 Complexity theory
19
Theorem 1.7.4
(a) If P1 is unsolvable, and P1 ≤P2, then P2 is unsolvable.
(b) If L1 is not recursive, and L1 ≤T L2, then L2 is not recursive.
Proof. We prove only (a), leaving (b) to the reader.
Suppose P2 were solvable, say by a TM T2. Since P1 ≤T P2, we could use
T2 as a subroutine in a TM T1 to solve P1, a contradiction.
Example 1.7.5. Let us use reductions to show that the decision problem
Pnonempty : Given a TM T, is L(T ) ̸= ∅?
is unsolvable. It sufﬁces to show that the halting problem reduces to Pnonempty.
Suppose there were a TM M solving Pnonempty; M takes an encoding of a TM
e(T ) as input and eventually writes 1 on its output tape if T accepts some string,
and writes 0 if T accepts no string. Then we could use M to solve the halting
problem as follows: on input e(T ) and w, create the encoding e(T ′) of a TM T ′
that ignores its input, writes w out on its tape, and then simulates T on w. If T
accepts w, then T ′ halts; otherwise it crashes. Thus, L(T ′) = ∗if T accepts
w and L(T ′) = ∅otherwise.
To solve the halting problem, we now run M on e(T ′) and answer whatever
M answers.
1.8 Complexity theory
In the previous section, we exhibited some problems (such as the halting
problem) that are, in general, unsolvable by a TM no matter how much time and
space are allocated to the solution. We might instead consider what problems
are solvable using only a reasonable amount of time and space. By putting
restrictions on the amount of time and space used by a TM, we obtain various
complexity classes of languages.
Many of the most important complexity classes deal with time. We say that
a TM M is of time complexity T (n) if, whenever M is started with an input w
of length n on its tape, it eventually halts after making at most T (n) moves.
The complexity class P is deﬁned to be the set of all languages that are
accepted by a deterministic TM of time complexity T (n), where T is a polyno-
mial. This complexity class includes many of the languages discussed in this
book, such as PRIMES1, EQ, EVEN, SQ, and ODDPAL. Roughly speaking, the
class P represents those languages in which membership is feasibly solvable,
that is, solvable in a reasonable length of time, in terms of the size of the input.

20
1
Review of formal languages and automata theory
Similarly, the complexity class NP is deﬁned to be the set of all languages
that are accepted by a nondeterministic TM of time complexity T (n), where T
is a polynomial. Of course, a nondeterministic TM may have many computa-
tional paths, and these paths could be of different lengths. Here, then, the time
complexity of a nondeterministic TM on a given input is taken to be the length
of the longest computational path over all nondeterministic choices.
A classical example of a language in NP is SAT, the language of encodings
of Boolean formulas in conjunctive normal form that have a satisfying assign-
ment, that is, an assignment to the variables that makes the formula evaluate
to “true”(or 1). Here a Boolean formula is an expression consisting of vari-
ables connected with the operations “and”(∧), “or”(∨), and “not”(typically
represented by an overline). A formula is said to be in conjunctive normal
form if it consists of clauses joined by ∧, where each clause is an ∨of vari-
ables or their negations. For example, a typical formula in conjunctive normal
form is
f = (x1 ∨x2 ∨x4) ∧(x2 ∨x3 ∨x5) ∧(x1 ∨x3 ∨x4),
and (x1, x2, x3, x4, x5) = (1, 1, 0, 1, 0) is a satisfying assignment for f .
In order to deﬁne the class NPC, the NP-complete languages, we need
the notion of Karp reduction. We say L1 Karp-reduces to L2, and we write
L1 ≤L2, if there is a polynomial-time computable function f such that x ∈L1
if and only if f (x) ∈L2.
Theorem 1.8.1. If L1 ∈P and L1 ≤L2, then L2 ∈P.
Proof.
On input x, run a TM for f , obtaining f (x). Since f is polynomial-
time computable, we have |f (x)| ≤T (|x|) for some polynomial T . Now run
a polynomial-time algorithm M2 to decide L2 on f (x). Return whatever M2
says. The total time is bounded by a polynomial.
We say a language L is NP-complete if
(a) L ∈NP;
(b) for all L′ ∈NP, we have L′ ≤L.
Thus, in some sense, the NP-complete problems are the “hardest”problems
in NP.
Theorem 1.8.2. SAT is NP-complete.
Proof Idea.
It is easy to see that SAT is in NP, for all we need do is guess
a satisfying assignment and then verify it. To show that every problem in NP
reduces to SAT, take a polynomial-time-bounded TM M. We need to transform

1.9 Exercises
21
an input x to a Boolean formula ϕx, such that ϕx is satisﬁable if and only if M
accepts x. We encode the computation of M on x as a string s of conﬁgurations
separated by delimiters and then create Boolean variables ci,a that are true if and
only if the ith symbol of s equals a. Using these variables, we can construct
ϕx to enforce the conditions that the conﬁgurations are legal, that the ﬁrst
conﬁguration represents the initial state, that a ﬁnal state is eventually reached,
and that each conﬁguration follows from the previous one by a legitimate move
of the machine.
A useful variation on SAT is 3-SAT; in this variant we force every clause to
have exactly three literals.
Theorem 1.8.3. 3-SAT is NP-complete.
Proof Idea.
The basic idea is to introduce new variables to create a formula
where every clause has exactly three literals in such a way that the new for-
mula is satisﬁable if and only if the old formula is satisﬁable. A clause such
as (x1 ∨x2) can be replaced by (x1 ∨x2 ∨y1) ∧(x1 ∨x2 ∨y1). Similarly, a
clause such as (x1 ∨x2 ∨x3 ∨x4 ∨x5) can be replaced by (x1 ∨x2 ∨y1) ∧
(y1 ∨x3 ∨y2) ∧(y2 ∨x4 ∨x5).
We now turn to space complexity. If a TM uses at most f (n) cells on any
input of length n, we say it is of space complexity f (n). One of the most
important theorems about space complexity is Savitch’s theorem.
Theorem 1.8.4 (Savitch). If L is decidable in f (n) nondeterministic space,
and f (n) ≥n, then it is decidable in O(f (n)2) deterministic space.
Proof Idea.
Use the divide-and-conquer strategy coupled with recursion to
decide if a nondeterministic TM M accepts x.
We deﬁne PSPACE to be the class of languages decidable in space bounded
by a polynomial on a deterministic TM and NPSPACE to be the class of
languages decidable in space bounded by a polynomial on a nondetermini-
stic TM.
Corollary 1.8.5. PSPACE = NPSPACE.
1.9 Exercises
1. Prove that for all words u, v ∈∗and integers e ≥0 we have (uv)eu =
u(vu)e.

22
1
Review of formal languages and automata theory
2. Let A, B, and C be languages. For each of the following identities, prove it
or give a counterexample:
(a) (A ∪B)C = AC ∪BC.
(b) (A ∩B)C = AC ∩BC.
3. Find palindromes in languages other than English.
4. The English word antsy has the property that every nontrivial preﬁx is
a valid English word. Can you ﬁnd a longer English word with this pro-
perty?
5. What is the smallest class of languages over  containing each singleton
{a} for a ∈ and closed under the operations of union, intersection, and
complement?
6. Give regular expressions for each of the following languages:
(a) The set of strings over {a, b, c} in which all the a’s precede all the b’s,
which in turn precede all the c’s.
(b) The complement of the language in (a).
(c) The same as in (a), but only the nonempty strings satisfying the condi-
tions.
(d) The set of strings over {a, b} that do not contain the substring aa.
(e) The set of strings over {a, b} that do not contain the substring aab.
(f) The set of strings over {a, b} containing both an even number of a’s and
an even number of b’s.
(g) The set of strings over {a, b} that do not contain two or more consecutive
occurrences of the same letter.
(h) The set of strings that contain at least one occurrence of ab and at least
one occurrence of ba. (These occurrences may overlap.)
(i) The set of strings over {a, b} that contain exactly one occurrence of
the string bbb. Note: Overlapping occurrences should be counted more
than once, so that the string abbbbba contains three occurrences of
bbb.
(j) The set of strings over {a, b} having an equal number of occurrences of
ab and ba.
(k) The set of strings over {a, b} containing at least one a and at least one
b.
7. Let L = {x ∈(a + b)∗: |x|a ̸= |x|b}. Give a regular expression for L2.
8. Suppose M = (Q, , δ, q0, F) is a DFA, and suppose there exists a state
q ∈Q, a string z ∈∗, and integers i, j > 0 such that δ(q, zi) = q =
δ(q, zj). Prove that δ(q, zgcd(i,j)) = q.
9. Let x, y be words. Prove that xy = yx if and only if there exists a word z
such that x2y2 = z2.

1.9 Exercises
23
10. A regular expression r is said to be in disjunctive normal form if it can be
written in the form r = r1 + r2 + · · · + rn for some n ≥1, where none of
the regular expressions r1, r2, . . . , rn contains the symbol + (union). For
example, the regular expression a∗b∗+ (ab)∗+ (c(acb)∗)∗is in disjunc-
tive normal form, but (a + b)∗is not. Prove that every regular language
can be speciﬁed by a regular expression in disjunctive normal form.
11. In an extended regular expression, intersection and complementation may
be used. Show how to write (aba)∗as a star-free extended regular expres-
sion. (That is, your expression can use intersection, union, concatenation,
and complementation, but may not use the Kleene closure or positive
closure operators.)
∗12. Show that allowing intersection in a regular expression can permit dra-
matically more concise regular expressions. More precisely, show that the
shortest regular expression for (· · · (((a2
0a1)2a2)2a3)2 · · · an)2 is of length

(2n), while there exists a regular expression involving intersection of
length O(n2).
13. Prove that each of the following languages is not regular:
(a) {ai bj : gcd(i, j) = 1};
(b) {aibjck : i2 + j 2 = k2}.
14. Consider the language
L = {xcy : x, y ∈{a, b}∗and y is a subsequence of x}.
Show that L is not context-free.
15. Consider the language
L = {xRcy : x, y ∈{a, b}∗and y is a subsequence of x}.
Show that L is context-free but not regular.
16. Prove that each of the following languages is not context-free:
(a) {xx : x ∈{a, b}∗};
(b) {x ∈{a, b, c}∗: |x|a = |x|b = |x|c};
(c) {wxw : w, x ∈{a, b}+};
(d) {x ∈{a, b, c}∗: |x|a = max(|x|b, |x|c)};
(e) {x ∈{a, b, c}∗: |x|a = min(|x|b, |x|c)}.
17. Let L1 and L2 be languages, and deﬁne
join(L1, L2) = {z : there exist x1 ∈L1, x2 ∈L2, with |x1| = |x2|,
such that z = x1x2}.
Prove that if L1 and L2 are regular, then join(L1, L2) is context-free.

24
1
Review of formal languages and automata theory
18. The order of a language L is the smallest integer k such that Lk = Lk+1.
(Note that the order may be inﬁnite.) Show that for each k ≥0, there exists
a regular language of order k.
19. Prove that the converse of the pumping lemma holds if L is a unary
language. Prove that it does not hold, in general, for larger alphabets.
20. Consider the following CFG: G = ({S}, {a, b}, P, S), where the set of
productions P is given by S →SSa | b.
Give an interpretation for L(G) based on the evaluation of an algebraic
expression.
Give another characterization for L(G) in terms of the number of a’s
and b’s in any word w ∈L(G). Prove that your characterization is correct.
21. In our deﬁnition of CFG we demanded that V ∩ = ∅. What happens if
we do not make this restriction?
22. Give CFGs for the following languages:
(a) the set of strings over {a, b} containing twice as many a’sas b’s;
(b) the complement of {(anb)n : n ≥1};
(c) {aibj : i, j ≥0 and i ̸= j and i ̸= 2j};
(d) {aibj : j ≤i ≤2j}.
23. Show that the following decision problems are unsolvable:
(a) Given a TM T , does T enter an inﬁnite loop on input ϵ?
(b) Given a TM T , does T accept ϵ in an even number of moves?
(c) Given a TM T and an input w, does T accept both w and wR?
24. Show that the following decision problems are solvable:
(a) Given a TM T , does T ever enter a state other than the initial state q0?
(b) Given a TM T , does T ever make a right move on input ϵ?
(c) Given a TM T , does T ever make a left move on input ϵ?
25. Give a formal proof that the grammar in Example 1.5.3 is correct.
26. A symbol of a CFG is called useless if it never participates in the derivation
of any terminal string. Give an algorithm that, on input a CFG G, outputs
a CFG G′ such that L(G′) = L(G) and G′ contains no useless symbols.
Your algorithm should not introduce any new ambiguities.
27. Give an algorithm that, on input a CFG G, outputs a CFG G′ such that
L(G′) = L(G) −{ϵ} and G′ has no ϵ-productions or unit productions. Your
algorithm should not introduce any new ambiguities.
28. Give an algorithm that, on input a CFG G, outputs a CFG G′ such that
L(G′) = L(G) −{ϵ} and G′ is in Chomsky normal form. Your algorithm
should not introduce any new ambiguities. Suppose G has m productions
and the length of the longest production is k. Show that G′ can be con-
structed in time polynomial in m and k.

1.9 Exercises
25
29. In this exercise we explore some variations on the TM model and show that
they are all equivalent in computing power to the vanilla TM model. For
each suggested model, give a formal deﬁnition and proof that the model
accepts the same class of languages as an ordinary TM.
(a) A multitrack TM. Here the TM has a single tape that is divided into
parallel tracks. In a single move, the TM can read all the tracks at once
and, based on the contents, move and change the contents of the tracks.
(b) A multitape TM. Here the TM has an arbitrary, but ﬁxed, number
of tapes with independent heads. At any step the TM can read the
contents of the symbols under all the tape heads and move the heads
independently in any direction (or stay stationary).
(c) A TM with doubly inﬁnite tape. Here the tape has no “leftedge”and
the head can move arbitrarily far in either direction.
(d) A TM with two-dimensional tape. Here the head is assumed to be
scanning a cell in an inﬁnite array of cells and, at any point, can move
up, down, right, or left one cell, or remain stationary.
(e) A nondeterministic TM. Here the machine accepts if some series of
choices leads to the halting state.
30. Show that every TM can be simulated by a TM that never writes the blank
symbol B on the tape. Hint: Instead of writing a blank symbol, write an
alternate symbol B.
31. Show that the subset construction for NFA-to-DFA conversion can be
performed in O(kn2n) time, where k is the alphabet size and n is the
number of states. Hint: Precompute all the possible unions.
32. Show that a language L is recursively enumerable if and only if there exists
a TM M with a special output tape, such that M never moves left on its
output tape, writes out a string of the form #x1#x2#x3#x4 · · · on its output
tape, where each xi ∈L, and every element of L eventually appears on the
output tape and exactly once.
33. Consider the following CFG for regular expressions:
S →E+ | E• | G
E+ →E+ + F | F + F
F →E• | G
E• →E• G | GG
G →E∗| C | P
C →∅| ϵ | a (a ∈)
E∗→G ∗
P →(S)

26
1
Review of formal languages and automata theory
The meaning of the variables is as follows:
• S generates all regular expressions.
• E+ generates all unparenthesized expressions where the last operator
was +.
• E• generates all unparenthesized expressions where the last operator
was · (implicit concatenation).
• E∗generates all unparenthesized expressions where the last operator
was ∗(Kleene closure).
• C generates all unparenthesized expressions where there was no last
operator (i.e., the constants).
• P generates all parenthesized expressions.
Here, by parenthesized we mean there is at least one pair of enclosing
parentheses. Note this grammar allows a ∗∗but disallows (
). Prove that
this grammar is correct and unambiguous.
34. Let L be a language. Show that the following are equivalent:
(a) ϵ ∈L and if x, y ∈L, then xy ∈L;
(b) L = L∗;
(c) there exists a language T such that L = T ∗.
1.10 Projects
1. Read some of the foundational papers in the theory of computing, such
as Turing [1936], Rabin and Scott [1959], and Cook [1971], and contrast
the presentation you ﬁnd there with the presentation found in more recent
books. Did Turing actually state the halting problem in his 1936 paper?
1.11 Research problems
1. Is there an inﬁnite family of distinct unary languages (Ln)n≥1 and constants
c, d such that Ln is accepted by an NFA with ≤cn states, but every regular
expression for Ln has ≥dn2 symbols?
1.12 Notes on Chapter 1
There are many excellent textbooks that introduce the reader to the theory
of computation, for example, Martin [1997], Hopcroft, Motwani, and Ullman
[2001], and Lewis and Papadimitriou [1998].
1.2 Some textbooks use the symbols λ or  to denote the empty string. We
use ϵ in this book.

1.12 Notes on Chapter 1
27
Some writers, particularly Europeans, use the term “factor”for what
we have called “subword”and the term “subword”for what we have
called “subsequence.”
1.3 Brzozowski [1962b] is a good introduction to the properties of regular
expressions.
1.4 The origins of ﬁnite automata include the neural net model of McCulloch
and Pitts [1943]. Rabin and Scott [1959] is the fundamental paper in
this area.
Although most writers agree on the conventions for ﬁnite automata
as speciﬁed here, there are some minor differences. For example, some
writers do not enforce the condition that the transition function of a DFA
be a complete function (deﬁned on all elements of its domain Q × ).
Some writers allow an NFA to have an initial “state”that is actually a
set of states.
1.5 The Indian philologist Panini (ca. 400 b.c.e.) used grammars to describe
the structure of Sanskrit. The modern mathematical treatment is due to
Chomsky [1956].
For a delightful collection of examples of ambiguous English sen-
tences, see Thornton [2003].
1.6 TMs were introduced by Turing [1936].
1.7 Oddly enough, Turing’s original paper [1936] did not state exactly what
we call the halting problem today. The ﬁrst use of the term seems to be
in Davis [1958, p. 70]. Also see Strachey [1965].
What we have called “unsolvable”is, in the literature, also called
“undecidable”or “uncomputable.”
1.8 The classic reference for NP-completeness and related topics is Garey
and Johnson [1979]. The book of Papadimitriou [1994] is a good gen-
eral reference on computational complexity. In this text, we have used
polynomial-time reductions, although it is more fashionable these days
to use logspace reductions.

2
Combinatorics on words
In 1906, the Norwegian mathematician Axel Thue initiated the study of what is
now called combinatorics on words—the properties of ﬁnite and inﬁnite strings
of symbols over a ﬁnite alphabet. Although combinatorics on words does not
directly involve machine models, its results have implications for many areas
of computer science and mathematics.
2.1 Basics
We start by deﬁning inﬁnite strings (or inﬁnite words or inﬁnite sequences—we
use the terms interchangeably). We let Z denote the integers, Z+ denote the pos-
itive integers {1, 2, 3, . . .}, and N denote the nonnegative integers {0, 1, 2, . . .}.
Then we usually take an inﬁnite string a0a1a2 · · · to be a map from N to
 (a ﬁnite alphabet), although occasionally we instead use a map from Z+
to .
Example 2.1.1. The following is an example of a right-inﬁnite string:
p = (pn)n≥1 = 0110101000101 · · · ,
where pn = 1 if n is a prime number and 0 otherwise. The sequence p is called
the characteristic sequence of the primes.
The set of all inﬁnite strings over  is denoted by ω. We deﬁne ∞=
∗∪ω. In this book, inﬁnite strings are typically given in boldface.
The notions of subword, preﬁx, and sufﬁx for ﬁnite strings have evident
analogues for inﬁnite strings. Let w = a0a1a2 · · · be an inﬁnite string. For
i ≥0, we deﬁne w[i] = ai. Also, for i ≥0 and j ≥i −1, we deﬁne w[i..j] =
aiai+1 · · · aj.
28

2.2 Morphisms
29
For a sequence of words (wi), we let

i≥1
wi
denote a string w1w2w3 · · · , which is inﬁnite if and only if wi ̸= ϵ inﬁnitely
often. We can also concatenate a ﬁnite string on the left with an inﬁnite string
on the right, but not vice versa. If x is a nonempty ﬁnite string, then xω is the
inﬁnite string xxx · · · . Such a string is called purely periodic. An inﬁnite string
w of the form x yω for y ̸= ϵ is called ultimately periodic. If w is ultimately
periodic, then we can write it uniquely as x yω where |x|, |y| are as small as
possible. In this case y is referred to as the period of w, and x is called the
preperiod of w. In some cases, the word period refers to the length |y|, and
similarly for “preperiod.”
If L is a language, we deﬁne
Lω = {w1w2w3 · · · : wi ∈L −{ϵ} for all i ≥1}.
2.2 Morphisms
In this section we introduce a fundamental tool of formal languages, the homo-
morphism, or just morphism for short. Let  and  be alphabets. A morphism
is a map h from ∗to ∗that obeys the identity h(xy) = h(x)h(y) for all
strings x, y ∈∗. Typically, we use the roman letters f, g, h and the Greek
letters µ, τ to denote morphisms.
Clearly if h is a morphism, then we must have h(ϵ) = ϵ. Furthermore, once
h is deﬁned for all elements of , it can be uniquely extended to a map from
∗to ∗. Henceforth, when we deﬁne a morphism, we usually give it by
specifying its action only on .
Example 2.2.1. Let  = {e, m, o, s} and  = {a, e, l, n, r, s, t}, and deﬁne
h(m) = ant;
h(o) = ϵ;
h(s) = ler;
h(e) = s.
Then h(moose) = antlers.
If  = , then we can iterate the application of h. We deﬁne h0(a) = a and
hi(a) = h(hi−1(a)) for all a ∈.

30
2
Combinatorics on words
Example 2.2.2. Let  =  = {0, 1}. Deﬁne the Thue–Morse morphism
µ(0) = 01 and µ(1) = 10. Then µ2(0) = 0110 and µ3(0) = 01101001.
We can also apply morphisms to inﬁnite strings. If w = c0c1c2 · · · is an
inﬁnite string, then we deﬁne
h(w) = h(c0)h(c1)h(c2) · · · .
2.3 The theorems of Lyndon–Sch¨utzenberger
In this section, we prove two beautiful and fundamental theorems due to Lyndon
and Sch¨utzenberger.
We start with one of the simplest and most basic results on strings, sometimes
known as Levi’s lemma:
Lemma 2.3.1. Let u, v, x, y ∈∗, and suppose that uv = xy. If |u| ≥|x|,
there exists t ∈∗such that u = xt and y = tv. If |u| < |x|, there exists
t ∈+ such that x = ut and v = ty.
Proof. Left to the reader.
To motivate the ﬁrst theorem of Lyndon–Sch¨utzenberger, consider the fol-
lowing problem: under what conditions can a string have a nontrivial proper
preﬁx and sufﬁx that are identical? Examples in English include reader, which
begins and ends with r, and alfalfa, which begins and ends with alfa. The
answer is given by the following theorem.
Theorem 2.3.2. Let x, y, z ∈+. Then xy = yz if and only if there exist
u ∈+, v ∈∗, and an integer e ≥0 such that x = uv, z = vu, and y =
(uv)eu = u(vu)e.
Proof. (⇐): This direction is easy. We have
xy = uv(uv)eu = (uv)e+1u;
yz = u(vu)evu = u(vu)e+1;
and these strings are equal by Exercise 1.1.
(⇒): The proof is by induction on |y|. If |y| = 1, then y = a for a ∈.
Then xa = az. Thus, x begins with a and z ends with a, so we can write
x = ax′ and z = z′a. Thus, ax′a = xa = az = az′a, and so x′ = z′. Thus we
can take u = a, v = x′, and e = 0.

2.3 The theorems of Lyndon–Sch¨utzenberger
31
Now suppose that |y| > 1. There are two cases:
Case I: If |x| ≥|y|, then we have a situation like the following:
x
z
y
y
w
By Levi’s lemma there exists w ∈∗such that x = yw and z = wy. Now
take u = y, v = w, e = 0, and we are done.
Case II: Now suppose that |x| < |y|. Then we have a situation like the following:
y
z
x
y
w
By Levi’s lemma there exists w ∈+ such that y = xw = wz. By induction
(since 0 < |w| = |y| −|x| < |y|), we know there exist u ∈+, v ∈∗, e ≥0
such that
x = uv;
z = vu;
w = (uv)eu = u(vu)e.
so it follows that y = xw = uv(uv)eu = (uv)e+1u.
To motivate the second theorem of Lyndon and Sch¨utzenberger, consider the
following problem: what are the solutions in strings to the equation x2 = y3?
If we take the positive integers as an analogy, then unique factorization into
primes suggests that the only possible solutions are when x is a cube of some
string z and y is the square of z.
Another motivation comes from the problem of determining when words
can commute: when can xy = yx? There are not too many nontrivial examples
in English; some examples are x = do, y = dodo and x = tar, y = tartar.
The general case is given by the following result.

32
2
Combinatorics on words
Theorem 2.3.3. Let x, y ∈+. Then the following three conditions are equiv-
alent:
1. xy = yx.
2. There exist z ∈+ and integers k, l > 0 such that x = zk and y = zl.
3. There exist integers i, j > 0 such that xi = yj.
Proof. We show that (1) ⇒(2), (2) ⇒(3), and (3) ⇒(1).
(1) ⇒(2): By induction on |xy|. If |xy| = 2, then |x| = |y| = 1, so x = y
and we may take z = x = y, k = l = 1.
Now assume that the implication is true for all x, y with |xy| < n. We prove
it for |xy| = n. Without loss of generality, assume |x| ≥|y|. Then we have a
situation like the following:
x
y
y
w
x
Hence there exists w ∈∗such that x = wy = yw. If |w| = 0, then x = y,
so we may take z = x = y and k = l = 1. Otherwise w ∈+. Now |wy| =
|x| < |xy| = n, so the induction hypothesis applies, and there exist z ∈+ and
integers k, l > 0 such that w = zk, y = zl. It follows that x = wy = zk+l.
(2) ⇒(3): By (2) there exist z ∈+ and integers k, l > 0 such that x = zk
and y = zl. Hence, taking i = l, j = k, we get
xi = (zk)i = zkl = (zl)k = (zl)j = yj,
as desired.
(3) ⇒(1): We have xi = yj. Without loss of generality, assume |x| ≥|y|.
Then we have a situation like the following:
x
x
x
x
y
y
y
y
y
y
w
That is, there exists w ∈∗such that x = yw. Hence, xi = (yw)i = yj, and
so y(wy)i−1w = yj. Therefore, (wy)i−1w = yj−1 and so, by multiplying by y

2.3 The theorems of Lyndon–Sch¨utzenberger
33
on the right, we get (wy)i = yj. Hence, (yw)i = (wy)i, and hence yw = wy.
It follows that x = yw = wy and xy = (yw)y = y(wy) = yx.
We now make the following deﬁnition: a string w ∈+ is a power if there
exists a string z and an integer k ≥2 such that w = zk. A string w ∈+
that is not a power is called primitive. For example, door is primitive, but
dodo = (do)2 is not.
Theorem 2.3.4. Every nonempty string w can be expressed uniquely in the
form w = xn, where n ≥1 and x is primitive.
Proof.
Choose n as large as possible so that w = xn has a solution; clearly,
1 ≤n ≤|w|. We claim that the resulting x is primitive. For if not, we could
write x = yk for some k ≥2 and then w = ykn, where kn > n.
To prove uniqueness, suppose that w has two representations w = xn = ym,
where both x, y are primitive and n, m ≥1. Then by Theorem 2.3.3, there
exists z with |z| ≥1 such that x = zk and y = zℓ. Since x, y are primitive,
however, we must have k = ℓ= 1. But then x = y = z, and hence n = m, and
the two representations are actually the same.
If w = xn, where x is primitive, then x is sometimes called the primitive
root of w.
The following theorem can be thought of as a generalization of Theo-
rem 2.3.3.
Theorem 2.3.5. Let w and x be nonempty words. Let y ∈w{w, x}ω and z ∈
x{w, x}ω. Then the following conditions are equivalent:
(a) y and z agree on a preﬁxof length at least |w| + |x| −gcd(|w|, |x|).
(b) wx = xw.
(c) y = z.
Proof.
(a) ⇒(b): We prove the contrapositive. Suppose wx ̸= xw.
Then we prove that y and z differ at a position ≤|w| + |x| −gcd(|w|, |x|).
The proof is by induction on |w| + |x|.
The base case is |w| + |x| = 2. Then |w| = |x| = 1 and |w| + |x| −
gcd(|w|, |x|) = 1. Since wx ̸= xw, we must have w = a, x = b with a ̸= b.
Then y and z differ at the ﬁrst position.
Now assume the result is true for |w| + |x| < k. We prove it for |w| + |x| =
k. If |w| = |x|, then y and z must disagree at the |w|th position or earlier, for
otherwise w = x and wx = xw; since |w| ≤|w| + |x| −gcd(|w|, |x|) = |w|,

34
2
Combinatorics on words
the result follows. So, without loss of generality, assume |w| < |x|. If w is not
a preﬁx of x, then y and z disagree on the |w|th position or earlier, and again
|w| ≤|w| + |x| −gcd(|w|, |x|).
So w is a proper preﬁx of x. Write x = wt for some nonempty word t.
Now any common divisor of |w| and |x| must also divide |x| −|w| = |t|, and
similarly any common divisor of both |w| and |t| must also divide |w| + |t| =
|x|. So gcd(|w|, |x|) = gcd(|w|, |t|).
Now wt ̸= tw, for otherwise we have wx = wwt = wtw = xw, a con-
tradiction. Then y = ww · · · and z = wt · · · . By induction (since |w| + |t| <
k), w−1y and w−1z disagree at position |w| + |t| −gcd(|w|, |t|) or earlier.
Hence, y and z disagree at position 2|w| + |t| −gcd(|w|, |t|) = |w| + |x| −
gcd(|w|, |x|) or earlier.
(b) ⇒(c): If wx = xw, then by the second theorem of Lyndon–
Sch¨utzenberger, both w and x are in u+ for some word u. Hence, y = uω = z.
(c) ⇒(a): Trivial.
There is another possible generalization of Theorem 2.3.3. To state it, we
need the notion of fractional power. If z = xnx′, where n ≥1, 1 ≤|x′| ≤|x|,
and x′ is a preﬁx of x, then we say z is a |z|/|x| power. For example, alfalfa
is a 7/3 power, as it equals (alf)2a. Similarly, if p/q is a rational number >
1, and |x| is divisible by q, then by xp/q we mean z = xax′, where a =
⌊p/q⌋, x′ is a preﬁx of x, and |z|/|x| = p/q. For example, (entanglem)4/3 =
entanglement.
Theorem 2.3.6. Let x and y be nonempty words. Then xy = yx if and only if
there are rational numbers α, β ≥2 such that xα = yβ.
Proof.
Suppose xy = yx. Then by Theorem 2.3.3, there must be integers
i, j ≥1 such that xi = yj. Hence we can take α = 2i, β = 2j.
Now suppose xα = yβ. Without loss of generality, we can assume |x| ≥|y|.
Then xω and yω agree on a preﬁx of length ≥α|x| ≥2x ≥|x| + |y| > |x| +
|y| −gcd(|x|, |y|). Hence by Theorem 2.3.5, xy = yx.
2.4 Conjugates and borders
We say a word w is a conjugate of a word x if w is a cyclic shift of x, that
is, if there exist words u, v such that w = uv and x = vu, and we write w ∼
x. For example, enlist and listen are conjugates (take u = en, and v =
list).

2.4 Conjugates and borders
35
Theorem 2.4.1. The conjugacy relation ∼is an equivalence relation.
Proof. Left to the reader as Exercise 10.
Theorem 2.4.2. Let w and x be conjugates. Then w is a power if and only if x
is a power. Furthermore, if w = yk, k ≥2, then x = zk, where z is a conjugate
of y.
Proof.
Since w and x are conjugates, there exist u, v such that w = uv,
x = vu. Furthermore, since w is a power, there exists a word y and an integer
k ≥2 such that w = yk. Hence, yk = uv.
If |u| is a multiple of |y|, then u = yi for some i, and so v = yk−i. Thus,
x = vu = yk.
Otherwise, assume |u| is not a multiple of |y|. Then we can write u =
yir, v = syk−i−1, where r, s ̸= ϵ and rs = y. Then x = vu = syk−i−1yir =
s(rs)k−1r = (sr)k. Thus, x is a power. Letting z = sr, we also see that x = zk,
and z is a conjugate of y.
Now we turn to borders. A word w ∈+ is said to be bordered if it can
be written as w = xyx, where x ∈+, y ∈∗. Alternatively, a word w is
bordered if and only if it is an α-power, for α a rational number > 1. Examples
of bordered words in English include outshout and photograph. If a word
is not bordered, it is unbordered.
Theorem 2.4.3. Let w be a nonempty word. Then w is primitive if and only if
w has an unbordered conjugate.
Proof. Suppose w is primitive. Let x be the lexicographically least conjugate
of w. I claim x is unbordered. For if x were bordered, we could write x = uvu,
where u ∈+ and v ∈∗. Now z = uuv is a conjugate of x and so is a
conjugate of w. If x = z, then uv = vu. If v = ϵ, then x = u2. Otherwise,
by Theorem 2.3.3, there exist a string t and integers i, j ≥1 such that u =
ti, v = tj. Then x = uvu = t2i+j. By Theorem 2.4.2, w is also a power, a
contradiction.
Now x < z, since x was lexicographically least among all conjugates of
w. Then uvu < uuv, so vu < uv. Then vuu < uvu = x, so we have found a
conjugate of w that is lexicographically smaller than x, a contradiction.
For the other direction, suppose w is not primitive, and let x be any conjugate
of w. By Theorem 2.4.2, x is also a power; that is, x = tk for some nonempty
t and integer k ≥2. Then x = ttk−2t, so x is bordered.

36
2
Combinatorics on words
We now apply our results about conjugates and borders to determine the
solutions to the equation in words xiyj = zk. But ﬁrst, we need a technical
lemma.
Lemma 2.4.4. Let x be a power, x = zk, k ≥2, and let w be a subword of x
with |w| > |z|. Then w is bordered.
Proof.
Since w is a subword of x, we have x = ywz for some words y, z.
Now consider wzy, a conjugate of x. By Theorem 2.4.2, since x is a kth power
of z, we know that wzy is a kth power of some conjugate t of z. Write w = tjt′,
where j ≥1 and t′ is a nonempty preﬁx of t (possibly equal to t). Then w
begins and ends with t′.
Now we turn to a famous equation in words, which might be considered as
the noncommutative version of Fermat’s last theorem.
Theorem 2.4.5. The equation
xi = yjzk
(2.1)
holds for strings x, y, z ∈+ and i, j, k ≥2 if and only if there exist a word
w ∈+ and integers l, m, n ≥1 such that x = wl, y = wm, z = wn, with
li = mj + nk.
Proof. Suppose x = wl, y = wm, and z = wn and li = mj + nk. Then xi =
wli = wmj+nk = wmjwnk = yjzk.
For the other direction, without loss of generality, assume x, y, and z are
primitive; otherwise, replace a nonprimitive string with its primitive root and
adjust the exponent. Assume x is of minimal length, satisfying an equation of
the form (2.1), and also assume, contrary to what we want to prove, that there
is no w ∈+ with x, y, z ∈w+.
If yp ∈x+ for some p, 1 ≤p ≤j, then by Theorem 2.3.3, y and x are both
powers of a word w, which can be assumed to be primitive. Now we can cancel
powers of w on both sides of Eq. (2.1) to get an equation of the form ws = zk.
It follows that z is also a power of w, a contradiction. By symmetry, the same
conclusion follows if zq ∈x+ for some q, 1 ≤q ≤k. Thus we can assume
yp, zq ̸∈x+ for 1 ≤p ≤j, 1 ≤q ≤k. In particular, |x| ̸= |y| and |x| ̸= |z|.
If |y| > |x|, then we have, by looking at a preﬁx of Eq. (2.1), that xα = y2
for some rational α > 2. Then by combining Theorems 2.3.6 and 2.3.3, we
see that there exists a string w and integers r, s ≥1 such that y = wr, x = ws.
Since |y| > |x|, we must have r ≥2. So y is not primitive, a contradiction. A
similar conclusion follows if |z| > |x|, by considering the reversal of Eq. (2.1).
So we can assume that |y|, |z| < |x|.

2.5 Repetitions in strings
37
Suppose i > 2. By Theorem 2.4.3, x has an unbordered conjugate f . Write
x = uv and f = vu. Then xi = (uv)i = u(vu)i−1v. Thus at least two copies
of f lie within xi = yjzk, so at least one copy is either entirely within yj or
entirely within zk. By Lemma 2.4.4, f is bordered. This is a contradiction.
It remains to consider the case i = 2. If |yj| = |zk|, then x = yj = zk, a
contradiction. Without loss of generality, we can assume |yj| > |zk|. Hence,
yj = xur for some primitive word u and integer r ≥1. Similarly, urzk = x.
Thus, multiplying both sides by ur, we get u2rzk = urx. Thus, u2rzk is a
conjugate of yj. Hence, by Theorem 2.4.2, u2rzk = vj for some v a conjugate of
y. Now we have an equation of the form (2.1) with |v| = |y| < |x|, contradicting
the minimality of x.
Thus our original assumption that there is no w ∈+ with x, y, z ∈w+ is
false, and such a w must exist.
2.5 Repetitions in strings
A square is a string of the form xx, such as the English word hotshots. If
w is a (ﬁnite or inﬁnite) string containing no nonempty subword of this form,
then it is said to be squarefree. Note that the string square is squarefree, while
the string squarefree is not.
It is easy to verify (see Exercise 3) that there are no squarefree strings of
length >3 over a two-letter alphabet. However, there are inﬁnite squarefree
strings over a three-letter alphabet. We construct one later in Theorem 2.5.2.
Similarly, a cube is a string of the form xxx, such as the English sort-of-word
shshsh. If w contains no nonempty cube, it is said to be cubefree. The string
cubefree is not squarefree, since it contains two consecutive occurrences of
the string e, but it is cubefree.
An overlap is a string of the form cxcxc, where x is a string and c is a single
letter. (The term overlap refers to the fact that such a string can be viewed as
two overlapping occurrences of the string cxc.) The English string alfalfa,
for example, is an overlap with c = a and x = lf. If w contains no overlap, it
is said to be overlap-free.
In this section, we prove some simple results in the theory of repetitions in
strings. We start by constructing an inﬁnite overlap-free string over an alphabet
of size 2.
Deﬁne
tn =

0,
if the number of 1s in the base-2 expansion of n is even;
1,
if the number of 1s in the base-2 expansion of n is odd.

38
2
Combinatorics on words
tk
t0
k



m



· · ·
x
x
a
u
a
a
tk+2m
tk+m
t =
v
Figure 2.1: Hypothesized overlap in t
Note that t2n = tn, because the base-2 expansions of n and 2n have the same
number of 1s. Similarly, t2n+1 = 1 −tn, since we get the base-2 expansion of
2n + 1 by concatenating the base-2 expansion of n with a 1.
We deﬁne t = t0t1t2 · · · = 01101001 · · · . The inﬁnite string t is usually
called the Thue–Morse sequence, named after Axel Thue and Marston Morse,
two of the ﬁrst mathematicians to study its properties. This inﬁnite sequence
occurs in many different areas of mathematics, physics, and computer sci-
ence (for a brief tour, see Section 2.6). In a moment we will prove Thue’s
theorem that t is overlap-free. It is interesting to note that Thue published
his result in an obscure Norwegian journal and it was overlooked for many
years. In the meantime, many people rediscovered the sequence and its
properties.
Theorem 2.5.1. The Thue–Morse sequence t is overlap-free.
Proof Idea.
Assume that t has an overlap; this implies that a certain set of
equations holds on the symbols of t. Use the identities t2n = tn and t2n+1 =
1 −tn for n ≥0 to derive a contradiction.
Proof. Assume, contrary to what we want to prove, that t contains an overlap.
Then we would be able to write t = uaxaxav for some ﬁnite strings u, x, an
inﬁnite string v, and a letter a (see Figure 2.1).
In other words, we would have tk+j = tk+m+j for 0 ≤j ≤m, where m =
|ax| and k = |u|. Assume m ≥1 is as small as possible. Then there are two
cases: (i) m is even and (ii) m is odd.
(i) If m is even, then let m = 2m′. Again there are two cases: (a) k is even and
(b) k is odd.
(a) If k is even, then let k = 2k′. Then we know tk+j = tk+m+j for 0 ≤
j ≤m, so it is certainly true that tk+2j ′ = tk+m+2j ′ for 0 ≤j ′ ≤m/2.
Hence, t2k′+2j ′ = t2k′+2j ′+2m′ for 0 ≤j ′ ≤m′, and so tk′+j ′ = tk′+j ′+m′
for 0 ≤j ′ ≤m′. But this contradicts the minimality of m.
(b) If k is odd, then let k = 2k′ + 1. Then as before we have tk+2j ′ =
tk+m+2j ′ for 0 ≤j ′ ≤m/2. Hence, t2k′+2j ′+1 = t2k′+2j ′+2m′+1 for 0 ≤

2.5 Repetitions in strings
39
j ′ ≤m′, and so tk′+j ′ = tk′+j ′+m′ for 0 ≤j ′ ≤m′, again contradicting
the minimality of m.
(ii) If m is odd, then there are three cases: (a) m ≥5, (b) m = 3, and (c) m = 1.
For n ≥1, we deﬁne bn = (tn + tn−1) mod 2. Note that b4n+2 = (t4n+2 +
t4n+1) mod 2. Since the base-2 representations of 4n + 2 and 4n + 1 are
identical, except that the last two bits are switched, we have t4n+2 = t4n+1,
and so b4n+2 = 0. On the other hand, b2n+1 = (t2n+1 + t2n) mod 2, and the
base-2 representations of 2n + 1 and 2n are identical except for the last bit;
hence, b2n+1 = 1.
(a) m odd, ≥5. We have bk+j = bk+m+j for 1 ≤j ≤m. Since m ≥5,
we can choose j such that k + j ≡2 (mod 4). Then for this value of
k + j, we have from earlier that bk+j = 0, but k + j + m is odd, so
bk+m+j = 1, a contradiction.
(b) m = 3. Again, bk+j = bk+j+3 for 1 ≤j ≤3. Choose j such that k +
j ≡2 or 3 (mod 4). If k + j ≡2 (mod 4), then the reasoning of the
previous case applies. Otherwise k + j ≡3 (mod 4) and then bk+j = 1,
while bk+j+3 = 0.
(c) m = 1. Then tk = tk+1 = tk+2. Hence, t2n = t2n+1 for n = ⌈k/2⌉, a con-
tradiction.
This completes the proof.
Using the fact that t is overlap-free, we may now construct a squarefree
inﬁnite string over the alphabet 3 = {0, 1, 2}.
Theorem 2.5.2. For n ≥1, deﬁne cn to be the number of 1s between the
nth and (n + 1)th occurrence of 0 in the string t. Set c = c1c2c3 · · · . Then
c = 210201 · · · is an inﬁnitesquarefree string over the alphabet 3.
Proof.
First, observe that c is over the alphabet {0, 1, 2}. For if there were
three or more 1s between two consecutive occurrences of 0 in t, then t would
not be overlap-free, a contradiction.
Next, assume that c is not squarefree. Then it contains a square of the form
xx, with x = x1x2 · · · xn and n ≥1. Then, from the deﬁnition of c, the string t
would contain a subword of the form
01x101x20 · · · 01xn01x101x20 · · · 01xn0,
which constitutes an overlap, a contradiction.
For alternate deﬁnitions of c, see Exercise 25.

40
2
Combinatorics on words
2.6 Applications of the Thue–Morse sequence
and squarefree strings
Both the Thue–Morsesequence and squarefree strings turn out to have many
applications in many different ﬁelds of mathematics and computer science. In
this section we brieﬂy survey some of these applications.
2.6.1 The Tarry–Escott problem
Let us begin with the Tarry–Escottproblem (also known as the problem of
multigrades). This old problem, still not completely solved, asks for solutions
to the equation

i∈I
ik =

j∈J
j k
for k = 0, 1, 2, . . . , n, where I and J are disjoint sets of integers.
Example 2.6.1. The equation
0k + 3k + 5k + 6k = 1k + 2k + 4k + 7k
holds for k = 0, 1, 2 (but not for k = 3). Of course, we deﬁne 00 = 1.
Although Escott and Tarry discussed the problem in 1910 and 1912, re-
spectively, ´Etienne Prouhet found an interesting connection between the Thue–
Morse sequence and the Tarry–Escottproblem in 1851. Here is (a weak version
of) what Prouhet discovered.
Theorem 2.6.2. Let t = t0t1t2 · · · be the Thue–Morse sequence and N be a
positive integer. Deﬁne
I = {i ∈{0, 1, . . . , 2N −1} : ti = 0};
J = {j ∈{0, 1, . . . , 2N −1} : tj = 1}.
Then for 0 ≤k < N, we have

i∈I
ik =

j∈J
j k.
Example 2.6.1 is the case N = 3 of this theorem. The proof is not terribly
difﬁcult, but is left to the reader as Exercise 21.

2.6 Applications of the Thue–Morse sequence and squarefree strings
41
2.6.2 Certain inﬁnite products
Next we turn to some interesting inﬁnite products. Consider the following
inﬁnite sequence:
1/2, 1
2/3
4,
1
2
3
4
/
5
6
7
8
, . . .
What is the limit of this sequence? Computing the ﬁrst few terms suggests that
the limit is about 0.7071, which you may recognize as
√
2/2. But how can we
prove this?
The ﬁrst step is to recognize the hidden occurrence of the Thue–Morse
sequence. In fact, it is not hard to see that we can write the limit as
∞

n=0
2n + 1
2n + 2
εn
,
(2.2)
where εn = (−1)tn and tn is the nth symbol of the Thue–Morsesequence. We
now prove that this inﬁnite product equals
√
2/2.
Let P and Q be the inﬁnite products deﬁned by
P =
∞

n=0
2n + 1
2n + 2
εn
,
Q =
∞

n=1

2n
2n + 1
εn
.
Then
PQ = 1
2
∞

n=1

n
n + 1
εn
= 1
2
∞

n=1

2n
2n + 1
ε2n ∞

n=0
2n + 1
2n + 2
ε2n+1
.
(We have to check convergence, but this is left to the reader.) Now, since
ε2n = εn and ε2n+1 = −εn, we get
PQ = 1
2
∞

n=1

2n
2n + 1
εn  ∞

n=0
2n + 1
2n + 2
εn−1
= 1
2
Q
P .
Since Q ̸= 0, this gives P 2 = 1/2, and the result follows since P is positive.
2.6.3 Chess and music
Now let us turn to applications of squarefree words. One of the ﬁrst applications
was to the game of chess.
Modern-day chess has several rules to avoid the possibility of inﬁnite games.
Rule 9.3 of the FIDE Laws of Chess states that if 50 consecutive moves are
made by each player without a pawn being moved or a piece being captured,

42
2
Combinatorics on words
then a draw can be claimed. Rule 9.2 states that if the same position occurs
three times, then a draw can be claimed.
Max Euwe (1901–1981), the Dutch chess grandmaster and world chess
champion from 1935 to 1937, discussed what would happen if Rule 9.3 were
discarded and Rule 9.2 were weakened to the following “Germanrule”:if
the same sequence of moves occurs twice in succession and is immediately
followed by the ﬁrst move of a third repetition, then a draw occurs. Could an
inﬁnite game of chess then be played?
Euwe showed the answer is yes, and his technique was based on what we
now call the Thue–Morsesequence. If we take the Thue-Morse sequence, and
replace each 0 by the following sequence of four moves
Ng1–f3
Ng8–f6
Nf3–g1
Nf6–g8
and each 1 by the following sequence of four moves
Nb1–c3
Nb8–c6
Nc3–b1
Nc6–b8
then the fact that t is overlap-free means that the German rule can never apply.
This may not be the world’s most interesting chess game, but it is inﬁnite.
The Thue–Morsesequence even appears in music. The Danish composer
Per Nørg˚ard (1932–) independently rediscovered the Thue–Morse sequence
and used it in some of his compositions, such as the ﬁrst movement of his
Symphony No. 3.
2.6.4 The Burnside problem
Finally, we mention the occurrence of repetition-free sequences in the solution
of the Burnside problem for groups.
Recall that a group G is a nonempty set together with a binary operation ·
that satisﬁes the following properties:
(a) a · b ∈G for all a, b ∈G.
(b) a · (b · c) = (a · b) · c.
(c) There exists a distinguished element e ∈G such that e · g = g · e = g for
all g ∈G.
(d) For all g ∈G there exists an element g′ ∈G such that g · g′ = g′ · g = e.
We usually write g′ = g−1.
For group multiplication we often write gg′ for g · g′.
If G is a group and X a nonempty subset of G, then ⟨X⟩, the subgroup
generated by X, is the set
{ae1
1 ae2
2 · · · aet
t
: ai ∈X, t ≥0, and ei ∈{1, −1}}.

2.7 Exercises
43
If there exist ﬁnitely many elements b1, b2, . . . , bn such that G = ⟨b1, b2,
. . . , bn⟩then G is said to be ﬁnitelygenerated.
The exponent of a group G is the least integer n > 0 such that xn = e for
all x ∈G. If no such n exists then G has exponent ∞. Note that it is possible
for each element to have ﬁnite order (i.e., for there to exist an r, depending on
x, such that xr = e) and yet the group’s exponent is inﬁnite (see Exercise 31).
If every element has ﬁnite order, the group is called “periodic”or “torsion.”
Burnside asked in 1902 if every group of ﬁnite exponent n with a ﬁnite
number of generators m is necessarily ﬁnite.
For m = 1 the answer is trivially yes, and for m > 1 and n = 2, 3, the answer
is also yes, although more work is needed. Sanov proved in 1940 that the answer
is yes for m > 1 and n = 4, and Hall proved in 1957 that the answer is yes for
m > 1 and n = 6. However, in 1968 Novikov and Adian proved that the answer
is no for m > 1 and n ≥4381 and odd. Later, Adian improved the bound from
4381 to 665. The proof is based in part on the existence of cubefree sequences.
2.7 Exercises
1. Deﬁne the strings Fn (n ≥1) as follows:
F1 = 0;
F2 = 1;
Fn = Fn−1Fn−2
for n ≥3.
Thus, for example, we ﬁnd F3 = 10, F4 = 101, and so on.
(a) Prove that no Fi contains either 00 or 111 as a substring.
(b) Guess the relationship between FiFi+1 and Fi+1Fi, and prove your
guess by induction.
2. Is the decimal expansion of π a squarefree string? Is it cubefree? Answer
the same questions for e (the base of natural logarithms), Euler’s constant,
√
2, and log 2. What are the highest powers you can ﬁnd in the decimal
expansions of these numbers?
3. Show there are no squarefree strings of length >3 over a two-letter alphabet.
4. Find necessary and sufﬁcient conditions for a bordered word to equal a
square.
5. Show that if x, w are strings, then xw cannot equal wx in all positions
except one.
6. A morphism h : ∗→∗is overlap-free if for any overlap-free word
w ∈∗, h(w) is overlap-free. Prove that if || > ||, then the only overlap-
free morphism h is the morphism deﬁned by h(a) = ϵ for all a ∈.

44
2
Combinatorics on words
7. Let t = (tn)n≥0 = 01101001 · · · be the Thue–Morseinﬁnite word. Show
that t = µ(t), where µ is the Thue–Morsemorphism introduced in Exam-
ple 2.2.2.
8. Show that every ﬁnite nonempty preﬁx of the Thue–Morse word
01101001 · · · is primitive.
9. Prove the following variation on Theorem 2.3.2: let y ∈∗, and x, z ∈+.
Then xy = yz if and only if there exist u, v ∈∗, and an integer e ≥0
such that x = uv, z = vu, and y = (uv)eu = u(vu)e.
10. Show that the relation ∼(“isa conjugate of”)is an equivalence relation.
11. Show that x and y are conjugates if and only if there exists a string t such
that xt = ty.
12. Consider the equation w2 = x2y2z2 in nonempty words. Show that there
exist solutions in which no pair of words chosen from {w, x, y, z} com-
mutes.
13. Let y, z be palindromes. Show that if at least one of |y|, |z| is even, then
some conjugate of yz is a palindrome. Show that if both |y|, |z| are odd,
then the result need not hold.
14. Find all solutions to the equation (vu)n = unvn in nonempty words for
n ≥1.
15. Prove that every conjugate of µn(0) is overlap-free, for n ≥0, where µ is
the Thue–Morsemorphism.
16. Let x ∈{0, 1}∗. Deﬁne x to be the string obtained by changing every 0 in
x to 1 and every 1 to 0. Deﬁne X0 = 0 and Xn+1 = XnXn for n ≥0. Thus,
X1 = 01, X2 = 0110, and so on. Show that Xn = µn(0), where µ is the
Thue–Morsemorphism of Section 2.2.
17. Prove the following “repetition theorem” for inﬁnite words. Let w =
w1w2w3 · · · , and suppose w = wk+1wk+2wk+3 · · · for some k > 0. Then
w = (w1w2 · · · wk)ω.
18. Suppose x, y are words with xy ̸= yx. Show that, for all n ≥1, at least
one of xny and xn+1y is primitive.
19. Let x, y, z be words. Show that xyz = zyx if and only if there exist words
u, v and integers i, j, k ≥0 with x = (uv)iu, y = (vu)jv, and z = (uv)ku.
20. Suppose xyz is a square and xyyz is a square. Show that xyiz is a square
for all i ≥0.
21. Prove Theorem 2.6.2 by induction on N.
22. Let
w = a1a2a3 · · ·
be an inﬁnite squarefree word. Show that all the “shifts”of w (namely, the
words a1a2a3 · · · , a2a3a4 · · · , a3a4a5 · · · , etc.) are distinct.

2.7 Exercises
45
23. Call a word w uneven if every nonempty subword has the property that
at least one letter appears an odd number of times. For example, abac is
uneven.
(a) Show that if w is an uneven word over an alphabet with k letters,
then |w| < 2k.
(b) Prove that the bound in (a) is sharp, by exhibiting an uneven word
of length 2k −1 over every alphabet of size k ≥1.
24. Let w = c0c1c2 · · · = 0010011010010110011 · · · be the inﬁnite se-
quence deﬁned by cn = the number of 0s (mod 2) in the binary expansion
of n. Prove or disprove that w is overlap-free.
Note: c0 = 0 because the binary expansion of 0 is understood to be ϵ,
the empty string.
25. In this exercise we explore some alternative constructions of c, the inﬁnite
squarefree word introduced in Section 2.5.
(a) Let t0t1t2 . . . be the Thue–Morse word. Deﬁne bn = τ(tn, tn+1),
where
τ(0, 0) = 1;
τ(0, 1) = 2;
τ(1, 0) = 0;
τ(1, 1) = 1.
Show that c = b0b1b2 · · · .
(b) Let f be the morphism that maps 2 →210, 1 →20, and 0 →1.
Show that f (c) = c.
(c) Let g be the morphism that maps a →ab, b →ca, c →cd, d →
ac, and let h be the coding that maps a →2, b →1, c →0, d →1.
Show that c = h(gω(a)).
26. Is the Thue–Morse sequence mirror invariant, that is, if w is a ﬁnite
subword of t, need its reversal wR also be a subword of t?
∗27. An inﬁnite string x is said to be recurrent if every subword that occurs
in x occurs inﬁnitely often in x.
(a) Show that an inﬁnite string is recurrent if and only if every subword
that occurs in x occurs at least twice.
(b) Show that if an inﬁnite string is mirror invariant, then it is recur-
rent.
∗∗28. Let x, y, z, w be ﬁnite strings. Find necessary and sufﬁcient conditions
for the following two equations to hold simultaneously: xy = zw and
yx = wz.

46
2
Combinatorics on words
29. Recall the deﬁnition of the M¨obius function,
µ(n) =

0,
if n is divisible by a square > 1;
(−1)j,
if n = p1p2 · · · pj, where the pi are distinct primes.
(2.3)
(Hopefully, there will be no confusion with the morphism µ deﬁned in
Section 2.2.)
Show that there are 	
d | n µ(d)kn/d distinct primitive strings of length
n over a k-letter alphabet. (Here 	
d | n means that the sum is over the
positive integer divisors d of n.)
30. Give an example of an inﬁnite periodic group.
31. Give an example of a group G where each element has ﬁnite order but the
group’s exponent is ∞.
∗32. It was once conjectured that if A and B are ﬁnite sets with AB = BA,
then both A and B are a union of powers of some ﬁnite set E.
(a) Prove that this conjecture holds if |A| = 1 and |B| = 2.
(b) Prove that the conjecture does not hold in general.
33. Let w = a1a2a3 · · · be any inﬁnite squarefree word over  = {0, 1, 2}.
Show that the inﬁnite word a1a1a2a2a3a3 · · · contains no subword of the
form ycy for y ∈+ and c ∈.
34. Let v, x ∈+ and w ∈∗. Show that the following two conditions are
equivalent:
(a) There exist integers k, l ≥1 such that vkw = wxl.
(b) There exist r, s ∈∗and integers m, n ≥1, p ≥0 such that v =
(rs)m, w = (rs)pr, and x = (sr)n.
35. Find all solutions in words x1, x2, x3 to the system of equations
x1x2x3 = x2x3x1 = x3x1x2.
36. Let  = {0, 1}. Let h : ∗→∗be the morphism deﬁned by h(0) = 0,
h(1) = 10, and let k : ∗→∗be the morphism deﬁned by k(0) = 0,
k(1) = 01. Prove that h(0w) = k(w0) for all ﬁnite words w.
37. Show that if x, y ∈∗with xy ̸= yx, then xyxxy is primitive.
38. Show that for all words w ∈{0, 1}∗, either w0 or w1 is primitive (or
both).
39. Consider the equation in words xXy = z2. Describe all solutions to this
equation. Hint: there are separate cases depending on whether |x|, |y| are
even or odd.

2.10 Notes on Chapter 2
47
40. Let x, y, z be strings. In the Lyndon–Sch¨utzenberger theorems, we proved
a necessary and sufﬁcient condition for xy = yx and xy = yz. Find similar
necessary and sufﬁcient conditions for
(a) xy = yRx;
(b) xy = yRz.
2.8 Projects
1. Investigate ω-languages, that is, those languages that consist of inﬁnite
words. Ordinary ﬁnite automata accept an ω-word if, according to one
criterion, the path labeled by the word passes through an accepting state
inﬁnitely often. Start with the survey by Thomas [1991] and the book of
Perrin and Pin [2003].
2. Look into efﬁcient algorithms for determining if a word has repetitions
of various kinds (overlaps, squares, cubes, etc.). Start with the papers of
Crochemore [1981] and Kfoury [1988].
3. Find out more about the Burnside problem mentioned in Section 2.6 and
how the Thue–Morseword 0110100110010110 · · · played a role in its
solution. Start with the book of Adian [1979] and the survey paper of Gupta
[1989].
2.9 Research problems
1. An abelian square is a word of the form xx′, |x| = |x′| > 0, with x′
a permutation of x. An example in English is reappear. Similarly, an
abelian cube is a word of the form xx′x′′, |x| = |x′| = |x′′| > 0, with x′
and x′′ both permutations of x. An example in English is deeded.
(a) Does there exist an inﬁnite word over a three-letter alphabet avoiding
abelian squares xx′ with |x| = |x′| ≥2?
(b) Does there exist an inﬁnite word over a two-letter alphabet avoiding
abelian cubes xx′x′′ with |x| = |x′| = |x′′| ≥2?
2. Find a simple characterization of the lexicographically least squarefree
word over a three-letter alphabet.
2.10 Notes on Chapter 2
Combinatorics on words is a vast subject that is becoming increasingly popular,
especially in Europe. For an introduction to this area, see Lothaire [1983, 2002].

48
2
Combinatorics on words
For the historical roots of combinatorics on words, see Berstel and Perrin
[2007].
2.1 The book of Perrin and Pin [2003] is a very good source of information
on inﬁnite words.
2.2 Harju and Karhum¨aki [1997] is an excellent discussion of morphisms.
2.3 For Theorems 2.3.2 and 2.3.3, see Lyndon and Sch¨utzenberger [1962].
Theorems 2.3.5 is a new generalization of a classical theorem of Fine
and Wilf [1965].
Theorem 2.3.6 is a new generalization of Theorem 2.3.3.
Some of the material of this section was taken essentially verba-
tim from Allouche and Shallit [2003], with permission of Cambridge
University Press.
2.4 Theorem 2.4.2 is due to Shyr and Thierrin [1977].
Theorem 2.4.5 is from Lyndon and Sch¨utzenberger [1962]. Our proof
is based on Harju and Nowotka [2004].
2.5 Much of the material of this section was taken essentially verbatim from
Allouche and Shallit [2003], with permission of Cambridge University
Press.
The area of repetitions on words is a huge one, with many dozens
of papers; see Allouche and Shallit [2003] for an extensive annotated
bibliography. Thue [1906] and Thue [1912] are the earliest papers on the
subject. For an English translation of Thue’s work, see Berstel [1995].
2.6 For more on the strange and wonderful properties of the Thue–Morse
sequence, see the survey paper of Allouche and Shallit [1999].
For the Tarry–Escottproblem, see Wright [1959] and Borwein and
Ingalls [1994].
The inﬁnite product (2.2) is due to Woods [1978]. The proof that it
equals
√
2/2 is due to Jean-Paul Allouche and is from Allouche and
Shallit [1999].
For the work of Max Euwe, see Euwe [1929] or the Web site
www.maxeuwe.nl. For the ofﬁcial rules of chess, see the FIDE Web
site,www.fide.com. For the Burnside problem, see Adian [1979].

3
Finite automata and regular languages
The ﬁnite automaton is one of the simplest and most fundamental computing
models. You are almost certainly familiar with this model from your ﬁrst course
in the theory of computing, but if not, you may want to review the material in
Sections 1.3–1.4.
In this chapter we reexamine the theory of ﬁnite automata from a more
advanced standpoint. In particular, we prove the very important Myhill–Nerode
theorem in Section 3.9.
We begin with some generalizations of the ﬁnite automaton model.
3.1 Moore and Mealy machines
In most introductory courses on automata theory, ﬁnite automata are viewed
as language recognizers, not as computers of functions. A deterministic ﬁnite
automaton (DFA), for example, takes a string as input and either accepts or
rejects it. Of course, we can view a DFA as computing a function f : ∗→
{0, 1}, where 0 represents rejection and 1 acceptance, but there are other ways
to associate outputs with machines.
In this section, we introduce two simple models of ﬁnite-state machines
with output, called Moore and Mealy machines. A Moore machine has outputs
associated with its states, while a Mealy machine has outputs associated with
its transitions.
We can use transition diagrams to represent both Moore and Mealy ma-
chines. In a Moore machine, a state labeled q/b indicates that when state q is
entered, the output b is produced. In a Mealy machine, a transition labeled a/b
indicates that when this transition is taken on input symbol a, the output b is
produced. The output corresponding to a given input string is the concatenation
of all the outputs produced successively by the machine.
Let us consider some examples.
49

50
3
Finite automata and regular languages
q2
0/0
q1
0
1
0
1
1
q
/1
/2
0
Figure 3.1: Example of a Moore machine
Example 3.1.1. Given n represented in base 2, the Moore machine in Figure 3.1
computes r mod 3 for all numbers r whose base-2 representation is a preﬁx of
that for n.
On input 101001, which is the base-2 representation of the number 41, we
successively enter states q0, q1, q2, q2, q1, q2, q2 and output 0122122.
We now consider examples of Mealy machines.
Example 3.1.2. The simplest nontrivial Mealy machine is illustrated in Fig-
ure 3.2. It takes an input w ∈{0, 1}∗and complements it, changing each 0 to 1
and each 1 to 0.
Example 3.1.3. Our next example is a little less trivial. The Mealy machine in
Figure 3.3 takes as input n expressed in base 2, starting with the least signiﬁcant
digit. The output is n + 1 in base 2. Note that if the input is all 1s, we must
also include a trailing 0 to get the correct output. This is necessary because
if n = 2k −1, then the binary representation of n + 1 is 1 bit longer than the
binary representation of n.
We now turn to formal deﬁnitions of Moore and Mealy machines. Both
machines are 6-tuples of the form M = (Q, , , δ, τ, q0), where  is the
nonempty output alphabet and τ is the output function. The difference is that in
a Moore machine we have τ : Q →, while in a Mealy machine τ : Q ×  →
.
On input x = a1a2 · · · an, a Moore machine M enters the states
q0, δ(q0, a1), δ(q0, a1a2), . . . , δ(q0, a1a2 · · · an)
q0
0/1
1/0
Figure 3.2: Example of a Mealy machine computing the complementary string

3.1 Moore and Mealy machines
51
1/0
0/1
0/0
1/1
q0
1
q
Figure 3.3: Example of a Mealy machine for incrementing in binary
and outputs
y = τ(q0) τ(δ(q0, a1)) τ(δ(q0, a1a2)) · · · τ(δ(q0, a1a2 · · · an)).
We write TM(x) = y. A Mealy machine M outputs
z = τ(q0, a1) τ(δ(q0, a1), a2) · · · τ(δ(q0, a1a2 · · · an−1), an).
We write UM(x) = z. Note that on an input of length n, a Moore machine
outputs a string of length n + 1 and a Mealy machine outputs a string of length
n. This is because a Moore machine always provides an output associated with
the initial state. In some sense, this output is not meaningful because it does
not depend at all on the input.
Despite this difference, we can deﬁne a notion of equivalence for Moore and
Mealy machines: we say a Moore machine M is equivalent to a Mealy machine
M′ if their input–outputbehavior is identical except that the ﬁrst output of the
Moore machine is disregarded. More formally, we say M is equivalent to M′
if, for all x ∈∗, we have TM(x) = TM(ϵ)UM′(x).
Theorem 3.1.4. Let M = (Q, , , δ, τ, q0) be a Moore machine. Then there
exists an equivalent Mealy machine M′ with the same number of states.
Proof Idea. The idea is to deﬁne the output function τ ′ of a simulating machine
so that its value depends on the output from the state that is reached after the
transition is made.
Proof. Formally, let M′ = (Q, , , δ, τ ′, q0), where τ ′(q, b) = τ(δ(q, b)).
If the input to M is x = a1a2 · · · an, then M′ outputs
τ ′(q0, a1) τ ′(δ(q0, a1), a2) τ ′(δ(q0, a1a2), a3) · · · τ ′(δ(q0, a1a2 · · · an−1), an).
But by our deﬁnition of τ ′, this is
τ(δ(q0, a1)) τ(δ(δ(q0, a1), a2)) · · · τ(δ(δ(q0, a1a2 · · · an−1), an))
= τ(δ(q0, a1))τ(δ(q0, a1a2)) · · · τ(δ(q0, a1a2 · · · an)).
It follows that TM(x) = TM(ϵ)UM′(x).

52
3
Finite automata and regular languages
Theorem 3.1.5. Let M′ = (Q′, , , δ′, τ ′, q′
0) be a Mealy machine. Then
there exists an equivalent Moore machine M with |Q′||| states.
Proof Idea. The idea is to store in the state of the simulating Moore machine
M both the state of the original Mealy machine M′ and the symbol that M′
would have output in its immediately previous transition. Thus states of M are
chosen from Q′ × . (For the ﬁrst state, there is no previous transition, so we
simply arbitrarily choose an element c from .)
Proof. Formally, let M = (Q, , , δ, τ, q0), where
• Q = Q′ × ;
• δ([q, b], a) = [δ′(q, a), τ ′(q, a)] for all q ∈Q′, b ∈, a ∈;
• q0 = [q′
0, c] for some arbitrary ﬁxed c ∈.
• τ([q, b]) = b for all q ∈Q′, b ∈.
Now on input x = a1a2 · · · an, M′ enters the states
q′
0, δ′(q′
0, a1), . . . , δ′(q′
0, a1a2 · · · an)
and outputs
τ ′(q′
0, a1)τ ′(δ′(q′
0, a1), a2) · · · τ ′(δ′(q′
0, a1 · · · an−1), an).
On the other hand, M enters the states
[q′
0, c], [δ′(q′
0, a1), τ ′(q′
0, a1)], [δ′(q′
0, a1a2), τ ′(δ′(q′
0, a1), a2)], . . . ,
[δ′(q′
0, a1 · · · an), τ ′(δ′(q′
0, a1 · · · an−1), an)]
and outputs
cτ ′(q′
0, a1)τ ′(δ′(q′
0, a1), a2) · · · τ ′(δ′(q′
0, a1 · · · an−1), an).
It follows that TM(x) = cUM′(x).
In Section 3.5 we consider a generalization of Mealy machines, the ﬁnite-
state transducer.
3.2 Quotients
In this section we introduce a new operation on languages, the quotient. Let
L1, L2 ⊆∗. We deﬁne
L1/L2 = {x ∈∗: there exists y ∈L2 such that xy ∈L1}.
Quotient is a kind of inverse to concatenation (but see Exercise 2).

3.2 Quotients
53
Example 3.2.1. Let L1 = a+bc+, L2 = bc+, and L3 = c+. Then L1/L2 = a+
and L1/L3 = a+bc∗.
Example 3.2.2. Let L = {an2 : n ≥0}. Then L/L = a(aa)∗+ (aaaa)∗. For
we have
L/L = {x : there exists y ∈L such that xy ∈L}
= {at : ∃m, n with t + m2 = n2}
= {an2−m2 : 0 ≤m ≤n}.
I now claim that t can be written in the form n2 −m2 for some 0 ≤m ≤n iff t is
of the form 2k + 1 or 4k for some k ≥0. For if t = n2 −m2, then both n and m
are of either the same parity, or different parity. If they are the same parity, then
t = (2n′)2 −(2m′)2 ≡0 (mod 4) or t = (2n′ + 1)2 −(2m′ + 1)2 ≡0 (mod 4).
If n and m are of different parity, then t = (2n′)2 −(2m′ + 1)2 ≡1 (mod 2) or
t = (2n′ + 1)2 −(2m′)2 ≡1 (mod 2).
On the other hand, if t = 2k + 1, then we can choose n = k + 1, m = k to
get t = n2 −m2. If t = 0, then we choose n = 0, k = 0. Otherwise, if t = 4k
with k ≥1, we choose n = k + 1, m = k −1, so n2 −m2 = t.
Theorem 3.2.3. Let L, R ⊆∗and suppose R is regular. Then R/L is regular.
Proof Idea. Let R, L ⊆∗and let M = (Q, , δ, q0, F) be a DFA accepting
R. Our goal is to create a machine M′ accepting R/L. On input x, the machine
M′ must somehow determine if there exists a y such that if M were to process
the symbols of y starting in the state δ(q0, x), it would arrive in a ﬁnal state.
Proof. Let M′ = (Q, , δ, q0, F ′), where
F ′ = {q ∈Q : there exists y ∈L such that δ(q, y) ∈F}.
Notice that M′ is exactly the same as M, except that we have changed the set
of ﬁnal states. Now we have
x ∈L(M′) ⇐⇒δ(q0, x) ∈F ′
⇐⇒δ(q0, x) = q and there exists y ∈L such that δ(q, y) ∈F
⇐⇒there exists y ∈L such that δ(q0, xy) ∈F
⇐⇒there exists y ∈L such that xy ∈R
⇐⇒x ∈R/L.
Note that Theorem 3.2.3 is not constructive in the sense that no algorithm
is provided to compute F ′. In fact, no such algorithm is possible in general,

54
3
Finite automata and regular languages
since L is arbitrary, and could be nonrecursive. You should not let this bother
you—much of mathematics is nonconstructive.
Under certain circumstances, M′ becomes effectively constructible. For
example, if L itself is speciﬁed by giving a DFA accepting it, then there is an
easy algorithm for computing F ′ (see Exercise 36).
Now let us look at two applications of the quotient operation.
Example 3.2.4. We consider the effect of removing “trailing zeros” from
words in a language. Let  be an alphabet containing the symbol 0, and let
L ⊆∗.
By removing trailing zeros from words of L, we mean the language
rtz(L) = {x ∈∗( −{0}) ∪{ϵ} : there exists i ≥0 with x0i ∈L}.
If L is regular, must rtz(L) necessarily be regular?
With quotient we can easily solve this problem. We claim that
rtz(L) = (L/0∗) ∩(∗( −{0}) ∪{ϵ}).
Hence if L is regular, so is rtz(L).
Similarly, we can remove leading zeros with
rlz(L) = rtz(LR)R.
Example 3.2.5. Recall the deﬁnition of the preﬁx language from Section 1.2:
Pref(L) = {x ∈∗: there exists y ∈L such that x is a preﬁx of y}.
If L is regular, need Pref(L) be regular? Noting that Pref(L) = L/∗, the
answer is yes.
3.3 Morphisms and substitutions
In this section we study two very useful transformations on languages, namely
morphisms and substitutions.
Recall from Section 2.2 that a homomorphism, or just morphism for short,
is a map that sends a letter to a string and is then extended to arbitrary strings
by concatenation. Alternatively, we say that h : ∗→∗is a morphism if
h(xy) = h(x)h(y) for all x, y ∈∗. Note that h(xϵ) = h(x)h(ϵ), and so it
follows that h(ϵ) = ϵ.
A morphism is then extended to a language L as follows: h(L) :=

x∈L{h(x)}.

3.3 Morphisms and substitutions
55
Theorem 3.3.1. For all languages L, L1, L2 ⊆∗and morphisms h : ∗→
∗, we have
(a) h(L1L2) = h(L1)h(L2);
(b) h(L1 ∪L2) = h(L1) ∪h(L2);
(c) h(L∗) = h(L)∗.
Proof. We omit the proof as a more general result is proved in Theorem 3.3.5
later.
Example 3.3.2. Deﬁne h(0) = a and h(1) = ba. Then we have h(010) = abaa
and h(0∗1) = a∗ba.
Common Error 3.3.3. Students sometimes try to deﬁne “morphisms”as fol-
lows: h(aa) = ab. This is not a morphism, because for whatever choice of h(a)
we make, the identity h(aa) = h(a)h(a) fails.
A substitution is a map s : ∗→2∗that sends each letter a ∈ to a
language La and obeys the rules s(ϵ) = {ϵ}, and s(xy) = s(x)s(y) for all x, y ∈
∗. We extend s to languages as we did for morphisms: s(L) := 
x∈L s(x).
Example 3.3.4. Deﬁne s(0) = {a, ab}∗and s(1) = (cd)∗. Then s(101) =
(cd)∗(a + ab)∗(cd)∗.
The previous example is an example of substitution by regular languages,
since each letter is mapped to a regular language.
Although, strictly speaking, a morphism is not a substitution (since a mor-
phism is word-valued, whereas a substitution is language-valued), we may
identify the word w with the language {w}. Thus if we prove that the class of
regular languages is closed under substitution by regular languages, we will
also have proved that this class is closed under morphism.
Theorem 3.3.5. The class of regular languages is closed under substitution by
regular languages.
Proof.
Let s be a substitution. We ﬁrst prove that, for languages L1, L2, L,
we have
(a) s(L1 ∪L2) = s(L1) ∪s(L2);
(b) s(L1L2) = s(L1)s(L2);
(c) s(L∗) = s(L)∗.

56
3
Finite automata and regular languages
For (a), we prove more generally that for any index set I, ﬁnite or inﬁnite,
we have s(
i∈I Li) = 
i∈I s(Li). We have
s

i∈I
Li

=

x∈
i∈I Li
s(x)
=

i∈I

x∈Li
s(x)
=

i∈I
s(Li).
For (b), we have
s(L1L2) = s({xy : x ∈L1, y ∈L2})
=

x∈L1, y∈L2
s(xy)
=

x∈L1, y∈L2
s(x)s(y)
=
 
x∈L1
s(x)
 

y∈L2
s(y)


= s(L1)s(L2).
For (c), we ﬁrst show by induction on n that s(Ln) = s(L)n. For n = 0, this
follows since L0 = {ϵ}. Now assume that the result is true for n < N; we prove
it for n = N. We then have
s(LN) = s(LN−1L) = s(LN−1)s(L)
by part (b). By induction s(LN−1) = s(L)N−1. Then s(LN) = s(L)N−1s(L) =
s(L)N, as was to be shown.
Now we have
s(L∗) = s

i≥0
Li

=

i≥0
s(Li) =

i≥0
s(L)i = s(L)∗.
This completes the proof of part (c).
Now we can complete the proof of the theorem. If L ⊆∗is regular, then
it can be represented as a regular expression r. We prove by induction on n, the
number of operators in r, that s(L) is regular. (Here we count all occurrences
of +, ∗, and implicit occurrences of concatenation.)
If r has no operators, then L equals either {a} for some a ∈, or {ϵ}, or ∅.
It is easy to see that s(L) is regular in each of these cases.

3.3 Morphisms and substitutions
57
Now assume the result is true for all n < N; we prove it for n = N. Assume
r has N operators. Then L can be written as L1L2, or L1 ∪L2, or L∗
1, where
L1, L2 are regular languages that can be represented by regular expressions
with ≤N −1 operators. Then s(L1 ∪L2) = s(L1) ∪s(L2) by part (a), which
proves regularity by induction. Similarly, s(L1L2) = s(L1)s(L2) by part (b),
which is regular by induction. Finally, s(L∗
1) = s(L1)∗by part (c), which is
regular by induction.
Example 3.3.6. Suppose L ⊆{a, b}∗is a regular language. Let us show that
the language formed by inserting the letter c in all possible ways into strings of
L is still regular. Deﬁne the substitution s(a) = c∗ac∗and s(b) = c∗bc∗. Then
if L does not contain ϵ, then s(L) is the desired language, while if L contains
ϵ, then s(L) ∪c∗is the desired language. In both cases the result is regular.
We now turn to a new type of transformation of languages, the inverse
morphism. If h : ∗→∗is a morphism, and L ⊆∗, then we deﬁne
h−1(L) = {x ∈∗: h(x) ∈L}.
The map h−1 can be viewed as a sort of inverse of h (see Exercise 6).
Example 3.3.7. In the previous example, we inserted the letter c in all possi-
ble ways into strings of L. We can accomplish the same result using inverse
morphisms. Deﬁne h(a) = a, h(b) = b, and h(c) = ϵ. Then h−1(L) achieves
our goal. For example, h−1({aba}) = c∗ac∗bc∗ac∗.
Example 3.3.8. More generally, we can use inverse morphism to give a formal
deﬁnition of a new operation on languages, the shufﬂe. The shufﬂe of two words
x and w (not necessarily of the same length) consists of all words obtained by
interleaving the letters as in shufﬂing a deck of cards. (Note this is not the same
as the perfect shufﬂe deﬁned in Section 1.2.) For example, shuff(ab, cd) =
{abcd, acbd, acdb, cabd, cadb, cdab}. The shufﬂe of two languages L1, and
L2 is deﬁned to be
shuff(L1, L2) =

x∈L1, y∈L2
shuff(x, y).
We can deﬁne the operation shuff in terms of morphisms and inverse mor-
phisms, as follows. Suppose L1, L2 ⊆∗for some alphabet . For each letter
a ∈, construct a new letter a′ and let ′ = {a′ : a ∈}. Deﬁne morphisms
h by h(a) = h(a′) = a for each a ∈; h1(a) = a and h1(a′) = ϵ for each
a ∈; and h2(a) = ϵ and h2(a′) = a for each a ∈. Then we have
shuff(L1, L2) = h(h−1
1 (L1) ∩h−1
2 (L2)).

58
3
Finite automata and regular languages
We now prove that the class of regular languages is closed under inverse
morphism.
Theorem 3.3.9. If L ⊆∗is regular, and h : ∗→∗is a morphism, then
h−1(L) is regular.
Proof Idea. Consider trying to build a DFA M′ for h−1(L), based on the DFA
M for L. What must M′ do? On input x, M′ must compute h(x) and determine
if it is in L, accepting x if and only if M accepts h(x). The easiest way to do
that is to “rewire”the DFA M′ so that on input x, M′ simulates M on h(x).
Think of it this way: L is a language of French words, and h is a map that
takes an English word to its French translation. How can we accept h−1(L)?
On input an English word, we apply h to get the equivalent French word, and
then see if the result is in L.
Proof.
Let M = (Q, , δ, q0, F) be a DFA accepting L. We create a DFA
M′ = (Q′, , δ′, q′
0, F ′), accepting h−1(L). We deﬁne
• Q′ := Q;
• δ′(q, a) := δ(q, h(a));
• q′
0 := q0;
• F ′ := F.
We now prove by induction on |x| that δ′(q, x) = δ(q, h(x)). The base case
is |x| = 0; that is, x = ϵ. Then δ′(q, x) = q = δ(q, h(x)).
Otherwise, assume the result is true for all x with |x| < n; we prove
it for |x| = n. Let x = ya, where a ∈. Then δ(q, h(x)) = δ(q, h(ya)) =
δ(δ(q, h(y)), h(a)) = δ(δ′(q, y), h(a)) = δ′(δ′(q, y), a) = δ′(q, ya)=δ′(q, x),
as desired.
It now follows that δ′(q′
0, x) ∈F ′ if and only if δ(q0, h(x)) ∈F; in other
words, x ∈L(M′) if and only if h(x) ∈L(M). Thus, L(M′) = h−1(L(M)).
3.4 Advanced closure properties of regular languages
In this section we introduce two new operations on regular languages and
show that the class of regular languages is closed under them. In both cases,
nondeterminism plays an essential role in the proofs.
We start with 1
2L. If L ⊆∗, we deﬁne
1
2L = {x ∈∗: there exists y ∈∗with |y| = |x| such that xy ∈L}.
Thus, 1
2L consists of the ﬁrst halves of even-length strings of L.
Theorem 3.4.1. If L is regular, then so is 1
2L.

3.4 Advanced closure properties of regular languages
59
Proof Idea.
There are a number of different ways to approach the solution.
Here is one. On input x, if we knew what state q we would be in after reading all
the symbols of x, we could simply step forward in tandem from q and q0. (From
q, of course, we would have to move forward in all possible ways, because the
only thing we demand about y is that |x| = |y|.) Then we would accept if we
got from q0 to q on x and from q to a state of F on the guessed symbols of y.
Of course, we do not know q, but using nondeterminism we can guess it and
check. The “thereexists”in the deﬁnition of 1
2L is a hint that nondeterminism
will be useful.
This suggests starting with a DFA M = (Q, , δ, q0, F) for L and con-
verting it into a nondeterministic ﬁnite automaton with ϵ-transitions (NFA-ϵ)
M′ = (Q′, , δ′, q′
0, F ′) for 1
2L. In M′, states will be triples; the ﬁrst element
records the guessed state q and does not change once it is initially recorded,
the second element records what state we are in after having processed some
preﬁx of the input x, starting from state q0, and the third element records what
state we are in after having processed some preﬁx of the guessed y, starting
from q.
Proof. Formally, we deﬁne Q′ = {q′
0} ∪Q × Q × Q and F ′ = {[q, q, p] :
p ∈F}. We also deﬁne
δ′(q′
0, ϵ) = {[q, q0, q] : q ∈Q};
δ′([q, p, r], a) = {[q, δ(p, a), δ(r, b)] : b ∈}.
An easy induction on |x| now shows that [q, p, r] ∈δ′(q′
0, x) if and only if
there exists y ∈∗, |x| = |y|, such that δ(q0, x) = p and δ(q, y) = r. Thus,
δ′(q′
0, x) ∩F ′ ̸= ∅
⇕
∃y ∈∗, |x| = |y|,
with δ(q0, x) = q, δ(q, y) ∈F
⇕
∃y ∈∗, |x| = |y|,
with δ(q0, xy) ∈F
⇕
∃y ∈∗, |x| = |y|,
with xy ∈L.
It follows that L(M′) = 1
2L.
There is a different approach to the preceding proof. Instead of guessing
q and moving forward from it, we could guess the appropriate ﬁnal state and
move backwards. This approach is left as Exercise 38.
Now, let us turn to another operation on formal languages, the cycle operation
cyc. Roughly speaking, cyc sends every string to the set of all its cyclic shifts,

60
3
Finite automata and regular languages
q
q
f
q
0
x
x


In M
q
q
x
x

ϵ
q0
′
ϵ
qf
q0
In M′
Figure 3.4: An NFA-ϵ for cyc(L)
or conjugates, as deﬁned in Section 2.4. For a language L, we deﬁne
cyc(L) := {x1x2 : x2x1 ∈L}.
Example 3.4.2. Suppose L = 0∗1∗. Then cyc(L) = 0∗1∗0∗+ 1∗0∗1∗.
It turns out that the class of regular languages is closed under the cyc
operation, as the following theorem shows.
Theorem 3.4.3. If L is regular, then so is cyc(L).
Proof Idea.
Let M = (Q, , δ, q0, F) be a DFA accepting L. We construct
an NFA-ϵ M′ that accepts cyc(L). The idea is to “guess”the state the DFA M
would be in after reading all the symbols of x2, then process the symbols x1,
verify that a ﬁnal state has been reached, and then continue with the symbols
of x2, returning to the ﬁrst state. This is illustrated by Figure 3.4.
Figure 3.4 is a little misleading, because we will actually need the states of
our machine for cyc(L) to be triples: if [p, q, a] ∈Q′, then
• p records the current state in the simulation of M;
• q records the “guessed”initial state; and
• a records whether or not a ﬁnal state has been reached.
Proof. More formally, let M′ = (Q′, , δ′, q′
0, F ′), where
Q′ = {q′
0} ∪Q × Q × {y, n}.
The transition function δ′ is deﬁned as follows:
δ′(q′
0, ϵ) = {[q, q, n] : q ∈Q} (get started)
δ′([p, q, n], a) = {[δ(p, a), q, n]} (simulate moves of M)
δ′([p, q, n], ϵ) = {[q0, q, y]} for all p ∈F
(allow transition to y if ﬁnal state is encountered)
δ′([p, q, y], a) = {[δ(p, a), q, y]} (simulate moves of M).

3.5 Transducers
61
q0
ϵ
ϵ
x
x


]
 q,q,n
[
]
p,q,n
[
q,q,y
[
]
]
q  ,q,y
0
[
′
Figure 3.5: An acceptance path for x2x1
Finally, we deﬁne F ′ = {[q, q, y] : q ∈Q}. We now claim that L(M′) =
cyc(L).
First we show cyc(L) ⊆L(M′). Suppose x2x1 ∈L. Then x2x1 is accepted
by M. Then, as in Figure 3.4, M starts in state q0, reads the symbols of x2 until
it reaches a state q, and then reads the symbols of x1 until it reaches a ﬁnal state
qf ∈F. In other words,
δ(q0, x2) = q;
δ(q, x1) = qf ∈F.
How does M′ behave on input x1x2? First, it starts in state q′
0. Then there is an
ϵ-transition to the state [q, q, n]. Then M′ simulates M on input x1, eventually
reaching [qf , q, n]. Since qf ∈F, there is an ϵ-transition to [q0, q, y]. Now
M′ simulates M on input x2, eventually reaching [q, q, y] at which point M′
accepts x1x2.
Next we show L(M′) ⊆cyc(L). If x ∈L(M′), then there must be a path
from q′
0 to [q, q, y] labeled with x. From q′
0, the machine M′ must enter a
state of the form [q, q, n] on an ϵ-transition. Since x is accepted, the path
must look like as shown in Figure 3.5 for some x1, x2 with x = x2x1. Hence,
δ(q, x2) = p for some p ∈F and δ(q0, x1) = q. It follows that δ(q0, x1x2) ∈F.
Hence, x1x2 ∈L. It follows that x2x1 ∈cyc(L), as desired.
3.5 Transducers
In Section 3.1, we studied the Mealy machine, which transformed inputs into
outputs. We now study a useful generalization of this idea, called the ﬁnite-state
transducer, or just transducer for short.
A ﬁnite-state transducer is similar to a Mealy machine, but differs in the
following ways:
• It is nondeterministic,
• It has ﬁnal states,
• Transitions are labeled with x/y, where x, y are strings, not necessarily just
single symbols. Here x is an input string, and y is an output string.

62
3
Finite automata and regular languages
q2
q1
q0
1/1
0/0
1/0
0/1
ϵ/1
Figure 3.6: Transducer for incrementing in binary
Example 3.5.1. Recall Example 3.1.3, where we constructed a Mealy ma-
chine to compute the base-2 representation of n + 1, given the base-2 repre-
sentation of n, starting with the least signiﬁcant digit. That example was not
completely satisfactory, because it did not behave properly if the input was of
the form 11 · · · 1. With the transducer in Figure 3.6, however, we can correct this
problem.
Example 3.5.2. Recall Example 3.2.4, where we showed how to remove lead-
ing zeros using a rather complicated expression involving quotient, intersection,
and reversal. With the transducer in Figure 3.7, however, we can easily solve
this problem (over the alphabet {0, 1}).
Now let us give the formal deﬁnition of a transducer. A ﬁnite-state transducer
is a 6-tuple T = (Q, , , S, q0, F), where Q is a ﬁnite nonempty set of states,
 is the input alphabet,  is the output alphabet, q0 is the start state, and F is the
set of ﬁnal states. The transition set S is a ﬁnite subset of Q × ∗× ∗× Q.
The intent is that if (p, x, y, q) ∈S, then if T is in state p, and reads the
string x, it has the option of entering state q and outputting y. An accepting
computation of T is a list of elements of S of the form
(p0, x0, y0, p1), (p1, x1, y1, p2), . . . , (pi, xi, yi, pi+1),
where p0 = q0 and pi+1 ∈F. In that case, the transducer maps the input string
x = x0x1 · · · xi to y = y0y1 · · · yi, and we write x →T y. By convention, there
is always an implicit transition of the form (p, ϵ, ϵ, p) for every state p. The
q1
1/1
q0
0/ϵ
1/1
0/0
Figure 3.7: Transducer for removing leading zeros

3.5 Transducers
63
set of all pairs {(x, y) : x →T y} is called the rational relation computed
by T .
Now we deﬁne how a transducer acts on a string x and a language L. We
deﬁne
T (x) = {y ∈∗: x →T y},
and
T (L) =

x∈L
T (x).
As we have seen, a transducer T = (Q, , , q0, F, S) can be viewed as
computing a transduction, that is, a function from 2∗to 2∗. The follow-
ing theorem, sometimes called Nivat’s theorem, precisely characterizes those
functions.
Theorem 3.5.3 (Nivat). Suppose f is a map from 2∗to 2∗. Then f is com-
puted by a transducer T = (Q, , , q0, F, S) iff there exist a ﬁnite alphabet
, a regular language R ⊆∗, and morphisms g : ∗→∗and h : ∗→∗
such that f (z) = g(h−1(z) ∩R) for all z ∈∗.
Proof Idea. The basic idea is to replace each transition in T with a new symbol.
We then mimic the reading of an input z by performing an inverse morphism
(to factorize z into elements of ) and intersecting with a regular language R
(to enforce the condition that the factorization correspond with a path through
T ). Finally, we get the output by applying another morphism. For the other
direction, we just reverse this construction.
Proof. Suppose f is computed by the transducer T . Let  be a new alphabet
with a letter ax,y for each transition labeled x/y in S. Now deﬁne R to be the
language accepted by the NFA M = (Q, , δ, q0, F), where each transition in
T , (p, x, y, q) ∈S, is replaced with δ(p, ax,y) = q in M. Finally, deﬁne the
morphisms g, h by g(ax,y) = y and h(ax,y) = x. We claim that for all z ∈,
we have T (z) = g(h−1(z) ∩R).
To see this, let w ∈T (z). Then we can factor z = z0z1 · · · zi, w =
w0w1 · · · wi, such that there is an accepting computation of T of the form
(p0, z0, w0, p1), (p1, z1, w1, p2), . . . , (pi, zi, wi, pi+1) with pi+1 ∈F. Then
h−1(z) and R both contain the string az0,w0 · · · azi,wi, so g(h−1(z) ∩R) con-
tains w. The other direction is similar.
For the other direction, we are given g, h, and R. Since R is regular, there
is a DFA M = (Q, , δ, q0, F) accepting R. Now create a transducer T =
(Q, , , q0, F, S), where S = {(p, h(a), g(a), q) : δ(p, a) = q}. We claim
that T (z) = g(h−1(z) ∩R). The details are left to the reader.

64
3
Finite automata and regular languages
Example 3.5.4. Consider the transducer depicted in Figure 3.7 that removes
leading zeroes. Deﬁne the new symbols b1, b2, b3 as follows:
b1 = a0,ϵ
b2 = a0,0
b3 = a1,1.
Then in the proof of Theorem 3.5.3 we get R = b∗
1 ∪b∗
1b3{b2, b3}∗. Now deﬁne
the morphisms g, h by
g(b1) = ϵ
h(b1) = 0
g(b2) = 0
h(b2) = 0
g(b3) = 1
h(b3) = 1.
Thus, to remove leading zeros from strings in a language L, we can use the
expression
g(h−1(L) ∩(b∗
1 ∪b∗
1b3{b2, b3}∗)).
Corollary 3.5.5. If L is regular, and T is a transducer, then T (L) is regular.
Proof.
We know that regular languages are closed under intersection, mor-
phism, and inverse morphism.
For the last result of this section, we prove that the composition of two
transductions is still a transduction.
Theorem 3.5.6. Suppose f : 2∗→2∗and g : 2∗→2∗are two transduc-
tions. Then so is f ◦g.
Proof.
First we observe that any transducer can be rewritten in a so-called
normal form, where every transition is of the form x/y with |x|, |y| ≤1. To see
c/c′
a/a b/b
c/c
q0
p1
T2
T1
b/
ba/c
a/a′ b/b′
q1
a/a
p0
b/b
ϵ
Figure 3.8: Transducers T1 and T2 to be composed

3.6 Two-way ﬁniteautomata
65
p1
p2
p0
b/b
b/ϵ
a/a
a/c
b/ϵ
T ′
1
Figure 3.9: The normal form T ′
1 for transducer T1
this, simply add extra states so that each transition inputs and outputs at most
one letter. Do this for both transducers and then employ the usual cross-product
construction for automata. If the ﬁrst transducer has a transition labeled x/y,
and the second has a transition labeled y/z, the new transducer has a transition
labeled x/z. (Note we must take into account the implicit transitions labeled
ϵ/ϵ from each state to itself.)
Example 3.5.7. Consider Figure 3.8, where two transducers T1 and T2 are
illustrated. Since T1 is not in the normal form, we convert it to a transducer T ′
1
in Figure 3.9. Then the cross-product construction gives the transducer T for
the composition of T ′
1 and T2 in Figure 3.10.
3.6 Two-way ﬁnite automata
As is proved in nearly every ﬁrst course on the theory of computation, the three
computing models
• deterministic ﬁnite automata (DFA)
• nondeterministic ﬁnite automata (NFA)
• nondeterministic ﬁnite automata with ϵ-transitions (NFA-ϵ)
[p0, q0]
[p2, q0]
[p0, q1]
[p2, q1]
b/ϵ
T
b/b
[p1, q0]
[p1, q1]
b/b′
b/ϵ
a/c′
a/a
b/
a/a′
b/
a/c
ϵ
ϵ
Figure 3.10: The transducer T for the composition T ′
1 ◦T2

66
3
Finite automata and regular languages
have equal computing power in the sense that they accept exactly the same
class of languages: the regular languages. Another way to say this is that the
class of regular languages is robust, by which we mean that small changes to
the model (such as adding nondeterminism, or ϵ-transitions, etc.) do not affect
the class of languages accepted.
In this section, we consider yet another variation on the computing model
of the ﬁnite automaton: we give the automaton the extra capability of moving
both left and right on its input tape. We call this new model a two-way ﬁnite
automaton, or 2DFA for short. As we will see, this extra power does not enlarge
the class of languages accepted.
A transition in a 2DFA is of the form δ(q, a) = (p, D), where D ∈{L, R}.
The meaning is that if the 2DFA is currently in state q scanning the symbol a,
then it ﬁrst changes to state p and then moves in direction D.
Recall that a DFA is said to accept an input string x if it is in a ﬁnal state
after processing the symbols of x. Similarly, we say a 2DFA accepts x if it
eventually walks off the right edge of the tape while in a ﬁnal state. However,
a 2DFA can exhibit more complex behavior than a DFA, because its ability to
move left and right means that it could
• walk off the left edge of the tape, which causes a crash; or
• enter an inﬁnite loop (e.g., a nonterminating computation where the 2DFA
never walks off either edge of the tape).
Formally, a 2DFA is a 5-tuple (Q, , δ, q0, F). Each of these components
is exactly the same as for a DFA, with the exception of δ, which now maps
Q ×  →Q × {L, R}. Here, L and R are codes that refer to a left and right
move, respectively.
As in the case of a DFA, we can represent a 2DFA by a transition diagram.
For example, consider a 2DFA
M = ({q0, q1}, {0, 1}, δ, q0, {q1}),
where δ is given as follows:
δ
0
1
q0
(q0, R)
(q1, R)
q1
(q1, L)
(q0, L)
This 2DFA can be represented by a transition diagram as shown in Fig-
ure 3.11.

3.6 Two-way ﬁniteautomata
67
0,R
1,R
0,L
1,L
1
q
0
q
Figure 3.11: Example of a 2DFA
It is not hard to see that this 2DFA accepts the regular language 0∗1. You
should verify that the following behaviors occur:
Input
Behavior
000
Walks off right edge in nonﬁnal state
001
Walks off right edge in ﬁnal state
100
Walks off left edge
111
Enters an inﬁnite loop
We now provide a way to record the current conﬁguration of a 2DFA.
A conﬁguration describes the input string, the current state, and the current
symbol being scanned. Formally, a conﬁguration is an element of ∗Q∗. The
meaning of the conﬁguration wqx is that the input to the 2DFA is wx and the
machine is currently in state q and is scanning the ﬁrst symbol of x (or has
fallen off the right edge of the tape if x = ϵ). (Using this convention, there is
no way to represent having walked off the left edge of the tape.)
Now it is possible to formally deﬁne the moves of a 2DFA. If the current
conﬁguration is wqax and there is a move δ(q, a) = (p, R), then the next
conﬁguration is wapx. We write wqax ⊢wapx. If the current conﬁguration
is waqbx and there is a move δ(q, b) = (p, L), then the next conﬁguration is
wpabx. We write waqbx ⊢wpabx. If the current conﬁguration is qbx and
there is a move δ(q, b) = (p, L), then the machine falls off the left edge of the
tape and crashes.
For example, for the 2DFA in Figure 3.11, we have
q0111 ⊢1q111 ⊢q0111 ⊢1q111 ⊢· · ·
and
q0001 ⊢0q001 ⊢00q01 ⊢001q1.
We write
∗⊢for the reﬂexive, transitive closure of ⊢. In other words, if
c
∗⊢c′, then there exists a sequence of conﬁgurations c1, c2, . . . , cr, r ≥1, such
that
c = c1 ⊢c2 ⊢c3 ⊢· · · ⊢cr = c′.

68
3
Finite automata and regular languages
1
2
...
s-1
s
q
q′
Figure 3.12: Path of a 2DFA
Finally, we are ready to give a formal deﬁnition of acceptance. We say a
string w is accepted by a 2DFA M = (Q, , δ, q0, F) if q0w
∗⊢wp for some
p ∈F. We deﬁne L(M), the language accepted by the 2DFA M, as follows:
L(M) = {w ∈∗: q0w
∗⊢wp for some p ∈F}.
We now prove that the class of languages accepted by 2DFAs is exactly the
regular languages. In other words, there is no gain in the ultimate computing
power if we allow the tape head to move both right and left (but see Exercise 22).
The idea behind the proof is a simulation. Clearly, a 2DFA can simulate an
ordinary DFA. Indeed, an ordinary DFA is a 2DFA in which every move is to
the right. The other direction, the simulation of a 2DFA by a DFA, is more
complicated.
Theorem 3.6.1. We can simulate the computations of a 2DFA with a DFA.
Proof Idea.
The idea behind the simulation is to keep track of the possible
behaviors of the 2DFA when it moves left from the current position. If the tape
head is currently scanning cell number s of the input, the only way cells 1, 2, . . .,
s −1 can affect the computation is if the tape head moves left, moves around
a bit, and exits back through cell s, having changed state. This is illustrated by
Figure 3.12.
We can therefore simulate the 2DFA if we keep track of a table, which tells
us for each state q ∈Q in what state we will eventually exit to the right. We also
need to know whether the input we have seen so far is potentially acceptable.
We build these tables, one for each possible nonempty input w, as follows:
the table τw is a map Q ∪{q} →Q ∪{0}. Here, q and 0 are two new symbols
assumed not to be in Q. If q ∈Q, then τw(q) := q′ if M, when started in state q
with w on the tape, scanning the rightmost symbol of w, ultimately falls off the
right edge of the tape in state q′, and 0 otherwise. We also deﬁne τw(q) := q′
if M, when started in state q0 with w on the tape, scanning the leftmost symbol
of w, ultimately falls off the right edge of the tape in state q′, and 0 otherwise.

3.6 Two-way ﬁniteautomata
69
There are only ﬁnitely many distinct tables, so the automaton we create has
ﬁnitely many states.
Proof. Deﬁne
τw(q) :=

q′,
if w = w′a, a ∈, and w′qa
∗⊢w′aq′;
0,
otherwise.
τw(q) :=

q′,
if w ∈+ and q0w
∗⊢wq′;
0,
otherwise.
We now prove the fundamental.
Lemma 3.6.2. If w, x are nonempty strings, a ∈, and τw = τx, then τwa =
τxa.
Proof.
Consider the behavior of the machine started in state q with tape
contents wa and xa, reading the rightmost symbol of each. The movement of
the machine depends only on δ(q, a), and so if we move right in either case,
we move right in both. If we move left, and enter state p, we encounter the last
symbol of w and x. Since τw = τx, we know when we reemerge, exiting to the
right, the machine will be in the same state whether w or x was on the tape. If
we do not reemerge, this must be due to falling off the left edge of the tape or
entering an inﬁnite loop, and τw(p) = τx(p) = 0. Then τwa(p) = τxa(p) = 0.
Thus, τwa(q) = τxa(q) for all q ∈Q.
It remains to consider q. If τw(q) = τx(q) = q′ ̸= 0, then on processing
either w or x we eventually fall off the right edge of the tape in state q′. If in
fact there is an a after either w or x, then if we move right on input a we will fall
off the edge of the tape in the same state. If we move left then by the preceding
argument we will have the same behavior for both inputs. If τw(q) = τx(q) = 0,
then we either fall off the left edge of the tape or enter an inﬁnite loop, and this
behavior is the same for wa and xa.
Now we build a simulating DFA for a 2DFA M = (Q, , δ, q0, F), where
δ : Q ×  →Q × {L, R}. Deﬁne M′ := (Q′, , δ′, q′
0, F ′), where
Q′ := {q′
0} ∪{τw : w ∈+};
δ′(q′
0, a) := τa for each a ∈;
δ′(τw, a) := τwa for w ∈+, a ∈;
F ′ := {τw : τw(q) ∈F} ∪{q′
0 : q0 ∈F}.
By the lemma, our deﬁnition δ′(τw, a) = τwa is consistent. An easy induction
now gives δ′(q′
0, w) = τw for all w ∈+.

70
3
Finite automata and regular languages
Now w ∈L(M) if and only if there exists a state p ∈F such that q0w
∗⊢wp,
if and only if τw(q) ∈F, iff δ′(q′
0, w) = τw and τw ∈F ′. It follows that w ∈
L(M) if and only if w ∈L(M′).
Remark. It may be worth noting that τw can be effectively computed. Simply
simulate M starting in the various states on the last symbol of w. Within |w||Q|
moves, one either falls off the left or right edge of the tape, or repeats a pair of
the form (state, input position), and so M is in an inﬁnite loop.
Remark. If M has n states, then M′ has ≤(n + 1)n+1 + 1 states.
Let us look at an example of Theorem 3.6.1. Apply the construction to the
2DFA in Figure 3.11. We ﬁnd
w
τw(q0)
τw(q1)
τw(q)
State name
0
q0
0
q0
B
1
q1
0
q1
C
00
q0
0
q0
B
01
q1
q1
q1
D
10
q0
0
0
E
11
q1
0
0
F
010
q0
0
0
E
011
q1
0
0
F
100
q0
0
0
E
101
q1
q1
0
G
110
q0
0
0
E
111
q1
0
0
F
1010
q0
0
0
E
1011
q1
0
0
F
which corresponds to the DFA shown in Figure 3.13, where A = q′
0:
Sometimes 2DFAs are useful for showing that certain languages are regular.
For example, deﬁne the following operation on languages:
root(L) := {w ∈∗: ∃n ≥1 such that wn ∈L}.
Theorem 3.6.3. If L is regular, then so is root(L).
Proof.
We build a 2DFA M′ accepting L′ := B root(L) E, where B, E are
new symbols not in the alphabet of L. The symbols B and E are delimiters
representing the beginning and end, respectively, of the input.
On input B w E, the 2DFA scans the input from left to right, simulating the
DFA M for L. If a ﬁnal state of M is reached on reading the endmarker E, we

3.7 The transformation automaton
71
A
B
C
E
F
D
G
1
0
0
1
0
1
0
1
0
1
1
0
1
0
Figure 3.13: An equivalent DFA
move off the right edge. Otherwise, we store the current state q of M, rewind
the read head to the start marker B, and continue processing the symbols of w
again, starting from state q in M. We keep doing this. If wi ∈L for some i,
we eventually accept and walk off the right edge of the input tape. Otherwise
we are in an inﬁnite loop and hence do not accept. It follows that L′ is regular.
Now deﬁne a morphism h mapping the symbols B, E to ϵ and every element of
 to itself. Then h(L′) = root(L), so root(L) is regular.
3.7 The transformation automaton
Given a DFA M = (Q, , δ, q0, F), there exists an associated automaton M′ =
(Q′, , δ′, q′
0, −) with interesting properties. Echoing the construction of the
previous section, the states of M′ are functions with domain and range Q. The
intent is that if M′ reaches state f after processing the input string x, then
f (q) gives the state that M would be in, had it started in state q and pro-
cessed x.
We call M′ the transformation automaton of M. Note that we do not specify
the set of ﬁnal states, which means that we are, in effect, deﬁning many different
automata, one for each choice of ﬁnal states.
Formally, here is the deﬁnition of the transformation automaton: the states
Q′ are QQ, which means all functions f : Q →Q. The initial state q′
0 is the
identity function i that maps each q ∈Q to itself. The transition function δ′ is
deﬁned as follows: δ′(f, a) = g if g(q) = δ(f (q), a) for all q ∈Q.
Theorem
3.7.1.
Let
M = (Q, , δ, q0, F)
be
a
DFA
and
M′ =
(Q′, , δ′, q′
0, −) its transformation automaton. Suppose δ′(q′
0, w) = f . Then
for each q ∈Q, f (q) = δ(q, w).

72
3
Finite automata and regular languages
0
0
0, 1
1
1
q2
q1
q0
Figure 3.14: A DFA for the language {x ∈{0, 1}∗: x contains no 11}
Proof. By induction on |w|. If |w| = 0, then w = ϵ. Then δ′(q′
0, ϵ) = q′
0 = i,
the identity function. So i(q) = q for all q, and δ(q, ϵ) = q.
Now assume the result is true for all |w| < n; we prove it for w = n. Write
w = xa. Then δ′(q′
0, xa) = δ′(δ′(q′
0, x), a). By induction if f = δ′(q′
0, x),
then f (q) = δ(q, x). By deﬁnition if g = δ′(f, a), then g(q) = δ(f (q), a). So
g(q) = δ(f (q), a) = δ(δ(q, x), a) = δ(q, xa) = δ(q, w).
One property of the transformation automaton that makes it useful is the
following.
Theorem 3.7.2. If δ′(q′
0, x) = f and δ′(q′
0, y) = g, then δ′(q′
0, xy) = g ◦f .
Proof.
We have δ(q, x) = f (q) for all q ∈Q and δ(r, y) = g(r) for all r ∈
Q. Now let r = f (q) to get δ(q, xy) = δ(δ(q, x), y) = δ(f (q), y) = δ(r, y) =
g(r) = g(f (q)).
Corollary 3.7.3. Let g = δ′(q′
0, xn). Then g = f n, where f = δ′(q′
0, x).
Example 3.7.4. An example may make this clearer. Figure 3.14 illustrates a
DFA that accepts the language of strings over {0, 1}∗that contain no occurrence
of the subword 11, and Figure 3.15 illustrates the associated transformation
automaton (with only the states reachable from the start state shown).
The transformation automaton can be used to solve many problems that
would otherwise be difﬁcult. The following example illustrates this.
Example 3.7.5. Consider the following transformation on languages. Given a
language L ⊆∗, consider the language
T (L) = {x ∈∗: x∗⊆L}.
Suppose L is regular. Then must T (L) be regular?
Using our familiar techniques, this might seem a difﬁcult problem, since
it would seem to require simulating a DFA for L on inﬁnitely many differ-
ent powers of x and ensuring that all are accepted. Using the transformation

3.8 Automata, graphs, and Boolean matrices
73
0
1
0
1
0
p2 : q1 →q2
p4 : q1 →q2
p1 : q1 →q0
q0 →q0
q2 →q2
q0 →q1
q2 →q2
q0 →q0
q2 →q2
q0 →q0
q2 →q2
q0 →q1
q2 →q2
q0 →q2
q2 →q2
p0 : q1 →q1
p5 : q1 →q 
p3 : q1 →q1
0
1
1
0
0,1
1
Figure 3.15: The corresponding transformation automaton
automaton, however, the solution is easy. Given a DFA M for L, create its
transformation automaton M′. Now let the set of ﬁnal states of M′ be
F ′ = {f ∈Q′ : f n(q0) ∈F for all n ≥0}.
Using Corollary 3.7.3, we see that M′ accepts T (L).
In Figure 3.15, for example, we would make the states p0, p1, p3, and p4
ﬁnal.
3.8 Automata, graphs, and Boolean matrices
In the previous section, we showed how to compute a function fw(q) that, for
each state q, determined the state a DFA A would be in after reading the input
word w. Now imagine that A is, instead, an NFA. Instead of being in a single
state after reading w, the NFA A could be in a set of states. How can we handle
this more complicated situation?
The transition diagram of an NFA or DFA can be viewed as a directed graph,
with a distinguished source vertex, the start state, and a set of distinguished sink
vertices, the ﬁnal states. Directed graphs, in turn, can be viewed as Boolean
matrices—that is, matrices with entries in {0, 1}—and doing so allows us to
easily solve problems that would otherwise be hard.
Boolean matrices are multiplied like ordinary matrices, except that instead
of addition and multiplication as the operations, we instead use the Boolean
operators ∨(or) and ∧(and).
Let us assume that the states of our automaton are {q0, q1, . . . , qn−1} for some
integer n ≥1 and that q0 is the start state. Given an NFA A = (Q, , δ, q0, F),
we can form the Boolean incidence matrix Ma for each a ∈ as follows: the
entry in row i and column j is 1 if qj ∈δ(qi, a), and 0 otherwise. To put it

74
3
Finite automata and regular languages
another way, (Ma)i,j = 1 if and only if in A’s transition diagram there is a
directed edge labeled a from state qi to state qj.
We can generalize this deﬁnition as follows: for a word w, the entry in row
i and column j of Mw is 1 if qj ∈δ(qi, w), and 0 otherwise. The next theorem
shows why matrices are useful.
Theorem 3.8.1. Let w = a1a2 · · · ai, i ≥0. Then Mw = Ma1Ma2 · · · Mai.
Proof.
By induction on i. (As usual, if w = ϵ, then Mw is the n × n identity
matrix.) The theorem is clearly true for i = 0, 1. Now assume that it is true for
i; we prove it for i + 1. Write w = xa with |w| = i + 1. Then |x| = i, and we
have Mw = Mxa. The entry in row i and column j of Mxa is, by deﬁnition,
1 if and only if qj ∈δ(qi, xa). But qj ∈δ(qi, xa) if and only if there exists a
state qk such that qk ∈δ(qi, x) and qj ∈δ(qk, a). This occurs iff there exists
k with a 1 in row i, column k of Mx, and a 1 in row k, column j of Ma. But
this occurs if and only if MxMa has a 1 in row i and column j. It follows that
Mxa = MxMa. By induction, Mx = Ma1Ma2 · · · Mai. Since a = ai+1, we get
Mw = Ma1Ma2 · · · MaiMai+1, as desired.
Corollary 3.8.2.
(a) For all words w, x, we have Mwx = MwMx.
(b) For all words w and integers n ≥0, we have Mwn = Mn
w.
With Boolean incidence matrices, we can turn the computations of an NFA
into an algebraic problem.
Theorem 3.8.3. Let A = (Q, , δ, q0, F) be an NFA with n states, and let Ma
be its associated matrices. Let u be the vector [1
n−1



0 0 · · · 0] and let v be the
vector where the ith entry is 1 if qi ∈F and 0 otherwise. Then A accepts w if
and only if uMwv = 1.
Proof.
The NFA A accepts w if and only if qj ∈δ(q0, w) for some qj ∈F,
if and only if Mw has 1 entry in row 0 and column j for some qj ∈F, if and
only if uMwv = 1.
Now let us look at an application of this approach. Consider an n-state NFA
A. Given a word w, how can we efﬁciently decide if A accepts wr for very
large r? We could, of course, simply simulate A on wr, but this would take
O(n2r|w|) steps by the usual approach. Can we do this more efﬁciently?
Using the incidence matrix approach solves this problem. Given w =
a1a2 · · · ai, we ﬁrst form the matrix M = Mw by multiplying together the

3.8 Automata, graphs, and Boolean matrices
75
matrices Ma1, . . . , Mai. Next, we raise M to the r power using the usual
“binarymethod.”In this approach, we use the identities M2i = (Mi)2 and
M2i+1 = M2iM to raise M to the r power using only O(log r) matrix multipli-
cations. Each multiplication uses O(ne) steps, where e is the optimal exponent
for matrix multiplication. (Currently, e = 2.376.) Finally, we look in row 0 to
see if there are any 1 entries in columns corresponding to ﬁnal states. Thus we
have shown
Theorem 3.8.4. For a word w, we can decide if an n-state NFA accepts wr in
O(ne(|w| + log r)) steps, where e is the exponent for matrix multiplication.
Here is another application of the matrix approach. Suppose we are given a
unary NFA A that accepts a ﬁnite language, and we want to enumerate all the
elements of L(A). How efﬁciently can we do this? The naive approach would
be to maintain a list L of the states of A, perhaps represented as a bit vector,
and update this list as we read additional symbols of input. If A has n states,
then the longest word accepted is of length ≤n −1. To update L after reading
each new symbol potentially requires a union of n sets, each with at most n
elements. Thus, the total cost is O(n3).
We can improve this using the incidence matrix approach. Suppose A has n
states. Take 2k to be the smallest power of 2 that is ≥n.
Now add 2k new states to A, labeled p0, p1, . . . , p2k−1. Let p0 be the new
initial state and add transitions from pi to pi+1 for 0 ≤i < p2k−1 and from
p2k−1 to q0. Call the result A′, and let M be the incidence matrix of A.
Now A accepts a word of length i if and only if there is a path of length i
from q0 to a ﬁnal state of A, if and only if there is a path of length 2k from pi
to a ﬁnal state of A′. So compute M2k through repeated squaring and check the
entry corresponding to the row for pi and the columns for the ﬁnal states of A′.
We must do this for each possible length, 0 through n −1, and so the total cost
is O(ne log n + n2).
We have proved
Theorem 3.8.5. If M is a unary NFA that accepts a ﬁnite language L, we
can enumerate the elements of L in O(ne log n) steps, where e is the optimal
exponent for matrix multiplication.
Up to now we have been considering Boolean matrix multiplication. But
ordinary matrix multiplication can also be useful, as the following theorem
shows.
Theorem 3.8.6. Let A be a DFA and Ma be the associated incidence matrix
corresponding to transitions on the symbol a. Let M = 	
a∈ Ma. Then the

76
3
Finite automata and regular languages
(ordinary) matrix power Mn has the property that the element in row i and
column j is the number of strings of length n that take A from state qi to state qj.
Proof. Similar to the proof of Theorem 3.8.1.
Example 3.8.7. Consider the DFA from Figure 3.14. The associated matrices
M0, M1, and M = M0 + M1 are then
M0 =


1
0
0
1
0
0
0
0
1


M1 =


0
1
0
0
0
1
0
0
1


M =


1
1
0
1
0
1
0
0
2

.
Thus, for example, to count the number of strings of length 4 accepted by
this DFA, we square M twice:
M2 =


2
1
1
1
1
2
0
0
4


M4 =


5
3
8
3
2
11
0
0
16

.
Thus, there are ﬁve strings of length four that take the DFA from state q0 to
q0 (namely, {0000, 0010, 0100, 1000, 1010}) and three strings from q0 to q1
(namely, {0001, 0101, 1001}) for a total of eight strings.
Let’s look at one more application of the Boolean matrix approach. Recall
that in Theorem 3.4.1, we proved that if L is regular, then so is
1
2L = {x ∈∗: there exists y ∈∗with |y| = |x| such that xy ∈L}.
Given a language L, deﬁne the analogous transformation log(L) as follows:
log(L) = {x ∈∗: there exists y ∈∗with |y| = 2|x| such that xy ∈L}.
We now prove
Proposition 3.8.8. If L is regular, then so is log(L).
Proof.
Given a DFA A = (Q, , δ, q0, F) for L, let Ma be the incidence
matrix corresponding to the input symbol a. Let M = 
a∈ Ma, the Boolean
“or”of all the matrices Ma. Then M has the property that there is an entry in
row i and column j if and only if there is a transition in A on some symbol
from qi to qj.
We now make a DFA A′ = (Q′, , δ′, q′
0, F ′) for log(L). Here
Q′ = {[B, C] : B, C are n × n Boolean matrices},
where n is the number of states in Q. The basic idea is that if on input x we
arrive at the state [B, C], then B = Mx and C = M2|x|. To enforce this, we set

3.9 The Myhill–Nerode theorem
77
q′
0 = [I, M], where I is the n × n identity matrix and deﬁne δ′([B, C], a) =
[BMa, C2]. We also set
F ′ = {[B, C] : BC has a 1 in row 0 and column j such that qj ∈F}.
Then x is accepted by A′ if and only if MxM2|x| has a 1 in row 0 and a
column corresponding to a ﬁnal state, which occurs if and only if there exists
y, |y| = 2|x|, such that xy ∈L(M).
3.9 The Myhill–Nerode theorem
The Myhill–Nerodetheorem is probably the single most important characteri-
zation of regular languages.
To begin, let us recall the deﬁnition of equivalence relation. A relation R
over a nonempty set S is a subset R ⊆S × S. We write xRy if (x, y) ∈R. A
relation R is said to be an equivalence relation if it obeys the following three
properties:
(a) Reﬂexive property. For all x, we have xRx.
(b) Symmetric property. If xRy, then yRx.
(c) Transitive property. If xRy, and yRz then xRz.
An equivalence relation partitions S into a number of disjoint equivalence
classes. An equivalence class E with representative x is E = {y ∈S : xRy}.
We sometimes write E = [x]. The number of distinct equivalence classes is
called the index of the equivalence relation; it may be inﬁnite. If the index is
ﬁnite, we say that R is of ﬁniteindex.
If we have two equivalence relations R1 and R2 over S, we say that R1 is a
reﬁnementof R2 if R1 ⊆R2, that is, if xR1y ⇒xR2y for all x, y ∈S. If R1 is
a reﬁnement of R2, then each equivalence class of R2 is a union of some of the
equivalence classes of R1.
Example 3.9.1. Let us consider some equivalence relations over Z, the integers.
For a positive integer n, deﬁne En as follows: xEny means x ≡y (mod n).
Then the index of En equals n, as En partitions Z into equivalence classes
corresponding to each of the residue classes, mod n.
We have x ≡y (mod 6) implies that x ≡y (mod 3), and so E6 is a reﬁnement
of E3. In fact, each equivalence class of E3 is a union of two of the equivalence
classes of E6.
Now let us consider two equivalence relations on ∗. For a DFA M =
(Q, , δ, q0, F), deﬁne RM by xRMy means δ(q0, x) = δ(q0, y). The index

78
3
Finite automata and regular languages
of this equivalence relation is at most |Q|, the number of states, and hence is
ﬁnite.
This particular equivalence relation has a very nice property: namely, it is
right invariant. We say that an equivalence relation R is right invariant if xRy
implies that for all z ∈∗, we have xzRyz. Suppose xRMy. Then
δ(q0, xz) = δ(δ(q0, x), z)
= δ(δ(q0, y), z)
= δ(q0, yz),
so xzRMyz and hence RM is right invariant.
Another property of RM is that L(M) is the union of some of RM’s equiva-
lence classes, namely, those classes corresponding to ﬁnal states of M.
Now let us turn to another equivalence relation. Consider any language (not
necessarily regular) L ⊆∗. With L we associate the equivalence relation
RL deﬁned by xRLy means that for all z ∈∗, we have xz ∈L if and only
if yz ∈L. This equivalence relation is sometimes called the Myhill–Nerode
equivalence relation. Once again, we have that RL is right-invariant: suppose
xRLy. Then xu ∈L if and only if yu ∈L. Take u = zv to get
(xz)v ∈L ⇐⇒x(zv) ∈L ⇐⇒y(zv) ∈L ⇐⇒(yz)v ∈L.
Now L is the union of some of the equivalence classes of RL: namely, those
equivalence classes corresponding to elements of L.
Common Error 3.9.2. One very common error that students make when ﬁrst
exposed to the Myhill–Nerodeequivalence relation is to think that it only deals
with strings in L. In fact, this equivalence relation can be used to compare any
pair of strings chosen from ∗. Of course, not all pairs are necessarily related.
Now we are ready for the fundamental observation about the Myhill–Nerode
equivalence relation:
Lemma 3.9.3. Let L ⊆∗, and let E be any right-invariant equivalence
relation on ∗such that L is the union of some of E’s equivalence classes.
Then E is a reﬁnementof RL, the Myhill–Nerode equivalence relation.
Proof.
Suppose xEy. Then xzEyz for all z ∈∗. Since L is the union of
some of E’s equivalence classes, this gives that xz ∈L if and only if yz ∈L.
Hence, xRLy.

3.9 The Myhill–Nerode theorem
79
Example 3.9.4. It is worthwhile pointing out that Lemma 3.9.3 holds even if
E or RL has inﬁnite index. For example, consider  = {a, b}, L = ({a, b}3)∗,
and let E be the equivalence relation xEy if |x| = |y|. Then each of the
three equivalence classes of RL is the union of inﬁnitely many equivalence
classes of E. For example, L itself is the union of the equivalence classes
[ϵ], [a3], [a6], . . . of E.
Now we are ready for the Myhill–Nerodetheorem:
Theorem 3.9.5. Let L ⊆∗be a language. The following statements are
equivalent:
(a) L is regular.
(b) L can be written as the union of some of the equivalence classes of E,
where E is a right-invariant equivalence relation of ﬁniteindex.
(c) Let RL be the Myhill–Nerode equivalence relation. Then RL is of ﬁnite
index.
Proof.
(a) ⇒(b): If L is regular, it is accepted by some DFA M. By the
preceding discussion L is the union of some of the equivalence classes of RM,
and RM is right-invariant and of ﬁnite index.
(b) ⇒(c): By Lemma 3.9.3, E is a reﬁnement of RL. Then the index of
RL must be ≤the index of E. But E is of ﬁnite index, so RL is of ﬁnite
index.
(c) ⇒(a): We construct a DFA M′ = (Q′, , δ′, q′
0, F ′) accepting L. To do
so we let Q′ = {[x] : x ∈∗}, q′
0 = [ϵ], F ′ = {[x] : x ∈L}, and δ′([x], a) =
[xa].
As usual when dealing with equivalence relations, we must see that the
deﬁnition is meaningful. In other words, we need to see that if we pick a
different representative from the equivalence class [x], say y, then [xa] = [ya].
But this is just what it means to be right-invariant.
Now a simple induction on |y| gives that δ′([ϵ], y) = [y] for all y ∈∗. It
follows that L(M) = L.
We now consider some applications of the Myhill–Nerode theorem. By
Theorem 1.4.1, we know that every NFA with n states can be simulated by a
DFA with at most 2n states. But is this bound of 2n best possible? The Myhill–
Nerode theorem allows us to prove that 2n is best possible for alphabets of
size ≥2.

80
3
Finite automata and regular languages
1
1
1
1
q
q
q
0
1
2
3
q
0
0
0
0
0
0
Figure 3.16: The NFA M4
Theorem 3.9.6. Let Mn = (Qn, , δn, q0, {q0}) be the NFA deﬁnedas follows:
• Qn = {q0, q1, . . . , qn−1};
•  = {0, 1};
• δn(qi, 0) = {q0, qi} for 1 ≤i < n;
• δn(q0, 0) = ∅;
• δn(qi, 1) = {q(i+1) mod n}.
Then any DFA for Mn has at least 2n states.
For example, Figure 3.16 illustrates M4.
Proof Idea. Our plan is to generate 2n inequivalent strings, one corresponding
to each of the 2n possible subsets of states of Mn. By the Myhill–Nerode
theorem, it will then follow that any DFA for Mn must have at least 2n states.
To generate these strings, we use the structure of the NFA Mn. Note that
processing an input of 1 increments the subscript number of the state, modulo
n, and processing an input of 0 has the effect of both staying in the same state
and resetting the state to q0 (unless the machine is already in q0).
Proof. For each subsetS ⊆Qn, we deﬁne a stringw(S) such that δ(q0, w(S)) =
S, as follows: if S = {qe1, qe2, . . . , qek} with e1 < e2 < · · · < ek, then
w(S) =



0,
if S = ∅;
1i,
if S = {qi};
1ek−ek−101ek−1−ek−20 · · · 1e2−e101e1,
otherwise.
To see that δ(q0, w(S)) = S for |S| ≥2, note that successively reading the
blocks of 1s separated by single 0s gives the following sequence of states
encountered:

3.10 Minimization of ﬁniteautomata
81
1ek−ek−1
qek−ek−1
0
q0, qek−ek−1
1ek−1−ek−2
qek−1−ek−2, qek−ek−1
0
q0, qek−1−ek−2, qek−ek−1
1ek−2−ek−3
qek−2−ek−3, qek−1−ek−3, qek−ek−3
...
...
1e2−e1
qe2−e1, qe3−e1, . . . , qek−e1
0
q0, qe2−e1, qe3−e1, . . . , qek−e1
1e1
qe1, qe2, . . . , qek
We now show that if S, T ⊆Qn with S ̸= T , then the strings w(S) and
w(T ) are inequivalent under the Myhill–Nerodeequivalence relation. If S ̸= T ,
then one of S, T contains an element qs not contained in the other. Without
loss of generality, assume qs ∈S and qs ̸∈T . Then w(S)1n−s ∈L(Mn) but
w(T )1n−s ̸∈L(Mn). Since there are 2n subsets of Qn, the result follows.
The Myhill–Nerodetheorem can even be used to generate the “minimal”
inﬁniteautomaton for nonregular languages, as the following example shows.
Example 3.9.7. Let L = {0n1n : n ≥0} and let us compute the equivalence
classes for the Myhill–Nerodeequivalence relation.
The equivalence classes are
{[ϵ], [0], [00], . . . , } ∪{[1], [01], [001], . . .}.
To see that these classes are pairwise distinct, consider w = 0m and x = 0n
for m < n. By choosing z = 1m we see wz ∈L but xz ̸∈L. Similarly, given
w = 0m1 and x = 0n1 with m < n, choose z = 1n−1 to get wz ̸∈L but xz ∈L.
Finally, if w = 0m and x = 0n1, choose z = 01m+1 to get wz ∈L and xz ̸∈L.
We leave it to the reader to verify that these are all the equivalence classes.
We can generate an inﬁnite automaton from the equivalence classes, as in
Figure 3.17.
3.10 Minimization of ﬁnite automata
One of the important consequences of the Myhill–Nerodetheorem is that the
automaton constructed from the equivalence relation RL is actually the smallest
possible for the regular language L.
Theorem 3.10.1. The automaton M′ in Theorem 3.9.5 is the unique minimal
automaton for L, up to renaming of the states.

82
3
Finite automata and regular languages
1
0
0
1
1
0, 1
[0001]
[000]
0
0
0
0
[01]
[001]
. . .
. . .
[00]
[1]
[ϵ]
1
0,1
[0]
1
1
1
Figure 3.17: Minimal inﬁnite automaton for {0n1n : n ≥0}
Proof.
Let M = (Q, , δ, q0, F) be a minimal DFA for L, and consider the
equivalence relation RM. From Lemma 3.9.3 we have that RM is a reﬁnement
of RL and hence the index of RM is ≥the index of RL. It follows that M has
at least as many states as M′, where M′ is the DFA constructed in the proof of
Theorem 3.9.5.
Assume M has the same number of states as M′. We now show how to
create an isomorphism between Q and Q′. Let q be a state of M. Then q must
be reachable from q0; that is, there must be an x such that δ(q0, x) = q. For
if not, we could remove q from M and hence obtain a smaller DFA. Now
associate q ∈Q with [x] in M′. This is consistent, since both RM and RL are
right-invariant.
Furthermore, we can use the Myhill–Nerode theorem as the basis of an
algorithm for minimizing a ﬁnite automaton. Suppose we are given a DFA M =
(Q, , δ, q0, F) for a regular language L. From the results given before, we
know that RM is a reﬁnement of RL, the Myhill–Nerodeequivalence relation.
It follows that if M is not minimal, then there must be at least two distinct states
p, q ∈Q such that both {x ∈∗: δ(q0, x) = p} and {y ∈∗: δ(q0, y) = q}
are contained in some equivalence class of RL. In other words, for all z,
we have δ(p, z) ∈F if and only if δ(q, z) ∈F. We call the states p and q
indistinguishable if this is the case, and we write p ≡q. It is easy to see that ≡
is actually an equivalence relation.
One way to minimize a DFA is to determine the distinguishable states. Once
they have been determined, we can construct a minimal equivalent automaton
in two steps: ﬁrst, discard all states not reachable from the start state q0, and
second, collapse any maximal set of mutually indistinguishable states into a
single state. The states of M reachable from q0 can be determined in O(kn)
time through depth-ﬁrst or breadth-ﬁrst search if M is over a k-letter input
alphabet and has n states. This time is dominated by the time for the rest of the
algorithm, so we do not discuss it further in this section.

3.10 Minimization of ﬁniteautomata
83
Algorithm
Worst-case complexity
In practice
Implementation
NAIVE-MINIMIZE
O(n3)
Reasonable
Easy
MINIMIZE
O(n2)
Good
Moderately easy
FAST-MINIMIZE
O(n log n)
Very good
Quite difficult
BRZOZOWSKI
O(n22n)
Often good
Easy
Figure 3.18: Four different minimization algorithms compared
There are several different minimization algorithms known. The properties
of some of the most important algorithms are listed in Figure 3.18. The running
times given assume a ﬁxed alphabet size.
We now present the algorithm NAIVE-MINIMIZE:
NAIVE-MINIMIZE(M)
0. For all unordered pairs {p, q}, p ̸= q, set U({p, q}) := 0
1. For all unordered pairs {p, q} with p ∈F and q ∈Q −F,
set U({p, q}) := 1
2. Set done := false
3. while not(done) do
4. done := true
5. T := U
6. for each unordered pair {p, q} with T ({p, q}) = 0, do
7. For each a ∈ do
8. If T ({δ(p, a), δ(q, a)}) = 1 then set U({p, q}) := 1
and set done := false
9. return(U)
If we set U({p, q}) to 1 in the algorithm, then we say the pair {p, q} is
marked.
Theorem 3.10.2. Algorithm NAIVE-MINIMIZE terminates and correctly re-
turns an array
U({p, q}) =

1,
if p ̸≡q;
0,
if p ≡q.
Furthermore, the pair {p, q} is marked at the nth iteration of the while loop if
and only if the shortest string distinguishing p from q is of length n.
Proof.
Clearly, the algorithm terminates, since there are a ﬁnite number of
pairs and so eventually we make it through the while loop starting on line 3
without marking any new pairs.

84
3
Finite automata and regular languages
We claim that the pair {p, q} is marked by the algorithm at iteration n iff
p ̸≡q and the shortest string distinguishing p from q is of length n.
Suppose {p, q} is marked. We prove by induction on the number of iterations
n of the loop on line 3 that p ̸≡q and, further, that p is distinguished from q
by a string of length n. The base case is 0 iterations; that is, {p, q} is marked
on line 1. Then p ∈F and q ∈Q −F, so p ̸≡q. So ϵ distinguishes p from
q. Now suppose the claim is true for those pairs marked in some iteration < n;
we prove it for iteration n.
Pairs are marked in step 8, and this occurs only if {δ(p, a), δ(q, a)} was
marked on some previous iteration. In fact, this marking must have occurred at
iteration n −1; otherwise we would have considered {p, q} at an earlier itera-
tion. Let r = δ(p, a) and s = δ(q, a). If {r, s} was marked, then by induction
r ̸≡s and r is distinguished from s by a string t of length n −1. Then the string
at distinguishes p from q and |at| = n.
For the converse, suppose p ̸≡q, and x is a shortest string distinguishing
p from q, and n = |x|. We will prove by induction on n that the pair {p, q}
gets marked at iteration n. If |x| = 0, then {p, q} gets marked on line 1 of the
algorithm, at iteration 0 of the while loop. Now assume the claim is true for
all x with |x| < n. We prove the claim for |x| = n. Write x = ay with |y| = k,
a ∈. Consider {p′, q′}, where p′ = δ(p, a) and q′ = δ(q, a). Now p′ ̸≡q′
since the string y distinguishes them. Let z be a shortest string distinguishing
p′ from q′. Then by induction {p′, q′} gets marked at iteration |z|. Then the ﬂag
done gets set in line 8, and {p, q} gets marked at the next iteration. It follows
that n = |z| + 1 = |ay|.
To estimate the running time of NAIVE-MINIMIZE, we need the following
theorem.
Theorem
3.10.3. If
M
has
n ≥2
states,
then
the
while
loop
of
NAIVE-MINIMIZE is performed at most n −1 times, and in the last iteration,
no new pairs are marked.
Proof. Deﬁne a relation ≡
k on states of M as follows: p ≡
k q if there is no string
x of length ≤k that distinguishes state p from state q. Then it is not hard to see
that ≡
k is an equivalence relation. Also, if p and q cannot be distinguished with
a string of length ≤k + 1, they cannot be distinguished with a string of length
≤k. Thus, ≡
k+1 is a reﬁnement of ≡
k .
Now from the argument of the previous theorem, p ≡
k q if and only if after
k iterations of NAIVE-MINIMIZE, we have failed to distinguish p from q. We
also have

3.10 Minimization of ﬁniteautomata
85
(i) If p ≡
k q, then p ≡
k+1 q if and only if δ(p, a) ≡
k δ(q, a) for all a ∈.
(ii) If the relations ≡
k and ≡
k+1 are identical, then ≡
k+a equals ≡
k for all a ≥0.
Now ≡
0 has two equivalence classes, and ≡is the same as ≡
i for some
i; without loss of generality, assume i is as small as possible. But either ≡
k+a
coincides with ≡
k for all a ≥1 or ≡
k+1 deﬁnes at least one more equivalence class
than ≡
k ; hence, ≡
i deﬁnes at least i + 2 equivalence classes. But i + 2 ≤n, and
so i ≤n −2.
Corollary 3.10.4. The algorithm NAIVE-MINIMIZE uses O(kn3) steps, where
k = || and n = |Q|.
Proof.
The while loop is performed at most n −1 times by Theorem 3.10.3.
Line 6 takes O(n2) time and line 7 takes O(k) time.
The next corollary to Theorem 3.10.3 is both surprising and useful. Basically,
it states that if we are given two DFAs that accept different languages, then
there is a relatively short string that is accepted by one but rejected by the other.
If the DFAs have m and n states, it is not difﬁcult to obtain a bound of mn for
this problem, but the following theorem says that we can do much better.
Theorem 3.10.5. Let L1, L2 be regular languages accepted by DFAs with m
and n states, respectively, with L1 ̸= L2. Then there exists a string x of length
≤m + n −2 that is in (L1 −L2) ∪(L2 −L1).
Proof.
Take the DFA M1 = (Q1, , δ1, q1, F1) and form the “directsum”
with M2 = (Q2, , δ2, q2, F2). That is, create the DFA M = (Q, , δ, q, F)
as follows:
Q = Q1 ∪Q2
δ(p, a) =

δ1(p, a),
if p ∈Q1;
δ2(p, a),
if p ∈Q2;
q = q1
F = F1 ∪F2.
Then if L1 ̸= L2, q1 ̸≡q2, and furthermore by Theorem 3.10.3 these two states
can be distinguished by a string of length ≤|Q| −2 = m + n −2, where
m = |Q1|, n = |Q2|.
In fact, as the following theorem shows, the bound of m + n −2 is best
possible:

86
3
Finite automata and regular languages
0
0
0
0
0
0
0
0
p4
0
q1
q2
q0
p0
p1
p2
p3
q3
Figure 3.19: The construction for m = 4, n = 5
Theorem 3.10.6. For all integers m, n ≥1, there exist DFAs M1 and M2 over
a unary alphabet with m and n states, respectively, such that the shortest string
accepted by one DFA but not the other is of length m + n −2.
Proof. Without loss of generality we may assume m ≤n. If m = 1, the result
is easy. Otherwise assume m ≥2. Let M1 = (Q1, , δ1, q0, F1) be the DFA
given as follows:
Q1 = {q0, q1, q2, . . . , qm−1}
 = {0}
δ1(qi, 0) = q(i+1) mod m
F1 = {qj : j ≡n −2 (mod m)}.
Let M2 = (Q2, , δ2, p0, F2) be the DFA given as follows:
Q2 = {p0, p1, p2, . . . , pn−1}
 = {0}
δ2(pi, 0) =

pi+1,
if i < n −1;
pn−1,
if i = n −1;
F2 = {pj : j ≡n −2 (mod m)}.
For example, Figure 3.19 illustrates this construction for m = 4, n = 5.
Then it is easy to verify that 0m+n−2 is the shortest string accepted by one
machine but rejected by the other. We see that M1 and M2 behave identically
on all strings of length up to n −2. For lengths n −1, n, . . . , n + m −3, M1
rejects (since the next largest string accepted after 0n−2 is 0m+n−2) and so does
M2. But M1 accepts 0m+n−2 while M2 rejects this string.
We now discuss how to modify NAIVE-MINIMIZE to improve the running
time to O(kn2). The main difference in MINIMIZE is that for each pair of states
{p, q}, we maintain a list of pairs. Recall that if U({p, q}) = 1, we say {p, q}
is marked.

3.10 Minimization of ﬁniteautomata
87
MINIMIZE(M)
0. For all unordered pairs {p, q}, p ̸= q, set U({p, q}) := 0
1. For all unordered pairs {p, q} with p ∈F and q ∈Q −F,
set U({p, q}) : = 1
2. For each unordered pair {p, q} with either p, q ∈F or p, q ̸∈F do
3. If U({δ(p, a), δ(q, a)}) = 1 for some symbol a ∈ then
4. U({p, q}) := 1
5. Recursively set U({p′, q′}) := 1 for all unmarked pairs {p′, q′} on the
list for {p, q}, and all pairs on those lists, etc.
6. Else
7. For all a ∈ do
8. If δ(p, a) ̸= δ(q, a), put {p, q} on the list for {δ(p, a), δ(q, a)}
9. return(U)
The proof that MINIMIZE is correct is very similar to that for
NAIVE-MINIMIZE, and we leave it as Exercise 17.
Theorem 3.10.7. The algorithm MINIMIZE uses O(kn2) steps.
Proof.
Line 1 uses O(n2) steps. The loop in line 2 uses O(kn2) steps, not
including the time to do the recursion in step 5. To count the number of steps
in line 5, we must think about it not during one step of the loop, but over the
entire algorithm. There are at most kn2 entries over all lists during the running
of the algorithm, so step 5 uses at most O(kn2) steps.
We mention that there exists a reﬁnement of the algorithm MINIMIZE that
runs in O(kn log n) steps. For more details, see the notes.
We now turn to another minimization algorithm originally due to Brzo-
zowski. This algorithm has the property that it is excellent for hand computa-
tion and often works well in practice. However, its worst-case running time is
exponential.
Given a DFA M = (Q, , δ, q0, F), deﬁne MR to be the machine obtained
by reversing the arrows in M’s transition diagram. Formally, deﬁne MR =
(Q, , δR, F, {q0}), where
δR(q′, a) = {q ∈Q : δ(q, a) = q′}.
Note that MR is not strictly an NFA, since it may have more than one initial state.
Nevertheless it should be clear how to treat acceptance in such a generalized
NFA, and furthermore we can easily perform the subset construction on MR

88
3
Finite automata and regular languages
to obtain an equivalent DFA; the only signiﬁcant difference is that the initial
state in the corresponding DFA is the set of initial states of the generalized
NFA.
For such a generalized NFA A, deﬁne S(A) to be the DFA that arises from
the subset construction, using only states reachable from the start state(s).
Theorem 3.10.8 (Brzozowski). The machine given by S((S(MR))R) is a mini-
mal DFA equivalent to M.
Proof.
We ﬁrst prove the following lemma. Our theorem will then follow by
applying the lemma twice.
Lemma 3.10.9. Suppose M = (Q, , δ, q0, F) is a DFA accepting the regular
language L, and suppose that every state of Q is reachable from q0. Then
N = S(MR) is a minimal DFA for LR.
Proof. Let MR = (Q, , δR, q′
0, F ′), where δR is deﬁned as earlier, and q′
0 =
F, F ′ = {q0}. Let S(MR) = N = (Q′′, , δ′′, q′′
0, F ′′). To show N is minimal,
it sufﬁces to show that no two states of N are equivalent. Let A, B ∈Q′′ be
states of N, so that A and B represent sets of states of M. Suppose A is
equivalent to B. We show A = B.
Let p ∈A. Since every state of M is reachable from the start state, there
exists w ∈∗such that δ(q0, w) = p. Hence, q0 ∈δR(p, wR) in MR. Thus,
δ′′(A, wR) ∈F ′′ in N.
If A is equivalent to B, then δ′′(B, wR) ∈F ′′ in N. Thus, there exists p′ ∈B
such that q0 ∈δR(p′, wR) in MR. Hence, p′ = δ(q0, w). But then p = p′, since
M is a DFA. Thus, p ∈B.
We have now shown A ⊆B and, by symmetry, B ⊆A. Hence, A = B.
This completes the proof of correctness of Brzozowski’s algorithm.
Figure 3.20 illustrates Brzozowski’s algorithm on a simple automaton.
We now estimate the worst-case running time of Brzozowski’s algorithm.
Theorem 3.10.10. Brzozowski’s algorithm can be made to run in O(kn22n)
time.
Proof.
The cost of reversals is negligible compared to the cost of the two
subset constructions, so we estimate those.
Using Exercise 1.31, we can perform the initial subset construction in
O(kn2n) time. The resulting DFA has at most 2n states. When we reverse
and perform another subset construction, we may have to perform as many as
n rounds of 2n unions of 2n states.

3.11 State complexity
89
q
0
q
1
2
q
1
0
0
1
0
1
1
0,1
{        }
q
q
1
2
,
B
}
{q0
2
q
1
q
,
,
1
0,1
{        }
q q
1
2
,
B
}
{q0
2
q
1
q
,
,
q
0
q
1
2
q
1
0
0
1
0
1
A
A
0
0
0
1
0,1
{  }
{  ,  }
B
A
B
Reverse
Subset
Reverse
Subset
Figure 3.20: Illustration of Brzozowski’s algorithm
3.11 State complexity
Some regular languages, such as {0, 1}∗, can be accepted by DFAs with very
few states, while others, such as {0, 1}∗1{0, 1}n, require many states. The state
complexity of a regular language L, denoted by sc(L), is the smallest number
of states in any DFA accepting L or, equivalently, the number of states in the
minimal DFA accepting L.

90
3
Finite automata and regular languages
Similarly, we can study the nondeterministic state complexity of L, denoted
nsc(L), which is the smallest number of states in any NFA accepting L.
Theorem 3.11.1. Let L, L′ ⊆∗be regular languages. Then sc(L ∩L′) ≤
sc(L)sc(L′).
Proof Idea. The idea is to use the “direct-product”construction for automata.
The states of the new automaton consist of pairs, with the ﬁrst component
simulating the automaton for L and the second component simulating the
automaton for L′.
Proof. Let L be accepted by the DFA (Q, , δ, q0, F) and L′ be accepted by
the DFA (Q′, , δ′, q′
0, F ′). Then L ∩L′ can be accepted by a DFA
(Q′′, , δ′′, q′′
0, F ′′),
where
• Q′′ := Q × Q′;
• q′′
0 := [q0, q′
0];
• F ′′ := F × F ′; and
• δ′′([p, q], a) = [δ(p, a), δ(q, a)].
This DFA has |Q||Q′| states.
We now show that the upper bound of the previous theorem is tight.
Theorem 3.11.2. If || ≥2, then for all m, n ≥1, there exist regular lan-
guages L, L′ such that sc(L) = m, sc(L′) = n, and sc(L ∩L′) = mn.
Proof. Let  be an alphabet containing the letters a, b. Deﬁne
L : = {x ∈∗: |x|a ≡0 (mod m)};
L′ : = {y ∈∗: |y|b ≡0 (mod n)}.
Then it is easy to see that sc(L) = m and sc(L′) = n. We claim sc(L ∩L′) =
mn. To see this, note that
L ∩L′ = {x ∈∗: |x|a ≡0 (mod m), |x|b ≡0 (mod n)}.
We claim that for 0 ≤i < m, 0 ≤j < n, each string aibj lies in a distinct equiv-
alence class of the Myhill–Nerodeequivalence relation. For choose w = aibj
and x = ai′bj ′ for 0 ≤i, i′ < m, 0 ≤j ′, j ′ < n. If w ̸= x, then either i ̸= i′
or j ̸= j ′. Without loss of generality, assume the former case holds. Then
wam−ibn−j ∈L ∩L′, but xam−ibn−j ̸∈L, since i′ + m −i ̸≡0 (mod m). It

3.11 State complexity
91
follows that the minimal DFA for L ∩L′ has ≥mn states, and by Theo-
rem 3.11.1, it must have exactly mn states.
Similar but more complicated theorems can be proved for the deterministic
state complexity of other operations (see Exercise 49).
Now we turn to nondeterministic state complexity. Here the situation is more
challenging, since a minimal NFA for a given regular language is not necessarily
unique (see Exercise 18). Furthermore, it is known that the following decision
problem is PSPACE-hard:
Instance: a DFA M and an integer k.
Question: Is there an NFA with ≤k states accepting L(M)?
However, there are two simple theorems that can often be used to give lower
bounds on nsc(L).
Theorem 3.11.3. Let L be a nonempty regular language, and let n be the
length of a shortest string in L. Then nsc(L) ≥n + 1.
Proof.
Let M = (Q, , δ, q0, F) be an NFA accepting L, and let x be a
shortest string in L with |x| = n. Suppose M has ≤n states. Now consider the
states encountered on an accepting computation for x. Since |x| = n, some
state must be encountered at least twice. We can now cut out this loop to ﬁnd a
shorter string accepted by M, a contradiction.
Theorem 3.11.4. Let L ⊆∗be a regular language, and let M =
(Q, , δ, q0, F) be an NFA accepting L.
Suppose there exists a set of pairs of words P = {(xi, wi) : 1 ≤i ≤n} such
that
(a) For all i with 1 ≤i ≤n, we have xiwi ∈L.
(b) For all i, j with 1 ≤i, j ≤n, and i ̸= j, at least one of xjwi ̸∈L and
xiwj ̸∈L holds.
Then nsc(L) ≥n.
Proof. We can deﬁne a function f : {1, 2 . . . , n} →Q as follows: for each i,
since xiwi ∈L, there must be a state q ∈δ(q0, xi) such that δ(q, wi) ∩F ̸= ∅.
Deﬁne f (i) = q. Note that δ(f (i), wi) ∩F ̸= ∅.
Now we claim that this map is an injection; that is, ifi ̸= j, then f (i) ̸= f (j).
For suppose i ̸= j but f (i) = f (j). Then xiwj ∈L since f (j) = f (i) ∈
δ(q0, xi). Similarly, xjwi ∈L since f (i) = f (j) ∈δ(q0, xj). This contradic-
tion proves that f is an injection.

92
3
Finite automata and regular languages
Since the domain of f has cardinality n and f is an injection, Q has
cardinality ≥n and nsc(L) ≥n.
Example 3.11.5. Let k ≥1 be an integer and consider the language Lk =
{0i1i2i : 0 ≤i < k}. Take the set of pairs P to be the set
P = {(0i1j, 1i−j2i) : 0 ≤j ≤i < k}.
Let (x, w) = (0i1j, 1i−j2i) and (x′, w′) = (0i′1j ′, 1i′−j ′2i′) be two such distinct
pairs. Then xw, x′w′ ∈L but xw′ = 0i1i′+j−j ′2i′ cannot be in L unless i = i′
and j = j ′. Hence there are at least |P| = k(k + 1)/2 states in any NFA that
accepts Lk.
3.12 Partial orders and regular languages
A partial order is a binary relation R on a set S that behaves like the usual
relation ≤on real numbers. More precisely, it must be
(i) reﬂexive, that is, aRa for all a ∈S;
(ii) antisymmetric, that is, aRb and bRa ⇒a = b;
(iii) transitive, that is, aRb and bRc ⇒aRc.
Given any two real numbers x and y, they are comparable in the sense that
either x ≤y or y ≤x. However, this is not the case for a general partial order.
If xRy or yRx, we say x and y are comparable; otherwise we say they are
incomparable.
There are two natural partial orders associated with strings. The ﬁrst is the
subword ordering: we write xSy if x is a subword of y, that is, if there exist
strings w, z such that y = wxz. The second is the subsequence ordering: we
write x | y if x is a subsequence of y, that is, if we can obtain x from y by
striking out 0 or more symbols from y. More precisely, x | y if there exist
an integer n ≥0 and strings xi, yj ∈∗, 1 ≤i ≤n, 1 ≤j ≤n + 1 such that
x = x1x2 · · · xn and y = y1x1y2x2 · · · ynxnyn+1. You should now verify that
both of these relations are partial orders.
For the subword ordering it is possible to ﬁnd an inﬁnite set of pairwise in-
comparable strings. For example, {abna : n ≥1} forms such a set. However,
the following theorem shows the somewhat surprising fact that the correspond-
ing situation cannot occur for the subsequence ordering.
Theorem 3.12.1. Let  be a ﬁnite alphabet. Every set of strings over  that
are pairwise incomparable for the subsequence ordering is ﬁnite.

3.12 Partial orders and regular languages
93
Proof.
Assume there exists an inﬁnite set of pairwise incomparable strings.
Then there is certainly an inﬁnite division-free sequence of strings (fi)i≥1, that
is, a sequence of strings f1, f2, . . . such that i < j ⇒fi|/fj.
Now iteratively choose a minimal such sequence, as follows:
• let f1 be a shortest word beginning an inﬁnite division-free sequence;
• let f2 be a shortest word such that f1, f2 begins an inﬁnite division-free
sequence;
• let f3 be a shortest word such that f1, f2, f3 begins an inﬁnite division-free
sequence; and so on.
By the pigeonhole principle, there exists an inﬁnite subsequence of the fi,
say fi1, fi2, fi3, . . ., such that each of the strings in this subsequence starts with
the same letter, say a. Deﬁne xj for j ≥1 by fij = axj. Then we claim that
f1, f2, f3, . . . , fi1−1, x1, x2, x3, . . .
is an inﬁnite division-free sequence that precedes (fi)i≥1, contradicting the
supposed minimality of (fi)i≥1.
To see that the constructed sequence is indeed division-free, note that fi|/fj
for 1 ≤i < j < i1 by assumption. Next, if fi | xj for some i with 1 ≤i < i1
and j ≥1, then fi | axj = fij , a contradiction. Finally, if xj | xk, then axj | axk,
and hence fij | fik, a contradiction.
Notice that although we have proved that there are no inﬁnite pairwise
incomparable sets for the subsequence ordering, there are arbitrarily large such
sets. For example, the language {0, 1}n consists of 2n strings that are pairwise
incomparable.
We now introduce two operations on languages, the subsequence and su-
persequence operations. Let L ⊆∗. We deﬁne
sup(L) = {x ∈∗: there exists y ∈L such that y | x};
sub(L) = {x ∈∗: there exists y ∈L such that x | y}.
Our goal is to prove that if L is a language, then sub(L) and sup(L) is regular.
First, though, we need some lemmas.
Lemma 3.12.2. Let L ⊆∗. Then
(a) L ⊆sup(L);
(b) L ⊆sub(L);
(c) sub(L) = sub(sub(L)).
Proof. Left to the reader.

94
3
Finite automata and regular languages
We now introduce some terminology. Let R be a partial order on a set S.
Then we say x ∈S is minimal if yRx ⇒y = x for y ∈S. Let D(y) be the set
{x ∈S : xRy}.
Lemma 3.12.3. Let R be a partial order on a set S.
(a) If x, y are distinct minimal elements, then x, y are incomparable.
(b) Suppose the set D(y) is ﬁnite. Then there exists a minimal y′ such that
y′Ry.
Proof.
(a) Suppose yRx . Then x = y since x is minimal. But x, y were
assumed distinct, a contradiction. A similar argument applies if xRy.
(b) If D(y) = {y}, then we may take y′ = y. Otherwise let z be an element in
D(y) −{y}. Then D(z) ⊂D(y), and D(z) ̸= D(y). If D(z) = {z}, we may take
y′ = z. Continue in this fashion. Since D(y) is ﬁnite, we eventually terminate
and the last element chosen can be taken as y′.
Lemma 3.12.4. Let L ⊆∗. Then
(a) there exists a ﬁnitesubset M ⊆L such that sup(L) = sup(M).
(b) there exists a ﬁnitesubset G ⊆∗such that sub(L) = ∗−sup(G).
Proof.
(a): Let M be the set of minimal elements of L. By Lemma 3.12.3 the
elements of M are pairwise incomparable. By Theorem 3.12.1, M is ﬁnite. It
remains to see that sup(L) = sup(M).
Proof that sup(M) ⊆sup(L): suppose x ∈sup(M). Then there exists y ∈
M ⊆L such that y | x. Thus, x ∈sup(L).
Proof that sup(L) ⊆sup(M): suppose x ∈sup(L). Then there exists y ∈L
such that y | x. By Lemma 3.12.3 (b), there exists y′ ∈M such that y′ | y. Then
y′ | y | x, and so x ∈sup(M).
(b) Let T = ∗−sub(L). I claim that T = sup(T ). The inclusion T ⊆
sup(T ) follows from Lemma 3.12.2(a). Suppose sup(T ) ̸⊆T ; then there ex-
ists an x ∈sup(T ) with x ̸∈T . Since T = ∗−sub(L), this means x ∈
sub(L). Since x ∈sup(T ), there exists y ∈T such that y | x. Hence, by
Lemma 3.12.2(c), we have y ∈sub(L). But then y ̸∈T , a contradiction.
Finally, by part (a), there exists a ﬁnite subset G such that sup(G) = sup(T ).
Then sup(G) = sup(T ) = T = ∗−sub(L), and so sub(L) = ∗−sup(G).
We are now ready to prove the main result of this section.

3.13 Exercises
95
Theorem 3.12.5. Let L be a language (not necessarily regular). Then both
sub(L) and sup(L) are regular.
Proof. Clearly, sup(L) is regular if L = {w} for some single word w. This is
because if w = a1a2 · · · ak, then
sup({w}) = ∗a1∗a2∗· · · ∗ak∗.
Similarly, for any ﬁnite language F ⊆∗, sup(F) is regular because
sup(F) =

w∈F
sup({w}).
Now let L ⊆∗, and let M and G be deﬁned as in the proof of
Lemma 3.12.4. Then sup(L) = sup(M), and so sup(L) is regular, since M
is ﬁnite. Also, sub(L) = ∗−sup(G), and so sub(L) is regular since G is
ﬁnite.
Example 3.12.6. Consider the language
P3 = {2, 10, 12, 21, 102, 111, 122, 201, 212, 1002, . . .},
which represents the primes in base 3. I claim that the minimal elements of P3
are {2, 10, 111}. Clearly, each of these are in P3 and no proper subsequence is
in P3. Now let x ∈P3. If 2|/x, then x ∈{0, 1}∗. If further 10|/x, then x ∈0∗1∗.
Since x represents a number, x cannot have leading zeros. It follows that x ∈1∗.
But the numbers represented by the strings 1 and 11 are not primes. However,
111 represents 13, which is prime.
It now follows that
sup(P3) = ∗2∗∪∗1∗0∗∪∗1∗1∗1∗,
where  = {0, 1, 2}.
On the other hand, sub(P3) = ∗. This follows from Dirichlet’s theorem on
primes in arithmetic progressions, which states that every arithmetic progres-
sion of the form (a + nb)n≥0, gcd(a, b) = 1, contains inﬁnitely many primes.
For base 10, you can use the card shown in Figure 3.21. Photocopy it on
thick cardboard stock and take it to a bar sometime.
3.13 Exercises
1. (a) Prove or disprove that L1/L2 = (L1/L2) for all languages L1, L2.
(b) Prove or disprove that L/{x} = (L/{x}) for all languages L and all ﬁnite
strings x.

96
3
Finite automata and regular languages
Figure 3.21: The prime game
2. Which of the following is true for all languages L1, L2?
(a) (L1/L2)L2 ⊆L1,
(b) L1 ⊆(L1/L2)L2.
3. Suppose L ⊆∗is regular. Prove that the language
2L := {a1a1a2a2 · · · akak : each ai ∈ and a1a2 · · · ak ∈L}
is regular.
4. Generalizing the previous exercise, suppose x ∈∗
k = {0, 1, . . . , k −1}
and y ∈∗for some alphabet . If |x| = |y|, and x = a1 · · · an, y =
b1 · · · bn, we deﬁne rep(x, y) to be the string ba1
1 ba2
2 · · · ban
n . If |x| ̸= |y|, we
deﬁne rep(x, y) = ∅. Thus, for example, rep(234, abc) = aabbbcccc. Ex-
tend this deﬁnition to languages, as follows: if L1 ⊆∗
k and L2 ⊆∗, then
rep(L1, L2) = 
x∈L1,y∈L2 rep(x, y). Thus, for example, rep(1∗, L) = L for
all languages L and rep(2∗, L) is just the language 2L of the previous
exercise. Show that if L1 and L2 are both regular, then so is rep(L1, L2).
5. Let L1 ⊆L2 be regular languages with L2 −L1 inﬁnite. Show that there
exists a regular language L such that L1 ⊆L ⊆L2 and L2 −L and L −L1
are both inﬁnite.
6. Let L be a language and h a morphism. Show that
(a) L ⊆h−1(h(L));
(b) h(h−1(L)) ⊆L.
Also give examples where L ̸= h−1(h(L)) and h(h−1(L)) ̸= L.
7. Recall the deﬁnition of Pref(L) from Example 3.2.5. Give another proof of
the fact that if L is regular, so is Pref(L), by directly modifying the DFA
for L.

3.13 Exercises
97
∗8. Let L ⊆∗be a language. For an integer n ≥0, deﬁne
L1/n = {x ∈∗: xn ∈L}.
Note that
root(L) :=

i≥1
L1/i.
(a) Show that if L is regular, so is L1/n for each n ≥1.
(b) Show that if L is accepted by a DFA with n states, then root(L) =

1≤i≤n L1/i.
∗∗9. Show that if L is a regular language, then so is
ROOT(L) := {w : w|w| ∈L}.
10. Deﬁne the perfect shufﬂe of languages as follows:
L1 X L2 = {x X y : x ∈L1, y ∈L2, and |x| = |y|}.
Show how to modify Example 3.3.8 to give a formal deﬁnition of the per-
fect shufﬂe in terms of morphisms, inverse morphisms, and intersection.
11. If shuff(L, {0}) is regular, need L be regular?
12. If L is regular, then must
pow(L) := {uk : u ∈L, k ≥0}
also be regular?
13. Deﬁne the operation perm on strings as follows: perm(x) is the set of all
permutations of the letters of x. For example,
perm(0121) = {0112, 0121, 0211, 1012, 1021, 1102, 1120, 1201,
1210, 2011, 2101, 2110}.
Extend perm to languages as follows: perm(L) = 
x∈L perm(x).
If L is regular, need perm(L) be regular?
14. Show that any DFA accepting the language
Ln := {x ∈{0, 1}∗: the nth symbol from the right is 1},
introduced in Section 1.4, must have at least 2n states.
∗15. Is the class of regular languages closed under inverse substitution? That
is, let L be a regular language and s be a substitution that maps each letter
a to a regular language La. Deﬁne
s−1(L) = {x : s(x) ⊆L}.
Must s−1(L) be regular?

98
3
Finite automata and regular languages
16. Consider replacing the deﬁnition for inverse substitution given in the
previous exercise with a new deﬁnition:
s[−1](L) := {x : s(x) ∩L ̸= ∅}.
Suppose s maps letters to regular languages, and L is regular. Must
s[−1](L) be regular?
17. Using the proof of Theorem 3.10.2 as a guide, prove the correctness of
algorithm MINIMIZE.
18. Theorem 3.10.1 says that if M1, M2 are minimal DFAs accepting a regular
language L, then M1 and M2 are isomorphic. (Isomorphic means that the
machines are identical, up to renaming the states.) Show, by means of an
example, that this result is not true if “DFA”is replaced by “NFA.”
19. Let L be a regular language, and let M be the minimal DFA accepting L.
Suppose M has n ﬁnal states. Show that any DFA accepting L must have
at least n ﬁnal states.
20. Let  = {0, 1}. Give an example of a language L ⊆∗for which the
Myhill–Nerodeequivalence relation RL has the property that every string
in ∗is an equivalence class by itself.
21. Let L be a regular language, and let M = (Q, , δ, q0, F) be its minimal
DFA. Let M′ = (Q′, , δ′, q′
0, −) be the transformation automaton of M,
as described in Section 3.7, but with all states not reachable from q′
0
discarded. Let RM′ be the equivalence relation based on M′, where x and
y are equivalent iff δ′(q′
0, x) = δ′(q′
0, y).
Now consider the equivalence relation x ≡y, where “x ≡y”means
“for all u, v ∈∗, uxv ∈L iff uyv ∈L.” Show that the equivalence
relation ≡is the same as RM′.
∗22. The point of this exercise is to show that 2DFAs can be exponentially
more concise than DFAs in accepting certain languages.
Let n be an integer ≥1, and let Fn ⊆{0, 1, 2, 3, 4}∗be deﬁned as
follows:
Fn = {3 0i1 1 0i2 1 · · · 1 0in 2k 0ik 4 : 1 ≤k, j, ij ≤n}.
For example,
F2 = {3010204, 30102204, 300102004, 300102204,
30100204, 3010022004, 3001002004, 30010022004}.
(a) Show that Fn can be accepted by a 2DFA using O(n) states.
(b) Using the Myhill–Nerodetheorem, show that the smallest DFA ac-
cepting Fn has at least nn states.

3.13 Exercises
99
∗∗23. For a word w, deﬁne SD(w), the subword-doubling map, as follows:
SD(w) = {u ∈∗: there exist x, y, z ∈∗such that u = xyyz
and w = xyz}.
In other words, SD(w) is the language of all strings obtainable from
w by doubling some subword.
For a language L, deﬁne
SD(L) =

w∈L
SD(w).
Deﬁne SD0(w) = {w} and SDi(w) = SD(SDi−1(w)) for i ≥1.
Finally, for a word w, deﬁne SDC(w), the subword-doubling closure
of w, as follows:
SDC(w) =

i≥0
SDi(w).
Prove that SDC(012) is not regular.
24. Let L1, L2 be languages with L1 ⊆∗, L2 ⊆∗, and let h : ∗→∗
be a morphism. Prove or disprove that
h(L1) −L2 ⊆h(L1 −h−1(L2)).
Note: A −B means {x ∈A : x ̸∈B}.
25. The star height of a regular expression is the maximum number of nested
occurrences of the ∗operator. The star height of a regular language L is
the minimum of the star heights of all regular expressions representing
L.
Show that the star height of any regular language over a one-letter
alphabet is ≤1.
26. Consider an alternate deﬁnition of quotient, deﬁned as follows for
L1, L2 ⊆∗:
L1 ÷ L2 = {x ∈∗: xy ∈L1 for all y ∈L2}.
If R is regular and L is any language, must R ÷ L be regular?
27. Characterize all regular languages L ⊆∗with the following property:
there exists a constant c, depending on L, such that for all n ≥0, we
have |L ∩n| ≤c.
28. Fix an alphabet . Give good upper and lower bounds on the number of
distinct languages that can be accepted by some DFA with n states.

100
3
Finite automata and regular languages
29. Let L1, L2 be languages. Consider the equation X = L1X + L2. Assum-
ing L1 does not contain the empty string, ﬁnd a solution X of this equation
and prove it is unique. What if L1 contains ϵ?
30. Let k = {0, 1, . . . , k −1}. Let L ⊆∗
k, and consider the set L(L) of
lexicographically largest strings of each length in L. Thus, for example,
in L({0, 1}∗) = 1∗and L(ϵ + 1(0 + 01)∗) = (10)∗(ϵ+1).
Show that if L is regular, so is L(L).
31. (Continuation.) Instead of taking the lexicographically largest strings of
each length, consider taking the lexicographically median string of each
length. (That is, if there are r strings of length n, take string number ⌈r/2⌉.)
Show that if the original language is regular, the resulting language need
not be regular.
32. In analogy with Example 3.12, compute the minimal elements for the
language
{2, 3, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, . . .}
of those strings that do not represent squares in base 10.
33. (a) A ﬁnite string x = a1a2a3 · · · an ∈∗is said to possess the Fried-
man property if for all i, j with 1 ≤i < j ≤n/2, we have
ai · · · a2i|/aj · · · a2j. (Recall that w | x means w is a subsequence of
x.) Prove that for each , there is a longest ﬁnite string with the
Friedman property.
(b) By analogy with (a), an inﬁnite string x = a1a2a3 · · · ∈ω is said
to possess the Friedman property if for all i, j with 1 ≤i < j, we
have ai · · · a2i|/aj · · · a2j. Prove that no inﬁnite string possesses the
Friedman property.
∗34. Recall the operation cyc(L) from Section 3.4. Give an example of a
language L such that cyc(L) is regular, but L is not.
∗35. Let n be a positive integer and let Ln = (0 + (0 1∗)n−10)∗. Show that Ln
can be accepted by an NFA with n states, but no DFA with less than 2n
states accepts Ln.
36. Show that if R and L are regular languages given by DFAs, then there is
an algorithm to construct a DFA for R/L.
37. Consider the language Ak = {w ∈{0, 1}k : w = wR}. Show that any
NFA accepting Ak has at least 2⌊k/2⌋+1 + 2⌊(k+1)/2⌋−2 states. Also show
that this bound is tight.
38. Give a different proof of Theorem 3.4.1, based on the suggestion following
the proof.
39. Find the equivalence classes for the Myhill–Nerodeequivalence relation
for

3.13 Exercises
101
(a) L = {w ∈{0, 1}∗: |w|0 = |w|1}.
(b) L = {anbncn : n ≥1}.
∗∗40. Let  be a ﬁnite alphabet with at least two letters. A language L ⊆∗
is said to be sparse if
lim
n→∞
|L ∩n|
|n|
= 0.
Prove or disprove that there exists a sparse language L such that LL =
∗.
41. Describe a family of DFAs Mn for which Brzozowski’s algorithm takes
exponential time.
42. Show that for any word w ∈+, there exists a regular expression rw for
the language of all preﬁxes of w, such that |rw| ≤4|w|.
43. Show that there exists a constant c such that for any word w ∈+, there
exists a regular expression for ∗−{w} of length ≤c|w|.
∗44. Construct a family of regular expressions rn such that
(a) L(rn) ⊆0∗.
(b) There exists a constant c such that |rn| = O(nc).
(c) There exists a constant d such that the shortest regular expression
for L(rn) is of length >2dn.
45. For each k ≥0, show how to construct a regular language Lk over a ﬁnite
alphabet k such that Lk has exactly nk words of length n, for all n ≥0.
(Note that 00 = 1.)
∗∗46. Describe a family of NFAs (Mn)n≥1 over a ﬁnite alphabet  satisfying
the following three conditions:
(a) Mn has O(n) states.
(b) L(Mn) ̸= ∗.
(c) the shortest string not accepted by Mn is of length ≥2n.
47. Give a proof of Theorem 1.4.2, (b) ⇒(a), along the following lines. Let
M = (Q, , δ, q1, F) be a DFA, where Q = {q1, q2, . . . , qn}. Deﬁne
Ri,j,k to be the language of all strings that take us from state i to state
j without passing through a state numbered higher than k. Now give a
recursion formula that allows you to compute Ri,j,k.
48. (Continuation.) Using the previous exercise, prove that if M is a DFA
with n = |Q| and k = ||, then L(M) is speciﬁed by a regular expression
of length O(kn4n).
∗49. Show that if M1 is a DFA with m ≥1 states and M2 is a DFA with
n ≥2 states, then there is a DFA with ≤m2n −2n−1 states accepting
L(M1)L(M2). Furthermore, show that this bound is tight.

102
3
Finite automata and regular languages
50. We can consider the input–outputbehavior of a ﬁnite-state transducer on
an inﬁnitelylong input. Now think about real numbers expressed in base-3
notation. Every real number between 0 and 1 has a base-3 expansion of
the form
.a1a2a3 · · · ,
where ai ∈{0, 1, 2}; this expansion represents the number 	
i≥1 ai3−i.
(You should think of the period as representing a “ternarypoint,”not
a decimal point.) Provided that the expansion does not end in inﬁnitely
many 2s, this expansion is unique.
The Cantor set C is deﬁned to be the set of all real numbers between
0 and 1 whose base-3 expansion contains only 0s and 2s. Prove that
every real number x between 0 and 1 can be written as the sum of
two numbers y and z chosen from C by giving a ﬁnite state transducer
that, on input a1a2a3 · · · , outputs the string [b1, c1][b2, c2][b3, c3] · · · ,
where x = .a1a2a3 · · · , y = .b1b2b3 · · · , z = .c1c2c3 · · · , y, z ∈C, and
x = y + z.
51. (J. Buss) For each integer n ≥1, compute the deterministic state com-
plexity of the language Ln = {x ∈{0, 1}∗: [x]2 ≡0 (mod n)}, where by
[x]2 we mean the integer represented in base 2 by the string x.
52. Give a short regular expression for the language L(Mn) in Theorem 3.9.6.
By short we mean having O(n) symbols.
53. In an extended regular expression, intersection and complement may be
used. Give a short extended regular expression for the language L(w, x) =
{y : every occurrence of w in y is followed by x}.
54. Let  = {1, 2, . . . , n}, and deﬁne
Ln = {w ∈∗: |w|i = 1 for all i}.
For example, L3 = {123, 132, 213, 231, 312, 321}.
(a) Prove that no regular expression of length <2n−1 can specify Ln.
(b) Prove that there exists a regular expression of length O(n2) specifying
Ln.
∗55. Let  = {1, 2, . . . , n}. Consider the language Ln consisting of all strings
over  such that there exists a way to partition the symbols of the string
into two multisets whose sum is equal. For example, 1122 ∈L2, but
11221 ̸∈L2. Show that each Ln is regular for n ≥1.
56. Let M = (Q, , δ, q0, F) be a DFA. Show that the language of strings
x that cause M to visit every state of Q while processing x is a regular
language.
57. An NFA M = (Q, , δ, q0, F) is ambiguous if there exists at least one
string w that M accepts via at least two distinct computation paths.

3.13 Exercises
103
(a) Show that an n-state NFA M is ambiguous if and only if it is ambigu-
ous on an input of length <n2 + n.
(b) Is this bound tight?
58. Let M be a DFA with n states, and suppose L(M) contains at least one
palindrome. Show that L(M) contains a palindrome of length <2n2.
59. Let E be a regular expression such that ϵ ̸∈L(E). Show that there exists
a regular expression using only the operators union, concatenation, and
positive closure that speciﬁes L(E).
60. Show that the Myhill–Nerodeequivalence classes for a language L are
the same as those for L.
61. Deﬁne the map r : ∗→∗as follows: if w = a1a2 · · · an with each ai ∈
, then r(w) = an
1an
2 · · · an
n. For example, r(abc) = aaabbbccc. Note
that r is not a morphism. Now extend r in the obvious way to languages
as follows: r(L) = 
x∈L r(x).
(a) Let L = (a + ba)∗(ϵ + b), the language of words not containing bb
as a subword. What is r−1(L)?
(b) Show by means of an example that if L is regular then r(L) need not
be regular.
(c) Deﬁne r−1(L) = {x : r(x) ∈L}. Prove that if L is regular then so is
r−1(L).
∗62. Let M = (Q, , δ, q0, F) be an NFA, and let a ∈. Let t ≥0, c ≥1
be the smallest integers such that δ(q0, at) = δ(q0, at+c). Show that t ≤
(n −1)2 + 1 and c divides lcm(1, 2, . . . , n).
63. Call a language L commutative if for all x, y ∈L, we have xy = yx.
Show that L is commutative if and only if there exists a word w such that
L ⊆w∗.
64. In this exercise you will develop an efﬁcient algorithm to determine if
a regular language speciﬁed by an NFA M is palindromic, that is, if
x = xR for all x ∈L(M). Your algorithm should run in time bounded by
a polynomial in the number of states of M.
(a) First, prove the following lemma: Let x, u, v, w, y, x′, u′, v′, w′, y′
be words. If
(i) y′x′ = xy
(ii) y′u′x′ = xuy
(iii) y′v′x′ = xvy
(iv) y′w′x′ = xwy
(v) y′v′u′x′ = xuvy
(vi) y′w′v′x′ = xvwy
all hold, then y′w′v′u′x′ = xuvwy.
(b) Using the previous lemma, show that L(M) is palindromic if and only
if {x ∈L(M) : |x| < 3n} is palindromic.

104
3
Finite automata and regular languages
(c) Show that there exists an NFA M′ with O(r) states that accepts only
nonpalindromes and accepts all the nonpalindromes of length <r.
(d) Complete the proof of the algorithm by forming the cross-product of
M with M′ to compute L(M) ∩L(M′). Finally, show that one can
efﬁciently determine if the resulting NFA accepts any word.
65. Show that for all n ≥2, there exists an NFA with n states such that the
shortest nonpalindrome accepted is of length 3n −1.
66. Show
that
if
L
is
regular,
then
L′ = {u ∈+ : there exists v ∈
∗such that uvu ∈L} is regular.
67. A word w ∈∗is said to bordered if it can be written in the form uvu
with u ∈+, v ∈∗. Show that the following problem is solvable in
polynomial time: given an NFA M, does M accept a bordered word?
68. For a word w ∈∗, we deﬁne palc(w) to be the palindromic closure of
w, that is, the (unique) shortest palindrome x such that w is a preﬁx of x.
Deﬁne palc(L) = 
w∈L{palc(w)}.
(a) Show that palc(w) = wt−1wR, where t is the longest palindromic
sufﬁx of w.
(b) If L is regular, need palc(L) be regular?
(c) If L is regular, need palc−1(L) = {x ∈∗: palc(x) ∈L} be regular?
69. Let x1, x2, . . . , xk ∈∗. Show that ∗−x∗
1x∗
2 · · · x∗
k is ﬁnite if and only
if || = 1 and gcd(|x1|, |x2|, . . . , |xk|) = 1.
70. Let ,  be alphabets. We say a word pattern p ∈∗matches a word
z ∈∗if there exists a nonerasing morphism h such that h(p) = z. (By
nonerasing, we mean that h(a) ̸= ϵ for all a ∈.) For example, xx is a
pattern for the English word murmur. Deﬁne Pat(L) to be the language of
all patterns that match words in L.
(a) Give an example of a nonregular language L such that Pat(L) = L.
(b) Give an example of a nonregular language L such that Pat(L) is regular.
(c) Prove that if L ⊆∗is regular, then the language
Pat(L) = {y ∈∗: there exists z ∈L such that y is a pattern for z}
is regular.
71. Let h : ∗→∗be a morphism, and let L ⊂∗be a language. Deﬁne
h−∗(L) =

i≥0
h−i(L),
where by h−i(L), we mean
i times



h−1(h−1(· · · h−1(L))). Show that if L is regular,
so is h−∗(L).
72. Let L be a language, and deﬁne q(L) = {z ∈L : xy ̸= z for all x, y ∈L}.
Show that if L is regular, so is q(L).

3.15 Research problems
105
73. Let us say that a language L′ is an nth-order approximation to L if L ∩
≤n = L′ ∩≤n.
(a) Deﬁne AL(n), the automaticity function of L, to be the smallest number
of states in any DFA M such that L(M) is an nth-order approximation
to L. Find good upper and lower bounds for AL(n).
(b) Prove that if L is not regular, then AL(n) ≥(n + 3)/2 inﬁnitely often.
3.14 Projects
1. Find out more about applications of automata theory to game theory and
economics (“boundedrationality”).You could start with Axelrod [1984],
Rubinstein [1986], and Linster [1992].
2. Find out more about the connections between ﬁnite automata and number
theory. You can start with the book of Allouche and Shallit [2003].
3. Look into how regular languages can be efﬁciently learned. The paper of
Angluin [1987] is a good place to start. Also see Ibarra and Jiang [1991].
4. Look into the relationship between cellular automata and regular lan-
guages. Start with Wolfram [1984]. The book of Wolfram [2002] also has
some useful information, but at a much less technical level.
5. Look into generalizations of Theorem 3.4.1 and Theorem 3.8.8, the class of
regularity-preserving transformations. Start with the papers of Stearns and
Hartmanis [1963], Seiferas and McNaughton [1976], and Kozen [1996].
6. Look into the star-height problem, which asks for the minimum number of
nested stars needed in a regular expression for a given language. Start with
the papers of Eggan [1963] and Hashiguchi [1982, 1988].
3.15 Research problems
1. A DFA is called synchronizing if there exists a ﬁnite word w and a state
q such that reading w starting in any state leads to q. Cerny’s conjecture
asks if the length of the shortest synchronizing word for an automaton
with n states is ≤(n −1)2. The best starting point is the Web page of Pin,
http://www.liafa.jussieu.fr/~jep/Problemes/Cerny.html.
2. Suppose you are given two distinct words u, v with |u|, |v| ≤n. What is
the size of the smallest DFA that accepts u but rejects v, or vice versa? If
u and v are of different lengths then a simple argument gives a O(log n)
upper bound. How about if u and v are of the same length? Robson [1989]
showed that in this case a machine of size O(n2/5(log n)3/5) exists. Can
this bound be improved?

106
3
Finite automata and regular languages
3.16 Notes on Chapter 3
A good survey on regular languages is Yu [1997].
3.1 For Moore machines, see Moore [1956]. For Mealy machines, see
Mealy [1955].
3.2 Quotients were introduced by Ginsburg and Spanier [1963]. In the lit-
erature, L1/L2 is sometimes denoted L1L−1
2 and is called the right quo-
tient. There is a corresponding notion of left quotient, written L−1
1 L2.
3.3 For more on morphisms, see Harju and Karhum¨aki [1997].
3.4 For generalizations of Theorem 3.4.1, see Seiferas and McNaughton
[1976].
3.5 Transducers are covered extensively in the book of Berstel [1979].
The particular kind of transducer we have studied in this section is
the most powerful ﬁnite-state version. Other, weaker, types of trans-
ducers include the generalized sequential machine or GSM, the se-
quential transducer (in left and right versions), and the subsequential
transducer.
3.6 The material in this section is from Shepherdson [1959]. In the litera-
ture, 2DFAs are often equipped with distinguished symbols that serve
as endmarkers for the input.
3.7 The transformation automaton can be found, in somewhat disguised
form, in McNaughton and Papert [1968]. Also see Lawson [2004,
chapters 8 and 9].
3.8 For more about the applications of Boolean matrices, see Zhang
[1999].
The current record for efﬁcient n × n matrix multiplication is held
by Coppersmith and Winograd; it can be done in O(n2.376) steps.
3.9 The Myhill–Nerodetheorem is due to Nerode [1958]. Myhill [1957]
proved a similar result.
3.10 Theorem 3.10.5 can be considered with “NFA”replacing “DFA.”See
Nozaki [1979].
The algorithm MINIMIZE can be found in Hopcroft and Ullman
[1979].
For fast algorithms for DFA minimization, see, for example,
Hopcroft [1971], Gries [1973], and Blum [1996].
For Brzozowski’s algorithm, see Brzozowski [1962a]. This simple
algorithm has been rediscovered several times, for example, see Brauer
[1988] and Brzozowski [1989]. Our treatment of the equivalence rela-
tion ≡
k is based on Wood [1987].

3.16 Notes on Chapter 3
107
3.11 Theorem 3.11.1 is due to Maslov [1970] and Yu, Zhuang, and Salomaa
[1994].
The PSPACE-completeness result is due to Jiang and Ravikumar
[1993, theorem 3.2].
Theorem 3.11.4 is due to Birget [1992] and was rediscovered in a
weaker form by Glaister and Shallit [1996], which is the source of
Example 3.11.5.
3.12 The material in this section is based on the treatment in Lothaire [1983,
§6.1] and Harrison [1978, §6.6]. Also see Haines [1969].

4
Context-free grammars and languages
In this chapter we consider some advanced topics on context-free grammars
and languages. We start with closure properties.
4.1 Closure properties
You may recall from a ﬁrst course on formal languages that the class of context-
free languages (CFLs) is closed under the operations of union, concatenation,
and Kleene ∗. (This follows easily from an argument using the representation
of a CFL by a grammar.)
Also recall that the class of CFLs is not closed under the operations of
intersection and complement. For example,
L1 = {aibicj : i, j ≥0}
and
L2 = {aibjcj : i, j ≥0}
provide examples of two CFLs such that their intersection is not context-free.
Similarly, L = {anbncn : n ≥0} is a non-CFL, but L is context-free (see
Exercise 1).
In this section we consider some of the operations we introduced in Sec-
tions 3.3–3.4, but for CFLs.
Theorem 4.1.1. The class of CFLs is closed under substitution by context-free
languages.
Proof.
Let ,  be alphabets, let L ⊆∗be a CFL, and suppose s is a
substitution such that s(a) = La ⊆∗is a CFL for each a ∈. We wish to
show that s(L) is context-free.
108

4.1 Closure properties
109
To see this, let G = (V, , P, S) be a context-free grammar generating
L, and for each a ∈, let Ga = (Va, , Pa, Sa) be a context-free grammar
generating La. By renaming variables, if necessary, we may assume that the
sets V and Va for a ∈ are pairwise disjoint.
Now we construct a grammar for s(L). First, replace every occurrence of a
in G with Sa, the start symbol for the grammar Ga, and call the resulting set of
productions P ′. Now form the grammar
G′ = (V ∪

a∈
Va, , P ′ ∪

a∈
Pa, S).
Clearly, L(G′) = s(L).
Corollary 4.1.2. The class of CFLs is closed under morphism.
Example 4.1.3. Theorem 4.1.1 and Corollary 4.1.2 are useful to prove that
certain languages are not context-free. For example, consider the language
ODD := {a1b3a5b7 · · · b4n−1 : n ≥1}.
To see that ODD is not context-free, let h(a) = h(b) = c. Assume ODD is context-
free. Then by Corollary 4.1.2, h(ODD) would be context-free, too. But h(ODD) =
{c4n2 : n ≥1}. This last language is easily seen not to be context-free using
the pumping lemma.
Theorem 4.1.4. The class of CFLs is closed under inverse morphism.
Proof Idea. Let L ⊆∗be a CFL and h : ∗→∗be a morphism. We want
to show that h−1(L) is context-free.
First, recall how the analogous property for regular languages was proved.
We took a deterministic ﬁnite automaton (DFA) M = (Q, , δ, q0, F) for
L and changed the “wiring,”deﬁning δ′(q, a) = δ(q, h(a)). By analogy we
could start with a pushdown automaton (PDA) M = (Q, , , δ, q0, Z0, F)
accepting by ﬁnal state and modify it in a similar way. Unfortunately, this
approach does not work directly for PDAs, since h(a) is a string, and on
processing a string a PDA may make many moves, including multiple pops of
the stack. In our PDA model there is no way to pop multiple symbols from a
stack in one move.
The solution is as follows: we read a symbol a from the input, compute h(a),
and then process the symbols of h(a) one by one. When we are done with the
symbols of h(a) we read another symbol from the input. The easiest way to
accomplish this is to store the as-yet-unprocessed symbols of h(a) in the state
of the PDA.

110
4
Context-free grammars and languages
Proof. More precisely, given M = (Q, , , δ, q0, Z0, F) we create a PDA
M′ = (Q′, , , δ′, q′
0, Z0, F ′),
where Q′ = Q × T and
T = {x : there exists a ∈ such that x is a sufﬁx of h(a)}.
Since  is ﬁnite, so is T . Also, we deﬁne q′
0 = [q0, ϵ] and F ′ = F × {ϵ}.
The transition function δ′ is deﬁned as follows: there are Type 1 transitions
of the form
δ′([q, ϵ], a, X) = {([q, h(a)], X)}
for all q ∈Q, a ∈, X ∈, and Type 2 transitions of the form
δ′([q, bx], ϵ, X) = {([p, x], γ ) : (p, γ ) ∈δ(q, b, X)}
for all q ∈Q, b ∈ ∪{ϵ}, X ∈, and bx ∈T .
To see that this actually works, we must prove by induction on |x| that
(q, h(x), α)
∗⊢(p, ϵ, β) iff ([q, ϵ], x, α)
∗⊢([p, ϵ], ϵ, β). The details are left to
the reader.
Theorem 4.1.5. Let T = (Q, , , q0, F, S) be a ﬁnite-state transducer and
let L ⊆∗be a CFL. Then T (L) is context-free.
Proof. From Theorem 3.5.3 we know that the action of T on L can be repre-
sented as g(h−1(L) ∩R), where g and h are morphisms and R is a regular lan-
guage. But the context-free languages are closed under these operations.
Theorem 4.1.6. The class of CFLs is not closed under quotient.
Proof.
Let L = {a2nban : n ≥1}. Then L is easily seen to be context-free.
Deﬁne
L1 = (Lb)+ab
and
L2 = b(Lb)+.
Then both L1 and L2 are CFLs, since the class of CFLs is closed under the
operations of concatenation and Kleene ∗. If the CFLs were closed under
quotient then
L1/L2 = {x : ∃y ∈L2 such that xy ∈L1}

4.2 Unary context-free languages
111
would be a CFL. If xy ∈L1 then
xy = ac1bac2b · · · ac2k−1bac2kbac2k+1b
for some positive integers c1, c2, . . . , c2k+1 with c2i−1 = 2c2i for 1 ≤i ≤k and
c2k+1 = 1. If y ∈L2 then
y = bac2j bac2j+1b · · · ac2kbac2k+1b
for some j, 1 ≤j ≤k with c2t = 2c2t+1 for j ≤t ≤k and c2k+1 = 1. Now
c2k = 2, c2k−1 = 4, and so on, so it follows that
x = ac1bac2b · · · bac2j−1
with c2j−1 = 4k+1−j. Hence,
(L1/L2) ∩a∗= {a4n : n ≥1},
and this language would also be context-free by Theorem 1.5.8. But {a4n : n ≥
1} is easily seen not to be context-free using the pumping lemma, a contradic-
tion.
However, if L is a CFL and R is a regular language, then L/R is a CFL (see
Exercise 18).
4.2 Unary context-free languages
Consider CFLs over an alphabet consisting of a single symbol. In this section
we prove the following.
Theorem 4.2.1. A unary language is context-free if and only if it is regular.
Proof. If L is regular then it is context-free by Corollary 1.5.9.
For the converse, let L be a CFL with L ⊆0∗, and let n be the constant
in the pumping lemma for CFLs. For each m ≥n with 0m ∈L, the pumping
lemma applied to the string z = 0m says that there is some decomposition
0m = uvwxy, where |vx| ≥1 and |vwx| ≤n, such that uviwxiy ∈L for all
i ≥0. Now let am = |uwy| and bm = |vx|. Then 1 ≤bm ≤n, m = am + bm,
and 0am+ibm ∈L for all i ≥0.
Let M = {m ≥n : 0m ∈L}, and let L′ = L ∩{ϵ, 0, 02, . . . , 0n−1}. Then
L = L′ ∪{0m : m ∈M} ⊆L′ ∪

m∈M
0am(0bm)∗⊆L,
so L = L′ ∪
m∈M 0am(0bm)∗.

112
4
Context-free grammars and languages
Now each language 0am(0bm)∗is a subset of 0a(0bm)∗with a = am mod bm.
So deﬁne the ﬁnite set
A = {(a, b) : 1 ≤b ≤n, 0 ≤a < b, and there exists m ∈M with bm = b
and am ≡a (mod bm)},
and, for all pairs (a, b) ∈A, deﬁne
qa,b = min{am : m ∈M and bm = b and am ≡a (mod bm)}.
Then

m∈M
bm=b
am≡a (mod bm)
0am(0bm)∗=

(a,b)∈A
0qa,b(0b)∗,
so
L = L′ ∪

(a,b)∈A
0qa,b(0b)∗.
Thus we have written L as the union of a ﬁnite set (L′) and at most n2 regular
languages, so L is regular.
4.3 Ogden’s lemma
The pumping lemma is one of the most important tools we have for proving
languages not context-free (see Theorem 1.5.5). In this section we state and
prove a more powerful version of the pumping lemma known as Ogden’s
lemma.
Recall that the statement of the ordinary pumping lemma refers to a suf-
ﬁciently long string z, which we write as z = uvwxy with |vwx| ≤n and
|vx| ≥1. By contrast, Ogden’s lemma permits us to identify certain symbols in
the string z as “marked”and consider only repetitions involving these symbols.
Lemma 4.3.1. Let L be a CFL generated by a grammar G with k variables,
where the right-hand side of every production is of length ≤d. Set n = dk+1.
Then for all z ∈L with |z| ≥n, if n or more symbols of z are marked arbitrarily,
there exists a decomposition z = uvwxy such that
(a) vx has at least one marked symbol.
(b) vwx has at most n marked symbols.
(c) There exists a variable A in G such that S =⇒∗uAy, A =⇒∗vAx,and
A =⇒∗w.
Hence, uviwxiy ∈L for all i ≥0.

4.3 Ogden’s lemma
113
u
v
w
x
y
S
A
A
Figure 4.1: Proof of Ogden’s lemma
Note that by marking every symbol of z, we obtain the ordinary pumping
lemma for CFLs as a special case of Ogden’s lemma.
Proof. Let z ∈L with |z| ≥n = dk+1, and let T be a derivation tree for z. For
each vertex γ in T , we let m(γ ) be the number of marked descendants of γ .
We iteratively construct a path P in T having the property that for all vertices
α in P, we have m(α) > 0.
First, we add the root of T to P. If α is the last vertex added to P, we
consider the children of α. If α has no children, we stop. If α has only one
child β with m(β) > 0, add β to P and continue the construction starting with
β. Otherwise, if two or more children of α have marked descendants, call α a
branch point, pick a child β that maximizes the number of marked descendants,
add β to P, and continue the construction starting with β.
Now let the branch points on P, from top to bottom, be α0, α1, α2, . . . , αj. By
our construction, m(αi+1) ≥1
d m(αi) for 0 ≤i < j. Now m(α0) ≥dk+1 (since z
has ≥n marked descendants) and so inductively we get m(α1) ≥1
d m(α0) ≥dk,
m(α2) ≥1
d m(α1) ≥dk−1, and so forth, until m(αj) ≥dk−j+1. But m(αj) ≤d
(since otherwise αj could not be the last branch point). It follows that d ≥
dk+1−j, and so 1 ≥k + 1 −j. Thus, j ≥k.
Hence there are at least k + 1 branch points. Each branch point is labeled
with a variable, and there are only k variables, so among the last k + 1 branch
points there are two labeled with the same variable, say A. Thus we have a
situation like that depicted in Figure 4.1.
It follows that there exist strings u, v, w, x, y ∈∗such that S
∗
=⇒uAy,
A
∗
=⇒vAx, and A
∗
=⇒w. Furthermore, vwx is the yield of the subtree of T
rooted at the branch point p labeled with the higher occurrence of A, and this
branch point has at most k other branch points below it in the path P. So p = αi
for i ≥j −k. Now αj has at most d marked descendants; αj−1 has at most d2
marked descendants, and so on, so αi has at most dk+1 marked descendants.
Hence, vwx has no more than dk+1 = n marked symbols. Since the higher

114
4
Context-free grammars and languages
occurrence of A is a branch point, we know it has at least two children, say β
and γ , with marked descendants. Now w is the yield of one of these children,
so the other child with marked descendants yields at least one symbol in vx.
By induction on i we get S
∗
=⇒uAy
∗
=⇒uviAxiy
∗
=⇒uviwxiy and so
uviwxiy ∈L(G). This completes the proof of Ogden’s lemma.
Example 4.3.2. Let us prove that
L = {aibjck : i = j or j = k but not both}
is not context-free. Note that the ordinary pumping lemma does not sufﬁce
to prove L not context-free, since, for example, if we choose z = anbncr,
r ̸= n, we cannot rule out the possibility that vx contains nothing but c’s, so
by pumping we cannot force the number of b’s to equal the number of c’s.
However with Ogden’s lemma and z = anbncn+n!, we can mark the a’s.
(This idea is sometimes called the n! trick.) Let z = uvwxy be the resulting
decomposition. Since vx must have at least one marked symbol, either (a)
v = ϵ and x contains an a or (b) v contains an a. In case (a), x cannot contain
any b’sor c’s, for otherwise uv2wx2y ̸∈L. Hence, x consists only of a’s, say
x = al for some l ≥1. Then uv2wx2y = an+lbncn+n! ̸∈L.
In case (b), v cannot contain any b’sor c’s, for otherwise uv2wx2y ̸∈L.
Hence, v consists only of a’s, say v = al for some l with 1 ≤l ≤n. Now x
cannot contain two types of letters, for if it did, uv2wx2y ̸∈L. So either x = am,
or x = bm, or x = cm for some m. If x = am or cm, then uv2wx2y ̸∈L. If
x = bm, then either m ̸= l or m = l. If m ̸= l, then, by choosing i = 0, we get
that uwy has unequal numbers of a’s and b’s, so it is not in L. If m = l, then
choosing i = n!/l + 1 we get uv(n!/l)+1wx(n!/l)+1y = an+n!bn+n!cn+n! ̸∈L.
Common Error 4.3.3. Note that Ogden’s lemma does not restrict the length of
v, w, or x other than the restriction implied by the number of marked symbols.
In particular, Ogden’s lemma does not necessarily imply that |v|, |w|, |x| ≤n.
4.4 Applications of Ogden’s lemma
In this section we look at two additional applications of Ogden’s lemma. The
ﬁrst concerns inherent ambiguity, while the second concerns the optimality of
a common construction for converting from a PDA to a CFG.
As we have seen in Section 1.5, a context-free grammar can be ambiguous;
that is, at least one word in the language generated has at least two different
leftmost derivations. Often it is possible to generate the language using a
different grammar that is unambiguous. However, this is not always the case:

4.4 Applications of Ogden’s lemma
115
there exist CFLs for which every grammar is ambiguous. Such languages
are called inherently ambiguous. Note that inherent ambiguity is a property of
languages, while ambiguity is a property of grammars. It is notoriously difﬁcult
to prove that a given language is inherently ambiguous.
Theorem 4.4.1. Let
La = {aibjck : i = j or j = k}.
Then La is a CFL that is inherently ambiguous.
The reason why this theorem is difﬁcult to prove is that we have to show that,
no matter what context-free grammar is chosen to generate La, some string with
at least two different derivations exists. While this is believable—intuiti vely
we will need some variable that derives strings of the form aibicj and another
that derives strings of the form aibjcj, and hence some string of the form
aibici will be derived by both variables—note that it is not true that the same
string be derived ambiguously for all grammars. For every string you pick,
there is a CFG that generates La and generates that particular string in only
one way.
Proof.
Clearly, La is context-free, as it is the union of {aibi : i ≥0}c∗and
a∗{bici : i ≥0}, both of which are CFLs.
Let n be the constant in Ogden’s lemma. Consider the string z = ambmcm+m!,
where m = max(n, 3), and mark the a’s. By Ogden’s lemma there exists a
factorization z = uvwxy and a variable A such that S =⇒∗uAy, A =⇒∗vAx,
and A =⇒∗w. Thus, uviwxiy ∈La for all i ≥0; take i = 2 to get α :=
uv2wx2y ∈La. Also, vx has at most m b’s, because that is the total number of
b’sin z. Since m ≥3, we have m < m!, and so |α|b ≤2m < m + m! ≤|α|c.
Thus, |α|a = |α|b. Thus, vx contains the same number of a’sas b’s and must
contain at least one a since vx contains at least one marked symbol. Now
v and x can each contain only one type of symbol, for otherwise α ̸∈La.
Thus, v = aj, x = bj, for 1 ≤j ≤m. Now let i = m!
j + 1 to get a deriva-
tion of
β = uviwxiy = am+m!bm+m!cm+m!
by using the production A =⇒∗vAx i times.
Now play exactly the same game starting with the string z′ = am+m!bmcm,
but this time, mark the c’s. Once again, we get a factorization z′ = u′v′w′x′y′
and a variable A′ such that S =⇒∗u′A′y′, A′ =⇒∗v′A′x′, and A′ =⇒∗w′.
And once again, we get that v′ = bj ′, x′ = cj ′ for some j ′, 1 ≤j ′ ≤m. Now

116
4
Context-free grammars and languages
let i′ = m!
j ′ + 1 to get a derivation of
β = u′v′i′w′x′i′y′ = am+m!bm+m!cm+m!.
I claim these two derivations are different. To see this, observe that ﬁrst
obtains all but m −j of its b’s through the production A =⇒∗ajAbj, while
the second obtains all but m −j ′ of its b’s through the production A′ =⇒∗bj ′
A′bj ′.
Now let us turn to our second application of Ogden’s lemma.
In Section 1.5 we examined a well-known construction for ﬁnding a context-
free grammar that generates the same language as that accepted by a given PDA
(accepting by empty stack). Students seeing this construction for the ﬁrst time
are often surprised at how difﬁcult it seems. After all, the construction is not
particularly obvious. This leads to the following natural question: can the triple
construction be simpliﬁed? Perhaps surprisingly, the answer is no, at least with
respect to the number of variables, as the following theorem shows. As a bonus,
we get a nice application of Ogden’s lemma.
Theorem 4.4.2. For all integers n, p ≥1, there exists a PDA M = M(n, p)
with n states and p stack symbols such that every context-free grammar G
generating Le(M) uses at least n2p variables.
Proof. Let M = (Q, , , δ, q1, Z1, ∅), where
• Q = {qi : 1 ≤i ≤n};
•  = {Zj : 1 ≤j ≤p};
•  = {ai,j, ti,j : 1 ≤i ≤n, 1 ≤j ≤p} ∪{bk, dk : 1 ≤k ≤n};
and δ is deﬁned as follows:
δ(q1, ai,j, Z1) = {(qi, Zj)}
δ(qi, ti,j, Zj) = {(qi, ZjZj)}
δ(qi, bk, Zj) = {(qk, Zj)}
δ(qk, dk, Zj) = {(qk, ϵ)}.
Let L = Le(M) and let G = (V, , P, S) be a context-free grammar such that
L(G) = L. Let m be the constant in Ogden’s lemma applied to G. For every
triple (i, j, k) such that 1 ≤i, k ≤n, 1 ≤j ≤p, let
z = z(i, j, k) = ai,jtm
i,jbkdm+1
k
.

4.4 Applications of Ogden’s lemma
117
We claim that M accepts z for we have the accepting computation
(q1, z, Z1) ⊢(qi, tm
i,jbkdm+1
k
, Zj)
∗⊢(qi, bkdm+1
k
, Zm+1
j
)
∗⊢(qk, dm+1
k
, Zm+1
j
)
∗⊢(qk, ϵ, ϵ).
Now, using Ogden’s lemma, mark all the symbols ti,j and dk; hence, 2m + 1 ≥
m symbols are marked. By Ogden’s lemma we can write z = uvwxy such that
vx has at least one marked letter
vwx has ≤m marked letters,
and there exists a variable Ai,j,k ∈V such that
S
∗
=⇒uAi,j,ky
∗
=⇒uvrAi,j,kxry
∗
=⇒uvrwxry
for all r ≥0. This gives n2p variables. Now we must show they are all distinct.
If uwy ∈Le(M), it must contain one more d letter than t letter, so vx must
contain the same number of t’sas d’s. By Ogden’s lemma, vx contains at least
one t or d, so it must contain both a t and a d. Since vwx has ≤m t’s and d’s, u
and y must be nonempty. Hence there exist integers s, σ ≥1 with s + σ ≤m
and
u = ai,jtm−s
i,j ;
vwx = ts
i,jbkdσ
k ;
y = dm+1−σ
k
.
Let Ai,j,k be a variable in G associated with z(i, j, k) = uvwxy. Let Ai′,j ′,k′
be a variable in G associated with z(i′, j ′, k′) = u′v′w′x′y′. Suppose Ai,j,k =
Ai′,j ′,k′. Then we have
S
∗
=⇒uAi,j,ky = uAi′,j ′,k′y
∗
=⇒uv′w′x′y = ai,jtm−s
i,j ts′
i′,j ′bk′dσ ′
k′ dm+1−σ
k
∈L.
But after reading ai,jtm−s
i,j , the conﬁguration of M is of the form (qi, −, Zm−s+1
j
),
and it will now crash on reading ti′,j ′ unless i′ = i and j ′ = j. Simi-
larly, after having read uv′w′x′, the conﬁguration of M is of the form
(qk′, −, Zm+1−s+s′−σ ′
j
) and will crash on reading dk unless k = k′. Thus,
Ai,j,k = Ai′,j ′,k′ implies that (i, j, k) = (i′, j ′, k′).

118
4
Context-free grammars and languages
4.5 The interchange lemma
There are not many tools known to prove that a given language is not a CFL.
In this section we examine another tool: the interchange lemma. Roughly
speaking, the interchange lemma says that if a language is context-free, then
there is a large subset of the words of length n such that one can take two
strings from this subset, and interchange the subwords appearing at the same
position, and still get strings in the language. We then use this tool to prove that
the language of square-containing words is not a CFL.
Lemma 4.5.1. Let L be a CFL. Then there is a constant c > 0, depending on
L, such that for all integers n ≥2, all subsets R ⊆L ∩n, and all integers
with 2 ≤m ≤n, there exists a subset Z ⊆R, Z = {z1, z2, . . . , zk} such that
k ≥
|R|
c(n+1)2 and there exist decompositions zi = wixiyi, 1 ≤i ≤k, such that
(a) |w1| = |w2| = · · · = |wk|;
(b) |y1| = |y2| = · · · = |yk|;
(c) m
2 < |x1| = |x2| = · · · = |xk| ≤m; and
(d) wixjyi ∈L for all i, j, 1 ≤i, j ≤k.
First, we prove two technical lemmas.
Lemma 4.5.2. Let G = (V, , P, S) be a context-free grammar in Chomsky
normal form generating L. Let m be an integer with m ≥2. Then for all strings
z ∈L with |z| ≥m, there exists a variable A ∈V and a derivation
S
∗
=⇒wAy
∗
=⇒wxy = z
with m
2 < |x| ≤m.
Proof.
Let p be root of T , a parse tree for z. Clearly, p has ≥m descendants
that are terminal symbols. If p has exactly m descendants that are terminals,
take w = y = ϵ, A = S, and x = z. So assume p has >m descendants. Now
repeatedly replace p with the child that has the larger number of terminal
descendants, until p has ≤m descendants. Since the parent of p had >m
descendants, p must have >m/2 descendants. Let A be the label of p. Then
there exist strings w, y with S
∗
=⇒wAy and A
∗
=⇒x with m
2 < |x| ≤m.
We now deﬁne certain sets involving derivations. Choose a subset R ⊆
L ∩n. For integers n1, n2 with 0 ≤n1, n2 ≤n deﬁne
Qn,R(n1, A, n2) : = {z ∈R : there exists a derivation S
∗
=⇒wAy
∗
=⇒
wxy = z, |w| = n1, |y| = n2}.

4.5 The interchange lemma
119
Lemma 4.5.3. Let G = (V, , P, S) be a context-free grammar in Chomsky
normal form generating L. Let 2 ≤m ≤n. Then for all subsets R ⊆L ∩n,
there exist integers 0 ≤n1, n2 ≤n such that
m
2 < n −n1 −n2 ≤m
and a variable A ∈V such that
|Qn,R(n1, A, n2)| ≥
|R|
|V |(n + 1)2 .
Proof. We have
R =

A∈V
0≤n1,n2≤n
Qn,R(n1, A, n2) =

A∈V
0≤n1,n2≤n
m
2 <n−n1−n2≤m
Qn,R(n1, A, n2),
where in the last line we used Lemma 4.5.2. Thus we have written R as the
union of at most (n + 1)2|V | sets, so at least one of these sets has
|R|
|V |(n+1)2
elements.
We can now prove the interchange lemma.
Proof.
Let G = (V, , P, S) be a context-free grammar in Chomsky nor-
mal form generating L −{ϵ}. Let c = |V |, and choose a subset R ⊆L ∩n.
Then by Lemma 4.5.3 there exist integers 0 ≤n1, n2 ≤n with
m
2 < n −
n1 −n2 ≤m and a variable A such that |Qn,R(n1, A, n2)| ≥
|R|
|V |(n+1)2 . Take
Z = Qn,R(n1, A, n2). Every string zi ∈Z has a derivation of the form
S
∗
=⇒wiAyi
∗
=⇒wixiyi = zi
with |wi| = n1 and |yi| = n2. Thus,
S
∗
=⇒wiAyi
∗
=⇒wixjyi ∈L.
The proof of the interchange lemma is now complete.
We now give an application of the interchange lemma. Let Li be the language
{xyyz : x, z ∈∗, y ∈+},
where  = {0, 1, . . . , i −1}. Thus, Li is the language of all words containing
(nontrivial) squares over an alphabet of i letters. It is not hard to see that L1
and L2 are context-free. (In fact, they are regular, by Exercise 2.3.) We will use
the interchange lemma to prove the following theorem.
Theorem 4.5.4. The language Li is not a CFL for i ≥3.

120
4
Context-free grammars and languages
Proof.
First we prove the result for i = 6. At the end we explain how to get
the result for all i ≥3.
Assume that L6 is context-free. Let c be the constant in the interchange
lemma and choose n sufﬁciently large so that it is divisible by 8 and
2n/4
c(n + 1)2 > 2n/8.
(The reason for this choice will be clear in a moment.)
By Theorem 2.5.2 there exists a squarefree string of every length over a
three-letter alphabet. Choose such a string r′ of length n
4 −1 over {0, 1, 2} and
deﬁne r = 3r′. Deﬁne
An := {rrXs : s ∈{4, 5}n/2},
where X is the perfect shufﬂe introduced in Section 1.2. Thus, every string in
An is of length n.
The strings in An have the following useful properties:
1. If z1 = w1x1y1, and z2 = w2x2y2 are strings in An with |w1| = |w2|,
|x1| = |x2|, and |y1| = |y2|, then w1x2y1 and w2x1y2 are both in An, too. For
z1 = rrXs, z2 = rrXs′, and substituting x2 for x1 leaves the symbols corre-
sponding to rr the same, while possibly changing the symbols corresponding
to s. But since any s is permissible, this change does not affect membership
in An.
2. If z ∈An, then z contains a square if and only if z is a square. For if z
contained a square, then considering only the symbols of z in {0, 1, 2, 3}, we
would still have a square. But this is impossible, since r is squarefree.
Now deﬁne the following subset of An:
Bn := L6 ∩An
= {rrXss : s ∈{4, 5}n/4}.
Clearly, |Bn| = 2
n
4 . Since Bn ⊆L6, the interchange lemma applies with m =
n/2 and R = Bn. Then there is a subset Z ⊆Bn, Z = {z1, z2, . . . , zk} with
zi = wixiyi satisfying the conclusions of that lemma. In particular, k = |Z| ≥
2n/4
c(n+1)2 > 2n/8.
There are now two cases to consider, and each will lead to a contradiction.
Case 1: There exist indices g, h such that xg ̸= xh.
Case 2: There do not exist such indices.
In Case 1, we know from the interchange lemma that wgxhyg ∈L. Since
xg ̸= xh, there must be a 4 or 5 in one-half of zg that is changed. But since
|xh| ≤n/2, the corresponding symbol is not changed in the other half of the

4.6 Parikh’s theorem
121
string. So wgxhyg cannot be a square. Since wgxhyg ∈An, by observation (2)
given before see that wgxhyg cannot contain a square, either. Thus, wgxhyg ̸∈L,
a contradiction.
In Case 2, all the xi must be the same. So there are at least n/4 positions in
which all the zi agree. This set of n/4 positions contains at least n/8 4’s and
5’s. There are only at most n/8 positions where we are free to make a choice
between 4 and 5. Thus, |Z| ≤2n/8, a contradiction.
It now follows that L6 is not a CFL.
We now show how to get the result for Li, i ≥3. In Exercise 34 you are
asked to show that the following morphism h is squarefree-preserving; that is,
x is squarefree if and only if h(x) is squarefree.
0 →0102012022012102010212
1 →0102012022201210120212
2 →0102012101202101210212
3 →0102012101202120121012
4 →0102012102010210120212
5 →0102012102120210120212.
Now suppose L3 is context-free. Then, by Theorem 4.1.4, the language
h−1(L3) is context-free. But since h is squarefree-preserving, h−1(L3) is the
language of all square-containing words over a six-letter alphabet, that is, L6,
which we have just proved non-context-free, a contradiction.
Finally, suppose Li is context-free for some i ≥3. Then Li ∩{0, 1, 2}∗is
context-free by Theorem 1.5.8. But this is L3, a contradiction.
4.6 Parikh’s theorem
In Section 4.2 we saw that every CFL over a unary alphabet is regular. In
this section we consider a beautiful generalization of this result, called Parikh’s
theorem. Parikh’s theorem has been described as “amongthe most fundamental,
yet subtly difﬁcult to prove, in the theory [of context-free languages].”
If L ⊆a∗is a unary language, then we can consider the associated set
lengths(L) = {i : ai ∈L}.
As we have seen in the proof of Theorem 4.2.1, if L is context-free, then
lengths(L) has the property that it is the union of ﬁnitely many arithmetic
progressions, that is, sets of the form {k + it : i ≥0}. It is natural to wonder
if this observation can be generalized to languages over larger alphabets.

122
4
Context-free grammars and languages
It can. To do so, we need to introduce the concepts of linear and semilinear
sets. Let k be an integer ≥1. A subset A of Nk is said to be linear if there exist
u0, u1, . . . , ur ∈Nk such that
A = {u0 + a1u1 + · · · + arur : a1, a2, . . . , ar ∈N}.
(4.1)
The right-hand side of Eq. (4.1) is sometimes written as u0 + ⟨u1, u2, . . . , ur⟩.
A subset A of Nk is said to be semilinear if it is the union of ﬁnitely many
linear sets.
Next, we introduce the Parikh map ψ. We start with an ordered alphabet
 = {a1, a2, . . . , ak}. Then ψ : ∗→Nk maps a word w ∈∗to the vector
of length k given by
(|w|a1, |w|a2, . . . , |w|ak).
For example, if  = {a, b, c, d, e} and is ordered by alphabetic order, then
ψ(beaded) = (1, 1, 0, 2, 2). The map ψ can be extended to languages L as
follows:
ψ(L) =

w∈L
{ψ(w)}.
Note that the Parikh map is essentially the commutative image of a word and
that ψ(xy) = ψ(x) + ψ(y) for all strings x, y ∈∗.
A very useful result about semilinear sets is the following:
Theorem 4.6.1. For any k ≥1, the class of semilinear sets of Nk is closed
under union, intersection, and complement.
Proof. The result about union is clear. Unfortunately, the proof for intersection
and complement is quite difﬁcult and we omit it. A proof can be found in the
references at the end of this chapter.
Now, Parikh’s theorem says that if L is a CFL, then ψ(L) is semilinear. Note
that the converse does not hold (see Example 4.6.2(d)).
Example 4.6.2. Let us look at some examples:
(a) L0 = {0, 01}∗. Then ψ(L1) = ⟨(1, 0), (1, 1)⟩.
(b) L1 = {0, 1}2. Then ψ(L1) = {(0, 2), (1, 1), (2, 0)}. (This can be written as
((0, 2) + ⟨(0, 0)⟩) ∪((1, 1) + ⟨(0, 0)⟩) ∪((2, 0) + ⟨(0, 0)⟩) .)
(c) L2 = {0n1n : n ≥1}. Then ψ(L2) = (1, 1) + ⟨(1, 1)⟩.
(d) L3 = {0n1n2n : n ≥1}. Then ψ(L3) = (1, 1, 1) + ⟨(1, 1, 1)⟩.
(e) L4 = PAL = {w ∈{0, 1}∗: w = wR}. Then
ψ(L4)=⟨(0, 2), (2, 0)⟩∪((0, 1)+⟨(0, 2), (2, 0)⟩) ∪((1, 0)+⟨(0, 2), (2, 0)⟩) .

4.6 Parikh’s theorem
123
To see this, note that we can generate a palindrome with any given Parikh
image, except the case where both coordinates are odd.
(f) L5 = {0m1m2n3n : m, n ≥1}. Then ψ(L5) = (1, 1, 1, 1) + ⟨(1, 1, 0, 0),
(0, 0, 1, 1)⟩.
(g) L6 = EQ = {w ∈{0, 1}∗: |w|0 = |w|1}. Then ψ(L6) = ⟨(1, 1)⟩.
(h) L7 = {x ∈{0, 1}∗: x is not of the form ww}. Then
ψ(L7) = ((0, 1) + X) ∪((1, 0) + X) ∪((1, 1) + X) ∪((2, 2) + X) ,
where X = ⟨(0, 2), (2, 0)⟩. The proof is left as Exercise 31.
The next theorem gives a relationship between semilinear sets and regular
languages.
Theorem 4.6.3. Let X ⊆Nk be a semilinear set. Then there exists a regular
language L ⊆∗, where  = {a1, a2, . . . , ak}, such that ψ(L) = X.
Proof.
A semilinear set is a union of a ﬁnite number of linear sets. So it
sufﬁces to show the result for a linear set T .
Let T = u0 + ⟨u1, u2, . . . , ut⟩, where ui = (vi,1, vi,2, . . . , vi,k). Now let
L = av0,1
1
av0,2
2
· · · av0,k
k
 
1≤i≤t
avi,1
1 avi,2
2
. . . avi,k
k
∗
.
Then L is regular and ψ(L) = T .
Now let us prove Parikh’s theorem. First, we need a lemma.
Lemma 4.6.4. Let G = (V, , P, S) be a context-free grammar with k vari-
ables in Chomsky normal form. Let p = 2k+1. For all integers j ≥1, if
z ∈L(G) and |z| ≥pj, every derivation S =⇒∗z has the same derivation
tree as a derivation of the form
S =⇒∗uAy
=⇒∗uv1Ax1y
=⇒∗uv1v2Ax2x1y
...
=⇒∗uv1v2 · · · vjAxj · · · x2x1y
=⇒∗uv1v2 · · · vjwxj · · · x2x1y = z,
where A ∈V , vixi ̸= ϵ for 1 ≤i ≤j, and |v1v2 · · · vjxj · · · x2x1| ≤pj.

124
4
Context-free grammars and languages
Proof. Consider a derivation tree for z. As in the proof of the ordinary pump-
ing lemma for context-free grammars, since |z| ≥pj = 2j(k+1), there must be
a path P of length ≥j(k + 1) + 1 from the root S to a terminal. This path P
contains j(k + 1) + 1 variables, so some variable must occur at least j + 1
times. Now trace a path P ′ from the last node on P backup until some variable
A occurs at least j + 1 times for the ﬁrst time. The yield of the highest A
in P ′ is of length at most 2j(k+1) = pj. Each A gives a derivation of the form
A =⇒∗viAxi; combining these with S =⇒∗uAy and the yield of the lowest A,
A =⇒∗w, gives a derivation of the desired form with |v1v2 · · · vjxj · · · x2x1| ≤
pj. Each vixi is nonempty because these correspond to a variable other than
A, which must derive a nonempty string.
We are now ready for the proof of Parikh’s theorem.
Theorem 4.6.5. If L is a CFL, then ψ(L) is semilinear.
Proof.
We can assume without loss of generality that L does not contain ϵ,
for if it does, we can ﬁrst compute ψ(L) for the CFL L −{ϵ} and then add in
the vector (0, 0, . . . , 0).
So let G = (V, , P, S) be a context-free grammar in Chomsky normal
form generating L −{ϵ}, and let p be the constant in the previous lemma.
Let U ⊆V be any set of variables containing S. Deﬁne LU to be the set
of all words generated by G for which there exists a derivation including
precisely the variables in U—no more, no less. Now there are only ﬁnitely many
LU, and L = 
{S}⊆U⊆V LU. Thus, it sufﬁces to show that each LU is semi-
linear.
Now ﬁx an arbitrary U and assume that all derivations use only the variables
from U. Let ℓ= |U|, and deﬁne the languages
E = {w ∈LU : |w| < pℓ}
and
F = {vx : 1 ≤|vx| ≤pℓand A =⇒∗vAx for some variable A ∈U}.
Note that both E and F are ﬁnite, so ψ(EF ∗) is semilinear. We will show that
ψ(LU) = ψ(EF ∗).
First, let us show that ψ(LU) ⊆ψ(EF ∗). Let z ∈LU; we prove that ψ(z) ∈
ψ(EF ∗) by induction on |z|. The base case is |z| < pℓ. In this case, z ∈E ⊆
EF ∗, so ψ(z) ∈ψ(EF ∗).

4.6 Parikh’s theorem
125
For the induction step, assume |z| ≥pℓand z ∈LU. By Lemma 4.6.4, there
is a derivation for z that can be written in the form
S =⇒∗uAy
(d1) =⇒∗uv1Ax1y
(d2) =⇒∗uv1v2Ax2x1y
...
(dℓ) =⇒∗uv1v2 · · · vℓAxℓ· · · x2x1y
=⇒∗uv1v2 · · · vℓwxℓ· · · x2x1y = z,
where the derivations have been labeled d1, d2, . . . , dℓ.
Now, with each of the ℓ−1 variables B ∈U −{A}, associate (arbitrarily)
a derivation di if B occurs in di. Since there are ℓlabeled derivations and only
ℓ−1 variables in B ∈U −{A}, there must be at least one derivation di that is
not associated with any variable. We can therefore omit di to get a derivation
S =⇒∗uv1 · · · vi−1vi+1 · · · vℓwxℓ· · · xi+1xi−1 · · · x1y = z′,
where z′ ∈LU. Since |z′| < |z|, by induction we have ψ(z′) ∈ψ(EF ∗). Now
vixi ∈F, so ψ(z) = ψ(z′vixi) ∈ψ(EF ∗), as desired.
Now we prove the other direction. Let z ∈EF ∗; then z = e0f1f2 · · · ft for
some t ≥0, where e0 ∈E and fi ∈F for 1 ≤i ≤t. We prove by induction on
t that ψ(z) ∈ψ(LU).
The base case is t = 0. In that case, z = e0 ∈E. Then by deﬁnition, z ∈LU,
so ψ(z) ∈ψ(LU).
For the induction step, assume z = e0f1 · · · ft, where e0 ∈E and each
fi ∈F. Since ft ∈F, we can write ft = vx, where 1 ≤|vx| ≤pl and
A =⇒∗vAx. By induction we know ψ(e0f1 · · · ft−1) = ψ(z′) for some
z′ ∈LU. But since z′ ∈LU, there exists a derivation of z′ using the vari-
able A, say S =⇒∗v′Ax′ =⇒∗v′w′x′ = z′. But then S =⇒∗v′Ax′ =⇒∗
v′vAxx′ =⇒∗v′vwxx′ = z′′, so z′′ ∈LU. Now ψ(z′′) = ψ(z′) + ψ(vx) =
ψ(e0f1 · · · ft−1) + ψ(ft) = ψ(z), so ψ(z) ∈ψ(LU), as desired.
Sometimes Parikh’s theorem is useful for proving certain languages non-
context-free when other methods, such as the pumping lemma, fail.
Example 4.6.6. Let us prove that L = {aibj : j ̸= i2} is not context-free.
Suppose it is. Then by Parikh’s theorem, the set ψ(L) = {(i, j) : j ̸= i2} is
semilinear. By Theorem 4.6.1, T := ψ(L) = {(i, i2) : i ≥0} is semilinear.
But by Theorem 4.6.3, there exists a regular language R ⊆{a, b}∗such that
ψ(R) = T . Now consider the morphism h : {a, b}∗→c∗deﬁned by h(a) =

126
4
Context-free grammars and languages
h(b) = c. Then h(R) = {cn2+n : n ≥0}, which is easily seen to be nonregular
using the pumping lemma, a contradiction (since regular languages are closed
under application of a morphism). Thus, L is not context-free.
You might ﬁnd this proof a bit unsatisfactory, as it depends on Theorem 4.6.1,
which we did not prove. We can prove the result about L without Theorem 4.6.1,
but it requires a bit more work. To do so, assume that ψ(L) is semilinear.
Then it is the union of linear sets, each of which can be written in the form
u0 + ⟨u1, u2, . . . , uj⟩. Let d′ be the maximum of all the integers occurring in
all vectors deﬁning ψ(L), and let d = max(d′, 3). Now let m = d! and n =
(d!)2 −d!. Clearly, n ̸= m2, so (m, n) ∈ψ(L), and so (m, n) belongs to some
linear set u0 + ⟨u1, u2, . . . , uj⟩. We ﬁrst claim that some ui, 1 ≤i ≤j, must be
of the form (0, r) for some r, 1 ≤r ≤d. For otherwise, all the ﬁrst coordinates
of the ui, 1 ≤i ≤j, are at least 1. Since (m, n) ∈u0 + ⟨u1, u2, . . . , uj⟩, we
can write
(m, n) = (q0, r0) +

1≤i≤j
ai(qi, ri),
where ui = (qi, ri). Since each qi ≥1, it follows that m ≥a1 + a2 + · · · + aj.
Now n = r0 + a1r1 + · · · + ajrj ≤(m + 1)d. Now, since d ≥3, we have d! >
d + 2 and so n = (d!)2 −d! > (d + 2)d! −d! = (d + 1)d! > d(d! + 1) =
d(m + 1), a contradiction.
Thus some ui is of the form (0, r). It follows that (m, n) + t(0, r) ∈ψ(L)
for all t ≥0. Now take t = d!/r; this is an integer because 1 ≤r ≤d. Then
(m, n) + t(0, r) = (m, n) + d!
r (0, r) = (m, n + d!) = (d!, (d!)2) ∈ψ(L),
a contradiction.
4.7 Deterministic context-free languages
An interesting subclass of the CFLs is the deterministic context-free languages,
or DCFLs.
To formally deﬁne this class, recall that a PDA is given by a 7-tuple M =
(Q, , , δ, q0, Z0, F). The PDA M accepts a string x by ﬁnal state if and
only if (q0, x, Z0)
∗⊢(q, ϵ, α) for some q ∈F and α ∈∗. We say that M is
deterministic if the following two conditions hold:
(a) |δ(q, a, A)| ≤1 for all q ∈Q, a ∈ ∪{ϵ}, and A ∈;
(b) for all q ∈Q and A ∈, if δ(q, ϵ, A) ̸= ∅, then δ(q, a, A) = ∅for all
a ∈.

4.7 Deterministic context-free languages
127
The intent is that condition (a) prevents a choice between two non-ϵ moves,
while condition (b) prevents a choice between an ϵ move and a non-ϵ move.
If L is accepted by a DPDA by ﬁnal state, then we say that L is a DCFL.
One of the most important observations about the DCFLs is that this class
is closed under complement. Recall how we proved that the class of regular
languages is closed under complement: we took a DFA M and created a new
machine M′ to accept L(M) by changing the “ﬁnality”of each state: ﬁnal states
became nonﬁnal and vice versa. We would like to do this with a DPDA, but
two problems occur.
First, a DPDA M may enter a state in which it never consumes additional
input symbols. This can occur because M has no deﬁned move, or because the
stack has been emptied, or because it enters an inﬁnite loop on ϵ-transitions.
If M never reads past x, then it cannot accept any string of the form xy for
y ∈+. If we simply changed the “ﬁnality”of each state of M to obtain M′,
then M′ would also never read past x, and so it would also not accept any string
of the form xy for y ∈+. We can ﬁx this problem by forcing M to scan its
entire input.
Second, after reading a string x, M may enter a sequence of states on
ϵ-transitions. Suppose that it enters at least one ﬁnal state and at least one
nonﬁnal state. Then by our deﬁnition of acceptance in PDAs, x would be
accepted. However, if we simply change the ﬁnality of each state to get M′,
then x would still be accepted, which it should not. We can ﬁx this problem by
recording, in between states where M actually reads an input, whether or not
M has seen a ﬁnal state so far.
Lemma 4.7.1. If M = (Q, , , δ, q0, Z0, F) is a DPDA, then there exists
a DPDA M′ = (Q′, , ′, δ′, q′
0, X0, F ′) such that L(M) = L(M′) and M′
always scans its entire input. More formally, for all inputs x ∈∗, there exists
a computation in M′ such that (q′
0, x, X0)
∗⊢(q, ϵ, α) for some q ∈Q′ and
α ∈′∗.
Proof.
As earlier, the basic idea is simple: we add transitions, so there is
always a next move, and we add transitions to avoid inﬁnite loops on ϵ-moves.
The actual implementation is a bit complex.
Suppose
M = (Q, , , δ, q0, Z0, F).
We
deﬁne
a
machine
M′ =
(Q′, , ′, δ′, q′
0, X0, F ′), where Q′ = Q ∪{q′
0, d, f }, ′ =  ∪{X0}, F ′ =
F ∪{f }, and δ′ is deﬁned as follows:
(a) δ′(q′
0, ϵ, X0) = {(q0, Z0X0)}.
(b) If δ(q, a, X) = ∅and δ(q, ϵ, X) = ∅, then δ′(q, a, X) = {(d, X)}.
(c) δ′(q, a, X0) = {(d, X0)} for all q ∈Q and a ∈.

128
4
Context-free grammars and languages
(d) δ′(d, a, X) = {(d, X)} for all a ∈ and X ∈′.
(e) If M enters an inﬁnite loop on ϵ-transitions from the conﬁguration
(q, ϵ, X), then δ′(q, ϵ, X) = {(d, X)}, provided no state encountered in
the inﬁnite loop is ﬁnal, and δ′(q, ϵ, X) = {(f, X)} otherwise.
(f) δ′(f, a, X) = {(d, X)} for all a ∈ and X ∈′.
(g) For all q ∈Q, a ∈ ∪{ϵ}, X ∈ for which δ′(q, a, X) has not been
deﬁned earlier, δ′(q, a, X) = δ(q, a, X).
We leave the formal proof that this construction works to the reader.
We can now prove the fact that the DCFLs are closed under complement.
Theorem 4.7.2. If L is a DCFL then so is L.
Proof. If L is a DCFL, it is accepted by a DPDA M = (Q, , , δ, q0, Z0, F).
By Lemma 4.7.1 we may assume that M scans its entire input. We now modify
M by adding a code to the state that says whether or not a ﬁnal state has been
seen since the last “real”(i.e., non-ϵ) input. The meaning of y in the second
component is that a ﬁnal state has been seen; the meaning of n is that it has
not; and the meaning of A is that the machine is about to read another “real”
input and has not entered a ﬁnal state since the last “real”input.
Formally, let M′ = (Q′, , , δ′, q′
0, Z0, F ′), where Q′ = Q × {y, n, A};
q′
0 =

[q0, y],
if q0 ∈F;
[q0, n],
if q0 ̸∈F
and F ′ = Q × {A}. We also deﬁne δ′ as follows: if δ(q, a, X) = (p, γ ) for
a ∈, then
δ′([q, y], a, X) :=

([p, y], γ ),
if p ∈F;
([p, n], γ ),
if p ̸∈F;
δ′([q, A], a, X) :=

([p, y], γ ),
if p ∈F;
([p, n], γ ),
if p ̸∈F;
δ′([q, n], ϵ, X) := ([q, A], X).
If δ(q, ϵ, X) = (p, γ ), then
δ′([q, y], ϵ, X) := ([p, y], γ )
δ′([q, n], ϵ, X) :=

([p, y], γ ),
if p ∈F;
([p, n], γ ),
if p ̸∈F;

4.7 Deterministic context-free languages
129
We now argue that L(M′) = L(M). To see this, suppose x = a1a2 · · · an ∈
L(M). Then M enters a ﬁnal state after reading an. In this case, the second
component of the simulating M′ will be y and cannot enter a state with second
component A before the next input symbol. Hence, x is not accepted by M′. If
x ̸∈L(M), then M never enters a ﬁnal state after reading an. In this case, the
second component of the simulating M′ will be n, and eventually all ϵ-moves
will be exhausted, and the machine will attempt to read a “real”input. At this
point the second component is changed to A and M′ accepts.
Theorem 4.7.2 is an important tool in proving languages not DCFLs.
Example 4.7.3. Let us prove that the language
L := {w ∈{a, b}∗: w ̸= xx for all x ∈{a, b}∗}
is not a DCFL.
Assume that L is a DCFL. Then
L = {xx : x ∈{a, b}∗}
is a DCFL, since DCFLs are closed under complement. Now every DCFL
is a CFL, so L is a CFL. But L is not a CFL by the pumping lemma (see
Exercise 1.16).
Sometimes Theorem 4.7.2 is of no help in proving that a particular CFL is not
a DCFL. For example, consider PAL = {x ∈{a, b}∗: x = xR}, the palindrome
language. Now PAL is also a CFL—see Exercise 1.22—so Theorem 4.7.2 alone
cannot be used to show that PAL is not a DCFL. In these cases the following
theorem may be useful.
Theorem 4.7.4. Let L ⊆∗be a language such that each Myhill–Nerode
equivalence class is of ﬁnitecardinality. Then L is not a DCFL.
Proof. Suppose L is a DCFL. Then, by Lemma 4.7.1, there exists a DPDA M
accepting L that scans its entire input. It is now easy to modify this DPDA so
that every move is either a pop or a push; it never replaces a symbol X on top
of the stack with a string of the form γ Y with X ̸= Y (see Exercise 26).
Let x ∈∗. Let yx be a string such that the height of M’s stack after
processing xyx is as small as possible. Our assumption about the moves of M
now implies that after processing xyx, M never pops or changes any of the
symbols currently on the stack. Note that we could have xyx = zyz for x ̸= z,
but there are still inﬁnitely many strings of the form u = xyx for which after
processing u, M never pops or changes any of the symbols currently on the
stack.

130
4
Context-free grammars and languages
From these inﬁnitely many strings, by the inﬁnite pigeonhole principle, there
must be an inﬁnite subset of the strings u such that M is in the same state after
processing u (including any ϵ-moves needed to minimize the stack height).
Further, there must be an inﬁnite subset of these strings such that the top stack
symbol after processing u is the same. Finally, there must be an inﬁnite subset
of these strings such that either all of them are in L or all of them are in L.
Thus there is an inﬁnite set S of strings {u1, u2, . . .} such that
(q0, ui, Z0)
∗⊢(q, ϵ, Aαi)
for some q ∈Q, αi ∈∗, A ∈, and all i ≥1, where this is a derivation of
minimal stack height. Hence for all z ∈+ and all i ≥1, we have
(q0, uiz, Z0)
∗⊢(q, z, Aαi).
Since A is never popped from the stack in this computation, the αi cannot
contribute in any way to future conﬁgurations. It follows that, for z ̸= ϵ, and
for all i and j, M accepts uiz if and only if it accepts ujz. On the other hand,
M accepts ui if and only if it accepts uj because we chose the set S such
that either S ⊆L or S ⊆L. Hence all the (inﬁnitely many) ui are in the same
Myhill–Nerodeequivalence class for L, a contradiction.
Corollary 4.7.5. The language PAL = {x ∈{a, b}∗: x = xR} is not a DCFL.
Proof.
We leave it to the reader as Exercise 27 to verify that in the Myhill–
Nerode equivalence relation for PAL, every string is in an equivalence class by
itself.
4.8 Linear languages
There is an interesting subclass of the CFLs known as the linear languages. To
deﬁne them we ﬁrst deﬁne the notion of a linear grammar. A linear grammar
is one in which no right-hand side of any production contains more than one
occurrence of a variable. A language is linear if it has a linear grammar.
Example 4.8.1. The language ODDPAL = {x ∈{a, b}∗: x = xR and |x| is
odd} from Section 1.5 is linear, since it is generated by the linear grammar
S →a
S →b
S →aSa
S →bSb.

4.8 Linear languages
131
The following is a pumping lemma for linear languages.
Lemma 4.8.2. If L is linear, then there exists a constant n such that if z ∈L
with |z| ≥n, then there exists a decomposition z = uvwxy with |uvxy| ≤n
and |vx| ≥1 such that uviwxiy ∈L for all i ≥0.
Proof. We can assume that G contains no unit productions, for otherwise we
may remove them using the algorithm of Exercise 1.27, without changing the
fact that the grammar is linear.
Let k be the number of variables in G, and let t be the length of the longest
right-hand side of any production. We may take n = (k + 1)t. If z ∈L, and
|z| ≥n, then it is easy to see that a parse tree for z must contain some variable
twice. Tracing down from the root of the parse tree, we see further that this
repeated variable—call it A—must occur within the ﬁrst k + 1 variables starting
from the top. Thus we have
S
∗
=⇒uAy,
A
∗
=⇒vAx,
A
∗
=⇒w
for some strings u, v, w, x, y, and these last two derivations represent sentential
forms derived from the closest A to the top and the second closest, respectively.
Then the total distance from S to the second A from the top is a path of
length at most k, so |uvxy| ≤(k + 1)t = n. Similarly, since G contains no
unit productions, we must have |vx| ≥1. Then, combining the derivations,
we get
S
∗
=⇒uAy
∗
=⇒uviAxiy
∗
=⇒uviwxiy
for all i ≥0, and hence uviwxiy ∈L, as desired.
We now give an example of a nonlinear CFL.
Example 4.8.3. We claim that L = {aibicjdj : i, j ≥0} is a CFL that is not
linear.
To see this, use Lemma 4.8.2. Assume L is linear. Let z = anbncndn, where n
is the constant in the lemma. Consider decompositions of the form z = uvwxy
with |uvxy| ≤n and |vx| ≥1. Then we must have v ∈a∗, x ∈d∗. Hence,
pumping with i = 2, we get z′ := uv2wx2y = an+kbncndn+l, where k = |v|,
l = |x|, and k + l ≥1, and z′ ̸∈L. It follows that L is not linear.

132
4
Context-free grammars and languages
4.9 Exercises
1. Prove that if L = {anbncn : n ≥0}, then L is context-free but L is not.
2. Give an example of a DCFL that is not regular.
3. Let L be a CFL. Which of the following are always CFLs?
(a) 1
2L = {x : there exists y, |y| = |x|, with xy ∈L}.
(b) L1/2 = {x : xx ∈L}.
(c) {x : x∗⊆L}.
4. Let
L = {w ∈{a, b}∗: |w|a = 2n; |w|b = 2n+1} for some n ≥0.
Show that neither L nor L is context-free.
5. Suppose G = (V, , P, S) is a context-free grammar generating a CFL L.
Show how to create a grammar G′ to generate pref(L), where
pref(L) = {x ∈∗: ∃y such that xy ∈L}.
6. (a) Prove or disprove that
L3 = {aibjaibjaibj : i, j ≥1}
is the intersection of two CFLs.
(b) Generalizing (a), prove or disprove the same result (intersection of two
CFLs) for
Lk = {ai1bj1ai2bj2 · · · aikbjk : i1 = i2 = · · · = ik ≥1;
j1 = j2 = · · · = jk ≥1},
for each k ≥2.
7. Is the class of CFLs closed under the shufﬂe operation shuff (introduced
in Section 3.3)? How about perfect shufﬂe?
8. If shuff(L, {0}) is a CFL, need L be a CFL?
9. Recall the deﬁnition of σ(L) from Exercise 3.26: let L ⊆∗be a language,
and deﬁne σ(L) = {x ∈∗: xy ∈L for all y ∈∗}. If L is context-free,
must σ(L) also be context-free?
10. Let w be the inﬁnite word
w = a1a2 · · · = 1110100100001 · · · ;
here ak = 1 if k = Fn for some n, and ak = 0 otherwise. Here Fn is the
nth Fibonacci number, deﬁned by F0 = 0, F1 = 1, and Fn = Fn−1 + Fn−2
for n ≥2. Show that the language of all ﬁnite preﬁxes of w
{ϵ, 1, 11, 111, 1110, 11101, . . .}
is a co-CFL; that is, its complement is a CFL.

4.9 Exercises
133
11. In early versions of FORTRAN, a string constant was written
d(n)Ha1a2 · · · an, where d(n) is the decimal representation of n, and
each ai ∈, for some ﬁnite alphabet of legitimate characters . Show
that the language of all such strings is not context-free.
12. A pure context-free grammar (PCF grammar) is one where there is no
distinction between variables and terminals; everything is a terminal. A
production rewrites a single terminal as a string of terminals, possibly
empty. More formally, G is a PCF grammar if G = (, P, S), where
 is the set of terminals, P the set of productions, and S a ﬁnite set of
words. Then L(G), the language generated by G, is deﬁned to be
L(G) = {x ∈∗: for some s ∈S, s
∗
=⇒x}.
For example, the PCF grammar given by
({a, b}, {s →asb}, {asb})
generates the language
{ansbn : n ≥1}.
(a) Prove the following lemma. Suppose L ⊆{a, b}∗satisﬁes the fol-
lowing three conditions: (i) L is inﬁnite; (ii) every word in L contains
a3 and b3 as subwords; and (iii) there exists a real constant c > 0
such that each word w ∈L contains some subword y, |y| ≥c|w|,
and y does not contain two consecutive occurrences of the same
letter. Then L is not generated by a PCF grammar.
(b) Use (a) to prove that a3b3(ab)∗is a regular language that is not
generated by any PCF grammar.
13. The census generating function of a language L is deﬁned to be
	
n≥0 tnXn, where tn counts the number of distinct strings in L of length
n. Give an example of a linear language for which the census generating
function is not rational.
∗∗14. Let L be a language over {0, 1, . . . , k −1} for some integer k ≥1. The
language of minimal words of L, M(L), is deﬁned by taking the union
of the lexicographically least word of each length (if it exists). Show that
M(L) is context-free if L is.
15. Let G = (V, , P, S) be a context-free grammar.
(a) Prove that the language of all sentential forms derivable from S is
context-free.
(b) Prove that the language consisting of all sentential forms derivable
by a leftmost derivation from S is context-free.

134
4
Context-free grammars and languages
∗16. Recall that a word w ∈∗is said to be unbordered if it cannot be written
in the form xyx, where |x| ≥1 and |y| ≥0. For example, the word 001
is unbordered, but 0010 is not (take x = 0 and y = 01). Deﬁne
P = {x ∈{0, 1}∗: x is unbordered}.
Show that P is not context-free. Hint: Use Ogden’s lemma.
17. Suppose L is an inherently ambiguous CFL. Then we know that for every
context-free grammar G with L = L(G), at least one word in L has at
least two different derivations in G. Show that in fact inﬁnitely many
words in L must have at least two different derivations in G.
18. Show that if L is a CFL and R is a regular language, then the quotient
L/R is a CFL.
19. Let u, v, w, x, y ∈∗, and deﬁne
L = L(u, v, w, x, y) = {uviwxiy : i ≥0}.
Show that for all u, v, w, x, y, the language L is context-free.
20. Let # be a symbol not contained in the alphabet  = {a, b}, and consider
the language
L = {x#y : x, y ∈∗and x is not a subword of y}.
Prove or disprove that L is not a CFL.
21. Let # be a symbol not contained in the alphabet  = {a, b}, and consider
the language
L = {x#y : x, y ∈∗and x is not a subsequence of y}.
(Recall from Section 3.12 that we say x is a subsequence of y if x can be
obtained by striking out 0 or more letters from y.) Prove or disprove that
L is a CFL.
22. Suppose we modify our PDA model as follows: instead of requiring
δ(q, a, A) to be a ﬁnite set {(q1, γ1), . . . , (qk, γk)} representing the nonde-
terministic choices of the PDA, we allow the PDA to nondeterministically
choose among a potentially inﬁnite set, but this set must be context-free.
More formally, we allow
δ(q, a, A) = (q1 × L1) ∪(q2 × L2) ∪· · · ∪(qk × Lk),
where each Li is a CFL. We accept by empty stack.
Prove or disprove that the class of languages accepted by these more
powerful PDAs is precisely the class of CFLs.

4.9 Exercises
135
∗23. Recall the deﬁnition of inverse substitution from Exercise 3.15. Are the
CFLs closed under inverse substitution? That is, let L be a CFL, and let
s be a substitution that maps each letter a to a CFL La. Deﬁne
s−1(L) = {x : s(x) ⊆L}.
Must s−1(L) be context-free? How about if s maps letters to ﬁnite sets?
24. Recall the alternate deﬁnition of inverse substitution from Exercise 3.16:
s[−1](L) := {x : s(x) ∩L ̸= ∅}.
Suppose s maps letters to regular languages and L is context-free. Must
s[−1](L) be context-free? How about if s maps letters to CFLs?
∗25. Let α ≥0 be a real number, and deﬁne Lα = {ai bj : i/j ≤α, i ≥
0, j ≥1}. Prove that Lα is context-free if and only if α is rational.
26. Show that, given a PDA M, there exists a PDA M′ with the property that
M′ has no moves of the form (p, γ Y) ∈δ(q, a, X) with X ̸= Y. That is,
all of the moves of M′ either pop a symbol or push a string of symbols
on top of the stack; none replaces the current symbol on top of the stack.
Show further that if M is a DPDA, then M′ can be constructed to be a
DPDA, too.
27. Show that in the Myhill–Nerodeequivalence relation for PAL, every string
is in an equivalence class by itself.
28. Give an example of a CFL with an unambiguous grammar that is not a
DCFL.
29. (T. Biedl) Clickomania is a game whose goal is to remove all the colored
squares in an array. Squares are removed by clicking on a connected
set of at least two squares of the same color and then these disappear
(see http://www.clickomania.ch for more information). Consider a
simpliﬁed version where the squares are arranged in a 1 × n array and
come in only two colors, a and b. A string of a’s and b’sis solvable if
there is some choice of moves that reduces it to the empty string. For
example, abbaaba can be reduced to the empty string as follows, where
the underline portion denotes the part that is removed at each step:
abbaaba →abbba →aa →ϵ.
Show that CL, the language of all solvable strings, is a CFL.
∗30. Let M = (Q, , , δ, q0, Z0, ∅) be a PDA that accepts by empty stack.
Further, assume that every move of M is either a pop move or a move
where a single symbol is pushed on top of the current stack contents.

136
4
Context-free grammars and languages
(a) Suppose M accepts ϵ, the empty string. Prove that there must be an
accepting computation for ϵ where the maximum stack height during
the computation is ≤|Q|2||.
(b) How close to the bound |Q|2|| can you come?
31. Compute ψ(L7), where ψ is the Parikh map and L7 = {x ∈{0, 1}∗:
x is not of the form ww}.
32. If ψ is the Parikh map, ﬁnd some examples of long English words w and
suitable subsets of the alphabet for which:
(a) ψ(w) has all entries equal to 1;
(b) ψ(w) has all entries equal to 2;
(c) ψ(w) has all entries equal to 3;
(d) ψ(w) has all entries ≥2;
(e) ψ(w) is a permutation of (1, 2, 3, . . . , n);
(f) ψ(w) is a permutation of (1, 2, 2, 3, 3, 3, . . . ,
n

n ).
33. In this exercise we will construct a CFL such that none of its Myhill–Nerode
equivalence classes is context-free. Let
L = {x ∈{a, b, c}∗: |x|a ̸= |x|b or |x|b ̸= |x|c or |x|a ̸= |x|c}.
(a) Explain why L is context-free.
(b) Deﬁne γ (x) = (|x|a −|x|b, |x|b −|x|c). Show that γ (xy) = γ (x) +
γ (y).
(c) Show that x ̸∈L if and only if γ (x) = (0, 0).
(d) Show that for all pairs of integers (i, j), there exists a string x ∈
{a, b, c}∗such that γ (x) = (i, j).
(e) Show that x is related to y under the Myhill–Nerode equivalence
relation for L if and only if γ (x) = γ (y).
(f) Show that each Myhill–Nerode equivalence relation for L is not
context-free.
34. Show that the following morphism is squarefree-preserving, that is, x is
squarefree if and only if h(x) is squarefree.
0 →abacabcacbabcbacabacbc
1 →abacabcacbcabcbabcacbc
2 →abacabcbabcacbabcbacbc
3 →abacabcbabcacbcabcbabc
4 →abacabcbacabacbabcacbc
5 →abacabcbacbcacbabcacbc.

4.9 Exercises
137
∗35. Use the interchange lemma to prove that the following languages are not
context-free over a sufﬁciently large alphabet:
(a) the language of all strings containing at least one overlap;
(b) the language of all strings containing at least one cube;
(c) the language of all strings containing at least one abelian square. (An
abelian square is a string of the form xx′, where x′ is a permutation
of x.)
∗36. Is the class of CFLs closed under the cyc operation introduced in Sec-
tion 3.4?
37. Recall the deﬁnition of perm(L) from Exercise 3.13.
(a) Give an example of a regular language L such that perm(L) is not
context-free.
(b) Show that if L is a regular language over an alphabet of two symbols,
then perm(L) is context-free.
∗38. For strings x, w of equal length, deﬁne match(x, y) to be the number of
indices i such that ai = bi, where x = a1a2 · · · an and y = b1b2 · · · bn.
Deﬁne
L = {y ∈∗: y = xw, |x| = |w|, and match(x, w) ≥2}.
Is L context-free? Your answer may depend on the cardinality of .
∗39. We say a word w ∈∗is balanced if ||u|a −|v|a| ≤1 for all subwords
u, v of w, with |u| = |v|, and all a ∈; otherwise w is unbalanced. For
example, 01101 is balanced and 1100 is unbalanced.
(a) Prove that w ∈{0, 1}∗is unbalanced if and only if there exists a
palindrome x such that w contains both 0x0 and 1x1 as subwords.
(b) Prove that the set of unbalanced words over {0, 1} is context-free.
40. Recall the deﬁnition of h−∗(L) from Exercise 3.71. If L is context-free,
need h−∗(L) be context-free?
41. Prove that the language of all words over {0, 1} that are not preﬁxes of
the Thue–Morseword is context-free. Generalize.
42. For a language L, deﬁne llc(L) to be the union of the lexicographically
least conjugate of each member of L. Give an example of a regular
language L over a two-letter alphabet such that llc(L) is not context-
free.
43. For a language L, deﬁne llp(L) to be the union of the lexicographically
least permutation of each member of L. Show that if L is a regular
language over a two-letter alphabet, then llp(L) is context-free, while if
L is over a larger alphabet, then llp(L) need not be context-free.

138
4
Context-free grammars and languages
4.10 Projects
1. Find out about applications of the theory of formal languages to the study of
natural languages, such as English. You can start with Shieber [1985] and
Gazdar, Klein, Pullum, and Sag [1985].
2. Find out more about the class of languages that can be expressed as the
intersection of a ﬁnite number of CFLs. What are the closure properties of
this class? You can start with the paper of Liu and Weiner [1973].
3. Find out about graph grammars, a combination of formal language theory
and graph theory. An immense survey is the three-volume compendium of
Rozenberg [1997], Ehrig, Engels, Kreowski, and Rozenberg [1999], and
Ehrig, Kreowski, Montanari, and Rozenberg [1999].
4.11 Research problems
1. Given CFLs L1, and L2 with L1 ⊂L2 and L2 −L1 inﬁnite, need there
be a CFL L3 with L1 ⊂L3 ⊂L2 such that both L2 −L3 and L3 −L1 are
inﬁnite? This question is due to Bucher [1980].
2. Are the primitive words over {0, 1} context-free? This classic problem has
been open for at least 20 years.
3. Let p(n) be a polynomial with rational coefﬁcients such that p(n) ∈N for
all n ∈N. Prove or disprove that the language of the base-k representations
of all integers in {p(n) : n ≥0} is context-free if and only if the degree of
p is ≤1.
4.12 Notes on Chapter 4
4.1 Theorem 4.1.6 is due to Ginsburg and Spanier [1963]. Our proof is new.
4.2 The theorem in this section is due to Ginsburg and Rice [1962], although
our proof is different.
4.3 For Ogden’s lemma, see Ogden [1968]. There are very few techniques
known for proving languages not context-free. Another is the method
of Bader and Moura [1982].
4.4 Theorem 4.4.1 is originally due to Maurer [1969]. Our proof is from Du
and Ko [2001, pp. 149–150].
The result on the optimality of the triple construction is from Goldstine,
Price, and Wotschke [1982a, b].
4.5 For the interchange lemma, see Main [1982], Ross and Winklmann
[1982], Gabarro [1985], and (especially) Ogden, Ross, and Winklmann

4.12 Notes on Chapter 4
139
[1985]. Boonyavatana and Slutzki [1988] compared the power of the in-
terchange and pumping lemmas. The morphism h is from Brandenburg
[1983].
4.6 Parikh’s theorem was originally proved by Parikh in an obscure 1961
technical report. It later appeared as Parikh [1966]. Our proof is based
on Goldstine [1977]. Also see Pilling [1973]. The second proof in
Example 4.6.6 is from Du and Ko [2001, pp. 153–154].
For the closure of semilinear sets under intersection and complement,
see Ginsburg [1966]. No really simple proof of this result seems to be
known.
4.7 For more on DCFLs, see Fischer [1963], Sch¨utzenberger [1963], and
Ginsburg and Greibach [1966].
A weaker version of Theorem 4.7.4 was observed by M. Van Bies-
brouck (personal communication) after reading a similar theorem given
in Martin [1997].
4.8 For the linear languages, see Ginsburg and Spanier [1966].

5
Parsing and recognition
In this chapter we investigate methods for parsing and recognition in context-
free grammars (CFGs). Both problems have signiﬁcant practical applications.
Parsing, for example, is an essential feature of a compiler, which translates
from one computer language (the “source”)to another (the “target”).Typically,
the source is a high-level language, while the target is machine language.
The ﬁrst compilers were built in the early 1950s. Computing pioneer Grace
Murray Hopper built one at Remington Rand during 1951–1952.At that time,
constructing a compiler was a black art that was very time consuming. When
John Backus led the project that produced a FORTRAN compiler in 1955–1957,
it took 18 person-years to complete.
Today, modern parser generators, such as Yacc (which stands for “yetanother
compiler-compiler”)and Bison, allow a single person to construct a compiler
in a few hours or days. These tools are based on LALR(1) parsing, a variant
of one of the parsing methods we will discuss here. Parsing is also a feature of
natural language recognition systems.
In Section 5.1 we will see how to accomplish parsing in an arbitrary CFG in
polynomial time. More precisely, if the grammar G is in Chomsky normal form,
we can parse an arbitrary string w ∈L(G) of length n in O(n3) time. While
a running time of O(n3) is often considered tractable in computer science,
as programs get bigger and bigger, it becomes more and more essential that
parsing be performed in linear time.
Can general grammars be parsed faster than cubic time? Yes, Valiant has
shown that parsing can be reduced to matrix multiplication. Since it is known
how to multiply two n × n matrices in O(n2.376) steps, we can parse general
grammars within this time bound. But this method is not likely to be useful for
any grammars that people actually use, since it is quite complicated and the
hidden constant is large.
140

5.1 Recognition and parsing in general grammars
141
It is currently not known if general grammars can be parsed in O(n2) time, so
computer scientists instead have turned to restricting the class of grammars so
that linear-time parsing can be achieved. Because of its importance to computer
science, it should come as no surprise to learn that many different parsing
methods have been proposed. The two main paradigms are top-down parsing,
where we begin with the start symbol S of the grammar and attempt to determine
the correct sequence of productions in a derivation starting with S, and bottom-
up parsing, where we begin with the string w and attempt to construct a
derivation by doing the productions “inreverse”until we wind up with S.
Although top-down parsing may seem more natural, it appears to be less
powerful and is less often used in practice. Nevertheless, understanding its
principles is instructive and we cover this topic in Section 5.3.
Most modern parser generators use a form of bottom-up parsing. We discuss
bottom-up parsing in Section 5.5.
5.1 Recognition and parsing in general grammars
Suppose we are given an CFG grammar G = (V, , P, S) and we wish to
determine, given a string w, whether w ∈L(G). This problem is less general
than parsing, since we do not demand that the algorithm produce the parse tree
if indeed w ∈L(G).
First, we convert G to Chomsky normal form (see Exercise 1.28).
Next, we use a dynamic programming algorithm due (independently) to
Cocke, Younger, and Kasami, and often called the CYK algorithm. Let us write
w = a1a2 · · · an and w[i..j] = ai · · · aj. Suppose we are trying to determine
whether A
∗
=⇒w[i..j]. Since G is in Chomsky normal form, this is easy: if
i = j, then we just need to check to see if A →ai is a production in P. If
i < j, then the ﬁrst step of the derivation must look like A →BC for some
variables B, C ∈V . This implies that there exists k such that B
∗
=⇒w[i..k]
and C
∗
=⇒w[k + 1..j].
These observations suggest the following dynamic programming algorithm:
determine, for all variables A ∈V and subwords y of w, whether A
∗
=⇒y. We
do this in order of increasing length of y.
CYK(G, w)
Input: G = (V, , P, S) is a context-free grammar in Chomsky normal form,
w = a1a2 · · · an is a string
Output: U[i, j] contains all variables A such that A
∗
=⇒ai · · · aj

142
5
Parsing and recognition
for i := 1 to n do
U[i, i] := {A ∈V : A →ai is a production}
for d := 1 to n −1 do
for i := 1 to n −d do
j := i + d
U[i, j] := ∅
for k := i to j −1 do
U[i, j] := U[i, j] ∪{A ∈V : A →BC is a production
and B ∈U[i, k] and C ∈U[k + 1, j]}
if (S ∈U[1, n]) then
return(true)
else
return(false)
Theorem 5.1.1. Given a CFG G in Chomsky normal form and an input w
of length n, we can determine in O(n3) steps whether w ∈L(G). Here the
constant in the big-O may depend on G.
Proof.
The CYK algorithm has three nested loops. Each loop is executed at
most n times.
Example 5.1.2. Let us look at an example. Consider the following grammar
in Chomsky normal form:
S →AB | b
A →CB | AA | a
B →AS | b
C →BS | c.
For the input cabab, the CYK algorithm ﬁlls in the table, as follows:
i\j
1
2
3
4
5
1
C
∅
A
A
S, B
2
A
S, B
∅
C
3
S, B
∅
C
4
A
S, B
5
S, B
Since S is in U[1, 5], it follows that cabab ∈L(G).
We can modify the preceding CYK algorithm to produce parse trees for
strings in L(G). To do so, we add a new array CYK[A, i, j]. The intent

5.1 Recognition and parsing in general grammars
143
is that CYK[A, i, j] contains the triple (B, C, k) if there exists a derivation
A
∗
=⇒w[i..j] whose ﬁrst step is A =⇒BC and such that B
∗
=⇒w[i..k] and
C
∗
=⇒w[k + 1..j]. We use a procedure called maketree that takes a variable
A and a set of strings γ1, γ2, . . . , γl as arguments and returns a tree with A as
the root and the γ ’s as children.
CYK-Make-Parse-Table(G, w)
Input: G = (V, , P, S) is a context-free grammar in Chomsky normal form
w = a1a2 · · · an is a string
for i := 1 to n do
U[i, i] := {A ∈V : A →ai is a production}
for all variables A ∈V , indices i, j with 1 ≤i ≤j ≤n
set CYK[A, i, j] := the empty list
for d := 1 to n −1 do
for i := 1 to n −d do
j := i + d
U[i, j] := ∅
for k := i to j −1 do
U[i, j] := U[i, j] ∪{A ∈V : A →BC is a production
and B ∈U[i, k] and C ∈U[k + 1, j]}
For each A →BC just added, append (B, C, k) to the list
CYK[A, i, j]
Buildtree(A, i, j)
if i = j then
return(maketree(A →ai)) for some production A →ai
else if CYK[A, i, j] = ∅
then error
else
choose an element (B, C, k) from CYK[A, i, j]
l := Buildtree(B, i, k)
r := Buildtree(C, k + 1, j)
return(maketree(A →l, r))
CYK-Parse(G, w)
1. Call CYK-Make-Parse-Table(G, w)
2. Return Buildtree(S, 1, |w|)

144
5
Parsing and recognition
c
b
a
a
b
A
S
A
C
S
S
B
B
A
Figure 5.1: A parse tree for the string cabab
Theorem 5.1.3. We can produce a parse tree for a string generated by a CFG
in Chomsky normal form in O(n3) steps, where n is the length of the string.
Proof. Left to the reader.
Example 5.1.4. Let us continue Example 5.1.2. For the input cabab, the
CYK-Make-Parse-Table algorithm creates the following entries:
i\j
1
2
3
4
5
1
C
∅
A : (C, B, 1)
A : (A, A, 3)
S : (A, B, 3), (A, B, 4)
B : (A, S, 3), (A, S, 4)
2
A
S : (A, B, 2)
∅
C : (B, S, 3)
B : (A, S, 2)
3
S, B
∅
C : (B, S, 3)
4
A
S : (A, B, 4)
B : (A, S, 4)
5
S, B
One resulting parse tree is shown in Figure 5.1.
5.2 Earley’s method
Earley’s method is a general parsing method that runs in O(n3) time on general
grammars, O(n2) time on unambiguous grammars, and can be modiﬁed to run
in linear time on LR(1) grammars.
Like the CYK parsing algorithm, it works in two stages. First, it builds a
parsing table for the input string. Once this is complete, we can recognize
whether the input is in L(G) or not. Next, we can use the information in the
parsing table to construct a parse tree for w.
We start with the construction of the parsing table. The table is an array
M = (Mi,j)0≤i≤j≤n, where w = a1a2 · · · an is the input and G = (V, , P, S)

5.2 Earley’s method
145
is the grammar. Each entry holds a number of items, which are objects of
the form A →α • β, where A →αβ is a production of P. The • serves as a
placeholder. An item is said to be complete if it is of the form A →α•.
The goal is to generate the entries of the table M such that the item A →
α • β is in Mi,j if and only if the production A →αβ is in P and there exists
δ ∈(V ∪)∗such that
S
∗
=⇒a1a2 · · · aiAδ
and
α
∗
=⇒ai+1 · · · aj
(5.1)
both hold.
Once this is done, we can recognize whether w ∈L(G) quickly, as follows.
Lemma 5.2.1. We have w ∈L(G) if and only if there exists an item of the form
S →α• ∈M0,n.
Proof.
If w ∈L(G), then there is a derivation of the form S =⇒α
∗
=⇒w.
Now (5.1) is satisﬁed for i = 0, A = S, δ = ϵ, and j = n, so S →α• ∈M0,n.
On the other hand, if S →α• ∈M0,n, then α
∗
=⇒a1a2 · · · an and S →α is
a production of P, so putting these together we get S
∗
=⇒w.
The following algorithm constructs the parsing table:
Make-Earley-Table
A. Add S →•γ to M0,0 for all productions S →γ in P.
Do the following steps until no more items can be added to M:
B. If Mi,j contains A→α • aj+1β, then add A→αaj+1 • β to Mi,j+1;
C. If Mi,j contains A →α • Bβ and Mj,k contains B →γ •, then
add A →αB • β to Mi,k;
D. If Mi,j contains A→α • Bβ, then add B →•γ to Mj,j for all
B →γ in P.
Example 5.2.2. Consider the following grammar, where S is the start symbol:
S →T +S
S →T
T →F∗T
T →F
F →(S)
F →a.
This grammar generates some simple algebraic expressions over the alphabet
 = {a, +, ∗, (, )}.

146
5
Parsing and recognition
Consider applying Earley’s algorithm to w = (a + a) ∗a. We can, if we
choose, do the algorithm in order of increasing j. Initially, by step A, we put
S →•T + S
and
S →•T in M0,0.
Then, by step D, we put
T →•F ∗T
and
T →•F in M0,0.
Finally, by step D again, we put
F →•(S)
and
F →•a in M0,0.
Next, by step B we put
F →( • S) in M0,1.
By step D we put
S →•T + S
and
S →•T in M1,1.
Then by step D we add, successively,
T →•F ∗T ,
T →•F,
F →•(S),
and
F →•a to M1,1.
Next, by step B we add
F →a• to M1,2.
And so, by step C (i = 1, j = 1, k = 2), since M1,1 contains T →•F ∗T and
M1,2 contains F →a•, we add T →F • ∗T to M1,2.
Eventually, we ﬁnd the following contents of the parsing table:
M0,0
S →•T + S
S →•T
T →•F ∗T
T →•F
F →•(S)
F →•a
M0,1
F →(•S)
M0,2
F →(S•)
M0,4
F →(S•)
M0,5
F →(S)•
S →T •
T →F • ∗T
S →T • +S
T →F•
M0,6
T →F ∗•T
M0,7
T →F ∗T •
S →T • +S
S →T •
M1,1
S →•T + S
S →•T
T →•F ∗T
T →•F
F →•(S)
F →•a
M1,2
F →a•
T →F • ∗T
T →F•
S →T • +S
S →T •
M1,3
S →T + •S
M1,4
S →T + S•
M3,3
S →•T + S
S →•T
T →•F ∗T
T →•F
F →•(S)
F →•a
M3,4
F →a•
T →F • ∗T
T →F•
S →T • +S
S →T •
M6,6
T →•F ∗T
T →•F
F →•(S)
F →•a
M6,7
F →a•
T →F • ∗T
T →F•

5.2 Earley’s method
147
The next theorem proves that Earley’s method works.
Theorem 5.2.3. The item C →η • γ gets added to Mu,v iff the production
C →ηγ is in P, η
∗
=⇒w[u + 1..v], and there exists δ ∈(V ∪)∗such that
S
∗
=⇒w[1..u]Cδ.
Proof.
Suppose C →η • γ gets added to Mu,v. We prove the desired result
by induction on the number of items currently added in all entries of M.
For the base case, after step A has been performed, we have that S →•γ is
in M0,0, so η = ϵ, C = S, and we can take δ = ϵ.
Now let us prove the induction step. Suppose C →η • γ is added to Mu,v.
This can occur in either step B, C, or D of the algorithm.
If it occurs in step B, then u = i, v = j + 1, η = αaj+1, γ = β, and A =
C. Then Mi,j contains A →α • aj+1β, which must have been added at an
earlier step. By induction α
∗
=⇒w[i + 1..j] and S
∗
=⇒w[1..i]Aδ. Hence,
η = αaj+1
∗
=⇒w[i + 1..j + 1].
If it occurs in step C, then η = αB, γ = β, u = i, v = k, and C = A.
Then Mi,j contains the item A →α • Bβ, which must have been added at an
earlier step. By induction, α
∗
=⇒w[i + 1..j] and S
∗
=⇒w[1..i]Aδ. Also, Mj,k
contains B →γ •, so by induction γ
∗
=⇒w[j + 1..k]. Hence,
η = αB
∗
=⇒w[i + 1..j]B
=⇒w[i + 1..j]γ,
since B →γ is in P
∗
=⇒w[i + 1..j]w[j + 1..k] = w[i + 1..k].
And we already have S
∗
=⇒w[1..i]Aδ.
If it occurs in step D, then η = ϵ, C = B, and u = v = j. By induction,
α
∗
=⇒w[i + 1..j] and there exists δ such that S
∗
=⇒w[1..i]Aδ. Hence,
S
∗
=⇒w[1..i]Aδ
=⇒w[1..i]αBβδ
∗
=⇒w[1..i]w[i + 1..j]Bβδ = w[1..j]Bβδ.
This completes one direction of the proof.
For the other direction, we will prove that C →•ηγ ∈Mu,u and C →
η • γ ∈Mu,v. In order to do this, we prove two lemmas. Let µ ∈(V ∪)∗.
Lemma 5.2.4. If A →α • µβ is in Mi,j and µ
∗
=⇒w[j + 1..k], then A →
αµ • β is in Mi,k.
Proof. By induction on r, the length of the derivation µ
∗
=⇒w[j + 1..k].

148
5
Parsing and recognition
The base case is r = 0. In this case we have µ = w[j + 1..k] and then
A →α • w[j + 1..k]β ∈Mi,j. Hence,
A →αaj+1 • w[j + 2..k]β
gets added to Mi,j+1 by step B
A →αaj+1aj+2 • w[j + 3..k]β
gets added to Mi,j+2 by step B
...
A →αw[j + 1..k] • β
gets added to Mi,k by step B.
For the induction step, assume r > 0. Then µ must contain a variable, say,
µ = µ1Bµ2 for B ∈V . Since µ
∗
=⇒w[j + 1..k], we have
µ1
∗
=⇒w[j + 1..l]
B =⇒γ
∗
=⇒w[l + 1..m],
where B →γ is a production
µ2
∗
=⇒w[m + 1..k]
for some integers l, m and the preceding derivations from µ1, γ, µ2 each take
<r steps. Now A →α • µβ ∈Mi,j; that is,
A →α • µ1Bµ2β ∈Mi,j,
(5.2)
so
A →αµ1 • Bµ2β ∈Mi,l
(5.3)
by induction and (5.2). Now
B →•γ ∈Ml,l
(5.4)
by step D applied to Eq. (5.3). Hence,
B →γ • ∈Ml,m
(5.5)
by (5.4) and induction. Now
A →αµ1B • µ2β ∈Mi,m
(5.6)
by step C and Eqs. (5.3) and (5.5). Finally, A →αµ1Bµ2 • β ∈Mi,k by
Eq. (5.6) and induction. Thus, Lemma 5.2.4 is proved.
Lemma 5.2.5. If S
∗
=⇒w[1..i]Bδ, then B →•γ ∈Mi,i for all productions
B →γ ∈P.
Proof. By induction on r, the length of the derivation S
∗
=⇒w[1..i]Bδ.
The base case is r = 0. Then i = 0, S = B, δ = ϵ, and by step A we have
S →•γ ∈M0,0 for all productions S →γ ∈P.

5.2 Earley’s method
149
For the induction step, assume r ≥1. Then look at the step of the derivation
where the displayed occurrence of B is introduced:
S
∗
=⇒µAδ1 =⇒µαBβδ1
∗
=⇒w[1..i]Bδ,
where A →αBβ is a production in P and µα
∗
=⇒w1w2 · · · wi. Deﬁne j so
that
µ
∗
=⇒w[1..j];
α
∗
=⇒w[j + 1..i].
Hence, A →•αBβ ∈Mj,j by induction applied to the derivation S
∗
=⇒
µAδ1
∗
=⇒w[1..j]Aδ1. Hence, A →α • Bβ ∈Mj,i by Lemma 5.2.4. Thus,
B →•γ ∈Mi,i by step D.
We can now complete the proof of Theorem 5.2.3.
If η
∗
=⇒w[u + 1..v] and there exists a δ such that S
∗
=⇒w[1..u]Cδ, then
C →•ηγ ∈Mu,u by Lemma 5.2.5. Then C →η • γ ∈Mu,v by Lemma 5.2.4
with A = C, α = ϵ, µ = η, γ = β, i = u, j = u, and k = v.
Next we prove a theorem about how efﬁciently Earley’s method can be
implemented.
Theorem 5.2.6. The parse table M = (Mi,j) can be constructed in O(n3)
steps, where n = |w|.
Proof.
We compute the table in order of increasing j. It sufﬁces to see that
all the entries in Mi,j for all i can be computed in O(n2) steps. There are O(n)
possible entries, and for each one we have to do at most O(n) work in step C.
Theorem 5.2.7. If G is an unambiguous grammar with no useless symbols, no
unit productions, and no ϵ-productions, then M = (Mi,j) can be constructed
in O(n2) steps, where n = |w|.
Proof. We use three data structures:
• Ij, 0 ≤j ≤n, a list of pairs (A →α • β, i) such that (A →α • β, i) ∈Ij
if and only if A →α • β ∈Mi,j;
• Lj(X), 0 ≤j ≤n, X ∈V ∪, a list of pairs (A →α • Xβ, i) such that
A →α • Xβ ∈Mi,j;
• BVj, 0 ≤j ≤n, a bit vector, initially all 0, that for each item A →α • β
tells whether the pair (A →α • β, j) has already been added to Ij.

150
5
Parsing and recognition
A subroutine ADD maintains these lists. Here is the code for ADD:
ADD((A →α • Xβ, i), j)
1. append (A →α • Xβ, i) to Ij
2. append (A →α • Xβ, i) to Lj(X)
3. If i = j, set BVj[A →α • Xβ] to 1.
4. If X ∈V , for all productions X →γ do
if BVj[X →γ •] = 1
then call ADD ((A →αX • β, i), j)
Our implementation of Earley’s algorithm now implements steps A–Das
follows:
A. For all productions S →α, call ADD((S →•α, 0), 0).
For j = 0, 1, . . . , n do:
B. If j > 0 then do
for each pair (A →α • wjβ, i) on the list Lj−1(wj),
call ADD ((A →αwj • β, i), j)
1.
For each pair on the list Ij do:
If the pair is of the form (B →γ •, i)
C. For each pair (A →α • Bβ, k) on the list Li(B), call
ADD((A →αB • β, k), j)
If the pair is of the form (A →α • Bβ, i)
D. If BVj[B →•γ ] = 0, then call ADD((B →•γ, j), j).
Note that Ij lengthens as the algorithm proceeds.
We claim that the algorithm presented is correct and uses O(n2) steps. The
correctness proof is left to the reader; the only trick is that step 4 of ADD is needed
if (A →α • Bβ, i) is added to Ij after the pair (B →γ •, j) is considered in
line 1.
For the time bound, note that there exists a constant c such that at most
c(j + 1) pairs appear on the list Ij. For if the pair is of the form (A →α • β, i)
with α ̸= ϵ, then by Exercise 9 we try to add this item to Ij at most once, since
G is unambiguous. If the pair is of the form (A →•β, i), then it is added only
in steps A and D, and in step D the bit vector is checked before the item is
added. Thus the total number of pairs is ≤	
0≤j≤n c(j + 1) = O(n2).
The running time now follows if we can show that the total time associated
with a list entry is O(1). We do so by an amortized analysis argument. We
allocate $2 to each list entry. $1 is used to pay for the cost of examining the

5.2 Earley’s method
151
entry in line 1, and $1 is used to pay for the cost of adding the entry to a list.
But our algorithm examines and adds each entry at most once. Thus the total
cost is O(n2).
Up until now we have used Earley’s algorithm as a recognizer, not a parser.
We now discuss how the parsing table constructed by the algorithm may be
used to parse.
PARSE (A →β•, i, j)
{ Finds a rightmost derivation of w[i + 1..j] starting with production
A →β. }
If β = X1X2 · · · Xm, set k := m and l := j
Repeat until k = 0:
If Xk ∈
set k := k −1 and l := l −1
Else {Xk ∈V }
(*) ﬁnd a complete item of the form Xk →γ • in Mr,l for some r
such that A →X1X2 · · · Xk−1 • Xk · · · Xm ∈Mi,r
Call PARSE(Xk →γ •, r, l)
Set k := k −1 and l := r
This algorithm can be used to obtain a rightmost derivation of the input
string. We call PARSE with the arguments (S →α•, 0, n), where S →α• is an
item in M0,n.
Theorem 5.2.8. Assume that G contains neither ϵ-productions nor unit pro-
ductions and that the table [Mi,j] has already been computed. If A =⇒
β
∗
=⇒wi+1 · · · wj, then a call to PARSE(A →β•, i, j) produces a parse of
wi+1 · · · wj starting with A →β in O((j −i)2) steps.
Proof.
For correctness, observe that if you have shown Xk+1 · · · Xm
∗
=⇒
wl+1 · · · wj, then
A =⇒β = X1X2 · · · Xk−1XkXk+1 · · · Xm
∗
=⇒X1X2 · · · Xkwl+1 · · · wj.
But A →X1 · · · Xk−1 • Xk · · · Xm ∈Mi,r, so X1 · · · Xk−1
∗
=⇒wi+1 · · · wr.
Hence, A
∗
=⇒wi+1 · · · wrXkwl+1 · · · wj. But Xk →γ • ∈Mr,l, so γ
∗
=⇒
wr+1 · · · wl. Hence,
A
∗
=⇒wi+1 · · · wrγ wl+1 · · · wj
∗
=⇒wi+1 · · · wrwr+1 · · · wlwl+1 · · · wj,
as desired.

152
5
Parsing and recognition
For the running time analysis, let us prove by induction on j −i that the
running time of the call PARSE(A →β•, i, j) is bounded by c(j −i)2 for some
constant c. This is clearly true if j −i = 1. Now suppose β = X1X2 · · · Xm
and that Xj
∗
=⇒wij+1 · · · wij+1 for 1 ≤j ≤m. Note i = i1 and j = im+1. In
step (*) we need to examine all lists Mr,l for r = l, l −1, . . . , i until the desired
item is found. This can be done in d(j −i) steps for some constant d. The total
cost is therefore
≤c((i2 −i1)2 + · · · + (im+1 −im)2) + d((i2 −i1) + · · · + (im+1 −im)).
By telescoping cancelation, the second sum is just d(j −i). Since G does not
contain ϵ-productions or unit productions, 1 < m ≤j −i. Now by Exercise 13,
((i2 −i1)2 + · · · + (im+1 −im)2) is bounded by (j −i −1)2 + 1 = (j −i)2 −
2(j −i) + 2. Hence, taking c = d, the desired inequality follows.
5.3 Top-down parsing
As mentioned previously, a top-down parser attempts to construct the deriva-
tion tree for a word w ∈L(G) from the “topdown,”that is, by starting with
the start symbol S of the grammar and choosing the correct productions in
order.
The most popular top-down methods are called LL(k). The ﬁrst L stands
for left-to-right scan of the input, the second L stands for producing a leftmost
derivation, and the k refers to the number of symbols of “lookahead”that are
allowed. Lookahead is a feature of many parsing algorithms, and refers to how
many symbols of the input past the current position the algorithm is allowed to
refer to.
In this section we focus on LL(1) parsing. It is known that LL(1) parsers
are not as powerful as LL(k) parsers for arbitrary k; in fact, there are ex-
amples known of grammars that can be parsed with LL(k) parsers but not
LL(k −1).
It is conceptually easy to think of a top-down parser as a one-state nondeter-
ministic pushdown automaton (PDA) with a write-only output tape, as shown
in Figure 5.2.
The conﬁguration of such a parser is
[aiai+1 · · · an$, γjγj−1 · · · γ2γ1#, n1n2 · · · nk].
Here the input is a1 · · · an, the current stack contents is γj · · · γ1, with the top
symbol on the stack being γj, and n1 · · · nk is the output.

5.3 Top-down parsing
153
j
1
#
Input tape
Stack
Output tape
a1
· · ·
· · ·
· · ·
$
an
...
n1
n2
k
a2
ai
ai+1
n
n
3
Program
γ
γ
Figure 5.2: Top-down parser illustrated
The initial conﬁguration of the parser is
[a1a2 · · · an$, S#, ϵ].
Note: the special symbol $ should be thought of as the “end-of-input”marker,
and the special symbol # is the “bottom-of-stack”marker.
At any stage in the computation, the parser can do exactly one of three
things:
1. If the symbol on the top of the stack is a variable X, the parser pops X
and pushes the string α1 · · · αt, where X →α1 · · · αt is a production in G.
The parser then writes the appropriate production, or a code for it, on the
output tape. (In general, there may be several such productions with X on
the left-hand side; the parser must choose the right one, based on X and the
current symbol being scanned.)
2. If the symbol on the top of the stack is a terminal x, it is compared with the
current input symbol a. If x = a, the stack is popped and the input pointer
is advanced one cell. If x ̸= a, the parser writes “reject”on the output tape
and halts.
3. If the symbol on the top of the stack is # and the input is $, the parser writes
“accept”on the output tape and halts.
The decision about what to do is based on a transition function M(γ, x).

154
5
Parsing and recognition
γ\x
a
b
c
$
S
S →AB
S →AB
S →c
Reject
A
A →aS
A →b
Reject
Reject
B
B →AS
B →AS
B →c
Reject
a
Pop
Reject
Reject
Reject
b
Reject
Pop
Reject
Reject
c
Reject
Reject
Pop
Reject
#
Reject
Reject
Reject
Accept
Figure 5.3: LL(1) parsing table
Example 5.3.1. Consider a grammar G with the following numbered produc-
tions:
1.
S →AB;
2.
S →c;
3.
A →aS;
4.
A →b;
5.
B →AS;
6.
B →c.
Then the transition function M(γ, x) is as given in Figure 5.3.
For example, here are the conﬁgurations of the parser on input acbbc:
(acbbc$, S#, ϵ) ⊢(acbbc$, AB#, 1) ⊢(acbbc$, aSB#, 13)
⊢(cbbc$, SB#, 13) ⊢(cbbc$, cB#, 132) ⊢(bbc$, B#, 132)
⊢(bbc$, AS#, 1325) ⊢(bbc$, bS#, 13254) ⊢(bc$, S#, 13254)
⊢(bc$, AB#, 132541) ⊢(bc$, bB#, 1325414) ⊢(c$, B#, 1325414)
⊢(c$, c#, 13254146) ⊢($, #, 13254146).
(5.7)
This corresponds to using the productions 1, 3, 2, 5, 4, 1, 4, 6 in a leftmost
derivation of acbbc.
The whole game of LL(1) parsing is to construct the transition function
M. The parser must know which production to use, based only on the current
symbol on top of the stack and the current input symbol being scanned.
To compute M, we introduce two functions:
• FIRST(α), deﬁned for all α ∈( ∪V )∗;
• FOLLOW(A), deﬁned for all A ∈V .
The range of each of these functions is a set of symbols.

5.3 Top-down parsing
155
Intuitively, we put a terminal a in FIRST(α) if it is possible to derive a
sentential form from α that begins with a.
Formally, we deﬁne
FIRST(α) = {a ∈ : there exists a derivation α
∗
=⇒aβ for some
β ∈( ∪V )∗} ∪{ϵ : there exists a derivation α
∗
=⇒ϵ}.
(5.8)
The heuristic description of FOLLOW(A) is as follows: we put a terminal a
in FOLLOW(A) if it could appear immediately following A in some sentential
form derivable from S.
Formally, we deﬁne
FOLLOW(A) = {a ∈ : there exists a derivation S
∗
=⇒αAaβ for some
α, β ∈(V ∪)∗} ∪{$ : there exists a derivation S
∗
=⇒αA
for some α ∈(V ∪)∗}.
(5.9)
Example 5.3.2. Consider the following grammar G:
S →AB | a
A →CD
B →bAB | ϵ
C →dSd | c
D →cCD | ϵ.
Then FIRST(AB) = {c, d}, because we have the two derivations:
AB =⇒CDB =⇒dSdDB;
AB =⇒CDB =⇒cDB.
Similarly, we have
FIRST(a) = {a};
FIRST(CD) = {c, d};
FIRST(bAB) = {b};
FIRST(ϵ) = {ϵ};
FIRST(dSd) = {d};
FIRST(c) = {c};
FIRST(cCD) = {c}.

156
5
Parsing and recognition
We also have FOLLOW(C) = {$, b, c, d}, because we have the derivations
S =⇒AB =⇒A =⇒CD =⇒C;
S =⇒AB =⇒CDB =⇒CB =⇒CbAB;
S =⇒AB =⇒CDB =⇒CcCD;
S =⇒AB =⇒CDB =⇒dSdDB =⇒dABdDB =⇒dCDBdDB =⇒
dCBdDB =⇒dCdDB.
Similarly, we have
FOLLOW(S) = {$, d};
FOLLOW(A) = {$, b, d};
FOLLOW(B) = {$, d};
FOLLOW(D) = {$, b, d}.
Now we give an algorithm to compute M(γ, x), assuming we have algo-
rithms for FIRST and FOLLOW (which we discuss next).
Compute-LL-Table
1. M(a, a) = “pop”for all a ∈;
2. M(#, $) = “accept”;
3. For each production X →α do
(a) For each terminal b ∈FIRST(α), set M(X, b) = “applyproduction
X →α”;
(b) If ϵ ∈FIRST(α), set M(X, b) = “applyproduction X →α”for all b ∈
FOLLOW(X);
4. M(X, b) = “reject”for all other cases.
Example 5.3.3. Let us compute the LL(1) parsing table for the grammar G of
Example 5.3.2 (see Figure 5.4). (Blank spaces mean “reject.”)
Note: It is possible that the function M(γ, x) is multiply deﬁned. In that
case, we say the parsing table has a conﬂict, and the corresponding grammar is
not LL(1).
There are several approaches to handle conﬂicts in LL(1) grammars. We can
try to provide disambiguating rules that tell which of several productions we
should choose if there is a conﬂict. We can try all the possibilities, which forces
backtracking. We can attempt to rewrite the grammar to obtain an equivalent
LL(1) grammar. Finally, we can use more symbols of lookahead, which leads
to LL(2), LL(3), and so on.

5.3 Top-down parsing
157
γ\x
a
b
c
d
$
S
S →a
S →AB
S →AB
A
A →CD
A →CD
B
B →bAB
B →ϵ
B →ϵ
C
C →c
C →dSd
D
D →ϵ
D →cCD
D →ϵ
D →ϵ
a
Pop
b
Pop
c
Pop
d
Pop
#
Accept
Figure 5.4: Parsing table M(γ, x) for G of Example 5.3.2
Deﬁnition. We say grammar G is LL(1) if M(γ, x) is single-valued. We say a
language L is LL(1) if there exists some LL(1) grammar G such that L = L(G).
The term LL(1) arises as follows: the ﬁrst L stands for a left-to-right scan of
the input, the second L stands for the fact that the method produces a leftmost
derivation, and the 1 refers to 1 symbol of lookahead.
Theorem 5.3.4. If G is an LL(1) grammar, then G has a determinis-
tic top-down parser given by the algorithm to compute M(γ, x) given in
Compute-LL-Table.
Proof.
We show that if z ∈L(G), then the parser correctly constructs the
leftmost derivation of z and if z ̸∈L(G), then the parser rejects z.
Let z = a1a2 · · · an. The initial conﬁguration of the parser is
[a1a2 · · · an$, S#, ϵ].
The program ﬁrst examines M(S, a1). If the entry is “reject,”that means a1 ̸∈
FIRST(α) for all α with S →α a production. Hence, no string in L(G) derivable
from S can begin with a1. Hence, z ̸∈L(G).
Suppose M(S, a1) does contain a production. Then the production is S →α
and can come from 3(a) or 3(b). If it comes from 3(a), then a1 ∈FIRST(α).
Since M(γ, x) is single-valued, if S →β is any other S-production, a1 ̸∈
FIRST(β). If 3(b), then ϵ ∈FIRST(α), a1 ∈FOLLOW(S), and a1 ̸∈FIRST(β)
for any other S-production S →β. Hence, a1 = $, z = ϵ, and S →α is the
ﬁrst production used to derive z.
We have shown that if z is derivable from S, then the parser correctly
determines the ﬁrst production used.

158
5
Parsing and recognition
Similarly, suppose that after several moves the parser has reached the con-
ﬁguration
[akak+1 · · · an$, Ax1x2 · · · xp#, y].
We want to show that it correctly determines the next production.
If M(A, ak) = “reject”,then I claim we cannot derive a string beginning
with ak from Ax1x2 · · · xp. For we know ak ̸∈FIRST(α) for all A-productions
A →α. It is possible however, that A
∗
=⇒ϵ. In that case, ak ∈FOLLOW(A),
so by 3(b), M(A, ak) ̸= “reject.”
Now suppose M(A, ak) is a production. Then it must contain only one of the
form A →α. If this production came from step 3(a), then ak ∈FIRST(α) and
the production must be applied to derive akak+1 · · · an, since ak ̸∈FIRST(β)
for all other β.
If it came from step 3(b), then ϵ ∈FIRST(α), ak ̸∈FIRST(α). Thus we must
transform A to ϵ ﬁrst to derive akak+1 · · · an from Ax1x2 · · · xp. Then we derive
ak · · · an from x1 · · · xp. Hence the parser chooses the correct production here,
too.
To complete the proof we observe that eventually ak appears on the top of
the stack and is popped. Thus we eventually either accept or reject z, and we
accept if and only if a sequence of derivations actually produced z.
It remains to see how to compute the sets FIRST and FOLLOW.
To compute FIRST(α) we ﬁrst show how to compute FIRST(X) when
X ∈V ; then we show how to use this to compute FIRST(α) for α ∈(V ∪)∗.
Here is the algorithm for FIRST(X):
COMPUTE-FIRST(G)
Input: G = (V, , P, S) is a context-free grammar
1. Initialize F(B) = ∅for all B ∈V , and F(a) = {a} for all a ∈.
2. For each production of G do
3.
(i) If the production is A →aα for a ∈, set F(A) := F(A) ∪{a}.
(ii) If the production is A →ϵ, set F(A) := F(A) ∪{ϵ}.
(iii) If the production is A →Y1Y2 · · · Yk
(a) If there is a smallest index j such that ϵ ̸∈F(Yj),
set F(A) := F(A) ∪(F(Y1) ∪F(Y2) ∪· · · ∪F(Yj) −{ϵ})
(b) If there is no such index, set F(A) := F(A) ∪F(Y1)
∪F(Y2) ∪· · · ∪F(Yk).
4. If any of the sets F(B) were changed in size, return to step 2.
5. Otherwise set FIRST(A) := F(A) for all A ∈V .

5.3 Top-down parsing
159
Theorem 5.3.5. The preceding algorithm correctly computes FIRST(X) for
X ∈V .
Proof. First, let us show that at all times during the algorithm’s execution we
have F(X) ⊆FIRST(X) for X ∈V ∪. This is clearly true after step 1. Now
assume it is true for all iterations up to iteration k; we prove it for iteration k.
At this step, a symbol gets added to F(X) only as a result of steps 3(i), (ii),
or (iii).
In step (i), we add a to F(A) if and only if there exists a production A →aα,
and indeed a ∈FIRST(A). In step (ii), we add ϵ to F(A) if and only if there
exists a production A →ϵ, but then ϵ ∈FIRST(A). Finally, in step (iii), there
are two possibilities. In possibility (a), there is a smallest index j such that
ϵ ̸∈F(Yj). In this case, ϵ ∈FIRST(Yi) for 1 ≤i < j, so by induction Yi
∗
=⇒ϵ
for 1 ≤i < j. Hence all the terminals in F(Yi) for 1 ≤i < j are in FIRST(Yi).
A similar argument applies to possibility (b).
Next, let us show that if x ∈FIRST(A), then x eventually gets added to
F(A) by some step of the algorithm. Let x ∈FIRST(A). There are two cases
to consider:
Case 1: x = a ∈. Then there exists a derivation
A =⇒γ1 =⇒γ2 =⇒· · · =⇒γn−1 =⇒γn = aβ;
(5.10)
without loss of generality, let this be the shortest such that it results in a
beginning a right-hand side derivable from A. We claim that a is added to F(A)
before or during the nth iteration.
If n = 1, then A =⇒aβ is a production of G, so a is added to FIRST(A)
during step 3(i). Suppose our claim is true for all variables A and terminals
a for all n < n′. We prove the claim holds for n = n′. Suppose (5.10) holds.
Then let γ1 = Y1Y2 · · · Ys, where Yi ∈V ∪. Thus,
A =⇒Y1Y2 · · · Ys
∗
=⇒aβ.
Now there exists j such that Yi
∗
=⇒ϵ with 1 ≤i < j, Yj
∗
=⇒aα, and
Yj+1 · · · Ys
∗
=⇒δ, where αδ = β. All these derivations are of length < n′,
so by induction a ∈F(Yj). Hence, a is added to F(A) at the latest at step n′.
Case 2: x = ϵ. Similar to Case 1, and left to the reader.
Now that we know how to compute FIRST(X) for X ∈V ∪ ∪{ϵ}, we
show how to compute FIRST(α) for an arbitrary sentential form α.

160
5
Parsing and recognition
Lemma 5.3.6. If α = Z1Z1 · · · Zm, where Zi ∈V ∪ for 1 ≤i ≤m, m ≥2,
then
FIRST(α) =



if there exists a small-
FIRST(Z1) ∪· · · ∪FIRST(Zj)−{ϵ},
est index j, 1≤j ≤m
FIRST(Z1) ∪· · · ∪FIRST(Zm),
such that ϵ ̸∈FIRST
(Zj); otherwise
Proof. Left to the reader.
Now we turn to the computation of FOLLOW(X).
COMPUTE-FOLLOW(G)
Input: G = (V, , P, S) is a context-free grammar with no useless symbols
1. Initialize H(B) = ∅for all B ∈V , B ̸= S and H(S) = {$}.
2. For each production of G do
3.
For each variable X on the right-hand side of the production,
write the production as A →αXβ and
(i) add all the terminals in FIRST(β) to H(X); do not add ϵ;
(ii) if ϵ ∈FIRST(β), add all symbols of H(A) to H(X).
4. If any of the sets H(B) were changed in size, return to step 2.
5. Otherwise set FOLLOW(A) := H(A) for all A ∈V .
Theorem
5.3.7. The
algorithm
COMPUTE-FOLLOW
correctly
computes
FOLLOW(X) for all X ∈V .
Proof.
First, let us prove that the invariant H(A) ⊆FOLLOW(A) holds at
every step of the algorithm. We do this by induction on the number of iterations
of step 2.
Initially, we have H(A) = ∅⊆FOLLOW(A) for all A ̸= S. Also, H(S) =
{$}, and $ ∈FOLLOW(S) because of the derivation S
∗
=⇒S.
Now assume that the invariant holds at step i; we prove it for step i + 1.
A symbol can be added to H(X) in step (i) or (ii). Suppose a ∈ is added
in step (i). Then a ∈FIRST(β), so there is a derivation β
∗
=⇒aγ for some
γ ∈(V ∪)∗. Combining this with the derivation A
∗
=⇒αXβ, we get A
∗
=⇒
αXβ
∗
=⇒αXaγ , so indeed a ∈FOLLOW(X).
If step (ii) is taken, then we have ϵ ∈FIRST(β), and hence there is a
derivation β
∗
=⇒ϵ. Let a ∈H(A). Then by induction a ∈FOLLOW(A). If

5.4 Removing LL(1) conﬂicts
161
a ∈, then there is a derivation S
∗
=⇒δAaγ . Combining this with the pro-
duction A →αXβ, we get the derivation
S
∗
=⇒δAaγ =⇒δαXβaγ
∗
=⇒δαXaγ,
so indeed a ∈FOLLOW(X). If a = $, then there is a derivation S
∗
=⇒δA.
Combining this with A →αXβ, we get
S
∗
=⇒δA =⇒δαXβ
∗
=⇒δαX,
so indeed $ ∈FOLLOW(X).
The converse is left to the reader.
5.4 Removing LL(1) conﬂicts
In this section we look at two basic techniques for converting a grammar to
LL(1) form. They are not always guaranteed to work, but they are useful in
many situations.
The ﬁrst technique is removing left recursion. We say a grammar has imme-
diate left recursion if there exists a variable E with a production of the form
E →Eα. Immediate left recursion can be removed from a grammar without
ϵ-productions as follows. Suppose the E-productions are
E →Eα1 | Eα2 | · · · | Eαk | β1 | β2 | · · · | βj,
where no βi starts with E. Then we remove these productions from the grammar
and replace them with the productions
E →β1E′ | β2E′ | · · · | βjE′;
E′ →α1E′ | α2E′ | · · · | αkE′ | ϵ.
It is easy to see that this change does not affect the language generated by the
grammar.
Example 5.4.1. The production rules
E →EA | EB | c | d
would be replaced by
E →cE′ | dE′;
E′ →AE′ | BE′ | ϵ.

162
5
Parsing and recognition
However, it is also possible to have left recursion consisting of several steps,
as in the grammar given next.
S →Aa | b;
A →Sd | ϵ.
This type of left recursion can be removed as follows:
REMOVE-LEFT-RECURSION(G)
Input: G = (V, , P, S) is a context-free grammar
1. Arrange the variables in V in some order A1, A2, . . . , An.
2. For i := 1 to n do
3.
For j := 1 to i −1 do
4.
Suppose the Aj-productions are Aj →δ1 | δ2 | · · · | δk.
Replace each production Ai →Ajγ with Ai →δ1γ | δ2γ | · · · | δkγ .
5.
Eliminate immediate left recursion among the Ai as mentioned earlier.
Example 5.4.2. Suppose we start with the grammar
A1 →A2a | b;
A2 →A2c | A1d |ϵ.
Then step 4 of the preceding algorithm results in the new grammar
A1 →A2a | b;
A2 →A2c | A2ad | bd | ϵ.
Finally, step 5 gives
A1 →A2a | b;
A2 →bdA′
2 | A′
2;
A′
2 →cA′
2 | adA′
2 | ϵ.
The second method to attempt to resolve LL(1) conﬂicts is called “factoring.”
The idea here is that if two or more productions have right-hand sides that begin
in the same way, this common preﬁx may be “factored”out by introducing a
new variable.
Example 5.4.3. Consider the “if-then-else”grammar
S →iEtSeS | iEtS | x;
E →y.

5.5 Bottom-up parsing
163
We observe that the right-hand sides of the two productions S →iEtSeS
and S →iEtS share a common preﬁx of iEtS. This suggests introducing a
new variable S′ and creating the grammar
S →iEtSS′ | x;
S′ →eS | ϵ;
E →y.
This idea can be turned into an algorithm as follows:
FACTOR(G)
Input: G = (V, , P, S) is a context-free grammar
1. For each variable A ∈V do
2.
Find the longest preﬁx α common to two or more right-hand sides of
A-productions.
3.
If α ̸= ϵ, replace productions A →αβ1 | · · · | αβn | γ1 | γ2 | . . . | γm
with productions A →αA′ | γ1 | γ2 | . . . | γm
A′ →β1 | β2 | · · · | βn
where A′ is a new variable.
4.
Repeat until no two right-hand sides of A-productions share a nontrivial
common preﬁx.
5.5 Bottom-up parsing
In the previous two sections we covered some aspects of top-down parsing. In
this section we continue our study of parsing methods by turning to bottom-up
parsing.
A bottom-up parsing method constructs a derivation by starting with a
string x and attempting to ﬁnd the immediately previous sentential forms. The
best-known bottom-up methods are the LR(k) methods. Here the L stands
for left-to-right scan of the input and the R stands for a rightmost derivation,
produced in reverse by the algorithm. Once again the number k refers to the
number of symbols of lookahead performed by the algorithm.
We focus here on LR(0) parsing. While LR(0) parsing is generally consid-
ered too weak for practical parsers, it shares aspects with more complicated
methods that make it worthy of study.
The LR(0) algorithm depends on some specialized terminology. Suppose
we have a grammar G = (V, , P, S) with no useless symbols. Recall from
Section 5.2 that an item is a production of G with a dot in the middle. A right

164
5
Parsing and recognition
sentential form is a sentential form that can appear somewhere in a rightmost
derivation in G. A handle of a right sentential form is a substring that could be
introduced in the immediately preceding step of a rightmost derivation. More
formally, if S
∗
=⇒rm δAw =⇒rm δαw, for some δ, α ∈(V ∪)∗, A ∈V , and
w ∈∗, then α is said to be a handle of δαw. A viable preﬁxof a right sentential
form γ ∈(V ∪)∗is any preﬁx of γ ending at or to the left of the right end
of a handle of γ . Finally, we say that an item A →α • β is valid for a viable
preﬁxγ if there is a rightmost derivation
S =⇒∗
rm δAw =⇒rm δαβw
and γ = δα.
Example 5.5.1. Consider the following grammar G:
S →T
T →aT a
T →bT b
T →c.
Then S =⇒T =⇒aT a =⇒abT ba =⇒abcba is a rightmost derivation of
abcba. Also, γ = abT ba is a right sentential form and bT b is a handle of
γ . The viable preﬁxes of abT ba are ϵ, a, ab, abT , and abT b. The item
T →bT • b is valid for viable preﬁx abT , as is shown by the rightmost
derivation S
∗
=⇒aT a =⇒rm abT ba. (Take δ = a, A = T , w = a, α = bT ,
β = b.)
The interesting thing about viable preﬁxes is that for any grammar G with-
out useless symbols—whether LR(0) or not—there is a deterministic ﬁnite
automaton (DFA) that accepts the viable preﬁxes of G, and furthermore, the
name of the corresponding state is the set of valid items for that viable preﬁx.
We now show how to compute this DFA, which we call the Knuth DFA.
Given G = (V, , P, S), we ﬁrst show how to compute a certain NFA-ϵ
M(G) = (Q, , δ, q0, F), where
• Q = items(G) ∪{q0};
•  =  ∪V ;
• F = Q;
and δ is deﬁned as follows:
1. δ(q0, ϵ) = {S →•α : S →α is a production of G};
2. δ(A →α • Bβ, ϵ) = {B →•ρ : B →ρ is a production of G};
3. δ(A →α • Xβ, X) = {A →αX • β}, X ∈V ∪.

5.5 Bottom-up parsing
165
c
T →aTa•
T →bTb•
S →T
T →•c
T →a • Ta
T →c•
T →b • Tb
T →aT • a
T →bT • b
T →•
•
•
bTb
T →• aTa
a
b
T
ϵ
ϵ
ϵ
ϵ
T
q0
ϵ
ϵ
a
ϵ
T
b
S →T
ϵ
ϵ
ϵ
Figure 5.5: Knuth NFA-ϵ for the grammar of Example 5.5.1
We call this NFA-ϵ the Knuth NFA-ϵ. The Knuth DFA is computed from the
Knuth NFA-ϵ by the usual subset construction.
Example 5.5.2. The NFA-ϵ in Figure 5.5 is the Knuth NFA-ϵ corresponding
to the grammar of Example 5.5.1. The DFA in Figure 5.6 is obtained by the
subset construction. (Useless transitions have been deleted.)
Theorem 5.5.3. Assume G is a grammar with no useless symbols. Then the
corresponding Knuth NFA-ϵ has the following property: A →α • β ∈δ(q0, γ )
iff A →α • β is valid for γ .
Proof. Suppose A →α • β ∈δ(q0, γ ). We must show
A →α • β is valid for γ.
(5.11)
A natural way to prove (5.11) would be by induction on |γ |, but the Knuth
NFA-ϵ we constructed has some edges labeled ϵ. So instead we prove assertion
(5.11) by induction on the length l of the shortest path labeled γ from q0 to
A →α • β.
The base case is l = 1. Then γ = ϵ. Then we have the situation represented
in Figure 5.7.
Then S →•β is in δ(q0, γ ). Now S
∗
=⇒δAw =⇒δαβw with A = S, w =
ϵ, α = ϵ, and δ = ϵ. Hence, S →•β is valid for γ .

166
5
Parsing and recognition
a
a
T →•aTa
a
T →•aTa
T →•c
a
T →•c
b
b
c
S →T•
T →a • Ta
c
T →•bTb
T →•c
T →•bTb
q0
T →b • Tb
T →•bTb
T →•aTa
S →•T
T →c•
T →aT • a
T →bT • b
T →aTa•
T →bTb•
c
b
T
T
T
b
Figure 5.6: Knuth DFA for the grammar of Example 5.5.1
Now assume (5.11) holds for all paths of length <l; we prove it for a path
of length l. Let there be a path labeled γ from q0 to A →α • β. There are two
cases to consider:
Case 1: The last edge in the path is labeled with a nonempty symbol X ∈
V ∪ (see Figure 5.8).
Write γ = γ ′X and α = α′X. Then, by induction, A →α′ • Xβ is valid for
γ ′. Hence there exists a derivation
S
∗
=⇒δAw =⇒δα′Xβw
with δα′ = γ ′. Thus, γ ′X = δα′X = δα. It follows that A →α′X • β is valid
for γ .
Case 2: The last edge in the path is labeled ϵ. In this case we have the
situation represented in Figure 5.9.
S →•β
ϵ
q0
Figure 5.7: Base case of the proof

5.5 Bottom-up parsing
167
γ ′
X
q0
A →α ′ • Xβ
A →α ′X • β
Figure 5.8: Case 1 of the induction step
Hence, by induction, B →α′ • Aβ′ is valid for γ . This means there exists
a rightmost derivation
S
∗
=⇒δBw =⇒δα′Aβ′w,
where γ = δα′. Now, since there are no useless symbols, β′ must eventually
derive some terminal string, so suppose β′
∗
=⇒x. Then
S
∗
=⇒δBw =⇒δα′Aβ′w
∗
=⇒δα′Axw =⇒δα′βxw.
Thus, A →•β is indeed valid for δα′ = γ . This completes the proof of one
direction.
For the other direction, suppose A →α • β is valid for γ . This means there
is a rightmost derivation
S
∗
=⇒δ′Aw =⇒δ′αβw,
(5.12)
where γ = δ′α. If we could show that δ(q0, δ′) contains A →•αβ, then by
successively applying rule (3) of the deﬁnition of the Knuth NFA-ϵ, it would
follow that δ(q0, δ′α) contains A →α • β, as desired.
So let us prove by induction on the length of the rightmost derivation (5.12)
that δ(q0, δ′) contains A →•αβ.
The base case is when this derivation is of length 1. Then δ′ = ϵ, A = S,
and w = ϵ. Then δ(q0, ϵ) contains A →•αβ by rule (1).
For the induction step, suppose
S
∗
=⇒δ2Bx =⇒δ2δ3Aδ4x
∗
=⇒δ2δ3Ayx,
where B →δ3Aδ4 is a production. Thus, δ′ = δ2δ3 and yx = w. By induction
we know B ←•δ3Aδ4 is in δ(q0, δ2). Then by rule (3) we have that B →
δ3 • Aδ4 is in δ(q0, δ2δ3). Finally, by rule (2) we conclude A →•αβ is in
δ(q0, δ2δ3) = δ(q0, δ′). The proof is complete.
We now formally deﬁne the concept of LR(0) grammar.
ϵ
q0
B →α ′ • A
′
γ
A →•β
β
Figure 5.9: Case 2 of the induction step

168
5
Parsing and recognition
Deﬁnition. A grammar G = (V, , P, S) is LR(0) if each of the following
conditions holds:
(a) G has no useless symbols.
(b) The start symbol S does not appear on the right-hand side of any pro-
duction.
(c) For all viable preﬁxes γ , if A →α• is a complete item valid for γ , then
no other complete item nor any item with a terminal immediately to the right
of the • is valid for γ .
How do we parse using the Knuth DFA? An LR(0) parser is a deterministic
PDA that generates a rightmost derivation. The stack of the DPDA holds a viable
preﬁx of a right sentential form α, including all variables of α. Actually, the
stack holds this viable preﬁx together with states of the Knuth DFA interspersed
between symbols of the viable preﬁx. The remainder of α appears as the
unexpended input.
Initially, the LR(0) parser is in a conﬁguration (q, w, q0), where q0 now
means the initial state of the corresponding Knuth DFA for the grammar. (Our
description uses only a single state q, but allows the PDA to pop multiple
symbols in a single step. Any implementation by a normal PDA would require
extra states to handle these pops.)
At each step, the parser has two choices: (i) to shift a symbol from the input
to the stack, updating the state of the Knuth DFA, or (ii) to “reduce”or pop 2|α|
symbols from the stack, where A →α• is a complete item on top of the stack,
and then push A and the appropriate state of the Knuth DFA back on top of the
stack. For this reason, LR parsers are sometimes called shift-reduce parsers.
More formally, an LR(0) parser behaves as follows: a typical conﬁguration
before a move looks like
(q, atat+1 · · · an, qkXkqk−1Xk−1 · · · q1X1q0),
where x = a1 · · · an is the input, X1 · · · Xkatat+1 · · · an is the current right
sentential form, and qj = δ(qj−1, Xj), 1 ≤j ≤k, where δ is the transition
function of the Knuth DFA. If qk contains a complete item of the form A →α•,
then α = Xi+1 · · · Xk for some i ≥0, and the new conﬁguration is
(q, atat+1 · · · an, q′AqiXiqi−1 · · · q1X1q0),
where q′ = δ(qi, A). Otherwise, the new conﬁguration is
(q, at+1 · · · an, q′atqkXkqk−1Xk−1 · · · q1X1q0),
where q′ = δ(qk, at). We accept, by emptying the stack, if there is a complete
item S →α• on top of the stack.

5.5 Bottom-up parsing
169
Let us prove that the LR(0) parsing method works. First we prove the
following theorem.
Theorem 5.5.4. Let G be an LR(0) grammar, let x ∈L(G), and let α ̸= S be
a right sentential form appearing in a derivation of x, that is, suppose S
∗
=⇒
α
∗
=⇒x by a rightmost derivation. Then there is a unique right sentential form
β such that S
∗
=⇒β =⇒α
∗
=⇒x.
Proof. Suppose the rightsententialform is α = X1X2 · · · Xky,y ∈∗,andone
rightmost derivation is S
∗
=⇒X1X2 · · · XjAy =⇒α = X1X2 · · · Xky
∗
=⇒x,
using the production A →Xj+1 · · · Xk. Suppose there is another possible right
sentential form previous to α, and consider the corresponding right end of the
handle in α. There are three possibilities:
(i) the handle ends to the right of Xk (and hence the end is inside y);
(ii) the handle ends at Xk;
(iii) the handle ends at Xt for some t < k.
Consider s = δ(q0, X1X2 · · · Xk) in the Knuth DFA for G. Then s contains a
complete item, namely A →Xj+1 · · · Xk•. But by the LR(0) rules, this means
that s contains no other complete items (ruling out case (ii)) and contains no
items with a terminal immediately to the right of the dot (ruling out case (i)).
Finally, we have to rule out case (iii). To do so, suppose there is a rightmost
derivation
X1X2 · · · XrBXt+1 · · · Xky =⇒X1X2 · · · Xky
using a production B →Xr+1 · · · Xt. Since the derivation is rightmost,
each of Xt+1, . . . , Xk is a terminal.
(5.13)
Now complete item B →Xr+1 · · · Xt• is valid for viable preﬁx X1 · · · Xt, but
then, since X1X2 · · · Xk is also a viable preﬁx, there must be some other item
valid for X1 · · · Xt. And by (5.13), this item must have a terminal to the right of
the dot or be complete. But this would violate the LR(0) rules, a contradiction.
Corollary 5.5.5. If G is LR(0), then it is unambiguous.
Proof. For every right sentential form in the derivation of w ∈L(G), there is
only one previous right sentential form.
Now we prove that the LR(0) algorithm works.

170
5
Parsing and recognition
Theorem 5.5.6. Let M be the DPDA speciﬁed earlier, based on the LR(0)
grammar G. Then L(G) = Le(M), where by Le we mean acceptance by empty
stack.
Proof. First we prove that Le(M) ⊆L(G).
Suppose that x ∈Le(M). We will prove that x ∈L(G) by producing a
rightmost derivation of x.
We deﬁne αi, the right sentential form represented by the conﬁguration of
the DPDA M at step i, to be the string X1X2 · · · Xky if the DPDA at step i has
conﬁguration
(q, y, skXksk−1 · · · s1X1s0).
We will prove the following two assertions by induction on i. Let α−1 = α0.
1. δ(q0, X1X2 · · · Xj) = sj for all j, 0 ≤j ≤k, where δ is the transition func-
tion of the associated Knuth DFA.
2. Either αi =⇒rm αi−1 or αi = αi−1.
For i = 0, both (1) and (2) are true. (1) is true since the initial conﬁguration
of the DPDA is (q, x, q0), and δ(q0, ϵ) = q0. (2) is true since α0 = α−1 = x.
Now assume the assertions are true for steps <i; we prove them for i.
Suppose the conﬁguration of the DPDA before step i is (q, y, skXksk−1
· · · s1X1s0). At step i the DPDA either reduces or shifts.
(A) Reduce move: If the DPDA makes a reduce move, we know that sk con-
tains a complete item A →γ •. By induction sk = δ(q0, X1X2 · · · Xk). Since
δ is the transition function for the Knuth automaton, we know that A →γ •
is valid for viable preﬁx X1X2 · · · Xk. In other words, there exists a rightmost
derivation
S
∗
=⇒βAz =⇒βγ z,
where βγ = X1X2 · · · Xk. It follows that γ is a sufﬁx of X1X2 · · · Xk and hence
when we pop 2|γ | symbols from the stack we are left with
sjXjsj−1 · · · s1X1s0
for some j, with X1 · · · Xj = β, Xj+1 · · · Xk = γ . Then we push A and δ(sj, A)
maintaining the invariant (1).
On the other hand, the invariant (2) is preserved because we have
αi = X1X2 · · · XjAy =⇒X1X2 · · · Xjγy = X1X2 · · · Xky = αi−1.
(B) Shift move: If the DPDA makes a shift move, the invariant (1) is trivially
preserved and (2) is preserved because αi = αi−1.

5.5 Bottom-up parsing
171
Finally, since x ∈Le(M), we know the DPDA eventually empties its stack
and accepts its input. This can occur only if at some step, say step n, the
conﬁguration is (q, ϵ, skXksk−1 · · · s1X1s0) and sk contains a complete item of
the form S →γ •. If this is the case, by the reasoning given earlier, there is a
rightmost derivation
S
∗
=⇒βSz =⇒βγ z,
with X1X2 · · · Xk = βγ . However, since S does not appear on the right-hand
side of any production, we must have S = βSz, so it follows that β = ϵ and
z = ϵ. Hence, X1X2 · · · Xk = γ . Now deﬁne αn+1 = S. Then we have αi
∗
=⇒
αi−1 for 1 ≤i ≤n + 1. Since α0 = x, this gives a derivation of x in G.
Now let us show that L(G) ⊆Le(M). Let x ∈L(G). As we have seen in
Theorem 5.5.4, there is only one rightmost derivation S
∗
=⇒x. Suppose this
derivation is of length n and
S = αn =⇒αn−1 =⇒· · · =⇒α0 = x.
We want to argue that M, when given x, eventually pops its stack and halts.
To do so, we need to create a measure of “progress”toward an accepting
computation. Suppose C = (q, y, skXksk−1 · · · s1X1s0) is a conﬁguration of
M on input x. If X1X2 · · · Xky = αi, then we deﬁne the weight of C to be
n −i + |y|. We then argue that each move of the DPDA is correct and reduces
the weight of its conﬁguration.
Initially, the conﬁguration is (q, x, q0), with weight n + |x|. At each step,
M either reduces or shifts. Consider what happens at step i.
If M reduces, then there must be a complete item on top of the stack. By
Theorem 5.5.4, there is only one handle in X1 · · · Xky and it must be Xj+1 · · · Xk
with corresponding production A →Xj+1 · · · Xk. The machine now performs
a reduce move, and the corresponding weight decreases by 1.
If M shifts, then there is no complete item on top of the stack. We now
argue that shifting is the right thing to do. Suppose there were a complete item
A →γ buried in the stack. Then this complete item would have been added at
some point. Consider the very next step. Since a complete item is on top, we
would do a reduce move, popping 2|γ | symbols from the stack, so if γ ̸= ϵ,
this complete item gets popped from the stack and cannot be buried. If γ = ϵ,
then A and δ(sk, A) are put on top of A →• in the stack. If, in any future step,
X1X2 · · · Xk has not risen to the top of the stack, then there will be a variable
on top of X1X2 · · · Xk. But then ϵ (via A →ϵ) cannot be the handle of any
right sentential form X1X2 · · · Xkϵβ, where β contains a variable, because then
X1X2 · · · XkAβ =⇒X1X2 · · · Xkβ would not be a rightmost derivation. Thus

172
5
Parsing and recognition
the handle must include some symbols of the input, and shifting is the right
thing to do. This reduces the weight of the conﬁguration by 1.
Eventually, the weight of the conﬁguration becomes 0. At this point, we have
i = n and y = ϵ, and the DPDA pops its stack, accepting. Thus, x ∈Le(M).
5.6 Exercises
1. Consider the grammar G given by the following productions:
S →AB | b
A →BC | a
B →AS | CB | b
C →SS | a.
Using the CYK algorithm, show that babbbab ∈L(G) and ﬁnd a parse tree
for this string. Show the tables in the algorithms.
2. Suppose G is a CFG and w ∈L(G). Show how to compute the number of
distinct parse trees for w in G in polynomial time.
3. Give an example of a grammar for which constructing the parse table by
Earley’s method uses 
(n3) steps. Hint: Consider the grammar S →SS | a.
4. Compute the table M(γ, x) for the grammar
S →aA | aB
A →a
B →b.
Is this grammar LL(1)?
5. Find an LL(1) grammar for the following set: the set of strings over {a, b}∗
containing an equal number of a’s and b’s. Be sure to prove that your
grammar is correct and that it is LL(1).
6. Let G be a CFG with no useless symbols. Prove that G is an LL(1) grammar
if and only if, for any two distinct productions of the form X →α,
and X →β, the following holds: if x and y are in FOLLOW(X), then
FIRST(αx) ∩FIRST(βy) = ∅. The symbols x and y need not be distinct.
7. Show that every regular language has an LL(1) grammar.
8. Give an example of an LR(0) grammar such that there exists a viable preﬁx
γ and items A →•, B →α • β, which are both valid for γ .
9. Let G = (V, , P, S) be an unambiguous grammar with no useless symbols
and w1w2 · · · wn be a string in ∗. Show that if α ̸= ϵ, the algorithm to
construct the Earley table attempts to add an item A →α • β to Mij at most
once.

5.8 Notes on Chapter 5
173
10. Give an example of a grammar that is LL(k + 1) but not LL(k).
∗11. Give an example of a context-free language that is LL(k + 1) but not
LL(k).
∗∗12. Give an example of a context-free language that is not LL(k) for any k.
13. Show that if i, j, m, i1, . . . , im+1 are all integers with i1 = i, im+1 = j,
m ≥2, and i1 < i2 < · · · < im < im+1, then ((i2 −i1)2 + · · · + (im+1 −
im)2) is bounded by (j −i −1)2 + 1.
14. Let G be an LR(0) grammar with A →α•, α ̸= ϵ, valid for some viable
preﬁx γ . Prove that no other item can be valid for γ .
5.7 Projects
1. Study software packages for LR parsing, such as Yacc and Bison. How
do they compare in terms of features and efﬁciency?
2. Read about Valiant’s method for recognition in o(n3) time. You can start
with Valiant [1975].
5.8 Notes on Chapter 5
For Valiant’s result on parsing general CFGs, see Valiant [1975].
5.1 For the CYK algorithm, see, for example, Younger [1967].
5.2 Earley’s original paper is Earley [1970], but this is somewhat difﬁcult
to read. I have followed Urbanek [1990] for one direction of the proof
of Theorem 5.2.3. While very clever, this paper unfortunately contains
several typographical errors. I have followed Aho and Ullman [1972]
for other parts of the presentation.
5.3 For two early papers on top-down parsing, see Rosenkrantz and Stearns
[1970] and Lewis and Stearns [1968]. Our presentation is based on the
book of Drobot [1989], but we have corrected many errors.
5.4 The material in this section is based on Aho, Sethi, and Ullman [1986].
5.5 For LR(k) grammars, see Knuth [1965].
The material in this section is based on Hopcroft and Ullman [1979],
but the proof of correctness (Theorem 5.5.6) is new and improved.

6
Turing machines
In this chapter we explore some advanced topics relating to Turing machines
(TMs): Kolmogorov complexity and unsolvability aspects of context-free gram-
mars and languages. We begin by discussing unrestricted grammars and their
languages.
6.1 Unrestricted grammars
In previous chapters we have studied context-free grammars (CFGs) and vari-
ants as LL(1) and LR(0) grammars. None of these grammars is powerful enough
to generate the class of recursively enumerable languages. We introduce a new
model, the unrestricted grammar, which has more power.
In an unrestricted grammar, both the left and right side of productions can
be any string of variables and terminals, subject to the left side being nonempty.
In other words, a production is of the form α →β, with α ∈(V ∪)+ and
β ∈(V ∪)∗. We apply a production in the same manner as for other kinds of
grammars; that is, if a sentential form is γ αδ, then we can apply the production
α →β to get the new sentential form γβδ. As usual we write γ αδ =⇒γβδ
and let
∗
=⇒be the reﬂexive, transitive closure of =⇒. Finally, as usual we
deﬁne L(G) for an unrestricted grammar G to be {w ∈∗: S
∗
=⇒w}.
Example 6.1.1. The following is an unrestricted grammar for the language
{ai2 : i ≥1}:
S →BRAE
B →BRAA
RA →aAR
Ra →aR
RE →E
B →X
174

6.1 Unrestricted grammars
175
XA →X
Xa →aX
XE →ϵ.
Here is the idea behind this example. It is based on the identity 1 + 3 +
5 + · · · + (2n −1) = n2. The B and E symbols are endmarkers. The ﬁrst two
productions create strings of the form B(RAA)iRAE for all i ≥0. Thus, each
R has an odd number of As to its right. Each R then moves to the right, creating
a new a for each A encountered. When R hits E at the right, it disappears.
Finally, the symbol B changes to X and causes the As, X, and E to disappear.
Here are two examples of derivations using this grammar:
S =⇒BRAE =⇒BaARE =⇒BaAE =⇒XaAE =⇒aXAE =⇒aXE =⇒a.
S
∗
=⇒BRAARAE =⇒BRAARAARAE =⇒BaARARAARAE
=⇒BaARARAAaARE =⇒BaAaARRAAaARE
=⇒BaAaARaARAaARE =⇒BaAaARaARAaAE
=⇒BaAaARaAaARaAE =⇒BaAaAaRAaARaAE
=⇒BaAaAaaARaARaAE =⇒BaAaAaaAaRARaAE
=⇒BaAaAaaAaaARRaAE =⇒BaAaAaaAaaARaRAE
=⇒BaAaAaaAaaAaRRAE =⇒BaAaAaaAaaAaRaARE
=⇒BaAaAaaAaaAaRaAE =⇒BaAaAaaAaaAaaRAE
=⇒BaAaAaaAaaAaaaARE =⇒BaAaAaaAaaAaaaAE
=⇒XaAaAaaAaaAaaaAE =⇒aXAaAaaAaaAaaaAE
=⇒aXaAaaAaaAaaaAE =⇒aaXAaaAaaAaaaAE
=⇒aaXaaAaaAaaaAE
∗
=⇒aaaaaaaaaXAE
=⇒aaaaaaaaaXE =⇒aaaaaaaaa.
The following two theorems characterize the power of unrestricted gram-
mars.
Theorem 6.1.2. Let G = (V, , P, S) be an unrestricted grammar. Then L(G)
is recursively enumerable.
Proof.
We show how to construct a TM accepting L(G). Our TM is a non-
deterministic four-tape model. Tape 1 holds the input w, and will never change.
Tape 2 holds a sentential form. Tape 3 holds the left side of a production, and
tape 4 holds the corresponding right side.

176
6
Turing machines
Initially, we write S on tape 2. Now, we perform the following loop forever,
until a halting state is reached. First, we nondeterministically choose a cell on
tape 2. Then, we nondeterministically choose a production rule α →β and
write α on tape 3 and β on tape 4. Then we see if the symbols on tape 3 match
the symbols beginning in the cell being scanned on tape 2. If they do, we replace
the symbols with the contents of tape 4; this will require shifting the rest of the
symbols on tape 2 to the left if |α| > |β| and to the right if |α| < |β|. Next, we
compare the contents of tape 1 with tape 2; if they agree, we accept. Otherwise,
we return to the beginning of the loop.
We know from Exercise 1.29 that a nondeterministic four-tape machine is
equivalent to a deterministic one-tape machine, and hence L(G) is recursively
enumerable.
Theorem 6.1.3. Let L be a recursively enumerable language. Then there exists
an unrestricted grammar G such that L(G) = L.
Proof. If L is recursively enumerable, then there exists a deterministic one-tape
TM M accepting L. We now modify M to get a nondeterministic “language
generator”M′ as follows: M′ has two tracks. Starting with an initially blank
tape, M′ writes a nondeterministically-chosen string w ∈∗on track 1. Then
it copies w to track 2. Then it simulates M on track 1, and if M accepts, M′
erases track 1, copies track 2 back to the tape (so the second track disappears
and there is now just one track), and enters the halting state h. If M does not
accept, M′ crashes (by not having a next move).
In order to erase the tape, we need a small technical trick. Using Exer-
cise 1.30, we can assume that M′ never writes a blank symbol B; instead, it
writes a new symbol B. Furthermore, we may assume that M′ stops with the
tape head immediately to the right of w, scanning a B or B. Thus, M′ has the
following behavior: it starts with a blank tape, and after some computation, it
eventually halts with a nondeterministically chosen member of L(M) (if one
exists) written on its tape, followed by some number of B symbols, with the
rest of the tape consisting of B symbols. We assume the tape head is scanning
the ﬁrst B or B symbol when the machine halts.
Once we have such an M′ = (Q, , , δ, q0, h), we create an unrestricted
grammar mimicking its computations, with productions as follows:
S →q0S1
S1 →BS1 | T
pX →qY for all X, Y ∈, p, q ∈Q such that (q, Y, S) ∈δ(p, X)
pX →Yq for all X, Y ∈, p, q ∈Q such that (q, Y, R) ∈δ(p, X)

6.2 Kolmogorov complexity
177
ZpX →qZY for all X, Y, Z ∈, p, q ∈Q such that (q, Y, L) ∈δ(p, X)
hB →h
hB →h
hT →ϵ.
To see that this construction works, note that the productions have been
designed in such a way that intermediate sentential forms represent conﬁgura-
tions of M′. Starting with S, we derive a string of the form q0Bn for some n. We
then perform moves of M′ until h is reached, after which point the remaining B
and B symbols are removed, and the h symbol is removed. The resulting string
is the contents of the tape of M′, which contains a nondeterministically chosen
element of L(M), if one exists.
6.2 Kolmogorov complexity
In this section we discuss the basic notions of Kolmogorov complexity.
When is a string of symbols complex? Intuitively, a string such as
0101010101010101010101010
is not complex, because there is an easy way to describe it, whereas a string
such as
0110101011001110010001010
is complex because it appears to have no simple description.
Here is another way to think about it. Suppose I ﬂip a fair coin, recording
0 for heads and 1 for tails. If I produce 0110101011001110010001010 and
claim that it is a record of 25 tosses, no one would be surprised. But a few
eyebrows would be raised if I produced 0101010101010101010101010 as
my record of tosses. Thus complexity and randomness are linked.
Laplace noticed this connection two centuries ago. In 1819, he wrote
In the game of heads and tails, if heads comes up a hundred times in a row then this
appears to us extraordinary, because the almost inﬁnite number of combinations
that can arise in a hundred throws are divided in regular sequences, or those in
which we observe a rule that is easy to grasp, and in irregular sequences, that are
incomparably more numerous.
Kolmogorov complexity is a way to measure the complexity, or randomness,
of a ﬁnite string. Roughly speaking, the Kolmogorov complexity C(x) of a string
x is the size (number of bits) in the shortest Pascal program P + input i that

178
6
Turing machines
will print x and then halt. (If you do not like Pascal, feel free to substitute C,
APL, or your favorite programming language.)
If the Kolmogorov complexity of a string x is small, then there is a simple
way to describe x. If the Kolmogorov complexity of x is large, then x is hard
to describe; we say it is “complex”,“random”,or possesses “highinformation
content.”
We can also view the combination of program and input (P, i) as an optimal
way to compress x. In this interpretation, instead of storing x, we could store
(P, i), since we could always recover x by running P on input i. (Note that this
approach disregards the running time of P on input i.)
For a more formal deﬁnition of Kolmogorov complexity, we need a universal
TM U. The input to U is a self-delimiting binary encoding of a TM T , followed
by y ∈{0, 1}∗, the input for T . By self-delimiting we mean that given e(T )y
we can tell where the encoding of T ends and y begins. We assume that T ’s
input alphabet, as well as U’s, is {0, 1}. U then simulates T on input y. It is
assumed that T has an output tape, and the output of U is what T outputs if
and when it halts.
Then C(x) is formally deﬁned to be the length of a shortest input e(T )y that
causes U to output x.
Theorem 6.2.1. We have C(x) ≤|x| + O(1).
Note that the constant in the big-O is independent of x.
Proof. Informally, we can use the following Pascal program:
program print(input);
begin
write(input);
end.
Clearly, the length of this program is |x| + c, where c is the number of
characters in the preceding template.
Formally, there exists some TM T that simply copies the input to the output.
Then the input to U is e(T )x, which is of length |x| + |e(T )| = |x| + O(1).
Example 6.2.2. Let us show C(xx) ≤C(x) + O(1). Informally, given a Pascal
program P to print x, we simply call it twice to print xx. The extra cost to build
the “wrapper”program and call P twice corresponds to the O(1) term.
The next theorem, called the invariance theorem, shows that the particular
choice of programming language or universal TM is irrelevant, at least up to
an additive constant.

6.2 Kolmogorov complexity
179
Theorem 6.2.3. Suppose we deﬁne CAPL, CJAV A, and so on, analogously.
Then we have, for example,
CAPL(x) ≤C(x) + O(1);
C(x) ≤CAPL(x) + O(1).
Thus all these measures are the same up to an additive constant.
Proof.
We prove C(x) ≤CAPL(x) + O(1), leaving the other direction to
the reader. Suppose CAPL(x) = d. Then there exists an APL program p to
print out x, of size d. Now write an APL interpreter as a TM T ; such a
machine can be fed with p to output x. Thus, C(x) ≤d + O(1) as desired,
since C(x) ≤|e(T )p| = |e(T )| + |p|.
We can think of the representation of a string x as e(T )y as a sort of optimal
“compression”method (like the Unix compress command). The e(T ) captures
the “regular”aspects of x, while the y captures the “irregular”aspects of x.
We call a string x incompressible or random if C(x) ≥|x|. We cannot
explicitly exhibit long incompressible strings x, but we can prove they exist:
Theorem 6.2.4. For all n ≥0, there exists at least one string x of length n
such that C(x) ≥|x|.
Proof.
There are 2n strings of length n, but at most 1 + 2 + · · · + 2n−1 =
2n −1 shorter descriptions.
Unfortunately, Kolmogorov complexity is uncomputable, so perfect com-
pression is unattainable.
Theorem 6.2.5. The quantity C(x) is uncomputable.
Proof. Assume C(x) is computable by a TM T that takes x as input. Create a
new TM T ′ that, on input l, examines all strings of size l in lexicographic order
until it ﬁnds a string y with C(y) ≥|y| = l, using T as a subroutine. Such a
string exists by Theorem 6.2.4. Then T ′ outputs y.
Now let us compute the Kolmogorov complexity of y. On the one hand, we
have C(y) ≥l. On the other hand, the string y is completely determined by T ′
and l, so C(y) ≤|e(T ′)| + (log2 l) + 1.
l ≤C(y) ≤|e(T )| + (log2 l) + c
(6.1)
for a constant c. Now choose l sufﬁciently large so that l > |e(T )| + (log2 l) +
c. This inequality contradicts Eq. (6.1).

180
6
Turing machines
6.3 The incompressibility method
The basic idea in this method is that “most”strings cannot be compressed very
much. Generally speaking, a proof works by selecting a typical instance and
arguing about its properties. In the incompressibility method, we pick a random
“incompressible”string and argue about it.
Example 6.3.1. Let π(x) denote the number of primes ≤x. A celebrated
theorem known as the prime number theorem states that π(x) ∼
x
log x . Using
the incompressibility method, however, we can prove the weaker inequality
π(n) > cn/(log n)2 for inﬁnitely many n. In fact, we show that this inequality
is true for inﬁnitely many n of the form n = 1 + ⌊2dm(log2 m)2⌋for a constant
d.
Consider the ordinary binary representation of the nonnegative integers, so
that, for example, 43 is represented by 101011. If n ≥1 is represented by
a string x, then it is easy to see that |x| = ⌊log2 n⌋+ 1. Unfortunately, there
are also other possible representations for 43, such as 0101011. To avoid the
“leadingzeroes”problem, we can deﬁne a 1–1mapping between the natural
numbers and elements of {0, 1}∗as follows: e(n) is deﬁned to be the string
obtained by taking the ordinary base-2 expansion of n + 1 and then dropping
the leading bit 1. For example, the representations of the ﬁrst eight natural
numbers are given in the following table:
n
e(n)
0
ϵ
1
0
2
1
3
00
4
01
5
10
6
11
7
000
Note that |e(n)| = ⌊log2(n + 1)⌋+ 1 −1 = ⌊log2(n + 1)⌋. Now t + 1 ≤2t
for t ≥1, so
log2(t + 1) ≤log2(2t) ≤(log2 t) + 1.
(6.2)
It follows that
(log2 n) −1 ≤|e(n)| ≤(log2 n) + 1.
(6.3)

6.3 The incompressibility method
181
Previously we deﬁned a binary string x to be random if C(x) ≥|x|. Since we
now have a bijection between binary strings and natural numbers, we can deﬁne
a natural number N to be random if C(e(N)) ≥|e(N)|. By Theorem 6.2.4, there
exist inﬁnitely many random integers.
We will also need a preﬁx-free encoding of the natural numbers. By preﬁx-
free, we mean that no preﬁx of an encoding of a number m is the encoding
of some other number. Preﬁx-free encodings are useful because they enable
us to encode k-tuples of integers by simply concatenating the encodings. The
preﬁx-free property then ensures unique decoding.
There are many different ways to create preﬁx-free encodings, but the fol-
lowing one will sufﬁce for us. Given a natural number m, we deﬁne
E(m) = 1|e(|e(m)|)|0e(|e(m)|)e(m).
Here are some examples of this encoding.
m
E(m)
0
0
1
1000
2
1001
3
10100
4
10101
5
10110
6
10111
7
11000000
From (6.3) we get
|E(m)| ≤2(log2((log2 m) + 1) + 1) + 1 + (log2 m) + 1
≤2(log2 log2 m + 2) + log2 m + 2
= 2 log2 log2 m + log2 m + 6.
Before we get started on the main result, let us use the ideas given earlier
to give a proof that there are inﬁnitely many prime numbers. Suppose there are
only ﬁnitely many primes, say p1, p2, . . . , pk, and let N be random. Then
C(e(N)) ≥|e(N)| ≥(log2 N) −1.
(6.4)
On the other hand, from the well-known result that every integer can be factored
as a product of primes, we can write N = pa1
1 · · · pak
k for nonnegative integers
a1, a2, . . . , ak. Clearly, ai ≤log2 N for 1 ≤i ≤k. It follows that we can encode
N by e(T )E(a1)E(a2) · · · E(ak), where T is a TM that reconstructs N from

182
6
Turing machines
the exponents in its prime factorization. The length of this representation is
O(k log2 log2 N), which contradicts (6.4).
Now let us prove π(n) > cn/(log n)2 for inﬁnitely many n. We note that an
integer N can be encoded by the string x := e(T )E(m)e(N/pm), where pm is
the largest prime dividing N and T is a TM that deduces m, computes pm, and
multiplies it by N/pm to get N. If N is random, then the length of x must be
at least as long as e(N). Hence we have
|e(T )E(m)e(N/pm)| ≥(log2 N) −1
for inﬁnitely many N. In fact, among these inﬁnitely many N, there must be
inﬁnitely many distinct m, for otherwise all random N could be factorized into
some ﬁnite set of primes, which is impossible as we have seen earlier.
Using the preceding inequalities for E and e, we get
log2 pm ≤log2 m + 2 log2 log2 m + d
for some constant d ≥1. Now, raising 2 to both sides, we get pm ≤
2dm(log2 m)2. Now set n = ⌊2dm(log2 m)2⌋+ 1. We then have π(n) ≥m.
It now remains to see that m ≥
n
2d(log2 n)2 . Assume, contrary to what we want
to prove, that
m <
n
2d(log2 n)2 .
(6.5)
From our deﬁnition of n, we have
2dm(log2 m)2 < n ≤2dm(log2 m)2 + 1.
(6.6)
Thus,
log2 n > d + log2 m + 2 log2 log2 m.
(6.7)
Now, using (6.5) ﬁrst and then (6.7), we get, for m ≥2,
n > 2dm(log2 n)2
> 2dm(d + log2 m + 2 log2 log2 m)2
≥2dm(log2 m + 1)2
> 2dm(log2 m)2 + 1,
which contradicts (6.6). Thus our assumption in (6.5) is false and hence m ≥
n
2d(log2 n)2 . Since π(n) ≥m, we get π(n) ≥
n
2d(log2 n)2 , our desired result.
We now turn to applications of the incompressibility method to formal
languages, speciﬁcally, to proving that certain languages are not regular.

6.4 The busy beaver problem
183
Example 6.3.2. Let L = {0k1k
: k ≥1}. We prove that L is not regu-
lar. Suppose it were. Then it would be accepted by a deterministic ﬁnite
automaton (DFA) M = (Q, , δ, q0, F). We could then encode each inte-
ger n by providing a description of M (in O(1) bits) and q = δ(q0, 0n)
(in O(1) bits), because then n is uniquely speciﬁed as the least i with
δ(q, 1i) ∈F. Hence, C(e(n))) = O(1). But there exist inﬁnitely many n with
C(e(n)) ≥log2 n + O(1), a contradiction.
We can generalize the previous example, as follows:
Lemma 6.3.3. Let L ⊆∗be regular, and deﬁne Lx = {y : xy ∈L}. Then
there exists a constant c such that for each x, if z is the nth string in Lx in
lexicographic order, then C(z) ≤C(e(n)) + c.
Proof. The string z can be encoded by the DFA for L (in O(1) bits), plus the
state of the DFA after processing x (in O(1) bits), and the encoding e(n).
We now consider some applications of this lemma.
Example 6.3.4. Let us prove that L = {1p : p prime} is not regular. Let
x = 1pk, where pk is the kth prime. Then the second element of Lx is y =
1pk+1−pk. But as k →∞, the difference pk+1 −pk is unbounded (because, for
example, the n −1 consecutive numbers n! + 2, n! + 3, n! + 4, . . . , n! + n
are all composite for n ≥2). Hence, C(1pk+1−pk) is unbounded. However, by
Lemma 6.3.3, we have C(y) ≤C(e(2)) + c = O(1), a contradiction.
Example 6.3.5. Let us prove that L = {xxRw : x, w ∈{0, 1}+} is not regular.
Let x = (01)m, where m is random (i.e., C(e(m)) ≥|e(m)| ≥log2 m −1). Then
the lexicographically ﬁrst element of Lx is y = (10)m0. Hence, C(y) = O(1).
But C(y) ≥log2 m + O(1), a contradiction.
Example 6.3.6. Let us prove that L = {0i1j : gcd(i, j) = 1} is not regular.
Let x = 0(p−1)!1, where p is a prime, and |e(p)| = n. Then the second word in
Lx is y = 1p−1, which gives C(y) = O(1). But C(e(p)) ≤C(y) + O(1), and
there are inﬁnitely many primes, so C(e(p)) = O(1) for inﬁnitely many primes
p, a contradiction.
6.4 The busy beaver problem
In this section we describe a problem of Rado on TMs now known as the busy
beaver problem. We assume that our TMs are deterministic and have a tape
alphabet consisting of a single symbol 1 and the usual blank symbol B. (In
Rado’s original description, the symbol 0 was used as a blank.) We also assume

184
6
Turing machines
B/1, R
3
1
2
B/1, L
h
1/1, R
B/1, L
1/1, L
1/1, R
Figure 6.1: A three-state busy beaver
that our TM has a single “doublyinﬁnite”tape, initially completely blank, and
that the machine must move either right or left at each step—it cannot remain
stationary. There is a single halting state from which no transitions emerge, and
this halting state is not counted in the total number of states.
Rado’s function (n) is deﬁned to be the maximum number of (not neces-
sarily consecutive) 1s left on the tape after such an n-state TM halts. He also
deﬁned a function S(n) that counts the maximum number of moves that can be
made by an n-state halting TM of this form.
For example, consider the three-state TM as shown in Figure 6.1.
If this machine is started with a tape of all blanks, it halts after 13 moves
with six consecutive 1s on the tape—check this. This shows that (3) ≥6 and
S(3) ≥13. In fact, it can be shown that (3) = 6 and S(3) = 21.
If a TM writes the maximum possible number of 1s for its number of
states—that is, (n) 1s—then it is called a “busy beaver.”Busy beavers are
hard to ﬁnd, even for relatively small n, for two reasons. First, the search space
is extremely large: there are (4(n + 1))2n different TMs with n states. (For each
nonhalting state, there are two transitions out, so there are 2n total transitions,
and each transition has two possibilities for the symbol being written, two
possibilities for the direction to move—left or right, and (n + 1) possibilities
for what state to go to—including the halting state.) Second, it is in general not
possible to determine whether a particular TM will halt, so it may not be easy
to distinguish between a machine that goes into an inﬁnite loop from one that
goes for thousands, millions, or billions of steps before halting. For example,
it is known (by explicitly producing an example) that S(6) ≥102879. It is also
known that (6) ≥101439.
In fact, we will show in a moment that neither (n) nor S(n) is computable
function. This means that there is no halting TM that, on arbitrary input n,
will always halt and successfully compute these functions. Nevertheless, it is
possible to compute (n) and S(n) for some small values of n by a brute-force
approach, and this has been done by many investigators.

6.4 The busy beaver problem
185
n
(n)
S(n)
Source
1
1
1
Lin and Rado
2
4
6
Lin and Rado
3
6
21
Lin and Rado
4
13
107
Brady
5
4,098
47,176, 870
Marxen and Buntrock
6
101439
102879
Ligocki and Ligocki
≥
≥
≥
≥
Figure 6.2: (n) and S(n) for 1 ≤n ≤6
Figure 6.2 shows what is known about (n) and S(n) for 1 ≤n ≤6.
Figure 6.3 illustrates the six-state TM that makes about >102879 moves
before halting with about 101439 1s on its tape.
Now let us prove that neither (n) nor S(n) is computable.
Theorem 6.4.1. The function (n) is not computable by a TM.
Proof. The idea is to show that if f (n) is any computable function, then there
exists n0 such that (n) > f (n) for n ≥n0.
Our model of computable function is that a TM calculating f (n) starts with
a tape with a block of n 1s immediately to the right of the starting blank and
halts after a ﬁnite number of moves with a block of f (n) consecutive 1s on the
tape.
Let f be an arbitrary computable function and deﬁne
F(x) =

0≤i≤x
(f (i) + i2).
Since f is computable, so is F. In fact, there is a TM MF that, for all
strings y, when started in the conﬁguration yB1xq0B halts in the conﬁgura-
tion yB1xB1F(x)hB. Assume MF has n states.
1
5
h
B/1,L
1/B,R
1/B, L
B/1, L
B/1, R
B/1, L
1/1, L
B/1, L
1/B, L
1/1, R
6
2
3
4
B/1, L
1/B,R
Figure 6.3: A six-state busy beaver candidate

186
6
Turing machines
Consider a TM M that, on input ϵ, ﬁrst writes x 1s on an initially blank tape
and then halts with its head on the blank immediately following the rightmost
1. This can be done with x + 1 states. Next, M simulates MF on this tape,
resulting in the conﬁguration B1xB1F(x)hB. Finally, M simulates MF again on
this tape, resulting in the conﬁguration B1xB1F(x)B1F(F(x))hB. This machine has
x + 1 + 2n states.
Now any busy beaver machine of x + 1 + 2n states will leave at least as
many 1s as M does when started on input ϵ. Hence we have
(x + 1 + 2n) ≥x + F(x) + F(F(x)).
But from its deﬁnition, F(x) ≥x2, and there exists a constant c1 such that
x2 > x + 1 + 2n for all x ≥c1. It follows that F(x) > x + 1 + 2n for x ≥c1.
Now from its deﬁnition we have F(x) > F(y) if x > y, so we have F(F(x)) >
F(x + 1 + 2n) for x ≥c1. It follows that
(x + 1 + 2n) ≥x + F(x) + F(F(x)) > F(F(x)) > F(x + 1 + 2n)
≥f (x + 1 + 2n)
for x ≥c1. It follows that  is eventually greater than f . Since f was arbitrary,
 is noncomputable.
Corollary 6.4.2. The function S(n) is also noncomputable.
Proof. There exists a TM M with n states that writes (n) 1s on its tape before
halting. Such a TM must make at least (n) moves. Hence, S(n) ≥(n). Since
(n) is eventually greater than any computable function, so is S(n). Hence, S
is also noncomputable.
6.5 The Post correspondence problem
In this section we discuss a famous unsolvable problem, the Post correspon-
dence problem, often abbreviated as PCP. The problem is very simple to de-
scribe: we are given as input two morphisms g, h : ∗→∗, and the question
we would like to solve is, does there exist a nonempty word x ∈∗such that
g(x) = h(x)?
Example 6.5.1. Consider the morphisms g and h deﬁned by
g(1) = 001
h(1) = 00
g(2) = 11
h(2) = 011
g(3) = 01
h(3) = 000
g(4) = 010
h(4) = 10.

6.5 The Post correspondence problem
187
This instance of PCP has no solution, for if g(x) = h(x), then x must start with
1; however, this choice results in one more 1 in the image of g than in the
image of h, and subsequence choices of letters do not allow this difference to
be made up.
On the other hand, the PCP instance deﬁned by
g(1) = 0
h(1) = 1
g(2) = 1
h(2) = 011
g(3) = 011
h(2) = 0.
has a solution, but the shortest nonempty x with g(x) = h(x) has length 75. (One
such solution is x = 3113323111233312311231233123131123123311331
23122332231222211221132332122122.)
We now prove
Theorem 6.5.2. PCP is unsolvable.
Proof Idea.
The basic idea is simple; we reduce from the halting problem.
Given a TM M and an input w, we structure our morphisms so that there is a
solution x to f (x) = g(x) iff there is an accepting computation for M on w.
To do this, we force f (u) to “lagbehind”g(u), and it can “catchup”only if
the computation halts.
The details, however, are somewhat messy. First, we deﬁne a variant of PCP
called MPCP (the modiﬁed Post correspondence problem). In this variant, we
look for a solution to f (x) = g(x), but demand that x start with a given ﬁxed
letter. Then we show that the halting problem reduces to MPCP, and MPCP
reduces to PCP.
Proof. More precisely, MPCP is deﬁned as follows: we are given morphisms
g, h : ∗→∗, where  = {0, 1, . . . , k}, and we want to know if there is a
word w such that g(0w) = h(0w).
Let us show that MPCP reduces to PCP. To make the notation a little
less cumbersome, we introduce the following notation for morphisms: if the
domain alphabet  has an obvious ordering, such as  = {1, 2, . . . , k}, we
write g = (w1, w2, . . . , wk) to denote that f (i) = wi for 1 ≤i ≤k.
Suppose g = (w1, w2, . . . , wk) and h = (x1, x2, . . . , xk) be an instance of
MPCP. We convert this to an instance of PCP as follows: we introduce two
new symbols ♯and ♭not in the alphabet of the wi and xi, and we let
g′ = (y0, y1, . . . , yk+1) and h′ = (z0, z1, . . . , zk+1), where y0 := ♯y1, z0 := z1,

188
6
Turing machines
yk+1 := ♭, zk+1 := ♯♭, and
yi := wiX
|wi|
  
♯· · · ♯,
zi :=
|xi|
  
♯· · · ♯Xxi
for
1 ≤i ≤k.
For
example,
if
g = (10111, 1, 10)
and
h =
(10, 111, 0),
then
g′ = (♯1♯0♯1♯1♯1♯, 1♯0♯1♯1♯1♯, ♯1♯, 1♯0♯, ♭)
and
h′ = (♯1♯0, ♯1♯0, ♯1♯1♯1, ♯0, ♯♭).
We now argue that MPCP is solvable for g and h if and only if PCP is
solvable for g′ and h′. Suppose 1i2i3 · · · ir is a solution to MPCP for g and
h. Then it is easy to see that 0i2 · · · ir(k + 1) is a solution to PCP for g′ and
h′. On the other hand, if i1, i2, . . . , ir is a solution to PCP for g′ and h′, then
we must have i1 = 0, for otherwise the images of the solution under g′ and
h′ would begin with two different letters. Similarly, we must have ir = k + 1,
for otherwise the images of the solution under g′ and h′ would end with two
different letters. Now let j be the smallest integer such that ij = k + 1. Then
i1i2 · · · ij is a solution to PCP for g′ and h′, and 1i2 · · · ij−1 is then a solution
to MPCP for g and h. We have now shown that MPCP reduces to PCP.
It remains to show that the halting problem reduces to MPCP. Suppose we
are given a TM M = (Q, , , δ, q0, h) and an input w ∈∗. We construct
an instance of MPCP as follows. To simplify the construction, we give the
corresponding values of the morphisms g and h in the following table, without
specifying the input alphabet:
Group
Image of g
Image of h
Condition
Group 1
♯
♯q0Bw♯
Group 2
♯
♯
X
X
for X ∈
Group 3a
qX
Yp
if δ(q, X) = (p, Y, R)
ZqX
pZy
if δ(q, X) = (p, Y, L)
qX
pY
if δ(q, X) = (p, Y, S)
Group 3b
q♯
Yp♯
if δ(q, B) = (p, Y, R)
Zq♯
pZY♯
if δ(q, B) = (p, Y, L)
q♯
pY♯
if δ(q, B) = (p, Y, S)
for q ∈Q −{h}, p ∈Q, X, Y, Z ∈
Group 4
XhY
h
Xh
h
hY
h
for X, Y ∈
Group 5
h♯♯
♯

6.5 The Post correspondence problem
189
We now claim that M halts on w if and only if MPCP has a solution for the
two morphisms deﬁned this table. (Note that the image of 0 is given by the ﬁrst
line of the table.)
To see this, suppose M halts on w. Then there is a sequence of conﬁgura-
tions C1 = q0Bw, C2, . . . , Ct = αhβ representing an accepting computation.
We leave it to the reader to see that we can then choose a sequence of pairs
matching this computation, resulting in g(x) = h(x) = ♯C1♯C2♯· · · ♯Ct♯z for
some string z.
Now suppose MPCP has a solution x. It is not hard to see that such a solution
corresponds to a computation of M. Consider forming g(x) and h(x) symbol
by symbol. Then since g(x) must begin ♯and h(x) begin ♯q0Bw♯, and because
the pairs in Groups 2 and 3 do not increase the length of the image of h, there
is no way the length of the image of g can “catchup”unless pairs in Groups 4
and 5 are used. But we cannot use these symbols unless the halting state h is
reached, so M must halt.
Now let us see an application of the PCP.
Theorem 6.5.3. The following problem is unsolvable: given an arbitrary CFG
G, decide if it is ambiguous.
Proof. We show that if we could decide ambiguity for CFGs, we could solve
the PCP.
Let  = {1, 2, . . . , k}, and let g, h : ∗→∗be the morphisms from an
instance of PCP. Without loss of generality, we may assume that  and  are
disjoint. We now construct a grammar G with the productions
S →S1 | S2
S1 →g(i) i,
1 ≤i ≤k
S1 →g(i) S1 i,
1 ≤i ≤k
S2 →h(i) i,
1 ≤i ≤k
S2 →h(i) S2 i,
1 ≤i ≤k.
We claim that G is ambiguous iff the PCP instance has a solution.
Suppose G is ambiguous. Now it is easy to see that the subgrammars deﬁned
by taking either S1 or S2 to be the start symbol are each unambiguous, so any
ambiguity results from a word generated by S1 and S2. If a word w is derived
from both, then we have
w = g(i1)g(i2) · · · g(ir)ir · · · i2i1 = h(j1)h(j2) · · · g(js)js · · · j2j1

190
6
Turing machines
for some nonempty words i1i2 · · · ir, j1j2 · · · js ∈∗. But then, since  and
 are disjoint, we must have r = s and i1 = j1, i2 = j2, . . . , ir = jr. Then
g(i1 · · · ir) = h(i1 · · · ir) and so we have a solution to PCP.
Similarly, if we have a solution i1i2 · · · ir to PCP, then the word
g(i1)g(i2) · · · g(ir)irir−1 · · · i2i1 has two distinct leftmost derivations in G, one
starting S =⇒S1 and the other starting S =⇒S2.
6.6 Unsolvability and context-free languages
In this section we discuss some unsolvability results dealing with context-
free languages and grammars. First we discuss two languages: valid(M), the
language of valid computations of a TM, and invalid(M), the language of
invalid computations of a TM.
A valid computation of a TM M = (Q, , , δ, q0, h) is deﬁned to be a
string of the form w1#wR
2 #w3#wR
4 # · · · #wR
2k# or w1#wR
2 #w3#wR
4 # · · · #w2k−1#
for some integer k ≥1, n = 2k, or 2k −1 as appropriate, where # is a symbol
not in  or Q such that
1. Each wi is a valid conﬁguration of M.
2. w1 is a valid initial conﬁguration of M; that is, it is of the form q0Bx for
x ∈∗.
3. wn is a valid accepting or ﬁnal conﬁguration of M, that is, of the form yhz,
where h is the halting state.
4. wi ⊢wi+1 for 1 ≤i < n.
The language of all valid computations of M is denoted valid(M). The lan-
guage of invalid computations of M, denoted invalid(M), is deﬁned to be
( ∪Q ∪{#})∗—v alid(M).
Theorem 6.6.1. There exists an algorithm that, given a TM M as input, pro-
duces two CFGs G1 and G2 such that valid(M) = L(G1) ∩L(G2).
Proof.
It is actually somewhat easier to sketch the construction of two
pushdown automatons (PDAs) M1 and M2 that accept L1 := L(G1) and
L2 := L(G2), respectively. By Theorem 1.5.7 we know that we can effectively
produce CFGs from these PDAs.
Both L1 and L2 consist of strings of the form x1#x2#x3 · · · #xm#. We use M1
to enforce the condition xi ⊢xR
i+1 for i odd and M2 to enforce the condition
xR
i ⊢xi+1 for i even. The machine M2 also checks to see that x1 is a valid
initial conﬁguration of M. The condition that xm (respectively xR
m) is a valid
ﬁnal conﬁguration is checked by M1 or M2, respectively, according to whether
m is odd or even.

6.6 Unsolvability and context-free languages
191
Deﬁne L3 = {y#zR : y ⊢z}. We sketch how L3 may be accepted by a
PDA M3. The PDA reads the input up to # and ensures that y is really a valid
conﬁguration (i.e., of the form ∗Q∗). As it does so, it computes zR on the
ﬂy and pushes it onto its stack.
In more detail, M3 reads each symbol of y and pushes it onto its stack until
it reads a state p. Then M3 stores p in its ﬁnite control and reads the next
symbol X. It now looks up the corresponding transition in its ﬁnite control. If
δ(p, X) = (q, Y, R), then M3 pushes qY onto its stack. If δ(p, X) = (q, Y, S),
then M3 pushes Yq onto its stack. Finally, if δ(p, X) = (q, Y, L), then M3
reads the symbol on top of the stack (call it Z). Then M3 pops the Z and pushes
YZq in its place. (We have described the usual case, but some special cases
are needed if # appears in this process.) Finally, M3 reads the rest of the input
symbols up to the # and pushes them onto its stack. Now M3 pops its stack and
compares it to the rest of the input, symbol by symbol. It accepts if they agree;
otherwise it rejects.
In a similar way we can construct a PDA for L4 := {yR#z : y ⊢z}. Now
deﬁne
L1 = (L3#)∗({ϵ} ∪∗h∗#);
L2 = q0B∗#(L4#)∗({ϵ} ∪∗h∗#).
Given PDAs M3 accepting L3 and M4 accepting L4 we can easily construct
PDAs M5 accepting L1 and M6 accepting L2. Finally, we claim that valid(M) =
L1 ∩L2.
Now we can use Theorem 6.6.1 to prove an unsolvability result about gram-
mars.
Theorem 6.6.2. The following problem is unsolvable: given two CFGs G1 and
G2, decide if L(G1) ∩L(G2) = ∅.
Proof.
Suppose there were an algorithm to solve the given problem. Then
we could solve the following problem (the emptiness problem for TMs): given
a TM M, decide if L(M) = ∅. To see this, we reduce the emptiness problem
for TMs to our grammar problem. Given a TM M, we use Theorem 6.6.1
to create G1 and G2 such that valid(M) = L(G1) ∩L(G2). Now we use our
hypothesized algorithm to decide if L(G1) ∩L(G2) = ∅. Note that this occurs
iff valid(M) = ∅, which occurs if and only if L(M) = ∅. Hence we could
decide if L(M) = ∅, which is known to be unsolvable.
Even more results can be obtained using invalid(M).
Theorem 6.6.3. There exists an algorithm that, given a TM M, produces a
CFG G such that invalid(M) = L(G).

192
6
Turing machines
Proof.
The proof is generally along the lines of the proof of Theorem 6.6.1,
but is somewhat simpler because we do not need to ensure that wi ⊢wi+1 for
all i; instead it sufﬁces to ﬁnd a single i for which wi ⊬wi+1.
We claim that if w represents an invalid computation, then at least one of
the following conditions holds:
1. w is not of the form x1#x2# · · · xm#, where each xi is a conﬁguration;
2. x1 ̸∈q0B∗;
3. xm ̸∈∗h∗;
4. xi ⊬xR
i+1 for some odd i;
5. xR
i ⊬xi+1 for some even i.
It is easy to check conditions (1)–(3)with a ﬁnite automaton, while conditions
(4) and (5) can be checked with a single PDA using the same kind of argument
we used in proving Theorem 6.6.1. Hence we can constructively create a CFG
G such that invalid(M) = L(G).
Using Theorem 6.6.3 we can prove a large number of unsolvability results
about context-free grammars. Here are several; others are given in the exer-
cises.
Corollary 6.6.4. The following problem is unsolvable: given a CFG G =
(V, , P, S), decide if L(G) = ∗.
Proof.
We reduce from the emptiness problem for TMs. Given a TM M, we
can effectively construct a CFG G such that invalid(M) = L(G). Now note that
invalid(M) = ∗if and only if L(M) = ∅, where  =  ∪Q ∪{#}.
Corollary 6.6.5. The following problem is unsolvable: given two CFGs G1
and G2, decide if L(G1) = L(G2).
Proof. We reduce from the problem, given a CFG G1, is L(G1) = ∗. We can
easily construct a grammar G2 such that L(G2) = ∗. Hence, L(G1) = L(G2)
if and only if L(G1) = ∗, and this reduction gives the desired result.
Theorem 6.6.6. The following problem is unsolvable: given a CFG G, decide
if L(G) is regular.
Proof. Let G = (V, , P, S) and let L = L(G). Suppose || is large enough
that the problem of deciding if L(G) = ∗is unsolvable. Let L0 ⊆∗be
a nonregular, context-free language generated by a CFG G0. Consider the
language L1 = L0#∗∪∗#L. Since the class of context-free languages is
effectively closed under concatenation and union, L1 is a context-free language,

6.7 Complexity and regular languages
193
and a CFG G1 such that L1 = L(G1) can be effectively computed from G and
G0. We claim L1 is regular if and only if L = ∗.
Suppose L = ∗. Then L1 = ∗#∗is a regular language. Now suppose
L ̸= ∗. Then there exists w ̸∈L, and we have L1/#w = L0. Since the class of
regular languages is closed under quotient, if L1 is regular, then L0 is regular.
By assumption, L0 is not regular, so L1 must not be regular as well. Thus, L1
is regular if and only if L = ∗. Since the problem of deciding if L = ∗is
unsolvable, the result follows.
The next result shows that even if a “birdie”tells you that a given CFG
generates a regular language, there is no algorithm to compute an equivalent
DFA.
Theorem 6.6.7. There exists no algorithm that, given a CFG G such that L(G)
is regular, produces a DFA A such that L(A) = L(G).
Proof. Suppose to the contrary that such an algorithm exists. Let M = (Q, ,
, δ, q0, h) be a TM such that L(M) is not recursive. By Theorem 6.6.3, there
exists an algorithm to compute a CFG G′ such that L(G′) = invalid(M). Let
 =  ∪Q ∪Q ∪{#}. For x ∈∗, deﬁne
Fx = {q0y#z : y ̸= x and z ∈∗}.
The language Fx is regular, and a DFA B accepting Fx can easily be
computed. Given the DFA B and the grammar G′, a CFG Gx such that
L(Gx) = invalid(M) ∪Fx can be effectively computed.
If x ∈L(M), then L(Gx) = ∗−{w}, where w is the accepting computa-
tion of M on x. If x ̸∈L(M), then L(Gx) = ∗. In either case, L(Gx) is regular.
By our initial assumption, we can compute a DFA A such that L(A) = L(Gx).
Given such a DFA A, we can decide whether or not L(Gx) = ∗, and hence
decide whether or not x ∈L(M). This is a contradiction, since L(M) is not
recursive.
6.7 Complexity and regular languages
In this section we prove several fundamental results about the computational
complexity of problems dealing with regular languages.
We start with the universality problem for regular expressions and non-
deterministic ﬁnite automaton (NFAs). Given a regular expression r over a
base alphabet  (respectively, an NFA M with input alphabet ), we would
like to know if L(r) ̸= ∗(respectively, if L(M) ̸= ∗). Perhaps surprisingly,
both of these problems are PSPACE-complete.

194
6
Turing machines
We start with regular expressions. The fundamental result is the following
lemma.
Lemma 6.7.1. Let T = (Q, , , δ, q0, h) be a one-tape deterministic TM and
p(n) be a polynomial such that T never uses more than p(|x|) cells on input x.
Let # be a new symbol not in , and let  =  ∪Q. Then there is a polynomial
q(n) such that we can construct a regular expression rx in ≤q(n) steps, such
that L(rx) ̸= ( ∪{#})∗if and only if T accepts x.
Proof Idea. Although the details are somewhat messy, the basic idea is simple.
We encode a computation of the TM T as a string of successive conﬁgurations,
separated by the delimiter #. Then we construct the regular expression rx so that
it speciﬁes all strings that do not represent accepting conﬁgurations. A string
might fail to represent an accepting conﬁguration because it is in the wrong
format, or because the initial conﬁguration is wrong, or because T never enters
the halting state h, or because in two consecutive conﬁgurations, the second
does not follow from the ﬁrst by a valid move of T . All these possibilities can
be speciﬁed by rx.
Proof. We represent a computation of T as a string
#x1#x2# · · · #xk#,
where each xi is a conﬁguration, that is, a string in ∗Q∗. We will assume
that T always has a next move, which can be accomplished by creating a new
“deadstate”to enter if there is no move, and we will also assume that T simply
stays in the halting state without moving its tape head once the halting state is
reached.
We will further assume that the length of each conﬁguration between #
signs is p(n) + 1, which we can achieve by padding on the right with the
blank symbol B, if necessary. The “+1”term comes from the fact that each
conﬁguration contains a state symbol.
Now we construct rx to specify strings that are not valid accepting compu-
tations. A string y fails to represent an accepting computation if and only if at
least one of the following conditions is met:
A: y is not of the form #x1#x2# · · · #xk# for some k ≥1, where each xi is of
length p(n) + 1 and all but one symbol of xi is in , with the exception
being in Q.
B: y begins with something other than #q0Ba1a2 · · · anB · · · B#, where x =
a1a2 · · · an and the number of blanks is p(n) −n.

6.7 Complexity and regular languages
195
C: The halting state h never appears in a conﬁguration.
D: y has a subword of the form #xi#xi+1#, where xi+1 does not follow from
xi in one step of T .
Now we can construct regular expressions A, B, C, and D for each of the
conditions.
For A, we need to specify subexpressions
A1: strings containing less than two instances of #: ∗+ ∗#∗;
A2: strings not beginning or ending with #: ( + #)∗+ ( + #)∗;
A3: strings with no q ∈Q appearing between two consecutive occurrences of
#: ( + #)∗#∗#( + #)∗;
A4: strings with two or more occurrences of q ∈Q appearing between two
consecutive occurrences of #: ( + #)∗#∗Q∗Q∗#( + #)∗;
A5: strings with more than p(n) + 1 symbols of  appearing between two
consecutive occurrences of #: ( + #)∗#p(n)+2∗#( + #)∗;
A6: strings with fewer than p(n) + 1 symbols of  appearing between
two consecutive occurrences of #: ( + #)∗#(ϵ +  + 2 + · · · +
p(n))#( + #)∗.
Now A = A1 + A2 + A3 + A4 + A5 + A6.
We construct B as the union B = B0 + B1 + B2 + · · · + Bp(n)+1, where
each Bi represents a conﬁguration that differs from the correct initial conﬁgu-
ration in the ith location. So, letting a0 = B, we deﬁne
B0 = #( −{q0})p(n)#( + #)∗;
Bi = #i( −{ai−1})p(n)−i#( + #)∗,
1 ≤i ≤n + 1;
Bj = #j( −{B})p(n)−j#( + #)∗,
n + 2 ≤j ≤p(n).
We construct C as ( −{h} + #)∗.
Finally, to construct D, we observe that given any four consecutive symbols
c1c2c3c4 of a conﬁguration, we can determine what symbol will replace c2 in
the next conﬁguration, that is, the symbol p(n) + 2 symbols to the right of c2.
Thus, assuming the possible moves are
δ(p, X) =



(q, Y, L)
(q, Y, R)
(q, Y, S)

196
6
Turing machines
we can deﬁne a function f (c1c2c3c4) as follows (where U, V, W, X, Y, Z are
in , and p, q ∈Q):
c1c2c3c4
f (c1c2c3c4)
UV WZ
V
Y if left move
pXV W
q if right move
Y if stationary move
V if left move
VpXW
Y if right move
q if stationary move
q if left move
V WpX
W if right move
W if stationary move
UV Wp
V
There are also some additional cases involving the delimiter #, which we
leave to the reader.
Now, given f , we can deﬁne a regular expression for D as follows:
D =

c1,c2,c3,c4
( + #)∗c1c2c3c4( + #)p(n)−1( ∪{#} −{f (c1c2c3c4)})( + #)∗.
Now it is easy to see that the length of rx is a polynomial that depends only
on x and T . Furthermore, rx denotes ( ∪{#})∗if and only if T does not ac-
cept x.
Now we want to create a language Lregex that encodes the problem “Is
L(rx) ̸= ∗,”but there is a slight technical problem to overcome. The problem
is that the regular expressions are over an arbitrary alphabet, while Lregex must
be over a ﬁxed alphabet. To solve this problem, we simply encode regular
expressions so that the ﬁrst alphabetic symbol is represented by [1], the second
by [10], the third by [11], and so on. Thus, we deﬁne Lregex to be the set of all
encodings of regular expressions r, over the alphabet
[, ], (, ), ∅, ϵ, +, ∗, 0, 1,
such that L(r) ̸= ( ∪{#})∗.
Theorem 6.7.2. Lregex is PSPACE-complete.
Proof. First, let us prove that Lregex is in PSPACE. By Theorem 1.8.4 (Savitch’s
theorem), it sufﬁces to give a nondeterministic polynomial-space-bounded TM
that accepts Lregex.

6.7 Complexity and regular languages
197
Given an encoded regular expression re as input, we decode it to determine
r. Now we convert r to an NFA-ϵ M with n = O(|re|) states using the usual
technique. We now guess the string x that is not in L(M) symbol by symbol,
and simulate M on x. If we ﬁnd that M fails to accept x, we accept the input
re. If our guess fails, and we have guessed at least 2n symbols, we reject. (We
can count up to 2n using O(n) space.)
Now we prove that if L ∈PSPACE, then L ≤Lregex. Let M be a deter-
ministic TM accepting L. Then by Lemma 6.7.1 we can construct a regular
expression rx of polynomial size in x such that L(rx) ̸= ( ∪{#})∗if and only
if x ∈L. Now convert rx to its encoding.
Corollary 6.7.3. The following problem is PSPACE-complete: given an NFA
M = (Q, , δ, q0, F), decide if L(M) ̸= ∗.
Our proof that the universality problem for regular expressions and NFAs is
PSPACE-complete no longer works for a unary alphabet. Over a unary alphabet,
we have the following.
Theorem 6.7.4. Let  = {a}. Consider the following problem: given a unary
regular expression r, decide if L(r) ̸= ∗. Then this problem is NP-complete.
Proof. First, let us see that the problem is in NP. Given a regular expression of
size k, we can easily convert it to an equivalent NFA M with n = O(k) states
using the standard algorithm. Now we guess a length m such that am ̸∈L(M)
and then verify our guess using the Boolean matrix technique of Theorem 3.8.4.
If there is indeed such a word not in L(M), then one exists of length m < 2n.
(To see this, convert M to a DFA with at most 2n states. If a string labels a path
to a nonﬁnal state, there must be such a path of length <2n.)
To show the problem is NP-hard, we reduce from 3-SAT. Suppose we have
an instance of 3-SAT, say a formula ϕ = C1 ∧C2 ∧· · · ∧Cn, where each Ci
is a clause, using variables x1, x2, . . . , xk and their negations. Let pi denote the
ith prime. We now construct an integer yi based on each clause Ci, which is
most easily deﬁned in terms of an example. If the clause C1 is (x3 ∨x5 ∨x6),
then we let y1 be the unique integer, 0 ≤y1 < p3p5p6 = 5 · 11 · 13 = 715 such
that
y1 ≡0 (mod p3)
y1 ≡1 (mod p5)
y1 ≡0 (mod p6).
So in this case, y1 = 650. Note that the right side of a congruence is 1 if the
corresponding variable is negated in the clause; 0 otherwise. Now we make the

198
6
Turing machines
regular expression E = E0 + · · · + En, where
E0 =

1≤i≤k

2≤j<pi
aj(api)∗
and, in the example we chose earlier,
E1 = ay1(ap3p5p6)∗
and the other Ei are deﬁned similarly, based on the variables that occur in their
clauses.
I now claim L(E) ̸= a∗if and only if ϕ is satisﬁable. For the way E is con-
structed, it omits a string if and only if it corresponds to a satisfying assignment.
E0, for example, speciﬁes all strings that correspond to invalid assignments,
where we assign a value of 2 or more to a variable.
6.8 Exercises
1. Use a more efﬁcient encoding than E in Example 6.3.1 to obtain the result
π(x) ≥c
x
(log x)(log log x)2 inﬁnitely often.
2. Use Kolmogorov complexity and the incompressibility method to prove that
the language
L = {xx : x ∈{0, 1}∗}
is not regular.
3. A common creationist claim is that gene duplication cannot generate infor-
mation. Prove the creationists wrong by showing that there exist inﬁnitely
many strings x such that C(xx) > C(x).
4. Let H be the entropy function deﬁned by H(α, β) = −α log2 α −β log2 β.
Suppose a binary string x of length n is chosen by ﬂipping a coin with a
bias of α, 0 ≤α ≤1
2, where β = 1 −α is the probability of tails. Show that
with very high probability, such a string satisﬁes C(x) ≍H(α, β)n.
5. Show that the following problems are recursively unsolvable:
(a) Given a CFG G and a regular expression r, is L(G) = L(r)?
(b) Given a CFG G and a regular expression r, is L(r) ⊆L(G)?
(c) Given CFGs G1 and G2, is L(G1) ⊆L(G2)?
6. Prove that the following decision problem is unsolvable. Given a grammar
G over an alphabet , is ∗−L(G) ﬁnite?
7. Show that the following decision problem is solvable: given an arbitrary
CFG G and an arbitrary regular expression r as input, decide whether or not
L(G) ⊆L(r).
8. Show the following problem is recursively unsolvable: given a CFG G, is
L(G) a linear language?

6.8 Exercises
199
9. Suppose the deﬁnition of unrestricted grammar is relaxed to allow pro-
ductions of the form ϵ →α. In other words, at any time during a deriva-
tion the string α can be inserted between any two symbols. Show that
this new model of unrestricted grammar also generates the class of re-
cursively enumerable languages.
10. Give an unrestricted grammar for {ww : w ∈{0, 1}∗}.
11. Give an unrestricted grammar for the language
{ai : i ≥4 is not a prime}.
Give a sketch of the proof that your grammar is correct.
12. Show that the language of Kolmogorov-incompressible strings is not
recursive.
13. From Theorem 6.2.4 we know that, for all n ≥0, there is at least one
string x of length n with C(x) ≥n. Prove that in fact there are 2n−c such
strings, for some constant c.
14. Let L1, L2 be context-free languages. Show that the following problem
is unsolvable: decide whether or not L1/L2 is context-free.
15. Prove that there exists a constant c such that there are inﬁnitely many
strings x and y, where x is a subword of y, and C(x) > 2C(y)−c.
16. Prove that the following problem is PSPACE-complete: given an NFA
M = (Q, , δ, q0, F), decide if ∗−L(M) is ﬁnite.
17. Prove that the following problem is unsolvable: given a string x, deter-
mine if C(x) < |x|.
∗∗18. Recall the deﬁnition of pattern matching a text given in Exercise 3.70:
p ∈∗matches t ∈∗if there is a nonerasing morphism h : ∗→∗
such that h(p) = t.
(a) Show that if the alphabets  and  are of ﬁxed size, then we can
decide if p matches t in time polynomial in |p| and |t|.
(b) Show that the problem of deciding if p matches t for arbitrary
alphabets is NP-complete.
(c) Suppose we are given a pattern p and an NFA M, and we want to
decide if p matches t for some t ∈L(M). Show that this problem is
PSPACE-complete.
(d) Suppose we are given a pattern p and a CFG G, and we want to
decide if p matches t for some t ∈L(G). Show that this problem is
unsolvable even if p is just the pattern xx.
19. Which of the following PCP instances have solutions?
(a) (11, 01, 011), (1,110, 0);
(b) (11, 01, 101), (0, 011, 1);
(c) (0, 01, 1), (1, 0, 101).

200
6
Turing machines
20. Show that the PCP is solvable if the images of the two morphisms are over
a unary alphabet.
21. Show that the following problem is PSPACE-complete: given n DFAs
M1, M2, . . . , Mn, decide if there exists a string accepted by all of them.
Hint: Use the automata to check that a string represents a valid computation
of a polynomial-space-bounded TM.
22. Show that the following problem is PSPACE-complete: given an NFA M,
decide if it accepts some string unambiguously, that is, if there exists some
accepted string for which there is only one acceptance path in the NFA.
6.9 Projects
1. Read papers about constructing “bad”examples of the PCP. Start with
Lorentz [2001] and Zhao [2003].
6.10 Research problems
1. Try ﬁnding busy beaver TMs for some variations on the TM model given
here: for larger alphabets, for one-directional tapes, and so on.
6.11 Notes on Chapter 6
6.1 Unrestricted grammars are sometimes called Type 0 grammars. The
grammar in Example 6.1.1 is due to J. Rideout.
Our proof of Theorem 6.1.3 is based on a suggestion of A. F. Nevrau-
mont.
6.2 An excellent introduction to Kolmogorov complexity can be found in Li
and Vit´anyi [1997]. Although the concept is attributed to Kolmogorov,
similar ideas were proposed about the same time by R. Solomonoff and
G. J. Chaitin.
6.3 The proof in Example 6.3.1 that π(x) ≥cx/(log x)2 inﬁnitely often is
sketched in Li and Vit´anyi [1997, pp. 4–5],where it is attributed to J.
Tromp, improving a result of P. Berman, which in turn was based on a
proof of G. Chaitin [1979].
The material on proofs of nonregularity is based on Li and Vit´anyi
[1995].
6.4 The busy beaver problem was introduced by Rado [1962]. Lin and Rado
[1965] gave the busy beavers with one, two, and three states, and Brady
[1983] solved the case of four states. Marxen and Buntrock [1990] gave
a candidate for the busy beaver with ﬁve states. The best example known

6.11 Notes on Chapter 6
201
on six states is due to Ligocki and Ligocki in December 2007 and can be
found at http://www.drb.insel.de/~heiner/BB/simLig62 b.
html. Heniner Marxen’s Web page, http://www.drb.insel.de/
~heiner/BB/, is the best online resource for the problem.
6.5 The PCP is due to Post [1946].
For a Web page giving record examples of the PCP, see http://www.
theory.informatik.uni-kassel.de/~stamer/pcp/pcpcontest
en.html.
Theorem 6.5.3 is due to Cantor [1962], Floyd [1962], and Chomsky
and Sch¨utzenberger [1963].
6.6 The results in this section are from Bar-Hillel, Perles, and Shamir [1961]
and Ginsburg and Rose [1963].
6.7 Our presentation of Theorem 6.7.2 is based strongly on that in Aho,
Hopcroft, and Ullman [1974, §10.6].

7
Other language classes
In this chapter we discuss some less familiar language classes, such as the
context-sensitive languages (CLs) and the 2DPDA languages.
7.1 Context-sensitive languages
In this section we introduce a variant on the context-free grammar, known as
the context-sensitive grammar or CSG.
A grammar G = (V, , P, S) is said to be context-sensitive if every pro-
duction in P is of the form αBγ →αβγ for some α, γ ∈(V
∪)∗,
β ∈(V
∪)+, and B ∈V . A language L is said to be context-sensitive
(or a CSL) if L −{ϵ} is generated by some CSG. (The funny condition in-
volving ϵ arises because a CSG cannot have ϵ-productions, and hence, cannot
generate ϵ.)
The name context-sensitive comes from the fact that we can consider the
allowed productions to be of the form B →β, but they can be applied only in
the “context”α— γ .
Example 7.1.1. Consider the following CFG G1:
S →ABSc
S →Abc
BA →CA
CA →CB
CB →AB
Bb →bb
A →a.
202

7.1 Context-sensitive languages
203
We claim that L(G1) = {anbncn : n ≥1}. Here is an informal argument that
this is the case. Note that BA
∗
=⇒AB by the series of context-sensitive pro-
ductions BA =⇒CA =⇒CB =⇒AB. We argue that anbncn ∈L(G1) for
all n ≥1. We can use the following derivation:
S =⇒ABSc =⇒ABABScc
∗
=⇒(AB)n−1Scn−1
=⇒(AB)n−1Abcn = A(BA)n−1bcn
∗
=⇒A(AB)n−2ABbcn
=⇒A(AB)n−2Abbcn = AA(BA)n−2bbcn
∗
=⇒AA(AB)n−2bbcn = AA(AB)n−3ABbbcn
=⇒AA(AB)n−3Abbbcn = AAA(BA)n−3bbbcn
∗
=⇒Anbncn
∗
=⇒anbncn.
It remains to see that L(G1) ⊆{anbncn : n ≥1}. The basic idea is that
any derivation of a terminal string must proceed roughly along the lines given
earlier. We leave the argument as an exercise.
There is an alternative characterization of CSLs in terms of grammars. We
say a grammar G = (V, , P, S) is length-increasing if every production in
P is of the form α →β, with α, β ∈(V ∪)+ and |α| ≤|β|. (As before, a
length-increasing grammar cannot generate the empty string.)
Example 7.1.2. Consider the following length-increasing grammar G2:
S →aBSc
S →abc
Ba →aB
Bb →bb.
We claim that L(G2) = {anbncn : n ≥1}.
Theorem 7.1.3. A language L is generated by a length-increasing grammar if
and only if L is context-sensitive.
Proof.
Clearly, if L is generated by a CSG, then it is generated by a length-
increasing grammar, because every CSG is actually length increasing.
To prove the other direction, we show how to take a length-increasing
grammar and transform it by a series of steps into a CSG, without changing the
language generated.
First, we do a transformation that changes all the occurrences of a terminal
in a production to a variable. Namely, we replace every occurrence of a in a

204
7
Other language classes
production (on both sides) with the new variable Ca, for each a ∈. Then we
add productions (*) Ca →a for each a ∈. Clearly, this does not change the
language generated. Furthermore, these new productions are already context-
sensitive.
Now the right-hand sides of all productions (except those labeled (*)) consist
of strings of variables only. The rest of the transformation will be by example.
Take a production of the form CDE →JKLMN. Delete it and add the
following productions:
CDE →A1DE
A1DE →A1A2E
A1A2E →A1A2A3
A1A2A3 →JA2A3
JA2A3 →JKA3
JKA3 →JKLMN.
The effect of these productions is to replace CDE with JKLMN, via a series
of productions that are of the desired form. Note that all but the ﬁrst production
have a distinguished variable Ai that does not appear in any other productions,
so the productions must be used in the order given, and cannot be used in any
other situation. (Different distinguished variables are used in other productions,
of course).
Here is a formal proof that the preceding construction works.
Deleting the terminals a that appear in productions and replacing them by
Ca and then adding the productions Ca →a clearly does not change the
language, so we will concentrate on the second part of the construction, for
which an example was already given, namely, how to replace the production
CDE →JKLMN with a “chain”of productions of the desired form. In what
follows, we will use that example and refer to the variables such as A1, A2, A3
as auxiliary variables.
Let G be the original CSG and let G′ denote the grammar modiﬁed as
described earlier.
Claim: α
∗
=⇒β in G if and only if α
∗
=⇒β in G′ and α, β contain no
auxiliary variables.
Proof: Suppose α
∗
=⇒β in G. We prove by induction on the length of the
derivation that α
∗
=⇒β in G′. Clearly, this is true for derivations of length 0.
Now suppose α
∗
=⇒β in G, and consider the last production used. Without
loss of generality, assume it was CDE →JKLMN. Then
α = α1CDEα2 =⇒i−1 α1JKLMNα2 = β.

7.1 Context-sensitive languages
205
By induction, α =⇒∗
α1CDEα2 in G′, and from the series of produc-
tions given before, we easily see CDE =⇒∗JKLMN in G′, so α =⇒∗
α1JKLMNα2 = β in G′.
The other direction is a little harder. Suppose α
∗
=⇒β in G′ and α, β contain
no auxiliary variables. Then if this derivation is of length 0, it clearly also
occurs in G. For the induction step, assume that α =⇒i β in G′. Without loss
of generality, assume that CDE →A1DE is the ﬁrst production used in the
derivation in G′. Then all the other productions listed in the chain (introducing
the other auxiliary variables A2, A3, . . .) must eventually be used, and in the
order listed; otherwise the auxiliary variables do not disappear. For example,
once A1 is introduced the production A1DE →A1A2E must eventually be
used, as there is no other production involving A1 that does not have an A2 to
its right, and so on. This is not to say that other productions could not be used in
between productions of the chain for CDE; in fact, this may well happen. Then
I claim that we can assume that all the productions listed in the chain for CDE
are actually used one after the other in the order listed, with no intervening
productions. For assume some other productions “intrude,”namely
α1CDEβ1 =⇒α1A1DEβ1
∗
=⇒α2A1DEβ2 =⇒α2A1A2Eβ2.
Then it is clear that in fact α1
∗
=⇒α2 (since the productions involving A1 are
very speciﬁc) and DEβ1 =⇒∗DEβ2 (for the same reason). Hence we could
just as well have used the following derivation in G′:
α1CDEβ1
∗
=⇒α2CDEβ2 =⇒α2A1DEβ2 =⇒α2A1A2Eβ2.
Hence we have moved the offending productions to the “front”before any
productions in the chain for CDE are actually used.
This works ﬁne when new auxiliary variables are being introduced. How
about when they are being removed? Well, suppose
α1JA2A3β1 =⇒α1JKA3β1
∗
=⇒α2JKA3β2 =⇒α2JKLMNβ2.
Then again, we must have α1JK
∗
=⇒α2JK and β1
∗
=⇒β2. Then we could
have instead used the following derivation in G′:
α1JA2A3β1 =⇒α1JKA3β1 =⇒α1JKLMNβ1
∗
=⇒α2JKLMNβ2,
which pushes the offending production to the “back.”(In the middle of the
chain of productions introduced for CDE →JKLMN, we get to push the
offending production to the front or back.)

206
7
Other language classes
Now, by repeatedly applying this idea, we can bring together all of the
productions in the chain for CDE, so they appear consecutively, in the order
listed. By similar juggling, we can bring together all of the productions in the
chains for other productions.
This proves the claim. Now to ﬁnish up the proof, assume without loss
of generality that CDE →JKLMN is the ﬁrst production used. Hence,
α = α1CDEβ1 =⇒i β in G′. By the lemma, we can use all the productions
in the chain for CDE immediately, and hence
α1CDEβ1
∗
=⇒α1JKLMNβ1,
where α1, β1 contain no auxiliary variables. Hence,
α =⇒α1JKLMNβ1 =⇒j β
in G′, where j < i. Thus we can use induction to say that α1JKLMNβ1
∗
=⇒β
in G. But CDE →JKLMN is a production in G, so α
∗
=⇒β in G. This
ﬁnishes the proof.
We now turn to a machine model for the CSLs. This model, called the linear-
bounded automaton or LBA, is very much like a one-tape nondeterministic
Turing machine (TM), but with the following changes: the contents of the input
tape is initially ♯w♭, where w is the input and ♯and ♭are distinguished symbols,
called endmarkers, that can never be changed on the tape. Note that ♯and ♭are
tape symbols that are not contained in the input alphabet . Further, the tape
head can move left, right, or stay stationary, but can never move left from ♯
or right from ♭. The effect of these rules is that the space used by the LBA is
limited to the length of the input. An LBA accepts if, as in the case of TMs,
it enters a distinguished state called the halting state, and is usually denoted
by h.
Theorem 7.1.4. If L is context-sensitive, then it is accepted by an LBA.
Proof.
If L is context-sensitive, then L is generated by a length-increasing
grammar G. We now show how to accept L with an LBA M. First, by expanding
the alphabet size of M to 4-tuples, we can assume that M has four “tracks.”
Now we carry out the algorithm given in the proof of Theorem 6.1.2, with
the following difference: we use the tracks of M instead of different tapes,
and if at any time the length of the sentential form on track 2 exceeds the
length of the input—which is detected by attempting to move right on ♭—the
simulation crashes. This simulation accepts an input w if and only if there is
some derivation in G in which no intermediate step is longer than |w|. Since G
is length-increasing, w is accepted if and only if w ∈L(G).

7.1 Context-sensitive languages
207
Theorem 7.1.5. If L is accepted by an LBA, then L is context-sensitive.
Proof. The idea is similar to the proof of Theorem 6.1.3. There, we created a
grammar that would simulate the conﬁgurations of a TM—a nondeterministic
“language generator.” We apply the same general strategy here to make a
length-increasing grammar, but with some additional complications.
Suppose L is accepted by an LBA M. Then we can easily convert M to an
LBA M′ that has the following property: it ﬁrst copies its input w to a second
track. It then performs the computations of M on the ﬁrst track. If M ever
accepts, M′ writes the input w back on the tape (the two tracks disappear and
become just one), moves the tape head left to scan the left endmarker ♯, and
halts. This new LBA M′, then, has the property that if it halts on input w, the
string w is left on the tape at the end of the computation.
We now simulate the computation of M′ with a length-increasing grammar.
One additional problem is that at the end of a derivation we cannot simply
make the state and endmarkers disappear with ϵ-productions, since these would
violate the length-increasing rules. Instead, we incorporate these symbols into
adjacent symbols, making new single composite symbols such as [♯pX] and
[♯pX♭]. More precisely, we associate each permanent symbol (i.e., member of
) with all evanescent symbols (i.e., endmarker or state) to its left, including
any symbols to the right if we are at the right end of the tape. This simply
involves increasing the size of the tape alphabet.
The productions can be divided into several groups. To initialize the simu-
lated tape, we use the productions
S →[q0♯a♭] | [q0♯a]S1
S1 →aS1 | [a♭]
for all a ∈.
To simulate right moves of the LBA we use the productions
[p♯X♭] →[♯qX♭]
[p♯X] →[♯qX]
for states p, q, tape symbols X, and moves δ(p, ♯) = (q, ♯, R). We use produc-
tions
[pX]Z →Y[qZ]
[pX][Z♭] →Y[qZ♭]
[♯pX]Z →[♯Y][qZ]
[♯pX♭] →[♯Yq♭]
[♯pX][Z♭] →[♯Y][qZ♭]

208
7
Other language classes
for moves δ(p, X) = (q, Y, R) and tape symbols Z.
To simulate stationary moves of the LBA we use the productions
[p♯X♭] →[q♯X♭]
[p♯X] →[q♯X]
for moves δ(p, ♯) = (q, ♯, S), productions
[pX] →[qY]
[pX♭] →[qY♭]
[♯pX] →[♯qY]
[♯pX♭] →[♯qY♭]
for all moves δ(p, X) = (q, Y, S), and productions
[Xp♭] →[Xq♭]
for all moves δ(p, ♭) = (q, ♭, S).
To simulate left moves of the LBA we use the productions
Z[pX] →[qZ]Y
[♯Z][pX] →[♯qZ]Y
[♯Z][pX♭] →[♯qZ][Y♭]
Z[pX♭] →[qZ][Y♭]
[♯pX] →[q♯Y]
[♯pX♭] →[q♯Y♭]
for all productions δ(p, X) = (q, Y, L) and productions
[♯Zp♭] →[♯qZ♭]
[Zp♭] →[qZ♭]
for productions δ(p, ♭) = (q, ♭, L).
Finally, we add productions
[h♯X♭] →X
[h♯X]Y →X[Y♮]
[X♮]Y →X[Y♮]
[X♮][Y♭] →XY
[h♯X][Y♭] →XY

7.1 Context-sensitive languages
209
to simulate a left-to-right scan of the sentential form, converting all composite
symbols to single symbols. Here, ♮is a new symbol. This occurs only when the
halting state h is reached.
We now prove two theorems about relationship between the class of CSLs
and the class of recursive languages.
Theorem 7.1.6. Every CSL is recursive.
Proof.
Suppose L is a CSL. We give an algorithm to test membership in
L that always terminates. Since L is a CSL, it is generated by some length-
increasing grammar G = (V, , P, S). Hence, x ∈L if and only if S
∗
=⇒x.
The length-increasing property implies that any intermediate sentential form in
a derivation of x must be of length ≤|x|. We can enumerate them all, and for
each pair of sentential forms, (α, β), we can determine if α =⇒β. Now make a
ﬁnite directed graph whose vertices are these sentential forms and whose edges
are given by (α, β) when α =⇒β. There is a path from S to x if and only if
S
∗
=⇒x, if and only if x ∈L. Now just use breadth-ﬁrst search to determine if
such a path exists.
Theorem 7.1.7. There exists a recursive language that is not a CSL.
Proof. First we prove the following lemma: we say a TM T is always-halting
if on any input x ∈∗, the TM halts with either 1 or 0 written on its tape.
In this case we deﬁne L(T ) = {x : T writes 1 on input x}. By an effective
enumeration of a set S we mean a TM with a write-only output tape, where
the tape head moves only right and successively writes on its output tape an
encoding of the elements of S such that (i) no element is written twice and
(ii) every element eventually gets written.
Lemma 7.1.8. Let M1, M2, . . . be an effective enumeration of some set of
always-halting TMs. Then there exists a recursive language L such that L ̸=
L(Mi) for all i ≥1.
Proof.
Let us enumerate the elements of {0, 1}∗as follows: x0 = ϵ, x1 = 0,
x2 = 1, x3 = 00, and so on. Now deﬁne L = {xi : Mi writes 0 on input xi}.
Now L is recursive, because on input xi we can determine i, then simulate Mi
on xi, and write 1 if and only if Mi writes 0. But L ̸= L(Mi), since xi ∈L if
and only if xi ̸∈L(Mi).
We can now prove Theorem 7.1.7.

210
7
Other language classes
Proof.
It sufﬁces to show that we can effectively enumerate a set of always-
halting TM that accept the CSLs over {0, 1}. To do so, we simply encode each
CSG as a binary number, arrange them in ascending order, and create a TM
accepting each using the algorithm in the proof of Theorem 7.1.6.
Finally, we turn to the question of complementation of CSLs. The question
of whether the CSLs were closed under complement was open for many years,
until it was solved independently and nearly simultaneously by Immerman
and Szelepcs´enyi in 1988. The technique they used, called inductive counting,
is actually much more widely applicable. In this text, however, we restrict
ourselves to its application to the context-sensitive languages.
Theorem 7.1.9. If L is a CSL, then L is a CSL.
Proof Idea.
The basic idea is as follows: given an LBA M accepting L, we
construct an LBA M′ accepting L. On input x, M′ attempts to verify that no
accepting conﬁguration in M can be reached starting from the conﬁguration
q0♯x♭. To know that all conﬁgurations have been checked, M′ needs to count the
number of reachable conﬁgurations, a computation it carries out inductively.
Proof.
Let M = (Q, , , δ, q0, h) be an LBA accepting L. We can assume
without loss of generality that if M accepts x of length n by reaching the halting
state h, it then erases its input tape and moves the head to the left end. This
means that x is accepted by M if and only if
q0♯x♭
∗⊢h♯Bn♭.
Thus, we want to create M′ that accepts x if and only if the conﬁguration q0♯x♭
does not lead to h♯Bn♭in M.
Now if M accepts an input x, and |x| = n, then there is an accepting com-
putation for x that consists of at most
C := |Q|||n(n + 2)
moves, since this is the number of different possible conﬁgurations of M. (A
longer computation would imply that a conﬁguration is repeated, and hence
we could cut out a portion of the computation to get a shorter one.) Suppose we
knew that exactly R conﬁgurations of M were reachable from q0♯x♭. Then we
could accept L with an LBA that implements the following nondeterministic
algorithm:
Input: x and R.
Set reached := 0

7.1 Context-sensitive languages
211
For all conﬁgurations γ , |γ | = n + 3, do
(1) Guess a computation of M of s moves (s ≤C), beginning at q0♯x♭,
one move at a time.
If the computation ends at the conﬁguration γ then
reached := reached+1
(2) If γ = h♯Bn♭, then reject
(3) If reached = R then accept
(4) else reject
First, let us see why the algorithm works. If x ∈L, then some sequence of
guesses in line (1) will result in each of the R reachable conﬁgurations being
examined, and the variable reached is then incremented until reached = R.
Since q0♯x♭does not lead to h♯Bn♭, we do not reject in line (2). Thus, there is
a computation that accepts in line (3).
On the other hand, if x ̸∈L, then q0♯x♭
∗⊢h♯Bn♭. Then we either guess a
computation in line (1) that terminates at h♯Bn♭, and so we reject in line (2),
or failure to guess correctly leads to some conﬁguration being omitted from
consideration. In this case the test in line (3) fails and so we reject in line (4).
It now remains to see how to determine R. Let us deﬁne Ri to be the
number of conﬁgurations of M that are reachable from q0♯x♭in ≤i steps. We
inductively compute R0, R1, . . . , using the following algorithm:
Input: x.
Set R0 := 1 and i := 0
Repeat
i := i + 1
Ri := 0
For all conﬁgurations β, |β| = n + 3, do { check if β is reachable }
Set reachable := false
Set reached := 0
For all conﬁgurations γ , |γ | = n + 3, do { check all possible
predecessors }
(1) Guess a computation of M of ≤i −1 moves
beginning at q0♯x♭, one move at a time.
If the computation ends at γ then
Set reached := reached + 1
If γ ⊢β or γ = β then
reachable := true
If reachable then Ri := Ri + 1

212
7
Other language classes
(2) Else if reached ̸= Ri−1 then reject
Until
Ri−1 = Ri
(3) R := Ri
Return(R)
Again, let us see why this algorithm works. We ﬁrst argue that some sequence
of guesses in line (1) will result in R = Ri being returned. This is because
for each conﬁguration β reachable in ≤j steps, we correctly guess either
β or a predecessor conﬁguration γ in line (1). Furthermore, the number of
predecessor conﬁgurations is Ri−1, so the test in line (2) fails. Since Rj ≤C =
|Q|||n(n + 2) and Rj ≥Rj−1 for all j, eventually we must have Ri−1 = Ri
for some i.
Next, we argue that if R = Ri is returned, then it is correct. The only way
R can be returned in line (3) is for the test on line (2) to fail each iteration.
Thus, reached = Ri−1, which guarantees that all predecessor conﬁgurations
have been examined.
Finally, putting together these two algorithms gives us an algorithm to accept
L. It now remains to see that these algorithms can actually be implemented by
an LBA M′. To do so, we use the usual trick of expanding the alphabet size so
that M′ can use multiple “tracks,”each of which can store n symbols. Note that
log2 C = log2 |Q| + n log2 || + log2(n + 2), so we need only O(n) symbols
to store a counter for the number of conﬁgurations, where the constant in the
big-O depends only on |Q| and  and not n. Similarly, we need only O(n)
symbols to store reached and the conﬁgurations we are examining. Finally,
we need only O(n) symbols to store the Ri, since we do not save all of them,
but only the last two.
7.2 The Chomsky hierarchy
Noam Chomsky (1928–),the inﬂuential American linguist and scholar, deﬁned
a hierarchy of language classes, which is summarized (in extended form) in
Figure 7.1. It is now called the Chomsky hierarchy. Chomsky used the termi-
nology “Type 3 grammar”for what we call today a regular grammar, “Type 2
grammar”for context-free grammar, and so on.
Here are some comments about the Chomsky hierarchy. First, note that
nearly every class is represented by both a machine model and a grammar
type. This “duality”is extremely useful in proving theorems, since one has
the freedom to choose the appropriate representation. The exception is the

7.3 2DPDAs and Cook’s theorem
213
Abbreviation
Language class
Machine model
Grammar
REG
Regular languages
DFA, NFA,
Regular grammar
NFA- , 2DFA
(aka Type 3)
DCFL
Deterministic context-free 
languages
Deterministic PDA
LR(k) grammar, k ≥1
CFL
Context-free languages
PDA
Context-free grammar
(aka Type 2)
CSL
Context-sensitive languages
LBA
Context-sensitive grammar,
length-increasing grammar
(aka Type 1)
REC
Recursive languages
Always-halting TM
RE
Recursively enumerable 
languages
TM
Unrestricted grammar
(aka Type 0)
ϵ
Figure 7.1: The Chomsky hierarchy
class of recursive languages, which does not have a corresponding grammar
model.
Also note that each class of languages is strictly contained in the one
after it.
7.3 2DPDAs and Cook’s theorem
A 2DPDA is a two-way deterministic pushdown automaton. This model dif-
fers from the ordinary pushdown automaton (PDA) model as described in
Section 1.5 in three ways:
• It is deterministic. There are no ϵ-moves, and from every conﬁguration there
is at most one move.
• It is a two-way machine. The input head can move left or remain stationary,
as well as move right.
• It has endmarkers. A left endmarker ♯is at the left end of the input and a
right endmarker ♭is at the right end of the input.
More formally, we deﬁne a 2DPDA to be a 9-tuple M = (Q, , , ♯, ♭,
δ, q0, Z0, F), where
• Q is a ﬁnite nonempty set of states.
•  is a ﬁnite nonempty input alphabet, not containing ♯or ♭.
•  is a ﬁnite nonempty stack alphabet.
• ♯and ♭are the left and right endmarkers.
• δ is the transition (partial) function, mapping Q × ( ∪{♯, ♭}) ×  →
Q × {−1, 0, 1} × ∗. The meaning of δ(q, a, X) = (p, j, α) is that in state
q, scanning symbol a, and with X on top of the stack, the 2DPDA enters
state p, moves left, remains stationary, or moves right according to whether

214
7
Other language classes
j = −1, 0, 1, respectively, and replaces X with the string α. If δ(q, a, X) is
undeﬁned, the machine crashes.
• q0 ∈Q is the initial state.
• Z0 ∈ is the initial stack symbol.
• F ⊆Q is the set of ﬁnal states.
A full conﬁguration is a triple (q, h, α), where q is the current state, h is an
integer representing the current position of the input head (where h = 0 means
that we are scanning the ♯sign), and α is the stack contents.
We go from one full conﬁguration to another in an (hopefully) obvious way.
For example, if δ(q, a, X) = (p, 1, α), then if we are currently scanning a at
position j we have (q, j, Xβ) ⊢(p, j + 1, αβ). We write
∗⊢to denote the
reﬂexive, transitive closure of ⊢, and we sometimes write
∗⊢x to emphasize
that the tape contents is x. Note that the tape contents never changes since the
2DPDA cannot write on the tape.
We deﬁne
L(M) = {x ∈∗: (q0, 0, Z0)
∗⊢x(p, i, α) for some i, 0 ≤i ≤|x| + 1, α ∈∗,
and p ∈F}.
We can depict 2DPDAs graphically using a transition diagram similar to
that used for deterministic ﬁnite automaton (DFAs). If δ(q, a, X) = (p, j, α),
we draw a transition as shown in Figure 7.2.
Example 7.3.1. The language L = {0i1i2i : i ≥1} is a non-CFL that is a
2DPDA language.
To accept L, start at the left-hand side of the input. If the ﬁrst symbol seen
is not 0, reject. For each successive 0 seen, push a’s onto the stack until a 1
is seen. When a 1 is seen, start popping off symbols and moving right on the
input until Z0 (the initial stack symbol) reappears or 2 appears in the input. If
2 appears before Z0 encountered, reject (since there are more 0s than 1s). If 0
appears, reject (since there is a 0 after a 1). Otherwise look at the next input
symbol. If it is a 1, reject (since there are more 1s than 0s). If it is a 0, reject
(since there is a 0 after a 1). If you have not rejected, then the number of 0s
equals the number of 1s in the ﬁrst part of the string.
q
p
a, X/j, α
Figure 7.2: The transition δ(q, a, X) = (p, j, α) depicted

7.3 2DPDAs and Cook’s theorem
215
Now move left until a 0 is encountered, and do the same thing with the 1s
and 2s that you did with the 0s and 1s. If you have not rejected, then the number
of 1s equals the number of 2s. Move right and accept.
Example 7.3.2. The language {ww : w ∈{0, 1}∗} is a non-CFL that is a
2DPDA language.
To see this, we use the following method: (1) ﬁrst, scan the input from left
to right pushing a’s onto the stack until ♭is reached. Also keep track of the
parity of the number of characters and reject if it is odd. Now move back to the
beginning of the input and move right, popping two a’s from the stack for each
character read. When Z0 reappears, you are at the middle of the input. (2) Now
move right, pushing input symbols onto the stack until ♭is reached. Now push
a b onto the stack. Now repeat (1); when b reappears, you are at the middle of
the input. Now move left, matching input symbols against the stack contents.
If all symbols match, and ♯is reached, accept.
Example 7.3.3. The language
{xcy : x, y ∈{a, b}∗and y is a subword of x}
is a 2DPDA language.
To see this, use the following algorithm, embodied in Figure 7.3:
1. M moves right on input until c is encountered.
2. M moves left, copying the symbols of x onto the stack until ♯is encountered.
Now xZ0 (not xR) is on the stack. Let x = a1a2 · · · an.
3. M moves right until c is encountered and then moves one symbol to the
right.
4. While the symbol on top of the stack matches the next symbol of the input,
pop off the symbol on top of the stack and move the input head one square
to the right.
5. If the input head is scanning ♭, accept. If the input head is not scanning ♭,
but Z0 is on top of the stack, reject. Otherwise the top of stack symbol and
input symbol disagree. Restore the stack by moving the input head to the
left until c is reached, copying. At this point M has aiai+1 · · · anZ0 on the
stack for some i, 1 ≤i ≤n. If i = n + 1, M halts and rejects. Otherwise M
pops ai and moves its input head to the right to c and then one more symbol
to the right. Return to step 4.
Here is a sample computation on input:
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
♯
a
b
b
a
b
b
a
b
a
b
c
a
b
a
♭

216
7
Other language classes
Conﬁguration
State
Input
Stack
number
position
contents
0
q0
0
Z0
1
q0
1
Z0
...
...
...
...
10
q0
10
Z0
11
q0
11
Z0
12
q1
10
Z0
13
q1
9
bZ0
14
q1
8
abZ0
...
...
...
...
21
q1
1
bbabbababZ0
22
q1
0
abbabbababZ0
23
q2
1
abbabbababZ0
...
...
...
...
33
q2
11
abbabbababZ0
34
q3
12
abbabbababZ0
35
q3
13
bbabbababZ0
36
q3
14
babbababZ0
37
q4
13
babbababZ0
38
q4
12
bbabbababZ0
39
q4
11
abbabbababZ0
40
q3
12
bbabbababZ0
41
q4
11
bbabbababZ0
42
q3
12
babbababZ0
43
q4
11
babbababZ0
44
q3
12
abbababZ0
45
q3
13
bbababZ0
46
q3
14
bababZ0
47
q4
13
bababZ0
48
q4
12
bbababZ0
49
q4
11
abbababZ0
50
q3
12
bbababZ0
51
q4
11
bbababZ0
52
q3
12
bababZ0
53
q4
11
bababZ0
54
q3
12
ababZ0
55
q3
13
babZ0
56
q3
14
abZ0
57
q3
15
bZ0
58
qf
15
Z0

7.3 2DPDAs and Cook’s theorem
217
b, Z0/1, Z0
a, Z0/1, Z0
♯, Z0/1, Z0
a, Z0/
1, aZ0
b, Z0/
1, bZ0
a, a/
1, aa
b, a/
1, ba
a, b/
1, ab
b, b/
1, bb
c, Z0/
1, Z0
a, a/1, a
a, b/1, b
b, a/1, a
b, b/1, b
♯, a/1, a
♯, b/1, b
a, a/1, ϵ
b, b/1, ϵ
c, a/1, a
c, b/1, b
♭, Z0/0, ϵ
♭, a/0, ϵ
♭, b/0, ϵ
a, a/ −1, aa
a, b/ −1, ab
b, a/ −
−
−
−
−
−
−
−
1, ba
b, b/ −1, bb
(Backup, restore stack)
b, a/ −1, a
a, b/ −1, b
c, a/1, ϵ
(Cycle to
next suffi
 x of
x to check)
♭, Z0/0, ϵ
ϵ
♭, a/0,
♭, b/0, ϵ
(Move right to c)
(Copy x on to stack)
(Move right to c)
(Attempt to match y to x)
(Mismatch found)
q0
q1
q2
q3
qf
c, b/1, ϵ
q4
Figure 7.3: A 2DPDA for string matching
Note that the pattern-matching 2DPDA of the previous example can take

(|x||y|) steps on input xcy. This is 
(r2) if |x| = 2r and |y| = r. The fol-
lowing theorem of Cook, then, is very surprising.
Theorem 7.3.4. A 2DPDA can be simulated in linear time on a RAM.
Our RAM model is an assembler-type machine with an inﬁnite number of
registers, x0, x1, x2, . . ., where each register can hold an arbitrary integer. A
RAM program contains instructions that allow a register to be loaded with a
constant, to add or subtract the contents of two registers and store their sum
or difference in a third, to perform indirect addressing, to do comparisons, and
to read input and print output. The reader should check that these operations
sufﬁce to implement the algorithm given next.

218
7
Other language classes
We deﬁne a partial conﬁguration to be a triple of the form (q, p, A), where
q is a state of the 2DPDA, p is a position that the head is scanning, and A is
the current symbol on top of the stack.
The idea of the proof is to attempt to “short-circuit”the computation of M
by determining a terminator for each partial conﬁguration. The terminator of a
conﬁguration i is deﬁned to be the conﬁguration from which, on the next move,
the stack height ﬁrst dips below that of i. If we reenter the conﬁguration i, we
can simply bypass all intermediate computation (which can never depend on
symbols buried in the stack) and proceed directly to the terminator and perform
its transition, popping the stack.
To simplify the proof, we assume without loss of generality that every
move either pops, makes a horizontal move, or pushes a single additional
symbol onto the stack. We use another stack S to keep track of those partial
conﬁgurations whose terminator is still sought. We use an array next[
] to
store the terminators known so far.
The algorithm uses the following variables:
• q, the current state of the 2DPDA M being simulated;
• p, the current position of M’s tape head;
• G, the current contents of M’s stack, with the top at the left;
• i, a current partial conﬁguration of the form (q, p, top(G));
• S, a stack of lists that is used to hold partial conﬁgurations whose terminator
is currently unknown;
• flags[
], a bit array indexed by partial conﬁgurations, initially 0;
• next[
], an array of pairs ⊆N × Q indexed by partial conﬁgurations,
initially 0.
Here is the 2DPDA simulation algorithm:
{ initialize }
q := q0
p := 0
G := Z0
i := (q0, 0, Z0)
{ initialize simulation }
S := empty stack
push(S, empty list)
while (|δ(i)| ̸= 0) do { while it is possible to move do }
if next[i] = 0 then
(q′, d, β) := δ(i) { get next move }
if (|β| ≥1) do { horizontal move or push }

7.3 2DPDAs and Cook’s theorem
219
if flags[i] = 1 then
halt and reject
{ because M was in partial conﬁguration i at some previous time
in the computation and the computation path has led back to
partial conﬁguration i at the same or higher-level of the stack;
hence M is in an inﬁnite loop }
else
append i to top(S)
flags[i] := 1
q := q′
p := p + d
replace top symbol of G with β
if |β| = 2 { push } push(S, empty list)
i := (q, p, top(G))
if (|β| = 0) do { pop the stack }
pop(G)
next[i] := (q′, p + d)
for each element x in top(S)
set next[x] := (q′, p + d)
flags[x] := 0
pop(S)
q := q′
p := p + d
i := (q, p, top(G))
else {next[i] ̸= 0; perform ‘shortcircuit’}
(q′, p′) := next[i]
pop(G)
q := q′
p := p′
for each element x in top(S)
set next[x] := (q′, p′)
flags[x] := 0
pop(S)
i := (q, p, top(G))
{ end do }
if i is an accepting conﬁguration
accept
otherwise
reject

220
7
Other language classes
Theorem 7.3.5. The 2DPDA simulation runs in linear time.
First we prove two lemmas.
Lemma 7.3.6. During the course of the simulation, every partial conﬁguration
gets pushed and popped off the stack S at most once.
Proof.
Suppose a partial conﬁguration i is placed on the stack. When this
happens, flags[i] is set to 1. If the next action involving i is a push at a
higher level of the stack, then we are in an inﬁnite loop, and this is detected by
checking flags[i]. So the next action must be a pop. At this point the next
array is updated. The next time i is encountered, the next entry has been set,
and so we pop, not push.
Lemma 7.3.7. Every element of next is assigned at most once.
Proof. Suppose not. Then some element, say next[i], is assigned twice. But
next[i] gets assigned only when i is popped from S. The ﬁrst time next[i]
is assigned, i is popped off. The next time next[i] is assigned, i is popped
off again. If this occurs because there was an i buried in the stack below the i
that was ﬁrst popped off, then there was an i encountered above another i, and
we are in an inﬁnite loop, which is detected by examining flags[
]. So this
cannot happen.
So another i must have been pushed after the ﬁrst was popped. But this
cannot happen, since when i is considered for the second time, next[i] was
already set, so we would not push i onto the stack.
We are now ready to prove Theorem 7.3.5.
Proof.
We claim that the simulation runs in time linear in the number of
reachable partial conﬁgurations, which is bounded by |Q|(|x| + 2)||.
To see this, let each of the following actions cost $1:
• appending a partial conﬁguration to top(S);
• popping a partial conﬁguration from top(S);
• setting next[i];
• reading next[i];
• pushing onto G;
• popping from G.
Now assign $7 to each partial conﬁguration i. The money is allocated as
follows:

7.4 Exercises
221
• We allocate $5 for reading next[i] when next[i] = 0. This can occur only
once by Lemma 7.3.7, for either the stack height stays the same or goes up
between the next time i is considered, in which case we are in an inﬁnite
loop, or the stack height dips below its current height. But in this latter case,
i was previously appended to top(S), so when it is popped off S, next[i] is
set. $2 pays for the cost of reading next[i] and appending i to top(S). The
other $3 is allocated as follows: $1 pays for the cost of pushing onto G and
the other $2 is “stapled”to the symbol we push onto G, for later use.
• We allocate $2 for the cost of setting next[i] and popping i from the stack
S. From Lemma 7.3.7 this occurs only once for each i.
Finally, we have to account for the cost of what happens when next[i] ̸= 0.
This is always accompanied by a pop of the stack G. So when this occurs, we
can pop the stack and recover the $2 that was stored there. $1 is used to pay
for reading next[i] and the other $1 is used to pay for the cost of popping the
stack.
7.4 Exercises
1. Under which operations in the following list is the class of CSLs closed:
Union, concatenation, intersection, substitution by CSLs, inverse homomor-
phism, Kleene closure, positive closure?
2. Suppose we modify our PDA model as follows: instead of a ﬁnite number of
nondeterministic choices, depending on the current symbol being read (or
ϵ) and the current top-of-stack contents, we allow the PDA to nondetermin-
istically replace the symbol currently on top of the stack with any member
of a given CSL. More formally, we allow
δ(a, X) = L(a, X),
where a ∈ ∪{ϵ}, and X ∈, where L(a, X) is a CSL. We accept by
empty stack.
Prove or disprove that the set of languages accepted by these more pow-
erful PDAs is precisely the set of CSLs. (One direction is easy.)
3. Show that every context-free language is accepted by a deterministic LBA.
4. The Boolean closure of the CFLs is the class of all languages that can be
obtained from CFLs by a ﬁnite number of applications of union, intersection,
and complement operations. Show that if L is contained in the Boolean
closure of the CFLs, then L is accepted by a deterministic LBA.
5. Show that there is a language L accepted by a deterministic LBA that is not
in the Boolean closure of the CFLs.

222
7
Other language classes
6. Give a length-increasing grammar generating the language
{0Fn : n ≥1} = {0, 00, 000, 00000, 00000000, . . .},
where Fn is the nth Fibonacci number, deﬁned by F0 = 0, F1 = 1, and
Fn = Fn−1 + Fn−2 for n ≥2.
7. Show that the set of primitive strings over {a, b} is context-sensitive. (For
the deﬁnition of primitive strings, see Section 2.3.)
8. Give an example of a CSL that is not a CFL.
9. Find a length-increasing grammar to generate the languages
(a) L = {0i1j2max(i,j) : i, j ≥1};
(b) L = {0i1j2min(i,j) : i, j ≥1};
10. Construct a 2DPDA to accept the language L = {w ∈{a, b}∗: w is
primitive }. Conclude that L can be recognized in linear time on a RAM.
11. Show that the set of bordered words over {a, b} is a 2DPDA language.
(The deﬁnition of bordered appears in Exercise 4.16)
12. Show that the language L = {a2n : n ≥1} is a 2DPDA language that is not
a CFL.
13. Show that the language L = {0n1mn : m, n ≥1} is a 2DPDA language that
is not a CFL.
14. Show that the language L = {0n1n2 : n ≥1} is accepted by a 2DPDA.
15. Show that for every DFA M = (Q, , δ, q0, F), there exists a “small”
2DPDA M′ over a unary alphabet that on input 1n accepts iff |n ∩
L(M)| ≥1, that is, if M accepts at least one string of length n. By small
I mean that the size of your 2DPDA (i.e., the number of transition rules)
should be O(|||Q|).
7.5 Projects
1. Find out about L-systems and their applications to computer graphics. A
good place to start is Prusinkiewicz and Hanan [1989] and Prusinkiewicz
and Lindenmayer [1990].
7.6 Research problems
1. A conjunctive grammar is a generalization of context-free grammars where
productions such as A →α1 ∧· · · ∧αm, αi ∈(V ∪)∗, are allowed. This
production means A generates those words generated by all the α’s. The
conjunctive languages are those generated by conjunctive grammars. Is
the family of conjunctive languages closed under complementation? This
question is due to Okhotin [2006].

7.7 Notes on Chapter 7
223
2. A Boolean grammar is a generalization of context-free grammars where
productions such as A →α1 ∧· · · ∧αm ∧(¬β1) ∧· · · ∧(¬βn), αi, βi ∈
(V ∪)∗, are allowed. This production means that A generates those words
generated by all the α’s and none of the β’s. Are there any languages rec-
ognized by deterministic LBAs in O(n2) time that cannot be speciﬁed by
Boolean grammars? This question is due to Okhotin [2006].
7.7 Notes on Chapter 7
7.1 Length-increasing
grammars
should
really
be
called
“length-
nondecreasing”grammars, but the former term is the one used in the
literature.
Kuroda [1964] proved the equivalence of CSLs and LBAs. The clo-
sure of the context-sensitive languages under complementation was
proved independently by Immerman [1988] and Szelepcs´enyi [1988].
Our proof is based on notes by J. Buss.
7.2 The Chomsky hierarchy was ﬁrst deﬁned by Chomsky [1956, 1959].
7.3 2DPDAs were introduced in Gray, Harrison, and Ibarra [1967]. Aho,
Hopcroft, and Ullman [1968] proved that any 2DPDA language can be
recognized in O(n2) time on a RAM. This was further improved to O(n)
time by Cook [1972]. An exposition of Cook’s proof can be found in
Aho, Hopcroft, and Ullman [1974, §9.4]. Also see Jones [1977]. Our
presentation is based largely on unpublished notes of Daniel Boyd.


Bibliography
Adian, S. I. (1979). The Burnside Problem and Identities in Groups. Springer-Verlag.
Aho, A. V., Hopcroft, J. E., and Ullman, J. D. (1968). Time and tape complexity of
pushdown automaton languages. Inf. Control 13, 186–206.
Aho, A. V., Hopcroft, J. E., and Ullman, J. D. (1974). The Design and Analysis of
Computer Algorithms. Addison-Wesley.
Aho, A. V., Sethi, R., and Ullman, J. D. (1986). Compilers: Principles, Techniques, and
Tools. Addison-Wesley.
Aho, A. V., and Ullman, J. D. (1972). The Theory of Parsing, Translation, and Compiling,
Vol. 1. Prentice Hall.
Allouche J.-P., and Shallit, J. (1999). The ubiquitous Prouhet-Thue-Morse sequence. In
C. Ding, T. Helleseth, and H. Niederreiter, editors, Sequences and Their Applications,
Proceedings of SETA ’98, pp. 1–16. Springer-Verlag.
Allouche, J.-P., and Shallit, J. (2003). Automatic Sequences: Theory, Applications,
Generalizations. Cambridge University Press.
Angluin, D. (1987). Learning regular sets from queries and counterexamples. Inf. Con-
trol 75, 87–106.
Axelrod, R. M. (1984). The Evolution of Cooperation. Basic Books.
Bader, C., and Moura, A. (1982). A generalization of Ogden’s lemma. J. Assoc. Comput.
Mach. 29, 404–407.
Bar-Hillel, Y., Perles, M., and Shamir, E. (1961). On formal properties of simple phrase
structure grammars. Z. Phonetik. Sprachwiss. Kommuniationsforsch. 14, 143–172.
Berstel, J. (1979). Transductions and Context-Free Languages. Teubner.
Berstel, J. (1995). Axel Thue’s Papers on Repetitions in Words: A Translation. Number
20 in Publications du Laboratoire de Combinatoire et d’Informatique Math´ematique.
Universit´e du Qu´ebec `a Montr´eal, February.
Berstel, J., and Perrin, D. (2007). The origins of combinatorics on words. Eur. J. Comb.
28, 996–1022.
Birget, J.-C. (1992). Intersection and union of regular languages and state complexity.
Inf. Process. Lett. 43, 185–190.
Blum, N. (1996). An O(n log n) implementation of the standard method for minimizing
n-state ﬁnite automata. Inf. Process. Lett. 57, 65–69.
Boonyavatana, R., and Slutzki, G. (1988). The interchange or pump (di)lemmas for
context-free languages. Theo. Comput. Sci. 56, 321–338.
225

226
Bibliography
Borwein, P., and Ingalls, C. (1994). The Prouhet-Tarry-Escott problem revisited.
Enseign. Math. 40, 3–27.
Brady, A. H. (1983). The determination of the value of Rado’s noncomputable function
(k) for four-state Turing machines. Math. Comput. 40, 647–665.
Brandenburg, F.-J. (1983). Uniformly growing k-th power-free homomorphisms. Theo.
Comput. Sci. 23, 69–82.
Brauer, W. (June 1988). On minimizing ﬁnite automata. Bull. Eur. Assoc. Theor. Comput.
Sci. No. 35, 113–116.
Brzozowski, J. (1962a). Canonical regular expressions and minimal state graphs for
regular events. In Mathematical Theory of Automata, pp. 529–561. Polytechnic Press.
Brzozowski, J. (1962b). A survey of regular expressions and their applications. IEEE
Trans. Electr. Comput. 11, 324–335.
Brzozowski, J. A. (February 1989). Minimization by reversal is not new. Bull. Eur.
Assoc. Theor. Comput. Sci. No. 37, 130.
Bucher, W. (January 1980). A density problem for context-free languages. Bull. Eur.
Assoc. Theor. Comput. Sci. No. 10, 53.
Cantor, D. C. (1962). On the ambiguity problem of Backus systems. J. Assoc. Comput.
Mach. 9, 477–479.
Chaitin, G. J. (1979). Toward a mathematical deﬁnition of “life.” In R. D. Levine and
M. Tribus, editors, The Maximum Entropy Formalism, pp. 477–498. MIT Press.
Chomsky, N. (1956). Three models for the description of language. IRE Trans. Inf.
Theory 2, 113–124.
Chomsky, N. (1959). On certain formal properties of grammars. Inf. Control 2, 137–
167.
Chomsky, N., and Sch¨utzenberger, M. P. (1963). The algebraic theory of context-free
languages. In P. Braffort and D. Hirschberg, editors, Computer Programming and
Formal Systems, pp. 118–161. North-Holland, Amsterdam.
Cook, S. A. (1971). The complexity of theorem-proving procedures. In Proceedings of
the Third Annual ACM Symposium on Theory of Computing, pp. 151–158. ACM
Press.
Cook, S. A. (1972). Linear time simulation of deterministic two-way pushdown au-
tomata. In C. V. Freiman, editor, Information Processing 71 (Proceedings of the IFIP
Congress 71), Vol. 1, pp. 75–80. North-Holland.
Coppersmith, D., and Winograd, S. (1990). Matrix multiplication via arithmetic pro-
gressions. J. Sym. Comput. 9, 251–280.
Crochemore, M. (1981). An optimal algorithm for computing the repetitions in a word.
Inf. Process. Lett. 12, 244–250.
Davis, M. (1958). Computability and Unsolvability. McGraw-Hill. Enlarged edition,
reprinted by Dover, 1982.
Drobot, V. (1989). Formal Languages and Automata Theory. Computer Science Press.
Du, D.-Z., and Ko, K.-I. (2001). Problem Solving in Automata, Languages, and Com-
plexity. Wiley.
Earley, J. (1970). An efﬁcient context-free parsing algorithm. Commun. ACM 13, 94–
102.
Eggan, L. C. (1963). Transition graphs and the star-height of regular events. Michigan
Math. J. 10, 385–397.

Bibliography
227
Ehrig, H., Engels, G., Kreowski, H.-J., and Rozenberg, G., editors. (1999). Handbook
of Graph Grammars and Computing by Graph Transformation, Vol. 2: Applications,
Languages, and Tools. World Scientiﬁc.
Ehrig, H., Kreowski, H.-J., Montanari, U., and Rozenberg, G., editors. (1999). Handbook
of Graph Grammars and Computing by Graph Transformation, Vol. 2: Concurrency,
Parallelism, and Distribution. World Scientiﬁc.
Euwe, M. (1929). Mengentheoretische Betrachtungen ¨uber das Schachspiel. Proc.
Konin. Akad. Wetenschappen, Amsterdam 32, 633–642.
Fine, N. J., and Wilf, H. S. (1965). Uniqueness theorems for periodic functions. Proc.
Am. Math. Soc. 16, 109–114.
Fischer, P. C. (1963). On computability by certain classes of restricted Turing machines.
In Proceedings of the 4th IEEE Symposium on Switching Circuit Theory and Logical
Design, pp. 23–32. IEEE Press.
Floyd, R. W. (1962). On ambiguity in phrase structure languages. Commumn. ACM 5,
526–534.
Gabarro, J. (February 1985). Some applications of the interchange lemma. Bull. Eur.
Assoc. Theor. Comput. Sci. No. 25, 19–21.
Garey, M. R., and Johnson, D. S. (1979). Computers and Intractability: A Guide to the
Theory of NP-Completeness. Freeman.
Gazdar, G., Klein, E., Pullum, G., and Sag, I. (1985). Generalized Phrase Structure
Grammar. Harvard University Press.
Ginsburg, S. (1966). The Mathematical Theory of Context Free Languages. McGraw-
Hill.
Ginsburg, S., and Greibach, S. A. (1966). Deterministic context-free languages. Inf.
Control 9, 563–582.
Ginsburg, S., and Rice, H. G. (1962). Two families of languages related to ALGOL. J.
Assoc. Comput. Mach. 9, 350–371.
Ginsburg, S., and Rose, G. F. (1963). Some recursively unsolvable problems in ALGOL-
like languages. J. Assoc. Comput. Mach. 10, 29–47.
Ginsburg, S., and Spanier, E. H. (1963). Quotients of context-free languages. J. Assoc.
Comput. Mach. 10, 487–492.
Ginsburg, S., and Spanier, E. H. (1966). Finite-turn pushdown automata. J. SIAM Control
4, 429–453.
Glaister, I., and Shallit, J. (1996). A lower bound technique for the size of nondetermin-
istic ﬁnite automata. Inf. Process. Lett. 59, 75–77.
Goldstine, J. (1977). A simpliﬁed proof of Parikh’s theorem. Discrete Math. 19, 235–
239.
Goldstine, J., Price, J. K., and Wotschke, D. (1982a). A pushdown automaton or a con-
text-free grammar—which is more economical? Theo. Comput. Sci. 18, 33–40.
Goldstine, J., Price, J. K., and Wotschke, D. (1982b). On reducing the number of states
in a PDA. Math. Syst. Theory 15, 315–321.
Gray, J. N., Harrison, M. A., and Ibarra, O. H. (1967). Two-way pushdown automata.
Inf. Control 11, 30–70.
Gries, D. (1973). Describing an algorithm by Hopcroft. Acta Inf. 2, 97–109.
Gupta, N. (1989). On groups in which every element has ﬁnite order. Am. Math. Mon.
96, 297–308.

228
Bibliography
Haines, L. H. (1969). On free monoids partially ordered by embedding. J. Comb. Theory
6, 94–98.
Harju, T., and Karhum¨aki, J. (1997). Morphisms. In G. Rozenberg and A. Salomaa,
editors, Handbook of Formal Languages, Vol. 1, pp. 439–510. Springer-Verlag.
Harju, T., and Nowotka, D. (2004). The equation xi = yjzk in a free semigroup. Semi-
group Forum 68, 488–490.
Harrison, M. A. (1978). Introduction to Formal Language Theory. Addison-Wesley.
Hashiguchi, K. (1982). Regular languages of star height one. Inf. Control 53, 199–210.
Hashiguchi, K. (1988). Algorithms for determining relative star height and star height.
Inf. Comput. 78, 124–169.
Hopcroft, J. (1971). An n log n algorithm for minimizing states in a ﬁnite auto-
maton. In Theory of Machines and Computations, pp. 189–196. Academic Press.
Hopcroft, J. E., Motwani, R., and Ullman, J. D. (2001). Introduction to Automata Theory,
Languages, and Computation. Addison-Wesley.
Hopcroft, J. E., and Ullman, J. D. (1979). Introduction to Automata Theory, Languages,
and Computation. Addison-Wesley.
Ibarra, O. H., and Jiang, T. (1991). Learning regular languages from counterexamples.
J. Comput. Syst. Sci. 43, 299–316.
Immerman, N. (1988). Nondeterministic space is closed under complementation. SIAM
J. Comput. 17, 935–938.
Jiang, T., and Ravikumar, B. (1993). Minimal NFA problems are hard. SIAM J. Comput.
22, 1117–1141.
Jones, N. D. (1977). A note on linear-time simulation of deterministic two-way push-
down automata. Inf. Process. Lett. 6, 110–112.
Kfoury, A.-J. (1988). A linear-time algorithm to decide whether a binary word contains
an overlap. RAIRO Inf. Th´eor. Appl. 22, 135–145.
Knuth, D. E. (1965). On the translation of languages from left to right. Inf. Control 8,
607–639.
Kozen, D. (February 1996). On regularity-preserving functions. Bull. Eur. Assoc. Theor.
Comput. Sci. No. 58, 131–138.
Kuroda, S. Y. (1964). Classes of languages and linear bounded automata. Inf. Control
7, 207–223.
Lawson, M. V. (2004). Finite Automata. CRC Press.
Lewis, H. R., and Papadimitriou, C. H. (1998). Elements of the Theory of Computation.
Prentice Hall.
Lewis II, P. M., and Stearns, R. E. (1968). Syntax-directed transduction. J. Assoc.
Comput. Mach. 15, 465–488.
Li, M., and Vit´anyi, P. (1995). A new approach to formal language theory by Kolmogorov
complexity. SIAM J. Comput. 24, 398–410.
Li, M., and Vit´anyi, P. (1997). An Introduction to Kolmogorov Complexity and Its
Applications. Springer-Verlag.
Lin, S., and Rado, T. (1965). Computer studies of Turing machine problems. J. Assoc.
Comput. Mach. 12, 196–212.
Linster, B. G. (1992). Evolutionary stability in the inﬁnitely repeated prisoners’ dilemma
played by two-state Moore machines. South. Econ. J. 58, 880–903.
Liu, L. Y., and Weiner, P. (1973). An inﬁnite hierarchy of intersections of context-free
languages. Math. Syst. Theory 7, 185–192.

Bibliography
229
Lorentz, R. J. (2001). Creating difﬁcult instances of the Post correspondence problem.
In T. Marsland and I. Frank, editors, Computers and Games: 2nd International
Conference, CG 2001, Vol. 2063 of Lecture Notes in Computer Science, pp. 214–
228. Springer-Verlag.
Lothaire, M. (1983). Combinatorics on Words, Vol. 17 of Encyclopedia of Mathematics
and Its Applications. Addison-Wesley.
Lothaire, M. (2002). Algebraic Combinatorics on Words, Vol. 90 of Encyclopedia of
Mathematics and Its Applications. Cambridge University Press.
Lyndon, R. C., and Sch¨utzenberger, M. P. (1962). The equation aM = bNcP in a free
group. Michigan Math. J. 9, 289–298.
Main, M. G. (1982). Permutations are not context-free: An application of the interchange
lemma. Inf. Process. Lett. 15, 68–71.
Martin, J. C. (1997). Introduction to Languages and the Theory of Computation.
McGraw-Hill.
Marxen, H., and Buntrock, J. (1990). Attacking the busy beaver 5. Bull. Eur. Assoc.
Theor. Comput. Sci. No. 40 (February 1990), 247–251.
Maslov, A. N. (1970). Estimates of the number of states of ﬁnite automata. Dokl. Akad.
Nauk. SSSR 194, 1266–1268. In Russian. English translation in Sov. Math. Dokl. 11,
1373–1375.
Maurer, H. A. (1969). A direct proof of the inherent ambiguity of a simple context-free
language. J. Assoc. Comput. Mach. 16, 256–260.
McCulloch, W. S., and Pitts, W. (1943). A logical calculus of the ideas immanent in
nervous activity. Bull. Math. Biophy. 5, 115–133.
McNaughton, R., and Papert, S. (1968). The syntactic monoid of a regular event. In
M. A. Arbib, editor, Algebraic Theory of Machines, Languages, and Semigroups,
pp. 297–312. Academic Press.
Mealy, G. H. (1955). A method for synthesizing sequential circuits. Bell Syst. Tech. J.
34, 1045–1079.
Moore, E. F. (1956). Gedanken experiments on sequential machines. In C. E. Shannon
and J. McCarthy, editors, Automata Studies, pp. 129–153. Princeton University Press.
Myhill, J. (1957). Finite automata and the representation of events. Technical Report
WADD TR-57-624, Wright Patterson Air Force Base, Ohio.
Nerode, A. (1958). Linear automaton transformations. Proc. Am. Math. Soc. 9, 541–544.
Nozaki, A. (1979). Equivalence problem of nondeterministic ﬁnite automata. J. Comput.
Syst. Sci. 18, 8–17.
Ogden, W. (1968). A helpful result for proving inherent ambiguity. Math. Syst. Theory
2, 191–194.
Ogden, W., Ross, R. J., and Winklmann, K. (1985). An “interchange” lemma for context-
free languages. SIAM J. Comput. 14, 410–415.
Okhotin, A. (2006). Nine open problems on conjunctive and Boolean grammars. Tech-
nical Report 794, Turku Centre for Computer Science, Turku, Finland, 2006.
Papadimitriou, C. H. (1994). Computational Complexity. Addison-Wesley.
Parikh, R. J. (1966). On context-free languages. J. Assoc. Comput. Mach. 13, 570–581.
Perrin, D., and Pin, J.-E. (2003). Inﬁnite Words: Automata, Semigroups, Logic, and
Games. Academic Press.
Pilling, D. L. (1973). Commutative regular equations and Parikh’s theorem. J. Lond.
Math. Soc. 6, 663–666.

230
Bibliography
Post, E. (1946). A variant of a recursively unsolvable problem. Bull. Am. Math. Soc. 52,
264–268.
Prusinkiewicz, P., and Hanan, J. (1989). Lindenmayer Systems, Fractals, and Plants,
Vol. 79 of Lecture Notes in Biomathematics. Springer-Verlag.
Prusinkiewicz, P., and Lindenmayer, A. (1990). The Algorithmic Beauty of Plants.
Springer-Verlag.
Rabin, M. O., and Scott, D. (1959). Finite automata and their decision problems. IBM
J. Res. Dev. 3, 114–125.
Rado, T. (1962). On non-computable functions. Bell Syst. Tech. J. 41, 877–884.
Robson, J. M. (1989). Separating strings with small automata. Inf. Process. Lett. 30,
209–214.
Rosenkrantz, D. J., and Stearns, R. E. (1970). Properties of deterministic top-down
grammars. Inf. Control 17, 226–256.
Ross, R., and Winklmann, K. (1982). Repetitive strings are not context-free. RAIRO Inf.
Th´eor. Appl. 16, 191–199.
Rozenberg, G. (1997) editor. Handbook of Graph Grammars and Computing by Graph
Transformation, Vol. 1: Foundations. World Scientiﬁc.
Rubinstein, A. (1986). Finite automata play the repeated prisoner’s dilemma. J. Econ.
Theory 39, 83–96.
Sch¨utzenberger, M. P. (1963). On context-free languages and pushdown automata. Inf.
Control 6, 246–264.
Seiferas, J. I., and McNaughton, R. (1976). Regularity-preserving relations. Theor.
Comput. Sci. 2, 147–154.
Shepherdson, J. C. (1959). The reduction of two-way automata to one-way automata.
IBM J. Res. Dev. 3, 198–200.
Shieber, S. M. (1985). Evidence against the context-freeness of natural language. Lin-
guist. Phil. 8, 333–343.
Shyr, H. J., and Thierrin, G. (1977). Disjunctive languages and codes. In M. Karpi´nski,
editor, Fundamentals of Computation Theory: Proceedings of the 1977 Interna-
tional FCT-Conference, Vol. 56 of Lecture Notes in Computer Science, pp. 171–176.
Springer.
Stearns, R. E., and Hartmanis, J. (1963). Regularity preserving modiﬁcations of regular
expressions. Inf. Control 6, 55–69.
Strachey, C. (1965). An impossible program. Comput. J. 7, 313.
Szelepcs´enyi, R. (1988). The method of forced enumeration for nondeterministic au-
tomata. Acta Inf. 26, 279–284.
Thomas, W. (1991). Automata on inﬁnite objects. In J. van Leeuwen, editor, Handbook
of Theoretical Computer Science, Vol. B (Formal Models and Semantics), pp. 133–
191. MIT Press.
Thornton, R. J. (2003). The Lexicon of Intentionally Ambiguous Recommendations.
Sourcebooks.
Thue, A. (1906). ¨Uber unendliche Zeichenreihen. Norske vid. Selsk. Skr. Mat. Nat. Kl.
7, 1–22. Reprinted in T. Nagell, editor, Selected Mathematical Papers of Axel Thue,
pp. 139–158. Universitetsforlaget, Oslo, 1977.
Thue, A. (1912). ¨Uber die gegenseitige Lage gleicher Teile gewisser Zeichenreihen.
Norske vid. Selsk. Skr. Mat. Nat. Kl. 1, 1–67. Reprinted in T. Nagell, editor, Selected
Mathematical Papers of Axel Thue, pp. 413–478, Universitetsforlaget, Oslo, 1977.

Bibliography
231
Turing, A. M. (1936). On computable numbers, with an application to the Entschei-
dungsproblem. Proc. Lond. Math. Soc. 42, 230–265.
Urbanek, F. (1990). A simple completeness proof for Earley’s algorithm. Bull. Eur.
Assoc. Theor. Comput. Sci. No. 42, 194.
Valiant, L. (1975). General context-free recognition in less than cubic time. J. Comput.
Syst. Sci. 10, 308–315.
Wolfram, S. (1984). Computation theory of cellular automata. Commun. Math. Phys.
96, 15–57. Reprinted in S. Wolfram, editor, Cellular Automata and Complexity:
Collected Papers, pp. 159–202. Addison-Wesley, 1994.
Wolfram, S. (2002). A New Kind of Science. Wolfram Media.
Wood, D. (1987). Theory of Computation. Wiley.
Woods, D. R. (1978). Elementary problem proposal E 2692. Am. Math. Mon. 85, 48.
Solution by D. Robbins, 86 (1979), 394–395.
Wright, E. M. (1959). Prouhet’s 1851 solution of the Tarry-Escott problem of 1910. Am.
Math. Mon. 66, 199–201.
Younger, D. H. (1967). Recognition and parsing of context-free languages in time n3.
Inf. Control 10, 189–208.
Yu, S. (1997). Regular languages. In G. Rozenberg and A. Salomaa, editors, Handbook
of Formal Languages, Vol. 1, pp. 41–110. Springer-Verlag.
Yu, S., Zhuang, Q., and Salomaa, K. (1994). The state complexities of some basic
operations on regular languages. Theor. Comput. Sci. 125, 315–328.
Zhang, G.-Q. (1999). Automata, Booleanmatrices, andultimateperiodicity.Inf.Comput.
152, 138–154.
Zhao, L. (2003). Tackling Post’s correspondence problem. In J. Schaeffer, M. M¨uller,
and Y. Bj¨ornsson, editors, Computers and Games, 3rd International Conference, CG
2002, Vol. 2883 of Lecture Notes in Computer Science, pp. 326–344. Springer-Verlag.


Index
2DFA, 66
2DPDA, 213
3-SAT, 21
abelian cube, 47
abelian square, 47, 137
Ackerman, M., xi
Adian, S. I., 43, 47, 48
Aho, A. V., 173, 201, 223
Alces, A., 29
alfalfa, 30, 34, 37
Allouche, J.-P., 48, 105
alphabet, 1
unary, 1
always-halting TM, 209
ambiguous, 10
NFA, 102
Angluin, D., 105
angst
existential, 54
antisymmetric, 92
assignment
satisfying, 20
automaton
pushdown, 11
synchronizing, 105
two-way, 66
Axelrod, R. M., 105
Bader, C., 138
balanced word, 137
Bar-Hillel, Y., 201
Barkhouse, C., xi
base alphabet, 4
Berman, P., 200
Berstel, J., 48, 106
Biedl, T., 135
Van Biesbrouck, M., 139
Birget, J.-C., 107
blank symbol, B, 14
Blum, N., 106
Boolean closure, 221
Boolean grammar, 223
Boolean matrices, 73, 197
Boonyavatana, R., 139
border, 35, 104
bordered, 222
bordered word, 35
Borges, M., xi
Borwein, P., 48
Boyd, D., 223
Brady, A. H., 200
branch point, 113
Brandenburg, F.-J., 139
Brauer, W., 106
Brzozowski, J., 27, 106
Bucher, W., 138
Buntrock, J., 200
Burnside problem for groups, 42
Buss, J., xi, 223
busy beaver problem, 183, 184, 200
calliope, 3
Cantor, D. C., 201
Cantor set, 102
cardinality, 1
census generating function, 133
Cerny’s conjecture, 105
Chaitin, G. J., 200
characteristic sequence, 28
233

234
Index
chess, 41
German rule in, 42
laws of, 41
Chomsky hierarchy, 212
Chomsky, N., 27, 201, 212, 223
Chomsky normal form, 10
clickomania, 135
closed, 7
closure properties, 7
co-CFL, 132
Cocke, J., 141
cold, 3
commutative image, 122
comparable, 92
complement
of a set, 1
complete item, 145
complex, 178
complexity class, 19
compression, 178
optimal, 179
computing model
robust, 66
concatenation
of languages, 3
of strings, 3
conﬁguration, 12, 14
conﬂict, 156
conjugate, 34, 44, 60, 137
conjunctive grammar, 222
conjunctive language, 222
conjunctive normal form, 20
context-free grammar, 8, 9
deﬁnition of, 9
pure, 133
context-free language
deﬁnition of, 10
context-free languages
Boolean closure of, 221
closed under inverse morphism, 109
closed under substitution, 108
context-sensitive grammar, 202
context-sensitive language, 202
Cook, S. A., 26, 223
Coppersmith, D., 106
Crochemore, M., 47
CSG, 202
CSL, 202
cube, 37
abelian, 47
cubefree, 37
cycle operation, 59
cyclic shift, 34, 60
CYK algorithm, 141, 142, 144
Damon, T., xi
Davis, M., 27
DCFL’s, 126
closure under complement, 128
decision problem
solvable, 17
deeded, 47
deiﬁed, 3
derivation, 10
derivation tree, 10
deterministic context-free languages,
126
deterministic ﬁnite automaton, 4
deﬁnition, 4
deterministic PDA, 126
DFA, 4
DiRamio, M., xi
directed graph, 73
Dirichlet’s theorem, 95
division-free sequence, 93
drawer, 3
Drobot, V., 173
Du, D.-Z., 139
Earley, J., 173
Earley’s parsing method, 144
Eggan, L. C., 105
Ehrig, H., 138
Ellul, K., xi
empty stack
acceptance by in PDA, 12
empty string, 2
Engels, G., 138
enlightenment, 2
entanglement, 34
ϵ-production, 10
ϵ-transition, 6
equivalence classes, 77
equivalence relation, 77
index, 77
Myhill-Nerode, 78
reﬁnement of, 77
right invariant, 78
Euwe, M., 42, 48
exponent
of a group, 43
extended regular expression, 23

Index
235
factor, 27
factoring, 162
Fermat’s last theorem, 36
Fernandes, S., xi
Fibonacci number, 132
ﬁnal state, 4
acceptance by in PDA, 12
Fine, N. J., 48
ﬁnite index, 77
ﬁnite-state transducer, 61, 102
Fischer, P. C., 139
Floyd, R. W., 201
FORTRAN, 133
fractional power, 34
Franz, J., xi
Friedman property, 100
full conﬁguration
of a 2DPDA, 214
function
partial, 14
Gabarro, J., 138
Garey, M. R., 27
Gaspar, C., xi
Gazdar, G., 138
general sequential machine,
106
German rule, 42
Ginsburg, S., 106, 138, 139, 201
Glaister, I., 107
Glover, M., xi
Golbeck, R., xi
Goldstine, J., 138, 139
Golod, D., xi
grammar, 8
Boolean, 223
conjunctive, 222
context-free, 8
context-sensitive, 202
length-increasing, 203
Type 0, 200
unrestricted, 174–176
Gray, J. N., 223
Greibach, S. A., 139
Gries, D., 106
group
exponent of, 43
inﬁnite periodic, 46
periodic, 43
torsion, 43
Gupta, N., 47
Hadley, R., xi
Haines, L. H., 107
Hall, M., 43
halting state, 14
Hanan, J., 222
handle, 164
Harju, T., 48, 106
Harrison, M. A., 107, 223
Hartmanis, J., 105
Hashiguchi, K., 105
high information content, 178
homomorphism, 29, 54
iterated, 29
Hopcroft, J. E., 26, 106, 173, 201, 223
Ibarra, O. H., 105, 223
immediate left recursion, 161
Immerman, N., 210, 223
incidence matrix, 73
incompressibility method, 180,
182
incompressible, 179
index
of an equivalence relation, 77
ﬁnite, 77
indistinguishable, 82
inductive counting, 210
inﬁnite sequence, 28
inﬁnite string, 28
inﬁnite word, 28
Ingalls, C., 48
inherently ambiguous, 114, 115
initial state, 5
input
accepted, 4
rejected, 4
integer
random, 181
interchange lemma, 118–121
invalid computation, 190
invariance theorem, 178
inverse morphism, 57
inverse substitution, 97, 135
alternate deﬁnition, 98, 135
Istead, J., xi
item, 163
complete, 145
Jiang, T., 105, 107
Johnson, D. S., 27
Jones, N. D., 223

236
Index
Kalbﬂeisch, R., xi
Kao, J.-Y., xi
Karhum¨aki, J., 48, 106
Karp reduction, 20
Kasami, T., 141
Keanie, A., xi
Kfoury, A. J., 47
Kleene closure, 4
Klein, E., 138
Knuth, D. E., 173
Knuth DFA, 164
Knuth NFA-ϵ, 165
Ko, K.-I., 139
Kolmogorov complexity, 177, 178
uncomputable, 179
Koo, A., xi
Kozen, D., 105
Kreowski, H.-J., 138
Krieger, D., xi
Kuroda, S. Y., 223
Landry, D., xi
language, 2
conjunctive, 222
context-free, 10
inherently ambiguous, 115
preﬁx, 3, 54, 96
recursive, 15
recursively enumerable, 15
regular, 4
sufﬁx, 3
languages, 3
product of, 3
Laplace, P. S., 177
Lawson, M. V., 106
LBA, 206
deterministic, 223
leading zeros
removing, 54, 62
Lee, J., xi
left quotient, 106
leftmost derivation, 10
length-increasing grammar, 203
letters, 1
Levi’s lemma, 30
Lewis, H. R., 26
lexicographic order, 3
lexicographically median, 100
Li, M., 200
Ligocki, S., 201
Ligocki, T. J., 201
Lin, S., 200
Lindenmayer, A., 222
linear, 122
linear bounded automaton, 206
linear grammar, 130
linear languages, 130
pumping lemma for, 131
Linster, B. G., 105
Litt, A., xi
Liu, L. Y., 138
log(L), 76
Lorentz, R. J., 200
Lothaire, M., 48, 107
LR(0) grammar
deﬁnition of, 167
Lucier, B., xi
Lutz, B., xi
Lyndon, R. C., 30, 48
Lyndon–Sch¨utzenberger theorems, 30, 31
MacDonald, I., xi
Main, M. G., 138
Martin, A., xi
Martin, G., xi
Martin, J. C., 26, 139
Martinez, A., xi
Marxen, H., 200, 201
Maslov, A. N., 107
matrices
Boolean, 73, 197
matrix multiplication, 75
Maurer, H. A., 138
McCulloch, W. S., 27
McNaughton, R., 105, 106
Mealy, G. H., 106
Mealy machine, 49
Mehta, G., xi
Miltchman, O., xi
minimal
in partially ordered set, 94
minimal word, 133
mirror invariant, 45
M¨obius function, 46
Montanari, U., 138
Moore, E. F., 106
Moore machine, 49
moose, 3
antlers, 29
morphism, 29, 54
convention when deﬁning, 29
iterated, 29

Index
237
nonerasing, 104
overlap-free, 43
Morse, M., 38
Motwani, R., 26
Moura, A., 138
multigrade problem, 40
murmur, 104
Myhill, J., 106
Myhill–Nerodeequivalence relation,
78
Myhill–Nerodetheorem, 77
n! trick, 114
Nazari, S., xi
Nerode, A., 106
Nevraumont, A. F., 200
NFA, 6
ambiguous, 102
NFA-ϵ, 6
Ng, S., xi
Nguyen, L., xi
Nichols, M., xi
Nivat’s theorem
for transducers, 63
nonconstructive, 54
nondeterministic ﬁnite automaton, 6
nondeterministic state complexity, 90
nonerasing morphism, 104
nonpalindromes, 9
nontrivial
preﬁx, 2
sufﬁx, 2
Nørg˚ard, P., 42
normal form for transducers, 64
Novikov, P. S., 43
Nowotka, D., 48
Nozaki, A., 106
NP-complete, 20
NP (nondeterministic polynomial time),
20
NPSPACE, 21
ODDPAL, 9
Ogden, W., 138, 139
Ogden’s lemma, 112
example of, 114
Okhotin, A., 222, 223
ω-language, 47
order
lexicographic, 3
radix, 3
outshout, 35
overlap-free, 37, 39
morphism, 43
P (polynomial time complexity class), 19
Palacios, A., xi
palindrome, 3
palindromic closure, 104
Panini, 27
Papadimitriou, C. H., 26, 27
Papert, S., 106
Parikh map, 122
Parikh, R. J., 139
parse tree, 10
partial conﬁguration
of a 2DPDA, 218
partial function, 14
partial order, 92
pattern, 104, 199
pattern matching, 199
PCF grammar, 133
PCP, 186
PDA
acceptance by empty stack, 12
acceptance by ﬁnal state, 12
deterministic, 126
perfect shufﬂe, 3, 120
period, 29
periodic
purely, 29
ultimately, 29
Perles, M., 201
permutations, 97
Perrin, D., 47, 48
photograph, 35
π, 43
Pilling, D. L., 139
Pin, J.-E., 47, 48, 105
Pitts, W., 27
positive closure, 4, 103
Post correspondence problem, 186, 201
Post, E., 201
power, 33
power set, 1
preﬁx, 2
nontrivial, 2
proper, 2
preﬁx language, 3, 54, 96
preﬁx-free encoding, 181
preperiod, 29
Price, J. K., 138

238
Index
prime number theorem, 180
primes
in unary, 7
PRIMES2 (primes in base 2), 2, 8, 17
irregularity of, 7
primitive
root, 33
string, 33
primitive strings, 222
enumeration of, 46
primitive word, 138
proper preﬁx, 2
proper sufﬁx, 2
Prouhet, ´E, 40
Prusinkiewicz, P., 222
PSPACE, 21
Pullum, G., 138
pure context-free grammar, 133
purely periodic, 29
pushdown
automaton, 11
pushdown store, 11
quotient
of languages, 52
Rabin, M. O., 26, 27
radar, 3
radix order, 3
Rado, T., 200
Rampersad, N., xi
random, 178, 179
random integer, 181
rat, 3
rational relation, 63
Ravikumar, B., 107
reappear, 47
Reardon, J., xi
recurrent, 45
recursive, 15
recursively enumerable, 15
redivider, 3
reﬁnement, 77–79, 82, 84
reﬂexive, 92
regular expressions, 4
regular language, 4
regularity-preserving transformation, 105
relation, 77
equivalence, 77
rational, 63
removing leading zeros, 54, 62
removing trailing zeros, 54
repaper, 3
reversal, 3
reward, 3
Rice, H. G., 138
Rideout, J., xi, 200
right invariant, 78
right quotient, 106
right sentential form, 164
rightmost derivation, 10
Robinson, R., xi
Robson, J. M., 105
robustness, 66
Rose, G. F., 201
Rose, K., xi
Rosenkrantz, D. J., 173
Ross, R. J., 138, 139
rotator, 3
Rozenberg, G., 138
Rubinstein, A., 105
Sag, I., 138
Salomaa, K., 107
Sanov, I. N., 43
Santean, N., xi
SAT, 20
satisfying assignment, 20
Savitch’s theorem, 196
Sch¨utzenberger, M. P., 30, 48, 139, 201
Scott, D., 26, 27
Seiferas, J. I., 105, 106
semilinear, 122
sentence, 1
sentential form, 9
sequence
inﬁnite, 28
sequential transducer, 106
Sethi, R., 173
Shallit, J., 48, 105, 107
Shamir, E., 201
Shepherdson, J. C., 106
Shieber, S. M., 138
shift-reduce parsing, 168
shoe, 3
shufﬂe, 57, 97, 132
perfect, 3, 120
Shyr, H. J., 48
Slutzki, G., 139
Socha, J., xi
Solomonoff, R., 200
solvable, 17

Index
239
Spanier, E. H., 106, 138, 139
sparse, 101
square, 37
abelian, 47
squarefree, 37, 39, 45
squarefree-preserving, 121, 136
stack, 11
star height, 99
start state, 5
start symbol, 9
state complexity
nondeterministic, 90
Stearns, R. E., 105, 173
Stebila, D., xi
Strachey, C., 27
string, 1
inﬁnite, 28
length of, 2
primitive, 33, 222
right-inﬁnite, 28
strings, 2
subsequence, 2, 27, 92, 93, 100, 134
subsequence ordering, 92
subsequential transducer, 106
substitution, 55
inverse, 97, 135
by regular languages, 55
subword, 2, 27
subword ordering, 92
subword-doubling closure, 99
subword-doubling map, 99
sufﬁx, 2
nontrivial, 2
proper, 2
sufﬁx language, 3
Sun, W., xi
supersequence, 93
symbols, 1
synchronizing automaton, 105
Szelepcs´enyi, R., 210, 223
Tarry–Escottproblem, 40, 48
terminator, 218
Thierrin, G., 48
Thomas, W., 47
Thornton, R. J., 27
Thue, A., 38, 48
Thue–Morsemorphism, 30, 44
Thue–Morsesequence, 38, 40–42, 45, 48
music and, 42
Thue–Morseword, 137
Tikuisis, A., xi
time complexity, 19
TM, 13
trailing zeros
removing, 54
transducer
ﬁnite-state, 61, 102
normal form for, 64
sequential, 106
subsequential, 106
transduction, 63
transition diagram, 5
transition function, 4
extended, 5
transitive, 92
triple construction, 116
Tromp, J., 200
Turing, A. M., 26, 27
Turing machine, 13, 14
always-halting, 209
computing a function, 16
Turing reduction, 18
two-way ﬁnite automaton, 66
Type 0 grammar, 200, 212
Type 1 grammar, 212
Type 2 grammar, 212
Type 3 grammar, 212
Ullman, J. D., 26, 106, 173, 201,
223
ultimately periodic, 29
unambiguous, 10
unary alphabet, 1
unary context-free language, 111
unbalanced word, 137
unbordered, 134
unbordered word, 35
uncomputable, 27
undecidable, 27
uneven word, 45
unit productions, 10
universality problem, 193, 197
unrestricted grammar, 174–176
unsolvable, 27
Urbanek, F., 173
useless symbol, 24
Valiant, L., 140, 173
valid
for viable preﬁx, 164
valid computation, 190

240
Index
viable preﬁx, 164
Vit´anyi, P., 200
Wallace, J., xi
Wang, X., xi
Weiner, P., 138
Wilf, H. S., 48
Winklmann, K., 138, 139
Winograd, S., 106
Wolfram, S., 105
Wood, D., 106
Woods, D. R., 48
word, 1
balanced, 137
bordered, 35, 104
inﬁnite, 28
primitive, 138
unbalanced, 137
unbordered, 35, 134
uneven, 45
Wotschke, D., 138
Wright, E. M., 48
Xu, Z., xi
yield, 10
Young, C., xi
Younger, D., 141
Younger, D. H., 173
Yu, S., 106, 107
Zhang, G.-Q., 106
Zhang, N., xi
Zhao, L, 200
Zhuang, Q., 107
Zimny, B., xi


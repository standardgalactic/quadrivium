


 

Library of Congress Cataloging-in-Publication Data
Names: Dalton, Lori A., author. | Dougherty, Edward R., author.
Title: Optimal Bayesian classification / Lori A. Dalton and Edward R. Dougherty.
Description: Bellingham, Washington : SPIE Press, [2019] | Includes bibliographical
references and index.
Identifiers: LCCN 2019028901 (print) | LCCN 2019028902 (ebook) | ISBN
9781510630697 (paperback) | ISBN 9781510630710 (pdf) | ISBN 9781510630703
(epub) | ISBN 9781510630727 (kindle edition)
Subjects: LCSH: Bayesian statistical decision theory. | Statistical decision.
Classification: LCC QA279.5 D355 2020 (print) | LCC QA279.5 (ebook) |
DDC 519.5/42–dc23
LC record available at https://lccn.loc.gov/2019028901
LC ebook record available at https://lccn.loc.gov/2019028902
Published by
SPIE
P.O. Box 10
Bellingham, Washington 98227-0010 USA
Phone: +1 360.676.3290
Fax: +1 360.647.1445
Email: books@spie.org
Web: http://spie.org
Copyright © 2020 Society of Photo-Optical Instrumentation Engineers (SPIE)
All rights reserved. No part of this publication may be reproduced or distributed
in any form or by any means without written permission of the publisher.
The content of this book reflects the work and thought of the author. Every effort
has been made to publish reliable and accurate information herein, but the
publisher is not responsible for the validity of the information or for any
outcomes resulting from reliance thereon.
Cover photographs: Matt Anderson Photography/Moment via Getty Images and
Jackie Niam/iStock via Getty Images.
Printed in the United States of America.
First Printing.
For updates to this book, visit http://spie.org and type “PM310” in the search field.

To our spouses,
Yousef and Terry,
For being our companions and sources of support and encouragement.


Contents
Preface
xi
Acknowledgments
xv
1
Classification and Error Estimation
1
1.1
Classifiers
1
1.2
Constrained Classifiers
5
1.3
Error Estimation
11
1.4
Random Versus Separate Sampling
16
1.5
Epistemology and Validity
18
1.5.1
RMS bounds
20
1.5.2
Error RMS in the Gaussian model
22
2
Optimal Bayesian Error Estimation
25
2.1
The Bayesian MMSE Error Estimator
25
2.2
Evaluation of the Bayesian MMSE Error Estimator
32
2.3
Performance Evaluation at a Fixed Point
33
2.4
Discrete Model
36
2.4.1
Representation of the Bayesian MMSE error estimator
37
2.4.2
Performance and robustness in the discrete model
38
2.5
Gaussian Model
46
2.5.1
Independent covariance model
47
2.5.2
Homoscedastic covariance model
50
2.5.3
Effective class-conditional densities
53
2.5.4
Bayesian MMSE error estimator for linear classification
61
2.6
Performance in the Gaussian Model with LDA
65
2.6.1
Fixed circular Gaussian distributions
65
2.6.2
Robustness to falsely assuming identity covariances
68
2.6.3
Robustness to falsely assuming Gaussianity
69
2.6.4
Average performance under proper priors
75
2.7
Consistency of Bayesian Error Estimation
76
2.7.1
Convergence of posteriors
77
2.7.2
Sufficient conditions for consistency
79
2.7.3
Discrete and Gaussian models
82
vii

2.8
Calibration
85
2.8.1
MMSE calibration function
85
2.8.2
Performance with LDA
88
2.9
Optimal Bayesian ROC-based Analysis
91
2.9.1
Bayesian MMSE FPR and TPR estimation
92
2.9.2
Bayesian MMSE ROC and AUC estimation
93
2.9.3
Performance study
95
3
Sample-Conditioned MSE of Error Estimation
101
3.1
Conditional MSE of Error Estimators
101
3.2
Evaluation of the Conditional MSE
104
3.3
Discrete Model
106
3.4
Gaussian Model
110
3.4.1
Effective joint class-conditional densities
110
3.4.2
Sample-conditioned MSE for linear classification
117
3.4.3
Closed-form expressions for functions I and R
129
3.5
Average Performance in the Gaussian Model
135
3.6
Convergence of the Sample-Conditioned MSE
136
3.7
A Performance Bound for the Discrete Model
137
3.8
Censored Sampling
140
3.8.1
Gaussian model
141
3.9
Asymptotic Approximation of the RMS
143
3.9.1
Bayesian–Kolmogorov asymptotic conditions
145
3.9.2
Conditional expectation
147
3.9.3
Unconditional expectation
153
3.9.4
Conditional second moments
155
3.9.5
Unconditional second moments
161
3.9.6
Unconditional MSE
164
4
Optimal Bayesian Classification
169
4.1
Optimal Operator Design Under Uncertainty
169
4.2
Optimal Bayesian Classifier
173
4.3
Discrete Model
174
4.4
Gaussian Model
176
4.4.1
Both covariances known
176
4.4.2
Both covariances diagonal
178
4.4.3
Both covariances scaled identity or general
179
4.4.4
Mixed covariance models
188
4.4.5
Average performance in the Gaussian model
190
4.5
Transformations of the Feature Space
195
4.6
Convergence of the Optimal Bayesian Classifier
196
4.7
Robustness in the Gaussian Model
203
4.7.1
Falsely assuming homoscedastic covariances
203
4.7.2
Falsely assuming the variance of the features
205
viii
Contents

4.7.3
Falsely assuming the mean of a class
206
4.7.4
Falsely assuming Gaussianity under Johnson distributions
208
4.8
Intrinsically Bayesian Robust Classifiers
210
4.9
Missing Values
212
4.9.1
Computation for application
219
4.10
Optimal Sampling
219
4.10.1
MOCU-based optimal experimental design
220
4.10.2
MOCU-based optimal sampling
222
4.11
OBC for Autoregressive Dependent Sampling
225
4.11.1
Prior and posterior distributions for VAR processes
225
4.11.2
OBC for VAR processes
228
5
Optimal Bayesian Risk-based Multi-class Classification
237
5.1
Bayes Decision Theory
238
5.2
Bayesian Risk Estimation
239
5.3
Optimal Bayesian Risk Classification
240
5.4
Sample-Conditioned MSE of Risk Estimation
245
5.5
Efficient Computation
246
5.6
Evaluation of Posterior Mixed Moments: Discrete Model
248
5.7
Evaluation of Posterior Mixed Moments: Gaussian Models
250
5.7.1
Known covariance
250
5.7.2
Homoscedastic general covariance
251
5.7.3
Independent general covariance
255
5.8
Simulations
256
6
Optimal Bayesian Transfer Learning
261
6.1
Joint Prior Distribution
261
6.2
Posterior Distribution in the Target Domain
265
6.3
Optimal Bayesian Transfer Learning Classifier
273
6.3.1
OBC in the target domain
276
6.4
OBTLC with Negative Binomial Distribution
281
7
Construction of Prior Distributions
287
7.1
Prior Construction Using Data from Discarded Features
288
7.2
Prior Knowledge from Stochastic Differential Equations
295
7.2.1
Binary classification of Gaussian processes
296
7.2.2
SDE prior knowledge in the BCGP model
299
7.3
Maximal Knowledge-Driven Information Prior
303
7.3.1
Conditional probabilistic constraints
307
7.3.2
Dirichlet prior distribution
309
7.4
REMLP for a Normal-Wishart Prior
312
7.4.1
Pathway knowledge
312
7.4.2
REMLP optimization
314
ix
Contents

7.4.3
Application of a normal-Wishart prior
315
7.4.4
Incorporating regulation types
318
7.4.5
A synthetic example
320
References
325
Index
341
x
Contents

Preface
The most basic problem of engineering is the design of optimal (or close-to-
optimal) operators. The design of optimal operators takes different forms
depending on the random process constituting the scientific model and the
operator class of interest. The operators might be filters, controllers, or
classifiers, each having numerous domains of application. The underlying
random process might be a random signal/image for filtering, a Markov
process for control, or a feature-label distribution for classification. Here we
are interested in classification, and an optimal operator is a Bayes classifier,
which is a classifier minimizing the classification error.
With sufficient knowledge we can construct the feature-label distribution
and thereby find a Bayes classifier. Rarely, and in practice virtually never, do
we possess such knowledge. On the other hand, if we had unlimited data, we
could accurately estimate the feature-label distribution and obtain a Bayes
classifier. Rarely do we possess sufficient data. Therefore, we must use
whatever knowledge and data are available to design a classifier whose
performance is hopefully close to that of a Bayes classifier.
Classification theory has historically developed mainly on the side of data,
the classical case being linear discriminant analysis (LDA), where a Bayes
classifier is deduced from a Gaussian model and the parameters of the
classifier are estimated from data via maximum-likelihood estimation. The
idea is that we have knowledge that the true feature-label distribution is
Gaussian (or close to Gaussian) and the data can fill in the parameters, in this
case, the mean vectors and common co-variance matrix. Much contemporary
work takes an even less knowledge-driven approach by assuming some very
general classifier form such as a neural network and estimating the network
parameters by fitting the network to the data in some manner. The more
general the classifier form, the more parameters to determine, and the more
data needed. Moreover, there is growing danger of overfitting the classifier
form to the data as the classifier structure becomes more complex. Lack of
knowledge presents us with model uncertainty, and hypothesizing a classifier
form and then estimating the parameters is an ad hoc way of dealing with that
uncertainty. It is ad hoc because the designer postulates a classification rule
based on some heuristics and then applies the rule to the data.
xi

This book takes a Bayesian approach to modeling the feature-label
distribution and designs an optimal classifier relative to a posterior
distribution governing an uncertainty class of feature-label distributions. In
this way it takes full advantage of knowledge regarding the underlying system
and the available data. Its origins lie in the need to estimate classifier error
when there is insufficient data to hold out test data, in which case an optimal
error estimate can be obtained relative to the uncertainty class. A natural next
step is to forgo classical ad hoc classifier design and simply find an optimal
classifier relative to the posterior distribution over the uncertainty class—this
being an optimal Bayesian classifier.
A critical point is that, in general, for optimal operator design, the prior
distribution is not on the parameters of the operator (controller, filter,
classifier), but on the unknown parameters of the scientific model, which for
classification is the feature-label distribution. If the model were known with
certainty, then one would optimize with respect to the known model; if the
model is uncertain, then the optimization is naturally extended to include
model uncertainty and the prior distribution on that uncertainty. Model
uncertainty induces uncertainty on the operator parameters, and the
distribution of the latter uncertainty follows from the prior distribution on
the model. If one places the prior directly on the operator parameters while
ignoring model uncertainty, then there is a scientific gap, meaning that the
relation between scientific knowledge and operator design is broken.
The first chapter reviews the basics of classification and error estimation.
It addresses the issue that confronts much of contemporary science and
engineering: How do we characterize validity when data are insufficient for
the complexity of the problem? In particular, what can be said regarding the
accuracy of an error estimate? This is the most fundamental question for
classification since the error estimate characterizes the predictive capacity of a
classifier.
Chapter 2 develops the theory of optimal Bayesian error estimation: What
is the best estimate of classifier error given our knowledge and the data? It
introduces what is perhaps the most important concept in the book: effective
class-conditional densities. Optimal classifier design and error estimation for a
particular feature-label distribution are based on the class-conditional
densities. In the context of an uncertainty class of class-conditional densities,
the key role is played by the effective class-conditional densities, which are the
expected densities relative to the posterior distribution. The Bayesian
minimum-mean-square error (MMSE) theory is developed for the discrete
multinomial model and several Gaussian models. Sufficient conditions for
error-estimation consistency are provided. The chapter closes with a
discussion of optimal Bayesian ROC estimation.
Chapter 3 addresses error-estimation accuracy. In the typical ad hoc
classification paradigm, there is no way to address the accuracy of a particular
xii
Preface

error estimate. We can only quantify the mean-square error (MSE) of error
estimation relative to the sampling distribution. With Bayesian MMSE error
estimation, we can compute the MSE of the error estimate conditioned on the
actual sample relative to the uncertainty class. The sample-conditioned MSE
is studied in the discrete and Gaussian models, and its consistency is
established. Because a running MSE calculation can be performed as new
sample points are collected, one can do censored sampling: stop sampling
when the error estimate and MSE of the error estimate are sufficiently small.
Section 3.9 provides double-asymptotic approximations of the first and
second moments of the Bayesian MMSE error estimate relative to the
sampling distribution and the uncertainty class, thereby providing asymptotic
approximation to the MSE, or, its square root, the root-mean-square (RMS)
error. Double asymptotic convergence means that both the sample size and
the dimension of the space increase to infinity at a fixed rate between the two.
Even though we omit many theoretical details (referring instead to the
literature), this section is rather long, on account of double asymptotics, and
contains many complicated equations. Nevertheless, it provides an instructive
analysis of the relationship between the conditional and unconditional RMS.
Regarding the chapter as a whole, it is not a logical prerequisite for succeeding
chapters and can be skipped by those wishing to move directly to
classification.
Chapter 4 defines an optimal Bayesian classifier as one possessing
minimum expected error relative to the uncertainty class, this expectation
agreeing with the Bayesian MMSE error estimate. Optimal Bayesian
classifiers are developed for a discrete model and several Gaussian models,
and convergence to a Bayes classifier for the true feature-label distribution is
studied. The robustness of assumptions on the prior distribution is discussed.
The chapter has a section on intrinsically Bayesian robust classification, which
is equivalent to optimal Bayesian classification with a null dataset. It next has
a section showing how missing values in the data are incorporated into the
overall optimization without having to implement an intermediate imputation
step, which would cause a loss of optimality. The chapter closes with two
sections in which sampling is not random. Section 4.10 considers optimal
sampling, and Section 4.11 examines the effect of dependent sampling.
Chapter 5 extends the theory to multi-class classification via optimal
Bayesian risk classification. It includes evaluation of the sample-conditioned
MSE of risk estimation and evaluation of the posterior mixed moments for
both the discrete and Gaussian models.
Chapter 6 extends the multi-class theory to transfer learning. Here, there
are data from a different (source) feature-label distribution, and one wishes to
use this data together with whatever data are available from the (target)
feature-label distribution of interest. The source and target are linked via a
joint prior distribution, and an optimal Bayesian transfer learning classifier is
xiii
Preface

derived for the posterior distribution in the target domain. Both Gaussian and
negative binomial distributions are considered.
The final chapter addresses the fundamental problem of prior construc-
tion: How do we transform prior knowledge into a prior distribution? The
first two sections address special cases: using data from discarded features,
and using knowledge from a partially known physical system. The heart of the
chapter is the development of a general method for transforming scientific
knowledge into a prior distribution by performing an information-theoretic
optimization over a class of potential priors with the optimization constrained
by a set of conditional probability statements characterizing our scientific
knowledge.
In a sense, this book is the last of a trilogy. The Evolution of Scientific
Knowledge: From Certainty to Uncertainty (Dougherty, 2016) traces the
epistemology of modern science from its deterministic beginnings in the
Seventeenth century up through the inherent stochasticity of quantum theory
in the first half of the Twentieth century, and then to the uncertainty in
scientific models that has become commonplace in the latter part of the
Twentieth century. This uncertainty leads to an inability to validate physical
models, thereby limiting the scope of valid science. The last chapter of the
book presents, from a philosophical perspective, the structure of operator
design in the context of model uncertainty. Optimal Signal Processing Under
Uncertainty (Dougherty, 2018) develops the mathematical theory articulated
in that last chapter, applying it to filtering, control, classification, clustering,
and experimental design. In this book, we extensively develop the classifica-
tion theory summarized in that book.
Edward R. Dougherty
Lori A. Dalton
December 2019
xiv
Preface

Acknowledgments
The material in this book has been developed over a number of years and
involves the contributions of numerous students and colleagues, whom we
would like to acknowledge: Mohammadmahdi Rezaei Yousefi, Amin
Zollanvari, Mohammad Shahrokh Esfahani, Shahin Boluki, Alireza Karba-
layghareh, Siamak Zamani Dadaneh, Roozbeh Dehghannasiri, Xiaoning
Qian, Byung-Jun Yoon, and Ulisses Braga-Neto. We also extend our
appreciation to Dara Burrows for her careful editing of the book.
xv

Chapter 1
Classification
and Error Estimation
A classifier operates on a vector of features, which are random variables, and
outputs a decision as to which class the feature vector belongs. We consider
binary classification, meaning that the decision is between two classes.
Typically, a classifier is designed and its error estimated from sample data.
Two basic questions arise. First, given a set of features, how does one design a
classifier from the sample data that provides good classification over the
general population? Second, how does one estimate the error of a designed
classifier from the data? This book examines both classifier design and error
estimation when one is given prior knowledge regarding the population. The
first chapter provides basic background knowledge.
1.1 Classifiers
Classification involves a feature vector X ¼ ½X 1, X 2, : : : , X DT composed
of random variables (features) on a D-dimensional Euclidean space X ¼ RD, a
binary random variable (label) Y, and a classifier c : RD →f0, 1g to predict
Y ∈{0, 1}. We assume a joint feature-label distribution f X,Yðx, yÞ for
the random vector–label pair ðX, YÞ. The feature-label distribution char-
acterizes the classification problem and may be a generalized function.
f XjYðxj0Þ and f XjYðxj1Þ, the distributions of X given Y ¼ 0 and Y ¼ 1,
respectively, are known as the class-conditional distributions. The classification
error is relative to the feature-label distribution and equals the probability of
incorrect classification Pr ðcðXÞ ≠YÞ. The error also equals the expected
(mean)
absolute
difference
between
the
label
and
the
classification
E½jY  cðXÞj. An optimal classifier is one having minimal error among all
classifiers c : RD →f0, 1g. An optimal classifier cBayes is called a Bayes
classifier and its error εBayes is called the Bayes error. The Bayes error is intrinsic
to the feature-label distribution; however, there may be more than one Bayes
classifier.
1

The error of an arbitrary classifier c can be expressed as
ε ¼
Z
X
PrðcðXÞ ≠YjX ¼ xÞf XðxÞdx
¼
Z
cðxÞ¼0
hðxÞf XðxÞdx þ
Z
cðxÞ¼1
½1  hðxÞ f XðxÞdx,
(1.1)
where f XðxÞ is the marginal density of X, and hðxÞ ¼ Pr ðY ¼ 1jX ¼ xÞ is the
posterior
probability
that
Y ¼ 1.
The
posterior
distribution
of
Y
is
proportional to the product of the prior distribution of Y and the class-
conditional density for x via Bayes’ theorem:
Pr ðY ¼ yjX ¼ xÞ ¼ Pr ðY ¼ yÞf XjYðxjyÞ
f XðxÞ
.
(1.2)
Hence, the error is decomposed as
ε ¼ cε0 þ ð1  cÞε1,
(1.3)
where c ¼ PrðY ¼ 0Þ is the a priori probability of class 0,
ε0 ¼ PrðcðXÞ ¼ 1jY ¼ 0Þ
¼
Z
cðxÞ¼1
f XjYðxj0Þdx
(1.4)
is the probability of an element from class 0 being wrongly classified (the error
contributed by class 0), and, similarly, ε1 ¼ PrðcðXÞ ¼ 0jY ¼ 1Þ.
Since 0 ≤hðxÞ ≤1, the right-hand side of Eq. 1.1 is minimized by the
classifier
cBayesðxÞ ¼
 0
if hðxÞ ≤0.5,
1
otherwise,
¼
 0
if cf XjYðxj0Þ ≥ð1  cÞf XjYðxj1Þ,
1
otherwise.
(1.5)
Hence, the Bayes classifier cBayesðxÞ is defined to be 0 or 1 according to
whether Y is more likely to be 0 or 1 given x (ties may be broken arbitrarily).
It follows from Eqs. 1.1 and 1.5 that the Bayes error is given by
εBayes ¼
Z
hðxÞ≤0.5
hðxÞf XðxÞdx þ
Z
hðxÞ.0.5
½1  hðxÞ f XðxÞdx.
(1.6)
In practice, the feature-label distribution is typically unknown, and a
classifier must be designed from sample data. We assume a dataset consisting
of n points,
2
Chapter 1

Sn ¼ fðx1, y1Þ, ðx2, y2Þ, : : : , ðxn, ynÞg,
(1.7)
which is a realization of vector–label pairs drawn from the feature-
label
distribution.
A
classification
rule
is
a
mapping
of
the
form
Cn : ½RD  f0, 1gn →F, where F is the family of {0, 1}-valued functions
on RD. (Without further mention, all functions are assumed to be
measurable.) The subscript n emphasizes that a classification rule is actually
a collection of classification rules depending on n. Given a sample Sn, we
obtain a designed classifier cn ¼ CnðSnÞ ∈F according to the rule Cn.
Although perhaps it would be more complete to write cnðx; SnÞ rather than
cnðxÞ, we use the simpler notation, keeping in mind that cn has been trained
from a sample. Note that, relative to the sample, cn is a random function
depending on the sampling distribution generating the sample. For a particular
sample (dataset), cn is simply a binary function on RD. As a binary function,
cn is determined by a decision boundary between two regions.
Letting εn denote the error of a designed classifier cn, relative to the Bayes
error, there is a design cost Dn ¼ εn  εBayes, where εn and Dn are sample-
dependent random variables. The expected design cost is E½Dn, and the
expected error of cn is decomposed as
E½εn ¼ εBayes þ E½Dn.
(1.8)
The expectations E½εn and E½Dn are relative to the sampling distribution, and
these quantities measure the performance of the classification rule relative to
the feature-label distribution, rather than the performance of an individual
designed classifier. Nonetheless, a classification rule for which E½εn is small
will also tend to produce designed classifiers possessing small error.
Asymptotic properties of a classification rule concern large samples
(as n →`). A rule is said to be consistent for a distribution of ðX, YÞ if Dn →0
in probability, meaning that PrðjDnj . dÞ →0 as n →` for any d > 0.
A classification rule is universally consistent if Dn →0 in probability for any
distribution of ðX, YÞ. By Markov’s inequality, consistency is assured if
E½Dn →0.
A classification rule is said to be strongly consistent for a distribution of
ðX, YÞ if Dn →0 almost surely (a.s.) as n →`, meaning that Dn →0 with
probability 1. Specifically, Dn →0 as n →` for all sequences of datasets of
increasing size fS1, S2, : : : g, except on a set of sequences with probability 0.
Strong consistency implies consistency. A rule is universally strongly
consistent if Dn →0 almost surely for any distribution of ðX, YÞ.
To illustrate consistency, suppose that hnðxÞ is an estimate of hðxÞ based
on a sample Sn. Then the plug-in classification rule is defined by replacing hðxÞ
with hnðxÞ in Eq. 1.5. For this rule,
3
Classification and Error Estimation

Dn ¼ εn  εBayes
¼
Z
cnðxÞ¼0, cBayesðxÞ¼1
½2hðxÞ  1 f XðxÞdx
þ
Z
cnðxÞ¼1, cBayesðxÞ¼0
½1  2hðxÞ f XðxÞdx
¼
Z
cnðxÞ≠cBayesðxÞ
j2hðxÞ  1jf XðxÞdx.
(1.9)
Since |hðxÞ  0.5| ≤|hðxÞ  hnðxÞ| when cnðxÞ ≠cBayesðxÞ,
E½Dn ≤2E½jhðXÞ  hnðXÞj.
(1.10)
Applying the Cauchy–Schwarz inequality yields
E½Dn ≤2E½jhðXÞ  hnðXÞj2
1
2.
(1.11)
Hence, if hnðXÞ converges to hðXÞ in the mean-square sense, then the
plug-in classifier is consistent. In fact, E½Dn converges much faster than
does E½jhðXÞ  hnðXÞj21∕2, which in turn shows that classification is easier
than density estimation, in the sense that it requires less data. Specifically,
if E½jhðXÞ  hnðXÞj2 →0 as n →`, then for the plug-in rule (Devroye et al.,
1996),
lim
n→`
E½Dn
E½jhðXÞ  hnðXÞj2
1
2 ¼ 0.
(1.12)
As another example of consistency, we consider the cubic histogram rule,
in which RD is partitioned into hypercubes of side length rn. For each point x
in RD, cnðxÞ is defined to be 0 or 1 according to which is the majority among
the labels for points in the cube containing x. If the cubes are defined such that
rn →0 and nrDn →` as n →`, then the rule is universally consistent (Gordon
and Olshen, 1978; Devroye et al., 1996).
Lastly,
for
the
k-nearest-neighbor
(kNN)
rule
with
k
odd,
the
k points closest to x are selected, and cnðxÞ is defined to be 0 or 1 according
to which is the majority among the labels of these points. For a fixed k,
limn→` E½Dn ≤ðkeÞ1∕2
(Devroye
et
al.,
1996),
which
does
not
give
consistency. However, if k →` and k∕n →0 as n →`, then the kNN rule is
universally consistent (Stone, 1977).
While universal consistency is an appealing property, its practical
application is dubious. For instance, the expected design cost E½Dn cannot
be made universally small for fixed n. Indeed, for any t > 0, fixed n, and
designed classifier cn, there exists a distribution of ðX, YÞ such that εBayes ¼ 0
4
Chapter 1

and E½εn . 0.5  t (Devroye, 1982). Moreover, even if a classifier is
universally consistent, the rate at which E½Dn →0 is critical to application.
If we desire a classifier whose expected error is within some tolerance of the
Bayes error, consistency is not sufficient. Rather, we would like a statement of
the following form: for any t > 0, there exists nðtÞ such that, for n.nðtÞ,
E½Dn,t for any distribution of ðX, YÞ. Unfortunately, even if a classification
rule is universally consistent, the design error converges to 0 arbitrarily slowly
relative to all possible distributions. To wit, if {an} is a decreasing sequence
such that 1/16 ≥a1 ≥a2 ≥⋯> 0, then for any sequence of designed classifiers
cn, there exists a distribution of ðX, YÞ such that εBayes ¼ 0 and E½εn . an
(Devroye, 1982).
More generally, consistency is of little consequence for small-sample
classifier design, which is a key focus of the current text.
1.2 Constrained Classifiers
A common problem with small-sample design is that E[Dn] tends to be large.
A classification rule may yield a classifier that performs well on the sample
data. However, if the small sample does not represent the distribution
sufficiently, then the designed classifier will not perform well on the
distribution. Constraining classifier design means restricting the functions
from which a classifier can be chosen to a class C. Constraining the classifier
can reduce the expected design error, but at the cost of increasing the error of
the best possible classifier. Since optimization in C is over a subclass of
classifiers, the error εC of an optimal classifier cC ∈C will typically exceed the
Bayes error, unless a Bayes classifier happens to be in C. We call
DC ¼ εC  εBayes the cost of constraint. A classification rule yields a classifier
cn,C ∈C with error εn,C, where εn,C ≥εC ≥εBayes. Design error for constrained
classification is Dn,C ¼ εn,C  εC. The expected error of the designed classifier
from C can be decomposed as
E½εn,C ¼ εBayes þ DC þ E½Dn,C.
(1.13)
The constraint is beneficial if and only if the cost of constraint is less than the
decrease in expected design cost. The dilemma is that strong constraint
reduces E½Dn,C at the cost of increasing εC.
Historically, discriminant functions have played an important role in
classification. Keeping in mind that the logarithm is a strictly increasing
function, if we define the discriminant function by
dyðxÞ ¼ ln f XjYðxjyÞ þ ln Pr ðY ¼ yÞ,
(1.14)
then
the
misclassification
error
is
minimized
by
cBayesðxÞ ¼ y
if
dyðxÞ ≥djðxÞ for j ¼ 0, 1, or equivalently, cBayesðxÞ ¼ 0 if and only if
5
Classification and Error Estimation

g(x) ¼ d1(x)  d0(x) ≤0. We are particularly interested in quadratic discrimi-
nation, where for D  D matrix A, length D column vector a, and constant b,
gðxÞ ¼ xTAx þ aTx þ b,
(1.15)
where superscript T denotes matrix transpose, and the resulting classifier
produces quadratic decision boundaries. An important special case is linear
discrimination, for which
gðxÞ ¼ aTx þ b,
(1.16)
and the resulting classifier produces a hyperplane decision boundary.
If the conditional densities f XjYðxj0Þ and f XjYðxj1Þ are Gaussian with
positive definite covariance matrices, then
f XjYðxjyÞ ¼
1
ð2pÞ
D
2jSyj
1
2 exp

 1
2 ðx  myÞTS1
y ðx  myÞ

,
(1.17)
where my and Sy are the mean vector and covariance matrix for class
y ∈{0, 1}, respectively, and | ⋅| is the matrix determinant operator. The
covariance matrix of a multivariate Gaussian distribution must be positive
semi-definite, and it is positive definite unless one variable is a linear function
of the other variables. Conversely, every symmetric positive semi-definite
matrix is a covariance matrix. Recall that a symmetric real matrix S is positive
definite if xTSx > 0 for any nonzero vector x, and it is positive semi-definite if
xTSx ≥0. A symmetric matrix is positive definite if and only if all of its
eigenvalues are positive.
Inserting f XjYðxjyÞ into Eq. 1.14 and dropping the constant terms yields
dyðxÞ ¼  1
2 ðx  myÞTS1
y ðx  myÞ  1
2 ln jSyj þ ln Pr ðY ¼ yÞ.
(1.18)
Assuming that the Gaussian distributions are known, and denoting the class-0
probability by c ¼ PrðY ¼ 0Þ, the Bayes (optimal) classifier is given by the
quadratic discriminant
gBayesðxÞ ¼ xTABayesx þ aT
Bayesx þ bBayes,
(1.19)
with constant matrix ABayes, column vector aBayes, and scalar bBayes given by
ABayes ¼  1
2 ðS1
1
 S1
0 Þ,
(1.20)
aBayes ¼ S1
1 m1  S1
0 m0,
(1.21)
bBayes ¼  1
2 ðmT
1 S1
1 m1  m0
TS1
0 m0Þ þ 1
2 ln
jS0j
jS1j

þ ln
1  c
c

.
(1.22)
6
Chapter 1

If the classes possess a common covariance matrix S, then lnjSyj can also
be dropped from dyðxÞ. The Bayes classifier is now given by the linear
discriminant
gBayesðxÞ ¼ aT
Bayesx þ bBayes,
(1.23)
where
aBayes ¼ S1ðm1  m0Þ,
(1.24)
bBayes ¼  1
2 ðm1  m0ÞTS1ðm1 þ m0Þ þ ln
1  c
c

.
(1.25)
A Gaussian model is called heteroscedastic if the class covariance matrices are
unequal and homoscedastic if they are equal.
When
the
parameters
of
the
distributions
are
unknown,
plug-in
classification rules are formed by estimating the covariance matrices, mean
vectors, and prior class probabilities from the data by using the standard
sample estimators (sample covariance and sample mean). Quadratic discrimi-
nant analysis (QDA) substitutes the sample means for each class,
ˆmy ¼ 1
ny
X
ny
i¼1
xy
i ,
(1.26)
and the sample covariances for each class,
ˆSy ¼
1
ny  1
X
ny
i¼1
ðxy
i  ˆmyÞðxy
i  ˆmyÞT,
(1.27)
for the corresponding true parameters my and Sy, where ny is the number
of sample points in class y∈{0, 1}, and xy
1, : : : , xy
ny are the sample points in
class y. For the class probability c, QDA substitutes either a fixed value or a
maximum-likelihood estimate n0∕n. Linear discriminant analysis (LDA)
substitutes the sample means in the true means m0 and m1, the pooled sample
covariance
ˆS ¼ ðn0  1Þ ˆS0 þ ðn1  1Þ ˆS1
n  2
(1.28)
in the common true covariance S, and either a fixed value or estimated class-0
probability for the true class-0 probability c. If it is assumed that c ¼ 0.5, then
from Eq. 1.23 the LDA classifier produces cnðxÞ ¼ 0 if and only if WðxÞ ≤0,
with discriminant
7
Classification and Error Estimation

WðxÞ ¼ ð ˆm1  ˆm0ÞT ˆS1

x  ˆm1 þ ˆm0
2

.
(1.29)
WðxÞ is known as the Anderson W statistic (Anderson, 1951). Much of the
classical theory concerning LDA in the Gaussian model uses the Anderson W
statistic because it greatly simplifies analysis. Practically, it is reasonable to
use WðxÞ if there is good reason to believe that c  0.5, or when estimating c is
problematic and it is believed that c is not too far from 0.5.
If we further assume that S ¼ s2ID for some scalar s2, where ID is the
D  D identity matrix, then the Bayes classifier simplifies to
gBayesðxÞ ¼ ðm1  m0ÞT

x  m1 þ m0
2

þ s2 ln
1  c
c

.
(1.30)
The corresponding plug-in classification rule substitutes the sample means,
trð ˆSÞ∕D, and either a fixed value or an estimate of the class-0 probability for
the true parameters m0, m1, s2, and c, respectively, where trð⋅Þ denotes the
matrix trace operator. If it is assumed that c ¼ 0.5, then gBayes assigns each
point the class label with closer mean. The corresponding plug-in rule is the
nearest-mean-classification (NMC) rule, which substitutes the sample means
for the corresponding true parameters m0 and m1 to arrive at
gNMCðxÞ ¼ ð ˆm1  ˆm0ÞT

x  ˆm1 þ ˆm0
2

.
(1.31)
QDA and LDA can perform reasonably well so long as the class-
conditional densities are not too far from Gaussian and there are sufficient
data to obtain good parameter estimates. Owing to the greater number of
parameters (greater complexity) to be estimated for QDA as opposed to LDA,
one can proceed with smaller samples with LDA than with QDA.
We have defined QDA and LDA in two possible ways, plugging in a fixed
value for c, or employing the maximum-likelihood estimate ˆc ¼ n0∕n, the
default being the latter. For small samples, knowing c provides significant
benefit, the reason being that while ˆc →c in probability as n →` (by
Bernoulli’s weak law of large numbers), ˆc possesses significant variance for
small n. Indeed, for small n, as long as j0.5  cj is not too great, the Anderson
W statistic, which uses ˆc ¼ 0.5, can outperform the default LDA. These
considerations are illustrated in Fig. 1.1, which is based on Gaussian class-
conditional densities with class 0 having mean 0D ≡½0, 0, : : : , 0T, class 1
having mean 1D ≡½1, 1, : : : , 1T, and both classes having covariance matrix
ID, with D ¼ 3. In the figure, the horizontal and vertical axes give c and the
average true error, respectively. Parts (a) and (b) are for n ¼ 20 and n ¼ 50,
respectively, where each part shows the performance of LDA with c known,
8
Chapter 1

c estimated by n0/n (default LDA), and c substituted by 0.5 (Anderson W
statistic). The advantage of knowing c is evident, and this advantage is
nonnegligible for small n. Estimating c is sufficiently difficult with small
samples that in the case of n ¼ 20, for 0.38 ≤c ≤0.62, the Anderson W statistic
outperforms the default LDA.
The true error for any classifier c with linear discriminant gðxÞ ¼ aTx þ b
on Gaussian distributions with known means my and covariances Sy (S0 ≠S1
allowable) is given in closed form by placing
εy ¼ F
0
@ð1ÞygðmyÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSya
q
1
A
(1.32)
in Eq. 1.3 for y ¼ 0, 1, where F is the standard normal cumulative distribution
function (CDF) (Sitgreaves, 1961). Hence, the Bayes error when S ¼ S0 ¼ S1 is
given by
εBayes ¼ cεBayes,0 þ ð1  cÞεBayes,1,
(1.33)
where
εBayes,y ¼ F
0
@ð1ÞygBayesðmyÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aT
BayesSaBayes
q
1
A.
(1.34)
Generally speaking, the more complex a classifier class C, the smaller the
constraint cost and the greater the design cost. By this we mean that the more
finely the functions in C partition the feature space RD, the better functions
within it can approximate a Bayes classifier, and, concomitantly, the more
they can overfit the data. This notion can be illustrated via a celebrated
theorem that provides bounds for E½Dn,C. It concerns the empirical error
(a)
(b)
Figure 1.1
Average true error of LDA under Gaussian classes with respect to c:
(a) n ¼ 20; (b) n ¼ 50. [Reprinted from (Esfahani and Dougherty, 2013).]
9
Classification and Error Estimation

classification rule, which chooses the classifier in C that makes the least
number of errors on the sample data. For this rule,
E½Dn,C ≤4
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
V Clog2n þ 4
2n
r
,
(1.35)
where VC is the Vapnik–Chervonenkis (VC) dimension of C (Vapnik, 1971)
[see (Devroye et al., 1996) for the definition]. It is not our intent to pursue this
well-studied topic, but only to note that the VC dimension provides a measure
of classifier complexity and that n must greatly exceed VC for the preceding
bound to be small. The VC dimension of a linear classifier is D þ 1. For a
neural network with an even number of neurons k, the VC dimension has the
lower bound VC ≥Dk. If k is odd, then VC ≥Dðk  1Þ. Thus, the bound
exceeds 4
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
½Dðk  1Þlog2 n∕ð2nÞ
p
, which is not promising for small n.
A common source of complexity is the number of features on which a
classifier operates. This dimensionality problem motivates feature selection
when designing classifiers. In general, feature selection is part of the overall
classification rule and, relative to this rule, the number of variables is the
number in the data measurements, not the final number used in the designed
classifier. Feature selection results in a subfamily of the original family of
classifiers and thereby constitutes a form of constraint. For instance, if there
are D features available and LDA is used directly, then the classifier family
consists of all hyperplanes in a D-dimensional space, but if a feature-selection
algorithm reduces the number of variables to d < D prior to application
of LDA, then the classifier family consists of all hyperplanes in the
D-dimensional space that are confined to d-dimensional subspaces. Since the
role of feature selection is constraint, assessing the worth of feature selection
involves us in the standard dilemma: increasing constraint (decreasing the
number of features selected) incurs a cost but reduces design error.
Given a sample, in principle one could consider all feature sets of sizes 1
through D, design the classifier corresponding to each feature set, and choose
the feature set Dn,d of size d whose designed classifier cn,d has the smallest
error εn,d. The first problem with this exhaustive search is computational: too
many classifiers to design. Moreover, to select a subset of d features from a
set of D features with minimum error among all feature sets of size d, all
d-element subsets must be checked unless there is distributional knowledge that
mitigates the search requirement (Cover and Van Campenhout, 1977). Thus, a
full search cannot be avoided if we want to assure optimality. Second, since the
errors of all designed classifiers over all feature sets have to be estimated,
inaccurate estimation can lead to poor feature-set ranking—a problem
exacerbated with small samples (Sima et al., 2005). Numerous suboptimal
feature-selection algorithms have been proposed to address computational
10
Chapter 1

limitations. Those that do not employ error estimation are still impacted by the
need to estimate certain parameters or statistics in the selection process.
Now, consider a classification problem in which there are D potential
features, listed as x1, x2, . . . , xD, the feature-label distribution is known for
the full set of features, and a sequence of classifiers is designed by adding one
feature at a time. Let cd be the optimal classifier for feature-set size d and εd
be the true error of cd on the known distribution. Then, as features are added,
the optimal error is non-increasing, that is, εdþ1 ≤εd for d ¼ 1, 2, . . . , D  1.
However, if cn,d is trained from a sample of size n, then it is commonplace to
have the expected error (across the sampling distribution) decrease and then
increase for increasing d. This behavior is known as the peaking phenomenon
(Hughes, 1968; Hua et al., 2005). In fact, the situation can be more
complicated and depends on the classification rule and feature-label
distribution (Dougherty et al., 2009b; Hua et al., 2009). For instance, the
optimal number of features need not increase as the sample size increases.
Moreover, for fixed sample size the error curve need not be concave; for
instance, the expected error may decrease, increase, and then decrease again
as the feature-set size grows. Finally, if a feature-selection algorithm is
applied, then the matter becomes far more complicated (Sima and
Dougherty, 2008).
1.3 Error Estimation
Abstractly, any pair ðc, εÞ composed of a function c : RD →f0, 1g and a real
number ε ∈½0, 1 constitutes a classifier model, with ε being simply a number,
not necessarily specifying an actual error probability corresponding to c.
ðc, εÞ becomes a scientific model when it is understood in the context of a
feature-label distribution. A designed classifier produces a classifier model,
namely, ðcn, εnÞ. Relative to the sampling distribution generating the sample
Sn, ðcn, εnÞ consists of a random function cn ¼ CnðSnÞ and a corresponding
error εn. When applying a classification rule to a sample, we are not as
interested in the classifier it produces as in the error of the classifier. Since this
error is a random variable, it is characterized by its distribution. The first
exact expression for the expected error of LDA (the Anderson W statistic) in
the Gaussian model with a common known covariance was derived by
Sitgreaves in 1961 (Sitgreaves, 1961). In the same year, John obtained a
simpler expression as an infinite sum (John, 1961). Two years later, Okamoto
obtained an asymptotic expansion ðn →` Þ for the expected error in the same
model (Okamoto, 1968). Other asymptotic expansions followed, including an
asymptotic expansion for the variance by McLachlan in 1973 (McLachlan,
1973). In Section 1.5 we will consider “double asymptotic” expansions for the
expected error, where n →` and D →` such that D∕n →l for some positive
constant l (Raudys, 1967, 1972; Serdobolskii, 2000).
11
Classification and Error Estimation

Since εn depends on the feature-label distribution, which is unknown, εn is
unknown. It must be estimated via an error estimation rule Ξn. Thus, a sample
Sn yields a classifier cn ¼ CnðSnÞ and an error estimate ˆεn ¼ ΞnðSnÞ, which
together constitute a classifier model ðcn, ˆεnÞ. Overall, classifier design
involves a classification rule model ðCn, ΞnÞ used to determine a sample-
dependent classifier model ðcn, ˆεnÞ. Both ðcn, εnÞ and ðcn, ˆεnÞ are random
pairs relative to the sampling distribution. Given a feature-label distribution,
the relation between the true and estimated errors is completely characterized
by the joint distribution of the random vector ðεn, ˆεnÞ. Error estimation
accuracy is commonly summarized by the mean-square error (MSE) defined
by MSEðˆεnÞ ¼ E½ðˆεn  εnÞ2 or, equivalently, by the square root of the MSE,
known as the root-mean-square (RMS) error. The expectation used here is
relative to the sampling distribution induced by the feature-label distribution.
The MSE is decomposed into the deviation variance varðˆεn  εnÞ and the bias
biasðˆεnÞ ¼ E½ˆεn  εn of the error estimator relative to the true error:
MSEðˆεnÞ ¼ varðˆεn  εnÞ þ biasðˆεnÞ2.
(1.36)
When the sample size is large, it can be split into independent training and
test sets, the classifier being designed on the training data and its error being
estimated by the proportion of errors on the test data, which is the holdout
error estimator. For holdout, we have the distribution-free bound
RMSðˆεholdoutÞ ≤1∕
ﬃﬃﬃﬃﬃﬃﬃ
4m
p
,
(1.37)
where m is the size of the test sample (Devroye et al., 1996). But when data are
limited, the sample cannot be split without leaving too little data to design a
good classifier. Hence, training and error estimation must take place on the
same dataset. We next consider several training-data-based error estimators.
The resubstitution error estimator ˆεresub is the fraction of errors made by
the designed classier cn on the training sample:
ˆεresub ¼ 1
n
X
n
i¼1
jyi  cnðxiÞj.
(1.38)
The resubstitution estimator is typically (but not always) low-biased, meaning
that E½ˆεresub,E½εn. This bias can be severe for small samples, depending on the
complexity of the classification rule (Duda et al., 2001; Devroye et al., 1996).
Cross-validation is a resampling strategy in which surrogate classifiers are
designed from parts of the sample, each is tested on the remaining data, and εn
is estimated by averaging the errors (Lunts and Brailovsky, 1967; Stone,
1974). In k-fold cross-validation, the sample Sn is randomly partitioned into k
folds (subsets) SðiÞ for i ¼ 1, 2, . . . , k, each fold is left out of the design process,
a surrogate classifier cn,i is designed on the set difference Sn \ SðiÞ, and the
12
Chapter 1

cross-validation error estimate is
ˆεcvðkÞ ¼ 1
n
X
k
i¼1
X
ðx, yÞ∈SðiÞ
jy  cn,iðxÞj.
(1.39)
A k-fold cross-validation estimator is an unbiased estimator of εn  n/k, the
error of a classifier trained on a sample of size n  n/k, meaning that
E½ˆεcvðkÞ ¼ E½εnn∕k.
(1.40)
The special case of n-fold cross-validation yields the leave-one-out estimator
ˆεloo, which is an unbiased estimator of εn  1. While not suffering from severe
bias, cross-validation has large variance in small-sample settings, the result
being high RMS (Braga-Neto and Dougherty, 2004b). In an effort to reduce
the variance, k-fold cross-validation can be repeated using different folds, the
final estimate being an average of the estimates.
Bootstrap is a general resampling strategy that can be applied to error
estimation (Efron, 1979, 1983). A bootstrap sample consists of n equally likely
draws with replacement from the original sample Sn. Some points may be
drawn multiple times, whereas others may not be drawn at all. For the basic
bootstrap estimator ˆεboot, a classifier is designed on the bootstrap sample and
tested on the points left out; this is done repeatedly, and the bootstrap
estimate is the average error made on the left-out points. ˆεboot tends to be a
high-biased estimator of εn since the number of points available for design is
on average only 0.632n. The 0.632 bootstrap estimator tries to correct this bias
via a weighted average of ˆεboot and resubstitution (Efron, 1979):
ˆε0.632 boot ¼ 0.632ˆεboot þ 0.368ˆεresub.
(1.41)
The 0.632 bootstrap estimator is a special case of a convex estimator, the
general form of which is aˆεlow þ bˆεhigh, where a > 0, b > 0, and a þ b ¼ 1 (Sima
and Dougherty, 2006a). Given a feature-label distribution, a classification rule,
and low- and high-biased estimators, an optimal convex estimator is found by
finding the weights a and b that minimize the RMS.
In resubstitution there is no distinction between points near and far
from the decision boundary; the bolstered-resubstitution estimator is based
on the heuristic that, relative to making an error, more confidence should
be attributed to points far from the decision boundary than points near it
(Braga-Neto and Dougherty, 2004a). This is achieved by placing a distribu-
tion, called a bolstering kernel, at each point (rather than simply counting the
points as with resubstitution). Specifically, for i ¼ 1, 2, . . . , n, consider a
probability density function (PDF) f ⋄
i (a bolstering kernel) and define a
bolstered empirical distribution f ⋄by
13
Classification and Error Estimation

f ⋄ðx, yÞ ¼ 1
n
X
n
i¼1
f ⋄
i ðx  xiÞIy¼yi,
(1.42)
where the indicator function IE equals 1 if E is true and 0 otherwise. The
bolstered resubstitution estimator is defined by
ˆεbol ¼ Ef ⋄½jY  cnðXÞj
¼ 1
n
X
n
i¼1

Iyi¼0
Z
cnðxÞ¼1
f ⋄
i ðx  xiÞdx þ Iyi¼1
Z
cnðxÞ¼0
f ⋄
i ðx  xiÞdx

. (1.43)
The subscript in the expectation denotes the joint distribution of ðX, YÞ .
A key issue is the amount of bolstering (spread of the bolstering kernels).
A simple method has been proposed to compute this spread based on the data
(Braga-Neto and Dougherty, 2004a); however, except for small dimensions,
this simple procedure fails and an optimization method must be used to find
the spread (Sima et al., 2011). Figure 1.2 illustrates the error for linear
classification when the bolstering kernels are uniform circular distributions.
When resubstitution is heavily low-biased, it may not be good to spread
incorrectly classified data points because that increases the optimism of the
error estimate (low bias). The semi-bolstered-resubstitution estimator results
from not bolstering (no spread) for incorrectly classified points. Bolstering
can be applied to any error-counting estimation procedure. Bolstered leave-
one-out estimation involves bolstering the resubstitution estimates on the
surrogate classifiers.
Figure 1.2
Bolstered resubstitution for LDA, assuming uniform circular bolstering kernels.
The error contribution made by a point equals the area of the associated shaded region
divided by the area of the associated circle. The bolstered resubstitution error is the sum of
all error contributions divided by the number of points. [Reprinted from (Braga-Neto and
Dougherty, 2004a).]
14
Chapter 1

Finally, the plug-in error estimator ˆεplug-in requires that we parameterize
the class-conditional distributions f ðxjyÞ, where here and henceforth we do
not use the subscript “XjY ” when denoting class-conditional distributions. By
assuming that the estimates of the distribution parameters are the same as the
true parameters, the “true” error of the classifier can be computed exactly to
obtain this estimate.
The consequences of training-set error estimation are readily explained by
the following formula for the deviation variance:
varðˆεn  εnÞ ¼ s2εn þ s2
ˆεn  2rsˆεnsεn,
(1.44)
where s2εn, s2
ˆεn, and r are the variance of the error, the variance of the error
estimate, and the correlation between the true and estimated errors,
respectively. The deviation variance is driven down by small variances or a
correlation coefficient near 1. A large deviation variation results in large
RMS.
The problem with cross-validation is characterized in Eq. 1.44: for small
samples it has large variance and little correlation with the true error. Hence,
although with small folds cross-validation does not significantly suffer from
bias, it typically has large deviation variance. Figure 1.3 shows a scatter plot
and linear regression line for a cross-validation estimate (horizontal axis) and
true error (vertical axis) with a sample size of 50 using LDA. What we observe
is typical for small samples: large variance and negligible regression between
the true and estimated errors. Indeed, one even sees negatively sloping
regression lines for cross-validation and bootstrap, and negative correlation
Figure 1.3
Scatter plot and linear regression for cross-validation (horizontal axis) and the
true error (vertical axis) for linear discrimination between two classes of 50 breast cancer
patients. [Reprinted from (Dougherty, 2012).]
15
Classification and Error Estimation

between the true and cross-validation estimated errors has been mathemati-
cally demonstrated in some basic models (Braga-Neto and Dougherty, 2005).
1.4 Random Versus Separate Sampling
Thus far, we have assumed random sampling, under which the dataset Sn is
drawn independently from a fixed distribution of feature–label pairs ðX, YÞ.
In particular, this means that if a sample of size n is drawn for a binary
classification problem, then the number of sample points in classes 0 and 1, n0
and n1, respectively, are binomial random variables such that n0 þ n1 ¼ n. An
immediate consequence of the random-sampling assumption is that the prior
probability c ¼ PrðY ¼ 0Þ can be consistently estimated by the sampling ratio
ˆc ¼ n0∕n, namely, n0∕n →c in probability.
While random sampling is almost invariably assumed (often tacitly) in
classification theory, it is quite common in real-world situations for sampling
not to be random. Specifically, with separate sampling, the class ratios n0∕n
and n1∕n are chosen prior to sampling. Here, Sn ¼ Sn0 ∪Sn1, where the
sample points in Sn0 and Sn1 are selected randomly from class 0 and class 1,
respectively, but, given n, the individual class counts n0 and n1 are not
random. In this case, n0∕n is not a meaningful estimate of c. When c is not
known, both QDA and LDA involve the estimate ˆc ¼ n0∕n by default. Hence,
in the case of separate sampling when c is unknown, they are problematic.
More generally, most classification rules make no explicit mention of c;
however, their behavior depends on the sampling ratio, and their expected
performances can be significantly degraded by separate sampling. In the
special case when c is known and n0∕n  c for separate sampling, the
sampling is said to be stratified.
Example 1.1. Consider a model composed of multivariate Gaussian distribu-
tions with a block covariance structure (Esfahani and Dougherty, 2013). The
model has several parameters that can generate various covariance matrices.
For example, a 3-block covariance matrix with block size 5 has the structure
Sy ¼
2
4
By
055
055
055
By
055
055
055
By
3
5,
(1.45)
where 0k  l is a matrix of size k  l with all elements being 0, and
By ¼ s2y
2
66664
1
r
r
r
r
r
1
r
r
r
r
r
1
r
r
r
r
r
1
r
r
r
r
r
1
3
77775
,
(1.46)
16
Chapter 1

in which s2y is the variance of each variable, and r is the correlation
coefficient within a block. Two covariance matrix settings are considered:
identical covariance matrices with s2
0 ¼ s2
1 ¼ 0.4 and unequal covariance
matrices with s2
0 ¼ 0.4 and s2
1 ¼ 1.6. We use block size l ¼ 5 and r ¼ 0.8,
which models tight correlation within a block. The means are m0 ¼ 0.1 ⋅1D
and m1 ¼ 0.8 ⋅1D, and the feature-set size is 15. For a given sample size n,
sampling ratio r ¼ n0∕n, and classification rule, the conditional expected
true error rate E½εnjr is approximated via Monte Carlo simulation. The
conditional expected true errors for common covariance matrices (the
homoscedastic case) under LDA, QDA, and linear support vector machines
(L-SVMs), all assuming that ˆc ¼ n0∕n, are given in the top row of Fig. 1.4,
where
each
plot
gives
E½εnjr
for
different
sampling
ratios
and
c ∈f0.3, 0.4, 0.5, 0.6, 0.7g. Results for different covariance matrices (the
heteroscedastic case) are shown in the bottom row of Fig. 1.4. We observe
that the expected error is close to minimal when r ¼ c and that it can greatly
(a)
(b)
(c)
(d)
(e)
(f)
Figure 1.4
Expected true error rates for three classification rules (n ¼ 100) when
covariance matrices are homoscedastic (top row) and heteroscedastic (bottom row):
(a) homoscedastic, LDA; (b) homoscedastic, QDA; (c) homoscedastic, linear support vector
machine (L-SVM); (d) heteroscedastic, LDA; (e) heteroscedastic, QDA; (f) heteroscedastic,
L-SVM. [Reprinted from (Esfahani and Dougherty, 2013).]
17
Classification and Error Estimation

increase when r ≠c. The problem is that r is often chosen deterministically
without regard to c.
Can separate sampling be superior to random sampling? To answer this
question, consider random sampling and suppose that r0  c. Then, typically,
E½εnjr0,Er½E½εnjr ¼ E½εn,
(1.47)
where E½εnjr0 is the expected error for separate sampling with r ¼ r0, and the
inequality
typically
holds
because
E½εnjr0,E½εnjr
for
most
of
the
probability mass for r (which is random with random sampling) when r0 is
close to c. Here, Er denotes an expectation over r, where r is drawn from the
same distribution as the sampling ratio under random sampling. Conse-
quently, separate sampling is beneficial if c is known and the sampling ratio
is selected to reflect c. The problem is that in practice r is often chosen
without regard to c, the result being the kind of poor performance shown in
Fig. 1.4.
An interesting characteristic of the error curves in Fig. 1.4 is that they
appear to cross at a single value of r. In fact, this phenomenon is fairly
common; however, we must be careful in examining it because the figure
shows continuous curves, but r is actually a discrete variable. The issue is
examined in detail in (Esfahani and Dougherty, 2013).
1.5 Epistemology and Validity
From a scientific perspective, error estimation is the critical epistemological
issue [see (Dougherty and Bittner, 2011) for a comprehensive discussion focused
on biology and including classifier validity]. A scientific theory consists of two
parts: (1) a mathematical model composed of symbols (variables and relations
between the variables), and (2) a set of operational definitions that relate the
symbols to data. A mathematical model alone does not constitute a scientific
theory. The formal mathematical structure must yield experimental predictions
in accord with experimental observations. As stated succinctly by Richard
Feynman, “It is whether or not the theory gives predictions that agree with
experiment. It is not a question of whether a theory is philosophically delightful,
or easy to understand, or perfectly reasonable from the point of view of
common sense” (Feynman, 1985). Model validity is characterized by predictive
relations, without which the model lacks empirical content. Validation requires
that the symbols be tied to observations by some semantic rules that relate not
necessarily to the general principles of the mathematical model themselves but
to conclusions drawn from the principles. There must be a clearly defined tie
between the mathematical model and experimental methodology. Percy
Bridgman was the first who said precisely that these relations of coordination
consist in the description of physical operations. He called them, therefore,
operational definitions. As we have written elsewhere, “Operational definitions
18
Chapter 1

are required, but their exact formulation in a given circumstance is left open.
Their specification constitutes an epistemological issue that must be addressed
in mathematical (including logical) statements. Absent such a specification, a
purported scientific theory is meaningless” (Dougherty, 2008). With regard
to a classifier model, predictive capacity is quantified by the error. Thus, when
using an estimate in place of the true error, the accuracy of the estimate is
epistemologically paramount.
The validity of a scientific theory depends on the choice of validity criteria
and the mathematical properties of those criteria. The observational measure-
ments and the manner in which they are to be compared to the mathematical
model must be formally specified. This necessarily takes the form of mathe-
matical statements. As previously noted, the RMS (and equivalently the
MSE) is the most commonly applied criterion for error estimation accuracy,
and therefore for model validity. The validity of a theory is relative to the
choice of validity criterion, but what is not at issue is the necessity of a set of
relations tying the model to measurements.
Suppose that a sample is collected, a classification rule Cn applied, and
the classifier error estimated by an error-estimation rule Ξn to arrive at the
classifier model ðcn, ˆεnÞ. If no assumptions are posited regarding the feature-
label distribution (as is commonly the case), then the entire procedure is
completely distribution-free. There are three possibilities. First, if no validity
criterion is specified, then the classifier model is ipso facto epistemologically
meaningless. Second, if a validity criterion is specified, say RMS, and no
distribution-free results are known about the RMS for Cn and Ξn, then,
again, the model is meaningless. Third, if there exist distribution-free RMS
bounds concerning Cn and Ξn, then these bounds can, in principle, be used to
quantify the performance of the error estimator and thereby quantify model
validity.
To illustrate the third possibility, we consider multinomial discrimination,
where the feature components are random variables with the discrete range
f1, 2, : : : , bg, corresponding to choosing a fixed partition in RD with b cells,
and the histogram rule assigns to each cell the majority label in the cell. The
following is a distribution-free RMS bound for the leave-one-out error
estimator with the discrete histogram rule and tie-breaking in the direction of
class 0 (Devroye et al., 1996):
RMSðˆεlooÞ ≤
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ 6e1
n
þ
6
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
pðn  1Þ
p
s
.
(1.48)
Although this bound holds for all distributions, it is useless for small samples:
for n ¼ 200 this bound is 0.506. In general, there are very few cases in which
distribution-free bounds are known and, when they are known, they are
useless for small samples.
19
Classification and Error Estimation

1.5.1 RMS bounds
Distribution-based bounds are needed. These require knowledge of the RMS,
which means knowledge concerning the second-order moments of the joint
distribution between the true and estimated errors. More generally, for full
characterization of the relationship between the true and estimated errors, we
need to know their joint distribution. Oddly, this problem has historically
been ignored,
notwithstanding the
fact that error
estimation is the
epistemological ground for classification. Going back to the 1970s, there
were some results on the mean and variance of some error estimators for the
Gaussian model using LDA. In 1966, Hills obtained the expected value of the
resubstitution and plug-in estimators in the univariate model with known
common variance (Hills, 1966). In 1972, Foley obtained the expected value of
resubstitution in the multivariate model with known common covariance
matrix (Foley, 1972). In 1973, Sorum derived results for the expected value
and variance for both resubstitution and leave-one-out in the univariate
model with known common variance (Sorum, 1971). In 1973, McLachlan
derived an asymptotic representation for the expected value of resubstitution
in the multivariate model with an unknown common covariance matrix
(McLachlan, 1973). In 1975, Moran obtained new results for the expected
value of resubstitution and plug-in for the multivariate model with a known
covariance matrix (Moran, 1975). In 1977, Goldstein and Wolf obtained the
expected value of resubstitution for multinomial discrimination (Goldstein
and Wolf, 1977). Following the latter, there was a gap of 15 years before
Davison and Hall derived asymptotic representations for the expected value
and variance of bootstrap and leave-one-out in the univariate Gaussian
model with unknown and possibly different covariances (Davison and Hall,
1992). None of these papers provided representation of the joint distribution
or representation of second-order mixed moments, which are needed for
the RMS.
This problem has only recently been addressed beginning in 2005, in
particular, for the resubstitution and leave-one-out estimators. For the
multinomial model, complete enumeration was used to obtain the marginal
distributions for the error estimators (Braga-Neto and Dougherty, 2005), and
then the joint distributions (Xu et al., 2006). Subsequently, exact closed-form
representations for second-order moments, including the mixed moments,
were obtained, thereby obtaining exact RMS representations for both
estimators (Braga-Neto and Dougherty, 2010). For the Gaussian model
using LDA, the exact marginal distributions for both estimators in the
univariate model with known but not necessarily equal class variances
(the heteroscedastic model) and approximations in the multivariate model
with known and equal class covariance matrices (the homoscedastic model)
were obtained (Zollanvari et al., 2009). Subsequently, these were extended to
the joint distributions for the true and estimated errors in a Gaussian model
20
Chapter 1

(Zollanvari et al., 2010). More recently, exact closed-form representations
for the second-order moments in the heteroscedastic univariate model
were discovered, thereby providing exact expressions of the RMS for both
estimators (Zollanvari et al., 2012). Moreover, double asymptotic representa-
tions for the second-order moments in the homoscedastic multivariate model
were found, thereby providing double asymptotic expressions for the RMS
(Zollanvari et al., 2011). For the most part, both the early papers and the later
papers concern separate sampling.
An obvious way to proceed would be to say that a classifier model ðcn, ˆεnÞ
is valid for the feature-label distribution F to the extent that ˆεn approximates
the classifier error εn on F, where the degree of approximation is measured by
some distance between εn and ˆεn, say jˆεn  εnj. To do this we would have to
know the true error and F. But if we knew F, then we would use the Bayes
classifier and would not need to design a classifier from sample data. Since it is
the precision of the error estimate that is of consequence, a natural way to
proceed would be to characterize validity in terms of the precision of the error
estimator ˆεn ¼ ΞnðSnÞ as an estimator of εn, say by RMSðˆεnÞ. This makes
sense because both the true and estimated errors are random functions of the
sample
and
the
RMS
measures
their
closeness
across
the
sampling
distribution. But again there is a catch: the RMS depends on F, which we
do not know. Thus, given the sample without knowledge of F, we cannot
compute the RMS.
To proceed, prior knowledge is required, in the sense that we need to
assume that the actual (unknown) feature-label distribution belongs to some
uncertainty class U of feature-label distributions. Once RMS representations
have been obtained for feature-label distributions in U, distribution-based
RMS bounds follow:
RMSðˆεnÞ ≤sup
u∈U
RMSðˆεnjuÞ,
(1.49)
where RMSðˆεnjuÞ is the RMS of the error estimator under the assumption
that the feature-label distribution is identified with u. We do not know the
actual feature-label distribution precisely, but prior knowledge allows us to
bound the RMS.
Except in rare cases, to have scientific content classification requires prior
knowledge. Regarding the feature-label distribution there are two extremes:
(1) the feature-label distribution is known, in which case the entire
classification problem collapses to finding a Bayes classifier and Bayes error,
so there is no classifier design or error estimation issue; and (2) the uncertainty
class consists of all feature-label distributions, the distribution-free case, and
we typically have no bound, or one that is too loose for practice. In the middle
ground, there is a trade-off between the size of the uncertainty class and the
size of the sample. The uncertainty class must be sufficiently constrained
21
Classification and Error Estimation

(equivalently, the prior knowledge must be sufficiently great) that an
acceptable bound can be achieved with an acceptable sample size.
1.5.2 Error RMS in the Gaussian model
To illustrate the error RMS problem, we first consider RMS for LDA in the
one-dimensional heteroscedastic model (means m0 and m1, and variances s2
0
and s2
1) with separate sampling using fixed class sample sizes n0 and n1, and
resubstitution error estimation. Here, LDA is given by cLDAðxÞ ¼ 0 if and
only if WðxÞ,0, where WðxÞ is the Anderson W statistic in Eq. 1.29; relative
to our default definition, this variant of LDA assumes that ˆc ¼ 0.5 and assigns
the decision boundary to class 1 instead of class 0. The true error and
resubstitution estimate for class y are denoted by εy
n and ˆεy
resub, respectively.
From here forward we will typically denote the class associated with an error
or error estimate using superscripts to avoid cluttered notation. The MSE is
MSEðˆεnÞ ¼ E
cε0n þ ð1  cÞε1n 
n0
n ˆε0
resub þ n1
n ˆε1
resub

2
.
(1.50)
To evaluate the MSE we need expressions for the various second-order
moments involved. These are derived in (Zollanvari et al., 2012) and are
provided in (Braga-Neto and Dougherty, 2015). To illustrate the complexity
of the problem, even in this very simple setting, we state the forms of the
required second-order moments for the RMS. In all cases, expressions of the
form Z < 0 and Z ≥0 mean that all components of Z are negative and
nonnegative, respectively.
The second-order moments for εy
n are
E
	
ðε0nÞ2
¼ PrðZI ,0Þ þ PrðZI ≥0Þ,
(1.51)
E
	
ε0nε1n

¼ PrðZII ,0Þ þ PrðZII ≥0Þ,
(1.52)
E
	
ðε1nÞ2
¼ PrðZIII ,0Þ þ PrðZIII ≥0Þ,
(1.53)
where Zj, for j ¼ I, II, III, are 3-variate Gaussian random vectors with means
mZI ¼ mZII ¼ mZIII ¼
2
4
m0m1
2
m1  m0
m0m1
2
3
5,
(1.54)
and covariance matrices
SZI ¼
2
4
s þ s2
0
2d
s
2d
4s
2d
s
2d
s þ s2
0
3
5,
(1.55)
22
Chapter 1

SZII ¼
2
4
s þ s2
0
2d
s
2d
4s
2d
s
2d
s þ s2
1
3
5,
(1.56)
SZIII ¼
2
4
s þ s2
1
2d
s
2d
4s
2d
s
2d
s þ s2
1
3
5,
(1.57)
where s ¼
s2
0
4n0 þ
s2
1
4n1, and d ¼
s2
0
4n0 
s2
1
4n1 (Zollanvari et al., 2012). The second-
order moments for ˆεy
resub are
E
	
ðˆε0
resubÞ2
¼ 1
n0
	
PrðZI ,0Þ þ PrðZI ≥0Þ

þ n0  1
n0
	
PrðZIII ,0Þ þ PrðZIII ≥0Þ

,
(1.58)
E
	
ˆε0
resubˆε1
resub

¼ PrðZV ,0Þ þ PrðZV ≥0Þ,
(1.59)
E
	
ðˆε1
resubÞ2
¼ 1
n1
	
PrðZII ,0Þ þ PrðZII ≥0Þ

þ n1  1
n1
	
PrðZIV ,0Þ þ PrðZIV ≥0Þ

,
(1.60)
where Zj for j ¼ I, II are bivariate Gaussian random vectors, and Zj for j ¼ III,
IV, V are 3-variate Gaussian random vectors, whose means and covariance
matrices are provided in (Zollanvari et al., 2012). Lastly, the mixed moments
of εy
n and ˆεy
resub are
E
	
ε0nˆε0
resub

¼ PrðZI ,0Þ þ PrðZI ≥0Þ,
(1.61)
E
	
ε0nˆε1
resub

¼ PrðZII ,0Þ þ PrðZII ≥0Þ,
(1.62)
E
	
ε1nˆε0
resub

¼ PrðZIII ,0Þ þ PrðZIII ≥0Þ,
(1.63)
E
	
ε1nˆε1
resub

¼ PrðZIV ,0Þ þ PrðZIV ≥0Þ,
(1.64)
where Zj, for j ¼ I, . . . , IV, are 3-variate Gaussian random vectors, whose
means and covariance matrices are provided in (Zollanvari et al., 2012).
Even for LDA in the multi-dimensional homoscedastic Gaussian model,
the problem is more difficult since exact expressions for the moments are
unknown. However, in that model there exist double-asymptotic approxima-
tions for all of the second-order moments (Zollanvari et al., 2011). Double
asymptotic convergence means that n →` as the dimension D →` in a
limiting
proportional
manner.
Specifically,
n0 →`,
n1 →`,
D →`,
23
Classification and Error Estimation

D∕n0 →l0, and D∕n1 →l1, where l0 and l1 are positive finite constants. It is
also assumed that the Mahalanobis distance,
dD ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðmD,0  mD,1ÞTS1
D ðmD,0  mD,1Þ
q
,
(1.65)
converges to d . 0 as D →`, where mD,y is the mean of class y under D
dimensions, and SD is the covariance of both classes under D dimensions. An
example of the kind of result one obtains is the approximation
E
	
ðε0nÞ2
 F
0
B
@ dD
2 ⋅
1 þ D
d2
D

1
n1  1
n0

f 0ðn0, n1, D, d2
DÞ ;
1
n1 þ D
2d2
D

1
n2
0 þ 1
n2
1

f 2
0ðn0, n1, D, d2
DÞ
1
C
A,
(1.66)
where Fða, b; rÞ is the bivariate CDF of standard normal random variables
with correlation coefficient r, Fða; rÞ ¼ Fða, a; rÞ, and
f 0ðn0, n1, D, d2
DÞ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1 þ 1
n1
þ D
d2
D
 1
n0
þ 1
n1

þ D
2d2
D
 1
n2
0
þ 1
n2
1

s
.
(1.67)
It is doubly asymptotic in the sense that both the left- and right-hand
sides of the approximation converge to the same value as n →` and D →`.
Since the limit of the right-hand side is determined by the double-asymptotic
assumptions, the theoretical problem is to prove that E½ðε0nÞ2 converges
to that limit. Corresponding doubly asymptotically exact approximations
exist for the other second-order moments required for the RMS (Zollanvari
et al., 2011).
These results show a practical limiting usefulness of Eq. 1.49. If the RMS
is so complicated in a model as simple as the one-dimensional Gaussian model
for LDA and resubstitution, and it is only known approximately in the multi-
dimensional model even for the homoscedastic case, bounding the RMS does
not hold much promise for validation, even if further results of this kind can
be obtained. Thus, we must look in a different direction for error estimation.
24
Chapter 1

Chapter 2
Optimal Bayesian
Error Estimation
Given that a distributional model is needed to achieve useful performance
bounds for classifier error estimation when using the training data, a natural
course of action is to define a prior distribution over the uncertainty class of
feature-label distributions and then find an optimal minimum-mean-square-
error (MMSE) error estimator relative to the uncertainty class (Dalton and
Dougherty, 2011b).
2.1 The Bayesian MMSE Error Estimator
Consider finding a MMSE estimator (filter) of a nonnegative function
gðX, YÞ of two random variables based on observing Y; that is, minimize
EX,Y½jgðX, YÞ  hðYÞj2 over all Borel measurable functions h. The optimal
estimator,
ˆg ¼ arg min
h
EX,Y½jgðX, YÞ  hðYÞj2,
(2.1)
is given by the conditional expectation
ˆgðYÞ ¼ EX½gðX, YÞj Y:
(2.2)
Moreover, ˆgðYÞ is an unbiased estimator over the distribution f ðx, yÞ of ðX, YÞ:
EY½ˆgðYÞ ¼ EX,Y½gðX, YÞ:
(2.3)
The fact that ˆgðYÞ is an unbiased MMSE estimator of gðX, YÞ over f ðx, yÞ
does not tell us how well ˆgðYÞ estimates gð¯x, YÞ for some specific value
X ¼ ¯x. This has to do with the expected difference
25

EY½jgð¯x, YÞ  ˆgðYÞj2 ¼ EY
gð¯x, YÞ 
Z `
`
gðx, YÞf ðxjYÞdx

2
¼ EY

Z `
`
gðx, YÞ½dðx  ¯xÞ  f ðxjYÞdx

2
≤EY
Z `
`
gðx, YÞjdðx  ¯xÞ  f ðxjYÞjdx
2
,
(2.4)
where dð⋅Þ is the Dirac delta function. This inequality reveals that the accuracy
of the estimate at a point depends on the degree to which the mass of the
conditional distribution for X given Y is concentrated at ¯x on average for Y. If
we replace the single random variable Y by a sequence fY ng`
n¼1 of random
variables such that f ðxjY nÞ →dðx  ¯xÞ in a suitable sense (relative to the
convergence
of
generalized
functions),
then
we
are
assured
that
ˆgðY nÞ  gð¯x, Y nÞ →0 in the mean-square sense.
We desire ˆgðYÞ to estimate gð¯x, YÞ but are uncertain of ¯x. Instead, we can
obtain an unbiased MMSE estimator for gðX, YÞ, which means good
performance across all possible values of X relative to the distribution of X
and Y; however, the performance of that estimator for a particular value
X ¼ ¯x depends on the concentration of the conditional mass of X relative to ¯x.
When applying MMSE estimation theory to error estimation, the
uncertainty manifests itself in a Bayesian framework relative to a space of
feature-label distributions and samples. The random variable X is replaced by
a random vector u governed by a specified prior distribution pðuÞ, where each
u corresponds to a feature-label distribution parameterized by u, and denoted
by f uðx, yÞ. The parameter space is denoted by U. The random variable Y is
replaced by a sample Sn, which is used to train a classifier cn, and we set
gðX, YÞ ¼ εnðu, cnÞ,
(2.5)
which is the true error on fu of cn. In this scenario, ˆgðYÞ becomes the error
estimator
ˆεnðSn, cnÞ ¼ Ep½εnðu, cnÞjSn,
(2.6)
which we call the Bayesian MMSE error estimator. The conditional
distribution f ðxjYÞ becomes the posterior distribution pð uj SnÞ ¼ f ð uj SnÞ,
which for simplicity we write as p∗ðuÞ, tacitly keeping in mind conditioning on
the sample. We will sometimes write the true error as εnðuÞ or εn, and the
Bayesian MMSE error estimator as
26
Chapter 2

ˆεn ¼ Ep∗½εn,
(2.7)
which is short-hand for ˆεnðSn, cnÞ expressed in Eq. 2.6.
Throughout this book, we will use several notations to clarify the
probability space for expectations and probabilities. In Eq. 2.7 we use a
subscript with the name of the distribution of the random vector, in this case
the posterior distribution p on u. Occasionally, the prior distribution will be
used instead of the posterior. We may also apply conditioning to this notation
for emphasis; for example, we sometimes use notation similar to Eq. 2.6 rather
than Eq. 2.7 to emphasize conditioning on the sample Sn. When the
distribution is understood, we will sometimes drop subscripts, or list random
variables in the subscript. For example, EX,Y in Eq. 2.1 denotes an expectation
over the joint distribution between X and Y.
From here on, we will denote an arbitrary classifier and its error by c and
ε, respectively. To emphasize that a classifier is trained from a sample Sn, we
denote a trained classifier and its error by cn and εn, respectively. We will
always refer to the Bayesian MMSE error estimator as ˆεn, which always
depends on the sample Sn. Other error estimators will be denoted with a
subscript naming the corresponding error estimation rule. For clarity, we will
sometimes write the true error as a function of u and Sn, or as a function of u
and the classifier. Similarly, we will sometimes write an error estimator as a
function of Sn only, or as a function of Sn and the classifier.
In the context of classification, u is a random vector composed of three
parts: the parameters u0 of the class-0 conditional distribution, the parameters
u1 of the class-1 conditional distribution, and the class probability c for class 0
(with 1  c for class 1). We define Uy to be the parameter space for uy and
write the class-conditional distributions as f uyðxjyÞ. The marginal prior
densities of the class-conditional distributions are denoted by pðuyÞ for
y ∈f0, 1g. The marginal density for the a priori class probability is denoted by
pðcÞ. In using common Bayesian terminology, we also refer to these prior
distributions as “prior probabilities.”
It is up to the investigator to consider the nature of the problem at hand
and to choose appropriate models for pðuyÞ and pðcÞ (Jaynes, 1968). One
might utilize theoretical knowledge to construct a prior, or, alternatively, use
an objective prior. Objective, or non-informative, priors attempt to characterize
only basic information about the variable of interest, for instance, the support
of the distribution, and are useful if one wishes to avoid using subjective data.
An example would be the flat prior, which assumes that the prior distribution
is constant over its support (or more generally, that the prior is proportional
to 1 over its support). In Chapter 7 we consider the construction of priors
based on existing scientific knowledge. A conjugate prior is one for which the
prior and posterior are in the same family of distributions. This permits a
closed form for the posterior and avoids difficult integration. Even in many
27
Optimal Bayesian Error Estimation

classical problems, there is no universal agreement on the “right” prior to use.
Based on the introductory filter analysis, we would like p∗ðuÞ to be close to
dðu  ¯uÞ, where ¯u corresponds to the actual feature-label distribution from
which the data have come, but we do not know ¯u, and an overzealous effort to
concentrate the conditional mass at a particular value of u can have
detrimental effects if that value is far from ¯u.
The Bayesian MMSE error estimate is not guaranteed to be the optimal
error estimate for any particular feature-label distribution (the true error being
the best estimate and perfect), but for a given sample, and assuming the
parameterized model and prior distribution, it is both optimal on average with
respect to MSE (and therefore RMS) and unbiased when averaged over all
parameters and samples. These implications apply for any classification rule
that is not a function of the true parameters, and produces a classifier that is
fixed given the sample.
To facilitate analytic representations, we assume that c is independent
from both u0 and u1 prior to observing the data. This assumption allows us to
separate the Bayesian MMSE error estimator into components representing
the error contributed by each class. We will maintain the assumption that c
and ðu0, u1Þ are independent throughout this chapter and Chapter 4. At times,
we will make the further assumption that c, u0, and u1 are mutually
independent; in particular, this assumption is made in Chapter 3.
Given pðuyÞ and pðcÞ, data are used to find the joint posterior density. By
the product rule,
p∗ðc, uyÞ ¼ f ðc, uyjSnÞ
¼ f ðcjSn, uyÞf ðuyjSnÞ:
(2.8)
Given ny, c is independent of the sample values and the distribution
parameters for class y. Hence,
f ðcjSn, uyÞ ¼ f ðcjny, fx0
i gn0
1 , fx1
i gn1
1 , uyÞ
¼ f ðcjnyÞ,
(2.9)
where fxy
i g
ny
1 denotes the collection of training points in class y ∈f0, 1g, and
we assume that ny is fixed in this notation. We have that c remains
independent of u0 and u1 posterior to observing the data:
p∗ðc, uyÞ ¼ f ðcjnyÞf ðuyjSnÞ
¼ p∗ðcÞp∗ðuyÞ,
(2.10)
where p∗ðcÞ and p∗ðuyÞ are the marginal posterior densities for c and uy,
respectively. By Bayes’ theorem, p∗ðuyÞ ∝pðuyÞf ðSnjuyÞ, where the constant
28
Chapter 2

of proportionality can be found by normalizing the integral of p∗ðuyÞ to 1.
The term f ðSnjuyÞ is called the likelihood function. Furthermore,
p∗ðuÞ ¼ f ðc, u0, u1jSnÞ
¼ p∗ðcÞp∗ðu0, u1Þ,
(2.11)
where p∗ðu0, u1Þ ¼ f ðu0, u1jSnÞ.
Owing to the posterior independence between c and uy, and since εy
n is a
function of uy only, the Bayesian MMSE error estimator can be expressed as
ˆεn ¼ Ep∗½εn
¼ Ep∗½cε0n þ ð1  cÞε1n
¼ Ep∗½cEp∗½ε0n þ ð1  Ep∗½cÞEp∗½ε1n,
(2.12)
where Ep∗½c depends on our prior assumptions about the prior class
probability. Ep∗½εy
n may be viewed as the posterior expectation for the error
contributed by class y, that is, the Bayesian MMSE error estimate contributed
by class y. If we let ˆε y
n ¼ Ep∗½εy
n, then we can rewrite the estimator as
ˆεn ¼ Ep∗½cˆε 0n þ ð1  Ep∗½cÞˆε 1n :
(2.13)
With a fixed classifier and given uy, the true error εy
nðuyÞ is deterministic, and
ˆε y
n ¼ Ep∗½εy
n ¼
Z
Uy
εy
nðuyÞp∗ðuyÞduy:
(2.14)
If c, u0, and u1 are mutually independent prior to observing the data, then
the above still holds. In addition, given ny and the sample, distribution
parameters for class y are independent from sample points not from class y.
Thus,
p∗ðuyÞ ¼ f ðuyjSnÞ
¼ f ðuyjny, fx0
i gn0
1 , fx1
i gn1
1 Þ
¼ f ðuyjny, fxy
i g
ny
1 Þ
¼ f ðuyjfxy
i g
ny
1 Þ:
(2.15)
Furthermore, c, u0, and u1 remain independent posterior to observing the
data, that is,
p∗ðuÞ ¼ f ðcjn0Þf ðu0jfx0
i gn0
1 Þf ðu1jfx1
i gn1
1 Þ
¼ p∗ðcÞp∗ðu0Þp∗ðu1Þ:
(2.16)
29
Optimal Bayesian Error Estimation

Further, by Bayes’ theorem,
p∗ðuyÞ ¼ f ðuyjfxy
i g
ny
1 Þ
∝pðuyÞf ðfxy
i g
ny
1 juyÞ
¼ pðuyÞ
Y
ny
i¼1
f uyðxy
i jyÞ:
(2.17)
We assume that sample points are independent throughout, with the exception
of Sections 4.10 and 4.11. As is common in Bayesian analysis, we often
characterize a posterior as being proportional to a prior times a likelihood; the
normalization constant will still be accounted for throughout our analysis.
Although we call pðuyÞ the “prior probabilities,” they are not required to
be valid density functions. The priors are proper if the integral of pðuyÞ is
finite, and they are improper if the integral of pðuyÞ is infinite, i.e., if pðuyÞ
induces a s-finite measure but not a finite probability measure. The flat prior
over an infinite support is necessarily an improper prior. When improper
priors are used, Bayes’ theorem does not apply. However, as long as the
product of the prior and likelihood function is integrable, we define the
posterior to be their normalized product, e.g., we take Eq. 2.17 (following
normalization) as the definition of the posterior in the mutually independent
case.
For the class prior probabilities, under random sampling, we only need to
consider the size of each class:
p∗ðcÞ ¼ f ðcjn0Þ ∝pðcÞf ðn0jcÞ ∝pðcÞcn0ð1  cÞn1,
(2.18)
where we have taken advantage of the fact that n0 has a binomialðn, cÞ
distribution given c. We consider three models for the prior distributions of
the a priori class probabilities: beta, uniform, and known.
Suppose that the prior distribution for c follows a betaða0, a1Þ distribution,
pðcÞ ¼ ca01ð1  cÞa11
Bða0, a1Þ
,
(2.19)
where
Bða0, a1Þ ¼ Gða0ÞGða1Þ
Gða0 þ a1Þ
(2.20)
is the beta function, and
GðaÞ ¼
Z `
0
xa1exdx
(2.21)
30
Chapter 2

is the gamma function. We call parameters of the prior model, in this case a0
and a1, hyperparameters. The posterior distribution for c can be obtained
from Eq. 2.18. From this beta-binomial model, with random sampling the
form of p∗ðcÞ is still a beta distribution with updated hyperparameters
a∗y ¼ ay þ ny:
p∗ðcÞ ¼ ca∗
01ð1  cÞa∗
11
Bða∗
0, a∗
1Þ
:
(2.22)
The expectation of this distribution is given by (Devore, 1995)
Ep∗½c ¼
a∗
0
a∗
0 þ a∗
1
:
(2.23)
In the special case of a uniform prior, a0 ¼ a1 ¼ 1, and
p∗ðcÞ ¼ ðn þ 1Þ!
n0! n1! cn0ð1  cÞn1,
(2.24)
Ep∗½c ¼ n0 þ 1
n þ 2 :
(2.25)
In the case of separate sampling, there is no posterior for c since the data
provide no information regarding c. Hence, for separate sampling we must
assume that c is known; otherwise, we can only compute the Bayesian MMSE
error estimates contributed by the individual classes.
Bayesian error estimation for classification is not completely new. In the
1960s, two papers made small forays into the area. In (Sorum, 1968), a
Bayesian error estimator is given for the univariate Gaussian model with
known covariance matrices. In (Geisser, 1967), the problem is addressed in the
multivariate Gaussian model for a particular linear classification rule based
on Fisher’s discriminant for a common unknown covariance matrix and
known class probabilities by using a specific prior on the means and the
inverse of the covariance matrix. In neither case were the properties,
optimality, or performance of these estimators extensively considered. We
derive
the
Bayesian
MMSE
error
estimator
for
an
arbitrary
linear
classification rule in the multivariate Gaussian model with unknown
covariance matrices and unknown class probabilities using a general class
of priors on the means and an intermediate parameter that allows us to
impose structure on the covariance matrices, and we extensively study the
performance of Bayesian MMSE estimators in the Gaussian model.
31
Optimal Bayesian Error Estimation

2.2 Evaluation of the Bayesian MMSE Error Estimator
Direct evaluation of the Bayesian MMSE error estimator is accomplished by
deriving Ep∗½εy
n for each class using Eq. 2.14, finding Ep∗½c according to the
prior model for c, and referring to Eq. 2.13 for the complete Bayesian MMSE
error estimator. This can be tedious owing to the need to evaluate a
challenging integral. Fortunately, the matter can be greatly simplified.
Relative to the uncertainty class U and the posterior distribution p∗ðuyÞ for
y ¼ 0, 1, we define the effective class-conditional density as
f UðxjyÞ ¼
Z
Uy
f uyðxjyÞp∗ðuyÞduy:
(2.26)
The next theorem shows that the effective class-conditional density, which is
the average of the state-specific conditional densities relative to the posterior
distribution, can be used to more easily obtain the Bayesian MMSE error
estimator than by going through the route of direct evaluation.
Theorem 2.1 (Dalton and Dougherty, 2013a). Let c be a fixed classifier given
by cðxÞ ¼ 0 if x ∈R0, and cðxÞ ¼ 1 if x ∈R1, where R0 and R1 are measurable
sets partitioning the feature space. Then the Bayesian MMSE error estimator
can be found by
ˆεnðSn, cÞ ¼ Ep∗½c
Z
R1
f Uðxj0Þdx þ ð1  Ep∗½cÞ
Z
R0
f Uðxj1Þdx
¼
Z
X
½Ep∗½cf Uðxj0ÞIx∈R1 þ ð1  Ep∗½cÞf Uðxj1ÞIx∈R0dx:
(2.27)
Proof. For a fixed distribution uy and classifier c, the true error contributed by
class y ∈f0, 1g may be written as
εy
nðuy, cÞ ¼
Z
R1y
f uyðxjyÞdx:
(2.28)
Averaging over the posterior yields
Ep∗½εy
nðuy, cÞ ¼
Z
Uy
εy
nðuy, cÞp∗ðuyÞduy
¼
Z
Uy
Z
R1y
f uyðxjyÞdxp∗ðuyÞduy
¼
Z
R1y
Z
Uy
f uyðxjyÞp∗ðuyÞduydx
¼
Z
R1y
f UðxjyÞdx,
(2.29)
32
Chapter 2

where switching the integrals in the third line is justified by Fubini’s theorem.
Finally,
ˆεnðSn, cÞ ¼ Ep∗½cEp∗½ε0nðu0, cÞ þ ð1  Ep∗½cÞEp∗½ε1nðu1, cÞ
¼ Ep∗½c
Z
R1
f Uðxj0Þdx þ ð1  Ep∗½cÞ
Z
R0
f Uðxj1Þdx
¼
Z
X
ðEp∗½c f Uðxj0ÞIx∈R1 þ ð1  Ep∗½cÞf Uðxj1ÞIx∈R0Þdx,
(2.30)
which completes the proof.
▪
In the proof of the theorem, Eq. 2.30 was obtained by applying Eq. 2.29
to the y ¼ 0 and y ¼ 1 terms individually. Hence, the theorem could be
expressed as
ˆε y
n ¼ Ep∗½εy
n
¼
Z
X
f UðxjyÞIx∈X\Rydx
¼
Z
X\Ry
f UðxjyÞdx
(2.31)
for y ∈f0, 1g.
When a closed-form solution for the Bayesian MMSE error estimator is
not available, Eq. 2.27 can be approximated by drawing a synthetic sample
from the effective class-conditional densities f UðxjyÞ and substituting the
proportion of points from class y misclassified by c for the integral
R
X\Ry f UðxjyÞdx.
2.3 Performance Evaluation at a Fixed Point
The error rate of a classifier is the probability of misclassifying a random
point (vector) drawn from the feature-label distribution. The Bayesian MMSE
error estimator produces an estimate of this probability. That being said,
in some applications, one may be interested not in the probability of
misclassification for a random point, but in the probability of misclassification
for a specific test point x drawn from the true feature-label distribution. When
the true distribution is known precisely, the probability that x has label
y ∈f0, 1g is simply
PrðY ¼ yjX ¼ x, uÞ ¼
cy f uyðxjyÞ
P1
i¼0 ci f uiðxjiÞ ,
(2.32)
where cy ¼ c if y ¼ 0, and cy ¼ 1  c if y ¼ 1. If one has in mind a specific label
for x, perhaps the label cðxÞ produced by some classifier c, then one may be
33
Optimal Bayesian Error Estimation

interested in the misclassification probability for this prediction. For known
parameters, this is
PrðY ≠cðxÞjX ¼ x, uÞ ¼
X
y≠cðxÞ
cy f uyðxjyÞ
P1
i¼0 ci f uiðxjiÞ ,
(2.33)
where in binary classification the outer sum has a single term indexed by
y ¼ 1  cðxÞ. The true error of a classifier under a known distribution can be
expressed as
εnðu, cÞ ¼ PrðY ≠cðXÞjuÞ
¼
Z
X
PrðY ≠cðxÞjX ¼ x, uÞ
X
1
i¼0
ci f uiðxjiÞdx
¼
X
1
y¼0
Z
cðxÞ≠y
cyf uyðxjyÞdx:
(2.34)
In the Bayesian framework, the probability corresponding to Eq. 2.32 is
given by
PrðY ¼ yjX ¼ x, SnÞ ¼
Z
U
PrðY ¼ yjX ¼ x, Sn, uÞf ðujX ¼ x, SnÞdu
∝
Z
U
PrðY ¼ yjX ¼ x, Sn, uÞf ðxjSn, uÞp∗ðuÞdu
¼
Z
U
PrðY ¼ yjX ¼ x, uÞf ðxjuÞp∗ðuÞdu,
(2.35)
where f ðujX ¼ x, SnÞ is the posterior of u given Sn and the test point x, x is
independent from the sample given u, and f ðxjuÞ ¼ P1
i¼0 cif uiðxjiÞ. Now
PrðY ¼ yjX ¼ x, uÞ is available in Eq. 2.32; hence,
PrðY ¼ yjX ¼ x, SnÞ ∝
Z
U
cyf uyðxjyÞp∗ðuÞdu
¼
Z 1
0
cyp∗ðcyÞdcy
Z
U0
Z
U1
f uyðxjyÞp∗ðu0, u1Þdu0du1
¼ Ep∗½cyf UðxjyÞ,
(2.36)
where in the second line we have used the independence between cy and
ðu0, u1Þ and Eq. 2.11. After normalizing,
34
Chapter 2

PrðY ¼ yjX ¼ x, SnÞ ¼
Ep∗½cy f UðxjyÞ
P1
i¼0 Ep∗½ci f UðxjiÞ :
(2.37)
Not surprisingly, this is of the same form as Eq. 2.32 with the effective density
and estimated class probability plugged in for the true values. Similarly, for a
given label prediction cðxÞ at x, the probability of misclassification is
PrðY ≠cðxÞjX ¼ x, SnÞ ¼
X
y≠cðxÞ
Ep∗½cy f UðxjyÞ
P1
i¼0 Ep∗½ci f UðxjiÞ :
(2.38)
The same result is derived in (Knight et al., 2014). Further, since
f ðxjSnÞ ¼
Z
U
f ðxjSn, uÞf ðujSnÞdu
¼
Z
U
f ðxjuÞp∗ðuÞdu
¼
Z
U
X
1
y¼0
f ðxjY ¼ y, uÞ PrðY ¼ yjuÞp∗ðuÞdu
¼
X
1
y¼0
Z 1
0
cyp∗ðcyÞdcy
Z
U0
Z
U1
f uyðxjyÞp∗ðu0, u1Þdu0du1
¼
X
1
y¼0
Ep∗½cy f UðxjyÞ,
(2.39)
it follows from Eq. 2.38 that the Bayesian MMSE error estimator can be
expressed as
ˆεnðSn, cÞ ¼ PrðY ≠cðxÞjSnÞ
¼
Z
X
PrðY ≠cðxÞjX ¼ x, SnÞf ðxjSnÞdx
¼
Z
X
PrðY ≠cðxÞjX ¼ x, SnÞ
X
1
i¼0
Ep∗½ci f UðxjiÞdx
¼
X
1
y¼0
Z
cðxÞ≠y
Ep∗½cy f UðxjyÞdx:
(2.40)
35
Optimal Bayesian Error Estimation

2.4 Discrete Model
Consider multinomial discrimination, in which there is a discrete sample space
X ¼ f1, 2, : : : , bg with b ≥2 bins. Let pi and qi be the class-conditional
probabilities in bin i ∈f1, 2, : : : , bg for class 0 and 1, respectively, and define
U0
i and U1
i to be the number of sample points observed in bin i ∈f1, 2, : : : , bg
from class 0 and 1, respectively. The class sizes are thus n0 ¼ Pb
i¼1 U0
i and
n1 ¼ Pb
i¼1 U1
i . A general discrete classifier assigns each bin to a class, so
c : f1, 2, : : : , bg →f0, 1g. The true error of an arbitrary classifier c is given by
Eq. 1.3, where
ε0n ¼
X
b
i¼1
piIcðiÞ¼1,
(2.41)
ε1n ¼
X
b
i¼1
qiIcðiÞ¼0:
(2.42)
The
discrete
Bayesian
model
defines
u0 ¼ ½ p1, p2, : : : , pb
and
u1 ¼ ½q1, q2, : : : , qb.
The
parameter
space
of
u0
is
the
standard
(b  1)-simplex U0 ¼ Db1 ⊂Rb, which is the set of all valid bin probabilities;
e.g., ½ p1, p2, : : : , pb ∈U0 if and only if 0 ≤pi ≤1 for i ∈f1, 2, : : : , bg and
Pb
i¼1 pi ¼ 1. The parameter space U1 is defined similarly. With the parametric
model established, define Dirichlet priors:
pðu0Þ ¼
1
Bða0
1, : : : , a0
bÞ
Y
b
i¼1
p
a0
i 1
i
,
(2.43)
pðu1Þ ¼
1
Bða1
1, : : : , a1
bÞ
Y
b
i¼1
q
a1
i 1
i
,
(2.44)
where B is the multivariate beta function,
Bða1, : : : , abÞ ¼
Gða1Þ ··· GðabÞ
Gða1 þ ··· þ abÞ :
(2.45)
For proper priors, the hyperparameters ay
i for i ∈f1, 2, : : : , bg and y ∈f0, 1g
must be positive, and for uniform priors, ay
i ¼ 1 for all i and y. For a
Dirichlet distribution on ½ p1, p2, : : : , pb with concentration parameter
½a0
1, a0
2, : : : , a0
b, the mean of pj is well known and given by
E½pj ¼
a0
j
Pb
k¼1 a0
k
:
(2.46)
36
Chapter 2

Second-order moments are also well known:
E½p2
j  ¼
a0
j ða0
j þ 1Þ
Pb
k¼1 a0
k

1 þ Pb
k¼1 a0
k
 ,
(2.47)
E½pipj ¼
a0
i a0
j
Pb
k¼1 a0
k

1 þ Pb
k¼1 a0
k
 :
(2.48)
Similar equations hold for moments of q1, q2, : : : , qb in class 1.
2.4.1 Representation of the Bayesian MMSE error estimator
The next theorem provides the posterior distributions for the discrete model.
This property of Dirichlet distributions is well known (Johnson et al., 1997).
Theorem 2.2. For the discrete model with Dirichletðay
1, : : : , ay
bÞ priors for
class y ∈f0, 1g, ay
i . 0 for all i and y, the posterior distributions are
DirichletðUy
1 þ ay
1, : : : , Uy
b þ ay
bÞ and given by
p∗ðu0Þ ¼ Gðn0 þ Pb
i¼1 a0
i Þ
Qb
k¼1 GðU0
k þ a0
kÞ
Y
b
i¼1
p
U0
i þa0
i 1
i
,
(2.49)
p∗ðu1Þ ¼ Gðn1 þ Pb
i¼1 a1
i Þ
Qb
k¼1 GðU1
k þ a1
kÞ
Y
b
i¼1
q
U1
i þa1
i 1
i
:
(2.50)
Proof. Consider class 0. f u0ðx0
i j0Þ equals the bin probability corresponding to
class 0 and bin x0
i . Thus, we have the likelihood function
Y
n0
i¼1
f u0ðx0
i j0Þ ¼
Y
b
i¼1
p
U0
i
i :
(2.51)
By Eq. 2.17, the posterior of u0 is still proportional to the product of the bin
probabilities:
p∗ðu0Þ ∝
Y
b
i¼1
p
U0
i þa0
i 1
i
:
(2.52)
We realize that this is a DirichletðU0
1 þ a0
1, : : : , U0
b þ a0
bÞ distribution. The
density function for this distribution is precisely Eq. 2.49. Class 1 is similar. ▪
Theorem 2.3 (Dalton and Dougherty, 2013a). For the discrete model with
Dirichletðay
1, : : : , ay
bÞ priors for class y ∈f0, 1g, ay
i . 0 for all i and y, the
effective class-conditional densities are given by
37
Optimal Bayesian Error Estimation

f UðjjyÞ ¼
Uy
j þ ay
j
ny þ Pb
i¼1 ay
i
:
(2.53)
Proof. The effective density for class y ¼ 0 is
f Uðjj0Þ ¼
Z
U0
pjp∗ðp1, p2, : : : , pbÞdp1dp2 ··· dpb ¼ Ep∗½pj:
(2.54)
Plugging in the hyperparameters for the posterior p∗ðp1, p2, : : : , pbÞ, we
obtain Eq. 2.53 from Eq. 2.46. Class 1 is treated similarly.
▪
In the preceding theorem, f Uðjj0Þ and f Uðjj1Þ may be viewed as effective
bin probabilities for each class after combining prior knowledge and
observed data.
Theorem 2.1 yields the Bayesian MMSE estimator:
ˆεn ¼
X
b
j¼1
Ep∗½c
U0
j þ a0
j
n0 þ Pb
i¼1 a0
i
IcðjÞ¼1 þ ð1  Ep∗½cÞ
U1
j þ a1
j
n1 þ Pb
i¼1 a1
i
IcðjÞ¼0:
(2.55)
In particular, the first posterior moment of the true error for y ∈f0, 1g is
given by
ˆε y
n ¼ Ep∗½εy
n ¼
X
b
j¼1
Uy
j þ ay
j
ny þ Pb
i¼1 ay
i
IcðjÞ¼1y:
(2.56)
In the special case where we have random sampling, uniform c, and uniform
priors for the bin probabilities (ay
i ¼ 1 for all i and y), the Bayesian MMSE
error estimate is
ˆεn ¼
X
b
j¼1
n0 þ 1
n þ 2 ⋅
U0
j þ 1
n0 þ b IcðjÞ¼1 þ n1 þ 1
n þ 2 ⋅
U1
j þ 1
n1 þ b IcðjÞ¼0:
(2.57)
In addition, the Bayesian MMSE error estimator assuming random sampling
and (improper) ay ¼ ay
i ¼ 0 for all i and y is equivalent to resubstitution.
2.4.2 Performance and robustness in the discrete model
We next examine the performance of discrete Bayesian error estimators with
the histogram rule via examples involving simulation studies. The first
example considers the performance of Bayesian MMSE error estimators for
two bins and different beta prior distributions for the bin probabilities. By
studying beta priors that target specific values for the bin probabilities, we will
38
Chapter 2

observe the benefits of informative priors and assess the robustness of discrete
Bayesian error estimators to poor prior-distribution modeling. The second
example treats the performance of Bayesian MMSE error estimators with
uniform priors for an arbitrary number of bins. These simulations show how
and when non-informative Bayesian error estimators improve on the
resubstitution and leave-one-out error estimators, especially as the number
of bins increases. The third example considers average performance under
uniform priors, including bias and deviation variance.
A summary of the basic Monte Carlo simulation methodology is shown in
Fig. 2.1, which lists the main steps and flow of information. At the start we
have a single distribution and a prior. In each iteration, step A generates a
labeled training sample from class-conditional distributions via random
sampling, stratified sampling, or separate sampling. These labeled sample
points are used to update the prior to a posterior in step B and to train one or
more classifiers in step C. Feature selection, if implemented, is considered part
of step C. In step D, for each classifier we compute and store several statistics
used in performance evaluation. The true error εn is either found exactly or
approximated by evaluating the proportion of misclassified points from a
large synthetic testing set drawn from the true distributions parameterized by
c, u0, and u1. We evaluate the Bayesian MMSE error estimator ˆεn from the
classifier and posterior using closed-form expressions if available and by
evaluating the proportion of misclassified points from a large synthetic testing
set drawn from the effective density otherwise. We also find the MSE of ˆεn
conditioned on the sample, denoted by MSEðˆεnjSnÞ, which we will study
extensively in Chapter 3 and is expressed in Theorem 3.1. Several classical
training-data error estimators are also computed. The sample-conditioned
MSE of the classical error estimators may be evaluated for each iteration
according to Theorem 3.2. Steps A through D are repeated to obtain t samples
and sets of output. Typically, for each sample and error estimator we evaluate
the difference ˆεn  εn and the squared difference ðˆεn  εnÞ2, and average these
to produce Monte Carlo approximations of bias and RMS over the sampling
distribution.
Figure 2.1
Simulation methodology for fixed sample sizes under a fixed feature-label
distribution or a large dataset representing a population.
39
Optimal Bayesian Error Estimation

Example 2.1. For all simulations in this example, we use random sampling in
a discrete model with b ¼ 2, true a priori class probability c ¼ 0.5, and fixed
bin probabilities governed by fixed class-0 bin-1 probability p and class-1 bin-
1 probability q. The Bayes error, or the optimal true error obtained from the
optimal classifier (not to be confused with Bayesian error estimators), is
min(p, q). To generate a random sample we first determine the sample size for
each class using a binomial(n, c) experiment and then assign each sample
point a bin number according to the distribution of its class. The sample is
then used to train a histogram classifier, where the class assigned to each bin is
determined by a majority vote. The same samples are used for classifier design
and error estimation. The true error of this histogram classifier is calculated
via our known distribution parameters. The same sample is used to find
resubstitution, leave-one-out, and Bayesian MMSE error estimates for the
designed classifier. This process is repeated t ¼ 10,000,000 times to find a
Monte Carlo approximation for the RMS deviation from the true error for
each error estimator.
All results are presented in Fig. 2.2. For all Bayesian MMSE error
estimators, c is assumed to have a uniform prior and the top row of Fig. 2.2
shows different beta distributions used as priors for p. We set q ¼ 1p so that
the priors for q are the same but flipped about 0.5, i.e., a1
1 ¼ a0
2 and a1
2 ¼ a0
1,
and Ep½q ¼ 1  Ep½p. Part (a) of the figure shows five priors with varying
means ðEp½p ¼ 0:5; 0:6; 0:7; 0:8; 0:9Þ and relatively low variance. Part (b)
shows several beta distributions with Ep½p ¼ 0.5 (including the uniform prior)
with varying weights between middle versus edge values of p. In all priors in
part (b), the ay
i are equal for all i.
The graphs below these priors present RMS deviation from true error for
the resubstitution and leave-one-out error estimators, and for the Bayesian
MMSE error estimators corresponding to the priors at the top, with respect to
both sample size (middle row) and the true distributions (bottom row). Each
point on these graphs represents a fixed sample size and true distribution
(c ¼ 0.5, p, and q), and under these fixed conditions we evaluate the
performance of each error estimator using Monte Carlo simulations.
In the second row of Fig. 2.2, we fix the true distributions at p ¼ 0.8 and
q ¼ 0.2, and observe performance for increasing sample size. Naturally, these
simulations show that priors with a high density around the true distributions
have better performance and tend to converge more quickly to the true error.
For example, in part (a) the prior with Ep½p ¼ 0.8 matches the distribution
p ¼ 0.8 very well, and this is reflected in the RMS deviation in Fig. 2.2(c), where
we observe remarkable performance. On the other hand, when our priors have
a small mass around the true distributions, performance can be quite poor
compared to resubstitution and leave-one-out, and converge very slowly with
increasing sample size. See, for example, the prior with Ep½p ¼ 0.5 in part (c).
40
Chapter 2

The third row of Fig. 2.2 shows performance graphs with sample size
n ¼ 20, as a function of p. These illustrate how each prior performs as the true
distributions vary. In all cases, performance is best in the ranges of p and q
that are well represented in the prior distributions, but outside this range
results can be poor. This is best seen in Fig. 2.2(e), where the RMS curves
move to the right as the priors move right. High-information priors offer
better performance if they are within the targeted range of parameters, but
0
0.2
0.4
0.6
(a)
(b)
(c)
(d)
(e)
(f)
0.8
1
0
2
4
6
8
10
12
 
 
1
0=25, 2
0
1
0=30, 2
0
1
0=35, 2
0
1
0=40, 2
0
1
0=45, 2
0
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
 
 
1
0=
2
0=0.1
1
0=
2
0=0.5
1
0=
2
0=1
1
0=
2
0=2
1
0=
2
0=10
5
10
15
20
25
30
0
0.05
0.1
0.15
0.2
0.25
0.3
RMS deviation from true error
 
 
resub/plugin
loo
sample size
5
10
15
20
25
30
0
0.05
0.1
0.15
0.2
0.25
0.3
RMS deviation from true error
 
 
resub/plugin
loo
sample size
0
0.2
0.4
0.6
0.8
1
0
0.05
0.1
0.15
0.2
0.25
RMS deviation from true error
 
 
resub/plugin
loo
0
0.2
0.4
0.6
0.8
1
0
0.05
0.1
0.15
0.2
0.25
RMS deviation from true error
 
 
resub/plugin
loo
Figure 2.2
RMS deviation from true error for discrete classification (b ¼ 2, c ¼ 0.5): (a) using
low variance priors; (b) using priors centered at p ¼ 0.5; (c) versus sample size (low variance
priors, p ¼ 0.8); (d) versus sample size (centered priors, p ¼ 0.8); (e) versus p (low variance
priors, n ¼ 20); (f) versus p (centered priors, n ¼ 20). Lines without markers represent Bayesian
MMSE error estimators with different beta priors, which are labeled and shown in the graph at
the top of the corresponding column. [Reprinted from (Dalton and Dougherty, 2011b).]
41
Optimal Bayesian Error Estimation

performance outside this range is reciprocally worse. Meanwhile, low-
information priors tend to give safer results by avoiding catastrophic behavior
at the expense of performance. For example, in Fig. 2.2(f) the curves
corresponding to priors with large ay
i dip well below resubstitution (good
performance) when p is near 0.5; however, away from this range performance
rapidly deteriorates. Meanwhile, the curves corresponding to priors with
ay
i ¼ 1 have good performance over all states.
Example 2.2. We now consider the RMS performance of Bayesian MMSE
error estimators with non-informative uniform priors for an arbitrary number
of bins. This example treats performance relative to a fixed distribution. As
noted in (Braga-Neto and Dougherty, 2005), discrete classification for these
bin sizes corresponds to regulatory rule design in binary regulatory networks.
The distributions are fixed with c ¼ 0.5. We use Zipf’s power law model,
where pi ∝ia and qi ¼ pbiþ1 for i ¼ 1, 2, : : : , b [see (Braga-Neto and
Dougherty, 2005)]. The parameter a ≥0 is a free parameter used to target a
specific Bayes error, with larger a corresponding to smaller Bayes error. We
again use random sampling and the histogram classification rule throughout.
Figure 2.3 shows the RMS as a function of sample size for b ¼ 16 and
different
Bayes
errors,
where
increasing
Bayes
error
corresponds
to
increasingly difficult classification. Figure 2.4 shows RMS as a function of
Bayes error for sample size 20 for our fixed Zipf distributions, again for
b ¼ 16. In sum, these graphs show that performance for Bayesian MMSE
error estimators (denoted by BEE in the legends) is superior to resubstitution
and leave-one-out for most distributions and tends to be especially favorable
5
10
15
20
25
30
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
sample size
(a)
(b)
RMS deviation from true error
 
 
resub/plugin
loo
BEE
5
10
15
20
25
30
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
sample size
RMS deviation from true error
 
 
resub/plugin
loo
BEE
Figure 2.3
RMS deviation from true error for discrete classification and uniform priors with
respect to sample size (c ¼ 0.5): (a) b ¼ 16, Bayes error ¼ 0.1; (b) b ¼ 16, Bayes error ¼ 0.2.
[Reprinted from (Dalton and Dougherty, 2011b).]
42
Chapter 2

with moderate to high Bayes errors and small sample sizes. From Fig. 2.4, it
appears that performance of the Bayesian MMSE error estimators tends to be
favorable across a wide range of distributions, while the other error
estimators, especially resubstitution, favor a small Bayes error. While the
Bayesian MMSE error estimator is optimal with respect to MSE relative to
the prior distribution and sampling distribution, it need not be optimal for a
specific distribution. A clear weakness with uniform priors occurs when the
Bayes error is very small.
To explain this latter phenomenon, suppose that the true distributions are
perfectly separated by the bins, for instance, p1 ¼ 1 and qb ¼ 1, thereby giving
a Bayes error of zero. If we observe five sample points from each class, these
will be perfectly separated into two bins with nonzero probability, and the
histogram classifier will assign the correct class to each of these bins.
Resubstitution and leave-one-out will both give estimates of 0, which is
correct; however, since the true distribution is unknown, the Bayesian MMSE
error estimator with ay
i ¼ 1 for all i and y assigns a nonzero value to all bins in
the effective density, even bins for which no points have been observed. This
improves the average performance, but not for cases with zero (or very small)
Bayes error. Of course, if it is suspected before the experiment that the Bayes
error is very low, or if any additional information about the parameters is
available to incorporate into the priors, then we can improve the Bayesian
MMSE error estimator using informed priors as demonstrated in Example 2.1
with beta priors and b ¼ 2.
0
0.1
0.2
0.3
0.4
0.5
0
0.1
0.2
0.3
0.4
Bayes error
RMS deviation from true error
 
 
resub/plugin
loo
BEE
Figure 2.4
RMS deviation from true error for discrete classification and uniform priors with
respect to Bayes error (c ¼ 0.5, b ¼ 16, n ¼ 20). [Reprinted from (Dalton and Dougherty,
2011b).]
43
Optimal Bayesian Error Estimation

Example 2.3. This example considers average performance with uniform
priors, where simulations assume that the feature-label distribution is
generated randomly from a given prior. A summary of the methodology is
shown in Fig. 2.5. We emulate the entire Bayesian model by specifying
hyperparameters for priors over c, u0, and u1, drawing random parameters
according to the prior distribution (step 1), generating a random sample for
each fixed feature-label distribution (step 2A), updating the prior to a
posterior (step 2B), training a classifier (step 2C), and evaluating the true error
and estimates of the true error (step 2D). Steps 2A through 2D are essentially
the same as steps A through D in the simulation methodology outlined in
Fig. 2.1. Step 1 is repeated to produce T different feature-label distributions
(corresponding to the randomly selected parameters), and steps 2A through
2D are repeated t times for each fixed feature-label distribution, producing a
total of tT samples and sets of output results in each simulation. Typically, for
each sample and error estimator, we evaluate the difference ˆεn  εn and the
squared difference ðˆεn  εnÞ2, and average these to produce Monte Carlo
approximations of bias and RMS over the prior and sampling distributions.
As long as the model and prior used for Bayesian MMSE error estimation are
the same as those used to generate random parameters, the Bayesian MMSE
error estimator is unbiased and has optimal RMS in these simulations.
We present results in the Bayesian framework for non-informative uniform
priors and an arbitrary number of bins. We again use random sampling and
histogram classification throughout. Figure 2.6 gives the average RMS
deviation from the true error, as a function of sample size, over all distributions
in the model with uniform priors for the bin probabilities and c. To generate
these graphs, the true distributions and c are randomly selected, a random
sample is generated according to the current distributions, and the performance
for each error estimator is calculated. This procedure is repeated to obtain
Monte Carlo approximations of the RMS deviation from the true error. This
figure indicates that the Bayesian MMSE error estimator has excellent average
performance for each fixed n. Indeed, it is optimal with respect to MSE relative
to the prior distribution and sampling distribution. The Bayesian MMSE error
estimator shows great improvement over resubstitution and leave-one-out,
especially for small samples or a large number of bins. Also note that, as has
Figure 2.5
Simulation methodology for fixed sample sizes under random feature-label
distributions drawn from a Bayesian framework.
44
Chapter 2

been demonstrated analytically for discrete histogram classification, resubstitu-
tion is superior to leave-one-out for small numbers of bins but poorer for large
numbers (on account of increasing bias) (Braga-Neto and Dougherty, 2005).
The Bayesian MMSE error estimator is unbiased when averaged over all
distributions in the model and all possible samples from these distributions;
however, it can be quite biased for a fixed distribution. Figure 2.7 examines
bias and deviation variance for a 16-bin problem using resubstitution, leave-
one-out, and the Bayesian MMSE error estimator with uniform priors. The
left column shows bias and deviation variance under random distributions
drawn from a uniform prior. Unbiasedness is observed in Fig. 2.7(a), which
shows the average bias over all distributions and samples with respect to
sample size. Figure 2.7(c) shows the significant small-sample advantage in
average deviation variance of the Bayesian MMSE error estimator with
respect to leave-one-out and resubstitution.
The right column in Fig. 2.7 shows bias and deviation variance under
fixed Zipf distributions with c ¼ 0.5 and sample size 20 as a function of the
Bayes error of feature-label distributions governed by the prior distribution
(not averaged across the prior distribution). Leave-one-out is nearly unbiased
with a very large deviation variance, while resubstitution is quite optimisti-
cally biased with a much lower deviation variance. In contrast, the Bayesian
MMSE error estimator is pessimistically biased when the true classes are well
separated (low Bayes error), but tends to be optimistically biased when the
true classes are highly mixed (high Bayes error). This correlates with our
previous RMS graphs, where the performance of the error estimator is usually
best with moderate Bayes error.
5
10
15
20
25
30
0.05
0.1
0.15
0.2
0.25
0.3
0.35
sample size
RMS deviation from true error
 
 
resub/plugin
loo
BEE
Figure 2.6
RMS deviation from true error for discrete classification and uniform priors with
respect to sample size (b ¼ 16, bin probabilities and c uniform). [Reprinted from (Dalton and
Dougherty, 2011b).]
45
Optimal Bayesian Error Estimation

2.5 Gaussian Model
In the Gaussian model, each sample point is a column vector of D
multivariate Gaussian features; thus, the feature space is X ¼ RD. For
y ∈f0, 1g, assume a Gaussian distribution with parameters uy ¼ ðmy, lyÞ,
where my is the mean of the class-conditional distribution, and ly is a
collection of parameters that determine the covariance matrix Sy of the class.
We distinguish between ly and Sy to enable us to impose a structure on the
covariance. In fact, we will consider four types of models: a fixed covariance
ðSy is known perfectlyÞ, a scaled identity covariance having uncorrelated
features with equal variances ðly ¼ s2y is a scalar and Sy ¼ s2yIDÞ, a diagonal
covariance ðly ¼ ½s2
y,1, s2
y,2, : : : , s2
y,DT is a length D vector corresponding to
5
10
15
20
25
30
−0.4
−0.3
−0.2
−0.1
0
0.1
sample size
bias
resub/plugin
loo
BEE
(a)
0
0.1
0.2
0.3
0.4
0.5
−0.5
0
0.5
Bayes error
bias
resub/plugin
loo
BEE
(b)
5
10
15
20
25
30
0
0.02
0.04
0.06
0.08
sample size
deviation variance
resub/plugin
loo
BEE
(c)
0
0.1
0.2
0.3
0.4
0.5
0
0.005
0.01
0.015
0.02
0.025
Bayes error
deviation variance
resub/plugin
loo
BEE
(d)
Figure 2.7
Bias and deviation variance from true error for discrete classification and
uniform priors (b ¼ 16): (a) bias versus sample size (bin probabilities and c uniform); (b) bias
versus Bayes error (n ¼ 20, c ¼ 0.5); (c) deviation variance versus sample size (bin
probabilities and c uniform); (d) deviation variance versus Bayes error (n ¼ 20, c ¼ 0.5).
[Reprinted from (Dalton and Dougherty, 2011b).]
46
Chapter 2

the diagonal elements of SyÞ, and a general (unconstrained, but valid)
covariance matrix Sy ¼ ly. The parameter space of my is RD. The parameter
space of ly, denoted by Ly, must be carefully defined to permit only valid
covariance matrices. We write Sy without explicitly showing its dependence
on ly; that is, we write Sy instead of SyðlyÞ. A multivariate Gaussian
distribution with mean m and covariance S is denoted by f m,SðxÞ; therefore,
the parameterized class-conditional distributions are f uyðxjyÞ ¼ f my,SyðxÞ.
In addition to the four covariance structures described above, we use two
models to relate the covariances of each class: independent and homoscedas-
tic. We next characterize the independent covariance and homoscedastic
covariance models.
2.5.1 Independent covariance model
In the independent covariance model, we assume that c, u0 ¼ ðm0, l0Þ, and
u1 ¼ ðm1, l1Þ are all independent prior to observing the data so that
pðuÞ ¼ pðcÞpðu0Þpðu1Þ. Assuming that p(c) and p∗ðcÞ have been established,
we have to define priors pðuyÞ and find posteriors p∗ðuyÞ for both classes; note
that Eq. 2.16 applies in the independent covariance case. We begin by
specifying conjugate priors for u0 and u1. Let n be a real number, m a length D
real vector, k a real number (although we may restrict k to be an integer to
guarantee nice properties for the Bayesian MMSE error estimator), and S a
symmetric D  D matrix. Define
f mðm; n, m, lÞ ¼ jSj1
2 exp

 n
2 ðm  mÞTS1ðm  mÞ

,
(2.58)
f cðl; k, SÞ ¼ jSjkþDþ1
2 etr

 1
2 SS1

,
(2.59)
where S is a function of l, and etrð⋅Þ ¼ expðtrð⋅ÞÞ. If n > 0, then fm is an
unnormalized Gaussian distribution with mean m and covariance S/n.
Likewise, fc simplifies to many different forms, depending on the definition of
SðlÞ; for example, if S ¼ l, k . D−1, and S is symmetric positive definite,
then fc is an unnormalized inverse-WishartðS, kÞ distribution. In general,
fm and fc are not necessarily normalizable and thus can be used for improper
priors.
Considering one class y ∈f0, 1g at a time, we assume that Sy is invertible
(almost surely), and that for invertible Sy our priors for uy are of the form
pðuyÞ ¼ pðmyjlyÞpðlyÞ,
(2.60)
47
Optimal Bayesian Error Estimation

where
pðmyjlyÞ ∝f mðmy; ny, my, lyÞ,
(2.61)
pðlyÞ ∝f cðly; ky, SyÞ:
(2.62)
Alternatively, pðlyÞ may be set to a point mass at a fixed covariance. In this
way, different covariance structures (fixed, scaled identity, diagonal, and
general) may be imposed on each class.
If ny . 0, then the prior pðmyjlyÞ for the mean conditioned on the
covariance is proper and Gaussian with mean my and covariance Sy∕ny. The
hyperparameter my can be viewed as a target for the mean, where the larger ny
is, the more localized the prior is about my.
In the general covariance model where Sy ¼ ly, pðlyÞ is proper if
ky . D  1 and Sy is symmetric positive definite. If in addition ny . 0, then
pðuyÞ is a normal-inverse-Wishart distribution, which is the conjugate prior
for normal distributions with unknown mean and covariance (DeGroot, 1970;
Raiffa and Schlaifer, 1961). When ky . D þ 1,
Ep½Sy ¼
1
ky  D  1 Sy:
(2.63)
The larger ky is, the more certainty we have about Sy in the prior. In the other
covariance models, properties and constraints on ky and Sy may be different.
Theorem 2.4 (Dalton and Dougherty, 2013a). In the independent covariance
model with unknown covariances, assuming that p∗ðuyÞ is normalizable, the
posterior distributions possess the same form as the priors and satisfy
p∗ðuyÞ ∝f mðmy; n∗y, m∗y, lyÞf cðly; k∗y, S∗yÞ,
(2.64)
with updated hyperparameters
n∗y ¼ ny þ ny,
(2.65)
m∗y ¼ nymy þ ny ˆmy
ny þ ny
,
(2.66)
k∗y ¼ ky þ ny,
(2.67)
S∗y ¼ Sy þ ðny  1Þ ˆSy þ
nyny
ny þ ny
ð ˆmy  myÞð ˆmy  myÞT:
(2.68)
Proof. For notational ease, in the proof we write ky, Sy, ny, and my as k, S, n,
and m, respectively. Then, for fixed k, S, n, and m, the posterior probabilities
of the distribution parameters are found from Eq. 2.17. After some
48
Chapter 2

simplification, we have
p∗ðuyÞ ∝pðuyÞjSyj
ny
2 etr

 ny  1
2
ˆSyS1
y

 exp

 ny
2 ðmy  ˆmyÞTS1
y ðmy  ˆmyÞ

:
(2.69)
Our prior has a similar form to this expression and can be merged with the rest
of the equation, giving
p∗ðuyÞ ∝jSyj
kþnyþDþ2
2
etr

 1
2
	
S þ ðny  1Þ ˆSy

S1
y

exp

 M
2

,
(2.70)
where, as long as either ny > 0 or n > 0,
M ¼ nyðmy  ˆmyÞTS1
y ðmy  ˆmyÞ þ nðmy  mÞTS1
y ðmy  mÞ
¼ ðny þ nÞ

my  ny ˆmy þ nm
ny þ n
T
S1
y

my  ny ˆmy þ nm
ny þ n

þ
nny
n þ ny
ð ˆmy  mÞTS1
y ð ˆmy  mÞ
¼ ðny þ nÞ

my  ny ˆmy þ nm
ny þ n
T
S1
y

my  ny ˆmy þ nm
ny þ n

þ
nny
n þ ny
trðð ˆmy  mÞð ˆmy  mÞTS1
y Þ:
(2.71)
This leads to Eq. 2.64.
▪
The parameters in Eqs. 2.65 through 2.68 may be viewed as the updated
parameters after observing the data. Similar results are found in (DeGroot,
1970). Note that the choice of ly will affect the proportionality constant in
p∗ðuyÞ. Rewriting Eq. 2.66 as
m∗y ¼
1
1 þ ny∕ny
my þ
1
ny∕ny þ 1 ˆmy
(2.72)
shows the weighting effect of the prior and the data. Moreover, as ny →`, the
first summand converges to zero and the second summand converges to
the sample mean (although here convergence must be characterized relative to
the random vector ˆmy, a topic we will take up in a general framework when
discussing error-estimation consistency). Similar comments apply to S∗y and
the sample covariance.
For an unknown covariance, we may also write the posterior probability as
p∗ðuyÞ ¼ p∗ðmy jl yÞp∗ðlyÞ,
(2.73)
49
Optimal Bayesian Error Estimation

where
p∗ðmyjlyÞ ¼ f m∗y, 1
n∗ySyðmyÞ,
(2.74)
p∗ðlyÞ ∝jSyj
k∗yþDþ1
2
etr

 1
2 S∗yS1
y

:
(2.75)
For a fixed covariance matrix, the posterior density p∗ðmyjlyÞ for the mean is
still given by Eq. 2.74. Assuming that n∗y . 0, p∗ðmyjlyÞ is always normalizable
and Gaussian. The validity of p∗ðlyÞ depends on the definition of ly.
The model allows for improper priors. Some useful examples of improper
priors occur when Sy ¼ 0DD and ny ¼ 0. In this case, our prior has the form
pðuyÞ ∝jSyj
kyþDþ2
2
:
(2.76)
If ky þ D þ 2 ¼ 0, we obtain the flat prior used by Laplace (de Laplace, 1812).
Alternatively, if ly ¼ Sy, then with ky ¼ 0 we obtain the Jeffreys rule prior,
which is designed to be invariant to differentiable one-to-one transformations
of the parameters (Jeffreys, 1946, 1961), and with ky ¼ 1 we obtain the
independent Jeffreys prior, which uses the same principle as the Jeffreys rule
prior but treats the mean and covariance matrix as independent parameters.
When improper priors are used, the posterior must always be a valid
probability density; e.g., for the general covariance model we require that
n∗y . 0, k∗y . D  1, and that S∗y be symmetric positive definite.
2.5.2 Homoscedastic covariance model
In this section, we modify the Gaussian Bayesian model to allow unknown
means and a common (homoscedastic) unknown covariance matrix; that is, we
assume that uy ¼ ðmy, lÞ, where l ðand the covariance SÞ is the same for class
0 and 1. The homoscedastic known covariance case is covered in Section 2.5.1.
Thus, u ¼ ðc, m0, m1, lÞ. We further assume that c is independent so that
Eq. 2.11 applies, and that m0 and m1 are independent given l. Hence,
pðu0, u1Þ ¼ pðm0jm1, lÞpðm1jlÞpðlÞ
¼ pðm0jlÞpðm1jlÞpðlÞ:
(2.77)
Given the definitions in Eqs. 2.58 and 2.59, we assume that S is invertible
(almost surely) and that for invertible S, pðmyjlÞ ∝f mðmy; ny, my, lÞ for
y ∈f0, 1g and pðlÞ ∝f cðl; k, SÞ. Constraints and interpretations for the
hyperparameters ny, my, k, and S are similar to those of ny, my, ky, and Sy in
the independent covariance model, except that now k and S represent both
50
Chapter 2

classes. Any covariance structure may be assumed by pðlÞ, with the same
structure being used in both classes.
Given a sample Sn, the posterior of u0 and u1 is given by
p∗ðu0, u1Þ ¼ f ðm0, m1, ljSnÞ:
(2.78)
Theorem 2.5 (Dalton and Dougherty, 2013a). In the homoscedastic covariance
model, assuming that p∗ðu0, u1Þ is normalizable, the posterior distribution
possesses the same form as the prior and satisfies
p∗ðu0, u1Þ ∝f mðm0; n∗
0, m∗
0, lÞf mðm1; n∗
1, m∗
1, lÞf cðl; k∗, S∗Þ,
(2.79)
where n∗y is given in Eq. 2.65, m∗y is given in Eq. 2.66, and
k∗¼ k þ n,
(2.80)
S∗¼ S þ
X
1
y¼0
ðny  1Þ ˆSy þ
nyny
ny þ ny
ð ˆmy  myÞð ˆmy  myÞT:
(2.81)
Proof. From Bayes’ theorem,
f ðm0, m1, ljSnÞ ∝f ðSnjm0, m1, lÞf ðm0, m1, lÞ
¼
"Y
n0
i¼1
f m0,Sðx0
i Þ
#"Y
n1
i¼1
f m1,Sðx1
i Þ
#
pðm0jlÞpðm1jlÞpðlÞ
∝
"Y
n0
i¼1
f m0,Sðx0
i Þ
#
f mðm0; n0, m0, lÞf cðl; k, SÞ

"Y
n1
i¼1
f m1,Sðx1
i Þ
#
f mðm1; n1, m1, lÞ:
(2.82)
As in the independent covariance model,
f mðm0; n0, m0, lÞf cðl; k, SÞ
Y
n0
i¼1
f m0,Sðx0
i Þ ∝f mðm0; n∗
0, m∗
0, lÞf cðl; k∗
0, S∗
0Þ
(2.83)
with updated hyperparameters given by
n∗
0 ¼ n0 þ n0,
(2.84)
51
Optimal Bayesian Error Estimation

m∗
0 ¼ n0m0 þ n0 ˆm0
n0 þ n0
,
(2.85)
k∗
0 ¼ k þ n0,
(2.86)
S∗
0 ¼ S þ ðn0  1Þ ˆS0 þ
n0n0
n0 þ n0
ð ˆm0  m0Þð ˆm0  m0ÞT:
(2.87)
These equations have the same form as the updated hyperparameters in
Theorem 2.4. At this point we have
f ðm0, m1, ljSnÞ ∝f mðm0; n∗
0, m∗
0, lÞ
 f mðm1; n1, m1, lÞf cðl; k∗
0, S∗
0Þ
Y
n1
i¼1
f m1,Sðx1
i Þ:
(2.88)
Once again,
f mðm1; n1, m1, lÞf cðl; k∗
0, S∗
0Þ
Y
n1
i¼1
f m1,Sðx1
i Þ ∝f mðm1; n∗
1, m∗
1, lÞf cðl; k∗, S∗Þ
(2.89)
with new updated hyperparameters given by
n∗
1 ¼ n1 þ n1,
(2.90)
m∗
1 ¼ n1m1 þ n1 ˆm1
n1 þ n1
,
(2.91)
k∗¼ k∗
0 þ n1 ¼ k þ n,
(2.92)
S∗¼ S∗
0 þ ðn1  1Þ ˆS1 þ
n1n1
n1 þ n1
ð ˆm1  m1Þð ˆm1  m1ÞT
¼ S þ ðn0  1Þ ˆS0 þ
n0n0
n0 þ n0
ð ˆm0  m0Þð ˆm0  m0ÞT
þ ðn1  1Þ ˆS1 þ
n1n1
n1 þ n1
ð ˆm1  m1Þð ˆm1  m1ÞT:
(2.93)
Combining these results yields the theorem.
▪
We require each component in Eq. 2.79 to be normalizable so that
p∗ðu0, u1Þ is proper. The normalized version of f mðmy; n∗y, m∗y, lÞ is
N ðm∗y, S∕n∗yÞ for y ∈f0, 1g (we require that n∗y . 0), and we name the
normalized distributions p∗ðmyjlÞ. Similarly, we let p∗ðlÞ be the normalized
version of f cðl; k∗, S∗Þ.Therefore, we may write
52
Chapter 2

p∗ðu0, u1Þ ¼ p∗ðm0jlÞp∗ðm1jlÞp∗ðlÞ:
(2.94)
Furthermore, the marginal posterior of uy is
p∗ðuyÞ ¼ p∗ðmyjlÞp∗ðlÞ:
(2.95)
2.5.3 Effective class-conditional densities
This section provides effective class-conditional densities for different
covariance models. Since the effective class-conditional densities are found
separately, a different covariance model may be used for each class, and these
results apply for both the independent and homoscedastic covariance models.
To simplify notation, in this section and the next section, we denote
hyperparameters without subscripts. The equations for n∗and m∗are given by
Eqs. 2.65 and
2.66, respectively, in both independent and homoscedastic
models. The equations for k∗and S∗are given by Eqs. 2.67 and 2.68,
respectively, in independent models, and by Eqs. 2.80 and 2.81, respectively,
in homoscedastic models. We first consider a fixed covariance matrix.
Theorem 2.6 (Dalton and Dougherty, 2013a). Assuming that n∗. 0 and that
Sy is symmetric positive definite, the effective class-conditional density for the
fixed
covariance
model
is
Gaussian
with
mean
m∗
and
covariance
½ðn∗þ 1Þ∕n∗Sy:
f UðxjyÞ ¼ f m∗, n∗þ1
n∗SyðxÞ
¼
1
ð2pÞ
D
2j n∗þ1
n∗Syj
1
2
 exp

 1
2 ðx  m∗ÞT
n∗þ 1
n∗
Sy
1
ðx  m∗Þ

:
(2.96)
Proof. From the definition of the effective class-conditional distribution, and
given that the posterior p∗ðmyjSyÞ is essentially f mðmy; n∗, m∗, SyÞ (the latter
is not normalized to have an integral equal to 1),
f UðxjyÞ ¼
Z
RD f my,SyðxÞp∗ðmyjSyÞdmy
¼
Z
RD
1
ð2pÞ
D
2jSyj
1
2 exp

 1
2 ðx  myÞTS1
y ðx  myÞ


ðn∗Þ
D
2
ð2pÞ
D
2jSyj
1
2 exp

 n∗
2 ðmy  m∗ÞTS1
y ðmy  m∗Þ

dmy:
(2.97)
53
Optimal Bayesian Error Estimation

Some algebra shows that
ðx  myÞTS1
y ðx  myÞ þ n∗ðmy  m∗ÞTS1
y ðmy  m∗Þ
¼ ðn∗þ 1Þ

my  x þ n∗m∗
n∗þ 1
T
S1
y

my  x þ n∗m∗
n∗þ 1

þ
n∗
n∗þ 1 ðx  m∗ÞTS1
y ðx  m∗Þ
¼ ðn∗þ 1Þðmy  x0ÞTS1
y ðmy  x0Þ þ
n∗
n∗þ 1 ðx  m∗ÞTS1
y ðx  m∗Þ,
(2.98)
where x0 ¼ ðx þ n∗m∗Þ∕ðn∗þ 1Þ. Hence,
f UðxjyÞ ¼
ðn∗Þ
D
2
ð2pÞDjSyj exp


n∗
2ðn∗þ 1Þ ðx  m∗ÞTS1
y ðx  m∗Þ


Z
RD exp

 n∗þ 1
2
ðmy  x0ÞTS1
y ðmy  x0Þ

dmy
¼
ðn∗Þ
D
2
ðn∗þ 1Þ
D
2ð2pÞ
D
2jSyj
1
2 exp


n∗
2ðn∗þ 1Þ ðx  m∗ÞTS1
y ðx  m∗Þ

¼ f m∗, n∗þ1
n∗SyðxÞ,
(2.99)
as stated in the theorem.
▪
See (Geisser, 1964) for a special case of Theorem 2.6 assuming flat priors
on the means. We next consider a scaled identity covariance matrix, that is,
ly ¼ s2y and Sy ¼ s2yID. In this model, the parameter space of s2y is
Ly ¼ ð0, `Þ. The following lemma shows that the posterior on s2y is inverse-
gamma distributed. This fact also holds for the prior, i.e., the special case
where there is no data. Thus, the scaled identity model equivalently assumes
an inverse-gamma prior on the variance shared between all features.
Lemma 2.1 (Dalton and Dougherty, 2011c). Under the scaled identity model,
assuming
that
p∗ðuyÞ
is
normalizable,
p∗ðs2yÞ
has
an
inverse-gamma
distribution,
p∗ðs2yÞ ¼
1
GðaÞ ba
1
ðs2yÞaþ1 exp

 b
s2y

,
(2.100)
with shape and scale parameters
54
Chapter 2

a ¼ ðk∗þ D þ 1ÞD
2
 1,
(2.101)
b ¼ 1
2 trðS∗Þ:
(2.102)
For a proper posterior, we require that a . 0 and b . 0, or, equivalently, that
ðk∗þ D þ 1ÞD . 2 and S∗is symmetric positive definite.
Proof. In the scaled identity covariance model, we set ly ¼ s2y and Sy ¼ s2yID.
From Theorems 2.4 and 2.5, the posterior p∗ðs2yÞ has the form given in
Eq. 2.59. Plugging in Sy ¼ s2yID and the prior or posterior hyperparameters of
either the independent or homoscedastic models, this results in a univariate
function on s2y that is an inverse-gamma distribution, as in Eq. 2.100, for
which the normalization is well-known.
▪
Theorem 2.7 (Dalton and Dougherty, 2013a). Assuming that n∗. 0, a . 0,
and b . 0, where a and b are given by Eqs. 2.101 and 2.102, respectively, the
effective class-conditional density for the scaled identity covariance model is a
multivariate t-distribution with 2a degrees of freedom, location vector m∗, and
scale matrix ½bðn∗þ 1Þ∕ðan∗ÞID:
f UðxjyÞ ¼
1
ð2aÞ
D
2p
D
2
 bðn∗þ1Þ
an∗
ID

1
2 ⋅Gða þ D
2Þ
GðaÞ


1 þ 1
2a ðx  m∗ÞT
bðn∗þ 1Þ
an∗
ID
1
ðx  m∗Þ
aD
2,
(2.103)
This distribution is proper, the mean exists and is m∗as long as a . 0.5, and the
covariance exists and is ½bðn∗þ 1Þ∕ðða  1Þn∗ÞID as long as a . 1.
Proof. By definition,
f UðxjyÞ ¼
Z `
0
Z
RD f my, s2yIDðxÞp∗ðmyjs2yÞp∗ðs2yÞdmyds2y
¼
Z `
0
f m∗, n∗þ1
n∗s2yIDðxÞp∗ðs2yÞds2y,
(2.104)
where we have applied Theorem 2.6 for fixed covariance modeling.
Continuing,
55
Optimal Bayesian Error Estimation

f UðxjyÞ ¼
Z `
0
ðn∗Þ
D
2
ðn∗þ 1Þ
D
2ð2pÞ
D
2ðs2yÞ
D
2
 exp


n∗
2s2yðn∗þ 1Þ ðx  m∗ÞTðx  m∗Þ


1
GðaÞ ba
1
ðs2yÞaþ1 exp

 b
s2y

ds2y
¼
Z `
0
ðn∗Þ
D
2ba
ðn∗þ 1Þ
D
2ð2pÞ
D
2GðaÞðs2yÞaþD
2þ1
 exp



b þ
n∗
2ðn∗þ 1Þ ðx  m∗ÞTðx  m∗Þ
 1
s2y

ds2y
¼
ðn∗Þ
D
2baGða þ D
2Þ
ðn∗þ 1Þ
D
2ð2pÞ
D
2GðaÞ½b þ
n∗
2ðn∗þ1Þ ðx  m∗ÞTðx  m∗ÞaþD
2 ,
(2.105)
where the last line follows because the integrand is essentially an inverse-
gamma distribution. This is a multivariate t-distribution as stated in the
theorem. It is proper because bðn∗þ 1Þ∕ðan∗Þ . 0 (so the scaled matrix is
positive definite) and 2a . 0.
▪
Next we consider the diagonal covariance model, where Sy is unknown
with a diagonal structure; i.e., ly ¼ ½s2
y,1, s2
y,2, : : : , s2
y,DT and s2
y,i is the ith
diagonal entry of Sy. In this model, the parameter space of s2
y,i is ð0, `Þ for all
i ¼ 1, 2, : : : , D; thus, Ly ¼ ð0, `ÞD. The following lemma shows that the s2
y,i
are independent and inverse-gamma distributed. This holds for both the prior
and posterior. Thus, the diagonal covariance model equivalently assumes
independent inverse-gamma priors on the variance of each feature.
Lemma 2.2. Under the diagonal covariance model, assuming that p∗ðuyÞ is
normalizable, p∗ðs2
y,1, s2
y,2, : : : , s2
y,DÞ is the joint density of D independent
inverse-gamma distributions,
p∗ðs2
y,1, s2
y,2, : : : , s2
y,DÞ ¼
Y
D
i¼1
1
GðaÞ ba
i
1
ðs2
y,iÞaþ1 exp

 bi
s2
y,i

,
(2.106)
with shape and scale parameters
a ¼ k∗þ D  1
2
,
(2.107)
56
Chapter 2

bi ¼ s∗
ii
2 ,
(2.108)
where s∗
ii is the ith diagonal element of S∗. For a proper posterior, we require
that
a . 0
and
bi . 0
for
all
i ¼ 1, 2, : : : , D,
or,
equivalently,
that
k∗þ D  1 . 0 and s∗
ii . 0.
Proof. From Theorems 2.4 and 2.5, the posterior p∗ðs2
y,1, s2
y,2, : : : , s2
y,DÞ has
the form given in Eq. 2.59. Plugging in the posterior hyperparameters of either
the independent or homoscedastic models, this results in a function on
s2
y,1, s2
y,2, : : : , s2
y,D of the form
 Y
D
i¼1
s2
y,i
!kþDþ1
2
exp
 
 1
2
X
D
j¼1
s∗
jj
s2
y,j
!
¼
Y
D
i¼1
ðs2
y,iÞkþDþ1
2
exp

 s∗
ii
2s2
y,i

:
(2.109)
Each term in the product on the right-hand side is an inverse-gamma
distribution with parameters given in Eqs. 2.107 and 2.108.
▪
Theorem 2.8. Assuming that n∗. 0, a . 0, and bi . 0 for all i ¼ 1, 2, : : : , D,
where a and bi are given by Eqs. 2.107 and 2.108, respectively, the effective
class-conditional density for the diagonal covariance model is the joint density
between D independent non-standardized Student’s t-distributions. That is,
f UðxjyÞ ¼
Y
D
i¼1
Gða þ 1
2Þ
GðaÞp
1
2ð2aÞ
1
2
biðn∗þ 1Þ
an∗
1
2


1 þ 1
2a
biðn∗þ 1Þ
an∗
1
ðxi  m∗
i Þ2
ðaþ1
2Þ
,
(2.110)
where xi is the ith element in x, and m∗
i is the ith element in m∗. The ith
distribution has 2a ¼ k∗þ D  1 degrees of freedom, location parameter m∗
i ,
and scale parameter
biðn∗þ 1Þ
an∗
¼
s∗
iiðn∗þ 1Þ
ðk∗þ D  1Þn∗:
(2.111)
f UðxjyÞ is proper, the mean exists as long as a . 0.5 and equals m∗, and the
covariance exists as long as a . 1 and equals a diagonal matrix with ith
diagonal entry
biðn∗þ 1Þ
an∗
⋅
2a
2a  2 ¼
s∗
iiðn∗þ 1Þ
ðk∗þ D  3Þn∗:
(2.112)
57
Optimal Bayesian Error Estimation

Proof. By definition,
f UðxjyÞ ¼
Z
Ly
Z
RD f my,SyðxÞp∗ðmyjlyÞp∗ðlyÞdmydly
¼
Z
Ly
f m∗, n∗þ1
n∗SyðxÞp∗ðlyÞdly,
(2.113)
where we have applied Theorem 2.6 for fixed covariance modeling.
Continuing,
f UðxjyÞ ¼
Z `
0
···
Z `
0
ðn∗Þ
D
2
ðn∗þ 1Þ
D
2ð2pÞ
D
2ðQD
i¼1 s2
y,iÞ
1
2
 exp


n∗
2ðn∗þ 1Þ
X
D
i¼1
ðxi  m∗
i Þ2
s2
y,i


Y
D
j¼1
1
GðaÞ ba
j
1
ðs2
y,jÞaþ1 exp

 bj
s2
y,j

ds2
y,1 ··· ds2
y,D
¼
Y
D
i¼1
Z `
0

n∗
ðn∗þ 1Þ2ps2
y,i
1
2 exp

 n∗ðxi  m∗
i Þ2
2ðn∗þ 1Þs2
y,i


1
GðaÞ ba
i
1
ðs2
y,iÞaþ1 exp

 bi
s2
y,i

ds2
y,i:
(2.114)
Going further,
f UðxjyÞ ¼
Y
D
i¼1
1
GðaÞ ba
i

n∗
ðn∗þ 1Þ2p
1
2

Z `
0
 1
s2
y,i
aþ3
2 exp



bi þ n∗ðxi  m∗
i Þ2
2ðn∗þ 1Þ
 1
s2
y,i

ds2
y,i
¼
Y
D
i¼1
Gða þ 1
2Þ
GðaÞ
ba
i

n∗
ðn∗þ 1Þ2p
1
2
bi þ n∗ðxi  m∗
i Þ2
2ðn∗þ 1Þ
ðaþ1
2Þ
,
(2.115)
where the last line follows because the integrand is essentially an inverse-
gamma distribution. We can also write
58
Chapter 2

f UðxjyÞ ¼
Y
D
i¼1
Gða þ 1
2Þ
GðaÞp
1
2ð2aÞ
1
2
biðn∗þ 1Þ
an∗
1
2


1 þ 1
2a
biðn∗þ 1Þ
an∗
1
ðxi  m∗
i Þ2
ðaþ1
2Þ
:
(2.116)
Each term in the product is a non-standardized Student’s t-distribution as
stated in the theorem. The distribution is proper because biðn∗þ 1Þ∕ðan∗Þ . 0
(so the scale parameter is positive) and 2a . 0.
▪
Lastly, we consider the general covariance model, Sy ¼ ly, the parameter
space being the set of all symmetric positive definite matrices, which we
denote by Ly ¼ fSy : Sy ≻0g. We require the multivariate gamma function,
which is defined by the following integral over the space of D  D positive
definite matrices (O’Hagan and Forster, 2004; Mardia et al., 1979):
GDðaÞ ¼
Z
S≻0
jSjaDþ1
2 etrðSÞdS:
(2.117)
An equivalent formulation, one better suited for numerical approximations, is
given by
GDðaÞ ¼ pDðD1Þ∕4 Y
D
j¼1
G

a þ 1  j
2

:
(2.118)
Lemma 2.3 (Dalton and Dougherty, 2011c). Under the general covariance
model, assuming that p∗ðuyÞ is normalizable, p∗ðSyÞ has an inverse-Wishart
distribution,
p∗ðSyÞ ¼
jS∗j
k∗
2
2
k∗D
2 GDðk∗
2 Þ
jSyjk∗þDþ1
2
etr

 1
2 S∗S1
y

:
(2.119)
For a proper posterior, we require that k∗. D  1 and that S∗be symmetric
positive definite.
Proof. From Theorems 2.4 and 2.5, the posterior p∗ðSyÞ has the form given in
Eq. 2.59. Plugging in the posterior hyperparameters of either the independent
or homoscedastic models, this results in a function on Sy that is essentially an
inverse-Wishart distribution, as in Eq. 2.119, for which the normalization is
well-known (Muller and Stewart, 2006).
▪
59
Optimal Bayesian Error Estimation

Theorem 2.9 (Dalton and Dougherty, 2013a). Assuming that n∗. 0,
k∗. D  1, and S∗is symmetric positive definite, the effective class-conditional
density for the general covariance model is a multivariate t-distribution with
k ¼ k∗ D þ 1 degrees of freedom, location vector m∗, and scale matrix
½ðn∗þ 1Þ∕ððk∗ D þ 1Þn∗ÞS∗. That is,
f UðxjyÞ ¼
GðkþD
2 Þ
Gðk
2Þk
D
2p
D
2j
n∗þ1
ðk∗Dþ1Þn∗S∗j
1
2


1 þ 1
k ðx  m∗ÞT

n∗þ 1
ðk∗ D þ 1Þn∗S∗
1
ðx  m∗Þ
kþD
2 :
(2.120)
This distribution is proper, the mean exists and is m∗as long as k∗. D, and the
covariance exists and is ½ðn∗þ 1Þ∕ððk∗ D  1Þn∗ÞS∗as long as k∗. D þ 1.
Proof. By definition,
f UðxjyÞ ¼
Z
Ly
Z
RD f my,SyðxÞp∗ðmyjSyÞp∗ðSyÞdmydSy
¼
Z
Ly
f m∗, n∗þ1
n∗SyðxÞp∗ðSyÞdSy,
(2.121)
where in the last line we have used Theorem 2.6 for the fixed covariance
model. Continuing,
f UðxjyÞ ¼
Z
Ly
ðn∗Þ
D
2
ðn∗þ 1Þ
D
2ð2pÞ
D
2jSyj
1
2
 exp


n∗
2ðn∗þ 1Þ ðx  m∗ÞTS1
y ðx  m∗Þ


jS∗j
k∗
2
2
k∗D
2 GDðk∗
2 Þ
jSyjk∗þDþ1
2
etr

 1
2 S∗S1
y

dSy
¼
Z
Ly
ðn∗Þ
D
2
ðn∗þ 1Þ
D
2ð2pÞ
D
2 ⋅
jS∗j
k∗
2
2
k∗D
2 GDðk∗
2 Þ
jSyjk∗þDþ2
2
 etr

 1
2

S∗þ
n∗
n∗þ 1 ðx  m∗Þðx  m∗ÞT

S1
y

dSy:
(2.122)
The integrand is essentially an inverse-Wishart distribution and, therefore,
60
Chapter 2

f UðxjyÞ ¼
ðn∗Þ
D
2
ðn∗þ 1Þ
D
2ð2pÞ
D
2 ⋅
jS∗j
k∗
2
2
k∗D
2 GDðk∗
2 Þ

2
ðk∗þ1ÞD
2
GDðk∗þ1
2 Þ
S∗þ
n∗
n∗þ1 ðx  m∗Þðx  m∗ÞT
k∗þ1
2
¼
Gðk∗þ1
2 Þðn∗Þ
D
2jS∗j
k∗
2
Gðk∗Dþ1
2
Þðn∗þ 1Þ
D
2p
D
2

S∗þ
n∗
n∗þ 1 ðx  m∗Þðx  m∗ÞT

k∗þ1
2 :
(2.123)
This is a multivariate t-distribution as stated in the theorem. This distribution
is
proper
because
ðn∗þ 1Þ∕½ðk∗ D þ 1Þn∗ . 0
and
S∗
is
symmetric
positive definite (so the scale matrix is symmetric positive definite) and
k∗ D þ 1 . 0.
▪
See (Geisser, 1964) for a special case of Theorem 2.9 assuming flat priors
on the means and priors of the form pðS1
y Þ ∝jSyjK∕2 for K , ny on
independent and homoscedastic precision matrices.
2.5.4 Bayesian MMSE error estimator for linear classification
Suppose that the classifier discriminant is linear in form as in Eq. 1.16 with
gðxÞ ¼ aTx þ b, where vector a and scalar b are unrestricted functions of the
sample. The classifier predicts class 0 if gðxÞ ≤0 and 1 otherwise.
With fixed distribution parameters and nonzero a, the true error for a
class-y Gaussian distribution f my,Sy is given by Eq. 1.32. If a ¼ 0D, then ε0n and
ε1n are deterministically 0 or 1, depending on the sign of b, and the Bayesian
MMSE
error
estimator
is
either
Ep∗½c
(for
b . 0)
or
1  Ep∗½c
(for b ≤0). Hence, from here forward we assume that a ≠0D.
For a fixed invertible covariance Sy, we require that n∗. 0 to ensure that
the posterior p∗ðmyjlyÞ is proper.
Theorem 2.10 (Dalton and Dougherty, 2011c). In the Gaussian model with
fixed invertible covariance matrix Sy, assuming that n∗. 0,
ˆε y
n ¼ Ep∗½εy
n ¼ FðdÞ
(2.124)
for y ∈f0, 1g, where
d ¼ ð1Þygðm∗Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSya
q
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n∗
n∗þ 1
r
:
(2.125)
61
Optimal Bayesian Error Estimation

Proof. By Theorem 2.6, the effective class-conditional distribution f UðxjyÞ is
Gaussian with mean m∗and variance ½ðn∗þ 1Þ∕n∗Sy, as given in Eq. 2.96. By
Eq. 2.31,
ˆε y
n ðSn, cÞ ¼
Z
ð1ÞyðaTxþbÞ.0
f UðxjyÞdx:
(2.126)
This is the integral of a D-dimensional multivariate Gaussian distribution on
one side of a hyperplane, which is equivalent to Eq. 1.32. Hence,
ˆε y
n ðSn, cÞ ¼ F
0
B
@ ð1Þygðm∗Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTðn∗þ1
n∗SyÞa
q
1
C
A,
(2.127)
which is equivalent to the claimed result.
▪
Before giving the Bayesian MMSE error estimators for the scaled
covariance and general covariance Gaussian models, we require a technical
lemma. The lemma utilizes a Student’s t-CDF, which may be found using
standard lookup tables, or equivalently, a special function involving an Euler
integral called a regularized incomplete beta function defined by
Iðx; a, bÞ ¼
1
Bða, bÞ
Z x
0
ta1ð1  tÞb1dt
(2.128)
for 0 ≤x ≤1, a . 0, and b . 0, where the beta function Bða, bÞ normalizes
Ið⋅; a, bÞ so that Ið1; a, bÞ ¼ 1. We only need to evaluate Iðx; 1∕2, bÞ for
0 ≤x , 1 and b . 0. Although the integral does not have a closed-form
solution for arbitrary parameters, in Chapter 4 we will provide exact
expressions for Iðx; 1∕2, N∕2Þ for positive integers N. Restricting b to be an
integer or half-integer, which in all cases equivalently restricts k to be an
integer, guarantees that these equations may be applied so that Bayesian
MMSE error estimators for the Gaussian model with linear classification may
be evaluated exactly using finite sums of common single-variable functions.
Note that jj ⋅jjp denotes the p-norm for vectors and the entrywise p-norm for
matrices. Moreover, the sign function sgnðxÞ equals 1 if x is positive, 1 if x is
negative, and zero otherwise.
Lemma 2.4 (Dalton and Dougherty, 2011c). Let X have a multivariate
t-distribution with D dimensions, location vector m, symmetric positive definite
scale matrix S, and n . 0 degrees of freedom. To wit, its density is given by
f ðxÞ ¼
GðnþD
2 Þ
Gðn
2Þn
D
2p
D
2jSj
1
2½1 þ 1
n ðx  mÞTS1ðx  mÞ
nþD
2 :
(2.129)
62
Chapter 2

Then
Z
aTxþb≤0
f ðxÞdx ¼ Pr

Z , ðaTm þ bÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSa
p

¼ 1
2  1
2 sgnðaTm þ bÞI

ðaTm þ bÞ2
ðaTm þ bÞ2 þ nðaTSaÞ ; 1
2 , n
2

,
(2.130)
where Z is a Student’s t-random variable with n degrees of freedom.
Proof. If X is governed by a multivariate t-distribution with the given
parameters, and a is a nonzero vector, then Y ¼ aTX þ b is a non-
standardized Student’s t-random variable with location parameter aTm þ b,
positive scale parameter aTSa, and n degrees of freedom (Kotz and
Nadarajah, 2004). Let Z ¼ ½Y  ðaTm þ bÞ∕
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSa
p
, which is now standard
Student’s t with n degrees of freedom. The CDF for a Student’s t-random
variable is available in closed form (Shaw, 2004):
PrðZ , zÞ ¼ 1
2 þ sgnðzÞ
2
I

z2
z2 þ n ; 1
2 , n
2

:
(2.131)
Our desired integral is obtained by plugging in z ¼ ðaTm þ bÞ∕
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSa
p
.
▪
Theorem 2.11 (Dalton and Dougherty, 2011c). In the Gaussian model with
scaled identity covariance matrix Sy ¼ s2yID,
ˆε y
n ¼ Ep∗½εy
n ¼ 1
2

1 þ sgnðAÞI

A2
A2 þ 2b ; 1
2 , a

,
(2.132)
where
A ¼ ð1Þygðm∗Þ
jjajj2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n∗
n∗þ 1
r
,
(2.133)
a ¼ ðk∗þ D þ 1ÞD
2
 1,
(2.134)
b ¼ 1
2 trðS∗Þ,
(2.135)
and it is assumed that n∗. 0, a . 0, and b . 0.
Proof. By Theorem 2.7, the effective class-conditional density for the scaled
identity covariance model is the multivariate t-distribution f UðxjyÞ given in
63
Optimal Bayesian Error Estimation

Eq. 2.103. By Theorem 2.1 and Lemma 2.4,
ˆεy
nðSn, cÞ ¼ 1 
Z
ð1ÞyðaTxþbÞ≤0
f UðxjyÞdx
¼ 1
2 þ sgnðð1ÞyðaTm∗þ bÞÞ
2
 I
 
½ð1ÞyðaTm∗þ bÞ2
½ð1ÞyðaTm∗þ bÞ2 þ 2aaT	bðn∗þ1Þ
an∗
ID

a
; 1
2, 2a
2
!
¼ 1
2 þ sgnðð1ÞyðaTm∗þ bÞÞ
2
I
 
ðaTm∗þ bÞ2
ðaTm∗þ bÞ2 þ 2bðn∗þ1Þ
n∗
kak2
2
; 1
2, a
!
,
(2.136)
which completes the proof.
▪
Theorem 2.12 (Dalton and Dougherty, 2011c). In the Gaussian model with
general covariance matrix Sy ¼ ly and Ly containing all symmetric positive
definite matrices, assuming that n∗. 0, k∗. D  1, and S∗is symmetric
positive definite,
ˆε y
n ¼ Ep∗½εy
n ¼ 1
2

1 þ sgnðAÞI

A2
A2 þ 2b ; 1
2 , a

,
(2.137)
where
A ¼ ð1Þygðm∗Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n∗
n∗þ 1
r
,
(2.138)
a ¼ k∗ D þ 1
2
,
(2.139)
b ¼ aTS∗a
2
:
(2.140)
Proof. By Theorem 2.9, the effective class-conditional density for the general
covariance model is a multivariate t-distribution, given in Eq. 2.120. By
Theorem 2.1 and Lemma 2.4,
ˆε y
n ðSn, cÞ ¼ 1 
Z
ð1ÞyðaTxþbÞ≤0
f UðxjyÞdx
¼ 1
2 þ sgnðð1ÞyðaTm∗þ bÞÞ
2
 I

ðaTm∗þ bÞ2
ðaTm∗þ bÞ2 þ n∗þ1
n∗aTS∗a ; 1
2 , k∗ D þ 1
2

,
(2.141)
which completes the proof.
▪
64
Chapter 2

Consider the Gaussian model with a diagonal covariance matrix. By
Theorem 2.8 the effective class-conditional density f UðxjyÞ for the diagonal
covariance model is the joint density between independent non-standardized
Student’s t-distributions given in Eq. 2.110. By Theorem 2.1,
ˆε y
n ðSn, cÞ ¼ 1 
Z
ð1ÞyðaTxþbÞ≤0
f UðxjyÞdx
¼ 1  Pr

ð1Þy
X
D
i¼1
aiX i þ b

≤0

,
(2.142)
where ai is the ith element of a, and X 1, X 2, : : : , X D are independent
non-standardized Student’s t-random variables with the same degrees of
freedom. The issue now is to find the CDF for a sum of independent Student’s
t-random variables. This has been found in closed form if k∗þ D  1 is odd, in
which case it can be written as a finite mixture of t-distributions (Walker and
Saw, 1978; Kotz and Nadarajah, 2004). However, the procedure is quite
involved, and we refer readers to the references for details.
2.6 Performance in the Gaussian Model with LDA
This section presents simulation studies examining various aspects of
performance for the Bayesian MMSE error estimator in the Gaussian
models. First, we provide performance results under circular Gaussian
distributions, then under several non-circular Gaussian distributions. These
demonstrate robustness relative to the covariance modeling assumptions. We
then graph performance under Johnson distributions, which are outside the
assumed Gaussian model. These simulations illustrate robustness relative to
the Gaussian assumption. This is important in practice since we cannot
guarantee Gaussianity. The results will show that performance does
require nearly Gaussian distributions, but there is some degree of flexibility
(in skewness and kurtosis).
2.6.1 Fixed circular Gaussian distributions
Here we evaluate the performance of error estimators under fixed circular
Gaussian distributions with c ¼ 0.5 and random sampling. In all simulations,
the mean of class 0 is fixed at m0 ¼ 0D and the mean of class 1 at
m1 ¼

1
0D1

:
(2.143)
The covariance of each class is chosen to make the distributions mirror images
with respect to the hyperplane between the two means. This plane is the
optimal linear classifier, and the classifier designed from the data is meant to
approximate it. In this section, the covariances of both classes are scaled
65
Optimal Bayesian Error Estimation

identity matrices, with the same scaling factor s2 in both classes, i.e.,
S0 ¼ S1 ¼ s2ID. The scale s2 of the covariance matrix is used to control the
Bayes error (Eqs. 1.33 and 1.34), where a low Bayes error corresponds to
small variance and high Bayes error to high variance.
The simulation is based on Fig. 2.1. A random sample is drawn from the
fixed distribution (step A), the prior is updated to a posterior to use in
Bayesian MMSE error estimation later (step B), and the sample is used,
without feature selection, to train an LDA classifier defined by
a ¼ ˆS1ð ˆm1  ˆm0Þ,
(2.144)
b ¼  1
2 ð ˆm1  ˆm0ÞT ˆS1ð ˆm1 þ ˆm0Þ þ ln n1
n0
(2.145)
(see Eqs. 1.24 and 1.25), where the pooled covariance matrix ˆS is given by
Eq. 1.28 (step C). In step D, the true error of this classifier is calculated via
Eq. 1.32 using the fixed true distribution parameters. The same sample is used
to find five non-parametric error estimates: resubstitution, leave-one-out,
cross-validation, 0.632 bootstrap, and bolstered resubstitution. We also find a
parametric plug-in error estimate, which is computed via Eq. 1.32 using the
sample mean and sample covariance in place of the real values, and Eq. 1.3
using the a priori class probability estimate ˆc ¼ n0∕n in place of c.
Three Bayesian MMSE error estimators are also provided, using uniform
priors on c and the simple improper priors in Eq. 2.76 with Sy ¼ 0DD and
ny ¼ 0 (my does not matter because ny ¼ 0). Two of these assume general
independent covariances, one with ky þ D þ 2 ¼ 0 (the flat prior) and one
with ky ¼ 0 (the Jeffreys rule prior), and the last assumes scaled identity
independent covariances with ky þ D þ 2 ¼ 0 (the flat prior). Using the
closed-form expression in Eq. 2.14, these error estimates can be computed
very quickly. For all Bayesian MMSE error estimators, in the rare event
where the sample size for one class is so small that the posteriors used to find
the Bayesian MMSE error estimator cannot be normalized, ky is increased
until the posterior is valid. For each iteration and error estimator, the squared
difference jˆεn  εnj2 is computed. The process is repeated over t ¼ 100,000
samples to find a Monte Carlo approximation for the RMS deviation from
the true error for each error estimator.
Figure 2.8 shows the RMS of all error estimators with respect to the Bayes
error for D ¼ 2 and n ¼ 30. We see that the Bayesian MMSE error estimator
for general covariances using the flat prior is best for distributions with
moderate Bayes error, but poor for very small or large Bayes error.
A similar result was found in the discrete classification problem. Bolstered
resubstitution is very competitive with the Bayesian MMSE error estimator
for general covariances using the Jeffreys rule prior, and it is also very flexible
66
Chapter 2

since it can be applied fairly easily to any classifier; however, keep in mind
that bolstering is known to perform particularly well with circular densities
(uncorrelated equal-variance features) like those in this example. The
Bayesian MMSE error estimator for general covariances using the Jeffreys
rule prior ðky ¼ 0Þ shifts performance in favor of lower Bayes error. Recall
from the form of the priors in Eq. 2.76 that a larger ky will put more weight on
covariances with a small determinant (usually corresponding to a small Bayes
error)
and
less
weight
on
those
with
a
large
determinant
(usually
corresponding to a large Bayes error). If the Bayes error is indeed very
small, then the Bayesian MMSE error estimator using the Jeffreys rule prior is
usually the best followed by the plug-in rule, which performs exceptionally
well because the sample mean and sample variance are accurate even with a
small sample. Finally, regarding Fig. 2.8, note that the Bayesian MMSE error
estimator assuming scaled identity covariances tends to be better than the one
assuming general covariances with ky ¼ 0 over the entire range of Bayes error.
This makes clear the benefit of using more-constrained assumptions as long as
the assumptions are correct.
RMS with respect to sample size is graphed in Fig. 2.9 for two dimensions
and Bayes errors of 0.2 and 0.4. Graphs like these can be used to determine
the sample size needed to guarantee a certain RMS. As the sample size
increases, RMS for the parametrically based error estimators (the plug-in rule
and Bayesian MMSE error estimators) tends to converge to zero much more
quickly than the distribution-free error estimators. This is not surprising since
for a large sample the sample parameter estimates tend to be very accurate.
Bayesian MMSE error estimators can greatly improve on traditional error
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.04
0.06
0.08
0.1
0.12
0.14
Bayes error
RMS deviation from true error
 
 
resub
loo
cv
boot
bol
plugin
BEE, identity, flat
BEE, general, flat
BEE, general, Jeff.
Figure 2.8
RMS deviation from true error for Gaussian distributions with respect to Bayes
error (D ¼ 2, n ¼ 30). [Reprinted from (Dalton and Dougherty, 2011c).]
67
Optimal Bayesian Error Estimation

estimators. For only one or two features, the benefit is clear, especially for
moderate Bayes error as in part (a) of Fig. 2.9. In higher dimensions, there are
many options to constrain the covariance matrix and choose different priors,
so the picture is more complex.
2.6.2 Robustness to falsely assuming identity covariances
The Bayesian MMSE error estimator assuming scaled identity covariances
performs very well for many cases in the preceding simulation, where the
scaled identity covariance assumption is correct. We consider two examples to
investigate robustness relative to this assumption.
For the first example, define r to be the correlation coefficient for class 0
in a two-feature problem. The correlation coefficient for class 1 is r to
ensure mirror-image distributions. Thus, the covariance matrices are given by
Sy ¼

s2
ð1Þyrs2
ð1Þyrs2
s2

(2.146)
for y ∈f0, 1g. Illustrations of the distributions used in this experiment are
shown in Fig. 2.10(a), and simulation results are shown in Fig. 2.10(b). For
the simulations, we have fixed s ¼ 0.7413, which corresponds to a Bayes error
of 0.25 when there is no correlation. The Bayesian MMSE error estimators
assuming general covariances are not significantly affected by correlation, and
the performance of the error estimator assuming identity covariances is also
fairly robust to correlation in this particular model, although some
degradation can be seen for r . 0.8. Meanwhile, bolstering also appears to
be somewhat negatively affected by high correlation, probably owing to the
use of spherical kernels when the true distributions are not spherical.
50
100
150
200
0.03
0.04
0.05
0.06
0.07
0.08
sample size
(a)
(b)
RMS deviation from true error
resub
loo
cv
boot
bol
plugin
BEE, identity, flat
BEE, general, flat
BEE, general, Jeff.
50
100
150
200
0.04
0.06
0.08
0.1
0.12
sample size
RMS deviation from true error
resub
loo
cv
boot
bol
plugin
BEE, identity, flat
BEE, general, flat
BEE, general, Jeff.
Figure 2.9
RMS deviation from true error for Gaussian distributions with respect to sample
size: (a) D ¼ 2, Bayes error ¼ 0.2; (b) D ¼ 2, Bayes error ¼ 0.4. [Reprinted from (Dalton and
Dougherty, 2011c).]
68
Chapter 2

Figure 2.11 presents a second experiment using different variances for
each feature. The covariances are given by
S0 ¼ S1 ¼

s2
0
0
0
s2
1

,
(2.147)
and
we
fix
the
average
variance
between
the
classes
such
that
0.5ðs2
0 þ s2
1Þ ¼ 0.74132. When s2
0 ¼ s2
1, the Bayes error of the classification
problem is again 0.25. These simulations show that the Bayesian MMSE error
estimator assuming identity covariances can be highly sensitive to unbalanced
features; however, this problem may be alleviated by normalizing the raw data.
2.6.3 Robustness to falsely assuming Gaussianity
Since Bayesian error estimators depend on parametric models of the true
distributions, one may apply a Kolmogorov–Smirnov normality test or other
hypothesis test to discern if a sample deviates substantially from being
0
0.2
(b)
(a)
0.4
0.6
0.8
0.05
0.055
0.06
0.065
RMS deviation from true error
 
resub
loo
cv
boot
bol
plugin
BEE, identity, flat
BEE, general, flat
BEE, general, Jeff.
Figure 2.10
RMS deviation from the true error with respect to correlation (D ¼ 2,
s ¼ 0.7413, n ¼ 50): (a) distributions used in RMS graphs; (b) RMS deviation from true
error. [Reprinted from (Dalton and Dougherty, 2011c).]
σ   = σ
0
1
σ   = 2σ
0
1
2σ   = σ
0
1
0.4
0.6
0.8
1
0.02
0.03
0.04
0.05
0.06
0.07
0.08
σ0
RMS deviation from true error
 
resub
loo
cv
boot
bol
plugin
BEE, identity, flat
BEE, general, flat
BEE, general, Jeff.
(a)
(b)
Figure 2.11
RMS deviation from the true error with respect to s0 (D ¼ 2, s ¼ 0.7413,
n ¼ 50): (a) distributions used in RMS graphs; (b) RMS deviation from true error. [Reprinted
from (Dalton and Dougherty, 2011c).]
69
Optimal Bayesian Error Estimation

Gaussian; nevertheless, the actual distribution is very unlikely to be truly
Gaussian, so we need to investigate robustness relative to the Gaussian
assumption. To explore this issue in a systematic setting, we apply Gaussian-
derived Bayesian MMSE error estimators to Johnson distributions in one
dimension. Johnson distributions are a flexible family of distributions with
four free parameters, including mean and variance (Johnson, 1949; Johnson et
al., 1994). There are two main classes in the Johnson system of distributions:
Johnson SU (for unbounded) and Johnson SB (for bounded). The normal and
log-normal distributions are also considered classes in this system, and in fact
they are limiting cases of the SU and SB distributions.
The Johnson system can be summarized as follows. If Z is a standard
normal random variable, then X is Johnson if
Z  g
d
¼ f
X  h
l

,
(2.148)
where f is a simple function satisfying some desirable properties such as
monotonicity (Johnson, 1949); Johnson et al., 1994). For log-normal
distributions f ðyÞ ¼ lnðyÞ, for Johnson SU distributions f ðyÞ ¼ sinh1ðyÞ,
and for Johnson SB distributions f ðyÞ ¼ lnðy∕ð1  yÞÞ ¼ 2tanh1ð2y  1Þ.
For reference, example graphs of these distributions are given in Fig. 2.12.
Johnson SU distributions are always unimodal, while SB distributions can
also be bimodal. In particular, an SB distribution is bimodal if d , 1∕
ﬃﬃﬃ
2
p
and
jgj , d1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  2d2
p
 2dtanh1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  2d2
p
:
(2.149)
The parameters g and d control the shape of the Johnson distribution and
together essentially determine its skewness and kurtosis, which are normalized
third and fourth moments. In particular, skewness is m3∕s3 and kurtosis is
m4∕s4, where mk is the kth mean-adjusted moment of a random variable, and
s2 ¼ m2 is the variance. Skewness and kurtosis are very useful statistics to
measure normality: Gaussian distributions always have a skewness of 0 and
kurtosis of 3. For Johnson distributions, skewness is more influenced by g and
kurtosis by d, but the relationship is not exclusive. Once the shape of the
distribution is determined by g and d, h and l determine the mean and
variance.
Figure 2.13 illustrates the values of skewness and kurtosis obtainable
within the Johnson family. The region below the log-normal line can be
achieved with Johnson SU distributions, while the region above can be
achieved with Johnson SB distributions. In fact, the normal, log-normal, SU,
and SB systems partition the entire obtainable region of the skewness-kurtosis
plane, so there is just one distribution corresponding to each skewness–
kurtosis pair. All distributions satisfy kurtosis ≥skewness2 þ 1, where equality
70
Chapter 2

corresponds to a distribution taking on two values, say one with probability p
and the other with probability 1  p. In Fig. 2.13, g ¼ 0 corresponds to points
on the left axis. The gray diagonal lines represent skewness and kurtosis
obtainable with SU distributions and fixed values of d. As we increase d, these
lines move up in an almost parallel manner. As we increase g, kurtosis
increases along with skewness until we converge to a point on the log-normal
line. As a quick example, if kurtosis is fixed at 4.0, then d . 2.3, which is
limited by the worst-case scenario, where g ¼ 0. Also, for SU distributions
with this kurtosis, the maximum squared skewness is about 0.57, which is
achieved using d , 4.1.
The simulation procedure in this section is the same as that in Section
2.6.1, except that the sample points are each assigned a Johnson-distributed
value rather than Gaussian. We use mirror images of the same Johnson
distribution for both classes. In the following, the parameters d and g refer to
class 0, while class 1 has parameters d and g. Meanwhile, for each class, h
−4
−2
0
2
4
0
0.1
0.2
0.3
0.4
feature
PDF
γ = 0.0
γ = 2.0
(a)
−4
−2
0
2
4
0
0.1
0.2
0.3
0.4
0.5
0.6
feature
PDF
δ = 1.5
δ = 0.5
(b)
0
0.2
0.4
0.6
0.8
1
0
1
2
3
4
5
6
7
feature
PDF
γ = 2.0
γ = 0.0
(c)
0
0.2
0.4
0.6
0.8
1
0
0.5
1
1.5
2
2.5
feature
PDF
δ = 1.5
δ = 0.5
(d)
Figure 2.12
Example Johnson distributions with one parameter fixed and the other varying
in increments of 0.1 (h ¼ 0, l ¼ 1): (a) Johnson SU, d ¼ 0.9; (b) Johnson SU, g ¼ 0.0;
(c) Johnson SB, d ¼ 0.9; (d) Johnson SB, g ¼ 0.0. [Reprinted from (Dalton and Dougherty,
2011c).]
71
Optimal Bayesian Error Estimation

and l are jointly selected to give the appropriate mean and covariance pair.
The sample size is fixed at n ¼ 30, the means are fixed at 0 and 1, and the
standard deviations are fixed at s ¼ 0.7413, which corresponds to a Bayes
error of 0.25 for the Gaussian distribution. With one feature, n ¼ 30, and
Gaussian distributions with a Bayes error of 0.25, the Bayesian MMSE error
estimators using the flat prior and the Jeffreys rule prior perform quite well
with RMSs of about 0.060 and 0.066, respectively. These are followed by the
plug-in rule with an RMS of 0.070 and bolstering with an RMS of 0.073. We
wish to observe how well this ordering is preserved as the skewness and
kurtosis of the original Gaussian distributions are distorted using Johnson
distributions.
Figures 2.14(a) and (b) show the RMS of all error estimators for various
Johnson SU distributions, and Figs. 2.14(c) and (d) show analogous graphs
for Johnson SB distributions. In each subfigure, we fix either d or g and vary
the other parameter to observe a slice of the performance behavior. The scale
for the RMS of all error estimators is provided on the left axis as usual, and a
graph of either skewness (when d is fixed) or kurtosis (when g is fixed) is added
as a black dotted line with the scale shown on the right axis. These skewness
skewness
kurtosis
2
0.0
3.0
lognormal line
region obtainable
by Johnson SU
region obtainable
by Johnson SB
4.0
0.57
Gaussian
 δ = 2.3
 δ = 4.1
1.0
impossible region
kurtosis = skewness   + 1
2
2.0
Figure 2.13
Skewness–kurtosis-obtainable regions for Johnson distributions. [Reprinted
from (Dalton and Dougherty, 2011c).]
72
Chapter 2

and kurtosis graphs help illustrate the non-Gaussianity of the distributions
represented by each point.
Figure 2.14(b) presents a simulation observing the effect of d (which has
more influence on kurtosis) with SU distributions and g ¼ 0. For g ¼ 0 there
is no skewness, and this graph shows that the Bayesian MMSE error estimator
with the flat prior requires that d ≥1.5 before it surpasses all of the other error
estimators (the last being bolstering). This corresponds to a kurtosis of about
7.0. A similar performance graph with Johnson SB distributions and g ¼ 0 is
given in Fig. 2.14(d), in which the same error estimator is the best whenever
d . 0.4, corresponding to kurtosis greater than about 1.5. So although
Gaussian distributions have a kurtosis of 3.0, in this example the Bayesian
MMSE error estimator is still better than all of the other error estimators
whenever there is no skewness and kurtosis is between 1.5 and 7.0.
Interestingly, performance can actually improve as we move away from
Gaussianity. For example, in the SU system, for larger d the RMS of the
Bayesian
estimators
seems
to
monotonically
decrease
with
g,
as
in
−5
−4
−3
−2
−1
0
1
2
3
4
5
0.05
0.06
0.07
0.08
0.09
0.1
γ
RMS deviation from true error
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
skewness
resub
loo
cv
boot
bol
plugin
skewness
(a)
0
1
2
3
4
5
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
0.11
δ
RMS deviation from true error
3
4
5
6
7
8
9
10
11
12
13
14
15
kurtosis
resub
loo
cv
boot
bol
plugin
kurtosis
(b)
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
0.05
0.06
0.07
0.08
0.09
0.1
0.11
0.12
0.13
γ
RMS deviation from true error
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
skewness
resub
loo
cv
boot
bol
plugin
skewness
(c)
0
1
2
3
4
5
0.05
0.06
0.07
0.08
0.09
0.1
0.11
0.12
δ
RMS deviation from true error
1
1.5
2
2.5
3
kurtosis
resub
loo
cv
boot
bol
plugin
kurtosis
(d)
BEE, flat
BEE, Jeffreys’
BEE, flat
BEE, Jeffreys’
BEE, flat
BEE, Jeffreys’
BEE, flat
BEE, Jeffreys’
Figure 2.14
RMS deviation from true error for Johnson SU and SB distributions
(1D, s ¼ 0.7413, n ¼ 30): (a) SU, d ¼ 2.0, with skewness; (b) SU, g ¼ 0.0, with kurtosis;
(c) SB, d ¼ 0.7, with skewness; (d) SB, g ¼ 0.0, with kurtosis. [Reprinted from (Dalton and
Dougherty, 2011c).]
73
Optimal Bayesian Error Estimation

Fig. 2.14(a), suggesting that they favor negative skewness (positive g), where
the classes have less overlapping mass. Simulations with Johnson SB
distributions also appear to favor slight negative skewness (negative g),
although performance is not monotonic.
Finally, in Fig. 2.15 we present a graph summarizing the performance of
Bayesian MMSE error estimators on Johnson distributions with respect to the
skewness and kurtosis of class 0. The skewness-kurtosis plane shown in this
figure is essentially the same as that illustrated in Fig. 2.13, but also shows two
sides to distinguish between distributions with more overlapping mass (class 0
has positive skewness) and less overlapping mass (class 0 has negative
skewness). For mirror-image distributions, the kurtosis of class 1 is the same
but skewness is negative. Each dot in Fig. 2.15 represents a fixed class-
conditional Johnson distribution. As before, we fix s ¼ 0.7413, corresponding
to a Bayes error of 0.25 for the Gaussian distribution (which has skewness 0
and kurtosis 3).
Black dots in Fig. 2.15 represent distributions where the Bayesian MMSE
error estimator with the flat prior performs better than all of the other six
standard error estimators, while white dots pessimistically represent distribu-
tions where at least one other error estimator is better. With one feature,
n ¼ 30, and s ¼ 0.7413, the black dots cover a relatively large range of
skewness and kurtosis (especially with negative skewness), indicating that
Bayesian MMSE error estimators can be used relatively reliably even if the
true distributions are not perfectly Gaussian. Similar graphs or studies may be
−5
−4
−3
−2
−1
0
1
2
3
4
5
1
2
3
4
5
6
7
8
9
10
skewness2
kurtosis
SU
SB
impossible
region
−skewness2
BEE is not best
BEE is best
Figure 2.15
RMS deviation from true error for Johnson distributions, varying both
skewness and kurtosis (1D, s ¼ 0.7413, n ¼ 30). Black dots are where the Bayesian MMSE
error estimator is best; white dots are where any other error estimator is best. [Reprinted
from (Dalton and Dougherty, 2011c).]
74
Chapter 2

used to determine an “acceptable” region for Gaussian modeling assumptions,
which may be useful for designing hypothesis tests. However, performance in
this graph depends heavily on the particular distributions. Recall that the
Bayesian MMSE error estimator may not be the best error estimator for a
specific Gaussian distribution, let alone a Johnson distribution.
2.6.4 Average performance under proper priors
To illustrate the average performance of Bayesian MMSE error estimators
over all distributions in a model, we require proper priors, so for the sake of
demonstration we will use a carefully designed proper prior in this section
rather than the improper priors used previously. We assume the general
independent covariance model for both classes, and define the prior
parameters ky ¼ ny ¼ 5D and Sy¼ 0.74132ðky  D  1ÞID. For class 0 we
also define m0 ¼ 0D, and for class 1 we define m1 as in Eq. 2.143. For each
class, this prior is always proper. In addition, we assume a uniform
distribution for c.
The simulation procedure follows Fig. 2.5. In each iteration of step 1, and
for each class independently, a random covariance Sy is drawn from the
inverse-Wishart distribution with parameters ky and Sy using methods in
(Johnson, 1987). Conditioned on Sy, a random mean is generated using
the Gaussian distribution pðmyjSyÞ  N ðmy, Sy∕nyÞ, resulting in a normal-
inverse-Wishart distributed mean and covariance pair. Each feature-label
distribution is determined by a class probability c drawn from a uniformð0, 1Þ
distribution, and the set of means my and covariances Sy for y ∈f0, 1g.
Step 2A generates a random training sample from the realized class-
conditional distributions N ðmy, SyÞ, and the prior is updated to a posterior
in step 2B. The labeled sample points are used to train an LDA classifier in
step 2C, with no feature selection. In step 2D, since the classifier is linear, we
compute the true error and Bayesian MMSE error estimators exactly using
closed-form expressions. Resubstitution, leave-one-out, cross-validation,
0.632 bootstrap, bolstered resubstitution, and plug-in are also found. The
difference ˆεn  εn and the squared difference jˆεn  εnj2 are averaged to
produce Monte Carlo approximations of bias and RMS, respectively, over
the prior and sampling distributions. The whole process is repeated over
T ¼ 100,000 feature-label distributions and t ¼ 10 samples per feature-label
distribution.
Results for five features are given in Fig. 2.16. These graphs show (as they
must)
that
Bayesian
MMSE
error
estimators
possess
optimal
RMS
performance when averaged over all distributions in the parameterized family
and are unbiased for each sample size. In fact, performance of the Bayesian
MMSE error estimator improves significantly relative to the other error
estimators as the number of features increases.
75
Optimal Bayesian Error Estimation

2.7 Consistency of Bayesian Error Estimation
This section treats consistency of the Bayesian MMSE error estimator: As
more data are collected, does the estimator converge to the true error? We are
interested in the asymptotic behavior ðn →`Þ with respect to a fixed true
parameter and its sampling distribution. Specifically, suppose that ¯u ∈U is
the unknown true parameter. Let S` represent an infinite sample drawn from
the true distribution and Sn denote the first n observations of this sample. The
sampling distribution will be specified in the subscript of probabilities and
expectations using a notation of the form S`j¯u.
Doob (Doob, 1948) proved that Bayesian estimates are consistent up to a
null set of the prior measure; that is, the set of parameters where the estimator
is not consistent has prior probability 0. If the true distribution is in the
uncertainty class, and the true parameter is thought of as a random variable
determined by a known random mechanism represented by our prior, this
result is satisfactory, in the sense that consistency is guaranteed with
probability 1. If the true parameter is a fixed state of nature and our prior
is simply a model of our uncertainty, it is important to determine for that
specific parameter whether the Bayesian MMSE error estimator is consistent.
To wit, when we talk about a specific true distribution, unless the prior has a
point mass on that specific true distribution (which is the case in the discrete
model, making that problem easy), according to Doob we do not know if the
specific true distribution is or is not in the null set. We need a stronger result,
ensuring convergence for all distributions in the parameter space, not just all
distributions except for an unspecified null set.
Throughout this section, we will denote the true error by εnð¯u, SnÞ rather
than εnð¯u, cnÞ to emphasize that it is a function of the sample. A sequence of
40
60
80
100
120
140
160
180
200
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
sample size
(a)
(b)
RMS deviation from true error
resub
loo
cv
boot
bol
plugin
BEE
40
60
80
100
120
140
160
180
200
−0.05
−0.04
−0.03
−0.02
−0.01
0
0.01
0.02
0.03
0.04
0.05
sample size
bias
resub
loo
cv
boot
bol
plugin
BEE
Figure 2.16
RMS deviation from true error and bias for linear classification of Gaussian
distributions, averaged over all distributions and samples using a proper prior: (a) RMS,
D ¼ 5; (b) bias, D ¼ 5. [Reprinted from (Dalton and Dougherty, 2011c).]
76
Chapter 2

estimators ˆεnðSn, cnÞ of a sequence of functions εnðu, SnÞ of the parameter is
said to be weakly consistent at ¯u if
lim
n→` ˆεnðSn, cnÞ  εnð¯u, SnÞ ¼ 0
(2.150)
in probability. ˆεnðSn, cnÞ is weakly consistent, or simply consistent, if
Eq. 2.150 is true for all ¯u ∈U. For any r ≥1, ˆεnðSn, cnÞ is consistent in the
rth mean if there is convergence in the rth absolute moments,
lim
n→`ESnj¯u
	ˆεnðSn, cnÞ  εnð¯u, SnÞ
r
¼ 0,
(2.151)
for all ¯u ∈U. rth mean consistency always implies weak consistency. Of
particular interest is the case of r ¼ 2, where there is convergence in the mean-
square. If jˆεnðSn, cnÞ  εnð¯u, SnÞj is bounded, which is always true for
classifier error estimation, then weak consistency implies rth mean consistency
for all r ≥1; therefore, the two notions of consistency are equivalent.
ˆεnðSn, cnÞ is said to be strongly consistent if there is almost sure
convergence:
PrS`juðˆεnðSn, cnÞ  εnðu, SnÞ →0Þ ¼ 1.
(2.152)
Strong consistency always implies weak consistency. For Bayesian MMSE
error estimators, we will prove Eq. 2.152, assuming fairly weak conditions on
the model and classification rule.
2.7.1 Convergence of posteriors
The salient point is to show that the posteriors of the parameters c, u0, and u1
converge in some sense to delta functions on the true parameters ¯c ∈½0, 1,
¯u0 ∈U0, and ¯u1 ∈U1. This is a property of the posterior distribution, whereas
the preceding definitions of consistency are only properties of the estimator
itself. Posterior convergence is formalized via the concept of weak∗
consistency, whose elucidation requires some comments regarding measure
theory.
Suppose that the sample space X and the parameter space U are Borel
subsets of complete separable metric spaces, each being endowed with the
induced s-algebra from the Borel s-algebra on its respective metric space. For
instance, if A is the Borel s-algebra for the space containing U, then U ∈A,
and U is endowed with the induced s-algebra AU ¼ fU ∩A : A ∈Ag.
If Pn and P are probability measures on U, then Pn →P weak∗(that is, in
the weak∗topology on the space of all probability measures over U) if and
only if R
U f dPn →R
U f dP for all bounded continuous functions f on U. This
is the Helly–Bray theorem. Further, if du is a point mass at u ∈U, then it can
77
Optimal Bayesian Error Estimation

be shown that Pn →du
weak∗
if and only if PnðUÞ →1 for every
neighborhood U of u.
Our interest is in the convergence of posteriors, which are themselves
random due to the sampling distribution. In particular, Bayesian modeling
parameterizes a family of distributions, or, equivalently, a family of
probability measures fF u : u ∈Ug on X. For a fixed true parameter ¯u, we
denote the infinite labeled sampling distribution under u by F`
¯u, which is a
measure on the space of infinite samples on X. We call the posterior of u
weak∗consistent at ¯u ∈U if the posterior probability of the parameter
converges weak∗to d¯u for all infinite samples, excluding a subset with
probability 0 on F`
¯u. To wit, for all bounded continuous functions f on U,
PrS`j¯uðEujSn½ f ðuÞ →f ð¯uÞÞ ¼ 1.
(2.153)
Equivalently, the posterior probability (given a fixed sample) of any
neighborhood U of the true parameter ¯u converges to 1 almost surely with
respect to the sampling distribution, i.e.,
PrS`j¯uðPrujSnðUÞ →1Þ ¼ 1.
(2.154)
The posterior is called weak∗consistent if it is weak∗consistent for every
¯u ∈U. If there are only a finite number of possible outcomes and no
neighborhood of the true parameter has prior probability 0, as has long
been known, posteriors are weak∗consistent (Freedman, 1963; Diaconis and
Freedman, 1986).
Returning to our classification problem, to establish weak∗consistency for
the posteriors of c, u0, and u1 in both the discrete and Gaussian models (in the
usual topologies), we assume proper priors on c, u0, and u1.
First consider the parameter c. The sample space for c is the binary set
f0, 1g, which is trivially a complete separable metric space, and the parameter
space is the interval ½0, 1, which is itself a complete separable metric space in
the usual topology. We use the L1-norm. Suppose that c is not known and
labeled training points are drawn using random sampling, where n0 →` when
¯c . 0 and n1 →` when ¯c , 1. Since the number of possible outcomes for
labels is finite (in fact binary), p∗ðcÞ is weak∗consistent as n →` when using a
beta prior on c, which has positive mass in every open interval in ½0, 1. This
holds regardless of the class-conditional density models (whether they are
discrete, Gaussian, or otherwise). By Eq. 2.153, Ep∗½c →¯c almost surely over
the labeled sampling distribution. On the other hand, if c is assumed to be
known, then Ep∗½c ¼ ¯c trivially.
Now consider u0 and u1. In the discrete model, the feature space is
X ¼ f1, 2, : : : , bg, which is again trivially a complete separable metric space,
and for bin probabilities ½ p1, : : : , pb or ½q1, : : : , qb the parameter space
78
Chapter 2

Uy ¼ Db1 is a complete separable metric space in the L1-norm. Since sample
points again have a finite number of possible outcomes, the posteriors of uy
are weak∗consistent as ny →`.
For other Bayesian models on uy, we assume that the feature space X and
the parameter space Uy are finite-dimensional Borel subsets of complete
separable metric spaces. In the independent general covariance Gaussian
models, the feature space X ¼ RD is a complete separable metric space, while
the parameter space for ðmy, SyÞ is
Uy ¼ RD  fSy ∈RDD : Sy ≻0g,
(2.155)
which is contained in the complete separable metric space RD  RDD with
the L1-norm, and is a Borel subset. Similar results hold for our other Gaussian
models. As long as the parameter space is finite-dimensional, the true feature-
label distribution is in the interior (not the boundary) of the parameterized
family of distributions, the likelihood function is a bounded continuous
function of the parameter that is not under-identified (not flat for a range of
values of the parameter), and the prior is proper with all neighborhoods of the
true parameter having positive probability, then the posterior distribution
of the parameter approaches a normal distribution centered at the true
parameter with variance proportional to 1∕n as n →` (Gelman et al., 2004).
For a Gaussian model on class y ∈f0, 1g, and either random or separate
sampling, these regularity conditions hold; hence, the posterior of uy is weak∗
consistent as ny →`.
Weak∗consistency of the posteriors for uy (which holds in the discrete and
Gaussian models) and Eq. 2.153 imply that, as ny →`,
PrS`j¯uyðEuyjSn½ f ðuyÞ →f ð¯uyÞÞ ¼ 1
(2.156)
for all ¯uy ∈Uy and any bounded continuous function f on Uy.
2.7.2 Sufficient conditions for consistency
The following theorem decouples the issue of consistency in the prior for c
from consistency in the priors for u0 and u1. Hence, consistency in the
Bayesian MMSE error estimator becomes an issue of consistency in the
estimation of the error due to each class.
Theorem 2.13 (Dalton and Dougherty, 2012c). Let c ∈½0, 1, u0 ∈U0, and
u1 ∈U1: Define u ¼ ðc, u0, u1Þ: Suppose that
1. EujSn½c →¯c as n →` almost surely over the infinite labeled sampling
distribution under ¯u (henceforth we denote this limit by →
a:s:).
79
Optimal Bayesian Error Estimation

2. c and ðu0, u1Þ are independent in their priors (and all posteriors).
3. n0 →` if ¯c . 0 and n1 →` if ¯c , 1 almost surely.
4. If ny →`, then
EuyjSn
	εy
nðuy, SnÞ  εy
nð¯uy, SnÞ

→
a:s: 0.
(2.157)
Then for all finite k ≥1,
EujSn
	εnðu, SnÞ  εnð¯u, SnÞ
k
→
a:s: 0.
(2.158)
In particular, k ¼ 1 implies that the Bayesian MMSE error estimator is
strongly consistent.
Proof. Note that
εnðu, SnÞ  εnð¯u, SnÞ
¼ cε0nðu0, SnÞ þ ð1  cÞε1nðu1, SnÞ  ¯cε0nð¯u0, SnÞ  ð1  ¯cÞε1nð¯u1, SnÞ
¼ ðc  ¯cÞε0nðu0, SnÞ þ ð¯c  cÞε1nðu1, SnÞ
þ ¯c½ε0nðu0, SnÞ  ε0nð¯u0, SnÞ þ ð1  ¯cÞ½ε1nðu1, SnÞ  ε1nð¯u1, SnÞ:
(2.159)
By condition (2) and the triangle inequality,
0 ≤EujSn½jεnðu, SnÞ  εnð¯u, SnÞj
≤EujSn½jc  ¯cjEujSn½ε0nðu0, SnÞ
þ EujSn½j¯c  cjEujSn½ε1nðu1, SnÞ
þ ¯cEujSn½e0nðu0, SnÞ þ ð1  ¯cÞEujSn½e1nðu1, SnÞ,
(2.160)
where
ey
nðuy, SnÞ ¼ jεy
nðuy, SnÞ  εy
nð¯uy, SnÞj:
(2.161)
Furthermore, by condition (1) and the fact that EujSn½εy
nðuy, SnÞ is bounded,
EujSn½jc  ¯cjEujSn½ε0nðu0, SnÞ þ EujSn½j¯c  cjEujSn½ε1nðu1, SnÞ →
a:s: 0.
(2.162)
Suppose that 0 , ¯c , 1. Applying conditions (3) and (4), and the fact that any
linear combination of almost surely convergent quantities is almost surely
convergent,
¯cEujSn½e0nðu0, SnÞ þ ð1  ¯cÞEujSn½e1nðu1, SnÞ →
a:s: ¯c ⋅0 þ ð1  ¯cÞ ⋅0 ¼ 0.
(2.163)
80
Chapter 2

If ¯c ¼ 0, then since EujSn½e0nðu0, SnÞ is bounded, ¯cEujSn½e0nðu0, SnÞ →
a:s: 0. If
¯c ¼ 1, likewise ð1  ¯cÞEujSn½e1nðu1, SnÞ →
a:s: 0. Again applying conditions (3)
and (4), we have that Eq. 2.163 holds in all cases.
Combining Eqs. 2.162 and
2.163, we have that the right-hand
side
of
Eq.
2.160
converges
to
zero
almost
surely,
thus
EujSn½jεnðu, SnÞ  εnð¯u, SnÞj →
a:s: 0. This implies Eq. 2.158 for k ¼ 1. Applying
Jensen’s inequality,
0 ≤jEujSn½εnðu, SnÞ  εnð¯u, SnÞj ≤EujSn½jεnðu, SnÞ  εnð¯u, SnÞj:
(2.164)
Therefore, jEujSn½εnðu, SnÞ  εnð¯u, SnÞj →
a:s: 0, which implies Eq. 2.152. Thus,
the Bayesian MMSE error estimator is strongly consistent. For k . 1, note
that jεnðu, SnÞ  εnð¯u, SnÞj ≤1. Hence,
0 ≤jεnðu, SnÞ  εnð¯u, SnÞjk ≤jεnðu, SnÞ  εnð¯u, SnÞj,
(2.165)
and after taking the expectation,
0 ≤EujSn½jεnðu, SnÞ  εnð¯u, SnÞjk ≤EujSn½jεnðu, SnÞ  εnð¯u, SnÞj:
(2.166)
Since
EujSn½jεnðu, SnÞ  εnð¯u, SnÞj →0
almost
surely,
Eq.
2.158
must
hold.
▪
Condition (1) is assured if p∗ðcÞ is weak∗consistent, as is the case with
beta priors and random sampling. Condition (2) is the usual assumption of
independence used in the Bayesian framework. Condition (3) is a property of
the sampling methodology. Essentially, we require that points from each class
should be observed infinitely often (almost surely), unless one class actually
has zero probability. Condition (4) will be proven under fairly broad
conditions in the following theorem, which proves that the Bayesian MMSE
error estimator is strongly consistent as long as the true error functions
εy
nðuy, SnÞ form equicontinuous sets for fixed samples and the posterior of uy
is weak∗consistent at ¯uy. In the next section, we prove that this property holds
for all classification rules in the discrete model and all linear classification
rules in the general covariance Gaussian model.
Theorem 2.14 (Dalton and Dougherty, 2012c). Let ¯u ∈U be an unknown
true parameter and let FðS`Þ ¼ ff ð⋅, SnÞg`
n¼1 be a uniformly bounded collection
of
measurable
functions
associated
with
the
sample
S`,
where
f ð⋅, SnÞ : U →½0, `Þ, f ð⋅, SnÞ ≤MðS`Þ for each n ¼ 1, 2, : : : , and MðS`Þ is
a positive finite constant. If FðS`Þ is equicontinuous at ¯u (almost surely with
81
Optimal Bayesian Error Estimation

respect to the sampling distribution) and the posterior of u is weak∗consistent at
¯u, then
PrS`j¯uðEujSn½jf ðu, SnÞ  f ð¯u, SnÞj →0Þ ¼ 1.
(2.167)
Proof. Let dU be the metric associated with U. For fixed S` and e . 0, if
equicontinuity
holds
for
FðS`Þ,
then
there
exists
d . 0
such
that
jf ðu, SnÞ  f ð¯u, SnÞj , e for all f ∈FðS`Þ whenever dUðu, ¯uÞ , d. Hence,
EujSn½jf ðu, SnÞ  f ð¯u, SnÞj
¼ EujSn½jf ðu, SnÞ  f ð¯u, SnÞjIdUðu,¯uÞ,d
þ EujSn½jf ðu, SnÞ  f ð¯u, SnÞjIdUðu,¯uÞ≥d
≤EujSn½eIdUðu,¯uÞ,d þ EujSn½2MðS`ÞIdUðu,¯uÞ≥d
¼ eEujSn½IdUðu,¯uÞ,d þ 2MðS`ÞEujSn½IdUðu,¯uÞ≥d
¼ e PrujSnðdUðu, ¯uÞ , dÞ þ 2MðS`Þ PrujSnðdUðu, ¯uÞ ≥dÞ:
(2.168)
From the weak∗consistency of the posterior of u at ¯u, Eq. 2.154 holds, and
lim sup
n→`
EujSn½jf ðu, SnÞ  f ð¯u, SnÞj
≤e lim sup
n→`
PrujSnðdUðu, ¯uÞ , dÞ
þ 2MðS`Þlim sup
n→`
PrujSnðdUðu, ¯uÞ ≥dÞ
¼
a:s: e ⋅1 þ 2MðS`Þ ⋅0 ¼ e:
(2.169)
Finally, since this is (almost surely) true for all e . 0,
EujSn½jf ðu, SnÞ  f ð¯u, SnÞj →
a:s: 0,
(2.170)
which completes the proof.
▪
2.7.3 Discrete and Gaussian models
Equicontinuity essentially guarantees that the true errors for designed
classifiers are somewhat “robust” near the true parameter. Loosely speaking,
with equicontinuity there exists (almost surely) a neighborhood U of the true
parameter such that at any parameter in U, all errors, under any classifier
produced by the given rule and truncated samples drawn from the fixed
infinite sample, are as close as desired to the true error. This property is a
sufficient condition for consistency and is not a stringent requirement. Indeed,
the following two theorems prove that it holds for both the discrete and
82
Chapter 2

Gaussian models herein. Combining these results with Theorems 2.13 and
2.14, the Bayesian MMSE error estimator is strongly consistent for both the
discrete model with any classification rule and the general covariance
Gaussian model with any linear classification rule.
Theorem 2.15 (Dalton and Dougherty, 2012c). In the discrete Bayesian model
with any classification rule, FyðS`Þ ¼ fεy
nð⋅, SnÞg`
n¼1 is equicontinuous at every
¯uy ∈Uy for y ∈f0, 1g.
Proof. This is a slightly stronger proof than required in Theorem 2.14, since
equicontinuity is always true for any sample, not only almost surely. Also, we
may use any classification rule; any sequence of classifiers may be applied
across each value of n.
In
a
b-bin
model,
suppose
that
the
sequence
of
classifiers
cn : f1, 2, : : : , bg →f0, 1g is obtained from a given sample. The error of
classifier cn contributed by class 0 at parameter u0 ¼ ½ p1, p2, : : : pb ∈U0 is
ε0nðu0, SnÞ ¼
X
b
i¼1
piIcnðiÞ¼1:
(2.171)
For any fixed sample S`, fixed true parameter ¯u0 ¼ ½ ¯p1, ¯p2, : : : , ¯pb, and any
u0 ¼ ½ p1, p2, : : : , pb,
jε0nðu0, SnÞ  ε0nð¯u0, SnÞj ¼

X
b
i¼1
ðpi  ¯piÞIcnðiÞ¼1

≤
X
b
i¼1
jpi  ¯pij
¼ jju0  ¯u0jj1:
(2.172)
Since ¯u0 is arbitrary, F0ðS`Þ is equicontinuous. A similar argument shows
that F 1ðS`Þ ¼ fPb
i¼1 qiIcnðiÞ¼0g`
n¼1 is equicontinuous, which completes the
proof.
▪
Theorem 2.16 (Dalton and Dougherty, 2012c). In a general covariance
Gaussian Bayesian model with D features and any linear classification rule,
FyðS`Þ ¼ fεy
nð⋅, SnÞg`
n¼1 is equicontinuous at every ¯uy ∈Uy for y ∈f0, 1g.
Proof. Given S`, suppose that we obtain a sequence of linear classifiers
cn : RD →f0, 1g with discriminant functions gnðxÞ ¼ aTn x þ bn defined by
vectors an and constants bn. If an ¼ 0D for some n, then the classifier and
83
Optimal Bayesian Error Estimation

classifier errors are constant (with respect to the feature space). In this case,
jεy
nðuy, SnÞ  εy
nð¯uy, SnÞj ¼ 0 for all uy, ¯uy ∈Uy, so this classifier does not
affect the equicontinuity of F yðS`Þ. Hence, without loss of generality, we
assume that an ≠0D for all n so that the error of classifier cn contributed by
class y at parameter uy ¼ ½my, Sy is given by
εy
nðuy, SnÞ ¼ F
0
@ð1ÞygnðmyÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTn Syan
q
1
A:
(2.173)
Since scaling gn does not affect the decision of classifier cn and an ≠0D,
without loss of generality, we also assume that gn is normalized such that
maxijan,ij ¼ 1 for all n, where an,i is the ith element of an.
Treating both classes at the same time, it is enough to show that
fgnðmÞg`
n¼1 is equicontinuous at every ¯m ∈RD, and faTn Sang`
n¼1 is equicontin-
uous at every symmetric positive definite ¯S (considering one fixed ¯S at a time,
by
positive
definiteness,
aTn ¯San . 0).
For
any
fixed
but
arbitrary
¯m ¼ ½ ¯m1, ¯m2, : : : , ¯mDT and any m ¼ ½m1, m2, : : : , mDT,
jgnðmÞ  gnð ¯mÞj ¼

X
D
i¼1
an,iðmi  ¯miÞ

≤max
i
jan,ij
X
D
i¼1
jmi  ¯mij
¼ jjm  ¯mjj1:
(2.174)
This proves that fgnðmÞg`
n¼1 is equicontinuous. For any fixed ¯S, we denote ¯sij
as its ith row, jth column element and use similar notation for an arbitrary
matrix S. Then
jaTn San  aTn ¯Sanj ¼ jaTn ðS  ¯SÞanj
¼

X
D
i¼1
X
D
j¼1
an,ian,jðsij  ¯sijÞ

≤max
i
jan,ij2 X
D
i¼1
X
D
j¼1
jsij  ¯sijj
¼ jjS  ¯Sjj1:
(2.175)
Hence, faTn Sang`
n¼1 is equicontinuous.
▪
84
Chapter 2

2.8 Calibration
When an analytical representation of the Bayesian MMSE error estimator is
not available, it may be approximated using Monte Carlo methods—for
instance, in the case of Gaussian models with nonlinear classification (Dalton
and Dougherty, 2011a); however, approximating a Bayesian MMSE error
estimator is much more computationally intensive than classical counting
methods, such as cross-validation, and may be infeasible. Thus, we consider
optimally calibrating arbitrary error estimators within a Bayesian framework
(Dalton and Dougherty, 2012a). Assuming a fixed sample size, fixed
classification and error estimation schemes, and a prior distribution over an
uncertainty class, a calibration function mapping error estimates (from the
specified error estimation rule) to their calibrated values is computed, and this
calibration function is used as a lookup table to calibrate the final error
estimates. A salient property of a calibrated error estimator is that it is
unbiased relative to the true error.
2.8.1 MMSE calibration function
An optimal calibration function is associated with four assumptions: a fixed
sample size n, a Bayesian model with a proper prior pðuÞ ¼ pðc, u0, u1Þ,
a fixed classification rule (including possibly a feature selection scheme), and a
fixed uncalibrated error estimator ˆε•. Given these assumptions, the optimal
MMSE calibration function is the expected true error conditioned on the
observed error estimate,
E½εnjˆε• ¼
Z 1
0
εnf ðεnjˆε•Þdεn
¼
R 1
0 εnf ðεn, ˆε•Þdεn
f ðˆε•Þ
,
(2.176)
where f ðεn, ˆε•Þ is the unconditional joint density between the true and
estimated errors, and f ðˆε•Þ is the unconditional marginal density of the
estimated error. Viewed as a function of ˆε•, this expectation is called the
MMSE calibration function. It may be used to calibrate any error estimator to
have optimal MSE performance for the assumed model. Evaluated at a
particular value of ˆε•, it is called the MMSE calibrated error estimate and will
be denoted by ˆε∗. Owing to the basic conditional-expectation property
E½E½XjY ¼ E½X,
E½ˆε∗ ¼ E½E½εnjˆε• ¼ E½εn:
(2.177)
In the first and last terms, both expectations are over u and Sn. In the middle
term, the inner expectation is over u and Sn conditioned on ˆε•, and the outer
expectation is over ˆε•. Therefore, the calibrated estimate is unbiased.
85
Optimal Bayesian Error Estimation

We say that an arbitrary estimator ˆεo (be it calibrated or not) has ideal
regression if E½εnjˆεo ¼ ˆεo almost surely. This means that, given the estimate,
the mean of the true error equals the estimate. Hence, the regression of the
true error on the estimate is the 45° line through the origin. The next theorem
is a special case of the tower property of conditional expectation and
guarantees that both Bayesian MMSE and calibrated error estimators possess
ideal regression. We first prove a lemma using a measure-theoretic definition
of conditional expectation based on the Radon–Nikodym theorem (Loève,
1978). The measure theoretic definition conditions on an entire sub-s-algebra
so that the conditional expectation is viewed as a function or a random
variable itself. This is one of those instances where it is easier to prove a
general measure-theoretic result than to work with densities or probability
distribution functions.
Lemma 2.5 (Dalton and Dougherty, 2012a). Consider a probability space
ðV, A, PÞ. Let X be any A-measurable function whose integral exists, and B be
a s-algebra contained in A. Then
E½XjE½XjB ¼ E½XjB
(2.178)
almost surely.
Proof. Let PB be the restriction of P to B. By definition, E½XjB is the
conditional expectation of X given B, which is a B-measurable function and is
defined up to PB measure zero by
Z
B
E½XjBdPB ¼
Z
B
XdP
(2.179)
for any B ∈B. The existence of E½XjB is guaranteed by the Radon–Nikodym
theorem because PB is absolutely continuous with respect to P. Since E½XjB is
B-measurable, the s-algebra C generated by E½XjB is a sub-algebra of B and
therefore a sub-algebra of A. By definition, E½XjC is the conditional
expectation of X given C, which is a C-measurable function defined up to PC
measure zero by
Z
C
E½XjCdPC ¼
Z
C
XdP
(2.180)
for any C ∈C. Since C ⊆B, Eqs. 2.179 and 2.180 imply that
Z
C
E½XjBdPC ¼
Z
C
E½XjCdPC
(2.181)
86
Chapter 2

for any C ∈C. Hence, E½XjB ¼ E½XjC almost surely relative to PC, which
yields Eq. 2.178 because E½XjE½XjB ¼ E½XjC by definition, C being the
s-algebra generated by E½XjB.
▪
Theorem 2.17 (Dalton and Dougherty, 2012a). Bayesian MMSE and MMSE
calibrated error estimators possess ideal regression.
Proof. Consider a probability space ðV, A, PÞ and let X be an integrable
random variable and Y be a random vector. Then from the lemma we deduce
that
E½XjE½XjY ¼ E½XjY
(2.182)
almost surely. To see this, let B be the s-algebra generated by Y. Then in the
lemma, E½XjB becomes E½XjY, C becomes the s-algebra generated by
E½XjY, and E½XjC becomes E½XjE½XjY. Now, εn is an integrable random
variable because the true error is bounded. In Eq. 2.182, letting X ¼ εn and
Y ¼ ˆε• yields
E½εnjˆε∗ ¼ E½εnjE½εnjˆε• ¼ E½εnjˆε• ¼ ˆε∗:
(2.183)
Therefore, calibrated error estimators are ideal. If we now let Y ¼ Sn be the
observed sample, then
E½εnjˆεn ¼ E½εnjE½εnjSn ¼ E½εnjSn ¼ ˆεn:
(2.184)
Therefore, Bayesian MMSE error estimators are ideal.
▪
If an analytical representation for the joint density between true and
estimated errors for fixed distributions f ðεn, ˆε• ju Þ is available, then
f ðεn, ˆε•Þ ¼
Z
U
f ðεn, ˆε•juÞpðuÞdu:
(2.185)
f ðˆε•Þ may be found either directly from f ðεn, ˆε•Þ or from analytical
representations of f ðˆε•juÞ via
f ðˆε•Þ ¼
Z
U
f ðˆε•juÞpðuÞdu:
(2.186)
From Eq. 2.185, it is clear that f ðεn, ˆε•Þ utilizes all of our modeling
assumptions, including the classification rule (because different classifiers will
have different true errors), the error estimation rule, and the Bayesian prior.
If analytical results for f ðεn, ˆε•juÞ and f ðˆε•juÞ are not available, then
E½εnjˆε• may be found via Monte Carlo approximation by simulating the
model and classification procedure to generate a large collection of true
and estimated error pairs. The MMSE calibration function may then be
87
Optimal Bayesian Error Estimation

approximated by estimating the joint density f ðεn, ˆε•Þ, or by partitioning
error estimates into bins and finding the corresponding average true error for
estimated errors falling in each bin.
Even though calibrated error estimation is suboptimal compared to
Bayesian MMSE error estimation, it has several practical advantages:
1. Given a sampling strategy and sample size, a prior on u, a classification
rule, and an error estimate ˆε•, a calibration function may be found off-
line with straightforward Monte Carlo approximation.
2. Analytical solutions may be derived using independent theoretical
work providing representations for f ðεn, ˆε•juÞ and f ðˆε•juÞ.
3. Once
a
calibration
function
has
been
established,
it
may
be
applied by post-processing a final error estimate with a simple lookup
table.
2.8.2 Performance with LDA
We again use the simulation methodology shown in Fig. 2.5 to evaluate the
performance of calibrated error estimators under a given prior model and
stratified samples of fixed size. Consider the independent general covariance
Gaussian model with known class-0 probability c ¼ 0.5 and normal-inverse-
Wishart priors on the mean and covariance pairs for each class. We use
hyperparameters
ky ¼ 3D,
n0 ¼ 6D,
n1 ¼ D,
Sy ¼ 0.03ðky  D  1ÞID,
m0 ¼ 0D, and m1 ¼ 0.1210 ⋅1D. In general, the amount of information in
each prior is reflected in the values of ky and ny, which increase as the amount
of information in the prior increases. For class 1, m1 has been adjusted to give
an expected true error of about 0.25.
In step 1 of Fig. 2.5, we generate, independently for each class, my and Sy
from the normal-inverse-Wishart prior for class y. Step 2A generates a
stratified training sample of size n ¼ 30 from the realized class-conditional
distributions N ðmy, SyÞ, with the sample sizes of both classes being fixed at
n0 ¼ n1 ¼ n∕2. These labeled sample points are used to find the posterior and
to train an LDA classifier with no feature selection in steps 2B and 2C. In step
2D, we compute the true error εn, the Bayesian MMSE error estimator ˆεn, and
the MSE of ˆεn conditioned on the sample, MSEðˆεnjSnÞ, which is discussed in
detail in Chapter 3. Since the classifier is linear, εn, ˆεn, and MSEðˆεnjSnÞ may
be computed exactly using closed-form expressions. Several classical training-
data error estimators are also computed: 5-fold cross-validation, 0.632
bootstrap, and bolstered resubstitution. The sample-conditioned MSEs of
these error estimators are also evaluated in each iteration using Theorem 3.2,
which will be discussed in Chapter 3. For each fixed feature-label distribution,
steps 2A through 2D (collectively, step 2) are repeated to obtain t ¼ 1000
samples and sets of output. Steps 1 and 2 are repeated for T ¼ 10,000
different feature-label distributions (corresponding to the randomly selected
88
Chapter 2

parameters). In total, each simulation produces tT ¼ 10,000,000 samples and
sets of output results.
After the simulation is complete, the synthetically generated true and
estimated error pairs are used to find the expected true error E½εnjˆε•
conditioned on each non-Bayesian error estimate, where ˆε• can be cross-
validation, bootstrap, or bolstering. The conditional expectation is approxi-
mated by uniformly partitioning the interval ½0, 1 into 500 bins and averaging
the true errors corresponding to error estimates that fall in each bin.
Moreover, the average true error is only found for bins with at least 100
points; otherwise, the bin is considered “rare” and the lookup table simply
leaves the error estimate unchanged (an identity mapping). The result is a
calibration function (a lookup table) mapping each of the 500 error estimate
bins to a corresponding expected true error.
Once a lookup table has been generated for each error estimator, the
entire experiment is repeated again using the same prior model, classification
rule, and classical training-data error estimators; however, at the end of each
iteration in step 2D, this time we apply the corresponding MMSE calibration
lookup table to each non-Bayesian error estimator to obtain ˆε∗. We also
report the exact true error and Bayesian sample-conditioned MSEs again, but
the Bayesian MMSE error estimator is not needed since it is not changed by
calibration, and performance would be theoretically identical to the original
experiment. As before, the procedure is iterated t ¼ 1000 times for each fixed
feature-label distribution for T ¼ 10,000 sets of feature-label distribution
parameters.
Figure 2.17 illustrates the performance of calibrated error estimators for
D ¼ 2. Figure 2.17(a) shows the expected true error conditioned on the error
estimate across iterations. The thin gray diagonal line represents an ideal error
estimator equal to the true error. Part (b) of the same figure shows the RMS
for each error estimator conditioned on the error estimate itself, which, by
definition, is given by
RMSðˆεojˆεoÞ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
E½ðεn  ˆεoÞ2jˆεo
q
,
(2.187)
and part (c) shows RMS conditioned on the true error,
RMSðˆεojεnÞ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
E½ðεn  ˆεoÞ2jεn
q
,
(2.188)
where ˆεo ∈fˆε•, ˆε∗, ˆεng. These graphs indicate error estimation accuracy for
fixed error estimates and fixed true errors. Part (d) has probability densities
for the theoretically computed RMS conditioned on the sample for each error
estimator.
Figure 2.17(a) demonstrates that calibrated error estimators and Bayesian
MMSE error estimation have ideal regression with the true error, as they must
89
Optimal Bayesian Error Estimation

according to Theorem 2.17. Furthermore, the RMS conditioned on calibrated
error estimators is significantly improved relative to their uncalibrated
counterparts, usually tracking just above the Bayesian MMSE error
estimator. The results illustrate good performance for calibrated error
estimators relative to their uncalibrated classical counterparts.
The RMS conditioned on uncalibrated error estimators tends to have a
“V” shape, achieving a minimum RMS for a very small window of estimated
errors. The RMS conditioned on a low estimated error tends to be high
because the error estimator is usually low-biased. Conditioning on a high
estimated error tends to result in a high RMS because the error estimator is
high-biased. The error estimate where the RMS is minimized approximately
corresponds to the point where the expected true error conditioned on the
error estimate crosses the ideal dotted line. Note that in a small-sample
setting, without modeling assumptions, this window where the estimated error
cv
boot
bol
cal cv
cal boot
cal bol
BEE
0
0.1
0.2
0.3
0.4
0.5
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
estimated error
E[ true error | estimated error]
(a)
0
0.1
0.2
0.3
0.4
0.5
0.04
0.06
0.08
0.1
0.12
estimated error
RMS( estimated error | estimated error)
(b)
0
0.1
0.2
0.3
0.4
0.5
0.04
0.06
0.08
0.1
0.12
true error
RMS (estimated error | true error)
(c)
0
0.05
0.1
0.15
0
10
20
30
40
50
60
70
RMS conditioned on the sample
PDF
(d)
Figure 2.17
Performance of calibrated error estimators (Gaussian model, D ¼ 2, n ¼ 30,
LDA): (a) E½εnjˆεn; (b) RMSðˆεnjˆεnÞ; (c) RMSðˆεnjεnÞ; (d) probability densities of RMSðˆεnjSnÞ.
[Reprinted from (Dalton and Dougherty, 2012a).]
90
Chapter 2

is most accurate is unknown, in contrast to Bayesian modeling where these
graphs demonstrate how to find the optimal window. Furthermore, the error-
estimate-conditioned RMS of calibrated error estimators and Bayesian
MMSE error estimators tends to monotonically increase; therefore, the
accuracy of error estimation is usually higher when the estimated error is low.
Figure 2.17(c) is a very typical representative for the behavior of the RMS
conditioned on true errors. Uncalibrated error estimators tend to be best for
low true errors, which is consistent with many studies on error estimation
accuracy (Glick, 1978; Zollanvari et al., 2011, 2012). As we have seen
previously, Bayesian MMSE error estimators are usually best for moderate
true errors, where small-sample classification is most interesting. This is also
true for calibrated error estimators, which have true-error-conditioned RMS
plots usually tracking just above the Bayesian MMSE error estimator.
Although the unconditional RMS for Bayesian MMSE error estimators is
guaranteed to be optimal (within the assumed model), in some cases the
conditional RMS of calibrated error estimators can actually outperform that
of the Bayesian MMSE error estimator for some small ranges of the true
error. Furthermore, although the unconditional RMS for a calibrated error
estimator is guaranteed to be lower than its uncalibrated counterpart,
uncalibrated error estimators can even outperform Bayesian MMSE error
estimators for some values of the true error. Nonetheless, the distribution of
the RMS conditioned on the sample for calibrated error estimators tends to
have more mass toward lower values of RMS than uncalibrated error
estimators, with the Bayesian MMSE error estimator being even more shifted
to the left.
2.9 Optimal Bayesian ROC-based Analysis
In this section, we apply optimal Bayesian estimation theory to receiver
operator characteristic (ROC) curves, which are popular tools to evaluate
classifier performance over a range of decision thresholds (Pepe et al., 2004;
Spackman, 1989; Fawcett, 2006). By varying the threshold, one can control
the trade-off between specificity and sensitivity in a test. The area under the
ROC curve (AUC) maps the entire ROC curve into a single number that
reflects the overall performance of the classifier over all thresholds. The false
positive rate (FPR) and true positive rate (TPR) evaluate performance for a
specific threshold. As with classifier error, in practice, the ROC, AUC, FPR,
and TPR must be estimated from data. A simulation study based on both
synthetic and real data by (Hanczar et al., 2010) concluded that popular
resampling estimates of the FPR, TPR, and AUC have considerable RMS
deviation from their true values over the sampling distribution for fixed class-
conditional densities. Here, we define MMSE estimates of the FPR, TPR, and
AUC relative to a posterior. The sample-conditioned MSE of any FPR or
91
Optimal Bayesian Error Estimation

TPR estimator, and an optimal Neyman–Pearson classifier based on the
theory herein, can be found in (Dalton, 2016).
2.9.1 Bayesian MMSE FPR and TPR estimation
The FPR is the probability of erroneously classifying a class-0 point
(a “negative” state), and the TPR is the probability of correctly classifying
a class-1 point (a “positive” state). Letting εi,y
n ðuy, cÞ denote the probability
that a class-y point under parameter uy is assigned class i by classifier c given
a sample of size n, FPRðu0, cÞ ¼ ε1,0
n ðu0, cÞ and TPRðu1, cÞ ¼ ε1,1
n ðu1, cÞ.
Bayesian MMSE estimates of the FPR and TPR are called the expected FPR
(EFPR) and expected TPR (ETPR), respectively. Given a classifier c that
does not depend on the parameters, the EFPR and ETPR are
d
FPRðSn, cÞ ¼ Ep∗½FPRðu0, cÞ ¼ ˆε 1,0
n ðSn, cÞ,
(2.189)
d
TPRðSn, cÞ ¼ Ep∗½TPRðu1, cÞ ¼ ˆε 1,1
n ðSn, cÞ,
(2.190)
respectively, where
ˆε i,y
n ðSn, cÞ ¼ Ep∗½εi,y
n ðuy, cÞ
(2.191)
is the posterior probability of assigning label i to a class-y point. Assuming
that c is independent of u0 and u1, in terms of EFPR and ETPR estimates, the
Bayesian MMSE error estimate takes the form
ˆεnðSn, cÞ ¼ ˆcðSnÞˆε 1,0
n ðSn, cÞ þ ð1  ˆcðSnÞÞˆε 0, 1
n
ðSn, cÞ
¼ ˆcðSnÞˆε 1,0
n ðSn, cÞ þ ð1  ˆcðSnÞÞ½1  ˆε 1,1
n ðSn, cÞ
¼ ˆcðSnÞ d
FPRðSn, cÞ þ ð1  ˆcðSnÞÞ½1  d
TPRðSn, cÞ,
(2.192)
where ˆcðSnÞ ¼ Ep∗½c is the Bayesian MMSE estimate of c. Based on the
reasoning of Theorem 2.1, d
FPRðSn, cÞ and d
TPRðSn, cÞ can be found from
the effective class-conditional densities via
ˆε i,y
n ðSn, cÞ ¼
Z
Ri
f UðxjyÞdx:
(2.193)
The EFPR can be approximated via Monte Carlo integral approximation by
drawing a large synthetic sample from the effective class-0 density f Uðxj0Þ and
evaluating the proportion of false positives. The ETPR may similarly be
found using f Uðxj1Þ and evaluating the proportion of true positives.
92
Chapter 2

2.9.2 Bayesian MMSE ROC and AUC estimation
Consider a classifier c defined via a discriminant g by cðxÞ ¼ 0 if gðxÞ ≤t and
cðxÞ ¼ 1 otherwise. An ROC curve graphs classifier performance over a
continuum of thresholds t. The horizontal axis is the FPR, the vertical axis
is the TPR, and each point on the curve represents a classifier with some
threshold t. As t increases, the point converges to ½0, 0 in the ROC space,
representing a classifier that always predicts class 0; as t decreases, the point
converges to ½1, 1, representing a classifier that always predicts class 1. The
point ½0, 1 corresponds to perfect classification, while the diagonal line from
½0, 0 to ½1, 1 corresponds to a family of classifiers with random blind decisions.
Given a discriminant g and N test points fxigN
i¼1, an empirical ROC curve
can be constructed by first sorting all test points in increasing order according
to its classification score gðxiÞ. Starting with a threshold below all scores, we
obtain an empirical FPR of 1 and a TPR of 1. By increasing the threshold
enough to cross over exactly one point, either the FPR will decrease slightly
(if the point is in the positive class) or the TPR will decrease slightly (if the
point is the negative class). Proceeding in this way, we construct a staircase-
shaped empirical ROC curve.
When test data are not available, the Bayesian framework naturally
facilitates an estimated ROC curve from training data. We define the expected
ROC (EROC) curve to be a plot of EFPR and ETPR pairs across all
thresholds. Since the EFPR and ETPR can be found as the FPR and TPR
under the effective class-conditional densities, the EROC curve is equivalent
to the ROC curve under the effective densities. Hence, the EROC may be
found by applying the ROC approximation methods described above using a
large pool of M synthetic labeled points from the effective class-conditional
densities. Since as many points as desired may be drawn from the effective
density, the EROC curve can be made as smooth as desired.
Although the AUC can be approximated by evaluating an integral, it is
also the probability that a randomly chosen sample point from the positive
class has a higher classification score than an independent randomly chosen
sample point from the negative class (assuming that scores higher than the
classifier threshold are assigned to the positive class) (Hand and Till, 2001);
that is,
AUC ¼ PrðgðX1Þ . gðX0Þju0, u1Þ,
(2.194)
where the Xy are independent random vectors governed by the class-y
densities f uyðxjyÞ. This interpretation leads to a simple way of approximating
the AUC from a number of test points: sort each test point in increasing order
according to its classification score, evaluate the sum R1 of ranks of sorted test
points in the positive class, and evaluate
93
Optimal Bayesian Error Estimation

AUC  R1  1
2 N1ðN1 þ 1Þ
N0N1
,
(2.195)
where N0 and N1 are the numbers of test points (not training points) in the
negative and positive classes, respectively.
In the absence of test data, the Bayesian framework naturally gives rise to
an expected AUC (EAUC), defined to be the MMSE estimate Ep∗½AUC of
the AUC.
Lemma 2.6 (Dalton, 2016). If u0 and u1 are independent, then the EAUC is
equivalent to the AUC under the effective densities.
Proof. Substituting Eq. 2.194 in the definition,
d
AUC ¼ Ep∗½AUC
¼ Ep∗½PrðgðX1Þ . gðX0Þ ju 0, u1Þ
¼
Z
U0
Z
U1
PrðgðX1Þ . gðX0Þju0, u1Þp∗ðu0Þp∗ðu1Þdu1du0:
(2.196)
Writing the probability as a double integral over the class-conditional
densities yields
d
AUC ¼
Z
U0
Z
U1
Z
X
Z
X
Igðx1Þ.gðx0Þf u0ðx0j0Þf u1ðx1j1Þdx1dx0
 p∗ðu0Þp∗ðu1Þdu1du0:
(2.197)
Interchanging the order of integration and applying the definition of the
effective densities yields
d
AUC ¼
Z
X
Z
X
Igðx1Þ.gðx0Þf Uðx0j0Þf Uðx1j1Þdx1dx0
¼ PrðgðZ1Þ . gðZ0ÞÞ,
(2.198)
where the Zy are independent random vectors governed by the effective class-y
conditional densities in Eq. 2.26.
▪
From Eq. 2.194, the AUC for a linear binary classifier gðxÞ ¼ aTx þ b
that predicts class 0 if gðxÞ ≤0 and 1 otherwise under N ðm0, S0Þ and
N ðm1, S1Þ distributions is given by
AUCðm0, S0, m1, S1Þ ¼ PrðaTðX0  X1Þ , 0jm0, S0, m1, S1Þ
¼ F
 
aTðm1  m0Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTðS0 þ S1Þa
p
!
,
(2.199)
94
Chapter 2

where
the
last
line
follows
because
aTðX0  X1Þ  N ðaTðm0  m1Þ,
aTðS0 þ S1ÞaÞ. Consider posteriors for the fixed covariance case, where we
can have S0 ≠S1 or S0 ¼ S1. By Lemma 2.6,
Ep∗½AUC ¼ F
0
B
@
aTðm∗
1  m∗
0Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTn∗
0þ1
n∗
0 S0 þ
n∗
1þ1
n∗
1 S1

a
r
1
C
A:
(2.200)
In the independent scaled identity and independent general covariance models
(and any other model with independent u0 and u1), by Lemma 2.6 the EAUC
may be approximated in the same way that the true AUC is approximated,
that is, by generating a large number of test points from the effective densities
and evaluating Eq. 2.195.
The EAUC has also been solved in closed form for homoscedastic
covariance models with S0 ¼ S1, although Lemma 2.6 does not apply in this
case. The AUC for fixed parameters is given in Eq. 2.199, which is of the same
form as εy in Eq. 1.32 with S ≡S0 ¼ S1 in place of Sy, m ≡ðm1  m0Þ∕
ﬃﬃﬃ
2
p
in
place of my, ð1Þy ¼ 1, and b ¼ 0. Thus, we can leverage equations for
expectations of the true error to find expectations of the AUC. By
Theorem 2.5, conditioned on S, m is Gaussian with mean ðm∗
1  m∗
0Þ∕
ﬃﬃﬃ
2
p
and covariance S∕n∗, where n∗¼ 2n∗
0n∗
1∕ðn∗
0 þ n∗
1Þ. Under the homoscedastic
scaled identity covariance model, by Theorem 2.11 we have
Ep∗½AUC ¼ 1
2

1 þ sgnðBÞI

B2
B2 þ jjajj2 trðS∗Þ ; 1
2 , ðk∗þ D þ 1ÞD
2
 1

,
(2.201)
where
B ¼ aTðm∗
1  m∗
0Þ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n∗
0n∗
1
n∗
0 þ n∗
1 þ 2n∗
0n∗
1
s
:
(2.202)
Under the homoscedastic general covariance model, by Theorem 2.12,
Ep∗½AUC ¼ 1
2

1 þ sgnðBÞI

B2
B2 þ aTS∗a ; 1
2 , k∗ D þ 1
2

,
(2.203)
where B is again given by Eq. 2.202 (Hassan et al., 2019).
2.9.3 Performance study
To illustrate the ROC theory, we assume Gaussian class-conditional
densities of dimension D ¼ 2 with parameterized mean–covariance pairs
95
Optimal Bayesian Error Estimation

uy ¼ ðmy, SyÞ, where Sy is inverse-Wishart with ky degrees of freedom and
scale matrix Sy, and my given Sy is Gaussian with mean my and covariance
Sy∕ny. We use the following hyperparameters: n0 ¼ n1 ¼ 1, m0 ¼ m1 ¼ 0D,
k0 ¼ k1 ¼ D þ 2, and S0 ¼ S1 ¼ 0.3ID. These hyperparameters represent a
low-information prior, where ny and ky have the minimal integer values
necessary for the priors to be proper densities and the expected means and
covariances to exist (these are 0D and 0.3ID for both classes, respectively).
From each mean and covariance pair we draw a stratified sample of size n
with n∕2 points per class for classifier design and performance evaluation. We
assume that c ¼ 1∕2 is known. We classify using a support vector machine
(SVM) with radial basis function kernel (RBF-SVM). The SVM classifiers are
trained using the LIBSVM software package in MATLAB® (Chang and Lin,
2011). The “default” threshold for all of these classifiers is t ¼ 0.
To evaluate the performance of FPR and TPR estimators, we generate
10,000 random mean and covariance pairs from the prior, and from each pair
we draw a training sample of size n (varying from 20 to 100 points) with n∕2
points independently drawn from each class to train the classifiers using the
default threshold ðt ¼ 0Þ. The true FPR and TPR are found exactly for LDA.
For nonlinear classifiers they are approximated by drawing a synthetic testing
sample of size N ¼ 100,000 with N∕2 points in each class from the true
density parameterized by my and Sy, and evaluating the proportion of false
and true positives. Similarly, the EFPR and ETPR are found exactly for LDA
and are approximated for nonlinear classifiers by drawing a synthetic sample
of size M ¼ 100,000 with M∕2 points per class from the effective densities,
and evaluating the proportion of false and true positives.
We also evaluate four standard FPR and TPR estimators: resubstitution
(resub), leave-one-out (loo), 10-fold cross-validation with 10 repetitions (cv),
and 0.632 bootstrap with 250 repetitions (boot). The resub FPR estimator
d
FPRresub is the proportion of false positives among class-0 training points, and
the resub TPR estimator d
TPRresub is the proportion of true positives among
class-1 training points. There are multiple cross-validation- and bootstrap-
based methods for FPR, TPR, AUC, and ROC estimation. In our
implementation of cv for FPR and TPR estimation, in each repetition we
randomly partition the training set into stratified folds, and for each fold we
train a surrogate classifier using all points not in the fold and apply this
classifier to each point in the fold. The final cv FPR and TPR estimates are
the proportions of false positives and true positives, respectively, among held-
out points from all folds and repetitions. loo is a special case of cv, where each
fold contains precisely one point, and thus only one partition is possible and
repetition is not necessary. In each repetition of boot, we train a surrogate
classifier on a bootstrap sample and apply this classifier to each point not in
the bootstrap sample. The bootstrap-zero estimates d
FPRboot and d
TPRboot are
96
Chapter 2

the proportion of false positives and true positives among held-out points
from all repetitions, and the 0.632 bootstrap estimates are given by
d
FPR0.632 boot ¼ 0.368 d
FPRresub þ 0.632 d
FPRboot,
(2.204)
d
TPR0.632 boot ¼ 0.368 d
TPRresub þ 0.632 d
TPRboot:
(2.205)
Figure 2.18(a) provides the mean of d
FPR  FPR, or the mean bias of
FPR estimators over random distributions and samples with respect to sample
size for RBF-SVM and each of the five FPR estimation methods.
Figure 2.18(b) analogously shows the mean bias of TPR estimators. The
EFPR and ETPR not only appear unbiased, but they are theoretically
unbiased when conditioned on the sample; that is, Ep∗½ d
FPR  FPR ¼ 0 and
Ep∗½ d
TPR  TPR ¼ 0.
In Fig. 2.18(c), we plot the square root of the mean, over random
distributions and samples, of ð d
FPR  FPRÞ2 þ ð d
TPR  TPRÞ2 for RBF-
SVM and each estimator. This quantifies the accuracy of each FPR and TPR
estimator pair, and we denote it by RMSð d
FPR, d
TPRÞ. Even with low-
information priors, observe that EFPR and ETPR have significantly superior
performance relative to other FPR and TPR estimators. Indeed, they are
theoretically optimal in these graphs. To see why, recall that EFPR and
20
40
60
80
100
−0.1
−0.05
0
0.05
sample size
mean of FPR
FPR
resub
loo
cv
boot
EFPR
(a)
20
40
60
80
100
−0.05
0
0.05
0.1
sample size
mean of TPR
TPR
resub
loo
cv
boot
ETPR
(b)
20
40
60
80
100
0
0.1
0.2
0.3
sample size
RMS( FPR TPR)
resub
loo
cv
boot
Bayesian
(c)
Figure 2.18
Performance of FPR and TPR estimators for RBF-SVM with default threshold:
(a) FPR bias; (b) TPR bias; (c) RMSð d
FPR, d
TPRÞ. [Reprinted from (Dalton, 2016).]
97
Optimal Bayesian Error Estimation

ETPR minimize their respective sample-conditioned MSE and thus also
minimize the quantity
MSEð d
FPRjSnÞ þ MSEð d
TPRjSnÞ ¼ Ep∗
h
ð d
FPR  FPRÞ2 þ ð d
TPR  TPRÞ2i
,
(2.206)
which is the mean-square Euclidean distance of the point ½ d
FPR, d
TPR from
the point ½FPR, TPR in the ROC space, with respect to the posterior.
RMSð d
FPR, d
TPRÞ approximates the square root of the expectation of
Eq. 2.206 with respect to the sampling distribution. All of this holds for the
default threshold and any other thresholding method that does not depend on
knowing the parameters.
To examine ROC and AUC estimation methods, we (1) generate 10,000
random mean and covariance pairs from the prior, (2) for each pair draw a
training sample of size n with n∕2 independent points from each class, and (3)
from each sample find the discriminant function for each classifier. We then
use methods outlined previously to approximate the true ROC and AUC
using N ¼ 100,000 test points with N∕2 points from each class.
We examine five ROC and AUC estimation methods, corresponding to
the FPR and TPR estimation methods as discussed above. For resubstitution,
an empirical ROC curve and estimated AUC may be found by applying the
previous methods with training data rather than independent test data. For
leave-one-out, for each training point x, we design a surrogate classifier with
training data excluding x, and apply the surrogate classifier to the point x to
produce a classification score. The resulting score and label corresponding to
each left-out point are pooled into a single dataset, which is again used in
place of test data to find ROC curves and AUC estimates. Cross-validation
and bootstrap are performed similarly by generating random re-sampled
training sets in the usual manner (via folds in the case of cross-validation and
a bootstrap sample in the case of bootstrap estimation), training a surrogate
classifier for each re-sampled training set, and pooling the scores and labels
from holdout points in all folds and repetitions. This aggregated score-label
dataset is used to estimate the ROC and AUC. The bootstrap ROC and AUC
estimates are not averaged with resubstitution. Finally, we find the EROC
and EAUC using M ¼ 100,000 points drawn from the effective densities with
M∕2 points from each class.
Consider the true and estimated ROC curves for a single sample of size
n ¼ 60 and RBF-SVM in Fig. 2.19. The true ROC curve for the Bayes
classifier represents a bound in achievable performance, while the true ROC
curve for the RBF-SVM classifier is the curve we wish to estimate and must
always be below the ROC curve for the Bayes classifier. Estimated ROC
curves from each of the five estimation methods discussed are also shown.
98
Chapter 2

In this example, the EROC curve is smooth and quite accurate. Indeed, the
EROC curve is optimal in the sense that every point on the curve ð d
FPR, d
TPRÞ
minimizes Eq. 2.206 for the corresponding classifier and threshold.
Suppose that we desire an FPR of 0.1. This is illustrated in Fig. 2.19 as a
thin vertical gray line. Each ROC estimator crosses this line at coordinates
ð d
FPR, d
TPRÞ, where
d
FPR is as close as possible to 0.1. For each ROC
0
0.2
0.4
0.6
0.8
1
0
0.2
0.4
0.6
0.8
1
FPR
TPR
Bayes true
RBF-SVM true
RBF-SVM resub
RBF-SVM loo
RBF-SVM cv
RBF-SVM boot
RBF-SVM EROC
Figure 2.19
Example true and estimated ROC curves for RBF-SVM with a training set of
size n ¼ 60. Circles (from left to right: loo, boot, cv, resub, EROC) indicate the true FPR and
TPR for thresholds corresponding to the estimated FPR and TPR closest to the desired
FPR. [Reprinted from (Dalton, 2016).]
20
40
60
80
100
−0.1
−0.05
0
0.05
0.1
sample size
mean of AUC
AUC
resub
loo
cv
boot
EAUC
(a)
20
40
60
80
100
0
0.05
0.1
0.15
0.2
sample size
uncond. RMS of AUC
resub
loo
cv
boot
EAUC
(b)
Figure 2.20
Performance of AUC estimators for RBF-SVM: (a) bias; (b) unconditional
RMS. [Reprinted from (Dalton, 2016).]
99
Optimal Bayesian Error Estimation

estimator, this point corresponds to a specific threshold of the classifier, which
also corresponds to a true FPR and TPR, marked as circles at the coordinates
ðFPR, TPRÞ on the true ROC curve. Using the EFPR to select a threshold
results in a true FPR that is closer to the desired FPR than obtained by any
other FPR estimator in this example.
Bias and unconditional RMS graphs for AUC estimation are shown in
Fig. 2.20 for RBF-SVM. As expected, the EAUC appears unbiased with
superior RMS performance, while all other estimators are biased.
100
Chapter 2

Chapter 3
Sample-Conditioned MSE
of Error Estimation
There are two sources of randomness in the Bayesian model. The first is the
sample, which randomizes the designed classifier and its true error. Most
results on error estimator performance are averaged over random samples,
which demonstrates performance relative to a fixed feature-label distribution.
The second source of randomness is uncertainty in the underlying feature-
label distribution. The Bayesian MMSE error estimator addresses the second
source of randomness. This gives rise to a practical expected measure of
performance given a fixed sample and classifier.
Up until this point, unless otherwise stated, we have assumed that c and
ðu0, u1Þ are independent to simplify the Bayesian MMSE error estimator.
However, our interest is now in evaluating the MSE of an error estimator
itself, which generally depends on the variances and correlations between the
errors contributed by both classes. For the sake of simplicity, throughout this
chapter we avoid the need to evaluate correlations by making the stronger
assumption that c, u0, and u1 are all mutually independent. See Chapter 5 and
(Dalton and Yousefi, 2015) for derivations in the general case.
3.1 Conditional MSE of Error Estimators
For a fixed sample Sn, the sample-conditioned MSE of an arbitrary error
estimator ˆε• is defined to be
MSEðˆε•ðSn, cÞjSnÞ ¼ Ep½ðεnðu, cÞ  ˆε•ðSn, cÞÞ2:
(3.1)
This is precisely the objective function optimized by the Bayesian MMSE
error estimator. Also define the conditional MSE for the Bayesian MMSE
error estimate for each class:
MSEðˆε y
n ðSn, cÞjSnÞ ¼ Ep½ðεy
nðuy, cÞ  ˆε y
n ðSn, cÞÞ2:
(3.2)
101

The next theorem provides an expression for MSEðˆεnjSnÞ, the sample-
conditioned MSE of the Bayesian MMSE error estimator, in terms of the
MSE of the ˆε y
n , which may be decomposed into the first and second posterior
moments of εy
n.
Theorem 3.1 (Dalton and Dougherty, 2012b). Let c, u0, and u1 be mutually
independent given the sample. Then the conditional MSE of the Bayesian
MMSE error estimator is given by
MSEðˆεnjSnÞ ¼ varpðcÞðˆε 0n  ˆε 1n Þ2 þ Ep½c2 MSEðˆε 0n jSnÞ
þ Ep½ð1  cÞ2 MSEðˆε 1n jSnÞ,
(3.3)
where
MSEðˆε y
n jSnÞ ¼ Ep½ðεy
nðuyÞÞ2  ðˆε y
n Þ2:
(3.4)
Proof. According to MMSE estimation theory and applying the definition of
the Bayesian MMSE error estimator, by the orthogonality principle,
MSEðˆεnjSnÞ ¼ Eu½ðεnðuÞ  ˆεnÞ2jSn
¼ Eu½ðεnðuÞ  ˆεnÞεnðuÞjSn þ Eu½ðεnðuÞ  ˆεnÞˆεnjSn
¼ Eu½ðεnðuÞ  ˆεnÞεnðuÞjSn
¼ Eu½ðεnðuÞÞ2jSn  ðˆεnÞ2
¼ varuðεnðuÞjSnÞ;
(3.5)
that is, the conditional MSE of the Bayesian MMSE error estimator is
equivalent
to
the
posterior
variance
of
the
true
error.
Similarly,
MSEðˆε y
n jSnÞ ¼ varuyðεy
nðuyÞjSnÞ. By the law of total variance,
MSEðˆεnjSnÞ ¼ varc,u0,u1ðcε0nðu0Þ þ ð1  cÞε1nðu1ÞjSnÞ
¼ varcðEu0,u1½cε0nðu0Þ þ ð1  cÞε1nðu1Þjc, SnjSnÞ
þ Ec½varu0,u1ðcε0nðu0Þ þ ð1  cÞε1nðu1Þjc, SnÞjSn:
(3.6)
Further decomposing the inner expectation and variance, while recalling that
ˆε y
n ¼ Ep½εy
nðuyÞ, and c, u0, and u1 are mutually independent, yields
MSEðˆεnjSnÞ ¼ varcðcˆε0n þð1cÞˆε1njSnÞ
þEc½c2jSnvaru0ðε0nðu0ÞjSnÞ
þEc½ð1cÞ2jSnvaru1ðε1nðu1ÞjSnÞ
¼ varpðcÞðˆε0n  ˆε1nÞ2
þEp½c2varpðε0nðu0ÞÞþEp½ð1cÞ2varpðε1nðu1ÞÞ,
(3.7)
102
Chapter 3

which is equivalent to Eq. 3.3. Furthermore,
MSEðˆε y
n jSnÞ ¼ varpðεy
nðuyÞÞ ¼ Ep½ðεy
nðuyÞÞ2  ðˆε y
n Þ2,
(3.8)
which completes the proof.
▪
The variance and expectations related to the variable c depend on the
prior for c. For example, if pðcÞ is beta with hyperparameters ay, then
Ep½c ¼
a
0
a
0 þ a
1
(3.9)
and
Ep∗½c2 ¼
a∗
0ða∗
0 þ 1Þ
ða∗
0 þ a∗
1Þða∗
0 þ a∗
1 þ 1Þ .
(3.10)
Hence,
MSEðˆεnjSnÞ ¼
a
0a
1
ða
0 þ a
1Þ2ða
0 þ a
1 þ 1Þ ðˆε 0n  ˆε 1n Þ2
þ
a
0ða
0 þ 1Þ
ða
0 þ a
1Þða
0 þ a
1 þ 1Þ MSEðˆε 0n jSnÞ
þ
a
1ða
1 þ 1Þ
ða
0 þ a
1Þða
0 þ a
1 þ 1Þ MSEðˆε 1n jSnÞ
¼ 
ða
0Þ2
ða
0 þ a
1Þ2 ðˆε 0n Þ2 
ða
1Þ2
ða
0 þ a
1Þ2 ðˆε 1n Þ2

2a
0a
1
ða
0 þ a
1Þ2ða
0 þ a
1 þ 1Þ ˆε 0n ˆε 1n
þ
a
0ða
0 þ 1Þ
ða
0 þ a
1Þða
0 þ a
1 þ 1Þ Ep½ðε0nðu0ÞÞ2
þ
a
1ða
1 þ 1Þ
ða
0 þ a
1Þða
0 þ a
1 þ 1Þ Ep½ðε1nðu1ÞÞ2:
(3.11)
Therefore, the conditional MSE for fixed samples is solved if we can find the
first moment Ep½εy
nðuyÞ and second moment Ep½ðεy
nðuyÞÞ2 of the true error
for y ∈{0, 1}.
Having characterized the conditional MSE of the Bayesian MMSE
error estimator, it is easy to find analogous results for an arbitrary error
estimate.
103
Sample-Conditioned MSE of Error Estimation

Theorem 3.2 (Dalton and Dougherty, 2012b). Let ˆε• be an error estimate
evaluated from a given sample. Then
MSEðˆε•jSnÞ ¼ MSEðˆεnjSnÞ þ ðˆεn  ˆε•Þ2:
(3.12)
Proof. Since Eu½εnðuÞjSn ¼ ˆεn,
MSEðˆε•jSnÞ ¼ Eu½ðεnðuÞ  ˆε•Þ2jSn
¼ Eu½ðεnðuÞ  ˆεn þ ˆεn  ˆε•Þ2jSn
¼ Eu½ðεnðuÞ  ˆεnÞ2jSn
þ 2ðˆεn  ˆε•ÞEu½εnðuÞ  ˆεnjSn þ ðˆεn  ˆε•Þ2
¼ MSEðˆεnjSnÞ þ ðˆεn  ˆε•Þ2,
(3.13)
which completes the proof.
▪
If we find MSEðˆεnjSnÞ, it is trivial to evaluate the conditional MSE of
any error estimator MSEðˆε•jSnÞ under the Bayesian model. Furthermore,
Theorem 3.2 clearly shows that the conditional MSE of the Bayesian MMSE
error estimator lower bounds the conditional MSE of any other error estimator.
3.2 Evaluation of the Conditional MSE
Having found the first posterior error moments for the models we have been
considering, we now focus on evaluating the second posterior moments. In
each case, when combined with Theorem 3.1, this will provide a representa-
tion of the sample-conditioned MSE. The following theorem shows how
second posterior moments can be found via the effective joint class-conditional
density, which is defined by
f Uðx, zjyÞ ¼
Z
Uy
f uyðxjyÞf uyðzjyÞpðuyÞduy
¼ Ep
f uyðxjyÞf uyðzjyÞ

:
(3.14)
Theorem 3.3 (Dalton and Yousefi, 2015). Let c be a fixed classifier given by
cðxÞ ¼ 0 if x ∈R0 and cðxÞ ¼ 1 if x ∈R1, where R0 and R1 are measurable
sets partitioning the feature space. Then the second moment of the true error can
be found by
Ep½ðεy
nðuyÞÞ2 ¼
Z
X\Ry
Z
X\Ry
f Uðx, zjyÞdxdz:
(3.15)
104
Chapter 3

Proof. From Eq. 2.28,
Ep½ðεy
nðuyÞÞ2 ¼
Z
Uy
Z
X\Ry
f uyðxjyÞdx
Z
X\Ry
f uyðzjyÞdzpðuyÞduy
¼
Z
Uy
Z
X\Ry
Z
X\Ry
f uyðxjyÞf uyðzjyÞdxdzpðuyÞduy:
(3.16)
Since 0 ≤εy
nðuyÞ ≤1 and hence Ep½ðεy
nðuyÞÞ2 must be finite, and because
f Uðx, zjyÞ is a valid density, we obtain the result by Fubini’s theorem.
▪
Note that
Ep½ðεy
nðuyÞÞ2 ¼
Z
X
Z
X
Ix∈=RyIz∈=Ry f Uðx, zjyÞdxdz
¼ PrðX ∈= Ry, Z ∈= RyÞ,
(3.17)
where in the last line X and Z are random vectors drawn from the effective
joint density. Moreover, recognizing that f Uðx, zjyÞ is a valid density, the
marginals of the effective joint density are both precisely the effective density.
For example:
Z
X
f Uðx, zjyÞdz ¼
Z
X
Z
Uy
f uyðxjyÞf uyðzjyÞpðuyÞduydz
¼
Z
Uy
f uyðxjyÞ
Z
X
f uyðzjyÞdzpðuyÞduy
¼
Z
Uy
f uyðxjyÞpðuyÞduy
¼ f UðxjyÞ:
(3.18)
In the special case where p is a point mass at the true distribution
parameters
uy,
the
effective
joint
density
becomes
f Uðx, zjyÞ ¼
f uyðxjyÞf uyðzjyÞ, indicating that X and Z would be independent and identically
distributed (i.i.d.) with density f uy. According to Eq. 3.17,
PrðX ∈= Ry, Z ∈= RyÞ ¼ PrðX ∈= RyjuyÞ PrðZ ∈= RyjuyÞ ¼ ½εy
nðuyÞ2
(3.19)
and MSEðεy
nðuyÞÞ ¼ ½εy
nðuyÞ2  ½εy
nðuyÞ2 ¼ 0. In this case, there is no
uncertainty in the true error of the classifier. When there is uncertainty in
uy, the effective joint density loses independence so that X and Z must be
considered jointly. This leads to a positive MSE, or uncertainty in the true
error of the classifier.
105
Sample-Conditioned MSE of Error Estimation

If f Uðx, zjyÞ can be found for a specific model, Theorem 3.3 provides an
efficient method to evaluate or approximate the conditional MSE for any
classifier. For instance, in the Gaussian model, although we have closed-
form solutions for the conditional MSE under linear classifiers, the optimal
Bayesian classifier and many other classification rules are generally not
linear. To evaluate the conditional MSE for a classifier and model where
closed-form solutions are unavailable, one may approximate the conditional
MSE by generating a large number of synthetic sample pairs from
f Uðx, zjyÞ. Each sample pair can be realized by first drawing a realization
of X from the marginal effective density f UðxjyÞ, and then drawing a
realization
of
Z
given
X
from
the
conditional
effective
density
f Uðzjx, yÞ ¼ f Uðx, zjyÞ∕f UðxjyÞ. Equation 3.17 is approximated by evaluat-
ing the proportion of pairs for which both points are misclassified,
i.e., cðxÞ ≠y and cðzÞ ≠y. Further, since f UðxjyÞ is the effective density,
the same realizations of X can be used to approximate the Bayesian MMSE
error estimator by evaluating the proportion of x values that are
misclassified.
In the following sections, we derive closed-form expressions for f Uðx, zjyÞ,
f Uðzjx, yÞ, and the second moment of the true error for each class in our
discrete and Gaussian models.
3.3 Discrete Model
For the discrete model with Dirichlet priors, Theorem 2.2 shows that pðu0Þ
and pðu1Þ are also Dirichlet distributions given by Eqs. 2.49 and 2.50 with
updated hyperparameters a0
i
¼ a0
i þ U0
i and a1
i
¼ a1
i þ U1
i .
Theorem 3.4 (Dalton and Yousefi, 2015). Suppose that class y ¼ 0 is multinomial
with bin probabilities p1, p2, . . . , pb that have a Dirichletða0
1 , a0
2 , : : : , a0
b Þ
posterior, a0
i
. 0 for all i. The effective joint density for class 0 is given by
f Uði, jj0Þ ¼ Ep½pipj ¼
a0
i a0
j
ðPb
k¼1 a0
k Þð1 þ Pb
k¼1 a0
k Þ
(3.20)
for i, j ∈f1, 2, : : : , bg with i ≠j, and
f Uði, ij0Þ ¼ Ep½p2
i  ¼
a0
i ða0
i þ 1Þ
ðPb
k¼1 a0
k Þð1 þ Pb
k¼1 a0
k Þ
(3.21)
for i ∈f1, 2, : : : , bg. The effective joint density for class 1 is similar, except
with qi in place of pi and a1
i
in place of a0
i
for i ¼ 1, : : : , b.
106
Chapter 3

Proof. For bin indices i, j ∈f1, 2, : : : , bg, by definition,
f Uði, jj0Þ ¼ Ep½pipj:
(3.22)
Equation 3.20 follows from the second-order moments for Dirichlet
distributions given in Eq. 2.48. Similarly, Eq. 3.21 follows from Eq. 2.47.
The case for class 1 is similar.
▪
Theorem 3.5 (Dalton and Dougherty, 2012b). For the discrete model with
Dirichletðay
1 , ay
2 , : : : , ay
b Þ posteriors for class y ∈f0, 1g, ay
i
. 0 for all i
and y,
Ep½ðεy
nðuyÞÞ2 ¼ ðˆε y
n Þ2ðPb
k¼1 ay
k Þ
1 þ Pb
k¼1 ay
k
þ
ˆε y
n
1 þ Pb
k¼1 ay
k
:
(3.23)
Proof. By Theorems 3.3 and 3.4,
Ep½ðεy
nðuyÞÞ2 ¼
X
b
i¼1
X
b
j¼1
ay
i ðay
j þ dijÞ
ðPb
k¼1 ay
k Þð1 þ Pb
k¼1 ay
k Þ IcðiÞ≠yIcðjÞ≠y,
(3.24)
where dij is the Kronecker delta function, which equals 1 if i ¼ j and 0
otherwise. Continuing,
Ep ½ðεy
nðuyÞÞ2
¼
X
b
i¼1
ay
i
ðPb
k¼1 ay
k Þð1 þ Pb
k¼1 ay
k Þ IcðiÞ≠y
2
4X
b
j¼1
ay
j IcðjÞ≠y þ IcðiÞ≠y
3
5
¼
Pb
k¼1 ay
k
1 þ Pb
k¼1 ay
k
2
4X
b
i¼1
ay
i
ðPb
k¼1 ay
k Þ IcðiÞ≠y
3
5
2
4X
b
j¼1
ay
j
ðPb
k¼1 ay
k Þ IcðjÞ≠y
3
5
þ
1
1 þ Pb
k¼1 ay
k
2
4X
b
i¼1
ay
i
ðPb
k¼1 ay
k Þ IcðiÞ≠y
3
5:
(3.25)
We write this using the first moment via Eq. 2.56.
▪
By Theorem 3.5 and Eq. 2.56, the conditional MSE in the discrete model
is given by Eq. 3.3, where
MSEðˆε y
n jSnÞ ¼ ðˆε y
n Þ2ðPb
k¼1 ay
k Þ
1 þ Pb
k¼1 ay
k
þ
ˆε y
n
1 þ Pb
k¼1 ay
k
 ðˆε y
n Þ2
¼
ˆε y
n ð1  ˆε y
n Þ
1 þ Pb
k¼1 ay
k
:
(3.26)
107
Sample-Conditioned MSE of Error Estimation

Work in (Berikov and Litvinenko, 2003) derives expressions for the
characteristic
function
of
the
misclassification
probability
fðtÞ ¼
Ep½expðitεnðu, cÞÞ, where i is the imaginary unit. The same discrete Bayesian
model is used, allowing for an arbitrary number of classes K with known prior
class probabilities and the flat prior over the bin probabilities. Given the
characteristic function, the first and second moments of the true error,
equivalent to the Bayesian MMSE error estimator and conditional MSE of
the Bayesian MMSE error estimator, respectively, are derived. Higher-order
moments of the true error can also be found from the characteristic function,
although the equations become tedious.
Example 3.1. To demonstrate how the theoretical conditional RMS provides
practical performance results for small samples, in contrast with distribution-
free RMS bounds, which are too loose to be useful for small samples, we use the
simulation methodology outlined in Fig. 2.5 with a discrete model and fixed bin
size b. Assume that c ¼ 0.5 is known and fixed, and let the bin probabilities of
class
0
and
1
have
Dirichlet
priors
given
by
the
hyperparameters
a0
i ∝2b  2i þ 1 and a1
i ∝2i  1, respectively, where the ay
i are normalized
such that Pb
i¼1 ay
i ¼ b for y ∈f0, 1g. In step 1 of Fig. 2.5, we generate random
bin probabilities from the Dirichlet priors by first generating 2b independent
gamma-distributed random variables, gy
i  gammaðay
i , 1Þ for i ¼ 1, 2, . . . , b
and y ∈{0, 1}. The bin probabilities are then given by
pi ¼
g0
i
Pb
k¼1 g0
k
,
(3.27)
qi ¼
g1
i
Pb
k¼1 g1
k
:
(3.28)
In step 2A we generate a random sample of size n; that is, the number of
class-0 points n0 is determined using a binomialðn, cÞ experiment. Then n0
points are drawn from the discrete distribution ½p1, : : : , pb, and n1 ¼ nn0
points are drawn from the discrete distribution ½q1, : : : , qb. Although the
classes are equally likely, the number of training points from each class may
not be the same. In step 2B the prior is updated to a posterior given the full
training sample. In step 2C we train a discrete histogram classifier that breaks
ties toward class 0. Finally, in step 2D we evaluate the exact error of the
trained classifier, the Bayesian MMSE error estimator with “correct” priors
(used to generate the true bin probabilities), and a leave-one-out error
estimator. The Bayesian MMSE error estimator is found by evaluating
Eq. 2.13 with Ep½c ¼ 0.5 and ˆε y
n defined in Eq. 2.56. The sample-conditioned
RMS is computed from Eq. 3.3. The sampling procedure is repeated t ¼ 1000
108
Chapter 3

times for each fixed feature-label distribution, with T ¼ 10,000 feature-label
distributions, for a total of 10,000,000 samples.
From these experiments, it is possible to approximate the uncondi-
tional MSE (averaged over both the feature-label distribution and the
sampling distribution) for any error estimator ˆε• using one of two methods:
(1) the “semi-analytical” unconditional MSE computes MSEðˆε•jSnÞ using
closed-form expressions for the sample-conditioned MSE for each sample/
iteration and averages
over these values, and (2) the
“empirical”
unconditional MSE computes the squared difference ðεn  ˆε•Þ2 between
the true error εn and the error estimate ˆε• for each sample/iteration and
averages over these values. The semi-analytical RMS and empirical RMS
are the square roots of the semi-analytical MSE and empirical MSE,
respectively. Throughout, we use the semi-analytical unconditional MSE
unless otherwise indicated.
Figures 3.1(a) and (b) show the probability densities of the sample-
conditioned RMS for both the Bayesian MMSE error estimator and the
leave-one-out error estimator. The sample sizes for each experiment are
chosen such that the expected true error of the trained classifier is 0.25.
Within each plot, we also show the unconditional semi-analytical RMS of
both the leave-one-out and Bayesian MMSE error estimators, as well as the
distribution-free RMS bound on the leave-one-out error estimator for the
discrete histogram rule with tie-breaking in the direction of class 0 given in
Eq. 1.48. Jaggedness in part (a) is not due to poor density estimation or
Monte Carlo approximation, but rather is caused by the discrete nature of
0
0.05
0.1
0.15
0.2
0.25
0.3
0
20
40
60
80
RMS conditioned on the sample
(a)
(b)
PDF
Devroye:
loo:
BEE:
RMS≤ 1.0366 
RMS = 0.151
RMS = 0.0698
loo
BEE
0
0.05
0.1
0.15
0.2
0.25
0.3
0
20
40
60
80
100
120
RMS conditioned on the sample
PDF
Devroye:
loo:
BEE:
RMS≤ 0.8576 
RMS = 0.1103
RMS = 0.0518
loo
BEE
Figure 3.1
Probability densities for the conditional RMS of the leave-one-out and Bayesian
MMSE error estimators with correct priors: (a) b ¼ 8, n ¼ 16; (b) b ¼ 16, n ¼ 30. The sample
sizes for each experiment were chosen such that the expected true error is 0.25. The
unconditional RMS for both error estimators is also shown, as well as Devroye’s distribution-
free bound. [Reprinted from (Dalton and Dougherty, 2012c).]
109
Sample-Conditioned MSE of Error Estimation

the problem. In particular, the expressions for ˆε 0n , ˆε 1n , Ep½ðε0nðu0ÞÞ2,
and Ep½ðε1nðu1ÞÞ2 can take on only a finite set of values, which is especially
small for a small number of bins or sample points. In both parts of Fig. 3.1
(as well as in other unshown plots for different values of b and n), the density
of the conditional RMS for the Bayesian MMSE error estimator is much
tighter than that of leave-one-out. For example, in Fig. 3.1(b) the
conditional RMS of the Bayesian MMSE error estimator tends to be very
close to 0.05, whereas the leave-one-out error estimator has a long tail with
substantial mass between 0.05 and 0.2. Furthermore, the conditional RMS
for the Bayesian MMSE error estimator is concentrated on lower values of
RMS, so much so that in all cases the unconditional RMS of the Bayesian
MMSE error estimator is less than half that of the leave-one-out error
estimator.
Without any kind of modeling assumptions, distribution-free bounds on
the unconditional RMS are too loose to be useful. In fact, the bound from
Eq. 1.48 is greater than 0.85 in both subplots of Fig. 3.1. On the other hand, a
Bayesian framework facilitates exact expressions for the RMS conditioned on
the sample for both the Bayesian MMSE error estimator and any other error
estimation rule.
3.4 Gaussian Model
We next consider the Gaussian model. For linear classifiers, we have closed-
form Bayesian MMSE error estimators for four models: fixed covariance, scaled
identity covariance, diagonal covariance, and general covariance. However, note
that independence between u0 and u1 implies that the following results are not
applicable in homoscedastic models. To avoid cluttered notation, in this section
we denote hyperparameters without subscripts.
3.4.1 Effective joint class-conditional densities
Known Covariance
The following theorem provides a closed form for the effective joint density in
the known covariance model.
Theorem 3.6 (Dalton and Yousefi, 2015). If n . 0 and Sy is a fixed symmetric
positive definite matrix, then
f Uðx, zjyÞ  N
 "
m
m
#
,
" nþ1
n S
1
n Sy
1
n Sy
nþ1
n Sy
#!
:
(3.29)
110
Chapter 3

Proof. First note that
f uy ðxjyÞf uyðzjyÞpðmyjlyÞ
¼
1
ð2pÞ
D
2jSyj
1
2 exp

 1
2 ðx  myÞTS1
y ðx  myÞ


1
ð2pÞ
D
2jSyj
1
2 exp

 1
2 ðz  myÞTS1
y ðz  myÞ


ðnÞ
D
2
ð2pÞ
D
2jSyj
1
2 exp

 n
2 ðmy  mÞTS1
y ðmy  mÞ

:
(3.30)
We can view this as the joint Gaussian density for X, Z, and my, which is
currently of the form f ðx, z, myÞ ¼ f ðxjmyÞf ðzjmyÞf ðmyÞ. The effective joint
density f Uðx, zjyÞ is precisely the marginal density of X and Z:
f Uðx, zjyÞ ¼
ðnÞ
D
2
ð2pÞDðn þ 2Þ
D
2jSyj
 exp
 
 1
2
n þ 1
n þ 2 ðx  mÞTS1
y ðx  mÞ
!
 exp
 
 1
2
n þ 1
n þ 2 ðz  mÞTS1
y ðz  mÞ
!
 exp
 
 1
2
 2
n þ 2 ðx  mÞTS1
y ðz  mÞ
!
:
(3.31)
Note that
" nþ1
n Sy
1
n Sy
1
n Sy
nþ1
n Sy
#1
¼
"
nþ1
nþ2 S1
y

1
nþ2 S1
y

1
nþ2 S1
y
nþ1
nþ2 S1
y
#
:
(3.32)
It is straightforward to check that f Uðx, zjyÞ is Gaussian with mean and
covariance as given in Eq. 3.29.
▪
As expected, the marginal densities of X and Z are both equivalent to the
effective density (Dalton and Dougherty, 2013a), i.e.,
f UðxjyÞ  N

m, n þ 1
n
Sy

:
(3.33)
Further, conditioned on X ¼ x,
111
Sample-Conditioned MSE of Error Estimation

f Uðzjx, yÞ  N
nm þ x
n þ 1 , n þ 2
n þ 1 Sy

:
(3.34)
X and Z become independent as ny →`.
Scaled Identity Covariance
For a model with unknown covariance,
f Uðx, zjyÞ ¼
Z
Uy
f uyðxjyÞf uyðzjyÞpðuyÞduy
¼
Z
Ly
Z
RD f uyðxjyÞf uyðzjyÞpðmyjlyÞdmypðlyÞdly,
(3.35)
where ly parameterizes the covariance matrix. The inner integral was solved
in Theorem 3.6. Hence,
f Uðx, zjyÞ ¼
Z
Ly
ðnÞ
D
2
ð2pÞDðn þ 2Þ
D
2jSyj
 exp
 
 1
2
n þ 1
n þ 2 ðx  mÞTS1
y ðx  mÞ
!
 exp
 
 1
2
n þ 1
n þ 2 ðz  mÞTS1
y ðz  mÞ
!
 exp
 
 1
2
 2
n þ 2 ðx  mÞTS1
y ðz  mÞ
!
pðlyÞdly:
(3.36)
We introduce the following theorem, which will be used in the scaled identity,
diagonal, and general covariance models.
Theorem 3.7. In the independent covariance model, where Sy is parameterized
by ly and the posterior
pðlyÞ ∝jSyjkþDþ1
2
etr

 1
2 SS1
y

(3.37)
is normalizable, the effective joint density for class y is given by
f Uðx, zjyÞ ∝
Z
Ly
jSyjkþDþ3
2
etr

 1
2 hðx, z, yÞS1
y

dly,
(3.38)
112
Chapter 3

where
hðx, z, yÞ ¼ n þ 1
n þ 2 ðx  mÞðx  mÞT þ n þ 1
n þ 2 ðz  mÞðz  mÞT

2
n þ 2 ðz  mÞðx  mÞT þ S
¼
n
n þ 1 ðx  mÞðx  mÞT
þ n þ 1
n þ 2

z  m  x  m
n þ 1

z  m  x  m
n þ 1
T
þ S:
(3.39)
Proof. The proof follows from the definition
f Uðx, zjyÞ ∝
Z
Ly
jSyj1 exp
 
 1
2
n þ 1
n þ 2 ðx  mÞTS1
y ðx  mÞ
!
 exp
 
 1
2
n þ 1
n þ 2 ðz  mÞTS1
y ðz  mÞ
!
 exp
 
 1
2
 2
n þ 2 ðx  mÞTS1
y ðz  mÞ
!
 jSyjkþDþ1
2
etr

 1
2 SS1
y

dly
¼
Z
Ly
jSyjkþDþ3
2
etr

 1
2 hðx, z, yÞS1
y

dly:
(3.40)
The normalization constant depends on the parameterization variable ly. ▪
In the case of a scaled identity covariance, ly ¼ s2y, and from
Theorem 3.7,
f Uðx, zjyÞ ∝
Z `
0
ðs2yÞðkþDþ3ÞD
2
etr

 1
2s2y
hðx, z, yÞ

ds2y:
(3.41)
The integrand is essentially an inverse-Wishart distribution, which has a
known normalization constant; thus,
f Uðx, zjyÞ ∝trðhðx, z, yÞÞðkþDþ3ÞD2
2
,
(3.42)
where the proportionality treats x and z as variables. In (Dalton and Yousefi,
2015), it is shown that f Uðx, zjyÞ is a multivariate t-distribution with k  D
degrees of freedom, location vector ½ðmÞT, ðmÞTT, and scale matrix
113
Sample-Conditioned MSE of Error Estimation

trðSÞ
nðk  DÞ

ðn þ 1ÞID
ID
ID
ðn þ 1ÞID

,
(3.43)
where k ¼ ðk þ D þ 2ÞD  2.
Given X ¼ x, we have
f Uðzjx, yÞ ∝f Uðx, zjyÞ ∝trðhðx, z, yÞÞðkþDþ3ÞD2
2
,
(3.44)
where the first proportionality treats z as a variable and x as constant. Letting
S ¼
n
n þ 1 ðx  mÞðx  mÞT þ S,
(3.45)
we have
trðhðx, z, yÞÞ
¼ n þ 1
n þ 2

z  m  x  m
n þ 1
T
z  m  x  m
n þ 1

þ trðSÞ
∝1 þ
n þ 1
ðn þ 2Þ trðSÞ

z  m  x  m
n þ 1
T
z  m  x  m
n þ 1

:
(3.46)
Thus,
f Uðzjx, yÞ
∝
"
1 þ 1
k

z  m  x  m
n þ 1
T
ðUIDÞ1

z  m  x  m
n þ 1
#kþD
2
,
(3.47)
where
U ¼
ðn þ 2Þ trðS∗∗Þ
ðn þ 1Þ½ðk∗þ D þ 2ÞD  2 .
(3.48)
Hence, f Uðzjx, yÞ is a multivariate t-distribution with k degrees of freedom,
location vector ðnm þ xÞ∕ðn þ 1Þ, and scale matrix UID.
Diagonal Covariance
Applying Theorem 3.7 in the diagonal covariance model, with ly ¼
½s2
y1, s2
y2, : : : , s2
yD,
114
Chapter 3

f U ðx, zjyÞ
∝
Z `
0
···
Z `
0
Y
D
i¼1
ðs2
yiÞkþDþ3
2
etr

 1
2 hðx, z, yÞS1
y

ds2
y1 ··· s2
yD
¼
Z `
0
···
Z `
0
Y
D
i¼1
ðs2
yiÞkþDþ3
2
exp

 1
2
X
D
i¼1
hiðxi, zi, yÞ
s2
yi

ds2
y1 ··· s2
yD
¼
Y
D
i¼1
Z `
0
ðs2
yiÞkþDþ3
2
exp

 hiðxi, zi, yÞ
2s2
yi

ds2
yi,
(3.49)
where hiðxi, zi, yÞ is the ith diagonal element of hðx, z, yÞ [note that
hiðxi, zi, yÞ depends on only xi and zi, the ith elements of x and z,
respectively]. Similar to Eq. 3.41 in the scaled identity case, the integrand is
an unnormalized inverse-Wishart distribution. Thus,
f Uðx, zjyÞ ∝
Y
D
i¼1
½hiðxi, zi, yÞkþDþ1
2
:
(3.50)
In (Dalton and Yousefi, 2015), it is shown that f Uðx, zjyÞ is the joint
distribution of D independent pairs of bivariate t-random vectors, where the
ith pair has k  1 degrees of freedom, location parameter ½m
i , m
i T, and scale
parameter s
iiðnI2 þ 122Þ∕½nðk  1Þ. Note that k ¼ k þ D, m
i is the ith
element of m, and s
ii is the ith diagonal element of S.
The conditional density is given by
f Uðzjx, yÞ ∝f Uðx, zjyÞ
∝
Y
D
i¼1
½hiðxi, zi, yÞkþDþ1
2
∝
Y
D
i¼1
"
1 þ 1
k
ðn þ 1Þk
ðn þ 2Þs
ii

zi  m
i  xi  m
i
n þ 1
2#kþ1
2
,
(3.51)
where s
ii ¼ ½n∕ðn þ 1Þðxi  m
i Þ2 þ s
ii is the ith diagonal element of
S. Thus, f Uðzjx, yÞ is the joint distribution of D independent non-
standardized Student’s t-distributions, where the ith distribution has k degrees
of freedom, location parameter ðnm
i þ xiÞ∕ðn þ 1Þ, and scale parameter
ðn þ 2Þs
ii ∕½ðn þ 1Þk.
115
Sample-Conditioned MSE of Error Estimation

General Covariance
For the general covariance model, f Uðx, zjyÞ can be found from Theorem 3.7,
with ly ¼ Sy:
f Uðx, zjyÞ ∝
Z
Sy≻0
jSyjkþDþ3
2
etr

 1
2 hðx, z, yÞS1
y

dSy:
(3.52)
The parameter space Sy ≻0 is the space of all symmetric positive definite
matrices. The integrand is essentially an inverse-Wishart distribution, which
has a known normalization constant. Hence, letting
x ¼ m þ x  m
n þ 1 ,
(3.53)
we have
f Uðx, zjyÞ ∝jhðx, z, yÞjkþ2
2
¼

n
n þ 1 ðx  mÞðx  mÞT þ n þ 1
n þ 2 ðz  xÞðz  xÞT þ S

kþ2
2
∝

n þ 2
n þ 1 S þ ðz  xÞðz  xÞT

kþ2
2 :
(3.54)
Applying the property
jaaT þ Aj ¼ jAjð1 þ aTA1aÞ
(3.55)
yields
f U ðx, zjyÞ
∝

n þ 2
n þ 1 S

kþ2
2
"
1 þ ðz  xÞT
n þ 2
n þ 1 S
1
ðz  xÞ
#kþ2
2
∝

n þ 2
n þ 1 S

kþD
2

 
1 þ 1
k ðz  xÞT
 n þ 2
ðn þ 1Þk S
1
ðz  xÞ
!kþD
2
,
(3.56)
where k ¼ k  D þ 2.
Although f U ðx, zjyÞ is not easy to characterize in terms of standard
distributions, the conditional density is given by
116
Chapter 3

f U ðzjx, yÞ
∝f Uðx, zjyÞ
∝
 
1 þ 1
k ðz  xÞT
 n þ 2
ðn þ 1Þk S
1
ðz  xÞ
!kþD
2
:
(3.57)
Conditioned on X ¼ x, Z is a multivariate t-distribution with k degrees of
freedom, location parameter x, and scale matrix
n þ 2
ðn þ 1Þk S ¼
n þ 2
ðn þ 1Þðk  D þ 2Þ

n
n þ 1 ðx  mÞðx  mÞT þ S

:
(3.58)
3.4.2 Sample-conditioned MSE for linear classification
We will present closed-form expressions for the conditional MSE of Bayesian
MMSE error estimators under Gaussian distributions with linear classifica-
tion for three models: known, scaled identity, and general covariance. We
assume a linear classifier of the form cðxÞ ¼ 0 if gðxÞ ≤0 and cðxÞ ¼ 1
otherwise, where gðxÞ ¼ aTx þ b. Rather than use the effective joint density,
we take a direct approach, as in (Dalton and Dougherty, 2012b); see Section
5.7 for similar derivations based on the effective joint density. Note that the
required first and second moments are given by
Ep
h
ðεy
nðuyÞÞki
¼
Z
Uy
½εy
nðuyÞkpðuyÞduy
¼
Z
Ly
Z
RD ½εy
nðmy, lyÞkpðmy jl yÞdmypðlyÞdly
(3.59)
for k ¼ 1 and k ¼ 2, respectively, where Uy is the parameter space for uy, and
Ly is the parameter space for ly.
For a fixed invertible covariance Sy, we require that n . 0 to ensure that
the posterior pðmyjlyÞ is proper. To find the desired moments, we require a
technical lemma.
Lemma 3.1 (Dalton and Dougherty, 2012b). Let y ∈{0, 1}, n > 0, m ∈RD,
S be an invertible covariance matrix, and gðxÞ ¼ aTx þ b, where a ∈RD is a
nonzero length D vector, and b ∈R is a scalar. Then
117
Sample-Conditioned MSE of Error Estimation

Z
RD F2
ð1ÞygðmÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSa
p

f m, 1
nSðmÞdm
¼ Id.0½2FðdÞ  1 þ 1
p
Z tan1

 ﬃﬃﬃﬃﬃﬃ
nþ2
p
ﬃﬃﬃ
n
p

0
exp


d2
2sin2 u

du,
(3.60)
where d is given in Eq. 2.125.
Proof. Call this integral M. Then
M ¼
Z
RD F2
ð1ÞygðmÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSa
p


ðnÞ
D
2
ð2pÞ
D
2jSj
1
2 exp

 n
2 ðm  mÞTS1ðm  mÞ

dm:
(3.61)
Since S is an invertible covariance matrix, we can use singular value
decomposition to write S ¼ WWT with jSj ¼ jWj2. Using the linear change
of variables z ¼
ﬃﬃﬃﬃﬃ
n
p
W1ðm  m∗Þ,
M ¼
Z
RD F2
 ð1Þyð 1ﬃﬃﬃﬃ
n
p
aTWz þ aTm þ bÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSa
p
!

1
ð2pÞ
D
2 exp

 zTz
2

dz:
(3.62)
Define
a ¼ ð1ÞyWTa
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
naTSa
p
,
(3.63)
b ¼ ð1ÞygðmÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSa
p
,
(3.64)
and note that kak2 ¼ ðnÞ1. Then
M ¼
Z
RD F2ðaTz þ bÞ
1
ð2pÞ
D
2 exp

 zTz
2

dz
¼
Z
RD
Z aTzþb
`
1ﬃﬃﬃﬃﬃﬃ
2p
p
exp

 x2
2

dx
Z aTzþb
`
1ﬃﬃﬃﬃﬃﬃ
2p
p
exp

 y2
2

dy

1
ð2pÞ
D
2 exp

 zTz
2

dz
¼
Z
RD
Z aTzþb
`
Z aTzþb
`
1
ð2pÞ
Dþ2
2 exp
 
 x2 þ y2 þ zTz
2
!
dxdydz:
(3.65)
118
Chapter 3

Next consider a change of variables w ¼ Rz, where R rotates the vector a
to the vector ½1∕
ﬃﬃﬃﬃﬃ
n
p
, 0T
D1T. Since R is a rotation matrix, jRj ¼ 1 and RTR is
an identity matrix. Let the first element in the vector w be called w. Then the
preceding integral simplifies to
M ¼
Z
RD
Z aTRTwþb
`
Z aTRTwþb
`
1
ð2pÞ
Dþ2
2 exp

 x2 þ y2 þ wTw
2

dxdydw
¼
Z `
`
Z
1ﬃﬃﬃ
n
p wþb
`
Z
1ﬃﬃﬃ
n
p wþb
`
1
ð2pÞ
3
2 exp

 x2 þ y2 þ w2
2

dxdydw:
(3.66)
This reduces the problem to a three-dimensional space.
Now consider the following rotation of the coordinate system:
" x0
y0
w0
#
¼
2
6664

ﬃﬃﬃﬃﬃ
2n
p
2 ﬃﬃﬃﬃﬃﬃﬃﬃ
nþ2
p

ﬃﬃﬃﬃﬃ
2n
p
2 ﬃﬃﬃﬃﬃﬃﬃﬃ
nþ2
p
ﬃﬃ
2
p
ﬃﬃﬃﬃﬃﬃﬃﬃ
nþ2
p
ﬃﬃ
2
p
2

ﬃﬃ
2
p
2
0
1ﬃﬃﬃﬃﬃﬃﬃﬃ
nþ2
p
1ﬃﬃﬃﬃﬃﬃﬃﬃ
nþ2
p
ﬃﬃﬃﬃ
n
p
ﬃﬃﬃﬃﬃﬃﬃﬃ
nþ2
p
3
7775
" x
y
w
#
:
(3.67)
This rotates the vector ½x, y, wT ¼ ½1, 1,
ﬃﬃﬃﬃﬃ
n
p
T to the vector ½x0, y0, w0T ¼
½0, 0,
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 2
p
T. To determine the new region of integration, note that in the
½x, y, wT coordinate system the region of integration is defined by two
restrictions: x , ð
ﬃﬃﬃﬃﬃ
n
p
Þ1w þ b and y , ð
ﬃﬃﬃﬃﬃ
n
p
Þ1w þ b. In the new coordinate
system, the first restriction is

ﬃﬃﬃﬃﬃﬃﬃ
2n
p
2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 2
p
x0 þ
ﬃﬃﬃ
2
p
2 y0 þ
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 2
p
w0 ,
1ﬃﬃﬃﬃﬃ
n
p
 
ﬃﬃﬃ
2
p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 2
p
x0 þ
ﬃﬃﬃﬃﬃ
n
p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 2
p
w0
!
þ b:
(3.68)
Equivalently,
y0 ,
 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 2
p
ﬃﬃﬃﬃﬃ
n
p
!
x0 þ
ﬃﬃﬃ
2
p
b:
(3.69)
And, similarly, for the other restriction,
y0 ,
 
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 2
p
ﬃﬃﬃﬃﬃ
n
p
!
x0 þ
ﬃﬃﬃ
2
p
b:
(3.70)
We have designed our new coordinate system to make the variable w0
independent from these restrictions. Hence, w0 may be integrated out of our
original integral, which may be simplified to
119
Sample-Conditioned MSE of Error Estimation

M ¼
Z `
`
Z `
ﬃﬃﬃ
n
pﬃﬃﬃﬃﬃﬃ
nþ2
p
ðjy0j ﬃﬃ
2
p
bÞ
1
2p exp

 x02 þ y02
2

dx0dy0
¼ 2
Z `
0
Z `
ﬃﬃﬃ
n
pﬃﬃﬃﬃﬃﬃ
nþ2
p
ðy0 ﬃﬃ
2
p
bÞ
1
2p exp

 x02 þ y02
2

dx0dy0:
(3.71)
If b ≤0, then we convert to polar coordinates ðr, uÞ, using
x0 ¼ r cos
 
tan1

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 2
p
ﬃﬃﬃﬃﬃ
n
p

 u
!
(3.72)
and
y0 ¼ r sin
 
tan1

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 2
p
ﬃﬃﬃﬃﬃ
n
p

 u
!
(3.73)
to obtain
M ¼ 1
p
Z tan1

 ﬃﬃﬃﬃﬃﬃ
nþ2
p
ﬃﬃﬃ
n
p

0
Z `

ﬃﬃﬃ
n
p
b
ﬃﬃﬃﬃﬃﬃ
nþ1
p
sin u
exp

 r2
2

rdrdu:
(3.74)
Let u ¼ 0.5r2. Then
M ¼ 1
p
Z tan1

 ﬃﬃﬃﬃﬃﬃ
nþ2
p
ﬃﬃﬃ
n
p

0
Z `
n b2
2ðnþ1Þsin2 u
expðuÞdudu
¼ 1
p
Z tan1

 ﬃﬃﬃﬃﬃﬃ
nþ2
p
ﬃﬃﬃ
n
p

0
exp
 

nb2
2ðn þ 1Þsin2 u
!
du:
(3.75)
On the other hand, if b . 0, then from Eq. 3.71,
M ¼ 1
p
Z `
0
Z `
ﬃﬃﬃ
n
pﬃﬃﬃﬃﬃﬃ
nþ2
p
y0 exp

 x02 þ y02
2

dx0dy0
þ 1
p
Z `
0
Z
ﬃﬃﬃ
n
pﬃﬃﬃﬃﬃﬃ
nþ2
p
y0
ﬃﬃﬃ
n
pﬃﬃﬃﬃﬃﬃ
nþ2
p
ðy0 ﬃﬃ
2
p
bÞ
exp

 x02 þ y02
2

dx0dy0:
(3.76)
Using
the
result
for
b ≤0,
the
first
double
integral
equals
p1tan1ð
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 2
p
∕
ﬃﬃﬃﬃﬃ
n
p
Þ. For the second integral, we use the same polar
transformation and u-substitution as before:
120
Chapter 3

1
p
Z `
0
Z
ﬃﬃﬃ
n
pﬃﬃﬃﬃﬃﬃ
nþ2
p
y0
ﬃﬃﬃ
n
pﬃﬃﬃﬃﬃﬃ
nþ2
p
ðy0 ﬃﬃ
2
p
bÞ
exp

 x02 þ y02
2

dx0dy0
¼ 1
p
Z 0
tan1

 ﬃﬃﬃﬃﬃﬃ
nþ2
p
ﬃﬃﬃ
n
p

p
Z 
ﬃﬃﬃ
n
p
b
ﬃﬃﬃﬃﬃﬃ
nþ1
p
sin u
0
exp

 r2
2

rdrdu
¼ 1
p
Z 0
tan1

 ﬃﬃﬃﬃﬃﬃ
nþ2
p
ﬃﬃﬃ
n
p

p
Z
n b2
2ðnþ1Þsin2 u
0
expðuÞdudu
¼ 1
p
Z 0
tan1

 ﬃﬃﬃﬃﬃﬃ
nþ2
p
ﬃﬃﬃ
n
p

p
"
1  exp


nb2
2ðn þ 1Þsin2 u
#
du:
(3.77)
Thus,
M ¼ 1  1
p
Z 0
tan1

 ﬃﬃﬃﬃﬃﬃ
nþ2
p
ﬃﬃﬃ
n
p

p
exp


nb2
2ðn þ 1Þsin2 u

du:
(3.78)
This may be simplified by realizing that a component of this integral is
equivalent to an alternative representation for the Gaussian CDF function
(Craig, 1991). We first break the integral into two parts, and then use
symmetry in the integrand to simplify the result:
M ¼ 1  1
p
Z 0
p
exp


nb2
2ðn þ 1Þsin2 u

du
þ 1
p
Z tan1

 ﬃﬃﬃﬃﬃﬃ
nþ2
p
ﬃﬃﬃ
n
p

p
p
exp


nb2
2ðn þ 1Þsin2 u

du
¼ 1  1
p
Z p
0
exp


nb2
2ðn þ 1Þsin2 u

du
þ 1
p
Z tan1

 ﬃﬃﬃﬃﬃﬃ
nþ2
p
ﬃﬃﬃ
n
p

0
exp


nb2
2ðn þ 1Þsin2 u

du
¼ 2F

ﬃﬃﬃﬃﬃ
n
p
b
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 1
p

 1 þ 1
p
Z tan1

 ﬃﬃﬃﬃﬃﬃ
nþ2
p
ﬃﬃﬃ
n
p

0
exp


nb2
2ðn þ 1Þsin2 u

du,
(3.79)
which completes the proof.
▪
121
Sample-Conditioned MSE of Error Estimation

Theorem 3.8 (Dalton and Dougherty, 2012b). For the Gaussian model with
fixed covariance matrix Sy, assuming that n . 0,
Ep½ðεy
nðuyÞÞ2 ¼ Id.0½2FðdÞ  1
þ 1
p
Z tan1

 ﬃﬃﬃﬃﬃﬃ
nþ2
p
ﬃﬃﬃ
n
p

0
exp


d2
2sin2 u

du
(3.80)
for y ∈f0, 1g, where d is defined in Eq. 2.125.
Proof. The outer integral in the definition of the second moment in Eq. 3.59 is
not necessary because the current model has a fixed covariance. Hence, we
need only solve the inner integral, which is given by
Ep½ðεy
nðuyÞÞ2 ¼
Z
RD ½εy
nðmy, lyÞ2pðmyjlyÞdmy
¼
Z
RD F2
 
ð1ÞygðmyÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSya
q
!
f m, 1
nSyðmyÞdmy:
(3.81)
This integral is simplified to a well-behaved single integral in Lemma 3.1. ▪
Theorem 3.1 in conjunction with Eqs. 2.124 and 3.80 gives MSEðˆεnjSnÞ
for the fixed covariance model.
Continuing with the Gaussian model, now assume that Sy is a scaled
identity covariance matrix with ly ¼ s2y and Sy ¼ s2yID. The posterior
distribution is given in Lemma 2.1 as Eq. 2.100, and the first moment is given
in Eq. 2.132.
The second posterior moment in this case involves two special functions,
one being the regularized incomplete beta function given in Eq. 2.128. The
other function Rðx, y; aÞ is defined via the Appell hypergeometric function F1.
Specifically,
Rðx, 0; aÞ ¼ 1
p sin1ð
ﬃﬃﬃx
p Þ
(3.82)
and
Rðx, y; aÞ ¼
ﬃﬃﬃy
p
pð2a þ 1Þ

x
x þ y
aþ1
2
 F 1

a þ 1
2 ; 1
2 , 1; a þ 3
2 ; xðy þ 1Þ
x þ y
,
x
x þ y

(3.83)
for a > 0, 0 < x < 1, and y > 0, where
122
Chapter 3

F1ða; b, b0; c; z, z0Þ ¼
GðcÞ
GðaÞGðc  aÞ
Z 1
0
ta1ð1  tÞca1ð1  ztÞbð1  z0tÞb0dt
(3.84)
is defined for jzj , 1, jz0j , 1, and 0 < a < c. Closed-form expressions for both
R and the regularized incomplete beta function I for integer or half-integer
values of a are discussed in Section 3.4.3.
Lemma 3.2 (Dalton and Dougherty, 2012b). Let A ∈R, p/4 < B < p/2, a > 0,
and b > 0. Let f ðx; a, bÞ be the inverse-gamma distribution with shape
parameter a and scale parameter b. Then
Z `
0
 
IA.0
"
2F
 Aﬃﬃﬃz
p

 1
#
þ 1
p
Z B
0
exp


A2
2z sin2 u

du
!
f ðz; a, bÞdz
¼ IA.0I

A2
A2 þ 2b ; 1
2 , a

þ R

sin2 B, A2
2b ; a

:
(3.85)
Proof. Call this integral M. When A ¼ 0, it is easy to show that M ¼ B∕p.
For A ≠0,
M ¼
Z `
0
IA.0

2F
 Aﬃﬃﬃz
p

 1

f ðz; a, bÞdz
þ 1
p
Z `
0
Z B
0
exp


A2
2z sin2 u

duf ðz; a, bÞdz
¼ IA.0

2
Z `
0
F
 Aﬃﬃﬃz
p

f ðz; a, bÞdz  1

þ 1
p
Z `
0
Z B
0
exp


A2
2z sin2 u

duf ðz; a, bÞdz:
(3.86)
The integral in the first term is solved in (Dalton and Dougherty, 2011c,
Lemma D.1). We have
M ¼ IA.0 sgnðAÞI

A2
A2 þ 2b ; 1
2 , a

þ 1
p
Z `
0
Z B
0
exp


A2
2z sin2 u

duf ðz; a, bÞdz
¼ IA.0I

A2
A2 þ 2b ; 1
2 , a

þ 1
p
Z B
0
Z `
0
exp


A2
2z sin2 u

f ðz; a, bÞdzdu:
(3.87)
This intermediate result will be used in Lemma 3.3.
123
Sample-Conditioned MSE of Error Estimation

We next focus on the inner integral in the second term. Call this integral N.
We have
N ¼
Z `
0
exp


A2
2z sin2 u
 ba
GðaÞ ·
1
zaþ1 exp

 b
z

dz
¼ ba
GðaÞ
Z `
0
1
zaþ1 exp



b þ
A2
2sin2 u
 1
z

dz
¼ ba
GðaÞ ·
GðaÞ

b þ
A2
2sin2 u
a
¼
 
sin2 u
sin2 u þ A2
2b
!a
,
(3.88)
where we have solved this integral by noting that it is essentially an inverse-
gamma distribution. Thus,
M ¼ IA.0I

A2
A2 þ 2b ; 1
2 , a

þ 1
p
Z B
0
 
sin2 u
sin2 u þ A2
2b
!a
du:
(3.89)
Under the substitution u ¼ ðsin2 uÞ∕ðsin2 BÞ,
Z B
0
 
sin2 u
sin2 u þ A2
2b
!a
du
¼
Z 1
0
 
u sin2 B
u sin2 B þ A2
2b
!a sin B
2
u1
2ð1  u sin2 BÞ1
2du
¼ sin2aþ1B
2
2b
A2
a Z 1
0
ua1
2ð1  u sin2 BÞ1
2

1 þ u 2b sin2 B
A2
a
du:
(3.90)
This is essentially a one-dimensional Euler-type integral representation of the
Appell hypergeometric function F1. In other words,
Z B
0
 
sin2 u
sin2 u þ A2
2b
!a
du
¼ sin2aþ1B
2a þ 1
2b
A2
a
F 1

a þ 1
2 ; 1
2 , a; a þ 3
2 ; sin2 B,  2b sin2 B
A2

:
(3.91)
Finally, from the identity
124
Chapter 3

F1ða; b, b0; c; z, z0Þ ¼ ð1  z0ÞaF1

a; b, c  b  b0; c; z  z0
1  z0 , 
z0
1  z0

(3.92)
in (Slater, 1966), we obtain
Z B
0
 
sin2 u
sin2 u þ A2
2b
!a
du
¼
ﬃﬃﬃﬃ
A2
2b
q
2a þ 1
 
sin2 B
sin2 B þ A2
2b
!aþ1
2
 F1
 
a þ 1
2 ; 1
2 , 1; a þ 3
2 ;
ðA2
2b þ 1Þsin2 B
sin2 B þ A2
2b
,
sin2 B
sin2 B þ A2
2b
!
¼ pR

sin2 B, A2
2b ; a

:
(3.93)
Combining this result with Eq. 3.89 completes the proof.
▪
Theorem 3.9 (Dalton and Dougherty, 2012b). In the Gaussian model with
scaled identity covariance matrix Sy ¼ s2yID,
Ep½ðεy
nðuyÞÞ2 ¼ IA.0I

A2
A2 þ 2b ; 1
2 , a

þ R
 n þ 2
2ðn þ 1Þ , A2
2b ; a

,
(3.94)
where
A ¼ ð1ÞygðmÞ
kak2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n
n þ 1
r
,
(3.95)
a ¼ ðk þ D þ 1ÞD
2
 1,
(3.96)
b ¼ 1
2 trðSÞ,
(3.97)
and it is assumed that n . 0, a . 0, and b . 0.
Proof. We require that n . 0 to ensure that pðmyjlyÞ is proper, and we
require that a . 0 and b . 0 to ensure that pðs2yÞ is proper. Use Lemma 3.1
for the inner integral so that Eq. 3.59 reduces to
125
Sample-Conditioned MSE of Error Estimation

Ep½ðεy
nðuyÞÞ2
¼
Z `
0
 
IA.0
"
2F
 
Aﬃﬃﬃﬃﬃ
s2y
q
!
1
#
þ 1
p
Z B
0
exp


A2
2s2ysin2u

du
!
f ðs2y;a,bÞds2y,
(3.98)
where
B ¼ tan1ð
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 2
p
∕
ﬃﬃﬃﬃﬃ
n
p
Þ,
and
f ð⋅; a, bÞ
is
the
inverse-gamma
distribution with shape parameter a and scale parameter b. This integral is
solved in Lemma 3.2, leading to Eq. 3.94.
▪
Theorem 3.1 in conjunction with Eqs. 2.132 and 3.94 gives MSEðˆεnjSnÞ
for the scaled identity model.
We next consider the general covariance model, in which Sy ¼ ly, and
the parameter space Ly contains all symmetric positive definite matrices. The
posterior distribution is given in Eq. 2.119 and the first posterior moment in
Eq. 2.137. Prior to the theorem we provide a technical lemma.
Lemma 3.3 (Dalton and Dougherty, 2012b). Let A ∈R, p∕4 , B , p∕2,
a ∈RD be a nonzero column vector, k > D1, and S be a symmetric positive
definite
D  D
matrix.
Also
let
fIWðS; S, kÞ
be
an
inverse-Wishart
distribution with parameters S and k. Then
Z
S≻0
 
IA.0

2F

A
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSa
p

 1

þ 1
p
Z B
0
exp


A2
ð2sin2 uÞaTSa

du
!
 fIWðS; S, kÞdS
¼ IA.0I

A2
A2 þ aTSa ; 1
2 , k  D þ 1
2

þ R

sin2 B,
A2
aTSa ; k  D þ 1
2

:
(3.99)
Proof. Call this integral M. If A ¼ 0, it is easy to show that M ¼ B∕p. If
A ≠0, then
M ¼
Z
S≻0
IA.0

2F

A
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSa
p

 1

fIWðS; S, kÞdS
þ 1
p
Z
S≻0
Z B
0
exp


A2
ð2sin2 uÞaTSa

dufIWðS; S, kÞdS
¼ IA.0

2
Z
S≻0
F

A
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSa
p

fIWðS; S, kÞdS  1

þ 1
p
Z
S≻0
Z B
0
exp


A2
ð2sin2 uÞaTSa

dufIWðS; S, kÞdS:
(3.100)
126
Chapter 3

The integral in the first term is solved in (Dalton and Dougherty, 2011c,
Lemma E.1). We have
M ¼ IA.0sgnðAÞI

A2
A2 þ aTSa ; 1
2 , k  D þ 1
2

þ 1
p
Z
S≻0
Z B
0
exp


A2
ð2sin2 uÞaTSa

dufIWðS; S, kÞdS
¼ IA.0I

A2
A2 þ aTSa ; 1
2 , k  D þ 1
2

þ 1
p
Z B
0
Z
S≻0
exp


A2
ð2sin2 uÞaTSa

fIWðS; S, kÞdSdu:
(3.101)
Define the constant matrix
C ¼

aT
0D1
j
ID1

.
(3.102)
Since a is nonzero, with a simple reordering of the dimensions we can
guarantee that a1 ≠0. The value of aTSa is unchanged by such a redefinition,
so, without loss of generality, assume that C is invertible. Consider the change
of variables Y ¼ CSCT. Since C is invertible, Y is symmetric positive definite
if and only if S is also. Furthermore, the Jacobean determinant of this
transformation is jCjDþ1 (Arnold, 1981; Mathai and Haubold, 2008). Note
that y11 ¼ aTSa, where y11 is the upper-left element of Y, and we have
M ¼ IA.0I

A2
A2 þ aTSa ; 1
2 , k  D þ 1
2

þ 1
p
Z B
0
Z
Y≻0
exp


A2
2y11sin2 u

fIWðC1YðCTÞ1; S, kÞ
 jjCjD1jdY du
¼ IA.0I

A2
A2 þ aTSa ; 1
2 , k  D þ 1
2

þ 1
p
Z B
0
Z
Y≻0
exp


A2
2y11sin2 u

fIWðY; CSCT, kÞdY du:
(3.103)
Since the inner integrand now depends on only one parameter in Y, namely
y11, the other parameters can be integrated out. It can be shown that for
any D  D inverse-Wishart random matrix X with density fIWðX; V, mÞ,
the upper-left element x11 of X is also inverse-Wishart with density
fIWðx11; v11, m  D þ 1Þ, where v11 is the upper-left element of V (Muller
and Stewart, 2006). x11 is also an inverse-gamma random variable with
127
Sample-Conditioned MSE of Error Estimation

density f ðx11; ðm  D þ 1Þ∕2, v11∕2Þ. Here, the upper-left element of CSCT
is aTSa, so
M ¼ IA.0I

A2
A2 þ aTSa ; 1
2 , k  D þ 1
2

þ 1
p
Z B
0
Z `
0
exp


A2
2y11sin2 u

f

y11; k  D þ 1
2
, aTSa
2

dy11du
¼ IA.0I

A2
A2 þ 2b ; 1
2 , a

þ 1
p
Z B
0
Z `
0
exp


A2
2y11sin2 u

f ðy11; a, bÞdy11du,
(3.104)
where we have defined
a ¼ k  D þ 1
2
,
(3.105)
b ¼ aTSa
2
:
(3.106)
Note that a > 0, b > 0, and this integral is exactly the same as Eq. 3.87. Hence,
we apply Lemma 3.2 to complete the proof.
▪
Theorem 3.10 (Dalton and Dougherty, 2012b). In the Gaussian model with
general covariance matrix, assuming that n . 0, k . D  1, and S is
symmetric positive definite,
Ep½ðεy
nðuyÞÞ2 ¼ IA.0I

A2
A2 þ 2b ; 1
2 , a

þ R
 n þ 2
2ðn þ 1Þ , A2
2b ; a

,
(3.107)
where
A ¼ ð1ÞygðmÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n
n þ 1
r
,
(3.108)
a ¼ k  D þ 1
2
,
(3.109)
b ¼ aTSa
2
:
(3.110)
128
Chapter 3

Proof. Using the same method as in the case of a scaled covariance, note that
Ep½ðεy
nðuyÞÞ2 ¼
Z
Sy≻0
0
B
@IA.0
2
642F
0
B
@
A
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSya
q
1
C
A  1
3
75
þ 1
p
Z B
0
exp


A2
ð2sin2 uÞaTSya

du
1
C
AfIWðSy; S, kÞdSy,
(3.111)
where B ¼ tan1ð
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
n þ 2
p
∕
ﬃﬃﬃﬃﬃ
n
p
Þ, and fIWðSy; S, kÞ is the inverse-Wishart
distribution with parameters S and k. Applying Lemma 3.3 yields the
desired second moment.
▪
Theorem 3.1 in conjunction with Eqs. 2.137 and 3.107 gives MSEðˆεnjSnÞ
for the general covariance model.
3.4.3 Closed-form expressions for functions I and R
The solutions proposed in the previous sections utilize two Euler integrals.
The first is the incomplete beta function Iðx; a, bÞ defined in Eq. 2.128. In our
application, we only need to evaluate Iðx; 1∕2, aÞ for 0 ≤x < 1 and a > 0. The
second integral is the function Rðx, y; aÞ, defined in Eq. 3.83 via the Appell
hypergeometric function F1. Although these integrals do not have closed-form
solutions for arbitrary parameters, in this section we provide exact expressions
for Iðx; 1∕2, N∕2Þ and Rðx, y; N∕2Þ for positive integers N. Restricting k to
be an integer guarantees that these equations may be applied so that Bayesian
MMSE error estimators and their conditional MSEs for the Gaussian model
with linear classification may be evaluated exactly using finite sums of
common single-variable functions.
Lemma 3.4 (Dalton and Dougherty, 2011c). Let N be a positive integer and let
0 ≤x ≤1. Then Iðx; 1∕2, 1∕2Þ ¼ ð2∕pÞsin1ð
ﬃﬃﬃx
p Þ,
I

x; 1
2 , N
2

¼ 2
p sin1ð
ﬃﬃﬃx
p Þ þ 2
p
ﬃﬃﬃx
p X
N1
2
k¼1
ð2k  2Þ!!
ð2k  1Þ!! ð1  xÞk1
2
(3.112)
for any odd N . 1, Iðx; 1∕2, 1Þ ¼
ﬃﬃﬃx
p , and
I

x; 1
2 , N
2

¼
ﬃﬃﬃx
p þ
ﬃﬃﬃx
p X
N2
2
k¼1
ð2k  1Þ!!
ð2kÞ!!
ð1  xÞk
(3.113)
for any even N > 2, where !! denotes the double factorial.
129
Sample-Conditioned MSE of Error Estimation

Proof. Ið1; a, bÞ ¼ 1 is a property of the regularized incomplete beta function
for all a, b > 0. For the remainder of this proof we assume that 0 ≤x < 1.
We have
I

x; 1
2 , N
2

¼ G
Nþ1
2

G
1
2

G
N
2

Z x
0
t1
2ð1  tÞ
N2
2 dt:
(3.114)
Using the substitution sin u ¼
ﬃﬃt
p gives
I

x; 1
2 , N
2

¼ G
Nþ1
2

G
1
2

G
N
2

Z sin1 ﬃﬃx
p
sin1 ﬃﬃ
0
p
1
sin u ðcosN2uÞ2 sin u cos udu
¼ 2 G
Nþ1
2

G
1
2

G
N
2

Z sin1 ﬃﬃx
p
0
cosN1udu:
(3.115)
For 0 ≤a , p∕2 and k > 1, define
MkðaÞ ¼
Z a
0
coskudu:
(3.116)
Using integration by parts or integration tables, it is well known that
MkðaÞ ¼
8
>
>
<
>
>
:
a
if k ¼ 0,
sin a
if k ¼ 1,
k  1
k
Mk2ðaÞ þ sin a cosk1a
k
if k . 1:
(3.117)
The claims for N ¼ 1 and N ¼ 2 are easy to verify using the cases for k ¼ 0 and
k ¼ 1 above, respectively. For the other cases, we apply a recursion using the
equation for k > 1. If the recursion is applied i > 0 times such that
n  2i . 1, then
MnðaÞ ¼
ðn  1Þ!!
ðn  2i  1Þ!!
ðn  2iÞ!!
n!!
Mn2iðaÞ
þ
X
i
k¼1
ðn  1Þ!!
ðn  2k þ 1Þ!!
ðn  2kÞ!!
n!!
sin a cosn2kþ1a
¼ ðn  1Þ!! ðn  2iÞ!!
n!! ðn  2i  1Þ!! Mn2iðaÞ
þ ðn  1Þ!!
n!!
sin a
X
i
k¼1
ðn  2kÞ!!
ðn  2k þ 1Þ!! cosn2kþ1a:
(3.118)
In particular, for even n, repeat the recursion i ¼ n∕2 times to obtain
130
Chapter 3

MnðaÞ ¼ ðn  1Þ!! 0!!
n!! ð1Þ!! M0ðaÞ þ ðn  1Þ!!
n!!
sin a
X
n
2
k¼1
ðn  2kÞ!!
ðn  2k þ 1Þ!! cosn2kþ1a
¼ ðn  1Þ!!
n!!

a þ sin a
X
n
2
k¼1
ðn  2kÞ!!
ðn  2k þ 1Þ!! cosn2kþ1a

¼ ðn  1Þ!!
n!!

a þ sin a
X
n
2
k¼1
ð2k  2Þ!!
ð2k  1Þ!! cos2k1a

,
(3.119)
and for odd n, repeat the recursion i ¼ ðn  1Þ∕2 times to obtain
MnðaÞ ¼ ðn  1Þ!! 1!!
n!! 0!!
M1ðaÞ þ ðn  1Þ!!
n!!
sin a
X
n1
2
k¼1
ðn  2kÞ!!
ðn  2k þ 1Þ!! cosn2kþ1a
¼ ðn  1Þ!!
n!!
sin a

1 þ
X
n1
2
k¼1
ðn  2kÞ!!
ðn  2k þ 1Þ!! cosn2kþ1a

¼ ðn  1Þ!!
n!!
sin a

1 þ
X
n1
2
k¼1
ð2k  1Þ!!
ð2kÞ!!
cos2ka

,
(3.120)
where in each case we have redefined the indices of the sums in reverse order.
Returning to the original problem, for odd N > 1,
G
Nþ1
2

G
1
2

G
N
2
 ¼ 2N1N1
2

!
N1
2

!
pðN  2Þ!
¼ 2
N1
2 N1
2

!
pðN  2Þ!! ¼ ðN  1Þ!!
pðN  2Þ!!
(3.121)
and
I

x; 1
2 , N
2

¼ 2 G
Nþ1
2

G
1
2

G
N
2
 MN1ðsin1
ﬃﬃﬃx
p Þ
¼ 2 ðN  1Þ!!
pðN  2Þ!!
ðN  2Þ!!
ðN  1Þ!!


sin1
ﬃﬃﬃx
p þ
ﬃﬃﬃx
p X
N1
2
k¼1
ð2k  2Þ!!
ð2k  1Þ!! ð
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  x
p
Þ2k1

¼ 2
p sin1
ﬃﬃﬃx
p þ 2
p
ﬃﬃﬃx
p X
N1
2
k¼1
ð2k  2Þ!!
ð2k  1Þ!! ð1  xÞk1
2:
(3.122)
131
Sample-Conditioned MSE of Error Estimation

Finally, for even N . 2,
G
Nþ1
2

G
1
2

G
N
2
 ¼
N!
2NN
2

!
N2
2

! ¼ ðN  1Þ!!
2
N
2N2
2

!
¼ ðN  1Þ!!
2ðN  2Þ!!
(3.123)
and
I

x; 1
2 , N
2

¼ 2 G
Nþ1
2

G
1
2

G
N
2
 MN1ðsin1
ﬃﬃﬃx
p Þ
¼ 2 ðN  1Þ!!
2ðN  2Þ!!
ðN  2Þ!!
ðN  1Þ!!
ﬃﬃﬃx
p
"
1 þ
X
N2
2
k¼1
ð2k  1Þ!!
ð2kÞ!!

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  x
p
2k
#
¼
ﬃﬃﬃx
p þ
ﬃﬃﬃx
p X
N2
2
k¼1
ð2k  1Þ!!
ð2kÞ!!
ð1  xÞk,
(3.124)
which completes the proof.
▪
Lemma 3.5 (Dalton and Dougherty, 2012b). Let N be a positive integer,
0 < x < 1, and y ≥0. Then
R

x, y; 1
2

¼ 1
p sin1

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x þ y
1 þ y
r

 1
p tan1ð
ﬃﬃﬃy
p Þ,
(3.125)
R

x, y; N
2

¼ 1
p sin1

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
x þ y
1 þ y
r

 1
p tan1ð
ﬃﬃﬃy
p Þ

ﬃﬃﬃy
p
p
X
N1
2
i¼1
ð2i  2Þ!!
ð2i  1Þ!!

1
y þ 1
i 
1  I
yð1  xÞ
x þ y
; 1
2 , i

(3.126)
for any odd N > 1, and
R

x, y; N
2

¼ 1
p sin1ð
ﬃﬃﬃx
p Þ

ﬃﬃﬃy
p
2
X
N2
2
i¼0
ð2i  1Þ!!
ð2iÞ!!

1
y þ 1
iþ1
2 
1  I
yð1  xÞ
x þ y
; 1
2 , i þ 1
2

(3.127)
for any even N > 1. By definition, (1)!! ¼ 0!! ¼ 1. The regularized incomplete
beta function in these expressions can be found using Lemma 3.4.
132
Chapter 3

Proof. If y ¼ 0, then Rðx, 0; aÞ ¼ p1sin1ð
ﬃﬃﬃx
p Þ. This is consistent with all
equations in the statement of this lemma. Going forward we assume that y . 0.
To solve R for half-integer values, we first focus on the Appell function F 1.
Define w ¼ xðy þ 1Þ∕ðx þ yÞ and z ¼ x∕ðx þ yÞ, and note that 0 , z , w , 1.
For any real number a,
F 1

a þ 1; 1
2 , 1; a þ 2; w, z

¼ ða þ 1Þ
Z 1
0
uað1  wuÞ1
2ð1  zuÞ1du:
(3.128)
Some manipulation yields
F 1

a þ 1; 1
2 , 1; a þ 2; w, z

¼  a þ 1
z
Z 1
0
ua1ðzuÞð1  wuÞ1
2ð1  zuÞ1du
¼ a þ 1
z
Z 1
0
ua1ð1  wuÞ1
2ð1  zuÞ1du 
Z 1
0
ua1ð1  wuÞ1
2du

:
(3.129)
In the last integral, let v ¼ wu. Then
F 1

a þ 1; 1
2 , 1; a þ 2; w, z

¼ a þ 1
z
Z 1
0
ua1ð1  wuÞ1
2ð1  zuÞ1du
 a þ 1
z
wa
Z w
0
va1ð1  vÞ1
2dv:
(3.130)
The last integral is an incomplete beta function, and the first is again an
Appell function. Hence,
F 1

a þ 1; 1
2 , 1; a þ 2; w, z

¼  a þ 1
zwa B

a, 1
2

I

w; a, 1
2

þ a þ 1
az
F1

a; 1
2 , 1; a þ 1; w, z

:
(3.131)
A property of the regularized incomplete beta function is
Iðx; a, bÞ ¼ 1  Ið1  x; b, aÞ:
(3.132)
Hence,
F1

a þ 1; 1
2 , 1; a þ 2; w, z

¼ a þ 1
az
F1

a; 1
2 , 1; a þ 1; w, z

 a þ 1
zwa B

a, 1
2
 
1  I

1  w; 1
2 , a

:
(3.133)
133
Sample-Conditioned MSE of Error Estimation

By induction, for any positive integer k,
F1

a þ k; 1
2 , 1; a þ k þ 1; w, z

¼ a þ k
azk F 1

a; 1
2 , 1; a þ 1; w, z

 a þ k
wazk
X
k1
i¼0

z
w
iB

a þ i, 1
2
 
1  I

1  w; 1
2 , a þ i

:
(3.134)
We apply this to the definition of R before the statement of Lemma 3.2 to
decompose R into one of two Appell functions with known solutions. In
particular,
R

x, y; 1
2

¼
ﬃﬃﬃy
p z
2p F 1

1; 1
2 , 1; 2; w, z

,
(3.135)
R

x, y; N
2

¼
ﬃﬃﬃy
p z
2p F1

1; 1
2 , 1; 2; w, z


ﬃﬃﬃy
p z
2pw
X
N3
2
i¼0

z
w
iB

i þ 1, 1
2
 
1  I

1  w; 1
2 , i þ 1

(3.136)
for N > 1 odd, and
R

x, y; N
2

¼
ﬃﬃﬃﬃﬃ
yz
p
p F1
1
2 ; 1
2 , 1; 3
2 ; w, z


ﬃﬃﬃﬃﬃ
yz
p
2p
ﬃﬃﬃﬃw
p
X
N2
2
i¼0
z
w
i
B

i þ 1
2 , 1
2
 
1  I

1  w; 1
2 , i þ 1
2

(3.137)
for
N > 1
even.
We
can
write
Bði, 1∕2Þ ¼ ð2iÞ!!∕½ið2i  1Þ!!
and
Bði þ 1∕2, 1∕2Þ ¼ pð2i  1Þ!!∕ð2iÞ!! for integers i ≥1. Finally, to evaluate
R, it can be shown that
F1

1; 1
2 , 1; 2; w, z

¼
2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
zðw  zÞ
p
"
tan1

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
z
w  z
r

 tan1

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
zð1  wÞ
w  z
r
#
,
(3.138)
134
Chapter 3

and
F 1
1
2 ; 1
2 , 1; 3
2 ; w, z

¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
w  z
p
tan1

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
w  z
1  w
r

:
(3.139)
Further simplification gives the result in the statement of the lemma.
▪
3.5 Average Performance in the Gaussian Model
In this section we examine performance in Gaussian models under proper
priors with fixed sample size, illustrating that different samples condition
RMS performance to different extents and that models using more
informative priors have better RMS performance.
Consider an independent general covariance Gaussian model with known
and fixed c ¼ 0.5. Let n0 ¼ 6D, n1 ¼ 3D, m0 ¼ 0D, m1 ¼ 0.1719⋅1D, ky ¼ 3D,
and Sy ¼ 0.03ðky  D  1ÞID. This is a proper prior, where m1 has been
calibrated to give an expected true error of 0.25 with D ¼ 1.
Following the procedure in Fig. 2.5, in step 1, m0, S0, m1, and S1 are
generated according to the specified priors. This is done by generating a
random covariance according to the inverse-Wishart distribution pðSyÞ using
methods in (Johnson, 1987). Conditioned on the covariance, we generate a
random mean from the Gaussian distribution pðmyjSyÞ  N ðmy, Sy∕nyÞ,
resulting in a normal-inverse-Wishart distributed mean and covariance pair.
The parameters for class 0 are generated independently from those of class 1.
In step 2A we generate a random sample of size n, in step 2B the prior is
updated to a posterior, and in step 2C we train an LDA classifier. In step 2D,
the true error of the classifier is computed exactly, the training data are used
to evaluate the 5-fold cross-validation error estimator, a Bayesian MMSE
error estimator is found exactly using the posterior, and the theoretical
sample-conditioned RMS for the Bayesian MMSE error estimator is
computed exactly. The sampling procedure is repeated t ¼ 1000 times for
each fixed feature-label distribution, with T ¼ 10,000 feature-label distribu-
tions, for a total of 10,000,000 samples.
Table 3.1 shows the accuracy of the analytical formulas for conditional
RMS using n ¼ 60 with different feature sizes ðD ¼ 1, 2, and 5Þ. There
Table 3.1
Average true error, semi-analytical RMS, and absolute difference between the
semi-analytical and empirical RMS for the Bayesian MMSE error estimator.
Simulation settings
Average
true error
Semi-analytical
RMS
Absolute difference
between RMSs
n ¼ 60, D ¼ 1
0.2474
0.0377
5.208  10–6
n ¼ 60, D ¼ 2
0.1999
0.0358
3.110  10–5
n ¼ 60, D ¼ 5
0.1156
0.0262
8.971  10–5
135
Sample-Conditioned MSE of Error Estimation

is close agreement between the semi-analytical RMS and empirical RMS
(both defined in Section 3.3) of the Bayesian MMSE error estimator. The
table also provides the average true errors for each model.
Figure 3.2 shows the estimated densities of the conditional RMS, found
from the conditional RMS values recorded in each iteration of the experiment,
for both the cross-validation and Bayesian MMSE error estimators. The
conditional RMS for cross-validation is computed from Eq. 3.12. Results for
D ¼ 5 features and n ¼ 60 sample points are shown. The semi-analytical
unconditional RMS for each error estimator is also printed in each graph for
reference. The high variance of these distributions illustrates that different
samples condition the RMS to different extents. Meanwhile, the conditional
RMS for cross-validation has a much higher variance and is shifted to the
right, which is expected since the conditional RMS of the Bayesian MMSE
error estimator is optimal.
3.6 Convergence of the Sample-Conditioned MSE
Section 2.7 discussed consistency of the Bayesian MMSE error estimator,
defining weak, rth mean, and strong consistency. Throughout this section, we
denote the true error by εnðu, SnÞ rather than εnðu, cnÞ to emphasize
dependency on the sample. For the sample-conditioned MSE we are interested
in showing that for all u ∈U, MSEðˆεnðSn, cnÞjSnÞ →0 (almost surely), i.e.,
PrS`juðEujSn½ðˆεnðSn, cnÞ  εnðu, SnÞÞ2 →0Þ ¼ 1:
(3.140)
We refer to this property as conditional MSE convergence.
Since ˆεnðSn, cnÞ ¼ EujSn½εnðu, SnÞ, for conditional MSE convergence it
must be shown that
0
0.02
0.04
0.06
0.08
0.1
0
20
40
60
80
100
RMS conditioned on the sample
PDF
cv: RMS = 0.0425
BEE: RMS = 0.0262
cv
BEE
Figure 3.2
Probability densities for the conditional RMS of the cross-validation and
Bayesian MMSE error estimators with correct priors. The unconditional RMS for both error
estimators is also indicated ðD ¼ 5, n ¼ 60Þ. [Reprinted from (Dalton and Dougherty, 2012c).]
136
Chapter 3

PrS`juðEujSn½ðEujSn½εnðu, SnÞ  εnðu, SnÞÞ2 →0Þ
¼ PrS`juðEujSn½ðEujSn½ f nðu, SnÞ  f nðu, SnÞÞ2 →0Þ
¼ PrS`juðEujSn½ f 2nðu, SnÞ  ðEujSn½ f nðu, SnÞÞ2 →0Þ
¼ 1,
(3.141)
where f nðu, SnÞ ¼ εnðu, SnÞ  εnðu, SnÞ. Hence, both strong consistency and
conditional MSE convergence are proved if, for any true parameter u and
both k ¼ 1 and k ¼ 2,
PrS`ju

EujSn
h
j f nðu, SnÞjki
→0

¼ 1:
(3.142)
Recall that Theorem 2.13 not only addresses strong consistency, but also
addresses the stronger requirement in Eq. 3.142. In particular, Theorems 2.13
and 2.14 not only prove that the Bayesian MMSE error estimator is strongly
consistent as long as the true error functions εnðu, SnÞ form equicontinuous sets
for fixed samples and the posteriors are weak consistent with an appropriate
sampling methodology, but they also prove conditional MSE convergence
under the same constraints. Indeed, the conclusion of Theorem 2.13 can be
amended to include conditional MSE convergence by letting k ¼ 2.
3.7 A Performance Bound for the Discrete Model
In the previous section we have seen that MSEðˆεnjSnÞ →0 as n →` (almost
surely relative to the sampling process) for the discrete model. In fact, it is
possible to derive an upper bound on MSEðˆεnjSnÞ as a function of only the
sample size in some cases. In the discrete model, the conditional MSE is given
by Eq. 3.26 and can be written as
MSEðˆε y
n jSnÞ ¼
ˆε y
n ð1  ˆε y
n Þ
ny þ Pb
i¼1 ay
i þ 1 :
(3.143)
Plugging this expression for both y ¼ 0 and y ¼ 1 into Eq. 3.3 and assuming a
beta posterior model for c with hyperparameters ay yields
MSEðˆεnjSnÞ ¼
a
0a
1
ða
0 þ a
1Þ2ða
0 þ a
1 þ 1Þ ðˆε 0n  ˆε 1n Þ2
þ
a
0ða
0 þ 1Þ
ða
0 þ a
1Þða
0 þ a
1 þ 1Þ ⋅
ˆε 0n ð1  ˆε 0n Þ
n0 þ Pb
i¼1 a0
i þ 1
þ
a
1ða
1 þ 1Þ
ða
0 þ a
1Þða
0 þ a
1 þ 1Þ ⋅
ˆε 1n ð1  ˆε 1n Þ
n1 þ Pb
i¼1 a1
i þ 1 :
(3.144)
137
Sample-Conditioned MSE of Error Estimation

From this, it is clear that MSEðˆεnjSnÞ →0 (and these results apply for any
classification
rule).
In
particular,
as
long
as
a
0 ≤n0 þ Pb
i¼1 a0
i
and
a
1 ≤n1 þ Pb
i¼1 a1
i , which is often the case, then
ða
0 þ a
1 þ 1Þ MSEðˆεnjSnÞ
≤
a
0a
1
ða
0 þ a
1Þ2 ðˆε 0n  ˆε 1n Þ2 þ
a
0
a
0 þ a
1
ˆε 0n ð1  ˆε 0n Þ þ
a
1
a
0 þ a
1
ˆε 1n ð1  ˆε 1n Þ
¼ Ep½cEp½1  cðˆε 0n  ˆε 1n Þ2
þ Ep½cˆε 0n ð1  ˆε 0n Þ þ Ep½1  cˆε 1n ð1  ˆε 1n Þ,
(3.145)
where we have used Ep½c ¼ a
0∕ða
0 þ a
1Þ and Ep½1  c ¼ a
1∕ða
0 þ a
1Þ. To
help simplify this equation further, define x ¼ ˆε 0n , y ¼ ˆε 1n , and z ¼ Ep½c.
Then
MSEðˆεn j SnÞ ≤zð1  zÞðx  yÞ2 þ zxð1  xÞ þ ð1  zÞyð1  yÞ
a
0 þ a
1 þ 1
¼ zx þ ð1  zÞy  ½zx þ ð1  zÞy2
a
0 þ a
1 þ 1
.
(3.146)
From Eq. 2.13 note that ˆεn ¼ zx þ ð1  zÞy, and also note that 0 ≤ˆεn ≤1.
Hence,
MSEðˆεn j SnÞ ≤ˆεn  ðˆεnÞ2
a
0 þ a
1 þ 1 ≤
1
4ða
0 þ a
1 þ 1Þ ,
(3.147)
where the last inequality utilizes the fact that w  w2 ¼ wð1  wÞ ≤1∕4
whenever 0 ≤w ≤1. Thus, the conditional RMS of the Bayesian MMSE error
estimator for any discrete classifier, averaged over all feature-label distribu-
tions with beta posteriors on c such that ay ≥ny (which holds under proper
beta priors and random sampling) and Dirichlet priors on the bin probabilities
such that ay ≤ny þ Pb
i¼1 ay
i , satisfies
RMSðˆεn j SnÞ ≤
ﬃﬃﬃﬃﬃ
1
4n
r
.
(3.148)
Since this bound is only a function of the sample size, it holds if we remove the
conditioning on Sn.
For comparison, we consider a remarkably similar holdout bound. If the
data are split between training and test data, where the classifier is designed on
the training data and the classifier error is estimated on the test data, then we
have the distribution-free bound
138
Chapter 3

RMSðˆεholdout j Snm, c, u0, u1Þ ≤
ﬃﬃﬃﬃﬃﬃﬃ
1
4m
r
,
(3.149)
where m is the size of the test sample, and Snm is the training sample
(Devroye et al., 1996). Note that uncertainty here stems from the sampling
distribution of the test sample. In any case, the bound is still true upon
removing the conditioning. The RMS bound on the Bayesian MMSE
error estimator is always lower than that of the holdout estimate, which
is a testament to the power of modeling assumptions. Moreover, as m →n
for full holdout, the holdout bound converges down to the Bayes estimate
bound.
Example 3.2. We next compare the performance of Bayesian MMSE error
estimation versus holdout error estimation, inspired by the similarity between
the performance bounds given in Eqs. 3.148 and 3.149. We use the simulation
methodology outlined in Fig. 2.5 with a discrete model and fixed bin size b.
Let the prior for c be uniformð0, 1Þ ða0 ¼ a1 ¼ 1Þ and let the bin probabilities
of classes 0 and 1 have Dirichlet priors given by the hyperparameters
a0
i ∝2b  2i þ 1 and a1
i ∝2i  1, respectively, where the ay
i are normalized
such that Pb
i¼1 ay
i ¼ b for y ∈f0, 1g. These priors satisfy ay ≤Pb
i¼1 ay
i ;
therefore, with random sampling ay ≤ny þ Pb
i¼1 ay
i .
In step 1, generate a random c from the uniform distribution and generate
random
bin
probabilities
from
the
Dirichlet
priors
using
the
same
methodology outlined in Example 3.1. In step 2A generate a random sample
with fixed sample size n, and in step 2B update the prior to a posterior.
For a fair comparison between the Bayesian MMSE error estimator,
which is a full-sample error estimator, and the holdout estimator, which
partitions the sample into training and testing datasets, we will consider
separate experiments, one training a classifier with the full sample and
evaluating the Bayesian MMSE error estimate, the other training a classifier
on the sample data without holdout points and evaluating the holdout error
estimate on the held-out points.
In step 2C, we design a discrete histogram classifier using the full-training
sample, breaking ties toward class 0. In step 2D, the true error is found exactly
and the Bayesian MMSE error estimator for this classifier is found from the
classifier and posterior by evaluating Eq. 2.13 with Ep½c ¼ ðn0 þ 1Þ∕ðn þ 2Þ
and ˆε y
n defined in Eq. 2.56. The sample-conditioned RMS of the Bayesian
MMSE error estimator is computed from Eqs. 3.3 and 3.23.
In step 2C, for each m ¼ 1, 2, . . . , n the original sample is partitioned into
n  m training points and m holdout points, where the proportion of points
from each class in the holdout set is kept as close as possible to that of the
original sample. Each training set is used to find a discrete histogram
139
Sample-Conditioned MSE of Error Estimation

classifier. In step 2D, the true error is found exactly and the holdout estimate
is evaluated as the proportion of classification errors on the holdout subset.
The sampling procedure is repeated t ¼ 10,000 times for each fixed
feature-label distribution, and T ¼ 10,000 feature-label distributions are
generated (corresponding to randomly selected parameters), for a total of
100,000,000 samples. Results are shown in Fig. 3.3 for b ¼ 8 with n ¼ 16. This
setting is also used in Fig. 3.1. The results are typical, where part (a) shows the
expected true error, and part (b) shows the RMS between the true and
estimated errors, both as functions of the holdout sample size m. As expected,
the average true error of the classifier in the holdout experiment decreases and
converges to the average true error of the classifier trained from the full
sample as the holdout sample size decreases. In addition, the RMS
performance of the Bayesian MMSE error estimator consistently surpasses
that of the holdout error estimator, as suggested by the RMS bounds given in
Eqs. 3.148 and 3.149. Thus, under a Bayesian model, not only does using the
full sample to train the classifier result in a lower true error, but we can
achieve better RMS performance using training-data error estimation than by
holding out the entire sample for error estimation.
3.8 Censored Sampling
As discussed in Section 1.5, it is necessary to bound the RMS (or some other
criterion of estimation accuracy) of the error estimator in order for the
classifier model, classifier, and error estimate to be epistemologically
meaningful. In that section we considered a situation in which the
unconditioned RMS for every feature-label distribution in an uncertainty
class is known and therefore one can bound the unconditioned RMS over the
2
4
6
8
10
12
14
16
0.1
0.2
0.3
0.4
0.5
holdout sample size
(a)
(b)
average true error
BEE
holdout
2
4
6
8
10
12
14
16
0
0.1
0.2
0.3
0.4
holdout sample size
RMS deviation from true error
BEE
holdout
holdout upper bound
Figure 3.3
Comparison of the holdout error estimator and Bayesian MMSE error estimator
with correct priors with respect to the holdout sample size for a discrete model with b ¼ 8
bins and fixed sample size n ¼ 16: (a) average true error; (b) RMS performance. [Reprinted
from (Dalton and Dougherty, 2012c).]
140
Chapter 3

uncertainty class. In particular, one can compute a sample size to guarantee a
desired degree of error estimation accuracy. This applies to the common
setting in which the sample size is determined prior to sampling. With
censored sampling, the size of the sample is determined adaptively based on
some criterion determining when a sufficient number of sample points has
been observed. For example, suppose that one desires a sample-conditioned
MSE of at most r. Following the selection of a batch of sample points, the
sample-conditioned MSE can be computed. If MSEðˆεn j SnÞ . r, then another
batch (perhaps only one point) can be randomly drawn and adjoined to Sn;
otherwise, the sampling procedure ends. Since the conditional MSE converges
almost surely to 0 with increasing sample size, any desired threshold for the
conditional MSE will almost surely eventually be achieved. When sample
points are expensive, difficult, or time-consuming to obtain, censored
sampling allows one to stop sampling once a desired degree of accuracy is
achieved.
3.8.1 Gaussian model
When applying the conditional RMS to censored sampling with synthetic data
from the general covariance Gaussian model, step 1 shown in Fig. 2.5 remains
the same; that is, we still define a fixed set of hyperparameters and use these
priors to generate random feature-label distributions. However, the sampling
procedure in step 2 is modified, as shown in Fig. 3.4. Instead of fixing the
sample size ahead of time, sample points are collected in batches until a
stopping criterion is satisfied.
An example of censored sampling is provided in Fig. 3.5 for an
independent general covariance Gaussian model with D ¼ 2 features, c ¼ 0.5
fixed and known, and the following hyperparameters for the priors of
u0 and u1: n0 ¼ 36, n1 ¼ 18, m0 ¼ 02, m1 ¼ 0.2281 ⋅12, ky ¼ 18, and
Sy ¼ 0.03ðky  3ÞI2. We implement censored sampling with LDA. Under
these hyperparameters, the average Bayes error is 0.158.
In step A of each iteration, we draw a small initial training sample from
the feature-label distribution. In our implementation the training sample is
initialized with two sample points in each class, for a total of four sample
points. In step B we update hyperparameters, and in step C we train an LDA
Figure 3.4
Simulation methodology for censored sampling under a fixed feature-label
distribution or a large dataset representing a population.
141
Sample-Conditioned MSE of Error Estimation

classifier using the initial training sample with no feature selection. Step D
checks the current Bayesian MMSE error estimate as well as the conditional
MSE for the initial training sample and designed classifier. For LDA, these
may be found in closed form. If the stopping criteria, which we will outline in
detail shortly, are not satisfied, then a random sample of four points is
augmented to the current training set in step E. To do this, we first establish
the labels of the new sample points from an independent binomialð4, cÞ
experiment and then draw the sample points from the corresponding class-
conditional distributions. Hyperparameters are updated again (returning to
step B), a new LDA classifier is designed (step C), and the Bayesian MMSE
error estimate and conditional MSE are checked again (step D). This is
repeated until the stopping criteria are satisfied, or a maximum sample size of
160 is reached, in which case the sampling procedure halts. Step F collects
several outputs: the final censored sample size, the true error of the final
classifier (from the true distribution parameters), a 5-fold cross-validation
error estimate, the Bayesian MMSE error estimate, and the conditional MSE
of the Bayesian MMSE error estimate (the latter two already having been
found when checking the stopping criterion). All error estimates and the
conditional MSE are found relative to the final censored sample and final
classifier. This procedure is repeated t ¼ 1000 times for each fixed feature-
label distribution, for T ¼ 1000 random feature-label distributions, for a total
of tT ¼ 1,000,000 censored samples.
It
remains
to
specify
the
stopping
criteria.
We
desire
RMSðˆεn j SnÞ ≤0.0295, which is the semi-analytical RMS (root of the average
conditional MSE) reported for a fixed-sample-size experiment with LDA and
n ¼ 60. Our interest is primarily in the overall conditional MSE, but we also
implement five additional stopping conditions that must all be satisfied:
RMSðˆε 0n j SnÞ ≤0.0295  2, RMSðˆε 1n j SnÞ ≤0.0295  2, ˆεn ≤0.3, ˆε 0n ≤0.35,
0
0.01
0.02
0.03
0.04
0.05
0
500
1000
1500
2000
2500
3000
sample-conditioned RMS
(a)
(b)
PDF
0
50
100
150
0
200
400
600
800
1000
censored sample size
PDF
Figure 3.5
Classification performance under fixed sample size versus censored sampling
(D ¼ 2, c ¼ 0.5): (a) density of conditional RMS under fixed sample size, LDA, n ¼ 60;
(b) density of censored sample size, LDA.
142
Chapter 3

and ˆε 1n ≤0.35. The latter three conditions help avoid early stopping. The
average Bayes error for this model is well below 0.3, so in most cases the
conditional RMS determines the censored sample size.
The sample size is different in each trial because the conditional MSE
depends on the actual data obtained from sampling. Consistency guarantees
that MSEðˆεnjSnÞ will eventually reach the stopping criterion; thus, relative to
the conditional MSE, censored sampling may work to any degree desired.
Figure 3.5(a) shows a histogram of the conditional MSE for a fixed-sample-
size experiment with LDA and n ¼ 60. Here, the average true error is 0.170
and the root of the average MSE is 0.0295. For comparison, a histogram of
the sample size obtained with LDA and censored sampling experiments is
shown in Fig. 3.5(b). In this experiment, the average true error is 0.169, the
root of the average MSE is 0.029, and the average sample size is 62.1.
A key point is that the distribution in Fig. 3.5(a) has a wide support,
illustrating that the sample significantly conditions the RMS. In all cases, the
sample-conditioned RMS with censored sampling is close to the uncondi-
tioned RMS with fixed sampling, which is expected due to the stopping
criteria in the censored sampling process. The difference is that, while the
fixed-sample experiments have certainty in sample size with uncertainty in
the conditional RMS, censored sampling experiments have certainty in the
conditional RMS with uncertainty in the sample size. Hence, censored
sampling provides approximately the same RMS and average sample size,
while also guaranteeing a specified conditional RMS for each final sample in
the censored sampling process.
One needs to keep in mind that, when using a smaller sample size to
obtain a desired RMS, the true error of the classifier may increase. This
potential problem is alleviated by requiring that the Bayesian MMSE error
estimate itself reach a desired threshold before halting the sampling
procedure. In effect, one then places goodness criteria on both the error
estimation and classifier design to determine sample size.
3.9 Asymptotic Approximation of the RMS
The sample-conditioned MSE, which provides a measure of performance
across the uncertainty class U for a given sample Sn, involves various sample-
conditioned moments for the error estimator: Eu½ ˆεn j Sn, Eu½ðˆεnÞ2 j Sn, and
Eu½εnˆεn j Sn. These simplify to ˆεn, ðˆεnÞ2, and Eu½εn j Snˆεn ¼ ðˆεnÞ2, respectively.
One could also consider the MSE relative to a fixed feature-label distribution
in the uncertainty class and randomness relative to the sampling distribution.
This
would
yield
the
feature-label-distribution-conditioned
MSE
MSESnðˆεn j uÞ and corresponding moments: ESn½ ˆεn j u, ESn½ðˆεnÞ2 j u, and
ESn½εnˆεn j u. These moments help characterize estimator behavior relative to a
given feature-label distribution and are the kind of moments considered
143
Sample-Conditioned MSE of Error Estimation

historically. They facilitate performance comparison of the Bayesian MMSE
error estimator to classical error estimators. To evaluate performance across
both the uncertainty class and the sampling distribution requires the
unconditioned MSE MSEu,SnðˆεnÞ and corresponding moments: Eu,Sn½ ˆεn,
Eu,Sn½ðˆεnÞ2, and Eu,Sn½εnˆεn. Until this point, we have examined MSESnðˆεn j uÞ
and MSEu,SnðˆεnÞ via simulation studies in the discrete and Gaussian models.
In this section, following (Zollanvari and Dougherty, 2014), we provide
asymptotically exact approximations of these, along with the corresponding
moments, of the Bayesian MMSE error estimator for LDA in the Gaussian
model. These approximations are exact in a double-asymptotic sense, where
both sample size n and dimensionality D approach infinity at a fixed rate
between the two (Zollanvari et al., 2011). Finite-sample approximations from
the double-asymptotic method have been shown to be quite accurate
(Zollanvari et al., 2011; Wyman et al., 1990; Pikelis, 1976). There is a body
of work on the use of double asymptotics for the analysis of LDA and its
related statistics (Zollanvari et al., 2011; Raudys, 1972; Deev, 1970; Fujikoshi,
2000; Serdobolskii, 2000; Bickel and Levina, 2004). Raudys and Young provide
a good review of the literature on the subject (Raudys and Young, 2004).
Although the theoretical underpinning of both (Zollanvari et al., 2011)
and the present section relies on double-asymptotic expansions, in which
n, D →` at a proportional rate, practical interest concerns finite-sample
approximations corresponding to the asymptotic expansions. In (Wyman
et al., 1990), the accuracy of such finite-sample approximations is investigated
relative to the expected error of LDA in a Gaussian model. Several single-
asymptotic expansions ðn →`Þ are considered, along with double-asymptotic
expansions ðn, D →`Þ (Raudys, 1972; Deev, 1970). The results of (Wyman
et al., 1990) show that the double-asymptotic approximations are significantly
more accurate than the single-asymptotic approximations. In particular, even
with n∕D , 3, the double-asymptotic expansions yield “excellent approxima-
tions,” while the others “falter.”
The analysis of this section pertains to separate sampling under a Gaussian
model with a known common covariance matrix S. We classify with a variant of
LDA given by cnðxÞ ¼ 0 if WðxÞ þ w ≤0 and cnðxÞ ¼ 1 otherwise, where
WðxÞ is the discriminant given in Eq. 1.29 with ˆS ¼ S, w ¼ lnðc1∕c0Þ, and ci is
the true prior class-i probability. This classifier assumes that the ci and S are
known and assigns the decision boundary to class 1 instead of class 0. For
known covariance S and a Gaussian prior distribution on mi possessing mean
mi and covariance matrix S∕ni, i ∈f0, 1g, the Bayesian MMSE error estimator
is given by Eq. 2.13, with the true c0 in place of Ep½c and ˆε in given by
Theorem 2.10. Letting m ¼ ½mT
0 , mT
1 T, our interest is with the moments
ESn½ ˆεn j m, ESn½ðˆεnÞ2 j m, and ESn½εnˆεn j m used to obtain MSESnðˆεn j mÞ, and
the moments Em,Sn½ ˆεn, Em,Sn½ðˆεnÞ2, and Em,Sn½εnˆεn used to obtain MSEm,SnðˆεnÞ.
144
Chapter 3

3.9.1 Bayesian–Kolmogorov asymptotic conditions
The Raudys–Kolmogorov asymptotic conditions (Zollanvari et al., 2011) are
defined on a sequence of Gaussian discrimination problems with a sequence of
parameters and sample sizes:

ðmD,0, mD,1, SD, nD,0, nD,1Þ
`
D¼1, where the
means and the covariance matrix are arbitrary. The assumptions for Raudys–
Kolmogorov asymptotics are nD,0 →`, nD,1 →`, D →`, D∕nD,0 →l0,
D∕nD,1 →l1, and 0 , l0, l1 , `. For notational simplicity, we denote the
limit under these conditions by limRKac. For analysis related to LDA, we also
assume that the Mahalanobis distance,
dm,D ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðmD,0  mD,1ÞTS1
D ðmD,0  mD,1Þ
q
,
(3.150)
is finite, and limRKac dm,D ¼ dm (Serdobolskii, 2000, p. 4). This condition
assures the existence of limits of performance metrics of the relevant statistics
(Zollanvari et al., 2011; Serdobolskii, 2000).
To analyze the Bayesian MMSE error estimator, we modify the sequence
of Gaussian discrimination problems to
n
ðmD,0, mD,1, SD, nD,0, nD,1, mD,0, mD,1, nD,0, nD,1Þ
o`
D¼1,
(3.151)
where mD,i and nD,i are hyperparameters of the fixed covariance Gaussian
Bayesian model. In addition to the previous conditions, we assume that the
following limits exist for i, j ∈f0, 1g:
lim
RKacmT
D,iS1
D mD,j ¼ mT
i S1mj,
(3.152)
lim
RKacmT
D,iS1
D mD,j ¼ mT
i S1mj,
(3.153)
lim
RKac mT
D,iS1
D mD,j ¼ mT
i S1mj,
(3.154)
where mT
i S1mj, mT
i S1mj, and mT
i S1mj are symbolic representations of the
constants to which the limits converge. In (Serdobolskii, 2000), fairly mild
sufficient conditions are given for the existence of these limits.
We refer to all of the aforementioned conditions, along with nD,i →`,
nD,i∕nD,i →gi, and 0 , gi , `, as the Bayesian–Kolmogorov asymptotic
conditions (BKac). We denote the limit under these conditions by limBKac
or →
BKac, which means that, for i, j ∈f0, 1g,
145
Sample-Conditioned MSE of Error Estimation

lim
BKacð⋅Þ ¼
lim
D →`, nD,0 →`, nD,1 →`, nD,0 →`, nD,1 →`
D
nD,0 →l0,
D
nD,1 →l1, nD,0
nD,0 →g0, nD,1
nD,1 →g1
0 , l0 , `, 0 , l1 , `, 0 , g0 , `, 0 , g1 , `
mT
D,iS1
D mD,j →mT
i S1mj, 0 , mT
i S1mj , `
mT
D,iS1
D mD,j →mT
i S1mj, 0 , mT
i S1mj , `
mT
D,iS1
D mD,j →mT
i S1mj, 0 , mT
i S1mj , `
ð⋅Þ.
(3.155)
mT
D,iS1
D mD,j →mT
i S1mj implies that mT
D,iS1
D mD,j ¼ Oð1Þ as D →`. Simi-
larly, mT
D,iS1
D mD,j ¼ Oð1Þ and mT
D,iS1
D mD,j ¼ Oð1Þ. This limit is defined for
the case where there is conditioning on a specific value of mD,i. In this case
mD,i is not a random vector; rather, for each D, it is a vector of constants. To
remove conditioning, we model mD,i as Gaussian with mean mD,i and
covariance SD∕nD,i; i.e., the means are given the same prior as the Bayesian
MMSE error estimator. The sequence of discrimination problems and the
above limit reduce to
n
ðSD, nD,0, nD,1, mD,0, mD,1, nD,0, nD,1Þ
o`
D¼1
(3.156)
and
lim
BKacð⋅Þ ¼
lim
D →`, nD,i →`, nD,i →`
D
nD,i →li, nD,i
nD,i →gi, 0 , li , `, 0 , gi , `
mT
D,iS1
D mD,j →mT
i S1mj, 0 , mT
i S1mj , `
ð⋅Þ,
(3.157)
respectively. For notational simplicity we assume clarity from the context and
do not explicitly differentiate between these conditions. Convergence in
probability
under
the
Bayesian–Kolmogorov
asymptotic
conditions
is
denoted by plimBKac. For notational ease, we henceforth omit the subscript
“D” from the parameters.
Define
ha1, a2, a3, a4 ¼ ða1  a2ÞTS1ða3  a4Þ,
(3.158)
and for ease of notation write ha1;a2,a1;a2 as ha1;a2. There are two special cases:
(1) the square of the Mahalanobis distance in the parameter space of unknown
class-conditional densities, d2m ¼ hm0,m1 . 0, and (2) a distance measure for
prior distributions, D2m ¼ hm0,m1 . 0, where m ¼ ½mT
0 , mT
1 T. The conditions
in Eq. 3.155 assure the existence of limBKac ha1;a2;a3;a4, where the ajs can
be any combination of m0, m1, m0, and m1. ha1;a2;a3;a4, d2
m, and D2
m denote
limBKacha1;a2;a3;a4, limBKac d2m, and limBKac D2m, respectively.
146
Chapter 3

The ratio D∕ni is an indicator of complexity for LDA (and any linear
classification
rule),
the
Vapnik–Chervonenkis
dimension
being
D þ 1.
Therefore, the conditions in Eq. 3.155 characterize the asymptotic complexity
of the problem. The ratio ni∕ni is a measure of relative uncertainty: the smaller
ni∕ni is, the more we rely on the data and the less we rely on the prior
knowledge. Hence, the conditions in Eq. 3.155 characterize the asymptotic
uncertainty. We let bi ¼ ni∕ni so that bi ¼ ni∕ni →gi. We also denote the
sample mean for class i ∈f0, 1g by xi.
3.9.2 Conditional expectation
In this section, based on (Zollanvari and Dougherty, 2014), we use the
Bayesian–Kolmogorov asymptotic conditions to characterize the conditional
and unconditional first moments of the Bayesian MMSE error estimator. In
each case, the BKac limit suggests a finite-sample approximation. These are
tailored
after
the
so-called
Raudys-type
Gaussian-based
finite-sample
approximation for the expected LDA classification error (Raudys, 1972;
Raudys and Young, 2004):
ESn½ε0n ¼ PrðWðx0, x1, xÞ ≤wjx ∈Π0Þ
≂F
0
@ESn,x½Wðx0, x1, xÞjx ∈Π0 þ w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
varSn;xðWðx0, x1, xÞjx ∈Π0Þ
p
1
A,
(3.159)
with Π0 denoting the class-0 population and w being the discriminant
threshold. Here we have written the Anderson W statistic explicitly as a
function of the test point and the sample.
The asymptotic conditional expectation of the Bayesian MMSE error
estimator is characterized in the following theorem, where GB
0 , GB
1 , and L
depend on m.
Theorem 3.11 (Zollanvari and Dougherty, 2014). Consider the sequence of
Gaussian discrimination problems defined by Eq. 3.151. Then for i ∈f0, 1g,
lim
BKacESn½ ˆε injm ¼ F

ð1Þi GB
i þ w
ﬃﬃﬃﬃ
L
p

,
(3.160)
and therefore,
lim
BKacESn½ ˆεnjm ¼ c0F
GB
0 þ w
ﬃﬃﬃﬃ
L
p

þ c1F
GB
1  w
ﬃﬃﬃﬃ
L
p

,
(3.161)
147
Sample-Conditioned MSE of Error Estimation

where
GB
0 ¼
1
2ð1 þ g0Þ
h
g0ðhm0,m1  hm0;m0Þ þ d2
m þ ð1  g0Þl0 þ ð1 þ g0Þl1
i
,
(3.162)
GB
1 ¼
1
2ð1 þ g1Þ
h
g1ðhm1;m0  hm1;m1Þ þ d2
m þ ð1  g1Þl1 þ ð1 þ g1Þl0
i
,
(3.163)
L ¼ d2
m þ l0 þ l1.
(3.164)
Proof. For i ∈{0, 1}, let
bGB
0 ¼

m
0  x0 þ x1
2
T
S1ðx0  x1Þ,
(3.165)
where m
0 is defined in Eq. 3.66. Then
bGB
0 ¼
n0
n0 þ n0
mT
0 S1ðx0  x1Þ
þ
n0  n0
2ðn0 þ n0Þ

xT
0 S1x0  xT
0 S1x1

þ 1
2

xT
1 S1x1  xT
0 S1x1

.
(3.166)
For i, j ∈f0, 1g, and i ≠j, define the following random variables:
yi ¼ mT
i S1ðx0  x1Þ,
(3.167)
zi ¼ xT
i S1xi,
(3.168)
zij ¼ xT
i S1xj.
(3.169)
Given m, the sample mean xi for class i is Gaussian with mean mi and
covariance S/ni. The variance of yi given m does not depend on m. Therefore,
under the Bayesian–Kolmogorov conditions stated in Eq. 3.155, mT
i S1mj
and mT
i S1mj do not appear in the limit. Only mT
i S1mi matters, which
vanishes in the limit as follows:
varSnðyijmÞ ¼ mT
i
S1
n0
þ S1
n1

mi
→
BKac lim
n0→`
mT
i S1mi
n0
þ lim
n1→`
mT
i S1mi
n1
¼ 0.
(3.170)
148
Chapter 3

To find the variance of zi and zij, we first transform zi and zij to quadratic
forms and then use the results of (Kan, 2008) to find the variance of quadratic
functions of Gaussian random vectors. Specifically, from (Kan, 2008), for
y  N ðm, SÞ and any symmetric positive definite matrix A,
varðyTAyÞ ¼ 2 trðASASÞ þ 4mTASAm.
(3.171)
Some algebraic manipulation yields
varSnðzijmÞ ¼ 2 D
n2
i
þ 4 mT
i S1mi
ni
→
BKac 2lim
ni→`
li
ni
þ 4lim
ni→`
mT
i S1mi
ni
¼ 0,
(3.172)
varSnðzijjm Þ ¼ D
ninj
þ mT
i S1mi
nj
þ
mT
j S1mj
ni
→
BKac lim
nj→`
li
nj
þ lim
nj→`
mT
i S1mi
nj
þ lim
ni→`
mT
j S1mj
ni
¼ 0.
(3.173)
From
the
Cauchy–Schwarz
inequality,
covSnðyi, zkjmÞ →
BKac 0,
covSnðyi, zijjmÞ →
BKac 0, and covSnðzi, zijjmÞ →
BKac 0 for i, j, k ∈f0, 1g with i ≠j.
Furthermore,
ni  ni
2ðni þ niÞ →
BKac
1  gi
2ð1 þ giÞ ,
(3.174)
ni
ni þ ni
→
BKac
gi
1 þ gi
.
(3.175)
With this and using the same approach for bGB
1 (plug m
1 in Eq. 3.165) yields
varSnðbGB
i jm Þ →
BKac 0. Since varðX nÞ →0 implies that X n →limn→` E½X n in
probability whenever limn→`E½X n exists, for i, j ∈{0, 1}, and i ≠j,
149
Sample-Conditioned MSE of Error Estimation

plim
BKac
bGB
i jm ¼ lim
BKacESn½bGB
i jm 
¼ ð1Þi 1
2

mT
j S1mj þ lj

þ ð1Þi
gi
1 þ gi

mT
i S1mi  mT
i S1mj

þ ð1Þi
1  gi
2ð1 þ giÞ

mT
i S1mi þ li

 ð1Þi
 1  gi
2ð1 þ giÞ þ 1
2

mT
i S1mj
¼ GB
i .
(3.176)
Now let
bLi ¼ n
i þ 1
n
i
ðx0  x1ÞTS1ðx0  x1Þ ¼ n
i þ 1
n
i
ˆd 2,
(3.177)
where ˆd 2 ¼ ðx0  x1ÞTS1ðx0  x1Þ. Similar to deriving Eq. 3.173 via the
variance of quadratic forms of Gaussian variables, we can show that
varSnðˆd 2jmÞ ¼ 4d2m
 1
n0
þ 1
n1

þ 2D
 1
n0
þ 1
n1
2
.
(3.178)
Thus,
varSnðbLijmÞ ¼
n
i þ 1
n
i
2
varSnðˆd 2jmÞ →
BKac 0.
(3.179)
As before, from Chebyshev’s inequality it follows that
plim
BKac
bL0jm ¼ plim
BKac
bL1jm ¼ lim
BKac ESn½bLijm ¼ L,
(3.180)
with L being defined in Eq. 3.164. By the continuous mapping theorem
(continuous functions preserve convergence in probability),
150
Chapter 3

plim
BKac
ˆε injm ¼ plim
BKac
F

ð1Þi bGB
i þ w
ﬃﬃﬃﬃﬃ
bLi
q
m
¼ F

plim
BKac
ð1Þi bGB
i þ w
ﬃﬃﬃﬃﬃ
bLi
q
m

¼ F

ð1Þi GB
i þ w
ﬃﬃﬃﬃ
L
p

.
(3.181)
Convergence in probability implies convergence in distribution. Thus, from
Eq. 3.181,
ð1Þi bGB
i þ w
ﬃﬃﬃﬃﬃ
bLi
q

m →
d Zi,
(3.182)
where →
d means convergence in distribution, and Zi is a random variable with
point mass at the constant ð1ÞiðGB
i þ wÞ∕
ﬃﬃﬃﬃ
L
p
. Boundedness and continuity
of Fð⋅Þ along with Eq. 3.182 allow one to apply the Helly–Bray theorem
(Sen and Singer, 1993) to write
lim
BKacESn½ˆε in jm  ¼ lim
BKacESn

F

ð1Þi bGB
i þ w
ﬃﬃﬃﬃﬃ
bLi
q
m

¼ EZi½FðZiÞ
¼ F

ð1Þi GB
i þ w
ﬃﬃﬃﬃ
L
p

¼ plim
BKac
ˆε in jm ,
(3.183)
which completes the proof.
▪
Since d2m →d2
m, D∕n0 →l0, and D∕n1 →l1, Theorem 3.11 suggests the
following finite-sample approximation:
ESn½ ˆε 0n jm  ≂F
0
@ GB,f
0
þ w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2m þ D
n0 þ D
n1
q
1
A,
(3.184)
where GB,f
0
is obtained by using the finite-sample parameters of the problem in
Eq. 3.162, namely,
151
Sample-Conditioned MSE of Error Estimation

GB,f
0
¼
1
2ð1 þ b0Þ

b0ðhm0;m1  hm0;m0Þ þ d2m þ ð1  b0Þ D
n0
þ ð1 þ b0Þ D
n1

.
(3.185)
To obtain the corresponding approximation for ESn½ ˆε 1n jm, it suffices to use
Eq. 3.184 by changing the sign of w, and exchanging n0 with n1, n0 with n1
ðor b0 with b1Þ, m0 with m1, and m0 with m1 in GB,f
0
to produce GB,f
1 .
To obtain a Raudys-type finite-sample approximation, first note that the
Gaussian distribution in Eq. 2.124 can be rewritten as
ˆε 0n ¼ PrðU0ðx0, x1, zÞ ≤wjx0, x1, z ∈C0Þ,
(3.186)
where z is independent of Sn, Ci is the multivariate Gaussian distribution
N ðmi, ½ðni þ ni þ 1Þðni þ niÞ∕n2
i SÞ, and
Uiðx0, x1, zÞ ¼

ni
ni þ ni
z þ
nixi
ni þ ni
 x0 þ x1
2
T
S1ðx0  x1Þ.
(3.187)
Taking the expectation of ˆε 0n relative to the sampling distribution and
then applying the standard normal approximation yields a Raudys-type
approximation:
ESn½ˆε 0n jm ¼ PrðU0ðx0, x1, zÞ ≤wjz ∈C0, mÞ
≂F
ESn,z½U0ðx0, x1, zÞjz ∈C0, m þ w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
varSn,zðU0ðx0, x1, zÞjz ∈C0, mÞ
p

.
(3.188)
It is proven in (Zollanvari and Dougherty, 2014) via an equivalent
representation of the right-hand side of Eq. 3.188 that
ESn½ ˆε 0n jm ≂F
 
GB,R
0
þ w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LB,R
0
q
!
,
(3.189)
where
GB,R
0
¼ GB,f
0 ,
(3.190)
with GB,f
0
defined in Eq. 3.185, and
152
Chapter 3

LB,R
0
¼ d2m þ D
n0
þ D
n1
þ
2d2m
n0ð1 þ b0Þ2 þ
d2m
n1ð1 þ b0Þ
þ
b0
ð1 þ b0Þ2
hm0;m1  ð1  b0Þhm0;m0
n0
þ ð1 þ b0Þhm0;m1  hm0;m0
n1

þ ð3 þ b2
0ÞD
2n2
0ð1 þ b0Þ2 þ
ð2 þ b0ÞD
n0n1ð1 þ b0Þ2 þ D
2n2
1
.
(3.191)
The corresponding approximation for ESn½ ˆε 1n jm is
ESn½ ˆε 1n jm  ≂F
0
@GB,R
1
 w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LB,R
1
q
1
A,
(3.192)
where LB,R
1
and GB,R
1
are obtained from LB,R
0
and GB,R
0
, respectively, by
exchanging n0 with n1, n0 with n1, m0 with m1, and m0 with m1. It is
straightforward to show that
GB,R
0
→
BKac GB
0 ,
(3.193)
LB,R
0
→
BKac d2
m þ l0 þ l1,
(3.194)
with GB
0 being defined in Theorem 3.11. Therefore, the approximation
obtained in Eq. 3.189 is asymptotically exact, and Eqs. 3.184 and 3.189 are
asymptotically equivalent. Similar limits hold for class 1.
3.9.3 Unconditional expectation
The unconditional expectation of ˆε in under Bayesian–Kolmogorov asympto-
tics is provided by the next theorem; for the proof of this theorem we refer to
(Zollanvari and Dougherty, 2014).
Theorem 3.12 (Zollanvari and Dougherty, 2014). Consider the sequence of
Gaussian discrimination problems defined by Eq. 3.156. Then for i ∈f0, 1g,
lim
BKacEm,Sn½ ˆε in ¼ F

ð1Þi Hi þ w
ﬃﬃﬃﬃ
F
p

,
(3.195)
and therefore,
lim
BKacEm,Sn½ ˆεn ¼ c0F
H0 þ w
ﬃﬃﬃﬃ
F
p

þ c1F
H1  w
ﬃﬃﬃﬃ
F
p

,
(3.196)
153
Sample-Conditioned MSE of Error Estimation

where
H0 ¼ 1
2

D2
m þ l1  l0 þ l0
g0
þ l1
g1

,
(3.197)
H1 ¼  1
2

D2
m þ l0  l1 þ l0
g0
þ l1
g1

,
(3.198)
F ¼ D2
m þ l0 þ l1 þ l0
g0
þ l1
g1
.
(3.199)
Theorem 3.12 suggests the finite-sample approximation
Em,Sn½ ˆε 0n  ≂F
 
HR
0 þ w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
D2m þ D
n0 þ D
n1 þ D
n0 þ D
n1
q
!
,
(3.200)
where
HR
0 ¼ 1
2

D2m þ D
n1
 D
n0
þ D
n0
þ D
n1

.
(3.201)
From Eq. 3.186 we can get a Raudys-type approximation:
Em,Sn½ˆε 0n  ¼ Em½PrðU0ðx0, x1, zÞ ≤wjz ∈C0, mÞ
≂F
Em,Sn;z½U0ðx0, x1, zÞjz ∈C0 þ w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
varm,Sn;zðU0ðx0, x1, zÞjz ∈C0Þ
p

.
(3.202)
As proven in (Zollanvari and Dougherty, 2014) via an equivalent representa-
tion of the right-hand side of Eq. 3.202,
Em,Sn½ ˆε 0n  ≂F
 
HR
0 þ w
ﬃﬃﬃﬃﬃﬃﬃ
FR
0
q
!
,
(3.203)
where
F R
0 ¼

1 þ 1
n0
þ 1
n1
þ 1
n1

D2m þ D
 1
n0
þ 1
n1
þ 1
n0
þ 1
n1

þ D
 1
2n2
0
þ 1
2n2
1
þ 1
2n2
0
þ 1
2n2
1

þ D
 1
n1n0
þ
1
n1n1
þ
1
n0n1

.
(3.204)
It is straightforward to show that
HR
0
→
BKac H0,
(3.205)
154
Chapter 3

FR
0
→
BKac D2
m þ l0 þ l1 þ l0
g0
þ l1
g1
,
(3.206)
with H0 defined in Eq. 3.197. Hence, the approximation obtained in Eq. 3.203
is asymptotically exact, and Eqs. 3.200 and 3.203 are asymptotically
equivalent. Similar results hold for class 1.
3.9.4 Conditional second moments
This section employs Bayesian–Kolmogorov asymptotic analysis to charac-
terize the second and cross moments with the actual error, which provides an
asymptotic expression for the MSE of error estimation. As with first
moments, we will follow asymptotic theorems with suggested finite-sample
approximations and then state Raudys-type approximations, referring the
interested reader to (Zollanvari and Dougherty, 2014) for the derivations.
Defining two i.i.d. random vectors z and z0 yields the second-moment
representation
ESn½ðˆε0nÞ2jm
¼ ESn½Pr2ðU0ðx0, x1, zÞ ≤wjx0, x1, z ∈C0, mÞ
¼ ESn½PrðU0ðx0, x1, zÞ ≤wjx0, x1, z ∈C0, mÞ
 PrðU0ðx0, x1, z0Þ ≤wjx0, x1, z0 ∈C0, mÞ
¼ ESn½PrðU0ðx0, x1, zÞ ≤w, U0ðx0, x1, z0Þ ≤wjx0, x1, z ∈C0, z0 ∈C0, mÞ
¼ PrðU0ðx0, x1, zÞ ≤w, U0ðx0, x1, z0Þ ≤wjz ∈C0, z0 ∈C0, mÞ,
(3.207)
where z and z0 are independent of Sn, Ci is N ðmi, ½ðni þ ni þ 1Þðni þ niÞ∕n2
i SÞ,
and Uiðx0, x1, zÞ is defined in Eq. 3.187.
Theorem 3.13 (Zollanvari and Dougherty, 2014). Consider the sequence of
Gaussian discrimination problems in Eq. 3.151. Then for i, j ∈f0, 1g,
lim
BKacESn½ ˆε in ˆε j
njm ¼ F

ð1Þi GB
i þ w
ﬃﬃﬃﬃ
L
p

F

ð1Þj GB
j þ w
ﬃﬃﬃﬃ
L
p

,
(3.208)
and therefore,
lim
BKacESn½ðˆεnÞ2jm ¼

c0F
GB
0 þ w
ﬃﬃﬃﬃ
L
p

þ c1F
GB
1  w
ﬃﬃﬃﬃ
L
p
2
,
(3.209)
where GB
0 , GB
1 , and L are defined in Eqs. 3.162 through 3.164, respectively.
155
Sample-Conditioned MSE of Error Estimation

Proof. From Eq. 3.207,
ESn½ðˆε 0n Þ2 jm  ¼ ESn½PrðU0ðx0, x1, zÞ ≤w, U0ðx0, x1, z0Þ ≤w
jx0, x1, z ∈C0, z0 ∈C0, mÞ.
(3.210)
We characterize the conditional probability inside ESn½⋅. From the indepen-
dence of z, z0, x0, and x1,

U0ðx0, x1, zÞ
U0ðx0, x1, z0Þ
 x0, x1, z ∈C0, z0 ∈C0, m  N
0
@
" bGB
0
bGB
0
#
,
"
bL0
0
0
bL0
#1
A,
(3.211)
where bGB
0 and bL0 are defined in Eqs. 3.166 and 3.177, respectively. Thus,
ESn½ðˆε 0n Þ2 jm  ¼ ESn
2
4F2
 
bGB
0 þ w
ﬃﬃﬃﬃﬃﬃ
bL0
q
!m
3
5.
(3.212)
Similar to the proof of Theorem 3.11, we get
lim
BKacESn½ðˆε inÞ2 jm ¼ plim
BKac
ðˆε inÞ2 jm
¼

lim
BKacESn½ ˆε in jm 
2
¼ F2

ð1Þi GB
i þ w
ﬃﬃﬃﬃ
L
p

.
(3.213)
Similarly,
lim
BKacESn½ ˆε 0n ˆε 1n jm ¼ plim
BKac
ˆε 0n ˆε 1n jm,
(3.214)
and the result follows via the decomposition of the error estimate in terms of
the two class estimates.
▪
Theorem 3.13 suggests the finite-sample approximation
ESn½ðˆε 0n Þ2jm ≂F2
0
@ GB,f
0
þ w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2m þ D
n0 þ D
n1
q
1
A,
(3.215)
which is the square of the approximation in Eq. 3.184. Approximations for
ESn½ ˆε 0n ˆε 1n jm and ESn½ðˆε 1n Þ2jm are obtained analogously. A similar proof yields
the next theorem.
156
Chapter 3

Theorem 3.14 (Zollanvari and Dougherty, 2014). Consider the sequence of
Gaussian discrimination problems in Eq. 3.151. Then for i, j ∈f0, 1g,
lim
BKacESn½ ˆε inεj
njm ¼ F

ð1Þi GB
i þ w
ﬃﬃﬃﬃ
L
p

F

ð1Þj Gj þ w
ﬃﬃﬃﬃ
L
p

,
(3.216)
and therefore,
lim
BKacESn½ ˆεnεnjm ¼
X
1
i¼0
X
1
j¼0
cicjF

ð1Þi GB
i þ w
ﬃﬃﬃﬃ
L
p

F

ð1Þj Gj þ w
ﬃﬃﬃﬃ
L
p

,
(3.217)
where
G0 ¼ 1
2 ðd2
m þ l1  l0Þ,
(3.218)
G1 ¼  1
2 ðd2
m þ l0  l1Þ,
(3.219)
and GB
0 , GB
1 , and L are defined in Eqs. 3.162 through 3.164, respectively.
Theorem 3.14 suggests the finite-sample approximation
ESn½ ˆε 0n ε0njm ≂F
0
@ GB,f
0
þ w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2m þ D
n0 þ D
n1
q
1
AF
0
@ 1
2 ðd2m þ D
n1  D
n0Þ þ w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
d2m þ D
n0 þ D
n1
q
1
A.
(3.220)
This is a product of Eq. 3.184 and the finite-sample approximation for
ESn½ε0njm in (Zollanvari et al., 2011). Class 1 and mixed class cases are similar.
A consequence of Theorems 3.11, 3.13, and 3.14 is that the conditional
variances and covariances are asymptotically zero:
lim
BKacvarSnðˆεn jmÞ ¼ lim
BKacvarSnðεn jmÞ ¼ lim
BKaccovSnðεn, ˆεn jmÞ ¼ 0.
(3.221)
Hence, the deviation variance is also asymptotically zero:
lim
BKacvarSnðˆεn  εn jmÞ ¼ 0.
(3.222)
Define the conditional bias as
biasSnðˆεnjm Þ ¼ ESn½ ˆεn  εnjm .
(3.223)
Then the asymptotic RMS reduces to
157
Sample-Conditioned MSE of Error Estimation

lim
BKacRMSSnðˆεnjm Þ ¼ lim
BKacjbiasSnðˆεnjmÞj.
(3.224)
The asymptotic RMS is thus the asymptotic absolute bias. As proven in
(Zollanvari et al., 2011),
lim
BKacESn½εnjm ¼ c0F
G0 þ w
ﬃﬃﬃﬃ
L
p

þ c1F
G1  w
ﬃﬃﬃﬃ
L
p

.
(3.225)
It follows from Theorem 3.11 and Eq. 3.225 that
lim
BKacbiasSnðˆεnjmÞ ¼ c0

F
GB
0 þ w
ﬃﬃﬃﬃ
L
p

 F
G0 þ w
ﬃﬃﬃﬃ
L
p

þ c1

F
GB
1  w
ﬃﬃﬃﬃ
L
p

 F
G1  w
ﬃﬃﬃﬃ
L
p

.
(3.226)
Recall that the Bayesian MMSE error estimator is unconditionally unbiased:
biasm,SnðˆεnÞ ¼ Em,Sn½ ˆεn  εn ¼ 0.
To obtain Raudys-type approximations corresponding to Theorems 3.13
and 3.14, we utilize the joint distribution of Uiðx0, x1, zÞ and Ujðx0, x1, z0Þ,
defined in Eq. 3.187, with z and z0 being independently selected from
populations C0 or C1. We utilize the function
Fða, b; rÞ ¼
Z a
`
Z b
`
1
2p
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1  r2
p
exp
ðx2 þ y2  2rxyÞ
2ð1  r2Þ

dxdy,
(3.227)
which is the bivariate CDF of standard normal random variables with
correlation coefficient r. Note that Fða, `; rÞ ¼ FðaÞ, and Fða, b; 0Þ ¼
FðaÞFðbÞ. For notational simplicity, we write Fða, a; rÞ as Fða; rÞ. For any
Gaussian vector ½X, YT,
PrðX ≤x, Y ≤yÞ ¼ F
x  mX
sX
, y  mY
sY
; rXY

,
(3.228)
with mX ¼ E½X, mY ¼ E½Y, sX ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
varðXÞ
p
, sY ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
varðYÞ
p
, and correla-
tion coefficient rXY.
A Raudys-type approximation for the second conditional moment is
given by
ESn½ðˆε 0n Þ2 jm ≂F
0
@GB,R
0
þ w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LB,R
0
q
; CB,R
0
LB,R
0
1
A,
(3.229)
with GB,R
0
and LB,R
0
given in Eqs. 3.190 and 3.191, respectively, and
158
Chapter 3

CB,R
0
¼
d2m
n1ð1 þ b0Þ þ ð1  b0Þd2m
n0ð1 þ b0Þ2
þ
b0
ð1 þ b0Þ2
hm0,m1  ð1  b0Þhm0,m0
n0
þ ð1 þ b0Þhm0,m1  hm0,m0
n1

þ ð1  b0Þ2D
2n2
0ð1 þ b0Þ2 þ
D
n0n1ð1 þ b0Þ2 þ D
2n2
1
.
(3.230)
Similarly,
ESn½ðˆε 1n Þ2 jm ≂F
0
@GB,R
1
 w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LB,R
1
q
; CB,R
1
LB,R
1
1
A,
(3.231)
where LB,R
1
and GB,R
1
are defined immediately following Eq. 3.192, and CB,R
1
is
obtained from CB,R
0
using Eq. 3.230 by exchanging n0 with n1, n0 with n1, m0
with m1, and m0 with m1. Having CB,R
0
→
BKac 0 together with Eqs. 3.193 and
3.194 shows that Eq. 3.229 is asymptotically exact, that is, asymptotically
equivalent to ESn½ðˆε 0n Þ2jm obtained in Theorem 3.13. Class 1 is similar.
For the conditional mixed moments, first,
ESn½ ˆε 0n ˆε 1n jm ≂F
0
@GB,R
0
þ w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LB,R
0
q
, GB,R
1
 w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LB,R
1
q
;
CB,R
01
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LB,R
0
LB,R
1
q
1
A,
(3.232)
where
CB,R
01
¼ b0hm0,m0,m0,m1  b0b1hm0,m0,m1,m0 þ b1hm1,m1,m1,m0 þ b1d2m þ d2m
n0ð1 þ b0Þð1 þ b1Þ
þ b1hm1,m1,m1,m0  b0b1hm1,m1,m0,m1 þ b0hm0,m0,m0,m1 þ b0d2m þ d2m
n1ð1 þ b0Þð1 þ b1Þ
þ
D
n0n1ð1 þ b0Þð1 þ b1Þ þ ð1  b0ÞD
2n2
0ð1 þ b0Þ þ ð1  b1ÞD
2n2
1ð1 þ b1Þ .
(3.233)
Since CB,R
01
→
BKac 0, Eq. 3.232 is asymptotically exact; i.e., Eq. 3.232 becomes
equivalent to the result of Theorem 3.13. Next,
ESn½ ˆε 0n ε0njm ≂F
0
@GB,R
0
þ w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LB,R
0
q
, GR
0 þ w
ﬃﬃﬃﬃﬃﬃﬃ
LR
0
q
;
CBT,R
0
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LB,R
0
LR
0
q
1
A,
(3.234)
159
Sample-Conditioned MSE of Error Estimation

where
CBT,R
0
¼
1
n1ð1 þ b0Þ

d2m þ b0d2m þ b0hm0,m0,m0,m1

 ð1  b0ÞD
2n2
0ð1 þ b0Þ þ D
2n2
1
,
(3.235)
and
GR
0 ¼ 1
2

d2m þ D
n1
 D
n0

,
(3.236)
LR
0 ¼ d2m þ d2m
n1
þ D
 1
n0
þ 1
n1
þ 1
2n2
0
þ 1
2n2
1

.
(3.237)
Next,
ESn½ ˆε 1n ε1n jm  ≂F
GB,R
1
 w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LB,R
1
q
, GR
1  w
ﬃﬃﬃﬃﬃﬃﬃ
LR
1
p
;
CBT,R
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LB,R
1
LR
1
q

,
(3.238)
where LB,R
1
and GB,R
1
are obtained as in Eq. 3.231, and LR
1 , GR
1 , and CBT,R
1
are
obtained by exchanging n0 with n1, n0 with n1, m0 with m1, and m0 with m1 in
LR
0 , GR
0 , and CBT,R
0
, respectively. Finally,
ESn½ ˆε 0n ε1n jm ≂F
0
@GB,R
0
þ w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LB,R
0
q
, GR
1  w
ﬃﬃﬃﬃﬃﬃﬃ
LR
1
p
;
CBT,R
01
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LB,R
0
LR
1
q
1
A,
(3.239)
where
CBT,R
01
¼
1
n0ð1 þ b0Þ

d2m þ b0hm0,m0,m0,m1

þ ð1  b0ÞD
2n2
0ð1 þ b0Þ  D
2n2
1
,
(3.240)
and
ESn½ ˆε 1n ε0njm ≂F
0
@GB,R
1
 w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LB,R
1
q
, GR
0 þ w
ﬃﬃﬃﬃﬃﬃﬃ
LR
0
q
;
CBT,R
10
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LB,R
1
LR
0
q
1
A,
(3.241)
where CBT,R
10
is obtained by exchanging n0 with n1, n0 with n1, m0 with m1, and
m0 with m1 in CBT,R
01
.
160
Chapter 3

CBT,R
0
→
BKac 0, CBT,R
1
→
BKac 0, CBT,R
01
→
BKac 0, and CBT,R
10
→
BKac 0. Therefore,
from
Eq.
3.194
and
the
facts
that
GR
0
→
BKac d2
m þ l1  l0
and
LR
0
→
BKac d2
m þ l0 þ l1, we conclude that Eqs. 3.234, 3.238, 3.239, and 3.241
are all asymptotically exact (compare to Theorem 3.14).
3.9.5 Unconditional second moments
We now consider the unconditional second and cross moments. The following
theorems are proven similarly to Theorems 3.13 and 3.14.
Theorem 3.15 (Zollanvari and Dougherty, 2014). Consider the sequence of
Gaussian discrimination problems in Eq. 3.156. For i, j ∈{0, 1},
lim
BKacEm,Sn½ ˆε inˆε j
n ¼ F

ð1Þi Hi þ w
ﬃﬃﬃﬃ
F
p

F

ð1Þj Hj þ w
ﬃﬃﬃﬃ
F
p

,
(3.242)
and therefore,
lim
BKacEm,Sn½ðˆεnÞ2 ¼

c0F
H0 þ w
ﬃﬃﬃﬃ
F
p

þ c1F
H1  w
ﬃﬃﬃﬃ
F
p
2
,
(3.243)
where H0, H1, and F are defined in Eqs. 3.197 through 3.199, respectively.
Theorem 3.16 (Zollanvari and Dougherty, 2014). Consider the sequence of
Gaussian discrimination problems in Eq. 3.156. For i, j ∈{0, 1},
lim
BKacEm,Sn½ ˆε inεj
n ¼ lim
BKacEm,Sn½ ˆε inˆε j
n ¼ lim
BKacEm,Sn½εinεj
n,
(3.244)
and therefore,
lim
BKacEm,Sn½ ˆεnεn ¼
X
i¼0
X
j¼0
cicjF

ð1Þi Hi þ w
ﬃﬃﬃﬃ
F
p

F

ð1Þj Hj þ w
ﬃﬃﬃﬃ
F
p

,
(3.245)
where H0, H1, and F are defined in Eqs. 3.197 through 3.199, respectively.
Theorems 3.15 and 3.16 suggest the finite-sample approximations:
Em,Sn½ðˆε 0n Þ2 ≂Em,Sn½ˆε 0n ε0n ≂Em,Sn½ðε0nÞ2
≂F2
0
@ 1
2
D2m þ D
n1  D
n0 þ D
n0 þ D
n1  2w
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
D2m þ D
n0 þ D
n1 þ D
n0 þ D
n1
q
1
A.
(3.246)
161
Sample-Conditioned MSE of Error Estimation

A consequence of Theorems 3.12, 3.15, and 3.16 is that
lim
BKacvarm,Snðˆεn  εnÞ ¼ lim
BKacjbiasm,SnðˆεnÞj
¼ lim
BKacvarm,SnðˆεnÞ
¼ lim
BKacvarm,SnðεnÞ
¼ lim
BKaccovm,Snðεn, ˆεnÞ
¼ lim
BKacRMSm,SnðˆεnÞ ¼ 0.
(3.247)
Previously, we have shown that ˆεn is strongly consistent under rather
general conditions and that MSEmðˆεnjSnÞ →0 almost surely as n →` under
similar conditions. Here, we have shown that MSEm,SnðˆεnÞ →
BKac 0 under
conditions stated in Eq. 3.157. Intuitively, this means that MSEm,SnðˆεnÞ  0
for asymptotic and comparable dimensionality, sample size, and uncertainty
parameter.
A Raudys-type approximation for the unconditional second moment is
given by
Em,Sn½ðˆε 0n Þ2 ≂F
0
@HR
0 þ w
ﬃﬃﬃﬃﬃﬃﬃ
FR
0
q
; KB,R
0
F R
0
1
A,
(3.248)
with HR
0 and F R
0 given in Eqs. 3.201 and 3.204, respectively, and
KB,R
0
¼

1
n0ð1 þ b0Þ2 þ 1
n1
þ
1
n0ð1 þ b0Þ2 þ 1
n1

D2m
þ D
2n2
0
 D
n0n0
þ D
2n2
0
þ D
2n2
1
þ D
n1n1
þ D
2n2
1
þ
D
n0n1ð1 þ b0Þ2 þ
D
n0n1ð1 þ b0Þ2 þ
D
n1n0ð1 þ b0Þ2 .
(3.249)
Analogously,
Em,Sn½ðˆε 1n Þ2 ≂F
HR
1  w
ﬃﬃﬃﬃﬃﬃﬃ
FR
1
p
; KB,R
1
FR
1

,
(3.250)
where FR
1 , HR
1 , and KB,R
1
are obtained from FR
0 using Eq. 3.204, HR
0 using
Eq. 3.201, and KB,R
0
using Eq. 3.249, respectively, by exchanging n0 with n1, n0
with n1, m0 with m1, and m0 with m1. Having KB,R
0
→
BKac 0 together with
Eq. 3.206 makes Eq. 3.248 asymptotically exact. Class 1 is similar.
162
Chapter 3

Turning to unconditional mixed moments,
Em,Sn½ ˆε 0n ˆε 1n  ≂F
HR
0 þ w
ﬃﬃﬃﬃﬃﬃﬃ
FR
0
q
, HR
1  w
ﬃﬃﬃﬃﬃﬃﬃ
FR
1
p
;
KB,R
01
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
F R
0 FR
1
q

,
(3.251)
where
KB,R
01
¼
D
ðn0 þ n0Þðn1 þ n1Þ þ ðn0  n0ÞD
2n2
0ðn0 þ n0Þ þ ðn1  n1ÞD
2n2
1ðn1 þ n1Þ
þ
n0n1D
n0n1ðn0 þ n0Þðn1 þ n1Þ þ ðn0  n0ÞD
2n2
0ðn0 þ n0Þ þ ðn1  n1ÞD
2n2
1ðn1 þ n1Þ
þ
1
n0 þ n0

1 þ
n0
n1 þ n1
 n0
n0
 D
n0
þ
1
n1 þ n1

1 þ
n1
n0 þ n0
 n1
n1
 D
n1
þ
 1
n0
þ 1
n1

D2m.
(3.252)
Since
KB,R
01
→
BKac 0,
Eq.
3.251
is
asymptotically
exact
(compare
to
Theorem 3.15). Next,
Em,Sn½ ˆε 0n ε0n ≂F
0
@HR
0 þ w
ﬃﬃﬃﬃﬃﬃﬃ
FR
0
q
; KBT,R
0
F R
0
1
A,
(3.253)
where
KBT,R
0
¼

n0
n0ðn0 þ n0Þ þ 1
n1
þ 1
n1

D2m þ D
2n2
1
þ D
n1n1
þ D
2n2
1
þ
n0D
n1n0ðn0 þ n0Þ  ðn0  n0ÞD
2n2
0ðn0 þ n0Þ þ ðn0  n0ÞD
2n2
0ðn0 þ n0Þ þ
n0D
n0n1ðn0 þ n0Þ .
(3.254)
Finally,
Em,Sn½ ˆε 0n ε1n ≂F
0
@HR
0 þ w
ﬃﬃﬃﬃﬃﬃﬃ
FR
0
q
, HR
1  w
ﬃﬃﬃﬃﬃﬃﬃ
FR
1
p
;
KBT,R
01
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
FR
0 FR
1
q
1
A,
(3.255)
where
KBT,R
01
¼
 1
n0
þ 1
n1

D2m þ D
2n2
0
þ D
2n2
1
þ D
n0n1
 D
2n2
0
 D
2n2
1
.
(3.256)
163
Sample-Conditioned MSE of Error Estimation

Having KBT,R
0
→
BKac 0 and KBT,R
01
→
BKac 0 along with Eq. 3.206 makes Eqs. 3.253
and 3.255 asymptotically exact (compare to Theorem 3.16). Approximations
for Em,Sn½ ˆε 1n ε0n and Em,Sn½ ˆε 1n ε1n are obtained analogously.
3.9.6 Unconditional MSE
Computation of the unconditional MSE for the Bayesian MMSE error
estimator requires the second moments of the true error. The conditional
second moments of the true error are (Zollanvari et al., 2011)
ESn½ðε0nÞ2jm ≂F
0
@GR
0 þ w
ﬃﬃﬃﬃﬃﬃﬃ
LR
0
q
; CT,R
0
LR
0
1
A,
(3.257)
ESn½ðε1nÞ2jm ≂F
0
@GR
1  w
ﬃﬃﬃﬃﬃﬃﬃ
LR
1
p
; CT,R
1
LR
1
1
A,
(3.258)
ESn½ε0nε1njm ≂F
0
@GR
0 þ w
ﬃﬃﬃﬃﬃﬃﬃ
LR
0
q
, GR
1  w
ﬃﬃﬃﬃﬃﬃﬃ
LR
1
p
;
CT,R
01
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
LR
0 LR
1
q
1
A,
(3.259)
where GR
0 and LR
0 are defined in Eqs. 3.236 and 3.237, respectively, GR
1 and LR
1
are defined immediately after Eq. 3.238, and
CT,R
0
¼ d2m
n1
þ D
2n2
0
þ D
2n2
1
,
(3.260)
CT,R
1
¼ d2m
n0
þ D
2n2
0
þ D
2n2
1
,
(3.261)
CT,R
01
¼  D
2n2
0
 D
2n2
1
.
(3.262)
We have already seen Eq. 3.257 in Eq. 1.66.
The unconditional second moments of the true error are obtained in
(Zollanvari and Dougherty, 2014) for the purpose of obtaining the MSE.
First,
Em,Sn½ðε0nÞ2 ≂F
0
@HR
0 þ w
ﬃﬃﬃﬃﬃﬃﬃ
F R
0
q
; KT,R
0
FR
0
1
A,
(3.263)
164
Chapter 3

with HR
0 and F R
0 given in Eqs. 3.201 and 3.204, respectively, and
KT,R
0
¼
 1
n0
þ 1
n1
þ 1
n1

D2m þ D
2n2
0
þ D
2n2
1
þ D
n0n1
þ D
2n2
0
þ D
2n2
1
þ D
n1n0
þ D
n1n1
.
(3.264)
Second,
Em,Sn½ðε1nÞ2 ≂F
0
@HR
1  w
ﬃﬃﬃﬃﬃﬃﬃ
F R
1
p
; KT,R
1
F R
1
1
A,
(3.265)
with KT,R
1
obtained from KT,R
0
by exchanging n0 with n1 and n0 with n1.
Finally,
Em,Sn½ε0nε1n ≂F
0
@HR
0 þ w
ﬃﬃﬃﬃﬃﬃﬃ
F R
0
q
, HR
1  w
ﬃﬃﬃﬃﬃﬃﬃ
FR
1
p
;
KT,R
01
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
F R
0 F R
1
q
1
A,
(3.266)
with HR
0 and F R
0 given in Eqs. 3.201 and 3.204, respectively, and
KT,R
01
¼
 1
n0
þ 1
n1

D2m þ D
2n2
0
þ D
2n2
1
þ D
n0n1
 D
2n2
0
 D
2n2
1
.
(3.267)
All of the moments necessary to compute MSESnðˆεnjmÞ and MSEm,SnðˆεnÞ
have now been expressed asymptotically.
Example 3.3. Following (Zollanvari and Dougherty, 2014), we compare the
asymptotically exact finite-sample approximations to Monte Carlo estima-
tions in conditional and unconditional scenarios for 100 features. Throughout,
let S have diagonal elements 1 and off-diagonal elements 0.1, and let v0 and v1
be defined such that vi has equal elements, v0 ¼ v1, and hv0;v1 ¼ 4 (this
squared Mahalanobis distance corresponds to a Bayes error of 0.1586). We
also define the hyperparameters n0 ¼ n1 ¼ 50 and mi ¼ 1.01vi for the
Bayesian model.
In the conditional scenario, we set the class means to mi ¼ vi for
i ∈f0, 1g. Given the sample sizes, hyperparameters, ci, and mi, all
conditional asymptotic and Raudys-type approximations may be computed.
We also approximate conditional error moments and the conditional RMS
using Monte Carlo simulations. In each iteration, we generate training data
of size ni for each class i ∈f0, 1g using m0, m1, and S, and use these to train
an LDA classifier cn. We then find the true error of cn using the true means
165
Sample-Conditioned MSE of Error Estimation

and covariance, and the Bayesian MMSE error estimator using the sample
and above hyperparameters. This process is repeated 10,000 times, and ðˆεnÞ2,
ˆεnεn, and ðˆεn  εnÞ2 are averaged over all iterations to obtain Monte Carlo
approximations of ESn½ðˆεnÞ2jm, ESn½ ˆεnεnjm, and RMS2
SnðˆεnjmÞ, respectively.
In the unconditional case, all unconditional asymptotic and Raudys-type
approximations may be found given only the sample size, hyperparameters,
and ci. To produce Monte Carlo approximations, we generate random
realizations of the means m0 and m1 from the Bayesian model. Given m0, m1,
and S, as in the conditional scenario, we generate training data, train a
classifier, compute the true error, and compute the Bayesian MMSE error
estimate. We generate 300 pairs of means, and for each pair of means we
generate 300 training sets. The appropriate quantities are averaged over all
90,000 samples to obtain Monte Carlo approximations of Em,Sn½ðˆεnÞ2,
Em,Sn½ ˆεnεn, and RMS2
m,SnðˆεnÞ.
Figure
3.6(a)
shows
finite-sample
second-order
and
mixed-moment
approximations as a function of sample size. Each curve label has three fields:
(1) MC (Monte Carlo), FSA (Raudys-type finite-sample approximation),
or asym (Bayesian–Kolmorgorov-asymptotic finite-sample approximation);
(2) unconditional or conditional; and (3) second moment ½of ðˆεnÞ2 or mixed
moment ðof ˆεnεnÞ. For instance, “FSA unconditional second” corresponds to
Em,Sn½ðˆεnÞ2, and “FSA conditional mixed” corresponds to ESn½ ˆεnεnjm. Note
the closeness of the curves for both the second and mixed moments. Also note
that Raudys-type approximations have slightly better performance.
Figure 3.6(b) shows RMS curves. There is extremely close agreement
between the Raudys-type approximation and the MC curves for the
conditional RMS, so much so that they appear as one. Moreover, the
asymptotic finite-sample approximation curve is also very close to the MC
curve. Regarding unconditional RMS, the Raudys-type approximation and
the MC curves are practically identical. There is, however, a large difference
0.01
40
MC unconditional second
FSA unconditional second
asym unconditional second
MC conditional mixed
FSA conditional mixed
asym conditional mixed 
sample size
(a)
(b)
moments
0.00
MC unconditional
FSA unconditional
asym unconditional
MC conditional
FSA conditional
asym conditional
sample size
RMS
60
80
100
120
140
160
180
200
40
60
80
100
120
140
160
180
200
0.02
0.03
0.04
0.05
0.06
0.04
0.08
0.12
Figure 3.6
Comparison of conditional and unconditional finite-sample moment approxima-
tions versus sample size (D ¼ 100): (a) second moments with ðˆεnÞ2 ¼ ˆεnˆεn and mixed
moments with ˆεnεn; (b) RMS.
166
Chapter 3

between them and the finite-sample approximation curve; indeed the FSA
curves are identically 0. In fact, this is an immediate consequence of Eq. 3.247.
So we see that, while the Bayesian–Kolmogorov finite-sample approximations
work
reasonably
well
for
the
individual
moments,
when
the
latter
approximations are combined to form the RMS approximation, the result
is RMS ¼ 0.
167
Sample-Conditioned MSE of Error Estimation

Chapter 4
Optimal Bayesian Classification
Since prior knowledge is required to obtain a good error estimate in small-
sample settings, it is prudent to utilize that knowledge when designing a
classifier. When large amounts of data are available, one can appeal to the
Vapnik–Chervonenkis (VC) theory (and extensions), which bounds the
difference between the error of the best classifier in a family of classifiers
and the error of the designed classifier as a function of sample size, absent
prior distributional knowledge, but this VC bound is generally useless for
small samples. Rather than take a distribution-free approach, we can find a
classifier with minimum expected error, given the data and an uncertainty
class of feature-label distributions. This is accomplished in the framework of
optimal operator design under uncertainty.
4.1 Optimal Operator Design Under Uncertainty
Finding a Bayes classifier is an instance of the most basic problem of
engineering, the design of optimal operators. Design takes different forms
depending on the random process constituting the scientific model and the
operator class of interest. The operators might be filters, controllers,
classifiers, or clusterers. The underlying random process might be a random
signal/image for filtering, a Markov process for control, a feature-label
distribution for classification, or a random point set for clustering. Our
interest concerns optimal classification when the underlying process model,
the feature-label distribution, is not known with certainty, in the sense that it
belongs to an uncertainty class of possible feature-label distributions; that is,
rather than use a classification rule based solely on data, we want a rule based
on the data and prior knowledge concerning the feature-label distribution—
and we want the design to be optimal, not based merely on some heuristic.
Optimal operator design (synthesis) begins with the relevant scientific
knowledge constituted in a mathematical theory that is used to construct an
operator for optimally accomplishing a desired transformation under the
constraints imposed by the circumstances. A criterion called a cost function
169

(objective function) is defined to judge the goodness of the response—the
lower the cost, the better the operator. The objective is to find an optimal way
of manipulating the system, which means minimizing the cost function.
This kind of engineering synthesis originated with optimal time series
filtering in the classic work of Andrey Kolmogorov (Kolmogorov, 1941) and
Norbert Wiener (Wiener, 1949), an unpublished version having appeared in
1942. In the Wiener theory, the scientific model consists of two random
signals, one being the true signal and the other being an observed “noisy”
variant of the true signal. The engineering aim is to linearly operate on the
observed signal so as to transform it to be more like the true signal. Being that
a linear operator is formed by a weighted average, the synthesis problem is to
find an optimal weighting function for the linear operator, and the goodness
criterion is the MSE between the true and filtered signals. An optimal linear
filter is found in terms of the cross-correlation function between the true and
observed random signals. It is the solution of an integral equation known as
the Wiener–Hopf equation. In the case of wide-sense stationary signals, an
optimal filter is found in terms of the signal and noise power spectra and is
known as the Wiener filter. The cross-correlation function and power spectra
are called characteristics of the random processes, the term referring to any
deterministic function of the processes.
The synthesis scheme for optimal linear filtering has four components:
(1) the mathematical model consists of two jointly distributed random signals;
(2) the operator family consists of integral linear filters over an observation
window; (3) optimization involves minimizing the MSE; and (4) the optimi-
zation is performed by solving the Wiener–Hopf equation in terms of certain
process characteristics.
In general, an optimal operator is given by
copt ¼ arg min
c∈F CðcÞ,
(4.1)
where F is the operator family, and CðcÞ is the cost of applying operator c on
the model. The general synthesis scheme has four steps: (1) determine the
mathematical model; (2) define a family of operators; (3) define the
optimization problem via a cost function; and (4) solve the optimization
problem.
For binary classification, the four steps become: (1) determine the feature-
label distribution; (2) define a family of classifiers; (3) define the optimization
problem as minimizing classification error; and (4) solve the optimization
problem in terms of the feature-label distribution.
For instance, for any feature-label distribution, if the classifier family
consists of all classifiers, then an optimal classifier is given by Eq. 1.5. If the
feature-label distribution is Gaussian and the classifier family consists of all
classifiers, then an optimal classifier is given by the quadratic discriminant in
170
Chapter 4

Eq. 1.19. In the former case, the optimization is solved in terms of the class-
conditional distributions; in the latter, it is solved in terms of moments of the
feature-label distribution, which serve as characteristics.
The situation changes when the underlying model is not known with
certainty but belongs to an uncertainty class of models parameterized by a
vector u belonging to a parameter set U. In the case of linear filtering, the
cross-covariance function may contain
unknown parameters, or with
classification, it may be known that the feature-label distribution is Gaussian,
but the covariance matrices not fully known. Under such circumstances, we
define an intrinsically Bayesian robust (IBR) operator by
cU
IBR ¼ arg min
c∈F Eu½CuðcÞ,
(4.2)
where Cu is the cost function for model u (Dalton and Dougherty, 2014; Yoon
et al., 2013; Dougherty, 2018).
An IBR operator is robust in the sense that on average it performs well
over the uncertainty class. Since each u ∈U corresponds to a model, pðuÞ
quantifies our prior knowledge that some models are more likely to be the
actual model than are others. If there is no prior knowledge beyond the
uncertainty class itself, then the prior can sometimes be taken to be uniform.
With linear filtering, the IBR filter is found in terms of the effective auto-
and cross-covariance functions or effective power spectra (Dalton and
Dougherty, 2014). These are examples of effective characteristics, which
replace the original characteristics for a known model with characteristics
representing the entire uncertainty class. IBR design has been applied in
various engineering areas, including Kalman filtering, where the Kalman gain
matrix is replaced by the effective Kalman gain matrix (Dehghannasiri et al.,
2017a), and Karhunen–Loève compression, where the covariance matrix is
replaced by the effective covariance matrix (Dehghannasiri et al., 2018a).
When a set S of sample data is available, the prior distribution can be
updated to a posterior distribution pðuÞ ¼ pðujSÞ. The IBR operator relative
to the posterior distribution is called an optimal Bayesian operator (OBO)
and is denoted by cOBO. An IBR operator is an OBO with S ¼ ∅. Besides
classification, which is our topic here, OBO design has been developed for
Kalman filtering (Dehghannasiri et al., 2018b) and regression (Qian and
Dougherty, 2016).
To reduce computation when an IBR operator is found via a search, for
instance, when effective characteristics are not known, one can constrain
the minimization to operators that are optimal for some state u ∈U.
A model-constrained (state-constrained) Bayesian robust (MCBR) operator
is defined by
cU
MCBR ¼ arg min
c∈F UEu½CuðcÞ,
(4.3)
171
Optimal Bayesian Classification

where F U is the set of all operators in F that are optimal for some u ∈U. An
MCBR operator is suboptimal relative to an IBR operator.
This chapter develops the optimal Bayesian classifier (and therefore the
IBR classifier) for binary classification, and the following chapter extends
this to the optimal Bayesian classifier for multi-class classification. Whereas in
the case of Wiener and Kalman filtering, an IBR filter is determined in the
same form as the standard filter with the characteristics replaced by effective
characteristics, for classification we will see that an optimal IBR classifier is
obtained similarly to the usual Bayes classifier with the class-conditional
distributions being replaced by effective class-conditional distributions. This is
an example of an IBR operator being found in the same manner as the
standard optimal operator, but with the original random process replaced by
effective processes, another example being IBR clustering, where the original
underlying random labeled point sets are replaced by effective random labeled
point sets (Dalton et al., 2018).
In the 1960s, control theorists formulated the problem of model
uncertainty in a Bayesian manner by assuming an uncertainty class of
models, positing a prior distribution on the uncertainty class, and then
selecting the control policy that minimizes a cost function averaged over the
uncertainty class (Silver, 1963; Gozzolino et al., 1965; Martin, 1967).
Application was impeded by the extreme computational burden, the result
being that adaptive control became prevalent. The computational cost
remains a problem (Yousefi and Dougherty, 2014). In the 1970s, signal-
processing theorists took a minimax approach by assuming an uncertainty
class of covariance matrices (or power spectra) and choosing the linear filter
that gave the best worst-case performance over the uncertainty class
(Kuznetsov, 1976; Kassam and Lim, 1977; Poor, 1980). Later an MCBR
approach to filtering was taken (Grigoryan and Dougherty, 2001), followed
by use of the IBR methodology.
A critical point regarding IBR design is that the prior distribution is not
on the parameters of the operator (controller, filter, classifier, clusterer), but
on the unknown parameters of the scientific model. If the model were known
with certainty, then one would optimize with respect to the known model; if
the model is uncertain, then the optimization is naturally extended to include
model uncertainty and the prior distribution on that uncertainty. Model
uncertainty induces uncertainty on the model-specific optimal operator
parameters. If one places the prior directly on the operator parameters while
ignoring model uncertainty, then there is a scientific gap, meaning that the
relation between scientific knowledge and operator design is broken. For
instance, compare optimal Bayesian regression (Qian and Dougherty, 2016),
which is a form of optimal Bayesian filtering, to standard Bayesian linear
regression (Bernardo and Smith, 2000; Bishop, 2006; Hastie et al., 2009;
Murphy, 2012). In the latter, the connection between the regression functions
172
Chapter 4

and prior assumptions on the underlying physical system is unclear. Prior
knowledge should be dictated by scientific knowledge.
4.2 Optimal Bayesian Classifier
Ordinary Bayes classifiers minimize the misclassification probability when the
underlying distributions are known and utilize Bayes’ theorem according to
Eq. 1.2. On the other hand, optimal Bayesian classification trains a classifier
from data while assuming that the feature-label distribution is contained in an
uncertainty class parameterized by u ∈U that is governed by a prior distribution
pðuÞ. Via Eq. 4.2, we define an optimal Bayesian classifier (OBC) by
cOBC ¼ arg min
c∈C Ep½εnðu, cÞ,
(4.4)
where εnðu, cÞ is the cost function, and C is an arbitrary family of classifiers.
The family C in the definition allows one to control the classifiers considered,
for instance, insisting on a linear form, restricting complexity, or constraining
the search to cases with known closed-form error estimates or other desirable
properties.
Although Bayes classifiers and OBCs both employ Bayes’ theorem to
facilitate derivations, they are fundamentally different and apply Bayes’
theorem differently. Bayes classifiers place priors PrðY ¼ yÞ on the class
probabilities and evaluate posteriors PrðY ¼ yjX ¼ xÞ conditioned on the
test point via Eq. 1.2. For a sample Sn consisting of ny points xy
1, xy
2, : : : , xy
ny
from class y ∈f0, 1g, we see from Eqs. 2.17 and 2.18 that OBCs place priors
and posteriors on the feature-label distribution parameters ðincluding cÞ, where
posteriors are conditioned on the training data. Under the Bayesian framework,
PrðcðXÞ ≠YjSnÞ ¼ Ep½PrðcðXÞ ≠Yju, SnÞ
¼ Ep½εnðu, cÞ
¼ bεnðSn, cÞ:
(4.5)
Thus, OBCs minimize the expected misclassification probability relative to the
assumed model or, equivalently, minimize the Bayesian MMSE error estimate.
An OBC can be found by brute force using closed-form solutions for the
expected true error (the Bayesian MMSE error estimator), when available;
however, if C is the set of all classifiers (with measurable decision regions),
then the next theorem shows that an OBC can be found analogously to a
Bayes classifier for a fixed distribution. In particular, we can realize an
optimal solution without explicitly finding the error for every classifier
because the solution can be found pointwise: Choose the class with the highest
density at this point, where instead of using the class-conditional density, as in
the classical setting, we use the effective class-conditional density f UðxjyÞ. The
density at other points in the space does not affect the classifier decision.
173
Optimal Bayesian Classification

Theorem 4.1 (Dalton and Dougherty, 2013a). An OBC cOBC satisfying Eq. 4.4
over all c ∈C, the set of all classifiers with measurable decision regions, exists
and is given pointwise by
cOBCðxÞ ¼

0
if Ep½c f Uðxj0Þ ≥ð1  Ep½cÞ f Uðxj1Þ,
1
otherwise:
(4.6)
Proof. For any classifier c with measurable decision regions R0 and R1,
Ep½εnðu, cÞ is given by Eq. 2.27. To minimize the integral, we minimize the
integrand pointwise. This is achieved by the classifier in the theorem, which
indeed has measurable decision regions.
▪
If Ep½c ¼ 0, then this OBC is a constant and always assigns class 1, and if
Ep½c ¼ 1, then it always assigns class 0. Hence, we will typically assume that
0 , Ep½c , 1.
According to the theorem, to find an OBC we can average the class-
conditional densities f uyðxjyÞ relative to the posterior distribution to obtain
the effective class-conditional density f UðxjyÞ, whereby an OBC is found by
Eq. 4.6. Essentially, the optimal thing to do is to find the Bayes classifier using
f Uðxj0Þ and f Uðxj1Þ as the true class-conditional distributions. This is like a
plug-in rule using f UðxjyÞ. This methodology finds an OBC over all possible
classifiers, which is guaranteed to have minimum expected error in the given
model. Henceforth, we will only consider OBCs over the space of all classifiers
given in Theorem 4.1.
The OBC is defined by the optimization of Eq. 4.4, and Theorem 4.1
provides a general solution regardless of the feature-label distributions in the
uncertainty class as long as the classifier family consists of all classifiers with
measurable decision regions. The optimization fits into the general paradigm
of optimization under model uncertainty, and therefore the classifier of
Eq. 4.6 provides a solution to Eq. 4.2. In the Gaussian case, this classifier has
a long history in Bayesian statistics (Jeffreys, 1961; Geisser, 1964; Guttman
and Tiao, 1964; Dunsmore, 1966), with the effective density being called the
predictive density.
4.3 Discrete Model
For the discrete model, the OBC is found by applying Theorem 4.1 to the
effective class-conditional densities in Eq. 2.53:
cOBCð jÞ ¼
8
>
<
>
:
0
if Ep½c
U0
j þ a0
j
n0 þ Pb
i¼1 a0
i
≥ð1  Ep½cÞ
U1
j þ a1
j
n1 þ Pb
i¼1 a1
i
,
1
otherwise:
(4.7)
174
Chapter 4

The expected error of the optimal classifier is found via Theorem 2.1:
bεnðSn, cOBCÞ ¼
X
b
j¼1
min

Ep½c
U0
j þ a0
j
n0 þ Pb
i¼1 a0
i
, ð1  Ep½cÞ
U1
j þ a1
j
n1 þ Pb
i¼1 a1
i

:
(4.8)
In this case it is easy to verify that the OBC minimizes the Bayesian MMSE
error estimator by minimizing each term in the sum of Eq. 2.55. This is
achieved by assigning cð jÞ the class with the smaller constant scaling the
indicator function.
In
the
special
case
where
we
have
uniform
priors
for
c ðay ¼ 1 for y ∈f0, 1gÞ, random sampling, and uniform priors for the bin
probabilities ðay
i ¼ 1 for all i and yÞ, the OBC is
cOBCð jÞ ¼
8
<
:
0
if n0 þ 1
n0 þ b ðU0
j þ 1Þ ≥n1 þ 1
n1 þ b ðU1
j þ 1Þ,
1
otherwise,
(4.9)
and the expected error of the optimal classifier is
bεnðSn, cOBCÞ ¼
X
b
j¼1
min
n0 þ 1
n þ 2 ⋅
U0
j þ 1
n0 þ b , n1 þ 1
n þ 2 ⋅
U1
j þ 1
n1 þ b

:
(4.10)
Hence, when n0 ¼ n1, the discrete histogram rule is equivalent to an OBC that
assumes uniform priors and random sampling. Under arbitrary ny, it is also
equivalent to an OBC that assumes (improper priors) ay ¼ ay
i ¼ 0 for all i and
y and random sampling. Otherwise, the discrete histogram rule is not
necessarily optimal within an arbitrary Bayesian framework.
Example 4.1. To demonstrate the advantage of OBCs, consider a synthetic
simulation where c and the bin probabilities are generated randomly
according to uniform prior distributions. For each fixed feature-label
distribution, a binomialðn, cÞ experiment determines the number of sample
points in class 0, and the bin for each point is drawn from bin probabilities
corresponding to its class, thus generating a random sample of size n. Both
the histogram rule and the OBC from Eq. 4.9 are trained from the sample.
The true error for each classifier is calculated via Eqs. 2.41 and 2.42. This is
repeated 100,000 times to obtain the average true error for each classification
rule, presented in Fig. 4.1 for b ¼ 4 and 8 bins. The average performance of
the OBC is superior to that of the discrete histogram rule, especially for larger
bin sizes. However, OBCs are not guaranteed to be optimal for a specific
distribution (the optimal classifier is the Bayes classifier), but are only optimal
when averaged over all distributions relative to the posterior distribution.
175
Optimal Bayesian Classification

4.4 Gaussian Model
The four covariance models considered in Chapter 2 can be categorized into
three effective density models: Gaussian (known covariance), multivariate t
(scaled
identity
and
general
covariance
structure),
and
independent
non-standardized Student’s t (diagonal covariance). We will consider all
combinations
of
these
effective
density
models
for
the
two
classes:
(1) covariances known in both classes, (2) covariances diagonal in both
classes, (3) covariances scaled identity/general in both classes, and (4) mixed
covariance models. The homoscedastic covariance model can be applied in
the second and third cases, with the appropriate definition of the posterior
hyperparameters. There is no feature selection in the OBC designed here;
rather, these classifiers utilize all features in the model to minimize the
expected true error.
We also discuss the close relationships between OBCs and their plug-in
counterparts, LDA, QDA, and NMC, in terms of analytic formulation,
approximation, and convergence as n →`. Although all of these models
assume Gaussian class-conditional densities, in some cases the OBC is not
linear or quadratic, but rather takes a higher-order polynomial form, although
one can easily evaluate the classifier at a given test point x. When the OBC is
linear, the Bayesian MMSE error estimator can be found in closed form using
equations derived in Chapter 2; otherwise, it may be approximated via
Theorem 2.1 by sampling the effective density.
4.4.1 Both covariances known
In the presence of uncertainty, when both covariances are known, the effective
class-conditional distributions are Gaussian and are provided in Eq. 2.96 with
n ¼ ny and m ¼ my. The OBC is the optimal classifier between the effective
5
10
15
20
25
30
0.20
0.21
0.22
0.23
0.24
0.25
0.26
0.27
0.28
0.29
sample size
(a)
(b)
average true error
 
 
histogram
OBC
5
10
15
20
25
30
0.20
0.22
0.24
0.26
0.28
0.30
0.32
0.34
sample size
average true error
 
 
histogram
OBC
Figure 4.1
Average true errors on discrete distributions from known priors with uniform c
and bin probabilities versus sample size: (a) b ¼ 4; (b) b ¼ 8. [Reprinted from (Dalton and
Dougherty, 2013a).]
176
Chapter 4

Gaussians with class-0 probability Ep½c. That is, cOBCðxÞ ¼ 0 if gOBCðxÞ ≤0,
and cOBCðxÞ ¼ 1 if gOBCðxÞ . 0, where
gOBCðxÞ ¼ xTAOBCx þ aT
OBCx þ bOBC
(4.11)
is quadratic with
AOBC ¼  1
2

n
1
n
1 þ 1 S1
1

n
0
n
0 þ 1 S1
0

,
(4.12)
aOBC ¼
n
1
n
1 þ 1 S1
1 m
1 
n
0
n
0 þ 1 S1
0 m
0,
(4.13)
bOBC ¼  1
2

n
1
n
1 þ 1 m
1
TS1
1 m
1 
n
0
n
0 þ 1 m
0
TS1
0 m
0

þ ln
1  Ep½c
Ep½c
n
1ðn
0 þ 1Þ
n
0ðn
1 þ 1Þ
D
2jS0j
jS1j
1
2
:
(4.14)
This classifier is simply a QDA plug-in rule with [(ny þ 1)/ny] Sy in place of the
covariance
for
class
y,
my
in
place
of
the
mean
for
class
y ðmy ¼ bmy if n0 ¼ n1 ¼ 0Þ, and Ep½c in place of c (Ep½c ¼ n0∕n under
improper beta priors with ay ¼ 0 and random sampling).
If
S ¼ n
0 þ 1
n
0
S0 ¼ n
1 þ 1
n
1
S1,
(4.15)
then the classifier is linear with
gOBCðxÞ ¼ aT
OBCx þ bOBC
(4.16)
and
aOBC ¼ S1ðm
1  m
0Þ,
(4.17)
bOBC ¼  1
2 ðm
1  m
0ÞTS1ðm
1 þ m
0Þ þ ln 1  Ep½c
Ep½c
:
(4.18)
This is equivalent to a plug-in rule like LDA with S in place of the covariance
for both classes, my in place of the mean for class y, and Ep½c in place of c.
The expected true error for this case is simply the true error for this linear
classifier under the effective densities: Gaussian distributions with mean my
and covariance S. This is given by
bεnðSn, cOBCÞ ¼ Ep½cbε0nðSn, cOBCÞ þ ð1  Ep½cÞbε1nðSn, cOBCÞ,
(4.19)
177
Optimal Bayesian Classification

where
bεy
nðSn, cOBCÞ ¼ F
0
@ð1ÞygOBCðmyÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aT
OBCSaOBC
q
1
A ¼ F
0
@ð1ÞygOBCðmyÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aT
OBCSyaOBC
q
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ny
ny þ 1
s
1
A:
(4.20)
The last line also follows by plugging gOBC and aOBC into Eq. 2.124.
4.4.2 Both covariances diagonal
When both covariances are diagonal, whether they be independent or
homoscedastic,
the
effective
class-conditional
distribution
is
given
by
Eq. 2.110, where n ¼ ny and m ¼ my are given by Eqs. 2.65 and 2.66,
respectively. In addition, a and bi are given by Eqs. 2.107 and 2.108,
respectively, where k ¼ ky and S ¼ Sy are given by Eqs. 2.67 and 2.68,
respectively, in the independent covariance model, and k and S are given by
Eqs. 2.80 and 2.81, respectively, in the homoscedastic covariance model. Let
m
y,i be the ith element in my, ay be a for class y, and by,i be bi for class y.
Hence, the effective densities take the form
f UðxjyÞ ¼
Y
D
i¼1
G

ay þ 1
2
	
GðayÞp
1
2ð2ayÞ
1
2
by,iðny þ 1Þ
ayny
1
2


1 þ 1
2ay
by,iðny þ 1Þ
ayny
1
ðxi  m
y,iÞ2
ðayþ1
2Þ
:
(4.21)
The discriminant of the OBC can be simplified to
gOBCðxÞ ¼ K
Y
D
i¼1

1 þ
n
0
2b0,iðn
0 þ 1Þ ðxi  m
0,iÞ2
2a0þ1

Y
D
i¼1

1 þ
n
1
2b1,iðn
1 þ 1Þ ðxi  m
1,iÞ2
2a1þ1
,
(4.22)
where
K ¼
1  Ep½c
Ep½c
2ðn
0 þ 1Þn
1
n
0ðn
1 þ 1Þ
DY
D
i¼1
b0,i
b1,i
2
4Gða0ÞG

a1 þ 1
2
	
G

a0 þ 1
2
	
Gða1Þ
3
5
2D
:
(4.23)
178
Chapter 4

4.4.3 Both covariances scaled identity or general
In the scaled identity covariance model, the effective class-conditional
distribution is a multivariate t-distribution given by Eq. 2.103, where a and
b are given by Eqs. 2.101 and 2.102, respectively. In the general covariance
model, the effective density is again a multivariate t-distribution given by
Eq. 2.120. In all models, n ¼ ny and m ¼ my are given by Eqs. 2.65 and 2.66,
respectively. In all independent covariance models, k ¼ ky and S ¼ Sy are
given by Eqs. 2.67 and 2.68, respectively, and in all homoscedastic covariance
models k and S are given by Eqs. 2.80 and 2.81, respectively. If the
covariances are: (1) independent and follow some combination of the scaled
identity and general covariance models, (2) homoscedastic scaled identity, or
(3) homoscedastic general, then the effective class-conditional densities take
the form
f UðxjyÞ ¼
G
kyþD
2
	
G
ky
2
	
k
D
2yp
D
2jCyj
1
2


1 þ 1
ky
ðx  myÞTC1
y ðx  myÞ

kyþD
2 ,
(4.24)
where ky and Cy denote the degrees of freedom and the scale matrix for class
y, respectively (the location vector is always my). The discriminant of the OBC
can be simplified to
gOBCðxÞ ¼ K

1 þ 1
k0
ðx  m
0ÞTC1
0 ðx  m
0Þ
k0þD


1 þ 1
k1
ðx  m
1ÞTC1
1 ðx  m
1Þ
k1þD
,
(4.25)
where
K ¼
1  Ep½c
Ep½c
2k0
k1
D jC0j
jC1j
2
64
G

k0
2
	
G

k1þD
2
	
G

k0þD
2
	
G

k1
2
	
3
75
2
:
(4.26)
This classifier has a polynomial decision boundary if k0 and k1 are integers,
which is satisfied for both scaled identity and general covariance models with
independent covariances if k0 and k1 are integers, and with homoscedastic
covariances if k is an integer.
In the special case where k ¼ k0 ¼ k1, which always occurs in a
homoscedastic covariance model, the OBC can be further simplified to
179
Optimal Bayesian Classification

gOBCðxÞ ¼ ðx  m
0ÞT
C0
K0
1
ðx  m
0Þ
 ðx  m
1ÞT
C1
K1
1
ðx  m
1Þ þ kðK0  K1Þ,
(4.27)
where
K0 ¼

jC0j
ðEp½cÞ2

1
kþD,
(4.28)
K1 ¼

jC1j
ð1  Ep½cÞ2

1
kþD:
(4.29)
Thus, the OBC is quadratic and given by Eq. 4.11, with
AOBC ¼  1
2 ðK1C1
1
 K0C1
0 Þ,
(4.30)
aOBC ¼ K1C1
1 m
1  K0C1
0 m
0,
(4.31)
bOBC ¼  1
2 ðK1m
1
TC1
1 m
1  K0m
0
TC1
0 m
0Þ þ kðK0  K1Þ
2
:
(4.32)
The optimal classifier becomes quadratic like QDA, except with K1
y Cy in
place of the covariance for class y, my in place of the mean for class y
ðif n0 ¼ n1 ¼ 0, this is bmyÞ, and the threshold bOBC.
If k ¼ k0 ¼ k1 and C ¼ K1
0 C0 ¼ K1
1 C1, then the OBC is linear and
given by Eq. 4.16, with
aOBC ¼ C1ðm
1  m
0Þ,
(4.33)
bOBC ¼  1
2 aT
OBCðm
1 þ m
0Þ þ kðK0  K1Þ
2
:
(4.34)
This occurs in the (scaled identity or general) homoscedastic covariance
model if
n
0 þ 1
n
0
k
ðEp½cÞ2 ¼
n
1 þ 1
n
1
k
ð1  Ep½cÞ2,
(4.35)
which is guaranteed if n
0 ¼ n
1 and Ep½c ¼ 0.5.
OBCs under these models can be shown to be approximately equivalent to
plug-in counterparts, LDA, QDA, and NMC, under certain non-informative
priors. We consider several cases: (1) independent general covariances with
n0 ¼ n1 and Ep½c ¼ 0.5 (the OBC is similar to QDA with a modified
180
Chapter 4

estimated covariance and modified threshold); (2) independent scaled identity
covariances with n0 ¼ n1 and Ep½c ¼ 0.5 (similar to QDA with a modified
estimated scaled identity covariance and modified threshold); (3) homosce-
dastic general covariances (closely approximated by and converging to LDA
when plugging in Ep½c for the class-0 probability, and equivalent to LDA
when n0 ¼ n1 and Ep½c ¼ 0.5); and (4) homoscedastic scaled identity
covariances (closely approximated by and converging to NMC when we
plug Ep½c in for the class-0 probability, and equivalent to NMC when
n0 ¼ n1 and Ep½c ¼ 0.5).
There are three salient points. First, OBCs with these non-informative
priors assuming independent or homoscedastic covariances have close, and in
some cases nearly identical, performance to QDA and LDA, respectively.
This suggests that the classical LDA and QDA classifiers are nearly optimal,
given their respective covariance and prior assumptions. If informative priors
are known, then OBC can further improve performance. Second, experiments
with synthetic Gaussian data suggest that the performance of the OBC
converges to that of Bayes classification, in which case a non-informative
prior can be overcome when enough data are obtained as long as the true
distribution is contained in the uncertainty class. Third, the OBC essentially
unifies LDA and QDA in a general framework, showing how to optimally
design classifiers under different Gaussian modeling assumptions.
Independent General Covariances
Consider a Gaussian independent general covariance model with non-
informative priors having hyperparameters n0 ¼ n1 ¼ 0, k0 ¼ k1 ¼ 0, and
S0 ¼ S1 ¼ 0DD (the Jeffreys rule prior). It can be shown that my ¼ bmy and
Sy ¼ ðny  1ÞbSy, where bmy is the sample mean, and bSy is the sample
covariance for class y. If we further assume that n0 ¼ n1 points are observed in
each class, then n
0 ¼ n
1 ¼ n∕2 and k
0 ¼ k
1 ¼ n∕2. Finally, assume that
Ep½c ¼ 0.5. Define
eSy ¼ jbSyj 2
nþ2bSy:
(4.36)
Then the OBC is quadratic as in Eq. 4.11, with
AOBC ¼  1
2
eS1
1
 eS1
0
	
,
(4.37)
aOBC ¼ eS1
1 bm1  eS1
0 bm0,
(4.38)
bOBC ¼  1
2

bm1T eS1
1 bm1  bm0T eS1
0 bm0
	
þ 1
n
n
2 þ 1
n
2  1

jeS0j
2
n2Dþ2  jeS1j
2
n2Dþ2
	
:
(4.39)
181
Optimal Bayesian Classification

This is like the QDA classifier, which plugs estimated parameters into the
Bayes classifier for Gaussian distributions, except that it plugs in a modified
estimate eSy for Sy and uses a modified threshold bOBC.
Example 4.2. We next illustrate performance of the OBC using non-
informative priors under fixed heteroscedastic Gaussian distributions with
D ¼ 2 features and known c ¼ 0.83. Assume that the true means are m0 ¼ 0D
and m1 ¼ 1D, and that the true covariances are S0 ¼ 0.65ID and
S1 ¼ 0.35

1
0.5
0.5
1

:
(4.40)
Under these distributions, the Bayes classifier is quadratic.
We employ stratified sampling, where the proportion of class-0 points is
chosen to be as close as possible to c. From each sample, three classifiers
are designed: LDA, QDA, and OBC. OBC uses an independent general
covariance non-informative prior with ny ¼ 0, ky ¼ 0, and Sy ¼ 0DD.
my need not be specified because ny ¼ 0. Once all classifiers have been
trained, the true error for each is found under the known modeling
assumptions. For LDA the true error is found exactly from Eq. 1.32, and
for QDA and OBC (which is non-quadratic under the assumed model) the
true error is approximated by generating a stratified test sample of size
100,000. For each sample size, this entire process is repeated for t ¼ 10,000
samples.
Once this iteration is complete, for each classifier we find the average true
error with respect to sample size, which is reported in Fig. 4.2. This figure
contains two graphs: (a) a realization of a sample and classifiers (Bayes, LDA,
−2
−1
0
1
2
3
−2
−1
0
1
2
x1
x2
 
 
Bayes
LDA
QDA
OBC
0
20
40
60
80
100
120
0.09
0.1
0.11
0.12
0.13
0.14
0.15
sample size
(a)
(b)
average true error
 
 
Bayes
LDA
QDA
OBC
Figure 4.2
Classification of fixed Gaussian distributions with c ¼ 0.83 and unequal
covariances with respect to sample size: (a) example with n ¼ 60; (b) average true error.
OBC assumes independent general covariances and a non-informative prior.
182
Chapter 4

QDA, and OBC) and (b) the average true error for each classifier with respect
to sample size. In the realization of a sample and classifiers, points from class
0 are marked with circles and points from class 1 with x’s. To give an idea of
the location of the true distributions, level curves of both class-conditional
distributions are shown in thin gray lines. These have been found by setting
the Mahalanobis distance to 1. Although LDA initially performs best for very
small sample sizes, when more points are collected, the OBC has the best
performance of the three classifiers and makes a noticeable improvement over
QDA on average over the sampling distribution.
Independent Scaled Identity Covariances
Now consider a Gaussian independent scaled identity covariance model with
non-informative priors having hyperparameters n0 ¼ n1 ¼ 0, k0 ¼ k1 ¼ 0,
and S0 ¼ S1 ¼ 0DD. Again, my ¼ bmy and Sy ¼ ðny  1ÞbSy. Assume that
n0 ¼ n1 so that n
0 ¼ n
1 ¼ n∕2 and k
0 ¼ k
1 ¼ n∕2, and also assume that
Ep½c ¼ 0.5. Define
eSy ¼
trðbSyÞ
D
ðnþ2Dþ2ÞD4
ðnþ2Dþ4ÞD4
ID:
(4.41)
Then the OBC is quadratic, of the form in Eq. 4.11, with
AOBC ¼  1
2
eS1
1
 eS1
0
	
,
(4.42)
aOBC ¼ eS1
1 bm1  eS1
0 bm0,
(4.43)
bOBC ¼  1
2

bm1T eS
1
1 bm1  bm0T eS
1
0 bm0
	
þ D
n
n
2 þ 1
n
2  1
h
jeS0j
2
ðnþ2Dþ2ÞD4  jeS1j
2
ðnþ2Dþ2ÞD4
i
:
(4.44)
This is like QDA except that it plugs in a modified estimate eSy of Sy and uses
a modified threshold bOBC.
Homoscedastic General Covariances
Next consider a Gaussian homoscedastic general covariance model with
non-informative priors having hyperparameters n0 ¼ n1 ¼ 0, k ¼ 0, and
S ¼ 0DD.
Make
no
assumptions
on
Ep½c
or
on
n0
and
n1
ðn0 ≠n1 being possibleÞ. It can be shown that my ¼ bmy and S ¼ ðn  2ÞbS,
where bS is the pooled sample covariance defined in Eq. 1.28. Define
183
Optimal Bayesian Classification

eS0 ¼
n0 þ 1
n0
nDþ1
nþ1 ðEp½cÞ
2
nþ1bS,
(4.45)
eS1 ¼
n1 þ 1
n1
nDþ1
nþ1 ð1  Ep½cÞ
2
nþ1bS:
(4.46)
Then the OBC is quadratic, given by Eq. 4.11, with
AOBC ¼  1
2
eS1
1
 eS1
0
	
,
(4.47)
aOBC ¼ eS1
1 bm1  eS1
0 bm0,
(4.48)
bOBC ¼  1
2

bm1
T eS1
1 bm1  bm0
T eS1
0 bm0
	
þ n  2
2
n0 þ 1
n0
 D
nþ1
1
Ep½c
 2
nþ1
 n  2
2
n1 þ 1
n1
 D
nþ1
1
1  Ep½c
 2
nþ1:
(4.49)
Note that eS0  bS and eS1  bS even for relatively small n0 and n1. In addition,
the following lemma holds.
Lemma 4.1 (Dalton and Dougherty, 2013b). Suppose that both n0 →` and
n1 →` almost surely over the infinite labeled sampling distribution. Let
f ðcÞ ¼ lnðð1  cÞ∕cÞ and
f nðcÞ ¼ n  2
2
n0 þ 1
n0
 D
nþ11
c
 2
nþ1 
n1 þ 1
n1
 D
nþ1
1
1  c
 2
nþ1
:
(4.50)
Then f nðcÞ →f ðcÞ uniformly (almost surely) over any interval of the form
½d, 1  d with 0 , d , 0.5.
Proof. Let e . 0. There exists N . 0 such that 1 , ðn0 þ 1Þ∕n0 , ee∕D and
1 , ðn1 þ 1Þ∕n1 , ee∕D for all n . N. Hence, for c ∈½0, 1,
n  2
2
1
c
 2
nþ1 
 e
e
2
1  c
 2
nþ1
≤f nðcÞ
≤n  2
2
e
e
2
c
 2
nþ1 

1
1  c
 2
nþ1
:
(4.51)
The limit of the right-hand side is lnðð1  cÞ∕cÞ þ e∕2 for all c ∈½0, 1, and
convergence is uniform over c ∈½d, 1  d. Similarly, the limit of the left-hand
184
Chapter 4

side is lnðð1  cÞ∕cÞ  e∕2 for all c ∈½0, 1, and convergence is uniform over
c ∈½d, 1  d. Hence, there exists N0 . N such that for all n . N0 and
c ∈½d, 1  d,
ln
1  c
c

 e ≤f nðcÞ ≤ln
1  c
c

þ e,
(4.52)
which completes the proof.
▪
By Lemma 4.1, if 0 , c , 1 is the true c, then any consistent estimator bc
for c may be used in place of c, and we are guaranteed that f nðbcÞ →f ðcÞ.
When applied using Ep½c, this shows that the last two terms of bOBC in
Eq. 4.49 will track f ðEp½cÞ, which resembles the analogous term in LDA
classification, and this will ultimately converge to f ðcÞ, which is the analogous
term in the Bayes classifier.
Although the classifier in this case is quadratic, even for relatively small n
the approximations eS0  bS, eS1  bS, and f nðcÞ  f ðcÞ are very accurate.
Hence, even under small samples, the OBC is very closely approximated by a
variant of LDA using Ep½c rather than n0∕n as the plug-in estimate of c, and,
under either random sampling or known c, both OBC and LDA converge to
the Bayes classifier.
If we assume that n0 ¼ n1 and Ep½c ¼ 0.5, then the OBC is in fact
linear and of the form in Eq. 4.16 with aOBC ¼ bS
1ðbm1  bm0Þ and
bOBC ¼ 0.5aT
OBCðbm1 þ bm0Þ. This is exactly the LDA classifier; thus, LDA
classification is optimal in a Bayesian framework if we assume homoscedastic
covariances, the Jeffreys rule prior, n0 ¼ n1, and Ep½c ¼ 0.5.
Example 4.3. Next consider the performance of the OBC using non-
informative priors under fixed homoscedastic Gaussian distributions with
D ¼ 2 features and known c ¼ 0.83. Assume that the true means are m0 ¼ 0D
and m1 ¼ 1D, and that the true covariances are S0 ¼ S1 ¼ 0.5ID. Under these
distributions, the Bayes classifier is linear.
We employ stratified sampling where the proportion of class-0 points is set
as close as possible to c ¼ 0.5. From each sample, three classifiers are
designed: LDA, QDA, and OBC. The OBC uses a homoscedastic general
covariance non-informative prior with ny ¼ 0, k ¼ 0, and S ¼ 0DD. my need
not be specified because ny ¼ 0. Once all classifiers have been trained, the
true error is found exactly for LDA from Eq. 1.32, and for QDA and OBC
(which is quadratic, though approximately linear, under the assumed model)
the true error is approximated by generating a stratified test sample of
size 100,000. For each sample size, this entire process is repeated for
t ¼ 10,000 samples.
185
Optimal Bayesian Classification

Once this iteration is complete, for each classifier we find the average true
error with respect to sample size, which is reported in Fig. 4.3. As in Fig. 4.2,
this figure contains two graphs: (a) a realization of a sample and classifiers
(Bayes, LDA, QDA, and OBC) and (b) the average true error for each
classifier with respect to sample size. LDA and OBC happen to make a correct
modeling assumption that the covariances are equal, and not surprisingly they
have superior performance. Further, OBC makes a slight improvement over
LDA on average over the sampling distribution.
Homoscedastic Scaled Identity Covariances
Consider a Gaussian homoscedastic scaled identity covariance model with
non-informative priors having hyperparameters n0 ¼ n1 ¼ 0, k ¼ 0, and
S ¼ 0DD. Make no assumptions on Ep½c or on n0 and n1. As before,
my ¼ bmy and S ¼ ðn  2ÞbS. Define
eS0 ¼
n0 þ 1
n0
ðnþDþ1ÞD2
ðnþDþ2ÞD2ðEp½cÞ
2
ðnþDþ2ÞD2 trðbSÞ
D
ID,
(4.53)
eS1 ¼
n1 þ 1
n1
ðnþDþ1ÞD2
ðnþDþ2ÞD2ð1  Ep½cÞ
2
ðnþDþ2ÞD2 trðbSÞ
D
ID:
(4.54)
Then the OBC is quadratic, given by Eq. 4.11 with
AOBC ¼  1
2
eS1
1
 eS1
0
	
,
(4.55)
aOBC ¼ eS1
1 bm1  eS1
0 bm0,
(4.56)
−2
−1
0
1
2
−2
−1
0
1
2
3
x2
x1
Bayes
LDA
QDA
OBC
0
20
40
60
80
100
120
0.1
0.11
0.12
0.13
0.14
0.15
0.16
sample size
(a)
(b)
average true error
Bayes
LDA
QDA
OBC
Figure 4.3
Classification of fixed Gaussian distributions with c ¼ 0.83 and equal scaled
identity covariances with respect to sample size: (a) example with n ¼ 60; (b) average true
error. OBC assumes homoscedastic general covariances and a non-informative prior.
186
Chapter 4

bOBC ¼  1
2

bm1
T eS1
1 bm1  bm0
T eS1
0 bm0
	
þ Dðn  2Þ
2
n0 þ 1
n0

D
ðnþDþ2ÞD2
1
Ep½c

2
ðnþDþ2ÞD2
 Dðn  2Þ
2
n1 þ 1
n1

D
ðnþDþ2ÞD2
1
1  Ep½c

2
ðnþDþ2ÞD2:
(4.57)
Analogous
to
the
general
covariance
case,
eS0  D1 trðbSÞID
and
eS1  D1 trðbSÞID even for relatively small n0 and n1, and the following
lemma holds.
Lemma 4.2 (Dalton and Dougherty, 2013b). Suppose that both n0 →` and
n1 →` almost surely over the infinite labeled sampling distribution. Let
f ðcÞ ¼ lnðð1  cÞ∕cÞ and
f nðcÞ ¼ Dðn  2Þ
2
"n0 þ 1
n0

D
ðnþDþ2ÞD21
c

2
ðnþDþ2ÞD2

n1 þ 1
n1

D
ðnþDþ2ÞD2
1
1  c

2
ðnþDþ2ÞD2
#
:
(4.58)
Then f nðcÞ →f ðcÞ uniformly (almost surely) over any interval of the form
½d, 1  d with 0 , d , 0.5.
Proof. The proof is nearly identical to that of Lemma 4.1.
▪
As before, if 0 , c , 1, then any consistent estimator bc for c may be used
in place of c, and we are guaranteed that f nðbcÞ →f ðcÞ. Although the classifier
in this case is in general quadratic, even for relatively small n the
approximations eS0  D1 trðbSÞID, eS1  D1 trðbSÞID, and f nðcÞ  f ðcÞ are
very accurate. Hence, the OBC is very closely approximated by and quickly
tracks an NMC classifier with Ep½c rather than n0∕n as the plug-in estimate
of c, and, under random sampling or known c, both converge to the Bayes
classifier.
If we assume that n0 ¼ n1 and Ep½c ¼ 0.5, then the OBC is linear, given
by Eq. 4.16 with aOBC ¼ bm1  bm0 and bOBC ¼  1
2 aT
OBCðbm1 þ bm0Þ. This is
exactly the NMC classifier; thus, NMC classification is optimal in a Bayesian
framework if we assume homoscedastic scaled identity covariances, the
Jeffreys rule prior, n0 ¼ n1, and Ep½c ¼ 0.5.
187
Optimal Bayesian Classification

4.4.4 Mixed covariance models
If the covariance is known for one class, say class 0, and diagonal for the
other, the effective class-conditional distributions take the form
f Uðxj0Þ ¼
ðn
0Þ
D
2
ðn
0 þ 1Þ
D
2ð2pÞ
D
2jS0j
1
2
 exp


n
0
2ðn
0 þ 1Þ ðx  m
0ÞTS1
0 ðx  m
0Þ

,
(4.59)
f Uðxj1Þ ¼
Y
D
i¼1
G

a1 þ 1
2
	
Gða1Þp
1
2ð2a1Þ
1
2
b1,iðn
1 þ 1Þ
a1n
1
1
2


1 þ 1
2a1
b1,iðn
1 þ 1Þ
a1n
1
1
ðxi  m
1,iÞ2
ða1þ1
2Þ
:
(4.60)
The discriminant of the OBC can be simplified to
gOBCðxÞ ¼
n
0
n
0 þ 1 ðx  m
0ÞTS1
0 ðx  m
0Þ
 ð2a1 þ 1Þ
X
D
i¼1
ln

1 þ
n
1
2b1,iðn
1 þ 1Þ ðxi  m
1,iÞ2

þ K,
(4.61)
where
K ¼ 2 ln
0
B
@1  Ep½c
Ep½c
ðn
0 þ 1Þn
1
n
0ðn
1 þ 1Þ
D
2
jS0j
QD
i¼1 b1,i
1
2
2
4G

a1 þ 1
2
	
Gða1Þ
3
5
D1
C
A:
(4.62)
Now consider the case in which the covariance is known for only one class
and is scaled identity/general for the other. The effective class-conditional
distribution for the known class, say class 0, is Gaussian and again given by
Eq. 4.59. In the other class, it is a multivariate t-distribution with
k1 ¼ ðk
1 þ D þ 1ÞD  2 degrees of freedom, location vector m
1, and scale
matrix C1 ¼ ½trðS
1Þðn
1 þ 1Þ∕ðk1n
1ÞID in the scaled identity covariance model
(by Eq. 2.103), or k1 ¼ k
1  D þ 1 degrees of freedom, location vector m
1,
and scale matrix C1 ¼ ½ðn
1 þ 1Þ∕ðk1n
1ÞS
1 in the general covariance model
(by Eq. 2.120). Hence, the effective class-conditional densities take the form
188
Chapter 4

f Uðxj0Þ ¼
ðn
0Þ
D
2
ðn
0 þ 1Þ
D
2ð2pÞ
D
2jS0j
1
2
 exp


n
0
2ðn
0 þ 1Þ ðx  m
0ÞTS1
0 ðx  m
0Þ

,
(4.63)
f Uðxj1Þ ¼
G

k1þD
2
	
G

k1
2
	
k
D
2
1p
D
2jC1j
1
2

1 þ 1
k1
ðx  m
1ÞTC1
1 ðx  m
1Þ

k1þD
2 :
(4.64)
The discriminant of the OBC can be simplified to
gOBCðxÞ ¼
n
0
n
0 þ 1 ðx  m
0ÞTS1
0 ðx  m
0Þ
 ðk1 þ DÞ ln

1 þ 1
k1
ðx  m
1ÞTC1
1 ðx  m
1Þ

þ K,
(4.65)
where
K ¼ 2 ln
0
@1  Ep½c
Ep½c
2ðn
0 þ 1Þ
n
0k1
D
2jS0j
jC1j
1
2 G

k1þD
2
	
G

k1
2
	
1
A:
(4.66)
If the covariance is scaled identity/general for one class, say class 0, and
diagonal for the other, where n
0, m
0, k0, C0, n
1, m
1, a1, and b1,i for
i ¼ 1, : : : , D are defined appropriately, then the effective class-conditional
distributions are
f Uðxj0Þ ¼
G

k0þD
2
	
G

k0
2
	
k
D
2
0p
D
2jC0j
1
2

1 þ 1
k0
ðx  m
0ÞTC1
0 ðx  m
0Þ

k0þD
2 ,
(4.67)
f Uðxj1Þ ¼
Y
D
i¼1
G

a1 þ 1
2
	
Gða1Þp
1
2ð2a1Þ
1
2
b1,iðn
1 þ 1Þ
a1n
1
1
2


1 þ 1
2a1
b1,iðn
1 þ 1Þ
a1n
1
1
ðxi  m
1,iÞ2
ða1þ1
2Þ
:
(4.68)
The discriminant of the OBC can be simplified to
189
Optimal Bayesian Classification

gOBCðxÞ ¼ K

1 þ 1
k0
ðx  m
0ÞTC1
0 ðx  m
0Þ
k0þD

Y
D
i¼1

1 þ
n
1
2b1,iðn
1 þ 1Þ ðxi  m
1,iÞ2
2a1þ1
,
(4.69)
where
K ¼
1  Ep½c
Ep½c
2
k0n
1
2ðn
1 þ 1Þ
D
jC0j
QD
i¼1 b1,i
2
64
G

k0
2
	
GD
a1 þ 1
2
	
G

k0þD
2
	
GDða1Þ
3
75
2
:
(4.70)
4.4.5 Average performance in the Gaussian model
We next analyze the performance of OBCs via synthetic simulations with a
proper prior over an uncertainty class of Gaussian distributions and c ¼ 0.5
known. Throughout, two proper priors are considered. The first assumes
independent general covariances, with hyperparameters n0 ¼ 6D, n1 ¼ D,
m0 ¼ 0D, m1 ¼ 0.5 ⋅1D, ky ¼ 3D, and Sy∕ðky  D  1Þ ¼ 0.3ID. In this
model, the Bayes classifier and OBC are both always quadratic. The second
assumes a homoscedastic general covariance, with the same hyperparameters
ðk ¼ ky and S ¼ SyÞ. In the homoscedastic model, the Bayes classifier is linear
while the OBC is quadratic. We will address performance with respect to
sample size, Bayes error, and feature size. In both models, my has the form
my ⋅1D for some scalar my, and Sy∕ðky  D  1Þ has the form 0.3ID. These are
the expected mean and covariance; the actual mean and covariance will not
necessarily have this form. Examples of distributions, samples, and classifiers
realized in these models are shown in Fig. 4.4. Level curves of the Gaussian
−2
−1
0
(a)
(b)
1
−1.5
−1
−0.5
0
0.5
1
1.5
2
x1
x2
x2
x1
Bayes
LDA
QDA
OBC
−1
0
1
2
−2
−1
0
1
2
Bayes
LDA
QDA
OBC
Figure 4.4
Examples of a distribution, sample, and classifiers from Bayesian models
(c ¼ 0.5
known,
D ¼ 2,
n ¼ 60):
(a)
independent
covariances;
(b)
homoscedastic
covariances. [Reprinted from (Dalton and Dougherty, 2013a).]
190
Chapter 4

class-conditional distributions are shown as thin gray lines. These have been
found by setting the Mahalanobis distance to 1. Points from class 0 are
marked with circles and points from class 1 with x’s.
The first series of experiments addresses performance under Gaussian
models with respect to sample size. We follow the procedure in Fig. 2.5.
Random distributions with D ¼ 2 features are drawn from the assumed prior,
and for each we find the Bayes classifier and approximate the Bayes error by
evaluating the proportion of 100,000 stratified testing points drawn from the
true distribution that are misclassified. A stratified training set of a designated
size n is then drawn independently from the corresponding class-conditional
distribution (step 2A). Only even sample sizes are considered so that the
proportion of points in class 0 is always exactly c. The sample is used to
update the prior to a posterior (step 2B). From each sample, three classifiers
are designed (without feature selection): LDA, QDA, and OBC (step 2C). The
true error is found exactly for LDA from Eq. 1.32 and is found approximately
for QDA and OBC by evaluating the proportion of the same 100,000
stratified testing points used to evaluate the Bayes error that are misclassified.
For each covariance model and sample size, this entire process is repeated for
T ¼ 10,000 random distributions and t ¼ 100 samples per distribution.
Once this iteration is complete, the average true error and variance of the
true error are found for the Bayes classifier and each trained classifier. Results
are provided in Fig. 4.5. Experiments using independent covariance and
homoscedastic covariance priors are shown in the top and bottom rows,
respectively. The average true error and the variance of the true error for each
classifier with respect to sample size are shown in the left and right columns,
respectively. The optimality of OBCs is supported as we observe superior
performance relative to LDA and QDA, that is, significantly lower expected
true error. The variance of the true error is also significantly lower. Moreover,
in all cases the performance of OBC appears to converge to that of the Bayes
classifier as n →`. By combining prior modeling assumptions with observed
data, these results show that significantly improved performance can be
obtained.
The next series of experiments addresses performance with respect to
Bayes error for fixed sample size. The procedure is identical to that used in the
first series, except that once the iteration is complete we partition the Bayes
errors into ten uniform bins, which equivalently partitions the realized
samples into bins. The average true error and variance of the true error for
each classifier among all samples associated with each bin is found. Results
are provided in Fig. 4.6 with the independent covariance model and n ¼ 30
shown in the top row and the homoscedastic covariance model and n ¼ 18 in
the bottom row. The average difference between the true error and Bayes
error, and the variance of the true error, for each classification rule with
respect to the Bayes error, are shown in the left and right columns,
191
Optimal Bayesian Classification

respectively. Observe that the average deviation between the true and Bayes
errors increases as the Bayes error increases, until it drops sharply for very
high Bayes errors. By combining prior knowledge with observed data,
significant improvement in performance over LDA and QDA is obtained over
the whole range of Bayes errors. However, it is possible for a classifier to
outperform OBC over some range of Bayes errors in these graphs (although
LDA and QDA do not) because there is no guarantee that the average true
error is minimized for any fixed distribution, or for any subset of the
uncertainly class of distributions in the model (in this case, distributions
having Bayes error in a given range). Optimality is only guaranteed when
averaging over all distributions in the uncertainty class. Thus, OBC is
guaranteed to be optimal in these graphs in the sense that the weighted
average true error over the bins (weighted by the probabilities that a
distribution should have Bayes error falling in that bin) is optimal. Hence, no
classifier could outperform OBC over the entire range of Bayes errors.
0
20
40
60
80
100
120
0.2
0.25
0.3
0.35
sample size
(a)
(b)
(c)
(d)
average true error
Bayes
LDA
QDA
OBC
0
20
40
60
80
100
120
0.005
0.01
0.015
0.02
sample size
variance of true error
Bayes
LDA
QDA
OBC
0
20
40
60
80
100
120
0.2
0.25
0.3
0.35
0.4
sample size
average true error
Bayes
LDA
QDA
OBC
0
20
40
60
80
100
120
0.01
0.012
0.014
0.016
0.018
0.02
sample size
variance of true error
Bayes
LDA
QDA
OBC
Figure 4.5
Performance of classifiers on Gaussian models with known proper priors
versus sample size (c ¼ 0.5 known, D ¼ 2): (a) independent covariances, average true
error; (b) independent covariances, true error variance; (c) homoscedastic covariances,
average true error; (d) homoscedastic covariances, true error variance. [Reprinted from
(Dalton and Dougherty, 2013a).]
192
Chapter 4

Finally, we consider an important beneficial property of OBCs, namely,
that for any fixed sample size n, their average performance across the
sampling distribution and the uncertainty class improves monotonically as the
number of features increases. There is no peaking phenomenon. Indeed, since
for a fixed sample an OBC and its expected error are equivalent to an optimal
classifier and its error on an effective feature-label distribution, if one designs
OBCs cd
OBC on an increasing set of features and finds the expected true
errors bεnðSn, cd
OBCÞ relative to the posterior distribution, then these expected
true errors would be non-increasing, i.e., bεnðSn, cdþ1
OBCÞ ≤bεnðSn, cd
OBCÞ for
d ¼ 1, 2, : : : , D  1.
While OBCs are guaranteed to not peak relative to the Bayesian MMSE
error estimator, we next present a synthetic example in which OBCs also do
0
0.1
0.2
0.3
0.4
0.5
0
0.02
0.04
0.06
0.08
Bayes error
(a)
(b)
(c)
(d)
average true error − Bayes error
LDA
QDA
OBC
0
0.1
0.2
0.3
0.4
0.5
0
1
2
3
4 x 10
−3
Bayes error
variance of true error
Bayes
LDA
QDA
OBC
0
0.1
0.2
0.3
0.4
0.5
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
Bayes error
average true error − Bayes error
LDA
QDA
OBC
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
2
2.5
3
3.5 x 10
−3
Bayes error
variance of true error
Bayes
LDA
QDA
OBC
Figure 4.6
Performance of classifiers on Gaussian models with known proper priors
and fixed sample size versus Bayes error (c ¼ 0.5 known, D ¼ 2): (a) independent
covariance, n ¼ 30, average difference between true and Bayes error; (b) independent
covariance, n ¼ 30, true error variance; (c) homoscedastic covariance, n ¼ 18, average
difference between true and Bayes error; (d) homoscedastic covariance, n ¼ 18, true error
variance. [Reprinted from (Dalton and Dougherty, 2013a).]
193
Optimal Bayesian Classification

not peak relative to the true error. We assume the same two Gaussian models,
this time with D ¼ 30 features. A few minor modifications of the experimental
procedure are required. First, in the classification step, d ∈f1, 2, : : : , Dg
features are selected for training using a t-test. We find the Bayes classifier and
train LDA, QDA, and OBC classifiers, all using only the selected features.
LDA cannot be trained if the pooled covariance matrix is singular, and QDA
may not be applied if either sample covariance is singular. OBC sets the prior
on selected features to the marginal of the prior on all features. OBCs may
only be applied if the posterior is proper, which is guaranteed with proper
priors like the ones considered here. If a classifier cannot be evaluated for a
given sample, then the sample is omitted in the analysis for this classifier (but
not for all classifiers). Once all classifiers have been trained, the true error for
each under the true distribution is found as before. This entire process is
repeated over T ¼ 10,000 random distributions and a single sample per
distribution.
The average true error for each classifier is shown with respect to the
selected feature size for n ¼ 18 in Fig. 4.7. Parts (a) and (b) show performance
using the independent covariance and homoscedastic covariance models,
respectively. Results are only included if the probability that the classifier can
be trained is at least 90%. In general, QDA peaks first, followed by LDA, and
OBC does not peak. For instance, in Fig. 4.7(a) for the independent
covariance model, QDA peaks at around four features and LDA at around
eight features. At only n ¼ 18 points with 9 in each class, QDA is not
trainable for at least 10% of iterations with 7 or more features, and LDA for
more than about 10 features. In contrast, OBC with a proper prior can be
applied with the full 30 features, and its expected true error decreases
monotonically up to and including 30 features, thereby facilitating much
better performance.
0
5
10
15
20
25
30
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
feature set size
(a)
(b)
average true error
Bayes
LDA
QDA
OBC
0
5
10
15
20
25
30
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
feature set size
average true error
Bayes
LDA
QDA
OBC
Figure 4.7
Average true errors on Gaussian distributions from known proper priors with
fixed c and n ¼ 18 versus feature size: (a) independent covariance; (b) homoscedastic
covariance. [Reprinted from (Dalton and Dougherty, 2013a).]
194
Chapter 4

4.5 Transformations of the Feature Space
Consider an invertible transformation t : X →X, mapping some original
feature space X to a new space X (in the continuous case, assume that the
inverse map is continuously differentiable). The next theorem shows that
the OBC in the transformed space can be found by transforming the OBC in
the original feature space pointwise, and that both classifiers have the same
expected true error. The advantages of this fundamental property are at least
twofold. First, the data can be losslessly preprocessed without affecting
optimal classifier design or the expected true error, which is not true in
general, for example, with LDA classification under a nonlinear transforma-
tion. Secondly, it is possible to solve or interpret optimal classification and
error estimation problems by transforming to a more manageable space,
similar to the “kernel trick” used to map features to a high-dimensional
feature space having a meaningful linear classifier. We denote analogous
constants and functions in the transformed space with an overline; for
example, we write a point x in the transformed space as x.
Theorem 4.2 (Dalton and Dougherty, 2013b). Consider a Bayesian model with
posterior pðuÞ in a feature space X that is either discrete or Euclidean. Suppose
that cOBC is an OBC over all c ∈C, where C is a family of classifiers (not
necessarily all classifiers) with measurable decision regions. Moreover, suppose
that the original feature space is transformed by an invertible mapping t and that in
the continuous case t1 is continuously differentiable with an almost everywhere
full rank Jacobian. Then the optimal classifier in the transformed space among the
set of classifiers C ¼ fc : cðxÞ ¼ cðt1ðxÞÞ for all x ∈X and for some c ∈Cg is
cOBCðxÞ ¼ cOBCðt1ðxÞÞ,
(4.71)
and both classifiers possess the same Bayesian MMSE error estimate
Ep½εnðu, cOBCÞ.
Proof. With JðxÞ being the Jacobian determinant of t1 evaluated at x, for a
fixed class y ∈f0, 1g, in the continuous case the class-conditional density
parameterized by uy in the transformed space is
f uyðxjyÞ ¼ f uyðt1ðxÞjyÞjJðxÞj:
(4.72)
In the discrete case, f uyðxjyÞ ¼ f uyðt1ðxÞjyÞ and, to unify the two cases, we
say that jJðxÞj ¼ 1. Although each class-conditional density in the model
uncertainty class will change with the transformation, each may still be
indexed by the same parameter uy. Hence, the same prior and posterior may
be used in both spaces. The effective class-conditional density is thus given by
195
Optimal Bayesian Classification

f UðxjyÞ ¼
Z
Uy
f uyðxjyÞpðuyÞduy
¼
Z
Uy
f uyðt1ðxÞjyÞjJðxÞjpðuyÞduy
¼ f Uðt1ðxÞjyÞjJðxÞj:
(4.73)
Let c ∈C be an arbitrary fixed classifier given by cðxÞ ¼ Ix∈R1, where R1 is a
measurable set in the original feature space. Then cðxÞ ¼ Ix∈R1, where
R1 ¼ ftðxÞ : x ∈R1g is the equivalent classifier in the transformed space,
i.e., cðxÞ ¼ cðt1ðxÞÞ. Noting that Ep½c remains unchanged, by Theorem 2.1
the expected true error of c is given by
Ep½εnðu, cÞ ¼ Ep½c
Z
R1
f Uðxj0Þdx þ ð1  Ep½cÞ
Z
X\R1
f Uðxj1Þdx
¼ Ep½c
Z
R1
f Uðt1ðxÞj0ÞjJðxÞjdx
þ ð1  Ep½cÞ
Z
X\R1
f Uðt1ðxÞj1ÞjJðxÞjdx
¼ Ep½c
Z
R1
f Uðxj0Þdx þ ð1  Ep½cÞ
Z
X\R1
f Uðxj1Þdx
¼ Ep½εnðu, cÞ,
(4.74)
where the integrals in the second to last line have applied the substitution
x ¼ t1ðxÞ. If cOBC is an OBC in the original space and cOBC is the equivalent
classifier in the transformed space, then cOBC also minimizes the expected true
error and thus is an OBC in the transformed space.
▪
4.6 Convergence of the Optimal Bayesian Classifier
As discussed in Section 2.7, we expect the posteriors of c, u0, and u1 to
converge in some sense to their true values, c ∈½0, 1, u0 ∈U0, and u1 ∈U1,
respectively. This is true for c with random sampling, or if c is known.
Posterior
convergence
also
typically
holds
for
uy
as
long
as
every
neighborhood about the true parameter has nonzero prior probability and
the number of sample points observed from class y tends to infinity.
Convergence of the posterior, under typical conditions, leads to consistency in
the Bayesian MMSE error estimator.
196
Chapter 4

In this section, we show that pointwise convergence holds for optimal
Bayesian classification as long as the true distribution is contained in the
parameterized family with mild conditions on the prior. To emphasize that
the posteriors may be viewed as sequences, in the following theorem they are
denoted by pnðcÞ, pnðu0Þ, and pnðu1Þ. Similarly, the effective class-conditional
distributions are denoted by f nðxjyÞ for y ∈f0, 1g. Throughout, we assume
proper priors and that the priors assign positive mass on every open set.
The latter assumption may be relaxed as long as the true distribution is
included in the parameterized family of distributions and certain regularity
conditions hold.
Like Theorem 2.13, the proofs of Theorems 4.3 and 4.4 below treat the
classes separately by decoupling the sampling procedure and the evaluation
of the effective densities for class 0 and class 1. We assume weak
convergence of the individual posteriors separately, with careful treatment
of degenerate cases where c ¼ 0 or c ¼ 1. All that is required is to check
certain regularity conditions on the class-conditional densities. In particular,
Theorem 4.3 applies in the discrete model and Gaussian model with known
covariance.
Theorem 4.3 (Dalton and Dougherty, 2013b). Let c ∈½0, 1, u0 ∈U0, and
u1∈U1. Let c and (u0, u1) be independent in their priors. Suppose that
1. Epn½c →c as n →` almost surely over the infinite labeled sampling
distribution.
2. n0 →` if c . 0 and n1 →` if c , 1 almost surely over the infinite
labeled sampling distribution.
3. For any fixed y ∈f0, 1g, pnðuyÞ is weak consistent if ny →`.
4. For any fixed x, y, and sample size n, the effective density f nðxjyÞ is
finite.
5. For fixed x and y, there exists M . 0 such that f uyðxjyÞ , M for all uy.
Then the OBC (over the space of all classifiers with measurable decision
regions) constructed in Eq. 4.6 converges pointwise (almost surely) to a Bayes
classifier.
Proof. Since we are interested in pointwise convergence, throughout this proof
we fix x ∈X. First assume that 0 , c , 1, so n0, n1 →` (almost surely). For
fixed y ∈f0, 1g and any sample of size ny, by definition EuyjSn½ f uyðxjyÞ equals
the effective class-conditional density f nðxjyÞ. Since the f uyðxjyÞ are bounded
across all uy for fixed x and y, by Eq. 2.153 f nðxjyÞ converges pointwise to the
true class-conditional density f uyðxjyÞ (almost surely). Comparing the Bayes
classifier
197
Optimal Bayesian Classification

cBayesðxÞ ¼

0
if cf u0ðxj0Þ ≥ð1  cÞ f u1ðxj1Þ
1
otherwise
(4.75)
with the OBC given in Eq. 4.6 shows that the OBC converges pointwise
(almost surely) to a Bayes classifier.
If c ¼ 1, then f nðxj0Þ →f u0ðxj0Þ pointwise (almost surely), but f nðxj1Þ
may not converge to f u1ðxj1Þ almost surely (under random sampling, the
number of selected points from class 1 will almost surely be finite by the
Borel–Cantelli lemma). For any sequence in which the number of points in
class 1 is finite, let N be the sample size where the final point in class 1 is
observed. Since f nðxj1Þ is bounded after the last point, ð1  Epn½cÞf nðxj1Þ
converges to ð1  cÞf Nðxj1Þ ¼ 0, and the OBC again converges pointwise
(almost surely) to a Bayes classifier. The same argument holds if c ¼ 0.
▪
Theorem 4.3 does not apply to our Gaussian models with unknown
covariances because, at any point x, there exists a class-conditional
distribution that is arbitrarily large at x. Thus, even though there is weak
consistency of the posteriors, we may not apply Eq. 2.153. The situation can
be rectified by an additional constraint on the class-conditional distributions.
A sequence of probability measures Pn on a measure space U with Borel
s-algebra B converges weak to a point mass du if and only if R
U f dPn →f ðuÞ
for all bounded continuous functions f on U. A Borel measurable function
f : U →R is almost uniformly (a.u.) integrable relative to fPng`
n¼0 if for all
e . 0 there exists 0 , Me , ` and Ne . 0 such that for all n . Ne,
Z
jf ðuÞj≥Me
jf ðuÞjdPnðuÞ , e:
(4.76)
To proceed, we use the main theorem provided in (Zapała, 2008), which
guarantees that if Pn converges weak to du, then
R
U f dPn →f ðuÞ for any f
that is a.u. integrable relative to fPng`
n¼0.
Theorem 4.4 (Dalton and Dougherty, 2013b). Suppose that the first four
conditions in Theorem 4.3 hold and, in addition,
5. For fixed x and y, the class-conditional distributions considered in the
Bayesian model are a.u. integrable over uy relative to the sequence of
posteriors (almost surely).
Then the OBC (over the space of all classifiers with measurable decision
regions) constructed in Eq. 4.6 converges pointwise (almost surely) to a Bayes
classifier.
198
Chapter 4

Proof. By the main theorem in (Zapała, 2008), EuyjSn½ f uyðxjyÞ →f uyðxjyÞ
(almost surely). Substituting this fact for Eq. 2.153, the proof is exactly the
same as in Theorem 4.3.
▪
If the class-conditional distributions are bounded for a fixed x, then they
are also a.u. integrable for any sequence of weak consistent measures. Thus,
Theorem 4.4 is strictly stronger than Theorem 4.3. The next two lemmas show
that the Gaussian class-conditional distributions in the scaled identity and
general covariance models are a.u. integrable over uy relative to the sequence
of posteriors (almost surely), and therefore the OBC converges to a Bayes
classifier in both models.
Lemma 4.3 (Dalton and Dougherty, 2013b). For fixed x and y, Gaussian class-
conditional densities in a scaled identity covariance model are a.u. integrable
over uy relative to the sequence of posteriors (almost surely).
Proof. We assume that the covariances are independent. The homoscedastic
case is similar. Let 0 , K , ` and ny be a positive integer. We wish to bound
LðKÞ ¼
Z
jf uyðxjyÞj≥ð K
2pÞ
D
2 j f uyðxjyÞjpnyðuyÞduy:
(4.77)
Since f uyðxjyÞ ≤ð2pÞD∕2jSyj1∕2,
LðKÞ ≤
Z
1
K
0
1
ð2pÞ
D
2ðs2yÞ
D
2 ⋅ba
GðaÞ
1
ðs2yÞaþ1 exp

 b
s2y

ds2y
¼
G

a þ D
2

ð2pÞ
D
2b
D
2GðaÞ
Z
1
K
0
baþD
2
G

a þ D
2
 ⋅
1
ðs2yÞaþD
2þ1 exp

 b
s2y

ds2y,
(4.78)
where the hyperparameters a and b depend on ny. The integrand is the PDF
for an inverse-gamma distribution. Thus,
LðKÞ ≤
G

a þ D
2

ð2pÞ
D
2b
D
2GðaÞ
⋅G

a þ D
2 , Kb

G

a þ D
2

¼ G

a þ D
2 , Kb

ð2pÞ
D
2b
D
2GðaÞ
,
(4.79)
Gða, bÞ being the upper incomplete gamma function. For a . 1, B . 1, and
b . ða  1ÞB∕ðB  1Þ (Natalini and Palumbo, 2000),
Gða, bÞ , Bba1eb:
(4.80)
To apply this inequality here, note that a ¼ 0.5½ðny þ k þ D þ 1ÞD  2 is an
increasing affine function of the sample size, so we can find a sample size N
199
Optimal Bayesian Classification

guaranteeing that a þ D∕2 . 1. Furthermore, by the strong law of large
numbers, bSy →s2yID (almost surely). Thus, there exists N1 . N such that for
all ny . N1,
bSy  s2yID

1 , D
4 s2y
(4.81)
almost surely. Likewise, there exists C ∈R and N2 . N1 such that for all
ny . N2,
trðSÞ þ
nyny
ny þ ny
ðbmy  myÞTðbmy  myÞ ≥2C
(4.82)
almost surely. Thus, for ny large enough,
b ¼ 1
2 trðSÞ
≥C þ ny  1
2
tr
bSy
	
¼ C þ ny  1
2
h
trðs2yIDÞ þ tr
bSy  s2yID
	i
≥C þ ny  1
2

Ds2y 
bSy  s2yID

1
	
. C þ ny  1
2
⋅3D
4 s2y:
(4.83)
There exists N0 . N2 such that b . ðny  1ÞDs2y∕4 for all ny . N0. To apply
Eq. 4.80, we require some B . 1 such that b . ða þ D∕2  1ÞB∕½KðB  1Þ.
Both bounds on b are equal when B equals
Bðny, KÞ ¼
ðny  1ÞKDs2y
ðny  1ÞðKDs2y  2DÞ  2ðk þ D þ 3ÞD þ 8 :
(4.84)
Thus, this choice of B guarantees the desired bound on b for fixed ny > N0 and K.
Although this choice of B is always larger than 1 if ny > N3 ¼ (k þ D þ 4) þ
4/D, it only guarantees the desired bound for a specific pair of ny and K. Since
B∕ðB  1Þ is a decreasing function for B . 1, any B larger than Eq. 4.84 for all
ny and K in a given region also guarantees the desired bound for all ny and K in
this region. It can be shown that there exist constants B > 1, N00 > maxðN0, N3Þ,
and K00 > 0 such that for all ny . N00 and K . K00, B . Bðny, KÞ, and this
choice
of
B
guarantees
that
Kb . ða þ D∕2  1ÞB∕ðB  1Þ
for
all
ny > N00 and K > K00. Hence, by Eqs. 4.79 and 4.80,
LðKÞ ≤BðKbÞaþD
21eKb
ð2pÞ
D
2b
D
2GðaÞ
(4.85)
200
Chapter 4

for ny . N00 and K . K00. Rearranging terms a bit, for ny . N00 and K . K00,
LðKÞ ≤BK
D1
2
ð2pÞ
D
2þ1
2 ⋅ð2pÞ
1
2ðKbÞKb1
2eKb
GðKbÞ
⋅
GðKbÞ
ðKbÞKbaGðaÞb
1
2 :
(4.86)
Since Kb . a  1, the second term converges uniformly to 1 as ny →`
for K . K00 (Stirling’s formula). By a property of the G function, GðKbÞ ≤
ðKbÞKbaGðaÞ; thus, the third term is bounded by b1∕2, which is arbitrarily
small for large enough ny by Eq. 4.83. It is straightforward to show that for any
e . 0 there exists Ne . N00 and K00 , Ke , ` such that LðKeÞ , e for all
n . Ne.
▪
Lemma 4.4 (Dalton and Dougherty, 2013b). For fixed x and y, Gaussian class-
conditional densities in a general covariance model are a.u. integrable over uy
relative to the sequence of posteriors (almost surely).
Proof. We assume that the covariances are independent. The homoscedastic
case is similar. Let 0 , K , ` and ny be a positive integer. Define
LðKÞ ¼
Z
jf uyðxjyÞj≥K
D
2 ð2pÞD
2 jf uyðxjyÞjpnyðuyÞduy:
(4.87)
Since f uyðxjyÞ ≤ð2pÞD∕2jSyj1∕2,
LðKÞ ≤
Z
jSyj≤1
KD
jSj
k
2 jSyjkþDþ1
2
ð2pÞ
D
2jSyj
1
22
kD
2 GD

k
2
 etr

 1
2 SS1
y

dSy
¼
GD

kþ1
2

p
D
2jSj
1
2GD

k
2

Z
jSyj≤1
KD
f IWðSy; S, k þ 1ÞdSy
¼
G

kþ1
2

p
D
2jSj
1
2G

kDþ1
2
 Pr

jSyj ≤1
KD

,
(4.88)
where f IW is an inverse-Wishart distribution, and in the last line we view Sy as
an inverse-Wishart distributed random matrix with parameters S and k þ 1
(we suppress the subscript y in the hyperparameters).
By the strong law of large numbers, bSy →Sy and bmy →my (almost
surely). Therefore, S∕ðny  1Þ →Sy (almost surely). It can be shown that for
any sequence of symmetric matrices An that converges to a symmetric positive
definite matrix, there exists some N . 0 such that An is symmetric positive
definite for all n . N. Applied here, there exists N . 0 such that S is
symmetric positive definite for all ny . N (almost surely). Let S ¼ LLH be
the Cholesky decomposition of S, where H is the conjugate transpose. Then
201
Optimal Bayesian Classification

for ny . N, X ¼ LHS1
y L is a Wishart random matrix with parameters
LHðSÞ1L ¼ ID and k þ 1, and
LðKÞ ≤
G

kþ1
2

p
D
2jSj
1
2G

kDþ1
2
 PrðjXj ≥KDjSjÞ:
(4.89)
Further, jXj ≥KDjSj implies that at least one of the diagonal elements in X
must be greater than or equal to KjSj1∕D. Hence,
LðKÞ ≤
Gðkþ1
2 Þ
p
D
2jSj
1
2GðkDþ1
2
Þ
Pr

X 1 ≥KjSj
1
D
	
∪··· ∪

X D ≥KjSj
1
D
		
≤
G

kþ1
2

p
D
2jSj
1
2G

kDþ1
2

X
D
i¼1
Pr

X i ≥KjSj
1
D
	
,
(4.90)
where X i is the ith diagonal element of X. The marginal distributions of the
diagonal elements of Wishart random matrices are well known, and in this
case X i  chi-squaredðk þ 1Þ. Thus,
LðKÞ ≤
G

kþ1
2

D
p
D
2jSj
1
2G

kDþ1
2
 Pr

X i ≥KjSj
1
D
	
≤DG

kþ1
2 , K
2 jSj
1
D
p
D
2jSj
1
2G

kDþ1
2
 :
(4.91)
We use the same bound in Eq. 4.80 to bound the equation above. Recall
that k ¼ ny þ k in the independent covariance model, so it is a simple matter
to find N0 . N such that ðk þ 1Þ∕2 . 1 for all ny . N0. By Minkowski’s
determinant inequality, for ny large enough,
jSj
1
D ≥
ðny  1ÞbSy
1
D ¼ ðny  1Þ
bSy
1
D:
(4.92)
Since jbSyj is a strongly consistent estimator of jSyj, where Sy is the true value
of the covariance, there exists N00 . N0 such that jbSyj ≥jSyj∕2 (almost surely).
Hence, for ny large enough,
jSj
1
D ≥ny  1
2
1
D
jSyj
1
D:
(4.93)
The same procedure used in the previous lemma may be used to find constants
B . 1, N000 . N00, and K000 . 0 such that for all ny . N000 and K . K000 we
have 0.5KjSj1∕D . 0.5ðk  1ÞB∕ðB  1Þ, and thus
202
Chapter 4

LðKÞ ≤
DB

K
2 jSj
1
D
	k1
2 eK
2jSj
1
D
p
D
2jSj
1
2G

kDþ1
2
	
¼ BDK
D1
2
2
D
2p
Dþ1
2
⋅
ð2pÞ
1
2

K
2 jSj
1
D
	K
2jSj
1
D1
2eK
2jSj
1
D
G

K
2 jSj
1
D
	

G

K
2 jSj
1
D
	

K
2 jSj
1
D
	K
2jSj
1
DkDþ1
2
G

kDþ1
2
	
jSj
1
2D
:
(4.94)
To simplify the third fraction, note that GðbÞ ≤bbaGðaÞ, and thus for
ny . N000 and K . K000,
LðKÞ ≤BDK
D1
2
2
D
2p
Dþ1
2
⋅
ð2pÞ
1
2

K
2 jSj
1
D
	K
2jSj
1
D1
2eK
2jSj
1
D
G

K
2 jSj
1
D
	
jSj 1
2D:
(4.95)
Since 0.5KjSj1∕D . 0.5ðk  1Þ, the second fraction converges uniformly to 1
as ny →` over all K . K000 by Stirling’s formula. The third term may be made
arbitrarily small for large enough ny by Eq. 4.93.
▪
4.7 Robustness in the Gaussian Model
We next consider the important issue of robustness to false modeling
assumptions, with emphasis on priors making false assumptions and
possessing varying degrees of information.
4.7.1 Falsely assuming homoscedastic covariances
To observe the effects of falsely assuming a homoscedastic model, assume that
c ¼ 0.5 and that the underlying distributions are fixed Gaussian distributions
with D ¼ 2 features, means m0 ¼ 02 and m1 ¼ 12, and covariances
S0 ¼ 0.5

1
r
r
1

,
(4.96)
S1 ¼ 0.5

1
r
r
1

:
(4.97)
203
Optimal Bayesian Classification

With sample size n ¼ 18 we generate a stratified sample (9 points in each
class) and design LDA, QDA, and several OBCs, each using different
homoscedastic covariance priors of the form n0 ¼ n1 ¼ jD, m0 ¼ m0 ¼ 02,
m1 ¼ m1 ¼ 12, k ¼ 2jD, and S ¼ 0.5ðk  3ÞI2, where j ¼ 0, 1, : : : , 18. All
priors with j . 0 are proper, and each contains a different amount of
information, a larger j being more informative. These priors essentially
assume correct means and variances of the features, but when r ≠0 they
incorrectly assume homoscedastic covariances. For each r and each sample,
the true errors from the known distributions are found exactly for linear
classifiers and using 100,000 test points otherwise. The entire process is
repeated for t ¼ 10,000 samples.
Once this iteration is complete, we evaluate the average true error, which
is illustrated in Fig. 4.8. Note that QDA cannot be trained when r ¼ 1, where
the sample covariance in both classes is always singular. The OBCs with low-
information priors ðj ¼ 0 and j ¼ 1Þ perform close to LDA. Even though they
make the false assumption that the covariances are equal, their performance is
still better than QDA, which does not assume that the covariances are equal,
and which is a good approximation of the OBC with independent covariances
and non-informative priors, up to the correlation r  0.5. This indicates that
the homoscedastic OBC is somewhat robust to unequal covariances.
Furthermore, as information is added to the prior
ðj ¼ 2 and 18Þ the
performance of the OBC actually improves over the whole range of r.
The performance of the OBC with the prior indexed by j ¼ 18 is remarkable,
nearly reaching that of the Bayes classifier when r ¼ 0, and beating QDA for
correlations up to r  0.8, even though it incorrectly assumes homoscedastic
covariances.
0
0.2
0.4
0.6
0.8
1
0
0.05
0.1
0.15
0.2
0.25
average true error
Bayes
LDA
QDA
OBC
j = 0
j = 18
j = 2
j = 1
Figure 4.8
Performance of classification on fixed heteroscedastic Gaussian distributions
over a range of correlation r in class 0 and r in class 1. OBCs are shown with increasing
information priors falsely assuming homoscedastic covariances with an expected correlation
of zero (n ¼ 18, c ¼ 0.5, D ¼ 2). [Reprinted from (Dalton and Dougherty, 2013b).]
204
Chapter 4

4.7.2 Falsely assuming the variance of the features
We now consider a Gaussian model parameterized by a scaling factor in the
covariance (thereby controlling the Bayes error) and apply a prior tuned to
the case where the Bayes error is 0.25 with varying degrees of confidence.
We assume that c ¼ 0.5 is known and that the class-conditional distributions
are fixed Gaussian distributions with D ¼ 2 features, means m0 ¼ 02 and
m1 ¼ 12, and covariances of the form S0 ¼ S1 ¼ s2I2. The parameter s2 is
chosen between about 0.18 and 26.0, corresponding to a Bayes error between
0.05 and 0.45.
With sample size n ¼ 18, we generate a stratified sample and design LDA,
QDA, and several OBCs, each using different priors with increasing
information. All priors correctly assume homoscedastic covariances, with
hyperparameters n0 ¼ n1 ¼ 2j, m0 ¼ m0 ¼ 02, m1 ¼ m1 ¼ 12, k ¼ 4j, and
S ¼ 1.1231ðk  3ÞI2, where j ¼ 0, 1, : : : , 18. Each prior contains a different
amount of information, a larger j being more informative. All priors have
correct information about the means but always assume that E½s2 ¼ 1.1231,
corresponding to a Bayes error of 0.25, whereas in actuality the true value of
s2 varies. An OBC might not be trainable when j ¼ 0 since with k ¼ 0 we
have an improper prior and it is possible that the posterior is also improper.
The true errors are found under the known distributions: exactly for linear
classifiers and approximately with 100,000 test points for nonlinear classifiers.
For each s2, this entire process is repeated for t ¼ 10,000 samples.
The average true error is shown in Fig. 4.9. We only report results for the
OBC with j ¼ 0 if the probability that the classifier cannot be trained is less
than 10%, and the average true error is evaluated only over samples for which
0
0.1
0.2
0.3
0.4
0.5
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
Bayes error
average true error − Bayes error
j = 0
j = 1
j = 2
j = 3
j = 6
j = 12
j = 18
LDA
QDA
OBC
Figure
4.9
Performance
of
classification
on
fixed
homoscedastic
scaled
identity
covariance Gaussian distributions over a range of values for a common variance s2
between all features and both classes (controlling Bayes error). OBCs are shown with
increasing information priors correctly assuming homoscedastic covariances but with an
expected
variance
corresponding
to
a
Bayes
error
of
0.25
(n ¼ 18,
c ¼ 0.5,
D ¼ 2). [Reprinted from (Dalton and Dougherty, 2013b).]
205
Optimal Bayesian Classification

the classifier exists. OBCs are shown in black lines for j ¼ 0, 1, 2, 3, 6, 12, and
18. Even though information is increased in the prior using the wrong
covariance, performance still improves, outperforms LDA and QDA, and
even appears to converge to the Bayes classifier. We conjecture that this
occurs because “classification is easier than density estimation.” To wit, in this
case, changing the covariance of both classes at the same time tends not to
change the optimal classifier very much.
4.7.3 Falsely assuming the mean of a class
We next consider the effect of priors containing misinformation about the
mean of a class, in the sense that the mean of one of the priors does not equal
the mean of the corresponding class-conditional distribution. We assume
known c ¼ 0.5 and that the class-conditional distributions are fixed Gaussian
distributions with D ¼ 2 features, means m0 ¼ 02 and m1 ¼ m ⋅12, and
covariances S0 ¼ S1 ¼ 1.1231 ⋅I2. The parameter m takes on one of nine
values, where mj corresponds to a Bayes error of 0.05j. For example,
m1 ¼ 2.45, m5 ¼ 1.00, and m9 ¼ 0.19, giving a range of Bayes errors between
0.05 and 0.45. The scaling factor 1.1231 in the covariances has been calibrated
to give a Bayes error of 0.25 when m ¼ 1.
With sample size n ¼ 18 we generate a stratified sample, and from each
sample design an LDA, a QDA, and several OBCs using different priors with
increasing information. All priors correctly assume homoscedastic covar-
iances and apply hyperparameters n0 ¼ n1 ¼ 2j, m0 ¼ 02, m1 ¼ 12, k ¼ 4j,
and S ¼ 1.1231ðk  3ÞI2, where j ¼ 0, 1, : : : , 18, a larger j being more
informative. For each prior, the mean of m0 and the covariances are in fact the
true values, but the mean of m1 is incorrectly assumed to be 12, corresponding
to a Bayes error of 0.25. The true errors are found under the known
distributions: exactly for linear classifiers and approximately with 100,000 test
points for nonlinear classifiers. For each mj, this entire process is repeated for
t ¼ 10,000 samples.
The average true error is shown in Fig. 4.10. OBCs are shown in black
lines for j ¼ 0, 1, 2, 3, 6, 12, and 18. For a Bayes error of 0.25, the performance
of the OBC improves as we increase the amount of correct information in the
prior, with performance appearing to converge to that of the Bayes classifier.
That being said, performance on the left and right sides of this graph exhibits
quite different behavior. On the right edge, the actual Bayes error is higher
than 0.25 for the means corresponding to each prior, a situation in which the
means are closer together than suggested by the priors. In this range, adding
information to the prior appears to improve performance, which is far better
than LDA or QDA, even with low-information priors. On the left edge, where
the actual Bayes error is lower than that corresponding to the means assumed
in each prior, we see performance degrade as misinformation is added to the
priors, although as we observe more data these kinds of bad priors will
206
Chapter 4

eventually be overcome. At least in this example, the OBC is quite robust
when the assumed Bayes error is lower than reality, that is, when it assumes
that the means of the classes are farther apart than they should be, but
sensitive to cases where the assumed Bayes error is higher than in reality.
We next perform a similar experiment, but this time with a family of
OBCs that assume priors with the same amount of information but targeting
different values for the means. The setup is the same as in the previous
experiment, except that we examine priors that correctly assume homosce-
dastic covariances, with n0 ¼ n1 ¼ 18, m0 ¼ m0 ¼ 02, m1 ¼ mj ⋅12, k ¼ 36,
and S ¼ 1.1231ðk  3ÞI2, where j ¼ 1, : : : , 9. Each prior now contains the
same moderate amount of information and has correct information about m0
and the covariances, but assumes that m ¼ mj or, equivalently, that the Bayes
error is 0.05j, whereas in actuality the true value of m can be mi for some i ≠j
(we use i to index the true mean and j to index the expected mean in the prior).
The average true error for each classifier for different values of mi,
corresponding to different Bayes errors, is provided in Fig. 4.11. OBCs are
shown in black lines for j ¼ 1, 2, 3, 4, 5, 6, and 9. For each prior, indexed by j,
as expected, the best performance is around 0.05j; however, at a fixed Bayes
error of 0.05i, the best prior may not necessarily correspond to j ¼ i; that is,
the best prior may not necessarily be the one that uses the exact mean for class
1 in place of the hyperparameter m1. For instance, on the far right at Bayes
error 0.45, corresponding to i ¼ 9, the OBC with prior j ¼ 9 decreases
monotonically in the range shown until it reaches a point where it performs
about 0.01 worse than the Bayes classifier. At the same time, the OBC
corresponding to j ¼ 6 (for Bayes error 0.3) actually performs better than all
other classifiers shown over the range of Bayes error between 0.30 and 0.45,
0
0.1
0.2
0.3
0.4
0.5
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
Bayes error
average true error − Bayes error
j=12
j=18
LDA
QDA
OBC
j=6
j=3
j=2
j=1
j=0
Figure 4.10
Performance of classification on fixed homoscedastic scaled identity
covariance Gaussian distributions over a range of class-1 means (controlling Bayes error).
OBCs are shown with increasing information priors correctly assuming homoscedastic
covariances but with an expected class-1 mean corresponding to a Bayes error of 0.25
(n ¼ 18, c ¼ 0.5, D ¼ 2). [Reprinted from (Dalton and Dougherty, 2013b).]
207
Optimal Bayesian Classification

where it performs only about 0.005 above the Bayes classifier. This may, at
first, appear to contradict the theory, but recall that the Bayesian framework
does not guarantee optimal performance for any fixed distribution, even if it is
the expected distribution in the prior. It is only optimal when averaged over
all distributions.
4.7.4 Falsely assuming Gaussianity under Johnson distributions
Utilizing Johnson distributions, we now address the robustness of OBCs
using Gaussian models when applied to non-Gaussian distributions. The
simulations assume that c ¼ 0.5 and that the class-conditional distributions
contain D ¼ 2 features with means m0 ¼ 02 and m1 ¼ 12, and covariances
S0 ¼ 0.5 ⋅I2 and S1 ¼ 1.5 ⋅I2. For class 0, the features are i.i.d. Johnson
distributions with skewness between 3 and 3 in steps of 0.5, and kurtosis
between 0 and 10 in steps of 0.5. For a fixed class-0 skewness and kurtosis
pair, we first determine if the pair is impossible or the type of Johnson
distribution from Fig. 2.13, and then find values of g0 and d0 producing the
desired skewness and kurtosis. With these known we may find values of h0
and l0 producing the desired mean and variance. Both features in class 0 are
thus modeled by a Johnson distribution with parameters g0, d0, h0, and l0.
For class 1, essentially the parameters are set such that both classes are skewed
either away from (less overlapping mass) or toward (more overlapping mass)
each other. Features are again i.i.d., having the same kurtosis but opposite
(negative) skewness relative to those in class 0, that is, g1 ¼ g0 and d1 ¼ d0.
The type of Johnson distribution used is always the same as that used in class
0, and the values of h1 and l1 producing the desired mean and variance for
class 1 are then calculated using the same method as for class 0.
0
0.1
0.2
0.3
0.4
0.5
0
0.02
0.04
0.06
0.08
0.1
j = 1
j = 2
j = 3
j = 4
j = 5
j = 6
j = 9
Bayes error
average true error − Bayes error
LDA
QDA
OBC
Figure 4.11
Performance of classification on fixed homoscedastic scaled identity
covariance Gaussian distributions over a range of class-1 means (controlling Bayes error).
OBCs are shown with priors correctly assuming homoscedastic covariances and a range of
expected class-1 means (n ¼ 18, c ¼ 0.5, D ¼ 2). [Reprinted from (Dalton and Dougherty,
2013b).]
208
Chapter 4

The sample has n ¼ 18 points and is stratified with 9 points in each class.
LDA and QDA classifiers are trained, along with two OBCs, each using
independent general covariance priors. The first prior is non-informative, that
is, n0 ¼ n1 ¼ 0, k0 ¼ k1 ¼ 0, and S is an all-zero matrix. The second is a
proper prior with hyperparameters n0 ¼ n1 ¼ k0 ¼ k1 ¼ 6, m0 ¼ m0 ¼ 02,
m1 ¼ m1 ¼ 12, and Sy ¼ ðky  3ÞSy. In a few cases with extreme skewness and
kurtosis, LDA, QDA, and the OBC with non-informative priors may not be
trainable. In such cases the current sample is ignored in the analysis for the
untrainable classifiers (but not for all classifiers). OBCs with proper priors are
always trainable. True errors are found under the appropriate given Johnson
distributions using 100,000 test points. For each skewness and kurtosis pair,
this entire process is repeated for t ¼ 10,000 samples to find the average true
error for each classifier.
As a baseline, for the Gaussian case (skewness 0 and kurtosis 3), the
average true classifier errors are 0.249 for LDA, 0.243 for QDA, 0.240 for
OBCs with non-informative priors, and 0.211 for OBCs with informative
priors. Even with non-informative priors, the OBC beats LDA and QDA,
although this is not guaranteed in all settings. As expected for the Gaussian
case, the performance of OBCs improves significantly by adding a small
amount of information to the priors.
Simulation results over the skewness/kurtosis plane are shown in Fig. 4.12.
Each subplot shows performance for LDA, QDA, and one of the OBCs over
essentially the same skewness/kurtosis plane shown in Fig. 2.13, but now
having two sides to distinguish between positive and negative skewness in
class 0 (distributions have more or less overlapping mass, respectively). The
log-normal line and the boundary for the impossible region are also shown.
Each marker represents a specific Johnson distribution with the corresponding
skewness and kurtosis. A white circle, white square, or black circle at a point
means that LDA, QDA, or OBC has the lowest expected true error among
the three classifiers, respectively. Similar graphs may be used to design a
−5
0
5
(b)
(a)
2
4
6
8
10
kurtosis
LDA is best
QDA is best
OBC is best
impossible
region
−skewness2
−skewness2
skewness2
skewness2
SU
SB
−5
0
5
2
4
6
8
10
kurtosis
LDA is best
QDA is best
OBC is best
impossible
region
SU
SB
Figure 4.12
Performance under Johnson distributions (c ¼ 0.5, D ¼ 2, n ¼ 18): (a) non-
informative prior; (b) informative prior. [Reprinted from (Dalton and Dougherty, 2013b).]
209
Optimal Bayesian Classification

hypothesis test to determine a region where it is “safe” to make Gaussian
modeling assumptions. Performance in Fig. 4.12 depends on the simulation
settings; for instance, in some cases the OBC does not outperform LDA or
QDA, even on Gaussian distributions.
The OBC with non-informative priors appears to be very robust to
kurtosis given that most points near zero skewness are represented by black
dots in Fig. 4.12(a). There is some robustness to skewness, although the left
half of the plane (distributions having less overlap) is dominated by LDA and
the right half (distributions having more overlap) is mostly covered by QDA.
Although it is not immediately evident in the figure, the OBC with this non-
informative prior actually has performance very close to that of QDA, which
is consistent with our previous theoretical and empirical findings. So in this
scenario, LDA tends to perform better when distributions have less overlap,
QDA tends to perform better when they have more overlap, and the OBC
with non-informative priors has best performance for very small skewness and
tracks closely with QDA throughout, so it also has very good performance
when the distributions have more overlap. Perhaps more interestingly, as a
small amount of information is added to the priors, even though they assume
a Gaussian model, the region where the OBC outperforms both LDA and
QDA does not shrink but actually expands.
4.8 Intrinsically Bayesian Robust Classifiers
An IBR classifier is defined similarly to an OBC, except that p is replaced by
p in Eq. 4.4:
cIBR ¼ arg min
c∈C Ep½εðu, cÞ:
(4.98)
A model-constrained Bayesian robust (MCBR) classifier cMCBR is defined in
the same manner as an IBR classifier, except that C is replaced by CU, which is
the family of classifiers in C that are optimal for some u ∈U.
In the absence of data the OBC reduces to the IBR classifier relative to the
prior, and in the presence of data the OBC is the IBR classifier relative to the
posterior distribution. Whether uncertainty is relative to the prior or posterior
distribution, Eu½εðu, cÞ is the Bayesian MMSE error estimator. The effective
densities address the problem of finding an IBR classifier: simply apply the
effective densities with p instead of p. In particular, when the prior and
posterior possess the same form (as in the Gaussian model), we obtain an
IBR classifier by simply replacing the posterior distribution by the prior
distribution in our solutions.
Given that an IBR classifier can be obtained from the effective densities
relative to the prior, we are in a position to compare the MCBR and IBR
classifiers. In particular, are there cases when the two are equivalent and,
210
Chapter 4

when they are not, what is the difference between them? First, let us highlight
some cases in which they are the same.
In a Gaussian model with independent scaled identity or general
covariances, the set of model-specific optimal classifiers is the set of all
quadratic classifiers. If k0 ¼ k1, then the IBR classifier is quadratic as in
Eq. 4.11 and determined by Eqs. 4.30 through 4.32, for instance, in the
general covariance model with k0 ¼ k1 in the prior. If we assume known
unequal covariances between the classes, then the set of state-specific optimal
classifiers is the set of all quadratic classifiers with discriminant of the form
gðxÞ ¼ xTAx þ aTx þ b, with A ∝S1
1
 S1
0 , and the IBR classifier for this
model has the same form if n0 ¼ n1; if n0 ≠n1, then the IBR classifier is still
quadratic, but it is generally not among the set of optimal classifiers for all
states because the matrix AOBC in the discriminant will typically not have the
same form as A.
With homoscedastic scaled identity or general covariances, the set of
optimal classifiers is the set of all linear classifiers, and the IBR classifier is
also linear if n0 ¼ n1 and Ep½c ¼ 0.5. Finally, if we assume known equal
covariances between the classes, then the set of optimal classifiers is also the
set of all linear classifiers, and the IBR classifier is linear if n0 ¼ n1.
While in the preceding cases the MCBR and IBR classifiers are
equivalent, in most other cases this is not true. We compare performance in
a case where the IBR classifier is not in the family of state-optimal classifiers
and actually performs strictly better than the MCBR classifier. Consider a
synthetic Gaussian model with D ¼ 2 features, independent general covar-
iances, and a proper prior defined by known c ¼ 0.5 and hyperparameters
n0 ¼ k0 ¼ 20D, m0 ¼ 0D, n1 ¼ k1 ¼ 2D, m1 ¼ 1D, and Sy ¼ ðky  D  1ÞID.
Since k0 ≠k1, we have k0 ≠k1 and the IBR classifier is polynomial but
generally not quadratic. Based on our previous analysis of this model, it is
given by cIBRðxÞ ¼ 0 if gIBRðxÞ ≤0 and cIBRðxÞ ¼ 1 if gIBRðxÞ . 0, where
gIBRðxÞ ¼ K

1 þ 1
k0
ðx  m0ÞTC1
0 ðx  m0Þ
k0þD


1 þ 1
k1
ðx  m1ÞTC1
1 ðx  m1Þ
k1þD
,
(4.99)
ky ¼ ky  D þ 1, Cy ¼ ½ðny þ 1Þ∕ðkynyÞSy, and
K ¼
1  c
c
2k0
k1
D jC0j
jC1j
2
64
G

k0
2
	
G

k1þD
2
	
G

k0þD
2
	
G

k1
2
	
3
75
2
:
(4.100)
On the other hand, the Bayes classifier for any particular state is quadratic.
Using Monte Carlo methods we find the corresponding MCBR classifier
211
Optimal Bayesian Classification

cMCBR. We also consider a plug-in classifier cplug-in using the expected value
of each parameter, which is the Bayes classifier assuming that c ¼ 0.5,
m0 ¼ m0, m1 ¼ m1, and S0 ¼ S1 ¼ ID. This plug-in classifier is linear. The
average true errors are Eu½εðu, cplug-inÞ ¼ 0.2078, Eu½εðu, cMCBRÞ ¼ 0.2061,
and Eu½εðu, cIBRÞ ¼ 0.2007. The IBR classifier is superior to the MCBR
classifier. Figure 4.13 shows cIBR, cMCBR, and cplug-in. Level curves for the
distributions corresponding to the expected parameters, which have been
found by setting the Mahalanobis distance to 1, are shown in thin gray lines.
cIBR is the only classifier that is not quadratic, and it is quite distinct
from cMCBR.
4.9 Missing Values
Missing values are commonplace in many applications, especially when
feature values result from complex technology. Numerous methods have been
developed to impute missing values in the framework of standard classifica-
tion rules. OBCs are naturally suited for handling missing values. Once the
mechanism of missing data is modeled using an uncertainty class of
distributions, the effective class-conditional density can be adjusted to reflect
this mechanism. Analytic solution may be possible, such as for a Gaussian
model with independent features; however, typically, Markov chain Monte
Carlo (MCMC) methods must be utilized. Throughout this section, following
(Dadaneh et al., 2018a), we consider Gaussian feature-label distributions.
Although the OBC framework possesses the capacity to incorporate the
probabilistic modeling of arbitrary types of missing data, we assume that data
are missing completely at random (MCAR) (Little and Rubin, 2014). In this
−2
−1
0
1
2
3
−2
−1
0
1
2
3
x1
x2
MCBR
IBR
plug-in
Figure 4.13
Classifiers for an independent general covariance Gaussian model with D ¼ 2
features and proper priors having k0 ≠k1. The intrinsically Bayesian robust classifier is
polynomial with average true error 0.2007, whereas the state-constrained Bayesian robust
classifier is quadratic with average true error 0.2061. [Reprinted from (Dalton and
Dougherty, 2013b).]
212
Chapter 4

scenario, the parameters of the missingness mechanism are independent of
other model parameters. Thus, they vanish when taking the expectation in the
calculation of the effective class-conditional density so we can ignore
parameters related to the missing-data mechanism.
For each class y ∈f0, 1g, we assume a Gaussian distribution with
parameters uy ¼ ðmy, lyÞ, where my is the mean of the class-conditional
distribution, and ly is a collection of parameters that determines the
covariance matrix Sy of the class y. Except when needed, we drop the class
index y.
We first consider a general covariance model. Let class y be fixed.
Assuming n independent sample points x1, x2, : : : , xn under class y, each
possessing D-dimensional Gaussian distribution N ðm, SÞ, partition the
observations into G ≤n groups, where all ng points in group g ∈f1, : : : , Gg
have the same set Jg of observed features, with cardinality jJgj ¼ Dg. Let
Ig denote the set of sample point indices in group g, and represent the pattern
of missing data in group g by a Dg  D matrix Mg, where each row is a
D-dimensional vector with a single nonzero element with value 1 correspond-
ing to the index of an observed feature. The nonmissing portion of sample
point xi in group g is Mgxi and has Gaussian distribution N ðMgm, MgSMTg Þ.
We define the group-g statistics by
mg ¼ 1
ng
X
i∈Ig
Mgxi,
(4.101)
Sg ¼
X
i∈Ig
ðMgxi  mgÞðMgxi  mgÞT,
(4.102)
where mg and Sg are the sample mean and scatter matrix, respectively, when
employing only the observed data in group g. Let X be the portion of the full
dataset ½x1, x2, : : : , xn that is not missing. Given y and the corresponding
matrices Mg, and assuming that the sample points are independent, we can
write the data likelihood as
f ðXjm, SÞ ∝
Y
G
g¼1
jSgj
ng
2 etr

 1
2 SgS1
g

 exp

 1
2
X
G
g¼1
ngðmg  MgmÞTS1
g ðmg  MgmÞ

,
(4.103)
where Sg ¼ MgSMTg is the covariance matrix corresponding to group g.
Suppose that we use Gaussian and inverse-Wishart priors for m and S:
213
Optimal Bayesian Classification

m  N ðm, S∕nÞ and S  inverse-WishartðS, kÞ, where n . 0, k . D  1,
S is a symmetric positive definite matrix, and the PDF of the inverse-Wishart
prior is proportional to Eq. 2.59. This is generally not a conjugate prior, and
MCMC methods should be used to proceed with the analysis, as we will
discuss shortly.
Now consider independent features, where we can derive a closed-form
representation for the OBC with missing values. Let class y be fixed. Let Ik be
the set of sample point indices under class y for which feature k is not missing
and let jIkj ¼ lk. In a Gaussian model with a diagonal covariance matrix,
feature k ∈f1, : : : , Dg is distributed as N ðmk, s2
kÞ. Hence, the likelihood can
be expressed as
f ðXjm1, : : : , mD, s2
1, : : : , s2
DÞ
∝
Y
D
k¼1
ðs2
kÞ
lk
2 exp

 1
2s2
k

lkðmk  xkÞ2 þ
X
i∈Ik
ðxki  xkÞ2

,
(4.104)
where xk is the sample mean of feature k including only nonmissing values,
and xki is the value of the kth feature in sample point i in Ik. We shall use a
conjugate normal-inverse-gamma prior, where the parameters in class 0 are
independent from those of class 1, the ðmk, s2
kÞ pairs are independent, and
mkjs2
k  N ðmk, s2
k∕nkÞ,
s2
k  inverse-gammaðrk, ckÞ:
(4.105)
Theorem 4.5 (Dadaneh et al., 2018a). In the diagonal covariance model,
assuming a normal-inverse-gamma prior for feature k ∈f1, : : : , Dg with
nk . 0, rk . 0, and ck . 0, the posterior distributions are given by
mkjX, s2
k  N

hk,
s2
k
lk þ nk

,
(4.106)
s2
kjX  inverse-gamma

rk þ lk
2 , ck þ bk

,
(4.107)
where
hk ¼ lkxk þ nkmk
lk þ nk
,
(4.108)
bk ¼ 1
2
X
i∈Ik
ðxki  xkÞ2

þ 1
2
lknk
lk þ nk
ðxk  mkÞ2:
(4.109)
214
Chapter 4

Proof. The PDF of an inverse-gammaðr, cÞ random variable is given by
f ðxÞ ∝xr1 exp

 c
x

:
(4.110)
Therefore, the normal-inverse-gamma prior in Eq. 4.105 can be expressed as
pðmk, s2
kÞ ∝ðs2
kÞrk3
2 exp

 nkðmk  mkÞ2 þ 2ck
2s2
k

:
(4.111)
Combining this prior with the likelihood in Eq. 4.104 yields the posterior
density:
pðmk, s2
kÞ ∝ðs2
kÞrk
lk3
2
 exp

 nkðmk  mkÞ2 þ 2ck þ lkðmk  xkÞ2 þ P
i∈Ik ðxki  xkÞ2
2s2
k

:
(4.112)
To calculate mkjX, s2
k, complete the square on
nkðmk  mkÞ2 þ lkðmk  xkÞ2
(4.113)
to obtain (omitting the constant term)
ðnk þ lkÞ

mk  lkxk þ nkmk
lk þ nk
2
(4.114)
and hence the Gaussian distribution in Eq. 4.106. The posterior of s2
k in
Eq. 4.107 is obtained by marginalizing out mk in Eq. 4.112.
▪
Theorem 4.6 (Dadaneh et al., 2018a). Under the conditions of Theorem 4.5, the
effective class-conditional density is given by
f UðxjyÞ ¼
Y
D
k¼1
ð2pÞ1
2

lk þ nk
lk þ nk þ 1
1
2 G

rk þ lk
2 þ 1
2
	
G

rk þ lk
2
	

ðck þ bkÞrkþ
lk
2
h
ck þ bk þ 1
2
lkþnk
lkþnkþ1 ðxk  hkÞ2irkþ
lk
2 þ1
2
,
(4.115)
where xk is the kth feature in x.
215
Optimal Bayesian Classification

Proof. Let y be fixed. Marginalizing out mk using the conditional posterior in
Eq. 4.106 yields
f ðxkjs2
kÞ ¼
Z `
`
f ðxkjmk, s2
kÞpðmkjs2
kÞdmk
¼
Z `
`
ð2ps2
kÞ1
2 exp

 1
2s2
k
ðxk  mkÞ2



2p
s2
k
lk þ nk
1
2
exp

 lk þ nk
2s2
k
ðmk  hkÞ2

dmk
¼ ð2ps2
kÞ1
2

lk þ nk
lk þ nk þ 1
1
2 exp

 1
2s2
k
ðlk þ nkÞðxk  hkÞ2
lk þ nk þ 1

:
(4.116)
Using the posterior of s2
k in Eq. 4.107, the effective class-conditional density
for xk is
f ðxkjyÞ ¼
Z `
0
f ðxkjs2
kÞ ðck þ bkÞrkþ
lk
2
G

rk þ lk
2
	 ðs2
kÞrk
lk
2 1 exp

 ck þ bk
s2
k

ds2
k
¼ ð2pÞ1
2

lk þ nk
lk þ nk þ 1
1
2 ðck þ bkÞrkþ
lk
2
Gðrk þ lk
2Þ
Z `
0
ðs2
kÞrk
lk
2 3
2
 exp

 1
s2
k

ck þ bk þ 1
2
ðlk þ nkÞðxk  hkÞ2
lk þ nk þ 1

ds2
k
¼ ð2pÞ1
2

lk þ nk
lk þ nk þ 1
1
2 G

rk þ lk
2 þ 1
2
	
G

rk þ lk
2
	

ðck þ bkÞrkþ
lk
2
h
ck þ bk þ 1
2
lkþnk
lkþnkþ1 ðxk  hkÞ2irkþ
lk
2 þ1
2
,
(4.117)
which completes the proof.
▪
The OBC is obtained from the preceding theorem and Eq. 4.6. If the test
point is missing data, the OBC is obtained by omitting the features missing in
the test point completely from the analysis. The hyperparameters nk, mk, rk,
and ck should be specified for each feature k and each class, and hk and bk are
found from Eqs. 4.108 and 4.109, respectively. Lemma 2.2 and Theorem 2.8
are special cases of Theorems 4.5 and 4.6, respectively, where there is no
missing data, and nk and rk do not depend on k.
216
Chapter 4

For general covariance Gaussian models and general missing-data
mechanisms, we do not have a closed-form posterior. Thus, MCMC inference
of model parameters is employed, and then an OBC decision rule is obtained
via Monte Carlo approximation. For inference of covariance matrices in high-
dimensional settings, traditional random walk MCMC methods such as
Metropolis–Hastings suffer major limitations, for instance, high rejection rate.
Thus, a Hamiltonian Monte Carlo OBC (OBC-HMC) is obtained for the
corresponding OBC with missing values by employing the Hamiltonian Monte
Carlo (HMC) method (Neal, 2011). HMC has the ability to make large
changes to the system state while keeping the rejection rate small. We defer to
(Dadaneh et al., 2018a) for the details.
Example 4.4. First we compare OBC performance with missing-value
imputation in the Gaussian model when features are independent. Feature
dimensionality is D ¼ 10. Sample size is n0 ¼ n1 ¼ 12, where feature
k ∈f1, : : : , 10g
values
for
class
y ∈f0, 1g
are
drawn
independently
from N ðmk,y, s2
k,yÞ. The mean and variance parameters are generated
according to a normal-inverse-gamma model: mk,y  N ð0, s2
k,y∕nyÞ and
s2
k,y  inverse-gammaðry, cyÞ, where r0 ¼ r1 ¼ 101, c0 ¼ 60, and c1 ¼ 45.
This choice of hyperparameters avoids severe violation of the LDA-classifier
assumption that the two classes possess equal variances. The parameter ny is
used to adjust the difficulty of classification. We consider Bayes errors 0.05
and 0.15. To obtain an average Bayes error of 0.05, n0 ¼ 3 and n1 ¼ 1. To
obtain 0.15, n0 ¼ 12 and n1 ¼ 2.6. To approximate Bayes errors, in each run
of the simulation, 1000 test points are generated with the classes being
equiprobable. Missing-value percentages vary between 0% and 50%.
The simulation setup is repeated 10,000 times, and in each iteration
classifier accuracy is the proportion of correctly classified test points.
Figure 4.14 shows the average accuracy of different classifiers applied to
the test data for two levels of Bayes error. In addition to the OBC with
missing data, standard SVM, LDA, and QDA classifiers are applied
subsequent to a missing-data imputation scheme proposed in (Dadaneh
et al., 2018a) based on Gibbs sampling. The OBC with missing data is
theoretically optimal in this figure and indeed outperforms the imputation-
based methods.
To simulate data from general covariance Gaussian models, the training
data are generated with n0 ¼ n1 ¼ 12 sample points in each class, with
dimension D ¼ 10, drawn from N ðmy, SyÞ, where a normal-inverse-Wishart
prior
is
placed
on
the
parameters
of
this
Gaussian
distribution
as
my  N ðmy, Sy∕nyÞ and Sy  inverse-Wishartðk, SÞ. The hyperparameters
are m0 ¼ 0D, m1 ¼ m1 ⋅1D, n0 ¼ 6D, n1 ¼ D, and S ¼ 0.3ðk  D  1ÞID,
where m1 is a scalar. To assess the performance of the OBC-HMC, the
217
Optimal Bayesian Classification

hyperparameters are adjusted to yield different Bayes errors, the first setting
m1 ¼ 0.38 and k ¼ 3D for Bayes error 0.05, and the second setting m1 ¼ 0.28
and k ¼ 15D for Bayes error 0.15. 10,000 simulations are performed, and in
each simulation experiment, classifier accuracies are measured on 1000 test
points. For OBC-HMC and Gibbs-sampling-imputation implementation, see
(Dadaneh et al., 2018a). Figures 4.15(a) and (b) show the performance of
OBC-HMC compared to SVM, LDA, and QDA following imputation, as
well as the ordinary OBC trained on the complete data without missing
values. Missing-value rates are between 5% and 50%, and are simulated
similarly to the independent-feature case.
OBC
LDA-Gibbs
average accuracy
missing-value percentage
(a)
(b)
0
0.75
0.8
0.85
0.9
0.95
1
average accuracy
missing-value percentage
0.65
0.7
0.75
0.8
0.85
0.9
5
10
15
20
25
35
50
0
5
10
15
20
25
35
50
SVM-Gibbs
QDA-Gibbs
Bayes
Figure 4.14
Accuracy of various classifiers versus missing-data percentage in a setting
with independent features and two Bayes error rates: (a) Bayes error 0.05; (b) Bayes error
0.15. The dotted line indicates the Bayes classifier accuracy as an upper bound
for
classification
accuracy,
which
is
calculated
using
the
true
values
of
model
parameters. [Reprinted from (Dadaneh et al., 2018a).]
OBC
LDA-Gibbs
OBC-HMC
average accuracy
missing-value percentage
(a)
(b)
0.6
0.7
0.8
0.9
1
average accuracy
missing-value percentage
0.5
0.6
0.7
0.8
0.9
5
10
15
20
25
35
50
5
10
15
20
25
35
50
SVM-Gibbs
QDA-Gibbs
Bayes
Figure 4.15
Accuracy of various classifiers versus missing-value percentage in a setting
with general covariance structures and under two Bayes error rates: (a) Bayes error 0.05;
(b) Bayes error 0.15. OBC is trained by the complete data without missing values. OBC-
HMC integrates missing-value imputation into OBC derivation by marginalization. [Reprinted
from (Dadaneh et al., 2018a).]
218
Chapter 4

As reported in (Dadaneh et al., 2018a), several other imputation methods
were tried, but the small sample size and high missing-value rates in this
simulation setup proved too challenging to achieve reasonable prediction
accuracy.
4.9.1 Computation for application
Missing values lead to the need for MCMC computation. In fact the need for
such computational methods is common in applications because, except in
special circumstances, one does not have a conjugate prior leading to closed-
form posteriors. Here, we briefly mention some examples in the literature.
In (Knight et al., 2014), a hierarchical multivariate Poisson model is used
to represent RNA-Seq measurements, and model uncertainty leads to an
MCMC-based OBC for classification. Using an RNA-Seq dataset from
The Cancer Genome Atlas (TCGA), the paper develops a classifier for tumor
type that discriminates between lung adenocarcinoma and lung squamous cell
carcinoma. The same basic setup is used in (Knight et al., 2018), where the
OBC and Bayesian MMSE error estimator are used to examine gene sets that
can discriminate between experimentally generated phenotypes, the ultimate
purpose being to detect multivariate gene interactions in RNA-Seq data.
In
(Banerjee
and
Braga-Neto,
2017),
the
OBC
is
employed
for
classification of proteomic profiles generated by liquid chromatography-mass
spectrometry (LC-MS). The method is likelihood-free, utilizing approximate
Bayesian computation (ABC) implemented via MCMC. A similar computa-
tional approach for the OBC is used in (Nagaraja and Braga-Neto, 2018) for a
model-based OBC for classification of data obtained via selected reaction
monitoring-mass spectrometry (SRM-MS). Also, recall that for the OBC with
missing values we employed the Hamiltonian Monte Carlo method.
On account of the technology, standard gene-expression-based phenotype
classification using RNA-Seq measurements utilizes no timing information,
and measurements are averages across collections of cells. In (Karbalayghareh
et al., 2018a), IBR classification is considered in the context of gene regulatory
models under the assumption that single-cell measurements are sampled at a
sufficient rate to detect regulatory timing. Thus, observations are expression
trajectories. In effect, classification is performed on data generated by an
underlying gene regulatory network. This approach is extended to optimal
Bayesian classification in (Hajiramezanali et al., 2019), in which recursive
estimation, in the form of the Boolean Kalman filter, is employed, for which
the paper uses particle filtering (Imani and Braga-Neto, 2018).
4.10 Optimal Sampling
In the context of optimal Bayesian classification, sample data are collected
to update the prior distribution to a posterior via the likelihood function
219
Optimal Bayesian Classification

(Eq. 2.17). Thus far, we have focused on random sampling, where a fixed
number of points are drawn from the feature-label distribution, and separate
sampling, where a fixed number of points are drawn from each class-
conditional distribution. These methods can be inefficient if, for instance,
uncertainty in one of the class prior distributions is contributing more to
performance loss due to uncertainty than the other. In the extreme case,
consider the situation in which the class-0 feature-label distribution is known
and there is uncertainty only with regard to the class-1 feature-label
distribution. If the application allows for samples to be drawn sequentially,
and in particular if the final sample size can be variable, this can be alleviated
to some extent using censored sampling, as we have discussed previously. As
an alternative approach, in this section we consider optimal sampling in the
framework of optimal experimental design grounded on objective-based
uncertainty. This framework
was first formulated
in the context
of
intrinsically Bayesian robust operators (Dehghannasiri et al., 2015) and was
later generalized into a framework not restricted to operator design (Boluki
et al., 2019). Optimal sampling fits directly into the general approach, which
we describe next.
4.10.1 MOCU-based optimal experimental design
We assume a probability space U with probability measure p, a set C, and a
function C : U  C →½0, `Þ, where U, p, C, and C are called the uncertainty
class, prior distribution, action space, and cost function, respectively. Elements
of U and C are called uncertainty parameters and actions, respectively. For
any
u ∈U,
an
optimal
action
is
an
element
cu ∈C
such
that
Cðu, cuÞ ≤Cðu, cÞ for any c ∈C. An intrinsically Bayesian robust (IBR)
action is an element cU
IBR ∈C such that Eu½Cðu, cU
IBRÞ ≤Eu½Cðu, cÞ for any
c ∈C.
Whereas cU
IBR is optimal over U, for u ∈U, cu is optimal relative to u.
The objective cost of uncertainty is the performance loss owing to the
application of cU
IBR instead of cu on u: Cðu, cU
IBRÞ  Cðu, cuÞ. Averaging this
cost over U gives the mean objective cost of uncertainty (MOCU):
MCðUÞ ¼ Eu
h
C

u, cU
IBR
	
 Cðu, cuÞ
i
:
(4.118)
The action space is arbitrary so long as the cost function is defined on U  C.
Suppose that there is a set Ξ, called the experiment space, whose elements
j, called experiments, are random variables. One j is chosen from Ξ based on
some procedure; in a slight abuse of notation we also use j to denote the
outcome of an experiment. In some applications, the experiment selection
procedure itself may influence the distribution of u. Here we assume that,
while u is jointly distributed with the experiment, the marginal distribution of
220
Chapter 4

u does not depend on which experiment j is selected; thus, Ej½pðu, jÞ ¼ pðuÞ
for all j ∈Ξ. Given j ∈Ξ, the conditional distribution pðujjÞ is the posterior
distribution relative to j, and Ujj denotes the corresponding probability space,
called the conditional uncertainty class. Relative to Ujj, we define IBR actions
cUjj
IBR and the remaining MOCU,
MCðUjjÞ ¼ Eujj
h
C

u, cUjj
IBR
	
 Cðu, cuÞ
i
,
(4.119)
where the expectation is with respect to pðujjÞ. Since the experiment j is a
random variable, MCðUjjÞ is a random variable. Taking the expectation over
j gives the expected remaining MOCU,
DCðU, jÞ ¼ Ej
h
Eujj
h
C

u, cUjj
IBR
	
 Cðu, cuÞ
ii
,
(4.120)
which is called the experimental design value. DCðU, jÞ is a constant indexed
by the experiment j in the experiment space. An optimal experiment j ∈Ξ
minimizes DCðU, jÞ:
j ¼ arg min
j∈Ξ DCðU, jÞ:
(4.121)
Evaluating the preceding equation gives
j ¼ arg min
j∈Ξ Ej
h
Eujj
h
C

u, cUjj
IBR
	ii
,
(4.122)
the latter double expectation being the residual IBR cost, which is denoted by
RCðU, jÞ. Hence,
j ¼ arg min
j∈Ξ RCðU, jÞ:
(4.123)
Proceeding iteratively results in sequential experimental design. This can be
done in a greedy manner (greedy-MOCU), meaning that at each step we select
the best experiment without looking ahead to further experiments. We stop
when we reach a fixed budget of experiments, or, if the final number of
experiments can be variable, when some other stopping condition (such as the
MOCU, or the difference in MOCU between experiments) reaches some
specified threshold. An optimal policy, i.e., an optimal strategy for selecting
experiments, can also be found using dynamic programming over a finite
horizon of experiments (DP-MOCU) (Imani et al., 2018). Because DP-MOCU
reduces MOCU at the end of the horizon, whereas greedy-MOCU takes only
the next step into account, DP-MOCU achieves the lowest cost at the end of the
horizon; however, it typically requires much greater computation time.
In its original formulation (Yoon et al., 2013), MOCU depends on a class
of operators applied to a parameterized model in which u is a random vector
221
Optimal Bayesian Classification

whose distribution depends on a characterization of the uncertainty, as in
Section 4.1. In that setting, U is an uncertainty class of system models
parameterized by a vector u governed by a probability distribution pðuÞ, and
C is a class of operators on the models whose performances are measured by
the cost Cðu, cÞ of applying c on model u ∈U.
Lindley has proposed a general framework for Bayesian experimental
design (Lindley, 1972). The general MOCU-based design fits within this
framework. Moreover, knowledge gradient (Frazier et al., 2008) and efficient
global optimization (Jones et al., 1998) are specific implementations of
MOCU-based experimental design under their modeling assumptions; see
(Boluki et al., 2019). MOCU-based optimal experimental design has been
used in several settings, including structural intervention in gene regulatory
networks (Dehghannasiri et al., 2015; Mohsenizadeh et al., 2018; Imani and
Braga-Neto, 2018), canonical expansions (Dehghannasiri et al., 2017b), and
materials discovery (Dehghannasiri et al., 2017c).
4.10.2 MOCU-based optimal sampling
The cost function for MOCU-based optimal sampling for classification is
classification error. Beginning with the prior and an IBR classifier, we
determine the optimal experiment to obtain a sample point, derive the OBC
from the posterior, treat this OBC as a new IBR classifier and the posterior as
a new prior, and then repeat the procedure iteratively to generate a sample
and final OBC. The following scenario is discussed in (Dougherty, 2018). It
yields a sequential sampling procedure when applied iteratively. Here we
restrict our attention to greedy-MOCU.
Suppose that we can specify which class to sample, where, given the class,
sampling is random. We assume known class-0 prior probability c and
consider the multinomial model with Dirichlet priors possessing hyperpara-
meter vector ½ay
1, : : : , ay
b for class y. The priors for classes 0 and 1 are given in
Eqs. 2.43 and 2.44, respectively. The experiment space is fh0, h1g, where hy
selects the feature value of a sample from class y, which for the multinomial
distribution is the bin number. For h ∈fh0, h1g, cUjh
IBR ¼ cOBCjh is the IBR
classifier for the posterior distribution pðujhÞ, or, equivalently, it is the
OBC given h, Cðu, · Þ is the classification error for state u, and the residual
IBR cost is
RCðU, hÞ ¼ Eh

EpðujhÞ½εðu, cOBCjhÞ

¼ Eh½bεðh, cOBCjhÞ
¼ Eh
"X
b
j¼1
min

c
U0
j þ a0
j
n0 þ Pb
i¼1 a0
i
, ð1  cÞ
U1
j þ a1
j
n1 þ Pb
i¼1 a1
i
#
,
(4.124)
222
Chapter 4

where the second equality follows because the inner expectation is the
Bayesian MMSE error estimate of the OBC error, and the third equality
applies Eq. 4.8. Uy
j and ny inside the last expectation are implicitly functions
of h. For selecting a single data point from class 0, Eq. 4.124 reduces to
RCðUjh0Þ ¼ Eh0
"X
b
j¼1
min

c
Ih0¼j þ a0
j
1 þ a0
0
, ð1  cÞ
a1
j
a1
0
#
¼
X
b
i¼1
Prðh0 ¼ iÞ
X
b
j¼1
min

c
Ii¼j þ a0
j
1 þ a0
0
, ð1  cÞ
a1
j
a1
0

¼
X
b
i¼1
a0
i
a0
0
min

c 1 þ a0
i
1 þ a0
0
, ð1  cÞ a1
i
a1
0

þ a0
0  a0
i
a0
0
min

c
a0
i
1 þ a0
0
, ð1  cÞ a1
i
a1
0

,
(4.125)
where ay
0 ¼ Pb
i¼1 ay
i , and Prðh0 ¼ iÞ ¼ a0
i ∕a0
0 is the effective density of class 0
under the prior. An analogous expression gives RCðUjh1Þ. An optimal
experiment is determined by minðRCðUjh0Þ, RCðUjh1ÞÞ. We have that
h ¼ hz, where
z ¼ arg min
y∈f0,1g
X
b
i¼1
bpy
i dyþ
i
þ ð1  bpy
i Þdy
i :
(4.126)
Given class y, bpy
i ¼ ay
i ∕ay
0 is the probability that the new point is in bin i,
and dyþ
i
is the error contributed by bin i if the new point is in bin i;
in particular, d0þ
i
¼ minðcbp0þ
i , ð1  cÞbp1
i Þ, d1þ
i
¼ minðcbp0
i , ð1  cÞbp1þ
i Þ, and
bpyþ
i
¼ ð1 þ ay
i Þ∕ð1 þ ay
0Þ. Likewise, dy
i
is the error contributed by bin i if
the new point is not in bin i; in particular, d0
i
¼ minðcbp0
i , ð1  cÞbp1
i Þ,
d1
i
¼ minðcbp0
i , ð1  cÞbp1
i Þ, and bpy
i
¼ ay
i ∕ð1 þ ay
0Þ. When this procedure is
iterated, in each epoch the hyperparameters ay
i should be updated with the
data from all past experiments. If it is impossible to change the OBC classifier
by observing a single point [cbp0þ
i
, ð1  cÞbp1
i and cbp0
i , ð1  cÞbp1
i
for all bin i
where OBC outputs class 0, and ð1  cÞbp1þ
i
, cbp0
i and ð1  cÞbp1
i , cbp0
i
for all
bin i where OBC outputs class 1], it can be shown that the argument of the
minimum in Eq. 4.126 is equal for both classes; ties may be broken with
random sampling to ensure convergence to the Bayes error.
A similar sampling procedure is applied in (Broumand et al., 2015) that is
not MOCU-based.
223
Optimal Bayesian Classification

Example 4.5. Consider a discrete classification problem with b ¼ 8 bins in
which c ¼ 0.5 is known and fixed, ½ p1, : : : , pb is drawn from a prior with low
uncertainty given by a0
j ∝1∕j with Pb
j¼1 a0
j ¼ 100, and ½q1, : : : , qb is drawn
from a prior with high uncertainty given by a1
j ∝1∕ðb  j þ 1Þ with
Pb
j¼1 a1
j ¼ 1. A budget of 20 samples is available, where the label of each
sample can be requested sequentially, and given the labels each sample is
drawn independently.
Figure 4.16(a) presents the average of several classifier errors with respect
to the number of samples drawn over 100,000 randomly drawn bin
probabilities. For all classifiers, ties are broken by randomly selecting from
the labels with equal probability. The Bayes error is constant at 0.104 with
respect to sample size. The average error of a discrete histogram rule with
random sampling decreases, as expected, but at 20 points it only reaches
0.159. The average error of OBC with “correct” priors (the same priors used
to generate the true bin probabilities) and random sampling has less than half
the average error of the discrete histogram rule after observing just one point
(0.206 versus 0.418), then it continues to decrease until it reaches 0.117 after
observing 20 points. Finally, OBC with correct priors and optimal (one-step-
look-ahead) sampling performs even better, with an average error of 0.180
after one point and 0.112 after 20 points. The optimal one-step-look-ahead
tends to draw more points from class 1, since class 1 starts with more
uncertainty than class 0.
This experiment is repeated in Fig. 4.16(b) with c ¼ 0.1, b ¼ 64 bins, both
classes being drawn from priors having the same amount of uncertainty, given
by a0
j ∝1∕j and a1
j ∝1∕ðb  j þ 1Þ with Pb
j¼1 ay
j ¼ 1 for both y ¼ 0, 1.
Random sampling draws points from class 0 with probability c and ties in
all classifiers that are still broken by selecting labels with equal probability.
The difference between optimal and random is greater in part (a).
5
samples drawn
(a)
(b)
0
0.1
0.2
0.3
0.4
0.5
average error
Bayes error
DHR, random sample
OBC, random sample
OBC, optimal one-step-look-ahead
10
20
30
40
50
samples drawn
0
0.05
0.1
0.15
0.2
0.25
0.3
average error
Bayes error
DHR, random sample
OBC, random sample
OBC, optimal one-step-look-ahead
10
15
20
Figure 4.16
Performance of optimal sampling: (a) unequal initial uncertainty between
classes; (b) equal initial uncertainty between classes.
224
Chapter 4

We close this section by noting that other online learning approaches have
been proposed, for example, methods based on sequential measurements
aimed at model improvement, such as knowledge gradient and active
sampling (Cohn et al., 1994). Active sampling aims to control the selection
of potential unlabeled training points in the sample space to be labeled
and used for training. A generic active sampling algorithm is described in
(Saar-Tsechansky and Provost, 2004), and a Bayesian approach is considered
in (Golovin et al., 2010).
4.11 OBC for Autoregressive Dependent Sampling
In the preceding section, sample points are dependent because the sampling
procedure is guided by evidence from previous samples. One may consider
other forms of dependency in the sampling mechanism. For instance, sample
points can be modeled as the output of a stochastic process (Tubbs, 1980;
Lawoko and McLachlan, 1983, 1986; McLachlan, 2004; Zollanvari et al.,
2013). In this section, we derive the OBC for data generated by an
autoregressive process.
Historically, the performance of LDA with dependent observations was
studied
in
(Lawoko
and
McLachlan,
1986).
A
univariate
stationary
autoregressive (AR) model of order p was used to model the intraclass
correlation among observations. In (Lawoko and McLachlan, 1985), the same
authors examined the performance of the classifier obtained by replacing the
optimal parameters in the Bayes classifier by their maximum-likelihood
estimators obtained under the same AR model. The performance of LDA
under a general stochastic setting was considered in (Zollanvari et al., 2013),
special cases being when observations are generated from AR or moving-
average (MA) signal models of order 1.
Here we construct the OBC under two conditions: (1) the training
observations for class y are generated from VARyðpÞ, which denotes a multi-
dimensional vector autoregressive process of order p; and (2) there exists
uncertainty about parameters governing the VARyðpÞ model (Zollanvari and
Dougherty, 2019). We assume that model parameters (coefficient matrices)
are random with a prior distribution.
4.11.1 Prior and posterior distributions for VAR processes
Let xy
1, : : : , xy
ny be length q column vectors for ny training observations under
class y ¼ 0, 1, which are drawn from a q-dimensional (column) VARyðpÞ
vector autoregressive process of order p (Lutkepohl, 2005). In particular,
xy
t ¼ cy þ
X
p
i¼1
Ay
i xy
ti þ uy
t ,
(4.127)
225
Optimal Bayesian Classification

where
uy
t
is
q-dimensional
Gaussian
white
noise
with
E½uy
t  ¼ 0q,
E½uy
t ðuy
t ÞT ¼ Sy invertible, E½uy
t ðuy
sÞT ¼ 0qq for s ≠t, Ay
i is a q  q matrix
of model parameters, cy is the vector of q intercept terms, and, to ease
notation, the order p of the VAR processes is assumed to be the same for both
classes. Equivalently,
Xy ¼ AyZy þ Uy,
(4.128)
where
Xy ¼ ½xy
1, : : : , xy
ny
and
Uy ¼ ½uy
1, : : : , uy
ny
are
q  ny-dimensional
matrices, Ay ¼ ½cy, Ay
1, : : : , Ay
p is a q  ð1 þ pqÞ matrix of model para-
meters,
and
Zy ¼ ½zy
1, : : : , zy
ny
is
a
ð1 þ pqÞ  ny
matrix;
note
that
zy
t ¼ ½1, ðxy
t1ÞT, : : : , ðxy
tpÞTT and assume that xy
0 ¼ xy
1 ¼ ··· ¼ xy
1p ¼ 0q.
From the chain rule of probability, the likelihood function f ðXyjAyÞ can
be expressed as (Lütkepohl, 2005, p. 223; Kilian and Lütkepohl, 1977, p. 152)
f ðXyjAyÞ ¼
Y
ny
t¼1
f

xy
t jxy
t1, : : : , xy
tp, Ay
	
∝jSyj
ny
2 exp

 1
2
X
ny
t¼1
ðxy
t  Ayzy
t ÞTS1
y ðxy
t  Ayzy
t Þ

∝exp

 1
2 tr

ðXy  AyZyÞTS1
y ðXy  AyZyÞIny
	
¼ exp

 1
2 ½xy  ðZTy ⊗IqÞayTðIny ⊗S1
y Þ½xy  ðZTy ⊗IqÞay

¼ exp

 1
2 ðwy  WyayÞTðwy  WyayÞ

:
(4.129)
The second to last equality of Eq. 4.129 follows from the definitions
xy ¼ vecðXyÞ and ay ¼ vecðAyÞ, and the identities
trðATBCDÞ ¼ vecðAÞTðDT ⊗BÞ vecðCÞ,
(4.130)
vecðABÞ ¼ ðBT ⊗IrÞ vecðAÞ,
(4.131)
where r is the number of rows in A, vecð⋅Þ denotes the vectorization operator
(stacking matrix columns), and ⊗denotes the Kronecker product. The last
equality follows from the definitions
wy ¼

Iny ⊗S
1
2
y
	
xy,
(4.132)
226
Chapter 4

Wy ¼ ZTy ⊗S
1
2
y ,
(4.133)
the identity ðIr ⊗AÞ1∕2 ¼ Ir ⊗A1∕2, and the mixed-product property of the
Kronecker product. Let
Fy ¼ ðWTy WyÞ1 ¼ ðZyZTy ⊗S1
y Þ1,
(4.134)
my ¼ FyðZy ⊗S1
y Þxy:
(4.135)
Writing ay ¼ ay þ my  my and noting that wTy Wy ¼ mT
y WTy Wy yields
f ðXyjAyÞ
∝exp

 1
2 ½ðay  myÞTF1
y ðay  myÞ þ ðwy  WymyÞTðwy  WymyÞ

:
(4.136)
The prior distribution of ay is assumed to be a multivariate Gaussian with
known ð1 þ pqÞq-dimensional mean vector my and ð1 þ pqÞq  ð1 þ pqÞq-
dimensional invertible covariance matrix Cy:
pðayÞ ∝jCyj1
2 exp

 1
2 ðay  myÞTC1
y ðay  myÞ

:
(4.137)
The next theorem provides the posterior distribution. It appears as in
(Zollanvari and Dougherty, 2019), but is essentially the same as in
(Lütkepohl, 2005).
Theorem 4.7 (Lütkepohl, 2005). For the prior distribution of Eq. 4.137 and the
VARyðpÞ model of Eq. 4.128, the posterior distribution is Gaussian and given by
pðayÞ ¼
1
ð2pÞ
ð1þpqÞq
2
jCyj
1
2
exp

 1
2 ðay  myÞTC1
y ðay  myÞ

,
(4.138)
where
my ¼ CyðC1
y my þ F1
y myÞ,
(4.139)
Cy ¼ ðC1
y
þ F1
y Þ1:
(4.140)
227
Optimal Bayesian Classification

Proof. The posterior is defined by
pðayÞ ¼
pðayÞf ðXyjAyÞ
R
Rð1þpqÞq pðayÞf ðXyjAyÞday
:
(4.141)
Using Eqs. 4.136 and 4.137, and after some algebraic manipulations [e.g., see
(Lütkepohl, 2005), p. 223)], we obtain
pðayÞf ðXyjAyÞ ∝exp

 1
2 ðay  myÞTC1
y ðay  myÞ

 exp

 1
2 ðwy  WymyÞTðwy  WymyÞ

,
(4.142)
where
wy ¼
"
C
1
2
y my
wy
#
,
(4.143)
Wy ¼
"
C
1
2
y
Wy
#
:
(4.144)
Replacing Eq. 4.142 in Eq. 4.141, and noting that the second exponential term
in Eq. 4.142 does not depend on ay, we obtain Eq. 4.138.
▪
4.11.2 OBC for VAR processes
In constructing the OBC we assume that a future q-dimensional sample point
x is independent from all training observations, as in (Lawoko and
McLachlan, 1985). We also assume that f uyðxjyÞ  N ðcy, SyÞ, which is the
marginal distribution of the training point xy
1.
Theorem 4.8 (Zollanvari and Dougherty, 2019). For the prior distribution
of Eq. 4.137 and the VARyðpÞ model of Eq. 4.128, the effective density is
given by
f UðxjyÞ ¼
jC
̮
yj
1
2
ð2pÞ
q
2jSyj
1
2jCyj
1
2 ⋅
exp

 1
2 ð˜x  myÞTC1
y ð˜x  myÞ
	
exp

 1
2 ð˜x  m
̮
yðxÞÞTC
̮
1
y ð˜x  m
̮
yðxÞÞ
	 ,
(4.145)
228
Chapter 4

where ˜x ¼ vecð ˜XÞ, ˜X ¼ ½x, 0qpq, my and Cy are defined in Eqs. 4.139 and
4.140, respectively,
m
̮
yðxÞ ¼ C
̮
y
h
C1
y my þ ðD ⊗S1
y Þ˜x
i
,
(4.146)
C
̮
y ¼

C1
y
þ D ⊗S1
y
	1,
(4.147)
D is the ð1 þ pqÞ  ð1 þ pqÞ matrix D ¼ e1eT
1 , and e1 ¼ ½1, 0, : : : , 0T.
Proof. Using f uyðxjyÞ  N ðcy, SyÞ together with Eq. 4.138 yields the effective
density
f UðxjyÞ ¼
Z
Rð1þpqÞq
1
ð2pÞ
q
2jSyj
1
2ð2pÞ
ð1þpqÞq
2
jCyj
1
2
 exp

 1
2 ðx  cyÞTS1
y ðx  cyÞ

 exp

 1
2 ðay  myÞTC1
y ðay  myÞ

day:
(4.148)
The exponent of the first exponential inside the integral can be expressed as
ðx  cyÞTS1
y ðx  cyÞ ¼ ð˜x  ayÞTðD ⊗S1
y Þð˜x  ayÞ:
(4.149)
Then using Eqs. 4.146, 4.147, and 4.149, we can combine and rewrite the
exponents of both exponentials in Eq. 4.148 as
ðx  cyÞTS1
y ðx  cyÞ þ ðay  myÞTC1
y ðay  myÞ
¼ ˜xTðD ⊗S1
y Þ˜x þ mTy C1
y my
þ aTy C
̮
1
y ay  2aTy C
̮
1
y m
̮
y
¼ ˜xTðD ⊗S1
y Þ˜x þ mTy C1
y my
 m
̮ Ty C
̮
1
y m
̮
y þ ðay  m
̮
yÞTC
̮
1
y ðay  m
̮
yÞ,
(4.150)
where we have suppressed the dependency on x in the notation for m
̮
y.
Substituting this into Eq. 4.148 yields
229
Optimal Bayesian Classification

f UðxjyÞ
¼
exp

 1
2
h
˜xTðD ⊗S1
y Þ˜x þ mTy C1
y my  m
̮ Ty C
̮
1
y m
̮
y
i	
ð2pÞ
q
2jSyj
1
2jCyj
1
2jC
̮
yj1
2

Z
Rð1þpqÞq
exp

 1
2 ðay  m
̮
yÞTC
̮
1
y ðay  m
̮
yÞ
	
ð2pÞ
ð1þpqÞq
2
jC
̮
yj
1
2
day
¼
exp

 1
2
h
˜xTðD ⊗S1
y Þ˜x þ mTy C1
y my  m
̮ Ty C
̮
1
y m
̮
y
i	
ð2pÞ
q
2jSyj
1
2jCyj
1
2jC
̮
yj1
2
,
(4.151)
which reduces to Eq. 4.145.
▪
Under the assumption that the prior probability cy of class y is known for
y ¼ 0, 1 (previously we used c for class 0 and 1  c for class 1), applying
Theorem 4.1 and taking the logarithm in Eq. 4.145 gives the OBC:
cVAR
OBCðxÞ ¼

0
if g0ðxÞ ≥g1ðxÞ,
1
otherwise,
(4.152)
where
gyðxÞ ¼ 2 lnðcyÞ þ ln
jC
̮
yC1
y j
jSyj

þ D2ð˜x, m
̮
y, C
̮
yÞ  D2ð˜x, my, CyÞ,
(4.153)
and
Dðx, m, CÞ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ðx  mÞTC1ðx  mÞ
q
(4.154)
is the Mahalanobis distance of vector x from a distribution with the mean m
and covariance C.
For independent sampling, the observations are generated from VARyð0Þ
processes for y ¼ 0, 1 ðp ¼ 0Þ. Note that p ¼ 0 implies that Zy ¼ 1Tny and
˜x ¼ x. If we assume that Cy ¼ Sy∕ny for ny . 0, then my becomes the sample
mean of class y, my becomes the posterior mean of the mean (previously called
my), the effective density in Eq. 4.145 reduces to Eq. 2.96, and the preceding
OBC reduces to that of Eqs. 4.11 through 4.14.
For a second special case, consider the improper non-informative prior
pðayÞ ¼ b . 0. We require a matrix lemma given in (Zollanvari and
Dougherty, 2019).
230
Chapter 4

Lemma 4.5 (Zollanvari and Dougherty, 2019). Let A be an invertible r  r
matrix and D ¼ e1eT
1 . Then
D½Ir  ðA þ DÞ1D ¼ kD,
(4.155)
ðA þ DÞ1D ¼ kA1D,
(4.156)
A1D½Ir  ðA þ DÞ1e1 ¼ ðA þ DÞ1e1,
(4.157)
jðA þ DÞ1j
jA1j
¼ k,
(4.158)
where k ¼ ð1 þ eT
1 A1e1Þ1.
Proof. Right and left multiplication of a matrix by D nullifies all elements
except for those in the first column and row, respectively. Hence,
D½Ir  ðA þ DÞ1D ¼ D½Ir  ðA þ DÞ1ðA þ D  AÞD
¼ DðA þ DÞ1AD
¼ DðIr þ A1DÞ1D
¼ kD:
(4.159)
Therefore,
A1D½Ir  ðA þ DÞ1D ¼ kA1D:
(4.160)
From the Sherman–Morrison inversion formula,
ðA þ DÞ1D ¼ ðA1  A1DðIr þ A1DÞ1A1ÞD
¼ A1D½Ir  ðA þ DÞ1D
¼ kA1D,
(4.161)
which completes the proof of Eqs. 4.155 through 4.157. Furthermore, by the
matrix determinant lemma, we have
jðA þ DÞ1j
jA1j
¼ jA1jk
jA1j
¼ k,
(4.162)
which completes the proof of Eq. 4.158.
▪
Using the prior pðayÞ ¼ b in Eq. 4.141, the posterior has a Gaussian form
similar to Eq. 4.138, where
231
Optimal Bayesian Classification

my ¼ my ¼ ½ðZyZTy Þ1Zy ⊗Iqxy
¼ vec

XyZTy ðZyZTy Þ1	
,
(4.163)
Cy ¼ Fy ¼ ðZyZTy Þ1 ⊗Sy,
(4.164)
my and Fy are defined in Eq. 4.135 and Eq. 4.134, respectively, and we have
used
the
fact
that
for
any
s  r
matrix
A
and
q  r
matrix
B,
ðA ⊗IqÞ vecðBÞ ¼ vecðBATÞ. We also assume that ny . 1 þ pq to have the
inverse ðZyZTy Þ1. Then
m
̮
y ¼
h
ðZyZTy þ DÞ1Zy ⊗Iq
i
xy þ
h
ðZyZTy þ DÞ1D ⊗Iq
i
˜x
¼ vec

ðXyZTy þ ˜XÞðZyZTy þ DÞ1	
,
(4.165)
C
̮
y ¼ ðZyZTy þ DÞ1 ⊗Sy:
(4.166)
Substituting Eqs. 4.163 through 4.166 in Eq. 4.153 and applying the fact that
vecðXÞTðA ⊗BÞ vecðXÞ ¼ trðATXTBXÞ
(4.167)
for any q  r matrix X, r  r matrix A, and q  q matrix B, yields
D2ð˜x, m
̮
y, C
̮
yÞ  D2ð˜x, my, CyÞ
¼ tr

ðZyZTy þ DÞð ˜X  M
̮
yÞTS1
y ð ˜X  M
̮
yÞ
 ZyZTy ð ˜X  MyÞTS1
y ð ˜X  MyÞ
	
,
(4.168)
where
My ¼ XyZTy ðZyZTy Þ1,
(4.169)
M
̮
y ¼ ðXyZTy þ ˜XÞðZyZTy þ DÞ1:
(4.170)
Since
ð ˜X  M
̮
yÞðZyZTy þ DÞ ¼ ð ˜X  MyÞZyZTy ,
(4.171)
we have that
D2ð˜x, m
̮
y, C
̮
yÞ  D2ð˜x, my, CyÞ ¼ tr

ZyZTy ðM
̮
y  MyÞTS1
y ð ˜X  MyÞ
	
:
(4.172)
232
Chapter 4

By Eq. 4.171, we also have that
ðM
̮
y  MyÞZyZTy ¼ ð ˜X  M
̮
yÞD:
(4.173)
Thus,
D2ð˜x, m
̮
y, C
̮
yÞ  D2ð˜x, my, CyÞ ¼  tr

Dð ˜X  M
̮
yÞTS1
y ð ˜X  MyÞ
	
:
(4.174)
Using
the
fact
that
˜X ¼ ˜XD
and
Lemma
4.5,
and
the
fact
that
trðDAÞ ¼ eT
1 Ae1 for any matrix A of the appropriate size, we have that
D2ð˜x, m
̮
y, C
̮
yÞ  D2ð˜x, my, CyÞ ¼  tr

kyDð ˜X  MyÞTS1
y ð ˜X  MyÞ
	
¼ kyeT
1 ð ˜X  MyÞTS1
y ð ˜X  MyÞe1
¼ kyðx  gyÞTS1
y ðx  gyÞ,
(4.175)
where
ky ¼ ½1 þ eT
1 ðZyZTy Þ1e11,
(4.176)
gy ¼ Mye1 ¼ XyZTy ðZyZTy Þ1e1:
(4.177)
Thus,
cVAR
OBCðxÞ ¼

0
if xTHx þ hTx þ b ≥0,
1
otherwise,
(4.178)
where
H ¼ 1
2 ðk1S1
1
 k0S1
0 Þ,
(4.179)
h ¼ k0S1
0 g0  k1S1
1 g1,
(4.180)
b ¼ 1
2

k1g1S1
1 g1  k0g0S1
0 g0

þ ln
c0
c1
k0
k1
q
2jS1j
jS0j
1
2
:
(4.181)
The classifier in Eq. 4.178 has a quadratic form similar to Eq. 4.11 except that
vy∕ðvy þ 1Þ and my are replaced by ky and gy, respectively.
Once again, we can assume that observations are independent, having
been generated from VARyð0Þ processes for y ¼ 0, 1. Then, Zy ¼ 1Tny and,
therefore, ky ¼ ny∕ðny þ 1Þ and gy is the sample mean of class y. Using these
in Eqs. 4.179 through 4.181 yields the OBC given in Eqs. 4.11 through 4.14,
233
Optimal Bayesian Classification

which is similar to QDA. Assuming that ky ¼ ny∕ðny þ 1Þ  1 leads to QDA
with known covariance matrix. Further assuming that S ¼ S0 ¼ S1 leads to
LDA with known covariance. In sum, the LDA and QDA classifiers are
essentially special cases of the OBC for autoregressive processes under the
assumptions of sample independence and a non-informative prior [except for
a negligible Bayesian calibration factor ny∕ðny þ 1Þ].
Example 4.6. Assume that c0 ¼ c1, n0 ¼ n1 ∈½20, 100, q ¼ 3, p ¼ 2, c0 ¼ 0q,
c1 ¼ 1q, S0 ¼ S1 ¼ Iq,
A0
1 ¼
2
4
0.8
0
0
0.2
0.4
0.2
0
0
0.5
3
5,
(4.182)
A1
1 ¼
2
4
0.5
0.1
0.1
0
0.6
0
0
0
0.4
3
5,
(4.183)
A0
2 ¼
2
4
0.9
0
0
0
0.1
0
0.2
0.2
0
3
5,
(4.184)
A1
2 ¼
2
4
0.4
0.1
0
0.2
0.1
0
0
0
0.5
3
5:
(4.185)
The parameters Ay
i have been chosen to make the processes stable. The
process VARyðpÞ is stable when all roots of the reverse characteristic
polynomial lie outside the unit circle. To estimate the error of each classifier,
we generate 1000 independent observations. The procedure of generating
training observations and independent test observations is repeated 500 times
to estimate the expected error rate of each classifier for each ny. We consider
three classifiers: (1) cNI
OBCðxÞ, which is the OBC with a non-informative prior;
(2) cSP
OBCðxÞ, which is the OBC constructed from a “strong prior” using the
general results in Eqs. 4.152 through 4.154, in which the the prior means
my are set using parameters of Ay
i , namely, my ¼ ay ¼ vecðAyÞ, where
Ay ¼ ½cy, Ay
1, : : : , Ay
p, and the covariance matrix of the prior is an identity
matrix; and (3) cLDAðxÞ, which is the LDA classifier, and which approximates
the OBC from a non-informative prior and independent observations.
Figure 4.17 shows the performances of the three classifiers as functions of
the sample size n0 ¼ n1.
234
Chapter 4

20
sample size
0.2
0.21
0.22
0.23
0.24
0.25
0.26
0.27
average error
30
40
50
60
70
80
90
100
Figure 4.17
Performances of three classifiers with respect to sample size.
235
Optimal Bayesian Classification

Chapter 5
Optimal Bayesian Risk-based
Multi-class Classification
In this chapter we consider classification under multiple classes and allow
for different types of error to be associated with different levels of risk or loss.
A few classical classification algorithms naturally permit multiple classes
and arbitrary loss functions; for example, a plug-in rule takes the functional
form for an optimal Bayes decision rule under a given modeling assumption
and substitutes sample estimates of model parameters in place of the true
parameters. This can be done with LDA and QDA for multiple classes with
arbitrary loss functions, which essentially assume that the underlying class-
conditional densities are Gaussian with equal or unequal covariances,
respectively. Most training-data error estimation methods, for instance, cross-
validation, can also be generalized to handle multiple classes and arbitrary loss
functions. However, it is expected that the same difficulties encountered under
binary classes with simple zero-one loss functions (where the expected risk
reduces to the probability of misclassification) will carry over to the more
general setting, as they have in ROC curve estimation (Hanczar et al., 2010).
Support vector machines are inherently binary but can be adapted to
incorporate penalties that influence risk by implementing slack terms or
applying a shrinkage or robustifying objective function (Xu et al., 2009a,b). It is
also common to construct multi-class classifiers from binary classifiers using the
popular one-versus-all or all-versus-all strategies (Bishop, 2006). The former
method builds several binary classifiers by discriminating one class, in turn,
against all others, and at a given test point reports the class corresponding to the
highest classification score. The latter discriminates between each combination
of pairs of classes and reports a majority vote. However, it is unclear how one
may assess the precise effect of these adaptations on the expected risk.
Here we generalize the Bayesian MMSE error estimator, sample-conditioned
MSE, and OBC to treat multiple classes with arbitrary loss functions. We will
present the analogous concepts of the Bayesian risk estimator, sample-conditioned
MSE for risk estimators, and optimal Bayesian risk classifier. We will show that
237

the Bayesian risk estimator and optimal Bayesian risk classifier can be represented
in the same form as the expected risk and Bayes decision rule with unknown true
densities replaced by effective densities. This approach is distinct from the simple
plug-in rule since the form of the effective densities may not be the same as the
individual densities represented in the uncertainty class.
5.1 Bayes Decision Theory
Consider a classification problem in which the aim is to assign one of M
classes y ∈f0, : : : , M  1g to samples drawn from feature space X. Let f ðyjcÞ
be the probability mass function of Y, parameterized by a vector c, and for
each y let f ðxjy, uyÞ be the class-y-conditional density of X, parameterized by a
vector uy. The full feature-label distribution is parameterized by c and
u ¼ ðu0, : : : , uM1Þ.
Let Lði, yÞ be a loss function quantifying a penalty in predicting label i
when the true label is y. The conditional risk in predicting label i for a given
point x is defined as
Rði, x, c, uÞ ¼ E½Lði, YÞjx, c, u
¼
X
M1
y¼0
Lði, yÞf ðyjx, c, uÞ
¼
PM1
y¼0 Lði, yÞf ðxjy, c, uÞf ðyjc, uÞ
f ðxjc, uÞ
¼
PM1
y¼0 Lði, yÞf ðyjcÞf ðxjy, uyÞ
PM1
y¼0 f ðyjcÞf ðxjy, uyÞ
:
(5.1)
In expectations we denote conditioning on the event X ¼ x by simply
conditioning on x. Although c and u are fixed, here we denote them as
conditioned variables too, since they will be assigned priors later in the
Bayesian
analysis.
The
expected
risk
of
a
given
classification
rule
c : X →f0, : : : , M  1g is given by
Rðc, c, uÞ ¼ E½RðcðXÞ, X, c, uÞjc, u
¼
Z
X
RðcðxÞ, x, c, uÞf ðxjc, uÞdx
¼
X
M1
y¼0
X
M1
i¼0
Lði, yÞf ðyjcÞ
Z
Ri
f ðxjy, uyÞdx
¼
X
M1
y¼0
X
M1
i¼0
Lði, yÞf ðyjcÞεi,y
n ðc, uyÞ,
(5.2)
238
Chapter 5

where the classification probability
εi,y
n ðc, uyÞ ¼
Z
Ri
f ðxjy, uyÞdx
¼ PrðX ∈Rijy, uyÞ
(5.3)
is the probability that a class-y point will be assigned to class i by the classifier
c, the Ri ¼ fx ∈X : cðxÞ ¼ ig partition the feature space into decision
regions, and the third equality follows from the third equality in Eq. 5.1.
A Bayes decision rule (BDR) minimizes expected risk, or equivalently, the
conditional risk at each fixed point x:
cBDRðxÞ ¼ arg
min
i∈f0, : : : , M1gRði, x, c, uÞ
¼ arg
min
i∈f0, : : : , M1g
X
M1
y¼0
Lði, yÞf ðyjcÞf ðxjy, uyÞ,
(5.4)
where the second equality follows from Eq. 5.1, whose denominator is
not a function of i. By convention, we break ties with the lowest index
i ∈f0, : : : , M  1g minimizing Rði, x, c, uÞ.
The zero-one loss function is given by Lði, yÞ ¼ 0 if i ¼ y and Lði, yÞ ¼ 1
if i ≠y. In the binary case with the zero-one loss, the expected risk reduces to
the classification error and the BDR is a Bayes classifier.
5.2 Bayesian Risk Estimation
In
the
multi-class
framework,
we
assume
that
c
is
the
probability
mass function of Y, that is, c ¼ ½c0, : : : , cM1 ∈DM1, where f ðyjcÞ ¼ cy
and
DM1
is
the
standard
ðM  1Þ-simplex
defined
by
cy ∈½0, 1
for
y ∈f0, : : : , M  1g
and
PM1
y¼0 cy ¼ 1.
Also
assume
that
uy ∈Uy
for
some
parameter
space
Uy,
and
u ∈U ¼ U0  ···  UM1.
Let
C ¼ ½C0, : : : , CM1 and T ¼ ðT0, : : : , TM1Þ denote random vectors for
parameters c and u. We assume that C and T are independent prior to
observing data and assign prior probabilities pðcÞ and pðuÞ. Note the change
of notation: up until now, we had let c and u denote both the random
variables and the parameters. The change is being made to avoid confusion
regarding the expectations in this chapter.
Let Sn be a random sample, xy
i the ith sample point in class y, and ny the
number of sample points observed from class y. Given a sample, the priors are
updated to posteriors:
pðc, uÞ ¼ f ðc, ujSnÞ ∝pðcÞpðuÞ
Y
M1
y¼0
Y
ny
i¼1
f ðxy
i , yjc, uyÞ,
(5.5)
239
Optimal Bayesian Risk-based Multi-class Classification

where we have assumed independent sample points, and the product on the
right is the likelihood function. Since
f ðxy
i , yjc, uyÞ ¼ f ðxy
i jy, c, uyÞf ðyjc, uyÞ ¼ cyf ðxy
i jy, uyÞ,
(5.6)
where the last equality applies some independence assumptions, we may write
pðc, uÞ ¼ pðcÞpðuÞ,
(5.7)
where
pðcÞ ¼ f ðcjSnÞ ∝pðcÞ
Y
M1
y¼0
ðcyÞny
(5.8)
and
pðuÞ ¼ f ðujSnÞ ∝pðuÞ
Y
M1
y¼0
Y
ny
i¼1
f ðxy
i jy, uyÞ
(5.9)
are marginal posteriors of C and T. Independence between C and T is
preserved in the posterior. When the prior density is proper, this all follows
from Bayes’ theorem; otherwise, Eqs. 5.8 and 5.9 are taken as definitions,
where we require posteriors to be proper.
Given a Dirichlet prior on C with hyperparameters a ¼ ½a0, : : : , aM1,
under
random
sampling,
the
posterior
on
C
is
still
Dirichlet
with
hyperparameters ay ¼ ay þ ny. Defining
a
þ ¼
X
M1
i¼0
a
i ,
(5.10)
for y ≠z,
E½CyjSn ¼ ay
a
þ
,
(5.11)
E½C2yjSn ¼ ayð1 þ ayÞ
a
þð1 þ a
þÞ ,
(5.12)
E½CyCzjSn ¼
ayaz
a
þð1 þ a
þÞ :
(5.13)
5.3 Optimal Bayesian Risk Classification
A Bayesian risk estimator (BRE) is an MMSE estimate of the expected risk,
or equivalently, the conditional expectation of the expected risk given the
240
Chapter 5

observations. Given a sample Sn and a classifier c that is not informed by u,
owing to posterior independence between C and T, the BRE is given by
bRðc, SnÞ ¼ E½Rðc, C, TÞjSn
¼
X
M1
y¼0
X
M1
i¼0
Lði, yÞE½ f ðyjCÞjSnE½εi,y
n ðc, TyÞjSn:
(5.14)
Keeping in mind that f uyðxjyÞ ¼ f ðxjy, uyÞ, the effective density f UðxjyÞ
for class y is defined in Eq. 2.26, where we assume that ðX, YÞ and Sn are
independent given C and T (once the parameters are known, the sample does
not add information about the test point). We can also consider the effective
density:
f DðyÞ ¼
Z
DM1 f ðyjcÞpðcÞdc:
(5.15)
The effective densities are expressed via expectation by
f DðyÞ ¼ EC½ f ðyjCÞjSn ¼ E½CyjSn ¼ Ep½Cy,
(5.16)
f UðxjyÞ ¼ ETy½ f ðxjy, TyÞjSn:
(5.17)
We may thus write the BRE in Eq. 5.14 as
bRðc, SnÞ ¼
X
M1
y¼0
X
M1
i¼0
Lði, yÞf DðyÞˆεi,y
n ðc, SnÞ,
(5.18)
where
ˆεi,y
n ðc, SnÞ ¼ E½εi,y
n ðc, TyÞjSn:
(5.19)
The latter can be expressed via the effective density f UðxjyÞ as
ˆεi,y
n ðc, SnÞ ¼ E
Z
Ri
f ðxjy, TyÞdx
Sn

¼
Z
Ri
E½ f ðxjy, TyÞjSndx
¼
Z
Ri
f UðxjyÞdx,
(5.20)
where the second equality follows from Fubini’s theorem. Hence,
ˆεi,y
n ðc, SnÞ ¼ PrðX ∈Rijy, SnÞ
(5.21)
241
Optimal Bayesian Risk-based Multi-class Classification

is the posterior probability of assigning a class-y point to class i. Comparing
Eqs. 5.2 and 5.18, observe that f DðyÞ and f UðxjyÞ play roles analogous to
f ðyjcÞ and f ðxjy, uyÞ, respectively, in Bayes decision theory.
Before proceeding, we wish to remark on the various notations being
employed. We will be manipulating expressions involving various densities
and conditional densities, and, as is common, we will generally denote them
by f . For instance, we may write the prior and posterior as pðuÞ ¼ f ðuÞ and
pðuÞ ¼ f ðujSnÞ, respectively. We will also consider f ðyjSnÞ and f ðxjy, SnÞ.
Regarding the former,
f ðyjSnÞ ¼
Z
DM1 f ðy, cjSnÞdc
¼
Z
DM1 f ðyjc, SnÞf ðcjSnÞdc
¼
Z
DM1 f ðyjcÞf ðcjSnÞdc
¼
Z
DM1 f ðyjcÞpðcÞdc
¼ f DðyÞ:
(5.22)
Similarly,
f ðxjy, SnÞ ¼ f UðxjyÞ:
(5.23)
Whereas the
BRE addresses
overall
classifier
performance
across
the entire feature space X, we may also consider classification at a fixed
point x ∈X. This is analogous to the probability of misclassification
PrðY ≠cðxÞjX ¼ x, SnÞ derived in Section 2.3. We define the Bayesian
conditional risk estimator (BCRE) for class i ∈f0, : : : , M  1g at point
x ∈X to be the MMSE estimate of the conditional risk for label i given the
sample Sn and the current test point X ¼ x:
bRði, x, SnÞ ¼ E½Rði, x, C, TÞjSn, x
¼
X
M1
y¼0
Lði, yÞE½ f ðyjx, C, TÞjSn, x
¼
X
M1
y¼0
Lði, yÞE½PrðY ¼ yjx, C, TÞjSn, x:
(5.24)
The expectations are over a posterior on C and T updated with both Sn and
the unlabeled test point in question x. Now,
242
Chapter 5

E½PrðY ¼ yjx, C, TÞjSn, x
¼
Z
U
Z
DM1 PrðY ¼ yjx, c, uÞf ðc, ujSn, xÞdcdu
¼
Z
U
Z
DM1 PrðY ¼ yjx, c, uÞ f ðxjc, u, SnÞf ðc, ujSnÞ
f ðxjSnÞ
dcdu
∝
Z
U
Z
DM1 PrðY ¼ yjx, c, uÞf ðxjc, u, SnÞf ðc, ujSnÞdcdu
¼
Z
U
Z
DM1 PrðY ¼ yjx, c, uÞf ðxjc, uÞpðcÞpðuÞdcdu,
(5.25)
where the second equality follows from Bayes’ theorem, the proportionality in
the third line follows because the denominator does not depend on C or T,
and the fourth line follows (1) because X is independent of Sn given C and T,
and (2) from the definition of the posterior before we update with the test
point x. Applying Bayes’ theorem again,
E½PrðY ¼ yjx, C, TÞjSn, x
∝
Z
U
Z
DM1
cyf ðxjy, uyÞ
f ðxjc, uÞ
f ðxjc, uÞpðcÞpðuÞdcdu
¼
Z
DM1 cypðcÞdc
Z
Uy
f ðxjy, uyÞpðuyÞduy
¼ Ep½Cy f UðxjyÞ
¼ f DðyÞf UðxjyÞ:
(5.26)
The normalization constant is obtained by forcing the sum over y to be 1.
Thus,
E½PrðY ¼ yjx, C, TÞjSn, x ¼
f DðyÞf UðxjyÞ
PM1
i¼0 f DðiÞf UðxjiÞ :
(5.27)
Applying this to Eq. 5.24 yields
bRði, x, SnÞ ¼
PM1
y¼0 Lði, yÞf DðyÞf UðxjyÞ
PM1
k¼0 f DðkÞf UðxjkÞ
:
(5.28)
This is analogous to Eq. 5.1 in Bayes decision theory.
Furthermore, given a classifier c with decision regions R0, : : : , RM1,
E
h
bRðcðXÞ, X, SnÞjSn
i
¼
X
M1
i¼0
Z
Ri
bRði, x, SnÞf ðxjSnÞdx,
(5.29)
243
Optimal Bayesian Risk-based Multi-class Classification

where the expectation is over X ðnot C or TÞ given Sn. Note that
f ðxjSnÞ ¼
X
M1
y¼0
f ðyjSnÞf ðxjy, SnÞ
¼
X
M1
y¼0
f DðyÞf UðxjyÞ
(5.30)
is the marginal distribution of x given Sn. Proceeding,
E
h
bRðcðXÞ, X, SnÞjSn
i
¼
X
M1
i¼0
Z
Ri
PM1
y¼0 Lði, yÞf DðyÞf UðxjyÞ
PM1
k¼0 f DðkÞf UðxjkÞ
f ðxjSnÞdx
¼
X
M1
i¼0
Z
Ri
X
M1
y¼0
Lði, yÞf DðyÞf UðxjyÞdx
¼
X
M1
y¼0
X
M1
i¼0
Lði, yÞf DðyÞ
Z
Ri
f UðxjyÞdx
¼
X
M1
y¼0
X
M1
i¼0
Lði, yÞf DðyÞˆεi,y
n ðc, SnÞ
¼ bRðc, SnÞ:
(5.31)
Hence, the BRE of c is the mean of the BCRE across the feature space.
For binary classification, ˆεi,y
n ðc, SnÞ has been solved in closed form as
components of the Bayesian MMSE error estimator for both discrete models
under arbitrary classifiers and Gaussian models under linear classifiers, so the
BRE with an arbitrary loss function is available in closed form for these
models. When closed-form solutions for ˆεi,y
n ðc, SnÞ are not available, from
Eq. 5.20, ˆεi,y
n ðc, SnÞ may be approximated for all i and a given fixed y by
drawing a large synthetic sample from f UðxjyÞ and evaluating the proportion
of points assigned to class i. The final approximate BRE can be found by
plugging the approximate ˆεi,y
n ðc, SnÞ for each y and i into Eq. 5.18.
A number of practical considerations for Bayesian MMSE error estimators
addressed under binary classification naturally carry over to multiple classes,
including robustness to false modeling assumptions. Furthermore, classical
frequentist
consistency
holds
for
BREs
on
fixed
distributions
in
the
parameterized family owing to the convergence of posteriors in both the
discrete and Gaussian models.
244
Chapter 5

An optimal Bayesian risk classifier (OBRC) minimizes the BRE:
cOBRC ¼ arg min
c∈C
bRðc, SnÞ,
(5.32)
where C is a family of classifiers. If C is the set of all classifiers with measurable
decision regions, then cOBRC exists and is given for any x ∈X by
cOBRCðxÞ ¼ arg
min
i∈f0, : : : , M1g
bRði, x, SnÞ
¼ arg
min
i∈f0, : : : , M1g
X
M1
y¼0
Lði, yÞf DðyÞf UðxjyÞ:
(5.33)
The OBRC minimizes the average loss weighted by f DðyÞf UðxjyÞ. For future
reference we denote this weighted loss by ALðiÞ. The OBRC has the same
functional form as the BDR with f DðyÞ substituted for the true class
probability f ðyjcÞ and f UðxjyÞ substituted for the true density f ðxjy, uyÞ for
all y. Closed-form OBRC representation is available for any model in which
f UðxjyÞ has been found, including discrete and Gaussian models. A number
of properties also carry over, including invariance to invertible transforma-
tions, pointwise convergence to the Bayes classifier, and robustness to false
modeling assumptions.
For binary classification and zero-one loss,
bRðc, SnÞ ¼
X
1
y¼0
X
1
i¼0
Lði, yÞf DðyÞˆεi,y
n ðc, SnÞ
¼ f Dð0Þˆε1,0
n ðc, SnÞ þ f Dð1Þˆε0,1
n ðc, SnÞ
¼ f Dð0Þ
Z
R1
f Uðxj0Þdx þ f Dð1Þ
Z
R0
f Uðxj1Þdx,
(5.34)
which is identical to Eq. 2.27 since f Dð0Þ ¼ Ep½C0. Hence, the BRE reduces
to the Bayesian MMSE error estimator, and the OBRC reduces to the OBC.
5.4 Sample-Conditioned MSE of Risk Estimation
In a typical small-sample classification scenario, a classifier is trained from
data and a risk estimate is found for the true risk of this classifier. We can
measure the closeness of the risk estimate to the actual risk via the sample-
conditioned MSE of the BRE relative to the true expected risk:
MSEðbRðc, SnÞjSnÞ ¼ E½ðRðc, C, TÞ  bRðc, SnÞÞ2jSn
¼ varðRðc, C, TÞjSnÞ:
(5.35)
245
Optimal Bayesian Risk-based Multi-class Classification

This MSE is precisely the quantity that the BRE minimizes, and it quantifies
the accuracy of bR as an estimator of R, conditioned on the actual sample in
hand. Owing to posterior independence between C and T, it can be
decomposed as
MSEðbRðc, SnÞjSnÞ
¼
"X
M1
y¼0
X
M1
z¼0
X
M1
i¼0
X
M1
j¼0
Lði, yÞLðj, zÞEC½CyCzjSnmi,j
y,zðc, SnÞ
#
 bR2ðc, SnÞ,
(5.36)
where
mi,j
y,zðc, SnÞ ¼ E

εi,y
n ðc, TyÞεj,z
n ðc, TzÞjSn

(5.37)
is the posterior mixed moment of εi,y
n ðc, TyÞ and εj,z
n ðc, TzÞ, and where we
have applied Eq. 5.2 in Eq. 5.35, and used the fact that
E½ f ðyjCÞf ðzjCÞjSn ¼ E½CyCzjSn:
(5.38)
Second-order moments of Cy depend on the prior for C; for instance, under
Dirichlet posteriors they are given by Eqs. 5.12 and 5.13. Hence, evaluating
the conditional MSE of the BRE reduces to evaluating the BRE bRðc, SnÞ and
evaluating
posterior
mixed
moments
mi,j
y,zðc, SnÞ.
Furthermore,
if
we
additionally assume that T0, : : : , TM1 are pairwise independent, then when
y ≠z,
mi,j
y,zðc, SnÞ ¼ ˆεi,y
n ðc, SnÞˆεj,z
n ðc, SnÞ,
(5.39)
where ˆεi,y
n ðc, SnÞ, given in Eq. 5.20, is a component of the BRE.
The conditional MSE of an arbitrary risk estimate bR•ðc, SnÞ may be
found from the BRE and the MSE of the BRE:
MSEðbR•ðc, SnÞjSnÞ ¼ E½ðRðc, C, TÞ  bR•ðc, SnÞÞ2jSn
¼ MSEðbRðc, SnÞjSnÞ þ ½bRðc, SnÞ  bR•ðc, SnÞ2:
(5.40)
In this form, the optimality of the BRE is clear.
5.5 Efficient Computation
The following representation for mi,j
y,zðc, SnÞ is useful in both deriving analytic
forms for, and approximating, the MSE. Via Eq. 5.3 and Fubini’s theorem,
246
Chapter 5

mi,j
y,zðc, SnÞ ¼
Z
U
Z
Ri
f ðxjy, uyÞdx
Z
Rj
f ðwjz, uzÞdwpðuÞdu
¼
Z
Ri
Z
Rj
Z
U
f ðxjy, uyÞf ðwjz, uzÞpðuÞdudwdx
¼
Z
Ri
Z
Rj
f Uðx, wjy, zÞdwdx,
(5.41)
where the effective joint density in the last line is defined by
f Uðx, wjy, zÞ ¼
Z
U
f ðxjy, uyÞf ðwjz, uzÞpðuÞdu:
(5.42)
In terms of probability,
mi,j
y,zðc, SnÞ ¼ PrðX ∈Ri, W ∈Rjjy, z, SnÞ,
(5.43)
where X and W are random vectors drawn from f Uðx, wjy, zÞ.
The marginal densities of X and W under f Uðx, wjy, zÞ are precisely the
effective densities:
Z
X
f Uðx, wjy, zÞdw ¼
Z
X
Z
U
f ðxjy, uyÞf ðwjz, uzÞpðuÞdudw
¼
Z
U
f ðxjy, uyÞ
Z
X
f ðwjz, uzÞdwpðuÞdu
¼
Z
Uy
f ðxjy, uyÞpðuyÞduy
¼ f UðxjyÞ:
(5.44)
Furthermore, we have an effective conditional density of W given X:
f Uðwjx, y, zÞ ¼ f Uðx, wjy, zÞ
f UðxjyÞ
¼
Z
Uz
Z
Uy
f ðwjz, uzÞ
f ðxjy, uyÞpðuy, uzÞ
R
Uy f ðxjy, u
0
yÞpðu
0
yÞdu
0
y
duyduz:
(5.45)
As usual, pðuÞ represents the posterior relative to Sn, but, in addition, let
p
Sn∪fðx, yÞgðuÞ represent the posterior relative to Sn ∪fðx, yÞg. Expanding the
preceding expression yields
247
Optimal Bayesian Risk-based Multi-class Classification

f Uðwjx, y, zÞ
¼
Z
Uz
Z
Uy
f ðwjz, uzÞ
f ðxjy, uyÞpðuy, uzÞ
R
Uz
R
Uy f ðxjy, u
0
yÞpðu
0
y, u
0
zÞdu
0
ydu
0
z
duyduz
¼
Z
Uz
Z
Uy
f ðwjz, uzÞp
Sn∪fðx, yÞgðuy, uzÞduyduz
¼
Z
Uz
f ðwjz, uzÞp
Sn∪fðx, yÞgðuzÞduz
¼ f ðwjz, Sn ∪fðx, yÞgÞ,
(5.46)
where we have used the fact that the fractional term in the integrand of the
first equality is equivalent to the posterior updated with a new independent
sample point with feature vector x and label y, and the expression in the last
line is the effective density relative to Sn ∪fðx, yÞg.
The effective joint density may be easily found once the effective density is
known. Furthermore, from Eq. 5.43 we may approximate mi,j
y,zðc, SnÞ by
drawing a large synthetic sample from f UðxjyÞ, drawing a single point w from
the effective conditional density f ðwjz, Sn ∪fðx, yÞgÞ for each x, and
evaluating the proportion of pairs ðx, wÞ for which x ∈Ri and w ∈Rj.
Additionally, since x is marginally governed by the effective density, from
Eq. 5.21 we may approximate ˆεi,y
n ðc, SnÞ by evaluating the proportion of x
in Ri.
Evaluating the OBRC, BRE, and conditional MSE requires obtaining
E½CyjSn, E½C2yjSn, and E½CyCzjSn based on the posterior for C, and finding
the effective density f UðxjyÞ and the effective joint density f Uðx, wjy, zÞ based
on the posterior for T. At a fixed point x, one may then evaluate the BCRE
from Eq. 5.28. The OBRC is then found by choosing the class i that minimizes
ALðiÞ. For any classifier, the BRE is given by Eq. 5.18 with ˆεi,y
n ðc, SnÞ given
by Eq. 5.20 (or equivalently Eq. 5.21) using the effective density f UðxjyÞ. The
MSE of the BRE is then given by Eq. 5.36, where mi,j
y,zðc, SnÞ is given by
Eq. 5.39 when U0, : : : , UM1 are pairwise independent and y ≠z, and
mi,j
y,zðc, SnÞ is otherwise found from Eq. 5.41 (or equivalently Eq. 5.43) using
the effective joint density f Uðx, wjy, zÞ. The MSE of an arbitrary risk
estimator can also be found from Eq. 5.40 using the BRE and the MSE for
the BRE.
5.6 Evaluation of Posterior Mixed Moments: Discrete Model
Drawing on results from binary classification, in this section we evaluate the
posterior mixed moments mi,j
y,zðc, SnÞ in the discrete model.
248
Chapter 5

Consider a discrete feature space X ¼ f1, 2, : : : , bg. Let py
x be the
probability that a point from class y is observed in bin x ∈X, and let Uy
x be
the number of sample points observed from class y in bin x. Note that
ny ¼ Pb
x¼1 Uy
x. The discrete Bayesian model defines Ty ¼ ½Py
1, : : : , Py
b with
parameter space Uy ¼ Db1. For each y, we define Dirichlet priors on Ty with
hyperparameters ay ¼ ½ay
1, : : : , ay
b by
pðuyÞ ∝
Y
b
x¼1
ðpy
xÞay
x1:
(5.47)
Assume that the Ty are mutually independent. According to Eq. 2.52, the
posteriors are again Dirichlet with updated hyperparameters ay
x ¼ ay
x þ Uy
x
for all x and y. For proper posteriors, ay
x . 0 for all x and y. According to
Eq. 2.53, the effective density is
f UðxjyÞ ¼ ay
x
ay
þ
,
(5.48)
where
ay
þ ¼
X
b
x¼1
ay
x :
(5.49)
Thus,
ˆεi,y
n ðc, SnÞ ¼
X
b
x¼1
ay
x
ay
þ
IcðxÞ¼i:
(5.50)
The effective joint density f Uðx, wjy, zÞ for y ¼ z can be found from
properties of Dirichlet distributions. For any y ∈f0, : : : , M  1g and
x, w ∈X, by Theorem 3.4,
f Uðx, wjy, yÞ ¼ E½Py
xPy
wjSn ¼ ay
x ðay
w þ dxwÞ
ay
þ ðay
þ þ 1Þ :
(5.51)
From Eq. 5.41,
mi,j
y,yðc, SnÞ ¼
X
b
x¼1
X
b
w¼1
ay
x ðay
w þ dxwÞ
ay
þ ðay
þ þ 1Þ IcðxÞ¼iIcðwÞ¼j
¼ ˆεi,y
n ðc, SnÞ½ay
þ ˆεj,y
n ðc, SnÞ þ dij
ay
þ þ 1
:
(5.52)
This generalizes Eq. 3.23. When y ≠z, mi,j
y,zðcÞ may be found from Eq. 5.39.
249
Optimal Bayesian Risk-based Multi-class Classification

5.7 Evaluation of Posterior Mixed Moments: Gaussian Models
We now consider Gaussian models with mean vectors my and covariance
matrices Sy. All posteriors on parameters, effective densities, and effective
joint densities found in this section apply under arbitrary multi-class
classifiers. We also find analytic forms for posterior mixed moments, the
BRE, and the conditional MSE under binary linear classifiers c of the form
cðxÞ ¼

0
if gðxÞ ≤0,
1
otherwise,
(5.53)
where gðxÞ ¼ aTx þ b for some vector a and scalar b. Under arbitrary
classifiers, the BRE and conditional MSE may be approximated using
techniques described in Section 5.5.
5.7.1 Known covariance
Assume that Sy is known and is a valid invertible covariance matrix. Then
Ty ¼ my with parameter space Uy ¼ RD. We assume that the my are mutually
independent and use the prior in Eq. 2.58:
pðmyÞ ∝jSyj1
2 exp

 ny
2 ðmy  myÞTS1
y ðmy  myÞ
	
,
(5.54)
where ny ∈R and my ∈RD. The posterior is of the same form as the prior,
with updated hyperparameters ny and my given in Eqs. 2.65 and 2.66,
respectively. We require that ny . 0 for a proper posterior. The effective
density is given in Eq. 2.96 of Theorem 2.6.
To find the BRE we require ˆεi,y
n ðc, SnÞ. Under linear classifiers, this is
essentially given by Theorem 2.10, except that the exponent of 1 is altered
to obtain
ˆεi,y
n ðc, SnÞ ¼ F
0
B
@ð1Þiþ1gðmyÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSya
q
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ny
ny þ 1
s
1
C
A:
(5.55)
To find the MSE under linear classification, note that f Uðwjx, y, zÞ is of
the same form as f UðxjyÞ with posterior hyperparameters updated with ðx, yÞ
as a new sample point. Hence, for y ¼ z,
f Uðwjx, y, yÞ  N

my þ x  my
ny þ 1 , ny þ 2
ny þ 1 Sy
	
,
(5.56)
250
Chapter 5

and, as also shown in Theorem 3.6, the effective joint density is thus given by
f Uðx, wjy, yÞ  N
0
@
 my
my

,
2
4
nyþ1
ny Sy
1
ny Sy
1
ny Sy
nyþ1
ny Sy
3
5
1
A:
(5.57)
Now let P ¼ ð1ÞigðXÞ and Q ¼ ð1ÞjgðWÞ. Since X and W are governed by
the effective joint density in Eq. 5.57, the effective joint density of P and Q is
f Uðp, qjy, yÞ  N
0
@
 ð1ÞigðmyÞ
ð1ÞjgðmyÞ

,
2
4
nyþ1
ny aTSya
ð1Þiþj
ny
aTSya
ð1Þiþj
ny
aTSya
nyþ1
ny aTSya
3
5
1
A: (5.58)
Hence, from Eq. 5.43,
mi,j
y,yðc, SnÞ ¼ PrðP ≤0, Q ≤0jy, y, SnÞ
¼ F
0
B
@ ð1ÞigðmyÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSya
q
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ny
ny þ 1
s
,  ð1ÞjgðmyÞ
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
aTSya
q
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ny
ny þ 1
s
, ð1Þiþj
ny þ 1
1
C
A,
(5.59)
where Fð⋅, ⋅, rÞ is the joint CDF of two standard normal random variables
with correlation r. A special case of Eq. 5.59 is also provided in Theorem 3.8.
When y ≠z, mi,j
y,zðc, SnÞ is found from Eq. 5.39.
5.7.2 Homoscedastic general covariance
Consider
the
homoscedastic
model
with
general
covariance,
where
Ty ¼ ½my, S, the parameter space of my is RD, and the parameter space of
S consists of all symmetric positive definite matrices. Further, assume a
conjugate prior in which the my are mutually independent given S so that
pðuÞ ¼
" Y
M1
y¼0
pðmyjSÞ
#
pðSÞ,
(5.60)
where pðmyjSÞ is as in Eq. 5.54 with hyperparameters ny ∈R and my ∈RD,
and
pðSÞ ∝jSjkþDþ1
2
exp

 1
2 SS1
	
(5.61)
with hyperparameters k ∈R and S, a symmetric D  D matrix. Note that
Eq. 5.60 is the multi-class extension of Eq. 2.77 in which we are assuming the
general-covariance model l ¼ S (although we need not have). All comments
251
Optimal Bayesian Risk-based Multi-class Classification

in Chapter 2 regarding these priors apply. In particular, if ny . 0, then ðmyjSÞ
is Gaussian with mean my and covariance S∕ny, and if k . D  1 and
S is positive definite, then pðSÞ is an inverse-Wishart distribution with
hyperparameters k and S. Similar to Theorem 2.5, the posterior is of the
same form as the prior with ny, my, and k given in Eqs. 2.65, 2.66, and 2.80,
respectively, and
S ¼ S þ
X
M1
y¼0
ðny  1ÞbSy þ
nyny
ny þ ny
ðbmy  myÞðbmy  myÞT:
(5.62)
Evaluating Eq. 5.62 requires the sample mean bmy and sample variance bSy for
all classes y ¼ 0, : : : , M  1. The posteriors are proper if ny . 0, k . D  1,
and S is positive definite.
According to Theorem 2.9, the effective density for class y is multivariate t
with k ¼ k  D þ 1 degrees of freedom, location vector my, and scale matrix
½ðny þ 1Þ∕ðknyÞS:
f UðxjyÞ  t

my, ny þ 1
kny
S, k
	
:
(5.63)
To find the BRE under a binary linear classifier of the form in Eq. 5.53, let
P ¼ ð1ÞigðXÞ. Since P is an affine transformation of a multivariate t-random
vector,
it
has
a
non-standardized
Student’s
t-distribution
(Kotz
and
Nadarajah, 2004):
f UðpjyÞ  t

miy, ny þ 1
kny
g2, k
	
,
(5.64)
where miy ¼ ð1ÞigðmyÞ, and g2 ¼ aTSa. The CDF ϒ of a non-standardized
Student’s t-distribution with d degrees of freedom, location parameter m, and
scale parameter s2 is well known, and
ϒð0Þ ¼ 1
2  sgnðmÞ
2
I

m2
m2 þ ds2 ; 1
2 , d
2
	
,
(5.65)
where Iðx; a, bÞ is an incomplete regularized beta function (Johnson et al.,
1995). Hence,
ˆεi,y
n ðc, SnÞ ¼ 1
2  sgnðmiyÞ
2
I
 
m2
iy
m2
iy þ nyþ1
ny g2 ; 1
2 , k
2
!
:
(5.66)
This result is an extension of Theorem 2.12.
The MSE found in Chapter 3 assumes independent covariances, and thus
does not apply here. Instead, the effective conditional density for y ¼ z is
252
Chapter 5

solved by updating all of the hyperparameters associated with class y with the
new sample point ðx, yÞ, resulting in
f Uðwjx, y, yÞ  t

my þ x  my
ny þ 1 ,
ny þ 2
ðk þ 1Þðny þ 1Þ ½S þ SyðxÞ, k þ 1
	
,
(5.67)
where
SyðxÞ ¼
ny
ny þ 1 ðx  myÞðx  myÞT:
(5.68)
For y ≠z, f Uðwjx, y, zÞ is of the same form as the effective density with only
hyperparameters k and S updated:
f Uðwjx, y, zÞ  t

mz,
nz þ 1
ðk þ 1Þnz
½S þ SyðxÞ, k þ 1
	
:
(5.69)
The next lemma is used to derive the effective joint density.
Lemma 5.1 (Dalton and Yousefi, 2015). Suppose that X is a multivariate
t-random vector with density
f ðxÞ  t

my, ny þ 1
kny
g2ID, k
	
:
(5.70)
Further, suppose that W conditioned on X ¼ x is multivariate t with density
f ðwjxÞ  t
0
B
@mz þ I x  my
ny þ 1 ,
J
h
g2 þ
ny
nyþ1 ðx  myÞTðx  myÞ
i
k þ D
ID, k þ D
1
C
A,
(5.71)
where either I ¼ 0 and J ¼ ðnz þ 1Þ∕nz, or jIj ¼ 1 and J ¼ ðny þ 2Þ∕ðny þ 1Þ.
Then, the joint density is multivariate t:
f ðx, wÞ  t
 
my
mz

, g2
k
" nyþ1
ny ID
I 1
ny ID
I 1
ny ID
KID
#
, k
!
,
(5.72)
where K ¼ ðnz þ 1Þ∕nz when I ¼ 0 and K ¼ ðny þ 1Þ∕ny when jIj ¼ 1.
253
Optimal Bayesian Risk-based Multi-class Classification

Proof. After some simplification, one can show that
f ðx, wÞ ¼ f ðxÞf ðwjxÞ
∝

1 þ
ny
ny þ 1 ðx  myÞTðg2IDÞ1ðx  myÞ
kþD
2

g2ID þ
ny
ny þ 1 ðx  myÞTðx  myÞID

kþ2D1
2

g2ID þ
ny
ny þ 1 ðx  myÞTðx  myÞID þ 1
J vvT

kþ2D
2 ,
(5.73)
where
v ¼ w  mz  I x  my
ny þ 1 :
(5.74)
Further simplification yields
f ðx, wÞ
∝

g2 þ
ny
ny þ 1 ðx  myÞTðx  myÞ
þ 1
J

w  mz  I x  my
ny þ 1
	T
w  mz  I x  my
ny þ 1
	kþ2D
2 :
(5.75)
If I ¼ 0, then it can be shown that
f ðx, wÞ ∝

1 þ

x  my
w  mz
T
L1

x  my
w  mz
	kþ2D
2 ,
(5.76)
where
L ¼
" nyþ1
ny g2ID
0D
0D
nzþ1
nz g2ID
#
:
(5.77)
Similarly, if jIj ¼ 1, it can be shown that
f ðx, wÞ ∝

1 þ

x  my
w  mz
T
L1

x  my
w  mz
	kþ2D
2 ,
(5.78)
where
L ¼
" nyþ1
ny g2ID
I
ny g2ID
I
ny g2ID
nyþ1
ny g2ID
#
,
(5.79)
which completes the proof.
▪
254
Chapter 5

To
find
the
conditional
MSE
of
the
BRE
under
binary
linear
classification, let P ¼ ð1ÞigðXÞ and Q ¼ ð1ÞjgðWÞ. For y ¼ z, and
p ¼ ð1ÞigðxÞ,
f Uðqjx, y, yÞ
 t

mjy þ ð1Þiþj p  miy
ny þ 1 ,
ny þ 2
ðk þ 1Þðny þ 1Þ

g2 þ
ny
ny þ 1ðp  miyÞ2

, k þ 1
	
,
(5.80)
where we have used the fact that ð1ÞiaTðx  myÞ ¼ p  my. When y ≠z,
f Uðqjx, y, zÞ  t

mjz,
nz þ 1
ðk þ 1Þnz

g2 þ
ny
ny þ 1 ðp  miyÞ2

, k þ 1
	
:
(5.81)
Since dependency on X has been reduced to dependency on only P in both of
the above distributions, we may write f Uðqjx, y, zÞ ¼ f Uðqjp, y, zÞ for all y
and z. The lemma produces an effective joint density given an effective density
and an effective conditional density of a specified form. Indeed, the
distributions f UðpjyÞ and f Uðqjp, y, yÞ are precisely in the form required by
the lemma with D ¼ 1. Hence, ½P, QT follows a bivariate t-distribution when
y ¼ z,
f Uðp, qjy, yÞ  t
0
@
 miy
mjy

, g2
k
2
4
nyþ1
ny
ð1Þiþj
ny
ð1Þiþj
ny
nyþ1
ny
3
5, k
1
A,
(5.82)
and when y ≠z,
f Uðp, qjy, zÞ  t
  miy
mjz

, g2
k
" nyþ1
ny
0
0
nzþ1
nz
#
, k
!
:
(5.83)
Thus, mi,j
y,yðc, SnÞ can be found from Eq. 5.43. In particular, when y ¼ z,
mi,j
y,yðc, SnÞ ¼ PrðP ≤0, Q ≤0jy, y, SnÞ
¼ T
0
@ miy
g
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
kny
ny þ 1
s
,  mjy
g
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
kny
ny þ 1
s
, k, ð1Þiþj
ny þ 1
1
A,
(5.84)
and when y ≠z,
255
Optimal Bayesian Risk-based Multi-class Classification

mi,j
y,zðc, SnÞ ¼ PrðP ≤0, Q ≤0jy, z, SnÞ
¼ T
0
@ miy
g
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
kny
ny þ 1
s
,  mjz
g
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
knz
nz þ 1
s
, k, 0
1
A,
(5.85)
where Tðx, y, d, rÞ is the bivariate CDF of standard Student’s t-random
variables with d degrees of freedom and correlation coefficient r.
5.7.3 Independent general covariance
In the independent general covariance model, Ty ¼ ½my, Sy, the parameter
space of my is RD, the parameter space of Sy consists of all symmetric positive
definite matrices, the Ty are independent, and
pðuyÞ ¼ pðmyjSyÞpðSyÞ,
(5.86)
where pðmyjSyÞ is of the same form as in Eq. 5.54 with hyperparameters
ny ∈R and my ∈RD, and pðSyÞ is of the same form as in Eq. 5.61 with
hyperparameters ky ∈R and the symmetric D  D matrix Sy in place of k and
S. Via Theorem 2.4, the posterior is of the same form as the prior with ny, my,
ky, and Sy given in Eqs. 2.65 through 2.68, respectively. The posteriors are
proper if ny . 0, ky . D  1, and Sy is positive definite.
The effective density for class y is multivariate t as in Eq. 5.63 with
ky ¼ ky  D þ 1 and Sy in place of k and S, respectively (Theorem 2.9).
Further, Eq. 5.64 also holds with miy ¼ ð1ÞigðmyÞ and with ky and
g2y ¼ aTSya in place of k and g2, respectively. Under binary linear classification,
ˆεi,y
n ðc, SnÞ is given by Eq. 5.66 with ky and g2y in place of k and g2, respectively
(Theorem 2.12). mi,j
y,yðc, SnÞ is solved similarly to the homoscedastic case,
resulting in Eqs. 5.67, 5.80, and 5.82, and ultimately Eq. 5.84, with ky, Sy, and
g2y in place of k, S, and g2, respectively. A special case of Eq. 5.84 is also
provided in Theorem 3.10. mi,j
y,zðc, SnÞ for y ≠z is found from Eq. 5.39.
5.8 Simulations
We consider five classification rules: OBRC (under Gaussian models),
LDA, QDA, L-SVM, and RBF-SVM. Since SVM classifiers optimize
relative to their own objective function (for example, hinge loss) rather than
expected risk, we exclude them from our analysis when using a non–zero-one
loss function. For all classification rules, we calculate the true risk defined
in Eqs. 5.2 and 5.3. We find the exact value if a formula is available;
otherwise, we use a test sample of at least 10,000 points generated from
256
Chapter 5

the true feature-label distributions, stratified relative to the true class
prior probabilities. This will yield an approximation of the true risk with
RMS ≤1∕
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
4  10,000
p
¼ 0.005 by Eq. 1.37.
Four risk estimation methods are considered: BRE, 10-fold cross-
validation (cv), leave-one-out (loo), and 0.632 bootstrap (boot). When there
is no closed-form formula for calculating the BRE, we approximate it by
drawing a sample of 1,000,000 points from the effective density of each class.
In cv, the risk is estimated on the holdout data and the resulting risk values are
averaged to get the cv estimate. For cv and boot risk estimates, we use 10 and
100 repetitions, respectively.
Under linear classification, the sample-conditioned MSE from Eq. 5.36 is
found analytically: for mi,j
y,yðc, SnÞ, use Eq. 5.84 and plug in the appropriate
values for k and g2 depending on the covariance model, and for mi,j
y,zðc, SnÞ
with z ≠y, employ Eq. 5.39 for independent and Eq. 5.85 for homoscedastic
covariance models, and plug in appropriate values for k and g2. When
analytic forms are not available, the sample-conditioned MSE is approxi-
mated as follows. In independent covariance models, for each y, and for each
of the 1,000,000 sample points generated to approximate the BRE of class y,
draw a single point from the effective conditional density f Uðwjx; y; yÞ, giving
1,000,000 sample point pairs to approximate mi,j
y,yðc, SnÞ for all i and j.
In homoscedastic covariance models, for each y and z pair we draw 1,000,000
synthetic points from the effective density of class y (there are already 1,000,000
points available from approximating the BRE, which we distribute between
each z). For each of these points we draw a single point from the effective
conditional density f Uðwjx, y, zÞ. For each y and z, the corresponding
1,000,000 point pairs are used to approximate mi,j
y,zðc, SnÞ for all i and j.
In the simulations, all classes are equally likely, the data are stratified to
give an equal number of sample points from each class, there are Gaussian
feature-label distributions, and we employ a zero-one loss function. For each
prior model and a fixed sample size, we evaluate classification performance in
a Monte Carlo estimation loop with 10,000 iterations. In each iteration, a
two-step procedure is followed for sample generation: (1) generate random
feature-label distribution parameters from the prior (each serving as the true
underlying feature-label distribution), and (2) generate a random sample of
size n from this fixed feature-label distribution. The generated random sample
is used to train classifiers and evaluate their true risk. We consider four
Gaussian models.
For Models 1 and 2, the number of features is D ¼ 2, the number of classes
is M ¼ 5, n0, : : : , n4 are 12, 2, 2, 2, 2, respectively, the means m0, : : : , m4 are
½0, 0T, ½1, 1T, ½1,  1T, ½1,  1T, ½1, 1T, respectively, ky ¼ 6 for all y,
ðky  D  1Þ1Sy ¼ 0.3I2 for all y, and the covariances are independent general
and homoscedastic general, respectively. Figures 5.1 and 5.2 provide examples
of decision boundaries for Models 1 and 2, respectively. Under Model 1
257
Optimal Bayesian Risk-based Multi-class Classification

−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
(a)
(b)
(d)
(e)
(c)
Figure 5.2
Example decision boundaries for Model 2 with multi-class classification: (a) LDA;
(b) QDA; (c) OBRC; (d) L-SVM; (e) RBF-SVM. [Reprinted from (Dalton and Yousefi, 2015).]
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
−4
−3
−2
−1
0
1
2
3
4
(a)
(b)
(d)
(e)
(c)
Figure 5.1
Example decision boundaries for Model 1 with multi-class classification: (a) LDA;
(b) QDA; (c) OBRC; (d) L-SVM; (e) RBF-SVM. [Reprinted from (Dalton and Yousefi, 2015).]
258
Chapter 5

(independent covariances), the decision boundaries of OBRC are most similar
to QDA, although they are in general of a polynomial order. Under Model 2
(homoscedastic covariances), OBRC is most similar to LDA, although the
decision boundaries are not necessarily linear.
Figure 5.3 presents the mean and standard deviation of the true risk
with respect to all sample realizations as a function of sample size for
Models 1 and 2. OBRC outperforms all other classification rules with respect
to mean risk, as it must, since the OBRC is defined to minimize mean risk.
Although there is no guarantee that OBRC should minimize risk variance, in
these examples the risk variance is lower than in all other classification rules.
The performance gain is particularly significant for small samples. Consider
Figs. 5.3(a) and 5.3(b), where at sample size 10, the risk of OBRC has a mean
of about 0.16 and standard deviation of about 0.065, whereas the risk of the
next best classifier, RBF-SVM, has a mean of about 0.22 and standard
deviation of about 0.09.
For
higher-dimensional
examples,
we
consider
Models
3
and
4,
for which D ¼ 20 and M ¼ 5. For Model 3, n0, : : : , n4 are 12, 2, 2, 2, 2,
respectively, m0, : : : , m4 are 020, 0.1 ⋅120, 0.1 ⋅120, ½0.1 ⋅1T
10,  0.1 ⋅1T
10T,
½0.1 ⋅1T
10, 0.1 ⋅1T
10T, respectively, ky ¼ 20.65, ð3Þ1Sy ¼ 0.3I20, and the
covariances are independent scaled identity [see (Dalton and Yousefi, 2015)
for details of this case]. Model 4 is the same except n0 ¼ ··· ¼ n4 ¼ 20 and
10
30
50
70
90
110
130
0.1
0.15
0.2
0.25
sample size
(a)
(b)
(c)
(d)
average risk
LDA
QDA
OBRC
L-SVM
RBF-SVM
10
30
50
70
90
110
130
0.05
0.06
0.07
0.08
0.09
0.1
sample size
risk standard deviation
LDA
QDA
OBRC
L-SVM
RBF-SVM
10
30
50
70
90
110
130
0.1
0.15
0.2
0.25
sample size
average risk
LDA
QDA
OBRC
L-SVM
RBF-SVM
10
30
50
70
90
110
130
0.08
0.09
0.1
0.11
0.12
0.13
0.14
sample size
risk standard deviation
LDA
QDA
OBRC
L-SVM
RBF-SVM
Figure 5.3
True risk statistics for Models 1 (top row) and 2 (bottom row), and five
classification rules (LDA, QDA, OBRC, L-SVM, and RBF-SVM): (a) Model 1, mean;
(b) Model 1, standard deviation; (c) Model 2, mean; (d) Model 2, standard deviation.
[Reprinted from (Dalton and Yousefi, 2015).]
259
Optimal Bayesian Risk-based Multi-class Classification

m0 ¼ ··· ¼ m4 ¼ 020. Figure 5.4 presents the mean and standard deviation of
the true risk of all classifiers as a function of sample size for Models 3 and 4,
where Model 3 is designed to produce a low mean risk and Model 4 a high
mean risk. OBRC again outperforms all other classification rules with respect
to mean risk, as it should. There is no guarantee that OBRC will minimize
risk variance, and, although risk variance is lowest for OBRC in Fig. 5.4(b), in
Fig. 5.4(d) it is actually highest. Performance gain is particularly significant
for small samples.
20
60
100
140
180
220
260
300
0
0.1
0.2
0.3
0.4
0.5
sample size
(a)
(b)
(c)
(d)
average risk
LDA
QDA
OBRC
L-SVM
RBF-SVM
20
60
100
140
180
220
260
300
0.02
0.03
0.04
0.05
0.06
0.07
0.08
sample size
risk standard deviation
LDA
QDA
OBRC
L-SVM
RBF-SVM
20
60
100
140
180
220
260
300
0.3
0.4
0.5
0.6
0.7
0.8
sample size
average risk
LDA
QDA
OBRC
L-SVM
RBF-SVM
20
60
100
140
180
220
260
300
0.02
0.04
0.06
0.08
0.1
sample size
risk standard deviation
LDA
QDA
OBRC
L-SVM
RBF-SVM
Figure 5.4
True risk statistics for Models 3 (top row) and 4 (bottom row), and five
classification rules (LDA, QDA, OBRC, L-SVM, and RBF-SVM): (a) Model 3, mean;
(b) Model 3, standard deviation; (c) Model 4, mean; (d) Model 4, standard deviation.
[Reprinted from (Dalton and Yousefi, 2015).]
260
Chapter 5

Chapter 6
Optimal Bayesian Transfer
Learning
The theory of optimal Bayesian classification assumes that the sample data
come from the unknown true feature-label distribution, which is a standard
assumption in classification theory. When data from the true feature-label
distribution are limited, it is possible to use data from a related feature-label
distribution. This is the basic idea behind transfer learning, where data from a
source domain are used to augment data from a target domain, which may
follow a different feature-label distribution (Pan and Yang, 2010; Weiss et al.,
2016). The key issue is to quantify relatedness, which means providing a
rigorous mathematical framework to characterize transferability. This can be
achieved by extending the OBC framework so that transfer learning from the
source to target domain is via a joint prior distribution for the model
parameters of the feature-label distributions of the two domains (Karbalay-
ghareh et al., 2018b). In this way, the posterior distribution of the target
model parameters can be updated via the joint prior probability distribution
function in conjunction with the source and target data.
6.1 Joint Prior Distribution
We consider L common classes (labels) in each domain. Let Ss and St
denote samples from the source and target domains with sizes of Ns
and Nt, respectively. In practice one might expect Nt to be substantially
smaller than Ns. For l ¼ 1, 2, : : : , L, let Sls ¼ fxl
s,1, xl
s,2, : : : , xl
s,nlsg and
Sl
t ¼ fxl
t,1, xl
t,2, : : : , xl
t,nl
tg, where nls and nl
t denote the number of points in
the source and target domains, respectively, for the label l. Si
t ∩Sj
t ¼ ∅and
Sis ∩Sj
s ¼ ∅for i, j ∈f1, : : : , Lg. Moreover, Ss ¼ ∪L
l¼1 Sls, St ¼ ∪L
l¼1 Sl
t,
Ns ¼ PL
l¼1 nls, and Nt ¼ PL
l¼1 nlt.
Heretofore we have been considering priors on the covariance matrix and
have employed the inverse-Wishart distribution. Here, we desire priors for the
261

precision matrices Lls or Ll
t, and we use a Wishart distribution as the
conjugate prior. A random D  D symmetric positive definite matrix L has a
nonsingular Wishart distribution WishartðM, kÞ with k degrees of freedom if
k . D  1, M is a D  D symmetric positive definite matrix, and the density is
f WðL; M, kÞ ¼

2
kD
2 GD
k
2

jMj
k
2
1
jLj
kD1
2 etr

 1
2 M1L

,
(6.1)
where GDð⋅Þ is the multivariate gamma function.
We assume that there are two datasets separately sampled from the source
and target domains. Since the feature spaces are the same in both the source
and target domains, let xls and xl
t be D  1 vectors for the D features of the
source and target domains under class l, respectively. We utilize a Gaussian
model for the feature-label distribution in each domain:
xlz  N ðml
z, ðLlzÞ1Þ
(6.2)
for l ∈f1, : : : , Lg, where subscript z ∈fs, tg denotes the source s or target t
domain, mls and ml
t are D  1 mean vectors in the source and target domains
for label l, respectively, Lls and Ll
t are the D  D precision matrices in the
source and target domains for label l, respectively, and a joint normal-Wishart
distribution is employed as a prior for the mean and precision matrices of the
Gaussian models. Under these assumptions, the joint prior distribution for mls,
mlt, Lls, and Llt takes the form
pðmls, mlt, Lls, LltÞ ¼ f ðmls, mltjLls, LltÞpðLls, LltÞ:
(6.3)
To facilitate conjugate priors, we assume that, for any class l ∈f1, : : : , Lg, mls
and ml
t are conditionally independent given Lls and Ll
t. Thus,
pðml
s, ml
t, Lls, Ll
tÞ ¼ f ðml
sjLlsÞf ðml
tjLl
tÞpðLls, Ll
tÞ,
(6.4)
and both f ðmlsjLlsÞ and f ðml
tjLl
tÞ are Gaussian,
ml
zjLlz  N ðmlz, ðnlzLlzÞ1Þ,
(6.5)
where mlz is the D  1 mean vector of mlz, and nlz is a positive scalar
hyperparameter.
A key issue is the structure of the joint prior governing the target and
source precision matrices. Here we employ a family of joint priors that falls
out naturally from a collection of partitioned Wishart random matrices.
Specifically, we consider D  D matrices L  WishartðM, kÞ, where
262
Chapter 6

L ¼
 L11
L12
LT
12
L22

,
(6.6)
L11 and L22 are D1  D1 and D2  D2 submatrices, respectively, and
M ¼
 M11
M12
MT
12
M22

(6.7)
is the corresponding partition of M, with M11 and M22 being D1  D1 and
D2  D2
submatrices, respectively. For such L, we are assured that
L11  WishartðM11, kÞ and L22  WishartðM22, kÞ (Muirhead, 2009).
The next theorem gives the form of the joint distribution of the two
submatrices of a partitioned Wishart matrix. It provides a class of joint priors
for the target and source precision matrices.
Theorem
6.1
(Halvorsen
et
al.,
2016).
If
L
in
Eq.
6.6
is
a
ðD1 þ D2Þ  ðD1 þ D2Þ partitioned Wishart random matrix with k ≥D1 þ D2
degrees of freedom and the positive definite scale matrix M in Eq. 6.7, where the
two diagonal partitions of L and M are of sizes D1  D1 and D2  D2, then the
diagonal partitions L11 and L22 have the joint density function
f ðL11, L22Þ ¼ K etr

 1
2 ðM1
11 þ FTC2FÞL11

etr

 1
2 C1
2 L22

 jL11j
kD21
2
jL22j
kD11
2
0F1
k
2 ; 1
4 GðL11, L22Þ

,
(6.8)
where
C2 ¼ M22  MT
12M1
11 M12,
(6.9)
F ¼ C1
2 MT
12M1
11 ,
(6.10)
GðL11, L22Þ ¼ L
1
2
22FL11FTL
1
2
22,
(6.11)
K1¼ 2
ðD1þD2Þk
2
GD1
k
2

GD2
k
2

jMj
k
2,
(6.12)
X
1
2 denotes the unique positive definite square root of the positive definite matrix
X, and 0F1is the generalized matrix-variate hypergeometric function.
The generalized hypergeometric function of one matrix argument (Nagar
and Mosquera-Benıtez, 2017) is defined by
263
Optimal Bayesian Transfer Learning

pFqða1, : : : , ap; b1, : : : , bq; XÞ ¼
X
`
k¼0
X
t⊢k
ða1Þt ··· ðapÞt
ðb1Þt ··· ðbqÞt
CtðXÞ
k!
,
(6.13)
where ai for i ¼ 1, : : : , p and bj for j ¼ 1, : : : , q are arbitrary complex (real in
our case) numbers; CtðXÞ is the zonal polynomial of a D  D symmetric
matrix
X
corresponding
to
an
ordered
partition
t ¼ ðk1, : : : , kDÞ,
k1 ≥··· ≥kD ≥0, k1 þ ··· þ kD ¼ k; and P
t⊢k denotes a summation over
all partitions t. The generalized hypergeometric coefficient ðaÞt is defined by
ðaÞt ¼
Y
D
i¼1

a  i  1
2

ki
,
(6.14)
where ðaÞk ¼ aða þ 1Þ ··· ða þ k  1Þ for k ¼ 1, 2, : : : , and ðaÞ0 ¼ 1. As in
(Muirhead, 2009), we also define
CtðXYÞ ¼ CtðYXÞ ¼ CtðX
1
2YX
1
2Þ
(6.15)
for symmetric positive definite X and symmetric Y. This effectively extends
pFq to accept matrices of the form XY and YX.
Conditions for convergence of the series in Eq. 6.13 are available in
(Constantine, 1963). From Eq. 6.13 it follows that
0F0ðXÞ ¼
X
`
k¼0
X
t⊢k
CtðXÞ
k!
¼
X
`
k¼0
½trðXÞk
k!
¼ etrðXÞ,
(6.16)
1F0ða; XÞ ¼
X
`
k¼0
X
t⊢k
ðaÞtCtðXÞ
k!
¼ jID  Xja, kXk , 1,
(6.17)
0F1ðb; XÞ ¼
X
`
k¼0
X
t⊢k
CtðXÞ
ðbÞtk! ,
(6.18)
1F1ða; b; XÞ ¼
X
`
k¼0
X
t⊢k
ðaÞt
ðbÞt
CtðXÞ
k!
,
(6.19)
2F 1ða, b; c; XÞ ¼
X
`
k¼0
X
t⊢k
ðaÞtðbÞt
ðcÞt
CtðXÞ
k!
,
(6.20)
where kXk , 1 means that the maximum of the absolute values of the
eigenvalues of X is less than 1. 1F 1ða; b; XÞ and 2F 1ða, b; c; XÞ are called the
confluent hypergeometric function and Gauss hypergeometric function of one
matrix argument, respectively.
264
Chapter 6

Using Theorem 6.1, we define the joint prior distribution pðLls, Ll
tÞ in
Eq. 6.4 of the precision matrices of the source and target domains for class
l ∈f1, : : : , Lg. Given a Wishart matrix of the form in Eq. 6.6, and replacing
L11 and L22 with Ll
t and Lls, respectively, we obtain the joint prior
pðLl
t, LlsÞ ¼ Kl etr

 1
2
h
ðMl
tÞ1 þ ðFlÞTClFli
Ll
t

jLl
tj
klD1
2
 etr

 1
2 ðClÞ1Lls

jLlsj
klD1
2
0F1
kl
2 ; 1
4 GlðL11, L22Þ

,
(6.21)
where
Ml ¼

Ml
t
Ml
ts
ðMl
tsÞT
Mls

(6.22)
is a 2D  2D positive definite scale matrix, kl ≥2D denotes degrees of
freedom, and
Cl ¼ Mls  ðMl
tsÞTðMl
tÞ1Ml
ts,
(6.23)
Fl ¼ ðClÞ1ðMl
tsÞTðMl
tÞ1,
(6.24)
GlðL11, L22Þ ¼ ðLlsÞ
1
2FlLltðFlÞTðLlsÞ
1
2,
(6.25)
ðKlÞ1¼ 2DklG2
D
kl
2

jMlj
kl
2 :
(6.26)
Owing to the comment following Eq. 6.7, Ll
t and Lls possess Wishart marginal
distributions:
Llz  WishartðMlz, klÞ
(6.27)
for l ∈f1, : : : , Lg and z ∈fs, tg.
6.2 Posterior Distribution in the Target Domain
Having defined the prior distributions, we need to derive the posterior
distribution of the parameters of the target domain upon observing the
training source and target samples. The likelihood of the samples St and Ss is
conditionally independent given the parameters of the target and source
domains. The dependence between the two domains is due to the dependence
of the prior distributions of the precision matrices, as shown in Fig 6.1. Within
each domain, source or target, the likelihoods of the different classes are also
conditionally independent given the parameters of the classes.
265
Optimal Bayesian Transfer Learning

Let
mz ¼ ðm1z, : : : , mLz Þ,
and
Lz ¼ ðL1z, : : : , LLz Þ,
where
z ∈fs, tg.
Under these conditions, the joint likelihood of the samples St and Ss can be
written as
f ðSt, Ssjmt, ms, Lt, LsÞ ¼ f ðStjmt, LtÞf ðSsjms, LsÞ
¼ f ðS1t , : : : , SLt jm1t , : : : , mLt , L1t , : : : , LLt Þ
 f ðS1s, : : : , SLs jm1s, : : : , mLs , L1s, : : : , LLs Þ
¼
Y
L
l¼1
f ðSl
tjml
t, Ll
tÞ
Y
L
l¼1
f ðSlsjmls, LlsÞ:
(6.28)
The posterior of the parameters given St and Ss satisfies
pðmt, ms, Lt, LsjSt, SsÞ ∝f ðSt, Ssjmt, ms, Lt, LsÞpðmt, ms, Lt, LsÞ
¼
Y
L
l¼1
f ðSl
tjml
t, Ll
tÞ
Y
L
l¼1
f ðSlsjmls, LlsÞ
Y
L
l¼1
pðml
t, mls, Ll
t, LlsÞ,
(6.29)
where we assume that the priors of the parameters in different classes are
independent:
pðmt, ms, Lt, LsÞ ¼
Y
L
l¼1
pðml
t, mls, Ll
t, LlsÞ:
(6.30)
From Eqs. 6.4 and 6.29,
pðmt, ms, Lt, LsjSt, SsÞ
∝
Y
L
l¼1
f ðSltjmlt, LltÞf ðSlsjmls, LlsÞf ðmlsjLlsÞf ðmltjLltÞpðLls, LltÞ:
(6.31)
We observe that the posterior of the parameters equals the product of the
posteriors of the parameters of each class:
Target Domain
Source Domain
Figure 6.1
Dependency of the source and target domains through their precision matrices
for any class l ∈{1, . . . , L}. [Reprinted from (Karbalayghareh et al., 2018b).]
266
Chapter 6

pðmt, ms, Lt, LsjSt, SsÞ ¼
Y
L
l¼1
pðmlt, mls, Llt, LlsjSlt, SlsÞ,
(6.32)
where
pðml
t, ml
s, Ll
t, LlsjSl
t, SlsÞ
∝f ðSl
tjml
t, Ll
tÞf ðSlsjml
s, LlsÞf ðml
sjLlsÞf ðml
tjLl
tÞpðLls, Ll
tÞ:
(6.33)
Since we are interested in the posterior of the parameters of the target domain,
we integrate out the parameters of the source domain in Eq. 6.32:
pðmt, LtjSt, SsÞ ¼
Z
L1s ≻0, : : : , LLs ≻0
Z
ðRDÞL pðmt, ms, Lt, LsjSt, SsÞdmsdLs
¼
Y
L
l¼1
Z
Lls≻0
Z
RD pðml
t, mls, Ll
t, LlsjSl
t, SlsÞdmlsdLls
¼
Y
L
l¼1
pðmlt, LltjSlt, SlsÞ,
(6.34)
where
pðml
t, Ll
tjSl
t, SlsÞ ¼
Z
Lls≻0
Z
RD pðml
t, mls, Ll
t, LlsjSl
t, SlsÞdmlsdLls
∝f ðSl
tjml
t, Ll
tÞf ðml
tjLl
tÞ

Z
Lls≻0
Z
RD f ðSlsjmls, LlsÞf ðmlsjLlsÞpðLls, Ll
tÞdmlsdLls:
(6.35)
Theorem 6.4 derives the posterior for the target domain. To prove it, we
require two theorems. The first one is essentially a restatement of Theorem 2.4
for the Wishart distribution instead of the inverse-Wishart distribution.
Theorem 6.2 (Muirhead, 2009). If S ¼ fx1, : : : , xng, where xi is a D  1 vector
and xi  N ðm, L1Þ for i ¼ 1, : : : , n, and ðm, LÞ has a normal-Wishart prior
such that mjL  N ðm, ðnLÞ1Þ and L  WishartðM, kÞ, then the posterior of
ðm, LÞ upon observing S is also a normal-Wishart distribution:
mjL, S  N ðmn, ðnnLÞ1Þ,
(6.36)
LjS  WishartðMn, knÞ,
(6.37)
267
Optimal Bayesian Transfer Learning

where
nn ¼ n þ n,
(6.38)
kn ¼ k þ n,
(6.39)
mn ¼ nm þ n¯x
n þ n
,
(6.40)
M1
n
¼ M1 þ ðn  1ÞˆS þ
nn
n þ n ðm  ¯xÞðm  ¯xÞT,
(6.41)
¯x is the sample mean, and ˆS is the sample covariance matrix.
Theorem 6.3 (Gupta et al., 2016). Let Z be a symmetric positive definite
matrix, X a symmetric matrix, and a . ðD  1Þ∕2. Then
Z
R≻0
etrðZRÞjRjaDþ1
2 pF qða1, : : : , ap; b1, : : : , bq; XRÞdR
¼
Z
R≻0
etrðZRÞjRjaDþ1
2 pF qða1, : : : , ap; b1, : : : , bq; R
1
2XR
1
2ÞdR
¼ GDðaÞjZjapþ1Fqða1, : : : , ap, a; b1, : : : , bq; XZ1Þ:
(6.42)
Theorem 6.4 (Karbalayghareh et al., 2018b). Given the target St and source
Ss samples, the posterior distribution of target mean ml
t and target precision
matrix Ll
t for the class l ∈f1, : : : , Lg has a Gauss-hypergeometric-function
distribution:
pðml
t, Ll
tjSl
t, SlsÞ ¼ AljLl
tj
1
2 exp

 nl
t,n
2 ðml
t  ml
t,nÞTLl
tðml
t  ml
t,nÞ

 jLl
tj
klþnl
tD1
2
etr

 1
2 ðTl
tÞ1Ll
t

 1F1
kl þ nls
2
; kl
2 ; 1
2 FlLl
tðFlÞTTls

,
(6.43)
where Al is the constant of proportionality, given by
268
Chapter 6

ðAlÞ1 ¼
2p
nlt,n
D
22
Dðklþnl
tÞ
2
GD
kl þ nlt
2

jTl
tj
klþnl
t
2
 2F1
kl þ nls
2
, kl þ nlt
2
; kl
2 ; TlsFlTl
tðFlÞT

(6.44)
when Fl is full rank or Fl ¼ 0DD, and
nl
t,n ¼ nl
t þ nl
t,
(6.45)
ml
t,n ¼ nl
tml
t þ nl
t ¯xl
t
nl
t þ nl
t
,
(6.46)
ðTltÞ1 ¼ ðMltÞ1 þ ðFlÞTClFl
þ ðnl
t  1ÞˆSl
t þ
nl
tnl
t
nl
t þ nl
t
ðml
t  ¯xl
tÞðml
t  ¯xl
tÞT,
(6.47)
ðTlsÞ1 ¼ ðClÞ1 þ ðnls  1ÞˆSl
s þ
nlsnls
nls þ nls
ðmls  ¯xlsÞðmls  ¯xlsÞT,
(6.48)
with ¯xlz and ˆSl
z being the sample mean and covariance for z ∈fs, tg and
l ∈f1, : : : , Lg, respectively.
Proof. From Eq. 6.2, for each domain z ∈fs, tg,
f ðSlzjmlz, LlzÞ ¼ ð2pÞ
dnlz
2 jLlzj
nlz
2 exp

 1
2 qlz

,
(6.49)
where
qlz ¼
X
nlz
i¼1
ðxl
z,i  mlzÞTLlzðxl
z,i  mlzÞ:
(6.50)
Moreover, from Eq. 6.5, for each domain z ∈fs, tg,
f ðml
zjLlzÞ ¼ ð2pÞD
2ðnlzÞ
D
2jLlzj
1
2 exp

 nlz
2 ðml
z  mlzÞTLlzðml
z  mlzÞ

:
(6.51)
From Eqs. 6.21, 6.35, 6.49, and 6.51,
269
Optimal Bayesian Transfer Learning

pðml
t, Ll
tjSl
t, SlsÞ
∝jLltj
nl
t
2 exp

 1
2 qlt

jLltj
1
2 exp

 nlt
2 ðmlt  mltÞTLltðmlt  mltÞ

 jLltj
klD1
2
etr

 1
2
h
ðMltÞ1 þ ðFlÞTClFli
Llt


Z
Lls≻0
Z
RD jLlsj
nls
2 exp

 1
2 qls

 jLlsj
1
2 exp

 nls
2 ðmls  mlsÞTLlsðmls  mlsÞ

 jLlsj
klD1
2
etr

 1
2 ðClÞ1Lls

 0F1
kl
2 ; 1
4 ðLlsÞ
1
2FlLl
tðFlÞTðLlsÞ
1
2

dmlsdLls
¼ jLltj
1
2 exp

 nl
t,n
2 ðmlt  mlt,nÞTLltðmlt  mlt,nÞ

 jLltj
klþnl
tD1
2
etr

 1
2 ðTltÞ1Llt


Z
Lls≻0
Z
RD jLlsj
1
2 exp

 nls,n
2 ðml
s  mls,nÞTLlsðml
s  mls,nÞ

 jLlsj
klþnlsD1
2
etr

 1
2 ðTlsÞ1Lls

 0F1
kl
2 ; 1
4 ðLlsÞ
1
2FlLl
tðFlÞTðLlsÞ
1
2

dmlsdLls,
(6.52)
where the reduction of the equality utilizes Theorem 6.2, and where
nl
t,n ¼ nl
t þ nl
t,
(6.53)
nls,n ¼ nls þ nls,
(6.54)
ml
t,n ¼ nl
tml
t þ nl
t ¯xl
t
nlt þ nlt
,
(6.55)
mls,n ¼ nlsmls þ nls ¯xls
nls þ nls
,
(6.56)
270
Chapter 6

ðTl
tÞ1 ¼ ðMl
tÞ1 þ ðFlÞTClFl
þ ðnlt  1ÞˆSl
t þ
nltnlt
nl
t þ nl
t
ðmlt  ¯xltÞðmlt  ¯xltÞT,
(6.57)
ðTlsÞ1 ¼ ðClÞ1 þ ðnls  1ÞˆSl
s þ
nlsnls
nls þ nls
ðmls  ¯xlsÞðmls  ¯xlsÞT,
(6.58)
with sample mean ¯xlz and sample covariance matrix ˆSl
z for z ∈fs, tg. Using
the equation
Z
RD exp

 1
2 ðx  mÞTLðx  mÞ

dx ¼ ð2pÞ
D
2jLj1
2
(6.59)
and integrating out mls in Eq. 6.52 yields
pðml
t, Ll
tjSl
t, SlsÞ
∝jLl
tj
1
2 exp

 nl
t,n
2 ðml
t  ml
t,nÞTLl
tðml
t  ml
t,nÞ

 jLl
tj
klþnl
tD1
2
etr

 1
2 ðTl
tÞ1Ll
t


Z
Lls≻0
jLlsj
klþnlsD1
2
etr

 1
2 ðTlsÞ1Lls

 0F1
kl
2 ; 1
4 ðLlsÞ
1
2FlLltðFlÞTðLlsÞ
1
2

dLls:
(6.60)
The integral I in Eq. 6.60 can be evaluated using Theorem 6.3:
I ¼ GD
kl þ nls
2

j2Tlsj
klþnls
2
1F 1
kl þ nls
2
; kl
2 ; 1
2 FlLl
tðFlÞTTls

,
(6.61)
where 1F1ða; b; XÞ is the confluent hypergeometric function with the matrix
argument X. As a result, Eq. 6.60 becomes
pðml
t, Ll
tjSl
t, SlsÞ ¼ AljLl
tj
1
2 exp

 nl
t,n
2 ðml
t  ml
t,nÞTLl
tðml
t  ml
t,nÞ

 jLl
tj
klþnl
tD1
2
etr

 1
2 ðTl
tÞ1Ll
t

 1F1
kl þ nls
2
; kl
2 ; 1
2 FlLl
tðFlÞTTls

,
(6.62)
271
Optimal Bayesian Transfer Learning

where the constant of proportionality Al makes the integration of the
posterior pðml
t, Ll
tjSl
t, SlsÞ with respect to ml
t and Ll
t equal to 1. Hence,
ðAlÞ1 ¼
Z
Ll
t≻0
jLl
tj
klþnl
tD1
2
etr

 1
2 ðTl
tÞ1Ll
t

jLl
tj
1
2

Z
RD exp

 nl
t,n
2 ðml
t  ml
t,nÞTLl
tðml
t  ml
t,nÞ

dml
t
 1F1
kl þ nls
2
; kl
2 ; 1
2 FlLl
tðFlÞTTls

dLl
t:
(6.63)
Using Eq. 6.59, the inner integral equals
ð2pÞ
D
2jnl
t,nLl
tj1
2 ¼
2p
nl
t,n
D
2jLl
tj1
2:
(6.64)
Hence,
ðAlÞ1 ¼
2p
nl
t,n
D
2 Z
Ll
t≻0
jLl
tj
klþnl
tD1
2
etr

 1
2 ðTl
tÞ1Ll
t

 1F 1
kl þ nls
2
; kl
2 ; 1
2 FlLltðFlÞTTls

dLlt:
(6.65)
Suppose that Fl is full rank. With the variable change V ¼ FlLl
tðFlÞT, we have
dV ¼ jFljDþ1dLl
t,
(6.66)
Llt ¼ ðFlÞ1V½ðFlÞT1:
(6.67)
The region Ll
t ≻0 corresponds to V ≻0. Furthermore,
trðABCDÞ ¼ trðBCDAÞ ¼ trðCDABÞ ¼ trðDABCÞ,
(6.68)
and jABCj ¼ jAjjBjjCj. Thus, Al can be derived as
ðAlÞ1 ¼
2p
nl
t,n
D
2jFljðklþnl
tÞ

Z
V≻0
jVj
klþnl
tD1
2
etr

 1
2 ½ðFlÞT1ðTl
tÞ1ðFlÞ1V

 1F1
kl þ nls
2
; kl
2 ; 1
2 VTls

dV
¼
2p
nl
t,n
D
22
Dðklþnl
tÞ
2
GD
kl þ nl
t
2

jTl
tj
klþnl
t
2
 2F1
kl þ nls
2
, kl þ nl
t
2
; kl
2 ; TlsFlTl
tðFlÞT

,
(6.69)
272
Chapter 6

where the second equality follows from Theorem 6.3, and 2F1ða, b; c; XÞ is
the Gauss hypergeometric function with the matrix argument X. Hence, we
have derived the closed-form posterior distribution of the target parameters
ðml
t, Ll
tÞ in Eq. 6.43, where Al is given by Eq. 6.44.
Suppose that Fl ¼ 0DD, so that the 1F1 term in Eq. 6.65 equals 1, and
ðAlÞ1 ¼
2p
nl
t,n
D
2 Z
Ll
t≻0
jLltj
klþnl
tD1
2
etr

 1
2 ðTltÞ1Llt

dLlt:
(6.70)
The integrand is essentially a Wishart distribution; hence,
ðAlÞ1 ¼
2p
nl
t,n
D
22
ðklþnl
tÞD
2
jTltj
klþnl
t
2 GD
kl þ nlt
2

:
(6.71)
This is consistent with Eq. 6.44.
▪
6.3 Optimal Bayesian Transfer Learning Classifier
The optimal Bayesian transfer learning classifier (OBTLC) is the OBC in the
target domain given data from both the target and source datasets. To find the
OBTLC, we need to derive the effective class-conditional densities using the
posterior of the target parameters derived from both the target and source
datasets. Expressing the effective class-conditional density for class l explicitly
in terms of the model parameters, it is given by
f OBTLC
U
ðxjlÞ ¼
Z
Ll
t≻0
Z
RD f ðxjmlt, LltÞp∗ðmlt, LltÞdmltdLlt,
(6.72)
where
p∗ðml
t, Ll
tÞ ¼ pðml
t, Ll
tjSl
t, SlsÞ
(6.73)
is the posterior of ðml
t, Ll
tÞ upon observation of Sl
t and Sls.
Theorem 6.5 (Karbalayghareh et al., 2018b). Suppose that Fl is full rank or
Fl ¼ 0DD. Then the effective class-conditional density in the target domain for
class l is given by
273
Optimal Bayesian Transfer Learning

f OBTLC
U
ðxjlÞ
¼ pD
2
nlt,n
nlx
D
2
GD
kl þ nlt þ 1
2

G1
D
kl þ nlt
2

jTlxj
klþnl
tþ1
2
jTl
tj
klþnl
t
2
 2F 1
kl þ nls
2
, kl þ nl
t þ 1
2
; kl
2 ; TlsFlTlxðFlÞT

 2F 1
1
kl þ nls
2
, kl þ nlt
2
; kl
2 ; TlsFlTltðFlÞT

,
(6.74)
where
nlx ¼ nlt,n þ 1 ¼ nlt þ nlt þ 1,
(6.75)
ðTlxÞ1 ¼ ðTl
tÞ1 þ
nlt,n
nl
t,n þ 1 ðml
t,n  xÞðml
t,n  xÞT:
(6.76)
Proof. The likelihood f ðxjmlt, LltÞ and posterior pðmlt, LltjSlt, SlsÞ are given in
Eqs. 6.2 and 6.43, respectively. Hence,
f OBTLC
U
ðxjlÞ ¼ ð2pÞD
2Al
Z
Ll
t≻0
Z
RD jLl
tj
1
2 exp

 1
2 ðx  ml
tÞTLl
tðx  ml
tÞ

 jLl
tj
1
2 exp

 nl
t,n
2 ðml
t  ml
t,nÞTLl
tðml
t  ml
t,nÞ

 jLl
tj
klþnl
tD1
2
etr

 1
2 ðTl
tÞ1Ll
t

 1F1
kl þ nls
2
; kl
2 ; 1
2 FlLl
tðFlÞTTls

dml
tdLl
t
¼ ð2pÞD
2Al
Z
Ll
t≻0
Z
RD jLl
tj
1
2 exp

 nlx
2 ðml
t  mlxÞTLl
tðml
t  mlxÞ

 jLl
tj
klþnl
tþ1D1
2
etr

 1
2 ðTlxÞ1Ll
t

 1F1
kl þ nls
2
; kl
2 ; 1
2 FlLl
tðFlÞTTls

dml
tdLl
t,
(6.77)
where
nlx ¼ nl
t,n þ 1 ¼ nl
t þ nl
t þ 1,
(6.78)
274
Chapter 6

mlx ¼ nl
t,nml
t,n þ x
nt,n þ 1
,
(6.79)
ðTlxÞ1 ¼ ðTl
tÞ1 þ
nlt,n
nlt,n þ 1 ðml
t,n  xÞðml
t,n  xÞT:
(6.80)
The integration in Eq. 6.77 is similar to the one in Eq. 6.63. As a result, using
Eq. 6.44,
f OBTLC
U
ðxjlÞ ¼ ð2pÞD
2Al
2p
nlx
D
22
Dðklþnl
tþ1Þ
2
GD
kl þ nl
t þ 1
2

 jTlxj
klþnl
tþ1
2
2F1
kl þ nls
2
, kl þ nl
t þ 1
2
; kl
2 ; TlsFlTlxðFlÞT

:
(6.81)
By replacing the value of Al, we have the effective class-conditional density as
stated in Eq. 6.74.
▪
A Dirichlet prior is assumed for the prior probabilities cl
t that the
target sample belongs to class l: ct ¼ ½c1t , : : : , cLt   DirichletðatÞ, where
at ¼ ½a1t , : : : , aLt  is the vector of concentration parameters, and al
t . 0 for
l ∈f1, : : : , Lg. As the Dirichlet distribution is a conjugate prior for the
categorical distribution, upon observing nt ¼ ½n1t , : : : , nLt  sample points for
class l in the target domain, the posterior p∗ðctÞ ¼ pðctjntÞ has a Dirichlet
distribution,
Dirichletðat þ ntÞ ¼ Dirichletða1t þ n1t , : : : , aLt þ nLt Þ,
(6.82)
with
Ep∗½cl
t ¼ al
t þ nl
t
a0t þ Nt
,
(6.83)
where Nt ¼ PL
l¼1 nl
t, and a0
t ¼ PL
l¼1 al
t.
The OBTLC in the target domain is given by
cOBTLCðxÞ ¼ arg
max
l∈f1, : : : , LgEp∗½cl
t f OBTLC
U
ðxjlÞ:
(6.84)
If we lack prior knowledge for class selection, then we use the same
concentration parameter for all classes: at ¼ ½at, : : : , at. Hence, if the
number of samples in each class is the same, i.e., n1t ¼ ··· ¼ nLt , then Ep∗½cl
t is
the same for all classes, and Eq. 6.84 reduces to
275
Optimal Bayesian Transfer Learning

cOBTLCðxÞ ¼ arg
max
l∈f1, : : : , Lgf OBTLC
U
ðxjlÞ:
(6.85)
The effective class-conditional densities are derived in closed forms in
Eq. 6.74; however, deriving the OBTLC in Eq. 6.84 requires computing the
Gauss hypergeometric function of one matrix argument. Computing the exact
values of hypergeometric functions of a matrix argument using the series of
zonal polynomials, as in Eq. 6.20, is time-consuming and is not scalable to
high dimension. To facilitate computation, one can use the Laplace
approximation of this function, as in (Butler and Wood, 2002), which is
computationally efficient and scalable. The Laplace approximation is
discussed in (Karbalayghareh et al., 2018b).
6.3.1 OBC in the target domain
The beneficial effect of the source data can be seen by comparing the
OBTLC with the OBC based solely on target data. From Eqs. 6.5 and 6.27,
the priors for ml
t and Ll
t are given by ml
tjLl
t  N ðml
t, ðnl
tLl
tÞ1Þ and
Ll
t  WishartðMl
t, klÞ, respectively. Using Theorem 6.2, upon observing the
sample Sl
t, the posteriors of ml
t and Ll
t will be
ml
tjLl
t, Sl
t  N ðml
t,n, ðnl
t,nLl
tÞ1Þ,
(6.86)
Ll
tjSl
t  WishartðMl
t,n, kl
t,nÞ,
(6.87)
where
nl
t,n ¼ nl
t þ nl
t,
(6.88)
kl
t,n ¼ kl þ nl
t,
(6.89)
ml
t,n ¼ nl
tml
t þ nl
t ¯xl
t
nl
t þ nl
t
,
(6.90)
ðMl
t,nÞ1 ¼ ðMl
tÞ1 þ ðnl
t  1ÞˆSl
t þ
nl
tnl
t
nl
t þ nl
t
ðml
t  ¯xl
tÞðml
t  ¯xl
tÞT,
(6.91)
and ¯xl
t and ˆSl
t are the sample mean and covariance, respectively. The effective
class-conditional densities for the OBC are given by
f OBC
U
ðxjlÞ ¼ pD
2

nl
t,n
nl
t,n þ 1
D
2
GD
kl þ nl
t þ 1
2

 G1
D
kl þ nl
t
2

jMlxj
klþnl
tþ1
2
jMl
t,nj
klþnl
t
2 ,
(6.92)
276
Chapter 6

where
ðMlxÞ1 ¼ ðMl
t,nÞ1 þ
nlt,n
nl
t,n þ 1 ðml
t,n  xÞðml
t,n  xÞT:
(6.93)
Note that if L  WishartðM, kÞ, then S ¼ L1  inverse-WishartðM1, kÞ.
Thus, as expected, Eq. 6.92 is equivalent to Eq. 2.120 with nl
t,n in place of n∗,
mlt,n in place of m∗, kl þ nlt in place of k∗, and ðMlt,nÞ1 in place of S∗.
The multi-class OBC under a zero-one loss function is given by
cOBCðxÞ ¼ arg
max
l∈f1, : : : , LgEp∗½cl
t f OBC
U
ðxjlÞ:
(6.94)
In the case of equal expected prior probabilities for the classes,
cOBCðxÞ ¼ arg
max
l∈f1, : : : , Lgf OBC
U
ðxjlÞ:
(6.95)
The next two theorems state that, if there is no interaction between the
source and target domains in all of the classes, or there is no source data, then
the OBTLC reduces to the OBC in the target domain.
Theorem
6.6
(Karbalayghareh
et
al.,
2018b).
If
Mlts ¼ 0DD
for
all
l ∈f1, : : : , Lg, then cOBTLC ¼ cOBC.
Proof.
If
Mlts ¼ 0DD
for
all
l ∈f1, : : : , Lg,
then
Fl ¼ 0DD.
Since
2F1ða, b; c; 0DDÞ ¼ 1 for any values of a, b, and c, the Gauss hypergeometric
functions will disappear in Eq. 6.74. From Eqs. 6.47 and 6.91, Tl
t ¼ Ml
t,n.
From Eqs. 6.76 and 6.93, Tlx ¼ Mlx. Thus, f OBTLC
U
ðxjlÞ ¼ f OBC
U
ðxjlÞ, and
consequently, cOBTLCðxÞ ¼ cOBCðxÞ.
▪
Theorem 6.7. If nls ¼ 0 for all l ∈f1, : : : , Lg, then cOBTLC ¼ cOBC.
Proof. When nls ¼ 0, the posterior pðmlt, LltjSlt, SlsÞ in Eq. 6.43 is given by
pðml
t, Ll
tjSl
t, SlsÞ ∝jLl
tj
1
2 exp

 nl
t,n
2 ðml
t  ml
t,nÞTLl
tðml
t  ml
t,nÞ

 jLl
tj
klþnl
tD1
2
etr

 1
2 ðTl
tÞ1Ll
t

 etr
1
2 FlLl
tðFlÞTTls

,
(6.96)
277
Optimal Bayesian Transfer Learning

where we have used the property 1F1ða; a; XÞ ¼0F 0ðXÞ and Eq. 6.16. By
Eqs. 6.47 and 6.91, ðTltÞ1 ¼ ðMlt,nÞ1 þ ðFlÞTClFl. Further, in the absence of
data, Tls ¼ Cl by Eq. 6.48. Thus,
etr

 1
2 ðTl
tÞ1Ll
t

etr
1
2 FlLl
tðFlÞTTls

¼ etr

 1
2 ðMl
t,nÞ1Ll
t

:
(6.97)
Combining this with Eq. 6.96, we have that pðml
t, Ll
tjSl
t, SlsÞ is precisely the
posterior given by Eqs. 6.86 and 6.87. Thus, f OBTLC
U
ðxjlÞ ¼ f OBC
U
ðxjlÞ, and
cOBTLCðxÞ ¼ cOBCðxÞ.
▪
Example 6.1. Following (Karbalayghareh et al., 2018b), the aim is to
investigate the effect of the relatedness of the source and target domains.
The simulation setup assumes that D ¼ 10, L ¼ 2, the number of source
training data per class is ns ¼ nls ¼ 200, the number of target training data per
class is nt ¼ nl
t ¼ 10, k ¼ kl ¼ 25, nt ¼ nl
t ¼ 100, ns ¼ nls ¼ 100, m1t ¼ 0D,
m2t ¼ 0.05 ⋅1D, m1s ¼ m1t þ 1D, and m2s ¼ m2t þ 1D. For the scale matrix Ml in
Eq. 6.22, Ml
t ¼ ktID, Mls ¼ ksID, and Ml
ts ¼ ktsID for l ¼ 1, 2. Choosing an
identity matrix for Mlts makes sense when the order of the features in the two
domains is the same. Ml must be positive definite for any class l. Hence,
we have the following constraints on kt, ks, and kts: kt . 0, ks . 0, and
jktsj ,
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ktks
p
. Let kts ¼ a
ﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ktks
p
, where jaj , 1. The value of jaj shows the
amount of relatedness between the source and target domains. If jaj ¼ 0, then
the two domains are unrelated; as jaj approaches 1, there is greater
relatedness. We set kt ¼ ks ¼ 1 and plot the average classification error
curves for different values of jaj. All of the simulations assume equal prior
probabilities for the classes.
Prediction performance is measured by average classification errors.
To sample from the prior in Eq. 6.4, first sample from a 2D  2D
WishartðMl, klÞ distribution for each class l ¼ 1, 2. For a fixed l, call the
upper-left D  D block of this matrix Llt, and the lower-right D  D block Lls.
By Theorem 6.1, ðLl
t, LlsÞ is a joint sample from pðLl
t, LlsÞ in Eq. 6.21. Given
Ll
t and Lls, sample from Eq. 6.5 to get samples of ml
t and ml
s for l ¼ 1, 2.
Having ml
t, mls, Ll
t, and Lls, generate 100 different training and test sets from
Eq. 6.2. Training sets contain samples from both the target and source
domains, but the test set contains only samples from the target domain. Since
the numbers of source and target training data per class are ns and nt, there are
Lns and Lnt source and target training data in total, respectively. The size of
the test set per class is 1000. For each training and test set, use the OBTLC
and its target-only version, OBC, and calculate the error. Then average
all of the errors for 100 different training and test sets. Repeat the process
278
Chapter 6

1000 times for different realizations of Ll
t, Lls, ml
t, and ml
s for l ¼ 1, 2, and
finally average all of the errors and return the average classification error. In
all figures, the OBTLC hyperparameters are the same as those used for
simulating data, except for the figures showing the sensitivity of the
performance with respect to different hyperparameters, in which case it is
assumed that the true values of the hyperparameters used for simulating data
are unknown.
Figure 6.2(a) demonstrates how the source data improve the classifier in
the target domain. It shows the average classification error versus nt for the
OBC and OBTLC with different values of a. When a is close to 1, the
performance of the OBTLC is much better than that of the OBC, owing to
strong relatedness. Performance improvement is especially noticeable when
nt is small. Moreover, the errors of the OBTLC and OBC converge to
a similar value when nt gets very large, meaning that the source data are
redundant when there is a large amount of target data. When a is larger,
the error curves converge faster to the optimal error, which is the average
Bayes error of the target classifier. The corresponding Bayes error averaged
over 1000 randomly generated distributions equals 0.122. When a ¼ 0, the
OBTLC reduces to the OBC. By Eq. 6.74, the sign of a does not matter in the
performance of the OBTLC. Hence, we can use jaj in all cases. Figure 6.2(b)
depicts average classification error versus ns for the OBC and OBTLC with
different values of a. The error of the OBC is constant for all ns. The error of
the OBTLC equals that of the OBC when ns ¼ 0 and starts to decrease as ns
increases. A key point is that having a very large collection of source data
when the two domains are highly related can compensate for the lack of target
data and lead to a target classification error approaching an error not much
greater than the Bayes error in the target domain.
(a)
(b)
Figure 6.2
Average classification error in transfer learning (k ¼ 25): (a) versus the number
of target training data per class nt (ns ¼ 200); (b) versus the number of source training data
per class ns (nt ¼ 10). [Reprinted from (Karbalayghareh et al., 2018b).]
279
Optimal Bayesian Transfer Learning

Figure 6.3 treats
robustness
of
the
OBTLC
with
respect
to
the
hyperparameters. It shows the average classification error of the OBTLC with
respect to jaj, under the assumption that the true value atrue of the amount of
relatedness between source and target domains is unknown. The four parts
show error curves for atrue ¼ 0.3, 0.5, 0.7, 0.9. In all cases, the OBTLC
performance gain depends on the relatedness ðvalue of atrueÞ and the value of a
used in the classifier. Maximum gain is achieved at jaj ¼ atrue (not exactly at
atrue due to the Laplace approximation of the Gauss hypergeometric function).
Performance gain is higher when the two domains are highly related. When the
two domains are only modestly related, choosing large jaj leads to higher
performance loss compared to the OBC. This means that exaggeration in the
amount of relatedness between the two domains can result in “negative”
transfer learning. Figure 6.4 shows the errors versus k, assuming unknown true
value ktrue, for a ¼ 0.5, a ¼ 0.9, ktrue ¼ 25, and ktrue ¼ 50. The salient point
here is that OBTLC performance is not very sensitive to k if it is chosen in its
(a)
(b)
(c)
(d)
Figure 6.3
Average classification error versus jaj (k ¼ 25): (a) atrue ¼ 0.3; (b) atrue ¼ 0.5;
(c) atrue ¼ 0.7; (d) atrue ¼ 0.9. [Reprinted from (Karbalayghareh et al., 2018b).]
280
Chapter 6

allowable range, that is, k ≥2D. Figure 6.5 depicts average classification error
versus nt for a ¼ 0.5 and a ¼ 0.9, where the true value of nt is ntrue ¼ 50. Note
that the discrepancy between OBC in Figures 6.5(a) and (b) is only due to the
small number of iterations. If nt ≥20, then performance is fairly constant.
Figures 6.3, 6.4, and 6.5 reveal that, at least in the current simulation, OBTLC
performance improvement depends on the value of a and true relatedness atrue
between the two domains, and is robust with respect to the choices of the
hyperparameters k and nt. A reasonable range of a provides improved
performance, but a decent estimate of relatedness is important.
6.4 OBTLC with Negative Binomial Distribution
The negative binomial distribution is used to model RNA sequencing
(RNA-Seq) read counts owing to its suitability for modeling the over-dispersion
(a)
(b)
(c)
(d)
Figure 6.4
Average classification error versus k: (a) a ¼ 0.5, ktrue ¼ 25; (b) a ¼ 0.5,
ktrue ¼ 50; (c) a ¼ 0.9, ktrue ¼ 25; (d) a ¼ 0.9, ktrue ¼ 50. [Reprinted from (Karbalayghareh et
al., 2018b).]
281
Optimal Bayesian Transfer Learning

of the counts, meaning that the variance can be much greater than the mean
(Anders and Huber, 2010; Dadaneh et al., 2018b). In this section we will apply
optimal Bayesian transfer learning to count data using the negative binomial
distribution (Karbalayghareh et al., 2019). Although the theory is independent
of the application, we use gene terminology because it gives an empirical flavor
to the development.
In the current setting, Sls ¼ fxl
s,i,jg contains the nls sample points with the
counts of D genes in the source domain for the class l, where l ∈f1, : : : , Lg,
i ∈f1, : : : , Dg, and j ∈f1, : : : , nlsg. Analogously, Sl
t ¼ fxl
t,i,jg is the sample
for the target domain, containing nlt sample points with the counts of D genes
in the target domain for the class l. The counts xl
z,i,j are modeled by a negative
binomial distribution xl
z,i,j  negative-binomialðml
z,i, rl
z,iÞ, with the probability
mass function
Prðxl
z,i,j ¼ kjml
z,i, rl
z,iÞ ¼
Gðk þ rl
z,iÞ
Gðrl
z,iÞGðk þ 1Þ

ml
z,i
ml
z,i þ rl
z,i
k
rl
z,i
ml
z,i þ rl
z,i
rl
z,i
,
(6.98)
where ml
z,i and rl
z,i are the mean and shape parameters, respectively, of gene i
in domain z and class l. The mean and variance of xl
z,i,j are E½xl
z,i,j ¼ ml
z,i and
varðxl
z,i,jÞ ¼ ml
z,i þ ðml
z,iÞ2
rl
z,i
,
(6.99)
respectively.
Let the mean vector m contain all ml
s,i and ml
t,i for l ¼ 1, : : : , L and
i ¼ 1, : : : , D, and let the shape vector r contain all rl
s,i and rl
t,i for l ¼ 1, : : : , L
(a)
(b)
Figure
6.5
Average
classification
error
versus
nt
(ntrue ¼ 50):
(a)
a ¼ 0.5;
(b) a ¼ 0.9. [Reprinted from (Karbalayghareh et al., 2018b).]
282
Chapter 6

and i ¼ 1, : : : , D. Assuming that the priors for different genes and classes are
independent in each domain, the prior is factored as
pðm, rÞ ¼
Y
L
l¼1
Y
D
i¼1
pðml
s,i, ml
t,iÞpðrl
s,i, rl
t,iÞ:
(6.100)
To define a family of joint priors for two sets of positive parameters, ml
s,i
and ml
t,i, and rl
s,i and rl
t,i, we shall use the following version of Theorem 6.1 that
takes a specialized form with the matrices L and M being 2  2, and L and M
now having real-valued components luv and muv, respectively, where
u, v ∈f1, 2g.
Theorem 6.8 (Halvorsen et al., 2016). Let
L ¼

l11
l12
l12
l22

(6.101)
be a 2  2 Wishart random matrix with k ≥2 degrees of freedom and positive
definite scale matrix M. The joint distribution of the two diagonal entries l11
and l22 has density function
f ðl11, l22Þ ¼ K exp

 1
2

m1
11 þ c2f 2

l11

exp

 1
2 c1
2 l22

 ðl11Þ
k
21ðl22Þ
k
210F1
k
2 ; 1
4 f 2l11l22

,
(6.102)
where c2 ¼ m22  m2
12m1
11 , f ¼ c1
2 m12m1
11 , and K1¼ 2kG2ðk∕2ÞjMjk∕2.
We shall also apply the following lemma.
Lemma
6.1
(Muirhead,
2009).
If
2  2
L  WishartðM, kÞ,
then
lii∕mii  chi-squaredðkÞ
for
i ¼ 1, 2.
In
particular,
E½lii ¼ kmii,
and
varðliiÞ ¼ 2km2
ii for i ¼ 1, 2. Furthermore, the covariance and correlation
coefficient
between
l11
and
l22
are
covðl11, l22Þ ¼ 2km2
12
and
rl ¼ m2
12m1
11 m1
22 , respectively.
For each i ∈f1, : : : , Dg and l ∈f1, : : : , Lg, given L in Eq. 6.101, we
define the joint prior for ml
s,i and ml
t,i to take the form of the joint density of the
diagonals given in Eq. 6.102. Without loss of generality, we also parameterize
the prior using the correlation coefficient given in Lemma 6.1. In particular,
283
Optimal Bayesian Transfer Learning

pðml
s,i, ml
t,iÞ ¼ Kl
m,i exp


ml
s,i
2ml
s,ið1  rl
m,iÞ

exp


ml
t,i
2ml
t,ið1  rl
m,iÞ

 ðml
s,iÞ
km
2 1ðml
t,iÞ
km
2 10F 1
km
2 ;
rl
m,i
4ml
s,iml
t,ið1  rl
m,iÞ2 ml
s,iml
t,i

:
(6.103)
The hyperparameters are ml
t,i, ml
s,i, rl
m,i, and km, and the normalization
constant Kl
m,i depends on these parameters. By Lemma 6.1, note that
E½ml
z,i ¼ kmml
z,i and varðml
z,iÞ ¼ 2kmðml
z,iÞ2 for z ∈fs, tg, and we have the
correlation
coefficient
corrðml
s,i, ml
t,iÞ ¼ rl
m,i.
Similarly,
for
every
i ∈f1, : : : , Dg and l ∈f1, : : : , Lg, we can define the joint prior for rl
s,i and
rl
t,i by
pðrl
s,i, rl
t,iÞ ¼ Kl
r,i exp


rl
s,i
2sl
s,ið1  rl
r,iÞ

exp


rl
t,i
2sl
t,ið1  rl
r,iÞ

 ðrl
s,iÞ
kr
2 1ðrl
t,iÞ
kr
2 10F1
kr
2 ;
rl
r,i
4sl
s,isl
t,ið1  rl
r,iÞ2 rl
s,irl
t,i

,
(6.104)
with hyperparameters sl
t,i, sl
s,i, rl
r,i, and kr, and moments E½rl
z,i ¼ krsl
z,i,
varðrl
z,iÞ ¼ 2krðsl
z,iÞ2 for z ∈fs, tg, and corrðrl
s,i, rl
t,iÞ ¼ rl
r,i.
Independence
assumptions
for
all
genes
i ∈f1, : : : , Dg,
classes
l ∈f1, : : : , Lg, z ∈fs, tg, and sample points j ∈f1, : : : , nlzg are adopted for
the joint data likelihood function:
f ðSs, Stjm, rÞ ¼
Y
z∈fs,tg
Y
L
l¼1
Y
D
i¼1
Y
nlz
j¼1
f ðxl
z,i,jjml
z,i, rl
z,iÞ:
(6.105)
Therefore, the posteriors of different genes in different classes can be
factored as
pðml
s,i, ml
t,i, rl
s,i, rl
t,ijSls, Sl
tÞ
∝pðml
s,i, ml
t,iÞpðrl
s,i, rl
t,iÞ
Y
z∈fs,tg
Y
nlz
j¼1
f ðxl
z,i,jjml
z,i, rl
z,iÞ
(6.106)
for all i ∈f1, : : : , Dg and l ∈f1, : : : , Lg. Integrating out the source
parameters in Eq. 6.106 yields
284
Chapter 6

pðml
t,i, rl
t,ijSls, Sl
tÞ
∝
Z `
0
Z `
0
pðml
s,i, ml
t,iÞpðrl
s,i, rl
t,iÞ
Y
z∈fs,tg
Y
nlz
j¼1
f ðxl
z,i,jjml
z,i, rl
z,iÞdml
s,idrl
s,i:
(6.107)
Since the joint prior is not conjugate for the joint likelihood, the joint
posteriors and the target posteriors will not have closed forms. MCMC
methods are needed to obtain posterior samples in the target domain.
Hamilton Monte Carlo (HMC) (Neal, 2011) is used in (Karbalayghareh et al.,
2019).
The effective class-conditional densities are given by
f OBTLC
U
ðxjlÞ ¼
Z
rl
t.0
Z
ml
t.0
f ðxjml
t, rl
tÞp∗ðml
t, rl
tÞdml
tdrl
t
(6.108)
for l ∈f1, : : : , Lg, where ml
t contains all ml
t,i for i ¼ 1, : : : , D, rl
t contains all
rl
t,i for i ¼ 1, : : : , D, and p∗ðmlt, rltÞ ¼ QD
i¼1 pðml
t,i, rl
t,ijSlt, SlsÞ. Since we do not
have closed form for the posterior p∗ðml
t, rl
tÞ, the posterior samples generated
by HMC sampling are used to approximate the integration of Eq. 6.108. If
there are N posterior samples from all of the D genes in L classes, then the
approximation is
f OBTLC
U
ðxjlÞ ¼ 1
N
X
N
j¼1
Y
D
i¼1
f ðxij ¯ml
t,i,j, ¯rl
t,i,jÞ,
(6.109)
where xi is the ith gene in x, and ¯ml
t,i,j and ¯rl
t,i,j are the jth posterior samples of
gene i in class l of the target domain for the mean and shape parameters,
respectively. Alternatively, we may write Eq. 6.108 as
f OBTLC
U
ðxjlÞ ¼
Y
D
i¼1
Z `
0
Z `
0
f ðxijml
t,i, rl
t,iÞpðml
t,i, rl
t,ijSl
t, SlsÞdml
t,idrl
t,i,
(6.110)
which is a product of effective densities on each gene. We approximate this by
f OBTLC
U
ðxjlÞ ¼ 1
N
Y
D
i¼1
X
N
j¼1
f ðxij ¯ml
t,i,j, ¯rl
t,i,jÞ,
(6.111)
where the MCMC posterior samples may be drawn for each gene
independently.
As previously done, we assume a Dirichlet prior for the prior class
probabilities. The OBTLC is defined via Eq. 6.84, in this case for the target
parameters cl
t, ml
t,i, and rl
t,i for l ¼ 1, : : : , L and i ¼ 1, : : : , D.
285
Optimal Bayesian Transfer Learning

For deriving the OBC in the target domain using only target data, the
marginal priors for ml
t,i and rl
t,i are scaled chi-squared random variables:
ml
t,i∕ml
t,i  chi-squaredðkmÞ and rl
t,i∕sl
t,i  chi-squaredðkrÞ, with
pðml
t,iÞ ¼

ð2ml
t,iÞ
km
2 G
km
2
1
ml
t,i
km
2 1 exp
ml
t,i
2ml
t,i

,
(6.112)
pðrl
t,iÞ ¼

ð2sl
t,iÞ
kr
2 G
kr
2
1
rl
t,i
kr
2 1 exp
rl
t,i
2sl
t,i

:
(6.113)
Let the mean vector m contain ml
t,i for i ¼ 1, : : : , D and l ¼ 1, : : : , L, and let
the shape vector r contain rl
t,i for i ¼ 1, : : : , D and l ¼ 1, : : : , L. The prior is
factored as
pðm, rÞ ¼
Y
L
l¼1
Y
D
i¼1
pðml
t,iÞpðrl
t,iÞ:
(6.114)
The likelihood of the target data is
f ðStjm, rÞ ¼
Y
L
l¼1
Y
D
i¼1
Y
nl
t
j¼1
f ðxl
t,i,jjml
t,i, rl
t,iÞ:
(6.115)
The posteriors of different genes in different classes can be factored as
pðml
t,i, rl
t,ijSl
tÞ ∝pðml
t,iÞpðrl
t,iÞ
Y
nl
t
j¼1
f ðxl
t,i,jjml
t,i, rl
t,iÞ
(6.116)
for i ∈f1, : : : , Dg and l ∈f1, : : : , Lg. HMC is used to get samples from the
posteriors of the parameters ml
t,i and rl
t,i, and using Eq. 6.109 with these
samples yields an approximation of the effective class-conditional densities for
the OBC. The OBC is then defined using Eq. 6.94.
If all correlations between the mean and shape parameters of the target
and source domains are zero, that is, rl
m,i ¼ 0 and rl
r,i ¼ 0 for i ∈f1, : : : , Dg
and l ∈f1, : : : , Lg, then the joint priors for the OBTLC in Eq. 6.103 become
pðml
s,i, ml
t,iÞ ¼ pðml
s,iÞpðml
t,iÞ and pðrl
s,i, rl
t,iÞ ¼ pðrl
s,iÞpðrl
t,iÞ so that all mean
and shape parameters are independent between the two domains. Having
independent priors and likelihoods leads to independent posteriors for the two
domains, and the OBTLC reduces to the OBC. If, on the other hand, rl
m,i and
rl
r,i are close to 1, then the OBTLC yields significantly better performance
than the OBC.
286
Chapter 6

Chapter 7
Construction of Prior
Distributions
Up to this point we have ignored the issue of prior construction, assuming
that the characterization of uncertainty is known. For optimal Bayesian
classification, the problem consists of transforming scientific knowledge into a
probability distribution governing uncertainty in the feature-label distribu-
tion. Regarding prior construction in general, in 1968, E. T. Jaynes remarked,
“Bayesian methods, for all their advantages, will not be entirely satisfactory
until we face the problem of finding the prior probability squarely” (Jaynes,
1968). Twelve years later, he added, “There must exist a general formal theory
of determination of priors by logical analysis of prior information—and that
to develop it is today the top priority research problem of Bayesian theory”
(Jaynes, 1980).
Historically, prior construction has tended to utilize general methodolo-
gies not targeting any specific type of prior information and has usually been
treated independently (even subjectively) of real available prior knowledge
and sample data. Subsequent to the introduction of the Jeffreys rule prior
(Jeffreys, 1946), objective-based methods were proposed, two early ones
being (Kashyap, 1971) and (Bernardo, 1979). There appeared a series of
information-theoretic and statistical approaches: non-informative priors for
integers (Rissanen, 1983), entropic priors (Rodriguez, 1991), maximal data
information priors (MDIP) (Zellner, 1995), reference (non-informative) priors
obtained through maximization of the missing information (Berger and
Bernardo, 1992), and least-informative priors (Spall and Hill, 1990) [see also
(Bernardo, 1979; Kass and Wasserman, 1996; Berger et al., 2012)]. The
principle of maximum entropy can be seen as a method of constructing least-
informative priors (Jaynes, 1957, 1968). Except in the Jeffreys rule prior,
almost all of the methods are based on optimization: maximizing or
minimizing an objective function, usually an information theoretic one. The
least-informative prior in (Spall and Hill, 1990) is found among a restricted set
of distributions, where the feasible region is a set of convex combinations of
287

certain types of distributions. In (Zellner, 1996) several non-informative and
informative priors for different problems are found. In all of these methods
there is a separation between prior knowledge and observed sample data.
Moreover, although these methods are appropriate tools for generating prior
probabilities, they are quite general methodologies and do not target specific
scientific prior information.
Since, in the case of optimal Bayesian classification, uncertainty is directly
on the feature-label distribution and this distribution characterizes our joint
knowledge of the features and the labels, prior construction is at the scientific
level. A number of prior construction methods will be discussed in this
chapter. In the first two sections, we consider a purely data-driven method
that uses discarded data via the method of moments to determine the
hyperparameters (Dalton and Dougherty, 2011), and a prior distribution
derived from a stochastic differential equation that incompletely describes the
underlying system (Zollanvari and Dougherty, 2016). In the next two sections
we present a general methodology for transforming scientific knowledge into
a prior distribution via a constrained optimization where the constraints
represent scientific knowledge (Esfahani and Dougherty, 2014, 2015; Boluki
et al., 2017).
7.1 Prior Construction Using Data from Discarded Features
If we possess a large set G containing D features from which we have chosen a
subset H, containing d , D features to use for classification, the remaining
D  d features in G \ H can be used for construction of a prior distribution
governing the feature-label distribution of the selected features, the assump-
tion being that they implicitly contain useful information. Since with the
Bayesian approach we are usually concerned with small samples, D should be
much larger than d.
We use an independent general covariance Gaussian model with d
features used for classification. We will focus on one class at a time without
writing the class label y explicitly. Assume distribution parameters u ¼ ðm, SÞ
with S invertible. The prior distribution is defined in Eq. 2.60: the mean
conditioned on the covariance is Gaussian with mean m and covariance S∕n,
and the marginal distribution of the covariance is an inverse-Wishart
distribution. The hyperparameters of pðuÞ are a real number n, a length d
real vector m, a real number k, and a symmetric d  d matrix S. We restrict k
to be an integer to guarantee a closed-form solution. Given n observed sample
points, the posterior has the same form as the prior, with updated
hyperparameters given in Eqs. 2.65 through 2.68. To ensure a proper prior,
we require the following: k . d  1, symmetric positive definite S, and n . 0.
Following (Dalton and Dougherty, 2011a), we use a method-of-moments
approach to calibrate the hyperparameters. Since estimating a vector m and
288
Chapter 7

matrix S may be problematic for a small number of sample points, we limit
the number of model parameters by assuming
m ¼ m ⋅1d
(7.1)
and
S ¼ s2
2
6664
1
r
···
r
r
1
···
r
...
...
..
.
...
r
r
···
1
3
7775,
(7.2)
where m is a real number, s2 . 0, and 1 , r , 1. This structure is justified
because, prior to observing the data, there is no reason to think that any
feature, or pair of features, should have unique properties. In particular, the
structure of S in Eq. 7.2 implies that the prior correlation between any pair of
features does not depend on the indices of the features. Note that correlations
between features are generated randomly, and this structure in hyperpara-
meter S does not imply that correlations between features are assumed to be
the same.
There are five scalars to estimate for each class: n, m, k, s2, and r. To apply
the method of moments, we need the theoretical first and second moments of
the random variables m and S in terms of the desired hyperparameters.
Since S has a marginal prior possessing an inverse-Wishart distribution
with hyperparameters k and S,
E½S ¼
S
k  d  1 .
(7.3)
Given the previously defined structure on S,
s2 ¼ ðk  d  1ÞE½s11,
(7.4)
r ¼ E½s12
E½s11 ,
(7.5)
where sij is the ith row, jth column element of S. Owing to our imposed
structure, only E½s11 and E½s12 are needed.
The variance of the jth diagonal element in an inverse-Wishart distributed
S may be expressed as
varðsjjÞ ¼
2s2
ðk  d  1Þ2ðk  d  3Þ ¼ 2ðE½s11Þ2
k  d  3 ,
(7.6)
where we have applied Eq. 7.4 in the second equality. Solving for k,
289
Construction of Prior Distributions

k ¼ 2ðE½s11Þ2
varðs11Þ þ d þ 3.
(7.7)
Now consider the mean m that is parameterized by the hyperparameters n
and m. The marginal distribution of the mean is a multivariate t-distribution
(Rowe, 2003):
pðmÞ ¼ G
kþ1
2

G
kdþ1
2

ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
nd
pd ⋅
jSj1
½1 þ nðm  mÞTS1ðm  mÞkþ1
s
.
(7.8)
The mean and covariance of this distribution are
E½m ¼ m,
(7.9)
covðmÞ ¼
S
ðk  d  1Þn ¼ E½S
n
,
(7.10)
respectively. With the assumed structure on m,
m ¼ E½m1,
(7.11)
n ¼ E½s11
varðm1Þ ,
(7.12)
where mi is the ith element of m.
Our objective is to approximate the expectations in Eqs. 7.4 through 7.12
using the C ¼ D  d features in the calibration data G \ H. Let bmC be the
sample mean and bS
C be the sample covariance matrix of the calibration
data. From these we wish to find several sample moments of m and S
for the d-feature classification problem: bE½m1, c
varðm1Þ, bE½s11, bE½s12, and
c
varðs11Þ, where the hats indicate the sample moment of the corresponding
quantity.
To compress the set G of D features to solve an estimation problem on just
the d features in H, and to find these scalar sample moments in a balanced
way, we assume that the selected features are drawn uniformly. Since each
feature in G\H is equally likely to be selected as the ith feature, the sample
mean of the mean of the ith feature bE½mi is computed as the average of the
sample means bmC
1 , bmC
2 , : : : , bmC
C, where bmC
i is the ith element of bmC. This result
is the same for all i, and we use bE½m1 to represent all features:
bE½m1 ¼ 1
C
X
C
i¼1
bmC
i :
(7.13)
290
Chapter 7

Owing to uniform feature selection, all other moments are balanced over
all features or any pair of distinct features. The remaining sample moments
are obtained in a similar manner:
c
varðm1Þ ¼
1
C  1
X
C
i¼1

bmC
i  bE½m1
2,
(7.14)
bE½s11 ¼ 1
C
X
C
i¼1
bsC
ii ,
(7.15)
bE½s12 ¼
2
CðC  1Þ
X
C
i¼2
X
i1
j¼1
bsC
ij ,
(7.16)
c
varðs11Þ ¼
1
C  1
X
C
i¼1

bsC
ii  bE½s11
2,
(7.17)
where bsC
ij is the ith row, jth column element of bS
C. Here, c
varðm1Þ represents
the variance of each feature in the mean. We also have bE½s11 and bE½s12
representing the mean of diagonal elements and off-diagonal elements in S,
respectively. Finally, c
varðs11Þ represents the variance of the diagonal elements
in S.
Plugging the sample moments into Eqs. 7.4, 7.5, 7.7, 7.11, and 7.12 yields
s2 ¼ 2bE½s11
ðbE½s11Þ2
c
varðs11Þ þ 1

,
(7.18)
r ¼
bE½s12
bE½s11
,
(7.19)
k ¼ 2ðbE½s11Þ2
c
varðs11Þ þ d þ 3,
(7.20)
m ¼ bE½m1,
(7.21)
n ¼
bE½s11
c
varðm1Þ :
(7.22)
Note that Eq. 7.20 for k was plugged into Eq. 7.4 to obtain the final s2.
In sum, calibration for the prior hyperparameters is defined by Eqs. 7.18
through 7.22, the sample moments being given in Eqs. 7.13 through 7.17. The
estimates of k and n can be unstable since they rely on second moments
c
varðs11Þ and c
varðm1Þ in a denominator. These parameters can be made more
291
Construction of Prior Distributions

stable by discarding outliers when computing the sample moments. Herein,
we discard the 10% of the bmC
i with largest magnitude and the 10% of the bsC
ii
with largest value.
Many variants of the method are possible. For instance, to avoid an over-
defined
system
of
equations,
we
do not
incorporate
the
covariance
covðm1, m2Þ between distinct features in m, the variance varðs12Þ of off-
diagonal elements in S, or the covariance between distinct elements in S,
though it may be possible to use these. It may also be feasible to use other
estimation methods, such as maximum likelihood.
We illustrate the data-driven methodology using a synthetic high-
dimensional data model, originally proposed to model thousands of gene-
expression measurements, that uses blocked covariance matrices to model
groups of interacting variables with negligible interactions between groups
(Hua et al., 2009). The model is employed to emulate a feature-label
distribution with 20,000 total features. Features are categorized as either
“markers” or “non-markers.” Markers represent features that have different
class-conditional distributions in the two classes and are further divided into
two subtypes: global markers and heterogeneous markers. Non-markers have
the same distributions for both classes and thus have no discriminatory power.
These are divided into two subtypes: high-variance non-markers and low-
variance non-markers. A graphical categorization of the feature types is
shown in Fig. 7.1. There are four basic types:
1. Global (homogeneous) markers: each class has its own Gaussian
distribution—these are the best features to discriminate the classes;
class 0
class 1
global
markers
heterogeneous
markers
high-variance
non-markers
low-variance
non-markers
subclass 0 subclass 1
Figure 7.1
Different feature types in the high-dimensional synthetic microarray data
model. [Reprinted from (Hua et al., 2009).]
292
Chapter 7

2. Heterogeneous markers: one class is Gaussian and the other is a mixture
of Gaussians—these features can still discriminate the classes, but not
as well;
3. High-variance non-markers: all of these features are independent and,
for any given feature, all sample points come from the same mixture of
Gaussian distributions with a randomly selected mixture proportion;
4. Low-variance non-markers: all of these features are independent and,
for any given feature, all sample points come from the same univariate
Gaussian distribution.
We utilize 20 features as global markers that are homogeneous in each
class. In particular, the set of all global markers in class i has a Gaussian
distribution with mean mgm
i
and covariance matrix Sgm
i . Class 1 is divided into
two subclasses, called subclass 0 and subclass 1. The 100 heterogeneous
markers are also divided into two groups, with 50 features per group. Call
these groups A0 and A1. Features in A0 and A1 are independent of each other.
Under features in group A0, class-1-subclass-0 points are jointly Gaussian
with the same mean and covariance structure as class-1 global markers, and
class-1-subclass-1 points are jointly Gaussian with the same mean and
covariance structure as class-0 global markers. Under features in group A1,
class-1-subclass-0 points are jointly Gaussian with the same mean and
covariance structure as class-0 global markers, and class-1-subclass-1 points
are jointly Gaussian with the same mean and covariance structure as class-1
global markers. The model is simplified by assuming that mgm
i
¼ mi ⋅120 for
fixed scalars mi. We assume that Sgm
i
¼ s2
i S, where s2
0 and s2
1 are constants,
and S possesses a block covariance structure,
S ¼
2
64
Sr
···
0
...
. .
.
...
0
···
Sr
3
75,
(7.23)
with Sr being a 5  5 matrix with 1 on the diagonal and r ¼ 0.8 off the
diagonal.
We generate 2000 high-variance non-marker features that have indepen-
dent mixed Gaussian distributions given by pN ðm0, s2
0Þ þ ð1  pÞN ðm1, s2
1Þ,
where mi and s2
i are the same scalars as were defined for markers. The random
variable p is selected independently for each feature with a uniformð0, 1Þ
distribution and is applied to all sample points of both classes. The remaining
features are low-variance non-marker features having independent univariate
Gaussian distributions with mean m0 and variance s2
0.
In this model, heterogeneous markers are Gaussian within each subclass,
but the class-conditional distribution for class 1 is a mixed Gaussian
distribution (mixing the distributions of the subclasses) and is thus not
293
Construction of Prior Distributions

Gaussian. Furthermore, the high-variance features are also mixed Gaussian
distributions, so this model incorporates both Gaussian and non-Gaussian
features. To simplify the simulations, we set the a priori probability of both
classes to 0.5, fix the parameters m0 ¼ 0 and m1 ¼ 1, and define a single
parameter s2 ¼ s2
0 ¼ s2
1, which specifies the difficulty of the classification
problem. A summary of the model parameters is given in Table 7.1. In all
simulations, the values for s2 are chosen such that a single global marker has
a specific Bayes error εBayes ¼ Fð1∕ð2sÞÞ. For instance, s ¼ 0.9537 yields
Bayes error 0.3.
Several Monte Carlo simulations are run. In each experiment we fix the
training sample size n, the number of selected features d, and the difficulty of
the classification problem via s. In each iteration the sample size of each class
is determined by a binomialðn, 0.5Þ experiment, and the corresponding sample
points are randomly generated according to the distributions defined for each
class. Once the sample has been generated, a three-stage feature-selection
scheme is employed: (1) apply a t-test to obtain 1000 highly differentially
expressed features; (2) apply a Shapiro–Wilk Gaussianity test and eliminate
features that do not pass the test with 95% confidence—the number of passing
features is variable and, if there are not at least 30 passing features, then we
return the 30 features with the highest sum of the Shapiro–Wilk test statistics
for both classes; (3) use the same t-test values originally computed to obtain
the final set of d highly differentially expressed Gaussian features with which
to design the classifier. The C ¼ 1000  d features that pass the first stage of
feature selection but are not used for classification are saved as calibration
data. The performance of this feature-selection scheme is analyzed in (Dalton
and Dougherty, 2011a).
The feature-selected training data are used to train an LDA classifier.
With the classifier fixed, 5000 test points are drawn from exactly the same
distribution as the training data and used expressly to approximate the true
error. Subsequently, several training-data error estimators are computed,
Table 7.1
Synthetic high-dimensional data model parameters.
Parameters
Values/description
Total features
20,000
Global markers
20
Subclasses in class 1
2
Heterogeneous markers
50 per subclass (100 total)
High-variance features
2000
Low-variance features
17,880
Mean
m0 ¼ 0, m1 ¼ 1
Variances
s2 ¼ s2
0 ¼ s2
1 (controls Bayes error)
Block size
5
Block correlation
0.8
a priori probability of class 0
0.5
294
Chapter 7

including leave-one-out (loo), 5-fold cross-validation with 5 repetitions (cv),
0.632 bootstrap (boot), and bolstered resubstitution (bol). Two Bayesian
MMSE error estimators are also applied, one with the flat prior defined by
pðuÞ ¼ 1 (flat BEE), and the other with priors calibrated as described
previously (calibrated BEE). Since the classifier is linear, these Bayesian
MMSE error estimators are computed exactly. This entire process is repeated
120,000 times to approximate the RMS deviations from the true error for each
error estimator.
Figure 7.2 shows RMS deviation from true error for all error estimators
with respect to the expected true error for LDA classification with 1 or 7
selected features and 120 sample points. Given the sample size, it is prudent
to keep the number of selected features small to have satisfactory feature
selection (Sima and Dougherty, 2006b) and to avoid the peaking phenomena
(Hughes, 1968; Hua et al., 2005, 2009). Several classical error estimators are
shown, along with Bayesian MMSE error estimators with the flat prior and
calibrated priors. The calibrated-prior Bayesian MMSE error estimator has
best performance in the middle and high range. For an average true error of
about 0.25, the RMS for the calibrated Bayesian MMSE error estimator
outperforms 5-fold cross-validation for d ¼ 1 and 7 by 0.0366 and 0.0198,
respectively, representing 67% and 33% decreases in RMS, respectively. All
other error estimators typically have best performance for low average true
errors, with the flat-prior Bayesian MMSE error estimator having better
performance than the classical error estimation schemes.
7.2 Prior Knowledge from Stochastic Differential Equations
Stochastic differential equations (SDEs) are used to model phenomena in
many domains of science, engineering, and economics. This section utilizes
vector SDEs as prior knowledge in the classification of time series. Although
0
0.1
0.2
0.3
0.4
0.5
0
0.05
0.1
0.15
0.2
average true error
(a)
(b)
RMS deviation from true error
 
 
loo
cv
boot
bolstering
BEE, flat
BEE, calibrated
0
0.1
0.2
0.3
0.4
0.5
0
0.05
0.1
0.15
0.2
0.25
average true error
RMS deviation from true error
 
 
loo
cv
boot
bolstering
BEE, ﬂat
BEE, calibrated
Figure 7.2
RMS deviation from true error for the synthetic high-dimensional data model
with LDA classification versus average true error: (a) n ¼ 120, d ¼ 1; (b) n ¼ 120,
d ¼ 7. [Reprinted from (Dalton and Dougherty, 2011a).]
295
Construction of Prior Distributions

we will confine ourselves to a Gaussian problem, the basic ideas can be
extended by using MCMC methods. In the stochastic setting, training data are
collected over time processes. Given certain Gaussian assumptions, classifica-
tion in the SDE setting takes the same form as ordinary classification in the
Gaussian model, and we can apply optimal classification theory after
constructing a prior distribution based on known stochastic equations.
Specifically, we consider a vector SDE in integral form involving a drift vector
and dispersion matrix.
7.2.1 Binary classification of Gaussian processes
A stochastic process X with state space RD is a collection fXt : t ∈Ug of
RD-valued
random
vectors
defined
on
a
common
probability
space
ðV, F, PrÞ indexed by a (time) parameter t ∈U ⊆R, where V is the sample
space, and F is a s-algebra on V. The time set U and state space RD are given
the usual Euclidean topologies and Borel s-algebras. A stochastic process X is
a multivariate Gaussian process if, for every finite set of indices t1, : : : , tk ∈U,
½XTt1, : : : , XTtkT is a multivariate Gaussian random vector. Suppose that X is a
multivariate Gaussian process, and consider the D-dimensional column
random vectors Xt1, Xt2, : : : , XtN
for the fixed observation time vector
tN ¼ ½t1, t2, : : : , tNT. Then XtN ¼ ½XTt1, XTt2, : : : , XTtNT possesses a multivari-
ate Gaussian distribution N ðmtN, StNÞ, where
mtN ¼
2
6664
mt1
mt2
...
mtN
3
7775
(7.24)
is an ND  1 vector with mti ¼ E½Xti, and
StN ¼
2
66664
St1,t1
St1,t2
···
St1,tN
St2,t1
St2,t2
···
St2,tN
...
...
. .
.
...
StN,t1
StN,t2
···
StN,tN
3
77775
(7.25)
is an ND  ND matrix with
Sti,tj ¼ E½ðXti  E½XtiÞðXtj  E½XtjÞT.
(7.26)
For any fixed v ∈V, a sample path is a collection fXtðvÞ : t ∈Ug.
A realization of X at sample path v and time vector tN is denoted by xtNðvÞ.
Following (Zollanvari and Dougherty, 2016), we consider a general
framework, referred to as binary classification of Gaussian processes (BCGP).
296
Chapter 7

Consider two independent multivariate Gaussian processes X0 and X1, where,
for fixed tN, X0
tN possesses mean and covariance m0
tN and S0
tN, and X1
tN
possesses mean and covariance m1
tN and S1
tN. For y ¼ 0, 1, my
tN is defined
similarly to Eq. 7.24, where my
ti ¼ E½Xy
ti, and Sy
tN is defined similarly to
Eq. 7.25, with
Sy
ti, tj ¼ E½ðXy
ti  E½Xy
tiÞðXy
tj  E½Xy
tjÞT.
(7.27)
Let Sy
tN denote a set of ny sample paths from process Xy at tN:
Sy
tN ¼
n
xy
tNðv1Þ, xy
tNðv2Þ, : : : , xy
tNðvnyÞ
o
,
(7.28)
where y ∈f0, 1g is the label of the class-conditional process the sample path is
coming from, X0 or X1. Separate sampling is assumed. Stochastic-process
classification is defined relative to a set of sample paths that can be considered
as observations of ND-dimension. Let xy
tNðvsÞ denote a test sample path
observed on the same observation time vector as the training sample paths but
with y unknown. We desire a discriminant ctN to predict y:
y ¼
	
0
if ctNðxy
tNðvsÞÞ ≤0,
1
otherwise:
(7.29)
Other types of classification are possible in this stochastic-process setting.
We could classify a test sample path xy
tNþMðvsÞ whose observation time vector
is obtained by augmenting tN by another vector ½tNþ1, tNþ2, : : : , tNþMT,
M being a positive integer. In this case, the time of observation for the test
sample path is extended. The test sample path’s time of observation might be
a subset of time points in tN, or a set of time points totally or partially
different from time points in tN. We confine ourselves to the problem defined
in Eq. 7.29.
We consider SDEs defined via a Wiener process. A one-dimensional
Wiener process W ¼ fW t : t ≥0g is a Gaussian process satisfying the
following properties:
1. For 0 ≤t1 , t2 , `, W t2  W t1 is distributed as
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
t2  t1
p
N ð0, s2Þ,
where s . 0 (s ¼ 1 for the standard Wiener process).
2. For 0 ≤t1 , t2 ≤t3 , t4 , `, W t4  W t3 is independent of W t2  W t1.
3. W 0 ¼ 0 almost surely.
4. The sample paths of W are almost surely everywhere continuous.
In general, a d-dimensional Wiener process is defined using a homogeneous
Markov process X ¼ fXt : t ≥0g. A stochastic process X on ðV, F, PrÞ is
adapted to the filtration fF t : t ≥0g (a filtration is a family of non-decreasing
297
Construction of Prior Distributions

sub-s-algebras of F) if Xt is F t-measurable for each t ≥0. X is also a Markov
process if it satisfies the Markov property: for each t1, t2 ∈½0, `Þ with t1 , t2
and for each Borel set B ⊆Rd,
PrðXt2 ∈BjF t1Þ ¼ PrðXt2 ∈BjXt1Þ.
(7.30)
Thus, a Markov process X is characterized by a collection of transition
probabilities, which we denote by Prðt1, x; t2, BÞ ¼ PrðXt2 ∈BjXt1 ¼ xÞ for
0 ≤t1 , t2 , `, x ∈Rd, and Borel set B ⊆Rd. For fixed values of t1, x,
and t2, Prðt1, x; t2,⋅Þ is a probability measure on the s-algebra B of Borel sets
in Rd. A Markov process is homogeneous if its transition probability
Prðt1, x; t2, BÞ is stationary: for 0 ≤t1 , t2 , ` and 0 ≤t1 þ u , t2 þ u , `,
Prðt1 þ u, x; t2 þ u, BÞ ¼ Prðt1, x; t2, BÞ.
(7.31)
In this case, Prðt1, x; t2, BÞ is denoted by Prðt2  t1, x; BÞ. A d-dimensional
Wiener process is a d-dimensional homogeneous Markov process with
stationary
transition
probability
defined
by
a
multivariate
Gaussian
distribution:
Prðt, x; BÞ ¼
Z
B
1
ð2ptÞ
d
2 e
kyxk2
2
2t dy:
(7.32)
Each dimension of a d-dimensional Wiener process is a one-dimensional
Wiener process.
Let W ¼ fWt : t ≥0g be a d-dimensional Wiener process. For each
sample path and for 0 ≤t0 ≤t ≤T, we consider a vector SDE in the integral
form
XtðvÞ ¼ Xt0ðvÞ þ
Z t
t0
gðs, XsðvÞÞds þ
Z t
t0
Gðs, XsðvÞÞdWsðvÞ,
(7.33)
where
g : ½0, T  V →RD
(the
D-dimensional
drift
vector)
and
G : ½0, T  V →RDd (the D  d dispersion matrix). The first integral in
Eq. 7.33 is an ordinary Lebesgue integral, and we assume an Itô integration
for the second integral. Let L be the Lebesgue s-algebra on R. We say that a
function h : ½0, T  V →R on the probability space ðV, F, PrÞ belongs to
LV
T if it is L  F-measurable, hðt, ⋅Þ is F t-measurable for each t ∈½0, T, and
R T
0 h2ðs, vÞds , ` almost surely. Let gi and Gi,j denote the components of g
and G, respectively. If we assume that X0ðvÞ is F 0-measurable,
ﬃﬃﬃﬃﬃﬃﬃ
jgij
p
∈LV
T,
and Gi,j ∈LV
T, then each component of the D-dimensional process Xt is
F t-measurable (Kloeden and Platen, 1995). Equation 7.33 is commonly
written in a symbolic form as
298
Chapter 7

dXt ¼ gðt, XtÞdt þ Gðt, XtÞdWt,
(7.34)
which is the representation of a vector SDE.
7.2.2 SDE prior knowledge in the BCGP model
Prior knowledge in the form of a set of SDEs constrains the possible behavior
of the dynamical system to an uncertainty class. In (Zollanvari and
Dougherty, 2016), valid prior knowledge is defined as a set of SDEs with a
unique solution that does not contradict the Gaussianity assumption of the
dynamics of the model. For nonlinear gðt, XtÞ and Gðt, XtÞ, the solution of
Eq. 7.34 is in general a non-Gaussian process; however, for a wide class of
linear functions, the solutions are Gaussian and the SDEs become valid prior
knowledge for each class-conditional process defined in the BCGP model. We
focus on this type of SDE.
For class label y ¼ 0, 1, the linear classes of SDEs we consider are
defined by
gyðt, XtÞ ¼ AyðtÞXy
t þ ayðtÞ,
(7.35)
Gyðt, XtÞ ¼ ByðtÞ
(7.36)
in Eq. 7.34, with AyðtÞ a D  D matrix, ayðtÞ a D  1 vector, and ByðtÞ a
D  d
matrix,
these
being
measurable
and
bounded
on
½t0, T
and
independent of v. Hence,
dXy
t ¼ ½AyðtÞXy
t þ ayðtÞdt þ ByðtÞdWy
t ,
(7.37)
Xy
t0ðvÞ ¼ cyðvÞ,
(7.38)
where cy is a d  1 vector. This initial value problem has a unique solution
that is a Gaussian stochastic process if and only if the initial conditions cy are
constant or Gaussian (Arnold, 1974, Theorem 8.2.10). Moreover, the mean
(at time index ti) and the covariance matrix (at ti and tj) of the Gaussian
process Xy
t are given by (Arnold, 1974)
my
ti ¼ E½Xy
ti ¼ FyðtiÞ

E½cy þ
Z ti
t0
½FyðsÞ1ayðsÞds

,
(7.39)
Cy
ti,tj ¼ E

Xy
ti  E½Xy
ti

Xy
tj  E½Xy
tj
T
¼ FyðtiÞ

E
h
ðcy  E½cyÞðcy  E½cyÞTi
þ
Z ti
t0
½FyðuÞ1ByðuÞ½ByðuÞTð½FyðuÞ1ÞTdu

½FyðtjÞT,
(7.40)
299
Construction of Prior Distributions

where
t0 ≤ti ≤tj ≤T,
and
FyðtiÞ
is
the
fundamental
matrix
of
the
deterministic differential equation
˙Xy
t ¼ AyðtÞXy
t :
(7.41)
If the SDE in Eqs. 7.37 and 7.38 precisely represents the dynamics of the
underlying stochastic processes of the BCGP model, then no training sample
paths are needed. In this case, Eqs. 7.24 and 7.25 take the form
my
tN ¼ my
tN,
(7.42)
Sy
tN ¼ Cy
tN,
(7.43)
where
my
tN ¼
2
666664
my
t1
my
t2
...
my
tN
3
777775
(7.44)
is an ND  1 vector, and
Cy
tN ¼
2
666664
Cy
t1,t1
Cy
t1,t2
···
Cy
t1,tN
Cy
t2,t1
Cy
t2,t2
···
Cy
t2,tN
...
...
..
.
...
Cy
tN,t1
Cy
tN,t2
···
Cy
tN,tN
3
777775
(7.45)
is an ND  ND matrix, with my
ti and Cy
ti,tj being obtained from Eqs. 7.39 and
7.40, respectively. The (approximately) exact values of the means and auto-
covariances used to characterize the Gaussian processes in the BCGP model
can be obtained. If Eq. 7.41 can be solved analytically, then numerical
methods can be used to evaluate the integrations in Eqs. 7.39 and 7.40. For
example, if AyðtÞ ¼ Ay is independent of t, then the solution of Eq. 7.41 is
given by the matrix exponential FyðtÞ ¼ expððt  t0ÞAyÞ, which can be used in
Eqs. 7.39 and 7.40. If one cannot analytically solve Eq. 7.41, then numerical
methods can be used to obtain good approximations of my
ti and Cy
ti,tj [see
(Zollanvari and Dougherty, 2016) for details].
Assuming that the values of m0ti, m1ti, C0ti,tj, and C1ti,tj are (approximately)
known, because BCGP classification reduces to discriminating independent
observations of ND dimension generated from two multivariate Gaussian
distributions, the optimal discriminant in Eq. 7.29 is quadratic:
300
Chapter 7

cQDA
tN

xy
tNðvsÞ

¼ 1
2
h
xy
tNðvsÞ  m0
tN
iT
C0
tN
1h
xy
tNðvsÞ  m0
tN
i
 1
2
h
xy
tNðvsÞ  m1
tN
iTðC1tNÞ1h
xy
tNðvsÞ  m1
tN
i
þ 1
2 ln
C0
tN

C1
tN
 þ ln a1
a0 ,
(7.46)
where ay ¼ PrðY ¼ yÞ.
If the SDEs do not provide a complete description of the dynamics of each
class-conditional process, then Eqs. 7.42 and 7.43 do not hold; however, the
SDEs can be used as prior knowledge. In (Zollanvari and Dougherty, 2016),
two assumptions are made on the nature of the prior information to which
the set of SDEs corresponding to each class gives rise: (1) before observing the
sample paths at an observation time vector, the SDEs characterize the only
information that we have about the system; and (2) the statistical properties of
all Gaussian processes that may generate the data are on average (over the
parameter space) equivalent to the statistical properties determined from the
SDEs. Thus, we will supply the classification problem with prior knowledge in
the form of SDEs, which are used to specify the mean of a mean and the mean
of a covariance matrix characterizing the data from each class. We then
construct a conjugate prior consistent with this information. The effective
densities may then be found and used to find the OBC, as explained below, or
used to perform other analyses.
Fixing tN, under uncertainty, the BCGP model is characterized by a
random parameter uy
tN ¼ ðmy
tN, Sy
tNÞ, with my
tN and Sy
tN defined in Eqs. 7.24 and
7.25, respectively. uy
tN has a conjugate normal-inverse-Wishart prior distribution
pðuy
tNÞ, parameterized by a set of hyperparameters ðm
̮ y
tN, C
̮
y
tN, ny
tN, ky
tNÞ, and
given by
pðuy
tNÞ ∝
Sy
tN

1
2ðky
tN þNDþ1Þetr

 1
2 C
̮
y
tNðSy
tNÞ1


Sy
tN

1
2 exp

 ny
tN
2

my
tN  m
̮ y
tN
T
Sy
tN
1
my
tN  m
̮ y
tN

:
(7.47)
We assume that u0
tN and u1
tN are independent. From Eq. 7.47 we obtain
Ep
h
my
tN
i
¼ m
̮ y
tN,
(7.48)
Ep
h
Sy
tN
i
¼
C
̮
y
tN
ky
tN  ND  1 ,
(7.49)
301
Construction of Prior Distributions

the latter holding for ky
tN . ND þ 1. Hence, assumption (2) on the nature of
the prior information implies that
m
̮ y
tN ¼ my
tN,
(7.50)
C
̮
y
tN ¼ ðky
tN  ND  1ÞCy
tN,
(7.51)
with my
tN defined by Eqs. 7.39 and 7.44, and Cy
tN defined by Eqs. 7.40 and
7.45. The greater our confidence in a set of SDEs representing the underlying
stochastic processes, the larger we might choose the values of ny
tN and ky
tN, and
the more concentrated become the priors of the mean and covariance about
my
tN and Cy
tN, respectively. To ensure a proper prior and the existence of the
expectation in Eq. 7.49, we assume that C
̮
y
tN is symmetric positive definite,
ky
tN . ND þ 1, and ny
tN . 0.
Assume that all test and training sample paths are independent. The OBC
theory applies: (1) the effective class-conditional distributions of the processes
result from Eq. 2.120; and (2) extending the dimensionality to ND and using
the parameter set ðm
̮ y
tN, C
̮
y
tN, ny
tN, ky
tNÞ in the discriminant of Eq. 4.25 yields
cOBC
tN

xy
tNðvsÞ

¼ K

1 þ 1
k0

xy
tNðvsÞ  m0∗
tN
T
Π0
tN
1h
xy
tNðvsÞ  m0∗
tN
ik0þND


1 þ 1
k1
h
xy
tNðvsÞ  m1∗
tN
iT
Π1
tN
1h
xy
tNðvsÞ  m1∗
tN
ik1þND
,
(7.52)
where
K ¼

a1
a0
2
k0
k1
ND
Π0
tN

Π1
tN

2
64
G

k0
2

G

k1þND
2

G

k1
2

G

k0þND
2

3
75
2
,
(7.53)
Πy
tN ¼
ny∗
tN þ 1
ðky∗
tN  ND þ 1Þny∗
tN
Cy∗
tN,
(7.54)
Cy∗
tN ¼ C
̮
y
tN þ ðny  1ÞbS
y
tN þ
ny
tNny
ny
tN þ ny

bmy
tN  m
̮ y
tN

bmy
tN  m
̮ y
tN
T,
(7.55)
ny∗
tN ¼ ny
tN þ ny,
(7.56)
ky∗
tN ¼ ky
tN þ ny,
(7.57)
302
Chapter 7

ky ¼ ky∗
tN  ND þ 1,
(7.58)
my∗
tN ¼ ny
tNm
̮ y
tN þ nybmy
tN
ny
tN þ ny
,
(7.59)
m
̮ y
tN and C
̮
y
tN are determined from the SDEs via Eqs. 7.50 and 7.51, and bmy
tN
and bS
y
tN are the sample mean and sample covariance matrix obtained by using
the sample path training set Sy
tN:
bmy
tN ¼ 1
ny
X
ny
i¼1
xy
tNðviÞ,
(7.60)
bS
y
tN ¼
1
ny  1
X
ny
i¼1
h
xy
tNðviÞ  bmy
tN
ih
xy
tNðviÞ  bmy
tN
iT:
(7.61)
Since we are assuming separate sampling, there is no mechanism for
estimating ay, and thus it is assumed to be known, as is typical for separate
sampling.
7.3 Maximal Knowledge-Driven Information Prior
In this section, we present a formal structure for prior formation involving a
constrained optimization in which the constraints incorporate existing
scientific knowledge augmented by slackness variables (Boluki et al., 2017).
We will subsequently discuss instantiations of this formal structure. The
constraints tighten the prior distribution in accordance with prior knowledge
while at the same time avoiding inadvertent over-restriction of the prior. Prior
construction involves two steps: (1) functional information quantification; (2)
objective-based prior selection: combining sample data and prior knowledge,
and building an objective function in which the expected mean log-likelihood
is regularized by the quantified information in step 1. When there are no
sample data or only one data point available for prior construction, this
procedure reduces to a regularized extension of the maximum entropy
principle. The next two definitions provide the general framework. Notice in
the definitions the formalization of our prior knowledge. This means that,
whereas prior scientific knowledge may come in various forms, for our use it
must be mathematically formalized in a way that allows it to be precisely used
in the cost function, for instance, as equalities, inequalities, or conditional
probabilities.
303
Construction of Prior Distributions

Definition 7.1. If Π is a family of proper priors, then a maximal knowledge-
driven information prior (MKDIP) is a solution to the optimization
arg min
p∈Π Ep½Cuðj, p, SÞ,
(7.62)
where Cuðj, p, SÞ is a cost function depending on the random vector u
parameterizing the uncertainty class, a formalization j
of our prior
knowledge, the prior p, and part of the sample data S.
Alternatively, by parameterizing the prior probability as pðu; gÞ, with
g ∈G denoting the hyperparameters, an MKDIP can be found by solving
arg min
g∈G Epðu;gÞ½Cuðj, g, SÞ:
(7.63)
The MKDIP incorporates prior knowledge and part of the data to
construct an informative prior. There is no known way of determining the
optimal amount of observed data to use in the optimization of Definition 7.1;
however, the issue is addressed via simulations in (Esfahani and Dougherty,
2014) and will be discussed in the next subsection.
A special case arises when the cost function is additively decomposed into
costs on the hyperparameters and the data so that it takes the form
Cuðj, g, SÞ ¼ ð1  bÞgð1Þ
u ðj, gÞ þ bgð2Þ
u ðj, SÞ,
(7.64)
where b ∈½0, 1 is a regularization parameter, and gð1Þ
u
and gð2Þ
u
are cost
functions.
The next definition transforms the MKDIP into a constrained optimization.
Definition 7.2. A maximal knowledge-driven information prior with constraints
solves the MKDIP problem subject to the constraints
Epðu;gÞ
h
gð3Þ
u,i ðjÞ
i
¼ 0, i ¼ 1, 2, : : : , nc,
(7.65)
where gð3Þ
u,i , i ¼ 1, 2, : : : , nc, are constraints resulting from the state j of our
knowledge via a mapping
T : j →

Epðu;gÞ
h
gð3Þ
u,1ðjÞ
i
, Epðu;gÞ
h
gð3Þ
u,2ðjÞ
i
, : : : , Epðu;gÞ
h
gð3Þ
u,ncðjÞ
i
:
(7.66)
We restrict our attention to MKDIP with constraints and additive costs as
in Eq. 7.64. In addition, while the MKDIP formulation allows prior
304
Chapter 7

information in the cost function, we will restrict prior information to the
constraints so that Eq. 7.64 becomes
Cuðg, SÞ ¼ ð1  bÞgð1Þ
u ðgÞ þ bgð2Þ
u ðSÞ,
(7.67)
and the optimization of Eq. 7.63 becomes
arg min
g∈G Epðu;gÞ½ð1  bÞgð1Þ
u ðgÞ þ bgð2Þ
u ðSÞ.
(7.68)
A nonnegative slackness variable can be considered for each constraint to
make the constraint framework more flexible, thereby allowing potential error
or uncertainty in prior knowledge (allowing potential inconsistencies in prior
knowledge). When employed, slackness variables also become optimization
parameters, and a linear function (summation of all slackness variables) times
a regulatory coefficient is added to the cost function of the optimization in
Eq. 7.63 so that the optimization in Eq. 7.63 relative to Eq. 7.67 becomes
arg min
g∈G, e∈E
Epðu;gÞ

l1½ð1  bÞgð1Þ
u ðgÞ þ bgð2Þ
u ðSÞ þ l2
X
nc
i¼1
ei

subject to  ei ≤Epðu;gÞ½gð3Þ
u,i ðjÞ ≤ei, i ¼ 1, 2, : : : , nc,
(7.69)
where
l1
and
l2
are
nonnegative
regularization
parameters,
and
e ¼ ½e1, : : : , enc and E represent the vector of all slackness variables and the
feasible region for slackness variables, respectively. Each slackness variable
determines a range—the more uncertainty regarding a constraint, the greater
the range for the corresponding slackness variable.
We list three cost functions in the literature:
1. Maximum entropy: The principle of maximum entropy (Guiasu and
Shenitzer, 1985) yields the least informative prior given the constraints
in order to prevent adding spurious information. Under our general
framework, this principle can be formulated by setting b ¼ 0 and
gð1Þ
u ðgÞ ¼ log pðu; gÞ.
(7.70)
Thus,
Epðu;gÞ½gð1Þ
u ðgÞ ¼ HðuÞ,
(7.71)
where Hð⋅Þ denotes the Shannon entropy (in the discrete case) or
differential entropy (in the continuous case). The base of the logarithm
determines the unit of information.
2. Maximal data information: The maximal data information prior
(MDIP) (Zellner, 1984) provides a criterion for the constructed
305
Construction of Prior Distributions

probability distribution to remain maximally committed to the data,
meaning that the cost function gives “the a priori expected information
for discrimination between the data-generating distribution and the
prior” (Ebrahimi et al., 1999). To achieve MDIP in our general
framework, set b ¼ 0 and
gð1Þ
u ðgÞ ¼ log pðu; gÞ þ HðXÞ
¼ log pðu; gÞ  E½log f ðXjuÞju,
(7.72)
where X is a random vector with the u-conditioned density f ðxjuÞ.
3. Expected mean log-likelihood: This cost function (Esfahani and
Dougherty, 2014), which utilizes part of the observed data for prior
construction, results from setting b ¼ 1 and
gð2Þ
u ðSÞ ¼ lðu; SÞ,
(7.73)
where S consists of n independent points x1, : : : , xn, and
lðu; SÞ ¼ 1
n
X
n
i¼1
log f ðxijuÞ
(7.74)
is the mean log-likelihood function of the sample points used for prior
construction.
The mean log-likelihood function lðu; SÞ is related to the Kullback–
Leibler (KL) divergence. To see this, consider the true distribution utrue and
an arbitrary distribution u. KL divergence provides a measure of the
difference between the distributions:
KLðutrue, uÞ ¼
Z
X
f ðxjutrueÞ log f ðxjutrueÞ
f ðxjuÞ dx
¼
Z
X
h
f ðxjutrueÞ log f ðxjutrueÞ  f ðxjutrueÞ log f ðxjuÞ
i
dx:
(7.75)
Since KLðutrue, uÞ ≥0 and f ðxjutrueÞ is fixed, KLðutrue, uÞ is minimized by
maximizing
rðutrue, uÞ ¼
Z
X
f ðxjutrueÞ log f ðxjuÞdx ¼ E½log f ðXjuÞjutrue,
(7.76)
which can therefore be treated as a similarity measure between utrue and u.
Since the sample points are conditioned on utrue, rðutrue, uÞ has the sample-
mean estimate lðu; SÞ given in Eq. 7.74 (Akaike, 1973, 1978; Bozdogan,
1987).
306
Chapter 7

As originally proposed, the preceding three approaches did not involve
expectation over the uncertainty class. They were extended to the general
prior construction form in Definition 7.1, including the expectation, to
produce the regularized maximum entropy prior (RMEP) and the regularized
maximal data information prior (RMDIP) in (Esfahani and Dougherty,
2014), and the regularized expected mean log-likelihood prior (REMLP) in
(Esfahani and Dougherty, 2015). In all cases, optimization was subject to
specialized constraints. Three MKDIP methods result from using these
information-theoretic cost functions in the MKDIP prior construction
optimization framework.
7.3.1 Conditional probabilistic constraints
Knowledge is often in the form of conditional probabilities characterizing
conditional relations: specifically, PrðUi ∈AijVi ∈BiÞ, where Ui and Vi are
vectors composed of variables in the system, and Ai and Bi are subsets of the
ranges of Ui and Vi, respectively. For instance, if a system has m binary
random variables X 1, X 2, : : : , X m, then there are m2m1 probabilities of the
form
PrðX i ¼ kijX 1 ¼ k1, : : : , X i1 ¼ ki1, X iþ1 ¼ kiþ1, : : : , X m ¼ kmÞ
¼ aki
i ðk1, : : : , ki1, kiþ1, : : : , kmÞ.
(7.77)
Note that
a0
i ðk1, : : : , ki1, kiþ1, : : : , kmÞ ¼ 1  a1
i ðk1, : : : , ki1, kiþ1, : : : , kmÞ.
(7.78)
The chosen constraints will involve conditional probabilities whose values
are approximately known. For instance, if X 1 ¼ 1 if and only if X 2 ¼ 1 and
X 3 ¼ 0, regardless of other variables, then
a1
1ð1, 0, k4, : : : , kmÞ ¼ 1
(7.79)
and
a1
1ð1, 1, k4, : : : , kmÞ ¼ a1
1ð0, 0, k4, : : : , kmÞ ¼ a1
1ð0, 1, k4, : : : , kmÞ ¼ 0
(7.80)
for all k4, : : : , km.
In stochastic systems it is unlikely that conditioning will be so complete
that constraints take 0-1 forms; rather, the relation between variables in the
model will be conditioned on the context of the system being modeled, not
simply the activity being modeled. In this situation, conditional probabilities
take the form
307
Construction of Prior Distributions

a1
1ð1, 0, k4, : : : , kmÞ ¼ 1  d1ð1, 0, k4, : : : , kmÞ,
(7.81)
where d1ð1, 0, k4, : : : , kmÞ ∈½0, 1 is referred to as a conditioning parameter,
and
a1
1ð1, 1, k4, : : : , kmÞ ¼ h1ð1, 1, k4, : : : , kmÞ,
(7.82)
a1
1ð0, 0, k4, : : : , kmÞ ¼ h1ð0, 0, k4, : : : , kmÞ,
(7.83)
a1
1ð0, 1, k4, : : : , kmÞ ¼ h1ð0, 1, k4, : : : , kmÞ,
(7.84)
where h1ðr, s, k4, : : : , kmÞ ∈½0, 1 is referred to as a crosstalk parameter. The
“conditioning” and “crosstalk” terminology comes from (Dougherty et al.,
2009a), in which d quantifies loss of regulatory control based on the overall
context in which the system is operating and, analogously, h corresponds to
regulatory relations outside the model that result in the conditioned variable
X 1 taking the value 1. We typically do not know the conditioning and
crosstalk parameters for all combinations of k4, : : : , km; rather, we might just
know the average, in which case d1ð1, 0, k4, : : : , kmÞ reduces to d1ð1, 0Þ and
h1ð1, 1, k4, : : : , kmÞ reduces to h1ð1, 1Þ, etc. The basic scheme is very general
and applies to the Gaussian and discrete models in (Esfahani and Dougherty,
2014, 2015).
In this paradigm, the constraints resulting from the state of knowledge are
of the form
gð3Þ
u,i ðjÞ
¼ PrðX i ¼ kijX 1 ¼ k1, : : : , X i1 ¼ ki1, X iþ1 ¼ kiþ1, : : : , X m ¼ km, uÞ
 aki
i ðk1, : : : , ki1, kiþ1, : : : , kmÞ.
(7.85)
The first term is a probability based on the model conditioned on u, and aki
i
represents our real-world prior knowledge about this probability. With
slackness variables ei, the constraints take the form
aki
i ðk1, : : : , ki1, kiþ1, : : : , kmÞ  eiðk1, : : : , ki1, kiþ1, : : : , kmÞ
≤Epðu;gÞ½PrðX i ¼ kijX 1 ¼ k1, : : : , X i1 ¼ ki1,
X iþ1 ¼ kiþ1, : : : , X m ¼ km, uÞ
≤aki
i ðk1, : : : , ki1, kiþ1, : : : , kmÞ þ eiðk1, : : : , ki1, kiþ1, : : : , kmÞ.
(7.86)
308
Chapter 7

The general conditional probabilities will not likely be used because they will
likely not be known when there are many conditioning variables. When all
constraints in the optimization are of this form, we obtain the optimizations
MKDIP-E, MKDIP-D, and MKDIP-R, which correspond to using the same
cost functions as RMEP, RMDIP, and REMLP, respectively.
7.3.2 Dirichlet prior distribution
We
consider
the
multinomial
model
where
parameter
vector
p ¼ ½p1, p2, : : : , pb has a Dirichlet prior distribution DirichletðaÞ with
parameter vector a ¼ ½a1, a2, : : : , ab:
pðp; aÞ ¼ GðPb
k¼1 akÞ
Qb
k¼1 GðakÞ
Y
b
k¼1
pak1
k
:
(7.87)
The sum a0 ¼ Pb
k¼1 ak provides a measure of the strength of the prior
knowledge (Ferguson, 1973). The feasible region for a given a0 is given by
Π ¼ fDirichletðaÞ : Pb
k¼1 ak ¼ a0 and ak . 0 for all kg.
In the binary setting, the state space consists of the set of all length-m binary
vectors, or equivalently, integers from 1 to b ¼ 2m that index these vectors.
Fixing the value of a single random variable, X i ¼ 0 or X i ¼ 1 for some i in
{1, : : : , m}, corresponds to a partition of the state space X ¼ f1, : : : , bg.
Denote the portions of X for which ðX i ¼ k1, X j ¼ k2Þ and ðX i ≠k1,
X j ¼ k2Þ for i, j ∈f1, : : : , mg and k1, k2 ∈f0, 1g by Xi,jðk1, k2Þ and
X i,jðkc
1, k2Þ, respectively. For the Dirichlet distribution, the constraints on
the expectation over the conditional probability in Eq. 7.86 can be explicitly
written as functions of the hyperparameters. Define
ai,jðk1, k2Þ ¼
X
k∈Xi, jðk1,k2Þ
ak:
(7.88)
The notation is easily extended for cases having more than two fixed random
variables. More generally, we can replace X j by a vector Ri composed of a
subset of the random variables. In such a case, Xi,Riðki, riÞ denotes the portion
of X for which X i ¼ ki and Ri ¼ ri. Equation 7.88 holds with ai,Riðki, riÞ
replacing ai,jðk1, k2Þ and Xi,Riðki, riÞ replacing X i,jðk1, k2Þ.
To evaluate the expectations of the conditional probabilities, we require a
technical lemma proven in (Esfahani and Dougherty, 2015): for any non-
empty disjoint subsets A, B ⊂X ¼ f1, 2, : : : , bg,
Ep

P
i∈A pi
P
i∈A pi þ P
j∈B pj

¼
P
i∈A ai
P
i∈A ai þ P
j∈B aj
:
(7.89)
If, for any i, the vector of random variables other than X i and the vector
of their corresponding values are denoted by ˜Xi and ˜xi, respectively, then,
309
Construction of Prior Distributions

applying the preceding equation, the expectation over the conditional
probability in Eq. 7.86 is
Ep½PrðX i ¼ kijX 1 ¼ k1, : : : , X i1 ¼ ki1, X iþ1 ¼ kiþ1, : : : , X m ¼ km, pÞ
¼ Ep

P
k∈Xi, ˜Xiðki,˜xiÞ pk
P
k∈X i, ˜Xiðki,˜xiÞ pk þ P
k∈X i, ˜Xiðkc
i ,˜xiÞ pk

¼
ai, ˜Xiðki, ˜xiÞ
ai, ˜Xiðki, ˜xiÞ þ ai, ˜Xiðkc
i , ˜xiÞ
:
(7.90)
Note that each summation in this equation has only one probability because
X i, ˜Xiðki, ˜xiÞ and X i, ˜Xiðkc
i , ˜xiÞ each contain only one state.
If, on the other hand, there exists a subset of random variables that
completely determines the value of X i (or only a specific setup of their values
that determines the value), then the constraints on the conditional probability
conditioned on all of the random variables other than X i can be changed to be
conditioned on that subset only. Specifically, let Ri denote the vector of
random variables corresponding to such a set and suppose that there exists a
specific setup of their values ri that completely determines the value of X i.
Then the conditional probability can be expressed as
Ep½PrðX i ¼ kijRi ¼ riÞ ¼ Ep

P
k∈Xi,Riðki,riÞ pk
P
k∈X i,Riðki,riÞ pk þ P
k∈Xi,Riðkc
i ,riÞ pk

¼
ai,Riðki, riÞ
ai,Riðki, riÞ þ ai,Riðkc
i , riÞ :
(7.91)
For a multinomial model with a Dirichlet prior distribution, prior knowledge
about the conditional probabilities PrðX i ¼ kijRi ¼ riÞ may take the form in
Eq. 7.86 with expectations provided in Eq. 7.91.
We next express the optimization cost functions for the three prior
construction methods, RMEP, RMDIP, and REMLP, as functions of the
Dirichlet parameter.
For RMEP, the entropy of p is given by
HðpÞ ¼
X
b
k¼1
½ln GðakÞ  ðak  1ÞcðakÞ  ln Gða0Þ þ ða0  bÞcða0Þ,
(7.92)
where c is the digamma function cðxÞ ¼ d
dx ln GðxÞ (Bishop, 2006). Assuming
that a0 is given, it does not affect the minimization, and the cost function is
given by
310
Chapter 7

Ep½CpðaÞ ¼ 
X
b
k¼1
½ln GðakÞ  ðak  1ÞcðakÞ.
(7.93)
For RMDIP, according to Eq. 7.72 and employing Eq. 7.92, after
removing the constant terms,
Ep½CpðaÞ ¼ 
X
b
k¼1
½ln GðakÞ  ðak  1ÞcðakÞ  Ep
X
b
k¼1
pk ln pk

:
(7.94)
To evaluate the second summand, first bring the expectation inside the sum.
Given p  DirichletðaÞ with a0 defined as it is, it is known that pk is beta
distributed with pk  betaðak, a0  akÞ. Plugging the terms into the expecta-
tion and doing some manipulation of the terms inside the expectation integral
yields
Ep½pk ln pk ¼ ak
a0
E½ln w,
(7.95)
where w  betaðak þ 1, a0  akÞ. It is known that for X  betaða, bÞ,
E½ln X ¼ cðaÞ  cða þ bÞ.
(7.96)
Hence,
Ep½pk ln pk ¼ ak
a0
½cðak þ 1Þ  cða0 þ 1Þ,
(7.97)
and the summand on the right in Eq. 7.94 becomes
Ep
X
b
k¼1
pk ln pk

¼
X
b
k¼1
ak
a0
cðak þ 1Þ

 cða0 þ 1Þ.
(7.98)
Plugging this into Eq. 7.94 and dropping cða0 þ 1Þ yields the RMDIP cost
function:
Ep½CpðaÞ ¼ 
X
b
k¼1

ln GðakÞ  ðak  1ÞcðakÞ þ ak
a0
cðak þ 1Þ

:
(7.99)
For REMLP, if there are np ¼ Pb
k¼1 uk points for prior construction,
there being uk points observed for bin k, then the mean-log-likelihood
function for the multinomial distribution is
311
Construction of Prior Distributions

lðp; SÞ ¼ 1
np
X
b
k¼1
uk ln pk þ ln
np!
Qb
k¼1 uk! :
(7.100)
Since p  DirichletðaÞ and a0 ¼ P
i ai,
Ep½ln pk ¼ cðakÞ  cða0Þ
(7.101)
(Bishop, 2006). Hence,
Ep½lðp; SÞ ¼ 1
np
X
b
k¼1
uk½cðakÞ  cða0Þ þ ln
np!
Qb
k¼1 uk! :
(7.102)
Finally, removing the constant parts in Eq. 7.102, the REMLP Dirichlet prior
for known a0 involves optimization with respect to the cost function
Ep½Cpða, SÞ ¼  1
np
X
b
k¼1
ukcðakÞ.
(7.103)
For application of the MKDIP construction for the multinomial model
with a Dirichlet prior distribution, we refer to (Esfahani and Dougherty,
2015).
7.4 REMLP for a Normal-Wishart Prior
In this section based on (Esfahani and Dougherty, 2014), we construct the
REMLP for a normal-Wishart prior on an unknown mean and precision
matrix using pathway knowledge in a graphical model. We denote the entities
in a given set of pathways by xðiÞ (as the ith element of the feature vector x).
Whereas the application in (Esfahani and Dougherty, 2014) is aimed at genes,
from the perspective of prior construction, these entities are very general, with
only their mathematical formulation being relevant to the construction
procedure. In keeping with the original application, we refer to these entities
as “genes.” A simplified illustration of the pathways highly influential in colon
cancer is shown in Fig. 7.3.
7.4.1 Pathway knowledge
Define the term activating pathway segment (APS) xðiÞ →xð jÞ to mean that,
if xðiÞ is up-regulated (UR, or 1), then xð jÞ becomes UR (in some time steps).
Similarly, the term repressing pathway segment (RPS) xðiÞ ⊣xð jÞ means that,
if xðiÞ is UR, then xð jÞ becomes down-regulated (DR, or 0). A pathway is
defined to be an APS/RPS sequence, for instance xð1Þ →xð2Þ ⊣xð3Þ. In this
pathway, there are two pathway segments, the APS xð1Þ →xð2Þ and the
RPS xð2Þ ⊣xð3Þ. A set of pathways used as prior knowledge is denoted by G.
312
Chapter 7

GA and GR denote the sets of all APS and RPS segments in G, respectively.
More specifically, GA contains pairs of features ½i, j such that xðiÞ →xðjÞ,
and GR contains pairs of features ½i, j such that xðiÞ ⊣xð jÞ. Regulations of the
form xðiÞ →xð jÞ and xðiÞ ⊣xð jÞ are called “pairwise regulations.”
The regulatory set for gene xðiÞ is the set of genes that affect xðiÞ, i.e., genes
that regulate xðiÞ through some APS/RPS. Denote this set by RxðiÞ for gene
xðiÞ. Denote the union of gene xðiÞ with its regulatory set RxðiÞ by RxðiÞ. As an
example, for the pathways shown in Fig. 7.4, Rxð1Þ ¼ fxð3Þg, Rxð2Þ ¼ ∅,
Rxð3Þ ¼ fxð1Þg, Rxð4Þ ¼ fxð1Þ, xð2Þg, Rxð5Þ ¼ fxð2Þ, xð4Þg, and Rxð6Þ ¼ fxð5Þg.
Assuming that the pathways convey complete information, that is, they
are not affected by unspecified crosstalk or conflicting interaction, we
quantify the pairwise regulations in a conditional probabilistic manner:
APS : Prðxð jÞ ¼ URjxðiÞ ¼ URÞ ¼ 1,
(7.104)
RPS : Prðxð jÞ ¼ DRjxðiÞ ¼ URÞ ¼ 1.
(7.105)
This notation is shorthand for a directional relationship, where the state of
gene xðiÞ influences the state of gene xðjÞ at some future time. For Gaussian
joint distributions, the equalities are changed to simpler ones involving
correlation:
EGF
HGF
IL6
RAS
IL6
PIK3CA
STAT3
EGF
SPRY4
mTOR
IL6
PKC
MEK 1/2
TSC1/TSC2
Figure 7.3
A simplified wiring diagram showing the key components of the colon cancer
pathways used in (Hua et al., 2012). Dashed boxes are used to simplify the diagram and
represent identical counterparts in solid boxes. [Reprinted from (Esfahani and Dougherty,
2014).]
Figure 7.4
An example of pathways with feedback containing six genes. This contains
three RPSs and four APSs. [Reprinted from (Esfahani and Dougherty, 2014).]
313
Construction of Prior Distributions

APS : rxðiÞ,xð jÞ ¼ 1,
(7.106)
RPS : rxðiÞ,xð jÞ ¼ 1,
(7.107)
where rxðiÞ,xð jÞ denotes the correlation coefficient between xðiÞ and xð jÞ. The
definitions in Eqs. 7.104 and 7.105 are directional and asymmetric so that the
flow of influence is preserved; however, the definitions in Eqs. 7.106 and 7.107
are symmetric and not directional. Moreover, the interpretation of Eqs. 7.104
and 7.105 as correlations in Eqs. 7.106 and 7.107 may not be appropriate.
Specifically, in the case of a cycle (a directed loop regardless of the type of
regulation), this two-way interpretation is inapplicable [see Fig. 7.4, where
there is an APS from xð1Þ to xð3Þ and an RPS from xð3Þ to xð1Þ]. When using
Eqs. 7.106 and 7.107 for the Gaussian case, acyclic pathways are assumed.
We also employ the conditional entropy of a gene given the expressions of
the genes in its regulatory set via the constraint
HuðxðiÞjRxðiÞÞ ¼ 0 for all i ∈C,
(7.108)
where Huð⋅j⋅Þ is the conditional entropy obtained by a u-parameterized
distribution, and C is the set of all features i corresponding to genes xðiÞ with
non-empty regulatory sets. HuðxðiÞjRxðiÞÞ is the amount of information needed
to describe the outcome of xðiÞ given RxðiÞ.
7.4.2 REMLP optimization
Although we have in mind classification between two (or more) classes, we
omit the class index y and focus on prior construction for only one class. As
previously noted, preliminary data are used in prior construction. A sample Sn
is partitioned into two parts: a set Sprior
np
used for prior construction with np
points, and a set Strain
nt
used for classifier training with nt points, where
n ¼ np þ nt.
Taking the expectation over the uncertainty class in the constraints,
including the slack variables ei,j and ji for the conditional probabilities and
entropies, respectively, and splitting the regularization parameter for the slack
variables between the probability and entropy slack variables, the optimiza-
tion of Eq. 7.69 becomes
pREMLP ¼
arg min
p∈Π, ji∈Ei
eia, ja ≥0, eir, jr ≥0
 ð1  l1  l2ÞEp
h
lðu; Sprior
np
Þ
i
þ l1
X
i∈C
ji
þ l2
2
4 X
½ia;ja∈GA
eia;ja þ
X
½ir;jr∈GR
eir;jr
3
5
(7.109)
314
Chapter 7

subject to the constraints
Ep
h
Hu

xðiÞjRxðiÞ
i
≤ji, i ∈C,
(7.110)
Ep½Prðxð jaÞ ¼ URjxðiaÞ ¼ UR,uÞ ≥1  eia,ja, ½ia, ja ∈GA,
(7.111)
Ep½Prðxð jrÞ ¼ DRjxðirÞ ¼ UR,uÞ ≥1  eir,jr, ½ir, jr ∈GR,
(7.112)
where l1, l2 ≥0, l1 þ l2 ≤1, Ei is the feasible region for slackness variable ji,
Π is the feasible region for the prior distribution, and lðu; Sprior
np
Þ is the mean
log-likelihood function defined in Eq. 7.74.
In Eq. 7.109, Ep½lðu; Sprior
np
Þ reflects the expected similarity between the
observed data and the true model. Prior averaging performs marginalization
with respect to model parameterization, resulting in dependence only on the
hyperparameters.
Assuming Gaussian distributions, Eqs. 7.111 and 7.112 become
Ep
h
rxðiaÞ,xð jaÞ
i
≥1  eia,ja, ½ia, ja ∈GA,
(7.113)
Ep
h
rxðirÞ,xð jrÞ
i
≤1 þ eir,jr, ½ir, jr ∈GR:
(7.114)
7.4.3 Application of a normal-Wishart prior
Assume a multivariate Gaussian feature distribution, N ðm, L1Þ, with D
genes, precision matrix L ¼ S1, and u ¼ ðm, LÞ. For given n and k, define
the feasible region for the prior as
Π ¼ fnormal-Wishartðm, n, W, kÞ : m ∈RD, W ≻0g,
(7.115)
i.e., the set of all normal-Wishart distributions. The normal-Wishart
distribution is determined fully by four parameters, a D  1 vector m, a
scalar n, a D  D matrix W, and a scalar k:
mjL  N ðm,ðnLÞ1Þ,
(7.116)
L ¼ S1  WishartðW, kÞ.
(7.117)
The form of the Wishart density is given in Eq. 6.1. Setting n . 0, W ≻0, and
k . D  1 ensures a proper prior.
The general optimization framework in Eqs. 7.109 through 7.112 does not
yield a convex programming for which a guaranteed converging algorithm
exists. To facilitate convergence, the full procedure can be split into two
optimization problems (at the cost of yielding a suboptimal solution). The
effect of prior knowledge can be assessed by deriving analytical expressions
for the gradient and Hessian of the cost functions. First, assume that l2 ¼ 0
315
Construction of Prior Distributions

and solve the optimization in Eqs. 7.109 and 7.110. The goal of the second
optimization, which we treat in the next section, is to find a matrix close to the
solution of the first optimization problem that better satisfies the constraints
simplified to correlations in Eqs. 7.113 and 7.114.
Setting l2 ¼ 0, the REMLP optimization reduces to
min
m∈RD, W≻0, j∈E  ð1  l1Þ
Z
U
lðu; Sprior
np
ÞpðuÞdu þ l1j
subject to
Z
L≻0
Z
RD Hm,LðxjRxÞpðm, LÞdmdL ≤j,
(7.118)
where, for the sake of simplicity, we consider here the single-constraint
optimization with feasible region E for slackness variable j, and we omit the
gene index and simply denote the constrained gene by x. For the log-
likelihood of the Gaussian distribution,
2lðm, L; Sprior
np
Þ ¼ ln jLj  1
np
X
np
i¼1
tr

Lðxi  mÞðxi  mÞT
 D ln 2p:
(7.119)
Taking the expectation with respect to the mean and covariance matrix yields
2Ep
h
lðm, L; Sprior
np
Þ
i
¼ E½ln jLj  k
np
X
np
i¼1
tr

Wðxi  mÞðxi  mÞT
 D
n  D ln 2p
¼ ln jWj  k trðWVmÞ þ
X
D
d¼1
c

k þ 1  d
2

 D

ln p þ 1
n

,
(7.120)
where we assume that n and k are fixed, we have applied the fact that
E½ln jLj ¼ ln jWj þ
X
D
d¼1
c

k þ 1  d
2

þ D ln 2,
(7.121)
and we define
Vm ¼ 1
np
X
np
i¼1
ðxi  mÞðxi  mÞT:
(7.122)
We will see shortly that the constraint in Eq. 7.118 does not depend on m.
Thus, the optimization of m is free of constraints and equivalent to
minimizing trðWVmÞ. This gives
316
Chapter 7

bmREMLP ¼ 1
np
X
np
i¼1
xi:
(7.123)
We consider two cases for the covariance matrix and, consequently, for
W. Throughout, it is assumed that x ∈= Rx (no self-regulation), and, without
loss of generality, that genes in the feature vector x are ordered with genes in
Rx first, followed by the gene x, followed by all remaining genes.
Suppose that x contains only Rx, so that x contains only the constrained
gene x and genes in its regulating set. The precision matrix L for x can be
written in block form,
L ¼

LRx
L12
L21
Lx

,
(7.124)
where,
since
L  WishartðW, kÞ,
we
have
LRx  WishartðWRx, kÞ,
Lx  WishartðW x, kÞ, and
W ¼

WRx
W12
W21
W x

:
(7.125)
Note that Lx and W x are scalars. Given features in Rx, x is Gaussian with
variance L1
x . Hence, HuðxjRxÞ ¼ 0.5 lnð2peL1
x Þ. By Eq. 7.121,
Ep½HuðxjRxÞ ¼ 0.5 lnðpeÞ  0.5 ln jW xj  0.5cðk∕2Þ.
(7.126)
Assuming that x contains only Rx, and letting V denote Vm with m replaced
by Eq. 7.123, the optimization of Eq. 7.118 can be expressed as
CP1ðkÞ :
min
W≻0, j≥j
 1
2 ð1  l1Þ
h
ln jWj  k trðWVÞ
i
þ l1j
subject to  lnðW xÞ  c

k
2

≤j,
(7.127)
where j ¼  lnðpeÞ ensures that Ep½HuðxjRxÞ is always upper bounded by a
positive constant (Esfahani and Dougherty, 2014).
From the inequalities in (Dembo et al., 1991), one can see that the parts
containing ln jWj are concave, thereby making the optimization problem
CP1ðkÞ (the objective function and constraints) convex in the matrix W.
Now suppose that x contains Rx along with one or more other entities,
with the precision matrix and its prior represented in block format by
317
Construction of Prior Distributions

L ¼
2
4
LRx
L12
L13
L21
Lx
L23
L31
L32
L33
3
5,
(7.128)
W ¼
2
4
WRx
W12
W13
W21
W x
W23
W31
W32
W33
3
5:
(7.129)
Similar to before, Lx and W x are scalars. Given features in Rx, x is Gaussian
with variance B1, where B ¼ Lx  L23L1
33 L32. Any diagonal block of a
Wishart matrix is Wishart, and the Schur complement of any diagonal block
of a Wishart matrix is also Wishart. Since B is the Schur complement of L33 in
the matrix formed by removing the rows and columns in L corresponding to
Rx in Eq. 7.128, we have that
B  Wishart

W x  W23W1
33 W32, k  ðD  jRxj  1Þ

.
(7.130)
Since HuðxjRxÞ ¼ 0.5 lnð2peB1Þ, by Eq. 7.121,
Ep½HuðxjRxÞ ¼ 0.5 lnðpeÞ  0.5 lnðW x  W23W1
33 W32Þ  0.5cðAÞ, (7.131)
where
A ¼ ½k  ðD  jRxj  1Þ∕2.
Hence,
as
shown
in
(Esfahani
and
Dougherty, 2014), the optimization problem of Eq. 7.118 can be expressed as
CP2ðkÞ :
min
W≻0, j≥j
 1
2 ð1  l1Þ½ln jWj  k trðWVÞ þ l1j
subject to  lnðW x  W23W1
33 W32Þ  cðAÞ ≤j:
(7.132)
It is also shown in (Esfahani and Dougherty, 2014) that CP2ðkÞ is a
convex program and that the optimization problems CP1ðkÞ and CP2ðkÞ
satisfy Slater’s condition.
7.4.4 Incorporating regulation types
Given that the underlying feature distribution is jointly Gaussian, we
incorporate the APS and RPS effects using Eqs. 7.113 and 7.114. Analogous
to the development of CP1ðkÞ and CP2ðkÞ, we try to manipulate the expected
correlation coefficients; however, instead of taking the expectation of the
correlation coefficient, which yields a non-convex function, we fix the
variances according to what we obtain from CP2ðkÞ.
From the properties of the Wishart distribution, S  inverse-WishartðC, kÞ,
where C ¼ W1. Define C∗¼ ðW∗Þ1, where W∗is the optimal solution of
CP2ðkÞ. Denote the elements of S, C, and C∗by sij, cij, and c∗
ij, respectively,
where i, j ¼ 1, 2, : : : , D. The first moments of sij are E½sij ¼ cij∕ðk  D  1Þ,
318
Chapter 7

from which we obtain the approximation
E½rij ¼ E

sij
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
siisjj
p


E½sij
1
kD1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
c∗
iic∗
jj
p
¼
cij
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
c∗
iic∗
jj
p
,
(7.133)
where rij is the correlation coefficient between xðiÞ and xð jÞ. The goal of the
second
optimization
paradigm
is
to
satisfy
the
correlation-coefficient
constraints according to the regulation types and be as close to the CP2ðkÞ
solution as possible. Thus, we introduce a penalty term based on the distance
from the solution of CP2ðkÞ and aim to find the closest, in the sense of the
Frobenius norm, symmetric positive definite matrix C to the matrix C∗. This
leads to the following optimization problem, with optimization parameter C:
CP3 : min
C≻0,
eia, ja ≥0,
eir, jr ≥0
ð1  l2ÞkC  C∗k2
F þ l2
2
4 X
½ia,ja∈GA
eia,ja þ
X
½ir,jr∈GR
eir,jr
3
5
subject to 1  eia,ja ≤
ciaja
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
c∗iaiac∗ja ja
p
≤1, ½ia, ja ∈GA,
1  eir,jr ≤
cirjr
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
c∗irirc∗jrjr
p
≤1, ½ir, jr ∈GR,
(7.134)
where k ⋅kF denotes the Frobenius norm. The regularization parameter
l2 ∈ð0, 1Þ provides a balance between the two functions. It can be readily
shown that the optimization problem in Eq. 7.134 is convex. In sum, the
REMLP problem in Eqs. 7.109 through 7.112 with given n and k is
approximated by setting the hyperparameter m according to Eq. 7.123 and
solving for the hyperparameter W using two sequential problems: (1) the
optimization in Eq. 7.132 [CP2ðkÞ], and then (2) the optimization in Eq. 7.134
(CP3). Once again, we only consider the one-constraint problem. The
multiple-constraint problem can be treated similarly.
The optimization problem CP2ðkÞ is a nonlinear inequality constrained
programming problem, and in (Esfahani and Dougherty, 2014) an algorithm
employing the log-barrier interior point method is proposed for solving it. The
optimization problem CP3 is a linearly constrained quadratic programming
problem, which, absent the positive definiteness constraint, is easily solved;
however, to find a proper prior distribution, a symmetric positive definite
matrix is of interest. An algorithm is provided in (Esfahani and Dougherty,
2014).
One issue is in setting the parameter l1. Since larger k means that the prior
is more centered about the scale matrix, k can be viewed as a measure of
the total amount of information about the covariance in the prior. The
319
Construction of Prior Distributions

regularization parameter l1 balances two sources of information: (1) data
through the expected likelihood function, and (2) pathway knowledge through
slackness variables bounding the conditional entropy. Thus, we may view k as
a sum of the amount of information supplied by the prior construction
training data np and the amount of information supplied by the pathway
constraints npw. Heuristically, we may also view l1 as the ratio of pathway
information to total information. Thus,
l1 
npw
np þ npw
:
(7.135)
We are left with defining npw. In the simulations we let npw ¼ mD for different
values of m ≥2 and see that the performance is not very sensitive to m; thus, a
default value could simply be npw ¼ 2D.
7.4.5 A synthetic example
We present a synthetic example from (Esfahani and Dougherty, 2014).
Simulations require a fixed ground-truth model from which sample data are
taken and pathways are built up. The ground-truth model is Gaussian with
both covariance matrices having a blocked structure proposed in (Hua et al.,
2005) to model the covariance matrix of gene-expression microarrays. Here,
however, we place a small correlation between blocks. A 3-block covariance
matrix with block size 3 has the structure
S ¼
2
4
B1
C
C
C
B2
C
C
C
B3
3
5,
(7.136)
where
Bi ¼
2
4
s2
ris2
ris2
ris2
s2
ris2
ris2
ris2
s2
3
5,
(7.137)
C ¼
2
4
rcs2
rcs2
rcs2
rcs2
rcs2
rcs2
rcs2
rcs2
rcs2
3
5,
(7.138)
s2 is the variance of each variable, ri for i ∈f1, 2, 3g is the correlation
coefficient inside block Bi, and rc is the correlation coefficient between
elements of different blocks. In (Esfahani and Dougherty, 2014) a method for
generating synthetic pathways to serve as the true model governing the
stochastic regulations in the network is proposed. Sample points are generated
320
Chapter 7

according to the Gaussian model. The generated pathways and sample points
are then used for classifier design as depicted in Fig. 7.5. As shown in the
figure, the fundamental principle is that prior knowledge about the pathways
is integrated into the classification problem. As will be illustrated in this
synthetic example, prior calibration is used to calibrate the mathematical
equations to reflect the expert (pathway) knowledge.
For the simulations, there may be one set of pathways G corresponding to
one class, or two sets of pathways (G0, G1) corresponding to two classes. The
sample data are partitioned into Sprior
np
and Strain
nt
for prior construction and
training the OBC, respectively. Whereas previously only one class was
considered for prior construction, here Sn, Sprior
np
, and Strain
nt
contain points
from both classes. The pathways are combined with Sprior
np
to construct the
REMLP for each class. The constructed priors are utilized with Strain
nt
to train
the OBC.
Denoting the error of a classifier c under feature-label distributions
parameterized by u by εnðu, cÞ, and denoting the OBC designed via the
REMLP constructed using Sprior
np
and training points Strain
nt
by c
np
OBC,nt, we are
concerned with εnðu, c
np
OBC,ntÞ for some true parameter u. If the solutions to
the optimization paradigms stated in CP2ðkyÞ and CP3, pREMLPðuyÞ for
y ¼ 0, 1, produce good priors, that is, priors that have strong concentration
around the true parameter u, then it is likely that εnðu, c
np
OBC,ntÞ ≤εnðu, cÞ,
where c is some other classifier, the exact relation depending on the feature-
label distribution, classification rule, and sample size.
Fixing the true feature-label distribution, n points are generated that
compose Sn,i in the ith iteration for i ¼ 1, 2, : : : , M. These points are split
randomly into Sprior
np,i and Strain
nt,i , where np þ nt ¼ n. Denote the given pathways
by G. Using G and Sprior
np,i , construct prior distributions pREMLP,iðuyÞ for
y ∈f0, 1g. These are updated using the remaining points Strain
nt,i
from which
c
np
OBC,nt,i is trained. The expected true error εnðu, c
np
OBC,ntÞ is evaluated via
Monte Carlo simulations:
train OBC
partition sample
biological
pathways
sample
REMLP
prior construction
Figure 7.5
A prior construction methodology for OBC training. The sample is partitioned
into two parts; one part is integrated with scientific knowledge to construct the prior, and the
other is used for training as usual.
321
Construction of Prior Distributions

E½εnðu, c
np
OBC,ntÞ  1
M
X
M
i¼1
εn

u, c
np
OBC,nt,i

,
(7.139)
where the expectation is over the sampling distribution under the true
parameter, we set M ¼ 15,000 repetitions, and each error term in the sum is
estimated using 10,000 points. The overall strategy, repeated through Monte
Carlo simulations, is partly shown in Fig. 7.5 and implemented step-wise as
follows:
1. Fix the true parameterization for two classes: ðmy, SyÞ, y ∈f0, 1g.
2. Use the pathway-generation algorithm to generate two sets of
pathways Gy for y ∈f0, 1g.
3. Take observations from N ðmy, SyÞ to generate Sn.
4. Randomly
choose
np
points
from
Sn
to
form
Sprior
np
and
let
Strain
nt
¼ Sn  Sprior
np
.
5. Use Sprior
np
and Gy to construct the prior pREMLPðuyÞ for y ∈f0, 1g by
REMLP [CP2ðkyÞ and CP3].
6. Design the OBC c
np
OBC,nt for the priors pREMLPðuyÞ, y ∈f0, 1g, and
Strain
nt
.
We consider a setting with D ¼ 8 entities. The covariance matrices, S0
and S1, are of the form in Eq. 7.136 with block sizes 3, 3, and 2 for the first,
second, and third blocks, respectively. We set S0 with s2 ¼ 1, r1 ¼ r3 ¼ 0.3,
r2 ¼ 0.3, rc ¼ 0.1, and we set S1 ¼ 2S0. We also set m0 ¼ 0.5 ⋅1D,
m1 ¼ 0.5 ⋅1D, and the prior class-0 probability to c ¼ 0.5. These settings
correspond to the Bayes error εBayes ¼ 0.091. Given the feature-label
distribution parameters, sample sizes n ¼ 30; 50; 70 are considered. The
number of points in each class is fixed and is determined by n0 ¼ cn and
n1 ¼ n  n0.
The average true error of the designed OBC using REMLP priors from
CP2 and CP3 and the Jeffreys rule prior are computed. LDA and QDA
classifiers are also trained for comparison. For REMLP priors, we vary the
ratio rp of the number of sample points used for prior construction to the total
sample size from 0.1 to 0.9. Thus, we keep at most 90% of the points for prior
update and finding the posterior. The number of points available for prior
construction under class y is given by np,y ¼ drpnye. For example, when
c ¼ 0.6, n ¼ 30, and 50% of the points are used for prior construction, np,0 ¼ 9
and np,1 ¼ 6. We set l1 according to Eq. 7.135 with npw ¼ 2D, and l2 ¼ 0.5.
We also set ny ¼ np,y and ky ¼ 2D þ np,y.
Average true errors for OBC with REMLP priors are shown in Fig. 7.6
with respect to the percentage rp  100% of points used for prior construction.
OBC under Jeffreys rule prior, LDA, and QDA use all of the data directly for
322
Chapter 7

classifier training and thus appear as constants in the same graphs. In
Fig. 7.6(a), n ¼ 30, and by increasing the number of sample points used for
prior construction, the true error decreases. Thus, one should use at least 90%
of the sample points for prior construction. However, when the total number
of sample points increases, there is an optimal number of points that should be
utilized for prior construction. For instance, as illustrated in Fig. 7.6(c), after
about np ¼ 70  0.5 ¼ 35, the true error of the designed OBC increases.
The simulations demonstrate that splitting the data provides better
performance—that is, using np points (np , n) to design the prior and the
10
20
30
40
50
60
70
80
90
0.16
0.18
0.2
0.22
0.24
0.26
0.28
average true error
prior construction sample size (%)
(a)
(b)
(c)
LDA
QDA
OBC (REML prior)
10
20
30
40
50
60
70
80
90
0.14
0.15
0.16
0.17
0.18
0.19
0.2
average true error
prior construction sample size (%)
LDA
QDA
OBC (REML prior)
10
20
30
40
50
60
70
80
90
0.12
0.13
0.14
0.15
0.16
average true error
prior construction sample size (%)
LDA
QDA
OBC (REML prior)
Figure 7.6
Average true error as a function of the percentage of the sample points used for
prior construction, rp  100%: (a) n ¼ 30; (b) n ¼ 50; (c) n ¼ 70. [Reprinted from (Esfahani
and Dougherty, 2014).]
323
Construction of Prior Distributions

remaining n  np points to train the OBC can provide lower expected error.
To be precise, for a given n, the number of sample points for which the
minimum average true error is achieved is expressed by
n∗pðnÞ ¼ arg
min
np∈f2,3, : : : , ngE
h
εn

u, c
np
OBC,nnp
i
,
(7.140)
where the expectation is over the sampling distribution under the true
parameter. n∗pðnÞ represents the optimal amount of data to be used in prior
construction, after which the remaining points should be employed to update
the constructed prior. Since there is no closed form for the true error of the
OBC designed using an REMLP prior, the exact value of n∗p cannot be
determined. As considered in (Esfahani and Dougherty, 2014) via Monte
Carlo analysis, increasing n does not necessarily lead to a larger n∗p; on the
contrary, there is a saturation point after which increasing n does not
significantly influence the optimal sample size for prior construction.
Practically, one can find some number of points for prior construction that
can be taken as preliminary data, no matter the sample size n.
324
Chapter 7

References
Akaike, H., “Information theory and an extension of the maximum likelihood
principle,” in Proceedings of the 2nd International Symposium on
Information Theory, pp. 267–281, 1973.
Akaike, H., “A Bayesian analysis of the minimum AIC procedure,” Annals of
the Institute of Statistical Mathematics, vol. 30, no. 1, pp. 9–14, 1978.
Anders, S. and W. Huber, “Differential expression analysis for sequence
count data,” Genome Biology, vol. 11, no. 10, art. R106, 2010.
Anderson, T., “Classification by multivariate analysis,” Psychometrika,
vol. 16, pp. 31–50, 1951.
Arnold, L., Stochastic Differential Equations: Theory and Applications.
John Wiley & Sons, New York, 1974.
Arnold, S. F., The Theory of Linear Models and Multivariate Analysis.
John Wiley & Sons, New York, 1981.
Banerjee, U. and U. M. Braga-Neto, “Bayesian ABC-MCMC classification
of liquid chromatography–mass spectrometry data,” Cancer Informatics,
vol. 14(Suppl 5), pp. 175–182, 2017.
Bao, Y. and A. Ullah, “Expectation of quadratic forms in normal and
nonnormal variables with econometric applications,” Journal of Statisti-
cal Planning and Inference, vol. 140, pp. 1193–1205, 2010.
Berger, J. O. and J. M. Bernardo, “On the development of reference priors,”
Bayesian Statistics, vol. 4, no. 4, pp. 35–60, 1992.
Berger, J. O., J. M. Bernardo, and D. Sun, “Objective priors for discrete
parameter spaces,” Journal of the American Statistical Association,
vol. 107, no. 498, pp. 636–648, 2012.
Berikov, V. and A. Litvinenko, “The influence of prior knowledge on the
expected performance of a classifier,” Pattern Recognition Letters, vol. 24,
pp. 2537–2548, 2003.
Bernardo, J. M., “Reference posterior distributions for Bayesian inference,”
Journal of the Royal Statistical Society: Series B (Methodological),
vol. 41, no. 2, pp. 113–147, 1979.
Bernardo, J. M. and A. F. Smith, Bayesian Theory. John Wiley & Sons,
Chichester, UK, 2000.
325

Bickel, P. J. and E. Levina, “Some theory for Fisher’s linear discriminant
function, ‘naive Bayes’, and some alternatives when there are many more
variables than observations,” Bernoulli, vol. 10, no. 6, pp. 989–1010,
2004.
Bishop, C. M., Pattern Recognition and Machine Learning. Springer-Verlag,
New York, 2006.
Boluki, S., M. S. Esfahani, X. Qian, and E. R. Dougherty, “Incorporating
biological prior knowledge for Bayesian learning via maximal knowledge-
driven information priors,” BMC Bioinformatics, vol. 18(Suppl 14),
art. 552, 2017.
Boluki, S., X. Qian, and E. R. Dougherty, “Experimental design via
generalized mean objective cost of uncertainty,” IEEE Access, vol. 7,
no. 1, pp. 2223–2230, 2019.
Bozdogan, H., “Model selection and Akaike’s information criterion (AIC):
The general theory and its analytical extensions,” Psychometrika, vol. 52,
no. 3, pp. 345–370, 1987.
Braga-Neto, U. M. and E. R. Dougherty, “Bolstered error estimation,”
Pattern Recognition, vol. 37, no. 6, pp. 1267–1281, 2004a.
Braga-Neto, U. M. and E. R. Dougherty, “Is cross-validation valid for
small-sample microarray classification?” Bioinformatics, vol. 20, no. 3,
pp. 374–380, 2004b.
Braga-Neto, U. M. and E. R. Dougherty, “Exact performance of error
estimators for discrete classifiers,” Pattern Recognition, vol. 38, no. 11,
pp. 1799–1814, 2005.
Braga-Neto, U. M. and E. R. Dougherty, “Exact correlation between actual
and estimated errors in discrete classification,” Pattern Recognition
Letters, vol. 31, no. 5, pp. 407–412, 2010.
Braga-Neto, U. M. and E. R. Dougherty, Error Estimation for Pattern
Recognition. Wiley-IEEE Press, New York, 2015.
Broumand, A., B.-J. Yoon, M. S. Esfahani, and E. R. Dougherty, “Discrete
optimal Bayesian classification with error-conditioned sequential sam-
pling,” Pattern Recognition, vol. 48, no. 11, pp. 3766–3782, 2015.
Butler, R. W. and A. T. A. Wood, “Laplace approximations for hypergeo-
metric functions with matrix argument,” Annals of Statistics, vol. 30,
no. 4, pp. 1155–1177, 2002.
Carrasco, M. and J.-P. Florens, “Simulation-based method of moments and
efficiency,” Journal of Business & Economic Statistics, vol. 20, no. 4,
pp. 482–492, 2002.
Chang, C.-C. and C.-J. Lin, “LIBSVM: A library for support vector
machines,” ACM Transactions on Intelligent Systems and Technology,
vol. 2, art. 27, 2011.
Cohn, D., L. Atlas, and R. Ladner, “Improving generalization with active
learning,” Machine Learning, vol. 15, no. 2, pp. 201–221, 1994.
326
References

Constantine, A. G., “Some non-central distribution problems in multivariate
analysis,” Annals of Mathematical Statistics, vol. 34, no. 4, pp. 1270–1285,
1963.
Cover, T. M. and J. M. Van Campenhout, “On the possible orderings in the
measurement selection problem,” IEEE Transactions on Systems, Man,
and Cybernetics, vol. 7, no. 9, pp. 657–661, 1977.
Craig, J. W., “A new, simple and exact result for calculating the probability of
error for two-dimensional signal constellations,” in Proceedings of the
Military Communications Conference, pp. 571–575, 1991.
Dadaneh, S. Z., E. R. Dougherty, and X. Qian, “Optimal Bayesian
classification with missing values,” IEEE Transactions on Signal Proces-
sing, vol. 66, no. 16, pp. 4182–4192, 2018a.
Dadaneh, S. Z., X. Qian, and M. Zhou, “BNP-Seq: Bayesian nonparametric
differential expression analysis of sequencing count data,” Journal of the
American Statistical Association, vol. 113, no. 521, pp. 81–94, 2018b.
Dalton, L. A., “Application of the sample-conditioned MSE to non-linear
classification and censored sampling,” in Proceedings of the 21st European
Signal Processing Conference, art. 1569744691, 2013.
Dalton, L. A., “Optimal ROC-based classification and performance analysis
under Bayesian uncertainty models,” IEEE/ACM Transactions on Compu-
tational Biology and Bioinformatics, vol. 13, no. 4, pp. 719–729, 2016.
Dalton, L. A. and E. R. Dougherty, “Application of the Bayesian MMSE
estimator for classification error to gene expression microarray data,”
Bioinformatics, vol. 27, no. 13, pp. 1822–1831, 2011a.
Dalton, L. A. and E. R. Dougherty, “Bayesian minimum mean-square error
estimation for classification error—Part I: Definition and the Bayesian
MMSE error estimator for discrete classification,” IEEE Transactions on
Signal Processing, vol. 59, no. 1, pp. 115–129, 2011b.
Dalton, L. A. and E. R. Dougherty, “Bayesian minimum mean-square error
estimation for classification error—Part II: Linear classification of
Gaussian models,” IEEE Transactions on Signal Processing, vol. 59,
no. 1, pp. 130–144, 2011c.
Dalton, L. A. and E. R. Dougherty, “Optimal mean-square-error calibration
of classifier error estimators under Bayesian models,” Pattern Recogni-
tion, vol. 45, no. 6, pp. 2308–2320, 2012a.
Dalton, L. A. and E. R. Dougherty, “Exact sample conditioned MSE
performance of the Bayesian MMSE estimator for classification error—
Part I: Representation,” IEEE Transactions on Signal Processing, vol. 60,
no. 5, pp. 2575–2587, 2012b.
Dalton, L. A. and E. R. Dougherty, “Exact sample conditioned MSE
performance of the Bayesian MMSE estimator for classification error—
Part II: Consistency and performance analysis,” IEEE Transactions on
Signal Processing, vol. 60, no. 5, pp. 2588–2603, 2012c.
327
References

Dalton, L. A. and E. R. Dougherty, “Optimal classifiers with minimum
expected error within a Bayesian framework—Part I: Discrete and Gaussian
models,” Pattern Recognition, vol. 46, no. 5, pp. 1301–1314, 2013a.
Dalton, L. A. and E. R. Dougherty, “Optimal classifiers with minimum
expected error within a Bayesian framework—Part II: Properties and
performance analysis,” Pattern Recognition, vol. 46, no. 5, pp. 1288–1300,
2013b.
Dalton, L. A. and E. R. Dougherty, “Intrinsically optimal Bayesian
robust filtering,” IEEE Transactions on Signal Processing, vol. 62,
no. 3, pp. 657–670, 2014.
Dalton, L. A. and M. R. Yousefi, “On optimal Bayesian classification and
risk estimation under multiple classes,” EURASIP Journal on Bioinfor-
matics and Systems Biology, vol. 2015, art. 8, 2015.
Dalton, L. A., M. E. Benalcázar, and E. R. Dougherty, “Optimal clustering
under uncertainty,” PLOS ONE, vol. 13, no. 10, art. e0204627, 2018.
Davison, A. and P. Hall, “On the bias and variability of bootstrap and cross-
validation estimates of error rates in discrimination problems,” Biome-
trica, vol. 79, pp. 274–284, 1992.
de Laplace, P. S., Théorie Analytique des Probabilitiés. Courceir, Paris, 1812.
Deev, A. D., “Representation of statistics of discriminant analysis and
asymptotic expansion when space dimensions are comparable with sample
size,” Doklady Akademii Nauk SSSR, vol. 195, pp. 759–762, 1970.
DeGroot, M. H., Optimal Statistical Decisions. McGraw-Hill, New York, 1970.
Dehghannasiri, R., B.-J. Yoon, and E. R. Dougherty, “Optimal experimental
design for gene regulatory networks in the presence of uncertainty,”
IEEE/ACM Transactions on Computational Biology and Bioinformatics,
vol. 14, no. 4, pp. 938–950, 2015.
Dehghannasiri, R., M. S. Esfahani, and E. R. Dougherty, “Intrinsically
Bayesian robust Kalman filter: An innovation process approach,” IEEE
Transactions on Signal Processing, vol. 65, no. 10, pp. 2531–2546, 2017a.
Dehghannasiri, R., X. Qian, and E. R. Dougherty, “Optimal experimental
design in the context of canonical expansions,” IET Signal Processing,
vol. 11, no. 8, pp. 942–951, 2017b.
Dehghannasiri, R., D. Xue, P. V. Balachandran, M. R. Yousefi, L. A.
Dalton, T. Lookman, and E. R. Dougherty, “Optimal experimental
design
for
materials
discovery,”
Computational
Materials
Science,
vol. 129, pp. 311–322, 2017c.
Dehghannasiri, R., X. Qian, and E. R. Dougherty, “Intrinsically Bayesian
robust Karhunen-Loève compression,” EURASIP Signal Processing,
vol. 144, pp. 311–322, 2018a.
Dehghannasiri, R., M. S. Esfahani, X. Qian, and E. R. Dougherty, “Optimal
Bayesian Kalman filtering with prior update,” IEEE Transactions on
Signal Processing, vol. 66, no. 8, pp. 1982–1996, 2018b.
328
References

Dembo, A., T. M. Cover, and J. A. Thomas, “Information theoretic
inequalities,” IEEE Transactions on Information Theory, vol. 37, no. 6,
pp. 1501–1518, 1991.
Devore, J. L., Probability and Statistics for Engineering and the Sciences.
Brooks/Cole, Pacific Grove, California, fourth edition, 1995.
Devroye, L., “Necessary and sufficient conditions for the almost everywhere
convergence of nearest neighbor regression function estimates,” Zeits-
chrift fur Wahrscheinlichkeitstheorie und verwandte Gebiete, vol. 61,
pp. 467–481, 1982.
Devroye, L., L. Gyorfi, and G. Lugosi, A Probabilistic Theory of Pattern
Recognition. Stochastic Modelling and Applied Probability, Springer-
Verlag, New York, 1996.
Diaconis, P. and D. Freedman, “On the consistency of Bayes estimates,”
Annals of Statistics, vol. 14, no. 1, pp. 1–26, 1986.
Dillies, M.-A., A. Rau, J. Aubert, C. Hennequet-Antier, M. Jeanmougin,
N. Servant, C. Keime, G. Marot, D. Castel, J. Estelle, G. Guernec,
B. Jagla, L. Jouneau, D. Laloë, C. Le Gall, B. Schaëffer, S. Le Crom,
M. Guedj, and F. Jaffrézic, “A comprehensive evaluation of normaliza-
tion methods for illumina high-throughput RNA sequencing data
analysis,” Briefings in Bioinformatics, vol. 14, no. 6, pp. 671–683, 2013.
Doob, J. L., “Application of the theory of martingales,” in Colloques
Internationaux du Centre National de la Recherche Scientifique, pp. 22–28,
Centre National de la Recherche Scientifique, Paris, 1948.
Dougherty, E. R., “On the epistemological crisis in genomics,” Current
Genomics, vol. 9, pp. 69–79, 2008.
Dougherty, E. R., “Biomarker discovery: Prudence, risk, and reproducibil-
ity,” Bioessays, vol. 34, no. 4, pp. 277–279, 2012.
Dougherty, E. R., The Evolution of Scientific Knowledge: From Certainty to
Uncertainty. SPIE Press, Bellingham, Washington, 2016 [doi: 10.1117/3.
22633620].
Dougherty, E. R., Optimal Signal Processing Under Uncertainty. SPIE Press,
Bellingham, Washington, 2018 [doi: 10.1117/3.2317891].
Dougherty, E. R. and M. L. Bittner, Epistemology of the Cell: A Systems
Perspective on Biological Knowledge. IEEE Press Series on Biomedical
Engineering, John Wiley & Sons, New York, 2011.
Dougherty, E. R., M. Brun, J. M. Trent, and M. L. Bittner, “Conditioning-
based modeling of contextual genomic regulation,” IEEE/ACM Transac-
tions on Computational Biology and Bioinformatics, vol. 6, no. 2,
pp. 310–320, 2009a.
Dougherty, E. R., J. Hua, and C. Sima, “Performance of feature selection
methods,” Current Genomics, vol. 10, no. 6, pp. 365–374, 2009b.
Duda, R. O., P. E. Hart, and D. G. Stork, Pattern Classification. John Wiley
& Sons, New York, second edition, 2001.
329
References

Dunsmore, I., “A Bayesian approach to classification,” Journal of the Royal
Statistical Society: Series B (Methodological), vol. 28, no. 3, pp. 568–577,
1966.
Ebrahimi, N., E. Maasoumi, and E. S. Soofi, “Measuring informativeness of
data by entropy and variance,” in Advances in Econometrics, Income
Distribution and Scientific Methodology, D. J. Slottje, Ed., Physica-
Verlag, Heidelberg, chapter 5, pp. 61–77, 1999.
Efron, B., “Bootstrap methods: another look at the jackknife,” Annals of
Statistics, vol. 7, no. 1, pp. 1–26, 1979.
Efron, B., “Estimating the error rate of a prediction rule: improvement on
cross validation,” Journal of the American Statistical Association, vol. 78,
pp. 316–331, 1983.
Esfahani, M. S. and E. R. Dougherty, “Effect of separate sampling on
classification accuracy,” Bioinformatics, vol. 30, no. 2, pp. 242–250, 2013.
Esfahani, M. S. and E. R. Dougherty, “Incorporation of biological pathway
knowledge in the construction of priors for optimal Bayesian classifica-
tion,” IEEE/ACM Transactions on Computational Biology and Bioinfor-
matics, vol. 11, no. 1, pp. 202–218, 2014.
Esfahani, M. S. and E. R. Dougherty, “An optimization-based framework
for the transformation of incomplete biological knowledge into a proba-
bilistic structure and its application to the utilization of gene/protein
signaling pathways in discrete phenotype classification,” IEEE/ACM
Transactions on Computational Biology and Bioinformatics, vol. 12, no. 6,
pp. 1304–1321, 2015.
Fawcett, T., “An introduction to ROC analysis,” Pattern Recognition Letters,
vol. 27, pp. 861–874, 2006.
Ferguson, T. S., “A Bayesian analysis of some nonparametric problems,”
Annals of Statistics, vol. 1, no. 2, pp. 209–230, 1973.
Feynman, R., QED: The Strange Theory of Light and Matter. Princeton
University Press, Princeton, New Jersey, 1985.
Foley, D., “Considerations of sample and feature size,” IEEE Transactions on
Information Theory, vol. 18, no. 5, pp. 618–626, 1972.
Frazier, P. I., W. B. Powell, and S. Dayanik, “A knowledge-gradient policy
for sequential information collection,” SIAM Journal on Control and
Optimization, vol. 47, no. 5, pp. 2410–2439, 2008.
Freedman, D. A., “On the asymptotic behavior of Bayes’ estimates in the discrete
case,” Annals of Mathematical Statistics, vol. 34, no. 4, pp. 1386–1403, 1963.
Fujikoshi, Y., “Error bounds for asymptotic approximations of the linear
discriminant function when the sample size and dimensionality are large,”
Journal of Multivariate Analysis, vol. 73, pp. 1–17, 2000.
Geisser, S., “Posterior odds for multivariate normal classifications,” Journal
of the Royal Statistical Society: Series B (Methodological), vol. 26, no. 1,
pp. 69–76, 1964.
330
References

Geisser, S., “Estimation associated with linear discriminants,” Annals of
Mathematical Statistics, vol. 38, no. 3, pp. 807–817, 1967.
Gelman, A., J. B. Carlin, H. S. Stern, and D. B. Rubin, Bayesian Data Analysis.
Chapman & Hall/CRC, Boca Raton, Florida, second edition, 2004.
Ghaffari, N., M. R. Yousefi, C. D. Johnson, I. Ivanov, and E. R. Dougherty,
“Modeling the next generation sequencing sample processing pipeline for
the purposes of classification,” BMC Bioinformatics, vol. 14, art. 307,
2013.
Gilks, W. R., S. Richardson, and D. Spiegelhalter, Markov Chain Monte
Carlo in Practice. CRC press, Boca Raton, Florida, 1995.
Glick, N., “Additive estimators for probabilites of correct classification,”
Pattern Recognition, vol. 10, no. 3, pp. 211–222, 1978.
Goldstein, M. and E. Wolf, “On the problem of bias in multinomial
classification,” Biometrics, vol. 33, pp. 325–31, 1977.
Golovin, D., A. Krause, and D. Ray, “Near-optimal Bayesian active learning
with noisy observations,” in Proceedings of the 23rd International
Conference on Neural Information Processing Systems, pp. 766–774, 2010.
Gordon, L. and R. Olshen, “Asymptotically efficient solutions to the
classification problem,” Annals of Statistics, vol. 6, no. 3, pp. 515–533,
1978.
Gozzolino, J. M., R. Gonzalez-Zubieta, and R. L. Miller, “Markovian
decision processes with uncertain transition probabilities,” Technical
report, Massachusetts Institute of Technology Operations Reserach
Center, 1965.
Grigoryan, A. M. and E. R. Dougherty, “Bayesian robust optimal linear
filters,” Signal Processing, vol. 81, no. 12, pp. 2503–2521, 2001.
Guiasu, S. and A. Shenitzer, “The principle of maximum entropy,”
The Mathematical Intelligencer, vol. 7, no. 1, pp. 42–48, 1985.
Gupta, A. K., D. K. Nagar, and L. E. Sánchez, “Properties of matrix variate
confluent hypergeometric function distribution,” Journal of Probability
and Statistics, vol. 2016, art. 2374907, 2016.
Guttman, I. and G. C. Tiao, “A Bayesian approach to some best population
problems,” The Annals of Mathematical Statistics, vol. 35, no. 2,
pp. 825–835, 1964.
Hajiramezanali, E., M. Imani, U. Braga-Neto, X. Qian, and E. R. Dougherty,
“Scalable optimal Bayesian classification of single-cell trajectories under
regulatory model uncertainty,” BMC Genomics, vol. 20, art. 435, 2019.
Halvorsen, K. B., V. Ayala, and E. Fierro, “On the marginal distribution of
the diagonal blocks in a blocked Wishart random matrix,” International
Journal of Analysis, vol. 2016, art. 5967218, 2016.
Hanczar, B., J. Hua, C. Sima, J. Weinstein, M. Bittner, and E. R. Dougherty,
“Small-sample precision of ROC-related estimates,” Bioinformatics,
vol. 26, no. 6, pp. 822–830, 2010.
331
References

Hand, D. J. and R. J. Till, “A simple generalisation of the area under the
ROC curve for multiple class classification problems,” Machine Learning,
vol. 45, pp. 171–186, 2001.
Hansen, L. P. and K. J. Singleton, “Generalized instrumental variables
estimation of nonlinear rational expectations models,” Econometrica:
Journal of the Econometric Society, vol. 50, no. 5, pp. 1269–1286, 1982.
Hassan, S. S., H. Huttunen, J. Niemi, and J. Tohka, “Bayesian receiver
operating characteristic metric for linear classifiers,” Pattern Recognition
Letters, vol. 128, pp. 52–59, 2019.
Hastie, T., R. Tibshirani, and J. Friedman, The Elements of Statistical
Learning: Data Mining, Inference, and Prediction. Springer, New York,
2009.
Hills, M., “Allocation rules and their error rates,” Journal of the Royal
Statistical Society: Series B (Methodological), vol. 28, no. 1, pp. 1–31, 1966.
Hua, J., Z. Xiong, J. Lowey, E. Suh, and E. R. Dougherty, “Optimal number
of features as a function of sample size for various classification rules,”
Bioinformatics, vol. 21, no. 8, pp. 1509–1515, 2005.
Hua, J., W. D. Tembe, and E. R. Dougherty, “Performance of feature-
selection methods in the classification of high-dimension data,” Pattern
Recognition, vol. 42, no. 3, pp. 409–424, 2009.
Hua, J., C. Sima, M. Cypert, G. C. Gooden, S. Shack, L. Alla, E. A. Smith,
J.
M.
Trent,
E.
R.
Dougherty,
and
M.
L.
Bittner,
“Tracking
transcriptional
activities
with high-content
epifluorescent
imaging,”
Journal of Biomedical Optics, vol. 17, no. 4, art. 046008, 2012 [doi: 10.
1117/1.JBO.17.4.046008].
Hughes, G. F., “On the mean accuracy of statistical pattern recognizers,”
IEEE Transactions on Information Theory, vol. 14, no. 1, pp. 55–63, 1968.
Imani, M. and U. M. Braga-Neto, “Particle filters for partially-observed
Boolean dynamical systems,” Automatica, vol. 87, pp. 238–250, 2018.
Imani, M., R. Dehghannasiri, U. M. Braga-Neto, and E. R. Dougherty,
“Sequential experimental design for optimal structural intervention in
gene
regulatory
networks
based
on
the
mean
objective
cost
of
uncertainty,” Cancer Informatics, vol. 17, art. 1176935118790247, 2018.
Jaynes, E. T., “Information theory and statistical mechanics,” Physical
Review, vol. 106, no. 4, pp. 620–630, 1957.
Jaynes, E. T., “Prior probabilities,” IEEE Transactions on Systems Science
and Cybernetics, vol. 4, no. 3, pp. 227–241, 1968.
Jaynes, E. T., “What is the question?” Bayesian Statistics, J. Bernardo, M.
deGroot, D. Lindly, and A. Smith, Eds., Valencia University Press,
Valencia, 1980.
Jeffreys, H., “An invariant form for the prior probability in estimation
problems,” Proceedings of the Royal Society of London: Series A
(Mathematical and Physical Sciences), vol. 186, no. 1007, pp. 453–461, 1946.
332
References

Jeffreys, H., Theory of Probability. Oxford University Press, London, 1961.
John, S., “Errors in discrimination,” Annals of Mathematical Statistics,
vol. 32, no. 4, pp. 1125–1144, 1961.
Johnson, M. E., Multivariate Statistical Simulation. Wiley Series in Applied
Probability and Statistics, John Wiley & Sons, New York, 1987.
Johnson, N. L., “Systems of frequency curves generated by methods of
translation,” Biometrika, vol. 36, no. 1-2, pp. 149–176, 1949.
Johnson, N. L., S. Kotz, and N. Balakrishnan, Continuous Univariate
Distributions, Volume 1. Wiley Series in Probability and Mathematical
Statistics, John Wiley & Sons, New York, second edition, 1994.
Johnson, N. L., S. Kotz, and N. Balakrishnan, Continuous Univariate
Distributions, Volume 2. Wiley Series in Probability and Mathematical
Statistics, John Wiley & Sons, New York, second edition, 1995.
Johnson, N. L., S.
Kotz, and N.
Balakrishnan, Discrete Multivariate
Distributions. Wiley Series in Probability and Mathematical Statistics,
John Wiley & Sons, New York, 1997.
Jones, D. R., M. Schonlau, and W. J. Welch, “Efficient global optimization of
expensive black-box functions,” Journal of Global Optimization, vol. 13,
no. 4, pp. 455–492, 1998.
Kan, R., “From moments of sum to moments of product,” Journal of
Multivariate Analysis, vol. 99, pp. 542–554, 2008.
Karbalayghareh, A., U. Braga-Neto, and E. R. Dougherty, “Intrinsically
Bayesian robust classifier for single-cell gene expression trajectories in
gene regulatory networks,” BMC Systems Biology, vol. 12(Suppl 3), art.
23, 2018a.
Karbalayghareh, A., X. Qian, and E. R. Dougherty, “Optimal Bayesian
transfer learning,” IEEE Transactions on Signal Processing, vol. 16,
no. 14, pp. 3724–3739, 2018b.
Karbalayghareh, A., X. Qian, and E. R. Dougherty, “Optimal Bayesian
transfer learning for count data,” IEEE/ACM Transactions on Computa-
tional Biology and Bioinformatics, 2019 [doi: 10.1109/TCBB.2019.2920981].
Kashyap, R., “Prior probability and uncertainty,” IEEE Transactions on
Information Theory, vol. 17, no. 6, pp. 641–650, 1971.
Kass, R. E. and L. Wasserman, “The selection of prior distributions by formal
rules,” Journal of the American Statistical Association, vol. 91, no. 435,
pp. 1343–1370, 1996.
Kassam, S. A. and T. L. Lim, “Robust Wiener filters,” Journal of the Franklin
Institute, vol. 304, no. 4-5, pp. 171–185, 1977.
Kilian, L. and H. Lütkepohl, Structural Vector Autoregressive Analysis.
Themes
in
Modern
Econometrics,
Cambridge
University
Press,
Cambridge, UK, 2017.
Kloeden, P. E. and E. Platen, Numerical Solution of Stochastic Differential
Equations. Springer, New York, 1995.
333
References

Knight, J. M., I. Ivanov, and E. R. Dougherty, “MCMC implementation of
the optimal Bayesian classifier for non-Gaussian models: Model-
based RNA-Seq classification,” BMC Bioinformatics, vol. 15, art. 401,
2014.
Knight, J. M., I. Ivanov, K. Triff, R. S. Chapkin, and E. R. Dougherty,
“Detecting multivariate gene interactions in RNA-Seq data using optimal
Bayesian classification,” IEEE/ACM Transactions on Computational
Biology and Bioinformatics, vol. 15, no. 2, pp. 484–493, 2018.
Knight, J. M., I. Ivanov, R. S. Chapkin, and E. R. Dougherty, “Detecting
multivariate gene interactions in RNA-Seq data using optimal Bayesian
classification,” IEEE/ACM Transactions on Computational Biology and
Bioinformatics, vol. 15, no. 2, pp. 484–493, 2019.
Kolmogorov, A. N., “Stationary sequences in Hilbert space,” Bulletin of
Mathematics, University of Moscow, vol. 2, no. 6, pp. 1–40, 1941.
Kotz, S. and S. Nadarajah, Multivariate t Distributions and Their Applica-
tions. Cambridge University Press, New York, 2004.
Kuznetsov, V. P., “Stable detection when the signal and spectrum of normal
noise are inaccurately known,” Telecommunications and Radio Engineer-
ing, vol. 30/31, pp. 58–64, 1976.
Lawoko, C. R. O. and G. J. McLachlan, “Some asymptotic results on the
effect of autocorrelation on the error rates of the sample linear
discriminant function,” Pattern Recognition, vol. 16, pp. 119–121, 1983.
Lawoko, C. R. O. and G. J. McLachlan, “Discrimination with autocorrelated
observations,” Pattern Recognition, vol. 18, pp. 145–149, 1985.
Lawoko, C. R. O. and G. J. McLachlan, “Asymptotic error rates of the
W and Z statistics when the training observations are dependent,” Pattern
Recognition, vol. 19, pp. 467–471, 1986.
Lindley, D. V., Bayesian Statistics: A Review. SIAM, Philadelphia, 1972.
Little, R. J. A. and D. B. Rubin, Statistical Analysis with Missing Data.
John Wiley & Sons, Hoboken, New Jersey, second edition, 2014.
Loève, M., Probability Theory II. Graduate Texts in Mathematics, Springer-
Verlag, New York, fourth edition, 1978.
Lunts, A. and V. Brailovsky, “Evaluation of attributes obtained in statistical
decision rules,” Engineering Cybernetics, vol. 3, pp. 98–109, 1967.
Lütkepohl, H., New Introduction to Multiple Time Series Analysis. Springer-
Verlag, Berlin, 2005.
Mardia, K. V., J. T. Kent, and J. M. Bibby, Multivariate Analysis. Academic
Press, London, 1979.
Martin, J. J., Bayesian Decision Problems and Markov Chains. John Wiley,
New York, 1967.
Mathai, A. M. and H. J. Haubold, Special Functions for Applied Scientists.
Springer, New York, 2008.
334
References

McLachlan, G. J., “An asymptotic expansion of the expectation of the
estimated error rate in discriminant analysis,” Australian Journal of
Statistics, vol. 15, pp. 210–214, 1973.
McLachlan, G. J., Discriminant Analysis and Statistical Pattern Recognition.
John Wiley & Sons, Hoboken, New Jersey, 2004.
Mohsenizadeh, D. N., R. Dehghannasiri, and E. R. Dougherty, “Optimal
objective-based experimental design for uncertain dynamical gene net-
works with experimental error,” IEEE/ACM Transactions on Computa-
tional Biology and Bioinformatics, vol. 15, no. 1, pp. 218–230, 2018.
Moran, M., “On the expectation of errors of allocation associated with a linear
discriminant function,” Biometrika, vol. 62, no. 1, pp. 141–148, 1975.
Muirhead, R. J., Aspects of Multivariate Statistical Theory. John Wiley &
Sons, Hoboken, New Jersey, 2009.
Muller, K. E. and P. W. Stewart, Linear Model Theory: Univariate,
Multivariate, and Mixed Models. John Wiley & Sons, Hoboken,
New Jersey, 2006.
Murphy, K. P., Machine Learning: A Probabilistic Perspective. MIT Press,
Cambridge, Massachusetts, 2012.
Nagar, D. K. and J. C. Mosquera-Benıtez, “Properties of matrix variate
hypergeometric function distribution,” Applied Mathematical Sciences,
vol. 11, no. 14, pp. 677–692, 2017.
Nagaraja, K. and U. Braga-Neto, “Bayesian classification of proteomics
biomarkers from selected reaction monitoring data using an approximate
Bayesian computation-Markov chain Monte Carlo approach,” Cancer
Informatics, vol. 17, art. 1176935118786927, 2018.
Natalini, P. and B. Palumbo, “Inequalities for the incomplete gamma
function,” Mathematical Inequalities & Applications, vol. 3, no. 1,
pp. 69–77, 2000.
Neal, R. M., “MCMC using Hamiltonian dynamics,” in Handbook of
Markov Chain Monte Carlo, S. Brooks, A. Gelman, G. L. Jones, and
X.-L. Meng, Eds., Chapman & Hall/CRC, Boca Raton, Florida,
Chapter 5, pp. 113–162, 2011.
O’Hagan, A. and J. Forster, Kendalls Advanced Theory of Statistics,
Volume 2B: Bayesian Inference. Hodder Arnold, London, second
edition, 2004.
Okamoto, M., “An asymptotic expansion for the distribution of the linear
discriminant function,” Annals of Mathematical Statistics, vol. 34, no. 4,
pp. 1286–1301, 1968.
Pan, S. J. and Q. Yang, “A survey on transfer learning,” IEEE Transactions
on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345–1359, 2010.
Pepe, M. S., H. Janes, G. Longton, W. Leisenring, and P. Newcomb,
“Limitations of the odds ratio in guaging the performance of a diagnositc,
335
References

prognostic, or screening marker,” American Journal of Epidemiology,
vol. 159, pp. 882–890, 2004.
Pikelis, V., “Comparison of methods of computing the expected classification
errors,” Automation and Remote Control, vol. 5, no. 7, pp. 59–63, 1976.
Poor, H., “On robust Wiener filtering,” IEEE Transactions on Automatic
Control, vol. 25, no. 3, pp. 531–536, 1980.
Qian, X. and E. R. Dougherty, “Bayesian regression with network prior:
Optimal Bayesian filtering perspective,” IEEE Transactions on Signal
Processing, vol. 64, no. 23, pp. 6243–6253, 2016.
Raiffa, H. and R. Schlaifer, Applied Statistical Decision Theory. MIT Press,
Cambridge, Massachusetts, 1961.
Raudys, S., “On determining training sample size of a linear classifier,”
Computing Systems, vol. 28, pp. 79–87, 1967.
Raudys, S., “On the amount of a priori information in designing the
classification algorithm,” Technical Cybernetics, vol. 4, pp. 168–174, 1972.
Raudys, S. and D. M. Young, “Results in statistical discriminant analysis:
A review of the former soviet union literature,” Journal of Multivariate
Analysis, vol. 89, no. 1, pp. 1–35, 2004.
Rissanen, J., “A universal prior for integers and estimation by minimum
description length,” Annals of Statistics, vol. 11, no. 2, pp. 416–431, 1983.
Rodriguez, C. C., “Entropic priors,” Technical report, State University of
New York at Albany, Department of Mathematics and Statistics, Albany,
New York, 1991.
Rowe, D. B., Multivariate Bayesian Statistics: Models for Source Separation
and Signal Unmixing. Chapman & Hall/CRC, Boca Raton, Florida, 2003.
Saar-Tsechansky, M. and F. Provost, “Active sampling for class probability
estimation and ranking,” Machine Learning, vol. 54, no. 2, pp. 153–178,
2004.
Sen, P. K. and J. M. Singer, Large Sample Methods in Statistics. Chapman &
Hall, New York, 1993.
Serdobolskii, V., Multivariate Statistical Analysis: A High-Dimensional
Approach. Springer, New York, 2000.
Shaw, W. T., “Sampling Student’s T distribution - use of the inverse
cumulative distribution function,” Journal of Computational Finance,
vol. 9, no. 4, pp. 37–73, 2004.
Silver, E. A., “Markovian decision processes with uncertain transition
probabilities or rewards,” Technical report, Massachusetts Institute of
Technology Operations Research Center, Cambridge, 1963.
Sima, C. and E. R. Dougherty, “Optimal convex error estimators for
classification,” Pattern Recognition, vol. 39, no. 6, pp. 1763–1780, 2006a.
Sima, C. and E. R. Dougherty, “What should be expected from feature
selection in small-sample settings,” Bioinformatics, vol. 22, no. 19,
pp. 2430–2436, 2006b.
336
References

Sima, C. and E. R. Dougherty, “The peaking phenomenon in the presence of
feature selection,” Pattern Recognition Letters, vol. 29, pp. 1667–1674, 2008.
Sima, C., U. M. Braga-Neto, and E. R. Dougherty, “Superior feature-set
ranking for small samples using bolstered error estimation,” Bioinformat-
ics, vol. 21, no. 7, pp. 1046–1054, 2005.
Sima, C., U. M. Braga-Neto, and E. R. Dougherty, “High-dimensional
bolstered error estimation,” Bioinformatics, vol. 27, no. 21, pp. 3056–
3064, 2011.
Sitgreaves, R., “Some results on the distribution of the W-classification
statistic,” in Studies in Item Analysis and Prediction, H. Solomon, Ed.,
Stanford University Press, Stanford, California, pp. 241–251, 1961.
Slater, L. J., Generalized Hypergeometric Functions. Cambridge University
Press, Cambridge, UK, 1966.
Sorum,
M. J., Estimating
the Probability
of
Misclassification. Ph.D.
Dissertation, University of Minnesota, Minneapolis, 1968.
Sorum, M. J., “Estimating the conditional probability of misclassification,”
Technometrics, vol. 13, pp. 333–343, 1971.
Spackman, K. A., “Signal detection theory: Valuable tools for evaluating
inductive learning,” in Proceedings of the 6th International Workshop on
Machine Learning, pp. 160–163, 1989.
Spall, J. C. and S. D. Hill, “Least-informative Bayesian prior distributions for
finite samples based on information theory,” IEEE Transactions on
Automatic Control, vol. 35, no. 5, pp. 580–583, 1990.
Stone, C. J., “Consistent nonparametric regression,” Annals of Statistics,
vol. 5, no. 4, pp. 595–620, 1977.
Stone, M., “Cross-validatory choice and assessment of statistical predictions,”
Journal of the Royal Statistical Society: Series B (Methodological),
vol. 36, no. 2, pp. 111–147, 1974.
Tubbs, J. D., “Effect of autocorrelated training samples on Bayes’ probability
of misclassification,” Pattern Recognition, vol. 12, pp. 351–354, 1980.
Vapnik, V. N., “On the uniform convergence of relative frequencies of events
to their probabilities,” Theory of Probability and its Applications, vol. 16,
pp. 264–280, 1971.
Walker, G. A. and J. G. Saw, “The distribution of linear combinations of
t-variables,” Journal of the American Statistical Association, vol. 73,
no. 364, pp. 876–878, 1978.
Weiss, K., T. M. Khoshgoftaar, and D. Wang, “A survey of transfer
learning,” Journal of Big Data, vol. 3, art. 9, 2016.
Wiener, N., Extrapolation, Interpolation and Smoothing of Stationary Time
Series, with Engineering Applications. MIT Press, Cambridge, 1949.
Wyman, F. J., D. M. Young, and D. W. Turner, “A comparison of
asymptotic error rate expansions for the sample linear discriminant
function,” Pattern Recognition, vol. 23, no. 7, pp. 775–783, 1990.
337
References

Xu, H., C. Caramanis, and S. Mannor, “Robustness and regularization of
support vector machines,” Journal of Machine Learning Research, vol. 10,
pp. 1485–1510, 2009a.
Xu, H., C. Caramanis, S. Mannor, and S. Yun, “Risk sensitive robust support
vector machines,” in Proceedings of the Joint 48th IEEE Conference on
Decision and Control and 28th Chinese Control Conference, pp. 4655–
4661, 2009b.
Xu, Q., J. Hua, U. M. Braga-Neto, Z. Xiong, E. Suh, and E. R. Dougherty,
“Confidence intervals for the true classification error conditioned on the
estimated error,” Technology in Cancer Research and Treatment, vol. 5,
pp. 579–590, 2006.
Yoon, B.-J., X. Qian, and E. R. Dougherty, “Quantifying the objective cost of
uncertainty in complex dynamical systems,” IEEE Transactions on Signal
Processing, vol. 61, no. 9, pp. 2256–2266, 2013.
Yousefi, M. R. and E. R. Dougherty, “A comparison study of optimal and
suboptimal intervention policies for gene regulatory networks in the
presence of uncertainty,” EURASIP Journal on Bioinformatics and
Systems Biology, vol. 2014, art. 6, 2014.
Zapała, A. M., “Unbounded mappings and weak convergence of measures,”
Statistics & Probability Letters, vol. 78, pp. 698–706, 2008.
Zellner, A., Maximal Data Information Prior Distributions, Basic Issues in
Econometrics. University of Chicago Press, Chicago, 1984.
Zellner, A., Past and Recent Results on Maximal Data Information Priors.
Working Paper Series in Economics and Econometrics, University of
Chicago, Graduate School of Business, Department of Economics,
Chicago, 1995.
Zellner, A., “Models, prior information, and Bayesian analysis,” Journal of
Econometrics, vol. 75, no. 1, pp. 51–68, 1996.
Zollanvari, A. and E. R. Dougherty, “Moments and root-mean-square error
of the Bayesian MMSE estimator of classification error in the Gaussian
model,” Pattern Recognition, vol. 47, no. 6, pp. 2178–2192, 2014.
Zollanvari, A. and E. R. Dougherty, “Incorporating prior knowledge induced
from stochastic differential equations in the classification of stochastic
observations,” EURASIP Journal on Bioinformatics and Systems Biology,
vol. 2016, art. 2, 2016.
Zollanvari, A. and E. R. Dougherty, “Optimal Bayesian classification with
autoregressive data dependency,” IEEE Transactions on Signal Proces-
sing, vol. 67, no. 12, pp. 3073–3086, 2019.
Zollanvari, A., U. M. Braga-Neto, and E. R. Dougherty, “On the sampling
distribution of resubstitution and leave-one-out error estimators for linear
classifiers,” Pattern Recognition, vol. 42, no. 11, pp. 2705–2723, 2009.
Zollanvari, A., U. M. Braga-Neto, and E. R. Dougherty, “On the joint
sampling distribution between the actual classification error and the
338
References

resubstitution and leave-one-out error estimators for linear classifiers,”
IEEE Transactions on Information Theory, vol. 56, no. 2, pp. 784–804, 2010.
Zollanvari, A., U. M. Braga-Neto, and E. R. Dougherty, “Analytic study of
performance of error estimators for linear discriminant analysis,” IEEE
Transactions on Signal Processing, vol. 59, no. 9, pp. 4238–4255, 2011.
Zollanvari, A., U. M. Braga-Neto, and E. R. Dougherty, “Exact representa-
tion of the second-order moments for resubstitution and leave-one-out
error estimation for linear discriminant analysis in the univariate
heteroskedastic Gaussian model,” Pattern Recognition, vol. 45, no. 2,
pp. 908–917, 2012.
Zollanvari, A., J. Hua, and E. R. Dougherty, “Analytical study of
performance of linear discriminant analysis in stochastic settings,” Pattern
Recognition, vol. 46, pp. 3017–3029, 2013.
339
References


Index
0.632 bootstrap estimator, 13
A
a priori probability, 2
action space, 220
activating pathway segment
(APS), 312
almost surely, 3
almost uniformly (a.u.) integrable,
198
Anderson W statistic, 8
Appell hypergeometric function,
122
approximate Bayesian computation
(ABC), 219
area under the ROC curve
(AUC), 91
autoregressive process of order p, 225
B
Bayes classifier, 1
Bayes decision rule (BDR), 239
Bayes error, 1
Bayesian conditional risk estimator
(BCRE), 242
Bayesian–Kolmogorov asymptotic
conditions (BKac), 145
Bayesian MMSE error estimator, 26
Bayesian risk estimator (BRE), 240
beta distribution, 30
bias, 12
binary classification of Gaussian
processes (BCGP), 296
bolstered empirical distribution, 13
bolstered leave-one-out
estimation, 14
bolstered-resubstitution
estimator, 13
bolstering kernel, 13
bootstrap, 13
bootstrap sample, 13
C
characteristics of random
processes, 170
class-conditional distribution, 1
class-y-conditional density, 238
classification error, 1
classification probability, 239
classification rule, 3
classification rule model, 12
classifier, 1
classifier model, 11
conditional MSE, 101
conditional MSE convergence, 136
conditional risk, 238
conditional uncertainty class, 221
conditioning parameter, 308
confluent hypergeometric
function, 264
conjugate prior, 27
consistent estimators, 77
consistent in the rth mean
(estimators), 77
consistent rule, 3
convex estimator, 13
341

cost function, 169–170
cost of constraint, 5
cross-validation, 12
crosstalk parameter, 308
cubic histogram rule, 4
cumulative distribution function
(CDF), 9
D
d-dimensional Wiener process, 297
design cost, 3
deviation variance, 12
Dirichlet priors, 36
discriminant functions, 5
dispersion matrix, 298
double-asymptotic expansions, 144
down-regulated (DR, or 0), 312
drift vector, 298
E
effective characteristics, 171
effective class-conditional
density, 32
effective conditional density, 247
effective density, 241
effective joint class-conditional
density, 104
effective joint density, 247
effective processes, 172
empirical error classification
rule, 9–10
equicontinuous function, 81
error estimation rule, 12
expected AUC (EAUC), 94
expected design cost, 3
expected FPR (EFPR), 92
expected mean log-likelihood, 306
expected risk, 238
expected ROC (EROC) curve, 93
expected TPR (ETPR), 92
experiment space, 220
experimental design value, 221
experiments, 220
F
false positive rate (FPR)
feature-label distribution, 1
feature selection, 10
feature vector, 1
features, 1
flat prior, 27
G
Gauss hypergeometric function, 264
Gauss-hypergeometric-function
distribution, 268
Gaussian, 6
generalized hypergeometric
function, 263–264
global (homogeneous) markers, 292
H
Hamilton Monte Carlo OBC
(OBC-HMC), 217
Helly–Bray theorem, 77
heterogeneous markers, 293
heteroscedastic model, 7
high-variance non-markers, 293
histogram rule, 19
holdout error estimator, 12
homoscedastic model, 7
I
IBR action, 220
IBR classifiers, 210–212
ideal regression, 86
improper priors, 30
independent Jeffreys prior, 50
intrinsically Bayesian robust (IBR)
operator, 171
inverse-gamma distribution,
54–55
inverse-Wishart distribution, 47
J
Jeffreys rule prior, 50
Johnson SB, 70
342
Index

Johnson SU, 70
joint normal-Wishart distribution, 262
K
k-fold cross-validation, 12
k-nearest-neighbor (kNN) rule, 4
Kullback–Liebler (KL) divergence,
306
L
label, 1
leave-one-out estimator, 13
likelihood function, 29, 240
linear discriminant analysis (LDA), 7
loss function, 238
low-variance non-markers, 293
M
Mahalanobis distance, 24
marginal prior densities, 27
Markov chain Monte Carlo
(MCMC), 212
Markov property, 298
mathematical model, 18
maximal data information, 305
maximal data information
prior (MDIP), 305
maximal knowledge-driven
information prior
(MKDIP), 304
maximum entropy, 305
principle of, 305
mean objective cost of uncertainty
(MOCU), 220
mean-square error (MSE), 12
missing completely at random
(MCAR), 212
MKDIP with constraints, 304
MMSE calibrated error estimate, 85
MMSE calibration function, 85
MMSE error estimator, 25
model-constrained Bayesian robust
(MCBR) operator, 171–172, 210
multinomial discrimination, 19, 36
multivariate beta function, 36
multivariate gamma function, 59
multivariate Gaussian process, 296
multivariate t-distribution, 55
N
nature of the prior information, 301
nearest-mean-classification (NMC)
rule, 8
non-informative prior, 27
non-standardized Student’s
t-distribution, 57
normal-inverse-Wishart
distribution, 48
O
objective cost of uncertainty, 220
objective function, 170
objective prior, 27
observation time vector, 296
one-dimensional Wiener process,
297
optimal action, 220
optimal Bayesian classifier
(OBC), 173
optimal Bayesian operator
(OBO), 171
optimal Bayesian risk classifier
(OBRC), 245
optimal Bayesian transfer learning
classifier (OBTLC), 273
optimal operator, 170
P
peaking phenomenon, 11
plug-in classification rule, 3
plug-in error estimator, 15
posterior distribution, 26, 221
posterior distribution of target, 268
predictive density, 174
prior distribution, 26, 220
prior probabilities, 27
343
Index

probability density function, 13
proper priors, 30
Q
quadratic discriminant analysis
(QDA), 7
R
Radon–Nikodym theorem, 86
random sampling, 16
Raudys–Kolmogorov asymptotics,
145
Raudys-type Gaussian-based finite-
sample approximation, 147
receiver operator characteristic
(ROC) curves, 91
regularized expected mean log-
likelihood prior (REMLP), 307
regularized incomplete beta
function, 62
regularized maximal data
information prior (RMDIP),
307
regularized maximum entropy prior
(RMEP), 307
regulatory set, 313
relatedness between source and
target domains, 278
remaining MOCU, 221
repressing pathway segment
(RPS), 312
residual IBR cost, 221
resubstitution error estimator, 12
RMS conditioned on the true error, 89
RNA sequencing, 281–282
root-mean-square (RMS) error, 12
S
sample-conditioned MSE, 101, 245
sample path, 296
sampling distribution, 3
scientific gap, 172
semi-bolstered-redistribution
estimator, 14
separate sampling, 14
slackness variable, 305
source domain, 261
stochastic differential equation
(SDE), 295–296
stratified sampling, 16
strongly consistent estimators, 77
strongly consistent (rule), 3
surrogate classifiers, 12
T
target domain, 261
test point, 173
transfer learning, 261
transition probabilities, 298
true positive rate (TPR), 91
U
uncertainty actions, 220
uncertainty class, 21, 171, 220
uncertainty parameters, 220
universally consistent (rule), 3
up-regulated (UR, or 1), 312
V
valid prior knowledge, 299
Vapnik–Chervonenkis (VC)
dimension, 10
W
weak* consistent posterior, 78
weak topology, 77
weakly consistent estimators, 77
Wiener filter, 170
Wiener–Hopf equation, 170
Wishart distribution, 262
Z
zero-one loss function, 239
Zipf’s power law model, 42
344
Index


Lori A. Dalton received the B.S., M.S., and Ph.D. degrees in electrical
engineering at Texas A&M University, College Station, Texas, in 2001, 2002
and 2012, respectively. She is currently an Adjunct Professor of Electrical and
Computer Engineering at The Ohio State University. Dr. Dalton was awarded
an NSF CAREER Award in 2014. Her current research interests include
pattern recognition, estimation, and optimization, genomic signal processing,
systems biology, bioinformatics, robust filtering, and information theory.
Edward R. Dougherty is a Distinguished Professor in the Department of
Electrical and Computer Engineering at Texas A&M University in College
Station, Texas, where he holds the Robert M. Kennedy ‘26 Chair in Electrical
Engineering. He holds a Ph.D. in mathematics from Rutgers University
and an M.S. in Computer Science from Stevens Institute of Technology,
and was awarded an honoris causa doctorate by the Tampere University of
Technology in Finland. His previous works include SPIE Press titles Optimal
Signal Processing Under Uncertainty (2018), The Evolution of Scientific
Knowledge: From Certainty to Uncertainty (2016), and Hands-on Morphological
Image Processing (2003).


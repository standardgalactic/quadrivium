A Primer for 
Undergraduate 
Research
Aaron Wootton
Valerie Peterson
Christopher Lee
Editors
From Groups and Tiles to Frames 
and Vaccines
Foundations for Undergraduate Research in Mathematics


Foundations for Undergraduate Research
in Mathematics
Series editor
Aaron Wootton
Department of Mathematics, University of Portland, Portland, USA
More information about this series at http://www.springer.com/series/15561

Aaron Wootton • Valerie Peterson
Christopher Lee
Editors
A Primer for
Undergraduate Research
From Groups and Tiles to Frames
and Vaccines

Editors
Aaron Wootton
Department of Mathematics
University of Portland
Portland, OR, USA
Christopher Lee
Department of Mathematics
University of Portland
Portland, OR, USA
Valerie Peterson
Department of Mathematics
University of Portland
Portland, OR, USA
ISSN 2520-1212
ISSN 2520-1220
(electronic)
Foundations for Undergraduate Research in Mathematics
ISBN 978-3-319-66064-6
ISBN 978-3-319-66065-3
(eBook)
DOI 10.1007/978-3-319-66065-3
Library of Congress Control Number: 2017959749
Mathematics Subject Classiﬁcation: 00A05, 00A07, 00A08, 00B10
© Springer International Publishing AG 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information
storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology
now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or
the editors give a warranty, express or implied, with respect to the material contained herein or for any
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional
claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This book is published under the trade name Birkhäuser, www.birkhauser-science.com
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Contents
Coxeter Groups and the Davis Complex ......................................
1
Timothy A. Schroeder
A Tale of Two Symmetries: Embeddable and Non-embeddable
Group Actions on Surfaces......................................................
35
Valerie Peterson and Aaron Wootton
Tile Invariants for Tackling Tiling Questions .................................
61
Michael P. Hitchman
Forbidden Minors: Finding the Finite Few ...................................
85
Thomas W. Mattman
Introduction to Competitive Graph Coloring.................................
99
C. Dunn, V. Larsen, and J.F. Nordstrom
Matroids........................................................................... 127
Erin McNicholas, Nancy Ann Neudauer, and Colin Starr
Finite Frame Theory............................................................. 145
Somantika Datta and Jesse Oldroyd
Mathematical Decision-Making with Linear and Convex Programming.. 171
Jakob Kotas
Computing Weight Multiplicities............................................... 193
Pamela E. Harris
Vaccination Strategies for Small Worlds ...................................... 223
Winfried Just and Hannah Callender Highlander
Steady and Stable: Numerical Investigations of Nonlinear Partial
Differential Equations ........................................................... 265
R. Corban Harwood
Index............................................................................... 305
vii

Coxeter Groups and the Davis Complex
Timothy A. Schroeder
Suggested Prerequisites. Group theory, Graph theory, Combinatorial topology.
1
Introduction
This chapter presents topics in an area of mathematics at the intersection of geome-
try, topology, and algebra called Geometric Group Theory. It is likely that students
have been exposed to geometry and abstract algebra topics as undergraduates. Some
reading this may have also been introduced to topology, as well. In this chapter, we
will be using terms and concepts from each of these areas, the point being to develop
a working knowledge of these terms and concepts that allows us to progress toward
our goal: A reﬂection group acting on a topological space. We will not spend much
time on topological details (students are certainly encouraged to pursue that subject
formally). Geometric details, referenced throughout, but speciﬁcally in Section 3.2,
are suggested as a project in Section 6. That leaves algebra. Perhaps the best, and
most familiar place to begin. We recall the deﬁnition of a group G:
Deﬁnition 1. A group G is a set G, together with a closed binary operation,
denoted ·, such that the following hold:
•
Associativity: For all a, b, c ∈G, (a · b) · c = a · (b · c).
•
Identity: There is an element e in G such that for all x ∈G, e · x = x · e = x.
•
Inverse: For every a ∈G, there is an element a′ ∈G such that a · a′ = a′ · a = e.
Such an element a′ is unique, and is denoted a−1.
T.A. Schroeder ()
Murray State University, Murray, KY 42071, USA
e-mail: tschroeder@murraystate.edu
© Springer International Publishing AG 2017
A. Wootton et al. (eds.), A Primer for Undergraduate Research, Foundations for
Undergraduate Research in Mathematics, DOI 10.1007/978-3-319-66065-3_1
1

2
T.A. Schroeder
Students in an abstract algebra course begin with this deﬁnition, and likely
embark on a journey through group theory: element orders, subgroups, cosets,
homomorphisms, isomorphisms, cyclic groups, generators, etc. Eventually, students
may be introduced to group actions, along with their stabilizers and orbits. All the
while, most likely, using the favorite dihedral groups as prototypes.
As useful and correct and edifying as that exposure is, it may strike students
as somewhat sterile. The groups are axiomatically presented, they are thought of
abstractly, and pictures (aside from regular polygons or the occasional Cayley graph
of a ﬁnite group) may be few and far between. It is our intention that the groups we
present in this context will have a decidedly constructive, and even geometric, ﬂavor
to them.
In this chapter, we will study ﬁnitely presented groups, speciﬁcally Coxeter
groups, and we will present a constructive view of a topological space on which
these Coxeter groups act. We begin with an overview of group presentations and
graph theory, then deﬁne Coxeter groups and the associated spaces. The reader
should note that many technical terms are italicized, and can be referenced in the
included citations or elsewhere.
2
Group Presentations and Graphs
Let A be a set and deﬁne the set A−1 = {a−1 | a ∈A}. Deﬁne WA to be the
collection of all ﬁnite length words in A ∪A−1. A “word” being a ﬁnite string of
elements from the set A ∪A−1. (We are thinking of the set A as our “alphabet.”) For
example, if A = {a, b}, then a, ab, ba, aba−1, bb−1ab are all elements of the set of
words WA. Perhaps you sense that this list is redundant. That is, you may already be
thinking that two of the words are actually equivalent. You’re right, of course, for
as the notation suggests, elements of the set A−1 are to play the role of inverses. To
make this clear, let’s get a little more formal.
First, to the set WA, we include the word comprised of no elements, the so-called
empty word, denoted by the symbol 1.
Now, to have a group, we must have an associative operation, usually described
and understood as some sort of multiplication. So, on the set WA, we deﬁne
multiplication by concatenation. (In other words, “put next to each other.”) In the
example above, to multiply ba and aba−1, we have ba · aba−1 = baaba−1 =
ba2ba−1. You should verify that, in general, concatenation is in fact associative.
Next, we say w2 ∈WA is obtained from w1 ∈WA by an elementary reduction
(or expansion) if w2 is obtained from w1 by deleting (or inserting) a sub-word of
the form aa−1 or a−1a, for some element a ∈A. We say that two words w and
w′ are equivalent if we may pass from w to w′ by a ﬁnite sequence of elementary
reductions or expansions (both are allowed), and we write w ∼w′. (As you may
have guessed, this means that in our list above, ab ∼bb−1ab.) The relation ∼
deﬁnes an equivalence relation on the set WA. (See Exercise 1.)

Coxeter Groups and the Davis Complex
3
Finally, let FA be the collection of equivalence classes of ∼, where for w ∈WA,
[w] denotes the equivalence class containing w. Concatenation on WA induces a well-
deﬁned operation on FA: For u, v ∈WA, [u] ·[v] = [uv]. (See Exercise 2.) In fact, FA
is a group with identity the equivalence class containing 1. It is called the free group
on A.
2.1
Group Presentations
It could be the case that within a set of words, or within a group, there could be
many ways, besides using elementary reductions or expansions, to represent a given
element. To handle this more complicated situation, we add what are called relators
to our group, and study presentations of groups.
Let A be a set and consider again FA, the free group on A. Let R be a set of words
in the alphabet A ∪A−1, and deﬁne N(R) to be the smallest normal subgroup of FA
containing the equivalence classes of the elements of R. (This normal subgroup is
formed by taking the collection of all ﬁnite products of conjugates of elements of R
and their inverses in FA.) We have the following deﬁnition.
Deﬁnition 2. Let A, R and N(R) be as above. The group deﬁned by the presentation
⟨A | R⟩is the quotient group FA/N(R).
That is, a group deﬁned by a presentation is the quotient of a free group by the
normal subgroup generated by the words in some set R. We equate a group with its
presentation, writing FA/N(R) = ⟨A | R⟩, but note that it is the case that a given
group can have multiple presentations.
If A is a ﬁnite set (for it could be the case that A is an inﬁnite set), we say the
corresponding group is ﬁnitely generated. If R is also ﬁnite, we say that the group is
ﬁnitely presented. For reference on group presentations, see [6] or [10].
In practice we often take a slightly more constructive approach to deﬁning
and working with the elements of a ﬁnitely presented group. We highlight this
perspective next.
2.1.1
A Constructive Approach
Resetting the table, consider the set WA of words in the alphabet A ∪A−1, where A
is some ﬁnite set, and let R be a ﬁnite set of words contained in WA. The elements of
R, again called “relators,” will serve to identify certain words in WA, besides those
identiﬁed in the free group FA. Indeed, we say a word w2 in WA is obtained from
w1 ∈WA by a simple R-reduction (or R-expansion) if w2 is obtained from w1 by
deleting (or inserting) a sub-word r, where r ∈R. We then say two words w and w′
are R-equivalent, and write w ∼R w′, if there exist a ﬁnite sequence of simple R-
reductions, simple R-expansions, elementary reductions, and elementary expansions
leading from w to w′. As before, concatenation induces an operation on the set of
equivalence classes of R-equivalent words of WA, and we have the structure of a
group G with presentation ⟨A | R⟩. We again equate the group with its presentation
and write G = ⟨A | R⟩.

4
T.A. Schroeder
Now, for an element w
∈
WA, we think of w as representing its entire
equivalence class of R-equivalent words and drop the equivalence class notation
(or the associated coset), understanding that other words may be equivalent to w.
That is, we think of the words themselves as elements of the group; but in that set,
there is much redundancy. In particular, we write w = w′ as group elements when
w ∼R w′ as words (or when w and w′ are in the same coset of N(R)). Multiplication
is still represented by concatenation, and the identity element still represented by 1.
This means that the generating set A is considered a subset of the group itself. Also,
R-equivalence means that we are able to insert or delete each r ∈R into any part of a
word w without changing the group element. This amounts to equating each relator
r with the identity element 1, a perspective often indicated in the presentation as
each r ∈R will often be equated to 1 in the right hand side of the presentation.
Finally, note that if A = {a1, a2, . . . , an} and R = {r1, r2, . . . , rk}, then we may
write ⟨a1, a2, . . . , an | r1, r2, . . . , rk⟩to denote ⟨A | R⟩.
2.2
Some Basic Graph Theory
In order to fully explore group presentations, and the structures of the resulting
groups, it does us well to review (or introduce) some basic graph theory.
Let V be a set, and let E be a collection of two element subsets of V, where an
individual set can be repeated in the collection, and an individual element can be
repeated to form a two-element subset. Such sets deﬁne a graph Γ = (V, E) where
the elements of V are the vertices of the graph and the elements of E the edges. A
set {v, w} ∈E indicates an edge between vertices v and w, and {v, v} ∈E deﬁnes
an edge in Γ from v to itself, i.e. a loop in Γ. If a subset in E has multiplicity n, then
we include n edges between the associated vertices in Γ. Here, Γ is said to have
multi-edges.
We deﬁne a directed graph by taking E ⊆V × V where the ordered pair (v, w)
indicates a directed edge between the vertices v and w. We indicate this pictorially
by placing an arrow on the associated edge. Of course, this sort of structure can
indicate an orientation to the edges in the graph, where traversing the associated
edge in different directions has different implications. For our purposes, it is possible
to have a graph with both directed edges and undirected edges. In this case, the
edge set E will denote undirected edges by two element sets, and directed edges by
ordered pairs.
A graph or directed graph Γ = (V, E) is said to be labeled, or weighted, if there
is a function from the set of edges E to a set of labels.
A graph Γ is said to be ﬁnite if both V and E are ﬁnite sets. Γ is said to be simple
if Γ includes no loops nor multi-edges.
Two graphs Γ1 −(V1, E1) and Γ2 = (V2, E2) are isomorphic if there exists
a bijection f : V1 →V2 such that if {u, v} ∈E1, then {f(u), f(v)} ∈E2. A
similar deﬁnition exists for directed edges, that is if (u, v) ∈E1, then (f(u), f(v)) ∈
E2. If the bijection f maps V1 to itself, and E1 = E2, then the map f deﬁnes an
automorphism of the graph Γ1(V1, E1).

Coxeter Groups and the Davis Complex
5
Example 1. Let V = {a, b, c, d, e, f}, and consider edge sets
E1 = {{a, b}, {b, c}, {c, d}, {d, e}, {e, f}, {f, g}} , and
E2 = {(a, b), (b, c), (c, a), (d, e), (e, f), (f, d), {a, d}, {c, e}, {b, f}} .
The corresponding graphs Γ1 = (V, E1) and Γ2 = (V, E2) are shown Figure 1.
Example 2. Let V = Z, and E = {{n, n + 1} | n ∈Z}. Then Γ = (V, E) can be
understood as the real line, with vertices at every integer.
2.3
Cayley Graphs for Finitely Presented Groups
We now present the contruction of the Cayley graph associated to a ﬁnitely
presented group. The Cayley graph is a very useful graph, endowed with some of the
additional structure discussed above. Let G = ⟨A | R⟩, we create the corresponding
labeled Cayley graph Γ as follows:
•
V = G; that is, we include one vertex for every element of G.
•
E: Given any v ∈G and a ∈A, then there is a directed edge (v, va) from v to the
element va ∈G. We label this edge by the generator a.
This means that in Γ, each vertex has one edge emanating from it for each element
of A, and it will also have one edge entering for each element of A. It should be
noted that we view the vertices of the graph as elements of the group, that is, we
view G ⊆Γ. So, from a given vertex v, traversing an edge labeled a with the
orientation corresponds to multiplying v on the right by a, and traversing an edge
labeled a against the orientation corresponds to multiplying v on the right by a−1.
Example 3. Let G =

a | a6
. Then the Cayley graph of G can be viewed as the
graph Γ1 in Figure 1, but replace the edges there with directed edges all oriented to
ﬂow counter-clockwise around the hexagon, and with a = a, b = a2, c = a3, . . . etc.
Fig. 1 Γ1 and Γ2.
b
a
f
e
d
c
b
c
a
f
e
d

6
T.A. Schroeder
Fig. 2 The directed Cayley
graph for G in Example 4.
c2
c
1
c
c
c
cb
c2b
b
c
c
c
b
b
b
b
b
b
Example 4. Let G =

b, c | b2, c3, bcb−1c

. G has Cayley graph shown in Figure 2.
In Example 4, due to the generator b having order 2 (we have b2 = 1 in G),
the description of the Cayley graph given above prescribes multi-edges. For at any
g ∈G, since gb2 = g, there is an edge labeled b emanating from g to gb, and an edge
entering g from gb. To avoid these multi-edges, and to reﬂect the fact that b−1 = b,
it is our convention that when a generator b has order 2 we will identify incoming
and outgoing edges corresponding to b; and instead of two directed edges, we will
include one undirected edge. Thus, the Cayley graph for the group in Example 4
will actually be viewed as the graph Γ2 in Figure 1, with directed and undirected
edges.
Finally, we remark that the vertices of a Cayley graph correspond to elements of
the group, hence equivalence classes of words. As a result, it can be very difﬁcult
to tell when two words represent the same element. So, constructing the Cayley
graph is not as straightforward as one might expect. However, the description above
enables us to understand a local picture of the Cayley graph at any vertex.
Exercise 1. Show that the relations ∼and ∼R on the set WA described above
generate equivalence relations.
Exercise 2. Show that concatenation is well-deﬁned on equivalence classes. Then,
show that FA is a group, with identity element 1. (For a given u ∈WA, what is
[u]−1?) In general, is FA abelian?
Exercise 3. Let A = {a, b}, R = {aba−1b−1}. Show that ab ∼R ba.
Exercise 4. Construct the Cayley diagram for the following presented groups.
(a) ⟨a |
⟩
(b) ⟨a, b |
⟩
(c)

a, b | aba−1b−1
(d)

a, b | aba−1b

(e)

a, b | b2

Coxeter Groups and the Davis Complex
7
(f)

a, b | a2b−2
(g)

a, b | a4, b2
(h)

a, b | a2b−3
(i)

a, b, c, d | aba−1b−1, cdc−1d−1
Challenge Problem 1. A very strong connection between group theory and topol-
ogy is now at hand. Indeed, given a group G = ⟨A | R⟩with Cayley graph Γ (with
single edges for generators of order 2), Γ can be given the “path-metric” by making
each edge isometric to the unit interval. Deﬁne then the distance d between two
points to be the minimum path length (or inﬁmum) over all paths connecting the
two points. Show that this deﬁnes a metric on Γ. Notice that for vertices which
correspond to u, v ∈G, d(u, v) ∈Z. Show that in this case, d(u, v) is the minimum
length of words w in the alphabet A ∪A−1 representing the element u−1v. The
restriction of the metric to G ⊆Γ is the so called “word-length metric.”
Challenge Problem 2. A free group has a more ‘universal’ deﬁnition: Let G be a
group and let A ⊆G. We say that G is free on A, denoted GA, if for every group H,
and any function f : A →H, then there exists a unique homomorphism h : G →H
s.t. h|A = f.
For a given set A, show that
(a) If G is free on A, then A generates G. (That is, every element of G is a product
of elements of A and their inverses.)
(b) If G is free on A, then A contains no elements of ﬁnite order.
(c) The group FA (deﬁned above) is free on A. (That is, for any group H and function
f : A →H, we must verify that f extends uniquely to a homomorphism.)
(d) Let GA be a group that is free on A, then FA ∼= GA. (That is, the deﬁnitions are
equivalent!)
3
Coxeter Groups
In this section, we’ll explore a special type of ﬁnitely presented group called a
Coxeter group. Put succinctly, Coxeter groups are groups that are generated by
elements of order 2, often viewed as reﬂections in some geometric space. As
you may recall from an undergraduate geometry course, or can ﬁnd in a standard
geometry text such as [15, Chapter 10], isometries of geometric spaces can be
understood as compositions of reﬂections. Therefore, one can see the importance of
groups generated by reﬂections, and their natural connection to geometry. It should
also be noted that the study of such ﬁnite groups, generated by reﬂections acting on
R2, are essential in classifying Lie groups and Lie algebras, and the classiﬁcation of
regular polytopes. Coxeter groups are generalizations of this idea, where the order 2
generators are not necessarily viewed as reﬂections on Rn, but will almost certainly
be viewed as some sort of homeomorphism of a topological space.

8
T.A. Schroeder
As we’ll see, the review of some basic graph theory in Section 2.2 will be useful,
since the presentations of Coxeter groups can be encoded as ﬁnite, simple, labeled
graphs; and because the topological spaces on which we will have these groups act
are intimately related to their Cayley graphs.
3.1
The Presentation of a Coxeter Group
Let Γ = (S, E) be a ﬁnite simple graph with vertex set S and with edges labeled by
integers ≥2. Denote by mst the label on the edge {s, t}. Γ encodes the data for a
presentation of a Coxeter group WΓ
WΓ =

S | s2 = 1 for each s ∈S and (st)mst = 1, for each edge {s, t} of Γ

.
(1)
The pair (WΓ , S) (or simply (W, S) when the graph Γ is clear) is called a Coxeter
system. We call such a labeled graph Γ a Coxeter graph. Throughout this chapter,
we will take such a graph as the deﬁning data for our Coxeter groups, noting that
the vertices of the graph correspond to the generators of the group. This is standard
convention: To simultaneously view each s ∈S as a generator of the group, an
element of the group, and a vertex of the deﬁning Coxeter graph. See [4, 9] for
further reference on Coxeter groups and Coxeter systems. See [14] for further
treatment on the deﬁning Coxeter graphs.
Observe that the Coxeter graph Γ is not the Cayley graph associated to the group.
Rather, it is an efﬁcient way to encode the presentation of the group. There are other
ways of deﬁning Coxeter groups; for example Coxeter matrices or Dynkin diagrams.
But, any such effort is just encoding the above type of presentation in another way.
Our focus will be on the so-called Coxeter graphs.
In summary, a Coxeter group is generated by a set of elements that have order
2, and the only other relators are of the form (st)mst, where s ̸= t, where mst is the
order of the element (st). Also, since the generators each have order two, they are
their own inverses and we refer to them as reﬂections or involutions. That is to say,
a Coxeter group is a group that is generated by reﬂections.
As noted in Section 2.1, the relators of the type r2 and (st)mst amount to
equating these words to the identity element of the corresponding Coxeter group.
See Exercises 5, 6, 7, and 8 to explore the implications of these relators, and the
ensuing structure of the Coxeter group. Further, the reader should note that if two
vertices s and t are not connected by an edge in Γ, then they do not deﬁne a relator
and themselves generate an inﬁnite subgroup, as Example 5 illustrates.
Example 5. Let Γ be a graph with two vertices, and no edges. Then
W =

r, s | r2, s2
,
and its elements can be algorithmically listed: 1, r, s, rs, sr, rsr, srs, rsrs, srsr, . . .. Is
it clear why the generators alternate within each word in this list? This group is
called the inﬁnite dihedral group, and denoted D∞. Is it clear why the group is
inﬁnite?

Coxeter Groups and the Davis Complex
9
Example 6. Let D4 denote the dihedral group of order 8, that is, D4 is the group of
isometries of a square, and let W =

r, s | r2, s2, (rs)4
. We will show W ∼= D4. To
do this, recall the deﬁnition of a group deﬁned by a presentation in Deﬁnition 2,
and let F{r,s} denote the free group on generators r and s, and N the normal
subgroup generated by the relators {r2, s2, (rs)2}. That is, N is the smallest normal
subgroup of F{r,s} containing the relators. Deﬁne a map f : F{r,s} →D4 by
mapping r to a reﬂection across a diagonal of a square, s to a reﬂection across
an adjacent side bisector, and extending f to the rest of F{r,s} by requiring that f
be a homomorphism. That is, a word in the alphabet {r, s, r−1, s−1} is mapped to
the corresponding composition of the reﬂections on the square described above. In
particular, the student can verify that f is surjective and that all the relators are all
in ker f. From these observations, we can conclude two things: (1) Since ker f is
a normal subgroup of F{r,s} containing the relators, we must have that N ≤ker f;
and (2) By the universal property of quotient groups, [10, 5.6], f descends to a map
¯f : F{r,s}/N = W →D4. We have two commuting diagrams:
F{r,s}
D4
F{r,s}
D4
F{r,s}/N = W
F{r,s}/ker f
f
f
¯f
∼=
The diagram on the right being the classical ‘ﬁrst’ or ‘fundamental’ isomorphism
theorem. It gives us that

F{r,s} : ker f

= 8. Since N ≤ker f, and we have that

F{r,s} : N

=

F{r,s} : ker f

· [ker f : N], we know that
|W| =

F{r,s} : N

≤8.
But, by simply observing words in the alphabet {r, s, r−1, s−1} subject to the
relators, it is clear that any element of the group has as a representative a
word of length at most 4. Indeed, since (rs)4 = 1, we know rsrsrsrs = 1,
rsrsrsr = s, rsrsrs = sr, rsrsr = srs, rsrs = srsr, and so on. This means
that {1, r, s, rs, sr, rsr, srs, rsrs = srsr} exhausts the set of words that represent
the distinct elements of W, and so |W| ≤8. Thus, |W| = 8, which means that
[ker f : N] = 1, and there is actually only one diagram. In particular, we have that
¯f : W →D4 is an isomorphism.
Example 7. One can show, using an argument similar to that in Example 6, that the
Coxeter group W =

r, s | r2, s2, (rs)n
is isomorphic to Dn, the dihedral group of
order 2n.
Example 8. Let W =

r, s, t | r2, s2, t2, (rs)2, (st)2, (rt)2
. It is a Coxeter group and
in a similar manner to that above, one can show that W is isomorphic to Z2⊕Z2⊕Z2
(See Figure 3).

10
T.A. Schroeder
Fig. 3 Coxeter graphs for
Examples 7 and 8
respectively.
r
s
n
r
s
t
2
2
2
Example 9. Consider the group
W =

r, s, t, u, v | r2, s2, t2, u2, v2, (rs)2, (st)2, (tu)2, (uv)2, (rv)2
.
Verify that W has corresponding Coxeter graph Γ where Γ is a pentagon with each
edge labeled 2. Note that W is inﬁnite, as it contains copies of D∞as subgroups.
3.2
Coxeter Groups and Geometry
To set up a discussion of the relationship between geometry and Coxeter groups,
we recall the following facts, often covered in an undergraduate transformational
geometry course.
Fact 1 Let P be a (2-dimensional) geometric space meeting the axioms of so-called
“Neutral geometry.” Given a line l ⊂P, the reﬂection over the line l, denoted rl is an
isometry of the space. That is, the distance between two points p and q is the same
as the distance between their reﬂected images p′ and q′.
Fact 2 With P as above, if γ : P →P is an isometry, then γ can be understood as a
composition of 1, 2, or 3 reﬂections over a line.
Fact 3 The composition of reﬂections over lines whose (acute) angle between them
is α is a rotation through an angle of 2α.
(The student is encouraged to recall the construction of a reﬂection in neutral,
plane geometry and to verify that the composition of a reﬂection with itself is the
identity isometry on the geometric plane. In other words, a reﬂection deﬁned in this
way has order 2. For reference, see [15, Chapter 10].)
The ﬁrst two facts provide great motivation for the study of Coxeter groups
and their inherent connection to geometry. Indeed, they give us that the group of
isometries of a geometric plane is generated by elements of order 2. The third fact
gives context for the other relators present in the presentation of a Coxeter group.
In later chapters, and in the suggested project 2, we give a Coxeter group as our
given data, and from it try to determine an appropriate geometric model. Here and
in Challenge problem 5, we turn this around. Namely, we present some geometric
models and reﬂections, and ask the reader to determine the associated group.

Coxeter Groups and the Davis Complex
11
3.2.1
Euclidean Space and Reﬂections
View R2 as the set of 2-dimensional vectors over R equipped with the usual dot
product
⟨u, v⟩= u1v1 + u2v2
where u = (u1, u2) and v = (v1, v2) are vectors in R2. A line l in R2 through xo
in the direction of v has parametric equation x = x0 + vt, where t ∈R. Any such
line deﬁnes a reﬂection rl with formula
rl(x) = x −2 ⟨u, x −x0⟩u,
(2)
for any x ∈R2 and where u is a unit vector orthogonal to v.
3.2.2
Spherical Geometry and Reﬂections
Denote by S2 the subset of R3 of points (x1, x2, x3) for which x2
1 + x2
2 + x2
3 = 1 and
call it the “2-sphere.” With the usual dot product deﬁned above extended to R3, we
see that S2 can be viewed as the set of vectors x for which ⟨x, x⟩= 1.
A line l in S2 is deﬁned as the intersection of S2 with a plane through the origin
in R3. Any such line deﬁnes a reﬂection rl of S2 in a similar way to that above.
Indeed, for any x ∈S2, we have
rl(x) = x −2 ⟨u, x⟩u,
(3)
where u ∈R3 is unit vector orthogonal to the plane deﬁning the line l.
3.2.3
Hyperbolic Geometry and Reﬂections
Deﬁne a modiﬁed inner product on R3 by
⟨x, y⟩M = x1y1 + x2y2 −x3y3.
(The “M” stands for Minkowski, and the student should note that this formula does
not technically deﬁne an “inner product,” as it’s not the case that ⟨x, x⟩M ≥0 for
all vectors x.) Let H2 denote the subset of R3 for which ⟨x, x⟩M = −1 and x3 > 0.
This is just the upper sheet of the two-sheeted hyperboloid in R3 deﬁned by equation
x2 + y2 −z2 = −1, and called the hyperboloid model for hyperbolic space.
A line l in H2 is deﬁned as the intersection of H2 with a plane through the origin
in R3, and, as above, any such line deﬁnes a reﬂection in H2. In particular, let u ∈
R3 denote a unit vector orthogonal to plane deﬁning l, with respect to the modiﬁed
inner product ⟨
,
⟩M. (This means that ⟨u, v⟩M = 0 for any vector v in the plane
deﬁning l.) For any x ∈H2, the reﬂection rl is deﬁned by
rl(x) = x −2 ⟨u, x⟩M u.
(4)

12
T.A. Schroeder
3.2.4
The Poincaré Disk Model for Hyperbolic Space
There is another model we’ll consider for hyperbolic space called the “Poincaré disk
model,” denoted by H2
P. In terms of its points, H2
P consists of (x, y) ∈R2 for which
x2+y2 < 1, the interior of the unit disk in R2. However, distance in H2
P is calculated
in such a way that the distance from the origin to the boundary of the disk is inﬁnite.
As a result, lines come in two forms:
1. The portion of lines through the origin in R2 contained in the interior of the unit
disk, or
2. The portion of circles in R2 orthogonal to the boundary circle x2 + y2 = 1
contained in the interior of the unit disk.
Since lines come in two forms, the corresponding reﬂections come in two forms.
1. If l is a line of type (1) above, then rl is a restriction to the interior of the unit disk
of the standard Euclidean reﬂection deﬁned in Equation 2 with x0 = (0, 0).
2. If l is a line of type (2), then rl is the inversion in the circle orthogonal to the
boundary circle, applied to the points in the interior of the unit disk.
The student may recall that the inversion x′ of a point x in a circle of radius k with
center x0 is given by
x′ = x0 +
(x −x0)
⟨x −x0, x −x0⟩.
(5)
From the above formulas, one can easily see the how the hyperboloid model is
completely analogous to the Euclidean and spherical model. An advantage of the
Poincaré model over the hyperboloid model is that it is conformal. That is, angles
between lines in H2
P are equal to the angles between the corresponding lines or
circles in R2.
Exercise 5. Let r ∈A and suppose rr = r2 ∈R. Show that r ∼R r−1.
Exercise 6. Let r, s ∈A and suppose r2, s2, (rs)2 ∈R. Show that rs ∼R sr.
Exercise 7. Let r, s ∈A and suppose (rs)3 = rsrsrs, r2, s2 ∈R. Show that rsr ∼R
srs.
Exercise 8. Let r, s ∈A and suppose r2, s2, (rs)n ∈R, for some n ∈Z, n ≥2.
Then (sr)n = 1. (That is, if you have the relator (rs)n. You also have the relation
(sr)n.)
Exercise 9. Write the presentation of a Coxeter group for each of the three Coxeter
graphs shown in Figure 4.

Coxeter Groups and the Davis Complex
13
Fig. 4 The Coxeter graphs
for Exercise 9.
r
s
t
4
2
4
r
s
t
2
2
2
r
s
t
2
Exercise 10. Sketch the Coxeter graph that deﬁnes the following presentations.
(a)

r, s | r2, s2
(b)

r, s, t | r2, s2, t2, (rs)2, (st)3, (rt)6
(c)

r, s, t | r2, s2, t2, (rs)2, (st)4, (rt)4
(d)

r, s, t | r2, s2, t2, (rs)4, (st)4, (rt)4
(e)

r, s, t, u, v | r2, s2, t2, u2, v2, (rs)2, (st)2, (tu)4, (uv)2, (rv)4
Exercise 11. Cayley graphs for Coxeter groups: Recall the construction of
Cayley graphs described in Section 2.2. Note that the generators of a Coxeter group
always have order 2, so at each vertex of the Cayley graph of a Coxeter group,
the incoming and outgoing edges are identiﬁed to reﬂect the idea that for a given
generator r, r = r−1. Construct the Cayley graph for the groups in Examples 7,8,
and 9. Do the same for the groups presented in Exercises 9 and 10.
Exercise 12. By using an argument similar to that in Example 6, identify each of
the presented groups with a familiar group. Namely the student should attempt to
ﬁnd a map from an appropriate free group to a familiar group, then show that this
map descends to an isomorphism of the given presented group to the familiar group.
(a)

r, s | r2, s2, (rs)3
(b)

r, s | r2, s2, (rs)n
(See Example 7)
(c)

r, s, t | r2, s2, t2, (rs)2, (st)2, (rt)2
(See Example 8)
(e)

r, s, t | r2, s2, t2, (rs)3, (st)3, (rt)2
(Hint: This ﬁnite Coxeter group is quite
famous. Make an educated guess at its order and try to think of a group with
the same order.)

14
T.A. Schroeder
Challenge Problem 3. Given a Coxeter graph Γ = (V, E), show that an automor-
phism of Γ which preserves edge labels (that is, if m is the label on {r, s}, then m is
the label on the edge deﬁned by the images of r and s) induces an automorphism of
the corresponding Coxeter group.
Challenge Problem 4. Consider the formulas for reﬂections in Euclidean, spheri-
cal, and hyperbolic space given in equations 2, 3, 4, and 5. Let rl generically denote
one of these reﬂections. Show that
(a) rl is appropriately deﬁned for the non-Euclidean models. (Speciﬁcally, show that
it sends points on the sphere to points on the sphere, points on the hyperboloid
to points on the hyperboloid, and points in the interior of the unit disk, to points
in the interior of the unit disk.)
(b) For all the models, verify that rl ◦rl = the identity map on the space.
Challenge Problem 5. Consider again the 2-dimensional geometric models dis-
cussed above.
(a) Find a presentation for the group generated by a reﬂection over the x-axis, and
the line y =
√
3x in Euclidean space.
(b) Find a presentation for the group generated by reﬂections over the lines formed
by the planes z = 0, y = 0 and y = x in S2.
(c) Find a presentation for the group generated by reﬂections over the lines formed
by the planes y = 0 and y = x in H2.
(d) Find a presentation for the group generated by reﬂections over the x-axis, the
line y = x, and the hyperbolic line connecting the points

cos3 π
4
cos π
8
, 0

and

cos5 π
4
cos π
8
,

cos5 π
4
cos π
8

in H2
P.
For each of these, besides considering the angle between the given reﬂections as
a clue to appropriate group element orders, the student is also encouraged to use
a computer algebra system to calculate the order of the composition of reﬂections
directly.
4
Group Actions on Complexes
In Geometric Group Theory, the idea is to study the interplay between a ﬁnitely
presented group G, and a corresponding topological, or geometric, space X. In
particular, we view the elements of a group G as homeomorphisms of the space X.

Coxeter Groups and the Davis Complex
15
That is, each element of the group corresponds to a continuous, bijective function
X →X. For example, if G = D3, the dihedral group of order 6, then each element of
the group can be viewed as a function that maps a regular 3-gon X to itself. Perhaps,
the element reﬂects the triangle over an altitude, or rotates the triangle by 120◦. In
any case, each element of D3 is viewed as a function of the triangle to itself.
The language we use to encompass this sort of relationship between a group
G and a topological space X is to say that the group G acts on X. Group actions
are one of the richest areas of mathematics, for if the action is appropriate, many
of the properties of the group (orders of elements, subgroups, cosets, etc.) manifest
themselves in the topological space; and properties of the topological space (e.g. any
geometric structure) manifest themselves in the group. In this chapter, the approach
we take is to focus on ﬁnitely presented groups, mostly Coxeter groups, acting
on a special type of topological space called a CW-complex. For an overview of
Geometric Group Theory, see [2]. For more references on topological spaces, see
[12], and for references on CW-complexes, see [8] or [7].
4.1
CW-Complexes
CW-complexes are topological spaces whose construction can be done in a step-by-
step manner, using very simple topological spaces as building blocks. The blocks are
referred to as n-balls, and the steps in the process correspond to dimension. The idea
is to build a space out of points, then edges, then disks/squares/triangles/5-gons/etc.,
then tetrahedron/cubes/prisms/etc.,. . . and so on. Let us get a bit more speciﬁc.
Let Dn denote a topological n-ball, with boundary ∂Dn = Sn−1. In particular,
•
D0 is just a point, S−1 is the empty set.
•
D1 is just a line segment, S0 is the two endpoints of the edge.
•
D2 is a disk, S1 is the circle bounding the disk.
•
D3 is a (ﬁlled) ball, S2 is the surface of the ball.
...
In general, Dn can be viewed as the set {(x1, x2, x3, . . . , xn) ∈Rn | x2
1 + x2
2 + x2
3 +
. . .+x2
n ≤1}, with boundary Sn−1 viewed as the set of points {(x1, x2, x3, . . . , xn) ∈
Rn | x2
1+x2
2+x2
3+. . .+x2
n = 1}. It is important to note, however, that these balls and
their boundaries are thought to be completely independent of any sort of Cartesian
space. They are considered to be their own spaces. This isn’t difﬁcult to picture in
low dimensions, for it is easy to think of collections of points, segments, disks, and
balls as independent of any sort of coordinate axes. Though harder to picture (or
even impossible?), the same applies in higher dimensions. But for the sake of this
chapter, picturing things in dimensions ≤3 will sufﬁce.
We should also note that the n-balls used as building blocks do not have to
be round. For example, D2 could be a traditional disk, or it could be a square,
or a pentagon, or a very strange non-convex shape. See Figure 5. These are all

16
T.A. Schroeder
Fig. 5 Homeomorphic examples of a 2-ball.
topological 2-balls because any one of them can be stretched, bent, squashed,
etc. . . , but not torn; in a such a way as to match any of the others. This is the
idea of a homeomorphism. The study of homeomorphisms in topology is as central
as the study of isomorphisms in group theory, or even as central as the study
of differentiable functions in Calculus I. For a nice introductory reference on the
centrality of homeomorphisms to the study of topology, see [11].
The idea of a CW-complex, then, is to take n-balls of various dimensions, and
use them as the building blocks to construct a topological space. It’s almost as if
we are playing LEGOs R⃝, and the n-balls are our pieces. Within the context of the
larger space, we refer to the individual n-balls as n-cells. Now, just as a LEGO R⃝
construction can be given in steps, the construction of a CW-complex X, can be
described in steps, corresponding to dimension:
0. Start with a (discrete) set of 0-cells. Denote this set X0, called the 0-skeleton.
1. To X0, attach a set of 1-cells along their boundaries; forming X1 called the 1-
skeleton. (At this stage, your complex looks like a graph.)
2. To X1, attach a set of 2-cells along their boundaries; forming X2 called the 2-
skeleton.
3. To X2, attach a set of 3-cells along their boundaries; forming X3 called the 3-
skeleton.
...
The “attaching” described above should be described a bit more formally.
Inductively, for n ≥1, we form the n-skeleton Xn from the (n −1)-skeleton Xn−1
by attaching n-balls via functions f : ∂Dn = Sn−1 →Xn−1 (called attaching
maps), one function for each of the attached n-balls mapping its boundary to the
lower dimensional skeleton. If {Dn} is the collection of n-balls attached at step
n, then the n-skeleton Xn is understood as the disjoint union Xn−1 ∪{Dn} under
some identiﬁcations. In particular, for each attached n-ball Dn, each point x in the
boundary is identiﬁed with its image under f. In other words, new cells are “glued”
along their boundaries to the existing space. More formally, we have
Xn :=
	
Xn−1 ∪{Dn}

/ ∼,
where x ∼f(x) for each attached n-ball Dn, each x in ∂Dn, and corresponding
attaching map f. Precisely, Xn is deﬁned as a quotient space. Set X = ∪nXn, the

Coxeter Groups and the Davis Complex
17
union of all n-skeleta. It is a CW-complex. The “CW” stands for “Closure-Weak” in
reference to the topology of such a space. We will not get speciﬁc with the topology
of these spaces, only noting that each cell carries its own topology homeomorphic
to the unit ball in Rn.
If the process described above stops at some dimension n (that is, if there are no
m-balls attached for m > n), than we say X is ﬁnite dimensional. In particular, if the
process stops after X1, that is X = X1 and there are no m-balls for m > 1, then X is
a graph with vertices the 0-cells, and edges the 1-cells of X. (It may help to think of
CW-complexes as graphs generalized to higher dimensions.)
If X contains ﬁnitely many cells, we say X is ﬁnite. Note, however, that in any
given step, there may be inﬁnitely many cells, or there could be no cells to attach.
While we will look at some constructions in Examples 10 and 11, we often are given
the total space X, and understand the above process to have taken place, and refer to
the resulting cellulation of X.
Example 10. We can view a 2-sphere S2 as a CW-complex in many ways. One way
is with one 0-cell, no 1-cells, and one 2-cell attached to the 0-cell by identifying
all of its boundary to the 0-cell – like pulling a draw string on a bag to tighten the
opening. The attaching map is indicated with a solid arrow in Figure 6. Here we
see that since there are no 1-cells, X0 = X1. In general, it can be the case in a
CW-complex that the i-skeleton can equal the i + 1-skeleton.
Example 11. Figure 7 depicts a cellulation of a torus by one 0-cell, two 1-cells, and
one 2-cell. The attaching maps are indicated with solid arrows in the ﬁgure, resulting
skeleta indicated with dashed arrows. Two 1-cells are attached to the indicated
0-cell. One 2-cell, viewed as a rectangle, is attached to the 1-skeleton where the
corners are all identiﬁed with the 0-cell, and the edges are identiﬁed to the 1-cells
with orientations as shown. (Though it is not shaded, the rectangle shown represents
a 2-cell.)
Fig. 6 A cellulation of a
2-sphere.
X0
X2

18
T.A. Schroeder
X0
2-cell
X1
X = X2
Fig. 7 The cellular decomposition of the torus.
The Euler Characteristic To a ﬁnite CW-complex X we can attach a number
called the Euler characteristic of X, denoted χ(X), where
χ(X) =

cells σ
(−1)dim σ.
(6)
That is, χ(X) is the alternating sum of the number of cells in each dimension. It is
an interesting result of Algebraic Topology that the Euler characteristic of a space
X does not depend on the speciﬁc cellulation one uses, rather only on the homotopy
class of the space. In particular, homeomorphic spaces (regardless of cellulation)
will have the same Euler characteristic. See [8] for reference. For example, using
the cellulation of the sphere in Example 10, we get χ(S2) = 2. But it is the case that
no matter the cellulation of S2, we still get χ(S2) = 2. In fact, it is a fundamental
result of algebraic topology that the Euler characteristic classiﬁes surfaces, both
orientable and non-orientable. The interested student should refer to Project Idea 3
that further investigates the Euler characteristic.
4.2
Group Actions on CW-Complexes
As discussed in the introductory comments of Section 4, an important aspect of
group theory (and in fact all of mathematics) is the study of group actions. In
an undergraduate abstract algebra course, you may have studied group actions on
generic sets, or in the context of the Sylow Theorems. The automorphism groups of
graphs are commonly studied in this environment, as group elements can be viewed
as bijections of the vertex set or edge set of an associated graph. Some students may
have even studied how a group can “act” on its own Cayley graph. But as we noted
above, graphs are just 1-dimensional CW-complexes, and hence topological spaces.

Coxeter Groups and the Davis Complex
19
So a group “acting” on a graph is really an example of the more general sort of
group action we study here. That is, a group acting on a topological space.
Deﬁnition 3. An action of a group G on a topological space X is a homomorphism
φ : G →Homeo(X). Where Homeo(X) is the set of homeomorphisms X →X,
under composition.
Once again, this is very similar to the deﬁnition of group action you may see
in an abstract algebra text like [6], however, instead of each element of the group
corresponding to simply a bijection of a set X, we carry a topological requirement
that each element g ∈G corresponds to a homeomorphism φg : X →X. For g ∈G,
we write g · x for φg(x). That is, we think of g itself as sending x ∈X to some other
point of the space. Similarly, for a subset Y ⊆X, g·Y stands for “the points to which
g sends all of the points of the set Y.”
Since the topological spaces we consider in this chapter are CW-complexes, we
require our actions be cellular. That is, we consider the action on the level of cells
and require that, for g ∈G, and for any n-cell σ in X, g · σ is another n-cell of X. So
a cellular action is one that sends n-cells to n-cells, and all adjacency relationships
are maintained. In other words, if two cells are adjacent before being acted upon,
then they are adjacent after being acted upon. In this context, the familiar deﬁnitions
of orbit and stabilizer take on this cellular theme.
Deﬁnition 4. For an n-cell σ, the stabilizer of σ is
StabG(σ) = {g ∈G | g · σ = σ}
and the G-orbit of σ is the set of n-cells γ for which τ = g · σ for some g ∈G.
In the examples we consider, the actions are also be proper and co-compact.
Deﬁnition 5. An action of a group G on a complex X is proper if |StabG(σ)| < ∞
for each cell σ of X.
The next term we would like to deﬁne, “co-compact”, requires some set-up. For
a cellular action of a group G on a complex X, we deﬁne the quotient space X/G
to be a CW-complex where each cell σ of X is identiﬁed with its orbit. There are
some topological issues that we are bypassing, but for reference, the space X/G is
the endowed with the quotient topology. (See [12] or [8].) Put simply, X/G is a
CW-complex comprised of an n-cell representing each G-orbit of n-cells (for each
n), and containment relationships from the parent space X are maintained: If two n-
cells are in the same orbit, then they will identify as a single cell in X/G. If an n-cell
σ contains an m-cell τ in X, then they will have corresponding n- and m-cells in the
same containment relationship in X/G. We are now ready to deﬁne a co-compact
action.
Deﬁnition 6. An action of a group G on a complex X is co-compact if the quotient
space X/G is a ﬁnite complex.

20
T.A. Schroeder
Fig. 8 Z acts via
translations, R/Z is shown.
···
···
Z acts ↔
R/Z ∼= S1
Before exploring some examples, we have one last, closely related, term to deﬁne
for the action of a group G on a complex X.
Deﬁnition 7. Let G act on the CW-complex X. A closed subset C of X is a
fundamental domain for the action if G · C, the union of all orbits of cells in C,
contains X. A fundamental domain C of X is a strict fundamental domain if the
G-orbit of each cell intersects C in exactly one cell.
Example 12. The real line R can be thought of as a CW-complex, where each
integer point on the line corresponds to a 0-cell, and each resulting interval
corresponds to a 1-cell attached at two endpoints. The group (Z, +) acts on R by
translation. For example, if we take 5 ∈Z, and x ∈R, 5·x = 5+x. That is, the action
of 5 on R slides every point on the line 5 units right. −3 ∈Z slides each point 3
units left. This action is cellular, with (non-strict) fundamental domain any interval;
it is proper, the only cell-stabilizer is the identity 0 ∈Z; and it is co-compact. There
is one orbit of 0-cells and one orbit of 1-cells. Thus, the space R/Z should consist
of exactly two cells: one 0-cell, and one 1-cell. And, since each 1-cell is connected
to 0-cells on both ends, in the quotient space, the one 1-cell must be connected to
the one 0-cell at both ends. Thus, R/Z is a circle as shown in Figure 8.
The idea of a strict fundamental domain of an action is that it is a subset of
the space whose orbit covers the whole space, but does so efﬁciently. For example,
the interval [0, 1] is a fundamental domain for the action of Z on R described in
example 12, but it is not strict, since the endpoints of the interval are in the same
orbit. A similar situation occurs in the next example.
Example 13. The real plane R × R can be thought of as a CW-complex, where
each integer grid point corresponds to a 0-cell, horizontal and vertical segments
connecting the grid points correspond to 1-cells, and 2-cells correspond to the
resulting squares. Z × Z acts on R × R by horizontal and vertical translation. For
example, for (3, −2) ∈Z×Z and (x, y) ∈R×R, (3, −2)·(x, y) = (3+x, −2+y).
This action is cellular, proper, and co-compact. To see the quotient space, realize
that the square of the form [0, 1] × [0, 1] can be translated to cover the whole plane.

Coxeter Groups and the Davis Complex
21
Fig. 9 R/(Z × Z) ∼
= a torus.
···
···
r
s
Fig. 10 D∞acting on R by reﬂections. R/D∞is shown.
That is, this square is a fundamental domain of the action, but it is not strict. There is
one orbit of 2-cells, and thus one 2-cell in the quotient space. Within this one square,
the top and bottom edges are identiﬁed, as they are within the same orbit, and the
left and right edges are identiﬁed, as they are within the same orbit. However, the
horizontal edges are not translated to the vertical edges under this action. So there
are two orbits of 1-cells and thus two 1-cells in the quotient space. Finally, the four
corners are identiﬁed as a single point, as there is one orbit of 0-cells, and so the
quotient space has one 0-cell. (R × R)/(Z × Z) is the torus, the identiﬁcations
shown in Figure 9. Note that this is a slightly different perspective on the cellulation
of the torus described in Example 11.
Example 14. The inﬁnite dihedral group D∞=

r, s | r2, s2
acts on R where we
take r to correspond to a reﬂection about 0 ∈R, and s to be a reﬂection about
1 ∈R. See Figure 10. This action is cellular, proper (the stabilizers of the 0-cells
have order 2), and co-compact. There are two orbits of 0-cells, one orbit of 1-cells.
The quotient space then is a closed interval. Note that it can be identiﬁed with a
strict fundamental domain of the action.
Orbihedral Euler Characteristic If a group G acts properly and co-compactly on
a complex X, then the orbihedral Euler characteristic of X/G is the rational number

22
T.A. Schroeder
χorb(X/G) =

σ
(−1)dim σ
| StabG(σ)|,
(7)
where the sum is over the cells of X/G. (See [4] or [14] for reference on the
orbihedral Euler characteristic.) Note that (1) The orbihedral Euler characteristic
is the usual Euler characteristic in the case all cell stabilizers are trivial and (2) The
orbihedral Euler characteristic is multiplicative. That is, if H ≤G of index m, then
χorb(X/H) = mχorb(X/G).
(8)
(See Exercise 15 below.)
In Example 14 with R/D∞, we have a 1-cell stabilized by the trivial subgroup,
and two 0-cells stabilized by order two subgroups. So
χorb(R/D∞) = 1/2 + 1/2



0−cells
−
1

1−cell
= 0.
Exercise 13. Calculate χ(X) in the case X is
(a) An empty tetrahedron.
(b) An empty octahedron.
(c) An empty cube.
(d) The torus described in Example 11.
The cube, octahedron, and tetrahedron are “regular” cellulations of S2. Can you
think of any others? Calculate the corresponding Euler characteristic. Sketch an
non-regular cellulation of S2 and calculate χ.
Exercise 14. Calculate χorb(X/G) for each of the quotient spaces in Exam-
ples 12, 13, and 14.
Exercise 15. Prove Equation 8 above: If H ≤G of index m, then χorb(X/H) =
mχorb(X/G).
Exercise 16. Using the idea of Example 13, describe an action of D∞× D∞on
R × R. Sketch the quotient space R × R/(D∞× D∞) and calculate
χorb ((R × R)/(D∞× D∞)) .
Exercise 17. A group G = ⟨A | R⟩acts on its Cayley graph Γ in the following way:
For g ∈G, v a vertex (which is also an element of the group), we have g · v = gv.
This is a generalization of the (left) action of a group on itself. Verify that this deﬁnes

Coxeter Groups and the Davis Complex
23
a cellular action on the Cayley graph. (That is, edges are sent to edges). Consider
the following examples.
(a) Describe the action of Fa,b = ⟨a, b |
⟩on its Cayley graph Γ. Sketch Γ/Fa,b.
(b) Describe the action of G =

a, b | aba−1b−1
on its Cayley graph Γ. Sketch
Γ/G. (Compare with Example 13).
(c) Describe the action of W =

r, s | r2, s2, (rs)3
on its Cayley graph Γ. Sketch
Γ/W.
(d) Describe the action of W =

r, s, t | r2, s2, t2, (rs)2, (st)2, (rt)2
on its Cayley
graph Γ. Sketch Γ/W.
(e) Describe the action of
W =

r, s, t, u, v | r2, s2, t2, u2, v2, (rs)2, (st)2, (tu)2, (uv)2, (rv)2
on its Cayley graph Γ. Sketch Γ/W.
Exercise 18. Look up a cellular decomposition of a two-holed torus X (sometimes
called a genus 2 surface). It can be understood as an “identiﬁcation space” similar to
the torus in Example 13, though beginning with an octagon rather than a rectangle.
Calculate χ(X). Do the same with a “Klein bottle” and any “genus g-surface.”
Challenge Problem 6. Consider the element rs ∈D∞and the subgroup generated
by it, ⟨rs⟩≤D∞.
(a) What is the order of ⟨rs⟩?
(b) What is the index of ⟨rs⟩in D∞.
(c) Under the action of D∞on R described in example 10, describe the action of
the element rs on R. That is, for x ∈R, what is (rs) · x? What is (rs)−1 · x?
Deduce the action of (rs)n on R.
(d) Sketch the quotient space R/ ⟨rs⟩and verify equation 8.
The group ⟨rs⟩is called a ﬁnite index torsion free subgroup of D∞. This means that
it contains no elements of ﬁnite order, besides the identity. Project idea 3 asks the
student to consider such subgroups and similar questions in the context of different
Coxeter groups.
Challenge Problem 7. Repeat Problem 6 for D∞× D∞acting on R × R. That is,
ﬁnd a ﬁnite index subgroup in D∞× D∞acting on R × R, and answer the included
questions.

24
T.A. Schroeder
5
The Cellular Actions of Coxeter Groups: The Davis Complex
In several papers (e.g., [3], [4], and [5]), M. Davis describes a construction which
associates to any Coxeter system (W, S), a complex Σ(W, S), or simply Σ when
the Coxeter system is clear, on which W acts properly and co-compactly. This is the
Davis complex. We describe the construction here.
5.1
Spherical Subsets and the Strict Fundamental Domain
Let (W, S) be a Coxeter system with deﬁning graph Γ. For a subset of generators
U, denote by WU the subgroup of W generated by the elements of U. Of interest
are subsets of generators (vertices of the graph) that generate ﬁnite groups. We call
these spherical subsets. These spherical subsets will be the key to deﬁning an action
of the corresponding Coxeter group on a complex.
5.1.1
Spherical Subsets
Finite Coxeter groups are completely classiﬁed, codiﬁed by the so-called “Dynkin
diagrams;” and in general, one can detect if a given subset of generators of a Coxeter
group deﬁnes a ﬁnite subgroup. But for us to work through our low-dimensional
examples (dimension ≤3), we need only detect spherical subsets with three or
fewer elements, and we can do that in a way that doesn’t directly rely on knowing
Dynkin diagrams.
Let (W, S) be a Coxeter system with corresponding Coxeter graph Γ. First note
that every vertex of Γ corresponds to an order 2 generator, so every vertex deﬁnes a
spherical subset of order 1. Next, recall that any two vertices connected by an edge
generate a ﬁnite group, so all edges deﬁne a spherical subset of order 2. Furthermore,
these are the only spherical subsets of order 2, since any two vertices not connected
by an edge generate D∞. Also, from this we deduce that the vertices of any
spherical subset must be pairwise connected. Finally, we present the following fact
for spherical subsets of order 3: For pairwise connected vertices r, s, and t of Γ with
edge labels mrs, mst, and mrt, the subgroup {r, s, t} is ﬁnite if and only if
1
mrs
+ 1
mst
+ 1
mrt
> 1.
The reader is invited to check this fact against the Coxeter group examples and
exercises worked at the end of Section 3, and investigate the geometric implications
of such an inequality.
5.1.2
The Strict Fundamental Domain
With (W, S) a Coxeter system with Coxeter graph Γ, we deﬁne a ﬁnite complex K
that will be the strict fundamental domain of the action of W on the Davis complex.
The perspective we take is in some ways the inverse of the process laid out in
Section 4.2, where given a group acting on a space X, we calculated the quotient
space X/G. Here, we will construct the strict fundamental domain ﬁrst and use it

Coxeter Groups and the Davis Complex
25
to construct a space on which the Coxeter group acts. We do this by ‘reﬂecting’
K around in a tiling pattern that respects the structure of the group. Consider
Example 14. There we have a Coxeter group acting on the real line, with (strict)
fundamental domain an interval. Note that we can turn the perspective around,
starting with the interval between the reﬂection points and reﬂect it repeatedly over
its endpoints and develop (or cover) the full real line. The real line in this example,
with the cellulation and indicated action, is the Davis complex corresponding to the
Coxeter group D∞. (A fact the reader should conﬁrm after the construction process
is spelled out below.) In this case, as will be the case with all of the Davis complex
examples, that the strict fundamental domain corresponds to the quotient space of
the action.
The strict fundamental domain K is a complex with the following cell struc-
ture:
•
0-cells: Each spherical subset, including ∅, corresponds to a 0-cell.
•
1-cells: If U and V are spherical subsets with U ⊆V, then this inclusion
relationship corresponds to a 1-cell connecting vertices U and V.
•
2-cells: If U, V, and W are spherical subsets with U ⊆V ⊆W, then this inclusion
relationship corresponds to a 2-cell attached to the edges U ⊆V, V ⊆W, and
U ⊆W. (It is a triangle with edges corresponding to U ⊆V, V ⊆W, and
U ⊆W.)
•
3-cells: If U, V, W, Y are spherical subsets with U ⊆V ⊆W ⊆Y, then this
relationship corresponds to a 3-cell connected appropriately. (It is a tetrahedron
with triangle faces corresponding to the 4 triangles determined by the previous
step.)
...
In general, this process terminates and one forms the ﬁnite complex K. This process
actually forms K into what is called a simplicial complex, which is a special type of
CW-complex. We will also say that K is the geometric realization of the partially
ordered set of spherical subsets. Figure 11 shows K for the group in Example 9.
Fig. 11 K for the group
deﬁned in Example 9.
{r,v}
{r,s}
{s,t}
{t,u}
{u,v}
{r}
{s}
{t}
{u}
{v}
/0

26
T.A. Schroeder
5.2
The Davis Complex
We are now ready for the construction of the Davis complex for a Coxeter system
(W, S), with strict fundamental domain K.
For each generator s ∈S, the set of cells of K for which each corresponding
spherical subset contains {s} is called the s-mirror. (In Figure 11, these ‘mirrors’
correspond to ﬁve ‘faces’ of K.) We form the Davis complex Σ by taking a copy
of K for each element of the group W and “gluing” them together according to
mirrors. This corresponds to “reﬂecting” K in the associated mirror. More formally,
we deﬁne an equivalence relation ∼on the set W × K (a copy of K for each element
of the group) by (w, x) ∼(v, y) if and only if for some generator s, x = y in the
s-mirror of K and v = ws. Then deﬁne the Davis complex Σ as the quotient space
Σ = (W × K)/ ∼.
(9)
See [4] for reference. The (left) W-action on the set W × K deﬁned by w · (v, k) =
(wv, k) respects the equivalence relation and passes to a cellular action on Σ with
strict fundamental domain K. (See Exercise 20.)
We write wK for the union of all the equivalence classes of elements in w × K.
This notation is indicative of the group action, as wK stands for the w-translate of
the copy of K corresponding to the identity element of the Coxeter group. Thus, for
any generator s, in the equivalence relation above wK is ‘glued’ to wsK along the
s-mirror.
Example 15. Let Γ be a segment labeled with 3. Then
W =

r, s | r2, s2, (rs)3
and the Davis Complex is the hexagon shown in Figure 12. In this case, the Davis
complex is 2-dimensional, so all of the hexagon is “ﬁlled in,” but for emphasis,
we only shade the strict fundamental domain K. In the ﬁgure, the center vertex
corresponds to the spherical subset {r, s} and is thus in both the r- and s-mirrors.
So all copies of K are glued at this point. Finally, note that since rsr = srs in W,
rsrK = srsK in Σ. To better understand the term “mirror,” observe that Σ can be
developed by reﬂecting K repeatedly in the lines formed by extending the r- and
s-mirrors. Indeed, one should trace the orbit of the vertex denoted ∅under these
reﬂections.
5.3
The Mirror Cellulation of Σ
If a deﬁning Coxeter graph Γ is an m-gon, then we have simpler cellulation of K
that makes more clear the term “mirror.” So let Γ be an m-gon and take as the set
of 0-cells the spherical subsets of the form {r, s}, that is, pairs of adjacent vertices
in Γ. For each generator r, the spherical subset {r} is included in exactly two other

Coxeter Groups and the Davis Complex
27
spherical subsets, namely {r, s} and {r, t} where s and t are the adjacent to r in Γ.
We then declare the two edges {r} ⊆{r, s} and {r} ⊆{r, t} forming the r-mirror
as one 1-cell with endpoints attached to {r, s} and {r, t}. Take these as the 1-cells
of this new cellulation of K. Finally, use one 2-cell to represent the interior. This
is a coarser, but simpler cellulation of K than that described above. The full Davis
complex is formed as above, with the same identiﬁcations along mirrors. The group
action on the ﬁner cellulation described above descends to a cellular action here.
Example 16. Let W be the Coxeter group deﬁned in Example 9. We simplify the
cell structure of the strict fundamental domain shown in Figure 11 by taking ﬁve 0-
cells, ﬁve 1-cells, and one 2-cell. Now, using this cellulation, we think of developing
the space Σ by reﬂecting K successively in each of its mirrors, respecting the group
relations. Namely reﬂecting K by r and then s is equivalent to reﬂecting by s and then
r since (rs)2 = 1 implies rs = sr in W. This yields a copy of K for each element
of the group, as before, with the reﬂection actions very clearly deﬁned along the
faces of K. See Figure 13 for an incomplete image of Σ, drawn suggestively in the
Poincaré disk model of hyperbolic space. Note that reﬂections in the ﬁve bold lines
correspond to the ﬁve generators of the group, and the action of W on Σ is achieved
by reﬂecting over these lines. For reference on this simpler cellulation of Σ, see
[13]. For an investigation as to why this image is depicted in hyperbolic space, see
Project 2.
5.4
The Coxeter Cellulation
We ﬁnally present a cellulation of the Davis Complex that is a generalization of
the Cayley graph. We will see that for a given system (W, S), the 1-skeleton of this
cellulation of the corresponding Davis Complex is the (non-directed) Cayley graph
of (W, S). Moreover, the combinatorial data from the deﬁning Coxeter graph will
be on display. We should be clear: We are not constructing a new space. This is a
different cellulation of the space Σ under the same group action.
Fig. 12 Σ in the case
W = D3, the dihedral group
of order 6, with fundamental
chamber K = Σ/W shaded.
/0
{s}
{r}
K
rK
rsK
rsrK
srK
sK

28
T.A. Schroeder
K
tK
sK
tsK
uK
vK
utK
rK
vuK
rvK
srK
···
···
···
···
···
···
···
···
···
···
Fig. 13 Σ ∼
= H2, with fundamental chamber K = Σ/W shaded.
5.4.1
Euclidean Representations
Let (W, S) be a Coxeter system. For each spherical subset T ⊆S, there is an action
of WT on R|T|. We examine some low-dimensional examples:
•
T = ∅: Then WT = {1}, and we have a trivial action of the identity group on a
point (R0 is a point).
•
T = {s}, for some s ∈S: Then WT ∼= Z2, and we have a reﬂection action on R1.
Considering a reﬂection about the point x = 0, the non-negative real line forms
a strict fundamental domain.
•
T = {s, t}, for some s, t ∈S with edge label mst: Then WT ∼= Dn, and we
have a group action on R2. Speciﬁcally, consider reﬂections over the x-axis, say
this corresponds to s, and a reﬂection corresponding to t over the line through the
origin making an angle with π/mst with the x-axis. The student should verify that
the order of the composition of these reﬂections is mst. The region between, and
including, the two reﬂection lines is a strict fundamental domain for this action.
•
T = {r, s, t}, for some r, s, t ∈S: This is left for the reader.
...

Coxeter Groups and the Davis Complex
29
5.4.2
The Coxeter Cell of Type T
Now, given a spherical subset T, and a representation like that described above,
we form the Coxeter cell of type T: Begin with a point in the interior of the strict
fundamental domain described above, and calculate its orbit. The points in the orbit
are in 1–1 correspondence with the elements of WT. We then take the convex hull
of the resulting points. This means we form a CW-complex where the 0-cells are
the points, 1-cells are edges between these points, 2-cells are attached to cycles of
edges, and so on. The resulting space is a cellulation of a topological |T|-ball we call
a Coxeter cell of type T. In the examples we considered above, the student should
conﬁrm that we have:
•
T = ∅: A Coxeter cell of type ∅is a point.
•
T = {s}, for some s ∈S: A Coxeter cell of type {s} is an edge.
•
T = {s, t}, for some s, t ∈S with edge label mst: A Coxeter cell of type {s, t} is
a ﬁlled-in (2mst)-gon.
•
T = {r, s, t}, for some r, s, t ∈S: A Coxeter cell of type {r, s, t} is a 3-ball. A
solid polytope. The details are left to the reader.
...
We then have a new cellulation of Σ called the Coxeter cellulation. To emphasize
the Davis complex equipped with this cellulation, we write Σcc. The n-cells are
the Coxeter cells of type T where |T| = n, one cell of type T for each coset of
WT in W. Speciﬁcally, the 0-cells of Σcc correspond to the cosets of W∅, i.e. the
group elements. The 1-cells correspond to the cosets of Ws, for each generator s.
(This means that an edge of type {s} has endpoints w and ws, for arbitrary w ∈
W). The 2-cells correspond to cosets of W{s,t}, etc. As mentioned above there is a
nice relationship between this cellulation, and the deﬁning graph Γ. The student is
invited to explore the Coxeter cellulation in Project 1.
Example 17. Consider the case W =

r, s | r2, s2, (rs)3
with Σ shown in Fig-
ure 12. The Coxeter cellulation of the same space has six 0-cells, corresponding to
the translates of the 0-cell corresponding to ∅; six 1-cells, coming in two types, {r}
and {s}, connecting the 0-cells; and one 2-cell of type {r, s}. The student should
calculate the cosets of W{r} and W{s} in W, and check these against the vertex sets
of edges in Σcc. Finally, note that the entirety of Σcc is a prototype for a Coxeter cell
of type {r, s} when mrs = 3.
Exercise 19. Show that the relation ∼on W × K described above generates an
equivalence relation.
Exercise 20. Show that if (v, x) ∼(u, y) as described above, then for any w ∈W,
w · (v, x) ∼w · (u, y). That is, show that the left action of W on W × K respects
the equivalence relation and descends to an action of W on Σ. Show that this action
is cellular, proper, co-compact, and has strict fundamental domain K. Conclude that
Σ/W = K.

30
T.A. Schroeder
Exercise 21. Calculate χorb(K) for the K depicted in Figure 13. Calculate
χorb(K) for K depicted in Figure 11. Do you get the same answer?
Exercise 22. Sketch the Davis complex Σ associated to the inﬁnite dihedral group
D∞. Sketch the Cayley graph associated to D∞and consider the action of D∞on
this graph. How does it compare to Σ?
Exercise 23. Sketch the Davis complex Σ associated to each of the Coxeter graphs
in Figure 4. Also consider the case Γ is three points, no edges.
Exercise 24. Sketch Σ using the group deﬁned in Example 9, using the cellulation
described in Section 5.3 and Figure 13, but the relator (rv)2 is changed to (rv)4.
Calculate χorb(K).
Exercise 25. Sketch Σ using the cellulation described in Section 5.3 Figure 13 for
the group
W =

r, s, t, u | r2, s2, t2, u2, (rs)2, (st)2, (tu)2, (ru)2
.
Does the geometry in your picture need to be distorted in any way? Calculate
χorb(K).
Exercise 26. Sketch Σ for the Coxeter group deﬁned by Coxeter graph Γ a
hexagon, with each edge labeled 4. Calculate χorb(K).
6
Closing Remarks and Suggested Projects
Working through the above exercises and examples, along with checking and
verifying the references, should provide good material for undergraduate projects
or even comprise part or all of an independent study course. In addition, we invite
you to consider some of the following related ideas.
Research Project 1. Investigation of the cellulations Σ and Σcc.
Students are invited to explore the Coxeter cellulation described in Section 5.4 by
consulting [4]. In particular, the student should conﬁrm the assertions made above
by considering the following:
•
The construction of Σcc for the Coxeter groups presented throughout this chapter.

Coxeter Groups and the Davis Complex
31
•
The relationship between the ﬁne (simplicial) cellulation of Σ and the Coxeter
cellulation Σcc.
•
The poset of Coxeter cells in Σcc.
•
Verify that the 1-skeleton of Σcc is isomorphic to the Cayley graph of (W, S).
•
Look up the deﬁnition of the “Cayley 2-complex” and investigate its relationship
to Σcc.
•
Look up the deﬁnition of the nerve of a Coxeter system, and the link of a vertex
in a CW-complex. Show that the link of every vertex in Σcc is isomorphic to the
nerve, and that the 1-skeleton of the link is the deﬁning graph Γ.
•
For the case when the nerve of (W, S) is a sphere, determine the relationship
between the Coxeter cellulation and “mirror” cellulation described above. In
particular, note that Σ and Σcc are the same space, admitting the same group
action.
Furthermore, with just a few exceptions, it is known (and can be demonstrated
by the student) that if the deﬁning graph is homeomorphic to S1, then the Davis
complex is a 2-manifold. But this relationship generalizes to higher dimensions.
Namely, if the nerve of the Davis complex is homeomorphic to Sn−1, then Σ is an n-
manifold. Students are invited to explore these deﬁnitions, see why there are a “few
exceptions” to Σ a 2-manifold in the case the deﬁning graph is an m-gon, and use
this strategy as a vehicle to develop examples of group actions on manifolds. But the
connection between the nerve and the Coxeter cellulation also enables the efﬁcient
construction of the Davis complex (and therefore the Cayley graph) in cases when
the deﬁning graph or nerve are not isomorphic to cellulations of Sn−1. For starters,
consider the case when a deﬁning graph is a triangle with an extra edge extending
from one vertex, and all edges labeled with 2.
This survey activity is intended to be exploratory for the student, and may serve
to connect some concepts about Cayley graphs, groups, and cosets presented in an
undergraduate abstract algebra course.
Research Project 2. Geometry of Σ.
Refer to the geometry discussion in Section 3.2. Noting the deformity of the Davis
complex in Figure 13 (is that really a right-angled pentagon?), students are invited
to explore the relationship between the relators deﬁning the Coxeter group and
the corresponding geometry of the Σ. Given a Coxeter graph Γ an m-gon, and
considering the so-called “mirror” cellulation, students should understand what
angle should be present between the r- and s-mirrors to respect a relator of the form
(rs)n. Then, using some non-Euclidean, Euclidean, and Neutral geometry results
from a standard undergraduate geometry course, students are invited to explore
the geometry that must be imposed on various examples of Σ if these angles are
assigned between mirrors of K. For this exploration, students are encouraged to take

32
T.A. Schroeder
a deﬁning graph Γ an edge or an m-gon and classify the geometry of the resulting
Davis complexes. For an investigation of the 3-dimensional geometry of Σ, students
are directed to [13].
Research Project 3. Starting with Challenge Problems 6 and 7, ﬁnd ﬁnite
index, torsion free subgroups and classify the resulting surfaces covered by Σ.
The surface in Figure 9 is familiarly described as a “torus,” but it is also called
a genus 1 surface, due to the surface having one “hole.” Similarly, a sphere is a
genus 0 surface, a two-holed torus is a genus 2 surface, etc. As in Example 13, such
surfaces arise as quotient spaces of Group actions on manifolds. (In that case, Z×Z
acting on R2.) Moreover, it is known that compact, closed surfaces are uniquely
determined by their Euler characteristic: A compact, closed Surfaces of genus g
have χ = 2−2g. Indeed, we see in Example 11 that the torus has χ = 2−2(1) = 0.
As discussed in Project Idea 1, with Γ an m-gon, we have, with few exceptions,
that Σ is a 2-manifold. The space K = Σ/W is compact (it is a ﬁnite complex), but it
is not a closed surface like the torus. It has a boundary along the edge of K, whereas,
the torus has no edge or boundary to it at all. (We are dealing with just the surface of
the torus. Not the inside.) The issue with K is that in the action of W on Σ we have
non-trivial cell-stabilizers, whereas the action of Z × Z on R2, the identity element
is the only group element that ﬁxes any cell. In this case, K = Σ/W is called an
“orbifold” and it is for spaces like this that the orbihedral Euler characteristic is
meant.
As investigated in Challenge problems 6 and 7, it is the case that we can ﬁnd
ﬁnite index subgroups H of W for which Σ/H is a compact, closed surface and
therefore has genus determined by its (regular) Euler characteristic. In other words,
we can ﬁnd subgroups H ≤W that act on Σ with all cell-stabilizers being trivial.
These are called “torsion-free” subgroups. In general, a group is torsion free if it
contains no, non-trivial, elements with ﬁnite order.
The suggested project for the student is for a given m-gon Γ, use the ﬁrst
isomorphism theorem to detect ﬁnite index, torsion-free subgroups. In particular,
ﬁnd a homomorphism φ : W →G where G is a ﬁnite group, and where the identity
element of W is the only element in the kernel that stabilizes any cell of Σ. (That is,
all cell-stabilizers inject.) If such a map is found, H = ker φ is such a subgroup.
With that set-up, let Γ be an m-gon for which Σ is a 2-manifold. Develop a map
φ : W →G described above and verify in examples that H = ker φ is a ﬁnite-
index, torsion free subgroup. Verify, in general, that such an H is always a ﬁnite
index, torsion-free subgroup and that Σ/H is a compact, closed surface (see the
hint). Finally, use the multiplicative property of the orbihedral Euler characteristic in
Equation 8 to identify the resulting surface Σ/H. For small genus, students may also
be able to construct the surface as an identiﬁcation space in the mode of Figure 9.

Coxeter Groups and the Davis Complex
33
Hint: H being ﬁnite index is clear. To verify H is torsion free, note that any torsion
element h ∈H must stabilize a point of σ, hence a cell of Σ (this needs a ﬁxed point
theorem and an averaging argument that allows one to ﬁnd a “centroid” for an orbit.
See [1, Corollary 2.8] for reference.) Since this element h is in H = ker φ, it maps
to the identity in G and is in some cell stabilizer. But since φ is injective on cell
stabilizers, h must be trivial.
References
1. Bridson, M.R., Häﬂiger, André.: Metric Spaces of Non-Positive Curvature. Springer, Berlin
(1999)
2. Cannon, J.: Geometric Group Theory. In: Daverman, R.J., Sher, R.B. (eds.), chap. 6, pp. 261–
305. Elsevier, Amsterdam (2001)
3. Davis, M.W.: Groups generated by reﬂections and aspherical manifolds not covered by
Euclidean space. Ann. Math. 117, 293–324 (1983)
4. Davis, M.W.: The Geometry and Topology of Coxeter Groups. Princeton University Press,
Princeton (2007)
5. Davis, M.W., Moussong, G.: Notes on nonpositively curved polyhedra.
In: Broczky, K.,
Neumann, W., Stipicz, A. (eds.) Low Dimensional Topology, pp. 11–94. Janos Bolyai
Mathematical Society, Budapest (1999)
6. Fraleigh, J.B.: A First Course in Abstract Algebra, 7th edn. Pearson, Boston (2002)
7. Geoghegan, R.: Topological Methods in Group Theory. Springer, New York (2008)
8. Hatcher, A.: Algebraic Topology. Cambridge University Press, Cambridge (2002)
9. Humphreys, J.: Reﬂection Groups and Coxeter Groups. Cambridge University Press, Cam-
bridge (1990)
10. Hungerford, T.W.: Algebra. Springer, New York (1974)
11. Messer, R., Strafﬁn, P.: Topology Now! Mathematical Association of America, Washington,
DC (2006)
12. Munkres, J.: Toplogy, 2nd edn. Prentice Hall, Upper Saddle River, NJ (2000)
13. Schroeder, T.A.: Geometrization of 3-dimensional Coxeter orbifolds and Singer’s conjecture.
Geom. Dedicata. 140(1), 163ff (2009). doi:10.1007/s10711-008-9314-5
14. Schroeder, T.A.: ℓ2-homology and planar graphs (2013). Colloq. Math. doi:10.4064/cm131-
1-11
15. Venema, G.A.: Foundations of Geometry, 2nd edn. Pearson, Boston (2012)

A Tale of Two Symmetries: Embeddable and
Non-embeddable Group Actions on Surfaces
Valerie Peterson and Aaron Wootton
Suggested Prerequisites. An introductory course in group theory and a basic
understanding of three-dimensional Euclidean space.
1
Introduction
Mathematics has long been fascinated by symmetry, so it is perhaps unsurprising
that the topic continues to suffuse much of the current undergraduate curriculum.
Indeed, examining the symmetries of an object—say, the rigid rotations and
reﬂections of regular polygons in the plane or polytopes in space (such as the
Platonic solids in R3)—provides an intuitive context for a student’s ﬁrst foray into
group theory. Here we expand on that notion by extending symmetries of regular
polytopes to compact Riemann surfaces, or g-holed doughnuts. These surfaces are
objects possessed of deeper mathematical structure than regular polytopes but, we
claim, a rich source of problems tractable to those with only a modest amount of
algebraic experience.
In what follows, we distill the modern approach to ﬁnding ﬁnite (conformal)
symmetry groups of compact Riemann surfaces down to a few manageable tools
in an effort to provide a treatment accessible to a wide array of students, faculty,
and other enthusiasts.The beauty of this perspective is that jumping into the process
of ﬁnding symmetry groups does not require a deep understanding of conformal
maps or covering space theory. Rather, many of the foundational ideas can be
translated into relatively straightforward problems in computational ﬁnite group
theory using a contemporary adaption of “Riemann’s existence theorem,” which
V. Peterson () • A. Wootton
Department of Mathematics, University of Portland, 5000 N. Willamette Blvd, Portland,
OR 97203, USA
e-mail: petersov@up.edu; wootton@up.edu
© Springer International Publishing AG 2017
A. Wootton et al. (eds.), A Primer for Undergraduate Research, Foundations for
Undergraduate Research in Mathematics, DOI 10.1007/978-3-319-66065-3_2
35

36
V. Peterson and A. Wootton
veriﬁes exactly when a ﬁnite group can act on a Riemann surface in terms of two
concrete conditions, one group theoretic and one arithmetic. The introduction of this
powerful tool has mustered somewhat of a renaissance for the study of symmetry
groups of compact Riemann surfaces in the past thirty years, with signiﬁcant input
provided by undergraduates via summer research programs, independent studies,
and honors projects. References [1, 2, 4], and [23] are just a sample of the many
recent such works in this area. We suspect there are many more hidden gems in the
ﬁeld yet to be unearthed.
A typical way to introduce compact Riemann surfaces and their symmetry groups
is via so-called classical surfaces, due to their visual appeal. Intuitively speaking,
a classical surface is a smooth, genus g surface in three-dimensional Euclidean
space, the genus being the number of holes in the surface.1 One can now visualize
a symmetry (or symmetry group) by choosing a classical surface S in R3 with the
appropriate shape and positioning so that a group of rigid rotations about some axis
(or set of axes) naturally acts on S. For example, take any smooth genus g surface
in R3, embedded nicely so that its holes line up symmetrically along some ﬁxed
axis (illustrated in Figure 1). The group generated by rotating the surface through π
radians about this axis produces an action of Z2, the cyclic group of order 2, on this
surface. In this case we say that Z2 is a symmetry group for this surface.2
Deﬁnition 1. When a rigid rotation of R3 produces a symmetry of a classical
surface S embedded in R3, as in Figure 1, we say that it is an embeddable symmetry
of S. We then say that a group of symmetries G of a classical surface S is embeddable
if it can be realized as the restriction of a rotation subgroup of the full group of
isometries of R3 to the embedded surface S.
We note that this deﬁnition excludes reﬂections because, though they may indeed
produce embedded symmetries in a certain context, such maps are necessarily “anti-
conformal” and we wish to consider only those actions that preserve the underlying
structure of S (the so-called conformal automorphisms). Examining anti-conformal
symmetries could make an interesting (advanced) undergraduate project, however;
readers are referred to [5,24] for relevant background.
π
Fig. 1 The generator of the cyclic rotational symmetry group of order 2 on a genus g classical
surface.
1Stated more carefully, the extra structure that makes a classical surface a Riemann surface is a
complex analytic structure resulting from the presence of a collection of local C∞-charts, which
make the surface a one-dimensional complex manifold. As our approach here does not make direct
use of the complex analytic structure of these surfaces, however, we may safely eschew such
technical details.
2Our use of the word “symmetry” here is a colloquial choice to replace the more precise but less
widely known “conformal automorphism.”

Embeddable and Non-embeddable Group Actions on Surfaces
37
Further, we note that although every compact Riemann surface can be realized
as a classical surface (see [6] or [19]), very few groups can actually be embedded.
This follows from the fact that ﬁnite subgroups of the isometry group of R3 are
either cyclic, dihedral, or isomorphic to one of the alternating groups A4 or A5 or
to the symmetric group S4 [16, Thm 17.10]. However, a classical result of Hurwitz
[10] guarantees that every ﬁnite group acts on some compact Riemann surface of
sufﬁciently high genus; thus, indeed, “most” ﬁnite groups are not embeddable.
In the course of mentoring multiple undergraduate student research projects in
the area, we have formulated what we feel is a coherent and friendly overview of
the study of (conformal) symmetries of surfaces, one which assumes little more
than a basic grounding in ﬁnite group theory and Euclidean geometry. The primary
purpose of this article is, therefore, to share this analysis and perspective so that
others might appreciate the beauty and simplicity of the subject. As an application,
we employ the techniques and tools described below in order to detect all A4-actions
on Riemann surfaces of genus g ≥2. Further, necessary and sufﬁcient conditions
are given characterizing exactly which of those A4-actions are embedded. More
precisely, we construct all possible surfaces for which an action of A4, together with
its signature data, is embeddable, the distinction being that we are also interested in
preserving properties of the quotient prescribed by that action. Regarding the other
possible ﬁnite isometry subgroups, we speculate that determining conditions for the
embeddability of A5 or S4 analogous to the conditions that follow for A4 would
make fruitful student research projects. Finally, the theory developed herein holds
speciﬁcally for surfaces of genus g ≥2, so we ﬁx this assumption for the remainder
of the article.
2
Determining the Existence of a Group Action
With the goal of presenting necessary and sufﬁcient conditions for a ﬁnite group
to act by symmetry on a compact Riemann surface S of genus g ≥2, we ﬁrst
introduce and illustrate the needed terminology with some concrete and easy-to-
visualize examples. Though the examples we present all involve classical surfaces
in R3, the ideas discussed also apply to group actions on more abstract surfaces.
2.1
Realizing A4 as a Group of Rotations
The group of isometries (or “rigid symmetries”) of R3 can be described in various
ways, such as products of reﬂections. We refer the interested reader to [16] for more.
The subgroup of the full group of isometries that consists of orientation-preserving
symmetries (or “rigid motions”) is generated by translations and rotations. Ulti-
mately, we are interested in the different ways that A4 can act on surfaces as a group
of rotations, so we will begin by examining a familiar action of A4 on one of the
Platonic solids: a tetrahedron embedded in R3.

38
V. Peterson and A. Wootton
First recall that A4 is the alternating group on four elements; it is the subgroup of
the symmetric group S4 consisting of all even permutations. It is often convenient
to denote the elements of A4 using the following permutation notation:
A4 = {(1), (12)(34), (13)(24), (14)(23), (123), (132),
(124), (142), (134), (143), (234), (243)}
Note that A4 has many cyclic subgroups. Subgroups of order 2 are generated by
permutations of the form (ab)(cd) (the products of transpositions) and order 3
subgroups are generated by elements of the form (abc) (the 3-cycles). When taken
together with the identity, the three products of transpositions form a subgroup
of order four isomorphic to the Klein 4-group, V4. Readers may refer to [9], for
example, for a treatment of alternating and symmetric groups and their structure.
Now consider how A4 can act on an embedded regular tetrahedron T whose
center lies at the origin (see Figure 2). We view the permutation cycles of A4 listed
above as permuting the vertices of T and now seek to describe those permutations
as rotations of R3.
Consider an axis passing through the uppermost vertex of T and the origin.
Rotating T about this axis through an angle of 2π
3 yields a rotational symmetry of
T that cyclically permutes the remaining three vertices of T, as shown in Figure 3
(left). If we label the vertices as shown, this symmetry is represented by the order 3
permutation (1)(234) = (234). We obtain similar rotational symmetries about lines
passing through the origin and each of the other vertices and thereby obtain actions
on T represented by the remaining 3-cycles in A4.
In addition to these rotational symmetries, if we skewer any two non-adjacent
(opposite) edges of T by a line passing through their midpoints, as in Figure 3
Fig. 2 A regular tetrahedron
T in R3.
Fig. 3 Rotations of the
tetrahedron in R3 of order 3
(left) and order 2 (right).
2π
3
π
2
2
3
3
4
4
1
1

Embeddable and Non-embeddable Group Actions on Surfaces
39
(right), rotation about this axis through an angle of π also yields a symmetry of T,
this time of order 2. Representing this symmetry as a permutation of the vertices
as numbered above, rotation about the line depicted in Figure 3 yields (14)(23).
Rotating about similar lines through the other two pairs of opposing sides yields
the permutations (12)(34) and (13)(24). The rotational symmetry group of T, then,
contains A4. (In fact, there are no other rotations of T, so A4 is the full group of
rotations. See [16, Thm 17.2].)
Soon we will use the tetrahedron T to construct a number of different surfaces
on which A4 acts. Henceforth, when we refer to A4 as a group of rotations acting on
one of those surfaces, we shall be referring to the speciﬁc rotations just described.
2.2
Preliminary Examples
To learn more about group actions, we turn now to examples. These will also prepare
us for the construction of new surfaces on which A4 acts, wherein handles and
surfaces of higher genus are added to the tetrahedron.
Example 1. Let S be the classical genus 2 surface illustrated in Figure 4 with its
center at the origin, situated symmetrically across all axes.
If α is a rotation in R3 through π radians about the x-axis, then the group G1 =
⟨α⟩will be an embedded group of symmetries of S isomorphic to the cyclic group
of order 2. Likewise, if β is a rotation through π radians about the y-axis, then the
group G2 = ⟨β⟩will be an embedded group of symmetries of S also isomorphic to
the cyclic group of order 2. It is not hard to see that the group G = ⟨α, β⟩, which
is isomorphic to the Klein 4-group, is also an embedded group acting on S wherein
the element αβ = γ accomplishes rotation through π about the z-axis.
Observe that the groups G1 and G2 from Example 1 act very differently on the
genus 2 surface despite being isomorphic groups; α takes each hole to itself, for
example, whereas β swaps the holes. This should clarify the earlier comment that
we do not want simply to classify how to embed A4 itself, but to classify how to
embed all the different ways A4 can act. To determine exactly when a symmetry
group acts on a surface, we need to develop a coherent way of describing the effects
of group actions on a surface in order to distinguish between the actions themselves.
As motivation, let’s consider a few more examples; we’ll refer back to each of these
in the next section.
Fig. 4 Various Z2-actions on
a genus 2 surface.
β
α
γ
x
y
z

40
V. Peterson and A. Wootton
Fig. 5 A4 acts on an inﬂated
tetrahedron (left), and on an
inﬂated tetrahedron with a
“jack” added (right).
Fig. 6 A4 acts on a
tetrahedron with tripods (left)
and on the boundary of a
punctured solid tetrahedron
(right).
Example 2. Rather than considering the entire tetrahedron, now consider the
skeleton of T (i.e., just the vertices and the edges of T). If we “inﬂate” these
edges into tubes and smooth out the corners appropriately, we obtain a compact
Riemann surface on which A4 acts as an embedded group exactly as before; see
Figure 5 (left). Observe that this is a surface of genus 3: if we ﬂatten the inﬂated
tetrahedron by pushing down on the top vertex while expanding the base, the bottom
triangle becomes the outer ring of a three-holed torus. We denote this (pre-squashed)
inﬂated skeleton of T by T(0;2,2,3,3). The seemingly strange notation will become
self-explanatory in due time.
Example 3. Now consider the skeleton of the tetrahedron T with four additional
edges added as follows: an edge emanates from each vertex, and all four edges meet
and terminate at the center of T. As in the previous example, inﬂating these edges
into tubes and smoothing out the corners appropriately produces a compact Riemann
surface on which A4 acts; see Figure 5 (right). Observe that this is a surface of genus
6: though trickier to visualize, if we push down on the center of the jack to produce
three windowpanes below and then push the top vertex of the tetrahedron through
one of those panes, we will end up with a handle attached to 5 panes, for a total of
6 holes. (For a combinatorial argument that this surface has genus 6, see [7].) We
denote this “tetrahedron with a jack” surface by T(0;2,2,2,3,3).
Example 4. For a genus 8 surface on which A4 acts, consider the following. Begin
with the original tetrahedron and glue in a “tripod,” consisting of three cylinders
emanating symmetrically from a sphere, around each vertex of the tetrahedron. This
surface, denoted T(0;2,3,3,3,3), is pictured in Figure 6 (left).

Embeddable and Non-embeddable Group Actions on Surfaces
41
Example 5. For one last foundational example, we now return to the original
tetrahedron T acted on by A4 but now also place a smaller sphere inside T (this
construction works just as well by placing a smaller tetrahedron, or any surface
homeomorphic to a sphere, inside T but is easiest to visualize with the sphere).
Construct a new surface by drilling holes through the tetrahedron and sphere (at
the midpoints of the tetrahedron’s edges, say) and connecting the inner and outer
surfaces by gluing a cylinder into each pair of holes, so that the interior of the
new surface lies between the two original surfaces. Smoothing corners and edges
appropriately yields a compact Riemann surface on which A4 once again acts as an
embedded group; see Figure 6 (right). Note that this new surface has genus 5: even
though 6 holes were drilled, it is possible to deform the surface by stretching one of
these cylinders out and around the larger tetrahedron, leaving just 5 handles. (The
moral is that one must be careful when counting holes to determine genus. This
was also illustrated in Example 2, where the inﬂated tetrahedron skeleton—with 4
apparent holes—was deformed to reveal a surface of genus of 3.) We denote this
(pre-deformed) surface by T(0;3,3,3,3).
2.3
Signatures
As previously noted, the groups G1 and G2 from Example 1 are isomorphic but act
very differently as symmetry groups of the genus 2 surface in that example. In order
to capture the differences between groups actions, we now introduce the notion of
a signature. Signatures record information about the orbits and branch points of
an action. We will develop these notions via examples and then give more formal
deﬁnitions.
Recall that if a group G acts on a set X, then the orbit of an element x ∈X is the
set of all points in X to which x is mapped by elements of G; that is, the orbit of x is
the set {y ∈X | y = g(x) for some g ∈G}. For such an action, we then deﬁne the
quotient space X/G to be the set of all orbits of X under the action of G. Now, if G
is acting on a compact Riemann surface S, the quotient space S/G can be made into
a compact topological space after endowing it with the quotient topology. (In fact,
with appropriate local charts chosen, S/G is itself a compact Riemann surface, see
[17, Thm 3.4].) It will be convenient here to think of S/G as a subset of S consisting
of exactly one member from each possible orbit (i.e., as a fundamental domain of S).
Example 6. To illustrate this, let S be the classical genus 2 surface from Example 1
and let G1 = ⟨α⟩, the order 2 group of rotations of S about the x-axis. The quotient
surface S/G1 can be realized by taking half of the surface S (obtained by slicing S
with the xz-plane) and identifying the top and the bottom halves of each circle that
results from the intersection of S with the xz-plane; see Figure 7. Observe that the
surface S/G1 has genus 0.

42
V. Peterson and A. Wootton
p
Fig. 7 Acting by α on this genus 2 surface produces a genus 0 quotient
The previous example illustrates that sometimes it is not hard to describe the
quotient space S/G, or its genus, explicitly. It isn’t always so easy. Fortunately, we
shall see in the next section that there is a formula for determining the genus of a
quotient.
Deﬁnition 2. If G is a ﬁnite group acting on S, let πG : S →S/G denote the natural
quotient map between the surfaces S and S/G. That is, πG takes a point in S and
returns the orbit of that point, now thought of as a single element in the quotient
S/G. In the other direction, given an orbit y ∈S/G, then π−1
G (y) is that same orbit,
now thought of as a set of points in S. We say an orbit y ∈S/G is a branch point of
the map πG if |π−1
G (y)| < |G|. We deﬁne the order of y to be |G|/|π−1
G (y)|.
Note that if no nontrivial element of G ﬁxes a point x ∈S, the G-orbit of x
will consist of |G| distinct points. Thus, in some sense, “most” orbits (as a set of
points in S) have size |G| and branch points are simply those orbits whose size is
strictly smaller. We use the term branch “points” rather than branch “orbits” merely
to highlight the fact that we are referring to elements of S/G.
As it turns out, the order of a branch point is always an integer. For any point
x ∈S, the stabilizer of x is the subgroup of G that leaves x ﬁxed; we denote this by
StabG(x). The orbit-stabilizer theorem (see [9]) implies that the stabilizer subgroups
for points in the same orbit are conjugate, and that the size of an orbit is the (shared)
index of any (conjugate) stabilizer. Thus, the size of an orbit divides |G|. This also
gives us a way to determine the order of a branch point: choose any point x ∈S
from a given orbit, and count the number of elements of G that ﬁx that point. That
is, the order of the branch point πG(x) coincides with |StabG(x)|, the order of the
stabilizer subgroup of x. So, orbits of elements that are not ﬁxed by G (i.e., orbits of
elements whose stabilizer is the trivial subgroup {e}) have size |G| and we do not
refer to πG(x) as a branch point in this case.
Example 7. Again, let S be the classical genus 2 surface from Example 1 and let
G1 = ⟨α⟩. Observe that the map πG1 has six branch points, each corresponding to
an intersection point of S with the x-axis; more precisely, the intersection of the x-
axis with the fundamental domain shown in the middle of Figure 8. Since |G1| = 2
and there is exactly one point in each preimage π−1
G1 (y) for each branch point y, the
order of each branch point will be 2/1 = 2 (Figure 8).
We are now ready to introduce the main tool we shall use to describe and help
differentiate between group actions.

Embeddable and Non-embeddable Group Actions on Surfaces
43
p
Fig. 8 Branch points and quotient for the action of ⟨α⟩on a genus 2 surface.
x
p
Fig. 9
Acting by β on this genus 2 surface produces a genus 1 quotient.
Deﬁnition 3. Suppose that a ﬁnite group G acts on a surface S. We deﬁne the
signature of the action to be the tuple (h; m1, . . . , mr) where h is the genus of S/G
and r is the number of branch points of πG, the order of these branch points being
m1, m2, . . . , mr. The order in which we list the mi does not matter, though they
typically appear in non-decreasing order.
The signature of a group action encodes much of the topological and geometric
structure of how the group acts on a surface S. We illustrate with a couple of
examples using classical surfaces.
Example 8. Once again, let S be the classical genus 2 surface from Example 1 and
let G1 = ⟨α⟩. As illustrated in Example 6, the quotient space S/G1 has genus 0.
In Example 7, we showed that the map πG1 has 6 branch points, each of order 2.
Therefore, G1 acts with signature (0; 2, 2, 2, 2, 2, 2).
Now consider G2 = ⟨β⟩. The quotient surface S/G2 can be identiﬁed with the
half of S obtained by slicing S with the yz-plane and identifying the top and the
bottom halves of the circle where S intersects the yz-plane; see Figure 9. The surface
S/G2 still contains a hole, so has genus 1. Also observe that the map πG2 has two
branch points, each again corresponding to the intersection point of S (or, precisely,
of the fundamental domain of S/G shown in the middle of Figure 9) with the y-axis.
Since |G2| = 2 and there is exactly one point in each preimage π−1
G2 (y) for each
branch point y, the order of each branch point will be 2/1 = 2. Therefore, G2 acts
with signature (1; 2, 2).
We can use similar techniques to determine the signatures of the other groups
acting on S from Example 8.
Exercise 1. Show that the signature of ⟨αβ⟩(the z-axis rotation from Example 1)
is also (1; 2, 2).

44
V. Peterson and A. Wootton
Exercise 2. Show that the group G = ⟨α, β⟩is isomorphic to the Klein 4-group
and acts with signature (0; 2, 2, 2, 2, 2).
2.4
Generating Vectors and Riemann’s Existence Theorem
We introduced the notion of the signature of a group action to encode topological
data associated with the action. However, it is also one of the fundamental tools used
to provide necessary and sufﬁcient conditions for the existence of a group action on
a surface S of genus g ≥2. Before we provide these conditions, we present a few
needed deﬁnitions.
Deﬁnition 4. For a ﬁxed group G and elements x, y ∈G, we deﬁne the commutator
of x and y to be xyx−1y−1 and denote it by [x, y]. We call the subgroup of G generated
by all its commutators the commutator subgroup.
Deﬁnition 5. Given a ﬁnite group G with identity element eG, we say a vec-
tor (a1, b1, a2, b2, . . . , ah, bh, c1, . . . , cr) of elements of G is an (h; m1, . . . , mr)-
generating vector for G if all of the following hold:
1. G = ⟨a1, b1, a2, b2, . . . , ah, bh, c1, . . . , cr⟩
2. The order of cj is mj for 1 ≤j ≤r
3. h
i=1[ai, bi] r
j=1 cj = eG
We are now ready to state the main tool used in modern day group action
classiﬁcation, “Riemann’s existence theorem.”
Theorem 1. [3, Prop. 2.1] A ﬁnite group G acts on a compact Riemann surface S
of genus g ≥2 with signature (h; m1, . . . , mr) if and only if:
1. the Riemann–Hurwitz formula is satisﬁed:
g −1 = |G|(h −1) + |G|
2
r

j=1

1 −1
mj

,
and
2. there exists an (h; m1, . . . , mr)-generating vector for G.
Put another way, this theorem says we can guarantee the existence of some group
action simply by ﬁnding a generating vector and checking that the Riemann-Hurwitz
formula produces g ≥2. As we will now see with a few examples, constructing a
generating vector with the desired properties is often a straightforward task.
Example 9. The vector (x, x, x, x, x, x) is a (0; 2, 2, 2, 2, 2, 2)-generating vector for
the cyclic group Z2 = ⟨x⟩= {e, x} of order 2. The three conditions for being a

Embeddable and Non-embeddable Group Actions on Surfaces
45
generating vector are satisﬁed: clearly G = ⟨x, x, x, x, x, x⟩, the order of each element
is 2, and x · x · x · x · x · x = x6 = e. The Riemann-Hurwitz equation becomes
g −1 = 2(0 −1) + 2
2
6

j=1

1 −1
2

= −2 + 6 · 1
2 = −2 + 3 = 1,
from which we deduce that g = 2. Theorem 1 then implies that there is an action of
Z2 with signature (0; 2, 2, 2, 2, 2, 2) on a surface of genus 2. We exhibited such an
action in Example 8.
Example 10. The vector (e, e, x, x) is a (1; 2, 2)-generating vector for the same
cyclic group Z2 = ⟨x⟩of order 2. In this case, the Riemann-Hurwitz equation yields
g −1 = 2(1 −1) + 2
2
2

j=1

1 −1
2

= 0 + 2 · 1
2 = 1,
and again we obtain g = 2. Theorem 1 tells us there is an action of Z2 with signature
(1; 2, 2) on a surface of genus 2. This action, too, was exhibited in Example 8.
Example 11. The vector
	
(12)(34), (13)(24), (123), (124)

is a (0; 2, 2, 3, 3)-
generating vector for the alternating group A4. Verifying that that these four
elements indeed generate A4 is left to the reader. The Riemann–Hurwitz equation
becomes
g −1 = 12(0 −1) + 12
2

1 −1
2 + 1 −1
2 + 1 −1
3 + 1 −1
3 +

= −12 + 6 · 7
3 = −12 + 14 = 2,
giving g = 3. As such, Theorem 1 guarantees the existence of an action of A4 on a
surface of genus 3 with signature (0; 2, 2, 3, 3). An example of such an action was
presented in Example 2 wherein A4 acted on an inﬂated tetrahedron skeleton.
Exercise 3. Use Theorem 1 to show that there exists an action of the Klein 4-
group V4 on a surface of genus 2 with signature (0; 2, 2, 2, 2, 2). That is, construct
a (0; 2, 2, 2, 2, 2)-generating vector for V4 and then determine genus by using the
Riemann-Hurwitz formula. Note that an example of such an action was presented in
Example 1 with signature determined in Exercise 2.
Exercise 4. Use Theorem 1 to show that A4 acts with signatures (0; 2, 2, 2, 3, 3),
(0; 2, 3, 3, 3, 3) and (0; 3, 3, 3, 3) by constructing explicit generating vectors. Use
the Riemann-Hurwitz formula to determine the genus of the corresponding surface
in each case. As we shall see shortly, such actions were described explicitly in
Examples 3, 4, and 5 respectively.

46
V. Peterson and A. Wootton
Theorem 1 can also be used in the opposite direction to determine the genus of a
quotient. If one already has already identiﬁed a group action of G on a surface S of
genus g ≥2—not necessarily an embedded action but an action deﬁned explicitly
enough to determine the branching data for the quotient map—then Theorem 1
allows you to compute the genus of S/G and therefore the signature of the G-action
without ever having to construct a generating vector for G. Here follows an example.
Example 12. In Example 2 we constructed the “inﬂated tetrahedron” surface
T(0;2,2,3,3) on which the group A4 acts (as an embedded group, in fact). Notice
that for this action, the only ﬁxed points (i.e., points ﬁxed by at least one non-
identity group element) are the intersection points between the surface and the axes
of rotation. From this we can extract the data on branch points necessary to apply
Theorem 1.
First suppose that L is a line passing through a vertex (inﬂated) and the origin.
Then the two points of intersection of L with the surface (one “inside” the vertex,
closer to the origin, and one “outside”) are the only points ﬁxed by the rotation
about L. Next note that the action of A4 permutes the vertices of T(0;2,2,3,3), so in
particular, all ﬁxed points on the outside of a vertex of T(0;2,2,3,3) belong to the
same orbit and all points on the inside belong to the same orbit (and these two orbits
are distinct). Thus there are two such orbits of these vertex ﬁxed points, each orbit
having four points, and therefore two branch points of order 12/4 = 3 for the action
of rotation about L.
Now suppose that L is any one of the three lines that intersect the (inﬂated)
midpoints of a pair of opposite edges of the inﬂated tetrahedron. Then the four
points of intersection of L with T(0;2,2,3,3) (through the inner and outer midpoints
of the two inﬂated edges through which L passes) are the only points ﬁxed by the
rotation about L. Note that the action of A4 permutes the midpoints of each of the
edges of T(0;2,2,3,3) so, as was the case with vertices, all ﬁxed points on the outside
of a midpoint of T(0;2,2,3,3) belong to the same orbit and all points on the inside
belong to the same orbit and these two orbits are distinct. Thus there are two such
orbits, each with six points, implying two branch points of order 12/6 = 2.
Since there are no other branch points, and we know the genus of T(0;2,2,3,3) is 3,
we can now use the Riemann–Hurwitz formula to ﬁnd the genus of T(0;2,2,3,3)/A4
and hence ﬁnd the signature of this A4-action. Speciﬁcally, we have
3 −1 = 12(h −1) + 12
2

1 −1
2 + 1 −1
2 + 1 −1
3 + 1 −1
3

= 12(h −1) + 6 · 7
3 = 12(h −1) + 14.
Therefore 12(h −1) = −12 so h = 0. It follows that A4 acts with signature
(0; 2, 2, 3, 3) on T(0;2,2,3,3) (explaining our choice of subscript!). Note that in
Example 11 we proved the existence of an action with a given signature by
constructing an explicit generating vector for that action, whereas here we used

Embeddable and Non-embeddable Group Actions on Surfaces
47
our knowledge of an explicit action to compute branching data and then deduce
the signature for the action. This demonstrates two different ways of applying
Theorem 1.
Exercise 5. Repeat the process of Example 12 to recover the signatures for
the actions described in Examples 3, 4, and 5. Speciﬁcally, use the geometric
descriptions given for each action to identify branch points (and their respective
orders) and then apply the Riemann-Hurwitz formula to ﬁnd the genus of each
quotient surface.
3
Actions of the Alternating Group A4
To illustrate an application of the techniques presented above, we now deduce a
complete description of all possible actions of the alternating group A4. Moreover,
in addition to providing necessary and sufﬁcient conditions for when a signature
arises as the signature of some A4-action via Riemann’s existence theorem, we shall
go one step further and determine exactly which of these signatures correspond to
embedded actions.
3.1
Signatures for A4-Actions
In order to determine necessary and sufﬁcient conditions for when an action of A4
on a compact Riemann surface of genus g ≥2 has a given signature we need to
apply Theorem 1. First note that since A4 only contains (nontrivial) elements of
order 2 and 3, any signature for such an action will be of the form (h; 2P, 3Q) where
by “2P, 3Q” we mean P copies of 2 and Q copies of 3. In light of the fact that we
are restricting to genus g ≥2, the requirement that a signature satisfy the Riemann-
Hurwitz formula (condition (1) of Theorem 1) allows us to exclude a number of
signatures immediately.3 A bit of arithmetic shows that any other signature of the
form (h; 2P, 3Q) satisﬁes the Riemann–Hurwitz formula for some g ≥2, so to
determine which of the remaining signatures are actual signatures, we must either
construct a generating vector (by condition (2) of Theorem 1) or show that one can
not be constructed. Since part of the deﬁnition of a generating vector involves group
commutators and group generators, we’ll need two facts (left as exercises) about
commutators and generators to do this.
Lemma 1. The commutator subgroup of A4 is V4, the Klein 4-group.
3Speciﬁcally, the signatures (0; −), (1; −), (0; 2), (0; 2, 2), (0; 2, 2, 2), (0; 2, 2, 2, 2), (0; 3),
(0; 3, 3), (0; 3, 3, 3), (0; 2, 3), (0; 2, 2, 3), and (0; 2, 3, 3), can all be excluded, where (h; −) denotes
an action with no branch points.

48
V. Peterson and A. Wootton
Lemma 2. A subset {x1, . . . , xn} ⊆A4 generates A4 if and only if:
1. at least one of the xi has order 3; and,
2. if all xi have order 3, then n ≥2 and there exist i, j such that xi /∈⟨xj⟩.
We are now ready to provide necessary and sufﬁcient conditions for a signature
to be realizable as the signature of an A4-action.
Theorem 2. A signature of the form (h; 2P, 3Q) is the signature for some A4-action
on a surface of genus g ≥2 if and only if:
1. Q ̸= 1; and,
2. if h = 0 then Q ̸= 0; and,
3. (h; 2P, 3Q) does not belong to this list of exceptions: (1; −), (0; 3, 3), (0; 3, 3, 3),
(0; 2, 3, 3), where (h; −) denotes an action with no branch points.
Proof. First, we introduce some convenient notation. Since we are trying to con-
struct generating vectors for A4, and we know they must have the form (h; 2P, 3Q),
we shall denote such a generating vector by:
(a1, b1, a2, b2, . . . , ah, bh, c1, . . . , cP, d1, . . . dQ)
where a1, b1, a2, b2, . . . , ah, bh are as originally deﬁned, but c1, . . . , cP are all
elements of order 2 and d1, . . . dQ are all of order 3 (so we have split the last r
terms up according to order).
For the forward direction, assume (a1, b1, a2, b2, . . . , ah, bh, c1, . . . , cP, d1, . . . dQ)
is a generating vector with Q = 1. Then we must have
h

i=1
[ai, bi]
⎛
⎝
P

j=1
cj
⎞
⎠d1 = e
or equivalently
h

i=1
[ai, bi]
⎛
⎝
P

j=1
cj
⎞
⎠= d−1
1 .
By Lemma 1, however, we know [ai, bi] ∈V4 for any ai and bi, and we know
P
j=1 cj ∈V4, so it follows that h
i=1[ai, bi]
P
j=1 cj

∈V4, a contradiction since
we are assuming d1 has order 3. Thus a signature of the form (h; 2P, 31) cannot be
the signature of an A4-action.

Embeddable and Non-embeddable Group Actions on Surfaces
49
A vector with h = 0 and Q = 0 will be of the form (c1, . . . , cP). In particular, it
will contain only elements of order 2, so by Lemma 2 cannot generate A4. It follows
that there is no generating vector for A4 with h = 0 and Q = 0, and hence if h = 0,
we must have Q > 0. The Riemann–Hurwitz formula ensures that the signature
(h; 2P, 3Q) does not have the form of any of the exceptions listed in part 3 of the
theorem.
To prove the reverse direction, we start constructing generating vectors that
satisfy the conditions of Theorem 1. We can construct a generating vector of the
form (h; 2P, 3Q) with P, Q ≥2 as follows. We set ai = bi = e for all i. If
P is even, set ci = (12)(34) for all i, and if P is odd, set c1 = (12)(34),
c2 = (13)(24), c3 = (14)(23) and ci = (12)(34) for all i > 3. For Q even, we
set d2i−1 = (123) and d2i = (321) for 1 ≤i ≤Q/2 and for Q odd, we set
d1 = d2 = d3 = (123) and d2i = (123) and d2i+1 = (321) for 2 ≤i ≤(Q −1)/2.
By construction, it is easy to see that each of these generating vectors satisfy the
relation
h
i=1[ai, bi]
 P
j=1 cj
 Q
j=1 dj

= e and since P, Q ≥2, there is
always an element of order 2 and element of order 3 in this generating vector, so
the elements generate A4 by Lemma 2. Thus these signatures always give rise to
some A4-action.
To ﬁnish, we need to consider the case Q = 0 and the cases P = 0, 1 (Q = 1
is excluded by assumption). We shall proceed by cases, though in each case, our
argument will be the same: we provide an explicit generating vector for each
signature. To avoid unnecessary repeated arguments, we note that in each case these
generating vectors will contain elements of order 2 and 3, so by Lemma 2 will
generate A4. We also leave it as an easy exercise to the reader to verify that the
product
h
i=1[ai, bi]
 P
j=1 cj
 Q
j=1 dj

= e is satisﬁed.
Suppose that Q = 0. By the observations above, we must have h > 0. Now if
P = 0 as well, we must have h ≥2, and so we deﬁne a generating vector by
(a1, b1, a2, b2, a3, b3, . . . , ah, bh) = ((12)(34), e, (123), e, e, e, . . . , e, e).
If P = 1, we deﬁne a generating vector by
(a1, b1, a2, b2, . . . , ah, bh, c1) = ((123), (234), e, e, . . . , e, e, (14)(23)).
If P ≥2, we deﬁne a generating vector by a1 = (123), b1 = e, ai = bi = e for all
i > 1 and if P is even, set ci = (12)(34) for all i, and if P is odd, set c1 = (12)(34),
c2 = (13)(24), c3 = (14)(23) and ci = (12)(34) for all i > 3.
Now suppose that P = 0. By our arguments above, we may suppose Q ≥2. If
Q = 2 or Q = 3, we must have h ≥1 in which case we can deﬁne generating
vectors by
(a1, b1, a2, b2, . . . , ah, bh, d1, d2) = ((12)(34), e, e, e, . . . , e, e, (123), (321))

50
V. Peterson and A. Wootton
and
(a1, b1, a2, b2, . . . , ah, bh, d1, d2, d3) = ((12)(34), e, e, e, . . . , e, e, (123), (123), (123))
respectively. If Q ≥4, we deﬁne a generating vector by ai = bi = e for all i,
d1 = (234), d2 = (432), and for Q even, we set d2i−1 = (123) and d2i = (321)
for 3 ≤i ≤Q/2, and for Q odd, we set d3 = (123), d4 = (123), d5 = (123) and
d2i = (123) and d2i+1 = (321) for 3 ≤i ≤(Q −1)/2.
Finally suppose that P = 1. By our arguments above, we may again suppose
Q ≥2. Now, if Q = 2, we must have h ≥1 in which case we can deﬁne a generating
vector by
(a1, b1, a2, . . . , ah, bh, c1, d1, d2) = (e, e, e, . . . , e, e, (12)(34)), (123), (234)).
If Q = 3, we can deﬁne a generating vector by replacing the element d1 = (123)
above with the pair (321), (321). This has the effect of leaving the product ﬁxed
while adding an element of order 3:
(a1, b1, . . . , ah, bh, c1, d1, d2, d3) = (e, e, . . . , e, e, (12)(34), (321), (321), (234)).
If Q ≥4, we deﬁne a generating vector by ai = bi = e for all i, c1 = (12)(34),
d1 = (123), d2 = (234), and for Q even, we set d2i−1 = (123) and d2i = (321)
for 3 ≤i ≤Q/2, and for Q odd, we set d3 = (123), d4 = (123), d5 = (123) and
d2i = (123) and d2i+1 = (321) for 3 ≤i ≤(Q −1)/2.
⊓⊔
For a ﬁxed genus g, the Riemann-Hurwitz formula can be used to ﬁnd all the
possible signatures with which A4 might act on a surface of genus g. Theorem 2
now allows us to determine for which of those signatures there is an action. We
illustrate.
Example 13. We determine all signatures for which there exists an A4-action on
a surface of genus 16. Any such signature (h; 2P, 3Q) must satisfy the Riemann-
Hurwitz formula which simpliﬁes as follows
27 = 12h + 3P + 4Q.
Since h, P, Q ≥0 are integers, we can determine all possible solutions to this
equation. We get the following list of potential signatures: (0; 21, 36), (0; 25, 33),
(0; 27), (1; 21, 33), (1; 25), (2; 21). Applying Theorem 2, we see that all of these
signatures correspond to an A4-action on a surface of genus 16 except the signature
(0; 27).
Exercise 6. Determine all signatures for which there exists an A4-action on a
surface of genus 35.

Embeddable and Non-embeddable Group Actions on Surfaces
51
4
Embeddable A4-Actions
Theorem 2 provides necessary and sufﬁcient conditions for the existence of any
A4-action on a compact Riemann surface S in terms of the signature of the action.
Though the mathematics underlying this result is quite deep, only a basic under-
standing of ﬁnite group theory is required to apply the result. Accordingly, many
problems in the ﬁeld are made accessible to researchers with diverse backgrounds
and, in particular, are amenable to student investigations. To illustrate, we now apply
this result (together with a few additional results) to address the embeddability of
A4-actions. Speciﬁcally, we shall do the following: characterize exactly when a
signature S is the signature of an embedded action of A4 on some compact Riemann
surface.
4.1
Necessary and Sufﬁcient Conditions for Embeddability of A4
In work by Thomas Tucker [22], conditions are given identifying all possible genera
for surfaces admitting embeddable ﬁnite group actions, followed by constructions
for realizing these; in the case of A4, the construction involves drilling holes in
or attaching various gadgets to an inﬂated tetrahedron. As it turns out, although not
mentioned in [22], Tucker’s methods allow one to classify all possible signatures for
embedded actions, not just genera. (The distinction is that a single genus may give
rise to multiple signatures.) We will now exemplify Tucker’s methods and make the
construction in [22] explicit in the context of classifying all signatures of embedded
A4-actions. While the constructions in [22] are simpler than the ones that follow
here, we note that the goals are different: we describe signatures, rather than just
genera, which requires some technicalities that Tucker is able to avoid.
Necessary conditions for a signature to correspond to an embedded A4-action
can be deduced from [22], wherein Tucker characterizes the possible number
(and parity) of branch points for an action depending on whether the origin
lies inside or outside of the surface. In order to prove sufﬁciency, we present a
schema for constructing group actions for each possible signature satisfying the
appropriate conditions. Speciﬁcally, we ﬁrst introduce eight different base cases—
embedded surfaces on which A4 acts as a group of rigid symmetries—and then
describe three different operations that can be applied to these base cases (and
their corresponding signatures) to construct new embedded surfaces on which A4
acts (with new signatures). Four of the base cases are the surfaces T(0;2,2,3,3),
T(0;2,2,2,3,3), T(0;2,3,3,3,3), and T(0;3,3,3,3) constructed in Examples 2, 3, 4, and 5,
respectively and are reproduced in Figure 10 for reference.
The remaining four base cases are built by adding genus to the surfaces
T(0;2,2,3,3), T(0;2,2,2,3,3) in such a way as to preserve symmetries of the surface.
For brevity, Table 1 summarizes all the relevant information we need regarding
these latter four cases: it provides notation for each new case, the genus of the
corresponding surface (which is calculated by summing any holes added during
construction from Examples 2 and 3), a brief description of how the case is
constructed from previous cases, and a visual representation of each.

52
V. Peterson and A. Wootton
Fig. 10
From left to right: the inﬂated tetrahedron, also known as T(0;2,2,3,3), the tetrahedron
with jack, T(0;2,2,2,3,3), the tetrahedron with tripods, T(0;2,3,3,3,3), and the doubled tetrahedron
with drilled holes, T(0;3,3,3,3).
Table 1 Base cases for construction of embeddable A4-actions.
Notation
Genus
Construction
Picture
T(1;2,2)
7
Drill holes through all vertices of
T(0;2,2,3,3) so every axis of rotation
through a vertex no longer intersects
the surface
T(1;3,3)
9
Drill holes through all midpoints of
edges of T(0;2,2,3,3) so every axis of
rotation through midpoints of edges
no longer intersects the surface
T(1;2,3,3)
12
Drill holes through all midpoints of
edges of T(0;2,2,2,3,3) so every axis
of rotation through midpoints of
edges now intersects the surface only
through the central “jack”
T(2;−)
13
Drill holes through all midpoints of
edges and vertices of T(0;2,2,3,3) so
no axis of rotation intersects the
surface
Next we introduce the operations we will use to build group actions with
speciﬁed signatures. We describe the geometric construction for each of these
operations as well as the algebraic implications for the signature.

Embeddable and Non-embeddable Group Actions on Surfaces
53
Fig. 11 Operations for building new surfaces that admit embedded A4-actions. Depicted from left
to right are the handle operator H 3, the tripod operator T 2, and the generalized handle operator
G 2.
Handle Operator, H
Starting with any base case B and n ≥1 we deﬁne the
handle operator of order n, denoted H n, as follows. Let Mn denote n conjoined
handles (i.e., an n-holed torus with one end removed to reveal a pair of pant legs), as
pictured in Figure 11 (left). Attach a copy of Mn to each edge of B, centered about
the midpoint, in such a way that any element of A4 that switches two midpoints also
maps copies of Mn onto each other identically and any rotation about the midpoint of
an edge rotates the multi-handle onto itself. By construction, A4 acts on this surface,
though the genus of H n(B) is larger than the genus of B by 6n (since we have
added a genus n handle at each of 6 edges). We have also added 2n branch points
of order 2, orbit representatives being the intersection points of a speciﬁc Mn with
the axis of rotation passing through the midpoint of the corresponding edge of B.
Since the number of branch points of order 3 has not changed, we have all numerical
information needed to apply the Riemann–Hurwitz formula. One calculates that the
quotient genus of H n(B)/A4 is the same as the genus of B/A4. Therefore, given
the signature S of any base case, the handle operator H n constructs a surface on
which A4 acts with signature R obtained by appending 2n 2’s to the signature S . By
abuse of notation, we let H n also denote the corresponding operator on signatures,
i.e., H n(S ) = R.
Tripod Operator, T
Starting with any base case B, we deﬁne the tripod operator
of order n for n ≥1, denoted T n, as follows. Let Pn denote n conjoined tripods, as
in Figure 11 (center), which depicts P2. Attach a copy of Pn at every vertex of B in
such a way that any element of A4 that switches two vertices maps the corresponding
tripods onto each other and any rotation through a vertex rotates each Pn onto itself.
By construction, A4 acts on this new surface, though T n increases the genus of B
by 8n (a single tripod P1 adds genus two at each vertex, so Pn adds genus 2n at each
vertex). We have also added 2n branch points of order 3, orbit representatives being
the intersection points of a speciﬁc attached tripod with the axis of rotation passing
through the corresponding vertex of B. Since the number of branch points of order
2 has not changed, we again apply the Riemann–Hurwitz formula and ﬁnd that the
genus of the quotient T n(B)/A4 is the same as the genus of B/A4. Therefore,

54
V. Peterson and A. Wootton
given any base case B and corresponding signature S , the tripod operator T n
constructs an embedded surface on which A4 acts with signature R obtained by
appending 2n 3’s to the signature S . As before, we write T n(S ) = R.
Generalized Handle Operator, G Our last operation generalizes the ﬁrst. Starting
with any of the base cases B, we again use copies of Mn (n conjoined handles)
and deﬁne the generalized handle operator of order n for n ≥1, denoted G n, as
follows: we attach twelve copies of Mn to B positioned so they do not intersect any
of the axes of rotation of A4 and such that any element of A4 identically maps the
copies of Mn onto each other as in Figure 11 (right). By construction, A4 still acts
on this surface, but G n has increased the genus of B by 12n (since we have added
n handles at each of 12 described points). Since no copy of Mn intersects an axis of
rotation for A4, no additional branch points have been added and we can again apply
the Riemann–Hurwitz formula to ﬁnd that the genus of G n(B)/A4 is h + 1 where
h is the genus of B/A4. Therefore, given any base case B and its corresponding
signature S , the generalized handle operator G n constructs an embedded surface on
which A4 acts with signature R obtained by adding 1 to the genus in the signature
S . As before we write G n(S ) = R.
Remark 1. These three operators can be applied repeatedly to base cases and hence
can be applied to surfaces constructed from the base cases in the ways described
above. Moreover, these operations are independent in the sense that changing the
order of composition of operations does not change the resulting surface. The
schema we shall use to construct surfaces on which A4 acts will cite this fact.
Notationally, we represent consecutive application of operations using standard
function composition notation.
Before we provide sufﬁcient conditions, we consider an explicit example to
illustrate the basic argument at hand.
Example 14. Consider the surface T(2;−) (pictured last in Table 1). If we apply each
of the operators H , T , and G consecutively to this surface, we obtain a new surface
G (T (H (T(2;−)))) obtained by adding a handle at each midpoint of each edge, a
tripod at each vertex, and an additional twelve handles, each straddling the point
halfway between a vertex and the midpoint of an edge. Adding all these additional
handles to the original genus of T(2;−), we see that the genus of G (T (H (T(2;−))))
is 13 + 6 + 8 + 12 = 39. The addition of branch points and genus to the quotient
means that A4 acts on this surface with signature (3; 2, 2, 3, 3).
We are now ready to provide necessary conditions on the signature of an
embeddable A4 group action.
Theorem 3. There is an embedded A4-action on a compact Riemann surface of
some genus g ≥2 with signature (h; 2P, 3Q) satisfying Theorem 2 if and only if:
1. Q is even; and,
2. if P is odd, then Q ≥2.

Embeddable and Non-embeddable Group Actions on Surfaces
55
Proof. As noted above, necessity follows from [22]. To show sufﬁciency, we
construct an embedded surface on which A4 acts for each allowable signature. We
proceed by cases depending upon the parity of P and the value of h.
First suppose that P is even. Then for any h
≥
2 and any P, A4 acts
as an embedded group on the surface G h−2(H
P
2 (T
Q
2 (T(2;−)))) with signature
(h; 2P, 3Q). If h = 1 and P ̸= 0, then A4 acts as an embedded group on the surface
H
P−2
2 (T
Q
2 (T(1;2,2))) with signature (1; 2P, 3Q), and if h = 1 and Q ̸= 0, then
A4 acts as an embedded group on the surface H
P
2 (T
Q−2
2 (T(1;3,3))) with signature
(1; 2P, 3Q). Note that when h = 1, we cannot have both P = 0 and Q = 0, as this
signature was excluded by Theorem 2. For h = 0, we ﬁrst note that by Theorem 2,
we cannot have Q = 0. Now if P > 0, then the surface H
P−2
2 (T
Q−2
2 (T(0;2,2,3,3)))
is an embedded surface on which A4 acts with signature (0; 2P, 3Q). Finally, if
P = 0, then we must have Q ≥4, and in this case, the surface T
Q−4
2 (T(0;3,3,3,3))
is an embedded surface on which A4 acts with signature (0; 3Q).
Now suppose that P is odd. By Theorem 2 we know that Q > 0. Now if h ≥1
then A4 acts as an embedded group on the surface G h−1(H
P−1
2 (T
Q−2
2 (T(1;2,3,3))))
with signature (1; 2P, 3Q). If h = 0, to obtain a valid signature, we must have either
Q ≥4 or P ≥3. If Q ≥4, then A4 acts as an embedded group on the surface
H
P−1
2 (T
Q−4
2 (T(0;2,3,3,3,3))) with signature (0; 2P, 3Q) and if P ≥3, then A4 acts
as an embedded group on the surface H
P−3
2 (T
Q−2
2 (T(0;2,2,2,3,3))) with signature
(0; 2P, 3Q).
Since this includes all possible signatures given in Theorem 3, it follows that
these conditions are also sufﬁcient conditions for the signature to correspond to an
embedded action of A4 on a classical surface S, thus completing the proof.
⊓⊔
We note that for a given signature satisfying Theorem 3, it is not necessary that
all actions producing that signature embed, only that there exists at least one action
that does embed. We ﬁnish with an example comparing signatures for embeddable
and non-embeddable actions in the case of a ﬁxed genus.
Example 15. In Example 13 we showed that the signatures (0; 21, 36), (0; 25, 33),
(1; 21, 33), (1; 25), (2; 21) are all signatures for the action of A4 on a surface of
genus 16. Applying Theorem 3, we see that only one of these signatures, (0; 21, 36),
is a valid signature for an embedded A4-action.
Exercise 7. Use Theorem 3 to determine which (if any) of the allowable signatures
you generated in Exercise 6 actually correspond to embedded actions.
Challenge Problem 1. Use Theorem 3 to recreate Tucker’s result from
[22].
Namely, that embeddable A4-actions exist only on surfaces of genus g = 6m+8n+k,
where m, n are positive integers and k = 0, 3, 5, 7.

56
V. Peterson and A. Wootton
5
Suggested Projects
There are a number of interesting projects for which our exposition lays much of
the necessary groundwork. For example, in Theorem 2 we provided necessary and
sufﬁcient conditions for a signature to be a signature for an action of A4 on some
compact Riemann surface (not necessarily embedded). Similar results have been
developed for other classes of groups, such as cyclic groups (see [8]). Recreating
such results would substantially aid in developing the tools and techniques required
to work effectively with signatures of group actions.
Challenge Problem 2. Provide necessary and sufﬁcient conditions for a signature
to be associated with the action of a cyclic group Zn of order n on some compact
Riemann surface.
There are, of course, many families of groups for which such conditions are not
known, and this same question could be asked of any such family.
Research Project 1.
Fix a speciﬁc group G, or a family of groups. What
signatures can be realized as resulting from an action of this group (or family
of groups) on a Riemann surface?
Additionally, one could examine questions of embeddability of actions of the
other ﬁnite subgroups of the isometry group of R3. Much is already known about
the embeddability of cyclic and dihedral groups [13,20] but one could explore these
questions for the symmetric group, S4, or the alternating group, A5.
Research Project 2.
Starting with its action on a cube (or octahedron),
describe all embeddable signatures of S4.
Research Project 3.
Starting with its action on a dodecahedron (or icosahe-
dron), describe all embeddable signatures of A5.
Colloquially speaking, Theorems 2 and 3 taken together suggest that about half
of all possible signatures for A4-actions are actually embedabble (modulo a few
genus h = 0 cases). In addition, in [22], Tucker determined the genus spectrum of
embeddable A4-actions, or the sequence of all possible genera admitting embedded

Embeddable and Non-embeddable Group Actions on Surfaces
57
A4-actions, but did not consider how many allowable signatures embed for a given
genus. It would be interesting to explore this further. For example, we found earlier
in Example 13 that there are ﬁve allowable signatures corresponding to A4-actions
on a surface of genus 16, but only one of these signatures corresponds to an
embedded action. One might want to ask, do approximately half of the allowable
signatures in each genus embed, on average? Or does the ratio of embedded to non-
embedded actions depend a great deal on the speciﬁc genus one begins with?
Research Project 4.
Examine how the signatures for embedded A4-actions
are distributed across the genus spectrum of embeddable A4-actions.
Of course, a similar investigation can be undertaken for S4 and A5 upon the
completion of Projects 1 and 2.
Research Project 5.
Examine how the signatures for embedded S4 and A5-
actions are distributed across the genus spectrum of embeddable S4 and A5-
actions
Considered more generally, the genus spectrum of an arbitrary group G is the set
of all genera for which there exists a G-action on some surface of that genus. In [11],
it is shown that eventually (i.e., as the genus grows large), the genus spectrum for a
group G can be completely predicted. For relatively small genus though, it is more
difﬁcult, and determining the genus spectrum up to the point where it stabilizes is
still of signiﬁcant interest. Currently there are only a few classes of groups for which
the entire genus spectrum is known, see for example [12] for cyclic groups of prime
power order, [21] for semi-direct products of cyclic primes groups and [15] for other
certain special groups of prime power order. It would be a very useful exercise in
applying the techniques we have described to recreate some of these initial results
on genus spectra.
Challenge Problem 3. Determine the genus spectrum of the cyclic group Z2.
Challenge Problem 4. For a ﬁxed prime p, determine the genus spectrum of the
cyclic group Zp.
Using the techniques developed in these challenge problems, it might be possible
to take a speciﬁc ﬁxed group for which the genus spectrum is not known and study
in detail its genus spectrum.

58
V. Peterson and A. Wootton
Research Project 6.
For a given group G, what is the genus spectrum for G
(i.e., the possible genera of surfaces that admit a G-action)?
Additionally, though we have restricted our focus to conformal automorphisms
here (e.g., rigid rotations of a tetrahedron), reﬂections also give rise to symmetries,
though in this case they are anticonformal. For a more ambitious project, it would
be interesting to pursue similar questions about embeddability for groups admitting
both conformal and anticonformal automorphisms. A starting point for such a
project would be a closer look at the ﬁnite transformation groups of R3, for which
we suggest [16] as a good starting reference. Some progress has been made toward
this goal for cyclic and dihedral groups (see [5,13,14,18–20]) and we suggest using
these articles as a starting point for such an analysis.
Research Project 7.
Examine the embeddability questions above in the
context of both conformal and anticonformal symmetries.
References
1. Arbo, M., Benkowski, K., Coate, B., Nordstrom, H., Peterson, C., Wootton, A.: The genus level
of a group. Involve 2(3), 323–340 (2009)
2. Benim, R.: Classiﬁcation of quasiplatonic Abelian groups and signatures. Rose-Hulman
Undergrad. Math. J. 9(1), Article 1 (2008)
3. Broughton, S.A.: Classifying ﬁnite group actions on surfaces of low genus. J. Pure Appl.
Algebra 69, 233–270 (1990)
4. Broughton, S.A., Haney, D.M., McKeough, L.T., Mayﬁeld, B.S.: Divisible tilings in the
hyperbolic plane. New York J. Math. 6, 237–283 (2000)
5. Costa, A.F.: Embeddable anticonformal automorphisms of Riemann surfaces. Comment. Math.
Helv. 72(2), 203–215 (1997)
6. Garsia, A.M.: An embedding of closed Riemann surfaces in euclidean space. Comment. Math.
Helv. 35, 93–110 (1961)
7. Ghrist, R., Peterson, V.: The geometry and topology of reconﬁguration. Adv. Appl. Math.
38(3), 302–323 (2007)
8. Harvey, W.J.: Cyclic groups of automorphisms of a compact Riemann surface. Q. J. Math.
Oxford Ser. (2) 17, 86–97 (1966)
9. Hillman, A.P., Alexanderson, G.L.: Abstract Algebra: A First Undergraduate Course. Wave-
land Press, Inc., Prospect Heights, IL (1994)
10. Hurwitz, A.: Ueber algebraische Gebilde mit eindeutigen Transformationen in sich. Math. Ann.
41(3), 403–442 (1892)
11. Kulkarni, R.S.: Symmetries of surfaces. Topology 26(2), 195–203 (1987)
12. Kulkarni, R.S., Maclachlan, C.: Cyclic p-groups of symmetries of surfaces. Glasgow Math J.
33, 213–221 (1991)
13. Lin, F.L.: Embeddable dihedral groups of Riemann surfaces. Chinese J. Math. 7(2), 133–152
(1979)

Embeddable and Non-embeddable Group Actions on Surfaces
59
14. Lin, F.L.: Conformal symmetric embeddings of compact Riemann surfaces. Tamkang J. Math.
10(1), 97–103 (1979)
15. Maclachlan, C., Talu, Y.: p-groups of symmetries of surfaces. Michigan Math. J. 45, 315–332
(1998)
16. Martin, G.: Transformation Geometry: An Introduction to Symmetry. Undergraduate Texts in
Mathematics. Springer, New York (1982)
17. Miranda, R.: Algebraic Curves and Riemann Surfaces. Graduate Studies in Mathematics,
vol. 5. American Mathematical Society, Providence, RI (1995)
18. Rüedy, R.A.: Deformations of embedded Riemann surfaces. In: Advances in the Theory of
Riemann Surfaces (Proceedings of Conference, Stony Brook, NY, 1969) Annal of Mathematics
Studies, vol. 66, pp. 385–392. Princeton University Press, Princeton, NJ (1969)
19. Rüedy, R.A.: Embeddings of open Riemann surfaces. Comm. Math. Helv. 46, 214–225 (1971)
20. Rüedy, R.A.: Symmetric embeddings of Riemann surfaces. In: Discontinuous Groups and
Riemann Surfaces (Proceedings Conference, University of Maryland, College Park, MD,
1973), vol. 79, pp. 409–418. Annals of Mathematical Studies. Princeton University Press,
Princeton, NJ (1974)
21. Sullivan, C.O., Weaver, A.: A Diophantine Frobenius problem related to Riemann surfaces.
Glasgow Math J. 53, 501–522 (2011)
22. Tucker, T.W.: Two notes on maps and surface symmetry. In: Rigidity and Symmetry. Fields
Institute Communications, vol. 70, pp. 345–355. Springer, New York (2014)
23. Vinroot, C.R.: Symmetry and tiling groups for genus 4 and 5. Rose-Hulman Undergrad. Math.
J. 1(1), Article 5 (2000)
24. Zarrow, R.: Anticonformal automorphisms of compact Riemann surfaces. Proc. Am. Math.
Soc. 54, 162–164 (1976)

Tile Invariants for Tackling Tiling Questions
Michael P. Hitchman
Suggested Prerequisites. Number theory, Group theory.
1
Prologue
We begin with a question: For which values of n do translations of the two tiles in
Figure 1 tile the staircase region Sn with n steps?
The staircase problem, which appeared in a 1990 paper by Conway and Lagarias
[5], exhibits all that we ﬁnd engaging about the mathematics of tiling: it is simple
to state, simple to investigate, yet challenging to answer. Moreover, the inventive
solution in [5], which makes use of combinatorial group theory and planar topology,
sparked considerable research in the mathematics of tiling, as we shall see.
We can begin investigating the staircase problem by trying to tile some small
staircase regions. In short order, one can ﬁnd tilings of S2, S9, S11, and S12. Do it! A
tiling of S9 appears in Figure 1. On the other hand, regions like S7 can be eliminated
immediately for having the wrong area: since each tile has area 3, any tileable region
must have area divisible by 3. Other regions, such as S8, do not appear to be tileable,
even though they meet this area requirement.
We might then write down some generalities. The area of Sn is n(n + 1)/2 which
is divisible by 3 if and only if n ≡0 or 2 (mod 3); that is, n has remainder 0 or
2 when divided by 3. So Sn is not tileable if n ≡1 (mod 3). We also see that the
2 × 3 and 3 × 2 rectangles are tileable, from which we can work out that an n × 12
rectangle is also tileable if n ≡0 or 2 (mod 3), because such rectangles can be built
from these 2 × 3 and 3 × 2 bricks. From this fact we can show that if Sn is tileable,
M.P. Hitchman ()
Linﬁeld College, 900 SE Baker St., McMinnville, OR 97128-6894, USA
e-mail: mhitchm@linﬁeld.edu
© Springer International Publishing AG 2017
A. Wootton et al. (eds.), A Primer for Undergraduate Research, Foundations for
Undergraduate Research in Mathematics, DOI 10.1007/978-3-319-66065-3_3
61

62
M.P. Hitchman
S3
S4
S9
Fig. 1 Copies of these two L-shaped tiles can tile the staircase region S9.
Fig. 2 Partitioning Sn+12.
Sn
n×12
S12
then so is Sn+12, because we can partition Sn+12 into three tileable subregions as in
Figure 2. Take the time to check these facts.
With tilings of S2, S9, S11, and S12 in hand, along with the fact that if Sn is tileable
then so is Sn+12, we have a proof that if n ≡0, 2, 9, or 11 (mod 12) then Sn is tileable
by the L-shaped tiles in Figure 1.
Conway and Lagarias prove that the converse is also true by ruling out tilings
in the other cases. We know that if n ≡1, 4, 7, or 10 (mod 12) then Sn is not
tileable because its area is not divisible by 3. The nut of the problem appears to
be when n ≡3, 5, 6, or 8 (mod 12). We can imagine providing a brute-force proof
that S8, say, is not tileable, one that tracks each attempt at a tiling until it runs into
some impossibility due to the shape of the remaining untiled squares. But how about
S1208? A brute-force nonexistence proof is out of the question. What we require is
another tile invariant, some necessary feature of any tiling of Sn that can be used to
rule out tilings in these remaining cases.
Here is where Conway and Lagarias’ approach to this problem becomes so
inventive. As Figure 3 shows, if we expand the tile set to include the straight tiles t1
and t4, then each of the regions S3, S5, S6 and S8 can be tiled. To match notation we
develop later, we call this expanded tile set T3 (the subscript refers to the number of
squares in each tile), and the original tiles in the staircase problem are t2 and t3.
Conway and Lagarias then introduce combinatorial group-theoretic techniques
to this scene, which we present in Section 3.2. For now we state their key result:
The Conway/Lagarias Invariant In any tiling of a staircase region Sn by the set
T3, the number of t2 tiles used minus the number of t3 tiles used is constant.

Tile Invariants for Tackling Tiling Questions
63
t1
t2
t3
t4
S3
S5
S6
S8
Fig. 3 The larger tile set T3 tiles S3, S5, S6 and S8.
For instance, the tiling of S8 in Figure 3 contains 7 copies of t2 and 4 copies of t3
giving us a difference of 3. The Conway/Lagarias invariant says any other tiling of
S8 by T3 must produce this difference of 3. Similarly, any tiling of S6 would produce
the difference 2, because this is the difference in the tiling of S6 in Figure 3.
We can use this invariant to prove S8 is not tileable by {t2, t3}. If this set did
tile S8 it would do so with 12 tiles, giving us a2 + a3 = 12, where ai equals the
number of ti tiles used in the tiling. On the other hand, we know a2 −a3 = 3, thanks
to the Conway/Lagarias invariant. But no integer solutions exist to this system of
equations, so no tiling of S8 by {t2, t3} exists.
This argument can be generalized to prove nonexistence of a tiling for all n ≡
3, 5, 6, 8 (mod 12). In fact, we will provide a quick test in Section 4 (with Lemma 1)
which will allow us to see immediately from the tilings in Figure 3 that Sn is not
tileable by {t2, t3} in all these cases.
Although the Conway/Lagarias invariant was discovered in the service of the
staircase problem, considerable effort has been expended on the question of ﬁnding
tile invariants associated to a tile set and a family of regions, for their own sake,
independent of tileability questions. We discuss methods for ﬁnding tile invariants
in Section 3 and reconnect these invariants to tileability questions in Section 4. In
Section 3.4 we discuss the tile counting group, an algebraic structure that encodes
information about tile invariants, ﬁrst considered in [15]. In Section 5 we touch
brieﬂy on questions of enumeration, before offering some concluding remarks.
Project ideas, questions, exercises and challenge problems appear throughout the
paper to engage the reader in the mathematics of tiling.
2
Tiling Basics
All the tile sets and regions considered here are polyominoes. A polyomino is the
result of joining one or more equal squares edge to edge. The area of a region R,
denoted |R|, equals its number of squares. Dominoes, trominoes, tetrominoes, and
pentominoes have area two, three, four, and ﬁve, respectively. Figure 4 shows three
8-ominoes.
We assume our polyominoes live in the integer lattice, the plane tiled by unit
squares whose corners have integer coordinates. We let [a × b] denote a rectangle of
height a and width b in the lattice. To identify a particular cell in the lattice, let sx,y
denote the cell whose lower left corner is the point (x, y).

64
M.P. Hitchman
Fig. 4 Three 8-ominoes.
Fig. 5 The set T4 of ribbon
tile tetrominoes, labelled by
their binary signatures.
000
001
010
011
100
101
110
111
Fig. 6 L-trominoes do not
tile this deﬁcient rectangle.
t1
t2
t3
t4
A ribbon tile of area n is a polyomino consisting of n squares laid out in a path,
such that from an initial square, each step either goes up or to the right. We let Tn
denote the set of area n ribbon tiles, and note that Tn has 2n−1 elements. The tiles in
Tn can be indexed by binary signatures of length n−1, where each 0 represents a step
to the right, and 1 represents a step up. The set T4 of ribbon tile tetrominoes appears
in Figure 5 along with each tile’s binary signature. The set T3 is the (expanded) set
of tiles considered in the staircase problem.
Suppose T is a tile set. We say a region R is tileable by T, or T tiles R, if R can
be covered without gaps or overlaps by a collection of translations of some (or all)
of the tiles in T. We note that rotations or reﬂections of the tiles are not allowed in
any of the problems in this article. (If we want to allow for rotations or reﬂections,
we ought to put these shapes in the tile set.)
Example 1. A deﬁcient rectangle is a rectangle with a single cell missing. In [7]
Golomb showed that the set of four L-trominoes in Figure 6 can tile any deﬁcient
[2k × 2k], where k ≥1. This result has an elegant proof by induction, which is now
commonly found in texts teaching this proof technique.
Certainly not every deﬁcient [a × b] is tileable by L-trominoes. For starters, they
don’t all have area divisible by 3. This area condition is not sufﬁcient, though, as the
deﬁcient [5 × 5] in Figure 6 has area 24 but isn’t tileable. Try covering the cell to
the left of the missing cell with an L-tromino, and you immediately run into trouble
in either the upper left or lower left corner. It turns out that L-trominoes do, in fact,
tile most deﬁcient rectangles of the right area. The details can be found in [2].

Tile Invariants for Tackling Tiling Questions
65
Roughly speaking, the tile invariants of the next section are meant to help us
prove nonexistence without making use of local geometric constraints of the region
itself as we did here.
Exercise 1. Show that for each k ≥1 the set of four L-trominoes in Figure 6 tiles
any deﬁcient [2k × 2k].
3
Tile Invariants
Suppose the tile set T = {t1, t2, . . . , tn} has n tiles, and that R denotes a family of
regions. There may be different ways in which T tiles a region R ∈R. Suppose
α represents one such tiling. We let ai(α) equal the number of times tile ti appears
in the tiling α. A linear combination of the ai(α) whose value depends only on the
region R and not the particular tiling of it, for all the regions in the family R, is
called a tile invariant with respect to the tile set T and the family of regions R, or
simply a tile invariant for T over R.
A tile invariant has the form
k1 · a1(α) + k2 · a2(α) + · · · + kn · an(α) = c(R)
where each ki is an integer and c(R) is the value of the linear combination that
depends only on R. Furthermore, the equality may be taken modulo m for some
positive integer m. Since tile invariants are independent of the particular tiling, we
often omit the α from their notational descriptions.
Three commonly studied families of regions are Rall, the set of all regions; Rsc,
the set of simply connected regions; and Rrect, the set of rectangular regions. Simply
connected regions in the lattice are those that are connected (in one piece) and have
connected boundary. Regions that enclose a hole such as the region in Figure 7 are
not simply connected. Note that Rrect ⊂Rsc ⊂Rall.
We emphasize that tile invariants depend on the tile set as well as the family
of regions. The Conway/Lagarias invariant for T3 applies to the family of simply
connected regions Rsc, but not Rall. Figure 7 shows two tilings of a non-simply
connected region that generate different values for the linear combination a2 −a3:
a2(α) −a3(α) = 2, while a2(β) −a3(β) = −1. We note that this example does not
rule out the possibility that a2 −a3 is constant modulo 3 over Rall. In fact this turns
out to hold, as we see in Example 3.
Example 2. The area invariant.
Fig. 7 The Conway/Lagarias
invariant doesn’t hold for T3
over all regions.
a
b

66
M.P. Hitchman
Fig. 8 Coloring the lattice
with horizontal strips of like
colors.
x
y
0
1
2
0
1
0
1
2
0
1
0
1
2
0
1
0
1
2
0
1
0
1
2
0
1
t1
→0
t3
→2
t2
→1
t4
→0
Suppose the area of each tiles in T = {t1, . . . , tn} is m. Then we have the
following tile invariant, called the area invariant: For each region R ∈Rall, if α
is a tiling of R,
a1(α) + a2(α) + · · · + an(α) = |R|
m .
3.1
Coloring Invariants
Imagine “coloring” each cell in the lattice with an integer. Any region R then has a
color sum, found by summing the numbers in the cells of R. Now suppose we color
the lattice in such a way that for each t ∈T, the color sum of t is independent of
where the tile t is placed in the lattice, perhaps modulo m for some m, (remembering
once more that we cannot rotate or reﬂect t before placing it in the lattice). Such a
coloring would determine a tile invariant for T over Rall.
Example 3. Color the lattice with horizontal strips of 0s, 1s and 2s as in Figure 8
(the cell sx,y in the lattice gets color y, modulo 3), and consider the tile set T3. The
color sum of t1, wherever it is placed, is equivalent to 0 (mod 3). Tile t2 has color
sum 1 (mod 3); t3 has color sum 2 (mod 3), and t4 has color sum 0 (mod 3).
Let c(R) represent the color sum of a region R tileable by T3. This value is
independent of any tiling of R, but given a tiling α, c(R) will be the sum of the
color sums of the tiles in α. It follows that 0a1 + a2 + 2a3 + 0a4 ≡c(R) (mod 3).
Since 2 ≡−1 (mod 3) we may express the tile invariant determined by this coloring
as
a2 −a3 ≡c(R) (mod 3).
Note that If we restrict to simply connected regions, this coloring tile invariant is an
immediate consequence of the celebrated Conway/Lagarias invariant.
Example 4. We establish three tile invariants with respect to the set L of 4 L-
trominoes in Figure 6 over Rall.

Tile Invariants for Tackling Tiling Questions
67
Fig. 9 Coloring cell sx,y with
x + y (mod 3).
x
y
0
1
2
0
1
1
2
0
1
2
2
0
1
2
0
0
1
2
0
1
1
2
0
1
2
t1
→2
t2
→0
t3
→0
t4
→1
a1 + a2 + a3 + a4 = |R|
3
(1)
a4 −a1 ≡c(R) (mod 3)
(2)
a1 + a2 + 2a3 + 2a4 ≡d(R) (mod 3)
(3)
The ﬁrst invariant is the area invariant. The second invariant follows from the
coloring in which sx,y gets color x + y, taken modulo 3 as pictured in Figure 9. The
coloring in Figure 8 yields the third invariant a1 + a2 + 2a3 + 2a4 ≡c (mod 3) with
respect to L.
We may form new tile invariants by combining these three in various ways. For
instance, we obtain a2 −a3 ≡c∗(R) (mod 3) by adding invariants (2) and (3). A
natural question to ask is whether there are tile invariants in this setting that are
not consequences of the three above. In Section 3.4 we investigate the question of
determining all tile invariants when we consider the tile counting group.
3.2
Boundary Word Invariants
In this section we introduce the boundary word approach ﬁrst used by Conway and
Lagarias in [5]. We assume throughout the section that all tiles and regions are
simply connected.
Begin by orienting and labelling the edges in the lattice as follows: horizontal
edges are labelled x and oriented to the right; vertical edges are labelled y
and oriented up. Any path in the lattice determines a word, using the alphabet
{x, y, x−1, y−1}, by writing the letters you encounter as you proceed along the path,
following the convention that if you traverse an edge against its orientation, you
record the inverse of the label. These words determine elements of the free group F
on two generators x and y.
We associate a boundary word w(R) to any simply connected region R by
traversing a clockwise path around the boundary of R from a chosen basepoint.
In this way, we give each tile t in a tile set T a tile boundary word w(t). For instance,
the tile boundary words of the tiles in T3 (we choose bottom left-most basepoints)
are as indicated in Figure 10.

68
M.P. Hitchman
w(t1) = yx3y−1x−3
w(t2) = (yx)2y−2x−2
w(t3) = y2x2(y−1x−1)2
w(t4) = y3xy−3x−1
Fig. 10 Boundary words of the tiles in T3.
y
x
y
x
y
x
y
x
x
y
x
y
x
y
y
y
y
x
x
x
x
x
y
y
y
x
y
x
x
y
w(t2) = (yx)2y−2x−2
y
x
y
x
x
y
w(t3) = y2x2(y−1x−1)2
Fig. 11 The Cayley Graph of F/H.
Conway and Lagarias then discovered an alternate grid of streets labelled x and
y along which words in F trace new paths. Instead of streets forming rectangular
blocks as in the integer lattice, the blocks in the new setting are made of hexagons
and triangles, a portion of which is shown in Figure 11. The key feature of this
new grid is that the tile boundary words from Figure 10 still trace closed paths. The
images of w(t2) and w(t3) appear in Figure 11. Check that the images of w(t1) and
w(t4) are also closed paths.
We remark that the diagram in Figure 11 is called the Cayley graph of F/H,
where H is the normal subgroup of F consisting of all words that determine closed
paths in this diagram. (H is the normal subgroup of F generated by the words x3, y3,
and (yx)3.) Vertices in the graph correspond to elements in F/H, and a directed edge
labelled x runs from each element g ∈F/H to the element xg. Similarly, a directed
edge labelled y runs from each g ∈F/H to yg. We do not need this group-theoretic
interpretation of the diagram in Figure 11 in what follows, but it helps to explain
how Conway and Lagarias found it: They wanted a quotient group F/H in which
the tile boundary words were all trivial. Several good texts, including [12], develop
the combinatorial group theory at play here.
Now comes a dash of planar topology. Conway and Lagarias deﬁne a group
homomorphism h : H →Z via the winding number h(w) that counts the number
of hexagons that the closed path w in H encloses in the counterclockwise direction
minus the number of hexagons it encloses in the clockwise direction. Note that
h(w(t2)) = +1 because the path in Figure 11 traced by w(t2) encloses one
hexagon in the counterclockwise direction, while w(t3) = −1. One can check that
h(w(t1)) = h(w(t4)) = 0.

Tile Invariants for Tackling Tiling Questions
69
Fig. 12 A bouquet of
balloons. Here w(R) =
w(t4)xyw(t3)(xy)−1
xw(t2)x−1x3w(t4)x−3.
w(R) = y3x4y−3x−4
1
xy
x
x3
If R is tileable by a tile set T, then it turns out that w(R) can be expressed as
a product of conjugates of tile boundary words. The idea is that a tiling of R by
T may be visualized as a bouquet of tile-shaped balloons whose strings run from
the basepoint of R to the tile basepoints, as suggested in Figure 12. The boundary
word of R will then be equivalent to the word recorded by following each string
in turn, going around the tile boundary, then returning along the string in order to
move to the next string. This fact is a result in combinatorial group theory called
van Kampen’s Lemma.
So, if R is tileable by T, then w(R) is in H and we can determine the integer
h(w(R)) which is independent of any tiling of R. It counts the net number of
hexagons enclosed by w(R). On the other hand, if we have a tiling α of R having m
tiles, then
w(R) =
m

i=1
wiriw−1
i
where each wi ∈F and each ri is a tile boundary word. Because h is a group
homomorphism h(w(R)) will equal m
i=1 h(ri). Each copy of t2 in α will add 1
to this sum, and each copy of t3 in α will subtract 1 from the sum, while t1 and t4
contribute zero. So we have arrived at the Conway/Lagarias invariant:
a2(α) −a3(α) = h(w(R))
is constant for any tiling α of a given simply connected region R. In general, this
process gives us the following group-theoretic necessary condition for tileability.
Theorem 1. Suppose H is a normal subgroup of F containing the tile boundary
words of the tile set T, for some choice of tile basepoints. If a simply connected
region R is tileable by T then w(R) ∈H.
Conway and Lagarias’ choice of H is particular to the tile set T3, but the general
approach of using boundary words to generate invariants has been applied with
great success elsewhere. For instance, in a very well-written and accessible paper
[16], Propp applies a boundary word approach to the solution of a tiling question
involving skew tetrominoes and regions called aztec diamonds. Propp found his tile
invariant by ﬁrst considering a larger tile set, just as Conway and Lagarias had done
to solve the staircase problem.

70
M.P. Hitchman
In [10], Korn uses boundary words to ﬁnd many tile invariants, including
the invariant that for any simply connected region, a1 −a2 is constant for the
set {t1, t2, t7, t8) of the tetrominoes in Figure 24 (see [10, Theorem 8.3]). Other
boundary word arguments can be found in [13,14,18], and [20]. We also note that
the Conway/Lagarias invariant can be derived without the use of winding numbers
by appealing instead to a topological property of a 2-complex corresponding to the
quotient group F/H in their construction. This approach is worked out in [8].
Example 5. In [13] Pak and Moore use boundary words to generate tile invariants
for each tile set Tn over Rsc. They orient and label the edges of the lattice with
elements in the ﬁnite cyclic group Zn and then associate to each element a vector in
the complex plane in such a way that the boundary word of each tile in Tn generates
a closed polygonal path in C whose edges are these vectors. They then use signed
areas enclosed by these paths to determine tile invariants. To state the result we need
a bit more notation. Recall, ribbon tiles in Tn have a binary signatures. Let tϵ denote
the tile with signature ϵ = ϵ1ϵ2 · · · ϵn−1, where each ϵi ∈{0, 1}, and let aϵ(α)
denote the number of times the tile tϵ ∈Tn appears in a tiling α.
Theorem 2 ([13, Theorem 1.2]). Suppose n ≥2. For each 1 ≤i < n/2 we have
the following tile invariants for the set Tn over Rsc:

ϵ:ϵi=0,ϵn−i=1
aϵ(α) −

ϵ:ϵi=1,ϵn−i=0
aϵ(α) = ci(R).
Furthermore, if n is even, we have the following tile invariant:

ϵ:ϵn/2=1
aϵ(α) ≡c∗(R) (mod 2).
When n = 2, this theorem gives one invariant: In any tiling of R by dominoes, the
number of vertical dominoes is constant, modulo 2. When n = 3, the theorem gives
the Conway/Lagarias invariant. When n = 4, the theorem provides two invariants,
and these were proved in [14]. Moreover, we note that the invariant in the n = 2
case can be derived from the coloring sx,y →y (mod 2), but for n ≥3, the invariants
in this theorem cannot be found by coloring arguments [15, Theorem 1.8].
3.3
Invariants from Local Connectivity
Suppose R is a region tileable by T, and L is a ﬁnite set of small regions, each of
which can be tiled in at least two ways. A local move in L consists of replacing
one tiling of a region L in L (that appears within a tiling of R) with another tiling
of L. That is, a local move changes one tiling of R to another by removing a few
tiles and then tiling the now uncovered subregion in a different way. We say R has
local connectivity with respect to L and T if any tiling of R can be converted to any

Tile Invariants for Tackling Tiling Questions
71
Fig. 13 Any two domino
tilings of a simply connected
region can be made to agree
by a ﬁnite number of 2-ﬂips.
2-ﬂip
a
b
Fig. 14 Rsc has a local move property with respect to T3 consisting of these six 2-ﬂips.
other by performing a ﬁnite number of local moves in L . Finally, we say there is a
local move property for T and R if there exists a set of local moves L such that all
regions R ∈R have local connectivity with respect to L and T.
For instance, consider the set T2 of dominoes and let L = {[2×2]}. This square
has two tilings: one by two horizontal dominoes, the other by two vertical dominoes,
as shown in Figure 13. A local move in L then consists of swapping two horizontal
dominoes that cover a 2 × 2 square with two vertical dominoes covering that same
square, or vice versa. We call such a move a 2-ﬂip. Thurston proved in [20] that any
two tilings of a simply connected region can be made to agree by a ﬁnite sequence
of 2-ﬂips. That is, there is a local move property for dominoes and simply connected
regions. The reader may check that the tiling α in Figure 13 can be converted to the
tiling β by performing three 2-ﬂips.
Example 6. In [19, Theorem 1.1] Shefﬁeld generalized Thurston’s result by proving
the remarkable theorem that for each set Tn of ribbon tiles the set Rsc has a local
move property with respect to a ﬁnite set of 2-ﬂips, thus resolving a conjecture
appearing in [13]. For instance, for T3, any simply connected region R has local
connectivity with respect to the set of six local moves in Figure 14.
With this theorem in hand, the Conway/Lagarias invariant over Rsc follows
immediately: Suppose α is a tiling of a region R. Let a2(α) −a3(α) = c(R). If
β is any other tiling of R then we can get to β from α by performing a ﬁnite number
of local moves. Since the tile count a2 −a3 doesn’t change in any of these six local
moves, it follows that a2(β) −a3(β) = a2(α) −a3(α). Thus, a2 −a3 = c(R) is
a tile invariant over Rsc. The other tile invariants in Theorem 2 also follow from
Shefﬁeld’s result, and with the following theorem, Shefﬁeld effectively solves every
tiling question of the form: “Does Tn tile a simply connected region R?”
Theorem 3 ([19, Theorem 1.4]). There is a linear-time (i.e., linear in the number
of squares of R) algorithm for determining whether there exists a tiling by Tn of a
simply connected region R, and producing such a tiling when one exists.

72
M.P. Hitchman
Fig. 15 Any two tilings of a rectangle by T-tetrominoes can be made to agree by a ﬁnite number
of these two local moves.
Example 7. Korn proves in [10, Theorem 4.1] that the set of four T-tetrominoes has
a local move property for rectangular regions. In particular, any rectangle has local
connectivity with respect to the two local moves given in Figure 15.
The strongest statement one can hope to make about how any two tilings of a
given region must be related is to establish local connectivity, should it exist. If we
have a set L of local moves, it is straightforward to write down tile invariants.
However, local connectivity can be difﬁcult to prove in general. Objects called
height functions have been a helpful tool in this endeavor. Height functions were
ﬁrst used by Thurston in [20], and were used by both Shefﬁeld and Korn in the
above examples. We do not pursue height functions here, but refer the interested
reader to [10,13,19], and [20] for introductions to this technique.
We mention one way to prove that a family of regions R does not have a local
move property for a given tile set. Any region in R that has exactly two tilings must
be a local move. So, if one can demonstrate an inﬁnite family of regions in R, each
having exactly two tilings, then a local move property does not hold for the family
R with the tile set.
Returning to Example 7, Korn uses this approach to show that there is not a local
move property for the set of T-tetrominoes and Rsc. In particular, with [10, Theorem
4.13] he provides an inﬁnite family of simply connected regions, each having
exactly two tilings by T-tetrominoes. Like tile invariants, the existence of a local
move property for a tile set depends on the family of regions under consideration.
3.4
The Tile Counting Group
Given a tile set T and a family or regions R, the tile counting group G(T, R) was
introduced in [15] as a way to record information about the tile invariants associated
with T and R. This group is a quotient of the free abelian group Zn, where n is the
size of the tile set. We think of elements of Zn as n-tuples of integers, and addition
is done coordinate-wise: if w = (a1, . . . , an) and v = (b1, . . . , bn) and k ∈Z,
then w + v = (a1 + b1, . . . , an + bn), and we let kw denote (ka1, . . . , kan). The tile
counting group is deﬁned as follows. To any tiling α of a region R ∈R we associate
a tile vector vα ∈Zn given by vα = (a1(α), a2(α), . . . , an(α)). Two tilings α and
β of a region R determine the difference vector vα −vβ. Let H denote the normal
subgroup of Zn generated by all possible difference vectors obtainable from our
family of regions R and the tile set T. The tile counting group is the quotient group
G(T, R) = Zn/H.

Tile Invariants for Tackling Tiling Questions
73
In this context a tile invariant may be viewed as a linear function f of the form
f(a1, a2, . . . , an) = n
i=1 kiai that maps elements of Zn into Z or Zm, the ﬁnite
cyclic group of order m, such that f(v) = 0 for each v ∈H.
Example 8. With respect to the tile set L of 4 L-trominoes and the family Rall we
have three tile invariants from Example 4. We show that these effectively supply all
the tile invariants in this setting by using them as coordinate functions in a map that
establishes the tile counting group.
Theorem 4. G(L, Rall) ≃Z × Z3 × Z3.
Proof. Consider the group homomorphism φ : Z3 →Z × Z3 × Z3 deﬁned by
φ(a1, a2, a3, a4) = (a1 + a2 + a3 + a4, a4 −a1, a1 + a2 + 2a3 + 2a4),
where the second and third coordinates in the image are taken modulo 3. To see
that φ is surjective, note that φ(0, −1, 2, 0) = (1, 0, 0), φ(0, 0, −1, 1) = (0, 1, 0),
and φ(0, −1, 1, 0) = (0, 0, 1). Since φ maps to the generators of Z × Z3 × Z3, φ is
surjective.
Since the coordinate functions of φ are tile invariants, φ(vα −vβ) = (0, 0, 0) for
any difference vector, and it follows that H ⊆ker φ. To see the reverse containment,
suppose v = (a1, a2, a3, a4) ∈ker φ. Then we have three equations:
a1 + a2 + a3 + a4 = 0
(4)
a4 −a1 = 3k for some integer k
(5)
a1 + a2 + 2a3 + 2a4 = 3l for some integer l.
(6)
Equation (5) tells us a1 = a4 −3k. Subtracting (4) from (6) gives a3 = 3l −a4, and
substituting these expressions into Equation (4) gives us a2 in terms of a4. So, an
arbitrary element v of ker φ has the form
(a4−3k, 3k−3l−a4, 3l−a4, a4) = a4(1, −1, −1, 1)+k(−3, 3, 0, 0)+l(0, −3, 3, 0).
Figure 16 shows tilings of rectangles that generate these three difference vectors:
vα −vβ = (1, −1, −1, 1), vγ −vζ = (−3, 3, 0, 0), and vη −vθ = (0, −3, 3, 0),
(assuming these tilings agree on corresponding untiled [2 × 3] or [3 × 2] blocks).
Thus, any element of ker φ is a linear combination of difference vectors, and so is
in H. Thus, φ is onto with kernel equal to H, and by the ﬁrst isomorphism theorem
of groups, G(L, Rall) = Z4/H ≃Z × Z3 × Z3.
⊓⊔

74
M.P. Hitchman
a
q
z
g
b
h
Fig. 16 Generating difference vectors for the set L.
We remark that since the regions in Figure 16 are rectangles, and the tile
invariants apply to rectangles, we have also proved G(L, Rrect) ≃Z × Z3 × Z3.
Exercise 2. Why can’t we use Figure 11 to establish that a2 −a3 is invariant for
the set L over Rsc?
Challenge Problem 1. Prove that G(T3, Rsc) ≃Z2 by using the area and Con-
way/Lagarias invariants as coordinate functions.
Pak demonstrated in [15, Section 6] that the tile invariants in Theorem 2 together
with the area invariant generate the tile counting group for Tn over Rsc.
Theorem 5. If n = 2m + 1 then G(Tn, Rsc) ≃Zm+1. If n = 2m then G(Tn, Rsc) ≃
Zm × Z2.
4
Tile Invariants and Tileability
The colorings in Section 3.1 generate tile invariants. We can look at coloring
arguments in a different way to state necessary conditions for tileability of a given
region. Suppose we color each cell in the lattice with an element of an abelian group
G whose identity element is e. (G is often Z or Zn.) A coloring map φ : Rall →G
is a function that assigns to each region R its color sum φ(R) (the sum in G of
the elements covered by the region R). A T-coloring is a coloring map φ in which
φ(t) = e for each tile t in the tile set T, wherever t is placed in the lattice. We then
have the following necessary condition for the tileability of a region R:
Theorem 6. Suppose φ : Rall →G is a T-coloring. If a region R is tileable by T
then φ(R) = e.

Tile Invariants for Tackling Tiling Questions
75
Fig. 17 Can T-tetrominoes
tile a chessboard with a
[2 × 2] missing?
1
5
1
5
1
5
1
5
5
1
5
1
5
1
5
1
1
5
1
5
1
5
1
5
5
1
5
1
5
1
5
1
1
5
1
5
1
5
1
5
5
1
5
1
5
1
5
1
1
5
1
5
1
5
1
5
5
1
5
1
5
1
5
1
Example 9. Can the set of T-tetrominoes in Figure 17 tile an [8 × 8] with a [2 × 2]
removed? We may color the cells of the lattice according to sx,y →1 if x + y is even
and sx,y →5 if x+y is odd. Notice that the color sum of each T-tetromino, wherever
it is placed, is equivalent to 0, modulo 8. In other words, we have a T-coloring where
G is the abelian group Z8. Now, any [2 × 2] square covers two 1s and two 5s, so the
color sum of the [8 × 8] with a [2 × 2] removed is
30 · 1 + 30 · 5 ≡4 (mod 8),
meaning its color sum is not the identity in Z8, so this region is not tileable by
T-tetrominoes.
Example 10. We show that [a×b] with a [2×2] removed is not tileable by T4 when
a, b > 2 and a ≡0 (mod 4).
Consider the group Z3 whose identity element is e = (0, 0, 0). Let v0 = (1, 0, 0),
v1 = (0, 1, 0), v2 = (0, 0, 1) and v3 = (−1, −1, −1). Color the lattice according to
φ(sx,y) = vx+y (mod 4). Any tile t in T4, wherever it is placed in the lattice, has color
sum φ(t) = v0 + v1 + v2 + v3 = e. Since a = 4k, where k > 0, [a × b] is tileable by
kb copies of the vertical tile 111, and hence φ([a×b]) = e, wherever [a×b] is placed
in the lattice. Now suppose a [2×2] has been removed and the resulting region R has
been placed in the lattice so that the bottom left corner of the missing [2×2] is at that
origin. Then φ(R) = φ([a×b])−φ([2×2]) = e−(v0+2v1+v2) = −(1, 2, 1) ̸= e.
Thus R is not tileable by T4, by Theorem 6.
Exercise 3. Use a coloring argument to show that the set of 8 L-tetrominoes found
by rotations and reﬂections of the ribbon tile 001 in Figure 18 does not tile an [8×8]
with a [2 × 2] removed.
We return to ribbon tiles once more. Though Theorem 3 resolves tiling questions
for any simply connected region and the full set Tn, it remains interesting to
investigate tileability questions with subsets of Tn. Deﬁne the height of a ribbon
tile having binary signature ϵ = ϵ1ϵ2 · · · ϵn−1 to be the sum ϵ1 + ϵ2 + · · · + ϵn−1,
taken modulo 2. Heights partition Tn into two subsets of equal size, the set T0
n of
height-0 tiles and the set T1
n of height-1 tiles. Figure 18 shows the set T1
4.
The following tile invariant, called the height invariant in [15], is obtained by
considering the sum, taken modulo 2, of the ⌊n/2⌋tile invariants in Theorem 2.

76
M.P. Hitchman
Fig. 18 The set T1
4 of
height-1 ribbon tile
tetrominoes.
001
010
100
111
The Height Invariant If R is a simply connected region tileable by Tn, then the
number of height-1 tiles used in any tiling of R is constant, modulo 2. We let h(R)
denote the value of this constant.
We can now prove the following useful test for nonexistence of a tiling by height-
1 ribbon tiles.
Lemma 1 (Tileability Test for T1
n ). Suppose there exists a tiling of a simply
connected region R by Tn in which an odd number of height-0 tiles are used. Then
the set T1
n of height-1 tiles does not tile R.
Proof. Suppose Tn tiles R with a + b tiles where a counts the number of height-1
tiles and b counts the number of height-0 tiles, and suppose b ≥1 is odd. Then
h(R) ≡a · 1 + b · 0 ≡a (mod 2).
If T1
n tiles R then it does so with a + b tiles, in which case we would also have
h(R) ≡a + b (mod 2). But these two descriptions of h(R) would imply b ≡0 (mod
2), a contradiction since b is odd. Thus, no tiling of R by height-1 tiles exists.
⊓⊔
Example 11. Finishing off the Staircase Problem.
For ribbon tile trominoes, T1
3
= {t2, t3} (the L-trominoes in the staircase
problem), and T0
3 = {t1, t4} (the straight tiles). Figure 3 shows tilings of S3, S5, S6
and S8, and each one uses an odd number of height-0 tiles. Lemma 1 ensures that
the set {t2, t3} does not tile these regions. Moreover, if the set T3 can tile Sn with an
odd number of height-0 tiles, then it can tile Sn+12 with an odd number of height-0
tiles, since Sn+12 can be partitioned as in Figure 2, and the regions [n × 12] and T12
can be tiled without any height-0 tiles. So, Sn is not tileable by {t2, t3} if n ≡3, 5, 6,
or 8 (mod 12), and we have completed the proof that Sn is tileable by {t2, t3} if and
only if n ≡0, 2, 9, or 11 (mod 12).
Example 12. In [15], Pak used this approach to answer several tiling questions
involving rectangles and the sets T1
n , for various n. In [8] we consider which
modiﬁed rectangles M(a, b) (obtained from [a × b] by removing its upper-left and
lower-right cells) can be tiled by the set T1
4 in Figure 18. We show that for a, b > 1,
M(a, b) is tileable by T1
4 if and only if a ≡2 (mod 4) and b is odd, or a is odd and
ab ≡2 (mod 8). With Lemma 1 in hand, the format of the proof follows much as
the one used in the staircase problem. We ﬁnd tilings of small, base case M(a, b)s,
and then show inductively that tilings exist in all the cases listed above. We then rule
out tilings of other M(a, b) of the right area by demonstrating inductively that there

Tile Invariants for Tackling Tiling Questions
77
Fig. 19 M(5, 6) cannot be
tiled by height-1 ribbon tile
tetrominoes alone.
Fig. 20 Signed tiling of [1 × 3] and [3 × 1] by the subset {t2, t3} of T3. The solid tiles are +1
weighted and the dashed tiles are −1 weighted.
exist tilings of them in which an odd number of height-0 tiles are used. For instance,
by Lemma 1, the height-1 ribbon tile tetrominoes do not tile M(5, 6) because T4
tiles it with an odd number of height-0 tiles, as in Figure 19.
The boundary word arguments used to ﬁnd the tile invariants for Tn are
signiﬁcantly more involved than coloring arguments, and it is reasonable to ask
whether coloring arguments might unearth them. In fact, they cannot. Conway and
Lagarias prove that no coloring argument could have been used to solve the staircase
problem, and the same holds for the results in Example 12. To establish this fact they
turn to signed tilings.
A signed tiling of a region R by a tile set T consists of a placement of tiles from
T, each given a weight of +1 or −1, in such a way that the sum of the weights of the
tiles is +1 for every cell inside the region R, and 0 for every cell outside the region.
Figure 20 shows signed tilings of [1 × 3] and [3 × 1] by the subset {t2, t3} of T3.
It follows that any tiling of a region by T3 can be converted to a signed tiling by
{t2, t3}. For instance, since T3 tiles S8, we know there is a signed tiling of S8 by
{t2, t3}. Thanks to the following result, which appears in [5, Theorem 5.2] and [15,
Theorem 8.1], we know that if a tile set T cannot tile a region R and there exists a
signed tiling of R by T, then no coloring argument could have been used to prove
nonexistence of a tiling.
Theorem 7. Any T-coloring that proves nonexistence of a tiling of a region also
proves nonexistence of any signed tiling of the region.
Proof. We prove the contrapositive. Suppose φ : R →G is a T-coloring, and
{(t′
i, δi) | i = 1, · · · , m} represents a signed tiling of R by T, where each t′
i is a
translation of a tile in T and each δi ∈{−1, 1} gives the weight of the tile t′
i. Then
φ(R) = φ(s1) + · · · + φ(s|R|) where the sum (in G) is over all the cells in R. But
this sum equals
n

i=1
δiφ(t′
i) because the net weight from the signed tiling of each
cell in R is +1 and the net weight of each cell outside R is 0. But as a T-coloring,
φ(t′
i) = e and it follows that φ(R) = e. Thus, the T-coloring cannot be used to prove
nonexistence of a tiling of R.
⊓⊔

78
M.P. Hitchman
It is not hard to extend the scene of Figure 20 to the general case n ≥3. Namely,
each tile in Tn has a signed tiling by the subset T1
n of height-1 tiles. Applying
Theorem 7 and Lemma 1 to this setting we have the following fact: if a simply
connected region R can be tiled by Tn in such a way that an odd number of tiles are
height-0 tiles, then the set T1
n does not tile R, and no coloring argument could have
been used to prove this.
Research Project 1. The height-1 ribbon tile pentominoes, T1
5, have binary
signatures {0001, 0010, 0100, 1000, 0111, 1011, 1101, 1110}. Pak proved
that T1
5 tiles [a × b] (where a, b > 1) if and only if ab = 0 (mod 10), [15,
Theorem 7.2]. Which modiﬁed rectangles M(a, b) (see Example 12) can be
tiled by T1
5?
Research Project 2. We have focused on the height invariant and its useful-
ness for tiling problems involving the set T1
n , but other subsets of Tn have been
considered as well. For instance, [15, Theorems 0.2, 0.3] uses the i = 1 invari-
ant for T5 from Theorem 2 to determine which rectangles and which staircase
regions are tileable by the subset {0001, 0101, 0111, 1000, 1010, 1110} of
T5. Explore other subsets of Tn that may lend themselves to tiling questions
requiring the use of some of the ribbon tile invariants of Theorem 2.
Challenge Problem 2. Let Γm(a, b) be the “gamma”-shaped region obtained by
removing from [a × b] the square [m × m] from its lower right corner. Use Lemma 1
to prove that for odd m ≥1 and a, b > m, the set T1
4 tiles Γm(a, b) if and only if
a ≡m (mod 4) and b ≡m (mod 8); or a ≡3m (mod 4) and b ≡3m (mod 8).
(This unpublished result was established by the author and Ben Coate as part of a
student-faculty research project in 2008.)
Research Project 3. Consider again the regions Γm(a, b) and the tile set T1
4.
What can be said in the case m is even? A coloring argument may apply.

Tile Invariants for Tackling Tiling Questions
79
5
Enumeration
Counting the number of distinct tilings of a region R by T is a question in
enumerative combinatorics. A recent survey article by Propp [17] discusses general
techniques that can be brought to bear on the matter. Rather than talk generalities,
we present a few examples to give the reader a feel for enumeration questions.
Example 13 (Domino Tilings). The rectangle [2 × n] has Fn+1 tiling by dominoes,
where Fn is the nth term in the Fibonacci sequence 1, 1, 2, 3, 5, 8, . . .. This result
has a nice proof by induction. A more complicated formula, which we mention in
passing, enumerates the domino tilings of the rectangle [m × n]:
⌈m
2 ⌉

j=1
⌈n
2 ⌉

k=1
(4 cos2
jπ
m + 1 + 4 cos2
kπ
n + 1).
Yes, this expression is always an integer (and gives 0 if m and n are odd), and this
surprising result was established in 1961, independently in [6] and [9].
Exercise 4. Show that (a) [2 × n] has Fn+1 tilings by dominoes, and (b) [2 × 3k]
has F2k+1 tilings by T3.
Example 14. In [21] Ueno provides a recursive process for enumerating tilings of
rectangles by the set of four L-trominoes. The process involves matrices whose
entries correspond to the number of tilings of thin strips in the lattice. For instance,
Ueno shows there are 162 ways to tile the square [6 × 6]. One can check that just
two of these tilings avoid tiling any subrectangle within [6 × 6]. One of these tilings
appears in Figure 21, the other is its mirror image. The number of tilings grows
rapidly with the area of the rectangle. The square [9 × 9] has 1,193,600 tilings.
Which of these tilings, if any, avoid tiling a subrectangle? Perhaps none.
Here’s a question that appeared in [2]. Suppose we remove two cells at random
from a [2×n]. What is the probability that the remaining region (if its area is divisible
by 3) can be tiled by the 4 L-trominoes in Figure 6? For instance, if n = 4 we have
	8
2

possible pairs of points to remove. Perhaps 10 of these pairs leave the remaining
region untileable, in which case the probability of being able to tile a randomly
generated board of this size is 9/14. Can we say anything about the probability as

80
M.P. Hitchman
Fig. 21 A tiling of [6 × 6] by
L-trominoes.
Fig. 22 The colors of the
cells of [6 × 6]. Any ribbon
tile from T6 covers one cell of
each color.
0
1
2
3
4
5
1
2
3
4
5
0
2
3
4
5
0
1
3
4
5
0
1
2
4
5
0
1
2
3
5
0
1
2
3
4
n →∞? Is this question tractable? Work done in [1] may help. This question may
not be a good one to pursue ultimately, but perhaps investigating it will lead to one
that is.
Research Project 4. The set of 4 L-trominoes tiles the rectangle [2×3k] in 2k
ways. Is there a closed-form expression for the number of tilings of [4 × 3k]?
How about other families of rectangles? The question of enumerating tilings
of small height rectangles with Ls and [1 × 1] squares was considered in [4].
Example 15. The ribbon tile set Tn tiles the square [n × n] in n! ways.
This fact was stated in [17], where Propp cites it as an unpublished result of
Cristopher Moore’s. We provide some notation to help us work through a proof of
this nice result. First, place [n × n] in the lattice so that its lower left corner is at the
origin, and color cell sx,y with the value x + y, taken modulo n (see Figure 22 for
n = 6). The initial cell of any ribbon tile is its lower left most cell, and the level of
a ribbon tile placed in [n × n] is the color of its initial cell. Note that any ribbon tile
in Tn will cover one cell of each color, wherever it is placed. Furthermore, the initial
cell of any tile in a tiling of [n × n] must be on or below the main diagonal at level
n −1.
One can prove that no two tiles in any tiling of [n × n] by Tn can have the same
level, which implies there is exactly one tile of level i, for each i = 0, 1, . . . , n −1.
To enumerate the tilings we show there is a one-to-one correspondence between
the number of tilings of [n × n] and the number of permutations of the set
{0, 1, . . . , n −1}.
To establish the correspondence we focus on the cells on the main diagonal (the
cells sx,y such that x+y = n−1). Suppose we begin with a tiling of [n×n]. Record,

Tile Invariants for Tackling Tiling Questions
81
Fig. 23 The tiling of [6 × 6]
by T6 corresponding to
permutation (2,5,1,3,0,4).
2
5
1
3
0
4
in sequence, the level of the tile covering the cells on the main diagonal, proceeding
from upper left to lower right. (In Figure 23 the main diagonal cells are dotted.) The
result will be a permutation of {0, 1, . . . , n −1}, because each cell on this diagonal
is covered by a different tile, and in any tiling we have one tile of each level from 0
to n −1.
Conversely, any permutation uniquely determines a tiling. The ﬁrst number in
the permutation gives the level of the tile that covers the upper left cell of the main
diagonal, the cell s0,n−1. Once this level has been chosen, there is a unique tile
in Tn that can be placed at that level and cover s0,n−1. Once that tile is placed,
move to the second number in the permutation, which gives the level of the tile that
covers s1,n−2. Again, there is a unique tile in Tn that can work. In general, the ith
number in the permutation gives the level of the tile used to cover cell si−1,n−i, and
this level uniquely determines the tile that can work. For instance, the permutation
(2,5,1,3,0,4) uniquely determines the tiling of [6 × 6] in Figure 23. We encourage
the reader to work through the details.
6
Concluding Remarks
In this introduction to the mathematics of tiling we have placed an emphasis on tile
invariants and their role in tileability questions. Finding tile invariants for a given tile
set, especially those that provide us with nonexistence results that would otherwise
have been difﬁcult to ﬁnd, continues to be an interesting research opportunity. Many
of the papers referenced here make some customized use of coloring or boundary
word arguments to ﬁnd tile invariants for tackling a tiling question; and these papers
tend to generate as many questions as they solve. We encourage students to visit
these papers, carefully work through the arguments presented, then begin tinkering
with different questions. A ﬁne place to begin would be in Korn’s thesis [10], which
offers an engaging and accessible treatment of tiling techniques to solve many types
of tiling questions.
We have focused exclusively on regions living in a square lattice of the Euclidean
plane because our experiences in student-centered research lie here, but we mention
that the mathematics of tiling extends to other settings as well. For instance, in [5]
and [20] the authors apply boundary word techniques to regions living in a triangular
lattice, and also a hexagonal lattice. Much work on tilings has also been done in the

82
M.P. Hitchman
Fig. 24 The annular region
A2(3, 4) can be tiled by the
set of skew and T-tetrominoes
in 64 ways.
t1
t2
t3
t4
t5
t6
t7
t8
A2(3,4)
hyperbolic plane, on the surface of a sphere, and in higher dimensions. We leave it
to the interested reader to explore work done in these other settings.
Example 16. We close with a set of tiling questions initially investigated in an REU
in 2013. Let T consist of the skew and T-tetrominoes in Figure 24. The annular
region An(a, b) consists of a rectangular annulus of width n surrounding a rectangle
[a × b]. Figure 24 shows A2(3, 4). The following results are established in [3].
•
Every A2(a, b) is tileable by T, and A2(a, b) can be tiled in 2a+b−1 ways.
•
The set A2 of all width-2 annuli has no local move property with respect to T.
•
The tile counting group G(T, A2) ≃Z3×Z2 is generated by these tile invariants:
 ai = |R|
4 (area invariant)
a2 −a1 = c1(R)
a4 −a3 = c2(R)
a1 + a2 + a7 + a8 ≡c3(R) (mod 2)
Several open questions in this setting remain. Do analogous results hold for other
annuli? Tilings of A3(1, 3) and A3(2, 2) appearing in [3] show that except for the
area invariant, the tile invariants above do not apply for width-3 annuli. Furthermore,
the region A3(4, 4) does not appear to be tileable, though a non brute-force proof
has not been found. Since there is a signed tiling of A3(4, 4) by skews and Ts, no
coloring argument can prove nonexistence. Finally, we remark that this tile set does
not tile [6 × 6] or [6 × 10], as demonstrated in [11], though a brute-force approach
was used there. Can a more elegant solution be found?
Challenge Problem 3. Use a coloring argument to show that if a ≡3 (mod 4) and
b ≡2 (mod 4) then the subset {t3, t4, t7, t8} of skews and Ts in Figure 24 does not
tile the modiﬁed rectangle M(a, b). This result appeared in [11].

Tile Invariants for Tackling Tiling Questions
83
Research Project 5. Determine the tile counting group for the set of 4 skews
and 4 T-tetrominoes and the family of all width-3 annuli. Can the number of
tilings of A3(a, b) be enumerated?
Research Project 6. If a cell (or two) is randomly removed from an [a × b],
what is the probability that the remaining region is tileable by a given tile
set?
References
1. Aanjaneya, M.: Tromino tilings of domino-deﬁcient rectangles. Discret. Math. 309(4),
937–944 (2009)
2. Ash, J.M., Golomb S.W.: Tiling deﬁcient rectangles with trominoes. Math. Mag. 77(1), 46–55
(2004)
3. Bright, A., Clark, J.G., Dunn C., Evitts, K., Hitchman, M.P., Keating, B., Whetter, B.: Tiling
annular regions with skew and T-tetrominoes. Involve J. Math. 10(3), 505–521 (2017)
4. Chinn, P., Grimaldi, R., Heubach, S.: Tiling with Ls and squares. J. Integer Seq. 10, Article
07.2.8 (2007)
5. Conway, J.H., Lagarias, J.C.: Tiling with polyominoes and combinatorial group theory. J.
Combin. Theory Ser. A 53, 183–208 (1990)
6. Fisher, M., Temperley, H.: Dimer problem in statistcal mechanics - an exact result. Philos.
Mag. 6, 1061–1063 (1961)
7. Golomb, S.W.: Checker boards and polyominoes. Am. Math. Mon. 61, 675–682 (1954)
8. Hitchman, M.P.: The topology of tile invariants. Rocky Mt. J. Math. 45(2), 539–564 (2015)
9. Kasteleyn, P.: The statistics of dimers on a lattice I. The number of dimer arrangements on a
quadratic lattice Physica 27, 1209–1225 (1961)
10. Korn, M.: Geometric and algebraic properties of polyomino tilings. Ph.D. thesis, MIT (2004).
http://hdl.handle.net/1721.1/16628
11. Lester, C.: Tiling with T and skew tetrominoes. Querqus Linﬁeld J. Undergrad. Res. 1(1),
Article 3 (2012)
12. Magnus, W., Karrass, A., Solitar, D.: Combinatorial Group Theory, 2nd edn. Dover, New York
(1976)
13. Moore C., Pak, I.: Ribbon tile invariants from signed area. J. Combin. Theory Ser. A 98, 1–16
(2002)
14. Muchnik R., Pak, I.: On tilings by ribbon tetrominoes. J. Combin. Theory Ser. A 88, 188–193
(1999)
15. Pak, I.: Ribbon tile invariants. Trans. Am. Math. Soc. 352(12), 5525–5561 (2000)
16. Propp, J.: A pedestrian approach to a method of Conway, or, a tale of two cities. Math. Mag.
70(5), 327–340 (1997)
17. Propp, J.: Enumeration of tilings. In: Bona M. (ed.) Handbook of Enumerative Combinatorics.
CRC Press, Boca Raton (2015)

84
M.P. Hitchman
18. Reid, M.: Tile homotopy groups. Enseign. Math. 49(1/2), 123–155 (2003)
19. Shefﬁeld, S.: Ribbon tilings and multidimensional height functions. Trans. Am. Math. Soc.
354(12), 4789–4813 (2002)
20. Thurston, W.P.: Conway’s tiling groups. Am. Math. Mon. 95, 757–773 (1990). special
geometry issue
21. Ueno, C.: Matrices and tilings with right trominoes. Math. Mag. 81(5), 319–331 (2008)

Forbidden Minors: Finding the Finite Few
Thomas W. Mattman
Suggested Prerequisites. There are no serious prerequisites for this material. A
course on graph theory would be helpful, but there are many books for self-study,
including Marcus [18], that would quickly bring a student up to speed.
1
Introduction
Kuratowski’s Theorem [16], a highlight of undergraduate graph theory, classiﬁes
a graph as planar in terms of two forbidden subgraphs, K5 and K3,3. (We defer
a precise statement to the next paragraph.) We will write Forb(Pl) = {K5, K3,3}
where Pl denotes the planarity property. We can think of the Graph Minor Theorem
of Robertson and Seymour [23] as a powerful generalization of Kuratowski’s
Theorem. In particular, their theorem, which has been called the “deepest” and
“most important” result in all of graph theory [15], implies that each graph property
P, whatsoever, generates a corresponding ﬁnite list of graphs. This scaffolding
allows students to devise their own Kuratowski type theorems. As an example,
we will determine the seven forbidden minors for a property that we call strongly
almost-planar.
To proceed, we must deﬁne graph minor, which is a generalization of subgraph.
We will assume familiarity with the basic terminology of graph theory; West’s [27]
book is a good reference at the undergraduate level. While Diestel [8] is at a higher
level, it includes an accessible approach to graph minor theory. For us, graphs are
simple (no loops or double edges) and not directed. We can deﬁne the notion of a
minor using graph operations. We obtain subgraphs through the operations of edge
and vertex deletion. For minors, we allow an additional operation: edge contraction.
T.W. Mattman ()
Department of Mathematics and Statistics, CSU, Chico, Chico, CA 95929-0525, USA
e-mail: TMattman@CSUChico.edu
© Springer International Publishing AG 2017
A. Wootton et al. (eds.), A Primer for Undergraduate Research, Foundations for
Undergraduate Research in Mathematics, DOI 10.1007/978-3-319-66065-3_4
85

86
T.W. Mattman
Fig. 1 Edge contraction.
As in Figure 1, when we contract edge ab in G, we replace the pair of vertices with
a single vertex ¯a that is adjacent to each neighbor of a or b. The resulting graph G′
has one less vertex and at least one fewer edge than G. (If a and b share neighbors,
even more edges are lost.) A minor of graph G is any graph obtained by contracting
(zero or more) edges in a subgraph of G. Recall that K5 is the complete graph on
ﬁve vertices and K3,3 the complete bipartite graph with two parts of three vertices
each. We can now state Kuratowski’s Theorem, using the formulation in terms of
minors due to Wagner.
Theorem 1 (Kuratowski-Wagner [16, 26]). A graph G is planar if and only if it
has no K5 nor K3,3 minor.
Robertson and Seymour’s theorem can be stated as follows.
Theorem 2 (The Graph Minor Theorem [23]). In any inﬁnite sequence of graphs
G1, G2, G3, . . . there are indices i ̸= j such that Gi is a minor of Gj.
This yields two important corollaries, which we now describe.
Planarity is an example of a property that is minor closed: If H is a minor of a
planar graph G, then H must also be planar. If P is minor closed, the Graph Minor
Theorem implies a ﬁnite set of forbidden minors.
Corollary 1. Let P be a graph property that is minor closed. Then there is a ﬁnite
set of forbidden minors Forb(P) such that G has P if and only if it has no minor
in Forb(P).
In honor of the theorem for planar graphs, we call Forb(P) the Kuratowski set
for P.
Even if P is not minor closed, the Graph Minor Theorem determines a ﬁnite
set. For this, note that K5 and K3,3 are minor minimal nonplanar; each is nonplanar
with every proper minor planar. More generally, for graph property P, a graph G is
minor minimal for P or MMP if G has P, but no proper minor does.
Corollary 2. Let P be a graph property. The set of MMP graphs is ﬁnite.
Every P graph has a MMP minor. However, if the negation, ¬P, is not minor
closed, there will be graphs that are not P even though they have a MMP minor. In
addition to providing a necessary condition for P, the ﬁnite MMP list, therefore,
constitutes a certiﬁcate or sufﬁcient condition for ¬P; a graph with no MMP
minor deﬁnitely does not have property P. For these reasons, when P is not minor
closed, determining the list of MMP graphs is worthwhile, even if it does not
completely characterize P the way that a Kuratowski set would.

Forbidden Minors: Finding the Finite Few
87
In the next section we summarize the graph properties P with known MMP
or Kuratowski set. In Section 3 we illustrate how students might develop their own
Kuratowski type theorem through an explicit example: we determine Forb(P) for a
property that we call strongly almost-planar. In the ﬁnal section we propose several
concrete research projects and provide some suggestions about how to choose graph
properties P.
Throughout the paper, we present a list of ‘challenge problems’ and ‘research
projects.’ Challenges are warm up exercises for talented undergraduates. In most
cases, the solution is known and can be found through a web search or in the
references at the end of the paper. Research projects, on the other hand, are generally
open problems (as far as we know). Some are quite difﬁcult, but, we hope, all admit
openings. Indeed, we see this as a major theme in this area of research. Even if
we know MMP and Kuratowski sets are ﬁnite, a complete enumeration is often
elusive. However, it is generally not too hard to capture graphs that belong to the
set. These problems, then, promise a steady diet of small successes along the way
in our hunt to catch all of the ﬁnite few.
2
Properties with Known Kuratowski Set
In this section we summarize the graph properties with known MMP or Kuratowski
set. First, an important caveat. While the Graph Minor Theorem ensures these sets
are ﬁnite, the proof is not at all constructive and gives no guidance about their size.
It makes for nice alliteration to talk of the ‘ﬁnite few,’ but some ﬁnite numbers are
really rather large. A particularly dramatic cautionary tale is Y∇Y reducibility (we
omit the deﬁnition) for which Yu [28] has found more than 68 billion forbidden
minors.
On the other hand, bounding the order (number of vertices) or size (number
of edges) of a graph is a minor closed condition. For example, whatever property
you may be interested in, appending the condition “of seven or fewer vertices,”
ensures that the set of MMP graphs is no larger than 1044, the number of order
seven graphs. In general, it is quite difﬁcult to predict the size of a Kuratowski set
in advance and researchers in this area often do resort to restricting properties by
simple graph parameters such as order, size, or connectivity.
We will focus on results that generalize planarity in various ways. However,
we brieﬂy mention graphs of bounded tree-width as another important class of
examples. Let Tk denote the graphs of tree-width at most k. For the sake of brevity
we omit the deﬁnition of tree-width (which can be found in [8], for example) except
for noting that T1 is the set of forests, i.e., graphs whose components are trees. For
small k, the obstructions are quite simple: Forb(T1) = {K3}, Forb(T2) = {K4},
and Forb(T3) has four elements, including K5 [5, 25]. However, for k ≥4 the
Kuratowski set for Tk is unknown.

88
T.W. Mattman
Research Project 1. Find graphs in Forb(Tk) for k ≥4. Is Kk+2 always
forbidden? Is there always a planar graph in Forb(Tk)?
We can think of a planar graph as a ‘spherical’ graph since it can be embedded on
a sphere with no edges crossing. More generally, the set of graphs that embed on a
particular compact surface (orientable or not) is also minor closed. However, to date,
(in addition to the sphere) only the Kuratowski set for embeddings on a projective
plane is known; there are 35 forbidden minors [3, 12, 22]. The next step would
be toroidal graphs, those that embed on a torus; Gagarin, Myrvold, and Chambers
remark that there are at least 16 thousand forbidden minors [11]. In the same paper
they show only four of them have no K3,3 minor. This is a good example of how
a rather large Forb(P) can be tamed by adding conditions to the graph property
P. While observing that it’s straightforward to determine the toroidal obstructions
of lower connectivity, Mohar and Škoda [21] ﬁnd there are 68 forbidden minors
of connectivity two. Explicitly listing the forbidden minors of lower connectivity
would be a nice challenge for a strong undergraduate.
Challenge Problem 1. Determine the forbidden minors of connectivity less than
two for embedding in the torus. Find those for a surface of genus two.
The Kuratowski sets for more complicated surfaces are likely even larger than
the several thousand known for the torus.
Outerplanarity is a different way to force smaller Kuratowski sets. A graph is
outerplanar (or has property OPl) if it can be embedded in the plane with all
vertices on a single face. The set of forbidden minors for this property, Forb(OPl)
is well known and perhaps best attributed to folklore (although, see [7]).
Challenge Problem 2. Determine Forb(OPl). (Use Kuratowski’s theorem!)
Similarly, one can deﬁne outerprojective planar or outertoroidal graphs as graphs
that admit embeddings into those surfaces with all vertices on a single face. There
are 32 forbidden minors for the outerprojective planar property [4].
Challenge Problem 3. Find forbidden minors for the outerprojective planar
property.
Research Project 2. Find forbidden minors for the outertoroidal property.

Forbidden Minors: Finding the Finite Few
89
Apex vertices also lead to minor closed properties. Let v ∈V(G) be a vertex of
graph G. We will use G −v to denote the subgraph obtained from G by deleting
v (and all its edges). Given property P, we say that G has P′ (or is apex-P) if
there is a vertex v (called an apex) such that G −v has P. If P is minor closed,
then P′ is as well. For example, Ding and Dziobiak determined the 57 graphs in
Forb(OPl′) [9]. In the same paper, they report that there are at least 396 graphs in
the Kuratowski set for apex-planar.
Research Project 3. Find graphs in Forb(P′) when P is toroidal with no
K3,3, T1, T2, T3, or for some other property with small Forb(P).
The set of linklessly embeddable graphs are closely related to those that are apex-
planar. We say a graph is linklessly embeddable (or has property L ) if there is an
embedding in R3 that contains no pair of nontrivially linked cycles. (See [1] for a
gentle introduction to this idea). An early triumph of graph minor theory was the
proof that Forb(L ) has exactly seven graphs [24]. An apex-planar graph is also
L and, as part of an undergraduate research project, we showed that Forb(L ) ⊂
Forb(Pl′) [6]. The related idea of knotlessly embeddable (which, like L , is minor
closed) has more than 240 forbidden minors [13].
As a ﬁnal variation on properties related to planarity, rather than vertex deletion
(which gives apex properties), let’s think about the other two operations for graph
minors, edge deletion and contraction. For graph G and edge ab ∈E(G), let G −
ab denote the subgraph resulting from deletion and G/ab the minor obtained by
edge contraction. Unlike apex properties, in general edge operations do not preserve
closure under taking minors. This is why we frame some results below in terms of
MMP sets.
As we’ve mentioned, there are at least several hundred graphs in Forb(Pl′). In
an undergraduate research project [17] we found that there are also large numbers
of graphs that are not simply an edge away from planar. Call a graph G ND
(not edge deletion apex) if there is no edge ab with G −ab planar and similarly
NC (not contraction apex) if no G/ab is planar. We showed that the are at least
55 MMND and 82 MMNC graphs. On the other hand, if we switch from the
existential to the universal quantiﬁer, we obtain properties that are minor closed
with reasonably small Kuratowski sets; in the next challenge, each Forb(P) has at
most ten elements. Say that a graph G is CA (completely apex) if G−v is planar for
every vertex v, CD (completely edge deletion apex) if every G −ab is planar, and
CC (completely contraction apex) if every G/ab is planar.
Challenge Problem 4. For P = CA, show that P is minor closed and determine
Forb(P). Repeat for P = CD and CC.

90
T.W. Mattman
Instead of ﬂipping quantiﬁers, we can think about combing operations with other
logical connectives. For example, Gubser [14] calls G almost-planar if, for every
ab ∈E(G), G −ab or G/ab is planar. There are six forbidden minors for this
property [10]. In the next section we determine the Kuratowski set for a property
that we call strongly almost-planar or SAP: for every ab ∈E(G), both G −ab and
G/ab are planar. Note that every strongly almost-planar graph is almost-planar.
3
Strongly Almost–Planar Graphs
In this section we model how a research project in this area might play out through
an explicit example, the strongly almost-planar or SAP property: G is SAP if, ∀ab ∈
E(G), both G −ab and G/ab are planar.
Our ﬁrst task is to determine whether or not this property (or its negation) is
minor closed. If not, there is no hope of getting a Kuratowski set. Instead, we would
target the list of MMSAP graphs, as that would provide a necessary condition for
SAP and a sufﬁcient condition for SAP to fail. However, as we will now show, SAP
is minor closed, meaning our goal is instead Forb(SAP).
Lemma 1. SAP is minor closed
Proof. It is enough to observe that SAP is preserved by the three operations used in
constructing minors, vertex or edge deletion and edge contraction.
Suppose G is SAP and v ∈V(G). Let G′ = G −v. We must show that for each
ab ∈E(G′), both G′ −ab and G′/ab are planar. Since V(G′) ⊂V(G), we can think
of ab as an edge in E(G). Then it’s easy to identify G′ −ab as a subgraph of the
planar graph G −ab, which shows G′ −ab is also planar. Similarly, we’ll know
that G′/ab is planar once we show that it is a subgraph of G/ab. There are a few
cases to think about (Is a or b or both adjacent to v?) but it always turns out that
G′/ab = (G/ab) −v.
For this property, the argument for edge contraction and deletion is quite simple.
For any ab ∈E(G), by assumption G −ab and G/ab are planar. Then any minor
of these graphs is again planar, including those given by deleting or contracting an
edge.
⊓⊔
Next, we must generate examples of forbidden minors, meaning graphs that are
minor minimal for not SAP. We are looking for graphs G that are just barely not
SAP: although G is not SAP, every proper minor is. Most likely, there’s only a
single edge ab with G −ab or G/ab nonplanar. And that graph is probably minor
minimal nonplanar, so one of the Kuratowski graphs K5 or K3,3. We will use K to
represent a generic Kuratowski graph, that is K ∈{K5, K3,3}. In summary, we are
looking for graphs of the form K ‘plus an edge,’ where adding an edge includes the
idea of reversing an edge contraction.
We encourage you to take a minute to see what graphs you can discover that
have the form K ‘plus an edge’. Hopefully, you will ﬁnd ﬁve G for which G −ab
is nonplanar. Perhaps you have even more? Remember we want minor minimal
examples, so check if any pair are minors one of the other.

Forbidden Minors: Finding the Finite Few
91
Fig. 2 The graph K3,3 + 2e.
Since edge contraction may be a new idea for the reader, let’s delve a little further
into examples where G/ab is nonplanar. The reverse operation of edge contraction
is called a vertex split and deﬁned as follows. Replace a vertex ¯a with two vertices a
and b connected by an edge. Each neighbor of ¯a becomes a neighbor of at least one
of a and b.
Suppose G/ab = K3,3. There are essentially two ways to make a vertex split and
recover G. One is to make one of the new vertices, say a, adjacent to no neighbor
of ¯a and the other, b, adjacent to all three. Then, in G, a has degree one (its only
neighbor is b) and b will have degree four. The other option is to make a adjacent to
one neighbor of ¯a and let b have the other two. There are other possibilities since we
may choose to make both a and b adjacent to one of ¯a’s neighbors; but such graphs
will have one of the two we described earlier as a minor.
If G/ab = K5, there are three ways to split the degree four vertex ¯a. Two are
similar to the ones just described for K3,3 where we make a adjacent to zero or to
one neighbor of ¯a. The third option, split up the four neighbors of ¯a by making a and
b adjacent to two each, results in the graph K3,3 + 2e shown in Figure 2. However,
you should observe that this graph has a proper subgraph among those found by
adding an edge to G −ab = K3,3.
A little experimentation along these lines should lead to the seven graphs of
Figure 3. Note that the six graphs in the top two rows occur in pairs where we
perform similar operations on K5 and K3,3. We’ll write K ⊔K2, K ˙∪K2, and ¯K for
the pairs at left, center, and right, respectively and K3,3 + e for the seventh graph
at the bottom of the ﬁgure. The ﬁve graphs with G −ab nonplanar are K3,3 + e,
and the pairs K ⊔K2 and K ˙∪K2. The graphs obtained from G/ab = K where a is
made adjacent to a single neighbor of ¯a are the ¯K pair. When a shares no neighbors
with ¯a, we construct the graphs K ˙∪K2 for a second time. The graph K3,3 + 2e (see
Figure 2), obtained from K5 by splitting a vertex so that a and b are each adjacent
to two neighbors of ¯a, is not SAP. But, it has another non SAP graph, K3,3 + e, as
a proper subgraph and cannot be minor minimal. The others are both non SAP and
minor minimal, as we now verify.
Lemma 2. The seven graphs of Figure 3 are minor minimal for not SAP.
Proof. As we noticed, the two ¯K graphs become Kuratowski graphs after an edge
contraction and the rest have an edge deletion that leaves a nonplanar graph. This
shows that none of the graphs are SAP.

92
T.W. Mattman
Fig. 3 Forbidden minors for SAP.
It remains to show that every proper minor of each graph is SAP. Since SAP is
minor closed, it’s enough to verify this for the three basic operations vertex or edge
deletion and edge contraction. Actually, since none of our graphs has an isolated
vertex, we need only check edge deletion and contraction. For once we have those
in hand, then any graph of the form G −a is automatically SAP as it’s a subgraph
of one of the G −ab graphs formed by deleting an edge on a. (Recall that we just
proved that SAP is closed under taking minors.)
Note that planar graphs are SAP, so we can reduce to the case where an edge
deletion or contraction gives a nonplanar graph. Let G be a graph in the ﬁgure
and suppose G′ is a nonplanar minor obtained by an edge deletion or contraction.
Observe that, up to isolated vertices, G′ is simply a Kuratowski graph K. In
particular, E(G′) = E(K). Since K is minor minimal nonplanar, any further edge
deletion or contraction leaves a planar graph, which shows G′ is SAP, as required.
This completes the argument that the seven graphs in the ﬁgure are in Forb(SAP).
⊓⊔

Forbidden Minors: Finding the Finite Few
93
We will argue that there are no other graphs in Forb(SAP). We begin with graphs
that are not connected.
Lemma 3. If G ∈Forb(SAP) is not connected, then G = K ⊔K2 with K ∈
{K5, K3,3}.
Proof. Let G = G1 ⊔G2 in Forb(SAP) be the disjoint union of (nonempty) graphs
G1 and G2. Since planar graphs are SAP, at least one of G1 and G2, say G1, is not
planar.
We ﬁrst observe that G2 must have an edge, E(G2) ̸= ∅. Otherwise, since G is
not SAP, there is an edge ab ∈E(G) and a nonplanar minor G′, formed by deleting
or contracting ab. Since G2 has no edges, it is planar and ab ∈E(G1). If follows
that deleting or contracting ab in G1 already gives a nonplanar graph. That is, G1 is
a proper minor of G that is not SAP. This contradicts our assumption that G is minor
minimal not SAP.
By Kuratowski’s theorem, G1 has a Kuratowski graph minor, K. We claim that
G1 = K. Further, since G2 has an edge, we must have G2 = K2. For, if either of
these fail, the graph K ⊔K2, which is not SAP by the previous lemma, is a proper
minor of G. This contradicts our assumption that G is minor minimal for not SAP.
⊓⊔
We can now complete the argument.
Theorem 3. The seven graphs of Figure 3 are precisely the elements of Forb(SAP).
Proof. Using the previous two lemmas, it remains only to verify that if G is
connected and in Forb(SAP), then it is one of the ﬁve connected graphs in the ﬁgure.
Suppose G is connected and minor minimal not SAP. Since G is not SAP, there is an
ab ∈E(G) such that G′, a minor formed by deleting or contracting ab, is not planar.
Then G′ has a Kuratowski graph K as a minor. In fact K must appear as a subgraph
of G′. If not, one of the two ¯K graphs is a minor of G′ and, hence also of G. This
contradicts our assumption that G is minor minimal for not SAP.
Suppose that the nonplanar G′ is formed by edge deletion: G′ = G−ab. There are
several cases depending on the size of V(K) ∩{a, b}. If there is no common vertex,
then G has a K ⊔K2 minor. Since we assumed G is minor minimal for not SAP,
G = K ⊔K2, but this contradicts our assumption that G is connected. Suppose there
is one vertex in the intersection. Then G has a K ˙∪K2 minor. By minor minimality,
G = K ˙∪K2 and appears in Figure 3 as required. Finally, if {a, b} ⊂V(K), then K
must be K3,3 and, by minor minimality, G = K3,3 + e is one of the graphs in the
ﬁgure.
If instead G′ = G/ab, let ¯a denote the vertex that results from identifying a and
b. If ¯a ∈V(K), there are two possibilities. It may be that G has one of the ¯K or
K ˙∪K2 graphs of Figure 3 as a minor. But then, by minor minimality, G is one of
those graphs in the ﬁgure, as required. The other possibility is G has the K3,3 + 2e
graph of Figure 2 as a minor. Then, K3,3 + e is a proper minor, contradicting the
minor minimality of G. On the other hand, if ¯a ̸∈V(K), G must have a K ⊔K2
minor. By minor minimality, G = K ⊔K2, which contradicts our assumption that G
is connected.
⊓⊔

94
T.W. Mattman
While it’s difﬁcult to convey the hard work that went into ﬁnalizing the list of
seven graphs, we hope this account gives some of the ﬂavor of a project in this
area. This argument is, in fact, not so different from what appears in (soon to be)
published research, see [10, 17]. Recall that CA, CD, and CC all have Kuratowski
sets with at most 10 members (see Challenge 4). We can think of almost-planar
as CD or CC and SAP as CD and CC. This suggests that other combinations of
the three C properties are also likely to be minor closed with a small number of
forbidden minors. For example, here are two ways to combine CA and CD.
Research Project 4. Say graph G has property CACD if, for every edge ab
and every vertex v ̸∈{a, b}, either G −v or G −ab is planar. Determine
whether or not CACD is minor closed and ﬁnd the Kuratowski set or
MMCACD set. Repeat for strongly CACD, which requires both G −v and
G −ab planar.
4
Additional Project Ideas
In this section we propose several additional research projects along with general
strategies to develop even more.
Let Ek denote the graphs of size k or less. We have mentioned that this property
is minor closed.
Challenge Problem 5. Determine the Kuratowski set for edge-free graphs,
Forb(E0) and that for the corresponding apex property, Forb(E ′
0). What is Forb(Ek)
for k > 0?
However, E ′
1 is already interesting and general observations about higher k would
be worth pursuing.
Research Project 5. Find graphs in Forb(E ′
1). Find forbidden minors for E ′
k
when k ≥2. Can you formulate any conjectures about Forb(E ′
k )?
In a different direction, if P is minor closed, then so too are all P(k) where
P(k+1) = (P(k))′.

Forbidden Minors: Finding the Finite Few
95
Research Project 6. Find graphs in Forb(E0
′′). We might call E ′′
0 graphs 2-
apex edge free. Any conjectures about k-apex edge free?
How about working with order instead of size?
Research Project 7. Find forbidden minors for graphs of order at most k.
What about apex versions of these Kuratowski sets? Any conjectures?
Naturally, one can combine these. What is the Kuratowski set for graphs that have at
least two edges and three vertices? What of graphs that have either an edge or four
vertices?
These project ideas encourage you to formulate your own conjectures. As
examples of the kinds of conjectures that might arise, we refer to Research project 1.
There we noticed that Kk+2 is a forbidden minor in Tk for k = 1, 2, 3, which led
us to ask if the pattern persists. That research project also includes a guess about
planar graphs, again based on what is known for small k. Recently, we made similar
observations about forbidden minors for Pl(k) which is also called k-apex [19].
While proving that Kk+5 ∈Forb(Pl(k)) we were unable to conﬁrm a stronger
conjecture that all graphs in the Kk+5 family are forbidden. (Please refer to [19]
for the deﬁnition of a graph’s family.) We have a similar conjecture for graphs in the
family of K32,1k, a k + 2-partite graph with two parts of three vertices each and the
remainder having only one vertex.
Research Project 8. Prove the conjecture of [19]: The Kk+5 and K32,1k
families are in Forb(k −apex).
So far we have focused attention on graph properties that are minor closed
and most of the discussion in Section 2 described techniques for generating such
properties. The meta-problem of ﬁnding additional minor closed graph properties is
also worthwhile.

96
T.W. Mattman
Research Project 9. Find a minor closed graph property P different from
those described to this point. Find graphs in Forb(P).
A survey by Archdeacon [2] includes a listing of several more problems on
forbidden graphs; many of them would be great undergraduate research projects.
As in Corollary 1, minor closed P are attractive because Forb(P) then precisely
characterizes graphs with the property. On the other hand, as discussed following
Corollary 2, even if P is not minor closed, it is worth ﬁnding the ﬁnite list of MMP
graphs which provide both a necessary condition for P and a sufﬁcient condition
for its negation. The possible projects in this direction are virtually endless. Take
your favorite graph invariant (e.g., chromatic number, girth, diameter, minimum or
maximum degree, degree sequence, etc.) and see how many MMP graphs you can
ﬁnd for speciﬁc values of the invariant. Of course, if you choose a graph property at
random, you run the risk of stumbling onto a MMP list that, while ﬁnite, is rather
large. In that case, you can simply restrict by graph order or size, for example.
If you’re fortunate enough to be working with a student with some computer
skills, you might let her loose on the graph properties that are built into many
computer algebra systems. With computer resources, even the 300 thousand or so
graphs of order nine or less are not out of the question, see for example [20].
Finally, let us note that the recent vintage of the Graph Minor Theorem and
the rather speciﬁc interests of graph theorists leave a virtually untouched playing
ﬁeld open to those of us working with undergraduates. To date, serious researchers
have focused on ﬁnding forbidden minors for a fairly narrow range of properties
deemed important in the ﬁeld. For those of us who needn’t worry overly about the
signiﬁcance of the result, there is tremendous freedom to pursue pretty much any
idea that comes to mind and see where it takes us. These are early days in this area
and whichever path you choose to follow, there’s an excellent chance of capturing a
Kuratowski type theorem of your very own.
References
1. Adams, C.C.: The Knot Book: An Elementary Introduction to the Mathematical Theory of
Knots. American Mathematical Society, Providence, RI (2004). Revised reprint of the 1994
original
2. Archdeacon, D.: Variations on a theme of Kuratowski. Discret. Math. 302 22–31 (2005)
3. Archdeacon, D.: A Kuratowski theorem for the projective plane. J. Graph Theory 5, 243–246
(1981)
4. Archdeacon, D., Hartsﬁeld, N., Little, C.H.C., Mohar, B.: Obstruction sets for outer-projective-
planar graphs. Ars Combin. 49, 113–127 (1998)
5. Arnborg, S., Proskurowski, A., Corneil, D.G.: Forbidden minors characterization of partial
3-trees. Discret. Math. 810, 1–19 (1990)
6. Barsotti, J., Mattman, T.W.: Graphs on 21 edges that are not 2-apex. Involve 9, 591–621 (2016)

Forbidden Minors: Finding the Finite Few
97
7. Chartrand, G., Harary, F.: Planar permutation graphs. Ann. Inst. H. Poincar. Sect. B (N.S.) 3,
433–438 (1967)
8. Diestel, R.: Graph Theory, 4th edn. Graduate Texts in Mathematics, vol. 173. Springer,
Heidelberg (2010)
9. Ding, G., Dziobiak, S.: Excluded-minor characterization of apex-outerplanar graphs. Graphs
Combin. 32, 583–627 (2016)
10. Ding, G., Fallon, J., Marshall, E.: On almost-planar graphs. 2016 (Preprint)
11. Gagarin, A., Myrvold, W., Chambers, J.: The obstructions for toroidal graphs with no K3,3’s.
Discret. Math. 309 3625–3631 (2009)
12. Glover, H., Huneke, J.P., Wang, C.S.: 103 graphs that are irreducible for the projective plane.
J. Combin. Theory Ser. B 27, 332–370 (1979)
13. Goldberg, N., Mattman, T.W., Naimi, R.: Many, many more intrinsically knotted graphs.
Algebr. Geom. Topol. 14, 1801–1823 (2014)
14. Gubser, B.S.: A characterization of almost-planar graphs. Combin. Probab. Comput. 5,
227–245 (1996)
15. Kawarabayashi, K., Mohar, B.: Some recent progress and applications in graph minor theory.
Graphs Combin. 23, 1–46 (2007)
16. Kuratowski, K.: Sur le problème des courbes gauches en topologie. Fundam. Math. 15,
271–283 (1930)
17. Lipton, M., Mackall, E., Mattman, T.W., Pierce, M., Robinson, S., Thomas, J., Weinschelbaum,
I.: Six variations on a theme: almost planar graphs. Involve (2017, to appear)
18. Marcus, D.A.: Graph Theory. A Problem Oriented Approach. MAA Textbooks. Mathematical
Association of America, Washington, DC (2008)
19. Mattman, T.W., Pierce, M.: The Kn+5 and K32,1n families and obstructions to n-apex. In:
Knots, Links, Spatial Graphs, and Algebraic Invariants. Contemporary Mathematics, vol. 689,
pp. 137–158. American Mathematical Society, Providence (2017)
20. Mattman, T.W., Morris, C., Ryker, J.: Order nine MMIK graphs. In: Knots, Links, Spatial
Graphs, and Algebraic Invariants. Contemporary Mathematics, vol. 689, pp. 103–124. Ameri-
can Mathematical Society, Providence (2017)
21. Mohar, B., Škoda, P.: Obstructions of connectivity two for embedding graphs into the torus.
Canad. J. Math. 66, 1327–1357 (2014)
22. Mohar, B., Thomassen, C.: Graphs on Surfaces. Johns Hopkins University Press, Baltimore,
MD (2001)
23. Robertson, N., Seymour, P.: Graph minors. XX. Wagner’s conjecture. J. Combin. Theory Ser.
B 92, 325–357 (2004)
24. Robertson, Seymour, P., Thomas, R.: Sachs’ linkless embedding conjecture. J. Combin. Theory
Ser. B 64, 185–227 (1995)
25. Satyanarayana, A., Tung, L.: A characterization of partial 3-trees. Networks 20, 299–322
(1990)
26. Wagner, K.: Über eine Eigenschaft der ebenen Komplexe. Math. Ann. 114, 570–590 (1937)
27. West, D.B.: Introduction to Graph Theory. Prentice Hall, Inc., Upper Saddle River, NJ (1996)
28. Yu, Y.: More forbidden minors for wye-delta-wye reducibility. Electron. J. Combin. 13
Research Paper 7, 15 pp. (2006)

Introduction to Competitive Graph Coloring
C. Dunn, V. Larsen, and J.F. Nordstrom
Suggested Prerequisites. Ideally a ﬁrst course in Graph Theory or Discrete
Mathematics. However, mathematical maturity and experience writing proofs will
sufﬁce.
1
Introduction
The map-coloring game was ﬁrst presented in Martin Gardner’s “Mathematical
Games” column in Scientiﬁc American in 1981 [24]. Invented by Steven J. Brams,
the game involves two players, Alice and Bob, alternating coloring the countries on
a map such that two countries that share a nontrivial border must receive different
colors. The ﬁrst player, Alice, wants to ensure that the map eventually gets colored
with the ﬁnite set of colors with which the players begin. The second player, Bob,
however, wants to ensure that there comes a time in the game when there is an
uncolored country for which none of the existing colors can be used. The interesting
question is then, for a given map what is the least number of colors necessary such
that Alice has a winning strategy? Unfortunately, this game did not receive any
attention from the graph theory community at the time. Ten years later, Bodlaender
reintroduced the r-coloring game [4] within the broader context of graphs. In the
original formulation of the game on graphs, we begin with a ﬁnite graph, G, and a set
X of r colors. Two players, Alice and Bob, alternate coloring the uncolored vertices
of G using colors from X. At each step of the game, the players must choose to color
C. Dunn () • J.F. Nordstrom
Linﬁeld College, McMinnville, OR 97128, USA
e-mail: cdunn@linﬁeld.edu; jﬁrkins@linﬁeld.edu
V. Larsen
Kennesaw State University, Marietta, GA 30060, USA
e-mail: vlarsen@kennesaw.edu
© Springer International Publishing AG 2017
A. Wootton et al. (eds.), A Primer for Undergraduate Research, Foundations for
Undergraduate Research in Mathematics, DOI 10.1007/978-3-319-66065-3_5
99

100
C. Dunn et al.
u
v
w
Fig. 1 Alice and Bob playing with two colors on G.
an uncolored vertex with a legal color. Alice goes ﬁrst. In the basic formation of the
game, a color α ∈X is legal for an uncolored vertex u if u has no neighbors already
colored α. Alice wins the game if all vertices of the graph are colored; otherwise,
Bob wins. Although both players must use a legal color, Alice is trying to ensure
that at every stage of the game all vertices have a legal color available, while Bob
would like to force a situation in which an uncolored vertex exists for which there
is no legal color.
For example, suppose Alice and Bob are playing the game on the graph G in
Figure 1 with two colors which we will call α and β. If u is colored α, then only
β is legal for v. However, if u is colored α and w is colored β, then v will have no
legal color.
Notice, on this small example, we can see that if Alice ﬁrst colors v, with α, then
β is always legal for u and w. Thus Alice wins on G. However, if Alice ﬁrst colors
u with α, then Bob can color w with β, leaving v with no legal color. Hence Bob
wins.
The least r such that Alice has a winning strategy for this game on G is called
the game chromatic number of G, denoted χg(G). The game chromatic number was
ﬁrst introduced by Bodlaender in [4]. It has since been studied extensively, including
in [11,23,27].
In our example, it should be clear that Bob will win the 1-coloring game on G.
We have demonstrated a strategy for Alice to win the 2-coloring game on G. Thus,
χg(G) = 2.
1.1
Trees and Forests
We begin by looking at the game chromatic number for trees and forests and
comparing it to the usual chromatic number of a graph, denoted χ(G).
Consider the rooted tree T in Figure 2. The chromatic number of T, χ(T), is 2
since we can alternate colors on each level. In particular, each vertex will have a
different color from its parent and from its children. All children of a vertex v can
have the same color since no children of v are adjacent to each other.
Since every tree can be represented by a rooted tree, we can generalize the
method in the example to show that for any tree, T, χ(T) ≤2.
Now consider the game chromatic number of a tree T. If Bob is always able to
color two children of a vertex, v, different colors, then Alice cannot color T with
two colors, since v will have no legal color. The reader can check that the tree in
Figure 2 has game chromatic number greater than 2. No matter what Alice does,
Bob is able to force a vertex to have no legal color.

Introduction to Competitive Graph Coloring
101
Fig. 2 Tree T.
Fig. 3 The smallest tree T
with χg(T) = 4.
The following theorems give some known results for the game chromatic number
of trees and forests, which will be explored in more detail in Section 2.
Theorem 1 (Faigle, et al. [23]). If F is a forest, then χg(F) ≤4. Moreover, there
exists a forest F with χg(F) = 4.
Theorem 2 (Dunn et al. [18]). Let F be a forest and let ℓ(F) be the length of the
longest path in F. Then χg(F) = 2 if and only if:
1. 1 ≤ℓ(F) ≤2, or
2. ℓ(F) = 3, |V(F)| is odd, and every component with diameter 3 is a path.
Consider the tree in Figure 3. It is the smallest tree T with χg(T) = 4.
Theorem 3 (Dunn et al. [18]). If F is a forest with |V(F)| ≤13 then χg(F) ≤3.
Although we know a bound for the game chromatic number of trees and forests,
classifying trees with game chromatic number 3 or 4 remains open.
It has been well established that the parameter χg(G) has some interesting and
possibly unexpected properties. For example, while it is true that if H is a subgraph
of G then χ(H) ≤χ(G), it is not necessarily the case that χg(H) ≤χg(G). For
example, as discussed in [3], if G = Kn,n and n ≥2 we have that χg(G) = 3.
However, if M is any perfect matching in G, then χg(G −M) = n. So for n ≥4,
χg(G −M) > χg(G).
We now brieﬂy look at some variations of the coloring game.

102
C. Dunn et al.
1.2
The (r, d)-Relaxed Coloring Game
Let r be a positive integer, d be a nonnegative integer, and G be a ﬁnite graph.
Two players, Alice and Bob, play a game on G by coloring the uncolored vertices
with colors from a set X of r colors. At all times, the subgraph induced by a color
class must have maximum degree at most d. Alice wins the game if all vertices
are eventually colored; otherwise, Bob wins. We call this the (r, d)-relaxed coloring
game. In particular, we have modiﬁed the original coloring game by changing the
deﬁnition of a legal color. At any point in the game for any α ∈X, let Cα be the
set of vertices colored α at that point. We say that a color α ∈X is legal for the
uncolored vertex u, if after u is colored α we have that Δ(G[Cα]) ≤d, where G[Cα]
is the subgraph of G induced by the vertices in Cα.
The least r for which Alice has a winning strategy for this game on G is called
the d-relaxed game chromatic number of G, denoted dχg(G). When d = 0, we have
the usual coloring game and the game chromatic number of G, χg(G). The relaxed
game chromatic number has been considered in [6,14–16,26]. We will examine the
d-relaxed game chromatic number further in Section 3.
Suppose that Alice and Bob are playing the (r, d)-relaxed coloring game on a
graph G. For the purposes of analyzing strategies for Alice or Bob, we note that a
color α is legal for an uncolored vertex u if the following two conditions hold:
1. The vertex u has at most d neighbors already colored α.
2. If v is a neighbor of u and v is already colored α, then v has at most d −1
neighbors already colored α.
It might seem that if we increase the defect, then Alice could more easily win
with fewer colors. It is known that if G = Kn,n with n ≥2, then 1χg(G) = n. But
as we saw in Section 1.1, if G = Kn,n and n ≥2 we have that χg(G) = 3. Thus, for
n ≥4, there is a class of graphs for which 0χg(G) = 3 but 1χg(G) ≥4.
Although the (r, d)-relaxed game is a common way to change the deﬁnition
of a legal color, there are many other ways to relax the conditions of the game.
For example, we also explore the notion of a clique-relaxed game in Section 4. In
the k-clique-relaxed r-coloring game on a ﬁnite graph G, a color α is legal for an
uncolored vertex u if coloring u with α does not result in a monochromatic (k + 1)-
clique. As before, we can ﬁnd the least number of colors required for Alice to have
a winning strategy on G. We call this the k-clique-relaxed game chromatic number
of G, denoted χg(k)(G).
1.3
Edge Coloring and Total Coloring
Another variation of the coloring game is to consider the game on edges rather than
vertices. The rules are similar to the vertex coloring game. We begin with a ﬁnite
graph, G, and a set X of r colors. Two players, Alice and Bob, alternate coloring the

Introduction to Competitive Graph Coloring
103
uncolored edges of G using colors from X. At each step of the game, the players
must choose to color an uncolored edge with a legal color. Alice goes ﬁrst. A color
α ∈X is legal for an uncolored edge e if e has no incident edges already colored α.
Alice wins the game if all edges of the graph are colored; otherwise, Bob wins. The
least number of colors required for Alice to have a winning strategy on G is called
the game chromatic index of G, denoted χg′(G). This is explored in Section 5.
Once we have considered edge coloring and vertex coloring, a natural extension
might be to consider total coloring. In a total coloring game, explored in Section 6,
Alice and Bob alternate coloring vertices and edges.
2
Classifying Forests by Game Chromatic Number
In this section we examine the classic coloring game played on forests and trees. In
[23], it was shown that the game chromatic number of a forest is at most 4. In this
section, we focus on the question “Do there exist simple criteria for determining the
game chromatic number of a forest?”
It is trivial to see that only edgeless forests have game chromatic number 1.
Further exploration of this question is in the paper [18]. We will prove some
results from the paper to highlight strategies which are helpful in exploring game
coloring problems. First, we show that forests with game chromatic number 2 have
determining criteria. We also demonstrate how a separator strategy is used to prove
that small trees have game chromatic number at most 3. There is not any known
criteria for differentiating between forests with game chromatic number 3 and 4.
2.1
Forests with Game Chromatic Number 2
We begin by showing the proof of Theorem 2, restated below.
Theorem 2 (Dunn et al. [18]). Let F be a forest and let ℓ(F) be the length of the
longest path in F. Then χg(F) = 2 if and only if:
1. 1 ≤ℓ(F) ≤2, or
2. ℓ(F) = 3, |V(F)| is odd, and every component with diameter 3 is a path.
Proof. Suppose that the condition is not met by some forest F. If ℓ(F) < 1, then
it is easy to see that χg(F) = 1. Thus ℓ(F) ≥3. First, assume that ℓ(F) > 3
and that Alice colors x with α as her ﬁrst move. If there is a vertex y at distance 2
from x then Bob can color y with β. The common neighbor of x and y has no legal
color available, so Bob wins. Thus we assume that there is no such y. Then, some
component of F not containing x must contain a path subgraph with 5 vertices, P5.
Let v1, v2, v3, v4, v5 be the vertices of this P5. Bob will color v3 with α on his ﬁrst
turn, and on his second he can color either v1 or v5 with β and win the 2-coloring
game on F.

104
C. Dunn et al.
Now assume that ℓ(F) = 3 and that Alice colors x with α as her ﬁrst move; if
there is a vertex y at distance 2 from x, then Bob will win the 2-coloring game as
above. Thus x is not in a component with diameter 3. If there is a component with
diameter 3 that is not a path, then this component has a vertex, u, of degree 3 or
more that is adjacent to at least two leaves, v1 and v2. Bob colors v1 with α. Unless
Alice colors u with β, Bob will win on his second move by coloring an uncolored
neighbor of u with β. If Alice colors u with β, then because the component has
diameter 3, there exists a vertex at distance 2 from u. Bob colors this with α and
wins the 2-coloring game on F.
Thus every component with diameter 3 is a path, but |V(F)| is even. Let T1 be a
path component with diameter 3. Because |V(F)| is even Bob can play so that Alice
is the ﬁrst to color a vertex in T1 (unless Bob wins the game before that point). When
Alice colors a vertex x in T1, Bob immediately colors a vertex at distance 2 from x
with a different color and wins the 2-coloring game on F.
Now suppose that a forest F satisﬁes the condition of the theorem. We will show
that Alice can win the 2-coloring game on F. If diam(F) ≤2 then note that Bob
can only win if, in a component of diameter 2, two leaves are colored using different
colors before the central vertex is colored. Therefore, Alice will win the 2-coloring
game by using the following strategy. (1) If possible, color the central vertex in the
component Bob most recently played in. (2) Otherwise, color the central vertex in
a component with no colored vertices. (3) If neither of those are possible, color any
uncolored vertex.
If there are components of diameter 3, then they are all paths and |V(F)| is odd.
By parity, Alice can always choose a vertex to color so that either it is not in a
P4 component or it is in the same P4 component that Bob just played in. Alice’s
strategy is as follows: if Bob colors x in a P4 component using color α, Alice
colors the unique vertex at distance 2 from x with α. If it is Alice’s ﬁrst turn, or
if Bob did not color in a P4 component, then she follows her strategy from the case
where diam(F) ≤2 with the additional restriction that she does not choose a vertex
from a P4 component. Using this strategy, no uncolored vertex will ever have two
differently colored neighbors. Therefore, Alice will win the 2-coloring game on F.
In the proof above, we identify conﬁgurations which allow Bob to win the
r-coloring game (in this case, an uncolored vertex with two differently colored
neighbors), and implement a strategy for Bob (or Alice) which forces (or avoids)
these conﬁgurations. These two methods are at the heart of many proofs in game
coloring, so ﬁnding such conﬁgurations is both a tractable and useful exercise for
students.
2.2
Smallest Tree with Game Chromatic Number 4
Now that forests with game chromatic number 2 are classiﬁed, the investigation
turns to the differences between forests with game chromatic numbers 3 and 4. Some
simple observations can be made: in order for Bob to win the 3-coloring game on a

Introduction to Competitive Graph Coloring
105
Fig. 4 The tree T′ has game
chromatic number 4 by
Theorem 4
x1
x2
x3
x4
Fig. 5 The tree T0; the two
shaded vertices have color α
and unshaded vertices are
uncolored.
x1
x1′
x1′′
x2
x2′
x2′′
forest F there must be a vertex of degree 3 or more. Thus linear forests have game
chromatic number at most 3. It is a natural question to ask how small a forest with
game chromatic number 4 can be.
Using small conﬁgurations on which Bob can win the 3-coloring game, we show
that the graph T′ in Figure 4 is the smallest tree with game chromatic number 4.
Lemma 1. Let T0 be the partially colored tree shown in Figure 5. Bob can win the
3-coloring game on T0.
Proof. Suppose it is Alice’s turn. If she colors x1 or x2, or if she colors any uncolored
leaf with a color other than α, Bob can immediately color so that either x1 or x2 has
no legal color available. Therefore, we may assume without loss of generality that
Alice colors x′
1 with α. Bob will color x′
2 with β. Now if Alice colors x2 with γ, then
Bob can color x′′
1 with β and win. If Alice does not color x2 with γ, then Bob will
color either x1 or x′′
2 with γ and win because x2 has no legal color available.
Suppose instead that it is Bob’s turn. He colors x′
2 with β. Exactly as above, Bob
has a winning response to any move Alice can make.
Theorem 4 (Dunn et al.
[18]). The tree T′ (Figure 4) has game chromatic
number 4.
Proof. We show that χg(T′) = 4 by showing that Bob has a winning strategy when
playing the 3-coloring game on T′. Bob’s goal is to attain the conﬁguration T0 from
Lemma 1. If Alice colors (with α) one of x1, x2, x3, x4 or a leaf adjacent to either x1
or x4, then Bob colors one of the xi at distance 3 away from Alice’s move using α. By
Lemma 1, Bob will win the 3-coloring game. Otherwise, we may assume without

106
C. Dunn et al.
loss of generality that Alice colors a leaf adjacent to x2 with α; Bob responds by
coloring the other leaf adjacent to x2 with β. Unless Alice now colors x2, Bob can
make it uncolorable on his next turn. However, if Alice colors x2 (necessarily with
γ), then Bob can color a leaf of x4 with γ and win by Lemma 1.
The proof of Theorem 4 highlights some of the potential difﬁculty in using
partially colored subgraphs in a larger graph. When creating a strategy for one player
that focuses on a subgraph, an opponent can either color a vertex in that subgraph,
a vertex that is not on that subgraph but affects color choices for uncolored vertices
in the subgraph, or a vertex that has no effect on the subgraph in question.
To show that trees with fewer vertices than T′ have game chromatic number at
most 3, we need some lemmas regarding small trees. More detailed proofs can be
found in [18].
Lemma 2. If T is a tree with |V(G)| ≤13, then there exists a vertex v ∈V(T) such
that every component of T −v has at most 6 vertices.
Proof. Choose v so that the order of the largest component in T −v is minimized.
Lemma 3. If T is a tree on at most 7 vertices, then T has at most two vertices of
degree 3 or more and at most one vertex of degree 4 or more.
Proof. This can be shown by examining the degree sum of a counterexample.
In order to prove Theorem 3, it is useful to describe a separator strategy. At any
point in the game, the forest F will be partially colored. Using the partial coloring,
we deﬁne a collection of trunks as follows. For each colored vertex x with degree d,
split x into d colored vertices, say x1, x2, . . . , xd, so that each xi is colored the same
as x and is adjacent to exactly one neighbor of x. After each colored vertex is split
in this fashion, the resulting graph will be a collection of trees, called trunks. Any
colored vertex in a trunk must be a leaf of that trunk; if every vertex in a trunk is
colored then it is a K2 component. Deﬁned a different way, given a forest F and a
partial coloring, a trunk R is a maximal connected subgraph of F such that every
colored vertex in R is a leaf of R. It is important to note that coloring a vertex in one
trunk has no effect on available colors in another trunk. We now restate and prove
Theorem 3.
Theorem 3 (Dunn et al. [18]). If F is a forest with |V(F)| ≤13 then χg(F) ≤3.
Proof. Let F be a forest with |V(F)| ≤13 and let T be the component with the most
vertices. By Lemma 2, there exists a vertex v ∈V(T) such that every component of
T −v has size at most 6. Alice colors this vertex, and now each trunk of F has order
at most 7 and at most 1 colored vertex.
We call a vertex v dangerous if v has at least as many uncolored neighbors
as legal colors available for it. Alice will win the 3-coloring game if there are
no dangerous vertices in F. We show that Alice has a strategy which eliminates
dangerous vertices in each trunk of F, and therefore has a strategy that will win the

Introduction to Competitive Graph Coloring
107
3-coloring game on F. Alice will always play in the same trunk as Bob; if there
are no uncolored vertices in that trunk Alice will play in a new trunk as if Bob just
colored v.
Let R be the trunk of F where Bob just played. By Lemma 3, R has at most 2
dangerous vertices. If there are fewer than 2 dangerous vertices then Alice will color
any remaining dangerous vertex on her turn, and then no dangerous vertices exist in
R. If there are two dangerous vertices in R, then we consider the following cases:
Case 1 Some dangerous vertex x has no colored neighbors.
Alice colors the other dangerous vertex with any legal color. After Bob’s next
move in this trunk, Alice colors x if it is still uncolored.
Case 2 Some dangerous vertex x has two colored neighbors.
Alice colors x with any available color. Since the colored vertices of R lie on a
path, the other dangerous vertex x′ has at most one colored neighbor. After Bob’s
next move in this trunk, Alice colors x′ if it is still uncolored.
Case 3 Each dangerous vertex has exactly one colored neighbor.
By Lemma 3 at least one dangerous vertex, say x, has degree 3; let the other
dangerous vertex be x′. If x and x′ are not adjacent then Alice can color x ﬁrst and
then x′ later, as in Case 2. If x and x′ are adjacent, then x must have an uncolored
neighbor u distinct from x′. Alice colors u with the same color as the previously
colored neighbor of x. Therefore, x is no longer a dangerous vertex, and x′ has only
1 colored neighbor. After Bob’s next move in this trunk, Alice can color x′ if it is
still uncolored.
In all cases, Alice is able to eliminate the dangerous vertices before Bob is able to
surround any vertex with 3 distinct colors. Therefore, Alice can win the 3-coloring
game on F.
By using this trunk coloring strategy and considering the different cases on
maximum degree and diameter, it can be shown that T′ is in fact the unique forest
on 14 vertices with game chromatic number 4 [18]. A main idea in this proof is that
if there are few dangerous vertices, then Alice will have an easier time creating a
winning strategy. Further utilizing this idea, one can prove some conditions under
which a forest F has χg(F) ≤3.
Theorem 5 (Dunn et al. [18]). A forest F has game chromatic number at most 3
if there exists a vertex b such that, if Alice starts by coloring b, every trunk R of F
either
1. has no neighboring vertices whose degrees are both 3 or more, or
2. has a vertex v ∈R with degree 3 or more which is adjacent to every vertex in R
with degree 3 or more, and v is not adjacent to a colored vertex.
In fact, [18] proves something slightly stronger, but we omit that statement here
in order to avoid a technical deﬁnition.

108
C. Dunn et al.
The proofs of Theorems 4 and 3 might lead one to believe that, if we can restrict
the maximum degree of a forest, then Bob will not have the ﬂexibility required to
win the 3-coloring game. However, the following result shows that this is false.
Theorem 6 (Dunn et al.
[18]). There exists a tree T with Δ(T) = 3 and
χg(T) = 4.
When outlining a strategy for Alice in the 3-coloring game on trees with
maximum degree 3, the authors of [18] found a partially colored subgraph for
which the strategy did not work. In fact, Bob could win the 3-coloring game on
this subgraph. Working backwards, the authors pieced together a tree on which Bob
can force this partially colored subgraph to occur.
As an aside, this is not an uncommon path towards interesting results. Recently,
Steinberg’s Conjecture (every planar graph without 4-cycles and 5-cycles is 3-
colorable) was proven to be false [7] using this “method”. As a team of mathe-
maticians were working on a lemma needed to prove the conjecture, a gap in the
proof turned into a counterexample to that lemma, which led to a counterexample
to the entire conjecture!
Every known example of trees with Δ(T) = 3 and χg(T) = 4 have even order;
the proof of Theorem 6 relies on the fact that Alice cannot avoid coloring ﬁrst
in a particular subgraph. One unanswered question regarding forests with game
chromatic number 3 and 4 is if it is possible that maximum degree 3 implies
χg(T) ≤3 for trees with odd order.
3
Relaxed-Coloring Games
In this section we will consider a variation of the game in which the subgraphs
induced by each color class must satisfy the condition of having maximum degree
bounded by a predetermined constant d. In this version of the game, the players are
in the process of creating a defect coloring or relaxed coloring of the graph. Such
colorings have been examined in [8–10,21]. To highlight some of the strategies that
have been employed to provide upper bounds on the associated parameter, we will
provide results with trees. However, these strategies have been useful with many
classes of graphs.
First we will deﬁne the game, which we will refer to as the (r, d)-relaxed coloring
game. Let G be a ﬁnite graph and let X be a set of r colors, for some positive integer
r. Let d be a nonnegative integer. We call d the defect. For each α ∈X, we call
the set of all vertices colored α the color class of α, denoted Cα. In this version
of the game, a color α ∈X is legal for an uncolored vertex u if, after u has been
colored α, the subgraph induced by color class Cα must have maximum degree at
most d. More succinctly, at every point in the game, for every α ∈X, it must be true
that Δ(G[Cα]) ≤d. Note that if d = 0, this is the original version of the coloring
game. For a ﬁxed d, the least r such that Alice has a winning strategy for this game
is called the d-relaxed game chromatic number of G and is denoted by dχg(G). For
a ﬁxed r, the least d for which Alice has a winning strategy for this game is called
the r-game defect of G and is denoted by defg(G, r).

Introduction to Competitive Graph Coloring
109
At any time in the game, we deﬁne the defect of a colored vertex x ∈Cα to be
the number of neighbors of x already colored α. We denote this value by def(x).
In terms of the analysis of a strategy, one must evaluate two things when
considering the color α for the uncolored vertex u:
1. the vertex u must be adjacent to at most d vertices already colored α; and
2. if v is adjacent to u and v has already been colored α, then v can be adjacent to
at most d −1 vertices already colored α.
Theorem 7 (Chou et al. [6]). If T is a tree, then 1χg(T) ≤3.
We will present two proofs of this theorem. First, we will provide the argument
used by Chou, Wang, and Zhu in [6]. This uses a separator strategy as introduced
in Section 2. The second result uses an activation strategy. Activation strategies
have proven to be quite useful for many classes of graphs, and can be seen as the
culmination of the work in a number of papers [11,23,27,28,30,31]. We will present
a number of activation strategies in later sections.
Proof. (Separator Argument) Suppose in the process of the game, the tree T is
partially colored. We obtain a collection of subtrees as follows: for each colored
vertex x with degree d, we split x into d colored vertices, say x1, x2, . . . , xd, so that
each xi is colored the same color as x and is incident with exactly one of the original
edges incident to x in T. After splitting each of the colored vertices of T, we obtain
a collection of smaller partially colored trees, say T1, T2, . . . , Tm, which we will call
trunks, such that ∪m
i=1E(Ti) = E(T). Note that in each Ti, it is the case that only
some of the leaves may be colored.
Alice’s goal in selecting the vertices to color is simply to ensure that after she
has colored her chosen vertex, each of the trunks of the partially colored T has at
most two colored leaves. Suppose Alice can achieve this goal. Then after Bob colors
a vertex, each trunk Ti has at most two colored vertices, with the exception of one
trunk which may have three colored vertices. Moreover, if the trunk Ti has three
colored leaves, then one of those leaves was just colored by Bob. We will call this
the new colored leaf of Ti.
It is easy to prove inductively that Alice can achieve her goal. If after Bob’s move
there is a trunk Ti containing three colored leaves, then Alice will choose the vertex
that lies at the intersection of the paths joining these three colored vertices. Suppose
Alice has chosen vertex u ∈V(Ti) by this process. She will then choose a color for
u as follows: if the colored leaves of Ti use at most two colors, Alice will choose a
color that has not been used on Ti. Otherwise, Ti has three distinctly colored leaves.
Alice will color u with the color of the new colored leaf, v, of Ti.
To show that this works, it sufﬁces to show that in the case that Ti has three
distinctly colored leaves, then the color of v is legal for u. This is obvious as v has
no other colored neighbors. Thus, Alice can color u with the color assigned to v.
This will affect the defect of at most two vertices: u and v. Each will then have
defect of at most 1. Thus, following this strategy, all vertices of T will be colored
and Alice will win.

110
C. Dunn et al.
For the second proof of this result, we will deﬁne what we will call the Tree
Strategy for Alice. Suppose Alice and Bob are playing the coloring game on the
tree T = (V, E). Alice picks a vertex r and orients all edges in the tree toward r.
The resulting orientation on T has the property that every vertex v ∈V \ { r } has
a unique outneighbor which we denote p(v). We call p(v) the parent of v and refer
to v as a child of p(v). Continuing this notation, let p2(v) = p(p(v)). Inductively, if
pi(v) is deﬁned, let pi+1(v) = p(pi(v)). We deﬁne the set of descendants of a vertex
v by
G(v) := { w ∈V | v = pk(w) for some k }.
We then deﬁne G[v] := G(v) ∪{ v }.
Throughout the game, vertices go from uncolored to colored. At any time in the
game, let U be the set of uncolored vertices and C be the set of colored vertices.
Alice will maintain a set A of active vertices. One way of viewing the notion of an
active vertex is to think of it like a post-it note that Alice places on vertices that are
becoming dangerous, yet not requiring immediate attention. When her strategy leads
her to consider a vertex that already has such a post-it note, she knows that it is now
time to color this vertex. Alice will only color vertices that are active; however, she
may activate a vertex and color it on the same turn. For simplicity, we will assume
that any vertex that Bob colors also becomes active; thus C ⊆A. Whenever a vertex
is colored it is added to C, and whenever a vertex is activated it is added to A. When
a vertex v is colored, let c(v) be that color. For any color α ∈X, we say that α is
eligible for a vertex v if either p(v) ∈U or c(p(v)) ̸= α. Note that if |X| ≥2, every
uncolored vertex must have at least one eligible color.
Tree Strategy
On her ﬁrst move, Alice will color the vertex r with any color. Suppose Bob has just
colored vertex b. Alice’s strategy will have two stages: a search stage and a coloring
stage. In the search stage, she seeks the vertex u that she will color. In the coloring
stage, she selects the color for u.
Search Stage
•
Initial Step
– If p(b) ∈U, then set x := p(b) and move to the recursive step.
– If p(b) ∈C, c(b) = c(p(b)), and p2(b) ∈U, then set u := p2(b) and move to
the coloring stage.
– Otherwise, let u be any uncolored vertex such that p(u) ∈C and move to the
coloring stage, activating u if u is inactive.
•
Recursive Step
– If x /∈A and p(x) ∈U, then activate x, set x := p(x) and repeat the recursive
step.
– Otherwise, activate x if it is inactive, set u := x, and move to the coloring
stage.

Introduction to Competitive Graph Coloring
111
Coloring Stage
•
Choose an eligible color for u which minimizes def(u).
We are now ready to provide our second proof of Theorem 7.
Proof. (Activation Argument) Let T be a tree and let |X| = 3. Alice will employ the
Tree Strategy deﬁned above. Suppose u = p(v) and u is uncolored. Then the ﬁrst
time a vertex in G[v] is activated, necessarily by Bob, Alice will take action at u by
coloring (if u is already active) or by activating.
Suppose that Alice has chosen to color vertex u. Suppose x and y are the ﬁrst two
children of u to be activated, and x is activated ﬁrst. Note that on the turn that x is
activated, Alice takes action at u by activating it. Assuming that she does not also
color u on this turn, suppose that w is the ﬁrst vertex in G[y] to be activated. Then
when w is activated (by Bob coloring it), y is activated and Alice moves to color u.
Now suppose that Alice is choosing a color for u. First note that u has at most
three colored neighbors: p(u) and at most two children, again say x and y. If the
number of colors used on these three neighbors is at most two, then Alice’s strategy
dictates that she will use a color not used in the set of neighbors of u, N(u).
Otherwise, it must be the case that p(u), x, and y are all colored distinctly. Since
there is only one vertex in G[y] colored, then Alice can safely color u with c(y) as
this results in def(u) = def(y) = 1, with the defect of no other vertex affected. We
note that Bob can borrow this strategy at any time. So at any time in the game, it is
possible to color any uncolored vertex with a legal color. Thus, all vertices of T will
eventually be colored, and Alice will win.
We note that while the bound in Theorem 7 is known to be sharp [6], we do not
know the necessary and sufﬁcient characteristics of a tree T to determine whether
1χg(T) = 3 or 1χg(T) = 2.
4
The Clique-Relaxed Game
Earlier, we examined the graph coloring game using d-relaxed coloring rules on
vertices and on edges. Exploring different modiﬁcations to standard coloring rules
can open up a wide range of interesting competitive coloring problems. Another
relaxation of standard coloring rules is clique-relaxed coloring, where the aim is to
avoid large monochromatic cliques (complete subgraphs).
In this section, we examine the game coloring version of clique-relaxed coloring;
we focus on planar and outerplanar graphs. A planar graph is a graph that can be
drawn in the plane such that no edges cross each other. Each region bounded by the
edges is called a face and the inﬁnitely large unbounded face is called the outer face.
One speciﬁc type of planar graph is an outerplanar graph, which is a planar graph
that can be drawn so that every vertex belongs to the outer face.

112
C. Dunn et al.
Recall that ω(G) denotes the size of the largest clique in G. We say that a vertex
coloring of a graph G is a proper k-clique-relaxed coloring if ω(H) ≤k for each
subgraph H induced by one of the color classes. The k-clique-relaxed chromatic
number of G, denoted χ(k)(G), is the smallest k such that G has a proper k-clique-
relaxed coloring.
Although there appears to be very little proven about this parameter in the
literature, it seems like a natural variation of other coloring relaxations and has
interesting properties with regards to competitive graph coloring. Notice that for
any graph G, we get χ(1)(G) = 0χg(G) = χ(G) and that χ(k)(G) ≤χ(k−1)(G) for
every positive integer k. The following theorem gives an upper bound for χ(k)(G) in
terms of the standard chromatic number χ(G).
Theorem 8 (Dunn et al. [17]). Let G be a graph. Then χ(k)(G) ≤

χ(G)
k

for any
positive integer k.
Proof. Let G be a graph where χ(G) = r. Color the vertices of G properly with r
colors which we denote α1, . . . , αr. We divide the r colors into s :=
 r
k

groups, all
of size exactly k except possibly the last group. We label these groups A1, . . . , As and
recolor all vertices in Ai with color βi. There are s colors used in this new coloring.
Suppose, for the sake of contradiction, that H ⊆G is a (k + 1)-clique where each
vertex of H receives color βi. Then by the pigeonhole principle, at least two vertices
x and y in Ai were colored αj originally for some j. Thus xy cannot be an edge of G,
which contradicts the fact that H is a (k + 1)-clique.
Using this theorem we are able to give bounds for χ(k)(G) for some classes of
graphs. The Four Color Theorem [2] shows that χ(G) ≤4 for all planar graphs
G. Also, it is also easy to show that χ(G) ≤3 for outerplanar graphs G. These
observations lead to the following corollary.
Corollary 1. If G is a planar graph, then χ(k)(G) ≤2 when 2 ≤k ≤3 and
χ(k)(G) = 1 when k ≥4. Moreover, if G is an outerplanar graph, then χ(2)(G) ≤2
and χ(k)(G) = 1 when k ≥3.
These bounds are sharp as K4 is a planar graph with χ(k)(K4) = 2 when 2 ≤k ≤3
and K3 is an outerplanar graph with χ(2)(K3) = 2.
To play the k-clique-relaxed r-coloring game on a graph G, two players (Alice
and Bob) will take turns coloring uncolored vertices of G with legal colors coming
from a ﬁxed set X of r colors. A color α ∈X is legal for an uncolored vertex u if
coloring u with α does not create a monochromatic (k+1)-clique. Said another way,
α is not a legal color for u if G[N(u)] contains a k-clique H where each vertex of H
is colored α. Alice always colors ﬁrst, and she wins the game when all the vertices
are colored. Therefore, Bob will win if there is at least one uncolored vertex u in G
whose neighborhood contains monochromatic k-cliques in each of the r colors. The
k-clique-relaxed game chromatic number of G, denoted χg(k)(G), is the least r such
that Alice has a winning strategy in the k-clique-relaxed r-coloring game.

Introduction to Competitive Graph Coloring
113
The ﬁrst observation is that χg(1)(G) = 0χg(G). In [17], the authors investigate
the k-clique-relaxed game chromatic number on outerplanar graphs. Because the
maximum clique size in an outerplanar graph is 3, it follows that χg(k)(G) = 1
when G is an outerplanar graph and k ≥3. Therefore, the focus is on the 2-clique-
relaxed coloring game. In the following result, Alice’s strategy is to use a vertex
ordering given by Guan and Zhu [25] to implement a separator strategy similar to
the one used on trees (see Section 3).
Theorem 9 (Dunn et al.
[17]). Let G be an outerplanar graph. Then
χg(2)(G) ≤4.
Proof. Let G be an outerplanar graph. Alice’s strategy is to deﬁne auxiliary graphs
G′ and T, and to use these graphs to choose which vertex to color. First, let G′ be
the graph obtained by adding edges to G until the graph is maximally outerplanar.
Guan and Zhu [25] showed that for every maximally planar graph, there is a linear
ordering L := v1, . . . , vn of the vertices of H such that v1v2 is on the outer face
and, for all i ≥3, the vertex vi is adjacent to exactly two vertices va(i) and vb(i)
such that a(i) < b(i) < i. We call va(i) and vb(i) the major parent and minor parent
(respectively) of vi.
To create the graph T, Alice deletes from G′ all the edges of the form vivb(i).
Note that v1v2 is still an edge and for each i ≥3 the vertex vi is adjacent to exactly
one vertex with lower index; thus, T must be a tree. As in the separator strategy
of Section 3, Alice can ensure that after her turn each trunk of T has at most two
colored vertices. Bob may possibly color a third vertex in a trunk, so each uncolored
vertex vi that Alice chooses has at most 3 colored neighbors in T. It is possible that
vi has more colored neighbors in G, as vivb(i) could be an edge in G. Further, [25]
showed that each vertex in G′ is the minor parent to at most two vertices. Therefore
vi is adjacent to at most six colored vertices in the original graph G. In the 2-clique
relaxed 4-coloring game, a vertex will have a legal color unless it is adjacent to
monochromatic K2 subgraphs in each of the 4 colors. Because vi is adjacent to at
most 6 colored vertices, there is a legal color for Alice to use.
Bob will also always have a legal move, because Alice’s strategy leaves at most
two colored vertices in each trunk of T. Therefore, uncolored vertices have at most
5 colored neighbors in G on Bob’s turn.
The bound in Theorem 9 has no sharpness example; in [17] an example is given
where Bob has a winning strategy for the 2-clique-relaxed 2-coloring game on
an outerplanar graph. Bob’s strategy uses the symmetry of the graph to create a
particular partially colored subgraph on which an uncolored vertex can be made
uncolorable. It remains open whether or not there exists an outerplanar graph where
Bob has a winning strategy for the 2-clique-relaxed 3-coloring game.
Theorem 10 (Dunn et al.
[17]). There exists an outerplanar graph G with
χ(2)(G) ≥3.

114
C. Dunn et al.
Fig. 6 An outerplanar graph
with χ(2)(G) ≥3.
v
u1
u2
u3
Figure 6 provides an example of an outerplanar graph with χ(2)(G) ≥3. The
proof can be found in [17].
This raises the question of whether there exists an outerplanar graph G such that
χg(2)(G) = 4. Further results in [17] show a subclass of outerplanar graphs in
which no such example can be found.
The question of clique-relaxed coloring games on planar graphs also remains
open. Because the maximum clique size in a planar graph is four, the games of
interest on planar graphs are the 2- and 3-clique-relaxed games.
5
Edge Coloring
The focus of this section is a variation of the game in which Alice and Bob alternate
coloring edges rather than vertices. The most obvious consequence of analyzing this
version of the game is that the maximum degree of the graph becomes an important
component of the upper bounds for the corresponding parameters.
Let G be a ﬁnite graph and let r be a positive integer and d be a nonnegative
integer. As used before, d is the defect and X is a set of r colors. The players alternate
coloring, with Alice coloring an edge ﬁrst. We say that a color α ∈X is legal for an
uncolored edge e if the following conditions are satisﬁed:
1. the edge e is incident with at most d edges already colored α; and
2. if e′ is an edge incident to e and e′ has already been colored α, then e′ is adjacent
to at most d −1 edges already colored α.
Note that if e is colored α, then at this point in the game, every edge has at most
d neighbors colored α. Alice wins if every edge is eventually legally colored. Bob
wins if there comes a time in the game when there is an uncolored edge for which

Introduction to Competitive Graph Coloring
115
Fig. 7 For an edge
e = xp(x), the vertex
p(p(x)) = p2(x), the edge
p(e), and the sets B(e) and
S(e).
p(p(x))
p(x)
x
p(e)
e
B(e)
S(e)
no legal color exists. For a ﬁxed defect d, the least r such that Alice has a winning
strategy for this game is called the d-relaxed game chromatic index of G, denoted
dχg′(G). Similarly, for a ﬁxed r, the r-edge-game defect of G, denoted defg
′(G, r),
is the least d such that Alice has a winning strategy. This game was ﬁrst introduced
in [13].
We will present some of the work done in [19] restricted to trees, which
generalized the results in [13]. Further work in this area can be seen in [1,5,22,29].
We begin by deﬁning terminology and notation, and by providing a winning strategy
for Alice in the edge coloring game on trees. Let T be a tree with Δ(T) = Δ for
some positive integer Δ. For her strategy, Alice chooses an arbitrary leaf r ∈V(T)
at which she roots T. She then regards all edges in T as oriented toward r. Let e0 be
the unique edge in T that is incident to r. For each vertex v ∈V \{ r }, deﬁne p(v) to
be the unique outneighbor of v. Then for each edge e ∈E, there is a unique vertex
x ∈V such that e = xp(x). We now introduce some terminology, as illustrated in
Figure 7.
For every edge e = xp(x) with e ̸= e0, deﬁne the parent of e, denoted p(e), to
be the edge p(x)p2(x), where p2(x) = p(p(x)). We say that e is a child of p(e).
Note that, because p(x) is well deﬁned, p(e) is also well deﬁned. Whenever pi(e) is
deﬁned and pi(e) is not incident with the root, deﬁne pi+1(e) = p(pi(e)). As in the
second (activation) proof of Theorem 7, we deﬁne the descendants of e to be
G(e) = {e′ ∈E | e = pk(e′) for some positive integer k}.
For each edge e = xp(x), deﬁne the siblings of e to be
B(e) = {yp(y) ∈E | p(y) = p(x) and y ̸= x}
and B[e] = B(e) ∪{e}. Deﬁne the children of e to be
S(e) = {yp(y) ∈E | x = p(y)}.
We call the set of all edges incident to an edge e the neighborhood of e, denoted
N(e).

116
C. Dunn et al.
Fix j ∈[Δ −3] and let X be a set of Δ −j colors. Note that |X| ≥3. At any point
in the game, let C and U be the set of colored and uncolored edges, respectively. For
each α ∈X, we call the set of all edges colored α the color class of α, denoted Cα.
For a colored edge e, denote the color of e by c(e).
Similar to the notion with vertices, for each colored edge e, deﬁne the defect of
e to be the number of neighbors of e colored with c(e). If e is uncolored, we set the
defect of e to be zero. We denote the defect of e by def(e). Thus
def(e) =
  N(e) ∩Cc(e)
  ,
if e ∈C;
0,
otherwise.
We say that color α ∈X is eligible for edge e if p(e) /∈Cα. We denote the set
of eligible colors for e by X(e). When coloring an edge e, Alice always chooses an
eligible color. Note that since |X| ≥3, this is always possible.
In most strategies for vertex-coloring games, Alice avoids increasing the defect
of a vertex when she can. The interesting aspect of this edge strategy is that in some
cases, it will be necessary for her to do just the opposite. She will attempt to reach a
minimum threshold for the defect of some edges. For any edge e, we say that B[e] is
secure if there exist edges e1, e2, . . . , ej+1 ∈B(e) and a color α such that c(ei) = α
for i ∈[j + 1]. In other words, B[e] is secure if e has j + 1 siblings colored with the
same color. Note that if B[e] is secure, then the number of distinctly colored siblings
of e is at most
|B(e) \ {e1, e2, . . . , ej}| ≤Δ −j −2.
As |X(e)| ≥Δ −j −1, there is always a legal eligible color for an uncolored edge
e when B[e] is secure.
We will now deﬁne the strategy that Alice will use for this game with trees. This
strategy is a modiﬁcation of the activation strategy developed in [13]. In response to
Bob’s moves, Alice designates certain edges active, as above in the second proof of
Theorem 7; precisely how she chooses these edges will be explained below. When
an edge e becomes active, we say that e has been activated. In addition, all colored
edges are active. We denote the set of active edges by A, and note that C ⊆A. This
set has the property that once an edge e is in A, e will remain active for the remainder
of the game.
Tree Strategy for Edges
Alice begins the game by coloring e0 with any color. Suppose now that Bob has
just colored edge b = xp(x) in T for some x ∈V \ { r } and some i ∈[n]. Alice’s
response has two stages: a Search Stage and a Coloring Stage. In the Search Stage,
Alice ﬁnds an edge e to color. In the Coloring Stage, Alice chooses a color for e.

Introduction to Competitive Graph Coloring
117
Search Stage
•
If p(b) ∈U, then activate each edge along the (x, r)-path until reaching an edge g
with p(g) ∈A. [Note that this includes Alice activating the edge b.] If p(g) ∈C,
set e := g. Otherwise, set e := p(g).
•
If p(b) ∈C with c(p(b)) = c(b) and p2(b) ∈U, then set e := p2(b).
•
If p(b) ∈C with c(p(b)) = c(b), p2(b) ∈C, and B(p(b)) ∩U ̸= ∅, then set e to
be any uncolored sibling of p(b).
•
Otherwise, set e to be any uncolored edge whose parent is colored.
Coloring Stage
•
If B[e] is secure, then color e with an eligible color for e that does not appear
among the siblings of e.
•
Otherwise, B[e] is not secure. Let f be the last edge to be colored with a color
eligible for e such that c(f) = c(p(f)) and p(f) ∈B(e). If such an edge exists,
then color e with c(f). If no such edge exists, then color e with any eligible color
for e that minimizes def(e).
We now present the theorem and proof from [19] for trees.
Theorem 11 (Dunn et al. [19]). Let T be a tree and Δ(T) = Δ for some positive
integer Δ. Let j be an integer with 0 ≤j ≤Δ −1, and deﬁne h(j) = 2j + 2. Then
defg
′(T, Δ −j) ≤h(j). Moreover, if d ≥h(j) then dχg′(T) ≤Δ −j.
Proof. Suppose that Alice and Bob are playing the (Δ−j, d)-relaxed edge coloring
game on T for some d ≥h(j). Note that when either j = Δ −1 or j = Δ −2, the
result is immediate. Hence, it will sufﬁce to consider the game with color set X with
|X| ≥3. We will assume that Alice uses the Tree Strategy for Edges.
Claim. If e ∈U, then e has at most two active children. Furthermore, when e has
two active children, Alice colors e.
Proof. Let f be the ﬁrst active child of e. When Alice activates f, she also activates
e. Note that while e is uncolored, Alice never colors an edge in G(e) \ G(f) before
Bob. If Bob colors an edge b ∈G(e) \ G(f), Alice activates p(b), p2(b), . . ., and so
on, until she reaches e. Since e is active, Alice colors e.
Claim. Suppose that Alice has chosen to color edge e with α ∈X. Then at the end
of Alice’s turn, def(e) ≤j + 2.
Proof. Since α ∈X(e), then p(e) does not contribute to the defect of e. By Claim 5,
e has at most two active children; hence, e has at most two children colored α. If
B[e] is secure, then Alice would have chosen a color that does not appear among the
siblings of e. In this case, def(e) ≤2. Otherwise, when B[e] is not secure, there are
at most j siblings of e colored α. Thus, def(e) ≤j + 2.

118
C. Dunn et al.
Claim. Suppose that e is about to be colored α and p(e) ∈U. Then e has at most
one child colored α. Furthermore, if e has a child colored α, then Bob colors e and
Alice colors p(e).
Proof. If some sibling f ∈B(e) is the ﬁrst active child of p(e), then Alice colors
p(e) when the ﬁrst edge in G(p(e)) \ G(f) is activated. Since p(e) is uncolored and
e is to be colored, we conclude that e has no active children and hence no children
colored. So assume that e is the ﬁrst active child of p(e). Note that p(e) ∈A. If
an edge in G(p(e)) \ G(e) is then activated, Alice colors p(e). Otherwise, we may
assume that e has no colored siblings at the time when e is colored. Before e is
colored, it is incident with at most two colored edges, which are children of e. Since
|X| ≥3, there is a color that does not appear on any child of e. Then, because Alice
will choose a color to minimize def(e), Alice never chooses to color e with α if a
child of e has already been colored α. So, if e has two active children before e is
colored, then Alice colors e with α only when neither child is colored α. Thus, if
e has a child colored α, then Bob must be coloring e with α, and since p(e) ∈A,
Alice responds by coloring p(e).
Suppose f ∈S(e)∩Cα. By Claim 5, if f has a child colored α before f is colored,
then Bob must have colored f and Alice responds by coloring e. Thus def(f) = 2
once e is colored. Otherwise, f has no children colored α before f is colored. Since e
has at most two active children before e is colored, f has at most one sibling colored
α before e is colored. Then def(f) ≤2 once e is colored.
Now consider the siblings of e. If B[e] is secure, since Alice is choosing to color
e with α, then α does not appear among the siblings of e. Hence, coloring e with α
does not affect the defect of any edge in B(e).
Finally, we consider the case where B[e] is not secure.
Claim. Suppose Alice has chosen to color edge e with α ∈X and B[e] is not secure.
If there exists an edge f ∈B(e) ∩Cα, then def(f) ≤2j + 2 once e is colored.
Proof. Let E′ = B(e) ∩Cα. Since B[e] is not secure, we have that |E′| ≤j. Let
f ∈E′ such that |S(f) ∩Cα| is maximal, and let
S(f) ∩Cα = {s1, s2, . . . , sm},
where i < j implies that si is colored before sj. We show that m ≤|E′| + 2. By
Claim 5, f has at most two active children before it is colored. Hence, f has at most
two children colored α before f is colored. So only the following cases need be
considered:
Case 1 The edge f is colored before s1.
Since p(si) = f for each i ∈[n] and c(f) = α, Alice does not color any si. For
each si that Bob colors, Alice then colors p(e) if p(e) ∈U, an edge in E′ \ {f} if
p(e) ∈C, or e if E′ ∩U = ∅. Then at most |E′| + 1 children of f are colored α
before Alice colors e. Hence, m ≤|E′| + 1.

Introduction to Competitive Graph Coloring
119
Case 2 The edge f is colored after s1 and before s2.
Alice does not color si for any i ≥2. As in the previous case, when Bob colors
si with i ≥2, Alice then colors p(e), an edge in E′ \ {f}, or e. Thus, once f ∈C, at
most |E′| + 1 children of f are colored α. Including s1, we have that m ≤|E′| + 2.
Case 3 The edge f is colored after s2.
If p(e) ∈U, then Claim 5 implies that f has at most one child colored α before
f is colored. Since f has two children colored α before f is colored, p(e) must be
colored before f. Furthermore, Alice colors f immediately after s2 is colored, as
s1 and s2 must be the ﬁrst two active children of f. Once f is colored, each time
Bob colors a child of f with α, Alice colors an edge in E′ \ {f}. Therefore, once f
is colored, Bob can color at most |E′| children of f with α before e is colored. So
m ≤|E′| + 2.
Thus, in all cases, we have that
m = |S(f) ∩Cα| ≤|E′| + 2 ≤j + 2
once e is colored. Since f was chosen to maximize |S(f) ∩Cα| and each f ′ ∈E′ has
at most j siblings colored α before e is colored, we have that
def(f ′) ≤|S(f) ∩Cα| + j ≤2j + 2
for all f ′ ∈E′.
Note now that if Alice is coloring edge e with α, according to the Tree Strategy
for Edges, Claim 5 guarantees that def(e) ≤h(j). We have also shown that for any
edge f ∈N(e) ∩Cα, immediately after e is colored, def(f) ≤h(j). As Bob may
adopt Alice’s strategy at any point in the game, every edge is eventually colored,
and Alice wins the game. Thus
defg
′(T, Δ −j) ≤2j + 2 = h(j).
Moreover, if the game is being playing with some defect d > 2j + 2, and an edge e
eventually has defect at least d, then it must be through the actions of Bob that this
occurs. At the time that e is uncolored, the above arguments show that it is possible
to color e with an eligible color α such that coloring e does not increase the defect
of any edge e′ with def(e′) > 2j + 2. Thus, for any d ≥h(j), we have that
dχg
′(T) ≤Δ −j,
as desired.
In addition to possible improvements to the above bounds, there are many
properties of χg′(G) and dχg′(G) that remain to be studied. This variation of
the coloring game would be ideal for asking additional questions and examining
additional classes of graphs. While the above result is generalized in [19] to a larger
class of graphs (k-degenerate), no other classes have yet been considered.

120
C. Dunn et al.
6
Total Coloring
The total coloring game is a variation of the original coloring game in which Alice
and Bob are free to color vertices or edges on their turns. For the remainder of this
section, we will refer to vertices and edges as elements of the graph. The components
of the game are a ﬁnite graph G and a ﬁnite set of colors X with |X| = r. At any
point in the game, a color α ∈X is legal for an uncolored vertex u if u has no
neighbors colored α and is incident with no edges colored α. Similarly, α is legal
for an uncolored edge e if e is not incident with any edges colored α and neither
endpoint of e is colored α. Alice wins the game if all of the elements of G are
colored. Bob wins otherwise. In other words, Bob wins if there comes a time in the
game when there is an uncolored element for which no legal color exists. The least
r such that Alice has a winning strategy is called the total game chromatic number
of G and is denoted χg′′(G).
The following work is currently in preparation [20]. To prove the following
theorem, we will again have Alice employ an activation strategy, which we will
refer to as the Total Activation Strategy. Let G = (V, E) be a graph and ﬁx a linear
ordering L of V. Note that L lexicographically induces a linear ordering L of E. For
any element a we use L and L to separate N(a) into two sets, N+(a) and N−(a);
an element b ∈N(a) is only in N+(a) if b < a. Let N+[a] := N+(a) ∪{a}
and N−[a] := N−(a) ∪{a}. At any time in the game, let Uv and Ue be the sets of
uncolored vertices and edges of G, respectively. Alice will maintain two sets, Av and
Ae of active vertices and edges, respectively. Once an element has become active, it
will remain active for the remainder of the game. For a given vertex v, at any given
time in the game let m(v) = minL (N+[v] ∩Uv) be the mother of v. Similarly, for
a given edge e, we deﬁne m(e) = minL (N+[e] ∩Ue) to be the mother of e. We
note that the mother of any uncolored element must exist, as the element itself is a
candidate.
Total Activation Strategy
On Alice’s ﬁrst turn, she activates and colors the ﬁrst vertex in L. Now suppose that
Bob has just colored element b of type t, where t ∈{ v, e } and { t, t } = { v, e }.
(We again assume that Bob also activates b if it is not already active.) Alice must
now search for the element she will color and choose a color for this element.
Search Stage
•
Initial Step
– If m(b) exists, then set x := m(b) and move to the recursive step.
– If m(b) does not exist and Ut ̸= ∅, let u be the least element of Ut and move
to the coloring stage.
– Otherwise, let u be the least element of Ut and move to the coloring stage.
•
Recursive Step
– If x is active, set u := x and move to the coloring stage.
– Otherwise, activate x, set x := m(x), and repeat the recursive step.

Introduction to Competitive Graph Coloring
121
Coloring Stage
•
Color u with any color legal for u.
Theorem 12 (Dunn et al. [20]). If F is a forest with Δ(F) = Δ, then χg′′(F) ≤
Δ + 4.
Proof. Suppose Alice and Bob are playing the total coloring game on F with color
set X where |X| = Δ + 4. We will assume that Alice will use the activation strategy
outlined above. We will now show that for any uncolored element, there is always a
legal color available at any point in the game.
Suppose v is an uncolored vertex. At any time in the game, v is incident with at
most Δ edges, possibly all distinctly colored before v is activated. Additionally, at
most one vertex in N+(v) can be colored before v becomes active. Finally, at most
two vertices in N−(v) can be activated (or colored) before v is colored: one which
activates v, and one which forces Alice to immediately color v. Thus, v is incident
or adjacent to at most Δ + 3 uniquely colored elements before v is colored.
Now suppose that e = xy is an uncolored edge and x > y in L. At any point
in the game, e can be incident with at most two distinctly colored vertices before e
becomes active. In addition, at most Δ −1 edges in P(e) ∪B(e) can be distinctly
colored before e becomes active. Finally, two children of e can be activated (or
colored) before e is colored: one to activate e and one to trigger the coloring of e.
Thus, e is incident with at most Δ + 3 colored elements before it is colored.
Since Δ + 4 colors are available, and given that Bob may utilize Alice’s strategy,
Alice will win. Thus, χg′′(F) ≤Δ + 4.
A chordal graph is a graph in which every cycle subgraph with 4 or more vertices
has a chord; in other words, a chordal graph does not have Cn as an induced subgraph
for any n ≥4.
Theorem 13 (Dunn et al. [20]). If G is a chordal graph with Δ(G) = Δ and
ω(G) = k + 1, then χg′′(G) ≤Δ + 3k + 2.
Proof. Let X be a set of colors with |X| = Δ + 3k + 2 and suppose that Alice and
Bob are playing the total coloring game on G with X. We will assume that Alice
employs the Total Activation Strategy deﬁned above. We will show that at any time
in the game, any uncolored element has an eligible legal color available.
Suppose that v is an uncolored vertex. First, it is clear that there are at most Δ
edges incident with v that could be colored before v is activated (or colored). Also,
every vertex in N+(v) may be colored before v is colored. Let S ⊆N−(v) be the
set of all children of v that are activated before v is colored. Note that for any vertex
w ∈S, it must be the case that m(w) ∈N+[v] since G is a chordal graph. Thus, every
time a vertex in S is activated, Alice will respond by either activating or coloring a
vertex in N+[v]. So initially, this yields |S| ≤2 |N+[v]| ≤2(k + 1) = 2k + 2.
However, we can improve this bound slightly by considering the turn on which
Alice activates v. Suppose that Alice activates v in response to an action taken at

122
C. Dunn et al.
w ∈S. Alice will then take another action in N+[v]. So Alice has responded with
two actions in N+[v] due to the action taken at w. Thus, |S| ≤2k + 1. So we have
that before v is colored, the number of distinctly colored elements to which v may
be adjacent or incident is at most
Δ +
  N+(v)
  + |S| ≤Δ + 3k + 1.
Now suppose that e = xy is an uncolored edge with x < y in L. At any
point in the game e can be incident to at most two colored vertices before e is
activated. Additionally, at most Δ −1 edges in P(e) ∪B(e) can be colored before
e becomes active. Similar to the argument above for vertices, let S ⊆S(e) be the
set of children of e that are activated before e is colored. Note that each time an
edge in S is activated or colored, Alice will take action at an edge in H[e]. Thus,
S ≤2 |H[e]| ≤2k. Thus, before e is colored, the number of distinctly colored
elements to which e may be incident is at most
Δ −1 + |{ x, y }| + |H(e)| + |S| ≤Δ −1 + 2 + (k −1) + 2k
= Δ + 3k.
We note that there are multiple avenues for creating “defect” versions of this
game, depending on whether you allow for adjacent vertices to receive the same
color (or not), and similarly for edges. Each of these variations can lead to different
results for different classes of graphs.
7
Conclusions and Problems to Consider
The area of competitive graph coloring is rich with open problems. We have
presented the standard vertex coloring game along with variations in the deﬁnition
of a legal color. Additionally, we have presented variations in which the players
color edges. Each of these variations still have problems to consider, but we have
also given a framework which invites researchers to consider their own variations of
a legal color.
Questions in competitive graph coloring are ideally suited to undergraduate
research. Not only are there many interesting open questions, but students can
begin exploring these questions with very little background. For many questions,
students need only to be introduced to basic deﬁnitions in graph theory, the rules
of the game and the relevant parameters, along with strategies for Alice and Bob.
Experimentation with game variations and speciﬁc classes of graphs can lead
students to ask their own questions. We would note that the work in each of papers
in [17–20] was done with undergraduates and each paper has undergraduates listed
as coauthors.

Introduction to Competitive Graph Coloring
123
We summarize some open questions stemming from the games presented above,
but we also expect this paper to provide a model for asking and answering additional
questions in the area of competitive graph coloring.
Regarding the standard vertex coloring game, a number of problems still remain
open.
Research Project 1. Find criteria which differentiates between forests with
game chromatic numbers 3 and 4.
Research Project 2. Does there exists a forest F of odd order with Δ(F) = 3
where χg(F) = 4?
The game chromatic number has also been studied on other common classes of
graphs, and there are many open questions coming from this.
Research Project 3. For planar graphs G, the best known upper [32] and
lower [28] bounds show that 8 ≤χg(G) ≤17. For outerplanar graphs G, the
current bounds are 6 ≤χg(G) ≤7 [25]. Progress for either class would be of
interest.
Research Project 4. Another interesting question, asked by Zhu [30], is
whether χg(G) = k implies that Alice has a winning strategy for the (k + 1)-
coloring game on G.
Regarding the d-relaxed chromatic number dχg(G), Theorem 7 has been shown
to be sharp, and it was shown in [26] that if d ≥2 then for any tree T, dχg(T) ≤2.
However, similar to the classiﬁcation problem discussed in Section 2, we do not
know criteria to distinguish those trees T for which 1χg(T) = 3 and those for which
1χg(T) = 2.

124
C. Dunn et al.
Research Project 5. Find criteria to distinguish trees where 1χg(T) = 3
from those where 1χg(T) = 2.
Chou et al. [6] also showed that, where G is an outerplanar graph and d ≥1,
dχg(G) ≤6.
Research Project 6. Improve this bound, or provide an example which
shows that it is sharp. This has not been done for any d.
Clique-relaxed game chromatic number has not been as widely studied. In
particular, there are no current results for planar graphs. For outerplanar graphs G,
Theorems 9 and 10 show that 3 ≤χg(2)(G) ≤4.
Research Project 7. Determine bounds for the clique-relaxed game chro-
matic number for planar graphs. Once a sharp upper bound is known, the
question of classifying graphs with particular clique-relaxed game chromatic
number arises.
The edge coloring game also has many open questions. For forests F with
maximum degree Δ, it has been shown in [1, 22] that χg′(F) ≤Δ + 1 except
when Δ = 4, where the best result shows χg′(F) ≤6. Lam et al. [29], who showed
that χg′(F) ≤Δ + 2 for forests, also asked if there could be a constant c such that
χg′(G) ≤Δ + c for all graphs.
Research Project 8. Is there a constant c such that χg′(G) ≤Δ + c for all
graphs? A similar question can be asked of the d-relaxed edge coloring game,
or for the total coloring game.
Finally, in [12] it is shown that for every m ∈N, there exists a graph G such that
m ≤χg(G) < 1χg(G), which seems counterintuitive.

Introduction to Competitive Graph Coloring
125
Research Project 9. Is it true that for every nonnegative integer d, there
exists a graph G such that dχg(G) <
d+1χg(G)? This question was ﬁrst
proposed by Kierstead and would seem an interesting one to settle.
References
1. Andres, S.D.: The game chromatic index of forests of maximum degree Δ ≥5. Discret. Appl.
Math. 154(9), 1317–1323 (2006)
2. Appel, K., Haken, W., Koch, J.: Every planar map is four colorable. Part II: reducibility. Ill. J.
Math. 21, 491–567 (1977)
3. Bartnicki, T., Grytczuk, J., Kierstead, H.A., Zhu, X.: The map-coloring game. Am. Math. Mon.
114(9), 793–803 (2007)
4. Bodlaender, H.: On the complexity of some coloring games. In: Möhring, R. (ed.) Graph
Theoretical Concepts in Computer Science, vol. 484, pp. 30–40. Lecture notes in Computer
Science. Springer, Berlin (1991)
5. Cai, L., Zhu, X.: Game chromatic index of k-degenerate graphs. J. Graph Theory 36(3),
144–155 (2001)
6. Chou, C., Wang, W., Zhu, X.: Relaxed game chromatic number of graphs. Discret. Math.
262(1–3), 89–98 (2003)
7. Cohen-Addad, V., Hebdige M., Král’, D., Li, Z., Salgado, E.: Steinberg’s conjecture is false.
Preprint arXiv:1604.05108 (2016)
8. Cowen, L., Cowen, R., Woodall, D.: Defective colorings of graphs in surfaces: partitions into
subgraphs of bounded valency. J. Graph Theory 10, 187–195 (1986)
9. Cowen, L., Goddard, W., Jesurum, C.: Defective coloring revisited. J. Graph Theory 24,
205–219 (1997)
10. Deuber, W., Zhu, X.: Relaxed coloring of a graph. Graphs Combin. 14, 121–130 (1998)
11. Dinski, T., Zhu, X.: A bound for the game chromatic number of graphs. Discret. Math. 196,
109–115 (1999)
12. Dunn, C.: Complete multipartite graphs and the relaxed coloring game. Order 29(3), 507–512
(2012)
13. Dunn, C.: The relaxed game chromatic index of k-degenerate graphs. Discret. Math. 307,
1767–1775 (2007)
14. Dunn, C., Kierstead, H.A.: A simple competitive graph coloring algorithm II. J. Combin.
Theory Series B, 90, 93–106 (2004)
15. Dunn, C., Kierstead, H.A.: A simple competitive graph coloring algorithm III. J. Combin.
Theory Ser. B 92, 137–150 (2004)
16. Dunn, C., Kierstead, H.A.: The relaxed game chromatic number of outerplanar graphs. J. Graph
Theory 46, 69–78 (2004)
17. Dunn, C., Naymie, C., Nordstrom, J.F., Pitney, E., Sehorn, W., Suer, C.: Clique-relaxed graph
coloring. Involve 4(2), 127–138 (2011)
18. Dunn, C., Larsen, V., Lindke, K., Retter, T., Toci, D.: The game chromatic number of trees and
forests. Discret. Math. Theor. Comput. Sci. 17(2), 31–48 (2015)
19. Dunn, C., Morawski, D., Nordstrom, J.F.: The relaxed edge-coloring game and k-degenerate
graphs. Order 32(3), 347–361 (2015)
20. Dunn, C., Hays, T., Naftz, L., Nordstrom, J.F., Samelson, E., Vega, J., Total coloring games (in
preparation)
21. Eaton, N., Hull, T.: Defective list colorings of planar graphs. Bull. Inst. Combin. Appl. 25,
79–87 (1999)

126
C. Dunn et al.
22. Erd˝os, P., Faigle, U., Hochstättler, W., Kern, W.: Note on the game chromatic index of trees.
Theor. Comput. Sci. 313(3), 371–376 (2004)
23. Faigle, U., Kern, W., Kierstead, H.A., Trotter, W.: On the game chromatic number of some
classes of graphs. Ars Combinatoria 35, 143–150 (1993)
24. Gardner, M.: Mathematical games. Sci. Am. 23, (1981)
25. Guan, D., Xhu, X.: Game chromatic number of outerplanar graphs. J. Graph Theory 30, 67–70
(1999)
26. He, W., Wu, J., Zhu, X.: Relaxed game chromatic number of trees and outerplanar graphs.
Discret. Math. 281(1–3), 209–219 (2004)
27. Kierstead, H.A.: A simple competitive graph coloring algorithm. J. Combin. Theory Ser. B 78,
57–68 (2000)
28. Kierstead, H.A., Trotter, W.: Planar graph coloring with an uncooperative partner. J. Graph
Theory 18, 569–584 (1994)
29. Lam, P., Shiu, W., Xu, B.: Edge game-coloring of graphs. Graph Theory Notes N. Y. XXXVII,
17–19 (1999)
30. Zhu, X.: The game coloring number of planar graphs. J. Combin. Theory Ser. B 75, 245–258
(1999)
31. Zhu, X.: The game coloring number of pseudo partial k-trees. Discret. Math. 215, 245–262
(2000)
32. Zhu, X.: Reﬁned activation strategy for the marking game. J. Combin. Theory Ser. B 98, 1–18
(2008)

Matroids
Erin McNicholas, Nancy Ann Neudauer, and Colin Starr
Suggested Prerequisites. Linear Algebra is the primary prerequisite for this mate-
rial. Some familiarity with Graph Theory and ﬁnite ﬁelds is also desirable; in
particular, we rely on operations in Z2 a great deal.
1
Introduction
In recent years graph theory has emerged as one of the most popular ﬁelds for
undergraduate research. It is accessible, beautiful, and a powerful way to model
relationships. As in other areas of discrete mathematics, problems that seemed
intractable have become approachable with advances in computing power. Graph
theory and discrete mathematics more generally have opened up as fruitful areas for
research.
Matroids are an abstract generalization of graphs. Given any graph, we can deﬁne
an associated cycle matroid. Because all graphs can be represented as matroids, we
may use the more abstract setting of matroids to draw conclusions about graphs.
The converse, however, is not true; not all matroids are the cycle matroid of some
graph. Thus, in working with matroids we must guard against blindly treating them
like graphs.
Matroids also incorporate a notion of independence that generalizes linear
independence, so we can consider matroids a generalization of linear algebra; again,
E. McNicholas () • C. Starr
Willamette University, 900 State St., Salem, OR 97306, USA
e-mail: emcnicho@willamette.edu; cstarr@willamette.edu
N. Ann Neudauer
Paciﬁc University, 2043 College Way, Forest Grove, OR 97116, USA
e-mail: nancy@paciﬁcu.edu
© Springer International Publishing AG 2017
A. Wootton et al. (eds.), A Primer for Undergraduate Research, Foundations for
Undergraduate Research in Mathematics, DOI 10.1007/978-3-319-66065-3_6
127

128
E. McNicholas et al.
Fig. 1 A Graph
1
2
3
4
a
b
c
d
5
however, not all matroids can be represented this way, either. For those that can, we
can draw on the tools and techniques of Linear Algebra to further our cause.
Tutte said, “If a theorem about graphs can be expressed in terms of edges and
circuits only, it probably exempliﬁes a more general theorem about matroids.” This
has driven work in matroid theory and is the approach we take in this chapter to
extend the work on the problem of uniquely pancyclic graphs to matroids. In turn,
results about the matroids may solve open problems for the graphs.
2
Graphs
We start with a brief review of several common terms from Graph Theory. For
a more detailed introduction to the subject see [12] or [13]. A
graph G is
deﬁned by a set of vertices V(G) = {v1, . . . , vn} and a list of pairs of vertices
called the edges E(G) of G. Given an edge pair e =
{vi, vj}, we say the
vertices vi and vj are adjacent and joined by the edge e; a vertex is incident
with an edge it belongs to. For example, take V(G) = {a, b, c, d} and E(G) =
{{a, b}, {b, c}, {c, d}, {d, a}, {a, c}}. This graph is illustrated in Figure 1. Vertices
a and b are adjacent, but vertices b and d are not. Vertex a is incident with edge
5, but vertex b is not. For small graphs, it is usually easier to draw them than to
describe them as a set of vertices and an edge list.
For the research projects we discuss in this chapter, we need a few additional
terms and concepts associated with graphs.
First, a cycle in a graph is a sequence of adjacent vertices v1, v2, . . . , vk, v1
such that vi ̸= vj unless i = j. Alternatively, we can describe a cycle by the edges
v1v2, v2v3, . . . , vk−1vk, vkv1, with the same restriction vi ̸= vj unless i = j. (Note
that we abbreviated the edge {vi, vj} to vivj; this is common notation.) The length
of a cycle is the number of edges appearing in it. In our previous example, using the
edge description of a cycle, G has cycles 125, 534, and 1234 of lengths 3, 3, and 4,
respectively. Paths are deﬁned similarly except that the last vertex does not match
the ﬁrst: v1v2, v2v3, . . . , vk−1vk.
A graph G is simple if the edges E(G) are distinct and each edge consists of
two distinct vertices (that is, there is at most one edge between any two vertices,
and there are no loops connecting a vertex to itself). A graph is planar if it can be
embedded in the plane, or equivalently, it can be drawn in such a way that the edges
only meet at vertices. All the graphs presented in this chapter are simple and planar.
While graphs (e), (f), and (g) in Figure 2 are drawn with intersecting edges, edge a
could be drawn above edges 1 and 2 to demonstrate the planarity of these graphs.

Matroids
129
1
3
2
2
1
5
4
3
a
)
b
(
)a(
5
4
3
2
1
8
7
6
a
b
5
4
3
2
1
8
7
6
a
b
)
d
(
)c(
1
2
3
4
5
6
7
8
9
10
11
12
13
14
a
c
b
1
2
3
4
5
6
7
8
9
10
11
12
13
14
a
c
b
1
2
3
4
5
6
7
8
9
10
11
12
13
14
a
c
b
)
g
(
)f(
)e(
Fig. 2 The known UPC graphs
2.1
UPC Graphs
Graph Theory is rich with interesting open problems. In this chapter, we consider
the following problem posed by Entringer in 1973:
Question. Which simple graphs on n vertices have exactly one cycle of
size ℓfor each ℓsuch that 3 ≤ℓ≤n?

130
E. McNicholas et al.
Following Bondy[1], we call a simple graph G on n vertices uniquely pancyclic,
or unipancylic, if it has exactly one cycle of each possible length, 3, . . . , n. We
usually abbreviate unipancyclic to UPC. The graphs in Figure 2 are the only known
UPC graphs on up to 59 vertices [6, 10, 11]; for example, the 8-vertex graph
in Figure 2(c) has cycles 12a, 678b, 345ba, 12345b, 345678a, and 12345678, and
those are its only cycles.
Exercise 1. Verify that the graphs shown in Figure 2 are all UPC.
It is not known whether there are any more UPC graphs beyond those shown in
Figure 2.
Research Project 1. Find another UPC graph not shown in Figure 2 or show
that there are no more.
A UPC graph is necessarily Hamiltonian; that is, it contains a cycle that passes
through every vertex. Given a simple graph with a unique Hamiltonian cycle H, the
edges that are not part of H are called chords of the graph.
In 1986, Shi proved that there are no UPC graphs having exactly 4 chords [10]
and that the UPC graphs in Figure 2 are the only ones with three or fewer chords.
Markström [6] proved in 2009 that there are no other UPC graphs on 59 or fewer
vertices and that there are no UPC graphs with exactly 5 chords. Little other progress
has been made on this problem since it was ﬁrst posed in 1973.
Some possible avenues for progress include determining structural character-
istics of UPC graphs. Project 3 requires ideas in Graph Theory that we have not
introduced. See [12] for a general discussion of graph theoretic concepts.
Research Project 2. Determine whether a UPC graph must be planar. (The
known ones are.)
Research Project 3. Determine the possible chromatic numbers of UPC
graphs.

Matroids
131
In 2001, Starr and Turner generalized the notion of UPC graphs to UPC matroids
(unpublished work). Below, we discuss this generalization and the interesting
discovery of a non-graphic UPC matroid on 4 chords.
3
Matroids
3.1
Introduction to Matroids
In 1935, Whitney published the seminal paper on matroid theory, generalizing
familiar notions of dependence and independence that arise in several areas of
mathematics [4]. Since then, the ﬁeld has risen in importance due to its generality
and its wide application to other branches of mathematics.
Matroids are an axiomatically-deﬁned structure (as we will see in the deﬁni-
tion below) abstracting notions of algebraic dependence, linear dependence, and
geometric dependence (e.g., collinearity of points) that unify several areas of
discrete mathematics. Their abstract properties can sometimes be represented more
concretely, like with a graph or a geometry. The usefulness of matroids to pure
mathematical research is similar to that of groups: by studying an abstract version
of phenomena that occur in different realms of mathematics, we learn something
about all those realms simultaneously. Matroids are the objects we study in order to
understand dependence properties of discrete sets of points in space. There are also
applications of matroid theory to practical problems, and matroids are essential in
combinatorial optimization. Rota said that “Anyone who has worked with matroids
has come away with the conviction that matroids are one of the richest and most
useful ideas of our day.” [9]
Deﬁnition 1. Let E be a ﬁnite set of elements, and let I be a family of subsets
of E. We say the ordered pair M = (E, I ) is a matroid if I has the following
properties:
1. ∅∈I ;
2. If I1 ∈I and I2 ⊆I1, then I2 ∈I ;
3. If I1, I2 ∈I and |I1| < |I2|, then there exists e ∈I2 −I1 such that I1 ∪e ∈I .
Property 3 is known as the independence augmentation axiom.
The set E is referred to as the ground set of the matroid and I as the set of
independent sets
[8]. We say that a set is dependent if it is not independent. A
minimally dependent set (i.e., a dependent set such that the removal of any element
from it renders it independent) is called a circuit.
These structures in the matroid capture the essential notion of independence as
we encounter it in other contexts. Note that we adopt the common convention from
Graph Theory that if e is a single element, then A ∪e abbreviates A ∪{e}.
For a general introduction to Matroid Theory see [7] or [8].

132
E. McNicholas et al.
Fig. 3 A Graphic Matroid
2
1
5
4
3
a
3.2
The Cycle and Vector Matroids
You might ask how graphs can be matroids. Or how matroids are related to Linear
Algebra. There are several ways to deﬁne a matroid on a graph; the most common
we describe here. Below, we show how you can deﬁne a matroid on a set of vectors.
Let G be a graph. Let E = E(G) be the set of edges of G, and let I be the
set of all subsets of E that do not contain a cycle in the graph. Is this a matroid?
To check that this is in fact a matroid we merely need to check the three properties
from Deﬁnition 1. Certainly the empty set does not contain a cycle, so I satisﬁes
Property 1 of the deﬁnition. Any subset of a cycle-free subgraph of G is also cycle-
free, giving us Property 2. Property 3 is much harder to prove, but it does hold. We
defer further discussion until we develop an alternate deﬁnition of a matroid based
on circuits. Since it does satisfy the three axioms, we can deﬁne a matroid in this
way starting with any graph. This matroid is called the cycle matroid of the graph,
denoted M(G). Any matroid that is the cycle matroid of some graph is a graphic
matroid, or graphic. Recall from our earlier discussion that not all matroids are
graphic.
Example 1. Given the graph in Figure 3, we can deﬁne the cycle matroid
as follows: the ground set of the cycle matroid on G is {1,2,3,4,5,a}; the
maximal independent sets, called the bases of the matroid, are {1, 2, 3, 4},
{1, 2, 3, 5}, {1, 2, 4, 5}, {1, 3, 4, 5}, {2, 3, 4, 5}, {1, a, 3, 4}, {1, a, 3, 5}, {1, a, 4, 5},
{2, a, 3, 4}, {2, a, 3, 5}, and {2, a, 4, 5}. I is the set of all subsets of these sets. The
circuits of this matroid are {1, 2, 3, 4, 5}, {1, 2, a}, and {3, 4, 5, a}. As in all cycle
matroids, the circuits are precisely the cycles in the corresponding graph.
We see here how matroids abstract graphs, but what about their connection to
linear algebra?
We can deﬁne a matroid on any collection of vectors. Suppose that A is an
m × n matrix. To deﬁne a matroid, we need to decide what the ground set and
the independent sets are, and then to check that these satisfy the axioms. Let E be
the set of columns of A, and let I be the set of all linearly independent subsets of E.
Since the empty set is linearly independent, I satisﬁes Property 1 of the deﬁnition.
Every subset of a linearly independent set of vectors is also linearly independent, so
I satisﬁes Property 2, as well. Finally, we know that if I1, I2 ∈I with |I1| < |I2|,

Matroids
133
then I1 spans a subspace of smaller dimension than does I2, so I2 contains a vector
e not in the span of I1. Therefore, I1 ∪e is also linearly independent, and I satisﬁes
Property 3.
A matroid that arises in this way is called a vector matroid. A matroid is
representable over a ﬁeld F if it can be expressed as the vector matroid of some
matrix with entries in F. The entries could be real numbers or elements of Z2, for
example.
Example 2. As a speciﬁc example of a vector matroid, consider the matrix
⎡
⎢⎢⎣
1 2 3 4
5 a
1 0 0 0 1 1
0 1 0 0 1 1
0 0 1 0 1 0
0 0 0 1 1 0
⎤
⎥⎥⎦
over Z2. That means that when we consider linear combinations, the coefﬁcients
are either 0 or 1 and any arithmetic is performed modulo 2. Thus each linear
combination can be thought of a subset of the set of columns – each column either
appears or it doesn’t. Second, note that the rank of the matrix is 4 – that is, it
takes four linearly independent column vectors to create a basis for the column
space, which we are now thinking of as a basis of the matroid. We can see that
columns 1 through 4 are linearly independent, while columns 1 through 5 are
linearly dependent. Since removing any one of columns 1 through 5 leaves a linearly
independent set, columns {1, 2, 3, 4, 5} are minimally linearly dependent and hence
a circuit of this matroid. Similarly, {1, 2, a} and {3, 4, 5, a} are also circuits, and
these three circuits are the only circuits of the matroid.
Having identiﬁed the collection of circuits, we see that this vector matroid is
isomorphic to the cycle matroid of the graph in Figure 3!
What do we mean by isomorphic? Two matroids are isomorphic if there is a
bijection between their ground sets that also induces a bijection between their sets of
independent sets (or equivalently their circuits). Essentially, one is just a relabeling
of another.
It turns out that every graphic matroid is representable over Z2. Recall that this
means we can ﬁnd a collection of vectors over Z2 so that the graphic matroid is
the vector matroid on those vectors. We refer to any matroid representable over Z2
as binary. In fact, graphic matroids are representable over every ﬁeld. See [8], for
example, for a proof. A matroid representable over every ﬁeld is called regular. As
you may suspect, there are both binary matroids and regular matroids that are not
graphic.
The previous example justiﬁes the language of independence for matroids from
its use in linear algebra. However, there are other notions of independence that are
also included in this deﬁnition, as we saw in the cycle matroid of Example 1. In that
case, independence refers to an absence of cycles in a subset of edges of the graph.

134
E. McNicholas et al.
3.3
Beyond the Basics
Notice that in the examples above, the bases of each matroid were all the same size.
This is not a coincidence! It is always the case that for a given matroid the bases all
have the same size. The size of a basis is called the rank of the matroid, by analogy
with the rank of a matrix. We can, in fact, deﬁne the rank of any subset S ⊆E to
be the size of a largest possible independent set contained in S. Instead of deﬁning a
matroid in terms of its independent sets, a matroid can also be deﬁned by its set of
bases or by a rank function; see Oxley (for example) for further details[8].
The closure of a set D of elements is the set Cl(D) consisting of D and all
elements that can be added to D without changing the rank. For the cycle matroid in
Figure 3, the closure of {1, 2, 3} is {1, 2, 3, a} since a completes the cycle {1, 2, a}
but does not increase the size of the largest contained independent set, and therefore
the rank of {1, 2, 3} and {1, 2, 3, a} are the same (namely, 3).
In Example 2, the columns with these labels exhibit the same properties: columns
1, 2, 3, and a are linearly dependent over Z2, and they span a subspace of Z2
4 of
dimension 3 – the same subspace spanned by just columns 1, 2, and 3. The closure
of the set of columns {1, 2, 3} is thus {1, 2, 3, a}.
The previous examples illustrate matroids in the familiar settings of Graph
Theory and Linear Algebra. You may be asking yourself whether there are any
examples of matroids that are not graphic or representable over Z2. We consider
one now.
Example 3. Let E = {1, 2, 3, 4}, and let I = {S ⊆E : |S| ≤2}. In other
words, all sets of size two or smaller are independent sets. Then (E, I ) is a kind of
matroid known as a uniform matroid. This particular matroid is known as U2,4.
More generally, Uk,n = (E, I ) refers to the matroid with |E| = n and I = {S ⊆
E : |S| ≤k}.
Is U2,4 really a new type of matroid? It is natural to wonder whether there is
a graphic or vector representation that captures the same dependencies on four
elements. That is the kind of question that stimulates mathematical research! As
we see below (and you can show), U2,4 is not graphic, but it does have a vector
representation over ﬁelds of order 3 or more.
Exercise 2. Verify that the three independence axioms hold for U2,4.
Exercise 3. Show that U2,4 is not graphic.
Exercise 4. Verify that U2,4 is not representable over Z2 and ﬁnd a representation
of U2,4 over Z3.
There are many equivalent ways of deﬁning matroids. As we mentioned before,
we could deﬁne them in terms of the bases or the rank function instead of the
independent sets. For our purposes, the most useful alternative to the independent
sets deﬁnition is in terms of the circuits.

Matroids
135
Deﬁnition 2. The ordered pair M = (E, C ) is called a matroid if E is a ﬁnite set,
and C is a family of subsets of E satisfying:
1. ∅/∈C ;
2. if C ∈C , then no proper subset of C is in C ;
3. if C1, C2 ∈C and e ∈C1 ∩C2, then there exists C3 ∈C such that C3 ⊆
C1 ∪C2 −{e}.
Property 3 is known as the circuit elimination axiom.
We can show that the two deﬁnitions of a matroid are equivalent, or, more
precisely, cryptomorphic [3]. Let I be the set of all subsets of E that do not contain
(as a subset) any member of C .
Challenge Problem 1. Given a set C satisfying the properties in Deﬁnition 2, show
that the set I constructed as above fulﬁlls the properties and axioms of Deﬁnition 1,
and that the set of circuits of (E, I ) is precisely C . Conversely, show that if M =
(E, I ) is a matroid according to Deﬁnition 1, then its set C of circuits satisﬁes the
properties above and the set of all subsets of E that do not contain any member of
C is precisely the given set I .
Example 4. Graphic matroids are often naturally examined in light of the circuit
axioms above: rather than show that graphs satisfy the independence axioms, we
show they satisfy the circuit axioms. The set C of cycles of a graph certainly does
not include the empty set, and no proper subset of a cycle is also a cycle. We offer
an informal argument that C satisﬁes the circuit elimination axiom: suppose that e
belongs to two cycles C1 and C2. Say e = uv. Travel from u toward v along C1
(not going directly from u to v). Along the way, pay attention to any vertices shared
by C1 and C2. Similarly, travel from u toward v along C2. Since C1 and C2 both
include paths from u to v that avoid e, there exist vertices a and b such that C1 and
C2 stop sharing vertices at a and start sharing vertices again at b. Then traveling
from a to b along C1 and from b back to a along C2 gives a cycle in C1 ∪C2 −e.
Exercise 5. Consider the graphic matroid given in Figure 2(e), and the circuits C1
and C2 containing edges {1, b, 11, 10, c} and {1, 2, a}, respectively. Find the circuit
C3 whose existence is guaranteed by the circuit elimination axiom.
Each matroid has a dual deﬁned on the same ground set. The bases of the dual
of a matroid M on E are precisely the complements of the bases of M. If you are
familiar with Graph Theory, you may wonder whether the notions of duality for
cycle matroids and graphs coincide. (If you are less familiar, see [12] or [13] for
an exploration of these ideas.) In fact, they do: if M is the cycle matroid of a planar
graph G, the dual of M is isomorphic to the cycle matroid of the dual of G. The dual
of the cycle matroid of G is called the cutset matroid of G due to the fact that the
cycles of the graph become the cutsets in the dual. To learn more about this see [13].

136
E. McNicholas et al.
Notice that the matroid U2,4 in Example 3 is its own dual. Its bases are all subsets
of size 2 (the maximal independent sets), and, since the ground set has 4 elements,
their complements are also the subsets of size 2.
Exercise 6. Find the dual of U2,5.
We can operate on matroids by deleting or contracting elements or sets of
elements. If M is a matroid and e ∈E(M), the deletion of e from M is the matroid
M \ e on E(M) −{e} whose circuits are the circuits of M not involving e. For
example, U2,4 \ 4 gives the matroid on {1, 2, 3} with just the one circuit {1, 2, 3}.
The contraction of e in M is the matroid M/e on E(M) −{e} whose cir-
cuits are the minimal1 non-empty members of {C −e|C is a circuit of M}.
Contracting 4 in U2,4, for example, changes the set of circuits of M into the set
{{1, 2, 3}, {1, 2}, {1, 3}, {2, 3}}. Since {1, 2, 3} contains the set {1, 2}, it is not
minimal in this collection and thus is not a circuit of M/4, but the other three are.
That is, U2,4/4 is U1,3.
Interestingly, the operations of deletion and contraction commute with them-
selves and with each other, so if S is a set of elements to delete and T is a set of
elements to contract, the order in which they are deleted and contracted does not
matter. The result is the following.
Deﬁnition 3. A minor of a matroid M is a matroid obtained by a sequence of
deletions and contractions; that is, a matroid of the form M/T \ S for some
S, T ⊆E(M).
4
In Search of UPC Matroids
4.1
UPC Matroids
We generalize the notion of a UPC graph to that of a UPC matroid: a matroid M of
rank r is UPC if it has exactly one circuit each of sizes 3, 4, . . . , r + 1. We will refer
to the (r + 1)-circuit as the Hamiltonian circuit by analogy with graphs.
Note that all of the UPC graphs above can also be viewed as UPC matroids.
Given the difﬁculty in ﬁnding another UPC graph, Starr and Turner expanded the
question: Are there any non-graphic UPC matroids? Indeed, there is at least one.
The following is the rank-24, non-graphic, binary UPC matroid M24 found by Starr
and Turner in 2001:
1in the set-inclusion sense

Matroids
137
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24
25 a b c d
1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
1 1 0 0 1
0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
1 1 0 1 0
0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
1 0 1 1 0
0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
1 0 1 1 0
0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
1 0 1 1 0
0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
1 0 0 1 0
0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
1 0 0 1 1
0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
1 0 0 0 1
0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
1 0 0 0 1
0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0
1 0 0 0 1
0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0
1 0 0 0 1
0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0
1 0 0 0 1
0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0
1 0 0 0 1
0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0
1 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0
1 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0
1 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0
1 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0
1 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0
1 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0
1 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0
1 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0
1 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0
1 0 0 0 0
0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1
1 0 0 0 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
There are many ways to see that this is non-graphic; the simplest is probably to
try to ﬁnd a graph it represents.
Note that columns 1 through 25 give the Hamiltonian circuit, and columns a, b, c,
and d are the chords. Deleting2 the chord d and contracting3 elements 14 through
24 shows that the UPC graph on 14 vertices is a minor of this UPC matroid.
In fact, all of the known UPC graphs are minors of the rank-24 UPC matroid:
contracting columns 3 through 24 and deleting a, b, c, and d gives the triangle;
contracting 6 through 24 and deleting b, c, and d gives the 5-vertex UPC graph;
contracting 8 through 24 and deleting c and d gives both 8-vertex UPC graphs
depending on how we choose to label the graph edges; and contracting 14 through
24 and deleting d gives all three 14-vertex UPC graphs, again depending on where
we place different edges. For example, when we delete d and contract 14 through
24, we are left with a 13-cycle. Since there is no requirement that we label its edges
in numerical order, this leaves some choice for the placement of chords.
2To delete an edge from a matroid represented as a matrix, simply delete the column corresponding
to that element.
3To contract an edge from a matroid represented as a matrix, ﬁrst pivot on a given element so that
it is (a) 1 and (b) the only nonzero element in its column, then delete the row and column of that
entry. See [8] for further details.

138
E. McNicholas et al.
Exercise 7. Find the rank-7 minor of M24 described above and then ﬁnd two
non-isomorphic graph realizations of it based on the matrix labeling. Compare to
Figure 2.
Interestingly, although there are multiple non-isomorphic UPC graphs on 8 and
14 vertices, the corresponding UPC matroids of a given rank are unique. That is,
there is only one rank-13 UPC matroid even though it has three non-isomorphic
graph realizations. It is not clear whether this holds in general. Part of the difﬁculty
is that no other UPC matroids are known at the time of this writing. Perhaps you can
ﬁnd one!
Research Project 4. Determine whether two UPC matroids of the same rank
must be isomorphic.
Research Project 5. Find another UPC matroid.
Research Project 6. We know that the triangle (0 chords) is a UPC minor of
every UPC matroid, and, in fact, each of the known UPC matroids is a minor
of all of the higher-rank UPC matroids known. Does this continue for larger
UPC matroids? For example, must a UPC matroid on k chords contain a UPC
minor on k −1 chords? k −2?
4.2
Binary Matroids
We turn our attention to binary matroids. Recall that a matroid is binary if it is
representable over Z2. There are at least two reasons for looking at binary matroids
in particular: (1) every graphic matroid is also binary, so understanding binary UPC
matroids can help us understand UPC graphs; (2) binary matroids are relatively easy
to think about since the matrix representation is made up entirely of 0s and 1s.
The following deﬁnitions and theorems demonstrate the ease of working with
binary matroids. Several graph-theoretic deﬁnitions extend directly to binary
matroids, and identifying circuits is much easier than in representable matroids
in general.

Matroids
139
Deﬁnition 4. A matroid M is Hamiltonian if it has a circuit of size r + 1, where r
is the rank of M.
In this deﬁnition, we follow Borowiecki
[2] rather than Lalithambal and
Sridharan [5], who have a slightly different deﬁnition of Hamiltonian matroid
(although the deﬁnitions do coincide in the case of binary matroids).
Deﬁnition 5. In a matroid, a chord of a circuit C is an element of Cl(C) −C. If
the matroid is Hamiltonian with a unique Hamiltonian circuit H, we will reserve the
word chord to refer to a chord of H unless speciﬁed otherwise.
Deﬁnition 6. The support of column a, denoted S(a), in a binary matrix M is the
set of row indices labeling where a has a non-zero entry. The weight of column a,
denoted wt(a), is the cardinality of S(a).
The following deﬁnition leads to a way of determining when a set of elements
generates a circuit in a binary matroid.
Deﬁnition 7. Let K be a collection of vectors. Then K is covered if
 
v∈P1
v

·
 
v∈P2
v

̸= 0
for every partition {P1, P2} of K into two sets. Here, the dot refers to the usual dot
product over the reals, while the vector sums are calculated over Z2.
Example 5. For example, consider the following matrix over Z2:
⎡
⎢⎢⎣
1 2 3 4
5 a b c
1 0 0 0 1 1 0 0
0 1 0 0 1 1 1 0
0 0 1 0 1 0 1 1
0 0 0 1 1 0 0 1
⎤
⎥⎥⎦
We see that a and b are covered since a · b = 1, while a and c are not covered
since a · c = 0. The set {a, b, c} is covered since (a + b) · c = 1, (a + c) · b = 2,
and (b + c) · a = 1.
Notice our vector matroid examples have been in the form [Ir|D], where Ir is the
r × r identity matrix and the matroid M is of rank r. All representable matroids can
be expressed in this form, a fact we will use frequently; see [8] for a proof.
Theorem 1. Let M be a binary matroid represented by [Ir|D], and let A =
{f1, . . . , fℓ} be a set of columns from D. Let I index the columns from Ir such that

i∈I ei = l
i=1 fi. Then A ∪{ei|i ∈I} is a circuit if and only if A is covered. If A is
covered, we refer to the circuit A ∪{ei|i ∈I} as the circuit generated by A.

140
E. McNicholas et al.
The following chain of equivalences includes two important characterizations of
binary matroids. See [8] for a proof.
Theorem 2. Given a matroid M, the following are equivalent:
1. M is binary
2. If C1 and C2 are distinct circuits, their symmetric difference C1△C2 = (C1 −
C2) ∪(C2 −C1) is a disjoint union of circuits
3. M has no minor isomorphic to U2,4
Thus, for example, there is no way to represent U2,4 as a matrix over Z2 in the
way we did for the graph in Example 1, as you showed in an earlier exercise.
4.3
Binary UPC Matroids
Given the difﬁculty of working with matroids in general, the ease of working
with binary matroids, and the success of Starr and Turner in discovering a non-
graphic binary UPC matroid, it may be worth investigating binary UPC matroids
in particular. Should we restrict ourselves to considering only binary matroids in
seeking a UPC matroid, or is it worth looking elsewhere? This brings us to our next
research project.
Research Project 7. Determine whether a UPC matroid can have a minor
isomorphic to U2,4. That is, must all UPC matroids be binary?
Consider a binary Hamiltonian matroid M of rank r with a Hamiltonian circuit
H. We know that we can represent M in the form [Ir|D], but the presence of H allows
us to reﬁne this a little. We can label r elements of H with the columns of Ir and
the last with a column of all ones, 1. Then M has the binary representation [Ir|1K]
where the columns of Ir and 1 form H, and the columns of K are the chords of
M. The following illustrate some of the results we ﬁnd when we limit our scope to
binary matroids. If x1, . . . , xk form a covered subset of {1} ∪K, we will use the
notation < x1, . . . , xk > to denote the circuit generated by x1, . . . , xk. While the
circuit generated by a covered subset of {1} ∪K is unique in the binary case, this is
not necessarily the case for representable matroids over other ﬁelds.
Lemma 1. Let M be a binary Hamiltonian matroid represented by [Ir|1K]. Then
there are exactly two 1-chord circuits for each chord k in K.

Matroids
141
Proof. Let H be the Hamiltonian circuit represented by [Ir|1]. For each k in K there
are certainly two circuits: one that uses 1 and one that does not. By Corollary 9.3.5
of Oxley[8], for any edge e ∈E, there are at most two circuits contained in H ∪e.
Lemma 2. In a binary Hamiltonian matroid M, every set of two chords creates at
least one circuit.
Proof. Given a Hamiltonian matroid M with binary representation [Ir|1K], let b and
c be two distinct chords in K. If {b, c} is covered, it generates a two-chord circuit.
If it is not covered there must be some row i where b has a 1 and c has a 0 and some
row j where b has a 0 and c has a 1. Consider the set {1, b, c}. These ith and jth
rows ensure that (1 + c) · b ̸= 0, (1 + b) · c ̸= 0, and (b + c) · 1 ̸= 0. Thus, the set
{1, b, c} is covered. In either case, the chords b and c induce a circuit.
These results lead to the following bound on the number of chords.
Theorem 3. The number of chords k in a binary UPC matroid of rank r is
bounded by
k ≤
√8r −7
2
−3
2.
Proof. Consider a binary UPC matroid M of rank r with k chords. The total number
of circuits in M is r −1. By Lemmas 1 and 2, there are exactly 2k single-chord
circuits and at least
	k
2

two-chord circuits in addition to the Hamiltonian circuit.
Thus 1 + 2k + k(k−1)
2
≤r −1. Using the quadratic formula to simplify, we ﬁnd
k ≤
√8r−7
2
−3
2.
Markström has a reﬁnement of this that takes into account 3-chord cycles for
UPC graphs [6].
Note that Theorem 3 gives a lower bound for the rank in terms of the number of
chords. Can we ﬁnd an upper bound?
Research Project 8. Find an upper bound on the rank r in terms of the
number of chords k in a binary UPC matroid.
Shi considers particular drawings of UPC graphs to make arguments about their
properties by setting the vertices on a circle and drawing chords of the graph as
chords of the circle, so the chords often cross. Using structural properties about
the number of crossing chords in the 3-cycles and 4-cycles of UPC graphs, Shi
has shown that the known UPC graphs with three chords are the only ones, and
that there are no UPC graphs with exactly four chords[10]. Can we ﬁnd analogous
results for binary UPC matroids? Recall the support of a column d in a binary matrix

142
E. McNicholas et al.
M, denoted S(d), refers to the set of row indices labeling where d has a non-zero
entry. We can use this idea of support to deﬁne the chordal relationship analogous
to crossing in the drawings we have of UPC graphs.
Deﬁnition 8. Two chords of a binary matroid M are skew if the intersection of
their supports is non-empty and they both contain support elements not in the
intersection.
Note that if two chords b and c are skew, then they generate two circuits: {b, c}
and {1, b, c} are both covered in this case (following the reasoning in Lemma 2).
The following are a few structural results for binary UPC matroids. While the
results themselves may be useful, the proofs also demonstrate how one might study
the structural properties of binary UPC matroid circuits.
Lemma 3. If M is a binary UPC matroid with chord b having weight equal to 3
(i.e., b induces a 1-chord 4-circuit), then b is not skew to other chords in M.
Proof. By way of contradiction, suppose b and c are chords in a UPC Matroid M
such that wt(b) = 3 and b is skew to c. There exists a labeling of the Hamiltonian
circuit such that the matroid is representable as follows with chord c having either
form c1 or c2.
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
Ir 1
b c1
c2
1
1
1
1
0
1
1
0
0
0
...
0
1
1
1
⎫
⎬
⎭x
⎧
⎨
⎩
1
1
1
0
0
0
...
...
...
0
0
0
· · ·
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
Recall the circuit C =< x, y, z > generated by x, y, and z (where x, y, and z
are chords or 1) is the circuit consisting of these elements as well as any necessary
elements of the Hamiltonian circuit. If chord c has the form c1, then circuits <
1, c > and < 1, b, c > both have length r −x + 1, and M is not UPC. If c has the
form c2 then circuits < c > and < b, c > both have length x + 3, and M is not
UPC. Since these are the only ways for a chord c to be skew to the chord of weight
3, the lemma follows.
Note that the pancyclicity of M was not used in Lemma 3. Similar techniques
prove the more general result:

Matroids
143
Lemma 4. If M is a binary UPC matroid, the chords in the 4-circuit of M can not
be skew to each other.
This gives us a nice structural result on binary UPC matroids:
Corollary 1. A binary matroid consisting entirely of a Hamiltonian circuit and two
skew chords is not UPC.
Proof. Suppose M is a binary matroid consisting of a Hamiltonian circuit and two
skew chords. It follows that M has exactly 7 circuits, and thus the rank must be 8.
By Lemma 3, it follows that the 4-circuit must involve the 2 skew chords. Thus, by
Lemma 4, M is not UPC.
Research Project 9. What other structural requirements can you ﬁnd for
binary UPC Matroids?
Research Project 10. Use structural requirements to ﬁnd or to argue against
the existence of binary UPC matroids with ﬁve or more chords.
5
Further Reading
Although the literature on this problem is sparse, there are a few resources for further
reading that we have found helpful.
1. Cornuejols, G., Guenin, C.: Ideal binary clutters, connectivity, and a conjecture
of Seymour. SIAM J. Discret. Math. 15(3), 329–352 (2002)
2. George, J.C., Khodkar, A., Wallis, W.D.: Pancyclic and Bipancyclic Graphs, pp.
49–67. Springer, New York (2016)
3. Oxley, J., Whittle, G.: A note on the non-spanning circuits of a matroid. Eur. J.
Comb. 12, 259–261 (1999)
4. Piotrowski, W.: Chordal characterizations of graphic matroids. Discret. Math. 68,
273–279 (1988)
5. Wild, M.: Axiomatizing simple binary matroids by their closed circuits. Appl.
Math. Lett. 6(6), 39–40 (1993)

144
E. McNicholas et al.
References
1. Bondy, J.A., Murty, U.S.R.: Graph Theory with Applications. North-Holland, Amsterdam
(1976)
2. Borowiecki, M., On Hamiltonian matroids. In: Graph Theory: Proceedings of a Conference,
held in Lagow, February 10–13 (1981). Lecture Notes in Mathematics, vol. 1018. Springer,
Berlin (2006)
3. Gordon, G., McNulty, J.: MATROIDS: A Geometric Introduction. Cambridge University Press,
Cambridge (2012)
4. Hassler, W.: On the abstract properties of linear dependence. Am. J. Math. 57(3), 509–533
(1935). The Johns Hopkins University Press, Baltimore
5. Lalithambal, R., Sridharan, N.: A study on hamiltonian matroids. Electron Notes Discret. Math.
15, 201 (2003)
6. Markström, K.: A note on uniquely pancyclic graphs. Aust. J. Combin. 44, 105–110 (2009)
7. Neel, D.L., Neudauer, N.A.: Matroids you have known. Math. Mag. 82(1), 26–41 (2009)
8. Oxley, J.: Matroid Theory. Oxford University Press, New York (1992)
9. Rota, G.-C.: Indiscrete Thoughts. Birkhäuser, Boston, MA (1996)
10. Shi, Y.B.: Some theorems of uniquely pancyclic graphs. Discret. Math. 59, 167–180 (1986).
MR 87j:05103
11. Shi, Y.B., Yap, H.P., Teo, S.K.: On uniquely r-pancyclic graphs. In: Graph Theory and Its
Applications: East and West, (Jinan, 1986). Annals of the New York Academy of Sciences,
vol. 576, pp. 487–499. New York Academy of Sciences, New York (1989). MR 93d:05088
12. West, D.B.: Introduction to Graph Theory, 2nd edn. Prentice Hall, Upper Saddle River (2000)
13. Wilson, R.J.: Graph Theory, 4th edn. Addison Wesley Longman Limited, Harlow, Essex (1996)

Finite Frame Theory
Somantika Datta and Jesse Oldroyd
Suggested Prerequisites. Linear algebra: Bases, Eigenvalues, Eigenvectors, Inner
products. Complex variables: Basic properties of complex numbers.
1
Introduction
Given a signal, whether it is a discrete vector or a continuous function, one desires
to write it in terms of simpler components. Typically, these components or “building
blocks” form what is called a basis. A basis is an optimal set, having the minimal
number of elements, such that any vector (signal) in the underlying space can
be written uniquely as a linear combination of the basis vectors. Bases play an
important role in the analysis of vector spaces, since the characteristics of the signal
can be read off from the coefﬁcients in the basis representation. However, if the
coefﬁcients get corrupted by noise or if some get lost during transmission then
valuable signal information can get lost beyond recovery. The main problem with
bases is this lack of ﬂexibility - even a slight modiﬁcation of a basis can leave
us with a set that is no longer a basis. Since the basis representation is typically
nonredundant one can try to bring in more ﬂexibility by adding some extra elements
and sacriﬁcing the uniqueness property of a basis representation. This leads to the
notion of a frame. A frame can be thought of as a redundant basis, having more
elements than needed. In fact, in any ﬁnite dimensional vector space every ﬁnite
spanning set is a frame. Although the redundancy of a frame leads to non-unique
S. Datta ()
University of Idaho, Moscow, ID 83844-1103, USA
e-mail: sdatta@uidaho.edu
J. Oldroyd
West Virginia Wesleyan College, Buckhannon, WV 26201, USA
e-mail: oldroyd.j@wvwc.edu
© Springer International Publishing AG 2017
A. Wootton et al. (eds.), A Primer for Undergraduate Research, Foundations for
Undergraduate Research in Mathematics, DOI 10.1007/978-3-319-66065-3_7
145

146
S. Datta and J. Oldroyd
representations, this also makes the corresponding signal representations resilient to
noise and robust to transmission losses. In applications, such robustness might be
more desirable than having a unique representation.
It is widely acknowledged that the idea of frames originated in the 1952 paper
by Dufﬁn and Schaeffer [11], but frames have only gained signiﬁcant popularity
relatively recently due to work such as [10]. Frames are now standard tools in signal
processing and are of great interest to mathematicians and engineers alike. This
chapter focuses on frames in ﬁnite dimensional spaces. The notion of frames for
inﬁnite dimensional spaces like function spaces is far more subtle [6, 9, 26] and
will not be discussed here. Along with introducing ﬁnite frame theory, this chapter
discusses a special highly desirable class of frames called equiangular tight frames.
Possible research ideas suitable for an undergraduate curriculum are also discussed.
Throughout the chapter, many of the well known results will be stated without
proofs and the reader will be provided with the necessary references. It is assumed
that readers have taken some undergraduate linear algebra course, and are familiar
with the notion of a basis and its fundamental properties. For an in depth study
of linear algebra, readers may refer to [12, 17]. A nominal knowledge of complex
numbers is also assumed. In all that follows, R will denote the set of real numbers,
and C will denote the set of complex numbers. For a given c ∈C, the complex
conjugate of c is denoted by c, and the modulus of c is denoted by |c|. In a setting
that applies to both R and C we will use the notation F, the elements of which are
called scalars.
We ﬁrst recall one of the most signiﬁcant properties of a basis in the following
result [12].
Theorem 1 ([12]). Let V be a vector space and B = {v1, v2, . . . , vn} be a subset of
V . Then B is a basis for V if and only if each v ∈V can be uniquely expressed as a
linear combination of vectors of B, that is, there exist unique scalars c1, c2, . . . , cn
such that
v = c1v1 + c2v2 + · · · + cnvn.
(1)
The scalars ci in (1) are called the coefﬁcients of v with respect to B. Note that each
basis for a given vector space has the same number of elements, and this number
is the dimension of the underlying vector space. As will become apparent later, in
the case of a frame there is an added ﬂexibility in that frames for the same space
can differ in the number of elements. The computation of the coefﬁcients in (1)
is important since this allows us to represent v in terms of the basis elements.
However, this process can be cumbersome. The concept of an inner product can
greatly simplify these calculations. For the convenience of the reader, we next recall
some standard deﬁnitions and properties pertinent to inner products.
Deﬁnition 1. Let V be a vector space over F. An inner product on V is a function
that assigns to every ordered pair of vectors u, v ∈V , a scalar in F denoted by
⟨u, v⟩, such that for all u, v, w ∈V and all c ∈F, the following hold:

Finite Frame Theory
147
(a) ⟨u + w, v⟩= ⟨u, v⟩+ ⟨w, v⟩.
(b) ⟨cu, v⟩= c⟨u, v⟩.
(c) ⟨u, v⟩= ⟨v, u⟩, where the bar denotes complex conjugation.
(d) ⟨u, u⟩≥0, with equality if and only if u = 0.
A vector space endowed with an inner product is called an inner product space. If
F = R, it is called a real inner product space and if F = C, it is called a complex
inner product space.
Example 1. In Fn, an inner product of two vectors u = (a1, a2, . . . , an) and v =
(b1, b2, . . . , bn) can be deﬁned as
⟨u, v⟩=
n

i=1
aibi.
This is the standard inner product of Fn. When F = R, the conjugations are not
needed, and we have what is commonly referred to as the dot product, often written
as u · v.
The deﬁnition of an inner product is used to generalize the notion of length in
a vector space. Recall that in R3, the Euclidean length of a vector v = (a, b, c) is
given by
√
a2 + b2 + c2 =

⟨v, v⟩. This leads to the following.
Deﬁnition 2. Let V be an inner product space. For v ∈V , the norm or length of v
is deﬁned by
∥v∥=

⟨v, v⟩.
If ∥v∥= 1 then v is called unit normed.
Deﬁnition 3. Two distinct vectors u, v in an inner product space V are said to be
orthogonal if ⟨u, v⟩= 0. A subset S of V is called an orthogonal set if any two
distinct vectors in S are orthogonal.
Recall that in R2 or R3 two vectors that are mutually perpendicular to each other
have dot product equal to zero. The notion of an inner product can thus be used to
infer the (angular) distance between vectors in a vector space, and leads to a special
kind of basis called an orthonormal basis.
Deﬁnition 4. A basis B of a vector space V is called an orthonormal basis (ONB)
if B is an orthogonal set in which every vector is unit normed.
Every ﬁnite dimensional vector space has an orthonormal basis. This is a
consequence of the Gram-Schmidt orthogonalization process [12]. One of the main
advantages of an ONB is that the coefﬁcients in the basis representation, when using
an ONB, are very easy to compute. This is due to the following result.

148
S. Datta and J. Oldroyd
Theorem 2 ([12]). Let B = {ui}n
i=1 be an ONB of V . Any vector v ∈V can be
written in terms of the vectors in B as
v =
n

i=1
⟨v, ui⟩ui.
Theorem 2 shows that the unique coefﬁcients in the basis representation in (1), when
using an ONB, are just given by the inner products ⟨v, ui⟩, and therefore very simple
to calculate. Theorem 2 leads to Parseval’s Formula which can be thought of as a
generalization of the Pythagorean Identity.
Proposition 1 (Parseval’s Formula).
[15] Let {ui}n
i=1 be an ONB of a vector
space V . Then for every v ∈V
∥v∥2 =
n

i=1
|⟨v, ui⟩|2.
(2)
Parseval’s Formula is particularly intimate to frame theory, and the importance of
this can be understood by taking a closer look at (2). It says that the norm of the
signal v is completely determined by the orthonormal basis coefﬁcients {⟨v, ui⟩}.
Suppose that the signal v cannot be analyzed directly but one can measure the
coefﬁcients {⟨v, ui⟩}. Since both sides of (2) have the meaning of energy, this
suggests that some valuable information of the signal can be obtained solely from
its coefﬁcients even if one does not know what the signal is.
A natural question is: why do we wish to generalize bases or why do we want to
look beyond bases? First of all, it might be worth pointing out that once a basis
for a vector space V has been ﬁxed, for each v ∈V , one can just work with
the coefﬁcients of v that appear in the basis representation. This means that if
B = {v1, v2, . . . , vn} is a basis of V and if c1, c2, . . . , cn are the unique coefﬁcients
representing v, that is,
v = c1v1 + c2v2 + · · · + cnvn,
(3)
then in order to store or transmit v, one only uses the vector (c1, c2, . . . , cn). Once
these coefﬁcients are known, one can recover v using (3).
Now suppose that in transmitting v, the coefﬁcients {ci}n
i=1 get corrupted by
noise, and as a result what is received is {ci + μi}n
i=1. During the recovery process,
one obtains
/v =
n

i=1
(ci + μi)vi =
n

i=1
civi +
n

i=1
μivi = v + ϵ.
Instead of a basis suppose that one uses a spanning set, that is, a set that spans V
but is linearly dependent. Such a set could be {v1, . . . , vn, vn+1, . . . , vm} = {vi}m
i=1,

Finite Frame Theory
149
m > n, obtained from B by adding some additional vectors of V . Then v can be
recovered by calculating
/v =
m

i=1
civi +
m

i=1
μivi
where ci = 0, n < i ≤m. Since {vi}m
i=1 is a linearly dependent set, there
is a possibility that the second summation m
i=1 μivi will become zero, thereby
canceling the noise, something that is never possible when using a basis. Intuitively,
this shows how the effect of noise can be reduced by using a redundant set.
As another instance of the beneﬁt of having a redundant spanning set, let us
consider v = (1, 1) ∈R2.1 Using the standard orthonormal basis of R2, {e1 =
(1, 0), e2 = (0, 1)}, v can be written as
v = 1 · e1 + 1 · e2.
If one of the coefﬁcients, say the second coefﬁcient is lost, then at the reconstruction
stage one gets
/v = 1 · e1 + 0 · e2 = (1, 0)
and the error in the reconstruction is
∥v −/v∥2 = 1
where ∥.∥2 is the Euclidean distance in R2. Instead of an ONB, let us now use the
redundant set
{ fk = (cos(2kπ/6), sin(2kπ/6))}5
k=0.
See Figure 1(a). In terms of this set, the vector (1, 1) can be written as
v = 0.333f0 + 0.455f1 + 0.122f2 −0.333f3 −0.455f4 −0.122f5.
If the coefﬁcient corresponding to f0 is lost then one obtains
/v = 0.455f1 + 0.122f2 −0.333f3 −0.455f4 −0.122f5
and the reconstruction error is
∥v −/v∥2 = 1/3
1The authors would like to thank Andy Kebo for sharing this example.

150
S. Datta and J. Oldroyd
x
y
f1
f2
f4
f5
f0
f3
(a) The frame from Section 1.
x
y
f0
f2
f4
(b) The Mercedes-Benz frame.
x
y
z
f0
f1
f2
f3
f4
(c) The (5,3) Grassmannian frame.
Fig. 1 Examples of frames.
which is less than the error when the ﬁrst coefﬁcient was lost while using the
standard ONB of R2. Losing a single coefﬁcient with an ONB in R2 can be
thought of as losing 50% of the coefﬁcients. In the case of the redundant set under
consideration suppose that 50%, that is, three coefﬁcients are lost. If they are the
ﬁrst three, then the reconstructed vector is
//v = −0.333f3 −0.455f4 −0.122f5
and the reconstruction error is
∥v −//v∥2 = 1/
√
2
which is still less than when using the standard ONB of R2.
In the above, what we have sacriﬁced by using a linearly dependent spanning set
instead of a basis is that we no longer have the unique representation of Theorem 1.
The question is whether a unique representation is necessary for our purpose and the
answer is no; as long as we are able to represent every vector in terms of the set, it

Finite Frame Theory
151
does not matter. This notion of adding redundancy is what is incorporated in a frame
for a ﬁnite dimensional vector space. The set { fk = (cos(2kπ/6), sin(2kπ/6))}5
k=0
used in the discussion above is not a basis but is a frame for R2, and we have shown
how redundancy is better when we have transmission losses. The next section gives
the basics of ﬁnite frame theory. The reader is urged to read [6,15] for more details.
An excellent overview of the ideas underlying frames can be found in [18].
2
Frames in Finite Dimensional Spaces
To help us think of frames as generalizations of bases, let us look at Parseval’s
Formula in Proposition 1. Relaxing the condition in (2) gives the following
deﬁnition.
Deﬁnition 5. Let V be an inner product space and let { fi}i∈I be a subset of V
indexed by some countable set I .
(i) The set { fi}i∈I is a frame if there exist constants 0 < A ≤B < ∞such that
for every v ∈V ,
A∥v∥2 ≤

i∈I
|⟨v, fi⟩|2 ≤B∥v∥2.
(4)
(ii) The constants A and B are called the lower and upper frame bound, respec-
tively.
(iii) If A = B, the frame is called a tight frame.
(iv) If for each i ∈I , ∥fi∥= 1, the frame is called a unit normed frame.
Due to Parseval’s Formula, an orthonormal basis is a unit normed tight frame with
frame bound equal to 1. In a ﬁnite d-dimensional vector space V , a ﬁnite set { fi}n
i=1,
n ≥d, is a frame if and only if { fi}n
i=1 is a spanning set of V [6].
Let { fi}n
i=1 be a frame for a ﬁnite dimensional inner product space V . The Bessel
map F : V →Fn is deﬁned by
F(v) = {⟨v, fi⟩}n
i=1,
v ∈V .
(5)
The adjoint of F is given by
F∗: Fn →V ,
F∗({ci}n
i=1) =
n

i=1
ci fi.
(6)
The mapping F is often referred to as the analysis operator, while F∗is referred to
as the synthesis operator. In a ﬁnite dimensional space like Rd or Cd, the synthesis
operator F∗can be written as a d × n matrix whose columns are the frame vectors.
The analysis operator F is then an n × d matrix whose ith row is f ∗
i . In other words,
F∗is just the conjugate transpose of F in this setting.

152
S. Datta and J. Oldroyd
Lemma 1. The analysis operator F given by (5) is one to one.
Proof. To show that F is one to one, it is enough to show that the null space of F
consists of only the zero vector. Let Fv = 0 for some v ∈V . Then for i = 1, . . . , n,
⟨v, fi⟩= 0. Since { fi}n
i=1 spans V , v must equal zero. Thus F is one to one.
⊓⊔
Lemma 2. If {ei}n
i=1 is the standard orthonormal basis 2 of Fn, then for i =
1, . . . , n,
F∗(ei) = fi.
Proof. Let v be a vector in V . Then for i = 1, . . . , n,
⟨v, F∗ei⟩= ⟨Fv, ei⟩
= ⟨v, fi⟩.
Thus F∗ei = fi.
⊓⊔
Lemma 3. The synthesis operator given by (6) maps Fn onto V .
Proof. Let v be a vector in V . Since { fi}n
i=1 spans V , there exist constants
α1, . . . , αn such that
v = α1 f1 + . . . + αnfn
= α1F∗(e1) + . . . + αnF∗(en)
= F∗(α1e1 + . . . + αnen).
where the penultimate step follows from Lemma 2. Thus the vector α1e1 + . . . +
αnen ∈Fn gets mapped to v showing that F∗is onto.
⊓⊔
The composition of F∗with F gives the frame operator
S : V →V ,
S(v) = F∗F(v) =
n

i=1
⟨v, fi⟩fi.
(7)
If we restrict ourselves to either Rd or Cd, then the frame operator S is the d × d
matrix F∗F where F is the matrix corresponding to the analysis operator. Note that
in terms of the frame operator, for any v ∈V ,
n

i=1
|⟨v, fi⟩|2 = ⟨Sv, v⟩,
2ei is the vector whose ith coordinate is equal to 1 and the rest are zero.

Finite Frame Theory
153
and (4) can be rewritten as
A∥v∥2 ≤⟨Sv, v⟩≤B∥v∥2.
(8)
For a tight frame, when A = B, (8) implies that
S = AI,
and so
n

i=1
|⟨v, fi⟩|2 = A∥v∥2
or,
∥v∥2 =
n

i=1
1
A|⟨v, fi⟩|2.
(9)
Note that (9) resembles Parseval’s formula that is satisﬁed by an ONB. In this
regard, unit normed tight frames are the redundant counterparts of orthonormal
bases. Finally, since S = AI for a tight frame, (7) implies a representation of any
v ∈V in terms of the vectors of a tight frame:
v = 1
A
n

i=1
⟨v, fi⟩fi.
(10)
If we deﬁne a new set of vectors as
gi = 1
Afi,
then (10) can be written as
v =
n

i=1
⟨v, gi⟩fi =
n

i=1
⟨v, fi⟩gi.
(11)
As can be seen from (11), for a tight frame, the coefﬁcients in the expression for v
can be obtained simply by calculating the inner products with the gis or the fis, in
a manner similar to the case of an ONB. In the case of a general frame that is not
necessarily tight, a frame representation as in (10) or (11) is still possible and is now
given.
Theorem 3 ([6]). Let { fi}n
i=1 be a frame for a ﬁnite dimensional inner product
space V , and let S be the frame operator. Then
(i) S is invertible and self-adjoint, i.e., S = S∗.

154
S. Datta and J. Oldroyd
(ii) Every v ∈V can be represented as
v =
n

i=1
⟨v, S−1fi⟩fi =
n

i=1
⟨v, fi⟩S−1fi.
(12)
Proof.
(i) We will ﬁrst show that the frame operator S is one to one. By (8), Sv = 0 forces
v = 0. Therefore the null space of S contains only the zero vector and S is
one-to-one. Since S maps V to V , S is also onto, and hence invertible.
By properties of the adjoint operator:
S∗= (F∗F)∗= F∗(F∗)∗= F∗F = S.
Thus S is self-adjoint.
(ii) By (i), S−1 exists, and so for each v ∈V we have
v = S−1Sv = S−1
n

i=1
⟨v, fi⟩fi,
by (7),
=
n

i=1
⟨v, fi⟩S−1fi.
Note that since S is self-adjoint, so is S−1. Thus, again by (7),
v = SS−1v =
n

i=1
⟨S−1v, fi⟩fi
=
n

i=1
⟨v, S−1fi⟩fi.
⊓⊔
The set {S−1fi}n
i=1 is also a frame for V and is called the canonical dual of { fi}n
i=1.
The numbers ⟨v, S−1fi⟩are called the frame coefﬁcients.
If { fi}n
i=1 is a spanning set that is not a basis of V , there must exist constants
{γi}n
i=1, not all zero, such that n
i=1 γi fi = 0. Thus, by adding zero to the ﬁrst part
of (12),
v =
n

i=1
⟨v, S−1fi⟩fi +
n

i=1
γi fi
=
n

i=1
(⟨v, S−1fi⟩+ γi)fi,

Finite Frame Theory
155
and so there are inﬁnitely many representations of v in terms of the frame vectors.
The representation of v in (12) is called the canonical form. With a frame, we
therefore have much more ﬂexibility when choosing a representation for v compared
to a basis representation. However, the special feature of the frame coefﬁcients
{⟨v, S−1fi⟩} is that they have the minimal ℓ2-norm among all coefﬁcients {ci}n
i=1
such that v = n
i=1 ci fi. This can be stated as follows.
Theorem 4 ([6]). Let { fi}n
i=1 be a frame for a ﬁnite dimensional inner product
space V with frame operator S. If v ∈V has the representation v = n
i=1 ci fi,
then
n

i=1
|ci|2 =
n

i=1
|⟨v, S−1fi⟩|2 +
n

i=1
|ci −⟨v, S−1fi⟩|2.
Example 2 (Computing a Frame Expansion). Consider the frame in R2 given by
the rows of
F =
⎡
⎣
1 0
0 1
1 1
⎤
⎦
and let
v =
0 1
−2
1
.
The frame operator corresponding to this frame is given by
S = FTF =
02 1
1 2
1
,
and the frame coefﬁcients for v are
⟨v, S−1f1⟩= 4
3
⟨v, S−1f2⟩= −5
3
⟨v, S−1f3⟩= −1
3.
One can then verify that
v = 4
3f1 −5
3f2 −1
3f3,

156
S. Datta and J. Oldroyd
which is the canonical form of v. The ℓ2-norm of the coefﬁcients in the canonical
form is
2
14
3 . It is not too difﬁcult to see that v can also be written as v = f1 −2f2.
The ℓ2-norm of the coefﬁcients in this expansion is given by
√
5. As expected, this
is greater than the ℓ2-norm of the coefﬁcients in the canonical form.
Note that computing the frame coefﬁcients involves inverting the frame operator
S which can be numerically unstable. In the case of a tight frame, this step is
drastically simpliﬁed due to the fact that S−1 =
1
AI, a constant multiple of the
identity. This feature has made tight frames highly desirable. They emulate ONBs
and at the same time provide the beneﬁts of redundancy that come from using
frames.
The frame potential
[2] of the frame { fi}n
i=1 is the number FP({ fi}n
i=1)
deﬁned by
FP({ fi}n
i=1) =
n

i=1
n

j=1
|⟨fi, fj⟩|2.
The frame potential is used to give an important characterization of unit normed
tight frames and ONBs.
Theorem 5 (Theorem 6.2, [2]). Let d, n ∈N with d ≤n, and let { fi}n
i=1 be a
unit normed frame in Rd or Cd. Then FP({ fi}n
i=1) ≥n2
d with equality if and only if
{ fi}n
i=1 is a unit normed tight frame (or an ONB in the case n = d).
The optimal lower bound of a frame is the supremum over all constants A
that satisfy the left inequality in (4). Similarly, the optimal upper bound is the
inﬁmum over all constants B that satisfy the right side of inequality (4). In the
ﬁnite dimensional setting, the optimal lower and upper frame bounds are given by
the minimum and maximum eigenvalues of the matrix of S. The following theorem
gives some useful results involving the eigenvalues of the frame operator. The proof,
given in [6], is included here.
Theorem 6 ([6]). Let { fi}n
i=1 be a frame for a d-dimensional space V . Then the
following hold.
(i) The optimal lower frame bound is the smallest eigenvalue of S, and the optimal
upper frame bound is its largest eigenvalue.
(ii) Let {λi}d
i=1 denote the eigenvalues of S. Then
d

i=1
λi =
n

i=1
∥fi∥2.
(iii) If { fi}n
i=1 is a tight frame and for all i, ∥fi∥= 1, then the frame bound is
A = n
d.

Finite Frame Theory
157
Proof.
(i) Since the frame operator S : V →V is self-adjoint, there is an orthonormal
basis of V consisting of eigenvectors of S. Denote this eigenvector basis by
{ei}d
i=1 and the corresponding eigenvalues by {λi}d
i=1. Every v ∈V can be
written as
v =
d

i=1
⟨v, ei⟩ei.
Then
Sv =
d

i=1
⟨v, ei⟩Sei =
d

i=1
λi⟨v, ei⟩ei,
and
n

i=1
|⟨v, fi⟩|2 = ⟨Sv, v⟩=
d

i=1
λi|⟨v, ei⟩|2.
Therefore,
λmin∥v∥2 ≤
n

i=1
|⟨v, fi⟩|2 ≤λmax∥v∥2.
This shows that λmin is a lower frame bound and λmax is an upper frame
bound. Taking v to be an eigenvector corresponding to λmin, respectively,
λmax, proves that it is the optimal lower bound, respectively, upper bound.
(ii) We have
d

i=1
λi =
d

i=1
λi∥ei∥2 =
d

i=1
⟨Sei, ei⟩
=
d

i=1
n

j=1
|⟨ei, fj⟩|2.
Interchanging the order of summation and using the fact that {ei}d
i=1 is an ONB
for V gives the desired result.
(iii) By the assumptions, S = AI and so S has one eigenvalue equal to A with
multiplicity d. By part (ii), this means that
dA = n
and this gives A = n
d.
⊓⊔

158
S. Datta and J. Oldroyd
The n×n matrix FF∗is the Gram matrix of the set { fi}n
i=1. The Gram matrix has
rank d, and its non-zero eigenvalues are the same as the eigenvalues of the frame
operator S. The (i, j)th entry of the Gram matrix is the inner product ⟨fj, fi⟩.
Example 3 (Types of Frames).
(a) The frame { fk = (cos(2kπ/6), sin(2kπ/6))}5
k=0 given in Section 1 is a unit
normed tight frame of six vectors in R2. See Figure 1(a). The analysis operator
is given by
F =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
1
0
1/2
√
3/2
−1/2
√
3/2
−1
0
−1/2 −
√
3/2
1/2 −
√
3/2
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
and the frame operator is
S =
0 3 0
0 3
1
.
We thus have a tight frame with frame bound equal to 3.
(b) The vectors { f0, f2, f4} from the frame given in (a) form a special unit normed
tight frame known as the Mercedes-Benz frame. These vectors are the ﬁrst, third
and ﬁfth rows of the matrix F. The corresponding frame operator is the matrix
0 3
2 0
0 3
2
1
.
The frame bound for this frame is therefore equal to 3
2. See Figure 1(b). This
is an example of an equiangular tight frame, see Deﬁnition 6. Note that the
absolute value of the inner product of any two distinct vectors in this set is equal
to 1
2.
(c) Let μ =
1
√
5. The set { fk}4
k=0 ⊂R3 given by the rows of
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
0
0
1

1 −μ2
0
μ
μ
2
1−μ
1+μ
2
(1+2μ)(1−μ)
1+μ
μ
μ
2
1−μ
1+μ
−
2
(1+2μ)(1−μ)
1+μ
μ
−μ
2
1+μ
1−μ
2
(1−2μ)(1+μ)
1−μ
μ
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦

Finite Frame Theory
159
forms a unit normed frame, but not a tight frame, of ﬁve vectors in R3. This
is an example of a Grassmannian frame [3], see Deﬁnition 7. The eigenvalues
of the corresponding frame operator are 1 and 2. By Theorem 6, the optimal
lower frame bound must be 1 and the optimal upper frame bound must be 2. See
Figure 1(c).
(d) Given n ∈N, let ω = e−2πi
n . The n × n discrete Fourier transform (DFT) matrix
is given by
F =
1
√n
⎡
⎢⎢⎢⎢⎢⎣
1
1
1
. . .
1
1
ω
ω2
. . .
ωn−1
1 ω2
ω4
. . .
ω2(n−1)
...
...
...
...
...
1 ωn−1 ω2(n−1) . . . ω(n−1)(n−1)
⎤
⎥⎥⎥⎥⎥⎦
.
The rows form an orthonormal basis of Cn. Let /F denote the n×d matrix formed
by selecting d columns from F. The set { fk}n−1
k=0 given by the rows of /F forms a
tight frame in Cd, but not necessarily a unit normed frame.
3
Equiangular Tight Frames
In a communications system, when there are several signals trying to access the
same channel, it is desired that there is minimum interference between the signals.
The cross correlation between two signals, given by the absolute value of their inner
product, can be interpreted as a measure of their interference. This is minimized
when the two vectors (signals) are mutually orthogonal i.e. their inner product is
zero. However, in a ﬁnite dimensional space, the maximum number of mutually
orthogonal vectors is the same as the dimension. When the number of vectors
exceeds the dimension of the underlying space, we desire that the maximum cross
correlation between any two vectors is minimized. Welch [24] gave the following
lower bound for the maximum cross correlation among unit normed vectors { fi}n
i=1
in Fd:
max
i̸=j |⟨fi, fj⟩| ≥
3
n −d
d(n −1),
n ≥d.
(13)
In order to minimize the maximum cross correlation among vectors, one seeks sets
of vectors that meet the lower bound of (13). Equiangular tight frames (ETFs) are
a class of frames that meet the lower bound. ETFs are highly desirable and are
the closest one can come to an ONB while having the redundancy of a frame,
in the sense of minimizing the cross correlation while maintaining tightness. The
bound
2
n−d
d(n−1) in (13) is called the Welch bound and is denoted by α. The formal
deﬁnition of an ETF is as follows.

160
S. Datta and J. Oldroyd
Deﬁnition 6 ([22]). An equiangular tight frame (ETF) is a set { fi}n
i=1 in a d-
dimensional space V satisfying:
(i) F∗F = n
dI, i.e., the set is a tight frame.
(ii) ∥fi∥= 1, for i = 1, . . . , n, i.e., the set is unit normed.
(iii) |⟨fi, fj⟩| = α, 1 ≤i < j ≤n, where α is the Welch bound.
For a given dimension d and frame size n, an ETF of n vectors in Fd may not
exist [22]. Even when they do exist, ETFs are hard to construct. However, ETFs of
d + 1 vectors for dimension d always exist and can be viewed as the vertices of a
regular simplex centered at the origin [21,22]. Examples of such ETFs can be found
in [13], and an explicit construction is also given in [7]. Note that for such an ETF
the Welch bound is given by α = 1
d . An example of an ETF of three vectors in R2
was given earlier in Example 3(b).
Example 4 (An ETF of Four Vectors in R3). Let d = 3. Consider the four vectors
given by
f1 =
⎡
⎢⎢⎣
−2
√
6
−
√
2
3
−1
3
⎤
⎥⎥⎦,
f2 =
⎡
⎢⎢⎣
2
√
6
−
√
2
3
−1
3
⎤
⎥⎥⎦,
f3 =
⎡
⎢⎢⎣
0
2
√
2
3
−1
3
⎤
⎥⎥⎦,
f4 =
⎡
⎢⎢⎣
0
0
1
⎤
⎥⎥⎦.
In this case, for i ̸= j, ⟨fi, fj⟩= −1/3.
When ETFs cannot exist, the Welch bound cannot be attained by any set of n
vectors in Fd. However, the maximum cross correlation between frame vectors can
still be minimized even if the minimum does not coincide with the Welch bound.
Such sets are called Grassmannian frames [3].
Deﬁnition 7. A frame of n vectors in Fd is called a Grassmannian frame if it is a
solution to
min{max
i̸=j |⟨fi, fj⟩|}
where the minimum is taken over all unit normed frames { fi}n
i=1 in Fd.
An example of a Grassmannian frame of ﬁve vectors in R3 is shown in
Figure 1(c). Figure 2 demonstrates the relationships present between different
families of frames.
3.1
k-Angle Tight Frames
As mentioned previously, equiangular tight frames are useful in applications
because they minimize the maximum cross correlation among pairs of unit vectors,

Finite Frame Theory
161
Frames
Tight frames
UNTFs
ETFs
ONBs
Grassmannian
frames
Fig. 2 Families of frames.
but they are also rare. Therefore, it is desirable to construct sets that mimic ETFs in
some way. If { fi}n
i=1 is an ETF in Rd or Cd, then when i ̸= j, |⟨fi, fj⟩| must be equal
to the Welch bound α. Relaxing this condition, one obtains a larger class of frames
that contains the equiangular tight frames. In particular, a k-angle tight frame is a
set { fi}n
i=1 in a d-dimensional space V satisfying [7]:
(i) F∗F = n
dI, i.e., the set is a tight frame.
(ii) ∥fi∥= 1 for i = 1, . . . , n, i.e., the set is unit normed.
(iii) |⟨fi, fj⟩| ∈{αm}k
m=1 for 1 ≤i < j ≤n, where {αm}k
m=1 ⊂[0, 1].
Example 5 (Mutually Unbiased Bases). Two orthonormal bases { fi}d
i=1 and
{gi}d
i=1 in a d-dimensional space V form a pair of mutually unbiased bases if
|⟨fi, gj⟩| =
1
√
d for 1 ≤i, j ≤d. Let {4fi}2d
i=1 denote the union { fi}d
i=1 ∪{gi}d
i=1.
Then {4fi}2d
i=1 is a 2-angle tight frame, since |⟨4fi,4fj⟩| ∈{0,
1
√
d} for 1 ≤i < j ≤2d.
A way to construct k-angle tight frames uses ETFs as starting points.
Theorem 7. [7] Let d, k ∈N with k < d + 1, and set d′ =
	d+1
k

. Denote the
collection of all subsets of {1, . . . , d + 1} of size k by {Λi}d′
i=1. Let { fi}d+1
i=1 ⊆Rd
denote the ETF with ⟨fi, fj⟩= −1
d for i ̸= j. Deﬁne a new collection {gi}d′
i=1 as
follows:

162
S. Datta and J. Oldroyd
gi :=

j∈Λi fj
∥
j∈Λi fj∥.
Then {gi}d′
i=1 forms a ˆk-angle tight frame of d′ vectors in Rd, where ˆk ≤k.
The proof of Theorem 7 can be found in [7]. The construction of the starting ETF
with ⟨fi, fj⟩= −1
d , i ̸= j, can be done based on an algorithm in [7], and an example
of such an ETF for d = 3 has been provided earlier in this section in Example 4. The
main idea behind the proof of the tightness part is to compute the frame potential of
{gi}d′
i=1 and then use Theorem 5.
Example 6 (A 2-Angle Tight Frame in R2). Consider the Mercedes-Benz (MB)
frame discussed in Example 3(b), and shown below in Figure 3. It consists of the
three vectors in R2 given by
f0 =
0 1
0
1
, f1 =
0 −1/2
√
3/2
1
, f2 =
0 −1/2
−
√
3/2
1
.
Note that for i ̸= j, ⟨fi, fj⟩= −1/2. Let k = 2. Follow the construction given
in Theorem 7, i.e., for every distinct pair fi, fj, i < j, in the MB frame, calculate
fi+fj
∥fi+fj∥to get three more vectors. Now add these three vectors to the MB frame.
Doing so produces a unit normed tight frame of six vectors in R2, that was shown
in Figure 1(a), and is reproduced in Figure 4. { fi}5
i=0 is a 2-angle tight frame: for
0 ≤i < j ≤5, ⟨fi, fj⟩∈{−1
3, 1
3, −1} which means that |⟨fi, fj⟩| ∈{ 1
3, 1}.
3.2
Tight Frames and Graphs
Equiangular tight frames have important connections to graph theory. Perhaps the
most well-known connection is that between ETFs and graph theoretic objects
known as regular two-graphs [16, 21]. In particular, [16] gives a one-to-one
correspondence between equivalence classes of real ETFs and regular two-graphs.
Fig. 3 The Mercedes-Benz
frame.
x
y
f0
f1
f2

Finite Frame Theory
163
Fig. 4 A 2-angle tight frame
in R2.
x
y
f0
f1
f2
f3
f4
f5
Another important connection exists between ETFs and strongly regular graphs as
described below. A graph for which every vertex has the same number of neighbors
is a regular graph. A regular graph G is called a strongly regular graph if any two
adjacent vertices in G have λ common neighbors, and any two non-adjacent vertices
in G have μ common neighbors. In [23], it is shown that a real ETF { fi}n
i=1 in Rd
exists if and only if a particular strongly regular graph exists. Graph theory has
also proven useful in obtaining error estimates for signal reconstruction when using
ETFs in the presence of transmission losses [4].
These connections go beyond characterizing ETFs. A unit normed tight frame
{ fi}n
i=1 ⊂Rd is called a two-distance tight frame3 if ⟨fi, fj⟩∈{α1, α2} ⊂[−1, 1]
for α1 ̸= α2 and 1 ≤i < j ≤n. If G is the Gram matrix of a two-distance tight
frame, then G = I +α1Q1 +α2Q2, where Q1 and Q2 are symmetric binary matrices
with zeros on the diagonal. Recall that a graph G has adjacency matrix Q = [qij]
deﬁned by
qij =

1
if vertex i is adjacent to vertex j,
0
otherwise.
The matrices Q1 and Q2 in the decomposition of the Gram matrix G above can then
be viewed as adjacency matrices of graphs. In [1], the authors prove the following
result.
Theorem 8 (Proposition 3.2, [1]). Let { fi}n
i=1 be a two-distance tight frame with
Gram matrix G = I + α1Q1 + α2Q2, where α1 ̸= ±α2. Then Q1 and Q2 are both
adjacency matrices of strongly regular graphs.
3Note that two-distance tight frames as deﬁned here and in [1] are either ETFs or 2-angle tight
frames from the beginning of Section 3.1.

164
S. Datta and J. Oldroyd
4
Possible Research Topics
4.1
Construction of Grassmannian Frames
Equiangular tight frames are important in many applications, but as discussed
previously they do not always exist. However, ETFs are themselves a speciﬁc
example of a larger class of frames called Grassmannian frames (see Figure 2),
and for any choice of d, n, a Grassmannian frame always exists [3]. Grassmannian
frames are important in frame theory for many of the same reasons that ETFs are,
since Grassmannian frames minimize the maximum cross correlation among sets of
unit vectors.
Despite their importance, many questions remain unanswered about the con-
struction of Grassmannian frames for given values of d, n, in either Rd or Cd.
Constructions in R2 and R3 have been given in [3].
Research Project 1. Some interesting questions that can be explored in the
context of Grassmannian frames are the following.
•
How can one verify whether or not a given frame is a Grassmannian frame?
•
When can one construct Grassmannian frames from previously existing
“structured” objects such as ETFs?
•
Improvements on the Welch bound exist in certain situations, such as the
following bound given in [25]:
max
i̸=j |⟨fi, fj⟩| ≥max
3
n −d
d(n −1), 1 −2n−
1
d−1
5
.
How sharp are these bounds for various values of d and n? Using computer
experiments to ﬁnd a wide variety of unit normed frames and comparing
their maximum cross correlation with these bounds could give insight into
how close to the lower bound one can get and what the minimizers of the
maximum cross correlation could be when ETFs do not exist.
4.2
k-Angle Tight Frames and Regular Graphs
As mentioned in Section 3.2, there exists a correspondence between certain 2-angle
tight frames and strongly regular graphs. A natural course of action would be to
extend this result to k-angle tight frames where k ≥3, and some progress has already

Finite Frame Theory
165
been made in the case k = 3 [19]. Let G be the Gram matrix of a k-angle tight frame
{ fi}, and suppose that |⟨fi1, fj1⟩| = |⟨fi2, fj2⟩| if and only if ⟨fi1, fj1⟩= ⟨fi2, fj2⟩. In
other words, if α ∈{⟨fi, fj⟩}, then −α /∈{⟨fi, fj⟩}. Let {αi}k
i=1 = {⟨fi, fj⟩}, which
are the distinct off-diagonal entries of G. Then
G = I + α1Q1 + · · · + αkQk,
(14)
where the matrices Qi are symmetric binary matrices with 0s along the diagonal for
1 ≤i ≤k. Each matrix Qi is the adjacency matrix of a graph. The question then is
as follows:
Research Project 2.
What can be said about the structure of the graphs determined by the matrices
{Qi}k
i=1 in (14)?
To illustrate this question, consider the matrix G given by
G =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1
1
6
1
6
1
6
1
6 −2
3
1
6
1
6 −2
3 −2
3
1
6
1
1
6
1
6 −2
3
1
6
1
6 −2
3
1
6 −2
3
1
6
1
6
1 −2
3
1
6
1
6 −2
3
1
6
1
6 −2
3
1
6
1
6 −2
3
1
1
6
1
6
1
6 −2
3 −2
3
1
6
1
6 −2
3
1
6
1
6
1
1
6 −2
3
1
6 −2
3
1
6
−2
3
1
6
1
6
1
6
1
6
1 −2
3 −2
3
1
6
1
6
1
6
1
6 −2
3
1
6 −2
3 −2
3
1
1
6
1
6
1
6
1
6 −2
3
1
6 −2
3
1
6 −2
3
1
6
1
1
6
1
6
−2
3
1
6
1
6 −2
3 −2
3
1
6
1
6
1
6
1
1
6
−2
3 −2
3 −2
3
1
6
1
6
1
6
1
6
1
6
1
6
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
G is the Gram matrix of a 2-angle tight frame in R4. If Q1 and Q2 are deﬁned as

166
S. Datta and J. Oldroyd
Q1 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0 0 0 0 0 1 0 0 1 1
0 0 0 0 1 0 0 1 0 1
0 0 0 1 0 0 1 0 0 1
0 0 1 0 0 0 0 1 1 0
0 1 0 0 0 0 1 0 1 0
1 0 0 0 0 0 1 1 0 0
0 0 1 0 1 1 0 0 0 0
0 1 0 1 0 1 0 0 0 0
1 0 0 1 1 0 0 0 0 0
1 1 1 0 0 0 0 0 0 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
and
Q2 =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
0 1 1 1 1 0 1 1 0 0
1 0 1 1 0 1 1 0 1 0
1 1 0 0 1 1 0 1 1 0
1 1 0 0 1 1 1 0 0 1
1 0 1 1 0 1 0 1 0 1
0 1 1 1 1 0 0 0 1 1
1 1 0 1 0 0 0 1 1 1
1 0 1 0 1 0 1 0 1 1
0 1 1 0 0 1 1 1 0 1
0 0 0 1 1 1 1 1 1 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
then
G = I −2
3Q1 + 1
6Q2.
Since each row of Q1 contains three 1s, this means that every vertex of the graph
whose adjacency matrix is Q1 has three adjacent vertices, hence this graph is regular.
A similar statement is true for Q2 and the graph associated with it. See Figure 5. In
fact, the graphs associated to Q1 and Q2 are strongly regular as well by Theorem 8,
but the purpose of this example is only to illustrate how one obtains graphs from
k-angle tight frames and studies their structure.
1
2
3
4
5
6
7
8
9
10
1
2
3
4
5
6
7
8
9
10
Fig. 5 The graphs corresponding to Q1 and Q2.

Finite Frame Theory
167
4.3
Frame Design Issues
Designing frames with good properties is important. This chapter has discussed an
important class of frames called equiangular tight frames. When designing frames,
it is important to know the kinds of transformations under which a given frame
does not lose the properties it already possesses. For example, if U is a unitary
transformation and if { fi} is a unit normed tight frame, then the set {Ufi} is also a
unit normed tight frame. If { fi} is an ETF, then {Ufi} is also an ETF. Recall that for
a tight frame the frame operator S has a nice structure, being a constant multiple of
the identity. In other words, S is a diagonal matrix with all diagonal entries equal to
A, the frame bound. Having such a structure is useful for computational reasons. In
particular, it is convenient to invert S in the expansion formula (12), and the process
is numerically stable. One can ask the following.
Challenge Problem 1. What are the operators M for which {Mfi} is a frame whose
frame operator S is a diagonal matrix?
This is useful since diagonal matrices are also easy to invert and the frame {Mfi}
can therefore offer advantages similar to that of a tight frame. Of course, there are
other variations of the above question that can be considered.
In the context of erasures or losses, as discussed in Section 1, suppose that e
number of frame coefﬁcients are lost during transmission. If the index set of erasures
is denoted by E, then to the receiver it is as if the signal is to be recovered using the
frame { fi}i/∈E, where |E| = e, assuming that { fi}i/∈E is a frame. In the presence of
random noise, it has been shown [13] that when starting with a unit normed frame
the mean-squared error is minimized if the remaining vectors { fi}i/∈E form a tight
frame. Unfortunately, as given in Theorem 4.3 in [13], it is not possible for every
{ fi}i/∈E with |E| = e to be tight. In this context, it might be interesting to investigate
the following.
Research Project 3. Fix the number of erasures e such that e ≤n−d, where
n represents the size of a frame and d is the dimension. Starting with a unit
normed frame of n vectors in Fd, remove e vectors and see for how many of
the
	n
e

cases the set { fi}i/∈E is a tight frame. Try this for different unit normed
frames using a computer. Is it possible to characterize the starting frames that
maximize the number of cases when { fi}i/∈E is a tight frame?

168
S. Datta and J. Oldroyd
4.4
Frame Algorithms
In order to reconstruct a signal v from the frame coefﬁcients {⟨v, fi⟩}n
i=1 one
can use (12). However, that entails inverting the frame operator which can be
complicated if the dimension is large. To avoid inverting S one can ﬁnd successive
approximations to f. This can be done using a well-known algorithm known as the
frame algorithm [6]. This is given below in Lemma 4.
Lemma 4. [6] Let { fk}n
k=1 be a frame for V with frame bounds A, B. Given v ∈V ,
deﬁne the sequence {gk}∞
k=0 in V by
g0 = 0, gk = gk−1 +
2
A + BS( f −gk−1), k ≥1.
(15)
Then
∥f −gk∥≤
B −A
B + A
k
∥f∥.
It should be noted in Lemma 4 above that if B is much larger than A then the conver-
gence is slow. There are acceleration algorithms based on the Chebyshev method
and the conjugate gradient method [14] that improve the speed of convergence
in (15). Other algorithms on approximating the inverse frame operator include work
in [5,20]. An interesting project might be the following.
Research Project 4. Compare the various existing algorithms for inverting a
frame operator and study other techniques to improve existing methods.
4.5
Reconstruction in Presence of Random Noise
Assume that during transmission the frame coefﬁcients get corrupted by some
random noise so that what is received are the corrupted coefﬁcients {⟨v, fi⟩+ ηi}n
i=1
where each ηi has mean zero and variance σ2. Further, for i ̸= j, ηi and ηj are
uncorrelated. The reconstruction of the signal x from the noisy coefﬁcients is done
as follows:
/x =
n

i=1
[⟨v, fi⟩+ ηi]S−1fi = x +
n

i=1
ηiS−1fi.

Finite Frame Theory
169
Due to the assumptions,
E[/x] = x.
Assuming an unbiased estimator, the mean-squared error (MSE) is the trace of the
covariance matrix of /x. It has been shown in [13] that for a single erasure (or lost
coefﬁcient), among all unit normed frames, tight frames minimize the MSE. It is
also shown in [13] that under a single erasure from a unit normed frame, the MSE
averaged over all erasures is minimized if and only if the starting frame is tight. In
[13], a uniform distribution is taken for the erasure model, i.e., each coefﬁcient is
equally likely to be lost. This leads to the following.
Research Project 5. Study the effect of a general distribution for the era-
sure model, and investigate the properties of the starting frame that would
minimize the MSE. As one example, how well do k-angle tight frames, as
described in Section 3.1, perform in the face of erasures?
For more than two erasures, it is known that all tight frames do not minimize the two-
erasure MSE. It has been shown in [16] that under two erasures the optimal frames
are ones that are equiangular. So far, it has been assumed that when a frame vector
has been erased the remaining vectors still form a frame. In this case, assuming
that the location of the loss is known, the signal recovery is done using the frame
operator of the new frame. However, it may be the case that eliminating just a single
vector leaves a set that is no longer a frame. This consideration can lead to other
areas of research.
It might also be interesting to investigate the effect of random noise or other
random phenomena on equiangularity of an ETF.
Research Project 6. Starting from an ETF, estimate the deviations of the
frame from being equiangular or being tight by adding random perturbations
to the frame vectors.

170
S. Datta and J. Oldroyd
Acknowledgements The authors would like to thank the anonymous reviewer for in-depth
comments and helpful suggestions. The authors were partially supported by the NSF under Award
No. CCF-1422252.
References
1. Barg, A., Glazyrin, A., Okoudjou, K., Yu, W.-H.: Finite two-distance tight frames. Linear
Algebra Appl. 475, 163–175 (2015)
2. Benedetto, J.J., Fickus, M.: Finite normalized tight frames. Adv. Comput. Math. 18, 357–385
(2003)
3. Benedetto, J.J., Kolesar, J.D.: Geometric properties of Grassmannian frames for R2 and R3.
EURASIP J. Adv. Signal Process. 2006(1), 1–17 (2006)
4. Bodmann, B.G., Paulsen, V.I.: Frames, graphs and erasures. Linear Algebra Appl. 404(1–3),
118–146 (2005)
5. Casazza, P.G., Christensen, O.: Approximation of the inverse frame operator and applications
to Gabor frames. J. Approx. Theory 103(2), 338–356 (2000)
6. Christensen, O.: An Introduction to Frames and Riesz Bases. Birkhäuser, Boston (2003)
7. Datta, S., Oldroyd, J.: Construction of k-angle tight frames. Numer. Funct. Anal. Optim. 37(8),
975–989 (2016)
8. Datta, S., Oldroyd, J.: Low coherence unit norm tight frames. Linear Multilinear Algebra
(to appear)
9. Daubechies, I.: Ten Lectures on Wavelets. SIAM, Philadelphia (1992)
10. Daubechies, I., Grossmann, A., Meyer, Y.: Painless nonorthogonal expansions. J. Math. Phys.
27(5), 1271–1283 (1986)
11. Dufﬁn, R.J., Schaeffer, A.C.: A class of nonharmonic Fourier series. Trans. Am. Math. Soc.
72(2), 341–366 (1952)
12. Friedberg, S.H., Insel, A.J., Spence, L.E.: Linear Algebra, 4th edn. Prentice Hall, Inc. Upper
Saddle River, NJ (2003)
13. Goyal, V.K., Kovaˇcevi´c, J., Kelner, J.A.: Communicated Henrique, Malvar, S.: Quantized
frame expansions with erasures. Appl. Comput. Harmon. Anal. 10, 203–233 (2001)
14. Grochenig, K.: Acceleration of the frame algorithm. IEEE Trans. Signal Process. 41(12), 3331–
3340 (1993)
15. Han, D., Kornelson, K., Larson, D., Weber, E.: Student Mathematical Library. Frames for
Undergraduates, vol. 40. American Mathematical Society, Providence, RI (2007)
16. Holmes, R.B., Paulsen, V.I.: Optimal frames for erasures. Linear Algebra Appl. 377, 31–51
(2004)
17. Horn, R.A., Johnson, C.R.: Matrix Analysis. Cambridge University Press, Cambridge (1990)
18. Kovaˇcevi´c, J., Chebira, A.: Life beyond bases: the advent of frames (Part I). IEEE Signal
Process. Mag. 24(4), 86–104 (2007)
19. Oldroyd, J.: Regular graphs and k-angle tight frames. (In preparation)
20. Song, G., Gelb, A.: Approximating the inverse frame operator from localized frames. Appl.
Comput. Harmon. Anal. 35(1), 94–110 (2013)
21. Strohmer, T., Heath, R.W. Jr.: Grassmannian frames with applications to coding and commu-
nication. Appl. Comput. Harmon. Anal. 14(3), 257–275 (2003)
22. Sustik, M.A., Tropp, J.A., Dhillon, I.S., Heath, R.W. Jr.: On the existence of equiangular tight
frames. Linear Algebra Appl. 426(2–3), 619–635 (2007)
23. Waldron, S.: On the construction of equiangular tight frames from graphs. Linear Algebra
Appl. 431(11), 2228–2242 (2009)
24. Welch, L.R.: Lower bounds on the maximum cross correlation of signals. IEEE Trans. Inf.
Theory 20(3), 397–399 (1974)
25. Xia, P., Zhou, S., Giannakis, G.B.: Achieving the Welch bound with difference sets. IEEE
Trans. Inf. Theory 51(5), 1900–1907 (2005)
26. Young, R.M.: An Introduction to Nonharmonic Fourier Series. Pure and Applied Mathematics,
vol. 93. Academic [Harcourt Brace Jovanovich Publishers], New York (1980)

Mathematical Decision-Making with Linear
and Convex Programming
Jakob Kotas
Suggested Prerequisites. Differential calculus, Linear algebra.
1
Introduction
The Institute for Operations Research and the Management Sciences (INFORMS)
deﬁnes operations research (OR) as “a discipline that deals with the application
of advanced analytical methods to help make better decisions.” It uses tools and
techniques from mathematics, including modeling, statistics, data analysis, and
optimization, to ﬁnd a maximum (proﬁt, yield) or minimum (cost, risk) solution
to a problem, typically in the presence of one or more system constraints. OR
as a discipline began during World War II with the study of military planning
and resource allocation problems, and as such, has had an applied focus from its
inception. Today, principles of OR are widely used in business practice, and OR is
a well-established research ﬁeld.
OR’s application-driven focus lends itself excellently to a variety of practical
research problems of interest to industry, from scheduling to allocation to manage-
ment. At the same time, OR stands on rigorous mathematical footing, and students
with a more theoretical bent will have no shortage of problems involving structural
properties, uniqueness of solutions, and algorithms.
There are many classes of optimization problems that are of interest in OR
applications. A non-exhaustive list follows:
•
Linear programming: The problem of optimizing a linear objective function
subject to linear equality and inequality constraints. The subject gained popular-
J. Kotas ()
University of Portland, 5000 N Willamette Blvd, Portland, OR 97203, USA
e-mail: kotas@up.edu
© Springer International Publishing AG 2017
A. Wootton et al. (eds.), A Primer for Undergraduate Research, Foundations for
Undergraduate Research in Mathematics, DOI 10.1007/978-3-319-66065-3_8
171

172
J. Kotas
ity due to the development of the Simplex method, one of the ﬁrst reliable and
efﬁcient optimization algorithms. (It should be noted that the word “program,”
in the OR context, is used to mean an optimization problem, and its use predates
that of computer programs.)
•
Convex programming: A generalization of linear programming, convex pro-
gramming is the problem of optimizing a convex objective function subject to
convex constraints. Many practical optimization problems can be formulated
(or re-formulated) as convex problems. Again, efﬁcient algorithms for the
solution of convex problems have been developed, especially for problems of
certain standard forms; thus, formulation of a problem as convex often leads to
computational tractability.
•
Integer programming: The problem of optimizing a linear objective function
subject to linear equality and inequality constraints, in which some or all
variables are constrained to be integer-valued. Integer programs are non-convex
and are generally much more difﬁcult to solve than linear programs.
•
Stochastic programming: The problem of optimizing a system in which some
or all constraints or parameters depend on random variables.
•
Dynamic programming: A technique for solving an optimization problem by
separating it into a collection of simpler sub-problems. Among other appli-
cations, they are useful for sequential decision-making, where the problem of
reaching some optimal state in the future is broken down into a series of ﬁnite or
countable individual problems at each time step.
•
Combinatorial/network optimization: The problem of determining a discrete
optimal object from a ﬁnite set of objects on a graph, a set of vertices with arcs
connecting pairs of vertices. The famous Traveling Salesman problem of ﬁnding
a minimum-distance route connecting a known set of points is one such example.
In applications, oftentimes combinations of these problem classes are used.
However, in this short introduction, we focus on just the ﬁrst two classes of
problems: linear programs and convex programs.
In these problems, there are two different sorts of quantities of interest: param-
eters and decision variables. Parameters are typically known and are treated as
constants for the purposes of solving the problem. Decision variables are the values
that we seek. For example, in determining a shortest-distance route between two
points on a map, the parameters would be the (ﬁxed) distances of each road segment,
which are known; the decision variable would be the sequence of segments that we
decide to travel on. After the problem has been solved and we have a value for
the decision variables, we are often interested in how our solution depends on the
parameter values. In the example, how would our route change if any particular
segment on the map had been shorter, or longer, or removed altogether? This
procedure is called sensitivity analysis.
In this paper, we begin with linear programming, then move to the more general
convex programming, explaining the theory, providing examples, and describing
possible research ideas for both. We conclude with pointers to further reading as
well as software tools for solving these problems.

Mathematical Decision-Making with Linear and Convex Programming
173
2
Linear Programming
A linear program (LP) is a technique for optimization (minimization/maximization)
of a linear objective function subject to linear equality and inequality constraints.
Software packages exist for efﬁcient solution of LPs, even in high dimensions
with many variables and constraints. Thus, formulating a problem as a LP is often
computationally advantageous. LPs have been used in many applications, including
shift scheduling, network design, and manufacturing. We begin this section with an
example, the diet problem. We then discuss a general formulation of LPs, as well as
algorithms for their solution. Finally, we end with a discussion of duality.
2.1
Diet Problem
Let us illustrate the basic idea through an example. We wish to construct a daily diet
with the minimum possible cost. The diet is selected from certain candidate foods
and must satisfy certain nutritional requirements. To begin, we consider only two
candidate foods: milk and bread; and four nutritional requirements: protein, carbs,
vitamins, and sugar. The parameters are given in the following chart.
Unit
Milk
Bread
Min Req.
Max Req.
Protein
gram
6
2
6
none
Carbs
gram
5
15
15
none
Vitamins
gram
1
1
2
none
Sugar
gram
0.6
1
none
3
Cost
$
0.3
0.2
none
none
Let m be the number of units of milk to consume, and b the number of units of
bread; these are our decision variables. We seek m and b to minimize the total cost:
min 0.3m + 0.2b
(1)
subject to the constraints:
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
6m + 2b ≥6
5m + 15b ≥15
m + b
≥2
0.6m + b ≤3
m
≥0
b
≥0.
(2)
Here, the ﬁrst, second, and third constraints correspond to the minimum require-
ments for protein, carbs, and vitamins, respectively. The fourth constraint corre-
sponds to the maximum requirement for sugar. The ﬁfth and sixth constraints are
non-negativity constraints that ensure a physically meaningful result.

174
J. Kotas
Fig. 1 Constraints (solid
lines), feasible region (R), and
lines of constant cost (dotted
lines) for the diet problem.
m
b
R
1
1
direction of decreasing cost
P
Since we have only two variables, we can easily plot the constraints on a pair of
axes. The region where all constraints are satisﬁed is known as the feasible region or
feasible set, and points in the feasible region are called feasible points. On Figure 1,
the feasible region is labeled R. On the same plot, we show dotted lines of constant
cost: where 0.3m + 0.2b is a constant. By moving in a direction of decreasing cost,
we can visually see that the lowest-cost point within the feasible region occurs at
point P, at (m, b) = (0.5, 1.5), which is also a vertex of R. The cost at point P is the
value of the objective function there: 0.3(0.5) + 0.2(1.5) = 0.45.
Upon viewing Figure 1, we make several observations. First, the feasible region
will always be a convex polygon. (It may be bounded or unbounded.) In 2D, we can
think of a convex polygon as one in which all interior angles are ≤180◦; a more
general deﬁnition of convexity is discussed in section 3.1. In higher dimensions, this
generalizes to a convex polytope.
Second, in this problem, the unique solution was a vertex. This will become
important later when we discuss solution algorithms.
2.2
Standard Forms
We now consider a general minimization LP. To avoid trivialities, assume that the
feasible region R is non-empty and that the objective function is bounded below
within R. Then there exists an optimal solution which is a vertex of the feasible
region. (A proof of this statement is given in section 2.6 of Bertsimas and Tsitsiklis
[2]). If there are multiple, non-unique solutions, they occur at an entire edge (or
face, in higher dimensions,) of R. Going back to the diet problem for a moment, this
would have happened happen if the lines of constant cost were parallel with one of
the three minimum requirements of Figure 1. Even in this case, there still exists a
solution at some vertex (in 2 dimensions, the vertices on either end of a polygon
edge). Since in a typical decision-making framework we only need one solution to
implement, the issue of uniqueness is less important, and we may restrict our search
for the solution to only vertices of the feasible region. This is a key motivation for
the Simplex method, which will be discussed shortly.

Mathematical Decision-Making with Linear and Convex Programming
175
Let x ∈Rn be a vector containing the n decision variables and c ∈Rn be a vector
containing the parameters representing coefﬁcients of each corresponding variable
in the objective function. Assume the ith inequality constraint out of m total is of the
form n
j=1 aijxj ≥bi; then, all such constraints can be succinctly written in matrix-
vector notation as Ax ≥b, where A ∈Rm×n is a matrix containing the coefﬁcients
of the constraint functions (including non-negativity constraints), b ∈Rm is a vector
of the constraint right-hand side values, and “≥” is meant in the component-wise
sense. (We will shortly show that this assumption is not restrictive.) If this is the
case, then one general form of an LP is:
min c′x
subject to Ax ≥b.
(3)
We have denoted transpose by ′; thus, c′x = n
i=1 cixi. Other constraints can be
put into the form n
j=1 aijxj ≥bi as well: “≤” inequality constraints of the form
aijxj ≤bi can be changed to greater-than through multiplication by −1, and “=”
constraints of the form aijxj = bi can be written as the pair of inequality constraints
aijxj ≥bi and −aijxj ≥−bi. Finally, any linear maximization problem max c′x is
equivalent to min −c′x.
The feasible region R ⊆Rn is the set of points satisfying the constraints:
R =
7
x
  Ax ≥b
8
.
(4)
Geometrically when R exists it is a convex polytope.
While the general form (3) is geometrically intuitive, from an algorithmic
standpoint, it is often more convenient to write the constraints in a slightly different
way. Namely, we can equivalently deﬁne R of (4) as
R =
7
x
  Ax = b, x ≥0
8
.
(5)
for a different A, x, and b. Namely, equality constraints of the form aijxj = bi are
left alone, while inequality constraints are converted to equality constraints via the
introduction of so-called “slack” or “surplus” variables. We proceed by example.
Consider the “≤” constraint:
3x1 + 4x2 + 5x3 ≤10.
(6)
We introduce a new nonnegative variable x4, called the slack variable. Equa-
tion (6) can then be written equivalently as a pair of equality and non-negativity
constraints:
3x1 + 4x2 + 5x3 + x4 = 10, x4 ≥0
(7)

176
J. Kotas
A “≥” constraint can be handled in a similar way by introducing a so-called
surplus variable. The ﬁnal conversion is to eliminate free variables, that is, variables
that are not restricted to be non-negative or non-positive. If x6 is a free variable, we
eliminate it and introduce two new non-negative variables x7, x8. The free variable
x6 can then be replaced with
x6 = x7 −x8, x7 ≥0, x8 ≥0.
(8)
The LP with constraints written in the aforementioned form:
min c′x
subject to Ax = b,
x ≥0
(9)
is referred to as a “standard-form” LP.
Research Project 1. Develop a more realistic diet problem using nutritional
data and formulate it as an LP. For example, Bosch compared several fast
food chains to determine which, if any, offered a combination of entrees
could meet federal dietary guidelines at lowest cost [3]. Bosch’s formulation
was an integer program, where one or more variables is restricted to be
integer-valued. However, by relaxing this assumption, an analogous LP can
be developed.
Research Project 2. Scheduling problems are a classic area of interest in
OR. Develop a minimum-cost schedule for a company in which employees
work on various tasks by formulating the problem as a LP. Constraints could
include, but are not limited to, working hours for every individual employee;
total working hours spent on each task; and penalizing shift changes to avoid
frequent off-and-on times for each employee.
2.3
Solution of LPs
Many algorithms for solving LPs operate by ﬁrst ﬁnding a feasible point and then
iteratively moving in a direction of improving objective function value. Because
the objective function is linear, any local optimum is also a global optimum; thus,
a greedy algorithm will ﬁnd an optimum eventually. Once a feasible solution has
been found that moving in any direction would result in a worse objective function
value, the algorithm terminates. Algorithms for LPs thus often have an easily
understandable geometric interpretation. The main difﬁculty in solving LPs is that
most problems of interest lie in a high-dimensional space, oftentimes with thousands
(or more) of variables and constraints.

Mathematical Decision-Making with Linear and Convex Programming
177
Fig. 2 A toy problem in 3
variables: the feasible region
is a 3-dimensional convex
polyhedron. The Simplex
method, depicted with a red
arrow, follows edges from
vertex to vertex in a direction
of improving (decreasing, for
a minimization problem)
objective function value.
direction of
decreasing cost
In optimization, algorithmic computational complexity is often described in
terms of strongly polynomial, weakly polynomial, or non-polynomial time. We
assume basic arithmetic operations of addition, subtraction, multiplication, division,
and comparison take one time step to perform. The number of operations in a weakly
polynomial time algorithm is bounded by a polynomial in the number of constraints
and variables. For an algorithm to be strongly polynomial time, additionally the
memory used by the algorithm must be bounded by a polynomial in the number of
constraints and variables. Certain algorithms including interior-point methods are
weakly polynomial; however, no strongly polynomial method has yet been found
for LPs. In fact, the existence of such an algorithm was listed on Stephen Smale’s
18 open problems of mathematics for the 21st century [13]. Nevertheless, there
is a fortunate disconnect between theoretical and practical notions of run time, in
that algorithms exist for solving in LPs that are highly efﬁcient for many practical
problems of interest, despite the fact that they are not strongly (or in many cases,
even weakly) polynomial in theory.
The ﬁrst efﬁcient algorithm for solving LPs was the Simplex method, developed
by George Dantzig in the 1940s. The Simplex method has an intuitive geometrical
interpretation and was widely used for several decades; even today, it lies at the
heart of many LP solvers.
2.3.1
The Simplex Method
Without loss of generality, we consider a minimization problem. The Simplex
method begins with a subroutine to ﬁnd a vertex of the feasible region, if one exists.
It then chooses the edge of the polytope with the fastest decrease (steepest descent)
in objective function. It then moves the solution along that edge until hitting a
vertex; then repeats. There are certain caveats to take into consideration at so-called
degenerate points, where several constraints coincide at the same point, and when a
tie for the steepest descent direction occurs, but the essential geometry is shown in
Figure 2.
The run time is, in theory, exponential in the number of variables. Indeed, a
worst-case scenario for an n-variable problem was found by Klee and Minty where
the Simplex method visits every vertex of a perturbed n-dimensional hypercube, of
which there are 2n, before ﬁnding the optimal solution [8]. Nevertheless, the average
behavior of the Simplex method on problems of interest seems to be signiﬁcantly

178
J. Kotas
better. Of course, we have not provided a rigorous deﬁnition of “average behavior”;
indeed, the problem of deﬁning such a concept and using it to deﬁne the run time of
Simplex method remains somewhat open [1,12].
Research Project 3. Deﬁne an appropriate notion of average-case behavior
for the Simplex method. Computational experiments can be performed by
generating random problems according to some probabilistic setup, and
ﬁnding the run time of the Simplex method on each. Then, using regression,
ﬁnd the relationship between the mean and variance of the observed run times
as a function of the problem size (number of variables + constraints).
2.3.2
Interior Point Methods
In contrast to the Simplex method, where the algorithm visits vertices of the
feasible polytope by traveling along edges of the region, interior point methods
travel through the interior of the feasible region. The basic idea in interior point
methods is to choose a path that optimizes a combination of a reduction in objective
value function and the distance from the edge of the feasible region. This is often
done through the introduction of a barrier function: a function that is inversely
proportional to the distance from the nearest edge of the polytope and thus carries
low values in the center and approaches +∞on the edge itself. Let us introduce
a scalar α ≥0 that indicates the relative importance of the barrier function as
compared with the change in the objective value function c′x. By minimizing an
appropriate sum of the objective function value and α times the barrier function, we
ﬁnd a path through the interior of the feasible region that depends on α. This path is
counterintuitive, as the barrier function avoids the boundary of the feasible region,
when it is known that all solutions lie on the boundary. However, by letting α →0,
we can then recover the optimum of the original objective function.
Interior point methods have been developed which are weakly polynomial-time
in the number of variables. In practice they are often competitive with the Simplex
method and similar edge-tracing algorithms, and for some sparse problems are
signiﬁcantly faster.
2.4
Transportation Problem
The following example illustrates another application that can be modeled as a LP.
In order to produce a certain product, raw material must be transported from the
mine to a plant where it is reﬁned. Say a company has control over 2 mines in
Colorado and Virginia, and 3 plants in Alabama, Minnesota, and New Mexico. We
denote the mines C and V, and plants A, M, and N, respectively. Below is a table of
the parameters representing estimated shipping costs between each plant and mine,
in thousands of dollars per ton of material (Table 1).

Mathematical Decision-Making with Linear and Convex Programming
179
Table 1 Cost of shipping
from each mine to each plant,
in thousands of $ per ton of
ore.
Plant A
Plant M
Plant N
Mine C
22
18
7
Mine V
14
20
24
C
V
A
M
N
mines
plants
For a certain week, the Colorado mine is expected to output 150 tons of ore,
and the Virginia mine 130. To fulﬁll demand in the same week, the Alabama plant
requires at least 88 tons of ore, the Minnesota plant 125 tons, and the New Mexico
plant 55 tons.
Exercise 1.
(a) Formulate the problem as a linear program. Denote xCA as the decision variable
for the amount of ore shipped (in tons) from Colorado to Alabama, etc.
(b) A colleague suggests that it would be easiest if the Colorado mine exclusively
supplied New Mexico and Alabama, and the Virginia mine exclusively supplied
Minnesota (only enough to meet demand.) Check that this option is feasible.
Find the total shipping cost (in thousands of dollars) in this case.
(c) Solve for all x values using an off-the-shelf linear program solver. What is the
total shipping cost (in thousands of dollars) in this case? How much did we save
as compared to part (b)?
Solution 1.
(a)
min 22xCA + 18xCM + 7xCN + 14xVA + 20xVM + 24xVN
subject to
xCA + xCM + xCN ≤150
xVA + xVM + xVN ≤130
xCA + xVA ≥88
xCM + xVM ≥125
xCN + xVN ≥55
xCA, xCM, xCN, xVA, xVM, xVN ≥0

180
J. Kotas
(b)
x =

xCA, xCM, xCN, xVA, xVM, xVN
′ =

88, 0, 55, 0, 125, 0
′
c′x =

22, 18, 7, 14, 20, 24

88, 0, 55, 0, 125, 0
′ = 4831
(c)
x =

xCA, xCM, xCN, xVA, xVM, xVN
′ =

0, 95, 55, 88, 30, 0
′
c′x =

22, 18, 7, 14, 20, 24

0, 95, 55, 88, 30, 0
′ = 3927
This solution represents a savings of 4831 −3927 = 904 dollars.
Research Project 4. Formulate the problem of delivering electricity to con-
sumers as a LP. Electricity must be delivered to meet demand and can come
from a variety of sources, such as coal, natural gas, wind, and solar. Investigate
the effect of a carbon tax on the solution by penalizing electricity coming from
nonrenewable resources. Consider how future growth in a certain geographic
area will affect the solution.
2.5
Duality
In calculus, students are typically taught the Lagrange multiplier method for
optimizing functions subject to equality constraints. The key idea there is to
introduce a new scalar parameter for each equality constraint (the multiplier),
and reformulate the hard constraints as soft constraints additively combined with
the original objective function (and scaled by the appropriate multiplier)– this
quantity is the Lagrangian L. The new problem is to optimize L with no constraints.
Assuming differentiability of L, this can be solved by equating the partial derivatives
of L with zero. For the proper choice of the multipliers, the presence or absence of
each hard constraint does not affect the optimal value of the objective function; thus,
the optimal solution to the original constrained problem and the new unconstrained
problem are the same.
Duality theory is a generalization of the Lagrange multiplier method that also
accounts for inequality constraints. We again associate a multiplier with each
constraint and seek a set of values for the multipliers such that the speciﬁc value of
the constraint does not affect the optimal objective function value. We consider the
standard-form LP of (9), which we call the “primal” problem. Let x∗be an optimal
value of x, and assume x∗exists. We relax the standard-form problem by changing
the hard constraint Ax −b = 0 to a soft one with the introduction of a length-m
vector of multipliers λ. We then have the problem:

Mathematical Decision-Making with Linear and Convex Programming
181
min c′x + λ′(b −Ax)
subject to x ≥0
(10)
Let g be the optimal value of this relaxed problem. Since g depends on λ, let us
consider it to be a function: g(λ). The relaxed problem is broader than the original
problem in that if Ax = b, we recover the original problem, but there are also feasible
points in which Ax ̸= b. In other words, the feasible region of the relaxed problem R′
contains the feasible region of the original problem R: R ⊆R′. For this reason, the
minimum value of the objective function in R′ must be no larger than the minimum
value in R.
g(λ) = min
x≥0

c′x + λ′(b −Ax)

≤c′x∗+ λ′(b −Ax∗) = c′x∗.
(11)
Here, the inequality arises because x∗is a member of the set x ≥0 and thus is a
feasible solution to the primal problem, and the ﬁnal equality is due to the fact that
x∗satisﬁes Ax∗= b because it again is a feasible solution to the primal problem.
Thus, g(λ) is a lower bound for the optimal cost c′x∗.
Now consider the unconstrained problem:
max g(λ)
(12)
This problem gives us the tightest possible lower bound for the optimal cost c′x∗.
This problem is known as the dual problem. Strong duality proves that the optimal
cost of the dual problem is equal to the optimal cost c′x∗of the primal problem (see
Theorem 4.4 of Bertsimas and Tsitsiklis [2]). Continuing a bit further, we have
g(λ) = min
x≥0

c′x + λ′(b −Ax)

= λ′b + min
x≥0

(c′ −λ′A)x

.
(13)
Noting that the quantity (c′ −λ′A)x can be made arbitrarily small unless c′ −
λ′A ≥0, we restrict our search in the dual problem to c′ −λ′A ≥0. The dual
problem then becomes, in its ﬁnal form:
max λ′b
subject to λ′A ≤c′
(14)
One important feature of duality is that the dual of the dual of a primal
problem is itself (see Theorem 4.1 of Bertsimas and Tsitsiklis [2]). Because of the
correspondence of the solutions of the primal and dual problems, solvers can make
use of both the primal and dual problems in searching for a solution. For example,
a dual Simplex method can be developed that pivots on the vertices of the dual
problem’s feasible region.

182
J. Kotas
3
Convex Programming
In this section we investigate the problem of minimizing convex problems, which
are deﬁned as minimizing a convex function subject to convex constraints. Formally,
the problem is:
min
f0(x)
subject to fi(x) ≤bi, i = 1, . . . , m
(15)
where x ∈Rn are the variables, f0 : Rn →R is the objective function, and fi :
Rn →R, i = 1, . . . , m are the constraint functions, and the objective and constraint
functions are convex. We deﬁne convexity in the sections to follow.
In general, convex optimization problems do not have analytical solutions, but
there do exist efﬁcient algorithms for their solution. Convex programs are more
general than linear programs and a large number of problems can be formulated as
convex problems, though there are some tricks and such a formulation can often feel
more of an art than a science.
In this section, we begin by deﬁning convex sets and functions. We then
give several well-known examples of classes of problems that are convex, before
concluding with a mention of some software packages for solving convex problems.
3.1
Convex Sets
Whereas linear programs have feasible regions that are convex polytopes, convex
programs have feasible regions that are general convex sets. A set S ⊆Rn is
convex if
θx + (1 −θ)y ∈S, ∀0 ≤θ ≤1, x, y ∈S.
(16)
In other words, for any two points in S, the line segment connecting them lies
entirely within S. Some examples of convex sets are given here.
•
Convex hull: The convex hull of a set S is the intersection of all convex sets
containing S. Intuitively, the convex hull “ﬁlls in” the non-convex sections of the
set, or could be thought of as stretching an elastic band around S. If S ⊆Rn is
a set of ﬁnite, discrete points x1, . . . , xk ∈Rn, and θ1, . . . , θk are constants such
that k
i=1 θi = 1 and θi ≥0 ∀i = 1, . . . , k, then the convex hull R is the set of
points:
R =
9
x ∈Rn   x = θ1x1 + θ2x2 + . . . + θkxk
:
(17)
which is also known as the convex combination of x1, x2, . . . , xk (Figure 3).
•
Ellipsoid: A Euclidean ellipsoid R ⊂Rn centered at xc ∈Rn can be written as:
R =
9
xc + Au
   ||u||2 ≤1
:
(18)
with u ∈Rn and A ∈Rn×n and non-singular.

Mathematical Decision-Making with Linear and Convex Programming
183
Fig. 3 A is a set consisting of
the dark crescent along with
three isolated points. R
consists of the dark and light
regions and is the convex hull
of A.
A
R
•
Hyperplanes and halfspaces: Take x ∈Rn, a ∈Rn and b ∈R. A hyperplane
R ⊂Rn can be deﬁned as the set of points
R =
9
x
   a′x = b, a ̸= 0
:
(19)
for some a and b. Similarly, a halfspace can be written as the set of points
R =
9
x
   a′x ≤b, a ̸= 0
:
.
(20)
for some a and b.
•
Convex polytope: Take x ∈Rn, b ∈Rm, and A ∈Rm×n. As discussed in the
linear programming section, a convex polytope R ⊂Rn is the set of points:
R =
9
x
   Ax ≥b
:
.
(21)
Convex sets have many important properties, but perhaps the most important
is that the intersection of (even countably many) convex sets is convex. This fact
ensures that adding more convex constraints will still result in a convex program.
3.2
Convex Functions
A function f : Rn →R is convex if its domain is a convex set and:
f(θx + (1 −θ)y) ≤θf(x) + (1 −θ)f(y),
0 ≤θ ≤1
(22)
Further, f is strictly convex if its domain is a convex set and:
f(θx + (1 −θ)y) < θf(x) + (1 −θ)f(y),
0 ≤θ ≤1
(23)
Clearly, all strictly convex function are convex, but the reverse is not true. The
graphical interpretation follows directly from the deﬁnition and is illustrated through
Figures 4 and 5.

184
J. Kotas
Fig. 4 f (t) is a strictly
convex function. For any x,y
in the domain of f (t), the line
segment connecting f (x) and
f (y) is strictly above the
function f (t) on x < t < y.
f(t)
t
x
y
Fig. 5 f (t) is a convex, but
not strictly convex function.
For any x,y the line segment
connecting f (x) and f (y) is
strictly above the function
f (t) on x < t < y; however
this is not the case for x and z,
where the line segment
connecting f (x) and f (z) lies
directly on the function f (t).
f(t)
t
x
y
z
Some examples of convex functions follow.
•
Linear-afﬁne: f(x) = ax + b on R for any a, b ∈R
•
Exponential: eax on R for any a ∈R
•
Power: xa on x > 0 for a ≥1 or a ≤0
•
Negative logarithm: −log(ax) on x > 0 for a > 0
If a function f is twice differentiable with open domain D, then it is convex if and
only if
∇2f(x) ≥0 ∀x ∈D.
(24)
Here x ∈Rn and ∇2f(x) denotes the Hessian
∇2f(x)ij = ∂2f(x)
∂xi∂xj
, i, j = 1, . . . , n
(25)
and “≥” is meant in the sense of a generalized inequality; in this case, that ∇2f(x)
is positive semi-deﬁnite (psd) (Figure 6).

Mathematical Decision-Making with Linear and Convex Programming
185
Fig. 6 A convex function
f : R2 →R. The feasible
region is R; the minimum of f
over R is x∗.
x∗
x1
x2
z
z = f(x1, x2)
R
We can also show that f is convex if it can be obtained from simple convex
functions by convexity-preserving operations, such as
•
Nonnegative weighted sum of convex functions
•
Pointwise maximum of convex functions
•
Composition of convex functions
3.3
Classes of Convex Programs
Many solvers take advantage of certain structural properties of the convex program
in question. Here we list some important classes of convex programs seen in
applications.
•
Linear program (LP): A linear objective function subject to linear equality
and inequality constraints; the feasible region is a convex polytope. Discussed
previously.
•
Quadratic program (QP): A quadratic objective function subject to linear equality
and inequality constraints; the feasible region is therefore still a convex polytope.
The general form of a QP is:

186
J. Kotas
min 1
2x′Px + q′x + r
subject to Gx ≤h, Ax = b
(26)
where the decision variable is x ∈Rn, P is a psd matrix: P ∈Sn
+, q ∈Rn,
and r ∈Rn. Here we have explicitly separated the inequality constraints and
equality constraints; the m inequality constraints are contained within Gx ≤h
where G ∈Rm×n and h ∈Rm. The p equality constraints are contained within
Ax = b where A ∈Rp×n and b ∈Rp. P is required to be psd in order for the
objective function to be convex, as ∇2(x′Px + q′x + r) = P.
Exercise 2. Least-squares optimization
Formulate the linear least-squares approximation problem in n variables with
m > n data points as a QP.
Solution 2. If A is an m×n matrix containing the input data points and b is a m×1
vector containing the output data points, then the problem is:
min ||Ax −b||2
2 = x′A′Ax −2b′Ax + b′b.
(27)
The objective function here is quadratic in x, so we must show that A′A is psd. A
square matrix M ∈Rp×p is psd if x′Mx ≥0 for all x ∈Rp. For any x, we have
x′(A′A)x = (Ax)′(Ax) = ||Ax||2 ≥0.
(28)
Thus, A is psd and so the objective function is convex. Furthermore, the
constraints are trivially linear as there are none. Therefore, this is a QP. We can
thus also consider the constrained least-squares problem, in which values of x must
lie in an interval l ≤x ≤u; it is also a QP:
min ||Ax −b||2
2 = x′A′Ax −2b′Ax + b′b
subject to li ≤xi ≤ui, i = 1, . . . , n
(29)
Exercise 3. Portfolio optimization
This example is from section 4.7.6 of Boyd and Vandenberghe [4]. Consider the
problem of optimizing two criteria in assembling a ﬁnancial portfolio: maximize the
mean return and minimize the variance of the return. Let x be a vector containing the
fractions of each of four possible assets. Each asset’s mean and standard deviation
of return is given as:

Mathematical Decision-Making with Linear and Convex Programming
187
Asset
Mean Return
Std.Dev. of Return
1
12%
20%
2
10%
10%
3
7%
5%
4
3%
0%
Furthermore, the correlation coefﬁcients between assets are: ρ12 = 30% and ρ13 =
−40%; other pairs zero. Thus, the correlation matrix Σ is:
Σ =
⎡
⎢⎢⎣
0.22
0.3 × 0.2 × 0.1 −0.4 × 0.2 × 0.05 0
0.3 × 0.2 × 0.1
0.12
0
0
−0.4 × 0.2 × 0.05
0
0.052
0
0
0
0
0
⎤
⎥⎥⎦
We deﬁne the mean return vector to be p: p =

0.12, 0.1, 0.07, 0.3
′, and we
deﬁne a scaling factor μ that accounts for the relative importance of minimizing the
variance of the return against maximizing the mean return. Formulate the problem
as a quadratic program and solve.
Solution 3. The problem is formulated as:
min −p′x + μx′Σx
subject to 1′x = 1,
x ≥0.
(30)
Here 1 represents a vector the same length as x with all entries being 1. Due to the
form of the objective function, this is a quadratic program with linear constraints.
The solution for various values of μ is graphed below, showing explicitly the tradeoff
between the standard deviation of return versus the mean return (Figure 7).
Research Project 5. Chapter 16 of Nocedal and Wright [11] delves into
algorithms for QPs. Similar to the Simplex method, looking deeper into the
run time of these algorithms for sample problems could provide an avenue to
more theoretical projects.
•
Quadratically-constrained quadratic program (QCQP): A quadratic objective
function subject to convex quadratic constraints. The general form of a QCQP is:
min 1
2x′P0x + q′
0x + r0
subject to 1
2x′Pix + q′
ix + ri ≤0, i = 1, . . . , m
Ax = b
(31)
with Pi ∈Sn
+ ∀i = 0, . . . , m.

188
J. Kotas
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
Std.Dev. of Return
Mean Return
Fig. 7 Tradeoff between standard deviation and mean of the return.
•
Second-order cone program (SOCP): A linear objective function subject to so-
called second-order cone constraints. The general form of a SOCP is:
min f ′x
subject to ||Aix + bi||2 ≤c′
ix + di, i = 1, . . . , m
Fx = G
(32)
with Ai ∈Rn1×n and F ∈Rp×n. SOCPs are a generalization of QCQPs and LPs.
QCQPs can be turned into SOCPs by reformulating the objective function as a
constraint.
Research Project 6. One problem from the area of robotics and optimal
control is grasping a rigid body with robot ﬁngers. To do so, we must
determine the amount of force each ﬁnger shall exert. Lobo et al. [9] describe
a formulation of this problem as a SOCP which takes into account friction
and equilibrium constraints and limits on contact forces. We may simply
be interested in whether the object can be grasped at all: this amounts to a
feasibility problem, where we simply determine whether or not the feasible
region is non-empty. If a solution does exist, we can investigate a variety
(continued)

Mathematical Decision-Making with Linear and Convex Programming
189
of problems, such as ﬁnding the gentlest grip in some sense, or ﬁnding a
grip which has the smallest difference in forces on each ﬁnger. Formulate the
problem with a given number of ﬁngers and a given object. This problem
could also incorporate data-gathering, namely the physical properties of the
object to be grasped.
Research Project 7. Section 8.8 of Boyd and Vandenberghe [4] describes
a ﬂoor-planning problem, where we seek the minimum perimeter fence to
bound a set of rectangular objects of known dimension. Variants of the
problem, including having a minimum spacing between the objects and
allowing the dimensions (but not area) of the objects to vary, are considered.
These can be formulated as SOCPs (where the || · ||2 constraint corresponds
to a Euclidean distance) or in some cases even LPs depending on the variant.
Using this as a starting point, optimal packing problems can be considered
for boxes or other shapes. This application has uses in shipping and transport
problems.
•
Semi-deﬁnite program (SDP): One general form of an SDP is:
min c′x
subject to x1F1 + x2F2 + . . . + xnFn + G ≥0
Ax = b
(33)
with Fi, G ∈Sk, i = 1, 2, . . . , n. The ﬁrst constraint is known as a linear matrix
inequality (LMI) constraint; since the left hand side is a k × k matrix, the “≥”
here is a generalized inequality meaning that x1F1 + x2F2 + . . . + xnFn + G is
psd. If F1, . . . , Fn, G are all diagonal, then this formulation reduces to a linear
program. SDPs are a generalization of SOCPs, as the SOCP constraints can be
written as LMIs.
Research Project 8. Weinberger and Saul have formulated learning algo-
rithms with applications to image processing as SDPs. Such algorithms can
recognize characters in handwritten text, or identify whether faces from
different image are the same person’s, even from different angles. Projects
could be developed to identify a range of objects in images[14,15].

190
J. Kotas
Research Project 9. An abundance of SDP applications can be found in
the “Handbook of Semideﬁnite Programming” by Wolkowicz, Saigal, and
Vandenberghe
[16]. Projects could involve extending these applications
and/or formulating new problems as SDPs.
3.4
Solvers
Many software packages exist to solve convex problems. One convenient solver
for solving convex problems of moderate size (including LPs) is CVX, [5] which
can be downloaded and used as a Matlab R
⃝package. Other solvers include Gurobi
[6] and CPLEX, [7] all of which are free for academic use, as well as Matlab’s
Optimization ToolboxTM [10].
4
Concluding Remarks
Optimization is a powerful tool for solving many applied problems of interest to
operations research. In this brief chapter we discussed linear programming, followed
by the more general convex programming and speciﬁc forms therein. Many of
these classes of problems have efﬁcient algorithms for their solution, even in high
dimensions; thus, formulation of an optimization problem in one of these forms
often results in greatly improved computational tractability. For the undergraduate
student, there are many open problems that are application-based. In addition,
delving into the inner workings of algorithms for generic problems could provide
an avenue to interesting projects.
4.1
Further Reading
Parts of this chapter were adapted from the textbooks “Introduction to Linear
Optimization” by Bertsimas and Tsitsiklis, [2] and “Convex Optimization” by
Boyd and Vandenberghe [4]. Another comprehensive text is Nocedal and Wright’s
“Numerical Optimization.” [11].
References
1. Adler, I., Megiddo, N., Todd, M.J.: New results on the average behavior of simplex algorithms.
Bull. Am. Math. Soc. 11(2), 378–382 (1984)
2. Bertsimas, D., Tsitsiklis, J.N.: Introduction to Linear Optimization. Athena Scientiﬁc, Belmont
(1997)
3. Bosch, R.A.: The battle of the burger chains: which is best, burger king, mcdonald’s, or
wendy’s? Socio Econ. Plan. Sci. 30(3), 157–162 (1996)

Mathematical Decision-Making with Linear and Convex Programming
191
4. Boyd, S., Vandenberghe, L.: Convex Optimization. Cambridge University Press, Cambridge
(2004)
5. Grant, M., Boyd, S.: CVX: Matlab software for disciplined convex programming, version 2.1.
http://cvxr.com/cvx, Mar (2014)
6. Gurobi optimization. http://www.gurobi.com/products/gurobi-optimizer
7. IBM ILOG cplex optimization studio community edition.
https://www-01.ibm.com/software/websphere/products/optimization/cplex-studio-
community-edition
8. Klee, V., Minty, G.J.: How Good is the Simplex Algorithm? Defense Technical Information
Center (1970)
9. Lobo, M.S., Vandenberghe, L., Boyd, S., Lebret, H.: Applications of second-order cone
programming. Linear Algebra Appl. 284, 193–228 (1998)
10. MathWorks.: Matlab optimization toolbox. http://www.mathworks.com/products/optimization
11. Nocedal, J., Wright, S.J.: Numerical Optimization, 2nd edn. Springer, Berlin (2006)
12. Shamir, R.: The efﬁciency of the simplex method: a survey. Manag. Sci. 33(3), 301–34 (1987)
13. Smale, S.: Mathematical problems for the next century. Math. Intell. 20(2), 7–15 (1998)
14. Weinberger, K.Q., Saul, L.K.: Unsupervised learning of image manifolds by semideﬁnite
programming. Proc. 2004 IEEE Computer Society Conference on Computer Vision and Pattern
Recognition (2004)
15. Weinberger, K.Q., Saul, L.K.: Distance metric learning for large margin nearest neighbor
classiﬁcation. Machine Lear. Res. 10, 207–244 (2009)
16. Wolkowicz, H., Saigal, R., Vandenberghe, L. (eds.): Handbook of Semideﬁnite Programming.
Kluwer Academic Publishers, Boston (2000)

Computing Weight Multiplicities
Pamela E. Harris
Suggested Prerequisites. Group theory and introductory linear algebra.
1
Introduction
Representation theory is a subject at the intersection of abstract and linear algebra.
It uses tools from both ﬁelds to understand how elements of algebraic structures,
such as groups or algebras, behave as linear transformations between vector spaces.
The challenge in working in this ﬁeld is the daunting amount of background and
deﬁnitions necessary to understand the questions of interest. However, it is often
the case that these questions can be boiled down and presented in the language
of combinatorics. This is the approach we take to present the weight multiplicity
computations involved in the representation theory of Lie algebras.
Computing weight multiplicities, which is equivalent to determining the dimen-
sion of a vector subspace, is central to the study of the representation theory of Lie
algebras. In the language of combinatorics, the weight multiplicity computation is
determined by ﬁnding the number ways we can express a ﬁnite set of vectors (called
weights) as nonnegative integral sums of a ﬁxed set of vectors (called positive roots).
This is analogous to ﬁnding the number of ways to express a positive integer as a
sum of positive integers. For example, the integer 5 can be written as a sum of
positive integers in the following seven ways:
5, 4 + 1, 3 + 2, 3 + 1 + 1, 2 + 2 + 1, 2 + 1 + 1 + 1, 1 + 1 + 1 + 1 + 1.
P.E. Harris ()
Department of Mathematics and Statistics, Williams College, Williamstown, MA 01267, USA
e-mail: pamela.e.harris@williams.edu
© Springer International Publishing AG 2017
A. Wootton et al. (eds.), A Primer for Undergraduate Research, Foundations for
Undergraduate Research in Mathematics, DOI 10.1007/978-3-319-66065-3_9
193

194
P.E. Harris
Each of the above expressions are referred to as partitions of 5. In the Lie algebra
setting, the set of positive roots plays the role of the positive integers and a weight
plays the role of the integer we are partitioning. For example, in the Lie algebra
sl3(C) (deﬁned in Section 2.2) the set of positive roots is denoted by
Φ+ = {α1, α2, α1 + α2}
and the question of interest is: Given n, m ∈N := {0, 1, 2, 3, . . .} in how many
ways can we express nα1 + mα2 as a nonnegative integral sum of positive roots?
The answer is min(n, m)+1, as the number of distinct ways to express nα1+mα2 as
a sum of positive roots depends entirely on the number of times we use the positive
root α1 + α2. Since we can use the positive root α1 + α2 from 0 to min(n, m) times
we reach the desired result.
The connection between the above combinatorial question and computing weight
multiplicities comes from the use of Kostant’s weight multiplicity formula [21]:
m(λ, μ) =

σ∈W
(−1)ℓ(σ)℘(σ(λ + ρ) −(μ + ρ))
(1)
where the function ℘counts the number of ways to express σ(λ + ρ) −(μ + ρ)
as a nonnegative integral sum of positive roots. As it stands, Equation (1) is
involved and the main roadblock is understanding all of the moving pieces. We now
provide working deﬁnitions of the objects in this formula and present the technical
deﬁnitions in Section 2.
We begin by noting that m(λ, μ) is read as the multiplicity of the weight
μ in the ﬁnite-dimensional complex irreducible representation of a simple Lie
algebra with dominant integral highest weight λ. This multiplicity represents a
dimension of a very special vector subspace associated to μ called a weight space,
which can be thought of as a generalized eigenspace. Although, the weights μ
and λ are linear functionals (functions from a vector space to its ﬁeld of scalars)
we treat them as vectors that we express in two important bases: the simple
root basis and the fundamental weight basis (more on this in Section 2). The
importance of λ is that there is a bijection between dominant integral weights λ,
which are nonnegative integral sums of fundamental weights, and ﬁnite-dimensional
irreducible representations of a simple Lie algebra. This result is known as the
theorem of the highest weight.
We now describe the ﬁnite group W indexing the sum in Equation (1), which is
called the Weyl group. In the Lie algebra slr+1(C), which we study in this paper,
the Weyl group is isomorphic to Sr+1 the symmetric group on r + 1 letters. More
generally, the Weyl group is deﬁned as a group generated by reﬂections about
hyperplanes that lie perpendicular to the simple roots α1, α2, . . . , αr, which
are identiﬁed as vectors in Rr+1. When we specialize r = 2 and consider the Lie
algebra sl3(C), we can visualize the roots and the generators of the Weyl group,
denoted s1 and s2, as presented in Figure 1. Then the elements of the Weyl group

Computing Weight Multiplicities
195
Fig. 1 Roots of sl3(C) along
with the hyperplanes
perpendicular to the simple
roots.
a1
−a1
a2
−a2
a1 +a2
−a1 −a2
s1
s2
are concatenations of the reﬂections s1 and s2, that is, words in these letters. Given
that W ∼= S3 and since the adjacent transpositions (1 2), (2 3) generate S3, we can
identify s1 with (1 2) and s2 with (2 3) to generate all of the elements of the Weyl
group: 1, s1, s2, s1s2, s2s1, s1s2s1, where 1 denotes the empty word, which is the
identity of W. Given σ ∈W we let ℓ(σ) represent the smallest number k so that σ is
a product of k reﬂections. For example, ℓ(s1) = 1 as s1 is a minimal expression for
this Weyl group element, but ℓ(s1s1) = 0 as s1s1 = 1, which has length zero.
Lastly, we need to know how to compute σ(λ+ρ)−(μ+ρ) for σ ∈W, where ρ
is deﬁned as the half sum of the positive roots. Since the reﬂections perpendicular to
the simple roots generate all of the elements of W it is enough to know explicitly how
the generators act on λ+ρ. In the Lie algebra slr+1(C), if s1, s2, s3, . . . , sr denote
the reﬂections perpendicular to the simple roots α1, α2, α2, . . . , αr, respectively,
then
si(αj) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
αj
if |i −j| > 1
−αj
if i = j
αi + αj
if |i −j| = 1.
The action of any other Weyl group element uses the fact that the generators act
linearly. That is, for any r ∈N and constants c1, c2, . . . , cr all elements σ ∈W
satisfy
σ(c1α1 + c2α2 + · · · + crαr) = c1σ(α1) + c2σ(α2) + · · · + crσ(αr).
For example, in the Lie algebra sl3(C) we know ρ = 1
2

α∈Φ+ α = α1 + α2, and
if σ = s1s2, λ = 2α1 + α2, and μ = 0, then we can compute

196
P.E. Harris
s1s2(λ + ρ) −(μ + ρ) = s1s2(3α1 + 2α2) −(α1 + α2)
= 3s1s2(α1) + 2s1s2(α2) −(α1 + α2)
= 3s1(α1 + α2) + 2s1(−α2) −(α1 + α2)
= 3[s1(α1) + s1(α2)] −2s1(α2) −(α1 + α2)
= 3[(−α1) + (α1 + α2)] −2(α1 + α2) −(α1 + α2)
= −3α1,
which cannot be written as a nonnegative integral sum of positive roots, hence
℘(s1s2(λ + ρ) −ρ) = 0.
Exercise 1. In the Lie algebra sl3(C) verify that if n, m ∈N, then
s1s2s1(nα1 + mα2) = s2s1s2(nα1 + mα2).
Having fully described the moving pieces of Equation (1) we now consider
the types of complications that arise in weight multiplicity computations. First,
the number of terms in the computation is determined by the order of the Weyl
group. At ﬁrst glance this may not seem like a problem, as the Weyl group is a
ﬁnite group. However, we have remarked that the Weyl group of the Lie algebra
slr+1(C) is isomorphic to Sr+1 and, hence, the number of terms in Equation (1)
is (r + 1)!, which grows extremely fast. Second, we must compute the value of
Kostant’s partition function, a function for which no general closed form is known.
Thus, we have reached a point in which not only do we have a factorial number of
terms in the sum, but we also do not have a closed formula for computing the value
of each term.
These complications may dishearten even the most enthusiastic representation
theorist. Luckily for us, there are ways in which we can discover new results in this
area by focusing on two types of problems. The ﬁrst is motivated by the observation
that in practice most terms appearing in the weight multiplicity formula actually
contribute a value of zero. This means that for many σ ∈W it is not possible
to express σ(λ + ρ) −(μ + ρ) as a nonnegative integral sum of positive roots.
Hence, one can work on describing and enumerating the elements of the Weyl group
that contribute a nonzero value to m(λ, μ) for ﬁxed weights λ and μ. This leads to
numerous open problems. Second, once we can reduce the sum to only the terms
that contribute nontrivially, i.e. not a value of zero, we can focus on ﬁnding closed
formulas for the partition function. In light of this, the objective of this paper is to
present techniques to approach the computation of weight multiplicities from this
point of view.
This work is organized as follows: Section 2 provides the technical deﬁnitions
and the necessary background to begin our study. This background section assumes
the reader has a working knowledge of linear algebra, but no familiarity with Lie
algebras or their representations. Hence, this section is quite technical and some

Computing Weight Multiplicities
197
readers may decide to begin their study in Section 3, which is dedicated to learning
through examples, and return to the background section as deﬁnitions and concepts
are needed. The detailed exposition of Section 3 illustrates techniques one may use
to begin investigating the open problems provided in Section 4.
2
Background
This section provides a short historical account of the representation theory of
simple Lie algebras along with the technical background and deﬁnitions needed
to study this ﬁeld. The concepts presented in this section are technical and may
require multiple reads to digest all that is presented. However, these concepts are
presented here so that this article is self contained. The reader is encouraged to
skip ahead to Section 3 (Learning from examples) which limits the study to the Lie
algebra slr+1(C), and return to this section as needed for the technical deﬁnitions
and concepts.
2.1
History
The representation theory of Lie algebras has been understood for quite some time
due to the work of Élie Cartan [6], Hermann Weyl [24], and Harish-Chandra who
“. . . develop[ed] a general algebraic theory of the irreducible representations of (a
Lie algebra) g” [12,23]. Their collective work showed that the representations of a
semisimple Lie algebra g can be analyzed by choosing a Cartan subalgebra h. Then
the structure of a ﬁnite-dimensional irreducible representation τ : g →gl(V) of a
Lie algebra g on the vector space V can be studied by decomposing the vector space
V into the direct sum of subspaces:
V = ⊕Vα.
(2)
This direct sum is indexed via a ﬁnite set of eigenvalues α called weights, and the
corresponding eigenspace Vα is called a weight space. The multiplicity of a weight
appearing in (2) is deﬁned to be the dimension of the weight space Vα. This theory
reduces the analysis of representations to determining which weights occur in (2)
and with what multiplicity.
To compute this multiplicity, we use the theorem of the highest weight, which
states that every irreducible representation of a semisimple Lie algebra g arises as
a highest weight representation with highest dominant integral weight λ, which
we denote L(λ). The converse holds true: All highest weight representations are
irreducible representations of g. This theorem provides a one-to-one correspondence
between the ﬁnite-dimensional complex irreducible representations of a semisimple
Lie algebra and the set of dominant integral weights of g.
We now use Kostant’s weight multiplicity formula to compute the multiplicity of
the weight μ in the representation L(λ), which we denote by m(λ, μ) [21]:
m(λ, μ) =

σ∈W
(−1)ℓ(σ)℘(σ(λ + ρ) −(μ + ρ)),
(1)

198
P.E. Harris
where ρ = 1
2

α∈Φ+ α, W is the Weyl group, ℓ(σ) is the length of σ, and ℘denotes
Kostant’s partition function. The value of the partition function ℘(ξ) is computed by
counting the number of ways the weight ξ can be written as a nonnegative integral
combination of positive roots. Combinatorics will play a role in ﬁnding the value of
this partition function.
We remark that even though (1) exists to compute weight multiplicities, its
implementation can be extremely difﬁcult. This is due to the fact that the order of
the Weyl group grows factorially as the rank of the Lie algebra increases. Also there
is a lack of general closed formulas for the value of the partition function involved.
Motivated by these complications, the author’s previous work has focused on ﬁnding
concrete descriptions of Weyl alternation sets, which are subsets of the Weyl group
that contribute nonzero terms to the sum in (1), see [13–15, 17]. Additionally, the
author, with collaborators Insko and Omar, computed closed formulas for the value
of the partition function on the highest root [16].
Other work in the literature connects the value of the partition function to volume
computations for polytopes and includes vector partition formulas for multiplicities
of slr(C) and generating functions for weight multiplicities in the rank 2 Lie
algebras [1,2,4,22]. This growing literature continues to provide new insights and
results in this classical subject.
We remark that although this research seems very technical, the main question
of interest stems from determining when an element of the Weyl group is in the
Weyl alternation set and, hence, contributes a nonzero term to the multiplicity
formula. Using this information we turn to the question of counting the number
of ways one can write a weight as a sum of positive roots. Our approach to these
problems involves the use of combinatorial techniques accessible to undergraduate
students. Using these techniques students can contribute original research to the
representation theory of Lie algebras, a topic rarely seen at the undergraduate level.
2.2
Technical Background and Deﬁnitions
Throughout this work we assume that the reader has a working knowledge of linear
algebra, but little background in Lie algebras and their representations is assumed.
Following the notation of [10] we begin with the needed deﬁnitions to make our
approach precise. For the reader interested in more detailed background on Lie
algebras and their representations we recommend [9,10,18].
Deﬁnition 1. A vector space g over a ﬁeld F together with a bilinear map [·, ·] :
g × g →g is said to be a Lie algebra if the map satisﬁes:
1. [X, Y] = −[Y, X] for all X, Y ∈g (skew symmetry).
2. [X, [Y, Z]] + [Y, [Z, X]] + [Z, [X, Y]] = 0 for all X, Y, Z ∈g (Jacobi identity).

Computing Weight Multiplicities
199
The bilinear map is often referred to as the Lie bracket and the dimension of the Lie
algebra g is its dimension as a vector space over F.
Calculus students have studied Lie algebras in vector calculus, but usually not
under this name. Which leads us to another exercise.
Exercise 2. Conﬁrm that R3 is a Lie algebra under the cross product of vectors.
Finite-dimensional (classical) Lie algebras are classiﬁed into families in the
following way. For any r ≥2, let Mk(C) denote the set of k × k matrices with
complex valued entries, then the classical Lie algebras of type A, B, C, and D are
deﬁned as:
•
Type Ar (r ≥1): slr+1(C) = {X ∈Mr+1(C) : Trace(X) = 0}.
•
Type Br (r ≥2): so2r+1(C) = {X ∈M2r+1(C) : Xt = −X}.
•
Type Cr (r ≥3): sp2r(C) = {X ∈M2r(C) : XtJ = −JX}, where J =
0 0 I
−I 0
1
with I the r × r identity matrix.
•
Type Dr (r ≥4): so2r(C) = {X ∈M2r(C) : Xt = −X}.
In these cases, the Lie bracket is the commutator bracket. That is, if X, Y ∈g,
then [X, Y] = XY −YX. In addition to these families, there are ﬁve other ﬁnite-
dimensional simple Lie algebras, which do not belong to the Lie algebra families
mentioned above, and are called the exceptional Lie algebras. These Lie algebras are
identiﬁed as G2, F4, E6, E7, and E8. As we specialize to the Lie algebra slr+1(C) we
omit their deﬁnition, but point the interested reader to [20] for further information
on the exceptional Lie algebras. Having deﬁned a Lie algebra we now shift our
attention to their representations.
Deﬁnition 2. A representation of a Lie algebra g on a vector space V is a Lie
algebra homomorphism τ : g →gl(V), where gl(V) denotes the set of all linear
maps from the vector space V to itself.
A Lie algebra homomorphism is a linear map between two Lie algebras that is
compatible with the Lie bracket. Namely, the map f : g →g′ is said to be a Lie
algebra homomorphism if it satisﬁes
f([X, Y]) = [f(X), f(Y)]
for all X, Y ∈g.
(3)
The bracket on the left hand side of (3) is the bracket deﬁning the Lie algebra g,
whereas the bracket on the right hand side of (3) is the bracket deﬁning the Lie
algebra g′.
Deﬁnition 3. The Lie algebra representation τ : g →gl(V) is said to be irreducible
if the only subspaces W ⊆V satisfying τ(X)(W) ⊂W for all X ∈g are (0) and V.
One example of a Lie algebra representation is the trivial representation, which
is deﬁned by letting every element of the Lie algebra act as the identity map on the

200
P.E. Harris
vector space V. An important example of an irreducible representation is the adjoint
representation of a Lie algebra. This representation uses the fact that the Lie algebra
g is itself a vector space. Thus, we can set V = g and use the Lie bracket deﬁning
the Lie algebra g as the map by which each Lie algebra element acts on V = g.
More precisely, the adjoint representation is deﬁned as
ad : g →gl(g),
where each X ∈g deﬁnes a map from g to itself. That is, X →adX, where adX(Y) =
[X, Y] for all Y ∈g. Much is known about the adjoint representation and it will be the
object of detailed study in our examples. Later, we even show a connection between
this representation and the Fibonacci numbers, but before we do so we need to know
about Cartan subalgebras.
Deﬁnition 4. We say h is a Cartan subalgebra of g if it satisﬁes the following:
•
h is a subalgebra of g, i.e. h ⊂g that is closed under the Lie bracket.
•
h is nilpotent, meaning the lower central series
h > [h, h] > [[h, h], h] > [[[h, h], h], h] > · · ·
eventually terminates.
•
h is self-normalizing, namely if [X, Y] ∈h for all X ∈h, then Y ∈h.
For any Lie algebra g, there are many choices for a Cartan subalgebra. Following
the convention set in [10] for slr+1(C) we choose the Cartan subalgebra h as the set
of traceless diagonal matrices in slr+1(C). That is,
h =

diag[a1, a2, . . . , ar+1] : a1, a2, . . . , ar+1 ∈C and
r+1

i=1
ai = 0
5
.
Challenge Problem 1. Verify that h, as deﬁned above, satisﬁes Deﬁnition 4.
In general, we let h∗denote the dual of the (chosen) Cartan subalgebra, where
the dual of a vector space consists of all linear functionals on the vector space. For
us this means that an element ξ ∈h∗is a linear map ξ : h →C.
Deﬁnition 5. If h∗denotes the dual of the Cartan subalgebra h, then the weights of
the adjoint representation for the Lie algebra g are the linear functionals ξ ∈h∗, i.e.
maps ξ : h →C. For the weight α ∈h∗, deﬁne
gα = {X ∈g : [H, X] = α(H)X, for all H ∈h}.
(4)
We now summarize some Lie algebra information we will need in order to
compute weight multiplicities. First, note that (4) implies that the weights of the

Computing Weight Multiplicities
201
adjoint representation for a Lie algebra are a generalization of an eigenvalue and gα
is a generalization of an eigenspace. If α ̸= 0, and gα ̸= (0), then α is called a
root and gα is called a root space. Whenever α is a root the dim gα = 1. The set Φ
is called the set of roots for (g, h) the root system. Then the Lie algebra g has the
following root space (or Cartan) decomposition
g = h ⊕

α∈Φ
gα.
One can decompose1 the root system Φ = Φ+ ∪−Φ+, where Φ+ denotes the set
of positive roots and −Φ+ denotes the set of negative roots. A subset of the positive
roots Δ = {α1, α2, . . . , αr} ⊂Φ+ is a set of simple roots if every γ ∈Φ+ can
be written uniquely as γ = n1α1 + n2α2 + · · · + nrαr where n1, . . . , nr ∈N. It is
known that the set of simple roots is unique. This uniqueness, together with the fact
that Φ+ spans h∗, implies that Δ is a basis for h∗.
Combinatorially, we note that if β = n1α1 + · · · + nrαr with n1, . . . , nr ∈N is
a root, then we deﬁne the height of β, relative to Δ, as ht(β) = n1 + · · · + nr. The
positive roots of a Lie algebra are thus described as the roots β for which ht(β) > 0.
Moreover, a root β is called the highest root of Φ (or of the Lie algebra), relative to
the simple roots Δ, if ht(β) > ht(γ) for all roots γ ̸= β.
For each α ∈Φ+ there exist eα ∈gα and fα ∈g−α such that the element
hα = [eα, fα] ∈h satisﬁes α(hα) = 2 and we call hα, also denoted by ˇα, the coroot
of α. For X, Y ∈g, deﬁne the symmetric bilinear form (X, Y) = tr(XY) on g. Let hR
be the real span of the coroots, then the real dual space, h∗
R, is the real linear span of
the roots. Since the trace form is positive deﬁnite on hR, we can use it to identify hR
with h∗
R to obtain a positive deﬁnite inner product, (, ), on h∗
R. We deﬁne the simple
root reﬂection sα : h∗→h∗by
sα(β) = β −2(β, α)
(α, α) α.
(5)
The Weyl group is a ﬁnite group that plays an important role in the representation
theory of Lie algebras. The Weyl group is deﬁned as the group W = W(g, h) of
orthogonal transformations of h∗
R generated by the simple root reﬂections, as deﬁned
in (5). So each element σ ∈W can be written as the product of generators σ =
sαi1 sαi2 · · · sαik , where 1 ≤ij ≤r for all j = 1, . . . , k. If sαi1sαi2 · · · sαik is minimal
among all such expressions for σ, then we call k the length of σ and write ℓ(σ) = k.
If k = 0, then σ is the empty product of simple reﬂections and hence σ = 1 is the
identity element in W.
Let {ϖ1, . . . , ϖr} be the set of fundamental weights of g corresponding to our
choice of h. The fundamental weights are deﬁned by the conditions ϖi(hαj) = δij,
1The decomposition of the root system into positive and negative roots involves a choice of Borel
subgroup. The interested reader can see more details of this choice in [10].

202
P.E. Harris
where δij denotes the Kronecker delta function, which is 1 when i = j and 0
otherwise. The set of integral and dominant integral weights are deﬁned as
P(g) = {a1ϖ1 + · · · + arϖr|a1, . . . , ar ∈Z} and
(6)
P+(g) = {n1ϖ1 + · · · + nrϖr|n1, . . . , nr ∈N}, respectively.
(7)
With this background on Lie algebras, let us return to representation theory by
restating the theorem of the highest weight. This theorem “asserts that among the
weights that occur in the decomposition, there is a unique maximal element, relative
to a partial order coming from a choice of positive roots for G. This maximal
element, called the highest weight, occurs with multiplicity one and uniquely
determines the representation” [10, p. 129]. Conversely, every dominant integral
weight of a simple (in fact, semisimple) Lie algebra g is the highest weight of an
irreducible ﬁnite-dimensional representation of g. Therefore, for any λ ∈P+(g)
we let L(λ) denote the irreducible highest weight representation of g whose highest
weight is λ.
The theorem of the highest weight explicitly states a one-to-one correspondence
between dominant integral weights and ﬁnite-dimensional complex irreducible
representations of a Lie algebra. This together with the fact that all representations
of a ﬁnite-dimensional (semisimple) Lie algebra are completely reducible, i.e. can
be decomposed into the direct sum of irreducible representations, we can now focus
on studying the structure of the irreducible representations.
Let τ : g →gl(V) be an arbitrary irreducible ﬁnite-dimensional representation
of the Lie algebra g on the vector space V. The structure of this representation can
be studied by observing that the vector space V can be decomposed as the direct
sum
V =
;
Vα,
(8)
which is indexed by a ﬁnite number of linear functionals α ∈h∗. Moreover, the
linear functionals α ∈h∗, over which the direct sum in (8) is taken, are eigenvalues
as they satisfy
H(v) = α(H)v
for any H ∈h and v ∈Vα.
Deﬁnition 6. An eigenvalue α ∈h∗appearing in (8) is called a weight of the
representation, and the associated vector subspace Vα is called a weight space. The
dimension of the weight space Vα is called the multiplicity of the weight α.
As one can imagine, our goal is to now determine the weights that actually appear
in (8), as well as compute the dimension of the associated weight space. In other
words, we aim to compute m(λ, μ) the multiplicity of the weight μ in an irreducible
highest weight representation with highest weight λ, which we denote by L(λ). If
m(λ, μ) = 0, then dim(Vμ) = 0. Hence Vμ = (0) will appear in the sum (8).
However, this term appears trivially and does not provide any further information

Computing Weight Multiplicities
203
on the nontrivial subspaces that appear in the decomposition (8). Hence, we are
interested in ﬁnding those weight spaces Vμ for which dim(Vμ) > 0. In this case, if
m(λ, μ) > 0 then μ does appear in (8) and the dimension of Vμ is exactly m(λ, μ).
One way to compute the multiplicity m(λ, μ) is through the use of Kostant’s
weight multiplicity formula [21]:
m(λ, μ) =

σ∈W
(−1)ℓ(σ)℘(σ(λ + ρ) −(μ + ρ)),
(2)
where ρ =
1
2

α∈Φ+ α, W is the Weyl group, and ℘denotes Kostant’s partition
function, which counts the number of ways the weight σ(λ + ρ) −(μ + ρ) may be
written as a nonnegative integral sum of positive roots.
One complication in using (1) to compute weight multiplicities is that closed
formulas for the value of Kostant’s partition function are often unknown. Also we
must contend with the complication that the order of the Weyl group, over which
the sum in (1) is taken, grows factorially in terms of the Lie algebra’s rank.
In practice, one develops the intuition, as was noted in [8], that most of the terms
in Kostant’s weight multiplicity formula are zero and hence do not contribute to the
overall multiplicity. This means that the partition function involved in (1) allows us
to reduce the computation drastically as many of the terms σ(λ+ρ)−ρ−μ cannot be
expressed as nonnegative integral sums of positive roots. With this in mind, we aim
to describe the elements of W that actually contribute nonzero terms to (1) which
introduces the following.
Deﬁnition 7. For λ, μ integral weights of g deﬁne the Weyl alternation set to be
A (λ, μ) = {σ ∈W| ℘(σ(λ + ρ) −(μ + ρ)) > 0}.
This deﬁnition implies that σ ∈A (λ, μ) if and only if σ(λ + ρ) −(μ + ρ) can
be expressed as a nonnegative integral combination of positive roots. In particular,
since all positive roots are nonnegative integral combinations of the simple roots,
we deduce that σ ∈A (λ, μ) if and only if σ(λ + ρ) −(μ + ρ) can be expressed
as a nonnegative integral combination of the simple roots. There will be times
when specifying the rank of the Lie algebra is important. In these cases, we use
the notation Ar(λ, μ) in place of A (λ, μ).
We now continue our study with concrete examples, which help us develop the
necessary techniques to begin working on some open problems in this area.
3
Learning Through Examples
This section presents an approach to the computation of weight multiplicities
and shows that although the background material is extremely technical, the
techniques used in computing multiplicities are accessible to students with a linear
algebra and abstract algebra background. Working through some examples will help
students learn the background material and will lead students to make contributions
to the representation theory of Lie algebras, a topic rarely encountered at the
undergraduate level.

204
P.E. Harris
In this section we limit our study to the Lie algebra g = slr+1(C) and more
speciﬁcally to the case where r = 2. Let us record some important information
about sl3(C), which will be helpful shortly. The simple roots of sl3(C) are given by
Δ = {α1, α2},
the set of roots is
Φ = {α1, α2, α1 + α2, −α1, −α2, −α1 −α2},
the positive roots are
Φ+ = {α1, α2, α1 + α2}
and ρ = 1
2

α∈Φ+ α = α1 + α2. Since the highest root is the positive root whose
coefﬁcient sum (height) is the largest among all roots, we see that the highest root
of sl3(C) is α1 + α2, which we denote by ˜α = α1 + α2. It is a coincidence that
ρ = ˜α as this only happen in sl3(C).
As we described in Section 1, in sl3(C) the roots can be visualized on a 2-
dimensional plane, see Figure 2. In this ﬁgure the black dashed lines represent
the real span of the simple roots, and the red solid lines represent the reﬂections
perpendicular to the labeled simple root. Namely, s1 denotes the element of the Weyl
group that takes a root and reﬂects it about the line perpendicular to the simple root
α1, similarly for s2. As these simple reﬂections generate the Weyl group, all of the
elements will be concatenations (also referred to as words) of s1 and s2. We let 1
denote the empty word, which is the identity element in W. Note that the simple
reﬂections are elements of order two. If we reﬂect any weight (vector) about the
same hyperplane (line in this case) twice, it returns us to the original weight (vector).
Hence, it acts the same as if we did not reﬂect the weight at all.
We can now begin our study by working through an in-depth example.
Fig. 2 Roots of sl3(C) along
with the hyperplanes
perpendicular to the simple
roots.
a1
−a1
a2
−a2
a1 +a2
−a1 −a2
s1
s2

Computing Weight Multiplicities
205
Example 1. Compute the dimension of the zero weight space in the adjoint
representation of the Lie algebra sl3(C).
Solution 1. In general, the adjoint representation of a Lie algebra g is the repre-
sentation with highest weight equal to the highest root of g. Hence, computing the
dimension of the zero weight space in the adjoint representation is equivalent to
computing
m(˜α, 0) =

σ∈W
(−1)ℓ(σ)℘(σ(˜α + ρ) −ρ),
(9)
where ˜α is the highest root of g. To compute this multiplicity, we ﬁrst concretely
describe the Weyl alternation set A (˜α, 0). To do so we must compute σ(˜α + ρ) −ρ
for every element σ of the Weyl group, as the Weyl alternation set consists of Weyl
group elements for which the expression σ(˜α+ρ)−ρ can be written as a nonnegative
integral sum of the positive roots.
Since we need to compute the value of σ(˜α + ρ) −ρ for every element σ ∈W
we must know explicitly how s1 and s2 act on weights. For example, if we want
to determine s1s2(α1), ﬁrst we ﬁnd s2(α1) and then apply s1 to the result. Hence,
concatenation of simple reﬂections behave as compositions of functions, and we
must compute by working from the inner-most simple reﬂection and move outward.
Let us compute s1s2(α1) directly. Using Figure 1, note that reﬂecting α1 about s2
yields α1+α2. Then reﬂecting α1+α2 about s1 yields α2. Thus s1s2(α1+α2) = α2.
Following this process for all possible elements of the Weyl group, which are words
in s1 and s2, we arrive at the entries in Table 1, which provide the action of the Weyl
group elements on the roots of sl3(C).
Now that we understand how the elements of the Weyl group act on the roots of
the Lie algebra slr+1(C), we can compute σ(˜α + ρ) −ρ for every σ ∈W. Since
˜α + ρ = 2α1 + 2α2, we can use Table 1 to compute
1(˜α + ρ) −ρ = ˜α + ρ −ρ = ˜α = α1 + α2
(10)
s1(˜α + ρ) −ρ = 2s1(α1 + α2) −α1 −α2 = −α1 + α2
(11)
s2(˜α + ρ) −ρ = 2s2(α1 + a2) −α1 −α1 = α1 −α2
(12)
Table 1 Weyl group elements and their action on the roots of sl3(C).
Weyl group element σ σ(α1)
σ(α2)
σ(α1 + α2) σ(−α1)
σ(−α2)
σ(−α1 −α2)
1
α1
α2
α1 + α2
−α1
−α2
−α1 −α2
s1
−α1
α1 + α2
α2
α1
−α1 −α2 −α2
s2
α1 + α2
−α2
α1
−α1 −α2 α2
−α1
s1s2
α2
−α1 −α2 −α1
−α2
α1 + α2
α1
s2s1
−α1 −α2 α1
−α2
α1 + α2
−α1
α2
s1s2s1
−α2
−α1
−α1 −α2
α2
α1
α1 + α2

206
P.E. Harris
s1s2(˜α + ρ) −ρ = 2s1s2(α1 + α2) −α1 −α2 = −3α1 −α2
(13)
s2s1(˜α + ρ) −ρ = 2s2s1(α1 + α2) −α1 −α2 = −α1 −3α2
(14)
s1s2s1(˜α + ρ) −ρ = 2s1s2s1(α1 + α2) −α1 −α2 = −3α1 −3α2.
(15)
For every element of the Weyl group, we have written σ(˜α + ρ) −ρ as a linear
combination of simple roots. Now we need to determine which elements of the
Weyl group are elements of the Weyl alternation set A (˜α, 0). The question we need
to answer is:
For which σ ∈W can we write σ(˜α + ρ) −ρ
as a nonnegative integral sum of the positive roots?
Note that if an expression c1α2 + c2α2 is to be written as a nonnegative sum of
positive roots, you must be able to write it as a nonnegative integral sum of simple
roots, since, by deﬁnition, all positive roots are nonnegative integral sums of simple
roots. Hence, we can refer back to Equations (10)-(15) and if we see a negative
coefﬁcient on either of the simple roots α1 or α2, this immediately implies that the
respective expression cannot be written as a nonnegative integral sum of the positive
roots. This indicates that the value of the partition function on such expressions is
zero, and hence that particular element of the Weyl group is not an element of the
Weyl alternation set. Doing this establishes that in the Lie algebra sl3(C), the only
element in A (˜α, 0) is 1, the identity element of W.
This reduces the sum in the computation of the multiplicity m(˜α, 0) from the 6
terms, i.e. the six elements of the Weyl group, to only the term associated to the
identity element. Hence, we can reduce equation (9) to
m(˜α, 0) = (−1)ℓ(1)℘(˜α).
(16)
To ﬁnish the computation we restate the following facts: the length of the identity
element, denoted ℓ(1), is by deﬁnition 0, and for any weight ξ, ℘(ξ) counts the
number of ways the weight ξ can be written as the nonnegative integral sum of
positive roots. Observe that there are only two ways to write ˜α as a sum of positive
roots. One way to write ˜α is by using the two simple roots α1 and α2. The second
is to write ˜α using the positive root α1 + α2. This establishes that m(˜α, 0)=2.
In the previous example, we saw that by concatenating simple reﬂections we
eventually created all of the group elements of W. It is important to note that
sometimes different words in the letters s1 and s2 are in fact the same. For example,
s1s2s1 = s2s1s2, as the reader who worked out Exercise 1 showed. These types of
relations are called braid relations on Coxeter groups. Coxeter groups form a larger
family of groups, which include the Weyl groups, and there is a vast literature on
their combinatorial properties. We point the interested reader to [5,7,19] for a more
detailed study of Coxeter groups.
For our purposes, we now summarize information on the simple roots, funda-
mental weights, and the action of the simple reﬂections on the simple roots in the
general case of the Weyl group associated to slr+1(C) with r ≥1. This information
will be of importance in our next few examples.

Computing Weight Multiplicities
207
For the Lie algebra of type Ar, the simple roots are
Δ = {α1, α2, . . . , αr}.
The set of positive roots consists of the simple roots and sums of consecutive simple
roots:
Φ+ = Δ ∪{αi + αi+1 + · · · + αj : 1 ≤i < j ≤r}.
From this we observe that the highest root, the one with largest coefﬁcient sum, is
given by
˜α = α1 + α2 + · · · + αr.
(17)
If2 1 ≤i ≤r, then
ϖi = r + 1 −i
r + 1
(α1 + 2α2 + · · · + (i −1)αi−1)
+
i
r + 1((r −i + 1)αi + (r −i)αi+1 + · · · + αr).
(18)
By deﬁnition, ρ = 1
2

α∈Φ+ α. However there is an additional deﬁnition of ρ.
Challenge Problem 2. Prove that
ρ = ϖ1 + ϖ2 + · · · + ϖr,
(19)
where ϖ1, ϖ2, . . . , ϖr are the fundamental weights of the Lie algebra slr+1(C).
The Weyl group is generated by the simple reﬂections si (1 ≤i ≤r) associated
with the simple roots αi (1 ≤i ≤r). The reﬂections s1, s2, . . . , sr act on the
simple roots α1, α2, . . . , αr in the following way: If 1 ≤i, j ≤r are nonconsecutive
integers, then
si(αj) = αj
and
sj(αi) = αi.
(20)
If 1 ≤i ≤r −1, then
si(αi−1) = αi−1 + αi,
(21)
si(αi) = −αi,
(22)
si(αi+1) = αi + αi+1.
(23)
2This is [10, Exercise 3.2(a)].

208
P.E. Harris
If i = r, then
sr(αr−1) = αr−1 + αr
(24)
sr(αr) = −αr.
(25)
If 1 ≤i, j ≤r, then the action of the simple reﬂections on the fundamental weights
is deﬁned by
si(ϖj) =

ϖj −αi
if i = j
ϖj
if i ̸= j.
(26)
Another important property of the action of Weyl group elements on weights is that
the action is linear. That is, all elements σ ∈W satisfy
σ(c1ξ1 + c2ξ2 + · · · + cnξn) = c1σ(ξ1) + c2σ(ξ2) + · · · + cnσ(ξn)
for any n ∈N, constants c1, c2, . . . , cn, and weights ξ1, ξ2, . . . , ξn.
Example 1 illustrated a well-known result in Lie theory:
The dimension of the zero weight space in the adjoint representation
is equal to the rank of the Lie algebra, where the rank is
equal to the dimension of any of its Cartan subalgebras.
For the classical Lie algebras of types Ar, Br, Cr, and Dr, the rank is indicated by the
value r. In Example 1, we considered sl3(C), which has rank 2, and we conﬁrmed
that m(˜α, 0) = 2. Using techniques similar to those presented in Example 1, we
provided a combinatorial proof of the above mentioned result for the Lie algebra
slr+1(C) [15].
Theorem 1. Let r ≥2. If ˜α is the highest root of slr+1(C), then m(˜α, 0) = r.
In order to prove Theorem 1 we will need the following results, which ﬁrst
appeared in [13]. We summarize the ﬁndings below and provide proofs to results
that were previously omitted in [13].
Theorem 2. Let r ≥2 and let g = slr+1(C). Then σ ∈A (˜α, 0) if and only if
σ = 1 or σ = si1si2 · · · sik for some nonconsecutive integers i1, i2, . . . , ik between 2
and r −1.
The proof of Theorem 2 presented in [15] uses the fact that the Weyl group of
slr+1(C) is isomorphic to the group of permutations on r + 1 letters. A worthwhile
endeavor is to prove this result using the deﬁnition of the Weyl group as generated
by the reﬂections about hyperplanes perpendicular to the simple roots, as presented
in the current exposition.
Challenge Problem 3. Prove Theorem 2 using the deﬁnition of the elements of the
Weyl group as being generated by reﬂections s1, s2, . . . , sr.

Computing Weight Multiplicities
209
We can now state a connection between the adjoint representation of the Lie
algebra slr+1(C) and the Fibonacci numbers, which are deﬁned by the recurrence
relation
Fr = Fr−1 + Fr−2 for r ≥3, and F1 = F2 = 1.
Corollary 1. If r ≥2 and g = slr+1(C), then |A (˜α, 0)| = Fr, where Fr denotes
the rth Fibonacci number.
Proof. From Theorem 2 we know every element in A (˜α, 0) can be written as
si1si2 · · · sik where i1, i2, . . . ik are nonconsecutive integers between 2 and r −1.
Hence, the proof that |A (˜α, 0)| = Fr reduces to showing that there are Fr subsets
of {2, . . . , r −1} that contain only nonconsecutive integers.
Let Ar(˜α, 0) denote the set A (˜α, 0) in slr+1(C). We proceed by induction. If
r = 2, then the empty set is the only subset of ∅consisting of nonconsecutive
integers. So |A2(˜α, 0)| = 1 = F2. If r = 3, then there exist two subsets of {2}
consisting of nonconsecutive integers: ∅and {2}. Hence, |A3(˜α, 0)| = 2 = F3. If
r = 4, then there are three subsets of {2, 3} consisting of nonconsecutive integers:
∅, {2}, and {3}. Hence, |A4(˜α, 0)| = 3 = F4.
Assume that for 3 ≤k < r, |Ak(˜α, 0)| = Fk. Let us count the total number of
subsets of {2, 3, . . . , r −1} consisting of nonconsecutive integers. First note that
all of these subsets of {2, 3, . . . , r −1} will either contain r −1 or not. If they
do not contain r −1, then we are counting the subsets of {2, 3, . . . , r −2} which
consist of nonconsecutive integers. By our induction hypothesis, this is given by
Fr−1. If, on the other hand, r −1 is included in the subsets of {2, 3, . . . , r −1}
consisting of nonconsecutive integers, then we must count the number of subsets of
{2, 3, . . . , r −3} as r −3 would be the largest integer in that set which we could
include in our subset of nonconsecutive integers that already contain r −1. By our
induction hypothesis, this is given by Fr−2. As any given subset of nonconsecutive
integers either includes r −1 or not, and it cannot do both, we have shown that the
total number of subsets of {2, 3, . . . , r −1} consisting of nonconsecutive integers is
given by Fr−2 + Fr−1 = Fr−1. Thus |Ar(˜α, 0)| = Fr.
By having an explicit description of the elements of the Weyl alternation set
A (˜α, 0) along with a simpliﬁcation of σ(˜α + ρ) −ρ for each σ ∈A (˜α, 0) we
provide a closed formula for ℘(σ(˜α + ρ) −ρ). This is our next result.
Proposition 1. Let r ≥2. If ˜α is the highest root of slr+1(C) and σ ∈A (˜α, 0),
then ℘(σ(˜α + ρ) −ρ) = 2r−1−2ℓ(σ).
To prove Proposition 1 we need the following technical lemmas.
Lemma 1. Let r ≥2. If σ = si1si2 · · · sik with i1, i2, . . . , ik nonconsecutive integers
between 2 and r −1, then σ(˜α + ρ) −ρ = ˜α −k
j=1 αij.

210
P.E. Harris
Proof. Let σ = si1si2 · · · sik with i1, i2, . . . , ik nonconsecutive integers between 2
and r −1. Since sisj = sjsi whenever i and j are nonconsecutive, we can assume that
σ = si1si2 · · · sik where 1 < i1 < i2 < · · · < ik < r are nonconsecutive integers.
Since σ acts linearly, we know
σ(˜α + ρ) −ρ = σ(˜α) + σ(ρ) −ρ.
(27)
Again using the linearity of the action of the Weyl group elements and equa-
tions (20)–(23) we can observe that for any 1 ≤j ≤k
sij(˜α) = sij(α1 + α2 + · · · + αij−2 + αij−1 + αij + αij+1 + αij+2 + · · · + αr)
= sij(α1) + sij(α2) + · · · + sij(αij−2) + sij(αij−1) + sij(αij) + sij(αij+1)
+ sij(αij+2) + · · · + sij(αr)
= α1 + · · · + αij−2 + [αij−1 + αij] + [−αij] + [αij + αij+1] + αij+2 + · · · + αr
= α1 + · · · + αij−2 + αij−1 + αij + αij+1 + αij+2 + · · · + αr
= ˜α.
As σ = si1si2 · · · sik where 1 < i1 < i2 < · · · < ik < r are nonconsecutive integers,
we have shown that σ(˜α) = ˜α.
Recall ρ = ϖ1+ϖ2+· · ·+ϖr. By (26), if 1 ≤j ≤k, then sij(ϖℓ) = ϖℓ−δij,ℓαij,
where δij,ℓ= 1 if ij = ℓand 0 otherwise. Hence, for any 1 ≤j ≤k
sij(ρ) = sij(ϖ1 + ϖ2 + · · · + ϖr)
= sij(ϖ1) + sij(ϖ2) + · · · + sij(ϖij−1) + sij(ϖij) + sij(ϖij+1) + · · · + sij(ϖr)
= ϖ1 + ϖ2 + · · · + ϖij−1 + [ϖij −αij] + ϖij+1 + · · · + ϖr
= ρ −αij.
As σ = si1si2 · · · sik where 1 < i1 < i2 < · · · < ik < r are nonconsecutive integers,
we have shown σ(ρ) = ρ−k
j=1 αij. The result follows from substituting σ(˜α) = ˜α
and σ(ρ) = ρ −k
j=1 αij into equation (27).
Lemma 2. If r ≥1 and ˜α is the highest root of slr+1(C), then ℘(˜α) = 2r−1.
Proof. We proceed by induction on r. If r = 1, then Φ+ = {α1}, so ℘(α1) = 1 =
20. If r = 2, then Φ+ = {α1, α2, α1 + α2}, and ˜α = α1 + α2 can be written
using the simple roots α1 and α2, or we can use the positive root α1 + α2. Hence
℘(˜α) = 2 = 22−1.
Assume that ℘(˜α) = 2k−1 for any k ≤r. If k = r + 1, then let Φ+
r+1 be the set of
positive roots of slr+2(C) and Φ+
r be the set of positive roots of slr+1(C). Observe
that Φ+
r+1 \ Φ+
r = {αi + αi+1 + · · · + αr+1 : 1 ≤i ≤r + 1}. For 1 ≤i ≤r, let
ai = αi + · · · + αr+1 and let ar+1 = αr+1.

Computing Weight Multiplicities
211
To ﬁnd the number of ways to write ˜α = α1 + · · · + αr+1 as a nonnegative
integral sum of the positive roots in Φ+
r+1, we add the number of ways to write ˜α
when we use one of the positive roots in Φ+
r+1 \ Φ+
r . If we use a1, then we can
write ˜α = α1 + · · · + αr+1 in exactly 1 way. Observe that for 2 ≤i ≤r + 1,
if we use ai, then we need to count the number of ways we can write ˜α −ai =
α1 + α2 + · · · + αi−1, which by induction hypothesis, we can write in 2i−2 ways.
Therefore, the total number of ways to write ˜α as a sum of positive roots is given by
the number of ways we can write ˜α −ai (where 1 ≤i ≤r + 1) as a sum of positive
roots. Thus
℘(˜α) = 1 + 1 + 2 + · · · + 2r−3 + 2r−2 + 2r−1 = 1 +
r−1

i=0
2i = 1 + 1 −2r
1 −2 = 2r.
Lemma 3. Let r ≥2. If 1 ≤i < j ≤r, then ℘(αi + αi+1 + · · · + αj) = 2j−i.
Proof. We proceed by induction on r. If r = 2, then i = 1, j = 2 and by Lemma 2,
℘(α1 + α2) = 2 = 22−1. Assume that for any k ≤r, ℘(αi + αi+1 + · · · + αj) =
2j−i, whenever i, j ∈N satisfy 1 ≤i < j ≤k. Let k = r + 1, and let i, j be
integers such that 1 ≤i < j ≤r + 1. If j < r + 1, then by induction hypothesis
℘(αi + · · · + αj) = 2j−i. Now assume that j = r + 1. Let Φ+
r+1 be the set of
positive roots of slr+2(C) and Φ+
r be the set of positive roots of slr+1(C). Observe
that Φ+
r+1 \ Φ+
r = {αi + αi+1 + · · · + αr+1 : 1 ≤i ≤r + 1}. For 1 ≤i ≤r, let
ai = αi + · · · + αr+1 and let ar+1 = αr+1.
We want to know the number of ways to write αi + · · · + αr+1 as a nonnegative
integral sum of the positive roots in Φ+
r+1. We can compute this number by adding
the number of ways to write αi + · · · + αr+1 when we use one of the positive roots
in Φ+
r+1 \ Φ+
r . Notice that to write αi + · · · + αr+1 we will not use any am for
1 ≤m ≤i −1. If we use ai, then we can write αi + · · · + αr+1 in exactly 1 way.
Observe that for i + 1 ≤m ≤r + 1, if we use am, then, by induction hypothesis, we
can write αi + · · · + αr+1 in 2m−1−i ways.
Thus the total number of ways to write αi + · · · + αr + 1 is given by
℘(αi + · · · + αn+1) = 1 +
r−i

i=0
2i = 1 + 1 −2r+1−i
1 −2
= 2r+1−i.
We can now return to the proof of Proposition 1.
Proof (Proposition 1). Assume that r ≥1 and σ ∈A (˜α, 0), with k = ℓ(σ). We
proceed by induction on k. If σ ∈A (˜α, 0) and k = 0, then σ = 1. Hence, by
Lemma 2
℘(σ(˜α + ρ) −ρ) = ℘(˜α) = 2r−1.

212
P.E. Harris
If σ ∈A (˜α, 0) and k = 1, then σ = si for some 2 ≤i ≤r−1. Hence, σ(˜α+ρ)−ρ =
˜α −αi = α1 + · · · + αi−1 + αi+1 + · · · + αr. By Lemma 3,
℘(α1 + · · · + αi−1) = 2i−2
and
℘(αi+1 + · · · + αr) = 2r−i−1.
Since the subset of the positive roots used to write α1 + · · · + αi−1 is disjoint from
the subset of the positive roots used to write αi+1 + · · · + αn, we have
℘(α1 + · · · + αi−1 + αi+1 + · · · + αr) = ℘(α1 + · · · + αi−1) · ℘(αi+1 + · · · + αr)
= 2i−2 · 2r−i−1
= 2r−1−2(1).
Assume that for any σ ∈A (˜α, 0) with ℓ(σ) ≤k, ℘(σ(˜α+ρ)−ρ) = 2r−1−2ℓ(σ).
If σ ∈A (˜α, 0) with ℓ(σ) = k + 1, then σ = si1si2 · · · siksik+1 for some
nonconsecutive integers i1, . . . , ik+1 satisfying 2 ≤i1 < i2 < · · · < ik < ik+1 ≤
r −1. By Lemma 1,
σ(˜α + ρ) −ρ =
i1−1

j=1
αj +
i2−1

j=i1+1
αj +
i3−1

j=i2+1
αj + · · · +
ik+1−1

j=ik+1
αj +
r

j=ik+1+1
αj.
(28)
We need to determine the number of ways to write equation (28) as a nonnegative
integral sum of positive roots. By Lemma 3, we know
℘
⎛
⎝
i1−1

j=1
αj
⎞
⎠= 2i1−2,
℘
⎛
⎝
i2−1

j=i1+1
αj
⎞
⎠= 2(i2−1)−(i1+1) = 2i2−i1−2,
℘
⎛
⎝
i3−1

j=i2+1
αj
⎞
⎠= 2(i3−1)−(i2+1) = 2i3−i2−2,
...
℘
⎛
⎝
ik+1−1

j=ik+1
αj
⎞
⎠= 2(ik+1−1)−(ik+1) = 2ik+1−ik−2,
℘
⎛
⎝
r

j=ik+1+1
αj
⎞
⎠= 2r−(ik+1+1) = 2r−ik+1−1.

Computing Weight Multiplicities
213
Since the subsets of the positive roots of slr+1(C) used to write each of the above
sums are pairwise disjoint we have that
℘(σ(˜α + ρ) −ρ) = ℘
⎛
⎝
i1−1

j=1
αj +
i2−1

j=i1+1
αj +
i3−1

j=i2+1
αj + · · · +
ik+1−1

j=ik+1
αj +
r

j=ik+1+1
αj
⎞
⎠
= ℘
	i1−1

j=1
αj

℘
	 i2−1

j=i1+1
αj

· · · ℘
⎛
⎝
ik+1−1

j=ik+1
αj
⎞
⎠℘
⎛
⎝
r

j=ik+1+1
αj
⎞
⎠
= 2i1−2 · 2i2−i1−2 · · · 2ik+1−ik−2 · 2r−ik+1−1
= 2r−1−2(k+1).
Lemma 4. Let r ≥1 and let ˜α denote the highest root of slr+1. Then the cardinality
of the set {σ ∈A (˜α, 0) | ℓ(σ) = k} is
	r−1−k
k

and max{ ℓ(σ) | σ ∈A (˜α, 0)} =
⌊r−1
2 ⌋.
Proof. The proof that |{σ ∈A (˜α, 0)}| =
	r−1−k
k

follows from showing that the
number of ways to select k nonconsecutive integers from the set {2, 3, . . . , r −1}
is given by
	r−1−k
k

. We leave this proof to the reader. Now notice that if r is
odd, then we can choose at most
r−1
2
many nonconsecutive integers from the
set {2, 3, 4, . . . , r −1}, namely the even numbers. If r is even, then r −1 is
odd and we can choose at most r−2
2
many nonconsecutive integers from the set
{2, 3, 4, . . . , r −1}, either all the even or all the odd numbers. Observe that when
r is odd, r−1
2
= ⌊r−1
2 ⌋and when r is even, r−2
2
= ⌊r−1
2 ⌋. Thus max{ℓ(σ : σ ∈
A (˜α, 0))} = ⌊r−1
2 ⌋.
We can now prove that m(˜α, 0) = r.
Proof (Theorem 1). By Theorem 2 we can reduce
m(˜α, 0) =

σ∈W
(−1)ℓ(σ)℘(σ(˜α + ρ) −ρ)
=

σ∈A (˜α,0)
(−1)ℓ(σ)℘(σ(˜α + ρ) −ρ)
(29)
where σ ∈A (˜α, 0) if and only if σ = si1si2 · · · sik for some nonconsecutive integers
satisfying 2 ≤i1 < i2 < · · · < ik ≤r −1. Now by Proposition 1 we know that for
any σ ∈A (˜α, 0), ℘(σ(˜α + ρ) −ρ) = 2r−1−2ℓ(σ). Let k = ℓ(σ), then by Lemma 4
we know that min{k : σ ∈A (˜α, 0)} = 0, max{k : σ ∈A (˜α, 0)} =
< r−1
2
=
, and
there are
	r−1−k
k

elements in A (˜α, 0) of length k. Hence, (29) reduces to
m(˜α, 0) =
⌊r−1
2 ⌋

k=0
(−1)k2r−1−2k.
(30)

214
P.E. Harris
A proof by induction veriﬁes that
⌊r−1
2 ⌋

k=0
(−1)k2r−1−2k = r.
Having developed some experience computing Weyl alternation sets and creating
formulas for the value of Kostant’s partition function for special sets of weights, we
now focus on a more geometric style of problem. In Example 1 we focused our
attention on a problem where we ﬁxed both the weights λ and μ, but this is only
necessary if we are actually concerned with computing weight multiplicities. For
the sake of discovery, let us investigate what occurs to the Weyl alternation sets
A (λ, μ) when we ﬁx μ = 0 and let λ vary among the set of integral weights of
sl3(C).
Example 2. Compute the sets A (λ, 0) for all integral weights λ of sl3(C).
Solution 2. By (6), λ is an integral weight if λ = c1ϖ1+c2ϖ2 for some c1, c2 ∈Z.
There is a nice relationship between the simple roots α1 and α2 and the fundamental
weights ϖ1 and ϖ2,
ϖ1 = 2
3α1 + 1
3α2
(31)
ϖ2 = 1
3α1 + 2
3α2.
(32)
Figure 3 provides a visualization of the fundamental weights and the simple
roots on a 2-dimensional plane. The shaded region represents the dominant Weyl
chamber, and the weights lying on the lattice ϖ1N + ϖ2N index the irreducible
ﬁnite-dimensional representations of the Lie algebra sl3(C). This was the statement
of the theorem of the highest weight.
Fig. 3 Fundamental weights
in terms of simple roots.
a1
a2
v2
v1
r

Computing Weight Multiplicities
215
Since we want to describe the sets A (c1ϖ1 + c2ϖ2, 0) for all c1, c2 ∈Z, we
ﬁrst have to compute σ(c1ϖ1 + c2ϖ2 + ρ) −ρ for all σ ∈W. To do this, ﬁrst write
the fundamental weights and ρ in terms of the simple roots, then use Table 1 to
determine σ(c1ϖ1 + c2ϖ2 + ρ) −ρ for all σ ∈W. Writing the result in terms of the
simple roots, as we will need this information shortly, the reader should complete:
Exercise 3. Verify the following computations
1(c1ϖ1 + c2ϖ2 + ρ) −ρ =
2c1 + c2
3

α1 +
c1 + 2c2
3

α2
(33)
s1(c1ϖ1 + c2ϖ2 + ρ) −ρ =
−c1 + c2 −3
3

α1 +
c1 + 2c2
3

α2
(34)
s2(c1ϖ1 + c2ϖ2 + ρ) −ρ =
2c1 + c2
3

α1 +
c1 −c2 −3
3

α2
(35)
s1s2(c1ϖ1 + c2ϖ2 + ρ) −ρ =
−c1 −2c2 −6
3

α1 +
c1 −c2 −3
3

α2
(36)
s2s1(c1ϖ1 + c2ϖ2 + ρ) −ρ =
−c1 + c2 −3
3

α1 +
−2c1 −c2 −6
3

α2
(37)
s1s2s1(c1ϖ1 + c2ϖ2 + ρ) −ρ =
−c1 −2c2 −6
3

α1 +
−2c1 −c2 −6
3

α2.
(38)
As we noted in the solution to Example 1, in order to determine when an element
of the Weyl group is in the Weyl alternation set A (c1ϖ1 + c2ϖ2, 0) we must ﬁnd
when both of the coefﬁcients of α1 and α2 in the above equations are nonnegative
integers. For example, considering (33), the identity element will be in the Weyl
alternation set A (c1ϖ1 + c2ϖ2, 0) if and only if
2c1 + c2
3
and
c1 + 2c2
3
are nonnegative integers. We expand these conditions as follows:
Condition 1: 3|2c1 + c2
Condition 2: 3|c1 + 2c2
Condition 3: 2c1 + c2 ≥0 and
Condition 4: c1 + 2c2 ≥0.

216
P.E. Harris
v2-axis
v1-axis
v1-axis
v1-axis
v2-axis
v2-axis
(a) Condition 1 and 2
(b) Condition 3
(c) Condition 4
Fig. 4 Illustrating Conditions 1-4, so that 1 ∈A (c1ϖ1 + c2ϖ2, 0).
Observe that Condition 1 and Condition 2 are number theoretic conditions. We can
simplify them by observing that since 3|2c1 + c2 and 3|c1 + 2c2, then 3|(2c1 +
c2) −(c1 + 2c2), hence 3|c1 −c2. This implies that c1 −c2 = 3x for some integer
x and so c1 = 3x + c2. Letting c2 = y, we note that the only way Condition 1
and Condition 2 can be satisﬁed is if λ = c1ϖ1 + c2ϖ2 was actually λ = (3x +
y)ϖ1 +yϖ2 for some x, y ∈Z. The points that satisfy this condition are provided in
Figure 4a, where the lattice is given by Zϖ1 + Zϖ2. Now note that Condition 3 and
Condition 4 are linear inequalities whose solutions, as is standard, can be depicted
by shading the appropriate region of the plane in Figure 3. Figures 4b and 4c provide
the appropriately shaded regions. Since all four conditions must be satisﬁed, we ﬁnd
that the identity element will be in A (c1ϖ1 + c2ϖ2, 0) if and only if c1ϖ1 + c2ϖ2
is one of the integral weights we have highlighted by placing a black circle on the
weights location within the plane in Figure 5a.
Repeating this process for each σ ∈W, we determine the weights c1ϖ1 + c2ϖ2,
with c1, c2 ∈Z, for which σ ∈A (c1ϖ1 + c2ϖ2, 0). This is depicted in Figure 5,
and each subﬁgure presented is found in an analogous way as that presented above
for Figure 5a. Moreover, each subﬁgure shows which weights c1ϖ1 + c2ϖ2 would
have a given Weyl group element in their Weyl alternation set A (c1ϖ1 + c2ϖ2, 0).
We leave the veriﬁcation of these computations to the reader.
From Figure 5 one can observe that there are weights that have no elements in
their Weyl alternation set as they are never highlighted in any of the subﬁgures, i.e.
they have no black circles placed on top of the weights location within the plane.
Take for example the dominant integral weight 2ϖ1. This weight is not highlighted
in any of the subﬁgures of Figure 5. This implies that A (2ϖ1, 0) = ∅and hence
m(2ϖ1, 0) = 0.
Figure 5 also shows that there are weights that have a few elements of the Weyl
group in their Weyl alternation set. Those are weights that are highlighted in more
than one of the subﬁgures. Take for example the dominant integral weight 3ϖ1.
This weight is highlighted in Figures 5a and 5c, and not highlighted in the rest. This
implies that A (3ϖ2, 0) = {1, s2}.

Computing Weight Multiplicities
217
v2-axis
v2-axis
v2-axis
v2-axis
v2-axis
v2-axis
v1-axis
v1-axis
v1-axis
v1-axis
v1-axis
v1-axis
(a) Weights with 1 ∈A (l ,0)
(b) Weights with s1 ∈A (l ,0)
(c) Weights with s2 ∈A (l , 0)
(d) Weights with s1s2 ∈A (l ,0)
(e) Weights with s2s1 ∈A (l ,0)
(f) Weights with s1s2s1 ∈A (l ,0)
Fig. 5 Weights for which σ ∈A (λ, 0) where λ = c1ϖ1 + c2ϖ2 and c1, c2 ∈Z.

218
P.E. Harris
These observations motivate the need for a single diagram which provides all
of the information regarding the Weyl alternation sets A (c1ϖ1 + c2ϖ2, 0), where
c1, c2 ∈Z. This is exactly the deﬁnition of the
zero weight Weyl alternation
diagram. This diagram places a circle of the same color on the integral weights
λ and β whenever A (λ, 0) = A (β, 0) ̸= ∅. That is weights with the same Weyl
alternation sets get colored the same way, and weights whose Weyl alternation set
is the empty set do not get colored at all. Following this construction, the zero
weight Weyl alternation diagram for the Lie algebra sl3(C) is found in Figure 6.
This diagram allows one to quickly reduce a weight’s multiplicity computation as
it explicitly lists the elements of the Weyl group that provide nonzero terms to the
sum in the multiplicity formula.
We remark that the computation of weight multiplicities only make sense in a Lie
theoretic way when λ is a dominant integral weight. That is, when λ = c1ϖ1+c2ϖ2
for some c1, c2 ∈N. Whenever λ is a dominant integral weight, it represents a ﬁnite-
dimensional irreducible representation of the Lie algebra sl3(C). However, other
integral weights can be viewed as “virtual” representations, where we focus solely
on the computation and not on representation theory. Moreover, by considering the
Weyl alternation sets of all integral weights, we discovered the beautiful symmetry
presented in Figure 6. A symmetry we would have missed had we restricted the
computation of A (˜α, 0) for only dominant integral weights λ of sl3(C).
v1-axis
v2-axis
8 = {1}
8 = {1,s1}
8 = {s1}
8 = {s1,s2s1}
8 = {s2s1}
8 = {s2s1,s1s2s1}
8 = {s1s2s1}
8 = {s1s2,s1s2s1}
8 = {s1s2}
8 = {s2,s1s2}
8 = {s2}
8 = {1,s2}
Fig. 6 Weyl alternation diagram for the sets A (c1ϖ1 + c2ϖ2, 0), where c1, c2 ∈Z

Computing Weight Multiplicities
219
4
Open Problems
Having worked through the detailed examples and proofs provided in this paper,
students can now focus on the computation of weight multiplicities to study the
representation theory of Lie algebras. More concretely, students can use techniques
from combinatorics to:
1. Provide descriptions of Weyl alternation sets A (λ, μ) for integral weights λ and
μ in the Lie algebra slr+1(C);
2. Compute Weyl alternation diagrams for some low rank examples; and
3. Find closed formulas for the value of Kostant’s partition function, as well as
Kostant’s weight multiplicity formula for speciﬁc weights λ and μ.
Before stating the ﬁrst (concrete) open problem of this section, we begin by
sharing the work of Brett Harder, an undergraduate at Moravian College, whose
honors thesis focused on exploring the intricacies involved in computing weight
multiplicities for sl4(C) [11]. In his thesis, Harder formulated and proved results on
speciﬁc families of Weyl alternation sets including:
Theorem 3 (Theorem 5.1 [11]). If ˜α the highest root of sl4(C), then A (n˜α, 0) =
{1, s2} for all n ∈N.
Theorem 4 (Theorem 6.4 [11]). If ˜α the highest root of sl4(C), then
A (˜α, nαi) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
{1, s2}
if n = 1 and i = 1, 3
{1}
if n = 1 and i = 2
∅
if n > 1 and 1 ≤i ≤3.
Theorem 5 (Theorem 7.6
[11]). For the fundamental weights ϖ1, ϖ2, ϖ3 of
sl4(C),
1. A (nϖ1 + nϖ2 + nϖ3, 0) = {1, s1, s2, s3, s1s3} for all even n ∈N;
2. A (nϖ1 + nϖ2 + nϖ3, 0) = {s1s2s3s2s1, s2s3s1s2s1, s1s2s3s1s2, s1s2s3s2s1s2}
for all even integers n < −2; and
3. A (nϖ1 + nϖ2 + nϖ3, 0) = ∅for n = −2 and for all odd n ∈Z.
With the above results at hand and some supporting computational evidence,
Harder conjectured the following.
Conjecture 1 (Conjecture 5.2 [11]). If ˜α is the highest root of sl4(C), then
m(n˜α, 0) = (n + 1)(n + 2)
2
for all
n ∈N.

220
P.E. Harris
Conjecture 2 (Conjecture 6.5 [11]). If ˜α is the highest root of sl4(C), then for
1 ≤i ≤4,
m(˜α, nαi) =

1
if n = 1
0
for n > 1.
Conjecture 3 (Conjectures 7.3 and 7.5 [11]). For even n ∈N,
m(nϖ1 + nϖ2 + nϖ3, 0) =
1
2n + 1
4
−
1
2n
4
and for even n < −2,
m(nϖ1 + nϖ2 + nϖ3, 0) =
     
1
2n + 1
4
−
1
2n
4     .
A starting point for a student who desires to contribute to this body of knowledge is:
Research Project 1. Prove or provide counterexamples to Conjectures 1–3.
For those students more interested in computing Weyl alternation diagrams or
those with some programming background, we suggest:
Research Project 2. Consider the Lie algebra sl4(C). Fix μ = 0 and
compute all Weyl alternation sets A (λ, 0) where λ = c1ϖ1 + c2ϖ2 + c3ϖ3
and c1, c2, c3 ∈Z. From these sets create the zero weight Weyl alternation
diagram for sl4(C).
We note that the above mentioned Weyl alternation diagram will be a three
dimensional plot and the use of software is highly encouraged.
Research Project 3. Consider the Lie algebra sl3(C). Create a program
whose input is a pair of weights λ, μ and whose output is the associated μ
weight Weyl alternation diagram.

Computing Weight Multiplicities
221
Consider the Lie algebra slr+1(C). As we saw in Corollary 1, the cardinality
of A (˜α, 0) is Fr, where Fr denotes the rth Fibonacci number. This motivates the
following.
Research Project 4. Determine
all
pairs
of
weights
λ, μ
for
which
A (λ, μ) = Fr as the rank of the Lie algebra slr+1(C) increases.
Whether this is even possible for any other pair of weights aside from the highest
root ˜α and the zero weight, is currently unknown.
Research Project 5. Berenshtine and Zelevinskii provide a list of pairs of
weights λ, μ for which m(λ, μ) = 1, see [3]. An open problem is to describe
the sets A (λ, μ) for the pairs of weights λ and μ in these lists.
These are only a few of the open problem in this ﬁeld of study. The present work
focused on the Lie algebra slr+1(C), however, all of the work and questions we
have investigated can be extended to the other simple Lie algebras. This opens the
door for numerous other variations of these problems and extends the investigation
further than is currently presented.
Acknowledgements The author thanks the reviewer for their insightful comments and sugges-
tions, which greatly improved the exposition of this manuscript. The author also thanks Kevin
Chang, Edward Lauber, Haley Lescinsky, Grace Mabie, Gabriel Ngwe, Cielo Perez, Aesha
Siddiqui, and Anthony Simpson for reading and providing feedback on an initial draft of this
manuscript.
References
1. Baldoni, W., Vergne, M.: Kostant partitions functions and ﬂow polytopes. Transform. Groups
13(3–4), 447–469 (2008)
2. Baldoni, W., Beck, M., Cochet, C., Vergne, M.: Volume computation for polytopes and
partition functions for classical root systems. Discret. Comput. Geom. 35, 551–595 (2006).
Programs available at www.math.polytechnique.fr/cmat/vergne
3. Berenshtein, A.D., Zelevinskii, A.V.: When is the multiplicity of a weight equal to 1? Funct.
Anal. Appl. 24, 259–269 (1991)
4. Billey, S., Guillemin, V., Rassart, E.: A vector partition function for the multiplicities of slkC.
J. Algebra 278(1), 251–293 (2004)
5. Björner, A., Brenti, F.: Combinatorics of Coxeter groups. Springer, Berlin (2005)
6. Cartan, E.: Sur la structure des groupes de transformations ﬁnis et continus. Thèse, Paris, Nony
(1894) [Ouvres Completes, Partie I. 1, 137–287]

222
P.E. Harris
7. Carter, R.W.: Finite Groups of Lie Type: Conjugacy Classes and Complex Characters. Wiley
Classics Library. Wiley, Chichester (1993)
8. Cochet, C.: Vector partition function and representation theory. In: Conference Proceedings
Formal Power Series and Algebraic Combinatorics, Taormina, Sicile (2005), 12 pp.
9. Fulton, W., Harris, J.: Representation Theory - A First Course. Graduate Texts in Mathematics,
vol. 129. Springer, Berlin (2004)
10. Goodman, R., Wallach, N.R.: Symmetry, Representations and Invariants. Springer, New York
(2009)
11. Harder, B.: An exploration of lie algebras and Kostant’s weight multiplicity formula. Moravian
College, Undergraduate Honors Thesis (2016)
12. Harish-Chandra: On some applications of the universal enveloping algebra of a semi-simple
lie algebra. Trans. Am. Math. Soc. 70, 28–96 (1951)
13. Harris, P.E.: On the adjoint representation of sln and the Fibonacci numbers. C. R. Math. Acad.
Sci. Paris 935–937 (2011). arXiv:1106.1408
14. Harris, P.E.: Kostant’s weight multiplicity formula and the Fibonacci numbers. http://arxiv.org/
pdf/1111.6648v1.pdf
15. Harris, P.E.: Combinatorial problems related to Kostant’s weight multiplicity formula. Doctoral
dissertation. University of Wisconsin-Milwaukee, Milwaukee, WI (2012)
16. Harris, P.E., Insko, E., Omar, M.: The q-analog of Kostant’s partition function and the highest
root of the classical Lie algebras. Preprint http://arxiv.org/pdf/1508.07934 (Submitted)
17. Harris, P.E., Insko, E., Williams, L.K.: The adjoint representation of a classical Lie algebra and
the support of Kostant’s weight multiplicity formula . J. Comb. 7(1), 75–116 (2016)
18. Humphreys, J.E.: Introduction to Lie Algebras and Representation Theory. Springer, New York
(1978)
19. Humphreys, J.E.: Reﬂection Groups and Coxeter Groups. Cambridge University Press,
Cambridge (1990)
20. Knapp, A.W.: Lie Groups Beyond an Introduction. Birkhäuser Boston Inc. Boston, MA (2002)
21. Kostant, B.: A formula for the multiplicity of a weight. Proc. Natl. Acad. Sci, U. S. A. 44,
588–589 (1958)
22. Núñez, J.F., Fuertes, W.G., Perelomov, A.M.: Generating functions and multiplicity formulas:
the case of rank two simple Lie algebras. Preprint: http://arxiv.org/pdf/1506.07815v1.pdf
23. Varadarajan, V.S.: Lie Groups, Lie Algebras, and Their Representations. Springer, Berlin
(1984)
24. Weyl, H.: Theorie der Darstellung kontinuierlicher halbeinfacher Gruppen durch lineare Trans-
formationen, I, II, III, Nachtrag, Mathematische Zeitschrift 23 (1925), 24 271–309/328–
376/377–395/789–791 (1926)

Vaccination Strategies for Small Worlds
Winfried Just and Hannah Callender Highlander
Suggested Prerequisites. Knowledge of basic notions of probability theory and
prior exposure to theorem proving are essential. Some acquaintance with graph
theory, mathematical epidemiology, as well as experience with mathematical mod-
eling and simulations will be helpful, but are not required. Two of the ﬁve suggested
research projects involve coding in the NETLOGO language.
1
Introduction
The goal of this chapter is to empower students to work on the research projects
described in Section 6. Each of these projects concerns optimal vaccination
strategies. In plain language this means that we are asking, in a certain mathemat-
ically well-deﬁned setting, how one should choose individuals who will receive
vaccination that is in short supply so as to achieve maximal overall protection
against a disease outbreak for the entire population. Thus the questions of our
projects are closely related to important issues in public health, fairly intuitive,
and natural. Nevertheless, to the best of our knowledge, none of these projects
has been covered to a large extent by existing work in the literature. Each of
our proposed projects is open-ended and has a broad scope that leaves room for
genuinely novel discoveries by aspiring undergraduate researchers, with relatively
basic mathematical background on the one hand, and also for deep mathematical
theorems on the other hand.
W. Just ()
Department of Mathematics, Ohio University, Athens, OH 45701, USA
e-mail: mathjust@gmail.com
H.C. Highlander
Department of Mathematics, University of Portland, MSC 60, Portland, OR 97203, USA
e-mail: highland@up.edu
© Springer International Publishing AG 2017
A. Wootton et al. (eds.), A Primer for Undergraduate Research, Foundations for
Undergraduate Research in Mathematics, DOI 10.1007/978-3-319-66065-3_10
223

224
W. Just and H.C. Highlander
While none of our projects require highly specialized mathematical knowledge
at a very high level of abstraction, they do require some familiarity with a number of
concepts and techniques. Some background knowledge of mathematical epidemiol-
ogy and graph theory will be needed, but is not assumed here. In Section 1.1 we
give a bird’s-eye overview of these topics, while in Section 1.2 we give pointers
to sources where students can study this material in greater depth. Sections 2–5
introduce four crucial notions from graph theory that are essential for our projects:
Erd˝os-Rényi random graphs, clustering coefﬁcients, the small-world property, and
small-world networks. References to more in-depth readings about these concepts
will be given throughout these sections. Most of the material in Sections 1–5 has
been adapted from the modules that were developed in cooperation with Drew
LaMar and Ying Xin and are posted at QUBEShub.org [14]. For ease of browsing,
a complete list of the modules are also posted on the ﬁrst author’s website [12]. The
excerpts are chosen in such a way that they directly build up towards the research
projects. In Section 6 we describe the projects themselves.
Preparation for research requires not only absorbing a number of facts, but
also mastering certain skills. Our presentation includes numerous exercises that
we recommend be treated as an integral part of the text and attempted right away.
On the one hand, they are designed to deepen the understanding of the concepts
covered in the text; on the other hand, they will develop the same skills that are
needed for success with the research projects. Among these skills is familiarity
with using simulations for exploring predictions of models. In order to keep the
exposition focused on the concepts, we did not include a separate “how-to” section
on simulation studies in general and using our recommended software tool IONTW
in particular. Instead, we rely on students consulting the relevant references to
such material, and we provide ample opportunity to practice these skills in our
exercises and three challenge problems. Hints for selected exercises are provided
in the appendix. Solutions to the exercises that were adapted from the modules can
be provided upon request by research advisors at [14].
1.1
Modeling Infectious Diseases and Contact Networks
Epidemiology studies the spread of diseases caused by pathogens, such as viruses
or bacteria, in populations of hosts, which can be humans, animals, or plants. We
focus here on diseases that are transmitted by direct contact between two hosts1. The
goal is to predict the time course of an outbreak of a given disease in a population
and the effect of conceivable control measures, such as vaccination or behavior
modiﬁcation, on the severity of the outbreak.
Mathematical models can help answer these questions. Based on implementation
of such models in computer code, epidemiologists can simulate hypothetical out-
1This leaves out vector-born infectious diseases such as malaria and diseases such as cholera that
are transmitted through shared environmental resources.

Vaccination Strategies for Small Worlds
225
breaks under various assumptions about control that might possibly be implemented
and derive predictions about their likely effectiveness. For our simulations we will
use the customized software tool IONTW built upon the NETLOGO agent-based
modeling platform [28]. The IONTW code was developed by Drew LaMar in
consultation with us.
It is important to remember that mathematical models are greatly simpliﬁed
representations of reality. Thus when making inferences to the spread of infections
in populations of actual hosts, one needs to carefully examine how the assumptions
of the models inﬂuence their predictions. IONTW allows us to study how the
simplifying assumptions about the contact pattern that are embodied in the contact
network inﬂuence the outcomes of simulations. It also can be used for exploring the
sensitivity of these outcomes with respect to the disease transmission parameters of
the model.
A standing assumption of our modeling is that we investigate the spread of one
given infectious disease within a ﬁxed population of hosts that are numbered from 1
to N or from 0 to N −1. Thus we ignore demographics (births, deaths from causes
that are unrelated to the disease, immigration, and emigration). At any given time t,
host i can be in one of the following states:
•
S: Susceptible to infection.
•
I: Infectious, that is, able to infect other hosts through direct contact.
•
R: Removed, that is, not infectious and immune to subsequent infection (through
recovery or death from the disease, or through vaccination).
The sets of hosts that are in states S, I, R will be referred to as the S-compartment,
the I-compartment, and the R-compartment, respectively. Membership in the com-
partments changes over time, and one can conceptualize the time course of an
outbreak as movement of hosts between compartments.
Here we mostly limit ourselves to the study of SIR-models that are suitable
for immunizing infections where recovery from the disease confers permanent
immunity. Depending on the speciﬁcs of the disease under study, models of other
types, such as SIS, may be more appropriate; or a model may need to include
an Exposed state, where a host has been exposed to the pathogen but is not yet
infectious.
Figure 1 schematically depicts models of type SIR. Hosts move from the S- to
I-compartment upon effective contact. An effective contact between hosts i and j is
a contact that would result in infection of host i if host i were susceptible and host j
infectious at the time of the contact. We assume that for any given pair (i, j) of hosts
the probability of at least one effective contact over a time interval of length Δt
remains ﬁxed, and hosts make contacts independently. In discrete-time models this
probability is denoted by bi,j, and Δt is taken as the physical time that corresponds
to one time step in the model. In continuous-time models we assume that hosts i
and j make effective contact at a ﬁxed rate βi,j.
Similarly, in discrete-time models we assume that a host who is infectious at
time t will cease to be infectious at time t + 1 (Δt units of physical time later) with
probability a, which is ﬁxed and the same for all hosts. In continuous-time models
we assume that hosts will leave the I-compartment at a ﬁxed rate α.

226
W. Just and H.C. Highlander
Fig. 1 Schematic
representation of SIR-models.
When a = 0 or α = 0, then infectious hosts never recover, and we obtain
models of type SI. Such models are useful in some explorations of network structure.
Discrete-time models with a = 1 assume that hosts stay infectious for exactly one
time step. Since in these models there is a strict correspondence between the time
course of an outbreak and the so-called generations of the infection, we call them
next-generation models.
This short background review barely scratches the surface of disease modeling.
For more detailed information on how individuals transition from one state to
another, including a discussion on the differences between discrete-time and
continuous-time models, see our online review Network-based models of transmis-
sion of infectious diseases: a brief overview [15]. For readers who prefer a more
slow-paced and much more detailed development of this material we recommend
our book chapters [16,18].
1.1.1
Compartment-Level Models
Many mathematical models of disease transmission are based on the assumptions
of homogeneity of hosts and uniform mixing. In our terminology, the ﬁrst of these
assumptions boils down to assuming that the parameters a, α that govern the
transition from the I-compartment into the R-compartment are identical for each
host. We make this assumption in all our models. In contrast, the uniform mixing
assumption boils down to assuming that the parameters bi,j or βi,j that give the
probabilities or rates of making effective contact are identical for each pair (i, j)
of hosts with i ̸= j.
When both assumptions of homogeneity of hosts and uniform mixing are made,
hosts lose all individual characteristics, and the state of the model at time t can
be conceptualized as the vector of numbers of hosts in the compartments2. The
distribution of future states of the model is then entirely determined by the current
state. Models that operate entirely on the level of these counts or their expected
values are usually called compartment-based models, but we prefer the phrase
compartment-level models.
1.1.2
The Basic Reproductive Number
Typically we will consider outbreaks that start at time 0 with exactly one infectious
host j∗(the index case). When all other hosts are susceptible in the initial state, we
speak of introduction of one index case into an otherwise susceptible population.
2In our modules these numbers are denoted by (|S(t)|, |I(t)|, |R(t)|).

Vaccination Strategies for Small Worlds
227
Consider such an initial state. The number of secondary infections caused by
the index case j∗is a random variable (r.v.). Its mean value is the most important
parameter in disease modeling. It has a special name, provided in the deﬁnition
below.
Deﬁnition 1. The basic reproductive ratio or basic reproductive number R0 is the
mean number of secondary infections caused by an average index case in a large
and entirely susceptible population.
Now consider an outbreak that starts with introduction of an index case j∗into an
otherwise susceptible population. In an SIR-model, the ﬁnal size F of the outbreak,
that is, the proportion of hosts who experience infection, must be the proportion
of hosts that reside in the R-compartment when the outbreak is over. Note that F is
also a r.v. In compartment-level models, the expected value, and, more generally, the
distribution of F, cannot depend on the particular host j∗that started the outbreak,
since all hosts are assumed to have identical properties. Moreover, it is almost
entirely determined by R0 in the following sense: For very large population sizes N,
compartment-level models predict that with probability very close to 1:
•
When R0 ≤1, then F ≈0.
•
When R0 > 1, there exist 0 < r(∞), z∞< 1 that depend only on R0 such that:
– F ≈0 with probability ≈z∞.
– F ≈r(∞) with probability ≈1 −z∞.
In the former case we speak of a minor outbreak that affects only a negligible
fraction of hosts and in the latter case of a major outbreak that affects a signiﬁcant
fraction of hosts. If you run a version of Exercise 22(a) in Section 5.4 without
“vaccinating” any hosts and by choosing infection-prob := 0.015 instead of the
recommended value 0.03 for that exercise, you will get a nice illustration of this
distinction.
1.1.3
Models of Contact Networks: Graphs
In this chapter, instead of restricting ourselves to the assumptions of compartment-
level models, we study network-based models, where the infection can only be
spread if individuals are connected to one another in a given contact network.
Contact networks can be modeled by mathematical structures called graphs, and
we will treat the words “graph” and “network” as synonyms here, although in other
sources of the literature “networks” comprise a broader class of structures (see, for
example, our discussion in Section 9.3.1 in [16]). The terminology used in graph
theory is not as consistent as in other areas of mathematics, so let us brieﬂy review
the concepts that we will need here.
Technically, a graph G is a pair G = (V(G), E(G)). The elements of V(G)
are called nodes or vertices; we will use these words interchangeably. We will
always assume here that V(G) is a set of N nonnegative integers. For mathematical
explorations it is usually most convenient to take V(G)
=
{1, . . . , N}, but
NETLOGO will number the nodes from 0 to N −1. The set E(G) contains unordered

228
W. Just and H.C. Highlander
Fig. 2 The graph G1.
pairs of nodes that represent the edges of G. Figure 2 shows an example of a graph
that we call G1.
Here V(G1) = {1, . . . , 12}, and, for example, {1, 2} ∈E(G1) while {1, 3} /∈
E(G1). We say that nodes 1 and 2 are adjacent (in G1), while nodes 1 and 3 are not
adjacent. The total number of nodes j that are adjacent to node i is called the degree
of i and will be denoted by ki. In G1, each node has degree ki = 2. Graphs in which
all nodes have the same degree k are called k-regular graphs. Thus G1 is a 2-regular
graph.
The mean degree will be denoted by ⟨k⟩. For a network of size N it is equal to
⟨k⟩= 1
N
N

i=1
ki = k1 + k2 + · · · + kN
N
= 2|E(G)|
N
,
(1)
where |A| denotes the size of a ﬁnite set A.
If we remove part of the vertex set of a graph G and retain only V−⊂V(G)
together with all edges {i, j} ⊂V−that are in E(G), then we obtain a structure that
is called the subgraph of G induced by V−.
In Figure 2 two such structures stand out, and they are examples of important
classes of graphs that will be extensively considered in later sections.
Consider the subgraph of G1 that is induced by V−= {11, 12, 13}. It looks like
a triangle and illustrates why mathematicians speak of “vertices” and “edges” of a
graph. More importantly, it contains every possible edge between its 3 vertices and
will be denoted by K3. More generally, the complete graph KN is a graph with N
vertices that contains all possible edges between them. Our use of the articles “the”
and “a” in the preceding sentence may look inconsistent, because we could also have
another complete graph ˆK3 with three vertices, for example V(ˆK3) = {1, 2, 3}. But
since K3 and ˆK3 will differ only in the chosen numbering of the vertices, they will
be what mathematicians call isomorphic and have the exact same properties. For
our purposes here it will be convenient to leave it to our readers how they want to
number the vertices and somewhat informally treat isomorphic graphs as identical.
The subgraph of G1 that is induced by V−= {1, . . . , 10} is not a complete
graph. For example, {1, 3} would be a possible edge, but it is not in E(G1). You
can think of the adjacency relation in this induced subgraph as representing people

Vaccination Strategies for Small Worlds
229
sitting next to each other around a table. This induced subgraph will be denoted
here by GNN(10, 1). It is an example of a one-dimensional nearest-neighbor net-
work G1
NN(N, d). Such networks, and their two-dimensional counterparts G2
NN(N, d)
will play a key role in this chapter. We urge readers to work through our detailed
investigation of these networks in [21].
A path in a graph G is a sequence of nodes P = (i1, i2, . . . , im) such that each of
the pairs {i1, i2}, {i2, i3}, . . . , {im−1, im} is an edge of G. The length of a path P =
(i1, . . . , im) is ℓ= m −1, that is, the number of edges that we traverse along the
path. The length of the shortest path between two distinct nodes i and j in a graph G
is called the distance between i and j in G and is denoted by d(i, j). For example, in
G1 both (11, 12, 13) and (11, 13) are paths from node 11 to node 13, with length 2
and 1 respectively. The distance d(11, 13) = 1.
In G1, there is no path from node 1 to node 12 whatsoever. We will consider
the distance between nodes 1 and 12 as inﬁnite. The connected component of a
node i in a graph G is the subgraph whose vertex set comprises all nodes at a
ﬁnite distance from i. In G1, the connected component of node 1 is the subgraph
induced by V−= {1, . . . , 10}, and the connected component of node 12 is the
subgraph induced by V−= {11, 12, 13}. A graph G is connected if it has exactly
one connected component, that is, if the distance between any two nodes is ﬁnite.
The diameter of a graph G, denoted by diam(G), is the largest distance between any
two of its nodes. The diameter of G1 is inﬁnite; only connected graphs have ﬁnite
diameter. The diameter of the connected component of node 1 in G1 is 5, while the
diameter of the connected component of node 12 in G1 is 1. If, for example, we were
to add the edges {1, 6} and {3, 8}, then the diameter of the connected component
of node 1 would decrease to 4.
1.1.4
Network-Based Models
These models still use compartments—each node will belong to one of the S-, I-, or
R-compartments at a single point in time—but unlike compartment-level models,
they also allow for differences between individual hosts. More precisely, they are
based on the assumption that only hosts that are adjacent in a given contact network
can make effective contact.
The parameters of a network-based SIR-model are a graph G that represents
the contact network and disease-transmission parameters a, b (for discrete-time
models) or α, β (for continuous-time models). Here we retain the assumption of
homogeneity of hosts and consider effective contacts along each edge of the contact
network equally likely. Thus the transmission parameters bi,j or βi,j between pairs
of hosts can be written as bi,j = b (or βij = β) if {i, j} ∈E(G) and bi,j = 0 (or
βij = 0) otherwise.
Note that the uniform mixing assumption will be satisﬁed if, and only if, the
contact network is the complete graph KN. Thus in a sense, network-based models
include compartment-level models as special cases.

230
W. Just and H.C. Highlander
1.2
Further Reading
This introduction provides only a couple of highlights from the vast literature
on the basics of epidemiology and in particular on compartment-level models.
For students who want to learn more about these models, we recommend [26]
as an easily accessible introduction and [2] as a more comprehensive one, at a
more mathematically advanced level. More sources can be found in our online
review [15].
Network-based models are covered to a limited extent in some of the resources
that we listed above. A mathematically more advanced treatment can be found in [3].
We recommend our chapter [16] as a detailed elementary introduction into this type
of model.
The spread of infectious diseases is inherently a stochastic process. A course
on stochastic processes is not a prerequisite for success with our projects, but the
content in this paper relies heavily on probability theory. Our online module A brief
review of basic probability theory [6] covers the notions that are used here and in
our other materials such as [7–21]. While it cannot replace a regular textbook on
probability, it can serve as a short refresher course.
In what follows, we assume the reader has worked through our online module A
quick tour of IONTW [21]. In this module we provide detailed instructions on how
to use our software and guide the reader through some of the capabilities of IONTW.
Highlights include the types of networks supported, setting up various types of
models of disease transmission, observing the resulting dynamics, and collecting
statistics on the outcomes. Along the way, the module also illustrates basic notions
of graph theory. It contains many exercises, and sample solutions are included at its
end.
2
Exploring Erd˝os-Rényi Random Graphs
In this section we introduce the notion of random graphs and explain how such
networks can be used to derive meaningful predictions about disease transmission.
We then deﬁne the most basic type of random graphs and explore the distribution of
the sizes of their connected components. Along the way we introduce the important
concept of asymptotic almost sure convergence.
2.1
Random Graphs
In Section 1.1.4 we have tacitly assumed complete knowledge of the contact
network. But for transmission of most diseases in large real communities it is usually
impossible to construct the actual relevant contact network. Typically we will have
only some partial information about it; for example, estimates of some network
parameters such as the mean degree ⟨k⟩, or some knowledge of the mechanisms by
which contacts are established. How can we possibly draw meaningful inferences
from network-based models in the face of such uncertainty?

Vaccination Strategies for Small Worlds
231
Moreover, the networks we have looked at so far, complete graphs and nearest-
neighbor networks, have a well-deﬁned and very rigid structure. This will not be the
case for contact networks in almost any real population of hosts, which will be much
more messy. How can we possibly draw meaningful inferences from network-based
models in the face of such messiness?
Well, how about using the word “randomness” instead of “uncertainty” and
“messiness”? Then we see that the difﬁculties outlined in the preceding two para-
graphs are actually two sides of the same coin (aka a random number generator):
Usually some randomness is inherent in the processes by which contacts are
established. This accounts both for the observed lack of structure and for our
uncertainty. But if the processes by which contacts are established are random,
then we can think of the actual (messy and unknown) contact network as being
drawn from a certain probability distribution, about which we usually have some
partial knowledge and which perhaps is similar to a simple distribution with nice
mathematical properties.
Even when we have a reasonably good idea about the distribution, we won’t
know the actual contact network. But if we draw several graphs from the given
distribution, we might reasonably expect that they form a representative sample of
actual contact networks. Essentially this is the same trick that we use whenever
we run simulations. During the run of multiple simulations, the computer produces
in effect a representative sample of possible outbreaks for the given network, and
we can use the outcome of simulations to form reasonably reliable hypotheses
about what should happen in real outbreaks. The only novelty is that we are now
considering two distinct sources of uncertainty: Uncertainty about the actual contact
network, and uncertainty about the actual course of the outbreak.
Thus we may base a disease transmission model on a given distribution of
random graphs and explore these models either by simulations on a representative
sample of networks from this distribution, or by deriving theoretical predictions
about the expected courses of outbreaks in these models.
2.2
Deﬁnition of Erd˝os-Rényi Random Graphs
There are various constructions of random graphs; several of them are implemented
in IONTW. The most basic of these constructions gives Erd˝os-Rényi random graphs,
named after the two Hungarian mathematicians who ﬁrst systematically explored
these graphs in the seminal paper [4]. These graphs serve as a benchmark against
which all other constructions of random networks can be compared.
To construct such an Erd˝os-Rényi graph, we ﬁrst decide on the number N
of nodes. Then we list all edges e1, . . . , e N(N−1)
2
of the complete graph KN and
repeatedly toss a biased coin that comes up heads with probability p. We include eℓ
as an actual edge of the random graph if, and only if, the coin comes up heads in
toss number ℓ.
The mean degree ⟨k⟩of the resulting graph will be approximately
⟨k⟩≈λ = p(N −1).
(2)

232
W. Just and H.C. Highlander
Thus by choosing a suitable value of the connection probability p, one can assure
that the mean degree ⟨k⟩of an Erd˝os-Rényi random graph will be close to the values
that one might have estimated from data on real networks.
It will be more convenient if we think of Erd˝os-Rényi random graphs in terms of
the parameter λ instead of the parameter p. The connection probability p can then be
expressed as p =
λ
N−1. The symbol GER(N, λ) will denote an Erd˝os-Rényi random
graph that is constructed with parameters N and λ.
2.3
Properties That Hold Asymptotically Almost Surely
Note that we used the indeﬁnite article an in the previous sentence. A graph
GER(N, λ) is not uniquely determined; in fact, it could be any graph G with N
vertices. The symbol GER(N, λ) only signiﬁes that the graph is randomly drawn
from a speciﬁc probability distribution. We will call a particular graph that has been
constructed by the method described above an instance of GER(N, λ).
Note also that (2) contains the symbol ≈. For a given instance of an Erd˝os-Rényi
random graph we should not expect exact equality ⟨k⟩= λ. But ⟨k⟩will be very
close to λ.
But wait a minute. How could we possibly write that ⟨k⟩will be very close to λ?
If GER(N, λ) could be any graph with N nodes, then it could also be the graph
with no edges whatsoever (with ⟨k⟩= 0) or the complete graph KN (with ⟨k⟩=
N −1). So we cannot be sure that ⟨k⟩will be very close to λ, and should not have
written “will be.” What we should have written is something along the lines of
“we can be pretty darn sure that ⟨k⟩≈λ.” This is somewhat cumbersome, and
for this reason in the literature on random graphs “will be” is somewhat sloppily
used as shorthand for “pretty darn sure,” which of course isn’t a phrase that belongs
in a respectable research paper. In polite company mathematicians use the phrase
“the mean degree ⟨k⟩of GER(N, λ) converges to λ asymptotically almost surely
(abbreviated a.a.s).” This means that for any ﬁxed error bound ε > 0, the probability
that the mean degree ⟨k⟩of a randomly drawn instance of GER(N, λ) will differ
from λ by more than ε will approach 0 as N →∞.
Note that we cannot write here “almost surely” (which would mean with
probability 1), and cannot translate a.a.s. convergence into a property of instances
of GER(N, λ) for a ﬁxed N. It is a property of the class of all Erd˝os-Rényi random
graphs with parameter λ that contains instances of GER(N, λ) of arbitrarily large
size N. For convenience, we will use the symbol GER(N, λ) both for the entire class
of Erd˝os-Rényi random graphs with ﬁxed parameter λ and variable N and for a given
instance. In most cases the correct interpretation will be implied by the context;
when there is ambiguity we will insert the phrase “an instance of” or “the class” to
avoid confusion.
The phrase “asymptotically almost surely” also can be used for properties that
are categorical rather than expressed in numerical values. For example, we will
show in the next subsection that for any ﬁxed λ a graph GER(N, λ) will a.a.s. be
disconnected. This means that as N →∞the probability of drawing a connected

Vaccination Strategies for Small Worlds
233
Table 1 Parameter settings for Erd˝os-Rényi exploration.
infection-prob
end-infection-prob
network-type
num-nodes
lambda
auto-set
1
1
Erdos-Renyi
300
1.5
Off
Fig. 3 View of IONTW interface after selecting an Erd˝os-Rényi contact network with 300 nodes
and λ = 1.5.
instance of GER(N, λ) approaches zero. This does not rule out that even for very
large N we could accidentally draw a connected instance. It only implies that for
large N these events become very, very unlikely.
When we study properties of random graphs, usually the best possible results we
can hope for is that a given property will hold a.a.s.
2.4
Exploring the Connected Components of Erd˝os-Rényi
Random Graphs
Open IONTW, press Defaults, set the speed control slider to the extreme right, and
use the parameter settings in Table 1.
These parameter settings specify a next-generation SIR-model on an Erd˝os-Rényi
network GER(300, 1.5). Press New to look at the network. Your view in IONTW
should look like a variation of that in Figure 3.
With this default view it is not possible to visually make out the connected
components. (One can view the connected components by pressing the Spring
button on the interface, but we will determine the same information via another
route.) We can use the properties of the disease transmission model to visualize
them: Since the probability b of an effective contact until the next time step,
controlled by the input ﬁeld infection-prob, is equal to 1, all nodes in the connected
component of the index case j∗will eventually experience infection. The input
setting end-infection-prob = 1 speciﬁes a next-generation SIR-model in which
all infectious nodes will get removed after exactly one time step and turn grey. If
initially there is exactly one index case j∗in an otherwise susceptible population,
all nodes outside of the connected component of j∗will remain green, and the
connected component of j∗will show up in grey at the end of the simulation.

234
W. Just and H.C. Highlander
Table 2 Batch processing settings for testing major/minor outbreaks.
Repetitions
Measure runs using these reporters
Setup commands
100
count turtles with [removed?]
new-network
To see how this works, press Set to introduce one infectious node, and then Go.
Repeat about 10 times for this network using Reset and then Set. This will keep the
network ﬁxed, but will change the initially infectious node. Record the approximate
sizes of the connected component by moving your mouse over the relevant part of
the grey curve in the Disease Prevalence plot.
Exercise 1. What do you observe? Do you get connected components with a range
of different sizes? If the component is large, is it always the same one? How can
you tell from the plot?
The results may look puzzling. Repeat 10 more times, but look at a different
instance of GER(300, 1.5) each time by pressing New instead of Reset before
pressing Set.
Exercise 2. In what respect are the results similar to the ones of the previous
exercise; in what respect are they different?
This is interesting. It appears that there is always one very large component in
addition to many small ones.
Let us try to conﬁrm the results we have discovered so far by running a large
batch of simulations instead of looking at a few instances.
Switch auto-set: On
With the current parameter settings, deﬁne and run a batch processing experiment
by using the template given in [13]. Deﬁne a New experiment according to Table 2.
Exercise 3. After the experiment is completed, open and analyze your output ﬁles.
The column with the header count turtles with [removed?] reports the
sizes of the connected component of the initially infectious node. Try to detect
a distinctive gap between small and large components that were reported. Then
record the maximum size of the observed small components and the mean size
of the observed large components. Express these numbers as fractions of the total
population size. Do your results conﬁrm the preliminary observations that you made
in the previous exercises?
Now let us quote a theorem that explains the outcomes that you probably
observed in Exercises 1–3. Let Θ denote the proportion of the nodes in the largest
connected component, that is, the ratio that is obtained by dividing the size of the
largest connected component of a given graph GER(N, λ) by N.
Theorem 1. Consider the class of Erd˝os-Rényi random graphs GER(N, λ), where
λ > 0 is ﬁxed and N is arbitrary. Then there exist constants 0 < ϱ = ϱ(λ) < 1 and
csmall = csmall(λ) > 0 that depend only on λ such that:

Vaccination Strategies for Small Worlds
235
•
When λ < 1, then
– Θ →0 a.a.s.
– More precisely, a.a.s. all connected components of the graph E(GER(N, λ))
will have size ≤csmall ln(N).
•
When λ > 1, then
– Θ →ϱ a.a.s.
– A.a.s., all other connected components of the graph E(GER(N, λ)) will have
size ≤csmall ln(N).
Exercise 4. Before reading on, take a few minutes to think about the theorem.
Does it predict the pattern that you observed in your explorations? How would you
estimate the value of ϱ(1.5) based on your ﬁndings?
Note that the second item of the theorem for λ < 1 implies the ﬁrst since
lim
N→∞
csmall ln(N)
N
= 0.
(3)
We will call a connected component of GER(N, λ) small if its size does not
exceed csmall(λ) ln(N). For sufﬁciently large N, each small component will contain
only a negligible fraction of nodes. (Note that by (3) we are allowed to write “will”
here.) Since ϱ(λ) is always less than 1, the theorem implies that a.a.s. GER(N, λ)
will contain many small components, and, in particular, will a.a.s. be disconnected.
In contrast, when λ > 1 the largest connected component will a.a.s. comprise
a fraction of ≈ϱ(λ) of all nodes. By the last item of the theorem, it will be a.a.s.
the unique connected component with this property. In the literature, it is usually
referred to as the giant component.
The value ϱ = ϱ(λ) is the unique solution of the equation
1 −ϱ = e−λϱ
(4)
in the interval (0, 1). It can be shown that for λ > 1 there exists exactly one such
solution, while for λ ≤1, no solutions of (4) fall in this interval. The function ϱ(λ)
is strictly increasing, with limλ→1+ ϱ(λ) = 0 and limλ→∞ϱ(λ) = 1. For example,
ϱ(1.1) = 0.1761,
ϱ(1.5) = 0.5828,
ϱ(2.0) = 0.7968,
ϱ(3.0) = 0.9405.
(5)
For more information on the connected components of GER(N, λ) as well as the
history and extensions of Theorem 1 we recommend the survey article [24].
The alert reader will have noticed the analogy between small vs. large compo-
nents of GER(N, λ) and minor vs. major outbreaks of diseases. The itemized list
in Section 1.1.1 does look somewhat similar to the items of Theorem 1, and there
is in fact a close connection. With some careful wording of the assumptions, the
relation between R0 and minor and major outbreaks can even be phrased in terms
of a.a.s. convergence, although this is rarely done in the literature. Having a good
understanding of the precise nature of the connection between giant components in

236
W. Just and H.C. Highlander
certain random graphs and major outbreaks may be very useful for the theoretical
parts of some of our projects. While it would take us too far aﬁeld to include the
details here, we do recommend that students who did choose to work on one of our
projects consult in due course Section 3 of Module [10].
3
Clustering Coefﬁcients
In this section we introduce so-called clustering coefﬁcients. A motivating example
shows how these characteristics of the contact network may inﬂuence the spread of
an infectious disease. In later subsections we explore, both with the help of IONTW
and theoretically, the behavior of clustering coefﬁcients for various network types.
3.1
A Motivating Example
Random regular graphs GReg(N, k) are graphs with N nodes where every node has
degree k, but where the edges are randomly assigned (see modules [19, 21] for
further details). By Exercise 4 of our module A quick tour of IONTW [21], the one-
dimensional nearest-neighbor networks G1
NN(N, d) are k-regular for k = 2d when
N ≥2d + 1. Would it be reasonable to expect that diseases would spread on these
two types of networks in similar ways? Let us see whether simulations shed any
light on this question.
Open IONTW, click Defaults, and change the parameter settings in Table 3.
Press New to initialize a next-generation SIR-model on a network GReg(200, 4)
with one index case in an otherwise susceptible population. Similar models were
investigated in our modules Exploring random regular graphs with IONTW [19].
We would expect to see a signiﬁcant proportion of major outbreaks in addition to
some minor ones. You may want to run a few exploratory simulations to check
whether this is what you will see in the World window and the Disease Prevalence
plot.
Now let us conﬁrm the preliminary observations with a lager number of
simulations. Using the template that is provided in our online instructions [13],
set up a batch processing experiment for the current parameter settings, by deﬁning
a New experiment with the speciﬁcations outlined in Table 4.
Table 3 Parameter settings for the motivating example.
infection-prob
end-infection-prob
network-type
num-nodes
lambda
auto-set
0.5
1
Random Regular
200
4
On
Table 4 Settings for batch processing experiment on GReg(200, 4).
Repetitions
Measure runs using these reporters
Setup commands
100
count turtles with [removed?]
new-network

Vaccination Strategies for Small Worlds
237
Exercise 5.
(a) Run the experiment and analyze your data by sorting the output column
(count turtles with [removed?]) from lowest to highest. If you see
a distinct gap between minor and major outbreaks, report the number of minor
outbreaks, as well as the mean and maximum values of the output variable for
these outbreaks. Also report the minimum, mean, and maximum for the major
outbreaks, as well as the overall mean.
(b) Are the results consistent with your expectations?
Now set network-type →Nearest-neighbor 1 and d := 2. Press New to initialize
a next-generation SIR-model on a network G1
NN(200, 2) with one index case in an
otherwise susceptible population. Remember that this will give degree k = 2d = 4
to each node. Press Clear on the bar of the Command Center (not the Clear
button within the NETLOGO interface), then press Metrics to verify that <k> = 4.
You may want to run a few exploratory simulations to check whether you see
similar results in the World window and the Disease Prevalence plot as for the
previous network type.
Now let us conﬁrm the preliminary observations by setting up and running a New
batch processing experiment with 100 runs for the current parameter settings.
Exercise 6.
(a) Run the experiment and analyze your data as in Exercise 5.
(b) Are the results similar to those in the previous experiment? If not, does the
structure of G1
NN(200, 2) appear to increase or decrease the severity of outbreaks
relative to the corresponding random regular graph?
To illustrate what is going on here, let us consider a state at time t = 1
with exactly 4 infectious nodes j1, j2, j3, j4. Each of these nodes will have one
neighbor (the index case) who infected it and is no longer susceptible at time
t = 1. Let N1(j) denote the set of nodes i that are adjacent to j. Each one of
these nodes must be adjacent to the index case j∗. In a large random 4-regular
graph, with high probability it will be the case that the union of the neighborhoods
N1(j1), N1(j2), N1(j3), N1(j4) contain a total of 12 susceptible nodes to whom the
pathogen could be transmitted by time step 2.
Now let us see how the situation differs in graphs G1
NN(N, 2). Set infection-
prob := 1. The pattern you will observe here is the same for all sufﬁciently large
values of N, but for better visualization set num-nodes := 20.
Create a New network with one index case in an otherwise susceptible popula-
tion. In the World window you will see that N1(j∗) contains 4 nodes.
Move the speed slider to a very slow setting; adjust for comfortable viewing as
needed. Start a simulation with Go and stop it by pressing Go again when you see a
state with exactly 1 removed and 4 infectious nodes in the World window. Count the
number of green nodes at the end of red edges that could become infectious at the
next time step. It will be less than 12. This effect results from the special structure
of the network and explains the discrepancies that you observed.

238
W. Just and H.C. Highlander
In your World window you will see some white edges with two red endpoints.
Look at one of these edges. No effective contact between its endpoints by time t =
2 can lead to a new infection, and in some sense this edge decreases the number
of potential nodes that can become infectious at the next time step by 2 (one for
each endpoint). Clustering coefﬁcients allow us to quantify this effect. They indicate
whether we should expect many or relatively few such edges. They explain some of
the decrease in the number of candidates for infection at the next step from 12 to the
one you just found.
Each of the white edges with red endpoints that you see is an edge of a
triangle whose third endpoint is the grey node that represents the index case.
Clustering coefﬁcients can be deﬁned by counting the number of potential triangles;
high clustering coefﬁcients indicate that there are a lot of them; low clustering
coefﬁcients indicate few.
Count the number of white edges that connect two red nodes. There should be 3
of them; each one is part of a triangle whose third vertex is the index case and whose
other two edges are grey.
Are 3 white edges a lot? To make sense of the phrases a lot or few we need
to compare the observed numbers with some benchmark. In the case of clustering
coefﬁcients, the benchmark is the complete graph.
Choose network-type →Complete Graph and num-nodes := 5.
Create a New network. Run a simulation in slow motion for exactly one time step
and count the number of white edges that connect red nodes. This number gives us
the benchmark; it is the number of edges in a complete graph Kn, where n is the size
of N1(j∗) in the previous experiment. In our case n = 4 and the number of edges in
the complete graph is 6.
If we divide the number of white edges that we observed in the previous
experiment by 6, we obtain the node clustering coefﬁcient of the index case. The
formal deﬁnition will be given in the next subsection.
3.2
Deﬁnitions of Clustering Coefﬁcients
Several subtly different notions of clustering coefﬁcient also known as transitivity
have been studied in the literature. One always needs to carefully read the deﬁnition
to see what, exactly, these terms mean in the given source. We will work with four
such notions.
Consider a node i in a graph G. Recall that N1(i) denotes the set of i’s neighbors,
that is, nodes that are adjacent to i. Let tr(i) denote the number of edges {j1, j2} ∈
E(G) such that j1, j2 ∈N1. The number tr(i) is exactly the number of triangles that
node i forms with two of its neighbors.
Watts and Strogatz [27] deﬁne the node clustering coefﬁcient3 C(i) of i by
dividing tr(i) by its maximum possible value ki(ki −1)/2, where ki is the degree
of node i.
3Some authors refer to node clustering coefﬁcients as local clustering coefﬁcients.

Vaccination Strategies for Small Worlds
239
C(i) =
2tr(i)
ki(ki −1).
(6)
If ki < 2, then Equation (6) does not make sense, and we deﬁne C(i) as the edge
density, that is, the probability 2|E(G)|
N(N−1) that two randomly chosen nodes are adjacent.
When G represents friendships among people, the clustering coefﬁcient C(i)
measures the ratio of the number of friendships between any two of i’s friends
relative to a situation where all these friends would induce a complete subgraph
of G. Mathematicians actually refer to such sets that induce complete subgraphs as
cliques.
The network clustering coefﬁcient C is deﬁned as the mean of the node clustering
coefﬁcients C(i):
C = 1
N
N

i=1
C(i).
(7)
In Section 3.1 we have already seen one interpretation of clustering coefﬁcients.
Here is an alternative interpretation that is often given in the literature. Consider a
network G. Suppose we randomly pick a node i, and then we randomly pick two
nodes j1, j2 that are adjacent to i. Does this procedure make it more likely or less
likely that the pair {j1, j2} forms an edge in a given graph, relative to a completely
random choice of j1, j2? Intuitively, one would expect the answer “more likely” for
networks of social contacts. Two randomly chosen friends of yours are more likely
to be friends of each other than two randomly chosen persons. You and your friends
will form a cluster in the friendship graph.
To see the connection with clustering coefﬁcients, let us ﬁrst observe that our
procedure requires that j1, j2 ∈N1(i). If {j1, j2} is an edge, then the subgraph
of G that is induced by the set of nodes {i, j1, j2} will form a triangle. If G
contains relatively many triangles, as will be the case in large nearest neighbor
graphs with d > 1, we might expect that the answer will be “more likely.” On
the other hand, we should expect the answer to be “less likely” if G contains only
relatively few triangles. If G does contain some edges but no triangles at all, as in
graphs G1
NN(N, 1) for N > 3, then {j1, j2} simply cannot be an edge and the answer
will deﬁnitely be “less likely.”
But what, exactly, do the phrases “relatively few” and “relatively many” triangles
(or cliques of size 3) mean? The network clustering coefﬁcient C does not all by
itself tell us whether two randomly chosen nodes are more likely, on average, to
be adjacent in G if they share a common neighbor. A concrete example of this
phenomenon is explored in detail in Section 3 of Module [8]. To remedy this
drawback, let us introduce normalized clustering coefﬁcients. These are obtained
by dividing by the edge density:

240
W. Just and H.C. Highlander
Cnorm(i) = C(i)N(N −1)
2|E(G)|
and
Cnorm = CN(N −1)
2|E(G)| = CN −1
⟨k⟩
= 1
N
N

i=1
Cnorm(i).
(8)
In the second line of (8) we used the fact that 2|E(G)| = ⟨k⟩N.
While C(i), C are numbers between 0 and 1, the normalized clustering coefﬁ-
cients can take any nonnegative rational numbers as values. A value Cnorm(i) > 1
indicates that the nodes in N1(i) are more likely than average to be adjacent; a
value Cnorm(i) < 1 indicates that for average i the nodes in N1(i) are less likely to
be adjacent than randomly chosen nodes. If Cnorm > 1 we will say the graph exhibits
clustering; if Cnorm < 1 we will say that the graph avoids clustering.
Exercise 7. Find the clustering coefﬁcients C(i), C, Cnorm(i), Cnorm for (each node i
of) each of the following graphs and determine whether the graph exhibits or avoids
clustering.
(a) For the graph G1
NN(9, 2).
(b) For the graph G2
NN(15, 1).
(c) For the graph G1 of Figure 2.
One can think of Cnorm for a given G as comparing C of G with a benchmark.
As the next exercise shows, this benchmark would be an Erd˝os-Rényi random graph
with the same number of nodes and mean degree as G.
Exercise 8. Give an intuitive argument that for large N the normalized network
clustering coefﬁcient Cnorm in GER(N, λ) should be very close to 1.
The values of Cnorm can be very large; see [8] for some examples of empirically
studied networks where they are on the order of several hundreds. In such cases
one would like to say that the network exhibits strong clustering. But there is no
natural ﬁxed threshold value above which clustering would qualify as “strong.” A
mathematically meaningful deﬁnition of this notion will require us to consider a
class of graphs that contains representatives of arbitrarily large size N. We can
then say that this class of graphs exhibits strong clustering if Cnorm →∞a.a.s.
(asymptotically almost surely), which means here that for every probability q < 1
and ﬁxed Ctarget there exists N(q, Ctarget) such that with probability > q a randomly
drawn network of size N > N(q, Ctarget) in this class will satisfy the inequality
Cnorm > Ctarget.
Thus while it makes sense to say that a given network exhibits or avoids
clustering, the phrase “strong clustering” does not make sense for an individual
network; it applies only to classes of networks. By Exercise 8, for any given λ the
class of Erd˝os-Rényi networks GER(N, λ) does not exhibit strong clustering. In the
next subsection you will see examples of classes that do.

Vaccination Strategies for Small Worlds
241
Table 5 Parameter settings
for investigating clustering
coefﬁcients.
network-type
lambda
num-nodes
Erdos-Renyi
8
20, 40, 80, 160, 320
3.3
Exploring Clustering Coefﬁcients of Selected Networks
Open IONTW and press Defaults. Work with the parameter settings in Table 5.
For each of the speciﬁed network sizes create one network with New and then
press Metrics before creating the next network. When you are done, use the double
arrow on the bar Command Center to enlarge this window and look at the statistics
that you collected.
Exercise 9.
(a) Consider the values of Edge density, Clustering coefficient, and
Normalized clustering coefficient. Which limits do these values
appear to approach as N increases?
(b) Does the class of networks GER(N, 8) appear to exhibit strong clustering?
(c) Are these results consistent with what you learned in Section 3.2?
Press Clear to clean up the Command Center and minimize this window.
Change network-type →Nearest-neighbor 1 and d := 2.
Repeat the steps of the data collection that you did for Erd˝os-Rényi networks of
the sizes speciﬁed above. As you proceed, you may want to visualize the distribution
of the values of C(i) by choosing plot-metric →Normalized Coeffs and pressing
Update.
Exercise 10. Answer the analogous questions as in Exercise 9(a),(b).
Save your statistics, and then change network-type →Nearest-neighbor 2.
Repeat the exercise for the following values of num-nodes: 25, 36, 100, 225.
Repeat the steps that you did for the previous types of networks to collect data
on networks G2
NN(N2, 2) of the speciﬁed sizes N2. Inspect the data.
Exercise 11. Answer the analogous questions as in Exercise 9(a),(b).
Now set d :=1. Press New and then Metrics. The command center will show you
that both clustering coefﬁcients C and Cnorm are 0. This should be expected from the
deﬁnitions, as the graph in the World window contains no triangles whatsoever.
3.4
A General Theorem About Strong Clustering
The following result will be very useful for our work in Section 5.

242
W. Just and H.C. Highlander
Theorem 2. Suppose we are given a class of graphs G(N) that contains repre-
sentatives of arbitrarily large sizes N. Moreover, assume that the mean degree ⟨k⟩
approaches a.a.s. a ﬁnite limit as N increases without bound, and that tr(i) ≥1 for
each node i in all graphs G(N). Then this class exhibits strong clustering.
First note that for all ﬁxed d
≥
1 and sufﬁciently large N the graphs
G1
NN(N, d) are 2d-regular and thus satisfy the ﬁrst assumption of Theorem 2. The
graphs G2
NN(N2, d) are not regular, but one can show that for any ﬁxed value of d
they still satisfy this ﬁrst assumption. Thus in all these classes there exists some
ﬁnite upper bound kmax on the degrees so that ki ≤kmax for all nodes i in all
graphs G(N) of the class.
Exercise 12.
(a) Find an upper bound on the degree of any node in G2
NN(N2, d) that does not
depend on N.
(b) Show that the graphs G1
NN(N, d) and G2
NN(N2, d) with N > 2 and d > 1 have
the property that tr(i) ≥1 for each node i.
Thus Theorem 2 implies that all classes G1
NN(N, d) and G2
NN(N2, d) with
ﬁxed d > 1 exhibit strong clustering. You may want to compare this result with
your ﬁndings in Exercises 10 and 11.
Challenge Problem 1. Prove Theorem 2 under the additional assumption that there
exists some ﬁnite upper bound kmax on the degrees so that ki ≤kmax for all nodes i
in all graphs G(N).
4
Exploring Distances with IONTW
This section has two parts. The ﬁrst part is purely conceptual and invites readers to
critically evaluate popular claims based on Stanley Milgram’s famous experiment
that gave birth to the phrases “small-world property” and “six degrees of separation.”
In the second part we develop a formal deﬁnition of the small-world property.
IONTW-based exercises are used in that part to illustrate conceptual subtleties of
this notion.
4.1
Milgram’s Famous “Six Degrees of Separation” Experiment
Consider the network GFN whose nodes represent humans and whose edges connect
any two persons who are acquainted on a ﬁrst-name basis. Let d(i, j) be the distance
between two nodes in this network, that is, the number of edges in the shortest path
from i to j in GFN. If i, j are chosen randomly, then d(i, j) becomes a r.v. What can
we say about its distribution?

Vaccination Strategies for Small Worlds
243
The American social psychologist Stanley Milgram and his collaborators con-
ducted an ingenious experiment to answer this question; the results were reported
in [25]. The experimenters recruited 296 volunteers from Nebraska and the Boston
area. Each volunteer i was given a letter with some information about a Boston stock
broker j and was instructed to send it to a person with whom the volunteer was
acquainted on a ﬁrst-name basis (along an edge of GFN) and whom the volunteer
thought to be at a closer distance from j in GFN. Attached to the letter were
instructions to continue forwarding it in this manner, until it would be sent to
the stock broker j. The experimenters kept track of the number of intermediaries
i1, . . . , im (excluding i and j) that forwarded the letter. Out of the 296 letters, 64
eventually reached their target.
Exercise 13.
(a) Suppose the letter did eventually get sent to j. How is the number m of
intermediaries related to the distance d(i, j)?
(b) Does the success rate of
64
296 (approximately 22%) tell us anything about the
structure of the network GFN?
For the letters that did arrive, the researchers reported a mean number of 5.2
intermediaries. This result has inspired the popular claim that there are only six
degrees of separation between any two humans.
Exercise 14. Critically evaluate this claim. What do or don’t the results of Milgram
tell us about the likely maximum, mean, or median of d(i, j) in GFN?
We strongly recommend that you don’t try to look up our solution yet. Instead,
write down your own thoughts, and reevaluate your solution after you have worked
through the next subsection.
4.2
The IONTW Guide to the Small World (Property)
The claim that you evaluated in Exercise 14 gave birth to the phrase small-world
property. The literature contains various subtly distinct and not always entirely
rigorous deﬁnitions of this concept. Here we will construct one that may work best,
while using IONTW to build up some intuition about the subtleties involved.
Why would a mean or even maximum distance of 6 (or even 2 or 3) between
two randomly chosen people be considered surprisingly small? There would not be
anything remarkably small about this number in a village of 100 people or even in
the town of 20,000. But on the scale of the whole human population, the number 6
seems surprisingly small. We can see that a rigorous deﬁnition of the small-world
property will make sense only if we phrase it so that it applies to networks of
arbitrarily large sizes N. In other words, it should be a property of a whole class
of networks, not of an individual representative of this class. Let’s give it a ﬁrst try:

244
W. Just and H.C. Highlander
Table 6 Parameter settings
for investigating the
small-world property.
network-type
num-nodes
d
Nearest-neighbor 2
100
1
Preliminary Vague Deﬁnition. A class of networks has the small-world property
if the maximum or the average distance between nodes for networks in this class
scales at most logarithmically with network size N.
This “Deﬁnition” has some problems, doesn’t it? First of all, it is vague about
whether we should use the maximum or the average distance. And “the” average
distance isn’t well-deﬁned either, since there are several kinds of averages, most
prominently means and medians. You may or may not be already familiar with the
precise meaning of the phrase “scales at most logarithmically.” And if the class of
networks that we are interested in is a class of random graphs, shouldn’t there be an
“a.a.s.” somewhere in the deﬁnition?
We will address each of these issues and arrive at a deﬁnition of the small-world
property that avoids all the road hazards that we will encounter along the way.
Open IONTW, click Defaults, and choose the parameter settings in Table 6.
Create a two-dimensional nearest neighbor network G2
NN(100, 1) by pressing
New and then toggle Labels to see how the nodes are numbered.
Exercise 15.
(a) Find d(13, 21) and d(21, 66) in this network.
(b) Find the diameter of this network, that is, ﬁnd the maximum distance of any pair
of nodes. For which pairs of nodes is the maximum attained?
It would be a bit tedious to calculate the mean or median distances for randomly
chosen nodes in G2
NN(100, 1). Can you look it up in the Command Center after
pressing Metrics? Well, maybe. The display gives you a line
Average path length in largest component = 6.66667
The word “Average” is a bit ambiguous; it could refer to a mean, median, or
mode. “Path length” is also a bit vague. Since we will use this metric later in some
of our explorations, let us divulge that this average reported by IONTW is actually
a mean. But we highly recommend that you do Exercise 5 of Module [9] to ﬁnd out
more about the exact meaning of Average path length. Exercise 6 of this
module provides examples, with illustrations of some beautiful graphs, of how the
mean and median distances may differ.
The mean distance and the diameter are properties of a given network. Let us
see what they tell us about the spread of diseases. Click Defaults and choose the
settings in Table 7. This sets up a next-generation SIR-model where at each time
step every host is guaranteed to make effective contact with all adjacent hosts.
Create a New network G2
NN(100, 1), and toggle Labels to see the numbering of
the nodes. Make node 0 the index case by using the following procedure: Click Set.
In the dialogue box that appears enter: [0]. Click OK. Note that you must type the
square brackets around 0 for this to work properly.

Vaccination Strategies for Small Worlds
245
Table 7 Parameter settings for testing mean distance and diameter.
time-step
infection-prob
end-infection-prob
network-type
1
1
1
Nearest-neighbor 2
num-nodes
d
set-state-by
100
1
Vector from input
Make sure that everything worked as expected so that you have an initial state
where node 0 is infectious and all other nodes are susceptible. Set the speed slider
to a slow speed; adjust for comfortable viewing as needed. Now click Go and watch
the movie. If you want to restart it, use Last.
Exercise 16.
(a) What is the relationship between node 0 and the nodes that are infectious at time
step t?
(b) How is the time at the end of the outbreak, measured in ticks, related to the
network properties that we discussed earlier?
Now repeat the experiment twice, by using Reset: First set node 50 to be the
single index case; then repeat the experiment again with node 55 as the single index
case. To accomplish this, in the dialogue box that appears after clicking Set you will
need to ﬁrst enter [50]; after running the experiment for this index case, repeat by
entering [55] after clicking Set.
While previously the duration of the outbreak was diam(G2
NN(100, 1)) + 1, now
you get shorter outbreaks. But even after subtracting 1, you will get a number that
exceeds the mean distance. Let us use the symbol D(j∗) for the number that we
get after subtracting 1 from the duration of the outbreak caused by index case j∗
in an otherwise susceptible population in a model with the disease transmission
parameters listed above. This number can be deﬁned for every network, and it is
always somewhere between the mean and the maximum distance of pairs of nodes
(see Exercise 8 of Module [9]).
We have observed something very important: Two network parameters, the
diameter and the mean distance, give us upper and lower bounds on the mean
duration of outbreaks of certain types of diseases. Thus one could use these network
parameters to make predictions about the duration of outbreaks4.
Unfortunately, in simulations we are restricted to exploring rather small net-
works, while real outbreaks happen in populations of thousands or millions of hosts.
It is therefore of interest to investigate how the diameter and mean distance scale if
we retain the general structure of the network but increase the number of nodes.
4The relation between these network parameters and the duration of outbreaks becomes less tight
for disease transmission parameters that are different from the ones we considered here, but there
will still be a close connection.

246
W. Just and H.C. Highlander
Table 8 Settings to test how
diameter and mean distance
scale with N.
num-nodes
d
network-type
11
1
Nearest-neighbor 2
Let us start with very simple networks. Press Clear on the Command Center
bar and change the settings according to Table 8.
Click New and ﬁnd the diameter of the network by visual inspection. Record it,
and then click Metrics to verify your results. Repeat with num-nodes := 23, 47.
Note that in the case of prime numbers N the graphs G2
NN(N, 1) are not really
two-dimensional grids but simple paths.
Enlarge the Command Center with the double-arrow icon and look at the data
that you have recorded. Would it be fair to say that for graphs G2
NN(N, 1) with N
prime the diameter and the mean distance roughly double when you roughly double
the number of nodes?
Such a pattern is indicative of linear scaling. We say that the value of a
quantity χ(N) that depends on N scales linearly if
lim
N→∞
χ(N)
N
= c,
(9)
where c is a nonzero constant. This does not mean that χ(N) = cN, it only means
that for sufﬁciently large N we can take cN as a fairly good estimate of χ(N). Since
c(2N) = 2cN, linear scaling produces the pattern that you observed.
Nonlinear scaling can take many different forms. For example, if D is a ﬁxed
exponent, then we say that χ(N) scales like ND if
lim
N→∞
χ(N)
ND
= c ̸= 0.
(10)
Of course, (9) is nothing else than the special case of (10) for D = 1. In other
words, linear scaling is a special kind of power law scaling.
Now suppose χ(N) scales like N0.5 =
√
N. Then quadrupling N should have the
effect of roughly doubling χ(N) as c
√
4N =
√
4c
√
N.
Let us see whether the class of square grids without diagonals is such an example.
Repeat the previous experiment for num-nodes := 25, 100, 400.
Does it appear that the diameter and mean distance scale like
√
N in the class of
networks G2
NN(N, 1) with N = n2?
So far, we have considered only networks with a rigid structure. But now let us
explore some random graphs. Press Clear on the Command Center bar. Change
the parameter settings according to Table 9.
Click New to create an instance of GReg(50, 4). Use Metrics to ﬁnd the mean
distance between nodes.
Since every instance of a random graph is slightly different, we may want to look
at several instances drawn from the same distribution. Create 5 instances each of

Vaccination Strategies for Small Worlds
247
Table 9 Settings to test
scaling in random graphs.
num-nodes
lambda
network-type
50
4
Random Regular
GReg(50, 4), GReg(100, 4), GReg(200, 4) by changing num-nodes accordingly and
using New. For each new instance, click Metrics.
Enlarge the Command Center with the double-arrow icon and look at the data
that you have recorded. Would it be fair to say that the mean distance between nodes
roughly increases by a constant number when you double the number of nodes?
This pattern is indicative of logarithmic scaling rather than power law scaling.
We say that χ(N) scales logarithmically if
lim
N→∞
χ(N)
ln(N) = c ̸= 0.
(11)
Since c ln(2N) = c(ln(2) + ln(N)) = c ln(N) + c ln(2), if χ(N) scales
logarithmically, then for sufﬁciently large N we would see a roughly constant
increment of χ(N) by ≈c ln(2) when we double N.
We need to be very careful though when we want to assert that a numerical
characteristic χ(N) (the mean distance or diameter in our example) of a class of
random graphs (GReg(N, 4) in our example) scales (at most) logarithmically. We
would like this to mean that for some ﬁxed constant c > 0 and all N we have
χ(N) ≤c ln N. But since, for example, G1
NN(N, 2) is also a 4-regular graph of size N,
it could be drawn as an instance of GReg(N, 4), although the probability of this event
is very, very small. In view of our explorations above, we will not be able to say
that for GReg(N, 4) the average distance or diameter is always ≤c ln(N). But for a
suitable choice of c, this inequality will hold asymptotically almost surely, that is,
with probability arbitrarily close to 1 as N →∞. Yes, “a.a.s.” should appear in a
proper deﬁnition of the small-world property.
This clariﬁes some of the issues we had with our “preliminary vague deﬁnition,”
but doesn’t yet resolve them.
We are aiming here at one universal deﬁnition that works for all classes of graphs.
In particular, it should work both for classes that contain connected graphs and for
classes that contain disconnected graphs. Consider, for example, the class of Erd˝os-
Rényi random graphs GER(N, λ) for a ﬁxed λ > 1. According to Theorem 1 of
Section 2.4, the graphs in this class are a.a.s. disconnected, with one giant connected
component of size close to ϱ(λ)N. One can prove that the diameter of the largest
connected component a.a.s. scales logarithmically for graphs GER(N, λ) [1].
Exercise 17. How could you use IONTW to obtain a rough empirical conﬁrmation
of this theoretical result about the diameter of the largest component by considering
instances of GER(N, 4) for N = 50, 100, 200?
These results show that if we randomly pick two nodes in a graph GER(N, λ), then
with probability ≈(ϱ(λ))2 they will both belong to the giant component and their

248
W. Just and H.C. Highlander
distance will be very small. As (ϱ(λ))2 does not depend on the network size N,
is positive for λ > 1, and even very close to 1 when λ is sufﬁciently large (see
Section 2.4), we would still consider the class of graphs GER(N, λ) for any ﬁxed
λ > 1 as an example of a class with the small-world property.
But graphs in this class are a.a.s. disconnected, and with positive probability
two randomly chosen nodes will belong to different components and thus have an
inﬁnite distance. In particular, both the diameter and the mean distance between
nodes in GER(N, λ) are a.a.s. inﬁnite. This makes these characteristics unsuitable for
our deﬁnition of the small-world property. Similarly, the median will work only if λ
is sufﬁciently large so that (ϱ(λ))2 ≥0.5, which would be an artiﬁcial cutoff. These
considerations lead to the following deﬁnition that we will use in the remainder of
this chapter.
Deﬁnition 2. A class of graphs has the small-world property if, and only if, for
some ﬁxed P > 0 there exists a positive constant c such that the P-th percentile of the
distance between randomly chosen nodes a.a.s. satisﬁes the inequality ≤c ln(N).
By taking P < 100(ϱ(λ))2, from the above discussion we see that the class of
Erd˝os-Rényi graphs GER(N, λ) has this property for each choice of ﬁxed λ > 1.
Exercise 18. Formally prove that the following classes do not have the small-world
property:
(a) The class of Erd˝os-Rényi random graphs GER(N, λ) with any ﬁxed λ < 1.
(b) The classes of graphs G1
NN(N, d) and G2
NN(N2, d) for any choice of ﬁxed d.
5
Small-World Networks
Small-world networks are classes of networks that have both the small-world
property and exhibit strong clustering. Here we describe two constructions of such
networks that are implemented in IONTW and study, both theoretically and with
simulation experiments, the structure of these networks. We also illustrate how this
structure inﬂuences effectiveness of certain vaccination strategies.
5.1
The Small-World Property and Small-World Networks
In the preceding two sections we studied the notions of strong clustering and of the
small-world property separately. Classes of networks that have both the small-world
property and exhibit strong clustering will be called small-world networks.
While many empirically studied contact networks appear to be small-world
networks [27], none of the classes of graphs that we have explored so far qualify.
For λ > 1 the class of Erd˝os-Rényi random graphs GER(N, λ) has the small-
world property but doesn’t exhibit strong clustering. On the other hand, as you saw

Vaccination Strategies for Small Worlds
249
in Sections 3.4 and 4.2, the classes of nearest neighbor networks G1
NN(N, d) and
G2
NN(N, d) with d > 1 exhibit strong clustering, but do not have the small-world
property.
In the next subsection we will construct speciﬁc examples of small-world
networks and call them small-world models. Readers should be aware that there
is no consensus about the usage of the terms “small-world property,” “small-world
network,” and “small-world model.” Different sources in the literature use different
mappings between these phrases and the three concepts. One always needs to
carefully read the deﬁnition in each source to ﬁgure out the intended meaning.
5.2
Small-World Models
In [16] we considered the monastic order of the Sisters of the Round Table.
The sisters spend most of their lives in their individual cells, where they devote
themselves to prayer and meditation. The only time they have contact with each
other is during meals when they are seated in a ﬁxed order around a giant round
table. Within this community, diseases can be transmitted only during mealtime.
The probability of transmission will be largest between sisters who sit next to
each other, and then decrease with the distance at the table. It may depend on the
particular nature of the disease how far the infectious agents can travel, and for
building a contact network we need to decide on a cutoff. Let us assume that there is
a signiﬁcant probability of transmission from sister i to sister j if at most d−1 sisters
sit in between. Then the relevant contact network for a community of N sisters will
be the one-dimensional nearest-neighbor graph G1
NN(N, d).
There is a problem though with our story: Even in a strictly monastic setting,
contact networks will rarely have such a rigid structure as G1
NN(N, d). The Sisters
will not necessarily head straight to the table from their cells. More likely, along the
way they will exchange a few kind words with their next-cell neighbors who may
be seated across the table. We can incorporate these more informal contacts into our
network model by adding a few randomly chosen edges to a graph G1
NN(N, d). This
will result in a class of networks that simultaneously exhibits strong clustering and
has the small-world property and that will serve as one of our small-world models.
Quite literally, the Sisters can have the best of both worlds!
The construction that we are describing here is a modiﬁcation of the networks
that Watts and Strogatz introduced in their seminal paper [27]. The main difference
is that the small-world models of [27] were constructed by randomly rewiring a
small fraction of the edges in networks G1
NN(N, d) rather than randomly adding new
edges. The modiﬁcation we are using here was proposed in [23]. It gives networks
that have similar properties to the ones studied by Watts and Strogatz in [27], but
are slightly easier to deﬁne and a lot easier to analyze mathematically.
Let us give a mathematically rigorous deﬁnition of our small-world networks.
We construct random graphs G = Gdim
SW (N, d, λ) for dim = 1, 2, d ≥1, and λ > 0
as follows:

250
W. Just and H.C. Highlander
•
V(G) = {1, . . . , N} or V(G) = {0, . . . , N −1} depending on how you prefer to
number nodes.
•
Let Gshort = Gdim
NN (N, d).
•
Randomly choose an Erd˝os-Rényi graph Glong = GER(N, λ).
•
E(G) = E(Gshort) ∪E(Glong).
Notice that for ﬁxed values of dim, d, λ and variable N we have deﬁned another
class of random graphs. The symbol G = Gdim
SW (N, d, λ) will denote both the entire
class and its instances, and we will rely on the context to point to the intended
meaning.
Since we use 4 parameters and need to distinguish between several types of
networks, the notation is a bit heavy on symbols here. But we can intuitively
think of Gdim
SW (N, d, λ) as having two types of edges: short edges that it inherits
from Gdim
NN (N, d) and long edges that it inherits from GER(N, λ). In the example of
the Sisters of the Round Table the short edges represent the contacts between sisters
that are seated close to each other at the table, and the long edges represent the more
informal contacts that they make in the hall with sisters from across the table. It is
possible that some edges will be simultaneously “short” and “long” in this sense;
we will always treat them as long ones. Incidentally, this distinction shows what our
small-world models have in common with real contact networks: The short edges
can be thought of as connections made in the local neighborhood, where one would
expect strong clustering, and the long edges as connections made by long-distance
travel, which accounts for the small-world property. In real contact networks there
would typically also be a lot of randomness in the short edges, but imposing a rigid
structure on this part of a network gives us models that are relatively easy to study.
Let us mention that one can easily generalize the deﬁnition given above by
starting with higher-dimensional versions of nearest-neighbor networks and/or
using different types of random networks (such as the scale-free networks of [11] or
[20]) instead of Erd˝os-Rényi networks GER(N, λ) for the long edges. Exploration of
the resulting small-world models is suggested in our research projects of Section 6.
As a warm-up, we recommend the following:
Challenge Problem 2. Rigorously deﬁne G = Gdim
SW (N, d, λ) for dim > 2.
5.3
Exploring Small Worlds
5.3.1
Small World Models in IONTW
Open IONTW, click Defaults, and change the parameter settings to match those in
Table 10. Click New. In the World window you will see a network G2
NN(100, 2),
which is a 10-by-10 rectangular grid with diagonals. Since we have set lambda =
0, this isn’t really a small-world model; it has no long edges whatsoever. But we
want to use it as a baseline for comparison. Click Metrics to record its properties
for future reference.

Vaccination Strategies for Small Worlds
251
Table 10 Settings for
exploring Small World
models.
num-nodes
d
λ
network-type
100
2
0
Small World 2
Now change lambda := 0.2 and press New again. This will create an instance
of G2
SW(100, 2, 0.2). You will see a few long edges in addition to the short ones.
Click Metrics, and repeat for lambda := 0.4, 0.6. 0.8. Be sure to click Metrics after
creation of each network.
You will have seen more and more long edges appearing as you increased
lambda. Let us see what their addition did to the network properties. Enlarge the
Command Center by clicking on the double-arrow icon and look at the data. Most
likely you will observe that as the parameter λ increases in increments of 0.2, the
following changes in the characteristic properties of G2
SW(100, 2, λ) occur:
•
The mean degree ⟨k⟩increases; roughly in increments of 0.2.
•
The edge density slightly increases.
•
The (normalized) network clustering coefﬁcients decrease.
•
The mean distance decreases.
•
The diameter may decrease.
Your data may not give a clear-cut picture due to the inherent randomness, but
the general pattern should be discernible.
Let us look at a larger network. Set num-nodes := 400. Repeat the previous
experiment for lambda := 0, 0.8.
You may observe that although the mean degree increases by only about 10%, in
G2
SW(400, 2, 0.8) the mean distance and the diameter are less than half of what they
were in G2
SW(400, 2, 0).
How could adding a few random long edges have such a large effect? This
certainly seems puzzling. In Subsection 2.1 of our module Small-world models [7]
you will ﬁnd a sequence of explorations with IONTW that illustrate in detail the
mechanism behind this phenomenon.
5.3.2
Mathematical Derivations of Some Properties of Gdim
SW(N, d, λ)
We will focus on three network properties here: The mean degree ⟨k⟩, the normal-
ized network-clustering coefﬁcient Cnorm, and the median distance between any two
nodes. Moreover, in order to sidestep some distracting technicalities, for the two-
dimensional case we will mostly focus only on networks G2
SW(N2, d, λ), although
the results generalize to small-world models based on rectangular rather than square
grids, many of which are implemented in IONTW as G2
SW(N, d, λ). Let us begin
with a simpliﬁed version of Exercise 3 of [7].
Exercise 19. Find approximations to the mean degrees in G1
SW(N, d, λ) and in
G2
SW(N2, 1, λ) that a.a.s. approach the true mean degrees ⟨k⟩when N →∞.

252
W. Just and H.C. Highlander
We have optimistically labeled the graphs Gdim
SW (N, d, λ) “small-world models.”
But do they live up to this name? We will need to show that our two classes of
networks are small-world networks, that is, exhibit strong clustering and have the
small-world property.
Strong clustering requires that normalized network clustering coefﬁcients Cnorm
will grow without bounds when the network size increases without bound while the
other two parameters, d and λ, remain ﬁxed. By Exercise 12(b) of Section 3.4, the
graphs G1
NN(N, d) and G2
NN(N2, d) with d > 1 have the property that tr(i) ≥1
for each node i, and the same will continue to hold in the corresponding small-
world networks G1
SW(N, d, λ) and G2
SW(N2, d, λ). Moreover, by Exercise 19 above,
for each of these classes the expected mean degree ⟨k⟩will a.a.s. approach some
ﬁnite number that depends on the particular choice of d, λ. Thus each of the classes
G1
NN(N, d, λ) and G2
NN(N2, d, λ) with ﬁxed d > 1 and λ satisﬁes the assumptions of
Theorem 2 of Section 3.4, and we can conclude that each of these classes exhibits
strong clustering.
Exercise 20.
(a) Consider Gdim
SW (N, d, λ), where the parameters dim, N, d are ﬁxed, d > 1, and
the network size N is very large. Explain why you usually will see a decrease in
the normalized network clustering coefﬁcient Cnorm when you increase the value
of λ.
(b) What behavior of Cnorm would you expect in the setup for part (a) when you set
d = 1? How would you explain the predicted differences from part (a)?
Finally, let us consider the small-world property. We need to show that for some
constants P and c, the P-th percentile of the distance between two randomly chosen
nodes will a.a.s. be less than c ln(N).
If λ > 1, then the graph Glong = GER(N, λ) is predicted to a.a.s. contain a giant
component of diameter < cdiam ln(N) for some constant cdiam. Thus a.a.s., if i, j are
nodes in the giant component of Glong = GER(N, λ), there will be a path of length
< cdiam ln(N) from i to j. Since each such path is also a path in Gdim
SW (N, d, λ), the
small-world property of the latter class of networks follows.
However, if 0 < λ < 1, then the class of graphs GER(N, λ) does not have the
small-world property. But our small-world models do. Even if λ is very small but
positive, we will still be able to travel from any node i to any node j along a path of
very small length. This path may need to contain both short and long edges though.
The following exercise gives important insights into the structure of typical
small-world networks. A hint is given in the Appendix, before the references, but
ﬁrst try doing the problem without a hint.
Exercise 21. Give an intuitive explanation why for any choice of d and of λ > 0
the class of networks G1
SW(N, d, λ) should have the small-world property.
The following problem will be good practice for the projects of Section 6; we
recommend starting with the case dim = 1.

Vaccination Strategies for Small Worlds
253
Challenge Problem 3. Give a formal proof that for any choice of d, dim and of
λ > 0 the class of networks Gdim
SW (N, d, λ) has the small-world property.
5.4
Vaccination Strategies in Small-World Models
In order to avoid some technical complications, throughout this subsection and
Section 6 we will focus only on models of type SIR. For these models, vaccination
can be interpreted as moving a subset of all nodes into the R-compartment prior
to any outbreak so that these nodes will no longer be susceptible. A vaccination
strategy is a procedure for choosing the set of hosts to be vaccinated.
Vaccination is an important control measure; its goal is to reduce to the extent
possible the number of hosts who will experience infection during a possible
outbreak. Thus the effectiveness of a given vaccination strategy can be quantiﬁed
either in terms of how much it reduces the probability that a subsequent outbreak
caused by a single index case will be a major one, or how much it reduces the
expected ﬁnal size.
Of course, a strategy of vaccinating the entire population would prevent all
outbreaks. But such a strategy may not always be feasible: Not enough vaccine
may be available, logistical problems may prevent administering the vaccine to all
hosts, or some hosts may be unwilling to comply.
A more realistic approach may be to vaccinate xN randomly chosen hosts, where
x is a proportion with 0 < x < 1. These random vaccination strategies are in fact the
only ones that can be studied when all hosts are assumed to have identical properties
and the uniform mixing assumption is made. Recall that in network-based models
this assumption is made when the contact network is a complete graph. For models
with other given contact networks one can consider targeted vaccination strategies,
where a particular subset of all hosts is chosen to receive the available doses of
vaccine. We will explore one such strategy later in this subsection and ask you to
investigate more of them in some of the projects.
An important prediction of compartment-level models is that random vaccination
of xN nodes will guarantee that all outbreaks are restricted to minor ones as long as
x ≥HIT, where the critical proportion HIT is called the herd immunity threshold
and is given by
HIT = K
N = 1 −1
R0
.
(12)
For example, when R0 = 3.6, then HIT = 1 −
1
R0 = 1 −
1
3.6 = 0.7222. For
a population of N = 120 hosts, we would need to administer vaccine to at least
72.22% or 87 hosts.
We now turn to IONTW to investigate the role of HIT in simulated outbreaks
using a few exercises adapted from Project 9.1 of [17] and Section 9.4.3 of [17].
Open IONTW, click Defaults, and change the parameter settings according to
Table 11.

254
W. Just and H.C. Highlander
Table 11 Parameter settings
for investigating HIT in
compartment-level models.
infection-prob
end-infection-prob
num-nodes
0.03
1
120
Note that here we are simulating a next-generation SIR-model with N = 120
hosts. Press New followed by Metrics to verify R0 = 3.57, which we will round to
R0 = 3.6.
What will happen if we vaccinate K = 87 hosts? To vaccinate hosts (aka “turtles”
in IONTW), in the command center next to observer> type:
ask n-of 87 turtles [become-removed]
Press enter, then type (all in one line):
ask n-of 1 turtles with [susceptible?]
[become-infectious]
Press enter and then Go and investigate the severity of the outbreak. Press Last and
then Go to run a few more trials. What do you observe? Is the population avoiding
major outbreaks?
We can reach more reliable conclusions by running a batch experiment.
Set up a New experiment with 100 repetitions and use the following:
Reporters:
count turtles with [removed?]
Setup commands:
new-network
ask n-of 87 turtles [become-removed]
ask n-of 1 turtles with [susceptible?]
[become-infectious]
Exercise 22.
(a) Run the experiment and analyze your data by sorting the output column
(count turtles with [removed?]) from lowest to highest, as you did
in Exercise 5. For each run, calculate the proportion of unvaccinated hosts who
are not the index case and who experienced infection. Remember that your
output column will include the number of vaccinated hosts and the index case,
so you will need to account for this in your calculations.
Do your results indicate that the population of unvaccinated hosts avoided a
major outbreak every time? Should you have vaccinated more individuals to be
safe? What would you predict would happen if you vaccinated fewer hosts?

Vaccination Strategies for Small Worlds
255
(b) Redo part (a) by vaccinating 1.2 ∗87 and 0.8 ∗87 hosts, respectively. (Note:
Round up to the nearest integer.) Compare your results to the mean and
maximum proportions of unvaccinated hosts who experienced infection in part
(a). Also compare the proportions of simulations where only one host (the index
case) experiences infection. This means that no secondary infections occurred.
(c) Is there an equal effect in increasing the proportion of vaccinated hosts from
0.8 ∗HIT to HIT as there is in increasing the proportion from HIT to 1.2 ∗HIT?
What does this mean in terms of recommendations for public health policy?
Next we turn to a different type of contact network, where we will consider
targeted vaccination strategies. In such networks we might be able to prevent major
outbreaks by vaccinating a small number of hosts positioned at key locations in the
network. A major challenge lies in determining this optimal subset of hosts.
In IONTW click Defaults and change the parameter settings according to
Table 12.
Here we are simulating a next-generation SIR-model on a G1
NN(120, 2) contact
network. Press New then Metrics to verify R0 = 3.6, as in our previous example
on a complete graph. Do you expect we will need to vaccinate the same number of
hosts to obtain herd immunity? The next exercise will assist you in answering this
question.
Exercise 23. Set up a New experiment with 100 repetitions and enter the exact
same prompts that preceded Exercise 22. Repeat all steps and answer all questions
from part (a) of Exercise 22. What differences do you notice once your contact
network has changed, even though your R0 value is the same?
Suppose now that we have only a limited amount of vaccine so that we are only
able to vaccinate 10% of the population. In our population of 120 hosts, this amounts
to vaccinating 12 individuals. What would happen in the compartment-level model
(i.e., a complete graph contact network) if we only vaccinated 12 hosts? What do
you think would happen in the case of the G1
NN(120, 2) contact network? Let’s run
some more simulations. Select your previous batch processing experiments from
Exercises 22 and 23 and Edit each one by selecting a different Experiment name.
For each of these two experiments, edit the second line of Setup commands as
follows:
ask n-of 12 turtles [become-removed]
so that you are now only vaccinating 12 hosts instead of 87.
Table 12 Parameter settings for investigating HIT in G1
NN(N, d) models.
infection-prob
end-infection-prob
d
network-type
0.9
1
2
Nearest-neighbor 1

256
W. Just and H.C. Highlander
Exercise 24. Repeat the instructions in Exercises 22 and 23. Have your predic-
tions for the compartment-level model changed when the contact network was
G1
NN(120, 2)?
The results of the preceding exercise suggest that for a given class C of network-
based models that has representatives with arbitrarily large N, there might be a
proportion HITC such that if random vaccination is applied to more than xN hosts,
then for any ﬁxed x:
•
If x > HITC, then the expected ﬁnal size and the probability of a major outbreak
will a.a.s. approach 0.
•
If x < HITC, then the expected ﬁnal size and the probability of a major outbreak
will a.a.s. exceed some ﬁxed values r(x) with probability 1 −z∞(x) > 0.
When deﬁning the class C, one needs to specify the network type, the model
type, and the relevant network and disease transmission parameters, which may
depend on N. The classical result about the herd-immunity threshold says that if the
networks in C are complete graphs and the disease transmission parameters depend
on N in such a way that R0 remains ﬁxed for all networks in this class, then HITC
exists and is given by (12).
Under compartment-level models, we have no choice other than to vaccinate
hosts at random. However, perhaps with the G1
NN(120, 2) contact network we can
do better. In IONTW, to vaccinate a speciﬁc subset of hosts, it is most convenient to
create a .txt ﬁle containing a list of the hosts you wish to vaccinate. For example
if you wanted to vaccinated hosts 1, 2, 80, the only text your ﬁle should contain is:
[ 1 2 80 ]
Create such a ﬁle in the same directory where you are running IONTW, name it
vaccinate.txt. In the command line, next to observer> type:
ask turtles-from-file "vaccinate.txt" [become-removed]
Hit enter and then click Labels to ensure your ﬁle has indeed vaccinated the
intended hosts. Once you have veriﬁed your ﬁle is working properly, select
your previous batch processing experiment and Edit it by selecting a different
Experiment name. Then edit the second line of Setup commands as follows:
ask turtles-from-file "vaccinate.txt" [become-removed]
Such an experiment will now vaccinate only turtles labeled 1, 2, and 80.
Exercise 25. Now return to our problem of having only enough vaccine for
12 hosts. What would you consider to be the best vaccination strategy for our
G1
NN(120, 2) contact network? Edit your "vaccinate.txt" ﬁle according to

Vaccination Strategies for Small Worlds
257
Table 13 Parameter settings for testing vaccination strategy.
infection-prob
end-infection-prob
network-type
num-nodes
lambda
d
0.9
1
Small World 1
120
0
2
your proposed strategy, and run another batch experiment similar to that of
Exercise 23, but now with your 12 selected hosts to vaccinate. Compare your results
to previous results in terms of the minimum, maximum, and mean proportions of
hosts who experienced infection. Remember to account for the 12 individuals that
start in the Removed compartment in calculating the correct proportion. Note: It
is recommended that you try several vaccination strategies to determine which is
optimal.
Note that the contact network in the above explorations is the limiting case
G1
NN(120, 2) of G1
SW(120, 2, λ) for λ = 0. The performance of your vaccination
strategy will deteriorate when we increase λ. Before proceeding with an exploration
of this effect, to minimize the possibility of error, verify that your vaccination
strategy is equivalent to the one we propose in the Appendix hints.
Open IONTW, click Defaults, and change the parameter settings according to
Table 13. This will set up a next-generation SIR-model on a network G1
NN(120, 2).
Press New followed by Metrics, and look up the value of R0 for this model in the
Command Center. Convince yourself that it is R0 = 3.6.
Create a plain text ﬁle that contains your vector of hosts to be vaccinated as a
single line and save it under the name vaccinate.txt in the same directory in
which you are running IONTW.
To get a set of baseline data, run a batch of 100 simulations for this vaccination
strategy using the same set of batch processing commands as in Exercise 25.
Exercise 26. Run the experiment and analyze your output ﬁle. Record the mini-
mum, maximum, and mean number of removed nodes at the end. Also record the
numbers of runs where you found exactly 30, 48, 66, 84, 102, and 120 hosts who
experienced infection.
Now Edit your setup for batch processing by setting
["lambda" 0.05]
and specifying a new suggestive output ﬁle name. Run the experiment. Then Edit
your setup for batch processing again by setting
["lambda" 0.1]
and specifying another suggestive output ﬁle name. Run the experiment.
Exercise 27.
(a) Analyze your output ﬁles as you did for Exercise 26.

258
W. Just and H.C. Highlander
(b) Verbally summarize your ﬁndings. How does the parameter λ appear to
inﬂuence disease transmission? Does the strategy of vaccinating evenly spaced
groups of two adjacent hosts provide as good protection when the contact
network is G1
SW(120, 2, λ) as it does for contact networks G1
NN(120, 2)? Does
it provide better protection than randomly vaccinating 12 hosts?
(c) Find an explanation for the high frequencies with which you observed outcomes
of 30, 48, 66, 84, 102, or 120 hosts who experienced infection. How is this
pattern related to the structure of the network and the disease transmission
parameters?
6
Suggested Research Projects
Now that you have learned about disease transmission on small-world models,
vaccination strategies, and done a number of exercises, you are ready to attempt
some research-level projects. We describe ﬁve such projects here. Since these are
open research problems, we don’t know how difﬁcult it would be to arrive at a
complete answer to all parts of a project. However, we know that at least some parts
of each of these projects are feasible for aspiring researchers at the undergraduate
level. And even substantial partial answers are likely to go beyond what is already
known in the literature! In all cases, the best strategy would be to start with
simulations to form conjectures, and then to try ﬁnding rigorous mathematical
proofs for your conjectures. And don’t forget to consult the vast and rapidly growing
literature on the subject to see whether a solution to your project has been published
in the meantime!
We formulated all our projects for small-world networks of arbitrary dimension,
but they are of interest even if you restrict yourself just to the case dim = 1. This
case is the one that you want to always start with. The answers to the questions
in our projects will in general depend on whether you explore them for discrete-
time or continuous-time models and on the particular choice of disease-transmission
parameters. We recommend that you always start with exploring next-generation
models. For these models, the simulations run fast, and they are easier to analyze
mathematically. The simulations for the ﬁrst three projects (at least for dimensions
one and two) can be done with the current version of IONTW. The simulations
for last two projects require some software development, for example, a suitable
extension of the current capabilities of our NETLOGO code.
Research Project 1.
Explore the herd immunity threshold HITC for classes
C of models that are based on contact networks Gdim
SW (N, λ, d), where N is
arbitrarily large, while dim, λ, d and all disease transmission parameters are
ﬁxed.

Vaccination Strategies for Small Worlds
259
In particular, for Research Project 1 you want to:
(a) Explore numerically how HITC depends on λ and d for dim = 1, 2 and
representative choices of disease transmission parameters.
(b) Prove mathematically that HITC actually exists.
(c) Conjecture an approximate formula for the dependence of HITC in terms of the
deﬁning parameters of C, and prove it.
Project 1 assumes that individuals to be vaccinated are chosen randomly. The
next project aims at comparing the effectiveness of two kinds of targeted strategies.
Research Project 2.
Assume you have enough vaccine to vaccinate a ﬁxed
proportion x (where 0 < x < 1) of all hosts. Explore two types of targeted
vaccination strategies, B(x) and H(x), as described below.
In particular, for Research Project 2 you want to:
(a) Rigorously formulate a deﬁnition of the vaccination strategy B(x) that allows
for creating as many evenly spaced barriers in Gdim
SW (N, d, λ) with the available
amount of vaccine. Note that this strategy works in a similar way as the strategy
provided in the hint for Exercise 27.
(b) Rigorously formulate a deﬁnition of the vaccination strategy H(x) that allows
for vaccinating the nodes with the highest degrees with the available amount of
vaccine.
(c) For these strategies, explore with simulations with various values of dim, d, λ
and the disease transmission parameters the expected value of the ﬁnal size and
probabilities of major outbreaks. In particular, explore when B(x) appears to be
the better strategy and when H(x) outperforms it.
(d) Formulate mathematical conjectures based on your ﬁndings in point (c) and
rigorously prove them.
As far as we know, at the time of this writing there is no literature on the
effectiveness of B(x). However, some numerical explorations of the effectiveness
of strategy H(x) for one-dimensional small-world models have been reported in
[30]. For two-dimensional small-world models with d = 1 this strategy (along with
several additional ones) has also been explored numerically in [5] and for d = 2 in
[29]. However, the small-world models studied in these papers are based on rewiring
a small proportion of nodes rather than adding new random connections.
In Project 2 we assumed that we can in fact precisely target the set of hosts
to be vaccinated. In practice this may not be possible, due to logistic difﬁculties in
administering the vaccine or due to the growing problem of unwillingness to comply
with a vaccination program. Several of the articles in [22] discuss recent research
on this problem.

260
W. Just and H.C. Highlander
Note that in the case of strategy B(x) this may create some “leaky barriers,” and
in the case of strategy H(x) you may fail to vaccinate some highly connected hosts.
Research Project 3.
Redo the explorations of Research Project 2 under the
additional assumption that only a proportion of y randomly chosen hosts
among those you target for vaccination can in fact be vaccinated.
In particular, for Research Project 3 you want to:
(a) Rigorously formulate a deﬁnition of the vaccination strategy B(x, y) that allows
for creating as many evenly spaced barriers in Gdim
SW (N, d, λ) with the available
amount x of vaccine.
(b) Rigorously formulate a deﬁnition of the vaccination strategy H(x, y) that allows
for vaccinating the nodes with the highest degrees with the available amount x
of vaccine.
(c) Explore the strategies B(x, y) and H(x, y) as suggested for B(x), H(x) in the
instructions for Research Project 2.
There are other types of small-world models than the ones we introduced
here. For example, the random edges can be created by randomly rewiring some
connections as in [27], or be modeled as some other random graph, such as a random
scale-free network (see [11,20]).
Research Project 4.
Do an analogue of any of Research Projects 1–3
for your favorite alternative type of small-world model and compare the
ﬁndings with those for the small-world models that are already implemented
in IONTW.
In all of the above projects we have assumed that the network connectivity is
ﬁxed. But in the real world contact networks change over time. People may break
old connections and form new ones. This will happen slowly and independently
of any disease outbreak, but the process may be accelerated during an ongoing
outbreak, for example, when diseased hosts isolate themselves (thus breaking all
their connections), or people respond to the news of overall disease prevalence by
traveling less (thus breaking some of their long-distance connections). The study of
such responses to an outbreak is the domain of behavioral epidemiology; see [22]
for an overview of recent research in this ﬁeld.

Vaccination Strategies for Small Worlds
261
Research Project 5.
Redo your favorite among Research Projects 1–3 for a
model with rewiring.
In particular, for Research Project 5 you want to:
(a) Rigorously formulate a model of how a network gets rewired over time
according to your chosen scenario.
(b) Implement a simulation program that would represent the rewiring process you
formulated in point (a) when you start with a small-world model. This might be
done as an extension of IONTW.
(c) Redo the explorations for your favorite among Research Projects 1–3 for this
implementation and compare the ﬁndings with those for the underlying small-
world models without rewiring.
The projects we have listed here are representative of what can be explored along
these lines, but they are far from exhausting all possibilities. If you stumble upon a
question in this area that seems worth exploring with simulations and theoretically,
by all means, do explore them, and let us know about your ﬁndings!
Appendix: Hints for Selected Exercises
Hint for Exercises 1 and 2 In Exercise 1, the large components will always be of
the same size; in Exercise 2, their sizes will most likely be similar, but not exactly
the same.
Hint for Exercise 3 If you followed instructions on batch processing from [13],
choosing the output option Table output, the data you will need (total number of
removed turtles) should be in the rightmost column of your output ﬁle. You may
wish to sort your data in this column from lowest to highest. All hosts who are
removed by the end of the simulation must be in the same connected component as
the index case j∗. In the majority of simulations, these components should be large,
as in Exercise 2.
Hint for Exercise 4 You can use the mean size of the components that you
classiﬁed as “large” for your estimate of ϱ(1.5).
Hint for Exercise 12(a) Choose an N large enough so that you can test several
values of d. Find a pattern, dependent on d, for the maximum degree of any node.

262
W. Just and H.C. Highlander
Hint for Exercise 15 The results of Milgram’s experiment do not tell us anything
about the maximum or mean distance. They give a statistically signiﬁcant estimate
of some percentile of the distances, but not necessarily of the median.
Hint for Exercise 21 The mean degrees will a.a.s. approach ⟨k⟩≈2d + λ in
G1
SW(N, d, λ) and ⟨k⟩≈4 + λ in G2
SW(N2, 1, λ).
Hint for Exercise 23(a) Partition the vertex set V of the small-world model into
pairwise disjoint sets of consecutively numbered nodes Vi of roughly equal size.
Then form a new graph GI by making each of the sets Vi a vertex and drawing an
edge {Vi, Vj} if, and only if, there is at least one v∗∈Vi and at least one v∗∗∈
Vj such that {v∗, v∗∗} forms an edge in the original small-world network. For an
illustration of this construction see the hint at the very end of Module [7].
It can be shown that (a slight modiﬁcation of) this construction will give Erd˝os-
Rényi random graphs GI with sufﬁciently large mean degrees.
Hint for Exercise 24
(a) You may still see a few outbreaks that look fairly major, which is due to random
effects in a relatively small population.
(b) Results will vary, but you may see results similar to the following:
(rounded to 2 decimals)
K = 70
K = 87
K = 105
mean prop. of secondary infections
0.35
0.10
0.04
max. prop. of secondary infections
0.86
0.67
0.47
prop. of runs with no secondary infections
0.17
0.37
0.67
Hint for Exercise 25 Note: Do NOT duplicate your experiment from Exercise
24, as this will change the parameter settings. Instead, create a New experiment.
It is highly unlikely to see any major outbreaks. We got: mean prop. of secondary
infections = 0.04; max prop. of secondary infections = 0.15; prop. of runs with no
secondary infections = 0.31. Your results should be in the same ballpark.
Hint for Exercise 26 If you predicted you will still see major outbreaks, you would
be correct. For the complete network, we got: mean prop. of secondary infections =
0.90; max prop. of secondary infections = 0.97; and prop. of runs with no secondary
infections = 0.05. For the G1
NN(120, 2) network, we got: mean prop. of secondary
infections = 0.65; max prop. of secondary infections = 0.99; and prop. of runs with
no secondary infections = 0. Your results may be similar. After sorting the results
from the lowest to the highest value of the output column, you will most likely
observe a dramatic separation between major and minor outbreaks for the complete
network, and a gradual increase without a distinctive gap between minor and major
outbreaks for the G1
NN(120, 2) network.

Vaccination Strategies for Small Worlds
263
Hint for Exercise 27 The optimal strategy would create barriers that the pathogens
cannot cross by vaccinating evenly spaced groups of two adjacent hosts. One vector
that implements this type of strategy is
[ 1 2 21 22 41 42 61 62 81 82 101 102 ].
References
1. Bollobás, B.: Random Graphs, 2nd edn. Cambridge University Press, Cambridge (2001)
2. Diekmann, O., Heesterbeek, H., Britton, T.: Mathematical Tools for Understanding Infectious
Disease Dynamics. Princeton University Press, Princeton (2012)
3. Draief, M., Massoulié, L.: Epidemics and Rumors in Complex Networks. London Mathemati-
cal Society Lecture Note Series, vol. 369. Cambridge University Press, Cambridge (2010)
4. Erd˝os, P., Rényi, A.: On the evolution of random graphs. Selected Papers of Alfréd Rényi. 2,
482–525 (1976)
5. Hartvigsen, G., Dresch, J., Zielinski, A., Macula, A., Leary, C.: Network structure, and
vaccination strategy and effort interact to affect the dynamics of inﬂuenza epidemics. J. Theor.
Biol. 246(2), 205–213 (2007)
6. Just, W.: A brief review of basic probability theory. https://qubeshub.org/resources/366
7. Just, W., Callender, H.: Small-world models. https://qubeshub.org/resources/404
8. Just, W., Callender, H., LaMar, M.D.: Clustering coefﬁcients. https://qubeshub.org/resources/
406
9. Just, W., Callender, H., LaMar, M.D.: Exploring distances with IONTW. https://qubeshub.org/
resources/382
10. Just, W., Callender, H., LaMar, M.D.: Exploring Erd˝os-Rényi random graphs with IONTW.
https://qubeshub.org/resources/370
11. Just, W., Callender, H., LaMar, M.D.: Exploring generic scale-free networks. https://qubeshub.
org/resources/407
12. Just, W., Callender Highlander, H., LaMar, M.D.: Exploring transmission of infectious diseases
on networks with NETLOGO. https://people.ohio.edu/just/IONTW/
13. Just, W., Callender, H., LaMar, M.D.: How to use these modules. https://qubeshub.org/groups/
iontw/File:InstructionsQ.pdf
14. Just, W., Callender, H., LaMar, M.D.: Modules for exploring transmission of infectious
diseases on networks with NETLOGO. https://qubeshub.org/groups/iontw/iontwmodules
15. Just, W., Callender, H., LaMar, M.D.: Network-based models of transmission of infectious
diseases: a brief overview. https://qubeshub.org/resources/363
16. Just, W., Callender, H., LaMar, M.D.: Disease transmission dynamics on networks: Network
structure vs. disease dynamics. In: Robeva, R. (ed.) Algebraic and Discrete Mathematical
Methods for Modern Biology, pp. 217–235. Academic, New York (2015)
17. Just, W., Callender, H., LaMar, M.D.: Online appendix: Disease transmission dynamics on
networks: Network structure vs. disease dynamics. In: R. Robeva (ed.) Algebraic and Discrete
Mathematical Methods for Modern Biology. Academic, New York (2015). http://booksite.
elsevier.com/9780128012130/content/Chapter09_Appendix.pdf
18. Just, W., Callender, H., LaMar, M.D., Toporikova, N.: Transmission of infectious diseases:
Data, models, and simulations.
In: R. Robeva (ed.) Algebraic and Discrete Mathematical
Methods for Modern Biology, pp. 193–215. Academic Press (2015)
19. Just, W., Callender, H., LaMar, M.D., Xin, Y.: Exploring random regular graphs with IONTW.
https://qubeshub.org/resources/372
20. Just, W., Callender, H., LaMar, M.D., Xin, Y.: The preferential attachment model.
https://qubeshub.org/resources/384
21. Just, W., Xin, Y.: A quick tour of IONTW.
https://qubeshub.org/resources/201

264
W. Just and H.C. Highlander
22. Manfredi, P., D’Onofrio, A.: Modeling the Interplay Between Human Behavior and the Spread
of Infectious Diseases. Springer Science & Business Media, New York (2013)
23. Newman, M.E., Watts, D.J.: Renormalization group analysis of the small-world network
model. Phys. Lett. A 263(4), 341–346 (1999)
24. Spencer, J.: The giant component: the golden anniversary. Not. Am Math. Soc. 57(6), 720–724
(2010)
25. Travers, J., Milgram, S.: An experimental study of the small world problem. Sociometry 32(4),
425–443 (1969)
26. Vynnycky, E., White, R.: An Introduction to Infectious Disease Modelling. Oxford University
Press, Oxford (2010)
27. Watts, D.J., Strogatz, S.H.: Collective dynamics of ‘small-world’ networks. Nature 393(6684),
440–442 (1998)
28. Wilensky, U.: Netlogo home page. http://ccl.northwestern.edu/netlogo/
29. Xu, Z., Sui, D.Z.: Effect of small-world networks on epidemic propagation and intervention.
Geogr. Anal. 41(3), 263–282 (2009)
30. Zanette, D.H., Kuperman, M.: Effects of immunization in small-world epidemics. Physica A
309(3), 445–452 (2002)

Steady and Stable: Numerical Investigations of
Nonlinear Partial Differential Equations
R. Corban Harwood
Suggested Prerequisites. Differential equations, Linear algebra, Some program-
ming experience.
1
Introduction
Mathematics is a language which can describe patterns in everyday life as well as
abstract concepts existing only in our minds. Patterns exist in data, functions, and
sets constructed around a common theme, but the most tangible patterns are visual.
Visual demonstrations can help undergraduate students connect to abstract concepts
in advanced mathematical courses. The study of partial differential equations, in
particular, beneﬁts from numerical analysis and simulation.
Applications of mathematical concepts are also rich sources of visual aids to
gain perspective and understanding. Differential equations are a natural way to
model relationships between different measurable phenomena. Do you wish to
predict the future behavior of a phenomenon? Differential equations do just that–
after being developed to adequately match the dynamics involved. For instance,
say you are interested in how fast different parts of a frying pan heat up on the
stove. Derived from simplifying assumptions about density, speciﬁc heat, and the
conservation of energy, the heat equation will do just that! In Section 2.1 we use
the heat equation (6), called a test equation, as a control in our investigations
of more complicated partial differential equations (PDEs). To clearly see our
predictions of the future behavior, we will utilize numerical methods to encode
the dynamics modeled by the PDE into a program which does basic arithmetic to
approximate the underlying calculus. To be conﬁdent in our predictions, however,
R.C. Harwood ()
George Fox University, 414 North Meridian Street, Newberg, OR 97132, USA
e-mail: rharwood@georgefox.edu
© Springer International Publishing AG 2017
A. Wootton et al. (eds.), A Primer for Undergraduate Research, Foundations for
Undergraduate Research in Mathematics, DOI 10.1007/978-3-319-66065-3_11
265

266
R.C. Harwood
we need to make sure our numerical method is developed in a way that keeps
errors small for better accuracy. Since the method will compound error upon
error at every iteration, the method must manage how the total error grows in a
stable fashion. Else, the computed values will “blow up” towards inﬁnity–becoming
nonnumerical values once they have exceeded the largest number the computer can
store. Such instabilities are adamantly avoided in commercial simulations using
adaptive methods, such as the Rosenbrock method implemented as ode23s in
MATLAB [17]. These adaptive methods reduce the step size as needed to ensure
stability, but in turn increase the number of steps required for your prediction.
Section 2 gives an overview of numerical partial differential equations. Burden [3]
and Thomas [22] provide great beginner and intermediate introductions to the topic,
respectively. In Section 3 we compare basic and adaptive methods in verifying
accuracy, analyzing stability through fundamental deﬁnitions and theorems, and
ﬁnish by tracking oscillations in solutions. Researchers have developed many ways
to reduce the effect of numerically induced oscillations which can make solutions
appear infeasible [2,19]. Though much work has been done in studying the nature
of numerical oscillations in ordinary differential equations [4,9], some researchers
have applied this investigation to nonlinear evolution PDEs [11, 15]. Recently,
others have looked at the stability of steady-state and traveling wave solutions to
nonlinear PDEs [10,16,18], with more work to be done. We utilize these methods
in our parameter analysis in Section 4 and set up several project ideas for further
research. Undergraduate students have recently published related work, for example,
in steady-state and stability analysis [1, 20] and other numerical investigations of
PDEs [13].
2
Numerical Differential Equations
In applying mathematics to real-world problems, a differential equation can encode
information about how a quantity changes in time or space relative to itself more
easily than forming the function directly by ﬁtting the data. The mass of a bacteria
colony is such a quantity. In this example, tracking the intervals over which the
population’s mass doubles can be related to measurements of the population’s mass
to ﬁnd its exponential growth function. Differential equations are formed from
such relationships. Finding the pattern of this relationship allows us to solve the
differential equation for the function we seek. This pattern may be visible in the
algebra of the function, but can be even more clear in graphs of numerical solutions.
2.1
Overview of Differential Equations
An ordinary differential equation (ODE) is an equation involving derivatives of a
single variable whose solution is a function which satisﬁes the given relationship
between the function and its derivatives. Because the integration needed to undo
each derivative introduces a constant of integration, conditions are added for each

Numerical Investigations of Nonlinear PDEs
267
derivative to specify a single function. The order of a differential equation is the
highest derivative in the equation. Thus, a ﬁrst order ODE needs one condition while
a third order ODE needs three.
Deﬁnition 1. An initial value problem (IVP) with a ﬁrst order ODE is deﬁned as
dx
dt = f(x, t)
(1)
x(t0) = x0,
where t is the independent variable, x ≡x(t) is the dependent variable (also called
the unknown function) with initial value of x(t0) = x0, and f(x, t) is the slope
function.
As relationships between a function and its derivatives, a PDE and an ODE
are much alike. Yet PDEs involve multivariable functions and each derivative is
a partial derivative in terms of one or more independent variables. Recall that a
partial derivative focuses solely on one variable when computing derivatives. For
example, ∂
∂te−2t sin(3x) = −2e−2t sin(3x). Similar to the ways ordinary derivatives
are notated, partial derivatives can be written in operator form or abbreviated
with subscripts (e.g. ∂2u
∂x2 = uxx). Linear PDEs are composed of a sum of scalar
multiples of the unknown function, its derivatives, as well as functions of the
independent variables. A PDE is nonlinear when it has term which is not a scalar
multiple of an unknown, such as ρu(1 −u) in (3) or an arbitrary function of the
unknown. To introduce problems involving PDEs, we begin with the simplest type
of boundary conditions, named after mathematician Peter Dirichlet (1805–1859),
and a restriction to ﬁrst order in time (called evolution PDEs). Note that the number
of conditions needed for a unique solution to a PDE is the total of the orders in each
independent variable [22]. Sufﬁcient number of conditions, however, does not prove
uniqueness. The maximum principle and energy method are two ways uniqueness
of a solution can be proven [6], but such analysis is beyond the scope of this chapter.
Deﬁnition 2. An initial boundary value problem (IBVP) with a ﬁrst order (in time)
evolution PDE with Dirichlet boundary conditions is deﬁned as
∂u
∂t = f

x, t, u, ∂u
∂x, ∂2u
∂x2 , . . .

(2)
u(x, 0) = u0(x),
u(0, t) = a
u(L, t) = b
where x, t are the independent variables, u is the dependent variable (also called the
unknown function) with initial value of u(x, t) = u0(x) and boundary values u =
a, b whenever x = a, b respectively, and f can be any combination of independent
variables and any spatial partials of the dependent variable.

268
R.C. Harwood
Example 1. Let us analyze the components of the following initial boundary value
problem:
ut = δuxx + ρu(1 −u),
(3)
u(x, 0) = u0(x),
u(0, t) = 0
u(10, t) = 1
First, the PDE is nonlinear due to the u(1 −u) term. Second, the single initial
condition matches the 1st order in time (ut) and the two boundary values match
the 2nd order in space (uxx). Thus, this IBVP has sufﬁcient number of conditions
needed for a unique solution which supports but does not prove uniquess. Third,
parameters δ, ρ and the initial proﬁle function u0(x) are kept unspeciﬁed.
This reaction-diffusion equation is known as the Fisher-KPP equation for the
four mathematicians who all provided great analytical insight into it: Ronald Fisher
(1890–1962), Andrey Kolmogorov (1903–1987), Ivan Petrovsky (1901–1973), and
Nikolaj Piscounov (1908–1977) [7, 14]. Though it is more generally deﬁned as
an IVP, in this chapter we study it in its simpler IBVP form. Coefﬁcients δ, ρ
represent the diffusion and reaction rates and varying their values lead to many
interesting behaviors. The Fisher-KPP equation models how a quantity switches
between phases, such as genes switching to advantageous alleles where it was
originally studied [7].
The form of the initial condition function, u0(x), is kept vague due to the breadth
of physically meaning and theoretically interesting functions which could initialize
our problem. Thus, we will use a polynomial ﬁtting functions polyfit() and
polyval() in example code PDE_Analysis_Setup.m to set up a polynomial
of any degree which best goes through the boundary points and other provided
points. This description of the initial condition allows us to explore functions
constrained by their shape within the bounds of the equilibrium point ¯u analyzed
in Section 4.1.
Exercise 1. Consider the PDE
ut = 4uxx.
(4)
a) Determine the order in time and space and how many initial and boundary
conditions are needed to deﬁne a unique solution.
b) Using Deﬁnition (2) as a guide, write out the IBVP for an unknown u(x, t) such
that it has an initial proﬁle of sin(x), boundary value of 0 whenever x = 0 and
x = π, and is deﬁned for 0 ≤x ≤1, t ≥0.
c) Verify that you have enough initial and boundary conditions as determined
previously.
d) Verify that the function,
u(x, t) = e−4t sin(x),
(5)

Numerical Investigations of Nonlinear PDEs
269
0
5
10
0
10
20
30
40
0
0.2
0.4
0.6
0.8
1
x
t
U
0
5
10
0
10
20
30
40
0
0.2
0.4
0.6
0.8
1
x
t
U
Fig. 1 Comparison of numerical solutions using the adaptive Rosenbrock method (ode23s in
MATLAB) for the (left) linear Test equation (6) using ρ = 0 and (right) Fisher-KPP equation (3)
using ρ = 1, where all other parameters use the default values of a = 0, b = 1, L = 10, δ =
1, Δx = 0.05, degree = 2, c = 1
3 and initial condition from line 8 in PDE_Analysis_Setup.m
found in Appendix 5.3.
is a solution to equation (4) by evaluating both sides of the PDE and checking
the initial and boundary conditions.
Error is more difﬁcult to analyze for nonlinear PDEs, so it is helpful to have
an associated linear version of your equation to analyze ﬁrst. We will compare our
analysis of reaction-diffusion equations to the Test equation,
ut = δuxx,
(6)
u(x, 0) = u0(x),
u(0, t) = 0
u(10, t) = 1
which is the heat equation in one dimension with constant heat forced at the two end
points [3]. Note, this is not a direct linearization of the Fisher-KPP equation (3),
but it behaves similarly for large values of δ. Figure 1 provides a comparison of
the solutions to the linear Test equation (6) and the Fisher-KPP equation (3) for
δ = 1. Note how the step size in time for both solutions increases dramatically to
have large, even spacing as the solution nears the steady-state solution. Adaptive
methods, like MATLAB’s ode23s, adjust to a larger step size as the change in the
solution diminishes.
2.2
Overview of Numerical Methods
Numerical methods are algorithms which solve problems using arithmetic com-
putations instead of algebraic formulas. They provide quick visualizations and
approximations of solutions to problems which are difﬁcult or less helpful to solve
exactly.

270
R.C. Harwood
Numerical methods for differential equations began with methods for approx-
imating integrals: starting with left and right Riemann sums, then progressing to
the trapezoidal rule, Simpson’s rule and others to increase accuracy more and
more efﬁciently. Unfortunately, the value of the slope function for an ODE is
often unknown so such approximations require modiﬁcations, such as Taylor series
expansions for example, to predict and correct slope estimates. Such methods for
ODEs can be directly applied to evolution PDEs (2). Discretizing in space, we create
a system of ordinary differential equations with vector U(t) with components Um(t)
approximating the unknown function u(x, t) at discrete points xm. The coefﬁcients
of linear terms are grouped into matrix D(t) and nonlinear terms are left in vector
function R(t, U). In the following analysis, we will assume that t is not explicit in
the matrix D or nonlinear vector R(U) to obtain the general form of a reaction-
diffusion model (2)
dU
dt = DU + R(U) + B.
(7)
Example 2. We will discretize the Fisher-KPP equation (3) in space using default
parameter values a = 0, b = 1, L = 10, δ = 1, Δx = 0.05, ρ = 1 in
PDE_Analysis_ Setup.m found in Appendix 5.3. See Figure 1 (right) for the
graph. Evenly dividing the interval [0, 10] with Δx = 0.05 =
1
20 results in 199
spatial points, xm, where the function is unknown (plus the two end points where it
is known: U0 = a, U200 = b). Using a centered difference approximation of Uxx
[3],
(Uxx)1 ≈a −2U1 + U2
Δx2
,
(8)
(Uxx)m ≈Um−1 −2Um + Um+1
Δx2
, 2 ≤m ≤198,
(Uxx)199 ≈U198 −2U199 + b
Δx2
,
the discretization of (3) can be written as
dU
dt = DU + R(U) + B,
(9)
D =
δ
Δx2
⎡
⎢⎢⎢⎢⎣
−2 1 . . . 0
1 −2 ... . . .
. . . ... ... 1
0 . . . 1 −2
⎤
⎥⎥⎥⎥⎦

Numerical Investigations of Nonlinear PDEs
271
R(U) = ρ
⎡
⎣
U1(1 −U1)
. . .
U199(1 −U199)
⎤
⎦= ρ (I −diag(U)) U
B =
δ
Δx2
⎡
⎢⎢⎢⎢⎢⎣
a
0
. . .
0
b
⎤
⎥⎥⎥⎥⎥⎦
with a tridiagonal matrix D, a nonlinear vector function R which can be written as
a matrix product using diagonal matrix formed from a vector (diag()), and a sparse
constant vector B which collects the boundary information.
By the Fundamental Theorem of Calculus [3], the exact solution to (7) over a
small interval of time Δt is found by integration from tn to tn+1 = tn + Δt as
Un+1 = Un +
> tn+1
tn
(DU(t) + R(U(t)) + B) dt,
(10)
where each component Um(t) has been discretized in time to create an array of
components Un
m approximating the solution u(xm, tn). Note that having the unknown
function U(t) inside the integral (10) makes it impossible to integrate exactly, so we
must approximate. Approximating with a left Riemann sum results in the Forward
Euler (a.k.a. classic Euler) method [17],
Un+1 = Un + Δt (D(Un + R(Un) + B) ,
(11)
while approximating with a right Riemann sum results in the Backward Euler
method [17]
Un+1 = Un + Δt
	
DUn+1 + R(Un+1) + B

.
(12)
Although approximating integrals with left and right Riemann sums is a similar
task, in solving differential equations, they can be very different. Forward Euler (11)
is referred to as an explicit method since the unknown Un+1 can be directly
computed in terms of known quantities such as the current known approximation
Un, while Backward Euler
(12) is referred to as an implicit method since the
unknown Un+1 is solved in terms of both known Un and unknown Un+1 quantities.
Explicit methods are simple to set up and compute, while implicit methods may
not be solvable at all. If we set R(U) ≡0 to make equation (7) linear, then
an implicit method can be easily written in the explicit form, as shown in the
Example 4. Otherwise, an unsolvable implicit method can be approximated with a
numerical root-ﬁnding method such as Newton’s method (47) which is discussed in
Section 4.2, but nesting numerical methods is much less efﬁcient than implementing

272
R.C. Harwood
an explicit method as it employs a truncated Taylor series to mathematically
approximate the unknown terms. The main reasons to use implicit methods are for
stability, addressed in Section 3.3. The following examples demonstrate how to form
the two-level matrix form.
Deﬁnition 3. A two-level numerical method for an evolution equation (2) is an
iteration which can be written in the two-level matrix form
Un+1 = M Un + N,
(13)
where M is the combined transformation matrix and N is the resultant vector. Note,
both M and N may update every iteration, especially when the PDE is nonlinear,
but for many basic problems, M and N will be constant.
Example 3. (Forward Euler) Determine the two-level matrix form for the Forward
Euler method for the Fisher-KPP equation (3). Since Forward Euler is already
explicit, we simply factor out the Un components from equation (11) to form
Un+1 = Un + Δt (D(Un + ρ (I −diag(Un)) Un + B) ,
(14)
= MUn + N,
M = (I + ΔtD + Δtρ (I −diag(Un))) ,
N = ΔtB,
where I is the identity matrix, N is constant, and M updates with each iteration since
it depends on Un.
Example 4. (Linear Backward Euler) Determine the two-level matrix form for
the Backward Euler method for the Test equation (6). In the Backward Euler
method (12), the unknown Un+1 terms can be grouped and the coefﬁcient matrix
I −ΔtD inverted to write it explicitly as
Un+1 = MUn + N,
(15)
M = (I −ΔtD)−1
N = Δt (I −ΔtD)−1 B
where the method matrix M and additional vector N are constant.
Just as the trapezoid rule takes the average of the left and right Riemann sums,
the Crank-Nicolson method (16) averages the Forward and Backward Euler methods
[5].
Un+1 = Un + Δt
2 D
	
Un + Un+1
+ Δt
2
	
R(Un) + R(Un+1)

+ ΔtB.
(16)

Numerical Investigations of Nonlinear PDEs
273
One way to truncate an implicit method of a nonlinear equation into an explicit
method is called a semi-implicit method [3], which treats the nonlinearity as known
information (evaluated at current time tn) leaving the unknown linear terms at tn+1.
For example, the semi-implicit Crank-Nicolson method is
Un+1 =

I −Δt
2 D
−1 
Un + Δt
2 DUn + ΔtR(Un) + B

.
(17)
Exercise 2. After reviewing Example 3 and Example 4, complete the following for
the semi-implicit Crank-Nicolson method (17).
a) Determine the two-level matrix form for the Test equation (6). Note, set R = 0.
b) *Determine the two-level matrix form for the Fisher-KPP equation (3).
*See Section 3.3 for the answer and its analysis.
Taylor series expansions can be used to prove that while the Crank-Nicolson
method (16) for the linear Test equation (6) is second order accurate (See Def-
inition 18 in Section 5.1), the semi-implicit Crank-Nicolson method (17) for a
nonlinear PDE is only ﬁrst order accurate in time. To increase truncation error
accuracy, unknown terms in an implicit method can be truncated using a more
accurate explicit method. For example, blending the Crank-Nicolson method with
Forward Euler approximations creates the Improved Euler Crank-Nicolson method,
which is second order accurate in time for nonlinear PDEs.
U∗= Un + Δt (DUn + R(Un) + B) ,
(18)
Un+1 = Un + Δt
2 D (Un + U∗) + Δt
2 (R(Un) + R(U∗)) + ΔtB.
This improved Euler Crank-Nicolson method (18) is part of the family of Runge-
Kutta methods which embed a sequence of truncated Taylor expansions for implicit
terms to create an explicit method of any given order of accuracy [3]. Proofs of the
accuracy for the semi-implicit (17) and improved Euler (18) methods are included
in Appendix 5.1.
Exercise 3. After reviewing Example 3 and Example 4, complete the following for
the Improved Euler Crank-Nicolson method (18).
a) Determine the two-level matrix form for the Test equation (6). Note, set R = 0.
b) Determine the two-level matrix form for the Fisher-KPP equation (3).

274
R.C. Harwood
2.3
Overview of Software
Several software have been developed to compute numerical methods. Commer-
cially, MATLAB, Mathematica, and Maple are the best for analyzing such methods,
though there are other commercial software like COMSOL which can do numerical
simulation with much less work on your part. Open-source software capable of
the same (or similar) numerical computations, such as Octave, SciLab, FreeFEM,
etc. are also available. Once the analysis is complete and methods are fully tested,
simulation algorithms are trimmed down and often translated into Fortran or C/C++
for efﬁciency in reducing compiler time.
We will focus on programming in MATLAB, created by mathematician
Cleve Moler (born in 1939), one of the authors of the LINPACK and EISPACK
scientiﬁc subroutine libraries used in Fortran and C/C++ compilers [17]. Cleve
Moler originally created MATLAB to give his students easy access to these
subroutines without having to write in Fortran or C themselves. In the same
spirit, we will be working with simple demonstration programs, listed in the
appendix, to access the core ideas needed for our numerical investigations.
Programs PDE_Solution.m (Appendix 5.2), PDE_Analysis_Setup.m
(Appendix 5.3), and Method_Accuracy_ Verification.m (Appendix 5.5)
are MATLAB scripts, which means they can be run without any direct input
and leave all computed variables publicly available to analyze after they are run.
Programs CrankNicolson_SI.m (Appendix 5.4) and Newton_System.m
(Appendix 5.6) are MATLAB functions, which means they may require inputs
to run, keep all their computations private, and can be effectively embedded
in other functions or scripts. All demonstrations programs are run through
PDE_Solution.m, which is the main program for this group.
Example 5. The demonstration programs can be either downloaded from the pub-
lisher or typed into ﬁve separate MATLAB ﬁles and saved according to the name at
the top of the ﬁle (e.g. PDE_Analysis_Setup.m). To run them, open MATLAB
to the folder which contains these ﬁve programs. In the command window, type
help PDE_Solution to view the comments in the header of the main program.
Then type PDE_Solution to run the default demonstration. This will solve and
analyze the Fisher-KPP equation (3) using the default parameters, produce ﬁve
graph windows, and report three outputs on the command window. The ﬁrst graph is
the numerical solution using MATLAB’s built-in implementation of the Rosenbrock
method (ode23s), which is also demonstrated in Figure 1 (right). The second graph
plots the comparable eigenvalues for the semi-implicit Crank-Nicolson method (17)
based upon the maximum Δt step used in the chosen method (Rosenbrock by
default). The third graph shows a different steady-state solution to the Fisher-KPP
equation (3) found using Newton’s method (47). The fourth graph shows the rapid
reduction of the error of this method as the Newton iterations converge. The ﬁfth
graph shows the instability of the Newton steady-state solution by feeding a noisy
perturbation of it back into the Fisher-KPP equation (3) as an initial condition. This

Numerical Investigations of Nonlinear PDEs
275
noisy perturbation is compared to round-off perturbation in Figure 5 to see how long
this Newton steady-state solution can endure. Notice that the solution converges
back to the original steady-state solution found in the ﬁrst graph.
To use the semi-implicit Crank-Nicolson method (17) instead of MATLAB’s
ode23s, do the following. Now the actual eigenvalues of this method are plotted
in the second graph.
Exercise 4. Open PDE_Solution.m in the MATLAB Editor. Then comment
lines 7–9 (type a % in front of each line) and uncomment lines 12–15 (remove
the % in front of each line). Run PDE_Solution. Verify that the second graph
matches Figure 3.
The encoded semi-implicit Crank-Nicolson method (17) uses a ﬁxed step size Δt,
so it is generally not as stable as MATLAB’s built-in solver. It would be best to now
uncomment lines 7–9 and comment lines 12–15 to return to the default form before
proceeding. The main beneﬁt of the ode23s solver is that it is adaptive in choosing
the optimal Δt step size and adjusting it for regions where the equation is easier or
harder to solve than before. This method is also sensitive to stiff problems, where
stability conditions are complicated or varying. MATLAB has several other built-in
solvers to handle various situations. You can explore these by typing help ode.
Once you have run the default settings, open up PDE_Analysis_Setup in the
editor and tweak the equation parameter values a,b,L,delta,rho,degree,c
and logistical parameter dx. After each tweak, make sure you run the main
program PDE_Solution. The logistical parameters tspan,dt for the numerical
method can also be tweaked in PDE_Solution, and an inside view of Newton
iterations can be seen by uncommenting lines 38–39. Newton’s method is covered
in Section 4.2. A solution with Newton’s method is demonstrated in 2(left), while
all of the iterations are graphed in Figure 2(right). Note that Figure 2(right) is very
similar to a solution which varies over time, but it is not. The graph of the iterations
demonstrates how Newton’s method seeks better and better estimates of a ﬁxed
steady-state solution discussed in Section 4.1.
Exercise 5. In PDE_Analysis_Setup, set parameter values, a = 0, b = 0, L =
10, δ =
1
10, Δx =
1
20, ρ = 1, degree = 2, c = 1. Then, in
PDE_Solution,
uncomment lines 38–39 and run it. Verify that the third and fourth graphs matches
Figure 2.
Notice that the iterations of Newton’s method in Figure 2(right) demonstrate
oscillatory behavior in the form of waves which diminish in amplitude towards the
steady-state solution. These are referred to as stable numerical oscillations similar
to the behavior of an underdamped spring [3]. These stable oscillations suggest that
the steady-state solution is stable (attracting other initial proﬁles to it), but due to
the negative values in the solution in Figure 2(left), it is actually an unstable steady-
state for Fisher-KPP equation (3). You can see this demonstrated in the last graph
plotted when you ran PDE_Solution where there is a spike up to a value around
−4 × 1012. This paradox demonstrates that not all steady-state solutions are stable
and that the stability of Newton’s method differs from the stability of a steady-state
solution to an IBVP.

276
R.C. Harwood
0
1
2
3
4
5
6
7
8
9
10
−3
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
x
U
0
2
4
6
8
10
0
5
10
15
−3
−2
−1
0
1
2
x
Iterations
U
Fig. 2 An example steady-state solution using Newton’s method (left) and the iterations to that
steady-state (right) using the parameter values a = 0, b = 0, L = 10, δ =
1
10, Δx =
1
20, ρ =
1, degree = 2, c = 1 and initial condition from line 8 in PDE_Analysis_Setup.m found in
Appendix 5.3. Also, uncomment lines 38–39 in PDE_Solution.m found in Appendix 5.2.
Some best practices of programming in MATLAB are to clean up before running
new computations, preallocate memory, and store calculations which are used more
than once. Before new computations are stored in a script ﬁle, you can clean up
your view of previous results in the command window (clc), delete previously
held values and structure of all (clear all) or selected (clear name1,
name2) variables, and close all (close all) or selected ( close handle1,
handle2) ﬁgures. The workspace for running the main program PDE_Solution
is actually cleared in line 5 of
PDE_Analysis_Setup so that this supporting
ﬁle can be run independently when needed.
When you notice the same calculation being computed more than once in your
code, store it as a new variable to trim down the number of calculations done for
increased efﬁciency. Most importantly, preallocate the structure of a vector or matrix
that you will ﬁll in values with initial zeros (zeros(columns,rows)), so that
MATLAB does not create multiple copies of the variable in memory as you ﬁll it in.
The code BCs = zeros(M-2,1); in line 23 of PDE_Analysis_Setup.m
is an example of preallocation for a vector. Preallocation is one of most effective
ways of speeding up slow code.
Exercise 6. Find the four (total) lines of code in Newton_System.m, Method_
Accuracy_Verification.m, and CrankNicolson_SI.m which preallo-
cate a variable.
3
Error Analysis
To encourage conﬁdence in the numerical solution it is important to support the
theoretical results with numerical demonstrations. For example, a theoretical con-
dition for stability or oscillation-free behavior can be demonstrated by comparing

Numerical Investigations of Nonlinear PDEs
277
solutions before and after the condition within a small neighborhood of it. On the
other hand, the order of accuracy can be demonstrated by comparing subsequent
solutions over a sequence of step sizes, as we will see in Section 3.1. Demonstrating
stability ensures when, while accuracy ensures how rapidly, the approximation will
converge to the true solution. Showing when oscillations begin to occur prevents
any confusion over the physical dynamics being simulated, as we will investigate in
Section 3.4.
3.1
Verifying Accuracy
Since numerical methods for PDEs use arithmetic to approximate the underlying
calculus, we expect some error in our results, including inaccuracy measuring the
distance from our target solution as well as some imprecision in the variation of our
approximations. We must also balance the mathematical accuracy in setting up the
method with the round-off errors caused by computer arithmetic and storage of real
numbers in a ﬁnite representation. As we use these values in further computations,
we must have some assurance that the error is minimal. Thus, we need criteria to
describe how conﬁdent we are in these results.
Error is deﬁned as the difference between the true value u(xm, tn) and approx-
imate value Un
m, but this value lacks the context given by the magnitude of the
solution’s value and focuses on the error of individual components of a solution’s
vector. Thus, the relative error ϵ is more meaningful as it presents the absolute error
relative to the true value as long as u(xm, tn) ̸= 0 under a suitable norm such as the
max norm || · ||∞.
Deﬁnition 4. The relative error ϵ for a vector solution Un is the difference between
the true value u(xm, tn) and approximate value Un
m under a suitable norm ||·||, relative
to the norm of the true value as
ϵ = ||u(x, tn) −Un||
||u(x, tn)||
× 100%.
(19)
The signiﬁcant ﬁgures of a computation are those that can be claimed with
conﬁdence. They correspond to a number of conﬁdent digits plus one estimated
digit, conventionally set to half of the smallest scale division on the measurement
device, and speciﬁed precisely in Deﬁnition 5.
Deﬁnition 5. The value Un
m approximates u(xm, tn) to N signiﬁcant digits if N is the
largest non-negative integer for which the relative error is bounded by the signiﬁcant
error ϵs(N)
ϵs(N) =
	
5 × 10−N
× 100%
(20)

278
R.C. Harwood
To ensure all computed values in an approximation have about N signiﬁcant ﬁgures,
deﬁnition 5 implies
N + 1 > log10
5
ϵ

> N,
(21)
Although the true value is not often known, the relative error of a previous
approximation can be estimated using the best available approximation in place of
the true value.
Deﬁnition 6. For an iterative method with improved approximations U(0), U(1),
. . . , U(k), U(k), the approximate relative error at position (xm, tn) is deﬁned [3] as
the difference between current and previous approximations relative to the current
approximation, each under a suitable norm || · ||
E(k) = ||U(k+1) −U(k)||
||U(k+1)||
× 100%
(22)
closely approximates, ϵ(k), the relative error for the kth iteration assuming that the
iterations are converging (that is, as long as ϵ(k+1) is much less than ϵ(k)).
The following conservative theorem, proven in
[21], is helpful in clearly
presenting the lower bound on the number of signiﬁcant ﬁgures of our results.
Theorem 1. Approximation Un at step n with approximate relative error E(k) is
correct to at least N −1 signiﬁcant ﬁgures if
E(k) < ϵs(N)
(23)
Theorem 1 is conservatively true for the relative error ϵ, often underestimating
the number of signiﬁcant ﬁgures found. The approximate relative error E(k) (22),
however, underestimates the relative error ϵ and may predict one signiﬁcant digit
more for low order methods.
Combining theorem 1 with equation (21), the number of signiﬁcant ﬁgures has a
lower bound
N ≥
?
log10
 0.5
E(k)
@
.
(24)
Example 6. Table 1 presents these measures of error to analyze the Crank-Nicolson
method (16) for the linear Test equation (6) and the semi-implicit version of the
Crank-Nicolson method (17) for the Fisher-KPP equation (3). Column 1 tracks the
step size Δt as it is halved for improved approximations U(k) at t = 10. Columns
2 and 5 present the approximate errors in scientiﬁc notation for easy readability.
Scientiﬁc notation helps read off the minimum number of signiﬁcant ﬁgures ensured

Numerical Investigations of Nonlinear PDEs
279
Table 1 Verifying Accuracy in Time for Semi-Implicit Crank-Nicolson Method
Test Equation
Fisher-KPP Equation
Δt
Approximate
Errora
Sig.
Figsc
Order
of
Accuracyb
Approximate
Errora
Sig.
Figsc
Order
of
Accuracyb
1
3.8300e-05
4
2 (2.0051)
4.1353e-05
4
-1 (-0.5149)
1
2
9.5413e-06
4
2 (2.0009)
5.9091e-05
3
0 (0.4989)
1
4
2.3838e-06
5
2 (2.0003)
4.1818e-05
4
1 (0.7773)
1
8
5.9584e-07
5
2 (2.0001)
2.4399e-05
4
1 (0.8950)
1
16
1.4895e-07
6
2 (2.0000)
1.3120e-05
4
1 (0.9490)
1
32
3.7238e-08
7
2 (2.0000)
6.7962e-06
4
1 (0.9749)
1
64
9.3096e-09
7
2 (2.0001)
3.4578e-06
5
1 (0.9875)
1
128
2.3273e-09
8
2 (1.9999)
1.7439e-06
5
1 (0.9938)
a Approximate error under the max norm || · ||∞for numerical solution U(k) computed at tn = 10
compared to solution at next iteration U(k+1) whose time step is cut in half.
b Order of accuracy is measured as the power of 2 dividing the error as the step size is divided by
2
c Minimum number of signiﬁcant ﬁgures predicted by approximate error bounded by the
signiﬁcant error ϵs(N) as in equation (23)
by Theorem 1, as presented in columns 3 and 6. Notice how the errors in column 2
are divided by about 4 each iteration while those in column 5 are essentially divided
by 2. This ratio of approximate relative errors demonstrates the orders of accuracy,
p as a power of 2 since the step sizes are divided by 2 each iteration.
ϵ(k+1)
ϵ(k)
= C
2p ,
(25)
for some positive scalar C which underestimates the integer p when C > 1 and
overestimates p when C < 1. By rounding to mask the magnitude of C, the order p
can be computed as
p = round

log2
 ϵ(k)
ϵ(k+1)

.
(26)
Columns 4 and 7 present both rounded and unrounded measures of the order of
accuracy for each method and problem. Thus, we have veriﬁed that Crank-Nicolson
method (16) on a linear problem is second order accurate in time, whereas the semi-
implicit version of the Crank-Nicolson method (17) for the nonlinear Fisher-KPP
equation (3) is only ﬁrst order in time.
For comparison, Table 2 presents these same measures for the Rosenbrock
method built into MATLAB as ode23s. See example program Method_Accuracy
_Verification.m in Appendix 5.1 for how to ﬁx a constant step size in such
an adaptive solver by setting the initial step and max step to be Δt with a high
tolerance to keep the adaptive method from altering the step size.

280
R.C. Harwood
Table 2 Verifying Accuracy in Time for ode23s Solver in MATLAB
Test Equation
Fisher-KPP Equation
Δt
Approximate
Errora
Sig. Figsc
Order
of
Accuracyb
Approximate
Errora
Sig. Figsc
Order
of
Accuracyb
1
1.8829e-05
4
2 (2.00863)
7.4556e-05
3
2 (1.90005)
1
2
4.6791e-06
5
2 (2.00402)
1.9976e-05
4
2 (1.98028)
1
4
1.1665e-06
5
2 (2.00198)
5.0628e-06
4
2 (1.99722)
1
8
2.9123e-07
6
2 (2.00074)
1.2681e-06
5
2 (2.00060)
1
16
7.2771e-08
6
2 (2.00054)
3.169e-07
6
2 (2.00025)
1
32
1.8186e-08
7
2 (2.00027)
7.9211e-08
6
2 (1.99900)
1
64
4.5456e-09
8
2 (1.99801)
1.9817e-08
7
2 (2.00168)
1
128 1.138e-09
8
2 (1.99745)
4.9484e-09
8
2 (2.00011)
a Approximate error under the max norm || · ||∞for numerical solution U(k) computed at tn = 10
compared to solution at next iteration U(k+1) whose time step is cut in half.
b Order of accuracy is measured as the power of 2 dividing the error as the step size is divided by 2
c Minimum number of signiﬁcant ﬁgures predicted by approximate error bounded by the signiﬁcant
error ϵs(N) as in equation (23)
Exercise 7. Implement the Improved Euler Crank-Nicolson method (18) and verify
that the error in time is O
	
Δt2
on both the Test equation (6) and Fisher-KPP
equation (3) using a table similar to Table 1.
3.2
Convergence
Numerical methods provide dependable approximations Un
m of the exact solution
u (xm, tn) only if the approximations converge to the exact solution, Un
m →u (xm, tn)
as the step sizes diminish, Δx, Δt →0. Convergence of a numerical method relies
on both the consistency of the approximate equation to the original equation as
well as the stability of the solution constructed by the algorithm. Since consistency
is determined by construction, we need only analyze the stability of consistently
constructed schemes to determine their convergence. This convergence through
stability is proven generally by the Lax-Richtmeyer Theorem [22], but is more
speciﬁcally deﬁned for two-level numerical methods (13) in the Lax Equivalence
Theorem (2).
Deﬁnition 7. A problem is well-posed if there exists a unique solution which
depends continuously on the conditions.
Discretizing an initial-boundary-value problem (IBVP) into an initial-value problem
(IVP) as an ODE system ensures the boundary conditions are well developed for the
problem, but the initial conditions must also agree at the boundary for the problem to
be well-posed. Further, the slope function of the ODE system needs to be inﬁnitely
differential, like the Fisher-KPP equation (3), or at least Lipshitz-continuous, so that
Picard’s uniqueness and existence theorem via Picard iterations [3] applies to ensure
that the problem is well-posed [22]. Theorem 2, proved in [22] ties this altogether
to ensure convergence of the numerical solution to the true solution.

Numerical Investigations of Nonlinear PDEs
281
Theorem 2 (Lax Equivalence Theorem). A consistent, two-level difference
scheme (13) for a well-posed linear IVP is convergent if and only if it is stable.
3.3
Stability
The beauty of Theorem 2 (Lax Equivalence Theorem ) is that once we have a
consistent numerical method, we can explore the bounds on stability to ensure
convergence of the numerical solution. We begin with a few deﬁnitions and
examples to lead us to von Neumann stability analysis, named after mathematician
John von Neumann (1903–1957).
Taken from the word eigenwerte, meaning one’s own values in German, the
eigenvalues of a matrix deﬁne how a matrix operates in a given situation.
Deﬁnition 8. For a k × k matrix M, a scalar λ is an eigenvalue of M with
corresponding k × 1 eigenvector v ̸= 0 if
Mv = λv.
(27)
Lines 22–31 of the demonstration code PDE_Solution.m, found in
Appendix 5.2, compute several measures helpful in assessing stability, including the
graph of the eigenvalues of the method matrix for a two-level method (13) on the
complex plane. The spectrum, range of eigenvalues, of the default method matrix
is demonstrated in Figure 3, while the code also reports the range in the step size
ratio, range in the real parts of the eigenvalues, and computes the spectral radius.
Deﬁnition 9. The spectral radius of a matrix, μ(M) is the maximum magnitude of
all eigenvalues of M
μ(M) = max
i
|λi|.
(28)
The norm of a vector is a well-deﬁned measure of its size in terms of a speciﬁed
metric, of which the Euclidean distance (notated ||·||2), the maximum absolute value
(|| · ||∞), and the absolute sum (|| · ||1) are the most popular. See [12] for further
details. These measures of a vector’s size can be extended to matrices.
Deﬁnition 10. For any norm ||·||, the corresponding matrix norm |||·||| is deﬁned by
|||M||| = max
x
||Mx
x
.
(29)
A useful connection between norms and eigenvalues is the following theorem [12].
Theorem 3. For any matrix norm ||| · ||| and square matrix M, μ(M) ≤|||M|||.

282
R.C. Harwood
−1
−0.5
0
0.5
1
−1
−0.8
−0.6
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
X: −0.9975
Y: 0
Imag(λ)
Real(λ)
Fig. 3 Plot of real and imaginary components of all eigenvalues of method matrix M for semi-
implicit Crank-Nicolson method for Fisher-KPP equation (3) using the default parameter values
a = 0, b = 1, L = 10, δ = 1, Δx = 0.05, ρ = 1, degree = 2, c = 1
3 and initial condition as given
in line 16 of PDE_Analysis_Setup.m in Appendix 5.3.
Proof. Consider an eigenvalue λ of matrix M corresponding to eigenvector x whose
magnitude equals the spectral radius, |λ| = μ(M). Form a square matrix X whose
columns each equal the eigenvector x. Note that by Deﬁnition 8, MX = λX and
|||X||| ̸= 0 since x ̸= 0.
|λ||||X||| = ||λX||
(30)
= ||MX||
≤|||M||||||X|||
Therefore, |λ| = μ(M) ≤|||M|||.
Theorem 3 can be extended to an equality in Theorem 4 (proven in [12]),
Theorem 4. μ(M) = limk→∞|||Mk|||
1
k .
This offers a useful estimate of the matrix norm by the spectral radius, speciﬁcally
when the matrix is powered up in solving a numerical method.
Now, we can apply these deﬁnitions to the stability of a numerical method. An
algorithm is stable if small changes in the initial data produce only small changes

Numerical Investigations of Nonlinear PDEs
283
in the ﬁnal results [3], that is, the errors do not “grow too fast” as quantiﬁed in
Deﬁnition 11.
Deﬁnition 11. A two-level difference method (13) is said to be stable with respect
to the norm || · || if there exist positive max step sizes Δt0, and Δx0, and non-
negative constants K and β so that
||Un+1|| ≤Keβ△t||U0||,
for 0 ≤t, 0 < △x ≤△x0 and 0 < △t ≤△t0.
The von Neumann criterion for stability (31) allows for stable solution to an exact
solution which is not growing (using C = 0 for the tight von Neumann criterion) or
at most exponentially growing (using some C > 0) by bounding the spectral radius
of the method matrix,
μ(M) ≤1 + C△t,
(31)
for some C ≥0.
Using the properties of norms and an estimation using Theorem 4, we can
approximately bound the size of the numerical solution under the von Neumann
criterion as
||Un+1|| = ||Mn+1U0||
(32)
≤|||Mn+1||| ||U0||
≈μ(M)n+1 ||U0||
≤(1 + CΔt)n+1 ||U0||
= (1 + (n + 1)CΔt + . . .) ||U0||
≤e(n+1)CΔt ||U0||
= KeβΔt ||U0||
for K = 1, β = (n + 1)C, which makes the von Neumann criterion sufﬁcient for
stability of the solution in approximation for a general method matrix. When the
method matrix is symmetric, which occurs for many discretized PDEs including the
Test equation with Forward Euler, Backward Euler, and Crank-Nicolson methods,
the spectral radius equals the ||| · |||2 of the matrix. Then, the von Neumann
criterion (31) provides a precise necessary and sufﬁcient condition for stability [22].
If the eigenvalues are easily calculated, they provide a simple means for predict-
ing the stability and behavior of the solution. Von Neumann analysis, estimation of
the eigenvalues from the PDE itself, provides a way to extract information about the
eigenvalues, if not the exact eigenvalues themselves.
For a two-level numerical scheme (13), the eigenvalues of the combined transfor-
mation matrix indicate the stability of the solution. Seeking a solution to the linear

284
R.C. Harwood
difference scheme by separation of variables, as is used for linear PDEs, we can
show that the discrete error growth factors are the eigenvalues of the method matrix
M. Consider a two-level difference scheme (13) for a linear parabolic PDE so that
R = 0, then the eigenvalues can be deﬁned by the constant ratio [22]
Un+1
m
Unm
= Tn+1
Tn
= λm, where Un
m = XmTn.
(33)
The error ϵn
m = u (xm, tn) −Un
m satisﬁes the same equation as the approximate
solution Un
m, so the eigenvalues also deﬁne the ratio of errors in time called the error
growth (or ampliﬁcation) factor [22]. Further, the error can be represented in Fourier
form as ϵn
m = ˆϵeαnΔteimβΔx where ˆϵ is a Fourier coefﬁcient, α is the growth/decay
constant, i = √−1, and β is the wave number. Under this assumptions, the
eigenvalues are equivalent to the error growth factors of the numerical method,
λk = Un+1
m
Unm
= ϵn+1
m
ϵnm
= ˆϵeα(n+1)ΔteimβΔx
ˆϵeαnΔteimβΔx
= eαΔt.
We can use this equivalency to ﬁnding bounds on the eigenvalues of a numerical
scheme by plugging the representative growth factor eαΔt into the method dis-
cretization called von Neumann stability analysis (also known as Fourier stability
analysis) [22, 23]. The von Neumann criterion (31) ensures that the matrix stays
bounded as it is powered up. If possible, C is set to 0, called the tight von
Neumann criterion for simpliﬁed bounds on the step sizes. As another consequence
of this equivalence, analyzing the spectrum of the transformation matrix also reveals
patterns in the orientation, spread, and balance of the growth of errors for various
wave modes.
Before we dive into the stability analysis, it is helpful to review some identities
for reducing the error growth factors:
eix + e−ix
2
= cos(x), eix −e−ix
2
= i sin(x),
(34)
1 −cos(x)
2
= sin2 x
2

, 1 + cos(x)
2
= cos2 x
2

.
Example 7. Use the von Neumann stability analysis to determine conditions on
Δt, Δx to ensure stability of the Crank-Nicolson method (16) for the Test equa-
tion (6).
For von Neumann stability analysis, we replace each solution term Un
m in the
method with the representative error ϵn
m = ˆϵeαnΔteimβΔx,
Un+1
m
= rUn
m−1 + (1 −2r) Un
m + rUn
m+1,
(35)
ˆϵeαnΔt+αΔteimβΔx = rˆϵeαnΔteimβΔx−iβΔx + (1 −2r) ˆϵeαnΔteimβΔx
+rˆϵeαnΔteimβΔx+iβΔx,

Numerical Investigations of Nonlinear PDEs
285
where r = δΔt
Δx2 . Dividing through by the common ϵn
m term we can solve for the error
growth factor
eαΔt = 1 −2r + 2r
eiβΔx + e−iβΔx
2

,
(36)
= 1 −4r
1 −cos(βΔx)
2

,
= 1 −4r sin2
βΔx
2

,
reduced using the identities in (34). Using a tight (C
=
0) von Neumann
criterion (31), we bound |eαΔt| ≤1 with the error growth factor in place of the
spectral radius. Since the error growth factor is real, the bound is ensured in the two
components, eαΔt ≤1 which holds trivially and eαΔt ≥−1 which is true at the
extremum as long as r ≤1
2. Thus, as long as dt ≤Δx2
2δ , the Forward Euler method
for the Test equation is stable in the sense of the tight von Neumann criterion which
ensures diminishing errors at each step.
The balancing of explicit and implicit components in the Crank-Nicolson method
create a much less restrictive stability condition.
Exercise 8. Use the von Neumann stability analysis to determine conditions on
Δt, Δx to ensure stability of the Crank-Nicolson method (16) for the Test equa-
tion (6).
Hint: verify that
eαΔt =
1 −2r sin2 
βΔx
2

1 + 2r sin2 
βΔx
2
,
(37)
and show that both bounds are trivially true so that Crank-Nicolson method is
unconditionally stable.
The default method in PDE_Solution.m and demonstrated in Figure 3 is the
semi-implicit Crank-Nicolson method (17) for the Fisher-KPP equation (3). If you
set ρ = 0 in PDE_Analysis_Setup.m, however, Crank-Nicolson method for
the Test equation (6) is analyzed instead.
The two-level matrix form of the semi-implicit Crank-Nicolson method is
Un+1 = MUn + N,
(38)
M =

I −Δt
2 D
−1 
I + Δt
2 D + ρΔt (1 −Un)

,
N = Δt

I −Δt
2 D
−1
B.

286
R.C. Harwood
Notice in Figure 3 that all of the eigenvalues are real and they are all bounded
between −1 and 1. Such a bound, |λ| < 1, ensures stability of the method based
upon the default choice of Δt, Δx step sizes.
Example 8. Use the von Neumann stability analysis to determine conditions on
Δt, Δx to ensure stability of the semi-implicit Crank-Nicolson method (17) for the
Fisher-KPP equation (3).
Now we replace each solution term Un
m in the semi-implicit Crank-Nicolson
method (17) for the Fisher-KPP equation (3)
−r
2Un+1
m−1 +(1 −r) Un+1
m
−r
2Un+1
m+1 = r
2Un
m−1+(1 −r + ρ(1 −Un
m)) Un
m+ r
2Un
m+1
with the representative error ϵn
m = ˆϵeαnΔteimβΔx where again r =
δΔt
Δx2 . Again
dividing through by the common ϵn
m term we can solve for the error growth factor
eαΔt =
1 −2r sin2 
βΔx
2

+ Δtρ(1 −˜U)
1 + 2r sin2 
βΔx
2

where constant ˜U represents the extreme values of Un
m. Due to the equilibrium points
¯u = 0, 1 to be analyzed in Section 4.1, the bound 0 ≤Un
m ≤1 holds as long as the
initial condition is similarly bounded 0 ≤U0
m ≤1. Due to the potentially positive
term Δtρ(1 −˜U), the tight (C = 0) von Neumann criterion fails, but the general
von Neumann criterion (31), |eαΔt| ≤1 + CΔt, does hold for C ≥ρ. With this
assumption, eαΔt ≥−1 −CΔt is trivially true and eαΔt ≤1 + CΔt results in
	
ρ(1 −˜U) −C

Δx2 −4δ sin2
βΔx
2

≤2CδΔt sin2
βΔx
2

which is satisﬁed at all extrema as long as C ≥ρ since
	
ρ(1 −˜U) −C

≤0. Thus,
the semi-implicit Crank-Nicolson method is unconditionally stable in the sense of
the general von Neumann criterion, which bounds the growth of error less than an
exponential. This stability is not as strong as that of the Crank-Nicolson method for
the Test equation but it provides useful management of the error.
3.4
Oscillatory Behavior
Oscillatory behavior has been exhaustively studied for ODEs [4, 9] with much
numerical focus on researching ways to dampen oscillations in case they emerge
[2, 19]. For those wishing to keep their methods oscillation-free, Theorem 5 pro-
vides sufﬁciency of non-oscillatory behavior through the non-negative eigenvalue
condition (proven, for example, in [22]).

Numerical Investigations of Nonlinear PDEs
287
Theorem 5 (Non-negative
Eigenvalue
Condition). A
two-level
difference
scheme (13) is free of numerical oscillations if all the eigenvalues λi of the method
matrix M are non-negative.
Following von Neumann stability analysis from Section 3.3, we can use the error
growth factors previously computed to determine the non-negative eigenvalue
condition for a given method.
Example 9. To ﬁnd the non-negative eigenvalue condition for the semi-implicit
Crank-Nicolson method for the Fisher-KPP equation (3), we start by bounding our
previously computed error growth factor as eαΔt ≥0 to obtain
1 −2r sin2
βΔx
2

+ Δtρ(1 −˜U) ≥0
which is satisﬁed at the extrema, assuming 0 ≤˜U ≤1, by the condition
δΔt
Δx2 ≤1
2
(39)
which happens to be the same non-negative eigenvalue condition for the Crank-
Nicolson method applied to the linear Test equation (6).
A numerical approach to track numerical oscillations uses a slight modiﬁcation
of the standard deﬁnitions for oscillatory behavior from ODE research to identify
numerical oscillations in solutions to PDEs [15].
Deﬁnition 12. A continuous function u(x, t) is oscillatory about K if the difference
u(x, t) −K has an inﬁnite number of zeros for a ≤t < ∞for any a. Alternately, a
function is oscillatory over a ﬁnite interval if it has more than two critical points of
the same kind (max, min, inﬂection points) in any ﬁnite interval [a, b] [9].
Using the ﬁrst derivative test, this requires two changes in the sign of the derivative.
Using ﬁrst order ﬁnite differences to approximate the derivative results in the
following approach to track numerical oscillations.
Deﬁnition 13 (Numerical Oscillations). By tracking the sign change of the
derivative for each spatial component through sequential steps tn−2, tn−1, tn in
time, oscillations in time can be determined by the logical evaluation
(Un−2 −Un−1)(Un−1 −Un) < 0,
which returns true (inequality satisﬁed) if there is a step where the magnitude
oscillates through a critical point. Catching two such critical points will deﬁne a
numerical oscillation in the solution.
Crank-Nicolson method is known to be unconditionally stable, but damped
oscillations have been found for large time steps. The point at which oscillations
begin to occur is an open question, but it is known to be bounded from below by the

288
R.C. Harwood
non-negative eigenvalue condition, which can be rewritten from (39) as Δt ≤Δx2
2δ .
Breaking the non-negative eigenvalue condition, however, does not always create
oscillations.
Challenge Problem 1. Using the example code found in Appendix 5.2, uncom-
ment lines 12–15 (deleting %’s) and comment lines 7–9 (adding %’s) to use the
semi-implicit Crank-Nicolson method. Make sure the default values of Δx =
0.05, ρ = 1 are set in PDE_Analysis_Setup.m and choose step size Δt = 2
in
PDE_ Solution.m. Notice how badly the non-negative eigenvalue condition
Δt ≤
Δx2
2δ
fails and run PDE_Solution.m to see stable oscillations in the
solution. Run it again with smaller and smaller Δt values until the oscillations are
no longer visible. Save this point as (Δx, Δt). Change Δx = 0.1 and choose a large
enough Δt to see oscillations and repeat the process to identify the lowest time
step when oscillations are evident. Repeat this for Δx = 0.5 and Δx = 1, then
plot all the (Δx, Δt) points in MATLAB by typing plot(dx,dt) where dx,dt
are vector coordinates of the (Δx, Δt) points. On the Figure menu, click Tools,then
Basic Fitting, and check Show equations and choose a type of plot which best ﬁts
the data. Write this as a relationship between Δt and Δx.
Oscillations in linear problems can be difﬁcult to see, so it is best to catalyze
any slight oscillations with oscillatory variation in the initial condition, or for a
more extreme response, deﬁne the initial condition so that it fails to meet one
or more boundary condition. Notice that the IBVP will no longer have a unique
theoretical solution, but the numerical method will force an approximate solution to
the PDE and match the conditions as best as it can. If oscillations are permitted by
the method, then they will be clearly evident in this process.
Research Project 1. In PDE_Analysis_Setup.m, set rho=0 on line 12
and multiply line 16 by zero to keep the same number of elements,
u0 = 0*polyval(polyfit(...
Investigate
lowest
Δt
values
when
oscillations
occur
for
Δx
=
0.05, 0.1, 0.5, 1 and ﬁt the points with the Basic Fitting used in Challenge
Problem 1. Then, investigate a theoretical bound on the error growth
factor (37) for the Crank-Nicolson method to the Test equation which
approximates the ﬁtting curve. It may be helpful to look for patterns in the
computed eigenvalues at those (Δx, Δt) points.

Numerical Investigations of Nonlinear PDEs
289
0
2
4
6
8
10
0
2
4
6
8
0
0.2
0.4
0.6
0.8
1
x
Iterations
U
0
2
4
6
8
10
0
5
10
15
−5
0
5
x
Iterations
U
Fig. 4 Example graphs of Newton’s method using the parameter values a = 0, b = 0, L = 10, δ =
1, Δx =
1
20 , ρ = 1, degree = 2, c = 1 and replacing the default initial condition with (left)
sin  πt
L
 and (right) sin  2πt
L
 in PDE_Analysis_Setup.m found in Appendix 5.3.
4
Parameter Analysis
Though parameters are held ﬁxed when solving a PDE, varying their values can
have an interesting impact upon the shape of the solution. We will focus on how
changing parameters values and initial conditions affect the end behavior of IBVPs.
For example, Figure 4 compares two very different steady-state solutions based
upon similar sinusoidal initial conditions. Taking the limit of parameters in a given
model can help us see between which limiting functions the steady-state solutions
tend.
4.1
Steady-State Solutions
To consider steady-state solutions to the general reaction diffusion model (2), we
set the time derivative to zero to create the boundary-value problem (BVP) with
updated u ≡u(x) satisfying
0 = δuxx + R(u),
(40)
u(0) = a,
u(L) = b.
LeVeque [16], Marangell et. al. [10], and Aron et. al. [1] provide insightful
examples and helpful guides for analyzing steady-state and traveling wave solutions
of nonlinear PDEs. We will follow their guidelines here in our analysis of steady-
states of the Fisher-KPP equation (3). Notice that as δ →∞, equation (40)
simpliﬁes as 0 = uxx + 1
δ R(u) →0 = uxx. Under this parameter limit, all straight
lines are steady-states, but only one,
f∞(x) = b −a
L
x + a,
(41)

290
R.C. Harwood
ﬁts the boundary conditions u(0, t) = a, u(L, t) = b. On the other hand, as δ →0,
solutions to equation (40) tend towards equilibrium points, 0 = R(¯u), of the reaction
function inside the interval (0, L). Along with the ﬁxed boundary conditions, this
creates discontinuous limiting functions
f (¯u)
0 (x) =
⎧
⎨
⎩
a, x = 0
¯u, 0 < x < L,
b, x = L
(42)
deﬁned for each equilibrium point ¯u.
Example 10. The Fisher-KPP equation (3) reduces to the BVP with updated u ≡
u(x) satisfying
0 = δuxx + ρu(1 −u),
(43)
u(0) = 0,
u(10) = 1,
As parameter δ varies, the steady-state solutions of (43) transform from one of the
discontinuous limiting functions
f (¯u)
0 (x) =
⎧
⎨
⎩
0, x = 0
¯u, 0 < x < 10,
1, x = 10
(44)
for equilibrium ¯u = 0 or ¯u = 1, towards the line
f∞(x) = 1
10x.
(45)
4.2
Newton’s Method
The Taylor series expansion for an analytical function towards a root, that is 0 =
f(xn+1), gives us
0 = f(xn) + Δxf ′(xn) + O
	
Δx2
(46)
Truncating O
	
Δx2
terms, and expanding Δx = xn+1 −xn, an actual root of f(x)
can be approximated using Newton’s method [3]. Newton’s iteration method for a
single variable function can be generalized to a vector of functions G(u) solving
a (usually nonlinear) system of equations 0 = G(u) resulting in a sequence of
vector approximations {U(k)}. Starting near enough to a vector of roots which is
stable, the sequence of approximations will converge to it: limk→∞U(k) = Us.

Numerical Investigations of Nonlinear PDEs
291
Determining the intervals of convergence for a single variable Newton’s method can
be a challenge; even more so for this vector version. Note that the limiting vector of
roots is itself a discretized version of a solution, Us = u(x), to the BVP system (40).
Deﬁnition 14 (Vector Form of Newton’s Method). For system of equations 0 =
G(u),
U(k+1) = U(k) −J−1(U(k))G(U(k))
(47)
where J(U(k)) is the Jacobian matrix, ∂Gi(u)
∂Uj , which is the derivative of G(u) in
RM×M where M is the number of components of G(U(k)) [16].
Using the standard centered difference, we can discretize the nonlinear BVP (40)
to obtain the system of equations 0 = G (Un)
0 = δDUn + ρUn (1 −Un)
(48)
We need an initial guess for Newtons method, so we will use the initial condition
u0 from the IBVP (3). Table 3 shows the change in the solution measured by
||U(k+1) −U(k)||∞= ||J−1G||∞in each iteration. As expected, Newton’s method
appears to be converging quadratically, that is ϵ(k+1) = O
	
(ϵ(k))2
according to
Deﬁnition 15.
Deﬁnition 15. Given an iteration which converges, the order of convergence N is
the power function relationship which bounds subsequent approximation errors as
ϵ(k+1) = O

(ϵ(k))N
.
(49)
Note that scientiﬁc notation presents an effective display of these solution
changes. You can see that the powers are essentially doubling every iteration in
column 4 of Table 3 for the Approximate Errora in the Fisher-KPP equation. This
second order convergence, visually demonstrated in Figure 5, can be more carefully
measured by computing
order(k) = round

log
	
ϵ(k+1)
log
	
ϵ(k)

where rounding takes into account the variable coefﬁcient C in deﬁnition 15. For
instance, values in column 4 slightly above 2 demonstrate C < 1 and those
slightly below 2 demonstrate C > 1. Thus it is reasonable to round these to the
nearest integer. This is not evident, however, for the Test equation because the error
suddenly drops near the machine tolerance and further convergence is stymied by
round-off error. If we start with a different initial guess U(0) (but still close enough
to this solution), we would ﬁnd that the method still converges to this same solution.

292
R.C. Harwood
Table 3 Verifying Convergence of Newton’s Method
Test Equation
Fisher-KPP Equation
Approximate
Order of
Approximate
Order of
Iteration
Errora
Convergenceb
Errora
Convergenceb
1
1.6667e-01
18.2372
1.5421e+00
0 (-0.4712)
2
6.4393e-15
0.9956
8.1537e-01
-1 (-0.5866)
3
7.4385e-15
1.0585
1.1272e+00
-5 (-5.0250)
4
tol.c reached
5.4789e-01
2 (2.4533)
5
2.2853e-01
2 (2.2568)
6
3.5750e-02
2 (2.0317)
7
1.1499e-03
2 (2.0341)
8
1.0499e-06
2 (1.9878)
9
1.3032e-12
2 (1.2875)
10
tol.c reached
a Approximate error measured as the maximum absolute difference (|| · ||∞) between one iteration
and the next.
b Order of convergence is measured as the power each error is raised to produce the next: ϵi+1 =
ϵp
i →p = log(ϵi+1)/ log(ϵi)
c Stopping criterion is reached when error is less than tol = 10ϵ = 2.2204e −15.
Newton’s method can be shown to converge if we start with an initial guess that is
sufﬁciently close to a solution. How close depends on the nature of the problem. For
more sensitive problems one might have to start extremely close. In such cases it
may be necessary to use a technique such as continuation to ﬁnd suitable initial data
by varying a parameter, for example [16].
The solution found in Figure 6 for δ = 1 is an isolated (or locally unique) solution
in the sense that there are no other solutions very nearby. However, it does not follow
that this is the unique solution to the BVP (43) as shown by the convergence in
Figure 7 to another steady-state solution. In fact, this steady-state is unstable for the
Fisher-KPP equation (3), as demonstrated in Figure 7.
Project Idea 3.
For δ = 1, use Newton’s method in example code in
Section 5.2 in the Appendix to investigate steady-state solutions to the
bounded Fisher-KPP equation (43). Note the behavior of limiting solutions
found in relation to the initial condition used. Also, note the shape of functions
for which Newton’s method is unstable. One example behavior for a speciﬁc
quadratic is shown in Figure 6

Numerical Investigations of Nonlinear PDEs
293
1
2
3
4
5
6
7
8
9
10
−16
−14
−12
−10
−8
−6
−4
−2
0
2
Iterations k
Power of Relative Error (base 10)
Fig. 5 Log-plot of approximate relative error, ε(k)r
= log1 0(max |U(k + 1) −U(k)|) in
Newton’s method for the Fisher-KPP equation (3) using the default parameter values a = 0, b =
1, L = 10, δ = 1, Δx = 0.05, ρ = 1, degree = 2, c =
1
3 and initial condition as given in
PDE_Analysis_Setup.m found in Appendix 5.3.
Project Idea 4.
Find an initial condition which tends to a steady-state
different than either f0(x) or f∞(x). Investigate how the shape of the solution
changes as δ →0 and as δ →∞. Does it converge to a limiting function at
both ends?
Project Idea 5.
Once you ﬁnd an initial condition which tends to a
steady-state different than either f0(x) or f∞(x), run PDE_Solution.m
in Appendix 5.2 to investigate the time stability of this steady-state using
the built-in solver initialized by this steady-state perturbed by some small
normally distributed noise.

294
R.C. Harwood
0
1
2
3
4
5
6
7
8
9
10
−0.5
0
0.5
1
x
U
δ=0
δ =1
δ=∞
Fig. 6 Graph of three steady-state solutions to BVP (43) by Newton’s method for a δ-parameter
range of (1) δ = 0, (2) δ = 1, and (3) δ →∞using the other default parameter values
a = 0, b = 1, L = 10, Δx =
1
20, ρ = 1, degree = 2, c =
1
3 and initial condition as given in
PDE_Analysis_Setup.m found in Appendix 5.3.
Fig. 7 Demonstration of the instability of the steady-state in Figure 6 for δ = 1 as an initial
condition for the Fisher-KPP equation (3) adding (a) no additional noise and (b) some normally
distributed noise to slightly perturb the initial condition from the steady-state using the other default
parameter values a = 0, b = 1, L = 10, Δx =
1
20 , ρ = 1, degree = 2, c =
1
3 and ﬁrst initial
condition as given in
PDE_Analysis_Setup.m found in Appendix 5.3.
Note, the randn(i,j) function in MATLAB is used to create noise in PDE
_Solution.m using a vector of normally distributed psuedo-random variables
with mean 0 and standard deviation 1.

Numerical Investigations of Nonlinear PDEs
295
Newton’s method is called a local method since it converges to a stable solution
only if it is near enough. More precisely, there is an open region around each solution
called a basin, from which all initial functions behave the same (either all converging
to the solution or all diverging) [3].
Project Idea 6.
It is interesting to note that when b = 0 in the Fisher-KPP
equation, δ-limiting functions coalesce, f∞(x) = f0(x) ≡0, for ¯u = 0.
Starting with δ = 1, numerically verify that the steady-state behavior as
δ →∞remains at zero. Though there are two equilibrium solutions as δ →0,
¯u = 0 and ¯u = 1, one is part of a continuous limiting function qualitatively
similar to f∞(x) while the other is distinct from f∞(x) and discontinuous.
Numerically investigate the steady-state behavior as δ →0 starting at δ = 1.
Estimate the intervals, called Newton’s basins, which converge to each distinct
steady-state shape or diverge entirely. Note, it is easiest to distinguish steady-
state shapes by number of extremum. For example, smooth functions tending
towards f (1)
0
(x) have one extrema.
4.3
Traveling Wave Solutions
The previous analysis for ﬁnding steady-state solutions can also be used to ﬁnd
asymptotic traveling wave solutions to the initial value problem
ut = δuxx + R(u),
(50)
u(x, 0) = u0(x), −∞< x < ∞, t ≥0,
by introducing a moving coordinate frame: z = x −ct, with speed c where c > 0
moves to the right. Note that by the chain rule for u(z(x, t))
ut = uzzt = −cuz
(51)
ux = uzzx = uz
uxx = (uz)zzx = uzz.
Under this moving coordinate frame, equation (50) transforms into the boundary
value problem
0 = δuzz + cuz + R(u),
(52)
−∞< z < ∞,

296
R.C. Harwood
Project Idea 7.
Modify the example code PDE_Analysis_Setup
to introduce a positive speed starting with c
=
2 (updated after
the
initial
condition
is
deﬁned
to
avoid
a
conﬂict)
and
adding
+c/dx*spdiags(ones(M-2,1)*[-1 1 ],[-1 0], M-2, M-2)
to the line deﬁning matrix D and update BCs(1) accordingly. Once you have
implemented this transformation correctly, you can further investigate the
effect the wave speed c has on the existence and stability of traveling waves
as analyzed in [10]. Then apply this analysis to investigate traveling waves
of the FitzHugh-Nagumo equation (53) as steady-states of the transformed
equation.
5
Further Investigations
Now that we have some experience numerically investigating the Fisher-KPP
equation, we can branch out to other relevant nonlinear PDEs.
Project Idea 8.
Use MATLAB’s ode23s solver to investigate asymptotic
behavior of other relevant reaction-diffusion equations, such as the FitzHugh-
Nagumo equation (53) which models the phase transition of chemical
activation along neuron cells [8]. First investigate steady-state solutions
and then transform the IBVP to investigate traveling waves. Which is more
applicable to the model? A more complicated example is the Nonlinear
Schrödinger equation (54), whose solution is a complex-valued nonlinear
wave which models light propagation in ﬁber optic cable [15].
For the FitzHugh-Nagumo equation (53)
ut = δuxx + ρu(u −α)(1 −u), 0 < α < 1
(53)
u(x, 0) = u0(x),
u(0, t) = 0
u(10, t) = 1
For the Nonlinear Schrödinger (54), the boundary should not interfere or add to
the propagating wave. One example boundary condition often used is setting the
ends to zero for some large L representing ∞.

Numerical Investigations of Nonlinear PDEs
297
ut = iδuxx −iρ|u|2u,
(54)
u(x, 0) = u0(x),
u(−L, t) = 0
u(L, t) = 0
Additionally, once you have analyzed a relevant model PDE, it is helpful to
compare end behavior of numerical solutions with feasible bounds on the physical
quantities modeled. In modeling gene propagation, for example, with the Fisher-
KPP equation (3), the values of u are amounts of saturation of the advantageous
gene in a population. As such, it is feasible for 0 < u < 1 as was used in the
stability analysis. The steady-state solution shown in Figure 6, which we showed
was unstable in Figure 7, does not stay bounded in the feasible region. This is
an example where unstable solutions represent a non-physical solution. While
investigating other PDEs, keep in mind which steady-states have feasible shapes
and bounds. Also, if you have measurement data to compare to, consider which
range of parameter values yield the closest looking behavior. This will help deﬁne a
feasible region of the parameters.
Appendix
5.1
Proof of Method Accuracy
Numerical methods, such as those covered in Section 2.2, need to replace the
underlying calculus of a PDE with basic arithmetic operations in a consistent
manner (Deﬁnition 17) so that the numerical solution accurately approximates
the true solution to the PDE. To determine if a numerical method accurately
approximates the original PDE, all components are rewritten in terms of common
function values using Taylor series expansions (Deﬁnition 16).
Deﬁnition 16. Taylor series are power series representations of functions at some
point x = x0 + Δx using derivative information about the function at nearby point
x0.
f(x) = f(x0)+f ′(x0)Δx+ f ′′(x0)
2!
Δx2 +. . .+ f (n)(x0)
n!
Δxn +O
	
Δxn+1
,
(55)
where the local truncation error in big-O notation, ϵL = O
	
Δxn+1
, means ||ϵL|| ≤
C
	
Δxn+1
for some positive C [3].
For a function over a discrete variable x = (xm), the series expansion can be
rewritten in terms of indices Um ≡U(xm) as

298
R.C. Harwood
Um+1 = Um + U′
mΔx + U′′
m
2! Δx2 + . . . + U(n)
m
n! Δxn + O
	
Δxn+1
,
(56)
Deﬁnition 17. A method is consistent if the difference between the PDE and the
method goes to zero as all the step sizes, Δx, Δt for example, diminish to zero.
To quantify how accurate a consistent method is, we use the truncated terms from
the Taylor series expansion to gauge the order of accuracy [3].
Deﬁnition 18. The order of accuracy in terms of an independent variable x is the
lowest power p of the step size Δx corresponding to the largest term in the truncation
error. Speciﬁcally,
PDE −Method = O (Δxp) .
(57)
Theorem 6. If a numerical method for a PDE has orders of accuracy greater
than or equal to one for all independent variables, then the numerical method is
consistent.
Proof. Given that the orders of accuracy p1, . . . , pk ≥1 for independent variables
x1, . . . , xk, then the truncation error
PDE −Method = O (Δxp1
1 ) + · · · + O (Δxpk
k )
(58)
goes to zero as Δx1, . . . , Δxk →0 since Δxpi
i →0 as Δxi if pi ≥1.
Example 11. (Semi-Implicit Crank-Nicolson Method) The accuracy in space for
the Semi-Implicit Crank-Nicolson method is deﬁned by the discretizations of the
spatial derivatives. Following example (refex:discretization), a centered difference
will construct a second order accuracy in space.
δ
Δx2
	
Un
m−1 −2Un
m + Un
m+1

−δ(Un
m)xx
(59)
=
δ
Δx2
	
Δx2(Un
m)xx + O
	
Δx4

−δ(Un
m)xx
= O
	
Δx2
.
Then the discretized system of equations includes a tridiagonal matrix D with entries
Di,i−1 =
δ
Δx2 , Di,i = −2δ
Δx2 , Di,i+1 =
δ
Δx2 and a resultant vector R(Un) discretized
from R(u). The order of accuracy in time can be found by the difference between the
method and the spatially discretized ODE system. For the general reaction-diffusion
equation (2),
Un+1 −Un
Δt
−1
2
	
DUn + DUn+1 + R(Un) + R(Un+1)

(60)

Numerical Investigations of Nonlinear PDEs
299
−(Un
t −DUn −R(Un))
= (Un
t + O(Δt)) −1
2 (2DUn + O(Δt) + 2R(Un) + O(ΔU))
−(Un
t −DUn −R(Un))
= O(Δt)
since O(ΔU) = Un+1 −Un = ΔtUn
t + O(Δt2).
Example 12. (Improved Euler Method) Similar to Example 11, the accuracy in
space for the Improved Euler version of the Crank-Nicolson method (18) is deﬁned
by the same second order discretizations of the spatial derivatives. The order of
accuracy in time can be found by the difference between the method and the spatially
discretized ODE system. For the general reaction-diffusion equation (2),
Un+1 −Un
Δt
−1
2 (DUn + DU∗+ R(Un) + R(U∗))
(61)
−(Un
t −DUn −R(Un))
=

Un
t + Δt
2 Un
tt + O

Δt2
−1
2

2DUn + Δt

D2Un + DUnR(Un)

+O

Δt2
+ 2R(Un) + (DUn + R(Un)) Ru(Un) + O

Δt2
−(Un
t −DUn −R(Un))
= O

Δt2
since
Un
tt = D2Un + DR(Un) + (DUn + R(Un)) Ru(Un),
DU∗= DUn + Δt
	
D2Un + DR(Un)

,
R(U∗) = R(Un) + Δt (DUn + R(Un)) Ru(Un) + O
	
Δt2
.
5.2
Main Program for Graphing Numerical Solutions
%% PDE_Solution.m
% Numerical Solution with Chosen Solver
PDE_Analysis_Setup;
% call setup script
tspan = [0 40];
% set up solution time interval
% Comment when calling Crank-Nicolson method
[t,U] = ode23s(f, tspan, u0);
% Adaptive Rosenbrock Method
N = length(t);
dt = max(diff(t));
% Uncomment to call semi-implicit Crank-Nicolson method
% dt = 1;
% t = 0:dt:tspan(end);
% N = length(t);

300
R.C. Harwood
% U = CrankNicolson_SI(dt,N,D,R,u0,BCs);
U = [a*ones(N,1),U,b*ones(N,1)];
% Add boundary values
figure();
% new figure
mesh(x,t,U);
% plot surface as a mesh
title(’Numerical Solution to PDE’);
%% Analyze min/max step ratios and eigenvalues of numerical method matrix
ratio_range = [delta*min(diff(t))/dx^2, delta*max(diff(t))/dx^2],
% Example for semi-implicit CN method for Fisher-KPP and Test equation
Mat=(eye(size(D))-dt/2*D)\(eye(size(D))+dt/2*D+rho*dt*diag(1-U(end,2:end-1)));
eigenvalues = eig(Mat);
eig_real_range = [min(real(eigenvalues)),max(real(eigenvalues))],
spectral_radius = max(abs(eigenvalues)),
figure(); plot(real(eigenvalues),imag(eigenvalues),’*’);
title(’Eigenvalues’);
%% Call Newton’s Root finding method for finding steady state solution
[U_SS,err] = Newton_System(G,dG,u0);
U_SS = [a*ones(length(err),1),U_SS’,b*ones(length(err),1)]; % Add boundary values
figure(); plot(x,U_SS(end,:));
title(’Steady-State Solution by Newton‘s Method’)
% figure(); mesh(x,1:length(err),U_SS);
% title(’Convergence of Newton Iterations to Steady-State’);
figure(); plot(log10(err)); %plot(-log10(err));
title(’Log-Plot of Approximate Error in Newton‘s Method’)
order = zeros(length(err));
for i=1:length(err)-1, order(i) = log10(err(i+1))/log10(err(i)); end
%% Investigate stability of Steady-State Solution by Normal Perturbation
noise = 0.1;
[t,U] = ode23s(f, tspan, U_SS(end,2:end-1) + noise*randn(1,M-2) );
N = length(t);
U = [a*ones(N,1),U,b*ones(N,1)];
% Add boundary values
figure(); mesh(x,t,U);
title(’Stability of Steady-State Solution’);
5.3
Support Program for Setting up Numerical Analysis of PDE
%% PDE_Analysis_Setup.m
% Setup Script for Numerical Analysis of Evolution PDEs in MATLAB
%
Example: u_t = delta*u_xx + rho*u(1-u), u(x,0) = u0, u(0,t)=a, u(L,t)=b
close all; clear all;
%close figures, delete variables
a = 0; b = 1;
% Dirichlet boundary conditions
L = 10;
% length of interval
delta = 1;
% creating parameter for diffusion coefficient
dx = 0.05;
% step size in space
x=(0:dx:L)’;
% discretized interval
M = length(x);
% number of nodes in space
rho = 1;
% scalar for nonlinear reaction term
% Initial condition as polynomial fit through points (0,a),(L/2,c),(L,b)
degree=2; c=1/3;
% degree of Least-Squares fit polynomial
u0 = polyval(polyfit([0 L/2 L],[a c b],degree),x(2:end-1));
% Discretize in space with Finite-Differences: Boundary and Inner Equations
%
(a - 2U_2 + U_3)*(delta/dx^2) = rho*(-U_2 + U_2.^2),
%
(U_{i-1} - 2U_i + U_{i+1})*(delta/dx^2) = rho*(-U_{i} + (U_{i})^2)
%
(U_{M-2} - 2U_{M-1} + b)*(delta/dx^2) = rho*(-U_{M-1} + U_{M-1}.^2)
D = delta/dx^2*spdiags(ones(M-2,1)*[1 -2 1],[-1 0 1], M-2, M-2);
BCs = zeros(M-2,1); BCs(1) = a*delta/dx^2; BCs(end) = b*delta/dx^2;

Numerical Investigations of Nonlinear PDEs
301
% Anonymous functions for evaluting slope function components
f = @(t,u) D*u + BCs + rho*(u - u.^2);
% whole slope function for ode23s
R = @(u) rho*(u - u.^2);
% nonlinear component function
G = @(u) D*u + BCs + rho*(u - u.^2);
% rewritten slope function
dG = @(u) D + rho*diag(ones(M-2,1)-2*u);% Jacobian of slope function
5.4
Support Program for Running the Semi-Implicit
Crank-Nicolson Method
%% CrankNicolson_SI.m
% Semi-Implicit Crank-Nicolson Method
% function [U]=CrankNicolson_SI(dt,N,D,R,u0,BCs)
%
Solving U_t = D*U + R(U) on Dirichlet boundaries vector BCs as
%
(I+dt/2*D)*U^(k+1) = (U^(k)+dt*(1/2*D*U+BCs+R(U^(k))) with time step dt,
%
time iterations N, matrix D, function R, and initial condition vector u0
function [U]=CrankNicolson_SI(dt,N,D,R,u0,BCs)
sizeD = size(D,1);
% naming computation used twice
U = zeros(sizeD,N);
% preallocating vector
U(:,1) = u0(:);
% initializing with u0 as column vector
for k = 1:N-1
U(:,k+1) = (eye(sizeD)-dt/2*D)\...
(U(:,k) + dt*(1/2*D*U(:,k) + BCs + R(U(:,k))));
end
U = U’;
% Transpose for plotting
end
5.5
Support Program for Verifying Method Accuracy
%% Method_Accuracy_Verification.m
% Script for running numerical method for multiple dt step sizes to verify
% order of accuracy
PDE_Analysis_Setup;
% call setup script
T = 10;
dt = 1; N = 1+T/dt;
Nruns = 10;
TestU = zeros(Nruns,1);
Difference = TestU; Change = TestU; SF = TestU;
midpt = ceil((M-1)/2);
%U = CrankNicolson_SI(dt,N,D,R,u0,BCs);
% Comment for ode23s
options = odeset(’InitialStep’,dt, ’MaxStep’, dt,’AbsTol’,max(u0));
[t,U] = ode23s(f, [0 T], u0, options);
% Comment for CN
Err = zeros(Nruns,1); SF = Err; Order = Err;
% Preallocate
fprintf(sprintf(’App. Error\t Sig. Figs\t Order\n’));
for k = 1:Nruns-1
dt=dt/2;
N = 1+T/dt;
Uold = U;
% U = CrankNicolson_SI(dt,N,D,R,u0,BCs);
% Comment for ode23s
options = odeset(’InitialStep’,dt, ’MaxStep’, dt,’AbsTol’,max(u0));
[t,U] = ode23s(f, [0 T], u0, options);
% Comment for CN
Err(k+1) = norm(U(end,:)-Uold(end,:),Inf)/norm(U(end,:),Inf);
SF(k+1) = floor(log10(.5/Err(k+1)));
Order(k) = log2(Err(k)/Err(k+1));
fprintf(sprintf(’%0.5g\t %d\t %0.5f\n’, Err(k),SF(k),Order(k)));
end

302
R.C. Harwood
5.6
Support Program for Running Newton’s Method for ODE
Systems
%% Newton’s Method for Systems of Equations
% function [U,err] = Newton_System(G,dG,u0)
%
Compute vector U satisfying G(U)=0 with Jacobian dG(U) using iterations
%
U^(k+1) = U^(k) - dG(U^(k))\G(U^(k)), starting with initial guess u0,
%
and having approximate errors err
function [U,err] = Newton_System(G,dG,u0)
tol = 10*eps;
% tolerance relative to machine epsilon
cutoff = 500;
err = zeros(cutoff,1);
% Preallocation
U = zeros(length(u0),length(err));
U(:,1)=u0;
for k=1:cutoff
U(:,k+1) = U(:,k) - dG(U(:,k))\G(U(:,k));
% Newton Iteration
err(k) = norm(U(:,k+1)-U(:,k),Inf);
% Estimate current error
if err(k) < tol,
% stopping criterion
err = err(1:k); U = U(:,1:k);
%truncate to current
break;
% break from loop
end
end
end
References
1. Aron, M., Bowers, P., Byer, N., Decker, R., Demirkaya, A., Ryu, J.H.: Numerical results on
existence and stability of steady state solutions for the reaction-diffusion and KleinGordon
equations. Involve 7(6), 723–742 (2014)
2. Britz, D., Osterby, O., Strutwolf, J.: Damping of Crank–Nicolson error oscillations. Comput.
Biol. Chem. 27(3), 253–263 (2003)
3. Burden, R.L., Faires, J.: Numerical Analysis, 9th edn. Brooks/Cole, Cengage Learning, Boston
(2011)
4. Christodoulou, D.M., Graham-Eagle, J., Katatbeh, Q.D.: A program for predicting the intervals
of oscillations in the solutions of ordinary second-order linear homogeneous differential
equations. Adv. Differ. Equ. 1, 1–23 (2016)
5. Crank, J., Nicolson, P.: A practical method for numerical evaluation of solutions of partial
differential equations of the heat-conduction type. Proc. Camb. Philos. Soc. 43(1), 50–67
(1947)
6. Evans, L.C.: Partial Differential Equations, 2nd edn. American Math Society, Providence, RI
(2010)
7. Fisher, R.A.: The wave of advance of advantageous genes. Ann. Eugen. 7, 353–369 (1937)
8. FitzHugh, R.: Impulses and physiological states in theoretical models of nerve membrane.
Biophys. J. 1(6) 445 (1961)
9. Gao, J., Song, F.: Oscillation analysis of numerical solutions for nonlinear delay differential
equations of hematopoiesis with unimodel production rate. Appl. Math. Comput. 264, 72–84
(2015)
10. Harley, K., Marangell, R., Pettet, G.J., Wechselberger, M.: Numerical computation of an Evans
function in the Fisher/KPP equation. ArXiv: 1312.3685v3 (2015)
11. Harwood, R.C., Edwards, D.B., Manoranjan, V.S.: IEEE:Trans. on Energy Conversion. 26(4),
1109–1117 (2011)
12. Horn, R.A., Johnson, C.R.: Matrix Analysis, 2nd edn. Cambridge University Press, New York
(2012)
13. Juhnke, S., Ratzkin, J.: A numerical investigation of level sets of extremal Sobolev functions.
Involve. 8(5), 787–799 (2015)

Numerical Investigations of Nonlinear PDEs
303
14. Kolmogorov, A.: Selected Works of A.N. Kolmogorov I: a study of the diffusion equation
with increase in the amount of substance, and its application to a biological problem, 2nd edn.
Kluwer, Boston (1991)
15. Lakoba, T.I.:Instability of the ﬁnite-difference split-step method applied to the nonlinear
Schrödinger equation. I. standing soliton. Numer. Methods Partial Differ. Equ. 32(3), 1002–
1023 (2016)
16. LeVeque, R.J.: Finite Difference Methods for Ordinary and Partial Differential Equations.
Society for Industrial Mathematics and Computation, Philadelphia, PN (2007)
17. Moler, C. B.: Numerical Computing with MATLAB: Revised Reprint. Society for Industrial
Mathematics and Computation, Philadelphia, PN (2008)
18. Nadin, G., Perthame, B., Tang, M.: Can a traveling wave connect two unstable states? The case
of the nonlocal Fisher equation. C.R. Math. 349(9–10), 553–557 (2011)
19. Osterby, O.: Five ways of reducing the Crank-Nicolson oscillations. BIT Numer. Math. 43(4),
811–822 (2003)
20. Sarra, S., Meador, C.: Stability properties of a predictor-corrector implementation of an implicit
linear multistep method. Involve. 4(1), 43–51 (2011)
21. Scarborough, J.B.: Numerical Mathematical Analysis, 6th edn. Johns Hopkins Press, Baltimore
(1966)
22. Thomas, J.W.: Numerical Partial Differential Equations, Finite Difference Methods, Texts in
Applied Mathematics, vol. 22. Springer, Heidelberg (1995)
23. von Neumann, J., Richtmeyer, R.D.: A method for the numerical calculation of hydrodynamic
shocks. J Appl. Phys. 21, 232–237 (1950)

Index
A
Activation strategy, 109
Adaptive methods, 266, 269
Adjacent, 228
Analysis operator, 151, 152, 158
Anticonformal symmetries, 58
Apex-planar graph, 89
Approximate relative error, 278
Area invariant, 66, 67
B
Backward Euler method, 271, 272
Basis, 145, 146, 148. See also Orthonormal
basis (ONB)
Binary matroids, 133, 138–140
Boundary-value problem (BVP), 289, 290
Boundary word invariants
bouquet of balloons, 69
Cayley graph of F/H, 68
Conway/Lagarias invariant, 69, 70
edges, orientation and labelling, 67
tiles in T3, 67, 68
Branch points, 42–43
order of, 42
C
Cartan subalgebra, 200
Cayley graph, 5–7, 68
Chebyshev method, 168
Chordal graph, 121–122
Chords, 130, 137, 139
Circuit elimination axiom, 135
Clique-relaxed coloring games, 102, 111–114,
124
Clustering coefﬁcients
avoidance of clustering, 240
connection between, 239
counting number of potential triangles, 238
effective contact, 238
network clustering coefﬁcient, 239
node clustering coefﬁcient, 238
normalized clustering coefﬁcients, 239–240
one-dimensional nearest-neighbor
networks, 236
of selected networks, 241
strong clustering, 240, 242
theorem about strong clustering, 242
Co-compact, 19–21
Coloring games, see Competitive graph
coloring
Coloring tile invariant, 66–67, 74–76
Combinatorial group theory, 61, 68, 69
Commutator, commutator subgroup, 44, 47
Compact Riemann surfaces, symmetry groups,
56–58
A4-actions
embedded action, 51–55
inﬂated tetrahedron, 40, 46
permutation notation, 38
punctured solid tetrahedron, boundary
of, 40, 41
regular tetrahedron, 38
rotation of tetrahedron, 38–39
tetrahedron with tripods, 40
classical surfaces, 36
deﬁnition of, 35
signatures, group action, 41–43
Competitive graph coloring
clique-relaxed coloring games, 102,
111–114, 124
edge coloring game, 102–103, 114–119,
124
game chromatic number (see Game
chromatic number)
map-coloring game, 99
relaxed coloring game, 108–109
© Springer International Publishing AG 2017
A. Wootton et al. (eds.), A Primer for Undergraduate Research, Foundations for
Undergraduate Research in Mathematics, DOI 10.1007/978-3-319-66065-3
305

306
Index
Competitive graph coloring (cont.)
activation strategy, 109
d-relaxed game chromatic number, 102,
108, 123
(r, d)-relaxed coloring game, 102, 108
tree strategy, 110–111
total coloring game, 103
chordal graph, 121–122
total activation strategy, 120–121
total game chromatic number, 120
Complete graph, 228
Completely apex (CA), 89
Completely contraction apex (CC), 89, 94
Completely edge deletion apex (CD), 89, 94
Complex inner product space, 147
COMSOL, 274
Concatenation, 2–4
Conformal automorphism, 36
Conformal, Poincaré disk model, 12
Conjecture, 108
Conjugate gradient method, 168
Connected components
diameter of, 229, 247
distribution of sizes, 234
of Erd˝os-Rényi Random Graphs, 233–236
giant component, 235, 252
of node in a graph, 229
with range of different sizes, 234
Connection probability, 232
Contact network models
connected component of node in graph, 229
edges, 227–228
mean degree, 228
nodes/vertices, 227
one-dimensional nearest-neighbor network,
229
path length, 229
k-regular graphs, 228
Convex programming
classes of convex programs
least-squares optimization, 186
linear program (LP), 185
portfolio optimization, 186–187
quadratically-constrained quadratic
program (QCQP), 187
quadratic program (QP), 185–186
second-order cone program (SOCP),
188
semi-deﬁnite program (SDP), 189
convex functions, 183–185
convexity, deﬁned, 182
convex sets, 182–183
minimizing convex problems, 182
solvers, 190
Conway/Lagarias invariant, 62–63, 65, 69, 70
Coxeter cell of type T, 29
Coxeter cellulation, 27, 30–31
Coxeter cell of type T, 29
Euclidean representations, 28
Coxeter groups, 2, 31–32, 206
CW-complexes (see CW-complexes)
Davis complex, 31–32
Coxeter cellulation, 27–31
mirror cellulation, 26–28, 31
spherical subsets, 24, 26
strict fundamental domain, 24–26
and geometry
Euclidean space and reﬂections, 11
facts, 10
hyperbolic geometry and reﬂections, 11
Poincaré disk model, hyperbolic space,
12–14
spherical geometry and reﬂections, 11
order 2 generators, reﬂections, 7
presentation of, 8–10
Crank-Nicolson method, 272, 278, 279,
284–285
CW-complexes
cellular decomposition of torus, 17, 18
cellulation of 2-sphere, 17
Euler characteristic, 18
group actions, 18–23
homeomorphic examples of 2-balls, 15–16
quotient space, 16
Cycle matroid, 127, 132–133
D
Davis complex, Coxeter groups, 31–32
Coxeter cellulation, 27
Coxeter cell of type T, 29
Euclidean representations, 28
mirror cellulation, 26–28, 31
spherical subsets, 24, 26
strict fundamental domain, 24–26
Decision variables, 172
Deﬁcient rectangle, 64
Degree of a node, 228
Difference vectors, tile invariants, 72–74
Differential equations, 265, 266
numerical methods, 269–273
ODE (see Ordinary differential equation
(ODE))
PDEs (see Partial differential equations
(PDEs))
Dihedral groups, 2
Directed graph, 4–5
Dirichlet boundary conditions, 267

Index
307
Discrete Fourier transform (DFT) matrix, 159
Domino tilings, 79
Dot product, 147
d-relaxed game chromatic number, 102, 108,
123
Dynkin diagrams, 24
E
Edge coloring game, 102–103, 114–119, 124
Edge contraction, 85, 86, 89
Eigenvalues, 281–282
Elementary reduction, 2, 3
Embeddable symmetry, see Compact Riemann
surfaces, symmetry groups
Empty word, 2
Enumeration, tiling
domino tilings, 79
L-trominoes, 79–80
permutations, 80–81
ribbon tile, 80
Epidemiology, 224
Equiangular tight frames (ETFs), 146, 150,
158
deﬁnition of, 160
and graphs
regular two-graphs, 162
strongly regular graphs, 163
two-distance tight frame, 163
Grassmannian frames, 150, 160, 164
k-angle tight frames, 160–162, 164–166
Welch bound, 159, 160
Erd˝os-Rényi random graphs
“asymptotically almost surely” properties
convergence of mean degree, 232
connected components, 233–234
deﬁnition, 231–232
parameter settings for, 233
random graphs, 230–231
Error analysis, PDE
convergence, 280–281
oscillatory behavior, 286–288
stability, 281–286
verifying accuracy, 277–280
Euclidean distance, 149
Euclidean space, 11
Euler characteristic, 18, 32. See also Orbihedral
Euler characteristic
Explicit method, 271–273
F
Fibonacci numbers, 79, 209
Final size, 227
Finite dimensional inner product space, 151,
153–156
Finite frame theory, 146
Finite index torsion free subgroup, 23
Finitely presented group
Cayley graphs, 5–7
Coxeter groups (see Coxeter groups)
Fisher-KPP equation, 268
adaptive Rosenbrock method, 269
discretization in space, 270–271
MATLAB, 279, 280
Newton’s method, 274, 275, 289–295
semi-implicit Crank-Nicolson method
accuracy, 278–279
eigenvalues, 281, 282, 285, 286
non-negative eigenvalue condition for,
287
von Neumann stability analysis, 286
steady-state solutions, 289–290
two-level matrix form, Forward Euler
method, 272
FitzHugh-Nagumo equation, 296
Forbidden minors, see Graph minor theorem,
forbidden minors
Forward Euler method, 271, 272
Fourier stability analysis, see von Neumann
stability analysis
Frame operator, 152–155
Frames
algorithms, 168
designing, 167
equiangular tight frames, 146, 150, 158
deﬁnition of, 160
and graphs, 162–16
Grassmannian frames, 150, 160, 164
k-angle tight frames, 160–162, 164–166
Welch bound, 159, 160
in ﬁnite dimensional spaces, 151–159
random noise, 168–169
redundancy, 145
in signal processing, 146
Free group, 3, 7, 9, 13
G
Game chromatic index, 103
Game chromatic number, 124
d-relaxed game chromatic number, 102,
123
k-clique-relaxed game chromatic number,
102, 112, 113
total game chromatic number, 120
for trees and forests, 101–102
2-coloring game, 103–104

308
Index
Game chromatic number (cont.)
game chromatic numbers 3 and 4,
104–108
Generalized handle operator, 54
Generating vector, 44
Genus, compact Riemann surface, 36
Geometric group theory, see Group
Geometric realization, 25
Geometry and Coxeter groups
facts, 10
hyperbolic geometry and reﬂections, 11
Poincaré disk model, hyperbolic space,
12–14
spherical geometry and reﬂections, 11
Gram matrix, 158, 163, 165
Gram-Schmidt orthogonalization process, 147
Graphic matroids, 132, 133, 135
Graph minor theorem, forbidden minors
edge contraction, 85, 86
minor closed, 86
minor minimal P graphs, 86, 87
properties with known MM P/Kuratowski
set, 94–96
apex-planar, 89
bounded tree-width, graphs of, 87
CACD, 94
completely apex, 89
completely contraction apex, 89
completely edge deletion apex, 89
edge deletion and contraction, 89
not contraction apex, 89
not edge deletion apex, 89
order, size, 87
outerplanarity, 88
outerprojective planar property, 88
outertoroidal property, 88
‘spherical’ graph, 88
toroidal graphs, 88
SAP property (see Strongly almost-planar
(SAP) graph)
Graphs
Coxeter graph (see Coxeter groups)
cycle, 128
edges, 128
and ETFs, 162–163
group presentations
Cayley graphs, 5–7
directed graph, 4–5
paths, 128
simple and planar, 128
UPC graphs (see Uniquely pancyclic (UPC)
graphs)
vertex, incident, 128
vertices, 128
Grassmannian frames, 150, 159, 160, 164
Group, 14–15
Coxeter groups (see Coxeter groups)
CW-complexes
cellular decomposition of torus, 17, 18
cellulation of 2-sphere, 17
Euler characteristic, 18
group actions, 18–23
homeomorphic examples of 2-balls,
15–16
quotient space, 16
deﬁnition of, 1
presentations and graphs
Cayley graph, 5–7
directed graph, 4–5
set WA of words, 2–4
torsion-free subgroups, 32–33
H
Hamiltonian circuit, 136, 137, 139–143
Hamiltonian cycle, 130
Hamiltonian matroid, 139, 141
Handle operator, 53
Heat equation, 265, 269
Height invariant, 75, 76
Height-1 ribbon tile tetrominoes, 75–77
Homogeneity of hosts, 226
Hosts, 224
Hyperbolic geometry, 11
I
Implicit methods, 271
Improved Euler Crank-Nicolson method, 273,
299
Index case, 226
Induced subgraph, 228
Infectious, 225
Inﬂated tetrahedron surface, 40, 46
Initial boundary value problem (IBVP), 267,
268, 280
Initial value problem (IVP), 267, 268, 280
Inner product space, 147
IONTW software tool
construction of random graphs
implemented in, 231
contact network inﬂuence on simulation
outcomes, 225
end-infection-prob, 233
exploring clustering coefﬁcients, 241
guide to small world (property), 243–248
infection-prob, 233
interface, 233

Index
309
one-dimensional nearest- neighbor
network, 236
small world models in, 250–251, 260
vaccination of speciﬁc subset of hosts, 256
Irreducible highest weight representation with
highest weight, 202
Isomorphic, matroids, 133
K
k-angle tight frames, 160–162, 164–166
k-clique-relaxed game chromatic number, 102,
112, 113
k-clique-relaxed r-coloring game, 102, 112
Kostant’s partition function, 196, 198, 203, 214
Kostant’s weight multiplicity formula, 194,
197, 203
Kuratowski set, 94–96
apex-planar, 89
bounded tree-width, graphs of, 87
CACD, 94
completely apex, 89
completely contraction apex, 89
completely edge deletion apex, 89
edge deletion and contraction, 89
not contraction apex, 89
not edge deletion apex, 89
order, size, 87
outerplanarity, 88
outerprojective planar property, 88
outertoroidal property, 88
SAP property (see Strongly almost-planar
(SAP) graph)
‘spherical’ graph, 88
toroidal graphs, 88
L
Lax Equivalence Theorem, 281
Lax-Richtmeyer Theorem, 280
Lie algebra
act as identity map on vector space,
199–200
adjoint representation, 200
Cartan subalgebra, 200
under cross product of vectors, 199
dominant integral weights, 202
exceptional Lie algebras, 199
and Fibonacci numbers, 209
ﬁnite-dimensional irreducible
representation, 218
fundamental weights, 201–202, 207,
208
as highest weight representation, 197
irreducible ﬁnite-dimensional
representation, 197, 202, 214,
218
isomorphism, 199
Lie algebra homomorphism, 199
Lie bracket, 199
and Lie group, 201
positive root, 204
root space decomposition, 201
set of positive roots, 194
simple root reﬂection, 201
trivial representation, 199
uniqueness of simple roots, 201
weights of adjoint representations, 200–201
and Weyl group, 196, 205
Weyl group and, 201
zero weight in Weyl alternation diagram,
218
zero weight space in, 205
Linear maximization problem, 175
Linear partial differential equations, 267
Linear programming (LP)
decision variables, 172
deﬁned, 171–172
diet problem, 173–174
duality
multiplier, 180
optimal cost, 181
standard-form problem, 180
strong duality, 181
unconstrained problem, 181
parameters, 172
sensitivity analysis, 172
solution of LPs
algorithmic computational complexity,
177
global optimum, 176
interior point method, 178
simplex method, 177–178
transportation problem, 178–179
weakly polynomial method, 177
standard forms
with constraints, 176
decision-making framework, 174
to eliminate free variables, 176
feasible region, 175
general form of an LP, 175
non-negativity constraints, 175
slack or surplus variables, 175
vertices of the feasible region, 174
technique for optimization, 173
Linear scaling, 246
Linklessly embeddable graphs, 89
Local connectivity, tile invariants, 70–72

310
Index
Local move property, 71, 72
Logarithmic scaling, 247
Lower frame bound, 151, 156, 157, 159
L-trominoes, 64, 65, 79–80
M
Major outbreak, 227
Map-coloring game, 99
Maple, 274
Mathematica, 274
Mathematical epidemiology, 224
MATLAB, 269, 274–276
Matroids, 128
abstract properties, 131
axiomatically-deﬁned structure, 131
binary matroid, 133, 138–140
circuit elimination axiom, 135
cycle matroid, 127, 132–134
deletions and contractions, 136
dependence, 131
dual, 135, 136
graphic matroid, 132, 133, 135
independence, 127, 131, 133
independence augmentation axiom, 131
isomorphic, 133
rank, 134
regular matroid, 133
uniform matroid, 134
UPC matroids
binary UPC matroids, 140–143
Hamiltonian circuit, 136, 137
non-isomorphic graph, 138
rank-24 UPC matroid, 137
vector matroid, 132–133
Mean-squared error (MSE), 167, 169
Mercedes-Benz (MB) frame, 150, 158, 162
Milgram’s “Six Degrees of Separation”
experiment, 242–243
Minor outbreak, 227
N
Newton’s method, 271, 274–276, 289–295,
302
Next-generation models, 226
Nonlinear partial differential equations, 266,
267, 272. See also Partial differential
equations (PDEs)
IBVP, 268
improved Euler Crank-Nicolson method,
273
steady-state solutions, 289–290
traveling wave solutions, 295, 296
Non-negative eigenvalue condition, 287–288
Normalized clustering coefﬁcients, 239
Numerical oscillation, 287–288
Numerical partial differential equations, 266
O
Orbihedral Euler characteristic, 21–22, 32
Orbit, 41
fundamental domain, 41
Orbit-stabilizer theorem, 42
Ordinary differential equation (ODE)
deﬁnition, 266
initial value problem, 267
Newton’s method, support program for, 302
oscillatory behavior, 286–288
slope function, 267, 270, 280
Orthogonal set, 147
Orthonormal basis (ONB), 147–150, 152, 153,
156, 159
Outerplanarity, 88
Outerprojective planar property, 88
Outertoroidal property, 88
P
Parseval’s formula, 148, 151, 153
Partial differential equations (PDEs), 265,
296–297
accuracy, 297–299
adaptive Rosenbrock method, 266, 269
error analysis
convergence, 280–281
oscillatory behavior, 286–288
stability, 281–286
verifying accuracy, 277–280
evolution PDEs, 267–268, 270
Fisher-KPP equation (see Fisher-KPP
equation)
IBVP, 267, 268
linear PDEs, 267
linear Test equation, 269
MATLAB, 269, 274–276
multivariable functions, 267
numerical solution, main program for,
299–300
parameter analysis
Newton’s method, 289–295
steady-state solutions, 289–290
traveling wave solutions, 295, 296
partial derivative, 267
polynomial ﬁtting functions, 268
support program for, 300–302
Pathogens, 224
Permutations, 80–81

Index
311
Poincaré disk model, 12–14
Polyominoes, 63–64
Power law scaling, 246
Presentations of groups
Cayley graph, 5–7
graph theory, 4–5
set WA of words
concatenation, 2–4
elementary reductions/expansions, 2
empty word, 2
equivalence classes, 3
free group, 3
relators, 3
simple R-reduction/R-expansion, 3–4
Proper k-clique-relaxed coloring, 112
Q
Quotient space, 16, 19–21, 26, 32, 41–43
Quotient topology, 19
R
Random noise, 168–169
Rank of matroid, 134
Real inner product space, 147
Regular matroids, 133
Regular two-graphs, 162
Relative error, 277
Relators, 3, 4, 8–10, 31
Relaxed coloring game, 108–109
activation strategy, 109
d-relaxed game chromatic number, 102,
108, 123
(r, d)-relaxed coloring game, 102, 108
tree strategy
color stage, 111
search stage, 110
Removed, 225
Ribbon tiles, 64
binary signature, 64
tileability test for odd-height ribbon tiles,
76
Tn, area n ribbon tiles, 64
Riemann–Hurwitz formula, 44–50
Riemann’s existence theorem, 44–47
Root space decomposition, 201
Rosenbrock method, 266, 269, 274
Rotational symmetry, 38–39
S
Scalars, 146
Semi-implicit Crank-Nicolson method,
273–275
accuracy, 298–299
Fisher-KPP equation
accuracy, 278–279
eigenvalues, 281, 282, 285, 286
non-negative eigenvalue condition for,
287
von Neumann stability analysis, 286
support program for, 301
Semi-implicit method, 273
Sensitivity analysis, 172
Signatures
A4-action, 47–50
branch points, 41–43
deﬁnition of, 43
genus 2 surface, 41–43
orbits, 41
stabilizer subgroups, 42
Signed tiling, 77–78
Simplicial complex, 25
Simpson’s rule, 270
Slope function, 267
Small-world models, vaccination strategies
behavioral epidemiology, 260
herd immunity threshold, 253
no occurrence of secondary infections, 255
quantiﬁcation of effectiveness of strategy,
253
random vaccination, 256
random vaccination strategies, 253
suggested research projects, 258
vaccinating hosts in compartment-level
models, 256
vaccination as control measure, 253
Spherical geometry, 11
Spherical subsets, 24, 26
Stabilizer subgroup, 42
Strongly almost-planar (SAP) graph
minor closed, 90–92
minor minimal, 91–93
Strongly regular graphs, 163
Susceptible, 225
Symmetry groups, compact Riemann surfaces,
see Compact Riemann surfaces,
symmetry groups
Synthesis operator, 151, 152
T
Taylor series expansions, 270, 273, 297
Test equation, 265, 269, 272, 273
Theorem of the highest weight
computation of weight multiplicity, 197
direct sum of irreducible representations,
202

312
Index
Theorem of the highest weight (cont.)
fundamental weights in simple roots, 214
Lie algebra, ﬁnite dimensional irreducible
representations, 194
one-to-one correspondence, 202
representation theory and, 202
Tile counting group, 72–74
Tiling
deﬁcient rectangle, L-trominoes, 64, 65
enumeration
domino tilings, 79
L-trominoes, 79–80
permutations, 80–81
ribbon tile, 80
polyominoes, 63–64
ribbon tiles, 64
binary signature, 64
tileability test for odd-height ribbon
tiles, 76
Tn, area n ribbon tiles, 64
staircase problem, 76
combinatorial group theory and planar
topology, 61
Conway/Lagarias invariant, 62–63
L-shaped tiles, 62
partition Sn+12, 61–62
S9 region, tiling of, 61, 62
three 8-ominoes, 63, 64
tile invariants
area invariant, 66, 67
boundary word approach, 67–70
coloring invariants, 66–67, 74–76
Conway/Lagarias invariant, 65
height-1 ribbon tile tetrominoes, 75–77
linear combination, 65
local connectivity, 70–72
signed tiling, 77–78
tile counting group, 72–74
tile set and family of regions, 65
Toroidal graphs, 88
Total coloring game, 103
chordal graph, 121–122
total activation strategy
coloring stage, 121
search stage, 120
total game chromatic number, 120
Total game chromatic number, 120
Tree strategy
edge coloring game, 116–117
relaxed coloring game
coloring stage, 111
search stage, 110
Tripod operator, 53–54
T-tetrominoes, 72, 75
2-angle tight frame, 162, 163, 165
Two-distance tight frames, 163
Two-level numerical method, 272
U
Uniform matroid, 134
Uniform mixing, 226
Uniquely pancyclic (UPC) graphs, 129
chords, 130
Hamiltonian cycle, 130
UPC matroids, 131
binary UPC matroids, 140–143
Hamiltonian circuit, 136, 137
non-isomorphic graph, 138
rank-24 UPC matroid, 137
Unit normed tight frame, 151, 153, 156, 158,
162, 163, 167
Upper frame bound, 151, 156, 157, 159
V
Vaccination strategies for small world
clustering coefﬁcients
avoidance of clustering, 240
connection between, 239
counting number of potential triangles,
238
effective contact, 238
network clustering coefﬁcient, 239
node clustering coefﬁcient, 238
normalized clustering coefﬁcients,
239–240
one-dimensional nearest-neighbor
networks, 236
of selected networks, 241
strong clustering, 240, 242
theorem about strong clustering, 242
contact network models
connected component of node in graph,
229
edges, 227–228
k-regular graphs, 228
mean degree, 228
nodes or vertices, 227
one-dimensional nearest-neighbor
network, 229
path length, 229
deﬁnition of, 253
direct contact diseases, 224
Erd˝os-Rényi random graphs exploration
“asymptotically almost surely”
properties, 232–233
deﬁnition, 231–232

Index
313
parameter settings for, 233
random graphs, 230–231
infectious disease modeling and contact
networks
basic reproductive number, 226–227
compartment-level models, 226
generations of infection, 226
investigate spreading infectious disease,
225
IONTW software tool, 225
SIR models for immunizing infections,
225–226
IONTW guide to small world (property),
243–248
network-based models, 229
small-world networks
IONTW, small-world models in,
250–251
mathematical derivations of some
properties, 251–252
small-world models, 249–250
small-world property and, 248–249
strong clustering, 252
vaccination strategies in small-world
models
behavioral epidemiology, 260
herd immunity threshold, 253
no occurrence of secondary infections,
255
quantiﬁcation of effectiveness of
strategy, 253
random vaccination, 253, 256
suggested research projects, 258
vaccinating hosts in compartment-level
models, 256
vaccination as control measure, 253
Vector matroid, 132–133
Vertex split, 91
von Neumann criterion, 283, 284
von Neumann stability analysis, 283–286
W
Weight multiplicities computation
braid relations on Coxeter groups, 206
and Fibonacci numbers, 209
fundamental weights, 214
half sum of the positive roots, 195
hyperplanes perpendicular to simple roots,
204
identity element, 206
induction hypothesis, 211
Kostant’s weight multiplicity formula, 194,
203
and Lie algebra (see Lie algebra)
nonconsecutive integers, 209, 210, 213
nonnegative integral sum of positive roots,
194, 205, 206, 211, 212
positive integers, 193
reﬂections on the fundamental weights, 208
reﬂections perpendicular to simple roots,
195
representation theory, 193, 219
roots and the generators of, 194
set of positive roots, 193, 194
sum of positive integers, 193
technical background
history, 197–198
Lie algebra and their representation,
198–199
technical lemmas, 209–214
theorem of the highest weight, 194, 214
two types of problems, 196
Weyl alternation set, 203, 206, 215, 216,
219
Weyl group (see Weyl group)
zero weight space in the adjoint
representation, 205, 208
zero weight Weyl alternation diagram, 218,
220
Weight of the representation, 202
Weight space, 202
Welch bound, 159–161
Weyl alternation set, 203, 206
Weyl group
contributes nonzero term, 198
deﬁned, 194
denote Kostant’s partition function, 198
elements of, 194–195, 205, 206, 208
ﬁnite group, 196, 201
group generated by reﬂections, 194
of Lie algebra, 196, 205
nonzero terms, 218
nonzero value, 196
simple reﬂections of, 207
and their actions on roots, 205
uses generators, 195
zero weight space in the adjoint
representation, 205
Z
Zero weight space in adjoint representation,
205, 208

